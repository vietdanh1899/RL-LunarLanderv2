{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-satisfaction",
   "metadata": {},
   "source": [
    "# Keras-RL DQN Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-accident",
   "metadata": {},
   "source": [
    "At first we will import all necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "chronic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time  # to reduce the game speed when playing manually\n",
    "\n",
    "import gym  # Contains the game we want to play\n",
    "from pyglet.window import key  # for manual playing\n",
    "\n",
    "# import necessary blocks from keras to build the Deep Learning backbone of our agent\n",
    "from tensorflow.keras.models import Sequential  # To compose multiple Layers\n",
    "from tensorflow.keras.layers import Dense  # Fully-Connected layer\n",
    "from tensorflow.keras.layers import Activation  # Activation functions\n",
    "from tensorflow.keras.layers import Flatten  # Flatten function\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam  # Adam optimizer\n",
    "\n",
    "# Now the keras-rl2 agent. Dont get confused as it is only called rl and not keras-rl\n",
    "\n",
    "from rl.agents.dqn import DQNAgent  # Use the basic Deep-Q-Network agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bb26813",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mskyfall-blue\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/skyfall-blue/uncategorized/runs/1xo5shyn\" target=\"_blank\">smooth-paper-20</a></strong> to <a href=\"https://wandb.ai/skyfall-blue/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/skyfall-blue/uncategorized/runs/1xo5shyn?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1ab9829cd30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "wandb.init(config={\"hyper\": \"parameter\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-warehouse",
   "metadata": {},
   "source": [
    "Now we will create the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "uniform-ottawa",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = ENV_NAME = 'LunarLander-v2'  # https://gym.openai.com/envs/LunarLander-v2/\n",
    "env = gym.make(env_name)  # create the environment\n",
    "nb_actions = env.action_space.n  # get the number of possible actions\n",
    "NUMBER_STEPS = 150000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-admission",
   "metadata": {},
   "source": [
    "Lets watch how the game looks when chosing random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "naughty-membership",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()  # reset the environment to the initial state\n",
    "for _ in range(200):  # play for max 200 iterations\n",
    "    env.render(mode=\"human\")  # render the current game state on your screen\n",
    "    random_action = env.action_space.sample()  # chose a random action\n",
    "    env.step(random_action)  # execute that action\n",
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92184cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You got 188 points!\n"
     ]
    }
   ],
   "source": [
    "action = 0\n",
    "def key_press(k, mod):\n",
    "    '''\n",
    "    This function gets the key press for gym\n",
    "    '''\n",
    "    global action\n",
    "    if k == key.A: #fly left\n",
    "        action = 3\n",
    "    if k == key.D: #fly right\n",
    "        action = 1\n",
    "    if k == key.W: #fly up\n",
    "        action = 2\n",
    "    if k == key.S:\n",
    "        action = 0\n",
    "\n",
    "env.reset()\n",
    "rewards = 0\n",
    "for _ in range(1000):\n",
    "    env.render(mode=\"human\")\n",
    "    env.viewer.window.on_key_press = key_press  # update the key press\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    rewards+=1\n",
    "    if done:\n",
    "        print(f\"You got {rewards} points!\")\n",
    "        break\n",
    "    time.sleep(0.05)  # reduce speed a little bit\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-thinking",
   "metadata": {},
   "source": [
    "**TASK: Create the Neural Network for your Deep-Q-Agent**\n",
    "Take a look at the size of the action space and the size of the observation space.\n",
    "You are free to chose any architecture you want!\n",
    "Hint: It already works with three layers, each having 64 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "essential-monaco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                576       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 6,948\n",
      "Trainable params: 6,948\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# https://keras.io/api/layers/reshaping_layers/flatten/\n",
    "model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(32))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dense(nb_actions))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-spirituality",
   "metadata": {},
   "source": [
    "Lets create the DQN agent from keras-rl\n",
    "For this setting, the agent takes the following parameters:\n",
    "\n",
    "1. model = The model\n",
    "2. nb_actions = The number of actions (2 in this case)\n",
    "3. memory = The action replay memory. You can choose between the *SequentialMemory()* and *EpisodeParameterMemory() which is only used for one RL agent called CEM*\n",
    "4. nb_steps_warmup = How many iterations without training - Used to fill the memory\n",
    "5. target_model_update = When do we update the target model?\n",
    "6. Action Selection policy. You can choose between a *LinearAnnealedPolicy()*, *SoftmaxPolicy()*, *EpsGreedyQPolicy()*, *GreedyQPolicy()*, *GreedyQPolicy()*, *MaxBoltzmannQPolicy()* and *BoltzmannGumbelQPolicy()*. We use all of them during the next notebooks but feel free to try them out and inspect which works best here\n",
    "\n",
    "There are some more parameters, you can pass to the DQN Agent. Feel free to explore them, but we will also take a look at them together in the remaining notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sealed-pleasure",
   "metadata": {},
   "source": [
    "Here we initialize the circular buffer with a limit of 50000 and a window length of 1.\n",
    "The window length describes the number of subsequent actions stored for a state.\n",
    "This will be demonstrated in the next lecture, when we start dealing with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "residential-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.memory import SequentialMemory  # Sequential Memory for storing observations ( optimized circular buffer)\n",
    "\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-village",
   "metadata": {},
   "source": [
    "Then we define the Action Selection Policy: <br />\n",
    "We use *LinearAnnealedPolicy* in order to perform the epsilon greedy strategy with decaying epsilon. <br />\n",
    "*LinearAnnealedPolicy* accepts an action selection policy, its maximal and minimal values and a step number in order to create a dynimal policy. <br/>\n",
    "The minimal value epsilon can reach during training is 0.1.<br />\n",
    "For evaluation (e.g running the agent) it is fixed to 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "toxic-attack",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearAnnealedPolicy allows to decay the epsilon for the epsilon greedy strategy\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy\n",
    "\n",
    "policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), \n",
    "                              attr='eps',\n",
    "                              value_max=1.,\n",
    "                              value_min=.1,\n",
    "                              value_test=0,\n",
    "                              nb_steps=NUMBER_STEPS) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de666fcf",
   "metadata": {},
   "source": [
    "**TASK: Create the DQNAgent** <br />\n",
    "Feel free to play with the nb_steps_warump, target_model_update, batch_size and gamma parameters. <br />\n",
    "Hint:<br />\n",
    "You can try *nb_steps_warmup*=100, *target_model_update*=1000, *batch_size*=64 and *gamma*=0.992 as a first guess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "piano-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n",
    "               target_model_update=1000, policy=policy, batch_size=64, gamma=0.992)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-habitat",
   "metadata": {},
   "source": [
    "Finally we compile our model with the Adam optimizer and a learning rate of 0.001.<br />\n",
    "We log the Mean Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "opened-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use learning_rate instead of lr if you get warning\n",
    "dqn.compile(Adam(lr=0.001), metrics=['mae']) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-rabbit",
   "metadata": {},
   "source": [
    "Now we run the training for 150000 steps. You can change visualize=True if you want to watch your model learning.\n",
    "Keep in mind that this increases the running time\n",
    "The training time is around 30 min so grep your favorite beverage and stay tuned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "rational-championship",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 150000 steps ...\n",
      "     93/150000: episode: 1, duration: 0.133s, episode steps:  93, steps per second: 701, episode reward: -115.098, mean reward: -1.238 [-100.000, 12.663], mean action: 1.559 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n",
      "C:\\Users\\nguye\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`Model.state_updates` will be removed in a future version. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    210/150000: episode: 2, duration: 1.218s, episode steps: 117, steps per second:  96, episode reward: -171.760, mean reward: -1.468 [-100.000, 19.164], mean action: 1.624 [0.000, 3.000],  loss: 24.544618, mae: 1.193189, mean_q: 1.460490, mean_eps: 0.999070\n",
      "    294/150000: episode: 3, duration: 0.487s, episode steps:  84, steps per second: 172, episode reward: -80.888, mean reward: -0.963 [-100.000,  7.298], mean action: 1.583 [0.000, 3.000],  loss: 31.907632, mae: 1.597973, mean_q: 1.511593, mean_eps: 0.998491\n",
      "    414/150000: episode: 4, duration: 0.715s, episode steps: 120, steps per second: 168, episode reward: -379.890, mean reward: -3.166 [-100.000, 110.786], mean action: 1.450 [0.000, 3.000],  loss: 23.614650, mae: 2.098975, mean_q: 2.325129, mean_eps: 0.997879\n",
      "    506/150000: episode: 5, duration: 0.543s, episode steps:  92, steps per second: 169, episode reward: -331.912, mean reward: -3.608 [-100.000,  5.127], mean action: 1.598 [0.000, 3.000],  loss: 25.103136, mae: 2.500624, mean_q: 1.984108, mean_eps: 0.997243\n",
      "    594/150000: episode: 6, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -357.003, mean reward: -4.057 [-100.000,  2.393], mean action: 1.375 [0.000, 3.000],  loss: 21.315886, mae: 2.567499, mean_q: 0.764884, mean_eps: 0.996703\n",
      "    686/150000: episode: 7, duration: 0.553s, episode steps:  92, steps per second: 166, episode reward: -96.949, mean reward: -1.054 [-100.000, 11.946], mean action: 1.348 [0.000, 3.000],  loss: 20.300616, mae: 3.281930, mean_q: 0.970192, mean_eps: 0.996163\n",
      "    784/150000: episode: 8, duration: 0.590s, episode steps:  98, steps per second: 166, episode reward: -450.167, mean reward: -4.594 [-100.000,  2.428], mean action: 1.449 [0.000, 3.000],  loss: 22.135595, mae: 3.134641, mean_q: 0.807429, mean_eps: 0.995593\n",
      "    889/150000: episode: 9, duration: 0.620s, episode steps: 105, steps per second: 169, episode reward: -76.807, mean reward: -0.731 [-100.000, 11.145], mean action: 1.552 [0.000, 3.000],  loss: 17.355506, mae: 3.063171, mean_q: 0.240727, mean_eps: 0.994984\n",
      "    965/150000: episode: 10, duration: 0.450s, episode steps:  76, steps per second: 169, episode reward: -94.236, mean reward: -1.240 [-100.000, 15.678], mean action: 1.368 [0.000, 3.000],  loss: 13.721080, mae: 2.715511, mean_q: 0.482537, mean_eps: 0.994441\n",
      "   1056/150000: episode: 11, duration: 0.561s, episode steps:  91, steps per second: 162, episode reward: -84.069, mean reward: -0.924 [-100.000, 22.789], mean action: 1.549 [0.000, 3.000],  loss: 24.372398, mae: 2.923046, mean_q: 1.189298, mean_eps: 0.993940\n",
      "   1119/150000: episode: 12, duration: 0.435s, episode steps:  63, steps per second: 145, episode reward: -115.662, mean reward: -1.836 [-100.000,  6.761], mean action: 1.444 [0.000, 3.000],  loss: 22.383188, mae: 3.594937, mean_q: 1.897417, mean_eps: 0.993478\n",
      "   1216/150000: episode: 13, duration: 0.584s, episode steps:  97, steps per second: 166, episode reward: -95.299, mean reward: -0.982 [-100.000, 11.131], mean action: 1.557 [0.000, 3.000],  loss: 25.642489, mae: 3.769144, mean_q: 1.724758, mean_eps: 0.992998\n",
      "   1303/150000: episode: 14, duration: 0.529s, episode steps:  87, steps per second: 165, episode reward: -132.976, mean reward: -1.528 [-100.000,  7.319], mean action: 1.230 [0.000, 3.000],  loss: 25.364900, mae: 3.857790, mean_q: 2.121058, mean_eps: 0.992446\n",
      "   1399/150000: episode: 15, duration: 0.552s, episode steps:  96, steps per second: 174, episode reward: -316.739, mean reward: -3.299 [-100.000,  5.028], mean action: 1.552 [0.000, 3.000],  loss: 25.301258, mae: 3.805932, mean_q: 1.934462, mean_eps: 0.991897\n",
      "   1472/150000: episode: 16, duration: 0.451s, episode steps:  73, steps per second: 162, episode reward: -125.011, mean reward: -1.712 [-100.000,  7.256], mean action: 1.521 [0.000, 3.000],  loss: 22.070422, mae: 3.838246, mean_q: 1.789814, mean_eps: 0.991390\n",
      "   1545/150000: episode: 17, duration: 0.420s, episode steps:  73, steps per second: 174, episode reward: -102.498, mean reward: -1.404 [-100.000,  6.832], mean action: 1.726 [0.000, 3.000],  loss: 29.302615, mae: 3.839415, mean_q: 1.695730, mean_eps: 0.990952\n",
      "   1655/150000: episode: 18, duration: 0.662s, episode steps: 110, steps per second: 166, episode reward: -183.445, mean reward: -1.668 [-100.000, 16.506], mean action: 1.536 [0.000, 3.000],  loss: 25.980364, mae: 3.732849, mean_q: 2.085446, mean_eps: 0.990403\n",
      "   1781/150000: episode: 19, duration: 0.777s, episode steps: 126, steps per second: 162, episode reward: -60.760, mean reward: -0.482 [-100.000, 15.988], mean action: 1.635 [0.000, 3.000],  loss: 21.664564, mae: 3.725184, mean_q: 2.071692, mean_eps: 0.989695\n",
      "   1854/150000: episode: 20, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -93.250, mean reward: -1.277 [-100.000, 12.710], mean action: 1.370 [0.000, 3.000],  loss: 23.525035, mae: 3.957651, mean_q: 2.059763, mean_eps: 0.989098\n",
      "   1934/150000: episode: 21, duration: 0.512s, episode steps:  80, steps per second: 156, episode reward: -135.145, mean reward: -1.689 [-100.000, 31.207], mean action: 1.575 [0.000, 3.000],  loss: 18.134310, mae: 3.532435, mean_q: 2.298551, mean_eps: 0.988639\n",
      "   2043/150000: episode: 22, duration: 0.692s, episode steps: 109, steps per second: 157, episode reward: -278.415, mean reward: -2.554 [-100.000,  0.971], mean action: 1.569 [0.000, 3.000],  loss: 21.745122, mae: 3.923042, mean_q: 2.435586, mean_eps: 0.988072\n",
      "   2144/150000: episode: 23, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: -138.341, mean reward: -1.370 [-100.000, 27.223], mean action: 1.614 [0.000, 3.000],  loss: 15.828365, mae: 4.186327, mean_q: 2.786721, mean_eps: 0.987442\n",
      "   2250/150000: episode: 24, duration: 0.694s, episode steps: 106, steps per second: 153, episode reward: -169.733, mean reward: -1.601 [-100.000, 12.931], mean action: 1.425 [0.000, 3.000],  loss: 13.743928, mae: 4.363479, mean_q: 2.769529, mean_eps: 0.986821\n",
      "   2336/150000: episode: 25, duration: 0.539s, episode steps:  86, steps per second: 160, episode reward: -376.697, mean reward: -4.380 [-100.000,  0.112], mean action: 1.593 [0.000, 3.000],  loss: 13.825151, mae: 4.463016, mean_q: 2.750993, mean_eps: 0.986245\n",
      "   2414/150000: episode: 26, duration: 0.477s, episode steps:  78, steps per second: 164, episode reward:  1.029, mean reward:  0.013 [-100.000, 64.269], mean action: 1.551 [0.000, 3.000],  loss: 16.753841, mae: 4.663751, mean_q: 2.687259, mean_eps: 0.985753\n",
      "   2525/150000: episode: 27, duration: 0.701s, episode steps: 111, steps per second: 158, episode reward: -128.106, mean reward: -1.154 [-100.000, 14.263], mean action: 1.550 [0.000, 3.000],  loss: 18.297826, mae: 4.579930, mean_q: 2.567764, mean_eps: 0.985186\n",
      "   2618/150000: episode: 28, duration: 0.622s, episode steps:  93, steps per second: 150, episode reward: -239.842, mean reward: -2.579 [-100.000,  7.022], mean action: 1.505 [0.000, 3.000],  loss: 16.213098, mae: 4.375281, mean_q: 2.645303, mean_eps: 0.984574\n",
      "   2705/150000: episode: 29, duration: 0.670s, episode steps:  87, steps per second: 130, episode reward: -97.515, mean reward: -1.121 [-100.000, 17.995], mean action: 1.356 [0.000, 3.000],  loss: 15.457628, mae: 4.470663, mean_q: 2.523006, mean_eps: 0.984034\n",
      "   2829/150000: episode: 30, duration: 0.986s, episode steps: 124, steps per second: 126, episode reward: -120.050, mean reward: -0.968 [-100.000,  8.142], mean action: 1.573 [0.000, 3.000],  loss: 15.676964, mae: 4.335182, mean_q: 2.983806, mean_eps: 0.983401\n",
      "   2888/150000: episode: 31, duration: 0.358s, episode steps:  59, steps per second: 165, episode reward: -48.233, mean reward: -0.818 [-100.000, 12.629], mean action: 1.356 [0.000, 3.000],  loss: 21.702825, mae: 4.757501, mean_q: 2.936187, mean_eps: 0.982852\n",
      "   2980/150000: episode: 32, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -348.631, mean reward: -3.789 [-100.000,  0.277], mean action: 1.685 [0.000, 3.000],  loss: 15.528913, mae: 4.261560, mean_q: 3.086770, mean_eps: 0.982399\n",
      "   3039/150000: episode: 33, duration: 0.382s, episode steps:  59, steps per second: 154, episode reward: -86.501, mean reward: -1.466 [-100.000, 19.130], mean action: 1.441 [0.000, 3.000],  loss: 12.371638, mae: 4.738926, mean_q: 3.448950, mean_eps: 0.981946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   3140/150000: episode: 34, duration: 0.662s, episode steps: 101, steps per second: 153, episode reward: -121.345, mean reward: -1.201 [-100.000, 13.871], mean action: 1.475 [0.000, 3.000],  loss: 18.427108, mae: 5.363812, mean_q: 3.543080, mean_eps: 0.981466\n",
      "   3278/150000: episode: 35, duration: 0.886s, episode steps: 138, steps per second: 156, episode reward: -154.994, mean reward: -1.123 [-100.000,  7.176], mean action: 1.645 [0.000, 3.000],  loss: 16.396252, mae: 5.380090, mean_q: 3.677482, mean_eps: 0.980749\n",
      "   3345/150000: episode: 36, duration: 0.487s, episode steps:  67, steps per second: 138, episode reward: -95.707, mean reward: -1.428 [-100.000, 16.914], mean action: 1.478 [0.000, 3.000],  loss: 13.288225, mae: 5.221876, mean_q: 3.907543, mean_eps: 0.980134\n",
      "   3470/150000: episode: 37, duration: 0.866s, episode steps: 125, steps per second: 144, episode reward: -156.282, mean reward: -1.250 [-100.000,  5.550], mean action: 1.552 [0.000, 3.000],  loss: 18.133352, mae: 5.372973, mean_q: 3.759711, mean_eps: 0.979558\n",
      "   3576/150000: episode: 38, duration: 0.661s, episode steps: 106, steps per second: 160, episode reward: -478.025, mean reward: -4.510 [-100.000,  0.311], mean action: 1.358 [0.000, 3.000],  loss: 19.983842, mae: 5.382182, mean_q: 3.459592, mean_eps: 0.978865\n",
      "   3645/150000: episode: 39, duration: 0.475s, episode steps:  69, steps per second: 145, episode reward: -123.353, mean reward: -1.788 [-100.000, 30.942], mean action: 1.710 [0.000, 3.000],  loss: 27.674396, mae: 5.681601, mean_q: 3.236923, mean_eps: 0.978340\n",
      "   3728/150000: episode: 40, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -119.891, mean reward: -1.444 [-100.000,  6.585], mean action: 1.530 [0.000, 3.000],  loss: 19.615228, mae: 5.408394, mean_q: 3.784668, mean_eps: 0.977884\n",
      "   3813/150000: episode: 41, duration: 0.635s, episode steps:  85, steps per second: 134, episode reward: -141.807, mean reward: -1.668 [-100.000, 26.790], mean action: 1.447 [0.000, 3.000],  loss: 15.949291, mae: 5.381942, mean_q: 3.469315, mean_eps: 0.977380\n",
      "   3904/150000: episode: 42, duration: 0.745s, episode steps:  91, steps per second: 122, episode reward: -278.685, mean reward: -3.062 [-100.000, 15.930], mean action: 1.484 [0.000, 3.000],  loss: 18.016397, mae: 5.180421, mean_q: 3.489792, mean_eps: 0.976852\n",
      "   4009/150000: episode: 43, duration: 0.774s, episode steps: 105, steps per second: 136, episode reward: -327.133, mean reward: -3.116 [-100.000,  3.727], mean action: 1.629 [0.000, 3.000],  loss: 19.529468, mae: 5.072009, mean_q: 3.394358, mean_eps: 0.976264\n",
      "   4085/150000: episode: 44, duration: 0.595s, episode steps:  76, steps per second: 128, episode reward: -200.879, mean reward: -2.643 [-100.000,  7.260], mean action: 1.566 [0.000, 3.000],  loss: 16.439981, mae: 6.312844, mean_q: 3.710289, mean_eps: 0.975721\n",
      "   4172/150000: episode: 45, duration: 0.744s, episode steps:  87, steps per second: 117, episode reward: -170.397, mean reward: -1.959 [-100.000, 25.179], mean action: 1.540 [0.000, 3.000],  loss: 13.867018, mae: 6.069509, mean_q: 3.953249, mean_eps: 0.975232\n",
      "   4271/150000: episode: 46, duration: 1.053s, episode steps:  99, steps per second:  94, episode reward: -150.924, mean reward: -1.524 [-100.000,  7.881], mean action: 1.303 [0.000, 3.000],  loss: 12.670297, mae: 6.141579, mean_q: 3.726781, mean_eps: 0.974674\n",
      "   4390/150000: episode: 47, duration: 1.289s, episode steps: 119, steps per second:  92, episode reward: -131.561, mean reward: -1.106 [-100.000,  7.589], mean action: 1.479 [0.000, 3.000],  loss: 14.436843, mae: 6.127183, mean_q: 3.947853, mean_eps: 0.974020\n",
      "   4475/150000: episode: 48, duration: 0.818s, episode steps:  85, steps per second: 104, episode reward: -123.710, mean reward: -1.455 [-100.000, 10.458], mean action: 1.518 [0.000, 3.000],  loss: 14.492799, mae: 5.947930, mean_q: 4.131017, mean_eps: 0.973408\n",
      "   4568/150000: episode: 49, duration: 0.945s, episode steps:  93, steps per second:  98, episode reward: -28.945, mean reward: -0.311 [-100.000, 96.969], mean action: 1.409 [0.000, 3.000],  loss: 13.367179, mae: 6.190709, mean_q: 3.958899, mean_eps: 0.972874\n",
      "   4691/150000: episode: 50, duration: 1.063s, episode steps: 123, steps per second: 116, episode reward: -156.595, mean reward: -1.273 [-100.000,  4.874], mean action: 1.480 [0.000, 3.000],  loss: 19.648580, mae: 6.048591, mean_q: 3.874250, mean_eps: 0.972226\n",
      "   4813/150000: episode: 51, duration: 1.081s, episode steps: 122, steps per second: 113, episode reward: -64.932, mean reward: -0.532 [-100.000, 17.829], mean action: 1.557 [0.000, 3.000],  loss: 18.357118, mae: 6.086884, mean_q: 3.743352, mean_eps: 0.971491\n",
      "   4903/150000: episode: 52, duration: 0.706s, episode steps:  90, steps per second: 127, episode reward: -101.949, mean reward: -1.133 [-100.000, 14.233], mean action: 1.589 [0.000, 3.000],  loss: 16.949552, mae: 5.992758, mean_q: 4.159093, mean_eps: 0.970855\n",
      "   4981/150000: episode: 53, duration: 0.623s, episode steps:  78, steps per second: 125, episode reward: -115.312, mean reward: -1.478 [-100.000,  6.921], mean action: 1.423 [0.000, 3.000],  loss: 11.798558, mae: 5.958168, mean_q: 4.306607, mean_eps: 0.970351\n",
      "   5076/150000: episode: 54, duration: 0.719s, episode steps:  95, steps per second: 132, episode reward: -135.663, mean reward: -1.428 [-100.000, 11.008], mean action: 1.400 [0.000, 3.000],  loss: 17.837174, mae: 6.670062, mean_q: 3.969616, mean_eps: 0.969832\n",
      "   5185/150000: episode: 55, duration: 0.798s, episode steps: 109, steps per second: 137, episode reward: -282.845, mean reward: -2.595 [-100.000, 78.101], mean action: 1.440 [0.000, 3.000],  loss: 15.213002, mae: 7.172789, mean_q: 3.970953, mean_eps: 0.969220\n",
      "   5275/150000: episode: 56, duration: 0.659s, episode steps:  90, steps per second: 137, episode reward: -261.971, mean reward: -2.911 [-100.000,  6.310], mean action: 1.567 [0.000, 3.000],  loss: 14.504699, mae: 7.181021, mean_q: 3.667340, mean_eps: 0.968623\n",
      "   5400/150000: episode: 57, duration: 0.887s, episode steps: 125, steps per second: 141, episode reward: -114.557, mean reward: -0.916 [-100.000,  7.701], mean action: 1.472 [0.000, 3.000],  loss: 13.155445, mae: 6.893158, mean_q: 3.637199, mean_eps: 0.967978\n",
      "   5459/150000: episode: 58, duration: 0.379s, episode steps:  59, steps per second: 156, episode reward: -241.314, mean reward: -4.090 [-100.000, 44.872], mean action: 1.559 [0.000, 3.000],  loss: 12.104109, mae: 6.543019, mean_q: 4.285657, mean_eps: 0.967426\n",
      "   5556/150000: episode: 59, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -156.023, mean reward: -1.608 [-100.000, 18.031], mean action: 1.546 [0.000, 3.000],  loss: 12.573703, mae: 7.219485, mean_q: 3.531512, mean_eps: 0.966958\n",
      "   5620/150000: episode: 60, duration: 0.470s, episode steps:  64, steps per second: 136, episode reward: -200.643, mean reward: -3.135 [-100.000, 42.057], mean action: 1.375 [0.000, 3.000],  loss: 11.220124, mae: 6.844668, mean_q: 4.094527, mean_eps: 0.966475\n",
      "   5748/150000: episode: 61, duration: 0.883s, episode steps: 128, steps per second: 145, episode reward: -113.771, mean reward: -0.889 [-100.000, 12.246], mean action: 1.469 [0.000, 3.000],  loss: 13.972066, mae: 6.829935, mean_q: 3.585158, mean_eps: 0.965899\n",
      "   5813/150000: episode: 62, duration: 0.439s, episode steps:  65, steps per second: 148, episode reward: -128.274, mean reward: -1.973 [-100.000, 34.066], mean action: 1.462 [0.000, 3.000],  loss: 14.487140, mae: 6.972938, mean_q: 3.433629, mean_eps: 0.965320\n",
      "   5899/150000: episode: 63, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -76.410, mean reward: -0.888 [-100.000, 10.819], mean action: 1.407 [0.000, 3.000],  loss: 11.692378, mae: 6.833830, mean_q: 3.593269, mean_eps: 0.964867\n",
      "   6021/150000: episode: 64, duration: 0.865s, episode steps: 122, steps per second: 141, episode reward: -232.824, mean reward: -1.908 [-100.000,  4.818], mean action: 1.541 [0.000, 3.000],  loss: 12.544515, mae: 7.079640, mean_q: 4.117552, mean_eps: 0.964243\n",
      "   6091/150000: episode: 65, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -92.088, mean reward: -1.316 [-100.000, 36.836], mean action: 1.571 [0.000, 3.000],  loss: 13.810270, mae: 7.940336, mean_q: 4.422783, mean_eps: 0.963667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   6163/150000: episode: 66, duration: 0.461s, episode steps:  72, steps per second: 156, episode reward: -50.242, mean reward: -0.698 [-100.000, 11.999], mean action: 1.528 [0.000, 3.000],  loss: 13.400370, mae: 7.743819, mean_q: 5.035287, mean_eps: 0.963241\n",
      "   6269/150000: episode: 67, duration: 0.717s, episode steps: 106, steps per second: 148, episode reward: -285.082, mean reward: -2.689 [-100.000, 108.083], mean action: 1.500 [0.000, 3.000],  loss: 11.429059, mae: 7.966436, mean_q: 4.650674, mean_eps: 0.962707\n",
      "   6351/150000: episode: 68, duration: 0.640s, episode steps:  82, steps per second: 128, episode reward: -123.706, mean reward: -1.509 [-100.000,  8.191], mean action: 1.476 [0.000, 3.000],  loss: 12.435406, mae: 8.026945, mean_q: 4.363116, mean_eps: 0.962143\n",
      "   6488/150000: episode: 69, duration: 1.036s, episode steps: 137, steps per second: 132, episode reward: -211.840, mean reward: -1.546 [-100.000, 14.325], mean action: 1.723 [0.000, 3.000],  loss: 15.239830, mae: 8.051095, mean_q: 4.386737, mean_eps: 0.961486\n",
      "   6549/150000: episode: 70, duration: 0.459s, episode steps:  61, steps per second: 133, episode reward: -85.250, mean reward: -1.398 [-100.000, 30.569], mean action: 1.639 [0.000, 3.000],  loss: 9.300645, mae: 8.341609, mean_q: 4.252768, mean_eps: 0.960892\n",
      "   6653/150000: episode: 71, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -104.640, mean reward: -1.006 [-100.000, 12.297], mean action: 1.577 [0.000, 3.000],  loss: 12.321892, mae: 8.085947, mean_q: 4.605631, mean_eps: 0.960397\n",
      "   6723/150000: episode: 72, duration: 0.467s, episode steps:  70, steps per second: 150, episode reward: -96.039, mean reward: -1.372 [-100.000, 10.918], mean action: 1.586 [0.000, 3.000],  loss: 9.484796, mae: 8.196789, mean_q: 4.473190, mean_eps: 0.959875\n",
      "   6787/150000: episode: 73, duration: 0.424s, episode steps:  64, steps per second: 151, episode reward: -85.285, mean reward: -1.333 [-100.000,  7.534], mean action: 1.422 [0.000, 3.000],  loss: 12.936113, mae: 8.042929, mean_q: 4.599909, mean_eps: 0.959473\n",
      "   6917/150000: episode: 74, duration: 0.866s, episode steps: 130, steps per second: 150, episode reward: -78.401, mean reward: -0.603 [-100.000, 16.031], mean action: 1.331 [0.000, 3.000],  loss: 13.242859, mae: 8.070098, mean_q: 4.789736, mean_eps: 0.958891\n",
      "   7010/150000: episode: 75, duration: 0.580s, episode steps:  93, steps per second: 160, episode reward: -366.374, mean reward: -3.940 [-100.000,  0.115], mean action: 1.495 [0.000, 3.000],  loss: 14.408352, mae: 8.352772, mean_q: 4.799095, mean_eps: 0.958222\n",
      "   7071/150000: episode: 76, duration: 0.377s, episode steps:  61, steps per second: 162, episode reward: -92.532, mean reward: -1.517 [-100.000,  8.755], mean action: 1.246 [0.000, 3.000],  loss: 12.277572, mae: 9.420037, mean_q: 5.079244, mean_eps: 0.957760\n",
      "   7137/150000: episode: 77, duration: 0.411s, episode steps:  66, steps per second: 161, episode reward: -197.381, mean reward: -2.991 [-100.000, 57.200], mean action: 1.682 [0.000, 3.000],  loss: 13.439673, mae: 8.951017, mean_q: 5.209466, mean_eps: 0.957379\n",
      "   7232/150000: episode: 78, duration: 0.631s, episode steps:  95, steps per second: 151, episode reward: -36.845, mean reward: -0.388 [-100.000, 20.381], mean action: 1.432 [0.000, 3.000],  loss: 11.048859, mae: 8.879045, mean_q: 5.409947, mean_eps: 0.956896\n",
      "   7333/150000: episode: 79, duration: 0.662s, episode steps: 101, steps per second: 152, episode reward: -105.292, mean reward: -1.042 [-100.000, 12.621], mean action: 1.366 [0.000, 3.000],  loss: 10.852253, mae: 9.056146, mean_q: 5.392955, mean_eps: 0.956308\n",
      "   7432/150000: episode: 80, duration: 0.721s, episode steps:  99, steps per second: 137, episode reward: -103.479, mean reward: -1.045 [-100.000,  7.287], mean action: 1.535 [0.000, 3.000],  loss: 13.248922, mae: 9.173028, mean_q: 5.336189, mean_eps: 0.955708\n",
      "   7558/150000: episode: 81, duration: 0.846s, episode steps: 126, steps per second: 149, episode reward: -46.802, mean reward: -0.371 [-100.000,  8.500], mean action: 1.675 [0.000, 3.000],  loss: 12.487651, mae: 9.131971, mean_q: 5.332175, mean_eps: 0.955033\n",
      "   7661/150000: episode: 82, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -86.808, mean reward: -0.843 [-100.000, 14.472], mean action: 1.534 [0.000, 3.000],  loss: 13.142907, mae: 9.189865, mean_q: 5.114441, mean_eps: 0.954346\n",
      "   7747/150000: episode: 83, duration: 0.626s, episode steps:  86, steps per second: 137, episode reward: -114.220, mean reward: -1.328 [-100.000, 12.335], mean action: 1.395 [0.000, 3.000],  loss: 15.072293, mae: 9.345139, mean_q: 5.132587, mean_eps: 0.953779\n",
      "   7864/150000: episode: 84, duration: 0.809s, episode steps: 117, steps per second: 145, episode reward: -278.998, mean reward: -2.385 [-100.000, 90.948], mean action: 1.453 [0.000, 3.000],  loss: 11.507803, mae: 9.147995, mean_q: 5.350573, mean_eps: 0.953170\n",
      "   7951/150000: episode: 85, duration: 0.528s, episode steps:  87, steps per second: 165, episode reward: -205.312, mean reward: -2.360 [-100.000, 15.243], mean action: 1.529 [0.000, 3.000],  loss: 11.988898, mae: 9.700462, mean_q: 5.181514, mean_eps: 0.952558\n",
      "   8027/150000: episode: 86, duration: 0.491s, episode steps:  76, steps per second: 155, episode reward: -146.336, mean reward: -1.925 [-100.000,  9.333], mean action: 1.671 [0.000, 3.000],  loss: 14.072720, mae: 9.371829, mean_q: 5.572890, mean_eps: 0.952069\n",
      "   8107/150000: episode: 87, duration: 0.508s, episode steps:  80, steps per second: 158, episode reward: -91.867, mean reward: -1.148 [-100.000,  5.938], mean action: 1.637 [0.000, 3.000],  loss: 14.132175, mae: 10.563053, mean_q: 5.918343, mean_eps: 0.951601\n",
      "   8212/150000: episode: 88, duration: 0.728s, episode steps: 105, steps per second: 144, episode reward: -248.056, mean reward: -2.362 [-100.000,  0.485], mean action: 1.448 [0.000, 3.000],  loss: 10.850248, mae: 10.711540, mean_q: 5.223080, mean_eps: 0.951046\n",
      "   8302/150000: episode: 89, duration: 0.572s, episode steps:  90, steps per second: 157, episode reward: -81.499, mean reward: -0.906 [-100.000, 20.974], mean action: 1.578 [0.000, 3.000],  loss: 16.790941, mae: 10.507625, mean_q: 5.616654, mean_eps: 0.950461\n",
      "   8411/150000: episode: 90, duration: 0.736s, episode steps: 109, steps per second: 148, episode reward: -165.063, mean reward: -1.514 [-100.000, 11.231], mean action: 1.486 [0.000, 3.000],  loss: 14.050628, mae: 10.513776, mean_q: 5.526505, mean_eps: 0.949864\n",
      "   8503/150000: episode: 91, duration: 0.647s, episode steps:  92, steps per second: 142, episode reward: -143.237, mean reward: -1.557 [-100.000,  7.819], mean action: 1.511 [0.000, 3.000],  loss: 10.039402, mae: 10.445404, mean_q: 5.835890, mean_eps: 0.949261\n",
      "   8608/150000: episode: 92, duration: 0.690s, episode steps: 105, steps per second: 152, episode reward: -155.673, mean reward: -1.483 [-100.000,  8.567], mean action: 1.448 [0.000, 3.000],  loss: 12.924803, mae: 10.404852, mean_q: 5.812021, mean_eps: 0.948670\n",
      "   8707/150000: episode: 93, duration: 0.607s, episode steps:  99, steps per second: 163, episode reward: -124.075, mean reward: -1.253 [-100.000, 11.613], mean action: 1.384 [0.000, 3.000],  loss: 10.835006, mae: 10.550228, mean_q: 6.002290, mean_eps: 0.948058\n",
      "   8773/150000: episode: 94, duration: 0.405s, episode steps:  66, steps per second: 163, episode reward: -52.301, mean reward: -0.792 [-100.000, 13.296], mean action: 1.485 [0.000, 3.000],  loss: 11.222327, mae: 9.976468, mean_q: 6.612273, mean_eps: 0.947563\n",
      "   8898/150000: episode: 95, duration: 0.796s, episode steps: 125, steps per second: 157, episode reward: -207.181, mean reward: -1.657 [-100.000,  1.935], mean action: 1.536 [0.000, 3.000],  loss: 11.461208, mae: 10.379064, mean_q: 6.118290, mean_eps: 0.946990\n",
      "   8978/150000: episode: 96, duration: 0.491s, episode steps:  80, steps per second: 163, episode reward: -148.117, mean reward: -1.851 [-100.000, 16.557], mean action: 1.538 [0.000, 3.000],  loss: 10.333081, mae: 10.306512, mean_q: 5.640471, mean_eps: 0.946375\n",
      "   9075/150000: episode: 97, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -129.451, mean reward: -1.335 [-100.000, 16.573], mean action: 1.629 [0.000, 3.000],  loss: 11.375873, mae: 11.142555, mean_q: 5.957091, mean_eps: 0.945844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   9148/150000: episode: 98, duration: 0.491s, episode steps:  73, steps per second: 149, episode reward: -124.938, mean reward: -1.711 [-100.000,  9.914], mean action: 1.493 [0.000, 3.000],  loss: 12.929275, mae: 11.127684, mean_q: 5.931338, mean_eps: 0.945334\n",
      "   9236/150000: episode: 99, duration: 0.569s, episode steps:  88, steps per second: 155, episode reward: -311.146, mean reward: -3.536 [-100.000,  6.630], mean action: 1.602 [0.000, 3.000],  loss: 12.039179, mae: 11.220533, mean_q: 6.191321, mean_eps: 0.944851\n",
      "   9304/150000: episode: 100, duration: 0.424s, episode steps:  68, steps per second: 160, episode reward: -137.964, mean reward: -2.029 [-100.000, 15.702], mean action: 1.603 [0.000, 3.000],  loss: 8.473913, mae: 11.102570, mean_q: 5.624313, mean_eps: 0.944383\n",
      "   9369/150000: episode: 101, duration: 0.408s, episode steps:  65, steps per second: 159, episode reward: -102.909, mean reward: -1.583 [-100.000,  6.942], mean action: 1.800 [0.000, 3.000],  loss: 10.137023, mae: 11.353505, mean_q: 5.571027, mean_eps: 0.943984\n",
      "   9430/150000: episode: 102, duration: 0.445s, episode steps:  61, steps per second: 137, episode reward: -124.798, mean reward: -2.046 [-100.000,  5.235], mean action: 1.361 [0.000, 3.000],  loss: 12.355260, mae: 11.122309, mean_q: 6.054047, mean_eps: 0.943606\n",
      "   9524/150000: episode: 103, duration: 0.761s, episode steps:  94, steps per second: 124, episode reward: -112.373, mean reward: -1.195 [-100.000,  8.829], mean action: 1.617 [0.000, 3.000],  loss: 12.055286, mae: 11.191515, mean_q: 6.087221, mean_eps: 0.943141\n",
      "   9605/150000: episode: 104, duration: 0.546s, episode steps:  81, steps per second: 148, episode reward: -297.695, mean reward: -3.675 [-100.000, 110.900], mean action: 1.346 [0.000, 3.000],  loss: 10.943264, mae: 11.197520, mean_q: 6.320689, mean_eps: 0.942616\n",
      "   9704/150000: episode: 105, duration: 0.629s, episode steps:  99, steps per second: 157, episode reward: -403.225, mean reward: -4.073 [-100.000,  1.004], mean action: 1.556 [0.000, 3.000],  loss: 10.027367, mae: 11.437673, mean_q: 5.935436, mean_eps: 0.942076\n",
      "   9806/150000: episode: 106, duration: 0.665s, episode steps: 102, steps per second: 153, episode reward: -222.297, mean reward: -2.179 [-100.000,  6.119], mean action: 1.520 [0.000, 3.000],  loss: 11.551233, mae: 11.394166, mean_q: 5.647771, mean_eps: 0.941473\n",
      "   9890/150000: episode: 107, duration: 0.538s, episode steps:  84, steps per second: 156, episode reward: -121.088, mean reward: -1.442 [-100.000,  8.677], mean action: 1.655 [0.000, 3.000],  loss: 10.567910, mae: 11.496427, mean_q: 5.518203, mean_eps: 0.940915\n",
      "   9968/150000: episode: 108, duration: 0.503s, episode steps:  78, steps per second: 155, episode reward: -129.450, mean reward: -1.660 [-100.000, 41.462], mean action: 1.474 [0.000, 3.000],  loss: 13.956005, mae: 11.221833, mean_q: 6.061694, mean_eps: 0.940429\n",
      "  10080/150000: episode: 109, duration: 0.730s, episode steps: 112, steps per second: 153, episode reward: -188.992, mean reward: -1.687 [-100.000, 127.959], mean action: 1.464 [0.000, 3.000],  loss: 14.394460, mae: 12.317203, mean_q: 6.601867, mean_eps: 0.939859\n",
      "  10164/150000: episode: 110, duration: 0.550s, episode steps:  84, steps per second: 153, episode reward: -91.171, mean reward: -1.085 [-100.000, 18.431], mean action: 1.548 [0.000, 3.000],  loss: 8.768426, mae: 12.574885, mean_q: 6.389343, mean_eps: 0.939271\n",
      "  10249/150000: episode: 111, duration: 0.531s, episode steps:  85, steps per second: 160, episode reward: -23.889, mean reward: -0.281 [-100.000, 91.854], mean action: 1.529 [0.000, 3.000],  loss: 11.921529, mae: 12.956030, mean_q: 6.333822, mean_eps: 0.938764\n",
      "  10331/150000: episode: 112, duration: 0.495s, episode steps:  82, steps per second: 166, episode reward: -84.826, mean reward: -1.034 [-100.000,  6.610], mean action: 1.537 [0.000, 3.000],  loss: 9.357200, mae: 12.364864, mean_q: 7.132626, mean_eps: 0.938263\n",
      "  10417/150000: episode: 113, duration: 0.519s, episode steps:  86, steps per second: 166, episode reward: -105.541, mean reward: -1.227 [-100.000,  7.556], mean action: 1.558 [0.000, 3.000],  loss: 17.273128, mae: 12.157358, mean_q: 7.013533, mean_eps: 0.937759\n",
      "  10514/150000: episode: 114, duration: 0.649s, episode steps:  97, steps per second: 149, episode reward: -156.585, mean reward: -1.614 [-100.000,  8.594], mean action: 1.629 [0.000, 3.000],  loss: 12.123404, mae: 12.465786, mean_q: 6.745987, mean_eps: 0.937210\n",
      "  10608/150000: episode: 115, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -235.055, mean reward: -2.501 [-100.000,  0.524], mean action: 1.702 [0.000, 3.000],  loss: 15.648363, mae: 12.243364, mean_q: 7.178287, mean_eps: 0.936637\n",
      "  10713/150000: episode: 116, duration: 0.684s, episode steps: 105, steps per second: 154, episode reward: -260.500, mean reward: -2.481 [-100.000, 42.698], mean action: 1.695 [0.000, 3.000],  loss: 14.493563, mae: 12.632924, mean_q: 6.438381, mean_eps: 0.936040\n",
      "  10773/150000: episode: 117, duration: 0.404s, episode steps:  60, steps per second: 148, episode reward: -68.356, mean reward: -1.139 [-100.000,  7.579], mean action: 1.433 [0.000, 3.000],  loss: 11.178477, mae: 12.150961, mean_q: 6.074088, mean_eps: 0.935545\n",
      "  10867/150000: episode: 118, duration: 0.624s, episode steps:  94, steps per second: 151, episode reward: -153.002, mean reward: -1.628 [-100.000,  8.478], mean action: 1.564 [0.000, 3.000],  loss: 13.486776, mae: 12.568342, mean_q: 6.386041, mean_eps: 0.935083\n",
      "  10968/150000: episode: 119, duration: 0.673s, episode steps: 101, steps per second: 150, episode reward: -154.561, mean reward: -1.530 [-100.000,  5.155], mean action: 1.485 [0.000, 3.000],  loss: 11.610609, mae: 12.062466, mean_q: 7.001323, mean_eps: 0.934498\n",
      "  11048/150000: episode: 120, duration: 0.495s, episode steps:  80, steps per second: 162, episode reward: -114.111, mean reward: -1.426 [-100.000, 11.960], mean action: 1.500 [0.000, 3.000],  loss: 10.393907, mae: 12.756925, mean_q: 6.434813, mean_eps: 0.933955\n",
      "  11116/150000: episode: 121, duration: 0.449s, episode steps:  68, steps per second: 152, episode reward: -101.466, mean reward: -1.492 [-100.000,  8.578], mean action: 1.353 [0.000, 3.000],  loss: 12.976033, mae: 13.527197, mean_q: 6.834679, mean_eps: 0.933511\n",
      "  11194/150000: episode: 122, duration: 0.538s, episode steps:  78, steps per second: 145, episode reward: -87.937, mean reward: -1.127 [-100.000, 11.451], mean action: 1.590 [0.000, 3.000],  loss: 11.435376, mae: 13.201228, mean_q: 6.927893, mean_eps: 0.933073\n",
      "  11338/150000: episode: 123, duration: 0.979s, episode steps: 144, steps per second: 147, episode reward: -171.030, mean reward: -1.188 [-100.000,  6.352], mean action: 1.674 [0.000, 3.000],  loss: 12.991920, mae: 13.697908, mean_q: 6.807731, mean_eps: 0.932407\n",
      "  11421/150000: episode: 124, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -95.762, mean reward: -1.154 [-100.000, 16.835], mean action: 1.506 [0.000, 3.000],  loss: 12.107483, mae: 13.450967, mean_q: 6.965660, mean_eps: 0.931726\n",
      "  11533/150000: episode: 125, duration: 0.748s, episode steps: 112, steps per second: 150, episode reward: -50.468, mean reward: -0.451 [-100.000, 21.265], mean action: 1.571 [0.000, 3.000],  loss: 10.566610, mae: 13.508488, mean_q: 6.928934, mean_eps: 0.931141\n",
      "  11618/150000: episode: 126, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -75.451, mean reward: -0.888 [-100.000, 17.541], mean action: 1.424 [0.000, 3.000],  loss: 8.174546, mae: 13.445860, mean_q: 7.034694, mean_eps: 0.930550\n",
      "  11690/150000: episode: 127, duration: 0.461s, episode steps:  72, steps per second: 156, episode reward: -79.898, mean reward: -1.110 [-100.000,  7.513], mean action: 1.597 [0.000, 3.000],  loss: 8.459563, mae: 13.323304, mean_q: 7.372065, mean_eps: 0.930079\n",
      "  11771/150000: episode: 128, duration: 0.527s, episode steps:  81, steps per second: 154, episode reward: -361.027, mean reward: -4.457 [-100.000, 81.221], mean action: 1.593 [0.000, 3.000],  loss: 9.411826, mae: 13.880548, mean_q: 6.901197, mean_eps: 0.929620\n",
      "  11874/150000: episode: 129, duration: 0.653s, episode steps: 103, steps per second: 158, episode reward: -97.950, mean reward: -0.951 [-100.000,  7.584], mean action: 1.592 [0.000, 3.000],  loss: 8.990219, mae: 13.630288, mean_q: 7.445079, mean_eps: 0.929068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  11946/150000: episode: 130, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -138.816, mean reward: -1.928 [-100.000,  7.761], mean action: 1.597 [0.000, 3.000],  loss: 10.594880, mae: 13.542006, mean_q: 6.519893, mean_eps: 0.928543\n",
      "  12033/150000: episode: 131, duration: 0.570s, episode steps:  87, steps per second: 153, episode reward: -32.795, mean reward: -0.377 [-100.000, 11.218], mean action: 1.552 [0.000, 3.000],  loss: 12.190500, mae: 14.187948, mean_q: 7.037169, mean_eps: 0.928066\n",
      "  12137/150000: episode: 132, duration: 0.817s, episode steps: 104, steps per second: 127, episode reward: -187.941, mean reward: -1.807 [-100.000,  1.813], mean action: 1.644 [0.000, 3.000],  loss: 13.383901, mae: 15.008656, mean_q: 6.699350, mean_eps: 0.927493\n",
      "  12252/150000: episode: 133, duration: 0.843s, episode steps: 115, steps per second: 136, episode reward: -107.796, mean reward: -0.937 [-100.000,  5.788], mean action: 1.391 [0.000, 3.000],  loss: 13.526540, mae: 14.983747, mean_q: 7.089528, mean_eps: 0.926836\n",
      "  12345/150000: episode: 134, duration: 0.626s, episode steps:  93, steps per second: 149, episode reward: -124.682, mean reward: -1.341 [-100.000,  5.664], mean action: 1.570 [0.000, 3.000],  loss: 11.321056, mae: 14.755470, mean_q: 7.462072, mean_eps: 0.926212\n",
      "  12453/150000: episode: 135, duration: 0.731s, episode steps: 108, steps per second: 148, episode reward: -224.150, mean reward: -2.075 [-100.000, 50.143], mean action: 1.370 [0.000, 3.000],  loss: 8.265681, mae: 14.562903, mean_q: 7.295548, mean_eps: 0.925609\n",
      "  12573/150000: episode: 136, duration: 0.768s, episode steps: 120, steps per second: 156, episode reward: -439.905, mean reward: -3.666 [-100.000, 57.632], mean action: 1.667 [0.000, 3.000],  loss: 13.062385, mae: 14.635695, mean_q: 7.409147, mean_eps: 0.924925\n",
      "  12676/150000: episode: 137, duration: 0.690s, episode steps: 103, steps per second: 149, episode reward: -315.210, mean reward: -3.060 [-100.000,  1.435], mean action: 1.524 [0.000, 3.000],  loss: 12.360355, mae: 14.971633, mean_q: 6.256960, mean_eps: 0.924256\n",
      "  12764/150000: episode: 138, duration: 0.603s, episode steps:  88, steps per second: 146, episode reward: -90.104, mean reward: -1.024 [-100.000,  7.221], mean action: 1.477 [0.000, 3.000],  loss: 9.636107, mae: 14.711194, mean_q: 7.448241, mean_eps: 0.923683\n",
      "  12874/150000: episode: 139, duration: 0.739s, episode steps: 110, steps per second: 149, episode reward: -85.821, mean reward: -0.780 [-100.000, 87.564], mean action: 1.691 [0.000, 3.000],  loss: 9.317491, mae: 14.788428, mean_q: 7.041134, mean_eps: 0.923089\n",
      "  12959/150000: episode: 140, duration: 0.544s, episode steps:  85, steps per second: 156, episode reward: -91.702, mean reward: -1.079 [-100.000,  6.827], mean action: 1.576 [0.000, 3.000],  loss: 12.659550, mae: 14.575220, mean_q: 6.817129, mean_eps: 0.922504\n",
      "  13045/150000: episode: 141, duration: 0.656s, episode steps:  86, steps per second: 131, episode reward: -201.996, mean reward: -2.349 [-100.000,  3.889], mean action: 1.488 [0.000, 3.000],  loss: 11.320487, mae: 14.634859, mean_q: 7.538830, mean_eps: 0.921991\n",
      "  13133/150000: episode: 142, duration: 0.666s, episode steps:  88, steps per second: 132, episode reward: -151.724, mean reward: -1.724 [-100.000, 19.854], mean action: 1.341 [0.000, 3.000],  loss: 11.290389, mae: 15.260948, mean_q: 6.473481, mean_eps: 0.921469\n",
      "  13227/150000: episode: 143, duration: 0.655s, episode steps:  94, steps per second: 143, episode reward: -65.080, mean reward: -0.692 [-100.000, 28.841], mean action: 1.426 [0.000, 3.000],  loss: 7.761809, mae: 14.825413, mean_q: 7.774977, mean_eps: 0.920923\n",
      "  13291/150000: episode: 144, duration: 0.432s, episode steps:  64, steps per second: 148, episode reward: -53.342, mean reward: -0.833 [-100.000, 41.234], mean action: 1.656 [0.000, 3.000],  loss: 13.977358, mae: 15.475560, mean_q: 7.010533, mean_eps: 0.920449\n",
      "  13369/150000: episode: 145, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -93.573, mean reward: -1.200 [-100.000,  6.512], mean action: 1.603 [0.000, 3.000],  loss: 8.014302, mae: 16.012865, mean_q: 6.200238, mean_eps: 0.920023\n",
      "  13468/150000: episode: 146, duration: 0.626s, episode steps:  99, steps per second: 158, episode reward: -132.326, mean reward: -1.337 [-100.000,  4.507], mean action: 1.778 [0.000, 3.000],  loss: 7.529725, mae: 15.220029, mean_q: 7.335620, mean_eps: 0.919492\n",
      "  13570/150000: episode: 147, duration: 0.619s, episode steps: 102, steps per second: 165, episode reward: -104.650, mean reward: -1.026 [-100.000, 35.513], mean action: 1.598 [0.000, 3.000],  loss: 10.948159, mae: 15.556617, mean_q: 7.341832, mean_eps: 0.918889\n",
      "  13627/150000: episode: 148, duration: 0.357s, episode steps:  57, steps per second: 159, episode reward: -177.266, mean reward: -3.110 [-100.000,  8.109], mean action: 1.842 [0.000, 3.000],  loss: 8.766634, mae: 15.657765, mean_q: 6.920639, mean_eps: 0.918412\n",
      "  13687/150000: episode: 149, duration: 0.402s, episode steps:  60, steps per second: 149, episode reward: -76.509, mean reward: -1.275 [-100.000,  6.959], mean action: 1.550 [0.000, 3.000],  loss: 10.986724, mae: 16.173386, mean_q: 6.279199, mean_eps: 0.918061\n",
      "  13780/150000: episode: 150, duration: 0.590s, episode steps:  93, steps per second: 158, episode reward: -242.281, mean reward: -2.605 [-100.000, 35.619], mean action: 1.667 [0.000, 3.000],  loss: 8.204503, mae: 15.617425, mean_q: 7.117529, mean_eps: 0.917602\n",
      "  13856/150000: episode: 151, duration: 0.489s, episode steps:  76, steps per second: 155, episode reward: -80.422, mean reward: -1.058 [-100.000,  6.714], mean action: 1.724 [0.000, 3.000],  loss: 7.597196, mae: 15.068427, mean_q: 6.782148, mean_eps: 0.917095\n",
      "  13950/150000: episode: 152, duration: 0.598s, episode steps:  94, steps per second: 157, episode reward: -121.510, mean reward: -1.293 [-100.000, 13.293], mean action: 1.606 [0.000, 3.000],  loss: 12.540627, mae: 15.643845, mean_q: 6.554240, mean_eps: 0.916585\n",
      "  14031/150000: episode: 153, duration: 0.539s, episode steps:  81, steps per second: 150, episode reward: -65.144, mean reward: -0.804 [-100.000, 12.592], mean action: 1.617 [0.000, 3.000],  loss: 9.527960, mae: 16.017937, mean_q: 6.689892, mean_eps: 0.916060\n",
      "  14101/150000: episode: 154, duration: 0.487s, episode steps:  70, steps per second: 144, episode reward: -68.122, mean reward: -0.973 [-100.000,  8.011], mean action: 1.600 [0.000, 3.000],  loss: 8.779377, mae: 16.713633, mean_q: 7.030440, mean_eps: 0.915607\n",
      "  14169/150000: episode: 155, duration: 0.477s, episode steps:  68, steps per second: 143, episode reward: -137.230, mean reward: -2.018 [-100.000,  7.379], mean action: 1.632 [0.000, 3.000],  loss: 19.664206, mae: 16.352230, mean_q: 8.255395, mean_eps: 0.915193\n",
      "  14250/150000: episode: 156, duration: 0.623s, episode steps:  81, steps per second: 130, episode reward: -218.287, mean reward: -2.695 [-100.000,  6.372], mean action: 1.457 [0.000, 3.000],  loss: 11.755174, mae: 16.643790, mean_q: 6.823065, mean_eps: 0.914746\n",
      "  14397/150000: episode: 157, duration: 1.147s, episode steps: 147, steps per second: 128, episode reward: -76.657, mean reward: -0.521 [-100.000,  7.462], mean action: 1.565 [0.000, 3.000],  loss: 14.919827, mae: 16.453890, mean_q: 7.572069, mean_eps: 0.914062\n",
      "  14521/150000: episode: 158, duration: 0.902s, episode steps: 124, steps per second: 138, episode reward: -288.251, mean reward: -2.325 [-100.000, 78.224], mean action: 1.702 [0.000, 3.000],  loss: 12.238087, mae: 16.524942, mean_q: 7.080277, mean_eps: 0.913249\n",
      "  14602/150000: episode: 159, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -95.046, mean reward: -1.173 [-100.000,  8.566], mean action: 1.667 [0.000, 3.000],  loss: 11.527403, mae: 16.510637, mean_q: 7.064383, mean_eps: 0.912634\n",
      "  14703/150000: episode: 160, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -104.972, mean reward: -1.039 [-100.000,  8.669], mean action: 1.624 [0.000, 3.000],  loss: 13.664420, mae: 16.580495, mean_q: 7.205198, mean_eps: 0.912088\n",
      "  14826/150000: episode: 161, duration: 0.879s, episode steps: 123, steps per second: 140, episode reward: -99.343, mean reward: -0.808 [-100.000,  7.823], mean action: 1.528 [0.000, 3.000],  loss: 14.991182, mae: 16.551938, mean_q: 7.250371, mean_eps: 0.911416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  14895/150000: episode: 162, duration: 0.443s, episode steps:  69, steps per second: 156, episode reward: -85.697, mean reward: -1.242 [-100.000,  9.940], mean action: 1.507 [0.000, 3.000],  loss: 8.922827, mae: 16.574114, mean_q: 7.380854, mean_eps: 0.910840\n",
      "  14991/150000: episode: 163, duration: 0.637s, episode steps:  96, steps per second: 151, episode reward: -268.709, mean reward: -2.799 [-100.000,  4.555], mean action: 1.635 [0.000, 3.000],  loss: 10.439651, mae: 16.272304, mean_q: 7.679170, mean_eps: 0.910345\n",
      "  15080/150000: episode: 164, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -191.669, mean reward: -2.154 [-100.000, 22.924], mean action: 1.449 [0.000, 3.000],  loss: 11.294942, mae: 17.579012, mean_q: 7.130084, mean_eps: 0.909790\n",
      "  15178/150000: episode: 165, duration: 0.605s, episode steps:  98, steps per second: 162, episode reward: -112.564, mean reward: -1.149 [-100.000, 13.232], mean action: 1.541 [0.000, 3.000],  loss: 10.623696, mae: 17.486859, mean_q: 7.138445, mean_eps: 0.909229\n",
      "  15284/150000: episode: 166, duration: 0.675s, episode steps: 106, steps per second: 157, episode reward: -108.310, mean reward: -1.022 [-100.000, 13.135], mean action: 1.443 [0.000, 3.000],  loss: 11.511229, mae: 17.666696, mean_q: 7.225993, mean_eps: 0.908617\n",
      "  15370/150000: episode: 167, duration: 0.578s, episode steps:  86, steps per second: 149, episode reward: -68.675, mean reward: -0.799 [-100.000, 11.270], mean action: 1.488 [0.000, 3.000],  loss: 8.964550, mae: 17.396663, mean_q: 7.651501, mean_eps: 0.908041\n",
      "  15453/150000: episode: 168, duration: 0.624s, episode steps:  83, steps per second: 133, episode reward: -136.006, mean reward: -1.639 [-100.000, 50.240], mean action: 1.578 [0.000, 3.000],  loss: 6.201578, mae: 17.537744, mean_q: 7.824611, mean_eps: 0.907534\n",
      "  15564/150000: episode: 169, duration: 0.766s, episode steps: 111, steps per second: 145, episode reward: -41.540, mean reward: -0.374 [-100.000, 94.862], mean action: 1.514 [0.000, 3.000],  loss: 9.719827, mae: 17.287352, mean_q: 6.938051, mean_eps: 0.906952\n",
      "  15681/150000: episode: 170, duration: 0.772s, episode steps: 117, steps per second: 152, episode reward: -50.148, mean reward: -0.429 [-100.000, 18.685], mean action: 1.504 [0.000, 3.000],  loss: 11.551053, mae: 17.516600, mean_q: 7.246475, mean_eps: 0.906268\n",
      "  15799/150000: episode: 171, duration: 0.839s, episode steps: 118, steps per second: 141, episode reward: -71.945, mean reward: -0.610 [-100.000, 17.695], mean action: 1.441 [0.000, 3.000],  loss: 6.682312, mae: 17.146108, mean_q: 8.246255, mean_eps: 0.905563\n",
      "  15866/150000: episode: 172, duration: 0.537s, episode steps:  67, steps per second: 125, episode reward: -99.095, mean reward: -1.479 [-100.000,  4.408], mean action: 1.493 [0.000, 3.000],  loss: 15.145099, mae: 17.376396, mean_q: 7.434748, mean_eps: 0.905008\n",
      "  15950/150000: episode: 173, duration: 0.625s, episode steps:  84, steps per second: 134, episode reward: -144.478, mean reward: -1.720 [-100.000,  7.702], mean action: 1.464 [0.000, 3.000],  loss: 7.481525, mae: 17.283752, mean_q: 8.627468, mean_eps: 0.904555\n",
      "  16061/150000: episode: 174, duration: 0.827s, episode steps: 111, steps per second: 134, episode reward: -152.617, mean reward: -1.375 [-100.000,  3.110], mean action: 1.459 [0.000, 3.000],  loss: 10.366693, mae: 17.798408, mean_q: 7.580970, mean_eps: 0.903970\n",
      "  16138/150000: episode: 175, duration: 0.642s, episode steps:  77, steps per second: 120, episode reward: -154.392, mean reward: -2.005 [-100.000, 17.387], mean action: 1.429 [0.000, 3.000],  loss: 11.741501, mae: 18.466651, mean_q: 7.124630, mean_eps: 0.903406\n",
      "  16228/150000: episode: 176, duration: 0.699s, episode steps:  90, steps per second: 129, episode reward: -55.492, mean reward: -0.617 [-100.000,  6.514], mean action: 1.589 [0.000, 3.000],  loss: 16.462879, mae: 18.172802, mean_q: 8.864143, mean_eps: 0.902905\n",
      "  16301/150000: episode: 177, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -250.353, mean reward: -3.429 [-100.000, 31.555], mean action: 1.466 [0.000, 3.000],  loss: 11.258347, mae: 18.215853, mean_q: 7.504936, mean_eps: 0.902416\n",
      "  16431/150000: episode: 178, duration: 0.908s, episode steps: 130, steps per second: 143, episode reward: -105.325, mean reward: -0.810 [-100.000, 30.276], mean action: 1.592 [0.000, 3.000],  loss: 9.615874, mae: 18.166809, mean_q: 8.020037, mean_eps: 0.901807\n",
      "  16497/150000: episode: 179, duration: 0.497s, episode steps:  66, steps per second: 133, episode reward: -83.296, mean reward: -1.262 [-100.000, 21.265], mean action: 1.576 [0.000, 3.000],  loss: 9.702877, mae: 18.594639, mean_q: 7.146628, mean_eps: 0.901219\n",
      "  16589/150000: episode: 180, duration: 0.605s, episode steps:  92, steps per second: 152, episode reward: -134.120, mean reward: -1.458 [-100.000,  7.496], mean action: 1.457 [0.000, 3.000],  loss: 10.662786, mae: 18.606013, mean_q: 7.165043, mean_eps: 0.900745\n",
      "  16718/150000: episode: 181, duration: 1.125s, episode steps: 129, steps per second: 115, episode reward: -123.731, mean reward: -0.959 [-100.000, 26.780], mean action: 1.558 [0.000, 3.000],  loss: 10.841178, mae: 17.994066, mean_q: 7.738415, mean_eps: 0.900082\n",
      "  16802/150000: episode: 182, duration: 0.595s, episode steps:  84, steps per second: 141, episode reward: -351.946, mean reward: -4.190 [-100.000,  0.404], mean action: 1.714 [0.000, 3.000],  loss: 10.853743, mae: 18.706087, mean_q: 7.423218, mean_eps: 0.899443\n",
      "  16921/150000: episode: 183, duration: 0.752s, episode steps: 119, steps per second: 158, episode reward: -80.498, mean reward: -0.676 [-100.000, 17.892], mean action: 1.580 [0.000, 3.000],  loss: 8.817826, mae: 17.967051, mean_q: 8.158943, mean_eps: 0.898834\n",
      "  16989/150000: episode: 184, duration: 0.413s, episode steps:  68, steps per second: 165, episode reward: -135.250, mean reward: -1.989 [-100.000, 31.043], mean action: 1.603 [0.000, 3.000],  loss: 10.417955, mae: 18.694812, mean_q: 7.562504, mean_eps: 0.898273\n",
      "  17107/150000: episode: 185, duration: 0.773s, episode steps: 118, steps per second: 153, episode reward: -84.034, mean reward: -0.712 [-100.000, 46.734], mean action: 1.602 [0.000, 3.000],  loss: 8.828962, mae: 18.839963, mean_q: 8.113135, mean_eps: 0.897715\n",
      "  17192/150000: episode: 186, duration: 0.542s, episode steps:  85, steps per second: 157, episode reward: -103.210, mean reward: -1.214 [-100.000,  7.573], mean action: 1.600 [0.000, 3.000],  loss: 12.758458, mae: 18.691289, mean_q: 8.120745, mean_eps: 0.897106\n",
      "  17313/150000: episode: 187, duration: 0.750s, episode steps: 121, steps per second: 161, episode reward: -161.854, mean reward: -1.338 [-100.000, 31.681], mean action: 1.636 [0.000, 3.000],  loss: 9.283376, mae: 19.235897, mean_q: 7.565408, mean_eps: 0.896488\n",
      "  17400/150000: episode: 188, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -184.117, mean reward: -2.116 [-100.000, 39.019], mean action: 1.425 [0.000, 3.000],  loss: 10.778050, mae: 18.791201, mean_q: 6.931871, mean_eps: 0.895864\n",
      "  17514/150000: episode: 189, duration: 0.747s, episode steps: 114, steps per second: 153, episode reward: -148.272, mean reward: -1.301 [-100.000, 33.866], mean action: 1.807 [0.000, 3.000],  loss: 8.144416, mae: 19.168386, mean_q: 7.485106, mean_eps: 0.895261\n",
      "  17605/150000: episode: 190, duration: 0.570s, episode steps:  91, steps per second: 160, episode reward: -99.852, mean reward: -1.097 [-100.000, 18.576], mean action: 1.703 [0.000, 3.000],  loss: 10.513468, mae: 18.930559, mean_q: 7.641686, mean_eps: 0.894646\n",
      "  17722/150000: episode: 191, duration: 0.725s, episode steps: 117, steps per second: 161, episode reward: -97.885, mean reward: -0.837 [-100.000, 16.629], mean action: 1.487 [0.000, 3.000],  loss: 11.702518, mae: 18.693685, mean_q: 7.444970, mean_eps: 0.894022\n",
      "  17846/150000: episode: 192, duration: 0.797s, episode steps: 124, steps per second: 156, episode reward: -70.552, mean reward: -0.569 [-100.000, 12.669], mean action: 1.605 [0.000, 3.000],  loss: 10.096229, mae: 18.584124, mean_q: 7.743111, mean_eps: 0.893299\n",
      "  17939/150000: episode: 193, duration: 0.579s, episode steps:  93, steps per second: 161, episode reward: -107.261, mean reward: -1.153 [-100.000,  6.679], mean action: 1.333 [0.000, 3.000],  loss: 7.031063, mae: 19.109713, mean_q: 7.372798, mean_eps: 0.892648\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  18025/150000: episode: 194, duration: 0.536s, episode steps:  86, steps per second: 160, episode reward: -107.255, mean reward: -1.247 [-100.000, 27.514], mean action: 1.488 [0.000, 3.000],  loss: 6.349089, mae: 18.902823, mean_q: 7.534113, mean_eps: 0.892111\n",
      "  18141/150000: episode: 195, duration: 0.780s, episode steps: 116, steps per second: 149, episode reward: -71.174, mean reward: -0.614 [-100.000, 12.258], mean action: 1.500 [0.000, 3.000],  loss: 11.313719, mae: 20.082786, mean_q: 7.672297, mean_eps: 0.891505\n",
      "  18229/150000: episode: 196, duration: 0.557s, episode steps:  88, steps per second: 158, episode reward: -53.625, mean reward: -0.609 [-100.000, 15.754], mean action: 1.500 [0.000, 3.000],  loss: 7.659841, mae: 19.732425, mean_q: 7.785956, mean_eps: 0.890893\n",
      "  18298/150000: episode: 197, duration: 0.473s, episode steps:  69, steps per second: 146, episode reward: -120.373, mean reward: -1.745 [-100.000,  8.433], mean action: 1.681 [0.000, 3.000],  loss: 10.017442, mae: 19.735716, mean_q: 7.419286, mean_eps: 0.890422\n",
      "  18382/150000: episode: 198, duration: 0.521s, episode steps:  84, steps per second: 161, episode reward: -192.632, mean reward: -2.293 [-100.000,  5.345], mean action: 1.476 [0.000, 3.000],  loss: 7.976857, mae: 19.659109, mean_q: 8.290318, mean_eps: 0.889963\n",
      "  18472/150000: episode: 199, duration: 0.596s, episode steps:  90, steps per second: 151, episode reward: -99.473, mean reward: -1.105 [-100.000,  7.078], mean action: 1.578 [0.000, 3.000],  loss: 7.030227, mae: 19.532912, mean_q: 8.666698, mean_eps: 0.889441\n",
      "  18601/150000: episode: 200, duration: 0.827s, episode steps: 129, steps per second: 156, episode reward: -74.931, mean reward: -0.581 [-100.000, 13.464], mean action: 1.496 [0.000, 3.000],  loss: 7.472643, mae: 19.476080, mean_q: 7.996724, mean_eps: 0.888784\n",
      "  18680/150000: episode: 201, duration: 0.498s, episode steps:  79, steps per second: 159, episode reward: -101.253, mean reward: -1.282 [-100.000,  7.603], mean action: 1.646 [0.000, 3.000],  loss: 6.094900, mae: 19.781719, mean_q: 8.405573, mean_eps: 0.888160\n",
      "  18813/150000: episode: 202, duration: 0.863s, episode steps: 133, steps per second: 154, episode reward: -38.435, mean reward: -0.289 [-100.000, 96.634], mean action: 1.504 [0.000, 3.000],  loss: 10.192111, mae: 19.511451, mean_q: 7.982678, mean_eps: 0.887524\n",
      "  18896/150000: episode: 203, duration: 0.525s, episode steps:  83, steps per second: 158, episode reward: -81.142, mean reward: -0.978 [-100.000, 14.280], mean action: 1.506 [0.000, 3.000],  loss: 9.404693, mae: 19.997385, mean_q: 7.716247, mean_eps: 0.886876\n",
      "  18996/150000: episode: 204, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -76.538, mean reward: -0.765 [-100.000,  9.433], mean action: 1.500 [0.000, 3.000],  loss: 8.271892, mae: 19.201341, mean_q: 8.835458, mean_eps: 0.886327\n",
      "  19052/150000: episode: 205, duration: 0.346s, episode steps:  56, steps per second: 162, episode reward: -85.928, mean reward: -1.534 [-100.000,  5.591], mean action: 1.464 [0.000, 3.000],  loss: 5.331395, mae: 21.044201, mean_q: 8.824346, mean_eps: 0.885859\n",
      "  19152/150000: episode: 206, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: -76.886, mean reward: -0.769 [-100.000,  6.656], mean action: 1.620 [0.000, 3.000],  loss: 8.822658, mae: 20.778007, mean_q: 7.927048, mean_eps: 0.885391\n",
      "  19266/150000: episode: 207, duration: 0.741s, episode steps: 114, steps per second: 154, episode reward: -139.308, mean reward: -1.222 [-100.000, 14.415], mean action: 1.561 [0.000, 3.000],  loss: 14.558546, mae: 21.212594, mean_q: 7.813957, mean_eps: 0.884749\n",
      "  19330/150000: episode: 208, duration: 0.406s, episode steps:  64, steps per second: 158, episode reward: -91.806, mean reward: -1.434 [-100.000, 11.553], mean action: 1.578 [0.000, 3.000],  loss: 8.398684, mae: 21.080510, mean_q: 8.859583, mean_eps: 0.884215\n",
      "  19447/150000: episode: 209, duration: 0.752s, episode steps: 117, steps per second: 156, episode reward: -3.446, mean reward: -0.029 [-100.000, 133.110], mean action: 1.513 [0.000, 3.000],  loss: 8.542378, mae: 21.183170, mean_q: 7.397651, mean_eps: 0.883672\n",
      "  19525/150000: episode: 210, duration: 0.503s, episode steps:  78, steps per second: 155, episode reward: -140.787, mean reward: -1.805 [-100.000,  7.841], mean action: 1.205 [0.000, 3.000],  loss: 6.343709, mae: 20.773852, mean_q: 8.892349, mean_eps: 0.883087\n",
      "  19593/150000: episode: 211, duration: 0.433s, episode steps:  68, steps per second: 157, episode reward: -91.065, mean reward: -1.339 [-100.000, 62.717], mean action: 1.632 [0.000, 3.000],  loss: 6.914142, mae: 21.027030, mean_q: 7.514275, mean_eps: 0.882649\n",
      "  19665/150000: episode: 212, duration: 0.456s, episode steps:  72, steps per second: 158, episode reward: -84.216, mean reward: -1.170 [-100.000, 20.807], mean action: 1.389 [0.000, 3.000],  loss: 17.706742, mae: 20.622881, mean_q: 7.769143, mean_eps: 0.882229\n",
      "  19780/150000: episode: 213, duration: 0.747s, episode steps: 115, steps per second: 154, episode reward: -84.219, mean reward: -0.732 [-100.000,  8.118], mean action: 1.530 [0.000, 3.000],  loss: 9.529319, mae: 21.064092, mean_q: 7.907050, mean_eps: 0.881668\n",
      "  19884/150000: episode: 214, duration: 0.664s, episode steps: 104, steps per second: 157, episode reward: -86.296, mean reward: -0.830 [-100.000,  7.897], mean action: 1.692 [0.000, 3.000],  loss: 5.465820, mae: 20.805060, mean_q: 7.863950, mean_eps: 0.881011\n",
      "  19970/150000: episode: 215, duration: 0.539s, episode steps:  86, steps per second: 159, episode reward: -156.270, mean reward: -1.817 [-100.000, 10.443], mean action: 1.616 [0.000, 3.000],  loss: 6.640847, mae: 21.372467, mean_q: 7.491424, mean_eps: 0.880441\n",
      "  20045/150000: episode: 216, duration: 0.459s, episode steps:  75, steps per second: 163, episode reward: -98.052, mean reward: -1.307 [-100.000, 10.152], mean action: 1.480 [0.000, 3.000],  loss: 6.724036, mae: 21.834071, mean_q: 8.636888, mean_eps: 0.879958\n",
      "  20119/150000: episode: 217, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -135.520, mean reward: -1.831 [-100.000, 16.912], mean action: 1.473 [0.000, 3.000],  loss: 11.329150, mae: 22.275876, mean_q: 8.326961, mean_eps: 0.879511\n",
      "  20240/150000: episode: 218, duration: 0.766s, episode steps: 121, steps per second: 158, episode reward: -84.876, mean reward: -0.701 [-100.000,  9.739], mean action: 1.678 [0.000, 3.000],  loss: 9.526022, mae: 21.496211, mean_q: 9.528322, mean_eps: 0.878926\n",
      "  20299/150000: episode: 219, duration: 0.371s, episode steps:  59, steps per second: 159, episode reward: -66.820, mean reward: -1.133 [-100.000, 15.848], mean action: 1.525 [0.000, 3.000],  loss: 6.313708, mae: 21.787911, mean_q: 8.991735, mean_eps: 0.878386\n",
      "  20371/150000: episode: 220, duration: 0.441s, episode steps:  72, steps per second: 163, episode reward: -85.039, mean reward: -1.181 [-100.000,  5.733], mean action: 1.528 [0.000, 3.000],  loss: 9.126885, mae: 22.132461, mean_q: 9.274862, mean_eps: 0.877993\n",
      "  20488/150000: episode: 221, duration: 0.768s, episode steps: 117, steps per second: 152, episode reward: -204.073, mean reward: -1.744 [-100.000, 24.541], mean action: 1.470 [0.000, 3.000],  loss: 7.161215, mae: 21.872134, mean_q: 9.275556, mean_eps: 0.877426\n",
      "  20557/150000: episode: 222, duration: 0.434s, episode steps:  69, steps per second: 159, episode reward: -51.222, mean reward: -0.742 [-100.000, 12.618], mean action: 1.580 [0.000, 3.000],  loss: 6.946953, mae: 21.859357, mean_q: 8.822259, mean_eps: 0.876868\n",
      "  20629/150000: episode: 223, duration: 0.448s, episode steps:  72, steps per second: 161, episode reward: -81.732, mean reward: -1.135 [-100.000,  5.818], mean action: 1.639 [0.000, 3.000],  loss: 9.312547, mae: 22.507777, mean_q: 8.882951, mean_eps: 0.876445\n",
      "  20708/150000: episode: 224, duration: 0.495s, episode steps:  79, steps per second: 160, episode reward: -113.749, mean reward: -1.440 [-100.000, 17.822], mean action: 1.380 [0.000, 3.000],  loss: 21.726519, mae: 22.189366, mean_q: 8.199723, mean_eps: 0.875992\n",
      "  20780/150000: episode: 225, duration: 0.480s, episode steps:  72, steps per second: 150, episode reward: -77.318, mean reward: -1.074 [-100.000,  5.286], mean action: 1.500 [0.000, 3.000],  loss: 8.069377, mae: 21.965464, mean_q: 8.388654, mean_eps: 0.875539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  20877/150000: episode: 226, duration: 0.624s, episode steps:  97, steps per second: 155, episode reward: -123.656, mean reward: -1.275 [-100.000,  5.304], mean action: 1.794 [0.000, 3.000],  loss: 6.823948, mae: 22.218126, mean_q: 9.224854, mean_eps: 0.875032\n",
      "  20938/150000: episode: 227, duration: 0.384s, episode steps:  61, steps per second: 159, episode reward: -42.857, mean reward: -0.703 [-100.000, 16.644], mean action: 1.525 [0.000, 3.000],  loss: 12.613511, mae: 22.113790, mean_q: 9.168183, mean_eps: 0.874558\n",
      "  21003/150000: episode: 228, duration: 0.402s, episode steps:  65, steps per second: 162, episode reward: -35.461, mean reward: -0.546 [-100.000, 23.445], mean action: 1.538 [0.000, 3.000],  loss: 7.793853, mae: 22.083455, mean_q: 9.742774, mean_eps: 0.874180\n",
      "  21084/150000: episode: 229, duration: 0.501s, episode steps:  81, steps per second: 162, episode reward: -101.666, mean reward: -1.255 [-100.000,  8.232], mean action: 1.642 [0.000, 3.000],  loss: 9.019715, mae: 22.879476, mean_q: 9.805700, mean_eps: 0.873742\n",
      "  21162/150000: episode: 230, duration: 0.528s, episode steps:  78, steps per second: 148, episode reward: -126.833, mean reward: -1.626 [-100.000, 38.193], mean action: 1.385 [0.000, 3.000],  loss: 10.056599, mae: 23.461335, mean_q: 8.579105, mean_eps: 0.873265\n",
      "  21226/150000: episode: 231, duration: 0.410s, episode steps:  64, steps per second: 156, episode reward: -84.517, mean reward: -1.321 [-100.000,  6.729], mean action: 1.328 [0.000, 3.000],  loss: 8.668160, mae: 22.679077, mean_q: 9.581096, mean_eps: 0.872839\n",
      "  21314/150000: episode: 232, duration: 0.550s, episode steps:  88, steps per second: 160, episode reward: -250.656, mean reward: -2.848 [-100.000,  6.211], mean action: 1.455 [0.000, 3.000],  loss: 13.244479, mae: 22.968193, mean_q: 9.522601, mean_eps: 0.872383\n",
      "  21395/150000: episode: 233, duration: 0.502s, episode steps:  81, steps per second: 162, episode reward: -150.500, mean reward: -1.858 [-100.000,  6.336], mean action: 1.457 [0.000, 3.000],  loss: 6.644822, mae: 22.990564, mean_q: 9.701810, mean_eps: 0.871876\n",
      "  21473/150000: episode: 234, duration: 0.526s, episode steps:  78, steps per second: 148, episode reward: -115.066, mean reward: -1.475 [-100.000,  8.204], mean action: 1.551 [0.000, 3.000],  loss: 11.171973, mae: 23.391291, mean_q: 8.951751, mean_eps: 0.871399\n",
      "  21588/150000: episode: 235, duration: 0.731s, episode steps: 115, steps per second: 157, episode reward: -137.114, mean reward: -1.192 [-100.000, 22.169], mean action: 1.383 [0.000, 3.000],  loss: 8.201370, mae: 22.630695, mean_q: 9.356466, mean_eps: 0.870820\n",
      "  21679/150000: episode: 236, duration: 0.557s, episode steps:  91, steps per second: 163, episode reward: -85.421, mean reward: -0.939 [-100.000,  6.923], mean action: 1.725 [0.000, 3.000],  loss: 6.094671, mae: 22.775588, mean_q: 9.072569, mean_eps: 0.870202\n",
      "  21774/150000: episode: 237, duration: 0.590s, episode steps:  95, steps per second: 161, episode reward: -114.181, mean reward: -1.202 [-100.000,  9.493], mean action: 1.526 [0.000, 3.000],  loss: 6.876441, mae: 22.641122, mean_q: 9.201949, mean_eps: 0.869644\n",
      "  21847/150000: episode: 238, duration: 0.488s, episode steps:  73, steps per second: 149, episode reward: -86.487, mean reward: -1.185 [-100.000,  9.054], mean action: 1.521 [0.000, 3.000],  loss: 5.175130, mae: 22.713048, mean_q: 9.308610, mean_eps: 0.869140\n",
      "  21938/150000: episode: 239, duration: 0.576s, episode steps:  91, steps per second: 158, episode reward: -131.308, mean reward: -1.443 [-100.000, 33.493], mean action: 1.429 [0.000, 3.000],  loss: 7.196986, mae: 22.939020, mean_q: 9.446867, mean_eps: 0.868648\n",
      "  22040/150000: episode: 240, duration: 0.646s, episode steps: 102, steps per second: 158, episode reward: -173.055, mean reward: -1.697 [-100.000,  8.145], mean action: 1.471 [0.000, 3.000],  loss: 9.927540, mae: 23.006929, mean_q: 9.283925, mean_eps: 0.868069\n",
      "  22118/150000: episode: 241, duration: 0.484s, episode steps:  78, steps per second: 161, episode reward: -65.361, mean reward: -0.838 [-100.000, 19.704], mean action: 1.526 [0.000, 3.000],  loss: 12.431535, mae: 23.311359, mean_q: 9.966584, mean_eps: 0.867529\n",
      "  22222/150000: episode: 242, duration: 0.676s, episode steps: 104, steps per second: 154, episode reward: -86.454, mean reward: -0.831 [-100.000, 11.015], mean action: 1.558 [0.000, 3.000],  loss: 12.774572, mae: 23.237092, mean_q: 9.575196, mean_eps: 0.866983\n",
      "  22334/150000: episode: 243, duration: 0.692s, episode steps: 112, steps per second: 162, episode reward: -200.628, mean reward: -1.791 [-100.000,  1.684], mean action: 1.598 [0.000, 3.000],  loss: 7.801349, mae: 23.097096, mean_q: 9.828577, mean_eps: 0.866335\n",
      "  22405/150000: episode: 244, duration: 0.462s, episode steps:  71, steps per second: 154, episode reward: -110.261, mean reward: -1.553 [-100.000,  8.708], mean action: 1.676 [0.000, 3.000],  loss: 11.076716, mae: 22.779967, mean_q: 10.305810, mean_eps: 0.865786\n",
      "  22506/150000: episode: 245, duration: 0.663s, episode steps: 101, steps per second: 152, episode reward: -131.983, mean reward: -1.307 [-100.000, 11.120], mean action: 1.416 [0.000, 3.000],  loss: 7.823640, mae: 23.139576, mean_q: 10.090708, mean_eps: 0.865270\n",
      "  22589/150000: episode: 246, duration: 0.523s, episode steps:  83, steps per second: 159, episode reward: -86.982, mean reward: -1.048 [-100.000, 12.116], mean action: 1.410 [0.000, 3.000],  loss: 7.174603, mae: 23.406656, mean_q: 9.949354, mean_eps: 0.864718\n",
      "  22716/150000: episode: 247, duration: 0.795s, episode steps: 127, steps per second: 160, episode reward: -159.510, mean reward: -1.256 [-100.000, 12.834], mean action: 1.677 [0.000, 3.000],  loss: 7.114228, mae: 23.367514, mean_q: 9.942548, mean_eps: 0.864088\n",
      "  22840/150000: episode: 248, duration: 0.796s, episode steps: 124, steps per second: 156, episode reward: -174.652, mean reward: -1.408 [-100.000,  4.550], mean action: 1.613 [0.000, 3.000],  loss: 9.981604, mae: 23.564270, mean_q: 9.721359, mean_eps: 0.863335\n",
      "  22905/150000: episode: 249, duration: 0.416s, episode steps:  65, steps per second: 156, episode reward: -104.824, mean reward: -1.613 [-100.000,  7.513], mean action: 1.615 [0.000, 3.000],  loss: 12.505203, mae: 23.173022, mean_q: 9.817061, mean_eps: 0.862768\n",
      "  22979/150000: episode: 250, duration: 0.467s, episode steps:  74, steps per second: 159, episode reward: -5.385, mean reward: -0.073 [-100.000, 17.185], mean action: 1.595 [0.000, 3.000],  loss: 7.117789, mae: 23.066391, mean_q: 9.187196, mean_eps: 0.862351\n",
      "  23054/150000: episode: 251, duration: 0.465s, episode steps:  75, steps per second: 161, episode reward: -88.113, mean reward: -1.175 [-100.000, 15.245], mean action: 1.360 [0.000, 3.000],  loss: 7.134691, mae: 23.771190, mean_q: 10.271324, mean_eps: 0.861904\n",
      "  23153/150000: episode: 252, duration: 0.632s, episode steps:  99, steps per second: 157, episode reward: -97.470, mean reward: -0.985 [-100.000,  8.712], mean action: 1.616 [0.000, 3.000],  loss: 6.455176, mae: 23.921343, mean_q: 10.455506, mean_eps: 0.861382\n",
      "  23237/150000: episode: 253, duration: 0.544s, episode steps:  84, steps per second: 154, episode reward: -85.137, mean reward: -1.014 [-100.000, 13.593], mean action: 1.595 [0.000, 3.000],  loss: 6.217886, mae: 23.856830, mean_q: 10.293542, mean_eps: 0.860833\n",
      "  23360/150000: episode: 254, duration: 0.778s, episode steps: 123, steps per second: 158, episode reward: -55.932, mean reward: -0.455 [-100.000, 12.845], mean action: 1.626 [0.000, 3.000],  loss: 7.848247, mae: 23.647751, mean_q: 10.398873, mean_eps: 0.860212\n",
      "  23440/150000: episode: 255, duration: 0.497s, episode steps:  80, steps per second: 161, episode reward: -65.548, mean reward: -0.819 [-100.000,  9.526], mean action: 1.525 [0.000, 3.000],  loss: 8.172085, mae: 23.620971, mean_q: 11.152214, mean_eps: 0.859603\n",
      "  23558/150000: episode: 256, duration: 0.769s, episode steps: 118, steps per second: 153, episode reward: -74.425, mean reward: -0.631 [-100.000, 13.399], mean action: 1.449 [0.000, 3.000],  loss: 15.435926, mae: 24.021821, mean_q: 10.441952, mean_eps: 0.859009\n",
      "  23684/150000: episode: 257, duration: 0.801s, episode steps: 126, steps per second: 157, episode reward: -115.290, mean reward: -0.915 [-100.000,  7.512], mean action: 1.706 [0.000, 3.000],  loss: 8.741194, mae: 24.040603, mean_q: 10.355318, mean_eps: 0.858277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  23750/150000: episode: 258, duration: 0.412s, episode steps:  66, steps per second: 160, episode reward: -87.172, mean reward: -1.321 [-100.000, 10.715], mean action: 1.667 [0.000, 3.000],  loss: 8.767964, mae: 23.961454, mean_q: 9.965940, mean_eps: 0.857701\n",
      "  23823/150000: episode: 259, duration: 0.470s, episode steps:  73, steps per second: 155, episode reward: 45.813, mean reward:  0.628 [-100.000, 131.430], mean action: 1.589 [0.000, 3.000],  loss: 7.190174, mae: 23.789901, mean_q: 10.638375, mean_eps: 0.857284\n",
      "  23958/150000: episode: 260, duration: 0.876s, episode steps: 135, steps per second: 154, episode reward: -81.745, mean reward: -0.606 [-100.000,  9.963], mean action: 1.496 [0.000, 3.000],  loss: 11.059071, mae: 23.926115, mean_q: 10.773417, mean_eps: 0.856660\n",
      "  24021/150000: episode: 261, duration: 0.393s, episode steps:  63, steps per second: 160, episode reward: -91.382, mean reward: -1.451 [-100.000,  6.259], mean action: 1.857 [0.000, 3.000],  loss: 8.974356, mae: 24.207237, mean_q: 11.113940, mean_eps: 0.856066\n",
      "  24094/150000: episode: 262, duration: 0.454s, episode steps:  73, steps per second: 161, episode reward: -52.730, mean reward: -0.722 [-100.000, 11.849], mean action: 1.562 [0.000, 3.000],  loss: 7.059006, mae: 24.318617, mean_q: 11.092889, mean_eps: 0.855658\n",
      "  24172/150000: episode: 263, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -101.721, mean reward: -1.304 [-100.000,  7.751], mean action: 1.385 [0.000, 3.000],  loss: 10.426828, mae: 24.756396, mean_q: 10.620101, mean_eps: 0.855205\n",
      "  24262/150000: episode: 264, duration: 0.703s, episode steps:  90, steps per second: 128, episode reward: -100.765, mean reward: -1.120 [-100.000,  9.955], mean action: 1.800 [0.000, 3.000],  loss: 7.187333, mae: 24.204444, mean_q: 10.781003, mean_eps: 0.854701\n",
      "  24347/150000: episode: 265, duration: 0.602s, episode steps:  85, steps per second: 141, episode reward: -127.792, mean reward: -1.503 [-100.000, 16.808], mean action: 1.659 [0.000, 3.000],  loss: 8.506203, mae: 24.357103, mean_q: 11.850988, mean_eps: 0.854176\n",
      "  24436/150000: episode: 266, duration: 0.640s, episode steps:  89, steps per second: 139, episode reward: -92.193, mean reward: -1.036 [-100.000,  8.281], mean action: 1.449 [0.000, 3.000],  loss: 5.393649, mae: 24.325101, mean_q: 11.855529, mean_eps: 0.853654\n",
      "  24556/150000: episode: 267, duration: 0.887s, episode steps: 120, steps per second: 135, episode reward: -100.864, mean reward: -0.841 [-100.000,  6.861], mean action: 1.533 [0.000, 3.000],  loss: 11.514071, mae: 24.167256, mean_q: 11.276886, mean_eps: 0.853027\n",
      "  24644/150000: episode: 268, duration: 0.609s, episode steps:  88, steps per second: 145, episode reward: -132.741, mean reward: -1.508 [-100.000, 14.478], mean action: 1.670 [0.000, 3.000],  loss: 13.146259, mae: 24.588589, mean_q: 11.487566, mean_eps: 0.852403\n",
      "  24762/150000: episode: 269, duration: 0.806s, episode steps: 118, steps per second: 146, episode reward: -78.711, mean reward: -0.667 [-100.000, 11.432], mean action: 1.517 [0.000, 3.000],  loss: 8.416912, mae: 24.822749, mean_q: 11.270550, mean_eps: 0.851785\n",
      "  24872/150000: episode: 270, duration: 0.753s, episode steps: 110, steps per second: 146, episode reward: -68.135, mean reward: -0.619 [-100.000, 18.004], mean action: 1.609 [0.000, 3.000],  loss: 7.156376, mae: 24.666934, mean_q: 11.038063, mean_eps: 0.851101\n",
      "  25002/150000: episode: 271, duration: 0.816s, episode steps: 130, steps per second: 159, episode reward: -44.354, mean reward: -0.341 [-100.000, 23.340], mean action: 1.562 [0.000, 3.000],  loss: 10.362331, mae: 24.232456, mean_q: 11.628600, mean_eps: 0.850381\n",
      "  25088/150000: episode: 272, duration: 0.543s, episode steps:  86, steps per second: 159, episode reward: -190.244, mean reward: -2.212 [-100.000,  4.866], mean action: 1.616 [0.000, 3.000],  loss: 7.263087, mae: 24.836510, mean_q: 12.061356, mean_eps: 0.849733\n",
      "  25183/150000: episode: 273, duration: 0.623s, episode steps:  95, steps per second: 152, episode reward: -122.863, mean reward: -1.293 [-100.000, 16.286], mean action: 1.411 [0.000, 3.000],  loss: 13.784942, mae: 24.907736, mean_q: 11.916348, mean_eps: 0.849190\n",
      "  25302/150000: episode: 274, duration: 0.761s, episode steps: 119, steps per second: 156, episode reward: -265.517, mean reward: -2.231 [-100.000,  0.926], mean action: 1.513 [0.000, 3.000],  loss: 8.172749, mae: 24.988684, mean_q: 11.428060, mean_eps: 0.848548\n",
      "  25387/150000: episode: 275, duration: 0.641s, episode steps:  85, steps per second: 133, episode reward: -48.313, mean reward: -0.568 [-100.000, 14.611], mean action: 1.612 [0.000, 3.000],  loss: 11.618548, mae: 25.610704, mean_q: 10.741539, mean_eps: 0.847936\n",
      "  25480/150000: episode: 276, duration: 0.799s, episode steps:  93, steps per second: 116, episode reward: -50.927, mean reward: -0.548 [-100.000, 11.352], mean action: 1.699 [0.000, 3.000],  loss: 10.324776, mae: 25.310371, mean_q: 11.351872, mean_eps: 0.847402\n",
      "  25576/150000: episode: 277, duration: 0.716s, episode steps:  96, steps per second: 134, episode reward: -66.427, mean reward: -0.692 [-100.000,  9.890], mean action: 1.562 [0.000, 3.000],  loss: 9.533588, mae: 25.165713, mean_q: 11.073875, mean_eps: 0.846835\n",
      "  25650/150000: episode: 278, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -93.802, mean reward: -1.268 [-100.000, 20.982], mean action: 1.689 [0.000, 3.000],  loss: 7.968168, mae: 25.104205, mean_q: 12.048421, mean_eps: 0.846325\n",
      "  25729/150000: episode: 279, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -152.966, mean reward: -1.936 [-100.000, 11.429], mean action: 1.418 [0.000, 3.000],  loss: 11.659296, mae: 25.805282, mean_q: 10.809963, mean_eps: 0.845866\n",
      "  25800/150000: episode: 280, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -98.570, mean reward: -1.388 [-100.000, 10.587], mean action: 1.648 [0.000, 3.000],  loss: 6.565073, mae: 25.053648, mean_q: 11.609679, mean_eps: 0.845416\n",
      "  25903/150000: episode: 281, duration: 0.647s, episode steps: 103, steps per second: 159, episode reward: -232.783, mean reward: -2.260 [-100.000, 46.167], mean action: 1.495 [0.000, 3.000],  loss: 9.581506, mae: 24.864179, mean_q: 10.995523, mean_eps: 0.844894\n",
      "  26010/150000: episode: 282, duration: 0.690s, episode steps: 107, steps per second: 155, episode reward: -321.639, mean reward: -3.006 [-100.000,  4.459], mean action: 1.495 [0.000, 3.000],  loss: 8.762706, mae: 25.054324, mean_q: 12.036370, mean_eps: 0.844264\n",
      "  26066/150000: episode: 283, duration: 0.387s, episode steps:  56, steps per second: 145, episode reward: -63.125, mean reward: -1.127 [-100.000, 20.825], mean action: 1.804 [0.000, 3.000],  loss: 8.133678, mae: 26.155935, mean_q: 12.106827, mean_eps: 0.843775\n",
      "  26183/150000: episode: 284, duration: 0.749s, episode steps: 117, steps per second: 156, episode reward: -26.752, mean reward: -0.229 [-100.000, 15.245], mean action: 1.564 [0.000, 3.000],  loss: 12.806310, mae: 25.663606, mean_q: 12.248301, mean_eps: 0.843256\n",
      "  26292/150000: episode: 285, duration: 0.688s, episode steps: 109, steps per second: 158, episode reward: -75.669, mean reward: -0.694 [-100.000, 16.452], mean action: 1.385 [0.000, 3.000],  loss: 8.862425, mae: 26.144959, mean_q: 10.961057, mean_eps: 0.842578\n",
      "  26356/150000: episode: 286, duration: 0.397s, episode steps:  64, steps per second: 161, episode reward: -63.198, mean reward: -0.987 [-100.000, 10.858], mean action: 1.609 [0.000, 3.000],  loss: 11.417369, mae: 25.550125, mean_q: 12.783933, mean_eps: 0.842059\n",
      "  26450/150000: episode: 287, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -79.812, mean reward: -0.849 [-100.000, 14.330], mean action: 1.404 [0.000, 3.000],  loss: 9.118011, mae: 25.900843, mean_q: 12.650569, mean_eps: 0.841585\n",
      "  26519/150000: episode: 288, duration: 0.440s, episode steps:  69, steps per second: 157, episode reward: -165.587, mean reward: -2.400 [-100.000,  4.474], mean action: 1.536 [0.000, 3.000],  loss: 8.280825, mae: 26.029982, mean_q: 12.487156, mean_eps: 0.841096\n",
      "  26651/150000: episode: 289, duration: 0.861s, episode steps: 132, steps per second: 153, episode reward: -160.294, mean reward: -1.214 [-100.000,  4.971], mean action: 1.500 [0.000, 3.000],  loss: 7.638061, mae: 25.755821, mean_q: 12.278675, mean_eps: 0.840493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  26722/150000: episode: 290, duration: 0.475s, episode steps:  71, steps per second: 150, episode reward: -79.581, mean reward: -1.121 [-100.000,  9.054], mean action: 1.592 [0.000, 3.000],  loss: 7.008504, mae: 25.848175, mean_q: 12.372808, mean_eps: 0.839884\n",
      "  26789/150000: episode: 291, duration: 0.437s, episode steps:  67, steps per second: 153, episode reward: -76.360, mean reward: -1.140 [-100.000,  7.244], mean action: 1.716 [0.000, 3.000],  loss: 7.169850, mae: 26.287747, mean_q: 11.961996, mean_eps: 0.839470\n",
      "  26867/150000: episode: 292, duration: 0.495s, episode steps:  78, steps per second: 158, episode reward: -107.178, mean reward: -1.374 [-100.000, 12.242], mean action: 1.769 [0.000, 3.000],  loss: 5.651642, mae: 25.713452, mean_q: 12.921466, mean_eps: 0.839035\n",
      "  26960/150000: episode: 293, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -171.800, mean reward: -1.847 [-100.000, 16.518], mean action: 1.355 [0.000, 3.000],  loss: 9.364415, mae: 25.853615, mean_q: 11.677040, mean_eps: 0.838522\n",
      "  27046/150000: episode: 294, duration: 0.573s, episode steps:  86, steps per second: 150, episode reward: -55.123, mean reward: -0.641 [-100.000, 17.248], mean action: 1.512 [0.000, 3.000],  loss: 10.840374, mae: 26.658566, mean_q: 11.924837, mean_eps: 0.837985\n",
      "  27152/150000: episode: 295, duration: 0.694s, episode steps: 106, steps per second: 153, episode reward: -90.325, mean reward: -0.852 [-100.000,  9.129], mean action: 1.736 [0.000, 3.000],  loss: 7.347052, mae: 26.800338, mean_q: 12.264972, mean_eps: 0.837409\n",
      "  27238/150000: episode: 296, duration: 0.547s, episode steps:  86, steps per second: 157, episode reward: -195.257, mean reward: -2.270 [-100.000,  5.986], mean action: 1.616 [0.000, 3.000],  loss: 8.565084, mae: 26.924950, mean_q: 12.922411, mean_eps: 0.836833\n",
      "  27339/150000: episode: 297, duration: 0.644s, episode steps: 101, steps per second: 157, episode reward: -123.443, mean reward: -1.222 [-100.000, 15.946], mean action: 1.545 [0.000, 3.000],  loss: 9.028960, mae: 26.967320, mean_q: 12.253362, mean_eps: 0.836272\n",
      "  27449/150000: episode: 298, duration: 0.728s, episode steps: 110, steps per second: 151, episode reward: -91.886, mean reward: -0.835 [-100.000,  5.802], mean action: 1.564 [0.000, 3.000],  loss: 8.355589, mae: 26.589205, mean_q: 12.963347, mean_eps: 0.835639\n",
      "  27520/150000: episode: 299, duration: 0.456s, episode steps:  71, steps per second: 156, episode reward: -66.968, mean reward: -0.943 [-100.000, 21.114], mean action: 1.817 [0.000, 3.000],  loss: 9.790359, mae: 27.131203, mean_q: 12.593679, mean_eps: 0.835096\n",
      "  27631/150000: episode: 300, duration: 0.693s, episode steps: 111, steps per second: 160, episode reward: -159.037, mean reward: -1.433 [-100.000,  3.480], mean action: 1.640 [0.000, 3.000],  loss: 5.279614, mae: 26.942927, mean_q: 12.293150, mean_eps: 0.834550\n",
      "  27703/150000: episode: 301, duration: 0.460s, episode steps:  72, steps per second: 157, episode reward: -45.343, mean reward: -0.630 [-100.000, 46.864], mean action: 1.639 [0.000, 3.000],  loss: 9.489001, mae: 26.928053, mean_q: 11.816812, mean_eps: 0.834001\n",
      "  27796/150000: episode: 302, duration: 0.615s, episode steps:  93, steps per second: 151, episode reward: -91.291, mean reward: -0.982 [-100.000, 11.127], mean action: 1.656 [0.000, 3.000],  loss: 7.116927, mae: 27.294129, mean_q: 12.145231, mean_eps: 0.833506\n",
      "  27876/150000: episode: 303, duration: 0.506s, episode steps:  80, steps per second: 158, episode reward: -123.342, mean reward: -1.542 [-100.000, 13.502], mean action: 1.475 [0.000, 3.000],  loss: 6.123927, mae: 27.266624, mean_q: 11.270436, mean_eps: 0.832987\n",
      "  27948/150000: episode: 304, duration: 0.456s, episode steps:  72, steps per second: 158, episode reward: -48.981, mean reward: -0.680 [-100.000, 50.644], mean action: 1.375 [0.000, 3.000],  loss: 5.053753, mae: 26.909141, mean_q: 11.186096, mean_eps: 0.832531\n",
      "  28027/150000: episode: 305, duration: 0.504s, episode steps:  79, steps per second: 157, episode reward: -63.157, mean reward: -0.799 [-100.000, 12.394], mean action: 1.658 [0.000, 3.000],  loss: 17.248409, mae: 26.934537, mean_q: 13.331948, mean_eps: 0.832078\n",
      "  28151/150000: episode: 306, duration: 0.834s, episode steps: 124, steps per second: 149, episode reward: -75.740, mean reward: -0.611 [-100.000, 18.299], mean action: 1.589 [0.000, 3.000],  loss: 10.543964, mae: 27.817158, mean_q: 13.521070, mean_eps: 0.831469\n",
      "  28283/150000: episode: 307, duration: 0.830s, episode steps: 132, steps per second: 159, episode reward: -233.694, mean reward: -1.770 [-100.000, 133.065], mean action: 1.576 [0.000, 3.000],  loss: 10.455588, mae: 27.255021, mean_q: 13.558041, mean_eps: 0.830701\n",
      "  28366/150000: episode: 308, duration: 0.519s, episode steps:  83, steps per second: 160, episode reward: -80.665, mean reward: -0.972 [-100.000, 54.337], mean action: 1.590 [0.000, 3.000],  loss: 10.808168, mae: 27.771212, mean_q: 13.386609, mean_eps: 0.830056\n",
      "  28467/150000: episode: 309, duration: 0.677s, episode steps: 101, steps per second: 149, episode reward: -141.068, mean reward: -1.397 [-100.000,  5.165], mean action: 1.376 [0.000, 3.000],  loss: 10.555371, mae: 27.547221, mean_q: 13.393620, mean_eps: 0.829504\n",
      "  28538/150000: episode: 310, duration: 0.444s, episode steps:  71, steps per second: 160, episode reward: -78.470, mean reward: -1.105 [-100.000,  9.130], mean action: 1.577 [0.000, 3.000],  loss: 9.075955, mae: 27.626343, mean_q: 12.790338, mean_eps: 0.828988\n",
      "  28642/150000: episode: 311, duration: 0.653s, episode steps: 104, steps per second: 159, episode reward: -119.730, mean reward: -1.151 [-100.000,  7.541], mean action: 1.423 [0.000, 3.000],  loss: 9.930584, mae: 27.566207, mean_q: 12.781009, mean_eps: 0.828463\n",
      "  28737/150000: episode: 312, duration: 0.626s, episode steps:  95, steps per second: 152, episode reward: -97.195, mean reward: -1.023 [-100.000, 12.521], mean action: 1.611 [0.000, 3.000],  loss: 11.222173, mae: 27.385597, mean_q: 13.342487, mean_eps: 0.827866\n",
      "  28806/150000: episode: 313, duration: 0.470s, episode steps:  69, steps per second: 147, episode reward: -68.962, mean reward: -0.999 [-100.000, 10.061], mean action: 1.536 [0.000, 3.000],  loss: 12.662858, mae: 27.580688, mean_q: 12.380700, mean_eps: 0.827374\n",
      "  28872/150000: episode: 314, duration: 0.426s, episode steps:  66, steps per second: 155, episode reward: -89.659, mean reward: -1.358 [-100.000,  6.881], mean action: 1.652 [0.000, 3.000],  loss: 14.333322, mae: 27.419022, mean_q: 13.228741, mean_eps: 0.826969\n",
      "  28941/150000: episode: 315, duration: 0.448s, episode steps:  69, steps per second: 154, episode reward: -157.241, mean reward: -2.279 [-100.000, 11.156], mean action: 1.478 [0.000, 3.000],  loss: 15.384858, mae: 27.357744, mean_q: 13.012692, mean_eps: 0.826564\n",
      "  29013/150000: episode: 316, duration: 0.471s, episode steps:  72, steps per second: 153, episode reward: -101.689, mean reward: -1.412 [-100.000,  6.808], mean action: 1.542 [0.000, 3.000],  loss: 7.634714, mae: 27.384709, mean_q: 13.518932, mean_eps: 0.826141\n",
      "  29083/150000: episode: 317, duration: 0.471s, episode steps:  70, steps per second: 149, episode reward: -43.140, mean reward: -0.616 [-100.000,  7.638], mean action: 1.586 [0.000, 3.000],  loss: 9.676105, mae: 27.883937, mean_q: 12.658164, mean_eps: 0.825715\n",
      "  29145/150000: episode: 318, duration: 0.404s, episode steps:  62, steps per second: 154, episode reward: -80.936, mean reward: -1.305 [-100.000, 17.517], mean action: 1.710 [0.000, 3.000],  loss: 10.012495, mae: 28.010193, mean_q: 14.183500, mean_eps: 0.825319\n",
      "  29226/150000: episode: 319, duration: 0.512s, episode steps:  81, steps per second: 158, episode reward: -90.539, mean reward: -1.118 [-100.000,  7.082], mean action: 1.556 [0.000, 3.000],  loss: 9.371987, mae: 27.721300, mean_q: 14.192125, mean_eps: 0.824890\n",
      "  29346/150000: episode: 320, duration: 0.757s, episode steps: 120, steps per second: 159, episode reward: -82.922, mean reward: -0.691 [-100.000,  9.955], mean action: 1.492 [0.000, 3.000],  loss: 8.831904, mae: 28.105896, mean_q: 13.845156, mean_eps: 0.824287\n",
      "  29463/150000: episode: 321, duration: 0.786s, episode steps: 117, steps per second: 149, episode reward: -53.578, mean reward: -0.458 [-100.000, 20.822], mean action: 1.632 [0.000, 3.000],  loss: 13.331882, mae: 28.235000, mean_q: 14.265326, mean_eps: 0.823576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  29546/150000: episode: 322, duration: 0.524s, episode steps:  83, steps per second: 158, episode reward: -75.365, mean reward: -0.908 [-100.000, 12.023], mean action: 1.518 [0.000, 3.000],  loss: 7.352708, mae: 27.843152, mean_q: 13.862409, mean_eps: 0.822976\n",
      "  29630/150000: episode: 323, duration: 0.529s, episode steps:  84, steps per second: 159, episode reward: -64.072, mean reward: -0.763 [-100.000,  6.640], mean action: 1.548 [0.000, 3.000],  loss: 12.115000, mae: 27.494587, mean_q: 13.881583, mean_eps: 0.822475\n",
      "  29722/150000: episode: 324, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -56.758, mean reward: -0.617 [-100.000, 13.596], mean action: 1.413 [0.000, 3.000],  loss: 9.325412, mae: 28.197578, mean_q: 14.035832, mean_eps: 0.821947\n",
      "  29813/150000: episode: 325, duration: 0.603s, episode steps:  91, steps per second: 151, episode reward: -47.530, mean reward: -0.522 [-100.000, 27.732], mean action: 1.593 [0.000, 3.000],  loss: 13.961379, mae: 28.272974, mean_q: 14.891711, mean_eps: 0.821398\n",
      "  29919/150000: episode: 326, duration: 0.671s, episode steps: 106, steps per second: 158, episode reward: -85.145, mean reward: -0.803 [-100.000,  5.694], mean action: 1.500 [0.000, 3.000],  loss: 7.908998, mae: 27.971016, mean_q: 13.512732, mean_eps: 0.820807\n",
      "  29992/150000: episode: 327, duration: 0.463s, episode steps:  73, steps per second: 158, episode reward: -27.210, mean reward: -0.373 [-100.000, 13.067], mean action: 1.507 [0.000, 3.000],  loss: 11.949754, mae: 27.910984, mean_q: 15.024967, mean_eps: 0.820270\n",
      "  30079/150000: episode: 328, duration: 0.595s, episode steps:  87, steps per second: 146, episode reward: -108.251, mean reward: -1.244 [-100.000, 12.419], mean action: 1.598 [0.000, 3.000],  loss: 10.424096, mae: 28.634374, mean_q: 15.144071, mean_eps: 0.819790\n",
      "  30184/150000: episode: 329, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward: -113.034, mean reward: -1.077 [-100.000, 10.970], mean action: 1.524 [0.000, 3.000],  loss: 8.105559, mae: 28.666679, mean_q: 14.542409, mean_eps: 0.819214\n",
      "  30276/150000: episode: 330, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -122.754, mean reward: -1.334 [-100.000,  7.441], mean action: 1.446 [0.000, 3.000],  loss: 10.419827, mae: 28.791707, mean_q: 14.370485, mean_eps: 0.818623\n",
      "  30355/150000: episode: 331, duration: 0.515s, episode steps:  79, steps per second: 153, episode reward: -60.999, mean reward: -0.772 [-100.000, 12.825], mean action: 1.506 [0.000, 3.000],  loss: 7.460090, mae: 28.681606, mean_q: 14.452848, mean_eps: 0.818110\n",
      "  30435/150000: episode: 332, duration: 0.554s, episode steps:  80, steps per second: 144, episode reward: -82.559, mean reward: -1.032 [-100.000,  6.387], mean action: 1.400 [0.000, 3.000],  loss: 8.808724, mae: 29.151039, mean_q: 14.734888, mean_eps: 0.817633\n",
      "  30506/150000: episode: 333, duration: 0.469s, episode steps:  71, steps per second: 151, episode reward: -78.705, mean reward: -1.109 [-100.000, 18.750], mean action: 1.465 [0.000, 3.000],  loss: 13.167106, mae: 29.143324, mean_q: 15.859782, mean_eps: 0.817180\n",
      "  30610/150000: episode: 334, duration: 0.674s, episode steps: 104, steps per second: 154, episode reward: -97.343, mean reward: -0.936 [-100.000, 12.069], mean action: 1.548 [0.000, 3.000],  loss: 11.541815, mae: 28.764599, mean_q: 14.796213, mean_eps: 0.816655\n",
      "  30698/150000: episode: 335, duration: 0.587s, episode steps:  88, steps per second: 150, episode reward: -63.484, mean reward: -0.721 [-100.000, 24.553], mean action: 1.352 [0.000, 3.000],  loss: 9.326261, mae: 28.863434, mean_q: 13.699096, mean_eps: 0.816079\n",
      "  30821/150000: episode: 336, duration: 0.834s, episode steps: 123, steps per second: 148, episode reward: -165.289, mean reward: -1.344 [-100.000,  3.004], mean action: 1.528 [0.000, 3.000],  loss: 6.896164, mae: 28.470297, mean_q: 15.225984, mean_eps: 0.815446\n",
      "  30922/150000: episode: 337, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: -70.156, mean reward: -0.695 [-100.000, 20.284], mean action: 1.495 [0.000, 3.000],  loss: 10.809849, mae: 29.000053, mean_q: 14.255219, mean_eps: 0.814774\n",
      "  31024/150000: episode: 338, duration: 0.663s, episode steps: 102, steps per second: 154, episode reward: -138.708, mean reward: -1.360 [-100.000, 13.539], mean action: 1.480 [0.000, 3.000],  loss: 10.281146, mae: 28.752783, mean_q: 15.246112, mean_eps: 0.814165\n",
      "  31096/150000: episode: 339, duration: 0.479s, episode steps:  72, steps per second: 150, episode reward: -15.285, mean reward: -0.212 [-100.000, 16.784], mean action: 1.653 [0.000, 3.000],  loss: 11.271337, mae: 28.706611, mean_q: 15.052944, mean_eps: 0.813643\n",
      "  31202/150000: episode: 340, duration: 0.678s, episode steps: 106, steps per second: 156, episode reward: -227.568, mean reward: -2.147 [-100.000,  1.043], mean action: 1.557 [0.000, 3.000],  loss: 9.057155, mae: 28.622036, mean_q: 14.864075, mean_eps: 0.813109\n",
      "  31279/150000: episode: 341, duration: 0.498s, episode steps:  77, steps per second: 155, episode reward: -123.675, mean reward: -1.606 [-100.000, 10.723], mean action: 1.610 [0.000, 3.000],  loss: 9.911514, mae: 28.762571, mean_q: 13.717631, mean_eps: 0.812560\n",
      "  31390/150000: episode: 342, duration: 0.751s, episode steps: 111, steps per second: 148, episode reward: -170.410, mean reward: -1.535 [-100.000,  4.106], mean action: 1.541 [0.000, 3.000],  loss: 11.465237, mae: 29.023916, mean_q: 13.625587, mean_eps: 0.811996\n",
      "  31508/150000: episode: 343, duration: 0.766s, episode steps: 118, steps per second: 154, episode reward: -61.913, mean reward: -0.525 [-100.000, 17.434], mean action: 1.712 [0.000, 3.000],  loss: 10.366988, mae: 29.125084, mean_q: 14.363934, mean_eps: 0.811309\n",
      "  31631/150000: episode: 344, duration: 0.812s, episode steps: 123, steps per second: 152, episode reward: -130.981, mean reward: -1.065 [-100.000,  3.880], mean action: 1.504 [0.000, 3.000],  loss: 10.745216, mae: 29.029111, mean_q: 14.191909, mean_eps: 0.810586\n",
      "  31750/150000: episode: 345, duration: 0.791s, episode steps: 119, steps per second: 150, episode reward: -94.972, mean reward: -0.798 [-100.000,  6.047], mean action: 1.345 [0.000, 3.000],  loss: 12.368853, mae: 28.654581, mean_q: 15.054213, mean_eps: 0.809860\n",
      "  31835/150000: episode: 346, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -74.187, mean reward: -0.873 [-100.000,  7.342], mean action: 1.706 [0.000, 3.000],  loss: 11.272364, mae: 28.595484, mean_q: 15.131096, mean_eps: 0.809248\n",
      "  31908/150000: episode: 347, duration: 0.472s, episode steps:  73, steps per second: 155, episode reward: -50.894, mean reward: -0.697 [-100.000,  7.536], mean action: 1.699 [0.000, 3.000],  loss: 7.105833, mae: 29.057659, mean_q: 15.594260, mean_eps: 0.808774\n",
      "  31989/150000: episode: 348, duration: 0.534s, episode steps:  81, steps per second: 152, episode reward: -97.388, mean reward: -1.202 [-100.000,  6.455], mean action: 1.654 [0.000, 3.000],  loss: 8.809448, mae: 28.736169, mean_q: 14.673190, mean_eps: 0.808312\n",
      "  32074/150000: episode: 349, duration: 0.581s, episode steps:  85, steps per second: 146, episode reward: -81.365, mean reward: -0.957 [-100.000, 14.760], mean action: 1.624 [0.000, 3.000],  loss: 14.182684, mae: 29.073108, mean_q: 15.552888, mean_eps: 0.807814\n",
      "  32184/150000: episode: 350, duration: 0.735s, episode steps: 110, steps per second: 150, episode reward: -121.228, mean reward: -1.102 [-100.000,  8.801], mean action: 1.527 [0.000, 3.000],  loss: 11.669397, mae: 29.195710, mean_q: 15.646055, mean_eps: 0.807229\n",
      "  32278/150000: episode: 351, duration: 0.594s, episode steps:  94, steps per second: 158, episode reward: -90.960, mean reward: -0.968 [-100.000,  9.983], mean action: 1.511 [0.000, 3.000],  loss: 11.085165, mae: 30.117556, mean_q: 14.627182, mean_eps: 0.806617\n",
      "  32353/150000: episode: 352, duration: 0.517s, episode steps:  75, steps per second: 145, episode reward: -148.360, mean reward: -1.978 [-100.000,  7.935], mean action: 1.760 [0.000, 3.000],  loss: 11.023617, mae: 29.586235, mean_q: 15.615245, mean_eps: 0.806110\n",
      "  32450/150000: episode: 353, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -116.106, mean reward: -1.197 [-100.000, 10.340], mean action: 1.443 [0.000, 3.000],  loss: 7.620746, mae: 29.800356, mean_q: 15.966608, mean_eps: 0.805594\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  32548/150000: episode: 354, duration: 0.629s, episode steps:  98, steps per second: 156, episode reward: -44.067, mean reward: -0.450 [-100.000, 12.967], mean action: 1.745 [0.000, 3.000],  loss: 7.460371, mae: 29.792957, mean_q: 15.851097, mean_eps: 0.805009\n",
      "  32637/150000: episode: 355, duration: 0.568s, episode steps:  89, steps per second: 157, episode reward: -130.689, mean reward: -1.468 [-100.000,  5.566], mean action: 1.393 [0.000, 3.000],  loss: 8.809092, mae: 29.558242, mean_q: 16.008864, mean_eps: 0.804448\n",
      "  32738/150000: episode: 356, duration: 0.696s, episode steps: 101, steps per second: 145, episode reward: -75.536, mean reward: -0.748 [-100.000, 14.352], mean action: 1.653 [0.000, 3.000],  loss: 10.884951, mae: 29.421460, mean_q: 16.351891, mean_eps: 0.803878\n",
      "  32810/150000: episode: 357, duration: 0.467s, episode steps:  72, steps per second: 154, episode reward:  3.460, mean reward:  0.048 [-100.000, 21.374], mean action: 1.625 [0.000, 3.000],  loss: 6.653661, mae: 29.533008, mean_q: 15.114765, mean_eps: 0.803359\n",
      "  32911/150000: episode: 358, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: -98.008, mean reward: -0.970 [-100.000,  8.565], mean action: 1.515 [0.000, 3.000],  loss: 8.425837, mae: 29.641410, mean_q: 16.074552, mean_eps: 0.802840\n",
      "  33042/150000: episode: 359, duration: 0.897s, episode steps: 131, steps per second: 146, episode reward: -125.746, mean reward: -0.960 [-100.000, 20.251], mean action: 1.626 [0.000, 3.000],  loss: 9.111683, mae: 29.787874, mean_q: 16.797418, mean_eps: 0.802144\n",
      "  33164/150000: episode: 360, duration: 0.778s, episode steps: 122, steps per second: 157, episode reward: -162.090, mean reward: -1.329 [-100.000,  4.390], mean action: 1.697 [0.000, 3.000],  loss: 8.740901, mae: 29.760982, mean_q: 17.920476, mean_eps: 0.801385\n",
      "  33244/150000: episode: 361, duration: 0.498s, episode steps:  80, steps per second: 161, episode reward: -92.315, mean reward: -1.154 [-100.000, 10.376], mean action: 1.475 [0.000, 3.000],  loss: 8.203890, mae: 29.709443, mean_q: 16.756325, mean_eps: 0.800779\n",
      "  33372/150000: episode: 362, duration: 0.878s, episode steps: 128, steps per second: 146, episode reward: -71.533, mean reward: -0.559 [-100.000,  6.489], mean action: 1.625 [0.000, 3.000],  loss: 7.598985, mae: 30.279523, mean_q: 17.518711, mean_eps: 0.800155\n",
      "  33488/150000: episode: 363, duration: 0.760s, episode steps: 116, steps per second: 153, episode reward: -83.245, mean reward: -0.718 [-100.000, 12.168], mean action: 1.664 [0.000, 3.000],  loss: 5.867672, mae: 29.979723, mean_q: 17.283605, mean_eps: 0.799423\n",
      "  33587/150000: episode: 364, duration: 0.630s, episode steps:  99, steps per second: 157, episode reward: -135.552, mean reward: -1.369 [-100.000, 16.168], mean action: 1.515 [0.000, 3.000],  loss: 7.869435, mae: 30.165568, mean_q: 16.982833, mean_eps: 0.798778\n",
      "  33686/150000: episode: 365, duration: 0.676s, episode steps:  99, steps per second: 146, episode reward: -112.069, mean reward: -1.132 [-100.000,  6.280], mean action: 1.323 [0.000, 3.000],  loss: 7.389785, mae: 29.752965, mean_q: 17.780193, mean_eps: 0.798184\n",
      "  33804/150000: episode: 366, duration: 0.790s, episode steps: 118, steps per second: 149, episode reward: -92.290, mean reward: -0.782 [-100.000,  8.311], mean action: 1.542 [0.000, 3.000],  loss: 7.636603, mae: 29.880243, mean_q: 16.460371, mean_eps: 0.797533\n",
      "  33859/150000: episode: 367, duration: 0.357s, episode steps:  55, steps per second: 154, episode reward: -83.779, mean reward: -1.523 [-100.000,  6.957], mean action: 1.618 [0.000, 3.000],  loss: 8.685911, mae: 29.301985, mean_q: 17.131013, mean_eps: 0.797014\n",
      "  33953/150000: episode: 368, duration: 0.607s, episode steps:  94, steps per second: 155, episode reward: -73.101, mean reward: -0.778 [-100.000, 15.621], mean action: 1.521 [0.000, 3.000],  loss: 8.729969, mae: 29.963798, mean_q: 17.553120, mean_eps: 0.796567\n",
      "  34085/150000: episode: 369, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -63.198, mean reward: -0.479 [-100.000,  7.447], mean action: 1.553 [0.000, 3.000],  loss: 7.585697, mae: 29.947959, mean_q: 17.271172, mean_eps: 0.795889\n",
      "  34154/150000: episode: 370, duration: 0.542s, episode steps:  69, steps per second: 127, episode reward: -99.775, mean reward: -1.446 [-100.000,  6.240], mean action: 1.855 [0.000, 3.000],  loss: 8.773002, mae: 30.426358, mean_q: 17.747586, mean_eps: 0.795286\n",
      "  34273/150000: episode: 371, duration: 0.911s, episode steps: 119, steps per second: 131, episode reward: -72.550, mean reward: -0.610 [-100.000, 17.170], mean action: 1.613 [0.000, 3.000],  loss: 7.840276, mae: 30.239765, mean_q: 16.266555, mean_eps: 0.794722\n",
      "  34388/150000: episode: 372, duration: 0.824s, episode steps: 115, steps per second: 139, episode reward: -88.576, mean reward: -0.770 [-100.000,  6.216], mean action: 1.409 [0.000, 3.000],  loss: 6.650422, mae: 29.977321, mean_q: 17.075735, mean_eps: 0.794020\n",
      "  34505/150000: episode: 373, duration: 0.805s, episode steps: 117, steps per second: 145, episode reward: -149.342, mean reward: -1.276 [-100.000,  3.784], mean action: 1.419 [0.000, 3.000],  loss: 7.290934, mae: 29.993946, mean_q: 17.421017, mean_eps: 0.793324\n",
      "  34626/150000: episode: 374, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward: -53.149, mean reward: -0.439 [-100.000,  7.513], mean action: 1.496 [0.000, 3.000],  loss: 10.476968, mae: 30.077077, mean_q: 16.750405, mean_eps: 0.792610\n",
      "  34704/150000: episode: 375, duration: 0.562s, episode steps:  78, steps per second: 139, episode reward: -68.950, mean reward: -0.884 [-100.000, 14.808], mean action: 1.551 [0.000, 3.000],  loss: 9.371532, mae: 30.107026, mean_q: 16.608814, mean_eps: 0.792013\n",
      "  34794/150000: episode: 376, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -107.387, mean reward: -1.193 [-100.000,  5.282], mean action: 1.756 [0.000, 3.000],  loss: 7.715158, mae: 30.344710, mean_q: 16.874498, mean_eps: 0.791509\n",
      "  34872/150000: episode: 377, duration: 0.511s, episode steps:  78, steps per second: 153, episode reward: -97.335, mean reward: -1.248 [-100.000,  6.778], mean action: 1.590 [0.000, 3.000],  loss: 8.459799, mae: 30.133276, mean_q: 18.010357, mean_eps: 0.791005\n",
      "  34972/150000: episode: 378, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: -77.862, mean reward: -0.779 [-100.000, 11.630], mean action: 1.680 [0.000, 3.000],  loss: 7.663939, mae: 29.938790, mean_q: 17.384289, mean_eps: 0.790471\n",
      "  35103/150000: episode: 379, duration: 0.890s, episode steps: 131, steps per second: 147, episode reward: -86.293, mean reward: -0.659 [-100.000,  4.580], mean action: 1.496 [0.000, 3.000],  loss: 6.928013, mae: 30.840786, mean_q: 17.561084, mean_eps: 0.789778\n",
      "  35173/150000: episode: 380, duration: 0.448s, episode steps:  70, steps per second: 156, episode reward: -75.198, mean reward: -1.074 [-100.000, 22.002], mean action: 1.457 [0.000, 3.000],  loss: 6.529119, mae: 30.727700, mean_q: 17.801434, mean_eps: 0.789175\n",
      "  35243/150000: episode: 381, duration: 0.482s, episode steps:  70, steps per second: 145, episode reward: -65.855, mean reward: -0.941 [-100.000,  6.498], mean action: 1.586 [0.000, 3.000],  loss: 6.253640, mae: 30.442142, mean_q: 18.932989, mean_eps: 0.788755\n",
      "  35370/150000: episode: 382, duration: 0.824s, episode steps: 127, steps per second: 154, episode reward: -91.052, mean reward: -0.717 [-100.000, 17.044], mean action: 1.386 [0.000, 3.000],  loss: 6.751417, mae: 30.657831, mean_q: 17.631533, mean_eps: 0.788164\n",
      "  35443/150000: episode: 383, duration: 0.470s, episode steps:  73, steps per second: 155, episode reward: -86.824, mean reward: -1.189 [-100.000, 10.578], mean action: 1.493 [0.000, 3.000],  loss: 6.365860, mae: 30.991905, mean_q: 18.239846, mean_eps: 0.787564\n",
      "  35512/150000: episode: 384, duration: 0.443s, episode steps:  69, steps per second: 156, episode reward: -55.490, mean reward: -0.804 [-100.000, 12.704], mean action: 1.638 [0.000, 3.000],  loss: 6.500273, mae: 30.854160, mean_q: 16.738288, mean_eps: 0.787138\n",
      "  35610/150000: episode: 385, duration: 0.663s, episode steps:  98, steps per second: 148, episode reward: -70.255, mean reward: -0.717 [-100.000, 11.406], mean action: 1.592 [0.000, 3.000],  loss: 6.785493, mae: 31.152460, mean_q: 18.249561, mean_eps: 0.786637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  35687/150000: episode: 386, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -77.275, mean reward: -1.004 [-100.000, 17.048], mean action: 1.766 [0.000, 3.000],  loss: 6.901076, mae: 30.767627, mean_q: 18.802491, mean_eps: 0.786112\n",
      "  35795/150000: episode: 387, duration: 0.737s, episode steps: 108, steps per second: 146, episode reward: -112.353, mean reward: -1.040 [-100.000,  7.872], mean action: 1.509 [0.000, 3.000],  loss: 8.161109, mae: 31.306365, mean_q: 17.303164, mean_eps: 0.785557\n",
      "  35879/150000: episode: 388, duration: 0.575s, episode steps:  84, steps per second: 146, episode reward: -113.543, mean reward: -1.352 [-100.000, 12.096], mean action: 1.667 [0.000, 3.000],  loss: 7.983832, mae: 30.819564, mean_q: 18.267524, mean_eps: 0.784981\n",
      "  35941/150000: episode: 389, duration: 0.418s, episode steps:  62, steps per second: 148, episode reward: -48.951, mean reward: -0.790 [-100.000, 16.065], mean action: 1.597 [0.000, 3.000],  loss: 5.921978, mae: 31.285604, mean_q: 17.841787, mean_eps: 0.784543\n",
      "  36008/150000: episode: 390, duration: 0.439s, episode steps:  67, steps per second: 153, episode reward: -78.701, mean reward: -1.175 [-100.000, 10.628], mean action: 1.866 [0.000, 3.000],  loss: 8.252138, mae: 31.004162, mean_q: 18.350079, mean_eps: 0.784156\n",
      "  36091/150000: episode: 391, duration: 0.550s, episode steps:  83, steps per second: 151, episode reward: -105.578, mean reward: -1.272 [-100.000,  7.910], mean action: 1.518 [0.000, 3.000],  loss: 7.909808, mae: 31.858931, mean_q: 19.179546, mean_eps: 0.783706\n",
      "  36195/150000: episode: 392, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -113.134, mean reward: -1.088 [-100.000, 20.669], mean action: 1.356 [0.000, 3.000],  loss: 9.656413, mae: 32.014560, mean_q: 20.144271, mean_eps: 0.783145\n",
      "  36280/150000: episode: 393, duration: 0.565s, episode steps:  85, steps per second: 151, episode reward: -132.523, mean reward: -1.559 [-100.000,  9.537], mean action: 1.400 [0.000, 3.000],  loss: 6.694509, mae: 31.970315, mean_q: 19.139614, mean_eps: 0.782578\n",
      "  36340/150000: episode: 394, duration: 0.387s, episode steps:  60, steps per second: 155, episode reward: -79.873, mean reward: -1.331 [-100.000, 10.312], mean action: 1.617 [0.000, 3.000],  loss: 5.291765, mae: 31.925253, mean_q: 19.791206, mean_eps: 0.782143\n",
      "  36428/150000: episode: 395, duration: 0.587s, episode steps:  88, steps per second: 150, episode reward: -38.206, mean reward: -0.434 [-100.000, 23.744], mean action: 1.614 [0.000, 3.000],  loss: 5.086478, mae: 31.609363, mean_q: 18.050821, mean_eps: 0.781699\n",
      "  36530/150000: episode: 396, duration: 0.705s, episode steps: 102, steps per second: 145, episode reward: -93.192, mean reward: -0.914 [-100.000, 10.682], mean action: 1.480 [0.000, 3.000],  loss: 9.501787, mae: 32.085555, mean_q: 18.018271, mean_eps: 0.781129\n",
      "  36615/150000: episode: 397, duration: 0.561s, episode steps:  85, steps per second: 152, episode reward: -110.620, mean reward: -1.301 [-100.000,  7.693], mean action: 1.471 [0.000, 3.000],  loss: 4.445803, mae: 31.932347, mean_q: 17.559721, mean_eps: 0.780568\n",
      "  36734/150000: episode: 398, duration: 0.766s, episode steps: 119, steps per second: 155, episode reward: -88.501, mean reward: -0.744 [-100.000, 11.278], mean action: 1.655 [0.000, 3.000],  loss: 7.062330, mae: 32.153701, mean_q: 18.218742, mean_eps: 0.779956\n",
      "  36830/150000: episode: 399, duration: 0.640s, episode steps:  96, steps per second: 150, episode reward: -118.821, mean reward: -1.238 [-100.000, 11.440], mean action: 1.354 [0.000, 3.000],  loss: 6.048376, mae: 31.794065, mean_q: 18.904256, mean_eps: 0.779311\n",
      "  36942/150000: episode: 400, duration: 0.739s, episode steps: 112, steps per second: 152, episode reward: -133.525, mean reward: -1.192 [-100.000,  5.493], mean action: 1.473 [0.000, 3.000],  loss: 5.968680, mae: 31.740067, mean_q: 18.443730, mean_eps: 0.778687\n",
      "  37034/150000: episode: 401, duration: 0.586s, episode steps:  92, steps per second: 157, episode reward: -113.386, mean reward: -1.232 [-100.000,  5.040], mean action: 1.783 [0.000, 3.000],  loss: 5.766474, mae: 32.081151, mean_q: 18.968005, mean_eps: 0.778075\n",
      "  37112/150000: episode: 402, duration: 0.506s, episode steps:  78, steps per second: 154, episode reward: -31.444, mean reward: -0.403 [-100.000, 16.472], mean action: 1.449 [0.000, 3.000],  loss: 7.118307, mae: 31.834173, mean_q: 18.540733, mean_eps: 0.777565\n",
      "  37187/150000: episode: 403, duration: 0.532s, episode steps:  75, steps per second: 141, episode reward: -35.683, mean reward: -0.476 [-100.000, 10.443], mean action: 1.520 [0.000, 3.000],  loss: 7.277517, mae: 32.081957, mean_q: 19.492869, mean_eps: 0.777106\n",
      "  37287/150000: episode: 404, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -94.598, mean reward: -0.946 [-100.000,  6.539], mean action: 1.590 [0.000, 3.000],  loss: 7.502652, mae: 32.001813, mean_q: 19.008834, mean_eps: 0.776581\n",
      "  37380/150000: episode: 405, duration: 0.597s, episode steps:  93, steps per second: 156, episode reward: -305.491, mean reward: -3.285 [-100.000,  0.643], mean action: 1.602 [0.000, 3.000],  loss: 4.701860, mae: 31.772373, mean_q: 19.444060, mean_eps: 0.776002\n",
      "  37494/150000: episode: 406, duration: 0.760s, episode steps: 114, steps per second: 150, episode reward: -94.555, mean reward: -0.829 [-100.000, 12.167], mean action: 1.351 [0.000, 3.000],  loss: 6.733562, mae: 31.611419, mean_q: 19.007639, mean_eps: 0.775381\n",
      "  37567/150000: episode: 407, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -40.763, mean reward: -0.558 [-100.000, 20.998], mean action: 1.397 [0.000, 3.000],  loss: 9.790390, mae: 32.805953, mean_q: 18.396304, mean_eps: 0.774820\n",
      "  37665/150000: episode: 408, duration: 0.743s, episode steps:  98, steps per second: 132, episode reward: -58.873, mean reward: -0.601 [-100.000, 13.595], mean action: 1.612 [0.000, 3.000],  loss: 10.816384, mae: 32.243324, mean_q: 19.080495, mean_eps: 0.774307\n",
      "  37769/150000: episode: 409, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -180.958, mean reward: -1.740 [-100.000,  6.736], mean action: 1.433 [0.000, 3.000],  loss: 10.806942, mae: 32.194654, mean_q: 19.016313, mean_eps: 0.773701\n",
      "  37841/150000: episode: 410, duration: 0.494s, episode steps:  72, steps per second: 146, episode reward: -56.320, mean reward: -0.782 [-100.000, 31.958], mean action: 1.347 [0.000, 3.000],  loss: 7.107541, mae: 32.131677, mean_q: 19.215151, mean_eps: 0.773173\n",
      "  37949/150000: episode: 411, duration: 0.708s, episode steps: 108, steps per second: 153, episode reward: -113.915, mean reward: -1.055 [-100.000,  8.948], mean action: 1.657 [0.000, 3.000],  loss: 7.710605, mae: 32.020217, mean_q: 18.804976, mean_eps: 0.772633\n",
      "  38009/150000: episode: 412, duration: 0.387s, episode steps:  60, steps per second: 155, episode reward: -112.399, mean reward: -1.873 [-100.000, 28.292], mean action: 1.317 [0.000, 3.000],  loss: 6.882494, mae: 32.151157, mean_q: 19.433284, mean_eps: 0.772129\n",
      "  38133/150000: episode: 413, duration: 0.853s, episode steps: 124, steps per second: 145, episode reward: -36.090, mean reward: -0.291 [-100.000, 83.478], mean action: 1.629 [0.000, 3.000],  loss: 6.087614, mae: 32.075532, mean_q: 20.934148, mean_eps: 0.771577\n",
      "  38217/150000: episode: 414, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -76.272, mean reward: -0.908 [-100.000, 16.159], mean action: 1.738 [0.000, 3.000],  loss: 5.278961, mae: 32.522115, mean_q: 19.353142, mean_eps: 0.770953\n",
      "  38301/150000: episode: 415, duration: 0.544s, episode steps:  84, steps per second: 154, episode reward: -176.752, mean reward: -2.104 [-100.000, 15.146], mean action: 1.524 [0.000, 3.000],  loss: 9.681424, mae: 32.059745, mean_q: 20.209554, mean_eps: 0.770449\n",
      "  38407/150000: episode: 416, duration: 0.681s, episode steps: 106, steps per second: 156, episode reward: -63.575, mean reward: -0.600 [-100.000, 12.084], mean action: 1.557 [0.000, 3.000],  loss: 6.339162, mae: 32.327040, mean_q: 20.587382, mean_eps: 0.769879\n",
      "  38505/150000: episode: 417, duration: 0.678s, episode steps:  98, steps per second: 145, episode reward: -131.475, mean reward: -1.342 [-100.000,  5.511], mean action: 1.296 [0.000, 3.000],  loss: 11.684878, mae: 32.408845, mean_q: 19.986259, mean_eps: 0.769267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  38634/150000: episode: 418, duration: 0.842s, episode steps: 129, steps per second: 153, episode reward: -393.334, mean reward: -3.049 [-100.000, 45.490], mean action: 1.612 [0.000, 3.000],  loss: 7.206176, mae: 32.583365, mean_q: 19.111198, mean_eps: 0.768586\n",
      "  38716/150000: episode: 419, duration: 0.538s, episode steps:  82, steps per second: 152, episode reward: -100.596, mean reward: -1.227 [-100.000, 35.003], mean action: 1.366 [0.000, 3.000],  loss: 5.882298, mae: 32.444584, mean_q: 19.951000, mean_eps: 0.767953\n",
      "  38836/150000: episode: 420, duration: 0.832s, episode steps: 120, steps per second: 144, episode reward: -125.941, mean reward: -1.050 [-100.000, 26.796], mean action: 1.667 [0.000, 3.000],  loss: 8.523036, mae: 32.584182, mean_q: 19.579815, mean_eps: 0.767347\n",
      "  38932/150000: episode: 421, duration: 0.624s, episode steps:  96, steps per second: 154, episode reward: -112.816, mean reward: -1.175 [-100.000,  5.067], mean action: 1.740 [0.000, 3.000],  loss: 7.256391, mae: 32.723338, mean_q: 20.619795, mean_eps: 0.766699\n",
      "  39031/150000: episode: 422, duration: 0.663s, episode steps:  99, steps per second: 149, episode reward: -64.068, mean reward: -0.647 [-100.000, 13.849], mean action: 1.646 [0.000, 3.000],  loss: 5.589372, mae: 32.216735, mean_q: 19.146680, mean_eps: 0.766114\n",
      "  39119/150000: episode: 423, duration: 0.615s, episode steps:  88, steps per second: 143, episode reward: -79.073, mean reward: -0.899 [-100.000,  9.265], mean action: 1.568 [0.000, 3.000],  loss: 6.569020, mae: 32.073645, mean_q: 20.580177, mean_eps: 0.765553\n",
      "  39217/150000: episode: 424, duration: 0.646s, episode steps:  98, steps per second: 152, episode reward: -112.807, mean reward: -1.151 [-100.000,  6.126], mean action: 1.469 [0.000, 3.000],  loss: 11.821123, mae: 32.745690, mean_q: 21.010797, mean_eps: 0.764995\n",
      "  39304/150000: episode: 425, duration: 0.563s, episode steps:  87, steps per second: 154, episode reward: -78.482, mean reward: -0.902 [-100.000,  8.862], mean action: 1.310 [0.000, 3.000],  loss: 8.422361, mae: 32.648089, mean_q: 20.796829, mean_eps: 0.764440\n",
      "  39423/150000: episode: 426, duration: 0.817s, episode steps: 119, steps per second: 146, episode reward: -168.413, mean reward: -1.415 [-100.000,  8.314], mean action: 1.328 [0.000, 3.000],  loss: 9.565725, mae: 32.661990, mean_q: 20.984986, mean_eps: 0.763822\n",
      "  39534/150000: episode: 427, duration: 0.757s, episode steps: 111, steps per second: 147, episode reward: -65.417, mean reward: -0.589 [-100.000, 15.150], mean action: 1.775 [0.000, 3.000],  loss: 6.685282, mae: 32.611452, mean_q: 20.875988, mean_eps: 0.763132\n",
      "  39596/150000: episode: 428, duration: 0.408s, episode steps:  62, steps per second: 152, episode reward: -67.663, mean reward: -1.091 [-100.000,  4.977], mean action: 1.452 [0.000, 3.000],  loss: 7.434659, mae: 33.014838, mean_q: 20.363267, mean_eps: 0.762613\n",
      "  39704/150000: episode: 429, duration: 0.701s, episode steps: 108, steps per second: 154, episode reward: -71.674, mean reward: -0.664 [-100.000,  6.453], mean action: 1.435 [0.000, 3.000],  loss: 11.570972, mae: 32.741863, mean_q: 19.501608, mean_eps: 0.762103\n",
      "  39772/150000: episode: 430, duration: 0.512s, episode steps:  68, steps per second: 133, episode reward: -72.832, mean reward: -1.071 [-100.000, 11.205], mean action: 1.574 [0.000, 3.000],  loss: 10.316469, mae: 32.740824, mean_q: 20.360522, mean_eps: 0.761575\n",
      "  39866/150000: episode: 431, duration: 0.632s, episode steps:  94, steps per second: 149, episode reward: -82.757, mean reward: -0.880 [-100.000, 17.993], mean action: 1.660 [0.000, 3.000],  loss: 8.921808, mae: 32.717304, mean_q: 19.898667, mean_eps: 0.761089\n",
      "  39935/150000: episode: 432, duration: 0.450s, episode steps:  69, steps per second: 153, episode reward: -88.634, mean reward: -1.285 [-100.000,  6.808], mean action: 1.652 [0.000, 3.000],  loss: 8.892406, mae: 32.041185, mean_q: 19.946677, mean_eps: 0.760600\n",
      "  40051/150000: episode: 433, duration: 0.770s, episode steps: 116, steps per second: 151, episode reward: -51.555, mean reward: -0.444 [-100.000, 17.179], mean action: 1.543 [0.000, 3.000],  loss: 8.657865, mae: 32.528799, mean_q: 20.441977, mean_eps: 0.760045\n",
      "  40162/150000: episode: 434, duration: 0.752s, episode steps: 111, steps per second: 148, episode reward: -43.574, mean reward: -0.393 [-100.000, 12.917], mean action: 1.532 [0.000, 3.000],  loss: 10.094839, mae: 32.565030, mean_q: 19.720029, mean_eps: 0.759364\n",
      "  40271/150000: episode: 435, duration: 0.724s, episode steps: 109, steps per second: 151, episode reward: -91.758, mean reward: -0.842 [-100.000,  6.087], mean action: 1.523 [0.000, 3.000],  loss: 8.283279, mae: 32.162467, mean_q: 20.644542, mean_eps: 0.758704\n",
      "  40348/150000: episode: 436, duration: 0.500s, episode steps:  77, steps per second: 154, episode reward: -89.169, mean reward: -1.158 [-100.000,  9.921], mean action: 1.740 [0.000, 3.000],  loss: 6.106678, mae: 32.646252, mean_q: 21.279463, mean_eps: 0.758146\n",
      "  40444/150000: episode: 437, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: -82.975, mean reward: -0.864 [-100.000, 16.345], mean action: 1.667 [0.000, 3.000],  loss: 6.664262, mae: 32.078551, mean_q: 20.758964, mean_eps: 0.757627\n",
      "  40525/150000: episode: 438, duration: 0.535s, episode steps:  81, steps per second: 151, episode reward: -109.648, mean reward: -1.354 [-100.000,  8.199], mean action: 1.840 [0.000, 3.000],  loss: 15.030863, mae: 32.325520, mean_q: 19.987512, mean_eps: 0.757096\n",
      "  40609/150000: episode: 439, duration: 0.553s, episode steps:  84, steps per second: 152, episode reward: -46.042, mean reward: -0.548 [-100.000,  7.285], mean action: 1.440 [0.000, 3.000],  loss: 13.603793, mae: 32.618410, mean_q: 19.860856, mean_eps: 0.756601\n",
      "  40725/150000: episode: 440, duration: 0.784s, episode steps: 116, steps per second: 148, episode reward: -106.504, mean reward: -0.918 [-100.000, 11.502], mean action: 1.724 [0.000, 3.000],  loss: 11.875912, mae: 32.319305, mean_q: 20.725486, mean_eps: 0.756001\n",
      "  40843/150000: episode: 441, duration: 0.775s, episode steps: 118, steps per second: 152, episode reward: -83.849, mean reward: -0.711 [-100.000,  8.084], mean action: 1.475 [0.000, 3.000],  loss: 7.233231, mae: 32.548781, mean_q: 20.391330, mean_eps: 0.755299\n",
      "  40909/150000: episode: 442, duration: 0.438s, episode steps:  66, steps per second: 151, episode reward: -68.156, mean reward: -1.033 [-100.000, 10.167], mean action: 1.273 [0.000, 3.000],  loss: 7.210431, mae: 32.516223, mean_q: 20.566239, mean_eps: 0.754747\n",
      "  41002/150000: episode: 443, duration: 0.629s, episode steps:  93, steps per second: 148, episode reward: -191.327, mean reward: -2.057 [-100.000, 23.043], mean action: 1.516 [0.000, 3.000],  loss: 6.617531, mae: 32.320684, mean_q: 20.597581, mean_eps: 0.754270\n",
      "  41110/150000: episode: 444, duration: 0.729s, episode steps: 108, steps per second: 148, episode reward: -38.836, mean reward: -0.360 [-100.000, 59.461], mean action: 1.528 [0.000, 3.000],  loss: 6.756866, mae: 32.331626, mean_q: 19.154707, mean_eps: 0.753667\n",
      "  41187/150000: episode: 445, duration: 0.506s, episode steps:  77, steps per second: 152, episode reward: -71.223, mean reward: -0.925 [-100.000, 11.786], mean action: 1.792 [0.000, 3.000],  loss: 8.145605, mae: 32.243996, mean_q: 19.695443, mean_eps: 0.753112\n",
      "  41259/150000: episode: 446, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: -104.918, mean reward: -1.457 [-100.000,  4.972], mean action: 1.764 [0.000, 3.000],  loss: 5.744923, mae: 32.348609, mean_q: 20.626204, mean_eps: 0.752665\n",
      "  41346/150000: episode: 447, duration: 0.708s, episode steps:  87, steps per second: 123, episode reward: -121.115, mean reward: -1.392 [-100.000,  9.433], mean action: 1.690 [0.000, 3.000],  loss: 7.771732, mae: 32.704156, mean_q: 19.701834, mean_eps: 0.752188\n",
      "  41455/150000: episode: 448, duration: 0.850s, episode steps: 109, steps per second: 128, episode reward: -78.981, mean reward: -0.725 [-100.000, 11.974], mean action: 1.606 [0.000, 3.000],  loss: 9.112602, mae: 32.624766, mean_q: 20.186016, mean_eps: 0.751600\n",
      "  41539/150000: episode: 449, duration: 0.560s, episode steps:  84, steps per second: 150, episode reward: -68.705, mean reward: -0.818 [-100.000, 17.948], mean action: 1.643 [0.000, 3.000],  loss: 6.418258, mae: 32.895306, mean_q: 20.646968, mean_eps: 0.751021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  41621/150000: episode: 450, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -122.527, mean reward: -1.494 [-100.000, 21.356], mean action: 1.134 [0.000, 3.000],  loss: 4.903860, mae: 32.459277, mean_q: 20.114383, mean_eps: 0.750523\n",
      "  41714/150000: episode: 451, duration: 0.639s, episode steps:  93, steps per second: 145, episode reward: -115.683, mean reward: -1.244 [-100.000, 17.172], mean action: 1.495 [0.000, 3.000],  loss: 8.299766, mae: 32.113703, mean_q: 19.624049, mean_eps: 0.749998\n",
      "  41843/150000: episode: 452, duration: 0.845s, episode steps: 129, steps per second: 153, episode reward: -42.656, mean reward: -0.331 [-100.000,  7.154], mean action: 1.504 [0.000, 3.000],  loss: 9.779988, mae: 32.354453, mean_q: 20.160950, mean_eps: 0.749332\n",
      "  41911/150000: episode: 453, duration: 0.457s, episode steps:  68, steps per second: 149, episode reward: -148.433, mean reward: -2.183 [-100.000,  7.564], mean action: 1.412 [0.000, 3.000],  loss: 6.597249, mae: 32.432332, mean_q: 19.920374, mean_eps: 0.748741\n",
      "  41993/150000: episode: 454, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -41.799, mean reward: -0.510 [-100.000,  8.143], mean action: 1.561 [0.000, 3.000],  loss: 7.919655, mae: 32.318135, mean_q: 19.462687, mean_eps: 0.748291\n",
      "  42087/150000: episode: 455, duration: 0.617s, episode steps:  94, steps per second: 152, episode reward: -94.662, mean reward: -1.007 [-100.000, 12.925], mean action: 1.415 [0.000, 3.000],  loss: 9.563846, mae: 32.612709, mean_q: 20.432389, mean_eps: 0.747763\n",
      "  42152/150000: episode: 456, duration: 0.429s, episode steps:  65, steps per second: 152, episode reward: -54.413, mean reward: -0.837 [-100.000, 16.026], mean action: 1.446 [0.000, 3.000],  loss: 10.849479, mae: 32.860985, mean_q: 18.980067, mean_eps: 0.747286\n",
      "  42241/150000: episode: 457, duration: 0.590s, episode steps:  89, steps per second: 151, episode reward: -55.567, mean reward: -0.624 [-100.000, 12.219], mean action: 1.506 [0.000, 3.000],  loss: 7.110647, mae: 33.073816, mean_q: 21.667205, mean_eps: 0.746824\n",
      "  42327/150000: episode: 458, duration: 0.610s, episode steps:  86, steps per second: 141, episode reward: -53.931, mean reward: -0.627 [-100.000,  7.406], mean action: 1.581 [0.000, 3.000],  loss: 7.703996, mae: 32.644628, mean_q: 20.363266, mean_eps: 0.746299\n",
      "  42406/150000: episode: 459, duration: 0.525s, episode steps:  79, steps per second: 151, episode reward: -106.758, mean reward: -1.351 [-100.000, 11.095], mean action: 1.316 [0.000, 3.000],  loss: 6.395485, mae: 32.597037, mean_q: 20.464549, mean_eps: 0.745804\n",
      "  42508/150000: episode: 460, duration: 0.667s, episode steps: 102, steps per second: 153, episode reward: -148.874, mean reward: -1.460 [-100.000,  4.206], mean action: 1.569 [0.000, 3.000],  loss: 9.170164, mae: 32.553393, mean_q: 21.271517, mean_eps: 0.745261\n",
      "  42591/150000: episode: 461, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -98.605, mean reward: -1.188 [-100.000,  8.993], mean action: 1.687 [0.000, 3.000],  loss: 6.761790, mae: 32.560058, mean_q: 19.549642, mean_eps: 0.744706\n",
      "  42680/150000: episode: 462, duration: 0.611s, episode steps:  89, steps per second: 146, episode reward: -52.738, mean reward: -0.593 [-100.000, 17.699], mean action: 1.528 [0.000, 3.000],  loss: 6.292523, mae: 32.677424, mean_q: 20.875411, mean_eps: 0.744190\n",
      "  42767/150000: episode: 463, duration: 0.577s, episode steps:  87, steps per second: 151, episode reward: -42.937, mean reward: -0.494 [-100.000, 16.088], mean action: 1.483 [0.000, 3.000],  loss: 5.990875, mae: 32.334145, mean_q: 20.097716, mean_eps: 0.743662\n",
      "  42839/150000: episode: 464, duration: 0.474s, episode steps:  72, steps per second: 152, episode reward: -87.722, mean reward: -1.218 [-100.000,  4.465], mean action: 1.611 [0.000, 3.000],  loss: 8.198121, mae: 32.826904, mean_q: 19.931370, mean_eps: 0.743185\n",
      "  42906/150000: episode: 465, duration: 0.475s, episode steps:  67, steps per second: 141, episode reward: -63.454, mean reward: -0.947 [-100.000,  6.111], mean action: 1.552 [0.000, 3.000],  loss: 5.912547, mae: 32.396054, mean_q: 19.288541, mean_eps: 0.742768\n",
      "  43020/150000: episode: 466, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -132.299, mean reward: -1.161 [-100.000, 11.881], mean action: 1.746 [0.000, 3.000],  loss: 5.883731, mae: 32.786618, mean_q: 20.295375, mean_eps: 0.742225\n",
      "  43103/150000: episode: 467, duration: 0.548s, episode steps:  83, steps per second: 152, episode reward: -43.739, mean reward: -0.527 [-100.000, 12.850], mean action: 1.446 [0.000, 3.000],  loss: 5.589702, mae: 32.810340, mean_q: 20.796973, mean_eps: 0.741634\n",
      "  43221/150000: episode: 468, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -150.322, mean reward: -1.274 [-100.000,  4.559], mean action: 1.636 [0.000, 3.000],  loss: 10.420708, mae: 32.903049, mean_q: 20.640541, mean_eps: 0.741031\n",
      "  43313/150000: episode: 469, duration: 0.629s, episode steps:  92, steps per second: 146, episode reward: -106.986, mean reward: -1.163 [-100.000, 33.600], mean action: 1.620 [0.000, 3.000],  loss: 6.023368, mae: 32.526867, mean_q: 21.296508, mean_eps: 0.740401\n",
      "  43400/150000: episode: 470, duration: 0.596s, episode steps:  87, steps per second: 146, episode reward: -87.891, mean reward: -1.010 [-100.000,  8.946], mean action: 1.471 [0.000, 3.000],  loss: 7.925484, mae: 32.837077, mean_q: 19.906266, mean_eps: 0.739864\n",
      "  43505/150000: episode: 471, duration: 0.707s, episode steps: 105, steps per second: 148, episode reward: -35.807, mean reward: -0.341 [-100.000, 18.283], mean action: 1.657 [0.000, 3.000],  loss: 9.982200, mae: 32.705021, mean_q: 20.376066, mean_eps: 0.739288\n",
      "  43570/150000: episode: 472, duration: 0.469s, episode steps:  65, steps per second: 139, episode reward: -129.081, mean reward: -1.986 [-100.000,  7.204], mean action: 1.585 [0.000, 3.000],  loss: 6.813015, mae: 33.051339, mean_q: 19.530237, mean_eps: 0.738778\n",
      "  43629/150000: episode: 473, duration: 0.404s, episode steps:  59, steps per second: 146, episode reward: -50.088, mean reward: -0.849 [-100.000, 12.240], mean action: 1.525 [0.000, 3.000],  loss: 9.197358, mae: 32.778443, mean_q: 20.502710, mean_eps: 0.738406\n",
      "  43744/150000: episode: 474, duration: 0.852s, episode steps: 115, steps per second: 135, episode reward: -61.723, mean reward: -0.537 [-100.000, 12.065], mean action: 1.461 [0.000, 3.000],  loss: 5.733152, mae: 32.949661, mean_q: 20.545690, mean_eps: 0.737884\n",
      "  43904/150000: episode: 475, duration: 1.313s, episode steps: 160, steps per second: 122, episode reward: -111.214, mean reward: -0.695 [-100.000, 10.367], mean action: 1.444 [0.000, 3.000],  loss: 9.480790, mae: 32.847805, mean_q: 20.444382, mean_eps: 0.737059\n",
      "  43990/150000: episode: 476, duration: 0.657s, episode steps:  86, steps per second: 131, episode reward: -90.810, mean reward: -1.056 [-100.000,  7.777], mean action: 1.535 [0.000, 3.000],  loss: 6.921609, mae: 32.455351, mean_q: 19.136260, mean_eps: 0.736321\n",
      "  44082/150000: episode: 477, duration: 0.668s, episode steps:  92, steps per second: 138, episode reward: -64.031, mean reward: -0.696 [-100.000, 20.723], mean action: 1.543 [0.000, 3.000],  loss: 8.568198, mae: 33.374560, mean_q: 20.536567, mean_eps: 0.735787\n",
      "  44196/150000: episode: 478, duration: 0.868s, episode steps: 114, steps per second: 131, episode reward: -110.253, mean reward: -0.967 [-100.000, 11.880], mean action: 1.395 [0.000, 3.000],  loss: 7.258040, mae: 33.127193, mean_q: 19.745953, mean_eps: 0.735169\n",
      "  44284/150000: episode: 479, duration: 0.644s, episode steps:  88, steps per second: 137, episode reward: -60.533, mean reward: -0.688 [-100.000,  7.856], mean action: 1.568 [0.000, 3.000],  loss: 7.972269, mae: 32.650247, mean_q: 19.871130, mean_eps: 0.734563\n",
      "  44366/150000: episode: 480, duration: 0.585s, episode steps:  82, steps per second: 140, episode reward: -78.301, mean reward: -0.955 [-100.000, 13.028], mean action: 1.585 [0.000, 3.000],  loss: 4.788377, mae: 33.223570, mean_q: 20.137521, mean_eps: 0.734053\n",
      "  44434/150000: episode: 481, duration: 0.506s, episode steps:  68, steps per second: 134, episode reward: -49.972, mean reward: -0.735 [-100.000,  7.068], mean action: 1.412 [0.000, 3.000],  loss: 9.352428, mae: 33.139482, mean_q: 19.292596, mean_eps: 0.733603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  44514/150000: episode: 482, duration: 0.554s, episode steps:  80, steps per second: 144, episode reward: -27.953, mean reward: -0.349 [-100.000, 15.580], mean action: 1.325 [0.000, 3.000],  loss: 6.861582, mae: 33.649924, mean_q: 19.742610, mean_eps: 0.733159\n",
      "  44603/150000: episode: 483, duration: 0.585s, episode steps:  89, steps per second: 152, episode reward: -69.632, mean reward: -0.782 [-100.000, 10.537], mean action: 1.753 [0.000, 3.000],  loss: 5.937384, mae: 32.831088, mean_q: 19.363054, mean_eps: 0.732652\n",
      "  44709/150000: episode: 484, duration: 0.717s, episode steps: 106, steps per second: 148, episode reward: -66.825, mean reward: -0.630 [-100.000, 10.648], mean action: 1.519 [0.000, 3.000],  loss: 8.214511, mae: 33.492190, mean_q: 19.998093, mean_eps: 0.732067\n",
      "  44819/150000: episode: 485, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -31.774, mean reward: -0.289 [-100.000, 12.404], mean action: 1.618 [0.000, 3.000],  loss: 5.992909, mae: 33.210313, mean_q: 19.274331, mean_eps: 0.731419\n",
      "  44909/150000: episode: 486, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -77.514, mean reward: -0.861 [-100.000, 10.434], mean action: 1.511 [0.000, 3.000],  loss: 6.060509, mae: 33.005041, mean_q: 18.886165, mean_eps: 0.730819\n",
      "  45050/150000: episode: 487, duration: 0.974s, episode steps: 141, steps per second: 145, episode reward: -107.127, mean reward: -0.760 [-100.000,  7.325], mean action: 1.624 [0.000, 3.000],  loss: 7.080080, mae: 33.138426, mean_q: 20.649681, mean_eps: 0.730126\n",
      "  45135/150000: episode: 488, duration: 0.580s, episode steps:  85, steps per second: 146, episode reward: -63.535, mean reward: -0.747 [-100.000, 13.447], mean action: 1.671 [0.000, 3.000],  loss: 5.974269, mae: 32.893354, mean_q: 19.975242, mean_eps: 0.729448\n",
      "  45241/150000: episode: 489, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: -87.994, mean reward: -0.830 [-100.000, 13.781], mean action: 1.623 [0.000, 3.000],  loss: 4.578677, mae: 32.865906, mean_q: 19.107833, mean_eps: 0.728875\n",
      "  45326/150000: episode: 490, duration: 0.601s, episode steps:  85, steps per second: 141, episode reward: -13.106, mean reward: -0.154 [-100.000, 12.669], mean action: 1.635 [0.000, 3.000],  loss: 9.729048, mae: 33.013880, mean_q: 19.541811, mean_eps: 0.728302\n",
      "  45430/150000: episode: 491, duration: 0.734s, episode steps: 104, steps per second: 142, episode reward: -75.568, mean reward: -0.727 [-100.000,  9.280], mean action: 1.308 [0.000, 3.000],  loss: 6.918378, mae: 32.973044, mean_q: 19.092008, mean_eps: 0.727735\n",
      "  45496/150000: episode: 492, duration: 0.445s, episode steps:  66, steps per second: 148, episode reward: -90.267, mean reward: -1.368 [-100.000, 10.447], mean action: 1.394 [0.000, 3.000],  loss: 6.139608, mae: 32.761367, mean_q: 20.541921, mean_eps: 0.727225\n",
      "  45600/150000: episode: 493, duration: 0.688s, episode steps: 104, steps per second: 151, episode reward: -49.845, mean reward: -0.479 [-100.000, 13.993], mean action: 1.615 [0.000, 3.000],  loss: 9.092040, mae: 33.243156, mean_q: 19.194118, mean_eps: 0.726715\n",
      "  45719/150000: episode: 494, duration: 0.831s, episode steps: 119, steps per second: 143, episode reward: -101.099, mean reward: -0.850 [-100.000, 19.481], mean action: 1.723 [0.000, 3.000],  loss: 9.624940, mae: 32.808121, mean_q: 18.892948, mean_eps: 0.726046\n",
      "  45796/150000: episode: 495, duration: 0.519s, episode steps:  77, steps per second: 148, episode reward: -52.595, mean reward: -0.683 [-100.000, 10.074], mean action: 1.545 [0.000, 3.000],  loss: 9.026571, mae: 32.983285, mean_q: 18.789885, mean_eps: 0.725458\n",
      "  45865/150000: episode: 496, duration: 0.457s, episode steps:  69, steps per second: 151, episode reward: -76.414, mean reward: -1.107 [-100.000, 10.017], mean action: 1.652 [0.000, 3.000],  loss: 8.611504, mae: 32.893142, mean_q: 20.403673, mean_eps: 0.725020\n",
      "  45941/150000: episode: 497, duration: 0.529s, episode steps:  76, steps per second: 144, episode reward: -100.829, mean reward: -1.327 [-100.000, 11.381], mean action: 1.539 [0.000, 3.000],  loss: 7.936373, mae: 33.439732, mean_q: 18.784657, mean_eps: 0.724585\n",
      "  46024/150000: episode: 498, duration: 0.603s, episode steps:  83, steps per second: 138, episode reward:  8.782, mean reward:  0.106 [-100.000, 120.814], mean action: 1.566 [0.000, 3.000],  loss: 5.845604, mae: 33.370996, mean_q: 18.092209, mean_eps: 0.724108\n",
      "  46092/150000: episode: 499, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: -52.986, mean reward: -0.779 [-100.000,  7.400], mean action: 1.456 [0.000, 3.000],  loss: 6.206284, mae: 32.796259, mean_q: 17.886573, mean_eps: 0.723655\n",
      "  46208/150000: episode: 500, duration: 0.772s, episode steps: 116, steps per second: 150, episode reward: -62.594, mean reward: -0.540 [-100.000, 11.226], mean action: 1.612 [0.000, 3.000],  loss: 9.960474, mae: 33.122800, mean_q: 18.967463, mean_eps: 0.723103\n",
      "  46305/150000: episode: 501, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -103.802, mean reward: -1.070 [-100.000,  8.855], mean action: 1.680 [0.000, 3.000],  loss: 5.495162, mae: 33.009394, mean_q: 19.410094, mean_eps: 0.722464\n",
      "  46442/150000: episode: 502, duration: 0.919s, episode steps: 137, steps per second: 149, episode reward: -79.032, mean reward: -0.577 [-100.000, 16.120], mean action: 1.620 [0.000, 3.000],  loss: 6.391685, mae: 33.210581, mean_q: 19.179801, mean_eps: 0.721762\n",
      "  46536/150000: episode: 503, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -139.690, mean reward: -1.486 [-100.000, 10.529], mean action: 1.649 [0.000, 3.000],  loss: 7.155630, mae: 33.154487, mean_q: 18.373870, mean_eps: 0.721069\n",
      "  46632/150000: episode: 504, duration: 0.670s, episode steps:  96, steps per second: 143, episode reward: -32.918, mean reward: -0.343 [-100.000,  8.374], mean action: 1.604 [0.000, 3.000],  loss: 3.955996, mae: 33.388277, mean_q: 19.017047, mean_eps: 0.720499\n",
      "  46740/150000: episode: 505, duration: 0.732s, episode steps: 108, steps per second: 147, episode reward: -130.074, mean reward: -1.204 [-100.000, 10.469], mean action: 1.556 [0.000, 3.000],  loss: 9.183806, mae: 33.420114, mean_q: 18.932653, mean_eps: 0.719887\n",
      "  46813/150000: episode: 506, duration: 0.488s, episode steps:  73, steps per second: 150, episode reward: -80.413, mean reward: -1.102 [-100.000,  7.901], mean action: 1.671 [0.000, 3.000],  loss: 6.174142, mae: 33.313051, mean_q: 19.750757, mean_eps: 0.719344\n",
      "  46923/150000: episode: 507, duration: 0.754s, episode steps: 110, steps per second: 146, episode reward: -93.332, mean reward: -0.848 [-100.000, 10.344], mean action: 1.564 [0.000, 3.000],  loss: 10.650814, mae: 33.017713, mean_q: 19.288571, mean_eps: 0.718795\n",
      "  47006/150000: episode: 508, duration: 0.583s, episode steps:  83, steps per second: 142, episode reward: -84.395, mean reward: -1.017 [-100.000,  8.906], mean action: 1.542 [0.000, 3.000],  loss: 11.599483, mae: 32.869937, mean_q: 17.317914, mean_eps: 0.718216\n",
      "  47087/150000: episode: 509, duration: 0.540s, episode steps:  81, steps per second: 150, episode reward: -76.482, mean reward: -0.944 [-100.000, 12.094], mean action: 1.568 [0.000, 3.000],  loss: 7.639192, mae: 32.936887, mean_q: 19.808550, mean_eps: 0.717724\n",
      "  47168/150000: episode: 510, duration: 0.548s, episode steps:  81, steps per second: 148, episode reward: -95.200, mean reward: -1.175 [-100.000, 10.778], mean action: 1.654 [0.000, 3.000],  loss: 8.824720, mae: 33.137394, mean_q: 20.103400, mean_eps: 0.717238\n",
      "  47258/150000: episode: 511, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -130.387, mean reward: -1.449 [-100.000,  7.106], mean action: 1.489 [0.000, 3.000],  loss: 4.567222, mae: 33.302499, mean_q: 20.283869, mean_eps: 0.716725\n",
      "  47340/150000: episode: 512, duration: 0.569s, episode steps:  82, steps per second: 144, episode reward: -66.525, mean reward: -0.811 [-100.000, 20.049], mean action: 1.756 [0.000, 3.000],  loss: 4.937275, mae: 33.569147, mean_q: 20.506982, mean_eps: 0.716209\n",
      "  47444/150000: episode: 513, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -56.057, mean reward: -0.539 [-100.000,  8.274], mean action: 1.596 [0.000, 3.000],  loss: 5.681581, mae: 33.373263, mean_q: 20.209115, mean_eps: 0.715651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  47546/150000: episode: 514, duration: 0.696s, episode steps: 102, steps per second: 147, episode reward: -98.972, mean reward: -0.970 [-100.000, 10.900], mean action: 1.461 [0.000, 3.000],  loss: 7.559752, mae: 33.156245, mean_q: 20.851578, mean_eps: 0.715033\n",
      "  47634/150000: episode: 515, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: -115.189, mean reward: -1.309 [-100.000,  6.511], mean action: 1.455 [0.000, 3.000],  loss: 6.945227, mae: 33.188907, mean_q: 20.111664, mean_eps: 0.714463\n",
      "  47703/150000: episode: 516, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -50.480, mean reward: -0.732 [-100.000, 14.107], mean action: 1.652 [0.000, 3.000],  loss: 4.655087, mae: 33.363126, mean_q: 18.590262, mean_eps: 0.713992\n",
      "  47811/150000: episode: 517, duration: 0.723s, episode steps: 108, steps per second: 149, episode reward: -89.929, mean reward: -0.833 [-100.000, 13.225], mean action: 1.676 [0.000, 3.000],  loss: 10.156859, mae: 33.279927, mean_q: 20.154879, mean_eps: 0.713461\n",
      "  47900/150000: episode: 518, duration: 0.641s, episode steps:  89, steps per second: 139, episode reward: -85.850, mean reward: -0.965 [-100.000, 16.076], mean action: 1.528 [0.000, 3.000],  loss: 8.129365, mae: 33.491549, mean_q: 19.930695, mean_eps: 0.712870\n",
      "  47980/150000: episode: 519, duration: 0.562s, episode steps:  80, steps per second: 142, episode reward: -36.065, mean reward: -0.451 [-100.000, 22.715], mean action: 1.812 [0.000, 3.000],  loss: 5.223819, mae: 32.987230, mean_q: 20.528307, mean_eps: 0.712363\n",
      "  48102/150000: episode: 520, duration: 0.831s, episode steps: 122, steps per second: 147, episode reward: -84.719, mean reward: -0.694 [-100.000, 10.909], mean action: 1.426 [0.000, 3.000],  loss: 9.105728, mae: 33.162427, mean_q: 21.167879, mean_eps: 0.711757\n",
      "  48183/150000: episode: 521, duration: 0.587s, episode steps:  81, steps per second: 138, episode reward:  0.154, mean reward:  0.002 [-100.000, 41.017], mean action: 1.877 [0.000, 3.000],  loss: 6.937239, mae: 33.310631, mean_q: 20.432835, mean_eps: 0.711148\n",
      "  48289/150000: episode: 522, duration: 0.736s, episode steps: 106, steps per second: 144, episode reward: -83.908, mean reward: -0.792 [-100.000, 17.946], mean action: 1.519 [0.000, 3.000],  loss: 6.805354, mae: 33.514233, mean_q: 20.288960, mean_eps: 0.710587\n",
      "  48405/150000: episode: 523, duration: 0.823s, episode steps: 116, steps per second: 141, episode reward: -68.586, mean reward: -0.591 [-100.000, 17.283], mean action: 1.672 [0.000, 3.000],  loss: 5.876519, mae: 33.552652, mean_q: 20.265074, mean_eps: 0.709921\n",
      "  48509/150000: episode: 524, duration: 0.727s, episode steps: 104, steps per second: 143, episode reward: -52.523, mean reward: -0.505 [-100.000, 26.875], mean action: 1.740 [0.000, 3.000],  loss: 8.908086, mae: 33.489133, mean_q: 20.339846, mean_eps: 0.709261\n",
      "  48611/150000: episode: 525, duration: 0.703s, episode steps: 102, steps per second: 145, episode reward: -142.431, mean reward: -1.396 [-100.000, 10.980], mean action: 1.716 [0.000, 3.000],  loss: 7.328302, mae: 33.909450, mean_q: 20.753625, mean_eps: 0.708643\n",
      "  48675/150000: episode: 526, duration: 0.425s, episode steps:  64, steps per second: 151, episode reward: -66.741, mean reward: -1.043 [-100.000, 10.581], mean action: 1.719 [0.000, 3.000],  loss: 2.931695, mae: 33.026689, mean_q: 20.004853, mean_eps: 0.708145\n",
      "  48774/150000: episode: 527, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -58.331, mean reward: -0.589 [-100.000, 16.949], mean action: 1.828 [0.000, 3.000],  loss: 7.046735, mae: 33.209940, mean_q: 19.915617, mean_eps: 0.707656\n",
      "  48871/150000: episode: 528, duration: 0.717s, episode steps:  97, steps per second: 135, episode reward: -133.853, mean reward: -1.380 [-100.000, 10.046], mean action: 1.557 [0.000, 3.000],  loss: 7.240093, mae: 33.496199, mean_q: 20.102287, mean_eps: 0.707068\n",
      "  48942/150000: episode: 529, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -64.921, mean reward: -0.914 [-100.000, 10.899], mean action: 1.676 [0.000, 3.000],  loss: 5.636192, mae: 33.751275, mean_q: 20.165463, mean_eps: 0.706564\n",
      "  49040/150000: episode: 530, duration: 0.657s, episode steps:  98, steps per second: 149, episode reward: -60.532, mean reward: -0.618 [-100.000, 39.513], mean action: 1.867 [0.000, 3.000],  loss: 5.426255, mae: 33.473598, mean_q: 20.104794, mean_eps: 0.706057\n",
      "  49123/150000: episode: 531, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -113.671, mean reward: -1.370 [-100.000,  9.214], mean action: 1.494 [0.000, 3.000],  loss: 4.789377, mae: 33.243107, mean_q: 21.012272, mean_eps: 0.705514\n",
      "  49237/150000: episode: 532, duration: 0.822s, episode steps: 114, steps per second: 139, episode reward: -55.034, mean reward: -0.483 [-100.000,  8.865], mean action: 1.596 [0.000, 3.000],  loss: 5.396751, mae: 33.355447, mean_q: 20.373609, mean_eps: 0.704923\n",
      "  49328/150000: episode: 533, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: -62.291, mean reward: -0.685 [-100.000,  7.788], mean action: 1.495 [0.000, 3.000],  loss: 6.889116, mae: 33.818113, mean_q: 20.190117, mean_eps: 0.704308\n",
      "  49415/150000: episode: 534, duration: 0.588s, episode steps:  87, steps per second: 148, episode reward: -102.901, mean reward: -1.183 [-100.000, 11.870], mean action: 1.540 [0.000, 3.000],  loss: 4.922357, mae: 33.887600, mean_q: 19.911374, mean_eps: 0.703774\n",
      "  49544/150000: episode: 535, duration: 0.923s, episode steps: 129, steps per second: 140, episode reward: -40.697, mean reward: -0.315 [-100.000, 20.668], mean action: 1.729 [0.000, 3.000],  loss: 6.254702, mae: 33.635881, mean_q: 19.549952, mean_eps: 0.703126\n",
      "  49661/150000: episode: 536, duration: 0.778s, episode steps: 117, steps per second: 150, episode reward: -111.982, mean reward: -0.957 [-100.000,  6.219], mean action: 1.479 [0.000, 3.000],  loss: 5.965131, mae: 33.369841, mean_q: 21.112045, mean_eps: 0.702388\n",
      "  49762/150000: episode: 537, duration: 0.695s, episode steps: 101, steps per second: 145, episode reward: -54.678, mean reward: -0.541 [-100.000, 17.432], mean action: 1.465 [0.000, 3.000],  loss: 8.849202, mae: 33.705648, mean_q: 20.152445, mean_eps: 0.701734\n",
      "  49859/150000: episode: 538, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -42.233, mean reward: -0.435 [-100.000, 28.427], mean action: 1.670 [0.000, 3.000],  loss: 4.937115, mae: 33.188009, mean_q: 20.950075, mean_eps: 0.701140\n",
      "  49935/150000: episode: 539, duration: 0.512s, episode steps:  76, steps per second: 149, episode reward: -23.673, mean reward: -0.311 [-100.000, 11.473], mean action: 1.816 [0.000, 3.000],  loss: 6.334548, mae: 33.070709, mean_q: 20.027848, mean_eps: 0.700621\n",
      "  50004/150000: episode: 540, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -76.432, mean reward: -1.108 [-100.000,  7.437], mean action: 1.478 [0.000, 3.000],  loss: 6.813707, mae: 33.478699, mean_q: 19.608808, mean_eps: 0.700186\n",
      "  50098/150000: episode: 541, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -107.282, mean reward: -1.141 [-100.000,  6.430], mean action: 1.457 [0.000, 3.000],  loss: 8.137921, mae: 33.565540, mean_q: 20.661531, mean_eps: 0.699697\n",
      "  50195/150000: episode: 542, duration: 0.673s, episode steps:  97, steps per second: 144, episode reward: -94.121, mean reward: -0.970 [-100.000, 22.178], mean action: 1.433 [0.000, 3.000],  loss: 6.989102, mae: 33.803553, mean_q: 20.354395, mean_eps: 0.699124\n",
      "  50324/150000: episode: 543, duration: 0.871s, episode steps: 129, steps per second: 148, episode reward: -29.174, mean reward: -0.226 [-100.000, 22.271], mean action: 1.868 [0.000, 3.000],  loss: 7.513691, mae: 33.749313, mean_q: 20.900645, mean_eps: 0.698446\n",
      "  50442/150000: episode: 544, duration: 0.830s, episode steps: 118, steps per second: 142, episode reward: -83.615, mean reward: -0.709 [-100.000, 15.829], mean action: 1.542 [0.000, 3.000],  loss: 7.240811, mae: 33.845057, mean_q: 21.272572, mean_eps: 0.697705\n",
      "  50560/150000: episode: 545, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -153.250, mean reward: -1.299 [-100.000, 10.571], mean action: 1.551 [0.000, 3.000],  loss: 5.263118, mae: 33.565252, mean_q: 21.866387, mean_eps: 0.696997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  50655/150000: episode: 546, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -86.695, mean reward: -0.913 [-100.000,  6.764], mean action: 1.684 [0.000, 3.000],  loss: 5.015145, mae: 33.279986, mean_q: 20.521668, mean_eps: 0.696358\n",
      "  50751/150000: episode: 547, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: -106.419, mean reward: -1.109 [-100.000, 15.261], mean action: 1.375 [0.000, 3.000],  loss: 5.898320, mae: 33.909001, mean_q: 20.465735, mean_eps: 0.695785\n",
      "  50858/150000: episode: 548, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -67.228, mean reward: -0.628 [-100.000, 11.775], mean action: 1.505 [0.000, 3.000],  loss: 9.287596, mae: 33.220127, mean_q: 20.833539, mean_eps: 0.695176\n",
      "  50959/150000: episode: 549, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: -89.596, mean reward: -0.887 [-100.000, 12.506], mean action: 1.713 [0.000, 3.000],  loss: 6.221144, mae: 33.826143, mean_q: 19.860324, mean_eps: 0.694552\n",
      "  51034/150000: episode: 550, duration: 0.610s, episode steps:  75, steps per second: 123, episode reward: -47.462, mean reward: -0.633 [-100.000, 10.876], mean action: 1.893 [0.000, 3.000],  loss: 7.134578, mae: 33.369643, mean_q: 20.571568, mean_eps: 0.694024\n",
      "  51165/150000: episode: 551, duration: 0.913s, episode steps: 131, steps per second: 143, episode reward: -6.705, mean reward: -0.051 [-100.000, 18.353], mean action: 1.626 [0.000, 3.000],  loss: 10.128694, mae: 33.800230, mean_q: 19.291645, mean_eps: 0.693406\n",
      "  51259/150000: episode: 552, duration: 0.636s, episode steps:  94, steps per second: 148, episode reward: -84.259, mean reward: -0.896 [-100.000, 10.756], mean action: 1.436 [0.000, 3.000],  loss: 4.409045, mae: 33.406114, mean_q: 18.824204, mean_eps: 0.692731\n",
      "  51356/150000: episode: 553, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -60.988, mean reward: -0.629 [-100.000, 10.031], mean action: 1.722 [0.000, 3.000],  loss: 7.028455, mae: 33.544851, mean_q: 19.001192, mean_eps: 0.692158\n",
      "  51453/150000: episode: 554, duration: 0.662s, episode steps:  97, steps per second: 147, episode reward: -102.640, mean reward: -1.058 [-100.000,  7.283], mean action: 1.711 [0.000, 3.000],  loss: 8.797604, mae: 33.674250, mean_q: 19.618052, mean_eps: 0.691576\n",
      "  51571/150000: episode: 555, duration: 0.803s, episode steps: 118, steps per second: 147, episode reward: -38.603, mean reward: -0.327 [-100.000, 11.648], mean action: 1.576 [0.000, 3.000],  loss: 7.649442, mae: 33.558434, mean_q: 19.865572, mean_eps: 0.690931\n",
      "  51635/150000: episode: 556, duration: 0.459s, episode steps:  64, steps per second: 140, episode reward: -143.989, mean reward: -2.250 [-100.000,  6.177], mean action: 1.250 [0.000, 3.000],  loss: 8.879874, mae: 33.929300, mean_q: 19.121085, mean_eps: 0.690385\n",
      "  51750/150000: episode: 557, duration: 0.785s, episode steps: 115, steps per second: 147, episode reward: -86.859, mean reward: -0.755 [-100.000, 11.685], mean action: 1.478 [0.000, 3.000],  loss: 5.028843, mae: 33.639879, mean_q: 20.227063, mean_eps: 0.689848\n",
      "  51867/150000: episode: 558, duration: 0.801s, episode steps: 117, steps per second: 146, episode reward: -102.113, mean reward: -0.873 [-100.000,  7.083], mean action: 1.658 [0.000, 3.000],  loss: 10.708431, mae: 33.597943, mean_q: 18.527634, mean_eps: 0.689152\n",
      "  51957/150000: episode: 559, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -39.607, mean reward: -0.440 [-100.000, 47.027], mean action: 1.567 [0.000, 3.000],  loss: 9.319432, mae: 33.923810, mean_q: 20.009073, mean_eps: 0.688531\n",
      "  52039/150000: episode: 560, duration: 0.565s, episode steps:  82, steps per second: 145, episode reward: -57.449, mean reward: -0.701 [-100.000,  7.148], mean action: 1.549 [0.000, 3.000],  loss: 5.137671, mae: 33.363244, mean_q: 19.398048, mean_eps: 0.688015\n",
      "  52131/150000: episode: 561, duration: 0.619s, episode steps:  92, steps per second: 149, episode reward: -85.203, mean reward: -0.926 [-100.000,  5.892], mean action: 1.761 [0.000, 3.000],  loss: 4.235358, mae: 33.676686, mean_q: 19.581904, mean_eps: 0.687493\n",
      "  52271/150000: episode: 562, duration: 0.986s, episode steps: 140, steps per second: 142, episode reward: -15.663, mean reward: -0.112 [-100.000, 13.084], mean action: 1.414 [0.000, 3.000],  loss: 7.196080, mae: 33.470519, mean_q: 18.901939, mean_eps: 0.686797\n",
      "  52390/150000: episode: 563, duration: 0.808s, episode steps: 119, steps per second: 147, episode reward: -151.946, mean reward: -1.277 [-100.000,  5.852], mean action: 1.563 [0.000, 3.000],  loss: 4.812533, mae: 33.955571, mean_q: 19.837192, mean_eps: 0.686020\n",
      "  52514/150000: episode: 564, duration: 0.832s, episode steps: 124, steps per second: 149, episode reward: -102.550, mean reward: -0.827 [-100.000,  6.948], mean action: 1.589 [0.000, 3.000],  loss: 6.052687, mae: 33.567792, mean_q: 19.782713, mean_eps: 0.685291\n",
      "  52597/150000: episode: 565, duration: 0.603s, episode steps:  83, steps per second: 138, episode reward: -91.980, mean reward: -1.108 [-100.000, 12.858], mean action: 1.494 [0.000, 3.000],  loss: 7.386704, mae: 33.476703, mean_q: 19.794533, mean_eps: 0.684670\n",
      "  52689/150000: episode: 566, duration: 0.628s, episode steps:  92, steps per second: 146, episode reward: -25.008, mean reward: -0.272 [-100.000, 17.832], mean action: 1.652 [0.000, 3.000],  loss: 8.656379, mae: 33.444216, mean_q: 20.143057, mean_eps: 0.684145\n",
      "  52768/150000: episode: 567, duration: 0.561s, episode steps:  79, steps per second: 141, episode reward: -41.372, mean reward: -0.524 [-100.000,  9.094], mean action: 1.734 [0.000, 3.000],  loss: 4.728429, mae: 33.921241, mean_q: 20.650402, mean_eps: 0.683632\n",
      "  52866/150000: episode: 568, duration: 0.697s, episode steps:  98, steps per second: 141, episode reward: -74.096, mean reward: -0.756 [-100.000, 12.528], mean action: 1.786 [0.000, 3.000],  loss: 5.944958, mae: 33.749324, mean_q: 20.471800, mean_eps: 0.683101\n",
      "  52964/150000: episode: 569, duration: 0.675s, episode steps:  98, steps per second: 145, episode reward: -53.840, mean reward: -0.549 [-100.000, 18.759], mean action: 1.755 [0.000, 3.000],  loss: 5.888999, mae: 33.494484, mean_q: 19.416169, mean_eps: 0.682513\n",
      "  53083/150000: episode: 570, duration: 0.794s, episode steps: 119, steps per second: 150, episode reward: -114.489, mean reward: -0.962 [-100.000, 11.242], mean action: 1.647 [0.000, 3.000],  loss: 5.576333, mae: 33.795159, mean_q: 19.980019, mean_eps: 0.681862\n",
      "  53155/150000: episode: 571, duration: 0.535s, episode steps:  72, steps per second: 135, episode reward: -56.826, mean reward: -0.789 [-100.000,  8.211], mean action: 1.514 [0.000, 3.000],  loss: 3.811608, mae: 33.427821, mean_q: 20.040351, mean_eps: 0.681289\n",
      "  53267/150000: episode: 572, duration: 0.945s, episode steps: 112, steps per second: 118, episode reward: -59.400, mean reward: -0.530 [-100.000, 15.510], mean action: 1.482 [0.000, 3.000],  loss: 5.594029, mae: 33.395960, mean_q: 19.314433, mean_eps: 0.680737\n",
      "  53348/150000: episode: 573, duration: 0.634s, episode steps:  81, steps per second: 128, episode reward: -119.538, mean reward: -1.476 [-100.000, 10.449], mean action: 1.469 [0.000, 3.000],  loss: 10.408171, mae: 33.682089, mean_q: 19.499982, mean_eps: 0.680158\n",
      "  53460/150000: episode: 574, duration: 0.877s, episode steps: 112, steps per second: 128, episode reward: -76.447, mean reward: -0.683 [-100.000,  9.638], mean action: 1.491 [0.000, 3.000],  loss: 6.992164, mae: 33.704069, mean_q: 19.691640, mean_eps: 0.679579\n",
      "  53561/150000: episode: 575, duration: 0.847s, episode steps: 101, steps per second: 119, episode reward: -99.703, mean reward: -0.987 [-100.000,  5.732], mean action: 1.822 [0.000, 3.000],  loss: 5.941418, mae: 33.225642, mean_q: 20.424473, mean_eps: 0.678940\n",
      "  53657/150000: episode: 576, duration: 0.779s, episode steps:  96, steps per second: 123, episode reward: -98.455, mean reward: -1.026 [-100.000, 10.564], mean action: 1.594 [0.000, 3.000],  loss: 7.412554, mae: 34.047202, mean_q: 20.011757, mean_eps: 0.678349\n",
      "  53732/150000: episode: 577, duration: 0.669s, episode steps:  75, steps per second: 112, episode reward: -45.636, mean reward: -0.608 [-100.000, 12.769], mean action: 1.507 [0.000, 3.000],  loss: 4.262622, mae: 33.871964, mean_q: 20.780944, mean_eps: 0.677836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  53816/150000: episode: 578, duration: 0.590s, episode steps:  84, steps per second: 142, episode reward: -21.752, mean reward: -0.259 [-100.000, 17.104], mean action: 1.643 [0.000, 3.000],  loss: 8.515072, mae: 33.722496, mean_q: 19.382689, mean_eps: 0.677359\n",
      "  53920/150000: episode: 579, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -45.806, mean reward: -0.440 [-100.000, 13.315], mean action: 1.663 [0.000, 3.000],  loss: 5.334314, mae: 33.871264, mean_q: 21.374811, mean_eps: 0.676795\n",
      "  54008/150000: episode: 580, duration: 0.633s, episode steps:  88, steps per second: 139, episode reward: -76.860, mean reward: -0.873 [-100.000,  8.202], mean action: 1.500 [0.000, 3.000],  loss: 7.578843, mae: 33.789845, mean_q: 19.966911, mean_eps: 0.676219\n",
      "  54133/150000: episode: 581, duration: 0.846s, episode steps: 125, steps per second: 148, episode reward: -57.337, mean reward: -0.459 [-100.000,  7.230], mean action: 1.416 [0.000, 3.000],  loss: 7.051161, mae: 33.719210, mean_q: 20.790442, mean_eps: 0.675580\n",
      "  54204/150000: episode: 582, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -125.941, mean reward: -1.774 [-100.000,  8.821], mean action: 1.563 [0.000, 3.000],  loss: 6.045807, mae: 34.239129, mean_q: 20.820702, mean_eps: 0.674992\n",
      "  54291/150000: episode: 583, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -106.943, mean reward: -1.229 [-100.000, 15.776], mean action: 1.448 [0.000, 3.000],  loss: 7.314054, mae: 33.962470, mean_q: 20.976435, mean_eps: 0.674518\n",
      "  54424/150000: episode: 584, duration: 1.170s, episode steps: 133, steps per second: 114, episode reward: -60.592, mean reward: -0.456 [-100.000, 19.111], mean action: 1.684 [0.000, 3.000],  loss: 5.922913, mae: 33.515897, mean_q: 20.900988, mean_eps: 0.673858\n",
      "  54530/150000: episode: 585, duration: 0.852s, episode steps: 106, steps per second: 124, episode reward: -80.513, mean reward: -0.760 [-100.000, 18.772], mean action: 1.623 [0.000, 3.000],  loss: 7.913945, mae: 33.525383, mean_q: 19.631906, mean_eps: 0.673141\n",
      "  54622/150000: episode: 586, duration: 0.684s, episode steps:  92, steps per second: 135, episode reward: -141.908, mean reward: -1.542 [-100.000,  7.984], mean action: 1.457 [0.000, 3.000],  loss: 5.524576, mae: 34.098807, mean_q: 20.620034, mean_eps: 0.672547\n",
      "  54732/150000: episode: 587, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -113.407, mean reward: -1.031 [-100.000, 10.037], mean action: 1.418 [0.000, 3.000],  loss: 5.564389, mae: 33.873680, mean_q: 20.425891, mean_eps: 0.671941\n",
      "  54801/150000: episode: 588, duration: 0.490s, episode steps:  69, steps per second: 141, episode reward: -56.532, mean reward: -0.819 [-100.000, 19.607], mean action: 1.464 [0.000, 3.000],  loss: 6.026823, mae: 34.021450, mean_q: 20.420641, mean_eps: 0.671404\n",
      "  54920/150000: episode: 589, duration: 0.855s, episode steps: 119, steps per second: 139, episode reward: -66.968, mean reward: -0.563 [-100.000, 16.679], mean action: 1.471 [0.000, 3.000],  loss: 8.056402, mae: 33.988142, mean_q: 19.683870, mean_eps: 0.670840\n",
      "  55008/150000: episode: 590, duration: 0.617s, episode steps:  88, steps per second: 143, episode reward: -78.507, mean reward: -0.892 [-100.000, 15.807], mean action: 1.727 [0.000, 3.000],  loss: 3.238982, mae: 33.807907, mean_q: 19.483316, mean_eps: 0.670219\n",
      "  55109/150000: episode: 591, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: -79.182, mean reward: -0.784 [-100.000, 16.196], mean action: 1.525 [0.000, 3.000],  loss: 4.633976, mae: 34.041289, mean_q: 18.911936, mean_eps: 0.669652\n",
      "  55197/150000: episode: 592, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -111.530, mean reward: -1.267 [-100.000,  6.097], mean action: 1.568 [0.000, 3.000],  loss: 10.450600, mae: 34.383733, mean_q: 19.973418, mean_eps: 0.669085\n",
      "  55322/150000: episode: 593, duration: 0.866s, episode steps: 125, steps per second: 144, episode reward: -67.106, mean reward: -0.537 [-100.000, 11.149], mean action: 1.672 [0.000, 3.000],  loss: 6.247831, mae: 34.008780, mean_q: 20.309381, mean_eps: 0.668446\n",
      "  55444/150000: episode: 594, duration: 0.813s, episode steps: 122, steps per second: 150, episode reward:  4.226, mean reward:  0.035 [-100.000, 13.029], mean action: 1.566 [0.000, 3.000],  loss: 6.732795, mae: 34.059885, mean_q: 19.136424, mean_eps: 0.667705\n",
      "  55545/150000: episode: 595, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -108.719, mean reward: -1.076 [-100.000, 10.721], mean action: 1.535 [0.000, 3.000],  loss: 5.775786, mae: 34.003286, mean_q: 21.074181, mean_eps: 0.667036\n",
      "  55618/150000: episode: 596, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -74.322, mean reward: -1.018 [-100.000,  6.137], mean action: 1.466 [0.000, 3.000],  loss: 4.992118, mae: 34.210089, mean_q: 19.891237, mean_eps: 0.666514\n",
      "  55703/150000: episode: 597, duration: 0.576s, episode steps:  85, steps per second: 148, episode reward: -58.384, mean reward: -0.687 [-100.000,  6.992], mean action: 1.424 [0.000, 3.000],  loss: 4.847952, mae: 34.122183, mean_q: 18.371074, mean_eps: 0.666040\n",
      "  55808/150000: episode: 598, duration: 0.725s, episode steps: 105, steps per second: 145, episode reward: -114.953, mean reward: -1.095 [-100.000,  8.537], mean action: 1.438 [0.000, 3.000],  loss: 4.878800, mae: 34.133659, mean_q: 18.954863, mean_eps: 0.665470\n",
      "  55950/150000: episode: 599, duration: 0.995s, episode steps: 142, steps per second: 143, episode reward: -50.498, mean reward: -0.356 [-100.000, 13.305], mean action: 1.711 [0.000, 3.000],  loss: 6.507764, mae: 33.933593, mean_q: 20.394965, mean_eps: 0.664729\n",
      "  56061/150000: episode: 600, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -80.389, mean reward: -0.724 [-100.000,  9.853], mean action: 1.703 [0.000, 3.000],  loss: 6.003672, mae: 34.287419, mean_q: 20.736802, mean_eps: 0.663970\n",
      "  56162/150000: episode: 601, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -97.929, mean reward: -0.970 [-100.000,  8.623], mean action: 1.535 [0.000, 3.000],  loss: 5.892475, mae: 34.088233, mean_q: 20.282081, mean_eps: 0.663334\n",
      "  56315/150000: episode: 602, duration: 1.033s, episode steps: 153, steps per second: 148, episode reward: -93.543, mean reward: -0.611 [-100.000,  6.332], mean action: 1.621 [0.000, 3.000],  loss: 3.740415, mae: 34.368440, mean_q: 20.626225, mean_eps: 0.662572\n",
      "  56385/150000: episode: 603, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: -52.840, mean reward: -0.755 [-100.000,  7.536], mean action: 1.529 [0.000, 3.000],  loss: 6.656526, mae: 34.377232, mean_q: 19.818590, mean_eps: 0.661903\n",
      "  56537/150000: episode: 604, duration: 1.057s, episode steps: 152, steps per second: 144, episode reward: -86.210, mean reward: -0.567 [-100.000,  6.081], mean action: 1.664 [0.000, 3.000],  loss: 8.395933, mae: 34.420101, mean_q: 19.376641, mean_eps: 0.661237\n",
      "  56610/150000: episode: 605, duration: 0.507s, episode steps:  73, steps per second: 144, episode reward: -40.980, mean reward: -0.561 [-100.000,  8.202], mean action: 1.562 [0.000, 3.000],  loss: 7.713591, mae: 34.500465, mean_q: 19.999662, mean_eps: 0.660562\n",
      "  56691/150000: episode: 606, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -63.248, mean reward: -0.781 [-100.000,  6.910], mean action: 1.543 [0.000, 3.000],  loss: 4.286816, mae: 34.489568, mean_q: 19.924422, mean_eps: 0.660100\n",
      "  56792/150000: episode: 607, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: -54.695, mean reward: -0.542 [-100.000, 14.834], mean action: 1.614 [0.000, 3.000],  loss: 6.599880, mae: 34.263492, mean_q: 19.333870, mean_eps: 0.659554\n",
      "  56948/150000: episode: 608, duration: 1.104s, episode steps: 156, steps per second: 141, episode reward: -47.830, mean reward: -0.307 [-100.000,  8.410], mean action: 1.833 [0.000, 3.000],  loss: 5.417301, mae: 34.299618, mean_q: 20.695863, mean_eps: 0.658783\n",
      "  57041/150000: episode: 609, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: 16.083, mean reward:  0.173 [-100.000, 12.858], mean action: 1.602 [0.000, 3.000],  loss: 4.967083, mae: 34.201913, mean_q: 20.321308, mean_eps: 0.658036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  57141/150000: episode: 610, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: -67.699, mean reward: -0.677 [-100.000, 22.509], mean action: 1.650 [0.000, 3.000],  loss: 6.706931, mae: 34.437203, mean_q: 18.912294, mean_eps: 0.657457\n",
      "  57291/150000: episode: 611, duration: 1.002s, episode steps: 150, steps per second: 150, episode reward: -240.447, mean reward: -1.603 [-100.000, 25.560], mean action: 1.447 [0.000, 3.000],  loss: 5.803976, mae: 33.859607, mean_q: 19.865143, mean_eps: 0.656707\n",
      "  57428/150000: episode: 612, duration: 0.982s, episode steps: 137, steps per second: 140, episode reward: -91.274, mean reward: -0.666 [-100.000, 10.310], mean action: 1.708 [0.000, 3.000],  loss: 7.221682, mae: 34.414675, mean_q: 19.608450, mean_eps: 0.655846\n",
      "  57527/150000: episode: 613, duration: 0.670s, episode steps:  99, steps per second: 148, episode reward: -101.215, mean reward: -1.022 [-100.000,  9.722], mean action: 1.576 [0.000, 3.000],  loss: 7.453058, mae: 34.700111, mean_q: 19.973006, mean_eps: 0.655138\n",
      "  57658/150000: episode: 614, duration: 0.884s, episode steps: 131, steps per second: 148, episode reward: -28.550, mean reward: -0.218 [-100.000, 12.534], mean action: 1.702 [0.000, 3.000],  loss: 6.521541, mae: 34.314649, mean_q: 18.228117, mean_eps: 0.654448\n",
      "  57773/150000: episode: 615, duration: 0.810s, episode steps: 115, steps per second: 142, episode reward: -55.714, mean reward: -0.484 [-100.000,  6.808], mean action: 1.383 [0.000, 3.000],  loss: 4.006840, mae: 34.164292, mean_q: 19.731641, mean_eps: 0.653710\n",
      "  57874/150000: episode: 616, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -37.194, mean reward: -0.368 [-100.000,  9.831], mean action: 1.891 [0.000, 3.000],  loss: 6.356189, mae: 34.619933, mean_q: 19.930195, mean_eps: 0.653062\n",
      "  57991/150000: episode: 617, duration: 0.810s, episode steps: 117, steps per second: 144, episode reward: -33.348, mean reward: -0.285 [-100.000, 11.520], mean action: 1.547 [0.000, 3.000],  loss: 4.194417, mae: 34.145467, mean_q: 19.129543, mean_eps: 0.652408\n",
      "  58076/150000: episode: 618, duration: 0.603s, episode steps:  85, steps per second: 141, episode reward: -52.804, mean reward: -0.621 [-100.000, 10.487], mean action: 1.776 [0.000, 3.000],  loss: 7.748186, mae: 34.435793, mean_q: 18.742906, mean_eps: 0.651802\n",
      "  58155/150000: episode: 619, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -32.135, mean reward: -0.407 [-100.000,  6.477], mean action: 1.835 [0.000, 3.000],  loss: 6.307708, mae: 34.571487, mean_q: 19.375634, mean_eps: 0.651310\n",
      "  58229/150000: episode: 620, duration: 0.497s, episode steps:  74, steps per second: 149, episode reward: -56.940, mean reward: -0.769 [-100.000,  7.810], mean action: 1.676 [0.000, 3.000],  loss: 4.413882, mae: 33.967141, mean_q: 18.377313, mean_eps: 0.650851\n",
      "  58320/150000: episode: 621, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -42.825, mean reward: -0.471 [-100.000,  6.705], mean action: 1.692 [0.000, 3.000],  loss: 7.273078, mae: 34.297149, mean_q: 19.613203, mean_eps: 0.650356\n",
      "  58406/150000: episode: 622, duration: 0.617s, episode steps:  86, steps per second: 139, episode reward: -90.552, mean reward: -1.053 [-100.000,  7.533], mean action: 1.302 [0.000, 3.000],  loss: 6.621874, mae: 34.317586, mean_q: 20.609198, mean_eps: 0.649825\n",
      "  58507/150000: episode: 623, duration: 0.806s, episode steps: 101, steps per second: 125, episode reward: -88.877, mean reward: -0.880 [-100.000, 11.601], mean action: 1.673 [0.000, 3.000],  loss: 5.448025, mae: 34.499647, mean_q: 19.959995, mean_eps: 0.649264\n",
      "  58586/150000: episode: 624, duration: 0.609s, episode steps:  79, steps per second: 130, episode reward: -50.260, mean reward: -0.636 [-100.000,  6.768], mean action: 1.468 [0.000, 3.000],  loss: 6.495329, mae: 34.185804, mean_q: 20.121817, mean_eps: 0.648724\n",
      "  58677/150000: episode: 625, duration: 0.658s, episode steps:  91, steps per second: 138, episode reward: -1.037, mean reward: -0.011 [-100.000, 18.506], mean action: 1.527 [0.000, 3.000],  loss: 6.818342, mae: 34.354140, mean_q: 18.896554, mean_eps: 0.648214\n",
      "  58811/150000: episode: 626, duration: 0.917s, episode steps: 134, steps per second: 146, episode reward: -85.482, mean reward: -0.638 [-100.000,  7.133], mean action: 1.604 [0.000, 3.000],  loss: 7.444105, mae: 33.920972, mean_q: 19.418286, mean_eps: 0.647539\n",
      "  58879/150000: episode: 627, duration: 0.456s, episode steps:  68, steps per second: 149, episode reward: -173.726, mean reward: -2.555 [-100.000,  8.649], mean action: 1.603 [0.000, 3.000],  loss: 6.850133, mae: 34.558313, mean_q: 18.691424, mean_eps: 0.646933\n",
      "  58975/150000: episode: 628, duration: 0.683s, episode steps:  96, steps per second: 140, episode reward: -13.334, mean reward: -0.139 [-100.000, 19.502], mean action: 1.656 [0.000, 3.000],  loss: 7.979210, mae: 34.178937, mean_q: 19.434015, mean_eps: 0.646441\n",
      "  59085/150000: episode: 629, duration: 0.753s, episode steps: 110, steps per second: 146, episode reward: -79.931, mean reward: -0.727 [-100.000,  9.136], mean action: 1.655 [0.000, 3.000],  loss: 11.908951, mae: 34.739056, mean_q: 19.121359, mean_eps: 0.645823\n",
      "  59151/150000: episode: 630, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -67.188, mean reward: -1.018 [-100.000, 10.507], mean action: 1.530 [0.000, 3.000],  loss: 8.424400, mae: 34.529596, mean_q: 19.826889, mean_eps: 0.645295\n",
      "  59227/150000: episode: 631, duration: 0.539s, episode steps:  76, steps per second: 141, episode reward: -64.434, mean reward: -0.848 [-100.000,  8.975], mean action: 1.368 [0.000, 3.000],  loss: 7.921813, mae: 34.579163, mean_q: 20.017507, mean_eps: 0.644869\n",
      "  59321/150000: episode: 632, duration: 0.647s, episode steps:  94, steps per second: 145, episode reward: -112.295, mean reward: -1.195 [-100.000,  8.361], mean action: 1.819 [0.000, 3.000],  loss: 6.686746, mae: 34.203472, mean_q: 20.085188, mean_eps: 0.644359\n",
      "  59434/150000: episode: 633, duration: 0.767s, episode steps: 113, steps per second: 147, episode reward: -49.406, mean reward: -0.437 [-100.000, 11.000], mean action: 1.558 [0.000, 3.000],  loss: 11.859434, mae: 34.354254, mean_q: 20.318120, mean_eps: 0.643738\n",
      "  59521/150000: episode: 634, duration: 0.601s, episode steps:  87, steps per second: 145, episode reward: -30.780, mean reward: -0.354 [-100.000, 20.373], mean action: 1.609 [0.000, 3.000],  loss: 9.891578, mae: 33.991891, mean_q: 19.772446, mean_eps: 0.643138\n",
      "  59596/150000: episode: 635, duration: 0.532s, episode steps:  75, steps per second: 141, episode reward: -48.874, mean reward: -0.652 [-100.000,  9.737], mean action: 1.600 [0.000, 3.000],  loss: 6.478784, mae: 34.387929, mean_q: 20.027574, mean_eps: 0.642652\n",
      "  59693/150000: episode: 636, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -42.993, mean reward: -0.443 [-100.000, 10.902], mean action: 1.742 [0.000, 3.000],  loss: 8.058281, mae: 34.152354, mean_q: 21.396451, mean_eps: 0.642136\n",
      "  59788/150000: episode: 637, duration: 0.639s, episode steps:  95, steps per second: 149, episode reward: -154.011, mean reward: -1.621 [-100.000, 20.822], mean action: 1.611 [0.000, 3.000],  loss: 6.250321, mae: 34.283777, mean_q: 18.873258, mean_eps: 0.641560\n",
      "  59892/150000: episode: 638, duration: 0.738s, episode steps: 104, steps per second: 141, episode reward: -82.380, mean reward: -0.792 [-100.000, 10.619], mean action: 1.452 [0.000, 3.000],  loss: 8.571310, mae: 34.810650, mean_q: 18.951843, mean_eps: 0.640963\n",
      "  60024/150000: episode: 639, duration: 0.894s, episode steps: 132, steps per second: 148, episode reward: -107.509, mean reward: -0.814 [-100.000,  7.262], mean action: 1.788 [0.000, 3.000],  loss: 5.441905, mae: 34.136262, mean_q: 18.570224, mean_eps: 0.640255\n",
      "  60135/150000: episode: 640, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward: -12.524, mean reward: -0.113 [-100.000, 21.809], mean action: 1.577 [0.000, 3.000],  loss: 9.387061, mae: 34.617944, mean_q: 20.222678, mean_eps: 0.639526\n",
      "  60221/150000: episode: 641, duration: 0.641s, episode steps:  86, steps per second: 134, episode reward: -49.843, mean reward: -0.580 [-100.000,  9.449], mean action: 1.488 [0.000, 3.000],  loss: 6.346303, mae: 34.099555, mean_q: 19.907237, mean_eps: 0.638935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  60340/150000: episode: 642, duration: 0.803s, episode steps: 119, steps per second: 148, episode reward: -150.849, mean reward: -1.268 [-100.000, 11.264], mean action: 1.840 [0.000, 3.000],  loss: 10.729680, mae: 34.247815, mean_q: 19.123897, mean_eps: 0.638320\n",
      "  60469/150000: episode: 643, duration: 0.873s, episode steps: 129, steps per second: 148, episode reward: -79.156, mean reward: -0.614 [-100.000,  6.668], mean action: 1.620 [0.000, 3.000],  loss: 8.032526, mae: 34.591223, mean_q: 18.909427, mean_eps: 0.637576\n",
      "  60579/150000: episode: 644, duration: 0.790s, episode steps: 110, steps per second: 139, episode reward: -130.743, mean reward: -1.189 [-100.000,  3.967], mean action: 1.527 [0.000, 3.000],  loss: 4.421055, mae: 34.144685, mean_q: 18.508574, mean_eps: 0.636859\n",
      "  60700/150000: episode: 645, duration: 0.814s, episode steps: 121, steps per second: 149, episode reward: 30.014, mean reward:  0.248 [-100.000, 59.733], mean action: 1.612 [0.000, 3.000],  loss: 9.360707, mae: 34.422966, mean_q: 20.083960, mean_eps: 0.636166\n",
      "  60809/150000: episode: 646, duration: 0.771s, episode steps: 109, steps per second: 141, episode reward: -79.332, mean reward: -0.728 [-100.000,  7.260], mean action: 1.606 [0.000, 3.000],  loss: 6.643009, mae: 34.160375, mean_q: 19.197716, mean_eps: 0.635476\n",
      "  60921/150000: episode: 647, duration: 0.778s, episode steps: 112, steps per second: 144, episode reward: -34.226, mean reward: -0.306 [-100.000, 13.322], mean action: 1.545 [0.000, 3.000],  loss: 8.205706, mae: 34.579063, mean_q: 18.646245, mean_eps: 0.634813\n",
      "  61006/150000: episode: 648, duration: 0.573s, episode steps:  85, steps per second: 148, episode reward: -47.422, mean reward: -0.558 [-100.000,  7.385], mean action: 1.671 [0.000, 3.000],  loss: 5.772511, mae: 34.566939, mean_q: 18.032526, mean_eps: 0.634222\n",
      "  61093/150000: episode: 649, duration: 0.628s, episode steps:  87, steps per second: 139, episode reward: -64.654, mean reward: -0.743 [-100.000, 13.786], mean action: 1.770 [0.000, 3.000],  loss: 7.072076, mae: 33.953010, mean_q: 18.574381, mean_eps: 0.633706\n",
      "  61199/150000: episode: 650, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: -87.074, mean reward: -0.821 [-100.000, 21.084], mean action: 1.462 [0.000, 3.000],  loss: 6.187842, mae: 33.848505, mean_q: 18.106783, mean_eps: 0.633127\n",
      "  61294/150000: episode: 651, duration: 0.633s, episode steps:  95, steps per second: 150, episode reward: 43.167, mean reward:  0.454 [-100.000, 15.974], mean action: 1.600 [0.000, 3.000],  loss: 6.339229, mae: 33.857428, mean_q: 18.686450, mean_eps: 0.632524\n",
      "  61371/150000: episode: 652, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -60.521, mean reward: -0.786 [-100.000,  9.755], mean action: 1.571 [0.000, 3.000],  loss: 4.897202, mae: 33.893928, mean_q: 18.523928, mean_eps: 0.632008\n",
      "  61470/150000: episode: 653, duration: 0.752s, episode steps:  99, steps per second: 132, episode reward: -65.730, mean reward: -0.664 [-100.000, 10.697], mean action: 1.626 [0.000, 3.000],  loss: 9.479945, mae: 34.163269, mean_q: 17.642646, mean_eps: 0.631480\n",
      "  61560/150000: episode: 654, duration: 0.621s, episode steps:  90, steps per second: 145, episode reward: -89.360, mean reward: -0.993 [-100.000, 17.571], mean action: 1.767 [0.000, 3.000],  loss: 7.094330, mae: 34.040008, mean_q: 18.442110, mean_eps: 0.630913\n",
      "  61667/150000: episode: 655, duration: 0.733s, episode steps: 107, steps per second: 146, episode reward: -54.889, mean reward: -0.513 [-100.000, 23.534], mean action: 1.636 [0.000, 3.000],  loss: 7.122644, mae: 33.670268, mean_q: 18.785266, mean_eps: 0.630322\n",
      "  61756/150000: episode: 656, duration: 0.663s, episode steps:  89, steps per second: 134, episode reward: -100.561, mean reward: -1.130 [-100.000,  7.002], mean action: 1.674 [0.000, 3.000],  loss: 12.802180, mae: 34.129701, mean_q: 19.404098, mean_eps: 0.629734\n",
      "  61868/150000: episode: 657, duration: 0.777s, episode steps: 112, steps per second: 144, episode reward: 10.873, mean reward:  0.097 [-100.000, 17.308], mean action: 1.741 [0.000, 3.000],  loss: 8.778694, mae: 34.027816, mean_q: 19.840213, mean_eps: 0.629131\n",
      "  61955/150000: episode: 658, duration: 0.582s, episode steps:  87, steps per second: 149, episode reward: -68.927, mean reward: -0.792 [-100.000, 22.911], mean action: 1.517 [0.000, 3.000],  loss: 4.882060, mae: 33.911149, mean_q: 18.471509, mean_eps: 0.628534\n",
      "  62045/150000: episode: 659, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -94.006, mean reward: -1.045 [-100.000, 11.681], mean action: 1.622 [0.000, 3.000],  loss: 6.172145, mae: 34.317827, mean_q: 17.787802, mean_eps: 0.628003\n",
      "  62105/150000: episode: 660, duration: 0.415s, episode steps:  60, steps per second: 145, episode reward: -81.733, mean reward: -1.362 [-100.000,  3.955], mean action: 1.683 [0.000, 3.000],  loss: 5.695239, mae: 34.338385, mean_q: 18.104178, mean_eps: 0.627553\n",
      "  62262/150000: episode: 661, duration: 1.071s, episode steps: 157, steps per second: 147, episode reward: -95.314, mean reward: -0.607 [-100.000,  8.262], mean action: 1.720 [0.000, 3.000],  loss: 6.383680, mae: 34.574796, mean_q: 18.127456, mean_eps: 0.626902\n",
      "  62325/150000: episode: 662, duration: 0.451s, episode steps:  63, steps per second: 140, episode reward: -60.855, mean reward: -0.966 [-100.000, 10.666], mean action: 1.683 [0.000, 3.000],  loss: 4.342902, mae: 34.264159, mean_q: 20.070298, mean_eps: 0.626242\n",
      "  62434/150000: episode: 663, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: -26.388, mean reward: -0.242 [-100.000, 18.037], mean action: 1.642 [0.000, 3.000],  loss: 10.240203, mae: 34.549676, mean_q: 19.080561, mean_eps: 0.625726\n",
      "  62541/150000: episode: 664, duration: 0.920s, episode steps: 107, steps per second: 116, episode reward: 49.871, mean reward:  0.466 [-100.000, 14.605], mean action: 1.654 [0.000, 3.000],  loss: 6.164603, mae: 34.748319, mean_q: 19.258470, mean_eps: 0.625078\n",
      "  62657/150000: episode: 665, duration: 0.906s, episode steps: 116, steps per second: 128, episode reward: -84.663, mean reward: -0.730 [-100.000, 10.819], mean action: 1.647 [0.000, 3.000],  loss: 3.439838, mae: 34.592868, mean_q: 19.195699, mean_eps: 0.624409\n",
      "  62726/150000: episode: 666, duration: 0.533s, episode steps:  69, steps per second: 129, episode reward: -64.136, mean reward: -0.930 [-100.000,  8.416], mean action: 1.580 [0.000, 3.000],  loss: 4.681256, mae: 34.509479, mean_q: 19.531085, mean_eps: 0.623854\n",
      "  62847/150000: episode: 667, duration: 0.892s, episode steps: 121, steps per second: 136, episode reward: -83.052, mean reward: -0.686 [-100.000,  6.670], mean action: 1.504 [0.000, 3.000],  loss: 7.575097, mae: 34.695170, mean_q: 19.102174, mean_eps: 0.623284\n",
      "  62924/150000: episode: 668, duration: 0.698s, episode steps:  77, steps per second: 110, episode reward: -23.271, mean reward: -0.302 [-100.000, 22.360], mean action: 1.727 [0.000, 3.000],  loss: 8.924383, mae: 34.915149, mean_q: 18.249594, mean_eps: 0.622690\n",
      "  63030/150000: episode: 669, duration: 0.837s, episode steps: 106, steps per second: 127, episode reward: -79.674, mean reward: -0.752 [-100.000, 13.484], mean action: 1.698 [0.000, 3.000],  loss: 7.917097, mae: 34.739991, mean_q: 19.662564, mean_eps: 0.622141\n",
      "  63145/150000: episode: 670, duration: 0.798s, episode steps: 115, steps per second: 144, episode reward: -74.307, mean reward: -0.646 [-100.000,  6.395], mean action: 1.661 [0.000, 3.000],  loss: 5.325526, mae: 35.113496, mean_q: 19.121845, mean_eps: 0.621478\n",
      "  63227/150000: episode: 671, duration: 0.595s, episode steps:  82, steps per second: 138, episode reward: -23.556, mean reward: -0.287 [-100.000, 13.574], mean action: 1.671 [0.000, 3.000],  loss: 7.800311, mae: 34.524424, mean_q: 19.983227, mean_eps: 0.620887\n",
      "  63318/150000: episode: 672, duration: 0.621s, episode steps:  91, steps per second: 147, episode reward: -12.475, mean reward: -0.137 [-100.000, 12.354], mean action: 1.758 [0.000, 3.000],  loss: 4.127451, mae: 35.087238, mean_q: 19.667471, mean_eps: 0.620368\n",
      "  63427/150000: episode: 673, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -68.215, mean reward: -0.626 [-100.000, 13.209], mean action: 1.560 [0.000, 3.000],  loss: 7.997273, mae: 34.812765, mean_q: 18.215681, mean_eps: 0.619768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  63550/150000: episode: 674, duration: 0.868s, episode steps: 123, steps per second: 142, episode reward: -11.828, mean reward: -0.096 [-100.000, 16.708], mean action: 1.659 [0.000, 3.000],  loss: 4.127840, mae: 34.523863, mean_q: 18.345703, mean_eps: 0.619072\n",
      "  63639/150000: episode: 675, duration: 0.609s, episode steps:  89, steps per second: 146, episode reward: -41.734, mean reward: -0.469 [-100.000, 13.773], mean action: 1.438 [0.000, 3.000],  loss: 5.959537, mae: 34.586038, mean_q: 20.143349, mean_eps: 0.618436\n",
      "  63730/150000: episode: 676, duration: 0.660s, episode steps:  91, steps per second: 138, episode reward: -67.095, mean reward: -0.737 [-100.000, 10.808], mean action: 1.505 [0.000, 3.000],  loss: 7.065000, mae: 34.901721, mean_q: 19.475058, mean_eps: 0.617896\n",
      "  63809/150000: episode: 677, duration: 0.583s, episode steps:  79, steps per second: 136, episode reward: -94.313, mean reward: -1.194 [-100.000, 13.706], mean action: 1.481 [0.000, 3.000],  loss: 6.720678, mae: 34.670010, mean_q: 19.540962, mean_eps: 0.617386\n",
      "  63920/150000: episode: 678, duration: 0.810s, episode steps: 111, steps per second: 137, episode reward: -66.154, mean reward: -0.596 [-100.000,  7.840], mean action: 1.730 [0.000, 3.000],  loss: 7.122122, mae: 34.839386, mean_q: 19.742557, mean_eps: 0.616816\n",
      "  64031/150000: episode: 679, duration: 0.774s, episode steps: 111, steps per second: 143, episode reward: -43.631, mean reward: -0.393 [-100.000,  7.149], mean action: 1.676 [0.000, 3.000],  loss: 5.005772, mae: 34.886401, mean_q: 18.685553, mean_eps: 0.616150\n",
      "  64104/150000: episode: 680, duration: 0.531s, episode steps:  73, steps per second: 137, episode reward: -45.526, mean reward: -0.624 [-100.000, 20.705], mean action: 1.877 [0.000, 3.000],  loss: 3.267840, mae: 34.808286, mean_q: 18.918084, mean_eps: 0.615598\n",
      "  64189/150000: episode: 681, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -51.224, mean reward: -0.603 [-100.000, 13.419], mean action: 1.529 [0.000, 3.000],  loss: 5.147128, mae: 34.833677, mean_q: 20.052915, mean_eps: 0.615124\n",
      "  64265/150000: episode: 682, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: -135.487, mean reward: -1.783 [-100.000,  8.448], mean action: 1.579 [0.000, 3.000],  loss: 5.655348, mae: 34.905649, mean_q: 19.236667, mean_eps: 0.614641\n",
      "  64395/150000: episode: 683, duration: 0.932s, episode steps: 130, steps per second: 140, episode reward: -73.858, mean reward: -0.568 [-100.000,  9.116], mean action: 1.631 [0.000, 3.000],  loss: 6.832927, mae: 34.640267, mean_q: 18.825338, mean_eps: 0.614023\n",
      "  64487/150000: episode: 684, duration: 0.648s, episode steps:  92, steps per second: 142, episode reward: -17.983, mean reward: -0.195 [-100.000, 22.608], mean action: 1.717 [0.000, 3.000],  loss: 5.138800, mae: 34.701540, mean_q: 18.651879, mean_eps: 0.613357\n",
      "  64611/150000: episode: 685, duration: 0.825s, episode steps: 124, steps per second: 150, episode reward: -42.199, mean reward: -0.340 [-100.000, 21.428], mean action: 1.750 [0.000, 3.000],  loss: 4.953001, mae: 35.068424, mean_q: 20.060553, mean_eps: 0.612709\n",
      "  64735/150000: episode: 686, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: -63.354, mean reward: -0.511 [-100.000, 12.476], mean action: 1.524 [0.000, 3.000],  loss: 10.491777, mae: 34.902869, mean_q: 18.779679, mean_eps: 0.611965\n",
      "  64837/150000: episode: 687, duration: 0.698s, episode steps: 102, steps per second: 146, episode reward: -59.062, mean reward: -0.579 [-100.000,  9.415], mean action: 1.784 [0.000, 3.000],  loss: 9.290318, mae: 35.003243, mean_q: 20.294789, mean_eps: 0.611287\n",
      "  64944/150000: episode: 688, duration: 0.708s, episode steps: 107, steps per second: 151, episode reward: -31.195, mean reward: -0.292 [-100.000, 10.336], mean action: 1.477 [0.000, 3.000],  loss: 4.100169, mae: 35.031604, mean_q: 17.999041, mean_eps: 0.610660\n",
      "  65102/150000: episode: 689, duration: 1.118s, episode steps: 158, steps per second: 141, episode reward: -180.545, mean reward: -1.143 [-100.000, 12.424], mean action: 1.551 [0.000, 3.000],  loss: 4.690164, mae: 35.118007, mean_q: 19.337630, mean_eps: 0.609865\n",
      "  65216/150000: episode: 690, duration: 0.770s, episode steps: 114, steps per second: 148, episode reward: -15.736, mean reward: -0.138 [-100.000, 12.065], mean action: 1.693 [0.000, 3.000],  loss: 4.601220, mae: 35.361693, mean_q: 20.039314, mean_eps: 0.609049\n",
      "  65302/150000: episode: 691, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -92.925, mean reward: -1.081 [-100.000, 11.038], mean action: 1.651 [0.000, 3.000],  loss: 5.724815, mae: 35.136077, mean_q: 19.038065, mean_eps: 0.608449\n",
      "  65380/150000: episode: 692, duration: 0.576s, episode steps:  78, steps per second: 135, episode reward: -85.553, mean reward: -1.097 [-100.000,  9.008], mean action: 1.718 [0.000, 3.000],  loss: 4.855089, mae: 35.064757, mean_q: 18.272292, mean_eps: 0.607957\n",
      "  65474/150000: episode: 693, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -98.244, mean reward: -1.045 [-100.000,  5.999], mean action: 1.553 [0.000, 3.000],  loss: 4.031968, mae: 34.919962, mean_q: 18.799691, mean_eps: 0.607441\n",
      "  65558/150000: episode: 694, duration: 0.559s, episode steps:  84, steps per second: 150, episode reward: 14.187, mean reward:  0.169 [-100.000, 14.521], mean action: 1.607 [0.000, 3.000],  loss: 7.592201, mae: 35.171119, mean_q: 19.304616, mean_eps: 0.606907\n",
      "  65636/150000: episode: 695, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -73.109, mean reward: -0.937 [-100.000, 12.845], mean action: 1.667 [0.000, 3.000],  loss: 4.147592, mae: 34.785584, mean_q: 19.045912, mean_eps: 0.606421\n",
      "  65737/150000: episode: 696, duration: 0.755s, episode steps: 101, steps per second: 134, episode reward: -81.169, mean reward: -0.804 [-100.000, 12.633], mean action: 1.683 [0.000, 3.000],  loss: 7.361699, mae: 35.522094, mean_q: 19.588776, mean_eps: 0.605884\n",
      "  65821/150000: episode: 697, duration: 0.573s, episode steps:  84, steps per second: 147, episode reward: -75.387, mean reward: -0.897 [-100.000, 12.325], mean action: 1.738 [0.000, 3.000],  loss: 7.806322, mae: 35.140761, mean_q: 19.573931, mean_eps: 0.605329\n",
      "  65952/150000: episode: 698, duration: 0.889s, episode steps: 131, steps per second: 147, episode reward: -38.246, mean reward: -0.292 [-100.000,  7.038], mean action: 1.687 [0.000, 3.000],  loss: 4.180930, mae: 35.298612, mean_q: 19.370725, mean_eps: 0.604684\n",
      "  66052/150000: episode: 699, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: -47.295, mean reward: -0.473 [-100.000,  7.144], mean action: 1.610 [0.000, 3.000],  loss: 4.890831, mae: 35.090493, mean_q: 19.362786, mean_eps: 0.603991\n",
      "  66160/150000: episode: 700, duration: 0.757s, episode steps: 108, steps per second: 143, episode reward: -126.944, mean reward: -1.175 [-100.000,  6.156], mean action: 1.694 [0.000, 3.000],  loss: 4.452780, mae: 35.492556, mean_q: 19.183860, mean_eps: 0.603367\n",
      "  66263/150000: episode: 701, duration: 0.732s, episode steps: 103, steps per second: 141, episode reward: -47.740, mean reward: -0.463 [-100.000, 13.416], mean action: 1.806 [0.000, 3.000],  loss: 5.543384, mae: 35.484907, mean_q: 20.449976, mean_eps: 0.602734\n",
      "  66351/150000: episode: 702, duration: 0.640s, episode steps:  88, steps per second: 138, episode reward: -62.396, mean reward: -0.709 [-100.000, 10.704], mean action: 1.443 [0.000, 3.000],  loss: 7.719386, mae: 35.825962, mean_q: 20.574966, mean_eps: 0.602161\n",
      "  66445/150000: episode: 703, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -12.260, mean reward: -0.130 [-100.000, 14.538], mean action: 1.617 [0.000, 3.000],  loss: 3.192311, mae: 35.594746, mean_q: 19.843708, mean_eps: 0.601615\n",
      "  66528/150000: episode: 704, duration: 0.568s, episode steps:  83, steps per second: 146, episode reward: -13.364, mean reward: -0.161 [-100.000, 21.055], mean action: 1.663 [0.000, 3.000],  loss: 5.641013, mae: 35.323754, mean_q: 18.724342, mean_eps: 0.601084\n",
      "  66769/150000: episode: 705, duration: 1.682s, episode steps: 241, steps per second: 143, episode reward: -198.973, mean reward: -0.826 [-100.000,  9.376], mean action: 1.568 [0.000, 3.000],  loss: 4.603144, mae: 35.656179, mean_q: 18.993503, mean_eps: 0.600112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  66915/150000: episode: 706, duration: 0.997s, episode steps: 146, steps per second: 147, episode reward: -25.405, mean reward: -0.174 [-100.000, 22.052], mean action: 1.644 [0.000, 3.000],  loss: 3.563983, mae: 35.237519, mean_q: 19.668299, mean_eps: 0.598951\n",
      "  67050/150000: episode: 707, duration: 0.926s, episode steps: 135, steps per second: 146, episode reward: -37.293, mean reward: -0.276 [-100.000, 21.379], mean action: 1.681 [0.000, 3.000],  loss: 8.543249, mae: 35.557248, mean_q: 20.605136, mean_eps: 0.598108\n",
      "  67153/150000: episode: 708, duration: 0.709s, episode steps: 103, steps per second: 145, episode reward: -3.719, mean reward: -0.036 [-100.000, 18.073], mean action: 1.757 [0.000, 3.000],  loss: 8.119633, mae: 35.761494, mean_q: 21.876140, mean_eps: 0.597394\n",
      "  67267/150000: episode: 709, duration: 0.821s, episode steps: 114, steps per second: 139, episode reward: -40.720, mean reward: -0.357 [-100.000, 13.240], mean action: 1.632 [0.000, 3.000],  loss: 6.414598, mae: 35.777223, mean_q: 21.215323, mean_eps: 0.596743\n",
      "  67379/150000: episode: 710, duration: 0.756s, episode steps: 112, steps per second: 148, episode reward: -44.461, mean reward: -0.397 [-100.000, 13.264], mean action: 1.634 [0.000, 3.000],  loss: 6.327446, mae: 35.263782, mean_q: 21.864816, mean_eps: 0.596065\n",
      "  67499/150000: episode: 711, duration: 0.821s, episode steps: 120, steps per second: 146, episode reward: -53.517, mean reward: -0.446 [-100.000,  7.270], mean action: 1.525 [0.000, 3.000],  loss: 6.624555, mae: 35.608440, mean_q: 21.890209, mean_eps: 0.595369\n",
      "  67625/150000: episode: 712, duration: 0.884s, episode steps: 126, steps per second: 142, episode reward: -21.592, mean reward: -0.171 [-100.000, 13.385], mean action: 1.667 [0.000, 3.000],  loss: 4.528449, mae: 35.724324, mean_q: 21.785397, mean_eps: 0.594631\n",
      "  67722/150000: episode: 713, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -22.227, mean reward: -0.229 [-100.000, 11.382], mean action: 1.773 [0.000, 3.000],  loss: 7.088929, mae: 35.753658, mean_q: 21.220845, mean_eps: 0.593962\n",
      "  67799/150000: episode: 714, duration: 0.516s, episode steps:  77, steps per second: 149, episode reward: -62.380, mean reward: -0.810 [-100.000, 14.993], mean action: 1.506 [0.000, 3.000],  loss: 6.880829, mae: 35.547575, mean_q: 21.529032, mean_eps: 0.593440\n",
      "  67912/150000: episode: 715, duration: 0.809s, episode steps: 113, steps per second: 140, episode reward: -3.325, mean reward: -0.029 [-100.000, 16.881], mean action: 1.841 [0.000, 3.000],  loss: 7.879380, mae: 35.559071, mean_q: 21.635119, mean_eps: 0.592870\n",
      "  68042/150000: episode: 716, duration: 0.895s, episode steps: 130, steps per second: 145, episode reward: 29.430, mean reward:  0.226 [-100.000, 18.684], mean action: 1.838 [0.000, 3.000],  loss: 5.348825, mae: 35.793380, mean_q: 21.389748, mean_eps: 0.592141\n",
      "  68123/150000: episode: 717, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -27.495, mean reward: -0.339 [-100.000,  8.162], mean action: 1.679 [0.000, 3.000],  loss: 3.039068, mae: 35.543290, mean_q: 21.070532, mean_eps: 0.591508\n",
      "  68231/150000: episode: 718, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: -54.475, mean reward: -0.504 [-100.000, 12.741], mean action: 1.593 [0.000, 3.000],  loss: 7.765725, mae: 36.154503, mean_q: 20.931989, mean_eps: 0.590941\n",
      "  68356/150000: episode: 719, duration: 0.840s, episode steps: 125, steps per second: 149, episode reward: -52.756, mean reward: -0.422 [-100.000,  7.342], mean action: 1.736 [0.000, 3.000],  loss: 3.474166, mae: 35.907056, mean_q: 20.992865, mean_eps: 0.590242\n",
      "  68481/150000: episode: 720, duration: 0.860s, episode steps: 125, steps per second: 145, episode reward: -23.294, mean reward: -0.186 [-100.000,  9.021], mean action: 1.688 [0.000, 3.000],  loss: 7.209289, mae: 36.390743, mean_q: 21.652409, mean_eps: 0.589492\n",
      "  68565/150000: episode: 721, duration: 0.582s, episode steps:  84, steps per second: 144, episode reward: -63.986, mean reward: -0.762 [-100.000,  6.638], mean action: 1.940 [0.000, 3.000],  loss: 7.269158, mae: 36.300079, mean_q: 21.710704, mean_eps: 0.588865\n",
      "  68688/150000: episode: 722, duration: 0.829s, episode steps: 123, steps per second: 148, episode reward: -115.391, mean reward: -0.938 [-100.000, 26.355], mean action: 1.431 [0.000, 3.000],  loss: 5.360419, mae: 36.221335, mean_q: 20.722402, mean_eps: 0.588244\n",
      "  68812/150000: episode: 723, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: -91.554, mean reward: -0.738 [-100.000, 14.514], mean action: 1.661 [0.000, 3.000],  loss: 4.481031, mae: 36.113929, mean_q: 21.851270, mean_eps: 0.587503\n",
      "  68901/150000: episode: 724, duration: 0.619s, episode steps:  89, steps per second: 144, episode reward: -98.460, mean reward: -1.106 [-100.000, 10.311], mean action: 1.596 [0.000, 3.000],  loss: 4.636523, mae: 36.484809, mean_q: 21.410015, mean_eps: 0.586864\n",
      "  69030/150000: episode: 725, duration: 0.865s, episode steps: 129, steps per second: 149, episode reward: -241.389, mean reward: -1.871 [-100.000, 62.455], mean action: 1.682 [0.000, 3.000],  loss: 3.660970, mae: 36.410665, mean_q: 22.362284, mean_eps: 0.586210\n",
      "  69126/150000: episode: 726, duration: 0.684s, episode steps:  96, steps per second: 140, episode reward: -43.025, mean reward: -0.448 [-100.000, 11.077], mean action: 1.562 [0.000, 3.000],  loss: 3.514967, mae: 36.318483, mean_q: 21.351330, mean_eps: 0.585535\n",
      "  69230/150000: episode: 727, duration: 0.710s, episode steps: 104, steps per second: 146, episode reward: -43.769, mean reward: -0.421 [-100.000, 13.266], mean action: 1.683 [0.000, 3.000],  loss: 4.286851, mae: 36.195783, mean_q: 20.732494, mean_eps: 0.584935\n",
      "  69309/150000: episode: 728, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -17.824, mean reward: -0.226 [-100.000, 14.378], mean action: 1.772 [0.000, 3.000],  loss: 4.328065, mae: 36.482120, mean_q: 22.088511, mean_eps: 0.584386\n",
      "  69384/150000: episode: 729, duration: 0.536s, episode steps:  75, steps per second: 140, episode reward: -51.995, mean reward: -0.693 [-100.000, 10.833], mean action: 1.853 [0.000, 3.000],  loss: 5.349059, mae: 36.504904, mean_q: 20.961572, mean_eps: 0.583924\n",
      "  69526/150000: episode: 730, duration: 1.003s, episode steps: 142, steps per second: 142, episode reward: -28.501, mean reward: -0.201 [-100.000, 12.291], mean action: 1.479 [0.000, 3.000],  loss: 2.773069, mae: 36.195601, mean_q: 21.869101, mean_eps: 0.583273\n",
      "  69613/150000: episode: 731, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -72.872, mean reward: -0.838 [-100.000,  9.567], mean action: 1.713 [0.000, 3.000],  loss: 5.023647, mae: 36.802401, mean_q: 22.586802, mean_eps: 0.582586\n",
      "  69747/150000: episode: 732, duration: 0.946s, episode steps: 134, steps per second: 142, episode reward: -53.980, mean reward: -0.403 [-100.000, 14.334], mean action: 1.731 [0.000, 3.000],  loss: 5.308601, mae: 36.473396, mean_q: 22.440795, mean_eps: 0.581923\n",
      "  69858/150000: episode: 733, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -70.189, mean reward: -0.632 [-100.000,  6.519], mean action: 1.739 [0.000, 3.000],  loss: 4.205894, mae: 36.382450, mean_q: 22.363845, mean_eps: 0.581188\n",
      "  69944/150000: episode: 734, duration: 0.585s, episode steps:  86, steps per second: 147, episode reward: -77.620, mean reward: -0.903 [-100.000,  9.589], mean action: 1.640 [0.000, 3.000],  loss: 3.249365, mae: 36.490816, mean_q: 21.559542, mean_eps: 0.580597\n",
      "  70021/150000: episode: 735, duration: 0.542s, episode steps:  77, steps per second: 142, episode reward: -60.293, mean reward: -0.783 [-100.000,  8.044], mean action: 1.247 [0.000, 3.000],  loss: 5.305318, mae: 36.617161, mean_q: 22.022612, mean_eps: 0.580108\n",
      "  70116/150000: episode: 736, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -48.717, mean reward: -0.513 [-100.000, 14.350], mean action: 1.526 [0.000, 3.000],  loss: 5.813395, mae: 36.661232, mean_q: 22.499155, mean_eps: 0.579592\n",
      "  70199/150000: episode: 737, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -14.132, mean reward: -0.170 [-100.000,  8.113], mean action: 1.723 [0.000, 3.000],  loss: 8.442727, mae: 37.149132, mean_q: 20.778706, mean_eps: 0.579058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  70352/150000: episode: 738, duration: 1.108s, episode steps: 153, steps per second: 138, episode reward: -49.272, mean reward: -0.322 [-100.000,  9.479], mean action: 1.569 [0.000, 3.000],  loss: 6.111862, mae: 36.745453, mean_q: 22.146757, mean_eps: 0.578350\n",
      "  70424/150000: episode: 739, duration: 0.490s, episode steps:  72, steps per second: 147, episode reward: -45.653, mean reward: -0.634 [-100.000, 16.357], mean action: 1.792 [0.000, 3.000],  loss: 3.565128, mae: 36.613608, mean_q: 22.023906, mean_eps: 0.577675\n",
      "  70573/150000: episode: 740, duration: 0.994s, episode steps: 149, steps per second: 150, episode reward: -13.732, mean reward: -0.092 [-100.000, 14.822], mean action: 1.799 [0.000, 3.000],  loss: 4.018649, mae: 36.838660, mean_q: 21.461408, mean_eps: 0.577012\n",
      "  70703/150000: episode: 741, duration: 0.912s, episode steps: 130, steps per second: 143, episode reward: -19.721, mean reward: -0.152 [-100.000, 17.865], mean action: 1.577 [0.000, 3.000],  loss: 5.425392, mae: 36.523182, mean_q: 22.308418, mean_eps: 0.576175\n",
      "  70812/150000: episode: 742, duration: 0.747s, episode steps: 109, steps per second: 146, episode reward: -36.287, mean reward: -0.333 [-100.000, 18.094], mean action: 1.853 [0.000, 3.000],  loss: 4.352991, mae: 36.905960, mean_q: 21.843466, mean_eps: 0.575458\n",
      "  70910/150000: episode: 743, duration: 0.656s, episode steps:  98, steps per second: 149, episode reward: -4.719, mean reward: -0.048 [-100.000, 15.403], mean action: 1.684 [0.000, 3.000],  loss: 4.824387, mae: 36.396665, mean_q: 21.940711, mean_eps: 0.574837\n",
      "  71013/150000: episode: 744, duration: 0.747s, episode steps: 103, steps per second: 138, episode reward:  5.074, mean reward:  0.049 [-100.000, 17.726], mean action: 1.796 [0.000, 3.000],  loss: 8.454351, mae: 36.888464, mean_q: 22.778310, mean_eps: 0.574234\n",
      "  71162/150000: episode: 745, duration: 1.008s, episode steps: 149, steps per second: 148, episode reward: -16.755, mean reward: -0.112 [-100.000, 20.087], mean action: 1.785 [0.000, 3.000],  loss: 4.682479, mae: 36.139233, mean_q: 21.967696, mean_eps: 0.573478\n",
      "  71268/150000: episode: 746, duration: 0.743s, episode steps: 106, steps per second: 143, episode reward: -52.491, mean reward: -0.495 [-100.000,  6.740], mean action: 1.443 [0.000, 3.000],  loss: 3.917749, mae: 36.303776, mean_q: 21.759268, mean_eps: 0.572713\n",
      "  71359/150000: episode: 747, duration: 0.633s, episode steps:  91, steps per second: 144, episode reward: -41.060, mean reward: -0.451 [-100.000, 11.965], mean action: 1.659 [0.000, 3.000],  loss: 4.324324, mae: 35.676967, mean_q: 21.524586, mean_eps: 0.572122\n",
      "  71440/150000: episode: 748, duration: 0.694s, episode steps:  81, steps per second: 117, episode reward: -51.682, mean reward: -0.638 [-100.000, 10.022], mean action: 1.630 [0.000, 3.000],  loss: 8.389355, mae: 36.274605, mean_q: 21.593559, mean_eps: 0.571606\n",
      "  71553/150000: episode: 749, duration: 1.085s, episode steps: 113, steps per second: 104, episode reward: -43.887, mean reward: -0.388 [-100.000, 20.107], mean action: 1.646 [0.000, 3.000],  loss: 5.890268, mae: 35.958462, mean_q: 21.161815, mean_eps: 0.571024\n",
      "  71647/150000: episode: 750, duration: 0.800s, episode steps:  94, steps per second: 118, episode reward: 17.106, mean reward:  0.182 [-100.000, 23.677], mean action: 1.734 [0.000, 3.000],  loss: 3.934232, mae: 35.806577, mean_q: 20.039732, mean_eps: 0.570403\n",
      "  71740/150000: episode: 751, duration: 0.785s, episode steps:  93, steps per second: 118, episode reward: -58.757, mean reward: -0.632 [-100.000,  9.860], mean action: 1.634 [0.000, 3.000],  loss: 4.066548, mae: 35.871122, mean_q: 20.594357, mean_eps: 0.569842\n",
      "  71821/150000: episode: 752, duration: 0.708s, episode steps:  81, steps per second: 114, episode reward: -59.327, mean reward: -0.732 [-100.000, 16.359], mean action: 1.790 [0.000, 3.000],  loss: 5.045706, mae: 35.865047, mean_q: 20.771824, mean_eps: 0.569320\n",
      "  71910/150000: episode: 753, duration: 0.686s, episode steps:  89, steps per second: 130, episode reward: -43.078, mean reward: -0.484 [-100.000, 17.229], mean action: 1.708 [0.000, 3.000],  loss: 8.150986, mae: 36.285751, mean_q: 20.596742, mean_eps: 0.568810\n",
      "  72027/150000: episode: 754, duration: 0.861s, episode steps: 117, steps per second: 136, episode reward:  2.072, mean reward:  0.018 [-100.000, 15.233], mean action: 1.726 [0.000, 3.000],  loss: 3.316351, mae: 35.939502, mean_q: 21.553800, mean_eps: 0.568192\n",
      "  72292/150000: episode: 755, duration: 2.041s, episode steps: 265, steps per second: 130, episode reward: -261.541, mean reward: -0.987 [-100.000, 16.609], mean action: 1.766 [0.000, 3.000],  loss: 4.814669, mae: 36.105500, mean_q: 21.802585, mean_eps: 0.567046\n",
      "  72389/150000: episode: 756, duration: 0.707s, episode steps:  97, steps per second: 137, episode reward: -71.044, mean reward: -0.732 [-100.000,  9.979], mean action: 1.711 [0.000, 3.000],  loss: 4.128348, mae: 35.934794, mean_q: 20.348513, mean_eps: 0.565960\n",
      "  72525/150000: episode: 757, duration: 0.915s, episode steps: 136, steps per second: 149, episode reward: -28.042, mean reward: -0.206 [-100.000, 13.613], mean action: 1.691 [0.000, 3.000],  loss: 4.751277, mae: 36.330026, mean_q: 21.443783, mean_eps: 0.565261\n",
      "  72752/150000: episode: 758, duration: 1.565s, episode steps: 227, steps per second: 145, episode reward: -3.663, mean reward: -0.016 [-100.000, 44.721], mean action: 1.692 [0.000, 3.000],  loss: 5.710840, mae: 36.251321, mean_q: 20.914908, mean_eps: 0.564172\n",
      "  72845/150000: episode: 759, duration: 0.668s, episode steps:  93, steps per second: 139, episode reward: -117.690, mean reward: -1.265 [-100.000,  9.033], mean action: 1.914 [0.000, 3.000],  loss: 6.749282, mae: 36.293944, mean_q: 21.257512, mean_eps: 0.563212\n",
      "  72957/150000: episode: 760, duration: 0.841s, episode steps: 112, steps per second: 133, episode reward: -79.070, mean reward: -0.706 [-100.000,  6.281], mean action: 1.545 [0.000, 3.000],  loss: 5.141545, mae: 36.299545, mean_q: 20.552661, mean_eps: 0.562597\n",
      "  73078/150000: episode: 761, duration: 0.986s, episode steps: 121, steps per second: 123, episode reward: -12.946, mean reward: -0.107 [-100.000, 13.618], mean action: 1.488 [0.000, 3.000],  loss: 3.556545, mae: 35.772853, mean_q: 21.276152, mean_eps: 0.561898\n",
      "  73178/150000: episode: 762, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -68.307, mean reward: -0.683 [-100.000,  8.331], mean action: 1.750 [0.000, 3.000],  loss: 6.108676, mae: 35.890546, mean_q: 21.002347, mean_eps: 0.561235\n",
      "  73281/150000: episode: 763, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -64.277, mean reward: -0.624 [-100.000, 20.903], mean action: 1.592 [0.000, 3.000],  loss: 3.170224, mae: 35.969801, mean_q: 20.284536, mean_eps: 0.560626\n",
      "  73441/150000: episode: 764, duration: 1.104s, episode steps: 160, steps per second: 145, episode reward: -148.886, mean reward: -0.931 [-100.000,  3.158], mean action: 1.525 [0.000, 3.000],  loss: 3.157916, mae: 35.654537, mean_q: 20.709480, mean_eps: 0.559837\n",
      "  73550/150000: episode: 765, duration: 0.769s, episode steps: 109, steps per second: 142, episode reward: -86.901, mean reward: -0.797 [-100.000, 15.362], mean action: 1.853 [0.000, 3.000],  loss: 4.961266, mae: 35.870443, mean_q: 21.200992, mean_eps: 0.559030\n",
      "  73662/150000: episode: 766, duration: 0.780s, episode steps: 112, steps per second: 144, episode reward: -262.273, mean reward: -2.342 [-100.000, 71.286], mean action: 1.714 [0.000, 3.000],  loss: 3.275687, mae: 35.889108, mean_q: 20.726728, mean_eps: 0.558367\n",
      "  73783/150000: episode: 767, duration: 0.814s, episode steps: 121, steps per second: 149, episode reward: -3.793, mean reward: -0.031 [-100.000, 19.553], mean action: 1.785 [0.000, 3.000],  loss: 3.403918, mae: 35.986214, mean_q: 21.147194, mean_eps: 0.557668\n",
      "  73865/150000: episode: 768, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -35.007, mean reward: -0.427 [-100.000, 13.540], mean action: 1.659 [0.000, 3.000],  loss: 4.242954, mae: 35.551923, mean_q: 21.322710, mean_eps: 0.557059\n",
      "  74015/150000: episode: 769, duration: 1.039s, episode steps: 150, steps per second: 144, episode reward: -197.843, mean reward: -1.319 [-100.000, 72.997], mean action: 1.573 [0.000, 3.000],  loss: 4.273875, mae: 35.908107, mean_q: 21.589448, mean_eps: 0.556363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  74100/150000: episode: 770, duration: 0.618s, episode steps:  85, steps per second: 137, episode reward: -63.968, mean reward: -0.753 [-100.000, 12.973], mean action: 1.553 [0.000, 3.000],  loss: 6.437460, mae: 36.742774, mean_q: 21.593810, mean_eps: 0.555658\n",
      "  74222/150000: episode: 771, duration: 0.869s, episode steps: 122, steps per second: 140, episode reward: -71.738, mean reward: -0.588 [-100.000,  7.041], mean action: 1.697 [0.000, 3.000],  loss: 5.166299, mae: 36.440906, mean_q: 21.334916, mean_eps: 0.555037\n",
      "  74322/150000: episode: 772, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -57.358, mean reward: -0.574 [-100.000, 15.490], mean action: 1.680 [0.000, 3.000],  loss: 4.642952, mae: 36.725521, mean_q: 21.432260, mean_eps: 0.554371\n",
      "  74437/150000: episode: 773, duration: 0.776s, episode steps: 115, steps per second: 148, episode reward: -56.778, mean reward: -0.494 [-100.000, 18.160], mean action: 1.539 [0.000, 3.000],  loss: 5.619163, mae: 36.519520, mean_q: 21.920360, mean_eps: 0.553726\n",
      "  74506/150000: episode: 774, duration: 0.508s, episode steps:  69, steps per second: 136, episode reward: -41.556, mean reward: -0.602 [-100.000,  7.830], mean action: 1.623 [0.000, 3.000],  loss: 4.595090, mae: 36.163545, mean_q: 21.136884, mean_eps: 0.553174\n",
      "  74755/150000: episode: 775, duration: 1.687s, episode steps: 249, steps per second: 148, episode reward: -126.142, mean reward: -0.507 [-100.000, 15.325], mean action: 1.687 [0.000, 3.000],  loss: 4.380234, mae: 36.655147, mean_q: 21.677743, mean_eps: 0.552220\n",
      "  74891/150000: episode: 776, duration: 0.952s, episode steps: 136, steps per second: 143, episode reward: -66.526, mean reward: -0.489 [-100.000,  8.329], mean action: 1.559 [0.000, 3.000],  loss: 3.873906, mae: 36.662254, mean_q: 22.372681, mean_eps: 0.551065\n",
      "  75011/150000: episode: 777, duration: 0.802s, episode steps: 120, steps per second: 150, episode reward: -103.075, mean reward: -0.859 [-100.000, 12.853], mean action: 1.625 [0.000, 3.000],  loss: 4.349100, mae: 36.358925, mean_q: 20.940856, mean_eps: 0.550297\n",
      "  75129/150000: episode: 778, duration: 0.823s, episode steps: 118, steps per second: 143, episode reward: -71.501, mean reward: -0.606 [-100.000, 11.312], mean action: 1.449 [0.000, 3.000],  loss: 4.196645, mae: 36.808459, mean_q: 21.020142, mean_eps: 0.549583\n",
      "  75198/150000: episode: 779, duration: 0.491s, episode steps:  69, steps per second: 141, episode reward: -40.706, mean reward: -0.590 [-100.000, 11.580], mean action: 1.754 [0.000, 3.000],  loss: 3.908158, mae: 36.595754, mean_q: 22.217193, mean_eps: 0.549022\n",
      "  75298/150000: episode: 780, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -44.979, mean reward: -0.450 [-100.000, 11.957], mean action: 1.720 [0.000, 3.000],  loss: 4.242731, mae: 36.560308, mean_q: 20.284332, mean_eps: 0.548515\n",
      "  75433/150000: episode: 781, duration: 0.935s, episode steps: 135, steps per second: 144, episode reward: 20.395, mean reward:  0.151 [-100.000, 15.833], mean action: 1.793 [0.000, 3.000],  loss: 4.370534, mae: 36.828076, mean_q: 22.095911, mean_eps: 0.547810\n",
      "  75519/150000: episode: 782, duration: 0.600s, episode steps:  86, steps per second: 143, episode reward: -15.616, mean reward: -0.182 [-100.000,  9.509], mean action: 1.698 [0.000, 3.000],  loss: 3.824433, mae: 36.432795, mean_q: 22.282680, mean_eps: 0.547147\n",
      "  76519/150000: episode: 783, duration: 7.996s, episode steps: 1000, steps per second: 125, episode reward: -52.589, mean reward: -0.053 [-22.579, 21.348], mean action: 1.655 [0.000, 3.000],  loss: 4.577694, mae: 36.356741, mean_q: 20.829761, mean_eps: 0.543889\n",
      "  76611/150000: episode: 784, duration: 0.617s, episode steps:  92, steps per second: 149, episode reward: -88.421, mean reward: -0.961 [-100.000,  7.403], mean action: 1.413 [0.000, 3.000],  loss: 5.747871, mae: 35.950014, mean_q: 20.072426, mean_eps: 0.540613\n",
      "  76747/150000: episode: 785, duration: 0.909s, episode steps: 136, steps per second: 150, episode reward: -51.533, mean reward: -0.379 [-100.000, 17.293], mean action: 1.596 [0.000, 3.000],  loss: 6.506839, mae: 36.312094, mean_q: 20.135310, mean_eps: 0.539929\n",
      "  76833/150000: episode: 786, duration: 0.596s, episode steps:  86, steps per second: 144, episode reward: -34.633, mean reward: -0.403 [-100.000, 13.415], mean action: 1.640 [0.000, 3.000],  loss: 4.321814, mae: 35.456975, mean_q: 19.868764, mean_eps: 0.539263\n",
      "  76909/150000: episode: 787, duration: 0.516s, episode steps:  76, steps per second: 147, episode reward: -4.949, mean reward: -0.065 [-100.000, 12.075], mean action: 1.908 [0.000, 3.000],  loss: 6.797551, mae: 35.902047, mean_q: 19.141976, mean_eps: 0.538777\n",
      "  76985/150000: episode: 788, duration: 0.510s, episode steps:  76, steps per second: 149, episode reward: -24.347, mean reward: -0.320 [-100.000, 13.471], mean action: 1.513 [0.000, 3.000],  loss: 5.936214, mae: 35.878996, mean_q: 19.499187, mean_eps: 0.538321\n",
      "  77129/150000: episode: 789, duration: 0.969s, episode steps: 144, steps per second: 149, episode reward: -47.422, mean reward: -0.329 [-100.000, 17.031], mean action: 1.403 [0.000, 3.000],  loss: 5.005792, mae: 35.692809, mean_q: 20.640704, mean_eps: 0.537661\n",
      "  77231/150000: episode: 790, duration: 0.740s, episode steps: 102, steps per second: 138, episode reward: -51.113, mean reward: -0.501 [-100.000,  6.271], mean action: 1.667 [0.000, 3.000],  loss: 7.515250, mae: 35.967484, mean_q: 21.132468, mean_eps: 0.536923\n",
      "  77403/150000: episode: 791, duration: 1.204s, episode steps: 172, steps per second: 143, episode reward: -52.844, mean reward: -0.307 [-100.000, 14.588], mean action: 1.715 [0.000, 3.000],  loss: 5.258401, mae: 35.504668, mean_q: 20.835137, mean_eps: 0.536101\n",
      "  77520/150000: episode: 792, duration: 0.842s, episode steps: 117, steps per second: 139, episode reward: -12.763, mean reward: -0.109 [-100.000, 11.503], mean action: 1.718 [0.000, 3.000],  loss: 4.872863, mae: 35.466528, mean_q: 20.016095, mean_eps: 0.535234\n",
      "  77654/150000: episode: 793, duration: 0.897s, episode steps: 134, steps per second: 149, episode reward: -6.078, mean reward: -0.045 [-100.000, 11.167], mean action: 1.649 [0.000, 3.000],  loss: 4.967400, mae: 35.467295, mean_q: 20.536129, mean_eps: 0.534481\n",
      "  77806/150000: episode: 794, duration: 1.083s, episode steps: 152, steps per second: 140, episode reward: -113.227, mean reward: -0.745 [-100.000, 12.230], mean action: 1.737 [0.000, 3.000],  loss: 4.804559, mae: 35.663728, mean_q: 20.679481, mean_eps: 0.533623\n",
      "  77927/150000: episode: 795, duration: 0.834s, episode steps: 121, steps per second: 145, episode reward: -10.819, mean reward: -0.089 [-100.000, 30.080], mean action: 1.645 [0.000, 3.000],  loss: 5.005462, mae: 35.453552, mean_q: 19.417763, mean_eps: 0.532804\n",
      "  77996/150000: episode: 796, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -26.972, mean reward: -0.391 [-100.000, 10.701], mean action: 1.652 [0.000, 3.000],  loss: 3.679572, mae: 35.659533, mean_q: 19.767613, mean_eps: 0.532234\n",
      "  78166/150000: episode: 797, duration: 1.244s, episode steps: 170, steps per second: 137, episode reward: -106.823, mean reward: -0.628 [-100.000,  4.308], mean action: 1.771 [0.000, 3.000],  loss: 4.509367, mae: 35.453814, mean_q: 20.184122, mean_eps: 0.531517\n",
      "  78257/150000: episode: 798, duration: 0.622s, episode steps:  91, steps per second: 146, episode reward: -44.043, mean reward: -0.484 [-100.000, 12.331], mean action: 1.813 [0.000, 3.000],  loss: 5.996061, mae: 35.904939, mean_q: 21.139708, mean_eps: 0.530734\n",
      "  78376/150000: episode: 799, duration: 0.809s, episode steps: 119, steps per second: 147, episode reward: -7.062, mean reward: -0.059 [-100.000, 18.242], mean action: 1.790 [0.000, 3.000],  loss: 5.011944, mae: 35.754384, mean_q: 20.604812, mean_eps: 0.530104\n",
      "  78488/150000: episode: 800, duration: 0.789s, episode steps: 112, steps per second: 142, episode reward: -42.047, mean reward: -0.375 [-100.000,  8.610], mean action: 1.652 [0.000, 3.000],  loss: 4.602283, mae: 35.807922, mean_q: 20.558461, mean_eps: 0.529411\n",
      "  79488/150000: episode: 801, duration: 7.582s, episode steps: 1000, steps per second: 132, episode reward: 15.565, mean reward:  0.016 [-23.974, 48.603], mean action: 1.765 [0.000, 3.000],  loss: 5.081888, mae: 35.640990, mean_q: 20.747656, mean_eps: 0.526075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79641/150000: episode: 802, duration: 1.065s, episode steps: 153, steps per second: 144, episode reward: -186.349, mean reward: -1.218 [-100.000,  8.672], mean action: 1.673 [0.000, 3.000],  loss: 4.303985, mae: 35.575775, mean_q: 20.611393, mean_eps: 0.522616\n",
      "  79775/150000: episode: 803, duration: 0.912s, episode steps: 134, steps per second: 147, episode reward: -10.044, mean reward: -0.075 [-100.000, 15.200], mean action: 1.552 [0.000, 3.000],  loss: 6.506110, mae: 35.953140, mean_q: 20.787103, mean_eps: 0.521755\n",
      "  79933/150000: episode: 804, duration: 1.131s, episode steps: 158, steps per second: 140, episode reward: -151.013, mean reward: -0.956 [-100.000, 41.239], mean action: 1.797 [0.000, 3.000],  loss: 4.691068, mae: 35.455677, mean_q: 21.312353, mean_eps: 0.520879\n",
      "  80036/150000: episode: 805, duration: 0.691s, episode steps: 103, steps per second: 149, episode reward: -39.704, mean reward: -0.385 [-100.000, 15.872], mean action: 1.689 [0.000, 3.000],  loss: 5.614282, mae: 35.406254, mean_q: 19.930113, mean_eps: 0.520096\n",
      "  80154/150000: episode: 806, duration: 0.826s, episode steps: 118, steps per second: 143, episode reward: -41.870, mean reward: -0.355 [-100.000, 10.730], mean action: 1.746 [0.000, 3.000],  loss: 6.125961, mae: 35.844496, mean_q: 19.668017, mean_eps: 0.519433\n",
      "  80257/150000: episode: 807, duration: 0.705s, episode steps: 103, steps per second: 146, episode reward: -162.598, mean reward: -1.579 [-100.000,  2.252], mean action: 1.495 [0.000, 3.000],  loss: 6.821621, mae: 35.738516, mean_q: 20.645926, mean_eps: 0.518770\n",
      "  80393/150000: episode: 808, duration: 0.915s, episode steps: 136, steps per second: 149, episode reward: -81.556, mean reward: -0.600 [-100.000,  7.500], mean action: 1.559 [0.000, 3.000],  loss: 4.981175, mae: 35.504718, mean_q: 21.004957, mean_eps: 0.518053\n",
      "  80569/150000: episode: 809, duration: 1.245s, episode steps: 176, steps per second: 141, episode reward: -106.933, mean reward: -0.608 [-100.000, 10.834], mean action: 1.790 [0.000, 3.000],  loss: 4.906412, mae: 35.715554, mean_q: 21.065445, mean_eps: 0.517117\n",
      "  80657/150000: episode: 810, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -51.828, mean reward: -0.589 [-100.000, 10.330], mean action: 1.920 [0.000, 3.000],  loss: 6.217384, mae: 35.786314, mean_q: 20.121386, mean_eps: 0.516325\n",
      "  80764/150000: episode: 811, duration: 0.731s, episode steps: 107, steps per second: 146, episode reward: -64.617, mean reward: -0.604 [-100.000,  9.261], mean action: 1.523 [0.000, 3.000],  loss: 4.493007, mae: 35.851904, mean_q: 19.826113, mean_eps: 0.515740\n",
      "  80874/150000: episode: 812, duration: 0.908s, episode steps: 110, steps per second: 121, episode reward: -44.844, mean reward: -0.408 [-100.000, 11.934], mean action: 1.536 [0.000, 3.000],  loss: 6.060891, mae: 35.470042, mean_q: 20.957128, mean_eps: 0.515089\n",
      "  81295/150000: episode: 813, duration: 3.517s, episode steps: 421, steps per second: 120, episode reward: -133.024, mean reward: -0.316 [-100.000, 15.575], mean action: 1.734 [0.000, 3.000],  loss: 5.397520, mae: 35.567431, mean_q: 20.534688, mean_eps: 0.513496\n",
      "  81422/150000: episode: 814, duration: 0.969s, episode steps: 127, steps per second: 131, episode reward: -46.059, mean reward: -0.363 [-100.000, 15.462], mean action: 1.614 [0.000, 3.000],  loss: 6.828501, mae: 35.482960, mean_q: 20.016577, mean_eps: 0.511852\n",
      "  81580/150000: episode: 815, duration: 1.083s, episode steps: 158, steps per second: 146, episode reward: -35.568, mean reward: -0.225 [-100.000, 36.367], mean action: 1.500 [0.000, 3.000],  loss: 5.591426, mae: 35.580034, mean_q: 21.035507, mean_eps: 0.510997\n",
      "  81676/150000: episode: 816, duration: 0.690s, episode steps:  96, steps per second: 139, episode reward: -22.171, mean reward: -0.231 [-100.000, 18.828], mean action: 1.719 [0.000, 3.000],  loss: 5.250139, mae: 35.403021, mean_q: 20.943447, mean_eps: 0.510235\n",
      "  81799/150000: episode: 817, duration: 0.863s, episode steps: 123, steps per second: 143, episode reward: -61.055, mean reward: -0.496 [-100.000, 16.184], mean action: 1.715 [0.000, 3.000],  loss: 6.104762, mae: 35.473036, mean_q: 20.853394, mean_eps: 0.509578\n",
      "  81951/150000: episode: 818, duration: 1.077s, episode steps: 152, steps per second: 141, episode reward: -1.385, mean reward: -0.009 [-100.000, 19.242], mean action: 1.691 [0.000, 3.000],  loss: 6.263600, mae: 35.893554, mean_q: 20.055369, mean_eps: 0.508753\n",
      "  82114/150000: episode: 819, duration: 1.114s, episode steps: 163, steps per second: 146, episode reward: 13.026, mean reward:  0.080 [-100.000,  9.708], mean action: 1.718 [0.000, 3.000],  loss: 5.237599, mae: 35.468968, mean_q: 21.171157, mean_eps: 0.507808\n",
      "  82199/150000: episode: 820, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -19.540, mean reward: -0.230 [-100.000, 14.117], mean action: 1.565 [0.000, 3.000],  loss: 6.125871, mae: 35.731931, mean_q: 20.541586, mean_eps: 0.507064\n",
      "  82329/150000: episode: 821, duration: 0.940s, episode steps: 130, steps per second: 138, episode reward:  8.619, mean reward:  0.066 [-100.000, 18.036], mean action: 1.669 [0.000, 3.000],  loss: 4.914950, mae: 35.922277, mean_q: 20.118262, mean_eps: 0.506419\n",
      "  82450/150000: episode: 822, duration: 0.859s, episode steps: 121, steps per second: 141, episode reward: -30.627, mean reward: -0.253 [-100.000, 11.588], mean action: 1.521 [0.000, 3.000],  loss: 6.754905, mae: 35.644247, mean_q: 20.964024, mean_eps: 0.505666\n",
      "  82575/150000: episode: 823, duration: 0.906s, episode steps: 125, steps per second: 138, episode reward:  7.753, mean reward:  0.062 [-100.000, 14.680], mean action: 1.664 [0.000, 3.000],  loss: 4.959880, mae: 35.735304, mean_q: 20.788937, mean_eps: 0.504928\n",
      "  82683/150000: episode: 824, duration: 0.735s, episode steps: 108, steps per second: 147, episode reward: -90.269, mean reward: -0.836 [-100.000, 29.163], mean action: 1.648 [0.000, 3.000],  loss: 4.763344, mae: 35.656641, mean_q: 20.778176, mean_eps: 0.504229\n",
      "  83085/150000: episode: 825, duration: 2.839s, episode steps: 402, steps per second: 142, episode reward: -93.587, mean reward: -0.233 [-100.000, 15.450], mean action: 1.649 [0.000, 3.000],  loss: 6.422804, mae: 35.645006, mean_q: 21.079220, mean_eps: 0.502699\n",
      "  83746/150000: episode: 826, duration: 5.238s, episode steps: 661, steps per second: 126, episode reward: -143.164, mean reward: -0.217 [-100.000, 15.191], mean action: 1.744 [0.000, 3.000],  loss: 5.960707, mae: 35.502576, mean_q: 21.424155, mean_eps: 0.499510\n",
      "  83986/150000: episode: 827, duration: 1.696s, episode steps: 240, steps per second: 142, episode reward: -53.123, mean reward: -0.221 [-100.000, 45.734], mean action: 1.733 [0.000, 3.000],  loss: 5.621496, mae: 35.481159, mean_q: 21.987164, mean_eps: 0.496807\n",
      "  84125/150000: episode: 828, duration: 0.968s, episode steps: 139, steps per second: 144, episode reward: -68.546, mean reward: -0.493 [-100.000, 13.539], mean action: 1.683 [0.000, 3.000],  loss: 5.295702, mae: 35.673493, mean_q: 21.757947, mean_eps: 0.495670\n",
      "  85125/150000: episode: 829, duration: 7.570s, episode steps: 1000, steps per second: 132, episode reward: 22.406, mean reward:  0.022 [-24.348, 24.484], mean action: 1.575 [0.000, 3.000],  loss: 6.497860, mae: 35.199233, mean_q: 21.255008, mean_eps: 0.492253\n",
      "  85302/150000: episode: 830, duration: 1.198s, episode steps: 177, steps per second: 148, episode reward: -23.315, mean reward: -0.132 [-100.000, 11.201], mean action: 1.734 [0.000, 3.000],  loss: 6.471549, mae: 35.009728, mean_q: 21.705213, mean_eps: 0.488722\n",
      "  85426/150000: episode: 831, duration: 0.835s, episode steps: 124, steps per second: 149, episode reward: -29.155, mean reward: -0.235 [-100.000, 21.396], mean action: 1.645 [0.000, 3.000],  loss: 7.763185, mae: 34.689285, mean_q: 21.246245, mean_eps: 0.487819\n",
      "  85580/150000: episode: 832, duration: 1.071s, episode steps: 154, steps per second: 144, episode reward: -10.657, mean reward: -0.069 [-100.000, 10.838], mean action: 1.701 [0.000, 3.000],  loss: 6.757041, mae: 34.920906, mean_q: 20.857450, mean_eps: 0.486985\n",
      "  85732/150000: episode: 833, duration: 1.024s, episode steps: 152, steps per second: 148, episode reward: 32.891, mean reward:  0.216 [-100.000, 20.552], mean action: 1.684 [0.000, 3.000],  loss: 6.505469, mae: 35.214989, mean_q: 22.253549, mean_eps: 0.486067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  85845/150000: episode: 834, duration: 0.799s, episode steps: 113, steps per second: 141, episode reward: -23.486, mean reward: -0.208 [-100.000, 21.832], mean action: 1.717 [0.000, 3.000],  loss: 8.117824, mae: 35.277742, mean_q: 21.396441, mean_eps: 0.485272\n",
      "  85926/150000: episode: 835, duration: 0.552s, episode steps:  81, steps per second: 147, episode reward: -36.282, mean reward: -0.448 [-100.000, 10.965], mean action: 1.840 [0.000, 3.000],  loss: 9.446624, mae: 35.244554, mean_q: 21.342148, mean_eps: 0.484690\n",
      "  86163/150000: episode: 836, duration: 1.778s, episode steps: 237, steps per second: 133, episode reward: -73.835, mean reward: -0.312 [-100.000, 15.453], mean action: 1.759 [0.000, 3.000],  loss: 7.407952, mae: 34.959829, mean_q: 21.621873, mean_eps: 0.483736\n",
      "  86262/150000: episode: 837, duration: 0.657s, episode steps:  99, steps per second: 151, episode reward: -163.675, mean reward: -1.653 [-100.000,  1.490], mean action: 1.586 [0.000, 3.000],  loss: 8.212032, mae: 34.555781, mean_q: 22.227849, mean_eps: 0.482728\n",
      "  86426/150000: episode: 838, duration: 1.148s, episode steps: 164, steps per second: 143, episode reward: -180.754, mean reward: -1.102 [-100.000, 62.293], mean action: 1.726 [0.000, 3.000],  loss: 6.107570, mae: 34.504635, mean_q: 22.527803, mean_eps: 0.481939\n",
      "  86533/150000: episode: 839, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -15.367, mean reward: -0.144 [-100.000, 17.245], mean action: 1.598 [0.000, 3.000],  loss: 7.098134, mae: 35.200714, mean_q: 22.206715, mean_eps: 0.481126\n",
      "  86968/150000: episode: 840, duration: 3.184s, episode steps: 435, steps per second: 137, episode reward: -103.924, mean reward: -0.239 [-100.000, 16.910], mean action: 1.766 [0.000, 3.000],  loss: 6.816389, mae: 34.723196, mean_q: 22.034133, mean_eps: 0.479500\n",
      "  87115/150000: episode: 841, duration: 1.013s, episode steps: 147, steps per second: 145, episode reward: -15.989, mean reward: -0.109 [-100.000, 19.496], mean action: 1.347 [0.000, 3.000],  loss: 6.823284, mae: 35.079414, mean_q: 22.381924, mean_eps: 0.477754\n",
      "  87247/150000: episode: 842, duration: 0.880s, episode steps: 132, steps per second: 150, episode reward: -59.131, mean reward: -0.448 [-100.000,  9.511], mean action: 1.561 [0.000, 3.000],  loss: 6.422491, mae: 35.074170, mean_q: 21.895566, mean_eps: 0.476917\n",
      "  87391/150000: episode: 843, duration: 1.016s, episode steps: 144, steps per second: 142, episode reward: -15.955, mean reward: -0.111 [-100.000, 16.296], mean action: 1.576 [0.000, 3.000],  loss: 8.274765, mae: 34.965051, mean_q: 21.011541, mean_eps: 0.476089\n",
      "  87506/150000: episode: 844, duration: 0.786s, episode steps: 115, steps per second: 146, episode reward: -14.120, mean reward: -0.123 [-100.000,  9.814], mean action: 1.696 [0.000, 3.000],  loss: 6.785170, mae: 35.073935, mean_q: 22.776617, mean_eps: 0.475312\n",
      "  88265/150000: episode: 845, duration: 5.846s, episode steps: 759, steps per second: 130, episode reward: -116.295, mean reward: -0.153 [-100.000, 19.360], mean action: 1.777 [0.000, 3.000],  loss: 7.372237, mae: 34.918484, mean_q: 22.151521, mean_eps: 0.472690\n",
      "  88397/150000: episode: 846, duration: 0.897s, episode steps: 132, steps per second: 147, episode reward: -25.019, mean reward: -0.190 [-100.000, 13.539], mean action: 1.644 [0.000, 3.000],  loss: 6.909422, mae: 35.021709, mean_q: 22.357846, mean_eps: 0.470017\n",
      "  88654/150000: episode: 847, duration: 1.777s, episode steps: 257, steps per second: 145, episode reward: -160.905, mean reward: -0.626 [-100.000, 13.242], mean action: 1.661 [0.000, 3.000],  loss: 7.824690, mae: 34.905597, mean_q: 22.501442, mean_eps: 0.468850\n",
      "  88752/150000: episode: 848, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -266.744, mean reward: -2.722 [-100.000,  0.620], mean action: 1.735 [0.000, 3.000],  loss: 8.571570, mae: 34.837198, mean_q: 22.986872, mean_eps: 0.467785\n",
      "  88841/150000: episode: 849, duration: 0.636s, episode steps:  89, steps per second: 140, episode reward: -337.787, mean reward: -3.795 [-100.000,  0.570], mean action: 1.730 [0.000, 3.000],  loss: 8.438253, mae: 34.886386, mean_q: 23.182469, mean_eps: 0.467224\n",
      "  89324/150000: episode: 850, duration: 3.570s, episode steps: 483, steps per second: 135, episode reward: -208.763, mean reward: -0.432 [-100.000, 19.868], mean action: 1.770 [0.000, 3.000],  loss: 9.270411, mae: 34.945797, mean_q: 22.881412, mean_eps: 0.465508\n",
      "  90324/150000: episode: 851, duration: 8.721s, episode steps: 1000, steps per second: 115, episode reward: 38.247, mean reward:  0.038 [-20.033, 22.405], mean action: 1.635 [0.000, 3.000],  loss: 7.847772, mae: 34.632139, mean_q: 23.462788, mean_eps: 0.461059\n",
      "  90471/150000: episode: 852, duration: 1.148s, episode steps: 147, steps per second: 128, episode reward: -87.894, mean reward: -0.598 [-100.000,  3.466], mean action: 1.653 [0.000, 3.000],  loss: 8.178186, mae: 34.366293, mean_q: 23.026568, mean_eps: 0.457618\n",
      "  91471/150000: episode: 853, duration: 8.130s, episode steps: 1000, steps per second: 123, episode reward: -74.430, mean reward: -0.074 [-22.376, 16.140], mean action: 1.751 [0.000, 3.000],  loss: 7.885833, mae: 34.275232, mean_q: 23.300263, mean_eps: 0.454177\n",
      "  91974/150000: episode: 854, duration: 3.636s, episode steps: 503, steps per second: 138, episode reward: -68.803, mean reward: -0.137 [-100.000, 64.622], mean action: 1.720 [0.000, 3.000],  loss: 8.036730, mae: 33.865098, mean_q: 22.905469, mean_eps: 0.449668\n",
      "  92089/150000: episode: 855, duration: 0.802s, episode steps: 115, steps per second: 143, episode reward: -52.325, mean reward: -0.455 [-100.000, 11.579], mean action: 1.626 [0.000, 3.000],  loss: 8.752930, mae: 34.014210, mean_q: 22.854601, mean_eps: 0.447814\n",
      "  92191/150000: episode: 856, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -34.205, mean reward: -0.335 [-100.000,  4.844], mean action: 1.676 [0.000, 3.000],  loss: 8.468569, mae: 33.626166, mean_q: 22.659419, mean_eps: 0.447163\n",
      "  93191/150000: episode: 857, duration: 8.044s, episode steps: 1000, steps per second: 124, episode reward: -16.665, mean reward: -0.017 [-22.370, 37.773], mean action: 1.818 [0.000, 3.000],  loss: 8.738808, mae: 33.365216, mean_q: 23.033527, mean_eps: 0.443857\n",
      "  93477/150000: episode: 858, duration: 2.009s, episode steps: 286, steps per second: 142, episode reward: -219.898, mean reward: -0.769 [-100.000, 34.175], mean action: 1.556 [0.000, 3.000],  loss: 7.647557, mae: 32.774604, mean_q: 22.960178, mean_eps: 0.439999\n",
      "  93681/150000: episode: 859, duration: 1.373s, episode steps: 204, steps per second: 149, episode reward: -80.095, mean reward: -0.393 [-100.000,  5.063], mean action: 1.789 [0.000, 3.000],  loss: 8.171220, mae: 33.024388, mean_q: 23.170986, mean_eps: 0.438529\n",
      "  93800/150000: episode: 860, duration: 0.853s, episode steps: 119, steps per second: 139, episode reward: 22.319, mean reward:  0.188 [-100.000, 12.970], mean action: 1.824 [0.000, 3.000],  loss: 8.266218, mae: 32.886396, mean_q: 22.572926, mean_eps: 0.437560\n",
      "  94800/150000: episode: 861, duration: 7.438s, episode steps: 1000, steps per second: 134, episode reward: 10.294, mean reward:  0.010 [-24.051, 25.001], mean action: 1.525 [0.000, 3.000],  loss: 8.343103, mae: 32.937948, mean_q: 24.025984, mean_eps: 0.434203\n",
      "  95800/150000: episode: 862, duration: 8.027s, episode steps: 1000, steps per second: 125, episode reward: -11.130, mean reward: -0.011 [-21.004, 20.852], mean action: 1.510 [0.000, 3.000],  loss: 8.499094, mae: 32.243147, mean_q: 23.016518, mean_eps: 0.428203\n",
      "  96800/150000: episode: 863, duration: 7.490s, episode steps: 1000, steps per second: 134, episode reward: -60.616, mean reward: -0.061 [-22.172, 26.005], mean action: 1.679 [0.000, 3.000],  loss: 8.386689, mae: 31.758982, mean_q: 22.958407, mean_eps: 0.422203\n",
      "  97800/150000: episode: 864, duration: 7.497s, episode steps: 1000, steps per second: 133, episode reward: -21.918, mean reward: -0.022 [-12.616, 26.665], mean action: 1.631 [0.000, 3.000],  loss: 7.940961, mae: 31.395204, mean_q: 23.320692, mean_eps: 0.416203\n",
      "  98800/150000: episode: 865, duration: 8.768s, episode steps: 1000, steps per second: 114, episode reward: -13.176, mean reward: -0.013 [-9.101, 12.974], mean action: 1.729 [0.000, 3.000],  loss: 8.123252, mae: 30.534930, mean_q: 23.244690, mean_eps: 0.410203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  99009/150000: episode: 866, duration: 1.464s, episode steps: 209, steps per second: 143, episode reward: -44.859, mean reward: -0.215 [-100.000, 52.220], mean action: 1.579 [0.000, 3.000],  loss: 7.151001, mae: 30.051914, mean_q: 23.205500, mean_eps: 0.406576\n",
      "  99307/150000: episode: 867, duration: 2.098s, episode steps: 298, steps per second: 142, episode reward: -166.036, mean reward: -0.557 [-100.000, 18.946], mean action: 1.762 [0.000, 3.000],  loss: 7.398812, mae: 30.582721, mean_q: 24.413634, mean_eps: 0.405055\n",
      "  99463/150000: episode: 868, duration: 1.127s, episode steps: 156, steps per second: 138, episode reward: -55.357, mean reward: -0.355 [-100.000,  9.588], mean action: 1.641 [0.000, 3.000],  loss: 7.815472, mae: 30.394103, mean_q: 23.955935, mean_eps: 0.403693\n",
      "  99688/150000: episode: 869, duration: 1.579s, episode steps: 225, steps per second: 142, episode reward: -81.323, mean reward: -0.361 [-100.000, 19.505], mean action: 1.711 [0.000, 3.000],  loss: 8.012784, mae: 30.193883, mean_q: 23.806162, mean_eps: 0.402550\n",
      " 100688/150000: episode: 870, duration: 7.606s, episode steps: 1000, steps per second: 131, episode reward: 21.667, mean reward:  0.022 [-19.946, 22.503], mean action: 1.704 [0.000, 3.000],  loss: 7.913500, mae: 30.137689, mean_q: 23.934411, mean_eps: 0.398875\n",
      " 101688/150000: episode: 871, duration: 7.625s, episode steps: 1000, steps per second: 131, episode reward: -43.650, mean reward: -0.044 [-10.840, 21.634], mean action: 1.762 [0.000, 3.000],  loss: 8.510247, mae: 29.295404, mean_q: 23.718748, mean_eps: 0.392875\n",
      " 101922/150000: episode: 872, duration: 1.632s, episode steps: 234, steps per second: 143, episode reward: -54.821, mean reward: -0.234 [-100.000, 19.985], mean action: 1.803 [0.000, 3.000],  loss: 7.573802, mae: 29.122369, mean_q: 23.542422, mean_eps: 0.389173\n",
      " 102024/150000: episode: 873, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -68.267, mean reward: -0.669 [-100.000, 10.314], mean action: 1.618 [0.000, 3.000],  loss: 8.997639, mae: 29.078090, mean_q: 23.351493, mean_eps: 0.388165\n",
      " 102160/150000: episode: 874, duration: 0.913s, episode steps: 136, steps per second: 149, episode reward: -59.234, mean reward: -0.436 [-100.000, 26.595], mean action: 1.419 [0.000, 3.000],  loss: 7.752003, mae: 28.978607, mean_q: 24.697773, mean_eps: 0.387451\n",
      " 103160/150000: episode: 875, duration: 8.665s, episode steps: 1000, steps per second: 115, episode reward: 11.002, mean reward:  0.011 [-22.218, 22.509], mean action: 1.600 [0.000, 3.000],  loss: 7.974023, mae: 28.906004, mean_q: 24.049033, mean_eps: 0.384043\n",
      " 103498/150000: episode: 876, duration: 2.571s, episode steps: 338, steps per second: 131, episode reward: -71.249, mean reward: -0.211 [-100.000, 11.763], mean action: 1.660 [0.000, 3.000],  loss: 7.098737, mae: 28.641694, mean_q: 23.908333, mean_eps: 0.380029\n",
      " 103649/150000: episode: 877, duration: 1.093s, episode steps: 151, steps per second: 138, episode reward: 11.534, mean reward:  0.076 [-100.000, 15.443], mean action: 1.768 [0.000, 3.000],  loss: 8.318904, mae: 28.823324, mean_q: 23.972096, mean_eps: 0.378562\n",
      " 104649/150000: episode: 878, duration: 7.851s, episode steps: 1000, steps per second: 127, episode reward: 54.639, mean reward:  0.055 [-22.467, 22.702], mean action: 1.743 [0.000, 3.000],  loss: 8.558579, mae: 28.622840, mean_q: 24.280774, mean_eps: 0.375109\n",
      " 105649/150000: episode: 879, duration: 7.611s, episode steps: 1000, steps per second: 131, episode reward: 13.819, mean reward:  0.014 [-23.005, 23.032], mean action: 1.794 [0.000, 3.000],  loss: 8.627192, mae: 28.107960, mean_q: 24.443740, mean_eps: 0.369109\n",
      " 106211/150000: episode: 880, duration: 4.220s, episode steps: 562, steps per second: 133, episode reward: -261.033, mean reward: -0.464 [-100.000, 27.729], mean action: 1.904 [0.000, 3.000],  loss: 7.992407, mae: 27.687328, mean_q: 24.649899, mean_eps: 0.364423\n",
      " 107211/150000: episode: 881, duration: 7.884s, episode steps: 1000, steps per second: 127, episode reward: 111.282, mean reward:  0.111 [-20.133, 23.339], mean action: 1.333 [0.000, 3.000],  loss: 9.250638, mae: 27.293259, mean_q: 24.556780, mean_eps: 0.359737\n",
      " 107674/150000: episode: 882, duration: 3.481s, episode steps: 463, steps per second: 133, episode reward: -182.889, mean reward: -0.395 [-100.000, 21.526], mean action: 1.933 [0.000, 3.000],  loss: 7.594004, mae: 26.714251, mean_q: 24.552011, mean_eps: 0.355348\n",
      " 107805/150000: episode: 883, duration: 0.927s, episode steps: 131, steps per second: 141, episode reward: 26.818, mean reward:  0.205 [-100.000, 20.611], mean action: 1.656 [0.000, 3.000],  loss: 9.789029, mae: 26.755128, mean_q: 23.929877, mean_eps: 0.353566\n",
      " 107912/150000: episode: 884, duration: 0.776s, episode steps: 107, steps per second: 138, episode reward: 44.363, mean reward:  0.415 [-100.000, 14.683], mean action: 1.841 [0.000, 3.000],  loss: 8.083629, mae: 26.474517, mean_q: 23.887819, mean_eps: 0.352852\n",
      " 108396/150000: episode: 885, duration: 3.505s, episode steps: 484, steps per second: 138, episode reward: -117.111, mean reward: -0.242 [-100.000, 24.335], mean action: 1.919 [0.000, 3.000],  loss: 8.610033, mae: 26.292803, mean_q: 24.392386, mean_eps: 0.351079\n",
      " 108701/150000: episode: 886, duration: 2.126s, episode steps: 305, steps per second: 143, episode reward: -57.654, mean reward: -0.189 [-100.000, 25.131], mean action: 1.646 [0.000, 3.000],  loss: 9.905664, mae: 26.272158, mean_q: 23.609871, mean_eps: 0.348712\n",
      " 109701/150000: episode: 887, duration: 7.856s, episode steps: 1000, steps per second: 127, episode reward: 78.623, mean reward:  0.079 [-19.391, 22.791], mean action: 1.354 [0.000, 3.000],  loss: 9.043689, mae: 26.087902, mean_q: 24.523891, mean_eps: 0.344797\n",
      " 110195/150000: episode: 888, duration: 3.524s, episode steps: 494, steps per second: 140, episode reward: -113.646, mean reward: -0.230 [-100.000, 25.524], mean action: 1.725 [0.000, 3.000],  loss: 9.574002, mae: 25.928770, mean_q: 24.634965, mean_eps: 0.340315\n",
      " 110360/150000: episode: 889, duration: 1.162s, episode steps: 165, steps per second: 142, episode reward: -148.686, mean reward: -0.901 [-100.000, 10.148], mean action: 1.933 [0.000, 3.000],  loss: 8.809262, mae: 25.553641, mean_q: 24.872564, mean_eps: 0.338338\n",
      " 111360/150000: episode: 890, duration: 7.503s, episode steps: 1000, steps per second: 133, episode reward: 95.900, mean reward:  0.096 [-23.020, 23.109], mean action: 1.202 [0.000, 3.000],  loss: 8.013693, mae: 25.682395, mean_q: 25.048202, mean_eps: 0.334843\n",
      " 112360/150000: episode: 891, duration: 7.680s, episode steps: 1000, steps per second: 130, episode reward: 51.508, mean reward:  0.052 [-21.660, 26.954], mean action: 1.533 [0.000, 3.000],  loss: 8.232634, mae: 25.267229, mean_q: 25.072512, mean_eps: 0.328843\n",
      " 113360/150000: episode: 892, duration: 7.492s, episode steps: 1000, steps per second: 133, episode reward: 51.635, mean reward:  0.052 [-20.285, 22.832], mean action: 1.300 [0.000, 3.000],  loss: 9.063558, mae: 24.859703, mean_q: 25.131332, mean_eps: 0.322843\n",
      " 114360/150000: episode: 893, duration: 7.709s, episode steps: 1000, steps per second: 130, episode reward:  1.943, mean reward:  0.002 [-22.687, 23.312], mean action: 1.735 [0.000, 3.000],  loss: 8.272357, mae: 24.691043, mean_q: 25.324500, mean_eps: 0.316843\n",
      " 115360/150000: episode: 894, duration: 8.088s, episode steps: 1000, steps per second: 124, episode reward: 110.981, mean reward:  0.111 [-23.365, 22.934], mean action: 1.251 [0.000, 3.000],  loss: 8.651311, mae: 24.630885, mean_q: 25.604355, mean_eps: 0.310843\n",
      " 116360/150000: episode: 895, duration: 7.888s, episode steps: 1000, steps per second: 127, episode reward: 63.824, mean reward:  0.064 [-20.611, 22.846], mean action: 1.402 [0.000, 3.000],  loss: 8.428711, mae: 24.012449, mean_q: 25.172490, mean_eps: 0.304843\n",
      " 117360/150000: episode: 896, duration: 7.481s, episode steps: 1000, steps per second: 134, episode reward: 60.606, mean reward:  0.061 [-19.814, 22.024], mean action: 2.267 [0.000, 3.000],  loss: 8.632892, mae: 23.657083, mean_q: 25.204981, mean_eps: 0.298843\n",
      " 118360/150000: episode: 897, duration: 8.515s, episode steps: 1000, steps per second: 117, episode reward: 76.613, mean reward:  0.077 [-22.025, 23.124], mean action: 1.603 [0.000, 3.000],  loss: 8.977007, mae: 23.682635, mean_q: 25.482052, mean_eps: 0.292843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 119360/150000: episode: 898, duration: 7.605s, episode steps: 1000, steps per second: 131, episode reward: 153.488, mean reward:  0.153 [-24.100, 22.827], mean action: 1.089 [0.000, 3.000],  loss: 9.017034, mae: 23.155876, mean_q: 25.286232, mean_eps: 0.286843\n",
      " 119901/150000: episode: 899, duration: 4.095s, episode steps: 541, steps per second: 132, episode reward: -204.692, mean reward: -0.378 [-100.000, 16.156], mean action: 1.750 [0.000, 3.000],  loss: 9.133801, mae: 22.828125, mean_q: 25.539530, mean_eps: 0.282220\n",
      " 120901/150000: episode: 900, duration: 7.460s, episode steps: 1000, steps per second: 134, episode reward: 58.004, mean reward:  0.058 [-21.500, 23.964], mean action: 1.314 [0.000, 3.000],  loss: 8.508068, mae: 22.530595, mean_q: 25.537361, mean_eps: 0.277597\n",
      " 121901/150000: episode: 901, duration: 7.886s, episode steps: 1000, steps per second: 127, episode reward: -11.128, mean reward: -0.011 [-21.814, 14.587], mean action: 1.711 [0.000, 3.000],  loss: 8.809993, mae: 22.079764, mean_q: 25.254574, mean_eps: 0.271597\n",
      " 122901/150000: episode: 902, duration: 7.841s, episode steps: 1000, steps per second: 128, episode reward: 24.680, mean reward:  0.025 [-20.011, 19.386], mean action: 1.818 [0.000, 3.000],  loss: 8.716094, mae: 21.638868, mean_q: 24.768744, mean_eps: 0.265597\n",
      " 123881/150000: episode: 903, duration: 7.217s, episode steps: 980, steps per second: 136, episode reward: -197.825, mean reward: -0.202 [-100.000, 22.613], mean action: 1.850 [0.000, 3.000],  loss: 7.595725, mae: 21.604142, mean_q: 25.030321, mean_eps: 0.259657\n",
      " 124881/150000: episode: 904, duration: 8.429s, episode steps: 1000, steps per second: 119, episode reward: -28.336, mean reward: -0.028 [-20.179, 23.486], mean action: 1.794 [0.000, 3.000],  loss: 8.058826, mae: 21.355172, mean_q: 25.320868, mean_eps: 0.253717\n",
      " 125881/150000: episode: 905, duration: 7.853s, episode steps: 1000, steps per second: 127, episode reward: 46.321, mean reward:  0.046 [-24.312, 22.691], mean action: 2.051 [0.000, 3.000],  loss: 8.031562, mae: 21.038520, mean_q: 25.058695, mean_eps: 0.247717\n",
      " 126881/150000: episode: 906, duration: 7.679s, episode steps: 1000, steps per second: 130, episode reward: 103.817, mean reward:  0.104 [-23.198, 22.561], mean action: 1.330 [0.000, 3.000],  loss: 8.632450, mae: 21.150104, mean_q: 25.028578, mean_eps: 0.241717\n",
      " 127856/150000: episode: 907, duration: 7.031s, episode steps: 975, steps per second: 139, episode reward: 206.585, mean reward:  0.212 [-22.595, 100.000], mean action: 1.887 [0.000, 3.000],  loss: 7.476966, mae: 20.532820, mean_q: 24.106286, mean_eps: 0.235792\n",
      " 128856/150000: episode: 908, duration: 8.304s, episode steps: 1000, steps per second: 120, episode reward: 22.475, mean reward:  0.022 [-20.224, 22.733], mean action: 1.934 [0.000, 3.000],  loss: 7.536078, mae: 20.238007, mean_q: 24.037914, mean_eps: 0.229867\n",
      " 129856/150000: episode: 909, duration: 8.035s, episode steps: 1000, steps per second: 124, episode reward: 90.706, mean reward:  0.091 [-22.682, 23.758], mean action: 1.189 [0.000, 3.000],  loss: 6.086863, mae: 20.401951, mean_q: 24.755484, mean_eps: 0.223867\n",
      " 130856/150000: episode: 910, duration: 7.342s, episode steps: 1000, steps per second: 136, episode reward: 89.660, mean reward:  0.090 [-20.456, 23.384], mean action: 1.210 [0.000, 3.000],  loss: 6.127046, mae: 20.374796, mean_q: 25.083467, mean_eps: 0.217867\n",
      " 131856/150000: episode: 911, duration: 7.511s, episode steps: 1000, steps per second: 133, episode reward: 97.790, mean reward:  0.098 [-22.839, 23.708], mean action: 1.152 [0.000, 3.000],  loss: 5.329500, mae: 20.272331, mean_q: 25.086568, mean_eps: 0.211867\n",
      " 132501/150000: episode: 912, duration: 5.502s, episode steps: 645, steps per second: 117, episode reward: 222.210, mean reward:  0.345 [-19.947, 100.000], mean action: 1.290 [0.000, 3.000],  loss: 5.822323, mae: 20.024011, mean_q: 25.062293, mean_eps: 0.206932\n",
      " 133501/150000: episode: 913, duration: 8.031s, episode steps: 1000, steps per second: 125, episode reward: 82.066, mean reward:  0.082 [-23.825, 21.473], mean action: 1.133 [0.000, 3.000],  loss: 5.479495, mae: 19.898757, mean_q: 24.928216, mean_eps: 0.201997\n",
      " 134164/150000: episode: 914, duration: 4.959s, episode steps: 663, steps per second: 134, episode reward: 233.230, mean reward:  0.352 [-17.762, 100.000], mean action: 1.065 [0.000, 3.000],  loss: 4.992855, mae: 19.976328, mean_q: 25.013328, mean_eps: 0.197008\n",
      " 134545/150000: episode: 915, duration: 2.682s, episode steps: 381, steps per second: 142, episode reward: 294.945, mean reward:  0.774 [-17.372, 100.000], mean action: 1.622 [0.000, 3.000],  loss: 4.593479, mae: 20.085063, mean_q: 25.242307, mean_eps: 0.193876\n",
      " 135151/150000: episode: 916, duration: 4.389s, episode steps: 606, steps per second: 138, episode reward: 270.722, mean reward:  0.447 [-18.599, 100.000], mean action: 1.117 [0.000, 3.000],  loss: 5.393462, mae: 20.249019, mean_q: 25.449015, mean_eps: 0.190915\n",
      " 136151/150000: episode: 917, duration: 7.263s, episode steps: 1000, steps per second: 138, episode reward: 107.641, mean reward:  0.108 [-19.281, 22.906], mean action: 1.149 [0.000, 3.000],  loss: 5.681535, mae: 20.313771, mean_q: 25.860807, mean_eps: 0.186097\n",
      " 136935/150000: episode: 918, duration: 5.762s, episode steps: 784, steps per second: 136, episode reward: 191.095, mean reward:  0.244 [-18.168, 100.000], mean action: 2.111 [0.000, 3.000],  loss: 5.333574, mae: 20.450680, mean_q: 26.265476, mean_eps: 0.180745\n",
      " 137558/150000: episode: 919, duration: 4.516s, episode steps: 623, steps per second: 138, episode reward: 306.144, mean reward:  0.491 [-24.075, 100.000], mean action: 0.957 [0.000, 3.000],  loss: 5.148395, mae: 20.255063, mean_q: 25.925680, mean_eps: 0.176524\n",
      " 137968/150000: episode: 920, duration: 2.919s, episode steps: 410, steps per second: 140, episode reward: -164.841, mean reward: -0.402 [-100.000, 12.961], mean action: 1.756 [0.000, 3.000],  loss: 4.728936, mae: 19.863712, mean_q: 25.616634, mean_eps: 0.173425\n",
      " 138968/150000: episode: 921, duration: 7.224s, episode steps: 1000, steps per second: 138, episode reward: -90.688, mean reward: -0.091 [-4.858,  4.691], mean action: 1.779 [0.000, 3.000],  loss: 4.436804, mae: 20.272071, mean_q: 26.149805, mean_eps: 0.169195\n",
      " 139289/150000: episode: 922, duration: 2.243s, episode steps: 321, steps per second: 143, episode reward: 214.937, mean reward:  0.670 [-8.467, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 5.613106, mae: 20.503319, mean_q: 26.722146, mean_eps: 0.165232\n",
      " 140289/150000: episode: 923, duration: 7.516s, episode steps: 1000, steps per second: 133, episode reward: 31.412, mean reward:  0.031 [-19.471, 12.691], mean action: 1.564 [0.000, 3.000],  loss: 4.684790, mae: 20.248683, mean_q: 26.455072, mean_eps: 0.161269\n",
      " 141289/150000: episode: 924, duration: 8.092s, episode steps: 1000, steps per second: 124, episode reward: 146.954, mean reward:  0.147 [-18.352, 23.763], mean action: 0.863 [0.000, 3.000],  loss: 4.643184, mae: 20.488609, mean_q: 27.059469, mean_eps: 0.155269\n",
      " 141805/150000: episode: 925, duration: 3.979s, episode steps: 516, steps per second: 130, episode reward: 263.949, mean reward:  0.512 [-21.178, 100.000], mean action: 1.917 [0.000, 3.000],  loss: 5.247336, mae: 20.574942, mean_q: 27.116220, mean_eps: 0.150721\n",
      " 142386/150000: episode: 926, duration: 4.916s, episode steps: 581, steps per second: 118, episode reward: 230.064, mean reward:  0.396 [-19.481, 100.000], mean action: 0.954 [0.000, 3.000],  loss: 4.819347, mae: 20.480150, mean_q: 26.944067, mean_eps: 0.147430\n",
      " 142633/150000: episode: 927, duration: 1.923s, episode steps: 247, steps per second: 128, episode reward: 264.106, mean reward:  1.069 [-9.412, 100.000], mean action: 1.591 [0.000, 3.000],  loss: 3.832227, mae: 20.397477, mean_q: 26.960477, mean_eps: 0.144946\n",
      " 143290/150000: episode: 928, duration: 5.243s, episode steps: 657, steps per second: 125, episode reward: 264.894, mean reward:  0.403 [-20.802, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 4.712239, mae: 20.676616, mean_q: 27.316226, mean_eps: 0.142234\n",
      " 143426/150000: episode: 929, duration: 0.964s, episode steps: 136, steps per second: 141, episode reward: -5.396, mean reward: -0.040 [-100.000, 13.816], mean action: 1.971 [0.000, 3.000],  loss: 3.684033, mae: 20.610588, mean_q: 27.416432, mean_eps: 0.139855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 143771/150000: episode: 930, duration: 2.478s, episode steps: 345, steps per second: 139, episode reward: 236.485, mean reward:  0.685 [-14.019, 100.000], mean action: 1.522 [0.000, 3.000],  loss: 6.165199, mae: 20.828962, mean_q: 27.483780, mean_eps: 0.138412\n",
      " 144713/150000: episode: 931, duration: 7.255s, episode steps: 942, steps per second: 130, episode reward: 140.280, mean reward:  0.149 [-20.089, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 4.109369, mae: 20.768226, mean_q: 27.566711, mean_eps: 0.134551\n",
      " 145456/150000: episode: 932, duration: 5.700s, episode steps: 743, steps per second: 130, episode reward: 174.815, mean reward:  0.235 [-20.721, 100.000], mean action: 1.857 [0.000, 3.000],  loss: 4.049153, mae: 20.871992, mean_q: 27.780278, mean_eps: 0.129496\n",
      " 145878/150000: episode: 933, duration: 3.054s, episode steps: 422, steps per second: 138, episode reward: 254.777, mean reward:  0.604 [-10.768, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 4.593402, mae: 20.964077, mean_q: 27.897643, mean_eps: 0.126001\n",
      " 146179/150000: episode: 934, duration: 2.141s, episode steps: 301, steps per second: 141, episode reward: 233.057, mean reward:  0.774 [-16.102, 100.000], mean action: 1.910 [0.000, 3.000],  loss: 4.409941, mae: 21.271796, mean_q: 28.286290, mean_eps: 0.123832\n",
      " 146383/150000: episode: 935, duration: 1.413s, episode steps: 204, steps per second: 144, episode reward: 255.075, mean reward:  1.250 [-17.614, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 3.273195, mae: 21.267406, mean_q: 28.337403, mean_eps: 0.122317\n",
      " 146735/150000: episode: 936, duration: 2.506s, episode steps: 352, steps per second: 140, episode reward: 293.936, mean reward:  0.835 [-2.823, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 4.273325, mae: 21.510608, mean_q: 28.628458, mean_eps: 0.120649\n",
      " 146997/150000: episode: 937, duration: 1.847s, episode steps: 262, steps per second: 142, episode reward: 268.972, mean reward:  1.027 [-9.323, 100.000], mean action: 1.996 [0.000, 3.000],  loss: 2.696681, mae: 21.468177, mean_q: 28.636129, mean_eps: 0.118807\n",
      " 147551/150000: episode: 938, duration: 3.961s, episode steps: 554, steps per second: 140, episode reward: 265.810, mean reward:  0.480 [-18.879, 100.000], mean action: 1.025 [0.000, 3.000],  loss: 5.001952, mae: 21.751230, mean_q: 28.929986, mean_eps: 0.116359\n",
      " 147749/150000: episode: 939, duration: 1.354s, episode steps: 198, steps per second: 146, episode reward: 248.109, mean reward:  1.253 [-3.333, 100.000], mean action: 1.677 [0.000, 3.000],  loss: 4.176093, mae: 21.551781, mean_q: 28.628291, mean_eps: 0.114103\n",
      " 148749/150000: episode: 940, duration: 8.264s, episode steps: 1000, steps per second: 121, episode reward: -16.379, mean reward: -0.016 [-17.039, 12.359], mean action: 1.586 [0.000, 3.000],  loss: 4.684753, mae: 21.869294, mean_q: 29.063121, mean_eps: 0.110509\n",
      " 149240/150000: episode: 941, duration: 3.663s, episode steps: 491, steps per second: 134, episode reward: 242.262, mean reward:  0.493 [-19.211, 100.000], mean action: 1.132 [0.000, 3.000],  loss: 4.711354, mae: 21.938617, mean_q: 29.236764, mean_eps: 0.106036\n",
      " 149626/150000: episode: 942, duration: 2.919s, episode steps: 386, steps per second: 132, episode reward: 263.739, mean reward:  0.683 [-15.139, 100.000], mean action: 1.137 [0.000, 3.000],  loss: 3.910424, mae: 22.089773, mean_q: 29.402481, mean_eps: 0.103405\n",
      " 149862/150000: episode: 943, duration: 2.032s, episode steps: 236, steps per second: 116, episode reward: 231.513, mean reward:  0.981 [-14.715, 100.000], mean action: 1.585 [0.000, 3.000],  loss: 3.942204, mae: 21.693685, mean_q: 28.877046, mean_eps: 0.101539\n",
      "done, took 1086.260 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ab98598130>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=NUMBER_STEPS, visualize=False, verbose=2, callbacks=[WandbCallback()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b2b31f",
   "metadata": {},
   "source": [
    "Wow! After only some minutes of training, we achieve great results!\n",
    "The reason for this is, that keras-rl has implemented many optimization strategies (e.g the optimized replay buffer) which lead to a much faster convergence than our DQN implemented by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceramic-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training is done, we save the final weights.\n",
    "dqn.save_weights(f'OKv1_LunarLander_2x150000steps_y0,992_a0,001.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-symphony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=200, visualize=False, callbacks=[WandbCallback()])\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "804d3329",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights(f'OKv1_LunarLander_2x150000steps_y0,992_a0,001.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "mental-nirvana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 255.760, steps: 199\n",
      "Episode 2: reward: 228.585, steps: 346\n",
      "Episode 3: reward: 246.992, steps: 190\n",
      "Episode 4: reward: 270.068, steps: 192\n",
      "Episode 5: reward: 254.709, steps: 303\n"
     ]
    }
   ],
   "source": [
    "# Finally, evaluate our algorithm for 5 episodes.\n",
    "dqn.test(env, nb_episodes=5, visualize=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dc4dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
