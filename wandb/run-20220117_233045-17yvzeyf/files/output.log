Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 100000 steps ...
    90/100000: episode: 1, duration: 1.144s, episode steps:  90, steps per second:  79, episode reward: -176.039, mean reward: -1.956 [-100.000, 43.182], mean action: 1.356 [0.000, 3.000],  loss: 0.919073, mse: 1.753997, mean_q: -0.278132, mean_eps: 0.999505
   177/100000: episode: 2, duration: 0.590s, episode steps:  87, steps per second: 147, episode reward: -207.888, mean reward: -2.390 [-100.000, 22.632], mean action: 1.517 [0.000, 3.000],  loss: 25.607159, mse: 20.942771, mean_q: -0.477239, mean_eps: 0.998683
   285/100000: episode: 3, duration: 0.712s, episode steps: 108, steps per second: 152, episode reward: -365.742, mean reward: -3.386 [-100.000,  0.535], mean action: 1.704 [0.000, 3.000],  loss: 28.581446, mse: 29.902384, mean_q: 0.001510, mean_eps: 0.997718
   360/100000: episode: 4, duration: 0.486s, episode steps:  75, steps per second: 154, episode reward: -50.098, mean reward: -0.668 [-100.000, 81.092], mean action: 1.600 [0.000, 3.000],  loss: 35.200488, mse: 32.584795, mean_q: -0.579060, mean_eps: 0.996812
   450/100000: episode: 5, duration: 0.551s, episode steps:  90, steps per second: 163, episode reward: -260.523, mean reward: -2.895 [-100.000,  6.469], mean action: 1.522 [0.000, 3.000],  loss: 59.042517, mse: 55.188633, mean_q: 0.628216, mean_eps: 0.995995
   585/100000: episode: 6, duration: 0.847s, episode steps: 135, steps per second: 159, episode reward: -45.398, mean reward: -0.336 [-100.000, 21.003], mean action: 1.563 [0.000, 3.000],  loss: 62.482663, mse: 98.510105, mean_q: 2.191799, mean_eps: 0.994882
   697/100000: episode: 7, duration: 0.737s, episode steps: 112, steps per second: 152, episode reward: -205.554, mean reward: -1.835 [-100.000,  8.068], mean action: 1.518 [0.000, 3.000],  loss: 81.223935, mse: 214.391750, mean_q: 7.087138, mean_eps: 0.993659
   803/100000: episode: 8, duration: 0.720s, episode steps: 106, steps per second: 147, episode reward: -244.627, mean reward: -2.308 [-100.000, 33.796], mean action: 1.425 [0.000, 3.000],  loss: 94.408064, mse: 267.116969, mean_q: 9.978884, mean_eps: 0.992580
   877/100000: episode: 9, duration: 0.725s, episode steps:  74, steps per second: 102, episode reward: -134.631, mean reward: -1.819 [-100.000, 14.403], mean action: 1.378 [0.000, 3.000],  loss: 137.345497, mse: 378.163306, mean_q: 12.097770, mean_eps: 0.991689
   940/100000: episode: 10, duration: 0.733s, episode steps:  63, steps per second:  86, episode reward: -91.094, mean reward: -1.446 [-100.000,  6.726], mean action: 1.429 [0.000, 3.000],  loss: 192.004247, mse: 645.794077, mean_q: 15.506511, mean_eps: 0.991011
  1066/100000: episode: 11, duration: 0.897s, episode steps: 126, steps per second: 140, episode reward: -180.410, mean reward: -1.432 [-100.000,  3.324], mean action: 1.468 [0.000, 3.000],  loss: 230.903443, mse: 862.820152, mean_q: 17.897585, mean_eps: 0.990075
  1155/100000: episode: 12, duration: 0.775s, episode steps:  89, steps per second: 115, episode reward: -331.964, mean reward: -3.730 [-100.000,  1.531], mean action: 1.596 [0.000, 3.000],  loss: 234.730529, mse: 1045.161961, mean_q: 20.279999, mean_eps: 0.989011
  1214/100000: episode: 13, duration: 0.600s, episode steps:  59, steps per second:  98, episode reward: -82.751, mean reward: -1.403 [-100.000,  7.044], mean action: 1.288 [0.000, 3.000],  loss: 282.588203, mse: 1311.978485, mean_q: 21.321754, mean_eps: 0.988278
  1314/100000: episode: 14, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: -187.477, mean reward: -1.875 [-100.000,  6.712], mean action: 1.360 [0.000, 3.000],  loss: 253.615526, mse: 1803.857478, mean_q: 24.825285, mean_eps: 0.987491
  1402/100000: episode: 15, duration: 0.762s, episode steps:  88, steps per second: 115, episode reward: -13.369, mean reward: -0.152 [-100.000, 103.677], mean action: 1.386 [0.000, 3.000],  loss: 269.639812, mse: 1962.892813, mean_q: 25.401135, mean_eps: 0.986561
  1516/100000: episode: 16, duration: 0.729s, episode steps: 114, steps per second: 156, episode reward: -492.782, mean reward: -4.323 [-100.000,  1.321], mean action: 1.386 [0.000, 3.000],  loss: 390.685789, mse: 2579.415097, mean_q: 30.091330, mean_eps: 0.985561
  1578/100000: episode: 17, duration: 0.382s, episode steps:  62, steps per second: 162, episode reward: -66.643, mean reward: -1.075 [-100.000, 15.178], mean action: 1.339 [0.000, 3.000],  loss: 326.976373, mse: 2714.292884, mean_q: 30.343503, mean_eps: 0.984690
  1678/100000: episode: 18, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -114.251, mean reward: -1.143 [-100.000,  7.528], mean action: 1.360 [0.000, 3.000],  loss: 406.781672, mse: 3309.818168, mean_q: 34.286853, mean_eps: 0.983888
  1763/100000: episode: 19, duration: 0.525s, episode steps:  85, steps per second: 162, episode reward: -297.446, mean reward: -3.499 [-100.000, 107.109], mean action: 1.294 [0.000, 3.000],  loss: 389.389779, mse: 4093.580823, mean_q: 38.320464, mean_eps: 0.982972
  1878/100000: episode: 20, duration: 0.857s, episode steps: 115, steps per second: 134, episode reward: -179.193, mean reward: -1.558 [-100.000,  2.064], mean action: 1.452 [0.000, 3.000],  loss: 465.078856, mse: 5001.496948, mean_q: 41.579682, mean_eps: 0.981982
  1977/100000: episode: 21, duration: 0.750s, episode steps:  99, steps per second: 132, episode reward: 35.357, mean reward:  0.357 [-100.000, 100.977], mean action: 1.465 [0.000, 3.000],  loss: 507.839946, mse: 7089.823165, mean_q: 47.778502, mean_eps: 0.980923
  2036/100000: episode: 22, duration: 0.453s, episode steps:  59, steps per second: 130, episode reward: -88.853, mean reward: -1.506 [-100.000,  9.566], mean action: 1.627 [0.000, 3.000],  loss: 736.924721, mse: 8720.909591, mean_q: 53.140587, mean_eps: 0.980141
  2098/100000: episode: 23, duration: 0.425s, episode steps:  62, steps per second: 146, episode reward: -84.736, mean reward: -1.367 [-100.000, 20.493], mean action: 1.419 [0.000, 3.000],  loss: 852.468601, mse: 10040.824093, mean_q: 59.904271, mean_eps: 0.979542
  2169/100000: episode: 24, duration: 0.463s, episode steps:  71, steps per second: 153, episode reward: -49.633, mean reward: -0.699 [-100.000, 15.173], mean action: 1.423 [0.000, 3.000],  loss: 696.071387, mse: 11383.493800, mean_q: 64.135012, mean_eps: 0.978883
  2244/100000: episode: 25, duration: 0.490s, episode steps:  75, steps per second: 153, episode reward: -102.496, mean reward: -1.367 [-100.000, 12.580], mean action: 1.387 [0.000, 3.000],  loss: 676.501768, mse: 13847.149766, mean_q: 73.352891, mean_eps: 0.978161
  2349/100000: episode: 26, duration: 0.681s, episode steps: 105, steps per second: 154, episode reward: -304.227, mean reward: -2.897 [-100.000,  1.120], mean action: 1.581 [0.000, 3.000],  loss: 678.370305, mse: 15885.849223, mean_q: 77.602554, mean_eps: 0.977270
  2410/100000: episode: 27, duration: 0.413s, episode steps:  61, steps per second: 148, episode reward: -82.164, mean reward: -1.347 [-100.000,  9.820], mean action: 1.197 [0.000, 3.000],  loss: 596.510206, mse: 17652.468518, mean_q: 79.861451, mean_eps: 0.976448
  2478/100000: episode: 28, duration: 0.442s, episode steps:  68, steps per second: 154, episode reward: -124.685, mean reward: -1.834 [-100.000, 56.018], mean action: 1.456 [0.000, 3.000],  loss: 736.407228, mse: 18237.724868, mean_q: 80.901469, mean_eps: 0.975809
  2580/100000: episode: 29, duration: 0.628s, episode steps: 102, steps per second: 162, episode reward: -99.866, mean reward: -0.979 [-100.000, 13.878], mean action: 1.333 [0.000, 3.000],  loss: 559.462655, mse: 19150.868595, mean_q: 83.127985, mean_eps: 0.974968
  2736/100000: episode: 30, duration: 1.210s, episode steps: 156, steps per second: 129, episode reward: -75.316, mean reward: -0.483 [-100.000, 57.470], mean action: 1.487 [0.000, 3.000],  loss: 584.698119, mse: 21187.668620, mean_q: 88.497549, mean_eps: 0.973691
  2798/100000: episode: 31, duration: 0.433s, episode steps:  62, steps per second: 143, episode reward: -90.219, mean reward: -1.455 [-100.000, 15.449], mean action: 1.323 [0.000, 3.000],  loss: 472.845363, mse: 23190.118747, mean_q: 95.195250, mean_eps: 0.972612
  2924/100000: episode: 32, duration: 0.753s, episode steps: 126, steps per second: 167, episode reward: -306.335, mean reward: -2.431 [-100.000, 25.684], mean action: 1.524 [0.000, 3.000],  loss: 476.773668, mse: 22380.713449, mean_q: 95.738380, mean_eps: 0.971681
  3016/100000: episode: 33, duration: 0.573s, episode steps:  92, steps per second: 161, episode reward: -119.168, mean reward: -1.295 [-100.000, 13.435], mean action: 1.467 [0.000, 3.000],  loss: 411.261965, mse: 23169.537767, mean_q: 97.278878, mean_eps: 0.970602
  3091/100000: episode: 34, duration: 0.588s, episode steps:  75, steps per second: 128, episode reward: -97.524, mean reward: -1.300 [-100.000, 10.652], mean action: 1.653 [0.000, 3.000],  loss: 368.660735, mse: 21531.960117, mean_q: 95.840772, mean_eps: 0.969775
  3187/100000: episode: 35, duration: 0.589s, episode steps:  96, steps per second: 163, episode reward: -320.995, mean reward: -3.344 [-100.000,  0.921], mean action: 1.646 [0.000, 3.000],  loss: 400.024159, mse: 23188.602234, mean_q: 98.146911, mean_eps: 0.968929
  3280/100000: episode: 36, duration: 0.606s, episode steps:  93, steps per second: 153, episode reward: -181.307, mean reward: -1.950 [-100.000, 34.295], mean action: 1.634 [0.000, 3.000],  loss: 418.564589, mse: 24566.365812, mean_q: 104.049358, mean_eps: 0.967993
  3375/100000: episode: 37, duration: 0.655s, episode steps:  95, steps per second: 145, episode reward: -344.290, mean reward: -3.624 [-100.000,  1.631], mean action: 1.221 [0.000, 3.000],  loss: 351.382809, mse: 24327.314762, mean_q: 104.643542, mean_eps: 0.967063
  3486/100000: episode: 38, duration: 0.770s, episode steps: 111, steps per second: 144, episode reward: -229.291, mean reward: -2.066 [-100.000, 62.193], mean action: 1.486 [0.000, 3.000],  loss: 211.937733, mse: 24340.842316, mean_q: 104.446954, mean_eps: 0.966043
  3594/100000: episode: 39, duration: 0.657s, episode steps: 108, steps per second: 164, episode reward: -115.075, mean reward: -1.066 [-100.000, 11.015], mean action: 1.713 [0.000, 3.000],  loss: 232.776206, mse: 26219.004720, mean_q: 110.999457, mean_eps: 0.964959
  3712/100000: episode: 40, duration: 0.768s, episode steps: 118, steps per second: 154, episode reward: -205.945, mean reward: -1.745 [-100.000, 49.834], mean action: 1.508 [0.000, 3.000],  loss: 187.622719, mse: 27906.624495, mean_q: 114.891298, mean_eps: 0.963840
  3804/100000: episode: 41, duration: 0.573s, episode steps:  92, steps per second: 161, episode reward: -199.615, mean reward: -2.170 [-100.000,  9.820], mean action: 1.467 [0.000, 3.000],  loss: 207.778730, mse: 28493.457477, mean_q: 116.615415, mean_eps: 0.962801
  3912/100000: episode: 42, duration: 0.901s, episode steps: 108, steps per second: 120, episode reward: -306.430, mean reward: -2.837 [-100.000,  1.546], mean action: 1.472 [0.000, 3.000],  loss: 122.704933, mse: 30066.955729, mean_q: 119.428517, mean_eps: 0.961811
  4014/100000: episode: 43, duration: 0.813s, episode steps: 102, steps per second: 125, episode reward: -388.448, mean reward: -3.808 [-100.000,  1.362], mean action: 1.402 [0.000, 3.000],  loss: 93.085969, mse: 31114.081476, mean_q: 123.149216, mean_eps: 0.960771
  4092/100000: episode: 44, duration: 0.571s, episode steps:  78, steps per second: 137, episode reward: -116.885, mean reward: -1.499 [-100.000, 12.858], mean action: 1.692 [0.000, 3.000],  loss: 73.894806, mse: 31983.991862, mean_q: 126.642083, mean_eps: 0.959880
  4196/100000: episode: 45, duration: 0.859s, episode steps: 104, steps per second: 121, episode reward: -257.284, mean reward: -2.474 [-100.000,  5.275], mean action: 1.365 [0.000, 3.000],  loss: 61.637815, mse: 36091.350980, mean_q: 135.036128, mean_eps: 0.958979
  4275/100000: episode: 46, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -115.000, mean reward: -1.456 [-100.000, 11.281], mean action: 1.380 [0.000, 3.000],  loss: 70.690417, mse: 38893.317815, mean_q: 142.507285, mean_eps: 0.958073
  4348/100000: episode: 47, duration: 0.506s, episode steps:  73, steps per second: 144, episode reward: -307.590, mean reward: -4.214 [-100.000,  3.524], mean action: 1.753 [0.000, 3.000],  loss: 60.448632, mse: 41040.515143, mean_q: 146.773526, mean_eps: 0.957321
  4439/100000: episode: 48, duration: 0.552s, episode steps:  91, steps per second: 165, episode reward: -101.071, mean reward: -1.111 [-100.000, 12.499], mean action: 1.648 [0.000, 3.000],  loss: 77.847207, mse: 46311.579949, mean_q: 157.415922, mean_eps: 0.956509
  4529/100000: episode: 49, duration: 0.546s, episode steps:  90, steps per second: 165, episode reward: -255.657, mean reward: -2.841 [-100.000, 100.529], mean action: 1.400 [0.000, 3.000],  loss: 58.346042, mse: 48022.838151, mean_q: 160.757404, mean_eps: 0.955613
  4607/100000: episode: 50, duration: 0.494s, episode steps:  78, steps per second: 158, episode reward: -171.881, mean reward: -2.204 [-100.000,  8.231], mean action: 1.513 [0.000, 3.000],  loss: 56.954380, mse: 49869.975010, mean_q: 164.379679, mean_eps: 0.954782
  4715/100000: episode: 51, duration: 0.730s, episode steps: 108, steps per second: 148, episode reward: -161.333, mean reward: -1.494 [-100.000, 99.758], mean action: 1.556 [0.000, 3.000],  loss: 50.381227, mse: 53606.117133, mean_q: 175.368369, mean_eps: 0.953861
  4831/100000: episode: 52, duration: 0.893s, episode steps: 116, steps per second: 130, episode reward: -174.711, mean reward: -1.506 [-100.000,  5.525], mean action: 1.647 [0.000, 3.000],  loss: 59.001241, mse: 57979.395053, mean_q: 183.410823, mean_eps: 0.952752
  4937/100000: episode: 53, duration: 0.711s, episode steps: 106, steps per second: 149, episode reward: -145.532, mean reward: -1.373 [-100.000, 10.252], mean action: 1.547 [0.000, 3.000],  loss: 62.484868, mse: 61545.686265, mean_q: 190.841041, mean_eps: 0.951653
  5031/100000: episode: 54, duration: 0.583s, episode steps:  94, steps per second: 161, episode reward: -118.500, mean reward: -1.261 [-100.000,  8.928], mean action: 1.479 [0.000, 3.000],  loss: 60.041440, mse: 68249.440284, mean_q: 203.727346, mean_eps: 0.950663
  5144/100000: episode: 55, duration: 0.687s, episode steps: 113, steps per second: 164, episode reward: -284.321, mean reward: -2.516 [-100.000,  6.993], mean action: 1.558 [0.000, 3.000],  loss: 49.856305, mse: 77080.839844, mean_q: 217.701538, mean_eps: 0.949639
  5239/100000: episode: 56, duration: 0.571s, episode steps:  95, steps per second: 166, episode reward: -124.940, mean reward: -1.315 [-100.000,  6.358], mean action: 1.674 [0.000, 3.000],  loss: 60.466243, mse: 89095.803125, mean_q: 239.651400, mean_eps: 0.948609
  5305/100000: episode: 57, duration: 0.478s, episode steps:  66, steps per second: 138, episode reward: -92.873, mean reward: -1.407 [-100.000,  9.819], mean action: 1.621 [0.000, 3.000],  loss: 57.912423, mse: 94067.027995, mean_q: 244.775899, mean_eps: 0.947812
  5384/100000: episode: 58, duration: 0.539s, episode steps:  79, steps per second: 146, episode reward: -271.912, mean reward: -3.442 [-100.000,  2.119], mean action: 1.595 [0.000, 3.000],  loss: 65.067113, mse: 102386.223052, mean_q: 262.553103, mean_eps: 0.947094
  5445/100000: episode: 59, duration: 0.490s, episode steps:  61, steps per second: 125, episode reward: -139.995, mean reward: -2.295 [-100.000, 31.606], mean action: 1.279 [0.000, 3.000],  loss: 68.736722, mse: 104775.068263, mean_q: 266.202034, mean_eps: 0.946401
  5520/100000: episode: 60, duration: 0.636s, episode steps:  75, steps per second: 118, episode reward: -257.510, mean reward: -3.433 [-100.000, 112.824], mean action: 1.253 [0.000, 3.000],  loss: 68.125808, mse: 109028.912500, mean_q: 273.145910, mean_eps: 0.945728
  5603/100000: episode: 61, duration: 0.727s, episode steps:  83, steps per second: 114, episode reward: -244.016, mean reward: -2.940 [-100.000, 39.099], mean action: 1.735 [0.000, 3.000],  loss: 64.132751, mse: 111140.501459, mean_q: 280.602188, mean_eps: 0.944946
  5673/100000: episode: 62, duration: 0.546s, episode steps:  70, steps per second: 128, episode reward: -63.121, mean reward: -0.902 [-100.000, 21.213], mean action: 1.629 [0.000, 3.000],  loss: 66.177336, mse: 120192.288728, mean_q: 291.926517, mean_eps: 0.944189
  5756/100000: episode: 63, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -99.470, mean reward: -1.198 [-100.000, 15.157], mean action: 1.422 [0.000, 3.000],  loss: 75.068147, mse: 129357.159639, mean_q: 305.625754, mean_eps: 0.943431
  5841/100000: episode: 64, duration: 0.624s, episode steps:  85, steps per second: 136, episode reward: -108.216, mean reward: -1.273 [-100.000,  6.263], mean action: 1.612 [0.000, 3.000],  loss: 76.458070, mse: 133099.613695, mean_q: 312.201070, mean_eps: 0.942600
  5913/100000: episode: 65, duration: 0.589s, episode steps:  72, steps per second: 122, episode reward: -110.624, mean reward: -1.536 [-100.000,  9.559], mean action: 1.500 [0.000, 3.000],  loss: 67.538375, mse: 141223.495226, mean_q: 324.826992, mean_eps: 0.941823
  6040/100000: episode: 66, duration: 0.878s, episode steps: 127, steps per second: 145, episode reward: -117.428, mean reward: -0.925 [-100.000, 10.356], mean action: 1.496 [0.000, 3.000],  loss: 75.841892, mse: 154418.277805, mean_q: 341.987381, mean_eps: 0.940838
  6136/100000: episode: 67, duration: 0.697s, episode steps:  96, steps per second: 138, episode reward: -261.687, mean reward: -2.726 [-100.000, 78.751], mean action: 1.573 [0.000, 3.000],  loss: 73.554962, mse: 162895.718913, mean_q: 352.380045, mean_eps: 0.939734
  6245/100000: episode: 68, duration: 0.804s, episode steps: 109, steps per second: 136, episode reward: -444.029, mean reward: -4.074 [-100.000,  1.422], mean action: 1.569 [0.000, 3.000],  loss: 79.570927, mse: 179076.075903, mean_q: 369.349685, mean_eps: 0.938719
  6331/100000: episode: 69, duration: 0.612s, episode steps:  86, steps per second: 141, episode reward: -114.272, mean reward: -1.329 [-100.000,  7.593], mean action: 1.523 [0.000, 3.000],  loss: 74.962242, mse: 185908.676781, mean_q: 378.281303, mean_eps: 0.937754
  6424/100000: episode: 70, duration: 0.637s, episode steps:  93, steps per second: 146, episode reward: -85.189, mean reward: -0.916 [-100.000, 10.016], mean action: 1.473 [0.000, 3.000],  loss: 81.305549, mse: 201588.254116, mean_q: 399.410900, mean_eps: 0.936868
  6503/100000: episode: 71, duration: 0.529s, episode steps:  79, steps per second: 149, episode reward: -142.342, mean reward: -1.802 [-100.000, 22.753], mean action: 1.519 [0.000, 3.000],  loss: 78.235801, mse: 200999.274921, mean_q: 400.369661, mean_eps: 0.936016
  6581/100000: episode: 72, duration: 0.525s, episode steps:  78, steps per second: 148, episode reward: -211.789, mean reward: -2.715 [-100.000,  7.754], mean action: 1.423 [0.000, 3.000],  loss: 75.645079, mse: 216575.495192, mean_q: 411.231073, mean_eps: 0.935239
  6651/100000: episode: 73, duration: 0.510s, episode steps:  70, steps per second: 137, episode reward: -89.416, mean reward: -1.277 [-100.000,  9.682], mean action: 1.657 [0.000, 3.000],  loss: 90.332873, mse: 225442.073214, mean_q: 425.882422, mean_eps: 0.934507
  6735/100000: episode: 74, duration: 0.645s, episode steps:  84, steps per second: 130, episode reward: -246.882, mean reward: -2.939 [-100.000, 29.164], mean action: 1.702 [0.000, 3.000],  loss: 82.009556, mse: 235302.505952, mean_q: 433.122626, mean_eps: 0.933744
  6816/100000: episode: 75, duration: 0.529s, episode steps:  81, steps per second: 153, episode reward: -120.875, mean reward: -1.492 [-100.000,  6.031], mean action: 1.617 [0.000, 3.000],  loss: 76.406072, mse: 240820.780478, mean_q: 438.288246, mean_eps: 0.932928
  6898/100000: episode: 76, duration: 0.615s, episode steps:  82, steps per second: 133, episode reward: -95.010, mean reward: -1.159 [-100.000, 22.505], mean action: 1.744 [0.000, 3.000],  loss: 57.589687, mse: 254582.803544, mean_q: 448.582199, mean_eps: 0.932121
  6996/100000: episode: 77, duration: 0.640s, episode steps:  98, steps per second: 153, episode reward: -214.272, mean reward: -2.186 [-100.000, 77.366], mean action: 1.449 [0.000, 3.000],  loss: 78.631923, mse: 274239.951690, mean_q: 474.723278, mean_eps: 0.931230
  7073/100000: episode: 78, duration: 0.561s, episode steps:  77, steps per second: 137, episode reward: -269.248, mean reward: -3.497 [-100.000, 15.320], mean action: 1.857 [0.000, 3.000],  loss: 98.232788, mse: 290924.466315, mean_q: 485.079297, mean_eps: 0.930363
  7141/100000: episode: 79, duration: 0.577s, episode steps:  68, steps per second: 118, episode reward: -114.934, mean reward: -1.690 [-100.000, 32.036], mean action: 1.544 [0.000, 3.000],  loss: 93.210810, mse: 288730.104320, mean_q: 480.133644, mean_eps: 0.929646
  7209/100000: episode: 80, duration: 0.476s, episode steps:  68, steps per second: 143, episode reward: -83.894, mean reward: -1.234 [-100.000, 11.533], mean action: 1.618 [0.000, 3.000],  loss: 82.187194, mse: 305161.261719, mean_q: 499.552116, mean_eps: 0.928972
  7312/100000: episode: 81, duration: 0.639s, episode steps: 103, steps per second: 161, episode reward: -292.404, mean reward: -2.839 [-100.000, 59.516], mean action: 1.544 [0.000, 3.000],  loss: 85.346458, mse: 315167.898362, mean_q: 504.217416, mean_eps: 0.928126
  7374/100000: episode: 82, duration: 0.420s, episode steps:  62, steps per second: 148, episode reward: -46.721, mean reward: -0.754 [-100.000, 11.789], mean action: 1.387 [0.000, 3.000],  loss: 75.315711, mse: 326631.582913, mean_q: 508.660988, mean_eps: 0.927309
  7446/100000: episode: 83, duration: 0.494s, episode steps:  72, steps per second: 146, episode reward: -180.510, mean reward: -2.507 [-100.000, 27.288], mean action: 1.458 [0.000, 3.000],  loss: 91.540122, mse: 347413.910156, mean_q: 525.176851, mean_eps: 0.926646
  7516/100000: episode: 84, duration: 0.536s, episode steps:  70, steps per second: 131, episode reward: -97.170, mean reward: -1.388 [-100.000,  7.669], mean action: 1.529 [0.000, 3.000],  loss: 85.428236, mse: 359564.325000, mean_q: 537.893795, mean_eps: 0.925943
  7635/100000: episode: 85, duration: 0.938s, episode steps: 119, steps per second: 127, episode reward: -172.716, mean reward: -1.451 [-100.000,  8.378], mean action: 1.613 [0.000, 3.000],  loss: 92.331931, mse: 378246.841124, mean_q: 550.926938, mean_eps: 0.925007
  7740/100000: episode: 86, duration: 0.808s, episode steps: 105, steps per second: 130, episode reward: -265.993, mean reward: -2.533 [-100.000,  5.497], mean action: 1.657 [0.000, 3.000],  loss: 80.022972, mse: 391279.152679, mean_q: 559.664998, mean_eps: 0.923899
  7803/100000: episode: 87, duration: 0.448s, episode steps:  63, steps per second: 141, episode reward: -78.728, mean reward: -1.250 [-100.000,  8.129], mean action: 1.460 [0.000, 3.000],  loss: 75.539217, mse: 389425.669147, mean_q: 552.856138, mean_eps: 0.923067
  7916/100000: episode: 88, duration: 0.807s, episode steps: 113, steps per second: 140, episode reward: -226.826, mean reward: -2.007 [-100.000,  6.846], mean action: 1.540 [0.000, 3.000],  loss: 88.958057, mse: 409081.021018, mean_q: 562.289464, mean_eps: 0.922196
  8002/100000: episode: 89, duration: 0.649s, episode steps:  86, steps per second: 133, episode reward: -89.633, mean reward: -1.042 [-100.000,  8.227], mean action: 1.442 [0.000, 3.000],  loss: 64.242914, mse: 428071.370276, mean_q: 576.304809, mean_eps: 0.921211
  8070/100000: episode: 90, duration: 0.450s, episode steps:  68, steps per second: 151, episode reward: -122.772, mean reward: -1.805 [-100.000, 24.135], mean action: 1.618 [0.000, 3.000],  loss: 74.087391, mse: 431466.609835, mean_q: 575.455790, mean_eps: 0.920449
  8143/100000: episode: 91, duration: 0.504s, episode steps:  73, steps per second: 145, episode reward: -61.402, mean reward: -0.841 [-100.000,  9.777], mean action: 1.616 [0.000, 3.000],  loss: 82.239433, mse: 440209.163955, mean_q: 583.401586, mean_eps: 0.919751
  8256/100000: episode: 92, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -144.093, mean reward: -1.275 [-100.000,  3.092], mean action: 1.531 [0.000, 3.000],  loss: 76.003433, mse: 444917.148507, mean_q: 578.963721, mean_eps: 0.918830
  8327/100000: episode: 93, duration: 0.505s, episode steps:  71, steps per second: 141, episode reward: -128.120, mean reward: -1.805 [-100.000, 14.154], mean action: 1.268 [0.000, 3.000],  loss: 76.809068, mse: 466745.907130, mean_q: 599.483247, mean_eps: 0.917919
  8448/100000: episode: 94, duration: 0.811s, episode steps: 121, steps per second: 149, episode reward: -117.971, mean reward: -0.975 [-100.000,  8.043], mean action: 1.529 [0.000, 3.000],  loss: 74.010111, mse: 474228.885847, mean_q: 599.102226, mean_eps: 0.916969
  8542/100000: episode: 95, duration: 0.606s, episode steps:  94, steps per second: 155, episode reward: -140.600, mean reward: -1.496 [-100.000, 43.867], mean action: 1.691 [0.000, 3.000],  loss: 73.561606, mse: 477510.220080, mean_q: 595.462103, mean_eps: 0.915904
  8655/100000: episode: 96, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -151.224, mean reward: -1.338 [-100.000,  6.260], mean action: 1.407 [0.000, 3.000],  loss: 66.524667, mse: 478521.204646, mean_q: 595.327113, mean_eps: 0.914880
  8715/100000: episode: 97, duration: 0.376s, episode steps:  60, steps per second: 160, episode reward: -104.238, mean reward: -1.737 [-100.000, 16.865], mean action: 1.433 [0.000, 3.000],  loss: 71.696503, mse: 494519.682292, mean_q: 602.229037, mean_eps: 0.914023
  8799/100000: episode: 98, duration: 0.556s, episode steps:  84, steps per second: 151, episode reward: -260.376, mean reward: -3.100 [-100.000,  6.447], mean action: 1.369 [0.000, 3.000],  loss: 57.382728, mse: 481060.140625, mean_q: 590.405087, mean_eps: 0.913311
  8893/100000: episode: 99, duration: 0.613s, episode steps:  94, steps per second: 153, episode reward: -121.394, mean reward: -1.291 [-100.000,  6.828], mean action: 1.447 [0.000, 3.000],  loss: 60.473618, mse: 485042.330785, mean_q: 588.393361, mean_eps: 0.912430
  9023/100000: episode: 100, duration: 0.885s, episode steps: 130, steps per second: 147, episode reward: -331.908, mean reward: -2.553 [-100.000, 60.426], mean action: 1.546 [0.000, 3.000],  loss: 70.477683, mse: 499580.152885, mean_q: 601.548574, mean_eps: 0.911321
  9095/100000: episode: 101, duration: 0.509s, episode steps:  72, steps per second: 141, episode reward: -140.778, mean reward: -1.955 [-100.000,  5.277], mean action: 1.514 [0.000, 3.000],  loss: 56.174260, mse: 482119.282118, mean_q: 577.787843, mean_eps: 0.910321
  9164/100000: episode: 102, duration: 0.529s, episode steps:  69, steps per second: 131, episode reward: -58.380, mean reward: -0.846 [-100.000,  7.689], mean action: 1.522 [0.000, 3.000],  loss: 65.890822, mse: 495377.443388, mean_q: 588.884688, mean_eps: 0.909623
  9235/100000: episode: 103, duration: 0.552s, episode steps:  71, steps per second: 129, episode reward: -136.796, mean reward: -1.927 [-100.000,  7.980], mean action: 1.521 [0.000, 3.000],  loss: 59.145031, mse: 500333.982835, mean_q: 590.187099, mean_eps: 0.908930
  9341/100000: episode: 104, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: -263.230, mean reward: -2.483 [-100.000, 58.907], mean action: 1.623 [0.000, 3.000],  loss: 55.125157, mse: 479054.978774, mean_q: 569.275913, mean_eps: 0.908054
  9467/100000: episode: 105, duration: 0.782s, episode steps: 126, steps per second: 161, episode reward: -443.372, mean reward: -3.519 [-100.000, 58.902], mean action: 1.524 [0.000, 3.000],  loss: 58.900124, mse: 481012.549603, mean_q: 566.752927, mean_eps: 0.906905
  9535/100000: episode: 106, duration: 0.448s, episode steps:  68, steps per second: 152, episode reward: -86.904, mean reward: -1.278 [-100.000, 14.604], mean action: 1.574 [0.000, 3.000],  loss: 60.781559, mse: 480849.088695, mean_q: 564.718399, mean_eps: 0.905945
  9598/100000: episode: 107, duration: 0.403s, episode steps:  63, steps per second: 156, episode reward: -115.486, mean reward: -1.833 [-100.000,  9.252], mean action: 1.651 [0.000, 3.000],  loss: 46.208919, mse: 475471.712798, mean_q: 556.725115, mean_eps: 0.905297
  9667/100000: episode: 108, duration: 0.464s, episode steps:  69, steps per second: 149, episode reward: -195.486, mean reward: -2.833 [-100.000,  4.645], mean action: 1.435 [0.000, 3.000],  loss: 54.609631, mse: 500261.283967, mean_q: 581.560147, mean_eps: 0.904643
  9788/100000: episode: 109, duration: 0.744s, episode steps: 121, steps per second: 163, episode reward: -123.606, mean reward: -1.022 [-100.000,  8.036], mean action: 1.645 [0.000, 3.000],  loss: 52.362156, mse: 477840.105114, mean_q: 560.270583, mean_eps: 0.903703
  9903/100000: episode: 110, duration: 0.726s, episode steps: 115, steps per second: 158, episode reward: -205.660, mean reward: -1.788 [-100.000, 15.698], mean action: 1.583 [0.000, 3.000],  loss: 46.970394, mse: 473476.042391, mean_q: 547.749665, mean_eps: 0.902535
 10014/100000: episode: 111, duration: 0.692s, episode steps: 111, steps per second: 160, episode reward: -401.068, mean reward: -3.613 [-100.000, 72.914], mean action: 1.486 [0.000, 3.000],  loss: 52.305917, mse: 486830.122185, mean_q: 562.290692, mean_eps: 0.901416
 10138/100000: episode: 112, duration: 0.764s, episode steps: 124, steps per second: 162, episode reward: -157.555, mean reward: -1.271 [-100.000,  7.092], mean action: 1.532 [0.000, 3.000],  loss: 49.327933, mse: 481756.425655, mean_q: 549.910930, mean_eps: 0.900253
 10209/100000: episode: 113, duration: 0.487s, episode steps:  71, steps per second: 146, episode reward: -369.460, mean reward: -5.204 [-100.000, 106.624], mean action: 1.775 [0.000, 3.000],  loss: 56.380837, mse: 465711.697623, mean_q: 537.031574, mean_eps: 0.899287
 10293/100000: episode: 114, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -104.393, mean reward: -1.243 [-100.000,  6.780], mean action: 1.631 [0.000, 3.000],  loss: 51.876765, mse: 497321.559896, mean_q: 561.659108, mean_eps: 0.898520
 10404/100000: episode: 115, duration: 0.684s, episode steps: 111, steps per second: 162, episode reward: -114.843, mean reward: -1.035 [-100.000, 10.357], mean action: 1.568 [0.000, 3.000],  loss: 47.688946, mse: 482560.106700, mean_q: 543.661955, mean_eps: 0.897555
 10498/100000: episode: 116, duration: 0.587s, episode steps:  94, steps per second: 160, episode reward: -122.544, mean reward: -1.304 [-100.000, 22.726], mean action: 1.500 [0.000, 3.000],  loss: 52.170734, mse: 485114.487699, mean_q: 545.357480, mean_eps: 0.896540
 10605/100000: episode: 117, duration: 0.812s, episode steps: 107, steps per second: 132, episode reward:  7.228, mean reward:  0.068 [-100.000, 87.901], mean action: 1.477 [0.000, 3.000],  loss: 51.394212, mse: 474379.481016, mean_q: 536.256240, mean_eps: 0.895545
 10699/100000: episode: 118, duration: 0.671s, episode steps:  94, steps per second: 140, episode reward: -291.808, mean reward: -3.104 [-100.000,  0.275], mean action: 1.649 [0.000, 3.000],  loss: 47.606818, mse: 455464.796875, mean_q: 513.864068, mean_eps: 0.894550
 10829/100000: episode: 119, duration: 0.962s, episode steps: 130, steps per second: 135, episode reward: -183.659, mean reward: -1.413 [-100.000,  4.337], mean action: 1.538 [0.000, 3.000],  loss: 52.666226, mse: 469366.576322, mean_q: 528.948393, mean_eps: 0.893441
 10917/100000: episode: 120, duration: 0.638s, episode steps:  88, steps per second: 138, episode reward: -54.122, mean reward: -0.615 [-100.000, 28.835], mean action: 1.489 [0.000, 3.000],  loss: 52.653822, mse: 468877.947088, mean_q: 527.508812, mean_eps: 0.892362
 11060/100000: episode: 121, duration: 1.016s, episode steps: 143, steps per second: 141, episode reward: -109.689, mean reward: -0.767 [-100.000,  9.095], mean action: 1.573 [0.000, 3.000],  loss: 52.136523, mse: 463220.448099, mean_q: 522.235670, mean_eps: 0.891219
 11230/100000: episode: 122, duration: 1.175s, episode steps: 170, steps per second: 145, episode reward: -129.170, mean reward: -0.760 [-100.000, 21.073], mean action: 1.600 [0.000, 3.000],  loss: 52.699875, mse: 463828.500551, mean_q: 522.600865, mean_eps: 0.889669
 11351/100000: episode: 123, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward: -302.245, mean reward: -2.498 [-100.000,  0.866], mean action: 1.529 [0.000, 3.000],  loss: 58.252920, mse: 464511.766787, mean_q: 520.948749, mean_eps: 0.888229
 11487/100000: episode: 124, duration: 1.342s, episode steps: 136, steps per second: 101, episode reward: -385.109, mean reward: -2.832 [-100.000, 120.776], mean action: 1.640 [0.000, 3.000],  loss: 55.089982, mse: 470527.134766, mean_q: 525.757498, mean_eps: 0.886957
 11591/100000: episode: 125, duration: 0.798s, episode steps: 104, steps per second: 130, episode reward: -272.452, mean reward: -2.620 [-100.000, 29.323], mean action: 1.731 [0.000, 3.000],  loss: 55.948235, mse: 465128.865685, mean_q: 520.254067, mean_eps: 0.885769
 11711/100000: episode: 126, duration: 1.140s, episode steps: 120, steps per second: 105, episode reward: -177.354, mean reward: -1.478 [-100.000,  3.317], mean action: 1.542 [0.000, 3.000],  loss: 56.636123, mse: 483349.014062, mean_q: 536.877009, mean_eps: 0.884660
 11785/100000: episode: 127, duration: 0.584s, episode steps:  74, steps per second: 127, episode reward: -145.889, mean reward: -1.971 [-100.000, 19.334], mean action: 1.365 [0.000, 3.000],  loss: 52.686372, mse: 462003.792652, mean_q: 515.283258, mean_eps: 0.883700
 11928/100000: episode: 128, duration: 1.061s, episode steps: 143, steps per second: 135, episode reward: -155.817, mean reward: -1.090 [-100.000,  9.977], mean action: 1.524 [0.000, 3.000],  loss: 59.826804, mse: 447706.535184, mean_q: 504.194788, mean_eps: 0.882626
 12055/100000: episode: 129, duration: 0.865s, episode steps: 127, steps per second: 147, episode reward: -174.864, mean reward: -1.377 [-100.000, 62.094], mean action: 1.638 [0.000, 3.000],  loss: 56.062382, mse: 460972.454355, mean_q: 515.179831, mean_eps: 0.881289
 12196/100000: episode: 130, duration: 0.979s, episode steps: 141, steps per second: 144, episode reward: -44.708, mean reward: -0.317 [-100.000, 17.584], mean action: 1.511 [0.000, 3.000],  loss: 57.713553, mse: 454041.846631, mean_q: 510.381516, mean_eps: 0.879962
 12297/100000: episode: 131, duration: 0.817s, episode steps: 101, steps per second: 124, episode reward: -180.864, mean reward: -1.791 [-100.000,  1.468], mean action: 1.614 [0.000, 3.000],  loss: 58.881039, mse: 471568.274134, mean_q: 524.565608, mean_eps: 0.878765
 12404/100000: episode: 132, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -188.268, mean reward: -1.760 [-100.000, 14.598], mean action: 1.374 [0.000, 3.000],  loss: 62.056680, mse: 449886.620327, mean_q: 507.401110, mean_eps: 0.877735
 12501/100000: episode: 133, duration: 0.664s, episode steps:  97, steps per second: 146, episode reward: -60.564, mean reward: -0.624 [-100.000, 89.032], mean action: 1.619 [0.000, 3.000],  loss: 52.550141, mse: 445888.293492, mean_q: 501.214982, mean_eps: 0.876725
 12647/100000: episode: 134, duration: 1.019s, episode steps: 146, steps per second: 143, episode reward: -125.382, mean reward: -0.859 [-100.000,  6.197], mean action: 1.445 [0.000, 3.000],  loss: 58.378479, mse: 451697.621254, mean_q: 504.731696, mean_eps: 0.875522
 12773/100000: episode: 135, duration: 0.808s, episode steps: 126, steps per second: 156, episode reward: -57.482, mean reward: -0.456 [-100.000, 15.697], mean action: 1.563 [0.000, 3.000],  loss: 59.406682, mse: 457073.995660, mean_q: 512.813081, mean_eps: 0.874176
 12852/100000: episode: 136, duration: 0.499s, episode steps:  79, steps per second: 158, episode reward: -126.880, mean reward: -1.606 [-100.000, 19.812], mean action: 1.595 [0.000, 3.000],  loss: 55.427677, mse: 453539.280854, mean_q: 500.050504, mean_eps: 0.873161
 12984/100000: episode: 137, duration: 0.882s, episode steps: 132, steps per second: 150, episode reward: -120.628, mean reward: -0.914 [-100.000, 28.366], mean action: 1.477 [0.000, 3.000],  loss: 58.525826, mse: 457676.448627, mean_q: 509.692494, mean_eps: 0.872117
 13064/100000: episode: 138, duration: 0.519s, episode steps:  80, steps per second: 154, episode reward: -101.186, mean reward: -1.265 [-100.000, 40.578], mean action: 1.712 [0.000, 3.000],  loss: 51.554034, mse: 456403.607812, mean_q: 502.985271, mean_eps: 0.871067
 13183/100000: episode: 139, duration: 0.782s, episode steps: 119, steps per second: 152, episode reward: -107.425, mean reward: -0.903 [-100.000,  6.882], mean action: 1.571 [0.000, 3.000],  loss: 57.066693, mse: 434363.651786, mean_q: 485.726136, mean_eps: 0.870082
 13293/100000: episode: 140, duration: 0.749s, episode steps: 110, steps per second: 147, episode reward: -117.082, mean reward: -1.064 [-100.000, 11.384], mean action: 1.464 [0.000, 3.000],  loss: 59.728462, mse: 450892.936648, mean_q: 501.625109, mean_eps: 0.868949
 13440/100000: episode: 141, duration: 0.958s, episode steps: 147, steps per second: 153, episode reward: -173.224, mean reward: -1.178 [-100.000,  6.307], mean action: 1.503 [0.000, 3.000],  loss: 57.715686, mse: 442742.113627, mean_q: 490.849007, mean_eps: 0.867677
 13510/100000: episode: 142, duration: 0.452s, episode steps:  70, steps per second: 155, episode reward: -76.876, mean reward: -1.098 [-100.000,  6.929], mean action: 1.443 [0.000, 3.000],  loss: 50.984528, mse: 457044.512054, mean_q: 505.467983, mean_eps: 0.866602
 13582/100000: episode: 143, duration: 0.519s, episode steps:  72, steps per second: 139, episode reward: -51.655, mean reward: -0.717 [-100.000, 12.730], mean action: 1.500 [0.000, 3.000],  loss: 52.591651, mse: 443264.406250, mean_q: 493.950996, mean_eps: 0.865900
 13675/100000: episode: 144, duration: 0.634s, episode steps:  93, steps per second: 147, episode reward: -131.621, mean reward: -1.415 [-100.000,  8.327], mean action: 1.548 [0.000, 3.000],  loss: 57.357787, mse: 452697.792003, mean_q: 499.554937, mean_eps: 0.865083
 13836/100000: episode: 145, duration: 1.247s, episode steps: 161, steps per second: 129, episode reward: -149.812, mean reward: -0.931 [-100.000, 10.978], mean action: 1.652 [0.000, 3.000],  loss: 63.738330, mse: 441049.518731, mean_q: 487.615200, mean_eps: 0.863826
 13954/100000: episode: 146, duration: 0.866s, episode steps: 118, steps per second: 136, episode reward: -220.766, mean reward: -1.871 [-100.000,  8.133], mean action: 1.483 [0.000, 3.000],  loss: 59.132178, mse: 437713.271186, mean_q: 488.503268, mean_eps: 0.862444
 14019/100000: episode: 147, duration: 0.444s, episode steps:  65, steps per second: 146, episode reward: -59.338, mean reward: -0.913 [-100.000,  7.286], mean action: 1.646 [0.000, 3.000],  loss: 62.293723, mse: 455987.184615, mean_q: 500.274027, mean_eps: 0.861539
 14092/100000: episode: 148, duration: 0.532s, episode steps:  73, steps per second: 137, episode reward: -60.824, mean reward: -0.833 [-100.000,  8.350], mean action: 1.329 [0.000, 3.000],  loss: 59.574349, mse: 427007.871147, mean_q: 477.074309, mean_eps: 0.860855
 14227/100000: episode: 149, duration: 0.999s, episode steps: 135, steps per second: 135, episode reward: -93.771, mean reward: -0.695 [-100.000,  8.248], mean action: 1.452 [0.000, 3.000],  loss: 57.117177, mse: 444737.061111, mean_q: 493.884616, mean_eps: 0.859826
 14306/100000: episode: 150, duration: 0.550s, episode steps:  79, steps per second: 144, episode reward: -121.837, mean reward: -1.542 [-100.000,  7.407], mean action: 1.367 [0.000, 3.000],  loss: 59.471883, mse: 420618.233386, mean_q: 468.593970, mean_eps: 0.858767
 14391/100000: episode: 151, duration: 0.639s, episode steps:  85, steps per second: 133, episode reward: -97.002, mean reward: -1.141 [-100.000, 10.905], mean action: 1.694 [0.000, 3.000],  loss: 51.851754, mse: 444278.409926, mean_q: 492.392336, mean_eps: 0.857955
 14481/100000: episode: 152, duration: 0.664s, episode steps:  90, steps per second: 136, episode reward: -168.382, mean reward: -1.871 [-100.000,  7.524], mean action: 1.456 [0.000, 3.000],  loss: 65.967098, mse: 450464.403125, mean_q: 499.208567, mean_eps: 0.857089
 14635/100000: episode: 153, duration: 1.009s, episode steps: 154, steps per second: 153, episode reward: -201.300, mean reward: -1.307 [-100.000, 24.386], mean action: 1.571 [0.000, 3.000],  loss: 54.049763, mse: 446467.748782, mean_q: 497.264964, mean_eps: 0.855881
 14746/100000: episode: 154, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: -288.913, mean reward: -2.603 [-100.000, 74.670], mean action: 1.550 [0.000, 3.000],  loss: 53.155253, mse: 438978.059262, mean_q: 493.478585, mean_eps: 0.854569
 14856/100000: episode: 155, duration: 0.896s, episode steps: 110, steps per second: 123, episode reward: -399.593, mean reward: -3.633 [-100.000, 42.552], mean action: 1.436 [0.000, 3.000],  loss: 54.980358, mse: 437522.558523, mean_q: 486.064499, mean_eps: 0.853475
 14972/100000: episode: 156, duration: 0.915s, episode steps: 116, steps per second: 127, episode reward: -106.340, mean reward: -0.917 [-100.000,  6.947], mean action: 1.483 [0.000, 3.000],  loss: 52.594836, mse: 445626.308998, mean_q: 493.533087, mean_eps: 0.852356
 15090/100000: episode: 157, duration: 1.018s, episode steps: 118, steps per second: 116, episode reward: -121.176, mean reward: -1.027 [-100.000,  5.349], mean action: 1.695 [0.000, 3.000],  loss: 61.358436, mse: 439262.771849, mean_q: 490.507450, mean_eps: 0.851198
 15178/100000: episode: 158, duration: 0.660s, episode steps:  88, steps per second: 133, episode reward: -155.585, mean reward: -1.768 [-100.000,  4.660], mean action: 1.341 [0.000, 3.000],  loss: 53.042160, mse: 440687.889560, mean_q: 485.994772, mean_eps: 0.850178
 15312/100000: episode: 159, duration: 0.993s, episode steps: 134, steps per second: 135, episode reward: -201.944, mean reward: -1.507 [-100.000,  5.305], mean action: 1.493 [0.000, 3.000],  loss: 58.378128, mse: 440335.080690, mean_q: 490.390207, mean_eps: 0.849079
 15402/100000: episode: 160, duration: 0.626s, episode steps:  90, steps per second: 144, episode reward: -29.609, mean reward: -0.329 [-100.000, 85.703], mean action: 1.589 [0.000, 3.000],  loss: 56.673279, mse: 457777.941667, mean_q: 507.956738, mean_eps: 0.847971
 15485/100000: episode: 161, duration: 0.538s, episode steps:  83, steps per second: 154, episode reward: -163.463, mean reward: -1.969 [-100.000, 82.529], mean action: 1.518 [0.000, 3.000],  loss: 61.422262, mse: 440711.076054, mean_q: 489.786694, mean_eps: 0.847114
 15583/100000: episode: 162, duration: 0.650s, episode steps:  98, steps per second: 151, episode reward: -154.482, mean reward: -1.576 [-100.000,  3.134], mean action: 1.531 [0.000, 3.000],  loss: 63.408804, mse: 422780.793686, mean_q: 470.257833, mean_eps: 0.846218
 15673/100000: episode: 163, duration: 1.237s, episode steps:  90, steps per second:  73, episode reward: -94.788, mean reward: -1.053 [-100.000, 11.454], mean action: 1.422 [0.000, 3.000],  loss: 51.908858, mse: 418086.771701, mean_q: 464.767784, mean_eps: 0.845288
 15772/100000: episode: 164, duration: 0.721s, episode steps:  99, steps per second: 137, episode reward: -197.433, mean reward: -1.994 [-100.000,  8.530], mean action: 1.657 [0.000, 3.000],  loss: 59.995855, mse: 414108.612847, mean_q: 460.036530, mean_eps: 0.844352
 15867/100000: episode: 165, duration: 0.729s, episode steps:  95, steps per second: 130, episode reward: -80.649, mean reward: -0.849 [-100.000, 16.904], mean action: 1.779 [0.000, 3.000],  loss: 54.936595, mse: 412721.414803, mean_q: 459.340440, mean_eps: 0.843392
 15971/100000: episode: 166, duration: 0.788s, episode steps: 104, steps per second: 132, episode reward: -192.420, mean reward: -1.850 [-100.000, 92.553], mean action: 1.462 [0.000, 3.000],  loss: 69.349301, mse: 414817.897987, mean_q: 461.527113, mean_eps: 0.842407
 16046/100000: episode: 167, duration: 0.518s, episode steps:  75, steps per second: 145, episode reward: -186.619, mean reward: -2.488 [-100.000,  7.717], mean action: 1.520 [0.000, 3.000],  loss: 61.592092, mse: 423060.289583, mean_q: 467.029115, mean_eps: 0.841521
 16180/100000: episode: 168, duration: 0.967s, episode steps: 134, steps per second: 139, episode reward: -369.059, mean reward: -2.754 [-100.000, 75.252], mean action: 1.619 [0.000, 3.000],  loss: 56.570708, mse: 416613.463969, mean_q: 462.670079, mean_eps: 0.840486
 16306/100000: episode: 169, duration: 0.988s, episode steps: 126, steps per second: 128, episode reward: -248.011, mean reward: -1.968 [-100.000,  0.879], mean action: 1.587 [0.000, 3.000],  loss: 55.514565, mse: 429581.623016, mean_q: 477.792615, mean_eps: 0.839199
 16392/100000: episode: 170, duration: 0.589s, episode steps:  86, steps per second: 146, episode reward: -109.786, mean reward: -1.277 [-100.000,  5.869], mean action: 1.453 [0.000, 3.000],  loss: 58.709023, mse: 411571.574855, mean_q: 463.688409, mean_eps: 0.838150
 16502/100000: episode: 171, duration: 0.836s, episode steps: 110, steps per second: 132, episode reward: -342.618, mean reward: -3.115 [-100.000, 45.135], mean action: 1.573 [0.000, 3.000],  loss: 52.495210, mse: 412694.291619, mean_q: 462.469393, mean_eps: 0.837180
 16595/100000: episode: 172, duration: 0.736s, episode steps:  93, steps per second: 126, episode reward: -61.896, mean reward: -0.666 [-100.000, 13.015], mean action: 1.710 [0.000, 3.000],  loss: 53.815376, mse: 415654.349966, mean_q: 466.229133, mean_eps: 0.836175
 16691/100000: episode: 173, duration: 0.755s, episode steps:  96, steps per second: 127, episode reward: -155.683, mean reward: -1.622 [-100.000, 16.385], mean action: 1.510 [0.000, 3.000],  loss: 49.580562, mse: 408165.570475, mean_q: 457.818281, mean_eps: 0.835239
 16761/100000: episode: 174, duration: 0.628s, episode steps:  70, steps per second: 112, episode reward: -150.421, mean reward: -2.149 [-100.000, 17.768], mean action: 1.671 [0.000, 3.000],  loss: 54.735717, mse: 403191.933482, mean_q: 457.882748, mean_eps: 0.834418
 16870/100000: episode: 175, duration: 0.834s, episode steps: 109, steps per second: 131, episode reward: -154.959, mean reward: -1.422 [-100.000, 37.896], mean action: 1.413 [0.000, 3.000],  loss: 60.975253, mse: 409123.786124, mean_q: 461.623077, mean_eps: 0.833531
 16963/100000: episode: 176, duration: 0.699s, episode steps:  93, steps per second: 133, episode reward: -88.488, mean reward: -0.951 [-100.000,  8.239], mean action: 1.624 [0.000, 3.000],  loss: 60.486723, mse: 397284.424227, mean_q: 453.950405, mean_eps: 0.832532
 17034/100000: episode: 177, duration: 0.605s, episode steps:  71, steps per second: 117, episode reward: -104.826, mean reward: -1.476 [-100.000, 11.446], mean action: 1.521 [0.000, 3.000],  loss: 59.009507, mse: 411189.924956, mean_q: 463.273157, mean_eps: 0.831720
 17192/100000: episode: 178, duration: 1.234s, episode steps: 158, steps per second: 128, episode reward: -94.397, mean reward: -0.597 [-100.000, 75.788], mean action: 1.563 [0.000, 3.000],  loss: 50.735973, mse: 404531.264735, mean_q: 455.691631, mean_eps: 0.830586
 17271/100000: episode: 179, duration: 0.542s, episode steps:  79, steps per second: 146, episode reward: -152.936, mean reward: -1.936 [-100.000,  7.517], mean action: 1.646 [0.000, 3.000],  loss: 72.920931, mse: 409651.925831, mean_q: 462.170978, mean_eps: 0.829413
 17421/100000: episode: 180, duration: 1.082s, episode steps: 150, steps per second: 139, episode reward: -154.123, mean reward: -1.027 [-100.000,  9.438], mean action: 1.513 [0.000, 3.000],  loss: 59.009463, mse: 400005.441458, mean_q: 453.690713, mean_eps: 0.828280
 17550/100000: episode: 181, duration: 0.832s, episode steps: 129, steps per second: 155, episode reward: -150.874, mean reward: -1.170 [-100.000,  4.279], mean action: 1.767 [0.000, 3.000],  loss: 61.551576, mse: 401119.205911, mean_q: 456.921119, mean_eps: 0.826898
 17689/100000: episode: 182, duration: 1.009s, episode steps: 139, steps per second: 138, episode reward: -156.965, mean reward: -1.129 [-100.000, 14.123], mean action: 1.633 [0.000, 3.000],  loss: 60.127282, mse: 405155.045076, mean_q: 460.892142, mean_eps: 0.825572
 17770/100000: episode: 183, duration: 0.521s, episode steps:  81, steps per second: 155, episode reward: -81.055, mean reward: -1.001 [-100.000,  6.687], mean action: 1.358 [0.000, 3.000],  loss: 68.093675, mse: 422533.859568, mean_q: 476.234427, mean_eps: 0.824483
 17892/100000: episode: 184, duration: 0.875s, episode steps: 122, steps per second: 139, episode reward: -165.248, mean reward: -1.354 [-100.000,  6.695], mean action: 1.557 [0.000, 3.000],  loss: 72.393741, mse: 401392.808017, mean_q: 462.799673, mean_eps: 0.823478
 18028/100000: episode: 185, duration: 0.924s, episode steps: 136, steps per second: 147, episode reward: -30.233, mean reward: -0.222 [-100.000, 105.055], mean action: 1.603 [0.000, 3.000],  loss: 59.518113, mse: 425087.976218, mean_q: 478.334302, mean_eps: 0.822201
 18112/100000: episode: 186, duration: 0.600s, episode steps:  84, steps per second: 140, episode reward: -116.903, mean reward: -1.392 [-100.000,  8.130], mean action: 1.631 [0.000, 3.000],  loss: 56.791264, mse: 410941.367932, mean_q: 461.415898, mean_eps: 0.821112
 18220/100000: episode: 187, duration: 0.841s, episode steps: 108, steps per second: 128, episode reward: -116.632, mean reward: -1.080 [-100.000, 15.330], mean action: 1.565 [0.000, 3.000],  loss: 59.078434, mse: 422830.371672, mean_q: 470.698185, mean_eps: 0.820162
 18381/100000: episode: 188, duration: 1.093s, episode steps: 161, steps per second: 147, episode reward: -112.841, mean reward: -0.701 [-100.000, 14.969], mean action: 1.460 [0.000, 3.000],  loss: 60.703576, mse: 420738.111607, mean_q: 471.247259, mean_eps: 0.818830
 18481/100000: episode: 189, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -146.222, mean reward: -1.462 [-100.000, 17.725], mean action: 1.690 [0.000, 3.000],  loss: 57.525053, mse: 419977.260937, mean_q: 471.856655, mean_eps: 0.817538
 18585/100000: episode: 190, duration: 0.730s, episode steps: 104, steps per second: 142, episode reward: -220.969, mean reward: -2.125 [-100.000,  0.691], mean action: 1.904 [0.000, 3.000],  loss: 63.435636, mse: 418011.789062, mean_q: 472.838779, mean_eps: 0.816528
 18691/100000: episode: 191, duration: 0.688s, episode steps: 106, steps per second: 154, episode reward: -207.077, mean reward: -1.954 [-100.000, 97.210], mean action: 1.415 [0.000, 3.000],  loss: 54.632648, mse: 421558.434847, mean_q: 476.413270, mean_eps: 0.815489
 18795/100000: episode: 192, duration: 0.752s, episode steps: 104, steps per second: 138, episode reward: -53.976, mean reward: -0.519 [-100.000, 16.551], mean action: 1.779 [0.000, 3.000],  loss: 69.463501, mse: 428195.808894, mean_q: 481.889242, mean_eps: 0.814449
 18917/100000: episode: 193, duration: 0.874s, episode steps: 122, steps per second: 140, episode reward: -115.444, mean reward: -0.946 [-100.000, 12.526], mean action: 1.656 [0.000, 3.000],  loss: 56.120741, mse: 423473.894339, mean_q: 475.752404, mean_eps: 0.813331
 19034/100000: episode: 194, duration: 0.809s, episode steps: 117, steps per second: 145, episode reward: -156.189, mean reward: -1.335 [-100.000,  3.879], mean action: 1.821 [0.000, 3.000],  loss: 61.771513, mse: 427122.067575, mean_q: 482.111510, mean_eps: 0.812148
 19138/100000: episode: 195, duration: 0.809s, episode steps: 104, steps per second: 129, episode reward: -125.258, mean reward: -1.204 [-100.000, 63.661], mean action: 1.519 [0.000, 3.000],  loss: 58.333500, mse: 439519.048377, mean_q: 491.813864, mean_eps: 0.811054
 19214/100000: episode: 196, duration: 0.514s, episode steps:  76, steps per second: 148, episode reward: -176.727, mean reward: -2.325 [-100.000,  5.081], mean action: 1.697 [0.000, 3.000],  loss: 58.421039, mse: 429509.252262, mean_q: 486.706127, mean_eps: 0.810163
 19362/100000: episode: 197, duration: 1.085s, episode steps: 148, steps per second: 136, episode reward: -153.566, mean reward: -1.038 [-100.000, 12.236], mean action: 1.696 [0.000, 3.000],  loss: 59.973950, mse: 419560.742399, mean_q: 475.482151, mean_eps: 0.809054
 19429/100000: episode: 198, duration: 0.463s, episode steps:  67, steps per second: 145, episode reward: -149.434, mean reward: -2.230 [-100.000, 15.538], mean action: 1.582 [0.000, 3.000],  loss: 59.434012, mse: 432113.757696, mean_q: 487.940531, mean_eps: 0.807990
 19530/100000: episode: 199, duration: 0.741s, episode steps: 101, steps per second: 136, episode reward: -331.475, mean reward: -3.282 [-100.000,  0.707], mean action: 1.624 [0.000, 3.000],  loss: 68.453580, mse: 416311.002166, mean_q: 477.125006, mean_eps: 0.807158
 19675/100000: episode: 200, duration: 0.983s, episode steps: 145, steps per second: 147, episode reward: -147.012, mean reward: -1.014 [-100.000, 28.298], mean action: 1.614 [0.000, 3.000],  loss: 64.710384, mse: 426315.518750, mean_q: 479.537982, mean_eps: 0.805940
 19802/100000: episode: 201, duration: 0.908s, episode steps: 127, steps per second: 140, episode reward: -109.314, mean reward: -0.861 [-100.000, 33.652], mean action: 1.567 [0.000, 3.000],  loss: 59.938681, mse: 433891.545522, mean_q: 486.190204, mean_eps: 0.804594
 19941/100000: episode: 202, duration: 0.933s, episode steps: 139, steps per second: 149, episode reward: -81.259, mean reward: -0.585 [-100.000, 10.897], mean action: 1.748 [0.000, 3.000],  loss: 65.207185, mse: 437741.121178, mean_q: 488.036070, mean_eps: 0.803277
 20023/100000: episode: 203, duration: 0.527s, episode steps:  82, steps per second: 156, episode reward: -83.506, mean reward: -1.018 [-100.000, 11.707], mean action: 1.549 [0.000, 3.000],  loss: 61.682960, mse: 413347.765816, mean_q: 467.342411, mean_eps: 0.802183
 20112/100000: episode: 204, duration: 0.623s, episode steps:  89, steps per second: 143, episode reward: -125.028, mean reward: -1.405 [-100.000, 15.635], mean action: 1.382 [0.000, 3.000],  loss: 61.867274, mse: 414381.687500, mean_q: 465.868773, mean_eps: 0.801337
 20182/100000: episode: 205, duration: 0.503s, episode steps:  70, steps per second: 139, episode reward: -69.672, mean reward: -0.995 [-100.000,  8.807], mean action: 1.714 [0.000, 3.000],  loss: 60.749915, mse: 430337.129241, mean_q: 481.763568, mean_eps: 0.800550
 20240/100000: episode: 206, duration: 0.387s, episode steps:  58, steps per second: 150, episode reward: -127.998, mean reward: -2.207 [-100.000, 21.681], mean action: 1.534 [0.000, 3.000],  loss: 65.172251, mse: 410728.417026, mean_q: 462.773873, mean_eps: 0.799916
 20325/100000: episode: 207, duration: 0.540s, episode steps:  85, steps per second: 157, episode reward: -100.764, mean reward: -1.185 [-100.000, 23.895], mean action: 1.459 [0.000, 3.000],  loss: 72.914060, mse: 429557.807353, mean_q: 481.323281, mean_eps: 0.799208
 20432/100000: episode: 208, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -190.174, mean reward: -1.777 [-100.000, 11.200], mean action: 1.467 [0.000, 3.000],  loss: 65.421207, mse: 429115.257009, mean_q: 477.837309, mean_eps: 0.798258
 20579/100000: episode: 209, duration: 0.981s, episode steps: 147, steps per second: 150, episode reward: -100.595, mean reward: -0.684 [-100.000, 10.938], mean action: 1.565 [0.000, 3.000],  loss: 63.850833, mse: 407184.440582, mean_q: 457.957636, mean_eps: 0.797001
 20692/100000: episode: 210, duration: 0.736s, episode steps: 113, steps per second: 154, episode reward: -49.669, mean reward: -0.440 [-100.000, 20.420], mean action: 1.628 [0.000, 3.000],  loss: 68.844465, mse: 414987.332273, mean_q: 464.986900, mean_eps: 0.795714
 20785/100000: episode: 211, duration: 0.653s, episode steps:  93, steps per second: 143, episode reward: -197.765, mean reward: -2.127 [-100.000,  1.828], mean action: 1.774 [0.000, 3.000],  loss: 59.118287, mse: 422063.039987, mean_q: 473.855778, mean_eps: 0.794694
 20889/100000: episode: 212, duration: 0.666s, episode steps: 104, steps per second: 156, episode reward: -87.993, mean reward: -0.846 [-100.000,  7.365], mean action: 1.712 [0.000, 3.000],  loss: 68.087508, mse: 406030.387921, mean_q: 455.327879, mean_eps: 0.793719
 20994/100000: episode: 213, duration: 0.725s, episode steps: 105, steps per second: 145, episode reward: -337.055, mean reward: -3.210 [-100.000, 62.393], mean action: 1.724 [0.000, 3.000],  loss: 57.908582, mse: 404162.468304, mean_q: 457.166953, mean_eps: 0.792684
 21134/100000: episode: 214, duration: 0.976s, episode steps: 140, steps per second: 143, episode reward: -167.943, mean reward: -1.200 [-100.000,  9.438], mean action: 1.586 [0.000, 3.000],  loss: 69.184819, mse: 420518.901786, mean_q: 470.380763, mean_eps: 0.791471
 21234/100000: episode: 215, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -85.276, mean reward: -0.853 [-100.000,  6.473], mean action: 1.610 [0.000, 3.000],  loss: 67.461474, mse: 428315.190000, mean_q: 480.903628, mean_eps: 0.790283
 21365/100000: episode: 216, duration: 0.881s, episode steps: 131, steps per second: 149, episode reward: -183.968, mean reward: -1.404 [-100.000,  2.869], mean action: 1.504 [0.000, 3.000],  loss: 63.344241, mse: 413366.604008, mean_q: 470.929111, mean_eps: 0.789140
 21469/100000: episode: 217, duration: 0.691s, episode steps: 104, steps per second: 151, episode reward: -56.958, mean reward: -0.548 [-100.000, 10.198], mean action: 1.500 [0.000, 3.000],  loss: 62.161649, mse: 415148.583233, mean_q: 469.608073, mean_eps: 0.787977
 21582/100000: episode: 218, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -242.486, mean reward: -2.146 [-100.000,  1.081], mean action: 1.602 [0.000, 3.000],  loss: 67.016834, mse: 418934.171737, mean_q: 477.778289, mean_eps: 0.786903
 21659/100000: episode: 219, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -104.461, mean reward: -1.357 [-100.000, 15.816], mean action: 1.481 [0.000, 3.000],  loss: 84.913626, mse: 424853.809659, mean_q: 481.819348, mean_eps: 0.785962
 21770/100000: episode: 220, duration: 0.828s, episode steps: 111, steps per second: 134, episode reward: -112.629, mean reward: -1.015 [-100.000, 26.391], mean action: 1.649 [0.000, 3.000],  loss: 74.874139, mse: 419022.209178, mean_q: 474.359847, mean_eps: 0.785031
 21882/100000: episode: 221, duration: 0.816s, episode steps: 112, steps per second: 137, episode reward: -344.765, mean reward: -3.078 [-100.000, 78.312], mean action: 1.393 [0.000, 3.000],  loss: 60.527806, mse: 427450.904576, mean_q: 486.214074, mean_eps: 0.783928
 22015/100000: episode: 222, duration: 1.039s, episode steps: 133, steps per second: 128, episode reward: -90.770, mean reward: -0.682 [-100.000, 12.914], mean action: 1.586 [0.000, 3.000],  loss: 64.220798, mse: 431622.385221, mean_q: 491.180446, mean_eps: 0.782715
 22161/100000: episode: 223, duration: 1.102s, episode steps: 146, steps per second: 133, episode reward: -193.307, mean reward: -1.324 [-100.000, 25.294], mean action: 1.781 [0.000, 3.000],  loss: 63.115707, mse: 434689.754709, mean_q: 491.441880, mean_eps: 0.781334
 22333/100000: episode: 224, duration: 1.301s, episode steps: 172, steps per second: 132, episode reward: -89.839, mean reward: -0.522 [-100.000, 13.076], mean action: 1.610 [0.000, 3.000],  loss: 70.277986, mse: 454035.279524, mean_q: 508.312498, mean_eps: 0.779760
 22442/100000: episode: 225, duration: 0.828s, episode steps: 109, steps per second: 132, episode reward: -163.185, mean reward: -1.497 [-100.000,  9.511], mean action: 1.560 [0.000, 3.000],  loss: 68.445047, mse: 447728.717890, mean_q: 501.146825, mean_eps: 0.778369
 22528/100000: episode: 226, duration: 0.603s, episode steps:  86, steps per second: 143, episode reward: -96.942, mean reward: -1.127 [-100.000, 10.694], mean action: 1.628 [0.000, 3.000],  loss: 67.383563, mse: 422400.588663, mean_q: 479.872088, mean_eps: 0.777403
 22619/100000: episode: 227, duration: 0.697s, episode steps:  91, steps per second: 131, episode reward: -46.228, mean reward: -0.508 [-100.000, 16.072], mean action: 1.582 [0.000, 3.000],  loss: 61.864818, mse: 446906.666209, mean_q: 504.536115, mean_eps: 0.776527
 22796/100000: episode: 228, duration: 1.180s, episode steps: 177, steps per second: 150, episode reward: -84.588, mean reward: -0.478 [-100.000,  7.984], mean action: 1.576 [0.000, 3.000],  loss: 64.824156, mse: 447909.013153, mean_q: 509.476942, mean_eps: 0.775201
 22898/100000: episode: 229, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -27.649, mean reward: -0.271 [-100.000, 15.857], mean action: 1.657 [0.000, 3.000],  loss: 66.441337, mse: 444814.716605, mean_q: 504.131732, mean_eps: 0.773820
 23031/100000: episode: 230, duration: 0.866s, episode steps: 133, steps per second: 154, episode reward: -116.262, mean reward: -0.874 [-100.000, 11.420], mean action: 1.677 [0.000, 3.000],  loss: 62.023224, mse: 428749.989074, mean_q: 492.272086, mean_eps: 0.772656
 23112/100000: episode: 231, duration: 0.511s, episode steps:  81, steps per second: 158, episode reward: -108.847, mean reward: -1.344 [-100.000, 10.448], mean action: 1.383 [0.000, 3.000],  loss: 73.889893, mse: 424180.299576, mean_q: 489.510638, mean_eps: 0.771597
 23234/100000: episode: 232, duration: 0.862s, episode steps: 122, steps per second: 142, episode reward: -83.228, mean reward: -0.682 [-100.000, 109.256], mean action: 1.623 [0.000, 3.000],  loss: 60.621753, mse: 436555.912910, mean_q: 494.331942, mean_eps: 0.770592
 23314/100000: episode: 233, duration: 0.646s, episode steps:  80, steps per second: 124, episode reward: -104.637, mean reward: -1.308 [-100.000,  6.928], mean action: 1.725 [0.000, 3.000],  loss: 65.559633, mse: 439508.370703, mean_q: 500.532302, mean_eps: 0.769592
 23391/100000: episode: 234, duration: 0.549s, episode steps:  77, steps per second: 140, episode reward: -108.701, mean reward: -1.412 [-100.000,  4.284], mean action: 1.377 [0.000, 3.000],  loss: 66.233622, mse: 447081.071834, mean_q: 506.972594, mean_eps: 0.768815
 23481/100000: episode: 235, duration: 0.636s, episode steps:  90, steps per second: 142, episode reward: -70.406, mean reward: -0.782 [-100.000,  7.268], mean action: 1.533 [0.000, 3.000],  loss: 63.474606, mse: 451996.664410, mean_q: 509.419907, mean_eps: 0.767989
 23550/100000: episode: 236, duration: 0.472s, episode steps:  69, steps per second: 146, episode reward: -78.469, mean reward: -1.137 [-100.000, 16.019], mean action: 1.377 [0.000, 3.000],  loss: 68.009646, mse: 450318.133152, mean_q: 507.817470, mean_eps: 0.767201
 23621/100000: episode: 237, duration: 0.481s, episode steps:  71, steps per second: 148, episode reward: -85.006, mean reward: -1.197 [-100.000, 15.635], mean action: 1.789 [0.000, 3.000],  loss: 59.734345, mse: 438609.712588, mean_q: 496.958972, mean_eps: 0.766508
 23717/100000: episode: 238, duration: 0.637s, episode steps:  96, steps per second: 151, episode reward: -0.085, mean reward: -0.001 [-100.000, 18.592], mean action: 1.656 [0.000, 3.000],  loss: 60.442779, mse: 435418.249674, mean_q: 498.158401, mean_eps: 0.765682
 23850/100000: episode: 239, duration: 0.943s, episode steps: 133, steps per second: 141, episode reward: -26.340, mean reward: -0.198 [-100.000, 102.516], mean action: 1.669 [0.000, 3.000],  loss: 67.424113, mse: 432817.344455, mean_q: 491.782714, mean_eps: 0.764548
 23976/100000: episode: 240, duration: 1.090s, episode steps: 126, steps per second: 116, episode reward: -171.662, mean reward: -1.362 [-100.000,  2.990], mean action: 1.548 [0.000, 3.000],  loss: 63.665176, mse: 434764.723710, mean_q: 492.615047, mean_eps: 0.763266
 24088/100000: episode: 241, duration: 1.190s, episode steps: 112, steps per second:  94, episode reward: -147.757, mean reward: -1.319 [-100.000,  1.934], mean action: 1.741 [0.000, 3.000],  loss: 65.605370, mse: 449273.675502, mean_q: 503.805836, mean_eps: 0.762088
 24242/100000: episode: 242, duration: 1.458s, episode steps: 154, steps per second: 106, episode reward: -89.138, mean reward: -0.579 [-100.000, 14.074], mean action: 1.370 [0.000, 3.000],  loss: 110.127553, mse: 443499.852881, mean_q: 501.234500, mean_eps: 0.760771
 24400/100000: episode: 243, duration: 1.197s, episode steps: 158, steps per second: 132, episode reward: -218.001, mean reward: -1.380 [-100.000,  6.441], mean action: 1.551 [0.000, 3.000],  loss: 85.332594, mse: 450959.425237, mean_q: 502.967429, mean_eps: 0.759227
 24515/100000: episode: 244, duration: 0.737s, episode steps: 115, steps per second: 156, episode reward: 35.279, mean reward:  0.307 [-100.000, 86.035], mean action: 1.791 [0.000, 3.000],  loss: 78.655215, mse: 458550.925272, mean_q: 510.886230, mean_eps: 0.757876
 24657/100000: episode: 245, duration: 1.035s, episode steps: 142, steps per second: 137, episode reward: -22.618, mean reward: -0.159 [-100.000, 20.405], mean action: 1.606 [0.000, 3.000],  loss: 95.538813, mse: 463246.481954, mean_q: 516.302655, mean_eps: 0.756604
 24807/100000: episode: 246, duration: 1.022s, episode steps: 150, steps per second: 147, episode reward: -68.307, mean reward: -0.455 [-100.000, 17.616], mean action: 1.733 [0.000, 3.000],  loss: 70.193162, mse: 457270.424063, mean_q: 511.405936, mean_eps: 0.755158
 24892/100000: episode: 247, duration: 0.666s, episode steps:  85, steps per second: 128, episode reward: -106.086, mean reward: -1.248 [-100.000, 19.950], mean action: 1.765 [0.000, 3.000],  loss: 84.929341, mse: 440853.662500, mean_q: 501.647261, mean_eps: 0.753995
 25054/100000: episode: 248, duration: 1.129s, episode steps: 162, steps per second: 143, episode reward: -128.589, mean reward: -0.794 [-100.000,  8.194], mean action: 1.568 [0.000, 3.000],  loss: 66.391150, mse: 462677.246721, mean_q: 523.025143, mean_eps: 0.752772
 25219/100000: episode: 249, duration: 1.161s, episode steps: 165, steps per second: 142, episode reward: -77.210, mean reward: -0.468 [-100.000, 13.255], mean action: 1.618 [0.000, 3.000],  loss: 64.100598, mse: 454166.340057, mean_q: 514.879478, mean_eps: 0.751154
 25315/100000: episode: 250, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: -109.123, mean reward: -1.137 [-100.000,  7.800], mean action: 1.604 [0.000, 3.000],  loss: 62.761083, mse: 446400.456380, mean_q: 507.554733, mean_eps: 0.749862
 25418/100000: episode: 251, duration: 0.772s, episode steps: 103, steps per second: 133, episode reward: -241.851, mean reward: -2.348 [-100.000,  0.990], mean action: 1.505 [0.000, 3.000],  loss: 69.622479, mse: 460904.570085, mean_q: 523.806313, mean_eps: 0.748877
 25488/100000: episode: 252, duration: 0.508s, episode steps:  70, steps per second: 138, episode reward: -76.862, mean reward: -1.098 [-100.000, 11.975], mean action: 1.543 [0.000, 3.000],  loss: 84.963546, mse: 435359.726339, mean_q: 493.466670, mean_eps: 0.748020
 25652/100000: episode: 253, duration: 1.125s, episode steps: 164, steps per second: 146, episode reward: -71.564, mean reward: -0.436 [-100.000,  9.607], mean action: 1.646 [0.000, 3.000],  loss: 63.011877, mse: 449868.754954, mean_q: 513.572617, mean_eps: 0.746862
 25759/100000: episode: 254, duration: 0.811s, episode steps: 107, steps per second: 132, episode reward: -190.809, mean reward: -1.783 [-100.000,  1.528], mean action: 1.486 [0.000, 3.000],  loss: 77.381685, mse: 450076.126168, mean_q: 512.257237, mean_eps: 0.745520
 25852/100000: episode: 255, duration: 0.794s, episode steps:  93, steps per second: 117, episode reward: -209.327, mean reward: -2.251 [-100.000,  6.705], mean action: 1.710 [0.000, 3.000],  loss: 60.507400, mse: 448654.283266, mean_q: 509.518314, mean_eps: 0.744530
 25960/100000: episode: 256, duration: 0.804s, episode steps: 108, steps per second: 134, episode reward: -122.239, mean reward: -1.132 [-100.000, 15.198], mean action: 1.713 [0.000, 3.000],  loss: 96.291478, mse: 461710.873264, mean_q: 523.197065, mean_eps: 0.743536
 26035/100000: episode: 257, duration: 0.541s, episode steps:  75, steps per second: 139, episode reward: -139.482, mean reward: -1.860 [-100.000, 14.621], mean action: 1.627 [0.000, 3.000],  loss: 73.094661, mse: 460169.702917, mean_q: 522.791139, mean_eps: 0.742630
 26200/100000: episode: 258, duration: 1.233s, episode steps: 165, steps per second: 134, episode reward: -230.702, mean reward: -1.398 [-100.000,  1.781], mean action: 1.636 [0.000, 3.000],  loss: 76.781903, mse: 467089.727273, mean_q: 528.115390, mean_eps: 0.741442
 26334/100000: episode: 259, duration: 0.992s, episode steps: 134, steps per second: 135, episode reward: -122.094, mean reward: -0.911 [-100.000,  5.115], mean action: 1.612 [0.000, 3.000],  loss: 78.856444, mse: 464801.917211, mean_q: 526.035188, mean_eps: 0.739962
 26433/100000: episode: 260, duration: 0.730s, episode steps:  99, steps per second: 136, episode reward: -214.933, mean reward: -2.171 [-100.000,  1.125], mean action: 1.515 [0.000, 3.000],  loss: 71.821884, mse: 464252.242109, mean_q: 526.543858, mean_eps: 0.738808
 26547/100000: episode: 261, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -238.373, mean reward: -2.091 [-100.000,  1.652], mean action: 1.491 [0.000, 3.000],  loss: 69.138439, mse: 471852.780702, mean_q: 531.202940, mean_eps: 0.737754
 26654/100000: episode: 262, duration: 0.727s, episode steps: 107, steps per second: 147, episode reward: -201.867, mean reward: -1.887 [-100.000, 17.683], mean action: 1.607 [0.000, 3.000],  loss: 64.891472, mse: 471029.656834, mean_q: 529.472361, mean_eps: 0.736660
 26764/100000: episode: 263, duration: 0.809s, episode steps: 110, steps per second: 136, episode reward: -273.775, mean reward: -2.489 [-100.000,  1.755], mean action: 1.636 [0.000, 3.000],  loss: 71.992800, mse: 481074.532955, mean_q: 536.013639, mean_eps: 0.735586
 26888/100000: episode: 264, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -90.778, mean reward: -0.732 [-100.000, 18.723], mean action: 1.645 [0.000, 3.000],  loss: 73.268180, mse: 482486.336568, mean_q: 536.343714, mean_eps: 0.734428
 26994/100000: episode: 265, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -222.751, mean reward: -2.101 [-100.000,  5.166], mean action: 1.575 [0.000, 3.000],  loss: 81.545041, mse: 482081.469340, mean_q: 534.671905, mean_eps: 0.733289
 27068/100000: episode: 266, duration: 0.532s, episode steps:  74, steps per second: 139, episode reward: -63.133, mean reward: -0.853 [-100.000, 45.123], mean action: 1.676 [0.000, 3.000],  loss: 74.457935, mse: 482132.171453, mean_q: 531.682497, mean_eps: 0.732398
 27227/100000: episode: 267, duration: 1.123s, episode steps: 159, steps per second: 142, episode reward: -6.418, mean reward: -0.040 [-100.000, 83.403], mean action: 1.566 [0.000, 3.000],  loss: 66.998328, mse: 478059.605739, mean_q: 529.995290, mean_eps: 0.731245
 27361/100000: episode: 268, duration: 0.971s, episode steps: 134, steps per second: 138, episode reward: -79.744, mean reward: -0.595 [-100.000, 15.884], mean action: 1.522 [0.000, 3.000],  loss: 71.160946, mse: 470529.930970, mean_q: 523.549751, mean_eps: 0.729794
 27449/100000: episode: 269, duration: 0.671s, episode steps:  88, steps per second: 131, episode reward: -59.981, mean reward: -0.682 [-100.000, 19.272], mean action: 1.830 [0.000, 3.000],  loss: 81.996518, mse: 463789.393466, mean_q: 513.348755, mean_eps: 0.728695
 27546/100000: episode: 270, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -171.060, mean reward: -1.764 [-100.000,  5.094], mean action: 1.588 [0.000, 3.000],  loss: 88.901519, mse: 474722.802835, mean_q: 520.644062, mean_eps: 0.727780
 27676/100000: episode: 271, duration: 0.932s, episode steps: 130, steps per second: 139, episode reward: 12.075, mean reward:  0.093 [-100.000, 19.979], mean action: 1.685 [0.000, 3.000],  loss: 60.620650, mse: 493438.305288, mean_q: 540.706518, mean_eps: 0.726656
 27802/100000: episode: 272, duration: 0.976s, episode steps: 126, steps per second: 129, episode reward: -278.329, mean reward: -2.209 [-100.000,  1.112], mean action: 1.587 [0.000, 3.000],  loss: 67.472857, mse: 500354.078869, mean_q: 544.608411, mean_eps: 0.725389
 27932/100000: episode: 273, duration: 1.125s, episode steps: 130, steps per second: 116, episode reward: -328.683, mean reward: -2.528 [-100.000,  1.717], mean action: 1.569 [0.000, 3.000],  loss: 67.389382, mse: 493831.000481, mean_q: 536.466654, mean_eps: 0.724122
 28005/100000: episode: 274, duration: 0.511s, episode steps:  73, steps per second: 143, episode reward: -148.505, mean reward: -2.034 [-100.000,  9.127], mean action: 1.521 [0.000, 3.000],  loss: 95.070358, mse: 503170.389983, mean_q: 543.685152, mean_eps: 0.723117
 28098/100000: episode: 275, duration: 0.649s, episode steps:  93, steps per second: 143, episode reward: -21.618, mean reward: -0.232 [-100.000, 86.283], mean action: 1.548 [0.000, 3.000],  loss: 71.187527, mse: 493599.251344, mean_q: 539.174845, mean_eps: 0.722295
 28187/100000: episode: 276, duration: 0.662s, episode steps:  89, steps per second: 134, episode reward: -46.696, mean reward: -0.525 [-100.000, 16.723], mean action: 1.685 [0.000, 3.000],  loss: 67.275691, mse: 488663.073034, mean_q: 532.593538, mean_eps: 0.721394
 28295/100000: episode: 277, duration: 1.260s, episode steps: 108, steps per second:  86, episode reward: -148.581, mean reward: -1.376 [-100.000, 14.378], mean action: 1.630 [0.000, 3.000],  loss: 67.113608, mse: 488364.009549, mean_q: 532.862465, mean_eps: 0.720419
 28406/100000: episode: 278, duration: 0.866s, episode steps: 111, steps per second: 128, episode reward: -42.704, mean reward: -0.385 [-100.000, 100.867], mean action: 1.649 [0.000, 3.000],  loss: 71.820131, mse: 501803.497466, mean_q: 541.017851, mean_eps: 0.719335
 28524/100000: episode: 279, duration: 0.881s, episode steps: 118, steps per second: 134, episode reward: -60.122, mean reward: -0.510 [-100.000, 12.780], mean action: 1.822 [0.000, 3.000],  loss: 76.553857, mse: 488765.651880, mean_q: 534.101461, mean_eps: 0.718201
 28631/100000: episode: 280, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -77.783, mean reward: -0.727 [-100.000, 10.998], mean action: 1.645 [0.000, 3.000],  loss: 82.271358, mse: 483127.948014, mean_q: 527.619811, mean_eps: 0.717088
 28868/100000: episode: 281, duration: 1.983s, episode steps: 237, steps per second: 120, episode reward: -186.517, mean reward: -0.787 [-100.000, 10.894], mean action: 1.586 [0.000, 3.000],  loss: 74.042298, mse: 492481.032041, mean_q: 535.932046, mean_eps: 0.715385
 28973/100000: episode: 282, duration: 1.416s, episode steps: 105, steps per second:  74, episode reward: -58.327, mean reward: -0.555 [-100.000,  7.993], mean action: 1.533 [0.000, 3.000],  loss: 80.846656, mse: 505472.575595, mean_q: 541.678426, mean_eps: 0.713692
 29045/100000: episode: 283, duration: 0.676s, episode steps:  72, steps per second: 106, episode reward: -92.632, mean reward: -1.287 [-100.000,  9.434], mean action: 1.625 [0.000, 3.000],  loss: 68.491010, mse: 509062.340061, mean_q: 543.570520, mean_eps: 0.712816
 29119/100000: episode: 284, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -121.180, mean reward: -1.638 [-100.000,  8.304], mean action: 1.676 [0.000, 3.000],  loss: 70.427636, mse: 489988.198902, mean_q: 524.004843, mean_eps: 0.712093
 29282/100000: episode: 285, duration: 1.481s, episode steps: 163, steps per second: 110, episode reward: -45.791, mean reward: -0.281 [-100.000, 12.568], mean action: 1.583 [0.000, 3.000],  loss: 71.169475, mse: 496305.087423, mean_q: 538.828023, mean_eps: 0.710920
 29371/100000: episode: 286, duration: 0.604s, episode steps:  89, steps per second: 147, episode reward: -108.648, mean reward: -1.221 [-100.000, 14.239], mean action: 1.652 [0.000, 3.000],  loss: 73.748142, mse: 487474.603581, mean_q: 527.414526, mean_eps: 0.709673
 29502/100000: episode: 287, duration: 1.161s, episode steps: 131, steps per second: 113, episode reward: -151.827, mean reward: -1.159 [-100.000, 11.751], mean action: 1.557 [0.000, 3.000],  loss: 76.239078, mse: 493346.945611, mean_q: 536.375074, mean_eps: 0.708584
 29585/100000: episode: 288, duration: 0.653s, episode steps:  83, steps per second: 127, episode reward: -182.907, mean reward: -2.204 [-100.000,  5.235], mean action: 1.422 [0.000, 3.000],  loss: 79.623437, mse: 495774.867093, mean_q: 535.933089, mean_eps: 0.707524
 29695/100000: episode: 289, duration: 0.837s, episode steps: 110, steps per second: 131, episode reward: -104.966, mean reward: -0.954 [-100.000, 12.033], mean action: 1.564 [0.000, 3.000],  loss: 72.934367, mse: 496068.854261, mean_q: 536.107007, mean_eps: 0.706569
 29785/100000: episode: 290, duration: 0.706s, episode steps:  90, steps per second: 127, episode reward: -156.821, mean reward: -1.742 [-100.000,  7.456], mean action: 1.644 [0.000, 3.000],  loss: 73.536117, mse: 496020.760069, mean_q: 542.817902, mean_eps: 0.705579
 29880/100000: episode: 291, duration: 0.689s, episode steps:  95, steps per second: 138, episode reward: -50.936, mean reward: -0.536 [-100.000, 10.900], mean action: 1.737 [0.000, 3.000],  loss: 73.563862, mse: 494519.360197, mean_q: 538.164032, mean_eps: 0.704663
 30064/100000: episode: 292, duration: 1.319s, episode steps: 184, steps per second: 140, episode reward: -116.745, mean reward: -0.634 [-100.000, 49.959], mean action: 1.712 [0.000, 3.000],  loss: 72.997735, mse: 521585.985224, mean_q: 557.029285, mean_eps: 0.703282
 30194/100000: episode: 293, duration: 0.893s, episode steps: 130, steps per second: 146, episode reward: -103.050, mean reward: -0.793 [-100.000, 11.905], mean action: 1.669 [0.000, 3.000],  loss: 69.207125, mse: 532223.886298, mean_q: 564.365142, mean_eps: 0.701728
 30304/100000: episode: 294, duration: 0.834s, episode steps: 110, steps per second: 132, episode reward: -133.095, mean reward: -1.210 [-100.000, 16.657], mean action: 1.700 [0.000, 3.000],  loss: 81.453806, mse: 513813.894886, mean_q: 548.715419, mean_eps: 0.700540
 30430/100000: episode: 295, duration: 0.946s, episode steps: 126, steps per second: 133, episode reward: -65.867, mean reward: -0.523 [-100.000, 72.142], mean action: 1.532 [0.000, 3.000],  loss: 77.123279, mse: 529312.853671, mean_q: 559.286653, mean_eps: 0.699372
 30616/100000: episode: 296, duration: 1.465s, episode steps: 186, steps per second: 127, episode reward: -110.125, mean reward: -0.592 [-100.000, 11.374], mean action: 1.608 [0.000, 3.000],  loss: 78.741128, mse: 520234.249832, mean_q: 553.478862, mean_eps: 0.697827
 30725/100000: episode: 297, duration: 0.768s, episode steps: 109, steps per second: 142, episode reward: -112.135, mean reward: -1.029 [-100.000, 14.674], mean action: 1.569 [0.000, 3.000],  loss: 84.560902, mse: 536996.146502, mean_q: 568.282670, mean_eps: 0.696367
 30902/100000: episode: 298, duration: 1.273s, episode steps: 177, steps per second: 139, episode reward: -73.310, mean reward: -0.414 [-100.000, 18.392], mean action: 1.672 [0.000, 3.000],  loss: 80.560940, mse: 526274.846045, mean_q: 558.498489, mean_eps: 0.694951
 31045/100000: episode: 299, duration: 1.082s, episode steps: 143, steps per second: 132, episode reward: -83.717, mean reward: -0.585 [-100.000, 24.526], mean action: 1.636 [0.000, 3.000],  loss: 82.332023, mse: 521832.840909, mean_q: 553.165587, mean_eps: 0.693367
 31211/100000: episode: 300, duration: 1.222s, episode steps: 166, steps per second: 136, episode reward: -143.911, mean reward: -0.867 [-100.000, 24.144], mean action: 1.572 [0.000, 3.000],  loss: 80.063740, mse: 527943.064571, mean_q: 561.331233, mean_eps: 0.691838
 31353/100000: episode: 301, duration: 1.082s, episode steps: 142, steps per second: 131, episode reward: -248.751, mean reward: -1.752 [-100.000,  2.198], mean action: 1.528 [0.000, 3.000],  loss: 82.657685, mse: 537143.578345, mean_q: 569.785363, mean_eps: 0.690313
 31587/100000: episode: 302, duration: 1.708s, episode steps: 234, steps per second: 137, episode reward: -293.379, mean reward: -1.254 [-100.000, 29.722], mean action: 1.594 [0.000, 3.000],  loss: 80.717597, mse: 528768.444177, mean_q: 561.264046, mean_eps: 0.688452
 31801/100000: episode: 303, duration: 1.576s, episode steps: 214, steps per second: 136, episode reward: -150.579, mean reward: -0.704 [-100.000, 15.653], mean action: 1.659 [0.000, 3.000],  loss: 77.081986, mse: 544056.266647, mean_q: 574.460050, mean_eps: 0.686234
 31919/100000: episode: 304, duration: 0.846s, episode steps: 118, steps per second: 140, episode reward: -165.397, mean reward: -1.402 [-100.000,  6.425], mean action: 1.593 [0.000, 3.000],  loss: 83.756325, mse: 546319.352225, mean_q: 579.508332, mean_eps: 0.684591
 32011/100000: episode: 305, duration: 0.644s, episode steps:  92, steps per second: 143, episode reward: -77.733, mean reward: -0.845 [-100.000,  7.024], mean action: 1.685 [0.000, 3.000],  loss: 82.991518, mse: 538484.748641, mean_q: 576.193998, mean_eps: 0.683551
 32162/100000: episode: 306, duration: 1.073s, episode steps: 151, steps per second: 141, episode reward: -360.486, mean reward: -2.387 [-100.000, 114.111], mean action: 1.589 [0.000, 3.000],  loss: 82.834978, mse: 553666.800497, mean_q: 587.134537, mean_eps: 0.682349
 32380/100000: episode: 307, duration: 1.658s, episode steps: 218, steps per second: 131, episode reward: -197.106, mean reward: -0.904 [-100.000, 11.711], mean action: 1.633 [0.000, 3.000],  loss: 86.741841, mse: 541133.425172, mean_q: 574.022438, mean_eps: 0.680522
 32528/100000: episode: 308, duration: 1.501s, episode steps: 148, steps per second:  99, episode reward: -142.677, mean reward: -0.964 [-100.000,  7.849], mean action: 1.689 [0.000, 3.000],  loss: 82.147429, mse: 546200.092272, mean_q: 573.305413, mean_eps: 0.678710
 32678/100000: episode: 309, duration: 1.419s, episode steps: 150, steps per second: 106, episode reward: -108.483, mean reward: -0.723 [-100.000, 12.276], mean action: 1.567 [0.000, 3.000],  loss: 75.696667, mse: 554308.565000, mean_q: 582.352838, mean_eps: 0.677235
 32806/100000: episode: 310, duration: 1.117s, episode steps: 128, steps per second: 115, episode reward: -29.964, mean reward: -0.234 [-100.000, 12.358], mean action: 1.781 [0.000, 3.000],  loss: 79.499272, mse: 566550.739380, mean_q: 593.476826, mean_eps: 0.675859
 32960/100000: episode: 311, duration: 1.125s, episode steps: 154, steps per second: 137, episode reward: -169.767, mean reward: -1.102 [-100.000, 24.888], mean action: 1.591 [0.000, 3.000],  loss: 76.686451, mse: 561205.318385, mean_q: 587.352450, mean_eps: 0.674463
 33228/100000: episode: 312, duration: 1.932s, episode steps: 268, steps per second: 139, episode reward: -93.387, mean reward: -0.348 [-100.000, 66.826], mean action: 1.668 [0.000, 3.000],  loss: 78.315418, mse: 549587.253965, mean_q: 580.991907, mean_eps: 0.672374
 33319/100000: episode: 313, duration: 0.816s, episode steps:  91, steps per second: 112, episode reward: -67.878, mean reward: -0.746 [-100.000,  8.792], mean action: 1.637 [0.000, 3.000],  loss: 82.249870, mse: 573029.923077, mean_q: 599.095577, mean_eps: 0.670597
 33523/100000: episode: 314, duration: 1.616s, episode steps: 204, steps per second: 126, episode reward: -213.256, mean reward: -1.045 [-100.000,  5.298], mean action: 1.588 [0.000, 3.000],  loss: 74.248066, mse: 554188.538297, mean_q: 589.164462, mean_eps: 0.669137
 33721/100000: episode: 315, duration: 1.416s, episode steps: 198, steps per second: 140, episode reward: -375.456, mean reward: -1.896 [-100.000, 114.658], mean action: 1.636 [0.000, 3.000],  loss: 74.385145, mse: 551387.850379, mean_q: 585.015949, mean_eps: 0.667147
 33827/100000: episode: 316, duration: 0.723s, episode steps: 106, steps per second: 147, episode reward: -23.731, mean reward: -0.224 [-100.000, 79.476], mean action: 1.613 [0.000, 3.000],  loss: 73.288846, mse: 543631.949882, mean_q: 581.574278, mean_eps: 0.665642
 34005/100000: episode: 317, duration: 1.278s, episode steps: 178, steps per second: 139, episode reward: -110.015, mean reward: -0.618 [-100.000, 14.604], mean action: 1.663 [0.000, 3.000],  loss: 79.070576, mse: 552602.730337, mean_q: 586.448417, mean_eps: 0.664237
 35005/100000: episode: 318, duration: 7.881s, episode steps: 1000, steps per second: 127, episode reward: 22.871, mean reward:  0.023 [-20.453, 21.869], mean action: 1.700 [0.000, 3.000],  loss: 80.828450, mse: 534775.035250, mean_q: 571.175438, mean_eps: 0.658405
 35236/100000: episode: 319, duration: 1.789s, episode steps: 231, steps per second: 129, episode reward: -182.569, mean reward: -0.790 [-100.000,  9.756], mean action: 1.684 [0.000, 3.000],  loss: 81.370061, mse: 526419.347808, mean_q: 560.651824, mean_eps: 0.652312
 35434/100000: episode: 320, duration: 1.446s, episode steps: 198, steps per second: 137, episode reward: -67.551, mean reward: -0.341 [-100.000, 11.395], mean action: 1.631 [0.000, 3.000],  loss: 81.842038, mse: 530706.507891, mean_q: 563.328707, mean_eps: 0.650188
 35561/100000: episode: 321, duration: 0.853s, episode steps: 127, steps per second: 149, episode reward: -145.233, mean reward: -1.144 [-100.000, 12.616], mean action: 1.512 [0.000, 3.000],  loss: 76.772513, mse: 532570.586614, mean_q: 566.942033, mean_eps: 0.648580
 35674/100000: episode: 322, duration: 0.781s, episode steps: 113, steps per second: 145, episode reward: -165.234, mean reward: -1.462 [-100.000,  1.882], mean action: 1.673 [0.000, 3.000],  loss: 86.439317, mse: 536227.383296, mean_q: 573.387345, mean_eps: 0.647392
 36107/100000: episode: 323, duration: 3.074s, episode steps: 433, steps per second: 141, episode reward: -150.832, mean reward: -0.348 [-100.000,  9.251], mean action: 1.674 [0.000, 3.000],  loss: 78.682390, mse: 563139.494154, mean_q: 587.226559, mean_eps: 0.644689
 36181/100000: episode: 324, duration: 0.486s, episode steps:  74, steps per second: 152, episode reward: -161.955, mean reward: -2.189 [-100.000,  6.735], mean action: 1.689 [0.000, 3.000],  loss: 88.418488, mse: 570117.561233, mean_q: 590.726820, mean_eps: 0.642179
 36293/100000: episode: 325, duration: 0.750s, episode steps: 112, steps per second: 149, episode reward: -102.467, mean reward: -0.915 [-100.000, 76.275], mean action: 1.580 [0.000, 3.000],  loss: 84.279766, mse: 582073.419364, mean_q: 602.528842, mean_eps: 0.641259
 36447/100000: episode: 326, duration: 1.079s, episode steps: 154, steps per second: 143, episode reward: -41.635, mean reward: -0.270 [-100.000, 69.373], mean action: 1.675 [0.000, 3.000],  loss: 86.136483, mse: 553252.376218, mean_q: 576.471740, mean_eps: 0.639942
 36661/100000: episode: 327, duration: 1.475s, episode steps: 214, steps per second: 145, episode reward: -190.865, mean reward: -0.892 [-100.000,  7.219], mean action: 1.636 [0.000, 3.000],  loss: 81.177799, mse: 573764.965975, mean_q: 592.225330, mean_eps: 0.638120
 36895/100000: episode: 328, duration: 1.579s, episode steps: 234, steps per second: 148, episode reward: -332.910, mean reward: -1.423 [-100.000,  4.408], mean action: 1.752 [0.000, 3.000],  loss: 78.062927, mse: 567141.651042, mean_q: 590.218151, mean_eps: 0.635903
 37153/100000: episode: 329, duration: 1.828s, episode steps: 258, steps per second: 141, episode reward: -218.908, mean reward: -0.848 [-100.000,  6.847], mean action: 1.779 [0.000, 3.000],  loss: 81.218324, mse: 574405.775073, mean_q: 598.190920, mean_eps: 0.633467
 37237/100000: episode: 330, duration: 0.584s, episode steps:  84, steps per second: 144, episode reward: -71.256, mean reward: -0.848 [-100.000,  7.218], mean action: 1.726 [0.000, 3.000],  loss: 85.263024, mse: 570863.037946, mean_q: 591.575423, mean_eps: 0.631774
 37462/100000: episode: 331, duration: 1.545s, episode steps: 225, steps per second: 146, episode reward: -53.091, mean reward: -0.236 [-100.000,  7.702], mean action: 1.653 [0.000, 3.000],  loss: 85.700082, mse: 575390.925278, mean_q: 594.267692, mean_eps: 0.630245
 37622/100000: episode: 332, duration: 1.138s, episode steps: 160, steps per second: 141, episode reward: -86.461, mean reward: -0.540 [-100.000, 20.514], mean action: 1.806 [0.000, 3.000],  loss: 78.134142, mse: 581025.929688, mean_q: 599.950275, mean_eps: 0.628339
 37793/100000: episode: 333, duration: 1.196s, episode steps: 171, steps per second: 143, episode reward: -12.846, mean reward: -0.075 [-100.000, 15.436], mean action: 1.725 [0.000, 3.000],  loss: 76.242611, mse: 581366.062317, mean_q: 601.276282, mean_eps: 0.626701
 37907/100000: episode: 334, duration: 0.801s, episode steps: 114, steps per second: 142, episode reward: -59.975, mean reward: -0.526 [-100.000, 10.071], mean action: 1.649 [0.000, 3.000],  loss: 90.808397, mse: 582069.635417, mean_q: 602.400488, mean_eps: 0.625290
 38102/100000: episode: 335, duration: 1.289s, episode steps: 195, steps per second: 151, episode reward: -102.025, mean reward: -0.523 [-100.000, 13.893], mean action: 1.595 [0.000, 3.000],  loss: 82.289302, mse: 573061.123878, mean_q: 592.735303, mean_eps: 0.623760
 38377/100000: episode: 336, duration: 1.914s, episode steps: 275, steps per second: 144, episode reward: -160.701, mean reward: -0.584 [-100.000, 14.388], mean action: 1.742 [0.000, 3.000],  loss: 78.424639, mse: 586124.627841, mean_q: 603.792878, mean_eps: 0.621434
 38477/100000: episode: 337, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -31.103, mean reward: -0.311 [-100.000, 11.532], mean action: 1.740 [0.000, 3.000],  loss: 76.313804, mse: 575038.535000, mean_q: 595.853538, mean_eps: 0.619578
 38677/100000: episode: 338, duration: 1.463s, episode steps: 200, steps per second: 137, episode reward: -15.573, mean reward: -0.078 [-100.000,  8.331], mean action: 1.685 [0.000, 3.000],  loss: 85.691165, mse: 593379.257812, mean_q: 612.588829, mean_eps: 0.618093
 38760/100000: episode: 339, duration: 0.583s, episode steps:  83, steps per second: 142, episode reward: -295.127, mean reward: -3.556 [-100.000, 115.005], mean action: 1.301 [0.000, 3.000],  loss: 86.438114, mse: 592933.927334, mean_q: 611.728566, mean_eps: 0.616692
 38925/100000: episode: 340, duration: 1.261s, episode steps: 165, steps per second: 131, episode reward: -225.760, mean reward: -1.368 [-100.000, 67.492], mean action: 1.648 [0.000, 3.000],  loss: 83.996206, mse: 595414.249242, mean_q: 609.771104, mean_eps: 0.615464
 39059/100000: episode: 341, duration: 0.956s, episode steps: 134, steps per second: 140, episode reward: -110.427, mean reward: -0.824 [-100.000, 11.041], mean action: 1.754 [0.000, 3.000],  loss: 85.713832, mse: 614389.606810, mean_q: 627.322458, mean_eps: 0.613984
 39199/100000: episode: 342, duration: 1.046s, episode steps: 140, steps per second: 134, episode reward: -179.062, mean reward: -1.279 [-100.000, 75.771], mean action: 1.679 [0.000, 3.000],  loss: 89.023075, mse: 606617.607143, mean_q: 617.541180, mean_eps: 0.612628
 39286/100000: episode: 343, duration: 0.625s, episode steps:  87, steps per second: 139, episode reward: -69.911, mean reward: -0.804 [-100.000, 11.310], mean action: 1.851 [0.000, 3.000],  loss: 87.945933, mse: 598235.830819, mean_q: 612.927192, mean_eps: 0.611504
 39479/100000: episode: 344, duration: 1.419s, episode steps: 193, steps per second: 136, episode reward: -56.008, mean reward: -0.290 [-100.000, 33.899], mean action: 1.684 [0.000, 3.000],  loss: 88.028371, mse: 614539.096665, mean_q: 627.295519, mean_eps: 0.610118
 39601/100000: episode: 345, duration: 0.848s, episode steps: 122, steps per second: 144, episode reward: -68.608, mean reward: -0.562 [-100.000, 17.618], mean action: 1.639 [0.000, 3.000],  loss: 88.976452, mse: 641511.440574, mean_q: 651.744769, mean_eps: 0.608559
 39834/100000: episode: 346, duration: 1.813s, episode steps: 233, steps per second: 129, episode reward: -77.004, mean reward: -0.330 [-100.000, 16.872], mean action: 1.691 [0.000, 3.000],  loss: 86.217455, mse: 625703.777897, mean_q: 635.164714, mean_eps: 0.606802
 39991/100000: episode: 347, duration: 1.336s, episode steps: 157, steps per second: 118, episode reward: -127.870, mean reward: -0.814 [-100.000, 11.022], mean action: 1.682 [0.000, 3.000],  loss: 104.166210, mse: 633471.249602, mean_q: 644.520424, mean_eps: 0.604871
 40148/100000: episode: 348, duration: 1.240s, episode steps: 157, steps per second: 127, episode reward: -46.319, mean reward: -0.295 [-100.000, 14.475], mean action: 1.707 [0.000, 3.000],  loss: 99.269263, mse: 637412.190486, mean_q: 647.394036, mean_eps: 0.603317
 40394/100000: episode: 349, duration: 1.899s, episode steps: 246, steps per second: 130, episode reward: -187.704, mean reward: -0.763 [-100.000, 20.062], mean action: 1.744 [0.000, 3.000],  loss: 98.589804, mse: 649244.982470, mean_q: 659.157827, mean_eps: 0.601322
 40645/100000: episode: 350, duration: 1.969s, episode steps: 251, steps per second: 127, episode reward: -54.954, mean reward: -0.219 [-100.000, 15.922], mean action: 1.781 [0.000, 3.000],  loss: 97.034917, mse: 658226.936753, mean_q: 666.755541, mean_eps: 0.598862
 40806/100000: episode: 351, duration: 1.292s, episode steps: 161, steps per second: 125, episode reward: -115.246, mean reward: -0.716 [-100.000,  8.449], mean action: 1.708 [0.000, 3.000],  loss: 103.558245, mse: 684025.197981, mean_q: 688.430472, mean_eps: 0.596822
 41015/100000: episode: 352, duration: 1.460s, episode steps: 209, steps per second: 143, episode reward: -156.417, mean reward: -0.748 [-100.000, 20.871], mean action: 1.665 [0.000, 3.000],  loss: 117.746107, mse: 692429.727123, mean_q: 691.367806, mean_eps: 0.594991
 41148/100000: episode: 353, duration: 1.088s, episode steps: 133, steps per second: 122, episode reward: -96.537, mean reward: -0.726 [-100.000, 11.151], mean action: 1.662 [0.000, 3.000],  loss: 108.247681, mse: 710166.587641, mean_q: 704.811422, mean_eps: 0.593298
 41253/100000: episode: 354, duration: 0.832s, episode steps: 105, steps per second: 126, episode reward: -59.597, mean reward: -0.568 [-100.000, 15.461], mean action: 1.638 [0.000, 3.000],  loss: 119.110553, mse: 707782.186012, mean_q: 701.564525, mean_eps: 0.592120
 41336/100000: episode: 355, duration: 0.685s, episode steps:  83, steps per second: 121, episode reward: -30.236, mean reward: -0.364 [-100.000, 69.493], mean action: 1.807 [0.000, 3.000],  loss: 120.891351, mse: 697816.387801, mean_q: 705.103336, mean_eps: 0.591189
 41563/100000: episode: 356, duration: 1.731s, episode steps: 227, steps per second: 131, episode reward: -21.415, mean reward: -0.094 [-100.000, 19.025], mean action: 1.674 [0.000, 3.000],  loss: 127.700015, mse: 716571.644273, mean_q: 713.455399, mean_eps: 0.589655
 41671/100000: episode: 357, duration: 0.875s, episode steps: 108, steps per second: 123, episode reward: -130.639, mean reward: -1.210 [-100.000, 12.318], mean action: 1.713 [0.000, 3.000],  loss: 125.007809, mse: 724496.277778, mean_q: 716.631350, mean_eps: 0.587997
 41905/100000: episode: 358, duration: 1.709s, episode steps: 234, steps per second: 137, episode reward: -98.134, mean reward: -0.419 [-100.000,  7.996], mean action: 1.752 [0.000, 3.000],  loss: 138.260131, mse: 754729.821982, mean_q: 744.060148, mean_eps: 0.586304
 42039/100000: episode: 359, duration: 0.979s, episode steps: 134, steps per second: 137, episode reward: -210.362, mean reward: -1.570 [-100.000, 17.710], mean action: 1.634 [0.000, 3.000],  loss: 158.720998, mse: 739253.493237, mean_q: 732.569562, mean_eps: 0.584482
 42208/100000: episode: 360, duration: 1.213s, episode steps: 169, steps per second: 139, episode reward: -166.039, mean reward: -0.982 [-100.000,  3.755], mean action: 1.633 [0.000, 3.000],  loss: 160.031275, mse: 769260.581176, mean_q: 756.798552, mean_eps: 0.582982
 42360/100000: episode: 361, duration: 1.060s, episode steps: 152, steps per second: 143, episode reward: -160.131, mean reward: -1.053 [-100.000, 19.208], mean action: 1.750 [0.000, 3.000],  loss: 242.412010, mse: 772112.483964, mean_q: 760.425160, mean_eps: 0.581393
 42456/100000: episode: 362, duration: 0.648s, episode steps:  96, steps per second: 148, episode reward: -164.046, mean reward: -1.709 [-100.000,  7.342], mean action: 1.844 [0.000, 3.000],  loss: 228.648096, mse: 780690.698568, mean_q: 769.025260, mean_eps: 0.580166
 42802/100000: episode: 363, duration: 2.528s, episode steps: 346, steps per second: 137, episode reward: -239.700, mean reward: -0.693 [-100.000, 36.380], mean action: 1.633 [0.000, 3.000],  loss: 303.786251, mse: 787092.171785, mean_q: 777.736367, mean_eps: 0.577978
 43019/100000: episode: 364, duration: 1.547s, episode steps: 217, steps per second: 140, episode reward: -132.792, mean reward: -0.612 [-100.000, 13.674], mean action: 1.691 [0.000, 3.000],  loss: 339.148896, mse: 797133.281106, mean_q: 785.722003, mean_eps: 0.575191
 43136/100000: episode: 365, duration: 0.834s, episode steps: 117, steps per second: 140, episode reward: -204.847, mean reward: -1.751 [-100.000,  4.533], mean action: 1.709 [0.000, 3.000],  loss: 383.951869, mse: 813726.913996, mean_q: 797.276722, mean_eps: 0.573538
 43251/100000: episode: 366, duration: 0.800s, episode steps: 115, steps per second: 144, episode reward: -9.901, mean reward: -0.086 [-100.000,  7.821], mean action: 1.678 [0.000, 3.000],  loss: 261.191948, mse: 815165.969022, mean_q: 804.469205, mean_eps: 0.572389
 43483/100000: episode: 367, duration: 1.676s, episode steps: 232, steps per second: 138, episode reward: -26.607, mean reward: -0.115 [-100.000, 15.850], mean action: 1.625 [0.000, 3.000],  loss: 381.337568, mse: 834539.924838, mean_q: 822.872929, mean_eps: 0.570672
 43577/100000: episode: 368, duration: 0.663s, episode steps:  94, steps per second: 142, episode reward: -51.182, mean reward: -0.544 [-100.000, 18.898], mean action: 1.574 [0.000, 3.000],  loss: 368.329847, mse: 822009.793218, mean_q: 817.558507, mean_eps: 0.569058
 43964/100000: episode: 369, duration: 3.187s, episode steps: 387, steps per second: 121, episode reward: -329.118, mean reward: -0.850 [-100.000, 36.972], mean action: 1.775 [0.000, 3.000],  loss: 602.113199, mse: 838991.421027, mean_q: 833.917883, mean_eps: 0.566677
 44086/100000: episode: 370, duration: 0.933s, episode steps: 122, steps per second: 131, episode reward: -113.955, mean reward: -0.934 [-100.000, 10.828], mean action: 1.836 [0.000, 3.000],  loss: 586.839780, mse: 872514.103484, mean_q: 855.851607, mean_eps: 0.564157
 44284/100000: episode: 371, duration: 1.554s, episode steps: 198, steps per second: 127, episode reward: -102.148, mean reward: -0.516 [-100.000,  6.370], mean action: 1.636 [0.000, 3.000],  loss: 600.659698, mse: 875980.294508, mean_q: 857.032596, mean_eps: 0.562573
 44540/100000: episode: 372, duration: 1.870s, episode steps: 256, steps per second: 137, episode reward: -119.349, mean reward: -0.466 [-100.000,  9.563], mean action: 1.695 [0.000, 3.000],  loss: 522.501751, mse: 895647.951904, mean_q: 868.433394, mean_eps: 0.560326
 44828/100000: episode: 373, duration: 2.058s, episode steps: 288, steps per second: 140, episode reward: -274.765, mean reward: -0.954 [-100.000, 11.202], mean action: 1.656 [0.000, 3.000],  loss: 455.282765, mse: 885542.491970, mean_q: 861.673704, mean_eps: 0.557633
 45035/100000: episode: 374, duration: 1.476s, episode steps: 207, steps per second: 140, episode reward: -45.895, mean reward: -0.222 [-100.000, 18.864], mean action: 1.792 [0.000, 3.000],  loss: 595.299905, mse: 914879.425725, mean_q: 879.207122, mean_eps: 0.555183
 45294/100000: episode: 375, duration: 1.890s, episode steps: 259, steps per second: 137, episode reward: -226.332, mean reward: -0.874 [-100.000,  4.914], mean action: 1.602 [0.000, 3.000],  loss: 749.318335, mse: 977926.215734, mean_q: 916.414751, mean_eps: 0.552876
 45546/100000: episode: 376, duration: 1.842s, episode steps: 252, steps per second: 137, episode reward: -82.727, mean reward: -0.328 [-100.000, 20.372], mean action: 1.770 [0.000, 3.000],  loss: 663.037713, mse: 993854.400794, mean_q: 929.257186, mean_eps: 0.550347
 45702/100000: episode: 377, duration: 1.125s, episode steps: 156, steps per second: 139, episode reward: -82.466, mean reward: -0.529 [-100.000, 15.070], mean action: 1.590 [0.000, 3.000],  loss: 1012.944317, mse: 1033777.063702, mean_q: 956.387518, mean_eps: 0.548327
 45991/100000: episode: 378, duration: 2.199s, episode steps: 289, steps per second: 131, episode reward: -23.744, mean reward: -0.082 [-100.000, 26.965], mean action: 1.765 [0.000, 3.000],  loss: 857.800681, mse: 1006300.807526, mean_q: 946.746740, mean_eps: 0.546125
 46104/100000: episode: 379, duration: 0.928s, episode steps: 113, steps per second: 122, episode reward: -52.668, mean reward: -0.466 [-100.000,  8.324], mean action: 1.690 [0.000, 3.000],  loss: 647.057268, mse: 1032981.336283, mean_q: 968.755324, mean_eps: 0.544135
 46589/100000: episode: 380, duration: 3.796s, episode steps: 485, steps per second: 128, episode reward: -33.069, mean reward: -0.068 [-100.000, 14.970], mean action: 1.765 [0.000, 3.000],  loss: 917.716869, mse: 1050396.382861, mean_q: 984.822481, mean_eps: 0.541175
 46811/100000: episode: 381, duration: 1.668s, episode steps: 222, steps per second: 133, episode reward: -78.695, mean reward: -0.354 [-100.000,  8.965], mean action: 1.608 [0.000, 3.000],  loss: 644.910585, mse: 1101876.607264, mean_q: 1024.281698, mean_eps: 0.537675
 46965/100000: episode: 382, duration: 1.118s, episode steps: 154, steps per second: 138, episode reward: -159.873, mean reward: -1.038 [-100.000,  4.581], mean action: 1.597 [0.000, 3.000],  loss: 834.472501, mse: 1116112.469156, mean_q: 1036.853769, mean_eps: 0.535814
 47540/100000: episode: 383, duration: 4.860s, episode steps: 575, steps per second: 118, episode reward: -345.655, mean reward: -0.601 [-100.000, 18.142], mean action: 1.751 [0.000, 3.000],  loss: 903.739576, mse: 1147315.245435, mean_q: 1052.987168, mean_eps: 0.532205
 47710/100000: episode: 384, duration: 1.354s, episode steps: 170, steps per second: 126, episode reward: -256.781, mean reward: -1.510 [-100.000, 13.355], mean action: 1.729 [0.000, 3.000],  loss: 1257.774036, mse: 1189632.193015, mean_q: 1087.983218, mean_eps: 0.528517
 47887/100000: episode: 385, duration: 1.406s, episode steps: 177, steps per second: 126, episode reward:  0.665, mean reward:  0.004 [-100.000, 13.990], mean action: 1.689 [0.000, 3.000],  loss: 1000.598540, mse: 1168246.779661, mean_q: 1073.639153, mean_eps: 0.526800
 48034/100000: episode: 386, duration: 1.135s, episode steps: 147, steps per second: 130, episode reward: -146.507, mean reward: -0.997 [-100.000, 11.496], mean action: 1.741 [0.000, 3.000],  loss: 1026.542075, mse: 1165696.814201, mean_q: 1074.910949, mean_eps: 0.525196
 48145/100000: episode: 387, duration: 1.215s, episode steps: 111, steps per second:  91, episode reward: -56.038, mean reward: -0.505 [-100.000, 12.439], mean action: 1.694 [0.000, 3.000],  loss: 900.968213, mse: 1181374.453266, mean_q: 1083.050649, mean_eps: 0.523919
 48234/100000: episode: 388, duration: 0.710s, episode steps:  89, steps per second: 125, episode reward: -40.041, mean reward: -0.450 [-100.000, 11.116], mean action: 1.719 [0.000, 3.000],  loss: 909.409950, mse: 1193933.537219, mean_q: 1090.855127, mean_eps: 0.522929
 48336/100000: episode: 389, duration: 0.885s, episode steps: 102, steps per second: 115, episode reward: -7.035, mean reward: -0.069 [-100.000,  9.463], mean action: 1.814 [0.000, 3.000],  loss: 988.486953, mse: 1188718.070466, mean_q: 1099.038701, mean_eps: 0.521983
 48483/100000: episode: 390, duration: 1.238s, episode steps: 147, steps per second: 119, episode reward: -212.227, mean reward: -1.444 [-100.000, 21.151], mean action: 1.796 [0.000, 3.000],  loss: 1016.445433, mse: 1215605.159439, mean_q: 1112.135747, mean_eps: 0.520751
 48989/100000: episode: 391, duration: 4.313s, episode steps: 506, steps per second: 117, episode reward: -299.494, mean reward: -0.592 [-100.000, 19.017], mean action: 1.749 [0.000, 3.000],  loss: 1295.084616, mse: 1252808.089303, mean_q: 1131.116234, mean_eps: 0.517519
 49132/100000: episode: 392, duration: 1.022s, episode steps: 143, steps per second: 140, episode reward: -16.930, mean reward: -0.118 [-100.000, 16.956], mean action: 1.657 [0.000, 3.000],  loss: 1101.552781, mse: 1291005.870192, mean_q: 1152.383769, mean_eps: 0.514306
 49550/100000: episode: 393, duration: 3.185s, episode steps: 418, steps per second: 131, episode reward: -209.062, mean reward: -0.500 [-100.000, 23.151], mean action: 1.758 [0.000, 3.000],  loss: 1585.359415, mse: 1330450.198565, mean_q: 1171.718111, mean_eps: 0.511529
 49897/100000: episode: 394, duration: 2.916s, episode steps: 347, steps per second: 119, episode reward: -0.796, mean reward: -0.002 [-100.000, 15.351], mean action: 1.761 [0.000, 3.000],  loss: 1323.276246, mse: 1349185.112572, mean_q: 1181.086278, mean_eps: 0.507742
 50897/100000: episode: 395, duration: 8.259s, episode steps: 1000, steps per second: 121, episode reward: -13.932, mean reward: -0.014 [-23.357, 25.583], mean action: 1.706 [0.000, 3.000],  loss: 1831.437617, mse: 1454637.453625, mean_q: 1238.224223, mean_eps: 0.501075
 51258/100000: episode: 396, duration: 2.705s, episode steps: 361, steps per second: 133, episode reward: -376.259, mean reward: -1.042 [-100.000, 38.906], mean action: 1.825 [0.000, 3.000],  loss: 2089.150322, mse: 1548438.219875, mean_q: 1282.580724, mean_eps: 0.494338
 51441/100000: episode: 397, duration: 1.438s, episode steps: 183, steps per second: 127, episode reward: -134.060, mean reward: -0.733 [-100.000, 74.072], mean action: 1.803 [0.000, 3.000],  loss: 1760.103204, mse: 1589063.410519, mean_q: 1304.678789, mean_eps: 0.491645
 51545/100000: episode: 398, duration: 0.812s, episode steps: 104, steps per second: 128, episode reward: -35.163, mean reward: -0.338 [-100.000, 24.062], mean action: 1.712 [0.000, 3.000],  loss: 1051.346924, mse: 1610163.372596, mean_q: 1315.150654, mean_eps: 0.490224
 51669/100000: episode: 399, duration: 1.060s, episode steps: 124, steps per second: 117, episode reward: -34.722, mean reward: -0.280 [-100.000, 21.542], mean action: 1.621 [0.000, 3.000],  loss: 2147.061879, mse: 1642249.221774, mean_q: 1335.390526, mean_eps: 0.489096
 51756/100000: episode: 400, duration: 0.632s, episode steps:  87, steps per second: 138, episode reward: -182.402, mean reward: -2.097 [-100.000,  5.597], mean action: 1.701 [0.000, 3.000],  loss: 2092.148936, mse: 1649677.422414, mean_q: 1333.867676, mean_eps: 0.488051
 51853/100000: episode: 401, duration: 0.670s, episode steps:  97, steps per second: 145, episode reward: -45.587, mean reward: -0.470 [-100.000, 16.983], mean action: 1.660 [0.000, 3.000],  loss: 1348.958621, mse: 1639519.377577, mean_q: 1326.049299, mean_eps: 0.487140
 51968/100000: episode: 402, duration: 0.828s, episode steps: 115, steps per second: 139, episode reward:  3.523, mean reward:  0.031 [-100.000, 16.860], mean action: 1.617 [0.000, 3.000],  loss: 1894.173011, mse: 1682223.490217, mean_q: 1353.181221, mean_eps: 0.486091
 52044/100000: episode: 403, duration: 0.542s, episode steps:  76, steps per second: 140, episode reward: -12.116, mean reward: -0.159 [-100.000, 26.251], mean action: 1.776 [0.000, 3.000],  loss: 1720.839574, mse: 1686878.008224, mean_q: 1352.633951, mean_eps: 0.485146
 52141/100000: episode: 404, duration: 0.739s, episode steps:  97, steps per second: 131, episode reward: -146.278, mean reward: -1.508 [-100.000, 10.774], mean action: 1.928 [0.000, 3.000],  loss: 1485.469567, mse: 1706496.599227, mean_q: 1358.034358, mean_eps: 0.484289
 52250/100000: episode: 405, duration: 0.916s, episode steps: 109, steps per second: 119, episode reward: -51.794, mean reward: -0.475 [-100.000,  9.149], mean action: 1.661 [0.000, 3.000],  loss: 2006.959296, mse: 1696381.608945, mean_q: 1353.535054, mean_eps: 0.483269
 53250/100000: episode: 406, duration: 9.059s, episode steps: 1000, steps per second: 110, episode reward:  9.163, mean reward:  0.009 [-23.009, 19.806], mean action: 1.775 [0.000, 3.000],  loss: 1860.587659, mse: 1826195.484875, mean_q: 1407.862380, mean_eps: 0.477780
 53380/100000: episode: 407, duration: 0.984s, episode steps: 130, steps per second: 132, episode reward: 20.564, mean reward:  0.158 [-100.000, 15.221], mean action: 1.746 [0.000, 3.000],  loss: 2478.936099, mse: 1971605.951923, mean_q: 1461.530745, mean_eps: 0.472186
 54380/100000: episode: 408, duration: 8.366s, episode steps: 1000, steps per second: 120, episode reward:  2.510, mean reward:  0.003 [-18.187, 17.740], mean action: 1.793 [0.000, 3.000],  loss: 2206.028247, mse: 2177889.679750, mean_q: 1546.974195, mean_eps: 0.466593
 54502/100000: episode: 409, duration: 0.889s, episode steps: 122, steps per second: 137, episode reward: -19.466, mean reward: -0.160 [-100.000, 15.013], mean action: 1.811 [0.000, 3.000],  loss: 1651.700072, mse: 2391960.100410, mean_q: 1632.208055, mean_eps: 0.461039
 54608/100000: episode: 410, duration: 0.778s, episode steps: 106, steps per second: 136, episode reward:  1.436, mean reward:  0.014 [-100.000, 19.324], mean action: 1.689 [0.000, 3.000],  loss: 2714.179071, mse: 2422551.876179, mean_q: 1652.546792, mean_eps: 0.459910
 54852/100000: episode: 411, duration: 1.878s, episode steps: 244, steps per second: 130, episode reward: -262.928, mean reward: -1.078 [-100.000, 27.538], mean action: 1.676 [0.000, 3.000],  loss: 2544.041475, mse: 2463203.850922, mean_q: 1659.824891, mean_eps: 0.458178
 54989/100000: episode: 412, duration: 0.980s, episode steps: 137, steps per second: 140, episode reward: -188.710, mean reward: -1.377 [-100.000, 105.700], mean action: 1.964 [0.000, 3.000],  loss: 3875.919107, mse: 2459836.071168, mean_q: 1657.578555, mean_eps: 0.456292
 55198/100000: episode: 413, duration: 1.557s, episode steps: 209, steps per second: 134, episode reward: -127.142, mean reward: -0.608 [-100.000, 16.128], mean action: 1.775 [0.000, 3.000],  loss: 2881.227047, mse: 2501827.354067, mean_q: 1673.625693, mean_eps: 0.454579
 56198/100000: episode: 414, duration: 8.683s, episode steps: 1000, steps per second: 115, episode reward: -51.769, mean reward: -0.052 [-22.326, 77.768], mean action: 1.721 [0.000, 3.000],  loss: 2324.744656, mse: 2644285.967750, mean_q: 1725.792865, mean_eps: 0.448595
 56354/100000: episode: 415, duration: 1.131s, episode steps: 156, steps per second: 138, episode reward: -92.935, mean reward: -0.596 [-100.000, 19.947], mean action: 1.718 [0.000, 3.000],  loss: 2819.422262, mse: 2714936.966346, mean_q: 1752.795433, mean_eps: 0.442873
 56517/100000: episode: 416, duration: 1.270s, episode steps: 163, steps per second: 128, episode reward: 19.842, mean reward:  0.122 [-100.000, 18.747], mean action: 1.712 [0.000, 3.000],  loss: 3093.675896, mse: 2724762.450920, mean_q: 1755.200869, mean_eps: 0.441294
 56627/100000: episode: 417, duration: 0.788s, episode steps: 110, steps per second: 140, episode reward: -34.594, mean reward: -0.314 [-100.000, 10.110], mean action: 1.473 [0.000, 3.000],  loss: 2509.913855, mse: 2772776.722727, mean_q: 1776.422784, mean_eps: 0.439942
 57024/100000: episode: 418, duration: 3.297s, episode steps: 397, steps per second: 120, episode reward: -234.780, mean reward: -0.591 [-100.000, 12.767], mean action: 1.695 [0.000, 3.000],  loss: 2511.174203, mse: 2784667.373741, mean_q: 1784.098335, mean_eps: 0.437433
 57140/100000: episode: 419, duration: 1.005s, episode steps: 116, steps per second: 115, episode reward: -55.253, mean reward: -0.476 [-100.000,  7.022], mean action: 1.802 [0.000, 3.000],  loss: 3196.536192, mse: 2807091.683190, mean_q: 1800.835569, mean_eps: 0.434893
 57266/100000: episode: 420, duration: 1.318s, episode steps: 126, steps per second:  96, episode reward: -197.948, mean reward: -1.571 [-100.000, 18.647], mean action: 1.810 [0.000, 3.000],  loss: 3173.337685, mse: 2758897.105159, mean_q: 1779.366081, mean_eps: 0.433695
 58266/100000: episode: 421, duration: 9.446s, episode steps: 1000, steps per second: 106, episode reward: -55.287, mean reward: -0.055 [-21.704, 22.198], mean action: 1.780 [0.000, 3.000],  loss: 2682.820344, mse: 2836053.179750, mean_q: 1809.943206, mean_eps: 0.428122
 59266/100000: episode: 422, duration: 9.043s, episode steps: 1000, steps per second: 111, episode reward:  1.638, mean reward:  0.002 [-24.068, 20.757], mean action: 1.817 [0.000, 3.000],  loss: 2180.845251, mse: 2876886.667625, mean_q: 1824.867180, mean_eps: 0.418222
 59818/100000: episode: 423, duration: 5.028s, episode steps: 552, steps per second: 110, episode reward: -138.626, mean reward: -0.251 [-100.000, 21.744], mean action: 1.920 [0.000, 3.000],  loss: 2610.647162, mse: 3018053.093750, mean_q: 1878.306967, mean_eps: 0.410539
 59944/100000: episode: 424, duration: 1.510s, episode steps: 126, steps per second:  83, episode reward: -23.038, mean reward: -0.183 [-100.000, 50.369], mean action: 1.817 [0.000, 3.000],  loss: 3291.284204, mse: 3062903.970238, mean_q: 1898.963737, mean_eps: 0.407183
 60051/100000: episode: 425, duration: 1.192s, episode steps: 107, steps per second:  90, episode reward: 35.377, mean reward:  0.331 [-100.000, 16.178], mean action: 1.682 [0.000, 3.000],  loss: 2303.108700, mse: 3120652.586449, mean_q: 1922.965964, mean_eps: 0.406030
 60149/100000: episode: 426, duration: 0.890s, episode steps:  98, steps per second: 110, episode reward: -33.654, mean reward: -0.343 [-100.000, 11.472], mean action: 1.551 [0.000, 3.000],  loss: 3033.724608, mse: 3097813.163265, mean_q: 1904.201323, mean_eps: 0.405015
 60355/100000: episode: 427, duration: 1.876s, episode steps: 206, steps per second: 110, episode reward: -167.439, mean reward: -0.813 [-100.000,  9.540], mean action: 1.767 [0.000, 3.000],  loss: 2605.906996, mse: 3140070.266990, mean_q: 1920.226513, mean_eps: 0.403510
 61198/100000: episode: 428, duration: 8.634s, episode steps: 843, steps per second:  98, episode reward: -264.610, mean reward: -0.314 [-100.000, 25.349], mean action: 1.894 [0.000, 3.000],  loss: 2487.640618, mse: 3256715.291518, mean_q: 1954.409715, mean_eps: 0.398318
 61342/100000: episode: 429, duration: 1.050s, episode steps: 144, steps per second: 137, episode reward: -14.390, mean reward: -0.100 [-100.000, 25.504], mean action: 1.597 [0.000, 3.000],  loss: 3544.976567, mse: 3314754.840278, mean_q: 1967.985825, mean_eps: 0.393432
 61935/100000: episode: 430, duration: 5.574s, episode steps: 593, steps per second: 106, episode reward: -322.570, mean reward: -0.544 [-100.000, 50.994], mean action: 1.693 [0.000, 3.000],  loss: 2271.473179, mse: 3356581.223862, mean_q: 1979.265506, mean_eps: 0.389784
 62935/100000: episode: 431, duration: 8.847s, episode steps: 1000, steps per second: 113, episode reward: -43.034, mean reward: -0.043 [-12.636, 11.300], mean action: 1.727 [0.000, 3.000],  loss: 2390.527424, mse: 3423560.614750, mean_q: 1996.379867, mean_eps: 0.381898
 63054/100000: episode: 432, duration: 0.924s, episode steps: 119, steps per second: 129, episode reward:  4.871, mean reward:  0.041 [-100.000, 16.369], mean action: 1.639 [0.000, 3.000],  loss: 2943.335030, mse: 3481252.081933, mean_q: 2013.651904, mean_eps: 0.376359
 63348/100000: episode: 433, duration: 2.180s, episode steps: 294, steps per second: 135, episode reward: -278.383, mean reward: -0.947 [-100.000, 13.625], mean action: 1.861 [0.000, 3.000],  loss: 1971.198814, mse: 3464263.244048, mean_q: 2006.081256, mean_eps: 0.374315
 63584/100000: episode: 434, duration: 1.729s, episode steps: 236, steps per second: 137, episode reward: -239.521, mean reward: -1.015 [-100.000, 34.262], mean action: 1.784 [0.000, 3.000],  loss: 2253.673285, mse: 3434979.033898, mean_q: 1998.670602, mean_eps: 0.371692
 63698/100000: episode: 435, duration: 0.856s, episode steps: 114, steps per second: 133, episode reward: -48.317, mean reward: -0.424 [-100.000, 15.303], mean action: 1.711 [0.000, 3.000],  loss: 1013.290312, mse: 3400766.125000, mean_q: 1983.521330, mean_eps: 0.369959
 63842/100000: episode: 436, duration: 1.039s, episode steps: 144, steps per second: 139, episode reward: 34.077, mean reward:  0.237 [-100.000, 10.752], mean action: 1.778 [0.000, 3.000],  loss: 1975.178715, mse: 3417458.385417, mean_q: 1987.123163, mean_eps: 0.368682
 64842/100000: episode: 437, duration: 7.913s, episode steps: 1000, steps per second: 126, episode reward: -53.988, mean reward: -0.054 [-17.639, 15.124], mean action: 1.807 [0.000, 3.000],  loss: 1945.705086, mse: 3357044.786250, mean_q: 1961.530486, mean_eps: 0.363019
 64978/100000: episode: 438, duration: 1.040s, episode steps: 136, steps per second: 131, episode reward: -19.043, mean reward: -0.140 [-100.000, 11.125], mean action: 1.824 [0.000, 3.000],  loss: 1690.943537, mse: 3318008.027574, mean_q: 1949.935425, mean_eps: 0.357396
 65978/100000: episode: 439, duration: 9.398s, episode steps: 1000, steps per second: 106, episode reward: -61.846, mean reward: -0.062 [-15.307, 16.147], mean action: 1.787 [0.000, 3.000],  loss: 1593.698796, mse: 3332680.673250, mean_q: 1957.656230, mean_eps: 0.351773
 66687/100000: episode: 440, duration: 9.332s, episode steps: 709, steps per second:  76, episode reward: -36.999, mean reward: -0.052 [-100.000, 45.250], mean action: 1.828 [0.000, 3.000],  loss: 2218.704719, mse: 3331401.130113, mean_q: 1960.894534, mean_eps: 0.343313
 66879/100000: episode: 441, duration: 2.497s, episode steps: 192, steps per second:  77, episode reward: -37.928, mean reward: -0.198 [-100.000,  8.605], mean action: 1.792 [0.000, 3.000],  loss: 1928.126681, mse: 3321333.752604, mean_q: 1961.158721, mean_eps: 0.338853
 67009/100000: episode: 442, duration: 2.399s, episode steps: 130, steps per second:  54, episode reward: -38.059, mean reward: -0.293 [-100.000, 14.247], mean action: 1.838 [0.000, 3.000],  loss: 2114.911747, mse: 3308144.461538, mean_q: 1953.612902, mean_eps: 0.337259
 68009/100000: episode: 443, duration: 10.367s, episode steps: 1000, steps per second:  96, episode reward: -46.604, mean reward: -0.047 [-12.507, 18.655], mean action: 1.750 [0.000, 3.000],  loss: 2337.990788, mse: 3234855.137250, mean_q: 1936.872524, mean_eps: 0.331666
 68191/100000: episode: 444, duration: 1.564s, episode steps: 182, steps per second: 116, episode reward: -270.956, mean reward: -1.489 [-100.000, 18.360], mean action: 1.945 [0.000, 3.000],  loss: 2457.475772, mse: 3147069.543956, mean_q: 1919.739340, mean_eps: 0.325815
 68368/100000: episode: 445, duration: 1.615s, episode steps: 177, steps per second: 110, episode reward: -200.181, mean reward: -1.131 [-100.000, 10.355], mean action: 1.774 [0.000, 3.000],  loss: 2242.958074, mse: 3085496.475989, mean_q: 1896.461938, mean_eps: 0.324038
 68773/100000: episode: 446, duration: 3.566s, episode steps: 405, steps per second: 114, episode reward: -553.615, mean reward: -1.367 [-100.000, 12.214], mean action: 1.822 [0.000, 3.000],  loss: 1839.691635, mse: 3051474.390741, mean_q: 1887.958539, mean_eps: 0.321157
 69069/100000: episode: 447, duration: 2.390s, episode steps: 296, steps per second: 124, episode reward: -399.858, mean reward: -1.351 [-100.000,  5.022], mean action: 1.834 [0.000, 3.000],  loss: 2062.728782, mse: 3052052.763514, mean_q: 1890.422188, mean_eps: 0.317687
 69658/100000: episode: 448, duration: 4.889s, episode steps: 589, steps per second: 120, episode reward: -217.230, mean reward: -0.369 [-100.000, 12.890], mean action: 1.786 [0.000, 3.000],  loss: 2082.609028, mse: 3031978.289474, mean_q: 1890.947941, mean_eps: 0.313306
 69951/100000: episode: 449, duration: 2.485s, episode steps: 293, steps per second: 118, episode reward: -55.638, mean reward: -0.190 [-100.000, 16.608], mean action: 1.771 [0.000, 3.000],  loss: 1785.761103, mse: 3008948.222696, mean_q: 1885.487703, mean_eps: 0.308940
 70471/100000: episode: 450, duration: 4.603s, episode steps: 520, steps per second: 113, episode reward: -153.861, mean reward: -0.296 [-100.000, 13.006], mean action: 1.900 [0.000, 3.000],  loss: 1612.082815, mse: 2995828.984615, mean_q: 1884.060667, mean_eps: 0.304916
 71449/100000: episode: 451, duration: 9.710s, episode steps: 978, steps per second: 101, episode reward: -385.767, mean reward: -0.394 [-100.000, 26.361], mean action: 1.840 [0.000, 3.000],  loss: 2009.160983, mse: 2968480.531953, mean_q: 1880.944498, mean_eps: 0.297501
 71929/100000: episode: 452, duration: 4.508s, episode steps: 480, steps per second: 106, episode reward: -190.118, mean reward: -0.396 [-100.000,  9.596], mean action: 1.815 [0.000, 3.000],  loss: 1815.372284, mse: 2889907.754688, mean_q: 1861.034871, mean_eps: 0.290284
 72356/100000: episode: 453, duration: 3.611s, episode steps: 427, steps per second: 118, episode reward: -135.532, mean reward: -0.317 [-100.000, 11.933], mean action: 1.834 [0.000, 3.000],  loss: 2135.122102, mse: 2843387.165691, mean_q: 1849.569038, mean_eps: 0.285794
 72734/100000: episode: 454, duration: 2.893s, episode steps: 378, steps per second: 131, episode reward: -219.924, mean reward: -0.582 [-100.000,  7.180], mean action: 1.894 [0.000, 3.000],  loss: 1878.897086, mse: 2823495.363095, mean_q: 1844.799476, mean_eps: 0.281809
 73032/100000: episode: 455, duration: 2.519s, episode steps: 298, steps per second: 118, episode reward: -157.265, mean reward: -0.528 [-100.000, 11.804], mean action: 1.742 [0.000, 3.000],  loss: 1862.396964, mse: 2774893.721477, mean_q: 1823.140605, mean_eps: 0.278463
 73201/100000: episode: 456, duration: 1.270s, episode steps: 169, steps per second: 133, episode reward: -250.937, mean reward: -1.485 [-100.000,  4.439], mean action: 1.781 [0.000, 3.000],  loss: 1215.406644, mse: 2747293.557692, mean_q: 1817.617817, mean_eps: 0.276152
 73319/100000: episode: 457, duration: 0.838s, episode steps: 118, steps per second: 141, episode reward: -384.918, mean reward: -3.262 [-100.000,  3.279], mean action: 2.017 [0.000, 3.000],  loss: 2031.435432, mse: 2697603.235169, mean_q: 1796.806034, mean_eps: 0.274731
 73924/100000: episode: 458, duration: 5.361s, episode steps: 605, steps per second: 113, episode reward: -397.292, mean reward: -0.657 [-100.000,  7.829], mean action: 1.838 [0.000, 3.000],  loss: 1728.630577, mse: 2674121.782645, mean_q: 1794.198635, mean_eps: 0.271152
 74040/100000: episode: 459, duration: 1.023s, episode steps: 116, steps per second: 113, episode reward: -550.718, mean reward: -4.748 [-100.000,  3.457], mean action: 1.810 [0.000, 3.000],  loss: 1900.357539, mse: 2685145.105603, mean_q: 1800.559752, mean_eps: 0.267583
 74281/100000: episode: 460, duration: 2.434s, episode steps: 241, steps per second:  99, episode reward: -211.900, mean reward: -0.879 [-100.000, 10.776], mean action: 1.780 [0.000, 3.000],  loss: 1964.950643, mse: 2663258.617220, mean_q: 1790.453411, mean_eps: 0.265816
 74641/100000: episode: 461, duration: 3.349s, episode steps: 360, steps per second: 107, episode reward: -239.518, mean reward: -0.665 [-100.000,  6.450], mean action: 1.936 [0.000, 3.000],  loss: 1527.777569, mse: 2629265.277778, mean_q: 1775.428653, mean_eps: 0.262841
 75641/100000: episode: 462, duration: 8.692s, episode steps: 1000, steps per second: 115, episode reward: -231.456, mean reward: -0.231 [-8.623,  7.617], mean action: 1.845 [0.000, 3.000],  loss: 1572.211785, mse: 2548029.254875, mean_q: 1751.317386, mean_eps: 0.256109
 75805/100000: episode: 463, duration: 1.237s, episode steps: 164, steps per second: 133, episode reward: -268.638, mean reward: -1.638 [-100.000,  4.975], mean action: 1.799 [0.000, 3.000],  loss: 2073.651591, mse: 2488816.996189, mean_q: 1733.556767, mean_eps: 0.250347
 75943/100000: episode: 464, duration: 1.026s, episode steps: 138, steps per second: 135, episode reward: -269.854, mean reward: -1.955 [-100.000,  6.896], mean action: 1.848 [0.000, 3.000],  loss: 1954.463531, mse: 2499018.620471, mean_q: 1740.546629, mean_eps: 0.248852
 76926/100000: episode: 465, duration: 9.084s, episode steps: 983, steps per second: 108, episode reward: -337.010, mean reward: -0.343 [-100.000, 19.855], mean action: 1.859 [0.000, 3.000],  loss: 1760.155825, mse: 2477216.963123, mean_q: 1732.419525, mean_eps: 0.243303
 77301/100000: episode: 466, duration: 3.234s, episode steps: 375, steps per second: 116, episode reward: -512.864, mean reward: -1.368 [-100.000, 116.419], mean action: 1.795 [0.000, 3.000],  loss: 1606.089625, mse: 2430228.274333, mean_q: 1718.629498, mean_eps: 0.236581
 77680/100000: episode: 467, duration: 3.049s, episode steps: 379, steps per second: 124, episode reward: -258.395, mean reward: -0.682 [-100.000,  8.017], mean action: 1.823 [0.000, 3.000],  loss: 1790.576405, mse: 2393882.897757, mean_q: 1706.112640, mean_eps: 0.232849
 78068/100000: episode: 468, duration: 3.116s, episode steps: 388, steps per second: 125, episode reward: -368.187, mean reward: -0.949 [-100.000,  8.553], mean action: 1.843 [0.000, 3.000],  loss: 1244.407900, mse: 2357118.217784, mean_q: 1692.327927, mean_eps: 0.229052
 78222/100000: episode: 469, duration: 1.126s, episode steps: 154, steps per second: 137, episode reward: -357.071, mean reward: -2.319 [-100.000,  6.285], mean action: 1.799 [0.000, 3.000],  loss: 1353.623711, mse: 2348138.328734, mean_q: 1688.880783, mean_eps: 0.226369
 78769/100000: episode: 470, duration: 4.369s, episode steps: 547, steps per second: 125, episode reward: -510.875, mean reward: -0.934 [-100.000,  6.341], mean action: 1.920 [0.000, 3.000],  loss: 1381.476661, mse: 2298904.580210, mean_q: 1672.394243, mean_eps: 0.222900
 79162/100000: episode: 471, duration: 3.072s, episode steps: 393, steps per second: 128, episode reward: -379.850, mean reward: -0.967 [-100.000, 21.986], mean action: 1.819 [0.000, 3.000],  loss: 1586.201836, mse: 2281913.351781, mean_q: 1668.741441, mean_eps: 0.218246
 79340/100000: episode: 472, duration: 1.411s, episode steps: 178, steps per second: 126, episode reward: -358.495, mean reward: -2.014 [-100.000,  6.119], mean action: 1.775 [0.000, 3.000],  loss: 1086.371340, mse: 2292120.441011, mean_q: 1674.800753, mean_eps: 0.215420
 79480/100000: episode: 473, duration: 1.114s, episode steps: 140, steps per second: 126, episode reward: -323.610, mean reward: -2.312 [-100.000,  5.487], mean action: 1.843 [0.000, 3.000],  loss: 2268.169831, mse: 2273235.905357, mean_q: 1663.503597, mean_eps: 0.213846
 79621/100000: episode: 474, duration: 1.087s, episode steps: 141, steps per second: 130, episode reward: -353.504, mean reward: -2.507 [-100.000,  2.645], mean action: 1.972 [0.000, 3.000],  loss: 1458.618935, mse: 2300057.359929, mean_q: 1677.397389, mean_eps: 0.212455
 79739/100000: episode: 475, duration: 0.871s, episode steps: 118, steps per second: 135, episode reward: -397.340, mean reward: -3.367 [-100.000,  3.624], mean action: 2.042 [0.000, 3.000],  loss: 1231.211002, mse: 2330175.542373, mean_q: 1686.025306, mean_eps: 0.211173
 79841/100000: episode: 476, duration: 0.786s, episode steps: 102, steps per second: 130, episode reward: -283.328, mean reward: -2.778 [-100.000,  2.725], mean action: 1.971 [0.000, 3.000],  loss: 1717.969083, mse: 2344293.716912, mean_q: 1692.121630, mean_eps: 0.210084
 80001/100000: episode: 477, duration: 1.183s, episode steps: 160, steps per second: 135, episode reward: -442.905, mean reward: -2.768 [-100.000,  3.252], mean action: 1.894 [0.000, 3.000],  loss: 1466.631868, mse: 2362424.819531, mean_q: 1701.571825, mean_eps: 0.208787
 80118/100000: episode: 478, duration: 1.047s, episode steps: 117, steps per second: 112, episode reward: -327.900, mean reward: -2.803 [-100.000,  2.540], mean action: 1.872 [0.000, 3.000],  loss: 620.365932, mse: 2353749.867521, mean_q: 1697.698528, mean_eps: 0.207416
 80496/100000: episode: 479, duration: 3.173s, episode steps: 378, steps per second: 119, episode reward: -268.568, mean reward: -0.710 [-100.000,  5.744], mean action: 1.754 [0.000, 3.000],  loss: 1399.311926, mse: 2319682.788360, mean_q: 1685.575692, mean_eps: 0.204966
 80648/100000: episode: 480, duration: 1.418s, episode steps: 152, steps per second: 107, episode reward: -383.457, mean reward: -2.523 [-100.000,  8.534], mean action: 1.855 [0.000, 3.000],  loss: 1091.873111, mse: 2318555.870888, mean_q: 1686.942033, mean_eps: 0.202342
 81010/100000: episode: 481, duration: 3.034s, episode steps: 362, steps per second: 119, episode reward: -790.042, mean reward: -2.182 [-100.000,  3.722], mean action: 1.920 [0.000, 3.000],  loss: 1219.648241, mse: 2318895.441298, mean_q: 1688.100406, mean_eps: 0.199798
 81295/100000: episode: 482, duration: 2.232s, episode steps: 285, steps per second: 128, episode reward: -822.018, mean reward: -2.884 [-100.000,  3.515], mean action: 1.674 [0.000, 3.000],  loss: 1479.881604, mse: 2306555.765789, mean_q: 1680.733711, mean_eps: 0.196595
 81487/100000: episode: 483, duration: 1.454s, episode steps: 192, steps per second: 132, episode reward: -149.773, mean reward: -0.780 [-100.000,  4.802], mean action: 1.849 [0.000, 3.000],  loss: 1596.041960, mse: 2224478.576172, mean_q: 1651.874703, mean_eps: 0.194234
 81939/100000: episode: 484, duration: 4.033s, episode steps: 452, steps per second: 112, episode reward: -519.372, mean reward: -1.149 [-100.000, 18.547], mean action: 1.885 [0.000, 3.000],  loss: 1575.204140, mse: 2213189.843473, mean_q: 1645.417754, mean_eps: 0.191046
 82083/100000: episode: 485, duration: 1.284s, episode steps: 144, steps per second: 112, episode reward: -171.209, mean reward: -1.189 [-100.000,  3.513], mean action: 1.917 [0.000, 3.000],  loss: 1576.541645, mse: 2232769.364583, mean_q: 1653.281757, mean_eps: 0.188096
 82243/100000: episode: 486, duration: 1.403s, episode steps: 160, steps per second: 114, episode reward: -460.952, mean reward: -2.881 [-100.000,  3.206], mean action: 1.900 [0.000, 3.000],  loss: 1610.052133, mse: 2242464.514063, mean_q: 1655.673938, mean_eps: 0.186591
 82468/100000: episode: 487, duration: 1.864s, episode steps: 225, steps per second: 121, episode reward: -149.287, mean reward: -0.663 [-100.000, 11.117], mean action: 1.876 [0.000, 3.000],  loss: 1196.588306, mse: 2223939.572778, mean_q: 1649.696043, mean_eps: 0.184686
 82886/100000: episode: 488, duration: 3.379s, episode steps: 418, steps per second: 124, episode reward: -944.944, mean reward: -2.261 [-100.000,  5.071], mean action: 1.897 [0.000, 3.000],  loss: 1238.497928, mse: 2223245.903110, mean_q: 1650.514179, mean_eps: 0.181503
 83155/100000: episode: 489, duration: 2.290s, episode steps: 269, steps per second: 117, episode reward: -598.276, mean reward: -2.224 [-100.000,  3.780], mean action: 1.937 [0.000, 3.000],  loss: 1819.272771, mse: 2223236.680762, mean_q: 1651.208550, mean_eps: 0.178102
 83564/100000: episode: 490, duration: 3.749s, episode steps: 409, steps per second: 109, episode reward: -231.915, mean reward: -0.567 [-100.000, 18.013], mean action: 1.817 [0.000, 3.000],  loss: 1369.910118, mse: 2197746.871333, mean_q: 1639.822992, mean_eps: 0.174746
 84479/100000: episode: 491, duration: 7.294s, episode steps: 915, steps per second: 125, episode reward: -932.034, mean reward: -1.019 [-100.000,  6.213], mean action: 1.881 [0.000, 3.000],  loss: 1403.633833, mse: 2202733.831284, mean_q: 1646.321909, mean_eps: 0.168192
 84747/100000: episode: 492, duration: 1.983s, episode steps: 268, steps per second: 135, episode reward: -900.977, mean reward: -3.362 [-100.000,  1.940], mean action: 1.616 [0.000, 3.000],  loss: 1515.728344, mse: 2164851.979478, mean_q: 1629.749831, mean_eps: 0.162336
 85130/100000: episode: 493, duration: 2.789s, episode steps: 383, steps per second: 137, episode reward: -917.265, mean reward: -2.395 [-100.000,  2.415], mean action: 1.817 [0.000, 3.000],  loss: 1289.001927, mse: 2117763.296018, mean_q: 1608.994587, mean_eps: 0.159114
 85537/100000: episode: 494, duration: 3.051s, episode steps: 407, steps per second: 133, episode reward: -690.156, mean reward: -1.696 [-100.000, 10.754], mean action: 1.801 [0.000, 3.000],  loss: 1616.832648, mse: 2062640.708845, mean_q: 1584.231920, mean_eps: 0.155203
 85869/100000: episode: 495, duration: 2.592s, episode steps: 332, steps per second: 128, episode reward: -451.437, mean reward: -1.360 [-100.000,  4.423], mean action: 1.846 [0.000, 3.000],  loss: 1505.319502, mse: 1973451.890437, mean_q: 1552.697026, mean_eps: 0.151545
 86206/100000: episode: 496, duration: 2.453s, episode steps: 337, steps per second: 137, episode reward: -553.357, mean reward: -1.642 [-100.000,  6.059], mean action: 1.849 [0.000, 3.000],  loss: 1776.062630, mse: 1952070.123887, mean_q: 1545.806850, mean_eps: 0.148234
 86593/100000: episode: 497, duration: 2.927s, episode steps: 387, steps per second: 132, episode reward: -508.405, mean reward: -1.314 [-100.000,  6.390], mean action: 1.915 [0.000, 3.000],  loss: 1668.995672, mse: 1927456.701550, mean_q: 1535.198388, mean_eps: 0.144650
 86930/100000: episode: 498, duration: 2.523s, episode steps: 337, steps per second: 134, episode reward: -1027.805, mean reward: -3.050 [-100.000,  3.455], mean action: 1.938 [0.000, 3.000],  loss: 1255.209162, mse: 1872299.788576, mean_q: 1511.369577, mean_eps: 0.141066
 87296/100000: episode: 499, duration: 3.191s, episode steps: 366, steps per second: 115, episode reward: -371.282, mean reward: -1.014 [-100.000,  8.048], mean action: 1.839 [0.000, 3.000],  loss: 1191.184356, mse: 1844165.587432, mean_q: 1496.072286, mean_eps: 0.137586
 87575/100000: episode: 500, duration: 2.149s, episode steps: 279, steps per second: 130, episode reward: -1125.975, mean reward: -4.036 [-100.000,  2.032], mean action: 1.903 [0.000, 3.000],  loss: 1308.026167, mse: 1854031.417115, mean_q: 1504.190021, mean_eps: 0.134393
 87851/100000: episode: 501, duration: 2.538s, episode steps: 276, steps per second: 109, episode reward: -904.520, mean reward: -3.277 [-100.000,  2.054], mean action: 1.928 [0.000, 3.000],  loss: 1260.573903, mse: 1858175.716938, mean_q: 1502.954458, mean_eps: 0.131646
 88196/100000: episode: 502, duration: 2.889s, episode steps: 345, steps per second: 119, episode reward: -299.560, mean reward: -0.868 [-100.000,  9.153], mean action: 1.780 [0.000, 3.000],  loss: 1753.261133, mse: 1862602.583696, mean_q: 1506.865887, mean_eps: 0.128572
 88528/100000: episode: 503, duration: 2.852s, episode steps: 332, steps per second: 116, episode reward: -1470.474, mean reward: -4.429 [-100.000,  3.734], mean action: 1.858 [0.000, 3.000],  loss: 1472.427463, mse: 1848889.591114, mean_q: 1496.076268, mean_eps: 0.125221
 88836/100000: episode: 504, duration: 2.684s, episode steps: 308, steps per second: 115, episode reward: -643.792, mean reward: -2.090 [-100.000,  4.452], mean action: 1.740 [0.000, 3.000],  loss: 1225.299857, mse: 1897685.582792, mean_q: 1519.311441, mean_eps: 0.122053
 89098/100000: episode: 505, duration: 1.957s, episode steps: 262, steps per second: 134, episode reward: -949.343, mean reward: -3.623 [-100.000,  6.604], mean action: 1.660 [0.000, 3.000],  loss: 1326.326050, mse: 1882187.888836, mean_q: 1514.523024, mean_eps: 0.119232
 89440/100000: episode: 506, duration: 2.592s, episode steps: 342, steps per second: 132, episode reward: -588.656, mean reward: -1.721 [-100.000,  9.246], mean action: 1.635 [0.000, 3.000],  loss: 1983.911084, mse: 1879403.611111, mean_q: 1515.012021, mean_eps: 0.116242
 89772/100000: episode: 507, duration: 2.766s, episode steps: 332, steps per second: 120, episode reward: -352.625, mean reward: -1.062 [-100.000,  8.971], mean action: 1.428 [0.000, 3.000],  loss: 1187.567510, mse: 1820377.032756, mean_q: 1490.165365, mean_eps: 0.112906
 90772/100000: episode: 508, duration: 9.111s, episode steps: 1000, steps per second: 110, episode reward: -648.681, mean reward: -0.649 [-11.361, 44.094], mean action: 1.798 [0.000, 3.000],  loss: 1159.318087, mse: 1775392.872625, mean_q: 1470.196034, mean_eps: 0.106312
 91689/100000: episode: 509, duration: 7.737s, episode steps: 917, steps per second: 119, episode reward: -677.047, mean reward: -0.738 [-100.000, 10.231], mean action: 1.722 [0.000, 3.000],  loss: 1226.836540, mse: 1802217.925436, mean_q: 1486.630644, mean_eps: 0.096823
 92689/100000: episode: 510, duration: 8.561s, episode steps: 1000, steps per second: 117, episode reward: -381.788, mean reward: -0.382 [-8.261,  6.820], mean action: 1.740 [0.000, 3.000],  loss: 1247.253950, mse: 1839333.948125, mean_q: 1505.500406, mean_eps: 0.087334
 93443/100000: episode: 511, duration: 6.402s, episode steps: 754, steps per second: 118, episode reward: -588.169, mean reward: -0.780 [-100.000,  6.349], mean action: 1.771 [0.000, 3.000],  loss: 1471.408279, mse: 1886610.112235, mean_q: 1526.145641, mean_eps: 0.078652
 94443/100000: episode: 512, duration: 8.377s, episode steps: 1000, steps per second: 119, episode reward: -370.861, mean reward: -0.371 [-8.438,  6.305], mean action: 1.791 [0.000, 3.000],  loss: 1484.663005, mse: 1865336.969125, mean_q: 1518.434214, mean_eps: 0.069969
 95342/100000: episode: 513, duration: 7.584s, episode steps: 899, steps per second: 119, episode reward: -589.271, mean reward: -0.655 [-100.000,  4.812], mean action: 1.808 [0.000, 3.000],  loss: 1364.380321, mse: 1826372.356507, mean_q: 1505.110255, mean_eps: 0.060569
 95563/100000: episode: 514, duration: 1.810s, episode steps: 221, steps per second: 122, episode reward: -312.932, mean reward: -1.416 [-100.000,  3.191], mean action: 1.814 [0.000, 3.000],  loss: 1076.404123, mse: 1796147.001131, mean_q: 1494.570778, mean_eps: 0.055025
 95982/100000: episode: 515, duration: 3.547s, episode steps: 419, steps per second: 118, episode reward: -360.658, mean reward: -0.861 [-100.000,  4.929], mean action: 1.764 [0.000, 3.000],  loss: 1216.473475, mse: 1785288.109487, mean_q: 1488.238886, mean_eps: 0.051857
 96982/100000: episode: 516, duration: 8.206s, episode steps: 1000, steps per second: 122, episode reward: -472.820, mean reward: -0.473 [-8.111,  5.901], mean action: 1.551 [0.000, 3.000],  loss: 1244.565758, mse: 1732771.500000, mean_q: 1468.962703, mean_eps: 0.044833
 97982/100000: episode: 517, duration: 8.440s, episode steps: 1000, steps per second: 118, episode reward: -303.824, mean reward: -0.304 [-6.735,  6.585], mean action: 1.689 [0.000, 3.000],  loss: 1301.384984, mse: 1760888.969625, mean_q: 1481.187762, mean_eps: 0.034933
 98982/100000: episode: 518, duration: 8.401s, episode steps: 1000, steps per second: 119, episode reward: -363.259, mean reward: -0.363 [-8.423,  8.935], mean action: 1.819 [0.000, 3.000],  loss: 1266.268286, mse: 1738030.529375, mean_q: 1471.178896, mean_eps: 0.025033
 99982/100000: episode: 519, duration: 8.364s, episode steps: 1000, steps per second: 120, episode reward: -201.516, mean reward: -0.202 [-8.105,  7.396], mean action: 1.739 [0.000, 3.000],  loss: 1111.553668, mse: 1715816.307625, mean_q: 1462.602562, mean_eps: 0.015133
done, took 801.693 seconds
Testing for 5 episodes ...
Episode 1: reward: -420.564, steps: 1000
Episode 2: reward: -334.602, steps: 1000
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 100000 steps ...
    79/100000: episode: 1, duration: 1.053s, episode steps:  79, steps per second:  75, episode reward: -236.625, mean reward: -2.995 [-100.000, 18.499], mean action: 1.709 [0.000, 3.000],  loss: 1055.418635, mse: 1648223.417279, mean_q: 1428.531485, mean_eps: 0.999599
   144/100000: episode: 2, duration: 0.493s, episode steps:  65, steps per second: 132, episode reward: -101.082, mean reward: -1.555 [-100.000, 51.503], mean action: 1.708 [0.000, 3.000],  loss: 1509.018714, mse: 1633102.471154, mean_q: 1423.323582, mean_eps: 0.999001
   209/100000: episode: 3, duration: 0.515s, episode steps:  65, steps per second: 126, episode reward: -75.660, mean reward: -1.164 [-100.000, 14.295], mean action: 1.462 [0.000, 3.000],  loss: 729.967003, mse: 1625718.282692, mean_q: 1417.267460, mean_eps: 0.998416
   281/100000: episode: 4, duration: 0.522s, episode steps:  72, steps per second: 138, episode reward: -110.927, mean reward: -1.541 [-100.000, 16.435], mean action: 1.417 [0.000, 3.000],  loss: 1043.936971, mse: 1642356.520833, mean_q: 1428.978589, mean_eps: 0.997800
   364/100000: episode: 5, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -189.996, mean reward: -2.289 [-100.000, 78.265], mean action: 1.506 [0.000, 3.000],  loss: 981.042330, mse: 1635097.009036, mean_q: 1424.305264, mean_eps: 0.997102
   471/100000: episode: 6, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -154.157, mean reward: -1.441 [-100.000,  6.802], mean action: 1.383 [0.000, 3.000],  loss: 767.002051, mse: 1627161.473131, mean_q: 1419.950961, mean_eps: 0.996247
   541/100000: episode: 7, duration: 0.496s, episode steps:  70, steps per second: 141, episode reward: -187.660, mean reward: -2.681 [-100.000,  7.664], mean action: 1.429 [0.000, 3.000],  loss: 648.116002, mse: 1611337.680357, mean_q: 1418.593312, mean_eps: 0.995451
   624/100000: episode: 8, duration: 0.637s, episode steps:  83, steps per second: 130, episode reward: -122.902, mean reward: -1.481 [-100.000,  6.564], mean action: 1.518 [0.000, 3.000],  loss: 681.490538, mse: 1677488.234940, mean_q: 1446.907377, mean_eps: 0.994762
   698/100000: episode: 9, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -317.438, mean reward: -4.290 [-100.000,  1.315], mean action: 1.378 [0.000, 3.000],  loss: 1385.465080, mse: 1647405.878378, mean_q: 1430.188155, mean_eps: 0.994055
   812/100000: episode: 10, duration: 0.875s, episode steps: 114, steps per second: 130, episode reward: -371.742, mean reward: -3.261 [-100.000,  1.543], mean action: 1.649 [0.000, 3.000],  loss: 766.706043, mse: 1631233.169956, mean_q: 1421.774434, mean_eps: 0.993210
   976/100000: episode: 11, duration: 1.254s, episode steps: 164, steps per second: 131, episode reward: -206.098, mean reward: -1.257 [-100.000, 117.325], mean action: 1.305 [0.000, 3.000],  loss: 591.620906, mse: 1607909.947409, mean_q: 1410.762823, mean_eps: 0.991959
  1080/100000: episode: 12, duration: 0.822s, episode steps: 104, steps per second: 127, episode reward: -247.529, mean reward: -2.380 [-100.000, 61.312], mean action: 1.615 [0.000, 3.000],  loss: 1243.053015, mse: 1596352.296875, mean_q: 1401.053696, mean_eps: 0.990753
  1210/100000: episode: 13, duration: 0.948s, episode steps: 130, steps per second: 137, episode reward: -104.400, mean reward: -0.803 [-100.000, 12.614], mean action: 1.562 [0.000, 3.000],  loss: 1674.156035, mse: 1613802.005769, mean_q: 1411.010558, mean_eps: 0.989699
  1316/100000: episode: 14, duration: 0.752s, episode steps: 106, steps per second: 141, episode reward: -93.266, mean reward: -0.880 [-100.000, 28.736], mean action: 1.585 [0.000, 3.000],  loss: 1128.819952, mse: 1593519.347877, mean_q: 1404.981396, mean_eps: 0.988637
  1395/100000: episode: 15, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward: -97.871, mean reward: -1.239 [-100.000, 12.217], mean action: 1.684 [0.000, 3.000],  loss: 1407.215615, mse: 1560543.297468, mean_q: 1387.755904, mean_eps: 0.987805
  1486/100000: episode: 16, duration: 0.666s, episode steps:  91, steps per second: 137, episode reward: -79.654, mean reward: -0.875 [-100.000, 12.824], mean action: 1.527 [0.000, 3.000],  loss: 1491.793679, mse: 1547396.559066, mean_q: 1380.865803, mean_eps: 0.987040
  1573/100000: episode: 17, duration: 0.605s, episode steps:  87, steps per second: 144, episode reward: -281.757, mean reward: -3.239 [-100.000, 94.175], mean action: 1.494 [0.000, 3.000],  loss: 951.495729, mse: 1542038.380747, mean_q: 1377.127057, mean_eps: 0.986239
  1659/100000: episode: 18, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -111.395, mean reward: -1.295 [-100.000,  8.406], mean action: 1.593 [0.000, 3.000],  loss: 541.315393, mse: 1542809.731105, mean_q: 1381.031109, mean_eps: 0.985460
  1758/100000: episode: 19, duration: 0.714s, episode steps:  99, steps per second: 139, episode reward: -91.487, mean reward: -0.924 [-100.000,  6.616], mean action: 1.576 [0.000, 3.000],  loss: 498.974475, mse: 1526444.472222, mean_q: 1372.543880, mean_eps: 0.984628
  1878/100000: episode: 20, duration: 0.837s, episode steps: 120, steps per second: 143, episode reward: -162.576, mean reward: -1.355 [-100.000,  5.595], mean action: 1.533 [0.000, 3.000],  loss: 1473.462588, mse: 1510253.483333, mean_q: 1363.542175, mean_eps: 0.983643
  1947/100000: episode: 21, duration: 0.478s, episode steps:  69, steps per second: 144, episode reward: -371.888, mean reward: -5.390 [-100.000,  8.725], mean action: 1.464 [0.000, 3.000],  loss: 1174.519162, mse: 1494698.358696, mean_q: 1354.422554, mean_eps: 0.982792
  2028/100000: episode: 22, duration: 0.574s, episode steps:  81, steps per second: 141, episode reward: -124.322, mean reward: -1.535 [-100.000, 21.383], mean action: 1.506 [0.000, 3.000],  loss: 923.232258, mse: 1500794.936728, mean_q: 1357.573501, mean_eps: 0.982117
  2091/100000: episode: 23, duration: 0.470s, episode steps:  63, steps per second: 134, episode reward: -300.143, mean reward: -4.764 [-100.000,  1.356], mean action: 1.825 [0.000, 3.000],  loss: 751.296994, mse: 1505605.706349, mean_q: 1361.986603, mean_eps: 0.981469
  2200/100000: episode: 24, duration: 0.758s, episode steps: 109, steps per second: 144, episode reward: -150.094, mean reward: -1.377 [-100.000, 14.541], mean action: 1.523 [0.000, 3.000],  loss: 857.157385, mse: 1515484.574541, mean_q: 1366.244948, mean_eps: 0.980695
  2292/100000: episode: 25, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -292.368, mean reward: -3.178 [-100.000,  3.204], mean action: 1.402 [0.000, 3.000],  loss: 1047.413662, mse: 1501579.631793, mean_q: 1359.026617, mean_eps: 0.979791
  2370/100000: episode: 26, duration: 0.577s, episode steps:  78, steps per second: 135, episode reward: -79.679, mean reward: -1.022 [-100.000, 17.646], mean action: 1.423 [0.000, 3.000],  loss: 904.007008, mse: 1510813.118590, mean_q: 1362.530669, mean_eps: 0.979025
  2433/100000: episode: 27, duration: 0.437s, episode steps:  63, steps per second: 144, episode reward: -80.072, mean reward: -1.271 [-100.000,  7.210], mean action: 1.524 [0.000, 3.000],  loss: 1391.241203, mse: 1516281.605159, mean_q: 1364.984619, mean_eps: 0.978391
  2497/100000: episode: 28, duration: 0.445s, episode steps:  64, steps per second: 144, episode reward: -70.435, mean reward: -1.101 [-100.000, 10.522], mean action: 1.672 [0.000, 3.000],  loss: 1423.222176, mse: 1516280.962891, mean_q: 1366.390154, mean_eps: 0.977819
  2587/100000: episode: 29, duration: 0.652s, episode steps:  90, steps per second: 138, episode reward: -124.631, mean reward: -1.385 [-100.000,  6.142], mean action: 1.500 [0.000, 3.000],  loss: 557.317492, mse: 1471755.312500, mean_q: 1344.244321, mean_eps: 0.977127
  2706/100000: episode: 30, duration: 0.900s, episode steps: 119, steps per second: 132, episode reward: -96.969, mean reward: -0.815 [-100.000, 10.591], mean action: 1.521 [0.000, 3.000],  loss: 997.368169, mse: 1489214.881303, mean_q: 1350.065031, mean_eps: 0.976186
  2792/100000: episode: 31, duration: 0.625s, episode steps:  86, steps per second: 138, episode reward: -123.233, mean reward: -1.433 [-100.000, 52.899], mean action: 1.477 [0.000, 3.000],  loss: 607.468780, mse: 1447914.731105, mean_q: 1331.806754, mean_eps: 0.975263
  2883/100000: episode: 32, duration: 0.659s, episode steps:  91, steps per second: 138, episode reward: -247.052, mean reward: -2.715 [-100.000, 21.765], mean action: 1.626 [0.000, 3.000],  loss: 497.587654, mse: 1458757.263736, mean_q: 1332.741525, mean_eps: 0.974467
  2951/100000: episode: 33, duration: 0.528s, episode steps:  68, steps per second: 129, episode reward: -121.811, mean reward: -1.791 [-100.000,  7.511], mean action: 1.721 [0.000, 3.000],  loss: 634.235398, mse: 1455530.865809, mean_q: 1332.341152, mean_eps: 0.973751
  3070/100000: episode: 34, duration: 0.855s, episode steps: 119, steps per second: 139, episode reward: -132.914, mean reward: -1.117 [-100.000,  6.980], mean action: 1.714 [0.000, 3.000],  loss: 838.055359, mse: 1448679.975840, mean_q: 1325.770731, mean_eps: 0.972910
  3168/100000: episode: 35, duration: 0.684s, episode steps:  98, steps per second: 143, episode reward: -88.074, mean reward: -0.899 [-100.000, 12.975], mean action: 1.531 [0.000, 3.000],  loss: 726.131629, mse: 1456364.710459, mean_q: 1337.373865, mean_eps: 0.971934
  3231/100000: episode: 36, duration: 0.460s, episode steps:  63, steps per second: 137, episode reward: -106.265, mean reward: -1.687 [-100.000,  6.468], mean action: 1.794 [0.000, 3.000],  loss: 556.291669, mse: 1448966.751984, mean_q: 1325.597854, mean_eps: 0.971209
  3334/100000: episode: 37, duration: 0.718s, episode steps: 103, steps per second: 143, episode reward: -325.329, mean reward: -3.159 [-100.000,  7.606], mean action: 1.388 [0.000, 3.000],  loss: 869.696598, mse: 1449594.169903, mean_q: 1328.137072, mean_eps: 0.970462
  3467/100000: episode: 38, duration: 0.915s, episode steps: 133, steps per second: 145, episode reward: -424.419, mean reward: -3.191 [-100.000,  2.230], mean action: 1.594 [0.000, 3.000],  loss: 898.018346, mse: 1454774.990602, mean_q: 1334.163178, mean_eps: 0.969400
  3586/100000: episode: 39, duration: 0.846s, episode steps: 119, steps per second: 141, episode reward: -114.329, mean reward: -0.961 [-100.000,  6.702], mean action: 1.521 [0.000, 3.000],  loss: 909.034724, mse: 1449066.192227, mean_q: 1329.569852, mean_eps: 0.968266
  3680/100000: episode: 40, duration: 0.660s, episode steps:  94, steps per second: 142, episode reward: -173.137, mean reward: -1.842 [-100.000,  7.455], mean action: 1.553 [0.000, 3.000],  loss: 995.545996, mse: 1411013.301862, mean_q: 1309.790852, mean_eps: 0.967308
  3784/100000: episode: 41, duration: 0.726s, episode steps: 104, steps per second: 143, episode reward: -102.422, mean reward: -0.985 [-100.000,  9.736], mean action: 1.712 [0.000, 3.000],  loss: 1071.760720, mse: 1396256.490385, mean_q: 1300.133685, mean_eps: 0.966417
  3889/100000: episode: 42, duration: 0.738s, episode steps: 105, steps per second: 142, episode reward: -322.395, mean reward: -3.070 [-100.000,  6.864], mean action: 1.486 [0.000, 3.000],  loss: 1362.924417, mse: 1405513.478571, mean_q: 1305.962129, mean_eps: 0.965476
  3961/100000: episode: 43, duration: 0.502s, episode steps:  72, steps per second: 144, episode reward: -122.949, mean reward: -1.708 [-100.000,  6.860], mean action: 1.458 [0.000, 3.000],  loss: 963.721088, mse: 1432828.583333, mean_q: 1320.584508, mean_eps: 0.964679
  4066/100000: episode: 44, duration: 0.739s, episode steps: 105, steps per second: 142, episode reward: -210.811, mean reward: -2.008 [-100.000, 82.398], mean action: 1.514 [0.000, 3.000],  loss: 511.713408, mse: 1442490.721429, mean_q: 1325.959519, mean_eps: 0.963883
  4200/100000: episode: 45, duration: 0.954s, episode steps: 134, steps per second: 141, episode reward: -86.767, mean reward: -0.648 [-100.000, 12.226], mean action: 1.396 [0.000, 3.000],  loss: 666.488813, mse: 1453711.253731, mean_q: 1327.644738, mean_eps: 0.962807
  4300/100000: episode: 46, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -130.755, mean reward: -1.308 [-100.000,  5.979], mean action: 1.450 [0.000, 3.000],  loss: 810.232973, mse: 1462968.186250, mean_q: 1337.684323, mean_eps: 0.961755
  4416/100000: episode: 47, duration: 0.812s, episode steps: 116, steps per second: 143, episode reward: -77.073, mean reward: -0.664 [-100.000, 19.397], mean action: 1.491 [0.000, 3.000],  loss: 1031.321222, mse: 1445192.909483, mean_q: 1326.921839, mean_eps: 0.960783
  4522/100000: episode: 48, duration: 0.752s, episode steps: 106, steps per second: 141, episode reward: -169.375, mean reward: -1.598 [-100.000,  2.478], mean action: 1.415 [0.000, 3.000],  loss: 1332.850565, mse: 1435289.255896, mean_q: 1320.553110, mean_eps: 0.959784
  4597/100000: episode: 49, duration: 0.521s, episode steps:  75, steps per second: 144, episode reward: -133.942, mean reward: -1.786 [-100.000,  6.269], mean action: 1.613 [0.000, 3.000],  loss: 1109.599390, mse: 1447093.943333, mean_q: 1325.121707, mean_eps: 0.958969
  4709/100000: episode: 50, duration: 0.809s, episode steps: 112, steps per second: 139, episode reward: -486.460, mean reward: -4.343 [-100.000,  1.535], mean action: 1.509 [0.000, 3.000],  loss: 377.934212, mse: 1439570.248884, mean_q: 1323.420046, mean_eps: 0.958128
  4776/100000: episode: 51, duration: 0.537s, episode steps:  67, steps per second: 125, episode reward: -103.008, mean reward: -1.537 [-100.000,  7.013], mean action: 1.493 [0.000, 3.000],  loss: 662.160969, mse: 1446257.022388, mean_q: 1326.928370, mean_eps: 0.957322
  4838/100000: episode: 52, duration: 0.510s, episode steps:  62, steps per second: 121, episode reward: -109.096, mean reward: -1.760 [-100.000, 10.124], mean action: 1.710 [0.000, 3.000],  loss: 1006.835097, mse: 1420874.868952, mean_q: 1314.187065, mean_eps: 0.956742
  4906/100000: episode: 53, duration: 0.539s, episode steps:  68, steps per second: 126, episode reward: -55.294, mean reward: -0.813 [-100.000, 24.143], mean action: 1.382 [0.000, 3.000],  loss: 469.453091, mse: 1436987.016544, mean_q: 1322.654692, mean_eps: 0.956156
  5005/100000: episode: 54, duration: 0.772s, episode steps:  99, steps per second: 128, episode reward: -95.168, mean reward: -0.961 [-100.000, 13.710], mean action: 1.404 [0.000, 3.000],  loss: 1051.146824, mse: 1429628.324495, mean_q: 1317.073123, mean_eps: 0.955405
  5100/100000: episode: 55, duration: 0.759s, episode steps:  95, steps per second: 125, episode reward: -109.201, mean reward: -1.149 [-100.000, 20.474], mean action: 1.558 [0.000, 3.000],  loss: 715.184879, mse: 1440290.940789, mean_q: 1318.634626, mean_eps: 0.954532
  5210/100000: episode: 56, duration: 0.768s, episode steps: 110, steps per second: 143, episode reward: -173.295, mean reward: -1.575 [-100.000, 10.903], mean action: 1.627 [0.000, 3.000],  loss: 1006.362885, mse: 1446323.204545, mean_q: 1324.783210, mean_eps: 0.953609
  5328/100000: episode: 57, duration: 0.904s, episode steps: 118, steps per second: 131, episode reward: -139.598, mean reward: -1.183 [-100.000,  3.595], mean action: 1.508 [0.000, 3.000],  loss: 869.880878, mse: 1426346.885593, mean_q: 1314.085262, mean_eps: 0.952583
  5397/100000: episode: 58, duration: 0.528s, episode steps:  69, steps per second: 131, episode reward: -109.683, mean reward: -1.590 [-100.000, 14.711], mean action: 1.594 [0.000, 3.000],  loss: 668.590578, mse: 1445751.157609, mean_q: 1328.451538, mean_eps: 0.951742
  5506/100000: episode: 59, duration: 0.853s, episode steps: 109, steps per second: 128, episode reward: -272.662, mean reward: -2.501 [-100.000,  0.973], mean action: 1.661 [0.000, 3.000],  loss: 877.205326, mse: 1452000.701835, mean_q: 1328.386683, mean_eps: 0.950941
  5613/100000: episode: 60, duration: 0.823s, episode steps: 107, steps per second: 130, episode reward: -225.186, mean reward: -2.105 [-100.000,  0.811], mean action: 1.411 [0.000, 3.000],  loss: 864.558078, mse: 1459214.266355, mean_q: 1331.076838, mean_eps: 0.949969
  5677/100000: episode: 61, duration: 0.519s, episode steps:  64, steps per second: 123, episode reward: -80.889, mean reward: -1.264 [-100.000, 13.015], mean action: 1.781 [0.000, 3.000],  loss: 525.163573, mse: 1478443.988281, mean_q: 1342.659630, mean_eps: 0.949199
  5770/100000: episode: 62, duration: 1.158s, episode steps:  93, steps per second:  80, episode reward: -128.069, mean reward: -1.377 [-100.000, 19.217], mean action: 1.333 [0.000, 3.000],  loss: 574.064312, mse: 1447991.076613, mean_q: 1326.395753, mean_eps: 0.948493
  5882/100000: episode: 63, duration: 1.392s, episode steps: 112, steps per second:  80, episode reward: -40.202, mean reward: -0.359 [-100.000, 63.525], mean action: 1.625 [0.000, 3.000],  loss: 892.861703, mse: 1456939.983259, mean_q: 1328.401061, mean_eps: 0.947570
  5989/100000: episode: 64, duration: 0.924s, episode steps: 107, steps per second: 116, episode reward: -58.999, mean reward: -0.551 [-100.000, 11.724], mean action: 1.523 [0.000, 3.000],  loss: 797.893965, mse: 1462313.610981, mean_q: 1331.478494, mean_eps: 0.946585
  6067/100000: episode: 65, duration: 0.609s, episode steps:  78, steps per second: 128, episode reward: -147.915, mean reward: -1.896 [-100.000,  6.900], mean action: 1.513 [0.000, 3.000],  loss: 800.508487, mse: 1450030.857372, mean_q: 1321.344185, mean_eps: 0.945753
  6179/100000: episode: 66, duration: 0.957s, episode steps: 112, steps per second: 117, episode reward: -86.539, mean reward: -0.773 [-100.000, 19.635], mean action: 1.580 [0.000, 3.000],  loss: 746.670218, mse: 1429654.186384, mean_q: 1313.177259, mean_eps: 0.944897
  6245/100000: episode: 67, duration: 0.589s, episode steps:  66, steps per second: 112, episode reward: -85.443, mean reward: -1.295 [-100.000, 11.961], mean action: 1.621 [0.000, 3.000],  loss: 1285.989436, mse: 1459369.320076, mean_q: 1328.960209, mean_eps: 0.944097
  6348/100000: episode: 68, duration: 0.826s, episode steps: 103, steps per second: 125, episode reward: -109.569, mean reward: -1.064 [-100.000, 22.952], mean action: 1.485 [0.000, 3.000],  loss: 1175.748111, mse: 1465954.054612, mean_q: 1327.572468, mean_eps: 0.943336
  6475/100000: episode: 69, duration: 0.923s, episode steps: 127, steps per second: 138, episode reward: -32.015, mean reward: -0.252 [-100.000, 55.040], mean action: 1.472 [0.000, 3.000],  loss: 934.828880, mse: 1497585.459646, mean_q: 1347.789686, mean_eps: 0.942301
  6570/100000: episode: 70, duration: 0.830s, episode steps:  95, steps per second: 114, episode reward: -230.811, mean reward: -2.430 [-100.000,  5.591], mean action: 1.558 [0.000, 3.000],  loss: 524.999498, mse: 1481316.886842, mean_q: 1334.950221, mean_eps: 0.941302
  6640/100000: episode: 71, duration: 0.634s, episode steps:  70, steps per second: 110, episode reward: -71.809, mean reward: -1.026 [-100.000,  6.560], mean action: 1.643 [0.000, 3.000],  loss: 1423.580494, mse: 1479204.641071, mean_q: 1333.968283, mean_eps: 0.940559
  6736/100000: episode: 72, duration: 0.711s, episode steps:  96, steps per second: 135, episode reward: -259.586, mean reward: -2.704 [-100.000,  7.510], mean action: 1.396 [0.000, 3.000],  loss: 1106.756615, mse: 1468070.845052, mean_q: 1329.914327, mean_eps: 0.939813
  6800/100000: episode: 73, duration: 0.545s, episode steps:  64, steps per second: 117, episode reward: -92.175, mean reward: -1.440 [-100.000, 11.409], mean action: 1.266 [0.000, 3.000],  loss: 1472.649579, mse: 1462352.697266, mean_q: 1325.448368, mean_eps: 0.939092
  6896/100000: episode: 74, duration: 0.889s, episode steps:  96, steps per second: 108, episode reward: -104.849, mean reward: -1.092 [-100.000, 13.155], mean action: 1.552 [0.000, 3.000],  loss: 526.024934, mse: 1495313.408854, mean_q: 1340.612358, mean_eps: 0.938372
  6994/100000: episode: 75, duration: 0.733s, episode steps:  98, steps per second: 134, episode reward: -290.874, mean reward: -2.968 [-100.000, 36.955], mean action: 1.622 [0.000, 3.000],  loss: 652.203862, mse: 1477627.195153, mean_q: 1332.601459, mean_eps: 0.937499
  7083/100000: episode: 76, duration: 0.806s, episode steps:  89, steps per second: 110, episode reward: -173.147, mean reward: -1.945 [-100.000,  9.282], mean action: 1.416 [0.000, 3.000],  loss: 618.435473, mse: 1501550.272472, mean_q: 1347.705698, mean_eps: 0.936658
  7149/100000: episode: 77, duration: 0.533s, episode steps:  66, steps per second: 124, episode reward: -95.997, mean reward: -1.455 [-100.000, 16.180], mean action: 1.273 [0.000, 3.000],  loss: 1054.027639, mse: 1485614.371212, mean_q: 1333.995798, mean_eps: 0.935960
  7262/100000: episode: 78, duration: 0.817s, episode steps: 113, steps per second: 138, episode reward: -197.973, mean reward: -1.752 [-100.000,  1.211], mean action: 1.549 [0.000, 3.000],  loss: 766.470478, mse: 1497086.886062, mean_q: 1340.990507, mean_eps: 0.935155
  7325/100000: episode: 79, duration: 0.444s, episode steps:  63, steps per second: 142, episode reward: -110.751, mean reward: -1.758 [-100.000, 11.041], mean action: 1.270 [0.000, 3.000],  loss: 1077.516589, mse: 1476238.261905, mean_q: 1322.346184, mean_eps: 0.934363
  7435/100000: episode: 80, duration: 0.813s, episode steps: 110, steps per second: 135, episode reward: -77.804, mean reward: -0.707 [-100.000, 13.477], mean action: 1.500 [0.000, 3.000],  loss: 844.971215, mse: 1496275.782955, mean_q: 1339.290282, mean_eps: 0.933584
  7509/100000: episode: 81, duration: 0.529s, episode steps:  74, steps per second: 140, episode reward: -149.173, mean reward: -2.016 [-100.000, 15.408], mean action: 1.392 [0.000, 3.000],  loss: 647.678396, mse: 1468176.606419, mean_q: 1321.819686, mean_eps: 0.932756
  7581/100000: episode: 82, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -151.988, mean reward: -2.111 [-100.000,  5.807], mean action: 1.583 [0.000, 3.000],  loss: 967.246756, mse: 1465421.418403, mean_q: 1318.201937, mean_eps: 0.932100
  7677/100000: episode: 83, duration: 0.708s, episode steps:  96, steps per second: 136, episode reward: -87.129, mean reward: -0.908 [-100.000, 16.421], mean action: 1.604 [0.000, 3.000],  loss: 1119.406531, mse: 1478844.326823, mean_q: 1331.238617, mean_eps: 0.931343
  7759/100000: episode: 84, duration: 0.612s, episode steps:  82, steps per second: 134, episode reward: -102.655, mean reward: -1.252 [-100.000, 12.705], mean action: 1.476 [0.000, 3.000],  loss: 658.412557, mse: 1475634.760671, mean_q: 1322.878254, mean_eps: 0.930543
  7830/100000: episode: 85, duration: 0.507s, episode steps:  71, steps per second: 140, episode reward: -242.721, mean reward: -3.419 [-100.000,  5.974], mean action: 1.944 [0.000, 3.000],  loss: 682.538591, mse: 1505542.728873, mean_q: 1340.894361, mean_eps: 0.929854
  7908/100000: episode: 86, duration: 0.562s, episode steps:  78, steps per second: 139, episode reward: -95.386, mean reward: -1.223 [-100.000,  9.897], mean action: 1.667 [0.000, 3.000],  loss: 1669.354847, mse: 1478054.235577, mean_q: 1329.368687, mean_eps: 0.929183
  8043/100000: episode: 87, duration: 0.988s, episode steps: 135, steps per second: 137, episode reward: -122.389, mean reward: -0.907 [-100.000,  3.976], mean action: 1.556 [0.000, 3.000],  loss: 819.684894, mse: 1473267.377778, mean_q: 1322.786900, mean_eps: 0.928225
  8167/100000: episode: 88, duration: 0.870s, episode steps: 124, steps per second: 143, episode reward: -206.502, mean reward: -1.665 [-100.000,  9.851], mean action: 1.750 [0.000, 3.000],  loss: 713.997728, mse: 1463006.034274, mean_q: 1313.997584, mean_eps: 0.927060
  8286/100000: episode: 89, duration: 0.853s, episode steps: 119, steps per second: 140, episode reward: -148.325, mean reward: -1.246 [-100.000,  4.949], mean action: 1.387 [0.000, 3.000],  loss: 945.588295, mse: 1447756.142857, mean_q: 1307.434994, mean_eps: 0.925966
  8389/100000: episode: 90, duration: 0.745s, episode steps: 103, steps per second: 138, episode reward: -126.277, mean reward: -1.226 [-100.000, 16.557], mean action: 1.650 [0.000, 3.000],  loss: 1049.460458, mse: 1431147.921117, mean_q: 1301.736842, mean_eps: 0.924967
  8504/100000: episode: 91, duration: 0.821s, episode steps: 115, steps per second: 140, episode reward: -32.073, mean reward: -0.279 [-100.000, 35.524], mean action: 1.452 [0.000, 3.000],  loss: 757.291797, mse: 1452563.246739, mean_q: 1307.006047, mean_eps: 0.923986
  8577/100000: episode: 92, duration: 0.515s, episode steps:  73, steps per second: 142, episode reward: -103.403, mean reward: -1.416 [-100.000, 15.433], mean action: 1.630 [0.000, 3.000],  loss: 1039.993740, mse: 1466244.429795, mean_q: 1316.453625, mean_eps: 0.923140
  8674/100000: episode: 93, duration: 0.732s, episode steps:  97, steps per second: 132, episode reward: -238.772, mean reward: -2.462 [-100.000,  6.023], mean action: 1.680 [0.000, 3.000],  loss: 334.235440, mse: 1479479.810567, mean_q: 1323.003147, mean_eps: 0.922375
  8770/100000: episode: 94, duration: 0.676s, episode steps:  96, steps per second: 142, episode reward: -107.913, mean reward: -1.124 [-100.000, 20.449], mean action: 1.365 [0.000, 3.000],  loss: 538.494755, mse: 1501301.619792, mean_q: 1330.086562, mean_eps: 0.921507
  8867/100000: episode: 95, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -93.129, mean reward: -0.960 [-100.000, 71.332], mean action: 1.433 [0.000, 3.000],  loss: 1189.821444, mse: 1475928.798969, mean_q: 1318.620511, mean_eps: 0.920638
  8943/100000: episode: 96, duration: 0.598s, episode steps:  76, steps per second: 127, episode reward: -86.845, mean reward: -1.143 [-100.000, 13.742], mean action: 1.500 [0.000, 3.000],  loss: 553.651109, mse: 1495306.057566, mean_q: 1326.245328, mean_eps: 0.919860
  9043/100000: episode: 97, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: -61.939, mean reward: -0.619 [-100.000, 98.062], mean action: 1.570 [0.000, 3.000],  loss: 712.945493, mse: 1454758.892500, mean_q: 1306.434729, mean_eps: 0.919068
  9130/100000: episode: 98, duration: 0.621s, episode steps:  87, steps per second: 140, episode reward: -82.892, mean reward: -0.953 [-100.000, 18.552], mean action: 1.494 [0.000, 3.000],  loss: 712.120658, mse: 1469433.977011, mean_q: 1314.324035, mean_eps: 0.918226
  9230/100000: episode: 99, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: -123.637, mean reward: -1.236 [-100.000, 37.914], mean action: 1.550 [0.000, 3.000],  loss: 884.289860, mse: 1493913.172500, mean_q: 1327.575983, mean_eps: 0.917385
  9342/100000: episode: 100, duration: 0.818s, episode steps: 112, steps per second: 137, episode reward: -284.614, mean reward: -2.541 [-100.000, 13.744], mean action: 1.741 [0.000, 3.000],  loss: 1258.371550, mse: 1460513.642857, mean_q: 1307.961868, mean_eps: 0.916431
  9422/100000: episode: 101, duration: 0.557s, episode steps:  80, steps per second: 144, episode reward: -123.851, mean reward: -1.548 [-100.000,  4.608], mean action: 1.488 [0.000, 3.000],  loss: 522.483754, mse: 1474443.670313, mean_q: 1310.808194, mean_eps: 0.915566
  9505/100000: episode: 102, duration: 0.640s, episode steps:  83, steps per second: 130, episode reward: -93.822, mean reward: -1.130 [-100.000, 14.675], mean action: 1.566 [0.000, 3.000],  loss: 584.595767, mse: 1489953.557229, mean_q: 1320.125528, mean_eps: 0.914833
  9575/100000: episode: 103, duration: 0.506s, episode steps:  70, steps per second: 138, episode reward: -49.582, mean reward: -0.708 [-100.000, 22.620], mean action: 1.443 [0.000, 3.000],  loss: 467.994741, mse: 1465564.725000, mean_q: 1311.717072, mean_eps: 0.914144
  9681/100000: episode: 104, duration: 0.733s, episode steps: 106, steps per second: 145, episode reward: -69.294, mean reward: -0.654 [-100.000, 18.745], mean action: 1.538 [0.000, 3.000],  loss: 895.035765, mse: 1452376.689858, mean_q: 1301.353030, mean_eps: 0.913353
  9772/100000: episode: 105, duration: 0.635s, episode steps:  91, steps per second: 143, episode reward: -183.006, mean reward: -2.011 [-100.000,  7.388], mean action: 1.626 [0.000, 3.000],  loss: 608.606349, mse: 1460122.326923, mean_q: 1302.073352, mean_eps: 0.912466
  9879/100000: episode: 106, duration: 0.789s, episode steps: 107, steps per second: 136, episode reward: -203.721, mean reward: -1.904 [-100.000,  7.635], mean action: 1.421 [0.000, 3.000],  loss: 598.916357, mse: 1451940.496495, mean_q: 1295.012889, mean_eps: 0.911575
  9966/100000: episode: 107, duration: 0.614s, episode steps:  87, steps per second: 142, episode reward: -198.093, mean reward: -2.277 [-100.000, 72.854], mean action: 1.299 [0.000, 3.000],  loss: 528.054261, mse: 1410038.238506, mean_q: 1275.915091, mean_eps: 0.910702
 10039/100000: episode: 108, duration: 0.511s, episode steps:  73, steps per second: 143, episode reward: -55.687, mean reward: -0.763 [-100.000, 21.761], mean action: 1.616 [0.000, 3.000],  loss: 679.733668, mse: 1430090.083904, mean_q: 1284.035436, mean_eps: 0.909982
 10135/100000: episode: 109, duration: 0.710s, episode steps:  96, steps per second: 135, episode reward: -427.900, mean reward: -4.457 [-100.000, 103.993], mean action: 1.552 [0.000, 3.000],  loss: 610.774952, mse: 1448026.786458, mean_q: 1294.864263, mean_eps: 0.909222
 10215/100000: episode: 110, duration: 0.567s, episode steps:  80, steps per second: 141, episode reward: -110.735, mean reward: -1.384 [-100.000, 16.788], mean action: 1.700 [0.000, 3.000],  loss: 351.411769, mse: 1447489.060938, mean_q: 1289.305876, mean_eps: 0.908430
 10334/100000: episode: 111, duration: 0.832s, episode steps: 119, steps per second: 143, episode reward: -148.214, mean reward: -1.245 [-100.000, 16.284], mean action: 1.445 [0.000, 3.000],  loss: 551.324230, mse: 1435294.447479, mean_q: 1284.929965, mean_eps: 0.907534
 10416/100000: episode: 112, duration: 0.604s, episode steps:  82, steps per second: 136, episode reward: -173.270, mean reward: -2.113 [-100.000,  5.296], mean action: 1.610 [0.000, 3.000],  loss: 518.557864, mse: 1451553.076220, mean_q: 1291.991355, mean_eps: 0.906630
 10493/100000: episode: 113, duration: 0.540s, episode steps:  77, steps per second: 143, episode reward: -80.183, mean reward: -1.041 [-100.000, 11.652], mean action: 1.532 [0.000, 3.000],  loss: 445.399698, mse: 1459792.813312, mean_q: 1301.627215, mean_eps: 0.905914
 10588/100000: episode: 114, duration: 0.659s, episode steps:  95, steps per second: 144, episode reward: -31.350, mean reward: -0.330 [-100.000, 108.664], mean action: 1.463 [0.000, 3.000],  loss: 814.654991, mse: 1430638.476316, mean_q: 1285.223378, mean_eps: 0.905140
 10686/100000: episode: 115, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -137.600, mean reward: -1.404 [-100.000,  7.115], mean action: 1.500 [0.000, 3.000],  loss: 762.363644, mse: 1440282.303571, mean_q: 1284.340420, mean_eps: 0.904272
 10748/100000: episode: 116, duration: 0.466s, episode steps:  62, steps per second: 133, episode reward: -81.684, mean reward: -1.317 [-100.000, 21.538], mean action: 1.371 [0.000, 3.000],  loss: 773.232453, mse: 1446133.171371, mean_q: 1289.415291, mean_eps: 0.903552
 10836/100000: episode: 117, duration: 0.629s, episode steps:  88, steps per second: 140, episode reward: -92.563, mean reward: -1.052 [-100.000, 17.811], mean action: 1.670 [0.000, 3.000],  loss: 688.018272, mse: 1423600.423295, mean_q: 1278.267717, mean_eps: 0.902877
 10935/100000: episode: 118, duration: 0.680s, episode steps:  99, steps per second: 146, episode reward: -267.842, mean reward: -2.705 [-100.000,  0.674], mean action: 1.586 [0.000, 3.000],  loss: 859.747165, mse: 1406652.880051, mean_q: 1270.584854, mean_eps: 0.902035
 11045/100000: episode: 119, duration: 0.802s, episode steps: 110, steps per second: 137, episode reward: -121.005, mean reward: -1.100 [-100.000, 10.472], mean action: 1.627 [0.000, 3.000],  loss: 803.925210, mse: 1440152.072727, mean_q: 1287.065521, mean_eps: 0.901095
 11121/100000: episode: 120, duration: 0.543s, episode steps:  76, steps per second: 140, episode reward: -84.090, mean reward: -1.106 [-100.000, 16.514], mean action: 1.184 [0.000, 3.000],  loss: 1055.312993, mse: 1450453.705592, mean_q: 1296.015741, mean_eps: 0.900257
 11244/100000: episode: 121, duration: 0.852s, episode steps: 123, steps per second: 144, episode reward: -97.414, mean reward: -0.792 [-100.000,  6.336], mean action: 1.407 [0.000, 3.000],  loss: 818.502171, mse: 1435940.653455, mean_q: 1284.059677, mean_eps: 0.899362
 11318/100000: episode: 122, duration: 0.533s, episode steps:  74, steps per second: 139, episode reward: -66.898, mean reward: -0.904 [-100.000,  7.093], mean action: 1.541 [0.000, 3.000],  loss: 483.578811, mse: 1452794.993243, mean_q: 1288.903680, mean_eps: 0.898475
 11398/100000: episode: 123, duration: 0.601s, episode steps:  80, steps per second: 133, episode reward: -96.147, mean reward: -1.202 [-100.000,  7.292], mean action: 1.350 [0.000, 3.000],  loss: 980.347191, mse: 1417157.639063, mean_q: 1272.366832, mean_eps: 0.897782
 11480/100000: episode: 124, duration: 0.630s, episode steps:  82, steps per second: 130, episode reward: -93.662, mean reward: -1.142 [-100.000,  6.742], mean action: 1.524 [0.000, 3.000],  loss: 1099.323760, mse: 1407928.873476, mean_q: 1268.324173, mean_eps: 0.897054
 11541/100000: episode: 125, duration: 0.478s, episode steps:  61, steps per second: 128, episode reward: -85.721, mean reward: -1.405 [-100.000,  9.180], mean action: 1.574 [0.000, 3.000],  loss: 835.513106, mse: 1407632.196721, mean_q: 1264.720449, mean_eps: 0.896410
 11615/100000: episode: 126, duration: 0.631s, episode steps:  74, steps per second: 117, episode reward: -97.452, mean reward: -1.317 [-100.000,  7.631], mean action: 1.676 [0.000, 3.000],  loss: 397.687456, mse: 1402919.726351, mean_q: 1261.795917, mean_eps: 0.895802
 11702/100000: episode: 127, duration: 0.661s, episode steps:  87, steps per second: 132, episode reward: -71.875, mean reward: -0.826 [-100.000, 10.133], mean action: 1.598 [0.000, 3.000],  loss: 564.192723, mse: 1416629.606322, mean_q: 1276.113095, mean_eps: 0.895078
 11770/100000: episode: 128, duration: 0.513s, episode steps:  68, steps per second: 133, episode reward: -81.540, mean reward: -1.199 [-100.000,  9.341], mean action: 1.662 [0.000, 3.000],  loss: 464.434157, mse: 1402381.968750, mean_q: 1259.271973, mean_eps: 0.894381
 11900/100000: episode: 129, duration: 1.011s, episode steps: 130, steps per second: 129, episode reward: -138.231, mean reward: -1.063 [-100.000, 23.346], mean action: 1.615 [0.000, 3.000],  loss: 1086.946725, mse: 1425613.001923, mean_q: 1277.930941, mean_eps: 0.893490
 12031/100000: episode: 130, duration: 0.984s, episode steps: 131, steps per second: 133, episode reward: -101.737, mean reward: -0.777 [-100.000, 25.228], mean action: 1.481 [0.000, 3.000],  loss: 597.857872, mse: 1479366.565840, mean_q: 1302.196174, mean_eps: 0.892315
 12152/100000: episode: 131, duration: 0.859s, episode steps: 121, steps per second: 141, episode reward: -168.225, mean reward: -1.390 [-100.000,  7.945], mean action: 1.620 [0.000, 3.000],  loss: 528.911384, mse: 1459917.347107, mean_q: 1289.055025, mean_eps: 0.891181
 12265/100000: episode: 132, duration: 0.813s, episode steps: 113, steps per second: 139, episode reward: -141.925, mean reward: -1.256 [-100.000,  6.117], mean action: 1.549 [0.000, 3.000],  loss: 1451.574556, mse: 1481490.164823, mean_q: 1299.934074, mean_eps: 0.890128
 12372/100000: episode: 133, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: -66.706, mean reward: -0.623 [-100.000, 11.430], mean action: 1.579 [0.000, 3.000],  loss: 826.306722, mse: 1487270.102804, mean_q: 1302.626760, mean_eps: 0.889138
 12442/100000: episode: 134, duration: 0.493s, episode steps:  70, steps per second: 142, episode reward: -107.575, mean reward: -1.537 [-100.000,  6.415], mean action: 1.600 [0.000, 3.000],  loss: 592.338183, mse: 1510316.635714, mean_q: 1314.322208, mean_eps: 0.888341
 12525/100000: episode: 135, duration: 0.618s, episode steps:  83, steps per second: 134, episode reward: -16.052, mean reward: -0.193 [-100.000, 75.284], mean action: 1.434 [0.000, 3.000],  loss: 469.867868, mse: 1509251.188253, mean_q: 1313.028989, mean_eps: 0.887653
 12609/100000: episode: 136, duration: 0.592s, episode steps:  84, steps per second: 142, episode reward: -95.566, mean reward: -1.138 [-100.000,  7.167], mean action: 1.690 [0.000, 3.000],  loss: 783.486808, mse: 1495916.611607, mean_q: 1304.874440, mean_eps: 0.886902
 12712/100000: episode: 137, duration: 0.712s, episode steps: 103, steps per second: 145, episode reward: -108.982, mean reward: -1.058 [-100.000, 12.782], mean action: 1.553 [0.000, 3.000],  loss: 767.811375, mse: 1520454.713592, mean_q: 1316.318012, mean_eps: 0.886060
 12776/100000: episode: 138, duration: 0.473s, episode steps:  64, steps per second: 135, episode reward: -59.974, mean reward: -0.937 [-100.000,  6.430], mean action: 1.609 [0.000, 3.000],  loss: 936.285604, mse: 1526568.732422, mean_q: 1319.837990, mean_eps: 0.885308
 12889/100000: episode: 139, duration: 0.825s, episode steps: 113, steps per second: 137, episode reward: -117.399, mean reward: -1.039 [-100.000, 11.532], mean action: 1.611 [0.000, 3.000],  loss: 1604.770182, mse: 1515334.355088, mean_q: 1310.789513, mean_eps: 0.884512
 12976/100000: episode: 140, duration: 0.608s, episode steps:  87, steps per second: 143, episode reward: -183.990, mean reward: -2.115 [-100.000,  8.776], mean action: 1.529 [0.000, 3.000],  loss: 737.778089, mse: 1534887.803161, mean_q: 1318.234990, mean_eps: 0.883612
 13089/100000: episode: 141, duration: 0.844s, episode steps: 113, steps per second: 134, episode reward: -122.751, mean reward: -1.086 [-100.000, 10.397], mean action: 1.425 [0.000, 3.000],  loss: 992.888910, mse: 1567899.483407, mean_q: 1338.220971, mean_eps: 0.882712
 13164/100000: episode: 142, duration: 0.549s, episode steps:  75, steps per second: 137, episode reward: -141.342, mean reward: -1.885 [-100.000,  8.122], mean action: 1.507 [0.000, 3.000],  loss: 820.539735, mse: 1594641.718333, mean_q: 1349.488245, mean_eps: 0.881866
 13235/100000: episode: 143, duration: 0.492s, episode steps:  71, steps per second: 144, episode reward: -90.026, mean reward: -1.268 [-100.000,  8.740], mean action: 1.704 [0.000, 3.000],  loss: 725.173293, mse: 1569396.158451, mean_q: 1338.043440, mean_eps: 0.881209
 13336/100000: episode: 144, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: -108.156, mean reward: -1.071 [-100.000, 25.454], mean action: 1.406 [0.000, 3.000],  loss: 539.456320, mse: 1584780.196782, mean_q: 1340.010868, mean_eps: 0.880435
 13452/100000: episode: 145, duration: 0.853s, episode steps: 116, steps per second: 136, episode reward: -100.870, mean reward: -0.870 [-100.000, 10.018], mean action: 1.672 [0.000, 3.000],  loss: 682.885242, mse: 1596284.169181, mean_q: 1350.760618, mean_eps: 0.879458
 13526/100000: episode: 146, duration: 0.549s, episode steps:  74, steps per second: 135, episode reward: -123.547, mean reward: -1.670 [-100.000,  7.310], mean action: 1.419 [0.000, 3.000],  loss: 524.490588, mse: 1635145.486486, mean_q: 1363.776260, mean_eps: 0.878604
 13643/100000: episode: 147, duration: 0.821s, episode steps: 117, steps per second: 142, episode reward: -40.077, mean reward: -0.343 [-100.000, 12.149], mean action: 1.496 [0.000, 3.000],  loss: 1576.792737, mse: 1610212.787393, mean_q: 1351.385313, mean_eps: 0.877744
 13751/100000: episode: 148, duration: 0.802s, episode steps: 108, steps per second: 135, episode reward: -93.793, mean reward: -0.868 [-100.000, 16.360], mean action: 1.417 [0.000, 3.000],  loss: 1278.060110, mse: 1619212.962963, mean_q: 1363.053871, mean_eps: 0.876732
 13852/100000: episode: 149, duration: 0.695s, episode steps: 101, steps per second: 145, episode reward: -67.994, mean reward: -0.673 [-100.000, 11.919], mean action: 1.515 [0.000, 3.000],  loss: 1188.892228, mse: 1597312.773515, mean_q: 1347.162893, mean_eps: 0.875791
 13923/100000: episode: 150, duration: 0.492s, episode steps:  71, steps per second: 144, episode reward: -104.774, mean reward: -1.476 [-100.000,  7.986], mean action: 1.535 [0.000, 3.000],  loss: 578.551602, mse: 1616407.149648, mean_q: 1356.327941, mean_eps: 0.875017
 14000/100000: episode: 151, duration: 0.572s, episode steps:  77, steps per second: 135, episode reward: -70.495, mean reward: -0.916 [-100.000,  6.383], mean action: 1.442 [0.000, 3.000],  loss: 1112.766155, mse: 1619201.551948, mean_q: 1352.842369, mean_eps: 0.874351
 14113/100000: episode: 152, duration: 0.841s, episode steps: 113, steps per second: 134, episode reward: -143.734, mean reward: -1.272 [-100.000, 10.968], mean action: 1.708 [0.000, 3.000],  loss: 837.071188, mse: 1646226.672566, mean_q: 1369.758846, mean_eps: 0.873496
 14187/100000: episode: 153, duration: 0.642s, episode steps:  74, steps per second: 115, episode reward: -111.984, mean reward: -1.513 [-100.000, 15.610], mean action: 1.824 [0.000, 3.000],  loss: 581.739267, mse: 1674184.047297, mean_q: 1378.121666, mean_eps: 0.872654
 14257/100000: episode: 154, duration: 0.585s, episode steps:  70, steps per second: 120, episode reward: -39.754, mean reward: -0.568 [-100.000, 16.743], mean action: 1.414 [0.000, 3.000],  loss: 914.779744, mse: 1663013.116071, mean_q: 1369.642322, mean_eps: 0.872007
 14369/100000: episode: 155, duration: 1.102s, episode steps: 112, steps per second: 102, episode reward: -223.253, mean reward: -1.993 [-100.000,  7.589], mean action: 1.330 [0.000, 3.000],  loss: 1090.502437, mse: 1660538.091518, mean_q: 1370.937691, mean_eps: 0.871188
 14485/100000: episode: 156, duration: 1.021s, episode steps: 116, steps per second: 114, episode reward: -178.323, mean reward: -1.537 [-100.000,  3.565], mean action: 1.810 [0.000, 3.000],  loss: 488.184264, mse: 1613420.687500, mean_q: 1345.623899, mean_eps: 0.870161
 14579/100000: episode: 157, duration: 0.749s, episode steps:  94, steps per second: 125, episode reward: -74.523, mean reward: -0.793 [-100.000, 11.687], mean action: 1.670 [0.000, 3.000],  loss: 1081.091614, mse: 1608351.410904, mean_q: 1332.232004, mean_eps: 0.869216
 14689/100000: episode: 158, duration: 0.917s, episode steps: 110, steps per second: 120, episode reward: -366.122, mean reward: -3.328 [-100.000, 67.952], mean action: 1.736 [0.000, 3.000],  loss: 427.245160, mse: 1608370.601136, mean_q: 1343.588390, mean_eps: 0.868298
 14784/100000: episode: 159, duration: 0.711s, episode steps:  95, steps per second: 134, episode reward: -96.736, mean reward: -1.018 [-100.000, 11.506], mean action: 1.589 [0.000, 3.000],  loss: 739.147446, mse: 1621017.948684, mean_q: 1347.206609, mean_eps: 0.867376
 14901/100000: episode: 160, duration: 0.838s, episode steps: 117, steps per second: 140, episode reward: -70.638, mean reward: -0.604 [-100.000, 10.723], mean action: 1.598 [0.000, 3.000],  loss: 1042.647671, mse: 1627623.896368, mean_q: 1346.665612, mean_eps: 0.866422
 14992/100000: episode: 161, duration: 0.673s, episode steps:  91, steps per second: 135, episode reward: -70.830, mean reward: -0.778 [-100.000, 13.409], mean action: 1.725 [0.000, 3.000],  loss: 1286.071464, mse: 1621675.369505, mean_q: 1343.182770, mean_eps: 0.865486
 15069/100000: episode: 162, duration: 0.689s, episode steps:  77, steps per second: 112, episode reward: -111.325, mean reward: -1.446 [-100.000,  6.716], mean action: 1.494 [0.000, 3.000],  loss: 1317.078544, mse: 1643299.922078, mean_q: 1352.718718, mean_eps: 0.864730
 15166/100000: episode: 163, duration: 0.817s, episode steps:  97, steps per second: 119, episode reward: -281.887, mean reward: -2.906 [-100.000, 43.932], mean action: 1.474 [0.000, 3.000],  loss: 679.229844, mse: 1652019.550258, mean_q: 1355.750648, mean_eps: 0.863947
 15280/100000: episode: 164, duration: 0.879s, episode steps: 114, steps per second: 130, episode reward: 16.811, mean reward:  0.147 [-100.000, 92.262], mean action: 1.772 [0.000, 3.000],  loss: 599.227315, mse: 1626032.889254, mean_q: 1338.375326, mean_eps: 0.862997
 15376/100000: episode: 165, duration: 0.786s, episode steps:  96, steps per second: 122, episode reward: -254.403, mean reward: -2.650 [-100.000,  7.580], mean action: 1.604 [0.000, 3.000],  loss: 612.525298, mse: 1628093.881510, mean_q: 1338.236182, mean_eps: 0.862052
 15474/100000: episode: 166, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -273.462, mean reward: -2.790 [-100.000, 37.902], mean action: 1.367 [0.000, 3.000],  loss: 482.101072, mse: 1633935.586735, mean_q: 1350.169934, mean_eps: 0.861180
 15545/100000: episode: 167, duration: 0.547s, episode steps:  71, steps per second: 130, episode reward: -137.970, mean reward: -1.943 [-100.000, 10.706], mean action: 1.493 [0.000, 3.000],  loss: 917.628049, mse: 1611813.283451, mean_q: 1330.275661, mean_eps: 0.860419
 15651/100000: episode: 168, duration: 0.893s, episode steps: 106, steps per second: 119, episode reward: -118.369, mean reward: -1.117 [-100.000,  6.856], mean action: 1.425 [0.000, 3.000],  loss: 807.955838, mse: 1624375.508255, mean_q: 1336.272567, mean_eps: 0.859622
 15715/100000: episode: 169, duration: 0.475s, episode steps:  64, steps per second: 135, episode reward: -121.438, mean reward: -1.897 [-100.000,  9.305], mean action: 1.516 [0.000, 3.000],  loss: 673.555342, mse: 1630128.738281, mean_q: 1338.802526, mean_eps: 0.858858
 15817/100000: episode: 170, duration: 0.799s, episode steps: 102, steps per second: 128, episode reward: -143.645, mean reward: -1.408 [-100.000, 20.248], mean action: 1.520 [0.000, 3.000],  loss: 1218.978651, mse: 1634143.727941, mean_q: 1332.330538, mean_eps: 0.858110
 15915/100000: episode: 171, duration: 0.784s, episode steps:  98, steps per second: 125, episode reward: -109.898, mean reward: -1.121 [-100.000,  6.639], mean action: 1.510 [0.000, 3.000],  loss: 1053.509100, mse: 1652392.576531, mean_q: 1346.063219, mean_eps: 0.857210
 16032/100000: episode: 172, duration: 0.880s, episode steps: 117, steps per second: 133, episode reward: -110.412, mean reward: -0.944 [-100.000, 13.880], mean action: 1.513 [0.000, 3.000],  loss: 578.538448, mse: 1631715.634615, mean_q: 1330.094413, mean_eps: 0.856243
 16145/100000: episode: 173, duration: 0.803s, episode steps: 113, steps per second: 141, episode reward: -37.818, mean reward: -0.335 [-100.000, 81.567], mean action: 1.540 [0.000, 3.000],  loss: 1104.114484, mse: 1648069.873894, mean_q: 1340.103046, mean_eps: 0.855208
 16279/100000: episode: 174, duration: 1.024s, episode steps: 134, steps per second: 131, episode reward: -230.471, mean reward: -1.720 [-100.000,  4.678], mean action: 1.597 [0.000, 3.000],  loss: 795.962002, mse: 1665417.368470, mean_q: 1349.058330, mean_eps: 0.854097
 16357/100000: episode: 175, duration: 0.576s, episode steps:  78, steps per second: 135, episode reward: -94.316, mean reward: -1.209 [-100.000, 10.822], mean action: 1.538 [0.000, 3.000],  loss: 772.643856, mse: 1638754.964744, mean_q: 1337.595395, mean_eps: 0.853142
 16467/100000: episode: 176, duration: 0.902s, episode steps: 110, steps per second: 122, episode reward: -176.788, mean reward: -1.607 [-100.000, 30.979], mean action: 1.600 [0.000, 3.000],  loss: 807.936546, mse: 1622049.373864, mean_q: 1328.299289, mean_eps: 0.852296
 16571/100000: episode: 177, duration: 0.837s, episode steps: 104, steps per second: 124, episode reward: -53.384, mean reward: -0.513 [-100.000, 12.688], mean action: 1.702 [0.000, 3.000],  loss: 501.441647, mse: 1633886.328125, mean_q: 1334.914620, mean_eps: 0.851334
 16675/100000: episode: 178, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: -55.425, mean reward: -0.533 [-100.000, 19.693], mean action: 1.548 [0.000, 3.000],  loss: 495.619010, mse: 1661947.832933, mean_q: 1349.160734, mean_eps: 0.850397
 16765/100000: episode: 179, duration: 0.690s, episode steps:  90, steps per second: 130, episode reward: -96.046, mean reward: -1.067 [-100.000,  8.381], mean action: 1.700 [0.000, 3.000],  loss: 449.353006, mse: 1655610.341667, mean_q: 1342.991125, mean_eps: 0.849525
 16848/100000: episode: 180, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -90.677, mean reward: -1.092 [-100.000, 10.364], mean action: 1.494 [0.000, 3.000],  loss: 994.999426, mse: 1638504.843373, mean_q: 1335.984544, mean_eps: 0.848746
 16933/100000: episode: 181, duration: 0.601s, episode steps:  85, steps per second: 141, episode reward: -66.986, mean reward: -0.788 [-100.000,  6.547], mean action: 1.718 [0.000, 3.000],  loss: 662.583535, mse: 1657406.933824, mean_q: 1345.564259, mean_eps: 0.847990
 17047/100000: episode: 182, duration: 0.831s, episode steps: 114, steps per second: 137, episode reward: -85.653, mean reward: -0.751 [-100.000,  7.827], mean action: 1.544 [0.000, 3.000],  loss: 501.041207, mse: 1646986.342105, mean_q: 1338.463190, mean_eps: 0.847094
 17122/100000: episode: 183, duration: 0.525s, episode steps:  75, steps per second: 143, episode reward: -93.649, mean reward: -1.249 [-100.000, 10.245], mean action: 1.680 [0.000, 3.000],  loss: 657.976886, mse: 1640012.325000, mean_q: 1329.757996, mean_eps: 0.846244
 17220/100000: episode: 184, duration: 0.724s, episode steps:  98, steps per second: 135, episode reward: -79.912, mean reward: -0.815 [-100.000, 11.006], mean action: 1.602 [0.000, 3.000],  loss: 1844.343185, mse: 1692069.723214, mean_q: 1362.026431, mean_eps: 0.845466
 17322/100000: episode: 185, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -54.221, mean reward: -0.532 [-100.000, 15.510], mean action: 1.549 [0.000, 3.000],  loss: 1105.428255, mse: 1696236.285539, mean_q: 1362.504232, mean_eps: 0.844565
 17405/100000: episode: 186, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -72.995, mean reward: -0.879 [-100.000, 11.028], mean action: 1.651 [0.000, 3.000],  loss: 537.165441, mse: 1716217.801205, mean_q: 1375.499648, mean_eps: 0.843733
 17470/100000: episode: 187, duration: 0.475s, episode steps:  65, steps per second: 137, episode reward: -67.181, mean reward: -1.034 [-100.000, 17.361], mean action: 1.769 [0.000, 3.000],  loss: 544.534719, mse: 1688549.480769, mean_q: 1360.536460, mean_eps: 0.843067
 17625/100000: episode: 188, duration: 1.143s, episode steps: 155, steps per second: 136, episode reward: -186.267, mean reward: -1.202 [-100.000, 10.358], mean action: 1.529 [0.000, 3.000],  loss: 566.037026, mse: 1734590.383871, mean_q: 1379.231334, mean_eps: 0.842077
 17719/100000: episode: 189, duration: 0.681s, episode steps:  94, steps per second: 138, episode reward: -112.158, mean reward: -1.193 [-100.000,  8.809], mean action: 1.606 [0.000, 3.000],  loss: 1428.845554, mse: 1723902.941489, mean_q: 1373.213359, mean_eps: 0.840957
 17841/100000: episode: 190, duration: 0.858s, episode steps: 122, steps per second: 142, episode reward: -67.429, mean reward: -0.553 [-100.000, 54.113], mean action: 1.459 [0.000, 3.000],  loss: 915.264086, mse: 1745065.672131, mean_q: 1381.193885, mean_eps: 0.839984
 17933/100000: episode: 191, duration: 0.685s, episode steps:  92, steps per second: 134, episode reward: -95.374, mean reward: -1.037 [-100.000, 18.747], mean action: 1.717 [0.000, 3.000],  loss: 941.135867, mse: 1751836.149457, mean_q: 1384.973045, mean_eps: 0.839021
 18057/100000: episode: 192, duration: 0.875s, episode steps: 124, steps per second: 142, episode reward: -61.748, mean reward: -0.498 [-100.000, 16.975], mean action: 1.581 [0.000, 3.000],  loss: 1257.038512, mse: 1784488.862903, mean_q: 1399.628930, mean_eps: 0.838049
 18150/100000: episode: 193, duration: 0.649s, episode steps:  93, steps per second: 143, episode reward: -123.829, mean reward: -1.331 [-100.000,  7.606], mean action: 1.581 [0.000, 3.000],  loss: 1412.065197, mse: 1761606.290323, mean_q: 1387.271276, mean_eps: 0.837073
 18262/100000: episode: 194, duration: 0.840s, episode steps: 112, steps per second: 133, episode reward: -158.666, mean reward: -1.417 [-100.000, 33.285], mean action: 1.545 [0.000, 3.000],  loss: 702.602779, mse: 1790340.024554, mean_q: 1398.364600, mean_eps: 0.836150
 18369/100000: episode: 195, duration: 0.755s, episode steps: 107, steps per second: 142, episode reward: -126.797, mean reward: -1.185 [-100.000,  5.910], mean action: 1.514 [0.000, 3.000],  loss: 1118.317159, mse: 1805038.120327, mean_q: 1400.168954, mean_eps: 0.835165
 18439/100000: episode: 196, duration: 0.493s, episode steps:  70, steps per second: 142, episode reward: -66.233, mean reward: -0.946 [-100.000, 12.348], mean action: 1.443 [0.000, 3.000],  loss: 767.561007, mse: 1830209.421429, mean_q: 1413.921605, mean_eps: 0.834369
 18525/100000: episode: 197, duration: 0.622s, episode steps:  86, steps per second: 138, episode reward: -148.142, mean reward: -1.723 [-100.000,  5.400], mean action: 1.616 [0.000, 3.000],  loss: 378.183212, mse: 1807739.761628, mean_q: 1403.222580, mean_eps: 0.833667
 18626/100000: episode: 198, duration: 0.738s, episode steps: 101, steps per second: 137, episode reward: -186.450, mean reward: -1.846 [-100.000, 10.776], mean action: 1.515 [0.000, 3.000],  loss: 823.939599, mse: 1791108.844059, mean_q: 1390.545762, mean_eps: 0.832825
 18714/100000: episode: 199, duration: 0.611s, episode steps:  88, steps per second: 144, episode reward: -225.071, mean reward: -2.558 [-100.000,  6.587], mean action: 1.500 [0.000, 3.000],  loss: 632.576566, mse: 1859706.012784, mean_q: 1432.652100, mean_eps: 0.831975
 18847/100000: episode: 200, duration: 0.951s, episode steps: 133, steps per second: 140, episode reward: -117.593, mean reward: -0.884 [-100.000, 11.726], mean action: 1.519 [0.000, 3.000],  loss: 847.905997, mse: 1830142.295113, mean_q: 1409.953304, mean_eps: 0.830980
 18915/100000: episode: 201, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -44.978, mean reward: -0.661 [-100.000, 17.684], mean action: 1.515 [0.000, 3.000],  loss: 1680.073646, mse: 1823166.207721, mean_q: 1409.290307, mean_eps: 0.830076
 19029/100000: episode: 202, duration: 0.787s, episode steps: 114, steps per second: 145, episode reward: -93.092, mean reward: -0.817 [-100.000, 20.551], mean action: 1.430 [0.000, 3.000],  loss: 829.678922, mse: 1845693.049342, mean_q: 1419.561398, mean_eps: 0.829256
 19151/100000: episode: 203, duration: 0.873s, episode steps: 122, steps per second: 140, episode reward: -0.899, mean reward: -0.007 [-100.000, 83.377], mean action: 1.434 [0.000, 3.000],  loss: 1135.920779, mse: 1885198.630123, mean_q: 1429.985887, mean_eps: 0.828195
 19227/100000: episode: 204, duration: 0.544s, episode steps:  76, steps per second: 140, episode reward: -86.680, mean reward: -1.141 [-100.000,  4.473], mean action: 1.671 [0.000, 3.000],  loss: 1814.293606, mse: 1874197.828947, mean_q: 1430.282668, mean_eps: 0.827303
 19333/100000: episode: 205, duration: 0.743s, episode steps: 106, steps per second: 143, episode reward: -101.785, mean reward: -0.960 [-100.000, 18.119], mean action: 1.481 [0.000, 3.000],  loss: 955.160371, mse: 1890309.364387, mean_q: 1428.460002, mean_eps: 0.826485
 19403/100000: episode: 206, duration: 0.497s, episode steps:  70, steps per second: 141, episode reward: -76.743, mean reward: -1.096 [-100.000,  8.119], mean action: 1.557 [0.000, 3.000],  loss: 2029.727294, mse: 1932476.382143, mean_q: 1460.918797, mean_eps: 0.825693
 19479/100000: episode: 207, duration: 0.564s, episode steps:  76, steps per second: 135, episode reward: -115.388, mean reward: -1.518 [-100.000, 12.458], mean action: 1.408 [0.000, 3.000],  loss: 2538.018291, mse: 1895295.988487, mean_q: 1432.626058, mean_eps: 0.825035
 19565/100000: episode: 208, duration: 0.618s, episode steps:  86, steps per second: 139, episode reward: -100.776, mean reward: -1.172 [-100.000, 16.543], mean action: 1.535 [0.000, 3.000],  loss: 626.930190, mse: 1900506.722384, mean_q: 1433.711941, mean_eps: 0.824306
 19663/100000: episode: 209, duration: 0.689s, episode steps:  98, steps per second: 142, episode reward: -149.127, mean reward: -1.522 [-100.000,  6.580], mean action: 1.551 [0.000, 3.000],  loss: 1219.825174, mse: 1909438.659439, mean_q: 1444.604969, mean_eps: 0.823479
 19734/100000: episode: 210, duration: 0.688s, episode steps:  71, steps per second: 103, episode reward: -44.325, mean reward: -0.624 [-100.000, 17.130], mean action: 1.620 [0.000, 3.000],  loss: 1266.977509, mse: 1909659.862676, mean_q: 1440.878394, mean_eps: 0.822718
 19812/100000: episode: 211, duration: 0.639s, episode steps:  78, steps per second: 122, episode reward: -148.315, mean reward: -1.901 [-100.000,  6.368], mean action: 1.449 [0.000, 3.000],  loss: 822.650611, mse: 1880354.514423, mean_q: 1426.947806, mean_eps: 0.822047
 19910/100000: episode: 212, duration: 0.714s, episode steps:  98, steps per second: 137, episode reward: -186.048, mean reward: -1.898 [-100.000,  5.889], mean action: 1.673 [0.000, 3.000],  loss: 1108.923466, mse: 1833791.170918, mean_q: 1409.902737, mean_eps: 0.821256
 20015/100000: episode: 213, duration: 0.760s, episode steps: 105, steps per second: 138, episode reward: -71.380, mean reward: -0.680 [-100.000, 10.701], mean action: 1.543 [0.000, 3.000],  loss: 1333.514427, mse: 1820045.003571, mean_q: 1407.840848, mean_eps: 0.820342
 20094/100000: episode: 214, duration: 0.571s, episode steps:  79, steps per second: 138, episode reward: -175.218, mean reward: -2.218 [-100.000,  5.492], mean action: 1.633 [0.000, 3.000],  loss: 1565.563746, mse: 1802073.082278, mean_q: 1397.380091, mean_eps: 0.819514
 20174/100000: episode: 215, duration: 0.558s, episode steps:  80, steps per second: 143, episode reward: -67.695, mean reward: -0.846 [-100.000, 11.173], mean action: 1.600 [0.000, 3.000],  loss: 692.656798, mse: 1785753.589063, mean_q: 1393.633002, mean_eps: 0.818799
 20304/100000: episode: 216, duration: 1.024s, episode steps: 130, steps per second: 127, episode reward: -278.512, mean reward: -2.142 [-100.000, 103.913], mean action: 1.592 [0.000, 3.000],  loss: 631.036218, mse: 1828881.876923, mean_q: 1408.295329, mean_eps: 0.817854
 20384/100000: episode: 217, duration: 0.632s, episode steps:  80, steps per second: 127, episode reward: -162.010, mean reward: -2.025 [-100.000,  4.825], mean action: 1.800 [0.000, 3.000],  loss: 352.354629, mse: 1859396.592188, mean_q: 1418.589818, mean_eps: 0.816908
 20455/100000: episode: 218, duration: 0.540s, episode steps:  71, steps per second: 132, episode reward: -86.324, mean reward: -1.216 [-100.000, 12.172], mean action: 1.437 [0.000, 3.000],  loss: 876.890601, mse: 1817888.818662, mean_q: 1397.941312, mean_eps: 0.816229
 20570/100000: episode: 219, duration: 0.915s, episode steps: 115, steps per second: 126, episode reward: -81.969, mean reward: -0.713 [-100.000, 68.027], mean action: 1.574 [0.000, 3.000],  loss: 1083.780336, mse: 1802905.605435, mean_q: 1394.385248, mean_eps: 0.815392
 20669/100000: episode: 220, duration: 0.824s, episode steps:  99, steps per second: 120, episode reward: -507.757, mean reward: -5.129 [-100.000, 71.293], mean action: 1.465 [0.000, 3.000],  loss: 839.723530, mse: 1777408.164141, mean_q: 1386.391013, mean_eps: 0.814429
 20739/100000: episode: 221, duration: 0.556s, episode steps:  70, steps per second: 126, episode reward: -17.564, mean reward: -0.251 [-100.000, 11.558], mean action: 1.614 [0.000, 3.000],  loss: 504.253786, mse: 1816583.337500, mean_q: 1396.512019, mean_eps: 0.813668
 20836/100000: episode: 222, duration: 0.713s, episode steps:  97, steps per second: 136, episode reward: -143.036, mean reward: -1.475 [-100.000,  8.572], mean action: 1.464 [0.000, 3.000],  loss: 439.821416, mse: 1795777.207474, mean_q: 1391.172100, mean_eps: 0.812917
 20928/100000: episode: 223, duration: 0.675s, episode steps:  92, steps per second: 136, episode reward: -74.527, mean reward: -0.810 [-100.000,  6.885], mean action: 1.457 [0.000, 3.000],  loss: 481.038558, mse: 1807716.255435, mean_q: 1400.681671, mean_eps: 0.812066
 21007/100000: episode: 224, duration: 0.660s, episode steps:  79, steps per second: 120, episode reward: -207.339, mean reward: -2.625 [-100.000,  6.589], mean action: 1.823 [0.000, 3.000],  loss: 746.120633, mse: 1796712.112342, mean_q: 1384.373900, mean_eps: 0.811297
 21110/100000: episode: 225, duration: 0.863s, episode steps: 103, steps per second: 119, episode reward: -94.991, mean reward: -0.922 [-100.000, 13.693], mean action: 1.437 [0.000, 3.000],  loss: 1057.900390, mse: 1785825.672330, mean_q: 1386.047178, mean_eps: 0.810478
 21207/100000: episode: 226, duration: 0.709s, episode steps:  97, steps per second: 137, episode reward: -57.480, mean reward: -0.593 [-100.000, 18.142], mean action: 1.639 [0.000, 3.000],  loss: 780.972904, mse: 1787737.119845, mean_q: 1384.601305, mean_eps: 0.809578
 21285/100000: episode: 227, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -86.697, mean reward: -1.111 [-100.000, 19.087], mean action: 1.436 [0.000, 3.000],  loss: 937.717296, mse: 1775243.500000, mean_q: 1380.068366, mean_eps: 0.808790
 21375/100000: episode: 228, duration: 0.633s, episode steps:  90, steps per second: 142, episode reward: -130.007, mean reward: -1.445 [-100.000,  8.071], mean action: 1.733 [0.000, 3.000],  loss: 746.908165, mse: 1801368.613889, mean_q: 1384.255800, mean_eps: 0.808034
 21446/100000: episode: 229, duration: 0.538s, episode steps:  71, steps per second: 132, episode reward: -41.015, mean reward: -0.578 [-100.000, 12.458], mean action: 1.521 [0.000, 3.000],  loss: 400.942877, mse: 1831471.151408, mean_q: 1399.362846, mean_eps: 0.807310
 21568/100000: episode: 230, duration: 0.862s, episode steps: 122, steps per second: 142, episode reward: -134.101, mean reward: -1.099 [-100.000, 24.626], mean action: 1.475 [0.000, 3.000],  loss: 834.752663, mse: 1790151.619877, mean_q: 1378.481407, mean_eps: 0.806441
 21758/100000: episode: 231, duration: 1.440s, episode steps: 190, steps per second: 132, episode reward: -31.616, mean reward: -0.166 [-100.000, 41.188], mean action: 1.684 [0.000, 3.000],  loss: 766.557867, mse: 1820117.298026, mean_q: 1394.544353, mean_eps: 0.805037
 21924/100000: episode: 232, duration: 1.154s, episode steps: 166, steps per second: 144, episode reward: -82.403, mean reward: -0.496 [-100.000, 12.041], mean action: 1.518 [0.000, 3.000],  loss: 1063.158557, mse: 1824015.288404, mean_q: 1392.336327, mean_eps: 0.803435
 22033/100000: episode: 233, duration: 0.786s, episode steps: 109, steps per second: 139, episode reward: -104.019, mean reward: -0.954 [-100.000, 10.344], mean action: 1.514 [0.000, 3.000],  loss: 1006.006739, mse: 1771356.938073, mean_q: 1371.362170, mean_eps: 0.802198
 22129/100000: episode: 234, duration: 0.677s, episode steps:  96, steps per second: 142, episode reward: -31.210, mean reward: -0.325 [-100.000, 22.231], mean action: 1.583 [0.000, 3.000],  loss: 714.468073, mse: 1774085.757812, mean_q: 1372.739274, mean_eps: 0.801276
 22207/100000: episode: 235, duration: 0.547s, episode steps:  78, steps per second: 143, episode reward: -109.215, mean reward: -1.400 [-100.000,  5.383], mean action: 1.846 [0.000, 3.000],  loss: 684.968322, mse: 1778222.091346, mean_q: 1366.918593, mean_eps: 0.800492
 22345/100000: episode: 236, duration: 0.987s, episode steps: 138, steps per second: 140, episode reward: -141.987, mean reward: -1.029 [-100.000,  7.972], mean action: 1.703 [0.000, 3.000],  loss: 740.824090, mse: 1762394.707428, mean_q: 1369.768127, mean_eps: 0.799521
 22439/100000: episode: 237, duration: 0.664s, episode steps:  94, steps per second: 142, episode reward: -125.848, mean reward: -1.339 [-100.000, 10.193], mean action: 1.521 [0.000, 3.000],  loss: 878.858904, mse: 1764128.422872, mean_q: 1375.083675, mean_eps: 0.798476
 22539/100000: episode: 238, duration: 0.689s, episode steps: 100, steps per second: 145, episode reward: -253.401, mean reward: -2.534 [-100.000,  0.554], mean action: 1.690 [0.000, 3.000],  loss: 290.880460, mse: 1758793.751250, mean_q: 1366.372906, mean_eps: 0.797603
 22671/100000: episode: 239, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -166.335, mean reward: -1.260 [-100.000, 20.971], mean action: 1.667 [0.000, 3.000],  loss: 1274.165480, mse: 1809744.759470, mean_q: 1389.801430, mean_eps: 0.796560
 22769/100000: episode: 240, duration: 0.690s, episode steps:  98, steps per second: 142, episode reward: -132.866, mean reward: -1.356 [-100.000, 10.121], mean action: 1.622 [0.000, 3.000],  loss: 775.513761, mse: 1778332.200255, mean_q: 1373.152797, mean_eps: 0.795524
 22857/100000: episode: 241, duration: 0.690s, episode steps:  88, steps per second: 128, episode reward: -66.703, mean reward: -0.758 [-100.000,  9.434], mean action: 1.636 [0.000, 3.000],  loss: 1313.295734, mse: 1769457.203125, mean_q: 1369.138192, mean_eps: 0.794688
 22960/100000: episode: 242, duration: 0.898s, episode steps: 103, steps per second: 115, episode reward: -107.672, mean reward: -1.045 [-100.000, 14.315], mean action: 1.466 [0.000, 3.000],  loss: 922.345191, mse: 1767687.952670, mean_q: 1374.068433, mean_eps: 0.793828
 23128/100000: episode: 243, duration: 1.316s, episode steps: 168, steps per second: 128, episode reward: -111.287, mean reward: -0.662 [-100.000,  9.718], mean action: 1.649 [0.000, 3.000],  loss: 1123.271844, mse: 1727978.326637, mean_q: 1356.935620, mean_eps: 0.792609
 23198/100000: episode: 244, duration: 0.574s, episode steps:  70, steps per second: 122, episode reward: -39.702, mean reward: -0.567 [-100.000, 19.308], mean action: 1.629 [0.000, 3.000],  loss: 1250.472549, mse: 1692329.617857, mean_q: 1340.838576, mean_eps: 0.791538
 23272/100000: episode: 245, duration: 0.598s, episode steps:  74, steps per second: 124, episode reward: -81.560, mean reward: -1.102 [-100.000, 12.872], mean action: 1.851 [0.000, 3.000],  loss: 825.697148, mse: 1718633.966216, mean_q: 1350.124569, mean_eps: 0.790890
 23424/100000: episode: 246, duration: 1.143s, episode steps: 152, steps per second: 133, episode reward: -129.157, mean reward: -0.850 [-100.000, 28.383], mean action: 1.789 [0.000, 3.000],  loss: 1028.745240, mse: 1725860.647204, mean_q: 1350.236784, mean_eps: 0.789872
 23520/100000: episode: 247, duration: 0.726s, episode steps:  96, steps per second: 132, episode reward: -103.744, mean reward: -1.081 [-100.000, 12.663], mean action: 1.729 [0.000, 3.000],  loss: 607.285727, mse: 1692572.209635, mean_q: 1335.878574, mean_eps: 0.788756
 23620/100000: episode: 248, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -111.261, mean reward: -1.113 [-100.000,  6.500], mean action: 1.390 [0.000, 3.000],  loss: 1822.606455, mse: 1699889.398750, mean_q: 1341.087144, mean_eps: 0.787875
 23690/100000: episode: 249, duration: 0.496s, episode steps:  70, steps per second: 141, episode reward: -68.067, mean reward: -0.972 [-100.000, 17.898], mean action: 1.686 [0.000, 3.000],  loss: 336.887497, mse: 1723072.137500, mean_q: 1354.629442, mean_eps: 0.787109
 23758/100000: episode: 250, duration: 0.556s, episode steps:  68, steps per second: 122, episode reward: -91.938, mean reward: -1.352 [-100.000, 12.160], mean action: 1.618 [0.000, 3.000],  loss: 849.245256, mse: 1722257.795956, mean_q: 1354.142131, mean_eps: 0.786488
 23882/100000: episode: 251, duration: 1.011s, episode steps: 124, steps per second: 123, episode reward: -36.697, mean reward: -0.296 [-100.000, 26.639], mean action: 1.605 [0.000, 3.000],  loss: 1241.771397, mse: 1687830.179435, mean_q: 1333.564513, mean_eps: 0.785625
 23954/100000: episode: 252, duration: 0.505s, episode steps:  72, steps per second: 142, episode reward: -25.311, mean reward: -0.352 [-100.000, 18.416], mean action: 1.778 [0.000, 3.000],  loss: 899.342155, mse: 1688409.904514, mean_q: 1336.305969, mean_eps: 0.784743
 24021/100000: episode: 253, duration: 0.469s, episode steps:  67, steps per second: 143, episode reward: -63.197, mean reward: -0.943 [-100.000,  6.117], mean action: 1.791 [0.000, 3.000],  loss: 583.392571, mse: 1723957.919776, mean_q: 1356.295933, mean_eps: 0.784117
 24114/100000: episode: 254, duration: 0.710s, episode steps:  93, steps per second: 131, episode reward: -121.934, mean reward: -1.311 [-100.000,  6.165], mean action: 1.452 [0.000, 3.000],  loss: 901.443323, mse: 1698906.806452, mean_q: 1339.437411, mean_eps: 0.783397
 24184/100000: episode: 255, duration: 0.494s, episode steps:  70, steps per second: 142, episode reward: -57.861, mean reward: -0.827 [-100.000,  7.379], mean action: 1.757 [0.000, 3.000],  loss: 1120.497876, mse: 1703925.251786, mean_q: 1341.288286, mean_eps: 0.782663
 24274/100000: episode: 256, duration: 0.625s, episode steps:  90, steps per second: 144, episode reward: -49.692, mean reward: -0.552 [-100.000,  8.160], mean action: 1.689 [0.000, 3.000],  loss: 1070.343742, mse: 1741242.401389, mean_q: 1364.961282, mean_eps: 0.781943
 24348/100000: episode: 257, duration: 0.547s, episode steps:  74, steps per second: 135, episode reward: -105.488, mean reward: -1.426 [-100.000, 26.708], mean action: 1.527 [0.000, 3.000],  loss: 1232.027615, mse: 1733406.239865, mean_q: 1357.806075, mean_eps: 0.781205
 24438/100000: episode: 258, duration: 0.655s, episode steps:  90, steps per second: 138, episode reward: -64.554, mean reward: -0.717 [-100.000, 14.170], mean action: 1.589 [0.000, 3.000],  loss: 641.371047, mse: 1779525.786111, mean_q: 1389.548518, mean_eps: 0.780467
 24554/100000: episode: 259, duration: 0.803s, episode steps: 116, steps per second: 144, episode reward: -162.417, mean reward: -1.400 [-100.000,  9.586], mean action: 1.733 [0.000, 3.000],  loss: 1017.573332, mse: 1805805.936422, mean_q: 1393.491214, mean_eps: 0.779540
 24628/100000: episode: 260, duration: 0.539s, episode steps:  74, steps per second: 137, episode reward: -56.222, mean reward: -0.760 [-100.000, 11.228], mean action: 1.446 [0.000, 3.000],  loss: 492.851969, mse: 1724482.140203, mean_q: 1356.746567, mean_eps: 0.778686
 24703/100000: episode: 261, duration: 0.580s, episode steps:  75, steps per second: 129, episode reward: -88.849, mean reward: -1.185 [-100.000, 10.639], mean action: 1.493 [0.000, 3.000],  loss: 1194.362738, mse: 1672042.786667, mean_q: 1329.715814, mean_eps: 0.778015
 24867/100000: episode: 262, duration: 1.147s, episode steps: 164, steps per second: 143, episode reward: -273.861, mean reward: -1.670 [-100.000, 60.678], mean action: 1.500 [0.000, 3.000],  loss: 701.944865, mse: 1681541.643293, mean_q: 1337.693584, mean_eps: 0.776940
 24939/100000: episode: 263, duration: 0.514s, episode steps:  72, steps per second: 140, episode reward: -89.277, mean reward: -1.240 [-100.000,  8.324], mean action: 1.722 [0.000, 3.000],  loss: 516.694171, mse: 1693437.817708, mean_q: 1340.134423, mean_eps: 0.775877
 25018/100000: episode: 264, duration: 0.580s, episode steps:  79, steps per second: 136, episode reward: -72.445, mean reward: -0.917 [-100.000,  6.978], mean action: 1.392 [0.000, 3.000],  loss: 631.767290, mse: 1707080.822785, mean_q: 1345.703258, mean_eps: 0.775198
 25107/100000: episode: 265, duration: 0.618s, episode steps:  89, steps per second: 144, episode reward: -76.714, mean reward: -0.862 [-100.000, 10.124], mean action: 1.438 [0.000, 3.000],  loss: 903.707340, mse: 1705157.428371, mean_q: 1341.598097, mean_eps: 0.774442
 25226/100000: episode: 266, duration: 0.839s, episode steps: 119, steps per second: 142, episode reward: -32.982, mean reward: -0.277 [-100.000, 14.953], mean action: 1.580 [0.000, 3.000],  loss: 613.180936, mse: 1676592.914916, mean_q: 1331.091266, mean_eps: 0.773506
 25336/100000: episode: 267, duration: 0.820s, episode steps: 110, steps per second: 134, episode reward: -57.846, mean reward: -0.526 [-100.000,  7.489], mean action: 1.582 [0.000, 3.000],  loss: 627.542464, mse: 1657234.622727, mean_q: 1324.683018, mean_eps: 0.772476
 25459/100000: episode: 268, duration: 0.889s, episode steps: 123, steps per second: 138, episode reward: -141.198, mean reward: -1.148 [-100.000, 12.002], mean action: 1.691 [0.000, 3.000],  loss: 427.808125, mse: 1650510.920732, mean_q: 1326.204702, mean_eps: 0.771427
 25549/100000: episode: 269, duration: 0.657s, episode steps:  90, steps per second: 137, episode reward: -183.259, mean reward: -2.036 [-100.000,  7.573], mean action: 1.511 [0.000, 3.000],  loss: 475.122932, mse: 1626425.393056, mean_q: 1309.323062, mean_eps: 0.770468
 25654/100000: episode: 270, duration: 0.747s, episode steps: 105, steps per second: 141, episode reward: -98.788, mean reward: -0.941 [-100.000, 13.208], mean action: 1.771 [0.000, 3.000],  loss: 423.400401, mse: 1675976.869048, mean_q: 1339.256786, mean_eps: 0.769591
 25786/100000: episode: 271, duration: 0.928s, episode steps: 132, steps per second: 142, episode reward: -92.925, mean reward: -0.704 [-100.000, 11.779], mean action: 1.750 [0.000, 3.000],  loss: 1678.818779, mse: 1620370.468750, mean_q: 1308.617685, mean_eps: 0.768524
 25910/100000: episode: 272, duration: 0.969s, episode steps: 124, steps per second: 128, episode reward: -149.194, mean reward: -1.203 [-100.000, 12.751], mean action: 1.677 [0.000, 3.000],  loss: 496.301324, mse: 1644315.672379, mean_q: 1324.322754, mean_eps: 0.767373
 25977/100000: episode: 273, duration: 0.496s, episode steps:  67, steps per second: 135, episode reward: -81.380, mean reward: -1.215 [-100.000, 48.635], mean action: 1.463 [0.000, 3.000],  loss: 1332.583719, mse: 1675545.283582, mean_q: 1332.166688, mean_eps: 0.766513
 26148/100000: episode: 274, duration: 1.203s, episode steps: 171, steps per second: 142, episode reward: -27.081, mean reward: -0.158 [-100.000, 14.349], mean action: 1.561 [0.000, 3.000],  loss: 897.454756, mse: 1673916.932018, mean_q: 1340.577572, mean_eps: 0.765442
 26269/100000: episode: 275, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward: -212.708, mean reward: -1.758 [-100.000,  5.974], mean action: 1.711 [0.000, 3.000],  loss: 1091.106706, mse: 1653330.378099, mean_q: 1328.950785, mean_eps: 0.764128
 26346/100000: episode: 276, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward: -75.876, mean reward: -0.985 [-100.000, 15.114], mean action: 1.455 [0.000, 3.000],  loss: 945.201987, mse: 1661424.092532, mean_q: 1334.182926, mean_eps: 0.763237
 26443/100000: episode: 277, duration: 0.725s, episode steps:  97, steps per second: 134, episode reward: -138.989, mean reward: -1.433 [-100.000,  8.792], mean action: 1.495 [0.000, 3.000],  loss: 583.191694, mse: 1665742.117268, mean_q: 1341.218001, mean_eps: 0.762454
 26518/100000: episode: 278, duration: 0.543s, episode steps:  75, steps per second: 138, episode reward: -130.042, mean reward: -1.734 [-100.000,  5.098], mean action: 1.853 [0.000, 3.000],  loss: 695.600905, mse: 1643601.833333, mean_q: 1332.913763, mean_eps: 0.761680
 26664/100000: episode: 279, duration: 1.017s, episode steps: 146, steps per second: 144, episode reward: -128.730, mean reward: -0.882 [-100.000,  7.907], mean action: 1.685 [0.000, 3.000],  loss: 607.622385, mse: 1636686.726884, mean_q: 1326.016199, mean_eps: 0.760685
 26842/100000: episode: 280, duration: 1.319s, episode steps: 178, steps per second: 135, episode reward: -203.520, mean reward: -1.143 [-100.000, 13.620], mean action: 1.596 [0.000, 3.000],  loss: 439.729184, mse: 1609236.233848, mean_q: 1312.591025, mean_eps: 0.759227
 26948/100000: episode: 281, duration: 0.738s, episode steps: 106, steps per second: 144, episode reward: -50.650, mean reward: -0.478 [-100.000, 15.866], mean action: 1.689 [0.000, 3.000],  loss: 472.284398, mse: 1594258.959906, mean_q: 1306.234278, mean_eps: 0.757949
 27015/100000: episode: 282, duration: 0.518s, episode steps:  67, steps per second: 129, episode reward: -64.231, mean reward: -0.959 [-100.000,  7.369], mean action: 1.522 [0.000, 3.000],  loss: 349.084919, mse: 1615195.416045, mean_q: 1320.509840, mean_eps: 0.757171
 27124/100000: episode: 283, duration: 0.898s, episode steps: 109, steps per second: 121, episode reward: -66.227, mean reward: -0.608 [-100.000, 16.119], mean action: 1.587 [0.000, 3.000],  loss: 1032.492570, mse: 1606088.542431, mean_q: 1309.677411, mean_eps: 0.756379
 27248/100000: episode: 284, duration: 0.897s, episode steps: 124, steps per second: 138, episode reward: -164.233, mean reward: -1.324 [-100.000,  6.570], mean action: 1.548 [0.000, 3.000],  loss: 674.788288, mse: 1560857.579637, mean_q: 1292.767209, mean_eps: 0.755331
 27399/100000: episode: 285, duration: 1.102s, episode steps: 151, steps per second: 137, episode reward: -163.060, mean reward: -1.080 [-100.000, 11.737], mean action: 1.629 [0.000, 3.000],  loss: 707.303669, mse: 1557887.250000, mean_q: 1285.782291, mean_eps: 0.754093
 27480/100000: episode: 286, duration: 0.580s, episode steps:  81, steps per second: 140, episode reward: -59.074, mean reward: -0.729 [-100.000, 17.064], mean action: 1.519 [0.000, 3.000],  loss: 1270.386036, mse: 1582326.472222, mean_q: 1301.006443, mean_eps: 0.753049
 27655/100000: episode: 287, duration: 1.341s, episode steps: 175, steps per second: 130, episode reward: -67.413, mean reward: -0.385 [-100.000, 17.402], mean action: 1.526 [0.000, 3.000],  loss: 681.297151, mse: 1597178.378571, mean_q: 1305.526496, mean_eps: 0.751897
 27749/100000: episode: 288, duration: 0.683s, episode steps:  94, steps per second: 138, episode reward: -150.886, mean reward: -1.605 [-100.000,  8.055], mean action: 1.660 [0.000, 3.000],  loss: 763.731113, mse: 1547709.347074, mean_q: 1283.827220, mean_eps: 0.750687
 27834/100000: episode: 289, duration: 0.584s, episode steps:  85, steps per second: 145, episode reward: -16.738, mean reward: -0.197 [-100.000, 11.646], mean action: 1.659 [0.000, 3.000],  loss: 594.243985, mse: 1554374.955882, mean_q: 1286.402838, mean_eps: 0.749881
 27958/100000: episode: 290, duration: 0.918s, episode steps: 124, steps per second: 135, episode reward: -117.626, mean reward: -0.949 [-100.000,  9.653], mean action: 1.669 [0.000, 3.000],  loss: 1052.848822, mse: 1515958.493952, mean_q: 1275.706713, mean_eps: 0.748941
 28096/100000: episode: 291, duration: 0.959s, episode steps: 138, steps per second: 144, episode reward: -152.469, mean reward: -1.105 [-100.000, 15.084], mean action: 1.500 [0.000, 3.000],  loss: 368.695633, mse: 1521151.185688, mean_q: 1272.346611, mean_eps: 0.747762
 28218/100000: episode: 292, duration: 0.855s, episode steps: 122, steps per second: 143, episode reward: -197.098, mean reward: -1.616 [-100.000,  8.818], mean action: 1.779 [0.000, 3.000],  loss: 808.926160, mse: 1523438.059426, mean_q: 1273.727702, mean_eps: 0.746592
 28352/100000: episode: 293, duration: 1.035s, episode steps: 134, steps per second: 130, episode reward: -150.852, mean reward: -1.126 [-100.000, 21.562], mean action: 1.709 [0.000, 3.000],  loss: 714.676907, mse: 1520693.894590, mean_q: 1275.362219, mean_eps: 0.745439
 28466/100000: episode: 294, duration: 0.827s, episode steps: 114, steps per second: 138, episode reward: -127.950, mean reward: -1.122 [-100.000, 17.263], mean action: 1.754 [0.000, 3.000],  loss: 701.963951, mse: 1481673.557018, mean_q: 1253.170328, mean_eps: 0.744324
 28628/100000: episode: 295, duration: 1.189s, episode steps: 162, steps per second: 136, episode reward: -101.967, mean reward: -0.629 [-100.000,  9.149], mean action: 1.605 [0.000, 3.000],  loss: 588.613462, mse: 1485640.898920, mean_q: 1255.092932, mean_eps: 0.743082
 28785/100000: episode: 296, duration: 1.144s, episode steps: 157, steps per second: 137, episode reward: -213.594, mean reward: -1.360 [-100.000, 32.746], mean action: 1.771 [0.000, 3.000],  loss: 1132.770332, mse: 1486216.464172, mean_q: 1255.395841, mean_eps: 0.741646
 28855/100000: episode: 297, duration: 0.685s, episode steps:  70, steps per second: 102, episode reward: -58.789, mean reward: -0.840 [-100.000, 10.268], mean action: 1.700 [0.000, 3.000],  loss: 866.578528, mse: 1521060.375000, mean_q: 1274.451392, mean_eps: 0.740625
 28953/100000: episode: 298, duration: 0.863s, episode steps:  98, steps per second: 113, episode reward: -83.614, mean reward: -0.853 [-100.000, 13.772], mean action: 1.551 [0.000, 3.000],  loss: 1131.549739, mse: 1514601.422194, mean_q: 1273.426247, mean_eps: 0.739868
 29064/100000: episode: 299, duration: 0.858s, episode steps: 111, steps per second: 129, episode reward: -104.987, mean reward: -0.946 [-100.000, 15.881], mean action: 1.586 [0.000, 3.000],  loss: 633.499629, mse: 1509579.239865, mean_q: 1269.294061, mean_eps: 0.738928
 29256/100000: episode: 300, duration: 1.547s, episode steps: 192, steps per second: 124, episode reward: -126.332, mean reward: -0.658 [-100.000,  9.614], mean action: 1.641 [0.000, 3.000],  loss: 637.966609, mse: 1504910.736979, mean_q: 1267.981423, mean_eps: 0.737564
 29443/100000: episode: 301, duration: 1.478s, episode steps: 187, steps per second: 127, episode reward: -296.411, mean reward: -1.585 [-100.000, 43.241], mean action: 1.567 [0.000, 3.000],  loss: 537.737670, mse: 1558721.871658, mean_q: 1290.999450, mean_eps: 0.735859
 29520/100000: episode: 302, duration: 0.599s, episode steps:  77, steps per second: 128, episode reward: -45.124, mean reward: -0.586 [-100.000,  9.239], mean action: 1.571 [0.000, 3.000],  loss: 583.869952, mse: 1600579.133117, mean_q: 1302.620686, mean_eps: 0.734671
 29608/100000: episode: 303, duration: 0.729s, episode steps:  88, steps per second: 121, episode reward: -55.043, mean reward: -0.625 [-100.000,  8.383], mean action: 1.682 [0.000, 3.000],  loss: 1336.305520, mse: 1555365.321023, mean_q: 1287.230036, mean_eps: 0.733929
 29719/100000: episode: 304, duration: 0.805s, episode steps: 111, steps per second: 138, episode reward: -120.003, mean reward: -1.081 [-100.000, 12.124], mean action: 1.505 [0.000, 3.000],  loss: 307.415600, mse: 1597404.962838, mean_q: 1299.851866, mean_eps: 0.733033
 29846/100000: episode: 305, duration: 0.878s, episode steps: 127, steps per second: 145, episode reward: -155.859, mean reward: -1.227 [-100.000,  8.158], mean action: 1.496 [0.000, 3.000],  loss: 378.043505, mse: 1567361.801181, mean_q: 1291.318619, mean_eps: 0.731962
 29951/100000: episode: 306, duration: 0.822s, episode steps: 105, steps per second: 128, episode reward: -86.904, mean reward: -0.828 [-100.000,  9.611], mean action: 1.581 [0.000, 3.000],  loss: 747.539130, mse: 1580481.946429, mean_q: 1294.087466, mean_eps: 0.730918
 30032/100000: episode: 307, duration: 0.579s, episode steps:  81, steps per second: 140, episode reward: -87.930, mean reward: -1.086 [-100.000,  9.083], mean action: 1.556 [0.000, 3.000],  loss: 774.504906, mse: 1550029.097222, mean_q: 1280.285420, mean_eps: 0.730081
 30106/100000: episode: 308, duration: 0.528s, episode steps:  74, steps per second: 140, episode reward: -46.084, mean reward: -0.623 [-100.000, 14.526], mean action: 1.635 [0.000, 3.000],  loss: 779.040930, mse: 1537175.476351, mean_q: 1279.355611, mean_eps: 0.729384
 30279/100000: episode: 309, duration: 1.261s, episode steps: 173, steps per second: 137, episode reward: -298.524, mean reward: -1.726 [-100.000,  4.842], mean action: 1.561 [0.000, 3.000],  loss: 424.269134, mse: 1562537.603324, mean_q: 1276.463783, mean_eps: 0.728272
 30361/100000: episode: 310, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -46.760, mean reward: -0.570 [-100.000, 11.130], mean action: 1.402 [0.000, 3.000],  loss: 523.803033, mse: 1588573.689024, mean_q: 1287.500104, mean_eps: 0.727124
 30468/100000: episode: 311, duration: 0.742s, episode steps: 107, steps per second: 144, episode reward: -151.374, mean reward: -1.415 [-100.000,  7.354], mean action: 1.589 [0.000, 3.000],  loss: 883.776737, mse: 1561398.617991, mean_q: 1278.040988, mean_eps: 0.726274
 30592/100000: episode: 312, duration: 0.918s, episode steps: 124, steps per second: 135, episode reward: -170.870, mean reward: -1.378 [-100.000,  3.505], mean action: 1.758 [0.000, 3.000],  loss: 830.646313, mse: 1559225.573589, mean_q: 1275.119976, mean_eps: 0.725235
 30682/100000: episode: 313, duration: 0.629s, episode steps:  90, steps per second: 143, episode reward: -82.278, mean reward: -0.914 [-100.000,  4.970], mean action: 1.622 [0.000, 3.000],  loss: 1095.264171, mse: 1583117.690278, mean_q: 1282.647780, mean_eps: 0.724271
 30838/100000: episode: 314, duration: 1.124s, episode steps: 156, steps per second: 139, episode reward: -137.544, mean reward: -0.882 [-100.000, 15.158], mean action: 1.571 [0.000, 3.000],  loss: 755.708009, mse: 1606101.116987, mean_q: 1294.036754, mean_eps: 0.723165
 30962/100000: episode: 315, duration: 0.861s, episode steps: 124, steps per second: 144, episode reward: -173.619, mean reward: -1.400 [-100.000, 11.370], mean action: 1.435 [0.000, 3.000],  loss: 780.460971, mse: 1636664.924395, mean_q: 1311.477743, mean_eps: 0.721904
 31215/100000: episode: 316, duration: 1.820s, episode steps: 253, steps per second: 139, episode reward: -128.608, mean reward: -0.508 [-100.000,  7.622], mean action: 1.605 [0.000, 3.000],  loss: 881.115135, mse: 1647727.497036, mean_q: 1317.130911, mean_eps: 0.720208
 31312/100000: episode: 317, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -103.179, mean reward: -1.064 [-100.000, 10.303], mean action: 1.433 [0.000, 3.000],  loss: 1425.608394, mse: 1670014.039948, mean_q: 1327.115408, mean_eps: 0.718633
 31403/100000: episode: 318, duration: 0.662s, episode steps:  91, steps per second: 137, episode reward: -147.842, mean reward: -1.625 [-100.000, 15.025], mean action: 1.516 [0.000, 3.000],  loss: 611.527281, mse: 1683952.012363, mean_q: 1336.423835, mean_eps: 0.717787
 31529/100000: episode: 319, duration: 0.901s, episode steps: 126, steps per second: 140, episode reward: -131.413, mean reward: -1.043 [-100.000, 18.286], mean action: 1.500 [0.000, 3.000],  loss: 899.891464, mse: 1666013.440476, mean_q: 1330.754427, mean_eps: 0.716810
 31726/100000: episode: 320, duration: 1.531s, episode steps: 197, steps per second: 129, episode reward: -147.766, mean reward: -0.750 [-100.000,  9.035], mean action: 1.665 [0.000, 3.000],  loss: 547.411417, mse: 1663610.130711, mean_q: 1323.391786, mean_eps: 0.715357
 31792/100000: episode: 321, duration: 0.583s, episode steps:  66, steps per second: 113, episode reward: -56.796, mean reward: -0.861 [-100.000,  7.473], mean action: 1.500 [0.000, 3.000],  loss: 412.041755, mse: 1673653.045455, mean_q: 1324.682917, mean_eps: 0.714173
 31864/100000: episode: 322, duration: 0.570s, episode steps:  72, steps per second: 126, episode reward: -48.853, mean reward: -0.679 [-100.000, 14.946], mean action: 1.375 [0.000, 3.000],  loss: 502.249928, mse: 1689839.053819, mean_q: 1327.822274, mean_eps: 0.713553
 32031/100000: episode: 323, duration: 1.357s, episode steps: 167, steps per second: 123, episode reward: -173.389, mean reward: -1.038 [-100.000,  9.868], mean action: 1.503 [0.000, 3.000],  loss: 531.466185, mse: 1696510.456587, mean_q: 1330.783036, mean_eps: 0.712477
 32128/100000: episode: 324, duration: 0.742s, episode steps:  97, steps per second: 131, episode reward: -63.501, mean reward: -0.655 [-100.000, 15.355], mean action: 1.691 [0.000, 3.000],  loss: 673.705853, mse: 1687869.237113, mean_q: 1326.868667, mean_eps: 0.711289
 32272/100000: episode: 325, duration: 1.104s, episode steps: 144, steps per second: 130, episode reward: -78.423, mean reward: -0.545 [-100.000, 33.492], mean action: 1.569 [0.000, 3.000],  loss: 717.091451, mse: 1652329.501736, mean_q: 1318.258792, mean_eps: 0.710205
 32382/100000: episode: 326, duration: 0.790s, episode steps: 110, steps per second: 139, episode reward: -135.051, mean reward: -1.228 [-100.000, 10.052], mean action: 1.464 [0.000, 3.000],  loss: 534.989395, mse: 1664210.173864, mean_q: 1325.082824, mean_eps: 0.709062
 32520/100000: episode: 327, duration: 0.970s, episode steps: 138, steps per second: 142, episode reward: -223.095, mean reward: -1.617 [-100.000, 19.534], mean action: 1.500 [0.000, 3.000],  loss: 874.484571, mse: 1682951.404891, mean_q: 1329.632821, mean_eps: 0.707946
 32598/100000: episode: 328, duration: 0.599s, episode steps:  78, steps per second: 130, episode reward: -85.268, mean reward: -1.093 [-100.000,  6.242], mean action: 1.718 [0.000, 3.000],  loss: 428.117143, mse: 1701071.597756, mean_q: 1331.053054, mean_eps: 0.706974
 32679/100000: episode: 329, duration: 0.582s, episode steps:  81, steps per second: 139, episode reward: -47.984, mean reward: -0.592 [-100.000,  9.346], mean action: 1.580 [0.000, 3.000],  loss: 1137.834221, mse: 1717363.300926, mean_q: 1342.838659, mean_eps: 0.706258
 32802/100000: episode: 330, duration: 0.878s, episode steps: 123, steps per second: 140, episode reward: -51.409, mean reward: -0.418 [-100.000, 16.360], mean action: 1.626 [0.000, 3.000],  loss: 1045.426654, mse: 1681324.530488, mean_q: 1323.532106, mean_eps: 0.705340
 32885/100000: episode: 331, duration: 0.646s, episode steps:  83, steps per second: 128, episode reward: -27.712, mean reward: -0.334 [-100.000, 16.887], mean action: 1.639 [0.000, 3.000],  loss: 687.969241, mse: 1674880.003012, mean_q: 1320.592319, mean_eps: 0.704413
 33037/100000: episode: 332, duration: 1.087s, episode steps: 152, steps per second: 140, episode reward: -142.939, mean reward: -0.940 [-100.000,  7.325], mean action: 1.599 [0.000, 3.000],  loss: 804.253452, mse: 1630886.310033, mean_q: 1296.317867, mean_eps: 0.703356
 33212/100000: episode: 333, duration: 1.272s, episode steps: 175, steps per second: 138, episode reward: -79.609, mean reward: -0.455 [-100.000, 47.767], mean action: 1.651 [0.000, 3.000],  loss: 791.142462, mse: 1665390.703571, mean_q: 1314.398956, mean_eps: 0.701884
 33304/100000: episode: 334, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -22.868, mean reward: -0.249 [-100.000, 70.558], mean action: 1.467 [0.000, 3.000],  loss: 790.071696, mse: 1693729.201087, mean_q: 1324.520866, mean_eps: 0.700682
 33452/100000: episode: 335, duration: 1.066s, episode steps: 148, steps per second: 139, episode reward: -150.433, mean reward: -1.016 [-100.000,  5.132], mean action: 1.716 [0.000, 3.000],  loss: 926.965565, mse: 1682791.794764, mean_q: 1324.840773, mean_eps: 0.699602
 33690/100000: episode: 336, duration: 1.708s, episode steps: 238, steps per second: 139, episode reward: -253.476, mean reward: -1.065 [-100.000, 10.594], mean action: 1.622 [0.000, 3.000],  loss: 572.332535, mse: 1665705.099265, mean_q: 1313.541158, mean_eps: 0.697866
 33791/100000: episode: 337, duration: 0.748s, episode steps: 101, steps per second: 135, episode reward: -50.344, mean reward: -0.498 [-100.000, 16.369], mean action: 1.634 [0.000, 3.000],  loss: 368.202259, mse: 1668447.428218, mean_q: 1313.605153, mean_eps: 0.696340
 33883/100000: episode: 338, duration: 0.637s, episode steps:  92, steps per second: 144, episode reward: -30.124, mean reward: -0.327 [-100.000, 42.719], mean action: 1.576 [0.000, 3.000],  loss: 779.127245, mse: 1716206.437500, mean_q: 1329.002006, mean_eps: 0.695472
 34054/100000: episode: 339, duration: 1.224s, episode steps: 171, steps per second: 140, episode reward: -24.181, mean reward: -0.141 [-100.000, 49.666], mean action: 1.643 [0.000, 3.000],  loss: 1027.934087, mse: 1672439.600146, mean_q: 1314.104274, mean_eps: 0.694288
 34176/100000: episode: 340, duration: 0.955s, episode steps: 122, steps per second: 128, episode reward: -22.796, mean reward: -0.187 [-100.000, 19.419], mean action: 1.557 [0.000, 3.000],  loss: 758.127618, mse: 1641873.649590, mean_q: 1299.086904, mean_eps: 0.692970
 34323/100000: episode: 341, duration: 1.041s, episode steps: 147, steps per second: 141, episode reward: -129.551, mean reward: -0.881 [-100.000, 11.760], mean action: 1.633 [0.000, 3.000],  loss: 559.872602, mse: 1649482.339286, mean_q: 1302.141903, mean_eps: 0.691759
 34421/100000: episode: 342, duration: 0.728s, episode steps:  98, steps per second: 135, episode reward: -62.141, mean reward: -0.634 [-100.000, 27.309], mean action: 1.714 [0.000, 3.000],  loss: 870.627163, mse: 1648039.380102, mean_q: 1300.054517, mean_eps: 0.690657
 34597/100000: episode: 343, duration: 1.256s, episode steps: 176, steps per second: 140, episode reward: -50.291, mean reward: -0.286 [-100.000,  9.861], mean action: 1.653 [0.000, 3.000],  loss: 1001.453210, mse: 1684927.817472, mean_q: 1313.932933, mean_eps: 0.689424
 34708/100000: episode: 344, duration: 0.831s, episode steps: 111, steps per second: 134, episode reward: -106.610, mean reward: -0.960 [-100.000, 12.974], mean action: 1.622 [0.000, 3.000],  loss: 1034.732424, mse: 1650362.746622, mean_q: 1305.146620, mean_eps: 0.688132
 34853/100000: episode: 345, duration: 1.029s, episode steps: 145, steps per second: 141, episode reward: -89.953, mean reward: -0.620 [-100.000, 22.361], mean action: 1.586 [0.000, 3.000],  loss: 441.310540, mse: 1639013.088793, mean_q: 1301.806806, mean_eps: 0.686980
 35021/100000: episode: 346, duration: 1.231s, episode steps: 168, steps per second: 136, episode reward: -188.306, mean reward: -1.121 [-100.000,  5.544], mean action: 1.524 [0.000, 3.000],  loss: 689.387916, mse: 1630573.851190, mean_q: 1293.258123, mean_eps: 0.685572
 35175/100000: episode: 347, duration: 1.076s, episode steps: 154, steps per second: 143, episode reward: -210.435, mean reward: -1.366 [-100.000, 14.050], mean action: 1.519 [0.000, 3.000],  loss: 1269.463958, mse: 1614418.208604, mean_q: 1284.600402, mean_eps: 0.684122
 35296/100000: episode: 348, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -153.147, mean reward: -1.266 [-100.000, 16.698], mean action: 1.554 [0.000, 3.000],  loss: 1132.986017, mse: 1591538.127066, mean_q: 1280.038879, mean_eps: 0.682885
 35379/100000: episode: 349, duration: 0.597s, episode steps:  83, steps per second: 139, episode reward: -33.795, mean reward: -0.407 [-100.000, 12.604], mean action: 1.663 [0.000, 3.000],  loss: 866.827436, mse: 1529989.283133, mean_q: 1259.475008, mean_eps: 0.681967
 35549/100000: episode: 350, duration: 1.204s, episode steps: 170, steps per second: 141, episode reward: -89.042, mean reward: -0.524 [-100.000, 15.281], mean action: 1.582 [0.000, 3.000],  loss: 1016.677796, mse: 1547145.748529, mean_q: 1262.295199, mean_eps: 0.680828
 35637/100000: episode: 351, duration: 0.650s, episode steps:  88, steps per second: 135, episode reward: -80.818, mean reward: -0.918 [-100.000,  7.697], mean action: 1.739 [0.000, 3.000],  loss: 1187.111045, mse: 1569059.718750, mean_q: 1272.842954, mean_eps: 0.679667
 35709/100000: episode: 352, duration: 0.512s, episode steps:  72, steps per second: 141, episode reward: -62.491, mean reward: -0.868 [-100.000, 10.816], mean action: 1.486 [0.000, 3.000],  loss: 1375.219733, mse: 1561052.645833, mean_q: 1272.623562, mean_eps: 0.678948
 35866/100000: episode: 353, duration: 1.162s, episode steps: 157, steps per second: 135, episode reward: -163.098, mean reward: -1.039 [-100.000, 10.696], mean action: 1.739 [0.000, 3.000],  loss: 521.710135, mse: 1543993.121019, mean_q: 1266.929916, mean_eps: 0.677917
 35946/100000: episode: 354, duration: 0.579s, episode steps:  80, steps per second: 138, episode reward: -97.885, mean reward: -1.224 [-100.000, 10.465], mean action: 1.538 [0.000, 3.000],  loss: 787.262381, mse: 1513347.432813, mean_q: 1254.771568, mean_eps: 0.676851
 36018/100000: episode: 355, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: -26.136, mean reward: -0.363 [-100.000, 21.268], mean action: 1.639 [0.000, 3.000],  loss: 376.691188, mse: 1558906.520833, mean_q: 1269.093613, mean_eps: 0.676167
 36142/100000: episode: 356, duration: 0.878s, episode steps: 124, steps per second: 141, episode reward: -135.230, mean reward: -1.091 [-100.000,  6.593], mean action: 1.427 [0.000, 3.000],  loss: 482.092958, mse: 1522467.708669, mean_q: 1256.852743, mean_eps: 0.675285
 36211/100000: episode: 357, duration: 0.530s, episode steps:  69, steps per second: 130, episode reward: -20.261, mean reward: -0.294 [-100.000, 15.883], mean action: 1.565 [0.000, 3.000],  loss: 764.729975, mse: 1502436.588768, mean_q: 1253.873119, mean_eps: 0.674416
 36286/100000: episode: 358, duration: 0.535s, episode steps:  75, steps per second: 140, episode reward: -54.886, mean reward: -0.732 [-100.000,  7.605], mean action: 1.640 [0.000, 3.000],  loss: 537.839370, mse: 1476768.781667, mean_q: 1226.771044, mean_eps: 0.673768
 36401/100000: episode: 359, duration: 0.797s, episode steps: 115, steps per second: 144, episode reward: -178.717, mean reward: -1.554 [-100.000,  3.101], mean action: 1.904 [0.000, 3.000],  loss: 534.033166, mse: 1485751.153261, mean_q: 1232.597321, mean_eps: 0.672913
 36477/100000: episode: 360, duration: 0.589s, episode steps:  76, steps per second: 129, episode reward: -35.518, mean reward: -0.467 [-100.000, 14.410], mean action: 1.658 [0.000, 3.000],  loss: 413.034826, mse: 1457486.180921, mean_q: 1221.792328, mean_eps: 0.672054
 36558/100000: episode: 361, duration: 0.588s, episode steps:  81, steps per second: 138, episode reward: -89.407, mean reward: -1.104 [-100.000,  8.726], mean action: 1.605 [0.000, 3.000],  loss: 824.438123, mse: 1449201.841821, mean_q: 1214.696379, mean_eps: 0.671347
 36687/100000: episode: 362, duration: 0.892s, episode steps: 129, steps per second: 145, episode reward: -83.066, mean reward: -0.644 [-100.000, 26.730], mean action: 1.674 [0.000, 3.000],  loss: 680.394110, mse: 1412392.842539, mean_q: 1194.019247, mean_eps: 0.670402
 36861/100000: episode: 363, duration: 1.272s, episode steps: 174, steps per second: 137, episode reward: -183.992, mean reward: -1.057 [-100.000, 14.698], mean action: 1.603 [0.000, 3.000],  loss: 403.211275, mse: 1419153.769756, mean_q: 1194.950653, mean_eps: 0.669038
 36947/100000: episode: 364, duration: 0.596s, episode steps:  86, steps per second: 144, episode reward: -52.697, mean reward: -0.613 [-100.000, 12.545], mean action: 1.640 [0.000, 3.000],  loss: 253.543850, mse: 1443046.798692, mean_q: 1202.428517, mean_eps: 0.667868
 37084/100000: episode: 365, duration: 1.011s, episode steps: 137, steps per second: 136, episode reward: -80.347, mean reward: -0.586 [-100.000,  7.180], mean action: 1.679 [0.000, 3.000],  loss: 707.066421, mse: 1424991.063869, mean_q: 1197.010824, mean_eps: 0.666865
 37183/100000: episode: 366, duration: 0.729s, episode steps:  99, steps per second: 136, episode reward: -222.408, mean reward: -2.247 [-100.000,  4.997], mean action: 1.657 [0.000, 3.000],  loss: 801.018663, mse: 1434265.988005, mean_q: 1202.530262, mean_eps: 0.665803
 37297/100000: episode: 367, duration: 0.807s, episode steps: 114, steps per second: 141, episode reward: -207.086, mean reward: -1.817 [-100.000,  2.473], mean action: 1.737 [0.000, 3.000],  loss: 439.449081, mse: 1396599.884320, mean_q: 1185.206905, mean_eps: 0.664845
 37434/100000: episode: 368, duration: 1.010s, episode steps: 137, steps per second: 136, episode reward: -247.064, mean reward: -1.803 [-100.000,  1.573], mean action: 1.781 [0.000, 3.000],  loss: 503.375186, mse: 1389145.699818, mean_q: 1174.722344, mean_eps: 0.663715
 37532/100000: episode: 369, duration: 0.702s, episode steps:  98, steps per second: 140, episode reward: -96.140, mean reward: -0.981 [-100.000,  6.934], mean action: 1.653 [0.000, 3.000],  loss: 909.885924, mse: 1410013.764031, mean_q: 1180.439265, mean_eps: 0.662657
 37756/100000: episode: 370, duration: 1.637s, episode steps: 224, steps per second: 137, episode reward: -319.092, mean reward: -1.425 [-100.000,  3.393], mean action: 1.661 [0.000, 3.000],  loss: 382.230606, mse: 1385100.726842, mean_q: 1170.903962, mean_eps: 0.661208
 37929/100000: episode: 371, duration: 1.229s, episode steps: 173, steps per second: 141, episode reward: -214.265, mean reward: -1.239 [-100.000,  9.103], mean action: 1.705 [0.000, 3.000],  loss: 356.454438, mse: 1410580.830925, mean_q: 1182.725315, mean_eps: 0.659422
 38046/100000: episode: 372, duration: 0.876s, episode steps: 117, steps per second: 134, episode reward: -144.324, mean reward: -1.234 [-100.000,  3.470], mean action: 1.692 [0.000, 3.000],  loss: 425.820053, mse: 1422747.627137, mean_q: 1186.638351, mean_eps: 0.658117
 38201/100000: episode: 373, duration: 1.207s, episode steps: 155, steps per second: 128, episode reward: -143.125, mean reward: -0.923 [-100.000, 11.578], mean action: 1.742 [0.000, 3.000],  loss: 902.871763, mse: 1404305.486290, mean_q: 1176.264005, mean_eps: 0.656893
 38351/100000: episode: 374, duration: 1.233s, episode steps: 150, steps per second: 122, episode reward: -243.616, mean reward: -1.624 [-100.000,  2.024], mean action: 1.760 [0.000, 3.000],  loss: 493.588488, mse: 1351659.750417, mean_q: 1147.328490, mean_eps: 0.655520
 38532/100000: episode: 375, duration: 1.373s, episode steps: 181, steps per second: 132, episode reward: -54.411, mean reward: -0.301 [-100.000, 19.907], mean action: 1.680 [0.000, 3.000],  loss: 364.971303, mse: 1360158.902279, mean_q: 1152.985149, mean_eps: 0.654031
 38730/100000: episode: 376, duration: 1.422s, episode steps: 198, steps per second: 139, episode reward: -287.168, mean reward: -1.450 [-100.000, 12.566], mean action: 1.596 [0.000, 3.000],  loss: 521.777495, mse: 1354916.586174, mean_q: 1144.969503, mean_eps: 0.652325
 38928/100000: episode: 377, duration: 1.425s, episode steps: 198, steps per second: 139, episode reward: -177.973, mean reward: -0.899 [-100.000,  4.676], mean action: 1.662 [0.000, 3.000],  loss: 476.522904, mse: 1375750.887942, mean_q: 1156.076812, mean_eps: 0.650544
 39263/100000: episode: 378, duration: 2.583s, episode steps: 335, steps per second: 130, episode reward: -206.015, mean reward: -0.615 [-100.000, 51.713], mean action: 1.716 [0.000, 3.000],  loss: 440.583444, mse: 1336541.019776, mean_q: 1135.623848, mean_eps: 0.648145
 39467/100000: episode: 379, duration: 1.447s, episode steps: 204, steps per second: 141, episode reward: -145.812, mean reward: -0.715 [-100.000, 55.951], mean action: 1.618 [0.000, 3.000],  loss: 487.635564, mse: 1296199.697917, mean_q: 1113.476668, mean_eps: 0.645720
 39725/100000: episode: 380, duration: 1.874s, episode steps: 258, steps per second: 138, episode reward: -149.387, mean reward: -0.579 [-100.000, 13.499], mean action: 1.609 [0.000, 3.000],  loss: 478.559372, mse: 1291250.233043, mean_q: 1106.381881, mean_eps: 0.643641
 39845/100000: episode: 381, duration: 0.867s, episode steps: 120, steps per second: 138, episode reward: 36.701, mean reward:  0.306 [-100.000, 15.676], mean action: 1.792 [0.000, 3.000],  loss: 333.549093, mse: 1288537.113021, mean_q: 1105.673275, mean_eps: 0.641939
 40022/100000: episode: 382, duration: 1.296s, episode steps: 177, steps per second: 137, episode reward: -82.797, mean reward: -0.468 [-100.000, 35.225], mean action: 1.791 [0.000, 3.000],  loss: 615.147858, mse: 1326436.587218, mean_q: 1122.123004, mean_eps: 0.640603
 40114/100000: episode: 383, duration: 0.654s, episode steps:  92, steps per second: 141, episode reward: -42.171, mean reward: -0.458 [-100.000, 14.205], mean action: 1.717 [0.000, 3.000],  loss: 1086.406960, mse: 1339102.771739, mean_q: 1124.989423, mean_eps: 0.639393
 40193/100000: episode: 384, duration: 0.584s, episode steps:  79, steps per second: 135, episode reward: -45.520, mean reward: -0.576 [-100.000, 12.097], mean action: 1.835 [0.000, 3.000],  loss: 513.916417, mse: 1314469.257911, mean_q: 1113.422634, mean_eps: 0.638623
 40309/100000: episode: 385, duration: 0.858s, episode steps: 116, steps per second: 135, episode reward: -158.351, mean reward: -1.365 [-100.000,  1.829], mean action: 1.897 [0.000, 3.000],  loss: 801.744372, mse: 1331368.603448, mean_q: 1123.723784, mean_eps: 0.637745
 40452/100000: episode: 386, duration: 1.028s, episode steps: 143, steps per second: 139, episode reward: -142.575, mean reward: -0.997 [-100.000,  3.002], mean action: 1.748 [0.000, 3.000],  loss: 772.730106, mse: 1350655.432692, mean_q: 1130.885007, mean_eps: 0.636580
 40586/100000: episode: 387, duration: 0.963s, episode steps: 134, steps per second: 139, episode reward: -130.542, mean reward: -0.974 [-100.000,  2.546], mean action: 1.679 [0.000, 3.000],  loss: 798.888190, mse: 1352515.444496, mean_q: 1115.765956, mean_eps: 0.635334
 40709/100000: episode: 388, duration: 1.074s, episode steps: 123, steps per second: 115, episode reward: -74.074, mean reward: -0.602 [-100.000,  9.323], mean action: 1.545 [0.000, 3.000],  loss: 454.450559, mse: 1298835.856707, mean_q: 1099.622072, mean_eps: 0.634177
 40858/100000: episode: 389, duration: 1.249s, episode steps: 149, steps per second: 119, episode reward: -223.054, mean reward: -1.497 [-100.000,  2.804], mean action: 1.698 [0.000, 3.000],  loss: 711.141537, mse: 1300114.425336, mean_q: 1103.197839, mean_eps: 0.632953
 41559/100000: episode: 390, duration: 5.791s, episode steps: 701, steps per second: 121, episode reward: -293.227, mean reward: -0.418 [-100.000, 29.147], mean action: 1.663 [0.000, 3.000],  loss: 538.684015, mse: 1284429.003834, mean_q: 1089.149067, mean_eps: 0.629128
 41651/100000: episode: 391, duration: 0.671s, episode steps:  92, steps per second: 137, episode reward: -269.153, mean reward: -2.926 [-100.000,  5.419], mean action: 1.576 [0.000, 3.000],  loss: 1035.960504, mse: 1250141.637228, mean_q: 1074.528727, mean_eps: 0.625559
 41741/100000: episode: 392, duration: 0.647s, episode steps:  90, steps per second: 139, episode reward: -121.120, mean reward: -1.346 [-100.000,  6.424], mean action: 1.411 [0.000, 3.000],  loss: 1209.360041, mse: 1270556.250694, mean_q: 1086.243532, mean_eps: 0.624740
 41889/100000: episode: 393, duration: 1.030s, episode steps: 148, steps per second: 144, episode reward: -171.831, mean reward: -1.161 [-100.000,  1.920], mean action: 1.723 [0.000, 3.000],  loss: 1166.291594, mse: 1236668.652027, mean_q: 1070.559325, mean_eps: 0.623669
 41988/100000: episode: 394, duration: 0.746s, episode steps:  99, steps per second: 133, episode reward: -255.271, mean reward: -2.578 [-100.000, 70.845], mean action: 1.687 [0.000, 3.000],  loss: 557.654838, mse: 1291722.131313, mean_q: 1098.326658, mean_eps: 0.622558
 42090/100000: episode: 395, duration: 0.708s, episode steps: 102, steps per second: 144, episode reward: -49.322, mean reward: -0.484 [-100.000, 13.722], mean action: 1.627 [0.000, 3.000],  loss: 1208.648461, mse: 1248412.439338, mean_q: 1080.084330, mean_eps: 0.621653
 42258/100000: episode: 396, duration: 1.203s, episode steps: 168, steps per second: 140, episode reward: -225.182, mean reward: -1.340 [-100.000,  3.168], mean action: 1.720 [0.000, 3.000],  loss: 1276.303376, mse: 1273518.837426, mean_q: 1093.845964, mean_eps: 0.620439
 42355/100000: episode: 397, duration: 0.707s, episode steps:  97, steps per second: 137, episode reward: -118.555, mean reward: -1.222 [-100.000, 10.145], mean action: 1.588 [0.000, 3.000],  loss: 1565.820238, mse: 1304289.004510, mean_q: 1100.918376, mean_eps: 0.619246
 42421/100000: episode: 398, duration: 0.496s, episode steps:  66, steps per second: 133, episode reward: -81.702, mean reward: -1.238 [-100.000, 34.032], mean action: 1.712 [0.000, 3.000],  loss: 1393.561701, mse: 1246028.828598, mean_q: 1070.231556, mean_eps: 0.618512
 42595/100000: episode: 399, duration: 1.277s, episode steps: 174, steps per second: 136, episode reward: -193.055, mean reward: -1.110 [-100.000,  1.965], mean action: 1.695 [0.000, 3.000],  loss: 1074.832980, mse: 1242397.367816, mean_q: 1077.270503, mean_eps: 0.617432
 42700/100000: episode: 400, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -158.946, mean reward: -1.514 [-100.000,  2.995], mean action: 1.610 [0.000, 3.000],  loss: 788.904932, mse: 1243832.200595, mean_q: 1079.230647, mean_eps: 0.616177
 42932/100000: episode: 401, duration: 1.706s, episode steps: 232, steps per second: 136, episode reward: -136.260, mean reward: -0.587 [-100.000, 53.230], mean action: 1.711 [0.000, 3.000],  loss: 1362.673812, mse: 1253405.415409, mean_q: 1083.972130, mean_eps: 0.614661
 43078/100000: episode: 402, duration: 1.011s, episode steps: 146, steps per second: 144, episode reward: -159.019, mean reward: -1.089 [-100.000, 10.335], mean action: 1.884 [0.000, 3.000],  loss: 1166.337677, mse: 1235898.449058, mean_q: 1076.122760, mean_eps: 0.612959
 43245/100000: episode: 403, duration: 1.224s, episode steps: 167, steps per second: 136, episode reward: -132.865, mean reward: -0.796 [-100.000,  9.432], mean action: 1.677 [0.000, 3.000],  loss: 923.675650, mse: 1252096.586826, mean_q: 1083.005089, mean_eps: 0.611551
 43653/100000: episode: 404, duration: 3.003s, episode steps: 408, steps per second: 136, episode reward: -254.318, mean reward: -0.623 [-100.000, 49.037], mean action: 1.667 [0.000, 3.000],  loss: 1338.998765, mse: 1247547.745251, mean_q: 1080.635613, mean_eps: 0.608963
 43747/100000: episode: 405, duration: 0.678s, episode steps:  94, steps per second: 139, episode reward: -216.955, mean reward: -2.308 [-100.000,  9.011], mean action: 1.691 [0.000, 3.000],  loss: 1134.051407, mse: 1245682.535239, mean_q: 1080.994198, mean_eps: 0.606704
 43888/100000: episode: 406, duration: 0.994s, episode steps: 141, steps per second: 142, episode reward: -204.317, mean reward: -1.449 [-100.000, 19.510], mean action: 1.674 [0.000, 3.000],  loss: 1177.698027, mse: 1253913.339539, mean_q: 1085.927456, mean_eps: 0.605647
 44056/100000: episode: 407, duration: 1.219s, episode steps: 168, steps per second: 138, episode reward: -202.923, mean reward: -1.208 [-100.000,  9.011], mean action: 1.607 [0.000, 3.000],  loss: 1014.763570, mse: 1271520.710565, mean_q: 1087.128637, mean_eps: 0.604256
 44216/100000: episode: 408, duration: 1.120s, episode steps: 160, steps per second: 143, episode reward: -276.948, mean reward: -1.731 [-100.000,  9.400], mean action: 1.625 [0.000, 3.000],  loss: 767.477004, mse: 1286300.607422, mean_q: 1103.674573, mean_eps: 0.602780
 44395/100000: episode: 409, duration: 1.299s, episode steps: 179, steps per second: 138, episode reward: -102.668, mean reward: -0.574 [-100.000, 10.916], mean action: 1.682 [0.000, 3.000],  loss: 933.691130, mse: 1279185.211592, mean_q: 1096.094702, mean_eps: 0.601255
 44590/100000: episode: 410, duration: 1.368s, episode steps: 195, steps per second: 143, episode reward: -338.821, mean reward: -1.738 [-100.000,  7.936], mean action: 1.733 [0.000, 3.000],  loss: 1527.336290, mse: 1288788.824038, mean_q: 1105.901158, mean_eps: 0.599572
 44659/100000: episode: 411, duration: 0.527s, episode steps:  69, steps per second: 131, episode reward: -117.384, mean reward: -1.701 [-100.000,  7.209], mean action: 1.681 [0.000, 3.000],  loss: 1368.319786, mse: 1263938.956522, mean_q: 1086.185603, mean_eps: 0.598384
 44800/100000: episode: 412, duration: 0.997s, episode steps: 141, steps per second: 141, episode reward: -240.139, mean reward: -1.703 [-100.000, 43.568], mean action: 1.702 [0.000, 3.000],  loss: 2229.703043, mse: 1279858.554521, mean_q: 1101.492751, mean_eps: 0.597439
 44972/100000: episode: 413, duration: 1.227s, episode steps: 172, steps per second: 140, episode reward: -183.025, mean reward: -1.064 [-100.000,  5.162], mean action: 1.820 [0.000, 3.000],  loss: 1409.652153, mse: 1284547.004360, mean_q: 1099.644657, mean_eps: 0.596030
 45074/100000: episode: 414, duration: 0.724s, episode steps: 102, steps per second: 141, episode reward: -166.778, mean reward: -1.635 [-100.000,  8.433], mean action: 1.716 [0.000, 3.000],  loss: 1161.351498, mse: 1292996.681985, mean_q: 1106.638637, mean_eps: 0.594798
 45214/100000: episode: 415, duration: 0.977s, episode steps: 140, steps per second: 143, episode reward: -32.974, mean reward: -0.236 [-100.000, 12.390], mean action: 1.514 [0.000, 3.000],  loss: 1257.055054, mse: 1316955.060268, mean_q: 1119.920330, mean_eps: 0.593708
 45464/100000: episode: 416, duration: 1.815s, episode steps: 250, steps per second: 138, episode reward: -271.842, mean reward: -1.087 [-100.000,  8.016], mean action: 1.744 [0.000, 3.000],  loss: 1428.602959, mse: 1323665.258250, mean_q: 1118.916449, mean_eps: 0.591954
 45584/100000: episode: 417, duration: 0.887s, episode steps: 120, steps per second: 135, episode reward: -143.401, mean reward: -1.195 [-100.000,  2.906], mean action: 1.683 [0.000, 3.000],  loss: 1857.808917, mse: 1324491.970833, mean_q: 1119.996851, mean_eps: 0.590288
 45758/100000: episode: 418, duration: 1.234s, episode steps: 174, steps per second: 141, episode reward: -199.675, mean reward: -1.148 [-100.000, 32.604], mean action: 1.736 [0.000, 3.000],  loss: 1095.576541, mse: 1353314.904095, mean_q: 1138.455732, mean_eps: 0.588966
 45930/100000: episode: 419, duration: 1.236s, episode steps: 172, steps per second: 139, episode reward: -219.408, mean reward: -1.276 [-100.000,  2.693], mean action: 1.669 [0.000, 3.000],  loss: 1699.518069, mse: 1372892.709666, mean_q: 1148.512383, mean_eps: 0.587409
 46089/100000: episode: 420, duration: 1.105s, episode steps: 159, steps per second: 144, episode reward: -109.319, mean reward: -0.688 [-100.000, 33.819], mean action: 1.761 [0.000, 3.000],  loss: 1110.964623, mse: 1399184.817217, mean_q: 1166.224612, mean_eps: 0.585919
 46836/100000: episode: 421, duration: 5.854s, episode steps: 747, steps per second: 128, episode reward: -316.065, mean reward: -0.423 [-100.000, 44.380], mean action: 1.668 [0.000, 3.000],  loss: 1793.910387, mse: 1432996.748494, mean_q: 1181.539168, mean_eps: 0.581842
 47128/100000: episode: 422, duration: 2.341s, episode steps: 292, steps per second: 125, episode reward: -487.753, mean reward: -1.670 [-100.000, 75.424], mean action: 1.664 [0.000, 3.000],  loss: 1750.432914, mse: 1506645.798587, mean_q: 1213.098969, mean_eps: 0.577167
 47267/100000: episode: 423, duration: 1.080s, episode steps: 139, steps per second: 129, episode reward: -153.838, mean reward: -1.107 [-100.000,  4.669], mean action: 1.676 [0.000, 3.000],  loss: 1338.675651, mse: 1552842.502698, mean_q: 1241.896518, mean_eps: 0.575227
 47397/100000: episode: 424, duration: 0.956s, episode steps: 130, steps per second: 136, episode reward: -136.757, mean reward: -1.052 [-100.000, 10.486], mean action: 1.546 [0.000, 3.000],  loss: 1774.074298, mse: 1561056.393269, mean_q: 1245.623009, mean_eps: 0.574017
 47564/100000: episode: 425, duration: 1.183s, episode steps: 167, steps per second: 141, episode reward: -161.330, mean reward: -0.966 [-100.000,  7.422], mean action: 1.737 [0.000, 3.000],  loss: 2338.019745, mse: 1614079.364147, mean_q: 1274.095021, mean_eps: 0.572680
 47651/100000: episode: 426, duration: 0.632s, episode steps:  87, steps per second: 138, episode reward: -72.704, mean reward: -0.836 [-100.000,  5.595], mean action: 1.540 [0.000, 3.000],  loss: 2551.076962, mse: 1624102.485632, mean_q: 1270.708018, mean_eps: 0.571537
 47790/100000: episode: 427, duration: 0.963s, episode steps: 139, steps per second: 144, episode reward: -166.898, mean reward: -1.201 [-100.000, 17.831], mean action: 1.849 [0.000, 3.000],  loss: 2808.399102, mse: 1648506.073741, mean_q: 1288.025907, mean_eps: 0.570520
 47964/100000: episode: 428, duration: 1.261s, episode steps: 174, steps per second: 138, episode reward: -160.831, mean reward: -0.924 [-100.000, 12.566], mean action: 1.534 [0.000, 3.000],  loss: 2352.839508, mse: 1727032.194684, mean_q: 1335.597066, mean_eps: 0.569111
 48266/100000: episode: 429, duration: 2.169s, episode steps: 302, steps per second: 139, episode reward: -104.497, mean reward: -0.346 [-100.000, 22.118], mean action: 1.629 [0.000, 3.000],  loss: 2187.931314, mse: 1764822.033113, mean_q: 1346.361212, mean_eps: 0.566969
 48423/100000: episode: 430, duration: 1.095s, episode steps: 157, steps per second: 143, episode reward: -72.474, mean reward: -0.462 [-100.000, 14.278], mean action: 1.561 [0.000, 3.000],  loss: 2459.060675, mse: 1857439.980096, mean_q: 1392.284432, mean_eps: 0.564904
 48562/100000: episode: 431, duration: 1.021s, episode steps: 139, steps per second: 136, episode reward: -97.354, mean reward: -0.700 [-100.000, 21.114], mean action: 1.676 [0.000, 3.000],  loss: 3660.498189, mse: 1879058.843525, mean_q: 1405.328395, mean_eps: 0.563572
 48680/100000: episode: 432, duration: 0.816s, episode steps: 118, steps per second: 145, episode reward: -25.184, mean reward: -0.213 [-100.000, 14.912], mean action: 1.661 [0.000, 3.000],  loss: 2917.711841, mse: 1942121.932203, mean_q: 1437.285011, mean_eps: 0.562415
 48886/100000: episode: 433, duration: 1.478s, episode steps: 206, steps per second: 139, episode reward: -97.585, mean reward: -0.474 [-100.000, 14.214], mean action: 1.641 [0.000, 3.000],  loss: 3467.115966, mse: 1953717.655947, mean_q: 1446.736744, mean_eps: 0.560957
 49052/100000: episode: 434, duration: 1.154s, episode steps: 166, steps per second: 144, episode reward: -115.316, mean reward: -0.695 [-100.000, 30.686], mean action: 1.657 [0.000, 3.000],  loss: 3058.357938, mse: 1955309.984187, mean_q: 1443.972541, mean_eps: 0.559283
 49214/100000: episode: 435, duration: 1.800s, episode steps: 162, steps per second:  90, episode reward: -175.606, mean reward: -1.084 [-100.000, 11.421], mean action: 1.765 [0.000, 3.000],  loss: 2590.179223, mse: 1983556.635031, mean_q: 1456.366015, mean_eps: 0.557808
 49323/100000: episode: 436, duration: 1.274s, episode steps: 109, steps per second:  86, episode reward: -71.013, mean reward: -0.651 [-100.000, 21.657], mean action: 1.670 [0.000, 3.000],  loss: 2421.753431, mse: 2000574.011468, mean_q: 1465.754037, mean_eps: 0.556588
 49471/100000: episode: 437, duration: 1.738s, episode steps: 148, steps per second:  85, episode reward: -65.673, mean reward: -0.444 [-100.000, 40.180], mean action: 1.784 [0.000, 3.000],  loss: 3253.989014, mse: 2004119.076014, mean_q: 1464.532618, mean_eps: 0.555432
 49581/100000: episode: 438, duration: 1.337s, episode steps: 110, steps per second:  82, episode reward: -81.023, mean reward: -0.737 [-100.000, 15.376], mean action: 1.718 [0.000, 3.000],  loss: 4685.864162, mse: 2004888.637500, mean_q: 1464.578968, mean_eps: 0.554270
 49675/100000: episode: 439, duration: 1.093s, episode steps:  94, steps per second:  86, episode reward: -94.111, mean reward: -1.001 [-100.000, 20.841], mean action: 1.585 [0.000, 3.000],  loss: 2626.933393, mse: 1968382.973404, mean_q: 1447.259633, mean_eps: 0.553353
 49811/100000: episode: 440, duration: 1.475s, episode steps: 136, steps per second:  92, episode reward: -178.909, mean reward: -1.316 [-100.000, 31.705], mean action: 1.706 [0.000, 3.000],  loss: 2963.770971, mse: 2000702.946691, mean_q: 1457.878860, mean_eps: 0.552318
 49903/100000: episode: 441, duration: 0.846s, episode steps:  92, steps per second: 109, episode reward: -85.439, mean reward: -0.929 [-100.000, 23.142], mean action: 1.652 [0.000, 3.000],  loss: 4648.250575, mse: 2091646.501359, mean_q: 1499.729456, mean_eps: 0.551291
 50151/100000: episode: 442, duration: 2.276s, episode steps: 248, steps per second: 109, episode reward: -42.749, mean reward: -0.172 [-100.000, 12.544], mean action: 1.750 [0.000, 3.000],  loss: 3655.484877, mse: 2053419.528730, mean_q: 1483.408946, mean_eps: 0.549762
 50252/100000: episode: 443, duration: 0.934s, episode steps: 101, steps per second: 108, episode reward: -110.736, mean reward: -1.096 [-100.000,  6.714], mean action: 1.604 [0.000, 3.000],  loss: 4420.099457, mse: 2091089.702970, mean_q: 1506.107260, mean_eps: 0.548191
 50418/100000: episode: 444, duration: 1.741s, episode steps: 166, steps per second:  95, episode reward: -182.818, mean reward: -1.101 [-100.000,  7.697], mean action: 1.795 [0.000, 3.000],  loss: 3006.534730, mse: 2134913.385542, mean_q: 1519.125591, mean_eps: 0.546989
 50559/100000: episode: 445, duration: 1.540s, episode steps: 141, steps per second:  92, episode reward:  3.888, mean reward:  0.028 [-100.000, 16.759], mean action: 1.780 [0.000, 3.000],  loss: 2781.592948, mse: 2143675.578014, mean_q: 1525.056021, mean_eps: 0.545608
 50705/100000: episode: 446, duration: 1.526s, episode steps: 146, steps per second:  96, episode reward: -49.624, mean reward: -0.340 [-100.000, 27.844], mean action: 1.705 [0.000, 3.000],  loss: 3503.790645, mse: 2196070.127568, mean_q: 1548.884013, mean_eps: 0.544316
 50833/100000: episode: 447, duration: 1.480s, episode steps: 128, steps per second:  86, episode reward: -117.511, mean reward: -0.918 [-100.000,  2.867], mean action: 1.719 [0.000, 3.000],  loss: 3208.147594, mse: 2267030.779297, mean_q: 1568.956611, mean_eps: 0.543084
 51018/100000: episode: 448, duration: 1.948s, episode steps: 185, steps per second:  95, episode reward: -148.874, mean reward: -0.805 [-100.000,  8.484], mean action: 1.719 [0.000, 3.000],  loss: 2730.106816, mse: 2275810.732432, mean_q: 1579.855298, mean_eps: 0.541675
 51199/100000: episode: 449, duration: 1.944s, episode steps: 181, steps per second:  93, episode reward: -144.595, mean reward: -0.799 [-100.000,  4.337], mean action: 1.812 [0.000, 3.000],  loss: 4720.402419, mse: 2373078.894337, mean_q: 1618.648314, mean_eps: 0.540028
 51430/100000: episode: 450, duration: 2.193s, episode steps: 231, steps per second: 105, episode reward: -452.064, mean reward: -1.957 [-100.000,  9.528], mean action: 1.714 [0.000, 3.000],  loss: 3106.466178, mse: 2421693.943182, mean_q: 1632.604243, mean_eps: 0.538174
 51780/100000: episode: 451, duration: 3.217s, episode steps: 350, steps per second: 109, episode reward: -258.054, mean reward: -0.737 [-100.000,  3.717], mean action: 1.714 [0.000, 3.000],  loss: 4050.121412, mse: 2494866.472143, mean_q: 1658.702152, mean_eps: 0.535559
 51876/100000: episode: 452, duration: 0.911s, episode steps:  96, steps per second: 105, episode reward: -91.487, mean reward: -0.953 [-100.000,  8.244], mean action: 1.615 [0.000, 3.000],  loss: 4658.127685, mse: 2568351.649740, mean_q: 1691.852131, mean_eps: 0.533552
 52159/100000: episode: 453, duration: 2.570s, episode steps: 283, steps per second: 110, episode reward: -301.168, mean reward: -1.064 [-100.000, 13.107], mean action: 1.700 [0.000, 3.000],  loss: 3568.809368, mse: 2611266.010601, mean_q: 1703.513181, mean_eps: 0.531847
 52380/100000: episode: 454, duration: 2.356s, episode steps: 221, steps per second:  94, episode reward: -314.094, mean reward: -1.421 [-100.000, 10.573], mean action: 1.778 [0.000, 3.000],  loss: 4072.601090, mse: 2716749.587670, mean_q: 1741.016884, mean_eps: 0.529579
 52562/100000: episode: 455, duration: 1.817s, episode steps: 182, steps per second: 100, episode reward: -224.416, mean reward: -1.233 [-100.000, 13.849], mean action: 1.731 [0.000, 3.000],  loss: 2928.516001, mse: 2799110.872253, mean_q: 1770.574443, mean_eps: 0.527765
 52777/100000: episode: 456, duration: 1.900s, episode steps: 215, steps per second: 113, episode reward: -170.081, mean reward: -0.791 [-100.000,  2.967], mean action: 1.726 [0.000, 3.000],  loss: 3571.742136, mse: 2874119.172093, mean_q: 1797.733791, mean_eps: 0.525979
 53031/100000: episode: 457, duration: 2.318s, episode steps: 254, steps per second: 110, episode reward: -119.900, mean reward: -0.472 [-100.000, 13.477], mean action: 1.630 [0.000, 3.000],  loss: 4926.302521, mse: 2876890.869587, mean_q: 1793.195360, mean_eps: 0.523868
 53171/100000: episode: 458, duration: 1.216s, episode steps: 140, steps per second: 115, episode reward: -189.912, mean reward: -1.357 [-100.000,  2.043], mean action: 1.743 [0.000, 3.000],  loss: 4828.092593, mse: 2938216.974107, mean_q: 1811.164002, mean_eps: 0.522095
 53445/100000: episode: 459, duration: 2.622s, episode steps: 274, steps per second: 104, episode reward: -61.320, mean reward: -0.224 [-100.000, 16.342], mean action: 1.682 [0.000, 3.000],  loss: 4121.796883, mse: 3045455.398723, mean_q: 1844.118932, mean_eps: 0.520232
 53762/100000: episode: 460, duration: 2.882s, episode steps: 317, steps per second: 110, episode reward: -265.691, mean reward: -0.838 [-100.000, 18.624], mean action: 1.741 [0.000, 3.000],  loss: 5351.219819, mse: 3149826.608833, mean_q: 1883.153074, mean_eps: 0.517573
 53868/100000: episode: 461, duration: 0.944s, episode steps: 106, steps per second: 112, episode reward: -106.433, mean reward: -1.004 [-100.000, 12.386], mean action: 1.632 [0.000, 3.000],  loss: 3075.814640, mse: 3165448.759434, mean_q: 1883.563049, mean_eps: 0.515669
 54157/100000: episode: 462, duration: 2.831s, episode steps: 289, steps per second: 102, episode reward: -309.024, mean reward: -1.069 [-100.000,  7.879], mean action: 1.723 [0.000, 3.000],  loss: 3553.385008, mse: 3195100.404844, mean_q: 1896.275018, mean_eps: 0.513892
 54317/100000: episode: 463, duration: 1.454s, episode steps: 160, steps per second: 110, episode reward: -227.696, mean reward: -1.423 [-100.000,  8.918], mean action: 1.750 [0.000, 3.000],  loss: 5254.148230, mse: 3254481.703125, mean_q: 1911.405950, mean_eps: 0.511872
 54430/100000: episode: 464, duration: 1.093s, episode steps: 113, steps per second: 103, episode reward: -207.580, mean reward: -1.837 [-100.000,  8.774], mean action: 1.788 [0.000, 3.000],  loss: 3995.834079, mse: 3363594.696903, mean_q: 1951.452998, mean_eps: 0.510643
 54663/100000: episode: 465, duration: 2.025s, episode steps: 233, steps per second: 115, episode reward: -59.761, mean reward: -0.256 [-100.000, 19.249], mean action: 1.627 [0.000, 3.000],  loss: 6763.643315, mse: 3401476.589056, mean_q: 1960.191492, mean_eps: 0.509086
 54866/100000: episode: 466, duration: 1.817s, episode steps: 203, steps per second: 112, episode reward: -116.348, mean reward: -0.573 [-100.000, 19.616], mean action: 1.739 [0.000, 3.000],  loss: 4431.290674, mse: 3516331.561576, mean_q: 1991.771413, mean_eps: 0.507124
 55103/100000: episode: 467, duration: 2.160s, episode steps: 237, steps per second: 110, episode reward: -131.386, mean reward: -0.554 [-100.000, 27.277], mean action: 1.671 [0.000, 3.000],  loss: 5806.209501, mse: 3472887.632911, mean_q: 1972.447743, mean_eps: 0.505144
 55303/100000: episode: 468, duration: 1.745s, episode steps: 200, steps per second: 115, episode reward: -176.655, mean reward: -0.883 [-100.000, 16.427], mean action: 1.735 [0.000, 3.000],  loss: 4989.486665, mse: 3484276.045000, mean_q: 1987.812900, mean_eps: 0.503177
 55530/100000: episode: 469, duration: 2.062s, episode steps: 227, steps per second: 110, episode reward: -465.217, mean reward: -2.049 [-100.000, 10.277], mean action: 1.793 [0.000, 3.000],  loss: 5766.723552, mse: 3588048.783040, mean_q: 2012.161434, mean_eps: 0.501256
 55711/100000: episode: 470, duration: 1.655s, episode steps: 181, steps per second: 109, episode reward: -300.325, mean reward: -1.659 [-100.000, 17.441], mean action: 1.691 [0.000, 3.000],  loss: 4217.223546, mse: 3589589.241713, mean_q: 2012.698218, mean_eps: 0.499420
 55842/100000: episode: 471, duration: 1.184s, episode steps: 131, steps per second: 111, episode reward: -287.951, mean reward: -2.198 [-100.000, 24.342], mean action: 1.863 [0.000, 3.000],  loss: 7195.807640, mse: 3672241.809160, mean_q: 2029.127130, mean_eps: 0.498016
 56004/100000: episode: 472, duration: 1.427s, episode steps: 162, steps per second: 114, episode reward: -239.107, mean reward: -1.476 [-100.000, 46.544], mean action: 1.648 [0.000, 3.000],  loss: 7137.132517, mse: 3619723.587963, mean_q: 2010.133090, mean_eps: 0.496698
 56176/100000: episode: 473, duration: 1.561s, episode steps: 172, steps per second: 110, episode reward: -197.435, mean reward: -1.148 [-100.000, 10.784], mean action: 1.640 [0.000, 3.000],  loss: 6888.925226, mse: 3814621.005814, mean_q: 2061.298283, mean_eps: 0.495194
 56342/100000: episode: 474, duration: 1.761s, episode steps: 166, steps per second:  94, episode reward: -180.195, mean reward: -1.086 [-100.000,  8.682], mean action: 1.916 [0.000, 3.000],  loss: 7434.590035, mse: 3787340.012048, mean_q: 2055.385313, mean_eps: 0.493674
 56533/100000: episode: 475, duration: 1.892s, episode steps: 191, steps per second: 101, episode reward: -40.547, mean reward: -0.212 [-100.000, 12.395], mean action: 1.785 [0.000, 3.000],  loss: 5634.107095, mse: 3826153.219895, mean_q: 2063.135350, mean_eps: 0.492067
 56838/100000: episode: 476, duration: 2.970s, episode steps: 305, steps per second: 103, episode reward: -366.043, mean reward: -1.200 [-100.000,  5.179], mean action: 1.770 [0.000, 3.000],  loss: 5973.671208, mse: 3922047.244262, mean_q: 2092.257394, mean_eps: 0.489835
 56978/100000: episode: 477, duration: 1.233s, episode steps: 140, steps per second: 114, episode reward: -245.313, mean reward: -1.752 [-100.000, 12.116], mean action: 1.736 [0.000, 3.000],  loss: 7261.550556, mse: 4034140.976786, mean_q: 2113.624506, mean_eps: 0.487832
 57145/100000: episode: 478, duration: 1.421s, episode steps: 167, steps per second: 117, episode reward: -41.673, mean reward: -0.250 [-100.000, 12.458], mean action: 1.665 [0.000, 3.000],  loss: 6308.133423, mse: 3970779.452096, mean_q: 2096.199527, mean_eps: 0.486451
 57369/100000: episode: 479, duration: 1.962s, episode steps: 224, steps per second: 114, episode reward: -237.452, mean reward: -1.060 [-100.000,  4.364], mean action: 1.732 [0.000, 3.000],  loss: 6933.904579, mse: 4107214.758929, mean_q: 2149.023484, mean_eps: 0.484691
 57570/100000: episode: 480, duration: 1.817s, episode steps: 201, steps per second: 111, episode reward: -193.969, mean reward: -0.965 [-100.000,  4.338], mean action: 1.796 [0.000, 3.000],  loss: 6510.088767, mse: 4200209.747512, mean_q: 2169.348047, mean_eps: 0.482779
 57717/100000: episode: 481, duration: 1.296s, episode steps: 147, steps per second: 113, episode reward: -98.416, mean reward: -0.669 [-100.000, 20.803], mean action: 1.823 [0.000, 3.000],  loss: 9965.576277, mse: 4264903.471088, mean_q: 2178.326771, mean_eps: 0.481213
 57849/100000: episode: 482, duration: 1.158s, episode steps: 132, steps per second: 114, episode reward: -82.751, mean reward: -0.627 [-100.000,  6.588], mean action: 1.674 [0.000, 3.000],  loss: 5634.943660, mse: 4261936.952652, mean_q: 2187.542406, mean_eps: 0.479957
 58034/100000: episode: 483, duration: 1.647s, episode steps: 185, steps per second: 112, episode reward: -160.683, mean reward: -0.869 [-100.000, 39.917], mean action: 1.665 [0.000, 3.000],  loss: 11038.365186, mse: 4454719.164865, mean_q: 2238.241928, mean_eps: 0.478531
 58141/100000: episode: 484, duration: 0.900s, episode steps: 107, steps per second: 119, episode reward: -304.043, mean reward: -2.842 [-100.000,  3.533], mean action: 1.336 [0.000, 3.000],  loss: 12809.562206, mse: 4399631.932243, mean_q: 2214.471810, mean_eps: 0.477217
 58280/100000: episode: 485, duration: 1.306s, episode steps: 139, steps per second: 106, episode reward: -119.046, mean reward: -0.856 [-100.000, 38.554], mean action: 1.791 [0.000, 3.000],  loss: 8507.812125, mse: 4468659.829137, mean_q: 2240.347092, mean_eps: 0.476110
 58398/100000: episode: 486, duration: 1.000s, episode steps: 118, steps per second: 118, episode reward: -224.289, mean reward: -1.901 [-100.000, 44.519], mean action: 1.669 [0.000, 3.000],  loss: 8770.508694, mse: 4589391.815678, mean_q: 2264.474843, mean_eps: 0.474954
 58494/100000: episode: 487, duration: 0.899s, episode steps:  96, steps per second: 107, episode reward: -289.422, mean reward: -3.015 [-100.000, 19.439], mean action: 1.677 [0.000, 3.000],  loss: 7141.213472, mse: 4586448.882812, mean_q: 2269.918814, mean_eps: 0.473990
 58642/100000: episode: 488, duration: 1.290s, episode steps: 148, steps per second: 115, episode reward: -147.445, mean reward: -0.996 [-100.000, 13.499], mean action: 1.784 [0.000, 3.000],  loss: 6256.441139, mse: 4672657.116554, mean_q: 2285.121211, mean_eps: 0.472892
 59145/100000: episode: 489, duration: 4.699s, episode steps: 503, steps per second: 107, episode reward: -316.751, mean reward: -0.630 [-100.000,  5.035], mean action: 1.704 [0.000, 3.000],  loss: 6465.907592, mse: 4814715.902087, mean_q: 2311.164082, mean_eps: 0.469963
 59260/100000: episode: 490, duration: 0.998s, episode steps: 115, steps per second: 115, episode reward: -158.763, mean reward: -1.381 [-100.000,  6.261], mean action: 1.678 [0.000, 3.000],  loss: 5596.968921, mse: 4949522.263043, mean_q: 2340.231260, mean_eps: 0.467182
 59438/100000: episode: 491, duration: 1.582s, episode steps: 178, steps per second: 113, episode reward: -163.941, mean reward: -0.921 [-100.000,  9.806], mean action: 1.753 [0.000, 3.000],  loss: 7397.041668, mse: 5083632.234551, mean_q: 2379.443985, mean_eps: 0.465863
 59542/100000: episode: 492, duration: 0.907s, episode steps: 104, steps per second: 115, episode reward: -174.231, mean reward: -1.675 [-100.000, 10.949], mean action: 1.692 [0.000, 3.000],  loss: 5517.905383, mse: 5119554.084135, mean_q: 2377.818373, mean_eps: 0.464594
 59665/100000: episode: 493, duration: 1.069s, episode steps: 123, steps per second: 115, episode reward: -271.936, mean reward: -2.211 [-100.000, 31.902], mean action: 1.618 [0.000, 3.000],  loss: 7848.219428, mse: 5170092.398374, mean_q: 2399.519566, mean_eps: 0.463573
 59935/100000: episode: 494, duration: 2.410s, episode steps: 270, steps per second: 112, episode reward: -197.499, mean reward: -0.731 [-100.000,  4.865], mean action: 1.726 [0.000, 3.000],  loss: 7526.706892, mse: 5339122.482407, mean_q: 2431.526988, mean_eps: 0.461805
 60092/100000: episode: 495, duration: 1.343s, episode steps: 157, steps per second: 117, episode reward: -207.254, mean reward: -1.320 [-100.000,  3.360], mean action: 1.860 [0.000, 3.000],  loss: 7172.180358, mse: 5622576.656051, mean_q: 2505.895593, mean_eps: 0.459883
 60209/100000: episode: 496, duration: 1.134s, episode steps: 117, steps per second: 103, episode reward: -104.945, mean reward: -0.897 [-100.000, 27.323], mean action: 1.684 [0.000, 3.000],  loss: 5951.034176, mse: 5892300.619658, mean_q: 2557.461739, mean_eps: 0.458650
 60322/100000: episode: 497, duration: 0.965s, episode steps: 113, steps per second: 117, episode reward: -147.129, mean reward: -1.302 [-100.000, 10.976], mean action: 1.779 [0.000, 3.000],  loss: 10700.126994, mse: 5851987.849558, mean_q: 2557.880836, mean_eps: 0.457615
 60606/100000: episode: 498, duration: 2.664s, episode steps: 284, steps per second: 107, episode reward: -217.160, mean reward: -0.765 [-100.000, 59.155], mean action: 1.683 [0.000, 3.000],  loss: 7164.543724, mse: 6045334.044014, mean_q: 2607.014426, mean_eps: 0.455828
 60723/100000: episode: 499, duration: 1.029s, episode steps: 117, steps per second: 114, episode reward: -41.861, mean reward: -0.358 [-100.000, 36.575], mean action: 1.795 [0.000, 3.000],  loss: 8705.849464, mse: 6338513.615385, mean_q: 2667.409741, mean_eps: 0.454024
 61498/100000: episode: 500, duration: 7.747s, episode steps: 775, steps per second: 100, episode reward: -221.012, mean reward: -0.285 [-100.000, 11.512], mean action: 1.861 [0.000, 3.000],  loss: 8159.093083, mse: 6669090.145161, mean_q: 2737.272626, mean_eps: 0.450010
 61621/100000: episode: 501, duration: 1.123s, episode steps: 123, steps per second: 110, episode reward: -109.688, mean reward: -0.892 [-100.000,  8.805], mean action: 1.780 [0.000, 3.000],  loss: 8580.688576, mse: 7291519.975610, mean_q: 2868.099343, mean_eps: 0.445969
 61846/100000: episode: 502, duration: 1.988s, episode steps: 225, steps per second: 113, episode reward: -143.921, mean reward: -0.640 [-100.000,  4.501], mean action: 1.769 [0.000, 3.000],  loss: 7695.330595, mse: 7286960.202222, mean_q: 2865.153062, mean_eps: 0.444403
 61938/100000: episode: 503, duration: 0.784s, episode steps:  92, steps per second: 117, episode reward: -154.022, mean reward: -1.674 [-100.000, 31.901], mean action: 1.859 [0.000, 3.000],  loss: 10246.364403, mse: 7369183.385870, mean_q: 2882.969883, mean_eps: 0.442976
 62225/100000: episode: 504, duration: 2.601s, episode steps: 287, steps per second: 110, episode reward: -197.738, mean reward: -0.689 [-100.000,  6.658], mean action: 1.850 [0.000, 3.000],  loss: 10040.924922, mse: 7701159.604530, mean_q: 2949.450752, mean_eps: 0.441271
 62362/100000: episode: 505, duration: 1.267s, episode steps: 137, steps per second: 108, episode reward: -147.594, mean reward: -1.077 [-100.000,  8.111], mean action: 1.730 [0.000, 3.000],  loss: 10993.906013, mse: 8123325.149635, mean_q: 3035.598938, mean_eps: 0.439363
 62473/100000: episode: 506, duration: 1.046s, episode steps: 111, steps per second: 106, episode reward: -209.817, mean reward: -1.890 [-100.000, 27.314], mean action: 1.775 [0.000, 3.000],  loss: 7458.760188, mse: 8170848.981982, mean_q: 3035.615461, mean_eps: 0.438247
 63134/100000: episode: 507, duration: 6.950s, episode steps: 661, steps per second:  95, episode reward: -391.086, mean reward: -0.592 [-100.000, 16.016], mean action: 1.826 [0.000, 3.000],  loss: 8753.751605, mse: 8481305.866868, mean_q: 3089.105768, mean_eps: 0.434773
 63843/100000: episode: 508, duration: 6.962s, episode steps: 709, steps per second: 102, episode reward: -178.502, mean reward: -0.252 [-100.000, 31.453], mean action: 1.718 [0.000, 3.000],  loss: 9615.207524, mse: 9239607.356841, mean_q: 3226.499499, mean_eps: 0.428608
 64073/100000: episode: 509, duration: 2.149s, episode steps: 230, steps per second: 107, episode reward: -183.775, mean reward: -0.799 [-100.000, 12.703], mean action: 1.674 [0.000, 3.000],  loss: 8882.505520, mse: 9874938.326087, mean_q: 3341.017110, mean_eps: 0.424382
 64219/100000: episode: 510, duration: 1.066s, episode steps: 146, steps per second: 137, episode reward: -39.645, mean reward: -0.272 [-100.000, 18.971], mean action: 1.637 [0.000, 3.000],  loss: 12498.636358, mse: 9883198.993151, mean_q: 3342.156021, mean_eps: 0.422690
 64661/100000: episode: 511, duration: 3.341s, episode steps: 442, steps per second: 132, episode reward: -182.672, mean reward: -0.413 [-100.000, 18.328], mean action: 1.722 [0.000, 3.000],  loss: 11614.674160, mse: 10387594.427602, mean_q: 3439.825348, mean_eps: 0.420045
 65005/100000: episode: 512, duration: 2.494s, episode steps: 344, steps per second: 138, episode reward: -73.528, mean reward: -0.214 [-100.000, 22.526], mean action: 1.744 [0.000, 3.000],  loss: 11578.907625, mse: 10885436.843023, mean_q: 3522.243228, mean_eps: 0.416507
 65140/100000: episode: 513, duration: 0.982s, episode steps: 135, steps per second: 137, episode reward: -138.414, mean reward: -1.025 [-100.000,  9.042], mean action: 1.689 [0.000, 3.000],  loss: 13592.890896, mse: 11364480.762963, mean_q: 3610.573513, mean_eps: 0.414352
 65614/100000: episode: 514, duration: 3.500s, episode steps: 474, steps per second: 135, episode reward: -211.532, mean reward: -0.446 [-100.000, 11.906], mean action: 1.764 [0.000, 3.000],  loss: 13901.325343, mse: 11714340.943038, mean_q: 3658.446428, mean_eps: 0.411611
 66003/100000: episode: 515, duration: 2.888s, episode steps: 389, steps per second: 135, episode reward: -174.365, mean reward: -0.448 [-100.000, 11.861], mean action: 1.815 [0.000, 3.000],  loss: 10300.497585, mse: 12162267.143959, mean_q: 3732.163636, mean_eps: 0.407728
 66555/100000: episode: 516, duration: 4.256s, episode steps: 552, steps per second: 130, episode reward: -244.909, mean reward: -0.444 [-100.000, 22.336], mean action: 1.710 [0.000, 3.000],  loss: 13499.847917, mse: 12190001.494565, mean_q: 3732.128264, mean_eps: 0.403494
 66711/100000: episode: 517, duration: 1.101s, episode steps: 156, steps per second: 142, episode reward: -43.673, mean reward: -0.280 [-100.000, 22.269], mean action: 1.769 [0.000, 3.000],  loss: 16208.813528, mse: 12264155.826923, mean_q: 3725.688887, mean_eps: 0.400307
 66811/100000: episode: 518, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: -49.263, mean reward: -0.493 [-100.000,  9.875], mean action: 1.740 [0.000, 3.000],  loss: 9436.275479, mse: 12515200.630000, mean_q: 3776.529292, mean_eps: 0.399155
 66993/100000: episode: 519, duration: 1.321s, episode steps: 182, steps per second: 138, episode reward: -127.574, mean reward: -0.701 [-100.000,  8.365], mean action: 1.830 [0.000, 3.000],  loss: 12576.030525, mse: 12414633.912088, mean_q: 3746.845538, mean_eps: 0.397887
 67993/100000: episode: 520, duration: 7.862s, episode steps: 1000, steps per second: 127, episode reward: 34.707, mean reward:  0.035 [-24.701, 28.992], mean action: 1.675 [0.000, 3.000],  loss: 12364.685351, mse: 13215116.145000, mean_q: 3884.472379, mean_eps: 0.392568
 68993/100000: episode: 521, duration: 8.044s, episode steps: 1000, steps per second: 124, episode reward: -8.124, mean reward: -0.008 [-24.063, 23.793], mean action: 1.662 [0.000, 3.000],  loss: 14035.014509, mse: 14097101.015000, mean_q: 4021.335688, mean_eps: 0.383568
 69993/100000: episode: 522, duration: 9.103s, episode steps: 1000, steps per second: 110, episode reward: 63.140, mean reward:  0.063 [-23.546, 28.164], mean action: 1.692 [0.000, 3.000],  loss: 14297.704310, mse: 15192523.983000, mean_q: 4206.405257, mean_eps: 0.374567
 70131/100000: episode: 523, duration: 0.981s, episode steps: 138, steps per second: 141, episode reward: -70.435, mean reward: -0.510 [-100.000, 17.576], mean action: 1.696 [0.000, 3.000],  loss: 16055.152723, mse: 15543652.797101, mean_q: 4268.614720, mean_eps: 0.369446
 71131/100000: episode: 524, duration: 7.843s, episode steps: 1000, steps per second: 128, episode reward: 40.991, mean reward:  0.041 [-25.544, 25.623], mean action: 1.646 [0.000, 3.000],  loss: 15490.439422, mse: 15817779.328000, mean_q: 4311.652555, mean_eps: 0.364325
 71394/100000: episode: 525, duration: 1.842s, episode steps: 263, steps per second: 143, episode reward: -126.752, mean reward: -0.482 [-100.000, 12.709], mean action: 1.806 [0.000, 3.000],  loss: 15207.503363, mse: 16147048.752852, mean_q: 4350.295162, mean_eps: 0.358642
 71525/100000: episode: 526, duration: 1.117s, episode steps: 131, steps per second: 117, episode reward: -74.039, mean reward: -0.565 [-100.000,  9.482], mean action: 1.771 [0.000, 3.000],  loss: 12870.139349, mse: 16343921.442748, mean_q: 4380.278958, mean_eps: 0.356869
 72525/100000: episode: 527, duration: 9.192s, episode steps: 1000, steps per second: 109, episode reward: -20.336, mean reward: -0.020 [-24.719, 25.168], mean action: 1.445 [0.000, 3.000],  loss: 13225.986428, mse: 16602760.203000, mean_q: 4409.431506, mean_eps: 0.351779
 72954/100000: episode: 528, duration: 3.148s, episode steps: 429, steps per second: 136, episode reward: -281.224, mean reward: -0.656 [-100.000, 17.510], mean action: 1.825 [0.000, 3.000],  loss: 14643.760866, mse: 17207872.799534, mean_q: 4487.184397, mean_eps: 0.345349
 73253/100000: episode: 529, duration: 2.511s, episode steps: 299, steps per second: 119, episode reward: -266.259, mean reward: -0.890 [-100.000, 22.423], mean action: 1.676 [0.000, 3.000],  loss: 14039.243563, mse: 17617359.966555, mean_q: 4553.451562, mean_eps: 0.342073
 74253/100000: episode: 530, duration: 9.150s, episode steps: 1000, steps per second: 109, episode reward: -157.871, mean reward: -0.158 [-23.942, 17.006], mean action: 1.646 [0.000, 3.000],  loss: 16994.447226, mse: 17753085.447000, mean_q: 4573.262385, mean_eps: 0.336228
 75253/100000: episode: 531, duration: 8.220s, episode steps: 1000, steps per second: 122, episode reward:  9.252, mean reward:  0.009 [-23.227, 25.081], mean action: 1.834 [0.000, 3.000],  loss: 12866.124318, mse: 18729617.490000, mean_q: 4699.148158, mean_eps: 0.327227
 76253/100000: episode: 532, duration: 8.117s, episode steps: 1000, steps per second: 123, episode reward: -56.330, mean reward: -0.056 [-24.012, 23.502], mean action: 1.707 [0.000, 3.000],  loss: 17367.847071, mse: 20514136.767000, mean_q: 4928.045899, mean_eps: 0.318227
 77253/100000: episode: 533, duration: 8.022s, episode steps: 1000, steps per second: 125, episode reward: -45.977, mean reward: -0.046 [-23.408, 18.139], mean action: 1.824 [0.000, 3.000],  loss: 14759.235445, mse: 21261735.140000, mean_q: 5019.490525, mean_eps: 0.309227
 77606/100000: episode: 534, duration: 2.658s, episode steps: 353, steps per second: 133, episode reward: -257.566, mean reward: -0.730 [-100.000,  4.785], mean action: 1.745 [0.000, 3.000],  loss: 15082.537735, mse: 20941185.804533, mean_q: 4974.786682, mean_eps: 0.303139
 77825/100000: episode: 535, duration: 1.586s, episode steps: 219, steps per second: 138, episode reward: -227.733, mean reward: -1.040 [-100.000,  5.875], mean action: 1.744 [0.000, 3.000],  loss: 12001.620492, mse: 20726896.881279, mean_q: 4948.164301, mean_eps: 0.300565
 77973/100000: episode: 536, duration: 1.080s, episode steps: 148, steps per second: 137, episode reward: -193.031, mean reward: -1.304 [-100.000,  3.145], mean action: 1.912 [0.000, 3.000],  loss: 9087.043780, mse: 20893551.344595, mean_q: 4975.948618, mean_eps: 0.298913
 78501/100000: episode: 537, duration: 3.960s, episode steps: 528, steps per second: 133, episode reward: -211.834, mean reward: -0.401 [-100.000, 20.123], mean action: 1.968 [0.000, 3.000],  loss: 15200.067622, mse: 20852884.174242, mean_q: 4952.712087, mean_eps: 0.295871
 79501/100000: episode: 538, duration: 8.727s, episode steps: 1000, steps per second: 115, episode reward: -28.057, mean reward: -0.028 [-23.712, 25.321], mean action: 1.636 [0.000, 3.000],  loss: 14664.749879, mse: 20139625.572000, mean_q: 4866.771374, mean_eps: 0.288996
 79913/100000: episode: 539, duration: 3.510s, episode steps: 412, steps per second: 117, episode reward: -309.957, mean reward: -0.752 [-100.000, 11.231], mean action: 1.672 [0.000, 3.000],  loss: 17602.368737, mse: 19461964.070388, mean_q: 4779.861037, mean_eps: 0.282641
 80636/100000: episode: 540, duration: 5.581s, episode steps: 723, steps per second: 130, episode reward: -266.434, mean reward: -0.369 [-100.000, 15.813], mean action: 1.776 [0.000, 3.000],  loss: 12954.503744, mse: 19198824.390041, mean_q: 4758.956196, mean_eps: 0.277534
 81636/100000: episode: 541, duration: 8.495s, episode steps: 1000, steps per second: 118, episode reward: 22.829, mean reward:  0.023 [-25.070, 27.041], mean action: 1.484 [0.000, 3.000],  loss: 13916.673510, mse: 18941620.098000, mean_q: 4735.758954, mean_eps: 0.269780
 82611/100000: episode: 542, duration: 7.434s, episode steps: 975, steps per second: 131, episode reward: -375.751, mean reward: -0.385 [-100.000, 12.847], mean action: 1.724 [0.000, 3.000],  loss: 13744.008902, mse: 19061697.081026, mean_q: 4773.460036, mean_eps: 0.260893
 83611/100000: episode: 543, duration: 8.542s, episode steps: 1000, steps per second: 117, episode reward: -51.638, mean reward: -0.052 [-22.455, 24.567], mean action: 1.840 [0.000, 3.000],  loss: 13447.124292, mse: 18513423.496000, mean_q: 4703.900878, mean_eps: 0.252005
 84478/100000: episode: 544, duration: 6.796s, episode steps: 867, steps per second: 128, episode reward: 102.183, mean reward:  0.118 [-25.057, 100.000], mean action: 2.078 [0.000, 3.000],  loss: 13105.412097, mse: 18170931.914648, mean_q: 4672.070837, mean_eps: 0.243604
 85478/100000: episode: 545, duration: 8.581s, episode steps: 1000, steps per second: 117, episode reward:  3.771, mean reward:  0.004 [-23.847, 25.720], mean action: 1.603 [0.000, 3.000],  loss: 12665.699814, mse: 18034381.693000, mean_q: 4662.884842, mean_eps: 0.235202
 86478/100000: episode: 546, duration: 7.942s, episode steps: 1000, steps per second: 126, episode reward: 38.124, mean reward:  0.038 [-24.321, 25.314], mean action: 1.310 [0.000, 3.000],  loss: 13182.002604, mse: 17820247.332000, mean_q: 4651.889288, mean_eps: 0.226202
 87478/100000: episode: 547, duration: 8.159s, episode steps: 1000, steps per second: 123, episode reward: -82.569, mean reward: -0.083 [-22.142, 24.112], mean action: 1.649 [0.000, 3.000],  loss: 12802.887989, mse: 16881827.555000, mean_q: 4553.743675, mean_eps: 0.217202
 87891/100000: episode: 548, duration: 3.471s, episode steps: 413, steps per second: 119, episode reward: -70.131, mean reward: -0.170 [-100.000, 12.244], mean action: 1.724 [0.000, 3.000],  loss: 10467.741853, mse: 16469584.835351, mean_q: 4509.478519, mean_eps: 0.210844
 88760/100000: episode: 549, duration: 6.852s, episode steps: 869, steps per second: 127, episode reward: -367.159, mean reward: -0.423 [-100.000, 22.441], mean action: 1.611 [0.000, 3.000],  loss: 13573.953449, mse: 16169397.588032, mean_q: 4481.274884, mean_eps: 0.205075
 89760/100000: episode: 550, duration: 7.675s, episode steps: 1000, steps per second: 130, episode reward: -23.122, mean reward: -0.023 [-21.894, 24.090], mean action: 1.854 [0.000, 3.000],  loss: 10921.811189, mse: 15858345.348000, mean_q: 4449.678488, mean_eps: 0.196664
 90073/100000: episode: 551, duration: 2.251s, episode steps: 313, steps per second: 139, episode reward: -925.147, mean reward: -2.956 [-100.000,  6.937], mean action: 1.942 [0.000, 3.000],  loss: 9195.953247, mse: 15723900.686901, mean_q: 4433.158358, mean_eps: 0.190756
 90220/100000: episode: 552, duration: 1.030s, episode steps: 147, steps per second: 143, episode reward: -249.025, mean reward: -1.694 [-100.000,  3.308], mean action: 1.884 [0.000, 3.000],  loss: 8784.141257, mse: 15638429.911565, mean_q: 4419.355842, mean_eps: 0.188686
 90355/100000: episode: 553, duration: 0.992s, episode steps: 135, steps per second: 136, episode reward: -250.146, mean reward: -1.853 [-100.000,  2.471], mean action: 1.926 [0.000, 3.000],  loss: 6321.358814, mse: 15706061.970370, mean_q: 4424.493741, mean_eps: 0.187417
 91355/100000: episode: 554, duration: 7.921s, episode steps: 1000, steps per second: 126, episode reward: 30.302, mean reward:  0.030 [-22.696, 29.183], mean action: 1.926 [0.000, 3.000],  loss: 12563.501447, mse: 15845089.343000, mean_q: 4444.195791, mean_eps: 0.182309
 92355/100000: episode: 555, duration: 7.677s, episode steps: 1000, steps per second: 130, episode reward: 16.709, mean reward:  0.017 [-22.506, 24.616], mean action: 1.699 [0.000, 3.000],  loss: 11455.152982, mse: 15903659.593000, mean_q: 4441.017437, mean_eps: 0.173309
 92514/100000: episode: 556, duration: 1.121s, episode steps: 159, steps per second: 142, episode reward: -283.974, mean reward: -1.786 [-100.000,  5.907], mean action: 1.893 [0.000, 3.000],  loss: 8983.649947, mse: 15551942.484277, mean_q: 4383.870145, mean_eps: 0.168094
 92634/100000: episode: 557, duration: 0.861s, episode steps: 120, steps per second: 139, episode reward: -208.606, mean reward: -1.738 [-100.000,  2.176], mean action: 1.917 [0.000, 3.000],  loss: 5572.535341, mse: 15786627.333333, mean_q: 4445.235992, mean_eps: 0.166838
 92784/100000: episode: 558, duration: 1.069s, episode steps: 150, steps per second: 140, episode reward: -241.780, mean reward: -1.612 [-100.000,  3.329], mean action: 1.900 [0.000, 3.000],  loss: 9806.580684, mse: 15378077.373333, mean_q: 4366.056229, mean_eps: 0.165624
 92943/100000: episode: 559, duration: 1.164s, episode steps: 159, steps per second: 137, episode reward: -181.150, mean reward: -1.139 [-100.000,  4.883], mean action: 1.893 [0.000, 3.000],  loss: 6913.550383, mse: 15647033.050314, mean_q: 4397.140217, mean_eps: 0.164233
 93067/100000: episode: 560, duration: 0.860s, episode steps: 124, steps per second: 144, episode reward: -105.398, mean reward: -0.850 [-100.000,  2.511], mean action: 1.823 [0.000, 3.000],  loss: 9498.734733, mse: 15417942.677419, mean_q: 4349.341492, mean_eps: 0.162960
 93221/100000: episode: 561, duration: 1.161s, episode steps: 154, steps per second: 133, episode reward: -262.130, mean reward: -1.702 [-100.000,  3.110], mean action: 1.799 [0.000, 3.000],  loss: 10899.733016, mse: 15519859.577922, mean_q: 4384.632914, mean_eps: 0.161708
 93400/100000: episode: 562, duration: 1.247s, episode steps: 179, steps per second: 143, episode reward: -151.469, mean reward: -0.846 [-100.000,  4.566], mean action: 1.894 [0.000, 3.000],  loss: 6165.208684, mse: 15544979.039106, mean_q: 4379.138139, mean_eps: 0.160210
 93889/100000: episode: 563, duration: 4.080s, episode steps: 489, steps per second: 120, episode reward: -474.821, mean reward: -0.971 [-100.000,  4.852], mean action: 1.888 [0.000, 3.000],  loss: 8965.883978, mse: 15195340.200409, mean_q: 4328.728424, mean_eps: 0.157204
 94035/100000: episode: 564, duration: 1.134s, episode steps: 146, steps per second: 129, episode reward: -158.760, mean reward: -1.087 [-100.000,  4.746], mean action: 1.877 [0.000, 3.000],  loss: 5846.555264, mse: 15235770.061644, mean_q: 4336.032531, mean_eps: 0.154346
 94171/100000: episode: 565, duration: 0.946s, episode steps: 136, steps per second: 144, episode reward: -225.418, mean reward: -1.657 [-100.000,  5.360], mean action: 1.926 [0.000, 3.000],  loss: 9178.987423, mse: 15312674.014706, mean_q: 4352.584306, mean_eps: 0.153077
 94301/100000: episode: 566, duration: 0.975s, episode steps: 130, steps per second: 133, episode reward: -110.605, mean reward: -0.851 [-100.000,  2.929], mean action: 1.685 [0.000, 3.000],  loss: 8245.965288, mse: 15341680.161538, mean_q: 4357.279276, mean_eps: 0.151880
 94424/100000: episode: 567, duration: 0.862s, episode steps: 123, steps per second: 143, episode reward: -115.734, mean reward: -0.941 [-100.000,  2.390], mean action: 1.919 [0.000, 3.000],  loss: 8745.036480, mse: 15041867.000000, mean_q: 4300.328403, mean_eps: 0.150742
 94554/100000: episode: 568, duration: 0.922s, episode steps: 130, steps per second: 141, episode reward: -174.210, mean reward: -1.340 [-100.000,  2.779], mean action: 1.785 [0.000, 3.000],  loss: 12402.131197, mse: 15303397.930769, mean_q: 4348.470153, mean_eps: 0.149603
 94675/100000: episode: 569, duration: 0.873s, episode steps: 121, steps per second: 139, episode reward: -156.149, mean reward: -1.290 [-100.000,  2.872], mean action: 1.860 [0.000, 3.000],  loss: 6538.005122, mse: 15405012.165289, mean_q: 4354.149283, mean_eps: 0.148474
 94792/100000: episode: 570, duration: 0.811s, episode steps: 117, steps per second: 144, episode reward: -224.155, mean reward: -1.916 [-100.000,  5.486], mean action: 1.983 [0.000, 3.000],  loss: 5914.345575, mse: 15395506.564103, mean_q: 4352.060025, mean_eps: 0.147403
 94941/100000: episode: 571, duration: 1.082s, episode steps: 149, steps per second: 138, episode reward: -192.095, mean reward: -1.289 [-100.000,  5.001], mean action: 1.960 [0.000, 3.000],  loss: 8443.766472, mse: 15290555.932886, mean_q: 4337.697962, mean_eps: 0.146206
 95098/100000: episode: 572, duration: 1.148s, episode steps: 157, steps per second: 137, episode reward: -235.913, mean reward: -1.503 [-100.000,  3.182], mean action: 1.943 [0.000, 3.000],  loss: 5253.978049, mse: 15226982.229299, mean_q: 4334.353152, mean_eps: 0.144829
 95497/100000: episode: 573, duration: 3.138s, episode steps: 399, steps per second: 127, episode reward: -255.361, mean reward: -0.640 [-100.000, 10.833], mean action: 1.845 [0.000, 3.000],  loss: 10284.949507, mse: 15328104.716792, mean_q: 4347.595909, mean_eps: 0.142327
 96497/100000: episode: 574, duration: 8.799s, episode steps: 1000, steps per second: 114, episode reward: -212.747, mean reward: -0.213 [-7.683,  5.342], mean action: 1.829 [0.000, 3.000],  loss: 8827.993786, mse: 14971531.477000, mean_q: 4298.465516, mean_eps: 0.136031
 96704/100000: episode: 575, duration: 1.489s, episode steps: 207, steps per second: 139, episode reward: -159.811, mean reward: -0.772 [-100.000,  3.565], mean action: 1.831 [0.000, 3.000],  loss: 8806.652534, mse: 14504500.565217, mean_q: 4206.373737, mean_eps: 0.130600
 96866/100000: episode: 576, duration: 1.158s, episode steps: 162, steps per second: 140, episode reward: -222.797, mean reward: -1.375 [-100.000,  3.623], mean action: 1.895 [0.000, 3.000],  loss: 12848.937679, mse: 14474279.197531, mean_q: 4204.485092, mean_eps: 0.128940
 97652/100000: episode: 577, duration: 5.808s, episode steps: 786, steps per second: 135, episode reward: -800.394, mean reward: -1.018 [-100.000,  3.998], mean action: 1.838 [0.000, 3.000],  loss: 7959.518785, mse: 14298982.988550, mean_q: 4175.195687, mean_eps: 0.124673
 98652/100000: episode: 578, duration: 8.349s, episode steps: 1000, steps per second: 120, episode reward: -450.223, mean reward: -0.450 [-8.139,  3.994], mean action: 1.831 [0.000, 3.000],  loss: 8308.764710, mse: 14405410.337000, mean_q: 4184.174461, mean_eps: 0.116636
 99512/100000: episode: 579, duration: 6.816s, episode steps: 860, steps per second: 126, episode reward: -577.844, mean reward: -0.672 [-100.000,  5.050], mean action: 1.687 [0.000, 3.000],  loss: 9660.089424, mse: 14589154.106977, mean_q: 4214.139955, mean_eps: 0.108266
done, took 795.056 seconds
Testing for 5 episodes ...
Episode 1: reward: -669.028, steps: 740
Episode 2: reward: -745.638, steps: 738
Episode 3: reward: -1017.347, steps: 719
Episode 4: reward: -988.325, steps: 806
Episode 5: reward: -600.120, steps: 652
Training for 100000 steps ...
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    63/100000: episode: 1, duration: 1.173s, episode steps:  63, steps per second:  54, episode reward: -86.227, mean reward: -1.369 [-100.000,  7.761], mean action: 1.254 [0.000, 3.000],  loss: 10358.673215, mse: 13912782.711538, mean_q: 4099.077529, mean_eps: 0.999672
   126/100000: episode: 2, duration: 0.496s, episode steps:  63, steps per second: 127, episode reward: -84.414, mean reward: -1.340 [-100.000, 14.671], mean action: 1.270 [0.000, 3.000],  loss: 9019.863146, mse: 13752065.111111, mean_q: 4075.513180, mean_eps: 0.999154
   217/100000: episode: 3, duration: 0.706s, episode steps:  91, steps per second: 129, episode reward: -145.276, mean reward: -1.596 [-100.000,  7.728], mean action: 1.571 [0.000, 3.000],  loss: 11207.281811, mse: 13607228.285714, mean_q: 4051.837378, mean_eps: 0.998461
   320/100000: episode: 4, duration: 0.782s, episode steps: 103, steps per second: 132, episode reward: -48.735, mean reward: -0.473 [-100.000, 93.149], mean action: 1.359 [0.000, 3.000],  loss: 7350.126316, mse: 13299873.980583, mean_q: 4002.852120, mean_eps: 0.997588
   391/100000: episode: 5, duration: 0.552s, episode steps:  71, steps per second: 129, episode reward: -92.121, mean reward: -1.297 [-100.000, 21.829], mean action: 1.366 [0.000, 3.000],  loss: 11212.741780, mse: 13315475.323944, mean_q: 4012.831099, mean_eps: 0.996805
   487/100000: episode: 6, duration: 0.734s, episode steps:  96, steps per second: 131, episode reward: -184.508, mean reward: -1.922 [-100.000,  7.180], mean action: 1.594 [0.000, 3.000],  loss: 8565.752357, mse: 13226142.104167, mean_q: 3997.074259, mean_eps: 0.996054
   554/100000: episode: 7, duration: 0.534s, episode steps:  67, steps per second: 125, episode reward: -136.551, mean reward: -2.038 [-100.000,  6.771], mean action: 1.463 [0.000, 3.000],  loss: 7723.402011, mse: 12797113.373134, mean_q: 3915.124672, mean_eps: 0.995320
   655/100000: episode: 8, duration: 0.750s, episode steps: 101, steps per second: 135, episode reward: -427.458, mean reward: -4.232 [-100.000, 13.434], mean action: 1.535 [0.000, 3.000],  loss: 6105.591056, mse: 12562337.980198, mean_q: 3875.196379, mean_eps: 0.994564
   760/100000: episode: 9, duration: 0.770s, episode steps: 105, steps per second: 136, episode reward: -409.603, mean reward: -3.901 [-100.000,  4.445], mean action: 1.352 [0.000, 3.000],  loss: 14233.142977, mse: 12464832.380952, mean_q: 3861.456769, mean_eps: 0.993637
   857/100000: episode: 10, duration: 0.739s, episode steps:  97, steps per second: 131, episode reward: -367.843, mean reward: -3.792 [-100.000, 107.806], mean action: 1.505 [0.000, 3.000],  loss: 6306.978104, mse: 12314153.927835, mean_q: 3849.458813, mean_eps: 0.992728
   953/100000: episode: 11, duration: 0.696s, episode steps:  96, steps per second: 138, episode reward: -344.545, mean reward: -3.589 [-100.000,  3.513], mean action: 1.427 [0.000, 3.000],  loss: 7450.639039, mse: 12082223.291667, mean_q: 3792.563934, mean_eps: 0.991860
  1056/100000: episode: 12, duration: 0.880s, episode steps: 103, steps per second: 117, episode reward: -121.163, mean reward: -1.176 [-100.000,  6.976], mean action: 1.466 [0.000, 3.000],  loss: 11359.509319, mse: 11859706.883495, mean_q: 3765.835338, mean_eps: 0.990964
  1158/100000: episode: 13, duration: 1.023s, episode steps: 102, steps per second: 100, episode reward: -382.462, mean reward: -3.750 [-100.000,  1.585], mean action: 1.578 [0.000, 3.000],  loss: 7685.797662, mse: 11615637.725490, mean_q: 3717.195200, mean_eps: 0.990042
  1268/100000: episode: 14, duration: 0.864s, episode steps: 110, steps per second: 127, episode reward: -113.686, mean reward: -1.034 [-100.000, 17.242], mean action: 1.591 [0.000, 3.000],  loss: 7696.183065, mse: 11520098.272727, mean_q: 3711.547579, mean_eps: 0.989087
  1347/100000: episode: 15, duration: 0.580s, episode steps:  79, steps per second: 136, episode reward: -372.415, mean reward: -4.714 [-100.000,  5.655], mean action: 1.582 [0.000, 3.000],  loss: 6062.634084, mse: 11626944.000000, mean_q: 3733.333691, mean_eps: 0.988237
  1466/100000: episode: 16, duration: 0.936s, episode steps: 119, steps per second: 127, episode reward: -62.813, mean reward: -0.528 [-100.000,  6.161], mean action: 1.546 [0.000, 3.000],  loss: 2799.018078, mse: 11315754.563025, mean_q: 3670.230684, mean_eps: 0.987346
  1543/100000: episode: 17, duration: 0.704s, episode steps:  77, steps per second: 109, episode reward: -162.197, mean reward: -2.106 [-100.000,  8.471], mean action: 1.649 [0.000, 3.000],  loss: 8737.492476, mse: 11122979.090909, mean_q: 3633.162893, mean_eps: 0.986464
  1615/100000: episode: 18, duration: 0.536s, episode steps:  72, steps per second: 134, episode reward: -91.361, mean reward: -1.269 [-100.000, 12.591], mean action: 1.319 [0.000, 3.000],  loss: 11802.956757, mse: 10962316.361111, mean_q: 3612.809130, mean_eps: 0.985794
  1708/100000: episode: 19, duration: 0.800s, episode steps:  93, steps per second: 116, episode reward: -268.754, mean reward: -2.890 [-100.000, 19.981], mean action: 1.742 [0.000, 3.000],  loss: 10704.681966, mse: 10895314.107527, mean_q: 3605.058368, mean_eps: 0.985051
  1800/100000: episode: 20, duration: 0.894s, episode steps:  92, steps per second: 103, episode reward: -108.643, mean reward: -1.181 [-100.000,  6.327], mean action: 1.446 [0.000, 3.000],  loss: 9205.275925, mse: 10612689.989130, mean_q: 3549.309488, mean_eps: 0.984218
  1920/100000: episode: 21, duration: 1.004s, episode steps: 120, steps per second: 119, episode reward: -104.149, mean reward: -0.868 [-100.000,  8.420], mean action: 1.475 [0.000, 3.000],  loss: 6133.343178, mse: 10544860.716667, mean_q: 3546.437600, mean_eps: 0.983264
  2003/100000: episode: 22, duration: 0.690s, episode steps:  83, steps per second: 120, episode reward: -111.055, mean reward: -1.338 [-100.000, 13.282], mean action: 1.518 [0.000, 3.000],  loss: 3727.914282, mse: 10351634.481928, mean_q: 3507.314562, mean_eps: 0.982351
  2099/100000: episode: 23, duration: 0.740s, episode steps:  96, steps per second: 130, episode reward: -227.999, mean reward: -2.375 [-100.000, 18.786], mean action: 1.427 [0.000, 3.000],  loss: 6265.846476, mse: 10322325.817708, mean_q: 3503.040387, mean_eps: 0.981546
  2177/100000: episode: 24, duration: 0.610s, episode steps:  78, steps per second: 128, episode reward: -102.704, mean reward: -1.317 [-100.000, 16.421], mean action: 1.372 [0.000, 3.000],  loss: 6433.024590, mse: 10181836.724359, mean_q: 3478.033428, mean_eps: 0.980763
  2270/100000: episode: 25, duration: 0.751s, episode steps:  93, steps per second: 124, episode reward: -184.354, mean reward: -1.982 [-100.000, 29.826], mean action: 1.398 [0.000, 3.000],  loss: 6186.546404, mse: 10146048.000000, mean_q: 3464.408988, mean_eps: 0.979993
  2355/100000: episode: 26, duration: 0.671s, episode steps:  85, steps per second: 127, episode reward: -72.791, mean reward: -0.856 [-100.000, 17.086], mean action: 1.776 [0.000, 3.000],  loss: 3459.472886, mse: 10347218.023529, mean_q: 3513.368583, mean_eps: 0.979192
  2464/100000: episode: 27, duration: 0.857s, episode steps: 109, steps per second: 127, episode reward: -457.745, mean reward: -4.199 [-100.000, 63.839], mean action: 1.560 [0.000, 3.000],  loss: 4737.166389, mse: 9961304.779817, mean_q: 3430.072761, mean_eps: 0.978319
  2543/100000: episode: 28, duration: 0.590s, episode steps:  79, steps per second: 134, episode reward: -87.852, mean reward: -1.112 [-100.000,  9.185], mean action: 1.658 [0.000, 3.000],  loss: 4970.575093, mse: 9925960.753165, mean_q: 3415.747138, mean_eps: 0.977473
  2657/100000: episode: 29, duration: 0.843s, episode steps: 114, steps per second: 135, episode reward: -169.853, mean reward: -1.490 [-100.000,  6.153], mean action: 1.430 [0.000, 3.000],  loss: 7590.285450, mse: 10066990.166667, mean_q: 3459.522859, mean_eps: 0.976605
  2755/100000: episode: 30, duration: 0.743s, episode steps:  98, steps per second: 132, episode reward: -303.587, mean reward: -3.098 [-100.000, 103.612], mean action: 1.500 [0.000, 3.000],  loss: 6898.537575, mse: 9753618.877551, mean_q: 3396.339772, mean_eps: 0.975650
  2835/100000: episode: 31, duration: 0.645s, episode steps:  80, steps per second: 124, episode reward: -255.020, mean reward: -3.188 [-100.000, 11.953], mean action: 1.462 [0.000, 3.000],  loss: 9686.106227, mse: 9706487.581250, mean_q: 3387.259576, mean_eps: 0.974850
  2943/100000: episode: 32, duration: 0.985s, episode steps: 108, steps per second: 110, episode reward: -137.961, mean reward: -1.277 [-100.000, 13.137], mean action: 1.528 [0.000, 3.000],  loss: 6965.674838, mse: 9511985.333333, mean_q: 3354.350491, mean_eps: 0.974004
  3050/100000: episode: 33, duration: 0.784s, episode steps: 107, steps per second: 137, episode reward: -124.183, mean reward: -1.161 [-100.000,  7.158], mean action: 1.393 [0.000, 3.000],  loss: 7794.641267, mse: 9387658.177570, mean_q: 3331.710440, mean_eps: 0.973036
  3120/100000: episode: 34, duration: 0.684s, episode steps:  70, steps per second: 102, episode reward: -88.836, mean reward: -1.269 [-100.000,  6.401], mean action: 1.400 [0.000, 3.000],  loss: 4117.906495, mse: 9249513.114286, mean_q: 3304.326311, mean_eps: 0.972240
  3209/100000: episode: 35, duration: 0.866s, episode steps:  89, steps per second: 103, episode reward: -99.696, mean reward: -1.120 [-100.000,  7.210], mean action: 1.562 [0.000, 3.000],  loss: 4380.101772, mse: 9105690.870787, mean_q: 3276.342417, mean_eps: 0.971524
  3307/100000: episode: 36, duration: 0.822s, episode steps:  98, steps per second: 119, episode reward: -60.756, mean reward: -0.620 [-100.000, 50.525], mean action: 1.480 [0.000, 3.000],  loss: 3942.843036, mse: 8968762.755102, mean_q: 3250.227357, mean_eps: 0.970682
  3395/100000: episode: 37, duration: 0.695s, episode steps:  88, steps per second: 127, episode reward: -434.442, mean reward: -4.937 [-100.000,  0.290], mean action: 1.534 [0.000, 3.000],  loss: 7530.790428, mse: 8917208.261364, mean_q: 3234.180037, mean_eps: 0.969846
  3531/100000: episode: 38, duration: 1.025s, episode steps: 136, steps per second: 133, episode reward: -358.491, mean reward: -2.636 [-100.000, 48.403], mean action: 1.588 [0.000, 3.000],  loss: 3818.856496, mse: 8720523.338235, mean_q: 3203.213237, mean_eps: 0.968838
  3671/100000: episode: 39, duration: 1.110s, episode steps: 140, steps per second: 126, episode reward: -312.330, mean reward: -2.231 [-100.000,  7.694], mean action: 1.629 [0.000, 3.000],  loss: 5201.647059, mse: 8502122.303571, mean_q: 3155.438538, mean_eps: 0.967595
  3776/100000: episode: 40, duration: 0.977s, episode steps: 105, steps per second: 107, episode reward: -136.959, mean reward: -1.304 [-100.000,  5.822], mean action: 1.295 [0.000, 3.000],  loss: 5060.341520, mse: 8351835.747619, mean_q: 3126.859431, mean_eps: 0.966493
  3896/100000: episode: 41, duration: 1.102s, episode steps: 120, steps per second: 109, episode reward: -125.671, mean reward: -1.047 [-100.000, 11.359], mean action: 1.600 [0.000, 3.000],  loss: 11513.634111, mse: 8432694.333333, mean_q: 3146.627344, mean_eps: 0.965480
  3969/100000: episode: 42, duration: 0.570s, episode steps:  73, steps per second: 128, episode reward: -49.762, mean reward: -0.682 [-100.000, 31.336], mean action: 1.521 [0.000, 3.000],  loss: 4101.143357, mse: 8351529.958904, mean_q: 3148.295069, mean_eps: 0.964612
  4069/100000: episode: 43, duration: 1.191s, episode steps: 100, steps per second:  84, episode reward: -256.208, mean reward: -2.562 [-100.000,  7.756], mean action: 1.590 [0.000, 3.000],  loss: 6803.189425, mse: 8087271.070000, mean_q: 3090.570137, mean_eps: 0.963834
  4131/100000: episode: 44, duration: 0.676s, episode steps:  62, steps per second:  92, episode reward: -102.545, mean reward: -1.654 [-100.000,  5.095], mean action: 1.710 [0.000, 3.000],  loss: 3340.645919, mse: 8025892.274194, mean_q: 3075.612880, mean_eps: 0.963104
  4194/100000: episode: 45, duration: 0.675s, episode steps:  63, steps per second:  93, episode reward: -96.662, mean reward: -1.534 [-100.000, 10.282], mean action: 1.603 [0.000, 3.000],  loss: 5835.945011, mse: 8047329.587302, mean_q: 3067.462255, mean_eps: 0.962542
  4270/100000: episode: 46, duration: 0.746s, episode steps:  76, steps per second: 102, episode reward: -371.518, mean reward: -4.888 [-100.000,  4.700], mean action: 1.408 [0.000, 3.000],  loss: 8636.020632, mse: 7924064.276316, mean_q: 3046.988728, mean_eps: 0.961917
  4346/100000: episode: 47, duration: 0.801s, episode steps:  76, steps per second:  95, episode reward: -139.145, mean reward: -1.831 [-100.000,  7.968], mean action: 1.553 [0.000, 3.000],  loss: 4539.946960, mse: 7892198.796053, mean_q: 3042.724629, mean_eps: 0.961232
  4424/100000: episode: 48, duration: 0.832s, episode steps:  78, steps per second:  94, episode reward: -60.708, mean reward: -0.778 [-100.000, 11.457], mean action: 1.308 [0.000, 3.000],  loss: 2541.131381, mse: 8133491.788462, mean_q: 3101.519663, mean_eps: 0.960540
  4493/100000: episode: 49, duration: 0.629s, episode steps:  69, steps per second: 110, episode reward: -182.845, mean reward: -2.650 [-100.000,  7.935], mean action: 1.710 [0.000, 3.000],  loss: 5517.744154, mse: 7840110.094203, mean_q: 3033.475229, mean_eps: 0.959878
  4565/100000: episode: 50, duration: 0.654s, episode steps:  72, steps per second: 110, episode reward: -132.171, mean reward: -1.836 [-100.000,  7.485], mean action: 1.375 [0.000, 3.000],  loss: 7068.529400, mse: 7583281.569444, mean_q: 2986.316349, mean_eps: 0.959244
  4695/100000: episode: 51, duration: 1.181s, episode steps: 130, steps per second: 110, episode reward: 12.734, mean reward:  0.098 [-100.000, 80.667], mean action: 1.354 [0.000, 3.000],  loss: 4150.294357, mse: 7488446.965385, mean_q: 2969.336071, mean_eps: 0.958334
  4768/100000: episode: 52, duration: 0.654s, episode steps:  73, steps per second: 112, episode reward: -42.793, mean reward: -0.586 [-100.000, 12.219], mean action: 1.603 [0.000, 3.000],  loss: 8462.017395, mse: 7323417.294521, mean_q: 2931.558922, mean_eps: 0.957421
  4868/100000: episode: 53, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: -110.822, mean reward: -1.108 [-100.000, 11.557], mean action: 1.550 [0.000, 3.000],  loss: 3496.445883, mse: 7230003.535000, mean_q: 2910.936653, mean_eps: 0.956643
  4948/100000: episode: 54, duration: 0.608s, episode steps:  80, steps per second: 132, episode reward: -310.671, mean reward: -3.883 [-100.000,  2.572], mean action: 1.688 [0.000, 3.000],  loss: 3132.464159, mse: 7199075.762500, mean_q: 2909.365155, mean_eps: 0.955832
  5075/100000: episode: 55, duration: 0.921s, episode steps: 127, steps per second: 138, episode reward: -258.543, mean reward: -2.036 [-100.000, 10.757], mean action: 1.748 [0.000, 3.000],  loss: 5198.406451, mse: 7092149.149606, mean_q: 2882.179309, mean_eps: 0.954901
  5174/100000: episode: 56, duration: 0.765s, episode steps:  99, steps per second: 129, episode reward: -430.768, mean reward: -4.351 [-100.000,  0.681], mean action: 1.485 [0.000, 3.000],  loss: 4077.670481, mse: 6998780.257576, mean_q: 2856.440600, mean_eps: 0.953884
  5266/100000: episode: 57, duration: 0.666s, episode steps:  92, steps per second: 138, episode reward:  0.957, mean reward:  0.010 [-100.000, 113.674], mean action: 1.402 [0.000, 3.000],  loss: 6158.387548, mse: 6984907.331522, mean_q: 2855.325662, mean_eps: 0.953024
  5358/100000: episode: 58, duration: 0.666s, episode steps:  92, steps per second: 138, episode reward: -141.125, mean reward: -1.534 [-100.000, 11.166], mean action: 1.815 [0.000, 3.000],  loss: 3708.040464, mse: 6923555.423913, mean_q: 2835.049648, mean_eps: 0.952196
  5445/100000: episode: 59, duration: 0.652s, episode steps:  87, steps per second: 133, episode reward: -441.322, mean reward: -5.073 [-100.000,  0.178], mean action: 1.483 [0.000, 3.000],  loss: 5244.653891, mse: 6797109.218391, mean_q: 2811.935328, mean_eps: 0.951391
  5510/100000: episode: 60, duration: 0.497s, episode steps:  65, steps per second: 131, episode reward: -163.515, mean reward: -2.516 [-100.000,  5.588], mean action: 1.508 [0.000, 3.000],  loss: 5238.163166, mse: 6785081.484615, mean_q: 2811.294659, mean_eps: 0.950707
  5641/100000: episode: 61, duration: 1.036s, episode steps: 131, steps per second: 126, episode reward: -240.304, mean reward: -1.834 [-100.000, 84.566], mean action: 1.420 [0.000, 3.000],  loss: 3236.877422, mse: 6814300.820611, mean_q: 2825.141434, mean_eps: 0.949825
  5760/100000: episode: 62, duration: 0.975s, episode steps: 119, steps per second: 122, episode reward: -162.519, mean reward: -1.366 [-100.000,  8.924], mean action: 1.563 [0.000, 3.000],  loss: 4078.789040, mse: 6643741.852941, mean_q: 2784.264053, mean_eps: 0.948700
  5858/100000: episode: 63, duration: 0.708s, episode steps:  98, steps per second: 138, episode reward: -142.982, mean reward: -1.459 [-100.000,  8.479], mean action: 1.439 [0.000, 3.000],  loss: 4506.776519, mse: 6703326.045918, mean_q: 2800.409548, mean_eps: 0.947723
  5939/100000: episode: 64, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -393.078, mean reward: -4.853 [-100.000,  1.404], mean action: 1.370 [0.000, 3.000],  loss: 4999.389264, mse: 6613589.290123, mean_q: 2788.966637, mean_eps: 0.946918
  6043/100000: episode: 65, duration: 0.782s, episode steps: 104, steps per second: 133, episode reward: -161.065, mean reward: -1.549 [-100.000, 34.920], mean action: 1.423 [0.000, 3.000],  loss: 2674.931100, mse: 6621582.475962, mean_q: 2781.249200, mean_eps: 0.946085
  6173/100000: episode: 66, duration: 1.073s, episode steps: 130, steps per second: 121, episode reward: -244.734, mean reward: -1.883 [-100.000,  1.347], mean action: 1.562 [0.000, 3.000],  loss: 4503.404415, mse: 6568840.480769, mean_q: 2771.438296, mean_eps: 0.945032
  6298/100000: episode: 67, duration: 1.102s, episode steps: 125, steps per second: 113, episode reward: -157.757, mean reward: -1.262 [-100.000,  5.541], mean action: 1.488 [0.000, 3.000],  loss: 5023.135282, mse: 6387106.468000, mean_q: 2721.534855, mean_eps: 0.943885
  6374/100000: episode: 68, duration: 0.621s, episode steps:  76, steps per second: 122, episode reward: -30.351, mean reward: -0.399 [-100.000, 100.677], mean action: 1.434 [0.000, 3.000],  loss: 3025.185552, mse: 6406893.526316, mean_q: 2736.397336, mean_eps: 0.942981
  6476/100000: episode: 69, duration: 0.811s, episode steps: 102, steps per second: 126, episode reward: -271.693, mean reward: -2.664 [-100.000,  8.808], mean action: 1.598 [0.000, 3.000],  loss: 5760.156248, mse: 6283019.799020, mean_q: 2697.701675, mean_eps: 0.942180
  6546/100000: episode: 70, duration: 0.595s, episode steps:  70, steps per second: 118, episode reward: -70.648, mean reward: -1.009 [-100.000,  6.442], mean action: 1.557 [0.000, 3.000],  loss: 3593.791754, mse: 6169231.400000, mean_q: 2683.074683, mean_eps: 0.941405
  6642/100000: episode: 71, duration: 0.770s, episode steps:  96, steps per second: 125, episode reward: -132.857, mean reward: -1.384 [-100.000, 34.381], mean action: 1.625 [0.000, 3.000],  loss: 4734.645051, mse: 6168412.135417, mean_q: 2676.032867, mean_eps: 0.940659
  6724/100000: episode: 72, duration: 0.601s, episode steps:  82, steps per second: 136, episode reward: -120.749, mean reward: -1.473 [-100.000,  8.901], mean action: 1.476 [0.000, 3.000],  loss: 3593.057944, mse: 6334448.640244, mean_q: 2725.270603, mean_eps: 0.939858
  6831/100000: episode: 73, duration: 0.809s, episode steps: 107, steps per second: 132, episode reward: -184.138, mean reward: -1.721 [-100.000, 22.363], mean action: 1.626 [0.000, 3.000],  loss: 3275.819510, mse: 6213216.794393, mean_q: 2690.117404, mean_eps: 0.939007
  6953/100000: episode: 74, duration: 0.879s, episode steps: 122, steps per second: 139, episode reward: -98.579, mean reward: -0.808 [-100.000,  7.764], mean action: 1.492 [0.000, 3.000],  loss: 5190.063518, mse: 6058818.586066, mean_q: 2651.237957, mean_eps: 0.937976
  7046/100000: episode: 75, duration: 0.660s, episode steps:  93, steps per second: 141, episode reward: -330.989, mean reward: -3.559 [-100.000,  0.436], mean action: 1.591 [0.000, 3.000],  loss: 4825.021167, mse: 5896097.634409, mean_q: 2613.013850, mean_eps: 0.937009
  7116/100000: episode: 76, duration: 0.597s, episode steps:  70, steps per second: 117, episode reward: -49.891, mean reward: -0.713 [-100.000, 18.669], mean action: 1.529 [0.000, 3.000],  loss: 4349.282534, mse: 5968506.235714, mean_q: 2639.957893, mean_eps: 0.936275
  7183/100000: episode: 77, duration: 0.606s, episode steps:  67, steps per second: 111, episode reward: -131.435, mean reward: -1.962 [-100.000,  6.656], mean action: 1.373 [0.000, 3.000],  loss: 2355.975106, mse: 5966214.231343, mean_q: 2631.348400, mean_eps: 0.935659
  7291/100000: episode: 78, duration: 0.933s, episode steps: 108, steps per second: 116, episode reward: -213.308, mean reward: -1.975 [-100.000,  7.162], mean action: 1.324 [0.000, 3.000],  loss: 3707.567395, mse: 5841754.171296, mean_q: 2610.065796, mean_eps: 0.934871
  7359/100000: episode: 79, duration: 0.718s, episode steps:  68, steps per second:  95, episode reward: -47.915, mean reward: -0.705 [-100.000, 12.187], mean action: 1.603 [0.000, 3.000],  loss: 4049.634051, mse: 5923980.058824, mean_q: 2640.624540, mean_eps: 0.934080
  7431/100000: episode: 80, duration: 0.570s, episode steps:  72, steps per second: 126, episode reward: -37.591, mean reward: -0.522 [-100.000, 15.149], mean action: 1.431 [0.000, 3.000],  loss: 4308.998569, mse: 5852481.770833, mean_q: 2618.119856, mean_eps: 0.933449
  7512/100000: episode: 81, duration: 0.830s, episode steps:  81, steps per second:  98, episode reward: -228.607, mean reward: -2.822 [-100.000,  5.247], mean action: 1.506 [0.000, 3.000],  loss: 5815.005269, mse: 5806905.000000, mean_q: 2599.173057, mean_eps: 0.932761
  7590/100000: episode: 82, duration: 0.746s, episode steps:  78, steps per second: 105, episode reward: -143.124, mean reward: -1.835 [-100.000, 24.739], mean action: 1.564 [0.000, 3.000],  loss: 2622.348112, mse: 5829745.371795, mean_q: 2605.829017, mean_eps: 0.932045
  7701/100000: episode: 83, duration: 0.891s, episode steps: 111, steps per second: 125, episode reward: -131.250, mean reward: -1.182 [-100.000, 30.836], mean action: 1.396 [0.000, 3.000],  loss: 4128.353607, mse: 5906791.743243, mean_q: 2625.483502, mean_eps: 0.931195
  7795/100000: episode: 84, duration: 0.748s, episode steps:  94, steps per second: 126, episode reward: -79.732, mean reward: -0.848 [-100.000, 12.365], mean action: 1.638 [0.000, 3.000],  loss: 3916.845274, mse: 5855491.851064, mean_q: 2612.153424, mean_eps: 0.930273
  7865/100000: episode: 85, duration: 0.587s, episode steps:  70, steps per second: 119, episode reward: -108.236, mean reward: -1.546 [-100.000, 10.801], mean action: 1.714 [0.000, 3.000],  loss: 3219.938190, mse: 5686382.857143, mean_q: 2566.707652, mean_eps: 0.929534
  7949/100000: episode: 86, duration: 0.623s, episode steps:  84, steps per second: 135, episode reward: -190.556, mean reward: -2.269 [-100.000,  4.692], mean action: 1.345 [0.000, 3.000],  loss: 4773.061510, mse: 5797977.869048, mean_q: 2596.625189, mean_eps: 0.928842
  8069/100000: episode: 87, duration: 0.905s, episode steps: 120, steps per second: 133, episode reward: -124.690, mean reward: -1.039 [-100.000,  7.167], mean action: 1.533 [0.000, 3.000],  loss: 4322.143909, mse: 5685038.329167, mean_q: 2569.970746, mean_eps: 0.927924
  8183/100000: episode: 88, duration: 1.045s, episode steps: 114, steps per second: 109, episode reward: -250.237, mean reward: -2.195 [-100.000, 16.643], mean action: 1.421 [0.000, 3.000],  loss: 4698.750484, mse: 5713828.565789, mean_q: 2576.486058, mean_eps: 0.926871
  8254/100000: episode: 89, duration: 0.578s, episode steps:  71, steps per second: 123, episode reward: -173.785, mean reward: -2.448 [-100.000, 25.755], mean action: 1.549 [0.000, 3.000],  loss: 3672.465153, mse: 5667476.225352, mean_q: 2572.995544, mean_eps: 0.926038
done, took 69.054 seconds