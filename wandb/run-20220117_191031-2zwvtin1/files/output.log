Training for 150000 steps ...
     79/150000: episode: 1, duration: 0.145s, episode steps:  79, steps per second: 544, episode reward: -69.568, mean reward: -0.881 [-100.000, 44.501], mean action: 1.215 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    156/150000: episode: 2, duration: 0.869s, episode steps:  77, steps per second:  89, episode reward: -102.430, mean reward: -1.330 [-100.000,  9.943], mean action: 1.545 [0.000, 3.000],  loss: 10.394062, mse: 5.205252, mean_q: 0.075365, mean_eps: 0.999232
    221/150000: episode: 3, duration: 0.495s, episode steps:  65, steps per second: 131, episode reward: -76.899, mean reward: -1.183 [-100.000,  8.366], mean action: 1.477 [0.000, 3.000],  loss: 7.100100, mse: 3.607279, mean_q: 0.231374, mean_eps: 0.998872
    312/150000: episode: 4, duration: 0.649s, episode steps:  91, steps per second: 140, episode reward: -129.079, mean reward: -1.418 [-100.000,  6.418], mean action: 1.429 [0.000, 3.000],  loss: 7.338539, mse: 3.906228, mean_q: 0.431510, mean_eps: 0.998404
    433/150000: episode: 5, duration: 0.849s, episode steps: 121, steps per second: 143, episode reward: -90.984, mean reward: -0.752 [-100.000, 51.579], mean action: 1.678 [0.000, 3.000],  loss: 8.923445, mse: 5.120068, mean_q: 0.505390, mean_eps: 0.997768
    563/150000: episode: 6, duration: 0.961s, episode steps: 130, steps per second: 135, episode reward: -304.654, mean reward: -2.343 [-100.000,  3.183], mean action: 1.585 [0.000, 3.000],  loss: 13.211542, mse: 7.660891, mean_q: 0.587859, mean_eps: 0.997015
    625/150000: episode: 7, duration: 0.471s, episode steps:  62, steps per second: 132, episode reward: -114.922, mean reward: -1.854 [-100.000,  7.020], mean action: 1.258 [0.000, 3.000],  loss: 8.071118, mse: 5.264383, mean_q: 0.542913, mean_eps: 0.996439
    714/150000: episode: 8, duration: 0.691s, episode steps:  89, steps per second: 129, episode reward: -362.332, mean reward: -4.071 [-100.000,  0.503], mean action: 1.404 [0.000, 3.000],  loss: 9.608478, mse: 6.024581, mean_q: 0.586675, mean_eps: 0.995986
    783/150000: episode: 9, duration: 0.549s, episode steps:  69, steps per second: 126, episode reward: -249.704, mean reward: -3.619 [-100.000,  3.991], mean action: 1.232 [0.000, 3.000],  loss: 9.704143, mse: 6.424012, mean_q: 0.518821, mean_eps: 0.995512
    892/150000: episode: 10, duration: 0.830s, episode steps: 109, steps per second: 131, episode reward: -203.691, mean reward: -1.869 [-100.000,  9.658], mean action: 1.468 [0.000, 3.000],  loss: 13.776203, mse: 8.324485, mean_q: 0.686552, mean_eps: 0.994978
   1017/150000: episode: 11, duration: 0.908s, episode steps: 125, steps per second: 138, episode reward: -119.303, mean reward: -0.954 [-100.000,  5.771], mean action: 1.440 [0.000, 3.000],  loss: 9.830680, mse: 6.561562, mean_q: 0.753870, mean_eps: 0.994276
   1114/150000: episode: 12, duration: 0.746s, episode steps:  97, steps per second: 130, episode reward: -82.685, mean reward: -0.852 [-100.000, 17.450], mean action: 1.402 [0.000, 3.000],  loss: 7.361670, mse: 6.426025, mean_q: 1.370016, mean_eps: 0.993610
   1177/150000: episode: 13, duration: 0.463s, episode steps:  63, steps per second: 136, episode reward: -174.045, mean reward: -2.763 [-100.000, 68.454], mean action: 1.603 [0.000, 3.000],  loss: 11.146657, mse: 8.730989, mean_q: 1.286211, mean_eps: 0.993130
   1257/150000: episode: 14, duration: 0.592s, episode steps:  80, steps per second: 135, episode reward: -303.560, mean reward: -3.795 [-100.000,  3.938], mean action: 1.825 [0.000, 3.000],  loss: 10.238368, mse: 8.621618, mean_q: 1.391848, mean_eps: 0.992701
   1329/150000: episode: 15, duration: 0.539s, episode steps:  72, steps per second: 134, episode reward: -33.818, mean reward: -0.470 [-100.000, 62.774], mean action: 1.736 [0.000, 3.000],  loss: 11.273103, mse: 8.989423, mean_q: 1.155882, mean_eps: 0.992245
   1400/150000: episode: 16, duration: 0.567s, episode steps:  71, steps per second: 125, episode reward: -148.527, mean reward: -2.092 [-100.000,  6.533], mean action: 1.423 [0.000, 3.000],  loss: 6.063492, mse: 6.104838, mean_q: 1.301045, mean_eps: 0.991816
   1471/150000: episode: 17, duration: 0.585s, episode steps:  71, steps per second: 121, episode reward: -84.663, mean reward: -1.192 [-100.000,  9.197], mean action: 1.451 [0.000, 3.000],  loss: 7.602084, mse: 7.205787, mean_q: 1.270791, mean_eps: 0.991390
   1546/150000: episode: 18, duration: 0.533s, episode steps:  75, steps per second: 141, episode reward: -148.891, mean reward: -1.985 [-100.000,  3.465], mean action: 1.427 [0.000, 3.000],  loss: 13.216255, mse: 10.025750, mean_q: 1.314762, mean_eps: 0.990952
   1639/150000: episode: 19, duration: 0.752s, episode steps:  93, steps per second: 124, episode reward: -75.382, mean reward: -0.811 [-100.000, 21.852], mean action: 1.548 [0.000, 3.000],  loss: 9.181142, mse: 8.011521, mean_q: 1.391094, mean_eps: 0.990448
   1733/150000: episode: 20, duration: 0.726s, episode steps:  94, steps per second: 129, episode reward: -81.291, mean reward: -0.865 [-100.000, 18.190], mean action: 1.617 [0.000, 3.000],  loss: 9.634295, mse: 8.152745, mean_q: 1.330174, mean_eps: 0.989887
   1854/150000: episode: 21, duration: 0.921s, episode steps: 121, steps per second: 131, episode reward: -252.997, mean reward: -2.091 [-100.000,  7.421], mean action: 1.471 [0.000, 3.000],  loss: 11.370003, mse: 8.938511, mean_q: 1.409951, mean_eps: 0.989242
   1967/150000: episode: 22, duration: 0.884s, episode steps: 113, steps per second: 128, episode reward: -267.611, mean reward: -2.368 [-100.000, 18.117], mean action: 1.637 [0.000, 3.000],  loss: 11.079629, mse: 8.713055, mean_q: 1.341154, mean_eps: 0.988540
   2083/150000: episode: 23, duration: 0.927s, episode steps: 116, steps per second: 125, episode reward: -215.836, mean reward: -1.861 [-100.000,  6.164], mean action: 1.491 [0.000, 3.000],  loss: 9.052673, mse: 9.085256, mean_q: 2.099518, mean_eps: 0.987853
   2165/150000: episode: 24, duration: 1.189s, episode steps:  82, steps per second:  69, episode reward: -87.484, mean reward: -1.067 [-100.000, 21.432], mean action: 1.500 [0.000, 3.000],  loss: 9.631249, mse: 10.699028, mean_q: 2.212185, mean_eps: 0.987259
   2245/150000: episode: 25, duration: 0.979s, episode steps:  80, steps per second:  82, episode reward: -218.206, mean reward: -2.728 [-100.000, 93.501], mean action: 1.300 [0.000, 3.000],  loss: 10.890291, mse: 11.667454, mean_q: 2.168414, mean_eps: 0.986773
   2352/150000: episode: 26, duration: 1.612s, episode steps: 107, steps per second:  66, episode reward: -346.491, mean reward: -3.238 [-100.000,  5.036], mean action: 1.364 [0.000, 3.000],  loss: 11.856552, mse: 11.915946, mean_q: 2.089397, mean_eps: 0.986212
   2408/150000: episode: 27, duration: 0.912s, episode steps:  56, steps per second:  61, episode reward: -121.175, mean reward: -2.164 [-100.000,  7.247], mean action: 1.518 [0.000, 3.000],  loss: 11.835540, mse: 13.227412, mean_q: 2.032979, mean_eps: 0.985723
   2551/150000: episode: 28, duration: 1.705s, episode steps: 143, steps per second:  84, episode reward: -27.812, mean reward: -0.194 [-100.000, 71.852], mean action: 1.413 [0.000, 3.000],  loss: 9.096447, mse: 11.467521, mean_q: 2.117956, mean_eps: 0.985126
   2650/150000: episode: 29, duration: 0.884s, episode steps:  99, steps per second: 112, episode reward: -106.913, mean reward: -1.080 [-100.000, 18.478], mean action: 1.394 [0.000, 3.000],  loss: 6.953468, mse: 10.306223, mean_q: 2.185368, mean_eps: 0.984400
   2736/150000: episode: 30, duration: 0.743s, episode steps:  86, steps per second: 116, episode reward: -99.269, mean reward: -1.154 [-100.000,  9.876], mean action: 1.535 [0.000, 3.000],  loss: 10.508036, mse: 11.929133, mean_q: 2.117735, mean_eps: 0.983845
   2820/150000: episode: 31, duration: 0.702s, episode steps:  84, steps per second: 120, episode reward: -99.041, mean reward: -1.179 [-100.000,  7.248], mean action: 1.357 [0.000, 3.000],  loss: 9.621816, mse: 11.735492, mean_q: 2.094095, mean_eps: 0.983335
   2888/150000: episode: 32, duration: 0.526s, episode steps:  68, steps per second: 129, episode reward: -73.764, mean reward: -1.085 [-100.000,  9.410], mean action: 1.588 [0.000, 3.000],  loss: 13.958313, mse: 14.078961, mean_q: 2.149798, mean_eps: 0.982879
   2967/150000: episode: 33, duration: 0.732s, episode steps:  79, steps per second: 108, episode reward: -118.849, mean reward: -1.504 [-100.000, 35.023], mean action: 1.544 [0.000, 3.000],  loss: 8.716307, mse: 11.220610, mean_q: 2.127845, mean_eps: 0.982438
   3041/150000: episode: 34, duration: 0.637s, episode steps:  74, steps per second: 116, episode reward: -89.662, mean reward: -1.212 [-100.000, 39.385], mean action: 1.635 [0.000, 3.000],  loss: 11.098924, mse: 13.852318, mean_q: 2.454737, mean_eps: 0.981979
   3124/150000: episode: 35, duration: 0.644s, episode steps:  83, steps per second: 129, episode reward: -124.154, mean reward: -1.496 [-100.000,  5.990], mean action: 1.494 [0.000, 3.000],  loss: 8.610372, mse: 16.337883, mean_q: 2.961121, mean_eps: 0.981508
   3207/150000: episode: 36, duration: 0.644s, episode steps:  83, steps per second: 129, episode reward: -110.549, mean reward: -1.332 [-100.000, 23.174], mean action: 1.386 [0.000, 3.000],  loss: 10.230596, mse: 17.667247, mean_q: 2.905725, mean_eps: 0.981010
   3275/150000: episode: 37, duration: 0.533s, episode steps:  68, steps per second: 128, episode reward: -189.594, mean reward: -2.788 [-100.000,  3.728], mean action: 1.588 [0.000, 3.000],  loss: 7.950371, mse: 16.811038, mean_q: 3.000576, mean_eps: 0.980557
   3390/150000: episode: 38, duration: 0.894s, episode steps: 115, steps per second: 129, episode reward: -280.283, mean reward: -2.437 [-100.000,  4.893], mean action: 1.652 [0.000, 3.000],  loss: 8.498337, mse: 16.452427, mean_q: 2.834529, mean_eps: 0.980008
   3528/150000: episode: 39, duration: 1.307s, episode steps: 138, steps per second: 106, episode reward: -254.125, mean reward: -1.841 [-100.000, 60.387], mean action: 1.572 [0.000, 3.000],  loss: 10.463141, mse: 17.768872, mean_q: 3.044171, mean_eps: 0.979249
   3613/150000: episode: 40, duration: 0.617s, episode steps:  85, steps per second: 138, episode reward: -98.826, mean reward: -1.163 [-100.000, 12.146], mean action: 1.412 [0.000, 3.000],  loss: 11.206715, mse: 17.853212, mean_q: 2.998188, mean_eps: 0.978580
   3693/150000: episode: 41, duration: 0.612s, episode steps:  80, steps per second: 131, episode reward: -301.423, mean reward: -3.768 [-100.000,  5.299], mean action: 1.425 [0.000, 3.000],  loss: 13.733225, mse: 19.829455, mean_q: 2.925228, mean_eps: 0.978085
   3825/150000: episode: 42, duration: 1.072s, episode steps: 132, steps per second: 123, episode reward: 18.381, mean reward:  0.139 [-100.000, 109.529], mean action: 1.538 [0.000, 3.000],  loss: 9.503635, mse: 17.824331, mean_q: 2.946723, mean_eps: 0.977449
   3900/150000: episode: 43, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: 36.443, mean reward:  0.486 [-100.000, 92.522], mean action: 1.573 [0.000, 3.000],  loss: 7.410238, mse: 17.534443, mean_q: 3.028485, mean_eps: 0.976828
   3992/150000: episode: 44, duration: 0.673s, episode steps:  92, steps per second: 137, episode reward: -84.705, mean reward: -0.921 [-100.000, 21.458], mean action: 1.435 [0.000, 3.000],  loss: 8.697656, mse: 17.359350, mean_q: 3.033985, mean_eps: 0.976327
   4069/150000: episode: 45, duration: 0.611s, episode steps:  77, steps per second: 126, episode reward: -244.647, mean reward: -3.177 [-100.000, 84.703], mean action: 1.026 [0.000, 3.000],  loss: 10.148305, mse: 21.877457, mean_q: 3.736069, mean_eps: 0.975820
   4131/150000: episode: 46, duration: 0.513s, episode steps:  62, steps per second: 121, episode reward: -48.476, mean reward: -0.782 [-100.000, 17.370], mean action: 1.645 [0.000, 3.000],  loss: 12.188697, mse: 26.185147, mean_q: 3.775140, mean_eps: 0.975403
   4251/150000: episode: 47, duration: 0.931s, episode steps: 120, steps per second: 129, episode reward: -306.400, mean reward: -2.553 [-100.000,  3.119], mean action: 1.425 [0.000, 3.000],  loss: 9.509960, mse: 25.698604, mean_q: 3.753604, mean_eps: 0.974857
   4348/150000: episode: 48, duration: 0.874s, episode steps:  97, steps per second: 111, episode reward: -127.496, mean reward: -1.314 [-100.000,  7.588], mean action: 1.495 [0.000, 3.000],  loss: 13.697676, mse: 28.723744, mean_q: 3.762958, mean_eps: 0.974206
   4431/150000: episode: 49, duration: 0.644s, episode steps:  83, steps per second: 129, episode reward: -84.140, mean reward: -1.014 [-100.000, 17.667], mean action: 1.446 [0.000, 3.000],  loss: 9.787418, mse: 25.452710, mean_q: 3.897009, mean_eps: 0.973666
   4535/150000: episode: 50, duration: 0.817s, episode steps: 104, steps per second: 127, episode reward: -194.726, mean reward: -1.872 [-100.000,  5.758], mean action: 1.413 [0.000, 3.000],  loss: 10.622265, mse: 28.494882, mean_q: 3.804350, mean_eps: 0.973105
   4619/150000: episode: 51, duration: 0.659s, episode steps:  84, steps per second: 127, episode reward: -202.490, mean reward: -2.411 [-100.000,  4.379], mean action: 1.512 [0.000, 3.000],  loss: 7.948527, mse: 27.047271, mean_q: 3.706281, mean_eps: 0.972541
   4720/150000: episode: 52, duration: 0.916s, episode steps: 101, steps per second: 110, episode reward: -119.292, mean reward: -1.181 [-100.000, 14.822], mean action: 1.485 [0.000, 3.000],  loss: 10.931789, mse: 27.499680, mean_q: 3.761543, mean_eps: 0.971986
   4780/150000: episode: 53, duration: 0.495s, episode steps:  60, steps per second: 121, episode reward: -99.076, mean reward: -1.651 [-100.000,  7.362], mean action: 1.917 [0.000, 3.000],  loss: 7.194435, mse: 25.767376, mean_q: 3.996292, mean_eps: 0.971503
   4849/150000: episode: 54, duration: 0.562s, episode steps:  69, steps per second: 123, episode reward: -124.754, mean reward: -1.808 [-100.000,  8.619], mean action: 1.406 [0.000, 3.000],  loss: 9.913471, mse: 28.847287, mean_q: 3.948250, mean_eps: 0.971116
   4930/150000: episode: 55, duration: 0.613s, episode steps:  81, steps per second: 132, episode reward: -134.905, mean reward: -1.665 [-100.000,  9.553], mean action: 1.716 [0.000, 3.000],  loss: 9.060949, mse: 26.769027, mean_q: 3.922742, mean_eps: 0.970666
   4990/150000: episode: 56, duration: 0.494s, episode steps:  60, steps per second: 121, episode reward: -65.336, mean reward: -1.089 [-100.000, 40.428], mean action: 1.600 [0.000, 3.000],  loss: 7.507234, mse: 26.453243, mean_q: 3.818837, mean_eps: 0.970243
   5083/150000: episode: 57, duration: 0.824s, episode steps:  93, steps per second: 113, episode reward: -122.340, mean reward: -1.315 [-100.000,  7.021], mean action: 1.559 [0.000, 3.000],  loss: 7.637234, mse: 34.820157, mean_q: 5.048085, mean_eps: 0.969784
   5176/150000: episode: 58, duration: 0.794s, episode steps:  93, steps per second: 117, episode reward: -110.217, mean reward: -1.185 [-100.000,  6.649], mean action: 1.441 [0.000, 3.000],  loss: 9.785119, mse: 37.532835, mean_q: 5.032246, mean_eps: 0.969226
   5303/150000: episode: 59, duration: 1.041s, episode steps: 127, steps per second: 122, episode reward: -83.008, mean reward: -0.654 [-100.000, 13.031], mean action: 1.606 [0.000, 3.000],  loss: 8.900696, mse: 37.652562, mean_q: 5.166400, mean_eps: 0.968566
   5414/150000: episode: 60, duration: 0.964s, episode steps: 111, steps per second: 115, episode reward: -257.115, mean reward: -2.316 [-100.000,  3.886], mean action: 1.640 [0.000, 3.000],  loss: 7.099644, mse: 39.814483, mean_q: 5.209888, mean_eps: 0.967852
   5492/150000: episode: 61, duration: 0.617s, episode steps:  78, steps per second: 126, episode reward: -26.522, mean reward: -0.340 [-100.000, 16.649], mean action: 1.603 [0.000, 3.000],  loss: 7.433537, mse: 36.492561, mean_q: 5.110746, mean_eps: 0.967285
   5584/150000: episode: 62, duration: 0.698s, episode steps:  92, steps per second: 132, episode reward: -59.664, mean reward: -0.649 [-100.000, 15.231], mean action: 1.326 [0.000, 3.000],  loss: 12.249993, mse: 42.886688, mean_q: 5.161447, mean_eps: 0.966775
   5681/150000: episode: 63, duration: 0.910s, episode steps:  97, steps per second: 107, episode reward: -414.174, mean reward: -4.270 [-100.000,  0.947], mean action: 1.639 [0.000, 3.000],  loss: 9.732458, mse: 41.283141, mean_q: 5.109370, mean_eps: 0.966208
   5748/150000: episode: 64, duration: 0.573s, episode steps:  67, steps per second: 117, episode reward: -86.132, mean reward: -1.286 [-100.000, 20.228], mean action: 1.478 [0.000, 3.000],  loss: 9.229070, mse: 41.606530, mean_q: 5.206225, mean_eps: 0.965716
   5874/150000: episode: 65, duration: 0.987s, episode steps: 126, steps per second: 128, episode reward: -167.235, mean reward: -1.327 [-100.000,  7.436], mean action: 1.500 [0.000, 3.000],  loss: 9.250825, mse: 40.822713, mean_q: 5.233015, mean_eps: 0.965137
   5945/150000: episode: 66, duration: 0.648s, episode steps:  71, steps per second: 110, episode reward: -130.111, mean reward: -1.833 [-100.000,  7.560], mean action: 1.493 [0.000, 3.000],  loss: 12.545602, mse: 45.576421, mean_q: 5.290146, mean_eps: 0.964546
   6025/150000: episode: 67, duration: 0.707s, episode steps:  80, steps per second: 113, episode reward: -83.065, mean reward: -1.038 [-100.000,  6.212], mean action: 1.538 [0.000, 3.000],  loss: 6.251859, mse: 40.638136, mean_q: 5.173981, mean_eps: 0.964093
   6139/150000: episode: 68, duration: 1.010s, episode steps: 114, steps per second: 113, episode reward: -243.212, mean reward: -2.133 [-100.000, 12.229], mean action: 1.360 [0.000, 3.000],  loss: 10.751898, mse: 48.534147, mean_q: 5.335919, mean_eps: 0.963511
   6264/150000: episode: 69, duration: 1.110s, episode steps: 125, steps per second: 113, episode reward: -331.562, mean reward: -2.652 [-100.000, 89.759], mean action: 1.584 [0.000, 3.000],  loss: 9.067529, mse: 53.417121, mean_q: 5.323714, mean_eps: 0.962794
   6340/150000: episode: 70, duration: 0.793s, episode steps:  76, steps per second:  96, episode reward: -93.451, mean reward: -1.230 [-100.000,  8.978], mean action: 1.434 [0.000, 3.000],  loss: 7.728233, mse: 55.360329, mean_q: 5.565990, mean_eps: 0.962191
   6433/150000: episode: 71, duration: 0.740s, episode steps:  93, steps per second: 126, episode reward: -247.423, mean reward: -2.660 [-100.000, 10.983], mean action: 1.763 [0.000, 3.000],  loss: 15.547380, mse: 57.602383, mean_q: 5.344815, mean_eps: 0.961684
   6543/150000: episode: 72, duration: 1.033s, episode steps: 110, steps per second: 107, episode reward: -178.340, mean reward: -1.621 [-100.000,  7.043], mean action: 1.727 [0.000, 3.000],  loss: 11.446621, mse: 56.424727, mean_q: 5.222177, mean_eps: 0.961075
   6634/150000: episode: 73, duration: 1.020s, episode steps:  91, steps per second:  89, episode reward: -145.497, mean reward: -1.599 [-100.000,  9.302], mean action: 1.527 [0.000, 3.000],  loss: 12.851390, mse: 55.324577, mean_q: 5.383322, mean_eps: 0.960472
   7634/150000: episode: 74, duration: 10.057s, episode steps: 1000, steps per second:  99, episode reward: 51.133, mean reward:  0.051 [-24.979, 105.939], mean action: 1.449 [0.000, 3.000],  loss: 8.361928, mse: 66.288068, mean_q: 6.342075, mean_eps: 0.957199
   7724/150000: episode: 75, duration: 0.744s, episode steps:  90, steps per second: 121, episode reward: -83.078, mean reward: -0.923 [-100.000, 40.077], mean action: 1.678 [0.000, 3.000],  loss: 8.206093, mse: 77.155393, mean_q: 6.929251, mean_eps: 0.953929
   7809/150000: episode: 76, duration: 0.619s, episode steps:  85, steps per second: 137, episode reward: -252.192, mean reward: -2.967 [-100.000, 28.346], mean action: 1.447 [0.000, 3.000],  loss: 7.616630, mse: 76.431447, mean_q: 7.079830, mean_eps: 0.953404
   7894/150000: episode: 77, duration: 0.618s, episode steps:  85, steps per second: 137, episode reward: -221.058, mean reward: -2.601 [-100.000,  6.887], mean action: 1.494 [0.000, 3.000],  loss: 5.459294, mse: 78.158094, mean_q: 6.964376, mean_eps: 0.952894
   7959/150000: episode: 78, duration: 0.510s, episode steps:  65, steps per second: 127, episode reward: -43.685, mean reward: -0.672 [-100.000, 11.921], mean action: 1.508 [0.000, 3.000],  loss: 8.673369, mse: 71.320164, mean_q: 6.881198, mean_eps: 0.952444
   8096/150000: episode: 79, duration: 1.242s, episode steps: 137, steps per second: 110, episode reward: 27.056, mean reward:  0.197 [-100.000, 127.646], mean action: 1.372 [0.000, 3.000],  loss: 5.636633, mse: 81.102816, mean_q: 7.258206, mean_eps: 0.951838
   8177/150000: episode: 80, duration: 0.796s, episode steps:  81, steps per second: 102, episode reward: -187.992, mean reward: -2.321 [-100.000,  6.831], mean action: 1.531 [0.000, 3.000],  loss: 4.128765, mse: 88.485315, mean_q: 7.560840, mean_eps: 0.951184
   8248/150000: episode: 81, duration: 0.689s, episode steps:  71, steps per second: 103, episode reward: -157.462, mean reward: -2.218 [-100.000, 29.938], mean action: 1.465 [0.000, 3.000],  loss: 10.112431, mse: 95.959660, mean_q: 7.469243, mean_eps: 0.950728
   8320/150000: episode: 82, duration: 0.628s, episode steps:  72, steps per second: 115, episode reward: -78.787, mean reward: -1.094 [-100.000, 18.403], mean action: 1.472 [0.000, 3.000],  loss: 8.265291, mse: 91.692513, mean_q: 7.750517, mean_eps: 0.950299
   8426/150000: episode: 83, duration: 0.958s, episode steps: 106, steps per second: 111, episode reward: -292.513, mean reward: -2.760 [-100.000, 60.683], mean action: 1.736 [0.000, 3.000],  loss: 9.366691, mse: 96.429633, mean_q: 7.565167, mean_eps: 0.949765
   8499/150000: episode: 84, duration: 0.661s, episode steps:  73, steps per second: 110, episode reward: -72.714, mean reward: -0.996 [-100.000, 34.561], mean action: 1.507 [0.000, 3.000],  loss: 10.458265, mse: 89.850104, mean_q: 7.558834, mean_eps: 0.949228
   8567/150000: episode: 85, duration: 0.628s, episode steps:  68, steps per second: 108, episode reward: -285.530, mean reward: -4.199 [-100.000, 79.007], mean action: 1.647 [0.000, 3.000],  loss: 8.064673, mse: 102.324912, mean_q: 7.788489, mean_eps: 0.948805
   8677/150000: episode: 86, duration: 1.107s, episode steps: 110, steps per second:  99, episode reward: -126.667, mean reward: -1.152 [-100.000,  9.564], mean action: 1.582 [0.000, 3.000],  loss: 7.100116, mse: 102.037213, mean_q: 7.356614, mean_eps: 0.948271
   8763/150000: episode: 87, duration: 0.893s, episode steps:  86, steps per second:  96, episode reward: -116.410, mean reward: -1.354 [-100.000, 38.544], mean action: 1.291 [0.000, 3.000],  loss: 5.402779, mse: 91.851413, mean_q: 7.669927, mean_eps: 0.947683
   8863/150000: episode: 88, duration: 0.951s, episode steps: 100, steps per second: 105, episode reward: -78.157, mean reward: -0.782 [-100.000, 12.389], mean action: 1.350 [0.000, 3.000],  loss: 7.934574, mse: 91.164084, mean_q: 7.659340, mean_eps: 0.947125
   8945/150000: episode: 89, duration: 0.710s, episode steps:  82, steps per second: 116, episode reward: -125.999, mean reward: -1.537 [-100.000,  5.503], mean action: 1.695 [0.000, 3.000],  loss: 7.581193, mse: 96.234326, mean_q: 8.024390, mean_eps: 0.946579
   9053/150000: episode: 90, duration: 0.908s, episode steps: 108, steps per second: 119, episode reward: -62.052, mean reward: -0.575 [-100.000, 16.792], mean action: 1.546 [0.000, 3.000],  loss: 8.738914, mse: 110.118845, mean_q: 7.928686, mean_eps: 0.946009
   9124/150000: episode: 91, duration: 0.626s, episode steps:  71, steps per second: 113, episode reward: -140.213, mean reward: -1.975 [-100.000,  7.577], mean action: 1.324 [0.000, 3.000],  loss: 8.127963, mse: 114.652546, mean_q: 8.676554, mean_eps: 0.945472
   9237/150000: episode: 92, duration: 0.968s, episode steps: 113, steps per second: 117, episode reward: -240.153, mean reward: -2.125 [-100.000, 31.713], mean action: 1.602 [0.000, 3.000],  loss: 6.091315, mse: 127.464460, mean_q: 8.178474, mean_eps: 0.944920
   9316/150000: episode: 93, duration: 0.696s, episode steps:  79, steps per second: 113, episode reward: -307.916, mean reward: -3.898 [-100.000,  1.393], mean action: 1.557 [0.000, 3.000],  loss: 9.147769, mse: 123.951711, mean_q: 8.409824, mean_eps: 0.944344
   9434/150000: episode: 94, duration: 1.197s, episode steps: 118, steps per second:  99, episode reward: -96.526, mean reward: -0.818 [-100.000,  6.704], mean action: 1.415 [0.000, 3.000],  loss: 8.519942, mse: 122.065031, mean_q: 8.433086, mean_eps: 0.943753
   9518/150000: episode: 95, duration: 0.827s, episode steps:  84, steps per second: 102, episode reward: -115.283, mean reward: -1.372 [-100.000, 10.373], mean action: 1.667 [0.000, 3.000],  loss: 5.704936, mse: 124.767938, mean_q: 8.386008, mean_eps: 0.943147
   9590/150000: episode: 96, duration: 0.818s, episode steps:  72, steps per second:  88, episode reward: -79.367, mean reward: -1.102 [-100.000, 24.127], mean action: 1.569 [0.000, 3.000],  loss: 12.553920, mse: 129.075335, mean_q: 8.484474, mean_eps: 0.942679
   9730/150000: episode: 97, duration: 1.298s, episode steps: 140, steps per second: 108, episode reward: -153.259, mean reward: -1.095 [-100.000,  9.062], mean action: 1.650 [0.000, 3.000],  loss: 5.395384, mse: 125.182176, mean_q: 8.378471, mean_eps: 0.942043
   9836/150000: episode: 98, duration: 1.015s, episode steps: 106, steps per second: 104, episode reward: -188.327, mean reward: -1.777 [-100.000,  7.485], mean action: 1.358 [0.000, 3.000],  loss: 9.933976, mse: 122.861731, mean_q: 8.280209, mean_eps: 0.941305
   9956/150000: episode: 99, duration: 0.947s, episode steps: 120, steps per second: 127, episode reward: -243.772, mean reward: -2.031 [-100.000, 57.240], mean action: 1.492 [0.000, 3.000],  loss: 8.939017, mse: 125.936395, mean_q: 8.403466, mean_eps: 0.940627
  10026/150000: episode: 100, duration: 0.732s, episode steps:  70, steps per second:  96, episode reward: -85.012, mean reward: -1.214 [-100.000,  8.037], mean action: 1.600 [0.000, 3.000],  loss: 9.414120, mse: 125.030831, mean_q: 8.458041, mean_eps: 0.940057
  10118/150000: episode: 101, duration: 0.912s, episode steps:  92, steps per second: 101, episode reward: -199.339, mean reward: -2.167 [-100.000, 117.316], mean action: 1.609 [0.000, 3.000],  loss: 10.086552, mse: 147.656715, mean_q: 8.906154, mean_eps: 0.939571
  10216/150000: episode: 102, duration: 0.856s, episode steps:  98, steps per second: 114, episode reward: -165.055, mean reward: -1.684 [-100.000,  5.957], mean action: 1.704 [0.000, 3.000],  loss: 7.561620, mse: 134.096186, mean_q: 9.091011, mean_eps: 0.939001
  10360/150000: episode: 103, duration: 1.554s, episode steps: 144, steps per second:  93, episode reward: -180.512, mean reward: -1.254 [-100.000, 34.283], mean action: 1.389 [0.000, 3.000],  loss: 7.354446, mse: 149.116907, mean_q: 9.059467, mean_eps: 0.938275
  10478/150000: episode: 104, duration: 1.195s, episode steps: 118, steps per second:  99, episode reward: -145.100, mean reward: -1.230 [-100.000, 10.796], mean action: 1.661 [0.000, 3.000],  loss: 10.209830, mse: 144.398824, mean_q: 8.948286, mean_eps: 0.937489
  10547/150000: episode: 105, duration: 0.702s, episode steps:  69, steps per second:  98, episode reward: -122.966, mean reward: -1.782 [-100.000, 25.090], mean action: 1.522 [0.000, 3.000],  loss: 8.793187, mse: 137.999283, mean_q: 8.905879, mean_eps: 0.936928
  10631/150000: episode: 106, duration: 0.733s, episode steps:  84, steps per second: 115, episode reward: -105.970, mean reward: -1.262 [-100.000, 10.239], mean action: 1.369 [0.000, 3.000],  loss: 6.164302, mse: 158.884520, mean_q: 8.984489, mean_eps: 0.936469
  10705/150000: episode: 107, duration: 0.656s, episode steps:  74, steps per second: 113, episode reward: -75.999, mean reward: -1.027 [-100.000,  7.684], mean action: 1.486 [0.000, 3.000],  loss: 10.185467, mse: 158.188389, mean_q: 9.265060, mean_eps: 0.935995
  10795/150000: episode: 108, duration: 0.859s, episode steps:  90, steps per second: 105, episode reward: -104.505, mean reward: -1.161 [-100.000,  7.243], mean action: 1.689 [0.000, 3.000],  loss: 9.706006, mse: 142.426478, mean_q: 9.024504, mean_eps: 0.935503
  10889/150000: episode: 109, duration: 0.848s, episode steps:  94, steps per second: 111, episode reward: -209.230, mean reward: -2.226 [-100.000,  5.531], mean action: 1.340 [0.000, 3.000],  loss: 8.801434, mse: 157.159072, mean_q: 9.039134, mean_eps: 0.934951
  10957/150000: episode: 110, duration: 0.563s, episode steps:  68, steps per second: 121, episode reward: -67.952, mean reward: -0.999 [-100.000, 11.302], mean action: 1.485 [0.000, 3.000],  loss: 5.191623, mse: 149.825697, mean_q: 9.072889, mean_eps: 0.934465
  11082/150000: episode: 111, duration: 1.004s, episode steps: 125, steps per second: 125, episode reward: -230.246, mean reward: -1.842 [-100.000, 10.581], mean action: 1.688 [0.000, 3.000],  loss: 7.715035, mse: 166.608271, mean_q: 8.865967, mean_eps: 0.933886
  11158/150000: episode: 112, duration: 0.612s, episode steps:  76, steps per second: 124, episode reward: -217.558, mean reward: -2.863 [-100.000,  6.873], mean action: 1.408 [0.000, 3.000],  loss: 6.701861, mse: 154.521305, mean_q: 9.286510, mean_eps: 0.933283
  11239/150000: episode: 113, duration: 0.688s, episode steps:  81, steps per second: 118, episode reward: -51.660, mean reward: -0.638 [-100.000, 11.619], mean action: 1.580 [0.000, 3.000],  loss: 10.215960, mse: 165.114782, mean_q: 9.127740, mean_eps: 0.932812
  11327/150000: episode: 114, duration: 0.749s, episode steps:  88, steps per second: 117, episode reward: -60.303, mean reward: -0.685 [-100.000, 13.589], mean action: 1.716 [0.000, 3.000],  loss: 8.932478, mse: 166.207159, mean_q: 9.192194, mean_eps: 0.932305
  11437/150000: episode: 115, duration: 0.897s, episode steps: 110, steps per second: 123, episode reward: -149.414, mean reward: -1.358 [-100.000,  8.291], mean action: 1.555 [0.000, 3.000],  loss: 8.566525, mse: 169.876695, mean_q: 9.088644, mean_eps: 0.931711
  11548/150000: episode: 116, duration: 0.964s, episode steps: 111, steps per second: 115, episode reward: 21.408, mean reward:  0.193 [-100.000, 81.734], mean action: 1.477 [0.000, 3.000],  loss: 11.005568, mse: 181.939745, mean_q: 8.799062, mean_eps: 0.931048
  11644/150000: episode: 117, duration: 0.706s, episode steps:  96, steps per second: 136, episode reward: -115.781, mean reward: -1.206 [-100.000, 11.061], mean action: 1.333 [0.000, 3.000],  loss: 5.929471, mse: 180.062910, mean_q: 8.815355, mean_eps: 0.930427
  11730/150000: episode: 118, duration: 0.657s, episode steps:  86, steps per second: 131, episode reward: -157.895, mean reward: -1.836 [-100.000, 27.727], mean action: 1.535 [0.000, 3.000],  loss: 6.775743, mse: 165.410515, mean_q: 8.887305, mean_eps: 0.929881
  11841/150000: episode: 119, duration: 0.931s, episode steps: 111, steps per second: 119, episode reward: -151.232, mean reward: -1.362 [-100.000,  9.320], mean action: 1.613 [0.000, 3.000],  loss: 10.422030, mse: 179.197757, mean_q: 8.798147, mean_eps: 0.929290
  11959/150000: episode: 120, duration: 0.941s, episode steps: 118, steps per second: 125, episode reward: -77.864, mean reward: -0.660 [-100.000, 18.187], mean action: 1.559 [0.000, 3.000],  loss: 7.704348, mse: 175.529654, mean_q: 9.207861, mean_eps: 0.928603
  12053/150000: episode: 121, duration: 0.719s, episode steps:  94, steps per second: 131, episode reward: -85.080, mean reward: -0.905 [-100.000,  8.103], mean action: 1.266 [0.000, 3.000],  loss: 7.792642, mse: 180.724319, mean_q: 8.987390, mean_eps: 0.927967
  12136/150000: episode: 122, duration: 0.602s, episode steps:  83, steps per second: 138, episode reward: -90.174, mean reward: -1.086 [-100.000, 11.412], mean action: 1.578 [0.000, 3.000],  loss: 5.397029, mse: 183.239266, mean_q: 8.823578, mean_eps: 0.927436
  12201/150000: episode: 123, duration: 0.532s, episode steps:  65, steps per second: 122, episode reward: -74.902, mean reward: -1.152 [-100.000,  7.269], mean action: 1.477 [0.000, 3.000],  loss: 7.695859, mse: 186.503004, mean_q: 9.317178, mean_eps: 0.926992
  12296/150000: episode: 124, duration: 0.791s, episode steps:  95, steps per second: 120, episode reward: -185.214, mean reward: -1.950 [-100.000,  9.363], mean action: 1.305 [0.000, 3.000],  loss: 7.650927, mse: 185.473434, mean_q: 9.357388, mean_eps: 0.926512
  12406/150000: episode: 125, duration: 0.864s, episode steps: 110, steps per second: 127, episode reward: -208.251, mean reward: -1.893 [-100.000, 12.691], mean action: 1.518 [0.000, 3.000],  loss: 6.136873, mse: 202.559611, mean_q: 8.834072, mean_eps: 0.925897
  12510/150000: episode: 126, duration: 0.814s, episode steps: 104, steps per second: 128, episode reward: -162.717, mean reward: -1.565 [-100.000, 51.718], mean action: 1.442 [0.000, 3.000],  loss: 6.894475, mse: 203.121217, mean_q: 8.949030, mean_eps: 0.925255
  12581/150000: episode: 127, duration: 0.624s, episode steps:  71, steps per second: 114, episode reward: -83.738, mean reward: -1.179 [-100.000,  5.968], mean action: 1.493 [0.000, 3.000],  loss: 10.217188, mse: 179.050838, mean_q: 9.002747, mean_eps: 0.924730
  12669/150000: episode: 128, duration: 0.685s, episode steps:  88, steps per second: 128, episode reward: -110.120, mean reward: -1.251 [-100.000, 11.735], mean action: 1.466 [0.000, 3.000],  loss: 12.052304, mse: 193.968184, mean_q: 8.872587, mean_eps: 0.924253
  12772/150000: episode: 129, duration: 0.784s, episode steps: 103, steps per second: 131, episode reward: -165.537, mean reward: -1.607 [-100.000, 39.064], mean action: 1.466 [0.000, 3.000],  loss: 9.830972, mse: 190.511733, mean_q: 8.960508, mean_eps: 0.923680
  12869/150000: episode: 130, duration: 0.759s, episode steps:  97, steps per second: 128, episode reward: -301.741, mean reward: -3.111 [-100.000,  4.334], mean action: 1.546 [0.000, 3.000],  loss: 10.113055, mse: 202.836532, mean_q: 8.420674, mean_eps: 0.923080
  12946/150000: episode: 131, duration: 0.568s, episode steps:  77, steps per second: 136, episode reward: -174.686, mean reward: -2.269 [-100.000,  6.089], mean action: 1.429 [0.000, 3.000],  loss: 7.429563, mse: 197.245730, mean_q: 8.712245, mean_eps: 0.922558
  13068/150000: episode: 132, duration: 0.871s, episode steps: 122, steps per second: 140, episode reward: -115.514, mean reward: -0.947 [-100.000, 11.381], mean action: 1.467 [0.000, 3.000],  loss: 6.514998, mse: 214.533229, mean_q: 8.701732, mean_eps: 0.921961
  13159/150000: episode: 133, duration: 0.705s, episode steps:  91, steps per second: 129, episode reward: -134.162, mean reward: -1.474 [-100.000, 10.949], mean action: 1.440 [0.000, 3.000],  loss: 8.445350, mse: 220.001959, mean_q: 9.046035, mean_eps: 0.921322
  13243/150000: episode: 134, duration: 0.677s, episode steps:  84, steps per second: 124, episode reward: -409.607, mean reward: -4.876 [-100.000, 71.510], mean action: 1.750 [0.000, 3.000],  loss: 8.584121, mse: 238.422087, mean_q: 8.331974, mean_eps: 0.920797
  13346/150000: episode: 135, duration: 0.832s, episode steps: 103, steps per second: 124, episode reward: -310.856, mean reward: -3.018 [-100.000, 87.848], mean action: 1.408 [0.000, 3.000],  loss: 9.701720, mse: 227.466323, mean_q: 9.033076, mean_eps: 0.920236
  13460/150000: episode: 136, duration: 1.001s, episode steps: 114, steps per second: 114, episode reward: -133.193, mean reward: -1.168 [-100.000,  5.479], mean action: 1.623 [0.000, 3.000],  loss: 7.426502, mse: 230.953586, mean_q: 9.024823, mean_eps: 0.919585
  13513/150000: episode: 137, duration: 0.417s, episode steps:  53, steps per second: 127, episode reward: -121.024, mean reward: -2.283 [-100.000, 30.402], mean action: 1.604 [0.000, 3.000],  loss: 5.111539, mse: 236.258820, mean_q: 8.463696, mean_eps: 0.919084
  13667/150000: episode: 138, duration: 1.204s, episode steps: 154, steps per second: 128, episode reward: -30.897, mean reward: -0.201 [-100.000, 47.634], mean action: 1.558 [0.000, 3.000],  loss: 9.518376, mse: 252.180614, mean_q: 8.585962, mean_eps: 0.918463
  13751/150000: episode: 139, duration: 0.734s, episode steps:  84, steps per second: 114, episode reward: -112.425, mean reward: -1.338 [-100.000, 16.162], mean action: 1.643 [0.000, 3.000],  loss: 7.279754, mse: 242.857753, mean_q: 8.835468, mean_eps: 0.917749
  13881/150000: episode: 140, duration: 1.052s, episode steps: 130, steps per second: 124, episode reward: -68.938, mean reward: -0.530 [-100.000, 12.236], mean action: 1.531 [0.000, 3.000],  loss: 12.114374, mse: 248.034665, mean_q: 8.685791, mean_eps: 0.917107
  13949/150000: episode: 141, duration: 0.562s, episode steps:  68, steps per second: 121, episode reward: -118.663, mean reward: -1.745 [-100.000,  7.105], mean action: 1.529 [0.000, 3.000],  loss: 11.917328, mse: 242.570415, mean_q: 9.033130, mean_eps: 0.916513
  14046/150000: episode: 142, duration: 0.767s, episode steps:  97, steps per second: 127, episode reward: -243.055, mean reward: -2.506 [-100.000,  0.770], mean action: 1.423 [0.000, 3.000],  loss: 12.422270, mse: 259.642576, mean_q: 8.940599, mean_eps: 0.916018
  14144/150000: episode: 143, duration: 0.859s, episode steps:  98, steps per second: 114, episode reward: -102.371, mean reward: -1.045 [-100.000, 10.811], mean action: 1.592 [0.000, 3.000],  loss: 8.466720, mse: 239.097919, mean_q: 9.477601, mean_eps: 0.915433
  14231/150000: episode: 144, duration: 0.802s, episode steps:  87, steps per second: 109, episode reward: -401.047, mean reward: -4.610 [-100.000, 84.701], mean action: 1.437 [0.000, 3.000],  loss: 3.840779, mse: 257.240161, mean_q: 9.809960, mean_eps: 0.914878
  14327/150000: episode: 145, duration: 0.864s, episode steps:  96, steps per second: 111, episode reward: -92.594, mean reward: -0.965 [-100.000, 20.160], mean action: 1.667 [0.000, 3.000],  loss: 12.423123, mse: 272.399677, mean_q: 9.092834, mean_eps: 0.914329
  14422/150000: episode: 146, duration: 0.797s, episode steps:  95, steps per second: 119, episode reward: -173.689, mean reward: -1.828 [-100.000, 28.982], mean action: 1.547 [0.000, 3.000],  loss: 13.592648, mse: 285.860240, mean_q: 9.213402, mean_eps: 0.913756
  14512/150000: episode: 147, duration: 0.750s, episode steps:  90, steps per second: 120, episode reward: -516.726, mean reward: -5.741 [-100.000, 76.220], mean action: 1.633 [0.000, 3.000],  loss: 11.831273, mse: 288.392783, mean_q: 9.538754, mean_eps: 0.913201
  14621/150000: episode: 148, duration: 0.958s, episode steps: 109, steps per second: 114, episode reward: -98.848, mean reward: -0.907 [-100.000,  8.078], mean action: 1.468 [0.000, 3.000],  loss: 13.107718, mse: 302.054086, mean_q: 9.613601, mean_eps: 0.912604
  14721/150000: episode: 149, duration: 0.957s, episode steps: 100, steps per second: 104, episode reward: -108.339, mean reward: -1.083 [-100.000, 10.341], mean action: 1.580 [0.000, 3.000],  loss: 6.461976, mse: 257.812601, mean_q: 9.609833, mean_eps: 0.911977
  14798/150000: episode: 150, duration: 0.700s, episode steps:  77, steps per second: 110, episode reward: -92.825, mean reward: -1.206 [-100.000,  7.088], mean action: 1.455 [0.000, 3.000],  loss: 10.038099, mse: 277.239281, mean_q: 9.378898, mean_eps: 0.911446
  14858/150000: episode: 151, duration: 0.519s, episode steps:  60, steps per second: 116, episode reward: -102.478, mean reward: -1.708 [-100.000, 19.312], mean action: 1.500 [0.000, 3.000],  loss: 6.260846, mse: 281.626556, mean_q: 9.724263, mean_eps: 0.911035
  14925/150000: episode: 152, duration: 0.540s, episode steps:  67, steps per second: 124, episode reward: -140.710, mean reward: -2.100 [-100.000, 13.534], mean action: 1.582 [0.000, 3.000],  loss: 12.709456, mse: 254.497293, mean_q: 10.036137, mean_eps: 0.910654
  15047/150000: episode: 153, duration: 0.906s, episode steps: 122, steps per second: 135, episode reward: -167.051, mean reward: -1.369 [-100.000,  3.311], mean action: 1.549 [0.000, 3.000],  loss: 8.758087, mse: 277.980284, mean_q: 10.129863, mean_eps: 0.910087
  15109/150000: episode: 154, duration: 0.448s, episode steps:  62, steps per second: 138, episode reward: -117.162, mean reward: -1.890 [-100.000,  7.143], mean action: 1.548 [0.000, 3.000],  loss: 10.283992, mse: 302.140359, mean_q: 10.455278, mean_eps: 0.909535
  15179/150000: episode: 155, duration: 0.503s, episode steps:  70, steps per second: 139, episode reward: -74.786, mean reward: -1.068 [-100.000,  7.198], mean action: 1.386 [0.000, 3.000],  loss: 10.105922, mse: 308.322188, mean_q: 10.516733, mean_eps: 0.909139
  15305/150000: episode: 156, duration: 0.966s, episode steps: 126, steps per second: 130, episode reward: -151.184, mean reward: -1.200 [-100.000,  7.429], mean action: 1.389 [0.000, 3.000],  loss: 11.340443, mse: 314.372285, mean_q: 10.772726, mean_eps: 0.908551
  15395/150000: episode: 157, duration: 0.638s, episode steps:  90, steps per second: 141, episode reward: -69.011, mean reward: -0.767 [-100.000, 23.259], mean action: 1.400 [0.000, 3.000],  loss: 13.486689, mse: 307.664006, mean_q: 10.560634, mean_eps: 0.907903
  15523/150000: episode: 158, duration: 0.929s, episode steps: 128, steps per second: 138, episode reward: -255.745, mean reward: -1.998 [-100.000, 60.471], mean action: 1.562 [0.000, 3.000],  loss: 10.969412, mse: 310.647375, mean_q: 10.340202, mean_eps: 0.907249
  15603/150000: episode: 159, duration: 0.612s, episode steps:  80, steps per second: 131, episode reward: -99.398, mean reward: -1.242 [-100.000,  9.228], mean action: 1.863 [0.000, 3.000],  loss: 16.031131, mse: 326.347637, mean_q: 10.580031, mean_eps: 0.906625
  15694/150000: episode: 160, duration: 0.658s, episode steps:  91, steps per second: 138, episode reward:  3.586, mean reward:  0.039 [-100.000, 99.229], mean action: 1.451 [0.000, 3.000],  loss: 8.697908, mse: 311.386728, mean_q: 10.402439, mean_eps: 0.906112
  15762/150000: episode: 161, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -90.622, mean reward: -1.333 [-100.000,  9.011], mean action: 1.500 [0.000, 3.000],  loss: 9.026176, mse: 307.712084, mean_q: 10.669721, mean_eps: 0.905635
  15863/150000: episode: 162, duration: 0.765s, episode steps: 101, steps per second: 132, episode reward: -248.588, mean reward: -2.461 [-100.000,  0.340], mean action: 1.733 [0.000, 3.000],  loss: 7.165917, mse: 305.713766, mean_q: 10.843851, mean_eps: 0.905128
  15939/150000: episode: 163, duration: 0.553s, episode steps:  76, steps per second: 137, episode reward: -116.032, mean reward: -1.527 [-100.000, 11.886], mean action: 1.697 [0.000, 3.000],  loss: 8.691247, mse: 297.318338, mean_q: 10.820839, mean_eps: 0.904597
  16024/150000: episode: 164, duration: 0.610s, episode steps:  85, steps per second: 139, episode reward: -92.195, mean reward: -1.085 [-100.000,  6.860], mean action: 1.647 [0.000, 3.000],  loss: 11.504803, mse: 331.131589, mean_q: 10.619695, mean_eps: 0.904114
  16135/150000: episode: 165, duration: 0.843s, episode steps: 111, steps per second: 132, episode reward: -73.690, mean reward: -0.664 [-100.000, 28.284], mean action: 1.550 [0.000, 3.000],  loss: 6.953585, mse: 356.977322, mean_q: 10.987081, mean_eps: 0.903526
  16233/150000: episode: 166, duration: 0.711s, episode steps:  98, steps per second: 138, episode reward: -196.029, mean reward: -2.000 [-100.000, 28.449], mean action: 1.602 [0.000, 3.000],  loss: 10.619747, mse: 330.711602, mean_q: 11.182756, mean_eps: 0.902899
  16299/150000: episode: 167, duration: 0.479s, episode steps:  66, steps per second: 138, episode reward: -19.616, mean reward: -0.297 [-100.000, 14.455], mean action: 1.439 [0.000, 3.000],  loss: 12.554526, mse: 321.351685, mean_q: 11.667481, mean_eps: 0.902407
  16371/150000: episode: 168, duration: 0.529s, episode steps:  72, steps per second: 136, episode reward: -49.112, mean reward: -0.682 [-100.000, 10.848], mean action: 1.625 [0.000, 3.000],  loss: 13.142697, mse: 311.434015, mean_q: 10.838233, mean_eps: 0.901993
  16457/150000: episode: 169, duration: 0.668s, episode steps:  86, steps per second: 129, episode reward: -156.008, mean reward: -1.814 [-100.000,  8.104], mean action: 1.512 [0.000, 3.000],  loss: 12.041034, mse: 312.175187, mean_q: 11.266319, mean_eps: 0.901519
  16568/150000: episode: 170, duration: 0.812s, episode steps: 111, steps per second: 137, episode reward: -128.674, mean reward: -1.159 [-100.000, 14.899], mean action: 1.541 [0.000, 3.000],  loss: 8.193962, mse: 314.321859, mean_q: 10.839400, mean_eps: 0.900928
  16681/150000: episode: 171, duration: 0.838s, episode steps: 113, steps per second: 135, episode reward: -276.099, mean reward: -2.443 [-100.000,  3.585], mean action: 1.407 [0.000, 3.000],  loss: 12.905112, mse: 348.003539, mean_q: 10.772649, mean_eps: 0.900256
  16750/150000: episode: 172, duration: 0.522s, episode steps:  69, steps per second: 132, episode reward: -100.348, mean reward: -1.454 [-100.000, 18.693], mean action: 1.362 [0.000, 3.000],  loss: 7.458778, mse: 317.173420, mean_q: 11.172619, mean_eps: 0.899710
  16833/150000: episode: 173, duration: 0.596s, episode steps:  83, steps per second: 139, episode reward: -84.214, mean reward: -1.015 [-100.000, 13.662], mean action: 1.747 [0.000, 3.000],  loss: 11.088316, mse: 357.740496, mean_q: 10.855920, mean_eps: 0.899254
  16927/150000: episode: 174, duration: 0.664s, episode steps:  94, steps per second: 142, episode reward: -241.388, mean reward: -2.568 [-100.000, 85.545], mean action: 1.723 [0.000, 3.000],  loss: 6.913172, mse: 321.880191, mean_q: 11.044835, mean_eps: 0.898723
  17050/150000: episode: 175, duration: 0.974s, episode steps: 123, steps per second: 126, episode reward: -110.648, mean reward: -0.900 [-100.000,  7.497], mean action: 1.545 [0.000, 3.000],  loss: 13.156460, mse: 340.905511, mean_q: 10.793707, mean_eps: 0.898072
  17130/150000: episode: 176, duration: 0.575s, episode steps:  80, steps per second: 139, episode reward: -189.558, mean reward: -2.369 [-100.000, 48.840], mean action: 1.538 [0.000, 3.000],  loss: 11.802805, mse: 360.378090, mean_q: 10.521506, mean_eps: 0.897463
  17197/150000: episode: 177, duration: 0.476s, episode steps:  67, steps per second: 141, episode reward: -94.421, mean reward: -1.409 [-100.000, 14.124], mean action: 1.403 [0.000, 3.000],  loss: 11.924712, mse: 335.582446, mean_q: 10.803167, mean_eps: 0.897022
  17288/150000: episode: 178, duration: 0.691s, episode steps:  91, steps per second: 132, episode reward: -147.651, mean reward: -1.623 [-100.000, 10.152], mean action: 1.505 [0.000, 3.000],  loss: 8.199102, mse: 331.388373, mean_q: 10.638849, mean_eps: 0.896548
  17391/150000: episode: 179, duration: 0.764s, episode steps: 103, steps per second: 135, episode reward: -131.265, mean reward: -1.274 [-100.000, 13.980], mean action: 1.602 [0.000, 3.000],  loss: 9.376711, mse: 351.235185, mean_q: 10.699813, mean_eps: 0.895966
  17514/150000: episode: 180, duration: 0.874s, episode steps: 123, steps per second: 141, episode reward: -142.768, mean reward: -1.161 [-100.000, 26.689], mean action: 1.626 [0.000, 3.000],  loss: 13.081014, mse: 352.138757, mean_q: 10.690193, mean_eps: 0.895288
  17581/150000: episode: 181, duration: 0.518s, episode steps:  67, steps per second: 129, episode reward: -34.970, mean reward: -0.522 [-100.000, 16.666], mean action: 1.448 [0.000, 3.000],  loss: 17.533203, mse: 327.727747, mean_q: 10.774920, mean_eps: 0.894718
  17691/150000: episode: 182, duration: 0.803s, episode steps: 110, steps per second: 137, episode reward: -61.218, mean reward: -0.557 [-100.000, 15.532], mean action: 1.536 [0.000, 3.000],  loss: 8.414340, mse: 332.148814, mean_q: 10.654022, mean_eps: 0.894187
  17766/150000: episode: 183, duration: 0.551s, episode steps:  75, steps per second: 136, episode reward: -78.784, mean reward: -1.050 [-100.000,  9.058], mean action: 1.533 [0.000, 3.000],  loss: 12.120292, mse: 361.612162, mean_q: 9.908780, mean_eps: 0.893632
  17834/150000: episode: 184, duration: 0.497s, episode steps:  68, steps per second: 137, episode reward: -77.173, mean reward: -1.135 [-100.000, 43.598], mean action: 1.588 [0.000, 3.000],  loss: 9.324886, mse: 356.343102, mean_q: 10.345495, mean_eps: 0.893203
  17927/150000: episode: 185, duration: 0.706s, episode steps:  93, steps per second: 132, episode reward: -99.644, mean reward: -1.071 [-100.000, 13.082], mean action: 1.505 [0.000, 3.000],  loss: 6.493671, mse: 343.776773, mean_q: 10.802778, mean_eps: 0.892720
  18025/150000: episode: 186, duration: 0.733s, episode steps:  98, steps per second: 134, episode reward: -93.682, mean reward: -0.956 [-100.000,  7.806], mean action: 1.551 [0.000, 3.000],  loss: 11.306215, mse: 360.182802, mean_q: 10.490478, mean_eps: 0.892147
  18147/150000: episode: 187, duration: 0.989s, episode steps: 122, steps per second: 123, episode reward: -107.984, mean reward: -0.885 [-100.000,  7.609], mean action: 1.492 [0.000, 3.000],  loss: 12.327367, mse: 354.651383, mean_q: 11.055931, mean_eps: 0.891487
  18211/150000: episode: 188, duration: 0.530s, episode steps:  64, steps per second: 121, episode reward: -86.499, mean reward: -1.352 [-100.000,  7.751], mean action: 1.328 [0.000, 3.000],  loss: 8.176341, mse: 352.818667, mean_q: 11.313665, mean_eps: 0.890929
  18284/150000: episode: 189, duration: 0.596s, episode steps:  73, steps per second: 122, episode reward: -123.885, mean reward: -1.697 [-100.000, 16.148], mean action: 1.521 [0.000, 3.000],  loss: 11.912472, mse: 357.063012, mean_q: 10.953063, mean_eps: 0.890518
  18384/150000: episode: 190, duration: 0.817s, episode steps: 100, steps per second: 122, episode reward: -154.724, mean reward: -1.547 [-100.000, 10.729], mean action: 1.330 [0.000, 3.000],  loss: 11.952310, mse: 365.580067, mean_q: 10.820144, mean_eps: 0.889999
  18448/150000: episode: 191, duration: 0.613s, episode steps:  64, steps per second: 104, episode reward: -42.726, mean reward: -0.668 [-100.000, 23.488], mean action: 1.531 [0.000, 3.000],  loss: 12.501533, mse: 383.425658, mean_q: 10.678720, mean_eps: 0.889507
  18516/150000: episode: 192, duration: 0.681s, episode steps:  68, steps per second: 100, episode reward: -156.852, mean reward: -2.307 [-100.000, 10.760], mean action: 1.662 [0.000, 3.000],  loss: 13.340081, mse: 351.945651, mean_q: 11.230502, mean_eps: 0.889111
  18637/150000: episode: 193, duration: 1.106s, episode steps: 121, steps per second: 109, episode reward: -104.339, mean reward: -0.862 [-100.000, 21.129], mean action: 1.488 [0.000, 3.000],  loss: 6.644393, mse: 364.965978, mean_q: 11.143144, mean_eps: 0.888544
  18715/150000: episode: 194, duration: 0.780s, episode steps:  78, steps per second: 100, episode reward: -190.827, mean reward: -2.447 [-100.000,  8.027], mean action: 1.577 [0.000, 3.000],  loss: 9.320698, mse: 379.907580, mean_q: 11.389663, mean_eps: 0.887947
  18791/150000: episode: 195, duration: 1.015s, episode steps:  76, steps per second:  75, episode reward: -115.362, mean reward: -1.518 [-100.000,  7.614], mean action: 1.671 [0.000, 3.000],  loss: 9.813839, mse: 371.284268, mean_q: 10.864213, mean_eps: 0.887485
  18876/150000: episode: 196, duration: 1.160s, episode steps:  85, steps per second:  73, episode reward: -65.109, mean reward: -0.766 [-100.000,  6.881], mean action: 1.835 [0.000, 3.000],  loss: 12.285962, mse: 382.461753, mean_q: 11.042615, mean_eps: 0.887002
  18969/150000: episode: 197, duration: 1.097s, episode steps:  93, steps per second:  85, episode reward: -189.913, mean reward: -2.042 [-100.000, 56.900], mean action: 1.484 [0.000, 3.000],  loss: 11.805522, mse: 362.866060, mean_q: 10.770418, mean_eps: 0.886468
  19035/150000: episode: 198, duration: 0.617s, episode steps:  66, steps per second: 107, episode reward: -184.301, mean reward: -2.792 [-100.000,  5.838], mean action: 1.318 [0.000, 3.000],  loss: 8.941477, mse: 375.588300, mean_q: 11.439166, mean_eps: 0.885991
  19168/150000: episode: 199, duration: 1.295s, episode steps: 133, steps per second: 103, episode reward: -175.593, mean reward: -1.320 [-100.000,  6.308], mean action: 1.526 [0.000, 3.000],  loss: 15.007606, mse: 417.143821, mean_q: 11.482257, mean_eps: 0.885394
  19238/150000: episode: 200, duration: 0.740s, episode steps:  70, steps per second:  95, episode reward: -89.612, mean reward: -1.280 [-100.000,  8.586], mean action: 1.471 [0.000, 3.000],  loss: 6.648206, mse: 434.218507, mean_q: 11.099648, mean_eps: 0.884785
  19340/150000: episode: 201, duration: 0.770s, episode steps: 102, steps per second: 132, episode reward: -102.626, mean reward: -1.006 [-100.000,  6.458], mean action: 1.549 [0.000, 3.000],  loss: 11.958155, mse: 422.874772, mean_q: 11.384034, mean_eps: 0.884269
  19426/150000: episode: 202, duration: 0.627s, episode steps:  86, steps per second: 137, episode reward: -132.487, mean reward: -1.541 [-100.000, 17.261], mean action: 1.570 [0.000, 3.000],  loss: 11.845436, mse: 371.252732, mean_q: 11.896053, mean_eps: 0.883705
  19581/150000: episode: 203, duration: 1.146s, episode steps: 155, steps per second: 135, episode reward: -83.522, mean reward: -0.539 [-100.000, 10.647], mean action: 1.600 [0.000, 3.000],  loss: 14.050211, mse: 428.727124, mean_q: 11.401459, mean_eps: 0.882982
  19646/150000: episode: 204, duration: 0.445s, episode steps:  65, steps per second: 146, episode reward: -121.942, mean reward: -1.876 [-100.000,  4.389], mean action: 1.523 [0.000, 3.000],  loss: 11.483015, mse: 428.326349, mean_q: 11.463773, mean_eps: 0.882322
  19732/150000: episode: 205, duration: 0.610s, episode steps:  86, steps per second: 141, episode reward: -70.710, mean reward: -0.822 [-100.000, 19.849], mean action: 1.674 [0.000, 3.000],  loss: 17.327970, mse: 422.211715, mean_q: 11.764313, mean_eps: 0.881869
  19826/150000: episode: 206, duration: 0.708s, episode steps:  94, steps per second: 133, episode reward: -116.497, mean reward: -1.239 [-100.000, 18.408], mean action: 1.394 [0.000, 3.000],  loss: 13.772616, mse: 394.601867, mean_q: 12.009649, mean_eps: 0.881329
  19897/150000: episode: 207, duration: 0.505s, episode steps:  71, steps per second: 141, episode reward: -163.650, mean reward: -2.305 [-100.000,  5.773], mean action: 1.366 [0.000, 3.000],  loss: 9.301789, mse: 408.722438, mean_q: 11.537888, mean_eps: 0.880834
  19995/150000: episode: 208, duration: 0.704s, episode steps:  98, steps per second: 139, episode reward: -208.397, mean reward: -2.127 [-100.000, 24.016], mean action: 1.276 [0.000, 3.000],  loss: 11.315221, mse: 410.514297, mean_q: 11.499478, mean_eps: 0.880327
  20101/150000: episode: 209, duration: 0.784s, episode steps: 106, steps per second: 135, episode reward: -140.144, mean reward: -1.322 [-100.000,  9.885], mean action: 1.613 [0.000, 3.000],  loss: 10.845450, mse: 479.082606, mean_q: 12.452138, mean_eps: 0.879715
  20180/150000: episode: 210, duration: 0.724s, episode steps:  79, steps per second: 109, episode reward: -92.393, mean reward: -1.170 [-100.000, 12.695], mean action: 1.785 [0.000, 3.000],  loss: 15.837624, mse: 448.105601, mean_q: 12.896834, mean_eps: 0.879160
  20274/150000: episode: 211, duration: 0.706s, episode steps:  94, steps per second: 133, episode reward: -227.177, mean reward: -2.417 [-100.000,  0.708], mean action: 1.574 [0.000, 3.000],  loss: 7.489476, mse: 491.256948, mean_q: 11.857060, mean_eps: 0.878641
  20389/150000: episode: 212, duration: 0.906s, episode steps: 115, steps per second: 127, episode reward: -116.922, mean reward: -1.017 [-100.000,  4.926], mean action: 1.513 [0.000, 3.000],  loss: 12.124222, mse: 482.067705, mean_q: 12.597243, mean_eps: 0.878014
  20545/150000: episode: 213, duration: 1.230s, episode steps: 156, steps per second: 127, episode reward: -119.078, mean reward: -0.763 [-100.000, 12.284], mean action: 1.564 [0.000, 3.000],  loss: 9.431876, mse: 501.049031, mean_q: 12.291488, mean_eps: 0.877201
  20649/150000: episode: 214, duration: 0.850s, episode steps: 104, steps per second: 122, episode reward: -70.120, mean reward: -0.674 [-100.000,  9.311], mean action: 1.635 [0.000, 3.000],  loss: 15.905557, mse: 487.275754, mean_q: 12.409394, mean_eps: 0.876421
  20725/150000: episode: 215, duration: 0.588s, episode steps:  76, steps per second: 129, episode reward: -117.878, mean reward: -1.551 [-100.000, 26.117], mean action: 1.434 [0.000, 3.000],  loss: 8.096633, mse: 485.588057, mean_q: 12.954330, mean_eps: 0.875881
  20825/150000: episode: 216, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: -93.481, mean reward: -0.935 [-100.000,  8.953], mean action: 1.580 [0.000, 3.000],  loss: 9.036937, mse: 476.654606, mean_q: 12.169888, mean_eps: 0.875353
  20953/150000: episode: 217, duration: 1.040s, episode steps: 128, steps per second: 123, episode reward: -134.755, mean reward: -1.053 [-100.000,  9.733], mean action: 1.523 [0.000, 3.000],  loss: 9.537532, mse: 500.916650, mean_q: 11.776372, mean_eps: 0.874669
  21047/150000: episode: 218, duration: 0.728s, episode steps:  94, steps per second: 129, episode reward: -140.414, mean reward: -1.494 [-100.000,  7.515], mean action: 1.553 [0.000, 3.000],  loss: 10.933904, mse: 495.766608, mean_q: 12.764520, mean_eps: 0.874003
  21148/150000: episode: 219, duration: 0.727s, episode steps: 101, steps per second: 139, episode reward: -184.854, mean reward: -1.830 [-100.000,  7.820], mean action: 1.505 [0.000, 3.000],  loss: 8.986603, mse: 508.231893, mean_q: 13.032578, mean_eps: 0.873418
  21263/150000: episode: 220, duration: 0.867s, episode steps: 115, steps per second: 133, episode reward: -88.797, mean reward: -0.772 [-100.000,  8.041], mean action: 1.722 [0.000, 3.000],  loss: 11.603274, mse: 501.978896, mean_q: 13.789899, mean_eps: 0.872770
  21355/150000: episode: 221, duration: 0.638s, episode steps:  92, steps per second: 144, episode reward: -95.195, mean reward: -1.035 [-100.000, 11.295], mean action: 1.522 [0.000, 3.000],  loss: 9.036823, mse: 500.317016, mean_q: 13.099491, mean_eps: 0.872149
  21433/150000: episode: 222, duration: 0.558s, episode steps:  78, steps per second: 140, episode reward: -82.880, mean reward: -1.063 [-100.000, 13.671], mean action: 1.731 [0.000, 3.000],  loss: 9.930320, mse: 514.400363, mean_q: 12.990071, mean_eps: 0.871639
  21554/150000: episode: 223, duration: 0.927s, episode steps: 121, steps per second: 130, episode reward: -320.967, mean reward: -2.653 [-100.000, 60.372], mean action: 1.537 [0.000, 3.000],  loss: 5.263761, mse: 541.560477, mean_q: 13.030638, mean_eps: 0.871042
  21617/150000: episode: 224, duration: 0.452s, episode steps:  63, steps per second: 139, episode reward: -110.410, mean reward: -1.753 [-100.000, 14.940], mean action: 1.683 [0.000, 3.000],  loss: 6.251231, mse: 532.622796, mean_q: 12.615259, mean_eps: 0.870490
  21715/150000: episode: 225, duration: 0.700s, episode steps:  98, steps per second: 140, episode reward: -36.839, mean reward: -0.376 [-100.000, 26.209], mean action: 1.765 [0.000, 3.000],  loss: 8.460119, mse: 526.946722, mean_q: 12.819089, mean_eps: 0.870007
  21795/150000: episode: 226, duration: 0.587s, episode steps:  80, steps per second: 136, episode reward: -100.611, mean reward: -1.258 [-100.000,  6.274], mean action: 1.663 [0.000, 3.000],  loss: 9.337099, mse: 542.228278, mean_q: 13.315886, mean_eps: 0.869473
  21866/150000: episode: 227, duration: 0.525s, episode steps:  71, steps per second: 135, episode reward: -55.100, mean reward: -0.776 [-100.000, 10.733], mean action: 1.507 [0.000, 3.000],  loss: 15.538566, mse: 549.376261, mean_q: 12.916781, mean_eps: 0.869020
  21945/150000: episode: 228, duration: 0.569s, episode steps:  79, steps per second: 139, episode reward: -103.300, mean reward: -1.308 [-100.000, 10.442], mean action: 1.494 [0.000, 3.000],  loss: 9.093938, mse: 547.857522, mean_q: 13.275605, mean_eps: 0.868570
  22010/150000: episode: 229, duration: 0.459s, episode steps:  65, steps per second: 142, episode reward: -83.230, mean reward: -1.280 [-100.000, 14.493], mean action: 1.600 [0.000, 3.000],  loss: 7.032925, mse: 498.778447, mean_q: 13.493470, mean_eps: 0.868138
  22099/150000: episode: 230, duration: 0.705s, episode steps:  89, steps per second: 126, episode reward: -73.696, mean reward: -0.828 [-100.000, 11.929], mean action: 1.607 [0.000, 3.000],  loss: 8.486897, mse: 564.657502, mean_q: 13.840631, mean_eps: 0.867676
  22206/150000: episode: 231, duration: 0.769s, episode steps: 107, steps per second: 139, episode reward: -97.690, mean reward: -0.913 [-100.000, 11.380], mean action: 1.458 [0.000, 3.000],  loss: 13.072050, mse: 598.453492, mean_q: 13.439185, mean_eps: 0.867088
  22322/150000: episode: 232, duration: 0.828s, episode steps: 116, steps per second: 140, episode reward: -71.660, mean reward: -0.618 [-100.000, 17.442], mean action: 1.621 [0.000, 3.000],  loss: 10.570381, mse: 584.457785, mean_q: 13.824023, mean_eps: 0.866419
  22418/150000: episode: 233, duration: 0.739s, episode steps:  96, steps per second: 130, episode reward: -105.429, mean reward: -1.098 [-100.000, 74.956], mean action: 1.583 [0.000, 3.000],  loss: 8.783984, mse: 555.229702, mean_q: 13.835756, mean_eps: 0.865783
  22496/150000: episode: 234, duration: 0.688s, episode steps:  78, steps per second: 113, episode reward: -74.227, mean reward: -0.952 [-100.000,  7.065], mean action: 1.731 [0.000, 3.000],  loss: 10.960328, mse: 581.025691, mean_q: 13.836927, mean_eps: 0.865261
  22596/150000: episode: 235, duration: 0.821s, episode steps: 100, steps per second: 122, episode reward: -379.627, mean reward: -3.796 [-100.000, 88.220], mean action: 1.420 [0.000, 3.000],  loss: 8.733209, mse: 572.304187, mean_q: 13.966813, mean_eps: 0.864727
  22698/150000: episode: 236, duration: 0.864s, episode steps: 102, steps per second: 118, episode reward: -91.682, mean reward: -0.899 [-100.000,  7.598], mean action: 1.706 [0.000, 3.000],  loss: 11.433398, mse: 569.843223, mean_q: 14.039778, mean_eps: 0.864121
  22816/150000: episode: 237, duration: 0.950s, episode steps: 118, steps per second: 124, episode reward: -111.431, mean reward: -0.944 [-100.000, 18.142], mean action: 1.763 [0.000, 3.000],  loss: 11.623041, mse: 581.259770, mean_q: 13.982910, mean_eps: 0.863461
  22914/150000: episode: 238, duration: 0.853s, episode steps:  98, steps per second: 115, episode reward: -67.645, mean reward: -0.690 [-100.000, 17.581], mean action: 1.755 [0.000, 3.000],  loss: 12.541260, mse: 606.674296, mean_q: 13.931426, mean_eps: 0.862813
  22993/150000: episode: 239, duration: 0.634s, episode steps:  79, steps per second: 125, episode reward: -98.118, mean reward: -1.242 [-100.000,  8.771], mean action: 1.646 [0.000, 3.000],  loss: 10.244000, mse: 601.029388, mean_q: 13.553479, mean_eps: 0.862282
  23091/150000: episode: 240, duration: 0.745s, episode steps:  98, steps per second: 132, episode reward: -91.957, mean reward: -0.938 [-100.000, 17.561], mean action: 1.378 [0.000, 3.000],  loss: 9.724399, mse: 583.647782, mean_q: 13.950333, mean_eps: 0.861751
  23207/150000: episode: 241, duration: 0.861s, episode steps: 116, steps per second: 135, episode reward: -74.300, mean reward: -0.641 [-100.000,  8.969], mean action: 1.509 [0.000, 3.000],  loss: 10.140883, mse: 603.020921, mean_q: 13.406670, mean_eps: 0.861109
  23301/150000: episode: 242, duration: 0.672s, episode steps:  94, steps per second: 140, episode reward: -441.997, mean reward: -4.702 [-100.000, 98.752], mean action: 1.638 [0.000, 3.000],  loss: 10.324200, mse: 632.251717, mean_q: 13.331709, mean_eps: 0.860479
  23380/150000: episode: 243, duration: 0.562s, episode steps:  79, steps per second: 141, episode reward: -96.361, mean reward: -1.220 [-100.000,  7.146], mean action: 1.316 [0.000, 3.000],  loss: 13.967199, mse: 674.017672, mean_q: 13.617997, mean_eps: 0.859960
  23495/150000: episode: 244, duration: 0.849s, episode steps: 115, steps per second: 136, episode reward: -207.936, mean reward: -1.808 [-100.000,  8.928], mean action: 1.470 [0.000, 3.000],  loss: 10.316661, mse: 639.618482, mean_q: 13.577683, mean_eps: 0.859378
  23563/150000: episode: 245, duration: 0.494s, episode steps:  68, steps per second: 138, episode reward: -133.525, mean reward: -1.964 [-100.000, 19.524], mean action: 1.515 [0.000, 3.000],  loss: 11.197350, mse: 620.878933, mean_q: 13.541061, mean_eps: 0.858829
  23703/150000: episode: 246, duration: 0.968s, episode steps: 140, steps per second: 145, episode reward: -72.933, mean reward: -0.521 [-100.000,  6.737], mean action: 1.557 [0.000, 3.000],  loss: 11.902231, mse: 588.449795, mean_q: 14.061839, mean_eps: 0.858205
  23778/150000: episode: 247, duration: 0.546s, episode steps:  75, steps per second: 137, episode reward: -95.394, mean reward: -1.272 [-100.000,  8.920], mean action: 1.453 [0.000, 3.000],  loss: 16.656167, mse: 627.448182, mean_q: 13.317207, mean_eps: 0.857560
  23856/150000: episode: 248, duration: 0.597s, episode steps:  78, steps per second: 131, episode reward: -288.538, mean reward: -3.699 [-100.000,  0.923], mean action: 1.590 [0.000, 3.000],  loss: 13.127822, mse: 608.505751, mean_q: 13.771403, mean_eps: 0.857101
  23991/150000: episode: 249, duration: 0.957s, episode steps: 135, steps per second: 141, episode reward: -62.647, mean reward: -0.464 [-100.000,  7.400], mean action: 1.659 [0.000, 3.000],  loss: 11.208905, mse: 653.847218, mean_q: 13.279614, mean_eps: 0.856462
  24057/150000: episode: 250, duration: 0.477s, episode steps:  66, steps per second: 138, episode reward: -92.720, mean reward: -1.405 [-100.000,  9.581], mean action: 1.424 [0.000, 3.000],  loss: 15.920825, mse: 661.478531, mean_q: 14.661376, mean_eps: 0.855859
  24137/150000: episode: 251, duration: 0.599s, episode steps:  80, steps per second: 134, episode reward: -149.543, mean reward: -1.869 [-100.000,  8.576], mean action: 1.550 [0.000, 3.000],  loss: 15.801979, mse: 689.852874, mean_q: 14.589500, mean_eps: 0.855421
  24220/150000: episode: 252, duration: 0.578s, episode steps:  83, steps per second: 143, episode reward: -168.804, mean reward: -2.034 [-100.000,  6.815], mean action: 1.446 [0.000, 3.000],  loss: 13.344280, mse: 674.190107, mean_q: 14.857220, mean_eps: 0.854932
  24321/150000: episode: 253, duration: 0.708s, episode steps: 101, steps per second: 143, episode reward: -105.416, mean reward: -1.044 [-100.000, 37.573], mean action: 1.485 [0.000, 3.000],  loss: 11.391056, mse: 670.887445, mean_q: 14.765704, mean_eps: 0.854380
  24445/150000: episode: 254, duration: 0.936s, episode steps: 124, steps per second: 132, episode reward: -112.421, mean reward: -0.907 [-100.000, 12.431], mean action: 1.726 [0.000, 3.000],  loss: 16.457445, mse: 676.469046, mean_q: 14.688288, mean_eps: 0.853705
  24522/150000: episode: 255, duration: 0.541s, episode steps:  77, steps per second: 142, episode reward: -116.340, mean reward: -1.511 [-100.000,  7.196], mean action: 1.481 [0.000, 3.000],  loss: 10.144692, mse: 660.192111, mean_q: 15.097140, mean_eps: 0.853102
  24594/150000: episode: 256, duration: 0.551s, episode steps:  72, steps per second: 131, episode reward: -112.948, mean reward: -1.569 [-100.000,  7.243], mean action: 1.514 [0.000, 3.000],  loss: 21.654209, mse: 704.514130, mean_q: 14.822407, mean_eps: 0.852655
  24720/150000: episode: 257, duration: 1.111s, episode steps: 126, steps per second: 113, episode reward: -59.828, mean reward: -0.475 [-100.000, 26.876], mean action: 1.516 [0.000, 3.000],  loss: 9.751162, mse: 687.873808, mean_q: 14.253526, mean_eps: 0.852061
  24808/150000: episode: 258, duration: 0.673s, episode steps:  88, steps per second: 131, episode reward: -184.647, mean reward: -2.098 [-100.000, 26.992], mean action: 1.807 [0.000, 3.000],  loss: 9.209585, mse: 674.268132, mean_q: 14.187094, mean_eps: 0.851419
  24918/150000: episode: 259, duration: 0.802s, episode steps: 110, steps per second: 137, episode reward: -78.652, mean reward: -0.715 [-100.000, 17.874], mean action: 1.527 [0.000, 3.000],  loss: 17.899167, mse: 698.148087, mean_q: 13.967779, mean_eps: 0.850825
  24993/150000: episode: 260, duration: 0.579s, episode steps:  75, steps per second: 129, episode reward: -90.610, mean reward: -1.208 [-100.000, 23.616], mean action: 1.533 [0.000, 3.000],  loss: 8.973631, mse: 679.873703, mean_q: 14.663986, mean_eps: 0.850270
  25095/150000: episode: 261, duration: 0.738s, episode steps: 102, steps per second: 138, episode reward: -113.973, mean reward: -1.117 [-100.000, 12.840], mean action: 1.637 [0.000, 3.000],  loss: 11.035326, mse: 681.483285, mean_q: 14.124292, mean_eps: 0.849739
  25199/150000: episode: 262, duration: 0.910s, episode steps: 104, steps per second: 114, episode reward: -136.420, mean reward: -1.312 [-100.000, 15.908], mean action: 1.721 [0.000, 3.000],  loss: 15.891353, mse: 698.547028, mean_q: 14.622773, mean_eps: 0.849121
  25280/150000: episode: 263, duration: 0.621s, episode steps:  81, steps per second: 130, episode reward: -76.319, mean reward: -0.942 [-100.000, 11.503], mean action: 1.580 [0.000, 3.000],  loss: 13.817368, mse: 756.853044, mean_q: 14.256343, mean_eps: 0.848566
  25397/150000: episode: 264, duration: 0.940s, episode steps: 117, steps per second: 125, episode reward: -135.248, mean reward: -1.156 [-100.000, 17.784], mean action: 1.530 [0.000, 3.000],  loss: 14.011174, mse: 697.167736, mean_q: 14.263117, mean_eps: 0.847972
  25476/150000: episode: 265, duration: 0.617s, episode steps:  79, steps per second: 128, episode reward: -66.851, mean reward: -0.846 [-100.000, 19.458], mean action: 1.557 [0.000, 3.000],  loss: 12.814544, mse: 708.815498, mean_q: 14.560848, mean_eps: 0.847384
  25567/150000: episode: 266, duration: 0.700s, episode steps:  91, steps per second: 130, episode reward: -68.868, mean reward: -0.757 [-100.000, 22.816], mean action: 1.527 [0.000, 3.000],  loss: 13.760096, mse: 708.287879, mean_q: 13.807723, mean_eps: 0.846874
  25644/150000: episode: 267, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward: -59.625, mean reward: -0.774 [-100.000, 45.833], mean action: 1.468 [0.000, 3.000],  loss: 14.664373, mse: 776.441239, mean_q: 13.933886, mean_eps: 0.846370
  25801/150000: episode: 268, duration: 1.305s, episode steps: 157, steps per second: 120, episode reward: -83.691, mean reward: -0.533 [-100.000, 75.562], mean action: 1.522 [0.000, 3.000],  loss: 17.399712, mse: 682.577320, mean_q: 14.853788, mean_eps: 0.845668
  25866/150000: episode: 269, duration: 0.549s, episode steps:  65, steps per second: 118, episode reward: -73.299, mean reward: -1.128 [-100.000,  9.083], mean action: 1.569 [0.000, 3.000],  loss: 12.647467, mse: 669.083763, mean_q: 14.992086, mean_eps: 0.845002
  25997/150000: episode: 270, duration: 0.973s, episode steps: 131, steps per second: 135, episode reward: -29.647, mean reward: -0.226 [-100.000, 16.282], mean action: 1.733 [0.000, 3.000],  loss: 10.569024, mse: 748.222876, mean_q: 14.149258, mean_eps: 0.844414
  26098/150000: episode: 271, duration: 0.801s, episode steps: 101, steps per second: 126, episode reward: -76.369, mean reward: -0.756 [-100.000, 88.054], mean action: 1.515 [0.000, 3.000],  loss: 11.909982, mse: 715.690265, mean_q: 15.548386, mean_eps: 0.843718
  26198/150000: episode: 272, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: -155.683, mean reward: -1.557 [-100.000,  7.539], mean action: 1.450 [0.000, 3.000],  loss: 11.478228, mse: 701.973819, mean_q: 14.958770, mean_eps: 0.843115
  26315/150000: episode: 273, duration: 0.963s, episode steps: 117, steps per second: 121, episode reward: -65.919, mean reward: -0.563 [-100.000, 16.744], mean action: 1.667 [0.000, 3.000],  loss: 11.965946, mse: 682.899873, mean_q: 14.721922, mean_eps: 0.842464
  26409/150000: episode: 274, duration: 0.721s, episode steps:  94, steps per second: 130, episode reward: -31.072, mean reward: -0.331 [-100.000, 116.021], mean action: 1.638 [0.000, 3.000],  loss: 17.231703, mse: 710.964901, mean_q: 15.024691, mean_eps: 0.841831
  26493/150000: episode: 275, duration: 0.626s, episode steps:  84, steps per second: 134, episode reward: -71.000, mean reward: -0.845 [-100.000,  8.772], mean action: 1.560 [0.000, 3.000],  loss: 16.450880, mse: 730.914941, mean_q: 14.409078, mean_eps: 0.841297
  26600/150000: episode: 276, duration: 0.868s, episode steps: 107, steps per second: 123, episode reward: -82.283, mean reward: -0.769 [-100.000,  6.976], mean action: 1.645 [0.000, 3.000],  loss: 14.523297, mse: 723.286962, mean_q: 14.386114, mean_eps: 0.840724
  26739/150000: episode: 277, duration: 1.050s, episode steps: 139, steps per second: 132, episode reward: -199.285, mean reward: -1.434 [-100.000, 93.088], mean action: 1.597 [0.000, 3.000],  loss: 19.014165, mse: 758.570027, mean_q: 14.920026, mean_eps: 0.839986
  26818/150000: episode: 278, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -111.965, mean reward: -1.417 [-100.000,  9.808], mean action: 1.532 [0.000, 3.000],  loss: 11.245659, mse: 724.404116, mean_q: 15.243978, mean_eps: 0.839332
  26911/150000: episode: 279, duration: 0.769s, episode steps:  93, steps per second: 121, episode reward: -82.263, mean reward: -0.885 [-100.000, 20.712], mean action: 1.581 [0.000, 3.000],  loss: 14.238904, mse: 730.540246, mean_q: 14.482449, mean_eps: 0.838816
  26985/150000: episode: 280, duration: 0.586s, episode steps:  74, steps per second: 126, episode reward: -94.119, mean reward: -1.272 [-100.000, 27.015], mean action: 1.743 [0.000, 3.000],  loss: 13.010015, mse: 757.015964, mean_q: 15.017616, mean_eps: 0.838315
  27074/150000: episode: 281, duration: 0.677s, episode steps:  89, steps per second: 131, episode reward: -195.554, mean reward: -2.197 [-100.000,  6.499], mean action: 1.449 [0.000, 3.000],  loss: 5.519595, mse: 780.359248, mean_q: 14.458272, mean_eps: 0.837826
  27144/150000: episode: 282, duration: 0.542s, episode steps:  70, steps per second: 129, episode reward: -99.852, mean reward: -1.426 [-100.000,  6.954], mean action: 1.557 [0.000, 3.000],  loss: 14.513754, mse: 736.012741, mean_q: 14.894896, mean_eps: 0.837349
  27241/150000: episode: 283, duration: 0.733s, episode steps:  97, steps per second: 132, episode reward: -107.639, mean reward: -1.110 [-100.000,  6.573], mean action: 1.546 [0.000, 3.000],  loss: 14.177891, mse: 763.372753, mean_q: 15.116660, mean_eps: 0.836848
  27320/150000: episode: 284, duration: 0.593s, episode steps:  79, steps per second: 133, episode reward: -79.873, mean reward: -1.011 [-100.000, 14.330], mean action: 1.481 [0.000, 3.000],  loss: 12.022729, mse: 760.464638, mean_q: 15.261916, mean_eps: 0.836320
  27395/150000: episode: 285, duration: 0.581s, episode steps:  75, steps per second: 129, episode reward: -98.084, mean reward: -1.308 [-100.000, 12.880], mean action: 1.613 [0.000, 3.000],  loss: 15.688654, mse: 797.650091, mean_q: 15.041275, mean_eps: 0.835858
  27465/150000: episode: 286, duration: 0.560s, episode steps:  70, steps per second: 125, episode reward: -67.959, mean reward: -0.971 [-100.000,  8.663], mean action: 1.457 [0.000, 3.000],  loss: 14.425252, mse: 773.191672, mean_q: 15.345912, mean_eps: 0.835423
  27622/150000: episode: 287, duration: 1.209s, episode steps: 157, steps per second: 130, episode reward: -101.306, mean reward: -0.645 [-100.000, 11.581], mean action: 1.688 [0.000, 3.000],  loss: 9.638156, mse: 745.817987, mean_q: 15.147767, mean_eps: 0.834742
  27726/150000: episode: 288, duration: 0.771s, episode steps: 104, steps per second: 135, episode reward: -111.023, mean reward: -1.068 [-100.000,  7.802], mean action: 1.596 [0.000, 3.000],  loss: 9.776795, mse: 801.103438, mean_q: 15.614673, mean_eps: 0.833959
  27868/150000: episode: 289, duration: 1.051s, episode steps: 142, steps per second: 135, episode reward: -77.186, mean reward: -0.544 [-100.000,  7.231], mean action: 1.507 [0.000, 3.000],  loss: 12.311832, mse: 779.035842, mean_q: 15.443510, mean_eps: 0.833221
  27945/150000: episode: 290, duration: 0.549s, episode steps:  77, steps per second: 140, episode reward: -62.126, mean reward: -0.807 [-100.000, 17.718], mean action: 1.403 [0.000, 3.000],  loss: 14.187223, mse: 803.057605, mean_q: 14.697422, mean_eps: 0.832564
  28021/150000: episode: 291, duration: 0.594s, episode steps:  76, steps per second: 128, episode reward: -47.172, mean reward: -0.621 [-100.000, 13.284], mean action: 1.566 [0.000, 3.000],  loss: 14.150465, mse: 816.876789, mean_q: 16.306715, mean_eps: 0.832105
  28086/150000: episode: 292, duration: 0.518s, episode steps:  65, steps per second: 125, episode reward: -86.144, mean reward: -1.325 [-100.000,  6.728], mean action: 1.631 [0.000, 3.000],  loss: 13.527514, mse: 882.109724, mean_q: 15.375792, mean_eps: 0.831682
  28165/150000: episode: 293, duration: 0.629s, episode steps:  79, steps per second: 126, episode reward: -101.259, mean reward: -1.282 [-100.000, 10.173], mean action: 1.468 [0.000, 3.000],  loss: 7.545433, mse: 849.562585, mean_q: 15.807268, mean_eps: 0.831250
  28250/150000: episode: 294, duration: 0.658s, episode steps:  85, steps per second: 129, episode reward: -127.207, mean reward: -1.497 [-100.000,  8.858], mean action: 1.529 [0.000, 3.000],  loss: 22.296175, mse: 845.673422, mean_q: 16.108721, mean_eps: 0.830758
  28387/150000: episode: 295, duration: 1.196s, episode steps: 137, steps per second: 115, episode reward:  8.597, mean reward:  0.063 [-100.000, 66.532], mean action: 1.474 [0.000, 3.000],  loss: 17.139973, mse: 833.093278, mean_q: 16.151113, mean_eps: 0.830092
  28507/150000: episode: 296, duration: 1.065s, episode steps: 120, steps per second: 113, episode reward: -140.728, mean reward: -1.173 [-100.000,  3.592], mean action: 1.642 [0.000, 3.000],  loss: 13.541054, mse: 854.261574, mean_q: 16.116192, mean_eps: 0.829321
  28624/150000: episode: 297, duration: 0.926s, episode steps: 117, steps per second: 126, episode reward: -110.637, mean reward: -0.946 [-100.000,  9.857], mean action: 1.615 [0.000, 3.000],  loss: 15.732567, mse: 866.760766, mean_q: 15.751431, mean_eps: 0.828610
  28751/150000: episode: 298, duration: 0.911s, episode steps: 127, steps per second: 139, episode reward: -280.748, mean reward: -2.211 [-100.000,  3.505], mean action: 1.512 [0.000, 3.000],  loss: 12.945031, mse: 843.917113, mean_q: 16.306648, mean_eps: 0.827878
  28844/150000: episode: 299, duration: 0.720s, episode steps:  93, steps per second: 129, episode reward: -90.611, mean reward: -0.974 [-100.000, 10.091], mean action: 1.656 [0.000, 3.000],  loss: 14.588699, mse: 862.795402, mean_q: 16.016958, mean_eps: 0.827218
  28918/150000: episode: 300, duration: 0.532s, episode steps:  74, steps per second: 139, episode reward: -108.601, mean reward: -1.468 [-100.000,  6.842], mean action: 1.635 [0.000, 3.000],  loss: 19.492938, mse: 866.955108, mean_q: 16.175313, mean_eps: 0.826717
  29063/150000: episode: 301, duration: 1.066s, episode steps: 145, steps per second: 136, episode reward: -55.142, mean reward: -0.380 [-100.000, 12.638], mean action: 1.600 [0.000, 3.000],  loss: 9.805760, mse: 858.946974, mean_q: 16.114042, mean_eps: 0.826060
  29141/150000: episode: 302, duration: 0.688s, episode steps:  78, steps per second: 113, episode reward: -67.037, mean reward: -0.859 [-100.000,  7.035], mean action: 1.551 [0.000, 3.000],  loss: 15.097204, mse: 861.637703, mean_q: 16.516479, mean_eps: 0.825391
  29211/150000: episode: 303, duration: 0.545s, episode steps:  70, steps per second: 128, episode reward: -114.839, mean reward: -1.641 [-100.000, 17.102], mean action: 1.543 [0.000, 3.000],  loss: 13.083987, mse: 912.472601, mean_q: 16.346907, mean_eps: 0.824947
  29327/150000: episode: 304, duration: 0.894s, episode steps: 116, steps per second: 130, episode reward: -69.738, mean reward: -0.601 [-100.000, 13.860], mean action: 1.621 [0.000, 3.000],  loss: 12.608258, mse: 905.981355, mean_q: 16.749393, mean_eps: 0.824389
  29410/150000: episode: 305, duration: 0.673s, episode steps:  83, steps per second: 123, episode reward: -61.314, mean reward: -0.739 [-100.000, 11.268], mean action: 1.349 [0.000, 3.000],  loss: 20.644095, mse: 889.676093, mean_q: 16.645390, mean_eps: 0.823792
  29497/150000: episode: 306, duration: 0.661s, episode steps:  87, steps per second: 132, episode reward: -99.952, mean reward: -1.149 [-100.000,  7.400], mean action: 1.621 [0.000, 3.000],  loss: 11.834204, mse: 928.545769, mean_q: 16.500362, mean_eps: 0.823282
  29594/150000: episode: 307, duration: 0.707s, episode steps:  97, steps per second: 137, episode reward: -75.664, mean reward: -0.780 [-100.000, 10.276], mean action: 1.619 [0.000, 3.000],  loss: 7.769256, mse: 886.013082, mean_q: 16.656353, mean_eps: 0.822730
  29661/150000: episode: 308, duration: 0.515s, episode steps:  67, steps per second: 130, episode reward: -49.426, mean reward: -0.738 [-100.000, 15.780], mean action: 1.731 [0.000, 3.000],  loss: 17.100312, mse: 918.386941, mean_q: 16.933576, mean_eps: 0.822238
  29726/150000: episode: 309, duration: 0.540s, episode steps:  65, steps per second: 120, episode reward: -127.970, mean reward: -1.969 [-100.000,  6.147], mean action: 1.554 [0.000, 3.000],  loss: 13.896546, mse: 918.768593, mean_q: 17.016117, mean_eps: 0.821842
  29804/150000: episode: 310, duration: 0.565s, episode steps:  78, steps per second: 138, episode reward: -42.246, mean reward: -0.542 [-100.000, 20.850], mean action: 1.769 [0.000, 3.000],  loss: 15.715308, mse: 915.227537, mean_q: 16.655037, mean_eps: 0.821413
  29882/150000: episode: 311, duration: 0.561s, episode steps:  78, steps per second: 139, episode reward: -70.685, mean reward: -0.906 [-100.000, 16.365], mean action: 1.628 [0.000, 3.000],  loss: 16.522198, mse: 923.165661, mean_q: 16.646131, mean_eps: 0.820945
  29964/150000: episode: 312, duration: 0.616s, episode steps:  82, steps per second: 133, episode reward: -81.354, mean reward: -0.992 [-100.000,  6.541], mean action: 1.622 [0.000, 3.000],  loss: 20.862630, mse: 888.854054, mean_q: 16.820824, mean_eps: 0.820465
  30031/150000: episode: 313, duration: 0.501s, episode steps:  67, steps per second: 134, episode reward: -73.105, mean reward: -1.091 [-100.000,  6.993], mean action: 1.507 [0.000, 3.000],  loss: 14.746124, mse: 921.412178, mean_q: 16.500067, mean_eps: 0.820018
  30118/150000: episode: 314, duration: 0.635s, episode steps:  87, steps per second: 137, episode reward: -78.145, mean reward: -0.898 [-100.000,  9.064], mean action: 1.862 [0.000, 3.000],  loss: 16.142923, mse: 952.652462, mean_q: 17.745838, mean_eps: 0.819556
  30186/150000: episode: 315, duration: 0.483s, episode steps:  68, steps per second: 141, episode reward: -62.302, mean reward: -0.916 [-100.000,  7.692], mean action: 1.456 [0.000, 3.000],  loss: 12.302778, mse: 944.199615, mean_q: 17.941864, mean_eps: 0.819091
  30293/150000: episode: 316, duration: 0.804s, episode steps: 107, steps per second: 133, episode reward: -74.811, mean reward: -0.699 [-100.000,  7.560], mean action: 1.579 [0.000, 3.000],  loss: 10.880263, mse: 939.686693, mean_q: 18.005686, mean_eps: 0.818566
  30371/150000: episode: 317, duration: 0.569s, episode steps:  78, steps per second: 137, episode reward: -77.178, mean reward: -0.989 [-100.000, 11.062], mean action: 1.551 [0.000, 3.000],  loss: 12.602041, mse: 981.474975, mean_q: 18.881437, mean_eps: 0.818011
  30498/150000: episode: 318, duration: 0.951s, episode steps: 127, steps per second: 134, episode reward: -188.005, mean reward: -1.480 [-100.000,  3.671], mean action: 1.496 [0.000, 3.000],  loss: 10.346130, mse: 978.230395, mean_q: 18.249132, mean_eps: 0.817396
  30584/150000: episode: 319, duration: 0.636s, episode steps:  86, steps per second: 135, episode reward: -97.423, mean reward: -1.133 [-100.000, 10.183], mean action: 1.453 [0.000, 3.000],  loss: 13.073452, mse: 967.334040, mean_q: 18.379982, mean_eps: 0.816757
  30687/150000: episode: 320, duration: 0.741s, episode steps: 103, steps per second: 139, episode reward: -97.081, mean reward: -0.943 [-100.000,  7.678], mean action: 1.505 [0.000, 3.000],  loss: 18.107401, mse: 980.671359, mean_q: 17.462059, mean_eps: 0.816190
  30762/150000: episode: 321, duration: 0.520s, episode steps:  75, steps per second: 144, episode reward: -104.212, mean reward: -1.389 [-100.000,  9.209], mean action: 1.493 [0.000, 3.000],  loss: 16.999259, mse: 967.984412, mean_q: 18.749879, mean_eps: 0.815656
  30839/150000: episode: 322, duration: 0.582s, episode steps:  77, steps per second: 132, episode reward: -2.609, mean reward: -0.034 [-100.000, 63.721], mean action: 1.468 [0.000, 3.000],  loss: 14.343565, mse: 982.891322, mean_q: 17.521098, mean_eps: 0.815200
  30960/150000: episode: 323, duration: 0.979s, episode steps: 121, steps per second: 124, episode reward: -85.241, mean reward: -0.704 [-100.000,  8.417], mean action: 1.562 [0.000, 3.000],  loss: 15.806980, mse: 945.323660, mean_q: 18.434175, mean_eps: 0.814606
  31057/150000: episode: 324, duration: 0.844s, episode steps:  97, steps per second: 115, episode reward: -11.265, mean reward: -0.116 [-100.000, 17.466], mean action: 1.784 [0.000, 3.000],  loss: 24.070622, mse: 1002.846946, mean_q: 18.426676, mean_eps: 0.813952
  31154/150000: episode: 325, duration: 0.798s, episode steps:  97, steps per second: 122, episode reward: -102.317, mean reward: -1.055 [-100.000, 16.622], mean action: 1.495 [0.000, 3.000],  loss: 15.713335, mse: 1016.753775, mean_q: 18.520690, mean_eps: 0.813370
  31254/150000: episode: 326, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: 12.358, mean reward:  0.124 [-100.000, 76.076], mean action: 1.430 [0.000, 3.000],  loss: 13.195710, mse: 1017.806764, mean_q: 17.954260, mean_eps: 0.812779
  31352/150000: episode: 327, duration: 0.804s, episode steps:  98, steps per second: 122, episode reward: -85.709, mean reward: -0.875 [-100.000,  6.562], mean action: 1.714 [0.000, 3.000],  loss: 14.680756, mse: 1044.320662, mean_q: 18.577205, mean_eps: 0.812185
  31416/150000: episode: 328, duration: 0.485s, episode steps:  64, steps per second: 132, episode reward: -81.658, mean reward: -1.276 [-100.000,  6.287], mean action: 1.641 [0.000, 3.000],  loss: 8.793927, mse: 1041.476888, mean_q: 19.116473, mean_eps: 0.811699
  31558/150000: episode: 329, duration: 1.078s, episode steps: 142, steps per second: 132, episode reward: -34.396, mean reward: -0.242 [-100.000, 57.264], mean action: 1.380 [0.000, 3.000],  loss: 16.184796, mse: 1055.820846, mean_q: 18.630550, mean_eps: 0.811081
  31641/150000: episode: 330, duration: 0.637s, episode steps:  83, steps per second: 130, episode reward: -230.075, mean reward: -2.772 [-100.000,  5.101], mean action: 1.699 [0.000, 3.000],  loss: 16.849369, mse: 1021.432715, mean_q: 19.095597, mean_eps: 0.810406
  31773/150000: episode: 331, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -53.980, mean reward: -0.409 [-100.000, 13.309], mean action: 1.614 [0.000, 3.000],  loss: 12.761849, mse: 1050.705360, mean_q: 18.220436, mean_eps: 0.809761
  31836/150000: episode: 332, duration: 0.456s, episode steps:  63, steps per second: 138, episode reward: -78.037, mean reward: -1.239 [-100.000,  9.275], mean action: 1.635 [0.000, 3.000],  loss: 19.097534, mse: 982.717478, mean_q: 19.319768, mean_eps: 0.809176
  31901/150000: episode: 333, duration: 0.473s, episode steps:  65, steps per second: 137, episode reward: -60.891, mean reward: -0.937 [-100.000,  7.280], mean action: 1.431 [0.000, 3.000],  loss: 19.578936, mse: 1013.278629, mean_q: 18.567370, mean_eps: 0.808792
  32010/150000: episode: 334, duration: 0.811s, episode steps: 109, steps per second: 134, episode reward: -127.661, mean reward: -1.171 [-100.000,  9.797], mean action: 1.514 [0.000, 3.000],  loss: 13.410262, mse: 1078.044311, mean_q: 17.974947, mean_eps: 0.808270
  32148/150000: episode: 335, duration: 0.994s, episode steps: 138, steps per second: 139, episode reward: -91.742, mean reward: -0.665 [-100.000, 10.709], mean action: 1.616 [0.000, 3.000],  loss: 11.432056, mse: 1112.131092, mean_q: 19.806219, mean_eps: 0.807529
  32283/150000: episode: 336, duration: 1.014s, episode steps: 135, steps per second: 133, episode reward: -97.115, mean reward: -0.719 [-100.000, 31.979], mean action: 1.519 [0.000, 3.000],  loss: 12.367984, mse: 1064.081672, mean_q: 19.672932, mean_eps: 0.806710
  32362/150000: episode: 337, duration: 0.569s, episode steps:  79, steps per second: 139, episode reward:  2.333, mean reward:  0.030 [-100.000, 89.929], mean action: 1.620 [0.000, 3.000],  loss: 14.715728, mse: 1063.728958, mean_q: 19.693797, mean_eps: 0.806068
  32424/150000: episode: 338, duration: 0.455s, episode steps:  62, steps per second: 136, episode reward: -15.374, mean reward: -0.248 [-100.000, 17.553], mean action: 1.565 [0.000, 3.000],  loss: 11.995793, mse: 1127.803625, mean_q: 20.308209, mean_eps: 0.805645
  32542/150000: episode: 339, duration: 0.899s, episode steps: 118, steps per second: 131, episode reward: -65.988, mean reward: -0.559 [-100.000, 13.219], mean action: 1.492 [0.000, 3.000],  loss: 16.220324, mse: 1051.093869, mean_q: 20.398613, mean_eps: 0.805105
  32647/150000: episode: 340, duration: 0.759s, episode steps: 105, steps per second: 138, episode reward: -225.030, mean reward: -2.143 [-100.000,  0.715], mean action: 1.562 [0.000, 3.000],  loss: 14.801528, mse: 1117.762665, mean_q: 20.503490, mean_eps: 0.804436
  32755/150000: episode: 341, duration: 0.759s, episode steps: 108, steps per second: 142, episode reward: -106.584, mean reward: -0.987 [-100.000, 11.568], mean action: 1.667 [0.000, 3.000],  loss: 19.890109, mse: 1071.794268, mean_q: 19.615837, mean_eps: 0.803797
  32852/150000: episode: 342, duration: 0.735s, episode steps:  97, steps per second: 132, episode reward: -102.675, mean reward: -1.059 [-100.000, 13.328], mean action: 1.464 [0.000, 3.000],  loss: 17.309011, mse: 1059.630531, mean_q: 19.249903, mean_eps: 0.803182
  32957/150000: episode: 343, duration: 0.766s, episode steps: 105, steps per second: 137, episode reward: -111.261, mean reward: -1.060 [-100.000, 20.703], mean action: 1.581 [0.000, 3.000],  loss: 9.454653, mse: 1106.154971, mean_q: 20.366404, mean_eps: 0.802576
  33085/150000: episode: 344, duration: 0.941s, episode steps: 128, steps per second: 136, episode reward:  2.201, mean reward:  0.017 [-100.000, 73.579], mean action: 1.484 [0.000, 3.000],  loss: 15.725658, mse: 1166.327693, mean_q: 20.746245, mean_eps: 0.801877
  33196/150000: episode: 345, duration: 0.823s, episode steps: 111, steps per second: 135, episode reward: -212.659, mean reward: -1.916 [-100.000,  1.093], mean action: 1.622 [0.000, 3.000],  loss: 13.852260, mse: 1183.875170, mean_q: 20.999676, mean_eps: 0.801160
  33299/150000: episode: 346, duration: 0.730s, episode steps: 103, steps per second: 141, episode reward: -128.449, mean reward: -1.247 [-100.000,  4.814], mean action: 1.777 [0.000, 3.000],  loss: 12.792091, mse: 1214.043522, mean_q: 21.761858, mean_eps: 0.800518
  33414/150000: episode: 347, duration: 0.867s, episode steps: 115, steps per second: 133, episode reward: -145.670, mean reward: -1.267 [-100.000,  6.349], mean action: 1.765 [0.000, 3.000],  loss: 18.744489, mse: 1214.340422, mean_q: 21.070800, mean_eps: 0.799864
  33485/150000: episode: 348, duration: 0.510s, episode steps:  71, steps per second: 139, episode reward: -69.597, mean reward: -0.980 [-100.000, 57.439], mean action: 1.366 [0.000, 3.000],  loss: 17.008671, mse: 1179.618631, mean_q: 20.453459, mean_eps: 0.799306
  33573/150000: episode: 349, duration: 0.630s, episode steps:  88, steps per second: 140, episode reward: -53.418, mean reward: -0.607 [-100.000,  7.363], mean action: 1.545 [0.000, 3.000],  loss: 18.730967, mse: 1228.257176, mean_q: 21.373930, mean_eps: 0.798829
  33667/150000: episode: 350, duration: 0.686s, episode steps:  94, steps per second: 137, episode reward: -78.351, mean reward: -0.834 [-100.000,  7.157], mean action: 1.426 [0.000, 3.000],  loss: 14.163659, mse: 1195.672878, mean_q: 21.419451, mean_eps: 0.798283
  33746/150000: episode: 351, duration: 0.596s, episode steps:  79, steps per second: 133, episode reward: -91.733, mean reward: -1.161 [-100.000,  8.273], mean action: 1.671 [0.000, 3.000],  loss: 14.892305, mse: 1189.193249, mean_q: 21.479049, mean_eps: 0.797764
  33830/150000: episode: 352, duration: 0.614s, episode steps:  84, steps per second: 137, episode reward: -79.473, mean reward: -0.946 [-100.000, 13.544], mean action: 1.536 [0.000, 3.000],  loss: 22.433875, mse: 1231.009107, mean_q: 20.697610, mean_eps: 0.797275
  33930/150000: episode: 353, duration: 0.753s, episode steps: 100, steps per second: 133, episode reward: -126.462, mean reward: -1.265 [-100.000,  7.091], mean action: 1.730 [0.000, 3.000],  loss: 13.824587, mse: 1211.765131, mean_q: 20.810273, mean_eps: 0.796723
  34015/150000: episode: 354, duration: 0.663s, episode steps:  85, steps per second: 128, episode reward: -45.390, mean reward: -0.534 [-100.000, 18.340], mean action: 1.894 [0.000, 3.000],  loss: 16.520551, mse: 1177.214216, mean_q: 21.720666, mean_eps: 0.796168
  34124/150000: episode: 355, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: -79.208, mean reward: -0.727 [-100.000,  8.878], mean action: 1.752 [0.000, 3.000],  loss: 15.746463, mse: 1212.713602, mean_q: 22.016192, mean_eps: 0.795586
  34197/150000: episode: 356, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -67.100, mean reward: -0.919 [-100.000, 10.952], mean action: 1.726 [0.000, 3.000],  loss: 22.749585, mse: 1264.701097, mean_q: 21.626814, mean_eps: 0.795040
  34296/150000: episode: 357, duration: 0.750s, episode steps:  99, steps per second: 132, episode reward: -148.475, mean reward: -1.500 [-100.000, 12.126], mean action: 1.485 [0.000, 3.000],  loss: 10.150674, mse: 1256.712502, mean_q: 21.782199, mean_eps: 0.794524
  34373/150000: episode: 358, duration: 0.562s, episode steps:  77, steps per second: 137, episode reward: -17.121, mean reward: -0.222 [-100.000, 93.401], mean action: 1.623 [0.000, 3.000],  loss: 15.283553, mse: 1241.974976, mean_q: 21.708283, mean_eps: 0.793996
  34484/150000: episode: 359, duration: 0.798s, episode steps: 111, steps per second: 139, episode reward: -93.257, mean reward: -0.840 [-100.000,  9.816], mean action: 1.405 [0.000, 3.000],  loss: 14.971693, mse: 1257.128248, mean_q: 21.605409, mean_eps: 0.793432
  34580/150000: episode: 360, duration: 0.855s, episode steps:  96, steps per second: 112, episode reward: -75.210, mean reward: -0.783 [-100.000,  6.876], mean action: 1.479 [0.000, 3.000],  loss: 11.147077, mse: 1266.415759, mean_q: 22.377053, mean_eps: 0.792811
  34709/150000: episode: 361, duration: 1.070s, episode steps: 129, steps per second: 121, episode reward: -114.038, mean reward: -0.884 [-100.000,  7.981], mean action: 1.659 [0.000, 3.000],  loss: 15.718241, mse: 1280.772836, mean_q: 21.750827, mean_eps: 0.792136
  34829/150000: episode: 362, duration: 1.226s, episode steps: 120, steps per second:  98, episode reward: -83.005, mean reward: -0.692 [-100.000, 11.667], mean action: 1.650 [0.000, 3.000],  loss: 19.222130, mse: 1287.786666, mean_q: 22.449206, mean_eps: 0.791389
  34915/150000: episode: 363, duration: 0.783s, episode steps:  86, steps per second: 110, episode reward: -122.777, mean reward: -1.428 [-100.000,  6.553], mean action: 1.605 [0.000, 3.000],  loss: 11.897646, mse: 1231.625319, mean_q: 22.263292, mean_eps: 0.790771
  35022/150000: episode: 364, duration: 1.283s, episode steps: 107, steps per second:  83, episode reward: -74.233, mean reward: -0.694 [-100.000, 18.259], mean action: 1.598 [0.000, 3.000],  loss: 12.431299, mse: 1270.821481, mean_q: 21.799069, mean_eps: 0.790192
  35107/150000: episode: 365, duration: 0.852s, episode steps:  85, steps per second: 100, episode reward: -92.468, mean reward: -1.088 [-100.000, 11.901], mean action: 1.612 [0.000, 3.000],  loss: 13.256923, mse: 1284.795675, mean_q: 22.104522, mean_eps: 0.789616
  35183/150000: episode: 366, duration: 0.883s, episode steps:  76, steps per second:  86, episode reward: -63.750, mean reward: -0.839 [-100.000, 15.469], mean action: 1.500 [0.000, 3.000],  loss: 14.431748, mse: 1338.818799, mean_q: 22.934379, mean_eps: 0.789133
  35301/150000: episode: 367, duration: 1.147s, episode steps: 118, steps per second: 103, episode reward: -92.096, mean reward: -0.780 [-100.000,  8.386], mean action: 1.500 [0.000, 3.000],  loss: 15.123478, mse: 1322.801055, mean_q: 22.434634, mean_eps: 0.788551
  35396/150000: episode: 368, duration: 0.998s, episode steps:  95, steps per second:  95, episode reward: -89.536, mean reward: -0.942 [-100.000, 11.287], mean action: 1.684 [0.000, 3.000],  loss: 23.117272, mse: 1356.218632, mean_q: 21.109276, mean_eps: 0.787912
  35506/150000: episode: 369, duration: 0.963s, episode steps: 110, steps per second: 114, episode reward: -64.057, mean reward: -0.582 [-100.000, 10.419], mean action: 1.436 [0.000, 3.000],  loss: 16.326659, mse: 1361.508216, mean_q: 22.180625, mean_eps: 0.787297
  35593/150000: episode: 370, duration: 1.046s, episode steps:  87, steps per second:  83, episode reward: -79.531, mean reward: -0.914 [-100.000, 11.770], mean action: 1.632 [0.000, 3.000],  loss: 17.404872, mse: 1341.167579, mean_q: 23.379677, mean_eps: 0.786706
  35688/150000: episode: 371, duration: 0.776s, episode steps:  95, steps per second: 122, episode reward: -119.260, mean reward: -1.255 [-100.000, 11.369], mean action: 1.442 [0.000, 3.000],  loss: 20.476436, mse: 1277.370265, mean_q: 22.595654, mean_eps: 0.786160
  35804/150000: episode: 372, duration: 1.079s, episode steps: 116, steps per second: 107, episode reward: -137.731, mean reward: -1.187 [-100.000, 16.943], mean action: 1.267 [0.000, 3.000],  loss: 21.587061, mse: 1364.096527, mean_q: 22.578714, mean_eps: 0.785527
  35915/150000: episode: 373, duration: 0.845s, episode steps: 111, steps per second: 131, episode reward: -135.078, mean reward: -1.217 [-100.000,  9.739], mean action: 1.577 [0.000, 3.000],  loss: 12.171118, mse: 1414.012546, mean_q: 22.181412, mean_eps: 0.784846
  36038/150000: episode: 374, duration: 0.888s, episode steps: 123, steps per second: 139, episode reward: -72.455, mean reward: -0.589 [-100.000,  9.080], mean action: 1.472 [0.000, 3.000],  loss: 12.008230, mse: 1360.495129, mean_q: 22.020637, mean_eps: 0.784144
  36176/150000: episode: 375, duration: 0.988s, episode steps: 138, steps per second: 140, episode reward: -263.815, mean reward: -1.912 [-100.000, 46.302], mean action: 1.659 [0.000, 3.000],  loss: 15.205977, mse: 1450.864678, mean_q: 22.569486, mean_eps: 0.783361
  36249/150000: episode: 376, duration: 0.561s, episode steps:  73, steps per second: 130, episode reward: -89.154, mean reward: -1.221 [-100.000,  8.653], mean action: 1.534 [0.000, 3.000],  loss: 9.690741, mse: 1434.816740, mean_q: 23.770602, mean_eps: 0.782728
  36333/150000: episode: 377, duration: 0.605s, episode steps:  84, steps per second: 139, episode reward: -89.606, mean reward: -1.067 [-100.000,  7.352], mean action: 1.738 [0.000, 3.000],  loss: 18.141695, mse: 1457.012567, mean_q: 23.219238, mean_eps: 0.782257
  36461/150000: episode: 378, duration: 0.887s, episode steps: 128, steps per second: 144, episode reward: -87.098, mean reward: -0.680 [-100.000, 12.503], mean action: 1.633 [0.000, 3.000],  loss: 13.987015, mse: 1450.996193, mean_q: 22.845251, mean_eps: 0.781621
  36539/150000: episode: 379, duration: 0.602s, episode steps:  78, steps per second: 130, episode reward: -111.948, mean reward: -1.435 [-100.000,  8.146], mean action: 1.526 [0.000, 3.000],  loss: 26.721796, mse: 1488.373607, mean_q: 22.790588, mean_eps: 0.781003
  36625/150000: episode: 380, duration: 0.618s, episode steps:  86, steps per second: 139, episode reward: -123.028, mean reward: -1.431 [-100.000, 13.163], mean action: 1.291 [0.000, 3.000],  loss: 18.209533, mse: 1436.915056, mean_q: 23.974129, mean_eps: 0.780511
  36754/150000: episode: 381, duration: 0.950s, episode steps: 129, steps per second: 136, episode reward: -64.029, mean reward: -0.496 [-100.000, 11.562], mean action: 1.628 [0.000, 3.000],  loss: 18.062580, mse: 1442.456465, mean_q: 23.336030, mean_eps: 0.779866
  36858/150000: episode: 382, duration: 0.821s, episode steps: 104, steps per second: 127, episode reward: -85.381, mean reward: -0.821 [-100.000, 15.735], mean action: 1.500 [0.000, 3.000],  loss: 15.313798, mse: 1493.088486, mean_q: 23.607617, mean_eps: 0.779167
  36945/150000: episode: 383, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -87.809, mean reward: -1.009 [-100.000, 11.180], mean action: 1.425 [0.000, 3.000],  loss: 12.223830, mse: 1476.761431, mean_q: 23.704728, mean_eps: 0.778594
  37067/150000: episode: 384, duration: 0.900s, episode steps: 122, steps per second: 136, episode reward: -124.135, mean reward: -1.017 [-100.000,  6.927], mean action: 1.582 [0.000, 3.000],  loss: 13.899904, mse: 1507.312058, mean_q: 24.124041, mean_eps: 0.777967
  37158/150000: episode: 385, duration: 0.697s, episode steps:  91, steps per second: 130, episode reward: -86.708, mean reward: -0.953 [-100.000,  8.377], mean action: 1.484 [0.000, 3.000],  loss: 17.213241, mse: 1513.120479, mean_q: 24.958563, mean_eps: 0.777328
  37237/150000: episode: 386, duration: 0.588s, episode steps:  79, steps per second: 134, episode reward: -73.650, mean reward: -0.932 [-100.000, 11.903], mean action: 1.430 [0.000, 3.000],  loss: 22.301874, mse: 1493.069239, mean_q: 24.313659, mean_eps: 0.776818
  37343/150000: episode: 387, duration: 0.747s, episode steps: 106, steps per second: 142, episode reward: -119.074, mean reward: -1.123 [-100.000,  7.301], mean action: 1.613 [0.000, 3.000],  loss: 14.881719, mse: 1528.087215, mean_q: 23.703732, mean_eps: 0.776263
  37456/150000: episode: 388, duration: 0.911s, episode steps: 113, steps per second: 124, episode reward: -48.810, mean reward: -0.432 [-100.000,  6.811], mean action: 1.407 [0.000, 3.000],  loss: 15.540171, mse: 1550.400484, mean_q: 23.803953, mean_eps: 0.775606
  37545/150000: episode: 389, duration: 0.719s, episode steps:  89, steps per second: 124, episode reward: -43.283, mean reward: -0.486 [-100.000, 45.861], mean action: 1.596 [0.000, 3.000],  loss: 10.142616, mse: 1552.770240, mean_q: 24.303204, mean_eps: 0.775000
  37613/150000: episode: 390, duration: 0.555s, episode steps:  68, steps per second: 123, episode reward: -44.276, mean reward: -0.651 [-100.000, 16.499], mean action: 1.559 [0.000, 3.000],  loss: 12.376922, mse: 1514.644102, mean_q: 24.027455, mean_eps: 0.774529
  37688/150000: episode: 391, duration: 0.599s, episode steps:  75, steps per second: 125, episode reward: -82.750, mean reward: -1.103 [-100.000,  7.260], mean action: 1.720 [0.000, 3.000],  loss: 15.176018, mse: 1546.444310, mean_q: 24.456411, mean_eps: 0.774100
  37802/150000: episode: 392, duration: 0.910s, episode steps: 114, steps per second: 125, episode reward: -68.737, mean reward: -0.603 [-100.000,  6.994], mean action: 1.667 [0.000, 3.000],  loss: 13.858180, mse: 1551.889368, mean_q: 24.047670, mean_eps: 0.773533
  38802/150000: episode: 393, duration: 7.743s, episode steps: 1000, steps per second: 129, episode reward: 84.652, mean reward:  0.085 [-22.314, 48.254], mean action: 1.738 [0.000, 3.000],  loss: 16.396752, mse: 1602.416255, mean_q: 24.443312, mean_eps: 0.770191
  38882/150000: episode: 394, duration: 0.557s, episode steps:  80, steps per second: 144, episode reward: -63.430, mean reward: -0.793 [-100.000,  6.041], mean action: 1.600 [0.000, 3.000],  loss: 19.387987, mse: 1557.323749, mean_q: 25.971944, mean_eps: 0.766951
  39004/150000: episode: 395, duration: 0.856s, episode steps: 122, steps per second: 142, episode reward: -75.908, mean reward: -0.622 [-100.000,  7.622], mean action: 1.656 [0.000, 3.000],  loss: 15.414422, mse: 1713.282324, mean_q: 24.422347, mean_eps: 0.766345
  39093/150000: episode: 396, duration: 0.674s, episode steps:  89, steps per second: 132, episode reward: -54.770, mean reward: -0.615 [-100.000, 20.733], mean action: 1.562 [0.000, 3.000],  loss: 13.308214, mse: 1736.810797, mean_q: 25.673031, mean_eps: 0.765712
  39193/150000: episode: 397, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: -79.616, mean reward: -0.796 [-100.000, 22.523], mean action: 1.570 [0.000, 3.000],  loss: 14.260272, mse: 1738.723573, mean_q: 25.353923, mean_eps: 0.765145
  39322/150000: episode: 398, duration: 0.924s, episode steps: 129, steps per second: 140, episode reward: -70.867, mean reward: -0.549 [-100.000,  6.849], mean action: 1.667 [0.000, 3.000],  loss: 13.360684, mse: 1723.094502, mean_q: 25.720454, mean_eps: 0.764458
  39435/150000: episode: 399, duration: 1.017s, episode steps: 113, steps per second: 111, episode reward: -206.571, mean reward: -1.828 [-100.000,  4.412], mean action: 1.770 [0.000, 3.000],  loss: 18.454414, mse: 1767.234859, mean_q: 25.005027, mean_eps: 0.763732
  39512/150000: episode: 400, duration: 0.632s, episode steps:  77, steps per second: 122, episode reward: -120.551, mean reward: -1.566 [-100.000,  7.180], mean action: 1.442 [0.000, 3.000],  loss: 12.625401, mse: 1721.207393, mean_q: 25.094502, mean_eps: 0.763162
  39607/150000: episode: 401, duration: 0.776s, episode steps:  95, steps per second: 122, episode reward: -62.316, mean reward: -0.656 [-100.000, 21.701], mean action: 1.621 [0.000, 3.000],  loss: 11.431397, mse: 1727.128973, mean_q: 25.788361, mean_eps: 0.762646
  39682/150000: episode: 402, duration: 0.595s, episode steps:  75, steps per second: 126, episode reward: -91.694, mean reward: -1.223 [-100.000,  8.019], mean action: 1.627 [0.000, 3.000],  loss: 18.179491, mse: 1771.856183, mean_q: 24.948835, mean_eps: 0.762136
  39820/150000: episode: 403, duration: 1.088s, episode steps: 138, steps per second: 127, episode reward: -172.074, mean reward: -1.247 [-100.000, 13.226], mean action: 1.616 [0.000, 3.000],  loss: 16.793389, mse: 1746.017438, mean_q: 25.617215, mean_eps: 0.761497
  39922/150000: episode: 404, duration: 0.924s, episode steps: 102, steps per second: 110, episode reward: -109.263, mean reward: -1.071 [-100.000,  6.125], mean action: 1.520 [0.000, 3.000],  loss: 22.704115, mse: 1715.452929, mean_q: 25.791680, mean_eps: 0.760777
  39995/150000: episode: 405, duration: 0.543s, episode steps:  73, steps per second: 134, episode reward: -66.790, mean reward: -0.915 [-100.000, 16.971], mean action: 1.493 [0.000, 3.000],  loss: 12.960268, mse: 1718.047980, mean_q: 26.297784, mean_eps: 0.760252
  40090/150000: episode: 406, duration: 0.670s, episode steps:  95, steps per second: 142, episode reward: -73.563, mean reward: -0.774 [-100.000, 22.664], mean action: 1.558 [0.000, 3.000],  loss: 23.255380, mse: 1740.793132, mean_q: 26.384897, mean_eps: 0.759748
  40239/150000: episode: 407, duration: 1.094s, episode steps: 149, steps per second: 136, episode reward: -67.152, mean reward: -0.451 [-100.000,  5.982], mean action: 1.705 [0.000, 3.000],  loss: 15.003164, mse: 1824.891334, mean_q: 24.933666, mean_eps: 0.759016
  40340/150000: episode: 408, duration: 0.709s, episode steps: 101, steps per second: 143, episode reward: -47.456, mean reward: -0.470 [-100.000, 32.965], mean action: 1.465 [0.000, 3.000],  loss: 13.728131, mse: 1799.951515, mean_q: 25.194533, mean_eps: 0.758266
  40431/150000: episode: 409, duration: 0.647s, episode steps:  91, steps per second: 141, episode reward: -89.262, mean reward: -0.981 [-100.000, 10.358], mean action: 1.703 [0.000, 3.000],  loss: 18.694832, mse: 1819.141382, mean_q: 26.454485, mean_eps: 0.757690
  40540/150000: episode: 410, duration: 0.799s, episode steps: 109, steps per second: 136, episode reward: -65.406, mean reward: -0.600 [-100.000, 14.059], mean action: 1.587 [0.000, 3.000],  loss: 15.351102, mse: 1792.310736, mean_q: 24.661800, mean_eps: 0.757090
  40627/150000: episode: 411, duration: 0.624s, episode steps:  87, steps per second: 139, episode reward: -83.821, mean reward: -0.963 [-100.000, 10.993], mean action: 1.517 [0.000, 3.000],  loss: 14.457002, mse: 1808.245664, mean_q: 26.152810, mean_eps: 0.756502
  40770/150000: episode: 412, duration: 1.026s, episode steps: 143, steps per second: 139, episode reward: -87.652, mean reward: -0.613 [-100.000, 12.442], mean action: 1.615 [0.000, 3.000],  loss: 17.946071, mse: 1861.984463, mean_q: 25.600281, mean_eps: 0.755812
  40856/150000: episode: 413, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward: -105.950, mean reward: -1.232 [-100.000,  9.195], mean action: 1.767 [0.000, 3.000],  loss: 12.989737, mse: 1804.177107, mean_q: 26.101595, mean_eps: 0.755125
  40995/150000: episode: 414, duration: 0.975s, episode steps: 139, steps per second: 143, episode reward: -45.470, mean reward: -0.327 [-100.000,  7.755], mean action: 1.576 [0.000, 3.000],  loss: 19.702523, mse: 1777.436841, mean_q: 26.425391, mean_eps: 0.754450
  41103/150000: episode: 415, duration: 0.800s, episode steps: 108, steps per second: 135, episode reward: -69.358, mean reward: -0.642 [-100.000, 11.092], mean action: 1.713 [0.000, 3.000],  loss: 13.016460, mse: 1848.232563, mean_q: 25.858563, mean_eps: 0.753709
  41212/150000: episode: 416, duration: 0.759s, episode steps: 109, steps per second: 144, episode reward: -81.109, mean reward: -0.744 [-100.000,  8.363], mean action: 1.450 [0.000, 3.000],  loss: 17.525995, mse: 1868.642758, mean_q: 26.539590, mean_eps: 0.753058
  41325/150000: episode: 417, duration: 0.793s, episode steps: 113, steps per second: 142, episode reward: -167.560, mean reward: -1.483 [-100.000, 10.018], mean action: 1.504 [0.000, 3.000],  loss: 15.151762, mse: 1872.957313, mean_q: 25.632711, mean_eps: 0.752392
  41427/150000: episode: 418, duration: 0.787s, episode steps: 102, steps per second: 130, episode reward: -97.619, mean reward: -0.957 [-100.000,  8.968], mean action: 1.588 [0.000, 3.000],  loss: 15.198815, mse: 1913.363310, mean_q: 25.107424, mean_eps: 0.751747
  41531/150000: episode: 419, duration: 0.743s, episode steps: 104, steps per second: 140, episode reward: -74.579, mean reward: -0.717 [-100.000,  9.167], mean action: 1.510 [0.000, 3.000],  loss: 16.817078, mse: 1852.749252, mean_q: 26.452403, mean_eps: 0.751129
  41602/150000: episode: 420, duration: 0.497s, episode steps:  71, steps per second: 143, episode reward: -67.606, mean reward: -0.952 [-100.000, 13.765], mean action: 1.634 [0.000, 3.000],  loss: 12.168122, mse: 1878.192589, mean_q: 25.096175, mean_eps: 0.750604
  41699/150000: episode: 421, duration: 0.746s, episode steps:  97, steps per second: 130, episode reward: -110.613, mean reward: -1.140 [-100.000,  5.889], mean action: 1.588 [0.000, 3.000],  loss: 18.544342, mse: 1860.653339, mean_q: 26.982840, mean_eps: 0.750100
  41817/150000: episode: 422, duration: 0.858s, episode steps: 118, steps per second: 138, episode reward: -40.298, mean reward: -0.342 [-100.000, 12.519], mean action: 1.678 [0.000, 3.000],  loss: 11.505893, mse: 1854.892072, mean_q: 25.929076, mean_eps: 0.749455
  41930/150000: episode: 423, duration: 0.834s, episode steps: 113, steps per second: 135, episode reward: -63.554, mean reward: -0.562 [-100.000, 13.290], mean action: 1.416 [0.000, 3.000],  loss: 11.972202, mse: 1831.651033, mean_q: 26.523522, mean_eps: 0.748762
  42013/150000: episode: 424, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -94.364, mean reward: -1.137 [-100.000,  9.366], mean action: 1.711 [0.000, 3.000],  loss: 13.145909, mse: 1924.011739, mean_q: 25.653886, mean_eps: 0.748174
  42119/150000: episode: 425, duration: 0.739s, episode steps: 106, steps per second: 143, episode reward: -100.986, mean reward: -0.953 [-100.000, 16.691], mean action: 1.368 [0.000, 3.000],  loss: 15.217186, mse: 1999.750385, mean_q: 26.130763, mean_eps: 0.747607
  42188/150000: episode: 426, duration: 0.489s, episode steps:  69, steps per second: 141, episode reward: -94.574, mean reward: -1.371 [-100.000,  6.435], mean action: 1.406 [0.000, 3.000],  loss: 14.711562, mse: 2084.492249, mean_q: 26.625065, mean_eps: 0.747082
  42258/150000: episode: 427, duration: 0.542s, episode steps:  70, steps per second: 129, episode reward: -56.824, mean reward: -0.812 [-100.000,  7.148], mean action: 1.786 [0.000, 3.000],  loss: 11.354913, mse: 1918.737474, mean_q: 26.725747, mean_eps: 0.746665
  42318/150000: episode: 428, duration: 0.435s, episode steps:  60, steps per second: 138, episode reward: -95.471, mean reward: -1.591 [-100.000, 37.620], mean action: 1.650 [0.000, 3.000],  loss: 14.151990, mse: 1921.664677, mean_q: 27.769165, mean_eps: 0.746275
  42435/150000: episode: 429, duration: 0.820s, episode steps: 117, steps per second: 143, episode reward: -162.996, mean reward: -1.393 [-100.000,  2.303], mean action: 1.684 [0.000, 3.000],  loss: 11.359119, mse: 1984.108466, mean_q: 26.987934, mean_eps: 0.745744
  42535/150000: episode: 430, duration: 0.726s, episode steps: 100, steps per second: 138, episode reward: -167.184, mean reward: -1.672 [-100.000,  5.052], mean action: 1.670 [0.000, 3.000],  loss: 17.731642, mse: 1983.510978, mean_q: 26.385586, mean_eps: 0.745093
  42648/150000: episode: 431, duration: 0.850s, episode steps: 113, steps per second: 133, episode reward: -51.542, mean reward: -0.456 [-100.000, 12.889], mean action: 1.575 [0.000, 3.000],  loss: 15.460823, mse: 2013.988517, mean_q: 25.420728, mean_eps: 0.744454
  42739/150000: episode: 432, duration: 0.663s, episode steps:  91, steps per second: 137, episode reward: -113.212, mean reward: -1.244 [-100.000,  5.670], mean action: 1.824 [0.000, 3.000],  loss: 8.660168, mse: 1982.770358, mean_q: 27.490172, mean_eps: 0.743842
  42801/150000: episode: 433, duration: 0.444s, episode steps:  62, steps per second: 140, episode reward: -79.395, mean reward: -1.281 [-100.000, 10.053], mean action: 1.452 [0.000, 3.000],  loss: 17.160874, mse: 2034.962215, mean_q: 26.022222, mean_eps: 0.743383
  42911/150000: episode: 434, duration: 0.823s, episode steps: 110, steps per second: 134, episode reward: -44.569, mean reward: -0.405 [-100.000, 13.763], mean action: 1.445 [0.000, 3.000],  loss: 11.378973, mse: 1956.270603, mean_q: 27.054600, mean_eps: 0.742867
  43040/150000: episode: 435, duration: 0.912s, episode steps: 129, steps per second: 141, episode reward: -77.544, mean reward: -0.601 [-100.000, 15.758], mean action: 1.651 [0.000, 3.000],  loss: 12.893525, mse: 2094.914572, mean_q: 26.324724, mean_eps: 0.742150
  43135/150000: episode: 436, duration: 0.709s, episode steps:  95, steps per second: 134, episode reward: -49.370, mean reward: -0.520 [-100.000, 12.519], mean action: 1.558 [0.000, 3.000],  loss: 19.401855, mse: 2040.223349, mean_q: 26.398145, mean_eps: 0.741478
  43223/150000: episode: 437, duration: 0.639s, episode steps:  88, steps per second: 138, episode reward: -79.294, mean reward: -0.901 [-100.000, 10.305], mean action: 1.591 [0.000, 3.000],  loss: 15.108346, mse: 2110.830120, mean_q: 26.281590, mean_eps: 0.740929
  43322/150000: episode: 438, duration: 0.707s, episode steps:  99, steps per second: 140, episode reward: -83.636, mean reward: -0.845 [-100.000, 11.994], mean action: 1.475 [0.000, 3.000],  loss: 11.997675, mse: 2098.710081, mean_q: 27.147289, mean_eps: 0.740368
  43436/150000: episode: 439, duration: 0.846s, episode steps: 114, steps per second: 135, episode reward: -12.912, mean reward: -0.113 [-100.000, 10.517], mean action: 1.781 [0.000, 3.000],  loss: 16.969638, mse: 2110.195443, mean_q: 26.952299, mean_eps: 0.739729
  43528/150000: episode: 440, duration: 0.680s, episode steps:  92, steps per second: 135, episode reward: -38.976, mean reward: -0.424 [-100.000,  7.504], mean action: 1.804 [0.000, 3.000],  loss: 17.480743, mse: 2043.738094, mean_q: 28.007798, mean_eps: 0.739111
  43640/150000: episode: 441, duration: 0.793s, episode steps: 112, steps per second: 141, episode reward: -123.881, mean reward: -1.106 [-100.000,  7.243], mean action: 1.473 [0.000, 3.000],  loss: 17.219748, mse: 2087.344862, mean_q: 27.008055, mean_eps: 0.738499
  43737/150000: episode: 442, duration: 0.702s, episode steps:  97, steps per second: 138, episode reward: -107.754, mean reward: -1.111 [-100.000, 14.049], mean action: 1.608 [0.000, 3.000],  loss: 15.396252, mse: 2114.860597, mean_q: 27.391272, mean_eps: 0.737872
  43850/150000: episode: 443, duration: 0.814s, episode steps: 113, steps per second: 139, episode reward: -89.804, mean reward: -0.795 [-100.000,  6.651], mean action: 1.646 [0.000, 3.000],  loss: 15.376794, mse: 2161.547584, mean_q: 27.215266, mean_eps: 0.737242
  43958/150000: episode: 444, duration: 0.778s, episode steps: 108, steps per second: 139, episode reward: -101.072, mean reward: -0.936 [-100.000, 21.158], mean action: 1.537 [0.000, 3.000],  loss: 13.953849, mse: 2162.536431, mean_q: 26.527463, mean_eps: 0.736579
  44034/150000: episode: 445, duration: 0.598s, episode steps:  76, steps per second: 127, episode reward: -101.175, mean reward: -1.331 [-100.000, 18.452], mean action: 1.671 [0.000, 3.000],  loss: 16.325176, mse: 2165.783325, mean_q: 26.858677, mean_eps: 0.736027
  44124/150000: episode: 446, duration: 0.686s, episode steps:  90, steps per second: 131, episode reward: -87.360, mean reward: -0.971 [-100.000,  7.845], mean action: 1.600 [0.000, 3.000],  loss: 20.533532, mse: 2184.196714, mean_q: 28.247583, mean_eps: 0.735529
  44225/150000: episode: 447, duration: 0.722s, episode steps: 101, steps per second: 140, episode reward: -56.404, mean reward: -0.558 [-100.000, 14.078], mean action: 1.703 [0.000, 3.000],  loss: 15.266966, mse: 2184.181858, mean_q: 27.223891, mean_eps: 0.734956
  44305/150000: episode: 448, duration: 0.576s, episode steps:  80, steps per second: 139, episode reward: -74.683, mean reward: -0.934 [-100.000, 21.385], mean action: 1.500 [0.000, 3.000],  loss: 19.477640, mse: 2265.039261, mean_q: 28.195955, mean_eps: 0.734413
  44424/150000: episode: 449, duration: 0.875s, episode steps: 119, steps per second: 136, episode reward: -65.590, mean reward: -0.551 [-100.000, 14.186], mean action: 1.462 [0.000, 3.000],  loss: 17.724994, mse: 2227.691308, mean_q: 26.908712, mean_eps: 0.733816
  44547/150000: episode: 450, duration: 0.844s, episode steps: 123, steps per second: 146, episode reward: -88.993, mean reward: -0.724 [-100.000,  7.511], mean action: 1.545 [0.000, 3.000],  loss: 14.711209, mse: 2194.862663, mean_q: 28.255622, mean_eps: 0.733090
  44630/150000: episode: 451, duration: 0.613s, episode steps:  83, steps per second: 135, episode reward: -63.773, mean reward: -0.768 [-100.000, 13.002], mean action: 1.398 [0.000, 3.000],  loss: 18.730173, mse: 2221.475565, mean_q: 29.348943, mean_eps: 0.732472
  44736/150000: episode: 452, duration: 0.770s, episode steps: 106, steps per second: 138, episode reward: -128.423, mean reward: -1.212 [-100.000,  9.625], mean action: 1.811 [0.000, 3.000],  loss: 11.931926, mse: 2216.472988, mean_q: 29.010060, mean_eps: 0.731905
  44826/150000: episode: 453, duration: 0.645s, episode steps:  90, steps per second: 139, episode reward: -116.684, mean reward: -1.296 [-100.000, 12.322], mean action: 1.467 [0.000, 3.000],  loss: 14.104776, mse: 2277.079944, mean_q: 28.170471, mean_eps: 0.731317
  44947/150000: episode: 454, duration: 0.893s, episode steps: 121, steps per second: 135, episode reward: -117.235, mean reward: -0.969 [-100.000, 10.949], mean action: 1.570 [0.000, 3.000],  loss: 12.455562, mse: 2244.158846, mean_q: 27.489025, mean_eps: 0.730684
  45036/150000: episode: 455, duration: 0.638s, episode steps:  89, steps per second: 140, episode reward: -75.644, mean reward: -0.850 [-100.000, 11.907], mean action: 1.674 [0.000, 3.000],  loss: 12.264102, mse: 2284.117790, mean_q: 29.096127, mean_eps: 0.730054
  45129/150000: episode: 456, duration: 0.642s, episode steps:  93, steps per second: 145, episode reward: -44.109, mean reward: -0.474 [-100.000,  7.234], mean action: 1.527 [0.000, 3.000],  loss: 21.021770, mse: 2357.900556, mean_q: 30.027681, mean_eps: 0.729508
  45237/150000: episode: 457, duration: 0.803s, episode steps: 108, steps per second: 135, episode reward: -52.842, mean reward: -0.489 [-100.000, 12.002], mean action: 1.593 [0.000, 3.000],  loss: 12.197664, mse: 2285.945419, mean_q: 28.669503, mean_eps: 0.728905
  45309/150000: episode: 458, duration: 0.518s, episode steps:  72, steps per second: 139, episode reward: -89.353, mean reward: -1.241 [-100.000,  6.583], mean action: 1.542 [0.000, 3.000],  loss: 22.972105, mse: 2295.769168, mean_q: 29.341561, mean_eps: 0.728365
  45429/150000: episode: 459, duration: 0.829s, episode steps: 120, steps per second: 145, episode reward: -82.972, mean reward: -0.691 [-100.000, 27.163], mean action: 1.425 [0.000, 3.000],  loss: 18.080571, mse: 2344.689912, mean_q: 28.333741, mean_eps: 0.727789
  45522/150000: episode: 460, duration: 0.692s, episode steps:  93, steps per second: 134, episode reward: -95.587, mean reward: -1.028 [-100.000,  6.065], mean action: 1.602 [0.000, 3.000],  loss: 17.948799, mse: 2360.642772, mean_q: 28.364919, mean_eps: 0.727150
  45642/150000: episode: 461, duration: 0.883s, episode steps: 120, steps per second: 136, episode reward: -38.750, mean reward: -0.323 [-100.000, 14.107], mean action: 1.533 [0.000, 3.000],  loss: 16.651844, mse: 2280.848715, mean_q: 28.767650, mean_eps: 0.726511
  45772/150000: episode: 462, duration: 0.900s, episode steps: 130, steps per second: 144, episode reward: -98.255, mean reward: -0.756 [-100.000, 10.898], mean action: 1.485 [0.000, 3.000],  loss: 21.012096, mse: 2374.030277, mean_q: 29.155347, mean_eps: 0.725761
  45896/150000: episode: 463, duration: 0.907s, episode steps: 124, steps per second: 137, episode reward: -181.101, mean reward: -1.460 [-100.000,  8.625], mean action: 1.589 [0.000, 3.000],  loss: 15.198512, mse: 2312.507969, mean_q: 28.512704, mean_eps: 0.724999
  46015/150000: episode: 464, duration: 0.851s, episode steps: 119, steps per second: 140, episode reward: -117.866, mean reward: -0.990 [-100.000,  7.431], mean action: 1.689 [0.000, 3.000],  loss: 17.845620, mse: 2384.946964, mean_q: 28.123571, mean_eps: 0.724270
  46118/150000: episode: 465, duration: 0.762s, episode steps: 103, steps per second: 135, episode reward: -114.441, mean reward: -1.111 [-100.000,  6.156], mean action: 1.689 [0.000, 3.000],  loss: 16.116462, mse: 2382.929953, mean_q: 29.625574, mean_eps: 0.723604
  46228/150000: episode: 466, duration: 0.804s, episode steps: 110, steps per second: 137, episode reward: -61.702, mean reward: -0.561 [-100.000,  8.251], mean action: 1.591 [0.000, 3.000],  loss: 17.664963, mse: 2400.356446, mean_q: 28.574688, mean_eps: 0.722965
  46320/150000: episode: 467, duration: 0.642s, episode steps:  92, steps per second: 143, episode reward: -65.197, mean reward: -0.709 [-100.000, 11.275], mean action: 1.641 [0.000, 3.000],  loss: 18.873616, mse: 2457.682461, mean_q: 29.288965, mean_eps: 0.722359
  46406/150000: episode: 468, duration: 0.716s, episode steps:  86, steps per second: 120, episode reward: -107.013, mean reward: -1.244 [-100.000, 47.023], mean action: 1.523 [0.000, 3.000],  loss: 24.360637, mse: 2427.472199, mean_q: 29.294497, mean_eps: 0.721825
  46535/150000: episode: 469, duration: 1.011s, episode steps: 129, steps per second: 128, episode reward: -93.976, mean reward: -0.728 [-100.000, 10.938], mean action: 1.450 [0.000, 3.000],  loss: 11.304529, mse: 2395.176765, mean_q: 29.093250, mean_eps: 0.721180
  46640/150000: episode: 470, duration: 0.781s, episode steps: 105, steps per second: 134, episode reward: -79.856, mean reward: -0.761 [-100.000,  7.126], mean action: 1.552 [0.000, 3.000],  loss: 14.671216, mse: 2390.639931, mean_q: 29.527704, mean_eps: 0.720478
  46787/150000: episode: 471, duration: 1.187s, episode steps: 147, steps per second: 124, episode reward: -63.589, mean reward: -0.433 [-100.000, 39.787], mean action: 1.667 [0.000, 3.000],  loss: 14.953657, mse: 2427.874738, mean_q: 29.769854, mean_eps: 0.719722
  46861/150000: episode: 472, duration: 0.720s, episode steps:  74, steps per second: 103, episode reward: -42.908, mean reward: -0.580 [-100.000,  8.953], mean action: 1.851 [0.000, 3.000],  loss: 12.878422, mse: 2414.232932, mean_q: 30.629633, mean_eps: 0.719059
  46970/150000: episode: 473, duration: 0.853s, episode steps: 109, steps per second: 128, episode reward: -172.671, mean reward: -1.584 [-100.000, 34.089], mean action: 1.642 [0.000, 3.000],  loss: 21.334043, mse: 2443.729946, mean_q: 27.796156, mean_eps: 0.718510
  47094/150000: episode: 474, duration: 0.908s, episode steps: 124, steps per second: 137, episode reward: -74.874, mean reward: -0.604 [-100.000,  9.456], mean action: 1.694 [0.000, 3.000],  loss: 16.725742, mse: 2544.063368, mean_q: 29.792887, mean_eps: 0.717811
  47217/150000: episode: 475, duration: 0.897s, episode steps: 123, steps per second: 137, episode reward: -15.771, mean reward: -0.128 [-100.000, 11.203], mean action: 1.805 [0.000, 3.000],  loss: 17.706019, mse: 2557.055475, mean_q: 30.057274, mean_eps: 0.717070
  47313/150000: episode: 476, duration: 0.707s, episode steps:  96, steps per second: 136, episode reward: -230.188, mean reward: -2.398 [-100.000,  7.327], mean action: 1.625 [0.000, 3.000],  loss: 14.204874, mse: 2591.822451, mean_q: 30.635976, mean_eps: 0.716413
  47420/150000: episode: 477, duration: 0.754s, episode steps: 107, steps per second: 142, episode reward: -77.545, mean reward: -0.725 [-100.000,  8.945], mean action: 1.467 [0.000, 3.000],  loss: 13.018776, mse: 2577.696274, mean_q: 30.727275, mean_eps: 0.715804
  47488/150000: episode: 478, duration: 0.481s, episode steps:  68, steps per second: 141, episode reward: -103.299, mean reward: -1.519 [-100.000,  6.167], mean action: 1.647 [0.000, 3.000],  loss: 18.315240, mse: 2591.664459, mean_q: 30.310616, mean_eps: 0.715279
  47612/150000: episode: 479, duration: 0.919s, episode steps: 124, steps per second: 135, episode reward: -114.419, mean reward: -0.923 [-100.000, 17.396], mean action: 1.476 [0.000, 3.000],  loss: 13.964874, mse: 2566.439176, mean_q: 30.424217, mean_eps: 0.714703
  47707/150000: episode: 480, duration: 0.662s, episode steps:  95, steps per second: 144, episode reward: -52.472, mean reward: -0.552 [-100.000,  8.912], mean action: 1.674 [0.000, 3.000],  loss: 17.126482, mse: 2498.882821, mean_q: 30.906257, mean_eps: 0.714046
  47791/150000: episode: 481, duration: 0.588s, episode steps:  84, steps per second: 143, episode reward: -68.181, mean reward: -0.812 [-100.000, 14.250], mean action: 1.607 [0.000, 3.000],  loss: 14.756522, mse: 2619.586250, mean_q: 29.408251, mean_eps: 0.713509
  47910/150000: episode: 482, duration: 0.888s, episode steps: 119, steps per second: 134, episode reward: -24.682, mean reward: -0.207 [-100.000, 17.546], mean action: 1.630 [0.000, 3.000],  loss: 12.702815, mse: 2616.667932, mean_q: 30.717429, mean_eps: 0.712900
  48022/150000: episode: 483, duration: 0.789s, episode steps: 112, steps per second: 142, episode reward: -76.457, mean reward: -0.683 [-100.000, 16.021], mean action: 1.652 [0.000, 3.000],  loss: 13.539531, mse: 2535.240882, mean_q: 30.714417, mean_eps: 0.712207
  48170/150000: episode: 484, duration: 1.085s, episode steps: 148, steps per second: 136, episode reward: -65.874, mean reward: -0.445 [-100.000, 10.482], mean action: 1.561 [0.000, 3.000],  loss: 16.002019, mse: 2586.573053, mean_q: 32.203301, mean_eps: 0.711427
  48297/150000: episode: 485, duration: 0.885s, episode steps: 127, steps per second: 144, episode reward: -79.490, mean reward: -0.626 [-100.000, 18.113], mean action: 1.402 [0.000, 3.000],  loss: 15.032865, mse: 2634.549298, mean_q: 30.729502, mean_eps: 0.710602
  48429/150000: episode: 486, duration: 1.118s, episode steps: 132, steps per second: 118, episode reward: -107.313, mean reward: -0.813 [-100.000,  6.961], mean action: 1.530 [0.000, 3.000],  loss: 11.637011, mse: 2636.182753, mean_q: 30.953048, mean_eps: 0.709825
  48544/150000: episode: 487, duration: 0.950s, episode steps: 115, steps per second: 121, episode reward: -74.231, mean reward: -0.645 [-100.000, 11.464], mean action: 1.739 [0.000, 3.000],  loss: 12.348290, mse: 2684.777939, mean_q: 31.928168, mean_eps: 0.709084
  48658/150000: episode: 488, duration: 0.876s, episode steps: 114, steps per second: 130, episode reward: -52.506, mean reward: -0.461 [-100.000, 13.490], mean action: 1.667 [0.000, 3.000],  loss: 21.127164, mse: 2650.853929, mean_q: 32.560285, mean_eps: 0.708397
  48758/150000: episode: 489, duration: 0.850s, episode steps: 100, steps per second: 118, episode reward: -38.829, mean reward: -0.388 [-100.000, 12.644], mean action: 1.630 [0.000, 3.000],  loss: 14.574419, mse: 2669.328363, mean_q: 30.649441, mean_eps: 0.707755
  48856/150000: episode: 490, duration: 0.766s, episode steps:  98, steps per second: 128, episode reward: -77.111, mean reward: -0.787 [-100.000, 30.448], mean action: 1.582 [0.000, 3.000],  loss: 18.714518, mse: 2781.817365, mean_q: 30.885569, mean_eps: 0.707161
  48972/150000: episode: 491, duration: 0.863s, episode steps: 116, steps per second: 134, episode reward: -162.448, mean reward: -1.400 [-100.000, 15.177], mean action: 1.560 [0.000, 3.000],  loss: 12.476439, mse: 2632.336051, mean_q: 32.236113, mean_eps: 0.706519
  49115/150000: episode: 492, duration: 1.012s, episode steps: 143, steps per second: 141, episode reward: -3.170, mean reward: -0.022 [-100.000, 15.281], mean action: 1.476 [0.000, 3.000],  loss: 16.310996, mse: 2678.984590, mean_q: 32.158150, mean_eps: 0.705742
  49208/150000: episode: 493, duration: 0.653s, episode steps:  93, steps per second: 142, episode reward: -100.967, mean reward: -1.086 [-100.000,  9.793], mean action: 1.699 [0.000, 3.000],  loss: 15.473255, mse: 2653.370713, mean_q: 32.635987, mean_eps: 0.705034
  49341/150000: episode: 494, duration: 0.998s, episode steps: 133, steps per second: 133, episode reward: -143.143, mean reward: -1.076 [-100.000,  6.644], mean action: 1.654 [0.000, 3.000],  loss: 24.843072, mse: 2751.697091, mean_q: 32.434701, mean_eps: 0.704356
  49420/150000: episode: 495, duration: 0.553s, episode steps:  79, steps per second: 143, episode reward: -0.682, mean reward: -0.009 [-100.000, 18.394], mean action: 1.633 [0.000, 3.000],  loss: 14.942792, mse: 2701.498408, mean_q: 32.635848, mean_eps: 0.703720
  49545/150000: episode: 496, duration: 0.887s, episode steps: 125, steps per second: 141, episode reward: -107.868, mean reward: -0.863 [-100.000,  9.862], mean action: 1.336 [0.000, 3.000],  loss: 14.469499, mse: 2730.486412, mean_q: 32.447798, mean_eps: 0.703108
  49640/150000: episode: 497, duration: 0.720s, episode steps:  95, steps per second: 132, episode reward: -117.323, mean reward: -1.235 [-100.000,  8.684], mean action: 1.284 [0.000, 3.000],  loss: 15.535150, mse: 2736.706746, mean_q: 31.855910, mean_eps: 0.702448
  49739/150000: episode: 498, duration: 0.689s, episode steps:  99, steps per second: 144, episode reward: -73.213, mean reward: -0.740 [-100.000, 13.243], mean action: 1.495 [0.000, 3.000],  loss: 19.302004, mse: 2680.778826, mean_q: 31.988965, mean_eps: 0.701866
  49837/150000: episode: 499, duration: 0.692s, episode steps:  98, steps per second: 142, episode reward: -20.193, mean reward: -0.206 [-100.000, 11.563], mean action: 1.602 [0.000, 3.000],  loss: 14.743407, mse: 2716.694363, mean_q: 32.201585, mean_eps: 0.701275
  49937/150000: episode: 500, duration: 0.740s, episode steps: 100, steps per second: 135, episode reward: -99.496, mean reward: -0.995 [-100.000,  8.781], mean action: 1.630 [0.000, 3.000],  loss: 16.262793, mse: 2773.284120, mean_q: 31.666176, mean_eps: 0.700681
  50069/150000: episode: 501, duration: 0.926s, episode steps: 132, steps per second: 143, episode reward: -37.358, mean reward: -0.283 [-100.000, 17.287], mean action: 1.750 [0.000, 3.000],  loss: 14.652928, mse: 2754.134337, mean_q: 33.353294, mean_eps: 0.699985
  50202/150000: episode: 502, duration: 0.994s, episode steps: 133, steps per second: 134, episode reward: -103.576, mean reward: -0.779 [-100.000, 18.368], mean action: 1.579 [0.000, 3.000],  loss: 19.230412, mse: 2721.743159, mean_q: 32.749681, mean_eps: 0.699190
  50290/150000: episode: 503, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: -148.917, mean reward: -1.692 [-100.000,  8.934], mean action: 1.773 [0.000, 3.000],  loss: 16.472243, mse: 2752.259971, mean_q: 32.670048, mean_eps: 0.698527
  50386/150000: episode: 504, duration: 0.680s, episode steps:  96, steps per second: 141, episode reward: -71.149, mean reward: -0.741 [-100.000, 11.185], mean action: 1.771 [0.000, 3.000],  loss: 13.973393, mse: 2696.728040, mean_q: 33.586452, mean_eps: 0.697975
  50475/150000: episode: 505, duration: 0.653s, episode steps:  89, steps per second: 136, episode reward: -22.114, mean reward: -0.248 [-100.000, 13.103], mean action: 1.573 [0.000, 3.000],  loss: 12.753081, mse: 2670.767239, mean_q: 32.676207, mean_eps: 0.697420
  50608/150000: episode: 506, duration: 1.153s, episode steps: 133, steps per second: 115, episode reward: -55.279, mean reward: -0.416 [-100.000, 12.960], mean action: 1.564 [0.000, 3.000],  loss: 13.744595, mse: 2692.477007, mean_q: 32.990890, mean_eps: 0.696754
  50718/150000: episode: 507, duration: 0.811s, episode steps: 110, steps per second: 136, episode reward: -99.043, mean reward: -0.900 [-100.000, 14.981], mean action: 1.473 [0.000, 3.000],  loss: 15.364588, mse: 2771.129630, mean_q: 32.760302, mean_eps: 0.696025
  50798/150000: episode: 508, duration: 0.605s, episode steps:  80, steps per second: 132, episode reward: -86.307, mean reward: -1.079 [-100.000, 11.540], mean action: 1.575 [0.000, 3.000],  loss: 17.354630, mse: 2659.671002, mean_q: 33.270640, mean_eps: 0.695455
  50926/150000: episode: 509, duration: 0.916s, episode steps: 128, steps per second: 140, episode reward: -45.759, mean reward: -0.357 [-100.000, 30.177], mean action: 1.664 [0.000, 3.000],  loss: 13.343295, mse: 2733.863595, mean_q: 32.776923, mean_eps: 0.694831
  51049/150000: episode: 510, duration: 0.894s, episode steps: 123, steps per second: 138, episode reward: -59.259, mean reward: -0.482 [-100.000, 16.149], mean action: 1.520 [0.000, 3.000],  loss: 17.871388, mse: 2775.917287, mean_q: 34.080118, mean_eps: 0.694078
  51142/150000: episode: 511, duration: 0.652s, episode steps:  93, steps per second: 143, episode reward: -65.055, mean reward: -0.700 [-100.000, 18.983], mean action: 1.677 [0.000, 3.000],  loss: 12.824767, mse: 2863.734330, mean_q: 35.138016, mean_eps: 0.693430
  51245/150000: episode: 512, duration: 0.719s, episode steps: 103, steps per second: 143, episode reward: -82.835, mean reward: -0.804 [-100.000,  6.711], mean action: 1.398 [0.000, 3.000],  loss: 17.720354, mse: 2821.089896, mean_q: 35.212867, mean_eps: 0.692842
  51353/150000: episode: 513, duration: 0.777s, episode steps: 108, steps per second: 139, episode reward: -103.645, mean reward: -0.960 [-100.000, 11.227], mean action: 1.676 [0.000, 3.000],  loss: 12.767454, mse: 2833.784128, mean_q: 35.775182, mean_eps: 0.692209
  51456/150000: episode: 514, duration: 0.738s, episode steps: 103, steps per second: 140, episode reward: -55.254, mean reward: -0.536 [-100.000,  7.657], mean action: 1.495 [0.000, 3.000],  loss: 9.497585, mse: 2874.304379, mean_q: 35.571978, mean_eps: 0.691576
  51562/150000: episode: 515, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: -120.674, mean reward: -1.138 [-100.000,  6.354], mean action: 1.358 [0.000, 3.000],  loss: 14.352432, mse: 2844.078256, mean_q: 35.255809, mean_eps: 0.690949
  51655/150000: episode: 516, duration: 0.683s, episode steps:  93, steps per second: 136, episode reward: -72.786, mean reward: -0.783 [-100.000, 12.005], mean action: 1.570 [0.000, 3.000],  loss: 12.885031, mse: 2818.005122, mean_q: 34.945672, mean_eps: 0.690352
  51734/150000: episode: 517, duration: 0.566s, episode steps:  79, steps per second: 140, episode reward: -45.814, mean reward: -0.580 [-100.000,  6.716], mean action: 1.608 [0.000, 3.000],  loss: 16.943247, mse: 2792.416249, mean_q: 33.823207, mean_eps: 0.689836
  51822/150000: episode: 518, duration: 0.617s, episode steps:  88, steps per second: 143, episode reward: -77.243, mean reward: -0.878 [-100.000, 14.145], mean action: 1.636 [0.000, 3.000],  loss: 10.042284, mse: 2808.156611, mean_q: 34.640362, mean_eps: 0.689335
  51924/150000: episode: 519, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -133.347, mean reward: -1.307 [-100.000,  8.130], mean action: 1.539 [0.000, 3.000],  loss: 15.556189, mse: 2810.839482, mean_q: 35.897349, mean_eps: 0.688765
  52032/150000: episode: 520, duration: 0.795s, episode steps: 108, steps per second: 136, episode reward: -71.929, mean reward: -0.666 [-100.000, 17.570], mean action: 1.398 [0.000, 3.000],  loss: 14.810785, mse: 2822.200701, mean_q: 35.555851, mean_eps: 0.688135
  52100/150000: episode: 521, duration: 0.493s, episode steps:  68, steps per second: 138, episode reward: -84.517, mean reward: -1.243 [-100.000,  6.399], mean action: 1.603 [0.000, 3.000],  loss: 17.654981, mse: 2790.607472, mean_q: 35.833477, mean_eps: 0.687607
  52225/150000: episode: 522, duration: 0.904s, episode steps: 125, steps per second: 138, episode reward: -85.081, mean reward: -0.681 [-100.000, 11.987], mean action: 1.496 [0.000, 3.000],  loss: 9.170332, mse: 2870.754811, mean_q: 34.983359, mean_eps: 0.687028
  52319/150000: episode: 523, duration: 0.695s, episode steps:  94, steps per second: 135, episode reward: -109.886, mean reward: -1.169 [-100.000, 11.010], mean action: 1.840 [0.000, 3.000],  loss: 11.828619, mse: 2808.788613, mean_q: 35.630712, mean_eps: 0.686371
  52438/150000: episode: 524, duration: 0.828s, episode steps: 119, steps per second: 144, episode reward: -16.688, mean reward: -0.140 [-100.000, 19.219], mean action: 1.630 [0.000, 3.000],  loss: 13.306386, mse: 2843.788757, mean_q: 35.091095, mean_eps: 0.685732
  52541/150000: episode: 525, duration: 0.757s, episode steps: 103, steps per second: 136, episode reward: -126.627, mean reward: -1.229 [-100.000, 14.489], mean action: 1.757 [0.000, 3.000],  loss: 15.935754, mse: 2902.892690, mean_q: 34.999176, mean_eps: 0.685066
  52666/150000: episode: 526, duration: 0.916s, episode steps: 125, steps per second: 136, episode reward: -63.031, mean reward: -0.504 [-100.000, 17.138], mean action: 1.584 [0.000, 3.000],  loss: 12.928583, mse: 2824.662408, mean_q: 35.495275, mean_eps: 0.684382
  52794/150000: episode: 527, duration: 0.893s, episode steps: 128, steps per second: 143, episode reward: -88.956, mean reward: -0.695 [-100.000,  6.492], mean action: 1.719 [0.000, 3.000],  loss: 11.294917, mse: 2903.572824, mean_q: 33.913564, mean_eps: 0.683623
  52892/150000: episode: 528, duration: 0.748s, episode steps:  98, steps per second: 131, episode reward: -100.702, mean reward: -1.028 [-100.000,  8.051], mean action: 1.724 [0.000, 3.000],  loss: 24.133579, mse: 2899.508174, mean_q: 34.751704, mean_eps: 0.682945
  53011/150000: episode: 529, duration: 0.849s, episode steps: 119, steps per second: 140, episode reward: -87.272, mean reward: -0.733 [-100.000, 15.036], mean action: 1.496 [0.000, 3.000],  loss: 12.631593, mse: 2844.595994, mean_q: 35.803844, mean_eps: 0.682294
  53115/150000: episode: 530, duration: 0.731s, episode steps: 104, steps per second: 142, episode reward:  2.412, mean reward:  0.023 [-100.000, 15.637], mean action: 1.654 [0.000, 3.000],  loss: 15.562970, mse: 2937.036760, mean_q: 37.571656, mean_eps: 0.681625
  53195/150000: episode: 531, duration: 0.622s, episode steps:  80, steps per second: 129, episode reward: -83.711, mean reward: -1.046 [-100.000,  7.049], mean action: 1.725 [0.000, 3.000],  loss: 10.507121, mse: 2875.971765, mean_q: 36.867707, mean_eps: 0.681073
  53315/150000: episode: 532, duration: 0.871s, episode steps: 120, steps per second: 138, episode reward: -130.570, mean reward: -1.088 [-100.000, 11.725], mean action: 1.408 [0.000, 3.000],  loss: 21.278478, mse: 2838.565363, mean_q: 36.240639, mean_eps: 0.680473
  53442/150000: episode: 533, duration: 0.950s, episode steps: 127, steps per second: 134, episode reward: -78.544, mean reward: -0.618 [-100.000, 12.935], mean action: 1.378 [0.000, 3.000],  loss: 11.272083, mse: 2881.928545, mean_q: 36.722484, mean_eps: 0.679732
  53547/150000: episode: 534, duration: 0.772s, episode steps: 105, steps per second: 136, episode reward: -85.470, mean reward: -0.814 [-100.000,  6.668], mean action: 1.552 [0.000, 3.000],  loss: 11.860769, mse: 2867.040199, mean_q: 36.835179, mean_eps: 0.679036
  53636/150000: episode: 535, duration: 0.633s, episode steps:  89, steps per second: 141, episode reward: -62.726, mean reward: -0.705 [-100.000, 11.092], mean action: 1.674 [0.000, 3.000],  loss: 22.940529, mse: 2888.279031, mean_q: 36.774744, mean_eps: 0.678454
  53762/150000: episode: 536, duration: 0.923s, episode steps: 126, steps per second: 137, episode reward: -95.652, mean reward: -0.759 [-100.000,  8.995], mean action: 1.667 [0.000, 3.000],  loss: 13.818506, mse: 2883.565168, mean_q: 37.717120, mean_eps: 0.677809
  53854/150000: episode: 537, duration: 0.658s, episode steps:  92, steps per second: 140, episode reward: -24.256, mean reward: -0.264 [-100.000, 10.817], mean action: 1.696 [0.000, 3.000],  loss: 9.725589, mse: 2929.844241, mean_q: 36.620499, mean_eps: 0.677155
  53937/150000: episode: 538, duration: 0.593s, episode steps:  83, steps per second: 140, episode reward: -40.352, mean reward: -0.486 [-100.000, 13.402], mean action: 1.446 [0.000, 3.000],  loss: 13.187986, mse: 2931.142608, mean_q: 36.680436, mean_eps: 0.676630
  54045/150000: episode: 539, duration: 0.811s, episode steps: 108, steps per second: 133, episode reward: -148.554, mean reward: -1.375 [-100.000,  6.712], mean action: 1.713 [0.000, 3.000],  loss: 16.434657, mse: 2860.482946, mean_q: 37.860356, mean_eps: 0.676057
  54120/150000: episode: 540, duration: 0.538s, episode steps:  75, steps per second: 139, episode reward: -45.997, mean reward: -0.613 [-100.000, 19.629], mean action: 1.840 [0.000, 3.000],  loss: 15.431766, mse: 2898.320182, mean_q: 38.116642, mean_eps: 0.675508
  54191/150000: episode: 541, duration: 0.495s, episode steps:  71, steps per second: 143, episode reward: -65.062, mean reward: -0.916 [-100.000,  8.728], mean action: 1.704 [0.000, 3.000],  loss: 16.256270, mse: 2883.106294, mean_q: 38.623764, mean_eps: 0.675070
  54303/150000: episode: 542, duration: 0.795s, episode steps: 112, steps per second: 141, episode reward: -84.275, mean reward: -0.752 [-100.000,  6.329], mean action: 1.786 [0.000, 3.000],  loss: 15.545632, mse: 2859.382926, mean_q: 37.794844, mean_eps: 0.674521
  54412/150000: episode: 543, duration: 0.794s, episode steps: 109, steps per second: 137, episode reward: -27.644, mean reward: -0.254 [-100.000, 18.697], mean action: 1.670 [0.000, 3.000],  loss: 16.236920, mse: 2879.823764, mean_q: 36.552741, mean_eps: 0.673858
  54481/150000: episode: 544, duration: 0.485s, episode steps:  69, steps per second: 142, episode reward: -34.086, mean reward: -0.494 [-100.000, 13.274], mean action: 1.667 [0.000, 3.000],  loss: 12.161582, mse: 2837.703815, mean_q: 37.946755, mean_eps: 0.673324
  54594/150000: episode: 545, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -126.556, mean reward: -1.120 [-100.000,  6.168], mean action: 1.770 [0.000, 3.000],  loss: 13.243230, mse: 2863.185532, mean_q: 37.403827, mean_eps: 0.672778
  54716/150000: episode: 546, duration: 0.915s, episode steps: 122, steps per second: 133, episode reward: -47.116, mean reward: -0.386 [-100.000, 56.953], mean action: 1.574 [0.000, 3.000],  loss: 11.282914, mse: 2865.587947, mean_q: 36.339007, mean_eps: 0.672073
  54791/150000: episode: 547, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -61.813, mean reward: -0.824 [-100.000,  7.905], mean action: 1.787 [0.000, 3.000],  loss: 8.730987, mse: 2827.910752, mean_q: 36.766456, mean_eps: 0.671482
  54908/150000: episode: 548, duration: 0.831s, episode steps: 117, steps per second: 141, episode reward: -86.386, mean reward: -0.738 [-100.000,  6.326], mean action: 1.675 [0.000, 3.000],  loss: 14.235239, mse: 2868.789565, mean_q: 38.284638, mean_eps: 0.670906
  54994/150000: episode: 549, duration: 0.648s, episode steps:  86, steps per second: 133, episode reward: -6.404, mean reward: -0.074 [-100.000, 16.666], mean action: 1.744 [0.000, 3.000],  loss: 13.178562, mse: 2880.033007, mean_q: 38.015305, mean_eps: 0.670297
  55085/150000: episode: 550, duration: 0.655s, episode steps:  91, steps per second: 139, episode reward: -21.111, mean reward: -0.232 [-100.000, 13.699], mean action: 1.736 [0.000, 3.000],  loss: 18.898497, mse: 2956.546443, mean_q: 36.559407, mean_eps: 0.669766
  55162/150000: episode: 551, duration: 0.703s, episode steps:  77, steps per second: 110, episode reward: -48.721, mean reward: -0.633 [-100.000, 10.895], mean action: 1.636 [0.000, 3.000],  loss: 12.267726, mse: 3028.212830, mean_q: 36.397844, mean_eps: 0.669262
  55241/150000: episode: 552, duration: 0.729s, episode steps:  79, steps per second: 108, episode reward: -72.370, mean reward: -0.916 [-100.000, 13.607], mean action: 1.772 [0.000, 3.000],  loss: 9.189996, mse: 3017.734026, mean_q: 34.978104, mean_eps: 0.668794
  55330/150000: episode: 553, duration: 0.691s, episode steps:  89, steps per second: 129, episode reward: -98.142, mean reward: -1.103 [-100.000, 10.235], mean action: 1.449 [0.000, 3.000],  loss: 11.759956, mse: 2959.603557, mean_q: 38.206470, mean_eps: 0.668290
  55412/150000: episode: 554, duration: 0.654s, episode steps:  82, steps per second: 125, episode reward: -85.065, mean reward: -1.037 [-100.000,  9.694], mean action: 1.329 [0.000, 3.000],  loss: 15.025244, mse: 2950.169919, mean_q: 37.673413, mean_eps: 0.667777
  55572/150000: episode: 555, duration: 1.283s, episode steps: 160, steps per second: 125, episode reward: -37.455, mean reward: -0.234 [-100.000, 17.602], mean action: 1.488 [0.000, 3.000],  loss: 13.128135, mse: 2938.434280, mean_q: 36.299134, mean_eps: 0.667051
  55662/150000: episode: 556, duration: 0.690s, episode steps:  90, steps per second: 130, episode reward: -60.491, mean reward: -0.672 [-100.000,  5.951], mean action: 1.700 [0.000, 3.000],  loss: 16.527891, mse: 2898.119306, mean_q: 36.947025, mean_eps: 0.666301
  55760/150000: episode: 557, duration: 0.774s, episode steps:  98, steps per second: 127, episode reward: -61.925, mean reward: -0.632 [-100.000, 11.885], mean action: 1.673 [0.000, 3.000],  loss: 11.094828, mse: 2862.109771, mean_q: 38.514884, mean_eps: 0.665737
  55871/150000: episode: 558, duration: 0.846s, episode steps: 111, steps per second: 131, episode reward: -39.431, mean reward: -0.355 [-100.000, 13.005], mean action: 1.550 [0.000, 3.000],  loss: 17.686771, mse: 2913.538176, mean_q: 36.700564, mean_eps: 0.665110
  55972/150000: episode: 559, duration: 0.723s, episode steps: 101, steps per second: 140, episode reward: -96.686, mean reward: -0.957 [-100.000,  6.364], mean action: 1.366 [0.000, 3.000],  loss: 11.023064, mse: 2938.888314, mean_q: 37.563386, mean_eps: 0.664474
  56033/150000: episode: 560, duration: 0.482s, episode steps:  61, steps per second: 127, episode reward: -69.648, mean reward: -1.142 [-100.000,  7.231], mean action: 1.459 [0.000, 3.000],  loss: 18.666742, mse: 2979.048928, mean_q: 37.319201, mean_eps: 0.663988
  56129/150000: episode: 561, duration: 0.746s, episode steps:  96, steps per second: 129, episode reward: -23.639, mean reward: -0.246 [-100.000, 13.282], mean action: 1.552 [0.000, 3.000],  loss: 9.726053, mse: 3026.222010, mean_q: 37.187819, mean_eps: 0.663517
  56235/150000: episode: 562, duration: 0.962s, episode steps: 106, steps per second: 110, episode reward: -100.049, mean reward: -0.944 [-100.000,  7.997], mean action: 1.434 [0.000, 3.000],  loss: 12.695908, mse: 3027.054195, mean_q: 38.084435, mean_eps: 0.662911
  56899/150000: episode: 563, duration: 6.093s, episode steps: 664, steps per second: 109, episode reward: -114.497, mean reward: -0.172 [-100.000, 29.834], mean action: 1.538 [0.000, 3.000],  loss: 13.571656, mse: 3029.855028, mean_q: 37.839242, mean_eps: 0.660601
  56980/150000: episode: 564, duration: 0.754s, episode steps:  81, steps per second: 107, episode reward: -74.518, mean reward: -0.920 [-100.000,  7.017], mean action: 1.481 [0.000, 3.000],  loss: 14.807994, mse: 3038.054621, mean_q: 39.369194, mean_eps: 0.658366
  57094/150000: episode: 565, duration: 1.229s, episode steps: 114, steps per second:  93, episode reward: -56.673, mean reward: -0.497 [-100.000, 17.011], mean action: 1.632 [0.000, 3.000],  loss: 12.827547, mse: 3055.210456, mean_q: 37.983642, mean_eps: 0.657781
  57183/150000: episode: 566, duration: 1.485s, episode steps:  89, steps per second:  60, episode reward: -57.664, mean reward: -0.648 [-100.000, 17.884], mean action: 1.584 [0.000, 3.000],  loss: 12.814542, mse: 3114.318291, mean_q: 37.458027, mean_eps: 0.657172
  57285/150000: episode: 567, duration: 1.043s, episode steps: 102, steps per second:  98, episode reward: 24.499, mean reward:  0.240 [-100.000, 18.315], mean action: 1.775 [0.000, 3.000],  loss: 13.945434, mse: 3095.665953, mean_q: 38.057187, mean_eps: 0.656599
  57372/150000: episode: 568, duration: 0.872s, episode steps:  87, steps per second: 100, episode reward: -30.347, mean reward: -0.349 [-100.000, 16.775], mean action: 1.713 [0.000, 3.000],  loss: 14.613320, mse: 3088.764559, mean_q: 37.052883, mean_eps: 0.656032
  57480/150000: episode: 569, duration: 0.989s, episode steps: 108, steps per second: 109, episode reward: -82.304, mean reward: -0.762 [-100.000,  6.919], mean action: 1.509 [0.000, 3.000],  loss: 12.432755, mse: 2982.560895, mean_q: 38.053574, mean_eps: 0.655447
  57563/150000: episode: 570, duration: 0.673s, episode steps:  83, steps per second: 123, episode reward: -80.059, mean reward: -0.965 [-100.000, 10.595], mean action: 1.602 [0.000, 3.000],  loss: 16.108694, mse: 3152.457052, mean_q: 38.793552, mean_eps: 0.654874
  57692/150000: episode: 571, duration: 1.088s, episode steps: 129, steps per second: 119, episode reward: -42.383, mean reward: -0.329 [-100.000, 13.946], mean action: 1.752 [0.000, 3.000],  loss: 10.064588, mse: 3096.847088, mean_q: 38.933534, mean_eps: 0.654238
  57760/150000: episode: 572, duration: 0.642s, episode steps:  68, steps per second: 106, episode reward: -55.111, mean reward: -0.810 [-100.000, 13.377], mean action: 1.559 [0.000, 3.000],  loss: 13.665032, mse: 3096.305991, mean_q: 37.090152, mean_eps: 0.653647
  57846/150000: episode: 573, duration: 0.798s, episode steps:  86, steps per second: 108, episode reward:  9.448, mean reward:  0.110 [-100.000, 34.973], mean action: 1.779 [0.000, 3.000],  loss: 11.329267, mse: 3013.859117, mean_q: 36.890020, mean_eps: 0.653185
  57951/150000: episode: 574, duration: 1.069s, episode steps: 105, steps per second:  98, episode reward: -122.497, mean reward: -1.167 [-100.000,  7.051], mean action: 1.771 [0.000, 3.000],  loss: 16.102720, mse: 3097.086340, mean_q: 38.150122, mean_eps: 0.652612
  58067/150000: episode: 575, duration: 0.973s, episode steps: 116, steps per second: 119, episode reward: -77.772, mean reward: -0.670 [-100.000,  5.772], mean action: 1.655 [0.000, 3.000],  loss: 10.935571, mse: 3106.064813, mean_q: 37.326842, mean_eps: 0.651949
  58200/150000: episode: 576, duration: 1.093s, episode steps: 133, steps per second: 122, episode reward: -106.141, mean reward: -0.798 [-100.000, 10.638], mean action: 1.677 [0.000, 3.000],  loss: 9.885687, mse: 3087.792734, mean_q: 38.698969, mean_eps: 0.651202
  58284/150000: episode: 577, duration: 0.645s, episode steps:  84, steps per second: 130, episode reward: -58.717, mean reward: -0.699 [-100.000,  7.191], mean action: 1.786 [0.000, 3.000],  loss: 8.890501, mse: 3144.732239, mean_q: 37.650730, mean_eps: 0.650551
  58377/150000: episode: 578, duration: 0.805s, episode steps:  93, steps per second: 115, episode reward: -38.119, mean reward: -0.410 [-100.000, 10.544], mean action: 1.785 [0.000, 3.000],  loss: 13.302850, mse: 3100.638643, mean_q: 38.174299, mean_eps: 0.650020
  58471/150000: episode: 579, duration: 0.776s, episode steps:  94, steps per second: 121, episode reward: -52.343, mean reward: -0.557 [-100.000, 25.077], mean action: 1.532 [0.000, 3.000],  loss: 11.749099, mse: 3148.680934, mean_q: 37.973305, mean_eps: 0.649459
  58565/150000: episode: 580, duration: 0.758s, episode steps:  94, steps per second: 124, episode reward: -14.124, mean reward: -0.150 [-100.000, 11.966], mean action: 1.468 [0.000, 3.000],  loss: 12.694920, mse: 3085.128021, mean_q: 38.308341, mean_eps: 0.648895
  59565/150000: episode: 581, duration: 8.585s, episode steps: 1000, steps per second: 116, episode reward: -40.454, mean reward: -0.040 [-19.865, 43.350], mean action: 1.693 [0.000, 3.000],  loss: 11.204659, mse: 3138.065506, mean_q: 38.708647, mean_eps: 0.645613
  59664/150000: episode: 582, duration: 0.819s, episode steps:  99, steps per second: 121, episode reward: -145.287, mean reward: -1.468 [-100.000,  8.902], mean action: 1.434 [0.000, 3.000],  loss: 9.904580, mse: 3162.958994, mean_q: 40.117421, mean_eps: 0.642316
  59756/150000: episode: 583, duration: 0.747s, episode steps:  92, steps per second: 123, episode reward: -82.463, mean reward: -0.896 [-100.000, 15.008], mean action: 1.391 [0.000, 3.000],  loss: 14.278821, mse: 3122.694416, mean_q: 38.640606, mean_eps: 0.641743
  59829/150000: episode: 584, duration: 0.598s, episode steps:  73, steps per second: 122, episode reward: -29.650, mean reward: -0.406 [-100.000, 10.928], mean action: 1.274 [0.000, 3.000],  loss: 9.305041, mse: 3102.846790, mean_q: 38.862881, mean_eps: 0.641248
  59926/150000: episode: 585, duration: 0.824s, episode steps:  97, steps per second: 118, episode reward: -111.406, mean reward: -1.149 [-100.000,  7.232], mean action: 1.639 [0.000, 3.000],  loss: 11.565549, mse: 3191.483872, mean_q: 39.139805, mean_eps: 0.640738
  60002/150000: episode: 586, duration: 0.602s, episode steps:  76, steps per second: 126, episode reward: -126.358, mean reward: -1.663 [-100.000,  6.205], mean action: 1.829 [0.000, 3.000],  loss: 20.705670, mse: 3191.426999, mean_q: 39.140187, mean_eps: 0.640219
  60145/150000: episode: 587, duration: 1.136s, episode steps: 143, steps per second: 126, episode reward: -93.275, mean reward: -0.652 [-100.000, 10.758], mean action: 1.601 [0.000, 3.000],  loss: 11.306326, mse: 3267.782187, mean_q: 39.600435, mean_eps: 0.639562
  60316/150000: episode: 588, duration: 1.342s, episode steps: 171, steps per second: 127, episode reward: -29.756, mean reward: -0.174 [-100.000, 16.877], mean action: 1.427 [0.000, 3.000],  loss: 11.948127, mse: 3182.460606, mean_q: 39.400826, mean_eps: 0.638620
  60385/150000: episode: 589, duration: 0.533s, episode steps:  69, steps per second: 130, episode reward: -43.279, mean reward: -0.627 [-100.000,  5.456], mean action: 1.913 [0.000, 3.000],  loss: 10.478246, mse: 3181.662760, mean_q: 39.777920, mean_eps: 0.637900
  60518/150000: episode: 590, duration: 1.076s, episode steps: 133, steps per second: 124, episode reward: -254.191, mean reward: -1.911 [-100.000, 27.920], mean action: 1.466 [0.000, 3.000],  loss: 13.228594, mse: 3169.384681, mean_q: 40.144593, mean_eps: 0.637294
  60603/150000: episode: 591, duration: 0.648s, episode steps:  85, steps per second: 131, episode reward: -45.015, mean reward: -0.530 [-100.000, 13.140], mean action: 1.753 [0.000, 3.000],  loss: 10.913760, mse: 3270.582853, mean_q: 40.144422, mean_eps: 0.636640
  60679/150000: episode: 592, duration: 0.595s, episode steps:  76, steps per second: 128, episode reward: -44.675, mean reward: -0.588 [-100.000,  7.110], mean action: 1.671 [0.000, 3.000],  loss: 10.676872, mse: 3250.136979, mean_q: 39.485678, mean_eps: 0.636157
  60801/150000: episode: 593, duration: 1.001s, episode steps: 122, steps per second: 122, episode reward: -46.894, mean reward: -0.384 [-100.000, 11.146], mean action: 1.566 [0.000, 3.000],  loss: 10.013863, mse: 3218.469594, mean_q: 39.792546, mean_eps: 0.635563
  60908/150000: episode: 594, duration: 0.827s, episode steps: 107, steps per second: 129, episode reward: -40.849, mean reward: -0.382 [-100.000,  8.854], mean action: 1.813 [0.000, 3.000],  loss: 15.055757, mse: 3275.445662, mean_q: 40.769705, mean_eps: 0.634876
  61058/150000: episode: 595, duration: 1.218s, episode steps: 150, steps per second: 123, episode reward: -34.793, mean reward: -0.232 [-100.000, 14.701], mean action: 1.827 [0.000, 3.000],  loss: 11.571992, mse: 3225.700428, mean_q: 40.464076, mean_eps: 0.634105
  61159/150000: episode: 596, duration: 0.782s, episode steps: 101, steps per second: 129, episode reward: -63.900, mean reward: -0.633 [-100.000,  6.102], mean action: 1.614 [0.000, 3.000],  loss: 11.682989, mse: 3203.336276, mean_q: 39.768686, mean_eps: 0.633352
  61292/150000: episode: 597, duration: 1.117s, episode steps: 133, steps per second: 119, episode reward: -108.840, mean reward: -0.818 [-100.000,  5.519], mean action: 1.669 [0.000, 3.000],  loss: 12.386781, mse: 3281.172730, mean_q: 38.930197, mean_eps: 0.632650
  61396/150000: episode: 598, duration: 0.843s, episode steps: 104, steps per second: 123, episode reward: -65.116, mean reward: -0.626 [-100.000,  8.259], mean action: 1.471 [0.000, 3.000],  loss: 13.742996, mse: 3204.695613, mean_q: 38.291666, mean_eps: 0.631939
  61504/150000: episode: 599, duration: 0.989s, episode steps: 108, steps per second: 109, episode reward: -24.194, mean reward: -0.224 [-100.000, 17.617], mean action: 1.648 [0.000, 3.000],  loss: 13.815685, mse: 3179.019929, mean_q: 39.317016, mean_eps: 0.631303
  61623/150000: episode: 600, duration: 0.972s, episode steps: 119, steps per second: 122, episode reward: -33.755, mean reward: -0.284 [-100.000, 13.602], mean action: 1.580 [0.000, 3.000],  loss: 9.802902, mse: 3209.222459, mean_q: 39.773730, mean_eps: 0.630622
  61699/150000: episode: 601, duration: 0.599s, episode steps:  76, steps per second: 127, episode reward: -72.867, mean reward: -0.959 [-100.000,  6.070], mean action: 1.842 [0.000, 3.000],  loss: 7.367931, mse: 3264.658052, mean_q: 39.740583, mean_eps: 0.630037
  61816/150000: episode: 602, duration: 0.953s, episode steps: 117, steps per second: 123, episode reward: -29.853, mean reward: -0.255 [-100.000, 15.414], mean action: 1.624 [0.000, 3.000],  loss: 8.236238, mse: 3280.638227, mean_q: 39.426240, mean_eps: 0.629458
  61910/150000: episode: 603, duration: 0.733s, episode steps:  94, steps per second: 128, episode reward: -63.279, mean reward: -0.673 [-100.000, 29.275], mean action: 1.777 [0.000, 3.000],  loss: 13.891975, mse: 3231.809814, mean_q: 39.548222, mean_eps: 0.628825
  62015/150000: episode: 604, duration: 0.813s, episode steps: 105, steps per second: 129, episode reward: -86.721, mean reward: -0.826 [-100.000, 12.783], mean action: 1.676 [0.000, 3.000],  loss: 11.895858, mse: 3236.277983, mean_q: 38.524320, mean_eps: 0.628228
  62115/150000: episode: 605, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: -48.340, mean reward: -0.483 [-100.000, 13.040], mean action: 1.500 [0.000, 3.000],  loss: 15.478549, mse: 3336.790029, mean_q: 40.923769, mean_eps: 0.627613
  62226/150000: episode: 606, duration: 0.869s, episode steps: 111, steps per second: 128, episode reward: -59.372, mean reward: -0.535 [-100.000, 18.577], mean action: 1.586 [0.000, 3.000],  loss: 14.390213, mse: 3330.463324, mean_q: 40.675389, mean_eps: 0.626980
  62326/150000: episode: 607, duration: 0.795s, episode steps: 100, steps per second: 126, episode reward: -85.485, mean reward: -0.855 [-100.000, 15.197], mean action: 1.610 [0.000, 3.000],  loss: 14.162171, mse: 3326.942500, mean_q: 39.948870, mean_eps: 0.626347
  62417/150000: episode: 608, duration: 0.716s, episode steps:  91, steps per second: 127, episode reward: -60.421, mean reward: -0.664 [-100.000, 15.935], mean action: 1.868 [0.000, 3.000],  loss: 8.400225, mse: 3294.722128, mean_q: 39.764411, mean_eps: 0.625774
  62522/150000: episode: 609, duration: 0.800s, episode steps: 105, steps per second: 131, episode reward: -165.817, mean reward: -1.579 [-100.000, 38.361], mean action: 1.733 [0.000, 3.000],  loss: 11.173480, mse: 3225.708380, mean_q: 40.685824, mean_eps: 0.625186
  62627/150000: episode: 610, duration: 0.866s, episode steps: 105, steps per second: 121, episode reward: -67.389, mean reward: -0.642 [-100.000,  8.767], mean action: 1.590 [0.000, 3.000],  loss: 13.909565, mse: 3317.852227, mean_q: 40.617502, mean_eps: 0.624556
  62697/150000: episode: 611, duration: 0.557s, episode steps:  70, steps per second: 126, episode reward: -27.756, mean reward: -0.397 [-100.000, 12.911], mean action: 1.457 [0.000, 3.000],  loss: 9.371279, mse: 3283.832659, mean_q: 39.028351, mean_eps: 0.624031
  62816/150000: episode: 612, duration: 0.916s, episode steps: 119, steps per second: 130, episode reward: -23.117, mean reward: -0.194 [-100.000, 17.896], mean action: 1.655 [0.000, 3.000],  loss: 12.795347, mse: 3252.539637, mean_q: 40.615019, mean_eps: 0.623464
  62941/150000: episode: 613, duration: 1.030s, episode steps: 125, steps per second: 121, episode reward: -105.271, mean reward: -0.842 [-100.000,  6.710], mean action: 1.624 [0.000, 3.000],  loss: 9.558355, mse: 3294.083809, mean_q: 40.611325, mean_eps: 0.622732
  63066/150000: episode: 614, duration: 0.967s, episode steps: 125, steps per second: 129, episode reward: -28.976, mean reward: -0.232 [-100.000, 11.471], mean action: 1.504 [0.000, 3.000],  loss: 12.904864, mse: 3291.434230, mean_q: 41.259873, mean_eps: 0.621982
  63151/150000: episode: 615, duration: 0.754s, episode steps:  85, steps per second: 113, episode reward: 14.762, mean reward:  0.174 [-100.000, 16.944], mean action: 1.612 [0.000, 3.000],  loss: 8.618602, mse: 3313.604880, mean_q: 42.168569, mean_eps: 0.621352
  63228/150000: episode: 616, duration: 0.654s, episode steps:  77, steps per second: 118, episode reward: -79.962, mean reward: -1.038 [-100.000,  6.415], mean action: 1.429 [0.000, 3.000],  loss: 13.879707, mse: 3330.789005, mean_q: 42.121687, mean_eps: 0.620866
  63363/150000: episode: 617, duration: 1.205s, episode steps: 135, steps per second: 112, episode reward: -12.329, mean reward: -0.091 [-100.000, 10.545], mean action: 1.548 [0.000, 3.000],  loss: 13.093435, mse: 3347.296618, mean_q: 41.265920, mean_eps: 0.620230
  63484/150000: episode: 618, duration: 1.056s, episode steps: 121, steps per second: 115, episode reward: -31.097, mean reward: -0.257 [-100.000, 17.893], mean action: 1.752 [0.000, 3.000],  loss: 11.818622, mse: 3344.885012, mean_q: 41.423230, mean_eps: 0.619462
  63585/150000: episode: 619, duration: 0.841s, episode steps: 101, steps per second: 120, episode reward: -34.345, mean reward: -0.340 [-100.000, 31.500], mean action: 1.634 [0.000, 3.000],  loss: 11.217125, mse: 3331.698025, mean_q: 41.900355, mean_eps: 0.618796
  63687/150000: episode: 620, duration: 0.847s, episode steps: 102, steps per second: 120, episode reward: -48.935, mean reward: -0.480 [-100.000, 13.243], mean action: 1.559 [0.000, 3.000],  loss: 12.800878, mse: 3347.837498, mean_q: 41.075119, mean_eps: 0.618187
  63760/150000: episode: 621, duration: 0.570s, episode steps:  73, steps per second: 128, episode reward: -64.916, mean reward: -0.889 [-100.000, 12.967], mean action: 1.767 [0.000, 3.000],  loss: 10.007876, mse: 3356.297042, mean_q: 39.192976, mean_eps: 0.617662
  63860/150000: episode: 622, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: -39.479, mean reward: -0.395 [-100.000, 12.273], mean action: 1.510 [0.000, 3.000],  loss: 11.983121, mse: 3368.099021, mean_q: 41.547827, mean_eps: 0.617143
  63962/150000: episode: 623, duration: 0.834s, episode steps: 102, steps per second: 122, episode reward: -67.058, mean reward: -0.657 [-100.000, 10.476], mean action: 1.529 [0.000, 3.000],  loss: 9.023286, mse: 3351.743049, mean_q: 41.631142, mean_eps: 0.616537
  64049/150000: episode: 624, duration: 0.688s, episode steps:  87, steps per second: 126, episode reward: -9.713, mean reward: -0.112 [-100.000, 13.450], mean action: 1.563 [0.000, 3.000],  loss: 13.988050, mse: 3336.745507, mean_q: 41.453030, mean_eps: 0.615970
  64140/150000: episode: 625, duration: 0.712s, episode steps:  91, steps per second: 128, episode reward: -36.435, mean reward: -0.400 [-100.000,  9.337], mean action: 1.659 [0.000, 3.000],  loss: 13.833233, mse: 3485.885951, mean_q: 42.476999, mean_eps: 0.615436
  64234/150000: episode: 626, duration: 0.776s, episode steps:  94, steps per second: 121, episode reward: -10.934, mean reward: -0.116 [-100.000, 10.573], mean action: 1.766 [0.000, 3.000],  loss: 9.371596, mse: 3429.448699, mean_q: 41.847326, mean_eps: 0.614881
  64318/150000: episode: 627, duration: 0.648s, episode steps:  84, steps per second: 130, episode reward: -87.603, mean reward: -1.043 [-100.000,  7.349], mean action: 1.655 [0.000, 3.000],  loss: 12.087320, mse: 3465.109884, mean_q: 43.255321, mean_eps: 0.614347
  64454/150000: episode: 628, duration: 1.069s, episode steps: 136, steps per second: 127, episode reward: -36.704, mean reward: -0.270 [-100.000, 10.545], mean action: 1.632 [0.000, 3.000],  loss: 14.898204, mse: 3345.050767, mean_q: 41.827878, mean_eps: 0.613687
  64566/150000: episode: 629, duration: 0.898s, episode steps: 112, steps per second: 125, episode reward: -100.875, mean reward: -0.901 [-100.000, 27.546], mean action: 1.500 [0.000, 3.000],  loss: 12.882461, mse: 3418.365853, mean_q: 41.807357, mean_eps: 0.612943
  64678/150000: episode: 630, duration: 0.867s, episode steps: 112, steps per second: 129, episode reward: -30.905, mean reward: -0.276 [-100.000, 16.342], mean action: 1.616 [0.000, 3.000],  loss: 12.020486, mse: 3450.801398, mean_q: 41.540235, mean_eps: 0.612271
  64835/150000: episode: 631, duration: 1.261s, episode steps: 157, steps per second: 124, episode reward: -35.560, mean reward: -0.226 [-100.000, 15.711], mean action: 1.516 [0.000, 3.000],  loss: 11.318660, mse: 3403.828249, mean_q: 41.136599, mean_eps: 0.611464
  64955/150000: episode: 632, duration: 1.097s, episode steps: 120, steps per second: 109, episode reward: -32.364, mean reward: -0.270 [-100.000, 15.642], mean action: 1.733 [0.000, 3.000],  loss: 11.317546, mse: 3437.152470, mean_q: 41.097615, mean_eps: 0.610633
  65031/150000: episode: 633, duration: 0.742s, episode steps:  76, steps per second: 102, episode reward: -16.376, mean reward: -0.215 [-100.000, 12.065], mean action: 1.750 [0.000, 3.000],  loss: 10.701347, mse: 3432.829844, mean_q: 41.979337, mean_eps: 0.610045
  65102/150000: episode: 634, duration: 0.625s, episode steps:  71, steps per second: 114, episode reward: -42.131, mean reward: -0.593 [-100.000,  7.671], mean action: 1.704 [0.000, 3.000],  loss: 14.782236, mse: 3545.795056, mean_q: 41.602523, mean_eps: 0.609604
  65193/150000: episode: 635, duration: 0.803s, episode steps:  91, steps per second: 113, episode reward: -42.636, mean reward: -0.469 [-100.000, 14.596], mean action: 1.374 [0.000, 3.000],  loss: 11.928469, mse: 3432.319314, mean_q: 40.637569, mean_eps: 0.609118
  65297/150000: episode: 636, duration: 0.994s, episode steps: 104, steps per second: 105, episode reward: -70.988, mean reward: -0.683 [-100.000, 14.367], mean action: 1.596 [0.000, 3.000],  loss: 9.132623, mse: 3370.724069, mean_q: 41.742030, mean_eps: 0.608533
  65382/150000: episode: 637, duration: 0.731s, episode steps:  85, steps per second: 116, episode reward: -22.206, mean reward: -0.261 [-100.000, 16.497], mean action: 1.694 [0.000, 3.000],  loss: 6.728173, mse: 3424.029234, mean_q: 41.564436, mean_eps: 0.607966
  65458/150000: episode: 638, duration: 0.679s, episode steps:  76, steps per second: 112, episode reward: -82.463, mean reward: -1.085 [-100.000,  8.578], mean action: 1.447 [0.000, 3.000],  loss: 12.736603, mse: 3412.807845, mean_q: 42.537405, mean_eps: 0.607483
  65549/150000: episode: 639, duration: 0.747s, episode steps:  91, steps per second: 122, episode reward: -113.853, mean reward: -1.251 [-100.000,  8.793], mean action: 1.593 [0.000, 3.000],  loss: 11.054195, mse: 3433.797524, mean_q: 42.361915, mean_eps: 0.606982
  65619/150000: episode: 640, duration: 0.547s, episode steps:  70, steps per second: 128, episode reward: -11.106, mean reward: -0.159 [-100.000,  6.915], mean action: 1.629 [0.000, 3.000],  loss: 10.350808, mse: 3427.011670, mean_q: 41.762753, mean_eps: 0.606499
  65707/150000: episode: 641, duration: 0.691s, episode steps:  88, steps per second: 127, episode reward: -46.442, mean reward: -0.528 [-100.000, 10.197], mean action: 1.705 [0.000, 3.000],  loss: 10.611578, mse: 3474.092033, mean_q: 40.563689, mean_eps: 0.606025
  65795/150000: episode: 642, duration: 0.729s, episode steps:  88, steps per second: 121, episode reward: -17.740, mean reward: -0.202 [-100.000,  7.265], mean action: 1.682 [0.000, 3.000],  loss: 7.031483, mse: 3410.571006, mean_q: 41.723987, mean_eps: 0.605497
  65912/150000: episode: 643, duration: 0.906s, episode steps: 117, steps per second: 129, episode reward: -2.454, mean reward: -0.021 [-100.000, 16.769], mean action: 1.444 [0.000, 3.000],  loss: 9.489627, mse: 3408.052561, mean_q: 42.404761, mean_eps: 0.604882
  66028/150000: episode: 644, duration: 0.927s, episode steps: 116, steps per second: 125, episode reward: -68.616, mean reward: -0.592 [-100.000, 10.318], mean action: 1.655 [0.000, 3.000],  loss: 8.904034, mse: 3393.892753, mean_q: 41.108801, mean_eps: 0.604183
  66106/150000: episode: 645, duration: 0.614s, episode steps:  78, steps per second: 127, episode reward: -23.037, mean reward: -0.295 [-100.000, 17.789], mean action: 1.462 [0.000, 3.000],  loss: 11.647203, mse: 3460.230660, mean_q: 41.653531, mean_eps: 0.603601
  66214/150000: episode: 646, duration: 0.846s, episode steps: 108, steps per second: 128, episode reward: -115.938, mean reward: -1.073 [-100.000,  9.325], mean action: 1.435 [0.000, 3.000],  loss: 15.385179, mse: 3473.548663, mean_q: 41.983406, mean_eps: 0.603043
  66416/150000: episode: 647, duration: 1.614s, episode steps: 202, steps per second: 125, episode reward: -190.410, mean reward: -0.943 [-100.000, 10.892], mean action: 1.624 [0.000, 3.000],  loss: 11.041019, mse: 3457.489371, mean_q: 42.707367, mean_eps: 0.602113
  66538/150000: episode: 648, duration: 0.947s, episode steps: 122, steps per second: 129, episode reward: -53.281, mean reward: -0.437 [-100.000, 20.864], mean action: 1.672 [0.000, 3.000],  loss: 11.794928, mse: 3436.719100, mean_q: 43.356253, mean_eps: 0.601141
  66650/150000: episode: 649, duration: 0.885s, episode steps: 112, steps per second: 127, episode reward: -102.001, mean reward: -0.911 [-100.000,  7.542], mean action: 1.393 [0.000, 3.000],  loss: 11.344222, mse: 3409.746674, mean_q: 43.091147, mean_eps: 0.600439
  66750/150000: episode: 650, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: -64.472, mean reward: -0.645 [-100.000,  7.366], mean action: 1.440 [0.000, 3.000],  loss: 13.541253, mse: 3401.614629, mean_q: 42.527641, mean_eps: 0.599803
  66842/150000: episode: 651, duration: 0.911s, episode steps:  92, steps per second: 101, episode reward: -58.433, mean reward: -0.635 [-100.000, 20.400], mean action: 1.707 [0.000, 3.000],  loss: 9.380741, mse: 3487.614629, mean_q: 44.139418, mean_eps: 0.599227
  66949/150000: episode: 652, duration: 0.942s, episode steps: 107, steps per second: 114, episode reward: 29.656, mean reward:  0.277 [-100.000, 17.039], mean action: 1.925 [0.000, 3.000],  loss: 10.545352, mse: 3465.219777, mean_q: 42.559851, mean_eps: 0.598630
  67063/150000: episode: 653, duration: 0.939s, episode steps: 114, steps per second: 121, episode reward: -49.969, mean reward: -0.438 [-100.000, 15.703], mean action: 1.553 [0.000, 3.000],  loss: 12.849525, mse: 3450.958186, mean_q: 42.600086, mean_eps: 0.597967
  67206/150000: episode: 654, duration: 1.105s, episode steps: 143, steps per second: 129, episode reward: -159.743, mean reward: -1.117 [-100.000, 19.985], mean action: 1.650 [0.000, 3.000],  loss: 12.182001, mse: 3468.340037, mean_q: 41.807754, mean_eps: 0.597196
  67333/150000: episode: 655, duration: 1.058s, episode steps: 127, steps per second: 120, episode reward: -120.785, mean reward: -0.951 [-100.000, 12.144], mean action: 1.441 [0.000, 3.000],  loss: 9.167963, mse: 3390.225388, mean_q: 42.641369, mean_eps: 0.596386
  67514/150000: episode: 656, duration: 1.424s, episode steps: 181, steps per second: 127, episode reward: -77.804, mean reward: -0.430 [-100.000, 16.196], mean action: 1.724 [0.000, 3.000],  loss: 13.570746, mse: 3503.733906, mean_q: 42.614014, mean_eps: 0.595462
  67602/150000: episode: 657, duration: 0.713s, episode steps:  88, steps per second: 123, episode reward: 40.518, mean reward:  0.460 [-100.000, 11.265], mean action: 1.545 [0.000, 3.000],  loss: 8.090815, mse: 3412.536258, mean_q: 42.800630, mean_eps: 0.594655
  68602/150000: episode: 658, duration: 9.744s, episode steps: 1000, steps per second: 103, episode reward: 26.910, mean reward:  0.027 [-22.041, 31.878], mean action: 1.651 [0.000, 3.000],  loss: 9.756433, mse: 3469.854994, mean_q: 42.716714, mean_eps: 0.591391
  68673/150000: episode: 659, duration: 0.530s, episode steps:  71, steps per second: 134, episode reward: -38.418, mean reward: -0.541 [-100.000, 13.397], mean action: 1.521 [0.000, 3.000],  loss: 15.717444, mse: 3461.834868, mean_q: 42.742370, mean_eps: 0.588178
  68838/150000: episode: 660, duration: 1.390s, episode steps: 165, steps per second: 119, episode reward:  6.578, mean reward:  0.040 [-100.000, 11.667], mean action: 1.733 [0.000, 3.000],  loss: 14.213382, mse: 3502.863271, mean_q: 42.480455, mean_eps: 0.587470
  68911/150000: episode: 661, duration: 0.584s, episode steps:  73, steps per second: 125, episode reward: -79.971, mean reward: -1.095 [-100.000,  7.220], mean action: 1.575 [0.000, 3.000],  loss: 10.093007, mse: 3466.843947, mean_q: 43.939720, mean_eps: 0.586756
  69027/150000: episode: 662, duration: 1.019s, episode steps: 116, steps per second: 114, episode reward: -53.995, mean reward: -0.465 [-100.000,  8.933], mean action: 1.448 [0.000, 3.000],  loss: 13.825300, mse: 3480.304629, mean_q: 41.484723, mean_eps: 0.586189
  69103/150000: episode: 663, duration: 0.612s, episode steps:  76, steps per second: 124, episode reward: -5.295, mean reward: -0.070 [-100.000, 13.544], mean action: 1.711 [0.000, 3.000],  loss: 12.456327, mse: 3488.878469, mean_q: 41.895353, mean_eps: 0.585613
  69196/150000: episode: 664, duration: 0.693s, episode steps:  93, steps per second: 134, episode reward: -21.551, mean reward: -0.232 [-100.000, 11.197], mean action: 1.581 [0.000, 3.000],  loss: 7.536664, mse: 3453.311418, mean_q: 41.807482, mean_eps: 0.585106
  69355/150000: episode: 665, duration: 1.227s, episode steps: 159, steps per second: 130, episode reward: -32.515, mean reward: -0.204 [-100.000, 23.514], mean action: 1.849 [0.000, 3.000],  loss: 11.526471, mse: 3429.087399, mean_q: 41.083614, mean_eps: 0.584350
  69499/150000: episode: 666, duration: 1.033s, episode steps: 144, steps per second: 139, episode reward: -90.903, mean reward: -0.631 [-100.000,  4.076], mean action: 1.625 [0.000, 3.000],  loss: 11.803268, mse: 3460.355552, mean_q: 42.911875, mean_eps: 0.583441
  69646/150000: episode: 667, duration: 1.090s, episode steps: 147, steps per second: 135, episode reward: -8.219, mean reward: -0.056 [-100.000, 15.701], mean action: 1.735 [0.000, 3.000],  loss: 11.602436, mse: 3466.754777, mean_q: 42.483259, mean_eps: 0.582568
  69798/150000: episode: 668, duration: 1.076s, episode steps: 152, steps per second: 141, episode reward:  7.471, mean reward:  0.049 [-100.000, 20.387], mean action: 1.618 [0.000, 3.000],  loss: 12.963340, mse: 3505.972782, mean_q: 42.343068, mean_eps: 0.581671
  69939/150000: episode: 669, duration: 1.048s, episode steps: 141, steps per second: 135, episode reward: -64.049, mean reward: -0.454 [-100.000, 22.334], mean action: 1.773 [0.000, 3.000],  loss: 11.850032, mse: 3475.543280, mean_q: 42.550294, mean_eps: 0.580792
  70050/150000: episode: 670, duration: 0.769s, episode steps: 111, steps per second: 144, episode reward: -36.249, mean reward: -0.327 [-100.000,  8.829], mean action: 1.712 [0.000, 3.000],  loss: 10.503018, mse: 3492.772325, mean_q: 42.333684, mean_eps: 0.580036
  70149/150000: episode: 671, duration: 0.711s, episode steps:  99, steps per second: 139, episode reward: -17.711, mean reward: -0.179 [-100.000, 18.355], mean action: 1.576 [0.000, 3.000],  loss: 8.801675, mse: 3570.717897, mean_q: 42.488287, mean_eps: 0.579406
  70247/150000: episode: 672, duration: 0.723s, episode steps:  98, steps per second: 136, episode reward: -85.654, mean reward: -0.874 [-100.000,  5.319], mean action: 1.439 [0.000, 3.000],  loss: 18.092753, mse: 3589.573701, mean_q: 41.583092, mean_eps: 0.578815
  70370/150000: episode: 673, duration: 0.876s, episode steps: 123, steps per second: 140, episode reward: -13.529, mean reward: -0.110 [-100.000, 20.641], mean action: 1.756 [0.000, 3.000],  loss: 12.772333, mse: 3517.256772, mean_q: 41.663973, mean_eps: 0.578152
  70464/150000: episode: 674, duration: 0.669s, episode steps:  94, steps per second: 140, episode reward: -49.068, mean reward: -0.522 [-100.000,  7.301], mean action: 1.596 [0.000, 3.000],  loss: 10.509133, mse: 3558.977472, mean_q: 42.877750, mean_eps: 0.577501
  70551/150000: episode: 675, duration: 0.637s, episode steps:  87, steps per second: 137, episode reward: -84.026, mean reward: -0.966 [-100.000,  6.618], mean action: 1.598 [0.000, 3.000],  loss: 12.012586, mse: 3535.137274, mean_q: 41.892524, mean_eps: 0.576958
  70720/150000: episode: 676, duration: 1.199s, episode steps: 169, steps per second: 141, episode reward: -82.233, mean reward: -0.487 [-100.000, 13.772], mean action: 1.710 [0.000, 3.000],  loss: 14.748275, mse: 3484.356519, mean_q: 41.345641, mean_eps: 0.576190
  70805/150000: episode: 677, duration: 0.639s, episode steps:  85, steps per second: 133, episode reward: -45.850, mean reward: -0.539 [-100.000, 21.753], mean action: 1.506 [0.000, 3.000],  loss: 9.910420, mse: 3510.795272, mean_q: 42.038051, mean_eps: 0.575428
  70882/150000: episode: 678, duration: 0.547s, episode steps:  77, steps per second: 141, episode reward: -68.242, mean reward: -0.886 [-100.000,  6.573], mean action: 1.442 [0.000, 3.000],  loss: 16.791827, mse: 3506.342980, mean_q: 41.367107, mean_eps: 0.574942
  71014/150000: episode: 679, duration: 0.915s, episode steps: 132, steps per second: 144, episode reward: -23.572, mean reward: -0.179 [-100.000, 13.803], mean action: 1.568 [0.000, 3.000],  loss: 11.737106, mse: 3537.666643, mean_q: 42.274493, mean_eps: 0.574315
  71113/150000: episode: 680, duration: 0.836s, episode steps:  99, steps per second: 118, episode reward: -66.906, mean reward: -0.676 [-100.000, 15.069], mean action: 1.616 [0.000, 3.000],  loss: 11.035289, mse: 3535.650124, mean_q: 42.132465, mean_eps: 0.573622
  71230/150000: episode: 681, duration: 0.921s, episode steps: 117, steps per second: 127, episode reward: -53.409, mean reward: -0.456 [-100.000, 23.495], mean action: 1.769 [0.000, 3.000],  loss: 11.659210, mse: 3469.220342, mean_q: 41.516233, mean_eps: 0.572974
  71319/150000: episode: 682, duration: 0.677s, episode steps:  89, steps per second: 131, episode reward: -45.443, mean reward: -0.511 [-100.000, 17.625], mean action: 1.483 [0.000, 3.000],  loss: 16.164459, mse: 3458.149935, mean_q: 43.154113, mean_eps: 0.572356
  71455/150000: episode: 683, duration: 1.062s, episode steps: 136, steps per second: 128, episode reward: -122.195, mean reward: -0.898 [-100.000,  9.655], mean action: 1.669 [0.000, 3.000],  loss: 15.354323, mse: 3503.430942, mean_q: 42.562340, mean_eps: 0.571681
  71600/150000: episode: 684, duration: 1.263s, episode steps: 145, steps per second: 115, episode reward: -161.703, mean reward: -1.115 [-100.000, 44.975], mean action: 1.476 [0.000, 3.000],  loss: 15.263491, mse: 3432.680789, mean_q: 41.430799, mean_eps: 0.570838
  71732/150000: episode: 685, duration: 0.999s, episode steps: 132, steps per second: 132, episode reward: -54.560, mean reward: -0.413 [-100.000, 26.286], mean action: 1.697 [0.000, 3.000],  loss: 14.891861, mse: 3486.360451, mean_q: 43.936471, mean_eps: 0.570007
  71835/150000: episode: 686, duration: 0.718s, episode steps: 103, steps per second: 143, episode reward: -67.519, mean reward: -0.656 [-100.000, 11.319], mean action: 1.621 [0.000, 3.000],  loss: 11.794020, mse: 3447.196905, mean_q: 42.408663, mean_eps: 0.569302
  71906/150000: episode: 687, duration: 0.555s, episode steps:  71, steps per second: 128, episode reward: -50.177, mean reward: -0.707 [-100.000,  9.481], mean action: 1.479 [0.000, 3.000],  loss: 10.220041, mse: 3432.676077, mean_q: 42.260963, mean_eps: 0.568780
  72007/150000: episode: 688, duration: 0.756s, episode steps: 101, steps per second: 134, episode reward: -175.731, mean reward: -1.740 [-100.000,  6.488], mean action: 1.653 [0.000, 3.000],  loss: 9.629773, mse: 3427.387915, mean_q: 42.425312, mean_eps: 0.568264
  72096/150000: episode: 689, duration: 0.620s, episode steps:  89, steps per second: 143, episode reward: -26.741, mean reward: -0.300 [-100.000, 16.903], mean action: 1.562 [0.000, 3.000],  loss: 9.566712, mse: 3501.673304, mean_q: 43.072727, mean_eps: 0.567694
  72211/150000: episode: 690, duration: 0.863s, episode steps: 115, steps per second: 133, episode reward: -6.260, mean reward: -0.054 [-100.000, 28.752], mean action: 1.400 [0.000, 3.000],  loss: 12.469149, mse: 3496.510080, mean_q: 42.790444, mean_eps: 0.567082
  72332/150000: episode: 691, duration: 0.863s, episode steps: 121, steps per second: 140, episode reward: -123.282, mean reward: -1.019 [-100.000,  6.498], mean action: 1.744 [0.000, 3.000],  loss: 8.088923, mse: 3515.321527, mean_q: 43.958745, mean_eps: 0.566374
  72435/150000: episode: 692, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -46.756, mean reward: -0.454 [-100.000, 14.633], mean action: 1.602 [0.000, 3.000],  loss: 13.170832, mse: 3516.422619, mean_q: 43.498702, mean_eps: 0.565702
  72520/150000: episode: 693, duration: 0.650s, episode steps:  85, steps per second: 131, episode reward: -9.795, mean reward: -0.115 [-100.000, 12.155], mean action: 1.600 [0.000, 3.000],  loss: 7.753501, mse: 3550.568098, mean_q: 42.232280, mean_eps: 0.565138
  72622/150000: episode: 694, duration: 0.733s, episode steps: 102, steps per second: 139, episode reward: -49.673, mean reward: -0.487 [-100.000, 15.881], mean action: 1.539 [0.000, 3.000],  loss: 15.197309, mse: 3512.986520, mean_q: 43.108476, mean_eps: 0.564577
  72717/150000: episode: 695, duration: 0.675s, episode steps:  95, steps per second: 141, episode reward: -71.921, mean reward: -0.757 [-100.000, 10.445], mean action: 1.747 [0.000, 3.000],  loss: 12.129762, mse: 3460.255687, mean_q: 43.431723, mean_eps: 0.563986
  73717/150000: episode: 696, duration: 8.400s, episode steps: 1000, steps per second: 119, episode reward: 55.003, mean reward:  0.055 [-21.879, 25.467], mean action: 1.890 [0.000, 3.000],  loss: 11.443316, mse: 3524.838527, mean_q: 44.581183, mean_eps: 0.560701
  73793/150000: episode: 697, duration: 0.577s, episode steps:  76, steps per second: 132, episode reward: -41.239, mean reward: -0.543 [-100.000, 11.800], mean action: 1.526 [0.000, 3.000],  loss: 11.418888, mse: 3502.635829, mean_q: 44.865374, mean_eps: 0.557473
  73911/150000: episode: 698, duration: 0.870s, episode steps: 118, steps per second: 136, episode reward: 14.532, mean reward:  0.123 [-100.000, 15.518], mean action: 1.610 [0.000, 3.000],  loss: 11.552604, mse: 3565.157924, mean_q: 46.330660, mean_eps: 0.556891
  74029/150000: episode: 699, duration: 0.831s, episode steps: 118, steps per second: 142, episode reward: -55.181, mean reward: -0.468 [-100.000,  9.577], mean action: 1.746 [0.000, 3.000],  loss: 11.697696, mse: 3470.846092, mean_q: 44.320570, mean_eps: 0.556183
  74122/150000: episode: 700, duration: 0.688s, episode steps:  93, steps per second: 135, episode reward: -124.555, mean reward: -1.339 [-100.000,  8.899], mean action: 1.312 [0.000, 3.000],  loss: 10.588862, mse: 3582.185066, mean_q: 45.678385, mean_eps: 0.555550
  74249/150000: episode: 701, duration: 0.977s, episode steps: 127, steps per second: 130, episode reward: -70.590, mean reward: -0.556 [-100.000,  8.918], mean action: 1.764 [0.000, 3.000],  loss: 13.728482, mse: 3588.517717, mean_q: 44.656214, mean_eps: 0.554890
  74372/150000: episode: 702, duration: 0.911s, episode steps: 123, steps per second: 135, episode reward: -23.080, mean reward: -0.188 [-100.000, 11.035], mean action: 1.732 [0.000, 3.000],  loss: 10.076946, mse: 3533.750522, mean_q: 43.720609, mean_eps: 0.554140
  74509/150000: episode: 703, duration: 1.090s, episode steps: 137, steps per second: 126, episode reward: 22.606, mean reward:  0.165 [-100.000, 10.641], mean action: 1.730 [0.000, 3.000],  loss: 12.567151, mse: 3552.937614, mean_q: 45.215941, mean_eps: 0.553360
  74617/150000: episode: 704, duration: 0.787s, episode steps: 108, steps per second: 137, episode reward: -93.956, mean reward: -0.870 [-100.000,  6.337], mean action: 1.741 [0.000, 3.000],  loss: 9.386811, mse: 3523.124008, mean_q: 44.613461, mean_eps: 0.552625
  74716/150000: episode: 705, duration: 0.737s, episode steps:  99, steps per second: 134, episode reward: -20.210, mean reward: -0.204 [-100.000, 14.899], mean action: 1.828 [0.000, 3.000],  loss: 15.183598, mse: 3560.881520, mean_q: 44.582462, mean_eps: 0.552004
  74805/150000: episode: 706, duration: 0.635s, episode steps:  89, steps per second: 140, episode reward: -60.895, mean reward: -0.684 [-100.000,  9.764], mean action: 1.663 [0.000, 3.000],  loss: 10.990130, mse: 3525.705981, mean_q: 44.500477, mean_eps: 0.551440
  74913/150000: episode: 707, duration: 0.752s, episode steps: 108, steps per second: 144, episode reward: -2.298, mean reward: -0.021 [-100.000, 11.647], mean action: 1.630 [0.000, 3.000],  loss: 8.736805, mse: 3571.804088, mean_q: 45.515015, mean_eps: 0.550849
  75044/150000: episode: 708, duration: 0.964s, episode steps: 131, steps per second: 136, episode reward: 17.253, mean reward:  0.132 [-100.000, 28.632], mean action: 1.473 [0.000, 3.000],  loss: 17.225472, mse: 3601.297065, mean_q: 44.728863, mean_eps: 0.550132
  75143/150000: episode: 709, duration: 0.713s, episode steps:  99, steps per second: 139, episode reward: -36.360, mean reward: -0.367 [-100.000, 13.925], mean action: 1.657 [0.000, 3.000],  loss: 10.173211, mse: 3635.457961, mean_q: 45.082362, mean_eps: 0.549442
  75257/150000: episode: 710, duration: 0.805s, episode steps: 114, steps per second: 142, episode reward: -79.846, mean reward: -0.700 [-100.000, 17.717], mean action: 1.807 [0.000, 3.000],  loss: 11.400088, mse: 3546.886995, mean_q: 44.420043, mean_eps: 0.548803
  75673/150000: episode: 711, duration: 3.174s, episode steps: 416, steps per second: 131, episode reward: -202.814, mean reward: -0.488 [-100.000,  6.966], mean action: 1.647 [0.000, 3.000],  loss: 12.293672, mse: 3625.980293, mean_q: 45.926457, mean_eps: 0.547213
  75789/150000: episode: 712, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -21.362, mean reward: -0.184 [-100.000, 21.202], mean action: 1.569 [0.000, 3.000],  loss: 11.716315, mse: 3613.917352, mean_q: 45.863504, mean_eps: 0.545617
  75884/150000: episode: 713, duration: 0.718s, episode steps:  95, steps per second: 132, episode reward: -67.402, mean reward: -0.709 [-100.000, 21.208], mean action: 1.526 [0.000, 3.000],  loss: 7.985608, mse: 3604.635580, mean_q: 45.455088, mean_eps: 0.544984
  75976/150000: episode: 714, duration: 0.672s, episode steps:  92, steps per second: 137, episode reward: -69.681, mean reward: -0.757 [-100.000,  8.961], mean action: 1.848 [0.000, 3.000],  loss: 14.074417, mse: 3559.911236, mean_q: 44.871250, mean_eps: 0.544423
  76090/150000: episode: 715, duration: 0.798s, episode steps: 114, steps per second: 143, episode reward: -42.716, mean reward: -0.375 [-100.000, 14.617], mean action: 1.667 [0.000, 3.000],  loss: 10.530875, mse: 3654.567965, mean_q: 46.743269, mean_eps: 0.543805
  76204/150000: episode: 716, duration: 0.852s, episode steps: 114, steps per second: 134, episode reward: -114.526, mean reward: -1.005 [-100.000,  5.948], mean action: 1.561 [0.000, 3.000],  loss: 10.988231, mse: 3728.692301, mean_q: 47.124574, mean_eps: 0.543121
  76311/150000: episode: 717, duration: 0.768s, episode steps: 107, steps per second: 139, episode reward: -89.347, mean reward: -0.835 [-100.000, 12.508], mean action: 1.701 [0.000, 3.000],  loss: 8.717684, mse: 3661.816037, mean_q: 46.001408, mean_eps: 0.542458
  76462/150000: episode: 718, duration: 1.094s, episode steps: 151, steps per second: 138, episode reward: -85.475, mean reward: -0.566 [-100.000, 19.556], mean action: 1.834 [0.000, 3.000],  loss: 8.312147, mse: 3717.157404, mean_q: 46.658763, mean_eps: 0.541684
  76590/150000: episode: 719, duration: 0.914s, episode steps: 128, steps per second: 140, episode reward: -171.679, mean reward: -1.341 [-100.000, 37.893], mean action: 1.578 [0.000, 3.000],  loss: 9.605751, mse: 3686.283655, mean_q: 46.780211, mean_eps: 0.540847
  76687/150000: episode: 720, duration: 0.688s, episode steps:  97, steps per second: 141, episode reward: -59.799, mean reward: -0.616 [-100.000,  6.076], mean action: 1.814 [0.000, 3.000],  loss: 11.991923, mse: 3753.785705, mean_q: 48.221581, mean_eps: 0.540172
  76861/150000: episode: 721, duration: 1.285s, episode steps: 174, steps per second: 135, episode reward: -14.765, mean reward: -0.085 [-100.000, 20.311], mean action: 1.644 [0.000, 3.000],  loss: 11.092100, mse: 3700.065392, mean_q: 47.258354, mean_eps: 0.539359
  76948/150000: episode: 722, duration: 0.609s, episode steps:  87, steps per second: 143, episode reward: -67.957, mean reward: -0.781 [-100.000, 10.251], mean action: 1.644 [0.000, 3.000],  loss: 10.482994, mse: 3692.365857, mean_q: 46.801993, mean_eps: 0.538576
  77043/150000: episode: 723, duration: 0.706s, episode steps:  95, steps per second: 135, episode reward: -51.263, mean reward: -0.540 [-100.000, 13.520], mean action: 1.526 [0.000, 3.000],  loss: 14.199661, mse: 3749.862312, mean_q: 48.368606, mean_eps: 0.538030
  77152/150000: episode: 724, duration: 0.803s, episode steps: 109, steps per second: 136, episode reward: -53.163, mean reward: -0.488 [-100.000, 16.297], mean action: 1.761 [0.000, 3.000],  loss: 8.691205, mse: 3752.909887, mean_q: 47.834533, mean_eps: 0.537418
  77254/150000: episode: 725, duration: 0.716s, episode steps: 102, steps per second: 142, episode reward: -83.724, mean reward: -0.821 [-100.000, 19.064], mean action: 1.618 [0.000, 3.000],  loss: 13.107611, mse: 3734.898411, mean_q: 48.764867, mean_eps: 0.536785
  77367/150000: episode: 726, duration: 0.958s, episode steps: 113, steps per second: 118, episode reward: -0.268, mean reward: -0.002 [-100.000, 28.045], mean action: 1.531 [0.000, 3.000],  loss: 9.760707, mse: 3756.627951, mean_q: 48.922808, mean_eps: 0.536140
  77477/150000: episode: 727, duration: 0.775s, episode steps: 110, steps per second: 142, episode reward: -47.009, mean reward: -0.427 [-100.000, 74.407], mean action: 1.745 [0.000, 3.000],  loss: 11.644512, mse: 3738.889504, mean_q: 48.128936, mean_eps: 0.535471
  77565/150000: episode: 728, duration: 0.628s, episode steps:  88, steps per second: 140, episode reward: -83.434, mean reward: -0.948 [-100.000,  7.752], mean action: 1.784 [0.000, 3.000],  loss: 11.784476, mse: 3790.703527, mean_q: 48.113790, mean_eps: 0.534877
  77672/150000: episode: 729, duration: 0.811s, episode steps: 107, steps per second: 132, episode reward: -37.325, mean reward: -0.349 [-100.000, 10.811], mean action: 1.579 [0.000, 3.000],  loss: 9.199587, mse: 3790.217358, mean_q: 48.867352, mean_eps: 0.534292
  77768/150000: episode: 730, duration: 0.683s, episode steps:  96, steps per second: 141, episode reward: -48.276, mean reward: -0.503 [-100.000, 11.730], mean action: 1.677 [0.000, 3.000],  loss: 12.723424, mse: 3751.322342, mean_q: 47.755223, mean_eps: 0.533683
  77875/150000: episode: 731, duration: 0.763s, episode steps: 107, steps per second: 140, episode reward: -73.323, mean reward: -0.685 [-100.000, 23.409], mean action: 1.570 [0.000, 3.000],  loss: 12.537678, mse: 3751.117845, mean_q: 48.721627, mean_eps: 0.533074
  78240/150000: episode: 732, duration: 2.692s, episode steps: 365, steps per second: 136, episode reward: -254.864, mean reward: -0.698 [-100.000, 18.704], mean action: 1.578 [0.000, 3.000],  loss: 12.027933, mse: 3765.427515, mean_q: 48.196514, mean_eps: 0.531658
  78334/150000: episode: 733, duration: 0.690s, episode steps:  94, steps per second: 136, episode reward: 55.709, mean reward:  0.593 [-100.000, 23.530], mean action: 1.702 [0.000, 3.000],  loss: 16.490599, mse: 3703.531346, mean_q: 46.566470, mean_eps: 0.530281
  78449/150000: episode: 734, duration: 0.841s, episode steps: 115, steps per second: 137, episode reward: -7.289, mean reward: -0.063 [-100.000, 41.073], mean action: 1.670 [0.000, 3.000],  loss: 14.918107, mse: 3801.604518, mean_q: 48.803110, mean_eps: 0.529654
  78539/150000: episode: 735, duration: 0.704s, episode steps:  90, steps per second: 128, episode reward: 37.394, mean reward:  0.415 [-100.000, 20.578], mean action: 1.789 [0.000, 3.000],  loss: 9.124878, mse: 3834.901853, mean_q: 48.606779, mean_eps: 0.529039
  78660/150000: episode: 736, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward:  1.204, mean reward:  0.010 [-100.000, 10.537], mean action: 1.620 [0.000, 3.000],  loss: 13.110288, mse: 3797.477523, mean_q: 48.214900, mean_eps: 0.528406
  78776/150000: episode: 737, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -27.932, mean reward: -0.241 [-100.000, 16.331], mean action: 1.647 [0.000, 3.000],  loss: 10.754786, mse: 3751.099841, mean_q: 47.042165, mean_eps: 0.527695
  78853/150000: episode: 738, duration: 0.624s, episode steps:  77, steps per second: 123, episode reward: -87.254, mean reward: -1.133 [-100.000,  9.674], mean action: 1.468 [0.000, 3.000],  loss: 11.412310, mse: 3802.507223, mean_q: 47.889294, mean_eps: 0.527116
  79014/150000: episode: 739, duration: 1.367s, episode steps: 161, steps per second: 118, episode reward: -216.780, mean reward: -1.346 [-100.000, 55.389], mean action: 1.640 [0.000, 3.000],  loss: 10.134296, mse: 3736.277031, mean_q: 48.150789, mean_eps: 0.526402
  79139/150000: episode: 740, duration: 1.060s, episode steps: 125, steps per second: 118, episode reward: -28.775, mean reward: -0.230 [-100.000, 17.836], mean action: 1.576 [0.000, 3.000],  loss: 10.782814, mse: 3836.789449, mean_q: 47.977614, mean_eps: 0.525544
  79256/150000: episode: 741, duration: 0.877s, episode steps: 117, steps per second: 133, episode reward: -159.896, mean reward: -1.367 [-100.000, 31.183], mean action: 1.692 [0.000, 3.000],  loss: 8.902741, mse: 3849.020575, mean_q: 47.864889, mean_eps: 0.524818
  79337/150000: episode: 742, duration: 0.623s, episode steps:  81, steps per second: 130, episode reward: -10.765, mean reward: -0.133 [-100.000,  8.903], mean action: 1.556 [0.000, 3.000],  loss: 8.185516, mse: 3863.891433, mean_q: 48.363172, mean_eps: 0.524224
  79486/150000: episode: 743, duration: 1.100s, episode steps: 149, steps per second: 135, episode reward: 25.870, mean reward:  0.174 [-100.000, 21.623], mean action: 1.752 [0.000, 3.000],  loss: 12.936292, mse: 3806.061853, mean_q: 47.604089, mean_eps: 0.523534
  80486/150000: episode: 744, duration: 8.946s, episode steps: 1000, steps per second: 112, episode reward: 38.702, mean reward:  0.039 [-24.043, 24.846], mean action: 1.777 [0.000, 3.000],  loss: 9.883011, mse: 3816.436538, mean_q: 48.263137, mean_eps: 0.520087
  80595/150000: episode: 745, duration: 0.853s, episode steps: 109, steps per second: 128, episode reward: 48.822, mean reward:  0.448 [-100.000, 14.861], mean action: 1.761 [0.000, 3.000],  loss: 10.333509, mse: 3885.791408, mean_q: 49.279067, mean_eps: 0.516760
  80731/150000: episode: 746, duration: 1.159s, episode steps: 136, steps per second: 117, episode reward: 30.377, mean reward:  0.223 [-100.000, 16.867], mean action: 1.676 [0.000, 3.000],  loss: 11.455210, mse: 3824.421259, mean_q: 49.027900, mean_eps: 0.516025
  80824/150000: episode: 747, duration: 0.819s, episode steps:  93, steps per second: 114, episode reward: -7.376, mean reward: -0.079 [-100.000,  8.391], mean action: 1.699 [0.000, 3.000],  loss: 9.758151, mse: 3768.410621, mean_q: 48.075729, mean_eps: 0.515338
  80938/150000: episode: 748, duration: 0.905s, episode steps: 114, steps per second: 126, episode reward: -38.668, mean reward: -0.339 [-100.000, 14.522], mean action: 1.702 [0.000, 3.000],  loss: 13.095663, mse: 3773.013158, mean_q: 48.208386, mean_eps: 0.514717
  81025/150000: episode: 749, duration: 0.635s, episode steps:  87, steps per second: 137, episode reward: -34.774, mean reward: -0.400 [-100.000,  9.328], mean action: 1.782 [0.000, 3.000],  loss: 13.133383, mse: 3832.048564, mean_q: 48.940310, mean_eps: 0.514114
  81159/150000: episode: 750, duration: 0.981s, episode steps: 134, steps per second: 137, episode reward: -61.146, mean reward: -0.456 [-100.000,  6.714], mean action: 1.373 [0.000, 3.000],  loss: 13.173677, mse: 3835.847740, mean_q: 48.840956, mean_eps: 0.513451
  81269/150000: episode: 751, duration: 0.792s, episode steps: 110, steps per second: 139, episode reward:  7.532, mean reward:  0.068 [-100.000, 15.671], mean action: 1.600 [0.000, 3.000],  loss: 8.049208, mse: 3833.462027, mean_q: 49.661576, mean_eps: 0.512719
  81383/150000: episode: 752, duration: 0.882s, episode steps: 114, steps per second: 129, episode reward:  3.509, mean reward:  0.031 [-100.000, 28.634], mean action: 1.728 [0.000, 3.000],  loss: 11.915528, mse: 3815.222517, mean_q: 48.780660, mean_eps: 0.512047
  81482/150000: episode: 753, duration: 0.751s, episode steps:  99, steps per second: 132, episode reward: -51.048, mean reward: -0.516 [-100.000, 11.046], mean action: 1.667 [0.000, 3.000],  loss: 9.622360, mse: 3900.628541, mean_q: 49.934416, mean_eps: 0.511408
  81819/150000: episode: 754, duration: 2.784s, episode steps: 337, steps per second: 121, episode reward: -311.570, mean reward: -0.925 [-100.000, 10.931], mean action: 1.730 [0.000, 3.000],  loss: 12.334906, mse: 3782.727648, mean_q: 48.762900, mean_eps: 0.510100
  82805/150000: episode: 755, duration: 7.941s, episode steps: 986, steps per second: 124, episode reward: -298.544, mean reward: -0.303 [-100.000, 21.733], mean action: 1.391 [0.000, 3.000],  loss: 10.975929, mse: 3840.048582, mean_q: 48.695023, mean_eps: 0.506131
  82892/150000: episode: 756, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -56.011, mean reward: -0.644 [-100.000,  9.094], mean action: 1.713 [0.000, 3.000],  loss: 10.130238, mse: 3838.736513, mean_q: 49.005863, mean_eps: 0.502912
  82993/150000: episode: 757, duration: 0.712s, episode steps: 101, steps per second: 142, episode reward: -28.410, mean reward: -0.281 [-100.000, 17.670], mean action: 1.693 [0.000, 3.000],  loss: 10.732311, mse: 3851.151901, mean_q: 49.619003, mean_eps: 0.502348
  83105/150000: episode: 758, duration: 0.833s, episode steps: 112, steps per second: 135, episode reward: -73.029, mean reward: -0.652 [-100.000,  7.950], mean action: 1.500 [0.000, 3.000],  loss: 14.802748, mse: 3877.556157, mean_q: 48.242952, mean_eps: 0.501709
  83252/150000: episode: 759, duration: 1.065s, episode steps: 147, steps per second: 138, episode reward: -9.946, mean reward: -0.068 [-100.000, 10.089], mean action: 1.612 [0.000, 3.000],  loss: 10.731015, mse: 3898.409329, mean_q: 49.747906, mean_eps: 0.500932
  83374/150000: episode: 760, duration: 0.921s, episode steps: 122, steps per second: 133, episode reward: -75.359, mean reward: -0.618 [-100.000, 15.176], mean action: 1.844 [0.000, 3.000],  loss: 8.765803, mse: 3863.750628, mean_q: 48.846485, mean_eps: 0.500125
  83480/150000: episode: 761, duration: 0.759s, episode steps: 106, steps per second: 140, episode reward: -41.981, mean reward: -0.396 [-100.000, 11.637], mean action: 1.745 [0.000, 3.000],  loss: 10.017651, mse: 3918.712743, mean_q: 48.755528, mean_eps: 0.499441
  83618/150000: episode: 762, duration: 0.965s, episode steps: 138, steps per second: 143, episode reward: -55.226, mean reward: -0.400 [-100.000, 10.862], mean action: 1.659 [0.000, 3.000],  loss: 11.945156, mse: 3943.063059, mean_q: 49.861881, mean_eps: 0.498709
  83739/150000: episode: 763, duration: 0.923s, episode steps: 121, steps per second: 131, episode reward: -35.547, mean reward: -0.294 [-100.000,  7.002], mean action: 1.777 [0.000, 3.000],  loss: 8.695269, mse: 3899.748446, mean_q: 49.303257, mean_eps: 0.497932
  83865/150000: episode: 764, duration: 0.890s, episode steps: 126, steps per second: 142, episode reward: 12.714, mean reward:  0.101 [-100.000, 17.562], mean action: 1.556 [0.000, 3.000],  loss: 11.314078, mse: 3955.519423, mean_q: 49.556403, mean_eps: 0.497191
  83971/150000: episode: 765, duration: 0.792s, episode steps: 106, steps per second: 134, episode reward: -46.026, mean reward: -0.434 [-100.000, 11.165], mean action: 1.774 [0.000, 3.000],  loss: 10.418739, mse: 3905.911997, mean_q: 49.166042, mean_eps: 0.496495
  84386/150000: episode: 766, duration: 3.095s, episode steps: 415, steps per second: 134, episode reward: -321.973, mean reward: -0.776 [-100.000, 19.199], mean action: 1.639 [0.000, 3.000],  loss: 9.996316, mse: 3939.923843, mean_q: 49.154577, mean_eps: 0.494932
  84540/150000: episode: 767, duration: 1.108s, episode steps: 154, steps per second: 139, episode reward: -118.581, mean reward: -0.770 [-100.000, 17.767], mean action: 1.532 [0.000, 3.000],  loss: 11.067581, mse: 3995.505788, mean_q: 50.511526, mean_eps: 0.493225
  84617/150000: episode: 768, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: -28.317, mean reward: -0.368 [-100.000, 16.764], mean action: 1.623 [0.000, 3.000],  loss: 14.196675, mse: 4061.246002, mean_q: 49.894108, mean_eps: 0.492532
  84755/150000: episode: 769, duration: 0.969s, episode steps: 138, steps per second: 142, episode reward: -4.277, mean reward: -0.031 [-100.000, 14.389], mean action: 1.674 [0.000, 3.000],  loss: 13.555988, mse: 3937.739320, mean_q: 48.944969, mean_eps: 0.491887
  84860/150000: episode: 770, duration: 0.775s, episode steps: 105, steps per second: 135, episode reward:  7.428, mean reward:  0.071 [-100.000, 15.267], mean action: 1.543 [0.000, 3.000],  loss: 9.500072, mse: 3892.873363, mean_q: 49.739495, mean_eps: 0.491158
  84946/150000: episode: 771, duration: 0.627s, episode steps:  86, steps per second: 137, episode reward: -45.761, mean reward: -0.532 [-100.000, 33.152], mean action: 1.849 [0.000, 3.000],  loss: 11.894049, mse: 3950.701376, mean_q: 49.916715, mean_eps: 0.490585
  85072/150000: episode: 772, duration: 0.893s, episode steps: 126, steps per second: 141, episode reward: -13.718, mean reward: -0.109 [-100.000,  8.012], mean action: 1.571 [0.000, 3.000],  loss: 11.232710, mse: 3996.754085, mean_q: 50.328812, mean_eps: 0.489949
  85346/150000: episode: 773, duration: 2.201s, episode steps: 274, steps per second: 124, episode reward: -237.123, mean reward: -0.865 [-100.000,  5.702], mean action: 1.752 [0.000, 3.000],  loss: 10.758262, mse: 3949.074362, mean_q: 50.193400, mean_eps: 0.488749
  85447/150000: episode: 774, duration: 0.888s, episode steps: 101, steps per second: 114, episode reward: -33.222, mean reward: -0.329 [-100.000, 20.910], mean action: 1.515 [0.000, 3.000],  loss: 12.181052, mse: 3934.215625, mean_q: 48.840874, mean_eps: 0.487624
  85827/150000: episode: 775, duration: 3.485s, episode steps: 380, steps per second: 109, episode reward: -370.477, mean reward: -0.975 [-100.000, 67.661], mean action: 1.603 [0.000, 3.000],  loss: 8.955407, mse: 3937.744200, mean_q: 50.283884, mean_eps: 0.486181
  86092/150000: episode: 776, duration: 2.105s, episode steps: 265, steps per second: 126, episode reward: -246.285, mean reward: -0.929 [-100.000,  5.381], mean action: 1.702 [0.000, 3.000],  loss: 11.671029, mse: 3962.283273, mean_q: 49.770555, mean_eps: 0.484246
  86213/150000: episode: 777, duration: 0.917s, episode steps: 121, steps per second: 132, episode reward: -9.382, mean reward: -0.078 [-100.000, 18.265], mean action: 1.554 [0.000, 3.000],  loss: 10.493824, mse: 3974.014477, mean_q: 49.744243, mean_eps: 0.483088
  86383/150000: episode: 778, duration: 1.204s, episode steps: 170, steps per second: 141, episode reward: -292.825, mean reward: -1.722 [-100.000, 50.644], mean action: 1.641 [0.000, 3.000],  loss: 13.818323, mse: 4018.199421, mean_q: 50.632212, mean_eps: 0.482215
  86492/150000: episode: 779, duration: 0.802s, episode steps: 109, steps per second: 136, episode reward: -71.926, mean reward: -0.660 [-100.000,  9.800], mean action: 1.587 [0.000, 3.000],  loss: 9.480479, mse: 3981.967702, mean_q: 49.738916, mean_eps: 0.481378
  87070/150000: episode: 780, duration: 4.424s, episode steps: 578, steps per second: 131, episode reward: -99.212, mean reward: -0.172 [-100.000, 24.194], mean action: 1.587 [0.000, 3.000],  loss: 11.711967, mse: 3993.458046, mean_q: 50.080927, mean_eps: 0.479317
  87280/150000: episode: 781, duration: 1.493s, episode steps: 210, steps per second: 141, episode reward: -235.899, mean reward: -1.123 [-100.000, 10.615], mean action: 1.610 [0.000, 3.000],  loss: 12.514453, mse: 4025.112534, mean_q: 50.594993, mean_eps: 0.476953
  87513/150000: episode: 782, duration: 1.761s, episode steps: 233, steps per second: 132, episode reward: -39.993, mean reward: -0.172 [-100.000,  8.882], mean action: 1.682 [0.000, 3.000],  loss: 10.201851, mse: 4028.150888, mean_q: 50.448696, mean_eps: 0.475624
  87768/150000: episode: 783, duration: 1.885s, episode steps: 255, steps per second: 135, episode reward: -302.520, mean reward: -1.186 [-100.000,  5.416], mean action: 1.722 [0.000, 3.000],  loss: 9.901098, mse: 3985.865543, mean_q: 50.693789, mean_eps: 0.474160
  87899/150000: episode: 784, duration: 0.948s, episode steps: 131, steps per second: 138, episode reward: -38.055, mean reward: -0.290 [-100.000, 15.742], mean action: 1.634 [0.000, 3.000],  loss: 10.066564, mse: 3930.993857, mean_q: 49.822598, mean_eps: 0.473002
  88017/150000: episode: 785, duration: 0.858s, episode steps: 118, steps per second: 137, episode reward: -43.031, mean reward: -0.365 [-100.000,  9.061], mean action: 1.576 [0.000, 3.000],  loss: 14.033632, mse: 3983.372542, mean_q: 50.553060, mean_eps: 0.472255
  88144/150000: episode: 786, duration: 0.900s, episode steps: 127, steps per second: 141, episode reward: -3.911, mean reward: -0.031 [-100.000,  7.714], mean action: 1.551 [0.000, 3.000],  loss: 11.209622, mse: 4040.896561, mean_q: 49.276624, mean_eps: 0.471520
  88240/150000: episode: 787, duration: 0.792s, episode steps:  96, steps per second: 121, episode reward: -53.174, mean reward: -0.554 [-100.000,  7.537], mean action: 1.729 [0.000, 3.000],  loss: 8.710788, mse: 3984.300631, mean_q: 50.241726, mean_eps: 0.470851
  88333/150000: episode: 788, duration: 0.735s, episode steps:  93, steps per second: 127, episode reward: -68.690, mean reward: -0.739 [-100.000, 11.480], mean action: 1.441 [0.000, 3.000],  loss: 10.862340, mse: 4084.168861, mean_q: 50.855407, mean_eps: 0.470284
  88425/150000: episode: 789, duration: 0.715s, episode steps:  92, steps per second: 129, episode reward: -5.172, mean reward: -0.056 [-100.000, 14.160], mean action: 1.609 [0.000, 3.000],  loss: 13.064202, mse: 4129.893539, mean_q: 51.661415, mean_eps: 0.469729
  89275/150000: episode: 790, duration: 7.488s, episode steps: 850, steps per second: 114, episode reward: -336.646, mean reward: -0.396 [-100.000, 20.502], mean action: 1.656 [0.000, 3.000],  loss: 10.019379, mse: 4070.703663, mean_q: 50.800240, mean_eps: 0.466903
  89374/150000: episode: 791, duration: 0.762s, episode steps:  99, steps per second: 130, episode reward: -38.249, mean reward: -0.386 [-100.000, 20.822], mean action: 1.434 [0.000, 3.000],  loss: 10.567507, mse: 4125.242459, mean_q: 50.220074, mean_eps: 0.464056
  89649/150000: episode: 792, duration: 2.192s, episode steps: 275, steps per second: 125, episode reward: -231.694, mean reward: -0.843 [-100.000, 13.124], mean action: 1.615 [0.000, 3.000],  loss: 11.846676, mse: 4063.851520, mean_q: 50.358133, mean_eps: 0.462934
  89829/150000: episode: 793, duration: 1.381s, episode steps: 180, steps per second: 130, episode reward: -128.884, mean reward: -0.716 [-100.000, 10.273], mean action: 1.539 [0.000, 3.000],  loss: 10.340753, mse: 4127.043759, mean_q: 51.350077, mean_eps: 0.461569
  89953/150000: episode: 794, duration: 0.931s, episode steps: 124, steps per second: 133, episode reward: -25.329, mean reward: -0.204 [-100.000, 11.406], mean action: 1.524 [0.000, 3.000],  loss: 10.347524, mse: 4123.450071, mean_q: 50.842988, mean_eps: 0.460657
  90057/150000: episode: 795, duration: 0.801s, episode steps: 104, steps per second: 130, episode reward: -94.310, mean reward: -0.907 [-100.000, 12.139], mean action: 1.702 [0.000, 3.000],  loss: 10.150147, mse: 4143.660426, mean_q: 51.707169, mean_eps: 0.459973
  90189/150000: episode: 796, duration: 1.107s, episode steps: 132, steps per second: 119, episode reward: -61.170, mean reward: -0.463 [-100.000, 24.298], mean action: 1.682 [0.000, 3.000],  loss: 11.639535, mse: 4165.317984, mean_q: 50.676301, mean_eps: 0.459265
  90283/150000: episode: 797, duration: 0.797s, episode steps:  94, steps per second: 118, episode reward: -0.105, mean reward: -0.001 [-100.000, 11.565], mean action: 1.745 [0.000, 3.000],  loss: 11.939295, mse: 4246.881298, mean_q: 52.322581, mean_eps: 0.458587
  90359/150000: episode: 798, duration: 0.688s, episode steps:  76, steps per second: 110, episode reward: -12.956, mean reward: -0.170 [-100.000,  8.682], mean action: 1.513 [0.000, 3.000],  loss: 8.712807, mse: 4164.903796, mean_q: 51.937970, mean_eps: 0.458077
  90474/150000: episode: 799, duration: 0.930s, episode steps: 115, steps per second: 124, episode reward: -65.818, mean reward: -0.572 [-100.000,  8.122], mean action: 1.609 [0.000, 3.000],  loss: 10.920179, mse: 4125.508936, mean_q: 51.203307, mean_eps: 0.457504
  90567/150000: episode: 800, duration: 0.762s, episode steps:  93, steps per second: 122, episode reward:  8.990, mean reward:  0.097 [-100.000, 18.561], mean action: 1.753 [0.000, 3.000],  loss: 10.914539, mse: 4137.772731, mean_q: 50.916979, mean_eps: 0.456880
  91567/150000: episode: 801, duration: 8.630s, episode steps: 1000, steps per second: 116, episode reward: 62.546, mean reward:  0.063 [-22.407, 23.617], mean action: 1.817 [0.000, 3.000],  loss: 11.577278, mse: 4199.570861, mean_q: 52.473212, mean_eps: 0.453601
  91738/150000: episode: 802, duration: 1.321s, episode steps: 171, steps per second: 129, episode reward: -275.725, mean reward: -1.612 [-100.000,  5.102], mean action: 1.690 [0.000, 3.000],  loss: 13.302111, mse: 4138.838525, mean_q: 52.415877, mean_eps: 0.450088
  91961/150000: episode: 803, duration: 1.804s, episode steps: 223, steps per second: 124, episode reward: -153.908, mean reward: -0.690 [-100.000,  4.878], mean action: 1.677 [0.000, 3.000],  loss: 11.985887, mse: 4191.421050, mean_q: 52.666698, mean_eps: 0.448906
  92365/150000: episode: 804, duration: 3.004s, episode steps: 404, steps per second: 134, episode reward: -206.425, mean reward: -0.511 [-100.000, 14.460], mean action: 1.705 [0.000, 3.000],  loss: 12.335414, mse: 4260.821821, mean_q: 53.811978, mean_eps: 0.447025
  92636/150000: episode: 805, duration: 2.094s, episode steps: 271, steps per second: 129, episode reward: -141.537, mean reward: -0.522 [-100.000,  4.863], mean action: 1.686 [0.000, 3.000],  loss: 13.351835, mse: 4272.411117, mean_q: 53.838645, mean_eps: 0.445000
  92757/150000: episode: 806, duration: 0.926s, episode steps: 121, steps per second: 131, episode reward: -44.381, mean reward: -0.367 [-100.000,  8.332], mean action: 1.719 [0.000, 3.000],  loss: 10.982658, mse: 4203.600818, mean_q: 52.430867, mean_eps: 0.443824
  92871/150000: episode: 807, duration: 0.845s, episode steps: 114, steps per second: 135, episode reward: -29.058, mean reward: -0.255 [-100.000, 11.202], mean action: 1.596 [0.000, 3.000],  loss: 13.552008, mse: 4265.747021, mean_q: 53.641290, mean_eps: 0.443119
  92987/150000: episode: 808, duration: 0.853s, episode steps: 116, steps per second: 136, episode reward: -30.804, mean reward: -0.266 [-100.000, 15.371], mean action: 1.595 [0.000, 3.000],  loss: 16.068586, mse: 4216.171991, mean_q: 53.048328, mean_eps: 0.442429
  93086/150000: episode: 809, duration: 0.712s, episode steps:  99, steps per second: 139, episode reward: -35.271, mean reward: -0.356 [-100.000, 14.371], mean action: 1.576 [0.000, 3.000],  loss: 13.088552, mse: 4214.870687, mean_q: 53.279035, mean_eps: 0.441784
  93317/150000: episode: 810, duration: 1.705s, episode steps: 231, steps per second: 135, episode reward: -159.955, mean reward: -0.692 [-100.000,  4.868], mean action: 1.723 [0.000, 3.000],  loss: 13.081539, mse: 4269.138884, mean_q: 54.753251, mean_eps: 0.440794
  93437/150000: episode: 811, duration: 0.883s, episode steps: 120, steps per second: 136, episode reward: -15.597, mean reward: -0.130 [-100.000, 20.249], mean action: 1.617 [0.000, 3.000],  loss: 15.633596, mse: 4265.655910, mean_q: 53.977688, mean_eps: 0.439741
  93626/150000: episode: 812, duration: 1.524s, episode steps: 189, steps per second: 124, episode reward: -13.547, mean reward: -0.072 [-100.000, 18.139], mean action: 1.598 [0.000, 3.000],  loss: 12.631654, mse: 4325.484283, mean_q: 54.501106, mean_eps: 0.438814
  93911/150000: episode: 813, duration: 2.182s, episode steps: 285, steps per second: 131, episode reward: -203.192, mean reward: -0.713 [-100.000,  5.249], mean action: 1.695 [0.000, 3.000],  loss: 13.638443, mse: 4266.749898, mean_q: 53.359711, mean_eps: 0.437392
  94033/150000: episode: 814, duration: 0.959s, episode steps: 122, steps per second: 127, episode reward: -58.963, mean reward: -0.483 [-100.000, 11.318], mean action: 1.664 [0.000, 3.000],  loss: 16.443538, mse: 4330.440202, mean_q: 54.661756, mean_eps: 0.436171
  94148/150000: episode: 815, duration: 0.888s, episode steps: 115, steps per second: 130, episode reward: -48.975, mean reward: -0.426 [-100.000,  9.842], mean action: 1.661 [0.000, 3.000],  loss: 10.590359, mse: 4331.941517, mean_q: 54.666619, mean_eps: 0.435460
  94248/150000: episode: 816, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: -90.540, mean reward: -0.905 [-100.000,  8.446], mean action: 1.480 [0.000, 3.000],  loss: 11.610352, mse: 4324.051873, mean_q: 53.969074, mean_eps: 0.434815
  94328/150000: episode: 817, duration: 0.584s, episode steps:  80, steps per second: 137, episode reward: -66.703, mean reward: -0.834 [-100.000, 11.509], mean action: 1.812 [0.000, 3.000],  loss: 10.628260, mse: 4289.124500, mean_q: 53.217450, mean_eps: 0.434275
  94970/150000: episode: 818, duration: 4.846s, episode steps: 642, steps per second: 132, episode reward: -129.318, mean reward: -0.201 [-100.000, 14.642], mean action: 1.667 [0.000, 3.000],  loss: 13.742050, mse: 4310.832210, mean_q: 53.675168, mean_eps: 0.432109
  95039/150000: episode: 819, duration: 0.516s, episode steps:  69, steps per second: 134, episode reward: -34.882, mean reward: -0.506 [-100.000,  9.742], mean action: 1.913 [0.000, 3.000],  loss: 11.530884, mse: 4277.568158, mean_q: 53.877332, mean_eps: 0.429976
  95163/150000: episode: 820, duration: 0.876s, episode steps: 124, steps per second: 142, episode reward: -20.108, mean reward: -0.162 [-100.000, 11.073], mean action: 1.669 [0.000, 3.000],  loss: 13.406431, mse: 4313.231040, mean_q: 53.366131, mean_eps: 0.429397
  95569/150000: episode: 821, duration: 3.035s, episode steps: 406, steps per second: 134, episode reward: -189.101, mean reward: -0.466 [-100.000,  5.793], mean action: 1.707 [0.000, 3.000],  loss: 14.030892, mse: 4365.151387, mean_q: 54.650448, mean_eps: 0.427807
  95863/150000: episode: 822, duration: 2.441s, episode steps: 294, steps per second: 120, episode reward: -91.389, mean reward: -0.311 [-100.000, 10.988], mean action: 1.735 [0.000, 3.000],  loss: 13.615440, mse: 4360.547232, mean_q: 53.967798, mean_eps: 0.425707
  95951/150000: episode: 823, duration: 0.621s, episode steps:  88, steps per second: 142, episode reward: -26.910, mean reward: -0.306 [-100.000,  7.703], mean action: 1.807 [0.000, 3.000],  loss: 15.386657, mse: 4365.533184, mean_q: 53.118411, mean_eps: 0.424561
  96080/150000: episode: 824, duration: 0.917s, episode steps: 129, steps per second: 141, episode reward: -44.557, mean reward: -0.345 [-100.000, 16.778], mean action: 1.543 [0.000, 3.000],  loss: 13.818231, mse: 4396.874828, mean_q: 55.061765, mean_eps: 0.423910
  96440/150000: episode: 825, duration: 2.725s, episode steps: 360, steps per second: 132, episode reward: -183.782, mean reward: -0.511 [-100.000,  5.975], mean action: 1.733 [0.000, 3.000],  loss: 13.582006, mse: 4407.699481, mean_q: 54.810081, mean_eps: 0.422443
  97396/150000: episode: 826, duration: 7.935s, episode steps: 956, steps per second: 120, episode reward: -205.425, mean reward: -0.215 [-100.000, 23.259], mean action: 1.614 [0.000, 3.000],  loss: 13.672823, mse: 4400.801006, mean_q: 54.608556, mean_eps: 0.418495
  97507/150000: episode: 827, duration: 0.965s, episode steps: 111, steps per second: 115, episode reward: -10.817, mean reward: -0.097 [-100.000, 12.965], mean action: 1.586 [0.000, 3.000],  loss: 11.909598, mse: 4340.903905, mean_q: 54.408876, mean_eps: 0.415294
  97633/150000: episode: 828, duration: 1.085s, episode steps: 126, steps per second: 116, episode reward: -69.425, mean reward: -0.551 [-100.000,  6.496], mean action: 1.484 [0.000, 3.000],  loss: 13.484912, mse: 4422.183332, mean_q: 55.060422, mean_eps: 0.414583
  98633/150000: episode: 829, duration: 8.952s, episode steps: 1000, steps per second: 112, episode reward: 32.641, mean reward:  0.033 [-24.366, 27.303], mean action: 1.469 [0.000, 3.000],  loss: 14.498618, mse: 4436.241088, mean_q: 54.797198, mean_eps: 0.411205
  98949/150000: episode: 830, duration: 3.006s, episode steps: 316, steps per second: 105, episode reward: -214.555, mean reward: -0.679 [-100.000, 14.077], mean action: 1.655 [0.000, 3.000],  loss: 12.700770, mse: 4394.769905, mean_q: 54.528444, mean_eps: 0.407257
  99033/150000: episode: 831, duration: 0.615s, episode steps:  84, steps per second: 137, episode reward: -36.376, mean reward: -0.433 [-100.000,  8.132], mean action: 1.893 [0.000, 3.000],  loss: 14.117725, mse: 4411.765366, mean_q: 54.451037, mean_eps: 0.406057
  99200/150000: episode: 832, duration: 1.221s, episode steps: 167, steps per second: 137, episode reward: -58.256, mean reward: -0.349 [-100.000, 11.052], mean action: 1.695 [0.000, 3.000],  loss: 17.109976, mse: 4576.733951, mean_q: 55.553076, mean_eps: 0.405304
  99280/150000: episode: 833, duration: 0.591s, episode steps:  80, steps per second: 135, episode reward:  5.446, mean reward:  0.068 [-100.000, 12.327], mean action: 1.913 [0.000, 3.000],  loss: 13.259704, mse: 4616.860995, mean_q: 56.487528, mean_eps: 0.404563
 100280/150000: episode: 834, duration: 9.603s, episode steps: 1000, steps per second: 104, episode reward:  0.591, mean reward:  0.001 [-24.250, 23.761], mean action: 1.387 [0.000, 3.000],  loss: 14.325644, mse: 4517.051990, mean_q: 55.756260, mean_eps: 0.401323
 100388/150000: episode: 835, duration: 0.932s, episode steps: 108, steps per second: 116, episode reward: -35.151, mean reward: -0.325 [-100.000, 10.830], mean action: 1.731 [0.000, 3.000],  loss: 15.091494, mse: 4492.460931, mean_q: 54.757403, mean_eps: 0.397999
 100498/150000: episode: 836, duration: 0.808s, episode steps: 110, steps per second: 136, episode reward: -7.789, mean reward: -0.071 [-100.000, 20.515], mean action: 1.691 [0.000, 3.000],  loss: 11.558310, mse: 4404.585420, mean_q: 54.964153, mean_eps: 0.397345
 101498/150000: episode: 837, duration: 9.142s, episode steps: 1000, steps per second: 109, episode reward: -114.230, mean reward: -0.114 [-21.070, 18.225], mean action: 1.649 [0.000, 3.000],  loss: 13.033878, mse: 4443.883890, mean_q: 54.680884, mean_eps: 0.394015
 102465/150000: episode: 838, duration: 8.771s, episode steps: 967, steps per second: 110, episode reward: -339.132, mean reward: -0.351 [-100.000, 22.330], mean action: 1.806 [0.000, 3.000],  loss: 13.941879, mse: 4402.216264, mean_q: 55.040510, mean_eps: 0.388114
 103465/150000: episode: 839, duration: 8.691s, episode steps: 1000, steps per second: 115, episode reward: 35.976, mean reward:  0.036 [-24.433, 23.435], mean action: 1.304 [0.000, 3.000],  loss: 13.537899, mse: 4422.563108, mean_q: 55.412918, mean_eps: 0.382213
 103758/150000: episode: 840, duration: 2.619s, episode steps: 293, steps per second: 112, episode reward: -204.074, mean reward: -0.696 [-100.000, 13.710], mean action: 1.655 [0.000, 3.000],  loss: 13.602416, mse: 4410.713488, mean_q: 55.154698, mean_eps: 0.378334
 103940/150000: episode: 841, duration: 1.621s, episode steps: 182, steps per second: 112, episode reward: -164.444, mean reward: -0.904 [-100.000,  9.995], mean action: 1.637 [0.000, 3.000],  loss: 14.055947, mse: 4426.010045, mean_q: 55.570450, mean_eps: 0.376909
 104940/150000: episode: 842, duration: 10.933s, episode steps: 1000, steps per second:  91, episode reward: 90.118, mean reward:  0.090 [-23.984, 24.844], mean action: 1.170 [0.000, 3.000],  loss: 13.309793, mse: 4362.140914, mean_q: 55.526472, mean_eps: 0.373363
 105347/150000: episode: 843, duration: 3.452s, episode steps: 407, steps per second: 118, episode reward: -154.243, mean reward: -0.379 [-100.000, 12.349], mean action: 1.722 [0.000, 3.000],  loss: 12.863066, mse: 4348.547363, mean_q: 54.932634, mean_eps: 0.369142
 105473/150000: episode: 844, duration: 1.024s, episode steps: 126, steps per second: 123, episode reward:  3.613, mean reward:  0.029 [-100.000, 16.586], mean action: 1.683 [0.000, 3.000],  loss: 11.223791, mse: 4320.846912, mean_q: 54.841367, mean_eps: 0.367543
 105945/150000: episode: 845, duration: 4.527s, episode steps: 472, steps per second: 104, episode reward: -160.267, mean reward: -0.340 [-100.000, 17.766], mean action: 1.655 [0.000, 3.000],  loss: 14.471020, mse: 4364.919480, mean_q: 55.456222, mean_eps: 0.365749
 106945/150000: episode: 846, duration: 8.629s, episode steps: 1000, steps per second: 116, episode reward: 46.085, mean reward:  0.046 [-24.083, 15.612], mean action: 2.127 [0.000, 3.000],  loss: 13.377528, mse: 4261.715385, mean_q: 53.729878, mean_eps: 0.361333
 107081/150000: episode: 847, duration: 1.075s, episode steps: 136, steps per second: 127, episode reward: 24.955, mean reward:  0.183 [-100.000, 18.250], mean action: 1.860 [0.000, 3.000],  loss: 13.660463, mse: 4356.661282, mean_q: 55.664890, mean_eps: 0.357925
 107257/150000: episode: 848, duration: 1.319s, episode steps: 176, steps per second: 133, episode reward: -42.422, mean reward: -0.241 [-100.000, 15.732], mean action: 1.733 [0.000, 3.000],  loss: 13.120765, mse: 4318.290178, mean_q: 54.703581, mean_eps: 0.356989
 108257/150000: episode: 849, duration: 8.896s, episode steps: 1000, steps per second: 112, episode reward: -37.699, mean reward: -0.038 [-22.008, 22.866], mean action: 1.761 [0.000, 3.000],  loss: 14.895821, mse: 4358.210824, mean_q: 55.273158, mean_eps: 0.353461
 108584/150000: episode: 850, duration: 2.488s, episode steps: 327, steps per second: 131, episode reward: -286.675, mean reward: -0.877 [-100.000, 23.937], mean action: 1.670 [0.000, 3.000],  loss: 10.395809, mse: 4373.619533, mean_q: 56.244268, mean_eps: 0.349480
 108842/150000: episode: 851, duration: 2.027s, episode steps: 258, steps per second: 127, episode reward: -260.366, mean reward: -1.009 [-100.000, 13.330], mean action: 1.752 [0.000, 3.000],  loss: 14.726897, mse: 4295.046257, mean_q: 54.654411, mean_eps: 0.347725
 109055/150000: episode: 852, duration: 1.542s, episode steps: 213, steps per second: 138, episode reward: -182.359, mean reward: -0.856 [-100.000,  8.482], mean action: 1.671 [0.000, 3.000],  loss: 10.712279, mse: 4323.601941, mean_q: 54.983546, mean_eps: 0.346312
 110055/150000: episode: 853, duration: 7.725s, episode steps: 1000, steps per second: 129, episode reward: 82.285, mean reward:  0.082 [-23.506, 23.763], mean action: 1.171 [0.000, 3.000],  loss: 13.933316, mse: 4290.309467, mean_q: 54.461822, mean_eps: 0.342673
 110165/150000: episode: 854, duration: 0.787s, episode steps: 110, steps per second: 140, episode reward: -41.031, mean reward: -0.373 [-100.000, 22.818], mean action: 1.873 [0.000, 3.000],  loss: 14.750585, mse: 4355.809195, mean_q: 54.269648, mean_eps: 0.339343
 111165/150000: episode: 855, duration: 7.854s, episode steps: 1000, steps per second: 127, episode reward: 98.370, mean reward:  0.098 [-22.757, 23.425], mean action: 0.798 [0.000, 3.000],  loss: 14.271991, mse: 4266.568897, mean_q: 54.368546, mean_eps: 0.336013
 111997/150000: episode: 856, duration: 6.817s, episode steps: 832, steps per second: 122, episode reward: -222.991, mean reward: -0.268 [-100.000, 23.544], mean action: 1.880 [0.000, 3.000],  loss: 13.060316, mse: 4299.180396, mean_q: 54.312110, mean_eps: 0.330517
 112093/150000: episode: 857, duration: 0.804s, episode steps:  96, steps per second: 119, episode reward: -184.497, mean reward: -1.922 [-100.000, 43.368], mean action: 1.688 [0.000, 3.000],  loss: 9.206324, mse: 4196.966891, mean_q: 52.894812, mean_eps: 0.327733
 113093/150000: episode: 858, duration: 8.377s, episode steps: 1000, steps per second: 119, episode reward: 102.475, mean reward:  0.102 [-21.695, 26.089], mean action: 1.302 [0.000, 3.000],  loss: 14.283619, mse: 4231.455765, mean_q: 53.640576, mean_eps: 0.324445
 113199/150000: episode: 859, duration: 0.799s, episode steps: 106, steps per second: 133, episode reward: -36.513, mean reward: -0.344 [-100.000,  9.115], mean action: 1.613 [0.000, 3.000],  loss: 13.169435, mse: 4242.208019, mean_q: 53.017244, mean_eps: 0.321127
 114199/150000: episode: 860, duration: 8.460s, episode steps: 1000, steps per second: 118, episode reward: 28.451, mean reward:  0.028 [-22.302, 15.892], mean action: 1.293 [0.000, 3.000],  loss: 12.468341, mse: 4241.164796, mean_q: 53.667967, mean_eps: 0.317809
 115199/150000: episode: 861, duration: 8.169s, episode steps: 1000, steps per second: 122, episode reward: 105.004, mean reward:  0.105 [-21.391, 21.414], mean action: 1.490 [0.000, 3.000],  loss: 13.388602, mse: 4173.174559, mean_q: 53.220573, mean_eps: 0.311809
 115443/150000: episode: 862, duration: 1.789s, episode steps: 244, steps per second: 136, episode reward: -188.508, mean reward: -0.773 [-100.000, 13.894], mean action: 1.775 [0.000, 3.000],  loss: 15.126213, mse: 4119.478457, mean_q: 54.329562, mean_eps: 0.308077
 115674/150000: episode: 863, duration: 1.715s, episode steps: 231, steps per second: 135, episode reward: -221.342, mean reward: -0.958 [-100.000, 14.260], mean action: 1.736 [0.000, 3.000],  loss: 13.154423, mse: 4202.029260, mean_q: 54.449020, mean_eps: 0.306652
 116674/150000: episode: 864, duration: 8.103s, episode steps: 1000, steps per second: 123, episode reward: 59.588, mean reward:  0.060 [-23.588, 23.547], mean action: 1.138 [0.000, 3.000],  loss: 14.339113, mse: 4089.935164, mean_q: 53.576535, mean_eps: 0.302959
 117674/150000: episode: 865, duration: 7.866s, episode steps: 1000, steps per second: 127, episode reward: 75.057, mean reward:  0.075 [-21.647, 24.279], mean action: 1.181 [0.000, 3.000],  loss: 14.300864, mse: 4054.569979, mean_q: 53.173909, mean_eps: 0.296959
 118303/150000: episode: 866, duration: 4.831s, episode steps: 629, steps per second: 130, episode reward: 149.527, mean reward:  0.238 [-20.935, 100.000], mean action: 1.517 [0.000, 3.000],  loss: 11.995019, mse: 3941.698124, mean_q: 52.027489, mean_eps: 0.292072
 119303/150000: episode: 867, duration: 7.679s, episode steps: 1000, steps per second: 130, episode reward: 48.066, mean reward:  0.048 [-23.098, 23.773], mean action: 1.067 [0.000, 3.000],  loss: 11.323240, mse: 3938.269101, mean_q: 52.021089, mean_eps: 0.287185
 120303/150000: episode: 868, duration: 8.728s, episode steps: 1000, steps per second: 115, episode reward: 135.678, mean reward:  0.136 [-20.845, 22.014], mean action: 1.176 [0.000, 3.000],  loss: 11.480396, mse: 3878.883638, mean_q: 51.342649, mean_eps: 0.281185
 121303/150000: episode: 869, duration: 9.139s, episode steps: 1000, steps per second: 109, episode reward: 83.374, mean reward:  0.083 [-22.113, 25.325], mean action: 1.224 [0.000, 3.000],  loss: 11.414624, mse: 3890.873478, mean_q: 51.465573, mean_eps: 0.275185
 122303/150000: episode: 870, duration: 8.411s, episode steps: 1000, steps per second: 119, episode reward: 127.073, mean reward:  0.127 [-21.245, 22.890], mean action: 1.144 [0.000, 3.000],  loss: 11.256671, mse: 3790.975837, mean_q: 50.692733, mean_eps: 0.269185
 123057/150000: episode: 871, duration: 6.151s, episode steps: 754, steps per second: 123, episode reward: 179.151, mean reward:  0.238 [-24.016, 100.000], mean action: 1.861 [0.000, 3.000],  loss: 11.271522, mse: 3750.771344, mean_q: 50.087299, mean_eps: 0.263923
 124057/150000: episode: 872, duration: 8.699s, episode steps: 1000, steps per second: 115, episode reward: 121.242, mean reward:  0.121 [-21.018, 22.713], mean action: 1.231 [0.000, 3.000],  loss: 10.568731, mse: 3798.373517, mean_q: 51.045524, mean_eps: 0.258661
 125057/150000: episode: 873, duration: 8.845s, episode steps: 1000, steps per second: 113, episode reward: 88.811, mean reward:  0.089 [-21.594, 22.682], mean action: 1.458 [0.000, 3.000],  loss: 11.319450, mse: 3827.598234, mean_q: 51.669897, mean_eps: 0.252661
 126057/150000: episode: 874, duration: 9.451s, episode steps: 1000, steps per second: 106, episode reward: 105.547, mean reward:  0.106 [-20.903, 24.011], mean action: 0.941 [0.000, 3.000],  loss: 10.191994, mse: 3794.405471, mean_q: 51.846583, mean_eps: 0.246661
 127057/150000: episode: 875, duration: 8.878s, episode steps: 1000, steps per second: 113, episode reward: 111.712, mean reward:  0.112 [-21.286, 23.866], mean action: 1.009 [0.000, 3.000],  loss: 10.034472, mse: 3733.612681, mean_q: 51.980789, mean_eps: 0.240661
 127714/150000: episode: 876, duration: 5.605s, episode steps: 657, steps per second: 117, episode reward: 181.432, mean reward:  0.276 [-23.549, 100.000], mean action: 2.082 [0.000, 3.000],  loss: 9.619176, mse: 3624.641410, mean_q: 50.931557, mean_eps: 0.235690
 128714/150000: episode: 877, duration: 8.585s, episode steps: 1000, steps per second: 116, episode reward: 119.584, mean reward:  0.120 [-22.053, 25.154], mean action: 1.114 [0.000, 3.000],  loss: 10.183651, mse: 3568.330688, mean_q: 50.758152, mean_eps: 0.230719
 129714/150000: episode: 878, duration: 8.820s, episode steps: 1000, steps per second: 113, episode reward: 90.987, mean reward:  0.091 [-22.030, 22.770], mean action: 1.026 [0.000, 3.000],  loss: 9.336151, mse: 3486.783962, mean_q: 50.582286, mean_eps: 0.224719
 130714/150000: episode: 879, duration: 8.672s, episode steps: 1000, steps per second: 115, episode reward: 136.937, mean reward:  0.137 [-20.707, 24.115], mean action: 1.196 [0.000, 3.000],  loss: 9.555124, mse: 3517.094581, mean_q: 50.980990, mean_eps: 0.218719
 131168/150000: episode: 880, duration: 3.942s, episode steps: 454, steps per second: 115, episode reward: 191.185, mean reward:  0.421 [-14.062, 100.000], mean action: 1.489 [0.000, 3.000],  loss: 8.763471, mse: 3471.565439, mean_q: 50.724449, mean_eps: 0.214357
 132068/150000: episode: 881, duration: 8.342s, episode steps: 900, steps per second: 108, episode reward: 176.253, mean reward:  0.196 [-24.221, 100.000], mean action: 2.309 [0.000, 3.000],  loss: 9.701571, mse: 3497.465199, mean_q: 51.609554, mean_eps: 0.210295
 132586/150000: episode: 882, duration: 4.258s, episode steps: 518, steps per second: 122, episode reward: 238.501, mean reward:  0.460 [-17.924, 100.000], mean action: 1.029 [0.000, 3.000],  loss: 8.742329, mse: 3572.227099, mean_q: 52.407937, mean_eps: 0.206041
 133586/150000: episode: 883, duration: 8.295s, episode steps: 1000, steps per second: 121, episode reward: 130.225, mean reward:  0.130 [-21.377, 25.397], mean action: 1.381 [0.000, 3.000],  loss: 8.204682, mse: 3634.338412, mean_q: 53.232555, mean_eps: 0.201487
 134586/150000: episode: 884, duration: 8.985s, episode steps: 1000, steps per second: 111, episode reward: 58.042, mean reward:  0.058 [-20.756, 18.318], mean action: 1.321 [0.000, 3.000],  loss: 7.977002, mse: 3552.047032, mean_q: 52.598574, mean_eps: 0.195487
 135586/150000: episode: 885, duration: 9.383s, episode steps: 1000, steps per second: 107, episode reward: 139.950, mean reward:  0.140 [-22.782, 27.233], mean action: 0.862 [0.000, 3.000],  loss: 7.659395, mse: 3493.738298, mean_q: 52.276302, mean_eps: 0.189487
 136586/150000: episode: 886, duration: 8.769s, episode steps: 1000, steps per second: 114, episode reward: 142.573, mean reward:  0.143 [-20.574, 22.620], mean action: 1.131 [0.000, 3.000],  loss: 7.838403, mse: 3444.977639, mean_q: 52.466103, mean_eps: 0.183487
 137467/150000: episode: 887, duration: 7.888s, episode steps: 881, steps per second: 112, episode reward: 182.846, mean reward:  0.208 [-26.076, 100.000], mean action: 2.387 [0.000, 3.000],  loss: 6.094258, mse: 3482.749229, mean_q: 53.210127, mean_eps: 0.177844
 138467/150000: episode: 888, duration: 8.190s, episode steps: 1000, steps per second: 122, episode reward: -106.877, mean reward: -0.107 [-4.989,  5.156], mean action: 1.853 [0.000, 3.000],  loss: 6.616909, mse: 3396.172395, mean_q: 52.917830, mean_eps: 0.172201
 138990/150000: episode: 889, duration: 3.924s, episode steps: 523, steps per second: 133, episode reward: 237.371, mean reward:  0.454 [-17.657, 100.000], mean action: 0.958 [0.000, 3.000],  loss: 6.768205, mse: 3332.335004, mean_q: 52.630688, mean_eps: 0.167632
 139990/150000: episode: 890, duration: 8.388s, episode steps: 1000, steps per second: 119, episode reward: -31.887, mean reward: -0.032 [-4.606,  5.542], mean action: 1.826 [0.000, 3.000],  loss: 7.140372, mse: 3386.531100, mean_q: 53.382908, mean_eps: 0.163063
 140935/150000: episode: 891, duration: 7.510s, episode steps: 945, steps per second: 126, episode reward: 222.693, mean reward:  0.236 [-20.745, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 6.561504, mse: 3277.223541, mean_q: 52.721671, mean_eps: 0.157228
 141935/150000: episode: 892, duration: 7.592s, episode steps: 1000, steps per second: 132, episode reward: -52.054, mean reward: -0.052 [-4.848,  5.594], mean action: 1.842 [0.000, 3.000],  loss: 5.766775, mse: 3267.393924, mean_q: 52.717984, mean_eps: 0.151393
 142935/150000: episode: 893, duration: 7.639s, episode steps: 1000, steps per second: 131, episode reward: 111.586, mean reward:  0.112 [-20.196, 22.203], mean action: 1.850 [0.000, 3.000],  loss: 6.315272, mse: 3205.333820, mean_q: 52.666244, mean_eps: 0.145393
 143217/150000: episode: 894, duration: 2.360s, episode steps: 282, steps per second: 120, episode reward: 249.142, mean reward:  0.883 [-2.933, 100.000], mean action: 1.496 [0.000, 3.000],  loss: 6.419435, mse: 3194.084306, mean_q: 53.111727, mean_eps: 0.141547
 144217/150000: episode: 895, duration: 7.757s, episode steps: 1000, steps per second: 129, episode reward: -36.169, mean reward: -0.036 [-4.454,  5.863], mean action: 1.823 [0.000, 3.000],  loss: 5.404055, mse: 3172.978054, mean_q: 52.987313, mean_eps: 0.137701
 144535/150000: episode: 896, duration: 2.355s, episode steps: 318, steps per second: 135, episode reward: 216.470, mean reward:  0.681 [-10.566, 100.000], mean action: 1.318 [0.000, 3.000],  loss: 5.069797, mse: 3095.143202, mean_q: 52.839604, mean_eps: 0.133747
 145535/150000: episode: 897, duration: 8.878s, episode steps: 1000, steps per second: 113, episode reward: -36.394, mean reward: -0.036 [-4.625,  4.929], mean action: 1.742 [0.000, 3.000],  loss: 5.536101, mse: 3131.577644, mean_q: 53.117518, mean_eps: 0.129793
 146167/150000: episode: 898, duration: 4.927s, episode steps: 632, steps per second: 128, episode reward: 239.936, mean reward:  0.380 [-19.054, 100.000], mean action: 1.505 [0.000, 3.000],  loss: 6.094380, mse: 3086.161532, mean_q: 53.147477, mean_eps: 0.124897
 146970/150000: episode: 899, duration: 6.136s, episode steps: 803, steps per second: 131, episode reward: 270.477, mean reward:  0.337 [-19.641, 100.000], mean action: 0.710 [0.000, 3.000],  loss: 4.835525, mse: 3058.662723, mean_q: 53.026896, mean_eps: 0.120592
 147356/150000: episode: 900, duration: 2.812s, episode steps: 386, steps per second: 137, episode reward: 290.778, mean reward:  0.753 [-19.649, 100.000], mean action: 1.254 [0.000, 3.000],  loss: 4.985131, mse: 3079.395161, mean_q: 53.433979, mean_eps: 0.117025
 147786/150000: episode: 901, duration: 3.368s, episode steps: 430, steps per second: 128, episode reward: 237.142, mean reward:  0.551 [-17.699, 100.000], mean action: 1.258 [0.000, 3.000],  loss: 4.422730, mse: 3057.550814, mean_q: 53.259507, mean_eps: 0.114577
 148219/150000: episode: 902, duration: 3.237s, episode steps: 433, steps per second: 134, episode reward: 228.947, mean reward:  0.529 [-2.888, 100.000], mean action: 1.206 [0.000, 3.000],  loss: 5.267103, mse: 3165.418448, mean_q: 54.238652, mean_eps: 0.111988
 149219/150000: episode: 903, duration: 8.780s, episode steps: 1000, steps per second: 114, episode reward: -34.562, mean reward: -0.035 [-4.479,  5.445], mean action: 1.798 [0.000, 3.000],  loss: 5.134665, mse: 3153.700392, mean_q: 54.381537, mean_eps: 0.107689
done, took 1222.095 seconds
Testing for 5 episodes ...
Episode 1: reward: 259.395, steps: 320
Episode 2: reward: 243.217, steps: 380
Episode 3: reward: 235.885, steps: 373
Episode 4: reward: 257.537, steps: 264
Episode 5: reward: 264.579, steps: 307
Testing for 5 episodes ...
Episode 1: reward: 210.860, steps: 519
Episode 2: reward: -46.084, steps: 1000
Episode 3: reward: 254.116, steps: 307
Episode 4: reward: 300.687, steps: 290
Episode 5: reward: 249.022, steps: 339
Testing for 100 episodes ...
Episode 1: reward: -33.112, steps: 1000
Episode 2: reward: 278.191, steps: 300
Episode 3: reward: 235.324, steps: 275
Episode 4: reward: 294.762, steps: 295
Episode 5: reward: 242.278, steps: 322
Episode 6: reward: 232.649, steps: 296
Episode 7: reward: -28.998, steps: 1000
Episode 8: reward: -18.731, steps: 1000
Episode 9: reward: -40.376, steps: 1000
Episode 10: reward: -68.706, steps: 1000
Episode 11: reward: 226.422, steps: 273
Episode 12: reward: -57.357, steps: 1000
Episode 13: reward: 298.869, steps: 294
Episode 14: reward: 213.180, steps: 298
Episode 15: reward: -35.698, steps: 1000
Episode 16: reward: -52.620, steps: 1000
Episode 17: reward: 185.593, steps: 379
Episode 18: reward: -49.086, steps: 1000
Episode 19: reward: 236.057, steps: 276
Episode 20: reward: 202.694, steps: 860
Episode 21: reward: 242.228, steps: 295
Episode 22: reward: 278.022, steps: 308
Episode 23: reward: -57.059, steps: 1000
Episode 24: reward: 100.105, steps: 1000
Episode 25: reward: 233.637, steps: 303
Episode 26: reward: 274.448, steps: 297
Episode 27: reward: 252.149, steps: 328
Episode 28: reward: 277.145, steps: 308
Episode 29: reward: -23.803, steps: 1000
Episode 30: reward: 102.220, steps: 1000
Episode 31: reward: -54.881, steps: 1000
Episode 32: reward: 229.212, steps: 683
Episode 33: reward: 150.295, steps: 513
Episode 34: reward: 262.626, steps: 248
Episode 35: reward: 297.221, steps: 273
Episode 36: reward: 288.354, steps: 325
Episode 37: reward: 242.969, steps: 329
Episode 38: reward: 263.580, steps: 276
Episode 39: reward: 113.974, steps: 1000
Episode 40: reward: 275.116, steps: 353
Episode 41: reward: -38.534, steps: 1000
Episode 42: reward: 227.718, steps: 386
Episode 43: reward: 202.199, steps: 394
Episode 44: reward: 241.300, steps: 469
Episode 45: reward: 248.749, steps: 312
Episode 46: reward: 244.463, steps: 617
Episode 47: reward: 236.513, steps: 308
Episode 48: reward: 258.203, steps: 265
Episode 49: reward: 242.172, steps: 317
Episode 50: reward: -36.005, steps: 1000
Episode 51: reward: -40.716, steps: 1000
Episode 52: reward: -41.076, steps: 1000
Episode 53: reward: -63.070, steps: 1000
Episode 54: reward: 245.881, steps: 263
Episode 55: reward: -37.012, steps: 1000
Episode 56: reward: 282.653, steps: 287
Episode 57: reward: 204.239, steps: 896
Episode 58: reward: 280.103, steps: 297
Episode 59: reward: 231.895, steps: 372
Episode 60: reward: 219.198, steps: 379
Episode 61: reward: 241.379, steps: 287
Episode 62: reward: 232.427, steps: 496
Episode 63: reward: 244.279, steps: 297
Episode 64: reward: 300.143, steps: 318
Episode 65: reward: 136.837, steps: 481
Episode 66: reward: 152.371, steps: 449
Episode 67: reward: 195.709, steps: 355
Episode 68: reward: 204.545, steps: 914
Episode 69: reward: 242.946, steps: 268
Episode 70: reward: 298.825, steps: 276
Episode 71: reward: 258.868, steps: 306
Episode 72: reward: 203.272, steps: 632
Episode 73: reward: -13.488, steps: 1000
Episode 74: reward: -19.647, steps: 1000
Episode 75: reward: 274.504, steps: 296
Episode 76: reward: 84.149, steps: 984
Episode 77: reward: -43.861, steps: 1000
Episode 78: reward: 290.335, steps: 310
Episode 79: reward: 290.358, steps: 306
Episode 80: reward: -30.148, steps: 1000
Episode 81: reward: 259.430, steps: 315
Episode 82: reward: 253.187, steps: 368
Episode 83: reward: 231.594, steps: 939
Episode 84: reward: 240.787, steps: 331
Episode 85: reward: 211.286, steps: 476
Episode 86: reward: 215.390, steps: 364
Episode 87: reward: 268.904, steps: 275
Episode 88: reward: -54.834, steps: 1000
Episode 89: reward: 260.434, steps: 286
Episode 90: reward: 276.956, steps: 314
Episode 91: reward: 247.036, steps: 308
Episode 92: reward: 197.511, steps: 818
Episode 93: reward: 262.070, steps: 267
Episode 94: reward: 244.835, steps: 323
Episode 95: reward: 264.282, steps: 310
Episode 96: reward: 184.557, steps: 456
Episode 97: reward: 271.288, steps: 317
Episode 98: reward: 254.196, steps: 278
Episode 99: reward: 159.501, steps: 365
Episode 100: reward: 254.666, steps: 296
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_2 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_8 (Dense)              (None, 64)                576
_________________________________________________________________
activation_8 (Activation)    (None, 64)                0
_________________________________________________________________
dense_9 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_9 (Activation)    (None, 64)                0
_________________________________________________________________
dense_10 (Dense)             (None, 32)                2080
_________________________________________________________________
activation_10 (Activation)   (None, 32)                0
_________________________________________________________________
dense_11 (Dense)             (None, 4)                 132
_________________________________________________________________
activation_11 (Activation)   (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None