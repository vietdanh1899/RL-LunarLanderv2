Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 150000 steps ...
     75/150000: episode: 1, duration: 0.120s, episode steps:  75, steps per second: 624, episode reward: -213.798, mean reward: -2.851 [-100.000, 19.935], mean action: 1.480 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
    151/150000: episode: 2, duration: 0.833s, episode steps:  76, steps per second:  91, episode reward: 25.336, mean reward:  0.333 [-100.000, 100.839], mean action: 1.539 [0.000, 3.000],  loss: 50.850475, mse: 25.513201, mean_q: 0.086744, mean_eps: 0.999247
    277/150000: episode: 3, duration: 0.806s, episode steps: 126, steps per second: 156, episode reward: -454.883, mean reward: -3.610 [-100.000,  4.698], mean action: 1.532 [0.000, 3.000],  loss: 73.605155, mse: 37.677010, mean_q: 0.278089, mean_eps: 0.998719
    370/150000: episode: 4, duration: 0.621s, episode steps:  93, steps per second: 150, episode reward: -523.110, mean reward: -5.625 [-100.000, -0.258], mean action: 1.645 [0.000, 3.000],  loss: 69.454823, mse: 41.410243, mean_q: 0.050695, mean_eps: 0.998062
    446/150000: episode: 5, duration: 0.452s, episode steps:  76, steps per second: 168, episode reward: -112.703, mean reward: -1.483 [-100.000, 13.352], mean action: 1.316 [0.000, 3.000],  loss: 62.355968, mse: 43.981033, mean_q: 0.036480, mean_eps: 0.997555
    537/150000: episode: 6, duration: 0.690s, episode steps:  91, steps per second: 132, episode reward: -76.500, mean reward: -0.841 [-100.000, 12.285], mean action: 1.582 [0.000, 3.000],  loss: 42.049915, mse: 31.009491, mean_q: -0.089321, mean_eps: 0.997054
    609/150000: episode: 7, duration: 0.445s, episode steps:  72, steps per second: 162, episode reward: -212.593, mean reward: -2.953 [-100.000, 30.349], mean action: 1.472 [0.000, 3.000],  loss: 47.623251, mse: 36.772323, mean_q: -0.104694, mean_eps: 0.996565
    721/150000: episode: 8, duration: 0.764s, episode steps: 112, steps per second: 147, episode reward: -200.857, mean reward: -1.793 [-100.000, 16.334], mean action: 1.348 [0.000, 3.000],  loss: 39.366619, mse: 34.853266, mean_q: 0.344947, mean_eps: 0.996013
    849/150000: episode: 9, duration: 0.805s, episode steps: 128, steps per second: 159, episode reward: -182.824, mean reward: -1.428 [-100.000,  5.548], mean action: 1.547 [0.000, 3.000],  loss: 39.488656, mse: 35.243744, mean_q: 0.505638, mean_eps: 0.995293
    933/150000: episode: 10, duration: 0.524s, episode steps:  84, steps per second: 160, episode reward: -271.668, mean reward: -3.234 [-100.000, 55.331], mean action: 1.619 [0.000, 3.000],  loss: 30.322553, mse: 35.500608, mean_q: 0.956472, mean_eps: 0.994657
   1003/150000: episode: 11, duration: 0.420s, episode steps:  70, steps per second: 167, episode reward: -123.747, mean reward: -1.768 [-100.000, 59.029], mean action: 1.543 [0.000, 3.000],  loss: 34.095843, mse: 40.784445, mean_q: 0.876846, mean_eps: 0.994195
   1112/150000: episode: 12, duration: 0.734s, episode steps: 109, steps per second: 148, episode reward: -290.792, mean reward: -2.668 [-100.000, 15.138], mean action: 1.606 [0.000, 3.000],  loss: 36.723361, mse: 39.374784, mean_q: 0.820075, mean_eps: 0.993658
   1208/150000: episode: 13, duration: 0.812s, episode steps:  96, steps per second: 118, episode reward: -240.569, mean reward: -2.506 [-100.000, 53.954], mean action: 1.615 [0.000, 3.000],  loss: 31.201643, mse: 39.356826, mean_q: 1.068996, mean_eps: 0.993043
   1313/150000: episode: 14, duration: 0.965s, episode steps: 105, steps per second: 109, episode reward: -322.892, mean reward: -3.075 [-100.000, 53.574], mean action: 1.457 [0.000, 3.000],  loss: 32.361925, mse: 44.789591, mean_q: 0.973014, mean_eps: 0.992440
   1420/150000: episode: 15, duration: 0.804s, episode steps: 107, steps per second: 133, episode reward: -301.213, mean reward: -2.815 [-100.000, 107.351], mean action: 1.486 [0.000, 3.000],  loss: 30.721253, mse: 47.111726, mean_q: 0.924985, mean_eps: 0.991804
   1542/150000: episode: 16, duration: 1.075s, episode steps: 122, steps per second: 113, episode reward: -297.935, mean reward: -2.442 [-100.000,  4.090], mean action: 1.410 [0.000, 3.000],  loss: 40.585936, mse: 55.491238, mean_q: 0.730643, mean_eps: 0.991117
   1617/150000: episode: 17, duration: 0.739s, episode steps:  75, steps per second: 102, episode reward: -85.019, mean reward: -1.134 [-100.000, 39.681], mean action: 1.333 [0.000, 3.000],  loss: 47.310143, mse: 59.632585, mean_q: 0.973976, mean_eps: 0.990526
   1697/150000: episode: 18, duration: 0.654s, episode steps:  80, steps per second: 122, episode reward: -117.595, mean reward: -1.470 [-100.000, 16.919], mean action: 1.538 [0.000, 3.000],  loss: 44.674796, mse: 61.133071, mean_q: 1.058749, mean_eps: 0.990061
   1790/150000: episode: 19, duration: 0.752s, episode steps:  93, steps per second: 124, episode reward: -269.887, mean reward: -2.902 [-100.000, 101.269], mean action: 1.548 [0.000, 3.000],  loss: 32.898478, mse: 55.029667, mean_q: 0.879076, mean_eps: 0.989542
   1925/150000: episode: 20, duration: 0.931s, episode steps: 135, steps per second: 145, episode reward: 63.167, mean reward:  0.468 [-100.000, 113.023], mean action: 1.541 [0.000, 3.000],  loss: 42.246602, mse: 57.201727, mean_q: 1.095417, mean_eps: 0.988858
   2027/150000: episode: 21, duration: 0.652s, episode steps: 102, steps per second: 156, episode reward: -251.837, mean reward: -2.469 [-100.000,  6.824], mean action: 1.363 [0.000, 3.000],  loss: 40.352420, mse: 57.819933, mean_q: 1.291791, mean_eps: 0.988147
   2123/150000: episode: 22, duration: 0.585s, episode steps:  96, steps per second: 164, episode reward: -125.299, mean reward: -1.305 [-100.000,  8.271], mean action: 1.552 [0.000, 3.000],  loss: 50.486833, mse: 67.807295, mean_q: 2.156479, mean_eps: 0.987553
   2209/150000: episode: 23, duration: 0.624s, episode steps:  86, steps per second: 138, episode reward: -80.946, mean reward: -0.941 [-100.000,  5.198], mean action: 1.314 [0.000, 3.000],  loss: 46.289525, mse: 64.330723, mean_q: 2.309125, mean_eps: 0.987007
   2331/150000: episode: 24, duration: 0.956s, episode steps: 122, steps per second: 128, episode reward: -300.595, mean reward: -2.464 [-100.000, 87.821], mean action: 1.393 [0.000, 3.000],  loss: 41.895859, mse: 71.954064, mean_q: 2.396796, mean_eps: 0.986383
   2417/150000: episode: 25, duration: 0.562s, episode steps:  86, steps per second: 153, episode reward: -105.608, mean reward: -1.228 [-100.000, 25.334], mean action: 1.407 [0.000, 3.000],  loss: 49.730333, mse: 79.194653, mean_q: 2.195512, mean_eps: 0.985759
   2518/150000: episode: 26, duration: 0.694s, episode steps: 101, steps per second: 146, episode reward: -330.363, mean reward: -3.271 [-100.000,  0.846], mean action: 1.485 [0.000, 3.000],  loss: 47.289139, mse: 79.474429, mean_q: 1.980183, mean_eps: 0.985198
   2599/150000: episode: 27, duration: 0.527s, episode steps:  81, steps per second: 154, episode reward: -347.165, mean reward: -4.286 [-100.000, 16.225], mean action: 1.531 [0.000, 3.000],  loss: 40.501441, mse: 69.336369, mean_q: 2.180699, mean_eps: 0.984652
   2681/150000: episode: 28, duration: 0.519s, episode steps:  82, steps per second: 158, episode reward: -152.913, mean reward: -1.865 [-100.000, 12.531], mean action: 1.305 [0.000, 3.000],  loss: 51.570042, mse: 82.091914, mean_q: 1.913270, mean_eps: 0.984163
   2765/150000: episode: 29, duration: 0.533s, episode steps:  84, steps per second: 157, episode reward: -93.315, mean reward: -1.111 [-100.000, 12.339], mean action: 1.452 [0.000, 3.000],  loss: 46.510959, mse: 80.187887, mean_q: 2.019385, mean_eps: 0.983665
   2870/150000: episode: 30, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward: -99.842, mean reward: -0.951 [-100.000, 22.977], mean action: 1.362 [0.000, 3.000],  loss: 48.937813, mse: 77.880044, mean_q: 2.229196, mean_eps: 0.983098
   2977/150000: episode: 31, duration: 0.660s, episode steps: 107, steps per second: 162, episode reward: -118.315, mean reward: -1.106 [-100.000,  7.505], mean action: 1.421 [0.000, 3.000],  loss: 45.222869, mse: 83.712237, mean_q: 2.141498, mean_eps: 0.982462
   3079/150000: episode: 32, duration: 0.637s, episode steps: 102, steps per second: 160, episode reward: -291.224, mean reward: -2.855 [-100.000,  7.171], mean action: 1.706 [0.000, 3.000],  loss: 41.155012, mse: 79.981142, mean_q: 2.707174, mean_eps: 0.981835
   3164/150000: episode: 33, duration: 0.558s, episode steps:  85, steps per second: 152, episode reward: -195.468, mean reward: -2.300 [-100.000,  8.522], mean action: 1.329 [0.000, 3.000],  loss: 33.243976, mse: 93.516708, mean_q: 2.972838, mean_eps: 0.981274
   3246/150000: episode: 34, duration: 0.510s, episode steps:  82, steps per second: 161, episode reward: -103.971, mean reward: -1.268 [-100.000,  9.937], mean action: 1.671 [0.000, 3.000],  loss: 39.489628, mse: 95.653430, mean_q: 2.983383, mean_eps: 0.980773
   3324/150000: episode: 35, duration: 0.482s, episode steps:  78, steps per second: 162, episode reward: -105.891, mean reward: -1.358 [-100.000,  6.849], mean action: 1.487 [0.000, 3.000],  loss: 50.024153, mse: 105.935287, mean_q: 3.288392, mean_eps: 0.980293
   3409/150000: episode: 36, duration: 0.607s, episode steps:  85, steps per second: 140, episode reward: -98.620, mean reward: -1.160 [-100.000,  8.816], mean action: 1.412 [0.000, 3.000],  loss: 35.003107, mse: 99.417819, mean_q: 3.251648, mean_eps: 0.979804
   3483/150000: episode: 37, duration: 0.521s, episode steps:  74, steps per second: 142, episode reward: -91.477, mean reward: -1.236 [-100.000,  7.052], mean action: 1.514 [0.000, 3.000],  loss: 34.262059, mse: 100.818484, mean_q: 3.386803, mean_eps: 0.979327
   3565/150000: episode: 38, duration: 0.611s, episode steps:  82, steps per second: 134, episode reward: -98.927, mean reward: -1.206 [-100.000, 11.059], mean action: 1.585 [0.000, 3.000],  loss: 48.234222, mse: 102.768613, mean_q: 3.734863, mean_eps: 0.978859
   3668/150000: episode: 39, duration: 0.832s, episode steps: 103, steps per second: 124, episode reward: -284.674, mean reward: -2.764 [-100.000,  6.835], mean action: 1.301 [0.000, 3.000],  loss: 44.401887, mse: 105.049554, mean_q: 3.859031, mean_eps: 0.978304
   3736/150000: episode: 40, duration: 0.670s, episode steps:  68, steps per second: 101, episode reward: -94.342, mean reward: -1.387 [-100.000,  6.650], mean action: 1.221 [0.000, 3.000],  loss: 43.989434, mse: 97.570758, mean_q: 3.711823, mean_eps: 0.977791
   3842/150000: episode: 41, duration: 0.813s, episode steps: 106, steps per second: 130, episode reward: -376.737, mean reward: -3.554 [-100.000,  0.682], mean action: 1.528 [0.000, 3.000],  loss: 39.547158, mse: 95.778494, mean_q: 3.547546, mean_eps: 0.977269
   3924/150000: episode: 42, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -86.007, mean reward: -1.049 [-100.000, 12.691], mean action: 1.524 [0.000, 3.000],  loss: 43.088962, mse: 100.231670, mean_q: 3.475547, mean_eps: 0.976705
   4003/150000: episode: 43, duration: 0.678s, episode steps:  79, steps per second: 116, episode reward: -42.304, mean reward: -0.535 [-100.000, 16.516], mean action: 1.481 [0.000, 3.000],  loss: 44.289160, mse: 99.573718, mean_q: 3.725340, mean_eps: 0.976222
   4128/150000: episode: 44, duration: 1.045s, episode steps: 125, steps per second: 120, episode reward: -247.012, mean reward: -1.976 [-100.000, 23.826], mean action: 1.512 [0.000, 3.000],  loss: 42.472871, mse: 113.319606, mean_q: 4.147036, mean_eps: 0.975610
   4212/150000: episode: 45, duration: 0.527s, episode steps:  84, steps per second: 159, episode reward: -165.467, mean reward: -1.970 [-100.000, 27.918], mean action: 1.536 [0.000, 3.000],  loss: 43.713119, mse: 131.774496, mean_q: 4.450382, mean_eps: 0.974983
   4287/150000: episode: 46, duration: 0.487s, episode steps:  75, steps per second: 154, episode reward: -92.416, mean reward: -1.232 [-100.000, 11.336], mean action: 1.400 [0.000, 3.000],  loss: 42.236765, mse: 138.484614, mean_q: 4.077065, mean_eps: 0.974506
   4353/150000: episode: 47, duration: 0.466s, episode steps:  66, steps per second: 142, episode reward: -122.645, mean reward: -1.858 [-100.000,  5.910], mean action: 1.394 [0.000, 3.000],  loss: 34.730284, mse: 136.863075, mean_q: 4.238760, mean_eps: 0.974083
   4437/150000: episode: 48, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -343.912, mean reward: -4.094 [-100.000,  0.595], mean action: 1.452 [0.000, 3.000],  loss: 30.704911, mse: 129.062023, mean_q: 4.287116, mean_eps: 0.973633
   4505/150000: episode: 49, duration: 0.428s, episode steps:  68, steps per second: 159, episode reward: -94.072, mean reward: -1.383 [-100.000, 19.885], mean action: 1.426 [0.000, 3.000],  loss: 27.570544, mse: 134.180711, mean_q: 4.246728, mean_eps: 0.973177
   4629/150000: episode: 50, duration: 0.778s, episode steps: 124, steps per second: 159, episode reward: -134.314, mean reward: -1.083 [-100.000,  7.354], mean action: 1.573 [0.000, 3.000],  loss: 32.641999, mse: 133.924950, mean_q: 4.104748, mean_eps: 0.972601
   4704/150000: episode: 51, duration: 0.503s, episode steps:  75, steps per second: 149, episode reward: -120.281, mean reward: -1.604 [-100.000, 21.050], mean action: 1.760 [0.000, 3.000],  loss: 37.870853, mse: 141.012237, mean_q: 4.513847, mean_eps: 0.972004
   4808/150000: episode: 52, duration: 0.675s, episode steps: 104, steps per second: 154, episode reward: -57.363, mean reward: -0.552 [-100.000, 12.587], mean action: 1.721 [0.000, 3.000],  loss: 33.822395, mse: 133.886906, mean_q: 4.223616, mean_eps: 0.971467
   4893/150000: episode: 53, duration: 0.537s, episode steps:  85, steps per second: 158, episode reward: -103.937, mean reward: -1.223 [-100.000, 13.614], mean action: 1.365 [0.000, 3.000],  loss: 32.467316, mse: 134.983928, mean_q: 4.383223, mean_eps: 0.970900
   5018/150000: episode: 54, duration: 0.840s, episode steps: 125, steps per second: 149, episode reward: -110.222, mean reward: -0.882 [-100.000, 13.129], mean action: 1.576 [0.000, 3.000],  loss: 36.325534, mse: 134.542934, mean_q: 4.527008, mean_eps: 0.970270
   5084/150000: episode: 55, duration: 0.457s, episode steps:  66, steps per second: 144, episode reward: -65.151, mean reward: -0.987 [-100.000,  7.879], mean action: 1.212 [0.000, 3.000],  loss: 35.182521, mse: 211.199513, mean_q: 4.081389, mean_eps: 0.969697
   5160/150000: episode: 56, duration: 0.485s, episode steps:  76, steps per second: 157, episode reward: -72.294, mean reward: -0.951 [-100.000, 34.371], mean action: 1.632 [0.000, 3.000],  loss: 30.020296, mse: 194.671473, mean_q: 4.560214, mean_eps: 0.969271
   5268/150000: episode: 57, duration: 0.670s, episode steps: 108, steps per second: 161, episode reward: -200.011, mean reward: -1.852 [-100.000, 72.879], mean action: 1.491 [0.000, 3.000],  loss: 29.662616, mse: 184.115689, mean_q: 4.992087, mean_eps: 0.968719
   5347/150000: episode: 58, duration: 0.525s, episode steps:  79, steps per second: 150, episode reward: -92.192, mean reward: -1.167 [-100.000,  6.921], mean action: 1.418 [0.000, 3.000],  loss: 34.711566, mse: 202.911972, mean_q: 4.839008, mean_eps: 0.968158
   5515/150000: episode: 59, duration: 1.069s, episode steps: 168, steps per second: 157, episode reward: -36.480, mean reward: -0.217 [-100.000, 99.598], mean action: 1.589 [0.000, 3.000],  loss: 26.652231, mse: 191.932955, mean_q: 4.526187, mean_eps: 0.967417
   5624/150000: episode: 60, duration: 0.668s, episode steps: 109, steps per second: 163, episode reward: -232.572, mean reward: -2.134 [-100.000,  1.301], mean action: 1.450 [0.000, 3.000],  loss: 28.747319, mse: 178.823977, mean_q: 4.949005, mean_eps: 0.966586
   5709/150000: episode: 61, duration: 0.660s, episode steps:  85, steps per second: 129, episode reward: -131.111, mean reward: -1.542 [-100.000, 15.753], mean action: 1.412 [0.000, 3.000],  loss: 22.986171, mse: 172.624573, mean_q: 4.912437, mean_eps: 0.966004
   5834/150000: episode: 62, duration: 0.945s, episode steps: 125, steps per second: 132, episode reward: -559.744, mean reward: -4.478 [-100.000, 18.429], mean action: 1.440 [0.000, 3.000],  loss: 30.367515, mse: 167.302386, mean_q: 4.879702, mean_eps: 0.965374
   5951/150000: episode: 63, duration: 0.866s, episode steps: 117, steps per second: 135, episode reward: -393.000, mean reward: -3.359 [-100.000,  1.493], mean action: 1.342 [0.000, 3.000],  loss: 25.959762, mse: 174.869283, mean_q: 4.347411, mean_eps: 0.964648
   6026/150000: episode: 64, duration: 0.539s, episode steps:  75, steps per second: 139, episode reward: -208.186, mean reward: -2.776 [-100.000,  6.536], mean action: 1.560 [0.000, 3.000],  loss: 22.155346, mse: 168.554044, mean_q: 4.087159, mean_eps: 0.964072
   6106/150000: episode: 65, duration: 0.528s, episode steps:  80, steps per second: 151, episode reward: -176.192, mean reward: -2.202 [-100.000,  6.179], mean action: 1.562 [0.000, 3.000],  loss: 24.278576, mse: 197.335938, mean_q: 4.107464, mean_eps: 0.963607
   6231/150000: episode: 66, duration: 0.904s, episode steps: 125, steps per second: 138, episode reward: -237.034, mean reward: -1.896 [-100.000, 27.478], mean action: 1.696 [0.000, 3.000],  loss: 23.113362, mse: 197.949738, mean_q: 4.075540, mean_eps: 0.962992
   6342/150000: episode: 67, duration: 0.867s, episode steps: 111, steps per second: 128, episode reward: -21.932, mean reward: -0.198 [-100.000, 65.047], mean action: 1.604 [0.000, 3.000],  loss: 22.839464, mse: 191.447582, mean_q: 3.976506, mean_eps: 0.962284
   6419/150000: episode: 68, duration: 0.501s, episode steps:  77, steps per second: 154, episode reward: -123.540, mean reward: -1.604 [-100.000, 23.250], mean action: 1.494 [0.000, 3.000],  loss: 32.004527, mse: 222.160970, mean_q: 3.467726, mean_eps: 0.961720
   6541/150000: episode: 69, duration: 0.801s, episode steps: 122, steps per second: 152, episode reward: -85.836, mean reward: -0.704 [-100.000, 10.952], mean action: 1.500 [0.000, 3.000],  loss: 29.215719, mse: 196.290249, mean_q: 3.828201, mean_eps: 0.961123
   6661/150000: episode: 70, duration: 0.767s, episode steps: 120, steps per second: 156, episode reward: -184.024, mean reward: -1.534 [-100.000,  1.656], mean action: 1.325 [0.000, 3.000],  loss: 21.991667, mse: 210.747952, mean_q: 4.122258, mean_eps: 0.960397
   6769/150000: episode: 71, duration: 0.665s, episode steps: 108, steps per second: 162, episode reward: -131.397, mean reward: -1.217 [-100.000, 59.015], mean action: 1.611 [0.000, 3.000],  loss: 24.311764, mse: 188.732340, mean_q: 4.097062, mean_eps: 0.959713
   6835/150000: episode: 72, duration: 0.409s, episode steps:  66, steps per second: 162, episode reward: -89.419, mean reward: -1.355 [-100.000, 17.281], mean action: 1.409 [0.000, 3.000],  loss: 21.516578, mse: 187.217356, mean_q: 4.427432, mean_eps: 0.959191
   6920/150000: episode: 73, duration: 0.579s, episode steps:  85, steps per second: 147, episode reward: -183.753, mean reward: -2.162 [-100.000,  6.394], mean action: 1.471 [0.000, 3.000],  loss: 21.972533, mse: 211.437475, mean_q: 4.175967, mean_eps: 0.958738
   7024/150000: episode: 74, duration: 0.648s, episode steps: 104, steps per second: 160, episode reward: -122.011, mean reward: -1.173 [-100.000,  8.324], mean action: 1.500 [0.000, 3.000],  loss: 27.352446, mse: 214.073026, mean_q: 4.031504, mean_eps: 0.958171
   7086/150000: episode: 75, duration: 0.389s, episode steps:  62, steps per second: 159, episode reward: -102.423, mean reward: -1.652 [-100.000, 58.383], mean action: 1.161 [0.000, 3.000],  loss: 16.785805, mse: 258.062086, mean_q: 4.323908, mean_eps: 0.957673
   7151/150000: episode: 76, duration: 0.401s, episode steps:  65, steps per second: 162, episode reward: -177.604, mean reward: -2.732 [-100.000,  5.999], mean action: 1.600 [0.000, 3.000],  loss: 28.873096, mse: 253.557929, mean_q: 4.665635, mean_eps: 0.957292
   7246/150000: episode: 77, duration: 0.626s, episode steps:  95, steps per second: 152, episode reward: -103.226, mean reward: -1.087 [-100.000,  7.180], mean action: 1.526 [0.000, 3.000],  loss: 31.311452, mse: 232.338281, mean_q: 5.155440, mean_eps: 0.956812
   7345/150000: episode: 78, duration: 0.613s, episode steps:  99, steps per second: 162, episode reward: -104.042, mean reward: -1.051 [-100.000, 23.264], mean action: 1.515 [0.000, 3.000],  loss: 25.836541, mse: 227.704551, mean_q: 5.128582, mean_eps: 0.956230
   7408/150000: episode: 79, duration: 0.382s, episode steps:  63, steps per second: 165, episode reward: -77.531, mean reward: -1.231 [-100.000, 10.704], mean action: 1.429 [0.000, 3.000],  loss: 24.699812, mse: 242.926088, mean_q: 5.381965, mean_eps: 0.955744
   7522/150000: episode: 80, duration: 0.709s, episode steps: 114, steps per second: 161, episode reward: -127.636, mean reward: -1.120 [-100.000, 18.442], mean action: 1.509 [0.000, 3.000],  loss: 31.575196, mse: 255.592446, mean_q: 4.961282, mean_eps: 0.955213
   7626/150000: episode: 81, duration: 0.679s, episode steps: 104, steps per second: 153, episode reward: -231.617, mean reward: -2.227 [-100.000, 16.992], mean action: 1.702 [0.000, 3.000],  loss: 29.075201, mse: 232.315600, mean_q: 5.429826, mean_eps: 0.954559
   7722/150000: episode: 82, duration: 0.596s, episode steps:  96, steps per second: 161, episode reward: -109.058, mean reward: -1.136 [-100.000,  9.476], mean action: 1.438 [0.000, 3.000],  loss: 26.630920, mse: 251.700612, mean_q: 5.111774, mean_eps: 0.953959
   7825/150000: episode: 83, duration: 0.643s, episode steps: 103, steps per second: 160, episode reward: -116.912, mean reward: -1.135 [-100.000,  5.873], mean action: 1.417 [0.000, 3.000],  loss: 29.898849, mse: 237.068295, mean_q: 5.408794, mean_eps: 0.953362
   7907/150000: episode: 84, duration: 0.564s, episode steps:  82, steps per second: 145, episode reward: -112.360, mean reward: -1.370 [-100.000,  5.877], mean action: 1.451 [0.000, 3.000],  loss: 21.970869, mse: 243.872920, mean_q: 5.447512, mean_eps: 0.952807
   7996/150000: episode: 85, duration: 0.568s, episode steps:  89, steps per second: 157, episode reward: -152.057, mean reward: -1.709 [-100.000, 11.686], mean action: 1.506 [0.000, 3.000],  loss: 19.350635, mse: 250.658515, mean_q: 5.279727, mean_eps: 0.952294
   8064/150000: episode: 86, duration: 0.416s, episode steps:  68, steps per second: 163, episode reward: -53.260, mean reward: -0.783 [-100.000, 17.645], mean action: 1.500 [0.000, 3.000],  loss: 28.885658, mse: 279.300833, mean_q: 6.323001, mean_eps: 0.951823
   8151/150000: episode: 87, duration: 0.529s, episode steps:  87, steps per second: 165, episode reward: -375.775, mean reward: -4.319 [-100.000,  0.410], mean action: 1.506 [0.000, 3.000],  loss: 21.844104, mse: 247.545203, mean_q: 6.176032, mean_eps: 0.951358
   8224/150000: episode: 88, duration: 0.464s, episode steps:  73, steps per second: 157, episode reward: -201.849, mean reward: -2.765 [-100.000, 34.531], mean action: 1.466 [0.000, 3.000],  loss: 28.121786, mse: 270.715940, mean_q: 5.908286, mean_eps: 0.950878
   8320/150000: episode: 89, duration: 0.627s, episode steps:  96, steps per second: 153, episode reward: -42.565, mean reward: -0.443 [-100.000, 68.278], mean action: 1.479 [0.000, 3.000],  loss: 25.710853, mse: 306.940102, mean_q: 5.753622, mean_eps: 0.950371
   8451/150000: episode: 90, duration: 0.812s, episode steps: 131, steps per second: 161, episode reward: -95.119, mean reward: -0.726 [-100.000,  9.281], mean action: 1.588 [0.000, 3.000],  loss: 27.336149, mse: 275.902537, mean_q: 5.949609, mean_eps: 0.949690
   8520/150000: episode: 91, duration: 0.442s, episode steps:  69, steps per second: 156, episode reward: -154.296, mean reward: -2.236 [-100.000, 15.790], mean action: 1.420 [0.000, 3.000],  loss: 36.222416, mse: 304.698211, mean_q: 5.397730, mean_eps: 0.949090
   8584/150000: episode: 92, duration: 0.426s, episode steps:  64, steps per second: 150, episode reward: -123.993, mean reward: -1.937 [-100.000,  6.236], mean action: 1.641 [0.000, 3.000],  loss: 27.675986, mse: 298.724192, mean_q: 5.705924, mean_eps: 0.948691
   9584/150000: episode: 93, duration: 7.324s, episode steps: 1000, steps per second: 137, episode reward: 95.941, mean reward:  0.096 [-24.511, 119.757], mean action: 1.488 [0.000, 3.000],  loss: 28.003019, mse: 310.353860, mean_q: 5.039701, mean_eps: 0.945499
   9705/150000: episode: 94, duration: 0.740s, episode steps: 121, steps per second: 164, episode reward: -72.546, mean reward: -0.600 [-100.000, 15.370], mean action: 1.686 [0.000, 3.000],  loss: 27.385222, mse: 370.847878, mean_q: 3.432612, mean_eps: 0.942136
   9835/150000: episode: 95, duration: 0.825s, episode steps: 130, steps per second: 158, episode reward: -90.873, mean reward: -0.699 [-100.000,  9.631], mean action: 1.515 [0.000, 3.000],  loss: 26.472410, mse: 347.827226, mean_q: 3.617825, mean_eps: 0.941383
   9915/150000: episode: 96, duration: 0.517s, episode steps:  80, steps per second: 155, episode reward: -234.591, mean reward: -2.932 [-100.000,  6.747], mean action: 1.625 [0.000, 3.000],  loss: 32.949692, mse: 343.994117, mean_q: 4.022660, mean_eps: 0.940753
  10040/150000: episode: 97, duration: 0.811s, episode steps: 125, steps per second: 154, episode reward: -209.645, mean reward: -1.677 [-100.000, 47.410], mean action: 1.584 [0.000, 3.000],  loss: 24.873083, mse: 339.238689, mean_q: 3.679474, mean_eps: 0.940138
  10121/150000: episode: 98, duration: 0.509s, episode steps:  81, steps per second: 159, episode reward: -142.132, mean reward: -1.755 [-100.000,  5.449], mean action: 1.321 [0.000, 3.000],  loss: 24.396047, mse: 387.038011, mean_q: 3.617997, mean_eps: 0.939520
  10232/150000: episode: 99, duration: 0.721s, episode steps: 111, steps per second: 154, episode reward: -186.771, mean reward: -1.683 [-100.000,  6.550], mean action: 1.595 [0.000, 3.000],  loss: 20.163242, mse: 356.606885, mean_q: 4.608267, mean_eps: 0.938944
  10356/150000: episode: 100, duration: 0.764s, episode steps: 124, steps per second: 162, episode reward: -38.199, mean reward: -0.308 [-100.000, 69.342], mean action: 1.516 [0.000, 3.000],  loss: 27.962893, mse: 372.951328, mean_q: 4.715118, mean_eps: 0.938239
  10428/150000: episode: 101, duration: 0.432s, episode steps:  72, steps per second: 167, episode reward: -73.435, mean reward: -1.020 [-100.000,  6.996], mean action: 1.542 [0.000, 3.000],  loss: 16.340753, mse: 385.528481, mean_q: 4.458693, mean_eps: 0.937651
  10507/150000: episode: 102, duration: 0.519s, episode steps:  79, steps per second: 152, episode reward: -104.310, mean reward: -1.320 [-100.000, 21.732], mean action: 1.544 [0.000, 3.000],  loss: 20.574939, mse: 367.067028, mean_q: 4.636999, mean_eps: 0.937198
  10627/150000: episode: 103, duration: 0.759s, episode steps: 120, steps per second: 158, episode reward: -57.692, mean reward: -0.481 [-100.000, 18.093], mean action: 1.458 [0.000, 3.000],  loss: 22.835532, mse: 372.410870, mean_q: 5.040048, mean_eps: 0.936601
  10729/150000: episode: 104, duration: 0.633s, episode steps: 102, steps per second: 161, episode reward: -120.818, mean reward: -1.184 [-100.000,  6.627], mean action: 1.363 [0.000, 3.000],  loss: 23.571688, mse: 377.621890, mean_q: 5.066565, mean_eps: 0.935935
  10847/150000: episode: 105, duration: 0.756s, episode steps: 118, steps per second: 156, episode reward: -113.141, mean reward: -0.959 [-100.000, 11.217], mean action: 1.653 [0.000, 3.000],  loss: 16.708298, mse: 337.379509, mean_q: 5.487970, mean_eps: 0.935275
  10957/150000: episode: 106, duration: 0.716s, episode steps: 110, steps per second: 154, episode reward: -90.940, mean reward: -0.827 [-100.000, 10.014], mean action: 1.645 [0.000, 3.000],  loss: 23.139172, mse: 369.226212, mean_q: 4.768060, mean_eps: 0.934591
  11036/150000: episode: 107, duration: 0.522s, episode steps:  79, steps per second: 151, episode reward: -174.551, mean reward: -2.210 [-100.000, 41.579], mean action: 1.570 [0.000, 3.000],  loss: 19.697134, mse: 381.802017, mean_q: 5.285875, mean_eps: 0.934024
  11154/150000: episode: 108, duration: 0.770s, episode steps: 118, steps per second: 153, episode reward: -106.848, mean reward: -0.905 [-100.000, 18.845], mean action: 1.492 [0.000, 3.000],  loss: 22.361099, mse: 401.638846, mean_q: 5.501441, mean_eps: 0.933433
  11219/150000: episode: 109, duration: 0.427s, episode steps:  65, steps per second: 152, episode reward: -91.954, mean reward: -1.415 [-100.000, 45.684], mean action: 1.508 [0.000, 3.000],  loss: 24.805117, mse: 437.836195, mean_q: 5.594104, mean_eps: 0.932884
  11290/150000: episode: 110, duration: 0.460s, episode steps:  71, steps per second: 154, episode reward: -66.371, mean reward: -0.935 [-100.000,  7.458], mean action: 1.634 [0.000, 3.000],  loss: 22.864191, mse: 435.614799, mean_q: 5.422884, mean_eps: 0.932476
  11380/150000: episode: 111, duration: 0.604s, episode steps:  90, steps per second: 149, episode reward: -90.755, mean reward: -1.008 [-100.000, 10.088], mean action: 1.567 [0.000, 3.000],  loss: 23.538894, mse: 405.355473, mean_q: 6.535932, mean_eps: 0.931993
  11455/150000: episode: 112, duration: 0.584s, episode steps:  75, steps per second: 128, episode reward: -95.527, mean reward: -1.274 [-100.000, 38.555], mean action: 1.507 [0.000, 3.000],  loss: 18.251901, mse: 416.622742, mean_q: 5.944766, mean_eps: 0.931498
  11598/150000: episode: 113, duration: 1.171s, episode steps: 143, steps per second: 122, episode reward: -44.425, mean reward: -0.311 [-100.000, 108.361], mean action: 1.448 [0.000, 3.000],  loss: 19.785175, mse: 413.248286, mean_q: 6.205636, mean_eps: 0.930844
  11697/150000: episode: 114, duration: 0.729s, episode steps:  99, steps per second: 136, episode reward: -340.735, mean reward: -3.442 [-100.000,  3.540], mean action: 1.394 [0.000, 3.000],  loss: 23.852065, mse: 401.253838, mean_q: 6.750869, mean_eps: 0.930118
  11769/150000: episode: 115, duration: 0.575s, episode steps:  72, steps per second: 125, episode reward: -92.437, mean reward: -1.284 [-100.000, 12.570], mean action: 1.556 [0.000, 3.000],  loss: 20.310831, mse: 425.132523, mean_q: 5.731184, mean_eps: 0.929605
  11877/150000: episode: 116, duration: 0.851s, episode steps: 108, steps per second: 127, episode reward: -11.302, mean reward: -0.105 [-100.000, 25.146], mean action: 1.639 [0.000, 3.000],  loss: 18.226100, mse: 408.806226, mean_q: 6.088978, mean_eps: 0.929065
  11975/150000: episode: 117, duration: 0.795s, episode steps:  98, steps per second: 123, episode reward: -78.977, mean reward: -0.806 [-100.000, 80.188], mean action: 1.684 [0.000, 3.000],  loss: 22.529316, mse: 440.353264, mean_q: 5.820853, mean_eps: 0.928447
  12097/150000: episode: 118, duration: 1.196s, episode steps: 122, steps per second: 102, episode reward: -111.547, mean reward: -0.914 [-100.000, 16.296], mean action: 1.648 [0.000, 3.000],  loss: 18.965013, mse: 454.878474, mean_q: 6.060421, mean_eps: 0.927787
  12184/150000: episode: 119, duration: 0.775s, episode steps:  87, steps per second: 112, episode reward: -116.499, mean reward: -1.339 [-100.000,  8.061], mean action: 1.460 [0.000, 3.000],  loss: 28.190620, mse: 480.061323, mean_q: 6.569603, mean_eps: 0.927160
  12258/150000: episode: 120, duration: 0.627s, episode steps:  74, steps per second: 118, episode reward: -150.553, mean reward: -2.035 [-100.000,  5.640], mean action: 1.635 [0.000, 3.000],  loss: 18.143448, mse: 429.209721, mean_q: 6.984416, mean_eps: 0.926677
  12383/150000: episode: 121, duration: 0.923s, episode steps: 125, steps per second: 135, episode reward: -88.827, mean reward: -0.711 [-100.000,  6.483], mean action: 1.552 [0.000, 3.000],  loss: 22.598911, mse: 445.321359, mean_q: 6.721962, mean_eps: 0.926080
  12501/150000: episode: 122, duration: 0.909s, episode steps: 118, steps per second: 130, episode reward: -233.256, mean reward: -1.977 [-100.000, 14.123], mean action: 1.576 [0.000, 3.000],  loss: 15.084042, mse: 453.408632, mean_q: 7.189693, mean_eps: 0.925351
  12595/150000: episode: 123, duration: 1.088s, episode steps:  94, steps per second:  86, episode reward: -117.393, mean reward: -1.249 [-100.000, 11.719], mean action: 1.532 [0.000, 3.000],  loss: 21.056308, mse: 463.849439, mean_q: 7.267162, mean_eps: 0.924715
  12678/150000: episode: 124, duration: 0.786s, episode steps:  83, steps per second: 106, episode reward: -270.134, mean reward: -3.255 [-100.000,  4.358], mean action: 1.783 [0.000, 3.000],  loss: 29.064121, mse: 439.186497, mean_q: 7.227713, mean_eps: 0.924184
  12807/150000: episode: 125, duration: 1.230s, episode steps: 129, steps per second: 105, episode reward: -304.201, mean reward: -2.358 [-100.000,  3.590], mean action: 1.682 [0.000, 3.000],  loss: 21.563181, mse: 434.611704, mean_q: 6.904505, mean_eps: 0.923548
  12926/150000: episode: 126, duration: 1.434s, episode steps: 119, steps per second:  83, episode reward: -36.818, mean reward: -0.309 [-100.000, 91.778], mean action: 1.521 [0.000, 3.000],  loss: 20.526828, mse: 431.268086, mean_q: 6.896241, mean_eps: 0.922804
  13030/150000: episode: 127, duration: 1.391s, episode steps: 104, steps per second:  75, episode reward: -120.791, mean reward: -1.161 [-100.000, 16.090], mean action: 1.596 [0.000, 3.000],  loss: 18.328256, mse: 457.978275, mean_q: 7.380274, mean_eps: 0.922135
  13144/150000: episode: 128, duration: 1.175s, episode steps: 114, steps per second:  97, episode reward: -159.500, mean reward: -1.399 [-100.000,  7.164], mean action: 1.509 [0.000, 3.000],  loss: 20.851364, mse: 482.363084, mean_q: 7.383702, mean_eps: 0.921481
  13266/150000: episode: 129, duration: 0.912s, episode steps: 122, steps per second: 134, episode reward: -201.024, mean reward: -1.648 [-100.000, 24.841], mean action: 1.344 [0.000, 3.000],  loss: 24.267658, mse: 463.114754, mean_q: 7.622187, mean_eps: 0.920773
  13385/150000: episode: 130, duration: 0.834s, episode steps: 119, steps per second: 143, episode reward: -153.709, mean reward: -1.292 [-100.000, 33.084], mean action: 1.672 [0.000, 3.000],  loss: 20.514883, mse: 456.563761, mean_q: 7.613435, mean_eps: 0.920050
  13474/150000: episode: 131, duration: 0.690s, episode steps:  89, steps per second: 129, episode reward: -57.535, mean reward: -0.646 [-100.000, 11.599], mean action: 1.663 [0.000, 3.000],  loss: 19.691121, mse: 429.039272, mean_q: 7.142400, mean_eps: 0.919426
  13555/150000: episode: 132, duration: 0.553s, episode steps:  81, steps per second: 147, episode reward: -94.524, mean reward: -1.167 [-100.000, 22.338], mean action: 1.580 [0.000, 3.000],  loss: 19.493873, mse: 467.713233, mean_q: 7.307656, mean_eps: 0.918916
  13654/150000: episode: 133, duration: 0.638s, episode steps:  99, steps per second: 155, episode reward: -105.016, mean reward: -1.061 [-100.000, 14.004], mean action: 1.535 [0.000, 3.000],  loss: 24.384885, mse: 459.006114, mean_q: 7.663902, mean_eps: 0.918376
  13759/150000: episode: 134, duration: 0.702s, episode steps: 105, steps per second: 150, episode reward: -106.825, mean reward: -1.017 [-100.000,  7.041], mean action: 1.352 [0.000, 3.000],  loss: 22.961851, mse: 463.433594, mean_q: 7.901619, mean_eps: 0.917764
  13857/150000: episode: 135, duration: 0.649s, episode steps:  98, steps per second: 151, episode reward: -93.518, mean reward: -0.954 [-100.000, 16.774], mean action: 1.663 [0.000, 3.000],  loss: 21.863912, mse: 465.415916, mean_q: 7.952830, mean_eps: 0.917155
  13927/150000: episode: 136, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -85.936, mean reward: -1.228 [-100.000,  6.953], mean action: 1.571 [0.000, 3.000],  loss: 20.674103, mse: 465.240424, mean_q: 8.305237, mean_eps: 0.916651
  14033/150000: episode: 137, duration: 0.757s, episode steps: 106, steps per second: 140, episode reward: -99.412, mean reward: -0.938 [-100.000, 11.019], mean action: 1.575 [0.000, 3.000],  loss: 17.096241, mse: 478.451390, mean_q: 8.630013, mean_eps: 0.916123
  14114/150000: episode: 138, duration: 0.612s, episode steps:  81, steps per second: 132, episode reward: -102.345, mean reward: -1.264 [-100.000,  7.768], mean action: 1.383 [0.000, 3.000],  loss: 17.629563, mse: 538.297904, mean_q: 9.096792, mean_eps: 0.915562
  14208/150000: episode: 139, duration: 0.668s, episode steps:  94, steps per second: 141, episode reward: -116.321, mean reward: -1.237 [-100.000,  7.823], mean action: 1.649 [0.000, 3.000],  loss: 18.976471, mse: 498.986728, mean_q: 9.778529, mean_eps: 0.915037
  14333/150000: episode: 140, duration: 0.872s, episode steps: 125, steps per second: 143, episode reward: -125.641, mean reward: -1.005 [-100.000,  7.871], mean action: 1.560 [0.000, 3.000],  loss: 19.082751, mse: 500.228008, mean_q: 9.592203, mean_eps: 0.914380
  14416/150000: episode: 141, duration: 0.626s, episode steps:  83, steps per second: 133, episode reward: -59.779, mean reward: -0.720 [-100.000, 17.351], mean action: 1.530 [0.000, 3.000],  loss: 16.521372, mse: 526.409268, mean_q: 9.013967, mean_eps: 0.913756
  14544/150000: episode: 142, duration: 0.879s, episode steps: 128, steps per second: 146, episode reward: -156.106, mean reward: -1.220 [-100.000,  6.340], mean action: 1.523 [0.000, 3.000],  loss: 16.924359, mse: 508.669245, mean_q: 9.407349, mean_eps: 0.913123
  14609/150000: episode: 143, duration: 0.456s, episode steps:  65, steps per second: 142, episode reward: 17.735, mean reward:  0.273 [-100.000, 85.461], mean action: 1.308 [0.000, 3.000],  loss: 22.665551, mse: 522.410186, mean_q: 9.740914, mean_eps: 0.912544
  14729/150000: episode: 144, duration: 0.900s, episode steps: 120, steps per second: 133, episode reward: -232.193, mean reward: -1.935 [-100.000,  1.222], mean action: 1.558 [0.000, 3.000],  loss: 20.542430, mse: 523.187758, mean_q: 8.825401, mean_eps: 0.911989
  14838/150000: episode: 145, duration: 0.940s, episode steps: 109, steps per second: 116, episode reward: -183.118, mean reward: -1.680 [-100.000,  1.371], mean action: 1.706 [0.000, 3.000],  loss: 15.546807, mse: 484.886879, mean_q: 9.457027, mean_eps: 0.911302
  14935/150000: episode: 146, duration: 0.866s, episode steps:  97, steps per second: 112, episode reward: -400.435, mean reward: -4.128 [-100.000,  0.015], mean action: 1.680 [0.000, 3.000],  loss: 22.983936, mse: 503.076363, mean_q: 8.872446, mean_eps: 0.910684
  15014/150000: episode: 147, duration: 0.601s, episode steps:  79, steps per second: 131, episode reward: -66.911, mean reward: -0.847 [-100.000, 13.885], mean action: 1.430 [0.000, 3.000],  loss: 22.636631, mse: 504.166360, mean_q: 8.904175, mean_eps: 0.910156
  15115/150000: episode: 148, duration: 0.711s, episode steps: 101, steps per second: 142, episode reward: -188.183, mean reward: -1.863 [-100.000,  6.446], mean action: 1.535 [0.000, 3.000],  loss: 23.615520, mse: 557.879133, mean_q: 9.758682, mean_eps: 0.909616
  15178/150000: episode: 149, duration: 0.458s, episode steps:  63, steps per second: 138, episode reward: -107.096, mean reward: -1.700 [-100.000,  7.733], mean action: 1.508 [0.000, 3.000],  loss: 13.343739, mse: 542.694309, mean_q: 10.649754, mean_eps: 0.909124
  15275/150000: episode: 150, duration: 0.750s, episode steps:  97, steps per second: 129, episode reward: -93.029, mean reward: -0.959 [-100.000,  7.109], mean action: 1.505 [0.000, 3.000],  loss: 18.505775, mse: 548.236412, mean_q: 10.051458, mean_eps: 0.908644
  15352/150000: episode: 151, duration: 0.542s, episode steps:  77, steps per second: 142, episode reward: -75.961, mean reward: -0.987 [-100.000, 18.636], mean action: 1.532 [0.000, 3.000],  loss: 11.704234, mse: 527.223909, mean_q: 10.800823, mean_eps: 0.908122
  15499/150000: episode: 152, duration: 1.084s, episode steps: 147, steps per second: 136, episode reward: -70.610, mean reward: -0.480 [-100.000,  9.855], mean action: 1.537 [0.000, 3.000],  loss: 18.702622, mse: 547.695881, mean_q: 10.093925, mean_eps: 0.907450
  15631/150000: episode: 153, duration: 1.024s, episode steps: 132, steps per second: 129, episode reward: -344.278, mean reward: -2.608 [-100.000, 101.853], mean action: 1.545 [0.000, 3.000],  loss: 19.596469, mse: 538.655766, mean_q: 10.095299, mean_eps: 0.906613
  15731/150000: episode: 154, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: -111.837, mean reward: -1.118 [-100.000,  7.258], mean action: 1.410 [0.000, 3.000],  loss: 25.131071, mse: 551.943974, mean_q: 10.361991, mean_eps: 0.905917
  15835/150000: episode: 155, duration: 0.757s, episode steps: 104, steps per second: 137, episode reward: -70.234, mean reward: -0.675 [-100.000,  7.199], mean action: 1.548 [0.000, 3.000],  loss: 18.710075, mse: 517.163801, mean_q: 10.587654, mean_eps: 0.905305
  15904/150000: episode: 156, duration: 0.453s, episode steps:  69, steps per second: 152, episode reward: -66.202, mean reward: -0.959 [-100.000,  8.386], mean action: 1.536 [0.000, 3.000],  loss: 17.298632, mse: 544.169427, mean_q: 9.975578, mean_eps: 0.904786
  15990/150000: episode: 157, duration: 0.555s, episode steps:  86, steps per second: 155, episode reward: -70.954, mean reward: -0.825 [-100.000, 13.653], mean action: 1.488 [0.000, 3.000],  loss: 17.116574, mse: 554.700748, mean_q: 9.877141, mean_eps: 0.904321
  16095/150000: episode: 158, duration: 0.659s, episode steps: 105, steps per second: 159, episode reward: -110.884, mean reward: -1.056 [-100.000, 13.575], mean action: 1.514 [0.000, 3.000],  loss: 13.858009, mse: 599.514775, mean_q: 11.837371, mean_eps: 0.903748
  16183/150000: episode: 159, duration: 0.603s, episode steps:  88, steps per second: 146, episode reward: -126.117, mean reward: -1.433 [-100.000, 10.976], mean action: 1.432 [0.000, 3.000],  loss: 15.061837, mse: 599.381773, mean_q: 11.680001, mean_eps: 0.903169
  16287/150000: episode: 160, duration: 0.664s, episode steps: 104, steps per second: 157, episode reward: -314.196, mean reward: -3.021 [-100.000, 127.107], mean action: 1.596 [0.000, 3.000],  loss: 16.356253, mse: 595.260800, mean_q: 11.245596, mean_eps: 0.902593
  16360/150000: episode: 161, duration: 0.466s, episode steps:  73, steps per second: 157, episode reward: -154.593, mean reward: -2.118 [-100.000,  9.914], mean action: 1.452 [0.000, 3.000],  loss: 14.389293, mse: 615.450555, mean_q: 11.523157, mean_eps: 0.902062
  16434/150000: episode: 162, duration: 0.539s, episode steps:  74, steps per second: 137, episode reward: -116.453, mean reward: -1.574 [-100.000, 15.697], mean action: 1.473 [0.000, 3.000],  loss: 23.763778, mse: 596.508085, mean_q: 11.751211, mean_eps: 0.901621
  16568/150000: episode: 163, duration: 1.050s, episode steps: 134, steps per second: 128, episode reward: -83.834, mean reward: -0.626 [-100.000, 15.557], mean action: 1.433 [0.000, 3.000],  loss: 18.402831, mse: 605.114931, mean_q: 11.610760, mean_eps: 0.900997
  16677/150000: episode: 164, duration: 0.795s, episode steps: 109, steps per second: 137, episode reward: -5.802, mean reward: -0.053 [-100.000, 88.482], mean action: 1.367 [0.000, 3.000],  loss: 22.638983, mse: 605.147177, mean_q: 11.557758, mean_eps: 0.900268
  16802/150000: episode: 165, duration: 1.034s, episode steps: 125, steps per second: 121, episode reward: -115.054, mean reward: -0.920 [-100.000, 15.196], mean action: 1.440 [0.000, 3.000],  loss: 20.820698, mse: 611.668395, mean_q: 11.735965, mean_eps: 0.899566
  16874/150000: episode: 166, duration: 0.573s, episode steps:  72, steps per second: 126, episode reward: -60.367, mean reward: -0.838 [-100.000, 13.005], mean action: 1.569 [0.000, 3.000],  loss: 20.169347, mse: 564.172153, mean_q: 11.898808, mean_eps: 0.898975
  16970/150000: episode: 167, duration: 0.721s, episode steps:  96, steps per second: 133, episode reward: -207.097, mean reward: -2.157 [-100.000,  7.919], mean action: 1.615 [0.000, 3.000],  loss: 19.013123, mse: 582.091729, mean_q: 12.169454, mean_eps: 0.898471
  17077/150000: episode: 168, duration: 0.845s, episode steps: 107, steps per second: 127, episode reward: -158.037, mean reward: -1.477 [-100.000, 14.929], mean action: 1.636 [0.000, 3.000],  loss: 26.663205, mse: 617.609167, mean_q: 12.803909, mean_eps: 0.897862
  17163/150000: episode: 169, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward:  3.259, mean reward:  0.038 [-100.000, 100.616], mean action: 1.291 [0.000, 3.000],  loss: 18.918665, mse: 626.958958, mean_q: 12.528630, mean_eps: 0.897283
  17215/150000: episode: 170, duration: 0.368s, episode steps:  52, steps per second: 141, episode reward: -123.940, mean reward: -2.383 [-100.000,  6.936], mean action: 1.519 [0.000, 3.000],  loss: 19.301331, mse: 608.179212, mean_q: 12.145600, mean_eps: 0.896869
  17307/150000: episode: 171, duration: 0.657s, episode steps:  92, steps per second: 140, episode reward: -243.848, mean reward: -2.651 [-100.000,  5.627], mean action: 1.565 [0.000, 3.000],  loss: 17.889898, mse: 638.984414, mean_q: 12.322577, mean_eps: 0.896437
  17399/150000: episode: 172, duration: 0.653s, episode steps:  92, steps per second: 141, episode reward: -165.471, mean reward: -1.799 [-100.000,  5.630], mean action: 1.337 [0.000, 3.000],  loss: 18.645868, mse: 618.139703, mean_q: 12.199402, mean_eps: 0.895885
  17516/150000: episode: 173, duration: 1.173s, episode steps: 117, steps per second: 100, episode reward: -110.945, mean reward: -0.948 [-100.000,  7.056], mean action: 1.513 [0.000, 3.000],  loss: 22.374364, mse: 630.860569, mean_q: 12.641225, mean_eps: 0.895258
  17611/150000: episode: 174, duration: 1.083s, episode steps:  95, steps per second:  88, episode reward: -142.979, mean reward: -1.505 [-100.000,  5.360], mean action: 1.526 [0.000, 3.000],  loss: 24.106019, mse: 628.965079, mean_q: 12.463555, mean_eps: 0.894622
  17705/150000: episode: 175, duration: 1.158s, episode steps:  94, steps per second:  81, episode reward: -96.136, mean reward: -1.023 [-100.000, 11.746], mean action: 1.404 [0.000, 3.000],  loss: 21.698789, mse: 648.776668, mean_q: 12.561488, mean_eps: 0.894055
  17806/150000: episode: 176, duration: 1.162s, episode steps: 101, steps per second:  87, episode reward: -91.325, mean reward: -0.904 [-100.000,  8.326], mean action: 1.614 [0.000, 3.000],  loss: 21.417327, mse: 618.920889, mean_q: 13.168759, mean_eps: 0.893470
  17874/150000: episode: 177, duration: 0.503s, episode steps:  68, steps per second: 135, episode reward: -58.814, mean reward: -0.865 [-100.000, 10.986], mean action: 1.485 [0.000, 3.000],  loss: 25.805557, mse: 625.663199, mean_q: 13.347222, mean_eps: 0.892963
  18004/150000: episode: 178, duration: 1.212s, episode steps: 130, steps per second: 107, episode reward: -73.322, mean reward: -0.564 [-100.000, 11.058], mean action: 1.585 [0.000, 3.000],  loss: 17.824652, mse: 632.250243, mean_q: 13.421875, mean_eps: 0.892369
  18106/150000: episode: 179, duration: 0.712s, episode steps: 102, steps per second: 143, episode reward: -118.500, mean reward: -1.162 [-100.000, 18.419], mean action: 1.735 [0.000, 3.000],  loss: 26.444922, mse: 679.809270, mean_q: 13.898621, mean_eps: 0.891673
  18192/150000: episode: 180, duration: 0.604s, episode steps:  86, steps per second: 142, episode reward: -172.942, mean reward: -2.011 [-100.000, 13.111], mean action: 1.349 [0.000, 3.000],  loss: 25.861282, mse: 647.124022, mean_q: 14.494396, mean_eps: 0.891109
  18285/150000: episode: 181, duration: 0.654s, episode steps:  93, steps per second: 142, episode reward: -102.143, mean reward: -1.098 [-100.000,  8.154], mean action: 1.570 [0.000, 3.000],  loss: 21.025463, mse: 641.006397, mean_q: 14.049654, mean_eps: 0.890572
  18404/150000: episode: 182, duration: 1.011s, episode steps: 119, steps per second: 118, episode reward: -276.753, mean reward: -2.326 [-100.000, 27.611], mean action: 1.496 [0.000, 3.000],  loss: 21.942410, mse: 642.192498, mean_q: 14.548713, mean_eps: 0.889936
  18509/150000: episode: 183, duration: 0.963s, episode steps: 105, steps per second: 109, episode reward: -270.015, mean reward: -2.572 [-100.000, 11.817], mean action: 1.448 [0.000, 3.000],  loss: 21.481603, mse: 644.927526, mean_q: 14.017887, mean_eps: 0.889264
  18596/150000: episode: 184, duration: 0.856s, episode steps:  87, steps per second: 102, episode reward: -88.273, mean reward: -1.015 [-100.000, 11.062], mean action: 1.517 [0.000, 3.000],  loss: 25.089835, mse: 635.094766, mean_q: 14.453310, mean_eps: 0.888688
  18701/150000: episode: 185, duration: 0.985s, episode steps: 105, steps per second: 107, episode reward: -125.954, mean reward: -1.200 [-100.000, 11.290], mean action: 1.514 [0.000, 3.000],  loss: 18.538408, mse: 684.357405, mean_q: 14.270062, mean_eps: 0.888112
  18775/150000: episode: 186, duration: 0.633s, episode steps:  74, steps per second: 117, episode reward: -130.411, mean reward: -1.762 [-100.000, 26.876], mean action: 1.595 [0.000, 3.000],  loss: 17.187361, mse: 601.659029, mean_q: 15.005292, mean_eps: 0.887575
  18873/150000: episode: 187, duration: 0.728s, episode steps:  98, steps per second: 135, episode reward: -127.014, mean reward: -1.296 [-100.000,  9.756], mean action: 1.571 [0.000, 3.000],  loss: 18.161010, mse: 642.490683, mean_q: 14.131595, mean_eps: 0.887059
  18947/150000: episode: 188, duration: 0.497s, episode steps:  74, steps per second: 149, episode reward: -97.051, mean reward: -1.312 [-100.000, 16.548], mean action: 1.608 [0.000, 3.000],  loss: 20.169906, mse: 632.812474, mean_q: 15.032185, mean_eps: 0.886543
  19033/150000: episode: 189, duration: 0.550s, episode steps:  86, steps per second: 156, episode reward: -98.698, mean reward: -1.148 [-100.000,  7.980], mean action: 1.465 [0.000, 3.000],  loss: 20.683428, mse: 657.104377, mean_q: 15.155515, mean_eps: 0.886063
  19110/150000: episode: 190, duration: 0.526s, episode steps:  77, steps per second: 146, episode reward: -110.398, mean reward: -1.434 [-100.000,  6.122], mean action: 1.610 [0.000, 3.000],  loss: 20.142652, mse: 685.127439, mean_q: 15.125939, mean_eps: 0.885574
  19224/150000: episode: 191, duration: 0.885s, episode steps: 114, steps per second: 129, episode reward: -109.405, mean reward: -0.960 [-100.000,  6.608], mean action: 1.588 [0.000, 3.000],  loss: 18.693750, mse: 682.022384, mean_q: 15.062875, mean_eps: 0.885001
  19335/150000: episode: 192, duration: 0.831s, episode steps: 111, steps per second: 134, episode reward: -103.873, mean reward: -0.936 [-100.000,  9.807], mean action: 1.441 [0.000, 3.000],  loss: 21.772437, mse: 689.683492, mean_q: 15.498777, mean_eps: 0.884326
  19419/150000: episode: 193, duration: 0.638s, episode steps:  84, steps per second: 132, episode reward: -112.829, mean reward: -1.343 [-100.000, 34.920], mean action: 1.631 [0.000, 3.000],  loss: 18.459669, mse: 695.425913, mean_q: 14.607150, mean_eps: 0.883741
  19526/150000: episode: 194, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -175.633, mean reward: -1.641 [-100.000, 13.039], mean action: 1.308 [0.000, 3.000],  loss: 24.388354, mse: 711.709536, mean_q: 14.889751, mean_eps: 0.883168
  19651/150000: episode: 195, duration: 0.837s, episode steps: 125, steps per second: 149, episode reward: -68.984, mean reward: -0.552 [-100.000,  6.902], mean action: 1.384 [0.000, 3.000],  loss: 25.244005, mse: 705.418309, mean_q: 14.887949, mean_eps: 0.882472
  19732/150000: episode: 196, duration: 0.597s, episode steps:  81, steps per second: 136, episode reward: -107.892, mean reward: -1.332 [-100.000,  9.293], mean action: 1.531 [0.000, 3.000],  loss: 17.502781, mse: 723.593788, mean_q: 14.870044, mean_eps: 0.881854
  19832/150000: episode: 197, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -106.894, mean reward: -1.069 [-100.000,  8.433], mean action: 1.350 [0.000, 3.000],  loss: 20.013869, mse: 712.528255, mean_q: 14.771651, mean_eps: 0.881311
  19923/150000: episode: 198, duration: 0.645s, episode steps:  91, steps per second: 141, episode reward: -308.750, mean reward: -3.393 [-100.000, 109.647], mean action: 1.462 [0.000, 3.000],  loss: 18.888401, mse: 716.813358, mean_q: 15.045961, mean_eps: 0.880738
  19988/150000: episode: 199, duration: 0.496s, episode steps:  65, steps per second: 131, episode reward: -97.520, mean reward: -1.500 [-100.000,  8.826], mean action: 1.492 [0.000, 3.000],  loss: 15.615577, mse: 706.794477, mean_q: 15.295409, mean_eps: 0.880270
  20076/150000: episode: 200, duration: 0.648s, episode steps:  88, steps per second: 136, episode reward: -53.065, mean reward: -0.603 [-100.000, 12.494], mean action: 1.364 [0.000, 3.000],  loss: 17.828153, mse: 739.218573, mean_q: 16.688701, mean_eps: 0.879811
  20168/150000: episode: 201, duration: 0.628s, episode steps:  92, steps per second: 146, episode reward: -78.847, mean reward: -0.857 [-100.000, 15.088], mean action: 1.630 [0.000, 3.000],  loss: 23.449027, mse: 754.754432, mean_q: 16.712159, mean_eps: 0.879271
  20237/150000: episode: 202, duration: 0.477s, episode steps:  69, steps per second: 145, episode reward: -88.731, mean reward: -1.286 [-100.000, 22.013], mean action: 1.725 [0.000, 3.000],  loss: 15.755168, mse: 770.561685, mean_q: 16.769592, mean_eps: 0.878788
  20365/150000: episode: 203, duration: 0.970s, episode steps: 128, steps per second: 132, episode reward: -153.435, mean reward: -1.199 [-100.000,  6.300], mean action: 1.695 [0.000, 3.000],  loss: 19.507747, mse: 776.847917, mean_q: 15.892218, mean_eps: 0.878197
  20438/150000: episode: 204, duration: 0.506s, episode steps:  73, steps per second: 144, episode reward: 14.356, mean reward:  0.197 [-100.000, 73.813], mean action: 1.671 [0.000, 3.000],  loss: 17.600087, mse: 774.126384, mean_q: 15.819478, mean_eps: 0.877594
  20543/150000: episode: 205, duration: 0.759s, episode steps: 105, steps per second: 138, episode reward: -163.683, mean reward: -1.559 [-100.000, 16.800], mean action: 1.448 [0.000, 3.000],  loss: 19.651357, mse: 788.645680, mean_q: 16.278947, mean_eps: 0.877060
  20610/150000: episode: 206, duration: 0.489s, episode steps:  67, steps per second: 137, episode reward: -133.945, mean reward: -1.999 [-100.000, 39.011], mean action: 1.567 [0.000, 3.000],  loss: 9.045529, mse: 783.333582, mean_q: 16.416416, mean_eps: 0.876544
  20683/150000: episode: 207, duration: 0.507s, episode steps:  73, steps per second: 144, episode reward: -80.317, mean reward: -1.100 [-100.000,  7.119], mean action: 1.479 [0.000, 3.000],  loss: 18.167279, mse: 771.037277, mean_q: 16.367854, mean_eps: 0.876124
  20764/150000: episode: 208, duration: 0.547s, episode steps:  81, steps per second: 148, episode reward: -81.575, mean reward: -1.007 [-100.000, 13.000], mean action: 1.556 [0.000, 3.000],  loss: 11.851139, mse: 760.841882, mean_q: 16.902470, mean_eps: 0.875662
  20886/150000: episode: 209, duration: 0.902s, episode steps: 122, steps per second: 135, episode reward: -96.698, mean reward: -0.793 [-100.000, 10.266], mean action: 1.549 [0.000, 3.000],  loss: 18.265284, mse: 773.145548, mean_q: 16.509468, mean_eps: 0.875053
  20980/150000: episode: 210, duration: 0.669s, episode steps:  94, steps per second: 140, episode reward: -104.410, mean reward: -1.111 [-100.000, 11.074], mean action: 1.457 [0.000, 3.000],  loss: 25.445048, mse: 747.304694, mean_q: 16.682271, mean_eps: 0.874405
  21096/150000: episode: 211, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -98.728, mean reward: -0.851 [-100.000,  7.055], mean action: 1.448 [0.000, 3.000],  loss: 17.685703, mse: 836.010348, mean_q: 16.606604, mean_eps: 0.873775
  21212/150000: episode: 212, duration: 0.832s, episode steps: 116, steps per second: 139, episode reward: -116.549, mean reward: -1.005 [-100.000,  9.934], mean action: 1.448 [0.000, 3.000],  loss: 18.055636, mse: 842.640109, mean_q: 16.066764, mean_eps: 0.873079
  21362/150000: episode: 213, duration: 0.976s, episode steps: 150, steps per second: 154, episode reward: -84.354, mean reward: -0.562 [-100.000, 10.051], mean action: 1.480 [0.000, 3.000],  loss: 22.399057, mse: 847.546228, mean_q: 16.143229, mean_eps: 0.872281
  21424/150000: episode: 214, duration: 0.387s, episode steps:  62, steps per second: 160, episode reward: -74.508, mean reward: -1.202 [-100.000, 29.374], mean action: 1.565 [0.000, 3.000],  loss: 18.110933, mse: 831.041252, mean_q: 16.944581, mean_eps: 0.871645
  21484/150000: episode: 215, duration: 0.424s, episode steps:  60, steps per second: 141, episode reward: -84.269, mean reward: -1.404 [-100.000, 26.111], mean action: 1.717 [0.000, 3.000],  loss: 24.722577, mse: 832.749799, mean_q: 17.684935, mean_eps: 0.871279
  21567/150000: episode: 216, duration: 0.644s, episode steps:  83, steps per second: 129, episode reward: -84.544, mean reward: -1.019 [-100.000, 17.125], mean action: 1.482 [0.000, 3.000],  loss: 18.311047, mse: 824.868546, mean_q: 16.506536, mean_eps: 0.870850
  21683/150000: episode: 217, duration: 0.982s, episode steps: 116, steps per second: 118, episode reward: -184.728, mean reward: -1.592 [-100.000,  4.078], mean action: 1.534 [0.000, 3.000],  loss: 25.579211, mse: 851.351473, mean_q: 16.350446, mean_eps: 0.870253
  21770/150000: episode: 218, duration: 0.666s, episode steps:  87, steps per second: 131, episode reward: -64.170, mean reward: -0.738 [-100.000, 14.906], mean action: 1.517 [0.000, 3.000],  loss: 22.145912, mse: 842.509368, mean_q: 17.318135, mean_eps: 0.869644
  21847/150000: episode: 219, duration: 0.615s, episode steps:  77, steps per second: 125, episode reward: -64.628, mean reward: -0.839 [-100.000, 13.572], mean action: 1.481 [0.000, 3.000],  loss: 20.118598, mse: 846.676654, mean_q: 17.133770, mean_eps: 0.869152
  21956/150000: episode: 220, duration: 0.870s, episode steps: 109, steps per second: 125, episode reward: -56.313, mean reward: -0.517 [-100.000, 12.262], mean action: 1.596 [0.000, 3.000],  loss: 27.935275, mse: 812.638068, mean_q: 17.313737, mean_eps: 0.868594
  22023/150000: episode: 221, duration: 0.483s, episode steps:  67, steps per second: 139, episode reward: -134.367, mean reward: -2.005 [-100.000,  7.342], mean action: 1.731 [0.000, 3.000],  loss: 16.648222, mse: 826.798479, mean_q: 17.058449, mean_eps: 0.868066
  22089/150000: episode: 222, duration: 0.507s, episode steps:  66, steps per second: 130, episode reward: -124.464, mean reward: -1.886 [-100.000, 11.070], mean action: 1.500 [0.000, 3.000],  loss: 26.367216, mse: 897.274422, mean_q: 17.275486, mean_eps: 0.867667
  22184/150000: episode: 223, duration: 0.647s, episode steps:  95, steps per second: 147, episode reward: -92.133, mean reward: -0.970 [-100.000, 13.222], mean action: 1.632 [0.000, 3.000],  loss: 17.001961, mse: 866.917322, mean_q: 17.196071, mean_eps: 0.867184
  22321/150000: episode: 224, duration: 0.879s, episode steps: 137, steps per second: 156, episode reward: -54.106, mean reward: -0.395 [-100.000, 14.362], mean action: 1.635 [0.000, 3.000],  loss: 19.429160, mse: 859.410113, mean_q: 16.827547, mean_eps: 0.866488
  22433/150000: episode: 225, duration: 0.788s, episode steps: 112, steps per second: 142, episode reward: -351.911, mean reward: -3.142 [-100.000, 91.711], mean action: 1.554 [0.000, 3.000],  loss: 13.898072, mse: 865.137046, mean_q: 17.751054, mean_eps: 0.865741
  22512/150000: episode: 226, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -64.718, mean reward: -0.819 [-100.000, 21.091], mean action: 1.494 [0.000, 3.000],  loss: 23.574957, mse: 873.593804, mean_q: 17.058824, mean_eps: 0.865168
  22638/150000: episode: 227, duration: 0.868s, episode steps: 126, steps per second: 145, episode reward: -165.510, mean reward: -1.314 [-100.000,  5.408], mean action: 1.643 [0.000, 3.000],  loss: 18.800715, mse: 905.586210, mean_q: 17.381325, mean_eps: 0.864553
  22749/150000: episode: 228, duration: 0.868s, episode steps: 111, steps per second: 128, episode reward: -86.535, mean reward: -0.780 [-100.000,  7.081], mean action: 1.604 [0.000, 3.000],  loss: 14.658719, mse: 898.154709, mean_q: 17.416795, mean_eps: 0.863842
  22819/150000: episode: 229, duration: 0.607s, episode steps:  70, steps per second: 115, episode reward: -30.956, mean reward: -0.442 [-100.000, 11.181], mean action: 1.571 [0.000, 3.000],  loss: 19.227337, mse: 904.248968, mean_q: 16.265918, mean_eps: 0.863299
  22897/150000: episode: 230, duration: 0.558s, episode steps:  78, steps per second: 140, episode reward: -68.104, mean reward: -0.873 [-100.000,  6.568], mean action: 1.590 [0.000, 3.000],  loss: 25.850269, mse: 874.719150, mean_q: 17.199206, mean_eps: 0.862855
  22999/150000: episode: 231, duration: 0.798s, episode steps: 102, steps per second: 128, episode reward: -2.358, mean reward: -0.023 [-100.000, 69.899], mean action: 1.373 [0.000, 3.000],  loss: 18.932693, mse: 907.956404, mean_q: 16.736462, mean_eps: 0.862315
  23117/150000: episode: 232, duration: 0.864s, episode steps: 118, steps per second: 137, episode reward: -101.280, mean reward: -0.858 [-100.000,  9.214], mean action: 1.492 [0.000, 3.000],  loss: 20.322763, mse: 946.111191, mean_q: 17.779017, mean_eps: 0.861655
  23191/150000: episode: 233, duration: 0.525s, episode steps:  74, steps per second: 141, episode reward: -137.685, mean reward: -1.861 [-100.000, 10.086], mean action: 1.743 [0.000, 3.000],  loss: 14.618167, mse: 920.662418, mean_q: 18.662534, mean_eps: 0.861079
  23276/150000: episode: 234, duration: 0.617s, episode steps:  85, steps per second: 138, episode reward: -99.591, mean reward: -1.172 [-100.000,  8.944], mean action: 1.624 [0.000, 3.000],  loss: 18.370797, mse: 947.790655, mean_q: 17.537545, mean_eps: 0.860602
  23373/150000: episode: 235, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -170.050, mean reward: -1.753 [-100.000, 29.764], mean action: 1.433 [0.000, 3.000],  loss: 22.549699, mse: 945.068928, mean_q: 18.880912, mean_eps: 0.860056
  23506/150000: episode: 236, duration: 1.073s, episode steps: 133, steps per second: 124, episode reward: -36.820, mean reward: -0.277 [-100.000, 17.473], mean action: 1.564 [0.000, 3.000],  loss: 18.593860, mse: 936.900707, mean_q: 17.504113, mean_eps: 0.859366
  23596/150000: episode: 237, duration: 0.742s, episode steps:  90, steps per second: 121, episode reward: -116.683, mean reward: -1.296 [-100.000, 21.258], mean action: 1.656 [0.000, 3.000],  loss: 19.210194, mse: 929.785551, mean_q: 17.676241, mean_eps: 0.858697
  23701/150000: episode: 238, duration: 0.842s, episode steps: 105, steps per second: 125, episode reward: -96.037, mean reward: -0.915 [-100.000, 28.676], mean action: 1.552 [0.000, 3.000],  loss: 21.906490, mse: 971.268374, mean_q: 17.517501, mean_eps: 0.858112
  23786/150000: episode: 239, duration: 0.743s, episode steps:  85, steps per second: 114, episode reward: -98.645, mean reward: -1.161 [-100.000, 17.853], mean action: 1.541 [0.000, 3.000],  loss: 18.909531, mse: 974.431957, mean_q: 17.697947, mean_eps: 0.857542
  23930/150000: episode: 240, duration: 1.256s, episode steps: 144, steps per second: 115, episode reward: -97.988, mean reward: -0.680 [-100.000,  7.819], mean action: 1.556 [0.000, 3.000],  loss: 17.278786, mse: 931.714021, mean_q: 18.418744, mean_eps: 0.856855
  24035/150000: episode: 241, duration: 0.751s, episode steps: 105, steps per second: 140, episode reward: -146.361, mean reward: -1.394 [-100.000,  8.185], mean action: 1.581 [0.000, 3.000],  loss: 15.901500, mse: 942.328541, mean_q: 17.878018, mean_eps: 0.856108
  24120/150000: episode: 242, duration: 0.630s, episode steps:  85, steps per second: 135, episode reward: -217.285, mean reward: -2.556 [-100.000, 14.392], mean action: 1.576 [0.000, 3.000],  loss: 13.808809, mse: 992.571771, mean_q: 18.690185, mean_eps: 0.855538
  24233/150000: episode: 243, duration: 0.739s, episode steps: 113, steps per second: 153, episode reward: -44.884, mean reward: -0.397 [-100.000, 26.028], mean action: 1.398 [0.000, 3.000],  loss: 15.233641, mse: 985.146368, mean_q: 18.739842, mean_eps: 0.854944
  24326/150000: episode: 244, duration: 0.611s, episode steps:  93, steps per second: 152, episode reward: -263.727, mean reward: -2.836 [-100.000,  0.288], mean action: 1.624 [0.000, 3.000],  loss: 17.128184, mse: 970.523733, mean_q: 18.918483, mean_eps: 0.854326
  24436/150000: episode: 245, duration: 0.761s, episode steps: 110, steps per second: 145, episode reward: -165.346, mean reward: -1.503 [-100.000,  4.054], mean action: 1.491 [0.000, 3.000],  loss: 18.882403, mse: 974.677145, mean_q: 18.163438, mean_eps: 0.853717
  24538/150000: episode: 246, duration: 0.673s, episode steps: 102, steps per second: 152, episode reward: -68.393, mean reward: -0.671 [-100.000, 16.708], mean action: 1.500 [0.000, 3.000],  loss: 20.884570, mse: 1002.203156, mean_q: 18.802730, mean_eps: 0.853081
  24627/150000: episode: 247, duration: 0.577s, episode steps:  89, steps per second: 154, episode reward: -95.707, mean reward: -1.075 [-100.000,  7.576], mean action: 1.517 [0.000, 3.000],  loss: 16.538043, mse: 957.251024, mean_q: 19.620868, mean_eps: 0.852508
  24723/150000: episode: 248, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -151.778, mean reward: -1.581 [-100.000,  7.727], mean action: 1.708 [0.000, 3.000],  loss: 17.303966, mse: 987.102956, mean_q: 19.451419, mean_eps: 0.851953
  24826/150000: episode: 249, duration: 0.705s, episode steps: 103, steps per second: 146, episode reward: -108.005, mean reward: -1.049 [-100.000, 16.007], mean action: 1.612 [0.000, 3.000],  loss: 17.194034, mse: 1000.985413, mean_q: 18.870738, mean_eps: 0.851356
  24891/150000: episode: 250, duration: 0.427s, episode steps:  65, steps per second: 152, episode reward: -70.014, mean reward: -1.077 [-100.000,  6.894], mean action: 1.569 [0.000, 3.000],  loss: 16.441783, mse: 973.532091, mean_q: 18.869033, mean_eps: 0.850852
  24978/150000: episode: 251, duration: 0.570s, episode steps:  87, steps per second: 153, episode reward: -79.904, mean reward: -0.918 [-100.000,  6.488], mean action: 1.494 [0.000, 3.000],  loss: 18.422880, mse: 1015.071855, mean_q: 18.941432, mean_eps: 0.850396
  25101/150000: episode: 252, duration: 0.841s, episode steps: 123, steps per second: 146, episode reward: -108.327, mean reward: -0.881 [-100.000,  5.716], mean action: 1.577 [0.000, 3.000],  loss: 18.315840, mse: 1054.209347, mean_q: 20.011095, mean_eps: 0.849766
  25208/150000: episode: 253, duration: 0.731s, episode steps: 107, steps per second: 146, episode reward: -49.500, mean reward: -0.463 [-100.000, 13.277], mean action: 1.682 [0.000, 3.000],  loss: 19.137720, mse: 1062.262123, mean_q: 20.643554, mean_eps: 0.849076
  25306/150000: episode: 254, duration: 0.649s, episode steps:  98, steps per second: 151, episode reward: -122.545, mean reward: -1.250 [-100.000,  4.799], mean action: 1.551 [0.000, 3.000],  loss: 19.900115, mse: 1054.765409, mean_q: 20.151454, mean_eps: 0.848461
  25446/150000: episode: 255, duration: 0.977s, episode steps: 140, steps per second: 143, episode reward: -41.797, mean reward: -0.299 [-100.000, 66.941], mean action: 1.621 [0.000, 3.000],  loss: 13.293919, mse: 1047.804870, mean_q: 20.680641, mean_eps: 0.847747
  25515/150000: episode: 256, duration: 0.448s, episode steps:  69, steps per second: 154, episode reward: -74.640, mean reward: -1.082 [-100.000, 16.226], mean action: 1.391 [0.000, 3.000],  loss: 19.841310, mse: 1065.447140, mean_q: 20.940792, mean_eps: 0.847120
  25620/150000: episode: 257, duration: 0.675s, episode steps: 105, steps per second: 156, episode reward: -67.990, mean reward: -0.648 [-100.000,  7.304], mean action: 1.667 [0.000, 3.000],  loss: 16.555737, mse: 1076.399511, mean_q: 20.109801, mean_eps: 0.846598
  25718/150000: episode: 258, duration: 0.677s, episode steps:  98, steps per second: 145, episode reward: -119.358, mean reward: -1.218 [-100.000, 11.225], mean action: 1.643 [0.000, 3.000],  loss: 18.184964, mse: 1067.081473, mean_q: 20.283025, mean_eps: 0.845989
  25819/150000: episode: 259, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -95.249, mean reward: -0.943 [-100.000, 12.375], mean action: 1.574 [0.000, 3.000],  loss: 16.813257, mse: 1083.721223, mean_q: 20.856080, mean_eps: 0.845392
  25903/150000: episode: 260, duration: 0.547s, episode steps:  84, steps per second: 153, episode reward: -187.182, mean reward: -2.228 [-100.000, 45.028], mean action: 1.548 [0.000, 3.000],  loss: 21.067511, mse: 1083.031907, mean_q: 20.044543, mean_eps: 0.844837
  25991/150000: episode: 261, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: -59.528, mean reward: -0.676 [-100.000,  9.699], mean action: 1.511 [0.000, 3.000],  loss: 22.894140, mse: 1132.703560, mean_q: 20.028643, mean_eps: 0.844321
  26110/150000: episode: 262, duration: 0.854s, episode steps: 119, steps per second: 139, episode reward: -92.027, mean reward: -0.773 [-100.000,  7.366], mean action: 1.681 [0.000, 3.000],  loss: 16.394193, mse: 1097.934545, mean_q: 22.061683, mean_eps: 0.843700
  26177/150000: episode: 263, duration: 0.508s, episode steps:  67, steps per second: 132, episode reward: -81.202, mean reward: -1.212 [-100.000, 11.230], mean action: 1.731 [0.000, 3.000],  loss: 19.003720, mse: 1124.162136, mean_q: 21.920458, mean_eps: 0.843142
  26300/150000: episode: 264, duration: 0.839s, episode steps: 123, steps per second: 147, episode reward: -55.638, mean reward: -0.452 [-100.000, 10.935], mean action: 1.447 [0.000, 3.000],  loss: 17.471341, mse: 1139.797878, mean_q: 21.318444, mean_eps: 0.842572
  26433/150000: episode: 265, duration: 0.892s, episode steps: 133, steps per second: 149, episode reward: -59.023, mean reward: -0.444 [-100.000, 17.541], mean action: 1.481 [0.000, 3.000],  loss: 21.368062, mse: 1101.271055, mean_q: 21.348949, mean_eps: 0.841804
  26568/150000: episode: 266, duration: 0.881s, episode steps: 135, steps per second: 153, episode reward: -102.129, mean reward: -0.757 [-100.000, 22.083], mean action: 1.711 [0.000, 3.000],  loss: 21.533916, mse: 1106.459535, mean_q: 21.570879, mean_eps: 0.841000
  26684/150000: episode: 267, duration: 0.798s, episode steps: 116, steps per second: 145, episode reward: -117.770, mean reward: -1.015 [-100.000, 20.108], mean action: 1.578 [0.000, 3.000],  loss: 17.374982, mse: 1114.740492, mean_q: 21.503827, mean_eps: 0.840247
  26790/150000: episode: 268, duration: 0.687s, episode steps: 106, steps per second: 154, episode reward: -108.977, mean reward: -1.028 [-100.000, 13.414], mean action: 1.311 [0.000, 3.000],  loss: 13.785533, mse: 1128.209192, mean_q: 21.398792, mean_eps: 0.839581
  26900/150000: episode: 269, duration: 0.718s, episode steps: 110, steps per second: 153, episode reward: -54.356, mean reward: -0.494 [-100.000, 13.376], mean action: 1.709 [0.000, 3.000],  loss: 19.392056, mse: 1128.909977, mean_q: 22.127835, mean_eps: 0.838933
  26993/150000: episode: 270, duration: 0.652s, episode steps:  93, steps per second: 143, episode reward: -107.156, mean reward: -1.152 [-100.000, 17.212], mean action: 1.495 [0.000, 3.000],  loss: 20.124117, mse: 1089.943961, mean_q: 21.941041, mean_eps: 0.838324
  27110/150000: episode: 271, duration: 0.771s, episode steps: 117, steps per second: 152, episode reward: -73.095, mean reward: -0.625 [-100.000, 20.089], mean action: 1.513 [0.000, 3.000],  loss: 28.192151, mse: 1155.206269, mean_q: 22.027560, mean_eps: 0.837694
  27179/150000: episode: 272, duration: 0.455s, episode steps:  69, steps per second: 152, episode reward: 16.719, mean reward:  0.242 [-100.000, 100.694], mean action: 1.493 [0.000, 3.000],  loss: 21.645319, mse: 1169.840636, mean_q: 23.118647, mean_eps: 0.837136
  27272/150000: episode: 273, duration: 0.610s, episode steps:  93, steps per second: 152, episode reward: -97.474, mean reward: -1.048 [-100.000, 21.557], mean action: 1.484 [0.000, 3.000],  loss: 16.075660, mse: 1144.722973, mean_q: 22.139710, mean_eps: 0.836650
  27357/150000: episode: 274, duration: 0.584s, episode steps:  85, steps per second: 146, episode reward: -76.356, mean reward: -0.898 [-100.000,  7.930], mean action: 1.541 [0.000, 3.000],  loss: 17.007650, mse: 1145.894212, mean_q: 22.568228, mean_eps: 0.836116
  27516/150000: episode: 275, duration: 1.050s, episode steps: 159, steps per second: 151, episode reward: -64.543, mean reward: -0.406 [-100.000, 14.413], mean action: 1.572 [0.000, 3.000],  loss: 18.364101, mse: 1173.364911, mean_q: 21.811112, mean_eps: 0.835384
  27607/150000: episode: 276, duration: 0.616s, episode steps:  91, steps per second: 148, episode reward: -71.476, mean reward: -0.785 [-100.000,  7.385], mean action: 1.582 [0.000, 3.000],  loss: 17.476683, mse: 1153.117336, mean_q: 22.806464, mean_eps: 0.834634
  27729/150000: episode: 277, duration: 0.817s, episode steps: 122, steps per second: 149, episode reward: -57.000, mean reward: -0.467 [-100.000, 22.961], mean action: 1.500 [0.000, 3.000],  loss: 22.901827, mse: 1172.526494, mean_q: 22.723807, mean_eps: 0.833995
  27871/150000: episode: 278, duration: 0.925s, episode steps: 142, steps per second: 153, episode reward: -71.358, mean reward: -0.503 [-100.000, 54.178], mean action: 1.775 [0.000, 3.000],  loss: 14.282446, mse: 1177.065403, mean_q: 21.956778, mean_eps: 0.833203
  27968/150000: episode: 279, duration: 0.672s, episode steps:  97, steps per second: 144, episode reward: -103.717, mean reward: -1.069 [-100.000, 11.906], mean action: 1.464 [0.000, 3.000],  loss: 24.523698, mse: 1165.256841, mean_q: 23.110421, mean_eps: 0.832486
  28035/150000: episode: 280, duration: 0.453s, episode steps:  67, steps per second: 148, episode reward: -90.313, mean reward: -1.348 [-100.000, 16.721], mean action: 1.328 [0.000, 3.000],  loss: 15.667593, mse: 1199.263488, mean_q: 23.383410, mean_eps: 0.831994
  28132/150000: episode: 281, duration: 0.641s, episode steps:  97, steps per second: 151, episode reward: -111.857, mean reward: -1.153 [-100.000,  8.001], mean action: 1.381 [0.000, 3.000],  loss: 18.703528, mse: 1259.446809, mean_q: 22.243329, mean_eps: 0.831502
  28215/150000: episode: 282, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -140.051, mean reward: -1.687 [-100.000, 12.764], mean action: 1.313 [0.000, 3.000],  loss: 12.971735, mse: 1214.284967, mean_q: 23.980109, mean_eps: 0.830962
  28311/150000: episode: 283, duration: 0.668s, episode steps:  96, steps per second: 144, episode reward: -86.731, mean reward: -0.903 [-100.000,  9.231], mean action: 1.781 [0.000, 3.000],  loss: 20.390601, mse: 1239.053701, mean_q: 23.106224, mean_eps: 0.830425
  28395/150000: episode: 284, duration: 0.580s, episode steps:  84, steps per second: 145, episode reward: -66.344, mean reward: -0.790 [-100.000, 20.482], mean action: 1.548 [0.000, 3.000],  loss: 24.506968, mse: 1241.992831, mean_q: 23.408067, mean_eps: 0.829885
  28507/150000: episode: 285, duration: 0.770s, episode steps: 112, steps per second: 145, episode reward: -129.458, mean reward: -1.156 [-100.000,  6.831], mean action: 1.705 [0.000, 3.000],  loss: 16.240123, mse: 1243.321536, mean_q: 22.838054, mean_eps: 0.829297
  28612/150000: episode: 286, duration: 0.746s, episode steps: 105, steps per second: 141, episode reward: -98.358, mean reward: -0.937 [-100.000,  8.586], mean action: 1.619 [0.000, 3.000],  loss: 20.722237, mse: 1222.936338, mean_q: 22.420833, mean_eps: 0.828646
  28731/150000: episode: 287, duration: 0.789s, episode steps: 119, steps per second: 151, episode reward: -191.057, mean reward: -1.606 [-100.000,  9.161], mean action: 1.588 [0.000, 3.000],  loss: 16.046964, mse: 1251.615907, mean_q: 22.706756, mean_eps: 0.827974
  28834/150000: episode: 288, duration: 0.681s, episode steps: 103, steps per second: 151, episode reward: -162.500, mean reward: -1.578 [-100.000,  6.369], mean action: 1.437 [0.000, 3.000],  loss: 19.208193, mse: 1255.815521, mean_q: 23.077855, mean_eps: 0.827308
  28897/150000: episode: 289, duration: 0.458s, episode steps:  63, steps per second: 138, episode reward: -68.456, mean reward: -1.087 [-100.000, 11.461], mean action: 1.587 [0.000, 3.000],  loss: 26.068985, mse: 1272.741735, mean_q: 21.611617, mean_eps: 0.826810
  28989/150000: episode: 290, duration: 0.620s, episode steps:  92, steps per second: 148, episode reward: -118.418, mean reward: -1.287 [-100.000, 14.121], mean action: 1.620 [0.000, 3.000],  loss: 17.600014, mse: 1246.913982, mean_q: 23.107418, mean_eps: 0.826345
  29102/150000: episode: 291, duration: 0.738s, episode steps: 113, steps per second: 153, episode reward: -102.598, mean reward: -0.908 [-100.000, 14.409], mean action: 1.398 [0.000, 3.000],  loss: 17.996707, mse: 1239.681795, mean_q: 22.935878, mean_eps: 0.825730
  29164/150000: episode: 292, duration: 0.416s, episode steps:  62, steps per second: 149, episode reward: -90.231, mean reward: -1.455 [-100.000,  7.220], mean action: 1.129 [0.000, 3.000],  loss: 16.178266, mse: 1255.568700, mean_q: 22.873363, mean_eps: 0.825205
  29265/150000: episode: 293, duration: 0.737s, episode steps: 101, steps per second: 137, episode reward: -106.911, mean reward: -1.059 [-100.000, 18.424], mean action: 1.515 [0.000, 3.000],  loss: 16.397175, mse: 1256.852574, mean_q: 22.530863, mean_eps: 0.824716
  29343/150000: episode: 294, duration: 0.632s, episode steps:  78, steps per second: 123, episode reward: -67.430, mean reward: -0.864 [-100.000,  7.744], mean action: 1.654 [0.000, 3.000],  loss: 12.999702, mse: 1276.360676, mean_q: 22.918645, mean_eps: 0.824179
  29426/150000: episode: 295, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -118.598, mean reward: -1.429 [-100.000, 24.235], mean action: 1.373 [0.000, 3.000],  loss: 28.140610, mse: 1261.038152, mean_q: 23.314781, mean_eps: 0.823696
  29550/150000: episode: 296, duration: 0.874s, episode steps: 124, steps per second: 142, episode reward: -73.943, mean reward: -0.596 [-100.000,  6.234], mean action: 1.500 [0.000, 3.000],  loss: 17.645592, mse: 1232.341430, mean_q: 23.035490, mean_eps: 0.823075
  29621/150000: episode: 297, duration: 0.522s, episode steps:  71, steps per second: 136, episode reward: -97.550, mean reward: -1.374 [-100.000, 34.450], mean action: 1.493 [0.000, 3.000],  loss: 14.893620, mse: 1246.170124, mean_q: 23.292394, mean_eps: 0.822490
  29739/150000: episode: 298, duration: 0.794s, episode steps: 118, steps per second: 149, episode reward: -149.417, mean reward: -1.266 [-100.000, 11.324], mean action: 1.593 [0.000, 3.000],  loss: 19.685049, mse: 1257.387519, mean_q: 23.284177, mean_eps: 0.821923
  29871/150000: episode: 299, duration: 0.909s, episode steps: 132, steps per second: 145, episode reward: -46.194, mean reward: -0.350 [-100.000, 17.936], mean action: 1.598 [0.000, 3.000],  loss: 15.840938, mse: 1237.595348, mean_q: 23.848133, mean_eps: 0.821173
  29990/150000: episode: 300, duration: 0.775s, episode steps: 119, steps per second: 153, episode reward: -21.748, mean reward: -0.183 [-100.000, 14.759], mean action: 1.546 [0.000, 3.000],  loss: 15.031338, mse: 1256.643091, mean_q: 22.630704, mean_eps: 0.820420
  30091/150000: episode: 301, duration: 0.656s, episode steps: 101, steps per second: 154, episode reward: -109.991, mean reward: -1.089 [-100.000,  9.957], mean action: 1.673 [0.000, 3.000],  loss: 23.981404, mse: 1240.848741, mean_q: 22.466950, mean_eps: 0.819760
  30166/150000: episode: 302, duration: 0.525s, episode steps:  75, steps per second: 143, episode reward: -67.846, mean reward: -0.905 [-100.000, 11.144], mean action: 1.507 [0.000, 3.000],  loss: 22.011641, mse: 1253.516021, mean_q: 23.280373, mean_eps: 0.819232
  30268/150000: episode: 303, duration: 0.682s, episode steps: 102, steps per second: 150, episode reward: -233.739, mean reward: -2.292 [-100.000,  5.022], mean action: 1.667 [0.000, 3.000],  loss: 16.907025, mse: 1245.205110, mean_q: 23.320185, mean_eps: 0.818701
  30363/150000: episode: 304, duration: 0.622s, episode steps:  95, steps per second: 153, episode reward: -101.213, mean reward: -1.065 [-100.000, 10.725], mean action: 1.653 [0.000, 3.000],  loss: 17.385722, mse: 1242.684735, mean_q: 23.101402, mean_eps: 0.818110
  30431/150000: episode: 305, duration: 0.441s, episode steps:  68, steps per second: 154, episode reward: -70.252, mean reward: -1.033 [-100.000,  6.399], mean action: 1.206 [0.000, 3.000],  loss: 14.412133, mse: 1234.329391, mean_q: 23.763559, mean_eps: 0.817621
  30543/150000: episode: 306, duration: 0.852s, episode steps: 112, steps per second: 131, episode reward: -109.317, mean reward: -0.976 [-100.000, 10.776], mean action: 1.768 [0.000, 3.000],  loss: 22.804570, mse: 1227.375925, mean_q: 22.972202, mean_eps: 0.817081
  30628/150000: episode: 307, duration: 0.741s, episode steps:  85, steps per second: 115, episode reward: -75.741, mean reward: -0.891 [-100.000, 12.169], mean action: 1.482 [0.000, 3.000],  loss: 24.253665, mse: 1248.023676, mean_q: 22.416712, mean_eps: 0.816490
  30713/150000: episode: 308, duration: 0.647s, episode steps:  85, steps per second: 131, episode reward: -97.489, mean reward: -1.147 [-100.000,  9.871], mean action: 1.518 [0.000, 3.000],  loss: 19.395661, mse: 1296.358145, mean_q: 22.692482, mean_eps: 0.815980
  30817/150000: episode: 309, duration: 0.801s, episode steps: 104, steps per second: 130, episode reward: -101.815, mean reward: -0.979 [-100.000, 10.360], mean action: 1.788 [0.000, 3.000],  loss: 17.812539, mse: 1277.470166, mean_q: 23.073621, mean_eps: 0.815413
  30968/150000: episode: 310, duration: 1.413s, episode steps: 151, steps per second: 107, episode reward: -120.839, mean reward: -0.800 [-100.000,  3.525], mean action: 1.715 [0.000, 3.000],  loss: 20.414723, mse: 1269.685340, mean_q: 22.704601, mean_eps: 0.814648
  31065/150000: episode: 311, duration: 1.205s, episode steps:  97, steps per second:  81, episode reward: -47.159, mean reward: -0.486 [-100.000,  7.670], mean action: 1.598 [0.000, 3.000],  loss: 19.474866, mse: 1284.956448, mean_q: 22.309031, mean_eps: 0.813904
  31154/150000: episode: 312, duration: 0.937s, episode steps:  89, steps per second:  95, episode reward: -193.459, mean reward: -2.174 [-100.000,  8.988], mean action: 1.472 [0.000, 3.000],  loss: 12.600768, mse: 1272.915752, mean_q: 23.537120, mean_eps: 0.813346
  31273/150000: episode: 313, duration: 1.453s, episode steps: 119, steps per second:  82, episode reward: -108.455, mean reward: -0.911 [-100.000,  8.788], mean action: 1.613 [0.000, 3.000],  loss: 20.209507, mse: 1300.371108, mean_q: 23.126717, mean_eps: 0.812722
  31345/150000: episode: 314, duration: 0.577s, episode steps:  72, steps per second: 125, episode reward: -58.475, mean reward: -0.812 [-100.000, 10.587], mean action: 1.639 [0.000, 3.000],  loss: 23.014351, mse: 1332.625388, mean_q: 22.834515, mean_eps: 0.812149
  31432/150000: episode: 315, duration: 0.836s, episode steps:  87, steps per second: 104, episode reward: -52.018, mean reward: -0.598 [-100.000, 11.562], mean action: 1.644 [0.000, 3.000],  loss: 15.723873, mse: 1305.669985, mean_q: 23.775746, mean_eps: 0.811672
  31530/150000: episode: 316, duration: 0.803s, episode steps:  98, steps per second: 122, episode reward: -50.451, mean reward: -0.515 [-100.000, 55.177], mean action: 1.306 [0.000, 3.000],  loss: 21.567849, mse: 1313.197413, mean_q: 23.011209, mean_eps: 0.811117
  31631/150000: episode: 317, duration: 0.761s, episode steps: 101, steps per second: 133, episode reward: -111.277, mean reward: -1.102 [-100.000,  6.232], mean action: 1.634 [0.000, 3.000],  loss: 14.908486, mse: 1322.062252, mean_q: 23.715419, mean_eps: 0.810520
  31710/150000: episode: 318, duration: 0.763s, episode steps:  79, steps per second: 103, episode reward: -123.039, mean reward: -1.557 [-100.000,  5.946], mean action: 1.671 [0.000, 3.000],  loss: 21.357576, mse: 1290.588342, mean_q: 23.172566, mean_eps: 0.809980
  31832/150000: episode: 319, duration: 0.922s, episode steps: 122, steps per second: 132, episode reward: -103.385, mean reward: -0.847 [-100.000, 22.533], mean action: 1.566 [0.000, 3.000],  loss: 17.169540, mse: 1290.694551, mean_q: 23.934939, mean_eps: 0.809377
  31955/150000: episode: 320, duration: 0.922s, episode steps: 123, steps per second: 133, episode reward: -82.336, mean reward: -0.669 [-100.000,  9.662], mean action: 1.634 [0.000, 3.000],  loss: 15.742453, mse: 1327.962556, mean_q: 23.362599, mean_eps: 0.808642
  32027/150000: episode: 321, duration: 0.549s, episode steps:  72, steps per second: 131, episode reward: -243.074, mean reward: -3.376 [-100.000, 92.280], mean action: 1.583 [0.000, 3.000],  loss: 13.947975, mse: 1330.888467, mean_q: 23.802023, mean_eps: 0.808057
  32108/150000: episode: 322, duration: 0.715s, episode steps:  81, steps per second: 113, episode reward: -21.729, mean reward: -0.268 [-100.000, 87.795], mean action: 1.469 [0.000, 3.000],  loss: 18.124654, mse: 1353.989656, mean_q: 23.324199, mean_eps: 0.807598
  32220/150000: episode: 323, duration: 0.907s, episode steps: 112, steps per second: 124, episode reward: -84.122, mean reward: -0.751 [-100.000,  5.395], mean action: 1.446 [0.000, 3.000],  loss: 20.594465, mse: 1335.413219, mean_q: 23.854747, mean_eps: 0.807019
  32340/150000: episode: 324, duration: 0.964s, episode steps: 120, steps per second: 124, episode reward: -129.780, mean reward: -1.081 [-100.000,  8.395], mean action: 1.383 [0.000, 3.000],  loss: 16.318001, mse: 1342.859816, mean_q: 23.315760, mean_eps: 0.806323
  32474/150000: episode: 325, duration: 1.034s, episode steps: 134, steps per second: 130, episode reward:  0.083, mean reward:  0.001 [-100.000, 54.596], mean action: 1.410 [0.000, 3.000],  loss: 18.365696, mse: 1392.923933, mean_q: 23.321151, mean_eps: 0.805561
  32618/150000: episode: 326, duration: 1.371s, episode steps: 144, steps per second: 105, episode reward: -73.134, mean reward: -0.508 [-100.000,  7.486], mean action: 1.507 [0.000, 3.000],  loss: 17.545306, mse: 1346.518596, mean_q: 23.231337, mean_eps: 0.804727
  32702/150000: episode: 327, duration: 0.789s, episode steps:  84, steps per second: 107, episode reward: -54.306, mean reward: -0.646 [-100.000, 19.589], mean action: 1.524 [0.000, 3.000],  loss: 16.074787, mse: 1353.948374, mean_q: 23.168639, mean_eps: 0.804043
  32828/150000: episode: 328, duration: 1.162s, episode steps: 126, steps per second: 108, episode reward: -108.864, mean reward: -0.864 [-100.000,  7.899], mean action: 1.508 [0.000, 3.000],  loss: 22.322493, mse: 1327.146179, mean_q: 23.142480, mean_eps: 0.803413
  32906/150000: episode: 329, duration: 0.669s, episode steps:  78, steps per second: 117, episode reward: -74.349, mean reward: -0.953 [-100.000,  6.855], mean action: 1.679 [0.000, 3.000],  loss: 19.281985, mse: 1347.511339, mean_q: 23.843620, mean_eps: 0.802801
  33022/150000: episode: 330, duration: 1.023s, episode steps: 116, steps per second: 113, episode reward: -108.942, mean reward: -0.939 [-100.000,  8.313], mean action: 1.543 [0.000, 3.000],  loss: 22.307260, mse: 1369.870763, mean_q: 24.164689, mean_eps: 0.802219
  33163/150000: episode: 331, duration: 0.983s, episode steps: 141, steps per second: 143, episode reward: -66.017, mean reward: -0.468 [-100.000, 24.379], mean action: 1.610 [0.000, 3.000],  loss: 18.911074, mse: 1420.722318, mean_q: 25.574707, mean_eps: 0.801448
  33264/150000: episode: 332, duration: 0.710s, episode steps: 101, steps per second: 142, episode reward: -156.082, mean reward: -1.545 [-100.000, 23.256], mean action: 1.337 [0.000, 3.000],  loss: 12.029145, mse: 1427.158204, mean_q: 25.935337, mean_eps: 0.800722
  33402/150000: episode: 333, duration: 0.930s, episode steps: 138, steps per second: 148, episode reward: -86.228, mean reward: -0.625 [-100.000,  6.900], mean action: 1.522 [0.000, 3.000],  loss: 18.784745, mse: 1424.053819, mean_q: 25.362747, mean_eps: 0.800005
  33545/150000: episode: 334, duration: 0.980s, episode steps: 143, steps per second: 146, episode reward: -44.200, mean reward: -0.309 [-100.000, 48.365], mean action: 1.497 [0.000, 3.000],  loss: 17.649891, mse: 1420.538087, mean_q: 24.856837, mean_eps: 0.799162
  33615/150000: episode: 335, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: -80.435, mean reward: -1.149 [-100.000,  7.286], mean action: 1.229 [0.000, 3.000],  loss: 21.160567, mse: 1410.411012, mean_q: 26.363269, mean_eps: 0.798523
  33715/150000: episode: 336, duration: 0.661s, episode steps: 100, steps per second: 151, episode reward: -127.539, mean reward: -1.275 [-100.000,  6.006], mean action: 1.430 [0.000, 3.000],  loss: 17.209867, mse: 1438.191185, mean_q: 25.788317, mean_eps: 0.798013
  33820/150000: episode: 337, duration: 0.703s, episode steps: 105, steps per second: 149, episode reward: -186.966, mean reward: -1.781 [-100.000,  2.082], mean action: 1.600 [0.000, 3.000],  loss: 13.801248, mse: 1429.271406, mean_q: 25.965999, mean_eps: 0.797398
  33883/150000: episode: 338, duration: 0.455s, episode steps:  63, steps per second: 138, episode reward: -67.042, mean reward: -1.064 [-100.000,  9.320], mean action: 1.667 [0.000, 3.000],  loss: 21.892094, mse: 1435.154054, mean_q: 25.324066, mean_eps: 0.796894
  34041/150000: episode: 339, duration: 1.055s, episode steps: 158, steps per second: 150, episode reward: -111.039, mean reward: -0.703 [-100.000, 10.456], mean action: 1.525 [0.000, 3.000],  loss: 21.481364, mse: 1448.402887, mean_q: 25.195498, mean_eps: 0.796231
  34123/150000: episode: 340, duration: 0.530s, episode steps:  82, steps per second: 155, episode reward: -52.958, mean reward: -0.646 [-100.000,  8.039], mean action: 1.720 [0.000, 3.000],  loss: 17.613406, mse: 1429.790454, mean_q: 26.631557, mean_eps: 0.795511
  34230/150000: episode: 341, duration: 0.762s, episode steps: 107, steps per second: 140, episode reward: -81.937, mean reward: -0.766 [-100.000, 11.862], mean action: 1.664 [0.000, 3.000],  loss: 26.145540, mse: 1417.660404, mean_q: 25.961980, mean_eps: 0.794944
  34368/150000: episode: 342, duration: 0.921s, episode steps: 138, steps per second: 150, episode reward: -68.274, mean reward: -0.495 [-100.000, 40.763], mean action: 1.370 [0.000, 3.000],  loss: 15.836446, mse: 1412.704238, mean_q: 26.034234, mean_eps: 0.794209
  34464/150000: episode: 343, duration: 0.632s, episode steps:  96, steps per second: 152, episode reward: -189.904, mean reward: -1.978 [-100.000,  4.040], mean action: 1.698 [0.000, 3.000],  loss: 22.528073, mse: 1422.343987, mean_q: 25.840413, mean_eps: 0.793507
  34571/150000: episode: 344, duration: 0.773s, episode steps: 107, steps per second: 138, episode reward: -100.141, mean reward: -0.936 [-100.000,  6.312], mean action: 1.336 [0.000, 3.000],  loss: 20.453421, mse: 1446.031118, mean_q: 26.045952, mean_eps: 0.792898
  34668/150000: episode: 345, duration: 0.633s, episode steps:  97, steps per second: 153, episode reward: -133.318, mean reward: -1.374 [-100.000,  6.619], mean action: 1.443 [0.000, 3.000],  loss: 32.094982, mse: 1437.701012, mean_q: 25.769366, mean_eps: 0.792286
  34781/150000: episode: 346, duration: 0.747s, episode steps: 113, steps per second: 151, episode reward: -116.941, mean reward: -1.035 [-100.000,  6.171], mean action: 1.628 [0.000, 3.000],  loss: 19.031931, mse: 1446.531978, mean_q: 25.129877, mean_eps: 0.791656
  34906/150000: episode: 347, duration: 0.892s, episode steps: 125, steps per second: 140, episode reward: -50.733, mean reward: -0.406 [-100.000, 14.948], mean action: 1.752 [0.000, 3.000],  loss: 20.085422, mse: 1448.459268, mean_q: 26.203081, mean_eps: 0.790942
  34993/150000: episode: 348, duration: 0.573s, episode steps:  87, steps per second: 152, episode reward: -60.270, mean reward: -0.693 [-100.000, 16.618], mean action: 1.563 [0.000, 3.000],  loss: 12.512671, mse: 1420.267271, mean_q: 26.560727, mean_eps: 0.790306
  35103/150000: episode: 349, duration: 0.708s, episode steps: 110, steps per second: 155, episode reward: -86.729, mean reward: -0.788 [-100.000, 12.566], mean action: 1.518 [0.000, 3.000],  loss: 16.316117, mse: 1528.489462, mean_q: 26.589325, mean_eps: 0.789715
  35228/150000: episode: 350, duration: 0.871s, episode steps: 125, steps per second: 144, episode reward: -151.868, mean reward: -1.215 [-100.000,  6.162], mean action: 1.384 [0.000, 3.000],  loss: 20.673353, mse: 1511.938635, mean_q: 26.351041, mean_eps: 0.789010
  35317/150000: episode: 351, duration: 0.597s, episode steps:  89, steps per second: 149, episode reward: -90.218, mean reward: -1.014 [-100.000, 10.637], mean action: 1.573 [0.000, 3.000],  loss: 13.184530, mse: 1524.687223, mean_q: 26.193521, mean_eps: 0.788368
  35379/150000: episode: 352, duration: 0.408s, episode steps:  62, steps per second: 152, episode reward: -126.722, mean reward: -2.044 [-100.000, 10.675], mean action: 1.548 [0.000, 3.000],  loss: 25.178402, mse: 1534.928709, mean_q: 26.274314, mean_eps: 0.787915
  35505/150000: episode: 353, duration: 0.884s, episode steps: 126, steps per second: 142, episode reward: -121.012, mean reward: -0.960 [-100.000,  9.994], mean action: 1.556 [0.000, 3.000],  loss: 14.727058, mse: 1543.293229, mean_q: 27.698587, mean_eps: 0.787351
  35612/150000: episode: 354, duration: 0.732s, episode steps: 107, steps per second: 146, episode reward: -77.751, mean reward: -0.727 [-100.000, 21.512], mean action: 1.449 [0.000, 3.000],  loss: 16.390723, mse: 1556.944321, mean_q: 27.294446, mean_eps: 0.786652
  35735/150000: episode: 355, duration: 0.815s, episode steps: 123, steps per second: 151, episode reward: -195.270, mean reward: -1.588 [-100.000,  3.136], mean action: 1.455 [0.000, 3.000],  loss: 28.647717, mse: 1523.301884, mean_q: 26.999802, mean_eps: 0.785962
  35846/150000: episode: 356, duration: 0.764s, episode steps: 111, steps per second: 145, episode reward: -136.072, mean reward: -1.226 [-100.000,  8.657], mean action: 1.387 [0.000, 3.000],  loss: 21.417379, mse: 1509.639301, mean_q: 27.733224, mean_eps: 0.785260
  35949/150000: episode: 357, duration: 0.681s, episode steps: 103, steps per second: 151, episode reward: -82.292, mean reward: -0.799 [-100.000, 11.039], mean action: 1.544 [0.000, 3.000],  loss: 24.226315, mse: 1532.566840, mean_q: 26.833220, mean_eps: 0.784618
  36067/150000: episode: 358, duration: 0.785s, episode steps: 118, steps per second: 150, episode reward: -62.649, mean reward: -0.531 [-100.000, 10.972], mean action: 1.703 [0.000, 3.000],  loss: 19.886313, mse: 1564.807584, mean_q: 27.912261, mean_eps: 0.783955
  36175/150000: episode: 359, duration: 0.753s, episode steps: 108, steps per second: 144, episode reward: -85.278, mean reward: -0.790 [-100.000, 22.642], mean action: 1.602 [0.000, 3.000],  loss: 19.676711, mse: 1557.894562, mean_q: 27.919233, mean_eps: 0.783277
  36245/150000: episode: 360, duration: 0.471s, episode steps:  70, steps per second: 149, episode reward: -70.242, mean reward: -1.003 [-100.000,  8.999], mean action: 1.414 [0.000, 3.000],  loss: 19.396633, mse: 1576.017437, mean_q: 28.991784, mean_eps: 0.782743
  36348/150000: episode: 361, duration: 0.673s, episode steps: 103, steps per second: 153, episode reward: -178.654, mean reward: -1.735 [-100.000,  7.738], mean action: 1.573 [0.000, 3.000],  loss: 14.101366, mse: 1588.209793, mean_q: 28.090417, mean_eps: 0.782224
  36443/150000: episode: 362, duration: 0.638s, episode steps:  95, steps per second: 149, episode reward: -78.258, mean reward: -0.824 [-100.000,  7.182], mean action: 1.484 [0.000, 3.000],  loss: 20.041514, mse: 1573.560442, mean_q: 27.602469, mean_eps: 0.781630
  36521/150000: episode: 363, duration: 0.555s, episode steps:  78, steps per second: 141, episode reward: -132.050, mean reward: -1.693 [-100.000,  6.366], mean action: 1.385 [0.000, 3.000],  loss: 19.485988, mse: 1573.941979, mean_q: 28.171024, mean_eps: 0.781111
  36615/150000: episode: 364, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -117.422, mean reward: -1.249 [-100.000,  5.882], mean action: 1.287 [0.000, 3.000],  loss: 19.391839, mse: 1572.064188, mean_q: 28.554822, mean_eps: 0.780595
  36694/150000: episode: 365, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -72.906, mean reward: -0.923 [-100.000,  7.205], mean action: 1.443 [0.000, 3.000],  loss: 11.534562, mse: 1567.819824, mean_q: 27.517647, mean_eps: 0.780076
  36789/150000: episode: 366, duration: 0.644s, episode steps:  95, steps per second: 148, episode reward: -87.798, mean reward: -0.924 [-100.000, 11.478], mean action: 1.716 [0.000, 3.000],  loss: 14.329520, mse: 1572.300393, mean_q: 27.832791, mean_eps: 0.779554
  36904/150000: episode: 367, duration: 0.771s, episode steps: 115, steps per second: 149, episode reward: -15.933, mean reward: -0.139 [-100.000, 13.363], mean action: 1.652 [0.000, 3.000],  loss: 19.075355, mse: 1584.080773, mean_q: 28.831191, mean_eps: 0.778924
  37015/150000: episode: 368, duration: 0.759s, episode steps: 111, steps per second: 146, episode reward: -55.382, mean reward: -0.499 [-100.000, 12.366], mean action: 1.586 [0.000, 3.000],  loss: 14.970092, mse: 1575.150967, mean_q: 27.946080, mean_eps: 0.778246
  37082/150000: episode: 369, duration: 0.484s, episode steps:  67, steps per second: 139, episode reward: -95.988, mean reward: -1.433 [-100.000,  5.652], mean action: 1.403 [0.000, 3.000],  loss: 9.847090, mse: 1642.859180, mean_q: 29.876687, mean_eps: 0.777712
  37161/150000: episode: 370, duration: 0.539s, episode steps:  79, steps per second: 147, episode reward: -84.982, mean reward: -1.076 [-100.000, 23.686], mean action: 1.722 [0.000, 3.000],  loss: 15.560088, mse: 1600.578072, mean_q: 29.220561, mean_eps: 0.777274
  37228/150000: episode: 371, duration: 0.452s, episode steps:  67, steps per second: 148, episode reward: -54.061, mean reward: -0.807 [-100.000, 13.067], mean action: 1.388 [0.000, 3.000],  loss: 19.516907, mse: 1602.355600, mean_q: 29.227418, mean_eps: 0.776836
  37319/150000: episode: 372, duration: 0.599s, episode steps:  91, steps per second: 152, episode reward: -123.903, mean reward: -1.362 [-100.000,  5.769], mean action: 1.593 [0.000, 3.000],  loss: 23.403028, mse: 1638.119711, mean_q: 29.166938, mean_eps: 0.776362
  37426/150000: episode: 373, duration: 0.774s, episode steps: 107, steps per second: 138, episode reward: -99.445, mean reward: -0.929 [-100.000,  8.969], mean action: 1.645 [0.000, 3.000],  loss: 17.463375, mse: 1627.695712, mean_q: 29.164403, mean_eps: 0.775768
  37511/150000: episode: 374, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -105.406, mean reward: -1.240 [-100.000, 12.475], mean action: 1.412 [0.000, 3.000],  loss: 14.399687, mse: 1603.520976, mean_q: 28.991145, mean_eps: 0.775192
  37627/150000: episode: 375, duration: 0.804s, episode steps: 116, steps per second: 144, episode reward: -147.771, mean reward: -1.274 [-100.000,  4.547], mean action: 1.586 [0.000, 3.000],  loss: 20.487982, mse: 1636.737593, mean_q: 28.854834, mean_eps: 0.774589
  37741/150000: episode: 376, duration: 0.809s, episode steps: 114, steps per second: 141, episode reward: -41.418, mean reward: -0.363 [-100.000, 12.559], mean action: 1.368 [0.000, 3.000],  loss: 16.188876, mse: 1652.910612, mean_q: 28.565752, mean_eps: 0.773899
  37852/150000: episode: 377, duration: 0.766s, episode steps: 111, steps per second: 145, episode reward: -98.148, mean reward: -0.884 [-100.000,  7.624], mean action: 1.676 [0.000, 3.000],  loss: 21.087473, mse: 1665.498847, mean_q: 29.327500, mean_eps: 0.773224
  37946/150000: episode: 378, duration: 0.633s, episode steps:  94, steps per second: 149, episode reward: -98.224, mean reward: -1.045 [-100.000,  4.290], mean action: 1.426 [0.000, 3.000],  loss: 23.694966, mse: 1676.899082, mean_q: 28.767724, mean_eps: 0.772609
  38066/150000: episode: 379, duration: 0.829s, episode steps: 120, steps per second: 145, episode reward: -57.447, mean reward: -0.479 [-100.000, 12.286], mean action: 1.767 [0.000, 3.000],  loss: 14.917938, mse: 1735.715177, mean_q: 30.082591, mean_eps: 0.771967
  38201/150000: episode: 380, duration: 0.919s, episode steps: 135, steps per second: 147, episode reward: -60.278, mean reward: -0.447 [-100.000, 12.284], mean action: 1.356 [0.000, 3.000],  loss: 15.486738, mse: 1743.632227, mean_q: 30.550769, mean_eps: 0.771202
  38286/150000: episode: 381, duration: 0.682s, episode steps:  85, steps per second: 125, episode reward: -111.061, mean reward: -1.307 [-100.000,  5.657], mean action: 1.553 [0.000, 3.000],  loss: 12.429860, mse: 1741.165144, mean_q: 31.833433, mean_eps: 0.770542
  38360/150000: episode: 382, duration: 0.632s, episode steps:  74, steps per second: 117, episode reward: -93.856, mean reward: -1.268 [-100.000,  7.371], mean action: 1.257 [0.000, 3.000],  loss: 21.068934, mse: 1721.575332, mean_q: 29.917731, mean_eps: 0.770065
  38464/150000: episode: 383, duration: 0.854s, episode steps: 104, steps per second: 122, episode reward: -59.660, mean reward: -0.574 [-100.000, 13.252], mean action: 1.385 [0.000, 3.000],  loss: 19.061121, mse: 1726.228485, mean_q: 30.996608, mean_eps: 0.769531
  38566/150000: episode: 384, duration: 0.787s, episode steps: 102, steps per second: 130, episode reward: -77.031, mean reward: -0.755 [-100.000,  7.114], mean action: 1.676 [0.000, 3.000],  loss: 16.987689, mse: 1743.875089, mean_q: 30.051741, mean_eps: 0.768913
  38687/150000: episode: 385, duration: 0.959s, episode steps: 121, steps per second: 126, episode reward: -134.753, mean reward: -1.114 [-100.000,  8.149], mean action: 1.537 [0.000, 3.000],  loss: 16.689683, mse: 1748.428499, mean_q: 29.468236, mean_eps: 0.768244
  38794/150000: episode: 386, duration: 0.962s, episode steps: 107, steps per second: 111, episode reward: -103.260, mean reward: -0.965 [-100.000, 16.120], mean action: 1.645 [0.000, 3.000],  loss: 15.048734, mse: 1727.439273, mean_q: 30.543387, mean_eps: 0.767560
  38866/150000: episode: 387, duration: 0.585s, episode steps:  72, steps per second: 123, episode reward: -18.572, mean reward: -0.258 [-100.000, 17.688], mean action: 1.528 [0.000, 3.000],  loss: 14.573832, mse: 1727.843692, mean_q: 30.832103, mean_eps: 0.767023
  38939/150000: episode: 388, duration: 0.597s, episode steps:  73, steps per second: 122, episode reward: -86.735, mean reward: -1.188 [-100.000, 12.604], mean action: 1.630 [0.000, 3.000],  loss: 17.648635, mse: 1794.512700, mean_q: 30.537543, mean_eps: 0.766588
  39090/150000: episode: 389, duration: 1.242s, episode steps: 151, steps per second: 122, episode reward: -66.359, mean reward: -0.439 [-100.000, 15.190], mean action: 1.550 [0.000, 3.000],  loss: 16.693203, mse: 1789.058725, mean_q: 31.434250, mean_eps: 0.765916
  39164/150000: episode: 390, duration: 0.597s, episode steps:  74, steps per second: 124, episode reward: -39.009, mean reward: -0.527 [-100.000, 10.913], mean action: 1.486 [0.000, 3.000],  loss: 15.937278, mse: 1803.833631, mean_q: 30.027582, mean_eps: 0.765241
  39272/150000: episode: 391, duration: 0.759s, episode steps: 108, steps per second: 142, episode reward: -138.622, mean reward: -1.284 [-100.000,  3.550], mean action: 1.481 [0.000, 3.000],  loss: 13.735391, mse: 1768.522577, mean_q: 30.541121, mean_eps: 0.764695
  39370/150000: episode: 392, duration: 0.668s, episode steps:  98, steps per second: 147, episode reward: -128.406, mean reward: -1.310 [-100.000,  7.372], mean action: 1.531 [0.000, 3.000],  loss: 11.118722, mse: 1777.392937, mean_q: 31.094220, mean_eps: 0.764077
  39462/150000: episode: 393, duration: 0.675s, episode steps:  92, steps per second: 136, episode reward: -85.086, mean reward: -0.925 [-100.000,  6.953], mean action: 1.467 [0.000, 3.000],  loss: 13.935621, mse: 1772.282560, mean_q: 30.494831, mean_eps: 0.763507
  39581/150000: episode: 394, duration: 0.805s, episode steps: 119, steps per second: 148, episode reward: -146.616, mean reward: -1.232 [-100.000, 10.678], mean action: 1.571 [0.000, 3.000],  loss: 15.032029, mse: 1805.946787, mean_q: 31.156576, mean_eps: 0.762874
  39682/150000: episode: 395, duration: 0.740s, episode steps: 101, steps per second: 137, episode reward: -40.267, mean reward: -0.399 [-100.000, 22.950], mean action: 1.475 [0.000, 3.000],  loss: 13.115720, mse: 1792.318561, mean_q: 30.843913, mean_eps: 0.762214
  39758/150000: episode: 396, duration: 0.650s, episode steps:  76, steps per second: 117, episode reward: -275.204, mean reward: -3.621 [-100.000, 124.245], mean action: 1.618 [0.000, 3.000],  loss: 16.525344, mse: 1785.617904, mean_q: 31.254345, mean_eps: 0.761683
  39862/150000: episode: 397, duration: 0.841s, episode steps: 104, steps per second: 124, episode reward: -115.724, mean reward: -1.113 [-100.000,  7.318], mean action: 1.712 [0.000, 3.000],  loss: 18.079384, mse: 1822.870872, mean_q: 31.649054, mean_eps: 0.761143
  39970/150000: episode: 398, duration: 0.856s, episode steps: 108, steps per second: 126, episode reward: -88.080, mean reward: -0.816 [-100.000, 10.543], mean action: 1.426 [0.000, 3.000],  loss: 17.410660, mse: 1802.790324, mean_q: 30.137641, mean_eps: 0.760507
  40047/150000: episode: 399, duration: 0.599s, episode steps:  77, steps per second: 128, episode reward: -86.453, mean reward: -1.123 [-100.000, 16.439], mean action: 1.532 [0.000, 3.000],  loss: 18.758958, mse: 1846.317204, mean_q: 30.818956, mean_eps: 0.759952
  40131/150000: episode: 400, duration: 0.648s, episode steps:  84, steps per second: 130, episode reward: -64.558, mean reward: -0.769 [-100.000,  6.824], mean action: 1.655 [0.000, 3.000],  loss: 14.671865, mse: 1852.550523, mean_q: 31.266705, mean_eps: 0.759469
  40197/150000: episode: 401, duration: 0.485s, episode steps:  66, steps per second: 136, episode reward: -78.550, mean reward: -1.190 [-100.000,  6.955], mean action: 1.439 [0.000, 3.000],  loss: 14.589885, mse: 1887.028648, mean_q: 31.164537, mean_eps: 0.759019
  40274/150000: episode: 402, duration: 0.588s, episode steps:  77, steps per second: 131, episode reward: -43.390, mean reward: -0.564 [-100.000, 18.030], mean action: 1.506 [0.000, 3.000],  loss: 14.430065, mse: 1863.286789, mean_q: 31.827892, mean_eps: 0.758590
  40342/150000: episode: 403, duration: 0.545s, episode steps:  68, steps per second: 125, episode reward: -88.495, mean reward: -1.301 [-100.000,  7.171], mean action: 1.500 [0.000, 3.000],  loss: 11.884215, mse: 1832.698485, mean_q: 30.765158, mean_eps: 0.758155
  40412/150000: episode: 404, duration: 0.504s, episode steps:  70, steps per second: 139, episode reward: -109.603, mean reward: -1.566 [-100.000,  4.167], mean action: 1.714 [0.000, 3.000],  loss: 13.913189, mse: 1871.648282, mean_q: 31.630406, mean_eps: 0.757741
  40476/150000: episode: 405, duration: 0.441s, episode steps:  64, steps per second: 145, episode reward: -56.885, mean reward: -0.889 [-100.000, 21.840], mean action: 1.344 [0.000, 3.000],  loss: 14.263597, mse: 1857.010170, mean_q: 30.918160, mean_eps: 0.757339
  40568/150000: episode: 406, duration: 0.662s, episode steps:  92, steps per second: 139, episode reward: -109.834, mean reward: -1.194 [-100.000, 19.865], mean action: 1.500 [0.000, 3.000],  loss: 17.180130, mse: 1850.375141, mean_q: 31.559111, mean_eps: 0.756871
  40666/150000: episode: 407, duration: 0.755s, episode steps:  98, steps per second: 130, episode reward: -429.980, mean reward: -4.388 [-100.000, 84.823], mean action: 1.469 [0.000, 3.000],  loss: 15.325078, mse: 1908.909323, mean_q: 31.480290, mean_eps: 0.756301
  40762/150000: episode: 408, duration: 0.679s, episode steps:  96, steps per second: 141, episode reward: -84.647, mean reward: -0.882 [-100.000,  8.165], mean action: 1.500 [0.000, 3.000],  loss: 15.071567, mse: 1870.328171, mean_q: 31.304105, mean_eps: 0.755719
  40863/150000: episode: 409, duration: 0.834s, episode steps: 101, steps per second: 121, episode reward: -159.189, mean reward: -1.576 [-100.000, 10.241], mean action: 1.317 [0.000, 3.000],  loss: 10.381674, mse: 1886.488987, mean_q: 31.413596, mean_eps: 0.755128
  40947/150000: episode: 410, duration: 0.631s, episode steps:  84, steps per second: 133, episode reward: -64.841, mean reward: -0.772 [-100.000, 16.242], mean action: 1.667 [0.000, 3.000],  loss: 16.615850, mse: 1892.854534, mean_q: 31.367728, mean_eps: 0.754573
  41017/150000: episode: 411, duration: 0.495s, episode steps:  70, steps per second: 141, episode reward: -80.139, mean reward: -1.145 [-100.000, 17.924], mean action: 1.471 [0.000, 3.000],  loss: 13.225364, mse: 1843.069779, mean_q: 30.545593, mean_eps: 0.754111
  41092/150000: episode: 412, duration: 0.513s, episode steps:  75, steps per second: 146, episode reward: -69.115, mean reward: -0.922 [-100.000, 13.809], mean action: 1.720 [0.000, 3.000],  loss: 12.060994, mse: 1941.912340, mean_q: 32.159974, mean_eps: 0.753676
  41193/150000: episode: 413, duration: 0.763s, episode steps: 101, steps per second: 132, episode reward: -66.015, mean reward: -0.654 [-100.000, 16.864], mean action: 1.634 [0.000, 3.000],  loss: 16.467680, mse: 1918.789185, mean_q: 31.205278, mean_eps: 0.753148
  41275/150000: episode: 414, duration: 0.598s, episode steps:  82, steps per second: 137, episode reward: -122.321, mean reward: -1.492 [-100.000,  8.846], mean action: 1.634 [0.000, 3.000],  loss: 17.745092, mse: 1937.037319, mean_q: 30.910038, mean_eps: 0.752599
  41399/150000: episode: 415, duration: 0.851s, episode steps: 124, steps per second: 146, episode reward: -6.559, mean reward: -0.053 [-100.000, 11.911], mean action: 1.540 [0.000, 3.000],  loss: 18.188105, mse: 1941.253451, mean_q: 31.425278, mean_eps: 0.751981
  41516/150000: episode: 416, duration: 0.857s, episode steps: 117, steps per second: 136, episode reward: -129.636, mean reward: -1.108 [-100.000,  5.735], mean action: 1.513 [0.000, 3.000],  loss: 14.099106, mse: 1932.984110, mean_q: 31.406065, mean_eps: 0.751258
  41614/150000: episode: 417, duration: 0.754s, episode steps:  98, steps per second: 130, episode reward: -125.655, mean reward: -1.282 [-100.000, 10.118], mean action: 1.653 [0.000, 3.000],  loss: 12.420949, mse: 1937.621732, mean_q: 30.905975, mean_eps: 0.750613
  41731/150000: episode: 418, duration: 0.989s, episode steps: 117, steps per second: 118, episode reward: -105.688, mean reward: -0.903 [-100.000,  7.710], mean action: 1.299 [0.000, 3.000],  loss: 15.806131, mse: 1938.008982, mean_q: 32.806765, mean_eps: 0.749968
  41817/150000: episode: 419, duration: 0.957s, episode steps:  86, steps per second:  90, episode reward: -35.912, mean reward: -0.418 [-100.000, 12.561], mean action: 1.453 [0.000, 3.000],  loss: 13.190129, mse: 1900.022545, mean_q: 31.627125, mean_eps: 0.749359
  41945/150000: episode: 420, duration: 1.089s, episode steps: 128, steps per second: 118, episode reward: -135.481, mean reward: -1.058 [-100.000, 10.031], mean action: 1.594 [0.000, 3.000],  loss: 14.857036, mse: 1906.262859, mean_q: 31.329998, mean_eps: 0.748717
  42017/150000: episode: 421, duration: 0.597s, episode steps:  72, steps per second: 121, episode reward: -49.252, mean reward: -0.684 [-100.000, 17.772], mean action: 1.375 [0.000, 3.000],  loss: 16.385218, mse: 1939.212828, mean_q: 32.314756, mean_eps: 0.748117
  42123/150000: episode: 422, duration: 0.833s, episode steps: 106, steps per second: 127, episode reward: -124.127, mean reward: -1.171 [-100.000,  9.291], mean action: 1.330 [0.000, 3.000],  loss: 22.459173, mse: 2006.851971, mean_q: 32.119661, mean_eps: 0.747583
  42202/150000: episode: 423, duration: 0.629s, episode steps:  79, steps per second: 126, episode reward: -93.633, mean reward: -1.185 [-100.000, 12.921], mean action: 1.646 [0.000, 3.000],  loss: 18.565851, mse: 2068.551583, mean_q: 31.082115, mean_eps: 0.747028
  42281/150000: episode: 424, duration: 0.625s, episode steps:  79, steps per second: 126, episode reward: -23.251, mean reward: -0.294 [-100.000, 22.890], mean action: 1.595 [0.000, 3.000],  loss: 12.590703, mse: 2054.969710, mean_q: 30.948448, mean_eps: 0.746554
  42390/150000: episode: 425, duration: 0.803s, episode steps: 109, steps per second: 136, episode reward: -118.761, mean reward: -1.090 [-100.000,  4.422], mean action: 1.376 [0.000, 3.000],  loss: 14.781384, mse: 2007.197397, mean_q: 31.862961, mean_eps: 0.745990
  42475/150000: episode: 426, duration: 0.653s, episode steps:  85, steps per second: 130, episode reward: -124.722, mean reward: -1.467 [-100.000, 11.939], mean action: 1.612 [0.000, 3.000],  loss: 16.329946, mse: 2075.014502, mean_q: 33.098192, mean_eps: 0.745408
  42582/150000: episode: 427, duration: 0.828s, episode steps: 107, steps per second: 129, episode reward: -73.236, mean reward: -0.684 [-100.000,  7.613], mean action: 1.551 [0.000, 3.000],  loss: 12.888593, mse: 2031.031618, mean_q: 32.255750, mean_eps: 0.744832
  42675/150000: episode: 428, duration: 0.673s, episode steps:  93, steps per second: 138, episode reward: -98.087, mean reward: -1.055 [-100.000,  7.283], mean action: 1.656 [0.000, 3.000],  loss: 10.545974, mse: 2036.906982, mean_q: 33.102335, mean_eps: 0.744232
  42773/150000: episode: 429, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -71.332, mean reward: -0.728 [-100.000,  9.924], mean action: 1.480 [0.000, 3.000],  loss: 20.966750, mse: 2030.249558, mean_q: 31.703475, mean_eps: 0.743659
  42902/150000: episode: 430, duration: 1.073s, episode steps: 129, steps per second: 120, episode reward: -65.673, mean reward: -0.509 [-100.000,  6.463], mean action: 1.434 [0.000, 3.000],  loss: 14.221817, mse: 2022.891015, mean_q: 32.337124, mean_eps: 0.742978
  43018/150000: episode: 431, duration: 0.848s, episode steps: 116, steps per second: 137, episode reward: -9.001, mean reward: -0.078 [-100.000, 82.421], mean action: 1.509 [0.000, 3.000],  loss: 14.937635, mse: 2013.429403, mean_q: 31.978434, mean_eps: 0.742243
  43137/150000: episode: 432, duration: 0.869s, episode steps: 119, steps per second: 137, episode reward: -88.344, mean reward: -0.742 [-100.000,  6.578], mean action: 1.345 [0.000, 3.000],  loss: 17.075277, mse: 2066.929356, mean_q: 33.938310, mean_eps: 0.741538
  43226/150000: episode: 433, duration: 0.679s, episode steps:  89, steps per second: 131, episode reward: -101.137, mean reward: -1.136 [-100.000,  7.578], mean action: 1.292 [0.000, 3.000],  loss: 13.597479, mse: 2103.262029, mean_q: 33.686531, mean_eps: 0.740914
  43300/150000: episode: 434, duration: 0.525s, episode steps:  74, steps per second: 141, episode reward: -35.378, mean reward: -0.478 [-100.000, 23.158], mean action: 1.500 [0.000, 3.000],  loss: 22.539416, mse: 2135.466594, mean_q: 33.088230, mean_eps: 0.740425
  43389/150000: episode: 435, duration: 0.652s, episode steps:  89, steps per second: 136, episode reward: -89.348, mean reward: -1.004 [-100.000,  7.879], mean action: 1.461 [0.000, 3.000],  loss: 13.173948, mse: 2071.464567, mean_q: 34.247809, mean_eps: 0.739936
  43488/150000: episode: 436, duration: 0.705s, episode steps:  99, steps per second: 140, episode reward: -75.456, mean reward: -0.762 [-100.000,  9.643], mean action: 1.333 [0.000, 3.000],  loss: 12.621380, mse: 2106.369022, mean_q: 33.570668, mean_eps: 0.739372
  43558/150000: episode: 437, duration: 0.483s, episode steps:  70, steps per second: 145, episode reward: -65.777, mean reward: -0.940 [-100.000,  8.528], mean action: 1.657 [0.000, 3.000],  loss: 15.643101, mse: 2132.602733, mean_q: 33.465741, mean_eps: 0.738865
  43679/150000: episode: 438, duration: 0.868s, episode steps: 121, steps per second: 139, episode reward: -43.340, mean reward: -0.358 [-100.000, 11.903], mean action: 1.785 [0.000, 3.000],  loss: 16.910357, mse: 2109.553163, mean_q: 33.677227, mean_eps: 0.738292
  43795/150000: episode: 439, duration: 0.812s, episode steps: 116, steps per second: 143, episode reward: -81.193, mean reward: -0.700 [-100.000, 19.543], mean action: 1.638 [0.000, 3.000],  loss: 12.669031, mse: 2085.039352, mean_q: 33.658217, mean_eps: 0.737581
  43870/150000: episode: 440, duration: 0.593s, episode steps:  75, steps per second: 126, episode reward: -82.102, mean reward: -1.095 [-100.000,  9.556], mean action: 1.547 [0.000, 3.000],  loss: 16.651162, mse: 2080.431566, mean_q: 33.387131, mean_eps: 0.737008
  43961/150000: episode: 441, duration: 0.682s, episode steps:  91, steps per second: 133, episode reward: -59.191, mean reward: -0.650 [-100.000, 17.784], mean action: 1.681 [0.000, 3.000],  loss: 17.470155, mse: 2092.717311, mean_q: 33.216860, mean_eps: 0.736510
  44056/150000: episode: 442, duration: 0.674s, episode steps:  95, steps per second: 141, episode reward: -95.453, mean reward: -1.005 [-100.000, 15.163], mean action: 1.211 [0.000, 3.000],  loss: 11.851622, mse: 2109.324084, mean_q: 33.165560, mean_eps: 0.735952
  44133/150000: episode: 443, duration: 0.553s, episode steps:  77, steps per second: 139, episode reward: -33.671, mean reward: -0.437 [-100.000, 12.477], mean action: 1.545 [0.000, 3.000],  loss: 12.982185, mse: 2158.107224, mean_q: 34.375477, mean_eps: 0.735436
  44208/150000: episode: 444, duration: 0.523s, episode steps:  75, steps per second: 143, episode reward: -118.118, mean reward: -1.575 [-100.000, 11.920], mean action: 1.453 [0.000, 3.000],  loss: 17.470227, mse: 2149.813451, mean_q: 34.176319, mean_eps: 0.734980
  44291/150000: episode: 445, duration: 0.685s, episode steps:  83, steps per second: 121, episode reward: -57.525, mean reward: -0.693 [-100.000, 14.495], mean action: 1.530 [0.000, 3.000],  loss: 13.597948, mse: 2162.806449, mean_q: 33.749602, mean_eps: 0.734506
  44393/150000: episode: 446, duration: 0.743s, episode steps: 102, steps per second: 137, episode reward: -70.179, mean reward: -0.688 [-100.000, 10.170], mean action: 1.510 [0.000, 3.000],  loss: 14.611109, mse: 2173.136901, mean_q: 34.366429, mean_eps: 0.733951
  44506/150000: episode: 447, duration: 0.799s, episode steps: 113, steps per second: 141, episode reward: -80.186, mean reward: -0.710 [-100.000, 16.853], mean action: 1.584 [0.000, 3.000],  loss: 9.996758, mse: 2151.661535, mean_q: 33.699545, mean_eps: 0.733306
  44574/150000: episode: 448, duration: 0.550s, episode steps:  68, steps per second: 124, episode reward: -58.598, mean reward: -0.862 [-100.000,  6.907], mean action: 1.662 [0.000, 3.000],  loss: 12.631388, mse: 2196.430921, mean_q: 33.099550, mean_eps: 0.732763
  44673/150000: episode: 449, duration: 0.774s, episode steps:  99, steps per second: 128, episode reward: -80.910, mean reward: -0.817 [-100.000, 12.264], mean action: 1.626 [0.000, 3.000],  loss: 12.858428, mse: 2190.852701, mean_q: 33.523770, mean_eps: 0.732262
  44767/150000: episode: 450, duration: 0.662s, episode steps:  94, steps per second: 142, episode reward: -78.020, mean reward: -0.830 [-100.000,  9.083], mean action: 1.532 [0.000, 3.000],  loss: 15.788119, mse: 2162.496368, mean_q: 33.228724, mean_eps: 0.731683
  44835/150000: episode: 451, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -107.779, mean reward: -1.585 [-100.000,  9.732], mean action: 1.559 [0.000, 3.000],  loss: 11.645876, mse: 2179.983799, mean_q: 34.208588, mean_eps: 0.731197
  44903/150000: episode: 452, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: -82.531, mean reward: -1.214 [-100.000, 11.011], mean action: 1.353 [0.000, 3.000],  loss: 12.313695, mse: 2150.214460, mean_q: 33.935469, mean_eps: 0.730789
  44984/150000: episode: 453, duration: 0.586s, episode steps:  81, steps per second: 138, episode reward: -109.694, mean reward: -1.354 [-100.000,  5.329], mean action: 1.506 [0.000, 3.000],  loss: 20.313324, mse: 2181.625889, mean_q: 33.990041, mean_eps: 0.730342
  45086/150000: episode: 454, duration: 0.725s, episode steps: 102, steps per second: 141, episode reward: -159.800, mean reward: -1.567 [-100.000, 28.687], mean action: 1.598 [0.000, 3.000],  loss: 11.750964, mse: 2185.237093, mean_q: 33.809318, mean_eps: 0.729793
  45155/150000: episode: 455, duration: 0.534s, episode steps:  69, steps per second: 129, episode reward: -112.660, mean reward: -1.633 [-100.000,  8.585], mean action: 1.435 [0.000, 3.000],  loss: 19.768455, mse: 2111.497180, mean_q: 31.771215, mean_eps: 0.729280
  46155/150000: episode: 456, duration: 9.147s, episode steps: 1000, steps per second: 109, episode reward: -26.177, mean reward: -0.026 [-23.676, 30.373], mean action: 1.613 [0.000, 3.000],  loss: 15.235736, mse: 2190.262387, mean_q: 34.045366, mean_eps: 0.726073
  46287/150000: episode: 457, duration: 1.053s, episode steps: 132, steps per second: 125, episode reward: -77.526, mean reward: -0.587 [-100.000, 31.801], mean action: 1.371 [0.000, 3.000],  loss: 15.637145, mse: 2205.892586, mean_q: 33.671133, mean_eps: 0.722677
  46352/150000: episode: 458, duration: 0.496s, episode steps:  65, steps per second: 131, episode reward: -94.351, mean reward: -1.452 [-100.000,  5.277], mean action: 1.415 [0.000, 3.000],  loss: 19.686420, mse: 2268.194802, mean_q: 33.797448, mean_eps: 0.722086
  46466/150000: episode: 459, duration: 0.828s, episode steps: 114, steps per second: 138, episode reward: -71.345, mean reward: -0.626 [-100.000, 11.387], mean action: 1.482 [0.000, 3.000],  loss: 13.929175, mse: 2240.026544, mean_q: 34.980378, mean_eps: 0.721549
  46585/150000: episode: 460, duration: 0.826s, episode steps: 119, steps per second: 144, episode reward: -126.834, mean reward: -1.066 [-100.000, 17.772], mean action: 1.345 [0.000, 3.000],  loss: 16.316840, mse: 2248.324224, mean_q: 33.883283, mean_eps: 0.720850
  46648/150000: episode: 461, duration: 0.438s, episode steps:  63, steps per second: 144, episode reward: -34.215, mean reward: -0.543 [-100.000,  8.220], mean action: 1.460 [0.000, 3.000],  loss: 18.904114, mse: 2256.463848, mean_q: 35.983092, mean_eps: 0.720304
  46756/150000: episode: 462, duration: 0.795s, episode steps: 108, steps per second: 136, episode reward: -74.735, mean reward: -0.692 [-100.000,  6.304], mean action: 1.398 [0.000, 3.000],  loss: 14.645859, mse: 2280.165914, mean_q: 34.350133, mean_eps: 0.719791
  46821/150000: episode: 463, duration: 0.466s, episode steps:  65, steps per second: 140, episode reward: -42.468, mean reward: -0.653 [-100.000, 17.265], mean action: 1.631 [0.000, 3.000],  loss: 18.077790, mse: 2327.659499, mean_q: 35.168909, mean_eps: 0.719272
  46927/150000: episode: 464, duration: 0.762s, episode steps: 106, steps per second: 139, episode reward: -83.680, mean reward: -0.789 [-100.000, 17.206], mean action: 1.604 [0.000, 3.000],  loss: 11.395956, mse: 2222.345034, mean_q: 34.626674, mean_eps: 0.718759
  47006/150000: episode: 465, duration: 0.606s, episode steps:  79, steps per second: 130, episode reward: -73.828, mean reward: -0.935 [-100.000,  9.630], mean action: 1.557 [0.000, 3.000],  loss: 16.289687, mse: 2194.054117, mean_q: 34.290806, mean_eps: 0.718204
  47091/150000: episode: 466, duration: 0.612s, episode steps:  85, steps per second: 139, episode reward: -13.528, mean reward: -0.159 [-100.000, 23.892], mean action: 1.529 [0.000, 3.000],  loss: 17.308867, mse: 2238.899028, mean_q: 33.871694, mean_eps: 0.717712
  47161/150000: episode: 467, duration: 0.508s, episode steps:  70, steps per second: 138, episode reward: -105.014, mean reward: -1.500 [-100.000, 16.710], mean action: 1.586 [0.000, 3.000],  loss: 17.125126, mse: 2305.844331, mean_q: 33.783527, mean_eps: 0.717247
  47255/150000: episode: 468, duration: 0.666s, episode steps:  94, steps per second: 141, episode reward: -120.273, mean reward: -1.279 [-100.000,  6.324], mean action: 1.532 [0.000, 3.000],  loss: 13.976343, mse: 2324.026166, mean_q: 34.481175, mean_eps: 0.716755
  47319/150000: episode: 469, duration: 0.486s, episode steps:  64, steps per second: 132, episode reward: -92.607, mean reward: -1.447 [-100.000, 18.905], mean action: 1.625 [0.000, 3.000],  loss: 19.055429, mse: 2263.043152, mean_q: 34.739993, mean_eps: 0.716281
  47461/150000: episode: 470, duration: 1.017s, episode steps: 142, steps per second: 140, episode reward: -116.682, mean reward: -0.822 [-100.000, 21.058], mean action: 1.451 [0.000, 3.000],  loss: 19.609677, mse: 2283.202500, mean_q: 34.948022, mean_eps: 0.715663
  47550/150000: episode: 471, duration: 0.643s, episode steps:  89, steps per second: 138, episode reward: -87.596, mean reward: -0.984 [-100.000, 11.120], mean action: 1.652 [0.000, 3.000],  loss: 21.665787, mse: 2259.322028, mean_q: 34.015103, mean_eps: 0.714970
  47625/150000: episode: 472, duration: 0.564s, episode steps:  75, steps per second: 133, episode reward: -69.327, mean reward: -0.924 [-100.000,  6.866], mean action: 1.333 [0.000, 3.000],  loss: 14.985716, mse: 2266.971403, mean_q: 34.936959, mean_eps: 0.714478
  47710/150000: episode: 473, duration: 0.631s, episode steps:  85, steps per second: 135, episode reward: -37.837, mean reward: -0.445 [-100.000, 10.728], mean action: 1.776 [0.000, 3.000],  loss: 14.006497, mse: 2308.807254, mean_q: 33.541362, mean_eps: 0.713998
  47779/150000: episode: 474, duration: 0.519s, episode steps:  69, steps per second: 133, episode reward: -39.594, mean reward: -0.574 [-100.000,  7.285], mean action: 1.507 [0.000, 3.000],  loss: 12.950639, mse: 2279.863731, mean_q: 35.010090, mean_eps: 0.713536
  47867/150000: episode: 475, duration: 0.676s, episode steps:  88, steps per second: 130, episode reward: -73.000, mean reward: -0.830 [-100.000, 17.057], mean action: 1.591 [0.000, 3.000],  loss: 17.248631, mse: 2286.925803, mean_q: 33.987172, mean_eps: 0.713065
  48002/150000: episode: 476, duration: 0.973s, episode steps: 135, steps per second: 139, episode reward: -82.442, mean reward: -0.611 [-100.000, 19.418], mean action: 1.430 [0.000, 3.000],  loss: 19.204353, mse: 2258.987425, mean_q: 34.158398, mean_eps: 0.712396
  48113/150000: episode: 477, duration: 0.814s, episode steps: 111, steps per second: 136, episode reward: -115.230, mean reward: -1.038 [-100.000, 11.521], mean action: 1.541 [0.000, 3.000],  loss: 20.531906, mse: 2289.726945, mean_q: 34.318848, mean_eps: 0.711658
  48227/150000: episode: 478, duration: 0.870s, episode steps: 114, steps per second: 131, episode reward: -99.062, mean reward: -0.869 [-100.000, 21.591], mean action: 1.544 [0.000, 3.000],  loss: 13.076294, mse: 2291.695502, mean_q: 34.149713, mean_eps: 0.710983
  48726/150000: episode: 479, duration: 4.248s, episode steps: 499, steps per second: 117, episode reward: -83.268, mean reward: -0.167 [-100.000, 64.275], mean action: 1.503 [0.000, 3.000],  loss: 12.837277, mse: 2338.137098, mean_q: 34.740294, mean_eps: 0.709144
  48837/150000: episode: 480, duration: 0.775s, episode steps: 111, steps per second: 143, episode reward: -80.730, mean reward: -0.727 [-100.000, 11.345], mean action: 1.541 [0.000, 3.000],  loss: 19.601082, mse: 2328.463897, mean_q: 35.642973, mean_eps: 0.707314
  48934/150000: episode: 481, duration: 0.780s, episode steps:  97, steps per second: 124, episode reward: -50.742, mean reward: -0.523 [-100.000, 16.646], mean action: 1.670 [0.000, 3.000],  loss: 12.361011, mse: 2282.143556, mean_q: 35.261870, mean_eps: 0.706690
  49018/150000: episode: 482, duration: 0.625s, episode steps:  84, steps per second: 134, episode reward: -92.233, mean reward: -1.098 [-100.000, 12.314], mean action: 1.750 [0.000, 3.000],  loss: 15.848054, mse: 2276.019783, mean_q: 34.436678, mean_eps: 0.706147
  49103/150000: episode: 483, duration: 0.629s, episode steps:  85, steps per second: 135, episode reward: -11.774, mean reward: -0.139 [-100.000, 19.503], mean action: 1.824 [0.000, 3.000],  loss: 13.088729, mse: 2387.529413, mean_q: 35.465548, mean_eps: 0.705640
  49168/150000: episode: 484, duration: 0.494s, episode steps:  65, steps per second: 131, episode reward: -79.105, mean reward: -1.217 [-100.000, 16.913], mean action: 1.677 [0.000, 3.000],  loss: 25.709117, mse: 2313.657012, mean_q: 34.764147, mean_eps: 0.705190
  49254/150000: episode: 485, duration: 0.679s, episode steps:  86, steps per second: 127, episode reward: -106.485, mean reward: -1.238 [-100.000,  9.976], mean action: 1.453 [0.000, 3.000],  loss: 17.619254, mse: 2372.786314, mean_q: 35.527079, mean_eps: 0.704737
  49357/150000: episode: 486, duration: 0.922s, episode steps: 103, steps per second: 112, episode reward: -97.403, mean reward: -0.946 [-100.000,  5.463], mean action: 1.534 [0.000, 3.000],  loss: 19.376236, mse: 2360.669912, mean_q: 35.887837, mean_eps: 0.704170
  49454/150000: episode: 487, duration: 0.820s, episode steps:  97, steps per second: 118, episode reward: 21.107, mean reward:  0.218 [-100.000, 74.147], mean action: 1.918 [0.000, 3.000],  loss: 10.655891, mse: 2396.114615, mean_q: 35.268290, mean_eps: 0.703570
  49550/150000: episode: 488, duration: 0.804s, episode steps:  96, steps per second: 119, episode reward: -86.677, mean reward: -0.903 [-100.000, 16.356], mean action: 1.344 [0.000, 3.000],  loss: 11.934476, mse: 2387.510863, mean_q: 36.522667, mean_eps: 0.702991
  49661/150000: episode: 489, duration: 0.902s, episode steps: 111, steps per second: 123, episode reward: -68.807, mean reward: -0.620 [-100.000,  7.840], mean action: 1.450 [0.000, 3.000],  loss: 22.860885, mse: 2340.643236, mean_q: 35.258910, mean_eps: 0.702370
  49748/150000: episode: 490, duration: 0.788s, episode steps:  87, steps per second: 110, episode reward: -54.013, mean reward: -0.621 [-100.000,  7.809], mean action: 1.667 [0.000, 3.000],  loss: 14.071907, mse: 2367.871894, mean_q: 35.556287, mean_eps: 0.701776
  49860/150000: episode: 491, duration: 0.892s, episode steps: 112, steps per second: 126, episode reward: -25.548, mean reward: -0.228 [-100.000, 19.027], mean action: 1.536 [0.000, 3.000],  loss: 14.920882, mse: 2385.694287, mean_q: 35.478940, mean_eps: 0.701179
  49991/150000: episode: 492, duration: 1.214s, episode steps: 131, steps per second: 108, episode reward: -95.097, mean reward: -0.726 [-100.000,  7.297], mean action: 1.595 [0.000, 3.000],  loss: 12.256821, mse: 2384.597444, mean_q: 35.680507, mean_eps: 0.700450
  50091/150000: episode: 493, duration: 1.002s, episode steps: 100, steps per second: 100, episode reward: -84.585, mean reward: -0.846 [-100.000,  6.231], mean action: 1.550 [0.000, 3.000],  loss: 11.913400, mse: 2437.431630, mean_q: 35.781854, mean_eps: 0.699757
  50171/150000: episode: 494, duration: 0.715s, episode steps:  80, steps per second: 112, episode reward: -43.572, mean reward: -0.545 [-100.000, 13.285], mean action: 1.688 [0.000, 3.000],  loss: 16.141722, mse: 2418.156015, mean_q: 35.571386, mean_eps: 0.699217
  50277/150000: episode: 495, duration: 0.921s, episode steps: 106, steps per second: 115, episode reward: -43.571, mean reward: -0.411 [-100.000, 31.897], mean action: 1.651 [0.000, 3.000],  loss: 11.645532, mse: 2454.744647, mean_q: 35.638190, mean_eps: 0.698659
  50370/150000: episode: 496, duration: 0.764s, episode steps:  93, steps per second: 122, episode reward: -64.077, mean reward: -0.689 [-100.000, 22.705], mean action: 1.634 [0.000, 3.000],  loss: 18.699403, mse: 2455.063382, mean_q: 35.795581, mean_eps: 0.698062
  50449/150000: episode: 497, duration: 0.696s, episode steps:  79, steps per second: 114, episode reward: -50.131, mean reward: -0.635 [-100.000, 19.240], mean action: 1.608 [0.000, 3.000],  loss: 10.346871, mse: 2465.683818, mean_q: 36.145278, mean_eps: 0.697546
  50522/150000: episode: 498, duration: 0.679s, episode steps:  73, steps per second: 108, episode reward: -51.591, mean reward: -0.707 [-100.000,  8.718], mean action: 1.397 [0.000, 3.000],  loss: 19.010667, mse: 2425.898882, mean_q: 36.524276, mean_eps: 0.697090
  50616/150000: episode: 499, duration: 0.871s, episode steps:  94, steps per second: 108, episode reward: -82.156, mean reward: -0.874 [-100.000,  5.531], mean action: 1.489 [0.000, 3.000],  loss: 16.963257, mse: 2459.782714, mean_q: 36.518127, mean_eps: 0.696589
  50703/150000: episode: 500, duration: 0.875s, episode steps:  87, steps per second:  99, episode reward: 27.718, mean reward:  0.319 [-100.000, 43.273], mean action: 1.552 [0.000, 3.000],  loss: 12.819362, mse: 2462.062267, mean_q: 37.721261, mean_eps: 0.696046
  50808/150000: episode: 501, duration: 1.001s, episode steps: 105, steps per second: 105, episode reward: -91.607, mean reward: -0.872 [-100.000, 13.691], mean action: 1.390 [0.000, 3.000],  loss: 12.451363, mse: 2436.352615, mean_q: 37.319592, mean_eps: 0.695470
  50918/150000: episode: 502, duration: 0.832s, episode steps: 110, steps per second: 132, episode reward: -116.232, mean reward: -1.057 [-100.000, 10.720], mean action: 1.391 [0.000, 3.000],  loss: 18.537882, mse: 2460.713226, mean_q: 35.426627, mean_eps: 0.694825
  51030/150000: episode: 503, duration: 0.828s, episode steps: 112, steps per second: 135, episode reward: -93.291, mean reward: -0.833 [-100.000,  9.895], mean action: 1.438 [0.000, 3.000],  loss: 13.865853, mse: 2453.318157, mean_q: 36.523147, mean_eps: 0.694159
  51141/150000: episode: 504, duration: 1.046s, episode steps: 111, steps per second: 106, episode reward: -51.929, mean reward: -0.468 [-100.000,  8.379], mean action: 1.604 [0.000, 3.000],  loss: 19.190628, mse: 2455.512717, mean_q: 36.182469, mean_eps: 0.693490
  51257/150000: episode: 505, duration: 0.965s, episode steps: 116, steps per second: 120, episode reward: -52.686, mean reward: -0.454 [-100.000, 20.894], mean action: 1.474 [0.000, 3.000],  loss: 16.733479, mse: 2474.501373, mean_q: 36.872277, mean_eps: 0.692809
  51378/150000: episode: 506, duration: 1.027s, episode steps: 121, steps per second: 118, episode reward: -84.779, mean reward: -0.701 [-100.000, 11.368], mean action: 1.545 [0.000, 3.000],  loss: 21.760153, mse: 2463.007098, mean_q: 37.521076, mean_eps: 0.692098
  51455/150000: episode: 507, duration: 0.694s, episode steps:  77, steps per second: 111, episode reward: -57.493, mean reward: -0.747 [-100.000, 13.845], mean action: 1.403 [0.000, 3.000],  loss: 15.572225, mse: 2465.502613, mean_q: 36.930243, mean_eps: 0.691504
  51571/150000: episode: 508, duration: 0.935s, episode steps: 116, steps per second: 124, episode reward: -34.192, mean reward: -0.295 [-100.000, 20.225], mean action: 1.664 [0.000, 3.000],  loss: 19.546106, mse: 2464.785060, mean_q: 36.398647, mean_eps: 0.690925
  51702/150000: episode: 509, duration: 1.340s, episode steps: 131, steps per second:  98, episode reward: -106.265, mean reward: -0.811 [-100.000, 18.290], mean action: 1.405 [0.000, 3.000],  loss: 12.294886, mse: 2478.788586, mean_q: 38.036567, mean_eps: 0.690184
  51827/150000: episode: 510, duration: 1.434s, episode steps: 125, steps per second:  87, episode reward: -101.838, mean reward: -0.815 [-100.000,  8.590], mean action: 1.528 [0.000, 3.000],  loss: 19.701972, mse: 2461.469463, mean_q: 36.542969, mean_eps: 0.689416
  51891/150000: episode: 511, duration: 0.632s, episode steps:  64, steps per second: 101, episode reward: -39.684, mean reward: -0.620 [-100.000, 20.684], mean action: 1.531 [0.000, 3.000],  loss: 14.297515, mse: 2428.898815, mean_q: 36.192309, mean_eps: 0.688849
  51995/150000: episode: 512, duration: 0.874s, episode steps: 104, steps per second: 119, episode reward: -53.501, mean reward: -0.514 [-100.000, 13.140], mean action: 1.663 [0.000, 3.000],  loss: 21.876387, mse: 2487.504505, mean_q: 38.337210, mean_eps: 0.688345
  52089/150000: episode: 513, duration: 0.735s, episode steps:  94, steps per second: 128, episode reward: -43.656, mean reward: -0.464 [-100.000, 16.311], mean action: 1.511 [0.000, 3.000],  loss: 20.375460, mse: 2507.382825, mean_q: 36.855945, mean_eps: 0.687751
  52218/150000: episode: 514, duration: 1.083s, episode steps: 129, steps per second: 119, episode reward: -7.671, mean reward: -0.059 [-100.000, 19.923], mean action: 1.581 [0.000, 3.000],  loss: 18.846468, mse: 2513.020433, mean_q: 38.175434, mean_eps: 0.687082
  52292/150000: episode: 515, duration: 0.545s, episode steps:  74, steps per second: 136, episode reward: -83.675, mean reward: -1.131 [-100.000,  8.275], mean action: 1.716 [0.000, 3.000],  loss: 12.286707, mse: 2585.790643, mean_q: 39.004424, mean_eps: 0.686473
  52372/150000: episode: 516, duration: 0.686s, episode steps:  80, steps per second: 117, episode reward: -86.697, mean reward: -1.084 [-100.000, 21.519], mean action: 1.500 [0.000, 3.000],  loss: 15.361546, mse: 2487.899692, mean_q: 36.941188, mean_eps: 0.686011
  52473/150000: episode: 517, duration: 0.832s, episode steps: 101, steps per second: 121, episode reward: 28.650, mean reward:  0.284 [-100.000, 82.164], mean action: 1.426 [0.000, 3.000],  loss: 14.523574, mse: 2545.080762, mean_q: 39.820480, mean_eps: 0.685468
  52564/150000: episode: 518, duration: 0.653s, episode steps:  91, steps per second: 139, episode reward: -50.882, mean reward: -0.559 [-100.000, 21.959], mean action: 1.582 [0.000, 3.000],  loss: 15.786136, mse: 2536.740682, mean_q: 38.954445, mean_eps: 0.684892
  52673/150000: episode: 519, duration: 0.808s, episode steps: 109, steps per second: 135, episode reward: -54.200, mean reward: -0.497 [-100.000,  8.039], mean action: 1.495 [0.000, 3.000],  loss: 13.829380, mse: 2550.803803, mean_q: 38.244722, mean_eps: 0.684292
  52772/150000: episode: 520, duration: 0.768s, episode steps:  99, steps per second: 129, episode reward: -97.743, mean reward: -0.987 [-100.000, 15.731], mean action: 1.495 [0.000, 3.000],  loss: 25.552267, mse: 2517.467015, mean_q: 38.690514, mean_eps: 0.683668
  52898/150000: episode: 521, duration: 0.904s, episode steps: 126, steps per second: 139, episode reward: -86.548, mean reward: -0.687 [-100.000,  8.679], mean action: 1.548 [0.000, 3.000],  loss: 18.178990, mse: 2543.750948, mean_q: 37.366551, mean_eps: 0.682993
  52991/150000: episode: 522, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward: -63.518, mean reward: -0.683 [-100.000, 11.861], mean action: 1.516 [0.000, 3.000],  loss: 16.797397, mse: 2499.799389, mean_q: 37.854309, mean_eps: 0.682336
  53117/150000: episode: 523, duration: 0.928s, episode steps: 126, steps per second: 136, episode reward: -73.033, mean reward: -0.580 [-100.000,  8.372], mean action: 1.730 [0.000, 3.000],  loss: 21.025135, mse: 2594.585025, mean_q: 38.946509, mean_eps: 0.681679
  53212/150000: episode: 524, duration: 0.657s, episode steps:  95, steps per second: 145, episode reward: -45.005, mean reward: -0.474 [-100.000,  9.557], mean action: 1.800 [0.000, 3.000],  loss: 14.943386, mse: 2552.408771, mean_q: 37.309367, mean_eps: 0.681016
  53314/150000: episode: 525, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -93.877, mean reward: -0.920 [-100.000, 11.109], mean action: 1.588 [0.000, 3.000],  loss: 22.019932, mse: 2560.383706, mean_q: 38.526120, mean_eps: 0.680425
  53424/150000: episode: 526, duration: 0.801s, episode steps: 110, steps per second: 137, episode reward: -53.456, mean reward: -0.486 [-100.000, 15.629], mean action: 1.500 [0.000, 3.000],  loss: 18.287428, mse: 2586.920799, mean_q: 39.153107, mean_eps: 0.679789
  53557/150000: episode: 527, duration: 0.908s, episode steps: 133, steps per second: 147, episode reward: -55.500, mean reward: -0.417 [-100.000, 19.002], mean action: 1.692 [0.000, 3.000],  loss: 22.031525, mse: 2545.026263, mean_q: 37.107271, mean_eps: 0.679060
  53678/150000: episode: 528, duration: 0.855s, episode steps: 121, steps per second: 141, episode reward: -45.657, mean reward: -0.377 [-100.000, 24.279], mean action: 1.645 [0.000, 3.000],  loss: 16.092292, mse: 2557.034382, mean_q: 38.769887, mean_eps: 0.678298
  53758/150000: episode: 529, duration: 0.562s, episode steps:  80, steps per second: 142, episode reward: -64.950, mean reward: -0.812 [-100.000, 13.604], mean action: 1.663 [0.000, 3.000],  loss: 21.089666, mse: 2535.829192, mean_q: 38.136845, mean_eps: 0.677695
  53870/150000: episode: 530, duration: 0.759s, episode steps: 112, steps per second: 148, episode reward: -94.807, mean reward: -0.846 [-100.000,  6.435], mean action: 1.429 [0.000, 3.000],  loss: 17.247170, mse: 2565.043193, mean_q: 38.675231, mean_eps: 0.677119
  53947/150000: episode: 531, duration: 0.535s, episode steps:  77, steps per second: 144, episode reward: -84.805, mean reward: -1.101 [-100.000, 17.567], mean action: 1.610 [0.000, 3.000],  loss: 17.043831, mse: 2565.275265, mean_q: 38.822735, mean_eps: 0.676552
  54082/150000: episode: 532, duration: 0.978s, episode steps: 135, steps per second: 138, episode reward: -86.142, mean reward: -0.638 [-100.000,  9.625], mean action: 1.526 [0.000, 3.000],  loss: 20.008671, mse: 2534.002309, mean_q: 38.972492, mean_eps: 0.675916
  54195/150000: episode: 533, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -69.761, mean reward: -0.617 [-100.000, 33.891], mean action: 1.761 [0.000, 3.000],  loss: 16.167539, mse: 2582.893842, mean_q: 39.186663, mean_eps: 0.675172
  54294/150000: episode: 534, duration: 0.760s, episode steps:  99, steps per second: 130, episode reward: -49.614, mean reward: -0.501 [-100.000, 16.206], mean action: 1.586 [0.000, 3.000],  loss: 15.199563, mse: 2579.075398, mean_q: 38.899494, mean_eps: 0.674536
  54407/150000: episode: 535, duration: 0.878s, episode steps: 113, steps per second: 129, episode reward: -117.431, mean reward: -1.039 [-100.000, 10.502], mean action: 1.549 [0.000, 3.000],  loss: 15.594783, mse: 2629.702192, mean_q: 40.068563, mean_eps: 0.673900
  54493/150000: episode: 536, duration: 0.612s, episode steps:  86, steps per second: 141, episode reward: -48.372, mean reward: -0.562 [-100.000, 15.768], mean action: 1.640 [0.000, 3.000],  loss: 13.821433, mse: 2602.876147, mean_q: 39.944213, mean_eps: 0.673303
  54604/150000: episode: 537, duration: 0.797s, episode steps: 111, steps per second: 139, episode reward: -54.423, mean reward: -0.490 [-100.000,  9.068], mean action: 1.640 [0.000, 3.000],  loss: 16.366868, mse: 2622.396363, mean_q: 39.188472, mean_eps: 0.672712
  54718/150000: episode: 538, duration: 0.771s, episode steps: 114, steps per second: 148, episode reward: -29.264, mean reward: -0.257 [-100.000, 12.616], mean action: 1.658 [0.000, 3.000],  loss: 21.884433, mse: 2564.225510, mean_q: 39.564895, mean_eps: 0.672037
  54796/150000: episode: 539, duration: 0.538s, episode steps:  78, steps per second: 145, episode reward: -63.763, mean reward: -0.817 [-100.000,  9.685], mean action: 1.744 [0.000, 3.000],  loss: 13.873778, mse: 2573.374012, mean_q: 39.313264, mean_eps: 0.671461
  54925/150000: episode: 540, duration: 0.909s, episode steps: 129, steps per second: 142, episode reward: -98.683, mean reward: -0.765 [-100.000, 12.221], mean action: 1.496 [0.000, 3.000],  loss: 17.854277, mse: 2599.397991, mean_q: 39.379215, mean_eps: 0.670840
  55009/150000: episode: 541, duration: 0.600s, episode steps:  84, steps per second: 140, episode reward: -105.146, mean reward: -1.252 [-100.000,  6.072], mean action: 1.440 [0.000, 3.000],  loss: 12.709538, mse: 2624.353597, mean_q: 40.334810, mean_eps: 0.670201
  55140/150000: episode: 542, duration: 0.880s, episode steps: 131, steps per second: 149, episode reward: -53.231, mean reward: -0.406 [-100.000,  8.456], mean action: 1.458 [0.000, 3.000],  loss: 15.922881, mse: 2659.443261, mean_q: 40.252424, mean_eps: 0.669556
  55219/150000: episode: 543, duration: 0.632s, episode steps:  79, steps per second: 125, episode reward: -46.468, mean reward: -0.588 [-100.000, 10.335], mean action: 1.570 [0.000, 3.000],  loss: 21.765179, mse: 2574.354149, mean_q: 38.777882, mean_eps: 0.668926
  55337/150000: episode: 544, duration: 0.893s, episode steps: 118, steps per second: 132, episode reward: -30.899, mean reward: -0.262 [-100.000, 17.163], mean action: 1.669 [0.000, 3.000],  loss: 16.897224, mse: 2696.065956, mean_q: 41.707023, mean_eps: 0.668335
  55477/150000: episode: 545, duration: 0.998s, episode steps: 140, steps per second: 140, episode reward: -28.756, mean reward: -0.205 [-100.000, 37.533], mean action: 1.614 [0.000, 3.000],  loss: 11.450028, mse: 2642.809412, mean_q: 41.032400, mean_eps: 0.667561
  55561/150000: episode: 546, duration: 0.684s, episode steps:  84, steps per second: 123, episode reward: -67.238, mean reward: -0.800 [-100.000, 10.593], mean action: 1.345 [0.000, 3.000],  loss: 10.867551, mse: 2665.023725, mean_q: 41.494369, mean_eps: 0.666889
  55654/150000: episode: 547, duration: 0.692s, episode steps:  93, steps per second: 134, episode reward: -117.827, mean reward: -1.267 [-100.000, 24.192], mean action: 1.613 [0.000, 3.000],  loss: 21.316441, mse: 2662.278252, mean_q: 40.635099, mean_eps: 0.666358
  55770/150000: episode: 548, duration: 0.868s, episode steps: 116, steps per second: 134, episode reward: -76.625, mean reward: -0.661 [-100.000, 17.315], mean action: 1.397 [0.000, 3.000],  loss: 14.460624, mse: 2724.363111, mean_q: 40.001884, mean_eps: 0.665731
  55879/150000: episode: 549, duration: 0.795s, episode steps: 109, steps per second: 137, episode reward: -133.780, mean reward: -1.227 [-100.000,  5.402], mean action: 1.450 [0.000, 3.000],  loss: 12.819883, mse: 2725.164045, mean_q: 41.419861, mean_eps: 0.665056
  55979/150000: episode: 550, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: -65.572, mean reward: -0.656 [-100.000,  9.300], mean action: 1.550 [0.000, 3.000],  loss: 16.782737, mse: 2643.461643, mean_q: 40.512215, mean_eps: 0.664429
  56065/150000: episode: 551, duration: 0.629s, episode steps:  86, steps per second: 137, episode reward: -45.323, mean reward: -0.527 [-100.000, 25.253], mean action: 1.465 [0.000, 3.000],  loss: 24.092061, mse: 2654.219064, mean_q: 40.423507, mean_eps: 0.663871
  56164/150000: episode: 552, duration: 0.707s, episode steps:  99, steps per second: 140, episode reward: -85.350, mean reward: -0.862 [-100.000,  6.125], mean action: 1.616 [0.000, 3.000],  loss: 17.832361, mse: 2614.876979, mean_q: 41.162420, mean_eps: 0.663316
  56248/150000: episode: 553, duration: 0.594s, episode steps:  84, steps per second: 142, episode reward: -69.107, mean reward: -0.823 [-100.000, 26.881], mean action: 1.774 [0.000, 3.000],  loss: 14.137027, mse: 2603.351023, mean_q: 40.274141, mean_eps: 0.662767
  56332/150000: episode: 554, duration: 0.579s, episode steps:  84, steps per second: 145, episode reward: -16.121, mean reward: -0.192 [-100.000, 10.332], mean action: 1.560 [0.000, 3.000],  loss: 20.908710, mse: 2631.554257, mean_q: 39.789568, mean_eps: 0.662263
  56441/150000: episode: 555, duration: 0.920s, episode steps: 109, steps per second: 118, episode reward: -46.442, mean reward: -0.426 [-100.000, 60.296], mean action: 1.578 [0.000, 3.000],  loss: 17.496275, mse: 2640.021742, mean_q: 41.337397, mean_eps: 0.661684
  56528/150000: episode: 556, duration: 0.652s, episode steps:  87, steps per second: 133, episode reward: -112.788, mean reward: -1.296 [-100.000,  6.423], mean action: 1.575 [0.000, 3.000],  loss: 20.842403, mse: 2633.928413, mean_q: 41.909393, mean_eps: 0.661096
  56778/150000: episode: 557, duration: 1.975s, episode steps: 250, steps per second: 127, episode reward: -195.869, mean reward: -0.783 [-100.000, 111.909], mean action: 1.560 [0.000, 3.000],  loss: 17.184901, mse: 2629.459770, mean_q: 40.775583, mean_eps: 0.660085
  56855/150000: episode: 558, duration: 0.590s, episode steps:  77, steps per second: 131, episode reward: -78.534, mean reward: -1.020 [-100.000,  6.479], mean action: 1.481 [0.000, 3.000],  loss: 15.642494, mse: 2603.903487, mean_q: 39.661272, mean_eps: 0.659104
  56939/150000: episode: 559, duration: 0.713s, episode steps:  84, steps per second: 118, episode reward: -30.446, mean reward: -0.362 [-100.000, 15.570], mean action: 1.595 [0.000, 3.000],  loss: 16.307886, mse: 2584.941344, mean_q: 40.450195, mean_eps: 0.658621
  57018/150000: episode: 560, duration: 0.571s, episode steps:  79, steps per second: 138, episode reward: -70.379, mean reward: -0.891 [-100.000,  7.060], mean action: 1.443 [0.000, 3.000],  loss: 18.323836, mse: 2661.762692, mean_q: 40.710515, mean_eps: 0.658132
  57086/150000: episode: 561, duration: 0.521s, episode steps:  68, steps per second: 131, episode reward: -46.243, mean reward: -0.680 [-100.000, 11.183], mean action: 1.735 [0.000, 3.000],  loss: 13.970842, mse: 2736.210905, mean_q: 42.653896, mean_eps: 0.657691
  57182/150000: episode: 562, duration: 0.849s, episode steps:  96, steps per second: 113, episode reward: -110.350, mean reward: -1.149 [-100.000, 17.682], mean action: 1.667 [0.000, 3.000],  loss: 16.101835, mse: 2731.396192, mean_q: 41.269370, mean_eps: 0.657199
  57289/150000: episode: 563, duration: 0.863s, episode steps: 107, steps per second: 124, episode reward: -36.931, mean reward: -0.345 [-100.000,  7.999], mean action: 1.720 [0.000, 3.000],  loss: 19.874335, mse: 2750.857789, mean_q: 41.762816, mean_eps: 0.656590
  57385/150000: episode: 564, duration: 0.657s, episode steps:  96, steps per second: 146, episode reward: -49.386, mean reward: -0.514 [-100.000,  7.686], mean action: 1.510 [0.000, 3.000],  loss: 21.347767, mse: 2710.872976, mean_q: 40.742617, mean_eps: 0.655981
  57517/150000: episode: 565, duration: 0.943s, episode steps: 132, steps per second: 140, episode reward: -5.816, mean reward: -0.044 [-100.000, 17.172], mean action: 1.636 [0.000, 3.000],  loss: 14.251024, mse: 2722.855369, mean_q: 41.060893, mean_eps: 0.655297
  57647/150000: episode: 566, duration: 0.910s, episode steps: 130, steps per second: 143, episode reward: -90.385, mean reward: -0.695 [-100.000,  9.943], mean action: 1.454 [0.000, 3.000],  loss: 13.989441, mse: 2773.614188, mean_q: 42.472284, mean_eps: 0.654511
  57765/150000: episode: 567, duration: 0.818s, episode steps: 118, steps per second: 144, episode reward: -102.622, mean reward: -0.870 [-100.000,  9.459], mean action: 1.703 [0.000, 3.000],  loss: 16.254020, mse: 2752.352523, mean_q: 41.108080, mean_eps: 0.653767
  57834/150000: episode: 568, duration: 0.531s, episode steps:  69, steps per second: 130, episode reward: 13.338, mean reward:  0.193 [-100.000, 20.290], mean action: 1.420 [0.000, 3.000],  loss: 11.407235, mse: 2746.504794, mean_q: 42.520844, mean_eps: 0.653206
  57932/150000: episode: 569, duration: 0.722s, episode steps:  98, steps per second: 136, episode reward: -76.849, mean reward: -0.784 [-100.000, 28.081], mean action: 1.673 [0.000, 3.000],  loss: 14.219734, mse: 2753.235193, mean_q: 41.625963, mean_eps: 0.652705
  58036/150000: episode: 570, duration: 0.743s, episode steps: 104, steps per second: 140, episode reward:  5.372, mean reward:  0.052 [-100.000, 16.334], mean action: 1.683 [0.000, 3.000],  loss: 12.591141, mse: 2733.959914, mean_q: 41.209139, mean_eps: 0.652099
  58144/150000: episode: 571, duration: 0.805s, episode steps: 108, steps per second: 134, episode reward: -93.328, mean reward: -0.864 [-100.000,  9.980], mean action: 1.630 [0.000, 3.000],  loss: 19.704072, mse: 2777.573767, mean_q: 41.820846, mean_eps: 0.651463
  58218/150000: episode: 572, duration: 0.522s, episode steps:  74, steps per second: 142, episode reward: -50.956, mean reward: -0.689 [-100.000,  6.614], mean action: 1.514 [0.000, 3.000],  loss: 17.723670, mse: 2861.127524, mean_q: 43.078363, mean_eps: 0.650917
  58339/150000: episode: 573, duration: 0.806s, episode steps: 121, steps per second: 150, episode reward: -137.650, mean reward: -1.138 [-100.000,  2.855], mean action: 1.413 [0.000, 3.000],  loss: 17.282837, mse: 2806.123160, mean_q: 42.817726, mean_eps: 0.650332
  58455/150000: episode: 574, duration: 0.856s, episode steps: 116, steps per second: 136, episode reward: -194.398, mean reward: -1.676 [-100.000, 33.315], mean action: 1.466 [0.000, 3.000],  loss: 16.385029, mse: 2791.042788, mean_q: 42.448721, mean_eps: 0.649621
  58577/150000: episode: 575, duration: 0.853s, episode steps: 122, steps per second: 143, episode reward: -17.753, mean reward: -0.146 [-100.000, 11.836], mean action: 1.607 [0.000, 3.000],  loss: 13.986430, mse: 2769.204252, mean_q: 41.823307, mean_eps: 0.648907
  58665/150000: episode: 576, duration: 0.627s, episode steps:  88, steps per second: 140, episode reward: -44.251, mean reward: -0.503 [-100.000, 13.853], mean action: 1.580 [0.000, 3.000],  loss: 15.853773, mse: 2855.875277, mean_q: 43.361862, mean_eps: 0.648277
  58744/150000: episode: 577, duration: 0.840s, episode steps:  79, steps per second:  94, episode reward: -10.985, mean reward: -0.139 [-100.000, 17.078], mean action: 1.582 [0.000, 3.000],  loss: 16.698012, mse: 2830.856968, mean_q: 40.952744, mean_eps: 0.647776
  58882/150000: episode: 578, duration: 1.391s, episode steps: 138, steps per second:  99, episode reward: -63.393, mean reward: -0.459 [-100.000, 12.579], mean action: 1.594 [0.000, 3.000],  loss: 17.442802, mse: 2770.309411, mean_q: 41.409291, mean_eps: 0.647125
  58974/150000: episode: 579, duration: 0.799s, episode steps:  92, steps per second: 115, episode reward: -61.077, mean reward: -0.664 [-100.000, 18.499], mean action: 1.533 [0.000, 3.000],  loss: 29.002842, mse: 2766.617079, mean_q: 41.598970, mean_eps: 0.646435
  59088/150000: episode: 580, duration: 0.969s, episode steps: 114, steps per second: 118, episode reward: -44.372, mean reward: -0.389 [-100.000, 13.601], mean action: 1.447 [0.000, 3.000],  loss: 17.372447, mse: 2793.839268, mean_q: 41.770085, mean_eps: 0.645817
  59224/150000: episode: 581, duration: 1.139s, episode steps: 136, steps per second: 119, episode reward: -31.095, mean reward: -0.229 [-100.000, 16.737], mean action: 1.537 [0.000, 3.000],  loss: 15.684452, mse: 2790.960726, mean_q: 42.172318, mean_eps: 0.645067
  59324/150000: episode: 582, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: -49.324, mean reward: -0.493 [-100.000,  9.571], mean action: 1.730 [0.000, 3.000],  loss: 14.273275, mse: 2791.124202, mean_q: 41.958904, mean_eps: 0.644359
  59477/150000: episode: 583, duration: 1.149s, episode steps: 153, steps per second: 133, episode reward: -21.093, mean reward: -0.138 [-100.000, 11.817], mean action: 1.641 [0.000, 3.000],  loss: 15.950020, mse: 2824.156349, mean_q: 42.624570, mean_eps: 0.643600
  59568/150000: episode: 584, duration: 0.665s, episode steps:  91, steps per second: 137, episode reward: -50.209, mean reward: -0.552 [-100.000,  7.706], mean action: 1.516 [0.000, 3.000],  loss: 18.781155, mse: 2838.013768, mean_q: 42.277344, mean_eps: 0.642868
  59674/150000: episode: 585, duration: 0.727s, episode steps: 106, steps per second: 146, episode reward: -78.717, mean reward: -0.743 [-100.000,  7.033], mean action: 1.783 [0.000, 3.000],  loss: 22.102663, mse: 2833.182705, mean_q: 42.474319, mean_eps: 0.642277
  59806/150000: episode: 586, duration: 0.939s, episode steps: 132, steps per second: 141, episode reward:  6.446, mean reward:  0.049 [-100.000, 17.616], mean action: 1.659 [0.000, 3.000],  loss: 15.220961, mse: 2831.156884, mean_q: 42.486954, mean_eps: 0.641563
  59886/150000: episode: 587, duration: 0.544s, episode steps:  80, steps per second: 147, episode reward: -126.984, mean reward: -1.587 [-100.000, 12.735], mean action: 1.850 [0.000, 3.000],  loss: 17.860813, mse: 2829.464087, mean_q: 41.925363, mean_eps: 0.640927
  60008/150000: episode: 588, duration: 0.840s, episode steps: 122, steps per second: 145, episode reward: -6.447, mean reward: -0.053 [-100.000, 20.866], mean action: 1.648 [0.000, 3.000],  loss: 13.517628, mse: 2825.458486, mean_q: 41.957571, mean_eps: 0.640321
  60117/150000: episode: 589, duration: 0.788s, episode steps: 109, steps per second: 138, episode reward: -54.332, mean reward: -0.498 [-100.000, 12.317], mean action: 1.523 [0.000, 3.000],  loss: 15.185021, mse: 2825.588986, mean_q: 43.131496, mean_eps: 0.639628
  60256/150000: episode: 590, duration: 1.145s, episode steps: 139, steps per second: 121, episode reward: -106.297, mean reward: -0.765 [-100.000, 41.556], mean action: 1.525 [0.000, 3.000],  loss: 11.321066, mse: 2868.833551, mean_q: 43.316617, mean_eps: 0.638884
  60363/150000: episode: 591, duration: 0.897s, episode steps: 107, steps per second: 119, episode reward: -76.933, mean reward: -0.719 [-100.000,  9.710], mean action: 1.841 [0.000, 3.000],  loss: 14.890157, mse: 2833.426199, mean_q: 43.349384, mean_eps: 0.638146
  60444/150000: episode: 592, duration: 0.586s, episode steps:  81, steps per second: 138, episode reward: -75.028, mean reward: -0.926 [-100.000,  7.786], mean action: 1.667 [0.000, 3.000],  loss: 13.493141, mse: 2865.419153, mean_q: 42.973155, mean_eps: 0.637582
  60533/150000: episode: 593, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -36.197, mean reward: -0.407 [-100.000,  8.986], mean action: 1.652 [0.000, 3.000],  loss: 22.173413, mse: 2856.398989, mean_q: 44.016356, mean_eps: 0.637072
  60644/150000: episode: 594, duration: 0.789s, episode steps: 111, steps per second: 141, episode reward: -93.800, mean reward: -0.845 [-100.000,  7.708], mean action: 1.802 [0.000, 3.000],  loss: 14.896767, mse: 2924.543068, mean_q: 43.912285, mean_eps: 0.636472
  60720/150000: episode: 595, duration: 0.525s, episode steps:  76, steps per second: 145, episode reward: -53.253, mean reward: -0.701 [-100.000,  9.752], mean action: 1.724 [0.000, 3.000],  loss: 12.977352, mse: 2919.938653, mean_q: 43.502343, mean_eps: 0.635911
  60857/150000: episode: 596, duration: 0.950s, episode steps: 137, steps per second: 144, episode reward: -50.387, mean reward: -0.368 [-100.000, 11.457], mean action: 1.438 [0.000, 3.000],  loss: 20.203667, mse: 2857.541308, mean_q: 42.508154, mean_eps: 0.635272
  60983/150000: episode: 597, duration: 0.921s, episode steps: 126, steps per second: 137, episode reward: -12.276, mean reward: -0.097 [-100.000, 17.865], mean action: 1.429 [0.000, 3.000],  loss: 16.181902, mse: 2841.133495, mean_q: 42.920452, mean_eps: 0.634483
  61130/150000: episode: 598, duration: 1.129s, episode steps: 147, steps per second: 130, episode reward: -32.191, mean reward: -0.219 [-100.000, 22.031], mean action: 1.592 [0.000, 3.000],  loss: 18.981659, mse: 2896.892753, mean_q: 43.558536, mean_eps: 0.633664
  61262/150000: episode: 599, duration: 0.971s, episode steps: 132, steps per second: 136, episode reward: -55.343, mean reward: -0.419 [-100.000, 15.957], mean action: 1.439 [0.000, 3.000],  loss: 19.320991, mse: 2868.599369, mean_q: 43.252396, mean_eps: 0.632827
  61377/150000: episode: 600, duration: 0.826s, episode steps: 115, steps per second: 139, episode reward: -152.444, mean reward: -1.326 [-100.000,  3.994], mean action: 1.443 [0.000, 3.000],  loss: 20.812024, mse: 2910.965372, mean_q: 43.694010, mean_eps: 0.632086
  61437/150000: episode: 601, duration: 0.421s, episode steps:  60, steps per second: 142, episode reward: -45.707, mean reward: -0.762 [-100.000, 10.760], mean action: 1.600 [0.000, 3.000],  loss: 14.886138, mse: 2850.374530, mean_q: 44.597510, mean_eps: 0.631561
  61562/150000: episode: 602, duration: 0.931s, episode steps: 125, steps per second: 134, episode reward: -56.230, mean reward: -0.450 [-100.000,  9.233], mean action: 1.592 [0.000, 3.000],  loss: 15.295896, mse: 2876.211568, mean_q: 43.182424, mean_eps: 0.631006
  61644/150000: episode: 603, duration: 0.583s, episode steps:  82, steps per second: 141, episode reward: -46.163, mean reward: -0.563 [-100.000, 17.706], mean action: 1.549 [0.000, 3.000],  loss: 9.780411, mse: 2871.167201, mean_q: 42.644677, mean_eps: 0.630385
  61742/150000: episode: 604, duration: 0.686s, episode steps:  98, steps per second: 143, episode reward: -46.916, mean reward: -0.479 [-100.000, 19.486], mean action: 1.592 [0.000, 3.000],  loss: 19.870455, mse: 2849.873946, mean_q: 43.483184, mean_eps: 0.629845
  61835/150000: episode: 605, duration: 0.705s, episode steps:  93, steps per second: 132, episode reward: -3.818, mean reward: -0.041 [-100.000, 26.303], mean action: 1.452 [0.000, 3.000],  loss: 12.478476, mse: 2892.611444, mean_q: 43.821784, mean_eps: 0.629272
  61941/150000: episode: 606, duration: 0.746s, episode steps: 106, steps per second: 142, episode reward: -36.934, mean reward: -0.348 [-100.000, 12.907], mean action: 1.528 [0.000, 3.000],  loss: 12.855524, mse: 2851.143838, mean_q: 43.408310, mean_eps: 0.628675
  62041/150000: episode: 607, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: -66.424, mean reward: -0.664 [-100.000,  6.852], mean action: 1.780 [0.000, 3.000],  loss: 16.172591, mse: 2923.757395, mean_q: 44.427036, mean_eps: 0.628057
  62130/150000: episode: 608, duration: 0.654s, episode steps:  89, steps per second: 136, episode reward: -45.900, mean reward: -0.516 [-100.000, 11.846], mean action: 1.843 [0.000, 3.000],  loss: 13.774250, mse: 2930.356259, mean_q: 44.014925, mean_eps: 0.627490
  62216/150000: episode: 609, duration: 0.627s, episode steps:  86, steps per second: 137, episode reward: -89.830, mean reward: -1.045 [-100.000,  7.979], mean action: 1.837 [0.000, 3.000],  loss: 20.375744, mse: 2905.104606, mean_q: 44.153904, mean_eps: 0.626965
  62462/150000: episode: 610, duration: 1.772s, episode steps: 246, steps per second: 139, episode reward: -167.047, mean reward: -0.679 [-100.000, 15.652], mean action: 1.650 [0.000, 3.000],  loss: 17.990549, mse: 2883.815913, mean_q: 43.498566, mean_eps: 0.625969
  62573/150000: episode: 611, duration: 0.770s, episode steps: 111, steps per second: 144, episode reward: -142.372, mean reward: -1.283 [-100.000,  2.773], mean action: 1.414 [0.000, 3.000],  loss: 12.900473, mse: 2937.503334, mean_q: 44.270085, mean_eps: 0.624898
  62701/150000: episode: 612, duration: 0.892s, episode steps: 128, steps per second: 143, episode reward: -78.812, mean reward: -0.616 [-100.000,  8.163], mean action: 1.422 [0.000, 3.000],  loss: 11.773820, mse: 2892.343388, mean_q: 43.889485, mean_eps: 0.624181
  62807/150000: episode: 613, duration: 0.819s, episode steps: 106, steps per second: 129, episode reward: -64.809, mean reward: -0.611 [-100.000,  8.876], mean action: 1.755 [0.000, 3.000],  loss: 14.793571, mse: 2922.035080, mean_q: 43.747535, mean_eps: 0.623479
  62933/150000: episode: 614, duration: 1.173s, episode steps: 126, steps per second: 107, episode reward: -137.106, mean reward: -1.088 [-100.000,  6.456], mean action: 1.500 [0.000, 3.000],  loss: 12.618334, mse: 2870.834403, mean_q: 44.097815, mean_eps: 0.622783
  63052/150000: episode: 615, duration: 1.065s, episode steps: 119, steps per second: 112, episode reward: -22.186, mean reward: -0.186 [-100.000,  9.754], mean action: 1.756 [0.000, 3.000],  loss: 14.856848, mse: 2938.133338, mean_q: 44.286804, mean_eps: 0.622048
  63154/150000: episode: 616, duration: 0.868s, episode steps: 102, steps per second: 118, episode reward: -22.893, mean reward: -0.224 [-100.000, 10.459], mean action: 1.569 [0.000, 3.000],  loss: 16.385428, mse: 2944.999067, mean_q: 43.850773, mean_eps: 0.621385
  63261/150000: episode: 617, duration: 0.969s, episode steps: 107, steps per second: 110, episode reward: -110.365, mean reward: -1.031 [-100.000, 10.474], mean action: 1.701 [0.000, 3.000],  loss: 18.376540, mse: 2983.008794, mean_q: 43.812481, mean_eps: 0.620758
  63333/150000: episode: 618, duration: 0.629s, episode steps:  72, steps per second: 114, episode reward: -80.142, mean reward: -1.113 [-100.000, 11.006], mean action: 1.403 [0.000, 3.000],  loss: 14.986769, mse: 2948.797892, mean_q: 45.261313, mean_eps: 0.620221
  63408/150000: episode: 619, duration: 0.757s, episode steps:  75, steps per second:  99, episode reward: -15.725, mean reward: -0.210 [-100.000, 15.260], mean action: 1.773 [0.000, 3.000],  loss: 18.440967, mse: 2914.572523, mean_q: 44.141024, mean_eps: 0.619780
  63524/150000: episode: 620, duration: 1.009s, episode steps: 116, steps per second: 115, episode reward: -90.332, mean reward: -0.779 [-100.000, 27.579], mean action: 1.534 [0.000, 3.000],  loss: 20.730774, mse: 2948.857754, mean_q: 44.424217, mean_eps: 0.619207
  63615/150000: episode: 621, duration: 0.671s, episode steps:  91, steps per second: 136, episode reward: -82.439, mean reward: -0.906 [-100.000, 12.910], mean action: 1.582 [0.000, 3.000],  loss: 19.416362, mse: 2943.806799, mean_q: 43.952953, mean_eps: 0.618586
  63706/150000: episode: 622, duration: 1.013s, episode steps:  91, steps per second:  90, episode reward: -54.973, mean reward: -0.604 [-100.000, 14.239], mean action: 1.637 [0.000, 3.000],  loss: 20.818467, mse: 2859.985488, mean_q: 43.354657, mean_eps: 0.618040
  63833/150000: episode: 623, duration: 0.991s, episode steps: 127, steps per second: 128, episode reward: -15.668, mean reward: -0.123 [-100.000,  6.780], mean action: 1.528 [0.000, 3.000],  loss: 15.977910, mse: 2900.616213, mean_q: 43.285970, mean_eps: 0.617386
  63927/150000: episode: 624, duration: 0.648s, episode steps:  94, steps per second: 145, episode reward: -43.021, mean reward: -0.458 [-100.000, 10.536], mean action: 1.691 [0.000, 3.000],  loss: 15.131257, mse: 2942.229342, mean_q: 43.975320, mean_eps: 0.616723
  64016/150000: episode: 625, duration: 0.642s, episode steps:  89, steps per second: 139, episode reward: -29.891, mean reward: -0.336 [-100.000, 22.372], mean action: 1.528 [0.000, 3.000],  loss: 11.573530, mse: 2951.515683, mean_q: 43.785464, mean_eps: 0.616174
  64098/150000: episode: 626, duration: 0.601s, episode steps:  82, steps per second: 136, episode reward: -70.410, mean reward: -0.859 [-100.000,  7.739], mean action: 1.610 [0.000, 3.000],  loss: 17.221725, mse: 2930.762061, mean_q: 43.880262, mean_eps: 0.615661
  64175/150000: episode: 627, duration: 0.676s, episode steps:  77, steps per second: 114, episode reward: -92.514, mean reward: -1.201 [-100.000, 12.360], mean action: 1.532 [0.000, 3.000],  loss: 12.242502, mse: 3029.245926, mean_q: 45.216742, mean_eps: 0.615184
  64272/150000: episode: 628, duration: 1.166s, episode steps:  97, steps per second:  83, episode reward: -25.370, mean reward: -0.262 [-100.000, 18.597], mean action: 1.515 [0.000, 3.000],  loss: 13.003280, mse: 3007.913131, mean_q: 45.201951, mean_eps: 0.614662
  64369/150000: episode: 629, duration: 0.832s, episode steps:  97, steps per second: 117, episode reward: -88.371, mean reward: -0.911 [-100.000,  6.271], mean action: 1.784 [0.000, 3.000],  loss: 9.317772, mse: 3077.973469, mean_q: 44.813007, mean_eps: 0.614080
  64467/150000: episode: 630, duration: 0.891s, episode steps:  98, steps per second: 110, episode reward: -92.976, mean reward: -0.949 [-100.000, 19.332], mean action: 1.622 [0.000, 3.000],  loss: 15.499298, mse: 3017.450581, mean_q: 45.295841, mean_eps: 0.613495
  64557/150000: episode: 631, duration: 0.984s, episode steps:  90, steps per second:  91, episode reward: -76.098, mean reward: -0.846 [-100.000,  9.796], mean action: 1.700 [0.000, 3.000],  loss: 14.511795, mse: 3015.284757, mean_q: 45.531875, mean_eps: 0.612931
  64727/150000: episode: 632, duration: 1.351s, episode steps: 170, steps per second: 126, episode reward: -122.232, mean reward: -0.719 [-100.000,  4.116], mean action: 1.565 [0.000, 3.000],  loss: 22.514942, mse: 2970.143253, mean_q: 45.155200, mean_eps: 0.612151
  64838/150000: episode: 633, duration: 0.912s, episode steps: 111, steps per second: 122, episode reward: -72.859, mean reward: -0.656 [-100.000,  9.720], mean action: 1.838 [0.000, 3.000],  loss: 13.157966, mse: 2978.319613, mean_q: 45.044061, mean_eps: 0.611308
  64976/150000: episode: 634, duration: 1.620s, episode steps: 138, steps per second:  85, episode reward: -118.472, mean reward: -0.858 [-100.000,  4.909], mean action: 1.739 [0.000, 3.000],  loss: 13.901989, mse: 3028.006358, mean_q: 44.466820, mean_eps: 0.610561
  65115/150000: episode: 635, duration: 0.990s, episode steps: 139, steps per second: 140, episode reward: -122.406, mean reward: -0.881 [-100.000,  3.258], mean action: 1.525 [0.000, 3.000],  loss: 15.763754, mse: 2971.483553, mean_q: 44.074264, mean_eps: 0.609730
  65227/150000: episode: 636, duration: 0.949s, episode steps: 112, steps per second: 118, episode reward: -54.386, mean reward: -0.486 [-100.000, 13.423], mean action: 1.804 [0.000, 3.000],  loss: 18.662735, mse: 2990.843085, mean_q: 43.826979, mean_eps: 0.608977
  65373/150000: episode: 637, duration: 1.204s, episode steps: 146, steps per second: 121, episode reward: -45.504, mean reward: -0.312 [-100.000, 23.625], mean action: 1.514 [0.000, 3.000],  loss: 11.643767, mse: 2956.628813, mean_q: 43.776506, mean_eps: 0.608203
  65496/150000: episode: 638, duration: 0.981s, episode steps: 123, steps per second: 125, episode reward: -67.242, mean reward: -0.547 [-100.000,  7.618], mean action: 1.667 [0.000, 3.000],  loss: 18.073018, mse: 2924.070291, mean_q: 43.769832, mean_eps: 0.607396
  65564/150000: episode: 639, duration: 0.504s, episode steps:  68, steps per second: 135, episode reward: -79.017, mean reward: -1.162 [-100.000,  6.967], mean action: 1.662 [0.000, 3.000],  loss: 16.694664, mse: 3014.471734, mean_q: 45.556028, mean_eps: 0.606823
  65709/150000: episode: 640, duration: 1.080s, episode steps: 145, steps per second: 134, episode reward: -231.313, mean reward: -1.595 [-100.000, 52.282], mean action: 1.738 [0.000, 3.000],  loss: 16.211568, mse: 2956.919582, mean_q: 44.179114, mean_eps: 0.606184
  65806/150000: episode: 641, duration: 0.752s, episode steps:  97, steps per second: 129, episode reward: -186.025, mean reward: -1.918 [-100.000, 60.463], mean action: 1.814 [0.000, 3.000],  loss: 17.250941, mse: 2982.853699, mean_q: 43.772201, mean_eps: 0.605458
  65927/150000: episode: 642, duration: 0.947s, episode steps: 121, steps per second: 128, episode reward: -17.216, mean reward: -0.142 [-100.000, 11.230], mean action: 1.570 [0.000, 3.000],  loss: 11.099200, mse: 2959.346898, mean_q: 44.057598, mean_eps: 0.604804
  66046/150000: episode: 643, duration: 0.906s, episode steps: 119, steps per second: 131, episode reward: -162.285, mean reward: -1.364 [-100.000,  3.858], mean action: 1.538 [0.000, 3.000],  loss: 17.664276, mse: 2998.565454, mean_q: 44.225151, mean_eps: 0.604084
  66148/150000: episode: 644, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -83.303, mean reward: -0.817 [-100.000, 14.899], mean action: 1.510 [0.000, 3.000],  loss: 22.020476, mse: 3045.614492, mean_q: 44.099617, mean_eps: 0.603421
  66254/150000: episode: 645, duration: 0.754s, episode steps: 106, steps per second: 141, episode reward: -97.751, mean reward: -0.922 [-100.000,  7.595], mean action: 1.387 [0.000, 3.000],  loss: 13.820069, mse: 3018.856494, mean_q: 44.126359, mean_eps: 0.602797
  66342/150000: episode: 646, duration: 0.651s, episode steps:  88, steps per second: 135, episode reward: -31.548, mean reward: -0.358 [-100.000, 25.706], mean action: 1.864 [0.000, 3.000],  loss: 17.913205, mse: 3049.000954, mean_q: 44.818934, mean_eps: 0.602215
  66446/150000: episode: 647, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -152.024, mean reward: -1.462 [-100.000,  2.358], mean action: 1.510 [0.000, 3.000],  loss: 16.177434, mse: 2999.292408, mean_q: 43.892673, mean_eps: 0.601639
  66576/150000: episode: 648, duration: 0.910s, episode steps: 130, steps per second: 143, episode reward: -101.616, mean reward: -0.782 [-100.000,  7.847], mean action: 1.662 [0.000, 3.000],  loss: 16.222392, mse: 3013.548945, mean_q: 44.947970, mean_eps: 0.600937
  66694/150000: episode: 649, duration: 0.893s, episode steps: 118, steps per second: 132, episode reward: -116.785, mean reward: -0.990 [-100.000, 23.842], mean action: 1.525 [0.000, 3.000],  loss: 14.963222, mse: 3014.632864, mean_q: 44.463936, mean_eps: 0.600193
  66802/150000: episode: 650, duration: 0.791s, episode steps: 108, steps per second: 137, episode reward: -109.918, mean reward: -1.018 [-100.000,  3.546], mean action: 1.481 [0.000, 3.000],  loss: 12.961700, mse: 3004.288662, mean_q: 44.048589, mean_eps: 0.599515
  66930/150000: episode: 651, duration: 0.926s, episode steps: 128, steps per second: 138, episode reward: -51.652, mean reward: -0.404 [-100.000, 17.630], mean action: 1.594 [0.000, 3.000],  loss: 12.503267, mse: 3023.564236, mean_q: 44.887884, mean_eps: 0.598807
  67025/150000: episode: 652, duration: 0.683s, episode steps:  95, steps per second: 139, episode reward: -16.969, mean reward: -0.179 [-100.000, 12.162], mean action: 1.789 [0.000, 3.000],  loss: 21.163577, mse: 3006.362333, mean_q: 44.375691, mean_eps: 0.598138
  67140/150000: episode: 653, duration: 0.955s, episode steps: 115, steps per second: 120, episode reward: -47.467, mean reward: -0.413 [-100.000, 18.972], mean action: 1.687 [0.000, 3.000],  loss: 18.368084, mse: 3004.346238, mean_q: 43.601517, mean_eps: 0.597508
  67265/150000: episode: 654, duration: 1.078s, episode steps: 125, steps per second: 116, episode reward: -174.630, mean reward: -1.397 [-100.000, 43.238], mean action: 1.400 [0.000, 3.000],  loss: 13.844348, mse: 3073.363711, mean_q: 43.515687, mean_eps: 0.596788
  67396/150000: episode: 655, duration: 1.074s, episode steps: 131, steps per second: 122, episode reward: -94.205, mean reward: -0.719 [-100.000,  7.056], mean action: 1.771 [0.000, 3.000],  loss: 17.595194, mse: 3059.456701, mean_q: 44.232402, mean_eps: 0.596020
  67499/150000: episode: 656, duration: 0.904s, episode steps: 103, steps per second: 114, episode reward: -103.708, mean reward: -1.007 [-100.000,  9.286], mean action: 1.573 [0.000, 3.000],  loss: 15.476501, mse: 3042.414624, mean_q: 43.693777, mean_eps: 0.595318
  67600/150000: episode: 657, duration: 0.797s, episode steps: 101, steps per second: 127, episode reward: -60.057, mean reward: -0.595 [-100.000,  7.084], mean action: 1.594 [0.000, 3.000],  loss: 10.506583, mse: 3085.235755, mean_q: 44.410991, mean_eps: 0.594706
  67705/150000: episode: 658, duration: 0.788s, episode steps: 105, steps per second: 133, episode reward: -57.986, mean reward: -0.552 [-100.000, 16.972], mean action: 1.686 [0.000, 3.000],  loss: 12.278840, mse: 3060.413195, mean_q: 44.017019, mean_eps: 0.594088
  67818/150000: episode: 659, duration: 0.832s, episode steps: 113, steps per second: 136, episode reward: -108.140, mean reward: -0.957 [-100.000, 23.618], mean action: 1.575 [0.000, 3.000],  loss: 10.354962, mse: 3106.992691, mean_q: 44.569677, mean_eps: 0.593434
  67928/150000: episode: 660, duration: 0.754s, episode steps: 110, steps per second: 146, episode reward: -82.736, mean reward: -0.752 [-100.000,  5.460], mean action: 1.509 [0.000, 3.000],  loss: 14.870726, mse: 3028.080358, mean_q: 44.333117, mean_eps: 0.592765
  68030/150000: episode: 661, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -47.320, mean reward: -0.464 [-100.000,  7.935], mean action: 1.863 [0.000, 3.000],  loss: 19.084202, mse: 3033.395903, mean_q: 43.969184, mean_eps: 0.592129
  68175/150000: episode: 662, duration: 1.033s, episode steps: 145, steps per second: 140, episode reward: -73.877, mean reward: -0.509 [-100.000,  5.987], mean action: 1.641 [0.000, 3.000],  loss: 12.990068, mse: 3031.345629, mean_q: 43.959104, mean_eps: 0.591388
  68286/150000: episode: 663, duration: 0.777s, episode steps: 111, steps per second: 143, episode reward: -68.879, mean reward: -0.621 [-100.000, 16.996], mean action: 1.468 [0.000, 3.000],  loss: 17.186799, mse: 3023.185292, mean_q: 43.956298, mean_eps: 0.590620
  68374/150000: episode: 664, duration: 0.639s, episode steps:  88, steps per second: 138, episode reward: 11.235, mean reward:  0.128 [-100.000, 12.097], mean action: 1.670 [0.000, 3.000],  loss: 16.004819, mse: 2968.222082, mean_q: 43.519742, mean_eps: 0.590023
  68472/150000: episode: 665, duration: 0.688s, episode steps:  98, steps per second: 142, episode reward: -45.729, mean reward: -0.467 [-100.000, 13.886], mean action: 1.765 [0.000, 3.000],  loss: 14.844288, mse: 3034.054112, mean_q: 44.063143, mean_eps: 0.589465
  68618/150000: episode: 666, duration: 1.006s, episode steps: 146, steps per second: 145, episode reward: -101.352, mean reward: -0.694 [-100.000,  5.807], mean action: 1.726 [0.000, 3.000],  loss: 19.215449, mse: 3046.860517, mean_q: 44.037748, mean_eps: 0.588733
  68701/150000: episode: 667, duration: 0.625s, episode steps:  83, steps per second: 133, episode reward: -99.567, mean reward: -1.200 [-100.000,  8.761], mean action: 1.530 [0.000, 3.000],  loss: 10.240957, mse: 2995.205999, mean_q: 45.162500, mean_eps: 0.588046
  68801/150000: episode: 668, duration: 0.756s, episode steps: 100, steps per second: 132, episode reward: -54.127, mean reward: -0.541 [-100.000, 10.271], mean action: 1.700 [0.000, 3.000],  loss: 18.866491, mse: 3049.030430, mean_q: 43.927354, mean_eps: 0.587497
  68960/150000: episode: 669, duration: 1.181s, episode steps: 159, steps per second: 135, episode reward: -193.353, mean reward: -1.216 [-100.000, 37.823], mean action: 1.811 [0.000, 3.000],  loss: 14.228301, mse: 3007.235777, mean_q: 43.945479, mean_eps: 0.586720
  69062/150000: episode: 670, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -53.837, mean reward: -0.528 [-100.000, 37.569], mean action: 1.667 [0.000, 3.000],  loss: 16.713581, mse: 3112.847436, mean_q: 44.208901, mean_eps: 0.585937
  69179/150000: episode: 671, duration: 0.796s, episode steps: 117, steps per second: 147, episode reward: -88.802, mean reward: -0.759 [-100.000,  6.816], mean action: 1.487 [0.000, 3.000],  loss: 9.317496, mse: 3075.464581, mean_q: 43.832376, mean_eps: 0.585280
  69326/150000: episode: 672, duration: 1.072s, episode steps: 147, steps per second: 137, episode reward: 45.960, mean reward:  0.313 [-100.000, 17.894], mean action: 1.769 [0.000, 3.000],  loss: 11.900870, mse: 3063.317272, mean_q: 43.947086, mean_eps: 0.584488
  69416/150000: episode: 673, duration: 0.636s, episode steps:  90, steps per second: 141, episode reward: -86.770, mean reward: -0.964 [-100.000,  5.442], mean action: 1.844 [0.000, 3.000],  loss: 12.214020, mse: 3086.191621, mean_q: 44.081849, mean_eps: 0.583777
  69509/150000: episode: 674, duration: 0.650s, episode steps:  93, steps per second: 143, episode reward: -39.883, mean reward: -0.429 [-100.000, 11.564], mean action: 1.538 [0.000, 3.000],  loss: 15.508643, mse: 3083.958428, mean_q: 44.720434, mean_eps: 0.583228
  69714/150000: episode: 675, duration: 1.476s, episode steps: 205, steps per second: 139, episode reward: -110.393, mean reward: -0.539 [-100.000,  8.037], mean action: 1.678 [0.000, 3.000],  loss: 12.329248, mse: 3085.682980, mean_q: 45.052367, mean_eps: 0.582334
  69822/150000: episode: 676, duration: 0.727s, episode steps: 108, steps per second: 148, episode reward: -65.866, mean reward: -0.610 [-100.000, 11.145], mean action: 1.630 [0.000, 3.000],  loss: 11.474527, mse: 3045.653766, mean_q: 44.612218, mean_eps: 0.581395
  69898/150000: episode: 677, duration: 0.572s, episode steps:  76, steps per second: 133, episode reward: -33.935, mean reward: -0.447 [-100.000, 10.978], mean action: 1.776 [0.000, 3.000],  loss: 17.259273, mse: 3112.887975, mean_q: 44.968318, mean_eps: 0.580843
  70022/150000: episode: 678, duration: 0.879s, episode steps: 124, steps per second: 141, episode reward: -33.853, mean reward: -0.273 [-100.000, 13.083], mean action: 1.637 [0.000, 3.000],  loss: 18.603348, mse: 3046.690270, mean_q: 43.344075, mean_eps: 0.580243
  70189/150000: episode: 679, duration: 1.184s, episode steps: 167, steps per second: 141, episode reward: 12.161, mean reward:  0.073 [-100.000, 15.314], mean action: 1.760 [0.000, 3.000],  loss: 12.626424, mse: 3058.710151, mean_q: 43.820010, mean_eps: 0.579370
  70310/150000: episode: 680, duration: 0.951s, episode steps: 121, steps per second: 127, episode reward: -43.554, mean reward: -0.360 [-100.000,  8.644], mean action: 1.752 [0.000, 3.000],  loss: 12.128595, mse: 3028.452532, mean_q: 44.808656, mean_eps: 0.578506
  70444/150000: episode: 681, duration: 0.927s, episode steps: 134, steps per second: 145, episode reward: -20.041, mean reward: -0.150 [-100.000,  9.262], mean action: 1.687 [0.000, 3.000],  loss: 11.209368, mse: 3051.508498, mean_q: 44.959649, mean_eps: 0.577741
  70592/150000: episode: 682, duration: 1.061s, episode steps: 148, steps per second: 140, episode reward: -182.330, mean reward: -1.232 [-100.000,  4.891], mean action: 1.784 [0.000, 3.000],  loss: 17.352472, mse: 3039.811937, mean_q: 43.366100, mean_eps: 0.576895
  70722/150000: episode: 683, duration: 0.897s, episode steps: 130, steps per second: 145, episode reward: -147.501, mean reward: -1.135 [-100.000,  3.093], mean action: 1.546 [0.000, 3.000],  loss: 13.130418, mse: 3084.722376, mean_q: 44.658228, mean_eps: 0.576061
  70852/150000: episode: 684, duration: 0.977s, episode steps: 130, steps per second: 133, episode reward: -66.704, mean reward: -0.513 [-100.000, 10.994], mean action: 1.662 [0.000, 3.000],  loss: 11.079693, mse: 3093.446334, mean_q: 45.163823, mean_eps: 0.575281
  70989/150000: episode: 685, duration: 0.961s, episode steps: 137, steps per second: 143, episode reward: -32.364, mean reward: -0.236 [-100.000, 13.410], mean action: 1.693 [0.000, 3.000],  loss: 8.937484, mse: 3031.385769, mean_q: 43.850510, mean_eps: 0.574480
  71424/150000: episode: 686, duration: 3.299s, episode steps: 435, steps per second: 132, episode reward: -411.841, mean reward: -0.947 [-100.000, 40.454], mean action: 1.667 [0.000, 3.000],  loss: 11.171248, mse: 3088.539305, mean_q: 45.137626, mean_eps: 0.572764
  71510/150000: episode: 687, duration: 0.616s, episode steps:  86, steps per second: 140, episode reward: -16.629, mean reward: -0.193 [-100.000, 17.848], mean action: 1.721 [0.000, 3.000],  loss: 15.455597, mse: 3075.789494, mean_q: 44.056235, mean_eps: 0.571201
  71640/150000: episode: 688, duration: 0.921s, episode steps: 130, steps per second: 141, episode reward: -150.572, mean reward: -1.158 [-100.000,  3.264], mean action: 1.538 [0.000, 3.000],  loss: 12.574875, mse: 3128.543739, mean_q: 44.432597, mean_eps: 0.570553
  71804/150000: episode: 689, duration: 1.162s, episode steps: 164, steps per second: 141, episode reward: -68.802, mean reward: -0.420 [-100.000, 10.562], mean action: 1.652 [0.000, 3.000],  loss: 11.951607, mse: 3118.405717, mean_q: 44.465240, mean_eps: 0.569671
  71909/150000: episode: 690, duration: 0.725s, episode steps: 105, steps per second: 145, episode reward: -88.509, mean reward: -0.843 [-100.000, 17.695], mean action: 1.686 [0.000, 3.000],  loss: 13.526984, mse: 3079.901172, mean_q: 44.415079, mean_eps: 0.568864
  71997/150000: episode: 691, duration: 0.678s, episode steps:  88, steps per second: 130, episode reward: -47.716, mean reward: -0.542 [-100.000,  7.771], mean action: 1.795 [0.000, 3.000],  loss: 18.370514, mse: 3115.837427, mean_q: 45.270158, mean_eps: 0.568285
  72084/150000: episode: 692, duration: 0.740s, episode steps:  87, steps per second: 118, episode reward: -24.917, mean reward: -0.286 [-100.000, 13.566], mean action: 1.678 [0.000, 3.000],  loss: 16.232098, mse: 3083.957447, mean_q: 46.269466, mean_eps: 0.567760
  72245/150000: episode: 693, duration: 1.399s, episode steps: 161, steps per second: 115, episode reward: -3.454, mean reward: -0.021 [-100.000, 18.037], mean action: 1.770 [0.000, 3.000],  loss: 13.690149, mse: 3100.777071, mean_q: 44.673566, mean_eps: 0.567016
  72353/150000: episode: 694, duration: 0.809s, episode steps: 108, steps per second: 134, episode reward: -31.394, mean reward: -0.291 [-100.000, 15.911], mean action: 1.806 [0.000, 3.000],  loss: 11.065917, mse: 3092.963363, mean_q: 44.675378, mean_eps: 0.566209
  72485/150000: episode: 695, duration: 0.903s, episode steps: 132, steps per second: 146, episode reward: -30.109, mean reward: -0.228 [-100.000, 19.273], mean action: 1.432 [0.000, 3.000],  loss: 11.484396, mse: 3105.720527, mean_q: 44.315643, mean_eps: 0.565489
  72580/150000: episode: 696, duration: 0.699s, episode steps:  95, steps per second: 136, episode reward: -61.521, mean reward: -0.648 [-100.000, 12.286], mean action: 1.937 [0.000, 3.000],  loss: 15.640405, mse: 3058.952678, mean_q: 44.672525, mean_eps: 0.564808
  72668/150000: episode: 697, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -18.035, mean reward: -0.205 [-100.000, 22.952], mean action: 1.841 [0.000, 3.000],  loss: 11.356979, mse: 3109.402580, mean_q: 45.876171, mean_eps: 0.564259
  72792/150000: episode: 698, duration: 0.835s, episode steps: 124, steps per second: 148, episode reward: -77.953, mean reward: -0.629 [-100.000,  5.711], mean action: 1.347 [0.000, 3.000],  loss: 12.724101, mse: 3115.344573, mean_q: 45.345605, mean_eps: 0.563623
  72870/150000: episode: 699, duration: 0.591s, episode steps:  78, steps per second: 132, episode reward: -52.255, mean reward: -0.670 [-100.000,  7.724], mean action: 1.603 [0.000, 3.000],  loss: 12.794444, mse: 3104.500341, mean_q: 45.588816, mean_eps: 0.563017
  72987/150000: episode: 700, duration: 0.797s, episode steps: 117, steps per second: 147, episode reward: -160.218, mean reward: -1.369 [-100.000,  2.866], mean action: 1.761 [0.000, 3.000],  loss: 23.724782, mse: 3111.774141, mean_q: 45.522161, mean_eps: 0.562432
  73100/150000: episode: 701, duration: 0.770s, episode steps: 113, steps per second: 147, episode reward: -66.671, mean reward: -0.590 [-100.000,  7.680], mean action: 1.779 [0.000, 3.000],  loss: 19.283636, mse: 3159.295996, mean_q: 44.000423, mean_eps: 0.561742
  73246/150000: episode: 702, duration: 1.052s, episode steps: 146, steps per second: 139, episode reward: -44.601, mean reward: -0.305 [-100.000, 10.991], mean action: 1.493 [0.000, 3.000],  loss: 14.515741, mse: 3059.075595, mean_q: 43.958243, mean_eps: 0.560965
  73373/150000: episode: 703, duration: 0.946s, episode steps: 127, steps per second: 134, episode reward: -25.143, mean reward: -0.198 [-100.000, 16.055], mean action: 1.433 [0.000, 3.000],  loss: 17.540505, mse: 3111.143397, mean_q: 44.072151, mean_eps: 0.560146
  73488/150000: episode: 704, duration: 0.912s, episode steps: 115, steps per second: 126, episode reward: -17.956, mean reward: -0.156 [-100.000, 11.065], mean action: 1.713 [0.000, 3.000],  loss: 16.481482, mse: 3115.644731, mean_q: 44.773923, mean_eps: 0.559420
  73637/150000: episode: 705, duration: 1.117s, episode steps: 149, steps per second: 133, episode reward: -72.406, mean reward: -0.486 [-100.000, 11.187], mean action: 1.611 [0.000, 3.000],  loss: 17.145584, mse: 3144.208368, mean_q: 44.633073, mean_eps: 0.558628
  73767/150000: episode: 706, duration: 1.197s, episode steps: 130, steps per second: 109, episode reward: -41.437, mean reward: -0.319 [-100.000, 12.198], mean action: 1.654 [0.000, 3.000],  loss: 16.035300, mse: 3077.311831, mean_q: 45.353073, mean_eps: 0.557791
  73849/150000: episode: 707, duration: 0.599s, episode steps:  82, steps per second: 137, episode reward: -102.429, mean reward: -1.249 [-100.000, 10.076], mean action: 1.598 [0.000, 3.000],  loss: 15.765299, mse: 3067.059883, mean_q: 44.487393, mean_eps: 0.557155
  74032/150000: episode: 708, duration: 1.908s, episode steps: 183, steps per second:  96, episode reward: -123.118, mean reward: -0.673 [-100.000,  4.908], mean action: 1.645 [0.000, 3.000],  loss: 14.306631, mse: 3120.085565, mean_q: 45.316196, mean_eps: 0.556360
  74134/150000: episode: 709, duration: 0.916s, episode steps: 102, steps per second: 111, episode reward: 26.007, mean reward:  0.255 [-100.000, 50.842], mean action: 1.725 [0.000, 3.000],  loss: 10.642823, mse: 3131.361888, mean_q: 45.645395, mean_eps: 0.555505
  74278/150000: episode: 710, duration: 1.594s, episode steps: 144, steps per second:  90, episode reward: -48.896, mean reward: -0.340 [-100.000, 21.765], mean action: 1.535 [0.000, 3.000],  loss: 12.868930, mse: 3126.577684, mean_q: 45.488339, mean_eps: 0.554767
  74444/150000: episode: 711, duration: 1.413s, episode steps: 166, steps per second: 118, episode reward: -2.664, mean reward: -0.016 [-100.000,  9.289], mean action: 1.554 [0.000, 3.000],  loss: 15.230473, mse: 3176.835862, mean_q: 45.455609, mean_eps: 0.553837
  74561/150000: episode: 712, duration: 0.869s, episode steps: 117, steps per second: 135, episode reward: -73.867, mean reward: -0.631 [-100.000, 11.122], mean action: 1.513 [0.000, 3.000],  loss: 15.408847, mse: 3173.955633, mean_q: 45.637914, mean_eps: 0.552988
  74647/150000: episode: 713, duration: 0.633s, episode steps:  86, steps per second: 136, episode reward: -44.187, mean reward: -0.514 [-100.000, 14.262], mean action: 1.767 [0.000, 3.000],  loss: 14.237702, mse: 3205.519716, mean_q: 45.460710, mean_eps: 0.552379
  74778/150000: episode: 714, duration: 0.956s, episode steps: 131, steps per second: 137, episode reward: -51.918, mean reward: -0.396 [-100.000, 18.734], mean action: 1.580 [0.000, 3.000],  loss: 19.456876, mse: 3197.440860, mean_q: 45.828872, mean_eps: 0.551728
  74907/150000: episode: 715, duration: 0.885s, episode steps: 129, steps per second: 146, episode reward:  7.858, mean reward:  0.061 [-100.000, 20.895], mean action: 1.527 [0.000, 3.000],  loss: 16.279078, mse: 3148.839433, mean_q: 44.936474, mean_eps: 0.550948
  74985/150000: episode: 716, duration: 0.580s, episode steps:  78, steps per second: 134, episode reward: -45.420, mean reward: -0.582 [-100.000, 15.741], mean action: 1.551 [0.000, 3.000],  loss: 21.281784, mse: 3161.327834, mean_q: 45.317369, mean_eps: 0.550327
  75123/150000: episode: 717, duration: 1.002s, episode steps: 138, steps per second: 138, episode reward: -278.752, mean reward: -2.020 [-100.000,  4.762], mean action: 1.775 [0.000, 3.000],  loss: 10.924017, mse: 3197.773197, mean_q: 45.990110, mean_eps: 0.549679
  75237/150000: episode: 718, duration: 0.801s, episode steps: 114, steps per second: 142, episode reward: -28.649, mean reward: -0.251 [-100.000, 17.481], mean action: 1.684 [0.000, 3.000],  loss: 19.773581, mse: 3236.268904, mean_q: 46.261055, mean_eps: 0.548923
  75311/150000: episode: 719, duration: 0.556s, episode steps:  74, steps per second: 133, episode reward: -94.146, mean reward: -1.272 [-100.000, 16.475], mean action: 1.865 [0.000, 3.000],  loss: 19.339127, mse: 3305.564648, mean_q: 47.574940, mean_eps: 0.548359
  75400/150000: episode: 720, duration: 0.623s, episode steps:  89, steps per second: 143, episode reward: -61.608, mean reward: -0.692 [-100.000, 12.547], mean action: 1.562 [0.000, 3.000],  loss: 10.589098, mse: 3238.647993, mean_q: 45.860589, mean_eps: 0.547870
  75509/150000: episode: 721, duration: 0.768s, episode steps: 109, steps per second: 142, episode reward: -115.608, mean reward: -1.061 [-100.000,  6.573], mean action: 1.743 [0.000, 3.000],  loss: 14.530276, mse: 3246.332544, mean_q: 45.526501, mean_eps: 0.547276
  75612/150000: episode: 722, duration: 0.761s, episode steps: 103, steps per second: 135, episode reward: -67.925, mean reward: -0.659 [-100.000,  7.823], mean action: 1.621 [0.000, 3.000],  loss: 13.788319, mse: 3240.844063, mean_q: 46.768215, mean_eps: 0.546640
  75757/150000: episode: 723, duration: 1.054s, episode steps: 145, steps per second: 138, episode reward: -59.293, mean reward: -0.409 [-100.000, 13.425], mean action: 1.834 [0.000, 3.000],  loss: 13.721979, mse: 3172.695373, mean_q: 45.626312, mean_eps: 0.545896
  75859/150000: episode: 724, duration: 0.898s, episode steps: 102, steps per second: 114, episode reward: -93.399, mean reward: -0.916 [-100.000, 13.456], mean action: 1.833 [0.000, 3.000],  loss: 14.046881, mse: 3228.218661, mean_q: 47.419768, mean_eps: 0.545155
  76001/150000: episode: 725, duration: 1.222s, episode steps: 142, steps per second: 116, episode reward: -191.980, mean reward: -1.352 [-100.000, 49.637], mean action: 1.599 [0.000, 3.000],  loss: 13.965916, mse: 3191.967823, mean_q: 46.394296, mean_eps: 0.544423
  76149/150000: episode: 726, duration: 1.216s, episode steps: 148, steps per second: 122, episode reward: -258.040, mean reward: -1.744 [-100.000, 70.965], mean action: 1.588 [0.000, 3.000],  loss: 11.612143, mse: 3196.533700, mean_q: 45.320425, mean_eps: 0.543553
  76291/150000: episode: 727, duration: 1.105s, episode steps: 142, steps per second: 128, episode reward: -55.015, mean reward: -0.387 [-100.000,  7.029], mean action: 1.563 [0.000, 3.000],  loss: 18.693378, mse: 3240.234507, mean_q: 46.053321, mean_eps: 0.542683
  76370/150000: episode: 728, duration: 0.715s, episode steps:  79, steps per second: 111, episode reward: -30.783, mean reward: -0.390 [-100.000, 10.219], mean action: 1.734 [0.000, 3.000],  loss: 14.864002, mse: 3191.487014, mean_q: 45.618768, mean_eps: 0.542020
  76482/150000: episode: 729, duration: 0.873s, episode steps: 112, steps per second: 128, episode reward: -62.630, mean reward: -0.559 [-100.000, 17.101], mean action: 1.554 [0.000, 3.000],  loss: 12.636246, mse: 3184.125181, mean_q: 45.063685, mean_eps: 0.541447
  76598/150000: episode: 730, duration: 0.997s, episode steps: 116, steps per second: 116, episode reward: -85.756, mean reward: -0.739 [-100.000, 30.803], mean action: 1.724 [0.000, 3.000],  loss: 17.500143, mse: 3209.090656, mean_q: 45.494128, mean_eps: 0.540763
  76709/150000: episode: 731, duration: 0.863s, episode steps: 111, steps per second: 129, episode reward: -36.706, mean reward: -0.331 [-100.000, 11.175], mean action: 1.604 [0.000, 3.000],  loss: 12.658763, mse: 3230.218889, mean_q: 44.915233, mean_eps: 0.540082
  76832/150000: episode: 732, duration: 1.073s, episode steps: 123, steps per second: 115, episode reward: -43.549, mean reward: -0.354 [-100.000, 10.134], mean action: 1.659 [0.000, 3.000],  loss: 14.251865, mse: 3226.241503, mean_q: 45.302049, mean_eps: 0.539380
  76971/150000: episode: 733, duration: 1.437s, episode steps: 139, steps per second:  97, episode reward: -101.628, mean reward: -0.731 [-100.000, 10.214], mean action: 1.619 [0.000, 3.000],  loss: 13.674724, mse: 3216.966639, mean_q: 45.495095, mean_eps: 0.538594
  77078/150000: episode: 734, duration: 1.036s, episode steps: 107, steps per second: 103, episode reward: -17.299, mean reward: -0.162 [-100.000, 12.509], mean action: 1.804 [0.000, 3.000],  loss: 18.468772, mse: 3234.304012, mean_q: 44.901727, mean_eps: 0.537856
  77158/150000: episode: 735, duration: 0.818s, episode steps:  80, steps per second:  98, episode reward: -50.404, mean reward: -0.630 [-100.000,  5.953], mean action: 1.775 [0.000, 3.000],  loss: 12.068237, mse: 3220.661664, mean_q: 45.734051, mean_eps: 0.537295
  77261/150000: episode: 736, duration: 1.245s, episode steps: 103, steps per second:  83, episode reward: -51.027, mean reward: -0.495 [-100.000,  9.154], mean action: 1.583 [0.000, 3.000],  loss: 14.592316, mse: 3255.356822, mean_q: 45.981759, mean_eps: 0.536746
  77360/150000: episode: 737, duration: 0.796s, episode steps:  99, steps per second: 124, episode reward: -92.005, mean reward: -0.929 [-100.000,  9.897], mean action: 1.606 [0.000, 3.000],  loss: 12.106009, mse: 3227.581185, mean_q: 45.884803, mean_eps: 0.536140
  77508/150000: episode: 738, duration: 1.046s, episode steps: 148, steps per second: 141, episode reward: -86.315, mean reward: -0.583 [-100.000,  6.714], mean action: 1.399 [0.000, 3.000],  loss: 13.017951, mse: 3175.571414, mean_q: 44.809438, mean_eps: 0.535399
  77861/150000: episode: 739, duration: 3.352s, episode steps: 353, steps per second: 105, episode reward: -64.369, mean reward: -0.182 [-100.000, 16.541], mean action: 1.660 [0.000, 3.000],  loss: 13.515313, mse: 3215.018367, mean_q: 45.460591, mean_eps: 0.533896
  77975/150000: episode: 740, duration: 0.903s, episode steps: 114, steps per second: 126, episode reward: -73.973, mean reward: -0.649 [-100.000, 11.634], mean action: 1.667 [0.000, 3.000],  loss: 16.643180, mse: 3193.328260, mean_q: 45.262880, mean_eps: 0.532495
  78077/150000: episode: 741, duration: 0.892s, episode steps: 102, steps per second: 114, episode reward: -48.726, mean reward: -0.478 [-100.000, 13.339], mean action: 1.775 [0.000, 3.000],  loss: 10.607100, mse: 3180.382071, mean_q: 44.799394, mean_eps: 0.531847
  78243/150000: episode: 742, duration: 1.313s, episode steps: 166, steps per second: 126, episode reward: -202.337, mean reward: -1.219 [-100.000,  4.137], mean action: 1.669 [0.000, 3.000],  loss: 15.093155, mse: 3244.614636, mean_q: 46.631750, mean_eps: 0.531043
  78381/150000: episode: 743, duration: 1.035s, episode steps: 138, steps per second: 133, episode reward: -27.860, mean reward: -0.202 [-100.000, 12.284], mean action: 1.674 [0.000, 3.000],  loss: 15.219836, mse: 3159.520729, mean_q: 45.341534, mean_eps: 0.530131
  78481/150000: episode: 744, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: -9.040, mean reward: -0.090 [-100.000, 15.651], mean action: 1.620 [0.000, 3.000],  loss: 13.791678, mse: 3154.894998, mean_q: 44.958375, mean_eps: 0.529417
  78558/150000: episode: 745, duration: 0.553s, episode steps:  77, steps per second: 139, episode reward: -76.048, mean reward: -0.988 [-100.000,  6.420], mean action: 1.636 [0.000, 3.000],  loss: 10.477175, mse: 3198.913365, mean_q: 45.160375, mean_eps: 0.528886
  78628/150000: episode: 746, duration: 0.504s, episode steps:  70, steps per second: 139, episode reward: -36.912, mean reward: -0.527 [-100.000,  7.874], mean action: 1.686 [0.000, 3.000],  loss: 16.863388, mse: 3232.486293, mean_q: 47.314577, mean_eps: 0.528445
  79628/150000: episode: 747, duration: 9.151s, episode steps: 1000, steps per second: 109, episode reward: -10.234, mean reward: -0.010 [-24.669, 25.641], mean action: 1.870 [0.000, 3.000],  loss: 12.554712, mse: 3212.054943, mean_q: 46.280815, mean_eps: 0.525235
  79737/150000: episode: 748, duration: 0.799s, episode steps: 109, steps per second: 136, episode reward: -111.415, mean reward: -1.022 [-100.000,  9.534], mean action: 1.624 [0.000, 3.000],  loss: 12.609801, mse: 3184.990069, mean_q: 46.203456, mean_eps: 0.521908
  79819/150000: episode: 749, duration: 0.643s, episode steps:  82, steps per second: 127, episode reward: -39.597, mean reward: -0.483 [-100.000, 11.224], mean action: 1.756 [0.000, 3.000],  loss: 12.348690, mse: 3199.471269, mean_q: 45.868556, mean_eps: 0.521335
  79938/150000: episode: 750, duration: 0.906s, episode steps: 119, steps per second: 131, episode reward: -219.695, mean reward: -1.846 [-100.000, 48.886], mean action: 1.345 [0.000, 3.000],  loss: 13.670553, mse: 3183.164024, mean_q: 46.031193, mean_eps: 0.520732
  80080/150000: episode: 751, duration: 1.004s, episode steps: 142, steps per second: 141, episode reward: -41.853, mean reward: -0.295 [-100.000, 13.947], mean action: 1.570 [0.000, 3.000],  loss: 10.823299, mse: 3238.741275, mean_q: 46.886096, mean_eps: 0.519949
  80190/150000: episode: 752, duration: 0.820s, episode steps: 110, steps per second: 134, episode reward:  0.764, mean reward:  0.007 [-100.000, 93.244], mean action: 1.536 [0.000, 3.000],  loss: 16.580674, mse: 3248.914970, mean_q: 47.184064, mean_eps: 0.519193
  80274/150000: episode: 753, duration: 0.637s, episode steps:  84, steps per second: 132, episode reward: 40.383, mean reward:  0.481 [-100.000, 21.349], mean action: 1.726 [0.000, 3.000],  loss: 15.124537, mse: 3263.184001, mean_q: 45.885655, mean_eps: 0.518611
  80380/150000: episode: 754, duration: 0.786s, episode steps: 106, steps per second: 135, episode reward: 27.898, mean reward:  0.263 [-100.000, 37.263], mean action: 1.764 [0.000, 3.000],  loss: 12.437144, mse: 3205.399267, mean_q: 45.325436, mean_eps: 0.518041
  80503/150000: episode: 755, duration: 1.006s, episode steps: 123, steps per second: 122, episode reward: -76.173, mean reward: -0.619 [-100.000, 20.175], mean action: 1.512 [0.000, 3.000],  loss: 10.525526, mse: 3197.444701, mean_q: 47.495432, mean_eps: 0.517354
  80660/150000: episode: 756, duration: 1.231s, episode steps: 157, steps per second: 127, episode reward: -145.995, mean reward: -0.930 [-100.000,  5.337], mean action: 1.643 [0.000, 3.000],  loss: 22.714817, mse: 3203.344215, mean_q: 45.219761, mean_eps: 0.516514
  80727/150000: episode: 757, duration: 0.519s, episode steps:  67, steps per second: 129, episode reward: -27.333, mean reward: -0.408 [-100.000, 13.865], mean action: 1.716 [0.000, 3.000],  loss: 12.854527, mse: 3203.010083, mean_q: 45.895407, mean_eps: 0.515842
  80803/150000: episode: 758, duration: 0.563s, episode steps:  76, steps per second: 135, episode reward: -21.373, mean reward: -0.281 [-100.000, 16.759], mean action: 1.803 [0.000, 3.000],  loss: 17.168329, mse: 3318.325661, mean_q: 46.911013, mean_eps: 0.515413
  80892/150000: episode: 759, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: -104.639, mean reward: -1.176 [-100.000, 10.131], mean action: 1.562 [0.000, 3.000],  loss: 19.872356, mse: 3188.121648, mean_q: 46.412864, mean_eps: 0.514918
  80996/150000: episode: 760, duration: 0.732s, episode steps: 104, steps per second: 142, episode reward: 12.083, mean reward:  0.116 [-100.000, 14.752], mean action: 1.587 [0.000, 3.000],  loss: 14.117930, mse: 3187.226612, mean_q: 45.944515, mean_eps: 0.514339
  81110/150000: episode: 761, duration: 0.874s, episode steps: 114, steps per second: 130, episode reward: -7.532, mean reward: -0.066 [-100.000, 12.077], mean action: 1.851 [0.000, 3.000],  loss: 20.287709, mse: 3216.573426, mean_q: 47.327905, mean_eps: 0.513685
  81249/150000: episode: 762, duration: 0.985s, episode steps: 139, steps per second: 141, episode reward: 40.528, mean reward:  0.292 [-100.000, 17.734], mean action: 1.647 [0.000, 3.000],  loss: 17.376478, mse: 3235.865382, mean_q: 46.441743, mean_eps: 0.512926
  81366/150000: episode: 763, duration: 0.896s, episode steps: 117, steps per second: 131, episode reward: -80.318, mean reward: -0.686 [-100.000, 18.161], mean action: 1.641 [0.000, 3.000],  loss: 10.638157, mse: 3253.156250, mean_q: 47.015637, mean_eps: 0.512158
  81467/150000: episode: 764, duration: 0.731s, episode steps: 101, steps per second: 138, episode reward: -122.933, mean reward: -1.217 [-100.000, 15.367], mean action: 1.594 [0.000, 3.000],  loss: 12.244375, mse: 3226.328903, mean_q: 47.198989, mean_eps: 0.511504
  81604/150000: episode: 765, duration: 0.956s, episode steps: 137, steps per second: 143, episode reward: -75.479, mean reward: -0.551 [-100.000, 39.898], mean action: 1.474 [0.000, 3.000],  loss: 13.367466, mse: 3238.099627, mean_q: 47.081972, mean_eps: 0.510790
  81713/150000: episode: 766, duration: 0.824s, episode steps: 109, steps per second: 132, episode reward: 22.419, mean reward:  0.206 [-100.000, 23.074], mean action: 1.642 [0.000, 3.000],  loss: 15.920661, mse: 3207.819051, mean_q: 47.369974, mean_eps: 0.510052
  81883/150000: episode: 767, duration: 1.248s, episode steps: 170, steps per second: 136, episode reward: -56.382, mean reward: -0.332 [-100.000, 15.112], mean action: 1.653 [0.000, 3.000],  loss: 15.840104, mse: 3274.428276, mean_q: 47.124544, mean_eps: 0.509215
  81976/150000: episode: 768, duration: 0.750s, episode steps:  93, steps per second: 124, episode reward: -3.718, mean reward: -0.040 [-100.000,  8.072], mean action: 1.613 [0.000, 3.000],  loss: 15.819045, mse: 3137.709685, mean_q: 45.115497, mean_eps: 0.508426
  82115/150000: episode: 769, duration: 1.100s, episode steps: 139, steps per second: 126, episode reward:  4.828, mean reward:  0.035 [-100.000, 16.871], mean action: 1.827 [0.000, 3.000],  loss: 17.784571, mse: 3319.663954, mean_q: 48.803233, mean_eps: 0.507730
  82228/150000: episode: 770, duration: 0.930s, episode steps: 113, steps per second: 122, episode reward: -30.355, mean reward: -0.269 [-100.000, 15.481], mean action: 1.673 [0.000, 3.000],  loss: 16.000435, mse: 3334.964239, mean_q: 48.931003, mean_eps: 0.506974
  83162/150000: episode: 771, duration: 8.149s, episode steps: 934, steps per second: 115, episode reward: -254.319, mean reward: -0.272 [-100.000, 29.529], mean action: 1.875 [0.000, 3.000],  loss: 14.997352, mse: 3325.871614, mean_q: 48.877305, mean_eps: 0.503833
  83310/150000: episode: 772, duration: 1.321s, episode steps: 148, steps per second: 112, episode reward: -73.940, mean reward: -0.500 [-100.000, 13.303], mean action: 1.703 [0.000, 3.000],  loss: 16.300538, mse: 3371.690553, mean_q: 50.302833, mean_eps: 0.500587
  83472/150000: episode: 773, duration: 1.231s, episode steps: 162, steps per second: 132, episode reward: -60.446, mean reward: -0.373 [-100.000, 13.708], mean action: 1.630 [0.000, 3.000],  loss: 21.388802, mse: 3342.551031, mean_q: 49.267225, mean_eps: 0.499657
  83555/150000: episode: 774, duration: 0.671s, episode steps:  83, steps per second: 124, episode reward: -19.067, mean reward: -0.230 [-100.000,  7.677], mean action: 1.747 [0.000, 3.000],  loss: 17.873423, mse: 3342.251277, mean_q: 49.510364, mean_eps: 0.498922
  83647/150000: episode: 775, duration: 0.666s, episode steps:  92, steps per second: 138, episode reward: -6.826, mean reward: -0.074 [-100.000, 14.929], mean action: 1.815 [0.000, 3.000],  loss: 13.362309, mse: 3373.029289, mean_q: 49.789160, mean_eps: 0.498397
  83746/150000: episode: 776, duration: 0.752s, episode steps:  99, steps per second: 132, episode reward: -118.240, mean reward: -1.194 [-100.000, 11.605], mean action: 1.848 [0.000, 3.000],  loss: 19.430772, mse: 3358.442432, mean_q: 49.175142, mean_eps: 0.497824
  83946/150000: episode: 777, duration: 1.734s, episode steps: 200, steps per second: 115, episode reward: -297.541, mean reward: -1.488 [-100.000, 11.351], mean action: 1.750 [0.000, 3.000],  loss: 20.407657, mse: 3352.253282, mean_q: 49.545010, mean_eps: 0.496927
  84113/150000: episode: 778, duration: 1.457s, episode steps: 167, steps per second: 115, episode reward: -316.270, mean reward: -1.894 [-100.000, 25.321], mean action: 1.593 [0.000, 3.000],  loss: 18.222414, mse: 3383.140679, mean_q: 49.469223, mean_eps: 0.495826
  84249/150000: episode: 779, duration: 1.160s, episode steps: 136, steps per second: 117, episode reward: -133.531, mean reward: -0.982 [-100.000,  4.747], mean action: 1.669 [0.000, 3.000],  loss: 19.932927, mse: 3400.614633, mean_q: 50.038964, mean_eps: 0.494917
  84429/150000: episode: 780, duration: 1.515s, episode steps: 180, steps per second: 119, episode reward: -122.148, mean reward: -0.679 [-100.000,  5.366], mean action: 1.628 [0.000, 3.000],  loss: 21.674811, mse: 3351.678177, mean_q: 49.069493, mean_eps: 0.493969
  84618/150000: episode: 781, duration: 1.424s, episode steps: 189, steps per second: 133, episode reward: -18.269, mean reward: -0.097 [-100.000, 24.153], mean action: 1.603 [0.000, 3.000],  loss: 16.403369, mse: 3361.001913, mean_q: 49.552826, mean_eps: 0.492862
  84716/150000: episode: 782, duration: 0.717s, episode steps:  98, steps per second: 137, episode reward: -69.616, mean reward: -0.710 [-100.000,  5.585], mean action: 1.755 [0.000, 3.000],  loss: 13.512548, mse: 3383.394992, mean_q: 50.143591, mean_eps: 0.492001
  84804/150000: episode: 783, duration: 0.685s, episode steps:  88, steps per second: 128, episode reward: -59.396, mean reward: -0.675 [-100.000, 18.538], mean action: 1.364 [0.000, 3.000],  loss: 18.174359, mse: 3343.537778, mean_q: 49.443153, mean_eps: 0.491443
  84935/150000: episode: 784, duration: 0.989s, episode steps: 131, steps per second: 132, episode reward: -197.681, mean reward: -1.509 [-100.000, 63.880], mean action: 1.611 [0.000, 3.000],  loss: 15.671346, mse: 3350.964124, mean_q: 49.466438, mean_eps: 0.490786
  85088/150000: episode: 785, duration: 1.218s, episode steps: 153, steps per second: 126, episode reward: -81.417, mean reward: -0.532 [-100.000,  4.598], mean action: 1.425 [0.000, 3.000],  loss: 19.433619, mse: 3347.145669, mean_q: 48.849007, mean_eps: 0.489934
  85242/150000: episode: 786, duration: 1.220s, episode steps: 154, steps per second: 126, episode reward: -85.014, mean reward: -0.552 [-100.000,  7.702], mean action: 1.727 [0.000, 3.000],  loss: 15.913004, mse: 3503.152829, mean_q: 50.438189, mean_eps: 0.489013
  85336/150000: episode: 787, duration: 0.930s, episode steps:  94, steps per second: 101, episode reward: -11.790, mean reward: -0.125 [-100.000, 13.979], mean action: 1.521 [0.000, 3.000],  loss: 13.655538, mse: 3399.529860, mean_q: 49.344580, mean_eps: 0.488269
  86336/150000: episode: 788, duration: 8.520s, episode steps: 1000, steps per second: 117, episode reward:  7.531, mean reward:  0.008 [-24.126, 75.024], mean action: 1.852 [0.000, 3.000],  loss: 16.983493, mse: 3439.033531, mean_q: 49.773263, mean_eps: 0.484987
  86570/150000: episode: 789, duration: 1.893s, episode steps: 234, steps per second: 124, episode reward: -201.439, mean reward: -0.861 [-100.000,  9.252], mean action: 1.692 [0.000, 3.000],  loss: 17.995502, mse: 3421.436521, mean_q: 50.256421, mean_eps: 0.481285
  86767/150000: episode: 790, duration: 1.501s, episode steps: 197, steps per second: 131, episode reward: -187.283, mean reward: -0.951 [-100.000, 10.644], mean action: 1.635 [0.000, 3.000],  loss: 13.715009, mse: 3485.830934, mean_q: 50.922085, mean_eps: 0.479992
  86906/150000: episode: 791, duration: 1.026s, episode steps: 139, steps per second: 135, episode reward: -86.047, mean reward: -0.619 [-100.000, 18.016], mean action: 1.705 [0.000, 3.000],  loss: 13.004192, mse: 3376.481421, mean_q: 48.673352, mean_eps: 0.478984
  87022/150000: episode: 792, duration: 0.865s, episode steps: 116, steps per second: 134, episode reward: -56.339, mean reward: -0.486 [-100.000, 11.983], mean action: 1.526 [0.000, 3.000],  loss: 12.891438, mse: 3442.370298, mean_q: 49.861999, mean_eps: 0.478219
  87153/150000: episode: 793, duration: 0.971s, episode steps: 131, steps per second: 135, episode reward: -31.906, mean reward: -0.244 [-100.000,  8.073], mean action: 1.656 [0.000, 3.000],  loss: 15.657289, mse: 3472.291549, mean_q: 50.882396, mean_eps: 0.477478
  87254/150000: episode: 794, duration: 0.852s, episode steps: 101, steps per second: 118, episode reward: 15.029, mean reward:  0.149 [-100.000, 17.292], mean action: 1.723 [0.000, 3.000],  loss: 21.210772, mse: 3469.776718, mean_q: 50.784423, mean_eps: 0.476782
  87396/150000: episode: 795, duration: 1.183s, episode steps: 142, steps per second: 120, episode reward: -17.106, mean reward: -0.120 [-100.000, 17.860], mean action: 1.761 [0.000, 3.000],  loss: 12.771620, mse: 3532.891185, mean_q: 50.561481, mean_eps: 0.476053
  88396/150000: episode: 796, duration: 8.671s, episode steps: 1000, steps per second: 115, episode reward: 22.667, mean reward:  0.023 [-24.569, 29.979], mean action: 1.950 [0.000, 3.000],  loss: 16.255869, mse: 3507.463401, mean_q: 50.872651, mean_eps: 0.472627
  88527/150000: episode: 797, duration: 1.039s, episode steps: 131, steps per second: 126, episode reward: 12.542, mean reward:  0.096 [-100.000, 17.318], mean action: 1.786 [0.000, 3.000],  loss: 16.723833, mse: 3514.853294, mean_q: 50.067576, mean_eps: 0.469234
  88686/150000: episode: 798, duration: 1.334s, episode steps: 159, steps per second: 119, episode reward: -90.957, mean reward: -0.572 [-100.000,  7.537], mean action: 1.604 [0.000, 3.000],  loss: 10.946371, mse: 3554.847774, mean_q: 51.349301, mean_eps: 0.468364
  88804/150000: episode: 799, duration: 0.915s, episode steps: 118, steps per second: 129, episode reward: 12.630, mean reward:  0.107 [-100.000, 15.508], mean action: 1.797 [0.000, 3.000],  loss: 19.269582, mse: 3553.837934, mean_q: 51.202972, mean_eps: 0.467533
  88923/150000: episode: 800, duration: 0.876s, episode steps: 119, steps per second: 136, episode reward: -47.294, mean reward: -0.397 [-100.000, 10.242], mean action: 1.790 [0.000, 3.000],  loss: 15.017459, mse: 3505.247343, mean_q: 49.981273, mean_eps: 0.466822
  89036/150000: episode: 801, duration: 0.878s, episode steps: 113, steps per second: 129, episode reward: -91.370, mean reward: -0.809 [-100.000,  8.659], mean action: 1.699 [0.000, 3.000],  loss: 13.442424, mse: 3569.855709, mean_q: 51.000910, mean_eps: 0.466126
  89141/150000: episode: 802, duration: 0.791s, episode steps: 105, steps per second: 133, episode reward: -39.094, mean reward: -0.372 [-100.000, 11.620], mean action: 1.571 [0.000, 3.000],  loss: 12.240696, mse: 3507.919487, mean_q: 50.379537, mean_eps: 0.465472
  89288/150000: episode: 803, duration: 1.149s, episode steps: 147, steps per second: 128, episode reward: -75.536, mean reward: -0.514 [-100.000,  4.455], mean action: 1.728 [0.000, 3.000],  loss: 16.461926, mse: 3487.574054, mean_q: 50.166398, mean_eps: 0.464716
  89397/150000: episode: 804, duration: 0.836s, episode steps: 109, steps per second: 130, episode reward: 13.499, mean reward:  0.124 [-100.000, 20.841], mean action: 1.697 [0.000, 3.000],  loss: 13.417509, mse: 3554.315600, mean_q: 50.504446, mean_eps: 0.463948
  89530/150000: episode: 805, duration: 0.965s, episode steps: 133, steps per second: 138, episode reward: -76.673, mean reward: -0.576 [-100.000, 12.636], mean action: 1.564 [0.000, 3.000],  loss: 13.601651, mse: 3505.122426, mean_q: 50.599486, mean_eps: 0.463222
  89913/150000: episode: 806, duration: 3.279s, episode steps: 383, steps per second: 117, episode reward: -210.917, mean reward: -0.551 [-100.000, 19.454], mean action: 1.697 [0.000, 3.000],  loss: 13.895895, mse: 3505.171918, mean_q: 50.408213, mean_eps: 0.461674
  90077/150000: episode: 807, duration: 1.457s, episode steps: 164, steps per second: 113, episode reward: -79.546, mean reward: -0.485 [-100.000, 16.923], mean action: 1.744 [0.000, 3.000],  loss: 12.689325, mse: 3570.240091, mean_q: 51.383816, mean_eps: 0.460033
  90188/150000: episode: 808, duration: 0.912s, episode steps: 111, steps per second: 122, episode reward: -86.911, mean reward: -0.783 [-100.000,  9.740], mean action: 1.333 [0.000, 3.000],  loss: 11.396443, mse: 3553.688175, mean_q: 51.923579, mean_eps: 0.459208
  90308/150000: episode: 809, duration: 1.016s, episode steps: 120, steps per second: 118, episode reward: -48.569, mean reward: -0.405 [-100.000, 10.326], mean action: 1.700 [0.000, 3.000],  loss: 16.914812, mse: 3566.547919, mean_q: 51.592292, mean_eps: 0.458515
  90435/150000: episode: 810, duration: 1.163s, episode steps: 127, steps per second: 109, episode reward: -2.284, mean reward: -0.018 [-100.000, 20.340], mean action: 1.535 [0.000, 3.000],  loss: 15.530177, mse: 3599.507795, mean_q: 52.237737, mean_eps: 0.457774
  90566/150000: episode: 811, duration: 1.192s, episode steps: 131, steps per second: 110, episode reward: 10.976, mean reward:  0.084 [-100.000, 20.290], mean action: 1.649 [0.000, 3.000],  loss: 11.892234, mse: 3576.046892, mean_q: 51.567719, mean_eps: 0.457000
  90668/150000: episode: 812, duration: 0.946s, episode steps: 102, steps per second: 108, episode reward: -6.092, mean reward: -0.060 [-100.000, 21.197], mean action: 1.716 [0.000, 3.000],  loss: 18.124510, mse: 3577.995814, mean_q: 52.299897, mean_eps: 0.456301
  91668/150000: episode: 813, duration: 9.200s, episode steps: 1000, steps per second: 109, episode reward: 29.061, mean reward:  0.029 [-22.968, 24.042], mean action: 1.650 [0.000, 3.000],  loss: 14.892603, mse: 3583.296277, mean_q: 51.687201, mean_eps: 0.452995
  91786/150000: episode: 814, duration: 1.050s, episode steps: 118, steps per second: 112, episode reward: -41.550, mean reward: -0.352 [-100.000, 13.967], mean action: 1.703 [0.000, 3.000],  loss: 16.645682, mse: 3634.500681, mean_q: 51.962344, mean_eps: 0.449641
  91942/150000: episode: 815, duration: 1.392s, episode steps: 156, steps per second: 112, episode reward:  6.395, mean reward:  0.041 [-100.000, 23.789], mean action: 1.596 [0.000, 3.000],  loss: 14.833049, mse: 3666.104624, mean_q: 52.766955, mean_eps: 0.448819
  92061/150000: episode: 816, duration: 1.029s, episode steps: 119, steps per second: 116, episode reward: -47.336, mean reward: -0.398 [-100.000, 12.248], mean action: 1.521 [0.000, 3.000],  loss: 15.377747, mse: 3626.717445, mean_q: 52.012576, mean_eps: 0.447994
  92237/150000: episode: 817, duration: 1.339s, episode steps: 176, steps per second: 131, episode reward: -114.255, mean reward: -0.649 [-100.000,  3.756], mean action: 1.614 [0.000, 3.000],  loss: 16.940694, mse: 3759.337597, mean_q: 54.127755, mean_eps: 0.447109
  92360/150000: episode: 818, duration: 0.929s, episode steps: 123, steps per second: 132, episode reward: -123.557, mean reward: -1.005 [-100.000, 15.297], mean action: 1.520 [0.000, 3.000],  loss: 23.355672, mse: 3684.533052, mean_q: 52.714539, mean_eps: 0.446212
  93360/150000: episode: 819, duration: 7.955s, episode steps: 1000, steps per second: 126, episode reward: 36.870, mean reward:  0.037 [-21.696, 26.028], mean action: 1.389 [0.000, 3.000],  loss: 15.670450, mse: 3690.131662, mean_q: 53.269297, mean_eps: 0.442843
  93502/150000: episode: 820, duration: 1.065s, episode steps: 142, steps per second: 133, episode reward: -53.466, mean reward: -0.377 [-100.000, 22.101], mean action: 1.669 [0.000, 3.000],  loss: 18.532847, mse: 3702.531453, mean_q: 53.545228, mean_eps: 0.439417
  93655/150000: episode: 821, duration: 1.147s, episode steps: 153, steps per second: 133, episode reward: -69.677, mean reward: -0.455 [-100.000,  7.886], mean action: 1.621 [0.000, 3.000],  loss: 18.992735, mse: 3739.740668, mean_q: 52.696756, mean_eps: 0.438532
  93742/150000: episode: 822, duration: 0.652s, episode steps:  87, steps per second: 133, episode reward: -87.666, mean reward: -1.008 [-100.000,  8.628], mean action: 1.345 [0.000, 3.000],  loss: 19.359351, mse: 3750.544540, mean_q: 54.016603, mean_eps: 0.437812
  94608/150000: episode: 823, duration: 7.418s, episode steps: 866, steps per second: 117, episode reward: -166.883, mean reward: -0.193 [-100.000, 29.448], mean action: 1.594 [0.000, 3.000],  loss: 14.866157, mse: 3713.235868, mean_q: 53.779555, mean_eps: 0.434953
  95608/150000: episode: 824, duration: 9.195s, episode steps: 1000, steps per second: 109, episode reward: 77.275, mean reward:  0.077 [-22.020, 24.062], mean action: 1.949 [0.000, 3.000],  loss: 18.404318, mse: 3695.365486, mean_q: 52.842717, mean_eps: 0.429355
  96608/150000: episode: 825, duration: 9.232s, episode steps: 1000, steps per second: 108, episode reward: 35.690, mean reward:  0.036 [-22.320, 28.762], mean action: 1.622 [0.000, 3.000],  loss: 16.567623, mse: 3591.741947, mean_q: 52.217879, mean_eps: 0.423355
  97608/150000: episode: 826, duration: 8.409s, episode steps: 1000, steps per second: 119, episode reward: 22.612, mean reward:  0.023 [-22.494, 25.140], mean action: 1.493 [0.000, 3.000],  loss: 18.607811, mse: 3631.626261, mean_q: 53.270605, mean_eps: 0.417355
  97877/150000: episode: 827, duration: 2.627s, episode steps: 269, steps per second: 102, episode reward: -1.478, mean reward: -0.005 [-100.000, 13.095], mean action: 1.688 [0.000, 3.000],  loss: 18.060594, mse: 3695.423449, mean_q: 54.426825, mean_eps: 0.413548
  97993/150000: episode: 828, duration: 0.935s, episode steps: 116, steps per second: 124, episode reward: -13.295, mean reward: -0.115 [-100.000, 19.289], mean action: 1.767 [0.000, 3.000],  loss: 21.318549, mse: 3614.837956, mean_q: 54.167992, mean_eps: 0.412393
  98072/150000: episode: 829, duration: 0.598s, episode steps:  79, steps per second: 132, episode reward: -4.411, mean reward: -0.056 [-100.000, 13.512], mean action: 1.544 [0.000, 3.000],  loss: 17.700052, mse: 3676.099013, mean_q: 54.298642, mean_eps: 0.411808
  98266/150000: episode: 830, duration: 1.513s, episode steps: 194, steps per second: 128, episode reward: -63.698, mean reward: -0.328 [-100.000, 18.974], mean action: 1.655 [0.000, 3.000],  loss: 17.828002, mse: 3656.304354, mean_q: 54.790069, mean_eps: 0.410989
  99266/150000: episode: 831, duration: 7.841s, episode steps: 1000, steps per second: 128, episode reward: 84.924, mean reward:  0.085 [-22.347, 25.604], mean action: 1.853 [0.000, 3.000],  loss: 16.283044, mse: 3678.681457, mean_q: 54.294757, mean_eps: 0.407407
 100266/150000: episode: 832, duration: 8.333s, episode steps: 1000, steps per second: 120, episode reward: 56.561, mean reward:  0.057 [-21.574, 23.147], mean action: 1.274 [0.000, 3.000],  loss: 18.031338, mse: 3719.204001, mean_q: 55.517352, mean_eps: 0.401407
 101266/150000: episode: 833, duration: 7.852s, episode steps: 1000, steps per second: 127, episode reward: -10.208, mean reward: -0.010 [-22.447, 22.663], mean action: 1.442 [0.000, 3.000],  loss: 16.166506, mse: 3739.041916, mean_q: 55.929713, mean_eps: 0.395407
 102266/150000: episode: 834, duration: 8.040s, episode steps: 1000, steps per second: 124, episode reward: 76.963, mean reward:  0.077 [-20.322, 23.306], mean action: 1.817 [0.000, 3.000],  loss: 17.547712, mse: 3716.422240, mean_q: 55.890687, mean_eps: 0.389407
 102554/150000: episode: 835, duration: 2.263s, episode steps: 288, steps per second: 127, episode reward: -174.105, mean reward: -0.605 [-100.000, 12.886], mean action: 1.611 [0.000, 3.000],  loss: 13.206402, mse: 3706.467196, mean_q: 56.133462, mean_eps: 0.385543
 102664/150000: episode: 836, duration: 0.776s, episode steps: 110, steps per second: 142, episode reward: 29.658, mean reward:  0.270 [-100.000, 25.033], mean action: 1.718 [0.000, 3.000],  loss: 13.687382, mse: 3692.881037, mean_q: 56.752650, mean_eps: 0.384349
 102778/150000: episode: 837, duration: 0.839s, episode steps: 114, steps per second: 136, episode reward: -9.367, mean reward: -0.082 [-100.000, 16.114], mean action: 1.667 [0.000, 3.000],  loss: 17.734024, mse: 3694.710681, mean_q: 55.867150, mean_eps: 0.383677
 102860/150000: episode: 838, duration: 0.576s, episode steps:  82, steps per second: 142, episode reward: -12.646, mean reward: -0.154 [-100.000, 10.494], mean action: 1.439 [0.000, 3.000],  loss: 15.622039, mse: 3679.871341, mean_q: 55.952338, mean_eps: 0.383089
 102962/150000: episode: 839, duration: 0.721s, episode steps: 102, steps per second: 142, episode reward:  0.852, mean reward:  0.008 [-100.000, 17.460], mean action: 1.833 [0.000, 3.000],  loss: 15.121020, mse: 3707.860103, mean_q: 56.918558, mean_eps: 0.382537
 103962/150000: episode: 840, duration: 7.706s, episode steps: 1000, steps per second: 130, episode reward: 47.007, mean reward:  0.047 [-20.949, 22.122], mean action: 2.144 [0.000, 3.000],  loss: 16.161950, mse: 3737.283134, mean_q: 56.950994, mean_eps: 0.379231
 104962/150000: episode: 841, duration: 7.813s, episode steps: 1000, steps per second: 128, episode reward: 108.920, mean reward:  0.109 [-24.201, 21.681], mean action: 1.354 [0.000, 3.000],  loss: 15.154661, mse: 3722.417501, mean_q: 57.550244, mean_eps: 0.373231
 105291/150000: episode: 842, duration: 2.410s, episode steps: 329, steps per second: 137, episode reward: -5.774, mean reward: -0.018 [-100.000, 14.001], mean action: 1.802 [0.000, 3.000],  loss: 15.883027, mse: 3686.049846, mean_q: 58.057691, mean_eps: 0.369244
 106291/150000: episode: 843, duration: 7.868s, episode steps: 1000, steps per second: 127, episode reward: 138.175, mean reward:  0.138 [-23.417, 30.421], mean action: 1.072 [0.000, 3.000],  loss: 17.124964, mse: 3655.062408, mean_q: 57.533620, mean_eps: 0.365257
 107291/150000: episode: 844, duration: 7.742s, episode steps: 1000, steps per second: 129, episode reward: 52.421, mean reward:  0.052 [-22.998, 23.650], mean action: 1.397 [0.000, 3.000],  loss: 14.992925, mse: 3670.458652, mean_q: 58.195747, mean_eps: 0.359257
 107425/150000: episode: 845, duration: 1.009s, episode steps: 134, steps per second: 133, episode reward: 14.669, mean reward:  0.109 [-100.000, 17.055], mean action: 1.754 [0.000, 3.000],  loss: 14.145978, mse: 3728.699075, mean_q: 58.624650, mean_eps: 0.355855
 108425/150000: episode: 846, duration: 8.192s, episode steps: 1000, steps per second: 122, episode reward: 93.484, mean reward:  0.093 [-22.654, 23.425], mean action: 1.258 [0.000, 3.000],  loss: 13.616724, mse: 3715.149366, mean_q: 59.145708, mean_eps: 0.352453
 109425/150000: episode: 847, duration: 7.691s, episode steps: 1000, steps per second: 130, episode reward: 147.085, mean reward:  0.147 [-21.745, 23.075], mean action: 1.403 [0.000, 3.000],  loss: 17.422676, mse: 3754.719483, mean_q: 59.516211, mean_eps: 0.346453
 110425/150000: episode: 848, duration: 7.941s, episode steps: 1000, steps per second: 126, episode reward: 95.021, mean reward:  0.095 [-21.141, 23.646], mean action: 1.102 [0.000, 3.000],  loss: 14.508974, mse: 3732.762575, mean_q: 59.870983, mean_eps: 0.340453
 111425/150000: episode: 849, duration: 7.628s, episode steps: 1000, steps per second: 131, episode reward: 149.491, mean reward:  0.149 [-20.395, 22.139], mean action: 1.281 [0.000, 3.000],  loss: 15.817890, mse: 3742.892650, mean_q: 60.220892, mean_eps: 0.334453
 111556/150000: episode: 850, duration: 0.919s, episode steps: 131, steps per second: 143, episode reward: 51.137, mean reward:  0.390 [-100.000, 16.126], mean action: 1.779 [0.000, 3.000],  loss: 17.078008, mse: 3786.026606, mean_q: 61.129659, mean_eps: 0.331060
 112556/150000: episode: 851, duration: 7.790s, episode steps: 1000, steps per second: 128, episode reward: 96.086, mean reward:  0.096 [-24.131, 25.031], mean action: 1.357 [0.000, 3.000],  loss: 15.113559, mse: 3805.784567, mean_q: 61.294612, mean_eps: 0.327667
 113556/150000: episode: 852, duration: 7.496s, episode steps: 1000, steps per second: 133, episode reward:  7.173, mean reward:  0.007 [-24.181, 24.519], mean action: 1.321 [0.000, 3.000],  loss: 14.897530, mse: 3866.836381, mean_q: 62.392726, mean_eps: 0.321667
 114556/150000: episode: 853, duration: 7.952s, episode steps: 1000, steps per second: 126, episode reward: 36.189, mean reward:  0.036 [-20.751, 50.164], mean action: 1.095 [0.000, 3.000],  loss: 17.738597, mse: 3941.608826, mean_q: 63.225638, mean_eps: 0.315667
 114903/150000: episode: 854, duration: 2.664s, episode steps: 347, steps per second: 130, episode reward: -163.443, mean reward: -0.471 [-100.000,  4.909], mean action: 1.769 [0.000, 3.000],  loss: 16.586090, mse: 3973.884033, mean_q: 64.249450, mean_eps: 0.311626
 115903/150000: episode: 855, duration: 7.676s, episode steps: 1000, steps per second: 130, episode reward: -9.906, mean reward: -0.010 [-21.410, 22.441], mean action: 1.428 [0.000, 3.000],  loss: 15.171165, mse: 3944.787225, mean_q: 63.814835, mean_eps: 0.307585
 116903/150000: episode: 856, duration: 8.221s, episode steps: 1000, steps per second: 122, episode reward: 16.885, mean reward:  0.017 [-21.336, 26.996], mean action: 1.256 [0.000, 3.000],  loss: 13.916000, mse: 3942.239724, mean_q: 64.203704, mean_eps: 0.301585
 117903/150000: episode: 857, duration: 7.656s, episode steps: 1000, steps per second: 131, episode reward: 158.139, mean reward:  0.158 [-23.288, 22.253], mean action: 1.044 [0.000, 3.000],  loss: 15.135615, mse: 3967.429200, mean_q: 64.882074, mean_eps: 0.295585
 118903/150000: episode: 858, duration: 7.455s, episode steps: 1000, steps per second: 134, episode reward: 58.224, mean reward:  0.058 [-25.232, 24.458], mean action: 1.051 [0.000, 3.000],  loss: 14.450314, mse: 3917.932457, mean_q: 64.980068, mean_eps: 0.289585
 119009/150000: episode: 859, duration: 0.792s, episode steps: 106, steps per second: 134, episode reward: -0.930, mean reward: -0.009 [-100.000, 22.308], mean action: 1.585 [0.000, 3.000],  loss: 10.237678, mse: 3959.161029, mean_q: 64.924685, mean_eps: 0.286267
 120009/150000: episode: 860, duration: 7.553s, episode steps: 1000, steps per second: 132, episode reward: 168.724, mean reward:  0.169 [-21.344, 23.626], mean action: 1.607 [0.000, 3.000],  loss: 15.328348, mse: 3915.480172, mean_q: 65.284273, mean_eps: 0.282949
 121009/150000: episode: 861, duration: 7.852s, episode steps: 1000, steps per second: 127, episode reward: 158.509, mean reward:  0.159 [-21.320, 24.728], mean action: 1.214 [0.000, 3.000],  loss: 14.372060, mse: 3910.049347, mean_q: 65.182716, mean_eps: 0.276949
 122009/150000: episode: 862, duration: 7.645s, episode steps: 1000, steps per second: 131, episode reward: 102.427, mean reward:  0.102 [-20.860, 25.274], mean action: 1.200 [0.000, 3.000],  loss: 15.726565, mse: 3840.268871, mean_q: 64.644948, mean_eps: 0.270949
 123009/150000: episode: 863, duration: 8.118s, episode steps: 1000, steps per second: 123, episode reward: 133.576, mean reward:  0.134 [-19.893, 23.371], mean action: 1.227 [0.000, 3.000],  loss: 13.955584, mse: 3832.686259, mean_q: 64.930242, mean_eps: 0.264949
 124009/150000: episode: 864, duration: 8.428s, episode steps: 1000, steps per second: 119, episode reward: 100.833, mean reward:  0.101 [-23.426, 23.158], mean action: 1.365 [0.000, 3.000],  loss: 13.019615, mse: 3742.830300, mean_q: 64.193256, mean_eps: 0.258949
 125009/150000: episode: 865, duration: 8.176s, episode steps: 1000, steps per second: 122, episode reward: 148.990, mean reward:  0.149 [-21.209, 22.768], mean action: 1.295 [0.000, 3.000],  loss: 11.561194, mse: 3762.852804, mean_q: 64.726270, mean_eps: 0.252949
 125476/150000: episode: 866, duration: 3.837s, episode steps: 467, steps per second: 122, episode reward: 184.217, mean reward:  0.394 [-19.893, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 15.998308, mse: 3747.504230, mean_q: 64.574615, mean_eps: 0.248548
 126476/150000: episode: 867, duration: 9.008s, episode steps: 1000, steps per second: 111, episode reward: 135.816, mean reward:  0.136 [-20.460, 23.052], mean action: 1.311 [0.000, 3.000],  loss: 12.140493, mse: 3760.405369, mean_q: 65.144814, mean_eps: 0.244147
 127040/150000: episode: 868, duration: 4.440s, episode steps: 564, steps per second: 127, episode reward: 148.049, mean reward:  0.262 [-19.366, 100.000], mean action: 1.461 [0.000, 3.000],  loss: 12.388246, mse: 3787.415666, mean_q: 65.677042, mean_eps: 0.239455
 128040/150000: episode: 869, duration: 7.925s, episode steps: 1000, steps per second: 126, episode reward: 63.909, mean reward:  0.064 [-20.449, 15.602], mean action: 1.395 [0.000, 3.000],  loss: 13.656787, mse: 3764.750577, mean_q: 65.713137, mean_eps: 0.234763
 129040/150000: episode: 870, duration: 7.969s, episode steps: 1000, steps per second: 125, episode reward: 111.415, mean reward:  0.111 [-21.031, 22.720], mean action: 1.045 [0.000, 3.000],  loss: 11.865441, mse: 3696.853182, mean_q: 65.218014, mean_eps: 0.228763
 130040/150000: episode: 871, duration: 8.074s, episode steps: 1000, steps per second: 124, episode reward: 118.287, mean reward:  0.118 [-19.704, 23.323], mean action: 1.268 [0.000, 3.000],  loss: 10.979350, mse: 3648.206217, mean_q: 64.793506, mean_eps: 0.222763
 131040/150000: episode: 872, duration: 7.642s, episode steps: 1000, steps per second: 131, episode reward: 130.024, mean reward:  0.130 [-18.998, 14.417], mean action: 1.337 [0.000, 3.000],  loss: 11.572375, mse: 3639.402894, mean_q: 65.019642, mean_eps: 0.216763
 131450/150000: episode: 873, duration: 3.369s, episode steps: 410, steps per second: 122, episode reward: -275.597, mean reward: -0.672 [-100.000, 30.840], mean action: 1.729 [0.000, 3.000],  loss: 8.215383, mse: 3570.939296, mean_q: 64.379570, mean_eps: 0.212533
 131807/150000: episode: 874, duration: 2.585s, episode steps: 357, steps per second: 138, episode reward: 36.923, mean reward:  0.103 [-100.000, 21.488], mean action: 1.350 [0.000, 3.000],  loss: 11.268271, mse: 3559.181642, mean_q: 64.240658, mean_eps: 0.210232
 132807/150000: episode: 875, duration: 7.869s, episode steps: 1000, steps per second: 127, episode reward: 112.560, mean reward:  0.113 [-20.056, 23.094], mean action: 1.293 [0.000, 3.000],  loss: 9.726456, mse: 3483.464461, mean_q: 63.559235, mean_eps: 0.206161
 133299/150000: episode: 876, duration: 4.453s, episode steps: 492, steps per second: 110, episode reward: 242.633, mean reward:  0.493 [-19.797, 100.000], mean action: 1.425 [0.000, 3.000],  loss: 9.121234, mse: 3401.288325, mean_q: 62.840804, mean_eps: 0.201685
 133481/150000: episode: 877, duration: 1.300s, episode steps: 182, steps per second: 140, episode reward: 27.661, mean reward:  0.152 [-100.000,  9.274], mean action: 1.808 [0.000, 3.000],  loss: 6.376387, mse: 3375.219338, mean_q: 62.609125, mean_eps: 0.199663
 134018/150000: episode: 878, duration: 3.994s, episode steps: 537, steps per second: 134, episode reward: 196.828, mean reward:  0.367 [-19.444, 100.000], mean action: 1.272 [0.000, 3.000],  loss: 9.226496, mse: 3368.199099, mean_q: 62.346389, mean_eps: 0.197506
 135018/150000: episode: 879, duration: 7.550s, episode steps: 1000, steps per second: 132, episode reward: 56.890, mean reward:  0.057 [-22.827, 17.975], mean action: 1.191 [0.000, 3.000],  loss: 9.053480, mse: 3362.538147, mean_q: 62.913266, mean_eps: 0.192895
 135961/150000: episode: 880, duration: 7.337s, episode steps: 943, steps per second: 129, episode reward: 208.741, mean reward:  0.221 [-19.770, 100.000], mean action: 1.013 [0.000, 3.000],  loss: 9.449488, mse: 3459.221446, mean_q: 64.289259, mean_eps: 0.187066
 136961/150000: episode: 881, duration: 7.615s, episode steps: 1000, steps per second: 131, episode reward: 130.681, mean reward:  0.131 [-19.520, 22.804], mean action: 1.089 [0.000, 3.000],  loss: 9.748797, mse: 3336.149376, mean_q: 63.107886, mean_eps: 0.181237
 137721/150000: episode: 882, duration: 5.733s, episode steps: 760, steps per second: 133, episode reward: 271.621, mean reward:  0.357 [-21.070, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 9.335334, mse: 3240.873429, mean_q: 62.419198, mean_eps: 0.175957
 138431/150000: episode: 883, duration: 5.439s, episode steps: 710, steps per second: 131, episode reward: 169.720, mean reward:  0.239 [-17.720, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 8.260678, mse: 3217.933383, mean_q: 62.144380, mean_eps: 0.171547
 139431/150000: episode: 884, duration: 9.155s, episode steps: 1000, steps per second: 109, episode reward: -92.962, mean reward: -0.093 [-14.154, 16.326], mean action: 1.786 [0.000, 3.000],  loss: 8.318652, mse: 3212.155643, mean_q: 62.162316, mean_eps: 0.166417
 140336/150000: episode: 885, duration: 7.083s, episode steps: 905, steps per second: 128, episode reward: 173.424, mean reward:  0.192 [-19.186, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 8.741340, mse: 3150.264371, mean_q: 61.552771, mean_eps: 0.160702
 140660/150000: episode: 886, duration: 2.435s, episode steps: 324, steps per second: 133, episode reward: 267.926, mean reward:  0.827 [-8.715, 100.000], mean action: 1.457 [0.000, 3.000],  loss: 11.161101, mse: 3080.911121, mean_q: 61.095523, mean_eps: 0.157015
 141017/150000: episode: 887, duration: 2.681s, episode steps: 357, steps per second: 133, episode reward: 247.652, mean reward:  0.694 [-20.471, 100.000], mean action: 1.224 [0.000, 3.000],  loss: 7.518321, mse: 3097.762976, mean_q: 61.206687, mean_eps: 0.154972
 141399/150000: episode: 888, duration: 3.177s, episode steps: 382, steps per second: 120, episode reward: -210.368, mean reward: -0.551 [-100.000, 24.599], mean action: 1.558 [0.000, 3.000],  loss: 8.208230, mse: 3198.165628, mean_q: 62.051980, mean_eps: 0.152755
 142389/150000: episode: 889, duration: 7.690s, episode steps: 990, steps per second: 129, episode reward: 235.663, mean reward:  0.238 [-18.305, 100.000], mean action: 1.305 [0.000, 3.000],  loss: 8.169161, mse: 3179.235564, mean_q: 61.827393, mean_eps: 0.148639
 142784/150000: episode: 890, duration: 2.887s, episode steps: 395, steps per second: 137, episode reward: 232.749, mean reward:  0.589 [-20.080, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 5.323946, mse: 3142.216157, mean_q: 61.277567, mean_eps: 0.144484
 143131/150000: episode: 891, duration: 2.554s, episode steps: 347, steps per second: 136, episode reward: 277.799, mean reward:  0.801 [-10.200, 100.000], mean action: 1.311 [0.000, 3.000],  loss: 7.123291, mse: 3133.353932, mean_q: 61.160072, mean_eps: 0.142258
 143265/150000: episode: 892, duration: 0.967s, episode steps: 134, steps per second: 139, episode reward: -134.733, mean reward: -1.005 [-100.000, 26.678], mean action: 1.582 [0.000, 3.000],  loss: 6.582385, mse: 3157.842216, mean_q: 61.491416, mean_eps: 0.140815
 144265/150000: episode: 893, duration: 7.998s, episode steps: 1000, steps per second: 125, episode reward:  7.605, mean reward:  0.008 [-11.700, 16.007], mean action: 1.611 [0.000, 3.000],  loss: 6.779514, mse: 3136.388393, mean_q: 61.257302, mean_eps: 0.137413
 144729/150000: episode: 894, duration: 3.597s, episode steps: 464, steps per second: 129, episode reward: 215.734, mean reward:  0.465 [-19.223, 100.000], mean action: 1.069 [0.000, 3.000],  loss: 7.772472, mse: 3183.959199, mean_q: 61.822668, mean_eps: 0.133021
 145176/150000: episode: 895, duration: 4.046s, episode steps: 447, steps per second: 110, episode reward: 186.538, mean reward:  0.417 [-17.047, 100.000], mean action: 1.376 [0.000, 3.000],  loss: 6.835586, mse: 3164.819466, mean_q: 61.689621, mean_eps: 0.130288
 145960/150000: episode: 896, duration: 6.079s, episode steps: 784, steps per second: 129, episode reward: 249.407, mean reward:  0.318 [-20.760, 100.000], mean action: 0.760 [0.000, 3.000],  loss: 6.519409, mse: 3173.103513, mean_q: 61.632361, mean_eps: 0.126595
 146686/150000: episode: 897, duration: 5.551s, episode steps: 726, steps per second: 131, episode reward: 239.743, mean reward:  0.330 [-18.306, 100.000], mean action: 1.733 [0.000, 3.000],  loss: 6.031443, mse: 3181.791512, mean_q: 61.706356, mean_eps: 0.122065
 147227/150000: episode: 898, duration: 3.972s, episode steps: 541, steps per second: 136, episode reward: 244.607, mean reward:  0.452 [-19.067, 100.000], mean action: 0.760 [0.000, 3.000],  loss: 6.798715, mse: 3165.769815, mean_q: 61.549357, mean_eps: 0.118264
 147861/150000: episode: 899, duration: 5.050s, episode steps: 634, steps per second: 126, episode reward: 209.690, mean reward:  0.331 [-18.822, 100.000], mean action: 0.825 [0.000, 3.000],  loss: 6.326793, mse: 3214.772442, mean_q: 62.130555, mean_eps: 0.114739
 148414/150000: episode: 900, duration: 4.323s, episode steps: 553, steps per second: 128, episode reward: 220.736, mean reward:  0.399 [-20.197, 100.000], mean action: 1.058 [0.000, 3.000],  loss: 5.625047, mse: 3217.898692, mean_q: 62.014808, mean_eps: 0.111178
 148859/150000: episode: 901, duration: 3.346s, episode steps: 445, steps per second: 133, episode reward: 261.770, mean reward:  0.588 [-21.246, 100.000], mean action: 0.984 [0.000, 3.000],  loss: 6.624179, mse: 3226.652543, mean_q: 62.132494, mean_eps: 0.108184
 149859/150000: episode: 902, duration: 8.097s, episode steps: 1000, steps per second: 124, episode reward: 87.486, mean reward:  0.087 [-18.121, 22.878], mean action: 1.160 [0.000, 3.000],  loss: 6.504362, mse: 3245.274927, mean_q: 62.471118, mean_eps: 0.103849
done, took 1166.286 seconds
Testing for 5 episodes ...
Episode 1: reward: 124.910, steps: 1000
Episode 2: reward: 218.867, steps: 241
Episode 3: reward: 239.858, steps: 595
Episode 4: reward: 30.786, steps: 174
Episode 5: reward: 240.300, steps: 344
Testing for 5 episodes ...
Episode 1: reward: 234.710, steps: 237
Episode 2: reward: 121.252, steps: 1000
Episode 3: reward: 244.449, steps: 482
Episode 4: reward: 222.231, steps: 243
Episode 5: reward: 268.831, steps: 220
Testing for 5 episodes ...
Episode 1: reward: 226.732, steps: 220
Episode 2: reward: 127.890, steps: 1000
Episode 3: reward: 251.738, steps: 378
Episode 4: reward: 215.614, steps: 253
Episode 5: reward: 268.671, steps: 548
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_4 (Dense)              (None, 64)                576
_________________________________________________________________
activation_4 (Activation)    (None, 64)                0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_5 (Activation)    (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_6 (Activation)    (None, 32)                0
_________________________________________________________________
dense_7 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_7 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None