Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 300000 steps ...
     84/300000: episode: 1, duration: 1.026s, episode steps:  84, steps per second:  82, episode reward: 27.617, mean reward:  0.329 [-100.000, 94.817], mean action: 1.167 [0.000, 3.000],  loss: 15.070381, mse: 8.502617, mean_q: 1.404359, mean_eps: 0.999859
    152/300000: episode: 2, duration: 0.434s, episode steps:  68, steps per second: 157, episode reward: -135.481, mean reward: -1.992 [-100.000, 15.525], mean action: 1.500 [0.000, 3.000],  loss: 89.581020, mse: 50.951717, mean_q: 4.356563, mean_eps: 0.999648
    223/300000: episode: 3, duration: 0.443s, episode steps:  71, steps per second: 160, episode reward: -105.954, mean reward: -1.492 [-100.000,  6.653], mean action: 1.521 [0.000, 3.000],  loss: 70.880742, mse: 48.801361, mean_q: 6.439189, mean_eps: 0.999439
    320/300000: episode: 4, duration: 0.642s, episode steps:  97, steps per second: 151, episode reward: -121.992, mean reward: -1.258 [-100.000,  6.501], mean action: 1.588 [0.000, 3.000],  loss: 60.420488, mse: 83.175991, mean_q: 10.379698, mean_eps: 0.999187
    401/300000: episode: 5, duration: 0.487s, episode steps:  81, steps per second: 166, episode reward: -63.577, mean reward: -0.785 [-100.000, 23.283], mean action: 1.568 [0.000, 3.000],  loss: 88.662812, mse: 141.520498, mean_q: 13.307489, mean_eps: 0.998920
    499/300000: episode: 6, duration: 0.577s, episode steps:  98, steps per second: 170, episode reward: -244.317, mean reward: -2.493 [-100.000,  0.660], mean action: 1.520 [0.000, 3.000],  loss: 127.749839, mse: 238.788130, mean_q: 16.714450, mean_eps: 0.998652
    586/300000: episode: 7, duration: 0.535s, episode steps:  87, steps per second: 163, episode reward: -432.756, mean reward: -4.974 [-100.000,  3.842], mean action: 1.644 [0.000, 3.000],  loss: 140.231407, mse: 423.274379, mean_q: 20.475624, mean_eps: 0.998374
    664/300000: episode: 8, duration: 0.501s, episode steps:  78, steps per second: 156, episode reward: -114.955, mean reward: -1.474 [-100.000, 11.953], mean action: 1.628 [0.000, 3.000],  loss: 198.304031, mse: 512.485435, mean_q: 21.112572, mean_eps: 0.998126
    797/300000: episode: 9, duration: 0.774s, episode steps: 133, steps per second: 172, episode reward: -237.468, mean reward: -1.785 [-100.000, 115.154], mean action: 1.421 [0.000, 3.000],  loss: 184.615499, mse: 639.674766, mean_q: 22.571832, mean_eps: 0.997810
    919/300000: episode: 10, duration: 0.703s, episode steps: 122, steps per second: 173, episode reward: -347.534, mean reward: -2.849 [-100.000,  3.906], mean action: 1.467 [0.000, 3.000],  loss: 193.251618, mse: 912.561177, mean_q: 23.188007, mean_eps: 0.997428
   1007/300000: episode: 11, duration: 0.607s, episode steps:  88, steps per second: 145, episode reward: -329.592, mean reward: -3.745 [-100.000, 14.400], mean action: 1.500 [0.000, 3.000],  loss: 237.155565, mse: 1635.098431, mean_q: 28.897064, mean_eps: 0.997113
   1107/300000: episode: 12, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: -319.307, mean reward: -3.193 [-100.000,  0.721], mean action: 1.480 [0.000, 3.000],  loss: 234.228691, mse: 2299.427827, mean_q: 34.544786, mean_eps: 0.996831
   1181/300000: episode: 13, duration: 0.723s, episode steps:  74, steps per second: 102, episode reward: -135.311, mean reward: -1.829 [-100.000,  9.684], mean action: 1.757 [0.000, 3.000],  loss: 292.399694, mse: 3059.389239, mean_q: 39.124480, mean_eps: 0.996569
   1280/300000: episode: 14, duration: 0.815s, episode steps:  99, steps per second: 121, episode reward: -185.011, mean reward: -1.869 [-100.000,  8.382], mean action: 1.505 [0.000, 3.000],  loss: 248.117192, mse: 3470.645449, mean_q: 40.488375, mean_eps: 0.996310
   1391/300000: episode: 15, duration: 0.769s, episode steps: 111, steps per second: 144, episode reward: -158.786, mean reward: -1.431 [-100.000,  6.267], mean action: 1.468 [0.000, 3.000],  loss: 457.879508, mse: 4018.225723, mean_q: 42.811567, mean_eps: 0.995995
   1490/300000: episode: 16, duration: 0.845s, episode steps:  99, steps per second: 117, episode reward: -121.127, mean reward: -1.224 [-100.000,  6.858], mean action: 1.475 [0.000, 3.000],  loss: 357.482791, mse: 4807.506460, mean_q: 47.765564, mean_eps: 0.995680
   1613/300000: episode: 17, duration: 0.992s, episode steps: 123, steps per second: 124, episode reward: -38.898, mean reward: -0.316 [-100.000, 70.141], mean action: 1.675 [0.000, 3.000],  loss: 375.989689, mse: 5782.249304, mean_q: 51.910852, mean_eps: 0.995347
   1728/300000: episode: 18, duration: 0.889s, episode steps: 115, steps per second: 129, episode reward: -104.956, mean reward: -0.913 [-100.000,  7.534], mean action: 1.504 [0.000, 3.000],  loss: 468.011155, mse: 8220.234505, mean_q: 62.592572, mean_eps: 0.994990
   1825/300000: episode: 19, duration: 0.732s, episode steps:  97, steps per second: 133, episode reward: -361.122, mean reward: -3.723 [-100.000,  0.877], mean action: 1.464 [0.000, 3.000],  loss: 472.124752, mse: 8677.078070, mean_q: 64.459693, mean_eps: 0.994672
   1926/300000: episode: 20, duration: 0.734s, episode steps: 101, steps per second: 138, episode reward: -368.883, mean reward: -3.652 [-100.000,  1.506], mean action: 1.535 [0.000, 3.000],  loss: 486.634936, mse: 10179.377284, mean_q: 68.863137, mean_eps: 0.994375
   2017/300000: episode: 21, duration: 0.587s, episode steps:  91, steps per second: 155, episode reward: -129.582, mean reward: -1.424 [-100.000, 12.042], mean action: 1.462 [0.000, 3.000],  loss: 541.345604, mse: 9774.689214, mean_q: 66.665184, mean_eps: 0.994087
   2100/300000: episode: 22, duration: 0.620s, episode steps:  83, steps per second: 134, episode reward: -230.995, mean reward: -2.783 [-100.000, 109.693], mean action: 1.337 [0.000, 3.000],  loss: 404.915256, mse: 9867.594768, mean_q: 69.914231, mean_eps: 0.993826
   2215/300000: episode: 23, duration: 0.782s, episode steps: 115, steps per second: 147, episode reward: -119.066, mean reward: -1.035 [-100.000, 14.821], mean action: 1.626 [0.000, 3.000],  loss: 417.571861, mse: 10521.115791, mean_q: 73.650156, mean_eps: 0.993529
   2308/300000: episode: 24, duration: 0.650s, episode steps:  93, steps per second: 143, episode reward: -401.851, mean reward: -4.321 [-100.000,  0.791], mean action: 1.559 [0.000, 3.000],  loss: 435.185957, mse: 11848.794145, mean_q: 80.333837, mean_eps: 0.993217
   2409/300000: episode: 25, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -403.724, mean reward: -3.997 [-100.000,  0.510], mean action: 1.545 [0.000, 3.000],  loss: 332.888507, mse: 11913.836578, mean_q: 78.836041, mean_eps: 0.992926
   2491/300000: episode: 26, duration: 0.563s, episode steps:  82, steps per second: 146, episode reward: -245.421, mean reward: -2.993 [-100.000,  7.418], mean action: 1.634 [0.000, 3.000],  loss: 265.847428, mse: 11770.997434, mean_q: 75.983684, mean_eps: 0.992652
   2609/300000: episode: 27, duration: 0.841s, episode steps: 118, steps per second: 140, episode reward: -26.700, mean reward: -0.226 [-100.000, 112.454], mean action: 1.593 [0.000, 3.000],  loss: 317.760937, mse: 11715.382537, mean_q: 77.289206, mean_eps: 0.992351
   2713/300000: episode: 28, duration: 0.661s, episode steps: 104, steps per second: 157, episode reward: -315.529, mean reward: -3.034 [-100.000, 31.268], mean action: 1.519 [0.000, 3.000],  loss: 278.380256, mse: 12521.447500, mean_q: 80.597022, mean_eps: 0.992019
   2836/300000: episode: 29, duration: 0.818s, episode steps: 123, steps per second: 150, episode reward: -352.609, mean reward: -2.867 [-100.000, 85.219], mean action: 1.683 [0.000, 3.000],  loss: 358.158600, mse: 12467.903686, mean_q: 80.215758, mean_eps: 0.991678
   2947/300000: episode: 30, duration: 0.722s, episode steps: 111, steps per second: 154, episode reward: -231.250, mean reward: -2.083 [-100.000,  0.753], mean action: 1.514 [0.000, 3.000],  loss: 315.577179, mse: 12807.154117, mean_q: 81.326826, mean_eps: 0.991327
   3049/300000: episode: 31, duration: 0.651s, episode steps: 102, steps per second: 157, episode reward: -113.801, mean reward: -1.116 [-100.000, 13.258], mean action: 1.363 [0.000, 3.000],  loss: 277.947556, mse: 13334.542437, mean_q: 84.304244, mean_eps: 0.991008
   3116/300000: episode: 32, duration: 0.450s, episode steps:  67, steps per second: 149, episode reward: -122.146, mean reward: -1.823 [-100.000,  6.005], mean action: 1.627 [0.000, 3.000],  loss: 277.445315, mse: 13525.252594, mean_q: 87.128197, mean_eps: 0.990754
   3219/300000: episode: 33, duration: 0.667s, episode steps: 103, steps per second: 154, episode reward: -176.300, mean reward: -1.712 [-100.000, 17.926], mean action: 1.515 [0.000, 3.000],  loss: 219.120349, mse: 14775.477349, mean_q: 94.337710, mean_eps: 0.990499
   3319/300000: episode: 34, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -118.701, mean reward: -1.187 [-100.000,  8.228], mean action: 1.520 [0.000, 3.000],  loss: 287.008832, mse: 13150.460415, mean_q: 90.119858, mean_eps: 0.990195
   3456/300000: episode: 35, duration: 0.904s, episode steps: 137, steps per second: 152, episode reward: -211.387, mean reward: -1.543 [-100.000,  1.623], mean action: 1.577 [0.000, 3.000],  loss: 181.771265, mse: 13576.779732, mean_q: 92.726639, mean_eps: 0.989839
   3538/300000: episode: 36, duration: 0.553s, episode steps:  82, steps per second: 148, episode reward: -153.230, mean reward: -1.869 [-100.000,  9.035], mean action: 1.402 [0.000, 3.000],  loss: 141.608684, mse: 13813.227837, mean_q: 95.367241, mean_eps: 0.989510
   3651/300000: episode: 37, duration: 0.788s, episode steps: 113, steps per second: 143, episode reward: -238.850, mean reward: -2.114 [-100.000,  3.989], mean action: 1.584 [0.000, 3.000],  loss: 154.427807, mse: 13544.733865, mean_q: 94.934151, mean_eps: 0.989218
   3731/300000: episode: 38, duration: 0.508s, episode steps:  80, steps per second: 157, episode reward: -206.660, mean reward: -2.583 [-100.000, 17.175], mean action: 1.488 [0.000, 3.000],  loss: 150.079217, mse: 13202.667249, mean_q: 94.503489, mean_eps: 0.988928
   3803/300000: episode: 39, duration: 0.473s, episode steps:  72, steps per second: 152, episode reward: -90.322, mean reward: -1.254 [-100.000,  9.982], mean action: 1.375 [0.000, 3.000],  loss: 125.539542, mse: 13271.181817, mean_q: 97.357389, mean_eps: 0.988700
   3875/300000: episode: 40, duration: 0.454s, episode steps:  72, steps per second: 158, episode reward: -77.937, mean reward: -1.082 [-100.000,  7.880], mean action: 1.681 [0.000, 3.000],  loss: 95.235439, mse: 13623.720635, mean_q: 97.935007, mean_eps: 0.988484
   3959/300000: episode: 41, duration: 0.543s, episode steps:  84, steps per second: 155, episode reward: -92.592, mean reward: -1.102 [-100.000,  6.299], mean action: 1.405 [0.000, 3.000],  loss: 81.720094, mse: 13171.950428, mean_q: 96.552450, mean_eps: 0.988250
   4021/300000: episode: 42, duration: 0.412s, episode steps:  62, steps per second: 151, episode reward: -79.281, mean reward: -1.279 [-100.000, 16.969], mean action: 1.516 [0.000, 3.000],  loss: 84.819092, mse: 13243.018145, mean_q: 99.479470, mean_eps: 0.988032
   4128/300000: episode: 43, duration: 0.769s, episode steps: 107, steps per second: 139, episode reward: -99.225, mean reward: -0.927 [-100.000, 12.751], mean action: 1.561 [0.000, 3.000],  loss: 85.047074, mse: 13830.562317, mean_q: 100.009022, mean_eps: 0.987778
   4247/300000: episode: 44, duration: 0.824s, episode steps: 119, steps per second: 144, episode reward: -123.663, mean reward: -1.039 [-100.000,  7.817], mean action: 1.521 [0.000, 3.000],  loss: 67.239338, mse: 14103.331457, mean_q: 102.318674, mean_eps: 0.987439
   4333/300000: episode: 45, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -355.940, mean reward: -4.139 [-100.000,  0.347], mean action: 1.500 [0.000, 3.000],  loss: 74.888607, mse: 14913.936512, mean_q: 106.025472, mean_eps: 0.987132
   4403/300000: episode: 46, duration: 0.526s, episode steps:  70, steps per second: 133, episode reward: -105.353, mean reward: -1.505 [-100.000,  9.272], mean action: 1.643 [0.000, 3.000],  loss: 60.265987, mse: 14876.175014, mean_q: 106.178397, mean_eps: 0.986897
   4484/300000: episode: 47, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -325.113, mean reward: -4.014 [-100.000,  1.723], mean action: 1.617 [0.000, 3.000],  loss: 57.471685, mse: 15556.079343, mean_q: 108.426373, mean_eps: 0.986671
   4584/300000: episode: 48, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -124.604, mean reward: -1.246 [-100.000,  6.832], mean action: 1.490 [0.000, 3.000],  loss: 36.809806, mse: 15714.789355, mean_q: 110.396919, mean_eps: 0.986399
   4676/300000: episode: 49, duration: 0.671s, episode steps:  92, steps per second: 137, episode reward: -111.876, mean reward: -1.216 [-100.000,  8.011], mean action: 1.380 [0.000, 3.000],  loss: 41.212080, mse: 15720.897811, mean_q: 111.526422, mean_eps: 0.986111
   4789/300000: episode: 50, duration: 0.802s, episode steps: 113, steps per second: 141, episode reward: -0.917, mean reward: -0.008 [-100.000, 86.722], mean action: 1.540 [0.000, 3.000],  loss: 41.414201, mse: 16573.432220, mean_q: 114.787336, mean_eps: 0.985804
   4919/300000: episode: 51, duration: 0.879s, episode steps: 130, steps per second: 148, episode reward: -404.884, mean reward: -3.114 [-100.000,  1.476], mean action: 1.577 [0.000, 3.000],  loss: 47.043198, mse: 16570.120395, mean_q: 114.376187, mean_eps: 0.985440
   5018/300000: episode: 52, duration: 0.701s, episode steps:  99, steps per second: 141, episode reward: -113.697, mean reward: -1.148 [-100.000,  7.966], mean action: 1.333 [0.000, 3.000],  loss: 37.259160, mse: 17158.167209, mean_q: 118.209453, mean_eps: 0.985096
   5128/300000: episode: 53, duration: 0.779s, episode steps: 110, steps per second: 141, episode reward: -134.436, mean reward: -1.222 [-100.000, 32.088], mean action: 1.382 [0.000, 3.000],  loss: 38.081299, mse: 18034.935103, mean_q: 122.330487, mean_eps: 0.984782
   5218/300000: episode: 54, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -256.103, mean reward: -2.846 [-100.000, 15.351], mean action: 1.567 [0.000, 3.000],  loss: 30.566754, mse: 18661.529959, mean_q: 125.433350, mean_eps: 0.984483
   5305/300000: episode: 55, duration: 0.640s, episode steps:  87, steps per second: 136, episode reward: -297.036, mean reward: -3.414 [-100.000,  0.516], mean action: 1.483 [0.000, 3.000],  loss: 32.549883, mse: 19743.687534, mean_q: 128.678356, mean_eps: 0.984217
   5376/300000: episode: 56, duration: 0.540s, episode steps:  71, steps per second: 132, episode reward: -124.942, mean reward: -1.760 [-100.000, 17.217], mean action: 1.592 [0.000, 3.000],  loss: 27.078983, mse: 20656.457664, mean_q: 133.380601, mean_eps: 0.983980
   5458/300000: episode: 57, duration: 0.660s, episode steps:  82, steps per second: 124, episode reward: -359.236, mean reward: -4.381 [-100.000,  3.640], mean action: 1.537 [0.000, 3.000],  loss: 38.373508, mse: 21353.286859, mean_q: 137.794479, mean_eps: 0.983750
   5533/300000: episode: 58, duration: 0.689s, episode steps:  75, steps per second: 109, episode reward: -198.568, mean reward: -2.648 [-100.000,  6.511], mean action: 1.280 [0.000, 3.000],  loss: 38.928256, mse: 21808.331615, mean_q: 138.041993, mean_eps: 0.983515
   5629/300000: episode: 59, duration: 0.754s, episode steps:  96, steps per second: 127, episode reward: -243.520, mean reward: -2.537 [-100.000,  0.658], mean action: 1.594 [0.000, 3.000],  loss: 39.892761, mse: 21878.987813, mean_q: 136.479070, mean_eps: 0.983258
   5712/300000: episode: 60, duration: 0.578s, episode steps:  83, steps per second: 144, episode reward: -179.495, mean reward: -2.163 [-100.000,  9.374], mean action: 1.434 [0.000, 3.000],  loss: 34.290772, mse: 22980.619905, mean_q: 142.241791, mean_eps: 0.982990
   5804/300000: episode: 61, duration: 0.700s, episode steps:  92, steps per second: 131, episode reward: -125.813, mean reward: -1.368 [-100.000, 16.152], mean action: 1.315 [0.000, 3.000],  loss: 31.427170, mse: 24081.122134, mean_q: 147.348172, mean_eps: 0.982728
   5874/300000: episode: 62, duration: 0.510s, episode steps:  70, steps per second: 137, episode reward: -108.702, mean reward: -1.553 [-100.000, 11.592], mean action: 1.614 [0.000, 3.000],  loss: 30.236477, mse: 24409.984375, mean_q: 150.293669, mean_eps: 0.982484
   5935/300000: episode: 63, duration: 0.436s, episode steps:  61, steps per second: 140, episode reward: -83.994, mean reward: -1.377 [-100.000,  6.422], mean action: 1.492 [0.000, 3.000],  loss: 31.689640, mse: 24911.859471, mean_q: 151.967909, mean_eps: 0.982288
   6014/300000: episode: 64, duration: 0.549s, episode steps:  79, steps per second: 144, episode reward: -329.772, mean reward: -4.174 [-100.000, 83.733], mean action: 1.582 [0.000, 3.000],  loss: 27.032183, mse: 25822.761719, mean_q: 153.920866, mean_eps: 0.982078
   6093/300000: episode: 65, duration: 0.588s, episode steps:  79, steps per second: 134, episode reward: -111.061, mean reward: -1.406 [-100.000,  8.113], mean action: 1.722 [0.000, 3.000],  loss: 37.030283, mse: 26107.646336, mean_q: 154.530187, mean_eps: 0.981841
   6157/300000: episode: 66, duration: 0.558s, episode steps:  64, steps per second: 115, episode reward: -126.719, mean reward: -1.980 [-100.000,  5.982], mean action: 1.750 [0.000, 3.000],  loss: 31.651123, mse: 27448.963440, mean_q: 160.090012, mean_eps: 0.981626
   6239/300000: episode: 67, duration: 0.693s, episode steps:  82, steps per second: 118, episode reward: -281.378, mean reward: -3.431 [-100.000, 25.785], mean action: 1.500 [0.000, 3.000],  loss: 30.953193, mse: 27968.736209, mean_q: 162.711850, mean_eps: 0.981407
   6333/300000: episode: 68, duration: 0.664s, episode steps:  94, steps per second: 142, episode reward: -342.057, mean reward: -3.639 [-100.000,  6.632], mean action: 1.362 [0.000, 3.000],  loss: 29.692680, mse: 29276.205452, mean_q: 166.254233, mean_eps: 0.981144
   6442/300000: episode: 69, duration: 0.695s, episode steps: 109, steps per second: 157, episode reward: -122.196, mean reward: -1.121 [-100.000, 12.495], mean action: 1.587 [0.000, 3.000],  loss: 31.554386, mse: 29775.626702, mean_q: 166.666771, mean_eps: 0.980839
   6541/300000: episode: 70, duration: 0.714s, episode steps:  99, steps per second: 139, episode reward: -179.455, mean reward: -1.813 [-100.000,  4.456], mean action: 1.596 [0.000, 3.000],  loss: 32.505314, mse: 31017.877427, mean_q: 170.406736, mean_eps: 0.980527
   6609/300000: episode: 71, duration: 0.499s, episode steps:  68, steps per second: 136, episode reward: -229.794, mean reward: -3.379 [-100.000, 46.313], mean action: 1.485 [0.000, 3.000],  loss: 37.831023, mse: 31494.794548, mean_q: 171.789204, mean_eps: 0.980277
   6722/300000: episode: 72, duration: 0.783s, episode steps: 113, steps per second: 144, episode reward: -174.564, mean reward: -1.545 [-100.000,  2.513], mean action: 1.593 [0.000, 3.000],  loss: 30.900730, mse: 31528.984582, mean_q: 169.858622, mean_eps: 0.980005
   6790/300000: episode: 73, duration: 0.472s, episode steps:  68, steps per second: 144, episode reward: -65.736, mean reward: -0.967 [-100.000, 12.069], mean action: 1.485 [0.000, 3.000],  loss: 29.052107, mse: 33219.489545, mean_q: 174.913902, mean_eps: 0.979734
   6907/300000: episode: 74, duration: 0.827s, episode steps: 117, steps per second: 142, episode reward: -374.343, mean reward: -3.200 [-100.000, 76.147], mean action: 1.436 [0.000, 3.000],  loss: 34.319593, mse: 34053.685564, mean_q: 176.759496, mean_eps: 0.979456
   7022/300000: episode: 75, duration: 0.770s, episode steps: 115, steps per second: 149, episode reward: -205.612, mean reward: -1.788 [-100.000, 27.394], mean action: 1.426 [0.000, 3.000],  loss: 26.280662, mse: 34028.867612, mean_q: 175.292151, mean_eps: 0.979108
   7122/300000: episode: 76, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: -153.727, mean reward: -1.537 [-100.000,  6.606], mean action: 1.550 [0.000, 3.000],  loss: 30.843001, mse: 34865.809609, mean_q: 176.359911, mean_eps: 0.978786
   7217/300000: episode: 77, duration: 0.736s, episode steps:  95, steps per second: 129, episode reward: -119.198, mean reward: -1.255 [-100.000,  8.119], mean action: 1.579 [0.000, 3.000],  loss: 31.105881, mse: 36496.120600, mean_q: 180.762931, mean_eps: 0.978493
   7293/300000: episode: 78, duration: 0.636s, episode steps:  76, steps per second: 119, episode reward:  0.834, mean reward:  0.011 [-100.000, 47.726], mean action: 1.750 [0.000, 3.000],  loss: 35.677140, mse: 37107.177272, mean_q: 180.829818, mean_eps: 0.978236
   7388/300000: episode: 79, duration: 0.768s, episode steps:  95, steps per second: 124, episode reward: -117.532, mean reward: -1.237 [-100.000, 32.487], mean action: 1.411 [0.000, 3.000],  loss: 41.475202, mse: 36511.839576, mean_q: 176.962858, mean_eps: 0.977980
   7452/300000: episode: 80, duration: 0.455s, episode steps:  64, steps per second: 141, episode reward: -70.723, mean reward: -1.105 [-100.000,  8.153], mean action: 1.547 [0.000, 3.000],  loss: 35.112353, mse: 37775.423798, mean_q: 182.865632, mean_eps: 0.977742
   7529/300000: episode: 81, duration: 0.567s, episode steps:  77, steps per second: 136, episode reward: -190.306, mean reward: -2.472 [-100.000,  5.092], mean action: 1.364 [0.000, 3.000],  loss: 39.886636, mse: 37679.037160, mean_q: 181.406550, mean_eps: 0.977530
   7636/300000: episode: 82, duration: 0.762s, episode steps: 107, steps per second: 140, episode reward: -151.387, mean reward: -1.415 [-100.000, 10.603], mean action: 1.486 [0.000, 3.000],  loss: 40.842808, mse: 38652.389347, mean_q: 184.631112, mean_eps: 0.977254
   7710/300000: episode: 83, duration: 0.496s, episode steps:  74, steps per second: 149, episode reward: -334.794, mean reward: -4.524 [-100.000, 25.742], mean action: 1.770 [0.000, 3.000],  loss: 31.155708, mse: 38364.397910, mean_q: 182.699550, mean_eps: 0.976983
   7784/300000: episode: 84, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -288.927, mean reward: -3.904 [-100.000,  4.852], mean action: 1.243 [0.000, 3.000],  loss: 33.895281, mse: 37826.478489, mean_q: 178.762261, mean_eps: 0.976760
   7876/300000: episode: 85, duration: 0.673s, episode steps:  92, steps per second: 137, episode reward: -221.100, mean reward: -2.403 [-100.000,  9.660], mean action: 1.424 [0.000, 3.000],  loss: 39.173071, mse: 38179.048680, mean_q: 181.248482, mean_eps: 0.976511
   7944/300000: episode: 86, duration: 0.435s, episode steps:  68, steps per second: 156, episode reward: -150.050, mean reward: -2.207 [-100.000, 23.162], mean action: 1.515 [0.000, 3.000],  loss: 25.991422, mse: 38521.547105, mean_q: 182.202259, mean_eps: 0.976271
   8006/300000: episode: 87, duration: 0.455s, episode steps:  62, steps per second: 136, episode reward: -76.750, mean reward: -1.238 [-100.000,  7.187], mean action: 1.548 [0.000, 3.000],  loss: 21.106134, mse: 38946.879725, mean_q: 183.875382, mean_eps: 0.976077
   8078/300000: episode: 88, duration: 0.511s, episode steps:  72, steps per second: 141, episode reward: -275.231, mean reward: -3.823 [-100.000,  3.814], mean action: 1.028 [0.000, 3.000],  loss: 26.507491, mse: 38370.685221, mean_q: 181.879568, mean_eps: 0.975876
   8202/300000: episode: 89, duration: 0.767s, episode steps: 124, steps per second: 162, episode reward: -165.966, mean reward: -1.338 [-100.000,  5.270], mean action: 1.629 [0.000, 3.000],  loss: 39.423542, mse: 39457.605689, mean_q: 183.120128, mean_eps: 0.975582
   8314/300000: episode: 90, duration: 0.710s, episode steps: 112, steps per second: 158, episode reward: -147.508, mean reward: -1.317 [-100.000,  8.716], mean action: 1.518 [0.000, 3.000],  loss: 24.109246, mse: 39386.273629, mean_q: 182.408108, mean_eps: 0.975228
   8426/300000: episode: 91, duration: 0.701s, episode steps: 112, steps per second: 160, episode reward: -110.568, mean reward: -0.987 [-100.000, 10.830], mean action: 1.536 [0.000, 3.000],  loss: 27.675477, mse: 39000.023751, mean_q: 180.333607, mean_eps: 0.974892
   8520/300000: episode: 92, duration: 0.571s, episode steps:  94, steps per second: 165, episode reward: -218.079, mean reward: -2.320 [-100.000,  7.189], mean action: 1.521 [0.000, 3.000],  loss: 31.569447, mse: 39523.119182, mean_q: 181.121562, mean_eps: 0.974583
   8620/300000: episode: 93, duration: 0.625s, episode steps: 100, steps per second: 160, episode reward: -256.457, mean reward: -2.565 [-100.000,  1.745], mean action: 1.500 [0.000, 3.000],  loss: 30.061179, mse: 38915.555566, mean_q: 179.323962, mean_eps: 0.974291
   8716/300000: episode: 94, duration: 0.791s, episode steps:  96, steps per second: 121, episode reward: -247.857, mean reward: -2.582 [-100.000, 18.276], mean action: 1.406 [0.000, 3.000],  loss: 38.361392, mse: 39918.767456, mean_q: 182.825151, mean_eps: 0.973997
   8807/300000: episode: 95, duration: 0.629s, episode steps:  91, steps per second: 145, episode reward: -178.856, mean reward: -1.965 [-100.000, 25.359], mean action: 1.330 [0.000, 3.000],  loss: 22.382672, mse: 39242.595231, mean_q: 179.723954, mean_eps: 0.973717
   8900/300000: episode: 96, duration: 0.643s, episode steps:  93, steps per second: 145, episode reward: -70.806, mean reward: -0.761 [-100.000, 88.642], mean action: 1.452 [0.000, 3.000],  loss: 41.398557, mse: 40339.777491, mean_q: 183.183524, mean_eps: 0.973441
   8982/300000: episode: 97, duration: 0.592s, episode steps:  82, steps per second: 138, episode reward: -5.726, mean reward: -0.070 [-100.000, 67.328], mean action: 1.415 [0.000, 3.000],  loss: 28.482211, mse: 40575.275295, mean_q: 182.400355, mean_eps: 0.973179
   9048/300000: episode: 98, duration: 0.457s, episode steps:  66, steps per second: 145, episode reward: -51.314, mean reward: -0.777 [-100.000, 12.572], mean action: 1.545 [0.000, 3.000],  loss: 35.029170, mse: 38369.210938, mean_q: 173.327305, mean_eps: 0.972957
   9106/300000: episode: 99, duration: 0.360s, episode steps:  58, steps per second: 161, episode reward: -108.277, mean reward: -1.867 [-100.000, 15.241], mean action: 1.379 [0.000, 3.000],  loss: 49.369962, mse: 40288.844962, mean_q: 181.300728, mean_eps: 0.972770
   9249/300000: episode: 100, duration: 1.081s, episode steps: 143, steps per second: 132, episode reward: -52.800, mean reward: -0.369 [-100.000, 81.487], mean action: 1.497 [0.000, 3.000],  loss: 35.141057, mse: 37268.470758, mean_q: 168.879530, mean_eps: 0.972469
   9345/300000: episode: 101, duration: 0.623s, episode steps:  96, steps per second: 154, episode reward: -106.243, mean reward: -1.107 [-100.000, 48.429], mean action: 1.552 [0.000, 3.000],  loss: 47.672082, mse: 37581.933533, mean_q: 169.853993, mean_eps: 0.972110
   9429/300000: episode: 102, duration: 0.651s, episode steps:  84, steps per second: 129, episode reward: -183.519, mean reward: -2.185 [-100.000,  8.460], mean action: 1.452 [0.000, 3.000],  loss: 50.562402, mse: 38449.778390, mean_q: 171.847766, mean_eps: 0.971840
   9492/300000: episode: 103, duration: 0.495s, episode steps:  63, steps per second: 127, episode reward: -71.576, mean reward: -1.136 [-100.000,  7.230], mean action: 1.381 [0.000, 3.000],  loss: 33.778392, mse: 37362.643601, mean_q: 167.299904, mean_eps: 0.971620
   9568/300000: episode: 104, duration: 0.635s, episode steps:  76, steps per second: 120, episode reward: -102.856, mean reward: -1.353 [-100.000, 12.574], mean action: 1.513 [0.000, 3.000],  loss: 52.528898, mse: 37780.122995, mean_q: 168.713231, mean_eps: 0.971411
   9673/300000: episode: 105, duration: 0.840s, episode steps: 105, steps per second: 125, episode reward: -191.632, mean reward: -1.825 [-100.000,  1.900], mean action: 1.419 [0.000, 3.000],  loss: 48.487251, mse: 37351.416220, mean_q: 166.494849, mean_eps: 0.971140
   9748/300000: episode: 106, duration: 0.564s, episode steps:  75, steps per second: 133, episode reward: -137.164, mean reward: -1.829 [-100.000, 29.269], mean action: 1.413 [0.000, 3.000],  loss: 49.394884, mse: 37713.510313, mean_q: 169.242159, mean_eps: 0.970870
   9843/300000: episode: 107, duration: 0.709s, episode steps:  95, steps per second: 134, episode reward: -226.602, mean reward: -2.385 [-100.000, 33.472], mean action: 1.242 [0.000, 3.000],  loss: 46.870930, mse: 37259.262521, mean_q: 166.778995, mean_eps: 0.970615
   9919/300000: episode: 108, duration: 0.571s, episode steps:  76, steps per second: 133, episode reward: -166.808, mean reward: -2.195 [-100.000,  5.345], mean action: 1.395 [0.000, 3.000],  loss: 37.845023, mse: 36078.042686, mean_q: 161.649809, mean_eps: 0.970358
  10036/300000: episode: 109, duration: 0.797s, episode steps: 117, steps per second: 147, episode reward: -130.943, mean reward: -1.119 [-100.000, 12.327], mean action: 1.598 [0.000, 3.000],  loss: 46.413032, mse: 36087.553953, mean_q: 161.785196, mean_eps: 0.970069
  10103/300000: episode: 110, duration: 0.435s, episode steps:  67, steps per second: 154, episode reward: -103.838, mean reward: -1.550 [-100.000, 12.242], mean action: 1.343 [0.000, 3.000],  loss: 34.930827, mse: 36075.095149, mean_q: 163.407691, mean_eps: 0.969793
  10168/300000: episode: 111, duration: 0.434s, episode steps:  65, steps per second: 150, episode reward: -207.528, mean reward: -3.193 [-100.000, 11.448], mean action: 1.631 [0.000, 3.000],  loss: 29.411468, mse: 35746.256881, mean_q: 161.358540, mean_eps: 0.969595
  10275/300000: episode: 112, duration: 0.712s, episode steps: 107, steps per second: 150, episode reward: -180.420, mean reward: -1.686 [-100.000,  8.163], mean action: 1.252 [0.000, 3.000],  loss: 53.837303, mse: 35907.244305, mean_q: 162.556141, mean_eps: 0.969337
  10390/300000: episode: 113, duration: 0.734s, episode steps: 115, steps per second: 157, episode reward: -132.558, mean reward: -1.153 [-100.000, 11.703], mean action: 1.643 [0.000, 3.000],  loss: 28.987388, mse: 34976.314640, mean_q: 159.759814, mean_eps: 0.969004
  10452/300000: episode: 114, duration: 0.410s, episode steps:  62, steps per second: 151, episode reward: -57.104, mean reward: -0.921 [-100.000, 18.344], mean action: 1.565 [0.000, 3.000],  loss: 44.613542, mse: 34925.506426, mean_q: 158.570192, mean_eps: 0.968739
  10581/300000: episode: 115, duration: 0.869s, episode steps: 129, steps per second: 148, episode reward: -258.252, mean reward: -2.002 [-100.000,  1.367], mean action: 1.543 [0.000, 3.000],  loss: 34.498225, mse: 34904.522832, mean_q: 159.466123, mean_eps: 0.968452
  10668/300000: episode: 116, duration: 0.543s, episode steps:  87, steps per second: 160, episode reward: 31.636, mean reward:  0.364 [-100.000, 102.598], mean action: 1.621 [0.000, 3.000],  loss: 26.883057, mse: 35114.188757, mean_q: 160.195400, mean_eps: 0.968128
  10763/300000: episode: 117, duration: 0.607s, episode steps:  95, steps per second: 157, episode reward: -473.221, mean reward: -4.981 [-100.000, 37.652], mean action: 1.453 [0.000, 3.000],  loss: 27.629866, mse: 33861.959025, mean_q: 153.882612, mean_eps: 0.967855
  10833/300000: episode: 118, duration: 0.490s, episode steps:  70, steps per second: 143, episode reward: -191.681, mean reward: -2.738 [-100.000, 17.565], mean action: 1.571 [0.000, 3.000],  loss: 34.793675, mse: 34094.082422, mean_q: 154.836744, mean_eps: 0.967608
  10935/300000: episode: 119, duration: 0.675s, episode steps: 102, steps per second: 151, episode reward: -112.475, mean reward: -1.103 [-100.000, 10.860], mean action: 1.578 [0.000, 3.000],  loss: 49.545673, mse: 34440.297124, mean_q: 156.531764, mean_eps: 0.967349
  11010/300000: episode: 120, duration: 0.555s, episode steps:  75, steps per second: 135, episode reward: -93.781, mean reward: -1.250 [-100.000,  7.271], mean action: 1.600 [0.000, 3.000],  loss: 37.209669, mse: 33729.079818, mean_q: 154.902573, mean_eps: 0.967084
  11126/300000: episode: 121, duration: 0.765s, episode steps: 116, steps per second: 152, episode reward: -243.791, mean reward: -2.102 [-100.000, 35.473], mean action: 1.448 [0.000, 3.000],  loss: 38.660930, mse: 32927.782058, mean_q: 152.230446, mean_eps: 0.966797
  11247/300000: episode: 122, duration: 0.764s, episode steps: 121, steps per second: 158, episode reward: -172.208, mean reward: -1.423 [-100.000, 15.577], mean action: 1.479 [0.000, 3.000],  loss: 34.410077, mse: 32870.024487, mean_q: 150.077129, mean_eps: 0.966442
  11323/300000: episode: 123, duration: 0.466s, episode steps:  76, steps per second: 163, episode reward: -116.448, mean reward: -1.532 [-100.000,  6.041], mean action: 1.237 [0.000, 3.000],  loss: 24.393475, mse: 32177.738101, mean_q: 147.038293, mean_eps: 0.966146
  11415/300000: episode: 124, duration: 0.587s, episode steps:  92, steps per second: 157, episode reward: -339.714, mean reward: -3.693 [-100.000,  0.916], mean action: 1.609 [0.000, 3.000],  loss: 38.208968, mse: 31845.991211, mean_q: 146.823208, mean_eps: 0.965894
  11487/300000: episode: 125, duration: 0.509s, episode steps:  72, steps per second: 141, episode reward: -78.682, mean reward: -1.093 [-100.000, 13.672], mean action: 1.819 [0.000, 3.000],  loss: 28.008435, mse: 33133.958360, mean_q: 150.328875, mean_eps: 0.965648
  11618/300000: episode: 126, duration: 0.850s, episode steps: 131, steps per second: 154, episode reward: -230.341, mean reward: -1.758 [-100.000,  4.640], mean action: 1.611 [0.000, 3.000],  loss: 37.907933, mse: 32339.241591, mean_q: 148.216859, mean_eps: 0.965344
  11704/300000: episode: 127, duration: 0.529s, episode steps:  86, steps per second: 162, episode reward: -436.234, mean reward: -5.072 [-100.000, -0.197], mean action: 1.430 [0.000, 3.000],  loss: 46.077757, mse: 31826.185206, mean_q: 148.773179, mean_eps: 0.965019
  11804/300000: episode: 128, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: -196.780, mean reward: -1.968 [-100.000,  9.877], mean action: 1.640 [0.000, 3.000],  loss: 37.075253, mse: 31553.145957, mean_q: 147.103094, mean_eps: 0.964739
  11882/300000: episode: 129, duration: 0.501s, episode steps:  78, steps per second: 156, episode reward: -308.210, mean reward: -3.951 [-100.000,  2.681], mean action: 1.756 [0.000, 3.000],  loss: 30.876585, mse: 31043.909605, mean_q: 145.747910, mean_eps: 0.964473
  11993/300000: episode: 130, duration: 0.700s, episode steps: 111, steps per second: 159, episode reward: -129.630, mean reward: -1.168 [-100.000,  9.147], mean action: 1.658 [0.000, 3.000],  loss: 40.536608, mse: 30534.509871, mean_q: 142.521298, mean_eps: 0.964189
  12115/300000: episode: 131, duration: 0.792s, episode steps: 122, steps per second: 154, episode reward: -352.815, mean reward: -2.892 [-100.000, 21.252], mean action: 1.590 [0.000, 3.000],  loss: 37.931649, mse: 30918.697474, mean_q: 144.238732, mean_eps: 0.963839
  12247/300000: episode: 132, duration: 0.822s, episode steps: 132, steps per second: 161, episode reward: -157.199, mean reward: -1.191 [-100.000, 10.484], mean action: 1.530 [0.000, 3.000],  loss: 33.432664, mse: 31361.561094, mean_q: 146.701699, mean_eps: 0.963458
  12345/300000: episode: 133, duration: 0.589s, episode steps:  98, steps per second: 166, episode reward: -140.063, mean reward: -1.429 [-100.000,  9.186], mean action: 1.357 [0.000, 3.000],  loss: 36.301102, mse: 30189.519970, mean_q: 140.892827, mean_eps: 0.963113
  12416/300000: episode: 134, duration: 0.436s, episode steps:  71, steps per second: 163, episode reward: -29.479, mean reward: -0.415 [-100.000, 16.545], mean action: 1.521 [0.000, 3.000],  loss: 40.527826, mse: 30798.979478, mean_q: 143.865446, mean_eps: 0.962860
  12520/300000: episode: 135, duration: 0.699s, episode steps: 104, steps per second: 149, episode reward: -383.891, mean reward: -3.691 [-100.000,  3.625], mean action: 1.452 [0.000, 3.000],  loss: 36.981643, mse: 30320.763390, mean_q: 143.407767, mean_eps: 0.962597
  12644/300000: episode: 136, duration: 0.763s, episode steps: 124, steps per second: 163, episode reward: -290.520, mean reward: -2.343 [-100.000,  6.353], mean action: 1.573 [0.000, 3.000],  loss: 35.304146, mse: 29666.094364, mean_q: 141.227894, mean_eps: 0.962256
  12775/300000: episode: 137, duration: 0.826s, episode steps: 131, steps per second: 159, episode reward: -465.532, mean reward: -3.554 [-100.000, 111.619], mean action: 1.466 [0.000, 3.000],  loss: 38.766041, mse: 28866.523527, mean_q: 138.551955, mean_eps: 0.961873
  12887/300000: episode: 138, duration: 0.750s, episode steps: 112, steps per second: 149, episode reward: -84.673, mean reward: -0.756 [-100.000, 15.586], mean action: 1.688 [0.000, 3.000],  loss: 42.180037, mse: 28120.559222, mean_q: 136.705902, mean_eps: 0.961508
  12984/300000: episode: 139, duration: 0.645s, episode steps:  97, steps per second: 150, episode reward: -269.434, mean reward: -2.778 [-100.000,  0.781], mean action: 1.598 [0.000, 3.000],  loss: 34.004125, mse: 27732.685023, mean_q: 135.451666, mean_eps: 0.961195
  13052/300000: episode: 140, duration: 0.467s, episode steps:  68, steps per second: 146, episode reward: -117.152, mean reward: -1.723 [-100.000,  9.871], mean action: 1.544 [0.000, 3.000],  loss: 28.846343, mse: 27709.757841, mean_q: 136.031606, mean_eps: 0.960948
  13147/300000: episode: 141, duration: 0.664s, episode steps:  95, steps per second: 143, episode reward: -182.073, mean reward: -1.917 [-100.000, 18.855], mean action: 1.147 [0.000, 3.000],  loss: 34.458595, mse: 26978.740666, mean_q: 131.976999, mean_eps: 0.960703
  13264/300000: episode: 142, duration: 0.747s, episode steps: 117, steps per second: 157, episode reward: -163.699, mean reward: -1.399 [-100.000, 14.236], mean action: 1.521 [0.000, 3.000],  loss: 28.253047, mse: 26714.090027, mean_q: 132.554347, mean_eps: 0.960385
  13367/300000: episode: 143, duration: 0.643s, episode steps: 103, steps per second: 160, episode reward: -20.129, mean reward: -0.195 [-100.000, 93.629], mean action: 1.563 [0.000, 3.000],  loss: 20.062053, mse: 26090.788313, mean_q: 130.572622, mean_eps: 0.960055
  13467/300000: episode: 144, duration: 0.639s, episode steps: 100, steps per second: 157, episode reward: -292.895, mean reward: -2.929 [-100.000,  0.588], mean action: 1.650 [0.000, 3.000],  loss: 41.083473, mse: 25784.494512, mean_q: 128.140109, mean_eps: 0.959750
  13590/300000: episode: 145, duration: 0.780s, episode steps: 123, steps per second: 158, episode reward: -123.931, mean reward: -1.008 [-100.000,  6.736], mean action: 1.610 [0.000, 3.000],  loss: 38.049357, mse: 26193.426266, mean_q: 130.231738, mean_eps: 0.959416
  13677/300000: episode: 146, duration: 0.544s, episode steps:  87, steps per second: 160, episode reward: -54.173, mean reward: -0.623 [-100.000, 14.919], mean action: 1.724 [0.000, 3.000],  loss: 30.877695, mse: 26592.274448, mean_q: 131.515352, mean_eps: 0.959101
  13779/300000: episode: 147, duration: 0.631s, episode steps: 102, steps per second: 162, episode reward: -107.950, mean reward: -1.058 [-100.000, 17.713], mean action: 1.431 [0.000, 3.000],  loss: 36.965349, mse: 26146.245749, mean_q: 128.178762, mean_eps: 0.958817
  13906/300000: episode: 148, duration: 0.812s, episode steps: 127, steps per second: 156, episode reward: -209.498, mean reward: -1.650 [-100.000, 94.773], mean action: 1.551 [0.000, 3.000],  loss: 35.156770, mse: 25218.008781, mean_q: 126.737427, mean_eps: 0.958474
  14013/300000: episode: 149, duration: 0.658s, episode steps: 107, steps per second: 163, episode reward: -145.257, mean reward: -1.358 [-100.000, 15.198], mean action: 1.439 [0.000, 3.000],  loss: 38.244194, mse: 25421.198799, mean_q: 128.148710, mean_eps: 0.958123
  14137/300000: episode: 150, duration: 0.802s, episode steps: 124, steps per second: 155, episode reward: -133.652, mean reward: -1.078 [-100.000,  9.621], mean action: 1.516 [0.000, 3.000],  loss: 40.562460, mse: 25024.382245, mean_q: 125.736639, mean_eps: 0.957777
  14247/300000: episode: 151, duration: 0.685s, episode steps: 110, steps per second: 161, episode reward: -405.428, mean reward: -3.686 [-100.000,  5.301], mean action: 1.591 [0.000, 3.000],  loss: 29.805195, mse: 24635.387305, mean_q: 124.425909, mean_eps: 0.957426
  14354/300000: episode: 152, duration: 0.738s, episode steps: 107, steps per second: 145, episode reward: -233.501, mean reward: -2.182 [-100.000, 26.346], mean action: 1.654 [0.000, 3.000],  loss: 33.153965, mse: 24195.184470, mean_q: 122.499117, mean_eps: 0.957100
  14439/300000: episode: 153, duration: 0.612s, episode steps:  85, steps per second: 139, episode reward: -87.800, mean reward: -1.033 [-100.000,  9.551], mean action: 1.671 [0.000, 3.000],  loss: 31.381606, mse: 23956.177838, mean_q: 122.197748, mean_eps: 0.956812
  14524/300000: episode: 154, duration: 0.572s, episode steps:  85, steps per second: 149, episode reward: -152.985, mean reward: -1.800 [-100.000, 17.348], mean action: 1.576 [0.000, 3.000],  loss: 34.242334, mse: 23915.114212, mean_q: 123.986758, mean_eps: 0.956557
  14597/300000: episode: 155, duration: 0.472s, episode steps:  73, steps per second: 155, episode reward: -79.718, mean reward: -1.092 [-100.000, 15.818], mean action: 1.740 [0.000, 3.000],  loss: 27.834042, mse: 24252.671527, mean_q: 124.364416, mean_eps: 0.956320
  14702/300000: episode: 156, duration: 0.677s, episode steps: 105, steps per second: 155, episode reward: -229.869, mean reward: -2.189 [-100.000,  0.792], mean action: 1.562 [0.000, 3.000],  loss: 33.012774, mse: 24252.525744, mean_q: 122.863953, mean_eps: 0.956053
  14783/300000: episode: 157, duration: 0.542s, episode steps:  81, steps per second: 149, episode reward: -95.229, mean reward: -1.176 [-100.000, 37.315], mean action: 1.543 [0.000, 3.000],  loss: 29.510973, mse: 23226.371986, mean_q: 120.132014, mean_eps: 0.955774
  14883/300000: episode: 158, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: -159.854, mean reward: -1.599 [-100.000,  7.017], mean action: 1.520 [0.000, 3.000],  loss: 38.359116, mse: 23550.519941, mean_q: 123.009075, mean_eps: 0.955503
  14956/300000: episode: 159, duration: 0.492s, episode steps:  73, steps per second: 148, episode reward: -33.561, mean reward: -0.460 [-100.000, 16.855], mean action: 1.562 [0.000, 3.000],  loss: 28.507859, mse: 22808.713854, mean_q: 118.669506, mean_eps: 0.955243
  15043/300000: episode: 160, duration: 0.564s, episode steps:  87, steps per second: 154, episode reward: -426.709, mean reward: -4.905 [-100.000,  0.599], mean action: 1.667 [0.000, 3.000],  loss: 28.572409, mse: 24181.123339, mean_q: 125.107818, mean_eps: 0.955003
  15105/300000: episode: 161, duration: 0.437s, episode steps:  62, steps per second: 142, episode reward: -208.038, mean reward: -3.355 [-100.000,  3.153], mean action: 1.435 [0.000, 3.000],  loss: 29.084742, mse: 23879.587639, mean_q: 124.441377, mean_eps: 0.954780
  15190/300000: episode: 162, duration: 0.618s, episode steps:  85, steps per second: 137, episode reward: -141.263, mean reward: -1.662 [-100.000,  7.990], mean action: 1.600 [0.000, 3.000],  loss: 33.887287, mse: 23030.957364, mean_q: 118.787515, mean_eps: 0.954559
  15300/300000: episode: 163, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: -404.105, mean reward: -3.674 [-100.000,  4.756], mean action: 1.655 [0.000, 3.000],  loss: 30.895360, mse: 23373.781259, mean_q: 120.983390, mean_eps: 0.954266
  15421/300000: episode: 164, duration: 0.784s, episode steps: 121, steps per second: 154, episode reward: -182.310, mean reward: -1.507 [-100.000,  9.173], mean action: 1.554 [0.000, 3.000],  loss: 34.251907, mse: 22828.141255, mean_q: 118.193087, mean_eps: 0.953920
  15500/300000: episode: 165, duration: 0.629s, episode steps:  79, steps per second: 126, episode reward: -127.757, mean reward: -1.617 [-100.000,  9.195], mean action: 1.456 [0.000, 3.000],  loss: 39.352200, mse: 22825.397288, mean_q: 121.444016, mean_eps: 0.953620
  15640/300000: episode: 166, duration: 0.998s, episode steps: 140, steps per second: 140, episode reward: -363.868, mean reward: -2.599 [-100.000, 96.460], mean action: 1.650 [0.000, 3.000],  loss: 31.548144, mse: 23362.971777, mean_q: 120.741710, mean_eps: 0.953292
  15756/300000: episode: 167, duration: 0.778s, episode steps: 116, steps per second: 149, episode reward: -267.654, mean reward: -2.307 [-100.000,  2.101], mean action: 1.578 [0.000, 3.000],  loss: 31.535084, mse: 22906.217841, mean_q: 119.460370, mean_eps: 0.952907
  15865/300000: episode: 168, duration: 0.760s, episode steps: 109, steps per second: 143, episode reward: -193.085, mean reward: -1.771 [-100.000,  2.929], mean action: 1.385 [0.000, 3.000],  loss: 33.784040, mse: 23384.561138, mean_q: 122.086199, mean_eps: 0.952570
  15978/300000: episode: 169, duration: 0.727s, episode steps: 113, steps per second: 155, episode reward: -120.417, mean reward: -1.066 [-100.000,  6.755], mean action: 1.584 [0.000, 3.000],  loss: 31.262810, mse: 23587.374438, mean_q: 122.291665, mean_eps: 0.952237
  16074/300000: episode: 170, duration: 0.687s, episode steps:  96, steps per second: 140, episode reward: -391.487, mean reward: -4.078 [-100.000, 91.116], mean action: 1.500 [0.000, 3.000],  loss: 31.689789, mse: 22135.852030, mean_q: 119.246128, mean_eps: 0.951924
  16206/300000: episode: 171, duration: 0.891s, episode steps: 132, steps per second: 148, episode reward: -218.353, mean reward: -1.654 [-100.000,  2.505], mean action: 1.583 [0.000, 3.000],  loss: 27.646357, mse: 22403.542318, mean_q: 118.691719, mean_eps: 0.951581
  16307/300000: episode: 172, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: -267.819, mean reward: -2.652 [-100.000,  0.531], mean action: 1.604 [0.000, 3.000],  loss: 34.600361, mse: 22841.423093, mean_q: 120.009206, mean_eps: 0.951232
  16379/300000: episode: 173, duration: 0.478s, episode steps:  72, steps per second: 151, episode reward: -49.594, mean reward: -0.689 [-100.000, 25.622], mean action: 1.458 [0.000, 3.000],  loss: 27.832944, mse: 22056.388129, mean_q: 116.063864, mean_eps: 0.950972
  16469/300000: episode: 174, duration: 0.581s, episode steps:  90, steps per second: 155, episode reward: -260.675, mean reward: -2.896 [-100.000, 21.009], mean action: 1.600 [0.000, 3.000],  loss: 31.877699, mse: 21185.439985, mean_q: 113.038283, mean_eps: 0.950729
  16558/300000: episode: 175, duration: 0.552s, episode steps:  89, steps per second: 161, episode reward: -48.561, mean reward: -0.546 [-100.000, 16.031], mean action: 1.573 [0.000, 3.000],  loss: 28.342552, mse: 22226.259250, mean_q: 117.883248, mean_eps: 0.950461
  16658/300000: episode: 176, duration: 0.618s, episode steps: 100, steps per second: 162, episode reward: -238.385, mean reward: -2.384 [-100.000, 16.380], mean action: 1.760 [0.000, 3.000],  loss: 31.482288, mse: 21560.485977, mean_q: 116.617948, mean_eps: 0.950177
  16775/300000: episode: 177, duration: 0.826s, episode steps: 117, steps per second: 142, episode reward: -96.064, mean reward: -0.821 [-100.000, 10.791], mean action: 1.470 [0.000, 3.000],  loss: 31.132284, mse: 21667.826840, mean_q: 117.322911, mean_eps: 0.949852
  16855/300000: episode: 178, duration: 0.685s, episode steps:  80, steps per second: 117, episode reward: -125.113, mean reward: -1.564 [-100.000,  5.056], mean action: 1.600 [0.000, 3.000],  loss: 29.838096, mse: 21458.816919, mean_q: 119.104822, mean_eps: 0.949557
  16915/300000: episode: 179, duration: 0.467s, episode steps:  60, steps per second: 129, episode reward: -157.170, mean reward: -2.619 [-100.000, 73.153], mean action: 1.517 [0.000, 3.000],  loss: 28.710456, mse: 21997.877393, mean_q: 118.804513, mean_eps: 0.949346
  17035/300000: episode: 180, duration: 1.099s, episode steps: 120, steps per second: 109, episode reward: -268.916, mean reward: -2.241 [-100.000,  5.842], mean action: 1.450 [0.000, 3.000],  loss: 31.845265, mse: 21540.861646, mean_q: 118.563706, mean_eps: 0.949077
  17122/300000: episode: 181, duration: 0.671s, episode steps:  87, steps per second: 130, episode reward: -162.819, mean reward: -1.871 [-100.000, 49.388], mean action: 1.517 [0.000, 3.000],  loss: 23.620278, mse: 20500.008868, mean_q: 114.748538, mean_eps: 0.948766
  17266/300000: episode: 182, duration: 1.132s, episode steps: 144, steps per second: 127, episode reward: -116.464, mean reward: -0.809 [-100.000,  8.092], mean action: 1.528 [0.000, 3.000],  loss: 29.599044, mse: 20584.601305, mean_q: 113.879003, mean_eps: 0.948419
  17361/300000: episode: 183, duration: 0.659s, episode steps:  95, steps per second: 144, episode reward: -277.344, mean reward: -2.919 [-100.000, 25.081], mean action: 1.547 [0.000, 3.000],  loss: 31.233202, mse: 20852.782936, mean_q: 118.217832, mean_eps: 0.948061
  17468/300000: episode: 184, duration: 0.704s, episode steps: 107, steps per second: 152, episode reward: -151.519, mean reward: -1.416 [-100.000,  6.733], mean action: 1.579 [0.000, 3.000],  loss: 31.277699, mse: 20274.851636, mean_q: 114.670194, mean_eps: 0.947758
  17564/300000: episode: 185, duration: 0.685s, episode steps:  96, steps per second: 140, episode reward: -320.689, mean reward: -3.341 [-100.000,  0.619], mean action: 1.406 [0.000, 3.000],  loss: 34.897874, mse: 19780.211599, mean_q: 112.071940, mean_eps: 0.947453
  17621/300000: episode: 186, duration: 0.375s, episode steps:  57, steps per second: 152, episode reward: -172.114, mean reward: -3.020 [-100.000,  4.338], mean action: 1.474 [0.000, 3.000],  loss: 25.800518, mse: 20027.479287, mean_q: 115.481465, mean_eps: 0.947224
  17696/300000: episode: 187, duration: 0.496s, episode steps:  75, steps per second: 151, episode reward: -186.906, mean reward: -2.492 [-100.000, 29.058], mean action: 1.600 [0.000, 3.000],  loss: 28.376633, mse: 20119.681458, mean_q: 115.354425, mean_eps: 0.947026
  17775/300000: episode: 188, duration: 0.532s, episode steps:  79, steps per second: 148, episode reward: -71.016, mean reward: -0.899 [-100.000,  9.285], mean action: 1.481 [0.000, 3.000],  loss: 34.889814, mse: 19640.805812, mean_q: 112.230215, mean_eps: 0.946795
  17866/300000: episode: 189, duration: 0.609s, episode steps:  91, steps per second: 149, episode reward: -85.338, mean reward: -0.938 [-100.000, 10.513], mean action: 1.516 [0.000, 3.000],  loss: 26.419740, mse: 19552.704241, mean_q: 112.763129, mean_eps: 0.946540
  17958/300000: episode: 190, duration: 0.603s, episode steps:  92, steps per second: 153, episode reward: -116.223, mean reward: -1.263 [-100.000,  8.415], mean action: 1.598 [0.000, 3.000],  loss: 35.742140, mse: 19187.764617, mean_q: 110.456464, mean_eps: 0.946266
  18048/300000: episode: 191, duration: 0.584s, episode steps:  90, steps per second: 154, episode reward: -189.341, mean reward: -2.104 [-100.000, 54.466], mean action: 1.578 [0.000, 3.000],  loss: 22.150905, mse: 18959.951356, mean_q: 111.559618, mean_eps: 0.945993
  18181/300000: episode: 192, duration: 0.861s, episode steps: 133, steps per second: 155, episode reward: -188.426, mean reward: -1.417 [-100.000,  6.771], mean action: 1.541 [0.000, 3.000],  loss: 23.223764, mse: 18858.403548, mean_q: 110.777657, mean_eps: 0.945658
  18293/300000: episode: 193, duration: 0.756s, episode steps: 112, steps per second: 148, episode reward: -155.049, mean reward: -1.384 [-100.000,  5.440], mean action: 1.786 [0.000, 3.000],  loss: 26.077182, mse: 18468.312212, mean_q: 108.437056, mean_eps: 0.945290
  18351/300000: episode: 194, duration: 0.382s, episode steps:  58, steps per second: 152, episode reward: -41.223, mean reward: -0.711 [-100.000, 64.888], mean action: 1.345 [0.000, 3.000],  loss: 23.657565, mse: 18996.274515, mean_q: 111.809538, mean_eps: 0.945035
  18455/300000: episode: 195, duration: 0.736s, episode steps: 104, steps per second: 141, episode reward: -123.977, mean reward: -1.192 [-100.000, 12.686], mean action: 1.365 [0.000, 3.000],  loss: 39.295790, mse: 18370.510226, mean_q: 107.397200, mean_eps: 0.944793
  18606/300000: episode: 196, duration: 1.084s, episode steps: 151, steps per second: 139, episode reward: -163.450, mean reward: -1.082 [-100.000, 18.798], mean action: 1.470 [0.000, 3.000],  loss: 30.138925, mse: 18262.281884, mean_q: 107.669945, mean_eps: 0.944410
  18711/300000: episode: 197, duration: 0.731s, episode steps: 105, steps per second: 144, episode reward: -301.093, mean reward: -2.868 [-100.000, 21.602], mean action: 1.457 [0.000, 3.000],  loss: 21.923105, mse: 18078.913384, mean_q: 106.822667, mean_eps: 0.944026
  18779/300000: episode: 198, duration: 0.501s, episode steps:  68, steps per second: 136, episode reward: -96.856, mean reward: -1.424 [-100.000,  7.882], mean action: 1.191 [0.000, 3.000],  loss: 17.741777, mse: 18405.288344, mean_q: 108.446826, mean_eps: 0.943766
  18885/300000: episode: 199, duration: 0.777s, episode steps: 106, steps per second: 136, episode reward: -101.955, mean reward: -0.962 [-100.000,  6.817], mean action: 1.453 [0.000, 3.000],  loss: 25.481961, mse: 17355.858693, mean_q: 104.581477, mean_eps: 0.943506
  18976/300000: episode: 200, duration: 0.592s, episode steps:  91, steps per second: 154, episode reward: -248.201, mean reward: -2.727 [-100.000, 11.368], mean action: 1.637 [0.000, 3.000],  loss: 21.248578, mse: 17186.985330, mean_q: 102.873074, mean_eps: 0.943210
  19051/300000: episode: 201, duration: 0.482s, episode steps:  75, steps per second: 156, episode reward: -106.749, mean reward: -1.423 [-100.000,  9.521], mean action: 1.587 [0.000, 3.000],  loss: 22.710043, mse: 17299.078672, mean_q: 104.571883, mean_eps: 0.942961
  19146/300000: episode: 202, duration: 0.674s, episode steps:  95, steps per second: 141, episode reward: -124.028, mean reward: -1.306 [-100.000,  7.329], mean action: 1.653 [0.000, 3.000],  loss: 24.991253, mse: 16977.486194, mean_q: 101.993831, mean_eps: 0.942706
  19248/300000: episode: 203, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -219.998, mean reward: -2.157 [-100.000, 38.515], mean action: 1.451 [0.000, 3.000],  loss: 27.243178, mse: 17168.557847, mean_q: 103.101774, mean_eps: 0.942411
  19368/300000: episode: 204, duration: 0.772s, episode steps: 120, steps per second: 155, episode reward: -133.654, mean reward: -1.114 [-100.000, 16.293], mean action: 1.492 [0.000, 3.000],  loss: 27.039289, mse: 17035.183358, mean_q: 102.168403, mean_eps: 0.942077
  19483/300000: episode: 205, duration: 0.787s, episode steps: 115, steps per second: 146, episode reward: -384.161, mean reward: -3.341 [-100.000,  1.896], mean action: 1.461 [0.000, 3.000],  loss: 28.956689, mse: 17574.871722, mean_q: 105.794717, mean_eps: 0.941725
  19612/300000: episode: 206, duration: 0.841s, episode steps: 129, steps per second: 153, episode reward: -157.754, mean reward: -1.223 [-100.000,  6.889], mean action: 1.543 [0.000, 3.000],  loss: 28.956968, mse: 17193.937159, mean_q: 103.346882, mean_eps: 0.941359
  19702/300000: episode: 207, duration: 0.588s, episode steps:  90, steps per second: 153, episode reward: 38.879, mean reward:  0.432 [-100.000, 82.607], mean action: 1.367 [0.000, 3.000],  loss: 23.323249, mse: 16905.941569, mean_q: 102.634135, mean_eps: 0.941030
  19769/300000: episode: 208, duration: 0.461s, episode steps:  67, steps per second: 145, episode reward: -192.819, mean reward: -2.878 [-100.000,  6.124], mean action: 1.343 [0.000, 3.000],  loss: 20.632310, mse: 17124.447266, mean_q: 103.900783, mean_eps: 0.940795
  20769/300000: episode: 209, duration: 8.216s, episode steps: 1000, steps per second: 122, episode reward: 114.243, mean reward:  0.114 [-24.572, 120.136], mean action: 1.545 [0.000, 3.000],  loss: 24.679751, mse: 15945.592498, mean_q: 98.264081, mean_eps: 0.939195
  20933/300000: episode: 210, duration: 1.147s, episode steps: 164, steps per second: 143, episode reward: -235.830, mean reward: -1.438 [-100.000, 44.530], mean action: 1.476 [0.000, 3.000],  loss: 22.704540, mse: 15779.188810, mean_q: 96.760992, mean_eps: 0.937449
  21012/300000: episode: 211, duration: 0.521s, episode steps:  79, steps per second: 152, episode reward: -196.709, mean reward: -2.490 [-100.000,  7.700], mean action: 1.633 [0.000, 3.000],  loss: 28.840855, mse: 14850.939317, mean_q: 93.815329, mean_eps: 0.937084
  21101/300000: episode: 212, duration: 0.581s, episode steps:  89, steps per second: 153, episode reward: -118.372, mean reward: -1.330 [-100.000, 10.963], mean action: 1.539 [0.000, 3.000],  loss: 29.914964, mse: 15251.353110, mean_q: 95.707875, mean_eps: 0.936832
  21174/300000: episode: 213, duration: 0.525s, episode steps:  73, steps per second: 139, episode reward: -97.256, mean reward: -1.332 [-100.000, 16.146], mean action: 1.342 [0.000, 3.000],  loss: 33.726848, mse: 15244.336446, mean_q: 92.708740, mean_eps: 0.936589
  21251/300000: episode: 214, duration: 0.508s, episode steps:  77, steps per second: 152, episode reward: -189.723, mean reward: -2.464 [-100.000,  8.410], mean action: 1.714 [0.000, 3.000],  loss: 19.970295, mse: 15134.093014, mean_q: 93.740519, mean_eps: 0.936364
  21353/300000: episode: 215, duration: 0.665s, episode steps: 102, steps per second: 153, episode reward: -125.366, mean reward: -1.229 [-100.000, 14.083], mean action: 1.588 [0.000, 3.000],  loss: 23.158385, mse: 15229.400075, mean_q: 93.996394, mean_eps: 0.936095
  21447/300000: episode: 216, duration: 0.603s, episode steps:  94, steps per second: 156, episode reward: -121.469, mean reward: -1.292 [-100.000, 18.554], mean action: 1.362 [0.000, 3.000],  loss: 28.747671, mse: 15742.139783, mean_q: 97.387320, mean_eps: 0.935802
  21538/300000: episode: 217, duration: 0.661s, episode steps:  91, steps per second: 138, episode reward: -166.159, mean reward: -1.826 [-100.000,  7.744], mean action: 1.385 [0.000, 3.000],  loss: 28.796129, mse: 15402.377425, mean_q: 96.978604, mean_eps: 0.935524
  21642/300000: episode: 218, duration: 0.720s, episode steps: 104, steps per second: 144, episode reward: -235.132, mean reward: -2.261 [-100.000, 24.551], mean action: 1.423 [0.000, 3.000],  loss: 20.115060, mse: 15148.918898, mean_q: 95.651506, mean_eps: 0.935231
  21741/300000: episode: 219, duration: 0.616s, episode steps:  99, steps per second: 161, episode reward: -205.723, mean reward: -2.078 [-100.000,  7.169], mean action: 1.364 [0.000, 3.000],  loss: 27.797011, mse: 15074.870650, mean_q: 95.306115, mean_eps: 0.934927
  21841/300000: episode: 220, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -142.792, mean reward: -1.428 [-100.000,  5.371], mean action: 1.680 [0.000, 3.000],  loss: 25.056891, mse: 15399.737471, mean_q: 97.104597, mean_eps: 0.934629
  21934/300000: episode: 221, duration: 0.613s, episode steps:  93, steps per second: 152, episode reward: -288.708, mean reward: -3.104 [-100.000, 37.820], mean action: 1.591 [0.000, 3.000],  loss: 23.818168, mse: 14809.666971, mean_q: 93.071212, mean_eps: 0.934339
  22053/300000: episode: 222, duration: 0.762s, episode steps: 119, steps per second: 156, episode reward: -173.236, mean reward: -1.456 [-100.000, 22.147], mean action: 1.336 [0.000, 3.000],  loss: 25.849922, mse: 15064.797285, mean_q: 93.986645, mean_eps: 0.934021
  22153/300000: episode: 223, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -139.447, mean reward: -1.394 [-100.000,  6.104], mean action: 1.620 [0.000, 3.000],  loss: 26.636611, mse: 15246.246240, mean_q: 95.660172, mean_eps: 0.933692
  22245/300000: episode: 224, duration: 0.645s, episode steps:  92, steps per second: 143, episode reward: -267.665, mean reward: -2.909 [-100.000, 13.759], mean action: 1.359 [0.000, 3.000],  loss: 19.424125, mse: 15010.648820, mean_q: 95.338236, mean_eps: 0.933405
  22341/300000: episode: 225, duration: 0.624s, episode steps:  96, steps per second: 154, episode reward: -136.959, mean reward: -1.427 [-100.000,  5.827], mean action: 1.604 [0.000, 3.000],  loss: 27.506262, mse: 14557.492147, mean_q: 92.200043, mean_eps: 0.933122
  22408/300000: episode: 226, duration: 0.426s, episode steps:  67, steps per second: 157, episode reward: -65.281, mean reward: -0.974 [-100.000, 17.654], mean action: 1.522 [0.000, 3.000],  loss: 25.602303, mse: 14598.550927, mean_q: 91.172136, mean_eps: 0.932878
  22540/300000: episode: 227, duration: 0.883s, episode steps: 132, steps per second: 150, episode reward: -401.386, mean reward: -3.041 [-100.000,  4.443], mean action: 1.341 [0.000, 3.000],  loss: 27.752841, mse: 14772.368201, mean_q: 92.195562, mean_eps: 0.932580
  22674/300000: episode: 228, duration: 0.863s, episode steps: 134, steps per second: 155, episode reward: -217.750, mean reward: -1.625 [-100.000,  5.392], mean action: 1.433 [0.000, 3.000],  loss: 26.217523, mse: 14984.410025, mean_q: 92.824322, mean_eps: 0.932180
  22745/300000: episode: 229, duration: 0.477s, episode steps:  71, steps per second: 149, episode reward: -105.230, mean reward: -1.482 [-100.000, 19.407], mean action: 1.577 [0.000, 3.000],  loss: 21.078904, mse: 15124.189921, mean_q: 94.560474, mean_eps: 0.931873
  22843/300000: episode: 230, duration: 0.656s, episode steps:  98, steps per second: 149, episode reward: -172.726, mean reward: -1.763 [-100.000, 20.394], mean action: 1.612 [0.000, 3.000],  loss: 20.726988, mse: 14668.877352, mean_q: 91.863873, mean_eps: 0.931619
  22933/300000: episode: 231, duration: 0.576s, episode steps:  90, steps per second: 156, episode reward: -111.868, mean reward: -1.243 [-100.000, 11.520], mean action: 1.567 [0.000, 3.000],  loss: 20.587830, mse: 14745.226226, mean_q: 91.572152, mean_eps: 0.931337
  23011/300000: episode: 232, duration: 0.492s, episode steps:  78, steps per second: 159, episode reward: -123.310, mean reward: -1.581 [-100.000,  7.518], mean action: 1.436 [0.000, 3.000],  loss: 25.060678, mse: 14591.579214, mean_q: 90.269635, mean_eps: 0.931086
  23129/300000: episode: 233, duration: 0.760s, episode steps: 118, steps per second: 155, episode reward: -252.136, mean reward: -2.137 [-100.000,  1.437], mean action: 1.551 [0.000, 3.000],  loss: 20.144868, mse: 14713.244223, mean_q: 91.458645, mean_eps: 0.930791
  23212/300000: episode: 234, duration: 0.546s, episode steps:  83, steps per second: 152, episode reward: -234.316, mean reward: -2.823 [-100.000,  6.076], mean action: 1.566 [0.000, 3.000],  loss: 18.582652, mse: 14973.450913, mean_q: 92.357797, mean_eps: 0.930490
  23315/300000: episode: 235, duration: 0.654s, episode steps: 103, steps per second: 157, episode reward: -129.459, mean reward: -1.257 [-100.000,  6.542], mean action: 1.515 [0.000, 3.000],  loss: 27.878150, mse: 14841.107384, mean_q: 91.604340, mean_eps: 0.930211
  23408/300000: episode: 236, duration: 0.614s, episode steps:  93, steps per second: 151, episode reward: -216.515, mean reward: -2.328 [-100.000,  9.064], mean action: 1.333 [0.000, 3.000],  loss: 30.992679, mse: 15186.388472, mean_q: 94.771765, mean_eps: 0.929917
  23510/300000: episode: 237, duration: 0.733s, episode steps: 102, steps per second: 139, episode reward: -114.917, mean reward: -1.127 [-100.000,  8.284], mean action: 1.353 [0.000, 3.000],  loss: 23.630388, mse: 14796.940008, mean_q: 91.758172, mean_eps: 0.929624
  23597/300000: episode: 238, duration: 0.611s, episode steps:  87, steps per second: 142, episode reward: -102.961, mean reward: -1.183 [-100.000,  7.074], mean action: 1.437 [0.000, 3.000],  loss: 30.577665, mse: 14982.103875, mean_q: 94.661042, mean_eps: 0.929341
  23677/300000: episode: 239, duration: 0.548s, episode steps:  80, steps per second: 146, episode reward: 10.182, mean reward:  0.127 [-100.000, 60.764], mean action: 1.400 [0.000, 3.000],  loss: 33.782740, mse: 14834.771289, mean_q: 92.119564, mean_eps: 0.929090
  23803/300000: episode: 240, duration: 0.862s, episode steps: 126, steps per second: 146, episode reward: -145.203, mean reward: -1.152 [-100.000, 11.488], mean action: 1.468 [0.000, 3.000],  loss: 23.088446, mse: 14846.064104, mean_q: 91.641865, mean_eps: 0.928781
  23896/300000: episode: 241, duration: 0.599s, episode steps:  93, steps per second: 155, episode reward: -69.639, mean reward: -0.749 [-100.000, 17.482], mean action: 1.527 [0.000, 3.000],  loss: 26.526891, mse: 15026.685137, mean_q: 93.861799, mean_eps: 0.928453
  24019/300000: episode: 242, duration: 0.771s, episode steps: 123, steps per second: 160, episode reward: -188.959, mean reward: -1.536 [-100.000, 32.073], mean action: 1.398 [0.000, 3.000],  loss: 30.196276, mse: 15091.932816, mean_q: 93.511053, mean_eps: 0.928129
  24116/300000: episode: 243, duration: 0.662s, episode steps:  97, steps per second: 147, episode reward: -227.312, mean reward: -2.343 [-100.000,  0.837], mean action: 1.371 [0.000, 3.000],  loss: 24.239264, mse: 14641.260168, mean_q: 89.854458, mean_eps: 0.927799
  24216/300000: episode: 244, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: -161.341, mean reward: -1.613 [-100.000,  6.802], mean action: 1.400 [0.000, 3.000],  loss: 22.895362, mse: 14910.337158, mean_q: 92.069486, mean_eps: 0.927503
  24293/300000: episode: 245, duration: 0.498s, episode steps:  77, steps per second: 155, episode reward: -72.116, mean reward: -0.937 [-100.000,  7.361], mean action: 1.675 [0.000, 3.000],  loss: 17.962512, mse: 14817.624315, mean_q: 93.463146, mean_eps: 0.927238
  24393/300000: episode: 246, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: -183.058, mean reward: -1.831 [-100.000, 15.110], mean action: 1.560 [0.000, 3.000],  loss: 27.076700, mse: 14421.001621, mean_q: 90.732302, mean_eps: 0.926972
  24461/300000: episode: 247, duration: 0.490s, episode steps:  68, steps per second: 139, episode reward: -98.829, mean reward: -1.453 [-100.000,  6.264], mean action: 1.529 [0.000, 3.000],  loss: 26.636554, mse: 13921.746898, mean_q: 88.178120, mean_eps: 0.926720
  24549/300000: episode: 248, duration: 0.583s, episode steps:  88, steps per second: 151, episode reward: -65.476, mean reward: -0.744 [-100.000, 19.051], mean action: 1.614 [0.000, 3.000],  loss: 24.560234, mse: 13897.400452, mean_q: 85.818189, mean_eps: 0.926486
  24617/300000: episode: 249, duration: 0.438s, episode steps:  68, steps per second: 155, episode reward: -65.340, mean reward: -0.961 [-100.000,  9.427], mean action: 1.706 [0.000, 3.000],  loss: 23.051025, mse: 13720.854607, mean_q: 86.203827, mean_eps: 0.926253
  24743/300000: episode: 250, duration: 0.844s, episode steps: 126, steps per second: 149, episode reward: -100.651, mean reward: -0.799 [-100.000, 11.683], mean action: 1.429 [0.000, 3.000],  loss: 20.179546, mse: 13994.677037, mean_q: 89.800331, mean_eps: 0.925961
  24837/300000: episode: 251, duration: 0.626s, episode steps:  94, steps per second: 150, episode reward: -250.986, mean reward: -2.670 [-100.000, 11.449], mean action: 1.617 [0.000, 3.000],  loss: 19.969406, mse: 13901.220049, mean_q: 88.461373, mean_eps: 0.925632
  24909/300000: episode: 252, duration: 0.478s, episode steps:  72, steps per second: 151, episode reward: -44.594, mean reward: -0.619 [-100.000, 17.187], mean action: 1.542 [0.000, 3.000],  loss: 23.156797, mse: 13915.467611, mean_q: 88.213836, mean_eps: 0.925382
  24989/300000: episode: 253, duration: 0.555s, episode steps:  80, steps per second: 144, episode reward: -142.516, mean reward: -1.781 [-100.000,  7.482], mean action: 1.600 [0.000, 3.000],  loss: 21.770647, mse: 13205.093286, mean_q: 85.509133, mean_eps: 0.925154
  25077/300000: episode: 254, duration: 0.617s, episode steps:  88, steps per second: 143, episode reward: -74.030, mean reward: -0.841 [-100.000, 16.485], mean action: 1.625 [0.000, 3.000],  loss: 22.973966, mse: 13287.197632, mean_q: 86.755757, mean_eps: 0.924903
  25145/300000: episode: 255, duration: 0.461s, episode steps:  68, steps per second: 147, episode reward: -36.401, mean reward: -0.535 [-100.000, 15.071], mean action: 1.721 [0.000, 3.000],  loss: 31.776071, mse: 13306.430836, mean_q: 86.124642, mean_eps: 0.924669
  25219/300000: episode: 256, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -76.775, mean reward: -1.037 [-100.000,  9.886], mean action: 1.446 [0.000, 3.000],  loss: 23.040387, mse: 13501.419731, mean_q: 85.845440, mean_eps: 0.924455
  25346/300000: episode: 257, duration: 0.842s, episode steps: 127, steps per second: 151, episode reward: -55.496, mean reward: -0.437 [-100.000, 20.638], mean action: 1.551 [0.000, 3.000],  loss: 29.150477, mse: 13013.781411, mean_q: 83.423577, mean_eps: 0.924154
  25495/300000: episode: 258, duration: 1.067s, episode steps: 149, steps per second: 140, episode reward: -342.106, mean reward: -2.296 [-100.000, 108.079], mean action: 1.651 [0.000, 3.000],  loss: 26.505351, mse: 12721.703872, mean_q: 81.637468, mean_eps: 0.923740
  25619/300000: episode: 259, duration: 0.818s, episode steps: 124, steps per second: 152, episode reward: -36.364, mean reward: -0.293 [-100.000, 38.435], mean action: 1.677 [0.000, 3.000],  loss: 21.435826, mse: 12504.068611, mean_q: 80.706238, mean_eps: 0.923330
  25713/300000: episode: 260, duration: 0.621s, episode steps:  94, steps per second: 151, episode reward: -324.351, mean reward: -3.451 [-100.000,  0.692], mean action: 1.532 [0.000, 3.000],  loss: 21.562783, mse: 12694.645310, mean_q: 81.756908, mean_eps: 0.923003
  25792/300000: episode: 261, duration: 0.511s, episode steps:  79, steps per second: 155, episode reward: -64.382, mean reward: -0.815 [-100.000,  9.379], mean action: 1.405 [0.000, 3.000],  loss: 25.882158, mse: 12043.806783, mean_q: 78.442220, mean_eps: 0.922744
  25904/300000: episode: 262, duration: 0.700s, episode steps: 112, steps per second: 160, episode reward: -139.886, mean reward: -1.249 [-100.000, 10.184], mean action: 1.411 [0.000, 3.000],  loss: 19.250518, mse: 12026.252010, mean_q: 79.954472, mean_eps: 0.922457
  26020/300000: episode: 263, duration: 0.760s, episode steps: 116, steps per second: 153, episode reward: -276.171, mean reward: -2.381 [-100.000,  8.916], mean action: 1.629 [0.000, 3.000],  loss: 21.992873, mse: 12207.322169, mean_q: 81.246875, mean_eps: 0.922116
  26100/300000: episode: 264, duration: 0.538s, episode steps:  80, steps per second: 149, episode reward: -54.285, mean reward: -0.679 [-100.000, 11.629], mean action: 1.837 [0.000, 3.000],  loss: 24.500278, mse: 11970.261572, mean_q: 80.366761, mean_eps: 0.921821
  26235/300000: episode: 265, duration: 0.865s, episode steps: 135, steps per second: 156, episode reward: -236.532, mean reward: -1.752 [-100.000, 33.915], mean action: 1.526 [0.000, 3.000],  loss: 20.367257, mse: 11845.705324, mean_q: 79.722420, mean_eps: 0.921499
  26328/300000: episode: 266, duration: 0.679s, episode steps:  93, steps per second: 137, episode reward: -88.006, mean reward: -0.946 [-100.000,  9.995], mean action: 1.398 [0.000, 3.000],  loss: 19.360350, mse: 11600.130466, mean_q: 76.448242, mean_eps: 0.921157
  26459/300000: episode: 267, duration: 1.011s, episode steps: 131, steps per second: 130, episode reward: -152.650, mean reward: -1.165 [-100.000,  4.976], mean action: 1.496 [0.000, 3.000],  loss: 27.108345, mse: 11464.258129, mean_q: 78.107208, mean_eps: 0.920821
  26548/300000: episode: 268, duration: 0.659s, episode steps:  89, steps per second: 135, episode reward: -100.287, mean reward: -1.127 [-100.000,  8.738], mean action: 1.449 [0.000, 3.000],  loss: 24.129805, mse: 11439.815216, mean_q: 77.817623, mean_eps: 0.920491
  26649/300000: episode: 269, duration: 0.746s, episode steps: 101, steps per second: 135, episode reward: -112.063, mean reward: -1.110 [-100.000, 25.969], mean action: 1.465 [0.000, 3.000],  loss: 30.304282, mse: 11260.836324, mean_q: 77.334673, mean_eps: 0.920206
  26742/300000: episode: 270, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -160.019, mean reward: -1.721 [-100.000, 47.290], mean action: 1.645 [0.000, 3.000],  loss: 24.138309, mse: 11485.867503, mean_q: 78.848007, mean_eps: 0.919915
  26831/300000: episode: 271, duration: 0.612s, episode steps:  89, steps per second: 146, episode reward: -107.423, mean reward: -1.207 [-100.000, 15.480], mean action: 1.517 [0.000, 3.000],  loss: 15.652878, mse: 11390.229108, mean_q: 80.331995, mean_eps: 0.919642
  26907/300000: episode: 272, duration: 0.637s, episode steps:  76, steps per second: 119, episode reward: -70.201, mean reward: -0.924 [-100.000, 16.349], mean action: 1.553 [0.000, 3.000],  loss: 25.325356, mse: 10981.064697, mean_q: 77.028367, mean_eps: 0.919395
  27003/300000: episode: 273, duration: 0.733s, episode steps:  96, steps per second: 131, episode reward: -76.217, mean reward: -0.794 [-100.000,  8.674], mean action: 1.562 [0.000, 3.000],  loss: 19.427293, mse: 11248.913549, mean_q: 75.364940, mean_eps: 0.919136
  27119/300000: episode: 274, duration: 0.789s, episode steps: 116, steps per second: 147, episode reward: -229.434, mean reward: -1.978 [-100.000,  7.037], mean action: 1.603 [0.000, 3.000],  loss: 16.639406, mse: 11012.128064, mean_q: 75.406579, mean_eps: 0.918818
  27229/300000: episode: 275, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -230.525, mean reward: -2.096 [-100.000,  1.131], mean action: 1.555 [0.000, 3.000],  loss: 22.913453, mse: 10640.367583, mean_q: 71.660753, mean_eps: 0.918479
  27297/300000: episode: 276, duration: 0.495s, episode steps:  68, steps per second: 137, episode reward: -97.428, mean reward: -1.433 [-100.000, 35.731], mean action: 1.662 [0.000, 3.000],  loss: 22.431447, mse: 10722.190430, mean_q: 72.370998, mean_eps: 0.918212
  27392/300000: episode: 277, duration: 0.620s, episode steps:  95, steps per second: 153, episode reward: -129.113, mean reward: -1.359 [-100.000, 12.037], mean action: 1.632 [0.000, 3.000],  loss: 25.716222, mse: 11113.894290, mean_q: 73.678313, mean_eps: 0.917968
  27493/300000: episode: 278, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: -133.363, mean reward: -1.320 [-100.000,  6.644], mean action: 1.574 [0.000, 3.000],  loss: 25.053390, mse: 11217.186905, mean_q: 75.961273, mean_eps: 0.917674
  27614/300000: episode: 279, duration: 0.844s, episode steps: 121, steps per second: 143, episode reward: -108.799, mean reward: -0.899 [-100.000,  9.733], mean action: 1.488 [0.000, 3.000],  loss: 23.031238, mse: 10703.140524, mean_q: 71.835136, mean_eps: 0.917341
  27682/300000: episode: 280, duration: 0.449s, episode steps:  68, steps per second: 151, episode reward: -70.112, mean reward: -1.031 [-100.000, 11.051], mean action: 1.485 [0.000, 3.000],  loss: 30.380507, mse: 10619.713458, mean_q: 71.473018, mean_eps: 0.917058
  27774/300000: episode: 281, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -121.235, mean reward: -1.318 [-100.000, 18.483], mean action: 1.424 [0.000, 3.000],  loss: 24.886272, mse: 10724.171944, mean_q: 73.758787, mean_eps: 0.916818
  27853/300000: episode: 282, duration: 0.539s, episode steps:  79, steps per second: 147, episode reward: -70.917, mean reward: -0.898 [-100.000, 15.489], mean action: 1.810 [0.000, 3.000],  loss: 20.414494, mse: 10722.815473, mean_q: 74.557482, mean_eps: 0.916561
  27982/300000: episode: 283, duration: 0.972s, episode steps: 129, steps per second: 133, episode reward: -109.910, mean reward: -0.852 [-100.000,  6.053], mean action: 1.605 [0.000, 3.000],  loss: 19.216749, mse: 10914.641518, mean_q: 75.936699, mean_eps: 0.916249
  28076/300000: episode: 284, duration: 0.653s, episode steps:  94, steps per second: 144, episode reward: -86.654, mean reward: -0.922 [-100.000,  8.915], mean action: 1.798 [0.000, 3.000],  loss: 22.214987, mse: 11037.048906, mean_q: 75.605077, mean_eps: 0.915914
  28161/300000: episode: 285, duration: 0.617s, episode steps:  85, steps per second: 138, episode reward: -86.335, mean reward: -1.016 [-100.000, 12.281], mean action: 1.459 [0.000, 3.000],  loss: 18.400399, mse: 10457.851545, mean_q: 71.589135, mean_eps: 0.915646
  28251/300000: episode: 286, duration: 0.759s, episode steps:  90, steps per second: 119, episode reward: -135.910, mean reward: -1.510 [-100.000,  9.643], mean action: 1.633 [0.000, 3.000],  loss: 19.858553, mse: 10425.216732, mean_q: 72.926850, mean_eps: 0.915384
  28332/300000: episode: 287, duration: 0.647s, episode steps:  81, steps per second: 125, episode reward: -127.018, mean reward: -1.568 [-100.000,  4.833], mean action: 1.531 [0.000, 3.000],  loss: 18.505853, mse: 10184.988125, mean_q: 71.524654, mean_eps: 0.915127
  28400/300000: episode: 288, duration: 0.486s, episode steps:  68, steps per second: 140, episode reward: -89.143, mean reward: -1.311 [-100.000,  8.588], mean action: 1.691 [0.000, 3.000],  loss: 16.887882, mse: 10339.244586, mean_q: 74.830528, mean_eps: 0.914903
  28496/300000: episode: 289, duration: 0.661s, episode steps:  96, steps per second: 145, episode reward: -174.085, mean reward: -1.813 [-100.000, 13.932], mean action: 1.750 [0.000, 3.000],  loss: 20.830285, mse: 10094.197912, mean_q: 72.042180, mean_eps: 0.914658
  28574/300000: episode: 290, duration: 0.508s, episode steps:  78, steps per second: 153, episode reward: -112.347, mean reward: -1.440 [-100.000,  5.692], mean action: 1.718 [0.000, 3.000],  loss: 24.840530, mse: 9726.700321, mean_q: 70.065241, mean_eps: 0.914396
  28644/300000: episode: 291, duration: 0.456s, episode steps:  70, steps per second: 154, episode reward: -89.059, mean reward: -1.272 [-100.000,  6.210], mean action: 1.657 [0.000, 3.000],  loss: 18.058774, mse: 9676.814335, mean_q: 69.481295, mean_eps: 0.914174
  28779/300000: episode: 292, duration: 0.866s, episode steps: 135, steps per second: 156, episode reward: -119.574, mean reward: -0.886 [-100.000,  9.091], mean action: 1.504 [0.000, 3.000],  loss: 14.653467, mse: 9531.254583, mean_q: 68.856906, mean_eps: 0.913867
  28848/300000: episode: 293, duration: 0.476s, episode steps:  69, steps per second: 145, episode reward: -108.845, mean reward: -1.577 [-100.000,  6.539], mean action: 1.391 [0.000, 3.000],  loss: 29.424659, mse: 9919.937436, mean_q: 68.943975, mean_eps: 0.913561
  28976/300000: episode: 294, duration: 0.846s, episode steps: 128, steps per second: 151, episode reward: 30.815, mean reward:  0.241 [-100.000, 93.970], mean action: 1.617 [0.000, 3.000],  loss: 19.591765, mse: 9851.077957, mean_q: 68.471533, mean_eps: 0.913266
  29075/300000: episode: 295, duration: 0.646s, episode steps:  99, steps per second: 153, episode reward: -107.148, mean reward: -1.082 [-100.000,  6.912], mean action: 1.434 [0.000, 3.000],  loss: 19.610054, mse: 10137.332021, mean_q: 71.688728, mean_eps: 0.912925
  29162/300000: episode: 296, duration: 0.611s, episode steps:  87, steps per second: 142, episode reward: -125.028, mean reward: -1.437 [-100.000, 12.202], mean action: 1.644 [0.000, 3.000],  loss: 23.165937, mse: 10060.385742, mean_q: 71.344494, mean_eps: 0.912646
  29244/300000: episode: 297, duration: 0.560s, episode steps:  82, steps per second: 146, episode reward: -71.044, mean reward: -0.866 [-100.000, 16.150], mean action: 1.634 [0.000, 3.000],  loss: 20.344314, mse: 9726.808784, mean_q: 69.076065, mean_eps: 0.912392
  29338/300000: episode: 298, duration: 0.618s, episode steps:  94, steps per second: 152, episode reward: -161.595, mean reward: -1.719 [-100.000, 10.968], mean action: 1.489 [0.000, 3.000],  loss: 22.787175, mse: 9905.633420, mean_q: 71.578334, mean_eps: 0.912129
  29418/300000: episode: 299, duration: 0.537s, episode steps:  80, steps per second: 149, episode reward: -273.911, mean reward: -3.424 [-100.000, 96.052], mean action: 1.613 [0.000, 3.000],  loss: 27.016043, mse: 9710.871307, mean_q: 70.696815, mean_eps: 0.911867
  29545/300000: episode: 300, duration: 0.903s, episode steps: 127, steps per second: 141, episode reward: -96.752, mean reward: -0.762 [-100.000,  8.432], mean action: 1.512 [0.000, 3.000],  loss: 21.690699, mse: 10001.682156, mean_q: 72.993590, mean_eps: 0.911557
  29640/300000: episode: 301, duration: 0.606s, episode steps:  95, steps per second: 157, episode reward: -389.668, mean reward: -4.102 [-100.000, 37.494], mean action: 1.495 [0.000, 3.000],  loss: 20.981449, mse: 9800.330813, mean_q: 71.432370, mean_eps: 0.911224
  29788/300000: episode: 302, duration: 1.016s, episode steps: 148, steps per second: 146, episode reward: -191.658, mean reward: -1.295 [-100.000, 30.478], mean action: 1.486 [0.000, 3.000],  loss: 20.111370, mse: 10078.003738, mean_q: 73.591309, mean_eps: 0.910860
  29868/300000: episode: 303, duration: 0.621s, episode steps:  80, steps per second: 129, episode reward: -43.833, mean reward: -0.548 [-100.000, 16.361], mean action: 1.575 [0.000, 3.000],  loss: 15.725580, mse: 9890.284875, mean_q: 72.198605, mean_eps: 0.910517
  29967/300000: episode: 304, duration: 0.648s, episode steps:  99, steps per second: 153, episode reward: -114.106, mean reward: -1.153 [-100.000, 11.556], mean action: 1.657 [0.000, 3.000],  loss: 21.283334, mse: 9778.385505, mean_q: 70.600186, mean_eps: 0.910249
  30057/300000: episode: 305, duration: 0.641s, episode steps:  90, steps per second: 140, episode reward: -249.950, mean reward: -2.777 [-100.000, 12.048], mean action: 1.678 [0.000, 3.000],  loss: 22.748329, mse: 10310.679080, mean_q: 73.199535, mean_eps: 0.909965
  30159/300000: episode: 306, duration: 0.743s, episode steps: 102, steps per second: 137, episode reward: -96.067, mean reward: -0.942 [-100.000,  8.112], mean action: 1.510 [0.000, 3.000],  loss: 23.426449, mse: 9807.217113, mean_q: 71.979407, mean_eps: 0.909678
  30258/300000: episode: 307, duration: 0.667s, episode steps:  99, steps per second: 149, episode reward: -90.704, mean reward: -0.916 [-100.000, 24.439], mean action: 1.727 [0.000, 3.000],  loss: 22.016621, mse: 9374.763218, mean_q: 69.207379, mean_eps: 0.909376
  30373/300000: episode: 308, duration: 0.783s, episode steps: 115, steps per second: 147, episode reward: -130.951, mean reward: -1.139 [-100.000,  6.428], mean action: 1.496 [0.000, 3.000],  loss: 26.563453, mse: 9795.436884, mean_q: 72.167117, mean_eps: 0.909055
  30525/300000: episode: 309, duration: 1.093s, episode steps: 152, steps per second: 139, episode reward: -133.239, mean reward: -0.877 [-100.000,  5.642], mean action: 1.487 [0.000, 3.000],  loss: 18.482216, mse: 9514.555478, mean_q: 72.398576, mean_eps: 0.908655
  30604/300000: episode: 310, duration: 0.547s, episode steps:  79, steps per second: 144, episode reward: -85.743, mean reward: -1.085 [-100.000,  6.034], mean action: 1.494 [0.000, 3.000],  loss: 22.486668, mse: 9664.280020, mean_q: 72.286746, mean_eps: 0.908308
  30706/300000: episode: 311, duration: 0.736s, episode steps: 102, steps per second: 139, episode reward: -96.081, mean reward: -0.942 [-100.000,  6.815], mean action: 1.510 [0.000, 3.000],  loss: 21.510477, mse: 9489.525898, mean_q: 72.515600, mean_eps: 0.908037
  30788/300000: episode: 312, duration: 0.593s, episode steps:  82, steps per second: 138, episode reward: -90.443, mean reward: -1.103 [-100.000,  7.854], mean action: 1.427 [0.000, 3.000],  loss: 13.279074, mse: 9506.204947, mean_q: 73.905885, mean_eps: 0.907761
  30900/300000: episode: 313, duration: 0.776s, episode steps: 112, steps per second: 144, episode reward: -305.513, mean reward: -2.728 [-100.000, 106.234], mean action: 1.446 [0.000, 3.000],  loss: 23.883897, mse: 9573.800140, mean_q: 73.579114, mean_eps: 0.907470
  30985/300000: episode: 314, duration: 0.684s, episode steps:  85, steps per second: 124, episode reward: -109.655, mean reward: -1.290 [-100.000,  6.447], mean action: 1.741 [0.000, 3.000],  loss: 20.641071, mse: 9647.311363, mean_q: 73.342736, mean_eps: 0.907174
  31086/300000: episode: 315, duration: 0.796s, episode steps: 101, steps per second: 127, episode reward: -102.520, mean reward: -1.015 [-100.000, 16.481], mean action: 1.376 [0.000, 3.000],  loss: 24.911028, mse: 9331.721868, mean_q: 72.238314, mean_eps: 0.906895
  31194/300000: episode: 316, duration: 0.766s, episode steps: 108, steps per second: 141, episode reward: -232.947, mean reward: -2.157 [-100.000,  3.883], mean action: 1.806 [0.000, 3.000],  loss: 16.854604, mse: 9367.408949, mean_q: 73.117379, mean_eps: 0.906581
  31295/300000: episode: 317, duration: 0.744s, episode steps: 101, steps per second: 136, episode reward: -106.019, mean reward: -1.050 [-100.000, 15.042], mean action: 1.584 [0.000, 3.000],  loss: 21.626081, mse: 9372.349648, mean_q: 70.943092, mean_eps: 0.906268
  31419/300000: episode: 318, duration: 0.928s, episode steps: 124, steps per second: 134, episode reward: -114.464, mean reward: -0.923 [-100.000, 22.523], mean action: 1.435 [0.000, 3.000],  loss: 24.419985, mse: 9728.776064, mean_q: 75.280851, mean_eps: 0.905930
  31525/300000: episode: 319, duration: 0.861s, episode steps: 106, steps per second: 123, episode reward: -120.020, mean reward: -1.132 [-100.000,  8.317], mean action: 1.528 [0.000, 3.000],  loss: 18.820466, mse: 9394.604971, mean_q: 72.790660, mean_eps: 0.905586
  31590/300000: episode: 320, duration: 0.498s, episode steps:  65, steps per second: 131, episode reward: -76.427, mean reward: -1.176 [-100.000, 10.430], mean action: 1.523 [0.000, 3.000],  loss: 25.273308, mse: 9378.476315, mean_q: 73.065212, mean_eps: 0.905329
  31690/300000: episode: 321, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -340.167, mean reward: -3.402 [-100.000,  4.617], mean action: 1.510 [0.000, 3.000],  loss: 20.234476, mse: 9407.606060, mean_q: 73.863903, mean_eps: 0.905081
  31769/300000: episode: 322, duration: 0.590s, episode steps:  79, steps per second: 134, episode reward: 32.697, mean reward:  0.414 [-100.000, 113.424], mean action: 1.468 [0.000, 3.000],  loss: 18.037892, mse: 9522.164922, mean_q: 72.431493, mean_eps: 0.904813
  31857/300000: episode: 323, duration: 0.629s, episode steps:  88, steps per second: 140, episode reward: -70.357, mean reward: -0.800 [-100.000, 10.055], mean action: 1.727 [0.000, 3.000],  loss: 29.523991, mse: 9228.006731, mean_q: 70.382034, mean_eps: 0.904563
  31945/300000: episode: 324, duration: 0.624s, episode steps:  88, steps per second: 141, episode reward: -96.094, mean reward: -1.092 [-100.000,  9.677], mean action: 1.364 [0.000, 3.000],  loss: 19.957152, mse: 9666.682318, mean_q: 74.149011, mean_eps: 0.904299
  32945/300000: episode: 325, duration: 7.338s, episode steps: 1000, steps per second: 136, episode reward: 54.624, mean reward:  0.055 [-24.435, 131.365], mean action: 1.526 [0.000, 3.000],  loss: 22.011154, mse: 9445.373316, mean_q: 73.117374, mean_eps: 0.902667
  33016/300000: episode: 326, duration: 0.505s, episode steps:  71, steps per second: 141, episode reward: -99.510, mean reward: -1.402 [-100.000,  7.392], mean action: 1.493 [0.000, 3.000],  loss: 34.747573, mse: 9121.668767, mean_q: 72.667175, mean_eps: 0.901060
  33078/300000: episode: 327, duration: 0.437s, episode steps:  62, steps per second: 142, episode reward: -172.556, mean reward: -2.783 [-100.000,  8.007], mean action: 1.613 [0.000, 3.000],  loss: 23.144222, mse: 9158.538196, mean_q: 73.301299, mean_eps: 0.900861
  33170/300000: episode: 328, duration: 0.625s, episode steps:  92, steps per second: 147, episode reward: -96.518, mean reward: -1.049 [-100.000, 33.989], mean action: 1.652 [0.000, 3.000],  loss: 24.914472, mse: 9327.030847, mean_q: 73.970519, mean_eps: 0.900629
  33295/300000: episode: 329, duration: 0.869s, episode steps: 125, steps per second: 144, episode reward: -78.210, mean reward: -0.626 [-100.000, 11.839], mean action: 1.520 [0.000, 3.000],  loss: 26.701248, mse: 9510.314867, mean_q: 74.629194, mean_eps: 0.900304
  33357/300000: episode: 330, duration: 0.461s, episode steps:  62, steps per second: 134, episode reward: -58.798, mean reward: -0.948 [-100.000, 19.120], mean action: 1.339 [0.000, 3.000],  loss: 25.849386, mse: 9144.645941, mean_q: 72.651619, mean_eps: 0.900023
  33427/300000: episode: 331, duration: 0.477s, episode steps:  70, steps per second: 147, episode reward: -108.688, mean reward: -1.553 [-100.000,  6.767], mean action: 1.400 [0.000, 3.000],  loss: 30.433341, mse: 9258.169768, mean_q: 73.109716, mean_eps: 0.899826
  33500/300000: episode: 332, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -170.386, mean reward: -2.334 [-100.000, 65.478], mean action: 1.342 [0.000, 3.000],  loss: 28.652705, mse: 9378.282374, mean_q: 76.005503, mean_eps: 0.899611
  33587/300000: episode: 333, duration: 0.586s, episode steps:  87, steps per second: 149, episode reward: -123.821, mean reward: -1.423 [-100.000,  9.181], mean action: 1.575 [0.000, 3.000],  loss: 21.444220, mse: 9511.809104, mean_q: 75.806907, mean_eps: 0.899371
  33686/300000: episode: 334, duration: 0.726s, episode steps:  99, steps per second: 136, episode reward: -98.359, mean reward: -0.994 [-100.000, 11.552], mean action: 1.515 [0.000, 3.000],  loss: 19.983003, mse: 9647.886359, mean_q: 76.703887, mean_eps: 0.899092
  33761/300000: episode: 335, duration: 0.525s, episode steps:  75, steps per second: 143, episode reward: -91.653, mean reward: -1.222 [-100.000, 12.122], mean action: 1.640 [0.000, 3.000],  loss: 30.071394, mse: 9690.115983, mean_q: 76.383407, mean_eps: 0.898831
  33864/300000: episode: 336, duration: 0.688s, episode steps: 103, steps per second: 150, episode reward: -155.983, mean reward: -1.514 [-100.000, 12.376], mean action: 1.427 [0.000, 3.000],  loss: 24.158900, mse: 9299.892938, mean_q: 73.484637, mean_eps: 0.898564
  33932/300000: episode: 337, duration: 0.444s, episode steps:  68, steps per second: 153, episode reward: -86.722, mean reward: -1.275 [-100.000, 10.874], mean action: 1.515 [0.000, 3.000],  loss: 30.468564, mse: 9357.471364, mean_q: 72.005421, mean_eps: 0.898307
  34035/300000: episode: 338, duration: 0.716s, episode steps: 103, steps per second: 144, episode reward: -170.113, mean reward: -1.652 [-100.000,  1.599], mean action: 1.680 [0.000, 3.000],  loss: 20.970394, mse: 9253.430309, mean_q: 73.137231, mean_eps: 0.898051
  34168/300000: episode: 339, duration: 0.896s, episode steps: 133, steps per second: 148, episode reward: -65.382, mean reward: -0.492 [-100.000, 11.513], mean action: 1.444 [0.000, 3.000],  loss: 21.282993, mse: 9235.865047, mean_q: 74.829478, mean_eps: 0.897697
  34230/300000: episode: 340, duration: 0.432s, episode steps:  62, steps per second: 144, episode reward: -104.944, mean reward: -1.693 [-100.000, 12.454], mean action: 1.516 [0.000, 3.000],  loss: 18.518792, mse: 9054.203125, mean_q: 74.017157, mean_eps: 0.897405
  34295/300000: episode: 341, duration: 0.470s, episode steps:  65, steps per second: 138, episode reward: -130.498, mean reward: -2.008 [-100.000,  6.242], mean action: 1.415 [0.000, 3.000],  loss: 26.590714, mse: 8894.393292, mean_q: 73.185744, mean_eps: 0.897214
  34415/300000: episode: 342, duration: 0.803s, episode steps: 120, steps per second: 149, episode reward: -283.503, mean reward: -2.363 [-100.000,  3.672], mean action: 1.650 [0.000, 3.000],  loss: 26.314957, mse: 8832.034477, mean_q: 72.644463, mean_eps: 0.896936
  34488/300000: episode: 343, duration: 0.473s, episode steps:  73, steps per second: 154, episode reward: -110.594, mean reward: -1.515 [-100.000, 16.569], mean action: 1.644 [0.000, 3.000],  loss: 16.484705, mse: 9095.235760, mean_q: 74.054377, mean_eps: 0.896647
  34545/300000: episode: 344, duration: 0.376s, episode steps:  57, steps per second: 152, episode reward: -165.063, mean reward: -2.896 [-100.000,  8.808], mean action: 1.439 [0.000, 3.000],  loss: 19.328094, mse: 8550.545873, mean_q: 71.210793, mean_eps: 0.896452
  34647/300000: episode: 345, duration: 0.707s, episode steps: 102, steps per second: 144, episode reward: -115.270, mean reward: -1.130 [-100.000,  7.009], mean action: 1.461 [0.000, 3.000],  loss: 18.971861, mse: 8542.214020, mean_q: 72.463675, mean_eps: 0.896213
  34726/300000: episode: 346, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -135.126, mean reward: -1.710 [-100.000, 45.881], mean action: 1.696 [0.000, 3.000],  loss: 17.819159, mse: 8787.023184, mean_q: 74.379085, mean_eps: 0.895942
  34861/300000: episode: 347, duration: 0.872s, episode steps: 135, steps per second: 155, episode reward: -209.337, mean reward: -1.551 [-100.000, 22.903], mean action: 1.689 [0.000, 3.000],  loss: 24.195468, mse: 8447.811100, mean_q: 71.423090, mean_eps: 0.895621
  34938/300000: episode: 348, duration: 0.557s, episode steps:  77, steps per second: 138, episode reward: -104.642, mean reward: -1.359 [-100.000,  7.195], mean action: 1.506 [0.000, 3.000],  loss: 18.494551, mse: 8023.847231, mean_q: 68.591433, mean_eps: 0.895303
  35036/300000: episode: 349, duration: 0.671s, episode steps:  98, steps per second: 146, episode reward: -56.929, mean reward: -0.581 [-100.000, 10.335], mean action: 1.541 [0.000, 3.000],  loss: 21.770273, mse: 8286.949184, mean_q: 70.805080, mean_eps: 0.895040
  35168/300000: episode: 350, duration: 0.849s, episode steps: 132, steps per second: 155, episode reward: -92.447, mean reward: -0.700 [-100.000,  9.297], mean action: 1.455 [0.000, 3.000],  loss: 21.387555, mse: 8065.029918, mean_q: 69.066743, mean_eps: 0.894695
  35245/300000: episode: 351, duration: 0.585s, episode steps:  77, steps per second: 132, episode reward: -44.338, mean reward: -0.576 [-100.000,  9.094], mean action: 1.558 [0.000, 3.000],  loss: 29.924145, mse: 8178.881652, mean_q: 69.612800, mean_eps: 0.894382
  35331/300000: episode: 352, duration: 0.677s, episode steps:  86, steps per second: 127, episode reward: -93.430, mean reward: -1.086 [-100.000,  7.572], mean action: 1.488 [0.000, 3.000],  loss: 18.134739, mse: 8344.067417, mean_q: 70.804168, mean_eps: 0.894138
  35413/300000: episode: 353, duration: 0.552s, episode steps:  82, steps per second: 149, episode reward: -84.420, mean reward: -1.030 [-100.000, 12.004], mean action: 1.488 [0.000, 3.000],  loss: 15.822908, mse: 8257.836950, mean_q: 70.898727, mean_eps: 0.893885
  35536/300000: episode: 354, duration: 0.908s, episode steps: 123, steps per second: 135, episode reward: -80.101, mean reward: -0.651 [-100.000,  5.743], mean action: 1.732 [0.000, 3.000],  loss: 17.063387, mse: 8600.052711, mean_q: 72.537006, mean_eps: 0.893578
  35643/300000: episode: 355, duration: 0.916s, episode steps: 107, steps per second: 117, episode reward: -105.752, mean reward: -0.988 [-100.000, 10.342], mean action: 1.542 [0.000, 3.000],  loss: 22.267657, mse: 8357.860046, mean_q: 71.848933, mean_eps: 0.893233
  35740/300000: episode: 356, duration: 0.792s, episode steps:  97, steps per second: 123, episode reward: -74.970, mean reward: -0.773 [-100.000, 16.047], mean action: 1.619 [0.000, 3.000],  loss: 23.675703, mse: 8858.332887, mean_q: 73.398867, mean_eps: 0.892927
  35818/300000: episode: 357, duration: 0.678s, episode steps:  78, steps per second: 115, episode reward: -80.316, mean reward: -1.030 [-100.000, 20.270], mean action: 1.577 [0.000, 3.000],  loss: 21.768643, mse: 8965.989258, mean_q: 75.183300, mean_eps: 0.892664
  35924/300000: episode: 358, duration: 0.822s, episode steps: 106, steps per second: 129, episode reward: -247.227, mean reward: -2.332 [-100.000, 62.380], mean action: 1.358 [0.000, 3.000],  loss: 21.553355, mse: 8839.961347, mean_q: 74.919533, mean_eps: 0.892389
  36023/300000: episode: 359, duration: 0.726s, episode steps:  99, steps per second: 136, episode reward: -85.047, mean reward: -0.859 [-100.000, 11.557], mean action: 1.677 [0.000, 3.000],  loss: 16.694730, mse: 9243.664684, mean_q: 76.925393, mean_eps: 0.892081
  36096/300000: episode: 360, duration: 0.594s, episode steps:  73, steps per second: 123, episode reward: -92.581, mean reward: -1.268 [-100.000, 19.714], mean action: 1.589 [0.000, 3.000],  loss: 19.369669, mse: 9343.190891, mean_q: 79.601694, mean_eps: 0.891823
  36259/300000: episode: 361, duration: 1.290s, episode steps: 163, steps per second: 126, episode reward: -22.485, mean reward: -0.138 [-100.000, 83.703], mean action: 1.620 [0.000, 3.000],  loss: 18.913916, mse: 9285.808762, mean_q: 77.586175, mean_eps: 0.891469
  36321/300000: episode: 362, duration: 0.450s, episode steps:  62, steps per second: 138, episode reward: -119.142, mean reward: -1.922 [-100.000, 34.992], mean action: 1.419 [0.000, 3.000],  loss: 23.102627, mse: 9200.489573, mean_q: 76.692944, mean_eps: 0.891132
  36394/300000: episode: 363, duration: 0.560s, episode steps:  73, steps per second: 130, episode reward: -70.400, mean reward: -0.964 [-100.000, 17.063], mean action: 1.438 [0.000, 3.000],  loss: 10.824009, mse: 9369.967840, mean_q: 79.101866, mean_eps: 0.890929
  36501/300000: episode: 364, duration: 0.789s, episode steps: 107, steps per second: 136, episode reward: -119.078, mean reward: -1.113 [-100.000,  6.318], mean action: 1.589 [0.000, 3.000],  loss: 21.978181, mse: 9318.542636, mean_q: 76.185271, mean_eps: 0.890659
  36618/300000: episode: 365, duration: 0.859s, episode steps: 117, steps per second: 136, episode reward: -410.496, mean reward: -3.509 [-100.000, 97.989], mean action: 1.641 [0.000, 3.000],  loss: 21.698546, mse: 9506.246649, mean_q: 78.357310, mean_eps: 0.890323
  36705/300000: episode: 366, duration: 0.636s, episode steps:  87, steps per second: 137, episode reward: -203.184, mean reward: -2.335 [-100.000,  4.457], mean action: 1.586 [0.000, 3.000],  loss: 20.457742, mse: 9636.279931, mean_q: 78.737038, mean_eps: 0.890017
  36789/300000: episode: 367, duration: 0.599s, episode steps:  84, steps per second: 140, episode reward: -110.959, mean reward: -1.321 [-100.000, 16.677], mean action: 1.512 [0.000, 3.000],  loss: 24.586089, mse: 9496.589995, mean_q: 76.687740, mean_eps: 0.889760
  36873/300000: episode: 368, duration: 0.591s, episode steps:  84, steps per second: 142, episode reward: -162.950, mean reward: -1.940 [-100.000,  5.927], mean action: 1.464 [0.000, 3.000],  loss: 29.745195, mse: 9579.581589, mean_q: 79.194486, mean_eps: 0.889509
  36962/300000: episode: 369, duration: 0.629s, episode steps:  89, steps per second: 141, episode reward: -101.015, mean reward: -1.135 [-100.000,  8.784], mean action: 1.629 [0.000, 3.000],  loss: 21.601945, mse: 9779.966479, mean_q: 81.477460, mean_eps: 0.889249
  37034/300000: episode: 370, duration: 0.587s, episode steps:  72, steps per second: 123, episode reward: -90.764, mean reward: -1.261 [-100.000,  5.852], mean action: 1.583 [0.000, 3.000],  loss: 23.486047, mse: 9755.798923, mean_q: 79.811859, mean_eps: 0.889007
  37149/300000: episode: 371, duration: 0.903s, episode steps: 115, steps per second: 127, episode reward: -63.542, mean reward: -0.553 [-100.000, 12.776], mean action: 1.687 [0.000, 3.000],  loss: 30.710177, mse: 9780.531764, mean_q: 80.482418, mean_eps: 0.888727
  37261/300000: episode: 372, duration: 0.910s, episode steps: 112, steps per second: 123, episode reward: -155.631, mean reward: -1.390 [-100.000, 23.839], mean action: 1.848 [0.000, 3.000],  loss: 27.250193, mse: 9726.849186, mean_q: 80.343936, mean_eps: 0.888387
  37350/300000: episode: 373, duration: 0.695s, episode steps:  89, steps per second: 128, episode reward: -384.996, mean reward: -4.326 [-100.000,  0.377], mean action: 1.697 [0.000, 3.000],  loss: 24.799321, mse: 9785.461272, mean_q: 80.508758, mean_eps: 0.888085
  37432/300000: episode: 374, duration: 0.657s, episode steps:  82, steps per second: 125, episode reward: -112.260, mean reward: -1.369 [-100.000,  9.434], mean action: 1.476 [0.000, 3.000],  loss: 19.235414, mse: 9594.111608, mean_q: 79.491389, mean_eps: 0.887829
  37496/300000: episode: 375, duration: 0.530s, episode steps:  64, steps per second: 121, episode reward: -74.793, mean reward: -1.169 [-100.000,  8.490], mean action: 1.422 [0.000, 3.000],  loss: 18.497067, mse: 9726.519135, mean_q: 79.573331, mean_eps: 0.887609
  37626/300000: episode: 376, duration: 0.922s, episode steps: 130, steps per second: 141, episode reward: -185.846, mean reward: -1.430 [-100.000,  3.112], mean action: 1.515 [0.000, 3.000],  loss: 30.962515, mse: 9555.293502, mean_q: 78.807149, mean_eps: 0.887319
  37743/300000: episode: 377, duration: 0.846s, episode steps: 117, steps per second: 138, episode reward: -155.263, mean reward: -1.327 [-100.000, 19.770], mean action: 1.436 [0.000, 3.000],  loss: 25.822041, mse: 9575.699669, mean_q: 77.243569, mean_eps: 0.886948
  37845/300000: episode: 378, duration: 0.770s, episode steps: 102, steps per second: 132, episode reward: -200.356, mean reward: -1.964 [-100.000, 60.122], mean action: 1.559 [0.000, 3.000],  loss: 19.651125, mse: 9556.210933, mean_q: 79.901260, mean_eps: 0.886619
  37925/300000: episode: 379, duration: 0.588s, episode steps:  80, steps per second: 136, episode reward: -69.091, mean reward: -0.864 [-100.000, 11.645], mean action: 1.512 [0.000, 3.000],  loss: 21.262999, mse: 9592.283563, mean_q: 79.613274, mean_eps: 0.886346
  38006/300000: episode: 380, duration: 0.597s, episode steps:  81, steps per second: 136, episode reward: -103.441, mean reward: -1.277 [-100.000, 16.672], mean action: 1.420 [0.000, 3.000],  loss: 34.425270, mse: 9581.297050, mean_q: 80.274820, mean_eps: 0.886105
  38100/300000: episode: 381, duration: 0.711s, episode steps:  94, steps per second: 132, episode reward: -124.757, mean reward: -1.327 [-100.000,  5.788], mean action: 1.638 [0.000, 3.000],  loss: 19.080993, mse: 10018.905798, mean_q: 85.548519, mean_eps: 0.885843
  38192/300000: episode: 382, duration: 0.655s, episode steps:  92, steps per second: 141, episode reward: -103.532, mean reward: -1.125 [-100.000,  8.344], mean action: 1.522 [0.000, 3.000],  loss: 22.346215, mse: 9775.799858, mean_q: 82.056996, mean_eps: 0.885564
  38308/300000: episode: 383, duration: 0.779s, episode steps: 116, steps per second: 149, episode reward: -157.829, mean reward: -1.361 [-100.000,  6.866], mean action: 1.534 [0.000, 3.000],  loss: 28.066610, mse: 9520.419358, mean_q: 80.899513, mean_eps: 0.885252
  38427/300000: episode: 384, duration: 0.818s, episode steps: 119, steps per second: 146, episode reward: -101.928, mean reward: -0.857 [-100.000,  7.858], mean action: 1.504 [0.000, 3.000],  loss: 27.707268, mse: 9905.873950, mean_q: 80.984445, mean_eps: 0.884899
  38526/300000: episode: 385, duration: 0.664s, episode steps:  99, steps per second: 149, episode reward: -66.555, mean reward: -0.672 [-100.000, 17.737], mean action: 1.394 [0.000, 3.000],  loss: 16.576808, mse: 9905.634647, mean_q: 81.576154, mean_eps: 0.884572
  38613/300000: episode: 386, duration: 0.620s, episode steps:  87, steps per second: 140, episode reward: -99.236, mean reward: -1.141 [-100.000,  6.408], mean action: 1.552 [0.000, 3.000],  loss: 24.738559, mse: 10069.302201, mean_q: 82.125606, mean_eps: 0.884293
  38718/300000: episode: 387, duration: 0.788s, episode steps: 105, steps per second: 133, episode reward: -104.102, mean reward: -0.991 [-100.000,  6.262], mean action: 1.429 [0.000, 3.000],  loss: 22.611143, mse: 9936.902107, mean_q: 81.021306, mean_eps: 0.884005
  38786/300000: episode: 388, duration: 0.476s, episode steps:  68, steps per second: 143, episode reward: -171.662, mean reward: -2.524 [-100.000, 13.718], mean action: 1.324 [0.000, 3.000],  loss: 24.578334, mse: 10116.855009, mean_q: 80.396749, mean_eps: 0.883746
  38913/300000: episode: 389, duration: 0.869s, episode steps: 127, steps per second: 146, episode reward: 21.369, mean reward:  0.168 [-100.000, 83.017], mean action: 1.370 [0.000, 3.000],  loss: 25.634697, mse: 10111.448946, mean_q: 81.839876, mean_eps: 0.883453
  39003/300000: episode: 390, duration: 0.654s, episode steps:  90, steps per second: 138, episode reward: -229.399, mean reward: -2.549 [-100.000, 36.955], mean action: 1.644 [0.000, 3.000],  loss: 32.513700, mse: 10033.240761, mean_q: 81.444260, mean_eps: 0.883128
  39072/300000: episode: 391, duration: 0.520s, episode steps:  69, steps per second: 133, episode reward: -152.597, mean reward: -2.212 [-100.000, 25.833], mean action: 1.493 [0.000, 3.000],  loss: 31.070424, mse: 9997.201568, mean_q: 79.739698, mean_eps: 0.882889
  39189/300000: episode: 392, duration: 0.777s, episode steps: 117, steps per second: 151, episode reward: -118.147, mean reward: -1.010 [-100.000, 12.431], mean action: 1.556 [0.000, 3.000],  loss: 23.917692, mse: 10141.257408, mean_q: 79.675130, mean_eps: 0.882610
  39304/300000: episode: 393, duration: 0.885s, episode steps: 115, steps per second: 130, episode reward: -152.693, mean reward: -1.328 [-100.000, 12.324], mean action: 1.617 [0.000, 3.000],  loss: 21.412382, mse: 10307.964389, mean_q: 82.343162, mean_eps: 0.882262
  39408/300000: episode: 394, duration: 0.766s, episode steps: 104, steps per second: 136, episode reward: -73.035, mean reward: -0.702 [-100.000, 14.474], mean action: 1.587 [0.000, 3.000],  loss: 37.708469, mse: 10142.985389, mean_q: 82.378557, mean_eps: 0.881934
  39474/300000: episode: 395, duration: 0.447s, episode steps:  66, steps per second: 148, episode reward: -97.581, mean reward: -1.479 [-100.000, 29.163], mean action: 1.530 [0.000, 3.000],  loss: 30.310080, mse: 10561.147565, mean_q: 83.974837, mean_eps: 0.881678
  39567/300000: episode: 396, duration: 0.631s, episode steps:  93, steps per second: 147, episode reward: -97.007, mean reward: -1.043 [-100.000, 23.223], mean action: 1.376 [0.000, 3.000],  loss: 37.605948, mse: 10494.397718, mean_q: 85.055173, mean_eps: 0.881440
  39686/300000: episode: 397, duration: 0.944s, episode steps: 119, steps per second: 126, episode reward: -124.707, mean reward: -1.048 [-100.000, 14.873], mean action: 1.647 [0.000, 3.000],  loss: 26.384492, mse: 11050.328014, mean_q: 87.103703, mean_eps: 0.881122
  39751/300000: episode: 398, duration: 0.474s, episode steps:  65, steps per second: 137, episode reward: -170.268, mean reward: -2.620 [-100.000,  5.906], mean action: 1.354 [0.000, 3.000],  loss: 29.764255, mse: 10912.822641, mean_q: 85.791510, mean_eps: 0.880846
  39882/300000: episode: 399, duration: 0.956s, episode steps: 131, steps per second: 137, episode reward: -71.748, mean reward: -0.548 [-100.000,  9.436], mean action: 1.565 [0.000, 3.000],  loss: 27.634836, mse: 10900.891710, mean_q: 86.156941, mean_eps: 0.880552
  39988/300000: episode: 400, duration: 0.933s, episode steps: 106, steps per second: 114, episode reward: -126.768, mean reward: -1.196 [-100.000, 11.015], mean action: 1.698 [0.000, 3.000],  loss: 19.677000, mse: 11019.754970, mean_q: 88.545208, mean_eps: 0.880197
  40084/300000: episode: 401, duration: 0.690s, episode steps:  96, steps per second: 139, episode reward: -186.061, mean reward: -1.938 [-100.000,  7.128], mean action: 1.823 [0.000, 3.000],  loss: 23.928048, mse: 11188.648438, mean_q: 89.912293, mean_eps: 0.879893
  40159/300000: episode: 402, duration: 0.568s, episode steps:  75, steps per second: 132, episode reward: -161.916, mean reward: -2.159 [-100.000, 10.086], mean action: 1.627 [0.000, 3.000],  loss: 31.573334, mse: 11208.979661, mean_q: 91.045366, mean_eps: 0.879637
  40260/300000: episode: 403, duration: 0.771s, episode steps: 101, steps per second: 131, episode reward: -176.923, mean reward: -1.752 [-100.000,  5.768], mean action: 1.663 [0.000, 3.000],  loss: 20.712627, mse: 11412.777508, mean_q: 91.188926, mean_eps: 0.879373
  40398/300000: episode: 404, duration: 0.944s, episode steps: 138, steps per second: 146, episode reward: -205.929, mean reward: -1.492 [-100.000,  9.881], mean action: 1.638 [0.000, 3.000],  loss: 25.863261, mse: 11471.029863, mean_q: 90.231850, mean_eps: 0.879015
  40479/300000: episode: 405, duration: 0.556s, episode steps:  81, steps per second: 146, episode reward: -96.699, mean reward: -1.194 [-100.000,  7.455], mean action: 1.519 [0.000, 3.000],  loss: 20.005702, mse: 11277.669626, mean_q: 89.834642, mean_eps: 0.878686
  40590/300000: episode: 406, duration: 0.748s, episode steps: 111, steps per second: 148, episode reward: -110.619, mean reward: -0.997 [-100.000,  7.903], mean action: 1.577 [0.000, 3.000],  loss: 18.338048, mse: 11390.613844, mean_q: 90.765177, mean_eps: 0.878398
  40672/300000: episode: 407, duration: 0.545s, episode steps:  82, steps per second: 151, episode reward: -94.619, mean reward: -1.154 [-100.000, 11.826], mean action: 1.427 [0.000, 3.000],  loss: 22.995924, mse: 11371.700934, mean_q: 90.167402, mean_eps: 0.878108
  40744/300000: episode: 408, duration: 0.479s, episode steps:  72, steps per second: 150, episode reward: -133.838, mean reward: -1.859 [-100.000,  6.369], mean action: 1.625 [0.000, 3.000],  loss: 27.712354, mse: 11684.100247, mean_q: 91.891581, mean_eps: 0.877877
  40826/300000: episode: 409, duration: 0.568s, episode steps:  82, steps per second: 144, episode reward: -92.433, mean reward: -1.127 [-100.000,  6.503], mean action: 1.415 [0.000, 3.000],  loss: 26.562049, mse: 11770.312452, mean_q: 94.365209, mean_eps: 0.877647
  40887/300000: episode: 410, duration: 0.436s, episode steps:  61, steps per second: 140, episode reward: -68.328, mean reward: -1.120 [-100.000,  7.377], mean action: 1.672 [0.000, 3.000],  loss: 22.939249, mse: 11484.604556, mean_q: 92.953752, mean_eps: 0.877432
  40993/300000: episode: 411, duration: 0.753s, episode steps: 106, steps per second: 141, episode reward: -121.690, mean reward: -1.148 [-100.000,  5.825], mean action: 1.594 [0.000, 3.000],  loss: 30.538400, mse: 11725.644402, mean_q: 94.880319, mean_eps: 0.877182
  41060/300000: episode: 412, duration: 0.451s, episode steps:  67, steps per second: 148, episode reward: -73.562, mean reward: -1.098 [-100.000,  8.729], mean action: 1.567 [0.000, 3.000],  loss: 20.402206, mse: 11869.102510, mean_q: 94.641467, mean_eps: 0.876922
  41138/300000: episode: 413, duration: 0.565s, episode steps:  78, steps per second: 138, episode reward: -111.080, mean reward: -1.424 [-100.000,  7.688], mean action: 1.551 [0.000, 3.000],  loss: 23.308705, mse: 11491.007963, mean_q: 92.428230, mean_eps: 0.876704
  41242/300000: episode: 414, duration: 0.782s, episode steps: 104, steps per second: 133, episode reward: -77.582, mean reward: -0.746 [-100.000, 48.095], mean action: 1.558 [0.000, 3.000],  loss: 23.301382, mse: 12179.088905, mean_q: 96.129354, mean_eps: 0.876432
  41372/300000: episode: 415, duration: 0.924s, episode steps: 130, steps per second: 141, episode reward: -140.899, mean reward: -1.084 [-100.000, 15.879], mean action: 1.615 [0.000, 3.000],  loss: 28.842080, mse: 12027.462425, mean_q: 94.953050, mean_eps: 0.876081
  41469/300000: episode: 416, duration: 0.722s, episode steps:  97, steps per second: 134, episode reward: -185.546, mean reward: -1.913 [-100.000,  7.485], mean action: 1.577 [0.000, 3.000],  loss: 23.344120, mse: 12272.513883, mean_q: 95.014640, mean_eps: 0.875740
  41568/300000: episode: 417, duration: 0.745s, episode steps:  99, steps per second: 133, episode reward: -103.876, mean reward: -1.049 [-100.000,  7.760], mean action: 1.414 [0.000, 3.000],  loss: 28.513299, mse: 12155.300101, mean_q: 95.211877, mean_eps: 0.875446
  41649/300000: episode: 418, duration: 0.593s, episode steps:  81, steps per second: 137, episode reward: -126.031, mean reward: -1.556 [-100.000,  4.684], mean action: 1.704 [0.000, 3.000],  loss: 31.737001, mse: 11998.598585, mean_q: 94.780714, mean_eps: 0.875176
  41717/300000: episode: 419, duration: 0.490s, episode steps:  68, steps per second: 139, episode reward: -86.031, mean reward: -1.265 [-100.000,  5.197], mean action: 1.750 [0.000, 3.000],  loss: 34.651744, mse: 12401.975184, mean_q: 95.584750, mean_eps: 0.874953
  41827/300000: episode: 420, duration: 0.749s, episode steps: 110, steps per second: 147, episode reward: -172.583, mean reward: -1.569 [-100.000, 17.344], mean action: 1.545 [0.000, 3.000],  loss: 33.119965, mse: 12411.240474, mean_q: 96.116624, mean_eps: 0.874686
  41981/300000: episode: 421, duration: 1.012s, episode steps: 154, steps per second: 152, episode reward: -195.938, mean reward: -1.272 [-100.000, 83.843], mean action: 1.455 [0.000, 3.000],  loss: 27.624213, mse: 12384.699009, mean_q: 96.182658, mean_eps: 0.874289
  42079/300000: episode: 422, duration: 0.682s, episode steps:  98, steps per second: 144, episode reward: -125.171, mean reward: -1.277 [-100.000,  8.596], mean action: 1.673 [0.000, 3.000],  loss: 27.467220, mse: 12293.610122, mean_q: 95.874840, mean_eps: 0.873911
  42174/300000: episode: 423, duration: 0.652s, episode steps:  95, steps per second: 146, episode reward: -267.701, mean reward: -2.818 [-100.000, 21.671], mean action: 1.516 [0.000, 3.000],  loss: 21.146806, mse: 12569.108285, mean_q: 97.340562, mean_eps: 0.873622
  42291/300000: episode: 424, duration: 0.785s, episode steps: 117, steps per second: 149, episode reward: -177.302, mean reward: -1.515 [-100.000, 12.676], mean action: 1.556 [0.000, 3.000],  loss: 31.090081, mse: 12816.744767, mean_q: 98.588401, mean_eps: 0.873304
  42391/300000: episode: 425, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: -181.011, mean reward: -1.810 [-100.000,  2.087], mean action: 1.670 [0.000, 3.000],  loss: 23.735503, mse: 12449.537397, mean_q: 97.408482, mean_eps: 0.872978
  42465/300000: episode: 426, duration: 0.495s, episode steps:  74, steps per second: 150, episode reward: -109.190, mean reward: -1.476 [-100.000,  6.078], mean action: 1.432 [0.000, 3.000],  loss: 27.211022, mse: 12501.743586, mean_q: 98.280080, mean_eps: 0.872718
  42542/300000: episode: 427, duration: 0.529s, episode steps:  77, steps per second: 146, episode reward: -118.657, mean reward: -1.541 [-100.000,  9.512], mean action: 1.623 [0.000, 3.000],  loss: 30.272919, mse: 12251.061536, mean_q: 95.653390, mean_eps: 0.872491
  42629/300000: episode: 428, duration: 0.587s, episode steps:  87, steps per second: 148, episode reward: -130.993, mean reward: -1.506 [-100.000,  8.832], mean action: 1.621 [0.000, 3.000],  loss: 19.394690, mse: 12359.633823, mean_q: 95.513624, mean_eps: 0.872245
  42747/300000: episode: 429, duration: 0.905s, episode steps: 118, steps per second: 130, episode reward: -91.359, mean reward: -0.774 [-100.000,  8.014], mean action: 1.483 [0.000, 3.000],  loss: 29.871407, mse: 12705.308594, mean_q: 98.862685, mean_eps: 0.871937
  42860/300000: episode: 430, duration: 0.851s, episode steps: 113, steps per second: 133, episode reward: -180.131, mean reward: -1.594 [-100.000,  3.791], mean action: 1.743 [0.000, 3.000],  loss: 30.832465, mse: 12548.797947, mean_q: 97.849927, mean_eps: 0.871591
  42976/300000: episode: 431, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -125.399, mean reward: -1.081 [-100.000,  5.353], mean action: 1.457 [0.000, 3.000],  loss: 31.014404, mse: 12276.250808, mean_q: 95.175137, mean_eps: 0.871247
  43089/300000: episode: 432, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -107.270, mean reward: -0.949 [-100.000, 14.911], mean action: 1.469 [0.000, 3.000],  loss: 23.147637, mse: 12321.524379, mean_q: 96.330810, mean_eps: 0.870904
  43209/300000: episode: 433, duration: 0.844s, episode steps: 120, steps per second: 142, episode reward: -361.781, mean reward: -3.015 [-100.000, 89.713], mean action: 1.492 [0.000, 3.000],  loss: 27.027363, mse: 12322.919051, mean_q: 96.038746, mean_eps: 0.870555
  43293/300000: episode: 434, duration: 0.608s, episode steps:  84, steps per second: 138, episode reward: -99.933, mean reward: -1.190 [-100.000, 14.700], mean action: 1.500 [0.000, 3.000],  loss: 36.847822, mse: 12650.705160, mean_q: 98.167463, mean_eps: 0.870249
  43375/300000: episode: 435, duration: 0.578s, episode steps:  82, steps per second: 142, episode reward: -103.499, mean reward: -1.262 [-100.000,  6.220], mean action: 1.537 [0.000, 3.000],  loss: 20.674925, mse: 12662.386963, mean_q: 98.029186, mean_eps: 0.870000
  43475/300000: episode: 436, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: -109.884, mean reward: -1.099 [-100.000, 10.126], mean action: 1.780 [0.000, 3.000],  loss: 21.235314, mse: 12857.818223, mean_q: 98.870597, mean_eps: 0.869727
  43606/300000: episode: 437, duration: 0.919s, episode steps: 131, steps per second: 143, episode reward: -114.724, mean reward: -0.876 [-100.000,  9.237], mean action: 1.534 [0.000, 3.000],  loss: 34.474364, mse: 12580.138076, mean_q: 97.624581, mean_eps: 0.869380
  43680/300000: episode: 438, duration: 0.491s, episode steps:  74, steps per second: 151, episode reward: -34.059, mean reward: -0.460 [-100.000, 110.763], mean action: 1.378 [0.000, 3.000],  loss: 33.521255, mse: 12934.513448, mean_q: 97.377836, mean_eps: 0.869072
  43773/300000: episode: 439, duration: 0.620s, episode steps:  93, steps per second: 150, episode reward: -88.119, mean reward: -0.948 [-100.000,  9.424], mean action: 1.742 [0.000, 3.000],  loss: 48.801080, mse: 13247.893156, mean_q: 100.802449, mean_eps: 0.868822
  43843/300000: episode: 440, duration: 0.503s, episode steps:  70, steps per second: 139, episode reward: -72.372, mean reward: -1.034 [-100.000,  7.921], mean action: 1.543 [0.000, 3.000],  loss: 45.373871, mse: 13745.525823, mean_q: 103.566548, mean_eps: 0.868577
  43984/300000: episode: 441, duration: 1.045s, episode steps: 141, steps per second: 135, episode reward: -288.203, mean reward: -2.044 [-100.000, 73.586], mean action: 1.645 [0.000, 3.000],  loss: 22.447299, mse: 13383.297665, mean_q: 101.089462, mean_eps: 0.868261
  44076/300000: episode: 442, duration: 0.620s, episode steps:  92, steps per second: 148, episode reward: -120.118, mean reward: -1.306 [-100.000,  7.914], mean action: 1.511 [0.000, 3.000],  loss: 25.384784, mse: 13419.911918, mean_q: 102.437128, mean_eps: 0.867911
  44195/300000: episode: 443, duration: 0.876s, episode steps: 119, steps per second: 136, episode reward: -86.263, mean reward: -0.725 [-100.000, 12.105], mean action: 1.555 [0.000, 3.000],  loss: 26.846824, mse: 13863.407875, mean_q: 103.759190, mean_eps: 0.867595
  44304/300000: episode: 444, duration: 0.813s, episode steps: 109, steps per second: 134, episode reward: -114.638, mean reward: -1.052 [-100.000,  6.730], mean action: 1.422 [0.000, 3.000],  loss: 34.058154, mse: 13837.653679, mean_q: 104.049976, mean_eps: 0.867253
  44382/300000: episode: 445, duration: 0.566s, episode steps:  78, steps per second: 138, episode reward: 20.261, mean reward:  0.260 [-100.000, 78.450], mean action: 1.731 [0.000, 3.000],  loss: 26.849903, mse: 14153.415690, mean_q: 104.268665, mean_eps: 0.866972
  44459/300000: episode: 446, duration: 0.564s, episode steps:  77, steps per second: 136, episode reward: -118.612, mean reward: -1.540 [-100.000,  6.017], mean action: 1.649 [0.000, 3.000],  loss: 35.635355, mse: 14177.852070, mean_q: 105.527587, mean_eps: 0.866740
  44588/300000: episode: 447, duration: 1.022s, episode steps: 129, steps per second: 126, episode reward: -127.487, mean reward: -0.988 [-100.000, 15.543], mean action: 1.442 [0.000, 3.000],  loss: 27.784109, mse: 14110.242445, mean_q: 104.613726, mean_eps: 0.866431
  44707/300000: episode: 448, duration: 0.947s, episode steps: 119, steps per second: 126, episode reward: -216.329, mean reward: -1.818 [-100.000, 37.026], mean action: 1.420 [0.000, 3.000],  loss: 28.550658, mse: 14426.558471, mean_q: 106.917231, mean_eps: 0.866059
  44784/300000: episode: 449, duration: 0.648s, episode steps:  77, steps per second: 119, episode reward: -85.393, mean reward: -1.109 [-100.000,  8.579], mean action: 1.714 [0.000, 3.000],  loss: 31.108785, mse: 14573.364879, mean_q: 106.849983, mean_eps: 0.865765
  44867/300000: episode: 450, duration: 0.652s, episode steps:  83, steps per second: 127, episode reward: -135.816, mean reward: -1.636 [-100.000, 20.907], mean action: 1.663 [0.000, 3.000],  loss: 37.004142, mse: 14486.155250, mean_q: 106.552860, mean_eps: 0.865525
  44980/300000: episode: 451, duration: 0.882s, episode steps: 113, steps per second: 128, episode reward: -154.258, mean reward: -1.365 [-100.000,  6.976], mean action: 1.566 [0.000, 3.000],  loss: 37.377064, mse: 14573.754753, mean_q: 109.132944, mean_eps: 0.865231
  45101/300000: episode: 452, duration: 1.054s, episode steps: 121, steps per second: 115, episode reward: -74.034, mean reward: -0.612 [-100.000, 18.700], mean action: 1.711 [0.000, 3.000],  loss: 27.613708, mse: 14533.295011, mean_q: 107.435879, mean_eps: 0.864880
  45257/300000: episode: 453, duration: 1.166s, episode steps: 156, steps per second: 134, episode reward: -227.311, mean reward: -1.457 [-100.000, 70.856], mean action: 1.526 [0.000, 3.000],  loss: 33.688374, mse: 14455.441763, mean_q: 106.642758, mean_eps: 0.864464
  45314/300000: episode: 454, duration: 0.421s, episode steps:  57, steps per second: 135, episode reward: -144.909, mean reward: -2.542 [-100.000, 18.847], mean action: 1.579 [0.000, 3.000],  loss: 18.457740, mse: 14483.549102, mean_q: 106.194691, mean_eps: 0.864145
  45397/300000: episode: 455, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -60.539, mean reward: -0.729 [-100.000, 16.003], mean action: 1.747 [0.000, 3.000],  loss: 27.999891, mse: 15082.984822, mean_q: 109.594573, mean_eps: 0.863935
  45479/300000: episode: 456, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -142.514, mean reward: -1.738 [-100.000,  8.416], mean action: 1.512 [0.000, 3.000],  loss: 41.580535, mse: 14488.461878, mean_q: 106.268387, mean_eps: 0.863688
  45553/300000: episode: 457, duration: 0.509s, episode steps:  74, steps per second: 145, episode reward: -30.406, mean reward: -0.411 [-100.000,  8.255], mean action: 1.608 [0.000, 3.000],  loss: 25.248208, mse: 14201.007760, mean_q: 104.819556, mean_eps: 0.863453
  45620/300000: episode: 458, duration: 0.533s, episode steps:  67, steps per second: 126, episode reward: -80.708, mean reward: -1.205 [-100.000, 11.254], mean action: 1.493 [0.000, 3.000],  loss: 24.299841, mse: 14506.725542, mean_q: 105.798059, mean_eps: 0.863242
  45708/300000: episode: 459, duration: 0.653s, episode steps:  88, steps per second: 135, episode reward: -329.835, mean reward: -3.748 [-100.000,  0.218], mean action: 1.693 [0.000, 3.000],  loss: 32.029475, mse: 14556.906694, mean_q: 106.449873, mean_eps: 0.863009
  45785/300000: episode: 460, duration: 0.525s, episode steps:  77, steps per second: 147, episode reward: -86.572, mean reward: -1.124 [-100.000, 22.711], mean action: 1.675 [0.000, 3.000],  loss: 41.505725, mse: 14650.676415, mean_q: 106.846567, mean_eps: 0.862762
  45855/300000: episode: 461, duration: 0.479s, episode steps:  70, steps per second: 146, episode reward: -154.525, mean reward: -2.207 [-100.000,  8.818], mean action: 1.714 [0.000, 3.000],  loss: 39.673991, mse: 14669.635951, mean_q: 106.855261, mean_eps: 0.862542
  45982/300000: episode: 462, duration: 0.920s, episode steps: 127, steps per second: 138, episode reward: -241.028, mean reward: -1.898 [-100.000,  5.290], mean action: 1.646 [0.000, 3.000],  loss: 35.559626, mse: 14766.250354, mean_q: 106.652621, mean_eps: 0.862246
  46053/300000: episode: 463, duration: 0.513s, episode steps:  71, steps per second: 138, episode reward: -81.771, mean reward: -1.152 [-100.000,  6.910], mean action: 1.563 [0.000, 3.000],  loss: 38.862338, mse: 14512.098935, mean_q: 107.431812, mean_eps: 0.861949
  46152/300000: episode: 464, duration: 0.842s, episode steps:  99, steps per second: 118, episode reward: -19.278, mean reward: -0.195 [-100.000, 10.337], mean action: 1.677 [0.000, 3.000],  loss: 34.456408, mse: 14616.394147, mean_q: 106.075739, mean_eps: 0.861694
  46248/300000: episode: 465, duration: 0.813s, episode steps:  96, steps per second: 118, episode reward: -114.166, mean reward: -1.189 [-100.000, 17.049], mean action: 1.604 [0.000, 3.000],  loss: 32.177474, mse: 14521.946411, mean_q: 107.263651, mean_eps: 0.861402
  46380/300000: episode: 466, duration: 1.002s, episode steps: 132, steps per second: 132, episode reward: -195.584, mean reward: -1.482 [-100.000, 15.325], mean action: 1.538 [0.000, 3.000],  loss: 34.029535, mse: 14343.752412, mean_q: 106.156344, mean_eps: 0.861059
  46455/300000: episode: 467, duration: 0.627s, episode steps:  75, steps per second: 120, episode reward: -117.661, mean reward: -1.569 [-100.000,  7.563], mean action: 1.507 [0.000, 3.000],  loss: 34.490848, mse: 14888.940091, mean_q: 108.889265, mean_eps: 0.860749
  46588/300000: episode: 468, duration: 0.964s, episode steps: 133, steps per second: 138, episode reward: -117.238, mean reward: -0.881 [-100.000,  5.972], mean action: 1.579 [0.000, 3.000],  loss: 35.356483, mse: 14883.102664, mean_q: 108.129113, mean_eps: 0.860437
  46688/300000: episode: 469, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: -94.040, mean reward: -0.940 [-100.000, 26.363], mean action: 1.630 [0.000, 3.000],  loss: 26.162160, mse: 14803.470215, mean_q: 107.402084, mean_eps: 0.860088
  46772/300000: episode: 470, duration: 0.606s, episode steps:  84, steps per second: 139, episode reward: -88.680, mean reward: -1.056 [-100.000, 14.963], mean action: 1.381 [0.000, 3.000],  loss: 44.679290, mse: 14868.804036, mean_q: 106.814778, mean_eps: 0.859811
  46860/300000: episode: 471, duration: 0.597s, episode steps:  88, steps per second: 147, episode reward: -111.844, mean reward: -1.271 [-100.000, 11.574], mean action: 1.636 [0.000, 3.000],  loss: 39.981382, mse: 14738.469383, mean_q: 104.584246, mean_eps: 0.859554
  46964/300000: episode: 472, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -125.504, mean reward: -1.207 [-100.000, 15.211], mean action: 1.577 [0.000, 3.000],  loss: 33.110263, mse: 15114.737521, mean_q: 107.307085, mean_eps: 0.859266
  47032/300000: episode: 473, duration: 0.465s, episode steps:  68, steps per second: 146, episode reward: -115.765, mean reward: -1.702 [-100.000,  7.935], mean action: 1.485 [0.000, 3.000],  loss: 36.982549, mse: 14890.471881, mean_q: 104.725531, mean_eps: 0.859008
  47121/300000: episode: 474, duration: 0.639s, episode steps:  89, steps per second: 139, episode reward: -190.523, mean reward: -2.141 [-100.000, 34.630], mean action: 1.787 [0.000, 3.000],  loss: 32.454919, mse: 14834.968706, mean_q: 105.658355, mean_eps: 0.858772
  47235/300000: episode: 475, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -129.962, mean reward: -1.140 [-100.000,  9.995], mean action: 1.544 [0.000, 3.000],  loss: 24.876101, mse: 14711.507556, mean_q: 106.752880, mean_eps: 0.858468
  47311/300000: episode: 476, duration: 0.556s, episode steps:  76, steps per second: 137, episode reward: -61.458, mean reward: -0.809 [-100.000,  9.239], mean action: 1.487 [0.000, 3.000],  loss: 42.952223, mse: 14834.271240, mean_q: 105.335840, mean_eps: 0.858182
  47488/300000: episode: 477, duration: 1.300s, episode steps: 177, steps per second: 136, episode reward: -227.629, mean reward: -1.286 [-100.000, 81.605], mean action: 1.588 [0.000, 3.000],  loss: 34.144200, mse: 14874.169938, mean_q: 105.622177, mean_eps: 0.857803
  47563/300000: episode: 478, duration: 0.562s, episode steps:  75, steps per second: 133, episode reward: -83.556, mean reward: -1.114 [-100.000, 17.485], mean action: 1.693 [0.000, 3.000],  loss: 29.631973, mse: 14490.576367, mean_q: 102.699419, mean_eps: 0.857425
  47633/300000: episode: 479, duration: 0.525s, episode steps:  70, steps per second: 133, episode reward: -80.681, mean reward: -1.153 [-100.000,  8.388], mean action: 1.443 [0.000, 3.000],  loss: 28.716804, mse: 14692.947517, mean_q: 104.903850, mean_eps: 0.857207
  47730/300000: episode: 480, duration: 0.732s, episode steps:  97, steps per second: 132, episode reward: -120.884, mean reward: -1.246 [-100.000,  8.072], mean action: 1.381 [0.000, 3.000],  loss: 20.620786, mse: 14842.778018, mean_q: 104.385258, mean_eps: 0.856957
  47807/300000: episode: 481, duration: 0.585s, episode steps:  77, steps per second: 132, episode reward: -155.125, mean reward: -2.015 [-100.000, 14.510], mean action: 1.390 [0.000, 3.000],  loss: 47.820120, mse: 14232.375216, mean_q: 101.058643, mean_eps: 0.856696
  47906/300000: episode: 482, duration: 0.710s, episode steps:  99, steps per second: 139, episode reward: -72.620, mean reward: -0.734 [-100.000, 10.903], mean action: 1.444 [0.000, 3.000],  loss: 31.098866, mse: 14295.106672, mean_q: 102.107116, mean_eps: 0.856432
  47970/300000: episode: 483, duration: 0.476s, episode steps:  64, steps per second: 134, episode reward: -90.559, mean reward: -1.415 [-100.000,  7.186], mean action: 1.781 [0.000, 3.000],  loss: 32.585861, mse: 14389.970291, mean_q: 101.555383, mean_eps: 0.856187
  48101/300000: episode: 484, duration: 0.990s, episode steps: 131, steps per second: 132, episode reward: -100.615, mean reward: -0.768 [-100.000,  6.926], mean action: 1.611 [0.000, 3.000],  loss: 35.765068, mse: 14546.802093, mean_q: 101.716471, mean_eps: 0.855895
  48173/300000: episode: 485, duration: 0.543s, episode steps:  72, steps per second: 133, episode reward: -134.520, mean reward: -1.868 [-100.000,  5.511], mean action: 1.542 [0.000, 3.000],  loss: 24.143302, mse: 14632.790894, mean_q: 102.291478, mean_eps: 0.855590
  48331/300000: episode: 486, duration: 1.245s, episode steps: 158, steps per second: 127, episode reward: -243.506, mean reward: -1.541 [-100.000, 80.765], mean action: 1.544 [0.000, 3.000],  loss: 46.405944, mse: 14447.929205, mean_q: 100.510298, mean_eps: 0.855246
  48406/300000: episode: 487, duration: 0.557s, episode steps:  75, steps per second: 135, episode reward: -105.462, mean reward: -1.406 [-100.000, 11.915], mean action: 1.547 [0.000, 3.000],  loss: 32.004719, mse: 13842.137214, mean_q: 98.420492, mean_eps: 0.854896
  48488/300000: episode: 488, duration: 0.647s, episode steps:  82, steps per second: 127, episode reward: -65.630, mean reward: -0.800 [-100.000, 17.970], mean action: 1.561 [0.000, 3.000],  loss: 51.801703, mse: 14213.824731, mean_q: 100.403358, mean_eps: 0.854661
  48578/300000: episode: 489, duration: 0.689s, episode steps:  90, steps per second: 131, episode reward: -56.705, mean reward: -0.630 [-100.000, 69.818], mean action: 1.467 [0.000, 3.000],  loss: 43.198364, mse: 14013.918826, mean_q: 100.497807, mean_eps: 0.854403
  48718/300000: episode: 490, duration: 1.035s, episode steps: 140, steps per second: 135, episode reward: -79.898, mean reward: -0.571 [-100.000, 15.472], mean action: 1.586 [0.000, 3.000],  loss: 32.052775, mse: 14215.222468, mean_q: 101.360264, mean_eps: 0.854058
  48820/300000: episode: 491, duration: 0.801s, episode steps: 102, steps per second: 127, episode reward: -175.834, mean reward: -1.724 [-100.000, 10.962], mean action: 1.578 [0.000, 3.000],  loss: 49.908459, mse: 13720.403971, mean_q: 98.292846, mean_eps: 0.853694
  48923/300000: episode: 492, duration: 0.762s, episode steps: 103, steps per second: 135, episode reward: -80.195, mean reward: -0.779 [-100.000,  8.086], mean action: 1.495 [0.000, 3.000],  loss: 36.771978, mse: 14106.654752, mean_q: 98.451684, mean_eps: 0.853387
  49039/300000: episode: 493, duration: 0.816s, episode steps: 116, steps per second: 142, episode reward: -165.832, mean reward: -1.430 [-100.000,  9.518], mean action: 1.466 [0.000, 3.000],  loss: 40.843842, mse: 14326.132392, mean_q: 99.373457, mean_eps: 0.853059
  49148/300000: episode: 494, duration: 0.802s, episode steps: 109, steps per second: 136, episode reward: -78.010, mean reward: -0.716 [-100.000,  7.195], mean action: 1.514 [0.000, 3.000],  loss: 41.863329, mse: 14057.901107, mean_q: 98.257205, mean_eps: 0.852721
  49230/300000: episode: 495, duration: 0.615s, episode steps:  82, steps per second: 133, episode reward: -197.154, mean reward: -2.404 [-100.000,  8.094], mean action: 1.683 [0.000, 3.000],  loss: 37.269926, mse: 14570.462498, mean_q: 100.838635, mean_eps: 0.852434
  49335/300000: episode: 496, duration: 0.793s, episode steps: 105, steps per second: 132, episode reward: -119.816, mean reward: -1.141 [-100.000,  5.157], mean action: 1.781 [0.000, 3.000],  loss: 29.002599, mse: 14101.990895, mean_q: 98.069504, mean_eps: 0.852154
  49442/300000: episode: 497, duration: 0.874s, episode steps: 107, steps per second: 122, episode reward: -209.026, mean reward: -1.954 [-100.000, 17.684], mean action: 1.720 [0.000, 3.000],  loss: 26.041639, mse: 13999.525482, mean_q: 98.190950, mean_eps: 0.851836
  49518/300000: episode: 498, duration: 0.558s, episode steps:  76, steps per second: 136, episode reward: -91.430, mean reward: -1.203 [-100.000,  8.362], mean action: 1.500 [0.000, 3.000],  loss: 29.714790, mse: 14273.205836, mean_q: 100.930343, mean_eps: 0.851562
  49625/300000: episode: 499, duration: 0.765s, episode steps: 107, steps per second: 140, episode reward: -98.768, mean reward: -0.923 [-100.000, 11.491], mean action: 1.636 [0.000, 3.000],  loss: 26.676025, mse: 13879.609119, mean_q: 98.527790, mean_eps: 0.851287
  49706/300000: episode: 500, duration: 0.624s, episode steps:  81, steps per second: 130, episode reward: -108.347, mean reward: -1.338 [-100.000, 10.213], mean action: 1.840 [0.000, 3.000],  loss: 36.942327, mse: 14059.056737, mean_q: 98.240908, mean_eps: 0.851005
  49783/300000: episode: 501, duration: 0.580s, episode steps:  77, steps per second: 133, episode reward: -44.204, mean reward: -0.574 [-100.000,  7.022], mean action: 1.727 [0.000, 3.000],  loss: 23.328049, mse: 14762.778219, mean_q: 102.506722, mean_eps: 0.850768
  49887/300000: episode: 502, duration: 0.744s, episode steps: 104, steps per second: 140, episode reward: -119.790, mean reward: -1.152 [-100.000, 20.925], mean action: 1.702 [0.000, 3.000],  loss: 32.262993, mse: 14608.576961, mean_q: 102.113082, mean_eps: 0.850497
  49957/300000: episode: 503, duration: 0.552s, episode steps:  70, steps per second: 127, episode reward: -81.091, mean reward: -1.158 [-100.000, 11.553], mean action: 1.600 [0.000, 3.000],  loss: 36.184090, mse: 14749.946164, mean_q: 102.015768, mean_eps: 0.850236
  50041/300000: episode: 504, duration: 0.630s, episode steps:  84, steps per second: 133, episode reward: -151.536, mean reward: -1.804 [-100.000, 21.966], mean action: 1.512 [0.000, 3.000],  loss: 21.959536, mse: 14345.597005, mean_q: 99.808266, mean_eps: 0.850005
  50141/300000: episode: 505, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: -135.566, mean reward: -1.356 [-100.000,  6.467], mean action: 1.500 [0.000, 3.000],  loss: 38.564879, mse: 14250.864697, mean_q: 100.412510, mean_eps: 0.849728
  50280/300000: episode: 506, duration: 1.102s, episode steps: 139, steps per second: 126, episode reward: -163.371, mean reward: -1.175 [-100.000, 86.529], mean action: 1.576 [0.000, 3.000],  loss: 34.597095, mse: 14487.388271, mean_q: 100.701447, mean_eps: 0.849370
  50379/300000: episode: 507, duration: 0.724s, episode steps:  99, steps per second: 137, episode reward: -38.560, mean reward: -0.389 [-100.000, 12.711], mean action: 1.626 [0.000, 3.000],  loss: 33.673022, mse: 14755.931946, mean_q: 102.699182, mean_eps: 0.849013
  50446/300000: episode: 508, duration: 0.470s, episode steps:  67, steps per second: 142, episode reward: -86.592, mean reward: -1.292 [-100.000,  8.392], mean action: 1.448 [0.000, 3.000],  loss: 35.739268, mse: 14565.005247, mean_q: 101.653430, mean_eps: 0.848764
  50539/300000: episode: 509, duration: 0.668s, episode steps:  93, steps per second: 139, episode reward: -163.812, mean reward: -1.761 [-100.000, 10.393], mean action: 1.484 [0.000, 3.000],  loss: 30.909705, mse: 14660.516350, mean_q: 101.789669, mean_eps: 0.848524
  50621/300000: episode: 510, duration: 0.583s, episode steps:  82, steps per second: 141, episode reward: -115.462, mean reward: -1.408 [-100.000,  9.016], mean action: 1.610 [0.000, 3.000],  loss: 26.260879, mse: 14410.585306, mean_q: 101.367908, mean_eps: 0.848261
  50739/300000: episode: 511, duration: 0.833s, episode steps: 118, steps per second: 142, episode reward: -127.200, mean reward: -1.078 [-100.000,  5.597], mean action: 1.576 [0.000, 3.000],  loss: 38.048582, mse: 14729.137464, mean_q: 103.690976, mean_eps: 0.847962
  50835/300000: episode: 512, duration: 0.743s, episode steps:  96, steps per second: 129, episode reward: -66.480, mean reward: -0.693 [-100.000,  8.233], mean action: 1.729 [0.000, 3.000],  loss: 46.473586, mse: 14666.556539, mean_q: 102.642731, mean_eps: 0.847641
  50904/300000: episode: 513, duration: 0.548s, episode steps:  69, steps per second: 126, episode reward: -75.609, mean reward: -1.096 [-100.000, 11.874], mean action: 1.638 [0.000, 3.000],  loss: 35.695856, mse: 14874.855398, mean_q: 103.997087, mean_eps: 0.847393
  51008/300000: episode: 514, duration: 0.780s, episode steps: 104, steps per second: 133, episode reward: -105.003, mean reward: -1.010 [-100.000,  9.833], mean action: 1.625 [0.000, 3.000],  loss: 28.762475, mse: 14358.412072, mean_q: 101.293048, mean_eps: 0.847133
  51100/300000: episode: 515, duration: 0.676s, episode steps:  92, steps per second: 136, episode reward: -292.337, mean reward: -3.178 [-100.000, 125.921], mean action: 1.587 [0.000, 3.000],  loss: 20.886196, mse: 14226.784042, mean_q: 99.741749, mean_eps: 0.846839
  51169/300000: episode: 516, duration: 0.529s, episode steps:  69, steps per second: 130, episode reward: -115.929, mean reward: -1.680 [-100.000, 16.270], mean action: 1.420 [0.000, 3.000],  loss: 41.579440, mse: 14352.196700, mean_q: 102.724393, mean_eps: 0.846598
  51298/300000: episode: 517, duration: 0.994s, episode steps: 129, steps per second: 130, episode reward: -62.329, mean reward: -0.483 [-100.000, 13.350], mean action: 1.581 [0.000, 3.000],  loss: 53.532116, mse: 14869.865969, mean_q: 101.476103, mean_eps: 0.846301
  51430/300000: episode: 518, duration: 0.950s, episode steps: 132, steps per second: 139, episode reward: -304.780, mean reward: -2.309 [-100.000, 129.112], mean action: 1.644 [0.000, 3.000],  loss: 33.968695, mse: 14590.480136, mean_q: 102.642111, mean_eps: 0.845909
  51513/300000: episode: 519, duration: 0.584s, episode steps:  83, steps per second: 142, episode reward: -49.072, mean reward: -0.591 [-100.000, 52.693], mean action: 1.771 [0.000, 3.000],  loss: 48.429396, mse: 14564.791945, mean_q: 100.586561, mean_eps: 0.845587
  51599/300000: episode: 520, duration: 0.586s, episode steps:  86, steps per second: 147, episode reward: -118.071, mean reward: -1.373 [-100.000,  6.985], mean action: 1.407 [0.000, 3.000],  loss: 44.747838, mse: 14760.734307, mean_q: 103.007091, mean_eps: 0.845333
  51692/300000: episode: 521, duration: 0.644s, episode steps:  93, steps per second: 145, episode reward: -99.342, mean reward: -1.068 [-100.000, 11.365], mean action: 1.419 [0.000, 3.000],  loss: 30.601401, mse: 14674.585045, mean_q: 100.505708, mean_eps: 0.845065
  51777/300000: episode: 522, duration: 0.606s, episode steps:  85, steps per second: 140, episode reward: -93.233, mean reward: -1.097 [-100.000,  6.545], mean action: 1.659 [0.000, 3.000],  loss: 47.197300, mse: 14577.615326, mean_q: 100.986099, mean_eps: 0.844798
  51885/300000: episode: 523, duration: 0.783s, episode steps: 108, steps per second: 138, episode reward: -52.295, mean reward: -0.484 [-100.000, 12.763], mean action: 1.741 [0.000, 3.000],  loss: 40.716966, mse: 14496.774794, mean_q: 99.114015, mean_eps: 0.844508
  51953/300000: episode: 524, duration: 0.518s, episode steps:  68, steps per second: 131, episode reward: -68.667, mean reward: -1.010 [-100.000,  8.632], mean action: 1.441 [0.000, 3.000],  loss: 43.692904, mse: 14736.721694, mean_q: 100.820178, mean_eps: 0.844244
  52047/300000: episode: 525, duration: 0.695s, episode steps:  94, steps per second: 135, episode reward: -109.164, mean reward: -1.161 [-100.000, 19.019], mean action: 1.734 [0.000, 3.000],  loss: 48.857154, mse: 14534.045265, mean_q: 101.176313, mean_eps: 0.844002
  52172/300000: episode: 526, duration: 0.961s, episode steps: 125, steps per second: 130, episode reward: -73.985, mean reward: -0.592 [-100.000, 12.281], mean action: 1.544 [0.000, 3.000],  loss: 41.965064, mse: 14414.572516, mean_q: 101.063880, mean_eps: 0.843673
  52232/300000: episode: 527, duration: 0.435s, episode steps:  60, steps per second: 138, episode reward: -91.048, mean reward: -1.517 [-100.000, 47.886], mean action: 1.500 [0.000, 3.000],  loss: 40.141966, mse: 14577.962793, mean_q: 100.923429, mean_eps: 0.843396
  52300/300000: episode: 528, duration: 0.516s, episode steps:  68, steps per second: 132, episode reward: -95.180, mean reward: -1.400 [-100.000, 12.192], mean action: 1.456 [0.000, 3.000],  loss: 27.479526, mse: 14688.284424, mean_q: 101.939397, mean_eps: 0.843203
  52419/300000: episode: 529, duration: 0.865s, episode steps: 119, steps per second: 138, episode reward: -52.522, mean reward: -0.441 [-100.000,  8.659], mean action: 1.655 [0.000, 3.000],  loss: 36.447755, mse: 14513.555196, mean_q: 99.363412, mean_eps: 0.842923
  52529/300000: episode: 530, duration: 0.814s, episode steps: 110, steps per second: 135, episode reward: -68.446, mean reward: -0.622 [-100.000,  9.808], mean action: 1.673 [0.000, 3.000],  loss: 33.110764, mse: 14411.267560, mean_q: 98.356922, mean_eps: 0.842580
  52669/300000: episode: 531, duration: 1.024s, episode steps: 140, steps per second: 137, episode reward: -73.767, mean reward: -0.527 [-100.000, 18.407], mean action: 1.564 [0.000, 3.000],  loss: 40.368664, mse: 14455.692076, mean_q: 100.497168, mean_eps: 0.842205
  52757/300000: episode: 532, duration: 0.632s, episode steps:  88, steps per second: 139, episode reward: -100.338, mean reward: -1.140 [-100.000,  9.284], mean action: 1.580 [0.000, 3.000],  loss: 30.250553, mse: 14345.133356, mean_q: 98.856634, mean_eps: 0.841862
  52838/300000: episode: 533, duration: 0.617s, episode steps:  81, steps per second: 131, episode reward: -74.609, mean reward: -0.921 [-100.000,  6.166], mean action: 1.358 [0.000, 3.000],  loss: 51.795984, mse: 14026.312548, mean_q: 95.422272, mean_eps: 0.841609
  52917/300000: episode: 534, duration: 0.613s, episode steps:  79, steps per second: 129, episode reward: -37.527, mean reward: -0.475 [-100.000, 11.351], mean action: 1.633 [0.000, 3.000],  loss: 29.345600, mse: 14399.408117, mean_q: 98.197925, mean_eps: 0.841369
  52999/300000: episode: 535, duration: 0.597s, episode steps:  82, steps per second: 137, episode reward: -193.698, mean reward: -2.362 [-100.000,  6.096], mean action: 1.463 [0.000, 3.000],  loss: 31.991259, mse: 14636.085378, mean_q: 100.737249, mean_eps: 0.841127
  53124/300000: episode: 536, duration: 0.883s, episode steps: 125, steps per second: 142, episode reward: -144.281, mean reward: -1.154 [-100.000,  8.416], mean action: 1.440 [0.000, 3.000],  loss: 49.735426, mse: 14564.218484, mean_q: 99.971053, mean_eps: 0.840817
  53238/300000: episode: 537, duration: 0.894s, episode steps: 114, steps per second: 127, episode reward: -154.056, mean reward: -1.351 [-100.000,  6.289], mean action: 1.588 [0.000, 3.000],  loss: 43.409637, mse: 14438.618267, mean_q: 99.126453, mean_eps: 0.840458
  53367/300000: episode: 538, duration: 0.918s, episode steps: 129, steps per second: 140, episode reward: -132.267, mean reward: -1.025 [-100.000, 10.207], mean action: 1.581 [0.000, 3.000],  loss: 31.965960, mse: 14630.982869, mean_q: 101.900674, mean_eps: 0.840094
  53465/300000: episode: 539, duration: 0.831s, episode steps:  98, steps per second: 118, episode reward: -95.929, mean reward: -0.979 [-100.000, 25.674], mean action: 1.571 [0.000, 3.000],  loss: 38.214668, mse: 14761.804249, mean_q: 102.194342, mean_eps: 0.839754
  53560/300000: episode: 540, duration: 0.813s, episode steps:  95, steps per second: 117, episode reward: -111.342, mean reward: -1.172 [-100.000,  7.218], mean action: 1.474 [0.000, 3.000],  loss: 50.111970, mse: 14437.560598, mean_q: 101.183218, mean_eps: 0.839464
  53689/300000: episode: 541, duration: 1.019s, episode steps: 129, steps per second: 127, episode reward: -152.836, mean reward: -1.185 [-100.000,  6.719], mean action: 1.504 [0.000, 3.000],  loss: 36.003752, mse: 14582.699620, mean_q: 102.216589, mean_eps: 0.839128
  53766/300000: episode: 542, duration: 0.635s, episode steps:  77, steps per second: 121, episode reward: -101.783, mean reward: -1.322 [-100.000, 30.168], mean action: 1.857 [0.000, 3.000],  loss: 58.337397, mse: 14404.027902, mean_q: 99.120842, mean_eps: 0.838819
  53833/300000: episode: 543, duration: 0.527s, episode steps:  67, steps per second: 127, episode reward: -67.466, mean reward: -1.007 [-100.000,  6.774], mean action: 1.284 [0.000, 3.000],  loss: 51.064991, mse: 14286.767345, mean_q: 98.874148, mean_eps: 0.838603
  53937/300000: episode: 544, duration: 0.842s, episode steps: 104, steps per second: 123, episode reward: -110.916, mean reward: -1.067 [-100.000, 12.600], mean action: 1.346 [0.000, 3.000],  loss: 32.979999, mse: 14629.720177, mean_q: 101.230166, mean_eps: 0.838346
  54029/300000: episode: 545, duration: 0.775s, episode steps:  92, steps per second: 119, episode reward: -63.216, mean reward: -0.687 [-100.000, 13.460], mean action: 1.587 [0.000, 3.000],  loss: 27.784754, mse: 14720.290867, mean_q: 102.147244, mean_eps: 0.838052
  54099/300000: episode: 546, duration: 0.542s, episode steps:  70, steps per second: 129, episode reward: -123.666, mean reward: -1.767 [-100.000, 15.305], mean action: 1.500 [0.000, 3.000],  loss: 51.014863, mse: 14549.372810, mean_q: 101.783431, mean_eps: 0.837810
  54198/300000: episode: 547, duration: 0.752s, episode steps:  99, steps per second: 132, episode reward: -125.879, mean reward: -1.272 [-100.000,  8.658], mean action: 1.535 [0.000, 3.000],  loss: 50.041899, mse: 14578.935350, mean_q: 100.903402, mean_eps: 0.837556
  54270/300000: episode: 548, duration: 0.578s, episode steps:  72, steps per second: 125, episode reward: -77.814, mean reward: -1.081 [-100.000, 15.855], mean action: 1.375 [0.000, 3.000],  loss: 34.284783, mse: 14394.844021, mean_q: 98.158444, mean_eps: 0.837299
  54386/300000: episode: 549, duration: 0.860s, episode steps: 116, steps per second: 135, episode reward: -101.765, mean reward: -0.877 [-100.000, 18.951], mean action: 1.491 [0.000, 3.000],  loss: 38.548312, mse: 14424.862768, mean_q: 96.918242, mean_eps: 0.837018
  54479/300000: episode: 550, duration: 0.670s, episode steps:  93, steps per second: 139, episode reward: -75.163, mean reward: -0.808 [-100.000,  6.757], mean action: 1.559 [0.000, 3.000],  loss: 26.967845, mse: 14613.516875, mean_q: 99.083173, mean_eps: 0.836704
  54588/300000: episode: 551, duration: 0.845s, episode steps: 109, steps per second: 129, episode reward: -147.557, mean reward: -1.354 [-100.000, 12.010], mean action: 1.688 [0.000, 3.000],  loss: 32.072774, mse: 14932.135482, mean_q: 102.182350, mean_eps: 0.836401
  54670/300000: episode: 552, duration: 0.590s, episode steps:  82, steps per second: 139, episode reward: -85.648, mean reward: -1.044 [-100.000, 16.863], mean action: 1.488 [0.000, 3.000],  loss: 36.059542, mse: 15151.869200, mean_q: 102.370484, mean_eps: 0.836114
  54731/300000: episode: 553, duration: 0.441s, episode steps:  61, steps per second: 138, episode reward: -157.984, mean reward: -2.590 [-100.000,  6.069], mean action: 1.574 [0.000, 3.000],  loss: 59.418699, mse: 15278.809010, mean_q: 105.256985, mean_eps: 0.835900
  54792/300000: episode: 554, duration: 0.444s, episode steps:  61, steps per second: 138, episode reward: -121.663, mean reward: -1.994 [-100.000,  8.604], mean action: 1.656 [0.000, 3.000],  loss: 33.707335, mse: 15318.140577, mean_q: 106.525363, mean_eps: 0.835717
  54861/300000: episode: 555, duration: 0.545s, episode steps:  69, steps per second: 127, episode reward: -101.674, mean reward: -1.474 [-100.000, 17.460], mean action: 1.493 [0.000, 3.000],  loss: 47.379199, mse: 15802.103218, mean_q: 108.870126, mean_eps: 0.835522
  54935/300000: episode: 556, duration: 0.587s, episode steps:  74, steps per second: 126, episode reward: -102.793, mean reward: -1.389 [-100.000, 11.319], mean action: 1.554 [0.000, 3.000],  loss: 41.527553, mse: 15952.680902, mean_q: 109.187233, mean_eps: 0.835307
  55025/300000: episode: 557, duration: 0.721s, episode steps:  90, steps per second: 125, episode reward: -49.476, mean reward: -0.550 [-100.000,  9.752], mean action: 1.689 [0.000, 3.000],  loss: 40.549407, mse: 16182.493902, mean_q: 111.260623, mean_eps: 0.835062
  55148/300000: episode: 558, duration: 1.037s, episode steps: 123, steps per second: 119, episode reward: -111.488, mean reward: -0.906 [-100.000, 12.936], mean action: 1.667 [0.000, 3.000],  loss: 39.715082, mse: 16172.103738, mean_q: 110.109323, mean_eps: 0.834742
  55246/300000: episode: 559, duration: 0.844s, episode steps:  98, steps per second: 116, episode reward: -38.074, mean reward: -0.389 [-100.000, 84.482], mean action: 1.643 [0.000, 3.000],  loss: 52.817666, mse: 16301.919533, mean_q: 111.075677, mean_eps: 0.834410
  55370/300000: episode: 560, duration: 1.027s, episode steps: 124, steps per second: 121, episode reward:  1.099, mean reward:  0.009 [-100.000, 100.609], mean action: 1.548 [0.000, 3.000],  loss: 32.725655, mse: 16569.187768, mean_q: 113.142227, mean_eps: 0.834078
  55498/300000: episode: 561, duration: 0.980s, episode steps: 128, steps per second: 131, episode reward: -200.698, mean reward: -1.568 [-100.000, 35.177], mean action: 1.695 [0.000, 3.000],  loss: 32.291138, mse: 16830.602196, mean_q: 114.207932, mean_eps: 0.833700
  55558/300000: episode: 562, duration: 0.440s, episode steps:  60, steps per second: 136, episode reward: -91.674, mean reward: -1.528 [-100.000, 17.185], mean action: 1.533 [0.000, 3.000],  loss: 53.274463, mse: 16988.660238, mean_q: 114.957172, mean_eps: 0.833417
  55664/300000: episode: 563, duration: 0.938s, episode steps: 106, steps per second: 113, episode reward: -118.754, mean reward: -1.120 [-100.000, 10.824], mean action: 1.500 [0.000, 3.000],  loss: 37.664065, mse: 17050.690614, mean_q: 114.952965, mean_eps: 0.833169
  55753/300000: episode: 564, duration: 0.695s, episode steps:  89, steps per second: 128, episode reward: -110.262, mean reward: -1.239 [-100.000, 19.360], mean action: 1.787 [0.000, 3.000],  loss: 37.020864, mse: 17345.786133, mean_q: 115.705599, mean_eps: 0.832876
  55836/300000: episode: 565, duration: 0.606s, episode steps:  83, steps per second: 137, episode reward: -79.769, mean reward: -0.961 [-100.000, 13.701], mean action: 1.602 [0.000, 3.000],  loss: 70.495765, mse: 17319.128965, mean_q: 114.141583, mean_eps: 0.832618
  55933/300000: episode: 566, duration: 0.868s, episode steps:  97, steps per second: 112, episode reward: -247.018, mean reward: -2.547 [-100.000, 69.654], mean action: 1.485 [0.000, 3.000],  loss: 48.454724, mse: 17242.911525, mean_q: 112.587511, mean_eps: 0.832348
  56012/300000: episode: 567, duration: 0.727s, episode steps:  79, steps per second: 109, episode reward: -68.830, mean reward: -0.871 [-100.000, 13.733], mean action: 1.696 [0.000, 3.000],  loss: 53.105345, mse: 17409.527270, mean_q: 113.995913, mean_eps: 0.832084
  56109/300000: episode: 568, duration: 0.768s, episode steps:  97, steps per second: 126, episode reward: -120.549, mean reward: -1.243 [-100.000, 17.269], mean action: 1.371 [0.000, 3.000],  loss: 36.841647, mse: 17406.879913, mean_q: 112.941164, mean_eps: 0.831820
  56188/300000: episode: 569, duration: 0.586s, episode steps:  79, steps per second: 135, episode reward: -80.599, mean reward: -1.020 [-100.000, 12.693], mean action: 1.443 [0.000, 3.000],  loss: 48.194253, mse: 17795.883245, mean_q: 116.446989, mean_eps: 0.831556
  56323/300000: episode: 570, duration: 1.325s, episode steps: 135, steps per second: 102, episode reward: -28.538, mean reward: -0.211 [-100.000, 22.738], mean action: 1.548 [0.000, 3.000],  loss: 36.052486, mse: 17635.849552, mean_q: 113.881573, mean_eps: 0.831235
  56419/300000: episode: 571, duration: 0.922s, episode steps:  96, steps per second: 104, episode reward: -112.544, mean reward: -1.172 [-100.000, 12.395], mean action: 1.583 [0.000, 3.000],  loss: 56.388882, mse: 18002.344879, mean_q: 117.441834, mean_eps: 0.830889
  56489/300000: episode: 572, duration: 0.519s, episode steps:  70, steps per second: 135, episode reward: -100.666, mean reward: -1.438 [-100.000, 34.313], mean action: 1.514 [0.000, 3.000],  loss: 38.473806, mse: 18256.624107, mean_q: 117.878389, mean_eps: 0.830640
  56552/300000: episode: 573, duration: 0.430s, episode steps:  63, steps per second: 146, episode reward: -56.082, mean reward: -0.890 [-100.000,  7.314], mean action: 1.714 [0.000, 3.000],  loss: 29.843736, mse: 18742.810888, mean_q: 117.811278, mean_eps: 0.830440
  56632/300000: episode: 574, duration: 0.571s, episode steps:  80, steps per second: 140, episode reward: -141.158, mean reward: -1.764 [-100.000,  6.774], mean action: 1.587 [0.000, 3.000],  loss: 42.019364, mse: 19747.932874, mean_q: 122.979869, mean_eps: 0.830225
  56732/300000: episode: 575, duration: 0.835s, episode steps: 100, steps per second: 120, episode reward: -116.579, mean reward: -1.166 [-100.000,  6.264], mean action: 1.630 [0.000, 3.000],  loss: 50.227292, mse: 19498.682705, mean_q: 121.268520, mean_eps: 0.829955
  56826/300000: episode: 576, duration: 0.722s, episode steps:  94, steps per second: 130, episode reward: -65.656, mean reward: -0.698 [-100.000, 10.324], mean action: 1.596 [0.000, 3.000],  loss: 47.323321, mse: 20038.700195, mean_q: 124.210112, mean_eps: 0.829664
  56896/300000: episode: 577, duration: 0.580s, episode steps:  70, steps per second: 121, episode reward: -83.527, mean reward: -1.193 [-100.000,  9.534], mean action: 1.700 [0.000, 3.000],  loss: 60.012392, mse: 19529.846443, mean_q: 120.726506, mean_eps: 0.829418
  56968/300000: episode: 578, duration: 0.827s, episode steps:  72, steps per second:  87, episode reward: -69.808, mean reward: -0.970 [-100.000,  9.789], mean action: 1.444 [0.000, 3.000],  loss: 36.698309, mse: 19180.331326, mean_q: 117.548428, mean_eps: 0.829206
  57050/300000: episode: 579, duration: 0.805s, episode steps:  82, steps per second: 102, episode reward: -75.741, mean reward: -0.924 [-100.000,  7.209], mean action: 1.415 [0.000, 3.000],  loss: 30.121408, mse: 20013.632503, mean_q: 121.066948, mean_eps: 0.828975
  57132/300000: episode: 580, duration: 0.701s, episode steps:  82, steps per second: 117, episode reward: -55.452, mean reward: -0.676 [-100.000,  7.450], mean action: 1.598 [0.000, 3.000],  loss: 34.991713, mse: 19609.951172, mean_q: 120.785425, mean_eps: 0.828728
  57239/300000: episode: 581, duration: 0.838s, episode steps: 107, steps per second: 128, episode reward: -107.296, mean reward: -1.003 [-100.000, 14.735], mean action: 1.411 [0.000, 3.000],  loss: 43.678989, mse: 19417.803501, mean_q: 119.389814, mean_eps: 0.828445
  57303/300000: episode: 582, duration: 0.556s, episode steps:  64, steps per second: 115, episode reward: 13.061, mean reward:  0.204 [-100.000, 96.521], mean action: 1.391 [0.000, 3.000],  loss: 48.110310, mse: 20217.218857, mean_q: 123.579873, mean_eps: 0.828188
  57396/300000: episode: 583, duration: 0.747s, episode steps:  93, steps per second: 124, episode reward: -79.953, mean reward: -0.860 [-100.000, 11.286], mean action: 1.624 [0.000, 3.000],  loss: 41.561263, mse: 20415.396453, mean_q: 122.778422, mean_eps: 0.827953
  57498/300000: episode: 584, duration: 0.786s, episode steps: 102, steps per second: 130, episode reward: -106.397, mean reward: -1.043 [-100.000,  7.471], mean action: 1.608 [0.000, 3.000],  loss: 58.282226, mse: 19844.820638, mean_q: 121.564022, mean_eps: 0.827660
  57600/300000: episode: 585, duration: 0.708s, episode steps: 102, steps per second: 144, episode reward: -84.302, mean reward: -0.826 [-100.000,  7.491], mean action: 1.490 [0.000, 3.000],  loss: 57.175642, mse: 20445.150678, mean_q: 126.099423, mean_eps: 0.827354
  57688/300000: episode: 586, duration: 0.637s, episode steps:  88, steps per second: 138, episode reward: -108.152, mean reward: -1.229 [-100.000, 61.957], mean action: 1.568 [0.000, 3.000],  loss: 58.519506, mse: 21684.195290, mean_q: 130.901441, mean_eps: 0.827070
  57751/300000: episode: 587, duration: 0.471s, episode steps:  63, steps per second: 134, episode reward: -7.736, mean reward: -0.123 [-100.000, 81.769], mean action: 1.651 [0.000, 3.000],  loss: 34.440442, mse: 21897.962240, mean_q: 130.911611, mean_eps: 0.826843
  57841/300000: episode: 588, duration: 0.623s, episode steps:  90, steps per second: 144, episode reward: -88.853, mean reward: -0.987 [-100.000,  9.108], mean action: 1.389 [0.000, 3.000],  loss: 58.849362, mse: 21830.130621, mean_q: 129.697363, mean_eps: 0.826614
  57952/300000: episode: 589, duration: 0.801s, episode steps: 111, steps per second: 139, episode reward: -222.317, mean reward: -2.003 [-100.000, 37.109], mean action: 1.333 [0.000, 3.000],  loss: 47.035432, mse: 21820.198093, mean_q: 129.676783, mean_eps: 0.826312
  58038/300000: episode: 590, duration: 0.640s, episode steps:  86, steps per second: 134, episode reward: -111.728, mean reward: -1.299 [-100.000, 11.845], mean action: 1.791 [0.000, 3.000],  loss: 49.943621, mse: 21736.638967, mean_q: 126.779646, mean_eps: 0.826016
  58123/300000: episode: 591, duration: 0.622s, episode steps:  85, steps per second: 137, episode reward: -249.091, mean reward: -2.930 [-100.000, 14.992], mean action: 1.329 [0.000, 3.000],  loss: 46.331565, mse: 21288.610708, mean_q: 125.212023, mean_eps: 0.825760
  58207/300000: episode: 592, duration: 0.614s, episode steps:  84, steps per second: 137, episode reward: -130.003, mean reward: -1.548 [-100.000,  8.181], mean action: 1.631 [0.000, 3.000],  loss: 45.936135, mse: 21976.285342, mean_q: 127.946431, mean_eps: 0.825507
  58302/300000: episode: 593, duration: 0.686s, episode steps:  95, steps per second: 139, episode reward: -140.449, mean reward: -1.478 [-100.000,  7.242], mean action: 1.526 [0.000, 3.000],  loss: 51.209370, mse: 22783.167198, mean_q: 132.205432, mean_eps: 0.825238
  58397/300000: episode: 594, duration: 0.704s, episode steps:  95, steps per second: 135, episode reward: -174.246, mean reward: -1.834 [-100.000,  5.798], mean action: 1.474 [0.000, 3.000],  loss: 64.717064, mse: 22729.833655, mean_q: 128.167728, mean_eps: 0.824953
  58498/300000: episode: 595, duration: 0.711s, episode steps: 101, steps per second: 142, episode reward: -231.553, mean reward: -2.293 [-100.000, 19.492], mean action: 1.406 [0.000, 3.000],  loss: 32.699872, mse: 22441.340753, mean_q: 128.358755, mean_eps: 0.824659
  58626/300000: episode: 596, duration: 0.957s, episode steps: 128, steps per second: 134, episode reward: -69.430, mean reward: -0.542 [-100.000,  7.130], mean action: 1.422 [0.000, 3.000],  loss: 44.418996, mse: 22553.723808, mean_q: 127.685173, mean_eps: 0.824315
  58706/300000: episode: 597, duration: 0.598s, episode steps:  80, steps per second: 134, episode reward: -41.577, mean reward: -0.520 [-100.000, 14.049], mean action: 1.575 [0.000, 3.000],  loss: 80.924544, mse: 21728.830151, mean_q: 125.687988, mean_eps: 0.824004
  58803/300000: episode: 598, duration: 0.684s, episode steps:  97, steps per second: 142, episode reward: -113.824, mean reward: -1.173 [-100.000,  9.533], mean action: 1.505 [0.000, 3.000],  loss: 61.195868, mse: 23835.543774, mean_q: 133.802239, mean_eps: 0.823738
  58916/300000: episode: 599, duration: 0.810s, episode steps: 113, steps per second: 139, episode reward: -83.356, mean reward: -0.738 [-100.000, 12.190], mean action: 1.425 [0.000, 3.000],  loss: 46.868920, mse: 23096.876210, mean_q: 131.209573, mean_eps: 0.823423
  58988/300000: episode: 600, duration: 0.557s, episode steps:  72, steps per second: 129, episode reward: -48.530, mean reward: -0.674 [-100.000, 11.729], mean action: 1.444 [0.000, 3.000],  loss: 43.677815, mse: 22558.586914, mean_q: 129.015508, mean_eps: 0.823145
  59086/300000: episode: 601, duration: 0.727s, episode steps:  98, steps per second: 135, episode reward: -121.543, mean reward: -1.240 [-100.000, 12.365], mean action: 1.592 [0.000, 3.000],  loss: 92.548772, mse: 23744.799735, mean_q: 132.671425, mean_eps: 0.822890
  59163/300000: episode: 602, duration: 0.574s, episode steps:  77, steps per second: 134, episode reward: -135.030, mean reward: -1.754 [-100.000,  7.866], mean action: 1.636 [0.000, 3.000],  loss: 94.218307, mse: 23850.603807, mean_q: 133.940731, mean_eps: 0.822628
  59308/300000: episode: 603, duration: 1.111s, episode steps: 145, steps per second: 131, episode reward: -86.926, mean reward: -0.599 [-100.000,  9.847], mean action: 1.731 [0.000, 3.000],  loss: 46.225401, mse: 24335.473821, mean_q: 133.876737, mean_eps: 0.822295
  59371/300000: episode: 604, duration: 0.461s, episode steps:  63, steps per second: 137, episode reward: -150.006, mean reward: -2.381 [-100.000,  5.883], mean action: 1.492 [0.000, 3.000],  loss: 52.931862, mse: 24584.310795, mean_q: 136.075308, mean_eps: 0.821983
  59469/300000: episode: 605, duration: 0.672s, episode steps:  98, steps per second: 146, episode reward: -310.712, mean reward: -3.171 [-100.000,  0.310], mean action: 1.602 [0.000, 3.000],  loss: 67.338299, mse: 25246.266083, mean_q: 136.720338, mean_eps: 0.821742
  59572/300000: episode: 606, duration: 0.732s, episode steps: 103, steps per second: 141, episode reward: -141.149, mean reward: -1.370 [-100.000,  6.682], mean action: 1.505 [0.000, 3.000],  loss: 47.393870, mse: 25130.209032, mean_q: 137.837499, mean_eps: 0.821440
  59677/300000: episode: 607, duration: 0.735s, episode steps: 105, steps per second: 143, episode reward: -68.498, mean reward: -0.652 [-100.000,  8.511], mean action: 1.467 [0.000, 3.000],  loss: 73.095303, mse: 24595.673661, mean_q: 133.662567, mean_eps: 0.821128
  59776/300000: episode: 608, duration: 0.742s, episode steps:  99, steps per second: 133, episode reward: -48.664, mean reward: -0.492 [-100.000, 10.093], mean action: 1.687 [0.000, 3.000],  loss: 58.377248, mse: 25146.625355, mean_q: 136.628713, mean_eps: 0.820822
  59899/300000: episode: 609, duration: 0.973s, episode steps: 123, steps per second: 126, episode reward: -95.561, mean reward: -0.777 [-100.000, 10.430], mean action: 1.545 [0.000, 3.000],  loss: 61.544298, mse: 25630.606930, mean_q: 139.096835, mean_eps: 0.820489
  59972/300000: episode: 610, duration: 0.558s, episode steps:  73, steps per second: 131, episode reward: -116.656, mean reward: -1.598 [-100.000,  4.904], mean action: 1.589 [0.000, 3.000],  loss: 46.294680, mse: 25729.973058, mean_q: 137.083742, mean_eps: 0.820195
  60089/300000: episode: 611, duration: 0.824s, episode steps: 117, steps per second: 142, episode reward: -200.228, mean reward: -1.711 [-100.000, 89.143], mean action: 1.453 [0.000, 3.000],  loss: 53.594777, mse: 25390.321665, mean_q: 136.042074, mean_eps: 0.819910
  60156/300000: episode: 612, duration: 0.532s, episode steps:  67, steps per second: 126, episode reward: -77.036, mean reward: -1.150 [-100.000,  7.462], mean action: 1.612 [0.000, 3.000],  loss: 82.458115, mse: 26403.093954, mean_q: 141.071727, mean_eps: 0.819634
  60248/300000: episode: 613, duration: 0.671s, episode steps:  92, steps per second: 137, episode reward: -111.776, mean reward: -1.215 [-100.000, 12.799], mean action: 1.489 [0.000, 3.000],  loss: 81.534215, mse: 26549.885509, mean_q: 138.639907, mean_eps: 0.819395
  60345/300000: episode: 614, duration: 0.659s, episode steps:  97, steps per second: 147, episode reward: -75.712, mean reward: -0.781 [-100.000, 10.592], mean action: 1.495 [0.000, 3.000],  loss: 48.573585, mse: 26246.672519, mean_q: 139.764765, mean_eps: 0.819112
  60418/300000: episode: 615, duration: 0.532s, episode steps:  73, steps per second: 137, episode reward: -58.982, mean reward: -0.808 [-100.000, 15.881], mean action: 1.548 [0.000, 3.000],  loss: 40.830961, mse: 25546.158604, mean_q: 138.875667, mean_eps: 0.818857
  60485/300000: episode: 616, duration: 0.487s, episode steps:  67, steps per second: 138, episode reward: -109.637, mean reward: -1.636 [-100.000,  7.654], mean action: 1.731 [0.000, 3.000],  loss: 120.030042, mse: 26710.364535, mean_q: 141.481217, mean_eps: 0.818647
  60585/300000: episode: 617, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: -149.615, mean reward: -1.496 [-100.000,  5.185], mean action: 1.430 [0.000, 3.000],  loss: 48.402331, mse: 26596.558984, mean_q: 139.686525, mean_eps: 0.818397
  60667/300000: episode: 618, duration: 0.566s, episode steps:  82, steps per second: 145, episode reward: -107.012, mean reward: -1.305 [-100.000, 16.195], mean action: 1.646 [0.000, 3.000],  loss: 40.668073, mse: 26891.459199, mean_q: 141.133253, mean_eps: 0.818124
  60763/300000: episode: 619, duration: 0.698s, episode steps:  96, steps per second: 137, episode reward: -96.750, mean reward: -1.008 [-100.000, 10.200], mean action: 1.615 [0.000, 3.000],  loss: 64.947574, mse: 26461.535278, mean_q: 141.113998, mean_eps: 0.817856
  60879/300000: episode: 620, duration: 0.881s, episode steps: 116, steps per second: 132, episode reward: -79.895, mean reward: -0.689 [-100.000,  9.547], mean action: 1.681 [0.000, 3.000],  loss: 47.183864, mse: 27497.808105, mean_q: 140.884954, mean_eps: 0.817539
  60952/300000: episode: 621, duration: 0.556s, episode steps:  73, steps per second: 131, episode reward: -62.634, mean reward: -0.858 [-100.000, 21.230], mean action: 1.603 [0.000, 3.000],  loss: 87.860595, mse: 26857.765839, mean_q: 140.035175, mean_eps: 0.817255
  61068/300000: episode: 622, duration: 0.914s, episode steps: 116, steps per second: 127, episode reward: -60.081, mean reward: -0.518 [-100.000, 12.306], mean action: 1.578 [0.000, 3.000],  loss: 75.840745, mse: 27970.656940, mean_q: 143.087322, mean_eps: 0.816971
  61166/300000: episode: 623, duration: 0.683s, episode steps:  98, steps per second: 144, episode reward: -95.724, mean reward: -0.977 [-100.000, 15.634], mean action: 1.449 [0.000, 3.000],  loss: 42.409032, mse: 27798.114537, mean_q: 140.804372, mean_eps: 0.816651
  61283/300000: episode: 624, duration: 0.840s, episode steps: 117, steps per second: 139, episode reward: -67.770, mean reward: -0.579 [-100.000, 13.051], mean action: 1.607 [0.000, 3.000],  loss: 83.481806, mse: 27894.016743, mean_q: 139.703245, mean_eps: 0.816328
  61410/300000: episode: 625, duration: 0.869s, episode steps: 127, steps per second: 146, episode reward: -113.605, mean reward: -0.895 [-100.000,  9.682], mean action: 1.480 [0.000, 3.000],  loss: 78.655737, mse: 27803.581662, mean_q: 142.334627, mean_eps: 0.815962
  61500/300000: episode: 626, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -49.439, mean reward: -0.549 [-100.000, 12.139], mean action: 1.722 [0.000, 3.000],  loss: 86.230072, mse: 29480.155599, mean_q: 144.079014, mean_eps: 0.815636
  61585/300000: episode: 627, duration: 0.598s, episode steps:  85, steps per second: 142, episode reward: -88.494, mean reward: -1.041 [-100.000,  7.806], mean action: 1.459 [0.000, 3.000],  loss: 86.184183, mse: 29224.806870, mean_q: 142.938244, mean_eps: 0.815374
  61696/300000: episode: 628, duration: 0.788s, episode steps: 111, steps per second: 141, episode reward: -160.260, mean reward: -1.444 [-100.000, 15.283], mean action: 1.703 [0.000, 3.000],  loss: 66.805100, mse: 29336.313485, mean_q: 143.911141, mean_eps: 0.815080
  61773/300000: episode: 629, duration: 0.583s, episode steps:  77, steps per second: 132, episode reward: -120.767, mean reward: -1.568 [-100.000, 13.504], mean action: 1.571 [0.000, 3.000],  loss: 34.304215, mse: 30574.303166, mean_q: 147.304874, mean_eps: 0.814798
  61843/300000: episode: 630, duration: 0.537s, episode steps:  70, steps per second: 130, episode reward: -55.651, mean reward: -0.795 [-100.000, 10.728], mean action: 1.443 [0.000, 3.000],  loss: 113.352923, mse: 31353.283287, mean_q: 152.806182, mean_eps: 0.814578
  61949/300000: episode: 631, duration: 0.790s, episode steps: 106, steps per second: 134, episode reward: -196.089, mean reward: -1.850 [-100.000,  0.970], mean action: 1.613 [0.000, 3.000],  loss: 55.778311, mse: 30114.998397, mean_q: 148.868683, mean_eps: 0.814313
  62021/300000: episode: 632, duration: 0.560s, episode steps:  72, steps per second: 129, episode reward: -43.752, mean reward: -0.608 [-100.000, 16.760], mean action: 1.528 [0.000, 3.000],  loss: 71.433503, mse: 31004.195557, mean_q: 151.629777, mean_eps: 0.814047
  62161/300000: episode: 633, duration: 1.178s, episode steps: 140, steps per second: 119, episode reward: -35.659, mean reward: -0.255 [-100.000, 22.583], mean action: 1.464 [0.000, 3.000],  loss: 67.260495, mse: 31957.727148, mean_q: 151.314640, mean_eps: 0.813728
  62262/300000: episode: 634, duration: 0.812s, episode steps: 101, steps per second: 124, episode reward: -143.300, mean reward: -1.419 [-100.000,  9.452], mean action: 1.762 [0.000, 3.000],  loss: 96.092216, mse: 32749.821028, mean_q: 155.227311, mean_eps: 0.813367
  62340/300000: episode: 635, duration: 0.675s, episode steps:  78, steps per second: 116, episode reward: -95.117, mean reward: -1.219 [-100.000, 16.906], mean action: 1.397 [0.000, 3.000],  loss: 53.432829, mse: 33031.515700, mean_q: 153.126679, mean_eps: 0.813099
  62438/300000: episode: 636, duration: 0.778s, episode steps:  98, steps per second: 126, episode reward: -101.282, mean reward: -1.033 [-100.000,  6.692], mean action: 1.602 [0.000, 3.000],  loss: 88.808665, mse: 32891.470225, mean_q: 154.259454, mean_eps: 0.812835
  62556/300000: episode: 637, duration: 0.926s, episode steps: 118, steps per second: 127, episode reward: -78.176, mean reward: -0.663 [-100.000,  7.477], mean action: 1.619 [0.000, 3.000],  loss: 50.404137, mse: 33999.266370, mean_q: 155.316903, mean_eps: 0.812511
  62694/300000: episode: 638, duration: 1.066s, episode steps: 138, steps per second: 129, episode reward: -182.843, mean reward: -1.325 [-100.000, 51.340], mean action: 1.507 [0.000, 3.000],  loss: 53.159794, mse: 34079.533458, mean_q: 155.855714, mean_eps: 0.812126
  62790/300000: episode: 639, duration: 0.758s, episode steps:  96, steps per second: 127, episode reward: -110.622, mean reward: -1.152 [-100.000, 19.333], mean action: 1.417 [0.000, 3.000],  loss: 100.080930, mse: 35436.134562, mean_q: 158.356613, mean_eps: 0.811775
  62925/300000: episode: 640, duration: 1.020s, episode steps: 135, steps per second: 132, episode reward: -184.889, mean reward: -1.370 [-100.000, 20.041], mean action: 1.681 [0.000, 3.000],  loss: 109.043972, mse: 36788.777300, mean_q: 161.539034, mean_eps: 0.811429
  63027/300000: episode: 641, duration: 0.790s, episode steps: 102, steps per second: 129, episode reward: -80.870, mean reward: -0.793 [-100.000, 11.111], mean action: 1.520 [0.000, 3.000],  loss: 104.591421, mse: 36597.428213, mean_q: 163.907142, mean_eps: 0.811074
  63135/300000: episode: 642, duration: 0.794s, episode steps: 108, steps per second: 136, episode reward: -109.088, mean reward: -1.010 [-100.000, 20.725], mean action: 1.417 [0.000, 3.000],  loss: 73.272483, mse: 36873.863209, mean_q: 164.434667, mean_eps: 0.810759
  63229/300000: episode: 643, duration: 0.675s, episode steps:  94, steps per second: 139, episode reward: -83.398, mean reward: -0.887 [-100.000,  6.006], mean action: 1.394 [0.000, 3.000],  loss: 93.924129, mse: 38634.853744, mean_q: 167.312849, mean_eps: 0.810456
  63332/300000: episode: 644, duration: 0.790s, episode steps: 103, steps per second: 130, episode reward: -157.914, mean reward: -1.533 [-100.000, 14.232], mean action: 1.553 [0.000, 3.000],  loss: 52.010681, mse: 39322.072493, mean_q: 168.555353, mean_eps: 0.810160
  63438/300000: episode: 645, duration: 0.811s, episode steps: 106, steps per second: 131, episode reward: -222.100, mean reward: -2.095 [-100.000,  3.582], mean action: 1.434 [0.000, 3.000],  loss: 157.199990, mse: 40760.485646, mean_q: 171.433938, mean_eps: 0.809846
  63537/300000: episode: 646, duration: 0.823s, episode steps:  99, steps per second: 120, episode reward: -50.358, mean reward: -0.509 [-100.000,  9.832], mean action: 1.495 [0.000, 3.000],  loss: 93.874095, mse: 40858.499645, mean_q: 171.750466, mean_eps: 0.809539
  63638/300000: episode: 647, duration: 0.897s, episode steps: 101, steps per second: 113, episode reward: -77.473, mean reward: -0.767 [-100.000, 18.154], mean action: 1.614 [0.000, 3.000],  loss: 78.068195, mse: 41754.712407, mean_q: 173.419918, mean_eps: 0.809239
  63704/300000: episode: 648, duration: 0.550s, episode steps:  66, steps per second: 120, episode reward: -134.929, mean reward: -2.044 [-100.000, 16.065], mean action: 1.530 [0.000, 3.000],  loss: 94.778622, mse: 45997.148941, mean_q: 181.334389, mean_eps: 0.808988
  63827/300000: episode: 649, duration: 1.013s, episode steps: 123, steps per second: 121, episode reward: -100.883, mean reward: -0.820 [-100.000,  9.258], mean action: 1.740 [0.000, 3.000],  loss: 116.022758, mse: 42281.555926, mean_q: 176.517188, mean_eps: 0.808705
  63901/300000: episode: 650, duration: 0.611s, episode steps:  74, steps per second: 121, episode reward: -132.791, mean reward: -1.794 [-100.000, 19.119], mean action: 1.622 [0.000, 3.000],  loss: 68.019057, mse: 45673.397435, mean_q: 180.503600, mean_eps: 0.808410
  64009/300000: episode: 651, duration: 0.820s, episode steps: 108, steps per second: 132, episode reward: -52.629, mean reward: -0.487 [-100.000, 101.779], mean action: 1.315 [0.000, 3.000],  loss: 112.127978, mse: 44628.129919, mean_q: 176.781574, mean_eps: 0.808136
  64101/300000: episode: 652, duration: 0.674s, episode steps:  92, steps per second: 136, episode reward: -302.443, mean reward: -3.287 [-100.000, 76.823], mean action: 1.750 [0.000, 3.000],  loss: 184.384621, mse: 45400.788914, mean_q: 180.850119, mean_eps: 0.807837
  64201/300000: episode: 653, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: -89.839, mean reward: -0.898 [-100.000,  8.844], mean action: 1.540 [0.000, 3.000],  loss: 63.171442, mse: 44629.131074, mean_q: 178.199237, mean_eps: 0.807548
  64291/300000: episode: 654, duration: 0.651s, episode steps:  90, steps per second: 138, episode reward: -119.432, mean reward: -1.327 [-100.000, 44.811], mean action: 1.511 [0.000, 3.000],  loss: 129.858581, mse: 44001.304210, mean_q: 176.146628, mean_eps: 0.807263
  64404/300000: episode: 655, duration: 0.883s, episode steps: 113, steps per second: 128, episode reward: -374.847, mean reward: -3.317 [-100.000, 89.001], mean action: 1.513 [0.000, 3.000],  loss: 65.209373, mse: 46194.906959, mean_q: 182.005652, mean_eps: 0.806959
  64502/300000: episode: 656, duration: 0.738s, episode steps:  98, steps per second: 133, episode reward: -80.664, mean reward: -0.823 [-100.000,  6.331], mean action: 1.439 [0.000, 3.000],  loss: 111.288650, mse: 44655.449338, mean_q: 178.336436, mean_eps: 0.806642
  64600/300000: episode: 657, duration: 0.738s, episode steps:  98, steps per second: 133, episode reward: -39.803, mean reward: -0.406 [-100.000,  7.556], mean action: 1.541 [0.000, 3.000],  loss: 85.888854, mse: 45356.758929, mean_q: 178.038146, mean_eps: 0.806348
  64681/300000: episode: 658, duration: 0.623s, episode steps:  81, steps per second: 130, episode reward: -52.290, mean reward: -0.646 [-100.000,  7.059], mean action: 1.728 [0.000, 3.000],  loss: 192.727312, mse: 46718.900535, mean_q: 185.010310, mean_eps: 0.806080
  64820/300000: episode: 659, duration: 1.021s, episode steps: 139, steps per second: 136, episode reward: -102.091, mean reward: -0.734 [-100.000,  7.170], mean action: 1.417 [0.000, 3.000],  loss: 67.530294, mse: 47136.369632, mean_q: 184.852486, mean_eps: 0.805750
  64932/300000: episode: 660, duration: 0.778s, episode steps: 112, steps per second: 144, episode reward: -56.217, mean reward: -0.502 [-100.000, 11.790], mean action: 1.589 [0.000, 3.000],  loss: 114.797160, mse: 50427.999250, mean_q: 192.259303, mean_eps: 0.805373
  65054/300000: episode: 661, duration: 0.858s, episode steps: 122, steps per second: 142, episode reward: -72.181, mean reward: -0.592 [-100.000, 15.871], mean action: 1.541 [0.000, 3.000],  loss: 92.797966, mse: 51138.810179, mean_q: 191.518193, mean_eps: 0.805022
  65173/300000: episode: 662, duration: 0.820s, episode steps: 119, steps per second: 145, episode reward: -92.950, mean reward: -0.781 [-100.000, 10.606], mean action: 1.412 [0.000, 3.000],  loss: 124.274350, mse: 48738.021534, mean_q: 185.477937, mean_eps: 0.804661
  65266/300000: episode: 663, duration: 0.729s, episode steps:  93, steps per second: 128, episode reward: -62.396, mean reward: -0.671 [-100.000, 56.130], mean action: 1.194 [0.000, 3.000],  loss: 81.455739, mse: 48112.891885, mean_q: 183.881241, mean_eps: 0.804343
  65339/300000: episode: 664, duration: 0.583s, episode steps:  73, steps per second: 125, episode reward: -62.437, mean reward: -0.855 [-100.000,  6.917], mean action: 1.726 [0.000, 3.000],  loss: 79.551042, mse: 49061.958128, mean_q: 187.143396, mean_eps: 0.804094
  65416/300000: episode: 665, duration: 0.595s, episode steps:  77, steps per second: 129, episode reward: -99.982, mean reward: -1.298 [-100.000,  8.850], mean action: 1.455 [0.000, 3.000],  loss: 68.486733, mse: 52515.643897, mean_q: 193.783577, mean_eps: 0.803869
  65530/300000: episode: 666, duration: 0.828s, episode steps: 114, steps per second: 138, episode reward: -47.723, mean reward: -0.419 [-100.000, 13.078], mean action: 1.404 [0.000, 3.000],  loss: 82.036485, mse: 53590.279263, mean_q: 194.320812, mean_eps: 0.803583
  65635/300000: episode: 667, duration: 0.818s, episode steps: 105, steps per second: 128, episode reward: -98.992, mean reward: -0.943 [-100.000, 16.721], mean action: 1.571 [0.000, 3.000],  loss: 217.871608, mse: 50964.974330, mean_q: 192.701907, mean_eps: 0.803254
  65732/300000: episode: 668, duration: 0.752s, episode steps:  97, steps per second: 129, episode reward: -74.349, mean reward: -0.766 [-100.000, 15.731], mean action: 1.588 [0.000, 3.000],  loss: 116.406893, mse: 54047.544620, mean_q: 196.801526, mean_eps: 0.802951
  65849/300000: episode: 669, duration: 0.885s, episode steps: 117, steps per second: 132, episode reward: -122.139, mean reward: -1.044 [-100.000,  6.762], mean action: 1.376 [0.000, 3.000],  loss: 121.422334, mse: 54227.685831, mean_q: 196.161713, mean_eps: 0.802630
  65947/300000: episode: 670, duration: 0.720s, episode steps:  98, steps per second: 136, episode reward: -73.138, mean reward: -0.746 [-100.000, 28.041], mean action: 1.510 [0.000, 3.000],  loss: 169.891816, mse: 58537.713010, mean_q: 201.775237, mean_eps: 0.802307
  66020/300000: episode: 671, duration: 0.562s, episode steps:  73, steps per second: 130, episode reward: -86.341, mean reward: -1.183 [-100.000,  8.242], mean action: 1.534 [0.000, 3.000],  loss: 200.784510, mse: 54074.084118, mean_q: 198.505375, mean_eps: 0.802051
  66119/300000: episode: 672, duration: 0.732s, episode steps:  99, steps per second: 135, episode reward: -131.297, mean reward: -1.326 [-100.000, 14.418], mean action: 1.515 [0.000, 3.000],  loss: 116.527237, mse: 57118.004301, mean_q: 200.740885, mean_eps: 0.801793
  66198/300000: episode: 673, duration: 0.604s, episode steps:  79, steps per second: 131, episode reward: -68.052, mean reward: -0.861 [-100.000, 12.070], mean action: 1.494 [0.000, 3.000],  loss: 199.843940, mse: 56079.321697, mean_q: 198.505391, mean_eps: 0.801526
  66279/300000: episode: 674, duration: 0.581s, episode steps:  81, steps per second: 139, episode reward: -100.580, mean reward: -1.242 [-100.000,  9.544], mean action: 1.654 [0.000, 3.000],  loss: 197.717175, mse: 56092.815008, mean_q: 199.286734, mean_eps: 0.801286
  66396/300000: episode: 675, duration: 0.818s, episode steps: 117, steps per second: 143, episode reward: -25.577, mean reward: -0.219 [-100.000, 21.076], mean action: 1.598 [0.000, 3.000],  loss: 84.976424, mse: 58441.577791, mean_q: 203.798674, mean_eps: 0.800989
  66535/300000: episode: 676, duration: 1.010s, episode steps: 139, steps per second: 138, episode reward: -88.122, mean reward: -0.634 [-100.000,  9.280], mean action: 1.568 [0.000, 3.000],  loss: 148.584449, mse: 62548.066940, mean_q: 209.431252, mean_eps: 0.800605
  66619/300000: episode: 677, duration: 0.622s, episode steps:  84, steps per second: 135, episode reward: -83.880, mean reward: -0.999 [-100.000,  9.906], mean action: 1.750 [0.000, 3.000],  loss: 137.821453, mse: 62451.756301, mean_q: 208.833404, mean_eps: 0.800271
  66725/300000: episode: 678, duration: 0.839s, episode steps: 106, steps per second: 126, episode reward: -90.300, mean reward: -0.852 [-100.000, 16.602], mean action: 1.604 [0.000, 3.000],  loss: 100.593297, mse: 58492.078457, mean_q: 202.699204, mean_eps: 0.799986
  66829/300000: episode: 679, duration: 0.782s, episode steps: 104, steps per second: 133, episode reward: -64.620, mean reward: -0.621 [-100.000,  7.013], mean action: 1.615 [0.000, 3.000],  loss: 81.303129, mse: 61124.032846, mean_q: 208.611641, mean_eps: 0.799670
  66896/300000: episode: 680, duration: 0.478s, episode steps:  67, steps per second: 140, episode reward: -131.111, mean reward: -1.957 [-100.000,  8.790], mean action: 1.254 [0.000, 3.000],  loss: 112.997721, mse: 61893.535302, mean_q: 207.568099, mean_eps: 0.799414
  66998/300000: episode: 681, duration: 0.851s, episode steps: 102, steps per second: 120, episode reward: -97.406, mean reward: -0.955 [-100.000, 14.972], mean action: 1.775 [0.000, 3.000],  loss: 132.816785, mse: 61595.765989, mean_q: 205.923071, mean_eps: 0.799160
  67121/300000: episode: 682, duration: 1.069s, episode steps: 123, steps per second: 115, episode reward: -133.637, mean reward: -1.086 [-100.000,  7.786], mean action: 1.667 [0.000, 3.000],  loss: 234.337111, mse: 67505.135417, mean_q: 213.180058, mean_eps: 0.798823
  67195/300000: episode: 683, duration: 0.622s, episode steps:  74, steps per second: 119, episode reward: -107.334, mean reward: -1.450 [-100.000,  8.856], mean action: 1.676 [0.000, 3.000],  loss: 266.432259, mse: 59969.673485, mean_q: 205.021131, mean_eps: 0.798527
  67267/300000: episode: 684, duration: 0.601s, episode steps:  72, steps per second: 120, episode reward: -31.380, mean reward: -0.436 [-100.000, 49.963], mean action: 1.708 [0.000, 3.000],  loss: 198.837718, mse: 67631.729221, mean_q: 214.204387, mean_eps: 0.798308
  67368/300000: episode: 685, duration: 0.760s, episode steps: 101, steps per second: 133, episode reward: -246.404, mean reward: -2.440 [-100.000,  6.460], mean action: 1.584 [0.000, 3.000],  loss: 78.901725, mse: 68820.319114, mean_q: 213.093711, mean_eps: 0.798049
  67427/300000: episode: 686, duration: 0.427s, episode steps:  59, steps per second: 138, episode reward: -88.722, mean reward: -1.504 [-100.000,  9.237], mean action: 1.746 [0.000, 3.000],  loss: 110.741727, mse: 70541.711467, mean_q: 215.700266, mean_eps: 0.797809
  67538/300000: episode: 687, duration: 0.801s, episode steps: 111, steps per second: 139, episode reward: -65.989, mean reward: -0.594 [-100.000, 10.689], mean action: 1.640 [0.000, 3.000],  loss: 121.867412, mse: 70261.357158, mean_q: 214.792509, mean_eps: 0.797554
  67668/300000: episode: 688, duration: 1.046s, episode steps: 130, steps per second: 124, episode reward: -57.894, mean reward: -0.445 [-100.000, 13.573], mean action: 1.477 [0.000, 3.000],  loss: 127.039138, mse: 72637.377133, mean_q: 218.249395, mean_eps: 0.797192
  67741/300000: episode: 689, duration: 0.515s, episode steps:  73, steps per second: 142, episode reward: -25.019, mean reward: -0.343 [-100.000, 11.700], mean action: 1.616 [0.000, 3.000],  loss: 143.900404, mse: 74061.394852, mean_q: 219.926480, mean_eps: 0.796888
  67857/300000: episode: 690, duration: 0.899s, episode steps: 116, steps per second: 129, episode reward: -40.341, mean reward: -0.348 [-100.000, 17.630], mean action: 1.483 [0.000, 3.000],  loss: 403.548010, mse: 74642.001482, mean_q: 218.449366, mean_eps: 0.796604
  67937/300000: episode: 691, duration: 0.606s, episode steps:  80, steps per second: 132, episode reward: -71.574, mean reward: -0.895 [-100.000,  9.028], mean action: 1.587 [0.000, 3.000],  loss: 146.681073, mse: 74894.352783, mean_q: 218.710606, mean_eps: 0.796311
  68025/300000: episode: 692, duration: 0.622s, episode steps:  88, steps per second: 142, episode reward: 14.642, mean reward:  0.166 [-100.000, 85.724], mean action: 1.534 [0.000, 3.000],  loss: 135.762701, mse: 80318.094771, mean_q: 225.134646, mean_eps: 0.796058
  68126/300000: episode: 693, duration: 0.783s, episode steps: 101, steps per second: 129, episode reward: -209.441, mean reward: -2.074 [-100.000,  6.801], mean action: 1.446 [0.000, 3.000],  loss: 132.208777, mse: 77891.425433, mean_q: 224.750316, mean_eps: 0.795775
  68202/300000: episode: 694, duration: 0.577s, episode steps:  76, steps per second: 132, episode reward: -90.504, mean reward: -1.191 [-100.000,  6.776], mean action: 1.421 [0.000, 3.000],  loss: 343.154321, mse: 76016.931307, mean_q: 223.999665, mean_eps: 0.795509
  68274/300000: episode: 695, duration: 0.547s, episode steps:  72, steps per second: 132, episode reward: -77.187, mean reward: -1.072 [-100.000,  7.533], mean action: 1.389 [0.000, 3.000],  loss: 244.658120, mse: 88188.032362, mean_q: 232.835712, mean_eps: 0.795288
  68352/300000: episode: 696, duration: 0.576s, episode steps:  78, steps per second: 135, episode reward: -81.659, mean reward: -1.047 [-100.000, 22.934], mean action: 1.667 [0.000, 3.000],  loss: 421.127380, mse: 80180.899139, mean_q: 232.047452, mean_eps: 0.795063
  68423/300000: episode: 697, duration: 0.582s, episode steps:  71, steps per second: 122, episode reward: -99.441, mean reward: -1.401 [-100.000, 22.613], mean action: 1.563 [0.000, 3.000],  loss: 184.249603, mse: 83855.992848, mean_q: 229.562433, mean_eps: 0.794839
  68545/300000: episode: 698, duration: 0.877s, episode steps: 122, steps per second: 139, episode reward: -61.755, mean reward: -0.506 [-100.000,  9.007], mean action: 1.623 [0.000, 3.000],  loss: 123.569098, mse: 81935.055200, mean_q: 229.519703, mean_eps: 0.794550
  68623/300000: episode: 699, duration: 0.584s, episode steps:  78, steps per second: 134, episode reward: -71.768, mean reward: -0.920 [-100.000,  9.116], mean action: 1.603 [0.000, 3.000],  loss: 315.069931, mse: 78467.243039, mean_q: 225.692039, mean_eps: 0.794249
  68752/300000: episode: 700, duration: 1.101s, episode steps: 129, steps per second: 117, episode reward: -67.030, mean reward: -0.520 [-100.000, 10.139], mean action: 1.589 [0.000, 3.000],  loss: 305.550073, mse: 88180.515837, mean_q: 236.947569, mean_eps: 0.793939
  68893/300000: episode: 701, duration: 1.173s, episode steps: 141, steps per second: 120, episode reward: 11.417, mean reward:  0.081 [-100.000, 84.608], mean action: 1.532 [0.000, 3.000],  loss: 292.033343, mse: 92075.667248, mean_q: 241.168544, mean_eps: 0.793534
  68965/300000: episode: 702, duration: 0.605s, episode steps:  72, steps per second: 119, episode reward: -47.507, mean reward: -0.660 [-100.000, 10.655], mean action: 1.736 [0.000, 3.000],  loss: 148.547076, mse: 94221.178928, mean_q: 248.365734, mean_eps: 0.793214
  69077/300000: episode: 703, duration: 0.895s, episode steps: 112, steps per second: 125, episode reward: -85.909, mean reward: -0.767 [-100.000,  8.047], mean action: 1.634 [0.000, 3.000],  loss: 151.960183, mse: 91277.094727, mean_q: 245.840415, mean_eps: 0.792938
  69149/300000: episode: 704, duration: 0.586s, episode steps:  72, steps per second: 123, episode reward: -49.288, mean reward: -0.685 [-100.000, 17.849], mean action: 1.625 [0.000, 3.000],  loss: 76.882805, mse: 88012.722493, mean_q: 240.608120, mean_eps: 0.792663
  69231/300000: episode: 705, duration: 0.664s, episode steps:  82, steps per second: 124, episode reward: -83.474, mean reward: -1.018 [-100.000,  8.690], mean action: 1.573 [0.000, 3.000],  loss: 209.089371, mse: 89666.971989, mean_q: 241.481186, mean_eps: 0.792431
  69345/300000: episode: 706, duration: 0.873s, episode steps: 114, steps per second: 131, episode reward: -206.194, mean reward: -1.809 [-100.000,  5.497], mean action: 1.772 [0.000, 3.000],  loss: 176.474625, mse: 91520.648643, mean_q: 244.579789, mean_eps: 0.792137
  69423/300000: episode: 707, duration: 0.588s, episode steps:  78, steps per second: 133, episode reward: -126.268, mean reward: -1.619 [-100.000, 13.150], mean action: 1.487 [0.000, 3.000],  loss: 288.398530, mse: 96504.921074, mean_q: 246.079496, mean_eps: 0.791849
  69482/300000: episode: 708, duration: 0.473s, episode steps:  59, steps per second: 125, episode reward: -65.225, mean reward: -1.106 [-100.000,  9.175], mean action: 1.746 [0.000, 3.000],  loss: 336.860968, mse: 88403.480734, mean_q: 243.379473, mean_eps: 0.791644
  69573/300000: episode: 709, duration: 0.717s, episode steps:  91, steps per second: 127, episode reward: -88.845, mean reward: -0.976 [-100.000,  5.498], mean action: 1.484 [0.000, 3.000],  loss: 134.312363, mse: 90649.782624, mean_q: 245.115058, mean_eps: 0.791419
  69637/300000: episode: 710, duration: 0.513s, episode steps:  64, steps per second: 125, episode reward: -73.759, mean reward: -1.152 [-100.000,  9.280], mean action: 1.547 [0.000, 3.000],  loss: 211.852126, mse: 85892.952637, mean_q: 242.267795, mean_eps: 0.791187
  69758/300000: episode: 711, duration: 0.863s, episode steps: 121, steps per second: 140, episode reward: -91.596, mean reward: -0.757 [-100.000, 11.055], mean action: 1.603 [0.000, 3.000],  loss: 193.489169, mse: 82618.423069, mean_q: 238.074701, mean_eps: 0.790909
  69847/300000: episode: 712, duration: 0.707s, episode steps:  89, steps per second: 126, episode reward: -117.861, mean reward: -1.324 [-100.000,  5.714], mean action: 1.360 [0.000, 3.000],  loss: 228.844679, mse: 80112.380047, mean_q: 238.678674, mean_eps: 0.790594
  69962/300000: episode: 713, duration: 0.801s, episode steps: 115, steps per second: 144, episode reward: -76.325, mean reward: -0.664 [-100.000, 10.963], mean action: 1.661 [0.000, 3.000],  loss: 89.036412, mse: 75432.685394, mean_q: 234.447018, mean_eps: 0.790288
  70051/300000: episode: 714, duration: 0.641s, episode steps:  89, steps per second: 139, episode reward: -136.630, mean reward: -1.535 [-100.000, 22.966], mean action: 1.551 [0.000, 3.000],  loss: 160.307666, mse: 77286.697331, mean_q: 238.009563, mean_eps: 0.789982
  70119/300000: episode: 715, duration: 0.564s, episode steps:  68, steps per second: 121, episode reward: -65.657, mean reward: -0.966 [-100.000,  6.607], mean action: 1.500 [0.000, 3.000],  loss: 125.160061, mse: 70385.492360, mean_q: 231.952518, mean_eps: 0.789747
  70193/300000: episode: 716, duration: 0.538s, episode steps:  74, steps per second: 138, episode reward: -72.664, mean reward: -0.982 [-100.000,  6.099], mean action: 1.635 [0.000, 3.000],  loss: 86.780121, mse: 70261.582506, mean_q: 235.784124, mean_eps: 0.789533
  70265/300000: episode: 717, duration: 0.546s, episode steps:  72, steps per second: 132, episode reward: -150.952, mean reward: -2.097 [-100.000,  5.393], mean action: 1.750 [0.000, 3.000],  loss: 167.900621, mse: 67143.510037, mean_q: 234.045343, mean_eps: 0.789314
  70346/300000: episode: 718, duration: 0.658s, episode steps:  81, steps per second: 123, episode reward: -69.803, mean reward: -0.862 [-100.000, 13.606], mean action: 1.617 [0.000, 3.000],  loss: 103.159173, mse: 67927.750000, mean_q: 234.739497, mean_eps: 0.789085
  70442/300000: episode: 719, duration: 0.771s, episode steps:  96, steps per second: 124, episode reward: -108.375, mean reward: -1.129 [-100.000,  6.469], mean action: 1.531 [0.000, 3.000],  loss: 175.027523, mse: 67892.582560, mean_q: 235.763821, mean_eps: 0.788820
  70551/300000: episode: 720, duration: 0.904s, episode steps: 109, steps per second: 121, episode reward: -111.814, mean reward: -1.026 [-100.000,  9.663], mean action: 1.560 [0.000, 3.000],  loss: 164.037533, mse: 72093.189041, mean_q: 240.686842, mean_eps: 0.788512
  70649/300000: episode: 721, duration: 0.905s, episode steps:  98, steps per second: 108, episode reward: -86.763, mean reward: -0.885 [-100.000,  7.835], mean action: 1.520 [0.000, 3.000],  loss: 170.437981, mse: 71393.918248, mean_q: 241.069440, mean_eps: 0.788201
  70757/300000: episode: 722, duration: 1.038s, episode steps: 108, steps per second: 104, episode reward: -112.416, mean reward: -1.041 [-100.000,  8.290], mean action: 1.583 [0.000, 3.000],  loss: 150.646153, mse: 72766.016999, mean_q: 244.676455, mean_eps: 0.787893
  70887/300000: episode: 723, duration: 1.338s, episode steps: 130, steps per second:  97, episode reward: -80.016, mean reward: -0.616 [-100.000,  8.324], mean action: 1.577 [0.000, 3.000],  loss: 140.442499, mse: 75175.052194, mean_q: 247.326518, mean_eps: 0.787536
  70954/300000: episode: 724, duration: 0.661s, episode steps:  67, steps per second: 101, episode reward: -73.132, mean reward: -1.092 [-100.000, 20.727], mean action: 1.776 [0.000, 3.000],  loss: 120.662990, mse: 75564.163479, mean_q: 247.721065, mean_eps: 0.787240
  71079/300000: episode: 725, duration: 1.128s, episode steps: 125, steps per second: 111, episode reward: -136.422, mean reward: -1.091 [-100.000,  5.672], mean action: 1.672 [0.000, 3.000],  loss: 195.602231, mse: 76980.175625, mean_q: 251.577208, mean_eps: 0.786952
  71144/300000: episode: 726, duration: 0.527s, episode steps:  65, steps per second: 123, episode reward: -72.209, mean reward: -1.111 [-100.000, 15.570], mean action: 1.646 [0.000, 3.000],  loss: 61.002371, mse: 76796.364663, mean_q: 250.000015, mean_eps: 0.786667
  71242/300000: episode: 727, duration: 0.723s, episode steps:  98, steps per second: 136, episode reward: -103.271, mean reward: -1.054 [-100.000, 12.627], mean action: 1.520 [0.000, 3.000],  loss: 247.307181, mse: 77722.753827, mean_q: 254.785469, mean_eps: 0.786423
  71332/300000: episode: 728, duration: 0.722s, episode steps:  90, steps per second: 125, episode reward: -100.058, mean reward: -1.112 [-100.000,  6.966], mean action: 1.500 [0.000, 3.000],  loss: 241.019109, mse: 78545.625521, mean_q: 255.327869, mean_eps: 0.786141
  71407/300000: episode: 729, duration: 0.584s, episode steps:  75, steps per second: 128, episode reward: -52.711, mean reward: -0.703 [-100.000, 13.583], mean action: 1.693 [0.000, 3.000],  loss: 76.716483, mse: 79076.731875, mean_q: 253.083521, mean_eps: 0.785893
  71469/300000: episode: 730, duration: 0.578s, episode steps:  62, steps per second: 107, episode reward: -82.277, mean reward: -1.327 [-100.000, 11.554], mean action: 1.855 [0.000, 3.000],  loss: 111.182018, mse: 79695.171812, mean_q: 256.560358, mean_eps: 0.785687
  71582/300000: episode: 731, duration: 0.931s, episode steps: 113, steps per second: 121, episode reward: -126.330, mean reward: -1.118 [-100.000, 30.907], mean action: 1.681 [0.000, 3.000],  loss: 183.405715, mse: 74451.230814, mean_q: 248.800765, mean_eps: 0.785425
  71669/300000: episode: 732, duration: 0.769s, episode steps:  87, steps per second: 113, episode reward: -137.946, mean reward: -1.586 [-100.000,  5.323], mean action: 1.345 [0.000, 3.000],  loss: 76.446091, mse: 78336.999237, mean_q: 254.182917, mean_eps: 0.785125
  71800/300000: episode: 733, duration: 1.025s, episode steps: 131, steps per second: 128, episode reward: -67.032, mean reward: -0.512 [-100.000, 10.118], mean action: 1.534 [0.000, 3.000],  loss: 94.378353, mse: 76403.449755, mean_q: 249.241569, mean_eps: 0.784798
  71882/300000: episode: 734, duration: 0.736s, episode steps:  82, steps per second: 111, episode reward: -23.429, mean reward: -0.286 [-100.000, 11.293], mean action: 1.598 [0.000, 3.000],  loss: 275.147481, mse: 76778.790587, mean_q: 249.534088, mean_eps: 0.784478
  71999/300000: episode: 735, duration: 0.982s, episode steps: 117, steps per second: 119, episode reward: -60.502, mean reward: -0.517 [-100.000,  6.898], mean action: 1.513 [0.000, 3.000],  loss: 203.282058, mse: 75351.393096, mean_q: 249.258717, mean_eps: 0.784180
  72131/300000: episode: 736, duration: 1.102s, episode steps: 132, steps per second: 120, episode reward: -33.904, mean reward: -0.257 [-100.000, 11.395], mean action: 1.561 [0.000, 3.000],  loss: 137.828883, mse: 79567.014086, mean_q: 255.578564, mean_eps: 0.783807
  72253/300000: episode: 737, duration: 1.049s, episode steps: 122, steps per second: 116, episode reward: -182.358, mean reward: -1.495 [-100.000,  5.917], mean action: 1.639 [0.000, 3.000],  loss: 197.384631, mse: 77634.084144, mean_q: 253.015072, mean_eps: 0.783426
  72355/300000: episode: 738, duration: 0.745s, episode steps: 102, steps per second: 137, episode reward: -76.587, mean reward: -0.751 [-100.000, 10.768], mean action: 1.578 [0.000, 3.000],  loss: 62.159216, mse: 78998.546990, mean_q: 256.038613, mean_eps: 0.783089
  72438/300000: episode: 739, duration: 0.595s, episode steps:  83, steps per second: 139, episode reward: -122.566, mean reward: -1.477 [-100.000, 23.258], mean action: 1.470 [0.000, 3.000],  loss: 74.728710, mse: 81594.568242, mean_q: 259.975816, mean_eps: 0.782812
  72529/300000: episode: 740, duration: 0.652s, episode steps:  91, steps per second: 140, episode reward: -53.180, mean reward: -0.584 [-100.000, 11.509], mean action: 1.615 [0.000, 3.000],  loss: 191.318545, mse: 80755.991243, mean_q: 261.420224, mean_eps: 0.782551
  72644/300000: episode: 741, duration: 0.836s, episode steps: 115, steps per second: 138, episode reward: -109.307, mean reward: -0.950 [-100.000, 27.244], mean action: 1.661 [0.000, 3.000],  loss: 173.150298, mse: 82856.554959, mean_q: 264.543901, mean_eps: 0.782242
  72714/300000: episode: 742, duration: 0.581s, episode steps:  70, steps per second: 120, episode reward: -50.286, mean reward: -0.718 [-100.000, 15.817], mean action: 1.571 [0.000, 3.000],  loss: 114.912474, mse: 79116.160993, mean_q: 256.871535, mean_eps: 0.781964
  72815/300000: episode: 743, duration: 0.796s, episode steps: 101, steps per second: 127, episode reward: -46.192, mean reward: -0.457 [-100.000, 35.529], mean action: 1.525 [0.000, 3.000],  loss: 95.984197, mse: 83532.278543, mean_q: 264.848077, mean_eps: 0.781708
  72942/300000: episode: 744, duration: 0.979s, episode steps: 127, steps per second: 130, episode reward: -65.666, mean reward: -0.517 [-100.000, 12.924], mean action: 1.606 [0.000, 3.000],  loss: 157.165521, mse: 79874.272453, mean_q: 256.179321, mean_eps: 0.781366
  73016/300000: episode: 745, duration: 0.569s, episode steps:  74, steps per second: 130, episode reward: -52.506, mean reward: -0.710 [-100.000,  8.957], mean action: 1.730 [0.000, 3.000],  loss: 101.058235, mse: 79143.933119, mean_q: 255.186895, mean_eps: 0.781064
  73126/300000: episode: 746, duration: 0.794s, episode steps: 110, steps per second: 139, episode reward: -109.128, mean reward: -0.992 [-100.000,  8.285], mean action: 1.609 [0.000, 3.000],  loss: 119.324059, mse: 83887.100249, mean_q: 261.892684, mean_eps: 0.780788
  73205/300000: episode: 747, duration: 0.591s, episode steps:  79, steps per second: 134, episode reward: -70.998, mean reward: -0.899 [-100.000,  6.982], mean action: 1.241 [0.000, 3.000],  loss: 79.161342, mse: 77487.706290, mean_q: 251.547072, mean_eps: 0.780505
  73279/300000: episode: 748, duration: 0.574s, episode steps:  74, steps per second: 129, episode reward: -90.660, mean reward: -1.225 [-100.000, 15.602], mean action: 1.662 [0.000, 3.000],  loss: 51.843680, mse: 80290.687236, mean_q: 257.058694, mean_eps: 0.780276
  73356/300000: episode: 749, duration: 0.615s, episode steps:  77, steps per second: 125, episode reward: -107.963, mean reward: -1.402 [-100.000, 13.426], mean action: 1.545 [0.000, 3.000],  loss: 219.634531, mse: 78479.634892, mean_q: 254.592018, mean_eps: 0.780049
  73440/300000: episode: 750, duration: 0.596s, episode steps:  84, steps per second: 141, episode reward: -113.024, mean reward: -1.346 [-100.000, 11.696], mean action: 1.512 [0.000, 3.000],  loss: 81.635263, mse: 76777.571754, mean_q: 248.252610, mean_eps: 0.779808
  73556/300000: episode: 751, duration: 0.909s, episode steps: 116, steps per second: 128, episode reward: -61.190, mean reward: -0.528 [-100.000, 10.211], mean action: 1.655 [0.000, 3.000],  loss: 153.562017, mse: 79660.710702, mean_q: 254.162380, mean_eps: 0.779508
  73698/300000: episode: 752, duration: 1.039s, episode steps: 142, steps per second: 137, episode reward: -203.705, mean reward: -1.435 [-100.000, 58.323], mean action: 1.627 [0.000, 3.000],  loss: 117.473848, mse: 80064.462093, mean_q: 256.669584, mean_eps: 0.779120
  73792/300000: episode: 753, duration: 0.669s, episode steps:  94, steps per second: 140, episode reward: -204.096, mean reward: -2.171 [-100.000, 50.090], mean action: 1.202 [0.000, 3.000],  loss: 78.914894, mse: 80122.943941, mean_q: 260.426322, mean_eps: 0.778767
  73892/300000: episode: 754, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: -85.315, mean reward: -0.853 [-100.000, 19.809], mean action: 1.610 [0.000, 3.000],  loss: 114.429030, mse: 80904.404453, mean_q: 261.499526, mean_eps: 0.778475
  73963/300000: episode: 755, duration: 0.530s, episode steps:  71, steps per second: 134, episode reward: -77.834, mean reward: -1.096 [-100.000, 17.912], mean action: 1.479 [0.000, 3.000],  loss: 84.614410, mse: 79261.859155, mean_q: 259.599086, mean_eps: 0.778219
  74085/300000: episode: 756, duration: 0.888s, episode steps: 122, steps per second: 137, episode reward: -69.391, mean reward: -0.569 [-100.000,  8.275], mean action: 1.623 [0.000, 3.000],  loss: 246.849963, mse: 88107.321465, mean_q: 273.757949, mean_eps: 0.777929
  74163/300000: episode: 757, duration: 0.616s, episode steps:  78, steps per second: 127, episode reward: -42.217, mean reward: -0.541 [-100.000, 13.752], mean action: 1.436 [0.000, 3.000],  loss: 69.939800, mse: 89381.127103, mean_q: 275.691044, mean_eps: 0.777630
  74256/300000: episode: 758, duration: 0.674s, episode steps:  93, steps per second: 138, episode reward: -96.053, mean reward: -1.033 [-100.000, 11.392], mean action: 1.677 [0.000, 3.000],  loss: 100.804113, mse: 89328.619414, mean_q: 275.023996, mean_eps: 0.777373
  74323/300000: episode: 759, duration: 0.474s, episode steps:  67, steps per second: 141, episode reward: -149.387, mean reward: -2.230 [-100.000,  4.491], mean action: 1.433 [0.000, 3.000],  loss: 201.401523, mse: 86555.338270, mean_q: 275.481927, mean_eps: 0.777133
  74455/300000: episode: 760, duration: 1.034s, episode steps: 132, steps per second: 128, episode reward: -214.849, mean reward: -1.628 [-100.000,  2.541], mean action: 1.697 [0.000, 3.000],  loss: 92.148126, mse: 88928.523023, mean_q: 276.931138, mean_eps: 0.776834
  74548/300000: episode: 761, duration: 0.713s, episode steps:  93, steps per second: 130, episode reward: -130.683, mean reward: -1.405 [-100.000,  8.314], mean action: 1.591 [0.000, 3.000],  loss: 97.741066, mse: 89253.308048, mean_q: 274.894998, mean_eps: 0.776497
  74667/300000: episode: 762, duration: 0.951s, episode steps: 119, steps per second: 125, episode reward: -123.668, mean reward: -1.039 [-100.000,  9.582], mean action: 1.697 [0.000, 3.000],  loss: 83.289008, mse: 89980.090599, mean_q: 274.047891, mean_eps: 0.776179
  74774/300000: episode: 763, duration: 0.847s, episode steps: 107, steps per second: 126, episode reward: -147.470, mean reward: -1.378 [-100.000,  6.413], mean action: 1.579 [0.000, 3.000],  loss: 73.713171, mse: 89175.493611, mean_q: 274.009627, mean_eps: 0.775840
  74862/300000: episode: 764, duration: 0.715s, episode steps:  88, steps per second: 123, episode reward: -83.187, mean reward: -0.945 [-100.000,  5.949], mean action: 1.716 [0.000, 3.000],  loss: 88.034111, mse: 90759.297674, mean_q: 275.732341, mean_eps: 0.775548
  74944/300000: episode: 765, duration: 0.698s, episode steps:  82, steps per second: 118, episode reward: -92.732, mean reward: -1.131 [-100.000,  6.405], mean action: 1.488 [0.000, 3.000],  loss: 84.733413, mse: 92303.760337, mean_q: 276.690756, mean_eps: 0.775293
  75029/300000: episode: 766, duration: 0.737s, episode steps:  85, steps per second: 115, episode reward: -45.070, mean reward: -0.530 [-100.000,  8.282], mean action: 1.824 [0.000, 3.000],  loss: 59.121994, mse: 88040.403401, mean_q: 269.109454, mean_eps: 0.775042
  75129/300000: episode: 767, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: -87.674, mean reward: -0.877 [-100.000, 10.466], mean action: 1.490 [0.000, 3.000],  loss: 77.073407, mse: 89421.950078, mean_q: 270.306095, mean_eps: 0.774764
  75251/300000: episode: 768, duration: 0.918s, episode steps: 122, steps per second: 133, episode reward: -104.946, mean reward: -0.860 [-100.000, 11.424], mean action: 1.541 [0.000, 3.000],  loss: 83.588032, mse: 88050.049757, mean_q: 270.982507, mean_eps: 0.774432
  75349/300000: episode: 769, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -84.162, mean reward: -0.859 [-100.000,  6.297], mean action: 1.561 [0.000, 3.000],  loss: 82.378938, mse: 88582.555006, mean_q: 273.144072, mean_eps: 0.774101
  75434/300000: episode: 770, duration: 0.594s, episode steps:  85, steps per second: 143, episode reward: -73.231, mean reward: -0.862 [-100.000,  7.604], mean action: 1.600 [0.000, 3.000],  loss: 88.240587, mse: 94812.970588, mean_q: 279.386814, mean_eps: 0.773827
  75526/300000: episode: 771, duration: 0.694s, episode steps:  92, steps per second: 133, episode reward: -77.926, mean reward: -0.847 [-100.000, 11.345], mean action: 1.772 [0.000, 3.000],  loss: 140.268536, mse: 91805.279594, mean_q: 275.656175, mean_eps: 0.773561
  75656/300000: episode: 772, duration: 0.947s, episode steps: 130, steps per second: 137, episode reward: -181.887, mean reward: -1.399 [-100.000,  5.498], mean action: 1.492 [0.000, 3.000],  loss: 95.828679, mse: 93729.156400, mean_q: 279.102202, mean_eps: 0.773228
  75779/300000: episode: 773, duration: 0.905s, episode steps: 123, steps per second: 136, episode reward: -63.123, mean reward: -0.513 [-100.000, 12.887], mean action: 1.618 [0.000, 3.000],  loss: 105.180032, mse: 93764.560753, mean_q: 278.119180, mean_eps: 0.772849
  75852/300000: episode: 774, duration: 0.592s, episode steps:  73, steps per second: 123, episode reward: -61.945, mean reward: -0.849 [-100.000,  7.369], mean action: 1.699 [0.000, 3.000],  loss: 247.597721, mse: 95179.861676, mean_q: 284.180137, mean_eps: 0.772555
  75953/300000: episode: 775, duration: 0.792s, episode steps: 101, steps per second: 128, episode reward: -87.811, mean reward: -0.869 [-100.000, 13.910], mean action: 1.713 [0.000, 3.000],  loss: 160.606154, mse: 92521.886564, mean_q: 278.295601, mean_eps: 0.772294
  76079/300000: episode: 776, duration: 1.018s, episode steps: 126, steps per second: 124, episode reward: -239.633, mean reward: -1.902 [-100.000, 67.052], mean action: 1.746 [0.000, 3.000],  loss: 142.726973, mse: 92088.812438, mean_q: 277.497991, mean_eps: 0.771954
  76182/300000: episode: 777, duration: 0.791s, episode steps: 103, steps per second: 130, episode reward: -30.122, mean reward: -0.292 [-100.000, 17.587], mean action: 1.728 [0.000, 3.000],  loss: 109.929194, mse: 93965.202594, mean_q: 276.247317, mean_eps: 0.771610
  76275/300000: episode: 778, duration: 0.674s, episode steps:  93, steps per second: 138, episode reward: -83.420, mean reward: -0.897 [-100.000,  6.204], mean action: 1.753 [0.000, 3.000],  loss: 164.395718, mse: 93781.389407, mean_q: 280.588199, mean_eps: 0.771316
  76392/300000: episode: 779, duration: 0.976s, episode steps: 117, steps per second: 120, episode reward: -267.426, mean reward: -2.286 [-100.000,  7.474], mean action: 1.496 [0.000, 3.000],  loss: 74.395896, mse: 94138.407585, mean_q: 281.330780, mean_eps: 0.771001
  76503/300000: episode: 780, duration: 0.846s, episode steps: 111, steps per second: 131, episode reward: -140.533, mean reward: -1.266 [-100.000, 12.879], mean action: 1.505 [0.000, 3.000],  loss: 107.764697, mse: 94035.973184, mean_q: 278.829021, mean_eps: 0.770659
  76587/300000: episode: 781, duration: 0.609s, episode steps:  84, steps per second: 138, episode reward: -56.831, mean reward: -0.677 [-100.000,  7.895], mean action: 1.690 [0.000, 3.000],  loss: 150.474410, mse: 93257.237537, mean_q: 280.494587, mean_eps: 0.770367
  76737/300000: episode: 782, duration: 1.177s, episode steps: 150, steps per second: 127, episode reward: -125.680, mean reward: -0.838 [-100.000,  9.508], mean action: 1.647 [0.000, 3.000],  loss: 115.503068, mse: 96383.808958, mean_q: 283.230675, mean_eps: 0.770016
  76858/300000: episode: 783, duration: 0.896s, episode steps: 121, steps per second: 135, episode reward: -140.324, mean reward: -1.160 [-100.000, 11.272], mean action: 1.736 [0.000, 3.000],  loss: 88.848818, mse: 93491.854565, mean_q: 278.701232, mean_eps: 0.769609
  76940/300000: episode: 784, duration: 0.630s, episode steps:  82, steps per second: 130, episode reward: -79.807, mean reward: -0.973 [-100.000,  7.059], mean action: 1.463 [0.000, 3.000],  loss: 133.314901, mse: 95397.770675, mean_q: 281.095137, mean_eps: 0.769304
  77034/300000: episode: 785, duration: 0.704s, episode steps:  94, steps per second: 134, episode reward: -32.583, mean reward: -0.347 [-100.000, 11.733], mean action: 1.479 [0.000, 3.000],  loss: 106.847566, mse: 95385.196601, mean_q: 280.610785, mean_eps: 0.769041
  77123/300000: episode: 786, duration: 0.693s, episode steps:  89, steps per second: 128, episode reward: -79.981, mean reward: -0.899 [-100.000,  7.285], mean action: 1.764 [0.000, 3.000],  loss: 93.982206, mse: 92734.096603, mean_q: 274.741704, mean_eps: 0.768766
  77259/300000: episode: 787, duration: 1.079s, episode steps: 136, steps per second: 126, episode reward: -82.239, mean reward: -0.605 [-100.000, 14.118], mean action: 1.478 [0.000, 3.000],  loss: 66.508082, mse: 94451.103200, mean_q: 276.600768, mean_eps: 0.768428
  77384/300000: episode: 788, duration: 0.938s, episode steps: 125, steps per second: 133, episode reward: -81.227, mean reward: -0.650 [-100.000,  6.874], mean action: 1.808 [0.000, 3.000],  loss: 163.273061, mse: 91213.292312, mean_q: 271.873375, mean_eps: 0.768037
  77519/300000: episode: 789, duration: 0.989s, episode steps: 135, steps per second: 137, episode reward: -187.721, mean reward: -1.391 [-100.000, 28.015], mean action: 1.630 [0.000, 3.000],  loss: 114.644449, mse: 93823.153009, mean_q: 275.343187, mean_eps: 0.767647
  77599/300000: episode: 790, duration: 0.575s, episode steps:  80, steps per second: 139, episode reward: -77.228, mean reward: -0.965 [-100.000,  5.721], mean action: 1.675 [0.000, 3.000],  loss: 65.038773, mse: 93126.444336, mean_q: 276.735168, mean_eps: 0.767324
  77682/300000: episode: 791, duration: 0.589s, episode steps:  83, steps per second: 141, episode reward: -77.742, mean reward: -0.937 [-100.000,  7.048], mean action: 1.494 [0.000, 3.000],  loss: 226.431604, mse: 92397.788404, mean_q: 279.558182, mean_eps: 0.767080
  77756/300000: episode: 792, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -14.952, mean reward: -0.202 [-100.000, 20.098], mean action: 1.757 [0.000, 3.000],  loss: 99.712789, mse: 91719.894109, mean_q: 273.585540, mean_eps: 0.766844
  77838/300000: episode: 793, duration: 0.615s, episode steps:  82, steps per second: 133, episode reward: -129.329, mean reward: -1.577 [-100.000,  8.640], mean action: 1.537 [0.000, 3.000],  loss: 154.315171, mse: 95161.425734, mean_q: 275.991661, mean_eps: 0.766610
  77928/300000: episode: 794, duration: 0.629s, episode steps:  90, steps per second: 143, episode reward: -74.688, mean reward: -0.830 [-100.000, 13.290], mean action: 1.522 [0.000, 3.000],  loss: 90.883636, mse: 91020.575304, mean_q: 274.348079, mean_eps: 0.766353
  78059/300000: episode: 795, duration: 0.891s, episode steps: 131, steps per second: 147, episode reward: -140.418, mean reward: -1.072 [-100.000,  4.932], mean action: 1.695 [0.000, 3.000],  loss: 82.926430, mse: 90018.132276, mean_q: 270.920412, mean_eps: 0.766021
  78167/300000: episode: 796, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: -159.583, mean reward: -1.478 [-100.000, 11.356], mean action: 1.481 [0.000, 3.000],  loss: 83.200045, mse: 90815.512370, mean_q: 270.985605, mean_eps: 0.765663
  78234/300000: episode: 797, duration: 0.485s, episode steps:  67, steps per second: 138, episode reward: -64.270, mean reward: -0.959 [-100.000,  6.277], mean action: 1.881 [0.000, 3.000],  loss: 71.510103, mse: 92430.618120, mean_q: 270.578508, mean_eps: 0.765400
  78326/300000: episode: 798, duration: 0.699s, episode steps:  92, steps per second: 132, episode reward: -74.718, mean reward: -0.812 [-100.000, 11.493], mean action: 1.565 [0.000, 3.000],  loss: 109.229092, mse: 88641.588825, mean_q: 264.310463, mean_eps: 0.765161
  78414/300000: episode: 799, duration: 0.740s, episode steps:  88, steps per second: 119, episode reward: -49.285, mean reward: -0.560 [-100.000, 11.397], mean action: 1.682 [0.000, 3.000],  loss: 119.907126, mse: 92488.289240, mean_q: 273.611898, mean_eps: 0.764891
  78543/300000: episode: 800, duration: 1.204s, episode steps: 129, steps per second: 107, episode reward: -193.269, mean reward: -1.498 [-100.000,  7.557], mean action: 1.388 [0.000, 3.000],  loss: 68.634979, mse: 91913.956607, mean_q: 275.138415, mean_eps: 0.764566
  78638/300000: episode: 801, duration: 0.828s, episode steps:  95, steps per second: 115, episode reward: -203.892, mean reward: -2.146 [-100.000, 14.190], mean action: 1.695 [0.000, 3.000],  loss: 84.156310, mse: 94019.576357, mean_q: 277.395372, mean_eps: 0.764230
  78708/300000: episode: 802, duration: 0.530s, episode steps:  70, steps per second: 132, episode reward: -69.360, mean reward: -0.991 [-100.000,  8.904], mean action: 1.471 [0.000, 3.000],  loss: 89.785955, mse: 90484.255692, mean_q: 273.340141, mean_eps: 0.763983
  78788/300000: episode: 803, duration: 0.581s, episode steps:  80, steps per second: 138, episode reward: -67.733, mean reward: -0.847 [-100.000, 12.534], mean action: 1.750 [0.000, 3.000],  loss: 122.309556, mse: 97281.210156, mean_q: 281.384908, mean_eps: 0.763757
  78866/300000: episode: 804, duration: 0.551s, episode steps:  78, steps per second: 142, episode reward: -71.775, mean reward: -0.920 [-100.000,  9.383], mean action: 1.667 [0.000, 3.000],  loss: 123.993338, mse: 90636.971404, mean_q: 273.426941, mean_eps: 0.763520
  78948/300000: episode: 805, duration: 0.711s, episode steps:  82, steps per second: 115, episode reward: -87.626, mean reward: -1.069 [-100.000,  7.061], mean action: 1.671 [0.000, 3.000],  loss: 100.038214, mse: 94490.955650, mean_q: 275.027403, mean_eps: 0.763281
  79029/300000: episode: 806, duration: 0.701s, episode steps:  81, steps per second: 115, episode reward: -49.696, mean reward: -0.614 [-100.000, 19.541], mean action: 1.642 [0.000, 3.000],  loss: 82.573171, mse: 93283.663291, mean_q: 276.491713, mean_eps: 0.763036
  79134/300000: episode: 807, duration: 0.838s, episode steps: 105, steps per second: 125, episode reward: -67.259, mean reward: -0.641 [-100.000,  7.069], mean action: 1.552 [0.000, 3.000],  loss: 84.308175, mse: 96752.552641, mean_q: 280.967244, mean_eps: 0.762757
  79244/300000: episode: 808, duration: 0.860s, episode steps: 110, steps per second: 128, episode reward: -90.473, mean reward: -0.822 [-100.000, 11.589], mean action: 1.636 [0.000, 3.000],  loss: 97.111640, mse: 94603.217791, mean_q: 275.034443, mean_eps: 0.762435
  79318/300000: episode: 809, duration: 0.572s, episode steps:  74, steps per second: 129, episode reward: -41.749, mean reward: -0.564 [-100.000, 18.048], mean action: 1.595 [0.000, 3.000],  loss: 75.039670, mse: 97406.927470, mean_q: 277.439284, mean_eps: 0.762158
  79430/300000: episode: 810, duration: 0.877s, episode steps: 112, steps per second: 128, episode reward: -75.857, mean reward: -0.677 [-100.000, 18.307], mean action: 1.518 [0.000, 3.000],  loss: 69.872948, mse: 95016.178850, mean_q: 274.962070, mean_eps: 0.761880
  79517/300000: episode: 811, duration: 0.717s, episode steps:  87, steps per second: 121, episode reward: -111.054, mean reward: -1.276 [-100.000, 27.573], mean action: 1.632 [0.000, 3.000],  loss: 46.213628, mse: 92391.567843, mean_q: 273.622498, mean_eps: 0.761581
  79626/300000: episode: 812, duration: 0.776s, episode steps: 109, steps per second: 140, episode reward: -75.340, mean reward: -0.691 [-100.000, 10.340], mean action: 1.706 [0.000, 3.000],  loss: 55.667481, mse: 92568.058558, mean_q: 272.953755, mean_eps: 0.761287
  79721/300000: episode: 813, duration: 0.676s, episode steps:  95, steps per second: 141, episode reward: -156.272, mean reward: -1.645 [-100.000, 13.745], mean action: 1.337 [0.000, 3.000],  loss: 137.949291, mse: 93159.071916, mean_q: 266.252850, mean_eps: 0.760981
  79806/300000: episode: 814, duration: 0.616s, episode steps:  85, steps per second: 138, episode reward: -46.728, mean reward: -0.550 [-100.000,  9.620], mean action: 1.694 [0.000, 3.000],  loss: 99.875495, mse: 89684.533640, mean_q: 269.408231, mean_eps: 0.760711
  79898/300000: episode: 815, duration: 0.638s, episode steps:  92, steps per second: 144, episode reward: -91.527, mean reward: -0.995 [-100.000, 11.684], mean action: 1.630 [0.000, 3.000],  loss: 66.813532, mse: 90672.262695, mean_q: 272.023900, mean_eps: 0.760446
  79978/300000: episode: 816, duration: 0.560s, episode steps:  80, steps per second: 143, episode reward: -99.257, mean reward: -1.241 [-100.000, 10.852], mean action: 1.587 [0.000, 3.000],  loss: 134.564022, mse: 89509.089355, mean_q: 268.385346, mean_eps: 0.760188
  80097/300000: episode: 817, duration: 0.950s, episode steps: 119, steps per second: 125, episode reward: -48.380, mean reward: -0.407 [-100.000,  6.868], mean action: 1.571 [0.000, 3.000],  loss: 84.969510, mse: 91013.499967, mean_q: 267.996662, mean_eps: 0.759889
  80168/300000: episode: 818, duration: 0.557s, episode steps:  71, steps per second: 128, episode reward: -78.048, mean reward: -1.099 [-100.000,  8.742], mean action: 1.620 [0.000, 3.000],  loss: 79.308812, mse: 88137.109485, mean_q: 266.170599, mean_eps: 0.759604
  80312/300000: episode: 819, duration: 1.049s, episode steps: 144, steps per second: 137, episode reward: -324.141, mean reward: -2.251 [-100.000, 111.051], mean action: 1.542 [0.000, 3.000],  loss: 78.042776, mse: 92011.018311, mean_q: 268.987731, mean_eps: 0.759282
  80436/300000: episode: 820, duration: 1.003s, episode steps: 124, steps per second: 124, episode reward: -210.445, mean reward: -1.697 [-100.000, 99.470], mean action: 1.839 [0.000, 3.000],  loss: 194.765276, mse: 87647.603201, mean_q: 262.365239, mean_eps: 0.758879
  80518/300000: episode: 821, duration: 0.643s, episode steps:  82, steps per second: 127, episode reward: -153.695, mean reward: -1.874 [-100.000, 19.056], mean action: 1.744 [0.000, 3.000],  loss: 64.818865, mse: 89657.666349, mean_q: 264.783430, mean_eps: 0.758570
  80583/300000: episode: 822, duration: 0.562s, episode steps:  65, steps per second: 116, episode reward: -98.708, mean reward: -1.519 [-100.000,  5.931], mean action: 1.354 [0.000, 3.000],  loss: 123.093639, mse: 91088.370913, mean_q: 268.102561, mean_eps: 0.758350
  80694/300000: episode: 823, duration: 0.920s, episode steps: 111, steps per second: 121, episode reward: -58.501, mean reward: -0.527 [-100.000, 13.657], mean action: 1.613 [0.000, 3.000],  loss: 71.765062, mse: 87439.826225, mean_q: 264.056990, mean_eps: 0.758086
  80822/300000: episode: 824, duration: 1.003s, episode steps: 128, steps per second: 128, episode reward: -27.127, mean reward: -0.212 [-100.000, 55.721], mean action: 1.641 [0.000, 3.000],  loss: 67.942639, mse: 88629.291107, mean_q: 264.363659, mean_eps: 0.757727
  80961/300000: episode: 825, duration: 1.119s, episode steps: 139, steps per second: 124, episode reward: -102.819, mean reward: -0.740 [-100.000, 15.050], mean action: 1.612 [0.000, 3.000],  loss: 82.323998, mse: 90299.105581, mean_q: 268.526850, mean_eps: 0.757327
  81075/300000: episode: 826, duration: 0.839s, episode steps: 114, steps per second: 136, episode reward: -63.043, mean reward: -0.553 [-100.000, 13.637], mean action: 1.588 [0.000, 3.000],  loss: 68.482303, mse: 87681.670093, mean_q: 263.562945, mean_eps: 0.756948
  81158/300000: episode: 827, duration: 0.621s, episode steps:  83, steps per second: 134, episode reward: -59.657, mean reward: -0.719 [-100.000, 17.912], mean action: 1.578 [0.000, 3.000],  loss: 93.186946, mse: 88664.949030, mean_q: 262.964924, mean_eps: 0.756652
  81240/300000: episode: 828, duration: 0.579s, episode steps:  82, steps per second: 142, episode reward: -16.004, mean reward: -0.195 [-100.000, 34.509], mean action: 1.659 [0.000, 3.000],  loss: 55.647341, mse: 92738.356660, mean_q: 272.217076, mean_eps: 0.756405
  81347/300000: episode: 829, duration: 0.794s, episode steps: 107, steps per second: 135, episode reward: -53.442, mean reward: -0.499 [-100.000, 16.858], mean action: 1.720 [0.000, 3.000],  loss: 53.189443, mse: 91142.867224, mean_q: 269.041124, mean_eps: 0.756121
  81446/300000: episode: 830, duration: 0.779s, episode steps:  99, steps per second: 127, episode reward: -85.669, mean reward: -0.865 [-100.000, 11.383], mean action: 1.616 [0.000, 3.000],  loss: 50.306463, mse: 90497.662247, mean_q: 268.614781, mean_eps: 0.755812
  81536/300000: episode: 831, duration: 0.663s, episode steps:  90, steps per second: 136, episode reward: -86.954, mean reward: -0.966 [-100.000,  6.623], mean action: 1.644 [0.000, 3.000],  loss: 42.838790, mse: 86586.633594, mean_q: 264.795642, mean_eps: 0.755529
  81636/300000: episode: 832, duration: 0.778s, episode steps: 100, steps per second: 129, episode reward: -97.863, mean reward: -0.979 [-100.000,  4.499], mean action: 1.560 [0.000, 3.000],  loss: 121.662475, mse: 86775.788047, mean_q: 264.244035, mean_eps: 0.755243
  81711/300000: episode: 833, duration: 0.560s, episode steps:  75, steps per second: 134, episode reward: -42.315, mean reward: -0.564 [-100.000, 17.239], mean action: 1.613 [0.000, 3.000],  loss: 62.059280, mse: 88218.440312, mean_q: 263.622230, mean_eps: 0.754981
  81815/300000: episode: 834, duration: 0.801s, episode steps: 104, steps per second: 130, episode reward: -54.443, mean reward: -0.523 [-100.000, 12.955], mean action: 1.558 [0.000, 3.000],  loss: 54.195727, mse: 86580.208609, mean_q: 262.236491, mean_eps: 0.754713
  81895/300000: episode: 835, duration: 0.577s, episode steps:  80, steps per second: 139, episode reward: -115.553, mean reward: -1.444 [-100.000, 22.859], mean action: 1.637 [0.000, 3.000],  loss: 77.003138, mse: 79937.834473, mean_q: 259.055380, mean_eps: 0.754437
  81989/300000: episode: 836, duration: 0.697s, episode steps:  94, steps per second: 135, episode reward: -107.565, mean reward: -1.144 [-100.000,  7.481], mean action: 1.500 [0.000, 3.000],  loss: 82.826974, mse: 82759.600607, mean_q: 262.335396, mean_eps: 0.754176
  82080/300000: episode: 837, duration: 0.684s, episode steps:  91, steps per second: 133, episode reward: -51.187, mean reward: -0.562 [-100.000, 20.682], mean action: 1.648 [0.000, 3.000],  loss: 70.002098, mse: 82170.830958, mean_q: 265.388764, mean_eps: 0.753898
  82199/300000: episode: 838, duration: 0.840s, episode steps: 119, steps per second: 142, episode reward: -131.310, mean reward: -1.103 [-100.000,  7.868], mean action: 1.723 [0.000, 3.000],  loss: 62.018924, mse: 81367.812894, mean_q: 264.156721, mean_eps: 0.753583
  82285/300000: episode: 839, duration: 0.619s, episode steps:  86, steps per second: 139, episode reward: -14.584, mean reward: -0.170 [-100.000, 22.238], mean action: 1.872 [0.000, 3.000],  loss: 57.182792, mse: 78487.639126, mean_q: 259.884841, mean_eps: 0.753276
  82445/300000: episode: 840, duration: 1.297s, episode steps: 160, steps per second: 123, episode reward: 10.884, mean reward:  0.068 [-100.000, 13.652], mean action: 1.681 [0.000, 3.000],  loss: 68.720896, mse: 74177.693604, mean_q: 251.197119, mean_eps: 0.752907
  82520/300000: episode: 841, duration: 0.596s, episode steps:  75, steps per second: 126, episode reward: -85.506, mean reward: -1.140 [-100.000,  9.128], mean action: 1.507 [0.000, 3.000],  loss: 39.776525, mse: 72533.589531, mean_q: 250.024957, mean_eps: 0.752554
  82614/300000: episode: 842, duration: 0.755s, episode steps:  94, steps per second: 125, episode reward: -117.827, mean reward: -1.253 [-100.000,  7.792], mean action: 1.585 [0.000, 3.000],  loss: 96.835790, mse: 74570.659450, mean_q: 256.752184, mean_eps: 0.752301
  82740/300000: episode: 843, duration: 0.925s, episode steps: 126, steps per second: 136, episode reward: -358.787, mean reward: -2.848 [-100.000,  4.019], mean action: 1.437 [0.000, 3.000],  loss: 84.853933, mse: 73819.849175, mean_q: 254.620635, mean_eps: 0.751970
  82816/300000: episode: 844, duration: 0.575s, episode steps:  76, steps per second: 132, episode reward: -64.989, mean reward: -0.855 [-100.000, 16.979], mean action: 1.618 [0.000, 3.000],  loss: 58.197580, mse: 74753.714176, mean_q: 256.552462, mean_eps: 0.751668
  82884/300000: episode: 845, duration: 0.554s, episode steps:  68, steps per second: 123, episode reward: -79.330, mean reward: -1.167 [-100.000,  6.566], mean action: 1.779 [0.000, 3.000],  loss: 99.189327, mse: 74748.447151, mean_q: 255.505082, mean_eps: 0.751451
  82988/300000: episode: 846, duration: 0.771s, episode steps: 104, steps per second: 135, episode reward: -95.448, mean reward: -0.918 [-100.000,  9.153], mean action: 1.692 [0.000, 3.000],  loss: 80.920096, mse: 73894.076247, mean_q: 254.027041, mean_eps: 0.751193
  83109/300000: episode: 847, duration: 0.914s, episode steps: 121, steps per second: 132, episode reward: -7.464, mean reward: -0.062 [-100.000, 14.091], mean action: 1.603 [0.000, 3.000],  loss: 63.523957, mse: 74702.043421, mean_q: 256.232562, mean_eps: 0.750856
  83270/300000: episode: 848, duration: 1.314s, episode steps: 161, steps per second: 122, episode reward: -316.248, mean reward: -1.964 [-100.000, 97.965], mean action: 1.627 [0.000, 3.000],  loss: 60.731815, mse: 75048.260239, mean_q: 255.860952, mean_eps: 0.750433
  83360/300000: episode: 849, duration: 0.711s, episode steps:  90, steps per second: 127, episode reward: -84.772, mean reward: -0.942 [-100.000, 10.552], mean action: 1.611 [0.000, 3.000],  loss: 47.365237, mse: 73483.468490, mean_q: 252.807803, mean_eps: 0.750056
  83444/300000: episode: 850, duration: 0.653s, episode steps:  84, steps per second: 129, episode reward: -8.456, mean reward: -0.101 [-100.000, 68.559], mean action: 1.524 [0.000, 3.000],  loss: 54.006578, mse: 74560.108910, mean_q: 254.675484, mean_eps: 0.749795
  83552/300000: episode: 851, duration: 0.746s, episode steps: 108, steps per second: 145, episode reward: -93.329, mean reward: -0.864 [-100.000, 12.951], mean action: 1.509 [0.000, 3.000],  loss: 89.270993, mse: 74193.817057, mean_q: 252.344445, mean_eps: 0.749508
  83679/300000: episode: 852, duration: 0.887s, episode steps: 127, steps per second: 143, episode reward: -38.131, mean reward: -0.300 [-100.000, 14.231], mean action: 1.551 [0.000, 3.000],  loss: 87.476432, mse: 74341.241019, mean_q: 252.475505, mean_eps: 0.749155
  83756/300000: episode: 853, duration: 0.576s, episode steps:  77, steps per second: 134, episode reward: -93.288, mean reward: -1.212 [-100.000, 11.733], mean action: 1.662 [0.000, 3.000],  loss: 84.117234, mse: 78228.703429, mean_q: 262.576528, mean_eps: 0.748849
  83850/300000: episode: 854, duration: 0.667s, episode steps:  94, steps per second: 141, episode reward: -131.023, mean reward: -1.394 [-100.000, 16.764], mean action: 1.660 [0.000, 3.000],  loss: 68.355733, mse: 77616.122216, mean_q: 260.648545, mean_eps: 0.748593
  83942/300000: episode: 855, duration: 0.648s, episode steps:  92, steps per second: 142, episode reward: -109.605, mean reward: -1.191 [-100.000,  9.122], mean action: 1.620 [0.000, 3.000],  loss: 85.064725, mse: 76512.964164, mean_q: 258.346542, mean_eps: 0.748313
  84056/300000: episode: 856, duration: 0.835s, episode steps: 114, steps per second: 137, episode reward: -77.176, mean reward: -0.677 [-100.000, 16.176], mean action: 1.588 [0.000, 3.000],  loss: 47.558132, mse: 76653.439213, mean_q: 259.292420, mean_eps: 0.748004
  84144/300000: episode: 857, duration: 0.626s, episode steps:  88, steps per second: 141, episode reward: -93.649, mean reward: -1.064 [-100.000, 10.756], mean action: 1.557 [0.000, 3.000],  loss: 67.236481, mse: 76899.976829, mean_q: 259.191021, mean_eps: 0.747701
  84248/300000: episode: 858, duration: 0.745s, episode steps: 104, steps per second: 140, episode reward: -83.532, mean reward: -0.803 [-100.000, 20.296], mean action: 1.596 [0.000, 3.000],  loss: 62.543870, mse: 75043.783767, mean_q: 253.367192, mean_eps: 0.747413
  84316/300000: episode: 859, duration: 0.526s, episode steps:  68, steps per second: 129, episode reward: -43.019, mean reward: -0.633 [-100.000, 13.566], mean action: 1.647 [0.000, 3.000],  loss: 36.386774, mse: 75636.081227, mean_q: 253.333102, mean_eps: 0.747156
  84407/300000: episode: 860, duration: 0.687s, episode steps:  91, steps per second: 133, episode reward: -85.525, mean reward: -0.940 [-100.000, 13.056], mean action: 1.780 [0.000, 3.000],  loss: 68.382413, mse: 74927.267943, mean_q: 251.649163, mean_eps: 0.746917
  84505/300000: episode: 861, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -83.884, mean reward: -0.856 [-100.000, 11.702], mean action: 1.837 [0.000, 3.000],  loss: 52.149362, mse: 74317.733578, mean_q: 250.222527, mean_eps: 0.746634
  84639/300000: episode: 862, duration: 1.047s, episode steps: 134, steps per second: 128, episode reward: -44.963, mean reward: -0.336 [-100.000, 17.072], mean action: 1.567 [0.000, 3.000],  loss: 61.871760, mse: 74914.995015, mean_q: 251.028181, mean_eps: 0.746285
  84703/300000: episode: 863, duration: 0.478s, episode steps:  64, steps per second: 134, episode reward: -72.564, mean reward: -1.134 [-100.000, 16.403], mean action: 1.469 [0.000, 3.000],  loss: 32.408743, mse: 75399.380371, mean_q: 254.375201, mean_eps: 0.745988
  84830/300000: episode: 864, duration: 0.916s, episode steps: 127, steps per second: 139, episode reward: -59.579, mean reward: -0.469 [-100.000, 11.319], mean action: 1.449 [0.000, 3.000],  loss: 50.976057, mse: 75145.640287, mean_q: 252.717521, mean_eps: 0.745702
  84907/300000: episode: 865, duration: 0.609s, episode steps:  77, steps per second: 126, episode reward: -98.078, mean reward: -1.274 [-100.000, 10.318], mean action: 1.532 [0.000, 3.000],  loss: 34.449749, mse: 75018.017299, mean_q: 249.632988, mean_eps: 0.745396
  85040/300000: episode: 866, duration: 1.013s, episode steps: 133, steps per second: 131, episode reward: -188.888, mean reward: -1.420 [-100.000, 26.784], mean action: 1.759 [0.000, 3.000],  loss: 37.559611, mse: 74571.318580, mean_q: 250.803912, mean_eps: 0.745081
  85147/300000: episode: 867, duration: 0.755s, episode steps: 107, steps per second: 142, episode reward: -55.535, mean reward: -0.519 [-100.000,  8.721], mean action: 1.682 [0.000, 3.000],  loss: 48.336193, mse: 75116.021722, mean_q: 253.367156, mean_eps: 0.744721
  85256/300000: episode: 868, duration: 0.802s, episode steps: 109, steps per second: 136, episode reward: -88.067, mean reward: -0.808 [-100.000, 23.169], mean action: 1.495 [0.000, 3.000],  loss: 46.894623, mse: 74361.103856, mean_q: 251.043084, mean_eps: 0.744397
  85357/300000: episode: 869, duration: 0.731s, episode steps: 101, steps per second: 138, episode reward: -125.333, mean reward: -1.241 [-100.000, 31.858], mean action: 1.564 [0.000, 3.000],  loss: 39.360341, mse: 76607.552367, mean_q: 254.783423, mean_eps: 0.744082
  85462/300000: episode: 870, duration: 0.863s, episode steps: 105, steps per second: 122, episode reward: -73.804, mean reward: -0.703 [-100.000,  8.070], mean action: 1.562 [0.000, 3.000],  loss: 51.387777, mse: 74085.382589, mean_q: 247.786103, mean_eps: 0.743773
  85562/300000: episode: 871, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: -67.361, mean reward: -0.674 [-100.000,  9.886], mean action: 1.830 [0.000, 3.000],  loss: 50.701567, mse: 74833.642969, mean_q: 250.314382, mean_eps: 0.743466
  85653/300000: episode: 872, duration: 0.717s, episode steps:  91, steps per second: 127, episode reward: -81.326, mean reward: -0.894 [-100.000,  5.856], mean action: 1.615 [0.000, 3.000],  loss: 40.947658, mse: 74401.817050, mean_q: 252.784556, mean_eps: 0.743179
  85775/300000: episode: 873, duration: 1.065s, episode steps: 122, steps per second: 115, episode reward: -40.577, mean reward: -0.333 [-100.000,  9.081], mean action: 1.672 [0.000, 3.000],  loss: 45.887342, mse: 74678.336290, mean_q: 250.329176, mean_eps: 0.742859
  85847/300000: episode: 874, duration: 0.638s, episode steps:  72, steps per second: 113, episode reward: -149.281, mean reward: -2.073 [-100.000, 20.920], mean action: 1.458 [0.000, 3.000],  loss: 47.227879, mse: 73458.669108, mean_q: 249.571580, mean_eps: 0.742568
  85922/300000: episode: 875, duration: 0.717s, episode steps:  75, steps per second: 105, episode reward: -87.335, mean reward: -1.164 [-100.000,  7.051], mean action: 1.667 [0.000, 3.000],  loss: 38.703872, mse: 75132.524948, mean_q: 252.935635, mean_eps: 0.742348
  86023/300000: episode: 876, duration: 0.763s, episode steps: 101, steps per second: 132, episode reward: -71.742, mean reward: -0.710 [-100.000,  7.765], mean action: 1.693 [0.000, 3.000],  loss: 43.936773, mse: 73887.050472, mean_q: 249.131909, mean_eps: 0.742084
  86095/300000: episode: 877, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: -18.204, mean reward: -0.253 [-100.000, 46.023], mean action: 1.583 [0.000, 3.000],  loss: 37.653614, mse: 75198.155545, mean_q: 253.587878, mean_eps: 0.741824
  86181/300000: episode: 878, duration: 0.592s, episode steps:  86, steps per second: 145, episode reward: -40.050, mean reward: -0.466 [-100.000, 12.379], mean action: 1.686 [0.000, 3.000],  loss: 31.174612, mse: 73410.057049, mean_q: 247.016634, mean_eps: 0.741588
  86253/300000: episode: 879, duration: 0.495s, episode steps:  72, steps per second: 145, episode reward: -71.409, mean reward: -0.992 [-100.000, 17.493], mean action: 1.542 [0.000, 3.000],  loss: 36.228955, mse: 73115.797743, mean_q: 246.869361, mean_eps: 0.741350
  86357/300000: episode: 880, duration: 0.752s, episode steps: 104, steps per second: 138, episode reward: -119.185, mean reward: -1.146 [-100.000, 14.558], mean action: 1.529 [0.000, 3.000],  loss: 37.828816, mse: 71161.875751, mean_q: 241.077654, mean_eps: 0.741086
  86515/300000: episode: 881, duration: 1.109s, episode steps: 158, steps per second: 143, episode reward: -19.771, mean reward: -0.125 [-100.000, 11.134], mean action: 1.595 [0.000, 3.000],  loss: 40.302460, mse: 71787.006947, mean_q: 242.661536, mean_eps: 0.740694
  86653/300000: episode: 882, duration: 1.025s, episode steps: 138, steps per second: 135, episode reward: -71.118, mean reward: -0.515 [-100.000, 14.142], mean action: 1.536 [0.000, 3.000],  loss: 36.176170, mse: 71502.919412, mean_q: 241.235181, mean_eps: 0.740249
  86768/300000: episode: 883, duration: 0.887s, episode steps: 115, steps per second: 130, episode reward: -88.692, mean reward: -0.771 [-100.000,  8.703], mean action: 1.617 [0.000, 3.000],  loss: 32.990906, mse: 71081.899049, mean_q: 239.030559, mean_eps: 0.739870
  86904/300000: episode: 884, duration: 0.983s, episode steps: 136, steps per second: 138, episode reward:  0.559, mean reward:  0.004 [-100.000, 16.958], mean action: 1.735 [0.000, 3.000],  loss: 35.080160, mse: 71741.337230, mean_q: 242.422644, mean_eps: 0.739494
  87021/300000: episode: 885, duration: 0.824s, episode steps: 117, steps per second: 142, episode reward: -73.445, mean reward: -0.628 [-100.000,  6.555], mean action: 1.556 [0.000, 3.000],  loss: 37.769995, mse: 71227.198918, mean_q: 240.892083, mean_eps: 0.739114
  87114/300000: episode: 886, duration: 0.635s, episode steps:  93, steps per second: 147, episode reward: -112.749, mean reward: -1.212 [-100.000,  6.819], mean action: 1.581 [0.000, 3.000],  loss: 44.607738, mse: 70999.055570, mean_q: 240.133419, mean_eps: 0.738799
  87216/300000: episode: 887, duration: 0.714s, episode steps: 102, steps per second: 143, episode reward: -55.679, mean reward: -0.546 [-100.000,  6.670], mean action: 1.667 [0.000, 3.000],  loss: 41.242267, mse: 71344.112324, mean_q: 240.895712, mean_eps: 0.738507
  87311/300000: episode: 888, duration: 0.660s, episode steps:  95, steps per second: 144, episode reward: -115.103, mean reward: -1.212 [-100.000, 17.361], mean action: 1.632 [0.000, 3.000],  loss: 38.552710, mse: 69374.258347, mean_q: 235.718809, mean_eps: 0.738211
  87425/300000: episode: 889, duration: 0.851s, episode steps: 114, steps per second: 134, episode reward: -67.439, mean reward: -0.592 [-100.000,  9.150], mean action: 1.614 [0.000, 3.000],  loss: 36.447150, mse: 68837.849472, mean_q: 234.847802, mean_eps: 0.737898
  87515/300000: episode: 890, duration: 0.651s, episode steps:  90, steps per second: 138, episode reward: -33.827, mean reward: -0.376 [-100.000, 23.040], mean action: 1.656 [0.000, 3.000],  loss: 29.253579, mse: 68151.619748, mean_q: 233.622694, mean_eps: 0.737591
  87623/300000: episode: 891, duration: 0.817s, episode steps: 108, steps per second: 132, episode reward: -64.273, mean reward: -0.595 [-100.000, 12.187], mean action: 1.620 [0.000, 3.000],  loss: 36.253871, mse: 67969.272497, mean_q: 233.299519, mean_eps: 0.737294
  87705/300000: episode: 892, duration: 0.734s, episode steps:  82, steps per second: 112, episode reward: -70.309, mean reward: -0.857 [-100.000, 12.968], mean action: 1.805 [0.000, 3.000],  loss: 29.155115, mse: 68913.231612, mean_q: 233.859717, mean_eps: 0.737009
  87796/300000: episode: 893, duration: 0.832s, episode steps:  91, steps per second: 109, episode reward: -72.425, mean reward: -0.796 [-100.000,  8.814], mean action: 1.659 [0.000, 3.000],  loss: 23.116346, mse: 69624.358559, mean_q: 235.685697, mean_eps: 0.736750
  87892/300000: episode: 894, duration: 0.849s, episode steps:  96, steps per second: 113, episode reward: -22.097, mean reward: -0.230 [-100.000, 11.669], mean action: 1.635 [0.000, 3.000],  loss: 34.690107, mse: 70399.881388, mean_q: 240.539776, mean_eps: 0.736470
  87978/300000: episode: 895, duration: 0.742s, episode steps:  86, steps per second: 116, episode reward: -33.799, mean reward: -0.393 [-100.000,  9.588], mean action: 1.581 [0.000, 3.000],  loss: 37.186383, mse: 69371.611464, mean_q: 235.769566, mean_eps: 0.736196
  88084/300000: episode: 896, duration: 1.020s, episode steps: 106, steps per second: 104, episode reward: -52.554, mean reward: -0.496 [-100.000, 13.742], mean action: 1.557 [0.000, 3.000],  loss: 39.391579, mse: 69053.078014, mean_q: 234.497585, mean_eps: 0.735908
  88192/300000: episode: 897, duration: 0.916s, episode steps: 108, steps per second: 118, episode reward: -99.242, mean reward: -0.919 [-100.000, 12.451], mean action: 1.593 [0.000, 3.000],  loss: 35.469263, mse: 67516.354962, mean_q: 232.198715, mean_eps: 0.735587
  88319/300000: episode: 898, duration: 0.978s, episode steps: 127, steps per second: 130, episode reward: -114.924, mean reward: -0.905 [-100.000, 17.763], mean action: 1.567 [0.000, 3.000],  loss: 42.602045, mse: 67214.771561, mean_q: 230.209709, mean_eps: 0.735235
  88413/300000: episode: 899, duration: 0.678s, episode steps:  94, steps per second: 139, episode reward: -49.815, mean reward: -0.530 [-100.000, 13.688], mean action: 1.755 [0.000, 3.000],  loss: 31.022526, mse: 66131.949385, mean_q: 226.425012, mean_eps: 0.734904
  88504/300000: episode: 900, duration: 0.658s, episode steps:  91, steps per second: 138, episode reward: -48.994, mean reward: -0.538 [-100.000, 14.845], mean action: 1.593 [0.000, 3.000],  loss: 36.735987, mse: 67015.736650, mean_q: 229.981851, mean_eps: 0.734626
  88636/300000: episode: 901, duration: 1.088s, episode steps: 132, steps per second: 121, episode reward: -79.017, mean reward: -0.599 [-100.000, 11.467], mean action: 1.598 [0.000, 3.000],  loss: 27.763913, mse: 68888.616832, mean_q: 234.779103, mean_eps: 0.734291
  88737/300000: episode: 902, duration: 0.781s, episode steps: 101, steps per second: 129, episode reward: -67.996, mean reward: -0.673 [-100.000, 11.908], mean action: 1.465 [0.000, 3.000],  loss: 35.497898, mse: 68990.149946, mean_q: 235.502846, mean_eps: 0.733942
  88815/300000: episode: 903, duration: 0.653s, episode steps:  78, steps per second: 119, episode reward: -68.378, mean reward: -0.877 [-100.000, 16.269], mean action: 1.410 [0.000, 3.000],  loss: 33.969024, mse: 68786.627704, mean_q: 234.578954, mean_eps: 0.733674
  88904/300000: episode: 904, duration: 0.704s, episode steps:  89, steps per second: 126, episode reward: -53.015, mean reward: -0.596 [-100.000, 12.827], mean action: 1.573 [0.000, 3.000],  loss: 36.952445, mse: 69634.555916, mean_q: 236.874731, mean_eps: 0.733423
  89003/300000: episode: 905, duration: 0.860s, episode steps:  99, steps per second: 115, episode reward: -95.040, mean reward: -0.960 [-100.000,  7.374], mean action: 1.707 [0.000, 3.000],  loss: 40.397350, mse: 66797.561119, mean_q: 227.423566, mean_eps: 0.733141
  89091/300000: episode: 906, duration: 0.718s, episode steps:  88, steps per second: 123, episode reward: -128.697, mean reward: -1.462 [-100.000,  8.641], mean action: 1.739 [0.000, 3.000],  loss: 44.385806, mse: 70270.974121, mean_q: 237.667036, mean_eps: 0.732861
  89176/300000: episode: 907, duration: 0.650s, episode steps:  85, steps per second: 131, episode reward: -139.224, mean reward: -1.638 [-100.000, 10.268], mean action: 1.682 [0.000, 3.000],  loss: 35.895801, mse: 69043.389982, mean_q: 235.948099, mean_eps: 0.732601
  89273/300000: episode: 908, duration: 0.786s, episode steps:  97, steps per second: 123, episode reward: -52.760, mean reward: -0.544 [-100.000, 18.644], mean action: 1.608 [0.000, 3.000],  loss: 31.834299, mse: 68515.433755, mean_q: 232.144810, mean_eps: 0.732328
  89390/300000: episode: 909, duration: 1.004s, episode steps: 117, steps per second: 116, episode reward: -41.434, mean reward: -0.354 [-100.000,  6.714], mean action: 1.692 [0.000, 3.000],  loss: 30.714089, mse: 66295.709302, mean_q: 227.198909, mean_eps: 0.732007
  89511/300000: episode: 910, duration: 0.878s, episode steps: 121, steps per second: 138, episode reward: -90.706, mean reward: -0.750 [-100.000,  7.335], mean action: 1.496 [0.000, 3.000],  loss: 37.980713, mse: 66848.123709, mean_q: 230.432942, mean_eps: 0.731650
  89598/300000: episode: 911, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -73.925, mean reward: -0.850 [-100.000, 12.356], mean action: 1.644 [0.000, 3.000],  loss: 33.528723, mse: 66768.438398, mean_q: 231.185726, mean_eps: 0.731338
  89723/300000: episode: 912, duration: 0.993s, episode steps: 125, steps per second: 126, episode reward: -59.539, mean reward: -0.476 [-100.000, 12.468], mean action: 1.744 [0.000, 3.000],  loss: 36.277474, mse: 67237.252500, mean_q: 231.494300, mean_eps: 0.731020
  89785/300000: episode: 913, duration: 0.441s, episode steps:  62, steps per second: 141, episode reward: -107.078, mean reward: -1.727 [-100.000,  6.518], mean action: 1.565 [0.000, 3.000],  loss: 39.781889, mse: 67188.419355, mean_q: 229.669684, mean_eps: 0.730739
  89875/300000: episode: 914, duration: 0.666s, episode steps:  90, steps per second: 135, episode reward: -116.791, mean reward: -1.298 [-100.000,  6.652], mean action: 1.656 [0.000, 3.000],  loss: 23.107996, mse: 68221.396788, mean_q: 231.573597, mean_eps: 0.730511
  89998/300000: episode: 915, duration: 0.958s, episode steps: 123, steps per second: 128, episode reward: -51.798, mean reward: -0.421 [-100.000, 15.910], mean action: 1.626 [0.000, 3.000],  loss: 26.263675, mse: 66448.069931, mean_q: 229.628062, mean_eps: 0.730192
  90080/300000: episode: 916, duration: 0.590s, episode steps:  82, steps per second: 139, episode reward: -30.301, mean reward: -0.370 [-100.000, 14.060], mean action: 1.549 [0.000, 3.000],  loss: 36.417314, mse: 68039.341416, mean_q: 233.795370, mean_eps: 0.729885
  90171/300000: episode: 917, duration: 0.640s, episode steps:  91, steps per second: 142, episode reward: -31.339, mean reward: -0.344 [-100.000, 12.981], mean action: 1.505 [0.000, 3.000],  loss: 30.495381, mse: 67633.484718, mean_q: 234.123751, mean_eps: 0.729625
  90265/300000: episode: 918, duration: 0.711s, episode steps:  94, steps per second: 132, episode reward: -74.788, mean reward: -0.796 [-100.000, 14.935], mean action: 1.617 [0.000, 3.000],  loss: 33.088481, mse: 65916.243850, mean_q: 229.653130, mean_eps: 0.729348
  90404/300000: episode: 919, duration: 1.003s, episode steps: 139, steps per second: 139, episode reward: -15.443, mean reward: -0.111 [-100.000, 51.868], mean action: 1.554 [0.000, 3.000],  loss: 35.029407, mse: 64604.214984, mean_q: 227.038736, mean_eps: 0.728998
  90494/300000: episode: 920, duration: 0.671s, episode steps:  90, steps per second: 134, episode reward: -122.366, mean reward: -1.360 [-100.000,  8.690], mean action: 1.756 [0.000, 3.000],  loss: 29.506736, mse: 65554.983030, mean_q: 229.901528, mean_eps: 0.728654
  90606/300000: episode: 921, duration: 0.832s, episode steps: 112, steps per second: 135, episode reward: -82.385, mean reward: -0.736 [-100.000, 14.852], mean action: 1.643 [0.000, 3.000],  loss: 40.517358, mse: 67074.981550, mean_q: 232.691654, mean_eps: 0.728351
  90727/300000: episode: 922, duration: 0.943s, episode steps: 121, steps per second: 128, episode reward: -66.707, mean reward: -0.551 [-100.000,  9.490], mean action: 1.669 [0.000, 3.000],  loss: 31.575488, mse: 64874.276537, mean_q: 227.192595, mean_eps: 0.728002
  90820/300000: episode: 923, duration: 0.731s, episode steps:  93, steps per second: 127, episode reward: -68.102, mean reward: -0.732 [-100.000, 16.641], mean action: 1.548 [0.000, 3.000],  loss: 28.250778, mse: 64066.953503, mean_q: 224.784922, mean_eps: 0.727681
  90941/300000: episode: 924, duration: 0.893s, episode steps: 121, steps per second: 135, episode reward: -95.071, mean reward: -0.786 [-100.000, 10.556], mean action: 1.678 [0.000, 3.000],  loss: 36.413620, mse: 66090.056463, mean_q: 228.269960, mean_eps: 0.727360
  91020/300000: episode: 925, duration: 0.556s, episode steps:  79, steps per second: 142, episode reward: -74.563, mean reward: -0.944 [-100.000,  9.607], mean action: 1.595 [0.000, 3.000],  loss: 41.268313, mse: 69172.720728, mean_q: 237.135831, mean_eps: 0.727060
  91193/300000: episode: 926, duration: 1.230s, episode steps: 173, steps per second: 141, episode reward: -201.621, mean reward: -1.165 [-100.000, 63.249], mean action: 1.462 [0.000, 3.000],  loss: 43.583134, mse: 68140.232727, mean_q: 233.761787, mean_eps: 0.726682
  91274/300000: episode: 927, duration: 0.636s, episode steps:  81, steps per second: 127, episode reward: -66.008, mean reward: -0.815 [-100.000,  7.507], mean action: 1.543 [0.000, 3.000],  loss: 28.078411, mse: 68643.111208, mean_q: 233.689694, mean_eps: 0.726301
  91407/300000: episode: 928, duration: 1.007s, episode steps: 133, steps per second: 132, episode reward: -50.735, mean reward: -0.381 [-100.000, 25.485], mean action: 1.662 [0.000, 3.000],  loss: 31.755977, mse: 68557.991160, mean_q: 233.307377, mean_eps: 0.725980
  91494/300000: episode: 929, duration: 0.618s, episode steps:  87, steps per second: 141, episode reward: -79.815, mean reward: -0.917 [-100.000, 10.598], mean action: 1.793 [0.000, 3.000],  loss: 39.503911, mse: 69270.008351, mean_q: 235.258789, mean_eps: 0.725650
  91609/300000: episode: 930, duration: 0.802s, episode steps: 115, steps per second: 143, episode reward: -200.821, mean reward: -1.746 [-100.000, 32.628], mean action: 1.591 [0.000, 3.000],  loss: 45.129318, mse: 70281.747452, mean_q: 238.801649, mean_eps: 0.725347
  91709/300000: episode: 931, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: -87.501, mean reward: -0.875 [-100.000, 12.196], mean action: 1.680 [0.000, 3.000],  loss: 48.661364, mse: 68963.688711, mean_q: 235.551829, mean_eps: 0.725024
  91840/300000: episode: 932, duration: 0.966s, episode steps: 131, steps per second: 136, episode reward: -23.345, mean reward: -0.178 [-100.000, 23.960], mean action: 1.603 [0.000, 3.000],  loss: 35.327901, mse: 69128.680075, mean_q: 235.266626, mean_eps: 0.724678
  91905/300000: episode: 933, duration: 0.500s, episode steps:  65, steps per second: 130, episode reward: -47.361, mean reward: -0.729 [-100.000, 14.669], mean action: 1.677 [0.000, 3.000],  loss: 39.156564, mse: 69847.888221, mean_q: 238.719858, mean_eps: 0.724384
  92021/300000: episode: 934, duration: 0.871s, episode steps: 116, steps per second: 133, episode reward: -116.110, mean reward: -1.001 [-100.000, 15.164], mean action: 1.664 [0.000, 3.000],  loss: 36.814709, mse: 68771.735789, mean_q: 234.548757, mean_eps: 0.724113
  92135/300000: episode: 935, duration: 0.821s, episode steps: 114, steps per second: 139, episode reward: -131.315, mean reward: -1.152 [-100.000, 26.193], mean action: 1.649 [0.000, 3.000],  loss: 29.133614, mse: 70717.133258, mean_q: 238.475873, mean_eps: 0.723767
  92208/300000: episode: 936, duration: 0.552s, episode steps:  73, steps per second: 132, episode reward: -46.166, mean reward: -0.632 [-100.000, 10.006], mean action: 1.740 [0.000, 3.000],  loss: 37.270446, mse: 71182.335938, mean_q: 241.818832, mean_eps: 0.723487
  92359/300000: episode: 937, duration: 1.138s, episode steps: 151, steps per second: 133, episode reward: -44.220, mean reward: -0.293 [-100.000,  9.914], mean action: 1.603 [0.000, 3.000],  loss: 29.922767, mse: 69890.137314, mean_q: 238.299889, mean_eps: 0.723151
  92450/300000: episode: 938, duration: 0.659s, episode steps:  91, steps per second: 138, episode reward: -90.443, mean reward: -0.994 [-100.000, 11.830], mean action: 1.549 [0.000, 3.000],  loss: 31.008946, mse: 70621.352764, mean_q: 241.672353, mean_eps: 0.722788
  92552/300000: episode: 939, duration: 0.773s, episode steps: 102, steps per second: 132, episode reward: -121.637, mean reward: -1.193 [-100.000,  6.571], mean action: 1.471 [0.000, 3.000],  loss: 42.959378, mse: 68330.152918, mean_q: 235.469214, mean_eps: 0.722499
  92637/300000: episode: 940, duration: 0.642s, episode steps:  85, steps per second: 132, episode reward: -11.920, mean reward: -0.140 [-100.000, 18.993], mean action: 1.729 [0.000, 3.000],  loss: 44.587867, mse: 68719.919761, mean_q: 236.323757, mean_eps: 0.722218
  92730/300000: episode: 941, duration: 0.679s, episode steps:  93, steps per second: 137, episode reward: -95.395, mean reward: -1.026 [-100.000,  6.876], mean action: 1.495 [0.000, 3.000],  loss: 32.204638, mse: 68036.544649, mean_q: 236.157319, mean_eps: 0.721951
  92818/300000: episode: 942, duration: 0.654s, episode steps:  88, steps per second: 135, episode reward: -144.495, mean reward: -1.642 [-100.000,  9.957], mean action: 1.659 [0.000, 3.000],  loss: 36.035577, mse: 68290.914773, mean_q: 235.736810, mean_eps: 0.721680
  92937/300000: episode: 943, duration: 0.928s, episode steps: 119, steps per second: 128, episode reward: -80.722, mean reward: -0.678 [-100.000, 16.415], mean action: 1.790 [0.000, 3.000],  loss: 38.415571, mse: 67989.323890, mean_q: 234.806726, mean_eps: 0.721369
  93039/300000: episode: 944, duration: 0.742s, episode steps: 102, steps per second: 138, episode reward: -100.598, mean reward: -0.986 [-100.000, 17.975], mean action: 1.716 [0.000, 3.000],  loss: 32.747924, mse: 69104.020412, mean_q: 236.744372, mean_eps: 0.721038
  93127/300000: episode: 945, duration: 0.649s, episode steps:  88, steps per second: 136, episode reward: -70.076, mean reward: -0.796 [-100.000,  7.389], mean action: 1.773 [0.000, 3.000],  loss: 44.417079, mse: 67100.726119, mean_q: 232.176798, mean_eps: 0.720753
  93254/300000: episode: 946, duration: 0.930s, episode steps: 127, steps per second: 137, episode reward: -37.173, mean reward: -0.293 [-100.000, 17.231], mean action: 1.685 [0.000, 3.000],  loss: 25.242606, mse: 68271.439745, mean_q: 234.552133, mean_eps: 0.720430
  93345/300000: episode: 947, duration: 0.638s, episode steps:  91, steps per second: 143, episode reward: -45.081, mean reward: -0.495 [-100.000, 15.486], mean action: 1.692 [0.000, 3.000],  loss: 30.743001, mse: 67069.398609, mean_q: 231.308798, mean_eps: 0.720103
  93426/300000: episode: 948, duration: 0.605s, episode steps:  81, steps per second: 134, episode reward: -92.145, mean reward: -1.138 [-100.000, 15.400], mean action: 1.667 [0.000, 3.000],  loss: 34.121468, mse: 65810.731144, mean_q: 229.007368, mean_eps: 0.719845
  93552/300000: episode: 949, duration: 0.917s, episode steps: 126, steps per second: 137, episode reward: -40.368, mean reward: -0.320 [-100.000,  7.230], mean action: 1.706 [0.000, 3.000],  loss: 40.533430, mse: 66224.111111, mean_q: 227.165797, mean_eps: 0.719535
  93632/300000: episode: 950, duration: 0.624s, episode steps:  80, steps per second: 128, episode reward: -151.123, mean reward: -1.889 [-100.000, 12.075], mean action: 1.700 [0.000, 3.000],  loss: 48.635980, mse: 67028.585059, mean_q: 230.743048, mean_eps: 0.719226
  93717/300000: episode: 951, duration: 0.692s, episode steps:  85, steps per second: 123, episode reward: -78.291, mean reward: -0.921 [-100.000, 14.637], mean action: 1.553 [0.000, 3.000],  loss: 53.896384, mse: 69186.428998, mean_q: 237.355357, mean_eps: 0.718978
  93781/300000: episode: 952, duration: 0.519s, episode steps:  64, steps per second: 123, episode reward: -78.327, mean reward: -1.224 [-100.000, 10.540], mean action: 1.844 [0.000, 3.000],  loss: 69.492442, mse: 66002.282471, mean_q: 228.589396, mean_eps: 0.718754
  93893/300000: episode: 953, duration: 0.885s, episode steps: 112, steps per second: 127, episode reward: -60.289, mean reward: -0.538 [-100.000, 35.930], mean action: 1.545 [0.000, 3.000],  loss: 30.638647, mse: 66648.837577, mean_q: 229.247345, mean_eps: 0.718490
  93963/300000: episode: 954, duration: 0.523s, episode steps:  70, steps per second: 134, episode reward: -56.012, mean reward: -0.800 [-100.000, 14.948], mean action: 1.543 [0.000, 3.000],  loss: 29.030118, mse: 65310.394308, mean_q: 227.577057, mean_eps: 0.718217
  94055/300000: episode: 955, duration: 0.664s, episode steps:  92, steps per second: 139, episode reward: -58.979, mean reward: -0.641 [-100.000, 19.126], mean action: 1.478 [0.000, 3.000],  loss: 32.155891, mse: 66250.216330, mean_q: 232.279742, mean_eps: 0.717974
  94188/300000: episode: 956, duration: 0.957s, episode steps: 133, steps per second: 139, episode reward: -55.201, mean reward: -0.415 [-100.000, 21.123], mean action: 1.609 [0.000, 3.000],  loss: 39.566025, mse: 66427.996799, mean_q: 231.193802, mean_eps: 0.717637
  94272/300000: episode: 957, duration: 0.659s, episode steps:  84, steps per second: 128, episode reward: -58.072, mean reward: -0.691 [-100.000,  8.080], mean action: 1.405 [0.000, 3.000],  loss: 44.249569, mse: 65416.383464, mean_q: 230.554448, mean_eps: 0.717312
  94386/300000: episode: 958, duration: 0.876s, episode steps: 114, steps per second: 130, episode reward: -99.321, mean reward: -0.871 [-100.000,  7.747], mean action: 1.561 [0.000, 3.000],  loss: 39.248453, mse: 66550.624041, mean_q: 231.279953, mean_eps: 0.717014
  94500/300000: episode: 959, duration: 0.846s, episode steps: 114, steps per second: 135, episode reward: -91.405, mean reward: -0.802 [-100.000,  6.038], mean action: 1.711 [0.000, 3.000],  loss: 43.290765, mse: 66186.383052, mean_q: 231.904715, mean_eps: 0.716673
  94577/300000: episode: 960, duration: 0.607s, episode steps:  77, steps per second: 127, episode reward: -156.230, mean reward: -2.029 [-100.000,  9.417], mean action: 1.558 [0.000, 3.000],  loss: 43.503842, mse: 65141.470982, mean_q: 226.631902, mean_eps: 0.716386
  94654/300000: episode: 961, duration: 0.576s, episode steps:  77, steps per second: 134, episode reward: -62.442, mean reward: -0.811 [-100.000, 11.382], mean action: 1.403 [0.000, 3.000],  loss: 26.611178, mse: 64215.188819, mean_q: 225.699238, mean_eps: 0.716155
  94758/300000: episode: 962, duration: 0.728s, episode steps: 104, steps per second: 143, episode reward: -95.575, mean reward: -0.919 [-100.000, 12.156], mean action: 1.769 [0.000, 3.000],  loss: 26.496249, mse: 63053.988694, mean_q: 221.562480, mean_eps: 0.715883
  94882/300000: episode: 963, duration: 0.922s, episode steps: 124, steps per second: 134, episode reward: -60.504, mean reward: -0.488 [-100.000,  7.575], mean action: 1.637 [0.000, 3.000],  loss: 35.393306, mse: 62377.182838, mean_q: 220.251129, mean_eps: 0.715542
  94976/300000: episode: 964, duration: 0.729s, episode steps:  94, steps per second: 129, episode reward: -116.109, mean reward: -1.235 [-100.000,  8.050], mean action: 1.755 [0.000, 3.000],  loss: 45.192570, mse: 64138.164644, mean_q: 225.126715, mean_eps: 0.715215
  95058/300000: episode: 965, duration: 0.634s, episode steps:  82, steps per second: 129, episode reward: -65.302, mean reward: -0.796 [-100.000,  9.750], mean action: 1.695 [0.000, 3.000],  loss: 25.163775, mse: 63260.118378, mean_q: 223.203755, mean_eps: 0.714951
  95145/300000: episode: 966, duration: 0.644s, episode steps:  87, steps per second: 135, episode reward: -33.766, mean reward: -0.388 [-100.000, 15.252], mean action: 1.678 [0.000, 3.000],  loss: 64.284254, mse: 62177.467268, mean_q: 220.314332, mean_eps: 0.714697
  95274/300000: episode: 967, duration: 0.914s, episode steps: 129, steps per second: 141, episode reward: -104.203, mean reward: -0.808 [-100.000,  7.156], mean action: 1.628 [0.000, 3.000],  loss: 26.254318, mse: 63284.161186, mean_q: 223.199476, mean_eps: 0.714373
  95392/300000: episode: 968, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -95.951, mean reward: -0.813 [-100.000, 11.426], mean action: 1.797 [0.000, 3.000],  loss: 32.540193, mse: 62472.273239, mean_q: 220.685535, mean_eps: 0.714002
  95565/300000: episode: 969, duration: 1.258s, episode steps: 173, steps per second: 137, episode reward: 37.604, mean reward:  0.217 [-100.000, 95.338], mean action: 1.734 [0.000, 3.000],  loss: 41.130877, mse: 61076.067784, mean_q: 217.289409, mean_eps: 0.713566
  95647/300000: episode: 970, duration: 0.601s, episode steps:  82, steps per second: 136, episode reward: -64.021, mean reward: -0.781 [-100.000, 19.787], mean action: 1.695 [0.000, 3.000],  loss: 27.519905, mse: 59828.654154, mean_q: 214.571201, mean_eps: 0.713183
  95744/300000: episode: 971, duration: 0.751s, episode steps:  97, steps per second: 129, episode reward: -71.587, mean reward: -0.738 [-100.000,  8.142], mean action: 1.691 [0.000, 3.000],  loss: 34.136847, mse: 59519.445433, mean_q: 212.003187, mean_eps: 0.712915
  95862/300000: episode: 972, duration: 0.900s, episode steps: 118, steps per second: 131, episode reward: -29.273, mean reward: -0.248 [-100.000, 16.476], mean action: 1.619 [0.000, 3.000],  loss: 29.202052, mse: 61285.001159, mean_q: 218.375140, mean_eps: 0.712592
  95940/300000: episode: 973, duration: 0.589s, episode steps:  78, steps per second: 132, episode reward: -38.570, mean reward: -0.494 [-100.000, 11.626], mean action: 1.436 [0.000, 3.000],  loss: 24.828814, mse: 61161.466146, mean_q: 218.515329, mean_eps: 0.712299
  96073/300000: episode: 974, duration: 1.028s, episode steps: 133, steps per second: 129, episode reward: -85.585, mean reward: -0.643 [-100.000, 13.321], mean action: 1.474 [0.000, 3.000],  loss: 35.258268, mse: 62488.352091, mean_q: 222.089439, mean_eps: 0.711982
  96143/300000: episode: 975, duration: 0.498s, episode steps:  70, steps per second: 140, episode reward: -90.482, mean reward: -1.293 [-100.000,  7.685], mean action: 1.657 [0.000, 3.000],  loss: 28.473352, mse: 62448.986663, mean_q: 220.927077, mean_eps: 0.711677
  96218/300000: episode: 976, duration: 0.619s, episode steps:  75, steps per second: 121, episode reward: -71.508, mean reward: -0.953 [-100.000,  7.254], mean action: 1.547 [0.000, 3.000],  loss: 30.324317, mse: 61589.841615, mean_q: 219.520617, mean_eps: 0.711460
  96294/300000: episode: 977, duration: 0.770s, episode steps:  76, steps per second:  99, episode reward: -45.260, mean reward: -0.596 [-100.000, 19.679], mean action: 1.697 [0.000, 3.000],  loss: 44.470218, mse: 62597.190481, mean_q: 223.068670, mean_eps: 0.711233
  96423/300000: episode: 978, duration: 1.089s, episode steps: 129, steps per second: 118, episode reward: -78.215, mean reward: -0.606 [-100.000, 20.825], mean action: 1.674 [0.000, 3.000],  loss: 25.948220, mse: 62139.842690, mean_q: 221.035944, mean_eps: 0.710926
  96493/300000: episode: 979, duration: 0.582s, episode steps:  70, steps per second: 120, episode reward: -95.304, mean reward: -1.361 [-100.000,  8.537], mean action: 1.700 [0.000, 3.000],  loss: 38.911542, mse: 61961.118638, mean_q: 221.196406, mean_eps: 0.710627
  96572/300000: episode: 980, duration: 0.700s, episode steps:  79, steps per second: 113, episode reward: -39.887, mean reward: -0.505 [-100.000, 21.437], mean action: 1.544 [0.000, 3.000],  loss: 37.771973, mse: 60682.678254, mean_q: 216.645954, mean_eps: 0.710404
  96682/300000: episode: 981, duration: 0.900s, episode steps: 110, steps per second: 122, episode reward: 22.747, mean reward:  0.207 [-100.000, 57.419], mean action: 1.709 [0.000, 3.000],  loss: 43.747567, mse: 61115.882564, mean_q: 218.515196, mean_eps: 0.710120
  96798/300000: episode: 982, duration: 1.115s, episode steps: 116, steps per second: 104, episode reward: -104.548, mean reward: -0.901 [-100.000,  7.201], mean action: 1.621 [0.000, 3.000],  loss: 34.315223, mse: 59611.371835, mean_q: 213.864266, mean_eps: 0.709781
  96923/300000: episode: 983, duration: 1.100s, episode steps: 125, steps per second: 114, episode reward: -56.080, mean reward: -0.449 [-100.000, 12.050], mean action: 1.432 [0.000, 3.000],  loss: 35.290952, mse: 59947.364781, mean_q: 212.975340, mean_eps: 0.709420
  97040/300000: episode: 984, duration: 0.850s, episode steps: 117, steps per second: 138, episode reward: -72.519, mean reward: -0.620 [-100.000, 12.672], mean action: 1.556 [0.000, 3.000],  loss: 34.034922, mse: 60754.529347, mean_q: 217.140805, mean_eps: 0.709057
  97164/300000: episode: 985, duration: 0.887s, episode steps: 124, steps per second: 140, episode reward: -53.088, mean reward: -0.428 [-100.000, 12.692], mean action: 1.629 [0.000, 3.000],  loss: 35.628804, mse: 59334.120243, mean_q: 212.736228, mean_eps: 0.708695
  97274/300000: episode: 986, duration: 0.841s, episode steps: 110, steps per second: 131, episode reward: -66.290, mean reward: -0.603 [-100.000, 22.891], mean action: 1.655 [0.000, 3.000],  loss: 34.153289, mse: 58234.133168, mean_q: 210.724153, mean_eps: 0.708344
  97376/300000: episode: 987, duration: 0.823s, episode steps: 102, steps per second: 124, episode reward: -8.454, mean reward: -0.083 [-100.000, 16.590], mean action: 1.657 [0.000, 3.000],  loss: 25.340607, mse: 58333.735141, mean_q: 209.957724, mean_eps: 0.708026
  97474/300000: episode: 988, duration: 0.732s, episode steps:  98, steps per second: 134, episode reward: -144.652, mean reward: -1.476 [-100.000,  5.452], mean action: 1.439 [0.000, 3.000],  loss: 51.243413, mse: 60550.032246, mean_q: 218.336161, mean_eps: 0.707727
  97583/300000: episode: 989, duration: 0.835s, episode steps: 109, steps per second: 131, episode reward: -26.918, mean reward: -0.247 [-100.000, 13.172], mean action: 1.633 [0.000, 3.000],  loss: 35.977565, mse: 58845.474484, mean_q: 213.078747, mean_eps: 0.707416
  97733/300000: episode: 990, duration: 1.220s, episode steps: 150, steps per second: 123, episode reward: -4.823, mean reward: -0.032 [-100.000, 24.220], mean action: 1.673 [0.000, 3.000],  loss: 33.393306, mse: 60886.122604, mean_q: 218.888995, mean_eps: 0.707028
  97843/300000: episode: 991, duration: 1.048s, episode steps: 110, steps per second: 105, episode reward: -147.862, mean reward: -1.344 [-100.000, 10.151], mean action: 1.491 [0.000, 3.000],  loss: 30.972033, mse: 60540.794354, mean_q: 220.105999, mean_eps: 0.706638
  97974/300000: episode: 992, duration: 1.038s, episode steps: 131, steps per second: 126, episode reward: -165.541, mean reward: -1.264 [-100.000,  8.490], mean action: 1.573 [0.000, 3.000],  loss: 28.007020, mse: 61221.120736, mean_q: 221.525970, mean_eps: 0.706276
  98373/300000: episode: 993, duration: 3.007s, episode steps: 399, steps per second: 133, episode reward: -184.418, mean reward: -0.462 [-100.000, 29.667], mean action: 1.674 [0.000, 3.000],  loss: 39.652535, mse: 61313.053787, mean_q: 220.859826, mean_eps: 0.705481
  98474/300000: episode: 994, duration: 0.741s, episode steps: 101, steps per second: 136, episode reward: -10.459, mean reward: -0.104 [-100.000, 12.769], mean action: 1.634 [0.000, 3.000],  loss: 42.048414, mse: 63822.860613, mean_q: 228.245923, mean_eps: 0.704731
  98617/300000: episode: 995, duration: 1.046s, episode steps: 143, steps per second: 137, episode reward: -86.157, mean reward: -0.602 [-100.000, 12.619], mean action: 1.573 [0.000, 3.000],  loss: 46.698538, mse: 62784.270132, mean_q: 226.098432, mean_eps: 0.704365
  98716/300000: episode: 996, duration: 0.711s, episode steps:  99, steps per second: 139, episode reward: -62.419, mean reward: -0.630 [-100.000, 12.475], mean action: 1.626 [0.000, 3.000],  loss: 42.885216, mse: 60155.581834, mean_q: 217.078315, mean_eps: 0.704002
  98834/300000: episode: 997, duration: 0.881s, episode steps: 118, steps per second: 134, episode reward: -126.732, mean reward: -1.074 [-100.000,  5.843], mean action: 1.534 [0.000, 3.000],  loss: 44.431372, mse: 60872.062103, mean_q: 219.160109, mean_eps: 0.703676
  98924/300000: episode: 998, duration: 0.759s, episode steps:  90, steps per second: 119, episode reward: -41.974, mean reward: -0.466 [-100.000, 10.261], mean action: 1.700 [0.000, 3.000],  loss: 45.789352, mse: 60893.546788, mean_q: 220.322512, mean_eps: 0.703365
  99023/300000: episode: 999, duration: 0.791s, episode steps:  99, steps per second: 125, episode reward: -73.341, mean reward: -0.741 [-100.000, 21.232], mean action: 1.727 [0.000, 3.000],  loss: 44.273163, mse: 61011.533854, mean_q: 220.005445, mean_eps: 0.703081
  99084/300000: episode: 1000, duration: 0.485s, episode steps:  61, steps per second: 126, episode reward: -106.698, mean reward: -1.749 [-100.000, 21.556], mean action: 1.557 [0.000, 3.000],  loss: 37.295328, mse: 61642.026960, mean_q: 223.693554, mean_eps: 0.702841
  99189/300000: episode: 1001, duration: 0.775s, episode steps: 105, steps per second: 135, episode reward: -89.874, mean reward: -0.856 [-100.000, 16.012], mean action: 1.571 [0.000, 3.000],  loss: 41.107401, mse: 61057.080246, mean_q: 222.806491, mean_eps: 0.702592
  99293/300000: episode: 1002, duration: 0.933s, episode steps: 104, steps per second: 111, episode reward: -63.636, mean reward: -0.612 [-100.000, 57.663], mean action: 1.635 [0.000, 3.000],  loss: 39.575979, mse: 59979.656889, mean_q: 218.515926, mean_eps: 0.702279
  99367/300000: episode: 1003, duration: 0.741s, episode steps:  74, steps per second: 100, episode reward: -113.430, mean reward: -1.533 [-100.000,  7.156], mean action: 1.595 [0.000, 3.000],  loss: 35.781252, mse: 60387.097287, mean_q: 222.290782, mean_eps: 0.702012
  99488/300000: episode: 1004, duration: 0.968s, episode steps: 121, steps per second: 125, episode reward: -12.690, mean reward: -0.105 [-100.000, 50.247], mean action: 1.727 [0.000, 3.000],  loss: 42.227642, mse: 58245.176330, mean_q: 215.455797, mean_eps: 0.701719
  99561/300000: episode: 1005, duration: 0.643s, episode steps:  73, steps per second: 113, episode reward: -89.856, mean reward: -1.231 [-100.000,  8.811], mean action: 1.712 [0.000, 3.000],  loss: 46.520935, mse: 57458.414651, mean_q: 214.388003, mean_eps: 0.701428
  99691/300000: episode: 1006, duration: 1.050s, episode steps: 130, steps per second: 124, episode reward: -62.606, mean reward: -0.482 [-100.000,  7.051], mean action: 1.715 [0.000, 3.000],  loss: 42.380976, mse: 60494.130529, mean_q: 223.147713, mean_eps: 0.701124
  99786/300000: episode: 1007, duration: 0.755s, episode steps:  95, steps per second: 126, episode reward: -81.636, mean reward: -0.859 [-100.000,  7.069], mean action: 1.632 [0.000, 3.000],  loss: 29.578035, mse: 58520.991530, mean_q: 216.166741, mean_eps: 0.700786
  99898/300000: episode: 1008, duration: 0.836s, episode steps: 112, steps per second: 134, episode reward: -127.701, mean reward: -1.140 [-100.000,  5.400], mean action: 1.777 [0.000, 3.000],  loss: 37.716224, mse: 59568.422224, mean_q: 222.051396, mean_eps: 0.700475
  99999/300000: episode: 1009, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -101.051, mean reward: -1.001 [-100.000,  6.295], mean action: 1.663 [0.000, 3.000],  loss: 68.933303, mse: 58900.089148, mean_q: 219.717562, mean_eps: 0.700156
 100093/300000: episode: 1010, duration: 0.683s, episode steps:  94, steps per second: 138, episode reward: -58.065, mean reward: -0.618 [-100.000, 12.446], mean action: 1.564 [0.000, 3.000],  loss: 38.461098, mse: 58247.690991, mean_q: 217.733711, mean_eps: 0.699864
 100207/300000: episode: 1011, duration: 0.800s, episode steps: 114, steps per second: 143, episode reward: -67.265, mean reward: -0.590 [-100.000,  9.400], mean action: 1.614 [0.000, 3.000],  loss: 32.051876, mse: 58193.826412, mean_q: 216.136738, mean_eps: 0.699552
 100305/300000: episode: 1012, duration: 0.660s, episode steps:  98, steps per second: 148, episode reward: -52.243, mean reward: -0.533 [-100.000, 14.862], mean action: 1.704 [0.000, 3.000],  loss: 45.901838, mse: 59479.797951, mean_q: 219.110595, mean_eps: 0.699234
 100392/300000: episode: 1013, duration: 0.634s, episode steps:  87, steps per second: 137, episode reward: -50.093, mean reward: -0.576 [-100.000, 12.832], mean action: 1.598 [0.000, 3.000],  loss: 43.890832, mse: 58444.205594, mean_q: 217.548376, mean_eps: 0.698956
 100474/300000: episode: 1014, duration: 0.577s, episode steps:  82, steps per second: 142, episode reward: -53.898, mean reward: -0.657 [-100.000, 12.414], mean action: 1.549 [0.000, 3.000],  loss: 45.497436, mse: 57877.313024, mean_q: 215.593659, mean_eps: 0.698702
 100592/300000: episode: 1015, duration: 0.805s, episode steps: 118, steps per second: 147, episode reward: -59.521, mean reward: -0.504 [-100.000, 10.004], mean action: 1.653 [0.000, 3.000],  loss: 35.280155, mse: 58980.336699, mean_q: 216.879595, mean_eps: 0.698402
 100708/300000: episode: 1016, duration: 0.829s, episode steps: 116, steps per second: 140, episode reward: -103.846, mean reward: -0.895 [-100.000, 14.327], mean action: 1.655 [0.000, 3.000],  loss: 45.085638, mse: 59543.451441, mean_q: 219.571301, mean_eps: 0.698052
 100824/300000: episode: 1017, duration: 0.836s, episode steps: 116, steps per second: 139, episode reward: -75.927, mean reward: -0.655 [-100.000, 18.266], mean action: 1.621 [0.000, 3.000],  loss: 53.618292, mse: 59909.596410, mean_q: 218.691678, mean_eps: 0.697704
 100921/300000: episode: 1018, duration: 0.812s, episode steps:  97, steps per second: 119, episode reward: -20.796, mean reward: -0.214 [-100.000, 13.941], mean action: 1.619 [0.000, 3.000],  loss: 44.637831, mse: 62571.160237, mean_q: 226.793830, mean_eps: 0.697384
 101038/300000: episode: 1019, duration: 1.003s, episode steps: 117, steps per second: 117, episode reward: -122.281, mean reward: -1.045 [-100.000, 10.898], mean action: 1.487 [0.000, 3.000],  loss: 47.990475, mse: 60635.896067, mean_q: 221.336877, mean_eps: 0.697063
 101152/300000: episode: 1020, duration: 0.777s, episode steps: 114, steps per second: 147, episode reward: -44.346, mean reward: -0.389 [-100.000, 21.569], mean action: 1.623 [0.000, 3.000],  loss: 47.679026, mse: 61850.775870, mean_q: 226.937152, mean_eps: 0.696716
 101298/300000: episode: 1021, duration: 1.111s, episode steps: 146, steps per second: 131, episode reward: -92.940, mean reward: -0.637 [-100.000, 10.804], mean action: 1.664 [0.000, 3.000],  loss: 37.301972, mse: 61319.264180, mean_q: 225.293464, mean_eps: 0.696326
 101414/300000: episode: 1022, duration: 0.985s, episode steps: 116, steps per second: 118, episode reward: -46.160, mean reward: -0.398 [-100.000,  5.388], mean action: 1.664 [0.000, 3.000],  loss: 48.438908, mse: 62597.998384, mean_q: 229.343711, mean_eps: 0.695933
 101478/300000: episode: 1023, duration: 0.476s, episode steps:  64, steps per second: 134, episode reward: -124.380, mean reward: -1.943 [-100.000, 14.117], mean action: 1.578 [0.000, 3.000],  loss: 53.418588, mse: 62892.951050, mean_q: 230.515250, mean_eps: 0.695663
 101620/300000: episode: 1024, duration: 1.061s, episode steps: 142, steps per second: 134, episode reward: -48.135, mean reward: -0.339 [-100.000, 27.355], mean action: 1.732 [0.000, 3.000],  loss: 41.766474, mse: 64329.929495, mean_q: 233.623929, mean_eps: 0.695354
 101711/300000: episode: 1025, duration: 0.698s, episode steps:  91, steps per second: 130, episode reward: -82.081, mean reward: -0.902 [-100.000,  8.948], mean action: 1.571 [0.000, 3.000],  loss: 61.794257, mse: 65087.994677, mean_q: 236.923849, mean_eps: 0.695005
 101781/300000: episode: 1026, duration: 0.567s, episode steps:  70, steps per second: 124, episode reward: -57.061, mean reward: -0.815 [-100.000, 15.213], mean action: 1.671 [0.000, 3.000],  loss: 52.651277, mse: 65438.974609, mean_q: 239.124232, mean_eps: 0.694763
 101883/300000: episode: 1027, duration: 0.761s, episode steps: 102, steps per second: 134, episode reward: -56.989, mean reward: -0.559 [-100.000,  9.307], mean action: 1.686 [0.000, 3.000],  loss: 54.328793, mse: 64137.417701, mean_q: 236.360780, mean_eps: 0.694505
 101970/300000: episode: 1028, duration: 0.631s, episode steps:  87, steps per second: 138, episode reward: -100.108, mean reward: -1.151 [-100.000, 11.836], mean action: 1.552 [0.000, 3.000],  loss: 50.509302, mse: 63319.740302, mean_q: 234.569284, mean_eps: 0.694222
 102078/300000: episode: 1029, duration: 0.752s, episode steps: 108, steps per second: 144, episode reward: -132.260, mean reward: -1.225 [-100.000, 14.981], mean action: 1.509 [0.000, 3.000],  loss: 48.533364, mse: 63418.103805, mean_q: 235.053757, mean_eps: 0.693929
 102158/300000: episode: 1030, duration: 0.582s, episode steps:  80, steps per second: 138, episode reward: -68.636, mean reward: -0.858 [-100.000, 22.612], mean action: 1.512 [0.000, 3.000],  loss: 65.417717, mse: 66083.824707, mean_q: 241.884204, mean_eps: 0.693647
 102256/300000: episode: 1031, duration: 0.676s, episode steps:  98, steps per second: 145, episode reward: -78.919, mean reward: -0.805 [-100.000, 12.579], mean action: 1.612 [0.000, 3.000],  loss: 39.401084, mse: 64401.760603, mean_q: 236.987937, mean_eps: 0.693381
 102366/300000: episode: 1032, duration: 0.738s, episode steps: 110, steps per second: 149, episode reward: -113.519, mean reward: -1.032 [-100.000,  6.064], mean action: 1.527 [0.000, 3.000],  loss: 55.423235, mse: 65038.073473, mean_q: 239.302506, mean_eps: 0.693068
 102483/300000: episode: 1033, duration: 0.944s, episode steps: 117, steps per second: 124, episode reward: -69.761, mean reward: -0.596 [-100.000,  8.126], mean action: 1.641 [0.000, 3.000],  loss: 48.538795, mse: 66490.840612, mean_q: 242.572201, mean_eps: 0.692728
 102622/300000: episode: 1034, duration: 0.969s, episode steps: 139, steps per second: 143, episode reward: -262.679, mean reward: -1.890 [-100.000, 67.334], mean action: 1.525 [0.000, 3.000],  loss: 61.933929, mse: 66659.684605, mean_q: 241.972176, mean_eps: 0.692344
 102721/300000: episode: 1035, duration: 0.679s, episode steps:  99, steps per second: 146, episode reward: -75.060, mean reward: -0.758 [-100.000, 28.448], mean action: 1.626 [0.000, 3.000],  loss: 56.868198, mse: 66599.865491, mean_q: 242.133979, mean_eps: 0.691987
 102886/300000: episode: 1036, duration: 1.153s, episode steps: 165, steps per second: 143, episode reward: -171.110, mean reward: -1.037 [-100.000, 47.394], mean action: 1.527 [0.000, 3.000],  loss: 39.447900, mse: 66779.873509, mean_q: 242.031870, mean_eps: 0.691591
 102999/300000: episode: 1037, duration: 0.776s, episode steps: 113, steps per second: 146, episode reward: -67.165, mean reward: -0.594 [-100.000, 17.245], mean action: 1.451 [0.000, 3.000],  loss: 47.926218, mse: 68275.148887, mean_q: 245.028445, mean_eps: 0.691174
 103111/300000: episode: 1038, duration: 0.809s, episode steps: 112, steps per second: 138, episode reward: -78.650, mean reward: -0.702 [-100.000, 16.712], mean action: 1.723 [0.000, 3.000],  loss: 59.887879, mse: 69109.056117, mean_q: 244.475400, mean_eps: 0.690836
 103228/300000: episode: 1039, duration: 0.987s, episode steps: 117, steps per second: 119, episode reward: -38.168, mean reward: -0.326 [-100.000, 17.346], mean action: 1.581 [0.000, 3.000],  loss: 73.040583, mse: 69987.702457, mean_q: 250.149993, mean_eps: 0.690493
 103346/300000: episode: 1040, duration: 1.142s, episode steps: 118, steps per second: 103, episode reward: -37.176, mean reward: -0.315 [-100.000, 10.683], mean action: 1.610 [0.000, 3.000],  loss: 69.128449, mse: 69624.547041, mean_q: 249.337027, mean_eps: 0.690141
 103455/300000: episode: 1041, duration: 1.046s, episode steps: 109, steps per second: 104, episode reward: -69.622, mean reward: -0.639 [-100.000,  9.856], mean action: 1.697 [0.000, 3.000],  loss: 42.860404, mse: 69865.762686, mean_q: 249.883851, mean_eps: 0.689800
 103537/300000: episode: 1042, duration: 0.847s, episode steps:  82, steps per second:  97, episode reward: -131.663, mean reward: -1.606 [-100.000,  6.450], mean action: 1.634 [0.000, 3.000],  loss: 61.760477, mse: 68125.462986, mean_q: 245.558886, mean_eps: 0.689514
 103658/300000: episode: 1043, duration: 1.032s, episode steps: 121, steps per second: 117, episode reward: -72.850, mean reward: -0.602 [-100.000, 12.679], mean action: 1.570 [0.000, 3.000],  loss: 48.371168, mse: 69231.510653, mean_q: 247.422364, mean_eps: 0.689209
 103774/300000: episode: 1044, duration: 0.960s, episode steps: 116, steps per second: 121, episode reward: -438.846, mean reward: -3.783 [-100.000, 72.266], mean action: 1.638 [0.000, 3.000],  loss: 67.630232, mse: 70183.544619, mean_q: 249.945132, mean_eps: 0.688854
 103895/300000: episode: 1045, duration: 1.183s, episode steps: 121, steps per second: 102, episode reward: -11.275, mean reward: -0.093 [-100.000, 13.826], mean action: 1.686 [0.000, 3.000],  loss: 51.855828, mse: 70903.352725, mean_q: 251.469226, mean_eps: 0.688498
 104011/300000: episode: 1046, duration: 1.073s, episode steps: 116, steps per second: 108, episode reward: -90.689, mean reward: -0.782 [-100.000,  8.039], mean action: 1.681 [0.000, 3.000],  loss: 47.840745, mse: 71772.352303, mean_q: 255.907426, mean_eps: 0.688143
 104161/300000: episode: 1047, duration: 1.374s, episode steps: 150, steps per second: 109, episode reward: -140.975, mean reward: -0.940 [-100.000,  5.092], mean action: 1.860 [0.000, 3.000],  loss: 51.725188, mse: 70980.087526, mean_q: 251.495256, mean_eps: 0.687743
 104290/300000: episode: 1048, duration: 1.211s, episode steps: 129, steps per second: 106, episode reward: -42.467, mean reward: -0.329 [-100.000,  8.274], mean action: 1.543 [0.000, 3.000],  loss: 49.610985, mse: 71386.653464, mean_q: 253.274701, mean_eps: 0.687325
 104413/300000: episode: 1049, duration: 0.950s, episode steps: 123, steps per second: 129, episode reward: -2.035, mean reward: -0.017 [-100.000, 17.720], mean action: 1.642 [0.000, 3.000],  loss: 38.929019, mse: 70978.392753, mean_q: 251.204506, mean_eps: 0.686947
 104550/300000: episode: 1050, duration: 1.279s, episode steps: 137, steps per second: 107, episode reward: -53.217, mean reward: -0.388 [-100.000, 12.657], mean action: 1.569 [0.000, 3.000],  loss: 39.059823, mse: 73373.513258, mean_q: 257.362220, mean_eps: 0.686557
 104697/300000: episode: 1051, duration: 1.641s, episode steps: 147, steps per second:  90, episode reward: -134.734, mean reward: -0.917 [-100.000, 11.048], mean action: 1.653 [0.000, 3.000],  loss: 50.279805, mse: 74620.238175, mean_q: 260.417852, mean_eps: 0.686131
 104828/300000: episode: 1052, duration: 1.286s, episode steps: 131, steps per second: 102, episode reward: -89.331, mean reward: -0.682 [-100.000, 13.793], mean action: 1.641 [0.000, 3.000],  loss: 59.129907, mse: 73988.343034, mean_q: 258.388480, mean_eps: 0.685714
 104957/300000: episode: 1053, duration: 1.033s, episode steps: 129, steps per second: 125, episode reward:  3.800, mean reward:  0.029 [-100.000, 22.029], mean action: 1.605 [0.000, 3.000],  loss: 37.872191, mse: 75299.521651, mean_q: 262.291227, mean_eps: 0.685324
 105040/300000: episode: 1054, duration: 0.642s, episode steps:  83, steps per second: 129, episode reward: -14.729, mean reward: -0.177 [-100.000, 16.811], mean action: 1.663 [0.000, 3.000],  loss: 53.280100, mse: 75534.479810, mean_q: 261.310924, mean_eps: 0.685006
 105141/300000: episode: 1055, duration: 0.751s, episode steps: 101, steps per second: 135, episode reward: -58.534, mean reward: -0.580 [-100.000, 19.633], mean action: 1.683 [0.000, 3.000],  loss: 58.181224, mse: 75728.881846, mean_q: 261.716690, mean_eps: 0.684730
 105252/300000: episode: 1056, duration: 1.005s, episode steps: 111, steps per second: 110, episode reward: -31.859, mean reward: -0.287 [-100.000, 20.224], mean action: 1.495 [0.000, 3.000],  loss: 61.572112, mse: 74735.347832, mean_q: 260.137146, mean_eps: 0.684412
 105350/300000: episode: 1057, duration: 0.928s, episode steps:  98, steps per second: 106, episode reward: 16.456, mean reward:  0.168 [-100.000, 123.973], mean action: 1.776 [0.000, 3.000],  loss: 44.962169, mse: 74516.614158, mean_q: 260.855933, mean_eps: 0.684098
 105446/300000: episode: 1058, duration: 0.882s, episode steps:  96, steps per second: 109, episode reward: -327.010, mean reward: -3.406 [-100.000,  2.781], mean action: 1.656 [0.000, 3.000],  loss: 34.845544, mse: 75923.056681, mean_q: 262.447875, mean_eps: 0.683808
 105515/300000: episode: 1059, duration: 0.669s, episode steps:  69, steps per second: 103, episode reward: -47.701, mean reward: -0.691 [-100.000, 15.077], mean action: 1.609 [0.000, 3.000],  loss: 46.226575, mse: 73616.160553, mean_q: 258.695004, mean_eps: 0.683560
 105626/300000: episode: 1060, duration: 1.002s, episode steps: 111, steps per second: 111, episode reward: -118.468, mean reward: -1.067 [-100.000, 19.221], mean action: 1.405 [0.000, 3.000],  loss: 50.157866, mse: 74634.115885, mean_q: 260.159819, mean_eps: 0.683290
 105750/300000: episode: 1061, duration: 1.185s, episode steps: 124, steps per second: 105, episode reward: -56.399, mean reward: -0.455 [-100.000, 10.893], mean action: 1.677 [0.000, 3.000],  loss: 73.392445, mse: 75454.612431, mean_q: 260.679814, mean_eps: 0.682938
 105869/300000: episode: 1062, duration: 1.165s, episode steps: 119, steps per second: 102, episode reward: -76.073, mean reward: -0.639 [-100.000, 35.567], mean action: 1.681 [0.000, 3.000],  loss: 56.925123, mse: 77645.011292, mean_q: 265.881245, mean_eps: 0.682573
 105993/300000: episode: 1063, duration: 1.108s, episode steps: 124, steps per second: 112, episode reward: -41.846, mean reward: -0.337 [-100.000, 11.170], mean action: 1.815 [0.000, 3.000],  loss: 116.833717, mse: 79762.598444, mean_q: 272.002706, mean_eps: 0.682209
 106102/300000: episode: 1064, duration: 0.903s, episode steps: 109, steps per second: 121, episode reward: -84.941, mean reward: -0.779 [-100.000, 10.874], mean action: 1.633 [0.000, 3.000],  loss: 56.611812, mse: 81475.865145, mean_q: 274.349245, mean_eps: 0.681859
 106194/300000: episode: 1065, duration: 0.715s, episode steps:  92, steps per second: 129, episode reward: -89.834, mean reward: -0.976 [-100.000,  9.661], mean action: 1.620 [0.000, 3.000],  loss: 70.277646, mse: 81566.888120, mean_q: 274.573962, mean_eps: 0.681557
 106310/300000: episode: 1066, duration: 0.804s, episode steps: 116, steps per second: 144, episode reward: -94.786, mean reward: -0.817 [-100.000, 13.465], mean action: 1.681 [0.000, 3.000],  loss: 60.046820, mse: 79395.236699, mean_q: 269.182405, mean_eps: 0.681246
 106435/300000: episode: 1067, duration: 0.847s, episode steps: 125, steps per second: 148, episode reward: -79.552, mean reward: -0.636 [-100.000,  8.465], mean action: 1.520 [0.000, 3.000],  loss: 115.628878, mse: 80792.332156, mean_q: 272.293207, mean_eps: 0.680884
 106537/300000: episode: 1068, duration: 0.730s, episode steps: 102, steps per second: 140, episode reward: -85.546, mean reward: -0.839 [-100.000,  6.783], mean action: 1.676 [0.000, 3.000],  loss: 119.975139, mse: 82679.361520, mean_q: 276.239301, mean_eps: 0.680544
 106616/300000: episode: 1069, duration: 0.538s, episode steps:  79, steps per second: 147, episode reward: -141.110, mean reward: -1.786 [-100.000,  7.831], mean action: 1.418 [0.000, 3.000],  loss: 136.295737, mse: 80737.747429, mean_q: 274.076722, mean_eps: 0.680272
 106678/300000: episode: 1070, duration: 0.431s, episode steps:  62, steps per second: 144, episode reward: -63.442, mean reward: -1.023 [-100.000, 16.253], mean action: 1.710 [0.000, 3.000],  loss: 48.440384, mse: 82183.029864, mean_q: 277.136987, mean_eps: 0.680060
 106773/300000: episode: 1071, duration: 0.661s, episode steps:  95, steps per second: 144, episode reward: -116.164, mean reward: -1.223 [-100.000, 11.185], mean action: 1.684 [0.000, 3.000],  loss: 46.666343, mse: 83359.107771, mean_q: 281.466227, mean_eps: 0.679825
 106925/300000: episode: 1072, duration: 1.102s, episode steps: 152, steps per second: 138, episode reward: -83.074, mean reward: -0.547 [-100.000, 11.184], mean action: 1.638 [0.000, 3.000],  loss: 50.622596, mse: 83607.944310, mean_q: 280.457484, mean_eps: 0.679454
 107037/300000: episode: 1073, duration: 0.755s, episode steps: 112, steps per second: 148, episode reward: -29.403, mean reward: -0.263 [-100.000, 15.463], mean action: 1.527 [0.000, 3.000],  loss: 45.950821, mse: 86237.925991, mean_q: 285.180644, mean_eps: 0.679059
 107134/300000: episode: 1074, duration: 0.686s, episode steps:  97, steps per second: 141, episode reward: -122.016, mean reward: -1.258 [-100.000,  9.031], mean action: 1.515 [0.000, 3.000],  loss: 60.911097, mse: 87656.724468, mean_q: 289.450364, mean_eps: 0.678745
 107205/300000: episode: 1075, duration: 0.484s, episode steps:  71, steps per second: 147, episode reward: -89.563, mean reward: -1.261 [-100.000, 18.260], mean action: 1.901 [0.000, 3.000],  loss: 70.572070, mse: 88739.520577, mean_q: 289.000347, mean_eps: 0.678493
 107320/300000: episode: 1076, duration: 0.795s, episode steps: 115, steps per second: 145, episode reward: -163.861, mean reward: -1.425 [-100.000, 10.407], mean action: 1.626 [0.000, 3.000],  loss: 55.321930, mse: 87753.428057, mean_q: 286.295260, mean_eps: 0.678214
 107425/300000: episode: 1077, duration: 0.761s, episode steps: 105, steps per second: 138, episode reward: -72.111, mean reward: -0.687 [-100.000, 12.546], mean action: 1.714 [0.000, 3.000],  loss: 38.658695, mse: 87760.575186, mean_q: 289.711638, mean_eps: 0.677884
 107533/300000: episode: 1078, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -156.817, mean reward: -1.452 [-100.000, 19.581], mean action: 1.417 [0.000, 3.000],  loss: 61.680695, mse: 86085.598814, mean_q: 284.087609, mean_eps: 0.677564
 107662/300000: episode: 1079, duration: 0.897s, episode steps: 129, steps per second: 144, episode reward: -45.447, mean reward: -0.352 [-100.000, 13.491], mean action: 1.690 [0.000, 3.000],  loss: 71.627790, mse: 86359.589208, mean_q: 286.848086, mean_eps: 0.677209
 107744/300000: episode: 1080, duration: 0.588s, episode steps:  82, steps per second: 139, episode reward: -90.862, mean reward: -1.108 [-100.000, 32.671], mean action: 1.598 [0.000, 3.000],  loss: 48.483487, mse: 87228.314453, mean_q: 289.291181, mean_eps: 0.676893
 107896/300000: episode: 1081, duration: 1.044s, episode steps: 152, steps per second: 146, episode reward: -56.206, mean reward: -0.370 [-100.000,  8.720], mean action: 1.717 [0.000, 3.000],  loss: 61.113364, mse: 89588.057874, mean_q: 293.913061, mean_eps: 0.676542
 108008/300000: episode: 1082, duration: 0.781s, episode steps: 112, steps per second: 143, episode reward: -54.803, mean reward: -0.489 [-100.000, 16.989], mean action: 1.482 [0.000, 3.000],  loss: 76.462804, mse: 90107.660017, mean_q: 292.274938, mean_eps: 0.676145
 108141/300000: episode: 1083, duration: 0.947s, episode steps: 133, steps per second: 140, episode reward: -58.872, mean reward: -0.443 [-100.000,  6.766], mean action: 1.556 [0.000, 3.000],  loss: 57.969460, mse: 93629.145618, mean_q: 301.152147, mean_eps: 0.675778
 108269/300000: episode: 1084, duration: 0.894s, episode steps: 128, steps per second: 143, episode reward: -46.944, mean reward: -0.367 [-100.000, 14.238], mean action: 1.625 [0.000, 3.000],  loss: 118.974655, mse: 93077.208954, mean_q: 300.620399, mean_eps: 0.675387
 108382/300000: episode: 1085, duration: 0.811s, episode steps: 113, steps per second: 139, episode reward: -125.902, mean reward: -1.114 [-100.000, 11.535], mean action: 1.841 [0.000, 3.000],  loss: 121.850249, mse: 92729.080268, mean_q: 298.505877, mean_eps: 0.675025
 108494/300000: episode: 1086, duration: 0.759s, episode steps: 112, steps per second: 148, episode reward: -74.516, mean reward: -0.665 [-100.000,  8.627], mean action: 1.518 [0.000, 3.000],  loss: 67.505433, mse: 96379.311733, mean_q: 308.122510, mean_eps: 0.674687
 108616/300000: episode: 1087, duration: 0.878s, episode steps: 122, steps per second: 139, episode reward: -29.835, mean reward: -0.245 [-100.000, 20.045], mean action: 1.516 [0.000, 3.000],  loss: 52.349201, mse: 95346.517802, mean_q: 304.965633, mean_eps: 0.674337
 108769/300000: episode: 1088, duration: 1.057s, episode steps: 153, steps per second: 145, episode reward: -88.706, mean reward: -0.580 [-100.000, 12.713], mean action: 1.745 [0.000, 3.000],  loss: 62.296538, mse: 97124.175143, mean_q: 306.918492, mean_eps: 0.673924
 108870/300000: episode: 1089, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: -166.938, mean reward: -1.653 [-100.000,  1.819], mean action: 1.653 [0.000, 3.000],  loss: 60.793842, mse: 98075.717667, mean_q: 305.906297, mean_eps: 0.673543
 108968/300000: episode: 1090, duration: 0.703s, episode steps:  98, steps per second: 139, episode reward: -50.907, mean reward: -0.519 [-100.000,  8.190], mean action: 1.643 [0.000, 3.000],  loss: 125.651749, mse: 99204.849330, mean_q: 309.700769, mean_eps: 0.673244
 109101/300000: episode: 1091, duration: 0.908s, episode steps: 133, steps per second: 146, episode reward: -146.036, mean reward: -1.098 [-100.000, 10.498], mean action: 1.594 [0.000, 3.000],  loss: 54.845067, mse: 100264.003289, mean_q: 313.102421, mean_eps: 0.672898
 109175/300000: episode: 1092, duration: 0.498s, episode steps:  74, steps per second: 148, episode reward: -75.287, mean reward: -1.017 [-100.000,  8.838], mean action: 1.703 [0.000, 3.000],  loss: 62.625747, mse: 103135.483636, mean_q: 316.510171, mean_eps: 0.672587
 109310/300000: episode: 1093, duration: 0.964s, episode steps: 135, steps per second: 140, episode reward: -128.089, mean reward: -0.949 [-100.000, 10.904], mean action: 1.659 [0.000, 3.000],  loss: 126.341143, mse: 101385.907755, mean_q: 314.942394, mean_eps: 0.672274
 109389/300000: episode: 1094, duration: 0.538s, episode steps:  79, steps per second: 147, episode reward: -98.315, mean reward: -1.244 [-100.000, 11.950], mean action: 1.823 [0.000, 3.000],  loss: 64.080259, mse: 100598.231309, mean_q: 311.815094, mean_eps: 0.671953
 109462/300000: episode: 1095, duration: 0.497s, episode steps:  73, steps per second: 147, episode reward: -78.178, mean reward: -1.071 [-100.000,  8.132], mean action: 1.726 [0.000, 3.000],  loss: 81.023779, mse: 101561.112265, mean_q: 312.984321, mean_eps: 0.671725
 109571/300000: episode: 1096, duration: 0.783s, episode steps: 109, steps per second: 139, episode reward: -103.597, mean reward: -0.950 [-100.000, 17.258], mean action: 1.688 [0.000, 3.000],  loss: 109.173525, mse: 103674.474556, mean_q: 317.113553, mean_eps: 0.671452
 109704/300000: episode: 1097, duration: 0.913s, episode steps: 133, steps per second: 146, episode reward: -85.123, mean reward: -0.640 [-100.000,  9.278], mean action: 1.684 [0.000, 3.000],  loss: 50.837775, mse: 101403.926633, mean_q: 311.259973, mean_eps: 0.671089
 109809/300000: episode: 1098, duration: 0.711s, episode steps: 105, steps per second: 148, episode reward: -86.295, mean reward: -0.822 [-100.000, 11.069], mean action: 1.467 [0.000, 3.000],  loss: 51.868807, mse: 101983.019420, mean_q: 310.835068, mean_eps: 0.670732
 109925/300000: episode: 1099, duration: 0.824s, episode steps: 116, steps per second: 141, episode reward: -130.620, mean reward: -1.126 [-100.000,  8.435], mean action: 1.716 [0.000, 3.000],  loss: 60.398423, mse: 103818.235318, mean_q: 315.400792, mean_eps: 0.670400
 110020/300000: episode: 1100, duration: 0.642s, episode steps:  95, steps per second: 148, episode reward: -81.669, mean reward: -0.860 [-100.000,  9.251], mean action: 1.642 [0.000, 3.000],  loss: 113.298987, mse: 105616.954441, mean_q: 318.682669, mean_eps: 0.670084
 110131/300000: episode: 1101, duration: 0.753s, episode steps: 111, steps per second: 147, episode reward: -85.599, mean reward: -0.771 [-100.000, 10.785], mean action: 1.784 [0.000, 3.000],  loss: 67.168614, mse: 104950.828125, mean_q: 317.345186, mean_eps: 0.669775
 110235/300000: episode: 1102, duration: 0.741s, episode steps: 104, steps per second: 140, episode reward: -101.687, mean reward: -0.978 [-100.000,  9.866], mean action: 1.452 [0.000, 3.000],  loss: 108.943715, mse: 106600.982197, mean_q: 322.455042, mean_eps: 0.669453
 110342/300000: episode: 1103, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -164.942, mean reward: -1.542 [-100.000,  2.446], mean action: 1.729 [0.000, 3.000],  loss: 65.792108, mse: 104458.328271, mean_q: 314.538620, mean_eps: 0.669136
 110465/300000: episode: 1104, duration: 0.816s, episode steps: 123, steps per second: 151, episode reward: -166.376, mean reward: -1.353 [-100.000,  2.838], mean action: 1.585 [0.000, 3.000],  loss: 54.064347, mse: 105941.981707, mean_q: 318.644172, mean_eps: 0.668791
 110594/300000: episode: 1105, duration: 0.914s, episode steps: 129, steps per second: 141, episode reward: -187.342, mean reward: -1.452 [-100.000,  2.652], mean action: 1.690 [0.000, 3.000],  loss: 93.313591, mse: 108808.092539, mean_q: 322.797216, mean_eps: 0.668413
 110728/300000: episode: 1106, duration: 0.903s, episode steps: 134, steps per second: 148, episode reward: -51.588, mean reward: -0.385 [-100.000,  6.938], mean action: 1.716 [0.000, 3.000],  loss: 106.624115, mse: 109700.865847, mean_q: 323.985281, mean_eps: 0.668018
 110825/300000: episode: 1107, duration: 0.664s, episode steps:  97, steps per second: 146, episode reward: -8.299, mean reward: -0.086 [-100.000,  9.734], mean action: 1.649 [0.000, 3.000],  loss: 123.730578, mse: 110905.397632, mean_q: 326.419773, mean_eps: 0.667672
 110918/300000: episode: 1108, duration: 0.657s, episode steps:  93, steps per second: 142, episode reward: -96.498, mean reward: -1.038 [-100.000, 13.304], mean action: 1.591 [0.000, 3.000],  loss: 112.089654, mse: 111127.320733, mean_q: 328.915542, mean_eps: 0.667387
 111048/300000: episode: 1109, duration: 0.911s, episode steps: 130, steps per second: 143, episode reward: -58.310, mean reward: -0.449 [-100.000, 10.110], mean action: 1.662 [0.000, 3.000],  loss: 157.489903, mse: 112213.820373, mean_q: 329.257213, mean_eps: 0.667053
 111158/300000: episode: 1110, duration: 0.768s, episode steps: 110, steps per second: 143, episode reward: -55.302, mean reward: -0.503 [-100.000,  7.854], mean action: 1.682 [0.000, 3.000],  loss: 134.903563, mse: 114250.819673, mean_q: 331.138548, mean_eps: 0.666693
 111275/300000: episode: 1111, duration: 0.840s, episode steps: 117, steps per second: 139, episode reward: -73.465, mean reward: -0.628 [-100.000, 12.320], mean action: 1.675 [0.000, 3.000],  loss: 76.041833, mse: 112911.156116, mean_q: 328.186324, mean_eps: 0.666352
 111373/300000: episode: 1112, duration: 0.660s, episode steps:  98, steps per second: 149, episode reward: -72.623, mean reward: -0.741 [-100.000, 10.705], mean action: 1.714 [0.000, 3.000],  loss: 129.246124, mse: 110786.858897, mean_q: 325.905932, mean_eps: 0.666030
 111459/300000: episode: 1113, duration: 0.624s, episode steps:  86, steps per second: 138, episode reward: -84.377, mean reward: -0.981 [-100.000, 17.293], mean action: 1.709 [0.000, 3.000],  loss: 141.435697, mse: 109036.818405, mean_q: 324.046216, mean_eps: 0.665754
 111536/300000: episode: 1114, duration: 0.535s, episode steps:  77, steps per second: 144, episode reward: -56.274, mean reward: -0.731 [-100.000,  9.073], mean action: 1.688 [0.000, 3.000],  loss: 104.394754, mse: 107862.436485, mean_q: 321.601022, mean_eps: 0.665509
 111678/300000: episode: 1115, duration: 0.956s, episode steps: 142, steps per second: 148, episode reward: -12.860, mean reward: -0.091 [-100.000, 13.213], mean action: 1.641 [0.000, 3.000],  loss: 64.556215, mse: 110637.767055, mean_q: 328.812412, mean_eps: 0.665180
 111749/300000: episode: 1116, duration: 0.494s, episode steps:  71, steps per second: 144, episode reward: -125.223, mean reward: -1.764 [-100.000,  8.446], mean action: 1.676 [0.000, 3.000],  loss: 53.022357, mse: 111296.329335, mean_q: 328.502477, mean_eps: 0.664861
 111826/300000: episode: 1117, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -58.361, mean reward: -0.758 [-100.000,  7.163], mean action: 1.701 [0.000, 3.000],  loss: 78.412910, mse: 110591.474736, mean_q: 327.510246, mean_eps: 0.664639
 111906/300000: episode: 1118, duration: 0.548s, episode steps:  80, steps per second: 146, episode reward: -47.850, mean reward: -0.598 [-100.000, 19.680], mean action: 1.462 [0.000, 3.000],  loss: 91.567746, mse: 112220.216602, mean_q: 331.251142, mean_eps: 0.664404
 111982/300000: episode: 1119, duration: 0.522s, episode steps:  76, steps per second: 146, episode reward: -55.131, mean reward: -0.725 [-100.000,  8.499], mean action: 1.671 [0.000, 3.000],  loss: 76.208201, mse: 112631.914474, mean_q: 332.206367, mean_eps: 0.664169
 112093/300000: episode: 1120, duration: 0.781s, episode steps: 111, steps per second: 142, episode reward: -227.263, mean reward: -2.047 [-100.000,  1.102], mean action: 1.820 [0.000, 3.000],  loss: 76.267841, mse: 112653.423072, mean_q: 331.222124, mean_eps: 0.663889
 112196/300000: episode: 1121, duration: 0.695s, episode steps: 103, steps per second: 148, episode reward: -111.219, mean reward: -1.080 [-100.000,  7.875], mean action: 1.583 [0.000, 3.000],  loss: 68.144251, mse: 112183.099970, mean_q: 331.377942, mean_eps: 0.663568
 112271/300000: episode: 1122, duration: 0.511s, episode steps:  75, steps per second: 147, episode reward: -49.953, mean reward: -0.666 [-100.000,  8.339], mean action: 1.587 [0.000, 3.000],  loss: 65.239590, mse: 111271.188021, mean_q: 328.704875, mean_eps: 0.663301
 112413/300000: episode: 1123, duration: 0.994s, episode steps: 142, steps per second: 143, episode reward: -29.924, mean reward: -0.211 [-100.000,  9.037], mean action: 1.570 [0.000, 3.000],  loss: 215.612549, mse: 112556.210277, mean_q: 334.390013, mean_eps: 0.662976
 112535/300000: episode: 1124, duration: 0.823s, episode steps: 122, steps per second: 148, episode reward: -14.651, mean reward: -0.120 [-100.000, 10.668], mean action: 1.713 [0.000, 3.000],  loss: 126.044658, mse: 112746.836834, mean_q: 332.922971, mean_eps: 0.662579
 112634/300000: episode: 1125, duration: 0.678s, episode steps:  99, steps per second: 146, episode reward: -93.203, mean reward: -0.941 [-100.000,  5.637], mean action: 1.727 [0.000, 3.000],  loss: 94.358022, mse: 113196.864978, mean_q: 335.698629, mean_eps: 0.662248
 112735/300000: episode: 1126, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: -37.488, mean reward: -0.371 [-100.000, 14.201], mean action: 1.564 [0.000, 3.000],  loss: 155.223660, mse: 114961.167621, mean_q: 338.258930, mean_eps: 0.661948
 112833/300000: episode: 1127, duration: 0.666s, episode steps:  98, steps per second: 147, episode reward: -119.082, mean reward: -1.215 [-100.000,  6.206], mean action: 1.561 [0.000, 3.000],  loss: 123.831136, mse: 113143.035475, mean_q: 334.245738, mean_eps: 0.661649
 112941/300000: episode: 1128, duration: 0.735s, episode steps: 108, steps per second: 147, episode reward: -42.181, mean reward: -0.391 [-100.000,  9.887], mean action: 1.593 [0.000, 3.000],  loss: 67.706567, mse: 115759.897425, mean_q: 340.657489, mean_eps: 0.661340
 113061/300000: episode: 1129, duration: 0.879s, episode steps: 120, steps per second: 136, episode reward: -150.546, mean reward: -1.255 [-100.000,  4.596], mean action: 1.692 [0.000, 3.000],  loss: 70.550895, mse: 112574.868099, mean_q: 331.951761, mean_eps: 0.660998
 113193/300000: episode: 1130, duration: 0.931s, episode steps: 132, steps per second: 142, episode reward: -43.486, mean reward: -0.329 [-100.000, 11.387], mean action: 1.750 [0.000, 3.000],  loss: 61.664684, mse: 116788.409860, mean_q: 339.793950, mean_eps: 0.660620
 113280/300000: episode: 1131, duration: 0.597s, episode steps:  87, steps per second: 146, episode reward: -75.446, mean reward: -0.867 [-100.000, 11.585], mean action: 1.724 [0.000, 3.000],  loss: 59.248578, mse: 115916.200700, mean_q: 338.911203, mean_eps: 0.660292
 113417/300000: episode: 1132, duration: 0.968s, episode steps: 137, steps per second: 142, episode reward: -3.614, mean reward: -0.026 [-100.000, 12.588], mean action: 1.606 [0.000, 3.000],  loss: 111.614403, mse: 120396.364165, mean_q: 348.029774, mean_eps: 0.659956
 113537/300000: episode: 1133, duration: 0.844s, episode steps: 120, steps per second: 142, episode reward: -86.937, mean reward: -0.724 [-100.000, 22.468], mean action: 1.617 [0.000, 3.000],  loss: 134.612780, mse: 116867.086589, mean_q: 337.900033, mean_eps: 0.659570
 113678/300000: episode: 1134, duration: 1.217s, episode steps: 141, steps per second: 116, episode reward: -16.834, mean reward: -0.119 [-100.000, 17.130], mean action: 1.660 [0.000, 3.000],  loss: 73.391967, mse: 120799.201241, mean_q: 345.240445, mean_eps: 0.659179
 113771/300000: episode: 1135, duration: 0.725s, episode steps:  93, steps per second: 128, episode reward: -27.540, mean reward: -0.296 [-100.000, 10.467], mean action: 1.699 [0.000, 3.000],  loss: 102.951779, mse: 124582.426663, mean_q: 354.008720, mean_eps: 0.658828
 113887/300000: episode: 1136, duration: 0.944s, episode steps: 116, steps per second: 123, episode reward: -190.408, mean reward: -1.641 [-100.000,  1.578], mean action: 1.784 [0.000, 3.000],  loss: 83.937961, mse: 121741.600956, mean_q: 345.773089, mean_eps: 0.658515
 114020/300000: episode: 1137, duration: 1.100s, episode steps: 133, steps per second: 121, episode reward: -88.869, mean reward: -0.668 [-100.000, 25.480], mean action: 1.624 [0.000, 3.000],  loss: 89.674283, mse: 119977.363898, mean_q: 343.754281, mean_eps: 0.658141
 114108/300000: episode: 1138, duration: 0.794s, episode steps:  88, steps per second: 111, episode reward: -78.253, mean reward: -0.889 [-100.000, 19.703], mean action: 1.648 [0.000, 3.000],  loss: 65.526390, mse: 121237.947088, mean_q: 348.189848, mean_eps: 0.657810
 114219/300000: episode: 1139, duration: 0.827s, episode steps: 111, steps per second: 134, episode reward: -128.713, mean reward: -1.160 [-100.000,  7.528], mean action: 1.649 [0.000, 3.000],  loss: 147.767988, mse: 118536.220721, mean_q: 343.823836, mean_eps: 0.657511
 114377/300000: episode: 1140, duration: 1.073s, episode steps: 158, steps per second: 147, episode reward: -34.256, mean reward: -0.217 [-100.000, 16.706], mean action: 1.646 [0.000, 3.000],  loss: 169.656135, mse: 117717.329015, mean_q: 341.276035, mean_eps: 0.657107
 114484/300000: episode: 1141, duration: 0.754s, episode steps: 107, steps per second: 142, episode reward: -90.174, mean reward: -0.843 [-100.000, 10.193], mean action: 1.748 [0.000, 3.000],  loss: 145.624535, mse: 117908.100394, mean_q: 340.237523, mean_eps: 0.656710
 114590/300000: episode: 1142, duration: 0.734s, episode steps: 106, steps per second: 144, episode reward: -172.619, mean reward: -1.628 [-100.000,  2.702], mean action: 1.840 [0.000, 3.000],  loss: 88.702123, mse: 119853.611218, mean_q: 343.703320, mean_eps: 0.656390
 114686/300000: episode: 1143, duration: 0.640s, episode steps:  96, steps per second: 150, episode reward: -112.065, mean reward: -1.167 [-100.000,  9.459], mean action: 1.615 [0.000, 3.000],  loss: 53.460526, mse: 120230.727702, mean_q: 345.066182, mean_eps: 0.656088
 114812/300000: episode: 1144, duration: 0.869s, episode steps: 126, steps per second: 145, episode reward: -27.586, mean reward: -0.219 [-100.000, 11.739], mean action: 1.635 [0.000, 3.000],  loss: 94.130956, mse: 121003.113715, mean_q: 345.944938, mean_eps: 0.655755
 114894/300000: episode: 1145, duration: 0.553s, episode steps:  82, steps per second: 148, episode reward: -304.475, mean reward: -3.713 [-100.000,  0.593], mean action: 1.732 [0.000, 3.000],  loss: 64.236520, mse: 116748.794874, mean_q: 337.135187, mean_eps: 0.655442
 114988/300000: episode: 1146, duration: 0.647s, episode steps:  94, steps per second: 145, episode reward: -22.383, mean reward: -0.238 [-100.000, 18.636], mean action: 1.596 [0.000, 3.000],  loss: 70.029120, mse: 120906.176114, mean_q: 347.193233, mean_eps: 0.655179
 115118/300000: episode: 1147, duration: 1.053s, episode steps: 130, steps per second: 123, episode reward: -126.257, mean reward: -0.971 [-100.000,  3.977], mean action: 1.569 [0.000, 3.000],  loss: 52.938407, mse: 116561.258774, mean_q: 338.420146, mean_eps: 0.654842
 115257/300000: episode: 1148, duration: 1.020s, episode steps: 139, steps per second: 136, episode reward: -86.078, mean reward: -0.619 [-100.000, 11.046], mean action: 1.799 [0.000, 3.000],  loss: 144.102336, mse: 114737.464085, mean_q: 331.621372, mean_eps: 0.654439
 115353/300000: episode: 1149, duration: 0.750s, episode steps:  96, steps per second: 128, episode reward: -73.668, mean reward: -0.767 [-100.000,  9.608], mean action: 1.500 [0.000, 3.000],  loss: 68.237096, mse: 115463.376221, mean_q: 335.731458, mean_eps: 0.654087
 115423/300000: episode: 1150, duration: 0.522s, episode steps:  70, steps per second: 134, episode reward: -155.839, mean reward: -2.226 [-100.000, 12.239], mean action: 1.657 [0.000, 3.000],  loss: 81.105360, mse: 115097.091741, mean_q: 335.402421, mean_eps: 0.653837
 115535/300000: episode: 1151, duration: 0.793s, episode steps: 112, steps per second: 141, episode reward: -48.891, mean reward: -0.437 [-100.000, 20.578], mean action: 1.795 [0.000, 3.000],  loss: 103.923119, mse: 114561.853446, mean_q: 332.645668, mean_eps: 0.653564
 115670/300000: episode: 1152, duration: 0.930s, episode steps: 135, steps per second: 145, episode reward: -23.651, mean reward: -0.175 [-100.000, 18.853], mean action: 1.674 [0.000, 3.000],  loss: 61.441033, mse: 115233.008160, mean_q: 334.907526, mean_eps: 0.653194
 115778/300000: episode: 1153, duration: 0.774s, episode steps: 108, steps per second: 139, episode reward: -14.731, mean reward: -0.136 [-100.000, 10.960], mean action: 1.731 [0.000, 3.000],  loss: 79.642533, mse: 115205.994141, mean_q: 334.979055, mean_eps: 0.652829
 115900/300000: episode: 1154, duration: 0.828s, episode steps: 122, steps per second: 147, episode reward: -138.572, mean reward: -1.136 [-100.000, 10.207], mean action: 1.811 [0.000, 3.000],  loss: 94.626263, mse: 115207.979316, mean_q: 334.684535, mean_eps: 0.652484
 116040/300000: episode: 1155, duration: 1.003s, episode steps: 140, steps per second: 140, episode reward: -29.242, mean reward: -0.209 [-100.000, 18.153], mean action: 1.757 [0.000, 3.000],  loss: 111.860767, mse: 111580.586217, mean_q: 328.666687, mean_eps: 0.652091
 116154/300000: episode: 1156, duration: 0.759s, episode steps: 114, steps per second: 150, episode reward: -98.070, mean reward: -0.860 [-100.000, 11.044], mean action: 1.509 [0.000, 3.000],  loss: 85.109042, mse: 111359.904948, mean_q: 328.251056, mean_eps: 0.651710
 116281/300000: episode: 1157, duration: 0.894s, episode steps: 127, steps per second: 142, episode reward: -124.114, mean reward: -0.977 [-100.000,  6.080], mean action: 1.827 [0.000, 3.000],  loss: 92.247436, mse: 112071.726809, mean_q: 328.174747, mean_eps: 0.651349
 116380/300000: episode: 1158, duration: 0.678s, episode steps:  99, steps per second: 146, episode reward: -108.637, mean reward: -1.097 [-100.000, 12.495], mean action: 1.545 [0.000, 3.000],  loss: 92.488562, mse: 113171.235717, mean_q: 331.296225, mean_eps: 0.651010
 116470/300000: episode: 1159, duration: 0.601s, episode steps:  90, steps per second: 150, episode reward: -70.701, mean reward: -0.786 [-100.000, 18.574], mean action: 1.622 [0.000, 3.000],  loss: 115.929395, mse: 113306.095747, mean_q: 333.514926, mean_eps: 0.650726
 116590/300000: episode: 1160, duration: 0.876s, episode steps: 120, steps per second: 137, episode reward: -160.857, mean reward: -1.340 [-100.000,  2.782], mean action: 1.675 [0.000, 3.000],  loss: 57.930196, mse: 112678.255664, mean_q: 330.526850, mean_eps: 0.650412
 116712/300000: episode: 1161, duration: 0.856s, episode steps: 122, steps per second: 143, episode reward: -183.841, mean reward: -1.507 [-100.000,  2.448], mean action: 1.820 [0.000, 3.000],  loss: 147.687864, mse: 112488.096952, mean_q: 331.504059, mean_eps: 0.650048
 116788/300000: episode: 1162, duration: 0.519s, episode steps:  76, steps per second: 147, episode reward: -74.016, mean reward: -0.974 [-100.000, 19.403], mean action: 1.447 [0.000, 3.000],  loss: 112.671350, mse: 111396.151830, mean_q: 327.774626, mean_eps: 0.649751
 116914/300000: episode: 1163, duration: 0.888s, episode steps: 126, steps per second: 142, episode reward: -146.228, mean reward: -1.161 [-100.000, 17.345], mean action: 1.738 [0.000, 3.000],  loss: 139.536562, mse: 111236.816778, mean_q: 328.352510, mean_eps: 0.649448
 117047/300000: episode: 1164, duration: 0.897s, episode steps: 133, steps per second: 148, episode reward: -127.289, mean reward: -0.957 [-100.000, 18.853], mean action: 1.579 [0.000, 3.000],  loss: 72.328599, mse: 111577.128172, mean_q: 329.857192, mean_eps: 0.649060
 117149/300000: episode: 1165, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -35.549, mean reward: -0.349 [-100.000,  7.163], mean action: 1.676 [0.000, 3.000],  loss: 96.023378, mse: 111099.514476, mean_q: 330.010161, mean_eps: 0.648707
 117236/300000: episode: 1166, duration: 0.645s, episode steps:  87, steps per second: 135, episode reward: -30.755, mean reward: -0.354 [-100.000, 12.130], mean action: 1.816 [0.000, 3.000],  loss: 70.453982, mse: 112675.274964, mean_q: 334.882925, mean_eps: 0.648424
 117365/300000: episode: 1167, duration: 0.878s, episode steps: 129, steps per second: 147, episode reward: -102.206, mean reward: -0.792 [-100.000, 27.464], mean action: 1.605 [0.000, 3.000],  loss: 113.084684, mse: 111615.739341, mean_q: 330.622322, mean_eps: 0.648100
 117467/300000: episode: 1168, duration: 0.684s, episode steps: 102, steps per second: 149, episode reward: -213.076, mean reward: -2.089 [-100.000,  1.083], mean action: 1.716 [0.000, 3.000],  loss: 68.735666, mse: 114909.598575, mean_q: 340.003220, mean_eps: 0.647753
 117550/300000: episode: 1169, duration: 0.615s, episode steps:  83, steps per second: 135, episode reward: -60.429, mean reward: -0.728 [-100.000,  9.144], mean action: 1.578 [0.000, 3.000],  loss: 103.257573, mse: 111990.195124, mean_q: 330.655650, mean_eps: 0.647476
 117667/300000: episode: 1170, duration: 0.834s, episode steps: 117, steps per second: 140, episode reward: -29.555, mean reward: -0.253 [-100.000, 15.214], mean action: 1.615 [0.000, 3.000],  loss: 87.318554, mse: 115752.713275, mean_q: 341.788407, mean_eps: 0.647176
 117763/300000: episode: 1171, duration: 0.646s, episode steps:  96, steps per second: 149, episode reward: -74.792, mean reward: -0.779 [-100.000,  9.661], mean action: 1.604 [0.000, 3.000],  loss: 85.380843, mse: 114069.034424, mean_q: 335.815725, mean_eps: 0.646856
 117861/300000: episode: 1172, duration: 0.701s, episode steps:  98, steps per second: 140, episode reward: -228.052, mean reward: -2.327 [-100.000,  7.705], mean action: 1.704 [0.000, 3.000],  loss: 119.393174, mse: 117285.296476, mean_q: 344.163661, mean_eps: 0.646566
 117951/300000: episode: 1173, duration: 0.621s, episode steps:  90, steps per second: 145, episode reward: -118.820, mean reward: -1.320 [-100.000,  4.038], mean action: 1.844 [0.000, 3.000],  loss: 106.470265, mse: 118528.201215, mean_q: 345.846232, mean_eps: 0.646283
 118069/300000: episode: 1174, duration: 0.795s, episode steps: 118, steps per second: 148, episode reward: -185.198, mean reward: -1.569 [-100.000,  9.588], mean action: 1.771 [0.000, 3.000],  loss: 123.626092, mse: 120634.050847, mean_q: 350.681376, mean_eps: 0.645972
 118164/300000: episode: 1175, duration: 0.696s, episode steps:  95, steps per second: 137, episode reward: -102.338, mean reward: -1.077 [-100.000, 13.000], mean action: 1.579 [0.000, 3.000],  loss: 66.315820, mse: 122249.461020, mean_q: 353.293653, mean_eps: 0.645652
 118260/300000: episode: 1176, duration: 0.662s, episode steps:  96, steps per second: 145, episode reward: -136.439, mean reward: -1.421 [-100.000,  7.674], mean action: 1.594 [0.000, 3.000],  loss: 114.963099, mse: 123947.151042, mean_q: 356.627692, mean_eps: 0.645365
 118346/300000: episode: 1177, duration: 0.580s, episode steps:  86, steps per second: 148, episode reward: -34.471, mean reward: -0.401 [-100.000, 17.981], mean action: 1.593 [0.000, 3.000],  loss: 84.737690, mse: 122155.191134, mean_q: 352.728619, mean_eps: 0.645092
 118431/300000: episode: 1178, duration: 0.613s, episode steps:  85, steps per second: 139, episode reward: -50.461, mean reward: -0.594 [-100.000,  8.834], mean action: 1.635 [0.000, 3.000],  loss: 56.586436, mse: 121717.431801, mean_q: 352.158899, mean_eps: 0.644836
 118538/300000: episode: 1179, duration: 0.749s, episode steps: 107, steps per second: 143, episode reward: -13.165, mean reward: -0.123 [-100.000, 12.519], mean action: 1.551 [0.000, 3.000],  loss: 74.730820, mse: 124718.476489, mean_q: 358.336002, mean_eps: 0.644548
 118654/300000: episode: 1180, duration: 0.772s, episode steps: 116, steps per second: 150, episode reward: -27.494, mean reward: -0.237 [-100.000, 19.657], mean action: 1.629 [0.000, 3.000],  loss: 58.568854, mse: 123448.293171, mean_q: 355.365613, mean_eps: 0.644214
 118757/300000: episode: 1181, duration: 0.714s, episode steps: 103, steps per second: 144, episode reward: -164.229, mean reward: -1.594 [-100.000,  8.022], mean action: 1.738 [0.000, 3.000],  loss: 118.378253, mse: 124636.112637, mean_q: 356.861030, mean_eps: 0.643885
 118875/300000: episode: 1182, duration: 0.809s, episode steps: 118, steps per second: 146, episode reward: -112.550, mean reward: -0.954 [-100.000, 11.871], mean action: 1.695 [0.000, 3.000],  loss: 119.093519, mse: 128920.263573, mean_q: 363.965470, mean_eps: 0.643554
 118987/300000: episode: 1183, duration: 0.750s, episode steps: 112, steps per second: 149, episode reward: -248.532, mean reward: -2.219 [-100.000, 75.547], mean action: 1.384 [0.000, 3.000],  loss: 116.738252, mse: 129850.782227, mean_q: 364.588570, mean_eps: 0.643209
 119075/300000: episode: 1184, duration: 0.615s, episode steps:  88, steps per second: 143, episode reward: -59.611, mean reward: -0.677 [-100.000,  9.706], mean action: 1.580 [0.000, 3.000],  loss: 82.083999, mse: 133820.827060, mean_q: 372.624948, mean_eps: 0.642908
 119191/300000: episode: 1185, duration: 0.818s, episode steps: 116, steps per second: 142, episode reward: -152.606, mean reward: -1.316 [-100.000,  2.782], mean action: 1.853 [0.000, 3.000],  loss: 94.350934, mse: 131664.004512, mean_q: 367.174427, mean_eps: 0.642602
 119292/300000: episode: 1186, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -19.129, mean reward: -0.189 [-100.000,  8.470], mean action: 1.525 [0.000, 3.000],  loss: 67.424331, mse: 130250.067837, mean_q: 364.265885, mean_eps: 0.642277
 119388/300000: episode: 1187, duration: 0.678s, episode steps:  96, steps per second: 142, episode reward: -26.766, mean reward: -0.279 [-100.000, 10.420], mean action: 1.792 [0.000, 3.000],  loss: 119.602030, mse: 132649.618978, mean_q: 365.487347, mean_eps: 0.641981
 119506/300000: episode: 1188, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -30.491, mean reward: -0.258 [-100.000, 124.624], mean action: 1.619 [0.000, 3.000],  loss: 103.611503, mse: 134633.496557, mean_q: 371.428094, mean_eps: 0.641660
 119617/300000: episode: 1189, duration: 0.791s, episode steps: 111, steps per second: 140, episode reward: -47.794, mean reward: -0.431 [-100.000,  8.316], mean action: 1.649 [0.000, 3.000],  loss: 45.138584, mse: 132446.152801, mean_q: 367.383043, mean_eps: 0.641317
 119700/300000: episode: 1190, duration: 0.595s, episode steps:  83, steps per second: 139, episode reward: -79.148, mean reward: -0.954 [-100.000, 10.297], mean action: 1.422 [0.000, 3.000],  loss: 98.366316, mse: 133372.910862, mean_q: 369.577571, mean_eps: 0.641026
 119817/300000: episode: 1191, duration: 0.802s, episode steps: 117, steps per second: 146, episode reward: -37.820, mean reward: -0.323 [-100.000,  6.142], mean action: 1.761 [0.000, 3.000],  loss: 84.577675, mse: 137045.315905, mean_q: 379.045907, mean_eps: 0.640726
 119948/300000: episode: 1192, duration: 0.875s, episode steps: 131, steps per second: 150, episode reward: -68.717, mean reward: -0.525 [-100.000,  6.163], mean action: 1.649 [0.000, 3.000],  loss: 104.603802, mse: 133716.544907, mean_q: 369.577000, mean_eps: 0.640354
 120045/300000: episode: 1193, duration: 0.689s, episode steps:  97, steps per second: 141, episode reward: -53.334, mean reward: -0.550 [-100.000, 17.267], mean action: 1.546 [0.000, 3.000],  loss: 61.299520, mse: 132392.577642, mean_q: 368.565571, mean_eps: 0.640012
 120169/300000: episode: 1194, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -192.703, mean reward: -1.554 [-100.000, 53.324], mean action: 1.581 [0.000, 3.000],  loss: 125.036014, mse: 130243.304120, mean_q: 362.785208, mean_eps: 0.639680
 120271/300000: episode: 1195, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -94.074, mean reward: -0.922 [-100.000, 10.213], mean action: 1.784 [0.000, 3.000],  loss: 130.425452, mse: 132243.158778, mean_q: 368.073675, mean_eps: 0.639341
 120345/300000: episode: 1196, duration: 0.529s, episode steps:  74, steps per second: 140, episode reward: -48.931, mean reward: -0.661 [-100.000, 20.616], mean action: 1.486 [0.000, 3.000],  loss: 91.717282, mse: 132555.048775, mean_q: 369.219499, mean_eps: 0.639077
 120464/300000: episode: 1197, duration: 0.817s, episode steps: 119, steps per second: 146, episode reward: -25.477, mean reward: -0.214 [-100.000, 15.617], mean action: 1.815 [0.000, 3.000],  loss: 95.978850, mse: 132026.324383, mean_q: 368.203842, mean_eps: 0.638788
 120549/300000: episode: 1198, duration: 0.576s, episode steps:  85, steps per second: 148, episode reward:  8.118, mean reward:  0.096 [-100.000, 17.240], mean action: 1.529 [0.000, 3.000],  loss: 92.017958, mse: 134947.189522, mean_q: 373.148811, mean_eps: 0.638482
 120665/300000: episode: 1199, duration: 0.828s, episode steps: 116, steps per second: 140, episode reward: -109.344, mean reward: -0.943 [-100.000, 11.133], mean action: 1.845 [0.000, 3.000],  loss: 81.979132, mse: 136036.133957, mean_q: 375.508326, mean_eps: 0.638181
 120759/300000: episode: 1200, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -208.307, mean reward: -2.216 [-100.000,  0.548], mean action: 1.766 [0.000, 3.000],  loss: 98.783567, mse: 132563.586602, mean_q: 368.669448, mean_eps: 0.637865
 120827/300000: episode: 1201, duration: 0.461s, episode steps:  68, steps per second: 148, episode reward: -15.955, mean reward: -0.235 [-100.000, 16.734], mean action: 1.559 [0.000, 3.000],  loss: 115.763415, mse: 133182.994026, mean_q: 369.876928, mean_eps: 0.637622
 120949/300000: episode: 1202, duration: 0.849s, episode steps: 122, steps per second: 144, episode reward: -100.696, mean reward: -0.825 [-100.000, 10.043], mean action: 1.721 [0.000, 3.000],  loss: 61.376787, mse: 134559.182569, mean_q: 373.632036, mean_eps: 0.637338
 121085/300000: episode: 1203, duration: 0.922s, episode steps: 136, steps per second: 147, episode reward: -122.193, mean reward: -0.898 [-100.000,  5.213], mean action: 1.581 [0.000, 3.000],  loss: 106.921988, mse: 134642.339384, mean_q: 372.323297, mean_eps: 0.636950
 121173/300000: episode: 1204, duration: 0.594s, episode steps:  88, steps per second: 148, episode reward: -68.245, mean reward: -0.776 [-100.000,  7.726], mean action: 1.477 [0.000, 3.000],  loss: 77.204607, mse: 132091.900479, mean_q: 367.633064, mean_eps: 0.636614
 121268/300000: episode: 1205, duration: 0.671s, episode steps:  95, steps per second: 141, episode reward: -32.256, mean reward: -0.340 [-100.000, 11.368], mean action: 1.589 [0.000, 3.000],  loss: 83.936864, mse: 134146.239227, mean_q: 372.101517, mean_eps: 0.636340
 121388/300000: episode: 1206, duration: 0.828s, episode steps: 120, steps per second: 145, episode reward: -113.972, mean reward: -0.950 [-100.000,  6.448], mean action: 1.675 [0.000, 3.000],  loss: 118.868638, mse: 132581.983073, mean_q: 368.042827, mean_eps: 0.636018
 121495/300000: episode: 1207, duration: 0.720s, episode steps: 107, steps per second: 149, episode reward: -40.034, mean reward: -0.374 [-100.000, 16.307], mean action: 1.579 [0.000, 3.000],  loss: 94.831953, mse: 134771.348277, mean_q: 371.955283, mean_eps: 0.635677
 121585/300000: episode: 1208, duration: 0.637s, episode steps:  90, steps per second: 141, episode reward: -23.785, mean reward: -0.264 [-100.000, 27.167], mean action: 1.744 [0.000, 3.000],  loss: 172.425283, mse: 132537.773438, mean_q: 365.765608, mean_eps: 0.635381
 121686/300000: episode: 1209, duration: 0.696s, episode steps: 101, steps per second: 145, episode reward: -81.194, mean reward: -0.804 [-100.000, 10.583], mean action: 1.832 [0.000, 3.000],  loss: 78.336421, mse: 130737.422803, mean_q: 365.762171, mean_eps: 0.635095
 121763/300000: episode: 1210, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -111.166, mean reward: -1.444 [-100.000, 13.891], mean action: 1.740 [0.000, 3.000],  loss: 132.893941, mse: 129169.562500, mean_q: 360.014447, mean_eps: 0.634828
 121859/300000: episode: 1211, duration: 0.649s, episode steps:  96, steps per second: 148, episode reward: -205.033, mean reward: -2.136 [-100.000,  1.016], mean action: 1.781 [0.000, 3.000],  loss: 211.952740, mse: 128511.062988, mean_q: 359.759236, mean_eps: 0.634568
 121948/300000: episode: 1212, duration: 0.639s, episode steps:  89, steps per second: 139, episode reward: -32.981, mean reward: -0.371 [-100.000,  6.533], mean action: 1.697 [0.000, 3.000],  loss: 57.064997, mse: 131179.410551, mean_q: 367.975430, mean_eps: 0.634291
 122042/300000: episode: 1213, duration: 0.631s, episode steps:  94, steps per second: 149, episode reward: -116.744, mean reward: -1.242 [-100.000,  8.154], mean action: 1.660 [0.000, 3.000],  loss: 81.495734, mse: 131925.356134, mean_q: 366.978811, mean_eps: 0.634016
 122132/300000: episode: 1214, duration: 0.611s, episode steps:  90, steps per second: 147, episode reward: -42.184, mean reward: -0.469 [-100.000, 13.086], mean action: 1.711 [0.000, 3.000],  loss: 67.568811, mse: 133088.522656, mean_q: 369.048399, mean_eps: 0.633740
 122191/300000: episode: 1215, duration: 0.415s, episode steps:  59, steps per second: 142, episode reward: -87.766, mean reward: -1.488 [-100.000, 18.552], mean action: 1.458 [0.000, 3.000],  loss: 53.803457, mse: 132416.250000, mean_q: 367.055844, mean_eps: 0.633517
 122301/300000: episode: 1216, duration: 0.764s, episode steps: 110, steps per second: 144, episode reward: -141.607, mean reward: -1.287 [-100.000,  2.889], mean action: 1.855 [0.000, 3.000],  loss: 71.000140, mse: 130215.784659, mean_q: 362.585280, mean_eps: 0.633263
 122425/300000: episode: 1217, duration: 0.834s, episode steps: 124, steps per second: 149, episode reward: -110.185, mean reward: -0.889 [-100.000, 21.539], mean action: 1.694 [0.000, 3.000],  loss: 89.074215, mse: 132349.582283, mean_q: 369.303186, mean_eps: 0.632913
 122534/300000: episode: 1218, duration: 0.778s, episode steps: 109, steps per second: 140, episode reward: -172.205, mean reward: -1.580 [-100.000,  2.333], mean action: 1.890 [0.000, 3.000],  loss: 112.254811, mse: 132481.462944, mean_q: 368.695192, mean_eps: 0.632563
 122624/300000: episode: 1219, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -223.112, mean reward: -2.479 [-100.000, 14.439], mean action: 1.633 [0.000, 3.000],  loss: 96.899343, mse: 136659.688455, mean_q: 375.785555, mean_eps: 0.632265
 122754/300000: episode: 1220, duration: 0.881s, episode steps: 130, steps per second: 148, episode reward: -33.087, mean reward: -0.255 [-100.000, 13.804], mean action: 1.715 [0.000, 3.000],  loss: 68.438430, mse: 137923.788642, mean_q: 377.249091, mean_eps: 0.631934
 122862/300000: episode: 1221, duration: 0.774s, episode steps: 108, steps per second: 139, episode reward: -102.491, mean reward: -0.949 [-100.000, 17.091], mean action: 1.556 [0.000, 3.000],  loss: 103.160592, mse: 138955.466508, mean_q: 378.331592, mean_eps: 0.631578
 122960/300000: episode: 1222, duration: 0.843s, episode steps:  98, steps per second: 116, episode reward: -190.398, mean reward: -1.943 [-100.000,  1.470], mean action: 1.724 [0.000, 3.000],  loss: 65.783023, mse: 134877.873087, mean_q: 368.788745, mean_eps: 0.631269
 123063/300000: episode: 1223, duration: 0.821s, episode steps: 103, steps per second: 126, episode reward: -121.579, mean reward: -1.180 [-100.000, 12.358], mean action: 1.816 [0.000, 3.000],  loss: 108.359683, mse: 136524.822512, mean_q: 373.800534, mean_eps: 0.630967
 123188/300000: episode: 1224, duration: 1.006s, episode steps: 125, steps per second: 124, episode reward: -122.570, mean reward: -0.981 [-100.000, 13.713], mean action: 1.632 [0.000, 3.000],  loss: 87.218947, mse: 135183.327313, mean_q: 371.526888, mean_eps: 0.630625
 123282/300000: episode: 1225, duration: 0.693s, episode steps:  94, steps per second: 136, episode reward: -213.973, mean reward: -2.276 [-100.000, 41.767], mean action: 1.553 [0.000, 3.000],  loss: 61.657329, mse: 136407.680352, mean_q: 374.569857, mean_eps: 0.630297
 123410/300000: episode: 1226, duration: 0.976s, episode steps: 128, steps per second: 131, episode reward: -21.245, mean reward: -0.166 [-100.000, 43.009], mean action: 1.586 [0.000, 3.000],  loss: 91.341421, mse: 136318.376587, mean_q: 374.797087, mean_eps: 0.629963
 123478/300000: episode: 1227, duration: 0.515s, episode steps:  68, steps per second: 132, episode reward: -71.278, mean reward: -1.048 [-100.000,  9.880], mean action: 1.721 [0.000, 3.000],  loss: 94.823979, mse: 136425.143727, mean_q: 372.992017, mean_eps: 0.629670
 123536/300000: episode: 1228, duration: 0.404s, episode steps:  58, steps per second: 143, episode reward: -54.362, mean reward: -0.937 [-100.000, 11.248], mean action: 1.431 [0.000, 3.000],  loss: 123.991870, mse: 132766.364494, mean_q: 364.626618, mean_eps: 0.629480
 123652/300000: episode: 1229, duration: 0.825s, episode steps: 116, steps per second: 141, episode reward: -66.564, mean reward: -0.574 [-100.000,  6.279], mean action: 1.586 [0.000, 3.000],  loss: 142.341082, mse: 135032.498316, mean_q: 369.704353, mean_eps: 0.629220
 123804/300000: episode: 1230, duration: 1.048s, episode steps: 152, steps per second: 145, episode reward: 11.024, mean reward:  0.073 [-100.000, 17.958], mean action: 1.691 [0.000, 3.000],  loss: 53.173238, mse: 137280.617547, mean_q: 374.568810, mean_eps: 0.628818
 123913/300000: episode: 1231, duration: 0.739s, episode steps: 109, steps per second: 147, episode reward: -83.620, mean reward: -0.767 [-100.000,  8.820], mean action: 1.560 [0.000, 3.000],  loss: 91.068687, mse: 137476.450186, mean_q: 376.168082, mean_eps: 0.628426
 124015/300000: episode: 1232, duration: 0.731s, episode steps: 102, steps per second: 140, episode reward: -104.070, mean reward: -1.020 [-100.000, 10.991], mean action: 1.843 [0.000, 3.000],  loss: 106.804629, mse: 136580.723575, mean_q: 373.654212, mean_eps: 0.628109
 124110/300000: episode: 1233, duration: 0.647s, episode steps:  95, steps per second: 147, episode reward: -117.032, mean reward: -1.232 [-100.000, 11.024], mean action: 1.516 [0.000, 3.000],  loss: 76.498996, mse: 136065.676234, mean_q: 371.888378, mean_eps: 0.627814
 124231/300000: episode: 1234, duration: 0.809s, episode steps: 121, steps per second: 149, episode reward: -131.668, mean reward: -1.088 [-100.000,  6.809], mean action: 1.769 [0.000, 3.000],  loss: 174.116011, mse: 136471.475077, mean_q: 375.215997, mean_eps: 0.627490
 124343/300000: episode: 1235, duration: 0.784s, episode steps: 112, steps per second: 143, episode reward: -170.109, mean reward: -1.519 [-100.000, 88.583], mean action: 1.688 [0.000, 3.000],  loss: 106.995087, mse: 136522.409528, mean_q: 374.733957, mean_eps: 0.627140
 124682/300000: episode: 1236, duration: 2.629s, episode steps: 339, steps per second: 129, episode reward: -29.773, mean reward: -0.088 [-100.000, 80.493], mean action: 1.617 [0.000, 3.000],  loss: 140.682424, mse: 137908.201074, mean_q: 378.503753, mean_eps: 0.626464
 124840/300000: episode: 1237, duration: 1.145s, episode steps: 158, steps per second: 138, episode reward: -35.206, mean reward: -0.223 [-100.000,  7.241], mean action: 1.639 [0.000, 3.000],  loss: 84.964022, mse: 135638.294106, mean_q: 373.275838, mean_eps: 0.625719
 124951/300000: episode: 1238, duration: 0.868s, episode steps: 111, steps per second: 128, episode reward: -32.784, mean reward: -0.295 [-100.000,  9.338], mean action: 1.676 [0.000, 3.000],  loss: 122.869615, mse: 137479.582418, mean_q: 376.350040, mean_eps: 0.625315
 125090/300000: episode: 1239, duration: 0.960s, episode steps: 139, steps per second: 145, episode reward: -210.144, mean reward: -1.512 [-100.000, 102.198], mean action: 1.683 [0.000, 3.000],  loss: 144.068525, mse: 138366.140737, mean_q: 378.725103, mean_eps: 0.624940
 125220/300000: episode: 1240, duration: 0.916s, episode steps: 130, steps per second: 142, episode reward: -89.813, mean reward: -0.691 [-100.000, 12.013], mean action: 1.677 [0.000, 3.000],  loss: 109.385328, mse: 136274.082752, mean_q: 375.876722, mean_eps: 0.624537
 125300/300000: episode: 1241, duration: 0.598s, episode steps:  80, steps per second: 134, episode reward: -54.494, mean reward: -0.681 [-100.000,  9.085], mean action: 1.688 [0.000, 3.000],  loss: 86.373345, mse: 140352.329980, mean_q: 382.943547, mean_eps: 0.624221
 125373/300000: episode: 1242, duration: 0.493s, episode steps:  73, steps per second: 148, episode reward: -110.909, mean reward: -1.519 [-100.000, 10.534], mean action: 1.521 [0.000, 3.000],  loss: 179.825987, mse: 139356.808112, mean_q: 381.143099, mean_eps: 0.623992
 125493/300000: episode: 1243, duration: 0.863s, episode steps: 120, steps per second: 139, episode reward:  3.418, mean reward:  0.028 [-100.000, 20.629], mean action: 1.617 [0.000, 3.000],  loss: 109.460447, mse: 139118.905794, mean_q: 381.128365, mean_eps: 0.623703
 125600/300000: episode: 1244, duration: 0.803s, episode steps: 107, steps per second: 133, episode reward: -96.254, mean reward: -0.900 [-100.000, 11.680], mean action: 1.589 [0.000, 3.000],  loss: 175.316015, mse: 140853.401796, mean_q: 381.676919, mean_eps: 0.623362
 125671/300000: episode: 1245, duration: 0.507s, episode steps:  71, steps per second: 140, episode reward: -64.924, mean reward: -0.914 [-100.000, 15.649], mean action: 1.592 [0.000, 3.000],  loss: 199.574773, mse: 144725.287742, mean_q: 390.556006, mean_eps: 0.623095
 125796/300000: episode: 1246, duration: 0.940s, episode steps: 125, steps per second: 133, episode reward: -108.732, mean reward: -0.870 [-100.000, 11.543], mean action: 1.608 [0.000, 3.000],  loss: 101.356056, mse: 143252.446125, mean_q: 385.988002, mean_eps: 0.622801
 125917/300000: episode: 1247, duration: 0.834s, episode steps: 121, steps per second: 145, episode reward: -27.544, mean reward: -0.228 [-100.000, 13.057], mean action: 1.777 [0.000, 3.000],  loss: 173.318220, mse: 145917.408445, mean_q: 391.218713, mean_eps: 0.622432
 126000/300000: episode: 1248, duration: 0.553s, episode steps:  83, steps per second: 150, episode reward: -78.591, mean reward: -0.947 [-100.000, 24.132], mean action: 1.855 [0.000, 3.000],  loss: 159.276445, mse: 144755.131683, mean_q: 386.717698, mean_eps: 0.622126
 126085/300000: episode: 1249, duration: 0.595s, episode steps:  85, steps per second: 143, episode reward: -130.257, mean reward: -1.532 [-100.000,  7.292], mean action: 1.624 [0.000, 3.000],  loss: 143.796399, mse: 148499.146415, mean_q: 396.919676, mean_eps: 0.621874
 126185/300000: episode: 1250, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: -46.477, mean reward: -0.465 [-100.000, 11.495], mean action: 1.400 [0.000, 3.000],  loss: 209.212711, mse: 145309.468516, mean_q: 392.571498, mean_eps: 0.621596
 126271/300000: episode: 1251, duration: 0.601s, episode steps:  86, steps per second: 143, episode reward: -46.634, mean reward: -0.542 [-100.000, 13.427], mean action: 1.488 [0.000, 3.000],  loss: 72.301193, mse: 145935.799328, mean_q: 392.754602, mean_eps: 0.621317
 126399/300000: episode: 1252, duration: 0.909s, episode steps: 128, steps per second: 141, episode reward: -163.158, mean reward: -1.275 [-100.000,  7.679], mean action: 1.828 [0.000, 3.000],  loss: 170.685031, mse: 149132.591125, mean_q: 397.898602, mean_eps: 0.620996
 126476/300000: episode: 1253, duration: 0.541s, episode steps:  77, steps per second: 142, episode reward: -41.195, mean reward: -0.535 [-100.000, 15.252], mean action: 1.610 [0.000, 3.000],  loss: 107.780502, mse: 147047.334619, mean_q: 395.521048, mean_eps: 0.620689
 126560/300000: episode: 1254, duration: 0.569s, episode steps:  84, steps per second: 148, episode reward: -78.386, mean reward: -0.933 [-100.000, 14.638], mean action: 1.762 [0.000, 3.000],  loss: 98.403212, mse: 149889.197545, mean_q: 398.840482, mean_eps: 0.620447
 126658/300000: episode: 1255, duration: 0.682s, episode steps:  98, steps per second: 144, episode reward: -80.456, mean reward: -0.821 [-100.000,  6.895], mean action: 1.663 [0.000, 3.000],  loss: 138.341893, mse: 149935.082031, mean_q: 397.530800, mean_eps: 0.620175
 126771/300000: episode: 1256, duration: 0.804s, episode steps: 113, steps per second: 141, episode reward: -81.690, mean reward: -0.723 [-100.000, 11.461], mean action: 1.717 [0.000, 3.000],  loss: 161.286014, mse: 150004.073078, mean_q: 396.480001, mean_eps: 0.619858
 126857/300000: episode: 1257, duration: 0.581s, episode steps:  86, steps per second: 148, episode reward: -15.439, mean reward: -0.180 [-100.000, 13.416], mean action: 1.570 [0.000, 3.000],  loss: 161.373807, mse: 149834.188681, mean_q: 396.273241, mean_eps: 0.619560
 126951/300000: episode: 1258, duration: 0.643s, episode steps:  94, steps per second: 146, episode reward: -52.767, mean reward: -0.561 [-100.000,  7.536], mean action: 1.766 [0.000, 3.000],  loss: 102.300739, mse: 151345.694315, mean_q: 399.639542, mean_eps: 0.619289
 127070/300000: episode: 1259, duration: 0.835s, episode steps: 119, steps per second: 142, episode reward: -105.676, mean reward: -0.888 [-100.000,  7.306], mean action: 1.420 [0.000, 3.000],  loss: 156.689651, mse: 149033.897978, mean_q: 396.765298, mean_eps: 0.618970
 127180/300000: episode: 1260, duration: 0.744s, episode steps: 110, steps per second: 148, episode reward: -66.420, mean reward: -0.604 [-100.000, 11.451], mean action: 1.682 [0.000, 3.000],  loss: 105.784820, mse: 148502.975355, mean_q: 393.284845, mean_eps: 0.618627
 127275/300000: episode: 1261, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -83.109, mean reward: -0.875 [-100.000,  6.503], mean action: 1.821 [0.000, 3.000],  loss: 103.705553, mse: 148597.163651, mean_q: 393.654699, mean_eps: 0.618319
 127391/300000: episode: 1262, duration: 0.828s, episode steps: 116, steps per second: 140, episode reward: -132.358, mean reward: -1.141 [-100.000,  5.186], mean action: 1.741 [0.000, 3.000],  loss: 138.381333, mse: 148088.371094, mean_q: 393.007037, mean_eps: 0.618003
 127495/300000: episode: 1263, duration: 0.710s, episode steps: 104, steps per second: 147, episode reward: -330.483, mean reward: -3.178 [-100.000,  1.872], mean action: 1.423 [0.000, 3.000],  loss: 76.377908, mse: 151490.396785, mean_q: 400.127058, mean_eps: 0.617672
 127606/300000: episode: 1264, duration: 0.762s, episode steps: 111, steps per second: 146, episode reward: -167.389, mean reward: -1.508 [-100.000, 16.502], mean action: 1.703 [0.000, 3.000],  loss: 109.802400, mse: 148112.324606, mean_q: 393.677192, mean_eps: 0.617350
 127681/300000: episode: 1265, duration: 0.559s, episode steps:  75, steps per second: 134, episode reward: -91.626, mean reward: -1.222 [-100.000,  6.346], mean action: 1.960 [0.000, 3.000],  loss: 174.088920, mse: 148741.687604, mean_q: 393.621338, mean_eps: 0.617071
 127788/300000: episode: 1266, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -60.592, mean reward: -0.566 [-100.000,  6.488], mean action: 1.776 [0.000, 3.000],  loss: 112.570697, mse: 145477.791545, mean_q: 386.204275, mean_eps: 0.616798
 127903/300000: episode: 1267, duration: 0.784s, episode steps: 115, steps per second: 147, episode reward: -210.562, mean reward: -1.831 [-100.000,  6.518], mean action: 1.626 [0.000, 3.000],  loss: 102.313092, mse: 149342.999389, mean_q: 392.362344, mean_eps: 0.616465
 127971/300000: episode: 1268, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: -52.283, mean reward: -0.769 [-100.000,  6.862], mean action: 1.574 [0.000, 3.000],  loss: 97.160041, mse: 151538.371094, mean_q: 396.022895, mean_eps: 0.616190
 128048/300000: episode: 1269, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: -152.386, mean reward: -1.979 [-100.000, 31.322], mean action: 1.442 [0.000, 3.000],  loss: 96.536816, mse: 151901.245840, mean_q: 396.456326, mean_eps: 0.615973
 128142/300000: episode: 1270, duration: 0.638s, episode steps:  94, steps per second: 147, episode reward: -113.088, mean reward: -1.203 [-100.000,  9.388], mean action: 1.947 [0.000, 3.000],  loss: 147.205652, mse: 151901.856549, mean_q: 397.031191, mean_eps: 0.615716
 128226/300000: episode: 1271, duration: 0.569s, episode steps:  84, steps per second: 148, episode reward: -71.562, mean reward: -0.852 [-100.000, 15.743], mean action: 1.726 [0.000, 3.000],  loss: 101.615582, mse: 156453.221168, mean_q: 405.204522, mean_eps: 0.615449
 128317/300000: episode: 1272, duration: 0.645s, episode steps:  91, steps per second: 141, episode reward: -72.777, mean reward: -0.800 [-100.000,  9.178], mean action: 1.670 [0.000, 3.000],  loss: 90.248544, mse: 158933.888221, mean_q: 407.526265, mean_eps: 0.615187
 128427/300000: episode: 1273, duration: 0.738s, episode steps: 110, steps per second: 149, episode reward: -114.253, mean reward: -1.039 [-100.000,  7.487], mean action: 1.636 [0.000, 3.000],  loss: 109.377493, mse: 159566.132955, mean_q: 405.618028, mean_eps: 0.614885
 128513/300000: episode: 1274, duration: 0.577s, episode steps:  86, steps per second: 149, episode reward: -7.003, mean reward: -0.081 [-100.000, 21.020], mean action: 1.570 [0.000, 3.000],  loss: 116.870066, mse: 155938.759993, mean_q: 398.369341, mean_eps: 0.614591
 128619/300000: episode: 1275, duration: 0.741s, episode steps: 106, steps per second: 143, episode reward: -58.777, mean reward: -0.555 [-100.000,  5.736], mean action: 1.755 [0.000, 3.000],  loss: 116.158868, mse: 160334.567807, mean_q: 407.990183, mean_eps: 0.614304
 128705/300000: episode: 1276, duration: 0.587s, episode steps:  86, steps per second: 147, episode reward: -150.945, mean reward: -1.755 [-100.000,  4.382], mean action: 1.535 [0.000, 3.000],  loss: 160.807077, mse: 157312.706668, mean_q: 400.569177, mean_eps: 0.614016
 128798/300000: episode: 1277, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -85.308, mean reward: -0.917 [-100.000,  7.566], mean action: 1.656 [0.000, 3.000],  loss: 110.126051, mse: 160068.950269, mean_q: 406.575984, mean_eps: 0.613747
 128874/300000: episode: 1278, duration: 0.534s, episode steps:  76, steps per second: 142, episode reward: -50.992, mean reward: -0.671 [-100.000,  7.255], mean action: 1.658 [0.000, 3.000],  loss: 127.853945, mse: 161192.171053, mean_q: 408.144902, mean_eps: 0.613494
 129874/300000: episode: 1279, duration: 7.486s, episode steps: 1000, steps per second: 134, episode reward: 46.699, mean reward:  0.047 [-23.145, 38.402], mean action: 1.543 [0.000, 3.000],  loss: 146.820995, mse: 169577.688086, mean_q: 420.071557, mean_eps: 0.611880
 129975/300000: episode: 1280, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -121.712, mean reward: -1.205 [-100.000,  5.757], mean action: 1.564 [0.000, 3.000],  loss: 240.529817, mse: 177018.622293, mean_q: 434.226899, mean_eps: 0.610228
 130048/300000: episode: 1281, duration: 0.514s, episode steps:  73, steps per second: 142, episode reward: -59.361, mean reward: -0.813 [-100.000,  8.051], mean action: 1.575 [0.000, 3.000],  loss: 183.967334, mse: 176655.865903, mean_q: 433.673149, mean_eps: 0.609967
 130340/300000: episode: 1282, duration: 2.023s, episode steps: 292, steps per second: 144, episode reward: -232.741, mean reward: -0.797 [-100.000, 28.741], mean action: 1.712 [0.000, 3.000],  loss: 185.116718, mse: 175942.238014, mean_q: 433.153307, mean_eps: 0.609420
 130443/300000: episode: 1283, duration: 0.741s, episode steps: 103, steps per second: 139, episode reward: -61.107, mean reward: -0.593 [-100.000,  7.422], mean action: 1.641 [0.000, 3.000],  loss: 142.267820, mse: 176534.182266, mean_q: 434.478029, mean_eps: 0.608827
 130533/300000: episode: 1284, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: -66.302, mean reward: -0.737 [-100.000, 10.003], mean action: 1.533 [0.000, 3.000],  loss: 154.421244, mse: 177532.583941, mean_q: 435.039202, mean_eps: 0.608537
 130638/300000: episode: 1285, duration: 0.707s, episode steps: 105, steps per second: 149, episode reward: -139.526, mean reward: -1.329 [-100.000,  7.477], mean action: 1.590 [0.000, 3.000],  loss: 165.095458, mse: 175897.391815, mean_q: 432.734882, mean_eps: 0.608245
 130729/300000: episode: 1286, duration: 0.644s, episode steps:  91, steps per second: 141, episode reward: -94.045, mean reward: -1.033 [-100.000,  9.639], mean action: 1.495 [0.000, 3.000],  loss: 109.171632, mse: 176213.839973, mean_q: 431.749204, mean_eps: 0.607951
 130842/300000: episode: 1287, duration: 0.773s, episode steps: 113, steps per second: 146, episode reward: -65.107, mean reward: -0.576 [-100.000, 14.020], mean action: 1.558 [0.000, 3.000],  loss: 153.164022, mse: 177580.820243, mean_q: 432.923716, mean_eps: 0.607645
 130931/300000: episode: 1288, duration: 0.605s, episode steps:  89, steps per second: 147, episode reward: -34.236, mean reward: -0.385 [-100.000, 14.367], mean action: 1.854 [0.000, 3.000],  loss: 202.805045, mse: 175255.001756, mean_q: 427.763359, mean_eps: 0.607342
 131031/300000: episode: 1289, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: -60.423, mean reward: -0.604 [-100.000, 12.148], mean action: 1.490 [0.000, 3.000],  loss: 177.531621, mse: 174135.289062, mean_q: 426.422647, mean_eps: 0.607058
 131114/300000: episode: 1290, duration: 0.610s, episode steps:  83, steps per second: 136, episode reward:  8.166, mean reward:  0.098 [-100.000, 17.734], mean action: 1.651 [0.000, 3.000],  loss: 181.501336, mse: 176265.368599, mean_q: 427.318364, mean_eps: 0.606784
 131241/300000: episode: 1291, duration: 0.855s, episode steps: 127, steps per second: 148, episode reward: -31.775, mean reward: -0.250 [-100.000, 11.647], mean action: 1.433 [0.000, 3.000],  loss: 113.968165, mse: 181330.665846, mean_q: 438.348643, mean_eps: 0.606469
 131351/300000: episode: 1292, duration: 0.787s, episode steps: 110, steps per second: 140, episode reward: -25.304, mean reward: -0.230 [-100.000, 11.184], mean action: 1.627 [0.000, 3.000],  loss: 136.445054, mse: 183041.672727, mean_q: 437.917050, mean_eps: 0.606113
 131453/300000: episode: 1293, duration: 0.692s, episode steps: 102, steps per second: 147, episode reward:  7.546, mean reward:  0.074 [-100.000, 11.644], mean action: 1.627 [0.000, 3.000],  loss: 140.965554, mse: 186685.670496, mean_q: 443.084737, mean_eps: 0.605795
 131554/300000: episode: 1294, duration: 0.680s, episode steps: 101, steps per second: 148, episode reward: -45.476, mean reward: -0.450 [-100.000,  8.127], mean action: 1.663 [0.000, 3.000],  loss: 123.506004, mse: 186952.447865, mean_q: 443.359002, mean_eps: 0.605491
 131666/300000: episode: 1295, duration: 0.786s, episode steps: 112, steps per second: 142, episode reward: -61.050, mean reward: -0.545 [-100.000, 10.102], mean action: 1.491 [0.000, 3.000],  loss: 92.670792, mse: 188366.277762, mean_q: 445.690850, mean_eps: 0.605171
 131774/300000: episode: 1296, duration: 0.740s, episode steps: 108, steps per second: 146, episode reward: -41.676, mean reward: -0.386 [-100.000,  9.159], mean action: 1.685 [0.000, 3.000],  loss: 208.802020, mse: 191810.761429, mean_q: 448.322714, mean_eps: 0.604842
 131844/300000: episode: 1297, duration: 0.471s, episode steps:  70, steps per second: 149, episode reward: -49.005, mean reward: -0.700 [-100.000, 10.161], mean action: 1.857 [0.000, 3.000],  loss: 110.522247, mse: 193898.816071, mean_q: 451.966361, mean_eps: 0.604575
 131925/300000: episode: 1298, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -76.133, mean reward: -0.940 [-100.000,  8.851], mean action: 1.815 [0.000, 3.000],  loss: 192.988544, mse: 191018.972029, mean_q: 450.670188, mean_eps: 0.604348
 132032/300000: episode: 1299, duration: 0.729s, episode steps: 107, steps per second: 147, episode reward: -100.518, mean reward: -0.939 [-100.000, 18.115], mean action: 1.617 [0.000, 3.000],  loss: 192.086686, mse: 186828.724445, mean_q: 443.855925, mean_eps: 0.604066
 132136/300000: episode: 1300, duration: 0.834s, episode steps: 104, steps per second: 125, episode reward: -26.816, mean reward: -0.258 [-100.000, 12.348], mean action: 1.663 [0.000, 3.000],  loss: 211.650080, mse: 187404.426983, mean_q: 442.748743, mean_eps: 0.603749
 132205/300000: episode: 1301, duration: 0.635s, episode steps:  69, steps per second: 109, episode reward: -60.350, mean reward: -0.875 [-100.000, 16.640], mean action: 1.435 [0.000, 3.000],  loss: 123.445170, mse: 189755.728261, mean_q: 447.025850, mean_eps: 0.603490
 132291/300000: episode: 1302, duration: 0.679s, episode steps:  86, steps per second: 127, episode reward: 22.901, mean reward:  0.266 [-100.000, 12.778], mean action: 1.663 [0.000, 3.000],  loss: 190.893834, mse: 190233.920058, mean_q: 449.415793, mean_eps: 0.603257
 132386/300000: episode: 1303, duration: 0.726s, episode steps:  95, steps per second: 131, episode reward: -256.448, mean reward: -2.699 [-100.000, 58.040], mean action: 1.758 [0.000, 3.000],  loss: 219.361743, mse: 193565.535033, mean_q: 453.461389, mean_eps: 0.602986
 132452/300000: episode: 1304, duration: 0.520s, episode steps:  66, steps per second: 127, episode reward: -65.750, mean reward: -0.996 [-100.000,  7.256], mean action: 1.621 [0.000, 3.000],  loss: 143.475210, mse: 194649.012547, mean_q: 451.797815, mean_eps: 0.602744
 132566/300000: episode: 1305, duration: 0.874s, episode steps: 114, steps per second: 130, episode reward: -27.915, mean reward: -0.245 [-100.000, 15.516], mean action: 1.518 [0.000, 3.000],  loss: 230.602129, mse: 193002.861705, mean_q: 448.672416, mean_eps: 0.602474
 132668/300000: episode: 1306, duration: 0.863s, episode steps: 102, steps per second: 118, episode reward: -49.215, mean reward: -0.482 [-100.000, 10.840], mean action: 1.647 [0.000, 3.000],  loss: 211.039438, mse: 194062.396293, mean_q: 452.420689, mean_eps: 0.602150
 132743/300000: episode: 1307, duration: 0.565s, episode steps:  75, steps per second: 133, episode reward: -11.622, mean reward: -0.155 [-100.000, 15.993], mean action: 1.760 [0.000, 3.000],  loss: 147.791255, mse: 191953.798542, mean_q: 447.216618, mean_eps: 0.601885
 132858/300000: episode: 1308, duration: 0.816s, episode steps: 115, steps per second: 141, episode reward: -77.078, mean reward: -0.670 [-100.000, 14.145], mean action: 1.522 [0.000, 3.000],  loss: 194.465192, mse: 193229.558832, mean_q: 450.663318, mean_eps: 0.601600
 132953/300000: episode: 1309, duration: 0.639s, episode steps:  95, steps per second: 149, episode reward: -90.539, mean reward: -0.953 [-100.000,  5.977], mean action: 1.663 [0.000, 3.000],  loss: 205.000172, mse: 195605.725000, mean_q: 452.211300, mean_eps: 0.601285
 133057/300000: episode: 1310, duration: 0.715s, episode steps: 104, steps per second: 145, episode reward: -38.565, mean reward: -0.371 [-100.000, 11.389], mean action: 1.606 [0.000, 3.000],  loss: 213.121120, mse: 195945.387770, mean_q: 448.704172, mean_eps: 0.600986
 133819/300000: episode: 1311, duration: 5.865s, episode steps: 762, steps per second: 130, episode reward: -234.153, mean reward: -0.307 [-100.000, 32.450], mean action: 1.639 [0.000, 3.000],  loss: 185.314753, mse: 202510.176366, mean_q: 458.694623, mean_eps: 0.599687
 133925/300000: episode: 1312, duration: 0.789s, episode steps: 106, steps per second: 134, episode reward: -36.120, mean reward: -0.341 [-100.000, 16.308], mean action: 1.604 [0.000, 3.000],  loss: 289.923590, mse: 210070.263856, mean_q: 467.453316, mean_eps: 0.598386
 134026/300000: episode: 1313, duration: 0.743s, episode steps: 101, steps per second: 136, episode reward: -99.156, mean reward: -0.982 [-100.000,  6.132], mean action: 1.446 [0.000, 3.000],  loss: 285.207472, mse: 212798.156559, mean_q: 473.059166, mean_eps: 0.598075
 134146/300000: episode: 1314, duration: 0.813s, episode steps: 120, steps per second: 148, episode reward: -42.132, mean reward: -0.351 [-100.000,  8.587], mean action: 1.725 [0.000, 3.000],  loss: 174.093608, mse: 216138.323437, mean_q: 474.585177, mean_eps: 0.597743
 134228/300000: episode: 1315, duration: 0.579s, episode steps:  82, steps per second: 142, episode reward: -47.765, mean reward: -0.582 [-100.000, 20.756], mean action: 1.512 [0.000, 3.000],  loss: 128.479827, mse: 221785.353277, mean_q: 484.650318, mean_eps: 0.597440
 134400/300000: episode: 1316, duration: 1.163s, episode steps: 172, steps per second: 148, episode reward: -164.650, mean reward: -0.957 [-100.000, 39.928], mean action: 1.593 [0.000, 3.000],  loss: 172.213763, mse: 220420.976017, mean_q: 481.960040, mean_eps: 0.597060
 134525/300000: episode: 1317, duration: 0.892s, episode steps: 125, steps per second: 140, episode reward: -87.587, mean reward: -0.701 [-100.000,  5.711], mean action: 1.512 [0.000, 3.000],  loss: 191.177114, mse: 224321.941625, mean_q: 487.866862, mean_eps: 0.596614
 134625/300000: episode: 1318, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: -86.981, mean reward: -0.870 [-100.000,  6.668], mean action: 1.530 [0.000, 3.000],  loss: 218.721426, mse: 229039.269219, mean_q: 492.625046, mean_eps: 0.596276
 134734/300000: episode: 1319, duration: 0.726s, episode steps: 109, steps per second: 150, episode reward: -62.391, mean reward: -0.572 [-100.000, 13.556], mean action: 1.752 [0.000, 3.000],  loss: 264.496831, mse: 228753.894495, mean_q: 493.840884, mean_eps: 0.595963
 134821/300000: episode: 1320, duration: 0.631s, episode steps:  87, steps per second: 138, episode reward: -30.417, mean reward: -0.350 [-100.000, 16.375], mean action: 1.644 [0.000, 3.000],  loss: 304.378678, mse: 226577.711027, mean_q: 488.863718, mean_eps: 0.595669
 134950/300000: episode: 1321, duration: 0.915s, episode steps: 129, steps per second: 141, episode reward: -89.599, mean reward: -0.695 [-100.000, 14.555], mean action: 1.504 [0.000, 3.000],  loss: 230.978739, mse: 234851.369792, mean_q: 503.652888, mean_eps: 0.595345
 135046/300000: episode: 1322, duration: 0.663s, episode steps:  96, steps per second: 145, episode reward: 35.452, mean reward:  0.369 [-100.000, 18.633], mean action: 1.781 [0.000, 3.000],  loss: 134.700542, mse: 230334.783203, mean_q: 497.099367, mean_eps: 0.595008
 135341/300000: episode: 1323, duration: 2.064s, episode steps: 295, steps per second: 143, episode reward: -175.004, mean reward: -0.593 [-100.000, 18.678], mean action: 1.725 [0.000, 3.000],  loss: 243.402109, mse: 238238.252278, mean_q: 504.736747, mean_eps: 0.594421
 135420/300000: episode: 1324, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -56.153, mean reward: -0.711 [-100.000,  9.270], mean action: 1.797 [0.000, 3.000],  loss: 205.751809, mse: 243245.562698, mean_q: 509.419490, mean_eps: 0.593860
 135529/300000: episode: 1325, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: -70.692, mean reward: -0.649 [-100.000, 11.538], mean action: 1.743 [0.000, 3.000],  loss: 174.804463, mse: 247829.381307, mean_q: 517.011963, mean_eps: 0.593578
 135629/300000: episode: 1326, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: -62.596, mean reward: -0.626 [-100.000, 29.951], mean action: 1.520 [0.000, 3.000],  loss: 316.466254, mse: 248902.215156, mean_q: 516.373670, mean_eps: 0.593265
 135754/300000: episode: 1327, duration: 0.900s, episode steps: 125, steps per second: 139, episode reward: -31.704, mean reward: -0.254 [-100.000,  7.675], mean action: 1.632 [0.000, 3.000],  loss: 221.985403, mse: 251645.308000, mean_q: 517.947922, mean_eps: 0.592927
 135846/300000: episode: 1328, duration: 0.636s, episode steps:  92, steps per second: 145, episode reward: -25.648, mean reward: -0.279 [-100.000, 11.483], mean action: 1.739 [0.000, 3.000],  loss: 226.576225, mse: 257508.590863, mean_q: 526.252531, mean_eps: 0.592602
 135969/300000: episode: 1329, duration: 0.820s, episode steps: 123, steps per second: 150, episode reward: -96.647, mean reward: -0.786 [-100.000, 16.157], mean action: 1.610 [0.000, 3.000],  loss: 192.284675, mse: 258158.664126, mean_q: 525.481951, mean_eps: 0.592279
 136092/300000: episode: 1330, duration: 0.858s, episode steps: 123, steps per second: 143, episode reward: -51.090, mean reward: -0.415 [-100.000,  9.561], mean action: 1.659 [0.000, 3.000],  loss: 210.524180, mse: 252416.680386, mean_q: 516.250712, mean_eps: 0.591910
 136188/300000: episode: 1331, duration: 0.659s, episode steps:  96, steps per second: 146, episode reward: -100.566, mean reward: -1.048 [-100.000,  9.027], mean action: 1.792 [0.000, 3.000],  loss: 277.750292, mse: 260468.575521, mean_q: 527.791879, mean_eps: 0.591581
 136306/300000: episode: 1332, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -14.553, mean reward: -0.123 [-100.000, 14.141], mean action: 1.805 [0.000, 3.000],  loss: 219.075720, mse: 260357.431939, mean_q: 528.485877, mean_eps: 0.591261
 136401/300000: episode: 1333, duration: 0.671s, episode steps:  95, steps per second: 142, episode reward: -60.195, mean reward: -0.634 [-100.000, 57.760], mean action: 1.737 [0.000, 3.000],  loss: 231.373413, mse: 255399.877632, mean_q: 520.009446, mean_eps: 0.590941
 136475/300000: episode: 1334, duration: 0.514s, episode steps:  74, steps per second: 144, episode reward: -67.161, mean reward: -0.908 [-100.000, 39.829], mean action: 1.581 [0.000, 3.000],  loss: 301.127960, mse: 258535.091216, mean_q: 524.096860, mean_eps: 0.590688
 136558/300000: episode: 1335, duration: 0.558s, episode steps:  83, steps per second: 149, episode reward: -14.830, mean reward: -0.179 [-100.000, 16.071], mean action: 1.723 [0.000, 3.000],  loss: 249.740658, mse: 264417.618411, mean_q: 534.509967, mean_eps: 0.590452
 136679/300000: episode: 1336, duration: 0.890s, episode steps: 121, steps per second: 136, episode reward: -131.851, mean reward: -1.090 [-100.000,  6.610], mean action: 1.488 [0.000, 3.000],  loss: 272.826463, mse: 265610.701575, mean_q: 533.759074, mean_eps: 0.590146
 136799/300000: episode: 1337, duration: 0.843s, episode steps: 120, steps per second: 142, episode reward:  6.974, mean reward:  0.058 [-100.000, 45.490], mean action: 1.667 [0.000, 3.000],  loss: 153.209311, mse: 260146.137891, mean_q: 527.143150, mean_eps: 0.589784
 137799/300000: episode: 1338, duration: 8.108s, episode steps: 1000, steps per second: 123, episode reward: 36.262, mean reward:  0.036 [-23.663, 27.637], mean action: 1.615 [0.000, 3.000],  loss: 273.773728, mse: 260364.476891, mean_q: 529.085602, mean_eps: 0.588104
 137949/300000: episode: 1339, duration: 1.005s, episode steps: 150, steps per second: 149, episode reward: 11.740, mean reward:  0.078 [-100.000, 13.680], mean action: 1.687 [0.000, 3.000],  loss: 283.476758, mse: 261929.801458, mean_q: 537.347909, mean_eps: 0.586380
 138050/300000: episode: 1340, duration: 0.684s, episode steps: 101, steps per second: 148, episode reward: -13.017, mean reward: -0.129 [-100.000, 18.354], mean action: 1.703 [0.000, 3.000],  loss: 237.300396, mse: 270103.166306, mean_q: 545.744258, mean_eps: 0.586003
 138171/300000: episode: 1341, duration: 0.827s, episode steps: 121, steps per second: 146, episode reward: -31.821, mean reward: -0.263 [-100.000, 10.137], mean action: 1.653 [0.000, 3.000],  loss: 228.263812, mse: 265564.958290, mean_q: 540.663373, mean_eps: 0.585670
 138294/300000: episode: 1342, duration: 0.826s, episode steps: 123, steps per second: 149, episode reward: -54.001, mean reward: -0.439 [-100.000, 15.807], mean action: 1.504 [0.000, 3.000],  loss: 196.506887, mse: 275016.865346, mean_q: 551.613133, mean_eps: 0.585304
 138388/300000: episode: 1343, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -82.517, mean reward: -0.878 [-100.000,  7.905], mean action: 1.723 [0.000, 3.000],  loss: 194.263178, mse: 289454.127826, mean_q: 568.803515, mean_eps: 0.584979
 138535/300000: episode: 1344, duration: 1.002s, episode steps: 147, steps per second: 147, episode reward: -196.441, mean reward: -1.336 [-100.000, 38.023], mean action: 1.449 [0.000, 3.000],  loss: 420.124838, mse: 286187.737883, mean_q: 564.217367, mean_eps: 0.584617
 138636/300000: episode: 1345, duration: 0.694s, episode steps: 101, steps per second: 145, episode reward: -2.963, mean reward: -0.029 [-100.000, 16.847], mean action: 1.554 [0.000, 3.000],  loss: 361.884588, mse: 279386.349474, mean_q: 550.870057, mean_eps: 0.584245
 138725/300000: episode: 1346, duration: 0.621s, episode steps:  89, steps per second: 143, episode reward: -102.165, mean reward: -1.148 [-100.000, 14.536], mean action: 1.449 [0.000, 3.000],  loss: 233.704316, mse: 283316.140625, mean_q: 556.238276, mean_eps: 0.583960
 139010/300000: episode: 1347, duration: 2.045s, episode steps: 285, steps per second: 139, episode reward: -268.076, mean reward: -0.941 [-100.000, 17.404], mean action: 1.712 [0.000, 3.000],  loss: 353.198898, mse: 285086.393860, mean_q: 557.041370, mean_eps: 0.583399
 139094/300000: episode: 1348, duration: 0.730s, episode steps:  84, steps per second: 115, episode reward: -22.541, mean reward: -0.268 [-100.000, 14.424], mean action: 1.548 [0.000, 3.000],  loss: 574.632129, mse: 288073.295387, mean_q: 558.674364, mean_eps: 0.582846
 139163/300000: episode: 1349, duration: 0.502s, episode steps:  69, steps per second: 138, episode reward: -76.272, mean reward: -1.105 [-100.000,  9.695], mean action: 1.768 [0.000, 3.000],  loss: 197.075753, mse: 293582.486866, mean_q: 562.062259, mean_eps: 0.582616
 139252/300000: episode: 1350, duration: 0.606s, episode steps:  89, steps per second: 147, episode reward: -83.515, mean reward: -0.938 [-100.000, 13.284], mean action: 1.764 [0.000, 3.000],  loss: 321.739925, mse: 299935.589010, mean_q: 572.093576, mean_eps: 0.582379
 139364/300000: episode: 1351, duration: 0.791s, episode steps: 112, steps per second: 142, episode reward: -93.741, mean reward: -0.837 [-100.000, 14.035], mean action: 1.661 [0.000, 3.000],  loss: 309.686441, mse: 302559.149275, mean_q: 576.697482, mean_eps: 0.582077
 139461/300000: episode: 1352, duration: 0.659s, episode steps:  97, steps per second: 147, episode reward:  7.180, mean reward:  0.074 [-100.000, 14.544], mean action: 1.577 [0.000, 3.000],  loss: 282.136198, mse: 304130.248067, mean_q: 578.775189, mean_eps: 0.581764
 139550/300000: episode: 1353, duration: 0.599s, episode steps:  89, steps per second: 149, episode reward: -19.553, mean reward: -0.220 [-100.000, 19.986], mean action: 1.596 [0.000, 3.000],  loss: 385.066959, mse: 304474.960323, mean_q: 577.149460, mean_eps: 0.581485
 139645/300000: episode: 1354, duration: 0.748s, episode steps:  95, steps per second: 127, episode reward: -29.559, mean reward: -0.311 [-100.000,  7.794], mean action: 1.621 [0.000, 3.000],  loss: 315.228828, mse: 308787.508882, mean_q: 579.695681, mean_eps: 0.581209
 139760/300000: episode: 1355, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: -37.907, mean reward: -0.330 [-100.000, 16.341], mean action: 1.704 [0.000, 3.000],  loss: 390.471784, mse: 304767.261141, mean_q: 576.714811, mean_eps: 0.580894
 139850/300000: episode: 1356, duration: 0.610s, episode steps:  90, steps per second: 148, episode reward: -42.914, mean reward: -0.477 [-100.000,  7.930], mean action: 1.711 [0.000, 3.000],  loss: 481.676718, mse: 310281.172743, mean_q: 583.989622, mean_eps: 0.580587
 139951/300000: episode: 1357, duration: 0.724s, episode steps: 101, steps per second: 139, episode reward: -88.850, mean reward: -0.880 [-100.000,  9.735], mean action: 1.624 [0.000, 3.000],  loss: 327.607104, mse: 315282.187500, mean_q: 589.989290, mean_eps: 0.580300
 140053/300000: episode: 1358, duration: 0.689s, episode steps: 102, steps per second: 148, episode reward: -79.443, mean reward: -0.779 [-100.000,  9.691], mean action: 1.706 [0.000, 3.000],  loss: 391.092328, mse: 326666.012561, mean_q: 603.513010, mean_eps: 0.579995
 140154/300000: episode: 1359, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward:  1.899, mean reward:  0.019 [-100.000, 10.700], mean action: 1.713 [0.000, 3.000],  loss: 384.275321, mse: 319112.543317, mean_q: 593.134425, mean_eps: 0.579691
 140243/300000: episode: 1360, duration: 0.639s, episode steps:  89, steps per second: 139, episode reward: -87.789, mean reward: -0.986 [-100.000, 10.308], mean action: 1.697 [0.000, 3.000],  loss: 224.748405, mse: 316581.318118, mean_q: 589.804413, mean_eps: 0.579406
 140318/300000: episode: 1361, duration: 0.547s, episode steps:  75, steps per second: 137, episode reward: -27.716, mean reward: -0.370 [-100.000, 11.080], mean action: 1.640 [0.000, 3.000],  loss: 343.881158, mse: 314101.822917, mean_q: 589.825488, mean_eps: 0.579160
 140420/300000: episode: 1362, duration: 0.689s, episode steps: 102, steps per second: 148, episode reward: -79.288, mean reward: -0.777 [-100.000, 11.661], mean action: 1.461 [0.000, 3.000],  loss: 284.822761, mse: 306349.129136, mean_q: 576.371433, mean_eps: 0.578895
 140570/300000: episode: 1363, duration: 1.034s, episode steps: 150, steps per second: 145, episode reward: -118.712, mean reward: -0.791 [-100.000, 11.450], mean action: 1.540 [0.000, 3.000],  loss: 306.585994, mse: 306083.119583, mean_q: 577.678985, mean_eps: 0.578517
 140672/300000: episode: 1364, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -122.734, mean reward: -1.203 [-100.000, 10.522], mean action: 1.608 [0.000, 3.000],  loss: 234.357253, mse: 311802.322304, mean_q: 585.522434, mean_eps: 0.578138
 140758/300000: episode: 1365, duration: 0.875s, episode steps:  86, steps per second:  98, episode reward: -85.744, mean reward: -0.997 [-100.000,  8.238], mean action: 1.430 [0.000, 3.000],  loss: 449.270172, mse: 310481.131722, mean_q: 581.584254, mean_eps: 0.577856
 140844/300000: episode: 1366, duration: 1.012s, episode steps:  86, steps per second:  85, episode reward: -65.973, mean reward: -0.767 [-100.000, 13.278], mean action: 1.860 [0.000, 3.000],  loss: 429.076731, mse: 308032.120276, mean_q: 582.096871, mean_eps: 0.577599
 140935/300000: episode: 1367, duration: 0.742s, episode steps:  91, steps per second: 123, episode reward: -15.950, mean reward: -0.175 [-100.000,  9.852], mean action: 1.484 [0.000, 3.000],  loss: 381.907067, mse: 309432.955701, mean_q: 584.001044, mean_eps: 0.577333
 141026/300000: episode: 1368, duration: 0.684s, episode steps:  91, steps per second: 133, episode reward: -22.241, mean reward: -0.244 [-100.000, 15.019], mean action: 1.571 [0.000, 3.000],  loss: 349.009670, mse: 312479.749141, mean_q: 589.816691, mean_eps: 0.577060
 141113/300000: episode: 1369, duration: 0.766s, episode steps:  87, steps per second: 114, episode reward: -3.453, mean reward: -0.040 [-100.000,  8.163], mean action: 1.736 [0.000, 3.000],  loss: 366.991451, mse: 313701.976293, mean_q: 585.067592, mean_eps: 0.576793
 141204/300000: episode: 1370, duration: 0.721s, episode steps:  91, steps per second: 126, episode reward: -77.165, mean reward: -0.848 [-100.000, 20.864], mean action: 1.780 [0.000, 3.000],  loss: 296.894341, mse: 319181.178915, mean_q: 587.266954, mean_eps: 0.576526
 141313/300000: episode: 1371, duration: 0.838s, episode steps: 109, steps per second: 130, episode reward: -37.699, mean reward: -0.346 [-100.000, 18.103], mean action: 1.752 [0.000, 3.000],  loss: 538.417313, mse: 323904.018062, mean_q: 596.953544, mean_eps: 0.576226
 141445/300000: episode: 1372, duration: 0.996s, episode steps: 132, steps per second: 133, episode reward: -4.865, mean reward: -0.037 [-100.000,  6.122], mean action: 1.697 [0.000, 3.000],  loss: 266.676205, mse: 327457.014678, mean_q: 603.129546, mean_eps: 0.575865
 141541/300000: episode: 1373, duration: 0.716s, episode steps:  96, steps per second: 134, episode reward: -75.097, mean reward: -0.782 [-100.000, 14.592], mean action: 1.552 [0.000, 3.000],  loss: 452.532184, mse: 328714.144694, mean_q: 600.982643, mean_eps: 0.575523
 141640/300000: episode: 1374, duration: 0.767s, episode steps:  99, steps per second: 129, episode reward: 14.243, mean reward:  0.144 [-100.000, 13.227], mean action: 1.859 [0.000, 3.000],  loss: 432.590289, mse: 326247.072285, mean_q: 599.936940, mean_eps: 0.575230
 141724/300000: episode: 1375, duration: 0.579s, episode steps:  84, steps per second: 145, episode reward: -73.908, mean reward: -0.880 [-100.000,  8.367], mean action: 1.488 [0.000, 3.000],  loss: 284.883209, mse: 328979.821987, mean_q: 603.079132, mean_eps: 0.574956
 141805/300000: episode: 1376, duration: 0.546s, episode steps:  81, steps per second: 148, episode reward: -69.831, mean reward: -0.862 [-100.000, 11.403], mean action: 1.531 [0.000, 3.000],  loss: 296.977511, mse: 323580.648341, mean_q: 594.092243, mean_eps: 0.574708
 141889/300000: episode: 1377, duration: 0.621s, episode steps:  84, steps per second: 135, episode reward: -110.094, mean reward: -1.311 [-100.000, 11.425], mean action: 1.762 [0.000, 3.000],  loss: 482.037590, mse: 324397.145833, mean_q: 601.585896, mean_eps: 0.574461
 141976/300000: episode: 1378, duration: 0.618s, episode steps:  87, steps per second: 141, episode reward: -20.127, mean reward: -0.231 [-100.000, 12.626], mean action: 1.494 [0.000, 3.000],  loss: 322.411260, mse: 322677.158046, mean_q: 597.492969, mean_eps: 0.574204
 142096/300000: episode: 1379, duration: 0.807s, episode steps: 120, steps per second: 149, episode reward: -25.480, mean reward: -0.212 [-100.000, 11.955], mean action: 1.733 [0.000, 3.000],  loss: 251.431809, mse: 322914.270182, mean_q: 595.527436, mean_eps: 0.573894
 142217/300000: episode: 1380, duration: 0.824s, episode steps: 121, steps per second: 147, episode reward: -28.213, mean reward: -0.233 [-100.000, 10.587], mean action: 1.669 [0.000, 3.000],  loss: 576.016622, mse: 320411.321539, mean_q: 594.837202, mean_eps: 0.573532
 142289/300000: episode: 1381, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -83.210, mean reward: -1.156 [-100.000, 27.266], mean action: 1.611 [0.000, 3.000],  loss: 246.504618, mse: 313556.970486, mean_q: 586.179161, mean_eps: 0.573242
 142700/300000: episode: 1382, duration: 3.017s, episode steps: 411, steps per second: 136, episode reward: -247.838, mean reward: -0.603 [-100.000, 27.013], mean action: 1.779 [0.000, 3.000],  loss: 353.966513, mse: 321750.686093, mean_q: 593.362495, mean_eps: 0.572518
 142816/300000: episode: 1383, duration: 0.877s, episode steps: 116, steps per second: 132, episode reward: -45.597, mean reward: -0.393 [-100.000, 11.082], mean action: 1.629 [0.000, 3.000],  loss: 445.890479, mse: 338304.656789, mean_q: 611.614740, mean_eps: 0.571728
 142927/300000: episode: 1384, duration: 0.848s, episode steps: 111, steps per second: 131, episode reward: -31.473, mean reward: -0.284 [-100.000, 19.564], mean action: 1.577 [0.000, 3.000],  loss: 381.651093, mse: 335967.351211, mean_q: 607.688991, mean_eps: 0.571387
 143002/300000: episode: 1385, duration: 0.570s, episode steps:  75, steps per second: 132, episode reward: -47.261, mean reward: -0.630 [-100.000, 10.979], mean action: 1.880 [0.000, 3.000],  loss: 457.076193, mse: 335451.782500, mean_q: 607.821745, mean_eps: 0.571108
 143079/300000: episode: 1386, duration: 0.556s, episode steps:  77, steps per second: 138, episode reward: -47.140, mean reward: -0.612 [-100.000, 16.218], mean action: 1.468 [0.000, 3.000],  loss: 400.455246, mse: 332956.755885, mean_q: 609.446214, mean_eps: 0.570880
 143170/300000: episode: 1387, duration: 0.643s, episode steps:  91, steps per second: 141, episode reward: -78.552, mean reward: -0.863 [-100.000, 12.848], mean action: 1.670 [0.000, 3.000],  loss: 598.968670, mse: 337036.791209, mean_q: 611.285469, mean_eps: 0.570628
 143249/300000: episode: 1388, duration: 0.532s, episode steps:  79, steps per second: 149, episode reward: -81.612, mean reward: -1.033 [-100.000, 10.974], mean action: 1.658 [0.000, 3.000],  loss: 302.789593, mse: 339357.337816, mean_q: 612.363590, mean_eps: 0.570373
 143359/300000: episode: 1389, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -57.748, mean reward: -0.525 [-100.000, 16.934], mean action: 1.673 [0.000, 3.000],  loss: 243.848078, mse: 340456.339347, mean_q: 614.463542, mean_eps: 0.570090
 143464/300000: episode: 1390, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -72.168, mean reward: -0.687 [-100.000, 11.310], mean action: 1.495 [0.000, 3.000],  loss: 356.747592, mse: 340103.750595, mean_q: 611.276780, mean_eps: 0.569767
 143553/300000: episode: 1391, duration: 0.613s, episode steps:  89, steps per second: 145, episode reward: -8.460, mean reward: -0.095 [-100.000, 17.132], mean action: 1.708 [0.000, 3.000],  loss: 349.893655, mse: 338396.143258, mean_q: 608.966145, mean_eps: 0.569476
 143636/300000: episode: 1392, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -102.891, mean reward: -1.240 [-100.000, 11.892], mean action: 1.723 [0.000, 3.000],  loss: 257.757055, mse: 342841.816265, mean_q: 615.298869, mean_eps: 0.569218
 143753/300000: episode: 1393, duration: 0.831s, episode steps: 117, steps per second: 141, episode reward: -101.662, mean reward: -0.869 [-100.000, 19.811], mean action: 1.504 [0.000, 3.000],  loss: 477.217614, mse: 346962.415999, mean_q: 620.195203, mean_eps: 0.568918
 143837/300000: episode: 1394, duration: 0.574s, episode steps:  84, steps per second: 146, episode reward: -71.933, mean reward: -0.856 [-100.000,  6.254], mean action: 1.798 [0.000, 3.000],  loss: 243.281213, mse: 344516.790551, mean_q: 613.561344, mean_eps: 0.568616
 143959/300000: episode: 1395, duration: 0.842s, episode steps: 122, steps per second: 145, episode reward: -88.127, mean reward: -0.722 [-100.000, 16.268], mean action: 1.713 [0.000, 3.000],  loss: 290.918368, mse: 341762.854508, mean_q: 606.321479, mean_eps: 0.568308
 144062/300000: episode: 1396, duration: 0.730s, episode steps: 103, steps per second: 141, episode reward: -42.062, mean reward: -0.408 [-100.000,  9.814], mean action: 1.670 [0.000, 3.000],  loss: 380.463581, mse: 346709.807646, mean_q: 617.063349, mean_eps: 0.567970
 144154/300000: episode: 1397, duration: 0.628s, episode steps:  92, steps per second: 147, episode reward: -134.661, mean reward: -1.464 [-100.000, 10.316], mean action: 1.630 [0.000, 3.000],  loss: 398.786729, mse: 349970.953465, mean_q: 620.132804, mean_eps: 0.567677
 144238/300000: episode: 1398, duration: 0.586s, episode steps:  84, steps per second: 143, episode reward: -93.267, mean reward: -1.110 [-100.000, 21.702], mean action: 1.583 [0.000, 3.000],  loss: 196.827974, mse: 345287.575521, mean_q: 612.751556, mean_eps: 0.567414
 144313/300000: episode: 1399, duration: 0.540s, episode steps:  75, steps per second: 139, episode reward: -59.857, mean reward: -0.798 [-100.000,  6.342], mean action: 1.720 [0.000, 3.000],  loss: 437.000344, mse: 353110.293750, mean_q: 623.382084, mean_eps: 0.567175
 144423/300000: episode: 1400, duration: 0.794s, episode steps: 110, steps per second: 139, episode reward: -15.911, mean reward: -0.145 [-100.000, 20.098], mean action: 1.709 [0.000, 3.000],  loss: 308.462561, mse: 356364.034659, mean_q: 628.013389, mean_eps: 0.566898
 144544/300000: episode: 1401, duration: 0.871s, episode steps: 121, steps per second: 139, episode reward: -54.797, mean reward: -0.453 [-100.000,  6.483], mean action: 1.537 [0.000, 3.000],  loss: 349.797479, mse: 354456.100723, mean_q: 626.078380, mean_eps: 0.566551
 144627/300000: episode: 1402, duration: 0.592s, episode steps:  83, steps per second: 140, episode reward: -26.360, mean reward: -0.318 [-100.000,  8.670], mean action: 1.759 [0.000, 3.000],  loss: 424.208328, mse: 348685.888554, mean_q: 616.366425, mean_eps: 0.566245
 144744/300000: episode: 1403, duration: 0.810s, episode steps: 117, steps per second: 144, episode reward: -91.045, mean reward: -0.778 [-100.000, 12.052], mean action: 1.761 [0.000, 3.000],  loss: 450.946877, mse: 361416.598825, mean_q: 633.295798, mean_eps: 0.565945
 144849/300000: episode: 1404, duration: 0.702s, episode steps: 105, steps per second: 150, episode reward: -67.076, mean reward: -0.639 [-100.000,  9.595], mean action: 1.705 [0.000, 3.000],  loss: 312.619085, mse: 361737.150298, mean_q: 631.041934, mean_eps: 0.565612
 144922/300000: episode: 1405, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -49.210, mean reward: -0.674 [-100.000, 13.711], mean action: 1.521 [0.000, 3.000],  loss: 464.731864, mse: 359733.776541, mean_q: 628.421897, mean_eps: 0.565345
 145002/300000: episode: 1406, duration: 0.590s, episode steps:  80, steps per second: 136, episode reward: -110.441, mean reward: -1.381 [-100.000,  8.503], mean action: 1.525 [0.000, 3.000],  loss: 488.067804, mse: 364149.192188, mean_q: 632.360775, mean_eps: 0.565115
 145103/300000: episode: 1407, duration: 0.683s, episode steps: 101, steps per second: 148, episode reward:  3.944, mean reward:  0.039 [-100.000, 23.042], mean action: 1.644 [0.000, 3.000],  loss: 509.129177, mse: 366248.192760, mean_q: 636.976610, mean_eps: 0.564844
 145193/300000: episode: 1408, duration: 0.605s, episode steps:  90, steps per second: 149, episode reward: -70.728, mean reward: -0.786 [-100.000, 13.988], mean action: 1.722 [0.000, 3.000],  loss: 360.622419, mse: 365219.858333, mean_q: 637.395833, mean_eps: 0.564558
 145321/300000: episode: 1409, duration: 0.900s, episode steps: 128, steps per second: 142, episode reward: -50.580, mean reward: -0.395 [-100.000,  6.614], mean action: 1.656 [0.000, 3.000],  loss: 389.448565, mse: 371238.208008, mean_q: 642.918161, mean_eps: 0.564231
 145411/300000: episode: 1410, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -85.945, mean reward: -0.955 [-100.000, 36.210], mean action: 1.700 [0.000, 3.000],  loss: 488.989783, mse: 367520.953472, mean_q: 639.514601, mean_eps: 0.563904
 145532/300000: episode: 1411, duration: 0.811s, episode steps: 121, steps per second: 149, episode reward: -0.364, mean reward: -0.003 [-100.000, 20.649], mean action: 1.760 [0.000, 3.000],  loss: 242.935896, mse: 381451.956870, mean_q: 653.679300, mean_eps: 0.563587
 145628/300000: episode: 1412, duration: 0.677s, episode steps:  96, steps per second: 142, episode reward: -85.697, mean reward: -0.893 [-100.000,  8.346], mean action: 1.281 [0.000, 3.000],  loss: 411.667894, mse: 374260.492513, mean_q: 643.523408, mean_eps: 0.563261
 145726/300000: episode: 1413, duration: 0.681s, episode steps:  98, steps per second: 144, episode reward: -115.940, mean reward: -1.183 [-100.000, 12.883], mean action: 1.735 [0.000, 3.000],  loss: 541.511060, mse: 370342.042411, mean_q: 638.725398, mean_eps: 0.562970
 145893/300000: episode: 1414, duration: 1.137s, episode steps: 167, steps per second: 147, episode reward: 51.306, mean reward:  0.307 [-100.000, 20.629], mean action: 1.719 [0.000, 3.000],  loss: 440.177592, mse: 365885.066617, mean_q: 636.106612, mean_eps: 0.562573
 146023/300000: episode: 1415, duration: 0.912s, episode steps: 130, steps per second: 143, episode reward: -54.377, mean reward: -0.418 [-100.000,  9.246], mean action: 1.654 [0.000, 3.000],  loss: 295.467860, mse: 368718.228846, mean_q: 640.426178, mean_eps: 0.562127
 146108/300000: episode: 1416, duration: 0.579s, episode steps:  85, steps per second: 147, episode reward: -200.103, mean reward: -2.354 [-100.000, 40.251], mean action: 1.765 [0.000, 3.000],  loss: 291.048540, mse: 376561.491176, mean_q: 647.118071, mean_eps: 0.561805
 146239/300000: episode: 1417, duration: 0.904s, episode steps: 131, steps per second: 145, episode reward: -23.911, mean reward: -0.183 [-100.000, 15.982], mean action: 1.573 [0.000, 3.000],  loss: 367.679981, mse: 374755.835162, mean_q: 643.941720, mean_eps: 0.561481
 146418/300000: episode: 1418, duration: 1.213s, episode steps: 179, steps per second: 148, episode reward:  5.105, mean reward:  0.029 [-100.000, 15.842], mean action: 1.698 [0.000, 3.000],  loss: 428.601972, mse: 381235.308485, mean_q: 651.740525, mean_eps: 0.561016
 146524/300000: episode: 1419, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -54.059, mean reward: -0.510 [-100.000,  6.454], mean action: 1.377 [0.000, 3.000],  loss: 506.120309, mse: 378890.251179, mean_q: 649.711018, mean_eps: 0.560589
 146613/300000: episode: 1420, duration: 0.612s, episode steps:  89, steps per second: 145, episode reward: -65.560, mean reward: -0.737 [-100.000, 12.857], mean action: 1.618 [0.000, 3.000],  loss: 531.752439, mse: 387666.003511, mean_q: 657.073072, mean_eps: 0.560296
 146757/300000: episode: 1421, duration: 0.970s, episode steps: 144, steps per second: 149, episode reward: -41.208, mean reward: -0.286 [-100.000,  8.183], mean action: 1.542 [0.000, 3.000],  loss: 400.495973, mse: 393409.054470, mean_q: 664.372125, mean_eps: 0.559947
 146844/300000: episode: 1422, duration: 0.611s, episode steps:  87, steps per second: 142, episode reward: -59.294, mean reward: -0.682 [-100.000, 19.480], mean action: 1.506 [0.000, 3.000],  loss: 320.344134, mse: 397924.899066, mean_q: 666.171161, mean_eps: 0.559600
 146927/300000: episode: 1423, duration: 0.567s, episode steps:  83, steps per second: 146, episode reward: -78.442, mean reward: -0.945 [-100.000,  7.180], mean action: 1.747 [0.000, 3.000],  loss: 470.010921, mse: 400723.504142, mean_q: 669.313121, mean_eps: 0.559345
 147042/300000: episode: 1424, duration: 0.774s, episode steps: 115, steps per second: 149, episode reward: -57.608, mean reward: -0.501 [-100.000,  9.443], mean action: 1.670 [0.000, 3.000],  loss: 418.740490, mse: 399344.865761, mean_q: 667.416116, mean_eps: 0.559048
 147130/300000: episode: 1425, duration: 0.596s, episode steps:  88, steps per second: 148, episode reward: -35.546, mean reward: -0.404 [-100.000, 15.822], mean action: 1.568 [0.000, 3.000],  loss: 617.523317, mse: 406615.888849, mean_q: 673.638596, mean_eps: 0.558743
 147248/300000: episode: 1426, duration: 0.899s, episode steps: 118, steps per second: 131, episode reward: -9.781, mean reward: -0.083 [-100.000, 22.700], mean action: 1.593 [0.000, 3.000],  loss: 456.249330, mse: 408306.821504, mean_q: 676.724512, mean_eps: 0.558435
 147354/300000: episode: 1427, duration: 0.836s, episode steps: 106, steps per second: 127, episode reward: -35.341, mean reward: -0.333 [-100.000, 14.443], mean action: 1.500 [0.000, 3.000],  loss: 353.645725, mse: 401057.085495, mean_q: 667.454617, mean_eps: 0.558099
 147481/300000: episode: 1428, duration: 0.981s, episode steps: 127, steps per second: 129, episode reward:  8.679, mean reward:  0.068 [-100.000, 30.458], mean action: 1.748 [0.000, 3.000],  loss: 443.266286, mse: 406420.770669, mean_q: 675.968493, mean_eps: 0.557749
 147605/300000: episode: 1429, duration: 0.861s, episode steps: 124, steps per second: 144, episode reward: -119.408, mean reward: -0.963 [-100.000,  7.363], mean action: 1.637 [0.000, 3.000],  loss: 399.995316, mse: 409262.383569, mean_q: 678.712813, mean_eps: 0.557373
 147712/300000: episode: 1430, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -120.362, mean reward: -1.125 [-100.000,  4.010], mean action: 1.617 [0.000, 3.000],  loss: 397.275678, mse: 406433.858061, mean_q: 673.517429, mean_eps: 0.557026
 147809/300000: episode: 1431, duration: 0.765s, episode steps:  97, steps per second: 127, episode reward: -72.472, mean reward: -0.747 [-100.000, 10.877], mean action: 1.536 [0.000, 3.000],  loss: 375.291740, mse: 413013.340528, mean_q: 682.135640, mean_eps: 0.556720
 148809/300000: episode: 1432, duration: 7.767s, episode steps: 1000, steps per second: 129, episode reward: 29.054, mean reward:  0.029 [-24.670, 64.304], mean action: 1.617 [0.000, 3.000],  loss: 452.826062, mse: 417889.021500, mean_q: 685.078282, mean_eps: 0.555074
 148909/300000: episode: 1433, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -71.271, mean reward: -0.713 [-100.000,  7.113], mean action: 1.620 [0.000, 3.000],  loss: 349.412094, mse: 429951.227500, mean_q: 696.092702, mean_eps: 0.553425
 149018/300000: episode: 1434, duration: 0.827s, episode steps: 109, steps per second: 132, episode reward: -43.704, mean reward: -0.401 [-100.000, 11.854], mean action: 1.725 [0.000, 3.000],  loss: 505.503395, mse: 428344.400803, mean_q: 692.030127, mean_eps: 0.553111
 149157/300000: episode: 1435, duration: 0.962s, episode steps: 139, steps per second: 144, episode reward: -78.339, mean reward: -0.564 [-100.000,  6.805], mean action: 1.842 [0.000, 3.000],  loss: 660.090423, mse: 438470.591052, mean_q: 702.032066, mean_eps: 0.552739
 149271/300000: episode: 1436, duration: 0.797s, episode steps: 114, steps per second: 143, episode reward: -78.503, mean reward: -0.689 [-100.000, 12.933], mean action: 1.675 [0.000, 3.000],  loss: 508.657276, mse: 429719.608004, mean_q: 692.162781, mean_eps: 0.552360
 149411/300000: episode: 1437, duration: 1.028s, episode steps: 140, steps per second: 136, episode reward: -31.754, mean reward: -0.227 [-100.000, 12.305], mean action: 1.743 [0.000, 3.000],  loss: 458.284798, mse: 441589.686607, mean_q: 705.907353, mean_eps: 0.551979
 149478/300000: episode: 1438, duration: 0.514s, episode steps:  67, steps per second: 130, episode reward:  0.859, mean reward:  0.013 [-100.000, 11.355], mean action: 1.537 [0.000, 3.000],  loss: 519.452068, mse: 453873.694030, mean_q: 717.803093, mean_eps: 0.551668
 149573/300000: episode: 1439, duration: 0.649s, episode steps:  95, steps per second: 146, episode reward: -47.122, mean reward: -0.496 [-100.000, 23.068], mean action: 1.432 [0.000, 3.000],  loss: 448.769143, mse: 452167.037171, mean_q: 710.903455, mean_eps: 0.551425
 149659/300000: episode: 1440, duration: 0.586s, episode steps:  86, steps per second: 147, episode reward: -13.171, mean reward: -0.153 [-100.000,  9.973], mean action: 1.547 [0.000, 3.000],  loss: 428.451466, mse: 452999.153343, mean_q: 715.107024, mean_eps: 0.551153
 149769/300000: episode: 1441, duration: 0.781s, episode steps: 110, steps per second: 141, episode reward: -88.355, mean reward: -0.803 [-100.000, 27.219], mean action: 1.818 [0.000, 3.000],  loss: 419.888948, mse: 445677.802841, mean_q: 704.713498, mean_eps: 0.550860
 149896/300000: episode: 1442, duration: 0.871s, episode steps: 127, steps per second: 146, episode reward: -56.215, mean reward: -0.443 [-100.000, 10.902], mean action: 1.606 [0.000, 3.000],  loss: 329.920948, mse: 455121.736713, mean_q: 718.554019, mean_eps: 0.550504
 149981/300000: episode: 1443, duration: 0.580s, episode steps:  85, steps per second: 147, episode reward: -39.685, mean reward: -0.467 [-100.000, 12.449], mean action: 1.529 [0.000, 3.000],  loss: 433.099759, mse: 443262.340809, mean_q: 702.780410, mean_eps: 0.550186
 150076/300000: episode: 1444, duration: 0.689s, episode steps:  95, steps per second: 138, episode reward: -66.962, mean reward: -0.705 [-100.000, 11.122], mean action: 1.642 [0.000, 3.000],  loss: 341.094516, mse: 441571.899671, mean_q: 702.940871, mean_eps: 0.549916
 150157/300000: episode: 1445, duration: 0.644s, episode steps:  81, steps per second: 126, episode reward: -88.021, mean reward: -1.087 [-100.000, 10.072], mean action: 1.691 [0.000, 3.000],  loss: 333.390049, mse: 447449.114969, mean_q: 704.812582, mean_eps: 0.549652
 150285/300000: episode: 1446, duration: 1.033s, episode steps: 128, steps per second: 124, episode reward: 19.746, mean reward:  0.154 [-100.000, 18.014], mean action: 1.711 [0.000, 3.000],  loss: 363.819411, mse: 447922.413086, mean_q: 706.487246, mean_eps: 0.549339
 150403/300000: episode: 1447, duration: 0.975s, episode steps: 118, steps per second: 121, episode reward: -95.563, mean reward: -0.810 [-100.000, 13.346], mean action: 1.644 [0.000, 3.000],  loss: 551.226724, mse: 450841.258210, mean_q: 707.285102, mean_eps: 0.548969
 150489/300000: episode: 1448, duration: 0.676s, episode steps:  86, steps per second: 127, episode reward: -74.505, mean reward: -0.866 [-100.000,  6.358], mean action: 1.802 [0.000, 3.000],  loss: 405.970847, mse: 454165.886991, mean_q: 713.152860, mean_eps: 0.548663
 150560/300000: episode: 1449, duration: 0.577s, episode steps:  71, steps per second: 123, episode reward: -57.759, mean reward: -0.814 [-100.000,  7.466], mean action: 1.704 [0.000, 3.000],  loss: 240.899035, mse: 460525.473592, mean_q: 717.540793, mean_eps: 0.548428
 150638/300000: episode: 1450, duration: 0.667s, episode steps:  78, steps per second: 117, episode reward: -116.450, mean reward: -1.493 [-100.000, 16.466], mean action: 1.564 [0.000, 3.000],  loss: 413.703636, mse: 452123.097756, mean_q: 711.083700, mean_eps: 0.548205
 150753/300000: episode: 1451, duration: 0.856s, episode steps: 115, steps per second: 134, episode reward: -62.724, mean reward: -0.545 [-100.000, 10.436], mean action: 1.626 [0.000, 3.000],  loss: 421.118456, mse: 442895.600543, mean_q: 700.696327, mean_eps: 0.547915
 150864/300000: episode: 1452, duration: 0.762s, episode steps: 111, steps per second: 146, episode reward: -38.621, mean reward: -0.348 [-100.000, 10.347], mean action: 1.802 [0.000, 3.000],  loss: 316.309050, mse: 447256.428209, mean_q: 707.308507, mean_eps: 0.547576
 150946/300000: episode: 1453, duration: 0.602s, episode steps:  82, steps per second: 136, episode reward: -63.389, mean reward: -0.773 [-100.000,  6.883], mean action: 1.756 [0.000, 3.000],  loss: 397.325861, mse: 436046.733994, mean_q: 696.080916, mean_eps: 0.547286
 151042/300000: episode: 1454, duration: 0.665s, episode steps:  96, steps per second: 144, episode reward: -76.956, mean reward: -0.802 [-100.000, 11.029], mean action: 1.562 [0.000, 3.000],  loss: 322.596333, mse: 432961.688151, mean_q: 693.691047, mean_eps: 0.547019
 151160/300000: episode: 1455, duration: 0.800s, episode steps: 118, steps per second: 148, episode reward:  7.522, mean reward:  0.064 [-100.000, 15.157], mean action: 1.780 [0.000, 3.000],  loss: 483.682548, mse: 437338.539460, mean_q: 703.399551, mean_eps: 0.546698
 151248/300000: episode: 1456, duration: 0.628s, episode steps:  88, steps per second: 140, episode reward: -95.086, mean reward: -1.081 [-100.000, 11.462], mean action: 1.625 [0.000, 3.000],  loss: 366.023534, mse: 428046.771307, mean_q: 692.742715, mean_eps: 0.546390
 151389/300000: episode: 1457, duration: 0.979s, episode steps: 141, steps per second: 144, episode reward: -92.384, mean reward: -0.655 [-100.000, 16.933], mean action: 1.801 [0.000, 3.000],  loss: 424.156558, mse: 419548.084885, mean_q: 685.060187, mean_eps: 0.546046
 151464/300000: episode: 1458, duration: 0.505s, episode steps:  75, steps per second: 149, episode reward: -37.214, mean reward: -0.496 [-100.000, 12.762], mean action: 1.573 [0.000, 3.000],  loss: 381.264818, mse: 432223.985417, mean_q: 697.101983, mean_eps: 0.545722
 151552/300000: episode: 1459, duration: 0.630s, episode steps:  88, steps per second: 140, episode reward: -28.323, mean reward: -0.322 [-100.000, 22.032], mean action: 1.648 [0.000, 3.000],  loss: 228.244488, mse: 428159.879972, mean_q: 692.158814, mean_eps: 0.545478
 151635/300000: episode: 1460, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -76.938, mean reward: -0.927 [-100.000, 10.545], mean action: 1.446 [0.000, 3.000],  loss: 400.724291, mse: 434893.256401, mean_q: 701.349534, mean_eps: 0.545221
 151714/300000: episode: 1461, duration: 0.602s, episode steps:  79, steps per second: 131, episode reward: -58.351, mean reward: -0.739 [-100.000, 11.697], mean action: 1.595 [0.000, 3.000],  loss: 455.363024, mse: 427664.746044, mean_q: 694.296490, mean_eps: 0.544978
 151805/300000: episode: 1462, duration: 0.710s, episode steps:  91, steps per second: 128, episode reward: -82.088, mean reward: -0.902 [-100.000, 16.035], mean action: 1.484 [0.000, 3.000],  loss: 423.323094, mse: 418827.242102, mean_q: 681.376467, mean_eps: 0.544723
 151888/300000: episode: 1463, duration: 0.669s, episode steps:  83, steps per second: 124, episode reward: -80.657, mean reward: -0.972 [-100.000,  7.623], mean action: 1.542 [0.000, 3.000],  loss: 320.896251, mse: 420344.326054, mean_q: 681.528168, mean_eps: 0.544462
 151988/300000: episode: 1464, duration: 0.775s, episode steps: 100, steps per second: 129, episode reward: -38.202, mean reward: -0.382 [-100.000, 11.926], mean action: 1.560 [0.000, 3.000],  loss: 419.871887, mse: 423037.977500, mean_q: 687.828379, mean_eps: 0.544187
 152065/300000: episode: 1465, duration: 0.571s, episode steps:  77, steps per second: 135, episode reward: -56.292, mean reward: -0.731 [-100.000,  6.597], mean action: 1.519 [0.000, 3.000],  loss: 532.330489, mse: 426371.439529, mean_q: 691.378394, mean_eps: 0.543922
 152160/300000: episode: 1466, duration: 0.731s, episode steps:  95, steps per second: 130, episode reward: -27.397, mean reward: -0.288 [-100.000,  9.542], mean action: 1.705 [0.000, 3.000],  loss: 441.964374, mse: 425365.879276, mean_q: 692.994644, mean_eps: 0.543664
 152284/300000: episode: 1467, duration: 0.851s, episode steps: 124, steps per second: 146, episode reward: -135.619, mean reward: -1.094 [-100.000,  3.884], mean action: 1.815 [0.000, 3.000],  loss: 400.979529, mse: 422111.165071, mean_q: 688.394948, mean_eps: 0.543335
 152363/300000: episode: 1468, duration: 0.551s, episode steps:  79, steps per second: 143, episode reward: -68.223, mean reward: -0.864 [-100.000, 11.465], mean action: 1.608 [0.000, 3.000],  loss: 478.240873, mse: 428198.877373, mean_q: 695.135421, mean_eps: 0.543031
 152446/300000: episode: 1469, duration: 0.612s, episode steps:  83, steps per second: 136, episode reward: -45.343, mean reward: -0.546 [-100.000,  8.341], mean action: 1.590 [0.000, 3.000],  loss: 508.935693, mse: 424350.920557, mean_q: 689.955246, mean_eps: 0.542788
 152574/300000: episode: 1470, duration: 0.890s, episode steps: 128, steps per second: 144, episode reward: -27.167, mean reward: -0.212 [-100.000,  8.566], mean action: 1.633 [0.000, 3.000],  loss: 401.239452, mse: 422031.184082, mean_q: 689.213387, mean_eps: 0.542471
 152678/300000: episode: 1471, duration: 0.723s, episode steps: 104, steps per second: 144, episode reward: -115.778, mean reward: -1.113 [-100.000, 10.476], mean action: 1.702 [0.000, 3.000],  loss: 509.194656, mse: 409912.972656, mean_q: 677.632031, mean_eps: 0.542123
 152789/300000: episode: 1472, duration: 0.822s, episode steps: 111, steps per second: 135, episode reward: -23.411, mean reward: -0.211 [-100.000, 12.270], mean action: 1.559 [0.000, 3.000],  loss: 291.178723, mse: 411820.150901, mean_q: 678.893447, mean_eps: 0.541801
 152875/300000: episode: 1473, duration: 0.708s, episode steps:  86, steps per second: 121, episode reward: -148.282, mean reward: -1.724 [-100.000,  9.856], mean action: 1.756 [0.000, 3.000],  loss: 328.991997, mse: 423786.314317, mean_q: 692.590615, mean_eps: 0.541505
 152983/300000: episode: 1474, duration: 0.969s, episode steps: 108, steps per second: 111, episode reward: -87.003, mean reward: -0.806 [-100.000,  5.809], mean action: 1.602 [0.000, 3.000],  loss: 484.083822, mse: 420320.633970, mean_q: 688.086733, mean_eps: 0.541215
 153063/300000: episode: 1475, duration: 0.668s, episode steps:  80, steps per second: 120, episode reward: -103.811, mean reward: -1.298 [-100.000,  3.308], mean action: 1.550 [0.000, 3.000],  loss: 384.732939, mse: 417867.662109, mean_q: 681.702847, mean_eps: 0.540933
 153163/300000: episode: 1476, duration: 0.752s, episode steps: 100, steps per second: 133, episode reward: 17.721, mean reward:  0.177 [-100.000, 16.351], mean action: 1.600 [0.000, 3.000],  loss: 410.230191, mse: 410699.248437, mean_q: 675.327451, mean_eps: 0.540663
 153288/300000: episode: 1477, duration: 1.049s, episode steps: 125, steps per second: 119, episode reward: -15.529, mean reward: -0.124 [-100.000, 10.587], mean action: 1.720 [0.000, 3.000],  loss: 391.106621, mse: 414373.693250, mean_q: 684.978347, mean_eps: 0.540325
 153407/300000: episode: 1478, duration: 0.961s, episode steps: 119, steps per second: 124, episode reward: -24.067, mean reward: -0.202 [-100.000, 13.695], mean action: 1.655 [0.000, 3.000],  loss: 409.980665, mse: 415693.267595, mean_q: 682.633429, mean_eps: 0.539959
 153480/300000: episode: 1479, duration: 0.622s, episode steps:  73, steps per second: 117, episode reward: -12.194, mean reward: -0.167 [-100.000, 11.009], mean action: 1.753 [0.000, 3.000],  loss: 331.841997, mse: 414235.312928, mean_q: 681.658917, mean_eps: 0.539671
 153575/300000: episode: 1480, duration: 0.758s, episode steps:  95, steps per second: 125, episode reward: -47.878, mean reward: -0.504 [-100.000, 13.661], mean action: 1.863 [0.000, 3.000],  loss: 325.037095, mse: 412306.681250, mean_q: 675.155934, mean_eps: 0.539419
 153690/300000: episode: 1481, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: -51.490, mean reward: -0.448 [-100.000, 13.538], mean action: 1.696 [0.000, 3.000],  loss: 445.380342, mse: 410096.346739, mean_q: 674.662588, mean_eps: 0.539104
 153777/300000: episode: 1482, duration: 0.620s, episode steps:  87, steps per second: 140, episode reward: -27.388, mean reward: -0.315 [-100.000, 12.961], mean action: 1.736 [0.000, 3.000],  loss: 440.393025, mse: 409160.940014, mean_q: 675.551643, mean_eps: 0.538801
 153859/300000: episode: 1483, duration: 0.627s, episode steps:  82, steps per second: 131, episode reward: -75.555, mean reward: -0.921 [-100.000, 10.329], mean action: 1.634 [0.000, 3.000],  loss: 310.957339, mse: 410647.696265, mean_q: 677.452364, mean_eps: 0.538548
 153955/300000: episode: 1484, duration: 0.750s, episode steps:  96, steps per second: 128, episode reward: -11.161, mean reward: -0.116 [-100.000,  9.911], mean action: 1.760 [0.000, 3.000],  loss: 452.568726, mse: 410209.519857, mean_q: 676.419174, mean_eps: 0.538281
 154065/300000: episode: 1485, duration: 0.777s, episode steps: 110, steps per second: 141, episode reward: -184.096, mean reward: -1.674 [-100.000, 25.951], mean action: 1.827 [0.000, 3.000],  loss: 593.923252, mse: 409681.574716, mean_q: 676.833906, mean_eps: 0.537971
 154157/300000: episode: 1486, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -9.227, mean reward: -0.100 [-100.000, 13.443], mean action: 1.826 [0.000, 3.000],  loss: 369.839231, mse: 407386.848845, mean_q: 674.195117, mean_eps: 0.537668
 154282/300000: episode: 1487, duration: 0.858s, episode steps: 125, steps per second: 146, episode reward: -33.682, mean reward: -0.269 [-100.000,  9.531], mean action: 1.792 [0.000, 3.000],  loss: 461.563386, mse: 400852.534000, mean_q: 668.580516, mean_eps: 0.537343
 154404/300000: episode: 1488, duration: 0.877s, episode steps: 122, steps per second: 139, episode reward: -37.112, mean reward: -0.304 [-100.000, 19.153], mean action: 1.598 [0.000, 3.000],  loss: 374.155049, mse: 397058.712859, mean_q: 664.299989, mean_eps: 0.536972
 154493/300000: episode: 1489, duration: 0.605s, episode steps:  89, steps per second: 147, episode reward: -64.112, mean reward: -0.720 [-100.000, 12.751], mean action: 1.461 [0.000, 3.000],  loss: 288.348968, mse: 388294.079705, mean_q: 653.788620, mean_eps: 0.536656
 154580/300000: episode: 1490, duration: 0.596s, episode steps:  87, steps per second: 146, episode reward: -18.713, mean reward: -0.215 [-100.000, 10.806], mean action: 1.805 [0.000, 3.000],  loss: 312.041274, mse: 391934.031968, mean_q: 661.648717, mean_eps: 0.536392
 154669/300000: episode: 1491, duration: 0.601s, episode steps:  89, steps per second: 148, episode reward: -111.765, mean reward: -1.256 [-100.000, 12.051], mean action: 1.528 [0.000, 3.000],  loss: 389.420496, mse: 390759.103581, mean_q: 657.606694, mean_eps: 0.536128
 154811/300000: episode: 1492, duration: 0.995s, episode steps: 142, steps per second: 143, episode reward: 25.363, mean reward:  0.179 [-100.000, 15.843], mean action: 1.817 [0.000, 3.000],  loss: 369.733896, mse: 395665.021127, mean_q: 665.025262, mean_eps: 0.535782
 154899/300000: episode: 1493, duration: 0.605s, episode steps:  88, steps per second: 146, episode reward: -33.775, mean reward: -0.384 [-100.000,  9.878], mean action: 1.636 [0.000, 3.000],  loss: 360.870941, mse: 390893.518466, mean_q: 660.056966, mean_eps: 0.535436
 155008/300000: episode: 1494, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward:  0.763, mean reward:  0.007 [-100.000, 16.999], mean action: 1.789 [0.000, 3.000],  loss: 238.665725, mse: 389789.712729, mean_q: 660.623620, mean_eps: 0.535141
 155113/300000: episode: 1495, duration: 0.727s, episode steps: 105, steps per second: 144, episode reward: -29.943, mean reward: -0.285 [-100.000, 11.839], mean action: 1.562 [0.000, 3.000],  loss: 394.428259, mse: 400703.180357, mean_q: 670.572352, mean_eps: 0.534820
 155204/300000: episode: 1496, duration: 0.619s, episode steps:  91, steps per second: 147, episode reward: -41.397, mean reward: -0.455 [-100.000, 20.503], mean action: 1.791 [0.000, 3.000],  loss: 401.236100, mse: 400276.942995, mean_q: 669.908301, mean_eps: 0.534526
 155300/300000: episode: 1497, duration: 0.653s, episode steps:  96, steps per second: 147, episode reward: -40.558, mean reward: -0.422 [-100.000, 13.188], mean action: 1.823 [0.000, 3.000],  loss: 354.167615, mse: 407473.055013, mean_q: 677.860569, mean_eps: 0.534246
 155418/300000: episode: 1498, duration: 0.848s, episode steps: 118, steps per second: 139, episode reward: -28.526, mean reward: -0.242 [-100.000, 13.370], mean action: 1.653 [0.000, 3.000],  loss: 437.812867, mse: 397435.483316, mean_q: 665.427090, mean_eps: 0.533924
 155541/300000: episode: 1499, duration: 0.839s, episode steps: 123, steps per second: 147, episode reward: -106.166, mean reward: -0.863 [-100.000,  9.436], mean action: 1.789 [0.000, 3.000],  loss: 587.389985, mse: 393788.812246, mean_q: 660.949517, mean_eps: 0.533563
 155630/300000: episode: 1500, duration: 0.607s, episode steps:  89, steps per second: 147, episode reward: -57.152, mean reward: -0.642 [-100.000,  9.087], mean action: 1.730 [0.000, 3.000],  loss: 519.035118, mse: 391325.931882, mean_q: 658.467103, mean_eps: 0.533245
 155727/300000: episode: 1501, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -23.376, mean reward: -0.241 [-100.000,  9.955], mean action: 1.629 [0.000, 3.000],  loss: 436.472603, mse: 393917.503222, mean_q: 660.064784, mean_eps: 0.532966
 155820/300000: episode: 1502, duration: 0.640s, episode steps:  93, steps per second: 145, episode reward: -194.575, mean reward: -2.092 [-100.000, 59.698], mean action: 1.559 [0.000, 3.000],  loss: 399.792429, mse: 391009.574597, mean_q: 656.654924, mean_eps: 0.532681
 155904/300000: episode: 1503, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -8.835, mean reward: -0.105 [-100.000, 11.003], mean action: 1.833 [0.000, 3.000],  loss: 361.859533, mse: 395258.161458, mean_q: 660.935476, mean_eps: 0.532416
 156021/300000: episode: 1504, duration: 0.829s, episode steps: 117, steps per second: 141, episode reward: -59.552, mean reward: -0.509 [-100.000, 42.791], mean action: 1.709 [0.000, 3.000],  loss: 470.072685, mse: 407129.686699, mean_q: 674.343332, mean_eps: 0.532114
 156144/300000: episode: 1505, duration: 0.837s, episode steps: 123, steps per second: 147, episode reward: -75.303, mean reward: -0.612 [-100.000, 12.036], mean action: 1.553 [0.000, 3.000],  loss: 422.149233, mse: 406837.915142, mean_q: 675.608177, mean_eps: 0.531754
 156237/300000: episode: 1506, duration: 0.623s, episode steps:  93, steps per second: 149, episode reward: -29.816, mean reward: -0.321 [-100.000, 14.589], mean action: 1.774 [0.000, 3.000],  loss: 372.338265, mse: 402939.198253, mean_q: 667.391011, mean_eps: 0.531430
 156326/300000: episode: 1507, duration: 0.661s, episode steps:  89, steps per second: 135, episode reward: -128.096, mean reward: -1.439 [-100.000, 13.755], mean action: 1.506 [0.000, 3.000],  loss: 314.830610, mse: 414182.721208, mean_q: 681.796553, mean_eps: 0.531157
 156410/300000: episode: 1508, duration: 0.568s, episode steps:  84, steps per second: 148, episode reward:  7.644, mean reward:  0.091 [-100.000, 16.196], mean action: 1.631 [0.000, 3.000],  loss: 276.705990, mse: 416788.271205, mean_q: 681.639002, mean_eps: 0.530898
 156519/300000: episode: 1509, duration: 0.739s, episode steps: 109, steps per second: 147, episode reward: -172.548, mean reward: -1.583 [-100.000, 98.219], mean action: 1.789 [0.000, 3.000],  loss: 265.923986, mse: 416342.655963, mean_q: 682.198816, mean_eps: 0.530608
 156593/300000: episode: 1510, duration: 0.528s, episode steps:  74, steps per second: 140, episode reward: -35.122, mean reward: -0.475 [-100.000,  6.443], mean action: 1.378 [0.000, 3.000],  loss: 471.397749, mse: 411323.380912, mean_q: 681.749786, mean_eps: 0.530334
 156707/300000: episode: 1511, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -35.035, mean reward: -0.307 [-100.000, 18.396], mean action: 1.623 [0.000, 3.000],  loss: 358.005464, mse: 405407.388706, mean_q: 672.530815, mean_eps: 0.530052
 156772/300000: episode: 1512, duration: 0.449s, episode steps:  65, steps per second: 145, episode reward: -87.669, mean reward: -1.349 [-100.000,  8.788], mean action: 1.908 [0.000, 3.000],  loss: 277.850726, mse: 406337.410096, mean_q: 678.529749, mean_eps: 0.529783
 156856/300000: episode: 1513, duration: 0.580s, episode steps:  84, steps per second: 145, episode reward: -65.899, mean reward: -0.785 [-100.000,  8.372], mean action: 1.560 [0.000, 3.000],  loss: 399.037259, mse: 403894.611235, mean_q: 674.671350, mean_eps: 0.529559
 156920/300000: episode: 1514, duration: 0.487s, episode steps:  64, steps per second: 131, episode reward: -30.820, mean reward: -0.482 [-100.000, 12.141], mean action: 1.781 [0.000, 3.000],  loss: 309.404812, mse: 400823.895996, mean_q: 674.211202, mean_eps: 0.529338
 156993/300000: episode: 1515, duration: 0.493s, episode steps:  73, steps per second: 148, episode reward: -0.689, mean reward: -0.009 [-100.000, 12.532], mean action: 1.329 [0.000, 3.000],  loss: 393.615100, mse: 394475.078767, mean_q: 662.954173, mean_eps: 0.529132
 157064/300000: episode: 1516, duration: 0.478s, episode steps:  71, steps per second: 149, episode reward: -89.010, mean reward: -1.254 [-100.000,  8.167], mean action: 1.437 [0.000, 3.000],  loss: 371.725131, mse: 398822.516285, mean_q: 670.511400, mean_eps: 0.528916
 157153/300000: episode: 1517, duration: 0.606s, episode steps:  89, steps per second: 147, episode reward: -13.006, mean reward: -0.146 [-100.000, 17.768], mean action: 1.753 [0.000, 3.000],  loss: 398.256048, mse: 398751.850772, mean_q: 670.097606, mean_eps: 0.528676
 157225/300000: episode: 1518, duration: 0.551s, episode steps:  72, steps per second: 131, episode reward: -34.177, mean reward: -0.475 [-100.000, 23.168], mean action: 1.792 [0.000, 3.000],  loss: 488.549051, mse: 406524.369358, mean_q: 678.046689, mean_eps: 0.528435
 157341/300000: episode: 1519, duration: 0.814s, episode steps: 116, steps per second: 143, episode reward: -53.658, mean reward: -0.463 [-100.000, 10.017], mean action: 1.784 [0.000, 3.000],  loss: 325.482619, mse: 407286.084860, mean_q: 675.386601, mean_eps: 0.528152
 157468/300000: episode: 1520, duration: 0.862s, episode steps: 127, steps per second: 147, episode reward: 28.765, mean reward:  0.226 [-100.000, 13.338], mean action: 1.622 [0.000, 3.000],  loss: 315.450039, mse: 404188.398130, mean_q: 671.964929, mean_eps: 0.527788
 157539/300000: episode: 1521, duration: 0.531s, episode steps:  71, steps per second: 134, episode reward: -64.823, mean reward: -0.913 [-100.000,  7.422], mean action: 1.761 [0.000, 3.000],  loss: 307.183323, mse: 400365.385123, mean_q: 665.610215, mean_eps: 0.527491
 157646/300000: episode: 1522, duration: 0.736s, episode steps: 107, steps per second: 145, episode reward: -78.126, mean reward: -0.730 [-100.000, 11.166], mean action: 1.673 [0.000, 3.000],  loss: 297.706897, mse: 401191.598423, mean_q: 665.703970, mean_eps: 0.527224
 157728/300000: episode: 1523, duration: 0.557s, episode steps:  82, steps per second: 147, episode reward: -24.355, mean reward: -0.297 [-100.000, 14.778], mean action: 1.951 [0.000, 3.000],  loss: 389.097823, mse: 402685.480183, mean_q: 667.323292, mean_eps: 0.526941
 157842/300000: episode: 1524, duration: 0.802s, episode steps: 114, steps per second: 142, episode reward: -27.950, mean reward: -0.245 [-100.000, 19.101], mean action: 1.746 [0.000, 3.000],  loss: 443.813656, mse: 408956.257950, mean_q: 678.052067, mean_eps: 0.526647
 157946/300000: episode: 1525, duration: 0.725s, episode steps: 104, steps per second: 143, episode reward: -39.058, mean reward: -0.376 [-100.000, 12.146], mean action: 1.663 [0.000, 3.000],  loss: 368.269503, mse: 409260.554688, mean_q: 678.570668, mean_eps: 0.526320
 158051/300000: episode: 1526, duration: 0.716s, episode steps: 105, steps per second: 147, episode reward: -101.758, mean reward: -0.969 [-100.000, 11.550], mean action: 1.695 [0.000, 3.000],  loss: 424.160318, mse: 408937.311607, mean_q: 674.479247, mean_eps: 0.526006
 158180/300000: episode: 1527, duration: 0.890s, episode steps: 129, steps per second: 145, episode reward: -78.407, mean reward: -0.608 [-100.000,  6.053], mean action: 1.558 [0.000, 3.000],  loss: 361.960401, mse: 420655.646560, mean_q: 688.173020, mean_eps: 0.525655
 158304/300000: episode: 1528, duration: 0.843s, episode steps: 124, steps per second: 147, episode reward: -35.353, mean reward: -0.285 [-100.000, 29.613], mean action: 1.573 [0.000, 3.000],  loss: 456.174822, mse: 409417.133569, mean_q: 677.863311, mean_eps: 0.525276
 158390/300000: episode: 1529, duration: 0.597s, episode steps:  86, steps per second: 144, episode reward: -16.145, mean reward: -0.188 [-100.000, 16.426], mean action: 1.628 [0.000, 3.000],  loss: 358.556065, mse: 419373.175509, mean_q: 687.866953, mean_eps: 0.524960
 158489/300000: episode: 1530, duration: 0.719s, episode steps:  99, steps per second: 138, episode reward: -84.461, mean reward: -0.853 [-100.000, 20.366], mean action: 1.838 [0.000, 3.000],  loss: 444.721476, mse: 413956.108270, mean_q: 682.449094, mean_eps: 0.524683
 158571/300000: episode: 1531, duration: 0.557s, episode steps:  82, steps per second: 147, episode reward: -58.802, mean reward: -0.717 [-100.000, 23.265], mean action: 1.744 [0.000, 3.000],  loss: 418.092455, mse: 414062.145960, mean_q: 683.685496, mean_eps: 0.524411
 158681/300000: episode: 1532, duration: 0.763s, episode steps: 110, steps per second: 144, episode reward: -10.622, mean reward: -0.097 [-100.000, 14.038], mean action: 1.527 [0.000, 3.000],  loss: 442.320916, mse: 416873.526420, mean_q: 687.777850, mean_eps: 0.524124
 158761/300000: episode: 1533, duration: 0.575s, episode steps:  80, steps per second: 139, episode reward: -18.520, mean reward: -0.231 [-100.000, 18.903], mean action: 1.738 [0.000, 3.000],  loss: 249.834696, mse: 410929.909375, mean_q: 682.479063, mean_eps: 0.523838
 158848/300000: episode: 1534, duration: 0.599s, episode steps:  87, steps per second: 145, episode reward: -82.850, mean reward: -0.952 [-100.000, 14.471], mean action: 1.632 [0.000, 3.000],  loss: 411.760971, mse: 404986.269756, mean_q: 675.907328, mean_eps: 0.523588
 159848/300000: episode: 1535, duration: 8.728s, episode steps: 1000, steps per second: 115, episode reward: 54.043, mean reward:  0.054 [-24.508, 76.356], mean action: 1.820 [0.000, 3.000],  loss: 422.083245, mse: 418352.445688, mean_q: 689.264339, mean_eps: 0.521957
 159924/300000: episode: 1536, duration: 0.535s, episode steps:  76, steps per second: 142, episode reward: -81.591, mean reward: -1.074 [-100.000, 13.855], mean action: 1.803 [0.000, 3.000],  loss: 430.701789, mse: 425518.712582, mean_q: 697.497392, mean_eps: 0.520344
 160005/300000: episode: 1537, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -20.714, mean reward: -0.256 [-100.000, 13.238], mean action: 1.568 [0.000, 3.000],  loss: 424.087997, mse: 425818.430941, mean_q: 698.235988, mean_eps: 0.520108
 160114/300000: episode: 1538, duration: 0.771s, episode steps: 109, steps per second: 141, episode reward: -86.239, mean reward: -0.791 [-100.000,  7.524], mean action: 1.725 [0.000, 3.000],  loss: 327.132981, mse: 421234.399369, mean_q: 692.245684, mean_eps: 0.519823
 160209/300000: episode: 1539, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -50.456, mean reward: -0.531 [-100.000,  7.073], mean action: 1.758 [0.000, 3.000],  loss: 352.539200, mse: 423163.297368, mean_q: 695.114417, mean_eps: 0.519517
 160285/300000: episode: 1540, duration: 0.510s, episode steps:  76, steps per second: 149, episode reward: -28.092, mean reward: -0.370 [-100.000, 11.795], mean action: 1.605 [0.000, 3.000],  loss: 387.495894, mse: 415031.069490, mean_q: 687.834650, mean_eps: 0.519261
 160406/300000: episode: 1541, duration: 0.871s, episode steps: 121, steps per second: 139, episode reward:  3.510, mean reward:  0.029 [-100.000, 13.962], mean action: 1.851 [0.000, 3.000],  loss: 394.426492, mse: 431683.899535, mean_q: 704.409042, mean_eps: 0.518965
 160483/300000: episode: 1542, duration: 0.521s, episode steps:  77, steps per second: 148, episode reward: -61.077, mean reward: -0.793 [-100.000,  6.056], mean action: 1.273 [0.000, 3.000],  loss: 376.489732, mse: 427467.806006, mean_q: 699.070686, mean_eps: 0.518668
 160584/300000: episode: 1543, duration: 0.769s, episode steps: 101, steps per second: 131, episode reward: -42.868, mean reward: -0.424 [-100.000, 12.607], mean action: 1.416 [0.000, 3.000],  loss: 417.643361, mse: 421561.982983, mean_q: 693.948716, mean_eps: 0.518401
 160713/300000: episode: 1544, duration: 1.140s, episode steps: 129, steps per second: 113, episode reward: 37.041, mean reward:  0.287 [-100.000, 52.536], mean action: 1.806 [0.000, 3.000],  loss: 371.364728, mse: 416381.671269, mean_q: 689.350061, mean_eps: 0.518056
 160798/300000: episode: 1545, duration: 0.626s, episode steps:  85, steps per second: 136, episode reward: -9.556, mean reward: -0.112 [-100.000, 12.140], mean action: 1.753 [0.000, 3.000],  loss: 268.232366, mse: 424265.672794, mean_q: 700.843517, mean_eps: 0.517735
 160900/300000: episode: 1546, duration: 0.782s, episode steps: 102, steps per second: 130, episode reward: 10.709, mean reward:  0.105 [-100.000, 19.101], mean action: 1.686 [0.000, 3.000],  loss: 348.078617, mse: 415102.128064, mean_q: 689.614032, mean_eps: 0.517454
 161005/300000: episode: 1547, duration: 0.785s, episode steps: 105, steps per second: 134, episode reward: -119.319, mean reward: -1.136 [-100.000,  8.670], mean action: 1.581 [0.000, 3.000],  loss: 397.431878, mse: 417108.601488, mean_q: 696.489823, mean_eps: 0.517144
 161130/300000: episode: 1548, duration: 0.909s, episode steps: 125, steps per second: 138, episode reward: -61.042, mean reward: -0.488 [-100.000, 10.145], mean action: 1.872 [0.000, 3.000],  loss: 404.646474, mse: 418795.913000, mean_q: 695.751876, mean_eps: 0.516799
 161231/300000: episode: 1549, duration: 0.726s, episode steps: 101, steps per second: 139, episode reward: -18.759, mean reward: -0.186 [-100.000, 14.796], mean action: 1.396 [0.000, 3.000],  loss: 601.722302, mse: 415443.511448, mean_q: 690.695440, mean_eps: 0.516460
 161305/300000: episode: 1550, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -27.557, mean reward: -0.372 [-100.000, 24.996], mean action: 1.568 [0.000, 3.000],  loss: 371.064480, mse: 406250.080659, mean_q: 682.395795, mean_eps: 0.516198
 161380/300000: episode: 1551, duration: 0.533s, episode steps:  75, steps per second: 141, episode reward: -1.965, mean reward: -0.026 [-100.000, 12.315], mean action: 1.560 [0.000, 3.000],  loss: 306.898246, mse: 416500.438750, mean_q: 689.676963, mean_eps: 0.515974
 161450/300000: episode: 1552, duration: 0.495s, episode steps:  70, steps per second: 141, episode reward: -71.111, mean reward: -1.016 [-100.000, 13.045], mean action: 1.500 [0.000, 3.000],  loss: 516.888306, mse: 408858.953125, mean_q: 683.057268, mean_eps: 0.515756
 161561/300000: episode: 1553, duration: 0.790s, episode steps: 111, steps per second: 141, episode reward: -74.759, mean reward: -0.674 [-100.000,  8.856], mean action: 1.676 [0.000, 3.000],  loss: 380.913296, mse: 410737.350225, mean_q: 686.718939, mean_eps: 0.515485
 161673/300000: episode: 1554, duration: 0.804s, episode steps: 112, steps per second: 139, episode reward: -34.863, mean reward: -0.311 [-100.000, 11.454], mean action: 1.509 [0.000, 3.000],  loss: 424.385849, mse: 420557.493304, mean_q: 699.031449, mean_eps: 0.515150
 161818/300000: episode: 1555, duration: 0.994s, episode steps: 145, steps per second: 146, episode reward: 28.778, mean reward:  0.198 [-100.000, 14.845], mean action: 1.710 [0.000, 3.000],  loss: 452.764252, mse: 417330.362500, mean_q: 694.385701, mean_eps: 0.514765
 161920/300000: episode: 1556, duration: 0.725s, episode steps: 102, steps per second: 141, episode reward: -79.638, mean reward: -0.781 [-100.000, 13.244], mean action: 1.618 [0.000, 3.000],  loss: 497.345902, mse: 415375.996017, mean_q: 694.250032, mean_eps: 0.514394
 162017/300000: episode: 1557, duration: 0.685s, episode steps:  97, steps per second: 142, episode reward: -37.024, mean reward: -0.382 [-100.000, 22.276], mean action: 1.495 [0.000, 3.000],  loss: 237.546153, mse: 416475.563144, mean_q: 693.243194, mean_eps: 0.514096
 162136/300000: episode: 1558, duration: 0.837s, episode steps: 119, steps per second: 142, episode reward: -10.079, mean reward: -0.085 [-100.000, 15.415], mean action: 1.689 [0.000, 3.000],  loss: 384.354149, mse: 415397.631303, mean_q: 691.363698, mean_eps: 0.513772
 162192/300000: episode: 1559, duration: 0.411s, episode steps:  56, steps per second: 136, episode reward: -86.761, mean reward: -1.549 [-100.000, 12.880], mean action: 1.536 [0.000, 3.000],  loss: 402.126569, mse: 414095.222098, mean_q: 692.518437, mean_eps: 0.513509
 162305/300000: episode: 1560, duration: 0.784s, episode steps: 113, steps per second: 144, episode reward: -35.419, mean reward: -0.313 [-100.000, 14.884], mean action: 1.735 [0.000, 3.000],  loss: 285.073231, mse: 421060.708241, mean_q: 699.507511, mean_eps: 0.513256
 162425/300000: episode: 1561, duration: 0.797s, episode steps: 120, steps per second: 150, episode reward: -33.173, mean reward: -0.276 [-100.000, 14.005], mean action: 1.767 [0.000, 3.000],  loss: 398.629852, mse: 411703.052604, mean_q: 689.104323, mean_eps: 0.512907
 162553/300000: episode: 1562, duration: 0.914s, episode steps: 128, steps per second: 140, episode reward: -26.881, mean reward: -0.210 [-100.000, 17.565], mean action: 1.750 [0.000, 3.000],  loss: 416.204762, mse: 403891.639404, mean_q: 683.142908, mean_eps: 0.512535
 162634/300000: episode: 1563, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -28.763, mean reward: -0.355 [-100.000, 17.780], mean action: 1.494 [0.000, 3.000],  loss: 460.603198, mse: 399068.567515, mean_q: 676.621927, mean_eps: 0.512221
 162715/300000: episode: 1564, duration: 0.546s, episode steps:  81, steps per second: 148, episode reward: -68.640, mean reward: -0.847 [-100.000, 14.645], mean action: 1.790 [0.000, 3.000],  loss: 439.120987, mse: 403507.155864, mean_q: 684.650355, mean_eps: 0.511978
 162847/300000: episode: 1565, duration: 0.924s, episode steps: 132, steps per second: 143, episode reward: -10.468, mean reward: -0.079 [-100.000, 10.517], mean action: 1.621 [0.000, 3.000],  loss: 374.868662, mse: 402790.440814, mean_q: 681.568091, mean_eps: 0.511659
 162945/300000: episode: 1566, duration: 0.664s, episode steps:  98, steps per second: 148, episode reward: -5.957, mean reward: -0.061 [-100.000, 12.704], mean action: 1.694 [0.000, 3.000],  loss: 310.298803, mse: 406091.165816, mean_q: 688.443049, mean_eps: 0.511313
 163064/300000: episode: 1567, duration: 0.827s, episode steps: 119, steps per second: 144, episode reward: -29.905, mean reward: -0.251 [-100.000,  7.293], mean action: 1.714 [0.000, 3.000],  loss: 511.677631, mse: 406403.168330, mean_q: 686.568340, mean_eps: 0.510988
 163172/300000: episode: 1568, duration: 0.764s, episode steps: 108, steps per second: 141, episode reward: -45.825, mean reward: -0.424 [-100.000,  9.563], mean action: 1.750 [0.000, 3.000],  loss: 433.843482, mse: 409891.417535, mean_q: 688.202619, mean_eps: 0.510647
 163287/300000: episode: 1569, duration: 0.771s, episode steps: 115, steps per second: 149, episode reward: -6.478, mean reward: -0.056 [-100.000,  9.649], mean action: 1.861 [0.000, 3.000],  loss: 551.329624, mse: 415774.712228, mean_q: 694.426937, mean_eps: 0.510313
 163402/300000: episode: 1570, duration: 0.811s, episode steps: 115, steps per second: 142, episode reward: -14.256, mean reward: -0.124 [-100.000, 12.202], mean action: 1.670 [0.000, 3.000],  loss: 360.600552, mse: 413429.721739, mean_q: 691.660538, mean_eps: 0.509968
 163495/300000: episode: 1571, duration: 0.647s, episode steps:  93, steps per second: 144, episode reward: -0.915, mean reward: -0.010 [-100.000, 13.946], mean action: 1.516 [0.000, 3.000],  loss: 321.611302, mse: 408675.998656, mean_q: 687.230528, mean_eps: 0.509656
 163593/300000: episode: 1572, duration: 0.651s, episode steps:  98, steps per second: 150, episode reward: -74.194, mean reward: -0.757 [-100.000,  7.131], mean action: 1.633 [0.000, 3.000],  loss: 426.809735, mse: 412483.838329, mean_q: 689.313650, mean_eps: 0.509370
 163672/300000: episode: 1573, duration: 0.529s, episode steps:  79, steps per second: 149, episode reward: -50.196, mean reward: -0.635 [-100.000, 12.195], mean action: 1.797 [0.000, 3.000],  loss: 422.264864, mse: 414408.581487, mean_q: 691.581617, mean_eps: 0.509104
 163790/300000: episode: 1574, duration: 0.867s, episode steps: 118, steps per second: 136, episode reward: -16.231, mean reward: -0.138 [-100.000, 15.834], mean action: 1.695 [0.000, 3.000],  loss: 360.171096, mse: 413869.452595, mean_q: 690.703940, mean_eps: 0.508809
 163910/300000: episode: 1575, duration: 0.811s, episode steps: 120, steps per second: 148, episode reward:  0.438, mean reward:  0.004 [-100.000, 35.514], mean action: 1.592 [0.000, 3.000],  loss: 438.393303, mse: 402759.102604, mean_q: 682.633888, mean_eps: 0.508451
 164009/300000: episode: 1576, duration: 0.707s, episode steps:  99, steps per second: 140, episode reward: -38.229, mean reward: -0.386 [-100.000, 12.141], mean action: 1.788 [0.000, 3.000],  loss: 333.660746, mse: 404683.431818, mean_q: 682.884965, mean_eps: 0.508123
 164129/300000: episode: 1577, duration: 0.845s, episode steps: 120, steps per second: 142, episode reward: -30.140, mean reward: -0.251 [-100.000,  6.801], mean action: 1.625 [0.000, 3.000],  loss: 370.438570, mse: 402806.578125, mean_q: 682.350833, mean_eps: 0.507794
 164215/300000: episode: 1578, duration: 0.579s, episode steps:  86, steps per second: 149, episode reward: -68.781, mean reward: -0.800 [-100.000, 10.565], mean action: 1.826 [0.000, 3.000],  loss: 419.069408, mse: 398498.446221, mean_q: 677.861590, mean_eps: 0.507486
 164301/300000: episode: 1579, duration: 0.583s, episode steps:  86, steps per second: 148, episode reward: -10.140, mean reward: -0.118 [-100.000, 16.236], mean action: 1.826 [0.000, 3.000],  loss: 349.409986, mse: 394032.268895, mean_q: 674.588437, mean_eps: 0.507227
 164394/300000: episode: 1580, duration: 0.669s, episode steps:  93, steps per second: 139, episode reward: -36.552, mean reward: -0.393 [-100.000, 18.646], mean action: 1.796 [0.000, 3.000],  loss: 425.127781, mse: 395690.956989, mean_q: 672.816614, mean_eps: 0.506959
 164500/300000: episode: 1581, duration: 0.728s, episode steps: 106, steps per second: 146, episode reward: -54.798, mean reward: -0.517 [-100.000, 13.180], mean action: 1.745 [0.000, 3.000],  loss: 489.412259, mse: 390808.255896, mean_q: 668.248186, mean_eps: 0.506661
 164607/300000: episode: 1582, duration: 0.723s, episode steps: 107, steps per second: 148, episode reward: -88.473, mean reward: -0.827 [-100.000,  7.015], mean action: 1.280 [0.000, 3.000],  loss: 332.811151, mse: 393537.831776, mean_q: 674.702120, mean_eps: 0.506341
 164687/300000: episode: 1583, duration: 0.574s, episode steps:  80, steps per second: 139, episode reward: -45.222, mean reward: -0.565 [-100.000, 11.016], mean action: 1.875 [0.000, 3.000],  loss: 495.276984, mse: 388734.651562, mean_q: 666.274165, mean_eps: 0.506061
 164768/300000: episode: 1584, duration: 0.552s, episode steps:  81, steps per second: 147, episode reward: -84.363, mean reward: -1.042 [-100.000,  8.538], mean action: 1.580 [0.000, 3.000],  loss: 483.329681, mse: 389062.939429, mean_q: 666.863020, mean_eps: 0.505819
 164890/300000: episode: 1585, duration: 0.850s, episode steps: 122, steps per second: 144, episode reward: -81.943, mean reward: -0.672 [-100.000, 22.907], mean action: 1.795 [0.000, 3.000],  loss: 375.414583, mse: 382058.418801, mean_q: 658.410015, mean_eps: 0.505514
 164968/300000: episode: 1586, duration: 0.593s, episode steps:  78, steps per second: 132, episode reward: 12.020, mean reward:  0.154 [-100.000, 14.810], mean action: 1.500 [0.000, 3.000],  loss: 530.689698, mse: 380673.704728, mean_q: 657.170001, mean_eps: 0.505215
 165968/300000: episode: 1587, duration: 8.084s, episode steps: 1000, steps per second: 124, episode reward: 74.644, mean reward:  0.075 [-22.331, 23.576], mean action: 1.838 [0.000, 3.000],  loss: 449.729950, mse: 378062.170719, mean_q: 660.539298, mean_eps: 0.503597
 166083/300000: episode: 1588, duration: 0.791s, episode steps: 115, steps per second: 145, episode reward: -28.439, mean reward: -0.247 [-100.000, 18.779], mean action: 1.748 [0.000, 3.000],  loss: 396.383271, mse: 367004.008152, mean_q: 650.914444, mean_eps: 0.501925
 167083/300000: episode: 1589, duration: 7.678s, episode steps: 1000, steps per second: 130, episode reward: 55.765, mean reward:  0.056 [-23.077, 23.500], mean action: 1.740 [0.000, 3.000],  loss: 380.774177, mse: 372119.112812, mean_q: 656.533378, mean_eps: 0.500252
 167176/300000: episode: 1590, duration: 0.644s, episode steps:  93, steps per second: 144, episode reward:  1.539, mean reward:  0.017 [-100.000, 11.471], mean action: 1.441 [0.000, 3.000],  loss: 361.603296, mse: 378129.049395, mean_q: 667.382053, mean_eps: 0.498613
 167264/300000: episode: 1591, duration: 0.622s, episode steps:  88, steps per second: 142, episode reward:  6.927, mean reward:  0.079 [-100.000, 16.189], mean action: 1.773 [0.000, 3.000],  loss: 494.947246, mse: 381429.110795, mean_q: 669.119741, mean_eps: 0.498342
 167366/300000: episode: 1592, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -147.730, mean reward: -1.448 [-100.000,  5.742], mean action: 1.520 [0.000, 3.000],  loss: 500.593214, mse: 376316.327206, mean_q: 664.684087, mean_eps: 0.498057
 167484/300000: episode: 1593, duration: 0.833s, episode steps: 118, steps per second: 142, episode reward: -75.116, mean reward: -0.637 [-100.000, 12.213], mean action: 1.729 [0.000, 3.000],  loss: 345.316229, mse: 375947.677172, mean_q: 665.450354, mean_eps: 0.497727
 167565/300000: episode: 1594, duration: 0.567s, episode steps:  81, steps per second: 143, episode reward: -1.795, mean reward: -0.022 [-100.000, 20.428], mean action: 1.691 [0.000, 3.000],  loss: 485.096703, mse: 372776.049383, mean_q: 661.231139, mean_eps: 0.497428
 167649/300000: episode: 1595, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -7.740, mean reward: -0.092 [-100.000, 16.994], mean action: 1.500 [0.000, 3.000],  loss: 247.440393, mse: 378882.782738, mean_q: 668.506659, mean_eps: 0.497181
 167780/300000: episode: 1596, duration: 0.911s, episode steps: 131, steps per second: 144, episode reward: -45.055, mean reward: -0.344 [-100.000, 11.232], mean action: 1.718 [0.000, 3.000],  loss: 412.145893, mse: 379948.237118, mean_q: 669.652993, mean_eps: 0.496858
 167869/300000: episode: 1597, duration: 0.625s, episode steps:  89, steps per second: 142, episode reward: -194.187, mean reward: -2.182 [-100.000, 26.789], mean action: 1.719 [0.000, 3.000],  loss: 371.311485, mse: 388901.479635, mean_q: 679.594696, mean_eps: 0.496528
 167960/300000: episode: 1598, duration: 0.614s, episode steps:  91, steps per second: 148, episode reward: -46.918, mean reward: -0.516 [-100.000, 13.049], mean action: 1.791 [0.000, 3.000],  loss: 427.491172, mse: 387089.011676, mean_q: 677.102261, mean_eps: 0.496258
 168047/300000: episode: 1599, duration: 0.582s, episode steps:  87, steps per second: 149, episode reward: -61.160, mean reward: -0.703 [-100.000, 12.538], mean action: 1.828 [0.000, 3.000],  loss: 341.505997, mse: 388685.764368, mean_q: 679.502967, mean_eps: 0.495991
 168136/300000: episode: 1600, duration: 0.628s, episode steps:  89, steps per second: 142, episode reward: -37.404, mean reward: -0.420 [-100.000, 10.883], mean action: 1.674 [0.000, 3.000],  loss: 352.101921, mse: 384968.034410, mean_q: 675.318223, mean_eps: 0.495727
 168222/300000: episode: 1601, duration: 0.739s, episode steps:  86, steps per second: 116, episode reward: 14.702, mean reward:  0.171 [-100.000, 14.296], mean action: 1.872 [0.000, 3.000],  loss: 339.927766, mse: 388816.808503, mean_q: 680.850115, mean_eps: 0.495464
 168288/300000: episode: 1602, duration: 0.514s, episode steps:  66, steps per second: 128, episode reward: -30.797, mean reward: -0.467 [-100.000,  8.369], mean action: 1.576 [0.000, 3.000],  loss: 521.682895, mse: 391853.375947, mean_q: 681.772865, mean_eps: 0.495236
 168354/300000: episode: 1603, duration: 0.534s, episode steps:  66, steps per second: 124, episode reward: -75.512, mean reward: -1.144 [-100.000,  7.031], mean action: 1.773 [0.000, 3.000],  loss: 415.953916, mse: 384141.743845, mean_q: 671.461740, mean_eps: 0.495038
 168457/300000: episode: 1604, duration: 0.840s, episode steps: 103, steps per second: 123, episode reward: -56.932, mean reward: -0.553 [-100.000, 14.721], mean action: 1.660 [0.000, 3.000],  loss: 466.865910, mse: 388527.924454, mean_q: 677.930349, mean_eps: 0.494785
 168539/300000: episode: 1605, duration: 0.608s, episode steps:  82, steps per second: 135, episode reward: -97.727, mean reward: -1.192 [-100.000, 11.405], mean action: 1.610 [0.000, 3.000],  loss: 461.321702, mse: 386642.570503, mean_q: 679.822498, mean_eps: 0.494507
 168635/300000: episode: 1606, duration: 0.721s, episode steps:  96, steps per second: 133, episode reward: -13.776, mean reward: -0.143 [-100.000, 23.338], mean action: 1.510 [0.000, 3.000],  loss: 463.091497, mse: 391279.150716, mean_q: 684.185002, mean_eps: 0.494241
 168736/300000: episode: 1607, duration: 0.773s, episode steps: 101, steps per second: 131, episode reward: -102.886, mean reward: -1.019 [-100.000,  5.907], mean action: 1.911 [0.000, 3.000],  loss: 472.022616, mse: 387914.194616, mean_q: 679.370165, mean_eps: 0.493945
 168857/300000: episode: 1608, duration: 0.832s, episode steps: 121, steps per second: 145, episode reward: -7.806, mean reward: -0.065 [-100.000, 20.976], mean action: 1.694 [0.000, 3.000],  loss: 390.431293, mse: 386324.130424, mean_q: 678.099566, mean_eps: 0.493612
 168969/300000: episode: 1609, duration: 0.818s, episode steps: 112, steps per second: 137, episode reward: -29.284, mean reward: -0.261 [-100.000, 17.550], mean action: 1.545 [0.000, 3.000],  loss: 416.641836, mse: 386056.896763, mean_q: 678.450709, mean_eps: 0.493263
 169041/300000: episode: 1610, duration: 0.541s, episode steps:  72, steps per second: 133, episode reward: -46.840, mean reward: -0.651 [-100.000, 10.973], mean action: 1.486 [0.000, 3.000],  loss: 644.117601, mse: 378456.103299, mean_q: 669.216439, mean_eps: 0.492987
 169113/300000: episode: 1611, duration: 0.508s, episode steps:  72, steps per second: 142, episode reward: -29.397, mean reward: -0.408 [-100.000, 12.506], mean action: 1.667 [0.000, 3.000],  loss: 498.581284, mse: 383212.258681, mean_q: 674.829390, mean_eps: 0.492771
 169242/300000: episode: 1612, duration: 0.869s, episode steps: 129, steps per second: 149, episode reward:  4.951, mean reward:  0.038 [-100.000,  7.740], mean action: 1.504 [0.000, 3.000],  loss: 308.353160, mse: 374519.859012, mean_q: 664.884114, mean_eps: 0.492469
 169331/300000: episode: 1613, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -57.989, mean reward: -0.652 [-100.000, 12.876], mean action: 1.427 [0.000, 3.000],  loss: 472.061424, mse: 384226.406601, mean_q: 674.180404, mean_eps: 0.492142
 169425/300000: episode: 1614, duration: 0.637s, episode steps:  94, steps per second: 148, episode reward: -42.239, mean reward: -0.449 [-100.000, 11.733], mean action: 1.713 [0.000, 3.000],  loss: 543.416916, mse: 385950.800532, mean_q: 677.263130, mean_eps: 0.491868
 169518/300000: episode: 1615, duration: 0.623s, episode steps:  93, steps per second: 149, episode reward: -40.467, mean reward: -0.435 [-100.000, 15.664], mean action: 1.710 [0.000, 3.000],  loss: 515.443811, mse: 378098.753024, mean_q: 669.353909, mean_eps: 0.491587
 169605/300000: episode: 1616, duration: 0.597s, episode steps:  87, steps per second: 146, episode reward: -26.966, mean reward: -0.310 [-100.000, 18.573], mean action: 1.770 [0.000, 3.000],  loss: 533.146166, mse: 378580.402299, mean_q: 669.154810, mean_eps: 0.491317
 169695/300000: episode: 1617, duration: 0.610s, episode steps:  90, steps per second: 148, episode reward: 19.990, mean reward:  0.222 [-100.000, 20.656], mean action: 1.711 [0.000, 3.000],  loss: 505.942360, mse: 376838.947222, mean_q: 666.751224, mean_eps: 0.491051
 169788/300000: episode: 1618, duration: 0.721s, episode steps:  93, steps per second: 129, episode reward: -52.927, mean reward: -0.569 [-100.000,  8.347], mean action: 1.581 [0.000, 3.000],  loss: 389.942222, mse: 381597.711694, mean_q: 671.561441, mean_eps: 0.490777
 169895/300000: episode: 1619, duration: 0.790s, episode steps: 107, steps per second: 135, episode reward: -32.990, mean reward: -0.308 [-100.000, 16.462], mean action: 1.570 [0.000, 3.000],  loss: 287.481373, mse: 381168.337033, mean_q: 671.402538, mean_eps: 0.490477
 170007/300000: episode: 1620, duration: 0.855s, episode steps: 112, steps per second: 131, episode reward: -24.547, mean reward: -0.219 [-100.000,  7.602], mean action: 1.679 [0.000, 3.000],  loss: 537.174364, mse: 385366.190011, mean_q: 674.900056, mean_eps: 0.490148
 170149/300000: episode: 1621, duration: 0.987s, episode steps: 142, steps per second: 144, episode reward: -55.033, mean reward: -0.388 [-100.000, 12.322], mean action: 1.563 [0.000, 3.000],  loss: 463.885585, mse: 376283.496259, mean_q: 664.712171, mean_eps: 0.489767
 170243/300000: episode: 1622, duration: 0.737s, episode steps:  94, steps per second: 127, episode reward: 12.139, mean reward:  0.129 [-100.000, 22.479], mean action: 1.670 [0.000, 3.000],  loss: 460.062155, mse: 367710.477394, mean_q: 655.653793, mean_eps: 0.489414
 170326/300000: episode: 1623, duration: 0.582s, episode steps:  83, steps per second: 143, episode reward: -192.058, mean reward: -2.314 [-100.000, 14.191], mean action: 1.506 [0.000, 3.000],  loss: 444.157499, mse: 373940.498494, mean_q: 664.421749, mean_eps: 0.489148
 170451/300000: episode: 1624, duration: 0.834s, episode steps: 125, steps per second: 150, episode reward: -20.572, mean reward: -0.165 [-100.000, 12.151], mean action: 1.656 [0.000, 3.000],  loss: 294.121605, mse: 364546.732000, mean_q: 651.512072, mean_eps: 0.488836
 171451/300000: episode: 1625, duration: 7.477s, episode steps: 1000, steps per second: 134, episode reward: 93.575, mean reward:  0.094 [-23.278, 61.626], mean action: 1.379 [0.000, 3.000],  loss: 383.507674, mse: 355771.036187, mean_q: 644.856160, mean_eps: 0.487149
 171554/300000: episode: 1626, duration: 0.687s, episode steps: 103, steps per second: 150, episode reward: -62.343, mean reward: -0.605 [-100.000,  8.840], mean action: 1.592 [0.000, 3.000],  loss: 363.093800, mse: 353715.777609, mean_q: 644.236123, mean_eps: 0.485494
 171636/300000: episode: 1627, duration: 0.565s, episode steps:  82, steps per second: 145, episode reward: -15.473, mean reward: -0.189 [-100.000, 13.295], mean action: 1.573 [0.000, 3.000],  loss: 458.717220, mse: 348327.685213, mean_q: 632.248694, mean_eps: 0.485217
 171712/300000: episode: 1628, duration: 0.526s, episode steps:  76, steps per second: 144, episode reward: -65.249, mean reward: -0.859 [-100.000, 12.592], mean action: 1.882 [0.000, 3.000],  loss: 385.037986, mse: 353587.215049, mean_q: 641.316960, mean_eps: 0.484979
 171821/300000: episode: 1629, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -35.142, mean reward: -0.322 [-100.000, 27.564], mean action: 1.752 [0.000, 3.000],  loss: 434.785523, mse: 352468.364392, mean_q: 641.385607, mean_eps: 0.484702
 171910/300000: episode: 1630, duration: 0.594s, episode steps:  89, steps per second: 150, episode reward: -87.519, mean reward: -0.983 [-100.000, 13.560], mean action: 1.517 [0.000, 3.000],  loss: 324.878956, mse: 346828.465941, mean_q: 632.725591, mean_eps: 0.484405
 172009/300000: episode: 1631, duration: 0.705s, episode steps:  99, steps per second: 140, episode reward:  2.788, mean reward:  0.028 [-100.000, 17.828], mean action: 1.545 [0.000, 3.000],  loss: 290.643395, mse: 348742.809975, mean_q: 634.180789, mean_eps: 0.484123
 173009/300000: episode: 1632, duration: 7.995s, episode steps: 1000, steps per second: 125, episode reward: 82.684, mean reward:  0.083 [-24.509, 24.266], mean action: 1.806 [0.000, 3.000],  loss: 388.660982, mse: 333920.626828, mean_q: 620.444598, mean_eps: 0.482475
 173113/300000: episode: 1633, duration: 0.742s, episode steps: 104, steps per second: 140, episode reward: -114.248, mean reward: -1.099 [-100.000, 22.393], mean action: 1.683 [0.000, 3.000],  loss: 386.997275, mse: 333476.588341, mean_q: 621.228904, mean_eps: 0.480819
 173220/300000: episode: 1634, duration: 0.761s, episode steps: 107, steps per second: 141, episode reward: -53.642, mean reward: -0.501 [-100.000, 14.169], mean action: 1.589 [0.000, 3.000],  loss: 362.259697, mse: 334740.009054, mean_q: 625.196869, mean_eps: 0.480502
 173328/300000: episode: 1635, duration: 0.734s, episode steps: 108, steps per second: 147, episode reward: -77.645, mean reward: -0.719 [-100.000, 12.131], mean action: 1.639 [0.000, 3.000],  loss: 285.673677, mse: 334722.735822, mean_q: 626.503892, mean_eps: 0.480179
 173427/300000: episode: 1636, duration: 0.708s, episode steps:  99, steps per second: 140, episode reward: -35.464, mean reward: -0.358 [-100.000, 13.228], mean action: 1.747 [0.000, 3.000],  loss: 292.292023, mse: 326243.404040, mean_q: 616.274417, mean_eps: 0.479869
 173543/300000: episode: 1637, duration: 0.789s, episode steps: 116, steps per second: 147, episode reward:  1.881, mean reward:  0.016 [-100.000,  9.180], mean action: 1.698 [0.000, 3.000],  loss: 268.149287, mse: 331171.130657, mean_q: 623.154708, mean_eps: 0.479546
 173628/300000: episode: 1638, duration: 0.568s, episode steps:  85, steps per second: 150, episode reward: -80.277, mean reward: -0.944 [-100.000,  8.008], mean action: 1.682 [0.000, 3.000],  loss: 498.890579, mse: 334285.001103, mean_q: 623.054023, mean_eps: 0.479245
 173755/300000: episode: 1639, duration: 0.885s, episode steps: 127, steps per second: 143, episode reward: -59.003, mean reward: -0.465 [-100.000, 16.527], mean action: 1.772 [0.000, 3.000],  loss: 279.089850, mse: 337570.900098, mean_q: 629.643461, mean_eps: 0.478927
 173831/300000: episode: 1640, duration: 0.516s, episode steps:  76, steps per second: 147, episode reward: -130.172, mean reward: -1.713 [-100.000,  6.867], mean action: 1.671 [0.000, 3.000],  loss: 367.478515, mse: 334251.916118, mean_q: 624.164009, mean_eps: 0.478623
 173969/300000: episode: 1641, duration: 0.923s, episode steps: 138, steps per second: 150, episode reward: 39.025, mean reward:  0.283 [-100.000, 15.170], mean action: 1.442 [0.000, 3.000],  loss: 430.571290, mse: 337319.265851, mean_q: 629.193803, mean_eps: 0.478301
 174074/300000: episode: 1642, duration: 0.733s, episode steps: 105, steps per second: 143, episode reward:  2.750, mean reward:  0.026 [-100.000, 19.201], mean action: 1.667 [0.000, 3.000],  loss: 303.465824, mse: 340041.578571, mean_q: 632.549449, mean_eps: 0.477937
 174157/300000: episode: 1643, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -48.392, mean reward: -0.583 [-100.000, 11.624], mean action: 1.687 [0.000, 3.000],  loss: 288.205773, mse: 339585.120105, mean_q: 633.283837, mean_eps: 0.477655
 174255/300000: episode: 1644, duration: 0.658s, episode steps:  98, steps per second: 149, episode reward: -81.106, mean reward: -0.828 [-100.000,  9.696], mean action: 1.551 [0.000, 3.000],  loss: 311.438660, mse: 339925.274872, mean_q: 635.063913, mean_eps: 0.477384
 174342/300000: episode: 1645, duration: 0.619s, episode steps:  87, steps per second: 140, episode reward: 10.357, mean reward:  0.119 [-100.000, 20.177], mean action: 1.586 [0.000, 3.000],  loss: 356.646319, mse: 338063.455460, mean_q: 632.878421, mean_eps: 0.477106
 174476/300000: episode: 1646, duration: 0.919s, episode steps: 134, steps per second: 146, episode reward:  0.277, mean reward:  0.002 [-100.000, 15.111], mean action: 1.724 [0.000, 3.000],  loss: 327.179500, mse: 334793.386894, mean_q: 629.228701, mean_eps: 0.476774
 174570/300000: episode: 1647, duration: 0.630s, episode steps:  94, steps per second: 149, episode reward: -11.726, mean reward: -0.125 [-100.000, 10.690], mean action: 1.830 [0.000, 3.000],  loss: 453.440222, mse: 330012.270612, mean_q: 624.101641, mean_eps: 0.476432
 174657/300000: episode: 1648, duration: 0.623s, episode steps:  87, steps per second: 140, episode reward: -42.798, mean reward: -0.492 [-100.000,  8.027], mean action: 1.862 [0.000, 3.000],  loss: 339.603476, mse: 331776.312141, mean_q: 628.271698, mean_eps: 0.476161
 174779/300000: episode: 1649, duration: 0.826s, episode steps: 122, steps per second: 148, episode reward: 27.080, mean reward:  0.222 [-100.000, 16.510], mean action: 1.598 [0.000, 3.000],  loss: 309.179170, mse: 329698.277152, mean_q: 624.960183, mean_eps: 0.475847
 174879/300000: episode: 1650, duration: 0.675s, episode steps: 100, steps per second: 148, episode reward: -23.639, mean reward: -0.236 [-100.000, 15.512], mean action: 1.720 [0.000, 3.000],  loss: 432.879584, mse: 331305.692188, mean_q: 626.935777, mean_eps: 0.475515
 174987/300000: episode: 1651, duration: 0.801s, episode steps: 108, steps per second: 135, episode reward: -30.705, mean reward: -0.284 [-100.000, 16.231], mean action: 1.759 [0.000, 3.000],  loss: 348.675297, mse: 324297.351562, mean_q: 615.623484, mean_eps: 0.475203
 175067/300000: episode: 1652, duration: 0.541s, episode steps:  80, steps per second: 148, episode reward: 38.046, mean reward:  0.476 [-100.000, 15.293], mean action: 1.762 [0.000, 3.000],  loss: 261.959174, mse: 328690.737891, mean_q: 624.489727, mean_eps: 0.474920
 175191/300000: episode: 1653, duration: 0.885s, episode steps: 124, steps per second: 140, episode reward: 26.241, mean reward:  0.212 [-100.000, 15.184], mean action: 1.605 [0.000, 3.000],  loss: 309.585269, mse: 323649.691532, mean_q: 617.288146, mean_eps: 0.474614
 175272/300000: episode: 1654, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -41.733, mean reward: -0.515 [-100.000,  8.621], mean action: 1.654 [0.000, 3.000],  loss: 314.316541, mse: 320710.628858, mean_q: 616.230558, mean_eps: 0.474307
 175351/300000: episode: 1655, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -27.022, mean reward: -0.342 [-100.000,  8.069], mean action: 1.797 [0.000, 3.000],  loss: 196.105307, mse: 316628.583070, mean_q: 609.738942, mean_eps: 0.474067
 175467/300000: episode: 1656, duration: 0.788s, episode steps: 116, steps per second: 147, episode reward: -19.845, mean reward: -0.171 [-100.000, 11.470], mean action: 1.672 [0.000, 3.000],  loss: 412.153826, mse: 311358.837015, mean_q: 603.281449, mean_eps: 0.473774
 175572/300000: episode: 1657, duration: 0.750s, episode steps: 105, steps per second: 140, episode reward: -41.485, mean reward: -0.395 [-100.000, 10.992], mean action: 1.686 [0.000, 3.000],  loss: 287.249412, mse: 309306.676488, mean_q: 604.209509, mean_eps: 0.473443
 175657/300000: episode: 1658, duration: 0.589s, episode steps:  85, steps per second: 144, episode reward: -66.631, mean reward: -0.784 [-100.000, 11.987], mean action: 1.529 [0.000, 3.000],  loss: 298.294991, mse: 305693.726287, mean_q: 599.831228, mean_eps: 0.473158
 175721/300000: episode: 1659, duration: 0.443s, episode steps:  64, steps per second: 145, episode reward: -34.228, mean reward: -0.535 [-100.000, 16.950], mean action: 1.875 [0.000, 3.000],  loss: 438.522423, mse: 310006.354492, mean_q: 603.188479, mean_eps: 0.472934
 175812/300000: episode: 1660, duration: 0.661s, episode steps:  91, steps per second: 138, episode reward: -44.486, mean reward: -0.489 [-100.000,  9.727], mean action: 1.451 [0.000, 3.000],  loss: 241.437016, mse: 318286.248283, mean_q: 614.059759, mean_eps: 0.472702
 175923/300000: episode: 1661, duration: 0.812s, episode steps: 111, steps per second: 137, episode reward: -64.780, mean reward: -0.584 [-100.000,  6.190], mean action: 1.757 [0.000, 3.000],  loss: 341.050826, mse: 314437.151745, mean_q: 609.417466, mean_eps: 0.472399
 176019/300000: episode: 1662, duration: 0.646s, episode steps:  96, steps per second: 149, episode reward: -46.097, mean reward: -0.480 [-100.000, 11.564], mean action: 1.740 [0.000, 3.000],  loss: 249.425030, mse: 312193.215820, mean_q: 603.822193, mean_eps: 0.472089
 176149/300000: episode: 1663, duration: 0.880s, episode steps: 130, steps per second: 148, episode reward: -92.891, mean reward: -0.715 [-100.000,  8.874], mean action: 1.738 [0.000, 3.000],  loss: 247.636651, mse: 320882.727885, mean_q: 614.212477, mean_eps: 0.471749
 176245/300000: episode: 1664, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: -18.572, mean reward: -0.193 [-100.000, 16.699], mean action: 1.583 [0.000, 3.000],  loss: 368.311732, mse: 320077.877767, mean_q: 613.127323, mean_eps: 0.471411
 176339/300000: episode: 1665, duration: 0.629s, episode steps:  94, steps per second: 150, episode reward: -112.532, mean reward: -1.197 [-100.000,  7.862], mean action: 1.851 [0.000, 3.000],  loss: 400.528563, mse: 320594.118517, mean_q: 614.434076, mean_eps: 0.471126
 176466/300000: episode: 1666, duration: 0.905s, episode steps: 127, steps per second: 140, episode reward: -69.431, mean reward: -0.547 [-100.000,  7.132], mean action: 1.874 [0.000, 3.000],  loss: 280.379012, mse: 318399.227116, mean_q: 610.753051, mean_eps: 0.470794
 176600/300000: episode: 1667, duration: 1.170s, episode steps: 134, steps per second: 115, episode reward: -36.371, mean reward: -0.271 [-100.000, 19.027], mean action: 1.664 [0.000, 3.000],  loss: 408.066420, mse: 312518.437733, mean_q: 603.403517, mean_eps: 0.470403
 176678/300000: episode: 1668, duration: 0.622s, episode steps:  78, steps per second: 125, episode reward: -0.835, mean reward: -0.011 [-100.000, 19.268], mean action: 1.667 [0.000, 3.000],  loss: 312.334004, mse: 313298.278045, mean_q: 602.698468, mean_eps: 0.470084
 176751/300000: episode: 1669, duration: 0.614s, episode steps:  73, steps per second: 119, episode reward: -101.152, mean reward: -1.386 [-100.000, 12.649], mean action: 1.616 [0.000, 3.000],  loss: 348.378784, mse: 312350.220034, mean_q: 603.864422, mean_eps: 0.469858
 176851/300000: episode: 1670, duration: 0.781s, episode steps: 100, steps per second: 128, episode reward: -152.477, mean reward: -1.525 [-100.000,  2.805], mean action: 1.620 [0.000, 3.000],  loss: 350.159284, mse: 317114.082969, mean_q: 609.067626, mean_eps: 0.469599
 176975/300000: episode: 1671, duration: 1.075s, episode steps: 124, steps per second: 115, episode reward: -78.021, mean reward: -0.629 [-100.000,  9.403], mean action: 1.847 [0.000, 3.000],  loss: 332.118448, mse: 314162.061240, mean_q: 605.476653, mean_eps: 0.469263
 177054/300000: episode: 1672, duration: 0.692s, episode steps:  79, steps per second: 114, episode reward: -19.185, mean reward: -0.243 [-100.000, 16.774], mean action: 1.835 [0.000, 3.000],  loss: 398.033761, mse: 309697.667524, mean_q: 598.898485, mean_eps: 0.468958
 177142/300000: episode: 1673, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: 39.333, mean reward:  0.447 [-100.000, 18.386], mean action: 1.739 [0.000, 3.000],  loss: 338.155965, mse: 308113.663352, mean_q: 598.440823, mean_eps: 0.468707
 177236/300000: episode: 1674, duration: 0.691s, episode steps:  94, steps per second: 136, episode reward: -14.245, mean reward: -0.152 [-100.000, 11.560], mean action: 1.628 [0.000, 3.000],  loss: 251.734135, mse: 304184.070645, mean_q: 592.707412, mean_eps: 0.468434
 177348/300000: episode: 1675, duration: 1.013s, episode steps: 112, steps per second: 111, episode reward:  7.392, mean reward:  0.066 [-100.000, 19.686], mean action: 1.652 [0.000, 3.000],  loss: 216.666619, mse: 304145.786691, mean_q: 594.518836, mean_eps: 0.468126
 177433/300000: episode: 1676, duration: 0.656s, episode steps:  85, steps per second: 130, episode reward: -95.524, mean reward: -1.124 [-100.000, 11.851], mean action: 1.553 [0.000, 3.000],  loss: 270.497953, mse: 304689.859743, mean_q: 597.138410, mean_eps: 0.467830
 177530/300000: episode: 1677, duration: 0.800s, episode steps:  97, steps per second: 121, episode reward: -70.805, mean reward: -0.730 [-100.000,  8.909], mean action: 1.639 [0.000, 3.000],  loss: 321.004669, mse: 298519.072487, mean_q: 589.624504, mean_eps: 0.467557
 177660/300000: episode: 1678, duration: 1.003s, episode steps: 130, steps per second: 130, episode reward: -68.340, mean reward: -0.526 [-100.000, 17.808], mean action: 1.792 [0.000, 3.000],  loss: 332.195368, mse: 295449.464183, mean_q: 588.032688, mean_eps: 0.467217
 178591/300000: episode: 1679, duration: 7.046s, episode steps: 931, steps per second: 132, episode reward: -76.564, mean reward: -0.082 [-100.000, 49.223], mean action: 1.665 [0.000, 3.000],  loss: 323.263463, mse: 289134.596855, mean_q: 582.339218, mean_eps: 0.465625
 178695/300000: episode: 1680, duration: 0.759s, episode steps: 104, steps per second: 137, episode reward: -60.486, mean reward: -0.582 [-100.000, 11.363], mean action: 1.731 [0.000, 3.000],  loss: 316.991364, mse: 291453.103516, mean_q: 587.352439, mean_eps: 0.464072
 178813/300000: episode: 1681, duration: 0.883s, episode steps: 118, steps per second: 134, episode reward:  3.978, mean reward:  0.034 [-100.000, 21.099], mean action: 1.729 [0.000, 3.000],  loss: 270.945897, mse: 291058.266949, mean_q: 584.303835, mean_eps: 0.463739
 178953/300000: episode: 1682, duration: 1.040s, episode steps: 140, steps per second: 135, episode reward: -58.919, mean reward: -0.421 [-100.000, 10.152], mean action: 1.743 [0.000, 3.000],  loss: 315.541491, mse: 286368.656920, mean_q: 581.109519, mean_eps: 0.463353
 179064/300000: episode: 1683, duration: 0.855s, episode steps: 111, steps per second: 130, episode reward: -72.214, mean reward: -0.651 [-100.000,  7.587], mean action: 1.577 [0.000, 3.000],  loss: 320.508513, mse: 286556.177506, mean_q: 582.033999, mean_eps: 0.462976
 180064/300000: episode: 1684, duration: 7.825s, episode steps: 1000, steps per second: 128, episode reward: 53.508, mean reward:  0.054 [-23.975, 24.233], mean action: 1.832 [0.000, 3.000],  loss: 316.965780, mse: 277803.167812, mean_q: 571.010798, mean_eps: 0.461310
 180182/300000: episode: 1685, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -57.686, mean reward: -0.489 [-100.000, 19.531], mean action: 1.737 [0.000, 3.000],  loss: 331.115407, mse: 268697.164062, mean_q: 558.804637, mean_eps: 0.459632
 180288/300000: episode: 1686, duration: 0.731s, episode steps: 106, steps per second: 145, episode reward: -43.458, mean reward: -0.410 [-100.000, 23.871], mean action: 1.811 [0.000, 3.000],  loss: 343.743695, mse: 267965.107606, mean_q: 557.744363, mean_eps: 0.459297
 180397/300000: episode: 1687, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward: -29.232, mean reward: -0.268 [-100.000, 13.274], mean action: 1.661 [0.000, 3.000],  loss: 236.523536, mse: 263429.214450, mean_q: 554.091406, mean_eps: 0.458974
 180512/300000: episode: 1688, duration: 0.774s, episode steps: 115, steps per second: 148, episode reward: -110.550, mean reward: -0.961 [-100.000,  9.150], mean action: 1.800 [0.000, 3.000],  loss: 224.904103, mse: 265354.842391, mean_q: 557.510682, mean_eps: 0.458638
 180639/300000: episode: 1689, duration: 0.862s, episode steps: 127, steps per second: 147, episode reward: -44.465, mean reward: -0.350 [-100.000,  8.901], mean action: 1.661 [0.000, 3.000],  loss: 333.294315, mse: 267497.914862, mean_q: 561.277433, mean_eps: 0.458275
 180754/300000: episode: 1690, duration: 0.799s, episode steps: 115, steps per second: 144, episode reward: -32.669, mean reward: -0.284 [-100.000, 12.493], mean action: 1.609 [0.000, 3.000],  loss: 244.939747, mse: 265288.536685, mean_q: 557.567332, mean_eps: 0.457912
 180833/300000: episode: 1691, duration: 0.535s, episode steps:  79, steps per second: 148, episode reward: -96.706, mean reward: -1.224 [-100.000,  8.465], mean action: 1.506 [0.000, 3.000],  loss: 394.345506, mse: 262178.687698, mean_q: 554.194080, mean_eps: 0.457621
 180973/300000: episode: 1692, duration: 0.944s, episode steps: 140, steps per second: 148, episode reward: -9.842, mean reward: -0.070 [-100.000, 14.988], mean action: 1.664 [0.000, 3.000],  loss: 262.556421, mse: 260097.834375, mean_q: 552.538746, mean_eps: 0.457293
 181071/300000: episode: 1693, duration: 0.694s, episode steps:  98, steps per second: 141, episode reward: -53.169, mean reward: -0.543 [-100.000,  7.631], mean action: 1.735 [0.000, 3.000],  loss: 285.930750, mse: 261209.769770, mean_q: 554.950141, mean_eps: 0.456935
 181151/300000: episode: 1694, duration: 0.546s, episode steps:  80, steps per second: 147, episode reward: -6.517, mean reward: -0.081 [-100.000, 10.809], mean action: 1.788 [0.000, 3.000],  loss: 391.181626, mse: 256590.611914, mean_q: 548.892755, mean_eps: 0.456668
 181275/300000: episode: 1695, duration: 0.835s, episode steps: 124, steps per second: 148, episode reward: -238.358, mean reward: -1.922 [-100.000,  1.488], mean action: 1.589 [0.000, 3.000],  loss: 265.775818, mse: 255036.573085, mean_q: 546.238121, mean_eps: 0.456362
 181374/300000: episode: 1696, duration: 0.695s, episode steps:  99, steps per second: 142, episode reward: 15.802, mean reward:  0.160 [-100.000, 21.877], mean action: 1.606 [0.000, 3.000],  loss: 286.178057, mse: 247594.710543, mean_q: 535.151283, mean_eps: 0.456028
 181466/300000: episode: 1697, duration: 0.665s, episode steps:  92, steps per second: 138, episode reward: -69.246, mean reward: -0.753 [-100.000, 20.978], mean action: 1.685 [0.000, 3.000],  loss: 321.188082, mse: 244340.951257, mean_q: 534.541910, mean_eps: 0.455741
 181565/300000: episode: 1698, duration: 0.669s, episode steps:  99, steps per second: 148, episode reward:  7.168, mean reward:  0.072 [-100.000, 15.638], mean action: 1.606 [0.000, 3.000],  loss: 342.297160, mse: 244403.240688, mean_q: 535.878896, mean_eps: 0.455455
 181845/300000: episode: 1699, duration: 2.028s, episode steps: 280, steps per second: 138, episode reward: -158.245, mean reward: -0.565 [-100.000, 50.241], mean action: 1.675 [0.000, 3.000],  loss: 216.587408, mse: 243758.832366, mean_q: 536.739778, mean_eps: 0.454887
 181931/300000: episode: 1700, duration: 0.585s, episode steps:  86, steps per second: 147, episode reward: -169.958, mean reward: -1.976 [-100.000,  5.957], mean action: 1.419 [0.000, 3.000],  loss: 340.027879, mse: 236136.975654, mean_q: 524.309917, mean_eps: 0.454338
 182018/300000: episode: 1701, duration: 0.611s, episode steps:  87, steps per second: 142, episode reward: -70.778, mean reward: -0.814 [-100.000, 12.753], mean action: 1.667 [0.000, 3.000],  loss: 292.892497, mse: 238411.739224, mean_q: 526.845728, mean_eps: 0.454078
 182109/300000: episode: 1702, duration: 0.611s, episode steps:  91, steps per second: 149, episode reward: -43.615, mean reward: -0.479 [-100.000, 12.807], mean action: 1.593 [0.000, 3.000],  loss: 301.295342, mse: 239775.687328, mean_q: 528.510841, mean_eps: 0.453811
 182235/300000: episode: 1703, duration: 0.854s, episode steps: 126, steps per second: 148, episode reward: 20.753, mean reward:  0.165 [-100.000, 19.401], mean action: 1.738 [0.000, 3.000],  loss: 263.408181, mse: 238403.000868, mean_q: 526.833186, mean_eps: 0.453486
 182326/300000: episode: 1704, duration: 0.654s, episode steps:  91, steps per second: 139, episode reward: -44.926, mean reward: -0.494 [-100.000, 21.739], mean action: 1.593 [0.000, 3.000],  loss: 303.077790, mse: 235718.920501, mean_q: 524.977675, mean_eps: 0.453160
 182455/300000: episode: 1705, duration: 0.869s, episode steps: 129, steps per second: 148, episode reward: -124.083, mean reward: -0.962 [-100.000,  7.138], mean action: 1.651 [0.000, 3.000],  loss: 230.813366, mse: 229901.752544, mean_q: 516.574579, mean_eps: 0.452830
 182565/300000: episode: 1706, duration: 0.757s, episode steps: 110, steps per second: 145, episode reward: -74.127, mean reward: -0.674 [-100.000, 11.140], mean action: 1.736 [0.000, 3.000],  loss: 210.801043, mse: 228056.532670, mean_q: 516.329267, mean_eps: 0.452472
 182647/300000: episode: 1707, duration: 0.582s, episode steps:  82, steps per second: 141, episode reward: 27.374, mean reward:  0.334 [-100.000, 13.117], mean action: 1.817 [0.000, 3.000],  loss: 375.980008, mse: 224961.493140, mean_q: 511.713929, mean_eps: 0.452183
 183647/300000: episode: 1708, duration: 8.154s, episode steps: 1000, steps per second: 123, episode reward: 77.333, mean reward:  0.077 [-23.870, 24.599], mean action: 1.485 [0.000, 3.000],  loss: 275.085229, mse: 221136.361000, mean_q: 508.721523, mean_eps: 0.450560
 183755/300000: episode: 1709, duration: 0.741s, episode steps: 108, steps per second: 146, episode reward:  2.662, mean reward:  0.025 [-100.000, 19.328], mean action: 1.556 [0.000, 3.000],  loss: 236.130983, mse: 217580.049190, mean_q: 505.312962, mean_eps: 0.448898
 183879/300000: episode: 1710, duration: 0.839s, episode steps: 124, steps per second: 148, episode reward: -92.508, mean reward: -0.746 [-100.000, 21.010], mean action: 1.629 [0.000, 3.000],  loss: 199.407410, mse: 218156.906754, mean_q: 505.665926, mean_eps: 0.448550
 183967/300000: episode: 1711, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -43.308, mean reward: -0.492 [-100.000, 12.027], mean action: 1.648 [0.000, 3.000],  loss: 168.096218, mse: 219951.891868, mean_q: 508.190094, mean_eps: 0.448232
 184054/300000: episode: 1712, duration: 0.597s, episode steps:  87, steps per second: 146, episode reward: -90.840, mean reward: -1.044 [-100.000,  8.803], mean action: 1.747 [0.000, 3.000],  loss: 172.777976, mse: 214941.161818, mean_q: 500.396617, mean_eps: 0.447970
 184159/300000: episode: 1713, duration: 0.716s, episode steps: 105, steps per second: 147, episode reward: -57.385, mean reward: -0.547 [-100.000, 18.211], mean action: 1.838 [0.000, 3.000],  loss: 193.045146, mse: 212201.222321, mean_q: 497.734942, mean_eps: 0.447682
 184285/300000: episode: 1714, duration: 0.870s, episode steps: 126, steps per second: 145, episode reward: 22.299, mean reward:  0.177 [-100.000, 23.053], mean action: 1.627 [0.000, 3.000],  loss: 212.459556, mse: 211086.298611, mean_q: 497.017824, mean_eps: 0.447335
 184425/300000: episode: 1715, duration: 0.951s, episode steps: 140, steps per second: 147, episode reward: 21.448, mean reward:  0.153 [-100.000, 22.385], mean action: 1.657 [0.000, 3.000],  loss: 240.474932, mse: 213661.214732, mean_q: 500.610762, mean_eps: 0.446937
 184498/300000: episode: 1716, duration: 0.493s, episode steps:  73, steps per second: 148, episode reward: -57.601, mean reward: -0.789 [-100.000,  6.503], mean action: 1.699 [0.000, 3.000],  loss: 277.929773, mse: 218288.931293, mean_q: 507.117749, mean_eps: 0.446617
 184579/300000: episode: 1717, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -70.102, mean reward: -0.865 [-100.000,  7.923], mean action: 1.815 [0.000, 3.000],  loss: 270.356477, mse: 213829.151427, mean_q: 501.587225, mean_eps: 0.446386
 184680/300000: episode: 1718, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: -87.875, mean reward: -0.870 [-100.000,  9.354], mean action: 1.614 [0.000, 3.000],  loss: 255.154105, mse: 212215.487933, mean_q: 496.316170, mean_eps: 0.446113
 184781/300000: episode: 1719, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: -44.726, mean reward: -0.443 [-100.000, 17.989], mean action: 1.772 [0.000, 3.000],  loss: 204.888717, mse: 214841.447401, mean_q: 503.331145, mean_eps: 0.445810
 184897/300000: episode: 1720, duration: 0.781s, episode steps: 116, steps per second: 149, episode reward: 31.365, mean reward:  0.270 [-100.000, 24.725], mean action: 1.776 [0.000, 3.000],  loss: 285.006824, mse: 210041.753233, mean_q: 495.842812, mean_eps: 0.445485
 185032/300000: episode: 1721, duration: 0.927s, episode steps: 135, steps per second: 146, episode reward: -68.300, mean reward: -0.506 [-100.000,  9.213], mean action: 1.563 [0.000, 3.000],  loss: 262.954553, mse: 208206.082523, mean_q: 494.091254, mean_eps: 0.445108
 185109/300000: episode: 1722, duration: 0.516s, episode steps:  77, steps per second: 149, episode reward: -57.499, mean reward: -0.747 [-100.000, 12.752], mean action: 1.662 [0.000, 3.000],  loss: 275.579849, mse: 208708.908888, mean_q: 493.076286, mean_eps: 0.444790
 185208/300000: episode: 1723, duration: 0.664s, episode steps:  99, steps per second: 149, episode reward: -33.009, mean reward: -0.333 [-100.000, 11.474], mean action: 1.747 [0.000, 3.000],  loss: 280.926571, mse: 207659.184975, mean_q: 493.331795, mean_eps: 0.444526
 185318/300000: episode: 1724, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -297.159, mean reward: -2.701 [-100.000,  2.032], mean action: 1.400 [0.000, 3.000],  loss: 229.086615, mse: 208257.136648, mean_q: 492.106408, mean_eps: 0.444212
 185411/300000: episode: 1725, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -17.231, mean reward: -0.185 [-100.000, 15.510], mean action: 1.828 [0.000, 3.000],  loss: 206.297662, mse: 207986.049227, mean_q: 491.046152, mean_eps: 0.443908
 185508/300000: episode: 1726, duration: 0.648s, episode steps:  97, steps per second: 150, episode reward: -7.125, mean reward: -0.073 [-100.000, 14.352], mean action: 1.443 [0.000, 3.000],  loss: 259.289925, mse: 206482.817977, mean_q: 493.728792, mean_eps: 0.443623
 185614/300000: episode: 1727, duration: 0.759s, episode steps: 106, steps per second: 140, episode reward: -36.343, mean reward: -0.343 [-100.000,  5.694], mean action: 1.651 [0.000, 3.000],  loss: 206.312874, mse: 201370.718455, mean_q: 485.254285, mean_eps: 0.443319
 185709/300000: episode: 1728, duration: 0.664s, episode steps:  95, steps per second: 143, episode reward: -38.386, mean reward: -0.404 [-100.000, 16.579], mean action: 1.600 [0.000, 3.000],  loss: 265.496515, mse: 199672.019572, mean_q: 485.047437, mean_eps: 0.443017
 185794/300000: episode: 1729, duration: 0.573s, episode steps:  85, steps per second: 148, episode reward: -85.186, mean reward: -1.002 [-100.000, 19.888], mean action: 1.647 [0.000, 3.000],  loss: 221.292683, mse: 198501.950919, mean_q: 484.214911, mean_eps: 0.442747
 185930/300000: episode: 1730, duration: 0.995s, episode steps: 136, steps per second: 137, episode reward:  1.093, mean reward:  0.008 [-100.000, 18.168], mean action: 1.603 [0.000, 3.000],  loss: 280.248877, mse: 195634.797679, mean_q: 478.632309, mean_eps: 0.442416
 186026/300000: episode: 1731, duration: 0.647s, episode steps:  96, steps per second: 148, episode reward: -104.655, mean reward: -1.090 [-100.000,  8.689], mean action: 1.740 [0.000, 3.000],  loss: 236.052505, mse: 195080.525716, mean_q: 477.987983, mean_eps: 0.442068
 186109/300000: episode: 1732, duration: 0.554s, episode steps:  83, steps per second: 150, episode reward: -26.727, mean reward: -0.322 [-100.000, 12.040], mean action: 1.506 [0.000, 3.000],  loss: 217.366073, mse: 193894.630836, mean_q: 475.436953, mean_eps: 0.441799
 186177/300000: episode: 1733, duration: 0.473s, episode steps:  68, steps per second: 144, episode reward: -95.715, mean reward: -1.408 [-100.000, 10.389], mean action: 1.603 [0.000, 3.000],  loss: 185.069383, mse: 193646.651884, mean_q: 475.740462, mean_eps: 0.441572
 186275/300000: episode: 1734, duration: 0.780s, episode steps:  98, steps per second: 126, episode reward: -29.090, mean reward: -0.297 [-100.000, 19.612], mean action: 1.673 [0.000, 3.000],  loss: 159.993348, mse: 191407.923629, mean_q: 472.338873, mean_eps: 0.441323
 186361/300000: episode: 1735, duration: 0.690s, episode steps:  86, steps per second: 125, episode reward: 15.931, mean reward:  0.185 [-100.000, 18.532], mean action: 1.686 [0.000, 3.000],  loss: 179.312977, mse: 192271.206577, mean_q: 475.123585, mean_eps: 0.441047
 186473/300000: episode: 1736, duration: 0.875s, episode steps: 112, steps per second: 128, episode reward: 40.266, mean reward:  0.360 [-100.000, 17.562], mean action: 1.705 [0.000, 3.000],  loss: 162.419891, mse: 187702.740374, mean_q: 465.176371, mean_eps: 0.440751
 186537/300000: episode: 1737, duration: 0.498s, episode steps:  64, steps per second: 129, episode reward: -38.389, mean reward: -0.600 [-100.000, 18.068], mean action: 1.672 [0.000, 3.000],  loss: 145.391706, mse: 189282.519043, mean_q: 471.078465, mean_eps: 0.440487
 186652/300000: episode: 1738, duration: 0.834s, episode steps: 115, steps per second: 138, episode reward: -35.542, mean reward: -0.309 [-100.000, 12.281], mean action: 1.626 [0.000, 3.000],  loss: 259.657994, mse: 188207.135054, mean_q: 467.297093, mean_eps: 0.440218
 186832/300000: episode: 1739, duration: 1.380s, episode steps: 180, steps per second: 130, episode reward: -190.437, mean reward: -1.058 [-100.000, 13.797], mean action: 1.794 [0.000, 3.000],  loss: 268.535172, mse: 188176.591753, mean_q: 467.858023, mean_eps: 0.439775
 187000/300000: episode: 1740, duration: 1.140s, episode steps: 168, steps per second: 147, episode reward: -194.056, mean reward: -1.155 [-100.000,  5.183], mean action: 1.720 [0.000, 3.000],  loss: 225.439477, mse: 193119.894717, mean_q: 475.558689, mean_eps: 0.439254
 188000/300000: episode: 1741, duration: 7.504s, episode steps: 1000, steps per second: 133, episode reward: 86.230, mean reward:  0.086 [-24.842, 23.090], mean action: 1.558 [0.000, 3.000],  loss: 205.806792, mse: 184787.017922, mean_q: 464.594928, mean_eps: 0.437501
 188177/300000: episode: 1742, duration: 1.360s, episode steps: 177, steps per second: 130, episode reward: -88.830, mean reward: -0.502 [-100.000,  7.238], mean action: 1.808 [0.000, 3.000],  loss: 203.551445, mse: 183495.868379, mean_q: 463.884533, mean_eps: 0.435736
 188279/300000: episode: 1743, duration: 0.735s, episode steps: 102, steps per second: 139, episode reward: -11.001, mean reward: -0.108 [-100.000, 14.182], mean action: 1.775 [0.000, 3.000],  loss: 213.458293, mse: 180206.359835, mean_q: 459.585859, mean_eps: 0.435318
 188404/300000: episode: 1744, duration: 0.860s, episode steps: 125, steps per second: 145, episode reward: 22.873, mean reward:  0.183 [-100.000, 23.919], mean action: 1.544 [0.000, 3.000],  loss: 249.670595, mse: 177131.616625, mean_q: 454.357897, mean_eps: 0.434977
 188555/300000: episode: 1745, duration: 1.061s, episode steps: 151, steps per second: 142, episode reward: -468.303, mean reward: -3.101 [-100.000, 84.377], mean action: 1.987 [0.000, 3.000],  loss: 230.584255, mse: 171062.594785, mean_q: 443.587642, mean_eps: 0.434563
 188676/300000: episode: 1746, duration: 0.820s, episode steps: 121, steps per second: 147, episode reward: -355.362, mean reward: -2.937 [-100.000, 104.990], mean action: 1.653 [0.000, 3.000],  loss: 287.480340, mse: 173886.417097, mean_q: 446.466458, mean_eps: 0.434155
 188860/300000: episode: 1747, duration: 1.251s, episode steps: 184, steps per second: 147, episode reward: -164.768, mean reward: -0.895 [-100.000,  8.668], mean action: 1.609 [0.000, 3.000],  loss: 206.847247, mse: 174377.467519, mean_q: 445.533809, mean_eps: 0.433698
 188998/300000: episode: 1748, duration: 0.926s, episode steps: 138, steps per second: 149, episode reward: -89.008, mean reward: -0.645 [-100.000,  7.038], mean action: 1.761 [0.000, 3.000],  loss: 146.110565, mse: 170242.346298, mean_q: 441.026236, mean_eps: 0.433215
 189117/300000: episode: 1749, duration: 0.813s, episode steps: 119, steps per second: 146, episode reward: -162.670, mean reward: -1.367 [-100.000,  3.695], mean action: 1.605 [0.000, 3.000],  loss: 152.134395, mse: 173536.065257, mean_q: 445.777510, mean_eps: 0.432829
 189217/300000: episode: 1750, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -45.554, mean reward: -0.456 [-100.000,  8.293], mean action: 1.870 [0.000, 3.000],  loss: 176.767203, mse: 168999.440547, mean_q: 438.002183, mean_eps: 0.432501
 189305/300000: episode: 1751, duration: 0.590s, episode steps:  88, steps per second: 149, episode reward: 19.009, mean reward:  0.216 [-100.000, 14.371], mean action: 1.682 [0.000, 3.000],  loss: 128.450753, mse: 169383.463956, mean_q: 440.472748, mean_eps: 0.432219
 189426/300000: episode: 1752, duration: 0.822s, episode steps: 121, steps per second: 147, episode reward: 12.264, mean reward:  0.101 [-100.000, 16.539], mean action: 1.744 [0.000, 3.000],  loss: 236.300939, mse: 168336.736699, mean_q: 439.600654, mean_eps: 0.431905
 189526/300000: episode: 1753, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -29.343, mean reward: -0.293 [-100.000,  9.660], mean action: 1.610 [0.000, 3.000],  loss: 161.464017, mse: 165019.002109, mean_q: 435.080680, mean_eps: 0.431573
 189609/300000: episode: 1754, duration: 0.555s, episode steps:  83, steps per second: 150, episode reward: -55.207, mean reward: -0.665 [-100.000, 20.473], mean action: 1.759 [0.000, 3.000],  loss: 180.455957, mse: 164358.849115, mean_q: 435.866975, mean_eps: 0.431299
 189744/300000: episode: 1755, duration: 0.913s, episode steps: 135, steps per second: 148, episode reward: -155.015, mean reward: -1.148 [-100.000, 32.082], mean action: 1.563 [0.000, 3.000],  loss: 189.117389, mse: 162467.976968, mean_q: 430.941597, mean_eps: 0.430972
 189838/300000: episode: 1756, duration: 0.631s, episode steps:  94, steps per second: 149, episode reward: -83.915, mean reward: -0.893 [-100.000, 17.687], mean action: 1.436 [0.000, 3.000],  loss: 191.711844, mse: 162700.063165, mean_q: 433.937517, mean_eps: 0.430629
 189947/300000: episode: 1757, duration: 0.753s, episode steps: 109, steps per second: 145, episode reward: -93.026, mean reward: -0.853 [-100.000,  6.347], mean action: 1.725 [0.000, 3.000],  loss: 277.888887, mse: 157060.171947, mean_q: 425.127530, mean_eps: 0.430324
 190047/300000: episode: 1758, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -12.153, mean reward: -0.122 [-100.000, 18.036], mean action: 1.680 [0.000, 3.000],  loss: 219.366544, mse: 157739.585781, mean_q: 428.074980, mean_eps: 0.430011
 190213/300000: episode: 1759, duration: 1.201s, episode steps: 166, steps per second: 138, episode reward: -104.628, mean reward: -0.630 [-100.000, 14.745], mean action: 1.837 [0.000, 3.000],  loss: 233.081124, mse: 156985.169663, mean_q: 427.348668, mean_eps: 0.429611
 190315/300000: episode: 1760, duration: 0.678s, episode steps: 102, steps per second: 150, episode reward:  1.565, mean reward:  0.015 [-100.000, 17.330], mean action: 1.814 [0.000, 3.000],  loss: 155.109521, mse: 156738.541054, mean_q: 427.049234, mean_eps: 0.429210
 190418/300000: episode: 1761, duration: 0.722s, episode steps: 103, steps per second: 143, episode reward: 21.299, mean reward:  0.207 [-100.000, 15.871], mean action: 1.709 [0.000, 3.000],  loss: 148.896075, mse: 155724.847618, mean_q: 425.176609, mean_eps: 0.428902
 190657/300000: episode: 1762, duration: 1.606s, episode steps: 239, steps per second: 149, episode reward: -134.546, mean reward: -0.563 [-100.000, 22.831], mean action: 1.732 [0.000, 3.000],  loss: 154.657265, mse: 152532.180112, mean_q: 419.935310, mean_eps: 0.428389
 190762/300000: episode: 1763, duration: 0.740s, episode steps: 105, steps per second: 142, episode reward: -40.606, mean reward: -0.387 [-100.000, 11.825], mean action: 1.752 [0.000, 3.000],  loss: 235.352126, mse: 150154.132440, mean_q: 416.897719, mean_eps: 0.427873
 190891/300000: episode: 1764, duration: 0.858s, episode steps: 129, steps per second: 150, episode reward: 11.042, mean reward:  0.086 [-100.000, 18.187], mean action: 1.837 [0.000, 3.000],  loss: 153.335222, mse: 148671.744973, mean_q: 415.863308, mean_eps: 0.427522
 190988/300000: episode: 1765, duration: 0.659s, episode steps:  97, steps per second: 147, episode reward: -137.451, mean reward: -1.417 [-100.000,  2.166], mean action: 1.732 [0.000, 3.000],  loss: 242.118357, mse: 144532.600918, mean_q: 409.777078, mean_eps: 0.427183
 191197/300000: episode: 1766, duration: 1.444s, episode steps: 209, steps per second: 145, episode reward: -113.433, mean reward: -0.543 [-100.000,  8.699], mean action: 1.718 [0.000, 3.000],  loss: 201.888827, mse: 144494.019625, mean_q: 408.201226, mean_eps: 0.426724
 191360/300000: episode: 1767, duration: 1.114s, episode steps: 163, steps per second: 146, episode reward: -265.087, mean reward: -1.626 [-100.000,  3.688], mean action: 1.779 [0.000, 3.000],  loss: 206.153524, mse: 142700.457822, mean_q: 406.147922, mean_eps: 0.426166
 191474/300000: episode: 1768, duration: 0.762s, episode steps: 114, steps per second: 150, episode reward: -33.372, mean reward: -0.293 [-100.000,  9.540], mean action: 1.842 [0.000, 3.000],  loss: 192.757343, mse: 140841.474370, mean_q: 403.297407, mean_eps: 0.425751
 191688/300000: episode: 1769, duration: 1.493s, episode steps: 214, steps per second: 143, episode reward: 39.051, mean reward:  0.182 [-100.000, 17.580], mean action: 1.785 [0.000, 3.000],  loss: 174.696327, mse: 142233.273693, mean_q: 404.567627, mean_eps: 0.425259
 191881/300000: episode: 1770, duration: 1.296s, episode steps: 193, steps per second: 149, episode reward: -118.597, mean reward: -0.614 [-100.000, 10.555], mean action: 1.782 [0.000, 3.000],  loss: 183.482790, mse: 143920.148357, mean_q: 408.100937, mean_eps: 0.424648
 192003/300000: episode: 1771, duration: 0.841s, episode steps: 122, steps per second: 145, episode reward: -127.420, mean reward: -1.044 [-100.000, 67.540], mean action: 1.811 [0.000, 3.000],  loss: 189.997688, mse: 142152.706199, mean_q: 404.722953, mean_eps: 0.424176
 192096/300000: episode: 1772, duration: 0.623s, episode steps:  93, steps per second: 149, episode reward: -82.309, mean reward: -0.885 [-100.000, 14.609], mean action: 1.742 [0.000, 3.000],  loss: 166.847182, mse: 140290.036626, mean_q: 402.566621, mean_eps: 0.423853
 192223/300000: episode: 1773, duration: 0.859s, episode steps: 127, steps per second: 148, episode reward: -138.686, mean reward: -1.092 [-100.000,  6.542], mean action: 1.764 [0.000, 3.000],  loss: 165.978782, mse: 139379.678334, mean_q: 400.156843, mean_eps: 0.423523
 192582/300000: episode: 1774, duration: 2.533s, episode steps: 359, steps per second: 142, episode reward: -34.588, mean reward: -0.096 [-100.000, 56.330], mean action: 1.669 [0.000, 3.000],  loss: 207.868922, mse: 137578.466269, mean_q: 397.928510, mean_eps: 0.422794
 192665/300000: episode: 1775, duration: 0.556s, episode steps:  83, steps per second: 149, episode reward: -51.848, mean reward: -0.625 [-100.000, 18.609], mean action: 1.578 [0.000, 3.000],  loss: 178.780532, mse: 135869.669898, mean_q: 394.782246, mean_eps: 0.422131
 192798/300000: episode: 1776, duration: 0.897s, episode steps: 133, steps per second: 148, episode reward: -34.169, mean reward: -0.257 [-100.000, 27.545], mean action: 1.774 [0.000, 3.000],  loss: 259.445978, mse: 135702.335115, mean_q: 396.731428, mean_eps: 0.421807
 192921/300000: episode: 1777, duration: 0.842s, episode steps: 123, steps per second: 146, episode reward: -203.971, mean reward: -1.658 [-100.000,  2.268], mean action: 1.333 [0.000, 3.000],  loss: 242.343025, mse: 136677.686103, mean_q: 398.077682, mean_eps: 0.421423
 193078/300000: episode: 1778, duration: 1.060s, episode steps: 157, steps per second: 148, episode reward: -109.511, mean reward: -0.698 [-100.000, 10.393], mean action: 1.561 [0.000, 3.000],  loss: 256.063533, mse: 136917.984922, mean_q: 399.521767, mean_eps: 0.421003
 193197/300000: episode: 1779, duration: 0.793s, episode steps: 119, steps per second: 150, episode reward: -90.386, mean reward: -0.760 [-100.000,  7.130], mean action: 1.756 [0.000, 3.000],  loss: 150.117837, mse: 139105.454438, mean_q: 403.287241, mean_eps: 0.420589
 193402/300000: episode: 1780, duration: 1.427s, episode steps: 205, steps per second: 144, episode reward: -85.516, mean reward: -0.417 [-100.000, 18.672], mean action: 1.766 [0.000, 3.000],  loss: 211.730751, mse: 134833.092416, mean_q: 395.342166, mean_eps: 0.420103
 193546/300000: episode: 1781, duration: 0.981s, episode steps: 144, steps per second: 147, episode reward: -31.239, mean reward: -0.217 [-100.000, 11.526], mean action: 1.757 [0.000, 3.000],  loss: 236.010085, mse: 134945.588596, mean_q: 397.183146, mean_eps: 0.419579
 193630/300000: episode: 1782, duration: 0.588s, episode steps:  84, steps per second: 143, episode reward: -48.508, mean reward: -0.577 [-100.000,  7.531], mean action: 1.738 [0.000, 3.000],  loss: 159.593593, mse: 137159.622954, mean_q: 402.972434, mean_eps: 0.419238
 193802/300000: episode: 1783, duration: 1.158s, episode steps: 172, steps per second: 149, episode reward: -206.644, mean reward: -1.201 [-100.000, 10.683], mean action: 1.797 [0.000, 3.000],  loss: 194.655645, mse: 134825.256632, mean_q: 398.391570, mean_eps: 0.418853
 193911/300000: episode: 1784, duration: 0.762s, episode steps: 109, steps per second: 143, episode reward: -66.333, mean reward: -0.609 [-100.000, 14.680], mean action: 1.752 [0.000, 3.000],  loss: 133.834748, mse: 137317.820814, mean_q: 402.456588, mean_eps: 0.418432
 194285/300000: episode: 1785, duration: 2.617s, episode steps: 374, steps per second: 143, episode reward: -196.671, mean reward: -0.526 [-100.000, 19.769], mean action: 1.799 [0.000, 3.000],  loss: 194.403753, mse: 139456.212609, mean_q: 406.544523, mean_eps: 0.417707
 194953/300000: episode: 1786, duration: 4.926s, episode steps: 668, steps per second: 136, episode reward: -287.695, mean reward: -0.431 [-100.000, 20.419], mean action: 1.795 [0.000, 3.000],  loss: 192.826600, mse: 137396.843317, mean_q: 403.386917, mean_eps: 0.416144
 195072/300000: episode: 1787, duration: 0.819s, episode steps: 119, steps per second: 145, episode reward: -79.025, mean reward: -0.664 [-100.000, 10.367], mean action: 1.639 [0.000, 3.000],  loss: 225.114895, mse: 134090.708902, mean_q: 399.954025, mean_eps: 0.414964
 195203/300000: episode: 1788, duration: 0.878s, episode steps: 131, steps per second: 149, episode reward: -172.110, mean reward: -1.314 [-100.000,  8.162], mean action: 1.771 [0.000, 3.000],  loss: 221.056275, mse: 132010.798187, mean_q: 396.417483, mean_eps: 0.414589
 195345/300000: episode: 1789, duration: 0.958s, episode steps: 142, steps per second: 148, episode reward: -77.537, mean reward: -0.546 [-100.000,  3.993], mean action: 1.634 [0.000, 3.000],  loss: 216.372717, mse: 131022.028279, mean_q: 392.207546, mean_eps: 0.414180
 195444/300000: episode: 1790, duration: 0.698s, episode steps:  99, steps per second: 142, episode reward: -22.818, mean reward: -0.230 [-100.000, 11.593], mean action: 1.747 [0.000, 3.000],  loss: 122.465276, mse: 129554.541193, mean_q: 392.023769, mean_eps: 0.413818
 195719/300000: episode: 1791, duration: 2.063s, episode steps: 275, steps per second: 133, episode reward: -235.412, mean reward: -0.856 [-100.000, 14.131], mean action: 1.651 [0.000, 3.000],  loss: 150.320126, mse: 128162.991023, mean_q: 388.528536, mean_eps: 0.413257
 195827/300000: episode: 1792, duration: 0.863s, episode steps: 108, steps per second: 125, episode reward: -41.721, mean reward: -0.386 [-100.000, 18.018], mean action: 1.611 [0.000, 3.000],  loss: 141.890983, mse: 125520.553385, mean_q: 383.791751, mean_eps: 0.412683
 195950/300000: episode: 1793, duration: 0.925s, episode steps: 123, steps per second: 133, episode reward: -34.860, mean reward: -0.283 [-100.000, 10.882], mean action: 1.813 [0.000, 3.000],  loss: 259.916305, mse: 124229.105501, mean_q: 381.318679, mean_eps: 0.412336
 196064/300000: episode: 1794, duration: 0.858s, episode steps: 114, steps per second: 133, episode reward: -93.841, mean reward: -0.823 [-100.000, 11.869], mean action: 1.605 [0.000, 3.000],  loss: 203.618319, mse: 122528.349438, mean_q: 378.371692, mean_eps: 0.411980
 196163/300000: episode: 1795, duration: 0.739s, episode steps:  99, steps per second: 134, episode reward: -49.787, mean reward: -0.503 [-100.000, 15.013], mean action: 1.838 [0.000, 3.000],  loss: 155.257289, mse: 120706.325521, mean_q: 376.160258, mean_eps: 0.411661
 196298/300000: episode: 1796, duration: 0.986s, episode steps: 135, steps per second: 137, episode reward: -106.219, mean reward: -0.787 [-100.000, 56.891], mean action: 1.830 [0.000, 3.000],  loss: 165.693680, mse: 118680.995197, mean_q: 373.131717, mean_eps: 0.411310
 196395/300000: episode: 1797, duration: 0.658s, episode steps:  97, steps per second: 147, episode reward: -23.919, mean reward: -0.247 [-100.000, 10.401], mean action: 1.784 [0.000, 3.000],  loss: 214.252963, mse: 118009.194910, mean_q: 374.315992, mean_eps: 0.410962
 196505/300000: episode: 1798, duration: 0.732s, episode steps: 110, steps per second: 150, episode reward: -19.295, mean reward: -0.175 [-100.000, 17.786], mean action: 1.682 [0.000, 3.000],  loss: 155.876827, mse: 119442.184872, mean_q: 376.517257, mean_eps: 0.410652
 196635/300000: episode: 1799, duration: 0.895s, episode steps: 130, steps per second: 145, episode reward: -323.859, mean reward: -2.491 [-100.000, 61.880], mean action: 1.708 [0.000, 3.000],  loss: 223.964732, mse: 118121.278486, mean_q: 373.574299, mean_eps: 0.410292
 196745/300000: episode: 1800, duration: 0.754s, episode steps: 110, steps per second: 146, episode reward: -96.922, mean reward: -0.881 [-100.000,  3.685], mean action: 1.818 [0.000, 3.000],  loss: 202.594678, mse: 118374.733239, mean_q: 373.391174, mean_eps: 0.409931
 196866/300000: episode: 1801, duration: 0.828s, episode steps: 121, steps per second: 146, episode reward: -98.411, mean reward: -0.813 [-100.000, 14.039], mean action: 1.760 [0.000, 3.000],  loss: 158.637470, mse: 119247.292485, mean_q: 375.024651, mean_eps: 0.409585
 197056/300000: episode: 1802, duration: 1.301s, episode steps: 190, steps per second: 146, episode reward: -124.852, mean reward: -0.657 [-100.000,  9.633], mean action: 1.747 [0.000, 3.000],  loss: 196.855090, mse: 117985.299959, mean_q: 372.469256, mean_eps: 0.409119
 197183/300000: episode: 1803, duration: 0.947s, episode steps: 127, steps per second: 134, episode reward: -75.927, mean reward: -0.598 [-100.000, 11.831], mean action: 1.669 [0.000, 3.000],  loss: 163.580515, mse: 115013.330832, mean_q: 369.155537, mean_eps: 0.408643
 197300/300000: episode: 1804, duration: 0.951s, episode steps: 117, steps per second: 123, episode reward: -46.097, mean reward: -0.394 [-100.000, 18.761], mean action: 1.889 [0.000, 3.000],  loss: 181.864814, mse: 113290.091413, mean_q: 365.859119, mean_eps: 0.408277
 197427/300000: episode: 1805, duration: 0.999s, episode steps: 127, steps per second: 127, episode reward: -193.383, mean reward: -1.523 [-100.000, 78.357], mean action: 1.835 [0.000, 3.000],  loss: 140.682127, mse: 111610.905635, mean_q: 360.903993, mean_eps: 0.407911
 197541/300000: episode: 1806, duration: 0.925s, episode steps: 114, steps per second: 123, episode reward: -80.713, mean reward: -0.708 [-100.000, 20.446], mean action: 1.719 [0.000, 3.000],  loss: 184.165891, mse: 109863.345600, mean_q: 357.992043, mean_eps: 0.407550
 197740/300000: episode: 1807, duration: 1.451s, episode steps: 199, steps per second: 137, episode reward: -164.567, mean reward: -0.827 [-100.000, 10.101], mean action: 1.844 [0.000, 3.000],  loss: 124.151900, mse: 108109.819763, mean_q: 356.184411, mean_eps: 0.407080
 197916/300000: episode: 1808, duration: 1.209s, episode steps: 176, steps per second: 146, episode reward: 22.539, mean reward:  0.128 [-100.000, 19.387], mean action: 1.631 [0.000, 3.000],  loss: 180.682985, mse: 103769.132280, mean_q: 349.666997, mean_eps: 0.406517
 198077/300000: episode: 1809, duration: 1.110s, episode steps: 161, steps per second: 145, episode reward: -366.377, mean reward: -2.276 [-100.000, 132.598], mean action: 1.590 [0.000, 3.000],  loss: 154.277835, mse: 102591.581716, mean_q: 347.934433, mean_eps: 0.406012
 198192/300000: episode: 1810, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: -89.065, mean reward: -0.774 [-100.000, 11.666], mean action: 1.687 [0.000, 3.000],  loss: 182.681679, mse: 101665.949524, mean_q: 346.151263, mean_eps: 0.405598
 198394/300000: episode: 1811, duration: 1.389s, episode steps: 202, steps per second: 145, episode reward: -191.584, mean reward: -0.948 [-100.000,  8.490], mean action: 1.718 [0.000, 3.000],  loss: 143.824873, mse: 100532.233408, mean_q: 344.452805, mean_eps: 0.405122
 198565/300000: episode: 1812, duration: 1.157s, episode steps: 171, steps per second: 148, episode reward: -177.953, mean reward: -1.041 [-100.000, 12.643], mean action: 1.825 [0.000, 3.000],  loss: 159.263696, mse: 97491.743193, mean_q: 338.488996, mean_eps: 0.404563
 199224/300000: episode: 1813, duration: 4.866s, episode steps: 659, steps per second: 135, episode reward: -303.187, mean reward: -0.460 [-100.000, 55.299], mean action: 1.938 [0.000, 3.000],  loss: 133.900483, mse: 97137.396766, mean_q: 338.767217, mean_eps: 0.403318
 199372/300000: episode: 1814, duration: 1.029s, episode steps: 148, steps per second: 144, episode reward: -41.756, mean reward: -0.282 [-100.000, 16.831], mean action: 1.797 [0.000, 3.000],  loss: 119.094854, mse: 94661.126056, mean_q: 333.407129, mean_eps: 0.402108
 199492/300000: episode: 1815, duration: 0.812s, episode steps: 120, steps per second: 148, episode reward: -19.424, mean reward: -0.162 [-100.000, 10.107], mean action: 1.842 [0.000, 3.000],  loss: 143.561548, mse: 95991.715430, mean_q: 336.610333, mean_eps: 0.401706
 200492/300000: episode: 1816, duration: 7.437s, episode steps: 1000, steps per second: 134, episode reward: 54.204, mean reward:  0.054 [-22.306, 26.431], mean action: 1.871 [0.000, 3.000],  loss: 138.210768, mse: 92702.764039, mean_q: 329.473096, mean_eps: 0.400025
 200625/300000: episode: 1817, duration: 0.904s, episode steps: 133, steps per second: 147, episode reward: -47.831, mean reward: -0.360 [-100.000, 12.640], mean action: 1.519 [0.000, 3.000],  loss: 112.385852, mse: 90479.749471, mean_q: 325.946175, mean_eps: 0.398326
 201369/300000: episode: 1818, duration: 5.487s, episode steps: 744, steps per second: 136, episode reward: -123.953, mean reward: -0.167 [-100.000, 20.159], mean action: 1.860 [0.000, 3.000],  loss: 145.345077, mse: 87152.931651, mean_q: 318.828490, mean_eps: 0.397010
 201467/300000: episode: 1819, duration: 0.650s, episode steps:  98, steps per second: 151, episode reward: 12.800, mean reward:  0.131 [-100.000, 20.063], mean action: 1.551 [0.000, 3.000],  loss: 138.219124, mse: 84272.314413, mean_q: 312.792409, mean_eps: 0.395747
 202467/300000: episode: 1820, duration: 7.222s, episode steps: 1000, steps per second: 138, episode reward: 66.054, mean reward:  0.066 [-23.278, 24.151], mean action: 1.876 [0.000, 3.000],  loss: 125.198577, mse: 84407.666352, mean_q: 313.036417, mean_eps: 0.394101
 202584/300000: episode: 1821, duration: 0.807s, episode steps: 117, steps per second: 145, episode reward:  8.155, mean reward:  0.070 [-100.000,  9.149], mean action: 1.761 [0.000, 3.000],  loss: 80.121446, mse: 85714.216747, mean_q: 314.947373, mean_eps: 0.392425
 202764/300000: episode: 1822, duration: 1.227s, episode steps: 180, steps per second: 147, episode reward: -128.365, mean reward: -0.713 [-100.000, 14.588], mean action: 1.833 [0.000, 3.000],  loss: 104.790626, mse: 84860.995052, mean_q: 314.915468, mean_eps: 0.391979
 202950/300000: episode: 1823, duration: 1.300s, episode steps: 186, steps per second: 143, episode reward: -102.704, mean reward: -0.552 [-100.000, 10.688], mean action: 1.602 [0.000, 3.000],  loss: 105.608511, mse: 83777.232317, mean_q: 313.578001, mean_eps: 0.391431
 203082/300000: episode: 1824, duration: 0.922s, episode steps: 132, steps per second: 143, episode reward: -23.639, mean reward: -0.179 [-100.000, 25.474], mean action: 1.682 [0.000, 3.000],  loss: 115.275356, mse: 83100.617661, mean_q: 314.149172, mean_eps: 0.390954
 203191/300000: episode: 1825, duration: 0.774s, episode steps: 109, steps per second: 141, episode reward: -66.649, mean reward: -0.611 [-100.000, 11.219], mean action: 1.706 [0.000, 3.000],  loss: 129.593811, mse: 80673.883888, mean_q: 307.488792, mean_eps: 0.390592
 203310/300000: episode: 1826, duration: 0.803s, episode steps: 119, steps per second: 148, episode reward: -14.955, mean reward: -0.126 [-100.000,  9.302], mean action: 1.840 [0.000, 3.000],  loss: 163.901515, mse: 79934.834099, mean_q: 306.521028, mean_eps: 0.390250
 204310/300000: episode: 1827, duration: 7.750s, episode steps: 1000, steps per second: 129, episode reward: 58.136, mean reward:  0.058 [-24.069, 27.866], mean action: 1.815 [0.000, 3.000],  loss: 114.729566, mse: 76598.441895, mean_q: 299.251936, mean_eps: 0.388572
 204959/300000: episode: 1828, duration: 5.157s, episode steps: 649, steps per second: 126, episode reward: -255.365, mean reward: -0.393 [-100.000, 23.056], mean action: 1.515 [0.000, 3.000],  loss: 124.760520, mse: 74102.581369, mean_q: 294.541818, mean_eps: 0.386098
 205959/300000: episode: 1829, duration: 7.773s, episode steps: 1000, steps per second: 129, episode reward: 154.559, mean reward:  0.155 [-21.209, 28.205], mean action: 1.716 [0.000, 3.000],  loss: 126.537926, mse: 68451.023719, mean_q: 282.449455, mean_eps: 0.383625
 206195/300000: episode: 1830, duration: 1.678s, episode steps: 236, steps per second: 141, episode reward: -149.387, mean reward: -0.633 [-100.000, 23.419], mean action: 1.581 [0.000, 3.000],  loss: 106.416994, mse: 66772.525159, mean_q: 280.178801, mean_eps: 0.381771
 206308/300000: episode: 1831, duration: 0.876s, episode steps: 113, steps per second: 129, episode reward: -78.143, mean reward: -0.692 [-100.000, 17.942], mean action: 1.743 [0.000, 3.000],  loss: 119.835099, mse: 66156.487417, mean_q: 277.934303, mean_eps: 0.381247
 207308/300000: episode: 1832, duration: 7.894s, episode steps: 1000, steps per second: 127, episode reward: 94.543, mean reward:  0.095 [-23.535, 27.134], mean action: 1.672 [0.000, 3.000],  loss: 124.802430, mse: 64177.606598, mean_q: 276.114433, mean_eps: 0.379578
 207679/300000: episode: 1833, duration: 2.634s, episode steps: 371, steps per second: 141, episode reward: -247.458, mean reward: -0.667 [-100.000, 23.542], mean action: 1.720 [0.000, 3.000],  loss: 111.327401, mse: 64588.632570, mean_q: 277.288771, mean_eps: 0.377521
 207863/300000: episode: 1834, duration: 1.248s, episode steps: 184, steps per second: 147, episode reward: -101.270, mean reward: -0.550 [-100.000, 20.226], mean action: 1.701 [0.000, 3.000],  loss: 108.577457, mse: 62069.416037, mean_q: 271.255472, mean_eps: 0.376689
 208863/300000: episode: 1835, duration: 8.441s, episode steps: 1000, steps per second: 118, episode reward: 63.530, mean reward:  0.064 [-24.098, 22.445], mean action: 1.628 [0.000, 3.000],  loss: 111.494215, mse: 57828.516656, mean_q: 261.026661, mean_eps: 0.374913
 209005/300000: episode: 1836, duration: 0.977s, episode steps: 142, steps per second: 145, episode reward: -11.522, mean reward: -0.081 [-100.000, 13.287], mean action: 1.775 [0.000, 3.000],  loss: 105.112063, mse: 57738.419454, mean_q: 261.944357, mean_eps: 0.373200
 209170/300000: episode: 1837, duration: 1.110s, episode steps: 165, steps per second: 149, episode reward: -53.477, mean reward: -0.324 [-100.000, 11.443], mean action: 1.764 [0.000, 3.000],  loss: 99.939783, mse: 58264.155043, mean_q: 262.584640, mean_eps: 0.372739
 209344/300000: episode: 1838, duration: 1.193s, episode steps: 174, steps per second: 146, episode reward: -63.214, mean reward: -0.363 [-100.000, 13.306], mean action: 1.793 [0.000, 3.000],  loss: 119.201275, mse: 57849.045640, mean_q: 261.528669, mean_eps: 0.372231
 209556/300000: episode: 1839, duration: 1.432s, episode steps: 212, steps per second: 148, episode reward: -4.879, mean reward: -0.023 [-100.000, 14.749], mean action: 1.830 [0.000, 3.000],  loss: 100.880669, mse: 57902.817604, mean_q: 261.584540, mean_eps: 0.371652
 210556/300000: episode: 1840, duration: 7.580s, episode steps: 1000, steps per second: 132, episode reward: 21.667, mean reward:  0.022 [-23.223, 24.935], mean action: 1.998 [0.000, 3.000],  loss: 112.798662, mse: 57094.709441, mean_q: 259.133060, mean_eps: 0.369834
 210672/300000: episode: 1841, duration: 0.785s, episode steps: 116, steps per second: 148, episode reward: -5.750, mean reward: -0.050 [-100.000, 12.892], mean action: 1.741 [0.000, 3.000],  loss: 96.502900, mse: 58556.476057, mean_q: 263.867611, mean_eps: 0.368159
 210763/300000: episode: 1842, duration: 0.623s, episode steps:  91, steps per second: 146, episode reward: -15.776, mean reward: -0.173 [-100.000, 10.672], mean action: 1.604 [0.000, 3.000],  loss: 87.245871, mse: 57632.517342, mean_q: 261.390328, mean_eps: 0.367849
 211095/300000: episode: 1843, duration: 2.342s, episode steps: 332, steps per second: 142, episode reward: -165.420, mean reward: -0.498 [-100.000, 22.257], mean action: 1.557 [0.000, 3.000],  loss: 104.072478, mse: 56059.118082, mean_q: 257.929116, mean_eps: 0.367214
 211365/300000: episode: 1844, duration: 1.855s, episode steps: 270, steps per second: 146, episode reward: -39.297, mean reward: -0.146 [-100.000, 47.203], mean action: 1.430 [0.000, 3.000],  loss: 110.352255, mse: 52548.509997, mean_q: 248.302023, mean_eps: 0.366312
 211725/300000: episode: 1845, duration: 2.559s, episode steps: 360, steps per second: 141, episode reward: -328.981, mean reward: -0.914 [-100.000, 17.816], mean action: 1.744 [0.000, 3.000],  loss: 94.997405, mse: 53225.214887, mean_q: 249.907968, mean_eps: 0.365366
 212725/300000: episode: 1846, duration: 7.634s, episode steps: 1000, steps per second: 131, episode reward: 111.388, mean reward:  0.111 [-24.185, 22.971], mean action: 1.723 [0.000, 3.000],  loss: 104.360040, mse: 53045.216512, mean_q: 248.276325, mean_eps: 0.363326
 213725/300000: episode: 1847, duration: 7.899s, episode steps: 1000, steps per second: 127, episode reward: 42.242, mean reward:  0.042 [-22.820, 23.118], mean action: 1.493 [0.000, 3.000],  loss: 102.165893, mse: 50440.660820, mean_q: 242.422696, mean_eps: 0.360326
 214725/300000: episode: 1848, duration: 7.901s, episode steps: 1000, steps per second: 127, episode reward: 22.051, mean reward:  0.022 [-21.929, 44.717], mean action: 1.592 [0.000, 3.000],  loss: 97.091638, mse: 47811.980340, mean_q: 236.097321, mean_eps: 0.357326
 214825/300000: episode: 1849, duration: 0.759s, episode steps: 100, steps per second: 132, episode reward: -52.132, mean reward: -0.521 [-100.000,  8.043], mean action: 1.740 [0.000, 3.000],  loss: 90.428041, mse: 47339.321602, mean_q: 235.696656, mean_eps: 0.355677
 214963/300000: episode: 1850, duration: 1.018s, episode steps: 138, steps per second: 136, episode reward: -50.307, mean reward: -0.365 [-100.000, 10.005], mean action: 1.826 [0.000, 3.000],  loss: 119.319533, mse: 47975.584466, mean_q: 237.021938, mean_eps: 0.355320
 215148/300000: episode: 1851, duration: 1.414s, episode steps: 185, steps per second: 131, episode reward: -121.677, mean reward: -0.658 [-100.000, 10.230], mean action: 1.795 [0.000, 3.000],  loss: 98.789114, mse: 46636.478104, mean_q: 233.338543, mean_eps: 0.354835
 216148/300000: episode: 1852, duration: 8.109s, episode steps: 1000, steps per second: 123, episode reward: 75.614, mean reward:  0.076 [-24.070, 23.871], mean action: 1.456 [0.000, 3.000],  loss: 94.984093, mse: 44909.449773, mean_q: 228.524187, mean_eps: 0.353058
 216295/300000: episode: 1853, duration: 1.102s, episode steps: 147, steps per second: 133, episode reward: -147.262, mean reward: -1.002 [-100.000, 11.986], mean action: 1.667 [0.000, 3.000],  loss: 91.145145, mse: 42932.301100, mean_q: 222.938170, mean_eps: 0.351337
 217295/300000: episode: 1854, duration: 8.837s, episode steps: 1000, steps per second: 113, episode reward: -43.465, mean reward: -0.043 [-21.942, 16.434], mean action: 1.896 [0.000, 3.000],  loss: 92.401476, mse: 43412.679691, mean_q: 225.816546, mean_eps: 0.349616
 217416/300000: episode: 1855, duration: 0.890s, episode steps: 121, steps per second: 136, episode reward: -124.519, mean reward: -1.029 [-100.000, 21.530], mean action: 1.752 [0.000, 3.000],  loss: 71.568337, mse: 43761.223011, mean_q: 228.468833, mean_eps: 0.347935
 217554/300000: episode: 1856, duration: 1.012s, episode steps: 138, steps per second: 136, episode reward: -44.003, mean reward: -0.319 [-100.000, 19.502], mean action: 1.652 [0.000, 3.000],  loss: 90.940005, mse: 43263.320964, mean_q: 227.042533, mean_eps: 0.347546
 217665/300000: episode: 1857, duration: 0.773s, episode steps: 111, steps per second: 144, episode reward: -99.412, mean reward: -0.896 [-100.000, 16.258], mean action: 1.748 [0.000, 3.000],  loss: 128.586585, mse: 42681.657165, mean_q: 225.826394, mean_eps: 0.347173
 217768/300000: episode: 1858, duration: 0.741s, episode steps: 103, steps per second: 139, episode reward: -87.233, mean reward: -0.847 [-100.000,  8.841], mean action: 1.689 [0.000, 3.000],  loss: 77.119102, mse: 42242.119046, mean_q: 223.993982, mean_eps: 0.346852
 217981/300000: episode: 1859, duration: 1.610s, episode steps: 213, steps per second: 132, episode reward: -206.189, mean reward: -0.968 [-100.000, 10.012], mean action: 1.953 [0.000, 3.000],  loss: 90.630689, mse: 42021.011004, mean_q: 223.116738, mean_eps: 0.346378
 218981/300000: episode: 1860, duration: 8.207s, episode steps: 1000, steps per second: 122, episode reward: 123.429, mean reward:  0.123 [-22.849, 23.903], mean action: 1.546 [0.000, 3.000],  loss: 88.700126, mse: 40802.824297, mean_q: 220.395223, mean_eps: 0.344558
 219124/300000: episode: 1861, duration: 1.050s, episode steps: 143, steps per second: 136, episode reward: -154.946, mean reward: -1.084 [-100.000,  8.298], mean action: 1.552 [0.000, 3.000],  loss: 95.435547, mse: 39752.000027, mean_q: 217.163290, mean_eps: 0.342844
 219251/300000: episode: 1862, duration: 0.927s, episode steps: 127, steps per second: 137, episode reward: -212.825, mean reward: -1.676 [-100.000, 32.401], mean action: 1.457 [0.000, 3.000],  loss: 73.676916, mse: 39144.699234, mean_q: 215.325491, mean_eps: 0.342439
 219367/300000: episode: 1863, duration: 0.835s, episode steps: 116, steps per second: 139, episode reward: -192.974, mean reward: -1.664 [-100.000, 10.217], mean action: 1.698 [0.000, 3.000],  loss: 107.064748, mse: 39084.218514, mean_q: 215.676804, mean_eps: 0.342075
 219474/300000: episode: 1864, duration: 0.740s, episode steps: 107, steps per second: 145, episode reward: -100.718, mean reward: -0.941 [-100.000, 15.474], mean action: 2.000 [0.000, 3.000],  loss: 95.623590, mse: 38272.673518, mean_q: 213.571157, mean_eps: 0.341740
 219657/300000: episode: 1865, duration: 1.247s, episode steps: 183, steps per second: 147, episode reward: -30.681, mean reward: -0.168 [-100.000, 15.846], mean action: 1.902 [0.000, 3.000],  loss: 106.172144, mse: 38208.308967, mean_q: 213.615823, mean_eps: 0.341305
 219935/300000: episode: 1866, duration: 1.915s, episode steps: 278, steps per second: 145, episode reward: -267.771, mean reward: -0.963 [-100.000, 37.471], mean action: 1.745 [0.000, 3.000],  loss: 87.940855, mse: 39205.733209, mean_q: 216.641107, mean_eps: 0.340613
 220935/300000: episode: 1867, duration: 7.794s, episode steps: 1000, steps per second: 128, episode reward: -4.181, mean reward: -0.004 [-23.684, 30.087], mean action: 1.802 [0.000, 3.000],  loss: 97.433394, mse: 38063.339863, mean_q: 214.065304, mean_eps: 0.338697
 221074/300000: episode: 1868, duration: 0.988s, episode steps: 139, steps per second: 141, episode reward: -95.684, mean reward: -0.688 [-100.000,  8.367], mean action: 1.856 [0.000, 3.000],  loss: 96.966523, mse: 37424.341179, mean_q: 213.136029, mean_eps: 0.336988
 221184/300000: episode: 1869, duration: 0.810s, episode steps: 110, steps per second: 136, episode reward: -89.541, mean reward: -0.814 [-100.000, 14.259], mean action: 1.818 [0.000, 3.000],  loss: 128.072618, mse: 37066.146911, mean_q: 212.230318, mean_eps: 0.336615
 221279/300000: episode: 1870, duration: 0.702s, episode steps:  95, steps per second: 135, episode reward: -76.002, mean reward: -0.800 [-100.000, 15.218], mean action: 1.789 [0.000, 3.000],  loss: 99.625615, mse: 37102.361410, mean_q: 212.004026, mean_eps: 0.336307
 222279/300000: episode: 1871, duration: 7.922s, episode steps: 1000, steps per second: 126, episode reward: 30.607, mean reward:  0.031 [-22.572, 33.268], mean action: 2.015 [0.000, 3.000],  loss: 95.278568, mse: 36016.884559, mean_q: 209.694850, mean_eps: 0.334664
 222362/300000: episode: 1872, duration: 0.559s, episode steps:  83, steps per second: 148, episode reward: -300.544, mean reward: -3.621 [-100.000,  2.850], mean action: 1.398 [0.000, 3.000],  loss: 63.106678, mse: 36052.567842, mean_q: 210.795709, mean_eps: 0.333040
 222452/300000: episode: 1873, duration: 0.603s, episode steps:  90, steps per second: 149, episode reward: -14.097, mean reward: -0.157 [-100.000, 19.097], mean action: 1.856 [0.000, 3.000],  loss: 104.412385, mse: 35703.435894, mean_q: 209.213086, mean_eps: 0.332781
 222561/300000: episode: 1874, duration: 0.769s, episode steps: 109, steps per second: 142, episode reward: -376.005, mean reward: -3.450 [-100.000, 54.062], mean action: 1.569 [0.000, 3.000],  loss: 76.354310, mse: 35965.848803, mean_q: 210.596152, mean_eps: 0.332482
 222686/300000: episode: 1875, duration: 0.858s, episode steps: 125, steps per second: 146, episode reward: -27.978, mean reward: -0.224 [-100.000, 20.470], mean action: 1.896 [0.000, 3.000],  loss: 68.800997, mse: 36159.780531, mean_q: 210.190928, mean_eps: 0.332131
 222764/300000: episode: 1876, duration: 0.531s, episode steps:  78, steps per second: 147, episode reward: -232.144, mean reward: -2.976 [-100.000,  5.578], mean action: 2.000 [0.000, 3.000],  loss: 124.870863, mse: 35648.795222, mean_q: 208.740999, mean_eps: 0.331827
 222903/300000: episode: 1877, duration: 0.983s, episode steps: 139, steps per second: 141, episode reward: -54.418, mean reward: -0.391 [-100.000, 10.283], mean action: 1.899 [0.000, 3.000],  loss: 77.060726, mse: 35167.693064, mean_q: 207.065307, mean_eps: 0.331501
 223009/300000: episode: 1878, duration: 0.757s, episode steps: 106, steps per second: 140, episode reward: -288.216, mean reward: -2.719 [-100.000,  5.292], mean action: 1.811 [0.000, 3.000],  loss: 104.108956, mse: 35313.153394, mean_q: 206.419758, mean_eps: 0.331134
 223079/300000: episode: 1879, duration: 0.503s, episode steps:  70, steps per second: 139, episode reward: -93.664, mean reward: -1.338 [-100.000,  9.084], mean action: 2.057 [0.000, 3.000],  loss: 82.388300, mse: 34467.127706, mean_q: 204.183155, mean_eps: 0.330869
 223165/300000: episode: 1880, duration: 0.619s, episode steps:  86, steps per second: 139, episode reward: -53.463, mean reward: -0.622 [-100.000, 10.086], mean action: 1.663 [0.000, 3.000],  loss: 86.229865, mse: 35026.606831, mean_q: 205.736641, mean_eps: 0.330635
 223471/300000: episode: 1881, duration: 2.462s, episode steps: 306, steps per second: 124, episode reward: -206.015, mean reward: -0.673 [-100.000, 71.997], mean action: 2.013 [0.000, 3.000],  loss: 102.883406, mse: 34710.436798, mean_q: 204.456396, mean_eps: 0.330047
 223746/300000: episode: 1882, duration: 2.300s, episode steps: 275, steps per second: 120, episode reward: -112.320, mean reward: -0.408 [-100.000, 13.338], mean action: 1.807 [0.000, 3.000],  loss: 84.237365, mse: 33439.860739, mean_q: 201.236838, mean_eps: 0.329176
 224054/300000: episode: 1883, duration: 2.527s, episode steps: 308, steps per second: 122, episode reward:  8.255, mean reward:  0.027 [-100.000, 30.251], mean action: 1.766 [0.000, 3.000],  loss: 91.436481, mse: 33127.296387, mean_q: 200.384663, mean_eps: 0.328301
 224520/300000: episode: 1884, duration: 3.834s, episode steps: 466, steps per second: 122, episode reward: -136.914, mean reward: -0.294 [-100.000, 17.351], mean action: 1.946 [0.000, 3.000],  loss: 90.133499, mse: 33394.692777, mean_q: 200.918585, mean_eps: 0.327141
 225520/300000: episode: 1885, duration: 8.471s, episode steps: 1000, steps per second: 118, episode reward: 63.220, mean reward:  0.063 [-24.521, 24.825], mean action: 1.985 [0.000, 3.000],  loss: 89.148511, mse: 33199.263990, mean_q: 200.875652, mean_eps: 0.324941
 225595/300000: episode: 1886, duration: 0.507s, episode steps:  75, steps per second: 148, episode reward: -184.267, mean reward: -2.457 [-100.000, 23.987], mean action: 1.587 [0.000, 3.000],  loss: 113.828121, mse: 32982.353802, mean_q: 201.974066, mean_eps: 0.323329
 225822/300000: episode: 1887, duration: 1.590s, episode steps: 227, steps per second: 143, episode reward: -119.900, mean reward: -0.528 [-100.000, 19.146], mean action: 1.890 [0.000, 3.000],  loss: 88.901768, mse: 32182.133862, mean_q: 198.036695, mean_eps: 0.322876
 225951/300000: episode: 1888, duration: 0.959s, episode steps: 129, steps per second: 135, episode reward: -82.463, mean reward: -0.639 [-100.000, 19.065], mean action: 1.752 [0.000, 3.000],  loss: 99.163615, mse: 31651.889868, mean_q: 196.427650, mean_eps: 0.322342
 226113/300000: episode: 1889, duration: 1.204s, episode steps: 162, steps per second: 135, episode reward: -188.900, mean reward: -1.166 [-100.000,  8.855], mean action: 1.648 [0.000, 3.000],  loss: 97.684344, mse: 31087.608254, mean_q: 194.256319, mean_eps: 0.321906
 226242/300000: episode: 1890, duration: 0.874s, episode steps: 129, steps per second: 148, episode reward: -76.674, mean reward: -0.594 [-100.000, 10.599], mean action: 1.620 [0.000, 3.000],  loss: 84.743626, mse: 30578.191709, mean_q: 192.238459, mean_eps: 0.321469
 226533/300000: episode: 1891, duration: 2.055s, episode steps: 291, steps per second: 142, episode reward: -154.403, mean reward: -0.531 [-100.000, 15.276], mean action: 1.866 [0.000, 3.000],  loss: 107.880841, mse: 30869.469898, mean_q: 194.422260, mean_eps: 0.320839
 226657/300000: episode: 1892, duration: 0.843s, episode steps: 124, steps per second: 147, episode reward: -40.653, mean reward: -0.328 [-100.000, 11.421], mean action: 1.565 [0.000, 3.000],  loss: 78.548607, mse: 31589.878749, mean_q: 197.122250, mean_eps: 0.320216
 226771/300000: episode: 1893, duration: 0.796s, episode steps: 114, steps per second: 143, episode reward: -173.078, mean reward: -1.518 [-100.000, 32.220], mean action: 1.746 [0.000, 3.000],  loss: 93.733279, mse: 31204.706534, mean_q: 195.568779, mean_eps: 0.319860
 227771/300000: episode: 1894, duration: 8.057s, episode steps: 1000, steps per second: 124, episode reward: 72.366, mean reward:  0.072 [-23.635, 23.085], mean action: 1.597 [0.000, 3.000],  loss: 76.356580, mse: 31531.844473, mean_q: 197.620482, mean_eps: 0.318188
 227872/300000: episode: 1895, duration: 0.726s, episode steps: 101, steps per second: 139, episode reward: -28.708, mean reward: -0.284 [-100.000, 12.526], mean action: 1.713 [0.000, 3.000],  loss: 94.506192, mse: 31729.648515, mean_q: 198.637209, mean_eps: 0.316537
 228013/300000: episode: 1896, duration: 1.012s, episode steps: 141, steps per second: 139, episode reward: -126.137, mean reward: -0.895 [-100.000, 12.012], mean action: 1.908 [0.000, 3.000],  loss: 88.747003, mse: 31846.020307, mean_q: 199.071697, mean_eps: 0.316174
 228107/300000: episode: 1897, duration: 0.663s, episode steps:  94, steps per second: 142, episode reward: -87.350, mean reward: -0.929 [-100.000, 11.240], mean action: 1.638 [0.000, 3.000],  loss: 53.052658, mse: 32794.788501, mean_q: 201.429562, mean_eps: 0.315821
 228219/300000: episode: 1898, duration: 0.764s, episode steps: 112, steps per second: 147, episode reward: -143.445, mean reward: -1.281 [-100.000, 27.781], mean action: 1.982 [0.000, 3.000],  loss: 99.589561, mse: 32678.068220, mean_q: 201.365595, mean_eps: 0.315513
 228386/300000: episode: 1899, duration: 1.182s, episode steps: 167, steps per second: 141, episode reward: -70.480, mean reward: -0.422 [-100.000, 14.729], mean action: 1.886 [0.000, 3.000],  loss: 83.584380, mse: 33359.847235, mean_q: 202.956572, mean_eps: 0.315094
 228601/300000: episode: 1900, duration: 1.526s, episode steps: 215, steps per second: 141, episode reward:  6.624, mean reward:  0.031 [-100.000, 15.471], mean action: 1.814 [0.000, 3.000],  loss: 87.252793, mse: 32253.105496, mean_q: 199.074225, mean_eps: 0.314521
 229601/300000: episode: 1901, duration: 8.127s, episode steps: 1000, steps per second: 123, episode reward: 101.532, mean reward:  0.102 [-22.602, 22.761], mean action: 1.752 [0.000, 3.000],  loss: 76.319126, mse: 31792.114887, mean_q: 196.484349, mean_eps: 0.312698
 229784/300000: episode: 1902, duration: 1.453s, episode steps: 183, steps per second: 126, episode reward: -23.096, mean reward: -0.126 [-100.000, 18.397], mean action: 1.738 [0.000, 3.000],  loss: 77.756070, mse: 30724.562137, mean_q: 192.473981, mean_eps: 0.310924
 229949/300000: episode: 1903, duration: 1.553s, episode steps: 165, steps per second: 106, episode reward: -226.627, mean reward: -1.373 [-100.000, 81.778], mean action: 1.782 [0.000, 3.000],  loss: 61.654473, mse: 30212.205753, mean_q: 190.593738, mean_eps: 0.310402
 230065/300000: episode: 1904, duration: 1.131s, episode steps: 116, steps per second: 103, episode reward: -23.437, mean reward: -0.202 [-100.000, 24.557], mean action: 1.578 [0.000, 3.000],  loss: 64.150321, mse: 30398.003822, mean_q: 191.061470, mean_eps: 0.309981
 230206/300000: episode: 1905, duration: 1.613s, episode steps: 141, steps per second:  87, episode reward: -22.433, mean reward: -0.159 [-100.000, 41.690], mean action: 1.823 [0.000, 3.000],  loss: 57.908596, mse: 30785.030405, mean_q: 192.279227, mean_eps: 0.309595
 230299/300000: episode: 1906, duration: 0.982s, episode steps:  93, steps per second:  95, episode reward: -74.921, mean reward: -0.806 [-100.000, 11.079], mean action: 1.914 [0.000, 3.000],  loss: 87.409956, mse: 30368.162844, mean_q: 190.854690, mean_eps: 0.309244
 230390/300000: episode: 1907, duration: 1.003s, episode steps:  91, steps per second:  91, episode reward: -126.803, mean reward: -1.393 [-100.000,  4.694], mean action: 2.077 [0.000, 3.000],  loss: 76.306424, mse: 31526.252876, mean_q: 195.863721, mean_eps: 0.308968
 230509/300000: episode: 1908, duration: 1.275s, episode steps: 119, steps per second:  93, episode reward: -136.497, mean reward: -1.147 [-100.000, 11.563], mean action: 1.916 [0.000, 3.000],  loss: 103.099774, mse: 31962.540540, mean_q: 196.671682, mean_eps: 0.308653
 230638/300000: episode: 1909, duration: 1.304s, episode steps: 129, steps per second:  99, episode reward: -171.231, mean reward: -1.327 [-100.000,  8.319], mean action: 1.899 [0.000, 3.000],  loss: 78.487515, mse: 32714.345567, mean_q: 198.700257, mean_eps: 0.308281
 230742/300000: episode: 1910, duration: 0.959s, episode steps: 104, steps per second: 108, episode reward: -86.112, mean reward: -0.828 [-100.000, 15.549], mean action: 1.894 [0.000, 3.000],  loss: 82.903141, mse: 32032.294847, mean_q: 195.542449, mean_eps: 0.307931
 230857/300000: episode: 1911, duration: 0.972s, episode steps: 115, steps per second: 118, episode reward: -109.395, mean reward: -0.951 [-100.000, 10.485], mean action: 1.835 [0.000, 3.000],  loss: 78.495960, mse: 31727.463298, mean_q: 195.600667, mean_eps: 0.307603
 231857/300000: episode: 1912, duration: 8.763s, episode steps: 1000, steps per second: 114, episode reward: -18.704, mean reward: -0.019 [-22.640, 24.212], mean action: 1.734 [0.000, 3.000],  loss: 72.294183, mse: 30536.658490, mean_q: 192.082559, mean_eps: 0.305930
 231979/300000: episode: 1913, duration: 0.910s, episode steps: 122, steps per second: 134, episode reward: -120.544, mean reward: -0.988 [-100.000,  8.288], mean action: 1.590 [0.000, 3.000],  loss: 77.858262, mse: 29823.736360, mean_q: 189.972793, mean_eps: 0.304247
 232095/300000: episode: 1914, duration: 0.789s, episode steps: 116, steps per second: 147, episode reward: -159.413, mean reward: -1.374 [-100.000, 15.028], mean action: 1.707 [0.000, 3.000],  loss: 85.217088, mse: 29132.093026, mean_q: 188.541822, mean_eps: 0.303891
 232254/300000: episode: 1915, duration: 1.097s, episode steps: 159, steps per second: 145, episode reward: -195.399, mean reward: -1.229 [-100.000,  6.036], mean action: 1.604 [0.000, 3.000],  loss: 63.252636, mse: 29856.168448, mean_q: 190.263289, mean_eps: 0.303478
 232423/300000: episode: 1916, duration: 1.180s, episode steps: 169, steps per second: 143, episode reward: -132.723, mean reward: -0.785 [-100.000, 11.257], mean action: 1.716 [0.000, 3.000],  loss: 62.041304, mse: 29978.035503, mean_q: 190.379787, mean_eps: 0.302986
 232647/300000: episode: 1917, duration: 1.542s, episode steps: 224, steps per second: 145, episode reward: -312.902, mean reward: -1.397 [-100.000, 12.496], mean action: 1.750 [0.000, 3.000],  loss: 71.708210, mse: 29472.782235, mean_q: 188.338475, mean_eps: 0.302397
 232785/300000: episode: 1918, duration: 0.980s, episode steps: 138, steps per second: 141, episode reward: -162.605, mean reward: -1.178 [-100.000,  9.946], mean action: 1.891 [0.000, 3.000],  loss: 61.511883, mse: 29340.010459, mean_q: 187.558669, mean_eps: 0.301853
 233534/300000: episode: 1919, duration: 6.300s, episode steps: 749, steps per second: 119, episode reward: -110.905, mean reward: -0.148 [-100.000, 22.338], mean action: 1.706 [0.000, 3.000],  loss: 75.477329, mse: 29951.545198, mean_q: 190.423828, mean_eps: 0.300523
 233644/300000: episode: 1920, duration: 0.902s, episode steps: 110, steps per second: 122, episode reward: -115.495, mean reward: -1.050 [-100.000,  8.601], mean action: 1.827 [0.000, 3.000],  loss: 78.170247, mse: 28984.986772, mean_q: 186.139277, mean_eps: 0.299234
 234529/300000: episode: 1921, duration: 7.874s, episode steps: 885, steps per second: 112, episode reward: -357.624, mean reward: -0.404 [-100.000, 16.158], mean action: 1.751 [0.000, 3.000],  loss: 65.838213, mse: 28242.147925, mean_q: 184.775053, mean_eps: 0.297742
 234703/300000: episode: 1922, duration: 1.231s, episode steps: 174, steps per second: 141, episode reward: -51.494, mean reward: -0.296 [-100.000, 18.979], mean action: 1.764 [0.000, 3.000],  loss: 88.559019, mse: 26273.020216, mean_q: 178.771567, mean_eps: 0.296153
 234793/300000: episode: 1923, duration: 0.659s, episode steps:  90, steps per second: 136, episode reward: -64.085, mean reward: -0.712 [-100.000, 22.542], mean action: 1.844 [0.000, 3.000],  loss: 89.532980, mse: 25891.167296, mean_q: 177.296997, mean_eps: 0.295758
 235793/300000: episode: 1924, duration: 7.896s, episode steps: 1000, steps per second: 127, episode reward: 101.202, mean reward:  0.101 [-24.173, 24.038], mean action: 1.528 [0.000, 3.000],  loss: 62.698713, mse: 24622.159994, mean_q: 172.186843, mean_eps: 0.294122
 236793/300000: episode: 1925, duration: 8.086s, episode steps: 1000, steps per second: 124, episode reward: 113.010, mean reward:  0.113 [-23.915, 22.889], mean action: 1.368 [0.000, 3.000],  loss: 58.329939, mse: 23804.218953, mean_q: 169.504407, mean_eps: 0.291123
 236890/300000: episode: 1926, duration: 0.680s, episode steps:  97, steps per second: 143, episode reward: 12.147, mean reward:  0.125 [-100.000, 15.208], mean action: 1.670 [0.000, 3.000],  loss: 52.516347, mse: 23088.085877, mean_q: 166.908027, mean_eps: 0.289477
 236973/300000: episode: 1927, duration: 0.599s, episode steps:  83, steps per second: 139, episode reward: -61.801, mean reward: -0.745 [-100.000, 12.155], mean action: 1.904 [0.000, 3.000],  loss: 67.599082, mse: 23077.713149, mean_q: 167.454302, mean_eps: 0.289207
 237133/300000: episode: 1928, duration: 1.160s, episode steps: 160, steps per second: 138, episode reward: -91.405, mean reward: -0.571 [-100.000,  4.527], mean action: 1.350 [0.000, 3.000],  loss: 65.470202, mse: 22881.425842, mean_q: 167.362442, mean_eps: 0.288843
 238133/300000: episode: 1929, duration: 7.845s, episode steps: 1000, steps per second: 127, episode reward: 152.111, mean reward:  0.152 [-24.916, 22.392], mean action: 1.814 [0.000, 3.000],  loss: 63.242666, mse: 22525.615984, mean_q: 165.934504, mean_eps: 0.287102
 238242/300000: episode: 1930, duration: 1.024s, episode steps: 109, steps per second: 106, episode reward: -39.776, mean reward: -0.365 [-100.000, 25.195], mean action: 1.514 [0.000, 3.000],  loss: 39.251855, mse: 21889.426408, mean_q: 163.299988, mean_eps: 0.285439
 239242/300000: episode: 1931, duration: 8.002s, episode steps: 1000, steps per second: 125, episode reward: 85.879, mean reward:  0.086 [-21.023, 22.883], mean action: 1.851 [0.000, 3.000],  loss: 55.648582, mse: 21015.204554, mean_q: 159.213672, mean_eps: 0.283775
 239371/300000: episode: 1932, duration: 0.895s, episode steps: 129, steps per second: 144, episode reward: -156.173, mean reward: -1.211 [-100.000, 22.896], mean action: 1.837 [0.000, 3.000],  loss: 54.150716, mse: 20713.094992, mean_q: 158.297483, mean_eps: 0.282082
 239450/300000: episode: 1933, duration: 0.586s, episode steps:  79, steps per second: 135, episode reward: -24.744, mean reward: -0.313 [-100.000, 15.370], mean action: 1.570 [0.000, 3.000],  loss: 62.234590, mse: 20313.880835, mean_q: 155.557241, mean_eps: 0.281770
 239634/300000: episode: 1934, duration: 1.605s, episode steps: 184, steps per second: 115, episode reward: -159.890, mean reward: -0.869 [-100.000,  4.726], mean action: 1.674 [0.000, 3.000],  loss: 52.930147, mse: 20669.870436, mean_q: 157.198402, mean_eps: 0.281376
 239782/300000: episode: 1935, duration: 1.294s, episode steps: 148, steps per second: 114, episode reward: -206.833, mean reward: -1.398 [-100.000, 11.524], mean action: 1.764 [0.000, 3.000],  loss: 61.099678, mse: 20169.404732, mean_q: 154.916435, mean_eps: 0.280878
 240270/300000: episode: 1936, duration: 3.875s, episode steps: 488, steps per second: 126, episode reward: -157.720, mean reward: -0.323 [-100.000, 14.239], mean action: 1.701 [0.000, 3.000],  loss: 58.356947, mse: 19633.243776, mean_q: 152.843500, mean_eps: 0.279923
 240432/300000: episode: 1937, duration: 1.257s, episode steps: 162, steps per second: 129, episode reward: -215.252, mean reward: -1.329 [-100.000, 16.526], mean action: 1.901 [0.000, 3.000],  loss: 48.595747, mse: 19433.367013, mean_q: 151.948256, mean_eps: 0.278948
 240513/300000: episode: 1938, duration: 0.601s, episode steps:  81, steps per second: 135, episode reward: -41.946, mean reward: -0.518 [-100.000,  9.292], mean action: 1.951 [0.000, 3.000],  loss: 54.088237, mse: 18999.278236, mean_q: 149.643850, mean_eps: 0.278584
 240599/300000: episode: 1939, duration: 0.683s, episode steps:  86, steps per second: 126, episode reward: 17.802, mean reward:  0.207 [-100.000, 24.742], mean action: 1.535 [0.000, 3.000],  loss: 43.776727, mse: 19560.267351, mean_q: 152.255516, mean_eps: 0.278333
 240705/300000: episode: 1940, duration: 0.768s, episode steps: 106, steps per second: 138, episode reward: -3.885, mean reward: -0.037 [-100.000, 18.308], mean action: 1.613 [0.000, 3.000],  loss: 57.887738, mse: 19917.976562, mean_q: 154.551129, mean_eps: 0.278046
 241705/300000: episode: 1941, duration: 8.119s, episode steps: 1000, steps per second: 123, episode reward: 12.612, mean reward:  0.013 [-22.681, 23.761], mean action: 1.414 [0.000, 3.000],  loss: 50.563308, mse: 19734.272239, mean_q: 152.867374, mean_eps: 0.276386
 242705/300000: episode: 1942, duration: 7.422s, episode steps: 1000, steps per second: 135, episode reward: 71.306, mean reward:  0.071 [-26.090, 29.613], mean action: 1.434 [0.000, 3.000],  loss: 45.472123, mse: 20577.239909, mean_q: 156.770694, mean_eps: 0.273386
 243264/300000: episode: 1943, duration: 4.314s, episode steps: 559, steps per second: 130, episode reward: -271.917, mean reward: -0.486 [-100.000, 24.438], mean action: 1.617 [0.000, 3.000],  loss: 41.830742, mse: 19574.145071, mean_q: 153.123521, mean_eps: 0.271048
 243429/300000: episode: 1944, duration: 1.166s, episode steps: 165, steps per second: 142, episode reward: -190.997, mean reward: -1.158 [-100.000,  4.648], mean action: 1.533 [0.000, 3.000],  loss: 39.914439, mse: 18813.471964, mean_q: 150.693019, mean_eps: 0.269962
 243563/300000: episode: 1945, duration: 0.923s, episode steps: 134, steps per second: 145, episode reward: -51.313, mean reward: -0.383 [-100.000, 17.221], mean action: 1.485 [0.000, 3.000],  loss: 48.985818, mse: 19141.659092, mean_q: 151.538957, mean_eps: 0.269513
 244563/300000: episode: 1946, duration: 7.720s, episode steps: 1000, steps per second: 130, episode reward: 19.174, mean reward:  0.019 [-24.147, 24.388], mean action: 1.646 [0.000, 3.000],  loss: 48.952460, mse: 18787.669812, mean_q: 150.402911, mean_eps: 0.267813
 244683/300000: episode: 1947, duration: 0.809s, episode steps: 120, steps per second: 148, episode reward: -19.341, mean reward: -0.161 [-100.000,  9.398], mean action: 1.608 [0.000, 3.000],  loss: 49.739536, mse: 18136.336108, mean_q: 146.442100, mean_eps: 0.266132
 245683/300000: episode: 1948, duration: 7.654s, episode steps: 1000, steps per second: 131, episode reward: 72.391, mean reward:  0.072 [-21.203, 23.265], mean action: 1.696 [0.000, 3.000],  loss: 44.599536, mse: 17770.320614, mean_q: 145.682473, mean_eps: 0.264452
 245810/300000: episode: 1949, duration: 0.857s, episode steps: 127, steps per second: 148, episode reward: -182.841, mean reward: -1.440 [-100.000,  5.507], mean action: 1.567 [0.000, 3.000],  loss: 38.253326, mse: 17462.971203, mean_q: 145.214696, mean_eps: 0.262762
 246099/300000: episode: 1950, duration: 2.047s, episode steps: 289, steps per second: 141, episode reward: -170.883, mean reward: -0.591 [-100.000, 15.833], mean action: 1.588 [0.000, 3.000],  loss: 44.664296, mse: 17367.364055, mean_q: 144.458859, mean_eps: 0.262138
 246338/300000: episode: 1951, duration: 1.849s, episode steps: 239, steps per second: 129, episode reward:  4.413, mean reward:  0.018 [-100.000, 15.347], mean action: 1.833 [0.000, 3.000],  loss: 40.315783, mse: 16787.961624, mean_q: 141.892730, mean_eps: 0.261346
 246487/300000: episode: 1952, duration: 1.120s, episode steps: 149, steps per second: 133, episode reward:  0.944, mean reward:  0.006 [-100.000, 16.259], mean action: 1.772 [0.000, 3.000],  loss: 28.377740, mse: 16526.277875, mean_q: 140.880166, mean_eps: 0.260764
 246865/300000: episode: 1953, duration: 3.288s, episode steps: 378, steps per second: 115, episode reward: -124.928, mean reward: -0.330 [-100.000, 21.636], mean action: 1.772 [0.000, 3.000],  loss: 38.082187, mse: 16019.597338, mean_q: 138.033499, mean_eps: 0.259973
 246959/300000: episode: 1954, duration: 0.792s, episode steps:  94, steps per second: 119, episode reward: -70.568, mean reward: -0.751 [-100.000,  8.924], mean action: 1.543 [0.000, 3.000],  loss: 45.241798, mse: 15768.934311, mean_q: 136.981250, mean_eps: 0.259265
 247077/300000: episode: 1955, duration: 0.958s, episode steps: 118, steps per second: 123, episode reward: 52.369, mean reward:  0.444 [-100.000, 20.083], mean action: 1.788 [0.000, 3.000],  loss: 50.534073, mse: 15575.245539, mean_q: 136.438757, mean_eps: 0.258947
 247478/300000: episode: 1956, duration: 2.965s, episode steps: 401, steps per second: 135, episode reward: -7.666, mean reward: -0.019 [-100.000, 23.004], mean action: 1.546 [0.000, 3.000],  loss: 40.831732, mse: 15628.213770, mean_q: 136.793399, mean_eps: 0.258169
 247855/300000: episode: 1957, duration: 3.075s, episode steps: 377, steps per second: 123, episode reward: -6.987, mean reward: -0.019 [-100.000, 94.079], mean action: 1.676 [0.000, 3.000],  loss: 39.601401, mse: 15837.176359, mean_q: 138.223932, mean_eps: 0.257002
 248025/300000: episode: 1958, duration: 1.249s, episode steps: 170, steps per second: 136, episode reward: 29.708, mean reward:  0.175 [-100.000, 16.155], mean action: 1.659 [0.000, 3.000],  loss: 52.225282, mse: 15962.542952, mean_q: 138.443322, mean_eps: 0.256182
 248162/300000: episode: 1959, duration: 1.082s, episode steps: 137, steps per second: 127, episode reward: 38.621, mean reward:  0.282 [-100.000, 15.303], mean action: 1.920 [0.000, 3.000],  loss: 48.560944, mse: 16480.084376, mean_q: 141.421525, mean_eps: 0.255721
 249064/300000: episode: 1960, duration: 7.228s, episode steps: 902, steps per second: 125, episode reward: 149.341, mean reward:  0.166 [-22.723, 100.000], mean action: 1.674 [0.000, 3.000],  loss: 37.833738, mse: 16130.561468, mean_q: 139.946809, mean_eps: 0.254163
 249174/300000: episode: 1961, duration: 0.827s, episode steps: 110, steps per second: 133, episode reward: -5.209, mean reward: -0.047 [-100.000, 11.443], mean action: 1.836 [0.000, 3.000],  loss: 42.995398, mse: 15842.764719, mean_q: 138.857977, mean_eps: 0.252644
 250174/300000: episode: 1962, duration: 8.648s, episode steps: 1000, steps per second: 116, episode reward: -75.415, mean reward: -0.075 [-21.643, 18.160], mean action: 1.710 [0.000, 3.000],  loss: 34.441207, mse: 15754.974259, mean_q: 138.451263, mean_eps: 0.250979
 250274/300000: episode: 1963, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: 55.543, mean reward:  0.555 [-100.000, 17.479], mean action: 1.680 [0.000, 3.000],  loss: 42.746081, mse: 15775.975752, mean_q: 138.379004, mean_eps: 0.249329
 251274/300000: episode: 1964, duration: 8.099s, episode steps: 1000, steps per second: 123, episode reward: 111.002, mean reward:  0.111 [-23.641, 22.972], mean action: 1.630 [0.000, 3.000],  loss: 38.934160, mse: 15520.300610, mean_q: 138.273582, mean_eps: 0.247679
 251411/300000: episode: 1965, duration: 1.072s, episode steps: 137, steps per second: 128, episode reward: 36.159, mean reward:  0.264 [-100.000, 14.625], mean action: 1.766 [0.000, 3.000],  loss: 40.559497, mse: 14431.052727, mean_q: 133.046072, mean_eps: 0.245974
 251691/300000: episode: 1966, duration: 2.090s, episode steps: 280, steps per second: 134, episode reward: -59.462, mean reward: -0.212 [-100.000, 17.517], mean action: 1.700 [0.000, 3.000],  loss: 49.056750, mse: 14285.748786, mean_q: 132.158683, mean_eps: 0.245348
 251837/300000: episode: 1967, duration: 1.047s, episode steps: 146, steps per second: 139, episode reward: -89.241, mean reward: -0.611 [-100.000,  8.408], mean action: 1.753 [0.000, 3.000],  loss: 46.415475, mse: 14205.657922, mean_q: 131.444526, mean_eps: 0.244709
 252474/300000: episode: 1968, duration: 5.025s, episode steps: 637, steps per second: 127, episode reward: 219.971, mean reward:  0.345 [-22.392, 100.000], mean action: 1.512 [0.000, 3.000],  loss: 37.596561, mse: 13709.042078, mean_q: 129.389016, mean_eps: 0.243535
 252598/300000: episode: 1969, duration: 0.829s, episode steps: 124, steps per second: 150, episode reward: -204.728, mean reward: -1.651 [-100.000,  1.946], mean action: 1.758 [0.000, 3.000],  loss: 37.041295, mse: 13442.585567, mean_q: 127.637221, mean_eps: 0.242394
 253598/300000: episode: 1970, duration: 9.752s, episode steps: 1000, steps per second: 103, episode reward: 24.743, mean reward:  0.025 [-23.952, 19.127], mean action: 1.740 [0.000, 3.000],  loss: 37.339129, mse: 12971.211396, mean_q: 124.983934, mean_eps: 0.240707
 254598/300000: episode: 1971, duration: 10.090s, episode steps: 1000, steps per second:  99, episode reward: 115.906, mean reward:  0.116 [-24.160, 22.957], mean action: 1.596 [0.000, 3.000],  loss: 34.194543, mse: 13522.151684, mean_q: 128.179753, mean_eps: 0.237707
 254844/300000: episode: 1972, duration: 2.732s, episode steps: 246, steps per second:  90, episode reward: -207.476, mean reward: -0.843 [-100.000,  3.940], mean action: 1.850 [0.000, 3.000],  loss: 36.230698, mse: 14041.802167, mean_q: 130.457054, mean_eps: 0.235838
 255201/300000: episode: 1973, duration: 3.564s, episode steps: 357, steps per second: 100, episode reward: -74.035, mean reward: -0.207 [-100.000, 19.213], mean action: 1.485 [0.000, 3.000],  loss: 41.612465, mse: 14135.334014, mean_q: 130.323010, mean_eps: 0.234934
 256201/300000: episode: 1974, duration: 10.190s, episode steps: 1000, steps per second:  98, episode reward: 86.580, mean reward:  0.087 [-23.380, 23.215], mean action: 1.431 [0.000, 3.000],  loss: 32.239668, mse: 14010.766958, mean_q: 129.540066, mean_eps: 0.232898
 256339/300000: episode: 1975, duration: 0.975s, episode steps: 138, steps per second: 141, episode reward: -16.539, mean reward: -0.120 [-100.000, 15.785], mean action: 1.717 [0.000, 3.000],  loss: 29.548147, mse: 13981.947485, mean_q: 130.037391, mean_eps: 0.231191
 256645/300000: episode: 1976, duration: 2.228s, episode steps: 306, steps per second: 137, episode reward: -106.047, mean reward: -0.347 [-100.000, 17.813], mean action: 1.441 [0.000, 3.000],  loss: 32.142650, mse: 14015.187149, mean_q: 130.114774, mean_eps: 0.230525
 256852/300000: episode: 1977, duration: 1.536s, episode steps: 207, steps per second: 135, episode reward: -209.238, mean reward: -1.011 [-100.000, 14.496], mean action: 1.348 [0.000, 3.000],  loss: 39.679676, mse: 14019.913100, mean_q: 129.849368, mean_eps: 0.229756
 257852/300000: episode: 1978, duration: 8.278s, episode steps: 1000, steps per second: 121, episode reward: 99.823, mean reward:  0.100 [-19.889, 40.177], mean action: 1.459 [0.000, 3.000],  loss: 35.170634, mse: 13690.889227, mean_q: 128.007075, mean_eps: 0.227945
 258852/300000: episode: 1979, duration: 8.002s, episode steps: 1000, steps per second: 125, episode reward: 110.450, mean reward:  0.110 [-22.893, 23.073], mean action: 1.442 [0.000, 3.000],  loss: 34.135885, mse: 13646.213335, mean_q: 127.390733, mean_eps: 0.224945
 259852/300000: episode: 1980, duration: 8.758s, episode steps: 1000, steps per second: 114, episode reward: 65.013, mean reward:  0.065 [-23.909, 28.108], mean action: 1.439 [0.000, 3.000],  loss: 35.246333, mse: 13654.094503, mean_q: 126.629042, mean_eps: 0.221945
 260844/300000: episode: 1981, duration: 8.608s, episode steps: 992, steps per second: 115, episode reward: 196.873, mean reward:  0.198 [-24.497, 100.000], mean action: 1.238 [0.000, 3.000],  loss: 29.139863, mse: 14598.498383, mean_q: 130.507518, mean_eps: 0.218957
 260983/300000: episode: 1982, duration: 1.055s, episode steps: 139, steps per second: 132, episode reward: 14.521, mean reward:  0.104 [-100.000, 13.484], mean action: 1.489 [0.000, 3.000],  loss: 37.697011, mse: 15243.112115, mean_q: 133.524063, mean_eps: 0.217261
 261983/300000: episode: 1983, duration: 9.397s, episode steps: 1000, steps per second: 106, episode reward: 39.379, mean reward:  0.039 [-21.564, 23.777], mean action: 1.465 [0.000, 3.000],  loss: 35.733507, mse: 15638.988094, mean_q: 134.546446, mean_eps: 0.215552
 262983/300000: episode: 1984, duration: 8.940s, episode steps: 1000, steps per second: 112, episode reward: 70.191, mean reward:  0.070 [-22.976, 23.262], mean action: 1.806 [0.000, 3.000],  loss: 28.774267, mse: 16452.330005, mean_q: 136.684678, mean_eps: 0.212553
 263983/300000: episode: 1985, duration: 8.882s, episode steps: 1000, steps per second: 113, episode reward: 37.280, mean reward:  0.037 [-20.547, 29.478], mean action: 1.428 [0.000, 3.000],  loss: 34.493799, mse: 16428.144395, mean_q: 135.001108, mean_eps: 0.209553
 264751/300000: episode: 1986, duration: 5.955s, episode steps: 768, steps per second: 129, episode reward: -177.304, mean reward: -0.231 [-100.000, 14.261], mean action: 1.708 [0.000, 3.000],  loss: 36.019653, mse: 16735.299814, mean_q: 135.284859, mean_eps: 0.206900
 265751/300000: episode: 1987, duration: 7.476s, episode steps: 1000, steps per second: 134, episode reward: 117.355, mean reward:  0.117 [-20.713, 22.469], mean action: 1.611 [0.000, 3.000],  loss: 32.186072, mse: 16446.189723, mean_q: 134.294573, mean_eps: 0.204248
 266360/300000: episode: 1988, duration: 4.411s, episode steps: 609, steps per second: 138, episode reward: 173.076, mean reward:  0.284 [-18.024, 100.000], mean action: 1.527 [0.000, 3.000],  loss: 31.042521, mse: 15268.303207, mean_q: 128.642904, mean_eps: 0.201835
 266823/300000: episode: 1989, duration: 3.302s, episode steps: 463, steps per second: 140, episode reward: 189.888, mean reward:  0.410 [-18.524, 100.000], mean action: 1.298 [0.000, 3.000],  loss: 31.932603, mse: 15252.375679, mean_q: 128.703787, mean_eps: 0.200227
 267787/300000: episode: 1990, duration: 7.298s, episode steps: 964, steps per second: 132, episode reward: -216.234, mean reward: -0.224 [-100.000, 20.414], mean action: 1.717 [0.000, 3.000],  loss: 26.223196, mse: 13938.982455, mean_q: 122.503375, mean_eps: 0.198086
 268787/300000: episode: 1991, duration: 7.825s, episode steps: 1000, steps per second: 128, episode reward: 124.314, mean reward:  0.124 [-20.474, 23.470], mean action: 1.582 [0.000, 3.000],  loss: 27.156395, mse: 13843.967070, mean_q: 123.329228, mean_eps: 0.195140
 269508/300000: episode: 1992, duration: 5.418s, episode steps: 721, steps per second: 133, episode reward: -157.700, mean reward: -0.219 [-100.000, 17.154], mean action: 1.836 [0.000, 3.000],  loss: 25.154751, mse: 13452.113078, mean_q: 122.522885, mean_eps: 0.192559
 270099/300000: episode: 1993, duration: 4.297s, episode steps: 591, steps per second: 138, episode reward: 168.954, mean reward:  0.286 [-18.680, 100.000], mean action: 1.343 [0.000, 3.000],  loss: 23.604148, mse: 13112.189245, mean_q: 121.432345, mean_eps: 0.190591
 271099/300000: episode: 1994, duration: 8.429s, episode steps: 1000, steps per second: 119, episode reward: 99.580, mean reward:  0.100 [-22.612, 22.767], mean action: 1.390 [0.000, 3.000],  loss: 24.629714, mse: 12911.748710, mean_q: 120.738114, mean_eps: 0.188204
 271492/300000: episode: 1995, duration: 2.811s, episode steps: 393, steps per second: 140, episode reward: 236.500, mean reward:  0.602 [-17.543, 100.000], mean action: 0.969 [0.000, 3.000],  loss: 27.200799, mse: 12452.367988, mean_q: 117.930338, mean_eps: 0.186115
 271617/300000: episode: 1996, duration: 0.899s, episode steps: 125, steps per second: 139, episode reward: -73.091, mean reward: -0.585 [-100.000,  9.172], mean action: 1.744 [0.000, 3.000],  loss: 28.839420, mse: 12498.791172, mean_q: 118.442809, mean_eps: 0.185338
 271744/300000: episode: 1997, duration: 0.922s, episode steps: 127, steps per second: 138, episode reward: 17.988, mean reward:  0.142 [-100.000, 13.354], mean action: 1.858 [0.000, 3.000],  loss: 20.130899, mse: 12661.749354, mean_q: 119.637126, mean_eps: 0.184960
 271913/300000: episode: 1998, duration: 1.235s, episode steps: 169, steps per second: 137, episode reward: 41.259, mean reward:  0.244 [-100.000, 10.090], mean action: 1.568 [0.000, 3.000],  loss: 25.839372, mse: 12842.596616, mean_q: 120.251840, mean_eps: 0.184516
 272090/300000: episode: 1999, duration: 1.339s, episode steps: 177, steps per second: 132, episode reward: -162.565, mean reward: -0.918 [-100.000,  4.516], mean action: 1.791 [0.000, 3.000],  loss: 27.983620, mse: 13266.032006, mean_q: 122.852008, mean_eps: 0.183997
 272804/300000: episode: 2000, duration: 5.205s, episode steps: 714, steps per second: 137, episode reward: -92.766, mean reward: -0.130 [-100.000, 20.853], mean action: 1.283 [0.000, 3.000],  loss: 24.916477, mse: 13642.891870, mean_q: 124.775033, mean_eps: 0.182660
 273220/300000: episode: 2001, duration: 2.955s, episode steps: 416, steps per second: 141, episode reward: -242.173, mean reward: -0.582 [-100.000, 32.513], mean action: 1.716 [0.000, 3.000],  loss: 25.493532, mse: 13186.235110, mean_q: 122.427765, mean_eps: 0.180965
 274220/300000: episode: 2002, duration: 8.114s, episode steps: 1000, steps per second: 123, episode reward: -44.509, mean reward: -0.045 [-20.325, 22.655], mean action: 1.732 [0.000, 3.000],  loss: 24.714349, mse: 12810.915604, mean_q: 119.654608, mean_eps: 0.178841
 275220/300000: episode: 2003, duration: 7.934s, episode steps: 1000, steps per second: 126, episode reward: 82.687, mean reward:  0.083 [-20.111, 22.545], mean action: 1.559 [0.000, 3.000],  loss: 25.423413, mse: 12330.148711, mean_q: 117.139274, mean_eps: 0.175841
 276220/300000: episode: 2004, duration: 8.450s, episode steps: 1000, steps per second: 118, episode reward: -64.407, mean reward: -0.064 [-20.228, 22.255], mean action: 1.779 [0.000, 3.000],  loss: 22.887190, mse: 11493.437299, mean_q: 113.117219, mean_eps: 0.172842
 277220/300000: episode: 2005, duration: 7.321s, episode steps: 1000, steps per second: 137, episode reward: 126.663, mean reward:  0.127 [-20.863, 22.397], mean action: 1.138 [0.000, 3.000],  loss: 22.923232, mse: 11211.024209, mean_q: 112.773034, mean_eps: 0.169842
 278220/300000: episode: 2006, duration: 7.437s, episode steps: 1000, steps per second: 134, episode reward: 127.022, mean reward:  0.127 [-20.860, 23.124], mean action: 1.338 [0.000, 3.000],  loss: 18.895548, mse: 11236.876026, mean_q: 112.893754, mean_eps: 0.166842
 279220/300000: episode: 2007, duration: 8.395s, episode steps: 1000, steps per second: 119, episode reward: -6.851, mean reward: -0.007 [-20.158, 20.968], mean action: 1.714 [0.000, 3.000],  loss: 18.130463, mse: 11208.546993, mean_q: 112.374574, mean_eps: 0.163841
 279641/300000: episode: 2008, duration: 2.970s, episode steps: 421, steps per second: 142, episode reward: 132.011, mean reward:  0.314 [-13.935, 100.000], mean action: 1.544 [0.000, 3.000],  loss: 21.341827, mse: 11570.079213, mean_q: 113.893116, mean_eps: 0.161710
 280052/300000: episode: 2009, duration: 2.918s, episode steps: 411, steps per second: 141, episode reward: -253.761, mean reward: -0.617 [-100.000, 34.363], mean action: 1.582 [0.000, 3.000],  loss: 19.304851, mse: 11314.682266, mean_q: 112.844149, mean_eps: 0.160462
 280337/300000: episode: 2010, duration: 2.155s, episode steps: 285, steps per second: 132, episode reward: -289.622, mean reward: -1.016 [-100.000,  8.892], mean action: 1.733 [0.000, 3.000],  loss: 22.352591, mse: 11268.861578, mean_q: 113.193844, mean_eps: 0.159418
 280471/300000: episode: 2011, duration: 1.017s, episode steps: 134, steps per second: 132, episode reward: -124.053, mean reward: -0.926 [-100.000,  2.359], mean action: 1.679 [0.000, 3.000],  loss: 18.852623, mse: 11693.118383, mean_q: 114.832959, mean_eps: 0.158789
 281174/300000: episode: 2012, duration: 5.264s, episode steps: 703, steps per second: 134, episode reward: 163.050, mean reward:  0.232 [-22.445, 100.000], mean action: 1.627 [0.000, 3.000],  loss: 18.749634, mse: 11322.399032, mean_q: 113.229247, mean_eps: 0.157534
 281437/300000: episode: 2013, duration: 1.792s, episode steps: 263, steps per second: 147, episode reward: -35.247, mean reward: -0.134 [-100.000, 12.205], mean action: 1.684 [0.000, 3.000],  loss: 21.343781, mse: 11205.092699, mean_q: 112.881511, mean_eps: 0.156085
 281663/300000: episode: 2014, duration: 1.534s, episode steps: 226, steps per second: 147, episode reward: -274.280, mean reward: -1.214 [-100.000,  7.385], mean action: 1.686 [0.000, 3.000],  loss: 18.017770, mse: 11052.916428, mean_q: 112.467081, mean_eps: 0.155351
 281945/300000: episode: 2015, duration: 2.091s, episode steps: 282, steps per second: 135, episode reward: 215.613, mean reward:  0.765 [-11.087, 100.000], mean action: 1.582 [0.000, 3.000],  loss: 16.423744, mse: 11158.244601, mean_q: 113.131890, mean_eps: 0.154590
 282134/300000: episode: 2016, duration: 1.312s, episode steps: 189, steps per second: 144, episode reward: -168.508, mean reward: -0.892 [-100.000,  4.349], mean action: 1.841 [0.000, 3.000],  loss: 19.456330, mse: 10858.665385, mean_q: 111.559972, mean_eps: 0.153883
 283037/300000: episode: 2017, duration: 7.486s, episode steps: 903, steps per second: 121, episode reward: -377.228, mean reward: -0.418 [-100.000, 36.281], mean action: 1.330 [0.000, 3.000],  loss: 18.355458, mse: 10033.728760, mean_q: 107.738803, mean_eps: 0.152245
 284037/300000: episode: 2018, duration: 9.479s, episode steps: 1000, steps per second: 105, episode reward: -45.846, mean reward: -0.046 [-19.872, 22.750], mean action: 1.422 [0.000, 3.000],  loss: 15.409360, mse: 9474.477171, mean_q: 104.484848, mean_eps: 0.149390
 285037/300000: episode: 2019, duration: 8.606s, episode steps: 1000, steps per second: 116, episode reward: 100.018, mean reward:  0.100 [-20.323, 24.596], mean action: 1.299 [0.000, 3.000],  loss: 15.841593, mse: 8840.061043, mean_q: 100.809188, mean_eps: 0.146390
 286037/300000: episode: 2020, duration: 8.367s, episode steps: 1000, steps per second: 120, episode reward: 68.565, mean reward:  0.069 [-23.095, 22.886], mean action: 1.639 [0.000, 3.000],  loss: 14.069357, mse: 8691.443813, mean_q: 100.469006, mean_eps: 0.143390
 287037/300000: episode: 2021, duration: 9.485s, episode steps: 1000, steps per second: 105, episode reward: 102.923, mean reward:  0.103 [-20.910, 23.479], mean action: 1.433 [0.000, 3.000],  loss: 15.400977, mse: 8763.725107, mean_q: 100.769930, mean_eps: 0.140390
 288037/300000: episode: 2022, duration: 9.459s, episode steps: 1000, steps per second: 106, episode reward: 122.335, mean reward:  0.122 [-22.019, 23.140], mean action: 1.511 [0.000, 3.000],  loss: 16.776572, mse: 8738.126816, mean_q: 100.621155, mean_eps: 0.137390
 288334/300000: episode: 2023, duration: 2.550s, episode steps: 297, steps per second: 116, episode reward: 203.649, mean reward:  0.686 [-23.217, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 13.633261, mse: 8743.027355, mean_q: 100.534824, mean_eps: 0.135445
 288920/300000: episode: 2024, duration: 4.535s, episode steps: 586, steps per second: 129, episode reward: -352.092, mean reward: -0.601 [-100.000, 14.837], mean action: 1.811 [0.000, 3.000],  loss: 16.387552, mse: 8637.439902, mean_q: 99.508866, mean_eps: 0.134120
 289920/300000: episode: 2025, duration: 8.229s, episode steps: 1000, steps per second: 122, episode reward: 68.736, mean reward:  0.069 [-17.905, 25.561], mean action: 1.864 [0.000, 3.000],  loss: 14.794456, mse: 8489.163914, mean_q: 98.228014, mean_eps: 0.131741
 290004/300000: episode: 2026, duration: 0.557s, episode steps:  84, steps per second: 151, episode reward: -92.749, mean reward: -1.104 [-100.000,  7.490], mean action: 0.833 [0.000, 3.000],  loss: 11.095927, mse: 8144.548183, mean_q: 95.993308, mean_eps: 0.130115
 290753/300000: episode: 2027, duration: 6.310s, episode steps: 749, steps per second: 119, episode reward: 248.191, mean reward:  0.331 [-21.063, 100.000], mean action: 1.081 [0.000, 3.000],  loss: 14.681437, mse: 7915.343004, mean_q: 94.758936, mean_eps: 0.128866
 291753/300000: episode: 2028, duration: 10.331s, episode steps: 1000, steps per second:  97, episode reward: 50.339, mean reward:  0.050 [-24.793, 28.976], mean action: 2.170 [0.000, 3.000],  loss: 13.873605, mse: 7877.939305, mean_q: 94.604768, mean_eps: 0.126242
 292361/300000: episode: 2029, duration: 7.891s, episode steps: 608, steps per second:  77, episode reward: 196.770, mean reward:  0.324 [-24.997, 100.000], mean action: 1.196 [0.000, 3.000],  loss: 16.458865, mse: 8087.339673, mean_q: 95.846609, mean_eps: 0.123830
 292630/300000: episode: 2030, duration: 1.979s, episode steps: 269, steps per second: 136, episode reward: 226.614, mean reward:  0.842 [-18.646, 100.000], mean action: 1.346 [0.000, 3.000],  loss: 17.498251, mse: 7962.806748, mean_q: 94.939865, mean_eps: 0.122515
 293630/300000: episode: 2031, duration: 8.264s, episode steps: 1000, steps per second: 121, episode reward: 46.535, mean reward:  0.047 [-18.427, 23.579], mean action: 1.105 [0.000, 3.000],  loss: 13.488682, mse: 7864.037173, mean_q: 94.655790, mean_eps: 0.120611
 293724/300000: episode: 2032, duration: 0.680s, episode steps:  94, steps per second: 138, episode reward: -42.429, mean reward: -0.451 [-100.000, 13.143], mean action: 1.585 [0.000, 3.000],  loss: 11.252768, mse: 7577.943931, mean_q: 92.516785, mean_eps: 0.118970
 293834/300000: episode: 2033, duration: 0.842s, episode steps: 110, steps per second: 131, episode reward: -25.113, mean reward: -0.228 [-100.000, 12.092], mean action: 1.400 [0.000, 3.000],  loss: 13.914729, mse: 7615.660014, mean_q: 92.791301, mean_eps: 0.118664
 294022/300000: episode: 2034, duration: 1.458s, episode steps: 188, steps per second: 129, episode reward: -303.979, mean reward: -1.617 [-100.000, 10.534], mean action: 1.351 [0.000, 3.000],  loss: 12.910216, mse: 7643.675846, mean_q: 92.726654, mean_eps: 0.118217
 294108/300000: episode: 2035, duration: 0.660s, episode steps:  86, steps per second: 130, episode reward: -9.044, mean reward: -0.105 [-100.000, 10.590], mean action: 1.360 [0.000, 3.000],  loss: 12.686598, mse: 7670.278269, mean_q: 92.116290, mean_eps: 0.117806
 295108/300000: episode: 2036, duration: 8.762s, episode steps: 1000, steps per second: 114, episode reward: 143.503, mean reward:  0.144 [-23.954, 22.685], mean action: 1.367 [0.000, 3.000],  loss: 15.171802, mse: 7381.694699, mean_q: 90.607353, mean_eps: 0.116177
 295551/300000: episode: 2037, duration: 3.555s, episode steps: 443, steps per second: 125, episode reward: 248.354, mean reward:  0.561 [-18.143, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 15.411920, mse: 6983.073802, mean_q: 87.332404, mean_eps: 0.114013
 296019/300000: episode: 2038, duration: 3.592s, episode steps: 468, steps per second: 130, episode reward: 251.219, mean reward:  0.537 [-21.021, 100.000], mean action: 1.630 [0.000, 3.000],  loss: 12.088353, mse: 6720.383394, mean_q: 85.380739, mean_eps: 0.112646
 296222/300000: episode: 2039, duration: 1.456s, episode steps: 203, steps per second: 139, episode reward: -354.355, mean reward: -1.746 [-100.000, 10.385], mean action: 1.522 [0.000, 3.000],  loss: 15.356887, mse: 6820.161939, mean_q: 86.494949, mean_eps: 0.111640
 296329/300000: episode: 2040, duration: 0.777s, episode steps: 107, steps per second: 138, episode reward: -5.224, mean reward: -0.049 [-100.000, 13.945], mean action: 1.766 [0.000, 3.000],  loss: 13.736526, mse: 6985.035302, mean_q: 87.333301, mean_eps: 0.111175
 296563/300000: episode: 2041, duration: 1.729s, episode steps: 234, steps per second: 135, episode reward: 224.717, mean reward:  0.960 [-10.848, 100.000], mean action: 1.410 [0.000, 3.000],  loss: 15.912224, mse: 6741.372083, mean_q: 85.245026, mean_eps: 0.110663
 297319/300000: episode: 2042, duration: 6.216s, episode steps: 756, steps per second: 122, episode reward: 203.043, mean reward:  0.269 [-23.044, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 12.230713, mse: 6534.521329, mean_q: 84.256510, mean_eps: 0.109178
 297412/300000: episode: 2043, duration: 0.680s, episode steps:  93, steps per second: 137, episode reward: -71.831, mean reward: -0.772 [-100.000,  7.762], mean action: 1.656 [0.000, 3.000],  loss: 15.878583, mse: 6554.439275, mean_q: 83.963865, mean_eps: 0.107905
 297464/300000: episode: 2044, duration: 0.375s, episode steps:  52, steps per second: 139, episode reward: -162.482, mean reward: -3.125 [-100.000,  8.540], mean action: 1.615 [0.000, 3.000],  loss: 15.150726, mse: 6309.598116, mean_q: 82.118632, mean_eps: 0.107687
 297522/300000: episode: 2045, duration: 0.449s, episode steps:  58, steps per second: 129, episode reward: -167.608, mean reward: -2.890 [-100.000,  3.629], mean action: 2.362 [0.000, 3.000],  loss: 12.916763, mse: 6448.551354, mean_q: 83.577712, mean_eps: 0.107522
 298483/300000: episode: 2046, duration: 7.422s, episode steps: 961, steps per second: 129, episode reward: 243.698, mean reward:  0.254 [-22.453, 100.000], mean action: 1.160 [0.000, 3.000],  loss: 14.011993, mse: 6536.963917, mean_q: 83.999083, mean_eps: 0.105994
 299157/300000: episode: 2047, duration: 5.380s, episode steps: 674, steps per second: 125, episode reward: 215.642, mean reward:  0.320 [-24.512, 100.000], mean action: 1.521 [0.000, 3.000],  loss: 10.844437, mse: 6498.063888, mean_q: 83.704126, mean_eps: 0.103541
done, took 2273.974 seconds
Testing for 200 episodes ...
Episode 1: reward: 136.139, steps: 678
Episode 2: reward: 26.869, steps: 102
Episode 3: reward: 215.723, steps: 551
Episode 4: reward: 263.591, steps: 250
Episode 5: reward: 242.056, steps: 297
Episode 6: reward: 215.904, steps: 416
Episode 7: reward: -98.139, steps: 428
Episode 8: reward: 247.194, steps: 462
Episode 9: reward: -124.249, steps: 424
Episode 10: reward: 204.652, steps: 398
Episode 11: reward: 243.287, steps: 310
Episode 12: reward: 127.924, steps: 661
Episode 13: reward: 64.780, steps: 1000
Episode 14: reward: 242.513, steps: 332
Episode 15: reward: 179.950, steps: 724
Episode 16: reward: 252.628, steps: 463
Episode 17: reward: 155.117, steps: 566
Episode 18: reward: 253.043, steps: 518
Episode 19: reward: 291.904, steps: 229
Episode 20: reward: -29.086, steps: 411
Episode 21: reward: 100.886, steps: 1000
Episode 22: reward: 226.012, steps: 396
Episode 23: reward: 52.422, steps: 1000
Episode 24: reward: 43.432, steps: 1000
Episode 25: reward: 161.069, steps: 615
Episode 26: reward: 214.640, steps: 525
Episode 27: reward: 231.517, steps: 415
Episode 28: reward: 247.753, steps: 443
Episode 29: reward: 280.179, steps: 392
Episode 30: reward: -319.778, steps: 226
Episode 31: reward: 273.530, steps: 325
Episode 32: reward: 229.224, steps: 519
Episode 33: reward: 214.608, steps: 668
Episode 34: reward: 225.151, steps: 578
Episode 35: reward: 93.902, steps: 1000
Episode 36: reward: -130.720, steps: 587
Episode 37: reward: 240.783, steps: 391
Episode 38: reward: 69.005, steps: 1000
Episode 39: reward: 30.610, steps: 110
Episode 40: reward: 23.072, steps: 1000
Episode 41: reward: 119.395, steps: 864
Episode 42: reward: 251.287, steps: 236
Episode 43: reward: 204.329, steps: 634
Episode 44: reward: 45.109, steps: 98
Episode 45: reward: -184.499, steps: 240
Episode 46: reward: 218.645, steps: 372
Episode 47: reward: 126.303, steps: 620
Episode 48: reward: 267.729, steps: 406
Episode 49: reward: 221.483, steps: 548
Episode 50: reward: -144.524, steps: 315
Episode 51: reward: 208.113, steps: 570
Episode 52: reward: -359.012, steps: 226
Episode 53: reward: 247.788, steps: 298
Episode 54: reward: 248.625, steps: 311
Episode 55: reward: -176.441, steps: 433
Episode 56: reward: 196.172, steps: 378
Episode 57: reward: 204.438, steps: 406
Episode 58: reward: -28.459, steps: 1000
Episode 59: reward: 48.444, steps: 120
Episode 60: reward: 197.908, steps: 434
Episode 61: reward: -132.532, steps: 293
Episode 62: reward: 192.697, steps: 416
Episode 63: reward: 143.545, steps: 441
Episode 64: reward: 258.870, steps: 391
Episode 65: reward: 24.926, steps: 91
Episode 66: reward: 155.377, steps: 473
Episode 67: reward: 309.341, steps: 213
Episode 68: reward: 214.939, steps: 683
Episode 69: reward: 233.417, steps: 569
Episode 70: reward: -188.825, steps: 252
Episode 71: reward: 280.595, steps: 330
Episode 72: reward: 144.400, steps: 1000
Episode 73: reward: -100.339, steps: 154
Episode 74: reward: 109.212, steps: 777
Episode 75: reward: 237.908, steps: 499
Episode 76: reward: 235.490, steps: 210
Episode 77: reward: 148.497, steps: 890
Episode 78: reward: 251.975, steps: 483
Episode 79: reward: -136.353, steps: 289
Episode 80: reward: 200.914, steps: 595
Episode 81: reward: -9.959, steps: 84
Episode 82: reward: 200.322, steps: 572
Episode 83: reward: 216.676, steps: 304
Episode 84: reward: 131.728, steps: 782
Episode 85: reward: 222.552, steps: 577
Episode 86: reward: 86.046, steps: 1000
Episode 87: reward: 69.275, steps: 1000
Episode 88: reward: -127.736, steps: 427
Episode 89: reward: 94.919, steps: 667
Episode 90: reward: 260.996, steps: 285
Episode 91: reward: -281.773, steps: 299
Episode 92: reward: -168.237, steps: 238
Episode 93: reward: 231.517, steps: 180
Episode 94: reward: 106.774, steps: 1000
Episode 95: reward: 158.410, steps: 486
Episode 96: reward: 207.997, steps: 756
Episode 97: reward: 230.474, steps: 214
Episode 98: reward: 14.603, steps: 111
Episode 99: reward: 82.496, steps: 1000
Episode 100: reward: 225.256, steps: 625
Episode 101: reward: 237.977, steps: 427
Episode 102: reward: 169.675, steps: 586
Episode 103: reward: 24.246, steps: 103
Episode 104: reward: 240.974, steps: 465
Episode 105: reward: -17.966, steps: 419
Episode 106: reward: 291.081, steps: 266
Episode 107: reward: -70.032, steps: 433
Episode 108: reward: 234.084, steps: 444
Episode 109: reward: -38.699, steps: 101
Episode 110: reward: 6.665, steps: 103
Episode 111: reward: 107.526, steps: 1000
Episode 112: reward: 172.774, steps: 491
Episode 113: reward: 179.902, steps: 475
Episode 114: reward: -430.948, steps: 248
Episode 115: reward: 68.176, steps: 1000
Episode 116: reward: 216.909, steps: 356
Episode 117: reward: -360.989, steps: 405
Episode 118: reward: 158.301, steps: 1000
Episode 119: reward: -39.127, steps: 243
Episode 120: reward: 166.552, steps: 625
Episode 121: reward: 250.589, steps: 494
Episode 122: reward: 241.982, steps: 551
Episode 123: reward: -151.296, steps: 137
Episode 124: reward: 288.175, steps: 375
Episode 125: reward: -132.578, steps: 140
Episode 126: reward: -39.501, steps: 111
Episode 127: reward: 212.793, steps: 542
Episode 128: reward: 240.428, steps: 334
Episode 129: reward: 219.983, steps: 460
Episode 130: reward: 203.388, steps: 451
Episode 131: reward: 217.422, steps: 204
Episode 132: reward: 35.632, steps: 93
Episode 133: reward: 249.272, steps: 506
Episode 134: reward: 242.090, steps: 525
Episode 135: reward: 191.513, steps: 422
Episode 136: reward: 261.308, steps: 183
Episode 137: reward: 227.700, steps: 548
Episode 138: reward: 184.487, steps: 846
Episode 139: reward: 193.502, steps: 354
Episode 140: reward: 96.750, steps: 1000
Episode 141: reward: -123.604, steps: 197
Episode 142: reward: 215.320, steps: 395
Episode 143: reward: -94.481, steps: 154
Episode 144: reward: 225.561, steps: 569
Episode 145: reward: 302.154, steps: 225
Episode 146: reward: 76.334, steps: 1000
Episode 147: reward: 259.799, steps: 341
Episode 148: reward: 224.724, steps: 320
Episode 149: reward: -22.464, steps: 345
Episode 150: reward: 249.795, steps: 527
Episode 151: reward: 238.342, steps: 521
Episode 152: reward: -61.174, steps: 682
Episode 153: reward: 246.460, steps: 273
Episode 154: reward: 218.355, steps: 500
Episode 155: reward: 96.823, steps: 1000
Episode 156: reward: 229.038, steps: 407
Episode 157: reward: 221.886, steps: 298
Episode 158: reward: -127.760, steps: 572
Episode 159: reward: 221.737, steps: 411
Episode 160: reward: 185.722, steps: 503
Episode 161: reward: -144.381, steps: 136
Episode 162: reward: -123.731, steps: 307
Episode 163: reward: 229.743, steps: 539
Episode 164: reward: 180.922, steps: 443
Episode 165: reward: -189.320, steps: 277
Episode 166: reward: 188.487, steps: 426
Episode 167: reward: 196.615, steps: 361
Episode 168: reward: 125.471, steps: 1000
Episode 169: reward: 255.533, steps: 342
Episode 170: reward: 66.872, steps: 1000
Episode 171: reward: 232.139, steps: 306
Episode 172: reward: 233.838, steps: 289
Episode 173: reward: 178.180, steps: 644
Episode 174: reward: -196.705, steps: 251
Episode 175: reward: 143.861, steps: 719
Episode 176: reward: 144.981, steps: 647
Episode 177: reward: 197.068, steps: 458
Episode 178: reward: 31.908, steps: 112
Episode 179: reward: 175.378, steps: 634
Episode 180: reward: 273.932, steps: 304
Episode 181: reward: 103.077, steps: 1000
Episode 182: reward: 226.600, steps: 306
Episode 183: reward: 249.153, steps: 331
Episode 184: reward: 55.639, steps: 1000
Episode 185: reward: 222.117, steps: 331
Episode 186: reward: 203.531, steps: 685
Episode 187: reward: -351.637, steps: 246
Episode 188: reward: 32.517, steps: 1000
Episode 189: reward: 104.458, steps: 1000
Episode 190: reward: 29.390, steps: 343
Episode 191: reward: 258.996, steps: 148
Episode 192: reward: 200.644, steps: 524
Episode 193: reward: -170.082, steps: 289
Episode 194: reward: 213.374, steps: 401
Episode 195: reward: 237.447, steps: 418
Episode 196: reward: 191.150, steps: 417
Episode 197: reward: 203.658, steps: 642
Episode 198: reward: 268.091, steps: 364
Episode 199: reward: -38.895, steps: 487
Episode 200: reward: 232.940, steps: 369
Testing for 5 episodes ...
Episode 1: reward: 10.402, steps: 117
Episode 2: reward: 220.710, steps: 669
Episode 3: reward: 100.392, steps: 1000
Episode 4: reward: 27.595, steps: 107
Episode 5: reward: -103.167, steps: 250
Testing for 5 episodes ...
Episode 1: reward: 226.290, steps: 532
Episode 2: reward: 66.989, steps: 1000
Episode 3: reward: 195.471, steps: 274
Episode 4: reward: 224.744, steps: 268
Episode 5: reward: -4.834, steps: 484
Testing for 5 episodes ...
Episode 1: reward: 71.242, steps: 1000
Episode 2: reward: 79.507, steps: 1000
Episode 3: reward: 222.636, steps: 414
Episode 4: reward: 28.584, steps: 89
Episode 5: reward: 195.714, steps: 573
Testing for 5 episodes ...
Episode 1: reward: 81.120, steps: 1000
Episode 2: reward: 164.641, steps: 512
Episode 3: reward: -74.981, steps: 278
Episode 4: reward: 227.067, steps: 602
Episode 5: reward: 28.092, steps: 104
Testing for 5 episodes ...
Episode 1: reward: 180.469, steps: 570
Episode 2: reward: 239.984, steps: 433
Episode 3: reward: 155.914, steps: 551
Episode 4: reward: -38.183, steps: 1000
Episode 5: reward: 175.281, steps: 407
Testing for 5 episodes ...
Episode 1: reward: 280.252, steps: 239
Episode 2: reward: -205.574, steps: 256
Episode 3: reward: 278.024, steps: 343
Episode 4: reward: 198.854, steps: 586
Episode 5: reward: 224.431, steps: 380
Testing for 5 episodes ...
Episode 1: reward: 238.152, steps: 356
Episode 2: reward: 127.662, steps: 868
Episode 3: reward: 169.867, steps: 533
Episode 4: reward: 37.822, steps: 1000
Episode 5: reward: 254.983, steps: 366
Training for 100000 steps ...
    77/100000: episode: 1, duration: 0.476s, episode steps:  77, steps per second: 162, episode reward: -246.432, mean reward: -3.200 [-100.000,  6.052], mean action: 1.286 [0.000, 3.000],  loss: 18.101237, mse: 6343.704560, mean_q: 82.175433, mean_eps: 0.999869
   186/100000: episode: 2, duration: 0.779s, episode steps: 109, steps per second: 140, episode reward: -155.321, mean reward: -1.425 [-100.000,  8.341], mean action: 1.394 [0.000, 3.000],  loss: 10.234738, mse: 6270.332166, mean_q: 81.868657, mean_eps: 0.999607
   264/100000: episode: 3, duration: 0.711s, episode steps:  78, steps per second: 110, episode reward: -114.054, mean reward: -1.462 [-100.000, 10.800], mean action: 1.436 [0.000, 3.000],  loss: 11.203629, mse: 6211.659349, mean_q: 81.153300, mean_eps: 0.999326
   321/100000: episode: 4, duration: 0.535s, episode steps:  57, steps per second: 107, episode reward: -125.447, mean reward: -2.201 [-100.000,  5.272], mean action: 1.561 [0.000, 3.000],  loss: 15.288386, mse: 6282.395054, mean_q: 81.113937, mean_eps: 0.999124
   412/100000: episode: 5, duration: 0.635s, episode steps:  91, steps per second: 143, episode reward: -445.877, mean reward: -4.900 [-100.000,  0.508], mean action: 1.231 [0.000, 3.000],  loss: 11.566702, mse: 6319.775230, mean_q: 81.908220, mean_eps: 0.998902
   487/100000: episode: 6, duration: 0.521s, episode steps:  75, steps per second: 144, episode reward: -158.724, mean reward: -2.116 [-100.000, 14.690], mean action: 1.413 [0.000, 3.000],  loss: 11.520643, mse: 6352.225117, mean_q: 82.223345, mean_eps: 0.998653
   558/100000: episode: 7, duration: 0.507s, episode steps:  71, steps per second: 140, episode reward: -178.466, mean reward: -2.514 [-100.000, 13.826], mean action: 1.423 [0.000, 3.000],  loss: 8.267347, mse: 5983.481548, mean_q: 80.750932, mean_eps: 0.998434
   639/100000: episode: 8, duration: 0.606s, episode steps:  81, steps per second: 134, episode reward: -214.414, mean reward: -2.647 [-100.000,  5.622], mean action: 1.642 [0.000, 3.000],  loss: 8.192794, mse: 6149.713771, mean_q: 80.958515, mean_eps: 0.998206
   702/100000: episode: 9, duration: 0.526s, episode steps:  63, steps per second: 120, episode reward: -116.556, mean reward: -1.850 [-100.000, 24.673], mean action: 1.476 [0.000, 3.000],  loss: 6.564697, mse: 6164.649430, mean_q: 81.273476, mean_eps: 0.997990
   778/100000: episode: 10, duration: 0.576s, episode steps:  76, steps per second: 132, episode reward: -90.368, mean reward: -1.189 [-100.000, 15.491], mean action: 1.526 [0.000, 3.000],  loss: 8.435560, mse: 6091.176488, mean_q: 80.935905, mean_eps: 0.997781
   865/100000: episode: 11, duration: 0.696s, episode steps:  87, steps per second: 125, episode reward: -105.714, mean reward: -1.215 [-100.000,  7.505], mean action: 1.759 [0.000, 3.000],  loss: 6.881613, mse: 6214.797391, mean_q: 81.694116, mean_eps: 0.997537
   984/100000: episode: 12, duration: 0.950s, episode steps: 119, steps per second: 125, episode reward: -220.266, mean reward: -1.851 [-100.000, 13.687], mean action: 1.546 [0.000, 3.000],  loss: 14.089992, mse: 6183.604660, mean_q: 81.549569, mean_eps: 0.997228
  1085/100000: episode: 13, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -431.581, mean reward: -4.273 [-100.000,  0.797], mean action: 1.475 [0.000, 3.000],  loss: 15.649936, mse: 6223.083404, mean_q: 82.069840, mean_eps: 0.996898
  1158/100000: episode: 14, duration: 0.491s, episode steps:  73, steps per second: 149, episode reward: -237.161, mean reward: -3.249 [-100.000,  6.582], mean action: 1.603 [0.000, 3.000],  loss: 11.485690, mse: 6437.900799, mean_q: 83.092129, mean_eps: 0.996637
  1234/100000: episode: 15, duration: 0.643s, episode steps:  76, steps per second: 118, episode reward: -83.540, mean reward: -1.099 [-100.000, 17.011], mean action: 1.697 [0.000, 3.000],  loss: 10.210600, mse: 6229.724847, mean_q: 81.966035, mean_eps: 0.996413
  1347/100000: episode: 16, duration: 0.916s, episode steps: 113, steps per second: 123, episode reward: -125.046, mean reward: -1.107 [-100.000,  6.555], mean action: 1.549 [0.000, 3.000],  loss: 11.961306, mse: 6482.203039, mean_q: 83.339789, mean_eps: 0.996130
  1418/100000: episode: 17, duration: 0.483s, episode steps:  71, steps per second: 147, episode reward: -356.320, mean reward: -5.019 [-100.000,  2.423], mean action: 1.817 [0.000, 3.000],  loss: 14.150438, mse: 6206.579961, mean_q: 81.617588, mean_eps: 0.995854
  1513/100000: episode: 18, duration: 0.673s, episode steps:  95, steps per second: 141, episode reward: -89.216, mean reward: -0.939 [-100.000, 17.642], mean action: 1.642 [0.000, 3.000],  loss: 15.136730, mse: 6358.083542, mean_q: 82.677635, mean_eps: 0.995605
  1582/100000: episode: 19, duration: 0.492s, episode steps:  69, steps per second: 140, episode reward: -94.513, mean reward: -1.370 [-100.000,  6.101], mean action: 1.290 [0.000, 3.000],  loss: 18.585224, mse: 6312.773749, mean_q: 82.180124, mean_eps: 0.995359
  1679/100000: episode: 20, duration: 0.763s, episode steps:  97, steps per second: 127, episode reward: -338.783, mean reward: -3.493 [-100.000,  7.063], mean action: 1.536 [0.000, 3.000],  loss: 15.984844, mse: 6418.789737, mean_q: 82.962817, mean_eps: 0.995110
  1772/100000: episode: 21, duration: 0.755s, episode steps:  93, steps per second: 123, episode reward: -227.915, mean reward: -2.451 [-100.000,  0.894], mean action: 1.742 [0.000, 3.000],  loss: 17.050189, mse: 6358.936639, mean_q: 82.302058, mean_eps: 0.994825
  1882/100000: episode: 22, duration: 0.907s, episode steps: 110, steps per second: 121, episode reward: -277.462, mean reward: -2.522 [-100.000,  8.878], mean action: 1.555 [0.000, 3.000],  loss: 16.524841, mse: 6487.850004, mean_q: 83.407806, mean_eps: 0.994521
  1979/100000: episode: 23, duration: 0.766s, episode steps:  97, steps per second: 127, episode reward: -413.615, mean reward: -4.264 [-100.000,  1.060], mean action: 1.619 [0.000, 3.000],  loss: 9.257109, mse: 6552.758069, mean_q: 83.307134, mean_eps: 0.994210
  2071/100000: episode: 24, duration: 0.885s, episode steps:  92, steps per second: 104, episode reward: -427.459, mean reward: -4.646 [-100.000,  1.472], mean action: 1.663 [0.000, 3.000],  loss: 17.776464, mse: 6547.069962, mean_q: 82.975385, mean_eps: 0.993926
  2170/100000: episode: 25, duration: 0.796s, episode steps:  99, steps per second: 124, episode reward: -98.367, mean reward: -0.994 [-100.000, 12.461], mean action: 1.535 [0.000, 3.000],  loss: 20.150106, mse: 6636.233990, mean_q: 83.908291, mean_eps: 0.993640
  2249/100000: episode: 26, duration: 0.548s, episode steps:  79, steps per second: 144, episode reward: -306.010, mean reward: -3.874 [-100.000,  5.728], mean action: 1.519 [0.000, 3.000],  loss: 14.014830, mse: 6569.249363, mean_q: 83.048622, mean_eps: 0.993373
  2365/100000: episode: 27, duration: 0.807s, episode steps: 116, steps per second: 144, episode reward: -204.580, mean reward: -1.764 [-100.000, 19.264], mean action: 1.310 [0.000, 3.000],  loss: 12.619907, mse: 6599.220194, mean_q: 83.336096, mean_eps: 0.993081
  2491/100000: episode: 28, duration: 1.125s, episode steps: 126, steps per second: 112, episode reward: -99.760, mean reward: -0.792 [-100.000, 15.639], mean action: 1.778 [0.000, 3.000],  loss: 11.686645, mse: 6492.591092, mean_q: 82.587999, mean_eps: 0.992717
  2628/100000: episode: 29, duration: 1.025s, episode steps: 137, steps per second: 134, episode reward: -272.842, mean reward: -1.992 [-100.000,  7.194], mean action: 1.555 [0.000, 3.000],  loss: 19.130117, mse: 6674.881946, mean_q: 84.495810, mean_eps: 0.992323
  2713/100000: episode: 30, duration: 0.619s, episode steps:  85, steps per second: 137, episode reward: -150.990, mean reward: -1.776 [-100.000, 49.221], mean action: 1.553 [0.000, 3.000],  loss: 11.866596, mse: 6648.959852, mean_q: 83.344015, mean_eps: 0.991990
  2787/100000: episode: 31, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward: -73.862, mean reward: -0.998 [-100.000,  6.481], mean action: 1.405 [0.000, 3.000],  loss: 9.494286, mse: 6520.662690, mean_q: 82.894766, mean_eps: 0.991752
  2885/100000: episode: 32, duration: 0.659s, episode steps:  98, steps per second: 149, episode reward: -307.815, mean reward: -3.141 [-100.000,  0.572], mean action: 1.704 [0.000, 3.000],  loss: 12.462377, mse: 6600.991002, mean_q: 83.719560, mean_eps: 0.991493
  2989/100000: episode: 33, duration: 0.789s, episode steps: 104, steps per second: 132, episode reward: -180.049, mean reward: -1.731 [-100.000,  7.243], mean action: 1.635 [0.000, 3.000],  loss: 9.009276, mse: 6626.052002, mean_q: 83.414532, mean_eps: 0.991190
  3067/100000: episode: 34, duration: 0.548s, episode steps:  78, steps per second: 142, episode reward: -116.845, mean reward: -1.498 [-100.000,  7.510], mean action: 1.192 [0.000, 3.000],  loss: 11.790933, mse: 6730.486873, mean_q: 84.442861, mean_eps: 0.990917
  3136/100000: episode: 35, duration: 0.504s, episode steps:  69, steps per second: 137, episode reward: -131.914, mean reward: -1.912 [-100.000,  7.009], mean action: 1.391 [0.000, 3.000],  loss: 11.515157, mse: 6551.750736, mean_q: 82.481363, mean_eps: 0.990697
  3252/100000: episode: 36, duration: 0.940s, episode steps: 116, steps per second: 123, episode reward: -142.146, mean reward: -1.225 [-100.000, 43.073], mean action: 1.621 [0.000, 3.000],  loss: 13.639993, mse: 6605.883966, mean_q: 82.086508, mean_eps: 0.990420
  3367/100000: episode: 37, duration: 0.911s, episode steps: 115, steps per second: 126, episode reward: -341.960, mean reward: -2.974 [-100.000, 57.119], mean action: 1.704 [0.000, 3.000],  loss: 12.822056, mse: 6976.229726, mean_q: 84.899212, mean_eps: 0.990073
  3434/100000: episode: 38, duration: 0.543s, episode steps:  67, steps per second: 123, episode reward: -99.124, mean reward: -1.479 [-100.000, 18.642], mean action: 1.433 [0.000, 3.000],  loss: 13.174378, mse: 7116.880415, mean_q: 85.994688, mean_eps: 0.989800
  3529/100000: episode: 39, duration: 0.778s, episode steps:  95, steps per second: 122, episode reward: -125.834, mean reward: -1.325 [-100.000,  6.719], mean action: 1.611 [0.000, 3.000],  loss: 10.620947, mse: 6975.705685, mean_q: 84.760364, mean_eps: 0.989557
  3634/100000: episode: 40, duration: 0.765s, episode steps: 105, steps per second: 137, episode reward: -143.542, mean reward: -1.367 [-100.000, 10.296], mean action: 1.600 [0.000, 3.000],  loss: 13.054993, mse: 6724.660900, mean_q: 83.103023, mean_eps: 0.989257
  3745/100000: episode: 41, duration: 0.867s, episode steps: 111, steps per second: 128, episode reward: -226.080, mean reward: -2.037 [-100.000,  7.185], mean action: 1.514 [0.000, 3.000],  loss: 10.256716, mse: 6789.754856, mean_q: 83.530592, mean_eps: 0.988933
  3841/100000: episode: 42, duration: 0.770s, episode steps:  96, steps per second: 125, episode reward: -208.070, mean reward: -2.167 [-100.000, 10.983], mean action: 1.427 [0.000, 3.000],  loss: 12.413770, mse: 6761.233678, mean_q: 82.861561, mean_eps: 0.988622
  3909/100000: episode: 43, duration: 0.516s, episode steps:  68, steps per second: 132, episode reward: -122.162, mean reward: -1.796 [-100.000, 18.799], mean action: 1.544 [0.000, 3.000],  loss: 10.088117, mse: 6732.391882, mean_q: 83.278001, mean_eps: 0.988377
  3999/100000: episode: 44, duration: 0.655s, episode steps:  90, steps per second: 137, episode reward: 38.349, mean reward:  0.426 [-100.000, 96.890], mean action: 1.367 [0.000, 3.000],  loss: 14.262185, mse: 6843.495730, mean_q: 83.791917, mean_eps: 0.988139
  4070/100000: episode: 45, duration: 0.519s, episode steps:  71, steps per second: 137, episode reward: -89.454, mean reward: -1.260 [-100.000, 10.209], mean action: 1.648 [0.000, 3.000],  loss: 7.691049, mse: 6930.401078, mean_q: 83.944249, mean_eps: 0.987898
  4152/100000: episode: 46, duration: 0.622s, episode steps:  82, steps per second: 132, episode reward: -150.357, mean reward: -1.834 [-100.000,  5.706], mean action: 1.427 [0.000, 3.000],  loss: 11.424053, mse: 6981.356427, mean_q: 84.118214, mean_eps: 0.987669
  4262/100000: episode: 47, duration: 0.780s, episode steps: 110, steps per second: 141, episode reward: -169.684, mean reward: -1.543 [-100.000,  8.572], mean action: 1.673 [0.000, 3.000],  loss: 18.350741, mse: 6858.632089, mean_q: 83.430495, mean_eps: 0.987381
  4360/100000: episode: 48, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -225.283, mean reward: -2.299 [-100.000, 41.987], mean action: 1.592 [0.000, 3.000],  loss: 18.149649, mse: 6743.829914, mean_q: 83.290676, mean_eps: 0.987068
  4489/100000: episode: 49, duration: 0.936s, episode steps: 129, steps per second: 138, episode reward: -404.398, mean reward: -3.135 [-100.000,  2.538], mean action: 1.372 [0.000, 3.000],  loss: 12.199941, mse: 6636.718371, mean_q: 82.405574, mean_eps: 0.986728
  4603/100000: episode: 50, duration: 0.810s, episode steps: 114, steps per second: 141, episode reward: -171.752, mean reward: -1.507 [-100.000,  7.693], mean action: 1.570 [0.000, 3.000],  loss: 12.678016, mse: 6957.706809, mean_q: 84.495501, mean_eps: 0.986364
  4702/100000: episode: 51, duration: 0.745s, episode steps:  99, steps per second: 133, episode reward: -97.478, mean reward: -0.985 [-100.000, 10.491], mean action: 1.515 [0.000, 3.000],  loss: 13.485350, mse: 6826.969169, mean_q: 84.184538, mean_eps: 0.986044
  4779/100000: episode: 52, duration: 0.663s, episode steps:  77, steps per second: 116, episode reward: -130.006, mean reward: -1.688 [-100.000,  8.329], mean action: 1.636 [0.000, 3.000],  loss: 17.937040, mse: 6699.308904, mean_q: 83.636938, mean_eps: 0.985780
  4875/100000: episode: 53, duration: 0.700s, episode steps:  96, steps per second: 137, episode reward: -63.007, mean reward: -0.656 [-100.000, 18.029], mean action: 1.531 [0.000, 3.000],  loss: 13.267464, mse: 6899.938416, mean_q: 84.642624, mean_eps: 0.985520
  4943/100000: episode: 54, duration: 0.514s, episode steps:  68, steps per second: 132, episode reward: -238.787, mean reward: -3.512 [-100.000,  4.054], mean action: 1.279 [0.000, 3.000],  loss: 9.293242, mse: 6776.892391, mean_q: 84.015494, mean_eps: 0.985275
  5054/100000: episode: 55, duration: 0.786s, episode steps: 111, steps per second: 141, episode reward: -60.519, mean reward: -0.545 [-100.000, 80.617], mean action: 1.333 [0.000, 3.000],  loss: 30.352574, mse: 6855.219683, mean_q: 84.183288, mean_eps: 0.985006
  5181/100000: episode: 56, duration: 1.165s, episode steps: 127, steps per second: 109, episode reward: -178.537, mean reward: -1.406 [-100.000,  9.459], mean action: 1.622 [0.000, 3.000],  loss: 12.801355, mse: 6830.387638, mean_q: 83.978476, mean_eps: 0.984649
  5295/100000: episode: 57, duration: 0.784s, episode steps: 114, steps per second: 145, episode reward: -231.039, mean reward: -2.027 [-100.000,  5.551], mean action: 1.544 [0.000, 3.000],  loss: 13.889877, mse: 6959.875750, mean_q: 84.748988, mean_eps: 0.984287
  5392/100000: episode: 58, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -139.454, mean reward: -1.438 [-100.000, 17.048], mean action: 1.474 [0.000, 3.000],  loss: 13.381280, mse: 6996.576157, mean_q: 85.710607, mean_eps: 0.983971
  5481/100000: episode: 59, duration: 0.607s, episode steps:  89, steps per second: 147, episode reward: -277.035, mean reward: -3.113 [-100.000, 72.763], mean action: 1.416 [0.000, 3.000],  loss: 16.454815, mse: 6891.859786, mean_q: 84.777877, mean_eps: 0.983692
  5560/100000: episode: 60, duration: 0.555s, episode steps:  79, steps per second: 142, episode reward: -222.770, mean reward: -2.820 [-100.000, 32.412], mean action: 1.354 [0.000, 3.000],  loss: 10.381697, mse: 6971.415496, mean_q: 85.086827, mean_eps: 0.983440
  5629/100000: episode: 61, duration: 0.471s, episode steps:  69, steps per second: 147, episode reward: -106.605, mean reward: -1.545 [-100.000,  7.660], mean action: 1.435 [0.000, 3.000],  loss: 18.351051, mse: 7105.015059, mean_q: 85.137981, mean_eps: 0.983218
  5733/100000: episode: 62, duration: 0.727s, episode steps: 104, steps per second: 143, episode reward: -298.670, mean reward: -2.872 [-100.000, 20.618], mean action: 1.500 [0.000, 3.000],  loss: 14.691425, mse: 6898.221980, mean_q: 83.687250, mean_eps: 0.982958
  5847/100000: episode: 63, duration: 0.846s, episode steps: 114, steps per second: 135, episode reward: -121.919, mean reward: -1.069 [-100.000,  5.867], mean action: 1.561 [0.000, 3.000],  loss: 10.266547, mse: 7078.546040, mean_q: 85.270069, mean_eps: 0.982631
  5953/100000: episode: 64, duration: 0.859s, episode steps: 106, steps per second: 123, episode reward: -136.584, mean reward: -1.289 [-100.000, 13.014], mean action: 1.538 [0.000, 3.000],  loss: 14.292462, mse: 7001.870099, mean_q: 84.871072, mean_eps: 0.982302
  6030/100000: episode: 65, duration: 0.623s, episode steps:  77, steps per second: 124, episode reward: -105.584, mean reward: -1.371 [-100.000, 24.310], mean action: 1.545 [0.000, 3.000],  loss: 17.764970, mse: 6913.373085, mean_q: 84.166571, mean_eps: 0.982027
  6127/100000: episode: 66, duration: 0.771s, episode steps:  97, steps per second: 126, episode reward: -106.867, mean reward: -1.102 [-100.000,  7.439], mean action: 1.577 [0.000, 3.000],  loss: 11.394402, mse: 6943.780490, mean_q: 84.005709, mean_eps: 0.981766
  6221/100000: episode: 67, duration: 0.705s, episode steps:  94, steps per second: 133, episode reward: -236.904, mean reward: -2.520 [-100.000, 13.395], mean action: 1.670 [0.000, 3.000],  loss: 15.706419, mse: 7100.624075, mean_q: 84.596663, mean_eps: 0.981480
  6287/100000: episode: 68, duration: 0.486s, episode steps:  66, steps per second: 136, episode reward: -97.232, mean reward: -1.473 [-100.000,  7.989], mean action: 1.470 [0.000, 3.000],  loss: 13.783319, mse: 6907.710397, mean_q: 83.701563, mean_eps: 0.981240
  6372/100000: episode: 69, duration: 0.631s, episode steps:  85, steps per second: 135, episode reward: -98.194, mean reward: -1.155 [-100.000,  6.092], mean action: 1.506 [0.000, 3.000],  loss: 14.091140, mse: 6991.937523, mean_q: 83.682820, mean_eps: 0.981013
  6429/100000: episode: 70, duration: 0.455s, episode steps:  57, steps per second: 125, episode reward: -67.486, mean reward: -1.184 [-100.000, 14.878], mean action: 1.281 [0.000, 3.000],  loss: 8.899244, mse: 6811.751242, mean_q: 82.894411, mean_eps: 0.980800
  6551/100000: episode: 71, duration: 0.836s, episode steps: 122, steps per second: 146, episode reward: -144.655, mean reward: -1.186 [-100.000,  8.230], mean action: 1.467 [0.000, 3.000],  loss: 11.528851, mse: 6850.098553, mean_q: 82.800562, mean_eps: 0.980532
  6680/100000: episode: 72, duration: 0.922s, episode steps: 129, steps per second: 140, episode reward: -101.928, mean reward: -0.790 [-100.000,  8.705], mean action: 1.465 [0.000, 3.000],  loss: 11.832737, mse: 6749.581573, mean_q: 82.314263, mean_eps: 0.980155
  6753/100000: episode: 73, duration: 0.514s, episode steps:  73, steps per second: 142, episode reward: -90.223, mean reward: -1.236 [-100.000, 16.866], mean action: 1.521 [0.000, 3.000],  loss: 9.165432, mse: 6692.565296, mean_q: 81.635573, mean_eps: 0.979852
  6822/100000: episode: 74, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -172.693, mean reward: -2.503 [-100.000,  5.768], mean action: 1.391 [0.000, 3.000],  loss: 12.788015, mse: 6952.887872, mean_q: 83.206957, mean_eps: 0.979639
  6945/100000: episode: 75, duration: 0.838s, episode steps: 123, steps per second: 147, episode reward: -339.482, mean reward: -2.760 [-100.000,  4.710], mean action: 1.724 [0.000, 3.000],  loss: 15.067899, mse: 6521.760071, mean_q: 80.634755, mean_eps: 0.979351
  7021/100000: episode: 76, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -125.345, mean reward: -1.649 [-100.000, 11.305], mean action: 1.539 [0.000, 3.000],  loss: 16.011999, mse: 6741.376041, mean_q: 82.144110, mean_eps: 0.979052
  7117/100000: episode: 77, duration: 0.658s, episode steps:  96, steps per second: 146, episode reward: -192.770, mean reward: -2.008 [-100.000, 25.764], mean action: 1.604 [0.000, 3.000],  loss: 12.525897, mse: 6596.955795, mean_q: 80.844397, mean_eps: 0.978794
  7189/100000: episode: 78, duration: 0.622s, episode steps:  72, steps per second: 116, episode reward: -227.644, mean reward: -3.162 [-100.000, 43.408], mean action: 1.236 [0.000, 3.000],  loss: 11.247374, mse: 6654.389398, mean_q: 80.651272, mean_eps: 0.978543
  7301/100000: episode: 79, duration: 0.958s, episode steps: 112, steps per second: 117, episode reward: -312.038, mean reward: -2.786 [-100.000,  3.536], mean action: 1.384 [0.000, 3.000],  loss: 13.111442, mse: 6522.291879, mean_q: 79.864046, mean_eps: 0.978266
  7385/100000: episode: 80, duration: 0.712s, episode steps:  84, steps per second: 118, episode reward: -116.538, mean reward: -1.387 [-100.000,  7.193], mean action: 1.631 [0.000, 3.000],  loss: 22.637111, mse: 6593.258097, mean_q: 79.314439, mean_eps: 0.977972
  7470/100000: episode: 81, duration: 0.641s, episode steps:  85, steps per second: 133, episode reward: -148.742, mean reward: -1.750 [-100.000, 42.787], mean action: 1.482 [0.000, 3.000],  loss: 15.547394, mse: 6448.218876, mean_q: 78.929899, mean_eps: 0.977719
  7553/100000: episode: 82, duration: 0.656s, episode steps:  83, steps per second: 126, episode reward: -144.336, mean reward: -1.739 [-100.000,  7.113], mean action: 1.639 [0.000, 3.000],  loss: 15.444364, mse: 6470.431776, mean_q: 79.348154, mean_eps: 0.977467
  7623/100000: episode: 83, duration: 0.597s, episode steps:  70, steps per second: 117, episode reward: -124.129, mean reward: -1.773 [-100.000,  9.849], mean action: 1.586 [0.000, 3.000],  loss: 14.929593, mse: 6627.263302, mean_q: 79.976427, mean_eps: 0.977238
  7694/100000: episode: 84, duration: 0.609s, episode steps:  71, steps per second: 117, episode reward: -64.785, mean reward: -0.912 [-100.000, 16.206], mean action: 1.465 [0.000, 3.000],  loss: 12.747333, mse: 6529.889841, mean_q: 79.505637, mean_eps: 0.977026
  7757/100000: episode: 85, duration: 0.528s, episode steps:  63, steps per second: 119, episode reward: -66.032, mean reward: -1.048 [-100.000, 12.320], mean action: 1.397 [0.000, 3.000],  loss: 15.558509, mse: 6663.699544, mean_q: 81.748897, mean_eps: 0.976825
  7827/100000: episode: 86, duration: 0.617s, episode steps:  70, steps per second: 114, episode reward: -83.028, mean reward: -1.186 [-100.000, 15.778], mean action: 1.486 [0.000, 3.000],  loss: 8.795615, mse: 6696.930218, mean_q: 82.144304, mean_eps: 0.976625
  7958/100000: episode: 87, duration: 1.265s, episode steps: 131, steps per second: 104, episode reward: -158.355, mean reward: -1.209 [-100.000,  6.430], mean action: 1.420 [0.000, 3.000],  loss: 10.268434, mse: 6535.033527, mean_q: 79.712534, mean_eps: 0.976324
  8062/100000: episode: 88, duration: 0.943s, episode steps: 104, steps per second: 110, episode reward: -262.450, mean reward: -2.524 [-100.000, 24.511], mean action: 1.510 [0.000, 3.000],  loss: 18.087930, mse: 6570.469764, mean_q: 80.014965, mean_eps: 0.975971
  8169/100000: episode: 89, duration: 0.894s, episode steps: 107, steps per second: 120, episode reward: -306.199, mean reward: -2.862 [-100.000,  0.638], mean action: 1.449 [0.000, 3.000],  loss: 14.238451, mse: 6753.425571, mean_q: 81.830175, mean_eps: 0.975655
  8282/100000: episode: 90, duration: 1.009s, episode steps: 113, steps per second: 112, episode reward: -326.655, mean reward: -2.891 [-100.000, 20.062], mean action: 1.531 [0.000, 3.000],  loss: 10.701921, mse: 6735.210609, mean_q: 81.028216, mean_eps: 0.975325
  8342/100000: episode: 91, duration: 0.516s, episode steps:  60, steps per second: 116, episode reward: -89.827, mean reward: -1.497 [-100.000, 21.269], mean action: 1.633 [0.000, 3.000],  loss: 8.059194, mse: 6589.776017, mean_q: 80.709881, mean_eps: 0.975066
  8409/100000: episode: 92, duration: 0.511s, episode steps:  67, steps per second: 131, episode reward: -48.471, mean reward: -0.723 [-100.000, 13.408], mean action: 1.328 [0.000, 3.000],  loss: 9.431474, mse: 6406.875160, mean_q: 78.910278, mean_eps: 0.974875
  8536/100000: episode: 93, duration: 1.023s, episode steps: 127, steps per second: 124, episode reward: -141.271, mean reward: -1.112 [-100.000,  7.018], mean action: 1.559 [0.000, 3.000],  loss: 15.249694, mse: 6525.269908, mean_q: 78.865996, mean_eps: 0.974584
  8616/100000: episode: 94, duration: 0.711s, episode steps:  80, steps per second: 113, episode reward: -183.190, mean reward: -2.290 [-100.000,  6.128], mean action: 1.775 [0.000, 3.000],  loss: 21.409879, mse: 6630.664514, mean_q: 78.943889, mean_eps: 0.974274
  8708/100000: episode: 95, duration: 0.779s, episode steps:  92, steps per second: 118, episode reward: -220.308, mean reward: -2.395 [-100.000, 90.308], mean action: 1.446 [0.000, 3.000],  loss: 15.235015, mse: 6689.951788, mean_q: 80.489694, mean_eps: 0.974016
  8799/100000: episode: 96, duration: 0.766s, episode steps:  91, steps per second: 119, episode reward: -189.726, mean reward: -2.085 [-100.000,  8.064], mean action: 1.473 [0.000, 3.000],  loss: 16.199768, mse: 6520.634970, mean_q: 79.739346, mean_eps: 0.973741
  8903/100000: episode: 97, duration: 0.864s, episode steps: 104, steps per second: 120, episode reward: -336.935, mean reward: -3.240 [-100.000,  1.266], mean action: 1.558 [0.000, 3.000],  loss: 12.347588, mse: 6705.689256, mean_q: 80.805744, mean_eps: 0.973448
  8993/100000: episode: 98, duration: 0.728s, episode steps:  90, steps per second: 124, episode reward: -341.110, mean reward: -3.790 [-100.000,  0.743], mean action: 1.367 [0.000, 3.000],  loss: 13.077309, mse: 6735.272114, mean_q: 80.927337, mean_eps: 0.973158
  9059/100000: episode: 99, duration: 0.602s, episode steps:  66, steps per second: 110, episode reward: -116.871, mean reward: -1.771 [-100.000,  5.191], mean action: 1.485 [0.000, 3.000],  loss: 8.166699, mse: 6768.588150, mean_q: 81.492582, mean_eps: 0.972924
  9135/100000: episode: 100, duration: 0.647s, episode steps:  76, steps per second: 118, episode reward: -194.504, mean reward: -2.559 [-100.000, 26.987], mean action: 1.395 [0.000, 3.000],  loss: 11.600090, mse: 6579.185187, mean_q: 79.749587, mean_eps: 0.972711
  9243/100000: episode: 101, duration: 0.884s, episode steps: 108, steps per second: 122, episode reward: -159.019, mean reward: -1.472 [-100.000,  3.181], mean action: 1.519 [0.000, 3.000],  loss: 20.299513, mse: 6575.778316, mean_q: 79.758418, mean_eps: 0.972434
  9338/100000: episode: 102, duration: 0.804s, episode steps:  95, steps per second: 118, episode reward: -441.352, mean reward: -4.646 [-100.000,  4.256], mean action: 1.600 [0.000, 3.000],  loss: 9.512260, mse: 6662.301449, mean_q: 80.188085, mean_eps: 0.972130
  9451/100000: episode: 103, duration: 0.904s, episode steps: 113, steps per second: 125, episode reward: -237.105, mean reward: -2.098 [-100.000,  6.881], mean action: 1.442 [0.000, 3.000],  loss: 17.699787, mse: 6467.622956, mean_q: 79.565381, mean_eps: 0.971818
  9571/100000: episode: 104, duration: 1.054s, episode steps: 120, steps per second: 114, episode reward: -159.209, mean reward: -1.327 [-100.000,  7.199], mean action: 1.558 [0.000, 3.000],  loss: 12.694774, mse: 6552.092790, mean_q: 79.521840, mean_eps: 0.971469
  9635/100000: episode: 105, duration: 0.597s, episode steps:  64, steps per second: 107, episode reward: -107.042, mean reward: -1.673 [-100.000, 21.962], mean action: 1.547 [0.000, 3.000],  loss: 8.868395, mse: 6688.274185, mean_q: 80.675305, mean_eps: 0.971193
  9742/100000: episode: 106, duration: 1.162s, episode steps: 107, steps per second:  92, episode reward: -318.938, mean reward: -2.981 [-100.000, 99.275], mean action: 1.682 [0.000, 3.000],  loss: 16.512062, mse: 6670.341728, mean_q: 79.665914, mean_eps: 0.970936
  9842/100000: episode: 107, duration: 0.898s, episode steps: 100, steps per second: 111, episode reward: -331.841, mean reward: -3.318 [-100.000,  0.083], mean action: 1.460 [0.000, 3.000],  loss: 17.750017, mse: 6763.858350, mean_q: 81.598866, mean_eps: 0.970625
  9938/100000: episode: 108, duration: 0.995s, episode steps:  96, steps per second:  97, episode reward: -160.080, mean reward: -1.667 [-100.000, 32.944], mean action: 1.594 [0.000, 3.000],  loss: 12.387506, mse: 6563.651586, mean_q: 79.456565, mean_eps: 0.970332
 10058/100000: episode: 109, duration: 1.195s, episode steps: 120, steps per second: 100, episode reward: -484.127, mean reward: -4.034 [-100.000,  1.020], mean action: 1.575 [0.000, 3.000],  loss: 14.125106, mse: 6561.262195, mean_q: 79.773615, mean_eps: 0.970008
 10170/100000: episode: 110, duration: 1.031s, episode steps: 112, steps per second: 109, episode reward: -374.541, mean reward: -3.344 [-100.000, 10.727], mean action: 1.571 [0.000, 3.000],  loss: 11.338476, mse: 6606.856118, mean_q: 79.898715, mean_eps: 0.969659
 10264/100000: episode: 111, duration: 0.955s, episode steps:  94, steps per second:  98, episode reward: -288.434, mean reward: -3.068 [-100.000,  5.444], mean action: 1.617 [0.000, 3.000],  loss: 10.926210, mse: 6636.385753, mean_q: 79.153061, mean_eps: 0.969351
 10358/100000: episode: 112, duration: 1.020s, episode steps:  94, steps per second:  92, episode reward: -336.830, mean reward: -3.583 [-100.000, 99.858], mean action: 1.723 [0.000, 3.000],  loss: 9.703789, mse: 6542.555129, mean_q: 79.105905, mean_eps: 0.969068
 10427/100000: episode: 113, duration: 0.602s, episode steps:  69, steps per second: 115, episode reward: -121.271, mean reward: -1.758 [-100.000,  8.084], mean action: 1.464 [0.000, 3.000],  loss: 12.206355, mse: 6740.556924, mean_q: 80.674119, mean_eps: 0.968824
 10513/100000: episode: 114, duration: 0.788s, episode steps:  86, steps per second: 109, episode reward: -186.150, mean reward: -2.165 [-100.000, 14.332], mean action: 1.558 [0.000, 3.000],  loss: 11.156247, mse: 6582.006121, mean_q: 79.332430, mean_eps: 0.968591
 10583/100000: episode: 115, duration: 0.625s, episode steps:  70, steps per second: 112, episode reward: -68.045, mean reward: -0.972 [-100.000, 17.342], mean action: 1.400 [0.000, 3.000],  loss: 12.318309, mse: 6746.194678, mean_q: 80.820666, mean_eps: 0.968357
 10667/100000: episode: 116, duration: 0.677s, episode steps:  84, steps per second: 124, episode reward: -159.037, mean reward: -1.893 [-100.000, 27.267], mean action: 1.405 [0.000, 3.000],  loss: 13.645280, mse: 6669.358805, mean_q: 80.179108, mean_eps: 0.968127
 10781/100000: episode: 117, duration: 1.148s, episode steps: 114, steps per second:  99, episode reward: -123.705, mean reward: -1.085 [-100.000, 10.948], mean action: 1.377 [0.000, 3.000],  loss: 11.071922, mse: 6493.602222, mean_q: 78.503021, mean_eps: 0.967830
 10851/100000: episode: 118, duration: 0.626s, episode steps:  70, steps per second: 112, episode reward: -121.240, mean reward: -1.732 [-100.000, 10.445], mean action: 1.471 [0.000, 3.000],  loss: 15.295390, mse: 6509.236182, mean_q: 78.835078, mean_eps: 0.967553
 10943/100000: episode: 119, duration: 1.042s, episode steps:  92, steps per second:  88, episode reward: -115.346, mean reward: -1.254 [-100.000,  6.733], mean action: 1.413 [0.000, 3.000],  loss: 9.682243, mse: 6553.940239, mean_q: 78.609282, mean_eps: 0.967311
 11004/100000: episode: 120, duration: 0.601s, episode steps:  61, steps per second: 102, episode reward: -75.611, mean reward: -1.240 [-100.000,  8.086], mean action: 1.508 [0.000, 3.000],  loss: 15.527311, mse: 6599.557433, mean_q: 79.253086, mean_eps: 0.967081
 11085/100000: episode: 121, duration: 0.873s, episode steps:  81, steps per second:  93, episode reward: -176.855, mean reward: -2.183 [-100.000, 10.130], mean action: 1.235 [0.000, 3.000],  loss: 10.938178, mse: 6379.201817, mean_q: 77.927614, mean_eps: 0.966868
 11183/100000: episode: 122, duration: 0.903s, episode steps:  98, steps per second: 109, episode reward: -140.782, mean reward: -1.437 [-100.000,  7.764], mean action: 1.510 [0.000, 3.000],  loss: 14.791584, mse: 6363.711386, mean_q: 76.663414, mean_eps: 0.966599
 11262/100000: episode: 123, duration: 0.673s, episode steps:  79, steps per second: 117, episode reward: -157.117, mean reward: -1.989 [-100.000,  7.344], mean action: 1.544 [0.000, 3.000],  loss: 10.807180, mse: 6480.324590, mean_q: 78.369102, mean_eps: 0.966334
 11361/100000: episode: 124, duration: 0.864s, episode steps:  99, steps per second: 115, episode reward: -303.430, mean reward: -3.065 [-100.000,  5.123], mean action: 1.434 [0.000, 3.000],  loss: 12.644433, mse: 6452.172713, mean_q: 78.596011, mean_eps: 0.966067
 11483/100000: episode: 125, duration: 0.969s, episode steps: 122, steps per second: 126, episode reward: -101.766, mean reward: -0.834 [-100.000, 11.924], mean action: 1.615 [0.000, 3.000],  loss: 13.927528, mse: 6247.674380, mean_q: 77.123799, mean_eps: 0.965736
 11541/100000: episode: 126, duration: 0.447s, episode steps:  58, steps per second: 130, episode reward: -77.227, mean reward: -1.332 [-100.000,  7.390], mean action: 1.328 [0.000, 3.000],  loss: 9.408692, mse: 6448.589229, mean_q: 77.836014, mean_eps: 0.965466
 11618/100000: episode: 127, duration: 0.591s, episode steps:  77, steps per second: 130, episode reward: -90.229, mean reward: -1.172 [-100.000, 11.649], mean action: 1.571 [0.000, 3.000],  loss: 12.860872, mse: 6314.653181, mean_q: 77.039719, mean_eps: 0.965263
 11711/100000: episode: 128, duration: 0.756s, episode steps:  93, steps per second: 123, episode reward: -135.630, mean reward: -1.458 [-100.000,  7.585], mean action: 1.495 [0.000, 3.000],  loss: 13.615960, mse: 6419.385905, mean_q: 77.968354, mean_eps: 0.965008
 11807/100000: episode: 129, duration: 0.727s, episode steps:  96, steps per second: 132, episode reward: -161.941, mean reward: -1.687 [-100.000, 14.626], mean action: 1.531 [0.000, 3.000],  loss: 11.618982, mse: 6558.192403, mean_q: 77.325640, mean_eps: 0.964724
 11893/100000: episode: 130, duration: 0.655s, episode steps:  86, steps per second: 131, episode reward: -91.888, mean reward: -1.068 [-100.000, 11.257], mean action: 1.512 [0.000, 3.000],  loss: 13.603843, mse: 6340.246037, mean_q: 76.069282, mean_eps: 0.964451
 12005/100000: episode: 131, duration: 0.920s, episode steps: 112, steps per second: 122, episode reward: -232.246, mean reward: -2.074 [-100.000,  5.599], mean action: 1.473 [0.000, 3.000],  loss: 15.613515, mse: 6318.267081, mean_q: 75.839915, mean_eps: 0.964155
 12095/100000: episode: 132, duration: 0.691s, episode steps:  90, steps per second: 130, episode reward: -133.334, mean reward: -1.481 [-100.000, 11.560], mean action: 1.556 [0.000, 3.000],  loss: 11.190524, mse: 6473.101188, mean_q: 77.929797, mean_eps: 0.963852
 12155/100000: episode: 133, duration: 0.458s, episode steps:  60, steps per second: 131, episode reward: -86.927, mean reward: -1.449 [-100.000, 13.629], mean action: 1.300 [0.000, 3.000],  loss: 9.171840, mse: 6669.821851, mean_q: 79.686609, mean_eps: 0.963626
 12241/100000: episode: 134, duration: 0.695s, episode steps:  86, steps per second: 124, episode reward: -291.048, mean reward: -3.384 [-100.000, 112.194], mean action: 1.360 [0.000, 3.000],  loss: 15.018403, mse: 6785.434888, mean_q: 79.483452, mean_eps: 0.963407
 12352/100000: episode: 135, duration: 0.870s, episode steps: 111, steps per second: 128, episode reward: -135.606, mean reward: -1.222 [-100.000, 11.195], mean action: 1.712 [0.000, 3.000],  loss: 11.853360, mse: 6789.117113, mean_q: 79.583130, mean_eps: 0.963112
 12453/100000: episode: 136, duration: 0.780s, episode steps: 101, steps per second: 130, episode reward: -110.119, mean reward: -1.090 [-100.000,  6.993], mean action: 1.663 [0.000, 3.000],  loss: 12.960128, mse: 6629.053000, mean_q: 78.033702, mean_eps: 0.962794
 12556/100000: episode: 137, duration: 0.834s, episode steps: 103, steps per second: 124, episode reward: -198.065, mean reward: -1.923 [-100.000,  1.287], mean action: 1.689 [0.000, 3.000],  loss: 10.167917, mse: 6453.632154, mean_q: 76.591207, mean_eps: 0.962488
 12629/100000: episode: 138, duration: 0.568s, episode steps:  73, steps per second: 129, episode reward: -85.021, mean reward: -1.165 [-100.000,  8.102], mean action: 1.671 [0.000, 3.000],  loss: 14.632177, mse: 6423.794835, mean_q: 75.104791, mean_eps: 0.962224
 12716/100000: episode: 139, duration: 0.664s, episode steps:  87, steps per second: 131, episode reward: -256.279, mean reward: -2.946 [-100.000, 99.328], mean action: 1.414 [0.000, 3.000],  loss: 17.677056, mse: 6396.808308, mean_q: 75.046904, mean_eps: 0.961984
 12786/100000: episode: 140, duration: 0.599s, episode steps:  70, steps per second: 117, episode reward: -127.802, mean reward: -1.826 [-100.000,  5.504], mean action: 1.457 [0.000, 3.000],  loss: 26.238246, mse: 6407.805085, mean_q: 75.958325, mean_eps: 0.961749
 12925/100000: episode: 141, duration: 1.089s, episode steps: 139, steps per second: 128, episode reward: -278.535, mean reward: -2.004 [-100.000,  5.783], mean action: 1.403 [0.000, 3.000],  loss: 10.664473, mse: 6479.207534, mean_q: 76.007298, mean_eps: 0.961435
 13003/100000: episode: 142, duration: 0.603s, episode steps:  78, steps per second: 129, episode reward: -124.858, mean reward: -1.601 [-100.000,  7.814], mean action: 1.526 [0.000, 3.000],  loss: 14.706877, mse: 6367.871250, mean_q: 75.594823, mean_eps: 0.961110
 13071/100000: episode: 143, duration: 0.556s, episode steps:  68, steps per second: 122, episode reward: -66.459, mean reward: -0.977 [-100.000, 11.772], mean action: 1.485 [0.000, 3.000],  loss: 7.904226, mse: 6527.014426, mean_q: 75.575248, mean_eps: 0.960891
 13131/100000: episode: 144, duration: 0.494s, episode steps:  60, steps per second: 121, episode reward: -65.402, mean reward: -1.090 [-100.000,  8.092], mean action: 1.683 [0.000, 3.000],  loss: 8.832024, mse: 6432.303743, mean_q: 75.711567, mean_eps: 0.960699
 13236/100000: episode: 145, duration: 0.808s, episode steps: 105, steps per second: 130, episode reward: -99.437, mean reward: -0.947 [-100.000, 24.358], mean action: 1.619 [0.000, 3.000],  loss: 12.839448, mse: 6370.756059, mean_q: 75.912769, mean_eps: 0.960451
 13334/100000: episode: 146, duration: 0.783s, episode steps:  98, steps per second: 125, episode reward:  0.331, mean reward:  0.003 [-100.000, 108.422], mean action: 1.429 [0.000, 3.000],  loss: 15.589698, mse: 6317.844846, mean_q: 75.498183, mean_eps: 0.960147
 13396/100000: episode: 147, duration: 0.487s, episode steps:  62, steps per second: 127, episode reward: -133.603, mean reward: -2.155 [-100.000, 10.999], mean action: 1.274 [0.000, 3.000],  loss: 27.377862, mse: 6376.884884, mean_q: 76.325566, mean_eps: 0.959906
 13471/100000: episode: 148, duration: 0.571s, episode steps:  75, steps per second: 131, episode reward: -123.865, mean reward: -1.652 [-100.000,  7.069], mean action: 1.493 [0.000, 3.000],  loss: 18.143474, mse: 6481.720970, mean_q: 78.008429, mean_eps: 0.959701
 13597/100000: episode: 149, duration: 1.291s, episode steps: 126, steps per second:  98, episode reward: -351.151, mean reward: -2.787 [-100.000,  5.175], mean action: 1.484 [0.000, 3.000],  loss: 13.645748, mse: 6328.443363, mean_q: 77.502896, mean_eps: 0.959400
 13701/100000: episode: 150, duration: 1.182s, episode steps: 104, steps per second:  88, episode reward: -161.858, mean reward: -1.556 [-100.000,  6.412], mean action: 1.529 [0.000, 3.000],  loss: 19.045331, mse: 6400.837614, mean_q: 77.524487, mean_eps: 0.959054
 13823/100000: episode: 151, duration: 1.174s, episode steps: 122, steps per second: 104, episode reward: -117.980, mean reward: -0.967 [-100.000, 13.639], mean action: 1.418 [0.000, 3.000],  loss: 18.248637, mse: 6390.249496, mean_q: 77.352521, mean_eps: 0.958716
 13907/100000: episode: 152, duration: 0.933s, episode steps:  84, steps per second:  90, episode reward: -379.947, mean reward: -4.523 [-100.000, -0.930], mean action: 1.321 [0.000, 3.000],  loss: 17.142157, mse: 6386.497471, mean_q: 76.990427, mean_eps: 0.958407
 13982/100000: episode: 153, duration: 0.757s, episode steps:  75, steps per second:  99, episode reward: -83.196, mean reward: -1.109 [-100.000, 11.249], mean action: 1.387 [0.000, 3.000],  loss: 13.354187, mse: 6465.161536, mean_q: 77.242171, mean_eps: 0.958168
 14083/100000: episode: 154, duration: 0.974s, episode steps: 101, steps per second: 104, episode reward: -156.356, mean reward: -1.548 [-100.000, 10.125], mean action: 1.337 [0.000, 3.000],  loss: 16.359330, mse: 6490.441048, mean_q: 77.408819, mean_eps: 0.957904
 14160/100000: episode: 155, duration: 0.687s, episode steps:  77, steps per second: 112, episode reward: -146.632, mean reward: -1.904 [-100.000,  7.782], mean action: 1.351 [0.000, 3.000],  loss: 13.313774, mse: 6582.934938, mean_q: 79.289425, mean_eps: 0.957637
 14255/100000: episode: 156, duration: 0.798s, episode steps:  95, steps per second: 119, episode reward: -281.949, mean reward: -2.968 [-100.000, 96.229], mean action: 1.421 [0.000, 3.000],  loss: 16.386587, mse: 6581.088071, mean_q: 77.507897, mean_eps: 0.957379
 14338/100000: episode: 157, duration: 0.643s, episode steps:  83, steps per second: 129, episode reward: -155.584, mean reward: -1.875 [-100.000, 20.179], mean action: 1.590 [0.000, 3.000],  loss: 16.475904, mse: 6487.303952, mean_q: 77.803767, mean_eps: 0.957112
 14475/100000: episode: 158, duration: 1.066s, episode steps: 137, steps per second: 128, episode reward: -209.941, mean reward: -1.532 [-100.000, 39.544], mean action: 1.657 [0.000, 3.000],  loss: 16.989739, mse: 6706.923693, mean_q: 78.868534, mean_eps: 0.956782
 14539/100000: episode: 159, duration: 0.510s, episode steps:  64, steps per second: 125, episode reward: -86.496, mean reward: -1.351 [-100.000,  7.086], mean action: 1.500 [0.000, 3.000],  loss: 17.837173, mse: 6693.692772, mean_q: 78.766711, mean_eps: 0.956480
 14621/100000: episode: 160, duration: 0.633s, episode steps:  82, steps per second: 130, episode reward: -99.210, mean reward: -1.210 [-100.000,  7.124], mean action: 1.378 [0.000, 3.000],  loss: 16.683501, mse: 6679.443830, mean_q: 78.938106, mean_eps: 0.956261
 14702/100000: episode: 161, duration: 0.618s, episode steps:  81, steps per second: 131, episode reward: -213.659, mean reward: -2.638 [-100.000,  3.730], mean action: 1.210 [0.000, 3.000],  loss: 15.021340, mse: 6893.400234, mean_q: 80.936374, mean_eps: 0.956017
 14832/100000: episode: 162, duration: 1.037s, episode steps: 130, steps per second: 125, episode reward: -163.724, mean reward: -1.259 [-100.000,  5.711], mean action: 1.662 [0.000, 3.000],  loss: 16.273793, mse: 6883.178542, mean_q: 81.265084, mean_eps: 0.955700
 14909/100000: episode: 163, duration: 0.594s, episode steps:  77, steps per second: 130, episode reward: -157.928, mean reward: -2.051 [-100.000,  4.974], mean action: 1.623 [0.000, 3.000],  loss: 16.051691, mse: 6873.342577, mean_q: 81.806136, mean_eps: 0.955390
 15001/100000: episode: 164, duration: 0.706s, episode steps:  92, steps per second: 130, episode reward: -106.099, mean reward: -1.153 [-100.000, 16.730], mean action: 1.554 [0.000, 3.000],  loss: 10.116012, mse: 6870.075408, mean_q: 81.351374, mean_eps: 0.955136
 15074/100000: episode: 165, duration: 0.609s, episode steps:  73, steps per second: 120, episode reward: -107.352, mean reward: -1.471 [-100.000,  8.117], mean action: 1.493 [0.000, 3.000],  loss: 17.401751, mse: 6970.023464, mean_q: 81.815752, mean_eps: 0.954889
 15171/100000: episode: 166, duration: 0.914s, episode steps:  97, steps per second: 106, episode reward: -101.339, mean reward: -1.045 [-100.000,  6.319], mean action: 1.526 [0.000, 3.000],  loss: 17.776368, mse: 6963.442267, mean_q: 82.003297, mean_eps: 0.954634
 15256/100000: episode: 167, duration: 0.726s, episode steps:  85, steps per second: 117, episode reward: -90.744, mean reward: -1.068 [-100.000,  8.358], mean action: 1.494 [0.000, 3.000],  loss: 21.534329, mse: 6817.475816, mean_q: 81.133574, mean_eps: 0.954361
 15369/100000: episode: 168, duration: 1.007s, episode steps: 113, steps per second: 112, episode reward: -119.653, mean reward: -1.059 [-100.000,  6.067], mean action: 1.487 [0.000, 3.000],  loss: 13.109397, mse: 6873.403307, mean_q: 81.121346, mean_eps: 0.954064
 15437/100000: episode: 169, duration: 0.612s, episode steps:  68, steps per second: 111, episode reward: -120.039, mean reward: -1.765 [-100.000, 17.181], mean action: 1.515 [0.000, 3.000],  loss: 12.616569, mse: 6717.870505, mean_q: 79.702356, mean_eps: 0.953793
 15508/100000: episode: 170, duration: 0.641s, episode steps:  71, steps per second: 111, episode reward: -109.397, mean reward: -1.541 [-100.000,  9.656], mean action: 1.606 [0.000, 3.000],  loss: 8.674465, mse: 6777.729403, mean_q: 80.891615, mean_eps: 0.953584
 15634/100000: episode: 171, duration: 1.008s, episode steps: 126, steps per second: 125, episode reward: -17.505, mean reward: -0.139 [-100.000, 118.001], mean action: 1.444 [0.000, 3.000],  loss: 11.298461, mse: 6613.861208, mean_q: 78.116602, mean_eps: 0.953288
 15747/100000: episode: 172, duration: 0.869s, episode steps: 113, steps per second: 130, episode reward: -89.174, mean reward: -0.789 [-100.000,  9.836], mean action: 1.398 [0.000, 3.000],  loss: 17.106442, mse: 6457.274626, mean_q: 76.744958, mean_eps: 0.952930
 15836/100000: episode: 173, duration: 0.712s, episode steps:  89, steps per second: 125, episode reward: -175.640, mean reward: -1.973 [-100.000,  7.660], mean action: 1.539 [0.000, 3.000],  loss: 24.217682, mse: 6374.690984, mean_q: 76.088219, mean_eps: 0.952627
 15922/100000: episode: 174, duration: 0.673s, episode steps:  86, steps per second: 128, episode reward: -46.548, mean reward: -0.541 [-100.000, 12.016], mean action: 1.512 [0.000, 3.000],  loss: 20.776442, mse: 6430.764115, mean_q: 77.269748, mean_eps: 0.952364
 16007/100000: episode: 175, duration: 0.771s, episode steps:  85, steps per second: 110, episode reward: -116.229, mean reward: -1.367 [-100.000,  7.874], mean action: 1.588 [0.000, 3.000],  loss: 18.173711, mse: 6775.896427, mean_q: 80.804368, mean_eps: 0.952108
 16113/100000: episode: 176, duration: 0.973s, episode steps: 106, steps per second: 109, episode reward: -137.905, mean reward: -1.301 [-100.000,  5.544], mean action: 1.623 [0.000, 3.000],  loss: 11.190148, mse: 6574.617505, mean_q: 78.407559, mean_eps: 0.951821
 16197/100000: episode: 177, duration: 0.712s, episode steps:  84, steps per second: 118, episode reward: -134.445, mean reward: -1.601 [-100.000, 12.576], mean action: 1.500 [0.000, 3.000],  loss: 9.497357, mse: 6646.631295, mean_q: 79.314116, mean_eps: 0.951537
 16313/100000: episode: 178, duration: 1.016s, episode steps: 116, steps per second: 114, episode reward: -150.677, mean reward: -1.299 [-100.000,  3.740], mean action: 1.612 [0.000, 3.000],  loss: 12.546689, mse: 6492.273829, mean_q: 77.977392, mean_eps: 0.951236
 16393/100000: episode: 179, duration: 0.739s, episode steps:  80, steps per second: 108, episode reward: -179.655, mean reward: -2.246 [-100.000, 23.587], mean action: 1.525 [0.000, 3.000],  loss: 14.484267, mse: 6758.514893, mean_q: 79.624081, mean_eps: 0.950943
 16484/100000: episode: 180, duration: 0.772s, episode steps:  91, steps per second: 118, episode reward: -185.963, mean reward: -2.044 [-100.000, 109.025], mean action: 1.549 [0.000, 3.000],  loss: 14.439165, mse: 6656.889697, mean_q: 78.571574, mean_eps: 0.950686
 16565/100000: episode: 181, duration: 0.641s, episode steps:  81, steps per second: 126, episode reward: -389.754, mean reward: -4.812 [-100.000, -0.039], mean action: 1.630 [0.000, 3.000],  loss: 20.594648, mse: 6798.812428, mean_q: 79.432308, mean_eps: 0.950428
 16653/100000: episode: 182, duration: 0.677s, episode steps:  88, steps per second: 130, episode reward: -400.557, mean reward: -4.552 [-100.000, 25.008], mean action: 1.307 [0.000, 3.000],  loss: 23.612788, mse: 7004.909779, mean_q: 81.225479, mean_eps: 0.950174
 16722/100000: episode: 183, duration: 0.643s, episode steps:  69, steps per second: 107, episode reward: -64.521, mean reward: -0.935 [-100.000,  8.923], mean action: 1.478 [0.000, 3.000],  loss: 16.233272, mse: 6861.113890, mean_q: 79.726304, mean_eps: 0.949939
 16843/100000: episode: 184, duration: 1.076s, episode steps: 121, steps per second: 112, episode reward: -172.153, mean reward: -1.423 [-100.000,  3.227], mean action: 1.727 [0.000, 3.000],  loss: 14.712006, mse: 6889.751820, mean_q: 79.759844, mean_eps: 0.949654
 16962/100000: episode: 185, duration: 1.055s, episode steps: 119, steps per second: 113, episode reward: -34.481, mean reward: -0.290 [-100.000, 61.372], mean action: 1.529 [0.000, 3.000],  loss: 13.320137, mse: 6938.618492, mean_q: 80.583660, mean_eps: 0.949294
 17029/100000: episode: 186, duration: 0.556s, episode steps:  67, steps per second: 121, episode reward: -156.310, mean reward: -2.333 [-100.000,  8.023], mean action: 1.582 [0.000, 3.000],  loss: 16.533313, mse: 6905.385771, mean_q: 80.420107, mean_eps: 0.949015
 17142/100000: episode: 187, duration: 0.984s, episode steps: 113, steps per second: 115, episode reward: -178.083, mean reward: -1.576 [-100.000,  2.907], mean action: 1.708 [0.000, 3.000],  loss: 9.423671, mse: 6921.412252, mean_q: 80.655085, mean_eps: 0.948745
 17213/100000: episode: 188, duration: 0.611s, episode steps:  71, steps per second: 116, episode reward: -201.721, mean reward: -2.841 [-100.000, 18.181], mean action: 1.324 [0.000, 3.000],  loss: 16.233291, mse: 6920.411477, mean_q: 79.949497, mean_eps: 0.948469
 17312/100000: episode: 189, duration: 0.905s, episode steps:  99, steps per second: 109, episode reward: -198.143, mean reward: -2.001 [-100.000,  1.260], mean action: 1.273 [0.000, 3.000],  loss: 19.606565, mse: 6921.628758, mean_q: 80.717623, mean_eps: 0.948214
 17421/100000: episode: 190, duration: 0.957s, episode steps: 109, steps per second: 114, episode reward: -119.975, mean reward: -1.101 [-100.000, 12.073], mean action: 1.505 [0.000, 3.000],  loss: 20.381754, mse: 7095.578031, mean_q: 81.495747, mean_eps: 0.947902
 17535/100000: episode: 191, duration: 0.880s, episode steps: 114, steps per second: 130, episode reward: -157.181, mean reward: -1.379 [-100.000,  6.273], mean action: 1.447 [0.000, 3.000],  loss: 12.930507, mse: 7063.820942, mean_q: 81.171318, mean_eps: 0.947568
 17618/100000: episode: 192, duration: 0.672s, episode steps:  83, steps per second: 123, episode reward: -131.598, mean reward: -1.586 [-100.000, 11.774], mean action: 1.759 [0.000, 3.000],  loss: 24.864947, mse: 7082.334884, mean_q: 80.849130, mean_eps: 0.947272
 17724/100000: episode: 193, duration: 0.825s, episode steps: 106, steps per second: 129, episode reward: -103.201, mean reward: -0.974 [-100.000,  6.593], mean action: 1.679 [0.000, 3.000],  loss: 16.122287, mse: 7136.747227, mean_q: 81.389636, mean_eps: 0.946989
 17789/100000: episode: 194, duration: 0.501s, episode steps:  65, steps per second: 130, episode reward: -150.343, mean reward: -2.313 [-100.000,  9.322], mean action: 1.431 [0.000, 3.000],  loss: 14.030361, mse: 7286.215152, mean_q: 81.788958, mean_eps: 0.946732
 17873/100000: episode: 195, duration: 0.687s, episode steps:  84, steps per second: 122, episode reward: -134.181, mean reward: -1.597 [-100.000,  7.749], mean action: 1.357 [0.000, 3.000],  loss: 12.875700, mse: 7228.143857, mean_q: 81.100214, mean_eps: 0.946508
 17993/100000: episode: 196, duration: 0.917s, episode steps: 120, steps per second: 131, episode reward: -134.381, mean reward: -1.120 [-100.000, 32.787], mean action: 1.525 [0.000, 3.000],  loss: 12.854207, mse: 7190.079740, mean_q: 81.217550, mean_eps: 0.946202
 18166/100000: episode: 197, duration: 1.363s, episode steps: 173, steps per second: 127, episode reward: -357.109, mean reward: -2.064 [-100.000, 104.188], mean action: 1.578 [0.000, 3.000],  loss: 14.403160, mse: 7019.770302, mean_q: 80.790924, mean_eps: 0.945763
 18275/100000: episode: 198, duration: 0.834s, episode steps: 109, steps per second: 131, episode reward:  0.253, mean reward:  0.002 [-100.000, 95.328], mean action: 1.349 [0.000, 3.000],  loss: 18.843952, mse: 7227.170370, mean_q: 79.718640, mean_eps: 0.945340
 18398/100000: episode: 199, duration: 0.952s, episode steps: 123, steps per second: 129, episode reward: -152.299, mean reward: -1.238 [-100.000, 10.454], mean action: 1.496 [0.000, 3.000],  loss: 20.988871, mse: 7272.198373, mean_q: 81.310419, mean_eps: 0.944992
 18498/100000: episode: 200, duration: 0.789s, episode steps: 100, steps per second: 127, episode reward: -438.010, mean reward: -4.380 [-100.000,  0.705], mean action: 1.490 [0.000, 3.000],  loss: 14.109694, mse: 7235.599507, mean_q: 81.675750, mean_eps: 0.944657
 18578/100000: episode: 201, duration: 0.619s, episode steps:  80, steps per second: 129, episode reward: -86.465, mean reward: -1.081 [-100.000,  8.592], mean action: 1.512 [0.000, 3.000],  loss: 12.582508, mse: 7387.380444, mean_q: 81.331346, mean_eps: 0.944387
 18657/100000: episode: 202, duration: 0.625s, episode steps:  79, steps per second: 126, episode reward: -199.296, mean reward: -2.523 [-100.000, 24.777], mean action: 1.696 [0.000, 3.000],  loss: 14.460018, mse: 7381.717384, mean_q: 81.363712, mean_eps: 0.944149
 18763/100000: episode: 203, duration: 0.830s, episode steps: 106, steps per second: 128, episode reward: -135.025, mean reward: -1.274 [-100.000, 11.538], mean action: 1.311 [0.000, 3.000],  loss: 17.360321, mse: 7629.051228, mean_q: 83.538972, mean_eps: 0.943872
 18835/100000: episode: 204, duration: 0.551s, episode steps:  72, steps per second: 131, episode reward: -100.183, mean reward: -1.391 [-100.000, 13.364], mean action: 1.431 [0.000, 3.000],  loss: 15.883288, mse: 7635.898702, mean_q: 83.417372, mean_eps: 0.943604
 18952/100000: episode: 205, duration: 0.916s, episode steps: 117, steps per second: 128, episode reward: -145.357, mean reward: -1.242 [-100.000,  6.649], mean action: 1.564 [0.000, 3.000],  loss: 15.953243, mse: 7806.308594, mean_q: 85.475154, mean_eps: 0.943321
 19020/100000: episode: 206, duration: 0.557s, episode steps:  68, steps per second: 122, episode reward: -143.024, mean reward: -2.103 [-100.000, 16.443], mean action: 1.838 [0.000, 3.000],  loss: 12.076276, mse: 8022.187859, mean_q: 85.969292, mean_eps: 0.943044
 19097/100000: episode: 207, duration: 0.597s, episode steps:  77, steps per second: 129, episode reward: -77.472, mean reward: -1.006 [-100.000, 11.140], mean action: 1.506 [0.000, 3.000],  loss: 16.261432, mse: 7575.759575, mean_q: 83.217335, mean_eps: 0.942826
 19167/100000: episode: 208, duration: 0.526s, episode steps:  70, steps per second: 133, episode reward: -83.130, mean reward: -1.188 [-100.000, 28.907], mean action: 1.386 [0.000, 3.000],  loss: 12.908003, mse: 7666.822935, mean_q: 83.879256, mean_eps: 0.942606
 19260/100000: episode: 209, duration: 0.753s, episode steps:  93, steps per second: 123, episode reward: -75.652, mean reward: -0.813 [-100.000, 18.959], mean action: 1.462 [0.000, 3.000],  loss: 14.659090, mse: 7682.695538, mean_q: 84.042640, mean_eps: 0.942361
 19354/100000: episode: 210, duration: 0.724s, episode steps:  94, steps per second: 130, episode reward: -109.123, mean reward: -1.161 [-100.000, 11.991], mean action: 1.745 [0.000, 3.000],  loss: 12.142163, mse: 7799.659808, mean_q: 83.705179, mean_eps: 0.942081
 19450/100000: episode: 211, duration: 0.733s, episode steps:  96, steps per second: 131, episode reward: -78.931, mean reward: -0.822 [-100.000, 10.589], mean action: 1.594 [0.000, 3.000],  loss: 17.535218, mse: 7827.170141, mean_q: 83.491388, mean_eps: 0.941796
 19537/100000: episode: 212, duration: 0.705s, episode steps:  87, steps per second: 123, episode reward: -72.134, mean reward: -0.829 [-100.000, 11.921], mean action: 1.586 [0.000, 3.000],  loss: 18.692860, mse: 8000.058627, mean_q: 84.947700, mean_eps: 0.941521
 19606/100000: episode: 213, duration: 0.535s, episode steps:  69, steps per second: 129, episode reward: -114.026, mean reward: -1.653 [-100.000, 10.474], mean action: 1.406 [0.000, 3.000],  loss: 18.328590, mse: 7800.954144, mean_q: 83.355066, mean_eps: 0.941287
 19711/100000: episode: 214, duration: 0.811s, episode steps: 105, steps per second: 129, episode reward: -94.540, mean reward: -0.900 [-100.000, 17.528], mean action: 1.390 [0.000, 3.000],  loss: 12.982666, mse: 7862.067294, mean_q: 84.654735, mean_eps: 0.941026
 19799/100000: episode: 215, duration: 0.742s, episode steps:  88, steps per second: 119, episode reward: -125.787, mean reward: -1.429 [-100.000, 10.654], mean action: 1.557 [0.000, 3.000],  loss: 14.500747, mse: 8034.769054, mean_q: 85.146935, mean_eps: 0.940736
 19898/100000: episode: 216, duration: 0.768s, episode steps:  99, steps per second: 129, episode reward: -294.882, mean reward: -2.979 [-100.000, 85.964], mean action: 1.303 [0.000, 3.000],  loss: 12.237677, mse: 8112.300027, mean_q: 86.576584, mean_eps: 0.940456
 20006/100000: episode: 217, duration: 0.821s, episode steps: 108, steps per second: 132, episode reward: -428.231, mean reward: -3.965 [-100.000, 58.756], mean action: 1.463 [0.000, 3.000],  loss: 23.197874, mse: 8014.716914, mean_q: 85.014867, mean_eps: 0.940145
 20093/100000: episode: 218, duration: 0.711s, episode steps:  87, steps per second: 122, episode reward: -36.872, mean reward: -0.424 [-100.000, 10.774], mean action: 1.690 [0.000, 3.000],  loss: 18.194846, mse: 8010.194313, mean_q: 84.637937, mean_eps: 0.939853
 20215/100000: episode: 219, duration: 0.935s, episode steps: 122, steps per second: 131, episode reward: -277.226, mean reward: -2.272 [-100.000, 78.231], mean action: 1.459 [0.000, 3.000],  loss: 17.751617, mse: 8197.146480, mean_q: 86.872302, mean_eps: 0.939540
 20299/100000: episode: 220, duration: 0.630s, episode steps:  84, steps per second: 133, episode reward: -252.339, mean reward: -3.004 [-100.000, 24.481], mean action: 1.560 [0.000, 3.000],  loss: 18.615404, mse: 8305.686884, mean_q: 86.846392, mean_eps: 0.939230
 20381/100000: episode: 221, duration: 0.669s, episode steps:  82, steps per second: 123, episode reward: -50.822, mean reward: -0.620 [-100.000,  7.119], mean action: 1.549 [0.000, 3.000],  loss: 27.006918, mse: 8054.861852, mean_q: 85.809708, mean_eps: 0.938982
 20489/100000: episode: 222, duration: 0.816s, episode steps: 108, steps per second: 132, episode reward: -168.095, mean reward: -1.556 [-100.000, 25.989], mean action: 1.417 [0.000, 3.000],  loss: 15.161662, mse: 8232.308399, mean_q: 87.233868, mean_eps: 0.938696
 20592/100000: episode: 223, duration: 0.789s, episode steps: 103, steps per second: 131, episode reward: -126.664, mean reward: -1.230 [-100.000, 25.079], mean action: 1.505 [0.000, 3.000],  loss: 19.571174, mse: 8425.543372, mean_q: 88.619895, mean_eps: 0.938380
 20725/100000: episode: 224, duration: 1.061s, episode steps: 133, steps per second: 125, episode reward: -275.325, mean reward: -2.070 [-100.000,  8.545], mean action: 1.466 [0.000, 3.000],  loss: 24.004058, mse: 8517.309310, mean_q: 87.621770, mean_eps: 0.938026
 20846/100000: episode: 225, duration: 0.912s, episode steps: 121, steps per second: 133, episode reward: -92.525, mean reward: -0.765 [-100.000, 13.320], mean action: 1.364 [0.000, 3.000],  loss: 13.410559, mse: 8548.472337, mean_q: 87.618019, mean_eps: 0.937645
 20937/100000: episode: 226, duration: 0.728s, episode steps:  91, steps per second: 125, episode reward: -198.234, mean reward: -2.178 [-100.000, 62.267], mean action: 1.571 [0.000, 3.000],  loss: 16.947283, mse: 8690.883344, mean_q: 88.559828, mean_eps: 0.937327
 21047/100000: episode: 227, duration: 0.844s, episode steps: 110, steps per second: 130, episode reward: -165.928, mean reward: -1.508 [-100.000,  4.928], mean action: 1.355 [0.000, 3.000],  loss: 12.353236, mse: 8675.173531, mean_q: 88.092173, mean_eps: 0.937026
 21130/100000: episode: 228, duration: 0.631s, episode steps:  83, steps per second: 132, episode reward: -287.139, mean reward: -3.460 [-100.000,  6.260], mean action: 1.482 [0.000, 3.000],  loss: 19.539498, mse: 8477.242299, mean_q: 88.382031, mean_eps: 0.936736
 21216/100000: episode: 229, duration: 0.687s, episode steps:  86, steps per second: 125, episode reward: -128.223, mean reward: -1.491 [-100.000,  6.178], mean action: 1.698 [0.000, 3.000],  loss: 17.863047, mse: 8994.218313, mean_q: 91.829849, mean_eps: 0.936483
 21313/100000: episode: 230, duration: 0.738s, episode steps:  97, steps per second: 131, episode reward: -103.842, mean reward: -1.071 [-100.000, 17.084], mean action: 1.392 [0.000, 3.000],  loss: 19.729281, mse: 8887.323952, mean_q: 89.857357, mean_eps: 0.936208
 21389/100000: episode: 231, duration: 0.603s, episode steps:  76, steps per second: 126, episode reward: -91.065, mean reward: -1.198 [-100.000,  8.576], mean action: 1.500 [0.000, 3.000],  loss: 27.420910, mse: 8927.644827, mean_q: 90.579345, mean_eps: 0.935948
 21485/100000: episode: 232, duration: 0.762s, episode steps:  96, steps per second: 126, episode reward:  1.772, mean reward:  0.018 [-100.000, 114.645], mean action: 1.510 [0.000, 3.000],  loss: 14.842712, mse: 8838.193390, mean_q: 90.031767, mean_eps: 0.935691
 21584/100000: episode: 233, duration: 0.758s, episode steps:  99, steps per second: 131, episode reward: -396.047, mean reward: -4.000 [-100.000,  5.440], mean action: 1.323 [0.000, 3.000],  loss: 13.469143, mse: 8865.274192, mean_q: 89.006203, mean_eps: 0.935398
 21682/100000: episode: 234, duration: 0.892s, episode steps:  98, steps per second: 110, episode reward: -209.939, mean reward: -2.142 [-100.000, 32.947], mean action: 1.541 [0.000, 3.000],  loss: 14.731241, mse: 8870.395637, mean_q: 89.596259, mean_eps: 0.935103
 21767/100000: episode: 235, duration: 0.809s, episode steps:  85, steps per second: 105, episode reward: -239.579, mean reward: -2.819 [-100.000, 15.749], mean action: 1.553 [0.000, 3.000],  loss: 26.649001, mse: 8861.746180, mean_q: 89.954265, mean_eps: 0.934828
 21872/100000: episode: 236, duration: 0.925s, episode steps: 105, steps per second: 113, episode reward: -374.635, mean reward: -3.568 [-100.000,  3.980], mean action: 1.533 [0.000, 3.000],  loss: 18.117071, mse: 8695.098145, mean_q: 88.152536, mean_eps: 0.934543
 21972/100000: episode: 237, duration: 0.975s, episode steps: 100, steps per second: 103, episode reward: -241.608, mean reward: -2.416 [-100.000,  8.053], mean action: 1.630 [0.000, 3.000],  loss: 20.269441, mse: 8791.950605, mean_q: 88.973662, mean_eps: 0.934236
 22055/100000: episode: 238, duration: 0.728s, episode steps:  83, steps per second: 114, episode reward: -110.214, mean reward: -1.328 [-100.000,  8.935], mean action: 1.542 [0.000, 3.000],  loss: 27.912492, mse: 8661.357451, mean_q: 86.469937, mean_eps: 0.933961
 22131/100000: episode: 239, duration: 0.692s, episode steps:  76, steps per second: 110, episode reward: -228.893, mean reward: -3.012 [-100.000, 75.003], mean action: 1.382 [0.000, 3.000],  loss: 24.263868, mse: 8496.201345, mean_q: 85.905246, mean_eps: 0.933722
 22246/100000: episode: 240, duration: 0.926s, episode steps: 115, steps per second: 124, episode reward: -102.541, mean reward: -0.892 [-100.000, 83.580], mean action: 1.661 [0.000, 3.000],  loss: 18.631797, mse: 8752.471056, mean_q: 87.774996, mean_eps: 0.933436
 22315/100000: episode: 241, duration: 0.542s, episode steps:  69, steps per second: 127, episode reward: -238.044, mean reward: -3.450 [-100.000,  6.938], mean action: 1.580 [0.000, 3.000],  loss: 21.847951, mse: 8746.704774, mean_q: 88.289300, mean_eps: 0.933160
 22411/100000: episode: 242, duration: 0.793s, episode steps:  96, steps per second: 121, episode reward: -103.568, mean reward: -1.079 [-100.000,  6.693], mean action: 1.438 [0.000, 3.000],  loss: 21.057006, mse: 8920.087448, mean_q: 89.645597, mean_eps: 0.932912
 22483/100000: episode: 243, duration: 0.746s, episode steps:  72, steps per second:  97, episode reward: -175.992, mean reward: -2.444 [-100.000,  6.757], mean action: 1.528 [0.000, 3.000],  loss: 18.928547, mse: 8861.969564, mean_q: 88.713365, mean_eps: 0.932661
 22582/100000: episode: 244, duration: 0.850s, episode steps:  99, steps per second: 116, episode reward: -180.120, mean reward: -1.819 [-100.000,  7.303], mean action: 1.475 [0.000, 3.000],  loss: 14.088972, mse: 8722.568350, mean_q: 87.830775, mean_eps: 0.932404
 22650/100000: episode: 245, duration: 0.593s, episode steps:  68, steps per second: 115, episode reward: -79.246, mean reward: -1.165 [-100.000,  6.861], mean action: 1.338 [0.000, 3.000],  loss: 12.848314, mse: 8477.767169, mean_q: 87.310719, mean_eps: 0.932154
 22767/100000: episode: 246, duration: 1.202s, episode steps: 117, steps per second:  97, episode reward: -99.834, mean reward: -0.853 [-100.000,  7.891], mean action: 1.308 [0.000, 3.000],  loss: 16.230168, mse: 8682.954869, mean_q: 87.591461, mean_eps: 0.931876
 22866/100000: episode: 247, duration: 1.069s, episode steps:  99, steps per second:  93, episode reward: -65.526, mean reward: -0.662 [-100.000, 15.748], mean action: 1.576 [0.000, 3.000],  loss: 18.666348, mse: 8756.912612, mean_q: 86.913058, mean_eps: 0.931552
 22939/100000: episode: 248, duration: 0.859s, episode steps:  73, steps per second:  85, episode reward: -176.024, mean reward: -2.411 [-100.000, 30.619], mean action: 1.370 [0.000, 3.000],  loss: 15.766377, mse: 8737.167681, mean_q: 87.805998, mean_eps: 0.931294
 23014/100000: episode: 249, duration: 0.673s, episode steps:  75, steps per second: 111, episode reward: -65.483, mean reward: -0.873 [-100.000,  7.974], mean action: 1.560 [0.000, 3.000],  loss: 23.718100, mse: 8685.653919, mean_q: 86.455263, mean_eps: 0.931072
 23111/100000: episode: 250, duration: 0.715s, episode steps:  97, steps per second: 136, episode reward: -95.382, mean reward: -0.983 [-100.000, 11.387], mean action: 1.773 [0.000, 3.000],  loss: 15.285009, mse: 8553.745611, mean_q: 86.317038, mean_eps: 0.930814
 23226/100000: episode: 251, duration: 1.101s, episode steps: 115, steps per second: 104, episode reward: -319.926, mean reward: -2.782 [-100.000, 11.737], mean action: 1.296 [0.000, 3.000],  loss: 20.714393, mse: 8559.780503, mean_q: 85.644368, mean_eps: 0.930496
 23297/100000: episode: 252, duration: 0.699s, episode steps:  71, steps per second: 102, episode reward: -61.764, mean reward: -0.870 [-100.000, 11.054], mean action: 1.690 [0.000, 3.000],  loss: 20.845934, mse: 8388.868405, mean_q: 83.982149, mean_eps: 0.930217
 23356/100000: episode: 253, duration: 0.601s, episode steps:  59, steps per second:  98, episode reward: -153.155, mean reward: -2.596 [-100.000, 83.151], mean action: 1.373 [0.000, 3.000],  loss: 16.299613, mse: 8629.694096, mean_q: 85.803222, mean_eps: 0.930022
 23438/100000: episode: 254, duration: 0.646s, episode steps:  82, steps per second: 127, episode reward: -280.454, mean reward: -3.420 [-100.000, 21.036], mean action: 1.671 [0.000, 3.000],  loss: 18.022425, mse: 8848.073433, mean_q: 87.143224, mean_eps: 0.929810
 23504/100000: episode: 255, duration: 0.490s, episode steps:  66, steps per second: 135, episode reward: -174.388, mean reward: -2.642 [-100.000,  9.824], mean action: 1.576 [0.000, 3.000],  loss: 17.994585, mse: 8594.526944, mean_q: 84.130354, mean_eps: 0.929588
 23619/100000: episode: 256, duration: 0.824s, episode steps: 115, steps per second: 140, episode reward: -81.992, mean reward: -0.713 [-100.000,  9.032], mean action: 1.557 [0.000, 3.000],  loss: 24.876454, mse: 8536.328910, mean_q: 84.219370, mean_eps: 0.929317
 23740/100000: episode: 257, duration: 0.994s, episode steps: 121, steps per second: 122, episode reward:  3.788, mean reward:  0.031 [-100.000, 102.791], mean action: 1.438 [0.000, 3.000],  loss: 19.738655, mse: 8469.001525, mean_q: 84.371800, mean_eps: 0.928963
 23817/100000: episode: 258, duration: 0.701s, episode steps:  77, steps per second: 110, episode reward: -126.957, mean reward: -1.649 [-100.000, 13.202], mean action: 1.805 [0.000, 3.000],  loss: 12.599331, mse: 8514.264350, mean_q: 85.093818, mean_eps: 0.928666
 23931/100000: episode: 259, duration: 0.866s, episode steps: 114, steps per second: 132, episode reward: -294.559, mean reward: -2.584 [-100.000,  3.580], mean action: 1.579 [0.000, 3.000],  loss: 18.220098, mse: 8673.536604, mean_q: 84.278408, mean_eps: 0.928380
 24073/100000: episode: 260, duration: 1.011s, episode steps: 142, steps per second: 140, episode reward: -20.094, mean reward: -0.142 [-100.000, 81.406], mean action: 1.500 [0.000, 3.000],  loss: 23.417041, mse: 8631.835621, mean_q: 84.959560, mean_eps: 0.927995
 24181/100000: episode: 261, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: -294.074, mean reward: -2.723 [-100.000,  3.907], mean action: 1.787 [0.000, 3.000],  loss: 26.277584, mse: 8655.529464, mean_q: 84.267468, mean_eps: 0.927620
 24255/100000: episode: 262, duration: 0.575s, episode steps:  74, steps per second: 129, episode reward: -75.524, mean reward: -1.021 [-100.000,  8.541], mean action: 1.514 [0.000, 3.000],  loss: 25.302834, mse: 8242.409087, mean_q: 83.123294, mean_eps: 0.927347
 24327/100000: episode: 263, duration: 0.512s, episode steps:  72, steps per second: 141, episode reward: -134.537, mean reward: -1.869 [-100.000, 18.481], mean action: 1.597 [0.000, 3.000],  loss: 19.076489, mse: 8134.600410, mean_q: 80.997544, mean_eps: 0.927129
 24392/100000: episode: 264, duration: 0.451s, episode steps:  65, steps per second: 144, episode reward: -90.521, mean reward: -1.393 [-100.000, 11.294], mean action: 1.815 [0.000, 3.000],  loss: 18.354067, mse: 8083.446334, mean_q: 81.017650, mean_eps: 0.926923
 24496/100000: episode: 265, duration: 0.724s, episode steps: 104, steps per second: 144, episode reward: -100.450, mean reward: -0.966 [-100.000, 18.268], mean action: 1.510 [0.000, 3.000],  loss: 25.663959, mse: 8108.790992, mean_q: 80.668492, mean_eps: 0.926670
 24624/100000: episode: 266, duration: 0.920s, episode steps: 128, steps per second: 139, episode reward: -214.931, mean reward: -1.679 [-100.000, 65.381], mean action: 1.609 [0.000, 3.000],  loss: 26.025465, mse: 8236.384262, mean_q: 80.787019, mean_eps: 0.926322
 24695/100000: episode: 267, duration: 0.490s, episode steps:  71, steps per second: 145, episode reward: -150.082, mean reward: -2.114 [-100.000,  4.273], mean action: 1.732 [0.000, 3.000],  loss: 27.639898, mse: 8194.396876, mean_q: 81.363197, mean_eps: 0.926023
 24767/100000: episode: 268, duration: 0.485s, episode steps:  72, steps per second: 148, episode reward: -144.899, mean reward: -2.012 [-100.000,  8.911], mean action: 1.611 [0.000, 3.000],  loss: 21.164991, mse: 8189.046034, mean_q: 80.735474, mean_eps: 0.925809
 24861/100000: episode: 269, duration: 0.673s, episode steps:  94, steps per second: 140, episode reward: -197.227, mean reward: -2.098 [-100.000,  7.156], mean action: 1.468 [0.000, 3.000],  loss: 19.523766, mse: 8224.467493, mean_q: 80.617970, mean_eps: 0.925559
 24945/100000: episode: 270, duration: 0.577s, episode steps:  84, steps per second: 146, episode reward: -73.034, mean reward: -0.869 [-100.000, 10.348], mean action: 1.512 [0.000, 3.000],  loss: 25.456933, mse: 8388.067900, mean_q: 81.812474, mean_eps: 0.925292
 25039/100000: episode: 271, duration: 0.636s, episode steps:  94, steps per second: 148, episode reward: -103.699, mean reward: -1.103 [-100.000,  9.546], mean action: 1.543 [0.000, 3.000],  loss: 18.618557, mse: 8381.056173, mean_q: 81.968755, mean_eps: 0.925026
 25181/100000: episode: 272, duration: 1.008s, episode steps: 142, steps per second: 141, episode reward: -126.821, mean reward: -0.893 [-100.000,  6.317], mean action: 1.317 [0.000, 3.000],  loss: 27.056075, mse: 8244.065385, mean_q: 82.422886, mean_eps: 0.924671
 25272/100000: episode: 273, duration: 0.644s, episode steps:  91, steps per second: 141, episode reward: -86.051, mean reward: -0.946 [-100.000,  8.823], mean action: 1.527 [0.000, 3.000],  loss: 20.145979, mse: 8248.738759, mean_q: 81.354586, mean_eps: 0.924322
 25385/100000: episode: 274, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -154.311, mean reward: -1.366 [-100.000,  6.884], mean action: 1.540 [0.000, 3.000],  loss: 22.011726, mse: 8455.529530, mean_q: 82.516122, mean_eps: 0.924016
 25514/100000: episode: 275, duration: 0.959s, episode steps: 129, steps per second: 134, episode reward: -178.510, mean reward: -1.384 [-100.000, 15.472], mean action: 1.605 [0.000, 3.000],  loss: 19.692661, mse: 8287.817004, mean_q: 81.022795, mean_eps: 0.923653
 25591/100000: episode: 276, duration: 0.536s, episode steps:  77, steps per second: 144, episode reward: -148.534, mean reward: -1.929 [-100.000,  7.824], mean action: 1.545 [0.000, 3.000],  loss: 19.230386, mse: 8435.002854, mean_q: 81.531388, mean_eps: 0.923344
 25684/100000: episode: 277, duration: 0.627s, episode steps:  93, steps per second: 148, episode reward: -127.981, mean reward: -1.376 [-100.000,  7.996], mean action: 1.452 [0.000, 3.000],  loss: 22.249209, mse: 8765.190419, mean_q: 82.237129, mean_eps: 0.923089
 25761/100000: episode: 278, duration: 0.589s, episode steps:  77, steps per second: 131, episode reward: -114.075, mean reward: -1.481 [-100.000, 34.405], mean action: 1.377 [0.000, 3.000],  loss: 29.636311, mse: 8800.144937, mean_q: 83.380794, mean_eps: 0.922834
 25856/100000: episode: 279, duration: 0.666s, episode steps:  95, steps per second: 143, episode reward: -303.998, mean reward: -3.200 [-100.000,  0.488], mean action: 1.568 [0.000, 3.000],  loss: 25.727684, mse: 8995.616638, mean_q: 84.530625, mean_eps: 0.922576
 25924/100000: episode: 280, duration: 0.461s, episode steps:  68, steps per second: 147, episode reward: -82.613, mean reward: -1.215 [-100.000,  7.563], mean action: 1.471 [0.000, 3.000],  loss: 34.193075, mse: 8889.808493, mean_q: 82.832848, mean_eps: 0.922332
 26023/100000: episode: 281, duration: 0.705s, episode steps:  99, steps per second: 140, episode reward: -167.987, mean reward: -1.697 [-100.000, 29.688], mean action: 1.424 [0.000, 3.000],  loss: 21.814644, mse: 8957.036749, mean_q: 85.534599, mean_eps: 0.922081
 26131/100000: episode: 282, duration: 0.796s, episode steps: 108, steps per second: 136, episode reward: -137.951, mean reward: -1.277 [-100.000,  8.603], mean action: 1.713 [0.000, 3.000],  loss: 16.838934, mse: 8972.528180, mean_q: 83.891185, mean_eps: 0.921771
 26259/100000: episode: 283, duration: 0.892s, episode steps: 128, steps per second: 143, episode reward: -98.272, mean reward: -0.768 [-100.000, 32.090], mean action: 1.594 [0.000, 3.000],  loss: 33.472573, mse: 8962.889271, mean_q: 83.142185, mean_eps: 0.921417
 26349/100000: episode: 284, duration: 0.703s, episode steps:  90, steps per second: 128, episode reward: -209.637, mean reward: -2.329 [-100.000, 31.177], mean action: 1.622 [0.000, 3.000],  loss: 21.892220, mse: 9079.697222, mean_q: 82.794949, mean_eps: 0.921090
 26432/100000: episode: 285, duration: 0.613s, episode steps:  83, steps per second: 135, episode reward: -122.169, mean reward: -1.472 [-100.000, 13.934], mean action: 1.361 [0.000, 3.000],  loss: 39.881535, mse: 8916.527350, mean_q: 82.387854, mean_eps: 0.920830
 26548/100000: episode: 286, duration: 0.817s, episode steps: 116, steps per second: 142, episode reward: -184.033, mean reward: -1.586 [-100.000,  4.197], mean action: 1.629 [0.000, 3.000],  loss: 25.458468, mse: 8922.843607, mean_q: 81.762617, mean_eps: 0.920532
 26620/100000: episode: 287, duration: 0.523s, episode steps:  72, steps per second: 138, episode reward: -56.824, mean reward: -0.789 [-100.000, 17.825], mean action: 1.556 [0.000, 3.000],  loss: 20.871097, mse: 8908.400092, mean_q: 82.601656, mean_eps: 0.920250
 26702/100000: episode: 288, duration: 0.598s, episode steps:  82, steps per second: 137, episode reward: -180.600, mean reward: -2.202 [-100.000, 13.064], mean action: 1.354 [0.000, 3.000],  loss: 21.771546, mse: 8926.623476, mean_q: 82.007434, mean_eps: 0.920019
 26786/100000: episode: 289, duration: 0.572s, episode steps:  84, steps per second: 147, episode reward: -111.748, mean reward: -1.330 [-100.000,  6.570], mean action: 1.607 [0.000, 3.000],  loss: 32.766968, mse: 9243.188000, mean_q: 84.010544, mean_eps: 0.919769
 26879/100000: episode: 290, duration: 0.646s, episode steps:  93, steps per second: 144, episode reward: -100.832, mean reward: -1.084 [-100.000,  6.775], mean action: 1.505 [0.000, 3.000],  loss: 26.266610, mse: 9011.738371, mean_q: 82.833366, mean_eps: 0.919504
 26990/100000: episode: 291, duration: 0.866s, episode steps: 111, steps per second: 128, episode reward: -171.609, mean reward: -1.546 [-100.000,  8.156], mean action: 1.640 [0.000, 3.000],  loss: 28.653348, mse: 8919.470426, mean_q: 82.118987, mean_eps: 0.919198
 27077/100000: episode: 292, duration: 0.713s, episode steps:  87, steps per second: 122, episode reward: -105.959, mean reward: -1.218 [-100.000,  8.484], mean action: 1.506 [0.000, 3.000],  loss: 25.376548, mse: 9298.057499, mean_q: 83.719293, mean_eps: 0.918901
 27161/100000: episode: 293, duration: 0.687s, episode steps:  84, steps per second: 122, episode reward: -144.291, mean reward: -1.718 [-100.000,  7.509], mean action: 1.571 [0.000, 3.000],  loss: 17.679144, mse: 9015.180443, mean_q: 83.364837, mean_eps: 0.918645
 27286/100000: episode: 294, duration: 0.936s, episode steps: 125, steps per second: 133, episode reward: -51.951, mean reward: -0.416 [-100.000, 13.308], mean action: 1.544 [0.000, 3.000],  loss: 23.557958, mse: 9314.077117, mean_q: 84.781146, mean_eps: 0.918331
 27381/100000: episode: 295, duration: 0.639s, episode steps:  95, steps per second: 149, episode reward: -127.692, mean reward: -1.344 [-100.000,  6.504], mean action: 1.495 [0.000, 3.000],  loss: 21.671501, mse: 9537.387058, mean_q: 86.288934, mean_eps: 0.918001
 27492/100000: episode: 296, duration: 0.796s, episode steps: 111, steps per second: 140, episode reward: -345.569, mean reward: -3.113 [-100.000,  7.697], mean action: 1.541 [0.000, 3.000],  loss: 21.466925, mse: 9484.094370, mean_q: 86.530790, mean_eps: 0.917692
 27593/100000: episode: 297, duration: 0.756s, episode steps: 101, steps per second: 134, episode reward: -90.392, mean reward: -0.895 [-100.000, 21.186], mean action: 1.614 [0.000, 3.000],  loss: 21.418796, mse: 9237.098270, mean_q: 82.423180, mean_eps: 0.917374
 27659/100000: episode: 298, duration: 0.458s, episode steps:  66, steps per second: 144, episode reward: -79.073, mean reward: -1.198 [-100.000, 12.351], mean action: 1.439 [0.000, 3.000],  loss: 36.685866, mse: 9476.656871, mean_q: 83.170089, mean_eps: 0.917123
 27759/100000: episode: 299, duration: 0.697s, episode steps: 100, steps per second: 144, episode reward: -316.930, mean reward: -3.169 [-100.000,  3.905], mean action: 1.560 [0.000, 3.000],  loss: 29.449112, mse: 9389.523296, mean_q: 83.032311, mean_eps: 0.916875
 27851/100000: episode: 300, duration: 0.701s, episode steps:  92, steps per second: 131, episode reward: -57.347, mean reward: -0.623 [-100.000,  9.482], mean action: 1.522 [0.000, 3.000],  loss: 16.485233, mse: 9467.695886, mean_q: 83.486592, mean_eps: 0.916587
 27915/100000: episode: 301, duration: 0.492s, episode steps:  64, steps per second: 130, episode reward: -159.765, mean reward: -2.496 [-100.000, 92.432], mean action: 1.625 [0.000, 3.000],  loss: 32.619428, mse: 9638.934349, mean_q: 85.502794, mean_eps: 0.916353
 28010/100000: episode: 302, duration: 0.701s, episode steps:  95, steps per second: 136, episode reward: -246.689, mean reward: -2.597 [-100.000,  1.160], mean action: 1.463 [0.000, 3.000],  loss: 24.474785, mse: 9447.007607, mean_q: 82.921364, mean_eps: 0.916114
 28095/100000: episode: 303, duration: 0.719s, episode steps:  85, steps per second: 118, episode reward: -80.324, mean reward: -0.945 [-100.000, 12.190], mean action: 1.447 [0.000, 3.000],  loss: 19.270025, mse: 9660.982652, mean_q: 85.602556, mean_eps: 0.915844
 28203/100000: episode: 304, duration: 0.816s, episode steps: 108, steps per second: 132, episode reward: -350.346, mean reward: -3.244 [-100.000,  0.716], mean action: 1.463 [0.000, 3.000],  loss: 25.917832, mse: 9454.933684, mean_q: 84.089306, mean_eps: 0.915554
 28287/100000: episode: 305, duration: 0.602s, episode steps:  84, steps per second: 140, episode reward: -72.087, mean reward: -0.858 [-100.000,  8.082], mean action: 1.607 [0.000, 3.000],  loss: 23.248491, mse: 9290.047189, mean_q: 83.445792, mean_eps: 0.915266
 28391/100000: episode: 306, duration: 0.759s, episode steps: 104, steps per second: 137, episode reward: -180.908, mean reward: -1.740 [-100.000,  4.188], mean action: 1.394 [0.000, 3.000],  loss: 20.802120, mse: 9512.794715, mean_q: 85.675481, mean_eps: 0.914984
 28480/100000: episode: 307, duration: 0.638s, episode steps:  89, steps per second: 139, episode reward: -299.969, mean reward: -3.370 [-100.000,  3.938], mean action: 1.337 [0.000, 3.000],  loss: 22.390121, mse: 9628.512514, mean_q: 85.553607, mean_eps: 0.914695
 28586/100000: episode: 308, duration: 0.782s, episode steps: 106, steps per second: 135, episode reward: -173.622, mean reward: -1.638 [-100.000,  9.034], mean action: 1.604 [0.000, 3.000],  loss: 30.316657, mse: 9800.845832, mean_q: 86.978125, mean_eps: 0.914402
 28699/100000: episode: 309, duration: 0.856s, episode steps: 113, steps per second: 132, episode reward: -144.311, mean reward: -1.277 [-100.000,  4.980], mean action: 1.363 [0.000, 3.000],  loss: 26.772376, mse: 9705.626862, mean_q: 87.453760, mean_eps: 0.914074
 28771/100000: episode: 310, duration: 0.693s, episode steps:  72, steps per second: 104, episode reward: -102.242, mean reward: -1.420 [-100.000, 10.051], mean action: 1.472 [0.000, 3.000],  loss: 33.423785, mse: 9735.940884, mean_q: 87.706674, mean_eps: 0.913796
 28837/100000: episode: 311, duration: 0.580s, episode steps:  66, steps per second: 114, episode reward: -182.855, mean reward: -2.771 [-100.000,  6.099], mean action: 1.621 [0.000, 3.000],  loss: 27.679842, mse: 10014.713919, mean_q: 89.553771, mean_eps: 0.913589
 28909/100000: episode: 312, duration: 0.552s, episode steps:  72, steps per second: 131, episode reward: -28.891, mean reward: -0.401 [-100.000, 20.264], mean action: 1.444 [0.000, 3.000],  loss: 25.099320, mse: 9668.176093, mean_q: 86.765640, mean_eps: 0.913382
 28982/100000: episode: 313, duration: 0.570s, episode steps:  73, steps per second: 128, episode reward: -75.053, mean reward: -1.028 [-100.000, 10.934], mean action: 1.452 [0.000, 3.000],  loss: 26.232371, mse: 10273.935005, mean_q: 89.467855, mean_eps: 0.913165
 29070/100000: episode: 314, duration: 0.734s, episode steps:  88, steps per second: 120, episode reward: -108.398, mean reward: -1.232 [-100.000,  7.269], mean action: 1.443 [0.000, 3.000],  loss: 18.727793, mse: 9923.188182, mean_q: 88.979425, mean_eps: 0.912923
 29152/100000: episode: 315, duration: 0.721s, episode steps:  82, steps per second: 114, episode reward: -209.043, mean reward: -2.549 [-100.000, 16.711], mean action: 1.390 [0.000, 3.000],  loss: 24.718642, mse: 9793.933838, mean_q: 87.239987, mean_eps: 0.912668
 29272/100000: episode: 316, duration: 0.925s, episode steps: 120, steps per second: 130, episode reward: -131.104, mean reward: -1.093 [-100.000, 46.824], mean action: 1.767 [0.000, 3.000],  loss: 20.183323, mse: 10111.808923, mean_q: 88.959163, mean_eps: 0.912365
 29366/100000: episode: 317, duration: 0.671s, episode steps:  94, steps per second: 140, episode reward: -360.765, mean reward: -3.838 [-100.000,  0.518], mean action: 1.649 [0.000, 3.000],  loss: 36.947728, mse: 10196.952948, mean_q: 89.640135, mean_eps: 0.912044
 29439/100000: episode: 318, duration: 0.535s, episode steps:  73, steps per second: 136, episode reward: -105.477, mean reward: -1.445 [-100.000, 18.880], mean action: 1.479 [0.000, 3.000],  loss: 16.928634, mse: 10212.470904, mean_q: 87.187179, mean_eps: 0.911794
 29520/100000: episode: 319, duration: 0.592s, episode steps:  81, steps per second: 137, episode reward: -54.563, mean reward: -0.674 [-100.000, 20.053], mean action: 1.531 [0.000, 3.000],  loss: 28.181593, mse: 10164.608591, mean_q: 88.577991, mean_eps: 0.911563
 29613/100000: episode: 320, duration: 0.639s, episode steps:  93, steps per second: 146, episode reward: -243.899, mean reward: -2.623 [-100.000, 15.041], mean action: 1.570 [0.000, 3.000],  loss: 23.772249, mse: 9978.569357, mean_q: 86.006789, mean_eps: 0.911302
 29739/100000: episode: 321, duration: 1.064s, episode steps: 126, steps per second: 118, episode reward: -420.552, mean reward: -3.338 [-100.000, 50.951], mean action: 1.532 [0.000, 3.000],  loss: 36.231523, mse: 9903.670406, mean_q: 85.205426, mean_eps: 0.910973
 29820/100000: episode: 322, duration: 0.625s, episode steps:  81, steps per second: 130, episode reward: -64.336, mean reward: -0.794 [-100.000, 23.182], mean action: 1.309 [0.000, 3.000],  loss: 34.875677, mse: 10321.047996, mean_q: 89.537921, mean_eps: 0.910663
 29933/100000: episode: 323, duration: 0.795s, episode steps: 113, steps per second: 142, episode reward: -134.480, mean reward: -1.190 [-100.000,  6.657], mean action: 1.513 [0.000, 3.000],  loss: 24.204905, mse: 10050.222894, mean_q: 85.225425, mean_eps: 0.910372
 30014/100000: episode: 324, duration: 0.657s, episode steps:  81, steps per second: 123, episode reward: -96.377, mean reward: -1.190 [-100.000,  7.131], mean action: 1.469 [0.000, 3.000],  loss: 20.298721, mse: 10113.236394, mean_q: 87.326664, mean_eps: 0.910081
 30090/100000: episode: 325, duration: 0.642s, episode steps:  76, steps per second: 118, episode reward: -109.230, mean reward: -1.437 [-100.000,  6.932], mean action: 1.382 [0.000, 3.000],  loss: 30.711258, mse: 10066.488178, mean_q: 88.191801, mean_eps: 0.909845
 30205/100000: episode: 326, duration: 0.921s, episode steps: 115, steps per second: 125, episode reward: -337.401, mean reward: -2.934 [-100.000, 73.336], mean action: 1.435 [0.000, 3.000],  loss: 15.131860, mse: 10398.879726, mean_q: 89.301921, mean_eps: 0.909559
 30269/100000: episode: 327, duration: 0.551s, episode steps:  64, steps per second: 116, episode reward: -57.346, mean reward: -0.896 [-100.000, 11.877], mean action: 1.391 [0.000, 3.000],  loss: 20.469239, mse: 10313.070442, mean_q: 87.824296, mean_eps: 0.909291
 30349/100000: episode: 328, duration: 0.777s, episode steps:  80, steps per second: 103, episode reward: -82.711, mean reward: -1.034 [-100.000, 24.449], mean action: 1.700 [0.000, 3.000],  loss: 20.805283, mse: 10430.389319, mean_q: 88.442262, mean_eps: 0.909074
 30429/100000: episode: 329, duration: 0.687s, episode steps:  80, steps per second: 116, episode reward: -93.594, mean reward: -1.170 [-100.000, 10.933], mean action: 1.587 [0.000, 3.000],  loss: 29.370997, mse: 10617.845483, mean_q: 90.901287, mean_eps: 0.908834
 30548/100000: episode: 330, duration: 1.267s, episode steps: 119, steps per second:  94, episode reward: -86.936, mean reward: -0.731 [-100.000,  9.801], mean action: 1.588 [0.000, 3.000],  loss: 20.340862, mse: 10774.122751, mean_q: 89.809041, mean_eps: 0.908536
 30651/100000: episode: 331, duration: 1.175s, episode steps: 103, steps per second:  88, episode reward: -156.638, mean reward: -1.521 [-100.000,  6.956], mean action: 1.573 [0.000, 3.000],  loss: 28.243162, mse: 11215.496563, mean_q: 93.444103, mean_eps: 0.908203
 30744/100000: episode: 332, duration: 0.930s, episode steps:  93, steps per second: 100, episode reward: -213.633, mean reward: -2.297 [-100.000, 28.963], mean action: 1.505 [0.000, 3.000],  loss: 31.017683, mse: 11198.787340, mean_q: 91.958025, mean_eps: 0.907909
 30831/100000: episode: 333, duration: 0.732s, episode steps:  87, steps per second: 119, episode reward: -94.129, mean reward: -1.082 [-100.000,  6.568], mean action: 1.506 [0.000, 3.000],  loss: 37.880996, mse: 11495.071648, mean_q: 93.298880, mean_eps: 0.907639
 30952/100000: episode: 334, duration: 0.829s, episode steps: 121, steps per second: 146, episode reward: -225.469, mean reward: -1.863 [-100.000, 70.276], mean action: 1.579 [0.000, 3.000],  loss: 30.426846, mse: 11453.121699, mean_q: 91.659568, mean_eps: 0.907327
 31025/100000: episode: 335, duration: 0.533s, episode steps:  73, steps per second: 137, episode reward: -70.603, mean reward: -0.967 [-100.000,  5.875], mean action: 1.479 [0.000, 3.000],  loss: 24.513135, mse: 11530.599864, mean_q: 92.498287, mean_eps: 0.907036
 31097/100000: episode: 336, duration: 0.519s, episode steps:  72, steps per second: 139, episode reward: -88.729, mean reward: -1.232 [-100.000,  9.927], mean action: 1.278 [0.000, 3.000],  loss: 33.652764, mse: 11211.549805, mean_q: 89.511730, mean_eps: 0.906818
 31173/100000: episode: 337, duration: 0.579s, episode steps:  76, steps per second: 131, episode reward: -115.454, mean reward: -1.519 [-100.000, 41.409], mean action: 1.500 [0.000, 3.000],  loss: 35.615027, mse: 12019.880885, mean_q: 94.652294, mean_eps: 0.906597
 31287/100000: episode: 338, duration: 0.992s, episode steps: 114, steps per second: 115, episode reward: -55.081, mean reward: -0.483 [-100.000, 15.272], mean action: 1.623 [0.000, 3.000],  loss: 24.896064, mse: 11903.487750, mean_q: 94.108074, mean_eps: 0.906311
 31363/100000: episode: 339, duration: 0.689s, episode steps:  76, steps per second: 110, episode reward: -76.170, mean reward: -1.002 [-100.000,  8.476], mean action: 1.500 [0.000, 3.000],  loss: 31.938893, mse: 11640.207577, mean_q: 92.124227, mean_eps: 0.906026
 31429/100000: episode: 340, duration: 0.538s, episode steps:  66, steps per second: 123, episode reward: -68.293, mean reward: -1.035 [-100.000, 13.118], mean action: 1.652 [0.000, 3.000],  loss: 30.445607, mse: 11655.994880, mean_q: 91.900076, mean_eps: 0.905813
 31544/100000: episode: 341, duration: 0.947s, episode steps: 115, steps per second: 121, episode reward: -40.214, mean reward: -0.350 [-100.000, 69.393], mean action: 1.565 [0.000, 3.000],  loss: 26.352810, mse: 11809.434842, mean_q: 94.047663, mean_eps: 0.905542
 31658/100000: episode: 342, duration: 0.859s, episode steps: 114, steps per second: 133, episode reward: -88.762, mean reward: -0.779 [-100.000,  7.864], mean action: 1.482 [0.000, 3.000],  loss: 28.749679, mse: 11942.781293, mean_q: 93.270776, mean_eps: 0.905199
 31774/100000: episode: 343, duration: 0.833s, episode steps: 116, steps per second: 139, episode reward: -352.708, mean reward: -3.041 [-100.000, 85.771], mean action: 1.440 [0.000, 3.000],  loss: 27.605677, mse: 12124.917161, mean_q: 95.639779, mean_eps: 0.904853
 31881/100000: episode: 344, duration: 0.786s, episode steps: 107, steps per second: 136, episode reward: -124.962, mean reward: -1.168 [-100.000,  9.997], mean action: 1.607 [0.000, 3.000],  loss: 22.682584, mse: 12408.657500, mean_q: 97.306364, mean_eps: 0.904519
 31956/100000: episode: 345, duration: 0.516s, episode steps:  75, steps per second: 145, episode reward: -216.297, mean reward: -2.884 [-100.000,  6.708], mean action: 1.507 [0.000, 3.000],  loss: 21.127492, mse: 11706.389935, mean_q: 93.247500, mean_eps: 0.904246
 32036/100000: episode: 346, duration: 0.549s, episode steps:  80, steps per second: 146, episode reward: -29.206, mean reward: -0.365 [-100.000, 15.072], mean action: 1.613 [0.000, 3.000],  loss: 29.580106, mse: 12177.045227, mean_q: 95.200033, mean_eps: 0.904013
 32128/100000: episode: 347, duration: 0.637s, episode steps:  92, steps per second: 144, episode reward: -126.571, mean reward: -1.376 [-100.000,  8.802], mean action: 1.424 [0.000, 3.000],  loss: 23.734154, mse: 11914.052405, mean_q: 93.170272, mean_eps: 0.903756
 32224/100000: episode: 348, duration: 0.721s, episode steps:  96, steps per second: 133, episode reward: -279.313, mean reward: -2.910 [-100.000,  5.943], mean action: 1.417 [0.000, 3.000],  loss: 24.024196, mse: 12130.187337, mean_q: 96.091302, mean_eps: 0.903473
 32326/100000: episode: 349, duration: 0.757s, episode steps: 102, steps per second: 135, episode reward: -122.253, mean reward: -1.199 [-100.000, 29.820], mean action: 1.549 [0.000, 3.000],  loss: 18.104863, mse: 12506.758401, mean_q: 95.638077, mean_eps: 0.903177
 32433/100000: episode: 350, duration: 0.783s, episode steps: 107, steps per second: 137, episode reward: -109.180, mean reward: -1.020 [-100.000,  6.770], mean action: 1.523 [0.000, 3.000],  loss: 24.505630, mse: 12429.948452, mean_q: 94.172501, mean_eps: 0.902863
 32539/100000: episode: 351, duration: 0.776s, episode steps: 106, steps per second: 137, episode reward: -46.815, mean reward: -0.442 [-100.000, 15.015], mean action: 1.509 [0.000, 3.000],  loss: 22.851229, mse: 12630.468787, mean_q: 93.986922, mean_eps: 0.902544
 32657/100000: episode: 352, duration: 0.819s, episode steps: 118, steps per second: 144, episode reward: -146.225, mean reward: -1.239 [-100.000,  6.005], mean action: 1.746 [0.000, 3.000],  loss: 24.243208, mse: 13028.149679, mean_q: 98.276428, mean_eps: 0.902208
 32748/100000: episode: 353, duration: 0.667s, episode steps:  91, steps per second: 136, episode reward: -89.684, mean reward: -0.986 [-100.000, 10.430], mean action: 1.495 [0.000, 3.000],  loss: 18.791495, mse: 12727.551575, mean_q: 94.446281, mean_eps: 0.901894
 32854/100000: episode: 354, duration: 0.798s, episode steps: 106, steps per second: 133, episode reward: -132.844, mean reward: -1.253 [-100.000, 24.891], mean action: 1.387 [0.000, 3.000],  loss: 20.312271, mse: 13066.107053, mean_q: 97.991289, mean_eps: 0.901599
 32966/100000: episode: 355, duration: 0.805s, episode steps: 112, steps per second: 139, episode reward: -295.220, mean reward: -2.636 [-100.000,  6.909], mean action: 1.670 [0.000, 3.000],  loss: 31.073426, mse: 13419.090341, mean_q: 100.095947, mean_eps: 0.901272
 33079/100000: episode: 356, duration: 0.819s, episode steps: 113, steps per second: 138, episode reward: -356.706, mean reward: -3.157 [-100.000, 64.437], mean action: 1.372 [0.000, 3.000],  loss: 28.270313, mse: 13455.134429, mean_q: 99.546208, mean_eps: 0.900934
 33173/100000: episode: 357, duration: 0.658s, episode steps:  94, steps per second: 143, episode reward: -451.195, mean reward: -4.800 [-100.000,  0.731], mean action: 1.638 [0.000, 3.000],  loss: 20.215972, mse: 13570.756857, mean_q: 100.204979, mean_eps: 0.900624
 33225/100000: episode: 358, duration: 0.374s, episode steps:  52, steps per second: 139, episode reward: -146.622, mean reward: -2.820 [-100.000,  8.532], mean action: 1.481 [0.000, 3.000],  loss: 21.284605, mse: 13469.422326, mean_q: 100.393496, mean_eps: 0.900405
 33336/100000: episode: 359, duration: 0.808s, episode steps: 111, steps per second: 137, episode reward: -88.154, mean reward: -0.794 [-100.000, 21.362], mean action: 1.495 [0.000, 3.000],  loss: 30.596417, mse: 13220.440782, mean_q: 97.922697, mean_eps: 0.900160
 33411/100000: episode: 360, duration: 0.627s, episode steps:  75, steps per second: 120, episode reward: -140.527, mean reward: -1.874 [-100.000, 14.166], mean action: 1.440 [0.000, 3.000],  loss: 25.705372, mse: 13714.317943, mean_q: 101.447158, mean_eps: 0.899881
 33506/100000: episode: 361, duration: 0.672s, episode steps:  95, steps per second: 141, episode reward: -472.723, mean reward: -4.976 [-100.000,  0.890], mean action: 1.600 [0.000, 3.000],  loss: 23.152467, mse: 13665.672780, mean_q: 100.182887, mean_eps: 0.899626
 33619/100000: episode: 362, duration: 0.765s, episode steps: 113, steps per second: 148, episode reward: -341.953, mean reward: -3.026 [-100.000,  2.059], mean action: 1.301 [0.000, 3.000],  loss: 20.281781, mse: 13805.380963, mean_q: 100.725596, mean_eps: 0.899314
 33696/100000: episode: 363, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -129.213, mean reward: -1.678 [-100.000, 10.751], mean action: 1.675 [0.000, 3.000],  loss: 20.220296, mse: 13753.076197, mean_q: 99.668640, mean_eps: 0.899029
 33838/100000: episode: 364, duration: 0.959s, episode steps: 142, steps per second: 148, episode reward: -141.123, mean reward: -0.994 [-100.000, 35.134], mean action: 1.521 [0.000, 3.000],  loss: 27.071367, mse: 13992.075704, mean_q: 101.732556, mean_eps: 0.898701
 33956/100000: episode: 365, duration: 0.807s, episode steps: 118, steps per second: 146, episode reward: -143.299, mean reward: -1.214 [-100.000, 112.191], mean action: 1.458 [0.000, 3.000],  loss: 22.983199, mse: 13964.912482, mean_q: 101.251316, mean_eps: 0.898310
 34033/100000: episode: 366, duration: 0.532s, episode steps:  77, steps per second: 145, episode reward: -151.819, mean reward: -1.972 [-100.000, 31.714], mean action: 1.468 [0.000, 3.000],  loss: 22.943282, mse: 13861.630897, mean_q: 99.479073, mean_eps: 0.898018
 34125/100000: episode: 367, duration: 0.616s, episode steps:  92, steps per second: 149, episode reward: -10.176, mean reward: -0.111 [-100.000, 116.194], mean action: 1.565 [0.000, 3.000],  loss: 24.504368, mse: 14237.070790, mean_q: 103.533481, mean_eps: 0.897765
 34234/100000: episode: 368, duration: 0.737s, episode steps: 109, steps per second: 148, episode reward: -107.916, mean reward: -0.990 [-100.000, 13.341], mean action: 1.312 [0.000, 3.000],  loss: 41.369353, mse: 14353.009837, mean_q: 103.314085, mean_eps: 0.897463
 34327/100000: episode: 369, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -156.061, mean reward: -1.678 [-100.000, 13.631], mean action: 1.699 [0.000, 3.000],  loss: 25.948229, mse: 14000.356645, mean_q: 101.146290, mean_eps: 0.897160
 34442/100000: episode: 370, duration: 0.791s, episode steps: 115, steps per second: 145, episode reward: -82.221, mean reward: -0.715 [-100.000, 15.049], mean action: 1.591 [0.000, 3.000],  loss: 38.874760, mse: 14411.113731, mean_q: 103.735936, mean_eps: 0.896848
 34520/100000: episode: 371, duration: 0.563s, episode steps:  78, steps per second: 139, episode reward: -213.709, mean reward: -2.740 [-100.000, 20.341], mean action: 1.397 [0.000, 3.000],  loss: 36.829302, mse: 14524.490172, mean_q: 102.097232, mean_eps: 0.896558
 34641/100000: episode: 372, duration: 0.889s, episode steps: 121, steps per second: 136, episode reward: -64.787, mean reward: -0.535 [-100.000,  9.397], mean action: 1.702 [0.000, 3.000],  loss: 44.260023, mse: 14584.270169, mean_q: 103.408426, mean_eps: 0.896260
 34785/100000: episode: 373, duration: 1.013s, episode steps: 144, steps per second: 142, episode reward: -182.063, mean reward: -1.264 [-100.000,  4.405], mean action: 1.458 [0.000, 3.000],  loss: 22.444928, mse: 14850.784905, mean_q: 104.087800, mean_eps: 0.895863
 34854/100000: episode: 374, duration: 0.498s, episode steps:  69, steps per second: 139, episode reward: -110.590, mean reward: -1.603 [-100.000,  5.923], mean action: 1.522 [0.000, 3.000],  loss: 40.469690, mse: 14843.135289, mean_q: 105.043492, mean_eps: 0.895543
 34971/100000: episode: 375, duration: 0.914s, episode steps: 117, steps per second: 128, episode reward: -287.490, mean reward: -2.457 [-100.000, 82.235], mean action: 1.462 [0.000, 3.000],  loss: 31.064022, mse: 14923.069177, mean_q: 105.511368, mean_eps: 0.895264
 35083/100000: episode: 376, duration: 0.829s, episode steps: 112, steps per second: 135, episode reward: -110.466, mean reward: -0.986 [-100.000, 13.618], mean action: 1.598 [0.000, 3.000],  loss: 41.025016, mse: 15328.949733, mean_q: 107.777354, mean_eps: 0.894920
 35209/100000: episode: 377, duration: 0.973s, episode steps: 126, steps per second: 130, episode reward: -109.193, mean reward: -0.867 [-100.000,  8.832], mean action: 1.659 [0.000, 3.000],  loss: 25.166764, mse: 15455.930718, mean_q: 108.365282, mean_eps: 0.894563
 35347/100000: episode: 378, duration: 1.065s, episode steps: 138, steps per second: 130, episode reward: -226.063, mean reward: -1.638 [-100.000, 96.328], mean action: 1.594 [0.000, 3.000],  loss: 24.621775, mse: 15676.286940, mean_q: 107.664154, mean_eps: 0.894167
 35450/100000: episode: 379, duration: 0.720s, episode steps: 103, steps per second: 143, episode reward: -116.025, mean reward: -1.126 [-100.000, 14.599], mean action: 1.592 [0.000, 3.000],  loss: 37.507080, mse: 15817.356777, mean_q: 108.859274, mean_eps: 0.893806
 35571/100000: episode: 380, duration: 0.834s, episode steps: 121, steps per second: 145, episode reward: -134.221, mean reward: -1.109 [-100.000,  3.955], mean action: 1.413 [0.000, 3.000],  loss: 28.239833, mse: 15506.098140, mean_q: 105.014873, mean_eps: 0.893470
 35657/100000: episode: 381, duration: 0.581s, episode steps:  86, steps per second: 148, episode reward: -171.616, mean reward: -1.996 [-100.000,  5.155], mean action: 1.430 [0.000, 3.000],  loss: 22.913782, mse: 16136.156636, mean_q: 109.981593, mean_eps: 0.893159
 35737/100000: episode: 382, duration: 0.540s, episode steps:  80, steps per second: 148, episode reward: -75.205, mean reward: -0.940 [-100.000,  5.682], mean action: 1.538 [0.000, 3.000],  loss: 32.805323, mse: 15770.546338, mean_q: 107.090260, mean_eps: 0.892910
 35823/100000: episode: 383, duration: 0.621s, episode steps:  86, steps per second: 138, episode reward: -134.879, mean reward: -1.568 [-100.000, 37.712], mean action: 1.698 [0.000, 3.000],  loss: 28.687112, mse: 15709.362100, mean_q: 106.477060, mean_eps: 0.892661
 35937/100000: episode: 384, duration: 0.784s, episode steps: 114, steps per second: 145, episode reward: -141.068, mean reward: -1.237 [-100.000,  4.731], mean action: 1.772 [0.000, 3.000],  loss: 23.083174, mse: 15709.823859, mean_q: 106.765796, mean_eps: 0.892361
 35998/100000: episode: 385, duration: 0.412s, episode steps:  61, steps per second: 148, episode reward: -103.169, mean reward: -1.691 [-100.000, 12.475], mean action: 1.508 [0.000, 3.000],  loss: 26.747147, mse: 15996.122839, mean_q: 107.462337, mean_eps: 0.892099
 36092/100000: episode: 386, duration: 0.664s, episode steps:  94, steps per second: 142, episode reward: -57.492, mean reward: -0.612 [-100.000,  7.895], mean action: 1.457 [0.000, 3.000],  loss: 18.703160, mse: 15842.156510, mean_q: 107.980397, mean_eps: 0.891866
 36179/100000: episode: 387, duration: 0.615s, episode steps:  87, steps per second: 142, episode reward: -92.790, mean reward: -1.067 [-100.000, 16.861], mean action: 1.494 [0.000, 3.000],  loss: 26.618040, mse: 15878.222432, mean_q: 108.856573, mean_eps: 0.891595
 36256/100000: episode: 388, duration: 0.529s, episode steps:  77, steps per second: 145, episode reward: -125.736, mean reward: -1.633 [-100.000,  8.209], mean action: 1.364 [0.000, 3.000],  loss: 32.933203, mse: 16140.687056, mean_q: 110.015192, mean_eps: 0.891349
 36328/100000: episode: 389, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -186.202, mean reward: -2.586 [-100.000,  4.554], mean action: 1.486 [0.000, 3.000],  loss: 26.501774, mse: 16347.928738, mean_q: 110.967912, mean_eps: 0.891125
 36437/100000: episode: 390, duration: 0.772s, episode steps: 109, steps per second: 141, episode reward: -104.885, mean reward: -0.962 [-100.000, 10.266], mean action: 1.587 [0.000, 3.000],  loss: 23.166069, mse: 16500.205696, mean_q: 110.066729, mean_eps: 0.890854
 36524/100000: episode: 391, duration: 0.596s, episode steps:  87, steps per second: 146, episode reward: -86.592, mean reward: -0.995 [-100.000, 11.842], mean action: 1.609 [0.000, 3.000],  loss: 35.201370, mse: 16724.844480, mean_q: 110.058627, mean_eps: 0.890560
 36642/100000: episode: 392, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -370.742, mean reward: -3.142 [-100.000, 113.645], mean action: 1.602 [0.000, 3.000],  loss: 27.718359, mse: 16747.025490, mean_q: 110.267464, mean_eps: 0.890253
 36737/100000: episode: 393, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -121.038, mean reward: -1.274 [-100.000, 10.675], mean action: 1.411 [0.000, 3.000],  loss: 31.901600, mse: 16261.798407, mean_q: 108.450148, mean_eps: 0.889933
 36817/100000: episode: 394, duration: 0.570s, episode steps:  80, steps per second: 140, episode reward: -117.474, mean reward: -1.468 [-100.000,  7.741], mean action: 1.488 [0.000, 3.000],  loss: 39.499147, mse: 17112.677563, mean_q: 113.834207, mean_eps: 0.889671
 36906/100000: episode: 395, duration: 0.607s, episode steps:  89, steps per second: 147, episode reward: -161.691, mean reward: -1.817 [-100.000,  7.061], mean action: 1.551 [0.000, 3.000],  loss: 29.362736, mse: 16926.012355, mean_q: 110.867939, mean_eps: 0.889417
 36998/100000: episode: 396, duration: 0.708s, episode steps:  92, steps per second: 130, episode reward: -169.038, mean reward: -1.837 [-100.000,  4.220], mean action: 1.380 [0.000, 3.000],  loss: 22.470700, mse: 17152.539338, mean_q: 111.225921, mean_eps: 0.889145
 37117/100000: episode: 397, duration: 0.962s, episode steps: 119, steps per second: 124, episode reward: -239.741, mean reward: -2.015 [-100.000,  6.850], mean action: 1.504 [0.000, 3.000],  loss: 32.494829, mse: 17268.992787, mean_q: 114.095744, mean_eps: 0.888829
 37212/100000: episode: 398, duration: 0.651s, episode steps:  95, steps per second: 146, episode reward: -212.442, mean reward: -2.236 [-100.000,  9.127], mean action: 1.474 [0.000, 3.000],  loss: 47.485474, mse: 17502.509581, mean_q: 115.148327, mean_eps: 0.888508
 37279/100000: episode: 399, duration: 0.459s, episode steps:  67, steps per second: 146, episode reward: -67.481, mean reward: -1.007 [-100.000,  9.971], mean action: 1.433 [0.000, 3.000],  loss: 32.570949, mse: 17013.132361, mean_q: 111.640113, mean_eps: 0.888265
 37347/100000: episode: 400, duration: 0.482s, episode steps:  68, steps per second: 141, episode reward: -134.511, mean reward: -1.978 [-100.000,  9.079], mean action: 1.529 [0.000, 3.000],  loss: 19.660529, mse: 17781.817957, mean_q: 116.502756, mean_eps: 0.888062
 37410/100000: episode: 401, duration: 0.425s, episode steps:  63, steps per second: 148, episode reward: -74.233, mean reward: -1.178 [-100.000,  8.369], mean action: 1.460 [0.000, 3.000],  loss: 22.688806, mse: 17633.568220, mean_q: 114.583624, mean_eps: 0.887866
 37516/100000: episode: 402, duration: 0.714s, episode steps: 106, steps per second: 149, episode reward: -97.278, mean reward: -0.918 [-100.000, 12.134], mean action: 1.632 [0.000, 3.000],  loss: 29.351029, mse: 17511.038104, mean_q: 111.790515, mean_eps: 0.887612
 37613/100000: episode: 403, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -124.489, mean reward: -1.283 [-100.000,  5.800], mean action: 1.278 [0.000, 3.000],  loss: 30.460619, mse: 18271.849871, mean_q: 115.979386, mean_eps: 0.887308
 37683/100000: episode: 404, duration: 0.486s, episode steps:  70, steps per second: 144, episode reward: -63.662, mean reward: -0.909 [-100.000, 13.367], mean action: 1.500 [0.000, 3.000],  loss: 21.906232, mse: 17953.692048, mean_q: 114.116184, mean_eps: 0.887058
 37756/100000: episode: 405, duration: 0.492s, episode steps:  73, steps per second: 148, episode reward: -147.340, mean reward: -2.018 [-100.000, 16.652], mean action: 1.521 [0.000, 3.000],  loss: 28.835353, mse: 18140.947225, mean_q: 117.537908, mean_eps: 0.886843
 37846/100000: episode: 406, duration: 0.600s, episode steps:  90, steps per second: 150, episode reward: -68.063, mean reward: -0.756 [-100.000, 20.272], mean action: 1.389 [0.000, 3.000],  loss: 15.254549, mse: 17827.949316, mean_q: 114.639204, mean_eps: 0.886598
 37951/100000: episode: 407, duration: 0.730s, episode steps: 105, steps per second: 144, episode reward: -112.628, mean reward: -1.073 [-100.000,  6.825], mean action: 1.581 [0.000, 3.000],  loss: 21.222371, mse: 18710.317076, mean_q: 118.690198, mean_eps: 0.886306
 38027/100000: episode: 408, duration: 0.520s, episode steps:  76, steps per second: 146, episode reward: -125.452, mean reward: -1.651 [-100.000,  4.659], mean action: 1.632 [0.000, 3.000],  loss: 22.999896, mse: 17625.109169, mean_q: 113.172924, mean_eps: 0.886035
 38155/100000: episode: 409, duration: 0.863s, episode steps: 128, steps per second: 148, episode reward: -50.894, mean reward: -0.398 [-100.000, 23.346], mean action: 1.523 [0.000, 3.000],  loss: 19.863728, mse: 17673.066460, mean_q: 114.329337, mean_eps: 0.885728
 38227/100000: episode: 410, duration: 0.498s, episode steps:  72, steps per second: 144, episode reward: -31.873, mean reward: -0.443 [-100.000, 57.108], mean action: 1.389 [0.000, 3.000],  loss: 20.137043, mse: 18140.961331, mean_q: 117.487955, mean_eps: 0.885428
 38292/100000: episode: 411, duration: 0.448s, episode steps:  65, steps per second: 145, episode reward: -94.663, mean reward: -1.456 [-100.000,  5.191], mean action: 1.292 [0.000, 3.000],  loss: 30.658637, mse: 17627.254011, mean_q: 113.549705, mean_eps: 0.885223
 38388/100000: episode: 412, duration: 0.656s, episode steps:  96, steps per second: 146, episode reward: -411.928, mean reward: -4.291 [-100.000,  0.352], mean action: 1.708 [0.000, 3.000],  loss: 26.476785, mse: 17679.161184, mean_q: 113.438975, mean_eps: 0.884981
 38508/100000: episode: 413, duration: 0.807s, episode steps: 120, steps per second: 149, episode reward: -109.914, mean reward: -0.916 [-100.000, 30.136], mean action: 1.508 [0.000, 3.000],  loss: 36.336379, mse: 17821.163867, mean_q: 113.916989, mean_eps: 0.884657
 38624/100000: episode: 414, duration: 0.813s, episode steps: 116, steps per second: 143, episode reward: -184.484, mean reward: -1.590 [-100.000,  5.446], mean action: 1.629 [0.000, 3.000],  loss: 18.486612, mse: 18010.955566, mean_q: 115.873012, mean_eps: 0.884304
 38736/100000: episode: 415, duration: 0.756s, episode steps: 112, steps per second: 148, episode reward: -255.931, mean reward: -2.285 [-100.000,  1.288], mean action: 1.509 [0.000, 3.000],  loss: 24.243415, mse: 18465.981742, mean_q: 119.269820, mean_eps: 0.883962
 38836/100000: episode: 416, duration: 0.686s, episode steps: 100, steps per second: 146, episode reward: -106.423, mean reward: -1.064 [-100.000, 25.468], mean action: 1.640 [0.000, 3.000],  loss: 27.320375, mse: 18232.822334, mean_q: 117.562606, mean_eps: 0.883644
 38925/100000: episode: 417, duration: 0.756s, episode steps:  89, steps per second: 118, episode reward: -108.480, mean reward: -1.219 [-100.000, 20.813], mean action: 1.371 [0.000, 3.000],  loss: 34.804048, mse: 18598.557343, mean_q: 118.676291, mean_eps: 0.883360
 39030/100000: episode: 418, duration: 0.833s, episode steps: 105, steps per second: 126, episode reward: -161.739, mean reward: -1.540 [-100.000,  2.662], mean action: 1.476 [0.000, 3.000],  loss: 24.194589, mse: 18107.994187, mean_q: 115.778698, mean_eps: 0.883069
 39106/100000: episode: 419, duration: 0.597s, episode steps:  76, steps per second: 127, episode reward: -153.377, mean reward: -2.018 [-100.000,  6.272], mean action: 1.513 [0.000, 3.000],  loss: 39.992809, mse: 18341.750206, mean_q: 116.860483, mean_eps: 0.882797
 39185/100000: episode: 420, duration: 0.726s, episode steps:  79, steps per second: 109, episode reward: -91.252, mean reward: -1.155 [-100.000, 10.029], mean action: 1.646 [0.000, 3.000],  loss: 35.897395, mse: 18171.228676, mean_q: 114.675286, mean_eps: 0.882565
 39263/100000: episode: 421, duration: 0.674s, episode steps:  78, steps per second: 116, episode reward: -67.172, mean reward: -0.861 [-100.000, 20.172], mean action: 1.731 [0.000, 3.000],  loss: 20.971849, mse: 18612.238719, mean_q: 117.132888, mean_eps: 0.882330
 39342/100000: episode: 422, duration: 0.682s, episode steps:  79, steps per second: 116, episode reward: -94.311, mean reward: -1.194 [-100.000, 12.950], mean action: 1.405 [0.000, 3.000],  loss: 29.859380, mse: 19112.975982, mean_q: 118.173978, mean_eps: 0.882094
 39444/100000: episode: 423, duration: 0.912s, episode steps: 102, steps per second: 112, episode reward: -180.409, mean reward: -1.769 [-100.000,  6.874], mean action: 1.480 [0.000, 3.000],  loss: 22.001361, mse: 18672.421425, mean_q: 116.825659, mean_eps: 0.881822
 39546/100000: episode: 424, duration: 0.721s, episode steps: 102, steps per second: 141, episode reward: -54.194, mean reward: -0.531 [-100.000,  7.992], mean action: 1.461 [0.000, 3.000],  loss: 41.157664, mse: 18957.120730, mean_q: 118.457265, mean_eps: 0.881516
 39632/100000: episode: 425, duration: 0.588s, episode steps:  86, steps per second: 146, episode reward: -238.385, mean reward: -2.772 [-100.000, 15.394], mean action: 1.512 [0.000, 3.000],  loss: 24.124814, mse: 18823.383937, mean_q: 116.596606, mean_eps: 0.881235
 39730/100000: episode: 426, duration: 0.751s, episode steps:  98, steps per second: 131, episode reward: -369.355, mean reward: -3.769 [-100.000, 118.348], mean action: 1.653 [0.000, 3.000],  loss: 28.182440, mse: 18841.952278, mean_q: 117.075783, mean_eps: 0.880958
 39841/100000: episode: 427, duration: 0.916s, episode steps: 111, steps per second: 121, episode reward: -412.846, mean reward: -3.719 [-100.000,  1.627], mean action: 1.360 [0.000, 3.000],  loss: 23.706949, mse: 19536.177858, mean_q: 120.820249, mean_eps: 0.880645
 39942/100000: episode: 428, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: -114.214, mean reward: -1.131 [-100.000, 20.655], mean action: 1.743 [0.000, 3.000],  loss: 39.796188, mse: 18895.410765, mean_q: 117.627862, mean_eps: 0.880327
 40014/100000: episode: 429, duration: 0.515s, episode steps:  72, steps per second: 140, episode reward: -132.346, mean reward: -1.838 [-100.000,  6.024], mean action: 1.472 [0.000, 3.000],  loss: 28.339895, mse: 18464.722168, mean_q: 116.799709, mean_eps: 0.880067
 40109/100000: episode: 430, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -123.326, mean reward: -1.298 [-100.000,  7.248], mean action: 1.653 [0.000, 3.000],  loss: 25.259331, mse: 19325.690718, mean_q: 119.763159, mean_eps: 0.879817
 40199/100000: episode: 431, duration: 0.701s, episode steps:  90, steps per second: 128, episode reward: -120.560, mean reward: -1.340 [-100.000,  4.909], mean action: 1.422 [0.000, 3.000],  loss: 25.257948, mse: 18948.377658, mean_q: 116.766058, mean_eps: 0.879540
 40272/100000: episode: 432, duration: 0.680s, episode steps:  73, steps per second: 107, episode reward: -43.735, mean reward: -0.599 [-100.000, 20.376], mean action: 1.452 [0.000, 3.000],  loss: 35.095431, mse: 19224.743204, mean_q: 116.800218, mean_eps: 0.879295
 40344/100000: episode: 433, duration: 0.533s, episode steps:  72, steps per second: 135, episode reward: -79.928, mean reward: -1.110 [-100.000, 11.679], mean action: 1.569 [0.000, 3.000],  loss: 23.840301, mse: 18812.635566, mean_q: 114.016305, mean_eps: 0.879078
 40505/100000: episode: 434, duration: 1.178s, episode steps: 161, steps per second: 137, episode reward: -110.561, mean reward: -0.687 [-100.000,  9.319], mean action: 1.528 [0.000, 3.000],  loss: 31.024968, mse: 19796.923810, mean_q: 120.241458, mean_eps: 0.878728
 40588/100000: episode: 435, duration: 0.609s, episode steps:  83, steps per second: 136, episode reward: -129.777, mean reward: -1.564 [-100.000,  8.117], mean action: 1.627 [0.000, 3.000],  loss: 46.221194, mse: 19586.830384, mean_q: 115.826495, mean_eps: 0.878362
 40663/100000: episode: 436, duration: 0.558s, episode steps:  75, steps per second: 135, episode reward: -72.570, mean reward: -0.968 [-100.000, 13.540], mean action: 1.480 [0.000, 3.000],  loss: 67.335178, mse: 19852.413542, mean_q: 118.886552, mean_eps: 0.878125
 40789/100000: episode: 437, duration: 1.082s, episode steps: 126, steps per second: 116, episode reward: -34.116, mean reward: -0.271 [-100.000, 130.909], mean action: 1.452 [0.000, 3.000],  loss: 43.480659, mse: 19897.903599, mean_q: 118.414131, mean_eps: 0.877823
 40903/100000: episode: 438, duration: 0.828s, episode steps: 114, steps per second: 138, episode reward: -161.372, mean reward: -1.416 [-100.000,  6.890], mean action: 1.632 [0.000, 3.000],  loss: 33.890854, mse: 20295.725637, mean_q: 123.167469, mean_eps: 0.877463
 40978/100000: episode: 439, duration: 0.524s, episode steps:  75, steps per second: 143, episode reward: -131.789, mean reward: -1.757 [-100.000, 11.570], mean action: 1.373 [0.000, 3.000],  loss: 35.582398, mse: 20541.570703, mean_q: 121.404162, mean_eps: 0.877180
 41096/100000: episode: 440, duration: 0.860s, episode steps: 118, steps per second: 137, episode reward: -111.232, mean reward: -0.943 [-100.000, 12.614], mean action: 1.534 [0.000, 3.000],  loss: 31.028042, mse: 20581.812947, mean_q: 123.171773, mean_eps: 0.876890
 41204/100000: episode: 441, duration: 0.762s, episode steps: 108, steps per second: 142, episode reward: -215.440, mean reward: -1.995 [-100.000, 63.261], mean action: 1.472 [0.000, 3.000],  loss: 30.460672, mse: 20369.337104, mean_q: 122.490442, mean_eps: 0.876552
 41286/100000: episode: 442, duration: 0.584s, episode steps:  82, steps per second: 140, episode reward: -201.933, mean reward: -2.463 [-100.000, 33.556], mean action: 1.573 [0.000, 3.000],  loss: 25.367085, mse: 19550.376143, mean_q: 117.820708, mean_eps: 0.876267
 41392/100000: episode: 443, duration: 0.797s, episode steps: 106, steps per second: 133, episode reward: -363.096, mean reward: -3.425 [-100.000,  4.980], mean action: 1.528 [0.000, 3.000],  loss: 28.051266, mse: 19564.268877, mean_q: 119.072899, mean_eps: 0.875984
 41471/100000: episode: 444, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -100.901, mean reward: -1.277 [-100.000, 10.506], mean action: 1.696 [0.000, 3.000],  loss: 34.227766, mse: 19408.186511, mean_q: 118.307453, mean_eps: 0.875707
 41550/100000: episode: 445, duration: 0.545s, episode steps:  79, steps per second: 145, episode reward: -99.869, mean reward: -1.264 [-100.000,  8.105], mean action: 1.456 [0.000, 3.000],  loss: 40.028606, mse: 19070.745476, mean_q: 115.612006, mean_eps: 0.875470
 41655/100000: episode: 446, duration: 0.714s, episode steps: 105, steps per second: 147, episode reward: -110.651, mean reward: -1.054 [-100.000,  7.305], mean action: 1.581 [0.000, 3.000],  loss: 29.410711, mse: 19849.021261, mean_q: 117.950376, mean_eps: 0.875194
 41719/100000: episode: 447, duration: 0.465s, episode steps:  64, steps per second: 138, episode reward: -107.878, mean reward: -1.686 [-100.000,  9.109], mean action: 1.531 [0.000, 3.000],  loss: 29.863388, mse: 19837.400269, mean_q: 119.375620, mean_eps: 0.874940
 41799/100000: episode: 448, duration: 0.573s, episode steps:  80, steps per second: 140, episode reward: -148.770, mean reward: -1.860 [-100.000, 13.143], mean action: 1.600 [0.000, 3.000],  loss: 54.506067, mse: 19725.181714, mean_q: 118.633975, mean_eps: 0.874724
 41899/100000: episode: 449, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: -175.849, mean reward: -1.758 [-100.000,  7.859], mean action: 1.480 [0.000, 3.000],  loss: 51.206611, mse: 18873.305430, mean_q: 116.133070, mean_eps: 0.874454
 41985/100000: episode: 450, duration: 0.598s, episode steps:  86, steps per second: 144, episode reward: -82.627, mean reward: -0.961 [-100.000,  5.303], mean action: 1.407 [0.000, 3.000],  loss: 50.989643, mse: 19281.534725, mean_q: 120.400968, mean_eps: 0.874175
 42080/100000: episode: 451, duration: 0.722s, episode steps:  95, steps per second: 132, episode reward: -64.871, mean reward: -0.683 [-100.000, 14.077], mean action: 1.537 [0.000, 3.000],  loss: 35.237168, mse: 20140.467907, mean_q: 123.087995, mean_eps: 0.873904
 42148/100000: episode: 452, duration: 0.518s, episode steps:  68, steps per second: 131, episode reward: -125.215, mean reward: -1.841 [-100.000,  7.567], mean action: 1.735 [0.000, 3.000],  loss: 31.769132, mse: 19618.945600, mean_q: 122.608446, mean_eps: 0.873659
 42255/100000: episode: 453, duration: 0.759s, episode steps: 107, steps per second: 141, episode reward: -151.442, mean reward: -1.415 [-100.000, 12.551], mean action: 1.561 [0.000, 3.000],  loss: 36.118721, mse: 19534.973094, mean_q: 120.447807, mean_eps: 0.873397
 42324/100000: episode: 454, duration: 0.568s, episode steps:  69, steps per second: 121, episode reward: -39.816, mean reward: -0.577 [-100.000, 18.926], mean action: 1.449 [0.000, 3.000],  loss: 31.247247, mse: 18946.254826, mean_q: 118.692736, mean_eps: 0.873133
 42394/100000: episode: 455, duration: 0.502s, episode steps:  70, steps per second: 139, episode reward: -172.503, mean reward: -2.464 [-100.000,  7.935], mean action: 1.529 [0.000, 3.000],  loss: 34.983933, mse: 18812.713839, mean_q: 117.426289, mean_eps: 0.872925
 42501/100000: episode: 456, duration: 0.760s, episode steps: 107, steps per second: 141, episode reward: -211.764, mean reward: -1.979 [-100.000,  7.836], mean action: 1.505 [0.000, 3.000],  loss: 29.778675, mse: 19095.002592, mean_q: 120.100604, mean_eps: 0.872659
 42618/100000: episode: 457, duration: 0.861s, episode steps: 117, steps per second: 136, episode reward: -284.209, mean reward: -2.429 [-100.000,  0.990], mean action: 1.718 [0.000, 3.000],  loss: 38.899300, mse: 18915.226329, mean_q: 118.957653, mean_eps: 0.872323
 42683/100000: episode: 458, duration: 0.460s, episode steps:  65, steps per second: 141, episode reward: -56.153, mean reward: -0.864 [-100.000, 14.458], mean action: 1.692 [0.000, 3.000],  loss: 36.086621, mse: 19651.396109, mean_q: 122.906782, mean_eps: 0.872050
 42770/100000: episode: 459, duration: 0.608s, episode steps:  87, steps per second: 143, episode reward: -92.987, mean reward: -1.069 [-100.000, 62.919], mean action: 1.529 [0.000, 3.000],  loss: 36.977430, mse: 19487.340731, mean_q: 121.401937, mean_eps: 0.871822
 42846/100000: episode: 460, duration: 0.548s, episode steps:  76, steps per second: 139, episode reward: -61.803, mean reward: -0.813 [-100.000, 13.907], mean action: 1.447 [0.000, 3.000],  loss: 21.839960, mse: 19294.048070, mean_q: 121.667473, mean_eps: 0.871578
 42965/100000: episode: 461, duration: 0.909s, episode steps: 119, steps per second: 131, episode reward: -317.187, mean reward: -2.665 [-100.000,  1.303], mean action: 1.294 [0.000, 3.000],  loss: 33.195067, mse: 19286.224503, mean_q: 119.303234, mean_eps: 0.871285
 43063/100000: episode: 462, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -79.124, mean reward: -0.807 [-100.000,  8.150], mean action: 1.398 [0.000, 3.000],  loss: 28.973146, mse: 19642.843272, mean_q: 121.066503, mean_eps: 0.870960
 43178/100000: episode: 463, duration: 0.796s, episode steps: 115, steps per second: 145, episode reward: -155.123, mean reward: -1.349 [-100.000,  9.256], mean action: 1.783 [0.000, 3.000],  loss: 30.730727, mse: 20003.062517, mean_q: 123.158724, mean_eps: 0.870640
 43290/100000: episode: 464, duration: 0.801s, episode steps: 112, steps per second: 140, episode reward: -211.185, mean reward: -1.886 [-100.000,  5.021], mean action: 1.500 [0.000, 3.000],  loss: 32.762467, mse: 20473.722752, mean_q: 124.659769, mean_eps: 0.870300
 43398/100000: episode: 465, duration: 0.846s, episode steps: 108, steps per second: 128, episode reward: -342.078, mean reward: -3.167 [-100.000,  0.907], mean action: 1.611 [0.000, 3.000],  loss: 30.232974, mse: 20121.011249, mean_q: 123.343637, mean_eps: 0.869969
 43485/100000: episode: 466, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -85.796, mean reward: -0.986 [-100.000, 11.098], mean action: 1.529 [0.000, 3.000],  loss: 24.671331, mse: 20454.711173, mean_q: 125.575777, mean_eps: 0.869677
 43555/100000: episode: 467, duration: 0.534s, episode steps:  70, steps per second: 131, episode reward: -124.410, mean reward: -1.777 [-100.000, 27.449], mean action: 1.543 [0.000, 3.000],  loss: 31.804585, mse: 20358.807868, mean_q: 124.103081, mean_eps: 0.869441
 43620/100000: episode: 468, duration: 0.449s, episode steps:  65, steps per second: 145, episode reward: -134.157, mean reward: -2.064 [-100.000, 12.856], mean action: 1.754 [0.000, 3.000],  loss: 33.669737, mse: 20202.834210, mean_q: 122.484581, mean_eps: 0.869239
 43699/100000: episode: 469, duration: 0.550s, episode steps:  79, steps per second: 144, episode reward: -108.617, mean reward: -1.375 [-100.000, 32.694], mean action: 1.582 [0.000, 3.000],  loss: 25.039795, mse: 20117.981841, mean_q: 123.055197, mean_eps: 0.869023
 43799/100000: episode: 470, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -142.251, mean reward: -1.423 [-100.000,  3.211], mean action: 1.480 [0.000, 3.000],  loss: 28.631000, mse: 20151.023926, mean_q: 121.922296, mean_eps: 0.868754
 43893/100000: episode: 471, duration: 0.643s, episode steps:  94, steps per second: 146, episode reward: -136.378, mean reward: -1.451 [-100.000, 17.566], mean action: 1.649 [0.000, 3.000],  loss: 24.501527, mse: 20162.481383, mean_q: 122.281986, mean_eps: 0.868464
 43988/100000: episode: 472, duration: 0.653s, episode steps:  95, steps per second: 146, episode reward: -202.152, mean reward: -2.128 [-100.000,  8.946], mean action: 1.305 [0.000, 3.000],  loss: 29.361633, mse: 20320.270734, mean_q: 122.750795, mean_eps: 0.868180
 44063/100000: episode: 473, duration: 0.517s, episode steps:  75, steps per second: 145, episode reward: -87.414, mean reward: -1.166 [-100.000,  6.591], mean action: 1.533 [0.000, 3.000],  loss: 30.256024, mse: 20558.304193, mean_q: 125.017414, mean_eps: 0.867925
 44177/100000: episode: 474, duration: 0.796s, episode steps: 114, steps per second: 143, episode reward: -165.021, mean reward: -1.448 [-100.000,  3.267], mean action: 1.789 [0.000, 3.000],  loss: 38.606994, mse: 20099.031036, mean_q: 120.286257, mean_eps: 0.867642
 44249/100000: episode: 475, duration: 0.506s, episode steps:  72, steps per second: 142, episode reward: -96.214, mean reward: -1.336 [-100.000,  6.805], mean action: 1.681 [0.000, 3.000],  loss: 27.903173, mse: 20388.061415, mean_q: 122.724958, mean_eps: 0.867363
 44355/100000: episode: 476, duration: 0.753s, episode steps: 106, steps per second: 141, episode reward: -86.645, mean reward: -0.817 [-100.000, 23.615], mean action: 1.557 [0.000, 3.000],  loss: 28.838142, mse: 20288.563053, mean_q: 123.166039, mean_eps: 0.867095
 44423/100000: episode: 477, duration: 0.579s, episode steps:  68, steps per second: 117, episode reward: -161.647, mean reward: -2.377 [-100.000,  5.239], mean action: 1.397 [0.000, 3.000],  loss: 23.738442, mse: 20527.872085, mean_q: 124.927018, mean_eps: 0.866835
 44527/100000: episode: 478, duration: 0.752s, episode steps: 104, steps per second: 138, episode reward: -281.304, mean reward: -2.705 [-100.000,  1.005], mean action: 1.490 [0.000, 3.000],  loss: 23.458331, mse: 20172.246469, mean_q: 121.350396, mean_eps: 0.866577
 44638/100000: episode: 479, duration: 0.757s, episode steps: 111, steps per second: 147, episode reward: -102.479, mean reward: -0.923 [-100.000,  6.080], mean action: 1.514 [0.000, 3.000],  loss: 22.955022, mse: 19579.144505, mean_q: 118.483372, mean_eps: 0.866254
 44711/100000: episode: 480, duration: 0.549s, episode steps:  73, steps per second: 133, episode reward: -114.157, mean reward: -1.564 [-100.000, 13.737], mean action: 1.575 [0.000, 3.000],  loss: 19.267628, mse: 19784.669387, mean_q: 120.498787, mean_eps: 0.865978
 44771/100000: episode: 481, duration: 0.447s, episode steps:  60, steps per second: 134, episode reward: -131.244, mean reward: -2.187 [-100.000, 18.792], mean action: 1.633 [0.000, 3.000],  loss: 24.979280, mse: 19411.220166, mean_q: 116.425570, mean_eps: 0.865778
 44840/100000: episode: 482, duration: 0.500s, episode steps:  69, steps per second: 138, episode reward: -90.689, mean reward: -1.314 [-100.000,  6.834], mean action: 1.551 [0.000, 3.000],  loss: 22.171045, mse: 20275.902415, mean_q: 123.962862, mean_eps: 0.865585
 44946/100000: episode: 483, duration: 0.748s, episode steps: 106, steps per second: 142, episode reward: -247.252, mean reward: -2.333 [-100.000,  7.509], mean action: 1.547 [0.000, 3.000],  loss: 21.172115, mse: 20177.136249, mean_q: 120.280919, mean_eps: 0.865322
 45054/100000: episode: 484, duration: 0.806s, episode steps: 108, steps per second: 134, episode reward: -491.318, mean reward: -4.549 [-100.000, 47.900], mean action: 1.528 [0.000, 3.000],  loss: 26.349363, mse: 19941.365560, mean_q: 119.075803, mean_eps: 0.865002
 45182/100000: episode: 485, duration: 0.938s, episode steps: 128, steps per second: 136, episode reward: -164.835, mean reward: -1.288 [-100.000,  7.450], mean action: 1.664 [0.000, 3.000],  loss: 18.838690, mse: 19988.712982, mean_q: 119.652118, mean_eps: 0.864648
 45322/100000: episode: 486, duration: 1.047s, episode steps: 140, steps per second: 134, episode reward: -453.438, mean reward: -3.239 [-100.000, 74.607], mean action: 1.450 [0.000, 3.000],  loss: 26.226085, mse: 19938.390590, mean_q: 119.163565, mean_eps: 0.864245
 45402/100000: episode: 487, duration: 0.568s, episode steps:  80, steps per second: 141, episode reward: -90.152, mean reward: -1.127 [-100.000, 17.600], mean action: 1.512 [0.000, 3.000],  loss: 47.059486, mse: 19889.920923, mean_q: 119.336820, mean_eps: 0.863915
 45518/100000: episode: 488, duration: 0.885s, episode steps: 116, steps per second: 131, episode reward: -219.199, mean reward: -1.890 [-100.000, 10.242], mean action: 1.509 [0.000, 3.000],  loss: 32.135853, mse: 19929.615167, mean_q: 118.008794, mean_eps: 0.863621
 45586/100000: episode: 489, duration: 0.526s, episode steps:  68, steps per second: 129, episode reward: -117.628, mean reward: -1.730 [-100.000, 29.175], mean action: 1.515 [0.000, 3.000],  loss: 30.143091, mse: 20201.695614, mean_q: 118.866875, mean_eps: 0.863345
 45647/100000: episode: 490, duration: 0.469s, episode steps:  61, steps per second: 130, episode reward: -127.306, mean reward: -2.087 [-100.000,  8.757], mean action: 1.246 [0.000, 3.000],  loss: 34.061197, mse: 20034.901143, mean_q: 116.959081, mean_eps: 0.863152
 45743/100000: episode: 491, duration: 0.674s, episode steps:  96, steps per second: 142, episode reward: -104.050, mean reward: -1.084 [-100.000,  6.243], mean action: 1.604 [0.000, 3.000],  loss: 23.216515, mse: 20103.008301, mean_q: 120.125752, mean_eps: 0.862916
 45848/100000: episode: 492, duration: 0.733s, episode steps: 105, steps per second: 143, episode reward: -106.621, mean reward: -1.015 [-100.000, 11.957], mean action: 1.562 [0.000, 3.000],  loss: 58.071876, mse: 20035.780032, mean_q: 119.542671, mean_eps: 0.862615
 45963/100000: episode: 493, duration: 0.795s, episode steps: 115, steps per second: 145, episode reward: -170.975, mean reward: -1.487 [-100.000,  8.163], mean action: 1.487 [0.000, 3.000],  loss: 30.102836, mse: 20294.467731, mean_q: 118.650672, mean_eps: 0.862285
 46023/100000: episode: 494, duration: 0.406s, episode steps:  60, steps per second: 148, episode reward: -104.894, mean reward: -1.748 [-100.000,  6.102], mean action: 1.217 [0.000, 3.000],  loss: 41.988641, mse: 20917.596110, mean_q: 121.716081, mean_eps: 0.862022
 46112/100000: episode: 495, duration: 0.604s, episode steps:  89, steps per second: 147, episode reward: -81.023, mean reward: -0.910 [-100.000, 53.540], mean action: 1.472 [0.000, 3.000],  loss: 40.120894, mse: 20633.100783, mean_q: 119.185110, mean_eps: 0.861799
 46186/100000: episode: 496, duration: 0.533s, episode steps:  74, steps per second: 139, episode reward: -103.413, mean reward: -1.397 [-100.000,  7.508], mean action: 1.514 [0.000, 3.000],  loss: 29.517184, mse: 20937.502798, mean_q: 121.204823, mean_eps: 0.861555
 46264/100000: episode: 497, duration: 0.558s, episode steps:  78, steps per second: 140, episode reward: -56.009, mean reward: -0.718 [-100.000, 23.223], mean action: 1.308 [0.000, 3.000],  loss: 30.996519, mse: 19996.619191, mean_q: 114.983161, mean_eps: 0.861326
 46370/100000: episode: 498, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -100.927, mean reward: -0.952 [-100.000, 18.581], mean action: 1.651 [0.000, 3.000],  loss: 39.028991, mse: 20062.349877, mean_q: 115.948087, mean_eps: 0.861050
 46471/100000: episode: 499, duration: 0.715s, episode steps: 101, steps per second: 141, episode reward: -170.689, mean reward: -1.690 [-100.000, 11.464], mean action: 1.644 [0.000, 3.000],  loss: 25.544741, mse: 20063.545492, mean_q: 113.267824, mean_eps: 0.860740
 46580/100000: episode: 500, duration: 0.760s, episode steps: 109, steps per second: 143, episode reward: -245.675, mean reward: -2.254 [-100.000, 22.301], mean action: 1.550 [0.000, 3.000],  loss: 30.701218, mse: 20159.957730, mean_q: 117.206275, mean_eps: 0.860425
 46645/100000: episode: 501, duration: 0.442s, episode steps:  65, steps per second: 147, episode reward: -57.906, mean reward: -0.891 [-100.000,  9.690], mean action: 1.354 [0.000, 3.000],  loss: 43.085226, mse: 19663.267067, mean_q: 114.125780, mean_eps: 0.860164
 46731/100000: episode: 502, duration: 0.578s, episode steps:  86, steps per second: 149, episode reward: -125.026, mean reward: -1.454 [-100.000, 21.524], mean action: 1.593 [0.000, 3.000],  loss: 33.228039, mse: 19636.822073, mean_q: 115.750988, mean_eps: 0.859937
 46841/100000: episode: 503, duration: 0.800s, episode steps: 110, steps per second: 137, episode reward: -172.998, mean reward: -1.573 [-100.000, 24.887], mean action: 1.500 [0.000, 3.000],  loss: 27.440147, mse: 20022.073446, mean_q: 115.874113, mean_eps: 0.859644
 46954/100000: episode: 504, duration: 0.833s, episode steps: 113, steps per second: 136, episode reward: -66.436, mean reward: -0.588 [-100.000, 10.110], mean action: 1.646 [0.000, 3.000],  loss: 45.966621, mse: 19900.593690, mean_q: 114.481370, mean_eps: 0.859309
 47033/100000: episode: 505, duration: 0.570s, episode steps:  79, steps per second: 139, episode reward: -123.740, mean reward: -1.566 [-100.000, 18.022], mean action: 1.519 [0.000, 3.000],  loss: 29.856101, mse: 20166.578273, mean_q: 117.610526, mean_eps: 0.859021
 47108/100000: episode: 506, duration: 0.575s, episode steps:  75, steps per second: 130, episode reward: -114.436, mean reward: -1.526 [-100.000, 16.512], mean action: 1.640 [0.000, 3.000],  loss: 36.192751, mse: 20409.394167, mean_q: 116.565105, mean_eps: 0.858790
 47173/100000: episode: 507, duration: 0.487s, episode steps:  65, steps per second: 133, episode reward: -29.193, mean reward: -0.449 [-100.000, 67.565], mean action: 1.338 [0.000, 3.000],  loss: 46.054871, mse: 20542.324129, mean_q: 119.458921, mean_eps: 0.858580
 47254/100000: episode: 508, duration: 0.663s, episode steps:  81, steps per second: 122, episode reward: -220.101, mean reward: -2.717 [-100.000,  4.960], mean action: 1.568 [0.000, 3.000],  loss: 32.590596, mse: 20016.371335, mean_q: 114.420665, mean_eps: 0.858361
 47383/100000: episode: 509, duration: 0.969s, episode steps: 129, steps per second: 133, episode reward: -99.425, mean reward: -0.771 [-100.000,  7.212], mean action: 1.597 [0.000, 3.000],  loss: 24.921603, mse: 20262.408461, mean_q: 115.674442, mean_eps: 0.858046
 47493/100000: episode: 510, duration: 0.893s, episode steps: 110, steps per second: 123, episode reward: -115.648, mean reward: -1.051 [-100.000,  8.824], mean action: 1.564 [0.000, 3.000],  loss: 34.714909, mse: 21330.283993, mean_q: 121.985600, mean_eps: 0.857687
 47564/100000: episode: 511, duration: 0.521s, episode steps:  71, steps per second: 136, episode reward: -62.879, mean reward: -0.886 [-100.000,  7.503], mean action: 1.380 [0.000, 3.000],  loss: 21.966393, mse: 20631.913842, mean_q: 118.703581, mean_eps: 0.857416
 47647/100000: episode: 512, duration: 0.621s, episode steps:  83, steps per second: 134, episode reward: -109.646, mean reward: -1.321 [-100.000, 20.441], mean action: 1.602 [0.000, 3.000],  loss: 29.329927, mse: 21128.625647, mean_q: 122.162237, mean_eps: 0.857185
 47764/100000: episode: 513, duration: 0.878s, episode steps: 117, steps per second: 133, episode reward: -112.110, mean reward: -0.958 [-100.000, 11.996], mean action: 1.504 [0.000, 3.000],  loss: 35.829281, mse: 20450.840128, mean_q: 120.342774, mean_eps: 0.856885
 47830/100000: episode: 514, duration: 0.590s, episode steps:  66, steps per second: 112, episode reward: -89.495, mean reward: -1.356 [-100.000,  7.657], mean action: 1.545 [0.000, 3.000],  loss: 24.109608, mse: 20734.715021, mean_q: 119.579373, mean_eps: 0.856611
 47904/100000: episode: 515, duration: 0.635s, episode steps:  74, steps per second: 117, episode reward: -87.699, mean reward: -1.185 [-100.000,  9.983], mean action: 1.405 [0.000, 3.000],  loss: 24.983911, mse: 21553.404851, mean_q: 123.637919, mean_eps: 0.856400
 47974/100000: episode: 516, duration: 0.623s, episode steps:  70, steps per second: 112, episode reward: -117.524, mean reward: -1.679 [-100.000, 25.814], mean action: 1.800 [0.000, 3.000],  loss: 30.227529, mse: 21355.771833, mean_q: 122.530980, mean_eps: 0.856185
 48067/100000: episode: 517, duration: 0.839s, episode steps:  93, steps per second: 111, episode reward: -180.534, mean reward: -1.941 [-100.000, 12.743], mean action: 1.516 [0.000, 3.000],  loss: 31.650803, mse: 21751.361297, mean_q: 124.921600, mean_eps: 0.855940
 48171/100000: episode: 518, duration: 0.813s, episode steps: 104, steps per second: 128, episode reward: -91.631, mean reward: -0.881 [-100.000,  6.332], mean action: 1.798 [0.000, 3.000],  loss: 27.103893, mse: 21897.331139, mean_q: 124.579856, mean_eps: 0.855644
 48262/100000: episode: 519, duration: 0.761s, episode steps:  91, steps per second: 120, episode reward: -121.558, mean reward: -1.336 [-100.000,  9.975], mean action: 1.604 [0.000, 3.000],  loss: 31.869803, mse: 21948.162109, mean_q: 124.087165, mean_eps: 0.855352
 48358/100000: episode: 520, duration: 0.741s, episode steps:  96, steps per second: 130, episode reward: -126.749, mean reward: -1.320 [-100.000,  6.569], mean action: 1.635 [0.000, 3.000],  loss: 26.655379, mse: 21836.618998, mean_q: 123.515498, mean_eps: 0.855071
 48451/100000: episode: 521, duration: 0.708s, episode steps:  93, steps per second: 131, episode reward: -108.955, mean reward: -1.172 [-100.000,  6.528], mean action: 1.495 [0.000, 3.000],  loss: 32.497086, mse: 21771.928742, mean_q: 121.979112, mean_eps: 0.854788
 48580/100000: episode: 522, duration: 0.924s, episode steps: 129, steps per second: 140, episode reward: -341.540, mean reward: -2.648 [-100.000, 120.688], mean action: 1.705 [0.000, 3.000],  loss: 30.151300, mse: 21498.541311, mean_q: 118.564334, mean_eps: 0.854455
 48700/100000: episode: 523, duration: 0.824s, episode steps: 120, steps per second: 146, episode reward: -5.961, mean reward: -0.050 [-100.000, 64.950], mean action: 1.508 [0.000, 3.000],  loss: 31.729319, mse: 21457.649219, mean_q: 120.382807, mean_eps: 0.854081
 48762/100000: episode: 524, duration: 0.468s, episode steps:  62, steps per second: 132, episode reward: -87.481, mean reward: -1.411 [-100.000, 17.891], mean action: 1.565 [0.000, 3.000],  loss: 18.530537, mse: 21677.476941, mean_q: 121.474134, mean_eps: 0.853809
 48840/100000: episode: 525, duration: 0.555s, episode steps:  78, steps per second: 141, episode reward: -81.474, mean reward: -1.045 [-100.000, 12.576], mean action: 1.577 [0.000, 3.000],  loss: 39.849423, mse: 21654.447090, mean_q: 119.738187, mean_eps: 0.853598
 48919/100000: episode: 526, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -139.796, mean reward: -1.770 [-100.000,  6.195], mean action: 1.696 [0.000, 3.000],  loss: 23.881131, mse: 22036.643073, mean_q: 121.258749, mean_eps: 0.853363
 49021/100000: episode: 527, duration: 0.757s, episode steps: 102, steps per second: 135, episode reward: -127.527, mean reward: -1.250 [-100.000, 12.654], mean action: 1.402 [0.000, 3.000],  loss: 25.578857, mse: 21730.004442, mean_q: 119.859391, mean_eps: 0.853092
 49144/100000: episode: 528, duration: 0.922s, episode steps: 123, steps per second: 133, episode reward: -235.931, mean reward: -1.918 [-100.000,  1.106], mean action: 1.520 [0.000, 3.000],  loss: 42.474277, mse: 21713.516832, mean_q: 120.964984, mean_eps: 0.852754
 49242/100000: episode: 529, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -60.433, mean reward: -0.617 [-100.000,  7.186], mean action: 1.561 [0.000, 3.000],  loss: 36.653353, mse: 21761.966458, mean_q: 120.696649, mean_eps: 0.852422
 49329/100000: episode: 530, duration: 0.620s, episode steps:  87, steps per second: 140, episode reward: -85.475, mean reward: -0.982 [-100.000, 19.281], mean action: 1.379 [0.000, 3.000],  loss: 29.171445, mse: 22395.944886, mean_q: 122.516147, mean_eps: 0.852145
 49406/100000: episode: 531, duration: 0.570s, episode steps:  77, steps per second: 135, episode reward: -49.312, mean reward: -0.640 [-100.000, 11.818], mean action: 1.468 [0.000, 3.000],  loss: 24.858452, mse: 21798.423536, mean_q: 120.744934, mean_eps: 0.851899
 49505/100000: episode: 532, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -255.759, mean reward: -2.583 [-100.000,  0.841], mean action: 1.465 [0.000, 3.000],  loss: 26.813989, mse: 21983.374240, mean_q: 120.597874, mean_eps: 0.851635
 49568/100000: episode: 533, duration: 0.433s, episode steps:  63, steps per second: 146, episode reward: -91.875, mean reward: -1.458 [-100.000, 22.045], mean action: 1.524 [0.000, 3.000],  loss: 27.965813, mse: 22004.142237, mean_q: 119.884629, mean_eps: 0.851392
 49660/100000: episode: 534, duration: 0.667s, episode steps:  92, steps per second: 138, episode reward: -114.856, mean reward: -1.248 [-100.000,  5.845], mean action: 1.424 [0.000, 3.000],  loss: 26.702873, mse: 22445.528957, mean_q: 122.387966, mean_eps: 0.851159
 49787/100000: episode: 535, duration: 0.927s, episode steps: 127, steps per second: 137, episode reward: -183.297, mean reward: -1.443 [-100.000, 42.378], mean action: 1.543 [0.000, 3.000],  loss: 30.833045, mse: 22222.628476, mean_q: 120.778719, mean_eps: 0.850831
 49854/100000: episode: 536, duration: 0.456s, episode steps:  67, steps per second: 147, episode reward: -27.742, mean reward: -0.414 [-100.000, 14.906], mean action: 1.433 [0.000, 3.000],  loss: 20.476399, mse: 22389.173099, mean_q: 120.718191, mean_eps: 0.850540
 49947/100000: episode: 537, duration: 0.677s, episode steps:  93, steps per second: 137, episode reward: -108.018, mean reward: -1.161 [-100.000, 10.709], mean action: 1.538 [0.000, 3.000],  loss: 37.861989, mse: 22524.839770, mean_q: 123.594189, mean_eps: 0.850300
 50010/100000: episode: 538, duration: 0.556s, episode steps:  63, steps per second: 113, episode reward: -95.979, mean reward: -1.523 [-100.000, 11.310], mean action: 1.587 [0.000, 3.000],  loss: 31.209913, mse: 22932.287419, mean_q: 126.673251, mean_eps: 0.850066
 50101/100000: episode: 539, duration: 0.707s, episode steps:  91, steps per second: 129, episode reward: -116.490, mean reward: -1.280 [-100.000,  5.681], mean action: 1.407 [0.000, 3.000],  loss: 36.979788, mse: 22588.050738, mean_q: 125.976824, mean_eps: 0.849835
 50217/100000: episode: 540, duration: 0.824s, episode steps: 116, steps per second: 141, episode reward: -127.262, mean reward: -1.097 [-100.000, 12.873], mean action: 1.422 [0.000, 3.000],  loss: 27.410628, mse: 22467.794796, mean_q: 126.194501, mean_eps: 0.849524
 50305/100000: episode: 541, duration: 0.627s, episode steps:  88, steps per second: 140, episode reward: -89.676, mean reward: -1.019 [-100.000, 21.658], mean action: 1.670 [0.000, 3.000],  loss: 22.796693, mse: 22918.811568, mean_q: 127.112263, mean_eps: 0.849218
 50427/100000: episode: 542, duration: 0.866s, episode steps: 122, steps per second: 141, episode reward: -135.278, mean reward: -1.109 [-100.000, 33.626], mean action: 1.549 [0.000, 3.000],  loss: 29.450441, mse: 23340.206919, mean_q: 130.283890, mean_eps: 0.848903
 50494/100000: episode: 543, duration: 0.452s, episode steps:  67, steps per second: 148, episode reward: -118.607, mean reward: -1.770 [-100.000, 10.714], mean action: 1.612 [0.000, 3.000],  loss: 24.001880, mse: 23654.085559, mean_q: 132.807771, mean_eps: 0.848620
 50563/100000: episode: 544, duration: 0.529s, episode steps:  69, steps per second: 130, episode reward: -58.242, mean reward: -0.844 [-100.000,  7.924], mean action: 1.551 [0.000, 3.000],  loss: 18.970952, mse: 22793.289459, mean_q: 126.588218, mean_eps: 0.848416
 50652/100000: episode: 545, duration: 0.644s, episode steps:  89, steps per second: 138, episode reward: -114.986, mean reward: -1.292 [-100.000,  9.519], mean action: 1.517 [0.000, 3.000],  loss: 25.427529, mse: 23994.199921, mean_q: 131.315124, mean_eps: 0.848179
 50755/100000: episode: 546, duration: 0.735s, episode steps: 103, steps per second: 140, episode reward: -99.599, mean reward: -0.967 [-100.000, 14.955], mean action: 1.709 [0.000, 3.000],  loss: 33.430682, mse: 23343.376441, mean_q: 128.383318, mean_eps: 0.847891
 50816/100000: episode: 547, duration: 0.484s, episode steps:  61, steps per second: 126, episode reward: -50.140, mean reward: -0.822 [-100.000, 12.840], mean action: 1.361 [0.000, 3.000],  loss: 37.100962, mse: 23174.057185, mean_q: 128.687978, mean_eps: 0.847645
 50936/100000: episode: 548, duration: 0.921s, episode steps: 120, steps per second: 130, episode reward: -129.269, mean reward: -1.077 [-100.000,  8.306], mean action: 1.408 [0.000, 3.000],  loss: 25.965403, mse: 23285.878809, mean_q: 128.751047, mean_eps: 0.847374
 51004/100000: episode: 549, duration: 0.498s, episode steps:  68, steps per second: 137, episode reward: -106.406, mean reward: -1.565 [-100.000, 17.941], mean action: 1.515 [0.000, 3.000],  loss: 24.210739, mse: 24387.036449, mean_q: 134.106472, mean_eps: 0.847092
 51089/100000: episode: 550, duration: 0.626s, episode steps:  85, steps per second: 136, episode reward: -61.343, mean reward: -0.722 [-100.000, 14.514], mean action: 1.671 [0.000, 3.000],  loss: 32.371187, mse: 24039.388764, mean_q: 131.989003, mean_eps: 0.846862
 51162/100000: episode: 551, duration: 0.545s, episode steps:  73, steps per second: 134, episode reward: -193.391, mean reward: -2.649 [-100.000, 26.860], mean action: 1.616 [0.000, 3.000],  loss: 32.428799, mse: 23833.032454, mean_q: 132.467581, mean_eps: 0.846625
 51277/100000: episode: 552, duration: 0.797s, episode steps: 115, steps per second: 144, episode reward: -40.920, mean reward: -0.356 [-100.000, 23.490], mean action: 1.513 [0.000, 3.000],  loss: 29.952754, mse: 23715.076783, mean_q: 130.460866, mean_eps: 0.846343
 51370/100000: episode: 553, duration: 0.732s, episode steps:  93, steps per second: 127, episode reward: -85.560, mean reward: -0.920 [-100.000, 14.306], mean action: 1.505 [0.000, 3.000],  loss: 21.023928, mse: 24406.474504, mean_q: 130.632205, mean_eps: 0.846031
 51447/100000: episode: 554, duration: 0.610s, episode steps:  77, steps per second: 126, episode reward: -75.519, mean reward: -0.981 [-100.000, 10.821], mean action: 1.649 [0.000, 3.000],  loss: 25.510766, mse: 23991.723366, mean_q: 131.379276, mean_eps: 0.845776
 51548/100000: episode: 555, duration: 0.697s, episode steps: 101, steps per second: 145, episode reward: -80.874, mean reward: -0.801 [-100.000, 14.317], mean action: 1.644 [0.000, 3.000],  loss: 40.628294, mse: 24777.953724, mean_q: 133.209815, mean_eps: 0.845509
 51608/100000: episode: 556, duration: 0.411s, episode steps:  60, steps per second: 146, episode reward: -94.916, mean reward: -1.582 [-100.000,  7.723], mean action: 1.467 [0.000, 3.000],  loss: 33.092776, mse: 24968.052018, mean_q: 133.366809, mean_eps: 0.845268
 51679/100000: episode: 557, duration: 0.520s, episode steps:  71, steps per second: 137, episode reward: -105.043, mean reward: -1.479 [-100.000,  9.440], mean action: 1.577 [0.000, 3.000],  loss: 29.289574, mse: 24116.991692, mean_q: 128.030631, mean_eps: 0.845071
 51810/100000: episode: 558, duration: 0.926s, episode steps: 131, steps per second: 141, episode reward: -93.299, mean reward: -0.712 [-100.000, 18.348], mean action: 1.550 [0.000, 3.000],  loss: 27.502359, mse: 23694.860799, mean_q: 125.951249, mean_eps: 0.844768
 51897/100000: episode: 559, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -107.346, mean reward: -1.234 [-100.000,  9.156], mean action: 1.598 [0.000, 3.000],  loss: 26.581043, mse: 24463.104593, mean_q: 130.066113, mean_eps: 0.844441
 52019/100000: episode: 560, duration: 0.872s, episode steps: 122, steps per second: 140, episode reward: -132.470, mean reward: -1.086 [-100.000,  7.620], mean action: 1.525 [0.000, 3.000],  loss: 47.707990, mse: 24475.808946, mean_q: 130.603838, mean_eps: 0.844127
 52151/100000: episode: 561, duration: 0.938s, episode steps: 132, steps per second: 141, episode reward: -157.282, mean reward: -1.192 [-100.000, 15.223], mean action: 1.735 [0.000, 3.000],  loss: 28.462205, mse: 24554.709014, mean_q: 129.594964, mean_eps: 0.843746
 52244/100000: episode: 562, duration: 0.644s, episode steps:  93, steps per second: 145, episode reward: -116.118, mean reward: -1.249 [-100.000,  6.923], mean action: 1.452 [0.000, 3.000],  loss: 47.017269, mse: 24443.938907, mean_q: 132.101665, mean_eps: 0.843409
 52338/100000: episode: 563, duration: 0.682s, episode steps:  94, steps per second: 138, episode reward: -203.908, mean reward: -2.169 [-100.000, 72.112], mean action: 1.606 [0.000, 3.000],  loss: 24.536252, mse: 24299.798371, mean_q: 131.047934, mean_eps: 0.843128
 52426/100000: episode: 564, duration: 0.650s, episode steps:  88, steps per second: 135, episode reward: -177.618, mean reward: -2.018 [-100.000,  7.166], mean action: 1.477 [0.000, 3.000],  loss: 18.056308, mse: 23662.041815, mean_q: 127.239886, mean_eps: 0.842855
 52533/100000: episode: 565, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -83.255, mean reward: -0.778 [-100.000,  7.627], mean action: 1.467 [0.000, 3.000],  loss: 26.933895, mse: 23909.996386, mean_q: 127.620095, mean_eps: 0.842563
 52632/100000: episode: 566, duration: 0.712s, episode steps:  99, steps per second: 139, episode reward: -242.637, mean reward: -2.451 [-100.000,  4.672], mean action: 1.525 [0.000, 3.000],  loss: 36.837646, mse: 24192.353910, mean_q: 128.079503, mean_eps: 0.842254
 52742/100000: episode: 567, duration: 0.762s, episode steps: 110, steps per second: 144, episode reward: -90.028, mean reward: -0.818 [-100.000, 13.161], mean action: 1.627 [0.000, 3.000],  loss: 32.148152, mse: 24061.520348, mean_q: 129.256270, mean_eps: 0.841940
 52861/100000: episode: 568, duration: 0.807s, episode steps: 119, steps per second: 148, episode reward: -70.845, mean reward: -0.595 [-100.000, 12.344], mean action: 1.529 [0.000, 3.000],  loss: 29.213130, mse: 24401.401523, mean_q: 130.949448, mean_eps: 0.841597
 52941/100000: episode: 569, duration: 0.577s, episode steps:  80, steps per second: 139, episode reward: -70.851, mean reward: -0.886 [-100.000, 10.544], mean action: 1.625 [0.000, 3.000],  loss: 28.546965, mse: 24241.045508, mean_q: 130.750652, mean_eps: 0.841298
 53092/100000: episode: 570, duration: 1.117s, episode steps: 151, steps per second: 135, episode reward: -32.363, mean reward: -0.214 [-100.000, 39.004], mean action: 1.570 [0.000, 3.000],  loss: 31.881223, mse: 24354.441387, mean_q: 130.415013, mean_eps: 0.840952
 53159/100000: episode: 571, duration: 0.468s, episode steps:  67, steps per second: 143, episode reward: -105.832, mean reward: -1.580 [-100.000, 15.940], mean action: 1.358 [0.000, 3.000],  loss: 34.970370, mse: 23003.280929, mean_q: 123.269711, mean_eps: 0.840625
 53249/100000: episode: 572, duration: 0.684s, episode steps:  90, steps per second: 132, episode reward: -78.795, mean reward: -0.876 [-100.000, 18.301], mean action: 1.611 [0.000, 3.000],  loss: 26.820305, mse: 24241.698220, mean_q: 129.173550, mean_eps: 0.840390
 53355/100000: episode: 573, duration: 0.823s, episode steps: 106, steps per second: 129, episode reward: -256.809, mean reward: -2.423 [-100.000,  8.750], mean action: 1.566 [0.000, 3.000],  loss: 37.979261, mse: 24379.889409, mean_q: 129.332441, mean_eps: 0.840096
 53428/100000: episode: 574, duration: 0.520s, episode steps:  73, steps per second: 140, episode reward: -64.680, mean reward: -0.886 [-100.000,  9.949], mean action: 1.493 [0.000, 3.000],  loss: 30.374432, mse: 24047.752622, mean_q: 129.927283, mean_eps: 0.839827
 53525/100000: episode: 575, duration: 0.751s, episode steps:  97, steps per second: 129, episode reward: -90.018, mean reward: -0.928 [-100.000, 13.721], mean action: 1.392 [0.000, 3.000],  loss: 28.772538, mse: 24595.825930, mean_q: 129.908510, mean_eps: 0.839572
 53620/100000: episode: 576, duration: 0.738s, episode steps:  95, steps per second: 129, episode reward: -235.870, mean reward: -2.483 [-100.000,  6.614], mean action: 1.526 [0.000, 3.000],  loss: 26.330781, mse: 24397.651141, mean_q: 130.041047, mean_eps: 0.839284
 53682/100000: episode: 577, duration: 0.465s, episode steps:  62, steps per second: 133, episode reward: -68.331, mean reward: -1.102 [-100.000, 23.717], mean action: 1.468 [0.000, 3.000],  loss: 26.139980, mse: 23869.615581, mean_q: 128.215062, mean_eps: 0.839048
 53804/100000: episode: 578, duration: 0.896s, episode steps: 122, steps per second: 136, episode reward: -19.179, mean reward: -0.157 [-100.000, 83.514], mean action: 1.574 [0.000, 3.000],  loss: 37.136942, mse: 24553.571449, mean_q: 130.625085, mean_eps: 0.838772
 53917/100000: episode: 579, duration: 0.786s, episode steps: 113, steps per second: 144, episode reward: -118.752, mean reward: -1.051 [-100.000, 11.366], mean action: 1.637 [0.000, 3.000],  loss: 19.950783, mse: 24664.007484, mean_q: 130.305963, mean_eps: 0.838420
 54034/100000: episode: 580, duration: 0.784s, episode steps: 117, steps per second: 149, episode reward: -134.173, mean reward: -1.147 [-100.000,  5.726], mean action: 1.427 [0.000, 3.000],  loss: 30.363252, mse: 24015.883247, mean_q: 127.405887, mean_eps: 0.838075
 54148/100000: episode: 581, duration: 0.837s, episode steps: 114, steps per second: 136, episode reward: -294.454, mean reward: -2.583 [-100.000, 80.845], mean action: 1.544 [0.000, 3.000],  loss: 23.003969, mse: 24914.627202, mean_q: 130.989130, mean_eps: 0.837729
 54253/100000: episode: 582, duration: 0.745s, episode steps: 105, steps per second: 141, episode reward: -254.453, mean reward: -2.423 [-100.000,  4.425], mean action: 1.552 [0.000, 3.000],  loss: 25.925989, mse: 24641.734561, mean_q: 130.015302, mean_eps: 0.837400
 54353/100000: episode: 583, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: -160.792, mean reward: -1.608 [-100.000,  9.597], mean action: 1.300 [0.000, 3.000],  loss: 31.878549, mse: 24813.948027, mean_q: 132.117473, mean_eps: 0.837093
 54487/100000: episode: 584, duration: 1.036s, episode steps: 134, steps per second: 129, episode reward: -136.285, mean reward: -1.017 [-100.000,  5.118], mean action: 1.746 [0.000, 3.000],  loss: 26.326712, mse: 24479.276979, mean_q: 130.585770, mean_eps: 0.836741
 54601/100000: episode: 585, duration: 0.788s, episode steps: 114, steps per second: 145, episode reward: -158.654, mean reward: -1.392 [-100.000,  6.930], mean action: 1.430 [0.000, 3.000],  loss: 29.620877, mse: 24338.739618, mean_q: 128.382746, mean_eps: 0.836370
 54694/100000: episode: 586, duration: 0.653s, episode steps:  93, steps per second: 142, episode reward: -110.294, mean reward: -1.186 [-100.000,  8.038], mean action: 1.430 [0.000, 3.000],  loss: 26.128125, mse: 24852.273017, mean_q: 131.967300, mean_eps: 0.836059
 54767/100000: episode: 587, duration: 0.506s, episode steps:  73, steps per second: 144, episode reward: -22.259, mean reward: -0.305 [-100.000, 52.121], mean action: 1.562 [0.000, 3.000],  loss: 32.202986, mse: 24739.623582, mean_q: 131.168030, mean_eps: 0.835810
 54831/100000: episode: 588, duration: 0.436s, episode steps:  64, steps per second: 147, episode reward: -64.783, mean reward: -1.012 [-100.000, 16.868], mean action: 1.391 [0.000, 3.000],  loss: 25.275073, mse: 24550.491333, mean_q: 129.502046, mean_eps: 0.835604
 54921/100000: episode: 589, duration: 0.612s, episode steps:  90, steps per second: 147, episode reward: 13.612, mean reward:  0.151 [-100.000, 116.188], mean action: 1.567 [0.000, 3.000],  loss: 23.589338, mse: 24188.365126, mean_q: 129.940657, mean_eps: 0.835373
 54996/100000: episode: 590, duration: 0.503s, episode steps:  75, steps per second: 149, episode reward: -66.352, mean reward: -0.885 [-100.000,  6.463], mean action: 1.640 [0.000, 3.000],  loss: 22.614093, mse: 24156.043906, mean_q: 129.687632, mean_eps: 0.835126
 55089/100000: episode: 591, duration: 0.694s, episode steps:  93, steps per second: 134, episode reward: -261.662, mean reward: -2.814 [-100.000, 11.269], mean action: 1.366 [0.000, 3.000],  loss: 21.580733, mse: 23795.708039, mean_q: 126.721599, mean_eps: 0.834874
 55219/100000: episode: 592, duration: 0.877s, episode steps: 130, steps per second: 148, episode reward: -285.731, mean reward: -2.198 [-100.000, 82.510], mean action: 1.577 [0.000, 3.000],  loss: 35.684597, mse: 23646.332347, mean_q: 127.348828, mean_eps: 0.834540
 55353/100000: episode: 593, duration: 0.937s, episode steps: 134, steps per second: 143, episode reward: -114.427, mean reward: -0.854 [-100.000,  8.203], mean action: 1.627 [0.000, 3.000],  loss: 43.452408, mse: 23156.750583, mean_q: 125.589603, mean_eps: 0.834143
 55419/100000: episode: 594, duration: 0.459s, episode steps:  66, steps per second: 144, episode reward: -46.333, mean reward: -0.702 [-100.000, 17.398], mean action: 1.697 [0.000, 3.000],  loss: 31.033379, mse: 23454.637843, mean_q: 127.832805, mean_eps: 0.833843
 55523/100000: episode: 595, duration: 0.694s, episode steps: 104, steps per second: 150, episode reward: -114.123, mean reward: -1.097 [-100.000, 11.921], mean action: 1.606 [0.000, 3.000],  loss: 22.936268, mse: 23855.600811, mean_q: 128.526366, mean_eps: 0.833588
 55602/100000: episode: 596, duration: 0.542s, episode steps:  79, steps per second: 146, episode reward: -34.128, mean reward: -0.432 [-100.000, 21.978], mean action: 1.797 [0.000, 3.000],  loss: 24.513434, mse: 23249.040966, mean_q: 126.451328, mean_eps: 0.833314
 55676/100000: episode: 597, duration: 0.555s, episode steps:  74, steps per second: 133, episode reward: -87.503, mean reward: -1.182 [-100.000,  9.878], mean action: 1.554 [0.000, 3.000],  loss: 28.285802, mse: 23495.146722, mean_q: 129.997142, mean_eps: 0.833084
 55766/100000: episode: 598, duration: 0.660s, episode steps:  90, steps per second: 136, episode reward: -89.706, mean reward: -0.997 [-100.000, 13.077], mean action: 1.644 [0.000, 3.000],  loss: 29.263146, mse: 23052.078711, mean_q: 126.974871, mean_eps: 0.832839
 55869/100000: episode: 599, duration: 0.723s, episode steps: 103, steps per second: 142, episode reward: -176.631, mean reward: -1.715 [-100.000,  5.249], mean action: 1.534 [0.000, 3.000],  loss: 45.555896, mse: 22594.158981, mean_q: 125.044051, mean_eps: 0.832549
 55942/100000: episode: 600, duration: 0.573s, episode steps:  73, steps per second: 127, episode reward: -251.603, mean reward: -3.447 [-100.000, 67.895], mean action: 1.534 [0.000, 3.000],  loss: 20.832621, mse: 23070.206951, mean_q: 126.590651, mean_eps: 0.832285
 56070/100000: episode: 601, duration: 0.937s, episode steps: 128, steps per second: 137, episode reward: -76.885, mean reward: -0.601 [-100.000, 17.511], mean action: 1.594 [0.000, 3.000],  loss: 29.441274, mse: 22967.548218, mean_q: 126.320002, mean_eps: 0.831983
 56159/100000: episode: 602, duration: 0.690s, episode steps:  89, steps per second: 129, episode reward: -65.828, mean reward: -0.740 [-100.000,  7.055], mean action: 1.551 [0.000, 3.000],  loss: 30.902971, mse: 23108.700568, mean_q: 127.369106, mean_eps: 0.831658
 56233/100000: episode: 603, duration: 0.559s, episode steps:  74, steps per second: 132, episode reward: -60.998, mean reward: -0.824 [-100.000, 17.476], mean action: 1.514 [0.000, 3.000],  loss: 20.091382, mse: 23265.029798, mean_q: 128.291088, mean_eps: 0.831414
 56307/100000: episode: 604, duration: 0.592s, episode steps:  74, steps per second: 125, episode reward: -65.779, mean reward: -0.889 [-100.000, 17.844], mean action: 1.338 [0.000, 3.000],  loss: 19.234057, mse: 23084.217087, mean_q: 127.774352, mean_eps: 0.831192
 56425/100000: episode: 605, duration: 0.843s, episode steps: 118, steps per second: 140, episode reward: -110.793, mean reward: -0.939 [-100.000, 11.830], mean action: 1.475 [0.000, 3.000],  loss: 28.885053, mse: 23067.377152, mean_q: 126.700162, mean_eps: 0.830903
 56501/100000: episode: 606, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -96.798, mean reward: -1.274 [-100.000, 13.979], mean action: 1.553 [0.000, 3.000],  loss: 24.256604, mse: 23678.575863, mean_q: 128.996485, mean_eps: 0.830612
 56640/100000: episode: 607, duration: 0.999s, episode steps: 139, steps per second: 139, episode reward: -140.409, mean reward: -1.010 [-100.000,  3.904], mean action: 1.640 [0.000, 3.000],  loss: 29.888085, mse: 23862.931191, mean_q: 130.554057, mean_eps: 0.830290
 56735/100000: episode: 608, duration: 0.652s, episode steps:  95, steps per second: 146, episode reward: -100.694, mean reward: -1.060 [-100.000,  9.753], mean action: 1.674 [0.000, 3.000],  loss: 36.934980, mse: 24338.550000, mean_q: 132.351776, mean_eps: 0.829939
 56817/100000: episode: 609, duration: 0.620s, episode steps:  82, steps per second: 132, episode reward: -67.274, mean reward: -0.820 [-100.000, 10.912], mean action: 1.415 [0.000, 3.000],  loss: 22.009599, mse: 23909.715701, mean_q: 130.208548, mean_eps: 0.829674
 56911/100000: episode: 610, duration: 0.800s, episode steps:  94, steps per second: 118, episode reward: -99.614, mean reward: -1.060 [-100.000,  8.576], mean action: 1.596 [0.000, 3.000],  loss: 21.333760, mse: 24058.988344, mean_q: 130.732211, mean_eps: 0.829410
 56987/100000: episode: 611, duration: 0.619s, episode steps:  76, steps per second: 123, episode reward: -95.477, mean reward: -1.256 [-100.000,  6.767], mean action: 1.605 [0.000, 3.000],  loss: 19.192514, mse: 24604.156173, mean_q: 133.234219, mean_eps: 0.829155
 57078/100000: episode: 612, duration: 0.738s, episode steps:  91, steps per second: 123, episode reward: -116.483, mean reward: -1.280 [-100.000,  7.415], mean action: 1.758 [0.000, 3.000],  loss: 33.886647, mse: 24546.799386, mean_q: 132.444940, mean_eps: 0.828904
 57171/100000: episode: 613, duration: 0.744s, episode steps:  93, steps per second: 125, episode reward: -101.177, mean reward: -1.088 [-100.000, 15.768], mean action: 1.527 [0.000, 3.000],  loss: 38.444038, mse: 24664.784610, mean_q: 132.066560, mean_eps: 0.828628
 57305/100000: episode: 614, duration: 1.063s, episode steps: 134, steps per second: 126, episode reward: -67.777, mean reward: -0.506 [-100.000, 16.766], mean action: 1.500 [0.000, 3.000],  loss: 34.781995, mse: 25360.481664, mean_q: 136.941732, mean_eps: 0.828287
 57411/100000: episode: 615, duration: 0.938s, episode steps: 106, steps per second: 113, episode reward: -192.919, mean reward: -1.820 [-100.000,  1.632], mean action: 1.509 [0.000, 3.000],  loss: 30.820418, mse: 24467.654905, mean_q: 132.558438, mean_eps: 0.827928
 57518/100000: episode: 616, duration: 0.815s, episode steps: 107, steps per second: 131, episode reward: -163.994, mean reward: -1.533 [-100.000,  2.308], mean action: 1.589 [0.000, 3.000],  loss: 45.653181, mse: 24289.395097, mean_q: 132.911887, mean_eps: 0.827608
 57601/100000: episode: 617, duration: 0.638s, episode steps:  83, steps per second: 130, episode reward: -96.224, mean reward: -1.159 [-100.000, 11.291], mean action: 1.566 [0.000, 3.000],  loss: 23.181071, mse: 24247.933170, mean_q: 131.170295, mean_eps: 0.827323
 57697/100000: episode: 618, duration: 0.756s, episode steps:  96, steps per second: 127, episode reward: -75.136, mean reward: -0.783 [-100.000, 17.137], mean action: 1.771 [0.000, 3.000],  loss: 26.331340, mse: 24480.810913, mean_q: 133.077779, mean_eps: 0.827055
 57781/100000: episode: 619, duration: 0.605s, episode steps:  84, steps per second: 139, episode reward: -46.862, mean reward: -0.558 [-100.000,  7.413], mean action: 1.679 [0.000, 3.000],  loss: 41.396349, mse: 24876.927734, mean_q: 135.137691, mean_eps: 0.826784
 57863/100000: episode: 620, duration: 0.589s, episode steps:  82, steps per second: 139, episode reward: -173.900, mean reward: -2.121 [-100.000, 15.239], mean action: 1.598 [0.000, 3.000],  loss: 39.108803, mse: 24651.400676, mean_q: 131.502701, mean_eps: 0.826536
 57933/100000: episode: 621, duration: 0.557s, episode steps:  70, steps per second: 126, episode reward: -93.760, mean reward: -1.339 [-100.000,  8.994], mean action: 1.443 [0.000, 3.000],  loss: 26.095736, mse: 24834.357868, mean_q: 132.177739, mean_eps: 0.826308
 57999/100000: episode: 622, duration: 0.503s, episode steps:  66, steps per second: 131, episode reward: -79.397, mean reward: -1.203 [-100.000, 14.227], mean action: 1.833 [0.000, 3.000],  loss: 32.570545, mse: 24579.250621, mean_q: 129.062906, mean_eps: 0.826103
 58106/100000: episode: 623, duration: 0.839s, episode steps: 107, steps per second: 128, episode reward: -200.809, mean reward: -1.877 [-100.000, 28.173], mean action: 1.626 [0.000, 3.000],  loss: 33.769432, mse: 24915.234631, mean_q: 133.545839, mean_eps: 0.825844
 58183/100000: episode: 624, duration: 0.567s, episode steps:  77, steps per second: 136, episode reward: -108.313, mean reward: -1.407 [-100.000,  9.382], mean action: 1.429 [0.000, 3.000],  loss: 37.536901, mse: 25020.510425, mean_q: 132.566147, mean_eps: 0.825568
 58294/100000: episode: 625, duration: 0.794s, episode steps: 111, steps per second: 140, episode reward: -61.603, mean reward: -0.555 [-100.000, 13.727], mean action: 1.495 [0.000, 3.000],  loss: 36.132573, mse: 24265.715618, mean_q: 130.295776, mean_eps: 0.825286
 58427/100000: episode: 626, duration: 0.983s, episode steps: 133, steps per second: 135, episode reward: -91.972, mean reward: -0.692 [-100.000, 33.296], mean action: 1.504 [0.000, 3.000],  loss: 28.654597, mse: 24602.483127, mean_q: 131.306342, mean_eps: 0.824920
 58518/100000: episode: 627, duration: 0.651s, episode steps:  91, steps per second: 140, episode reward: -87.155, mean reward: -0.958 [-100.000, 17.861], mean action: 1.516 [0.000, 3.000],  loss: 34.488034, mse: 24501.294257, mean_q: 131.490994, mean_eps: 0.824584
 58597/100000: episode: 628, duration: 0.590s, episode steps:  79, steps per second: 134, episode reward: -53.505, mean reward: -0.677 [-100.000, 12.638], mean action: 1.456 [0.000, 3.000],  loss: 39.794896, mse: 24700.788914, mean_q: 129.736274, mean_eps: 0.824329
 58697/100000: episode: 629, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -105.136, mean reward: -1.051 [-100.000,  7.193], mean action: 1.670 [0.000, 3.000],  loss: 31.546917, mse: 24859.173652, mean_q: 134.101240, mean_eps: 0.824060
 58778/100000: episode: 630, duration: 0.567s, episode steps:  81, steps per second: 143, episode reward: -70.231, mean reward: -0.867 [-100.000, 10.300], mean action: 1.827 [0.000, 3.000],  loss: 32.579157, mse: 24638.162254, mean_q: 131.281542, mean_eps: 0.823789
 59778/100000: episode: 631, duration: 7.494s, episode steps: 1000, steps per second: 133, episode reward: 46.164, mean reward:  0.046 [-24.437, 91.884], mean action: 1.346 [0.000, 3.000],  loss: 35.112129, mse: 25037.606645, mean_q: 131.509381, mean_eps: 0.822168
 59857/100000: episode: 632, duration: 0.577s, episode steps:  79, steps per second: 137, episode reward: -128.756, mean reward: -1.630 [-100.000,  5.646], mean action: 1.380 [0.000, 3.000],  loss: 29.196348, mse: 25970.152443, mean_q: 134.595289, mean_eps: 0.820549
 59944/100000: episode: 633, duration: 0.654s, episode steps:  87, steps per second: 133, episode reward: -82.981, mean reward: -0.954 [-100.000,  7.144], mean action: 1.644 [0.000, 3.000],  loss: 29.971417, mse: 25962.158383, mean_q: 135.409510, mean_eps: 0.820300
 60015/100000: episode: 634, duration: 0.589s, episode steps:  71, steps per second: 120, episode reward: -38.075, mean reward: -0.536 [-100.000, 14.912], mean action: 1.549 [0.000, 3.000],  loss: 25.534649, mse: 25716.192974, mean_q: 133.429973, mean_eps: 0.820063
 60142/100000: episode: 635, duration: 0.912s, episode steps: 127, steps per second: 139, episode reward: -37.347, mean reward: -0.294 [-100.000, 12.845], mean action: 1.748 [0.000, 3.000],  loss: 30.130001, mse: 26391.281696, mean_q: 136.824320, mean_eps: 0.819766
 60250/100000: episode: 636, duration: 0.767s, episode steps: 108, steps per second: 141, episode reward: -185.690, mean reward: -1.719 [-100.000,  6.260], mean action: 1.546 [0.000, 3.000],  loss: 23.246690, mse: 25700.195710, mean_q: 134.907902, mean_eps: 0.819414
 60412/100000: episode: 637, duration: 1.115s, episode steps: 162, steps per second: 145, episode reward: -72.659, mean reward: -0.449 [-100.000, 45.832], mean action: 1.685 [0.000, 3.000],  loss: 38.605711, mse: 26300.262189, mean_q: 139.214510, mean_eps: 0.819008
 60488/100000: episode: 638, duration: 0.536s, episode steps:  76, steps per second: 142, episode reward: -171.429, mean reward: -2.256 [-100.000,  8.491], mean action: 1.592 [0.000, 3.000],  loss: 24.105999, mse: 26524.028860, mean_q: 138.080149, mean_eps: 0.818651
 60578/100000: episode: 639, duration: 0.670s, episode steps:  90, steps per second: 134, episode reward: -85.852, mean reward: -0.954 [-100.000,  7.155], mean action: 1.478 [0.000, 3.000],  loss: 31.279892, mse: 26334.461957, mean_q: 139.147444, mean_eps: 0.818402
 60653/100000: episode: 640, duration: 0.569s, episode steps:  75, steps per second: 132, episode reward: -114.407, mean reward: -1.525 [-100.000,  6.217], mean action: 1.680 [0.000, 3.000],  loss: 22.335493, mse: 26981.577187, mean_q: 139.050705, mean_eps: 0.818155
 60750/100000: episode: 641, duration: 0.732s, episode steps:  97, steps per second: 133, episode reward: -139.797, mean reward: -1.441 [-100.000,  6.139], mean action: 1.577 [0.000, 3.000],  loss: 35.461191, mse: 26786.570735, mean_q: 137.814743, mean_eps: 0.817897
 60838/100000: episode: 642, duration: 0.645s, episode steps:  88, steps per second: 136, episode reward: -115.525, mean reward: -1.313 [-100.000, 11.095], mean action: 1.443 [0.000, 3.000],  loss: 40.675072, mse: 26743.473455, mean_q: 140.274469, mean_eps: 0.817619
 60942/100000: episode: 643, duration: 0.778s, episode steps: 104, steps per second: 134, episode reward: -172.603, mean reward: -1.660 [-100.000,  8.503], mean action: 1.548 [0.000, 3.000],  loss: 42.820685, mse: 27077.740610, mean_q: 140.132353, mean_eps: 0.817331
 61044/100000: episode: 644, duration: 0.778s, episode steps: 102, steps per second: 131, episode reward: -203.398, mean reward: -1.994 [-100.000,  9.368], mean action: 1.647 [0.000, 3.000],  loss: 35.212804, mse: 26524.854511, mean_q: 137.388154, mean_eps: 0.817023
 61134/100000: episode: 645, duration: 0.760s, episode steps:  90, steps per second: 118, episode reward: -400.908, mean reward: -4.455 [-100.000, 97.242], mean action: 1.744 [0.000, 3.000],  loss: 31.710290, mse: 26809.325000, mean_q: 140.566662, mean_eps: 0.816735
 61224/100000: episode: 646, duration: 0.664s, episode steps:  90, steps per second: 136, episode reward: -104.173, mean reward: -1.157 [-100.000, 15.117], mean action: 1.656 [0.000, 3.000],  loss: 48.317135, mse: 27211.414974, mean_q: 142.557175, mean_eps: 0.816465
 61347/100000: episode: 647, duration: 0.833s, episode steps: 123, steps per second: 148, episode reward: -74.579, mean reward: -0.606 [-100.000,  7.491], mean action: 1.488 [0.000, 3.000],  loss: 53.449519, mse: 26875.703951, mean_q: 140.194771, mean_eps: 0.816145
 61435/100000: episode: 648, duration: 0.643s, episode steps:  88, steps per second: 137, episode reward: -105.381, mean reward: -1.198 [-100.000, 10.690], mean action: 1.477 [0.000, 3.000],  loss: 50.925570, mse: 27461.609464, mean_q: 141.504350, mean_eps: 0.815829
 61511/100000: episode: 649, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -125.628, mean reward: -1.653 [-100.000, 11.871], mean action: 1.566 [0.000, 3.000],  loss: 70.847031, mse: 26448.780248, mean_q: 137.910388, mean_eps: 0.815582
 61630/100000: episode: 650, duration: 0.813s, episode steps: 119, steps per second: 146, episode reward: -71.754, mean reward: -0.603 [-100.000, 11.176], mean action: 1.555 [0.000, 3.000],  loss: 33.084881, mse: 27005.327009, mean_q: 140.991566, mean_eps: 0.815290
 61735/100000: episode: 651, duration: 0.739s, episode steps: 105, steps per second: 142, episode reward: -125.282, mean reward: -1.193 [-100.000, 46.472], mean action: 1.476 [0.000, 3.000],  loss: 30.220639, mse: 26950.146801, mean_q: 138.976450, mean_eps: 0.814954
 61820/100000: episode: 652, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -144.716, mean reward: -1.703 [-100.000, 17.222], mean action: 1.565 [0.000, 3.000],  loss: 25.404921, mse: 27727.718658, mean_q: 144.429713, mean_eps: 0.814669
 61904/100000: episode: 653, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -92.384, mean reward: -1.100 [-100.000,  9.512], mean action: 1.607 [0.000, 3.000],  loss: 40.311823, mse: 28056.107561, mean_q: 143.902797, mean_eps: 0.814416
 61976/100000: episode: 654, duration: 0.549s, episode steps:  72, steps per second: 131, episode reward: -77.923, mean reward: -1.082 [-100.000, 17.955], mean action: 1.528 [0.000, 3.000],  loss: 47.745995, mse: 27642.771837, mean_q: 141.413775, mean_eps: 0.814182
 62070/100000: episode: 655, duration: 0.699s, episode steps:  94, steps per second: 135, episode reward: -128.928, mean reward: -1.372 [-100.000, 10.730], mean action: 1.798 [0.000, 3.000],  loss: 35.991259, mse: 28233.803399, mean_q: 144.177262, mean_eps: 0.813932
 62148/100000: episode: 656, duration: 0.599s, episode steps:  78, steps per second: 130, episode reward: -94.739, mean reward: -1.215 [-100.000,  6.696], mean action: 1.551 [0.000, 3.000],  loss: 43.851534, mse: 28524.028320, mean_q: 145.151889, mean_eps: 0.813674
 62251/100000: episode: 657, duration: 0.733s, episode steps: 103, steps per second: 140, episode reward: -112.278, mean reward: -1.090 [-100.000, 10.794], mean action: 1.699 [0.000, 3.000],  loss: 36.915683, mse: 28338.850937, mean_q: 144.900231, mean_eps: 0.813403
 62337/100000: episode: 658, duration: 0.640s, episode steps:  86, steps per second: 134, episode reward: -120.440, mean reward: -1.400 [-100.000, 13.475], mean action: 1.570 [0.000, 3.000],  loss: 24.519518, mse: 28676.698719, mean_q: 146.573097, mean_eps: 0.813119
 62408/100000: episode: 659, duration: 0.540s, episode steps:  71, steps per second: 132, episode reward: -56.727, mean reward: -0.799 [-100.000,  6.783], mean action: 1.662 [0.000, 3.000],  loss: 55.695490, mse: 28226.149565, mean_q: 143.098925, mean_eps: 0.812884
 62489/100000: episode: 660, duration: 0.558s, episode steps:  81, steps per second: 145, episode reward: -124.326, mean reward: -1.535 [-100.000,  6.374], mean action: 1.605 [0.000, 3.000],  loss: 30.563719, mse: 29016.263069, mean_q: 148.010224, mean_eps: 0.812656
 62586/100000: episode: 661, duration: 0.655s, episode steps:  97, steps per second: 148, episode reward: -66.380, mean reward: -0.684 [-100.000, 13.628], mean action: 1.423 [0.000, 3.000],  loss: 51.875696, mse: 28186.029921, mean_q: 143.355162, mean_eps: 0.812389
 62675/100000: episode: 662, duration: 0.646s, episode steps:  89, steps per second: 138, episode reward: -77.937, mean reward: -0.876 [-100.000, 26.172], mean action: 1.607 [0.000, 3.000],  loss: 35.788132, mse: 29342.060020, mean_q: 149.589713, mean_eps: 0.812110
 62782/100000: episode: 663, duration: 0.725s, episode steps: 107, steps per second: 148, episode reward: -147.617, mean reward: -1.380 [-100.000,  8.568], mean action: 1.477 [0.000, 3.000],  loss: 36.216095, mse: 29819.667896, mean_q: 150.354926, mean_eps: 0.811816
 62865/100000: episode: 664, duration: 0.559s, episode steps:  83, steps per second: 148, episode reward: -50.953, mean reward: -0.614 [-100.000, 10.951], mean action: 1.458 [0.000, 3.000],  loss: 26.656432, mse: 30071.842362, mean_q: 149.448770, mean_eps: 0.811531
 62926/100000: episode: 665, duration: 0.436s, episode steps:  61, steps per second: 140, episode reward: -72.641, mean reward: -1.191 [-100.000, 24.969], mean action: 1.738 [0.000, 3.000],  loss: 30.607945, mse: 29560.877818, mean_q: 145.490748, mean_eps: 0.811315
 62995/100000: episode: 666, duration: 0.491s, episode steps:  69, steps per second: 141, episode reward: -96.436, mean reward: -1.398 [-100.000, 21.470], mean action: 1.609 [0.000, 3.000],  loss: 24.763325, mse: 29971.875821, mean_q: 150.005909, mean_eps: 0.811120
 63096/100000: episode: 667, duration: 0.699s, episode steps: 101, steps per second: 145, episode reward: -107.344, mean reward: -1.063 [-100.000, 11.958], mean action: 1.604 [0.000, 3.000],  loss: 53.094018, mse: 29960.854521, mean_q: 148.807842, mean_eps: 0.810865
 63191/100000: episode: 668, duration: 0.655s, episode steps:  95, steps per second: 145, episode reward: -200.118, mean reward: -2.107 [-100.000,  6.831], mean action: 1.537 [0.000, 3.000],  loss: 55.453361, mse: 30209.018873, mean_q: 150.257401, mean_eps: 0.810571
 63269/100000: episode: 669, duration: 0.582s, episode steps:  78, steps per second: 134, episode reward: -201.934, mean reward: -2.589 [-100.000, 27.683], mean action: 1.462 [0.000, 3.000],  loss: 34.975096, mse: 30530.726237, mean_q: 151.381047, mean_eps: 0.810311
 63345/100000: episode: 670, duration: 0.548s, episode steps:  76, steps per second: 139, episode reward: -64.769, mean reward: -0.852 [-100.000,  9.114], mean action: 1.803 [0.000, 3.000],  loss: 26.847067, mse: 30395.421207, mean_q: 150.409733, mean_eps: 0.810080
 63471/100000: episode: 671, duration: 0.863s, episode steps: 126, steps per second: 146, episode reward: -125.641, mean reward: -0.997 [-100.000,  7.343], mean action: 1.619 [0.000, 3.000],  loss: 40.262208, mse: 30931.682137, mean_q: 151.375762, mean_eps: 0.809778
 63584/100000: episode: 672, duration: 0.797s, episode steps: 113, steps per second: 142, episode reward: -108.137, mean reward: -0.957 [-100.000,  9.184], mean action: 1.566 [0.000, 3.000],  loss: 33.342418, mse: 30697.192340, mean_q: 149.569857, mean_eps: 0.809419
 63671/100000: episode: 673, duration: 0.607s, episode steps:  87, steps per second: 143, episode reward: -118.249, mean reward: -1.359 [-100.000, 10.251], mean action: 1.437 [0.000, 3.000],  loss: 37.058529, mse: 31622.799142, mean_q: 155.410699, mean_eps: 0.809119
 63757/100000: episode: 674, duration: 0.583s, episode steps:  86, steps per second: 148, episode reward: -373.448, mean reward: -4.342 [-100.000,  5.267], mean action: 1.640 [0.000, 3.000],  loss: 24.729292, mse: 30641.327989, mean_q: 150.719790, mean_eps: 0.808860
 63845/100000: episode: 675, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -108.042, mean reward: -1.228 [-100.000,  8.230], mean action: 1.557 [0.000, 3.000],  loss: 34.457508, mse: 30873.816917, mean_q: 153.160175, mean_eps: 0.808598
 63980/100000: episode: 676, duration: 0.934s, episode steps: 135, steps per second: 144, episode reward: -82.534, mean reward: -0.611 [-100.000, 10.529], mean action: 1.533 [0.000, 3.000],  loss: 47.954770, mse: 30783.575058, mean_q: 151.407734, mean_eps: 0.808264
 64055/100000: episode: 677, duration: 0.501s, episode steps:  75, steps per second: 150, episode reward: -110.237, mean reward: -1.470 [-100.000, 16.806], mean action: 1.573 [0.000, 3.000],  loss: 30.085289, mse: 32223.683151, mean_q: 157.333279, mean_eps: 0.807949
 64161/100000: episode: 678, duration: 0.714s, episode steps: 106, steps per second: 149, episode reward: -159.437, mean reward: -1.504 [-100.000, 16.461], mean action: 1.736 [0.000, 3.000],  loss: 31.856485, mse: 31771.849020, mean_q: 153.479601, mean_eps: 0.807678
 64225/100000: episode: 679, duration: 0.465s, episode steps:  64, steps per second: 138, episode reward: -164.503, mean reward: -2.570 [-100.000,  8.071], mean action: 1.750 [0.000, 3.000],  loss: 35.099749, mse: 31287.159271, mean_q: 149.096225, mean_eps: 0.807422
 64347/100000: episode: 680, duration: 0.842s, episode steps: 122, steps per second: 145, episode reward: -119.841, mean reward: -0.982 [-100.000, 22.559], mean action: 1.607 [0.000, 3.000],  loss: 39.053595, mse: 31290.018363, mean_q: 150.423499, mean_eps: 0.807143
 64442/100000: episode: 681, duration: 0.642s, episode steps:  95, steps per second: 148, episode reward: -144.520, mean reward: -1.521 [-100.000, 35.804], mean action: 1.737 [0.000, 3.000],  loss: 27.216346, mse: 31577.090831, mean_q: 150.041990, mean_eps: 0.806818
 64538/100000: episode: 682, duration: 0.678s, episode steps:  96, steps per second: 142, episode reward: -68.653, mean reward: -0.715 [-100.000, 20.533], mean action: 1.615 [0.000, 3.000],  loss: 34.903122, mse: 30956.509562, mean_q: 147.821870, mean_eps: 0.806531
 64640/100000: episode: 683, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: -74.745, mean reward: -0.733 [-100.000, 18.646], mean action: 1.598 [0.000, 3.000],  loss: 52.911620, mse: 30834.206840, mean_q: 145.648630, mean_eps: 0.806234
 64767/100000: episode: 684, duration: 0.849s, episode steps: 127, steps per second: 150, episode reward: -120.193, mean reward: -0.946 [-100.000,  7.458], mean action: 1.543 [0.000, 3.000],  loss: 43.641382, mse: 31429.924428, mean_q: 147.678323, mean_eps: 0.805891
 64867/100000: episode: 685, duration: 0.744s, episode steps: 100, steps per second: 134, episode reward: -110.760, mean reward: -1.108 [-100.000,  7.524], mean action: 1.710 [0.000, 3.000],  loss: 62.147930, mse: 31718.056660, mean_q: 149.029631, mean_eps: 0.805550
 64967/100000: episode: 686, duration: 0.745s, episode steps: 100, steps per second: 134, episode reward: -161.351, mean reward: -1.614 [-100.000,  6.744], mean action: 1.470 [0.000, 3.000],  loss: 25.115924, mse: 32455.520527, mean_q: 153.089932, mean_eps: 0.805250
 65071/100000: episode: 687, duration: 0.779s, episode steps: 104, steps per second: 133, episode reward: -106.453, mean reward: -1.024 [-100.000, 17.579], mean action: 1.529 [0.000, 3.000],  loss: 44.272177, mse: 30897.736384, mean_q: 145.636067, mean_eps: 0.804945
 65193/100000: episode: 688, duration: 0.906s, episode steps: 122, steps per second: 135, episode reward: -463.359, mean reward: -3.798 [-100.000, 64.286], mean action: 1.467 [0.000, 3.000],  loss: 55.704172, mse: 31221.932665, mean_q: 147.582955, mean_eps: 0.804606
 65258/100000: episode: 689, duration: 0.455s, episode steps:  65, steps per second: 143, episode reward: -52.462, mean reward: -0.807 [-100.000, 15.368], mean action: 1.369 [0.000, 3.000],  loss: 65.227856, mse: 30674.663221, mean_q: 145.209735, mean_eps: 0.804325
 65354/100000: episode: 690, duration: 0.653s, episode steps:  96, steps per second: 147, episode reward: -147.754, mean reward: -1.539 [-100.000, 14.513], mean action: 1.469 [0.000, 3.000],  loss: 40.758079, mse: 32580.827983, mean_q: 156.005123, mean_eps: 0.804083
 65435/100000: episode: 691, duration: 0.685s, episode steps:  81, steps per second: 118, episode reward: -86.537, mean reward: -1.068 [-100.000, 18.691], mean action: 1.519 [0.000, 3.000],  loss: 32.035306, mse: 31435.848741, mean_q: 150.693018, mean_eps: 0.803818
 65551/100000: episode: 692, duration: 0.862s, episode steps: 116, steps per second: 135, episode reward: -160.871, mean reward: -1.387 [-100.000, 11.419], mean action: 1.733 [0.000, 3.000],  loss: 36.852835, mse: 31852.203041, mean_q: 152.809007, mean_eps: 0.803522
 65654/100000: episode: 693, duration: 0.712s, episode steps: 103, steps per second: 145, episode reward: -125.541, mean reward: -1.219 [-100.000, 16.784], mean action: 1.689 [0.000, 3.000],  loss: 29.978179, mse: 31453.130840, mean_q: 150.724158, mean_eps: 0.803194
 65718/100000: episode: 694, duration: 0.471s, episode steps:  64, steps per second: 136, episode reward: -116.883, mean reward: -1.826 [-100.000,  7.353], mean action: 1.609 [0.000, 3.000],  loss: 18.596670, mse: 31439.874390, mean_q: 151.112130, mean_eps: 0.802943
 65817/100000: episode: 695, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -78.219, mean reward: -0.790 [-100.000,  8.655], mean action: 1.586 [0.000, 3.000],  loss: 29.568905, mse: 31231.164812, mean_q: 151.913950, mean_eps: 0.802699
 65898/100000: episode: 696, duration: 0.629s, episode steps:  81, steps per second: 129, episode reward: -204.828, mean reward: -2.529 [-100.000, 28.071], mean action: 1.778 [0.000, 3.000],  loss: 27.578059, mse: 31742.893060, mean_q: 152.436102, mean_eps: 0.802429
 65983/100000: episode: 697, duration: 0.722s, episode steps:  85, steps per second: 118, episode reward: -105.913, mean reward: -1.246 [-100.000, 26.301], mean action: 1.729 [0.000, 3.000],  loss: 45.711062, mse: 32404.476264, mean_q: 155.021861, mean_eps: 0.802180
 66079/100000: episode: 698, duration: 0.754s, episode steps:  96, steps per second: 127, episode reward: -86.805, mean reward: -0.904 [-100.000, 10.928], mean action: 1.583 [0.000, 3.000],  loss: 39.241223, mse: 32218.525085, mean_q: 154.986329, mean_eps: 0.801909
 66155/100000: episode: 699, duration: 0.591s, episode steps:  76, steps per second: 129, episode reward: -131.288, mean reward: -1.727 [-100.000,  6.686], mean action: 1.461 [0.000, 3.000],  loss: 24.245193, mse: 31942.356420, mean_q: 152.272122, mean_eps: 0.801651
 66270/100000: episode: 700, duration: 0.916s, episode steps: 115, steps per second: 126, episode reward: -29.931, mean reward: -0.260 [-100.000, 12.931], mean action: 1.670 [0.000, 3.000],  loss: 45.966904, mse: 32348.443461, mean_q: 154.257454, mean_eps: 0.801364
 66365/100000: episode: 701, duration: 0.731s, episode steps:  95, steps per second: 130, episode reward: -120.494, mean reward: -1.268 [-100.000, 11.018], mean action: 1.537 [0.000, 3.000],  loss: 23.799191, mse: 32384.308141, mean_q: 154.118544, mean_eps: 0.801049
 66438/100000: episode: 702, duration: 0.564s, episode steps:  73, steps per second: 130, episode reward: -38.418, mean reward: -0.526 [-100.000,  8.927], mean action: 1.493 [0.000, 3.000],  loss: 24.288438, mse: 32144.015518, mean_q: 151.120122, mean_eps: 0.800797
 66524/100000: episode: 703, duration: 0.645s, episode steps:  86, steps per second: 133, episode reward: -91.175, mean reward: -1.060 [-100.000, 12.229], mean action: 1.663 [0.000, 3.000],  loss: 24.143584, mse: 32352.928257, mean_q: 152.922694, mean_eps: 0.800558
 66586/100000: episode: 704, duration: 0.446s, episode steps:  62, steps per second: 139, episode reward: -90.160, mean reward: -1.454 [-100.000, 11.122], mean action: 1.339 [0.000, 3.000],  loss: 33.659882, mse: 32679.451487, mean_q: 155.938319, mean_eps: 0.800337
 66698/100000: episode: 705, duration: 0.779s, episode steps: 112, steps per second: 144, episode reward: -85.049, mean reward: -0.759 [-100.000,  8.865], mean action: 1.402 [0.000, 3.000],  loss: 29.969162, mse: 32298.121983, mean_q: 153.852008, mean_eps: 0.800075
 66780/100000: episode: 706, duration: 0.577s, episode steps:  82, steps per second: 142, episode reward: -84.614, mean reward: -1.032 [-100.000, 11.199], mean action: 1.768 [0.000, 3.000],  loss: 24.269852, mse: 33199.931450, mean_q: 156.420572, mean_eps: 0.799785
 66868/100000: episode: 707, duration: 0.661s, episode steps:  88, steps per second: 133, episode reward: -36.970, mean reward: -0.420 [-100.000, 11.874], mean action: 1.739 [0.000, 3.000],  loss: 40.876814, mse: 33090.191406, mean_q: 156.212128, mean_eps: 0.799530
 66975/100000: episode: 708, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -83.994, mean reward: -0.785 [-100.000,  7.818], mean action: 1.617 [0.000, 3.000],  loss: 34.024224, mse: 32966.566917, mean_q: 155.266880, mean_eps: 0.799237
 67065/100000: episode: 709, duration: 0.611s, episode steps:  90, steps per second: 147, episode reward: -77.533, mean reward: -0.861 [-100.000, 18.217], mean action: 1.544 [0.000, 3.000],  loss: 24.114528, mse: 33380.929861, mean_q: 155.514859, mean_eps: 0.798941
 67156/100000: episode: 710, duration: 0.753s, episode steps:  91, steps per second: 121, episode reward: -66.367, mean reward: -0.729 [-100.000, 65.650], mean action: 1.604 [0.000, 3.000],  loss: 28.584875, mse: 33558.341453, mean_q: 157.184957, mean_eps: 0.798670
 67260/100000: episode: 711, duration: 0.860s, episode steps: 104, steps per second: 121, episode reward: -117.122, mean reward: -1.126 [-100.000,  8.485], mean action: 1.452 [0.000, 3.000],  loss: 47.051016, mse: 33717.293513, mean_q: 155.698303, mean_eps: 0.798377
 67332/100000: episode: 712, duration: 0.524s, episode steps:  72, steps per second: 137, episode reward: -103.030, mean reward: -1.431 [-100.000, 19.709], mean action: 1.694 [0.000, 3.000],  loss: 32.244002, mse: 32468.333713, mean_q: 150.785710, mean_eps: 0.798113
 67402/100000: episode: 713, duration: 0.515s, episode steps:  70, steps per second: 136, episode reward: -60.989, mean reward: -0.871 [-100.000, 11.289], mean action: 1.486 [0.000, 3.000],  loss: 30.912911, mse: 32884.465876, mean_q: 153.071530, mean_eps: 0.797901
 67477/100000: episode: 714, duration: 0.545s, episode steps:  75, steps per second: 138, episode reward: -30.847, mean reward: -0.411 [-100.000, 18.756], mean action: 1.560 [0.000, 3.000],  loss: 25.582638, mse: 34447.558411, mean_q: 157.958775, mean_eps: 0.797683
 67619/100000: episode: 715, duration: 1.002s, episode steps: 142, steps per second: 142, episode reward: -173.808, mean reward: -1.224 [-100.000, 80.050], mean action: 1.613 [0.000, 3.000],  loss: 29.724518, mse: 33670.716370, mean_q: 154.376484, mean_eps: 0.797357
 67736/100000: episode: 716, duration: 0.834s, episode steps: 117, steps per second: 140, episode reward: -45.973, mean reward: -0.393 [-100.000,  9.548], mean action: 1.479 [0.000, 3.000],  loss: 47.582862, mse: 33876.135317, mean_q: 156.150688, mean_eps: 0.796969
 67840/100000: episode: 717, duration: 0.741s, episode steps: 104, steps per second: 140, episode reward: -128.606, mean reward: -1.237 [-100.000, 11.493], mean action: 1.529 [0.000, 3.000],  loss: 43.900444, mse: 34393.627554, mean_q: 157.682190, mean_eps: 0.796637
 67933/100000: episode: 718, duration: 0.643s, episode steps:  93, steps per second: 145, episode reward: -122.290, mean reward: -1.315 [-100.000, 19.736], mean action: 1.527 [0.000, 3.000],  loss: 44.793923, mse: 34403.658392, mean_q: 158.217116, mean_eps: 0.796342
 68009/100000: episode: 719, duration: 0.548s, episode steps:  76, steps per second: 139, episode reward: -105.049, mean reward: -1.382 [-100.000, 12.360], mean action: 1.447 [0.000, 3.000],  loss: 30.190216, mse: 35090.611611, mean_q: 160.042891, mean_eps: 0.796089
 68083/100000: episode: 720, duration: 0.552s, episode steps:  74, steps per second: 134, episode reward: -103.792, mean reward: -1.403 [-100.000, 10.180], mean action: 1.351 [0.000, 3.000],  loss: 21.213538, mse: 35053.196659, mean_q: 160.014116, mean_eps: 0.795863
 68183/100000: episode: 721, duration: 0.779s, episode steps: 100, steps per second: 128, episode reward: -101.778, mean reward: -1.018 [-100.000,  8.648], mean action: 1.630 [0.000, 3.000],  loss: 29.496255, mse: 35573.671504, mean_q: 161.761054, mean_eps: 0.795602
 68255/100000: episode: 722, duration: 0.534s, episode steps:  72, steps per second: 135, episode reward: -91.890, mean reward: -1.276 [-100.000,  7.436], mean action: 1.542 [0.000, 3.000],  loss: 28.879423, mse: 35766.483561, mean_q: 161.691139, mean_eps: 0.795345
 68335/100000: episode: 723, duration: 0.637s, episode steps:  80, steps per second: 126, episode reward: -19.796, mean reward: -0.247 [-100.000, 23.652], mean action: 1.650 [0.000, 3.000],  loss: 39.617083, mse: 37811.084692, mean_q: 168.759030, mean_eps: 0.795117
 68436/100000: episode: 724, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: -76.404, mean reward: -0.756 [-100.000,  8.980], mean action: 1.446 [0.000, 3.000],  loss: 24.442292, mse: 37624.224841, mean_q: 168.169480, mean_eps: 0.794845
 68516/100000: episode: 725, duration: 0.565s, episode steps:  80, steps per second: 142, episode reward: -97.085, mean reward: -1.214 [-100.000,  5.200], mean action: 1.400 [0.000, 3.000],  loss: 39.637131, mse: 38501.679126, mean_q: 171.248279, mean_eps: 0.794574
 68595/100000: episode: 726, duration: 0.633s, episode steps:  79, steps per second: 125, episode reward: -79.078, mean reward: -1.001 [-100.000, 13.009], mean action: 1.823 [0.000, 3.000],  loss: 35.914524, mse: 38221.880192, mean_q: 168.772506, mean_eps: 0.794335
 68665/100000: episode: 727, duration: 0.515s, episode steps:  70, steps per second: 136, episode reward: -59.430, mean reward: -0.849 [-100.000, 12.609], mean action: 1.629 [0.000, 3.000],  loss: 33.797665, mse: 37846.153767, mean_q: 165.363352, mean_eps: 0.794111
 68768/100000: episode: 728, duration: 0.704s, episode steps: 103, steps per second: 146, episode reward: -98.075, mean reward: -0.952 [-100.000, 12.433], mean action: 1.515 [0.000, 3.000],  loss: 39.441181, mse: 37676.330344, mean_q: 167.830575, mean_eps: 0.793852
 68867/100000: episode: 729, duration: 0.669s, episode steps:  99, steps per second: 148, episode reward: -41.040, mean reward: -0.415 [-100.000, 17.430], mean action: 1.515 [0.000, 3.000],  loss: 36.635009, mse: 36829.336746, mean_q: 162.706263, mean_eps: 0.793549
 68956/100000: episode: 730, duration: 0.650s, episode steps:  89, steps per second: 137, episode reward: -133.058, mean reward: -1.495 [-100.000,  9.652], mean action: 1.719 [0.000, 3.000],  loss: 42.738548, mse: 38756.341599, mean_q: 169.964537, mean_eps: 0.793267
 69067/100000: episode: 731, duration: 0.768s, episode steps: 111, steps per second: 144, episode reward: -38.532, mean reward: -0.347 [-100.000, 10.254], mean action: 1.676 [0.000, 3.000],  loss: 34.265819, mse: 38753.578107, mean_q: 169.587767, mean_eps: 0.792967
 69152/100000: episode: 732, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -111.949, mean reward: -1.317 [-100.000,  8.167], mean action: 1.518 [0.000, 3.000],  loss: 50.271142, mse: 39257.105767, mean_q: 171.433482, mean_eps: 0.792673
 69228/100000: episode: 733, duration: 0.549s, episode steps:  76, steps per second: 138, episode reward: -8.368, mean reward: -0.110 [-100.000, 15.447], mean action: 1.724 [0.000, 3.000],  loss: 48.218866, mse: 38949.807283, mean_q: 169.538235, mean_eps: 0.792431
 69323/100000: episode: 734, duration: 0.668s, episode steps:  95, steps per second: 142, episode reward: -124.684, mean reward: -1.312 [-100.000,  6.614], mean action: 1.505 [0.000, 3.000],  loss: 41.395878, mse: 39043.139679, mean_q: 169.104366, mean_eps: 0.792175
 69409/100000: episode: 735, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -55.713, mean reward: -0.648 [-100.000, 19.493], mean action: 1.465 [0.000, 3.000],  loss: 54.507470, mse: 38882.570176, mean_q: 166.550149, mean_eps: 0.791903
 69481/100000: episode: 736, duration: 0.558s, episode steps:  72, steps per second: 129, episode reward: -30.427, mean reward: -0.423 [-100.000,  7.024], mean action: 1.778 [0.000, 3.000],  loss: 39.010603, mse: 38837.997721, mean_q: 167.739048, mean_eps: 0.791667
 69557/100000: episode: 737, duration: 0.608s, episode steps:  76, steps per second: 125, episode reward: -98.805, mean reward: -1.300 [-100.000, 13.054], mean action: 1.684 [0.000, 3.000],  loss: 29.765117, mse: 40614.460989, mean_q: 173.810167, mean_eps: 0.791444
 69656/100000: episode: 738, duration: 0.718s, episode steps:  99, steps per second: 138, episode reward: -80.046, mean reward: -0.809 [-100.000,  7.620], mean action: 1.545 [0.000, 3.000],  loss: 27.641564, mse: 40082.020557, mean_q: 171.642826, mean_eps: 0.791182
 69764/100000: episode: 739, duration: 0.750s, episode steps: 108, steps per second: 144, episode reward: -112.550, mean reward: -1.042 [-100.000,  6.769], mean action: 1.528 [0.000, 3.000],  loss: 33.999830, mse: 40630.998228, mean_q: 174.857667, mean_eps: 0.790871
 69840/100000: episode: 740, duration: 0.569s, episode steps:  76, steps per second: 134, episode reward: -98.618, mean reward: -1.298 [-100.000,  8.297], mean action: 1.539 [0.000, 3.000],  loss: 48.670092, mse: 40675.921361, mean_q: 174.204261, mean_eps: 0.790596
 69954/100000: episode: 741, duration: 0.792s, episode steps: 114, steps per second: 144, episode reward: -79.498, mean reward: -0.697 [-100.000,  5.968], mean action: 1.658 [0.000, 3.000],  loss: 37.144577, mse: 41779.539902, mean_q: 176.226763, mean_eps: 0.790311
 70018/100000: episode: 742, duration: 0.444s, episode steps:  64, steps per second: 144, episode reward: -75.679, mean reward: -1.182 [-100.000, 10.873], mean action: 1.734 [0.000, 3.000],  loss: 28.187319, mse: 41152.150482, mean_q: 174.569815, mean_eps: 0.790044
 70137/100000: episode: 743, duration: 0.835s, episode steps: 119, steps per second: 143, episode reward: -57.039, mean reward: -0.479 [-100.000, 13.132], mean action: 1.622 [0.000, 3.000],  loss: 53.831369, mse: 41016.618123, mean_q: 172.735089, mean_eps: 0.789769
 70251/100000: episode: 744, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -238.728, mean reward: -2.094 [-100.000,  9.985], mean action: 1.579 [0.000, 3.000],  loss: 32.610401, mse: 41341.091814, mean_q: 174.108171, mean_eps: 0.789419
 70336/100000: episode: 745, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -111.537, mean reward: -1.312 [-100.000, 14.509], mean action: 1.506 [0.000, 3.000],  loss: 35.232522, mse: 41254.659237, mean_q: 173.115778, mean_eps: 0.789121
 70417/100000: episode: 746, duration: 0.569s, episode steps:  81, steps per second: 142, episode reward: -60.106, mean reward: -0.742 [-100.000, 13.193], mean action: 1.765 [0.000, 3.000],  loss: 36.171023, mse: 41020.361810, mean_q: 173.823544, mean_eps: 0.788872
 70485/100000: episode: 747, duration: 0.492s, episode steps:  68, steps per second: 138, episode reward: -96.498, mean reward: -1.419 [-100.000,  6.946], mean action: 1.544 [0.000, 3.000],  loss: 41.992658, mse: 40851.786305, mean_q: 172.909525, mean_eps: 0.788648
 70588/100000: episode: 748, duration: 0.728s, episode steps: 103, steps per second: 141, episode reward: -32.115, mean reward: -0.312 [-100.000, 13.518], mean action: 1.485 [0.000, 3.000],  loss: 26.877611, mse: 42037.443075, mean_q: 177.068507, mean_eps: 0.788392
 70718/100000: episode: 749, duration: 1.044s, episode steps: 130, steps per second: 125, episode reward: -155.766, mean reward: -1.198 [-100.000,  6.606], mean action: 1.546 [0.000, 3.000],  loss: 29.339811, mse: 41777.830754, mean_q: 175.200344, mean_eps: 0.788042
 70785/100000: episode: 750, duration: 0.509s, episode steps:  67, steps per second: 132, episode reward: -67.810, mean reward: -1.012 [-100.000, 12.150], mean action: 1.642 [0.000, 3.000],  loss: 59.556059, mse: 41112.639313, mean_q: 173.749489, mean_eps: 0.787747
 70855/100000: episode: 751, duration: 0.509s, episode steps:  70, steps per second: 138, episode reward: -52.215, mean reward: -0.746 [-100.000,  7.706], mean action: 1.271 [0.000, 3.000],  loss: 26.077988, mse: 43271.132701, mean_q: 180.017319, mean_eps: 0.787542
 70965/100000: episode: 752, duration: 0.812s, episode steps: 110, steps per second: 135, episode reward: -103.336, mean reward: -0.939 [-100.000, 11.904], mean action: 1.573 [0.000, 3.000],  loss: 34.269450, mse: 43324.623810, mean_q: 178.696653, mean_eps: 0.787271
 71038/100000: episode: 753, duration: 0.665s, episode steps:  73, steps per second: 110, episode reward: -76.389, mean reward: -1.046 [-100.000, 11.103], mean action: 1.425 [0.000, 3.000],  loss: 49.079834, mse: 42846.710402, mean_q: 176.585046, mean_eps: 0.786997
 71142/100000: episode: 754, duration: 0.758s, episode steps: 104, steps per second: 137, episode reward: -88.130, mean reward: -0.847 [-100.000,  7.993], mean action: 1.654 [0.000, 3.000],  loss: 50.564503, mse: 42597.999474, mean_q: 176.848198, mean_eps: 0.786732
 71211/100000: episode: 755, duration: 0.485s, episode steps:  69, steps per second: 142, episode reward: -126.117, mean reward: -1.828 [-100.000, 10.614], mean action: 1.551 [0.000, 3.000],  loss: 30.299479, mse: 42838.869905, mean_q: 177.871094, mean_eps: 0.786472
 71328/100000: episode: 756, duration: 0.848s, episode steps: 117, steps per second: 138, episode reward: -250.775, mean reward: -2.143 [-100.000, 123.005], mean action: 1.547 [0.000, 3.000],  loss: 38.308662, mse: 43126.653663, mean_q: 176.593155, mean_eps: 0.786193
 71416/100000: episode: 757, duration: 0.602s, episode steps:  88, steps per second: 146, episode reward: -149.694, mean reward: -1.701 [-100.000, 35.955], mean action: 1.625 [0.000, 3.000],  loss: 33.891295, mse: 43846.155518, mean_q: 178.085427, mean_eps: 0.785885
 71501/100000: episode: 758, duration: 0.585s, episode steps:  85, steps per second: 145, episode reward: -95.238, mean reward: -1.120 [-100.000,  7.096], mean action: 1.741 [0.000, 3.000],  loss: 34.351738, mse: 43656.262385, mean_q: 178.401060, mean_eps: 0.785626
 71596/100000: episode: 759, duration: 0.706s, episode steps:  95, steps per second: 135, episode reward: -66.866, mean reward: -0.704 [-100.000, 10.056], mean action: 1.589 [0.000, 3.000],  loss: 38.790225, mse: 45066.254831, mean_q: 184.001627, mean_eps: 0.785356
 71700/100000: episode: 760, duration: 0.776s, episode steps: 104, steps per second: 134, episode reward: -90.943, mean reward: -0.874 [-100.000,  8.496], mean action: 1.529 [0.000, 3.000],  loss: 35.533468, mse: 44797.246920, mean_q: 181.203384, mean_eps: 0.785057
 71806/100000: episode: 761, duration: 0.757s, episode steps: 106, steps per second: 140, episode reward: -138.207, mean reward: -1.304 [-100.000,  9.005], mean action: 1.566 [0.000, 3.000],  loss: 31.361093, mse: 44674.781047, mean_q: 180.553790, mean_eps: 0.784743
 71906/100000: episode: 762, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: -118.363, mean reward: -1.184 [-100.000, 22.939], mean action: 1.470 [0.000, 3.000],  loss: 42.442114, mse: 46078.335430, mean_q: 182.378481, mean_eps: 0.784434
 71988/100000: episode: 763, duration: 0.580s, episode steps:  82, steps per second: 141, episode reward: -98.670, mean reward: -1.203 [-100.000, 15.718], mean action: 1.622 [0.000, 3.000],  loss: 28.611745, mse: 46864.594893, mean_q: 185.073370, mean_eps: 0.784161
 72079/100000: episode: 764, duration: 0.633s, episode steps:  91, steps per second: 144, episode reward: -105.968, mean reward: -1.164 [-100.000,  6.615], mean action: 1.615 [0.000, 3.000],  loss: 26.012134, mse: 46658.498648, mean_q: 184.564085, mean_eps: 0.783901
 72203/100000: episode: 765, duration: 0.895s, episode steps: 124, steps per second: 139, episode reward: -168.217, mean reward: -1.357 [-100.000, 12.154], mean action: 1.621 [0.000, 3.000],  loss: 29.228595, mse: 48257.637349, mean_q: 189.619585, mean_eps: 0.783579
 72313/100000: episode: 766, duration: 0.787s, episode steps: 110, steps per second: 140, episode reward: -149.153, mean reward: -1.356 [-100.000, 10.689], mean action: 1.591 [0.000, 3.000],  loss: 30.433358, mse: 47401.580362, mean_q: 186.568448, mean_eps: 0.783227
 72408/100000: episode: 767, duration: 0.662s, episode steps:  95, steps per second: 143, episode reward: -81.560, mean reward: -0.859 [-100.000, 19.458], mean action: 1.568 [0.000, 3.000],  loss: 29.980469, mse: 47535.177076, mean_q: 186.414028, mean_eps: 0.782920
 72486/100000: episode: 768, duration: 0.634s, episode steps:  78, steps per second: 123, episode reward: -79.036, mean reward: -1.013 [-100.000, 11.935], mean action: 1.577 [0.000, 3.000],  loss: 25.889133, mse: 47986.012645, mean_q: 189.244152, mean_eps: 0.782660
 72607/100000: episode: 769, duration: 0.873s, episode steps: 121, steps per second: 139, episode reward: -124.018, mean reward: -1.025 [-100.000,  5.654], mean action: 1.628 [0.000, 3.000],  loss: 28.479021, mse: 49610.408219, mean_q: 194.168974, mean_eps: 0.782362
 72709/100000: episode: 770, duration: 0.784s, episode steps: 102, steps per second: 130, episode reward: -129.576, mean reward: -1.270 [-100.000, 16.564], mean action: 1.706 [0.000, 3.000],  loss: 37.001863, mse: 48800.751455, mean_q: 193.349464, mean_eps: 0.782027
 72776/100000: episode: 771, duration: 0.540s, episode steps:  67, steps per second: 124, episode reward: -134.905, mean reward: -2.014 [-100.000, 15.222], mean action: 1.761 [0.000, 3.000],  loss: 35.787825, mse: 48762.702250, mean_q: 192.654135, mean_eps: 0.781774
 72889/100000: episode: 772, duration: 0.821s, episode steps: 113, steps per second: 138, episode reward: -55.258, mean reward: -0.489 [-100.000, 14.133], mean action: 1.372 [0.000, 3.000],  loss: 35.793663, mse: 48965.046668, mean_q: 192.834500, mean_eps: 0.781504
 72957/100000: episode: 773, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: -41.653, mean reward: -0.613 [-100.000,  9.891], mean action: 1.515 [0.000, 3.000],  loss: 66.808961, mse: 50014.008272, mean_q: 196.036777, mean_eps: 0.781232
 73052/100000: episode: 774, duration: 0.677s, episode steps:  95, steps per second: 140, episode reward: -170.331, mean reward: -1.793 [-100.000, 21.634], mean action: 1.537 [0.000, 3.000],  loss: 43.272937, mse: 50421.216735, mean_q: 198.680825, mean_eps: 0.780988
 73132/100000: episode: 775, duration: 0.600s, episode steps:  80, steps per second: 133, episode reward: -148.153, mean reward: -1.852 [-100.000,  4.559], mean action: 1.675 [0.000, 3.000],  loss: 56.203119, mse: 49731.572461, mean_q: 194.183853, mean_eps: 0.780725
 73207/100000: episode: 776, duration: 0.541s, episode steps:  75, steps per second: 139, episode reward: -65.608, mean reward: -0.875 [-100.000, 17.253], mean action: 1.627 [0.000, 3.000],  loss: 32.850539, mse: 49085.647396, mean_q: 194.057566, mean_eps: 0.780493
 73319/100000: episode: 777, duration: 0.905s, episode steps: 112, steps per second: 124, episode reward: -117.781, mean reward: -1.052 [-100.000, 22.147], mean action: 1.652 [0.000, 3.000],  loss: 37.350786, mse: 49801.199079, mean_q: 197.276574, mean_eps: 0.780213
 73437/100000: episode: 778, duration: 0.881s, episode steps: 118, steps per second: 134, episode reward: -45.428, mean reward: -0.385 [-100.000, 16.689], mean action: 1.610 [0.000, 3.000],  loss: 39.473765, mse: 48927.926245, mean_q: 193.118067, mean_eps: 0.779868
 73502/100000: episode: 779, duration: 0.480s, episode steps:  65, steps per second: 135, episode reward: -117.083, mean reward: -1.801 [-100.000, 10.749], mean action: 1.646 [0.000, 3.000],  loss: 40.176338, mse: 48737.067488, mean_q: 191.936694, mean_eps: 0.779593
 73587/100000: episode: 780, duration: 0.659s, episode steps:  85, steps per second: 129, episode reward: -72.230, mean reward: -0.850 [-100.000,  8.854], mean action: 1.576 [0.000, 3.000],  loss: 37.292878, mse: 50550.031434, mean_q: 197.526331, mean_eps: 0.779368
 73666/100000: episode: 781, duration: 0.636s, episode steps:  79, steps per second: 124, episode reward: -83.059, mean reward: -1.051 [-100.000,  8.860], mean action: 1.582 [0.000, 3.000],  loss: 31.156220, mse: 50481.146707, mean_q: 197.268914, mean_eps: 0.779122
 73724/100000: episode: 782, duration: 0.479s, episode steps:  58, steps per second: 121, episode reward: -93.342, mean reward: -1.609 [-100.000, 17.394], mean action: 1.690 [0.000, 3.000],  loss: 52.871371, mse: 48312.009429, mean_q: 191.761218, mean_eps: 0.778917
 73815/100000: episode: 783, duration: 0.665s, episode steps:  91, steps per second: 137, episode reward: -48.371, mean reward: -0.532 [-100.000, 12.167], mean action: 1.549 [0.000, 3.000],  loss: 26.818103, mse: 50438.639294, mean_q: 197.274559, mean_eps: 0.778693
 73897/100000: episode: 784, duration: 0.580s, episode steps:  82, steps per second: 141, episode reward: -76.789, mean reward: -0.936 [-100.000,  6.170], mean action: 1.439 [0.000, 3.000],  loss: 34.488135, mse: 50000.761576, mean_q: 194.764743, mean_eps: 0.778434
 73968/100000: episode: 785, duration: 0.562s, episode steps:  71, steps per second: 126, episode reward: -67.450, mean reward: -0.950 [-100.000, 10.125], mean action: 1.761 [0.000, 3.000],  loss: 48.407001, mse: 49557.987456, mean_q: 194.267111, mean_eps: 0.778204
 74044/100000: episode: 786, duration: 0.588s, episode steps:  76, steps per second: 129, episode reward: -102.199, mean reward: -1.345 [-100.000, 11.195], mean action: 1.421 [0.000, 3.000],  loss: 30.292403, mse: 48905.719470, mean_q: 191.941308, mean_eps: 0.777983
 74164/100000: episode: 787, duration: 0.942s, episode steps: 120, steps per second: 127, episode reward: -47.224, mean reward: -0.394 [-100.000, 89.097], mean action: 1.583 [0.000, 3.000],  loss: 31.201062, mse: 51441.660872, mean_q: 200.352231, mean_eps: 0.777689
 74270/100000: episode: 788, duration: 0.838s, episode steps: 106, steps per second: 127, episode reward: -220.407, mean reward: -2.079 [-100.000,  7.063], mean action: 1.698 [0.000, 3.000],  loss: 61.271334, mse: 50751.391565, mean_q: 195.292339, mean_eps: 0.777350
 74385/100000: episode: 789, duration: 0.987s, episode steps: 115, steps per second: 117, episode reward: -126.557, mean reward: -1.100 [-100.000, 15.788], mean action: 1.583 [0.000, 3.000],  loss: 31.972480, mse: 49946.625272, mean_q: 191.404794, mean_eps: 0.777019
 74527/100000: episode: 790, duration: 1.108s, episode steps: 142, steps per second: 128, episode reward: -76.875, mean reward: -0.541 [-100.000, 12.169], mean action: 1.676 [0.000, 3.000],  loss: 38.141666, mse: 50238.258858, mean_q: 193.554585, mean_eps: 0.776634
 74605/100000: episode: 791, duration: 0.566s, episode steps:  78, steps per second: 138, episode reward: -86.231, mean reward: -1.106 [-100.000, 11.941], mean action: 1.538 [0.000, 3.000],  loss: 28.161555, mse: 49679.071364, mean_q: 188.466299, mean_eps: 0.776303
 74707/100000: episode: 792, duration: 0.733s, episode steps: 102, steps per second: 139, episode reward: -104.658, mean reward: -1.026 [-100.000,  9.012], mean action: 1.363 [0.000, 3.000],  loss: 37.095898, mse: 51650.236137, mean_q: 196.115588, mean_eps: 0.776033
 74791/100000: episode: 793, duration: 0.647s, episode steps:  84, steps per second: 130, episode reward: -66.754, mean reward: -0.795 [-100.000,  7.610], mean action: 1.595 [0.000, 3.000],  loss: 53.982311, mse: 51046.278785, mean_q: 192.675789, mean_eps: 0.775755
 74873/100000: episode: 794, duration: 0.723s, episode steps:  82, steps per second: 113, episode reward: -64.656, mean reward: -0.788 [-100.000, 10.966], mean action: 1.634 [0.000, 3.000],  loss: 38.029392, mse: 50217.800114, mean_q: 190.507736, mean_eps: 0.775505
 74993/100000: episode: 795, duration: 1.191s, episode steps: 120, steps per second: 101, episode reward: -110.288, mean reward: -0.919 [-100.000,  8.606], mean action: 1.700 [0.000, 3.000],  loss: 49.264869, mse: 51607.200439, mean_q: 196.268843, mean_eps: 0.775203
 75076/100000: episode: 796, duration: 0.686s, episode steps:  83, steps per second: 121, episode reward: -74.735, mean reward: -0.900 [-100.000,  7.844], mean action: 1.578 [0.000, 3.000],  loss: 30.469347, mse: 51487.445877, mean_q: 194.755776, mean_eps: 0.774898
 75172/100000: episode: 797, duration: 0.747s, episode steps:  96, steps per second: 129, episode reward: -106.863, mean reward: -1.113 [-100.000,  5.985], mean action: 1.688 [0.000, 3.000],  loss: 49.741315, mse: 50572.754639, mean_q: 191.077051, mean_eps: 0.774630
 75262/100000: episode: 798, duration: 0.708s, episode steps:  90, steps per second: 127, episode reward: -79.163, mean reward: -0.880 [-100.000,  9.301], mean action: 1.578 [0.000, 3.000],  loss: 53.355260, mse: 52964.016276, mean_q: 198.427592, mean_eps: 0.774350
 75407/100000: episode: 799, duration: 1.069s, episode steps: 145, steps per second: 136, episode reward: -266.057, mean reward: -1.835 [-100.000, 87.550], mean action: 1.628 [0.000, 3.000],  loss: 41.761735, mse: 52504.988739, mean_q: 196.009421, mean_eps: 0.773998
 75535/100000: episode: 800, duration: 0.908s, episode steps: 128, steps per second: 141, episode reward: -34.772, mean reward: -0.272 [-100.000, 17.625], mean action: 1.602 [0.000, 3.000],  loss: 51.390802, mse: 51494.059387, mean_q: 192.062860, mean_eps: 0.773589
 75632/100000: episode: 801, duration: 0.722s, episode steps:  97, steps per second: 134, episode reward: -161.495, mean reward: -1.665 [-100.000,  9.446], mean action: 1.825 [0.000, 3.000],  loss: 50.119975, mse: 53050.964924, mean_q: 201.292312, mean_eps: 0.773251
 75733/100000: episode: 802, duration: 0.717s, episode steps: 101, steps per second: 141, episode reward: -127.520, mean reward: -1.263 [-100.000,  6.500], mean action: 1.594 [0.000, 3.000],  loss: 63.687795, mse: 52888.211208, mean_q: 197.345059, mean_eps: 0.772954
 75837/100000: episode: 803, duration: 0.764s, episode steps: 104, steps per second: 136, episode reward: -67.102, mean reward: -0.645 [-100.000,  7.128], mean action: 1.538 [0.000, 3.000],  loss: 29.090310, mse: 53704.738431, mean_q: 197.762925, mean_eps: 0.772647
 75923/100000: episode: 804, duration: 0.650s, episode steps:  86, steps per second: 132, episode reward: -87.555, mean reward: -1.018 [-100.000,  7.903], mean action: 1.698 [0.000, 3.000],  loss: 37.612635, mse: 54136.069222, mean_q: 200.577065, mean_eps: 0.772362
 75978/100000: episode: 805, duration: 0.399s, episode steps:  55, steps per second: 138, episode reward: -78.613, mean reward: -1.429 [-100.000, 38.732], mean action: 1.564 [0.000, 3.000],  loss: 34.190102, mse: 54158.671236, mean_q: 200.226892, mean_eps: 0.772150
 76078/100000: episode: 806, duration: 0.716s, episode steps: 100, steps per second: 140, episode reward: -126.038, mean reward: -1.260 [-100.000, 15.421], mean action: 1.570 [0.000, 3.000],  loss: 53.283837, mse: 52861.575703, mean_q: 197.522894, mean_eps: 0.771918
 76152/100000: episode: 807, duration: 0.541s, episode steps:  74, steps per second: 137, episode reward: -104.393, mean reward: -1.411 [-100.000, 11.756], mean action: 1.595 [0.000, 3.000],  loss: 29.284371, mse: 53708.587521, mean_q: 200.496985, mean_eps: 0.771656
 76225/100000: episode: 808, duration: 0.547s, episode steps:  73, steps per second: 133, episode reward: -72.890, mean reward: -0.998 [-100.000, 11.523], mean action: 1.616 [0.000, 3.000],  loss: 42.185999, mse: 53469.229827, mean_q: 199.600445, mean_eps: 0.771436
 76329/100000: episode: 809, duration: 0.746s, episode steps: 104, steps per second: 139, episode reward: -110.508, mean reward: -1.063 [-100.000, 15.148], mean action: 1.510 [0.000, 3.000],  loss: 43.150425, mse: 53471.620343, mean_q: 199.145118, mean_eps: 0.771171
 76423/100000: episode: 810, duration: 0.687s, episode steps:  94, steps per second: 137, episode reward: -96.606, mean reward: -1.028 [-100.000,  6.214], mean action: 1.574 [0.000, 3.000],  loss: 42.523583, mse: 52529.586893, mean_q: 195.641741, mean_eps: 0.770873
 76526/100000: episode: 811, duration: 0.800s, episode steps: 103, steps per second: 129, episode reward: -68.144, mean reward: -0.662 [-100.000,  7.933], mean action: 1.505 [0.000, 3.000],  loss: 28.548981, mse: 53996.889222, mean_q: 199.432139, mean_eps: 0.770578
 76591/100000: episode: 812, duration: 0.457s, episode steps:  65, steps per second: 142, episode reward: -83.549, mean reward: -1.285 [-100.000,  6.729], mean action: 1.662 [0.000, 3.000],  loss: 45.690617, mse: 53629.878005, mean_q: 198.607042, mean_eps: 0.770326
 76661/100000: episode: 813, duration: 0.494s, episode steps:  70, steps per second: 142, episode reward: -89.382, mean reward: -1.277 [-100.000, 17.538], mean action: 1.686 [0.000, 3.000],  loss: 49.357144, mse: 51860.467522, mean_q: 194.041330, mean_eps: 0.770124
 76771/100000: episode: 814, duration: 0.797s, episode steps: 110, steps per second: 138, episode reward: -148.153, mean reward: -1.347 [-100.000, 16.282], mean action: 1.673 [0.000, 3.000],  loss: 36.587899, mse: 53953.932741, mean_q: 199.722036, mean_eps: 0.769853
 76925/100000: episode: 815, duration: 1.071s, episode steps: 154, steps per second: 144, episode reward: -89.692, mean reward: -0.582 [-100.000, 36.491], mean action: 1.558 [0.000, 3.000],  loss: 38.971816, mse: 52887.909827, mean_q: 196.255044, mean_eps: 0.769458
 76995/100000: episode: 816, duration: 0.531s, episode steps:  70, steps per second: 132, episode reward: -71.076, mean reward: -1.015 [-100.000, 14.769], mean action: 1.371 [0.000, 3.000],  loss: 66.668876, mse: 52938.974442, mean_q: 197.506073, mean_eps: 0.769122
 77101/100000: episode: 817, duration: 0.817s, episode steps: 106, steps per second: 130, episode reward: -196.794, mean reward: -1.857 [-100.000, 76.264], mean action: 1.651 [0.000, 3.000],  loss: 45.497391, mse: 54172.872605, mean_q: 200.524991, mean_eps: 0.768858
 77225/100000: episode: 818, duration: 0.911s, episode steps: 124, steps per second: 136, episode reward: -27.964, mean reward: -0.226 [-100.000, 24.707], mean action: 1.540 [0.000, 3.000],  loss: 31.513211, mse: 53093.020980, mean_q: 198.245177, mean_eps: 0.768512
 77319/100000: episode: 819, duration: 0.684s, episode steps:  94, steps per second: 137, episode reward: -92.052, mean reward: -0.979 [-100.000,  7.794], mean action: 1.691 [0.000, 3.000],  loss: 34.594433, mse: 53115.015625, mean_q: 196.778659, mean_eps: 0.768185
 77422/100000: episode: 820, duration: 0.881s, episode steps: 103, steps per second: 117, episode reward: -122.697, mean reward: -1.191 [-100.000,  5.305], mean action: 1.670 [0.000, 3.000],  loss: 47.926587, mse: 55505.440193, mean_q: 202.677832, mean_eps: 0.767890
 77509/100000: episode: 821, duration: 0.635s, episode steps:  87, steps per second: 137, episode reward: -113.617, mean reward: -1.306 [-100.000,  6.918], mean action: 1.437 [0.000, 3.000],  loss: 52.080137, mse: 53910.959995, mean_q: 200.119715, mean_eps: 0.767605
 77615/100000: episode: 822, duration: 0.766s, episode steps: 106, steps per second: 138, episode reward: -33.906, mean reward: -0.320 [-100.000, 18.472], mean action: 1.472 [0.000, 3.000],  loss: 55.484386, mse: 54535.085200, mean_q: 203.113443, mean_eps: 0.767315
 77688/100000: episode: 823, duration: 0.569s, episode steps:  73, steps per second: 128, episode reward: -51.882, mean reward: -0.711 [-100.000,  8.038], mean action: 1.822 [0.000, 3.000],  loss: 58.924322, mse: 52935.775899, mean_q: 197.942693, mean_eps: 0.767047
 77802/100000: episode: 824, duration: 0.870s, episode steps: 114, steps per second: 131, episode reward: -89.994, mean reward: -0.789 [-100.000, 13.321], mean action: 1.675 [0.000, 3.000],  loss: 82.406562, mse: 53321.288035, mean_q: 199.226824, mean_eps: 0.766766
 77874/100000: episode: 825, duration: 0.656s, episode steps:  72, steps per second: 110, episode reward: -152.500, mean reward: -2.118 [-100.000, 11.168], mean action: 1.708 [0.000, 3.000],  loss: 36.069382, mse: 53230.386502, mean_q: 197.170669, mean_eps: 0.766487
 77967/100000: episode: 826, duration: 1.047s, episode steps:  93, steps per second:  89, episode reward: -81.766, mean reward: -0.879 [-100.000,  7.683], mean action: 1.538 [0.000, 3.000],  loss: 38.768891, mse: 52761.253780, mean_q: 196.805065, mean_eps: 0.766240
 78054/100000: episode: 827, duration: 0.669s, episode steps:  87, steps per second: 130, episode reward: -203.959, mean reward: -2.344 [-100.000, 13.473], mean action: 1.713 [0.000, 3.000],  loss: 36.322244, mse: 52538.865436, mean_q: 197.521919, mean_eps: 0.765970
 78158/100000: episode: 828, duration: 0.771s, episode steps: 104, steps per second: 135, episode reward: -67.080, mean reward: -0.645 [-100.000, 25.294], mean action: 1.702 [0.000, 3.000],  loss: 36.332984, mse: 51685.460261, mean_q: 194.533001, mean_eps: 0.765683
 78251/100000: episode: 829, duration: 0.680s, episode steps:  93, steps per second: 137, episode reward: -89.099, mean reward: -0.958 [-100.000, 17.223], mean action: 1.538 [0.000, 3.000],  loss: 35.729090, mse: 52373.251596, mean_q: 195.317468, mean_eps: 0.765388
 78316/100000: episode: 830, duration: 0.472s, episode steps:  65, steps per second: 138, episode reward: -52.361, mean reward: -0.806 [-100.000, 11.716], mean action: 1.385 [0.000, 3.000],  loss: 46.069726, mse: 50419.581851, mean_q: 190.039990, mean_eps: 0.765151
 78420/100000: episode: 831, duration: 0.904s, episode steps: 104, steps per second: 115, episode reward: -105.146, mean reward: -1.011 [-100.000, 17.621], mean action: 1.558 [0.000, 3.000],  loss: 34.676796, mse: 52965.514160, mean_q: 196.768252, mean_eps: 0.764898
 78518/100000: episode: 832, duration: 0.784s, episode steps:  98, steps per second: 125, episode reward: -70.430, mean reward: -0.719 [-100.000, 16.763], mean action: 1.755 [0.000, 3.000],  loss: 58.820631, mse: 52868.181362, mean_q: 198.799905, mean_eps: 0.764594
 78665/100000: episode: 833, duration: 1.172s, episode steps: 147, steps per second: 125, episode reward: -282.132, mean reward: -1.919 [-100.000,  6.644], mean action: 1.694 [0.000, 3.000],  loss: 43.766932, mse: 52633.893442, mean_q: 197.459927, mean_eps: 0.764227
 78775/100000: episode: 834, duration: 0.841s, episode steps: 110, steps per second: 131, episode reward: -36.465, mean reward: -0.332 [-100.000, 20.815], mean action: 1.527 [0.000, 3.000],  loss: 42.600114, mse: 52227.227770, mean_q: 197.124687, mean_eps: 0.763841
 78865/100000: episode: 835, duration: 0.662s, episode steps:  90, steps per second: 136, episode reward: -58.911, mean reward: -0.655 [-100.000, 10.201], mean action: 1.644 [0.000, 3.000],  loss: 41.113139, mse: 51066.363845, mean_q: 193.070457, mean_eps: 0.763542
 78937/100000: episode: 836, duration: 0.507s, episode steps:  72, steps per second: 142, episode reward: -93.504, mean reward: -1.299 [-100.000,  5.158], mean action: 1.528 [0.000, 3.000],  loss: 44.036244, mse: 49941.822754, mean_q: 192.542901, mean_eps: 0.763298
 79017/100000: episode: 837, duration: 0.598s, episode steps:  80, steps per second: 134, episode reward: -46.426, mean reward: -0.580 [-100.000,  8.046], mean action: 1.637 [0.000, 3.000],  loss: 44.869363, mse: 51091.248730, mean_q: 191.859419, mean_eps: 0.763070
 79135/100000: episode: 838, duration: 0.836s, episode steps: 118, steps per second: 141, episode reward: -69.984, mean reward: -0.593 [-100.000,  7.784], mean action: 1.576 [0.000, 3.000],  loss: 47.515830, mse: 52106.482587, mean_q: 196.795968, mean_eps: 0.762773
 79224/100000: episode: 839, duration: 0.643s, episode steps:  89, steps per second: 138, episode reward: -125.359, mean reward: -1.409 [-100.000, 16.260], mean action: 1.551 [0.000, 3.000],  loss: 40.286752, mse: 52931.453301, mean_q: 198.494235, mean_eps: 0.762463
 79304/100000: episode: 840, duration: 0.581s, episode steps:  80, steps per second: 138, episode reward: -80.959, mean reward: -1.012 [-100.000,  7.673], mean action: 1.825 [0.000, 3.000],  loss: 78.101624, mse: 53219.290918, mean_q: 199.242192, mean_eps: 0.762209
 79384/100000: episode: 841, duration: 0.798s, episode steps:  80, steps per second: 100, episode reward: -144.061, mean reward: -1.801 [-100.000,  9.237], mean action: 1.600 [0.000, 3.000],  loss: 40.509668, mse: 52311.782031, mean_q: 196.924414, mean_eps: 0.761969
 79472/100000: episode: 842, duration: 0.701s, episode steps:  88, steps per second: 126, episode reward: -85.314, mean reward: -0.969 [-100.000, 11.290], mean action: 1.591 [0.000, 3.000],  loss: 33.231707, mse: 54076.273704, mean_q: 201.922710, mean_eps: 0.761718
 79554/100000: episode: 843, duration: 0.623s, episode steps:  82, steps per second: 132, episode reward: -62.950, mean reward: -0.768 [-100.000, 15.539], mean action: 1.695 [0.000, 3.000],  loss: 44.525944, mse: 52311.679592, mean_q: 197.977539, mean_eps: 0.761463
 79645/100000: episode: 844, duration: 0.685s, episode steps:  91, steps per second: 133, episode reward: -149.119, mean reward: -1.639 [-100.000,  8.767], mean action: 1.549 [0.000, 3.000],  loss: 36.163144, mse: 51537.952696, mean_q: 195.370904, mean_eps: 0.761203
 79757/100000: episode: 845, duration: 0.795s, episode steps: 112, steps per second: 141, episode reward: -74.109, mean reward: -0.662 [-100.000,  9.796], mean action: 1.705 [0.000, 3.000],  loss: 60.803357, mse: 52358.295271, mean_q: 198.659772, mean_eps: 0.760898
 79842/100000: episode: 846, duration: 0.624s, episode steps:  85, steps per second: 136, episode reward: -129.216, mean reward: -1.520 [-100.000,  9.397], mean action: 1.341 [0.000, 3.000],  loss: 30.632189, mse: 52174.377252, mean_q: 194.648575, mean_eps: 0.760603
 79939/100000: episode: 847, duration: 0.843s, episode steps:  97, steps per second: 115, episode reward: -119.119, mean reward: -1.228 [-100.000, 14.357], mean action: 1.680 [0.000, 3.000],  loss: 42.055172, mse: 53015.476885, mean_q: 196.549228, mean_eps: 0.760330
 80053/100000: episode: 848, duration: 0.925s, episode steps: 114, steps per second: 123, episode reward: -71.115, mean reward: -0.624 [-100.000,  8.168], mean action: 1.465 [0.000, 3.000],  loss: 37.015113, mse: 53024.146176, mean_q: 196.942871, mean_eps: 0.760013
 80138/100000: episode: 849, duration: 0.680s, episode steps:  85, steps per second: 125, episode reward: -43.700, mean reward: -0.514 [-100.000, 16.353], mean action: 1.800 [0.000, 3.000],  loss: 52.650911, mse: 52641.003355, mean_q: 195.012010, mean_eps: 0.759715
 80238/100000: episode: 850, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: -130.586, mean reward: -1.306 [-100.000, 24.996], mean action: 1.750 [0.000, 3.000],  loss: 36.774766, mse: 53382.773008, mean_q: 196.396219, mean_eps: 0.759437
 80326/100000: episode: 851, duration: 0.599s, episode steps:  88, steps per second: 147, episode reward: -75.003, mean reward: -0.852 [-100.000,  8.624], mean action: 1.636 [0.000, 3.000],  loss: 31.177892, mse: 52797.135609, mean_q: 195.804724, mean_eps: 0.759156
 80419/100000: episode: 852, duration: 0.678s, episode steps:  93, steps per second: 137, episode reward: -95.230, mean reward: -1.024 [-100.000,  7.787], mean action: 1.516 [0.000, 3.000],  loss: 40.809522, mse: 53743.658182, mean_q: 200.172387, mean_eps: 0.758884
 80501/100000: episode: 853, duration: 0.662s, episode steps:  82, steps per second: 124, episode reward: -50.859, mean reward: -0.620 [-100.000,  6.946], mean action: 1.659 [0.000, 3.000],  loss: 40.146219, mse: 52366.051115, mean_q: 195.800992, mean_eps: 0.758621
 80622/100000: episode: 854, duration: 0.823s, episode steps: 121, steps per second: 147, episode reward: -87.551, mean reward: -0.724 [-100.000, 11.635], mean action: 1.645 [0.000, 3.000],  loss: 31.729615, mse: 52341.357470, mean_q: 196.433593, mean_eps: 0.758317
 80741/100000: episode: 855, duration: 0.875s, episode steps: 119, steps per second: 136, episode reward: -179.341, mean reward: -1.507 [-100.000,  2.959], mean action: 1.546 [0.000, 3.000],  loss: 37.436696, mse: 52921.791787, mean_q: 198.587271, mean_eps: 0.757957
 80817/100000: episode: 856, duration: 0.645s, episode steps:  76, steps per second: 118, episode reward: -68.223, mean reward: -0.898 [-100.000,  6.776], mean action: 1.605 [0.000, 3.000],  loss: 36.074657, mse: 51788.367444, mean_q: 195.097376, mean_eps: 0.757664
 80900/100000: episode: 857, duration: 0.620s, episode steps:  83, steps per second: 134, episode reward: -81.823, mean reward: -0.986 [-100.000,  8.539], mean action: 1.639 [0.000, 3.000],  loss: 46.020863, mse: 51057.339420, mean_q: 193.406186, mean_eps: 0.757426
 80959/100000: episode: 858, duration: 0.456s, episode steps:  59, steps per second: 129, episode reward: -110.305, mean reward: -1.870 [-100.000, 10.352], mean action: 1.763 [0.000, 3.000],  loss: 35.995156, mse: 50674.764897, mean_q: 190.754411, mean_eps: 0.757213
 81045/100000: episode: 859, duration: 0.668s, episode steps:  86, steps per second: 129, episode reward: -64.353, mean reward: -0.748 [-100.000,  7.054], mean action: 1.744 [0.000, 3.000],  loss: 54.839174, mse: 51536.714776, mean_q: 194.698045, mean_eps: 0.756996
 81153/100000: episode: 860, duration: 0.868s, episode steps: 108, steps per second: 124, episode reward: -93.932, mean reward: -0.870 [-100.000,  5.717], mean action: 1.648 [0.000, 3.000],  loss: 48.226282, mse: 51604.582429, mean_q: 194.580205, mean_eps: 0.756705
 81272/100000: episode: 861, duration: 1.003s, episode steps: 119, steps per second: 119, episode reward: -53.990, mean reward: -0.454 [-100.000,  8.171], mean action: 1.513 [0.000, 3.000],  loss: 55.896470, mse: 51081.714154, mean_q: 192.149089, mean_eps: 0.756364
 81347/100000: episode: 862, duration: 0.582s, episode steps:  75, steps per second: 129, episode reward: -111.629, mean reward: -1.488 [-100.000, 10.141], mean action: 1.533 [0.000, 3.000],  loss: 39.648630, mse: 48719.968333, mean_q: 188.428534, mean_eps: 0.756073
 81430/100000: episode: 863, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -115.066, mean reward: -1.386 [-100.000, 16.904], mean action: 1.482 [0.000, 3.000],  loss: 33.896053, mse: 47928.613046, mean_q: 186.144961, mean_eps: 0.755836
 81517/100000: episode: 864, duration: 0.694s, episode steps:  87, steps per second: 125, episode reward: -83.805, mean reward: -0.963 [-100.000, 19.413], mean action: 1.483 [0.000, 3.000],  loss: 58.233138, mse: 49528.118355, mean_q: 193.453888, mean_eps: 0.755581
 81621/100000: episode: 865, duration: 0.846s, episode steps: 104, steps per second: 123, episode reward: -67.075, mean reward: -0.645 [-100.000, 12.604], mean action: 1.731 [0.000, 3.000],  loss: 46.574672, mse: 49935.406964, mean_q: 194.350302, mean_eps: 0.755294
 81730/100000: episode: 866, duration: 0.933s, episode steps: 109, steps per second: 117, episode reward: -101.652, mean reward: -0.933 [-100.000, 27.105], mean action: 1.679 [0.000, 3.000],  loss: 62.830471, mse: 48725.438324, mean_q: 191.976242, mean_eps: 0.754975
 81842/100000: episode: 867, duration: 0.961s, episode steps: 112, steps per second: 117, episode reward: -174.793, mean reward: -1.561 [-100.000,  7.454], mean action: 1.554 [0.000, 3.000],  loss: 67.105911, mse: 50136.833391, mean_q: 193.587440, mean_eps: 0.754644
 81963/100000: episode: 868, duration: 0.964s, episode steps: 121, steps per second: 126, episode reward: -96.629, mean reward: -0.799 [-100.000,  6.831], mean action: 1.562 [0.000, 3.000],  loss: 49.766164, mse: 51110.995158, mean_q: 196.996051, mean_eps: 0.754294
 82080/100000: episode: 869, duration: 1.076s, episode steps: 117, steps per second: 109, episode reward: -94.211, mean reward: -0.805 [-100.000, 12.251], mean action: 1.735 [0.000, 3.000],  loss: 38.322577, mse: 50701.768930, mean_q: 195.128458, mean_eps: 0.753937
 82165/100000: episode: 870, duration: 0.681s, episode steps:  85, steps per second: 125, episode reward: -84.395, mean reward: -0.993 [-100.000, 23.609], mean action: 1.447 [0.000, 3.000],  loss: 41.581835, mse: 50472.841958, mean_q: 194.811775, mean_eps: 0.753634
 82241/100000: episode: 871, duration: 0.619s, episode steps:  76, steps per second: 123, episode reward: -71.833, mean reward: -0.945 [-100.000,  7.324], mean action: 1.658 [0.000, 3.000],  loss: 29.934503, mse: 51212.625565, mean_q: 196.727663, mean_eps: 0.753393
 82347/100000: episode: 872, duration: 0.808s, episode steps: 106, steps per second: 131, episode reward: -60.778, mean reward: -0.573 [-100.000, 17.890], mean action: 1.594 [0.000, 3.000],  loss: 33.081834, mse: 50602.418264, mean_q: 195.180250, mean_eps: 0.753119
 82445/100000: episode: 873, duration: 0.707s, episode steps:  98, steps per second: 139, episode reward: -113.302, mean reward: -1.156 [-100.000, 23.366], mean action: 1.776 [0.000, 3.000],  loss: 43.619458, mse: 50612.471002, mean_q: 194.319500, mean_eps: 0.752814
 82512/100000: episode: 874, duration: 0.491s, episode steps:  67, steps per second: 136, episode reward: -53.477, mean reward: -0.798 [-100.000,  9.397], mean action: 1.925 [0.000, 3.000],  loss: 34.131851, mse: 51000.529180, mean_q: 198.367302, mean_eps: 0.752566
 82585/100000: episode: 875, duration: 0.552s, episode steps:  73, steps per second: 132, episode reward: -16.911, mean reward: -0.232 [-100.000, 18.002], mean action: 1.562 [0.000, 3.000],  loss: 53.057928, mse: 51476.981325, mean_q: 199.219329, mean_eps: 0.752356
 82710/100000: episode: 876, duration: 0.917s, episode steps: 125, steps per second: 136, episode reward: -3.447, mean reward: -0.028 [-100.000, 112.386], mean action: 1.592 [0.000, 3.000],  loss: 39.088928, mse: 51229.238437, mean_q: 196.227177, mean_eps: 0.752059
 82784/100000: episode: 877, duration: 0.503s, episode steps:  74, steps per second: 147, episode reward: -55.794, mean reward: -0.754 [-100.000, 18.759], mean action: 1.689 [0.000, 3.000],  loss: 41.267097, mse: 52277.250422, mean_q: 198.671063, mean_eps: 0.751760
 82849/100000: episode: 878, duration: 0.444s, episode steps:  65, steps per second: 146, episode reward: -96.894, mean reward: -1.491 [-100.000, 23.540], mean action: 1.508 [0.000, 3.000],  loss: 41.581818, mse: 52670.615925, mean_q: 199.152813, mean_eps: 0.751552
 82960/100000: episode: 879, duration: 0.792s, episode steps: 111, steps per second: 140, episode reward: -105.028, mean reward: -0.946 [-100.000,  8.372], mean action: 1.604 [0.000, 3.000],  loss: 43.528734, mse: 53418.253625, mean_q: 202.432192, mean_eps: 0.751288
 83056/100000: episode: 880, duration: 0.695s, episode steps:  96, steps per second: 138, episode reward: -120.283, mean reward: -1.253 [-100.000,  8.820], mean action: 1.521 [0.000, 3.000],  loss: 58.250147, mse: 54853.113017, mean_q: 206.029036, mean_eps: 0.750977
 83113/100000: episode: 881, duration: 0.411s, episode steps:  57, steps per second: 139, episode reward: -119.692, mean reward: -2.100 [-100.000,  6.361], mean action: 1.649 [0.000, 3.000],  loss: 53.187589, mse: 53573.152344, mean_q: 200.598470, mean_eps: 0.750748
 83182/100000: episode: 882, duration: 0.492s, episode steps:  69, steps per second: 140, episode reward: -46.793, mean reward: -0.678 [-100.000, 10.716], mean action: 1.565 [0.000, 3.000],  loss: 69.711285, mse: 54221.383662, mean_q: 203.047976, mean_eps: 0.750559
 83315/100000: episode: 883, duration: 1.002s, episode steps: 133, steps per second: 133, episode reward: -77.892, mean reward: -0.586 [-100.000, 14.491], mean action: 1.744 [0.000, 3.000],  loss: 38.098102, mse: 53715.747034, mean_q: 202.261984, mean_eps: 0.750256
 83393/100000: episode: 884, duration: 0.793s, episode steps:  78, steps per second:  98, episode reward: -67.873, mean reward: -0.870 [-100.000,  9.358], mean action: 1.692 [0.000, 3.000],  loss: 39.029210, mse: 53327.039062, mean_q: 200.862537, mean_eps: 0.749940
 83460/100000: episode: 885, duration: 0.636s, episode steps:  67, steps per second: 105, episode reward: -97.471, mean reward: -1.455 [-100.000, 23.860], mean action: 1.448 [0.000, 3.000],  loss: 55.663926, mse: 55537.815940, mean_q: 206.430216, mean_eps: 0.749722
 83532/100000: episode: 886, duration: 0.743s, episode steps:  72, steps per second:  97, episode reward: -138.746, mean reward: -1.927 [-100.000, 11.536], mean action: 1.417 [0.000, 3.000],  loss: 35.161687, mse: 56939.849555, mean_q: 210.030767, mean_eps: 0.749513
 83625/100000: episode: 887, duration: 0.801s, episode steps:  93, steps per second: 116, episode reward: -57.427, mean reward: -0.617 [-100.000, 25.380], mean action: 1.591 [0.000, 3.000],  loss: 47.019658, mse: 55140.686324, mean_q: 203.359485, mean_eps: 0.749266
 83757/100000: episode: 888, duration: 1.182s, episode steps: 132, steps per second: 112, episode reward: 37.236, mean reward:  0.282 [-100.000, 105.182], mean action: 1.583 [0.000, 3.000],  loss: 40.895906, mse: 56349.845733, mean_q: 206.471425, mean_eps: 0.748928
 83838/100000: episode: 889, duration: 0.703s, episode steps:  81, steps per second: 115, episode reward: -79.040, mean reward: -0.976 [-100.000,  8.474], mean action: 1.765 [0.000, 3.000],  loss: 37.200128, mse: 54922.990548, mean_q: 201.552005, mean_eps: 0.748609
 83968/100000: episode: 890, duration: 1.148s, episode steps: 130, steps per second: 113, episode reward: -80.310, mean reward: -0.618 [-100.000, 13.296], mean action: 1.577 [0.000, 3.000],  loss: 39.209432, mse: 55734.816707, mean_q: 201.420913, mean_eps: 0.748293
 84091/100000: episode: 891, duration: 1.013s, episode steps: 123, steps per second: 121, episode reward: -120.549, mean reward: -0.980 [-100.000,  8.883], mean action: 1.390 [0.000, 3.000],  loss: 44.706855, mse: 54816.842734, mean_q: 200.529543, mean_eps: 0.747913
 84197/100000: episode: 892, duration: 0.844s, episode steps: 106, steps per second: 126, episode reward: -248.487, mean reward: -2.344 [-100.000,  1.581], mean action: 1.792 [0.000, 3.000],  loss: 42.064488, mse: 55882.194465, mean_q: 204.217223, mean_eps: 0.747569
 84314/100000: episode: 893, duration: 0.984s, episode steps: 117, steps per second: 119, episode reward: -79.463, mean reward: -0.679 [-100.000, 13.711], mean action: 1.769 [0.000, 3.000],  loss: 35.289945, mse: 54372.331664, mean_q: 200.665872, mean_eps: 0.747235
 84388/100000: episode: 894, duration: 0.558s, episode steps:  74, steps per second: 133, episode reward: -77.434, mean reward: -1.046 [-100.000,  8.258], mean action: 1.622 [0.000, 3.000],  loss: 61.089787, mse: 55624.790488, mean_q: 203.988979, mean_eps: 0.746949
 84477/100000: episode: 895, duration: 0.756s, episode steps:  89, steps per second: 118, episode reward: -94.535, mean reward: -1.062 [-100.000, 12.156], mean action: 1.697 [0.000, 3.000],  loss: 41.523416, mse: 54081.268302, mean_q: 200.859642, mean_eps: 0.746704
 84594/100000: episode: 896, duration: 1.329s, episode steps: 117, steps per second:  88, episode reward: -177.486, mean reward: -1.517 [-100.000,  5.550], mean action: 1.598 [0.000, 3.000],  loss: 39.885639, mse: 56267.957632, mean_q: 206.221308, mean_eps: 0.746395
 84693/100000: episode: 897, duration: 0.846s, episode steps:  99, steps per second: 117, episode reward: -75.394, mean reward: -0.762 [-100.000,  7.891], mean action: 1.586 [0.000, 3.000],  loss: 48.107666, mse: 54062.374961, mean_q: 198.217588, mean_eps: 0.746071
 84846/100000: episode: 898, duration: 1.100s, episode steps: 153, steps per second: 139, episode reward: -110.587, mean reward: -0.723 [-100.000, 15.859], mean action: 1.575 [0.000, 3.000],  loss: 61.316025, mse: 57172.982613, mean_q: 204.225486, mean_eps: 0.745693
 84950/100000: episode: 899, duration: 0.893s, episode steps: 104, steps per second: 117, episode reward: -113.275, mean reward: -1.089 [-100.000,  5.821], mean action: 1.519 [0.000, 3.000],  loss: 33.462934, mse: 58351.663011, mean_q: 208.966927, mean_eps: 0.745308
 85066/100000: episode: 900, duration: 1.939s, episode steps: 116, steps per second:  60, episode reward: -120.594, mean reward: -1.040 [-100.000, 32.531], mean action: 1.655 [0.000, 3.000],  loss: 41.888742, mse: 58327.529970, mean_q: 207.236593, mean_eps: 0.744978
 85149/100000: episode: 901, duration: 1.318s, episode steps:  83, steps per second:  63, episode reward: -53.856, mean reward: -0.649 [-100.000, 13.733], mean action: 1.735 [0.000, 3.000],  loss: 47.903339, mse: 57479.030168, mean_q: 205.524792, mean_eps: 0.744679
 85256/100000: episode: 902, duration: 1.332s, episode steps: 107, steps per second:  80, episode reward: -47.718, mean reward: -0.446 [-100.000, 19.065], mean action: 1.654 [0.000, 3.000],  loss: 52.500905, mse: 57734.153256, mean_q: 206.948857, mean_eps: 0.744394
 85374/100000: episode: 903, duration: 1.203s, episode steps: 118, steps per second:  98, episode reward: -73.307, mean reward: -0.621 [-100.000, 11.161], mean action: 1.610 [0.000, 3.000],  loss: 32.785915, mse: 57128.187235, mean_q: 203.611501, mean_eps: 0.744056
 85477/100000: episode: 904, duration: 1.035s, episode steps: 103, steps per second: 100, episode reward: -81.984, mean reward: -0.796 [-100.000, 13.181], mean action: 1.534 [0.000, 3.000],  loss: 50.895555, mse: 57723.506789, mean_q: 205.346946, mean_eps: 0.743725
 85638/100000: episode: 905, duration: 1.807s, episode steps: 161, steps per second:  89, episode reward: -82.772, mean reward: -0.514 [-100.000, 23.835], mean action: 1.733 [0.000, 3.000],  loss: 47.313897, mse: 57433.893367, mean_q: 203.495801, mean_eps: 0.743329
 85747/100000: episode: 906, duration: 1.084s, episode steps: 109, steps per second: 101, episode reward: -73.585, mean reward: -0.675 [-100.000, 33.694], mean action: 1.780 [0.000, 3.000],  loss: 43.094181, mse: 58036.217030, mean_q: 205.929902, mean_eps: 0.742924
 85814/100000: episode: 907, duration: 0.772s, episode steps:  67, steps per second:  87, episode reward: -44.958, mean reward: -0.671 [-100.000, 10.800], mean action: 1.687 [0.000, 3.000],  loss: 84.932633, mse: 60350.070138, mean_q: 211.620330, mean_eps: 0.742660
 85925/100000: episode: 908, duration: 1.319s, episode steps: 111, steps per second:  84, episode reward: -79.544, mean reward: -0.717 [-100.000, 10.741], mean action: 1.676 [0.000, 3.000],  loss: 40.141962, mse: 60245.686831, mean_q: 213.094890, mean_eps: 0.742393
 86050/100000: episode: 909, duration: 1.224s, episode steps: 125, steps per second: 102, episode reward: -55.411, mean reward: -0.443 [-100.000, 11.860], mean action: 1.624 [0.000, 3.000],  loss: 49.487047, mse: 58511.896531, mean_q: 210.470457, mean_eps: 0.742039
 86138/100000: episode: 910, duration: 1.058s, episode steps:  88, steps per second:  83, episode reward: -81.580, mean reward: -0.927 [-100.000, 14.110], mean action: 1.591 [0.000, 3.000],  loss: 38.376915, mse: 58525.341175, mean_q: 208.949444, mean_eps: 0.741720
 86219/100000: episode: 911, duration: 0.749s, episode steps:  81, steps per second: 108, episode reward: -102.752, mean reward: -1.269 [-100.000,  9.118], mean action: 1.457 [0.000, 3.000],  loss: 52.314852, mse: 60102.003231, mean_q: 213.147568, mean_eps: 0.741466
 86323/100000: episode: 912, duration: 0.965s, episode steps: 104, steps per second: 108, episode reward: -17.030, mean reward: -0.164 [-100.000, 110.670], mean action: 1.510 [0.000, 3.000],  loss: 49.914114, mse: 60521.300030, mean_q: 213.419650, mean_eps: 0.741189
 86434/100000: episode: 913, duration: 0.969s, episode steps: 111, steps per second: 115, episode reward: -109.774, mean reward: -0.989 [-100.000, 10.936], mean action: 1.613 [0.000, 3.000],  loss: 42.545641, mse: 60479.668321, mean_q: 213.582778, mean_eps: 0.740866
 86496/100000: episode: 914, duration: 0.670s, episode steps:  62, steps per second:  93, episode reward: -46.358, mean reward: -0.748 [-100.000, 12.643], mean action: 1.806 [0.000, 3.000],  loss: 37.981642, mse: 62237.296812, mean_q: 215.033034, mean_eps: 0.740607
 86587/100000: episode: 915, duration: 0.834s, episode steps:  91, steps per second: 109, episode reward: -64.747, mean reward: -0.712 [-100.000,  7.760], mean action: 1.670 [0.000, 3.000],  loss: 31.189976, mse: 60469.822931, mean_q: 212.092677, mean_eps: 0.740377
 86643/100000: episode: 916, duration: 0.536s, episode steps:  56, steps per second: 104, episode reward: -122.554, mean reward: -2.188 [-100.000,  7.638], mean action: 1.518 [0.000, 3.000],  loss: 28.683647, mse: 60520.793597, mean_q: 211.170417, mean_eps: 0.740156
 86771/100000: episode: 917, duration: 1.147s, episode steps: 128, steps per second: 112, episode reward: -98.537, mean reward: -0.770 [-100.000, 18.393], mean action: 1.430 [0.000, 3.000],  loss: 49.765868, mse: 62775.591370, mean_q: 215.728002, mean_eps: 0.739880
 86868/100000: episode: 918, duration: 0.760s, episode steps:  97, steps per second: 128, episode reward: -103.889, mean reward: -1.071 [-100.000,  4.483], mean action: 1.464 [0.000, 3.000],  loss: 34.557012, mse: 62726.972334, mean_q: 214.411702, mean_eps: 0.739543
 86969/100000: episode: 919, duration: 0.788s, episode steps: 101, steps per second: 128, episode reward: -76.705, mean reward: -0.759 [-100.000, 15.538], mean action: 1.683 [0.000, 3.000],  loss: 46.772189, mse: 62262.363629, mean_q: 211.373566, mean_eps: 0.739246
 87046/100000: episode: 920, duration: 0.791s, episode steps:  77, steps per second:  97, episode reward: -71.516, mean reward: -0.929 [-100.000,  6.063], mean action: 1.338 [0.000, 3.000],  loss: 44.597306, mse: 63157.502029, mean_q: 217.147828, mean_eps: 0.738979
 87125/100000: episode: 921, duration: 0.795s, episode steps:  79, steps per second:  99, episode reward: -25.013, mean reward: -0.317 [-100.000, 18.147], mean action: 1.620 [0.000, 3.000],  loss: 61.969658, mse: 63363.865407, mean_q: 215.706529, mean_eps: 0.738745
 87202/100000: episode: 922, duration: 0.872s, episode steps:  77, steps per second:  88, episode reward: -64.838, mean reward: -0.842 [-100.000,  6.821], mean action: 1.701 [0.000, 3.000],  loss: 78.689319, mse: 63508.218395, mean_q: 217.627510, mean_eps: 0.738511
 87326/100000: episode: 923, duration: 1.231s, episode steps: 124, steps per second: 101, episode reward: -90.558, mean reward: -0.730 [-100.000, 11.820], mean action: 1.726 [0.000, 3.000],  loss: 76.541534, mse: 64075.970766, mean_q: 219.786616, mean_eps: 0.738209
 87394/100000: episode: 924, duration: 0.520s, episode steps:  68, steps per second: 131, episode reward: -64.694, mean reward: -0.951 [-100.000, 10.899], mean action: 1.574 [0.000, 3.000],  loss: 58.536129, mse: 63145.402631, mean_q: 217.536412, mean_eps: 0.737922
 87480/100000: episode: 925, duration: 0.679s, episode steps:  86, steps per second: 127, episode reward: -76.931, mean reward: -0.895 [-100.000, 18.297], mean action: 1.453 [0.000, 3.000],  loss: 74.178692, mse: 61928.968932, mean_q: 215.573154, mean_eps: 0.737691
 87568/100000: episode: 926, duration: 0.754s, episode steps:  88, steps per second: 117, episode reward: -51.892, mean reward: -0.590 [-100.000,  7.916], mean action: 1.761 [0.000, 3.000],  loss: 47.551859, mse: 63510.469505, mean_q: 217.916377, mean_eps: 0.737429
 87685/100000: episode: 927, duration: 0.968s, episode steps: 117, steps per second: 121, episode reward: -66.677, mean reward: -0.570 [-100.000, 23.621], mean action: 1.504 [0.000, 3.000],  loss: 99.162391, mse: 61896.878806, mean_q: 215.115291, mean_eps: 0.737122
 87761/100000: episode: 928, duration: 0.716s, episode steps:  76, steps per second: 106, episode reward: -93.731, mean reward: -1.233 [-100.000,  7.972], mean action: 1.474 [0.000, 3.000],  loss: 106.632394, mse: 61262.756014, mean_q: 218.266679, mean_eps: 0.736832
 87897/100000: episode: 929, duration: 1.237s, episode steps: 136, steps per second: 110, episode reward: -32.010, mean reward: -0.235 [-100.000,  9.450], mean action: 1.610 [0.000, 3.000],  loss: 48.331328, mse: 61357.624684, mean_q: 216.527017, mean_eps: 0.736514
 87988/100000: episode: 930, duration: 0.846s, episode steps:  91, steps per second: 108, episode reward: -119.715, mean reward: -1.316 [-100.000,  6.984], mean action: 1.527 [0.000, 3.000],  loss: 47.726990, mse: 64144.137706, mean_q: 221.806807, mean_eps: 0.736174
 88086/100000: episode: 931, duration: 0.865s, episode steps:  98, steps per second: 113, episode reward: -56.200, mean reward: -0.573 [-100.000,  9.137], mean action: 1.582 [0.000, 3.000],  loss: 77.806103, mse: 64146.768893, mean_q: 222.338802, mean_eps: 0.735890
 88195/100000: episode: 932, duration: 1.001s, episode steps: 109, steps per second: 109, episode reward: -143.759, mean reward: -1.319 [-100.000,  4.841], mean action: 1.706 [0.000, 3.000],  loss: 56.390310, mse: 64679.124319, mean_q: 223.039094, mean_eps: 0.735580
 88356/100000: episode: 933, duration: 1.352s, episode steps: 161, steps per second: 119, episode reward: -83.752, mean reward: -0.520 [-100.000, 14.608], mean action: 1.714 [0.000, 3.000],  loss: 66.110906, mse: 62476.495875, mean_q: 215.680942, mean_eps: 0.735175
 88476/100000: episode: 934, duration: 1.127s, episode steps: 120, steps per second: 107, episode reward: -57.204, mean reward: -0.477 [-100.000, 10.499], mean action: 1.625 [0.000, 3.000],  loss: 43.610984, mse: 63918.879915, mean_q: 220.959398, mean_eps: 0.734754
 88576/100000: episode: 935, duration: 0.848s, episode steps: 100, steps per second: 118, episode reward: -101.661, mean reward: -1.017 [-100.000,  8.004], mean action: 1.670 [0.000, 3.000],  loss: 55.830857, mse: 64431.845820, mean_q: 219.381326, mean_eps: 0.734424
 88646/100000: episode: 936, duration: 0.562s, episode steps:  70, steps per second: 125, episode reward: -91.176, mean reward: -1.303 [-100.000,  6.103], mean action: 1.414 [0.000, 3.000],  loss: 44.619015, mse: 66823.688281, mean_q: 228.524100, mean_eps: 0.734168
 88763/100000: episode: 937, duration: 0.949s, episode steps: 117, steps per second: 123, episode reward: -221.983, mean reward: -1.897 [-100.000, 71.032], mean action: 1.496 [0.000, 3.000],  loss: 62.255662, mse: 64542.418670, mean_q: 221.430240, mean_eps: 0.733888
 88824/100000: episode: 938, duration: 0.494s, episode steps:  61, steps per second: 123, episode reward: -35.182, mean reward: -0.577 [-100.000, 13.201], mean action: 1.754 [0.000, 3.000],  loss: 73.728068, mse: 61291.008709, mean_q: 214.196346, mean_eps: 0.733621
 88933/100000: episode: 939, duration: 0.889s, episode steps: 109, steps per second: 123, episode reward: -33.378, mean reward: -0.306 [-100.000, 24.807], mean action: 1.477 [0.000, 3.000],  loss: 45.114485, mse: 62659.813253, mean_q: 216.680369, mean_eps: 0.733366
 89057/100000: episode: 940, duration: 1.191s, episode steps: 124, steps per second: 104, episode reward: -126.391, mean reward: -1.019 [-100.000,  3.733], mean action: 1.565 [0.000, 3.000],  loss: 38.794125, mse: 62352.052325, mean_q: 217.480870, mean_eps: 0.733017
 89158/100000: episode: 941, duration: 1.023s, episode steps: 101, steps per second:  99, episode reward: -76.656, mean reward: -0.759 [-100.000, 12.230], mean action: 1.723 [0.000, 3.000],  loss: 76.715625, mse: 63645.746558, mean_q: 219.363761, mean_eps: 0.732679
 89259/100000: episode: 942, duration: 1.004s, episode steps: 101, steps per second: 101, episode reward: -53.300, mean reward: -0.528 [-100.000, 19.124], mean action: 1.703 [0.000, 3.000],  loss: 47.645384, mse: 61889.296952, mean_q: 215.500419, mean_eps: 0.732376
 89326/100000: episode: 943, duration: 0.603s, episode steps:  67, steps per second: 111, episode reward: -149.510, mean reward: -2.231 [-100.000,  8.792], mean action: 1.433 [0.000, 3.000],  loss: 52.508439, mse: 65432.360191, mean_q: 223.051741, mean_eps: 0.732124
 89457/100000: episode: 944, duration: 1.313s, episode steps: 131, steps per second: 100, episode reward: -120.341, mean reward: -0.919 [-100.000,  7.118], mean action: 1.588 [0.000, 3.000],  loss: 59.807332, mse: 65194.044072, mean_q: 223.531657, mean_eps: 0.731827
 89530/100000: episode: 945, duration: 0.769s, episode steps:  73, steps per second:  95, episode reward: -84.800, mean reward: -1.162 [-100.000, 10.598], mean action: 1.507 [0.000, 3.000],  loss: 46.114527, mse: 64867.292755, mean_q: 221.931527, mean_eps: 0.731521
 89630/100000: episode: 946, duration: 1.156s, episode steps: 100, steps per second:  86, episode reward: -98.550, mean reward: -0.986 [-100.000, 11.570], mean action: 1.790 [0.000, 3.000],  loss: 37.976212, mse: 63979.407344, mean_q: 221.475758, mean_eps: 0.731262
 89702/100000: episode: 947, duration: 0.865s, episode steps:  72, steps per second:  83, episode reward: -95.754, mean reward: -1.330 [-100.000, 19.567], mean action: 1.611 [0.000, 3.000],  loss: 36.344836, mse: 63826.608941, mean_q: 223.346463, mean_eps: 0.731003
 89855/100000: episode: 948, duration: 1.832s, episode steps: 153, steps per second:  84, episode reward: -53.179, mean reward: -0.348 [-100.000, 23.653], mean action: 1.680 [0.000, 3.000],  loss: 47.919940, mse: 64521.656812, mean_q: 223.560314, mean_eps: 0.730666
 89963/100000: episode: 949, duration: 0.921s, episode steps: 108, steps per second: 117, episode reward: -125.739, mean reward: -1.164 [-100.000,  8.725], mean action: 1.824 [0.000, 3.000],  loss: 69.689213, mse: 64780.748336, mean_q: 223.348557, mean_eps: 0.730275
 90082/100000: episode: 950, duration: 1.055s, episode steps: 119, steps per second: 113, episode reward: -37.773, mean reward: -0.317 [-100.000, 10.549], mean action: 1.689 [0.000, 3.000],  loss: 62.923485, mse: 64116.922794, mean_q: 222.749409, mean_eps: 0.729934
 90149/100000: episode: 951, duration: 0.582s, episode steps:  67, steps per second: 115, episode reward: -67.679, mean reward: -1.010 [-100.000,  7.235], mean action: 1.463 [0.000, 3.000],  loss: 70.167590, mse: 64248.877740, mean_q: 224.537487, mean_eps: 0.729655
 90283/100000: episode: 952, duration: 1.236s, episode steps: 134, steps per second: 108, episode reward: -249.141, mean reward: -1.859 [-100.000, 57.313], mean action: 1.716 [0.000, 3.000],  loss: 43.343876, mse: 65165.333460, mean_q: 223.516840, mean_eps: 0.729354
 90357/100000: episode: 953, duration: 0.635s, episode steps:  74, steps per second: 117, episode reward: -4.843, mean reward: -0.065 [-100.000, 12.546], mean action: 1.581 [0.000, 3.000],  loss: 70.041873, mse: 66255.177893, mean_q: 223.952027, mean_eps: 0.729041
 90455/100000: episode: 954, duration: 0.842s, episode steps:  98, steps per second: 116, episode reward: -55.868, mean reward: -0.570 [-100.000, 19.476], mean action: 1.510 [0.000, 3.000],  loss: 47.954450, mse: 64519.113441, mean_q: 221.675048, mean_eps: 0.728784
 90600/100000: episode: 955, duration: 1.252s, episode steps: 145, steps per second: 116, episode reward: -31.139, mean reward: -0.215 [-100.000, 19.248], mean action: 1.731 [0.000, 3.000],  loss: 59.115084, mse: 67079.723707, mean_q: 228.553679, mean_eps: 0.728419
 90683/100000: episode: 956, duration: 0.645s, episode steps:  83, steps per second: 129, episode reward: -59.743, mean reward: -0.720 [-100.000,  8.091], mean action: 1.699 [0.000, 3.000],  loss: 35.958381, mse: 66919.912180, mean_q: 228.624002, mean_eps: 0.728077
 90824/100000: episode: 957, duration: 1.182s, episode steps: 141, steps per second: 119, episode reward: -112.682, mean reward: -0.799 [-100.000,  9.271], mean action: 1.596 [0.000, 3.000],  loss: 68.539872, mse: 66293.550338, mean_q: 226.284642, mean_eps: 0.727741
 90968/100000: episode: 958, duration: 1.199s, episode steps: 144, steps per second: 120, episode reward: -73.924, mean reward: -0.513 [-100.000,  7.926], mean action: 1.569 [0.000, 3.000],  loss: 65.546837, mse: 65683.813775, mean_q: 226.983716, mean_eps: 0.727313
 91103/100000: episode: 959, duration: 1.083s, episode steps: 135, steps per second: 125, episode reward: -74.926, mean reward: -0.555 [-100.000, 16.214], mean action: 1.667 [0.000, 3.000],  loss: 93.966433, mse: 64676.259549, mean_q: 221.684906, mean_eps: 0.726895
 91205/100000: episode: 960, duration: 0.792s, episode steps: 102, steps per second: 129, episode reward: -44.128, mean reward: -0.433 [-100.000, 12.776], mean action: 1.706 [0.000, 3.000],  loss: 69.013606, mse: 63744.454657, mean_q: 221.080784, mean_eps: 0.726540
 91303/100000: episode: 961, duration: 0.734s, episode steps:  98, steps per second: 133, episode reward: -132.456, mean reward: -1.352 [-100.000,  5.673], mean action: 1.827 [0.000, 3.000],  loss: 87.192226, mse: 64534.902463, mean_q: 222.696490, mean_eps: 0.726239
 91393/100000: episode: 962, duration: 0.694s, episode steps:  90, steps per second: 130, episode reward: -103.573, mean reward: -1.151 [-100.000,  8.180], mean action: 1.678 [0.000, 3.000],  loss: 66.121119, mse: 66975.836545, mean_q: 229.740049, mean_eps: 0.725957
 91515/100000: episode: 963, duration: 0.918s, episode steps: 122, steps per second: 133, episode reward: -140.833, mean reward: -1.154 [-100.000,  4.652], mean action: 1.582 [0.000, 3.000],  loss: 54.225817, mse: 66442.373015, mean_q: 225.312831, mean_eps: 0.725639
 91632/100000: episode: 964, duration: 0.956s, episode steps: 117, steps per second: 122, episode reward: -68.006, mean reward: -0.581 [-100.000, 18.662], mean action: 1.718 [0.000, 3.000],  loss: 48.779500, mse: 65112.859141, mean_q: 223.632689, mean_eps: 0.725281
 91747/100000: episode: 965, duration: 1.098s, episode steps: 115, steps per second: 105, episode reward: -37.608, mean reward: -0.327 [-100.000, 12.232], mean action: 1.635 [0.000, 3.000],  loss: 36.708399, mse: 67159.101936, mean_q: 227.994932, mean_eps: 0.724933
 91903/100000: episode: 966, duration: 1.450s, episode steps: 156, steps per second: 108, episode reward: -48.119, mean reward: -0.308 [-100.000, 17.071], mean action: 1.628 [0.000, 3.000],  loss: 69.306826, mse: 65660.580003, mean_q: 223.290303, mean_eps: 0.724526
 91991/100000: episode: 967, duration: 0.736s, episode steps:  88, steps per second: 120, episode reward: -19.949, mean reward: -0.227 [-100.000, 15.866], mean action: 1.568 [0.000, 3.000],  loss: 60.183612, mse: 65708.122337, mean_q: 225.817971, mean_eps: 0.724160
 92086/100000: episode: 968, duration: 0.958s, episode steps:  95, steps per second:  99, episode reward: -65.367, mean reward: -0.688 [-100.000, 10.980], mean action: 1.684 [0.000, 3.000],  loss: 39.311162, mse: 65768.664145, mean_q: 224.069730, mean_eps: 0.723886
 92168/100000: episode: 969, duration: 0.689s, episode steps:  82, steps per second: 119, episode reward: -114.140, mean reward: -1.392 [-100.000,  5.452], mean action: 1.866 [0.000, 3.000],  loss: 57.206369, mse: 65248.177925, mean_q: 219.790364, mean_eps: 0.723621
 92235/100000: episode: 970, duration: 0.487s, episode steps:  67, steps per second: 138, episode reward: -54.702, mean reward: -0.816 [-100.000, 24.279], mean action: 1.701 [0.000, 3.000],  loss: 74.063624, mse: 66926.388759, mean_q: 228.035940, mean_eps: 0.723397
 92323/100000: episode: 971, duration: 0.653s, episode steps:  88, steps per second: 135, episode reward: -49.903, mean reward: -0.567 [-100.000, 11.451], mean action: 1.636 [0.000, 3.000],  loss: 43.959896, mse: 64016.279830, mean_q: 220.712449, mean_eps: 0.723164
 92417/100000: episode: 972, duration: 0.789s, episode steps:  94, steps per second: 119, episode reward: -113.371, mean reward: -1.206 [-100.000,  9.084], mean action: 1.649 [0.000, 3.000],  loss: 58.533752, mse: 63937.130818, mean_q: 221.408843, mean_eps: 0.722892
 92557/100000: episode: 973, duration: 1.059s, episode steps: 140, steps per second: 132, episode reward: -48.273, mean reward: -0.345 [-100.000, 19.598], mean action: 1.650 [0.000, 3.000],  loss: 45.794344, mse: 66030.230357, mean_q: 227.659352, mean_eps: 0.722541
 92682/100000: episode: 974, duration: 1.007s, episode steps: 125, steps per second: 124, episode reward: -72.210, mean reward: -0.578 [-100.000, 11.797], mean action: 1.704 [0.000, 3.000],  loss: 58.337576, mse: 63225.747031, mean_q: 220.743867, mean_eps: 0.722143
 92804/100000: episode: 975, duration: 1.072s, episode steps: 122, steps per second: 114, episode reward: -142.639, mean reward: -1.169 [-100.000,  7.037], mean action: 1.410 [0.000, 3.000],  loss: 51.708995, mse: 62608.686668, mean_q: 219.467967, mean_eps: 0.721772
 92878/100000: episode: 976, duration: 0.861s, episode steps:  74, steps per second:  86, episode reward: -27.932, mean reward: -0.377 [-100.000, 21.879], mean action: 1.473 [0.000, 3.000],  loss: 46.799160, mse: 64463.191829, mean_q: 222.847528, mean_eps: 0.721479
 92998/100000: episode: 977, duration: 1.022s, episode steps: 120, steps per second: 117, episode reward: -86.020, mean reward: -0.717 [-100.000,  7.337], mean action: 1.567 [0.000, 3.000],  loss: 64.157478, mse: 63702.510677, mean_q: 221.145261, mean_eps: 0.721187
 93119/100000: episode: 978, duration: 1.159s, episode steps: 121, steps per second: 104, episode reward: -89.018, mean reward: -0.736 [-100.000,  6.480], mean action: 1.727 [0.000, 3.000],  loss: 62.625153, mse: 64116.151795, mean_q: 223.904063, mean_eps: 0.720826
 93214/100000: episode: 979, duration: 0.896s, episode steps:  95, steps per second: 106, episode reward: -143.108, mean reward: -1.506 [-100.000,  7.489], mean action: 1.737 [0.000, 3.000],  loss: 43.325881, mse: 64598.355345, mean_q: 225.682152, mean_eps: 0.720502
 93336/100000: episode: 980, duration: 1.018s, episode steps: 122, steps per second: 120, episode reward: -76.290, mean reward: -0.625 [-100.000,  6.961], mean action: 1.689 [0.000, 3.000],  loss: 48.883307, mse: 63527.859087, mean_q: 223.865042, mean_eps: 0.720176
 93403/100000: episode: 981, duration: 0.501s, episode steps:  67, steps per second: 134, episode reward: -66.715, mean reward: -0.996 [-100.000, 19.648], mean action: 1.448 [0.000, 3.000],  loss: 43.965316, mse: 64346.788654, mean_q: 224.320301, mean_eps: 0.719893
 93487/100000: episode: 982, duration: 0.621s, episode steps:  84, steps per second: 135, episode reward: -48.828, mean reward: -0.581 [-100.000, 10.927], mean action: 1.595 [0.000, 3.000],  loss: 91.468360, mse: 64115.623326, mean_q: 223.364000, mean_eps: 0.719667
 93647/100000: episode: 983, duration: 1.271s, episode steps: 160, steps per second: 126, episode reward: -211.310, mean reward: -1.321 [-100.000, 37.385], mean action: 1.688 [0.000, 3.000],  loss: 48.625810, mse: 66812.886914, mean_q: 232.857543, mean_eps: 0.719301
 93757/100000: episode: 984, duration: 1.152s, episode steps: 110, steps per second:  95, episode reward: -160.366, mean reward: -1.458 [-100.000,  7.988], mean action: 1.573 [0.000, 3.000],  loss: 50.741977, mse: 65915.247692, mean_q: 230.285735, mean_eps: 0.718895
 93856/100000: episode: 985, duration: 1.158s, episode steps:  99, steps per second:  86, episode reward: -118.072, mean reward: -1.193 [-100.000,  9.466], mean action: 1.667 [0.000, 3.000],  loss: 37.804095, mse: 66547.410906, mean_q: 230.266773, mean_eps: 0.718582
 93961/100000: episode: 986, duration: 0.921s, episode steps: 105, steps per second: 114, episode reward: -145.872, mean reward: -1.389 [-100.000,  8.056], mean action: 1.543 [0.000, 3.000],  loss: 106.685271, mse: 66847.150112, mean_q: 232.306785, mean_eps: 0.718276
 94056/100000: episode: 987, duration: 0.789s, episode steps:  95, steps per second: 120, episode reward: -89.974, mean reward: -0.947 [-100.000,  8.291], mean action: 1.621 [0.000, 3.000],  loss: 39.276182, mse: 66961.521258, mean_q: 232.790077, mean_eps: 0.717976
 94175/100000: episode: 988, duration: 0.851s, episode steps: 119, steps per second: 140, episode reward: 33.158, mean reward:  0.279 [-100.000, 94.273], mean action: 1.496 [0.000, 3.000],  loss: 47.193709, mse: 67890.840697, mean_q: 234.449999, mean_eps: 0.717655
 94272/100000: episode: 989, duration: 0.711s, episode steps:  97, steps per second: 136, episode reward: -21.982, mean reward: -0.227 [-100.000,  7.138], mean action: 1.711 [0.000, 3.000],  loss: 47.760459, mse: 67457.858570, mean_q: 232.573867, mean_eps: 0.717331
 94348/100000: episode: 990, duration: 0.520s, episode steps:  76, steps per second: 146, episode reward: -69.249, mean reward: -0.911 [-100.000, 11.365], mean action: 1.487 [0.000, 3.000],  loss: 51.377849, mse: 68356.930818, mean_q: 234.245399, mean_eps: 0.717072
 94434/100000: episode: 991, duration: 0.609s, episode steps:  86, steps per second: 141, episode reward: -125.401, mean reward: -1.458 [-100.000, 11.845], mean action: 1.558 [0.000, 3.000],  loss: 38.062720, mse: 69142.136174, mean_q: 239.147773, mean_eps: 0.716828
 94559/100000: episode: 992, duration: 0.848s, episode steps: 125, steps per second: 147, episode reward: -249.323, mean reward: -1.995 [-100.000, 69.316], mean action: 1.656 [0.000, 3.000],  loss: 57.625242, mse: 69629.923156, mean_q: 237.167211, mean_eps: 0.716512
 95559/100000: episode: 993, duration: 8.013s, episode steps: 1000, steps per second: 125, episode reward: 27.469, mean reward:  0.027 [-25.879, 32.726], mean action: 1.503 [0.000, 3.000],  loss: 68.424306, mse: 68293.940672, mean_q: 236.099674, mean_eps: 0.714824
 95660/100000: episode: 994, duration: 0.691s, episode steps: 101, steps per second: 146, episode reward: -194.615, mean reward: -1.927 [-100.000,  1.495], mean action: 1.505 [0.000, 3.000],  loss: 40.728551, mse: 66491.284731, mean_q: 234.428208, mean_eps: 0.713173
 95757/100000: episode: 995, duration: 0.704s, episode steps:  97, steps per second: 138, episode reward: -169.412, mean reward: -1.747 [-100.000, 11.336], mean action: 1.639 [0.000, 3.000],  loss: 75.421294, mse: 67351.479543, mean_q: 235.473159, mean_eps: 0.712876
 95823/100000: episode: 996, duration: 0.501s, episode steps:  66, steps per second: 132, episode reward: -91.132, mean reward: -1.381 [-100.000,  8.871], mean action: 1.682 [0.000, 3.000],  loss: 95.719886, mse: 67468.402758, mean_q: 237.149409, mean_eps: 0.712631
 95952/100000: episode: 997, duration: 0.981s, episode steps: 129, steps per second: 132, episode reward: -167.555, mean reward: -1.299 [-100.000,  9.579], mean action: 1.574 [0.000, 3.000],  loss: 86.055371, mse: 68416.734526, mean_q: 239.691347, mean_eps: 0.712339
 96044/100000: episode: 998, duration: 0.654s, episode steps:  92, steps per second: 141, episode reward: -47.025, mean reward: -0.511 [-100.000, 14.907], mean action: 1.663 [0.000, 3.000],  loss: 68.858469, mse: 67211.775645, mean_q: 236.198651, mean_eps: 0.712008
 96167/100000: episode: 999, duration: 0.858s, episode steps: 123, steps per second: 143, episode reward: -46.141, mean reward: -0.375 [-100.000, 16.748], mean action: 1.610 [0.000, 3.000],  loss: 78.228959, mse: 66239.815581, mean_q: 233.281712, mean_eps: 0.711685
 96231/100000: episode: 1000, duration: 0.431s, episode steps:  64, steps per second: 149, episode reward: -169.683, mean reward: -2.651 [-100.000,  4.365], mean action: 1.531 [0.000, 3.000],  loss: 93.333248, mse: 66401.551941, mean_q: 233.228688, mean_eps: 0.711404
 96309/100000: episode: 1001, duration: 0.527s, episode steps:  78, steps per second: 148, episode reward: -54.832, mean reward: -0.703 [-100.000, 20.409], mean action: 1.833 [0.000, 3.000],  loss: 92.701662, mse: 67513.197867, mean_q: 236.506761, mean_eps: 0.711191
 96468/100000: episode: 1002, duration: 1.162s, episode steps: 159, steps per second: 137, episode reward: -41.394, mean reward: -0.260 [-100.000, 11.246], mean action: 1.730 [0.000, 3.000],  loss: 56.846754, mse: 69022.694649, mean_q: 238.520247, mean_eps: 0.710836
 96575/100000: episode: 1003, duration: 0.795s, episode steps: 107, steps per second: 135, episode reward: -94.941, mean reward: -0.887 [-100.000,  5.843], mean action: 1.654 [0.000, 3.000],  loss: 49.812476, mse: 69696.750475, mean_q: 239.640163, mean_eps: 0.710437
 96701/100000: episode: 1004, duration: 0.885s, episode steps: 126, steps per second: 142, episode reward: -26.013, mean reward: -0.206 [-100.000, 72.847], mean action: 1.643 [0.000, 3.000],  loss: 59.747662, mse: 70649.757750, mean_q: 241.440688, mean_eps: 0.710087
 96817/100000: episode: 1005, duration: 0.822s, episode steps: 116, steps per second: 141, episode reward: -166.730, mean reward: -1.437 [-100.000, 88.481], mean action: 1.672 [0.000, 3.000],  loss: 45.719163, mse: 70718.847993, mean_q: 241.744657, mean_eps: 0.709724
 96923/100000: episode: 1006, duration: 0.711s, episode steps: 106, steps per second: 149, episode reward: -135.051, mean reward: -1.274 [-100.000, 17.883], mean action: 1.491 [0.000, 3.000],  loss: 72.493791, mse: 69049.043227, mean_q: 239.531351, mean_eps: 0.709391
 97079/100000: episode: 1007, duration: 1.192s, episode steps: 156, steps per second: 131, episode reward: -39.470, mean reward: -0.253 [-100.000, 17.261], mean action: 1.564 [0.000, 3.000],  loss: 51.388104, mse: 68241.446014, mean_q: 235.696179, mean_eps: 0.708998
 97155/100000: episode: 1008, duration: 0.619s, episode steps:  76, steps per second: 123, episode reward: -72.379, mean reward: -0.952 [-100.000,  7.164], mean action: 1.500 [0.000, 3.000],  loss: 69.041232, mse: 70033.609169, mean_q: 238.841897, mean_eps: 0.708651
 97254/100000: episode: 1009, duration: 0.830s, episode steps:  99, steps per second: 119, episode reward: -109.338, mean reward: -1.104 [-100.000,  5.509], mean action: 1.859 [0.000, 3.000],  loss: 47.804948, mse: 69454.633089, mean_q: 239.006269, mean_eps: 0.708388
 97405/100000: episode: 1010, duration: 1.300s, episode steps: 151, steps per second: 116, episode reward: -310.095, mean reward: -2.054 [-100.000, 54.989], mean action: 1.675 [0.000, 3.000],  loss: 50.260546, mse: 69013.754398, mean_q: 239.227196, mean_eps: 0.708013
 97504/100000: episode: 1011, duration: 0.733s, episode steps:  99, steps per second: 135, episode reward: -89.649, mean reward: -0.906 [-100.000,  6.875], mean action: 1.747 [0.000, 3.000],  loss: 62.916754, mse: 72184.128551, mean_q: 247.279899, mean_eps: 0.707638
 97593/100000: episode: 1012, duration: 0.704s, episode steps:  89, steps per second: 126, episode reward: -36.742, mean reward: -0.413 [-100.000,  8.397], mean action: 1.697 [0.000, 3.000],  loss: 48.965007, mse: 69952.055653, mean_q: 241.292225, mean_eps: 0.707356
 97689/100000: episode: 1013, duration: 0.674s, episode steps:  96, steps per second: 142, episode reward: -90.016, mean reward: -0.938 [-100.000, 10.748], mean action: 1.490 [0.000, 3.000],  loss: 65.564668, mse: 69558.159383, mean_q: 237.562387, mean_eps: 0.707079
 97801/100000: episode: 1014, duration: 0.809s, episode steps: 112, steps per second: 138, episode reward: -29.086, mean reward: -0.260 [-100.000,  6.697], mean action: 1.777 [0.000, 3.000],  loss: 65.015374, mse: 70618.709682, mean_q: 240.962285, mean_eps: 0.706766
 97903/100000: episode: 1015, duration: 0.731s, episode steps: 102, steps per second: 140, episode reward: -166.716, mean reward: -1.634 [-100.000,  7.223], mean action: 1.373 [0.000, 3.000],  loss: 75.747019, mse: 70108.982460, mean_q: 239.490092, mean_eps: 0.706446
 98045/100000: episode: 1016, duration: 0.967s, episode steps: 142, steps per second: 147, episode reward: -283.998, mean reward: -2.000 [-100.000, 23.913], mean action: 1.599 [0.000, 3.000],  loss: 73.059086, mse: 71013.111383, mean_q: 240.980272, mean_eps: 0.706080
 98160/100000: episode: 1017, duration: 0.824s, episode steps: 115, steps per second: 140, episode reward: -26.426, mean reward: -0.230 [-100.000, 25.480], mean action: 1.826 [0.000, 3.000],  loss: 45.392421, mse: 72296.375068, mean_q: 245.845049, mean_eps: 0.705694
 98233/100000: episode: 1018, duration: 0.509s, episode steps:  73, steps per second: 143, episode reward: -49.488, mean reward: -0.678 [-100.000, 11.529], mean action: 1.767 [0.000, 3.000],  loss: 41.299744, mse: 73054.369917, mean_q: 247.624656, mean_eps: 0.705412
 98325/100000: episode: 1019, duration: 0.686s, episode steps:  92, steps per second: 134, episode reward: -9.375, mean reward: -0.102 [-100.000, 11.889], mean action: 1.848 [0.000, 3.000],  loss: 47.481248, mse: 72131.765965, mean_q: 244.472925, mean_eps: 0.705165
 98446/100000: episode: 1020, duration: 0.960s, episode steps: 121, steps per second: 126, episode reward: -85.097, mean reward: -0.703 [-100.000,  8.263], mean action: 1.702 [0.000, 3.000],  loss: 75.046765, mse: 75026.951511, mean_q: 249.960224, mean_eps: 0.704845
 98582/100000: episode: 1021, duration: 1.063s, episode steps: 136, steps per second: 128, episode reward: -54.915, mean reward: -0.404 [-100.000, 46.552], mean action: 1.522 [0.000, 3.000],  loss: 47.136228, mse: 73550.566291, mean_q: 246.932246, mean_eps: 0.704459
 98668/100000: episode: 1022, duration: 0.648s, episode steps:  86, steps per second: 133, episode reward: -71.768, mean reward: -0.835 [-100.000,  6.761], mean action: 1.733 [0.000, 3.000],  loss: 91.842448, mse: 73655.424010, mean_q: 248.908689, mean_eps: 0.704127
 98788/100000: episode: 1023, duration: 1.042s, episode steps: 120, steps per second: 115, episode reward: -91.491, mean reward: -0.762 [-100.000,  7.553], mean action: 1.633 [0.000, 3.000],  loss: 56.058380, mse: 72908.926758, mean_q: 245.902456, mean_eps: 0.703817
 98866/100000: episode: 1024, duration: 0.741s, episode steps:  78, steps per second: 105, episode reward: -66.130, mean reward: -0.848 [-100.000,  8.589], mean action: 1.692 [0.000, 3.000],  loss: 60.813364, mse: 73904.448167, mean_q: 249.937989, mean_eps: 0.703520
 98970/100000: episode: 1025, duration: 1.022s, episode steps: 104, steps per second: 102, episode reward: -63.503, mean reward: -0.611 [-100.000,  8.824], mean action: 1.692 [0.000, 3.000],  loss: 55.862043, mse: 75161.612906, mean_q: 252.682041, mean_eps: 0.703248
 99086/100000: episode: 1026, duration: 1.039s, episode steps: 116, steps per second: 112, episode reward: -58.355, mean reward: -0.503 [-100.000, 14.301], mean action: 1.672 [0.000, 3.000],  loss: 85.367464, mse: 75253.791184, mean_q: 252.409263, mean_eps: 0.702917
 99182/100000: episode: 1027, duration: 0.951s, episode steps:  96, steps per second: 101, episode reward: -53.666, mean reward: -0.559 [-100.000, 28.392], mean action: 1.396 [0.000, 3.000],  loss: 45.483014, mse: 73142.281453, mean_q: 246.411279, mean_eps: 0.702599
 99319/100000: episode: 1028, duration: 1.096s, episode steps: 137, steps per second: 125, episode reward: -124.619, mean reward: -0.910 [-100.000,  8.396], mean action: 1.679 [0.000, 3.000],  loss: 66.815086, mse: 74632.358234, mean_q: 249.661768, mean_eps: 0.702250
 99429/100000: episode: 1029, duration: 0.838s, episode steps: 110, steps per second: 131, episode reward: -68.083, mean reward: -0.619 [-100.000, 23.078], mean action: 1.700 [0.000, 3.000],  loss: 60.217969, mse: 75639.959837, mean_q: 251.692595, mean_eps: 0.701879
 99568/100000: episode: 1030, duration: 1.010s, episode steps: 139, steps per second: 138, episode reward: -67.494, mean reward: -0.486 [-100.000, 13.789], mean action: 1.705 [0.000, 3.000],  loss: 60.139359, mse: 76653.794346, mean_q: 255.650192, mean_eps: 0.701506
 99681/100000: episode: 1031, duration: 0.833s, episode steps: 113, steps per second: 136, episode reward: -68.478, mean reward: -0.606 [-100.000, 12.681], mean action: 1.752 [0.000, 3.000],  loss: 44.766074, mse: 75730.846965, mean_q: 253.223820, mean_eps: 0.701128
 99861/100000: episode: 1032, duration: 1.324s, episode steps: 180, steps per second: 136, episode reward: -150.200, mean reward: -0.834 [-100.000,  6.621], mean action: 1.600 [0.000, 3.000],  loss: 122.725855, mse: 76223.189323, mean_q: 254.498034, mean_eps: 0.700689
 99959/100000: episode: 1033, duration: 0.691s, episode steps:  98, steps per second: 142, episode reward: -265.572, mean reward: -2.710 [-100.000,  1.237], mean action: 1.714 [0.000, 3.000],  loss: 61.160396, mse: 77585.642857, mean_q: 260.179996, mean_eps: 0.700272
done, took 781.690 seconds
Testing for 5 episodes ...
Episode 1: reward: 27.473, steps: 208
Episode 2: reward: -701.962, steps: 268
Episode 3: reward: -610.703, steps: 187
Episode 4: reward: -626.087, steps: 162
Episode 5: reward: 14.943, steps: 208
Testing for 5 episodes ...
Episode 1: reward: -613.325, steps: 182
Episode 2: reward: -614.808, steps: 75
Episode 3: reward: -591.081, steps: 149
Episode 4: reward: -329.387, steps: 411
Episode 5: reward: -672.838, steps: 227
Testing for 5 episodes ...
Episode 1: reward: -554.851, steps: 199
Episode 2: reward: -34.638, steps: 245
Episode 3: reward: -21.513, steps: 272
Episode 4: reward: -182.826, steps: 273
Episode 5: reward: -617.565, steps: 154