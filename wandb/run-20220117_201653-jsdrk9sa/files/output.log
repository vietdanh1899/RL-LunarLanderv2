Training for 150000 steps ...
     92/150000: episode: 1, duration: 0.205s, episode steps:  92, steps per second: 448, episode reward: -88.329, mean reward: -0.960 [-100.000, 12.252], mean action: 1.391 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
    180/150000: episode: 2, duration: 1.265s, episode steps:  88, steps per second:  70, episode reward: -137.164, mean reward: -1.559 [-100.000,  6.925], mean action: 1.625 [0.000, 3.000],  loss: 42.244740, mse: 22.315375, mean_q: 1.387211, mean_eps: 0.999160
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    278/150000: episode: 3, duration: 0.644s, episode steps:  98, steps per second: 152, episode reward: -294.678, mean reward: -3.007 [-100.000,  3.624], mean action: 1.337 [0.000, 3.000],  loss: 29.012396, mse: 19.758673, mean_q: 1.710504, mean_eps: 0.998629
    376/150000: episode: 4, duration: 0.757s, episode steps:  98, steps per second: 129, episode reward: -235.555, mean reward: -2.404 [-100.000,  0.730], mean action: 1.408 [0.000, 3.000],  loss: 23.864952, mse: 27.907755, mean_q: 0.877202, mean_eps: 0.998041
    490/150000: episode: 5, duration: 0.818s, episode steps: 114, steps per second: 139, episode reward: -412.674, mean reward: -3.620 [-100.000, 16.486], mean action: 1.526 [0.000, 3.000],  loss: 19.400707, mse: 32.604197, mean_q: 0.495232, mean_eps: 0.997405
    632/150000: episode: 6, duration: 1.150s, episode steps: 142, steps per second: 123, episode reward: -77.921, mean reward: -0.549 [-100.000, 10.004], mean action: 1.592 [0.000, 3.000],  loss: 20.139475, mse: 40.923410, mean_q: 0.098509, mean_eps: 0.996637
    695/150000: episode: 7, duration: 0.495s, episode steps:  63, steps per second: 127, episode reward: -132.629, mean reward: -2.105 [-100.000, 10.017], mean action: 1.444 [0.000, 3.000],  loss: 21.907999, mse: 42.455542, mean_q: 0.387274, mean_eps: 0.996022
    808/150000: episode: 8, duration: 1.053s, episode steps: 113, steps per second: 107, episode reward: -183.547, mean reward: -1.624 [-100.000, 84.502], mean action: 1.487 [0.000, 3.000],  loss: 19.245052, mse: 41.445846, mean_q: 0.762297, mean_eps: 0.995494
    877/150000: episode: 9, duration: 0.573s, episode steps:  69, steps per second: 120, episode reward: -93.432, mean reward: -1.354 [-100.000, 10.656], mean action: 1.652 [0.000, 3.000],  loss: 23.699763, mse: 39.025724, mean_q: 0.825481, mean_eps: 0.994948
    978/150000: episode: 10, duration: 0.874s, episode steps: 101, steps per second: 116, episode reward: -333.446, mean reward: -3.301 [-100.000,  2.323], mean action: 1.663 [0.000, 3.000],  loss: 37.843918, mse: 59.266134, mean_q: 0.399779, mean_eps: 0.994438
   1081/150000: episode: 11, duration: 1.226s, episode steps: 103, steps per second:  84, episode reward: -414.962, mean reward: -4.029 [-100.000,  0.761], mean action: 1.738 [0.000, 3.000],  loss: 28.410601, mse: 52.137472, mean_q: 0.234277, mean_eps: 0.993826
   1159/150000: episode: 12, duration: 0.877s, episode steps:  78, steps per second:  89, episode reward: -141.091, mean reward: -1.809 [-100.000, 12.293], mean action: 1.449 [0.000, 3.000],  loss: 22.967645, mse: 73.598567, mean_q: -0.212592, mean_eps: 0.993283
   1254/150000: episode: 13, duration: 0.945s, episode steps:  95, steps per second: 101, episode reward: -219.174, mean reward: -2.307 [-100.000, 13.081], mean action: 1.537 [0.000, 3.000],  loss: 18.217805, mse: 51.976716, mean_q: 0.186488, mean_eps: 0.992764
   1331/150000: episode: 14, duration: 0.690s, episode steps:  77, steps per second: 112, episode reward: -91.208, mean reward: -1.185 [-100.000, 20.505], mean action: 1.662 [0.000, 3.000],  loss: 26.280743, mse: 73.489840, mean_q: -0.103199, mean_eps: 0.992248
   1448/150000: episode: 15, duration: 0.854s, episode steps: 117, steps per second: 137, episode reward: -176.510, mean reward: -1.509 [-100.000,  9.711], mean action: 1.598 [0.000, 3.000],  loss: 19.080208, mse: 60.572356, mean_q: 0.431154, mean_eps: 0.991666
   1543/150000: episode: 16, duration: 0.703s, episode steps:  95, steps per second: 135, episode reward: -229.386, mean reward: -2.415 [-100.000,  7.636], mean action: 1.337 [0.000, 3.000],  loss: 27.010389, mse: 78.409288, mean_q: 0.112760, mean_eps: 0.991030
   1627/150000: episode: 17, duration: 0.650s, episode steps:  84, steps per second: 129, episode reward: -91.269, mean reward: -1.087 [-100.000,  7.045], mean action: 1.536 [0.000, 3.000],  loss: 26.965284, mse: 67.970290, mean_q: 0.184571, mean_eps: 0.990493
   1706/150000: episode: 18, duration: 0.605s, episode steps:  79, steps per second: 131, episode reward: -164.146, mean reward: -2.078 [-100.000,  7.436], mean action: 1.747 [0.000, 3.000],  loss: 26.463147, mse: 54.937696, mean_q: 0.399479, mean_eps: 0.990004
   1785/150000: episode: 19, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward: -199.394, mean reward: -2.524 [-100.000, 38.473], mean action: 1.557 [0.000, 3.000],  loss: 24.676186, mse: 59.917865, mean_q: 0.254328, mean_eps: 0.989530
   1897/150000: episode: 20, duration: 0.815s, episode steps: 112, steps per second: 137, episode reward: -259.600, mean reward: -2.318 [-100.000,  4.589], mean action: 1.411 [0.000, 3.000],  loss: 24.381922, mse: 60.860956, mean_q: 0.137613, mean_eps: 0.988957
   2015/150000: episode: 21, duration: 0.800s, episode steps: 118, steps per second: 147, episode reward: -152.682, mean reward: -1.294 [-100.000,  5.444], mean action: 1.661 [0.000, 3.000],  loss: 24.976114, mse: 66.758186, mean_q: 0.241958, mean_eps: 0.988267
   2117/150000: episode: 22, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -377.404, mean reward: -3.700 [-100.000,  0.130], mean action: 1.657 [0.000, 3.000],  loss: 17.660416, mse: 83.933755, mean_q: 0.817148, mean_eps: 0.987607
   2192/150000: episode: 23, duration: 0.527s, episode steps:  75, steps per second: 142, episode reward: -108.355, mean reward: -1.445 [-100.000,  6.404], mean action: 1.373 [0.000, 3.000],  loss: 23.661257, mse: 73.037799, mean_q: 1.117678, mean_eps: 0.987076
   2290/150000: episode: 24, duration: 0.712s, episode steps:  98, steps per second: 138, episode reward: -36.327, mean reward: -0.371 [-100.000, 84.449], mean action: 1.663 [0.000, 3.000],  loss: 17.360681, mse: 87.672787, mean_q: 1.039181, mean_eps: 0.986557
   2415/150000: episode: 25, duration: 0.856s, episode steps: 125, steps per second: 146, episode reward: -365.553, mean reward: -2.924 [-100.000, 78.472], mean action: 1.552 [0.000, 3.000],  loss: 26.553277, mse: 93.163398, mean_q: 0.652172, mean_eps: 0.985888
   2494/150000: episode: 26, duration: 0.546s, episode steps:  79, steps per second: 145, episode reward: -52.029, mean reward: -0.659 [-100.000, 17.742], mean action: 1.443 [0.000, 3.000],  loss: 28.320474, mse: 81.459800, mean_q: 1.083292, mean_eps: 0.985276
   2554/150000: episode: 27, duration: 0.414s, episode steps:  60, steps per second: 145, episode reward: -147.706, mean reward: -2.462 [-100.000,  4.795], mean action: 1.467 [0.000, 3.000],  loss: 25.417954, mse: 85.967944, mean_q: 1.365168, mean_eps: 0.984859
   2630/150000: episode: 28, duration: 0.501s, episode steps:  76, steps per second: 152, episode reward: -94.691, mean reward: -1.246 [-100.000,  8.176], mean action: 1.645 [0.000, 3.000],  loss: 24.771440, mse: 75.686592, mean_q: 1.329835, mean_eps: 0.984451
   2766/150000: episode: 29, duration: 0.912s, episode steps: 136, steps per second: 149, episode reward: -27.384, mean reward: -0.201 [-100.000, 113.967], mean action: 1.522 [0.000, 3.000],  loss: 25.138459, mse: 84.159421, mean_q: 1.207119, mean_eps: 0.983815
   2854/150000: episode: 30, duration: 0.646s, episode steps:  88, steps per second: 136, episode reward: -148.928, mean reward: -1.692 [-100.000,  8.125], mean action: 1.477 [0.000, 3.000],  loss: 33.874192, mse: 81.872843, mean_q: 1.316785, mean_eps: 0.983143
   2919/150000: episode: 31, duration: 0.440s, episode steps:  65, steps per second: 148, episode reward: -84.536, mean reward: -1.301 [-100.000,  7.862], mean action: 1.385 [0.000, 3.000],  loss: 37.044269, mse: 100.108945, mean_q: 0.967590, mean_eps: 0.982684
   3042/150000: episode: 32, duration: 0.798s, episode steps: 123, steps per second: 154, episode reward: -162.738, mean reward: -1.323 [-100.000,  3.560], mean action: 1.561 [0.000, 3.000],  loss: 31.266244, mse: 91.992761, mean_q: 1.298685, mean_eps: 0.982120
   3141/150000: episode: 33, duration: 0.684s, episode steps:  99, steps per second: 145, episode reward: -118.211, mean reward: -1.194 [-100.000, 26.962], mean action: 1.424 [0.000, 3.000],  loss: 29.954630, mse: 109.244716, mean_q: 1.833010, mean_eps: 0.981454
   3213/150000: episode: 34, duration: 0.492s, episode steps:  72, steps per second: 146, episode reward: -122.457, mean reward: -1.701 [-100.000,  6.104], mean action: 1.319 [0.000, 3.000],  loss: 25.219067, mse: 111.567928, mean_q: 1.952451, mean_eps: 0.980941
   3321/150000: episode: 35, duration: 0.701s, episode steps: 108, steps per second: 154, episode reward: -461.535, mean reward: -4.273 [-100.000,  1.220], mean action: 1.389 [0.000, 3.000],  loss: 32.040817, mse: 102.589147, mean_q: 2.204351, mean_eps: 0.980401
   3403/150000: episode: 36, duration: 0.526s, episode steps:  82, steps per second: 156, episode reward: -89.260, mean reward: -1.089 [-100.000, 16.310], mean action: 1.415 [0.000, 3.000],  loss: 33.248072, mse: 89.307953, mean_q: 2.153660, mean_eps: 0.979831
   3475/150000: episode: 37, duration: 0.552s, episode steps:  72, steps per second: 130, episode reward: -170.705, mean reward: -2.371 [-100.000,  7.524], mean action: 1.375 [0.000, 3.000],  loss: 34.126539, mse: 91.949646, mean_q: 2.204453, mean_eps: 0.979369
   3600/150000: episode: 38, duration: 0.848s, episode steps: 125, steps per second: 147, episode reward: -192.388, mean reward: -1.539 [-100.000,  6.671], mean action: 1.528 [0.000, 3.000],  loss: 33.609293, mse: 91.248125, mean_q: 2.355376, mean_eps: 0.978778
   3670/150000: episode: 39, duration: 0.456s, episode steps:  70, steps per second: 153, episode reward: -93.577, mean reward: -1.337 [-100.000, 10.537], mean action: 1.314 [0.000, 3.000],  loss: 28.018215, mse: 82.759745, mean_q: 2.426805, mean_eps: 0.978193
   3748/150000: episode: 40, duration: 0.534s, episode steps:  78, steps per second: 146, episode reward: -161.610, mean reward: -2.072 [-100.000,  7.314], mean action: 1.385 [0.000, 3.000],  loss: 30.496408, mse: 91.035368, mean_q: 2.592673, mean_eps: 0.977749
   3836/150000: episode: 41, duration: 0.620s, episode steps:  88, steps per second: 142, episode reward: -145.679, mean reward: -1.655 [-100.000,  5.187], mean action: 1.261 [0.000, 3.000],  loss: 26.785980, mse: 106.840027, mean_q: 1.986601, mean_eps: 0.977251
   3933/150000: episode: 42, duration: 0.666s, episode steps:  97, steps per second: 146, episode reward: -387.209, mean reward: -3.992 [-100.000,  0.811], mean action: 1.557 [0.000, 3.000],  loss: 34.137077, mse: 95.481631, mean_q: 2.211345, mean_eps: 0.976696
   4045/150000: episode: 43, duration: 0.762s, episode steps: 112, steps per second: 147, episode reward: -122.100, mean reward: -1.090 [-100.000,  6.523], mean action: 1.455 [0.000, 3.000],  loss: 27.333925, mse: 94.997855, mean_q: 2.281067, mean_eps: 0.976069
   4155/150000: episode: 44, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: -203.491, mean reward: -1.850 [-100.000, 29.683], mean action: 1.582 [0.000, 3.000],  loss: 28.036299, mse: 97.421764, mean_q: 2.702860, mean_eps: 0.975403
   4242/150000: episode: 45, duration: 0.574s, episode steps:  87, steps per second: 152, episode reward: -35.881, mean reward: -0.412 [-100.000, 90.896], mean action: 1.621 [0.000, 3.000],  loss: 30.079622, mse: 107.336977, mean_q: 2.664716, mean_eps: 0.974812
   4323/150000: episode: 46, duration: 0.535s, episode steps:  81, steps per second: 152, episode reward: -100.553, mean reward: -1.241 [-100.000,  7.759], mean action: 1.667 [0.000, 3.000],  loss: 25.334972, mse: 101.565022, mean_q: 2.372864, mean_eps: 0.974308
   4411/150000: episode: 47, duration: 0.609s, episode steps:  88, steps per second: 144, episode reward: -71.819, mean reward: -0.816 [-100.000, 17.628], mean action: 1.682 [0.000, 3.000],  loss: 31.093959, mse: 110.465050, mean_q: 2.762885, mean_eps: 0.973801
   4491/150000: episode: 48, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -160.486, mean reward: -2.006 [-100.000,  9.341], mean action: 1.413 [0.000, 3.000],  loss: 20.836503, mse: 95.207649, mean_q: 2.587589, mean_eps: 0.973297
   4578/150000: episode: 49, duration: 0.569s, episode steps:  87, steps per second: 153, episode reward: -145.960, mean reward: -1.678 [-100.000,  8.222], mean action: 1.437 [0.000, 3.000],  loss: 21.648655, mse: 107.064399, mean_q: 2.485707, mean_eps: 0.972796
   4666/150000: episode: 50, duration: 0.563s, episode steps:  88, steps per second: 156, episode reward: -250.494, mean reward: -2.847 [-100.000,  3.048], mean action: 1.375 [0.000, 3.000],  loss: 24.299032, mse: 96.091171, mean_q: 2.623359, mean_eps: 0.972271
   4767/150000: episode: 51, duration: 0.725s, episode steps: 101, steps per second: 139, episode reward: -12.892, mean reward: -0.128 [-100.000, 96.429], mean action: 1.634 [0.000, 3.000],  loss: 28.531503, mse: 113.565572, mean_q: 2.516904, mean_eps: 0.971704
   4871/150000: episode: 52, duration: 0.712s, episode steps: 104, steps per second: 146, episode reward: -240.277, mean reward: -2.310 [-100.000,  6.836], mean action: 1.462 [0.000, 3.000],  loss: 28.868695, mse: 110.035260, mean_q: 2.588058, mean_eps: 0.971089
   4940/150000: episode: 53, duration: 0.458s, episode steps:  69, steps per second: 151, episode reward: -76.522, mean reward: -1.109 [-100.000, 26.256], mean action: 1.290 [0.000, 3.000],  loss: 30.400509, mse: 106.303305, mean_q: 2.550284, mean_eps: 0.970570
   5036/150000: episode: 54, duration: 0.643s, episode steps:  96, steps per second: 149, episode reward: -399.568, mean reward: -4.162 [-100.000, 42.623], mean action: 1.448 [0.000, 3.000],  loss: 19.934390, mse: 107.749290, mean_q: 2.526053, mean_eps: 0.970075
   5101/150000: episode: 55, duration: 0.459s, episode steps:  65, steps per second: 142, episode reward: -68.419, mean reward: -1.053 [-100.000, 22.723], mean action: 1.615 [0.000, 3.000],  loss: 24.898583, mse: 145.451646, mean_q: 2.086352, mean_eps: 0.969592
   5200/150000: episode: 56, duration: 0.658s, episode steps:  99, steps per second: 151, episode reward: -193.286, mean reward: -1.952 [-100.000,  6.958], mean action: 1.576 [0.000, 3.000],  loss: 26.574093, mse: 141.660502, mean_q: 2.154327, mean_eps: 0.969100
   5306/150000: episode: 57, duration: 0.735s, episode steps: 106, steps per second: 144, episode reward: -195.586, mean reward: -1.845 [-100.000, 27.239], mean action: 1.274 [0.000, 3.000],  loss: 22.452969, mse: 138.694624, mean_q: 2.234084, mean_eps: 0.968485
   5416/150000: episode: 58, duration: 0.782s, episode steps: 110, steps per second: 141, episode reward: -332.357, mean reward: -3.021 [-100.000,  3.040], mean action: 1.336 [0.000, 3.000],  loss: 18.525897, mse: 135.888809, mean_q: 2.501223, mean_eps: 0.967837
   5521/150000: episode: 59, duration: 0.700s, episode steps: 105, steps per second: 150, episode reward: -260.202, mean reward: -2.478 [-100.000,  0.890], mean action: 1.552 [0.000, 3.000],  loss: 26.795128, mse: 153.520089, mean_q: 2.109052, mean_eps: 0.967192
   5636/150000: episode: 60, duration: 0.759s, episode steps: 115, steps per second: 151, episode reward: -275.826, mean reward: -2.398 [-100.000, 65.110], mean action: 1.383 [0.000, 3.000],  loss: 26.576090, mse: 138.376377, mean_q: 2.219408, mean_eps: 0.966532
   5735/150000: episode: 61, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward: -115.217, mean reward: -1.164 [-100.000, 16.668], mean action: 1.404 [0.000, 3.000],  loss: 24.585216, mse: 142.135729, mean_q: 2.197699, mean_eps: 0.965890
   5850/150000: episode: 62, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: -182.292, mean reward: -1.585 [-100.000, 12.073], mean action: 1.496 [0.000, 3.000],  loss: 21.743025, mse: 152.723977, mean_q: 2.076945, mean_eps: 0.965248
   5912/150000: episode: 63, duration: 0.404s, episode steps:  62, steps per second: 154, episode reward: -148.825, mean reward: -2.400 [-100.000,  6.941], mean action: 1.565 [0.000, 3.000],  loss: 24.436657, mse: 124.360925, mean_q: 2.537691, mean_eps: 0.964717
   5974/150000: episode: 64, duration: 0.409s, episode steps:  62, steps per second: 152, episode reward: -120.670, mean reward: -1.946 [-100.000,  9.304], mean action: 1.532 [0.000, 3.000],  loss: 27.031461, mse: 136.099513, mean_q: 2.410650, mean_eps: 0.964345
   6040/150000: episode: 65, duration: 0.471s, episode steps:  66, steps per second: 140, episode reward: -54.729, mean reward: -0.829 [-100.000, 10.967], mean action: 1.576 [0.000, 3.000],  loss: 26.861628, mse: 145.053164, mean_q: 2.429787, mean_eps: 0.963961
   6168/150000: episode: 66, duration: 0.852s, episode steps: 128, steps per second: 150, episode reward: -206.394, mean reward: -1.612 [-100.000,  2.493], mean action: 1.562 [0.000, 3.000],  loss: 23.090773, mse: 187.707708, mean_q: 2.019791, mean_eps: 0.963379
   6245/150000: episode: 67, duration: 0.503s, episode steps:  77, steps per second: 153, episode reward: -37.317, mean reward: -0.485 [-100.000, 13.712], mean action: 1.688 [0.000, 3.000],  loss: 17.549039, mse: 163.527666, mean_q: 2.126089, mean_eps: 0.962764
   6388/150000: episode: 68, duration: 0.996s, episode steps: 143, steps per second: 144, episode reward: -236.231, mean reward: -1.652 [-100.000, 14.713], mean action: 1.490 [0.000, 3.000],  loss: 19.107940, mse: 151.189580, mean_q: 2.499550, mean_eps: 0.962104
   6489/150000: episode: 69, duration: 0.671s, episode steps: 101, steps per second: 151, episode reward: -197.215, mean reward: -1.953 [-100.000, 42.839], mean action: 1.386 [0.000, 3.000],  loss: 24.517054, mse: 159.020416, mean_q: 2.422164, mean_eps: 0.961372
   6577/150000: episode: 70, duration: 0.572s, episode steps:  88, steps per second: 154, episode reward: -221.911, mean reward: -2.522 [-100.000, 22.257], mean action: 1.114 [0.000, 3.000],  loss: 27.837485, mse: 156.567358, mean_q: 2.211525, mean_eps: 0.960805
   6650/150000: episode: 71, duration: 0.515s, episode steps:  73, steps per second: 142, episode reward: -120.173, mean reward: -1.646 [-100.000, 14.058], mean action: 1.438 [0.000, 3.000],  loss: 19.525857, mse: 160.188283, mean_q: 2.435135, mean_eps: 0.960322
   6739/150000: episode: 72, duration: 0.608s, episode steps:  89, steps per second: 146, episode reward: -188.965, mean reward: -2.123 [-100.000,  5.398], mean action: 1.371 [0.000, 3.000],  loss: 19.931257, mse: 149.816209, mean_q: 2.634923, mean_eps: 0.959836
   6839/150000: episode: 73, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -103.383, mean reward: -1.034 [-100.000, 12.034], mean action: 1.550 [0.000, 3.000],  loss: 19.235504, mse: 153.687558, mean_q: 2.289283, mean_eps: 0.959269
   6925/150000: episode: 74, duration: 0.573s, episode steps:  86, steps per second: 150, episode reward: -303.277, mean reward: -3.526 [-100.000,  3.000], mean action: 1.384 [0.000, 3.000],  loss: 20.477552, mse: 149.770313, mean_q: 2.346229, mean_eps: 0.958711
   7014/150000: episode: 75, duration: 0.632s, episode steps:  89, steps per second: 141, episode reward: -171.415, mean reward: -1.926 [-100.000, 17.761], mean action: 1.427 [0.000, 3.000],  loss: 24.088174, mse: 152.378829, mean_q: 2.287008, mean_eps: 0.958186
   7098/150000: episode: 76, duration: 0.610s, episode steps:  84, steps per second: 138, episode reward: -88.033, mean reward: -1.048 [-100.000,  9.122], mean action: 1.667 [0.000, 3.000],  loss: 18.162565, mse: 194.861898, mean_q: 2.656217, mean_eps: 0.957667
   7214/150000: episode: 77, duration: 0.885s, episode steps: 116, steps per second: 131, episode reward: -83.125, mean reward: -0.717 [-100.000,  9.577], mean action: 1.405 [0.000, 3.000],  loss: 19.694390, mse: 202.583509, mean_q: 2.817097, mean_eps: 0.957067
   7337/150000: episode: 78, duration: 0.952s, episode steps: 123, steps per second: 129, episode reward: -133.114, mean reward: -1.082 [-100.000,  7.130], mean action: 1.398 [0.000, 3.000],  loss: 22.749673, mse: 204.250399, mean_q: 2.826390, mean_eps: 0.956350
   7451/150000: episode: 79, duration: 0.789s, episode steps: 114, steps per second: 144, episode reward: -160.886, mean reward: -1.411 [-100.000,  6.058], mean action: 1.377 [0.000, 3.000],  loss: 19.675572, mse: 219.126482, mean_q: 2.865002, mean_eps: 0.955639
   7531/150000: episode: 80, duration: 0.587s, episode steps:  80, steps per second: 136, episode reward: -206.984, mean reward: -2.587 [-100.000,  6.042], mean action: 1.650 [0.000, 3.000],  loss: 20.804125, mse: 196.516188, mean_q: 3.310885, mean_eps: 0.955057
   7640/150000: episode: 81, duration: 0.792s, episode steps: 109, steps per second: 138, episode reward: -415.163, mean reward: -3.809 [-100.000,  1.126], mean action: 1.339 [0.000, 3.000],  loss: 24.365950, mse: 215.524305, mean_q: 2.773900, mean_eps: 0.954490
   7721/150000: episode: 82, duration: 0.575s, episode steps:  81, steps per second: 141, episode reward: -104.429, mean reward: -1.289 [-100.000, 23.670], mean action: 1.420 [0.000, 3.000],  loss: 13.267439, mse: 192.718591, mean_q: 2.804929, mean_eps: 0.953920
   7793/150000: episode: 83, duration: 0.497s, episode steps:  72, steps per second: 145, episode reward: -72.643, mean reward: -1.009 [-100.000, 10.943], mean action: 1.444 [0.000, 3.000],  loss: 19.174762, mse: 180.316306, mean_q: 3.252755, mean_eps: 0.953461
   7888/150000: episode: 84, duration: 0.675s, episode steps:  95, steps per second: 141, episode reward: -69.153, mean reward: -0.728 [-100.000, 13.174], mean action: 1.442 [0.000, 3.000],  loss: 19.375052, mse: 208.206584, mean_q: 2.841674, mean_eps: 0.952960
   7980/150000: episode: 85, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -209.768, mean reward: -2.280 [-100.000,  1.015], mean action: 1.554 [0.000, 3.000],  loss: 21.195277, mse: 214.591181, mean_q: 2.628002, mean_eps: 0.952399
   8090/150000: episode: 86, duration: 0.736s, episode steps: 110, steps per second: 149, episode reward: -158.974, mean reward: -1.445 [-100.000, 56.417], mean action: 1.600 [0.000, 3.000],  loss: 22.773861, mse: 247.959071, mean_q: 3.342525, mean_eps: 0.951793
   8183/150000: episode: 87, duration: 0.664s, episode steps:  93, steps per second: 140, episode reward: -125.725, mean reward: -1.352 [-100.000, 19.082], mean action: 1.495 [0.000, 3.000],  loss: 19.144958, mse: 223.662431, mean_q: 3.993222, mean_eps: 0.951184
   8300/150000: episode: 88, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -123.496, mean reward: -1.056 [-100.000, 15.790], mean action: 1.496 [0.000, 3.000],  loss: 22.798269, mse: 236.738849, mean_q: 3.808509, mean_eps: 0.950554
   8376/150000: episode: 89, duration: 0.492s, episode steps:  76, steps per second: 154, episode reward: -222.584, mean reward: -2.929 [-100.000,  4.498], mean action: 1.382 [0.000, 3.000],  loss: 21.706819, mse: 211.150450, mean_q: 4.019166, mean_eps: 0.949975
   8462/150000: episode: 90, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -286.519, mean reward: -3.332 [-100.000, 75.320], mean action: 1.558 [0.000, 3.000],  loss: 19.224261, mse: 230.055874, mean_q: 3.846646, mean_eps: 0.949489
   8536/150000: episode: 91, duration: 0.540s, episode steps:  74, steps per second: 137, episode reward: -233.042, mean reward: -3.149 [-100.000,  4.437], mean action: 1.676 [0.000, 3.000],  loss: 23.355952, mse: 248.451150, mean_q: 3.716778, mean_eps: 0.949009
   8597/150000: episode: 92, duration: 0.422s, episode steps:  61, steps per second: 145, episode reward: -8.512, mean reward: -0.140 [-100.000, 95.754], mean action: 1.377 [0.000, 3.000],  loss: 16.560294, mse: 234.086948, mean_q: 3.822993, mean_eps: 0.948604
   8690/150000: episode: 93, duration: 0.607s, episode steps:  93, steps per second: 153, episode reward: -108.535, mean reward: -1.167 [-100.000, 15.018], mean action: 1.634 [0.000, 3.000],  loss: 24.170956, mse: 265.697091, mean_q: 3.649431, mean_eps: 0.948142
   8764/150000: episode: 94, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -84.431, mean reward: -1.141 [-100.000,  7.858], mean action: 1.378 [0.000, 3.000],  loss: 24.713564, mse: 256.438781, mean_q: 3.743164, mean_eps: 0.947641
   8845/150000: episode: 95, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -160.270, mean reward: -1.979 [-100.000,  8.184], mean action: 1.444 [0.000, 3.000],  loss: 17.676709, mse: 200.198546, mean_q: 4.104365, mean_eps: 0.947176
   8961/150000: episode: 96, duration: 0.782s, episode steps: 116, steps per second: 148, episode reward: -196.079, mean reward: -1.690 [-100.000, 29.034], mean action: 1.319 [0.000, 3.000],  loss: 17.577961, mse: 244.511469, mean_q: 3.842659, mean_eps: 0.946585
   9070/150000: episode: 97, duration: 0.727s, episode steps: 109, steps per second: 150, episode reward: -158.353, mean reward: -1.453 [-100.000,  2.080], mean action: 1.578 [0.000, 3.000],  loss: 20.832241, mse: 282.200390, mean_q: 3.515083, mean_eps: 0.945910
   9154/150000: episode: 98, duration: 0.613s, episode steps:  84, steps per second: 137, episode reward: -95.148, mean reward: -1.133 [-100.000,  7.216], mean action: 1.560 [0.000, 3.000],  loss: 16.083165, mse: 274.330855, mean_q: 4.174236, mean_eps: 0.945331
   9266/150000: episode: 99, duration: 0.823s, episode steps: 112, steps per second: 136, episode reward: -81.558, mean reward: -0.728 [-100.000, 84.162], mean action: 1.536 [0.000, 3.000],  loss: 20.376330, mse: 302.787738, mean_q: 3.755248, mean_eps: 0.944743
   9365/150000: episode: 100, duration: 0.760s, episode steps:  99, steps per second: 130, episode reward: -231.501, mean reward: -2.338 [-100.000, 34.740], mean action: 1.485 [0.000, 3.000],  loss: 21.378181, mse: 260.949124, mean_q: 4.130593, mean_eps: 0.944110
   9436/150000: episode: 101, duration: 0.578s, episode steps:  71, steps per second: 123, episode reward: -116.721, mean reward: -1.644 [-100.000, 44.446], mean action: 1.592 [0.000, 3.000],  loss: 18.154486, mse: 284.770661, mean_q: 3.914036, mean_eps: 0.943600
   9520/150000: episode: 102, duration: 0.656s, episode steps:  84, steps per second: 128, episode reward: -377.647, mean reward: -4.496 [-100.000,  0.285], mean action: 1.571 [0.000, 3.000],  loss: 14.259278, mse: 266.075197, mean_q: 3.740820, mean_eps: 0.943135
   9610/150000: episode: 103, duration: 0.659s, episode steps:  90, steps per second: 137, episode reward: -157.826, mean reward: -1.754 [-100.000,  6.014], mean action: 1.544 [0.000, 3.000],  loss: 17.819671, mse: 317.132493, mean_q: 3.338509, mean_eps: 0.942613
   9694/150000: episode: 104, duration: 0.650s, episode steps:  84, steps per second: 129, episode reward: -90.820, mean reward: -1.081 [-100.000,  6.649], mean action: 1.524 [0.000, 3.000],  loss: 29.054621, mse: 282.652735, mean_q: 3.830502, mean_eps: 0.942091
   9762/150000: episode: 105, duration: 0.515s, episode steps:  68, steps per second: 132, episode reward: -81.467, mean reward: -1.198 [-100.000, 21.560], mean action: 1.603 [0.000, 3.000],  loss: 20.738769, mse: 272.375845, mean_q: 3.608686, mean_eps: 0.941635
   9860/150000: episode: 106, duration: 0.696s, episode steps:  98, steps per second: 141, episode reward: -56.405, mean reward: -0.576 [-100.000, 21.176], mean action: 1.673 [0.000, 3.000],  loss: 26.039250, mse: 296.653281, mean_q: 3.784831, mean_eps: 0.941137
   9957/150000: episode: 107, duration: 0.715s, episode steps:  97, steps per second: 136, episode reward: -157.724, mean reward: -1.626 [-100.000,  6.862], mean action: 1.619 [0.000, 3.000],  loss: 19.616783, mse: 280.578401, mean_q: 3.555769, mean_eps: 0.940552
  10067/150000: episode: 108, duration: 0.780s, episode steps: 110, steps per second: 141, episode reward: -145.122, mean reward: -1.319 [-100.000, 36.585], mean action: 1.473 [0.000, 3.000],  loss: 20.074987, mse: 293.359422, mean_q: 4.208236, mean_eps: 0.939931
  10140/150000: episode: 109, duration: 0.492s, episode steps:  73, steps per second: 148, episode reward: -97.221, mean reward: -1.332 [-100.000,  8.205], mean action: 1.425 [0.000, 3.000],  loss: 22.200933, mse: 309.777288, mean_q: 4.339916, mean_eps: 0.939382
  10283/150000: episode: 110, duration: 0.945s, episode steps: 143, steps per second: 151, episode reward: -304.935, mean reward: -2.132 [-100.000, 99.352], mean action: 1.615 [0.000, 3.000],  loss: 16.902945, mse: 301.925862, mean_q: 4.540094, mean_eps: 0.938734
  10395/150000: episode: 111, duration: 0.816s, episode steps: 112, steps per second: 137, episode reward: -195.143, mean reward: -1.742 [-100.000, 32.475], mean action: 1.571 [0.000, 3.000],  loss: 17.718711, mse: 304.445038, mean_q: 4.512773, mean_eps: 0.937969
  10515/150000: episode: 112, duration: 0.825s, episode steps: 120, steps per second: 145, episode reward: -258.202, mean reward: -2.152 [-100.000,  0.748], mean action: 1.433 [0.000, 3.000],  loss: 17.377817, mse: 342.103345, mean_q: 4.096195, mean_eps: 0.937273
  10615/150000: episode: 113, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: -130.217, mean reward: -1.302 [-100.000, 10.890], mean action: 1.420 [0.000, 3.000],  loss: 19.893172, mse: 326.071320, mean_q: 4.189205, mean_eps: 0.936613
  10679/150000: episode: 114, duration: 0.441s, episode steps:  64, steps per second: 145, episode reward: -124.065, mean reward: -1.939 [-100.000, 15.556], mean action: 1.609 [0.000, 3.000],  loss: 23.526168, mse: 331.403705, mean_q: 3.851703, mean_eps: 0.936121
  10793/150000: episode: 115, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -110.813, mean reward: -0.972 [-100.000, 12.996], mean action: 1.518 [0.000, 3.000],  loss: 18.330999, mse: 314.650341, mean_q: 3.972996, mean_eps: 0.935587
  10909/150000: episode: 116, duration: 0.777s, episode steps: 116, steps per second: 149, episode reward: -101.258, mean reward: -0.873 [-100.000, 10.374], mean action: 1.466 [0.000, 3.000],  loss: 18.931633, mse: 317.436921, mean_q: 4.542122, mean_eps: 0.934897
  11001/150000: episode: 117, duration: 0.662s, episode steps:  92, steps per second: 139, episode reward: -87.212, mean reward: -0.948 [-100.000, 15.773], mean action: 1.315 [0.000, 3.000],  loss: 17.277934, mse: 311.801883, mean_q: 4.326285, mean_eps: 0.934273
  11100/150000: episode: 118, duration: 0.675s, episode steps:  99, steps per second: 147, episode reward: -167.544, mean reward: -1.692 [-100.000,  8.049], mean action: 1.535 [0.000, 3.000],  loss: 16.233113, mse: 370.849489, mean_q: 5.526432, mean_eps: 0.933700
  11168/150000: episode: 119, duration: 0.471s, episode steps:  68, steps per second: 144, episode reward: -108.749, mean reward: -1.599 [-100.000,  6.159], mean action: 1.382 [0.000, 3.000],  loss: 12.659883, mse: 391.725721, mean_q: 5.482957, mean_eps: 0.933199
  11255/150000: episode: 120, duration: 0.615s, episode steps:  87, steps per second: 141, episode reward: -147.331, mean reward: -1.693 [-100.000,  7.517], mean action: 1.552 [0.000, 3.000],  loss: 19.151061, mse: 396.941154, mean_q: 5.593130, mean_eps: 0.932734
  11346/150000: episode: 121, duration: 0.611s, episode steps:  91, steps per second: 149, episode reward: -252.635, mean reward: -2.776 [-100.000,  4.369], mean action: 1.780 [0.000, 3.000],  loss: 14.276476, mse: 402.997700, mean_q: 5.222844, mean_eps: 0.932200
  11437/150000: episode: 122, duration: 0.622s, episode steps:  91, steps per second: 146, episode reward: -104.709, mean reward: -1.151 [-100.000, 12.957], mean action: 1.560 [0.000, 3.000],  loss: 15.450514, mse: 389.588306, mean_q: 5.926088, mean_eps: 0.931654
  11516/150000: episode: 123, duration: 0.585s, episode steps:  79, steps per second: 135, episode reward: -139.363, mean reward: -1.764 [-100.000,  5.675], mean action: 1.468 [0.000, 3.000],  loss: 16.472576, mse: 385.451815, mean_q: 5.142154, mean_eps: 0.931144
  11645/150000: episode: 124, duration: 0.904s, episode steps: 129, steps per second: 143, episode reward: -165.792, mean reward: -1.285 [-100.000,  7.695], mean action: 1.512 [0.000, 3.000],  loss: 14.147242, mse: 374.489635, mean_q: 5.498738, mean_eps: 0.930520
  11743/150000: episode: 125, duration: 0.639s, episode steps:  98, steps per second: 153, episode reward: -212.536, mean reward: -2.169 [-100.000, 32.517], mean action: 1.439 [0.000, 3.000],  loss: 17.659412, mse: 413.646899, mean_q: 5.121693, mean_eps: 0.929839
  11863/150000: episode: 126, duration: 0.826s, episode steps: 120, steps per second: 145, episode reward: -153.695, mean reward: -1.281 [-100.000,  9.641], mean action: 1.508 [0.000, 3.000],  loss: 10.436478, mse: 363.607001, mean_q: 5.641487, mean_eps: 0.929185
  11976/150000: episode: 127, duration: 0.779s, episode steps: 113, steps per second: 145, episode reward: -144.223, mean reward: -1.276 [-100.000, 16.765], mean action: 1.522 [0.000, 3.000],  loss: 13.456356, mse: 364.062604, mean_q: 5.642149, mean_eps: 0.928486
  12088/150000: episode: 128, duration: 0.743s, episode steps: 112, steps per second: 151, episode reward: -81.622, mean reward: -0.729 [-100.000, 16.897], mean action: 1.661 [0.000, 3.000],  loss: 13.080097, mse: 380.340492, mean_q: 5.868473, mean_eps: 0.927811
  12161/150000: episode: 129, duration: 0.506s, episode steps:  73, steps per second: 144, episode reward: -167.695, mean reward: -2.297 [-100.000, 11.314], mean action: 1.342 [0.000, 3.000],  loss: 21.910797, mse: 399.642315, mean_q: 5.517090, mean_eps: 0.927256
  12247/150000: episode: 130, duration: 0.621s, episode steps:  86, steps per second: 138, episode reward: -48.640, mean reward: -0.566 [-100.000, 19.336], mean action: 1.593 [0.000, 3.000],  loss: 15.324248, mse: 405.184306, mean_q: 6.065233, mean_eps: 0.926779
  12354/150000: episode: 131, duration: 0.706s, episode steps: 107, steps per second: 152, episode reward: -63.323, mean reward: -0.592 [-100.000,  9.697], mean action: 1.664 [0.000, 3.000],  loss: 13.512197, mse: 433.615410, mean_q: 5.737738, mean_eps: 0.926200
  12448/150000: episode: 132, duration: 0.617s, episode steps:  94, steps per second: 152, episode reward: -113.580, mean reward: -1.208 [-100.000,  7.702], mean action: 1.383 [0.000, 3.000],  loss: 17.041708, mse: 407.643086, mean_q: 5.734941, mean_eps: 0.925597
  12541/150000: episode: 133, duration: 0.673s, episode steps:  93, steps per second: 138, episode reward: -83.604, mean reward: -0.899 [-100.000,  9.167], mean action: 1.484 [0.000, 3.000],  loss: 16.146812, mse: 423.231890, mean_q: 5.801531, mean_eps: 0.925036
  12618/150000: episode: 134, duration: 0.512s, episode steps:  77, steps per second: 150, episode reward: -192.550, mean reward: -2.501 [-100.000,  6.448], mean action: 1.416 [0.000, 3.000],  loss: 12.884953, mse: 405.327298, mean_q: 6.072861, mean_eps: 0.924526
  12731/150000: episode: 135, duration: 0.763s, episode steps: 113, steps per second: 148, episode reward: -94.172, mean reward: -0.833 [-100.000, 13.597], mean action: 1.407 [0.000, 3.000],  loss: 12.426404, mse: 402.232025, mean_q: 5.793675, mean_eps: 0.923956
  12828/150000: episode: 136, duration: 0.687s, episode steps:  97, steps per second: 141, episode reward: -409.906, mean reward: -4.226 [-100.000, 87.027], mean action: 1.330 [0.000, 3.000],  loss: 15.904320, mse: 446.723008, mean_q: 5.210096, mean_eps: 0.923326
  12922/150000: episode: 137, duration: 1.014s, episode steps:  94, steps per second:  93, episode reward: -168.005, mean reward: -1.787 [-100.000,  8.809], mean action: 1.638 [0.000, 3.000],  loss: 17.583183, mse: 412.638692, mean_q: 5.717750, mean_eps: 0.922753
  12992/150000: episode: 138, duration: 0.469s, episode steps:  70, steps per second: 149, episode reward: -50.706, mean reward: -0.724 [-100.000, 12.363], mean action: 1.443 [0.000, 3.000],  loss: 12.998151, mse: 423.892140, mean_q: 6.063910, mean_eps: 0.922261
  13106/150000: episode: 139, duration: 0.794s, episode steps: 114, steps per second: 144, episode reward: -253.849, mean reward: -2.227 [-100.000, 48.502], mean action: 1.684 [0.000, 3.000],  loss: 14.322624, mse: 470.088227, mean_q: 6.355291, mean_eps: 0.921709
  13213/150000: episode: 140, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: -76.966, mean reward: -0.719 [-100.000, 11.326], mean action: 1.626 [0.000, 3.000],  loss: 13.830232, mse: 502.853723, mean_q: 5.717628, mean_eps: 0.921046
  13323/150000: episode: 141, duration: 0.716s, episode steps: 110, steps per second: 154, episode reward: -150.278, mean reward: -1.366 [-100.000, 21.580], mean action: 1.555 [0.000, 3.000],  loss: 16.536614, mse: 486.902237, mean_q: 6.020611, mean_eps: 0.920395
  13437/150000: episode: 142, duration: 0.823s, episode steps: 114, steps per second: 139, episode reward: -137.897, mean reward: -1.210 [-100.000,  7.561], mean action: 1.412 [0.000, 3.000],  loss: 13.260277, mse: 536.426715, mean_q: 5.170368, mean_eps: 0.919723
  13536/150000: episode: 143, duration: 0.675s, episode steps:  99, steps per second: 147, episode reward: -418.565, mean reward: -4.228 [-100.000,  0.043], mean action: 1.333 [0.000, 3.000],  loss: 10.580603, mse: 499.909450, mean_q: 5.749354, mean_eps: 0.919084
  13644/150000: episode: 144, duration: 0.724s, episode steps: 108, steps per second: 149, episode reward: -347.097, mean reward: -3.214 [-100.000, 90.021], mean action: 1.306 [0.000, 3.000],  loss: 14.952692, mse: 517.633285, mean_q: 5.440389, mean_eps: 0.918463
  13736/150000: episode: 145, duration: 0.647s, episode steps:  92, steps per second: 142, episode reward: -198.499, mean reward: -2.158 [-100.000,  7.316], mean action: 1.652 [0.000, 3.000],  loss: 14.782469, mse: 535.108292, mean_q: 5.561242, mean_eps: 0.917863
  13860/150000: episode: 146, duration: 0.852s, episode steps: 124, steps per second: 145, episode reward: -116.537, mean reward: -0.940 [-100.000, 12.849], mean action: 1.613 [0.000, 3.000],  loss: 15.541236, mse: 492.559922, mean_q: 5.499123, mean_eps: 0.917215
  13958/150000: episode: 147, duration: 0.644s, episode steps:  98, steps per second: 152, episode reward: -176.776, mean reward: -1.804 [-100.000,  8.863], mean action: 1.602 [0.000, 3.000],  loss: 19.768177, mse: 502.590888, mean_q: 5.484000, mean_eps: 0.916549
  14065/150000: episode: 148, duration: 0.750s, episode steps: 107, steps per second: 143, episode reward: -208.289, mean reward: -1.947 [-100.000,  1.361], mean action: 1.533 [0.000, 3.000],  loss: 13.464445, mse: 529.241903, mean_q: 5.535381, mean_eps: 0.915934
  14162/150000: episode: 149, duration: 0.700s, episode steps:  97, steps per second: 138, episode reward: -100.488, mean reward: -1.036 [-100.000,  8.451], mean action: 1.402 [0.000, 3.000],  loss: 15.239924, mse: 555.270280, mean_q: 5.222778, mean_eps: 0.915322
  14233/150000: episode: 150, duration: 0.479s, episode steps:  71, steps per second: 148, episode reward: -249.054, mean reward: -3.508 [-100.000, 112.603], mean action: 1.437 [0.000, 3.000],  loss: 14.470986, mse: 545.069487, mean_q: 6.036562, mean_eps: 0.914818
  14355/150000: episode: 151, duration: 0.826s, episode steps: 122, steps per second: 148, episode reward: -70.280, mean reward: -0.576 [-100.000, 11.030], mean action: 1.631 [0.000, 3.000],  loss: 16.792640, mse: 540.353827, mean_q: 6.023287, mean_eps: 0.914239
  14459/150000: episode: 152, duration: 0.720s, episode steps: 104, steps per second: 145, episode reward: -120.500, mean reward: -1.159 [-100.000,  7.008], mean action: 1.423 [0.000, 3.000],  loss: 12.468896, mse: 515.898148, mean_q: 5.867092, mean_eps: 0.913561
  14544/150000: episode: 153, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -155.106, mean reward: -1.825 [-100.000,  5.812], mean action: 1.553 [0.000, 3.000],  loss: 15.875183, mse: 515.050366, mean_q: 6.101051, mean_eps: 0.912994
  14630/150000: episode: 154, duration: 0.571s, episode steps:  86, steps per second: 151, episode reward: -139.982, mean reward: -1.628 [-100.000, 41.979], mean action: 1.302 [0.000, 3.000],  loss: 15.982764, mse: 528.753584, mean_q: 6.340317, mean_eps: 0.912481
  14712/150000: episode: 155, duration: 0.710s, episode steps:  82, steps per second: 116, episode reward: -96.445, mean reward: -1.176 [-100.000, 11.542], mean action: 1.451 [0.000, 3.000],  loss: 13.257791, mse: 534.109197, mean_q: 5.573208, mean_eps: 0.911977
  14821/150000: episode: 156, duration: 0.793s, episode steps: 109, steps per second: 138, episode reward: -4.877, mean reward: -0.045 [-100.000, 102.854], mean action: 1.642 [0.000, 3.000],  loss: 15.508948, mse: 589.098186, mean_q: 5.131579, mean_eps: 0.911404
  14918/150000: episode: 157, duration: 0.652s, episode steps:  97, steps per second: 149, episode reward: -163.629, mean reward: -1.687 [-100.000, 12.609], mean action: 1.320 [0.000, 3.000],  loss: 14.395208, mse: 559.545268, mean_q: 5.520643, mean_eps: 0.910786
  15004/150000: episode: 158, duration: 0.618s, episode steps:  86, steps per second: 139, episode reward: -150.404, mean reward: -1.749 [-100.000, 15.363], mean action: 1.640 [0.000, 3.000],  loss: 12.148024, mse: 519.930388, mean_q: 5.877639, mean_eps: 0.910237
  15129/150000: episode: 159, duration: 0.840s, episode steps: 125, steps per second: 149, episode reward: -159.214, mean reward: -1.274 [-100.000, 27.928], mean action: 1.624 [0.000, 3.000],  loss: 15.316884, mse: 553.301264, mean_q: 6.313859, mean_eps: 0.909604
  15195/150000: episode: 160, duration: 0.440s, episode steps:  66, steps per second: 150, episode reward: -71.415, mean reward: -1.082 [-100.000,  8.664], mean action: 1.515 [0.000, 3.000],  loss: 13.882995, mse: 568.164075, mean_q: 6.773457, mean_eps: 0.909031
  15287/150000: episode: 161, duration: 0.650s, episode steps:  92, steps per second: 142, episode reward: -222.779, mean reward: -2.422 [-100.000,  7.653], mean action: 1.522 [0.000, 3.000],  loss: 11.858108, mse: 585.966521, mean_q: 6.430930, mean_eps: 0.908557
  15402/150000: episode: 162, duration: 0.787s, episode steps: 115, steps per second: 146, episode reward: -123.508, mean reward: -1.074 [-100.000, 11.183], mean action: 1.409 [0.000, 3.000],  loss: 17.108598, mse: 566.974026, mean_q: 6.683756, mean_eps: 0.907936
  15511/150000: episode: 163, duration: 0.733s, episode steps: 109, steps per second: 149, episode reward: -225.039, mean reward: -2.065 [-100.000,  7.023], mean action: 1.679 [0.000, 3.000],  loss: 18.538166, mse: 559.020492, mean_q: 6.406158, mean_eps: 0.907264
  15583/150000: episode: 164, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: -114.317, mean reward: -1.588 [-100.000, 27.109], mean action: 1.486 [0.000, 3.000],  loss: 18.390832, mse: 569.344897, mean_q: 6.437498, mean_eps: 0.906721
  15686/150000: episode: 165, duration: 0.715s, episode steps: 103, steps per second: 144, episode reward: -332.297, mean reward: -3.226 [-100.000,  1.492], mean action: 1.476 [0.000, 3.000],  loss: 13.449945, mse: 543.877710, mean_q: 6.574767, mean_eps: 0.906196
  15844/150000: episode: 166, duration: 1.078s, episode steps: 158, steps per second: 147, episode reward: 16.250, mean reward:  0.103 [-100.000, 105.892], mean action: 1.601 [0.000, 3.000],  loss: 16.958177, mse: 553.997802, mean_q: 6.192156, mean_eps: 0.905413
  15919/150000: episode: 167, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -65.019, mean reward: -0.867 [-100.000, 10.193], mean action: 1.680 [0.000, 3.000],  loss: 12.888559, mse: 539.248288, mean_q: 6.286027, mean_eps: 0.904714
  16012/150000: episode: 168, duration: 0.631s, episode steps:  93, steps per second: 147, episode reward: -129.675, mean reward: -1.394 [-100.000, 20.853], mean action: 1.452 [0.000, 3.000],  loss: 14.071327, mse: 567.968449, mean_q: 6.459847, mean_eps: 0.904210
  16126/150000: episode: 169, duration: 0.761s, episode steps: 114, steps per second: 150, episode reward: -111.786, mean reward: -0.981 [-100.000,  6.313], mean action: 1.526 [0.000, 3.000],  loss: 16.544713, mse: 635.320028, mean_q: 6.944573, mean_eps: 0.903589
  16261/150000: episode: 170, duration: 0.963s, episode steps: 135, steps per second: 140, episode reward: -205.010, mean reward: -1.519 [-100.000, 86.627], mean action: 1.556 [0.000, 3.000],  loss: 15.029234, mse: 631.524071, mean_q: 6.554526, mean_eps: 0.902842
  16340/150000: episode: 171, duration: 0.534s, episode steps:  79, steps per second: 148, episode reward: -143.997, mean reward: -1.823 [-100.000,  5.321], mean action: 1.734 [0.000, 3.000],  loss: 14.181720, mse: 663.616340, mean_q: 6.656567, mean_eps: 0.902200
  16419/150000: episode: 172, duration: 0.526s, episode steps:  79, steps per second: 150, episode reward: -242.249, mean reward: -3.066 [-100.000, 55.055], mean action: 1.468 [0.000, 3.000],  loss: 16.524771, mse: 646.070339, mean_q: 6.160175, mean_eps: 0.901726
  16524/150000: episode: 173, duration: 0.722s, episode steps: 105, steps per second: 145, episode reward: -350.832, mean reward: -3.341 [-100.000,  0.756], mean action: 1.495 [0.000, 3.000],  loss: 15.299583, mse: 641.458362, mean_q: 6.855709, mean_eps: 0.901174
  16627/150000: episode: 174, duration: 0.783s, episode steps: 103, steps per second: 131, episode reward: -110.805, mean reward: -1.076 [-100.000, 10.852], mean action: 1.544 [0.000, 3.000],  loss: 14.737944, mse: 642.441414, mean_q: 6.536424, mean_eps: 0.900550
  16718/150000: episode: 175, duration: 0.705s, episode steps:  91, steps per second: 129, episode reward: -223.202, mean reward: -2.453 [-100.000,  5.784], mean action: 1.593 [0.000, 3.000],  loss: 16.562175, mse: 649.441583, mean_q: 6.759490, mean_eps: 0.899968
  16798/150000: episode: 176, duration: 0.579s, episode steps:  80, steps per second: 138, episode reward: -84.233, mean reward: -1.053 [-100.000, 10.571], mean action: 1.562 [0.000, 3.000],  loss: 16.504673, mse: 613.656391, mean_q: 6.291765, mean_eps: 0.899455
  16869/150000: episode: 177, duration: 0.552s, episode steps:  71, steps per second: 129, episode reward: -82.441, mean reward: -1.161 [-100.000,  7.876], mean action: 1.408 [0.000, 3.000],  loss: 13.209252, mse: 635.481461, mean_q: 6.840329, mean_eps: 0.899002
  16974/150000: episode: 178, duration: 0.769s, episode steps: 105, steps per second: 137, episode reward: -174.952, mean reward: -1.666 [-100.000, 13.472], mean action: 1.543 [0.000, 3.000],  loss: 13.776719, mse: 667.502844, mean_q: 5.911371, mean_eps: 0.898474
  17092/150000: episode: 179, duration: 0.917s, episode steps: 118, steps per second: 129, episode reward: -105.567, mean reward: -0.895 [-100.000, 32.391], mean action: 1.542 [0.000, 3.000],  loss: 12.977400, mse: 656.635926, mean_q: 6.276981, mean_eps: 0.897805
  17191/150000: episode: 180, duration: 0.769s, episode steps:  99, steps per second: 129, episode reward: -119.462, mean reward: -1.207 [-100.000,  6.093], mean action: 1.717 [0.000, 3.000],  loss: 17.851547, mse: 617.091318, mean_q: 6.965747, mean_eps: 0.897154
  17289/150000: episode: 181, duration: 0.661s, episode steps:  98, steps per second: 148, episode reward: -92.336, mean reward: -0.942 [-100.000,  8.268], mean action: 1.735 [0.000, 3.000],  loss: 17.334731, mse: 675.997760, mean_q: 6.299697, mean_eps: 0.896563
  17379/150000: episode: 182, duration: 0.594s, episode steps:  90, steps per second: 152, episode reward: -103.626, mean reward: -1.151 [-100.000,  9.893], mean action: 1.600 [0.000, 3.000],  loss: 14.800894, mse: 681.742016, mean_q: 6.363809, mean_eps: 0.895999
  17449/150000: episode: 183, duration: 0.521s, episode steps:  70, steps per second: 134, episode reward: -143.246, mean reward: -2.046 [-100.000,  9.165], mean action: 1.357 [0.000, 3.000],  loss: 17.019519, mse: 697.735295, mean_q: 6.106264, mean_eps: 0.895519
  17565/150000: episode: 184, duration: 0.794s, episode steps: 116, steps per second: 146, episode reward: -94.883, mean reward: -0.818 [-100.000, 17.921], mean action: 1.716 [0.000, 3.000],  loss: 19.169944, mse: 691.255737, mean_q: 6.575536, mean_eps: 0.894961
  17674/150000: episode: 185, duration: 0.734s, episode steps: 109, steps per second: 148, episode reward: -144.595, mean reward: -1.327 [-100.000,  9.087], mean action: 1.661 [0.000, 3.000],  loss: 14.453558, mse: 625.551574, mean_q: 6.864223, mean_eps: 0.894286
  17769/150000: episode: 186, duration: 0.692s, episode steps:  95, steps per second: 137, episode reward: -125.143, mean reward: -1.317 [-100.000, 11.662], mean action: 1.474 [0.000, 3.000],  loss: 13.618497, mse: 686.998374, mean_q: 6.278165, mean_eps: 0.893674
  17900/150000: episode: 187, duration: 0.884s, episode steps: 131, steps per second: 148, episode reward: -90.431, mean reward: -0.690 [-100.000,  7.189], mean action: 1.450 [0.000, 3.000],  loss: 15.757721, mse: 672.690527, mean_q: 6.697155, mean_eps: 0.892996
  18002/150000: episode: 188, duration: 0.681s, episode steps: 102, steps per second: 150, episode reward: -115.390, mean reward: -1.131 [-100.000,  7.415], mean action: 1.686 [0.000, 3.000],  loss: 14.627706, mse: 709.368390, mean_q: 6.042623, mean_eps: 0.892297
  18133/150000: episode: 189, duration: 0.928s, episode steps: 131, steps per second: 141, episode reward: -127.621, mean reward: -0.974 [-100.000,  8.623], mean action: 1.481 [0.000, 3.000],  loss: 17.505810, mse: 732.990222, mean_q: 6.967807, mean_eps: 0.891598
  18212/150000: episode: 190, duration: 0.534s, episode steps:  79, steps per second: 148, episode reward: -19.165, mean reward: -0.243 [-100.000, 19.564], mean action: 1.506 [0.000, 3.000],  loss: 12.967288, mse: 753.320696, mean_q: 6.698835, mean_eps: 0.890968
  18333/150000: episode: 191, duration: 0.808s, episode steps: 121, steps per second: 150, episode reward: -22.130, mean reward: -0.183 [-100.000, 91.765], mean action: 1.603 [0.000, 3.000],  loss: 16.451691, mse: 718.041916, mean_q: 7.210302, mean_eps: 0.890368
  18449/150000: episode: 192, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -342.032, mean reward: -2.949 [-100.000, 45.743], mean action: 1.534 [0.000, 3.000],  loss: 15.487538, mse: 733.090826, mean_q: 7.259485, mean_eps: 0.889657
  18541/150000: episode: 193, duration: 0.629s, episode steps:  92, steps per second: 146, episode reward: -134.679, mean reward: -1.464 [-100.000, 10.001], mean action: 1.500 [0.000, 3.000],  loss: 16.133464, mse: 722.873158, mean_q: 6.830396, mean_eps: 0.889033
  18617/150000: episode: 194, duration: 0.507s, episode steps:  76, steps per second: 150, episode reward: -103.651, mean reward: -1.364 [-100.000, 12.026], mean action: 1.618 [0.000, 3.000],  loss: 15.829311, mse: 686.833413, mean_q: 7.450568, mean_eps: 0.888529
  18741/150000: episode: 195, duration: 1.041s, episode steps: 124, steps per second: 119, episode reward: -150.888, mean reward: -1.217 [-100.000, 55.378], mean action: 1.629 [0.000, 3.000],  loss: 14.798375, mse: 722.539496, mean_q: 7.208430, mean_eps: 0.887929
  18863/150000: episode: 196, duration: 0.969s, episode steps: 122, steps per second: 126, episode reward: -205.901, mean reward: -1.688 [-100.000,  1.548], mean action: 1.598 [0.000, 3.000],  loss: 20.867543, mse: 739.058074, mean_q: 7.062802, mean_eps: 0.887191
  18943/150000: episode: 197, duration: 0.631s, episode steps:  80, steps per second: 127, episode reward: -8.676, mean reward: -0.108 [-100.000, 68.928], mean action: 1.350 [0.000, 3.000],  loss: 25.802841, mse: 729.249372, mean_q: 7.315745, mean_eps: 0.886585
  19067/150000: episode: 198, duration: 0.928s, episode steps: 124, steps per second: 134, episode reward: -107.068, mean reward: -0.863 [-100.000, 32.087], mean action: 1.589 [0.000, 3.000],  loss: 16.761810, mse: 764.658918, mean_q: 7.500858, mean_eps: 0.885973
  19141/150000: episode: 199, duration: 0.649s, episode steps:  74, steps per second: 114, episode reward: -121.497, mean reward: -1.642 [-100.000, 16.724], mean action: 1.473 [0.000, 3.000],  loss: 13.243903, mse: 803.176661, mean_q: 6.512529, mean_eps: 0.885379
  19262/150000: episode: 200, duration: 0.994s, episode steps: 121, steps per second: 122, episode reward: -94.838, mean reward: -0.784 [-100.000, 13.454], mean action: 1.636 [0.000, 3.000],  loss: 13.099427, mse: 762.957873, mean_q: 7.503971, mean_eps: 0.884794
  19335/150000: episode: 201, duration: 0.511s, episode steps:  73, steps per second: 143, episode reward: -100.578, mean reward: -1.378 [-100.000,  6.414], mean action: 1.616 [0.000, 3.000],  loss: 20.374958, mse: 838.226736, mean_q: 7.375933, mean_eps: 0.884212
  19439/150000: episode: 202, duration: 0.708s, episode steps: 104, steps per second: 147, episode reward: -141.237, mean reward: -1.358 [-100.000,  9.939], mean action: 1.279 [0.000, 3.000],  loss: 21.853177, mse: 789.377152, mean_q: 7.351628, mean_eps: 0.883681
  19559/150000: episode: 203, duration: 0.846s, episode steps: 120, steps per second: 142, episode reward: -77.175, mean reward: -0.643 [-100.000, 10.856], mean action: 1.567 [0.000, 3.000],  loss: 13.809026, mse: 763.704751, mean_q: 6.826352, mean_eps: 0.883009
  19675/150000: episode: 204, duration: 0.786s, episode steps: 116, steps per second: 148, episode reward: -125.012, mean reward: -1.078 [-100.000, 19.818], mean action: 1.448 [0.000, 3.000],  loss: 12.929509, mse: 803.106647, mean_q: 7.427947, mean_eps: 0.882301
  19798/150000: episode: 205, duration: 0.828s, episode steps: 123, steps per second: 149, episode reward: -266.611, mean reward: -2.168 [-100.000, 72.179], mean action: 1.634 [0.000, 3.000],  loss: 13.926105, mse: 764.323603, mean_q: 7.885302, mean_eps: 0.881584
  19878/150000: episode: 206, duration: 0.577s, episode steps:  80, steps per second: 139, episode reward: -69.634, mean reward: -0.870 [-100.000,  7.633], mean action: 1.675 [0.000, 3.000],  loss: 14.490593, mse: 785.215220, mean_q: 8.247294, mean_eps: 0.880975
  19994/150000: episode: 207, duration: 0.777s, episode steps: 116, steps per second: 149, episode reward: -165.704, mean reward: -1.428 [-100.000,  8.278], mean action: 1.509 [0.000, 3.000],  loss: 12.989919, mse: 828.817222, mean_q: 7.168137, mean_eps: 0.880387
  20077/150000: episode: 208, duration: 0.588s, episode steps:  83, steps per second: 141, episode reward: -155.490, mean reward: -1.873 [-100.000,  6.630], mean action: 1.542 [0.000, 3.000],  loss: 15.713635, mse: 830.619716, mean_q: 7.354597, mean_eps: 0.879790
  20154/150000: episode: 209, duration: 0.568s, episode steps:  77, steps per second: 136, episode reward: -80.637, mean reward: -1.047 [-100.000,  5.816], mean action: 1.584 [0.000, 3.000],  loss: 19.876804, mse: 791.212432, mean_q: 8.599818, mean_eps: 0.879310
  20225/150000: episode: 210, duration: 0.487s, episode steps:  71, steps per second: 146, episode reward: -99.624, mean reward: -1.403 [-100.000,  8.591], mean action: 1.563 [0.000, 3.000],  loss: 17.790402, mse: 821.638764, mean_q: 7.717495, mean_eps: 0.878866
  20328/150000: episode: 211, duration: 0.702s, episode steps: 103, steps per second: 147, episode reward: -87.908, mean reward: -0.853 [-100.000, 10.091], mean action: 1.612 [0.000, 3.000],  loss: 23.556094, mse: 821.278264, mean_q: 7.798200, mean_eps: 0.878344
  20439/150000: episode: 212, duration: 0.781s, episode steps: 111, steps per second: 142, episode reward: -100.972, mean reward: -0.910 [-100.000,  7.656], mean action: 1.495 [0.000, 3.000],  loss: 17.424609, mse: 801.271758, mean_q: 7.817381, mean_eps: 0.877702
  20574/150000: episode: 213, duration: 0.967s, episode steps: 135, steps per second: 140, episode reward: -179.754, mean reward: -1.332 [-100.000, 37.929], mean action: 1.444 [0.000, 3.000],  loss: 13.815142, mse: 792.071991, mean_q: 8.385052, mean_eps: 0.876964
  20679/150000: episode: 214, duration: 0.694s, episode steps: 105, steps per second: 151, episode reward: -147.597, mean reward: -1.406 [-100.000, 22.293], mean action: 1.429 [0.000, 3.000],  loss: 18.594740, mse: 839.174444, mean_q: 7.544852, mean_eps: 0.876244
  20807/150000: episode: 215, duration: 0.917s, episode steps: 128, steps per second: 140, episode reward: -237.535, mean reward: -1.856 [-100.000,  4.572], mean action: 1.320 [0.000, 3.000],  loss: 17.730741, mse: 775.004480, mean_q: 8.509439, mean_eps: 0.875545
  20911/150000: episode: 216, duration: 0.718s, episode steps: 104, steps per second: 145, episode reward: -289.316, mean reward: -2.782 [-100.000,  6.225], mean action: 1.596 [0.000, 3.000],  loss: 15.210206, mse: 840.766222, mean_q: 7.910880, mean_eps: 0.874849
  21025/150000: episode: 217, duration: 0.780s, episode steps: 114, steps per second: 146, episode reward: -116.381, mean reward: -1.021 [-100.000, 10.316], mean action: 1.509 [0.000, 3.000],  loss: 16.148658, mse: 822.734642, mean_q: 8.247344, mean_eps: 0.874195
  21169/150000: episode: 218, duration: 1.046s, episode steps: 144, steps per second: 138, episode reward: -53.808, mean reward: -0.374 [-100.000, 13.544], mean action: 1.486 [0.000, 3.000],  loss: 22.309790, mse: 887.639396, mean_q: 9.171162, mean_eps: 0.873421
  21270/150000: episode: 219, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: -248.630, mean reward: -2.462 [-100.000,  5.541], mean action: 1.495 [0.000, 3.000],  loss: 24.056652, mse: 870.895838, mean_q: 8.976438, mean_eps: 0.872686
  21366/150000: episode: 220, duration: 0.694s, episode steps:  96, steps per second: 138, episode reward: -133.573, mean reward: -1.391 [-100.000, 16.097], mean action: 1.438 [0.000, 3.000],  loss: 16.069311, mse: 863.759848, mean_q: 9.032089, mean_eps: 0.872095
  21441/150000: episode: 221, duration: 0.541s, episode steps:  75, steps per second: 139, episode reward: -75.068, mean reward: -1.001 [-100.000,  8.517], mean action: 1.613 [0.000, 3.000],  loss: 15.944711, mse: 840.968077, mean_q: 8.918865, mean_eps: 0.871582
  21532/150000: episode: 222, duration: 0.613s, episode steps:  91, steps per second: 148, episode reward: -124.864, mean reward: -1.372 [-100.000, 12.881], mean action: 1.758 [0.000, 3.000],  loss: 20.567290, mse: 899.354305, mean_q: 8.949818, mean_eps: 0.871084
  21637/150000: episode: 223, duration: 0.691s, episode steps: 105, steps per second: 152, episode reward: -74.421, mean reward: -0.709 [-100.000, 10.813], mean action: 1.676 [0.000, 3.000],  loss: 14.791150, mse: 814.514976, mean_q: 9.002252, mean_eps: 0.870496
  21721/150000: episode: 224, duration: 0.620s, episode steps:  84, steps per second: 136, episode reward: -78.834, mean reward: -0.938 [-100.000, 16.524], mean action: 1.786 [0.000, 3.000],  loss: 19.022938, mse: 893.216423, mean_q: 9.492147, mean_eps: 0.869929
  21834/150000: episode: 225, duration: 0.790s, episode steps: 113, steps per second: 143, episode reward: -68.357, mean reward: -0.605 [-100.000, 10.435], mean action: 1.558 [0.000, 3.000],  loss: 17.347875, mse: 861.730034, mean_q: 9.135648, mean_eps: 0.869338
  21915/150000: episode: 226, duration: 0.556s, episode steps:  81, steps per second: 146, episode reward: -93.523, mean reward: -1.155 [-100.000, 11.621], mean action: 1.469 [0.000, 3.000],  loss: 17.247435, mse: 837.774619, mean_q: 10.031433, mean_eps: 0.868756
  21980/150000: episode: 227, duration: 0.463s, episode steps:  65, steps per second: 140, episode reward: -75.358, mean reward: -1.159 [-100.000, 16.765], mean action: 1.369 [0.000, 3.000],  loss: 14.483164, mse: 845.148391, mean_q: 8.930590, mean_eps: 0.868318
  22048/150000: episode: 228, duration: 0.489s, episode steps:  68, steps per second: 139, episode reward: -116.686, mean reward: -1.716 [-100.000, 47.768], mean action: 1.338 [0.000, 3.000],  loss: 15.371805, mse: 876.323698, mean_q: 8.994788, mean_eps: 0.867919
  22179/150000: episode: 229, duration: 0.888s, episode steps: 131, steps per second: 148, episode reward: 23.829, mean reward:  0.182 [-100.000, 60.128], mean action: 1.481 [0.000, 3.000],  loss: 14.480356, mse: 908.294855, mean_q: 8.873286, mean_eps: 0.867322
  22263/150000: episode: 230, duration: 0.580s, episode steps:  84, steps per second: 145, episode reward: -103.432, mean reward: -1.231 [-100.000,  5.097], mean action: 1.464 [0.000, 3.000],  loss: 15.036695, mse: 906.408243, mean_q: 9.089992, mean_eps: 0.866677
  22372/150000: episode: 231, duration: 0.786s, episode steps: 109, steps per second: 139, episode reward: -129.243, mean reward: -1.186 [-100.000,  8.934], mean action: 1.450 [0.000, 3.000],  loss: 15.852369, mse: 890.241904, mean_q: 9.791262, mean_eps: 0.866098
  22447/150000: episode: 232, duration: 0.521s, episode steps:  75, steps per second: 144, episode reward: -89.734, mean reward: -1.196 [-100.000,  9.330], mean action: 1.600 [0.000, 3.000],  loss: 11.179033, mse: 934.862434, mean_q: 8.900143, mean_eps: 0.865546
  22552/150000: episode: 233, duration: 0.697s, episode steps: 105, steps per second: 151, episode reward: -53.883, mean reward: -0.513 [-100.000, 16.520], mean action: 1.667 [0.000, 3.000],  loss: 14.511587, mse: 907.397547, mean_q: 9.353255, mean_eps: 0.865006
  22658/150000: episode: 234, duration: 0.768s, episode steps: 106, steps per second: 138, episode reward: -95.235, mean reward: -0.898 [-100.000, 11.840], mean action: 1.509 [0.000, 3.000],  loss: 15.383833, mse: 881.267237, mean_q: 9.607415, mean_eps: 0.864373
  22721/150000: episode: 235, duration: 0.431s, episode steps:  63, steps per second: 146, episode reward: -79.214, mean reward: -1.257 [-100.000,  7.810], mean action: 1.476 [0.000, 3.000],  loss: 16.104192, mse: 922.919729, mean_q: 10.159934, mean_eps: 0.863866
  22800/150000: episode: 236, duration: 0.535s, episode steps:  79, steps per second: 148, episode reward: -202.349, mean reward: -2.561 [-100.000, 13.932], mean action: 1.633 [0.000, 3.000],  loss: 16.892071, mse: 898.549553, mean_q: 9.550252, mean_eps: 0.863440
  22863/150000: episode: 237, duration: 0.444s, episode steps:  63, steps per second: 142, episode reward: -78.426, mean reward: -1.245 [-100.000,  8.506], mean action: 1.381 [0.000, 3.000],  loss: 14.942049, mse: 917.491142, mean_q: 8.896611, mean_eps: 0.863014
  22961/150000: episode: 238, duration: 0.722s, episode steps:  98, steps per second: 136, episode reward: -110.737, mean reward: -1.130 [-100.000,  6.412], mean action: 1.439 [0.000, 3.000],  loss: 13.946754, mse: 928.008660, mean_q: 9.898562, mean_eps: 0.862531
  23048/150000: episode: 239, duration: 0.608s, episode steps:  87, steps per second: 143, episode reward: -107.979, mean reward: -1.241 [-100.000,  5.685], mean action: 1.425 [0.000, 3.000],  loss: 13.186006, mse: 957.042785, mean_q: 9.312148, mean_eps: 0.861976
  23141/150000: episode: 240, duration: 0.664s, episode steps:  93, steps per second: 140, episode reward: -90.147, mean reward: -0.969 [-100.000, 11.701], mean action: 1.624 [0.000, 3.000],  loss: 13.714516, mse: 942.573116, mean_q: 9.852091, mean_eps: 0.861436
  23208/150000: episode: 241, duration: 0.453s, episode steps:  67, steps per second: 148, episode reward: -81.403, mean reward: -1.215 [-100.000, 17.971], mean action: 1.687 [0.000, 3.000],  loss: 13.861879, mse: 890.608006, mean_q: 10.267007, mean_eps: 0.860956
  23297/150000: episode: 242, duration: 0.635s, episode steps:  89, steps per second: 140, episode reward: -63.963, mean reward: -0.719 [-100.000,  7.512], mean action: 1.607 [0.000, 3.000],  loss: 12.745957, mse: 917.823176, mean_q: 9.650736, mean_eps: 0.860488
  23384/150000: episode: 243, duration: 0.584s, episode steps:  87, steps per second: 149, episode reward: -58.596, mean reward: -0.674 [-100.000, 13.000], mean action: 1.483 [0.000, 3.000],  loss: 13.630590, mse: 938.264995, mean_q: 9.861210, mean_eps: 0.859960
  23458/150000: episode: 244, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -107.685, mean reward: -1.455 [-100.000,  9.216], mean action: 1.568 [0.000, 3.000],  loss: 13.521009, mse: 937.335892, mean_q: 9.926421, mean_eps: 0.859477
  23533/150000: episode: 245, duration: 0.536s, episode steps:  75, steps per second: 140, episode reward: -92.699, mean reward: -1.236 [-100.000,  5.481], mean action: 1.360 [0.000, 3.000],  loss: 11.866914, mse: 906.806505, mean_q: 9.891959, mean_eps: 0.859030
  23617/150000: episode: 246, duration: 0.589s, episode steps:  84, steps per second: 143, episode reward: -109.331, mean reward: -1.302 [-100.000, 15.377], mean action: 1.690 [0.000, 3.000],  loss: 11.388223, mse: 925.821126, mean_q: 9.796251, mean_eps: 0.858553
  23694/150000: episode: 247, duration: 0.519s, episode steps:  77, steps per second: 148, episode reward: -80.631, mean reward: -1.047 [-100.000,  7.544], mean action: 1.701 [0.000, 3.000],  loss: 15.884022, mse: 910.385223, mean_q: 9.685681, mean_eps: 0.858070
  23777/150000: episode: 248, duration: 0.568s, episode steps:  83, steps per second: 146, episode reward: -95.189, mean reward: -1.147 [-100.000,  8.564], mean action: 1.639 [0.000, 3.000],  loss: 14.150156, mse: 901.586847, mean_q: 9.759111, mean_eps: 0.857590
  23898/150000: episode: 249, duration: 0.874s, episode steps: 121, steps per second: 138, episode reward: -76.727, mean reward: -0.634 [-100.000, 41.655], mean action: 1.545 [0.000, 3.000],  loss: 12.171457, mse: 942.514856, mean_q: 9.059488, mean_eps: 0.856978
  24034/150000: episode: 250, duration: 0.934s, episode steps: 136, steps per second: 146, episode reward: -102.726, mean reward: -0.755 [-100.000,  7.755], mean action: 1.551 [0.000, 3.000],  loss: 14.370605, mse: 944.723182, mean_q: 10.067574, mean_eps: 0.856207
  24133/150000: episode: 251, duration: 0.655s, episode steps:  99, steps per second: 151, episode reward: -104.002, mean reward: -1.051 [-100.000,  7.783], mean action: 1.475 [0.000, 3.000],  loss: 14.047115, mse: 945.777134, mean_q: 9.335541, mean_eps: 0.855502
  24253/150000: episode: 252, duration: 0.885s, episode steps: 120, steps per second: 136, episode reward: -189.722, mean reward: -1.581 [-100.000,  1.805], mean action: 1.642 [0.000, 3.000],  loss: 16.496887, mse: 945.111959, mean_q: 10.137015, mean_eps: 0.854845
  24318/150000: episode: 253, duration: 0.445s, episode steps:  65, steps per second: 146, episode reward: -119.035, mean reward: -1.831 [-100.000,  6.397], mean action: 1.446 [0.000, 3.000],  loss: 17.250835, mse: 966.236588, mean_q: 9.942187, mean_eps: 0.854290
  24389/150000: episode: 254, duration: 0.485s, episode steps:  71, steps per second: 146, episode reward: -108.657, mean reward: -1.530 [-100.000,  5.956], mean action: 1.423 [0.000, 3.000],  loss: 12.849678, mse: 936.484822, mean_q: 9.832317, mean_eps: 0.853882
  24468/150000: episode: 255, duration: 0.572s, episode steps:  79, steps per second: 138, episode reward: -91.638, mean reward: -1.160 [-100.000,  7.632], mean action: 1.392 [0.000, 3.000],  loss: 12.763741, mse: 1050.633698, mean_q: 9.037371, mean_eps: 0.853432
  24596/150000: episode: 256, duration: 0.967s, episode steps: 128, steps per second: 132, episode reward: -212.640, mean reward: -1.661 [-100.000, 25.165], mean action: 1.391 [0.000, 3.000],  loss: 14.509732, mse: 999.734857, mean_q: 10.180036, mean_eps: 0.852811
  24711/150000: episode: 257, duration: 0.810s, episode steps: 115, steps per second: 142, episode reward: -147.030, mean reward: -1.279 [-100.000,  9.630], mean action: 1.687 [0.000, 3.000],  loss: 18.999897, mse: 958.559126, mean_q: 10.361490, mean_eps: 0.852082
  24794/150000: episode: 258, duration: 0.602s, episode steps:  83, steps per second: 138, episode reward: -91.323, mean reward: -1.100 [-100.000, 15.946], mean action: 1.663 [0.000, 3.000],  loss: 13.603378, mse: 983.694653, mean_q: 10.091904, mean_eps: 0.851488
  24923/150000: episode: 259, duration: 0.884s, episode steps: 129, steps per second: 146, episode reward: -20.102, mean reward: -0.156 [-100.000, 87.091], mean action: 1.620 [0.000, 3.000],  loss: 12.309872, mse: 942.035513, mean_q: 10.180307, mean_eps: 0.850852
  25012/150000: episode: 260, duration: 0.605s, episode steps:  89, steps per second: 147, episode reward: -155.904, mean reward: -1.752 [-100.000, 19.004], mean action: 1.483 [0.000, 3.000],  loss: 14.245348, mse: 961.311253, mean_q: 10.017866, mean_eps: 0.850198
  25095/150000: episode: 261, duration: 0.614s, episode steps:  83, steps per second: 135, episode reward: -95.217, mean reward: -1.147 [-100.000,  5.445], mean action: 1.386 [0.000, 3.000],  loss: 17.353339, mse: 961.388876, mean_q: 10.817781, mean_eps: 0.849682
  25178/150000: episode: 262, duration: 0.573s, episode steps:  83, steps per second: 145, episode reward: -110.744, mean reward: -1.334 [-100.000, 10.586], mean action: 1.458 [0.000, 3.000],  loss: 13.735514, mse: 959.577036, mean_q: 10.803684, mean_eps: 0.849184
  25262/150000: episode: 263, duration: 0.603s, episode steps:  84, steps per second: 139, episode reward: -121.170, mean reward: -1.443 [-100.000,  7.748], mean action: 1.690 [0.000, 3.000],  loss: 14.538372, mse: 930.452678, mean_q: 11.222390, mean_eps: 0.848683
  25388/150000: episode: 264, duration: 0.888s, episode steps: 126, steps per second: 142, episode reward: -75.805, mean reward: -0.602 [-100.000, 12.635], mean action: 1.532 [0.000, 3.000],  loss: 13.182916, mse: 969.314298, mean_q: 10.497178, mean_eps: 0.848053
  25488/150000: episode: 265, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: -58.223, mean reward: -0.582 [-100.000, 13.396], mean action: 1.530 [0.000, 3.000],  loss: 12.636125, mse: 975.189890, mean_q: 10.870366, mean_eps: 0.847375
  25578/150000: episode: 266, duration: 0.637s, episode steps:  90, steps per second: 141, episode reward: -128.057, mean reward: -1.423 [-100.000,  6.156], mean action: 1.744 [0.000, 3.000],  loss: 11.630795, mse: 941.626174, mean_q: 11.090964, mean_eps: 0.846805
  25678/150000: episode: 267, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: -81.596, mean reward: -0.816 [-100.000, 13.298], mean action: 1.670 [0.000, 3.000],  loss: 14.067554, mse: 958.196758, mean_q: 11.088363, mean_eps: 0.846235
  25756/150000: episode: 268, duration: 0.570s, episode steps:  78, steps per second: 137, episode reward: -63.407, mean reward: -0.813 [-100.000, 21.722], mean action: 1.410 [0.000, 3.000],  loss: 14.175326, mse: 999.215494, mean_q: 9.925967, mean_eps: 0.845701
  25861/150000: episode: 269, duration: 0.804s, episode steps: 105, steps per second: 131, episode reward: -30.895, mean reward: -0.294 [-100.000, 16.044], mean action: 1.543 [0.000, 3.000],  loss: 14.019910, mse: 910.761536, mean_q: 11.190215, mean_eps: 0.845152
  25958/150000: episode: 270, duration: 0.739s, episode steps:  97, steps per second: 131, episode reward: -158.443, mean reward: -1.633 [-100.000, 14.385], mean action: 1.619 [0.000, 3.000],  loss: 12.712268, mse: 977.215864, mean_q: 11.095584, mean_eps: 0.844546
  26050/150000: episode: 271, duration: 0.731s, episode steps:  92, steps per second: 126, episode reward: -133.698, mean reward: -1.453 [-100.000, 18.231], mean action: 1.728 [0.000, 3.000],  loss: 14.024228, mse: 933.284217, mean_q: 11.816863, mean_eps: 0.843979
  26145/150000: episode: 272, duration: 0.704s, episode steps:  95, steps per second: 135, episode reward: -82.890, mean reward: -0.873 [-100.000,  7.946], mean action: 1.432 [0.000, 3.000],  loss: 15.411641, mse: 950.667625, mean_q: 11.743917, mean_eps: 0.843418
  26202/150000: episode: 273, duration: 0.519s, episode steps:  57, steps per second: 110, episode reward: -172.109, mean reward: -3.019 [-100.000,  5.306], mean action: 1.491 [0.000, 3.000],  loss: 15.230064, mse: 961.859121, mean_q: 12.309090, mean_eps: 0.842962
  26278/150000: episode: 274, duration: 0.626s, episode steps:  76, steps per second: 121, episode reward: -89.179, mean reward: -1.173 [-100.000, 12.011], mean action: 1.776 [0.000, 3.000],  loss: 16.366065, mse: 923.145621, mean_q: 13.926289, mean_eps: 0.842563
  26388/150000: episode: 275, duration: 0.827s, episode steps: 110, steps per second: 133, episode reward: -256.587, mean reward: -2.333 [-100.000, 62.028], mean action: 1.436 [0.000, 3.000],  loss: 14.629704, mse: 928.276717, mean_q: 12.545367, mean_eps: 0.842005
  26467/150000: episode: 276, duration: 0.555s, episode steps:  79, steps per second: 142, episode reward: -135.774, mean reward: -1.719 [-100.000,  8.196], mean action: 1.557 [0.000, 3.000],  loss: 12.936486, mse: 986.460954, mean_q: 12.213092, mean_eps: 0.841438
  26536/150000: episode: 277, duration: 0.501s, episode steps:  69, steps per second: 138, episode reward: -79.695, mean reward: -1.155 [-100.000,  7.341], mean action: 1.464 [0.000, 3.000],  loss: 12.864702, mse: 938.929496, mean_q: 12.826485, mean_eps: 0.840994
  26640/150000: episode: 278, duration: 0.744s, episode steps: 104, steps per second: 140, episode reward: -115.770, mean reward: -1.113 [-100.000,  9.178], mean action: 1.635 [0.000, 3.000],  loss: 13.041147, mse: 958.148663, mean_q: 12.952713, mean_eps: 0.840475
  26703/150000: episode: 279, duration: 0.438s, episode steps:  63, steps per second: 144, episode reward: -112.194, mean reward: -1.781 [-100.000, 14.785], mean action: 1.476 [0.000, 3.000],  loss: 10.580238, mse: 985.845045, mean_q: 11.755348, mean_eps: 0.839974
  26783/150000: episode: 280, duration: 0.558s, episode steps:  80, steps per second: 143, episode reward: -52.217, mean reward: -0.653 [-100.000,  7.296], mean action: 1.738 [0.000, 3.000],  loss: 17.128622, mse: 953.311871, mean_q: 12.795760, mean_eps: 0.839545
  26910/150000: episode: 281, duration: 0.924s, episode steps: 127, steps per second: 137, episode reward: -307.584, mean reward: -2.422 [-100.000,  7.632], mean action: 1.480 [0.000, 3.000],  loss: 13.890379, mse: 976.633108, mean_q: 11.817334, mean_eps: 0.838924
  27011/150000: episode: 282, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -79.167, mean reward: -0.784 [-100.000,  6.827], mean action: 1.653 [0.000, 3.000],  loss: 13.276782, mse: 933.874273, mean_q: 12.750513, mean_eps: 0.838240
  27104/150000: episode: 283, duration: 0.654s, episode steps:  93, steps per second: 142, episode reward: -83.176, mean reward: -0.894 [-100.000, 16.179], mean action: 1.548 [0.000, 3.000],  loss: 12.316209, mse: 998.142078, mean_q: 12.452142, mean_eps: 0.837658
  27212/150000: episode: 284, duration: 0.796s, episode steps: 108, steps per second: 136, episode reward: -243.607, mean reward: -2.256 [-100.000, 79.242], mean action: 1.602 [0.000, 3.000],  loss: 16.834309, mse: 988.959488, mean_q: 12.247256, mean_eps: 0.837055
  27280/150000: episode: 285, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: -44.003, mean reward: -0.647 [-100.000, 10.832], mean action: 1.456 [0.000, 3.000],  loss: 10.227758, mse: 1050.825302, mean_q: 11.666775, mean_eps: 0.836527
  27379/150000: episode: 286, duration: 0.681s, episode steps:  99, steps per second: 145, episode reward: -141.016, mean reward: -1.424 [-100.000,  7.507], mean action: 1.505 [0.000, 3.000],  loss: 14.801074, mse: 980.087249, mean_q: 12.897716, mean_eps: 0.836026
  27445/150000: episode: 287, duration: 0.483s, episode steps:  66, steps per second: 137, episode reward: -120.170, mean reward: -1.821 [-100.000, 14.437], mean action: 1.909 [0.000, 3.000],  loss: 11.999236, mse: 1000.973489, mean_q: 12.816267, mean_eps: 0.835531
  27523/150000: episode: 288, duration: 0.556s, episode steps:  78, steps per second: 140, episode reward: -71.473, mean reward: -0.916 [-100.000,  7.220], mean action: 1.718 [0.000, 3.000],  loss: 13.486633, mse: 1016.908696, mean_q: 12.980629, mean_eps: 0.835099
  27631/150000: episode: 289, duration: 0.749s, episode steps: 108, steps per second: 144, episode reward: -103.685, mean reward: -0.960 [-100.000,  6.625], mean action: 1.685 [0.000, 3.000],  loss: 9.856020, mse: 980.955807, mean_q: 12.770414, mean_eps: 0.834541
  27709/150000: episode: 290, duration: 0.539s, episode steps:  78, steps per second: 145, episode reward: -156.938, mean reward: -2.012 [-100.000,  9.223], mean action: 1.462 [0.000, 3.000],  loss: 15.443433, mse: 1016.032583, mean_q: 12.941240, mean_eps: 0.833983
  27808/150000: episode: 291, duration: 0.807s, episode steps:  99, steps per second: 123, episode reward: -141.570, mean reward: -1.430 [-100.000,  5.483], mean action: 1.808 [0.000, 3.000],  loss: 17.470152, mse: 979.327170, mean_q: 13.360568, mean_eps: 0.833452
  27919/150000: episode: 292, duration: 0.895s, episode steps: 111, steps per second: 124, episode reward: -114.925, mean reward: -1.035 [-100.000,  9.212], mean action: 1.432 [0.000, 3.000],  loss: 12.998902, mse: 1001.823015, mean_q: 13.337137, mean_eps: 0.832822
  27990/150000: episode: 293, duration: 0.560s, episode steps:  71, steps per second: 127, episode reward: -109.540, mean reward: -1.543 [-100.000,  9.374], mean action: 1.394 [0.000, 3.000],  loss: 14.254322, mse: 1011.824919, mean_q: 13.326035, mean_eps: 0.832276
  28103/150000: episode: 294, duration: 0.916s, episode steps: 113, steps per second: 123, episode reward: -68.161, mean reward: -0.603 [-100.000, 12.757], mean action: 1.558 [0.000, 3.000],  loss: 13.931586, mse: 1021.178178, mean_q: 13.654984, mean_eps: 0.831724
  28198/150000: episode: 295, duration: 0.813s, episode steps:  95, steps per second: 117, episode reward: -122.377, mean reward: -1.288 [-100.000,  5.984], mean action: 1.768 [0.000, 3.000],  loss: 12.298526, mse: 1013.990332, mean_q: 12.837522, mean_eps: 0.831100
  28308/150000: episode: 296, duration: 0.943s, episode steps: 110, steps per second: 117, episode reward: -106.373, mean reward: -0.967 [-100.000, 17.716], mean action: 1.645 [0.000, 3.000],  loss: 12.908914, mse: 1038.579908, mean_q: 13.026697, mean_eps: 0.830485
  28426/150000: episode: 297, duration: 0.940s, episode steps: 118, steps per second: 125, episode reward: -45.656, mean reward: -0.387 [-100.000, 102.603], mean action: 1.534 [0.000, 3.000],  loss: 12.840524, mse: 1009.491649, mean_q: 12.732623, mean_eps: 0.829801
  28530/150000: episode: 298, duration: 0.724s, episode steps: 104, steps per second: 144, episode reward: -204.334, mean reward: -1.965 [-100.000,  5.197], mean action: 1.433 [0.000, 3.000],  loss: 17.307886, mse: 1049.413422, mean_q: 12.753363, mean_eps: 0.829135
  28624/150000: episode: 299, duration: 0.707s, episode steps:  94, steps per second: 133, episode reward: -152.824, mean reward: -1.626 [-100.000,  9.339], mean action: 1.766 [0.000, 3.000],  loss: 12.522201, mse: 1012.128855, mean_q: 13.242876, mean_eps: 0.828541
  28694/150000: episode: 300, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -143.275, mean reward: -2.047 [-100.000, 10.603], mean action: 1.471 [0.000, 3.000],  loss: 16.066289, mse: 1010.755257, mean_q: 14.568888, mean_eps: 0.828049
  28778/150000: episode: 301, duration: 0.573s, episode steps:  84, steps per second: 147, episode reward: -86.188, mean reward: -1.026 [-100.000,  7.912], mean action: 1.595 [0.000, 3.000],  loss: 12.195281, mse: 1037.425345, mean_q: 12.905634, mean_eps: 0.827587
  28929/150000: episode: 302, duration: 1.079s, episode steps: 151, steps per second: 140, episode reward: -119.005, mean reward: -0.788 [-100.000,  7.490], mean action: 1.510 [0.000, 3.000],  loss: 12.833870, mse: 1025.890976, mean_q: 12.670011, mean_eps: 0.826882
  29010/150000: episode: 303, duration: 0.568s, episode steps:  81, steps per second: 142, episode reward: -83.994, mean reward: -1.037 [-100.000, 10.853], mean action: 1.519 [0.000, 3.000],  loss: 11.662058, mse: 1018.801863, mean_q: 12.984132, mean_eps: 0.826186
  29076/150000: episode: 304, duration: 0.455s, episode steps:  66, steps per second: 145, episode reward: -69.534, mean reward: -1.054 [-100.000, 11.424], mean action: 1.742 [0.000, 3.000],  loss: 13.938291, mse: 1021.948382, mean_q: 11.319971, mean_eps: 0.825745
  29146/150000: episode: 305, duration: 0.463s, episode steps:  70, steps per second: 151, episode reward: -96.627, mean reward: -1.380 [-100.000, 12.932], mean action: 1.600 [0.000, 3.000],  loss: 11.133345, mse: 1006.881312, mean_q: 13.066760, mean_eps: 0.825337
  29239/150000: episode: 306, duration: 0.694s, episode steps:  93, steps per second: 134, episode reward: -64.041, mean reward: -0.689 [-100.000, 10.773], mean action: 1.516 [0.000, 3.000],  loss: 11.732628, mse: 1030.935775, mean_q: 12.258925, mean_eps: 0.824848
  29313/150000: episode: 307, duration: 0.513s, episode steps:  74, steps per second: 144, episode reward: -184.217, mean reward: -2.489 [-100.000,  6.966], mean action: 1.527 [0.000, 3.000],  loss: 10.600968, mse: 992.937674, mean_q: 12.888644, mean_eps: 0.824347
  29391/150000: episode: 308, duration: 0.538s, episode steps:  78, steps per second: 145, episode reward: -110.686, mean reward: -1.419 [-100.000,  7.887], mean action: 1.333 [0.000, 3.000],  loss: 8.289027, mse: 1006.775625, mean_q: 14.033572, mean_eps: 0.823891
  29469/150000: episode: 309, duration: 0.569s, episode steps:  78, steps per second: 137, episode reward: -85.118, mean reward: -1.091 [-100.000,  6.835], mean action: 1.462 [0.000, 3.000],  loss: 11.485363, mse: 1016.238932, mean_q: 13.377000, mean_eps: 0.823423
  29611/150000: episode: 310, duration: 1.051s, episode steps: 142, steps per second: 135, episode reward: -72.538, mean reward: -0.511 [-100.000,  6.880], mean action: 1.570 [0.000, 3.000],  loss: 9.386780, mse: 1034.675691, mean_q: 13.577321, mean_eps: 0.822763
  29715/150000: episode: 311, duration: 0.766s, episode steps: 104, steps per second: 136, episode reward: -85.811, mean reward: -0.825 [-100.000, 12.931], mean action: 1.615 [0.000, 3.000],  loss: 13.513045, mse: 1015.006054, mean_q: 13.147726, mean_eps: 0.822025
  29783/150000: episode: 312, duration: 0.525s, episode steps:  68, steps per second: 130, episode reward: -55.886, mean reward: -0.822 [-100.000,  6.724], mean action: 1.412 [0.000, 3.000],  loss: 9.976727, mse: 1081.599337, mean_q: 13.105707, mean_eps: 0.821509
  29871/150000: episode: 313, duration: 0.640s, episode steps:  88, steps per second: 138, episode reward: -46.681, mean reward: -0.530 [-100.000, 17.960], mean action: 1.523 [0.000, 3.000],  loss: 14.298631, mse: 1063.600400, mean_q: 13.579058, mean_eps: 0.821041
  29959/150000: episode: 314, duration: 0.599s, episode steps:  88, steps per second: 147, episode reward: -69.894, mean reward: -0.794 [-100.000, 12.364], mean action: 1.409 [0.000, 3.000],  loss: 14.442030, mse: 1055.291265, mean_q: 13.433596, mean_eps: 0.820513
  30046/150000: episode: 315, duration: 0.600s, episode steps:  87, steps per second: 145, episode reward: -138.647, mean reward: -1.594 [-100.000, 33.181], mean action: 1.563 [0.000, 3.000],  loss: 15.274623, mse: 1049.379484, mean_q: 13.491000, mean_eps: 0.819988
  30159/150000: episode: 316, duration: 0.908s, episode steps: 113, steps per second: 124, episode reward: -126.324, mean reward: -1.118 [-100.000, 16.703], mean action: 1.575 [0.000, 3.000],  loss: 11.080640, mse: 1108.193091, mean_q: 13.301013, mean_eps: 0.819388
  30227/150000: episode: 317, duration: 0.475s, episode steps:  68, steps per second: 143, episode reward: -73.093, mean reward: -1.075 [-100.000,  6.032], mean action: 1.647 [0.000, 3.000],  loss: 7.854757, mse: 1060.592008, mean_q: 13.290698, mean_eps: 0.818845
  30335/150000: episode: 318, duration: 0.743s, episode steps: 108, steps per second: 145, episode reward: -112.617, mean reward: -1.043 [-100.000,  6.689], mean action: 1.704 [0.000, 3.000],  loss: 11.121629, mse: 1079.143612, mean_q: 13.624989, mean_eps: 0.818317
  30432/150000: episode: 319, duration: 0.714s, episode steps:  97, steps per second: 136, episode reward: -38.855, mean reward: -0.401 [-100.000, 16.591], mean action: 1.629 [0.000, 3.000],  loss: 14.854664, mse: 1105.400565, mean_q: 13.417135, mean_eps: 0.817702
  30532/150000: episode: 320, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: -60.290, mean reward: -0.603 [-100.000,  8.584], mean action: 1.570 [0.000, 3.000],  loss: 13.151052, mse: 1085.985312, mean_q: 13.505268, mean_eps: 0.817111
  30677/150000: episode: 321, duration: 1.046s, episode steps: 145, steps per second: 139, episode reward: 11.031, mean reward:  0.076 [-100.000, 58.170], mean action: 1.448 [0.000, 3.000],  loss: 14.138031, mse: 1091.802159, mean_q: 13.819529, mean_eps: 0.816376
  30796/150000: episode: 322, duration: 0.847s, episode steps: 119, steps per second: 141, episode reward: -40.658, mean reward: -0.342 [-100.000, 16.261], mean action: 1.563 [0.000, 3.000],  loss: 11.761114, mse: 1112.986778, mean_q: 13.689924, mean_eps: 0.815584
  30870/150000: episode: 323, duration: 0.523s, episode steps:  74, steps per second: 141, episode reward: -84.242, mean reward: -1.138 [-100.000, 12.982], mean action: 1.527 [0.000, 3.000],  loss: 13.787422, mse: 1059.240303, mean_q: 15.031415, mean_eps: 0.815005
  30942/150000: episode: 324, duration: 0.499s, episode steps:  72, steps per second: 144, episode reward: -75.919, mean reward: -1.054 [-100.000,  7.617], mean action: 1.375 [0.000, 3.000],  loss: 13.781313, mse: 1141.774929, mean_q: 14.230904, mean_eps: 0.814567
  31042/150000: episode: 325, duration: 0.780s, episode steps: 100, steps per second: 128, episode reward: -218.204, mean reward: -2.182 [-100.000,  1.002], mean action: 1.710 [0.000, 3.000],  loss: 15.966389, mse: 1123.737500, mean_q: 14.199540, mean_eps: 0.814051
  31169/150000: episode: 326, duration: 0.897s, episode steps: 127, steps per second: 142, episode reward: -45.960, mean reward: -0.362 [-100.000, 16.883], mean action: 1.638 [0.000, 3.000],  loss: 15.949072, mse: 1138.109813, mean_q: 14.579328, mean_eps: 0.813370
  31229/150000: episode: 327, duration: 0.418s, episode steps:  60, steps per second: 143, episode reward: -121.192, mean reward: -2.020 [-100.000, 39.433], mean action: 1.467 [0.000, 3.000],  loss: 9.223235, mse: 1098.300214, mean_q: 16.222303, mean_eps: 0.812809
  31307/150000: episode: 328, duration: 0.578s, episode steps:  78, steps per second: 135, episode reward: -66.343, mean reward: -0.851 [-100.000, 11.199], mean action: 1.436 [0.000, 3.000],  loss: 11.578433, mse: 1134.642365, mean_q: 14.882639, mean_eps: 0.812395
  31411/150000: episode: 329, duration: 0.721s, episode steps: 104, steps per second: 144, episode reward: -34.302, mean reward: -0.330 [-100.000, 11.418], mean action: 1.577 [0.000, 3.000],  loss: 9.831333, mse: 1136.546451, mean_q: 15.151495, mean_eps: 0.811849
  31531/150000: episode: 330, duration: 0.853s, episode steps: 120, steps per second: 141, episode reward: -153.886, mean reward: -1.282 [-100.000, 18.042], mean action: 1.625 [0.000, 3.000],  loss: 15.161997, mse: 1147.594670, mean_q: 14.548168, mean_eps: 0.811177
  31606/150000: episode: 331, duration: 0.560s, episode steps:  75, steps per second: 134, episode reward: -86.306, mean reward: -1.151 [-100.000, 13.831], mean action: 1.600 [0.000, 3.000],  loss: 9.328591, mse: 1103.189161, mean_q: 15.103998, mean_eps: 0.810592
  31688/150000: episode: 332, duration: 0.585s, episode steps:  82, steps per second: 140, episode reward: -131.137, mean reward: -1.599 [-100.000,  7.737], mean action: 1.329 [0.000, 3.000],  loss: 13.467070, mse: 1097.275630, mean_q: 14.836302, mean_eps: 0.810121
  31750/150000: episode: 333, duration: 0.434s, episode steps:  62, steps per second: 143, episode reward: -80.181, mean reward: -1.293 [-100.000, 10.265], mean action: 1.629 [0.000, 3.000],  loss: 13.414524, mse: 1129.472319, mean_q: 14.844247, mean_eps: 0.809689
  31812/150000: episode: 334, duration: 0.452s, episode steps:  62, steps per second: 137, episode reward: -63.028, mean reward: -1.017 [-100.000,  8.509], mean action: 1.323 [0.000, 3.000],  loss: 14.474605, mse: 1143.029889, mean_q: 14.325370, mean_eps: 0.809317
  31898/150000: episode: 335, duration: 0.629s, episode steps:  86, steps per second: 137, episode reward: -63.967, mean reward: -0.744 [-100.000, 11.831], mean action: 1.628 [0.000, 3.000],  loss: 12.328524, mse: 1134.281845, mean_q: 14.928871, mean_eps: 0.808873
  31981/150000: episode: 336, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -142.828, mean reward: -1.721 [-100.000,  7.256], mean action: 1.506 [0.000, 3.000],  loss: 12.275391, mse: 1115.402164, mean_q: 15.495899, mean_eps: 0.808366
  32058/150000: episode: 337, duration: 0.521s, episode steps:  77, steps per second: 148, episode reward: -73.809, mean reward: -0.959 [-100.000, 15.876], mean action: 1.701 [0.000, 3.000],  loss: 14.305152, mse: 1142.840675, mean_q: 15.199100, mean_eps: 0.807886
  32157/150000: episode: 338, duration: 0.699s, episode steps:  99, steps per second: 142, episode reward: -153.456, mean reward: -1.550 [-100.000, 10.947], mean action: 1.576 [0.000, 3.000],  loss: 12.567379, mse: 1194.480691, mean_q: 15.432708, mean_eps: 0.807358
  32285/150000: episode: 339, duration: 0.949s, episode steps: 128, steps per second: 135, episode reward: -181.962, mean reward: -1.422 [-100.000, 15.170], mean action: 1.484 [0.000, 3.000],  loss: 10.908617, mse: 1181.343325, mean_q: 14.565629, mean_eps: 0.806677
  32370/150000: episode: 340, duration: 0.588s, episode steps:  85, steps per second: 145, episode reward: -99.685, mean reward: -1.173 [-100.000,  7.021], mean action: 1.353 [0.000, 3.000],  loss: 10.419142, mse: 1157.096894, mean_q: 15.808186, mean_eps: 0.806038
  32426/150000: episode: 341, duration: 0.393s, episode steps:  56, steps per second: 142, episode reward: -82.968, mean reward: -1.482 [-100.000, 26.424], mean action: 1.518 [0.000, 3.000],  loss: 10.632491, mse: 1201.368338, mean_q: 15.972673, mean_eps: 0.805615
  32516/150000: episode: 342, duration: 0.685s, episode steps:  90, steps per second: 131, episode reward: -128.986, mean reward: -1.433 [-100.000,  6.775], mean action: 1.411 [0.000, 3.000],  loss: 13.279177, mse: 1199.374901, mean_q: 15.176643, mean_eps: 0.805177
  32582/150000: episode: 343, duration: 0.478s, episode steps:  66, steps per second: 138, episode reward: -76.454, mean reward: -1.158 [-100.000,  6.056], mean action: 1.621 [0.000, 3.000],  loss: 13.896635, mse: 1186.161235, mean_q: 16.074317, mean_eps: 0.804709
  32701/150000: episode: 344, duration: 0.825s, episode steps: 119, steps per second: 144, episode reward: -312.040, mean reward: -2.622 [-100.000, 125.073], mean action: 1.529 [0.000, 3.000],  loss: 13.774145, mse: 1162.764931, mean_q: 15.618441, mean_eps: 0.804154
  32775/150000: episode: 345, duration: 0.528s, episode steps:  74, steps per second: 140, episode reward: -107.006, mean reward: -1.446 [-100.000,  6.007], mean action: 1.432 [0.000, 3.000],  loss: 20.147608, mse: 1226.007561, mean_q: 15.479170, mean_eps: 0.803575
  32870/150000: episode: 346, duration: 0.698s, episode steps:  95, steps per second: 136, episode reward: -120.312, mean reward: -1.266 [-100.000,  5.165], mean action: 1.726 [0.000, 3.000],  loss: 12.068128, mse: 1184.620793, mean_q: 15.837285, mean_eps: 0.803068
  32997/150000: episode: 347, duration: 0.885s, episode steps: 127, steps per second: 144, episode reward: -23.558, mean reward: -0.185 [-100.000, 93.834], mean action: 1.559 [0.000, 3.000],  loss: 15.122909, mse: 1229.588216, mean_q: 15.053901, mean_eps: 0.802402
  33077/150000: episode: 348, duration: 0.578s, episode steps:  80, steps per second: 138, episode reward: -42.612, mean reward: -0.533 [-100.000, 19.691], mean action: 1.688 [0.000, 3.000],  loss: 14.134427, mse: 1165.154979, mean_q: 16.273608, mean_eps: 0.801781
  33187/150000: episode: 349, duration: 0.811s, episode steps: 110, steps per second: 136, episode reward: -159.623, mean reward: -1.451 [-100.000,  9.324], mean action: 1.509 [0.000, 3.000],  loss: 12.276940, mse: 1191.484149, mean_q: 17.060737, mean_eps: 0.801211
  33278/150000: episode: 350, duration: 0.622s, episode steps:  91, steps per second: 146, episode reward: -103.551, mean reward: -1.138 [-100.000,  7.023], mean action: 1.571 [0.000, 3.000],  loss: 13.003333, mse: 1203.118690, mean_q: 16.991600, mean_eps: 0.800608
  33364/150000: episode: 351, duration: 0.588s, episode steps:  86, steps per second: 146, episode reward: -97.469, mean reward: -1.133 [-100.000, 18.166], mean action: 1.395 [0.000, 3.000],  loss: 10.390776, mse: 1188.587939, mean_q: 17.714754, mean_eps: 0.800077
  33449/150000: episode: 352, duration: 0.641s, episode steps:  85, steps per second: 133, episode reward: -99.462, mean reward: -1.170 [-100.000,  6.968], mean action: 1.565 [0.000, 3.000],  loss: 12.418804, mse: 1194.444348, mean_q: 17.615985, mean_eps: 0.799564
  33542/150000: episode: 353, duration: 0.661s, episode steps:  93, steps per second: 141, episode reward: -67.835, mean reward: -0.729 [-100.000, 15.327], mean action: 1.570 [0.000, 3.000],  loss: 12.756982, mse: 1165.120643, mean_q: 16.949894, mean_eps: 0.799030
  33668/150000: episode: 354, duration: 0.892s, episode steps: 126, steps per second: 141, episode reward: -49.621, mean reward: -0.394 [-100.000, 11.002], mean action: 1.754 [0.000, 3.000],  loss: 12.528284, mse: 1177.299737, mean_q: 17.316014, mean_eps: 0.798373
  33778/150000: episode: 355, duration: 0.829s, episode steps: 110, steps per second: 133, episode reward: -87.222, mean reward: -0.793 [-100.000, 13.482], mean action: 1.345 [0.000, 3.000],  loss: 10.103046, mse: 1219.283124, mean_q: 17.234570, mean_eps: 0.797665
  33855/150000: episode: 356, duration: 0.549s, episode steps:  77, steps per second: 140, episode reward: -61.180, mean reward: -0.795 [-100.000, 22.682], mean action: 1.597 [0.000, 3.000],  loss: 8.373739, mse: 1166.150501, mean_q: 17.570804, mean_eps: 0.797104
  33975/150000: episode: 357, duration: 0.870s, episode steps: 120, steps per second: 138, episode reward: -41.200, mean reward: -0.343 [-100.000, 17.211], mean action: 1.508 [0.000, 3.000],  loss: 11.497339, mse: 1212.385587, mean_q: 17.153029, mean_eps: 0.796513
  34075/150000: episode: 358, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: -48.477, mean reward: -0.485 [-100.000, 13.350], mean action: 1.570 [0.000, 3.000],  loss: 11.474683, mse: 1269.728144, mean_q: 18.253947, mean_eps: 0.795853
  34212/150000: episode: 359, duration: 0.976s, episode steps: 137, steps per second: 140, episode reward: -262.823, mean reward: -1.918 [-100.000, 122.627], mean action: 1.562 [0.000, 3.000],  loss: 13.174951, mse: 1246.644305, mean_q: 17.667985, mean_eps: 0.795142
  34334/150000: episode: 360, duration: 0.913s, episode steps: 122, steps per second: 134, episode reward: -82.559, mean reward: -0.677 [-100.000, 10.739], mean action: 1.475 [0.000, 3.000],  loss: 17.054077, mse: 1272.623653, mean_q: 18.541509, mean_eps: 0.794365
  34409/150000: episode: 361, duration: 0.541s, episode steps:  75, steps per second: 139, episode reward: -155.069, mean reward: -2.068 [-100.000, 23.572], mean action: 1.480 [0.000, 3.000],  loss: 19.351262, mse: 1239.096592, mean_q: 18.026350, mean_eps: 0.793774
  34532/150000: episode: 362, duration: 0.848s, episode steps: 123, steps per second: 145, episode reward: -108.353, mean reward: -0.881 [-100.000, 11.418], mean action: 1.610 [0.000, 3.000],  loss: 16.739414, mse: 1245.796777, mean_q: 17.068045, mean_eps: 0.793180
  34600/150000: episode: 363, duration: 0.506s, episode steps:  68, steps per second: 134, episode reward: -50.688, mean reward: -0.745 [-100.000,  7.265], mean action: 1.574 [0.000, 3.000],  loss: 16.453669, mse: 1199.636652, mean_q: 18.819778, mean_eps: 0.792607
  34733/150000: episode: 364, duration: 0.961s, episode steps: 133, steps per second: 138, episode reward: -128.521, mean reward: -0.966 [-100.000, 52.882], mean action: 1.511 [0.000, 3.000],  loss: 13.203191, mse: 1231.637537, mean_q: 18.093818, mean_eps: 0.792004
  34812/150000: episode: 365, duration: 0.561s, episode steps:  79, steps per second: 141, episode reward: -83.749, mean reward: -1.060 [-100.000, 13.425], mean action: 1.646 [0.000, 3.000],  loss: 15.551377, mse: 1248.798824, mean_q: 17.855863, mean_eps: 0.791368
  34920/150000: episode: 366, duration: 0.897s, episode steps: 108, steps per second: 120, episode reward: -129.265, mean reward: -1.197 [-100.000,  6.616], mean action: 1.500 [0.000, 3.000],  loss: 14.981632, mse: 1246.219715, mean_q: 18.352204, mean_eps: 0.790807
  35035/150000: episode: 367, duration: 0.929s, episode steps: 115, steps per second: 124, episode reward: -49.098, mean reward: -0.427 [-100.000,  7.292], mean action: 1.487 [0.000, 3.000],  loss: 12.255627, mse: 1258.660485, mean_q: 18.657886, mean_eps: 0.790138
  35113/150000: episode: 368, duration: 0.618s, episode steps:  78, steps per second: 126, episode reward: -66.128, mean reward: -0.848 [-100.000, 10.733], mean action: 1.513 [0.000, 3.000],  loss: 15.242233, mse: 1286.256463, mean_q: 18.305067, mean_eps: 0.789559
  35219/150000: episode: 369, duration: 0.865s, episode steps: 106, steps per second: 122, episode reward: -86.151, mean reward: -0.813 [-100.000, 11.303], mean action: 1.566 [0.000, 3.000],  loss: 20.374503, mse: 1325.189735, mean_q: 17.874472, mean_eps: 0.789007
  35316/150000: episode: 370, duration: 0.748s, episode steps:  97, steps per second: 130, episode reward: -132.801, mean reward: -1.369 [-100.000, 10.627], mean action: 1.402 [0.000, 3.000],  loss: 17.163499, mse: 1333.817172, mean_q: 18.629697, mean_eps: 0.788398
  35402/150000: episode: 371, duration: 0.648s, episode steps:  86, steps per second: 133, episode reward: -89.933, mean reward: -1.046 [-100.000,  9.630], mean action: 1.488 [0.000, 3.000],  loss: 17.925464, mse: 1310.319534, mean_q: 18.234920, mean_eps: 0.787849
  35504/150000: episode: 372, duration: 0.766s, episode steps: 102, steps per second: 133, episode reward: -117.181, mean reward: -1.149 [-100.000, 11.436], mean action: 1.324 [0.000, 3.000],  loss: 14.277954, mse: 1293.497031, mean_q: 18.240387, mean_eps: 0.787285
  35639/150000: episode: 373, duration: 1.039s, episode steps: 135, steps per second: 130, episode reward: -61.111, mean reward: -0.453 [-100.000,  5.682], mean action: 1.444 [0.000, 3.000],  loss: 16.234917, mse: 1260.072975, mean_q: 17.868306, mean_eps: 0.786574
  35705/150000: episode: 374, duration: 0.480s, episode steps:  66, steps per second: 137, episode reward: -66.747, mean reward: -1.011 [-100.000,  6.359], mean action: 1.667 [0.000, 3.000],  loss: 15.159487, mse: 1322.349824, mean_q: 17.753488, mean_eps: 0.785971
  35789/150000: episode: 375, duration: 0.615s, episode steps:  84, steps per second: 137, episode reward: -46.942, mean reward: -0.559 [-100.000, 15.757], mean action: 1.571 [0.000, 3.000],  loss: 13.645996, mse: 1334.562513, mean_q: 17.383587, mean_eps: 0.785521
  35922/150000: episode: 376, duration: 0.935s, episode steps: 133, steps per second: 142, episode reward: -104.432, mean reward: -0.785 [-100.000,  6.524], mean action: 1.647 [0.000, 3.000],  loss: 16.471029, mse: 1358.868010, mean_q: 18.397353, mean_eps: 0.784870
  36023/150000: episode: 377, duration: 0.730s, episode steps: 101, steps per second: 138, episode reward: -68.215, mean reward: -0.675 [-100.000,  8.450], mean action: 1.396 [0.000, 3.000],  loss: 13.726741, mse: 1326.191463, mean_q: 18.845354, mean_eps: 0.784168
  36129/150000: episode: 378, duration: 0.762s, episode steps: 106, steps per second: 139, episode reward: -30.846, mean reward: -0.291 [-100.000, 16.404], mean action: 1.689 [0.000, 3.000],  loss: 16.373200, mse: 1331.962400, mean_q: 19.495473, mean_eps: 0.783547
  36201/150000: episode: 379, duration: 0.519s, episode steps:  72, steps per second: 139, episode reward: -76.569, mean reward: -1.063 [-100.000,  8.195], mean action: 1.514 [0.000, 3.000],  loss: 10.315565, mse: 1333.226720, mean_q: 18.996035, mean_eps: 0.783013
  36296/150000: episode: 380, duration: 0.670s, episode steps:  95, steps per second: 142, episode reward: -97.713, mean reward: -1.029 [-100.000,  8.041], mean action: 1.389 [0.000, 3.000],  loss: 12.287117, mse: 1343.888944, mean_q: 19.509347, mean_eps: 0.782512
  36408/150000: episode: 381, duration: 0.836s, episode steps: 112, steps per second: 134, episode reward: -91.974, mean reward: -0.821 [-100.000,  6.449], mean action: 1.625 [0.000, 3.000],  loss: 10.888061, mse: 1306.912021, mean_q: 18.689149, mean_eps: 0.781891
  36538/150000: episode: 382, duration: 0.901s, episode steps: 130, steps per second: 144, episode reward: -74.113, mean reward: -0.570 [-100.000, 23.653], mean action: 1.600 [0.000, 3.000],  loss: 13.488506, mse: 1324.722829, mean_q: 18.358659, mean_eps: 0.781165
  36645/150000: episode: 383, duration: 0.847s, episode steps: 107, steps per second: 126, episode reward: -96.224, mean reward: -0.899 [-100.000, 11.987], mean action: 1.617 [0.000, 3.000],  loss: 13.277541, mse: 1330.876011, mean_q: 19.791961, mean_eps: 0.780454
  36756/150000: episode: 384, duration: 0.787s, episode steps: 111, steps per second: 141, episode reward: -103.594, mean reward: -0.933 [-100.000,  9.375], mean action: 1.459 [0.000, 3.000],  loss: 12.262687, mse: 1324.173563, mean_q: 18.946284, mean_eps: 0.779800
  36828/150000: episode: 385, duration: 0.563s, episode steps:  72, steps per second: 128, episode reward: -29.141, mean reward: -0.405 [-100.000, 11.712], mean action: 1.583 [0.000, 3.000],  loss: 11.512372, mse: 1371.291512, mean_q: 19.210699, mean_eps: 0.779251
  36936/150000: episode: 386, duration: 1.021s, episode steps: 108, steps per second: 106, episode reward: -98.043, mean reward: -0.908 [-100.000, 18.756], mean action: 1.472 [0.000, 3.000],  loss: 12.453634, mse: 1332.583548, mean_q: 19.816254, mean_eps: 0.778711
  37044/150000: episode: 387, duration: 0.920s, episode steps: 108, steps per second: 117, episode reward: -101.430, mean reward: -0.939 [-100.000,  6.622], mean action: 1.398 [0.000, 3.000],  loss: 19.418445, mse: 1359.871507, mean_q: 19.609301, mean_eps: 0.778063
  37156/150000: episode: 388, duration: 0.915s, episode steps: 112, steps per second: 122, episode reward: -104.798, mean reward: -0.936 [-100.000, 17.801], mean action: 1.696 [0.000, 3.000],  loss: 16.696185, mse: 1406.724343, mean_q: 20.997593, mean_eps: 0.777403
  37265/150000: episode: 389, duration: 0.864s, episode steps: 109, steps per second: 126, episode reward: -155.038, mean reward: -1.422 [-100.000,  9.401], mean action: 1.606 [0.000, 3.000],  loss: 11.819566, mse: 1440.483611, mean_q: 20.461010, mean_eps: 0.776740
  37341/150000: episode: 390, duration: 0.585s, episode steps:  76, steps per second: 130, episode reward: -63.142, mean reward: -0.831 [-100.000, 13.817], mean action: 1.526 [0.000, 3.000],  loss: 13.903503, mse: 1470.739386, mean_q: 20.432844, mean_eps: 0.776185
  37427/150000: episode: 391, duration: 0.634s, episode steps:  86, steps per second: 136, episode reward: -111.562, mean reward: -1.297 [-100.000,  9.311], mean action: 1.453 [0.000, 3.000],  loss: 13.247309, mse: 1412.184925, mean_q: 21.268120, mean_eps: 0.775699
  37556/150000: episode: 392, duration: 0.949s, episode steps: 129, steps per second: 136, episode reward: -70.385, mean reward: -0.546 [-100.000, 15.223], mean action: 1.527 [0.000, 3.000],  loss: 11.027758, mse: 1459.702042, mean_q: 20.227167, mean_eps: 0.775054
  37661/150000: episode: 393, duration: 0.753s, episode steps: 105, steps per second: 139, episode reward: -111.745, mean reward: -1.064 [-100.000, 11.097], mean action: 1.552 [0.000, 3.000],  loss: 15.350883, mse: 1447.610781, mean_q: 20.675131, mean_eps: 0.774352
  37759/150000: episode: 394, duration: 0.745s, episode steps:  98, steps per second: 131, episode reward: -100.690, mean reward: -1.027 [-100.000, 10.395], mean action: 1.316 [0.000, 3.000],  loss: 11.073730, mse: 1477.866124, mean_q: 20.962530, mean_eps: 0.773743
  37850/150000: episode: 395, duration: 0.654s, episode steps:  91, steps per second: 139, episode reward: -76.576, mean reward: -0.841 [-100.000, 19.491], mean action: 1.637 [0.000, 3.000],  loss: 15.476830, mse: 1416.115783, mean_q: 20.606272, mean_eps: 0.773176
  37922/150000: episode: 396, duration: 0.509s, episode steps:  72, steps per second: 141, episode reward: -54.057, mean reward: -0.751 [-100.000,  7.809], mean action: 1.597 [0.000, 3.000],  loss: 11.681189, mse: 1455.648034, mean_q: 20.600720, mean_eps: 0.772687
  37995/150000: episode: 397, duration: 0.518s, episode steps:  73, steps per second: 141, episode reward: -54.130, mean reward: -0.742 [-100.000, 17.302], mean action: 1.603 [0.000, 3.000],  loss: 22.407860, mse: 1475.039549, mean_q: 20.224713, mean_eps: 0.772252
  38082/150000: episode: 398, duration: 0.651s, episode steps:  87, steps per second: 134, episode reward: -81.682, mean reward: -0.939 [-100.000,  7.112], mean action: 1.563 [0.000, 3.000],  loss: 13.581720, mse: 1591.365624, mean_q: 22.716013, mean_eps: 0.771772
  38170/150000: episode: 399, duration: 0.627s, episode steps:  88, steps per second: 140, episode reward: -119.051, mean reward: -1.353 [-100.000,  6.089], mean action: 1.341 [0.000, 3.000],  loss: 11.687699, mse: 1576.932784, mean_q: 21.680471, mean_eps: 0.771247
  38238/150000: episode: 400, duration: 0.477s, episode steps:  68, steps per second: 142, episode reward: -62.663, mean reward: -0.922 [-100.000, 16.924], mean action: 1.588 [0.000, 3.000],  loss: 15.244147, mse: 1602.707853, mean_q: 21.878437, mean_eps: 0.770779
  38303/150000: episode: 401, duration: 0.515s, episode steps:  65, steps per second: 126, episode reward: -113.274, mean reward: -1.743 [-100.000,  6.737], mean action: 1.631 [0.000, 3.000],  loss: 15.784848, mse: 1557.588929, mean_q: 22.185579, mean_eps: 0.770380
  38368/150000: episode: 402, duration: 0.519s, episode steps:  65, steps per second: 125, episode reward: -71.308, mean reward: -1.097 [-100.000,  9.332], mean action: 1.615 [0.000, 3.000],  loss: 10.718464, mse: 1578.554622, mean_q: 22.945580, mean_eps: 0.769990
  38457/150000: episode: 403, duration: 0.663s, episode steps:  89, steps per second: 134, episode reward: -109.536, mean reward: -1.231 [-100.000,  6.101], mean action: 1.719 [0.000, 3.000],  loss: 11.922341, mse: 1616.560078, mean_q: 21.775254, mean_eps: 0.769528
  38539/150000: episode: 404, duration: 0.674s, episode steps:  82, steps per second: 122, episode reward: -88.523, mean reward: -1.080 [-100.000,  7.187], mean action: 1.549 [0.000, 3.000],  loss: 15.770393, mse: 1552.320563, mean_q: 21.326558, mean_eps: 0.769015
  38627/150000: episode: 405, duration: 0.887s, episode steps:  88, steps per second:  99, episode reward: -90.436, mean reward: -1.028 [-100.000,  7.967], mean action: 1.511 [0.000, 3.000],  loss: 10.263680, mse: 1569.301339, mean_q: 21.167798, mean_eps: 0.768505
  38724/150000: episode: 406, duration: 0.853s, episode steps:  97, steps per second: 114, episode reward: -110.069, mean reward: -1.135 [-100.000, 11.797], mean action: 1.701 [0.000, 3.000],  loss: 12.087390, mse: 1556.660716, mean_q: 21.493252, mean_eps: 0.767950
  38849/150000: episode: 407, duration: 1.186s, episode steps: 125, steps per second: 105, episode reward: -160.345, mean reward: -1.283 [-100.000,  9.171], mean action: 1.688 [0.000, 3.000],  loss: 10.266265, mse: 1574.878204, mean_q: 21.745440, mean_eps: 0.767284
  38927/150000: episode: 408, duration: 0.697s, episode steps:  78, steps per second: 112, episode reward: -80.985, mean reward: -1.038 [-100.000, 12.587], mean action: 1.628 [0.000, 3.000],  loss: 11.558307, mse: 1574.723824, mean_q: 21.719200, mean_eps: 0.766675
  39008/150000: episode: 409, duration: 0.613s, episode steps:  81, steps per second: 132, episode reward: -74.741, mean reward: -0.923 [-100.000, 11.351], mean action: 1.457 [0.000, 3.000],  loss: 12.811203, mse: 1592.695199, mean_q: 22.253069, mean_eps: 0.766198
  39089/150000: episode: 410, duration: 0.635s, episode steps:  81, steps per second: 128, episode reward: -134.032, mean reward: -1.655 [-100.000, 13.881], mean action: 1.494 [0.000, 3.000],  loss: 9.540876, mse: 1669.436710, mean_q: 23.452162, mean_eps: 0.765712
  39161/150000: episode: 411, duration: 0.637s, episode steps:  72, steps per second: 113, episode reward: -59.765, mean reward: -0.830 [-100.000,  9.533], mean action: 1.806 [0.000, 3.000],  loss: 10.236525, mse: 1654.218792, mean_q: 21.724377, mean_eps: 0.765253
  39306/150000: episode: 412, duration: 1.286s, episode steps: 145, steps per second: 113, episode reward: -117.983, mean reward: -0.814 [-100.000, 11.065], mean action: 1.524 [0.000, 3.000],  loss: 17.729678, mse: 1676.232605, mean_q: 22.866405, mean_eps: 0.764602
  39389/150000: episode: 413, duration: 0.684s, episode steps:  83, steps per second: 121, episode reward: -49.574, mean reward: -0.597 [-100.000, 16.600], mean action: 1.289 [0.000, 3.000],  loss: 11.971078, mse: 1645.109138, mean_q: 21.385075, mean_eps: 0.763918
  39452/150000: episode: 414, duration: 0.459s, episode steps:  63, steps per second: 137, episode reward: -53.887, mean reward: -0.855 [-100.000,  7.066], mean action: 1.778 [0.000, 3.000],  loss: 12.683074, mse: 1632.022717, mean_q: 23.230911, mean_eps: 0.763480
  39567/150000: episode: 415, duration: 0.836s, episode steps: 115, steps per second: 138, episode reward: -80.131, mean reward: -0.697 [-100.000,  7.849], mean action: 1.557 [0.000, 3.000],  loss: 12.021619, mse: 1704.351589, mean_q: 22.256131, mean_eps: 0.762946
  39655/150000: episode: 416, duration: 0.671s, episode steps:  88, steps per second: 131, episode reward: -105.154, mean reward: -1.195 [-100.000, 14.232], mean action: 1.489 [0.000, 3.000],  loss: 12.879708, mse: 1709.415279, mean_q: 22.318996, mean_eps: 0.762337
  39754/150000: episode: 417, duration: 0.775s, episode steps:  99, steps per second: 128, episode reward: 34.922, mean reward:  0.353 [-100.000, 17.724], mean action: 1.727 [0.000, 3.000],  loss: 15.847569, mse: 1668.087423, mean_q: 22.870905, mean_eps: 0.761776
  39832/150000: episode: 418, duration: 0.580s, episode steps:  78, steps per second: 134, episode reward: -88.141, mean reward: -1.130 [-100.000, 13.955], mean action: 1.385 [0.000, 3.000],  loss: 8.692573, mse: 1595.608389, mean_q: 22.320725, mean_eps: 0.761245
  39959/150000: episode: 419, duration: 0.973s, episode steps: 127, steps per second: 130, episode reward: -50.334, mean reward: -0.396 [-100.000, 13.905], mean action: 1.638 [0.000, 3.000],  loss: 11.125170, mse: 1634.597140, mean_q: 22.091004, mean_eps: 0.760630
  40063/150000: episode: 420, duration: 0.740s, episode steps: 104, steps per second: 141, episode reward: -160.897, mean reward: -1.547 [-100.000, 14.735], mean action: 1.750 [0.000, 3.000],  loss: 13.113262, mse: 1753.881467, mean_q: 23.160350, mean_eps: 0.759937
  40137/150000: episode: 421, duration: 0.545s, episode steps:  74, steps per second: 136, episode reward: -66.476, mean reward: -0.898 [-100.000, 13.565], mean action: 1.703 [0.000, 3.000],  loss: 12.129306, mse: 1736.716676, mean_q: 24.588688, mean_eps: 0.759403
  40227/150000: episode: 422, duration: 0.724s, episode steps:  90, steps per second: 124, episode reward: -53.690, mean reward: -0.597 [-100.000,  9.246], mean action: 1.611 [0.000, 3.000],  loss: 15.225942, mse: 1777.078141, mean_q: 23.420796, mean_eps: 0.758911
  40297/150000: episode: 423, duration: 0.503s, episode steps:  70, steps per second: 139, episode reward: -89.248, mean reward: -1.275 [-100.000, 29.786], mean action: 1.843 [0.000, 3.000],  loss: 9.860210, mse: 1792.574344, mean_q: 24.175443, mean_eps: 0.758431
  40417/150000: episode: 424, duration: 0.848s, episode steps: 120, steps per second: 142, episode reward: -59.672, mean reward: -0.497 [-100.000, 13.961], mean action: 1.550 [0.000, 3.000],  loss: 9.662309, mse: 1748.550832, mean_q: 24.015331, mean_eps: 0.757861
  40484/150000: episode: 425, duration: 0.532s, episode steps:  67, steps per second: 126, episode reward: -87.499, mean reward: -1.306 [-100.000,  8.176], mean action: 1.403 [0.000, 3.000],  loss: 13.636841, mse: 1777.061476, mean_q: 24.260308, mean_eps: 0.757300
  40562/150000: episode: 426, duration: 0.580s, episode steps:  78, steps per second: 134, episode reward: -83.959, mean reward: -1.076 [-100.000, 10.769], mean action: 1.615 [0.000, 3.000],  loss: 19.047760, mse: 1802.435907, mean_q: 24.237548, mean_eps: 0.756865
  40667/150000: episode: 427, duration: 0.741s, episode steps: 105, steps per second: 142, episode reward: -172.926, mean reward: -1.647 [-100.000,  2.377], mean action: 1.619 [0.000, 3.000],  loss: 12.848075, mse: 1782.192892, mean_q: 24.400019, mean_eps: 0.756316
  40747/150000: episode: 428, duration: 0.571s, episode steps:  80, steps per second: 140, episode reward: -230.358, mean reward: -2.879 [-100.000,  6.970], mean action: 1.587 [0.000, 3.000],  loss: 11.205440, mse: 1828.142003, mean_q: 24.306621, mean_eps: 0.755761
  40827/150000: episode: 429, duration: 0.635s, episode steps:  80, steps per second: 126, episode reward: -111.407, mean reward: -1.393 [-100.000, 25.066], mean action: 1.812 [0.000, 3.000],  loss: 12.989735, mse: 1784.569119, mean_q: 23.744325, mean_eps: 0.755281
  40923/150000: episode: 430, duration: 0.677s, episode steps:  96, steps per second: 142, episode reward: -73.029, mean reward: -0.761 [-100.000,  6.874], mean action: 1.510 [0.000, 3.000],  loss: 14.205455, mse: 1790.184045, mean_q: 23.679096, mean_eps: 0.754753
  40994/150000: episode: 431, duration: 0.500s, episode steps:  71, steps per second: 142, episode reward: -34.528, mean reward: -0.486 [-100.000,  8.402], mean action: 1.732 [0.000, 3.000],  loss: 7.199909, mse: 1753.574900, mean_q: 25.327194, mean_eps: 0.754252
  41095/150000: episode: 432, duration: 0.771s, episode steps: 101, steps per second: 131, episode reward: -73.679, mean reward: -0.729 [-100.000,  8.221], mean action: 1.564 [0.000, 3.000],  loss: 13.507203, mse: 1846.744022, mean_q: 24.493198, mean_eps: 0.753736
  41235/150000: episode: 433, duration: 1.014s, episode steps: 140, steps per second: 138, episode reward: -40.572, mean reward: -0.290 [-100.000,  8.377], mean action: 1.579 [0.000, 3.000],  loss: 12.469217, mse: 1899.234165, mean_q: 24.734171, mean_eps: 0.753013
  41315/150000: episode: 434, duration: 0.553s, episode steps:  80, steps per second: 145, episode reward: -112.868, mean reward: -1.411 [-100.000,  7.675], mean action: 1.525 [0.000, 3.000],  loss: 13.020751, mse: 1812.625366, mean_q: 24.837589, mean_eps: 0.752353
  41408/150000: episode: 435, duration: 0.711s, episode steps:  93, steps per second: 131, episode reward: -89.882, mean reward: -0.966 [-100.000, 12.035], mean action: 1.473 [0.000, 3.000],  loss: 14.295421, mse: 1876.185094, mean_q: 24.280400, mean_eps: 0.751834
  41485/150000: episode: 436, duration: 0.563s, episode steps:  77, steps per second: 137, episode reward: -99.330, mean reward: -1.290 [-100.000,  9.738], mean action: 1.558 [0.000, 3.000],  loss: 11.607441, mse: 1741.376259, mean_q: 24.961945, mean_eps: 0.751324
  41623/150000: episode: 437, duration: 0.988s, episode steps: 138, steps per second: 140, episode reward: -68.670, mean reward: -0.498 [-100.000, 13.318], mean action: 1.486 [0.000, 3.000],  loss: 11.809825, mse: 1863.720908, mean_q: 24.372385, mean_eps: 0.750679
  41731/150000: episode: 438, duration: 0.809s, episode steps: 108, steps per second: 134, episode reward: -83.234, mean reward: -0.771 [-100.000,  9.790], mean action: 1.435 [0.000, 3.000],  loss: 9.403729, mse: 1828.913602, mean_q: 25.389108, mean_eps: 0.749941
  41826/150000: episode: 439, duration: 0.683s, episode steps:  95, steps per second: 139, episode reward: -98.326, mean reward: -1.035 [-100.000, 32.504], mean action: 1.632 [0.000, 3.000],  loss: 11.564233, mse: 1848.157639, mean_q: 24.591635, mean_eps: 0.749332
  41931/150000: episode: 440, duration: 0.752s, episode steps: 105, steps per second: 140, episode reward: -91.636, mean reward: -0.873 [-100.000, 11.414], mean action: 1.495 [0.000, 3.000],  loss: 8.893507, mse: 1833.537919, mean_q: 24.950354, mean_eps: 0.748732
  42019/150000: episode: 441, duration: 0.692s, episode steps:  88, steps per second: 127, episode reward: -8.111, mean reward: -0.092 [-100.000, 22.228], mean action: 1.477 [0.000, 3.000],  loss: 13.553854, mse: 1833.977276, mean_q: 24.730676, mean_eps: 0.748153
  42156/150000: episode: 442, duration: 0.995s, episode steps: 137, steps per second: 138, episode reward: -52.323, mean reward: -0.382 [-100.000, 24.540], mean action: 1.547 [0.000, 3.000],  loss: 8.848234, mse: 1867.587804, mean_q: 25.235594, mean_eps: 0.747478
  42277/150000: episode: 443, duration: 0.929s, episode steps: 121, steps per second: 130, episode reward: -74.938, mean reward: -0.619 [-100.000, 10.729], mean action: 1.463 [0.000, 3.000],  loss: 12.831631, mse: 1872.131721, mean_q: 26.052102, mean_eps: 0.746704
  42351/150000: episode: 444, duration: 0.609s, episode steps:  74, steps per second: 122, episode reward: -34.454, mean reward: -0.466 [-100.000,  7.216], mean action: 1.716 [0.000, 3.000],  loss: 9.411703, mse: 1883.639751, mean_q: 25.481584, mean_eps: 0.746119
  42469/150000: episode: 445, duration: 0.867s, episode steps: 118, steps per second: 136, episode reward: -72.827, mean reward: -0.617 [-100.000, 16.129], mean action: 1.407 [0.000, 3.000],  loss: 16.180305, mse: 1854.518165, mean_q: 24.669439, mean_eps: 0.745543
  42543/150000: episode: 446, duration: 0.549s, episode steps:  74, steps per second: 135, episode reward: -81.377, mean reward: -1.100 [-100.000,  7.830], mean action: 1.568 [0.000, 3.000],  loss: 10.732970, mse: 1905.697457, mean_q: 25.379700, mean_eps: 0.744967
  42648/150000: episode: 447, duration: 0.779s, episode steps: 105, steps per second: 135, episode reward: -143.377, mean reward: -1.365 [-100.000,  6.605], mean action: 1.533 [0.000, 3.000],  loss: 13.169004, mse: 1874.578716, mean_q: 25.970511, mean_eps: 0.744430
  42713/150000: episode: 448, duration: 0.494s, episode steps:  65, steps per second: 132, episode reward: -48.495, mean reward: -0.746 [-100.000,  7.074], mean action: 1.600 [0.000, 3.000],  loss: 19.972856, mse: 1866.641955, mean_q: 24.509475, mean_eps: 0.743920
  42789/150000: episode: 449, duration: 0.554s, episode steps:  76, steps per second: 137, episode reward: -69.389, mean reward: -0.913 [-100.000,  6.854], mean action: 1.461 [0.000, 3.000],  loss: 11.304939, mse: 1856.137114, mean_q: 25.297546, mean_eps: 0.743497
  42912/150000: episode: 450, duration: 0.937s, episode steps: 123, steps per second: 131, episode reward: -80.597, mean reward: -0.655 [-100.000,  7.465], mean action: 1.561 [0.000, 3.000],  loss: 15.672764, mse: 1819.744304, mean_q: 25.671460, mean_eps: 0.742900
  43021/150000: episode: 451, duration: 0.795s, episode steps: 109, steps per second: 137, episode reward: -53.164, mean reward: -0.488 [-100.000, 12.304], mean action: 1.560 [0.000, 3.000],  loss: 13.294260, mse: 1915.491695, mean_q: 26.105391, mean_eps: 0.742204
  43124/150000: episode: 452, duration: 0.743s, episode steps: 103, steps per second: 139, episode reward: -75.890, mean reward: -0.737 [-100.000,  7.260], mean action: 1.563 [0.000, 3.000],  loss: 8.097338, mse: 1995.980944, mean_q: 27.147452, mean_eps: 0.741568
  43223/150000: episode: 453, duration: 0.737s, episode steps:  99, steps per second: 134, episode reward: -21.269, mean reward: -0.215 [-100.000, 24.444], mean action: 1.556 [0.000, 3.000],  loss: 13.228622, mse: 2032.974471, mean_q: 25.935130, mean_eps: 0.740962
  43299/150000: episode: 454, duration: 0.564s, episode steps:  76, steps per second: 135, episode reward: -22.868, mean reward: -0.301 [-100.000, 43.863], mean action: 1.316 [0.000, 3.000],  loss: 10.616754, mse: 2054.367453, mean_q: 25.945343, mean_eps: 0.740437
  43424/150000: episode: 455, duration: 0.936s, episode steps: 125, steps per second: 134, episode reward: -74.043, mean reward: -0.592 [-100.000,  7.440], mean action: 1.592 [0.000, 3.000],  loss: 15.204234, mse: 2025.563982, mean_q: 26.605737, mean_eps: 0.739834
  43504/150000: episode: 456, duration: 0.625s, episode steps:  80, steps per second: 128, episode reward: -139.903, mean reward: -1.749 [-100.000, 21.660], mean action: 1.650 [0.000, 3.000],  loss: 8.247894, mse: 1995.579019, mean_q: 26.475113, mean_eps: 0.739219
  43609/150000: episode: 457, duration: 0.847s, episode steps: 105, steps per second: 124, episode reward: -149.391, mean reward: -1.423 [-100.000,  2.495], mean action: 1.619 [0.000, 3.000],  loss: 8.945436, mse: 2022.493610, mean_q: 26.317289, mean_eps: 0.738664
  43760/150000: episode: 458, duration: 1.244s, episode steps: 151, steps per second: 121, episode reward: -278.743, mean reward: -1.846 [-100.000, 82.552], mean action: 1.483 [0.000, 3.000],  loss: 11.341177, mse: 2038.408520, mean_q: 25.856924, mean_eps: 0.737896
  43874/150000: episode: 459, duration: 0.861s, episode steps: 114, steps per second: 132, episode reward: -65.838, mean reward: -0.578 [-100.000, 14.983], mean action: 1.570 [0.000, 3.000],  loss: 13.675719, mse: 2050.816968, mean_q: 26.089699, mean_eps: 0.737101
  43937/150000: episode: 460, duration: 0.491s, episode steps:  63, steps per second: 128, episode reward: -73.707, mean reward: -1.170 [-100.000,  7.364], mean action: 1.556 [0.000, 3.000],  loss: 10.723011, mse: 2005.860877, mean_q: 26.038349, mean_eps: 0.736570
  44064/150000: episode: 461, duration: 0.974s, episode steps: 127, steps per second: 130, episode reward: -59.524, mean reward: -0.469 [-100.000, 13.934], mean action: 1.449 [0.000, 3.000],  loss: 12.587574, mse: 2057.617241, mean_q: 26.973248, mean_eps: 0.736000
  44164/150000: episode: 462, duration: 0.730s, episode steps: 100, steps per second: 137, episode reward: -64.024, mean reward: -0.640 [-100.000, 18.396], mean action: 1.600 [0.000, 3.000],  loss: 7.591869, mse: 2098.814564, mean_q: 28.184503, mean_eps: 0.735319
  44240/150000: episode: 463, duration: 0.577s, episode steps:  76, steps per second: 132, episode reward: -31.267, mean reward: -0.411 [-100.000,  8.121], mean action: 1.658 [0.000, 3.000],  loss: 15.719460, mse: 2145.997114, mean_q: 28.013362, mean_eps: 0.734791
  44330/150000: episode: 464, duration: 0.666s, episode steps:  90, steps per second: 135, episode reward: -56.633, mean reward: -0.629 [-100.000, 12.644], mean action: 1.689 [0.000, 3.000],  loss: 16.290538, mse: 2148.451697, mean_q: 27.967302, mean_eps: 0.734293
  44407/150000: episode: 465, duration: 0.567s, episode steps:  77, steps per second: 136, episode reward: -24.309, mean reward: -0.316 [-100.000, 16.322], mean action: 1.610 [0.000, 3.000],  loss: 10.145042, mse: 2042.007069, mean_q: 28.590923, mean_eps: 0.733792
  44480/150000: episode: 466, duration: 0.567s, episode steps:  73, steps per second: 129, episode reward: -16.586, mean reward: -0.227 [-100.000, 14.619], mean action: 1.644 [0.000, 3.000],  loss: 9.534900, mse: 2101.385429, mean_q: 28.043188, mean_eps: 0.733342
  44545/150000: episode: 467, duration: 0.608s, episode steps:  65, steps per second: 107, episode reward: -63.413, mean reward: -0.976 [-100.000,  6.636], mean action: 1.754 [0.000, 3.000],  loss: 8.865196, mse: 2125.675916, mean_q: 29.052973, mean_eps: 0.732928
  44667/150000: episode: 468, duration: 0.930s, episode steps: 122, steps per second: 131, episode reward: -118.132, mean reward: -0.968 [-100.000, 11.617], mean action: 1.615 [0.000, 3.000],  loss: 10.559177, mse: 2064.492160, mean_q: 28.660096, mean_eps: 0.732367
  44803/150000: episode: 469, duration: 1.020s, episode steps: 136, steps per second: 133, episode reward: -27.193, mean reward: -0.200 [-100.000, 75.817], mean action: 1.574 [0.000, 3.000],  loss: 10.855066, mse: 2091.448787, mean_q: 29.209315, mean_eps: 0.731593
  44924/150000: episode: 470, duration: 0.896s, episode steps: 121, steps per second: 135, episode reward: 34.082, mean reward:  0.282 [-100.000, 63.193], mean action: 1.603 [0.000, 3.000],  loss: 14.297434, mse: 2090.587777, mean_q: 28.280540, mean_eps: 0.730822
  45011/150000: episode: 471, duration: 0.634s, episode steps:  87, steps per second: 137, episode reward: -130.736, mean reward: -1.503 [-100.000, 30.360], mean action: 1.586 [0.000, 3.000],  loss: 11.490190, mse: 2079.356847, mean_q: 28.764861, mean_eps: 0.730198
  45105/150000: episode: 472, duration: 0.710s, episode steps:  94, steps per second: 132, episode reward: 15.500, mean reward:  0.165 [-100.000, 15.831], mean action: 1.596 [0.000, 3.000],  loss: 11.503204, mse: 2148.791830, mean_q: 27.752316, mean_eps: 0.729655
  45183/150000: episode: 473, duration: 0.614s, episode steps:  78, steps per second: 127, episode reward: -77.506, mean reward: -0.994 [-100.000, 11.795], mean action: 1.449 [0.000, 3.000],  loss: 10.814112, mse: 2173.830530, mean_q: 27.163747, mean_eps: 0.729139
  45281/150000: episode: 474, duration: 0.714s, episode steps:  98, steps per second: 137, episode reward: -76.713, mean reward: -0.783 [-100.000, 11.384], mean action: 1.673 [0.000, 3.000],  loss: 13.679660, mse: 2216.676347, mean_q: 26.308682, mean_eps: 0.728611
  45362/150000: episode: 475, duration: 0.612s, episode steps:  81, steps per second: 132, episode reward: -85.248, mean reward: -1.052 [-100.000,  6.693], mean action: 1.346 [0.000, 3.000],  loss: 9.979358, mse: 2150.041899, mean_q: 27.319782, mean_eps: 0.728074
  45445/150000: episode: 476, duration: 0.790s, episode steps:  83, steps per second: 105, episode reward: -81.472, mean reward: -0.982 [-100.000, 13.669], mean action: 1.325 [0.000, 3.000],  loss: 9.647868, mse: 2225.445901, mean_q: 26.888095, mean_eps: 0.727582
  45575/150000: episode: 477, duration: 1.156s, episode steps: 130, steps per second: 112, episode reward: -152.761, mean reward: -1.175 [-100.000,  5.697], mean action: 1.469 [0.000, 3.000],  loss: 13.828937, mse: 2223.258586, mean_q: 27.897866, mean_eps: 0.726943
  45657/150000: episode: 478, duration: 0.865s, episode steps:  82, steps per second:  95, episode reward: -74.675, mean reward: -0.911 [-100.000,  6.714], mean action: 1.561 [0.000, 3.000],  loss: 14.970724, mse: 2182.819156, mean_q: 26.497505, mean_eps: 0.726307
  45792/150000: episode: 479, duration: 1.305s, episode steps: 135, steps per second: 103, episode reward: -101.490, mean reward: -0.752 [-100.000,  7.032], mean action: 1.570 [0.000, 3.000],  loss: 14.021807, mse: 2147.052801, mean_q: 26.897486, mean_eps: 0.725656
  45868/150000: episode: 480, duration: 0.700s, episode steps:  76, steps per second: 109, episode reward: -94.650, mean reward: -1.245 [-100.000,  5.483], mean action: 1.355 [0.000, 3.000],  loss: 11.181303, mse: 2216.544120, mean_q: 28.010101, mean_eps: 0.725023
  45980/150000: episode: 481, duration: 0.951s, episode steps: 112, steps per second: 118, episode reward: -110.669, mean reward: -0.988 [-100.000, 12.117], mean action: 1.402 [0.000, 3.000],  loss: 13.406233, mse: 2219.544171, mean_q: 28.003511, mean_eps: 0.724459
  46082/150000: episode: 482, duration: 0.779s, episode steps: 102, steps per second: 131, episode reward: -111.156, mean reward: -1.090 [-100.000,  9.639], mean action: 1.441 [0.000, 3.000],  loss: 12.789837, mse: 2237.817946, mean_q: 27.377555, mean_eps: 0.723817
  46180/150000: episode: 483, duration: 0.755s, episode steps:  98, steps per second: 130, episode reward: -74.826, mean reward: -0.764 [-100.000, 13.015], mean action: 1.561 [0.000, 3.000],  loss: 11.781311, mse: 2249.492266, mean_q: 28.551124, mean_eps: 0.723217
  46287/150000: episode: 484, duration: 0.797s, episode steps: 107, steps per second: 134, episode reward: -58.086, mean reward: -0.543 [-100.000, 13.429], mean action: 1.486 [0.000, 3.000],  loss: 13.992623, mse: 2242.059778, mean_q: 28.084886, mean_eps: 0.722602
  46386/150000: episode: 485, duration: 0.725s, episode steps:  99, steps per second: 137, episode reward: -99.689, mean reward: -1.007 [-100.000,  7.063], mean action: 1.687 [0.000, 3.000],  loss: 11.900793, mse: 2239.317957, mean_q: 27.966523, mean_eps: 0.721984
  46459/150000: episode: 486, duration: 0.542s, episode steps:  73, steps per second: 135, episode reward: -41.076, mean reward: -0.563 [-100.000, 11.659], mean action: 1.658 [0.000, 3.000],  loss: 13.300592, mse: 2197.776004, mean_q: 27.209996, mean_eps: 0.721468
  46540/150000: episode: 487, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -58.278, mean reward: -0.719 [-100.000, 15.400], mean action: 1.556 [0.000, 3.000],  loss: 12.078886, mse: 2238.050186, mean_q: 28.408058, mean_eps: 0.721006
  46637/150000: episode: 488, duration: 0.734s, episode steps:  97, steps per second: 132, episode reward: -89.944, mean reward: -0.927 [-100.000, 11.321], mean action: 1.474 [0.000, 3.000],  loss: 10.654509, mse: 2231.770333, mean_q: 27.949871, mean_eps: 0.720472
  46756/150000: episode: 489, duration: 0.921s, episode steps: 119, steps per second: 129, episode reward: -253.065, mean reward: -2.127 [-100.000, 98.601], mean action: 1.597 [0.000, 3.000],  loss: 11.405005, mse: 2233.455494, mean_q: 27.029673, mean_eps: 0.719824
  46859/150000: episode: 490, duration: 0.744s, episode steps: 103, steps per second: 139, episode reward: -82.522, mean reward: -0.801 [-100.000,  6.302], mean action: 1.573 [0.000, 3.000],  loss: 13.384540, mse: 2266.928681, mean_q: 27.431464, mean_eps: 0.719158
  46956/150000: episode: 491, duration: 0.725s, episode steps:  97, steps per second: 134, episode reward: -58.432, mean reward: -0.602 [-100.000, 10.015], mean action: 1.598 [0.000, 3.000],  loss: 10.903392, mse: 2220.270461, mean_q: 27.092961, mean_eps: 0.718558
  47017/150000: episode: 492, duration: 0.495s, episode steps:  61, steps per second: 123, episode reward: -43.591, mean reward: -0.715 [-100.000,  7.512], mean action: 1.738 [0.000, 3.000],  loss: 13.214946, mse: 2268.130541, mean_q: 26.255113, mean_eps: 0.718084
  47140/150000: episode: 493, duration: 0.906s, episode steps: 123, steps per second: 136, episode reward: -74.997, mean reward: -0.610 [-100.000, 17.974], mean action: 1.439 [0.000, 3.000],  loss: 13.284284, mse: 2284.341524, mean_q: 29.592234, mean_eps: 0.717532
  47258/150000: episode: 494, duration: 0.882s, episode steps: 118, steps per second: 134, episode reward: -87.122, mean reward: -0.738 [-100.000, 11.254], mean action: 1.475 [0.000, 3.000],  loss: 16.167704, mse: 2198.361164, mean_q: 28.251889, mean_eps: 0.716809
  47349/150000: episode: 495, duration: 0.684s, episode steps:  91, steps per second: 133, episode reward: -80.992, mean reward: -0.890 [-100.000,  8.215], mean action: 1.495 [0.000, 3.000],  loss: 11.307205, mse: 2254.726997, mean_q: 28.585298, mean_eps: 0.716182
  47466/150000: episode: 496, duration: 0.874s, episode steps: 117, steps per second: 134, episode reward: -104.864, mean reward: -0.896 [-100.000, 10.074], mean action: 1.684 [0.000, 3.000],  loss: 16.378684, mse: 2268.829641, mean_q: 28.155677, mean_eps: 0.715558
  47560/150000: episode: 497, duration: 0.717s, episode steps:  94, steps per second: 131, episode reward: -82.264, mean reward: -0.875 [-100.000, 12.483], mean action: 1.681 [0.000, 3.000],  loss: 11.358435, mse: 2276.394717, mean_q: 27.945958, mean_eps: 0.714925
  47647/150000: episode: 498, duration: 0.640s, episode steps:  87, steps per second: 136, episode reward: -95.153, mean reward: -1.094 [-100.000, 16.184], mean action: 1.379 [0.000, 3.000],  loss: 11.602766, mse: 2272.060620, mean_q: 29.113356, mean_eps: 0.714382
  47741/150000: episode: 499, duration: 0.671s, episode steps:  94, steps per second: 140, episode reward: -92.166, mean reward: -0.980 [-100.000,  7.807], mean action: 1.468 [0.000, 3.000],  loss: 18.949919, mse: 2261.042913, mean_q: 28.633044, mean_eps: 0.713839
  47844/150000: episode: 500, duration: 0.779s, episode steps: 103, steps per second: 132, episode reward: -137.705, mean reward: -1.337 [-100.000,  5.173], mean action: 1.573 [0.000, 3.000],  loss: 15.766027, mse: 2257.061027, mean_q: 28.890922, mean_eps: 0.713248
  47944/150000: episode: 501, duration: 0.768s, episode steps: 100, steps per second: 130, episode reward: -83.415, mean reward: -0.834 [-100.000, 13.259], mean action: 1.610 [0.000, 3.000],  loss: 9.898980, mse: 2295.236650, mean_q: 29.743159, mean_eps: 0.712639
  48054/150000: episode: 502, duration: 0.802s, episode steps: 110, steps per second: 137, episode reward: -13.530, mean reward: -0.123 [-100.000, 13.628], mean action: 1.591 [0.000, 3.000],  loss: 13.462468, mse: 2348.206233, mean_q: 30.268951, mean_eps: 0.712009
  48190/150000: episode: 503, duration: 1.036s, episode steps: 136, steps per second: 131, episode reward: -63.930, mean reward: -0.470 [-100.000,  7.568], mean action: 1.640 [0.000, 3.000],  loss: 15.345452, mse: 2357.182833, mean_q: 29.838479, mean_eps: 0.711271
  48285/150000: episode: 504, duration: 0.743s, episode steps:  95, steps per second: 128, episode reward: -95.854, mean reward: -1.009 [-100.000, 19.039], mean action: 1.695 [0.000, 3.000],  loss: 12.263166, mse: 2315.559050, mean_q: 30.013530, mean_eps: 0.710578
  48377/150000: episode: 505, duration: 0.665s, episode steps:  92, steps per second: 138, episode reward: -92.219, mean reward: -1.002 [-100.000,  8.509], mean action: 1.522 [0.000, 3.000],  loss: 11.672252, mse: 2336.001165, mean_q: 30.436961, mean_eps: 0.710017
  48502/150000: episode: 506, duration: 0.956s, episode steps: 125, steps per second: 131, episode reward: -142.478, mean reward: -1.140 [-100.000,  8.783], mean action: 1.536 [0.000, 3.000],  loss: 15.380515, mse: 2350.337720, mean_q: 30.565249, mean_eps: 0.709366
  48625/150000: episode: 507, duration: 0.920s, episode steps: 123, steps per second: 134, episode reward: -133.951, mean reward: -1.089 [-100.000,  3.783], mean action: 1.667 [0.000, 3.000],  loss: 16.670741, mse: 2335.675691, mean_q: 29.772273, mean_eps: 0.708622
  48725/150000: episode: 508, duration: 0.771s, episode steps: 100, steps per second: 130, episode reward: -108.032, mean reward: -1.080 [-100.000, 24.181], mean action: 1.570 [0.000, 3.000],  loss: 12.144310, mse: 2337.353163, mean_q: 29.040345, mean_eps: 0.707953
  48868/150000: episode: 509, duration: 1.048s, episode steps: 143, steps per second: 136, episode reward: -92.609, mean reward: -0.648 [-100.000, 10.137], mean action: 1.559 [0.000, 3.000],  loss: 14.678213, mse: 2263.549517, mean_q: 29.710338, mean_eps: 0.707224
  48996/150000: episode: 510, duration: 0.939s, episode steps: 128, steps per second: 136, episode reward: -39.242, mean reward: -0.307 [-100.000, 17.186], mean action: 1.664 [0.000, 3.000],  loss: 12.538607, mse: 2305.815527, mean_q: 29.971657, mean_eps: 0.706411
  49063/150000: episode: 511, duration: 0.538s, episode steps:  67, steps per second: 125, episode reward: -55.005, mean reward: -0.821 [-100.000,  8.144], mean action: 1.672 [0.000, 3.000],  loss: 10.875838, mse: 2322.931358, mean_q: 28.918155, mean_eps: 0.705826
  49139/150000: episode: 512, duration: 0.555s, episode steps:  76, steps per second: 137, episode reward: -48.728, mean reward: -0.641 [-100.000, 18.283], mean action: 1.697 [0.000, 3.000],  loss: 16.639570, mse: 2323.665977, mean_q: 29.098882, mean_eps: 0.705397
  49259/150000: episode: 513, duration: 0.854s, episode steps: 120, steps per second: 141, episode reward: -97.104, mean reward: -0.809 [-100.000, 11.600], mean action: 1.583 [0.000, 3.000],  loss: 16.686878, mse: 2365.351053, mean_q: 28.698930, mean_eps: 0.704809
  49350/150000: episode: 514, duration: 0.738s, episode steps:  91, steps per second: 123, episode reward: -109.870, mean reward: -1.207 [-100.000,  4.358], mean action: 1.604 [0.000, 3.000],  loss: 9.493857, mse: 2341.437513, mean_q: 27.773407, mean_eps: 0.704176
  49426/150000: episode: 515, duration: 0.567s, episode steps:  76, steps per second: 134, episode reward: -46.945, mean reward: -0.618 [-100.000, 10.385], mean action: 1.539 [0.000, 3.000],  loss: 17.991582, mse: 2294.231927, mean_q: 28.333128, mean_eps: 0.703675
  49499/150000: episode: 516, duration: 0.525s, episode steps:  73, steps per second: 139, episode reward: -48.993, mean reward: -0.671 [-100.000,  7.038], mean action: 1.589 [0.000, 3.000],  loss: 11.702859, mse: 2342.815304, mean_q: 29.578488, mean_eps: 0.703228
  49582/150000: episode: 517, duration: 0.648s, episode steps:  83, steps per second: 128, episode reward: 23.628, mean reward:  0.285 [-100.000, 62.693], mean action: 1.627 [0.000, 3.000],  loss: 18.590251, mse: 2396.744975, mean_q: 28.769386, mean_eps: 0.702760
  49673/150000: episode: 518, duration: 0.688s, episode steps:  91, steps per second: 132, episode reward: -54.373, mean reward: -0.598 [-100.000,  6.491], mean action: 1.582 [0.000, 3.000],  loss: 16.132164, mse: 2373.106569, mean_q: 29.875220, mean_eps: 0.702238
  49768/150000: episode: 519, duration: 0.687s, episode steps:  95, steps per second: 138, episode reward: -82.958, mean reward: -0.873 [-100.000,  9.682], mean action: 1.537 [0.000, 3.000],  loss: 7.735876, mse: 2380.260014, mean_q: 29.805088, mean_eps: 0.701680
  49877/150000: episode: 520, duration: 0.827s, episode steps: 109, steps per second: 132, episode reward: -96.540, mean reward: -0.886 [-100.000, 31.337], mean action: 1.550 [0.000, 3.000],  loss: 14.137556, mse: 2312.788201, mean_q: 29.565388, mean_eps: 0.701068
  49943/150000: episode: 521, duration: 0.509s, episode steps:  66, steps per second: 130, episode reward: -110.318, mean reward: -1.671 [-100.000, 10.189], mean action: 1.667 [0.000, 3.000],  loss: 10.161018, mse: 2351.143155, mean_q: 29.811763, mean_eps: 0.700543
  50040/150000: episode: 522, duration: 0.710s, episode steps:  97, steps per second: 137, episode reward: -68.100, mean reward: -0.702 [-100.000, 12.698], mean action: 1.670 [0.000, 3.000],  loss: 16.502298, mse: 2403.250016, mean_q: 30.651627, mean_eps: 0.700054
  50130/150000: episode: 523, duration: 0.688s, episode steps:  90, steps per second: 131, episode reward: -89.590, mean reward: -0.995 [-100.000, 11.492], mean action: 1.500 [0.000, 3.000],  loss: 11.264439, mse: 2444.361637, mean_q: 29.835421, mean_eps: 0.699493
  50222/150000: episode: 524, duration: 0.729s, episode steps:  92, steps per second: 126, episode reward: -74.218, mean reward: -0.807 [-100.000, 10.644], mean action: 1.598 [0.000, 3.000],  loss: 11.747125, mse: 2415.083987, mean_q: 29.108821, mean_eps: 0.698947
  50316/150000: episode: 525, duration: 0.696s, episode steps:  94, steps per second: 135, episode reward: -85.532, mean reward: -0.910 [-100.000, 20.863], mean action: 1.553 [0.000, 3.000],  loss: 7.479699, mse: 2453.797433, mean_q: 30.569947, mean_eps: 0.698389
  50447/150000: episode: 526, duration: 0.978s, episode steps: 131, steps per second: 134, episode reward: -37.599, mean reward: -0.287 [-100.000,  9.137], mean action: 1.702 [0.000, 3.000],  loss: 14.380618, mse: 2443.768371, mean_q: 29.725348, mean_eps: 0.697714
  50571/150000: episode: 527, duration: 0.934s, episode steps: 124, steps per second: 133, episode reward: -81.162, mean reward: -0.655 [-100.000,  9.805], mean action: 1.661 [0.000, 3.000],  loss: 13.692724, mse: 2410.002818, mean_q: 29.695466, mean_eps: 0.696949
  50645/150000: episode: 528, duration: 0.562s, episode steps:  74, steps per second: 132, episode reward: -67.799, mean reward: -0.916 [-100.000, 16.681], mean action: 1.608 [0.000, 3.000],  loss: 13.672502, mse: 2377.040316, mean_q: 28.661712, mean_eps: 0.696355
  50737/150000: episode: 529, duration: 0.696s, episode steps:  92, steps per second: 132, episode reward: -89.107, mean reward: -0.969 [-100.000,  7.965], mean action: 1.641 [0.000, 3.000],  loss: 17.815634, mse: 2385.897502, mean_q: 30.126257, mean_eps: 0.695857
  50994/150000: episode: 530, duration: 1.946s, episode steps: 257, steps per second: 132, episode reward:  8.444, mean reward:  0.033 [-100.000, 82.733], mean action: 1.584 [0.000, 3.000],  loss: 13.789111, mse: 2463.564636, mean_q: 30.502574, mean_eps: 0.694810
  51093/150000: episode: 531, duration: 0.784s, episode steps:  99, steps per second: 126, episode reward: -222.612, mean reward: -2.249 [-100.000, 94.168], mean action: 1.576 [0.000, 3.000],  loss: 12.240727, mse: 2475.301522, mean_q: 30.721423, mean_eps: 0.693742
  51168/150000: episode: 532, duration: 0.703s, episode steps:  75, steps per second: 107, episode reward: -56.546, mean reward: -0.754 [-100.000,  7.569], mean action: 1.533 [0.000, 3.000],  loss: 15.586121, mse: 2520.741548, mean_q: 30.809448, mean_eps: 0.693220
  51263/150000: episode: 533, duration: 0.851s, episode steps:  95, steps per second: 112, episode reward: -84.647, mean reward: -0.891 [-100.000, 10.515], mean action: 1.274 [0.000, 3.000],  loss: 15.375125, mse: 2550.260355, mean_q: 31.075180, mean_eps: 0.692710
  51387/150000: episode: 534, duration: 0.948s, episode steps: 124, steps per second: 131, episode reward: -74.320, mean reward: -0.599 [-100.000,  7.442], mean action: 1.524 [0.000, 3.000],  loss: 13.574942, mse: 2478.156723, mean_q: 30.020046, mean_eps: 0.692053
  51473/150000: episode: 535, duration: 0.641s, episode steps:  86, steps per second: 134, episode reward: -66.706, mean reward: -0.776 [-100.000,  7.307], mean action: 1.663 [0.000, 3.000],  loss: 13.179029, mse: 2497.970892, mean_q: 30.134785, mean_eps: 0.691423
  51545/150000: episode: 536, duration: 0.559s, episode steps:  72, steps per second: 129, episode reward: -98.169, mean reward: -1.363 [-100.000,  5.996], mean action: 1.694 [0.000, 3.000],  loss: 12.710584, mse: 2491.068122, mean_q: 30.224082, mean_eps: 0.690949
  51673/150000: episode: 537, duration: 0.944s, episode steps: 128, steps per second: 136, episode reward: -112.533, mean reward: -0.879 [-100.000,  6.030], mean action: 1.469 [0.000, 3.000],  loss: 16.100547, mse: 2556.323962, mean_q: 30.922232, mean_eps: 0.690349
  51737/150000: episode: 538, duration: 0.479s, episode steps:  64, steps per second: 134, episode reward: -58.601, mean reward: -0.916 [-100.000, 46.670], mean action: 1.469 [0.000, 3.000],  loss: 13.416857, mse: 2546.941257, mean_q: 29.667797, mean_eps: 0.689773
  51854/150000: episode: 539, duration: 0.930s, episode steps: 117, steps per second: 126, episode reward: -74.431, mean reward: -0.636 [-100.000,  8.784], mean action: 1.496 [0.000, 3.000],  loss: 13.164532, mse: 2512.541462, mean_q: 30.944930, mean_eps: 0.689230
  51926/150000: episode: 540, duration: 0.542s, episode steps:  72, steps per second: 133, episode reward: -30.165, mean reward: -0.419 [-100.000, 16.035], mean action: 1.569 [0.000, 3.000],  loss: 14.660762, mse: 2483.762249, mean_q: 30.217693, mean_eps: 0.688663
  52034/150000: episode: 541, duration: 0.833s, episode steps: 108, steps per second: 130, episode reward: -44.542, mean reward: -0.412 [-100.000, 15.571], mean action: 1.574 [0.000, 3.000],  loss: 16.312616, mse: 2451.477811, mean_q: 30.156243, mean_eps: 0.688123
  52166/150000: episode: 542, duration: 1.115s, episode steps: 132, steps per second: 118, episode reward: -44.609, mean reward: -0.338 [-100.000, 14.549], mean action: 1.614 [0.000, 3.000],  loss: 12.079578, mse: 2504.387467, mean_q: 30.062795, mean_eps: 0.687403
  52262/150000: episode: 543, duration: 0.798s, episode steps:  96, steps per second: 120, episode reward: -107.718, mean reward: -1.122 [-100.000, 13.997], mean action: 1.594 [0.000, 3.000],  loss: 12.766923, mse: 2585.778770, mean_q: 30.129302, mean_eps: 0.686719
  52370/150000: episode: 544, duration: 0.861s, episode steps: 108, steps per second: 125, episode reward: -105.029, mean reward: -0.972 [-100.000, 11.931], mean action: 1.509 [0.000, 3.000],  loss: 11.443900, mse: 2534.598032, mean_q: 29.903638, mean_eps: 0.686107
  52484/150000: episode: 545, duration: 0.955s, episode steps: 114, steps per second: 119, episode reward: -71.199, mean reward: -0.625 [-100.000, 10.896], mean action: 1.789 [0.000, 3.000],  loss: 13.669005, mse: 2546.231926, mean_q: 30.152520, mean_eps: 0.685441
  52569/150000: episode: 546, duration: 0.690s, episode steps:  85, steps per second: 123, episode reward: -35.550, mean reward: -0.418 [-100.000,  6.944], mean action: 1.694 [0.000, 3.000],  loss: 15.633521, mse: 2581.443716, mean_q: 31.297129, mean_eps: 0.684844
  52635/150000: episode: 547, duration: 0.523s, episode steps:  66, steps per second: 126, episode reward: -58.296, mean reward: -0.883 [-100.000, 11.201], mean action: 1.621 [0.000, 3.000],  loss: 14.050543, mse: 2565.701185, mean_q: 30.408251, mean_eps: 0.684391
  52728/150000: episode: 548, duration: 0.757s, episode steps:  93, steps per second: 123, episode reward: -63.846, mean reward: -0.687 [-100.000, 11.924], mean action: 1.839 [0.000, 3.000],  loss: 14.972692, mse: 2584.797086, mean_q: 30.240766, mean_eps: 0.683914
  52840/150000: episode: 549, duration: 0.826s, episode steps: 112, steps per second: 136, episode reward: -57.313, mean reward: -0.512 [-100.000,  6.673], mean action: 1.750 [0.000, 3.000],  loss: 11.824382, mse: 2522.844662, mean_q: 30.791553, mean_eps: 0.683299
  52911/150000: episode: 550, duration: 0.555s, episode steps:  71, steps per second: 128, episode reward: -87.096, mean reward: -1.227 [-100.000,  9.092], mean action: 1.549 [0.000, 3.000],  loss: 20.165869, mse: 2477.647616, mean_q: 29.586535, mean_eps: 0.682750
  52986/150000: episode: 551, duration: 0.582s, episode steps:  75, steps per second: 129, episode reward: -16.513, mean reward: -0.220 [-100.000, 12.184], mean action: 1.573 [0.000, 3.000],  loss: 8.550337, mse: 2567.494860, mean_q: 30.859351, mean_eps: 0.682312
  53080/150000: episode: 552, duration: 0.697s, episode steps:  94, steps per second: 135, episode reward: 10.318, mean reward:  0.110 [-100.000, 14.164], mean action: 1.628 [0.000, 3.000],  loss: 17.760014, mse: 2614.479865, mean_q: 29.992174, mean_eps: 0.681805
  53166/150000: episode: 553, duration: 0.645s, episode steps:  86, steps per second: 133, episode reward: -70.084, mean reward: -0.815 [-100.000, 13.852], mean action: 1.605 [0.000, 3.000],  loss: 12.674576, mse: 2619.689596, mean_q: 30.893766, mean_eps: 0.681265
  53260/150000: episode: 554, duration: 0.765s, episode steps:  94, steps per second: 123, episode reward: -49.816, mean reward: -0.530 [-100.000, 10.558], mean action: 1.628 [0.000, 3.000],  loss: 8.130871, mse: 2645.598895, mean_q: 31.065519, mean_eps: 0.680725
  53378/150000: episode: 555, duration: 0.865s, episode steps: 118, steps per second: 136, episode reward: -159.079, mean reward: -1.348 [-100.000,  8.845], mean action: 1.475 [0.000, 3.000],  loss: 13.583838, mse: 2628.621578, mean_q: 30.803710, mean_eps: 0.680089
  53493/150000: episode: 556, duration: 0.857s, episode steps: 115, steps per second: 134, episode reward: -68.921, mean reward: -0.599 [-100.000,  8.890], mean action: 1.496 [0.000, 3.000],  loss: 11.777743, mse: 2643.302713, mean_q: 30.983163, mean_eps: 0.679390
  53586/150000: episode: 557, duration: 0.703s, episode steps:  93, steps per second: 132, episode reward: -113.346, mean reward: -1.219 [-100.000,  7.275], mean action: 1.688 [0.000, 3.000],  loss: 10.412440, mse: 2665.010200, mean_q: 30.915971, mean_eps: 0.678766
  53694/150000: episode: 558, duration: 0.784s, episode steps: 108, steps per second: 138, episode reward: -53.906, mean reward: -0.499 [-100.000, 10.242], mean action: 1.704 [0.000, 3.000],  loss: 15.356328, mse: 2597.594464, mean_q: 30.609422, mean_eps: 0.678163
  53807/150000: episode: 559, duration: 0.887s, episode steps: 113, steps per second: 127, episode reward: -99.661, mean reward: -0.882 [-100.000, 12.719], mean action: 1.504 [0.000, 3.000],  loss: 13.873271, mse: 2690.042868, mean_q: 31.676167, mean_eps: 0.677500
  53928/150000: episode: 560, duration: 1.059s, episode steps: 121, steps per second: 114, episode reward: -158.789, mean reward: -1.312 [-100.000,  8.840], mean action: 1.752 [0.000, 3.000],  loss: 13.402433, mse: 2669.800957, mean_q: 31.726415, mean_eps: 0.676798
  54017/150000: episode: 561, duration: 0.783s, episode steps:  89, steps per second: 114, episode reward: -94.097, mean reward: -1.057 [-100.000,  9.561], mean action: 1.607 [0.000, 3.000],  loss: 12.009659, mse: 2632.002765, mean_q: 31.413206, mean_eps: 0.676168
  54082/150000: episode: 562, duration: 0.572s, episode steps:  65, steps per second: 114, episode reward: -82.628, mean reward: -1.271 [-100.000, 11.760], mean action: 1.477 [0.000, 3.000],  loss: 8.964942, mse: 2707.429453, mean_q: 31.872003, mean_eps: 0.675706
  54222/150000: episode: 563, duration: 1.163s, episode steps: 140, steps per second: 120, episode reward: -94.101, mean reward: -0.672 [-100.000, 10.589], mean action: 1.529 [0.000, 3.000],  loss: 11.026984, mse: 2691.009951, mean_q: 31.755652, mean_eps: 0.675091
  54298/150000: episode: 564, duration: 0.642s, episode steps:  76, steps per second: 118, episode reward: -59.689, mean reward: -0.785 [-100.000, 10.640], mean action: 1.671 [0.000, 3.000],  loss: 16.035911, mse: 2707.754322, mean_q: 31.600477, mean_eps: 0.674443
  54410/150000: episode: 565, duration: 0.901s, episode steps: 112, steps per second: 124, episode reward: -80.429, mean reward: -0.718 [-100.000,  7.602], mean action: 1.393 [0.000, 3.000],  loss: 12.199058, mse: 2657.063163, mean_q: 30.595433, mean_eps: 0.673879
  54484/150000: episode: 566, duration: 0.575s, episode steps:  74, steps per second: 129, episode reward: -2.701, mean reward: -0.037 [-100.000, 21.263], mean action: 1.797 [0.000, 3.000],  loss: 8.839671, mse: 2623.546502, mean_q: 31.622697, mean_eps: 0.673321
  54565/150000: episode: 567, duration: 0.644s, episode steps:  81, steps per second: 126, episode reward: -74.299, mean reward: -0.917 [-100.000, 12.017], mean action: 1.630 [0.000, 3.000],  loss: 16.284841, mse: 2735.868661, mean_q: 31.506779, mean_eps: 0.672856
  54647/150000: episode: 568, duration: 0.607s, episode steps:  82, steps per second: 135, episode reward: -75.115, mean reward: -0.916 [-100.000, 13.282], mean action: 1.707 [0.000, 3.000],  loss: 9.374976, mse: 2664.656039, mean_q: 31.870386, mean_eps: 0.672367
  54740/150000: episode: 569, duration: 0.695s, episode steps:  93, steps per second: 134, episode reward: -38.187, mean reward: -0.411 [-100.000, 16.369], mean action: 1.645 [0.000, 3.000],  loss: 9.344018, mse: 2711.203986, mean_q: 30.988602, mean_eps: 0.671842
  54846/150000: episode: 570, duration: 0.821s, episode steps: 106, steps per second: 129, episode reward: -46.351, mean reward: -0.437 [-100.000, 11.219], mean action: 1.623 [0.000, 3.000],  loss: 11.737629, mse: 2652.857174, mean_q: 30.632626, mean_eps: 0.671245
  54946/150000: episode: 571, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: -39.959, mean reward: -0.400 [-100.000, 19.680], mean action: 1.530 [0.000, 3.000],  loss: 9.000485, mse: 2611.758555, mean_q: 31.177019, mean_eps: 0.670627
  55015/150000: episode: 572, duration: 0.546s, episode steps:  69, steps per second: 126, episode reward: -59.288, mean reward: -0.859 [-100.000,  6.370], mean action: 1.522 [0.000, 3.000],  loss: 11.212280, mse: 2710.246494, mean_q: 31.640142, mean_eps: 0.670120
  55111/150000: episode: 573, duration: 0.794s, episode steps:  96, steps per second: 121, episode reward: -21.146, mean reward: -0.220 [-100.000, 13.723], mean action: 1.635 [0.000, 3.000],  loss: 10.705873, mse: 2710.845505, mean_q: 31.706201, mean_eps: 0.669625
  55175/150000: episode: 574, duration: 0.493s, episode steps:  64, steps per second: 130, episode reward: -82.931, mean reward: -1.296 [-100.000,  7.896], mean action: 1.656 [0.000, 3.000],  loss: 8.275166, mse: 2708.069302, mean_q: 31.825364, mean_eps: 0.669145
  55274/150000: episode: 575, duration: 0.734s, episode steps:  99, steps per second: 135, episode reward: -68.567, mean reward: -0.693 [-100.000,  9.191], mean action: 1.626 [0.000, 3.000],  loss: 11.819733, mse: 2754.853802, mean_q: 32.601867, mean_eps: 0.668656
  55385/150000: episode: 576, duration: 0.836s, episode steps: 111, steps per second: 133, episode reward: -77.107, mean reward: -0.695 [-100.000,  7.187], mean action: 1.495 [0.000, 3.000],  loss: 13.231790, mse: 2704.155040, mean_q: 31.125482, mean_eps: 0.668026
  55530/150000: episode: 577, duration: 1.083s, episode steps: 145, steps per second: 134, episode reward: -116.470, mean reward: -0.803 [-100.000,  8.025], mean action: 1.455 [0.000, 3.000],  loss: 9.754703, mse: 2670.167442, mean_q: 31.564261, mean_eps: 0.667258
  55662/150000: episode: 578, duration: 0.950s, episode steps: 132, steps per second: 139, episode reward: -109.425, mean reward: -0.829 [-100.000, 16.384], mean action: 1.644 [0.000, 3.000],  loss: 9.565162, mse: 2720.751295, mean_q: 32.285101, mean_eps: 0.666427
  55797/150000: episode: 579, duration: 1.049s, episode steps: 135, steps per second: 129, episode reward: -230.811, mean reward: -1.710 [-100.000, 22.650], mean action: 1.393 [0.000, 3.000],  loss: 12.409535, mse: 2713.433889, mean_q: 32.410685, mean_eps: 0.665626
  55904/150000: episode: 580, duration: 0.780s, episode steps: 107, steps per second: 137, episode reward: -98.533, mean reward: -0.921 [-100.000, 10.596], mean action: 1.579 [0.000, 3.000],  loss: 17.416707, mse: 2716.945648, mean_q: 30.364386, mean_eps: 0.664900
  55977/150000: episode: 581, duration: 0.557s, episode steps:  73, steps per second: 131, episode reward: -16.723, mean reward: -0.229 [-100.000, 17.056], mean action: 1.603 [0.000, 3.000],  loss: 14.583205, mse: 2644.198185, mean_q: 31.991701, mean_eps: 0.664360
  56055/150000: episode: 582, duration: 0.604s, episode steps:  78, steps per second: 129, episode reward: -73.065, mean reward: -0.937 [-100.000,  9.818], mean action: 1.731 [0.000, 3.000],  loss: 12.148790, mse: 2776.011218, mean_q: 32.552656, mean_eps: 0.663907
  56127/150000: episode: 583, duration: 0.540s, episode steps:  72, steps per second: 133, episode reward: -44.684, mean reward: -0.621 [-100.000,  6.505], mean action: 1.611 [0.000, 3.000],  loss: 13.052149, mse: 2856.006107, mean_q: 33.049669, mean_eps: 0.663457
  56199/150000: episode: 584, duration: 0.512s, episode steps:  72, steps per second: 141, episode reward: -75.537, mean reward: -1.049 [-100.000,  8.958], mean action: 1.417 [0.000, 3.000],  loss: 12.477661, mse: 2803.543013, mean_q: 31.801005, mean_eps: 0.663025
  56269/150000: episode: 585, duration: 0.536s, episode steps:  70, steps per second: 131, episode reward: -54.512, mean reward: -0.779 [-100.000, 16.477], mean action: 1.700 [0.000, 3.000],  loss: 8.718194, mse: 2766.859181, mean_q: 32.282332, mean_eps: 0.662599
  56338/150000: episode: 586, duration: 0.530s, episode steps:  69, steps per second: 130, episode reward: -55.460, mean reward: -0.804 [-100.000, 21.249], mean action: 1.667 [0.000, 3.000],  loss: 6.690790, mse: 2862.676627, mean_q: 32.130661, mean_eps: 0.662182
  56434/150000: episode: 587, duration: 0.720s, episode steps:  96, steps per second: 133, episode reward: -63.375, mean reward: -0.660 [-100.000, 10.962], mean action: 1.615 [0.000, 3.000],  loss: 12.817768, mse: 2818.735527, mean_q: 32.378896, mean_eps: 0.661687
  56522/150000: episode: 588, duration: 0.662s, episode steps:  88, steps per second: 133, episode reward: 25.589, mean reward:  0.291 [-100.000, 11.492], mean action: 1.716 [0.000, 3.000],  loss: 12.580371, mse: 2714.670776, mean_q: 31.314244, mean_eps: 0.661135
  56604/150000: episode: 589, duration: 0.640s, episode steps:  82, steps per second: 128, episode reward: -56.914, mean reward: -0.694 [-100.000, 12.996], mean action: 1.451 [0.000, 3.000],  loss: 11.193671, mse: 2850.864350, mean_q: 33.268635, mean_eps: 0.660625
  56691/150000: episode: 590, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -64.607, mean reward: -0.743 [-100.000, 15.631], mean action: 1.379 [0.000, 3.000],  loss: 9.434990, mse: 2812.702348, mean_q: 32.375136, mean_eps: 0.660118
  56788/150000: episode: 591, duration: 0.730s, episode steps:  97, steps per second: 133, episode reward: -121.278, mean reward: -1.250 [-100.000, 10.046], mean action: 1.598 [0.000, 3.000],  loss: 11.012009, mse: 2788.859566, mean_q: 32.693610, mean_eps: 0.659566
  56892/150000: episode: 592, duration: 0.875s, episode steps: 104, steps per second: 119, episode reward: -141.436, mean reward: -1.360 [-100.000,  6.011], mean action: 1.346 [0.000, 3.000],  loss: 8.768209, mse: 2817.657869, mean_q: 32.233884, mean_eps: 0.658963
  56972/150000: episode: 593, duration: 0.584s, episode steps:  80, steps per second: 137, episode reward: -47.913, mean reward: -0.599 [-100.000, 13.547], mean action: 1.650 [0.000, 3.000],  loss: 8.151427, mse: 2793.828326, mean_q: 32.523693, mean_eps: 0.658411
  57099/150000: episode: 594, duration: 0.977s, episode steps: 127, steps per second: 130, episode reward: -101.976, mean reward: -0.803 [-100.000,  5.740], mean action: 1.669 [0.000, 3.000],  loss: 15.276641, mse: 2837.874221, mean_q: 32.210972, mean_eps: 0.657790
  57210/150000: episode: 595, duration: 0.829s, episode steps: 111, steps per second: 134, episode reward: -29.213, mean reward: -0.263 [-100.000, 14.100], mean action: 1.739 [0.000, 3.000],  loss: 13.977756, mse: 2816.088843, mean_q: 30.927243, mean_eps: 0.657076
  57330/150000: episode: 596, duration: 0.891s, episode steps: 120, steps per second: 135, episode reward: -11.442, mean reward: -0.095 [-100.000, 11.806], mean action: 1.667 [0.000, 3.000],  loss: 9.340944, mse: 2877.676272, mean_q: 32.942816, mean_eps: 0.656383
  57452/150000: episode: 597, duration: 0.924s, episode steps: 122, steps per second: 132, episode reward: -76.842, mean reward: -0.630 [-100.000,  7.647], mean action: 1.459 [0.000, 3.000],  loss: 17.577699, mse: 2892.461880, mean_q: 31.915077, mean_eps: 0.655657
  57558/150000: episode: 598, duration: 0.762s, episode steps: 106, steps per second: 139, episode reward: -245.077, mean reward: -2.312 [-100.000,  0.979], mean action: 1.660 [0.000, 3.000],  loss: 11.551345, mse: 2914.973135, mean_q: 33.413206, mean_eps: 0.654973
  57677/150000: episode: 599, duration: 0.879s, episode steps: 119, steps per second: 135, episode reward: -65.418, mean reward: -0.550 [-100.000, 12.603], mean action: 1.563 [0.000, 3.000],  loss: 11.692377, mse: 2897.577823, mean_q: 32.812167, mean_eps: 0.654298
  57771/150000: episode: 600, duration: 0.726s, episode steps:  94, steps per second: 129, episode reward: -84.998, mean reward: -0.904 [-100.000, 10.204], mean action: 1.521 [0.000, 3.000],  loss: 12.761302, mse: 2834.375358, mean_q: 32.676642, mean_eps: 0.653659
  57864/150000: episode: 601, duration: 0.699s, episode steps:  93, steps per second: 133, episode reward: -76.974, mean reward: -0.828 [-100.000, 18.170], mean action: 1.763 [0.000, 3.000],  loss: 8.560147, mse: 2869.045998, mean_q: 33.483315, mean_eps: 0.653098
  57961/150000: episode: 602, duration: 0.734s, episode steps:  97, steps per second: 132, episode reward: -64.648, mean reward: -0.666 [-100.000,  6.687], mean action: 1.722 [0.000, 3.000],  loss: 15.045583, mse: 2906.035315, mean_q: 32.594858, mean_eps: 0.652528
  58056/150000: episode: 603, duration: 0.808s, episode steps:  95, steps per second: 118, episode reward: -45.749, mean reward: -0.482 [-100.000,  9.785], mean action: 1.463 [0.000, 3.000],  loss: 14.172497, mse: 2921.909169, mean_q: 33.887689, mean_eps: 0.651952
  58188/150000: episode: 604, duration: 0.991s, episode steps: 132, steps per second: 133, episode reward: -171.162, mean reward: -1.297 [-100.000, 34.013], mean action: 1.644 [0.000, 3.000],  loss: 11.818148, mse: 3041.768995, mean_q: 34.351940, mean_eps: 0.651271
  58299/150000: episode: 605, duration: 0.856s, episode steps: 111, steps per second: 130, episode reward: -105.702, mean reward: -0.952 [-100.000,  5.505], mean action: 1.604 [0.000, 3.000],  loss: 9.011299, mse: 2936.792364, mean_q: 33.963587, mean_eps: 0.650542
  58472/150000: episode: 607, duration: 0.840s, episode steps:  97, steps per second: 115, episode reward: -126.472, mean reward: -1.304 [-100.000,  5.810], mean action: 1.258 [0.000, 3.000],  loss: 9.207379, mse: 2944.721866, mean_q: 33.520650, mean_eps: 0.649462
  58570/150000: episode: 608, duration: 0.769s, episode steps:  98, steps per second: 127, episode reward: -82.713, mean reward: -0.844 [-100.000,  7.383], mean action: 1.643 [0.000, 3.000],  loss: 11.957967, mse: 2997.706291, mean_q: 35.420920, mean_eps: 0.648877
  58680/150000: episode: 609, duration: 0.796s, episode steps: 110, steps per second: 138, episode reward: -207.322, mean reward: -1.885 [-100.000,  2.504], mean action: 1.664 [0.000, 3.000],  loss: 10.742992, mse: 2979.383707, mean_q: 34.511530, mean_eps: 0.648253
  58761/150000: episode: 610, duration: 0.588s, episode steps:  81, steps per second: 138, episode reward: -65.198, mean reward: -0.805 [-100.000, 18.835], mean action: 1.605 [0.000, 3.000],  loss: 12.430183, mse: 2977.741561, mean_q: 34.433816, mean_eps: 0.647680
  58862/150000: episode: 611, duration: 0.791s, episode steps: 101, steps per second: 128, episode reward: -190.818, mean reward: -1.889 [-100.000,  4.297], mean action: 1.851 [0.000, 3.000],  loss: 12.232580, mse: 2952.157256, mean_q: 33.871256, mean_eps: 0.647134
  58956/150000: episode: 612, duration: 0.704s, episode steps:  94, steps per second: 134, episode reward:  1.441, mean reward:  0.015 [-100.000, 12.957], mean action: 1.745 [0.000, 3.000],  loss: 7.127004, mse: 2986.911354, mean_q: 34.361674, mean_eps: 0.646549
  59067/150000: episode: 613, duration: 0.797s, episode steps: 111, steps per second: 139, episode reward: -61.671, mean reward: -0.556 [-100.000, 15.106], mean action: 1.486 [0.000, 3.000],  loss: 10.337291, mse: 3051.075114, mean_q: 33.681908, mean_eps: 0.645934
  59143/150000: episode: 614, duration: 0.608s, episode steps:  76, steps per second: 125, episode reward: -77.211, mean reward: -1.016 [-100.000,  7.186], mean action: 1.776 [0.000, 3.000],  loss: 7.387346, mse: 2952.477738, mean_q: 33.851034, mean_eps: 0.645373
  59219/150000: episode: 615, duration: 0.566s, episode steps:  76, steps per second: 134, episode reward: -54.216, mean reward: -0.713 [-100.000,  9.533], mean action: 1.605 [0.000, 3.000],  loss: 9.403737, mse: 2981.452910, mean_q: 33.754080, mean_eps: 0.644917
  59337/150000: episode: 616, duration: 0.857s, episode steps: 118, steps per second: 138, episode reward: -71.644, mean reward: -0.607 [-100.000,  7.833], mean action: 1.585 [0.000, 3.000],  loss: 8.596565, mse: 2947.368054, mean_q: 33.920777, mean_eps: 0.644335
  59432/150000: episode: 617, duration: 0.743s, episode steps:  95, steps per second: 128, episode reward: -75.302, mean reward: -0.793 [-100.000, 11.033], mean action: 1.674 [0.000, 3.000],  loss: 15.030060, mse: 3033.530464, mean_q: 33.845282, mean_eps: 0.643696
  59574/150000: episode: 618, duration: 1.060s, episode steps: 142, steps per second: 134, episode reward: -93.282, mean reward: -0.657 [-100.000, 16.794], mean action: 1.711 [0.000, 3.000],  loss: 8.868900, mse: 2985.228012, mean_q: 33.702166, mean_eps: 0.642985
  59695/150000: episode: 619, duration: 0.935s, episode steps: 121, steps per second: 129, episode reward: -146.880, mean reward: -1.214 [-100.000, 21.440], mean action: 1.678 [0.000, 3.000],  loss: 8.033209, mse: 3000.087685, mean_q: 34.535360, mean_eps: 0.642196
  59773/150000: episode: 620, duration: 0.611s, episode steps:  78, steps per second: 128, episode reward: -9.049, mean reward: -0.116 [-100.000, 12.555], mean action: 1.718 [0.000, 3.000],  loss: 14.512261, mse: 3043.611810, mean_q: 34.793405, mean_eps: 0.641599
  59888/150000: episode: 621, duration: 0.889s, episode steps: 115, steps per second: 129, episode reward: -65.727, mean reward: -0.572 [-100.000,  8.561], mean action: 1.678 [0.000, 3.000],  loss: 10.788035, mse: 3039.117718, mean_q: 35.210086, mean_eps: 0.641020
  59994/150000: episode: 622, duration: 0.829s, episode steps: 106, steps per second: 128, episode reward: -127.783, mean reward: -1.206 [-100.000, 18.237], mean action: 1.519 [0.000, 3.000],  loss: 8.282197, mse: 2999.281033, mean_q: 33.800844, mean_eps: 0.640357
  60088/150000: episode: 623, duration: 0.726s, episode steps:  94, steps per second: 130, episode reward: -129.616, mean reward: -1.379 [-100.000, 11.730], mean action: 1.479 [0.000, 3.000],  loss: 10.503018, mse: 3057.652554, mean_q: 34.798499, mean_eps: 0.639757
  60190/150000: episode: 624, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -108.333, mean reward: -1.062 [-100.000,  5.413], mean action: 1.657 [0.000, 3.000],  loss: 7.571128, mse: 3080.822596, mean_q: 34.902617, mean_eps: 0.639169
  60331/150000: episode: 625, duration: 1.097s, episode steps: 141, steps per second: 129, episode reward: -3.392, mean reward: -0.024 [-100.000, 14.875], mean action: 1.660 [0.000, 3.000],  loss: 14.157335, mse: 3041.679244, mean_q: 34.453187, mean_eps: 0.638440
  60442/150000: episode: 626, duration: 0.848s, episode steps: 111, steps per second: 131, episode reward: -75.665, mean reward: -0.682 [-100.000, 13.110], mean action: 1.568 [0.000, 3.000],  loss: 8.578985, mse: 3047.333309, mean_q: 34.634832, mean_eps: 0.637684
  60578/150000: episode: 627, duration: 1.050s, episode steps: 136, steps per second: 130, episode reward: -14.095, mean reward: -0.104 [-100.000, 37.156], mean action: 1.654 [0.000, 3.000],  loss: 8.396416, mse: 3111.757269, mean_q: 35.359696, mean_eps: 0.636943
  60662/150000: episode: 628, duration: 0.696s, episode steps:  84, steps per second: 121, episode reward: -71.115, mean reward: -0.847 [-100.000, 40.732], mean action: 1.869 [0.000, 3.000],  loss: 14.662555, mse: 3032.454352, mean_q: 34.613448, mean_eps: 0.636283
  60760/150000: episode: 629, duration: 0.785s, episode steps:  98, steps per second: 125, episode reward: -163.384, mean reward: -1.667 [-100.000, 12.483], mean action: 1.582 [0.000, 3.000],  loss: 6.730144, mse: 3056.855152, mean_q: 34.750487, mean_eps: 0.635737
  60862/150000: episode: 630, duration: 0.857s, episode steps: 102, steps per second: 119, episode reward: -88.205, mean reward: -0.865 [-100.000, 12.547], mean action: 1.441 [0.000, 3.000],  loss: 10.830736, mse: 3063.766135, mean_q: 35.181765, mean_eps: 0.635137
  60942/150000: episode: 631, duration: 0.651s, episode steps:  80, steps per second: 123, episode reward:  7.239, mean reward:  0.090 [-100.000, 41.996], mean action: 1.700 [0.000, 3.000],  loss: 8.738035, mse: 3044.336664, mean_q: 34.080380, mean_eps: 0.634591
  61051/150000: episode: 632, duration: 0.928s, episode steps: 109, steps per second: 117, episode reward: -14.552, mean reward: -0.134 [-100.000, 11.909], mean action: 1.358 [0.000, 3.000],  loss: 11.924649, mse: 3152.585658, mean_q: 36.298239, mean_eps: 0.634024
  61159/150000: episode: 633, duration: 0.835s, episode steps: 108, steps per second: 129, episode reward: -80.579, mean reward: -0.746 [-100.000, 13.095], mean action: 1.491 [0.000, 3.000],  loss: 10.173919, mse: 3144.520295, mean_q: 35.602167, mean_eps: 0.633373
  61247/150000: episode: 634, duration: 0.666s, episode steps:  88, steps per second: 132, episode reward: -34.541, mean reward: -0.393 [-100.000, 19.770], mean action: 1.591 [0.000, 3.000],  loss: 9.517915, mse: 3115.730361, mean_q: 35.467238, mean_eps: 0.632785
  61368/150000: episode: 635, duration: 0.940s, episode steps: 121, steps per second: 129, episode reward: -57.103, mean reward: -0.472 [-100.000, 13.582], mean action: 1.512 [0.000, 3.000],  loss: 8.427190, mse: 3216.004203, mean_q: 36.624837, mean_eps: 0.632158
  61469/150000: episode: 636, duration: 0.743s, episode steps: 101, steps per second: 136, episode reward: -90.681, mean reward: -0.898 [-100.000,  6.465], mean action: 1.614 [0.000, 3.000],  loss: 8.831624, mse: 3169.029611, mean_q: 36.571241, mean_eps: 0.631492
  61548/150000: episode: 637, duration: 0.577s, episode steps:  79, steps per second: 137, episode reward: -83.268, mean reward: -1.054 [-100.000, 11.642], mean action: 1.709 [0.000, 3.000],  loss: 10.306668, mse: 3216.797277, mean_q: 36.004629, mean_eps: 0.630952
  61673/150000: episode: 638, duration: 0.991s, episode steps: 125, steps per second: 126, episode reward: -92.724, mean reward: -0.742 [-100.000, 37.971], mean action: 1.592 [0.000, 3.000],  loss: 5.658466, mse: 3215.753215, mean_q: 35.643135, mean_eps: 0.630340
  61801/150000: episode: 639, duration: 0.939s, episode steps: 128, steps per second: 136, episode reward: -129.542, mean reward: -1.012 [-100.000, 16.640], mean action: 1.570 [0.000, 3.000],  loss: 8.225664, mse: 3208.635857, mean_q: 36.941572, mean_eps: 0.629581
  61891/150000: episode: 640, duration: 0.669s, episode steps:  90, steps per second: 134, episode reward: -93.416, mean reward: -1.038 [-100.000, 10.543], mean action: 1.633 [0.000, 3.000],  loss: 15.370756, mse: 3185.679121, mean_q: 35.438628, mean_eps: 0.628927
  62008/150000: episode: 641, duration: 0.912s, episode steps: 117, steps per second: 128, episode reward: -58.934, mean reward: -0.504 [-100.000, 11.279], mean action: 1.675 [0.000, 3.000],  loss: 6.726291, mse: 3209.557041, mean_q: 37.056860, mean_eps: 0.628306
  62132/150000: episode: 642, duration: 0.916s, episode steps: 124, steps per second: 135, episode reward: -36.616, mean reward: -0.295 [-100.000, 13.425], mean action: 1.790 [0.000, 3.000],  loss: 13.231470, mse: 3196.707256, mean_q: 35.831059, mean_eps: 0.627583
  62226/150000: episode: 643, duration: 0.728s, episode steps:  94, steps per second: 129, episode reward: -71.735, mean reward: -0.763 [-100.000, 11.382], mean action: 1.660 [0.000, 3.000],  loss: 9.033677, mse: 3234.697621, mean_q: 35.892067, mean_eps: 0.626929
  62317/150000: episode: 644, duration: 0.672s, episode steps:  91, steps per second: 135, episode reward: -81.509, mean reward: -0.896 [-100.000, 20.774], mean action: 1.527 [0.000, 3.000],  loss: 12.815105, mse: 3236.246858, mean_q: 36.362135, mean_eps: 0.626374
  62427/150000: episode: 645, duration: 0.859s, episode steps: 110, steps per second: 128, episode reward: -100.646, mean reward: -0.915 [-100.000, 11.554], mean action: 1.609 [0.000, 3.000],  loss: 6.974395, mse: 3185.130067, mean_q: 34.593026, mean_eps: 0.625771
  62542/150000: episode: 646, duration: 1.045s, episode steps: 115, steps per second: 110, episode reward: -89.570, mean reward: -0.779 [-100.000, 11.879], mean action: 1.609 [0.000, 3.000],  loss: 9.632985, mse: 3169.962381, mean_q: 35.716044, mean_eps: 0.625096
  62626/150000: episode: 647, duration: 0.697s, episode steps:  84, steps per second: 121, episode reward: -95.443, mean reward: -1.136 [-100.000,  4.153], mean action: 1.845 [0.000, 3.000],  loss: 6.049515, mse: 3235.415469, mean_q: 36.070568, mean_eps: 0.624499
  62744/150000: episode: 648, duration: 1.030s, episode steps: 118, steps per second: 115, episode reward:  4.124, mean reward:  0.035 [-100.000, 14.715], mean action: 1.686 [0.000, 3.000],  loss: 9.331851, mse: 3161.544618, mean_q: 35.188243, mean_eps: 0.623893
  62852/150000: episode: 649, duration: 0.889s, episode steps: 108, steps per second: 121, episode reward:  4.195, mean reward:  0.039 [-100.000, 27.998], mean action: 1.704 [0.000, 3.000],  loss: 17.124550, mse: 3227.627184, mean_q: 36.311290, mean_eps: 0.623215
  62940/150000: episode: 650, duration: 0.693s, episode steps:  88, steps per second: 127, episode reward: -23.951, mean reward: -0.272 [-100.000, 13.358], mean action: 1.750 [0.000, 3.000],  loss: 9.466180, mse: 3233.578289, mean_q: 35.690720, mean_eps: 0.622627
  63016/150000: episode: 651, duration: 0.662s, episode steps:  76, steps per second: 115, episode reward: -34.556, mean reward: -0.455 [-100.000, 14.809], mean action: 1.658 [0.000, 3.000],  loss: 11.156590, mse: 3244.263993, mean_q: 36.747182, mean_eps: 0.622135
  63114/150000: episode: 652, duration: 0.746s, episode steps:  98, steps per second: 131, episode reward: -13.568, mean reward: -0.138 [-100.000,  6.082], mean action: 1.694 [0.000, 3.000],  loss: 8.403230, mse: 3321.470235, mean_q: 36.241999, mean_eps: 0.621613
  63267/150000: episode: 653, duration: 1.134s, episode steps: 153, steps per second: 135, episode reward: 13.397, mean reward:  0.088 [-100.000, 67.895], mean action: 1.667 [0.000, 3.000],  loss: 9.403776, mse: 3309.671299, mean_q: 37.425736, mean_eps: 0.620860
  63358/150000: episode: 654, duration: 0.689s, episode steps:  91, steps per second: 132, episode reward: -80.118, mean reward: -0.880 [-100.000, 15.902], mean action: 1.538 [0.000, 3.000],  loss: 15.045522, mse: 3330.902317, mean_q: 36.902123, mean_eps: 0.620128
  63431/150000: episode: 655, duration: 0.525s, episode steps:  73, steps per second: 139, episode reward: -76.032, mean reward: -1.042 [-100.000, 12.207], mean action: 1.548 [0.000, 3.000],  loss: 9.929099, mse: 3273.725920, mean_q: 36.218019, mean_eps: 0.619636
  63555/150000: episode: 656, duration: 0.953s, episode steps: 124, steps per second: 130, episode reward: -74.081, mean reward: -0.597 [-100.000, 15.853], mean action: 1.750 [0.000, 3.000],  loss: 9.930518, mse: 3257.051502, mean_q: 36.312747, mean_eps: 0.619045
  63671/150000: episode: 657, duration: 0.876s, episode steps: 116, steps per second: 132, episode reward: -92.918, mean reward: -0.801 [-100.000, 10.927], mean action: 1.526 [0.000, 3.000],  loss: 10.784951, mse: 3319.989144, mean_q: 36.591933, mean_eps: 0.618325
  63800/150000: episode: 658, duration: 0.931s, episode steps: 129, steps per second: 139, episode reward: -84.826, mean reward: -0.658 [-100.000, 23.417], mean action: 1.426 [0.000, 3.000],  loss: 11.051517, mse: 3279.579083, mean_q: 36.054286, mean_eps: 0.617590
  63902/150000: episode: 659, duration: 0.833s, episode steps: 102, steps per second: 122, episode reward: -42.444, mean reward: -0.416 [-100.000, 11.192], mean action: 1.765 [0.000, 3.000],  loss: 11.317376, mse: 3291.470023, mean_q: 36.243170, mean_eps: 0.616897
  63996/150000: episode: 660, duration: 0.707s, episode steps:  94, steps per second: 133, episode reward: -39.156, mean reward: -0.417 [-100.000, 13.082], mean action: 1.564 [0.000, 3.000],  loss: 8.835492, mse: 3313.191032, mean_q: 37.414751, mean_eps: 0.616309
  64128/150000: episode: 661, duration: 0.980s, episode steps: 132, steps per second: 135, episode reward: -41.161, mean reward: -0.312 [-100.000, 14.853], mean action: 1.439 [0.000, 3.000],  loss: 11.185687, mse: 3305.888576, mean_q: 37.510085, mean_eps: 0.615631
  64235/150000: episode: 662, duration: 0.819s, episode steps: 107, steps per second: 131, episode reward: -90.168, mean reward: -0.843 [-100.000, 30.591], mean action: 1.785 [0.000, 3.000],  loss: 14.492211, mse: 3297.092402, mean_q: 37.392388, mean_eps: 0.614914
  64325/150000: episode: 663, duration: 0.665s, episode steps:  90, steps per second: 135, episode reward: -48.043, mean reward: -0.534 [-100.000,  6.969], mean action: 1.756 [0.000, 3.000],  loss: 8.472079, mse: 3259.664068, mean_q: 37.432858, mean_eps: 0.614323
  64389/150000: episode: 664, duration: 0.472s, episode steps:  64, steps per second: 136, episode reward: -56.912, mean reward: -0.889 [-100.000, 10.047], mean action: 1.469 [0.000, 3.000],  loss: 8.889635, mse: 3319.391796, mean_q: 38.771034, mean_eps: 0.613861
  64502/150000: episode: 665, duration: 0.861s, episode steps: 113, steps per second: 131, episode reward: -75.759, mean reward: -0.670 [-100.000,  6.443], mean action: 1.673 [0.000, 3.000],  loss: 7.714939, mse: 3223.096824, mean_q: 36.636497, mean_eps: 0.613330
  64578/150000: episode: 666, duration: 0.555s, episode steps:  76, steps per second: 137, episode reward: -57.425, mean reward: -0.756 [-100.000, 31.595], mean action: 1.592 [0.000, 3.000],  loss: 8.247064, mse: 3242.895925, mean_q: 36.485099, mean_eps: 0.612763
  64671/150000: episode: 667, duration: 0.683s, episode steps:  93, steps per second: 136, episode reward: -33.347, mean reward: -0.359 [-100.000,  8.468], mean action: 1.699 [0.000, 3.000],  loss: 8.134857, mse: 3298.980603, mean_q: 37.604361, mean_eps: 0.612256
  64759/150000: episode: 668, duration: 0.689s, episode steps:  88, steps per second: 128, episode reward: -78.352, mean reward: -0.890 [-100.000,  8.069], mean action: 1.659 [0.000, 3.000],  loss: 6.820326, mse: 3166.348846, mean_q: 36.098833, mean_eps: 0.611713
  64873/150000: episode: 669, duration: 0.841s, episode steps: 114, steps per second: 135, episode reward: -118.944, mean reward: -1.043 [-100.000,  6.840], mean action: 1.693 [0.000, 3.000],  loss: 8.973337, mse: 3305.683887, mean_q: 37.085659, mean_eps: 0.611107
  64987/150000: episode: 670, duration: 0.872s, episode steps: 114, steps per second: 131, episode reward: -85.578, mean reward: -0.751 [-100.000, 12.970], mean action: 1.518 [0.000, 3.000],  loss: 7.704097, mse: 3243.375131, mean_q: 37.220940, mean_eps: 0.610423
  65120/150000: episode: 671, duration: 0.981s, episode steps: 133, steps per second: 136, episode reward: -59.322, mean reward: -0.446 [-100.000,  6.711], mean action: 1.466 [0.000, 3.000],  loss: 8.686690, mse: 3346.556406, mean_q: 38.054622, mean_eps: 0.609682
  65210/150000: episode: 672, duration: 0.672s, episode steps:  90, steps per second: 134, episode reward: -51.555, mean reward: -0.573 [-100.000,  9.262], mean action: 1.778 [0.000, 3.000],  loss: 9.057336, mse: 3458.920155, mean_q: 38.500158, mean_eps: 0.609013
  65288/150000: episode: 673, duration: 0.599s, episode steps:  78, steps per second: 130, episode reward: -66.877, mean reward: -0.857 [-100.000, 15.418], mean action: 1.756 [0.000, 3.000],  loss: 5.623021, mse: 3373.032396, mean_q: 36.964041, mean_eps: 0.608509
  65397/150000: episode: 674, duration: 0.806s, episode steps: 109, steps per second: 135, episode reward: -29.043, mean reward: -0.266 [-100.000, 16.510], mean action: 1.495 [0.000, 3.000],  loss: 6.027865, mse: 3399.660929, mean_q: 36.428788, mean_eps: 0.607948
  65475/150000: episode: 675, duration: 0.581s, episode steps:  78, steps per second: 134, episode reward: -95.191, mean reward: -1.220 [-100.000,  5.871], mean action: 1.756 [0.000, 3.000],  loss: 12.565412, mse: 3429.816300, mean_q: 38.242445, mean_eps: 0.607387
  65560/150000: episode: 676, duration: 0.656s, episode steps:  85, steps per second: 130, episode reward: -38.287, mean reward: -0.450 [-100.000, 14.756], mean action: 1.529 [0.000, 3.000],  loss: 10.523777, mse: 3425.688798, mean_q: 37.475109, mean_eps: 0.606898
  65703/150000: episode: 677, duration: 1.057s, episode steps: 143, steps per second: 135, episode reward: -15.602, mean reward: -0.109 [-100.000, 13.720], mean action: 1.713 [0.000, 3.000],  loss: 9.639858, mse: 3395.465544, mean_q: 37.372036, mean_eps: 0.606214
  65834/150000: episode: 678, duration: 0.950s, episode steps: 131, steps per second: 138, episode reward: -43.549, mean reward: -0.332 [-100.000, 10.267], mean action: 1.473 [0.000, 3.000],  loss: 10.788314, mse: 3434.336625, mean_q: 37.314733, mean_eps: 0.605392
  65971/150000: episode: 679, duration: 1.056s, episode steps: 137, steps per second: 130, episode reward: -46.861, mean reward: -0.342 [-100.000,  6.944], mean action: 1.620 [0.000, 3.000],  loss: 7.339816, mse: 3413.386484, mean_q: 37.992714, mean_eps: 0.604588
  66106/150000: episode: 680, duration: 0.982s, episode steps: 135, steps per second: 138, episode reward: -25.445, mean reward: -0.188 [-100.000, 13.392], mean action: 1.548 [0.000, 3.000],  loss: 6.251046, mse: 3404.306530, mean_q: 37.649753, mean_eps: 0.603772
  66208/150000: episode: 681, duration: 0.800s, episode steps: 102, steps per second: 128, episode reward: -23.919, mean reward: -0.235 [-100.000,  6.925], mean action: 1.637 [0.000, 3.000],  loss: 10.297527, mse: 3402.279340, mean_q: 38.411417, mean_eps: 0.603061
  66360/150000: episode: 682, duration: 1.111s, episode steps: 152, steps per second: 137, episode reward: -51.807, mean reward: -0.341 [-100.000,  7.528], mean action: 1.697 [0.000, 3.000],  loss: 8.021178, mse: 3391.833334, mean_q: 37.877066, mean_eps: 0.602299
  66473/150000: episode: 683, duration: 0.869s, episode steps: 113, steps per second: 130, episode reward: -21.303, mean reward: -0.189 [-100.000, 10.881], mean action: 1.558 [0.000, 3.000],  loss: 10.228491, mse: 3354.974277, mean_q: 36.404344, mean_eps: 0.601504
  66554/150000: episode: 684, duration: 0.595s, episode steps:  81, steps per second: 136, episode reward: -97.103, mean reward: -1.199 [-100.000,  7.100], mean action: 1.457 [0.000, 3.000],  loss: 10.291246, mse: 3451.358995, mean_q: 36.468929, mean_eps: 0.600922
  66677/150000: episode: 685, duration: 0.894s, episode steps: 123, steps per second: 138, episode reward: -83.701, mean reward: -0.680 [-100.000, 19.563], mean action: 1.382 [0.000, 3.000],  loss: 10.266260, mse: 3355.424249, mean_q: 36.688317, mean_eps: 0.600310
  66755/150000: episode: 686, duration: 0.616s, episode steps:  78, steps per second: 127, episode reward: -36.356, mean reward: -0.466 [-100.000, 11.664], mean action: 1.718 [0.000, 3.000],  loss: 10.276806, mse: 3397.561977, mean_q: 36.843507, mean_eps: 0.599707
  66840/150000: episode: 687, duration: 0.650s, episode steps:  85, steps per second: 131, episode reward: -45.131, mean reward: -0.531 [-100.000, 10.825], mean action: 1.776 [0.000, 3.000],  loss: 8.807639, mse: 3491.045203, mean_q: 37.538839, mean_eps: 0.599218
  66948/150000: episode: 688, duration: 0.777s, episode steps: 108, steps per second: 139, episode reward: -47.224, mean reward: -0.437 [-100.000, 20.309], mean action: 1.796 [0.000, 3.000],  loss: 7.654052, mse: 3411.757320, mean_q: 37.123665, mean_eps: 0.598639
  67038/150000: episode: 689, duration: 0.701s, episode steps:  90, steps per second: 128, episode reward: -67.578, mean reward: -0.751 [-100.000,  9.281], mean action: 1.689 [0.000, 3.000],  loss: 6.550771, mse: 3431.470315, mean_q: 36.952863, mean_eps: 0.598045
  67139/150000: episode: 690, duration: 0.758s, episode steps: 101, steps per second: 133, episode reward: -42.386, mean reward: -0.420 [-100.000, 14.062], mean action: 1.703 [0.000, 3.000],  loss: 5.352369, mse: 3468.253481, mean_q: 38.152350, mean_eps: 0.597472
  67220/150000: episode: 691, duration: 0.595s, episode steps:  81, steps per second: 136, episode reward: -52.085, mean reward: -0.643 [-100.000, 13.064], mean action: 1.556 [0.000, 3.000],  loss: 11.433167, mse: 3486.465172, mean_q: 37.423510, mean_eps: 0.596926
  67347/150000: episode: 692, duration: 0.961s, episode steps: 127, steps per second: 132, episode reward: -45.600, mean reward: -0.359 [-100.000, 13.710], mean action: 1.535 [0.000, 3.000],  loss: 8.862949, mse: 3491.250896, mean_q: 37.422202, mean_eps: 0.596302
  67466/150000: episode: 693, duration: 0.878s, episode steps: 119, steps per second: 136, episode reward: -46.986, mean reward: -0.395 [-100.000, 11.960], mean action: 1.597 [0.000, 3.000],  loss: 7.020733, mse: 3531.466834, mean_q: 38.203012, mean_eps: 0.595564
  67552/150000: episode: 694, duration: 0.617s, episode steps:  86, steps per second: 139, episode reward: -73.674, mean reward: -0.857 [-100.000, 10.846], mean action: 1.605 [0.000, 3.000],  loss: 5.971228, mse: 3541.048303, mean_q: 39.068355, mean_eps: 0.594949
  67663/150000: episode: 695, duration: 0.862s, episode steps: 111, steps per second: 129, episode reward: -63.555, mean reward: -0.573 [-100.000,  8.118], mean action: 1.577 [0.000, 3.000],  loss: 8.315534, mse: 3520.825945, mean_q: 37.713433, mean_eps: 0.594358
  67776/150000: episode: 696, duration: 0.854s, episode steps: 113, steps per second: 132, episode reward: -29.760, mean reward: -0.263 [-100.000, 16.481], mean action: 1.735 [0.000, 3.000],  loss: 7.090544, mse: 3471.388285, mean_q: 38.614952, mean_eps: 0.593686
  67881/150000: episode: 697, duration: 0.837s, episode steps: 105, steps per second: 125, episode reward: -67.559, mean reward: -0.643 [-100.000, 11.588], mean action: 1.695 [0.000, 3.000],  loss: 6.537791, mse: 3509.492234, mean_q: 38.947686, mean_eps: 0.593032
  67955/150000: episode: 698, duration: 0.573s, episode steps:  74, steps per second: 129, episode reward: -44.461, mean reward: -0.601 [-100.000, 20.775], mean action: 1.824 [0.000, 3.000],  loss: 7.365015, mse: 3615.084044, mean_q: 38.904653, mean_eps: 0.592495
  68350/150000: episode: 699, duration: 3.075s, episode steps: 395, steps per second: 128, episode reward: -83.705, mean reward: -0.212 [-100.000, 17.222], mean action: 1.699 [0.000, 3.000],  loss: 9.968965, mse: 3549.962198, mean_q: 38.081132, mean_eps: 0.591088
  68463/150000: episode: 700, duration: 0.880s, episode steps: 113, steps per second: 128, episode reward: -86.176, mean reward: -0.763 [-100.000, 36.620], mean action: 1.575 [0.000, 3.000],  loss: 6.447886, mse: 3490.986350, mean_q: 37.966057, mean_eps: 0.589564
  68532/150000: episode: 701, duration: 0.513s, episode steps:  69, steps per second: 134, episode reward: -31.085, mean reward: -0.451 [-100.000, 10.759], mean action: 1.638 [0.000, 3.000],  loss: 9.182883, mse: 3495.392953, mean_q: 38.107902, mean_eps: 0.589018
  68687/150000: episode: 702, duration: 1.135s, episode steps: 155, steps per second: 137, episode reward: -48.090, mean reward: -0.310 [-100.000, 13.021], mean action: 1.652 [0.000, 3.000],  loss: 7.610957, mse: 3523.137949, mean_q: 38.515692, mean_eps: 0.588346
  68757/150000: episode: 703, duration: 0.567s, episode steps:  70, steps per second: 124, episode reward: -73.482, mean reward: -1.050 [-100.000, 13.914], mean action: 1.686 [0.000, 3.000],  loss: 6.722057, mse: 3535.701179, mean_q: 39.219736, mean_eps: 0.587671
  68849/150000: episode: 704, duration: 0.686s, episode steps:  92, steps per second: 134, episode reward: -46.065, mean reward: -0.501 [-100.000, 10.850], mean action: 1.685 [0.000, 3.000],  loss: 9.151532, mse: 3660.519560, mean_q: 40.535561, mean_eps: 0.587185
  69013/150000: episode: 705, duration: 1.218s, episode steps: 164, steps per second: 135, episode reward: -15.567, mean reward: -0.095 [-100.000, 32.957], mean action: 1.671 [0.000, 3.000],  loss: 12.466426, mse: 3505.476871, mean_q: 38.262790, mean_eps: 0.586417
  69112/150000: episode: 706, duration: 0.735s, episode steps:  99, steps per second: 135, episode reward: -52.370, mean reward: -0.529 [-100.000,  6.966], mean action: 1.677 [0.000, 3.000],  loss: 10.677255, mse: 3585.006281, mean_q: 38.513712, mean_eps: 0.585628
  69189/150000: episode: 707, duration: 0.649s, episode steps:  77, steps per second: 119, episode reward: -16.621, mean reward: -0.216 [-100.000, 12.881], mean action: 1.636 [0.000, 3.000],  loss: 9.889737, mse: 3598.645454, mean_q: 38.910246, mean_eps: 0.585100
  69317/150000: episode: 708, duration: 1.065s, episode steps: 128, steps per second: 120, episode reward: -36.520, mean reward: -0.285 [-100.000, 10.247], mean action: 1.578 [0.000, 3.000],  loss: 8.799440, mse: 3555.521402, mean_q: 39.090911, mean_eps: 0.584485
  69406/150000: episode: 709, duration: 0.725s, episode steps:  89, steps per second: 123, episode reward: -44.509, mean reward: -0.500 [-100.000,  6.775], mean action: 1.618 [0.000, 3.000],  loss: 7.298266, mse: 3614.344482, mean_q: 38.857690, mean_eps: 0.583834
  69493/150000: episode: 710, duration: 0.683s, episode steps:  87, steps per second: 127, episode reward: -8.114, mean reward: -0.093 [-100.000, 12.492], mean action: 1.724 [0.000, 3.000],  loss: 8.208336, mse: 3646.871750, mean_q: 39.817833, mean_eps: 0.583306
  69616/150000: episode: 711, duration: 1.051s, episode steps: 123, steps per second: 117, episode reward: -109.198, mean reward: -0.888 [-100.000,  6.800], mean action: 1.472 [0.000, 3.000],  loss: 10.311814, mse: 3627.014847, mean_q: 39.320326, mean_eps: 0.582676
  69703/150000: episode: 712, duration: 0.666s, episode steps:  87, steps per second: 131, episode reward: -59.638, mean reward: -0.685 [-100.000,  6.320], mean action: 1.770 [0.000, 3.000],  loss: 8.143307, mse: 3588.545174, mean_q: 39.205450, mean_eps: 0.582046
  69775/150000: episode: 713, duration: 0.530s, episode steps:  72, steps per second: 136, episode reward: -101.554, mean reward: -1.410 [-100.000,  6.299], mean action: 1.708 [0.000, 3.000],  loss: 12.150032, mse: 3499.273471, mean_q: 37.885109, mean_eps: 0.581569
  69874/150000: episode: 714, duration: 0.785s, episode steps:  99, steps per second: 126, episode reward: -58.303, mean reward: -0.589 [-100.000, 10.535], mean action: 1.697 [0.000, 3.000],  loss: 10.027289, mse: 3630.054276, mean_q: 40.002117, mean_eps: 0.581056
  69976/150000: episode: 715, duration: 0.760s, episode steps: 102, steps per second: 134, episode reward: -6.978, mean reward: -0.068 [-100.000,  8.233], mean action: 1.794 [0.000, 3.000],  loss: 7.219867, mse: 3498.999078, mean_q: 37.000017, mean_eps: 0.580453
  70124/150000: episode: 716, duration: 1.118s, episode steps: 148, steps per second: 132, episode reward: -39.993, mean reward: -0.270 [-100.000, 17.142], mean action: 1.500 [0.000, 3.000],  loss: 9.915123, mse: 3575.147931, mean_q: 38.551205, mean_eps: 0.579703
  70231/150000: episode: 717, duration: 0.814s, episode steps: 107, steps per second: 131, episode reward: -88.460, mean reward: -0.827 [-100.000, 10.785], mean action: 1.785 [0.000, 3.000],  loss: 7.763820, mse: 3594.973140, mean_q: 37.349268, mean_eps: 0.578938
  70384/150000: episode: 718, duration: 1.106s, episode steps: 153, steps per second: 138, episode reward: -81.765, mean reward: -0.534 [-100.000,  6.513], mean action: 1.523 [0.000, 3.000],  loss: 9.204536, mse: 3600.587570, mean_q: 38.052425, mean_eps: 0.578158
  70489/150000: episode: 719, duration: 0.809s, episode steps: 105, steps per second: 130, episode reward: -42.964, mean reward: -0.409 [-100.000, 14.836], mean action: 1.210 [0.000, 3.000],  loss: 11.446064, mse: 3674.381524, mean_q: 39.520765, mean_eps: 0.577384
  70616/150000: episode: 720, duration: 0.921s, episode steps: 127, steps per second: 138, episode reward:  6.470, mean reward:  0.051 [-100.000, 17.180], mean action: 1.693 [0.000, 3.000],  loss: 8.916492, mse: 3643.251697, mean_q: 39.577849, mean_eps: 0.576688
  70733/150000: episode: 721, duration: 0.912s, episode steps: 117, steps per second: 128, episode reward: 42.722, mean reward:  0.365 [-100.000, 55.664], mean action: 1.752 [0.000, 3.000],  loss: 8.272144, mse: 3653.085695, mean_q: 38.540405, mean_eps: 0.575956
  70864/150000: episode: 722, duration: 0.953s, episode steps: 131, steps per second: 138, episode reward: -11.879, mean reward: -0.091 [-100.000, 16.607], mean action: 1.519 [0.000, 3.000],  loss: 7.893125, mse: 3657.298101, mean_q: 39.181670, mean_eps: 0.575212
  70995/150000: episode: 723, duration: 1.007s, episode steps: 131, steps per second: 130, episode reward: -80.078, mean reward: -0.611 [-100.000, 11.400], mean action: 1.588 [0.000, 3.000],  loss: 7.891425, mse: 3622.874238, mean_q: 38.425324, mean_eps: 0.574426
  71117/150000: episode: 724, duration: 1.087s, episode steps: 122, steps per second: 112, episode reward: -118.869, mean reward: -0.974 [-100.000,  8.720], mean action: 1.664 [0.000, 3.000],  loss: 7.310461, mse: 3760.166334, mean_q: 39.773962, mean_eps: 0.573667
  71237/150000: episode: 725, duration: 1.030s, episode steps: 120, steps per second: 117, episode reward: -67.141, mean reward: -0.560 [-100.000, 10.077], mean action: 1.650 [0.000, 3.000],  loss: 5.787646, mse: 3770.593801, mean_q: 40.047374, mean_eps: 0.572941
  71335/150000: episode: 726, duration: 0.803s, episode steps:  98, steps per second: 122, episode reward: -84.627, mean reward: -0.864 [-100.000, 11.001], mean action: 1.449 [0.000, 3.000],  loss: 11.083872, mse: 3771.406811, mean_q: 40.337672, mean_eps: 0.572287
  71444/150000: episode: 727, duration: 0.867s, episode steps: 109, steps per second: 126, episode reward: -136.351, mean reward: -1.251 [-100.000, 10.406], mean action: 1.606 [0.000, 3.000],  loss: 8.810926, mse: 3778.316229, mean_q: 39.217311, mean_eps: 0.571666
  71564/150000: episode: 728, duration: 1.055s, episode steps: 120, steps per second: 114, episode reward: -9.286, mean reward: -0.077 [-100.000, 26.416], mean action: 1.567 [0.000, 3.000],  loss: 9.723304, mse: 3761.726973, mean_q: 40.620177, mean_eps: 0.570979
  71726/150000: episode: 729, duration: 1.270s, episode steps: 162, steps per second: 128, episode reward: -50.444, mean reward: -0.311 [-100.000, 51.201], mean action: 1.722 [0.000, 3.000],  loss: 7.041151, mse: 3778.867472, mean_q: 39.253603, mean_eps: 0.570133
  71840/150000: episode: 730, duration: 0.864s, episode steps: 114, steps per second: 132, episode reward: -46.816, mean reward: -0.411 [-100.000, 17.485], mean action: 1.614 [0.000, 3.000],  loss: 7.367858, mse: 3818.429929, mean_q: 39.920536, mean_eps: 0.569305
  71966/150000: episode: 731, duration: 0.933s, episode steps: 126, steps per second: 135, episode reward: -224.521, mean reward: -1.782 [-100.000, 56.918], mean action: 1.540 [0.000, 3.000],  loss: 10.908998, mse: 3778.192106, mean_q: 38.980317, mean_eps: 0.568585
  72041/150000: episode: 732, duration: 0.587s, episode steps:  75, steps per second: 128, episode reward: -32.783, mean reward: -0.437 [-100.000, 13.449], mean action: 1.813 [0.000, 3.000],  loss: 10.752428, mse: 3763.786689, mean_q: 40.369219, mean_eps: 0.567982
  72138/150000: episode: 733, duration: 0.728s, episode steps:  97, steps per second: 133, episode reward: -58.645, mean reward: -0.605 [-100.000, 17.304], mean action: 1.753 [0.000, 3.000],  loss: 8.738823, mse: 3871.552933, mean_q: 40.893732, mean_eps: 0.567466
  72267/150000: episode: 734, duration: 0.947s, episode steps: 129, steps per second: 136, episode reward: -41.376, mean reward: -0.321 [-100.000,  9.702], mean action: 1.581 [0.000, 3.000],  loss: 10.555280, mse: 3824.908892, mean_q: 38.931044, mean_eps: 0.566788
  72386/150000: episode: 735, duration: 0.910s, episode steps: 119, steps per second: 131, episode reward: -33.240, mean reward: -0.279 [-100.000, 17.391], mean action: 1.471 [0.000, 3.000],  loss: 11.054006, mse: 3832.939074, mean_q: 39.744490, mean_eps: 0.566044
  72497/150000: episode: 736, duration: 0.808s, episode steps: 111, steps per second: 137, episode reward: 27.309, mean reward:  0.246 [-100.000, 23.353], mean action: 1.631 [0.000, 3.000],  loss: 9.347983, mse: 3800.726413, mean_q: 39.778123, mean_eps: 0.565354
  72590/150000: episode: 737, duration: 0.670s, episode steps:  93, steps per second: 139, episode reward: -72.343, mean reward: -0.778 [-100.000, 11.939], mean action: 1.645 [0.000, 3.000],  loss: 9.786165, mse: 3906.560258, mean_q: 41.003987, mean_eps: 0.564742
  72705/150000: episode: 738, duration: 0.891s, episode steps: 115, steps per second: 129, episode reward: -41.049, mean reward: -0.357 [-100.000, 10.805], mean action: 1.609 [0.000, 3.000],  loss: 7.161940, mse: 3975.081615, mean_q: 41.565726, mean_eps: 0.564118
  72896/150000: episode: 739, duration: 1.439s, episode steps: 191, steps per second: 133, episode reward: -191.544, mean reward: -1.003 [-100.000, 21.482], mean action: 1.670 [0.000, 3.000],  loss: 13.654559, mse: 3840.823750, mean_q: 40.952847, mean_eps: 0.563200
  72995/150000: episode: 740, duration: 0.767s, episode steps:  99, steps per second: 129, episode reward: -75.967, mean reward: -0.767 [-100.000,  9.319], mean action: 1.667 [0.000, 3.000],  loss: 13.230804, mse: 3928.567292, mean_q: 41.547858, mean_eps: 0.562330
  73090/150000: episode: 741, duration: 0.710s, episode steps:  95, steps per second: 134, episode reward: -57.025, mean reward: -0.600 [-100.000, 13.403], mean action: 1.895 [0.000, 3.000],  loss: 10.382690, mse: 3893.658360, mean_q: 39.776065, mean_eps: 0.561748
  73179/150000: episode: 742, duration: 0.673s, episode steps:  89, steps per second: 132, episode reward: -51.724, mean reward: -0.581 [-100.000,  7.883], mean action: 1.753 [0.000, 3.000],  loss: 6.364586, mse: 3890.947164, mean_q: 41.263211, mean_eps: 0.561196
  73260/150000: episode: 743, duration: 0.627s, episode steps:  81, steps per second: 129, episode reward: -8.029, mean reward: -0.099 [-100.000, 20.186], mean action: 1.741 [0.000, 3.000],  loss: 10.197853, mse: 4088.699816, mean_q: 42.577808, mean_eps: 0.560686
  73943/150000: episode: 744, duration: 5.700s, episode steps: 683, steps per second: 120, episode reward: -111.738, mean reward: -0.164 [-100.000, 37.477], mean action: 1.839 [0.000, 3.000],  loss: 8.782904, mse: 3885.695160, mean_q: 39.925446, mean_eps: 0.558394
  74070/150000: episode: 745, duration: 0.992s, episode steps: 127, steps per second: 128, episode reward: -273.855, mean reward: -2.156 [-100.000, 62.818], mean action: 1.520 [0.000, 3.000],  loss: 8.208377, mse: 3810.863448, mean_q: 39.160078, mean_eps: 0.555964
  74191/150000: episode: 746, duration: 0.887s, episode steps: 121, steps per second: 136, episode reward: -113.414, mean reward: -0.937 [-100.000,  5.300], mean action: 1.719 [0.000, 3.000],  loss: 8.030590, mse: 3847.894134, mean_q: 39.745765, mean_eps: 0.555220
  74286/150000: episode: 747, duration: 0.722s, episode steps:  95, steps per second: 132, episode reward: -60.095, mean reward: -0.633 [-100.000,  6.296], mean action: 1.653 [0.000, 3.000],  loss: 10.004880, mse: 3785.224743, mean_q: 38.828230, mean_eps: 0.554572
  74361/150000: episode: 748, duration: 0.572s, episode steps:  75, steps per second: 131, episode reward: -64.366, mean reward: -0.858 [-100.000, 10.191], mean action: 1.493 [0.000, 3.000],  loss: 7.722143, mse: 3845.105618, mean_q: 39.374293, mean_eps: 0.554062
  74456/150000: episode: 749, duration: 0.690s, episode steps:  95, steps per second: 138, episode reward: -122.833, mean reward: -1.293 [-100.000, 13.094], mean action: 1.600 [0.000, 3.000],  loss: 8.108849, mse: 3844.047176, mean_q: 38.862881, mean_eps: 0.553552
  74535/150000: episode: 750, duration: 0.579s, episode steps:  79, steps per second: 136, episode reward: -32.321, mean reward: -0.409 [-100.000, 17.106], mean action: 2.013 [0.000, 3.000],  loss: 8.109407, mse: 3826.514231, mean_q: 39.637824, mean_eps: 0.553030
  74643/150000: episode: 751, duration: 0.843s, episode steps: 108, steps per second: 128, episode reward: -61.734, mean reward: -0.572 [-100.000, 12.165], mean action: 1.546 [0.000, 3.000],  loss: 10.318065, mse: 3898.665891, mean_q: 39.967489, mean_eps: 0.552469
  74775/150000: episode: 752, duration: 0.956s, episode steps: 132, steps per second: 138, episode reward: -35.728, mean reward: -0.271 [-100.000,  9.350], mean action: 1.523 [0.000, 3.000],  loss: 10.132876, mse: 3780.883430, mean_q: 38.441355, mean_eps: 0.551749
  74908/150000: episode: 753, duration: 1.014s, episode steps: 133, steps per second: 131, episode reward: -175.804, mean reward: -1.322 [-100.000, 17.981], mean action: 1.827 [0.000, 3.000],  loss: 6.993495, mse: 3816.585827, mean_q: 38.739503, mean_eps: 0.550954
  75006/150000: episode: 754, duration: 0.724s, episode steps:  98, steps per second: 135, episode reward: -37.918, mean reward: -0.387 [-100.000, 16.115], mean action: 1.541 [0.000, 3.000],  loss: 8.610000, mse: 3818.865738, mean_q: 38.765972, mean_eps: 0.550261
  75124/150000: episode: 755, duration: 0.861s, episode steps: 118, steps per second: 137, episode reward: -104.743, mean reward: -0.888 [-100.000,  7.654], mean action: 1.492 [0.000, 3.000],  loss: 10.958949, mse: 3810.650283, mean_q: 40.627716, mean_eps: 0.549613
  75205/150000: episode: 756, duration: 0.632s, episode steps:  81, steps per second: 128, episode reward: -58.150, mean reward: -0.718 [-100.000,  6.845], mean action: 1.691 [0.000, 3.000],  loss: 9.912328, mse: 3831.222439, mean_q: 40.682092, mean_eps: 0.549016
  75308/150000: episode: 757, duration: 0.764s, episode steps: 103, steps per second: 135, episode reward: -30.536, mean reward: -0.296 [-100.000, 11.882], mean action: 1.718 [0.000, 3.000],  loss: 8.252862, mse: 3788.809703, mean_q: 38.847935, mean_eps: 0.548464
  75412/150000: episode: 758, duration: 0.770s, episode steps: 104, steps per second: 135, episode reward: -151.669, mean reward: -1.458 [-100.000,  8.291], mean action: 1.731 [0.000, 3.000],  loss: 7.015480, mse: 3864.203738, mean_q: 40.173300, mean_eps: 0.547843
  75537/150000: episode: 759, duration: 0.955s, episode steps: 125, steps per second: 131, episode reward: -202.106, mean reward: -1.617 [-100.000, 42.265], mean action: 1.624 [0.000, 3.000],  loss: 10.924343, mse: 3889.432904, mean_q: 39.693444, mean_eps: 0.547156
  76537/150000: episode: 760, duration: 8.463s, episode steps: 1000, steps per second: 118, episode reward:  6.966, mean reward:  0.007 [-24.417, 24.821], mean action: 1.747 [0.000, 3.000],  loss: 9.681929, mse: 3875.673541, mean_q: 39.638831, mean_eps: 0.543781
  76630/150000: episode: 761, duration: 0.680s, episode steps:  93, steps per second: 137, episode reward: -87.806, mean reward: -0.944 [-100.000, 13.512], mean action: 1.484 [0.000, 3.000],  loss: 8.707861, mse: 3876.285920, mean_q: 38.838973, mean_eps: 0.540502
  76720/150000: episode: 762, duration: 0.656s, episode steps:  90, steps per second: 137, episode reward: -22.279, mean reward: -0.248 [-100.000, 11.679], mean action: 1.744 [0.000, 3.000],  loss: 13.509476, mse: 3849.393821, mean_q: 38.654901, mean_eps: 0.539953
  76960/150000: episode: 763, duration: 1.825s, episode steps: 240, steps per second: 131, episode reward: -241.401, mean reward: -1.006 [-100.000, 15.876], mean action: 1.671 [0.000, 3.000],  loss: 11.078591, mse: 3897.821635, mean_q: 38.639229, mean_eps: 0.538963
  77072/150000: episode: 764, duration: 0.900s, episode steps: 112, steps per second: 125, episode reward: -57.496, mean reward: -0.513 [-100.000,  7.943], mean action: 1.714 [0.000, 3.000],  loss: 8.623893, mse: 3880.375065, mean_q: 37.882747, mean_eps: 0.537907
  77187/150000: episode: 765, duration: 0.857s, episode steps: 115, steps per second: 134, episode reward: -138.850, mean reward: -1.207 [-100.000,  6.589], mean action: 1.661 [0.000, 3.000],  loss: 8.782217, mse: 3962.554927, mean_q: 39.125291, mean_eps: 0.537226
  77310/150000: episode: 766, duration: 0.911s, episode steps: 123, steps per second: 135, episode reward: -84.252, mean reward: -0.685 [-100.000,  6.460], mean action: 1.382 [0.000, 3.000],  loss: 8.704070, mse: 4000.896524, mean_q: 40.814783, mean_eps: 0.536512
  77451/150000: episode: 767, duration: 1.077s, episode steps: 141, steps per second: 131, episode reward: 43.896, mean reward:  0.311 [-100.000, 13.495], mean action: 1.816 [0.000, 3.000],  loss: 6.888358, mse: 4048.056783, mean_q: 40.599736, mean_eps: 0.535720
  77541/150000: episode: 768, duration: 0.658s, episode steps:  90, steps per second: 137, episode reward: -59.563, mean reward: -0.662 [-100.000, 17.741], mean action: 1.933 [0.000, 3.000],  loss: 10.090047, mse: 3937.939499, mean_q: 39.692463, mean_eps: 0.535027
  77652/150000: episode: 769, duration: 1.074s, episode steps: 111, steps per second: 103, episode reward: -59.298, mean reward: -0.534 [-100.000, 10.679], mean action: 1.505 [0.000, 3.000],  loss: 6.753343, mse: 3990.559023, mean_q: 40.210649, mean_eps: 0.534424
  77800/150000: episode: 770, duration: 1.280s, episode steps: 148, steps per second: 116, episode reward: -36.957, mean reward: -0.250 [-100.000, 11.100], mean action: 1.716 [0.000, 3.000],  loss: 9.442422, mse: 3943.371663, mean_q: 38.732211, mean_eps: 0.533647
  77888/150000: episode: 771, duration: 0.734s, episode steps:  88, steps per second: 120, episode reward: -65.890, mean reward: -0.749 [-100.000,  9.482], mean action: 1.773 [0.000, 3.000],  loss: 7.313065, mse: 3939.423961, mean_q: 38.581443, mean_eps: 0.532939
  78027/150000: episode: 772, duration: 1.078s, episode steps: 139, steps per second: 129, episode reward: -38.935, mean reward: -0.280 [-100.000, 10.603], mean action: 1.597 [0.000, 3.000],  loss: 7.065242, mse: 3991.983103, mean_q: 39.184136, mean_eps: 0.532258
  78393/150000: episode: 773, duration: 2.856s, episode steps: 366, steps per second: 128, episode reward: -68.202, mean reward: -0.186 [-100.000, 16.063], mean action: 1.760 [0.000, 3.000],  loss: 9.437607, mse: 4058.346185, mean_q: 40.056350, mean_eps: 0.530743
  78486/150000: episode: 774, duration: 0.712s, episode steps:  93, steps per second: 131, episode reward: -34.459, mean reward: -0.371 [-100.000, 41.774], mean action: 1.667 [0.000, 3.000],  loss: 11.558355, mse: 3981.879626, mean_q: 40.500556, mean_eps: 0.529366
  78647/150000: episode: 775, duration: 1.173s, episode steps: 161, steps per second: 137, episode reward:  6.121, mean reward:  0.038 [-100.000, 14.793], mean action: 1.671 [0.000, 3.000],  loss: 7.361027, mse: 4063.611891, mean_q: 39.516508, mean_eps: 0.528604
  78788/150000: episode: 776, duration: 1.067s, episode steps: 141, steps per second: 132, episode reward: -27.990, mean reward: -0.199 [-100.000, 19.400], mean action: 1.582 [0.000, 3.000],  loss: 10.127782, mse: 4049.961145, mean_q: 40.045765, mean_eps: 0.527698
  78902/150000: episode: 777, duration: 0.840s, episode steps: 114, steps per second: 136, episode reward: -83.395, mean reward: -0.732 [-100.000, 17.815], mean action: 1.746 [0.000, 3.000],  loss: 9.560578, mse: 4029.504797, mean_q: 39.446603, mean_eps: 0.526933
  79072/150000: episode: 778, duration: 1.281s, episode steps: 170, steps per second: 133, episode reward: -32.778, mean reward: -0.193 [-100.000,  9.140], mean action: 1.788 [0.000, 3.000],  loss: 6.888561, mse: 4039.440523, mean_q: 40.510414, mean_eps: 0.526081
  79215/150000: episode: 779, duration: 1.047s, episode steps: 143, steps per second: 137, episode reward: 13.250, mean reward:  0.093 [-100.000, 56.751], mean action: 1.825 [0.000, 3.000],  loss: 10.426502, mse: 4026.060885, mean_q: 39.345768, mean_eps: 0.525142
  79338/150000: episode: 780, duration: 0.955s, episode steps: 123, steps per second: 129, episode reward: -151.279, mean reward: -1.230 [-100.000,  7.490], mean action: 1.740 [0.000, 3.000],  loss: 9.300468, mse: 4082.471241, mean_q: 40.616807, mean_eps: 0.524344
  79460/150000: episode: 781, duration: 1.056s, episode steps: 122, steps per second: 116, episode reward: -103.982, mean reward: -0.852 [-100.000, 11.878], mean action: 1.582 [0.000, 3.000],  loss: 13.066042, mse: 4056.454812, mean_q: 40.225183, mean_eps: 0.523609
  80095/150000: episode: 782, duration: 5.553s, episode steps: 635, steps per second: 114, episode reward: -154.391, mean reward: -0.243 [-100.000, 19.943], mean action: 1.728 [0.000, 3.000],  loss: 9.424991, mse: 4006.193230, mean_q: 40.244110, mean_eps: 0.521338
  80258/150000: episode: 783, duration: 1.185s, episode steps: 163, steps per second: 138, episode reward: 12.428, mean reward:  0.076 [-100.000, 11.283], mean action: 1.436 [0.000, 3.000],  loss: 10.730350, mse: 4043.827175, mean_q: 40.153597, mean_eps: 0.518944
  81258/150000: episode: 784, duration: 8.666s, episode steps: 1000, steps per second: 115, episode reward: 23.699, mean reward:  0.024 [-24.522, 26.179], mean action: 1.601 [0.000, 3.000],  loss: 10.674989, mse: 4010.728205, mean_q: 39.350901, mean_eps: 0.515455
  81358/150000: episode: 785, duration: 0.799s, episode steps: 100, steps per second: 125, episode reward:  0.982, mean reward:  0.010 [-100.000, 11.693], mean action: 1.770 [0.000, 3.000],  loss: 11.875566, mse: 4112.633945, mean_q: 39.879727, mean_eps: 0.512155
  82358/150000: episode: 786, duration: 9.383s, episode steps: 1000, steps per second: 107, episode reward: -97.122, mean reward: -0.097 [-21.761, 17.704], mean action: 1.767 [0.000, 3.000],  loss: 8.910453, mse: 4013.944453, mean_q: 38.638239, mean_eps: 0.508855
  83358/150000: episode: 787, duration: 9.129s, episode steps: 1000, steps per second: 110, episode reward: -4.498, mean reward: -0.004 [-23.258, 22.597], mean action: 1.754 [0.000, 3.000],  loss: 8.722892, mse: 3973.537782, mean_q: 37.884710, mean_eps: 0.502855
  83495/150000: episode: 788, duration: 1.109s, episode steps: 137, steps per second: 123, episode reward:  5.068, mean reward:  0.037 [-100.000, 18.020], mean action: 1.745 [0.000, 3.000],  loss: 7.177494, mse: 3934.044002, mean_q: 36.733031, mean_eps: 0.499444
  83624/150000: episode: 789, duration: 0.956s, episode steps: 129, steps per second: 135, episode reward: -52.609, mean reward: -0.408 [-100.000, 10.757], mean action: 1.667 [0.000, 3.000],  loss: 8.740031, mse: 4001.703926, mean_q: 38.524306, mean_eps: 0.498646
  83769/150000: episode: 790, duration: 1.122s, episode steps: 145, steps per second: 129, episode reward: -67.203, mean reward: -0.463 [-100.000, 12.485], mean action: 1.566 [0.000, 3.000],  loss: 7.728848, mse: 3911.982341, mean_q: 36.852781, mean_eps: 0.497824
  83907/150000: episode: 791, duration: 1.090s, episode steps: 138, steps per second: 127, episode reward:  0.842, mean reward:  0.006 [-100.000, 21.898], mean action: 1.761 [0.000, 3.000],  loss: 9.611499, mse: 3922.655670, mean_q: 37.748653, mean_eps: 0.496975
  84907/150000: episode: 792, duration: 8.911s, episode steps: 1000, steps per second: 112, episode reward: -62.018, mean reward: -0.062 [-24.061, 21.561], mean action: 1.777 [0.000, 3.000],  loss: 7.639878, mse: 3957.137047, mean_q: 37.644403, mean_eps: 0.493561
  85011/150000: episode: 793, duration: 0.814s, episode steps: 104, steps per second: 128, episode reward: -6.903, mean reward: -0.066 [-100.000, 10.175], mean action: 1.740 [0.000, 3.000],  loss: 8.344795, mse: 3939.877711, mean_q: 37.526092, mean_eps: 0.490249
  86011/150000: episode: 794, duration: 8.407s, episode steps: 1000, steps per second: 119, episode reward: 71.062, mean reward:  0.071 [-22.372, 23.684], mean action: 1.299 [0.000, 3.000],  loss: 7.641131, mse: 3883.475984, mean_q: 37.546178, mean_eps: 0.486937
  86105/150000: episode: 795, duration: 0.691s, episode steps:  94, steps per second: 136, episode reward: -72.643, mean reward: -0.773 [-100.000,  8.537], mean action: 1.777 [0.000, 3.000],  loss: 9.266494, mse: 3973.761921, mean_q: 39.698149, mean_eps: 0.483655
  86245/150000: episode: 796, duration: 1.066s, episode steps: 140, steps per second: 131, episode reward: 36.794, mean reward:  0.263 [-100.000, 22.600], mean action: 1.636 [0.000, 3.000],  loss: 6.872979, mse: 3956.941619, mean_q: 38.608784, mean_eps: 0.482953
  86388/150000: episode: 797, duration: 1.105s, episode steps: 143, steps per second: 129, episode reward: -111.581, mean reward: -0.780 [-100.000, 16.918], mean action: 1.692 [0.000, 3.000],  loss: 11.035779, mse: 3911.041850, mean_q: 37.890275, mean_eps: 0.482104
  86590/150000: episode: 798, duration: 1.549s, episode steps: 202, steps per second: 130, episode reward: 44.148, mean reward:  0.219 [-100.000, 13.057], mean action: 1.688 [0.000, 3.000],  loss: 8.022199, mse: 3892.696376, mean_q: 38.482695, mean_eps: 0.481069
  86728/150000: episode: 799, duration: 1.019s, episode steps: 138, steps per second: 135, episode reward: -23.872, mean reward: -0.173 [-100.000, 21.055], mean action: 1.688 [0.000, 3.000],  loss: 8.169594, mse: 3908.328849, mean_q: 38.400817, mean_eps: 0.480049
  86824/150000: episode: 800, duration: 0.711s, episode steps:  96, steps per second: 135, episode reward: 36.695, mean reward:  0.382 [-100.000, 13.582], mean action: 1.688 [0.000, 3.000],  loss: 8.438017, mse: 3909.265856, mean_q: 38.261642, mean_eps: 0.479347
  87213/150000: episode: 801, duration: 3.233s, episode steps: 389, steps per second: 120, episode reward: -259.678, mean reward: -0.668 [-100.000, 32.916], mean action: 1.686 [0.000, 3.000],  loss: 9.129511, mse: 3923.434714, mean_q: 38.658248, mean_eps: 0.477892
  87320/150000: episode: 802, duration: 0.954s, episode steps: 107, steps per second: 112, episode reward: -61.113, mean reward: -0.571 [-100.000, 10.066], mean action: 1.720 [0.000, 3.000],  loss: 14.030653, mse: 3942.572946, mean_q: 38.906271, mean_eps: 0.476404
  87395/150000: episode: 803, duration: 0.689s, episode steps:  75, steps per second: 109, episode reward: -42.579, mean reward: -0.568 [-100.000, 11.979], mean action: 1.813 [0.000, 3.000],  loss: 8.297260, mse: 3907.759645, mean_q: 38.112805, mean_eps: 0.475858
  87919/150000: episode: 804, duration: 4.457s, episode steps: 524, steps per second: 118, episode reward: -130.711, mean reward: -0.249 [-100.000, 68.753], mean action: 1.683 [0.000, 3.000],  loss: 8.457769, mse: 3900.159689, mean_q: 38.594640, mean_eps: 0.474061
  88081/150000: episode: 805, duration: 1.188s, episode steps: 162, steps per second: 136, episode reward: -102.422, mean reward: -0.632 [-100.000, 13.502], mean action: 1.660 [0.000, 3.000],  loss: 9.792532, mse: 3922.444137, mean_q: 38.915876, mean_eps: 0.472003
  88231/150000: episode: 806, duration: 1.211s, episode steps: 150, steps per second: 124, episode reward: 12.232, mean reward:  0.082 [-100.000, 12.352], mean action: 1.800 [0.000, 3.000],  loss: 11.308621, mse: 3888.348242, mean_q: 38.843104, mean_eps: 0.471067
  88377/150000: episode: 807, duration: 1.112s, episode steps: 146, steps per second: 131, episode reward: -9.178, mean reward: -0.063 [-100.000, 18.096], mean action: 1.801 [0.000, 3.000],  loss: 9.108236, mse: 3918.145090, mean_q: 39.197043, mean_eps: 0.470179
  88483/150000: episode: 808, duration: 0.834s, episode steps: 106, steps per second: 127, episode reward: -54.071, mean reward: -0.510 [-100.000, 12.875], mean action: 1.575 [0.000, 3.000],  loss: 8.489172, mse: 3881.185443, mean_q: 39.408622, mean_eps: 0.469423
  88600/150000: episode: 809, duration: 0.855s, episode steps: 117, steps per second: 137, episode reward: -66.508, mean reward: -0.568 [-100.000, 12.690], mean action: 1.436 [0.000, 3.000],  loss: 8.354216, mse: 3973.517902, mean_q: 38.965361, mean_eps: 0.468754
  88922/150000: episode: 810, duration: 2.461s, episode steps: 322, steps per second: 131, episode reward: -255.462, mean reward: -0.793 [-100.000, 11.231], mean action: 1.550 [0.000, 3.000],  loss: 11.284419, mse: 3895.234041, mean_q: 39.129441, mean_eps: 0.467437
  89056/150000: episode: 811, duration: 1.037s, episode steps: 134, steps per second: 129, episode reward: -35.665, mean reward: -0.266 [-100.000, 12.164], mean action: 1.597 [0.000, 3.000],  loss: 11.944268, mse: 3870.046055, mean_q: 38.869309, mean_eps: 0.466069
  89172/150000: episode: 812, duration: 0.855s, episode steps: 116, steps per second: 136, episode reward: -42.996, mean reward: -0.371 [-100.000, 10.308], mean action: 1.560 [0.000, 3.000],  loss: 7.904855, mse: 3846.816158, mean_q: 39.805585, mean_eps: 0.465319
  89318/150000: episode: 813, duration: 1.109s, episode steps: 146, steps per second: 132, episode reward: -17.005, mean reward: -0.116 [-100.000, 24.363], mean action: 1.623 [0.000, 3.000],  loss: 8.984159, mse: 3796.247953, mean_q: 39.709549, mean_eps: 0.464533
  89462/150000: episode: 814, duration: 1.067s, episode steps: 144, steps per second: 135, episode reward: -78.190, mean reward: -0.543 [-100.000,  6.820], mean action: 1.424 [0.000, 3.000],  loss: 10.970465, mse: 3810.244041, mean_q: 39.115524, mean_eps: 0.463663
  90462/150000: episode: 815, duration: 8.797s, episode steps: 1000, steps per second: 114, episode reward: -45.100, mean reward: -0.045 [-21.431, 21.660], mean action: 1.732 [0.000, 3.000],  loss: 8.968589, mse: 3857.419198, mean_q: 39.636840, mean_eps: 0.460231
  90580/150000: episode: 816, duration: 0.881s, episode steps: 118, steps per second: 134, episode reward: -95.323, mean reward: -0.808 [-100.000,  8.273], mean action: 1.542 [0.000, 3.000],  loss: 11.141621, mse: 3853.551694, mean_q: 40.158525, mean_eps: 0.456877
  90709/150000: episode: 817, duration: 0.955s, episode steps: 129, steps per second: 135, episode reward: -45.790, mean reward: -0.355 [-100.000, 11.017], mean action: 1.605 [0.000, 3.000],  loss: 9.190325, mse: 3878.018445, mean_q: 40.499322, mean_eps: 0.456136
  90881/150000: episode: 818, duration: 1.315s, episode steps: 172, steps per second: 131, episode reward:  6.615, mean reward:  0.038 [-100.000, 14.569], mean action: 1.663 [0.000, 3.000],  loss: 10.216713, mse: 3879.415553, mean_q: 39.959480, mean_eps: 0.455233
  91881/150000: episode: 819, duration: 8.474s, episode steps: 1000, steps per second: 118, episode reward:  5.724, mean reward:  0.006 [-19.868, 21.717], mean action: 1.453 [0.000, 3.000],  loss: 8.776302, mse: 3901.335387, mean_q: 39.834973, mean_eps: 0.451717
  92268/150000: episode: 820, duration: 3.079s, episode steps: 387, steps per second: 126, episode reward: -10.312, mean reward: -0.027 [-100.000, 19.294], mean action: 1.731 [0.000, 3.000],  loss: 7.900562, mse: 3893.189829, mean_q: 40.017701, mean_eps: 0.447556
  92379/150000: episode: 821, duration: 0.806s, episode steps: 111, steps per second: 138, episode reward: -37.185, mean reward: -0.335 [-100.000,  8.310], mean action: 1.784 [0.000, 3.000],  loss: 6.906740, mse: 3875.868124, mean_q: 39.899135, mean_eps: 0.446062
  92519/150000: episode: 822, duration: 1.112s, episode steps: 140, steps per second: 126, episode reward: -30.870, mean reward: -0.221 [-100.000, 12.528], mean action: 1.536 [0.000, 3.000],  loss: 8.346601, mse: 3852.288065, mean_q: 39.445353, mean_eps: 0.445309
  92694/150000: episode: 823, duration: 1.307s, episode steps: 175, steps per second: 134, episode reward:  0.432, mean reward:  0.002 [-100.000, 13.708], mean action: 1.697 [0.000, 3.000],  loss: 9.015421, mse: 3868.238051, mean_q: 39.578946, mean_eps: 0.444364
  93694/150000: episode: 824, duration: 8.756s, episode steps: 1000, steps per second: 114, episode reward: -17.597, mean reward: -0.018 [-24.768, 16.871], mean action: 1.729 [0.000, 3.000],  loss: 9.253001, mse: 3822.821503, mean_q: 39.447385, mean_eps: 0.440839
  93908/150000: episode: 825, duration: 1.694s, episode steps: 214, steps per second: 126, episode reward: -45.590, mean reward: -0.213 [-100.000, 18.868], mean action: 1.743 [0.000, 3.000],  loss: 9.546650, mse: 3795.091050, mean_q: 39.314915, mean_eps: 0.437197
  94072/150000: episode: 826, duration: 1.253s, episode steps: 164, steps per second: 131, episode reward: -9.524, mean reward: -0.058 [-100.000, 20.454], mean action: 1.622 [0.000, 3.000],  loss: 8.795584, mse: 3864.575511, mean_q: 40.342651, mean_eps: 0.436063
  94261/150000: episode: 827, duration: 1.411s, episode steps: 189, steps per second: 134, episode reward: -193.162, mean reward: -1.022 [-100.000,  5.310], mean action: 1.683 [0.000, 3.000],  loss: 8.643213, mse: 3892.323484, mean_q: 40.318801, mean_eps: 0.435004
  94503/150000: episode: 828, duration: 1.841s, episode steps: 242, steps per second: 131, episode reward: -14.571, mean reward: -0.060 [-100.000, 22.860], mean action: 1.769 [0.000, 3.000],  loss: 9.560176, mse: 3871.697670, mean_q: 40.776111, mean_eps: 0.433711
  95503/150000: episode: 829, duration: 9.221s, episode steps: 1000, steps per second: 108, episode reward: 20.784, mean reward:  0.021 [-23.990, 23.688], mean action: 1.628 [0.000, 3.000],  loss: 8.589220, mse: 3857.467300, mean_q: 40.413983, mean_eps: 0.429985
  95669/150000: episode: 830, duration: 1.306s, episode steps: 166, steps per second: 127, episode reward: -10.463, mean reward: -0.063 [-100.000, 17.660], mean action: 1.705 [0.000, 3.000],  loss: 8.080466, mse: 3824.796500, mean_q: 39.792084, mean_eps: 0.426487
  95823/150000: episode: 831, duration: 1.259s, episode steps: 154, steps per second: 122, episode reward: -23.406, mean reward: -0.152 [-100.000, 15.093], mean action: 1.701 [0.000, 3.000],  loss: 8.882962, mse: 3804.645433, mean_q: 39.761971, mean_eps: 0.425527
  96427/150000: episode: 832, duration: 4.731s, episode steps: 604, steps per second: 128, episode reward: -164.997, mean reward: -0.273 [-100.000, 14.374], mean action: 1.502 [0.000, 3.000],  loss: 9.459389, mse: 3792.789260, mean_q: 39.818321, mean_eps: 0.423253
  96563/150000: episode: 833, duration: 1.022s, episode steps: 136, steps per second: 133, episode reward: -56.756, mean reward: -0.417 [-100.000,  7.336], mean action: 1.691 [0.000, 3.000],  loss: 8.288968, mse: 3767.182355, mean_q: 40.054921, mean_eps: 0.421033
  96676/150000: episode: 834, duration: 0.852s, episode steps: 113, steps per second: 133, episode reward: 14.513, mean reward:  0.128 [-100.000, 17.385], mean action: 1.814 [0.000, 3.000],  loss: 8.102884, mse: 3775.809801, mean_q: 39.969005, mean_eps: 0.420286
  97676/150000: episode: 835, duration: 8.053s, episode steps: 1000, steps per second: 124, episode reward: -7.291, mean reward: -0.007 [-20.522, 24.025], mean action: 1.287 [0.000, 3.000],  loss: 8.249301, mse: 3738.386404, mean_q: 39.899370, mean_eps: 0.416947
  98676/150000: episode: 836, duration: 8.101s, episode steps: 1000, steps per second: 123, episode reward: 17.526, mean reward:  0.018 [-22.787, 23.468], mean action: 1.297 [0.000, 3.000],  loss: 8.613292, mse: 3642.770135, mean_q: 38.718908, mean_eps: 0.410947
  98848/150000: episode: 837, duration: 1.293s, episode steps: 172, steps per second: 133, episode reward: -23.339, mean reward: -0.136 [-100.000, 12.500], mean action: 1.523 [0.000, 3.000],  loss: 7.963027, mse: 3586.202090, mean_q: 37.768927, mean_eps: 0.407431
  99848/150000: episode: 838, duration: 8.345s, episode steps: 1000, steps per second: 120, episode reward: -55.876, mean reward: -0.056 [-22.961, 18.229], mean action: 1.869 [0.000, 3.000],  loss: 8.933002, mse: 3586.914814, mean_q: 37.825416, mean_eps: 0.403915
 100344/150000: episode: 839, duration: 3.856s, episode steps: 496, steps per second: 129, episode reward: -75.553, mean reward: -0.152 [-100.000, 20.567], mean action: 1.312 [0.000, 3.000],  loss: 9.416562, mse: 3563.661349, mean_q: 38.326825, mean_eps: 0.399427
 101344/150000: episode: 840, duration: 8.999s, episode steps: 1000, steps per second: 111, episode reward: 29.806, mean reward:  0.030 [-24.442, 22.698], mean action: 1.817 [0.000, 3.000],  loss: 9.353111, mse: 3579.401415, mean_q: 38.146529, mean_eps: 0.394939
 102344/150000: episode: 841, duration: 8.774s, episode steps: 1000, steps per second: 114, episode reward: 31.060, mean reward:  0.031 [-22.563, 23.814], mean action: 1.426 [0.000, 3.000],  loss: 9.076016, mse: 3408.136194, mean_q: 37.033621, mean_eps: 0.388939
 103344/150000: episode: 842, duration: 8.487s, episode steps: 1000, steps per second: 118, episode reward: 24.457, mean reward:  0.024 [-26.133, 27.159], mean action: 1.575 [0.000, 3.000],  loss: 8.631147, mse: 3322.757415, mean_q: 36.925555, mean_eps: 0.382939
 103480/150000: episode: 843, duration: 1.125s, episode steps: 136, steps per second: 121, episode reward:  1.041, mean reward:  0.008 [-100.000, 15.669], mean action: 1.794 [0.000, 3.000],  loss: 8.263732, mse: 3306.306992, mean_q: 36.147892, mean_eps: 0.379531
 104480/150000: episode: 844, duration: 8.359s, episode steps: 1000, steps per second: 120, episode reward: -10.813, mean reward: -0.011 [-23.949, 24.302], mean action: 1.234 [0.000, 3.000],  loss: 9.010122, mse: 3286.474688, mean_q: 36.728766, mean_eps: 0.376123
 105480/150000: episode: 845, duration: 8.057s, episode steps: 1000, steps per second: 124, episode reward: -135.906, mean reward: -0.136 [-13.787, 14.438], mean action: 1.730 [0.000, 3.000],  loss: 9.897028, mse: 3231.423435, mean_q: 36.150072, mean_eps: 0.370123
 106282/150000: episode: 846, duration: 6.548s, episode steps: 802, steps per second: 122, episode reward: -447.144, mean reward: -0.558 [-100.000, 20.895], mean action: 1.709 [0.000, 3.000],  loss: 8.690970, mse: 3218.807392, mean_q: 36.771290, mean_eps: 0.364717
 107282/150000: episode: 847, duration: 8.299s, episode steps: 1000, steps per second: 120, episode reward: 132.218, mean reward:  0.132 [-21.315, 22.868], mean action: 1.428 [0.000, 3.000],  loss: 9.226412, mse: 3228.786194, mean_q: 37.184090, mean_eps: 0.359311
 107432/150000: episode: 848, duration: 1.109s, episode steps: 150, steps per second: 135, episode reward:  5.122, mean reward:  0.034 [-100.000, 16.880], mean action: 1.747 [0.000, 3.000],  loss: 10.808065, mse: 3153.009116, mean_q: 35.537472, mean_eps: 0.355861
 108432/150000: episode: 849, duration: 8.277s, episode steps: 1000, steps per second: 121, episode reward: -65.273, mean reward: -0.065 [-5.217,  5.197], mean action: 1.762 [0.000, 3.000],  loss: 10.043731, mse: 3101.382135, mean_q: 36.098995, mean_eps: 0.352411
 109432/150000: episode: 850, duration: 8.493s, episode steps: 1000, steps per second: 118, episode reward: 40.381, mean reward:  0.040 [-22.784, 22.369], mean action: 1.779 [0.000, 3.000],  loss: 8.558805, mse: 3002.142254, mean_q: 36.148459, mean_eps: 0.346411
 109576/150000: episode: 851, duration: 1.115s, episode steps: 144, steps per second: 129, episode reward: -10.011, mean reward: -0.070 [-100.000, 26.917], mean action: 1.660 [0.000, 3.000],  loss: 9.529360, mse: 2980.264788, mean_q: 36.311268, mean_eps: 0.342979
 110576/150000: episode: 852, duration: 8.408s, episode steps: 1000, steps per second: 119, episode reward: 60.777, mean reward:  0.061 [-22.749, 24.730], mean action: 2.225 [0.000, 3.000],  loss: 9.984195, mse: 2982.395377, mean_q: 35.921809, mean_eps: 0.339547
 110894/150000: episode: 853, duration: 2.412s, episode steps: 318, steps per second: 132, episode reward: -77.568, mean reward: -0.244 [-100.000, 13.364], mean action: 1.557 [0.000, 3.000],  loss: 9.958304, mse: 3046.585732, mean_q: 36.793305, mean_eps: 0.335593
 111894/150000: episode: 854, duration: 9.216s, episode steps: 1000, steps per second: 109, episode reward: -87.110, mean reward: -0.087 [-20.334, 12.283], mean action: 1.762 [0.000, 3.000],  loss: 9.097821, mse: 3038.022568, mean_q: 36.412424, mean_eps: 0.331639
 112008/150000: episode: 855, duration: 0.849s, episode steps: 114, steps per second: 134, episode reward: -27.600, mean reward: -0.242 [-100.000, 17.796], mean action: 1.825 [0.000, 3.000],  loss: 8.664354, mse: 2902.998680, mean_q: 35.107362, mean_eps: 0.328297
 113008/150000: episode: 856, duration: 8.047s, episode steps: 1000, steps per second: 124, episode reward: -66.505, mean reward: -0.067 [-5.335,  5.830], mean action: 1.760 [0.000, 3.000],  loss: 8.941895, mse: 3009.750994, mean_q: 36.944551, mean_eps: 0.324955
 114008/150000: episode: 857, duration: 8.213s, episode steps: 1000, steps per second: 122, episode reward: -34.375, mean reward: -0.034 [-24.315, 20.450], mean action: 1.671 [0.000, 3.000],  loss: 8.679035, mse: 2972.390495, mean_q: 37.122848, mean_eps: 0.318955
 115008/150000: episode: 858, duration: 8.517s, episode steps: 1000, steps per second: 117, episode reward: 72.913, mean reward:  0.073 [-23.894, 24.186], mean action: 1.966 [0.000, 3.000],  loss: 9.511298, mse: 2964.602447, mean_q: 37.374448, mean_eps: 0.312955
 116008/150000: episode: 859, duration: 8.723s, episode steps: 1000, steps per second: 115, episode reward: 39.925, mean reward:  0.040 [-24.005, 26.458], mean action: 1.621 [0.000, 3.000],  loss: 8.328736, mse: 2873.161146, mean_q: 36.637438, mean_eps: 0.306955
 117008/150000: episode: 860, duration: 8.779s, episode steps: 1000, steps per second: 114, episode reward: -50.193, mean reward: -0.050 [-4.627,  4.659], mean action: 1.777 [0.000, 3.000],  loss: 8.504493, mse: 2842.301237, mean_q: 36.782956, mean_eps: 0.300955
 118008/150000: episode: 861, duration: 8.521s, episode steps: 1000, steps per second: 117, episode reward: -119.735, mean reward: -0.120 [-13.526, 11.540], mean action: 1.764 [0.000, 3.000],  loss: 8.108828, mse: 2812.561064, mean_q: 36.964184, mean_eps: 0.294955
 119008/150000: episode: 862, duration: 9.288s, episode steps: 1000, steps per second: 108, episode reward: 120.368, mean reward:  0.120 [-23.529, 25.147], mean action: 1.788 [0.000, 3.000],  loss: 8.924232, mse: 2846.580633, mean_q: 37.266480, mean_eps: 0.288955
 120008/150000: episode: 863, duration: 10.192s, episode steps: 1000, steps per second:  98, episode reward: 25.178, mean reward:  0.025 [-21.154, 23.459], mean action: 1.829 [0.000, 3.000],  loss: 7.690245, mse: 2770.784521, mean_q: 36.770325, mean_eps: 0.282955
 120150/150000: episode: 864, duration: 1.229s, episode steps: 142, steps per second: 116, episode reward: 12.791, mean reward:  0.090 [-100.000, 10.084], mean action: 1.514 [0.000, 3.000],  loss: 8.234649, mse: 2687.197432, mean_q: 36.717176, mean_eps: 0.279529
 120451/150000: episode: 865, duration: 3.214s, episode steps: 301, steps per second:  94, episode reward: 252.096, mean reward:  0.838 [-24.671, 100.000], mean action: 1.947 [0.000, 3.000],  loss: 9.048153, mse: 2668.228950, mean_q: 36.410350, mean_eps: 0.278200
 120568/150000: episode: 866, duration: 1.322s, episode steps: 117, steps per second:  89, episode reward: 11.688, mean reward:  0.100 [-100.000, 12.064], mean action: 1.949 [0.000, 3.000],  loss: 6.660771, mse: 2636.158885, mean_q: 36.364981, mean_eps: 0.276946
 121568/150000: episode: 867, duration: 9.950s, episode steps: 1000, steps per second: 100, episode reward: -9.644, mean reward: -0.010 [-20.633, 21.380], mean action: 1.541 [0.000, 3.000],  loss: 8.221790, mse: 2618.627045, mean_q: 36.263191, mean_eps: 0.273595
 121667/150000: episode: 868, duration: 0.914s, episode steps:  99, steps per second: 108, episode reward: 32.511, mean reward:  0.328 [-100.000, 19.149], mean action: 1.838 [0.000, 3.000],  loss: 8.189252, mse: 2507.575913, mean_q: 35.540444, mean_eps: 0.270298
 122667/150000: episode: 869, duration: 9.517s, episode steps: 1000, steps per second: 105, episode reward: 73.309, mean reward:  0.073 [-21.472, 23.295], mean action: 1.491 [0.000, 3.000],  loss: 7.821093, mse: 2569.564830, mean_q: 35.876156, mean_eps: 0.267001
 123667/150000: episode: 870, duration: 9.625s, episode steps: 1000, steps per second: 104, episode reward: 93.358, mean reward:  0.093 [-21.383, 21.787], mean action: 1.572 [0.000, 3.000],  loss: 7.896400, mse: 2541.786421, mean_q: 35.811898, mean_eps: 0.261001
 124667/150000: episode: 871, duration: 9.918s, episode steps: 1000, steps per second: 101, episode reward: 109.810, mean reward:  0.110 [-21.469, 22.678], mean action: 1.400 [0.000, 3.000],  loss: 7.468043, mse: 2555.498680, mean_q: 36.346695, mean_eps: 0.255001
 125667/150000: episode: 872, duration: 9.513s, episode steps: 1000, steps per second: 105, episode reward: 93.980, mean reward:  0.094 [-21.508, 23.222], mean action: 1.626 [0.000, 3.000],  loss: 7.357253, mse: 2515.656011, mean_q: 36.368516, mean_eps: 0.249001
 126667/150000: episode: 873, duration: 9.712s, episode steps: 1000, steps per second: 103, episode reward: 101.289, mean reward:  0.101 [-21.523, 23.027], mean action: 1.288 [0.000, 3.000],  loss: 7.149315, mse: 2493.256069, mean_q: 36.581541, mean_eps: 0.243001
 127667/150000: episode: 874, duration: 8.860s, episode steps: 1000, steps per second: 113, episode reward: 110.764, mean reward:  0.111 [-24.083, 22.966], mean action: 1.214 [0.000, 3.000],  loss: 6.730657, mse: 2440.645529, mean_q: 36.293979, mean_eps: 0.237001
 128667/150000: episode: 875, duration: 8.890s, episode steps: 1000, steps per second: 112, episode reward: 113.166, mean reward:  0.113 [-20.377, 24.683], mean action: 1.218 [0.000, 3.000],  loss: 7.119063, mse: 2381.550067, mean_q: 35.871189, mean_eps: 0.231001
 129667/150000: episode: 876, duration: 10.027s, episode steps: 1000, steps per second: 100, episode reward: 101.618, mean reward:  0.102 [-20.118, 22.649], mean action: 1.209 [0.000, 3.000],  loss: 6.032422, mse: 2358.075751, mean_q: 35.647129, mean_eps: 0.225001
 130627/150000: episode: 877, duration: 8.711s, episode steps: 960, steps per second: 110, episode reward: 283.797, mean reward:  0.296 [-24.141, 100.000], mean action: 1.150 [0.000, 3.000],  loss: 5.531819, mse: 2364.027462, mean_q: 35.747642, mean_eps: 0.219121
 131627/150000: episode: 878, duration: 9.188s, episode steps: 1000, steps per second: 109, episode reward: 102.621, mean reward:  0.103 [-23.533, 22.956], mean action: 1.301 [0.000, 3.000],  loss: 5.872041, mse: 2408.596132, mean_q: 36.228505, mean_eps: 0.213241
 132065/150000: episode: 879, duration: 4.120s, episode steps: 438, steps per second: 106, episode reward: 232.270, mean reward:  0.530 [-14.527, 100.000], mean action: 1.795 [0.000, 3.000],  loss: 5.159050, mse: 2413.969957, mean_q: 36.154247, mean_eps: 0.208927
 133065/150000: episode: 880, duration: 9.568s, episode steps: 1000, steps per second: 105, episode reward: 105.730, mean reward:  0.106 [-23.140, 22.137], mean action: 1.220 [0.000, 3.000],  loss: 5.133146, mse: 2514.636446, mean_q: 36.726858, mean_eps: 0.204613
 134065/150000: episode: 881, duration: 8.943s, episode steps: 1000, steps per second: 112, episode reward: 151.710, mean reward:  0.152 [-24.198, 23.273], mean action: 0.984 [0.000, 3.000],  loss: 5.025178, mse: 2533.059202, mean_q: 36.766236, mean_eps: 0.198613
 135065/150000: episode: 882, duration: 9.077s, episode steps: 1000, steps per second: 110, episode reward: 89.120, mean reward:  0.089 [-18.737, 23.187], mean action: 1.350 [0.000, 3.000],  loss: 5.226225, mse: 2537.726607, mean_q: 36.745724, mean_eps: 0.192613
 135185/150000: episode: 883, duration: 0.948s, episode steps: 120, steps per second: 127, episode reward:  4.625, mean reward:  0.039 [-100.000, 10.169], mean action: 1.958 [0.000, 3.000],  loss: 7.447901, mse: 2536.260471, mean_q: 36.389043, mean_eps: 0.189253
 135365/150000: episode: 884, duration: 1.513s, episode steps: 180, steps per second: 119, episode reward: 21.897, mean reward:  0.122 [-100.000, 11.792], mean action: 1.628 [0.000, 3.000],  loss: 4.690062, mse: 2550.860273, mean_q: 36.635487, mean_eps: 0.188353
 136365/150000: episode: 885, duration: 9.119s, episode steps: 1000, steps per second: 110, episode reward: 104.330, mean reward:  0.104 [-20.582, 23.088], mean action: 1.117 [0.000, 3.000],  loss: 5.170723, mse: 2629.227666, mean_q: 37.587670, mean_eps: 0.184813
 137336/150000: episode: 886, duration: 9.029s, episode steps: 971, steps per second: 108, episode reward: 178.729, mean reward:  0.184 [-23.157, 100.000], mean action: 1.427 [0.000, 3.000],  loss: 4.911463, mse: 2569.864027, mean_q: 37.241831, mean_eps: 0.178900
 138336/150000: episode: 887, duration: 9.386s, episode steps: 1000, steps per second: 107, episode reward: 128.919, mean reward:  0.129 [-23.564, 23.432], mean action: 1.242 [0.000, 3.000],  loss: 4.958185, mse: 2474.673989, mean_q: 37.278356, mean_eps: 0.172987
 139336/150000: episode: 888, duration: 9.415s, episode steps: 1000, steps per second: 106, episode reward: 101.186, mean reward:  0.101 [-20.353, 22.489], mean action: 1.490 [0.000, 3.000],  loss: 4.124344, mse: 2416.333558, mean_q: 37.248700, mean_eps: 0.166987
 140268/150000: episode: 889, duration: 8.746s, episode steps: 932, steps per second: 107, episode reward: 248.873, mean reward:  0.267 [-20.357, 100.000], mean action: 0.940 [0.000, 3.000],  loss: 3.580552, mse: 2304.815777, mean_q: 36.532107, mean_eps: 0.161191
 140601/150000: episode: 890, duration: 2.777s, episode steps: 333, steps per second: 120, episode reward: 304.688, mean reward:  0.915 [-9.766, 100.000], mean action: 1.075 [0.000, 3.000],  loss: 4.113454, mse: 2338.824615, mean_q: 36.779050, mean_eps: 0.157396
 141423/150000: episode: 891, duration: 7.251s, episode steps: 822, steps per second: 113, episode reward: 185.073, mean reward:  0.225 [-18.874, 100.000], mean action: 1.212 [0.000, 3.000],  loss: 3.900638, mse: 2249.344991, mean_q: 36.196539, mean_eps: 0.153931
 141808/150000: episode: 892, duration: 3.243s, episode steps: 385, steps per second: 119, episode reward: 266.116, mean reward:  0.691 [-10.423, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 3.315475, mse: 2286.703139, mean_q: 36.576288, mean_eps: 0.150310
 142128/150000: episode: 893, duration: 2.713s, episode steps: 320, steps per second: 118, episode reward: 256.089, mean reward:  0.800 [-10.325, 100.000], mean action: 1.403 [0.000, 3.000],  loss: 3.679696, mse: 2326.739818, mean_q: 37.108796, mean_eps: 0.148195
 142501/150000: episode: 894, duration: 3.141s, episode steps: 373, steps per second: 119, episode reward: 265.873, mean reward:  0.713 [-11.265, 100.000], mean action: 1.212 [0.000, 3.000],  loss: 3.196909, mse: 2382.385851, mean_q: 37.921198, mean_eps: 0.146116
 142912/150000: episode: 895, duration: 3.511s, episode steps: 411, steps per second: 117, episode reward: 274.473, mean reward:  0.668 [-19.493, 100.000], mean action: 1.204 [0.000, 3.000],  loss: 4.362977, mse: 2344.376342, mean_q: 37.541647, mean_eps: 0.143764
 143258/150000: episode: 896, duration: 3.046s, episode steps: 346, steps per second: 114, episode reward: 276.295, mean reward:  0.799 [-17.596, 100.000], mean action: 1.176 [0.000, 3.000],  loss: 3.959811, mse: 2404.829306, mean_q: 38.207682, mean_eps: 0.141493
 143586/150000: episode: 897, duration: 2.782s, episode steps: 328, steps per second: 118, episode reward: 257.002, mean reward:  0.784 [-8.905, 100.000], mean action: 1.351 [0.000, 3.000],  loss: 4.464641, mse: 2386.201759, mean_q: 38.247315, mean_eps: 0.139471
 143733/150000: episode: 898, duration: 1.209s, episode steps: 147, steps per second: 122, episode reward: 15.562, mean reward:  0.106 [-100.000, 14.990], mean action: 1.823 [0.000, 3.000],  loss: 4.543245, mse: 2407.581648, mean_q: 38.303431, mean_eps: 0.138046
 144733/150000: episode: 899, duration: 9.084s, episode steps: 1000, steps per second: 110, episode reward: 118.418, mean reward:  0.118 [-20.295, 24.500], mean action: 1.237 [0.000, 3.000],  loss: 4.291922, mse: 2362.319314, mean_q: 38.133308, mean_eps: 0.134605
 145226/150000: episode: 900, duration: 4.452s, episode steps: 493, steps per second: 111, episode reward: 210.723, mean reward:  0.427 [-19.904, 100.000], mean action: 1.391 [0.000, 3.000],  loss: 3.695249, mse: 2377.753783, mean_q: 38.415077, mean_eps: 0.130126
 145652/150000: episode: 901, duration: 3.736s, episode steps: 426, steps per second: 114, episode reward: 284.597, mean reward:  0.668 [-19.685, 100.000], mean action: 1.068 [0.000, 3.000],  loss: 2.573843, mse: 2359.134792, mean_q: 38.428941, mean_eps: 0.127369
 146091/150000: episode: 902, duration: 4.035s, episode steps: 439, steps per second: 109, episode reward: 277.570, mean reward:  0.632 [-18.172, 100.000], mean action: 1.298 [0.000, 3.000],  loss: 2.920337, mse: 2392.335724, mean_q: 38.588730, mean_eps: 0.124774
 146573/150000: episode: 903, duration: 4.206s, episode steps: 482, steps per second: 115, episode reward: 225.933, mean reward:  0.469 [-10.114, 100.000], mean action: 1.490 [0.000, 3.000],  loss: 3.589740, mse: 2411.296672, mean_q: 38.927394, mean_eps: 0.122011
 147025/150000: episode: 904, duration: 3.969s, episode steps: 452, steps per second: 114, episode reward: 263.311, mean reward:  0.583 [-18.277, 100.000], mean action: 1.208 [0.000, 3.000],  loss: 4.172037, mse: 2371.580148, mean_q: 38.553313, mean_eps: 0.119209
 147221/150000: episode: 905, duration: 1.682s, episode steps: 196, steps per second: 117, episode reward: -342.367, mean reward: -1.747 [-100.000,  4.753], mean action: 1.474 [0.000, 3.000],  loss: 4.631766, mse: 2490.912482, mean_q: 40.060925, mean_eps: 0.117265
 147523/150000: episode: 906, duration: 2.832s, episode steps: 302, steps per second: 107, episode reward: 269.580, mean reward:  0.893 [-3.044, 100.000], mean action: 1.533 [0.000, 3.000],  loss: 3.561516, mse: 2482.380181, mean_q: 39.960046, mean_eps: 0.115771
 147792/150000: episode: 907, duration: 2.366s, episode steps: 269, steps per second: 114, episode reward: 242.723, mean reward:  0.902 [-5.271, 100.000], mean action: 1.338 [0.000, 3.000],  loss: 2.774128, mse: 2450.426500, mean_q: 39.656314, mean_eps: 0.114058
 148204/150000: episode: 908, duration: 3.570s, episode steps: 412, steps per second: 115, episode reward: 272.395, mean reward:  0.661 [-17.738, 100.000], mean action: 1.347 [0.000, 3.000],  loss: 4.816601, mse: 2510.426888, mean_q: 40.357265, mean_eps: 0.112015
 148821/150000: episode: 909, duration: 5.246s, episode steps: 617, steps per second: 118, episode reward: 279.546, mean reward:  0.453 [-17.821, 100.000], mean action: 0.935 [0.000, 3.000],  loss: 3.604311, mse: 2582.962157, mean_q: 41.084788, mean_eps: 0.108928
 149411/150000: episode: 910, duration: 5.253s, episode steps: 590, steps per second: 112, episode reward: 255.782, mean reward:  0.434 [-20.230, 100.000], mean action: 1.044 [0.000, 3.000],  loss: 4.150962, mse: 2541.936991, mean_q: 40.834528, mean_eps: 0.105307
 149765/150000: episode: 911, duration: 3.022s, episode steps: 354, steps per second: 117, episode reward: 217.069, mean reward:  0.613 [-17.354, 100.000], mean action: 1.302 [0.000, 3.000],  loss: 4.615651, mse: 2549.121189, mean_q: 40.678123, mean_eps: 0.102475
done, took 1212.895 seconds
Testing for 100 episodes ...
Episode 1: reward: 91.160, steps: 892
Episode 2: reward: 98.891, steps: 1000
Episode 3: reward: 73.434, steps: 1000
Episode 4: reward: 131.143, steps: 864
Episode 5: reward: -141.453, steps: 208
Episode 6: reward: 11.747, steps: 1000
Episode 7: reward: 141.389, steps: 753
Episode 8: reward: 115.342, steps: 965
Episode 9: reward: 12.334, steps: 1000
Episode 10: reward: 8.227, steps: 1000
Episode 11: reward: -125.678, steps: 743
Episode 12: reward: 68.653, steps: 1000
Episode 13: reward: 251.708, steps: 507
Episode 14: reward: 49.153, steps: 861
Episode 15: reward: 70.338, steps: 899
Episode 16: reward: 158.754, steps: 824
Episode 17: reward: 222.584, steps: 429
Episode 18: reward: 21.129, steps: 1000
Episode 19: reward: 33.401, steps: 1000
Episode 20: reward: 36.377, steps: 1000
Episode 21: reward: 201.277, steps: 648
Episode 22: reward: -14.414, steps: 219
Episode 23: reward: -98.226, steps: 665
Episode 24: reward: 96.065, steps: 1000
Testing for 5 episodes ...
Episode 1: reward: 296.404, steps: 224
Episode 2: reward: -189.208, steps: 223
Episode 3: reward: 183.366, steps: 573
Episode 4: reward: -124.560, steps: 926
Episode 5: reward: -140.171, steps: 1000
Testing for 5 episodes ...
Episode 1: reward: 138.088, steps: 733
Episode 2: reward: 170.624, steps: 997
Episode 3: reward: 34.589, steps: 1000
Episode 4: reward: -97.583, steps: 715
Episode 5: reward: -2.985, steps: 1000
Testing for 5 episodes ...
Episode 1: reward: 33.099, steps: 1000
Episode 2: reward: 235.593, steps: 471
Episode 3: reward: -130.745, steps: 629
Episode 4: reward: 122.812, steps: 713
Episode 5: reward: -149.572, steps: 245
Testing for 5 episodes ...
Episode 1: reward: 154.685, steps: 694
Episode 2: reward: 91.713, steps: 943
Episode 3: reward: 30.048, steps: 1000
Episode 4: reward: 82.530, steps: 1000
Episode 5: reward: 25.393, steps: 1000
Training for 150000 steps ...
     98/150000: episode: 1, duration: 0.103s, episode steps:  98, steps per second: 950, episode reward: -345.866, mean reward: -3.529 [-100.000,  2.240], mean action: 1.571 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
    176/150000: episode: 2, duration: 0.603s, episode steps:  78, steps per second: 129, episode reward: -137.225, mean reward: -1.759 [-100.000,  8.503], mean action: 1.423 [0.000, 3.000],  loss: 12.007649, mse: 2563.899553, mean_q: 41.246834, mean_eps: 0.999172
    262/150000: episode: 3, duration: 0.756s, episode steps:  86, steps per second: 114, episode reward: -152.410, mean reward: -1.772 [-100.000, 12.007], mean action: 1.384 [0.000, 3.000],  loss: 5.084151, mse: 2530.286712, mean_q: 40.851442, mean_eps: 0.998689
    338/150000: episode: 4, duration: 0.630s, episode steps:  76, steps per second: 121, episode reward: -133.981, mean reward: -1.763 [-100.000,  5.776], mean action: 1.355 [0.000, 3.000],  loss: 12.622425, mse: 2467.519743, mean_q: 40.137247, mean_eps: 0.998203
    433/150000: episode: 5, duration: 0.815s, episode steps:  95, steps per second: 117, episode reward: -223.807, mean reward: -2.356 [-100.000, 15.548], mean action: 1.221 [0.000, 3.000],  loss: 4.655442, mse: 2476.859559, mean_q: 40.280396, mean_eps: 0.997690
    511/150000: episode: 6, duration: 0.636s, episode steps:  78, steps per second: 123, episode reward: -124.622, mean reward: -1.598 [-100.000,  7.474], mean action: 1.500 [0.000, 3.000],  loss: 8.611555, mse: 2531.857439, mean_q: 40.376611, mean_eps: 0.997171
    625/150000: episode: 7, duration: 0.987s, episode steps: 114, steps per second: 116, episode reward: -140.269, mean reward: -1.230 [-100.000,  6.035], mean action: 1.491 [0.000, 3.000],  loss: 3.196092, mse: 2567.895259, mean_q: 41.049815, mean_eps: 0.996595
    711/150000: episode: 8, duration: 0.735s, episode steps:  86, steps per second: 117, episode reward: -98.353, mean reward: -1.144 [-100.000, 10.003], mean action: 1.419 [0.000, 3.000],  loss: 6.338040, mse: 2688.466776, mean_q: 42.303858, mean_eps: 0.995995
    819/150000: episode: 9, duration: 0.869s, episode steps: 108, steps per second: 124, episode reward: -88.684, mean reward: -0.821 [-100.000,  6.576], mean action: 1.657 [0.000, 3.000],  loss: 6.879652, mse: 2557.108744, mean_q: 40.974074, mean_eps: 0.995413
    917/150000: episode: 10, duration: 0.806s, episode steps:  98, steps per second: 122, episode reward: -171.908, mean reward: -1.754 [-100.000, 39.864], mean action: 1.306 [0.000, 3.000],  loss: 5.792630, mse: 2460.345496, mean_q: 39.991219, mean_eps: 0.994795
   1050/150000: episode: 11, duration: 1.032s, episode steps: 133, steps per second: 129, episode reward: -99.239, mean reward: -0.746 [-100.000, 14.919], mean action: 1.602 [0.000, 3.000],  loss: 4.956011, mse: 2821.142241, mean_q: 43.427192, mean_eps: 0.994102
   1130/150000: episode: 12, duration: 0.647s, episode steps:  80, steps per second: 124, episode reward: -190.327, mean reward: -2.379 [-100.000, 14.982], mean action: 1.325 [0.000, 3.000],  loss: 3.739057, mse: 2687.728163, mean_q: 42.365423, mean_eps: 0.993463
   1199/150000: episode: 13, duration: 0.594s, episode steps:  69, steps per second: 116, episode reward: -90.489, mean reward: -1.311 [-100.000, 16.129], mean action: 1.290 [0.000, 3.000],  loss: 5.071620, mse: 2676.087751, mean_q: 42.031024, mean_eps: 0.993016
   1294/150000: episode: 14, duration: 0.812s, episode steps:  95, steps per second: 117, episode reward: -269.988, mean reward: -2.842 [-100.000,  5.177], mean action: 1.474 [0.000, 3.000],  loss: 5.076327, mse: 2779.931080, mean_q: 43.122823, mean_eps: 0.992524
   1416/150000: episode: 15, duration: 0.957s, episode steps: 122, steps per second: 127, episode reward: -201.592, mean reward: -1.652 [-100.000, 11.067], mean action: 1.508 [0.000, 3.000],  loss: 4.229740, mse: 2654.348073, mean_q: 42.067971, mean_eps: 0.991873
   1477/150000: episode: 16, duration: 0.482s, episode steps:  61, steps per second: 126, episode reward: -156.682, mean reward: -2.569 [-100.000,  5.136], mean action: 1.262 [0.000, 3.000],  loss: 4.145008, mse: 2652.134828, mean_q: 41.936540, mean_eps: 0.991324
   1565/150000: episode: 17, duration: 0.702s, episode steps:  88, steps per second: 125, episode reward: -111.493, mean reward: -1.267 [-100.000,  7.647], mean action: 1.443 [0.000, 3.000],  loss: 4.446297, mse: 2670.452892, mean_q: 42.480854, mean_eps: 0.990877
   1659/150000: episode: 18, duration: 0.944s, episode steps:  94, steps per second: 100, episode reward:  9.795, mean reward:  0.104 [-100.000, 103.580], mean action: 1.500 [0.000, 3.000],  loss: 6.059951, mse: 2737.895170, mean_q: 42.776217, mean_eps: 0.990331
   1740/150000: episode: 19, duration: 0.905s, episode steps:  81, steps per second:  90, episode reward: -126.347, mean reward: -1.560 [-100.000,  6.480], mean action: 1.815 [0.000, 3.000],  loss: 6.892135, mse: 2656.282633, mean_q: 42.167675, mean_eps: 0.989806
   1822/150000: episode: 20, duration: 0.793s, episode steps:  82, steps per second: 103, episode reward: -98.768, mean reward: -1.204 [-100.000, 17.811], mean action: 1.683 [0.000, 3.000],  loss: 3.381875, mse: 2643.754454, mean_q: 41.745072, mean_eps: 0.989317
   1929/150000: episode: 21, duration: 0.975s, episode steps: 107, steps per second: 110, episode reward: -281.202, mean reward: -2.628 [-100.000, 89.265], mean action: 1.617 [0.000, 3.000],  loss: 5.054377, mse: 2829.177994, mean_q: 43.322830, mean_eps: 0.988750
   2016/150000: episode: 22, duration: 0.706s, episode steps:  87, steps per second: 123, episode reward: -104.058, mean reward: -1.196 [-100.000, 17.187], mean action: 1.644 [0.000, 3.000],  loss: 7.465137, mse: 2755.212265, mean_q: 42.828080, mean_eps: 0.988168
   2122/150000: episode: 23, duration: 0.899s, episode steps: 106, steps per second: 118, episode reward: -226.991, mean reward: -2.141 [-100.000, 51.232], mean action: 1.368 [0.000, 3.000],  loss: 5.034375, mse: 2784.834792, mean_q: 43.448556, mean_eps: 0.987589
   2217/150000: episode: 24, duration: 0.879s, episode steps:  95, steps per second: 108, episode reward: -412.440, mean reward: -4.341 [-100.000, 68.282], mean action: 1.421 [0.000, 3.000],  loss: 6.578524, mse: 2747.782703, mean_q: 42.974065, mean_eps: 0.986986
   2327/150000: episode: 25, duration: 0.941s, episode steps: 110, steps per second: 117, episode reward: -109.679, mean reward: -0.997 [-100.000, 14.895], mean action: 1.400 [0.000, 3.000],  loss: 4.766927, mse: 2867.172615, mean_q: 44.175121, mean_eps: 0.986371
   2416/150000: episode: 26, duration: 0.801s, episode steps:  89, steps per second: 111, episode reward: -124.128, mean reward: -1.395 [-100.000, 27.831], mean action: 1.551 [0.000, 3.000],  loss: 10.942740, mse: 2911.686994, mean_q: 44.281107, mean_eps: 0.985774
   2505/150000: episode: 27, duration: 0.782s, episode steps:  89, steps per second: 114, episode reward: -276.507, mean reward: -3.107 [-100.000, 39.035], mean action: 1.820 [0.000, 3.000],  loss: 8.063304, mse: 2683.057905, mean_q: 42.049293, mean_eps: 0.985240
   2575/150000: episode: 28, duration: 0.614s, episode steps:  70, steps per second: 114, episode reward: -140.133, mean reward: -2.002 [-100.000,  6.163], mean action: 1.357 [0.000, 3.000],  loss: 7.550851, mse: 2901.804986, mean_q: 44.345054, mean_eps: 0.984763
   2635/150000: episode: 29, duration: 0.586s, episode steps:  60, steps per second: 102, episode reward: -106.726, mean reward: -1.779 [-100.000, 41.042], mean action: 1.483 [0.000, 3.000],  loss: 9.788793, mse: 2837.502384, mean_q: 43.620792, mean_eps: 0.984373
   2710/150000: episode: 30, duration: 0.671s, episode steps:  75, steps per second: 112, episode reward: -60.537, mean reward: -0.807 [-100.000, 14.877], mean action: 1.453 [0.000, 3.000],  loss: 4.320966, mse: 2986.262845, mean_q: 45.233185, mean_eps: 0.983968
   2799/150000: episode: 31, duration: 0.756s, episode steps:  89, steps per second: 118, episode reward: -151.562, mean reward: -1.703 [-100.000,  8.257], mean action: 1.326 [0.000, 3.000],  loss: 11.857879, mse: 2969.363273, mean_q: 44.679877, mean_eps: 0.983476
   2922/150000: episode: 32, duration: 1.294s, episode steps: 123, steps per second:  95, episode reward: -99.902, mean reward: -0.812 [-100.000, 37.328], mean action: 1.683 [0.000, 3.000],  loss: 7.137241, mse: 2882.457125, mean_q: 44.060174, mean_eps: 0.982840
   3008/150000: episode: 33, duration: 1.313s, episode steps:  86, steps per second:  65, episode reward: -142.460, mean reward: -1.657 [-100.000, 40.520], mean action: 1.395 [0.000, 3.000],  loss: 9.918048, mse: 2867.238017, mean_q: 43.678744, mean_eps: 0.982213
   3086/150000: episode: 34, duration: 0.966s, episode steps:  78, steps per second:  81, episode reward: -92.391, mean reward: -1.184 [-100.000,  7.917], mean action: 1.590 [0.000, 3.000],  loss: 11.143252, mse: 3083.416174, mean_q: 45.906198, mean_eps: 0.981721
   3186/150000: episode: 35, duration: 1.188s, episode steps: 100, steps per second:  84, episode reward: -74.999, mean reward: -0.750 [-100.000, 12.776], mean action: 1.330 [0.000, 3.000],  loss: 6.053339, mse: 3041.161703, mean_q: 45.573584, mean_eps: 0.981187
   3269/150000: episode: 36, duration: 1.458s, episode steps:  83, steps per second:  57, episode reward: -502.186, mean reward: -6.050 [-100.000, -0.227], mean action: 1.651 [0.000, 3.000],  loss: 5.279348, mse: 2961.552387, mean_q: 44.563979, mean_eps: 0.980638
   3372/150000: episode: 37, duration: 1.550s, episode steps: 103, steps per second:  66, episode reward: -26.374, mean reward: -0.256 [-100.000, 68.442], mean action: 1.408 [0.000, 3.000],  loss: 6.023923, mse: 2917.594360, mean_q: 44.813864, mean_eps: 0.980080
   3457/150000: episode: 38, duration: 0.971s, episode steps:  85, steps per second:  88, episode reward: -290.863, mean reward: -3.422 [-100.000, 24.202], mean action: 1.376 [0.000, 3.000],  loss: 14.910808, mse: 2894.449153, mean_q: 43.748758, mean_eps: 0.979516
   3560/150000: episode: 39, duration: 1.181s, episode steps: 103, steps per second:  87, episode reward: -277.999, mean reward: -2.699 [-100.000,  1.051], mean action: 1.680 [0.000, 3.000],  loss: 7.724073, mse: 3041.234499, mean_q: 45.105065, mean_eps: 0.978952
   3676/150000: episode: 40, duration: 1.089s, episode steps: 116, steps per second: 106, episode reward: -50.741, mean reward: -0.437 [-100.000, 26.501], mean action: 1.733 [0.000, 3.000],  loss: 6.839749, mse: 3043.434368, mean_q: 45.470001, mean_eps: 0.978295
   3770/150000: episode: 41, duration: 0.819s, episode steps:  94, steps per second: 115, episode reward: -307.819, mean reward: -3.275 [-100.000, 100.155], mean action: 1.521 [0.000, 3.000],  loss: 13.993039, mse: 2983.598415, mean_q: 44.822745, mean_eps: 0.977665
   3853/150000: episode: 42, duration: 0.901s, episode steps:  83, steps per second:  92, episode reward: -89.135, mean reward: -1.074 [-100.000, 20.504], mean action: 1.699 [0.000, 3.000],  loss: 4.907410, mse: 2972.491561, mean_q: 45.434914, mean_eps: 0.977134
   3922/150000: episode: 43, duration: 0.679s, episode steps:  69, steps per second: 102, episode reward: -26.207, mean reward: -0.380 [-100.000, 19.991], mean action: 1.725 [0.000, 3.000],  loss: 9.034993, mse: 2905.650587, mean_q: 44.363549, mean_eps: 0.976678
   4047/150000: episode: 44, duration: 1.122s, episode steps: 125, steps per second: 111, episode reward: -250.464, mean reward: -2.004 [-100.000,  5.470], mean action: 1.448 [0.000, 3.000],  loss: 9.592726, mse: 3043.891782, mean_q: 45.708072, mean_eps: 0.976096
   4142/150000: episode: 45, duration: 0.783s, episode steps:  95, steps per second: 121, episode reward: -214.521, mean reward: -2.258 [-100.000, 19.654], mean action: 1.737 [0.000, 3.000],  loss: 10.278452, mse: 3044.144599, mean_q: 46.048097, mean_eps: 0.975436
   4229/150000: episode: 46, duration: 0.719s, episode steps:  87, steps per second: 121, episode reward: -114.893, mean reward: -1.321 [-100.000,  8.194], mean action: 1.460 [0.000, 3.000],  loss: 5.257273, mse: 3057.049315, mean_q: 46.544734, mean_eps: 0.974890
   4331/150000: episode: 47, duration: 0.860s, episode steps: 102, steps per second: 119, episode reward: -154.521, mean reward: -1.515 [-100.000,  8.059], mean action: 1.559 [0.000, 3.000],  loss: 6.895782, mse: 3022.476667, mean_q: 45.840755, mean_eps: 0.974323
   4440/150000: episode: 48, duration: 0.867s, episode steps: 109, steps per second: 126, episode reward: -145.269, mean reward: -1.333 [-100.000, 18.343], mean action: 1.486 [0.000, 3.000],  loss: 9.306303, mse: 3051.364412, mean_q: 46.200271, mean_eps: 0.973690
   4528/150000: episode: 49, duration: 0.768s, episode steps:  88, steps per second: 115, episode reward: -424.752, mean reward: -4.827 [-100.000,  0.483], mean action: 1.784 [0.000, 3.000],  loss: 12.317869, mse: 3048.350128, mean_q: 45.265267, mean_eps: 0.973099
   4634/150000: episode: 50, duration: 0.859s, episode steps: 106, steps per second: 123, episode reward: -424.463, mean reward: -4.004 [-100.000,  0.906], mean action: 1.604 [0.000, 3.000],  loss: 8.481643, mse: 3071.510518, mean_q: 46.405585, mean_eps: 0.972517
   4723/150000: episode: 51, duration: 0.713s, episode steps:  89, steps per second: 125, episode reward: -81.856, mean reward: -0.920 [-100.000, 50.217], mean action: 1.685 [0.000, 3.000],  loss: 5.972665, mse: 3027.172344, mean_q: 45.661667, mean_eps: 0.971932
   4786/150000: episode: 52, duration: 0.568s, episode steps:  63, steps per second: 111, episode reward: -119.150, mean reward: -1.891 [-100.000,  8.383], mean action: 1.476 [0.000, 3.000],  loss: 14.062944, mse: 3116.993848, mean_q: 46.623097, mean_eps: 0.971476
   4871/150000: episode: 53, duration: 0.706s, episode steps:  85, steps per second: 120, episode reward: -42.939, mean reward: -0.505 [-100.000, 71.217], mean action: 1.671 [0.000, 3.000],  loss: 7.870191, mse: 3110.868708, mean_q: 46.601747, mean_eps: 0.971032
   4971/150000: episode: 54, duration: 0.805s, episode steps: 100, steps per second: 124, episode reward: -265.531, mean reward: -2.655 [-100.000,  8.569], mean action: 1.650 [0.000, 3.000],  loss: 13.833253, mse: 3185.788910, mean_q: 46.751393, mean_eps: 0.970477
   5051/150000: episode: 55, duration: 0.728s, episode steps:  80, steps per second: 110, episode reward: -110.981, mean reward: -1.387 [-100.000,  4.720], mean action: 1.600 [0.000, 3.000],  loss: 7.669290, mse: 2958.854918, mean_q: 45.879054, mean_eps: 0.969937
   5191/150000: episode: 56, duration: 1.130s, episode steps: 140, steps per second: 124, episode reward: -248.614, mean reward: -1.776 [-100.000,  5.869], mean action: 1.536 [0.000, 3.000],  loss: 8.812468, mse: 3062.993644, mean_q: 46.290839, mean_eps: 0.969277
   5279/150000: episode: 57, duration: 0.925s, episode steps:  88, steps per second:  95, episode reward: -143.599, mean reward: -1.632 [-100.000, 26.795], mean action: 1.443 [0.000, 3.000],  loss: 12.818199, mse: 3059.901643, mean_q: 46.064632, mean_eps: 0.968593
   5387/150000: episode: 58, duration: 0.869s, episode steps: 108, steps per second: 124, episode reward: -193.063, mean reward: -1.788 [-100.000, 25.653], mean action: 1.500 [0.000, 3.000],  loss: 7.115111, mse: 3053.910287, mean_q: 46.407842, mean_eps: 0.968005
   5465/150000: episode: 59, duration: 0.618s, episode steps:  78, steps per second: 126, episode reward: -168.643, mean reward: -2.162 [-100.000, 27.046], mean action: 1.577 [0.000, 3.000],  loss: 7.970258, mse: 3097.368920, mean_q: 46.845093, mean_eps: 0.967447
   5534/150000: episode: 60, duration: 0.572s, episode steps:  69, steps per second: 121, episode reward: -98.556, mean reward: -1.428 [-100.000, 17.233], mean action: 1.609 [0.000, 3.000],  loss: 10.538742, mse: 3152.443013, mean_q: 47.211679, mean_eps: 0.967006
   5598/150000: episode: 61, duration: 0.588s, episode steps:  64, steps per second: 109, episode reward: -71.209, mean reward: -1.113 [-100.000,  7.683], mean action: 1.359 [0.000, 3.000],  loss: 5.618750, mse: 3087.897612, mean_q: 46.504353, mean_eps: 0.966607
   5684/150000: episode: 62, duration: 0.703s, episode steps:  86, steps per second: 122, episode reward: -369.804, mean reward: -4.300 [-100.000, 68.530], mean action: 1.477 [0.000, 3.000],  loss: 13.328039, mse: 3049.258685, mean_q: 45.888936, mean_eps: 0.966157
   5778/150000: episode: 63, duration: 0.844s, episode steps:  94, steps per second: 111, episode reward: -161.255, mean reward: -1.715 [-100.000, 48.057], mean action: 1.457 [0.000, 3.000],  loss: 10.981221, mse: 3136.415086, mean_q: 47.192172, mean_eps: 0.965617
   5860/150000: episode: 64, duration: 0.742s, episode steps:  82, steps per second: 111, episode reward: -131.663, mean reward: -1.606 [-100.000,  8.795], mean action: 1.427 [0.000, 3.000],  loss: 10.322601, mse: 3011.645590, mean_q: 45.896960, mean_eps: 0.965089
   5930/150000: episode: 65, duration: 0.611s, episode steps:  70, steps per second: 115, episode reward: -69.354, mean reward: -0.991 [-100.000,  8.579], mean action: 1.357 [0.000, 3.000],  loss: 5.997608, mse: 3089.253430, mean_q: 47.040206, mean_eps: 0.964633
   6029/150000: episode: 66, duration: 0.919s, episode steps:  99, steps per second: 108, episode reward: -22.812, mean reward: -0.230 [-100.000, 102.140], mean action: 1.455 [0.000, 3.000],  loss: 8.616800, mse: 3172.605323, mean_q: 47.362133, mean_eps: 0.964126
   6143/150000: episode: 67, duration: 0.977s, episode steps: 114, steps per second: 117, episode reward: -95.054, mean reward: -0.834 [-100.000,  7.624], mean action: 1.535 [0.000, 3.000],  loss: 10.903040, mse: 3210.647830, mean_q: 47.659349, mean_eps: 0.963487
   6229/150000: episode: 68, duration: 0.748s, episode steps:  86, steps per second: 115, episode reward: -110.727, mean reward: -1.288 [-100.000, 12.197], mean action: 1.605 [0.000, 3.000],  loss: 7.731856, mse: 2972.560054, mean_q: 46.004504, mean_eps: 0.962887
   6336/150000: episode: 69, duration: 0.916s, episode steps: 107, steps per second: 117, episode reward: -184.515, mean reward: -1.724 [-100.000,  9.165], mean action: 1.458 [0.000, 3.000],  loss: 8.797971, mse: 3196.268814, mean_q: 47.787105, mean_eps: 0.962308
   6420/150000: episode: 70, duration: 0.665s, episode steps:  84, steps per second: 126, episode reward: -103.363, mean reward: -1.231 [-100.000,  7.230], mean action: 1.560 [0.000, 3.000],  loss: 12.878800, mse: 3149.375777, mean_q: 47.217675, mean_eps: 0.961735
   6513/150000: episode: 71, duration: 0.794s, episode steps:  93, steps per second: 117, episode reward: -108.431, mean reward: -1.166 [-100.000, 71.669], mean action: 1.430 [0.000, 3.000],  loss: 9.968696, mse: 3090.959362, mean_q: 46.489778, mean_eps: 0.961204
   6588/150000: episode: 72, duration: 0.664s, episode steps:  75, steps per second: 113, episode reward: -155.974, mean reward: -2.080 [-100.000, 35.273], mean action: 1.560 [0.000, 3.000],  loss: 10.455469, mse: 3152.041266, mean_q: 47.114491, mean_eps: 0.960700
   6691/150000: episode: 73, duration: 0.832s, episode steps: 103, steps per second: 124, episode reward: -171.513, mean reward: -1.665 [-100.000,  1.811], mean action: 1.621 [0.000, 3.000],  loss: 7.481272, mse: 3206.079462, mean_q: 47.681834, mean_eps: 0.960166
   6792/150000: episode: 74, duration: 0.853s, episode steps: 101, steps per second: 118, episode reward: -88.957, mean reward: -0.881 [-100.000, 12.197], mean action: 1.525 [0.000, 3.000],  loss: 8.205350, mse: 3148.115439, mean_q: 47.242042, mean_eps: 0.959554
   6905/150000: episode: 75, duration: 0.910s, episode steps: 113, steps per second: 124, episode reward: -116.233, mean reward: -1.029 [-100.000,  6.906], mean action: 1.425 [0.000, 3.000],  loss: 10.446179, mse: 3187.625915, mean_q: 47.701463, mean_eps: 0.958912
   7001/150000: episode: 76, duration: 0.797s, episode steps:  96, steps per second: 120, episode reward: -178.249, mean reward: -1.857 [-100.000,  9.178], mean action: 1.802 [0.000, 3.000],  loss: 5.538407, mse: 3226.689995, mean_q: 47.567572, mean_eps: 0.958285
   7066/150000: episode: 77, duration: 0.578s, episode steps:  65, steps per second: 112, episode reward: -119.987, mean reward: -1.846 [-100.000,  7.519], mean action: 1.477 [0.000, 3.000],  loss: 5.826246, mse: 3180.588728, mean_q: 47.686926, mean_eps: 0.957802
   7138/150000: episode: 78, duration: 0.595s, episode steps:  72, steps per second: 121, episode reward: -92.737, mean reward: -1.288 [-100.000,  6.866], mean action: 1.681 [0.000, 3.000],  loss: 8.046701, mse: 3439.507724, mean_q: 49.334787, mean_eps: 0.957391
   7237/150000: episode: 79, duration: 0.802s, episode steps:  99, steps per second: 124, episode reward: -114.754, mean reward: -1.159 [-100.000, 16.936], mean action: 1.394 [0.000, 3.000],  loss: 9.198273, mse: 3361.909071, mean_q: 49.038074, mean_eps: 0.956878
   7292/150000: episode: 80, duration: 0.455s, episode steps:  55, steps per second: 121, episode reward: -94.819, mean reward: -1.724 [-100.000, 21.195], mean action: 1.455 [0.000, 3.000],  loss: 6.701058, mse: 3366.716164, mean_q: 48.793322, mean_eps: 0.956416
   7370/150000: episode: 81, duration: 0.689s, episode steps:  78, steps per second: 113, episode reward: -84.494, mean reward: -1.083 [-100.000,  6.186], mean action: 1.615 [0.000, 3.000],  loss: 9.732229, mse: 3377.228569, mean_q: 49.109053, mean_eps: 0.956017
   7457/150000: episode: 82, duration: 0.702s, episode steps:  87, steps per second: 124, episode reward: -100.999, mean reward: -1.161 [-100.000, 10.226], mean action: 1.736 [0.000, 3.000],  loss: 8.336168, mse: 3467.100232, mean_q: 49.753540, mean_eps: 0.955522
   7551/150000: episode: 83, duration: 0.949s, episode steps:  94, steps per second:  99, episode reward: -149.880, mean reward: -1.594 [-100.000,  4.388], mean action: 1.543 [0.000, 3.000],  loss: 12.158228, mse: 3332.644443, mean_q: 48.765011, mean_eps: 0.954979
   7652/150000: episode: 84, duration: 0.937s, episode steps: 101, steps per second: 108, episode reward: -182.319, mean reward: -1.805 [-100.000,  4.438], mean action: 1.376 [0.000, 3.000],  loss: 6.635112, mse: 3378.838566, mean_q: 49.385504, mean_eps: 0.954394
   7722/150000: episode: 85, duration: 0.629s, episode steps:  70, steps per second: 111, episode reward: -67.866, mean reward: -0.970 [-100.000, 12.101], mean action: 1.700 [0.000, 3.000],  loss: 9.569694, mse: 3365.463156, mean_q: 48.977368, mean_eps: 0.953881
   7823/150000: episode: 86, duration: 0.908s, episode steps: 101, steps per second: 111, episode reward: -199.913, mean reward: -1.979 [-100.000, 71.632], mean action: 1.535 [0.000, 3.000],  loss: 11.595289, mse: 3376.770266, mean_q: 48.887498, mean_eps: 0.953368
   7917/150000: episode: 87, duration: 0.814s, episode steps:  94, steps per second: 115, episode reward: -113.613, mean reward: -1.209 [-100.000,  7.094], mean action: 1.766 [0.000, 3.000],  loss: 7.432066, mse: 3377.069009, mean_q: 48.631394, mean_eps: 0.952783
   8026/150000: episode: 88, duration: 0.959s, episode steps: 109, steps per second: 114, episode reward: -225.060, mean reward: -2.065 [-100.000,  7.532], mean action: 1.578 [0.000, 3.000],  loss: 5.650283, mse: 3325.516124, mean_q: 48.528240, mean_eps: 0.952174
   8094/150000: episode: 89, duration: 0.588s, episode steps:  68, steps per second: 116, episode reward: -79.778, mean reward: -1.173 [-100.000,  7.369], mean action: 1.574 [0.000, 3.000],  loss: 5.661984, mse: 3295.159704, mean_q: 48.061217, mean_eps: 0.951643
   8209/150000: episode: 90, duration: 0.923s, episode steps: 115, steps per second: 125, episode reward: -354.035, mean reward: -3.079 [-100.000,  1.717], mean action: 1.435 [0.000, 3.000],  loss: 6.213266, mse: 3437.769270, mean_q: 49.031326, mean_eps: 0.951094
   8302/150000: episode: 91, duration: 0.759s, episode steps:  93, steps per second: 123, episode reward: -97.536, mean reward: -1.049 [-100.000,  4.653], mean action: 1.613 [0.000, 3.000],  loss: 7.719224, mse: 3461.186392, mean_q: 49.340312, mean_eps: 0.950470
   8427/150000: episode: 92, duration: 1.003s, episode steps: 125, steps per second: 125, episode reward: -220.036, mean reward: -1.760 [-100.000,  7.990], mean action: 1.584 [0.000, 3.000],  loss: 12.637230, mse: 3420.317707, mean_q: 49.217128, mean_eps: 0.949816
   8515/150000: episode: 93, duration: 0.699s, episode steps:  88, steps per second: 126, episode reward: -104.621, mean reward: -1.189 [-100.000, 50.570], mean action: 1.534 [0.000, 3.000],  loss: 5.446190, mse: 3296.004594, mean_q: 48.110946, mean_eps: 0.949177
   8597/150000: episode: 94, duration: 0.697s, episode steps:  82, steps per second: 118, episode reward: -192.166, mean reward: -2.343 [-100.000,  8.249], mean action: 1.415 [0.000, 3.000],  loss: 12.840701, mse: 3453.849600, mean_q: 49.766031, mean_eps: 0.948667
   8699/150000: episode: 95, duration: 0.825s, episode steps: 102, steps per second: 124, episode reward: -120.403, mean reward: -1.180 [-100.000, 13.822], mean action: 1.618 [0.000, 3.000],  loss: 12.295726, mse: 3495.715911, mean_q: 49.920040, mean_eps: 0.948115
   8759/150000: episode: 96, duration: 0.478s, episode steps:  60, steps per second: 126, episode reward: -68.200, mean reward: -1.137 [-100.000, 21.280], mean action: 1.217 [0.000, 3.000],  loss: 9.846929, mse: 3389.880585, mean_q: 48.344948, mean_eps: 0.947629
   8856/150000: episode: 97, duration: 0.817s, episode steps:  97, steps per second: 119, episode reward: -96.776, mean reward: -0.998 [-100.000,  6.455], mean action: 1.443 [0.000, 3.000],  loss: 11.022679, mse: 3494.308236, mean_q: 49.561592, mean_eps: 0.947158
   8918/150000: episode: 98, duration: 0.506s, episode steps:  62, steps per second: 122, episode reward: -100.830, mean reward: -1.626 [-100.000,  9.002], mean action: 1.339 [0.000, 3.000],  loss: 15.023238, mse: 3479.572368, mean_q: 49.945920, mean_eps: 0.946681
   9006/150000: episode: 99, duration: 0.704s, episode steps:  88, steps per second: 125, episode reward: -188.886, mean reward: -2.146 [-100.000,  6.790], mean action: 1.500 [0.000, 3.000],  loss: 8.292608, mse: 3566.453880, mean_q: 50.565981, mean_eps: 0.946231
   9074/150000: episode: 100, duration: 0.586s, episode steps:  68, steps per second: 116, episode reward: -108.106, mean reward: -1.590 [-100.000, 12.182], mean action: 1.529 [0.000, 3.000],  loss: 7.757954, mse: 3386.730770, mean_q: 48.484815, mean_eps: 0.945763
   9143/150000: episode: 101, duration: 0.623s, episode steps:  69, steps per second: 111, episode reward: -132.111, mean reward: -1.915 [-100.000,  7.165], mean action: 1.493 [0.000, 3.000],  loss: 4.643771, mse: 3361.347961, mean_q: 48.179295, mean_eps: 0.945352
   9247/150000: episode: 102, duration: 0.851s, episode steps: 104, steps per second: 122, episode reward: -120.700, mean reward: -1.161 [-100.000,  9.373], mean action: 1.385 [0.000, 3.000],  loss: 13.337145, mse: 3496.050392, mean_q: 49.189757, mean_eps: 0.944833
   9357/150000: episode: 103, duration: 0.902s, episode steps: 110, steps per second: 122, episode reward: -141.220, mean reward: -1.284 [-100.000, 15.658], mean action: 1.609 [0.000, 3.000],  loss: 10.871437, mse: 3388.514060, mean_q: 48.406748, mean_eps: 0.944191
   9451/150000: episode: 104, duration: 0.777s, episode steps:  94, steps per second: 121, episode reward: -140.919, mean reward: -1.499 [-100.000,  7.124], mean action: 1.574 [0.000, 3.000],  loss: 7.068862, mse: 3434.106869, mean_q: 49.014302, mean_eps: 0.943579
   9557/150000: episode: 105, duration: 0.898s, episode steps: 106, steps per second: 118, episode reward: -75.301, mean reward: -0.710 [-100.000,  7.307], mean action: 1.387 [0.000, 3.000],  loss: 9.779971, mse: 3399.244100, mean_q: 48.101208, mean_eps: 0.942979
   9629/150000: episode: 106, duration: 0.625s, episode steps:  72, steps per second: 115, episode reward: 18.584, mean reward:  0.258 [-100.000, 110.300], mean action: 1.597 [0.000, 3.000],  loss: 10.509716, mse: 3552.536736, mean_q: 50.746528, mean_eps: 0.942445
   9710/150000: episode: 107, duration: 0.763s, episode steps:  81, steps per second: 106, episode reward: -102.708, mean reward: -1.268 [-100.000, 21.384], mean action: 1.395 [0.000, 3.000],  loss: 12.450643, mse: 3426.713453, mean_q: 48.505634, mean_eps: 0.941986
   9788/150000: episode: 108, duration: 0.777s, episode steps:  78, steps per second: 100, episode reward: -89.076, mean reward: -1.142 [-100.000,  7.008], mean action: 1.577 [0.000, 3.000],  loss: 8.115146, mse: 3489.482290, mean_q: 49.396486, mean_eps: 0.941509
   9910/150000: episode: 109, duration: 1.037s, episode steps: 122, steps per second: 118, episode reward: -138.591, mean reward: -1.136 [-100.000, 12.067], mean action: 1.533 [0.000, 3.000],  loss: 7.084899, mse: 3476.976873, mean_q: 49.531162, mean_eps: 0.940909
  10010/150000: episode: 110, duration: 0.801s, episode steps: 100, steps per second: 125, episode reward: -141.969, mean reward: -1.420 [-100.000,  7.756], mean action: 1.380 [0.000, 3.000],  loss: 15.738527, mse: 3515.176244, mean_q: 49.986193, mean_eps: 0.940243
  10090/150000: episode: 111, duration: 0.646s, episode steps:  80, steps per second: 124, episode reward: -94.116, mean reward: -1.176 [-100.000, 13.131], mean action: 1.538 [0.000, 3.000],  loss: 7.834009, mse: 3590.311334, mean_q: 50.402108, mean_eps: 0.939703
  10185/150000: episode: 112, duration: 0.769s, episode steps:  95, steps per second: 124, episode reward: -137.772, mean reward: -1.450 [-100.000, 17.184], mean action: 1.484 [0.000, 3.000],  loss: 9.088897, mse: 3559.643344, mean_q: 50.730465, mean_eps: 0.939178
  10318/150000: episode: 113, duration: 1.064s, episode steps: 133, steps per second: 125, episode reward: -108.154, mean reward: -0.813 [-100.000, 14.803], mean action: 1.511 [0.000, 3.000],  loss: 11.379701, mse: 3517.596584, mean_q: 50.302653, mean_eps: 0.938494
  10379/150000: episode: 114, duration: 0.532s, episode steps:  61, steps per second: 115, episode reward: -53.870, mean reward: -0.883 [-100.000, 18.079], mean action: 1.410 [0.000, 3.000],  loss: 14.255050, mse: 3775.492348, mean_q: 52.219094, mean_eps: 0.937912
  10450/150000: episode: 115, duration: 0.575s, episode steps:  71, steps per second: 123, episode reward: -110.443, mean reward: -1.556 [-100.000, 18.541], mean action: 1.577 [0.000, 3.000],  loss: 15.336288, mse: 3681.628662, mean_q: 51.862547, mean_eps: 0.937516
  10532/150000: episode: 116, duration: 0.646s, episode steps:  82, steps per second: 127, episode reward: -55.541, mean reward: -0.677 [-100.000, 19.913], mean action: 1.524 [0.000, 3.000],  loss: 18.745926, mse: 3728.580965, mean_q: 51.608394, mean_eps: 0.937057
  10646/150000: episode: 117, duration: 0.939s, episode steps: 114, steps per second: 121, episode reward: -207.247, mean reward: -1.818 [-100.000,  6.680], mean action: 1.544 [0.000, 3.000],  loss: 11.145093, mse: 3656.317373, mean_q: 51.239672, mean_eps: 0.936469
  10733/150000: episode: 118, duration: 0.703s, episode steps:  87, steps per second: 124, episode reward: -125.809, mean reward: -1.446 [-100.000,  8.238], mean action: 1.494 [0.000, 3.000],  loss: 7.478117, mse: 3569.037665, mean_q: 50.453313, mean_eps: 0.935866
  10830/150000: episode: 119, duration: 0.766s, episode steps:  97, steps per second: 127, episode reward: -157.361, mean reward: -1.622 [-100.000, 12.220], mean action: 1.732 [0.000, 3.000],  loss: 8.095670, mse: 3741.200032, mean_q: 52.031307, mean_eps: 0.935314
  10916/150000: episode: 120, duration: 0.738s, episode steps:  86, steps per second: 116, episode reward: -186.049, mean reward: -2.163 [-100.000, 33.054], mean action: 1.593 [0.000, 3.000],  loss: 11.338604, mse: 3660.986655, mean_q: 51.364383, mean_eps: 0.934765
  10986/150000: episode: 121, duration: 0.592s, episode steps:  70, steps per second: 118, episode reward: -115.690, mean reward: -1.653 [-100.000, 10.930], mean action: 1.414 [0.000, 3.000],  loss: 9.948532, mse: 3630.073689, mean_q: 51.243728, mean_eps: 0.934297
  11054/150000: episode: 122, duration: 0.542s, episode steps:  68, steps per second: 125, episode reward: -90.050, mean reward: -1.324 [-100.000,  4.750], mean action: 1.529 [0.000, 3.000],  loss: 19.258844, mse: 3672.028665, mean_q: 51.395607, mean_eps: 0.933883
  11155/150000: episode: 123, duration: 0.851s, episode steps: 101, steps per second: 119, episode reward: -220.660, mean reward: -2.185 [-100.000,  4.257], mean action: 1.535 [0.000, 3.000],  loss: 8.479278, mse: 3747.265821, mean_q: 52.287663, mean_eps: 0.933376
  11231/150000: episode: 124, duration: 0.646s, episode steps:  76, steps per second: 118, episode reward: -52.296, mean reward: -0.688 [-100.000, 13.339], mean action: 1.382 [0.000, 3.000],  loss: 5.199239, mse: 3674.849535, mean_q: 51.741487, mean_eps: 0.932845
  11338/150000: episode: 125, duration: 0.855s, episode steps: 107, steps per second: 125, episode reward: -216.668, mean reward: -2.025 [-100.000,  9.759], mean action: 1.449 [0.000, 3.000],  loss: 8.325048, mse: 3746.169128, mean_q: 52.228083, mean_eps: 0.932296
  11427/150000: episode: 126, duration: 0.741s, episode steps:  89, steps per second: 120, episode reward: -308.121, mean reward: -3.462 [-100.000,  0.375], mean action: 1.315 [0.000, 3.000],  loss: 11.188935, mse: 3699.604280, mean_q: 51.571269, mean_eps: 0.931708
  11500/150000: episode: 127, duration: 0.622s, episode steps:  73, steps per second: 117, episode reward: -60.127, mean reward: -0.824 [-100.000, 14.278], mean action: 1.466 [0.000, 3.000],  loss: 15.629342, mse: 3662.788845, mean_q: 51.550526, mean_eps: 0.931222
  11606/150000: episode: 128, duration: 0.863s, episode steps: 106, steps per second: 123, episode reward: -145.516, mean reward: -1.373 [-100.000, 11.840], mean action: 1.604 [0.000, 3.000],  loss: 11.651687, mse: 3846.042345, mean_q: 53.287211, mean_eps: 0.930685
  11681/150000: episode: 129, duration: 0.600s, episode steps:  75, steps per second: 125, episode reward: -93.479, mean reward: -1.246 [-100.000, 11.719], mean action: 1.373 [0.000, 3.000],  loss: 10.476902, mse: 3671.675181, mean_q: 51.486576, mean_eps: 0.930142
  11761/150000: episode: 130, duration: 0.724s, episode steps:  80, steps per second: 110, episode reward: 16.032, mean reward:  0.200 [-100.000, 98.322], mean action: 1.413 [0.000, 3.000],  loss: 10.317037, mse: 3754.995038, mean_q: 51.861390, mean_eps: 0.929677
  11836/150000: episode: 131, duration: 0.616s, episode steps:  75, steps per second: 122, episode reward: -47.948, mean reward: -0.639 [-100.000, 11.885], mean action: 1.493 [0.000, 3.000],  loss: 11.941210, mse: 3768.441302, mean_q: 51.859750, mean_eps: 0.929212
  11970/150000: episode: 132, duration: 1.136s, episode steps: 134, steps per second: 118, episode reward: -263.289, mean reward: -1.965 [-100.000, 100.919], mean action: 1.530 [0.000, 3.000],  loss: 9.841610, mse: 3887.538545, mean_q: 52.668255, mean_eps: 0.928585
  12034/150000: episode: 133, duration: 0.532s, episode steps:  64, steps per second: 120, episode reward: -112.798, mean reward: -1.762 [-100.000,  8.068], mean action: 1.531 [0.000, 3.000],  loss: 12.812162, mse: 3873.684395, mean_q: 52.886617, mean_eps: 0.927991
  12113/150000: episode: 134, duration: 0.640s, episode steps:  79, steps per second: 124, episode reward: -94.040, mean reward: -1.190 [-100.000,  6.318], mean action: 1.582 [0.000, 3.000],  loss: 8.264010, mse: 3909.172292, mean_q: 54.296948, mean_eps: 0.927562
  12197/150000: episode: 135, duration: 0.699s, episode steps:  84, steps per second: 120, episode reward: -234.149, mean reward: -2.787 [-100.000, 12.373], mean action: 1.583 [0.000, 3.000],  loss: 19.378425, mse: 3943.603984, mean_q: 54.146643, mean_eps: 0.927073
  12311/150000: episode: 136, duration: 0.947s, episode steps: 114, steps per second: 120, episode reward: -66.510, mean reward: -0.583 [-100.000, 22.734], mean action: 1.482 [0.000, 3.000],  loss: 8.516351, mse: 3842.925313, mean_q: 53.182492, mean_eps: 0.926479
  12400/150000: episode: 137, duration: 0.700s, episode steps:  89, steps per second: 127, episode reward: -99.896, mean reward: -1.122 [-100.000,  6.788], mean action: 1.315 [0.000, 3.000],  loss: 12.827042, mse: 3920.726473, mean_q: 53.943351, mean_eps: 0.925870
  12519/150000: episode: 138, duration: 0.989s, episode steps: 119, steps per second: 120, episode reward: -115.613, mean reward: -0.972 [-100.000, 12.753], mean action: 1.639 [0.000, 3.000],  loss: 9.614223, mse: 3827.173381, mean_q: 52.949493, mean_eps: 0.925246
  12604/150000: episode: 139, duration: 0.731s, episode steps:  85, steps per second: 116, episode reward: -119.308, mean reward: -1.404 [-100.000,  6.035], mean action: 1.494 [0.000, 3.000],  loss: 13.972334, mse: 3887.421530, mean_q: 53.921231, mean_eps: 0.924634
  12683/150000: episode: 140, duration: 0.633s, episode steps:  79, steps per second: 125, episode reward: -48.814, mean reward: -0.618 [-100.000, 17.505], mean action: 1.633 [0.000, 3.000],  loss: 19.066218, mse: 4014.194747, mean_q: 54.909350, mean_eps: 0.924142
  12787/150000: episode: 141, duration: 0.876s, episode steps: 104, steps per second: 119, episode reward: -114.507, mean reward: -1.101 [-100.000, 10.693], mean action: 1.663 [0.000, 3.000],  loss: 10.216676, mse: 4084.803997, mean_q: 55.159975, mean_eps: 0.923593
  12862/150000: episode: 142, duration: 0.622s, episode steps:  75, steps per second: 121, episode reward: -104.048, mean reward: -1.387 [-100.000, 15.355], mean action: 1.533 [0.000, 3.000],  loss: 11.418217, mse: 4086.588685, mean_q: 55.392281, mean_eps: 0.923056
  12963/150000: episode: 143, duration: 0.801s, episode steps: 101, steps per second: 126, episode reward:  4.749, mean reward:  0.047 [-100.000, 113.791], mean action: 1.436 [0.000, 3.000],  loss: 9.790549, mse: 4051.798806, mean_q: 54.828457, mean_eps: 0.922528
  13058/150000: episode: 144, duration: 0.806s, episode steps:  95, steps per second: 118, episode reward: -119.034, mean reward: -1.253 [-100.000,  7.173], mean action: 1.474 [0.000, 3.000],  loss: 8.608260, mse: 4005.246325, mean_q: 55.073089, mean_eps: 0.921940
  13184/150000: episode: 145, duration: 1.018s, episode steps: 126, steps per second: 124, episode reward: -230.562, mean reward: -1.830 [-100.000, 44.695], mean action: 1.532 [0.000, 3.000],  loss: 14.498662, mse: 4012.551915, mean_q: 55.128642, mean_eps: 0.921277
  13308/150000: episode: 146, duration: 1.048s, episode steps: 124, steps per second: 118, episode reward: -66.363, mean reward: -0.535 [-100.000, 14.052], mean action: 1.532 [0.000, 3.000],  loss: 9.711614, mse: 4054.234408, mean_q: 54.902130, mean_eps: 0.920527
  13420/150000: episode: 147, duration: 0.912s, episode steps: 112, steps per second: 123, episode reward: -286.379, mean reward: -2.557 [-100.000,  5.362], mean action: 1.438 [0.000, 3.000],  loss: 5.975563, mse: 3991.305317, mean_q: 54.786870, mean_eps: 0.919819
  13509/150000: episode: 148, duration: 0.736s, episode steps:  89, steps per second: 121, episode reward: -393.435, mean reward: -4.421 [-100.000,  0.376], mean action: 1.337 [0.000, 3.000],  loss: 10.892788, mse: 4095.210806, mean_q: 55.282171, mean_eps: 0.919216
  13620/150000: episode: 149, duration: 0.905s, episode steps: 111, steps per second: 123, episode reward: -105.189, mean reward: -0.948 [-100.000, 33.970], mean action: 1.414 [0.000, 3.000],  loss: 11.106049, mse: 4129.100753, mean_q: 55.524400, mean_eps: 0.918616
  13713/150000: episode: 150, duration: 0.884s, episode steps:  93, steps per second: 105, episode reward: -105.942, mean reward: -1.139 [-100.000, 12.633], mean action: 1.667 [0.000, 3.000],  loss: 9.667699, mse: 4142.201458, mean_q: 55.321242, mean_eps: 0.918004
  13777/150000: episode: 151, duration: 0.626s, episode steps:  64, steps per second: 102, episode reward: -139.630, mean reward: -2.182 [-100.000,  6.992], mean action: 1.516 [0.000, 3.000],  loss: 8.839185, mse: 4051.417534, mean_q: 54.980795, mean_eps: 0.917533
  13863/150000: episode: 152, duration: 0.787s, episode steps:  86, steps per second: 109, episode reward: -45.071, mean reward: -0.524 [-100.000,  8.078], mean action: 1.605 [0.000, 3.000],  loss: 9.626919, mse: 4120.918185, mean_q: 55.418250, mean_eps: 0.917083
  13982/150000: episode: 153, duration: 1.079s, episode steps: 119, steps per second: 110, episode reward: -292.817, mean reward: -2.461 [-100.000, 110.473], mean action: 1.773 [0.000, 3.000],  loss: 16.372605, mse: 4135.332005, mean_q: 55.408286, mean_eps: 0.916468
  14056/150000: episode: 154, duration: 0.636s, episode steps:  74, steps per second: 116, episode reward: -55.214, mean reward: -0.746 [-100.000, 17.085], mean action: 1.568 [0.000, 3.000],  loss: 10.728060, mse: 4251.888913, mean_q: 56.213505, mean_eps: 0.915889
  14137/150000: episode: 155, duration: 0.656s, episode steps:  81, steps per second: 123, episode reward: -89.930, mean reward: -1.110 [-100.000,  8.055], mean action: 1.667 [0.000, 3.000],  loss: 10.297090, mse: 4106.754024, mean_q: 55.440350, mean_eps: 0.915424
  14242/150000: episode: 156, duration: 0.901s, episode steps: 105, steps per second: 116, episode reward: -89.989, mean reward: -0.857 [-100.000, 16.306], mean action: 1.495 [0.000, 3.000],  loss: 14.464206, mse: 4026.985372, mean_q: 54.158642, mean_eps: 0.914866
  14321/150000: episode: 157, duration: 0.658s, episode steps:  79, steps per second: 120, episode reward: -38.672, mean reward: -0.490 [-100.000, 28.387], mean action: 1.418 [0.000, 3.000],  loss: 14.453337, mse: 4174.264216, mean_q: 55.210118, mean_eps: 0.914314
  14470/150000: episode: 158, duration: 1.323s, episode steps: 149, steps per second: 113, episode reward: -138.787, mean reward: -0.931 [-100.000,  7.453], mean action: 1.591 [0.000, 3.000],  loss: 10.994024, mse: 4190.346288, mean_q: 56.111920, mean_eps: 0.913630
  14569/150000: episode: 159, duration: 1.224s, episode steps:  99, steps per second:  81, episode reward: -101.700, mean reward: -1.027 [-100.000, 11.131], mean action: 1.434 [0.000, 3.000],  loss: 15.227618, mse: 4247.040806, mean_q: 56.042113, mean_eps: 0.912886
  14645/150000: episode: 160, duration: 0.885s, episode steps:  76, steps per second:  86, episode reward: -77.408, mean reward: -1.019 [-100.000, 12.925], mean action: 1.539 [0.000, 3.000],  loss: 10.413755, mse: 4200.532448, mean_q: 55.793870, mean_eps: 0.912361
  14724/150000: episode: 161, duration: 0.839s, episode steps:  79, steps per second:  94, episode reward: -107.985, mean reward: -1.367 [-100.000,  8.670], mean action: 1.709 [0.000, 3.000],  loss: 7.869495, mse: 4139.650060, mean_q: 56.288944, mean_eps: 0.911896
  14836/150000: episode: 162, duration: 1.032s, episode steps: 112, steps per second: 109, episode reward: -211.226, mean reward: -1.886 [-100.000,  5.866], mean action: 1.509 [0.000, 3.000],  loss: 11.971802, mse: 4313.059154, mean_q: 56.803288, mean_eps: 0.911323
  14920/150000: episode: 163, duration: 0.904s, episode steps:  84, steps per second:  93, episode reward: -146.529, mean reward: -1.744 [-100.000, 16.024], mean action: 1.631 [0.000, 3.000],  loss: 19.023624, mse: 4334.384184, mean_q: 56.660432, mean_eps: 0.910735
  14996/150000: episode: 164, duration: 0.785s, episode steps:  76, steps per second:  97, episode reward: -95.048, mean reward: -1.251 [-100.000,  6.137], mean action: 1.724 [0.000, 3.000],  loss: 21.358466, mse: 4292.436578, mean_q: 56.623180, mean_eps: 0.910255
  15057/150000: episode: 165, duration: 0.613s, episode steps:  61, steps per second:  99, episode reward: -73.781, mean reward: -1.210 [-100.000,  5.161], mean action: 1.787 [0.000, 3.000],  loss: 10.494383, mse: 4304.906602, mean_q: 57.522600, mean_eps: 0.909844
  15146/150000: episode: 166, duration: 1.060s, episode steps:  89, steps per second:  84, episode reward: -170.657, mean reward: -1.917 [-100.000, 59.435], mean action: 1.562 [0.000, 3.000],  loss: 14.575930, mse: 4200.350800, mean_q: 56.302348, mean_eps: 0.909394
  15290/150000: episode: 167, duration: 1.634s, episode steps: 144, steps per second:  88, episode reward: -97.240, mean reward: -0.675 [-100.000,  8.471], mean action: 1.646 [0.000, 3.000],  loss: 13.750825, mse: 4249.682288, mean_q: 56.690659, mean_eps: 0.908695
  15355/150000: episode: 168, duration: 0.750s, episode steps:  65, steps per second:  87, episode reward: -94.690, mean reward: -1.457 [-100.000, 10.725], mean action: 1.631 [0.000, 3.000],  loss: 13.222451, mse: 4274.739772, mean_q: 56.629552, mean_eps: 0.908068
  15469/150000: episode: 169, duration: 1.059s, episode steps: 114, steps per second: 108, episode reward: -100.673, mean reward: -0.883 [-100.000, 12.451], mean action: 1.553 [0.000, 3.000],  loss: 9.173012, mse: 4287.153830, mean_q: 56.774163, mean_eps: 0.907531
  15530/150000: episode: 170, duration: 0.712s, episode steps:  61, steps per second:  86, episode reward: -94.335, mean reward: -1.546 [-100.000, 14.276], mean action: 1.475 [0.000, 3.000],  loss: 9.456076, mse: 4387.103844, mean_q: 57.744407, mean_eps: 0.907006
  15614/150000: episode: 171, duration: 0.951s, episode steps:  84, steps per second:  88, episode reward: -168.135, mean reward: -2.002 [-100.000, 27.791], mean action: 1.393 [0.000, 3.000],  loss: 9.912892, mse: 4349.021467, mean_q: 57.108744, mean_eps: 0.906571
  15740/150000: episode: 172, duration: 1.302s, episode steps: 126, steps per second:  97, episode reward: -136.598, mean reward: -1.084 [-100.000,  6.567], mean action: 1.540 [0.000, 3.000],  loss: 17.214858, mse: 4329.117492, mean_q: 57.365360, mean_eps: 0.905941
  15817/150000: episode: 173, duration: 0.649s, episode steps:  77, steps per second: 119, episode reward: -84.821, mean reward: -1.102 [-100.000, 12.061], mean action: 1.532 [0.000, 3.000],  loss: 15.081532, mse: 4308.837076, mean_q: 57.091517, mean_eps: 0.905332
  15880/150000: episode: 174, duration: 0.548s, episode steps:  63, steps per second: 115, episode reward: -63.028, mean reward: -1.000 [-100.000, 25.282], mean action: 1.683 [0.000, 3.000],  loss: 13.340471, mse: 4343.060345, mean_q: 57.744099, mean_eps: 0.904912
  15987/150000: episode: 175, duration: 0.926s, episode steps: 107, steps per second: 116, episode reward: -257.186, mean reward: -2.404 [-100.000,  0.677], mean action: 1.383 [0.000, 3.000],  loss: 23.455620, mse: 4364.758207, mean_q: 57.682953, mean_eps: 0.904402
  16059/150000: episode: 176, duration: 0.576s, episode steps:  72, steps per second: 125, episode reward: -150.400, mean reward: -2.089 [-100.000, 17.300], mean action: 1.514 [0.000, 3.000],  loss: 12.979760, mse: 4461.197832, mean_q: 58.272399, mean_eps: 0.903865
  16117/150000: episode: 177, duration: 0.494s, episode steps:  58, steps per second: 118, episode reward: -104.460, mean reward: -1.801 [-100.000, 10.284], mean action: 1.190 [0.000, 3.000],  loss: 12.125274, mse: 4525.537109, mean_q: 58.516712, mean_eps: 0.903475
  16199/150000: episode: 178, duration: 0.661s, episode steps:  82, steps per second: 124, episode reward: -104.782, mean reward: -1.278 [-100.000, 24.914], mean action: 1.256 [0.000, 3.000],  loss: 17.565560, mse: 4542.633307, mean_q: 58.620662, mean_eps: 0.903055
  16296/150000: episode: 179, duration: 0.796s, episode steps:  97, steps per second: 122, episode reward: -113.433, mean reward: -1.169 [-100.000, 13.229], mean action: 1.639 [0.000, 3.000],  loss: 14.390779, mse: 4450.693254, mean_q: 58.026176, mean_eps: 0.902518
  16361/150000: episode: 180, duration: 0.538s, episode steps:  65, steps per second: 121, episode reward: -64.304, mean reward: -0.989 [-100.000,  7.904], mean action: 1.492 [0.000, 3.000],  loss: 14.592732, mse: 4487.175868, mean_q: 58.842110, mean_eps: 0.902032
  16468/150000: episode: 181, duration: 0.890s, episode steps: 107, steps per second: 120, episode reward: -191.285, mean reward: -1.788 [-100.000,  7.902], mean action: 1.617 [0.000, 3.000],  loss: 18.706533, mse: 4467.458713, mean_q: 57.317345, mean_eps: 0.901516
  16572/150000: episode: 182, duration: 0.862s, episode steps: 104, steps per second: 121, episode reward: -66.813, mean reward: -0.642 [-100.000, 15.144], mean action: 1.423 [0.000, 3.000],  loss: 15.785490, mse: 4492.975898, mean_q: 57.888303, mean_eps: 0.900883
  16681/150000: episode: 183, duration: 0.887s, episode steps: 109, steps per second: 123, episode reward: -129.524, mean reward: -1.188 [-100.000,  6.489], mean action: 1.495 [0.000, 3.000],  loss: 11.485446, mse: 4622.924939, mean_q: 59.069742, mean_eps: 0.900244
  16775/150000: episode: 184, duration: 0.813s, episode steps:  94, steps per second: 116, episode reward: -74.458, mean reward: -0.792 [-100.000, 13.169], mean action: 1.617 [0.000, 3.000],  loss: 15.002838, mse: 4621.006145, mean_q: 58.757178, mean_eps: 0.899635
  16858/150000: episode: 185, duration: 0.669s, episode steps:  83, steps per second: 124, episode reward: -121.307, mean reward: -1.462 [-100.000, 23.824], mean action: 1.518 [0.000, 3.000],  loss: 20.069526, mse: 4553.155991, mean_q: 59.228875, mean_eps: 0.899104
  16924/150000: episode: 186, duration: 0.515s, episode steps:  66, steps per second: 128, episode reward: -105.562, mean reward: -1.599 [-100.000,  6.702], mean action: 1.606 [0.000, 3.000],  loss: 15.023585, mse: 4498.867487, mean_q: 58.377074, mean_eps: 0.898657
  17067/150000: episode: 187, duration: 1.193s, episode steps: 143, steps per second: 120, episode reward: -27.026, mean reward: -0.189 [-100.000, 19.288], mean action: 1.573 [0.000, 3.000],  loss: 13.902063, mse: 4544.663069, mean_q: 58.080869, mean_eps: 0.898030
  17156/150000: episode: 188, duration: 0.711s, episode steps:  89, steps per second: 125, episode reward: -129.833, mean reward: -1.459 [-100.000,  5.567], mean action: 1.461 [0.000, 3.000],  loss: 15.091853, mse: 4657.438263, mean_q: 59.151085, mean_eps: 0.897334
  17220/150000: episode: 189, duration: 0.526s, episode steps:  64, steps per second: 122, episode reward: -184.844, mean reward: -2.888 [-100.000,  3.860], mean action: 1.516 [0.000, 3.000],  loss: 16.264098, mse: 4612.917007, mean_q: 58.461991, mean_eps: 0.896875
  17296/150000: episode: 190, duration: 0.641s, episode steps:  76, steps per second: 119, episode reward: -88.265, mean reward: -1.161 [-100.000,  9.144], mean action: 1.355 [0.000, 3.000],  loss: 7.895362, mse: 4664.677320, mean_q: 59.623846, mean_eps: 0.896455
  17425/150000: episode: 191, duration: 1.032s, episode steps: 129, steps per second: 125, episode reward:  5.608, mean reward:  0.043 [-100.000, 115.427], mean action: 1.465 [0.000, 3.000],  loss: 12.401769, mse: 4762.981394, mean_q: 59.908553, mean_eps: 0.895840
  17522/150000: episode: 192, duration: 0.808s, episode steps:  97, steps per second: 120, episode reward: -31.327, mean reward: -0.323 [-100.000,  8.177], mean action: 1.536 [0.000, 3.000],  loss: 12.197014, mse: 4719.130497, mean_q: 59.805054, mean_eps: 0.895162
  17591/150000: episode: 193, duration: 0.565s, episode steps:  69, steps per second: 122, episode reward: -88.583, mean reward: -1.284 [-100.000,  6.722], mean action: 1.667 [0.000, 3.000],  loss: 7.353837, mse: 4622.678778, mean_q: 58.279133, mean_eps: 0.894664
  17680/150000: episode: 194, duration: 0.704s, episode steps:  89, steps per second: 126, episode reward: -146.197, mean reward: -1.643 [-100.000,  6.571], mean action: 1.663 [0.000, 3.000],  loss: 12.023736, mse: 4821.698346, mean_q: 59.616150, mean_eps: 0.894190
  17753/150000: episode: 195, duration: 0.587s, episode steps:  73, steps per second: 124, episode reward: -115.573, mean reward: -1.583 [-100.000,  7.374], mean action: 1.562 [0.000, 3.000],  loss: 16.978217, mse: 4746.871321, mean_q: 59.685756, mean_eps: 0.893704
  17858/150000: episode: 196, duration: 0.912s, episode steps: 105, steps per second: 115, episode reward: -119.789, mean reward: -1.141 [-100.000, 87.795], mean action: 1.648 [0.000, 3.000],  loss: 9.903748, mse: 4720.262179, mean_q: 60.106657, mean_eps: 0.893170
  17952/150000: episode: 197, duration: 0.832s, episode steps:  94, steps per second: 113, episode reward: -251.639, mean reward: -2.677 [-100.000,  0.598], mean action: 1.670 [0.000, 3.000],  loss: 9.000276, mse: 4791.943292, mean_q: 59.945836, mean_eps: 0.892573
  18052/150000: episode: 198, duration: 0.837s, episode steps: 100, steps per second: 119, episode reward: -217.680, mean reward: -2.177 [-100.000,  7.667], mean action: 1.640 [0.000, 3.000],  loss: 19.425753, mse: 4990.255249, mean_q: 61.321741, mean_eps: 0.891991
  18184/150000: episode: 199, duration: 1.069s, episode steps: 132, steps per second: 123, episode reward: -56.225, mean reward: -0.426 [-100.000, 61.529], mean action: 1.515 [0.000, 3.000],  loss: 11.857978, mse: 5017.711117, mean_q: 61.970041, mean_eps: 0.891295
  18281/150000: episode: 200, duration: 0.782s, episode steps:  97, steps per second: 124, episode reward: -64.210, mean reward: -0.662 [-100.000, 12.357], mean action: 1.546 [0.000, 3.000],  loss: 14.923703, mse: 4971.915842, mean_q: 61.664303, mean_eps: 0.890608
  18418/150000: episode: 201, duration: 1.168s, episode steps: 137, steps per second: 117, episode reward: -346.913, mean reward: -2.532 [-100.000, 67.712], mean action: 1.423 [0.000, 3.000],  loss: 16.072415, mse: 5072.350752, mean_q: 61.878360, mean_eps: 0.889906
  18517/150000: episode: 202, duration: 0.789s, episode steps:  99, steps per second: 125, episode reward: -119.362, mean reward: -1.206 [-100.000,  8.238], mean action: 1.525 [0.000, 3.000],  loss: 12.575201, mse: 5112.287595, mean_q: 62.860337, mean_eps: 0.889198
  18621/150000: episode: 203, duration: 1.037s, episode steps: 104, steps per second: 100, episode reward: -84.292, mean reward: -0.810 [-100.000,  7.530], mean action: 1.596 [0.000, 3.000],  loss: 13.022994, mse: 5189.448672, mean_q: 63.226902, mean_eps: 0.888589
  18680/150000: episode: 204, duration: 0.655s, episode steps:  59, steps per second:  90, episode reward: -105.508, mean reward: -1.788 [-100.000,  9.354], mean action: 1.407 [0.000, 3.000],  loss: 15.723062, mse: 5359.571790, mean_q: 63.874441, mean_eps: 0.888100
  18754/150000: episode: 205, duration: 0.877s, episode steps:  74, steps per second:  84, episode reward: -112.188, mean reward: -1.516 [-100.000, 16.475], mean action: 1.473 [0.000, 3.000],  loss: 11.271044, mse: 5099.524510, mean_q: 62.553377, mean_eps: 0.887701
  18858/150000: episode: 206, duration: 0.928s, episode steps: 104, steps per second: 112, episode reward: -141.190, mean reward: -1.358 [-100.000,  5.848], mean action: 1.635 [0.000, 3.000],  loss: 18.146873, mse: 5193.445503, mean_q: 63.157625, mean_eps: 0.887167
  18968/150000: episode: 207, duration: 0.893s, episode steps: 110, steps per second: 123, episode reward: -193.854, mean reward: -1.762 [-100.000,  4.955], mean action: 1.536 [0.000, 3.000],  loss: 20.792583, mse: 5154.942873, mean_q: 62.807468, mean_eps: 0.886525
  19058/150000: episode: 208, duration: 0.771s, episode steps:  90, steps per second: 117, episode reward: -110.594, mean reward: -1.229 [-100.000,  7.338], mean action: 1.533 [0.000, 3.000],  loss: 21.896984, mse: 5134.976790, mean_q: 62.374480, mean_eps: 0.885925
  19140/150000: episode: 209, duration: 0.677s, episode steps:  82, steps per second: 121, episode reward: -166.982, mean reward: -2.036 [-100.000, 44.290], mean action: 1.683 [0.000, 3.000],  loss: 23.748308, mse: 5288.552657, mean_q: 63.747909, mean_eps: 0.885409
  19256/150000: episode: 210, duration: 1.083s, episode steps: 116, steps per second: 107, episode reward: -278.746, mean reward: -2.403 [-100.000,  5.883], mean action: 1.543 [0.000, 3.000],  loss: 13.521142, mse: 5234.000192, mean_q: 62.786049, mean_eps: 0.884815
  19327/150000: episode: 211, duration: 0.619s, episode steps:  71, steps per second: 115, episode reward: -95.634, mean reward: -1.347 [-100.000, 20.253], mean action: 1.563 [0.000, 3.000],  loss: 12.752412, mse: 5294.048202, mean_q: 63.880697, mean_eps: 0.884254
  19488/150000: episode: 212, duration: 1.338s, episode steps: 161, steps per second: 120, episode reward: -35.529, mean reward: -0.221 [-100.000, 92.982], mean action: 1.590 [0.000, 3.000],  loss: 13.366189, mse: 5329.520020, mean_q: 63.574286, mean_eps: 0.883558
  19560/150000: episode: 213, duration: 0.626s, episode steps:  72, steps per second: 115, episode reward: -0.261, mean reward: -0.004 [-100.000, 61.885], mean action: 1.500 [0.000, 3.000],  loss: 21.377489, mse: 5303.765744, mean_q: 63.920648, mean_eps: 0.882859
  19649/150000: episode: 214, duration: 0.724s, episode steps:  89, steps per second: 123, episode reward: -78.622, mean reward: -0.883 [-100.000,  6.771], mean action: 1.393 [0.000, 3.000],  loss: 15.268321, mse: 5305.026521, mean_q: 63.596567, mean_eps: 0.882376
  19758/150000: episode: 215, duration: 0.958s, episode steps: 109, steps per second: 114, episode reward: -119.024, mean reward: -1.092 [-100.000, 11.005], mean action: 1.450 [0.000, 3.000],  loss: 22.458490, mse: 5362.813342, mean_q: 63.640965, mean_eps: 0.881782
  19843/150000: episode: 216, duration: 0.832s, episode steps:  85, steps per second: 102, episode reward: -144.326, mean reward: -1.698 [-100.000, 11.211], mean action: 1.518 [0.000, 3.000],  loss: 15.986581, mse: 5405.011098, mean_q: 64.547733, mean_eps: 0.881200
  19933/150000: episode: 217, duration: 0.775s, episode steps:  90, steps per second: 116, episode reward: -122.633, mean reward: -1.363 [-100.000,  7.395], mean action: 1.489 [0.000, 3.000],  loss: 24.778283, mse: 5325.267733, mean_q: 63.521940, mean_eps: 0.880675
  20048/150000: episode: 218, duration: 0.977s, episode steps: 115, steps per second: 118, episode reward: -86.854, mean reward: -0.755 [-100.000, 15.324], mean action: 1.600 [0.000, 3.000],  loss: 19.332009, mse: 5368.979936, mean_q: 64.111388, mean_eps: 0.880060
  20184/150000: episode: 219, duration: 1.155s, episode steps: 136, steps per second: 118, episode reward: -33.046, mean reward: -0.243 [-100.000, 106.419], mean action: 1.779 [0.000, 3.000],  loss: 9.379124, mse: 5455.803363, mean_q: 65.198675, mean_eps: 0.879307
  20307/150000: episode: 220, duration: 1.057s, episode steps: 123, steps per second: 116, episode reward: -195.539, mean reward: -1.590 [-100.000, 18.115], mean action: 1.504 [0.000, 3.000],  loss: 16.273803, mse: 5417.292133, mean_q: 63.950891, mean_eps: 0.878530
  20398/150000: episode: 221, duration: 0.735s, episode steps:  91, steps per second: 124, episode reward: -142.868, mean reward: -1.570 [-100.000,  8.004], mean action: 1.440 [0.000, 3.000],  loss: 10.890156, mse: 5327.603250, mean_q: 64.004639, mean_eps: 0.877888
  20512/150000: episode: 222, duration: 0.960s, episode steps: 114, steps per second: 119, episode reward: -214.174, mean reward: -1.879 [-100.000,  8.001], mean action: 1.667 [0.000, 3.000],  loss: 11.732546, mse: 5584.700247, mean_q: 65.638235, mean_eps: 0.877273
  20592/150000: episode: 223, duration: 0.734s, episode steps:  80, steps per second: 109, episode reward: -302.810, mean reward: -3.785 [-100.000, 41.947], mean action: 1.350 [0.000, 3.000],  loss: 17.611822, mse: 5358.435068, mean_q: 64.255443, mean_eps: 0.876691
  20673/150000: episode: 224, duration: 0.689s, episode steps:  81, steps per second: 117, episode reward: -40.535, mean reward: -0.500 [-100.000, 20.164], mean action: 1.642 [0.000, 3.000],  loss: 12.460603, mse: 5487.131107, mean_q: 64.890938, mean_eps: 0.876208
  20743/150000: episode: 225, duration: 0.576s, episode steps:  70, steps per second: 121, episode reward: -117.733, mean reward: -1.682 [-100.000, 11.912], mean action: 1.557 [0.000, 3.000],  loss: 8.442991, mse: 5589.164467, mean_q: 65.813078, mean_eps: 0.875755
  20882/150000: episode: 226, duration: 1.153s, episode steps: 139, steps per second: 121, episode reward: -144.152, mean reward: -1.037 [-100.000,  6.010], mean action: 1.748 [0.000, 3.000],  loss: 20.012379, mse: 5449.536324, mean_q: 64.609202, mean_eps: 0.875128
  20985/150000: episode: 227, duration: 0.837s, episode steps: 103, steps per second: 123, episode reward: -169.182, mean reward: -1.643 [-100.000, 13.520], mean action: 1.495 [0.000, 3.000],  loss: 19.132924, mse: 5455.196678, mean_q: 64.464227, mean_eps: 0.874402
  21094/150000: episode: 228, duration: 1.032s, episode steps: 109, steps per second: 106, episode reward: -42.011, mean reward: -0.385 [-100.000, 41.958], mean action: 1.771 [0.000, 3.000],  loss: 22.112353, mse: 5447.135464, mean_q: 65.450986, mean_eps: 0.873766
  21197/150000: episode: 229, duration: 0.912s, episode steps: 103, steps per second: 113, episode reward: -99.167, mean reward: -0.963 [-100.000, 11.518], mean action: 1.650 [0.000, 3.000],  loss: 17.086929, mse: 5399.839903, mean_q: 64.986494, mean_eps: 0.873130
  21315/150000: episode: 230, duration: 1.052s, episode steps: 118, steps per second: 112, episode reward: -80.980, mean reward: -0.686 [-100.000,  6.872], mean action: 1.602 [0.000, 3.000],  loss: 14.925104, mse: 5332.161000, mean_q: 64.331406, mean_eps: 0.872467
  21373/150000: episode: 231, duration: 0.517s, episode steps:  58, steps per second: 112, episode reward: -67.824, mean reward: -1.169 [-100.000,  8.942], mean action: 1.569 [0.000, 3.000],  loss: 16.791770, mse: 5466.616000, mean_q: 64.603593, mean_eps: 0.871939
  21438/150000: episode: 232, duration: 0.529s, episode steps:  65, steps per second: 123, episode reward: -76.553, mean reward: -1.178 [-100.000,  7.191], mean action: 1.662 [0.000, 3.000],  loss: 16.656027, mse: 5394.111779, mean_q: 64.670938, mean_eps: 0.871570
  21564/150000: episode: 233, duration: 1.046s, episode steps: 126, steps per second: 120, episode reward: -66.549, mean reward: -0.528 [-100.000, 13.750], mean action: 1.548 [0.000, 3.000],  loss: 14.327083, mse: 5397.078697, mean_q: 64.698689, mean_eps: 0.870997
  21634/150000: episode: 234, duration: 0.562s, episode steps:  70, steps per second: 125, episode reward: -60.548, mean reward: -0.865 [-100.000, 13.001], mean action: 1.514 [0.000, 3.000],  loss: 12.479017, mse: 5408.914526, mean_q: 65.091848, mean_eps: 0.870409
  21747/150000: episode: 235, duration: 0.928s, episode steps: 113, steps per second: 122, episode reward: -116.235, mean reward: -1.029 [-100.000, 11.318], mean action: 1.504 [0.000, 3.000],  loss: 24.302179, mse: 5445.945494, mean_q: 65.365844, mean_eps: 0.869860
  21841/150000: episode: 236, duration: 0.804s, episode steps:  94, steps per second: 117, episode reward: -99.866, mean reward: -1.062 [-100.000, 15.531], mean action: 1.628 [0.000, 3.000],  loss: 16.035812, mse: 5421.641531, mean_q: 65.257129, mean_eps: 0.869239
  21906/150000: episode: 237, duration: 0.526s, episode steps:  65, steps per second: 124, episode reward: -153.594, mean reward: -2.363 [-100.000,  7.648], mean action: 1.554 [0.000, 3.000],  loss: 13.336491, mse: 5591.384604, mean_q: 66.300268, mean_eps: 0.868762
  21990/150000: episode: 238, duration: 0.677s, episode steps:  84, steps per second: 124, episode reward: -63.003, mean reward: -0.750 [-100.000, 18.439], mean action: 1.619 [0.000, 3.000],  loss: 12.536664, mse: 5452.331249, mean_q: 65.249074, mean_eps: 0.868315
  22094/150000: episode: 239, duration: 0.887s, episode steps: 104, steps per second: 117, episode reward: -124.465, mean reward: -1.197 [-100.000,  9.039], mean action: 1.712 [0.000, 3.000],  loss: 23.304272, mse: 5314.063312, mean_q: 64.728121, mean_eps: 0.867751
  22160/150000: episode: 240, duration: 0.541s, episode steps:  66, steps per second: 122, episode reward: -89.340, mean reward: -1.354 [-100.000,  9.966], mean action: 1.621 [0.000, 3.000],  loss: 14.408776, mse: 5357.005116, mean_q: 65.074633, mean_eps: 0.867241
  22238/150000: episode: 241, duration: 0.622s, episode steps:  78, steps per second: 125, episode reward: -86.870, mean reward: -1.114 [-100.000,  6.413], mean action: 1.705 [0.000, 3.000],  loss: 19.141521, mse: 5399.576604, mean_q: 65.605781, mean_eps: 0.866809
  22315/150000: episode: 242, duration: 0.621s, episode steps:  77, steps per second: 124, episode reward: -93.032, mean reward: -1.208 [-100.000,  8.046], mean action: 1.286 [0.000, 3.000],  loss: 22.224334, mse: 5342.292484, mean_q: 63.643631, mean_eps: 0.866344
  22375/150000: episode: 243, duration: 0.531s, episode steps:  60, steps per second: 113, episode reward: -153.804, mean reward: -2.563 [-100.000,  5.287], mean action: 1.400 [0.000, 3.000],  loss: 13.511347, mse: 5349.074398, mean_q: 64.371753, mean_eps: 0.865933
  22455/150000: episode: 244, duration: 0.658s, episode steps:  80, steps per second: 121, episode reward: -9.170, mean reward: -0.115 [-100.000, 18.224], mean action: 1.538 [0.000, 3.000],  loss: 26.753555, mse: 5261.729626, mean_q: 63.698807, mean_eps: 0.865513
  22540/150000: episode: 245, duration: 0.682s, episode steps:  85, steps per second: 125, episode reward: -41.956, mean reward: -0.494 [-100.000, 23.743], mean action: 1.588 [0.000, 3.000],  loss: 32.696342, mse: 5467.834013, mean_q: 65.368291, mean_eps: 0.865018
  22663/150000: episode: 246, duration: 1.031s, episode steps: 123, steps per second: 119, episode reward: -232.413, mean reward: -1.890 [-100.000,  7.890], mean action: 1.569 [0.000, 3.000],  loss: 13.615739, mse: 5421.985612, mean_q: 65.674720, mean_eps: 0.864394
  22760/150000: episode: 247, duration: 0.991s, episode steps:  97, steps per second:  98, episode reward: -79.398, mean reward: -0.819 [-100.000,  7.722], mean action: 1.505 [0.000, 3.000],  loss: 18.198155, mse: 5518.154611, mean_q: 65.511514, mean_eps: 0.863734
  22870/150000: episode: 248, duration: 1.096s, episode steps: 110, steps per second: 100, episode reward: -171.906, mean reward: -1.563 [-100.000,  8.754], mean action: 1.518 [0.000, 3.000],  loss: 13.336495, mse: 5524.855205, mean_q: 65.877811, mean_eps: 0.863113
  23008/150000: episode: 249, duration: 1.243s, episode steps: 138, steps per second: 111, episode reward: -129.909, mean reward: -0.941 [-100.000,  6.064], mean action: 1.703 [0.000, 3.000],  loss: 23.355082, mse: 5550.757108, mean_q: 66.156234, mean_eps: 0.862369
  23087/150000: episode: 250, duration: 0.755s, episode steps:  79, steps per second: 105, episode reward: -60.356, mean reward: -0.764 [-100.000,  8.757], mean action: 1.823 [0.000, 3.000],  loss: 20.319305, mse: 5675.563884, mean_q: 66.993223, mean_eps: 0.861718
  23167/150000: episode: 251, duration: 0.720s, episode steps:  80, steps per second: 111, episode reward: -104.771, mean reward: -1.310 [-100.000, 13.347], mean action: 1.575 [0.000, 3.000],  loss: 11.488035, mse: 5693.753046, mean_q: 68.118013, mean_eps: 0.861241
  23254/150000: episode: 252, duration: 0.763s, episode steps:  87, steps per second: 114, episode reward: -106.336, mean reward: -1.222 [-100.000, 32.551], mean action: 1.540 [0.000, 3.000],  loss: 16.574849, mse: 5676.300489, mean_q: 67.526754, mean_eps: 0.860740
  23357/150000: episode: 253, duration: 0.899s, episode steps: 103, steps per second: 115, episode reward: -64.650, mean reward: -0.628 [-100.000, 17.194], mean action: 1.563 [0.000, 3.000],  loss: 16.530052, mse: 5655.714531, mean_q: 67.928820, mean_eps: 0.860170
  23421/150000: episode: 254, duration: 0.595s, episode steps:  64, steps per second: 108, episode reward: -67.359, mean reward: -1.052 [-100.000,  8.836], mean action: 1.547 [0.000, 3.000],  loss: 9.235263, mse: 5752.443604, mean_q: 69.148053, mean_eps: 0.859669
  23498/150000: episode: 255, duration: 0.636s, episode steps:  77, steps per second: 121, episode reward: -122.372, mean reward: -1.589 [-100.000, 11.772], mean action: 1.429 [0.000, 3.000],  loss: 20.784826, mse: 5730.325164, mean_q: 68.524464, mean_eps: 0.859246
  23641/150000: episode: 256, duration: 1.226s, episode steps: 143, steps per second: 117, episode reward: -232.297, mean reward: -1.624 [-100.000, 97.986], mean action: 1.350 [0.000, 3.000],  loss: 18.136028, mse: 5928.525980, mean_q: 68.689942, mean_eps: 0.858586
  23715/150000: episode: 257, duration: 0.588s, episode steps:  74, steps per second: 126, episode reward: -95.669, mean reward: -1.293 [-100.000, 15.734], mean action: 1.554 [0.000, 3.000],  loss: 39.491872, mse: 5856.609514, mean_q: 67.891044, mean_eps: 0.857935
  23776/150000: episode: 258, duration: 0.483s, episode steps:  61, steps per second: 126, episode reward: -90.949, mean reward: -1.491 [-100.000,  5.626], mean action: 1.623 [0.000, 3.000],  loss: 20.597324, mse: 5841.089011, mean_q: 67.407993, mean_eps: 0.857530
  23856/150000: episode: 259, duration: 0.696s, episode steps:  80, steps per second: 115, episode reward: -74.818, mean reward: -0.935 [-100.000, 10.831], mean action: 1.613 [0.000, 3.000],  loss: 9.827357, mse: 5762.970386, mean_q: 67.368497, mean_eps: 0.857107
  23930/150000: episode: 260, duration: 0.605s, episode steps:  74, steps per second: 122, episode reward: -139.991, mean reward: -1.892 [-100.000, 21.046], mean action: 1.405 [0.000, 3.000],  loss: 19.767899, mse: 5916.236239, mean_q: 69.750680, mean_eps: 0.856645
  24060/150000: episode: 261, duration: 1.063s, episode steps: 130, steps per second: 122, episode reward: -86.447, mean reward: -0.665 [-100.000,  6.884], mean action: 1.569 [0.000, 3.000],  loss: 18.312185, mse: 5888.390813, mean_q: 68.616482, mean_eps: 0.856033
  24130/150000: episode: 262, duration: 0.580s, episode steps:  70, steps per second: 121, episode reward: -68.796, mean reward: -0.983 [-100.000,  8.127], mean action: 1.529 [0.000, 3.000],  loss: 26.040754, mse: 5868.756201, mean_q: 68.824026, mean_eps: 0.855433
  24213/150000: episode: 263, duration: 0.685s, episode steps:  83, steps per second: 121, episode reward: -112.566, mean reward: -1.356 [-100.000,  6.255], mean action: 1.602 [0.000, 3.000],  loss: 12.226155, mse: 5743.369805, mean_q: 68.155963, mean_eps: 0.854974
  24318/150000: episode: 264, duration: 0.838s, episode steps: 105, steps per second: 125, episode reward: -117.317, mean reward: -1.117 [-100.000, 18.568], mean action: 1.686 [0.000, 3.000],  loss: 12.269145, mse: 5944.730032, mean_q: 69.554611, mean_eps: 0.854410
  24399/150000: episode: 265, duration: 0.683s, episode steps:  81, steps per second: 119, episode reward: -101.126, mean reward: -1.248 [-100.000,  7.524], mean action: 1.691 [0.000, 3.000],  loss: 11.279987, mse: 5974.831278, mean_q: 69.449386, mean_eps: 0.853852
  24462/150000: episode: 266, duration: 0.505s, episode steps:  63, steps per second: 125, episode reward: -89.567, mean reward: -1.422 [-100.000,  9.653], mean action: 1.714 [0.000, 3.000],  loss: 31.156360, mse: 6023.131038, mean_q: 69.008971, mean_eps: 0.853420
  24542/150000: episode: 267, duration: 0.655s, episode steps:  80, steps per second: 122, episode reward: -26.214, mean reward: -0.328 [-100.000, 19.674], mean action: 1.700 [0.000, 3.000],  loss: 18.658425, mse: 5957.158484, mean_q: 69.489846, mean_eps: 0.852991
  24703/150000: episode: 268, duration: 1.337s, episode steps: 161, steps per second: 120, episode reward: -82.822, mean reward: -0.514 [-100.000,  6.242], mean action: 1.522 [0.000, 3.000],  loss: 10.152019, mse: 5932.634717, mean_q: 68.772640, mean_eps: 0.852268
  24787/150000: episode: 269, duration: 0.681s, episode steps:  84, steps per second: 123, episode reward: -108.849, mean reward: -1.296 [-100.000, 26.593], mean action: 1.345 [0.000, 3.000],  loss: 15.830477, mse: 5949.570225, mean_q: 69.289184, mean_eps: 0.851533
  24867/150000: episode: 270, duration: 0.715s, episode steps:  80, steps per second: 112, episode reward: -72.630, mean reward: -0.908 [-100.000,  9.046], mean action: 1.475 [0.000, 3.000],  loss: 18.227208, mse: 6020.567102, mean_q: 69.605226, mean_eps: 0.851041
  25004/150000: episode: 271, duration: 1.140s, episode steps: 137, steps per second: 120, episode reward: -133.766, mean reward: -0.976 [-100.000, 29.517], mean action: 1.547 [0.000, 3.000],  loss: 18.868914, mse: 6058.299673, mean_q: 69.738629, mean_eps: 0.850390
  25072/150000: episode: 272, duration: 0.544s, episode steps:  68, steps per second: 125, episode reward: -51.219, mean reward: -0.753 [-100.000, 11.942], mean action: 1.750 [0.000, 3.000],  loss: 20.979640, mse: 6358.746589, mean_q: 71.978528, mean_eps: 0.849775
  25161/150000: episode: 273, duration: 0.748s, episode steps:  89, steps per second: 119, episode reward: -421.385, mean reward: -4.735 [-100.000, -0.402], mean action: 1.719 [0.000, 3.000],  loss: 36.853582, mse: 6240.885545, mean_q: 70.432388, mean_eps: 0.849304
  25232/150000: episode: 274, duration: 0.578s, episode steps:  71, steps per second: 123, episode reward: -67.070, mean reward: -0.945 [-100.000,  7.173], mean action: 1.606 [0.000, 3.000],  loss: 27.294617, mse: 6141.281683, mean_q: 69.956977, mean_eps: 0.848824
  25293/150000: episode: 275, duration: 0.494s, episode steps:  61, steps per second: 124, episode reward: -61.270, mean reward: -1.004 [-100.000, 12.626], mean action: 1.623 [0.000, 3.000],  loss: 14.327753, mse: 6138.729428, mean_q: 69.089208, mean_eps: 0.848428
  25373/150000: episode: 276, duration: 0.657s, episode steps:  80, steps per second: 122, episode reward: -84.143, mean reward: -1.052 [-100.000, 17.147], mean action: 1.712 [0.000, 3.000],  loss: 22.624518, mse: 6287.485931, mean_q: 71.747059, mean_eps: 0.848005
  25443/150000: episode: 277, duration: 0.602s, episode steps:  70, steps per second: 116, episode reward: -55.276, mean reward: -0.790 [-100.000, 12.508], mean action: 1.571 [0.000, 3.000],  loss: 13.639049, mse: 6359.845989, mean_q: 71.350315, mean_eps: 0.847555
  25522/150000: episode: 278, duration: 0.637s, episode steps:  79, steps per second: 124, episode reward: -79.870, mean reward: -1.011 [-100.000,  7.200], mean action: 1.570 [0.000, 3.000],  loss: 17.096489, mse: 6199.680534, mean_q: 70.529568, mean_eps: 0.847108
  25609/150000: episode: 279, duration: 0.705s, episode steps:  87, steps per second: 123, episode reward: -79.042, mean reward: -0.909 [-100.000,  8.742], mean action: 1.437 [0.000, 3.000],  loss: 18.136162, mse: 5990.488444, mean_q: 67.905548, mean_eps: 0.846610
  25715/150000: episode: 280, duration: 0.894s, episode steps: 106, steps per second: 119, episode reward: -27.583, mean reward: -0.260 [-100.000, 12.766], mean action: 1.613 [0.000, 3.000],  loss: 11.755402, mse: 6215.234974, mean_q: 69.694752, mean_eps: 0.846031
  25783/150000: episode: 281, duration: 0.548s, episode steps:  68, steps per second: 124, episode reward: -142.022, mean reward: -2.089 [-100.000, 44.622], mean action: 1.662 [0.000, 3.000],  loss: 21.837786, mse: 6466.933414, mean_q: 71.856283, mean_eps: 0.845509
  25844/150000: episode: 282, duration: 0.496s, episode steps:  61, steps per second: 123, episode reward: -66.238, mean reward: -1.086 [-100.000, 40.017], mean action: 1.803 [0.000, 3.000],  loss: 16.489269, mse: 6351.375128, mean_q: 70.119962, mean_eps: 0.845122
  25952/150000: episode: 283, duration: 0.898s, episode steps: 108, steps per second: 120, episode reward: -98.137, mean reward: -0.909 [-100.000,  7.283], mean action: 1.796 [0.000, 3.000],  loss: 16.085647, mse: 6253.584916, mean_q: 70.036176, mean_eps: 0.844615
  26018/150000: episode: 284, duration: 0.554s, episode steps:  66, steps per second: 119, episode reward: -34.980, mean reward: -0.530 [-100.000, 11.481], mean action: 1.621 [0.000, 3.000],  loss: 15.525825, mse: 6067.839622, mean_q: 68.650879, mean_eps: 0.844093
  26127/150000: episode: 285, duration: 0.898s, episode steps: 109, steps per second: 121, episode reward: -161.369, mean reward: -1.480 [-100.000,  3.158], mean action: 1.560 [0.000, 3.000],  loss: 13.305653, mse: 6202.422475, mean_q: 69.085959, mean_eps: 0.843568
  26224/150000: episode: 286, duration: 0.830s, episode steps:  97, steps per second: 117, episode reward: -99.215, mean reward: -1.023 [-100.000,  9.930], mean action: 1.619 [0.000, 3.000],  loss: 10.858584, mse: 6438.104855, mean_q: 71.768424, mean_eps: 0.842950
  26344/150000: episode: 287, duration: 0.965s, episode steps: 120, steps per second: 124, episode reward: -70.314, mean reward: -0.586 [-100.000,  6.609], mean action: 1.758 [0.000, 3.000],  loss: 19.628017, mse: 6452.321676, mean_q: 72.143996, mean_eps: 0.842299
  26457/150000: episode: 288, duration: 0.972s, episode steps: 113, steps per second: 116, episode reward: -70.895, mean reward: -0.627 [-100.000, 21.188], mean action: 1.575 [0.000, 3.000],  loss: 18.561456, mse: 6460.840825, mean_q: 72.434217, mean_eps: 0.841600
  26530/150000: episode: 289, duration: 0.639s, episode steps:  73, steps per second: 114, episode reward: -40.934, mean reward: -0.561 [-100.000, 22.258], mean action: 1.562 [0.000, 3.000],  loss: 12.272581, mse: 6347.137495, mean_q: 71.832459, mean_eps: 0.841042
  26630/150000: episode: 290, duration: 0.813s, episode steps: 100, steps per second: 123, episode reward: -104.958, mean reward: -1.050 [-100.000,  5.386], mean action: 1.470 [0.000, 3.000],  loss: 19.114091, mse: 6340.767871, mean_q: 71.060863, mean_eps: 0.840523
  26755/150000: episode: 291, duration: 1.055s, episode steps: 125, steps per second: 119, episode reward: -124.918, mean reward: -0.999 [-100.000,  8.309], mean action: 1.800 [0.000, 3.000],  loss: 25.995939, mse: 6476.147715, mean_q: 71.961702, mean_eps: 0.839848
  26838/150000: episode: 292, duration: 0.677s, episode steps:  83, steps per second: 123, episode reward: -82.339, mean reward: -0.992 [-100.000,  6.810], mean action: 1.590 [0.000, 3.000],  loss: 34.072407, mse: 6585.406485, mean_q: 73.157849, mean_eps: 0.839224
  26921/150000: episode: 293, duration: 0.675s, episode steps:  83, steps per second: 123, episode reward: -97.252, mean reward: -1.172 [-100.000, 11.657], mean action: 1.518 [0.000, 3.000],  loss: 17.321193, mse: 6438.852827, mean_q: 71.875850, mean_eps: 0.838726
  26999/150000: episode: 294, duration: 0.698s, episode steps:  78, steps per second: 112, episode reward: -136.047, mean reward: -1.744 [-100.000, 16.205], mean action: 1.538 [0.000, 3.000],  loss: 17.043471, mse: 6662.744548, mean_q: 73.348980, mean_eps: 0.838243
  27074/150000: episode: 295, duration: 0.622s, episode steps:  75, steps per second: 121, episode reward: -90.384, mean reward: -1.205 [-100.000, 11.498], mean action: 1.707 [0.000, 3.000],  loss: 23.486455, mse: 6627.519303, mean_q: 73.288192, mean_eps: 0.837784
  27168/150000: episode: 296, duration: 0.756s, episode steps:  94, steps per second: 124, episode reward: -81.212, mean reward: -0.864 [-100.000,  9.815], mean action: 1.553 [0.000, 3.000],  loss: 18.265052, mse: 6642.661372, mean_q: 72.993104, mean_eps: 0.837277
  27252/150000: episode: 297, duration: 0.729s, episode steps:  84, steps per second: 115, episode reward: -117.219, mean reward: -1.395 [-100.000,  9.970], mean action: 1.488 [0.000, 3.000],  loss: 34.135102, mse: 6688.256946, mean_q: 73.196318, mean_eps: 0.836743
  27322/150000: episode: 298, duration: 0.564s, episode steps:  70, steps per second: 124, episode reward: -122.829, mean reward: -1.755 [-100.000,  6.786], mean action: 1.243 [0.000, 3.000],  loss: 11.958994, mse: 6625.710972, mean_q: 73.045316, mean_eps: 0.836281
  27433/150000: episode: 299, duration: 0.880s, episode steps: 111, steps per second: 126, episode reward: -180.331, mean reward: -1.625 [-100.000, 28.579], mean action: 1.441 [0.000, 3.000],  loss: 20.398243, mse: 6713.868305, mean_q: 73.805274, mean_eps: 0.835738
  27527/150000: episode: 300, duration: 0.825s, episode steps:  94, steps per second: 114, episode reward: -111.209, mean reward: -1.183 [-100.000, 11.142], mean action: 1.596 [0.000, 3.000],  loss: 10.430994, mse: 6683.031146, mean_q: 72.746613, mean_eps: 0.835123
  27648/150000: episode: 301, duration: 0.981s, episode steps: 121, steps per second: 123, episode reward: -53.561, mean reward: -0.443 [-100.000,  9.389], mean action: 1.653 [0.000, 3.000],  loss: 10.235721, mse: 6573.140213, mean_q: 72.302033, mean_eps: 0.834478
  27739/150000: episode: 302, duration: 0.742s, episode steps:  91, steps per second: 123, episode reward: -70.690, mean reward: -0.777 [-100.000, 12.508], mean action: 1.659 [0.000, 3.000],  loss: 10.914867, mse: 6698.981027, mean_q: 73.183004, mean_eps: 0.833842
  27857/150000: episode: 303, duration: 1.001s, episode steps: 118, steps per second: 118, episode reward: -287.060, mean reward: -2.433 [-100.000, 54.699], mean action: 1.602 [0.000, 3.000],  loss: 8.441074, mse: 6740.141024, mean_q: 73.443645, mean_eps: 0.833215
  27928/150000: episode: 304, duration: 0.572s, episode steps:  71, steps per second: 124, episode reward: -41.196, mean reward: -0.580 [-100.000,  8.235], mean action: 1.662 [0.000, 3.000],  loss: 12.531414, mse: 6842.565086, mean_q: 74.498671, mean_eps: 0.832648
  27994/150000: episode: 305, duration: 0.547s, episode steps:  66, steps per second: 121, episode reward: -134.839, mean reward: -2.043 [-100.000, 13.451], mean action: 1.621 [0.000, 3.000],  loss: 14.657390, mse: 6793.741470, mean_q: 73.521943, mean_eps: 0.832237
  28082/150000: episode: 306, duration: 0.746s, episode steps:  88, steps per second: 118, episode reward: -43.233, mean reward: -0.491 [-100.000, 13.188], mean action: 1.818 [0.000, 3.000],  loss: 17.089149, mse: 6934.785389, mean_q: 75.474122, mean_eps: 0.831775
  28144/150000: episode: 307, duration: 0.497s, episode steps:  62, steps per second: 125, episode reward: -62.558, mean reward: -1.009 [-100.000, 12.585], mean action: 1.661 [0.000, 3.000],  loss: 15.203383, mse: 6908.231934, mean_q: 75.072715, mean_eps: 0.831325
  28229/150000: episode: 308, duration: 0.686s, episode steps:  85, steps per second: 124, episode reward: -270.715, mean reward: -3.185 [-100.000, 129.750], mean action: 1.588 [0.000, 3.000],  loss: 9.868962, mse: 6729.379722, mean_q: 73.785707, mean_eps: 0.830884
  28358/150000: episode: 309, duration: 1.074s, episode steps: 129, steps per second: 120, episode reward: -87.762, mean reward: -0.680 [-100.000, 12.096], mean action: 1.690 [0.000, 3.000],  loss: 21.329745, mse: 6684.552265, mean_q: 73.903457, mean_eps: 0.830242
  28459/150000: episode: 310, duration: 0.814s, episode steps: 101, steps per second: 124, episode reward: -77.525, mean reward: -0.768 [-100.000,  6.673], mean action: 1.594 [0.000, 3.000],  loss: 10.885811, mse: 6958.534112, mean_q: 75.292113, mean_eps: 0.829552
  28580/150000: episode: 311, duration: 1.027s, episode steps: 121, steps per second: 118, episode reward: -163.286, mean reward: -1.349 [-100.000, 14.516], mean action: 1.636 [0.000, 3.000],  loss: 25.590913, mse: 6913.147893, mean_q: 74.270472, mean_eps: 0.828886
  28655/150000: episode: 312, duration: 0.601s, episode steps:  75, steps per second: 125, episode reward: -58.213, mean reward: -0.776 [-100.000, 15.204], mean action: 1.347 [0.000, 3.000],  loss: 9.530510, mse: 6706.712539, mean_q: 73.014800, mean_eps: 0.828298
  28770/150000: episode: 313, duration: 0.915s, episode steps: 115, steps per second: 126, episode reward: -160.852, mean reward: -1.399 [-100.000,  7.816], mean action: 1.417 [0.000, 3.000],  loss: 14.671739, mse: 6874.633577, mean_q: 74.489120, mean_eps: 0.827728
  28885/150000: episode: 314, duration: 1.096s, episode steps: 115, steps per second: 105, episode reward: -78.049, mean reward: -0.679 [-100.000, 11.007], mean action: 1.617 [0.000, 3.000],  loss: 15.442428, mse: 6939.703639, mean_q: 74.937020, mean_eps: 0.827038
  28983/150000: episode: 315, duration: 0.856s, episode steps:  98, steps per second: 115, episode reward: -74.755, mean reward: -0.763 [-100.000,  7.019], mean action: 1.622 [0.000, 3.000],  loss: 23.970355, mse: 6938.832589, mean_q: 75.179449, mean_eps: 0.826399
  29063/150000: episode: 316, duration: 0.731s, episode steps:  80, steps per second: 110, episode reward: -73.532, mean reward: -0.919 [-100.000, 17.741], mean action: 1.425 [0.000, 3.000],  loss: 12.595623, mse: 6854.422186, mean_q: 74.522619, mean_eps: 0.825865
  29160/150000: episode: 317, duration: 0.824s, episode steps:  97, steps per second: 118, episode reward: -127.478, mean reward: -1.314 [-100.000,  9.418], mean action: 1.474 [0.000, 3.000],  loss: 22.597779, mse: 7078.245439, mean_q: 75.242302, mean_eps: 0.825334
  29241/150000: episode: 318, duration: 0.686s, episode steps:  81, steps per second: 118, episode reward: -59.512, mean reward: -0.735 [-100.000, 11.793], mean action: 1.432 [0.000, 3.000],  loss: 15.082579, mse: 6987.017156, mean_q: 75.408734, mean_eps: 0.824800
  29326/150000: episode: 319, duration: 0.733s, episode steps:  85, steps per second: 116, episode reward: -40.708, mean reward: -0.479 [-100.000, 22.045], mean action: 1.529 [0.000, 3.000],  loss: 15.149873, mse: 6922.052240, mean_q: 73.987096, mean_eps: 0.824302
  29413/150000: episode: 320, duration: 0.692s, episode steps:  87, steps per second: 126, episode reward: -146.565, mean reward: -1.685 [-100.000, 39.069], mean action: 1.540 [0.000, 3.000],  loss: 16.957083, mse: 6745.228903, mean_q: 72.529181, mean_eps: 0.823786
  29523/150000: episode: 321, duration: 0.906s, episode steps: 110, steps per second: 121, episode reward: -110.853, mean reward: -1.008 [-100.000,  9.606], mean action: 1.627 [0.000, 3.000],  loss: 17.592648, mse: 6970.493701, mean_q: 73.952684, mean_eps: 0.823195
  29630/150000: episode: 322, duration: 0.915s, episode steps: 107, steps per second: 117, episode reward: -124.161, mean reward: -1.160 [-100.000, 16.043], mean action: 1.421 [0.000, 3.000],  loss: 20.349529, mse: 7166.197498, mean_q: 76.585691, mean_eps: 0.822544
  29755/150000: episode: 323, duration: 1.019s, episode steps: 125, steps per second: 123, episode reward: -88.009, mean reward: -0.704 [-100.000,  8.654], mean action: 1.568 [0.000, 3.000],  loss: 16.766393, mse: 6897.241996, mean_q: 73.982921, mean_eps: 0.821848
  29818/150000: episode: 324, duration: 0.544s, episode steps:  63, steps per second: 116, episode reward: -102.810, mean reward: -1.632 [-100.000,  9.154], mean action: 1.508 [0.000, 3.000],  loss: 34.748672, mse: 7152.417643, mean_q: 76.023754, mean_eps: 0.821284
  29990/150000: episode: 325, duration: 1.379s, episode steps: 172, steps per second: 125, episode reward: -51.233, mean reward: -0.298 [-100.000, 11.474], mean action: 1.599 [0.000, 3.000],  loss: 16.012247, mse: 7086.706980, mean_q: 76.117368, mean_eps: 0.820579
  30112/150000: episode: 326, duration: 1.082s, episode steps: 122, steps per second: 113, episode reward: -135.096, mean reward: -1.107 [-100.000, 11.222], mean action: 1.590 [0.000, 3.000],  loss: 23.103931, mse: 7176.982838, mean_q: 75.574164, mean_eps: 0.819697
  30208/150000: episode: 327, duration: 0.769s, episode steps:  96, steps per second: 125, episode reward: -77.536, mean reward: -0.808 [-100.000, 11.886], mean action: 1.458 [0.000, 3.000],  loss: 16.486189, mse: 7196.603521, mean_q: 75.797354, mean_eps: 0.819043
  30290/150000: episode: 328, duration: 0.661s, episode steps:  82, steps per second: 124, episode reward: -82.364, mean reward: -1.004 [-100.000,  6.085], mean action: 1.415 [0.000, 3.000],  loss: 32.200172, mse: 7078.032102, mean_q: 75.491817, mean_eps: 0.818509
  30352/150000: episode: 329, duration: 0.533s, episode steps:  62, steps per second: 116, episode reward: -43.556, mean reward: -0.703 [-100.000, 12.139], mean action: 1.726 [0.000, 3.000],  loss: 14.363132, mse: 7071.652218, mean_q: 74.028002, mean_eps: 0.818077
  30448/150000: episode: 330, duration: 0.788s, episode steps:  96, steps per second: 122, episode reward: -106.475, mean reward: -1.109 [-100.000,  7.079], mean action: 1.688 [0.000, 3.000],  loss: 11.885238, mse: 7184.104080, mean_q: 76.257105, mean_eps: 0.817603
  30546/150000: episode: 331, duration: 0.938s, episode steps:  98, steps per second: 104, episode reward: -106.189, mean reward: -1.084 [-100.000,  6.565], mean action: 1.673 [0.000, 3.000],  loss: 18.389335, mse: 7181.749297, mean_q: 75.485890, mean_eps: 0.817021
  30673/150000: episode: 332, duration: 1.199s, episode steps: 127, steps per second: 106, episode reward: -86.465, mean reward: -0.681 [-100.000, 11.439], mean action: 1.457 [0.000, 3.000],  loss: 26.846740, mse: 7139.637049, mean_q: 74.998495, mean_eps: 0.816346
  30763/150000: episode: 333, duration: 0.821s, episode steps:  90, steps per second: 110, episode reward: -132.261, mean reward: -1.470 [-100.000, 21.379], mean action: 1.767 [0.000, 3.000],  loss: 19.095025, mse: 7155.058946, mean_q: 75.578296, mean_eps: 0.815695
  30860/150000: episode: 334, duration: 0.915s, episode steps:  97, steps per second: 106, episode reward: -482.804, mean reward: -4.977 [-100.000, -0.100], mean action: 1.701 [0.000, 3.000],  loss: 20.068551, mse: 7128.158636, mean_q: 74.645347, mean_eps: 0.815134
  30957/150000: episode: 335, duration: 0.846s, episode steps:  97, steps per second: 115, episode reward: -83.284, mean reward: -0.859 [-100.000, 18.929], mean action: 1.619 [0.000, 3.000],  loss: 32.405687, mse: 7041.375221, mean_q: 74.607850, mean_eps: 0.814552
  31067/150000: episode: 336, duration: 0.930s, episode steps: 110, steps per second: 118, episode reward: -87.122, mean reward: -0.792 [-100.000,  8.584], mean action: 1.691 [0.000, 3.000],  loss: 26.497863, mse: 7132.624836, mean_q: 75.412256, mean_eps: 0.813931
  31191/150000: episode: 337, duration: 1.004s, episode steps: 124, steps per second: 124, episode reward: -156.365, mean reward: -1.261 [-100.000,  4.924], mean action: 1.565 [0.000, 3.000],  loss: 22.494010, mse: 7327.254962, mean_q: 76.011779, mean_eps: 0.813229
  31320/150000: episode: 338, duration: 1.057s, episode steps: 129, steps per second: 122, episode reward: -158.695, mean reward: -1.230 [-100.000,  4.290], mean action: 1.752 [0.000, 3.000],  loss: 24.917399, mse: 7402.055270, mean_q: 77.007981, mean_eps: 0.812470
  31396/150000: episode: 339, duration: 0.735s, episode steps:  76, steps per second: 103, episode reward: -109.977, mean reward: -1.447 [-100.000,  6.842], mean action: 1.737 [0.000, 3.000],  loss: 15.675831, mse: 7240.215955, mean_q: 75.293938, mean_eps: 0.811855
  31479/150000: episode: 340, duration: 0.805s, episode steps:  83, steps per second: 103, episode reward: -146.680, mean reward: -1.767 [-100.000, 24.348], mean action: 1.530 [0.000, 3.000],  loss: 15.099500, mse: 7424.718821, mean_q: 76.001837, mean_eps: 0.811378
  31582/150000: episode: 341, duration: 0.901s, episode steps: 103, steps per second: 114, episode reward: -64.326, mean reward: -0.625 [-100.000, 13.834], mean action: 1.466 [0.000, 3.000],  loss: 17.824074, mse: 7361.498360, mean_q: 75.671077, mean_eps: 0.810820
  31661/150000: episode: 342, duration: 0.642s, episode steps:  79, steps per second: 123, episode reward: -122.420, mean reward: -1.550 [-100.000, 19.491], mean action: 1.835 [0.000, 3.000],  loss: 17.144613, mse: 7438.906738, mean_q: 76.766044, mean_eps: 0.810274
  31782/150000: episode: 343, duration: 0.976s, episode steps: 121, steps per second: 124, episode reward: -29.172, mean reward: -0.241 [-100.000, 18.611], mean action: 1.628 [0.000, 3.000],  loss: 25.601823, mse: 7449.018183, mean_q: 76.277811, mean_eps: 0.809674
  31901/150000: episode: 344, duration: 0.998s, episode steps: 119, steps per second: 119, episode reward: -35.072, mean reward: -0.295 [-100.000, 11.162], mean action: 1.622 [0.000, 3.000],  loss: 14.367054, mse: 7451.962636, mean_q: 75.809367, mean_eps: 0.808954
  32007/150000: episode: 345, duration: 0.850s, episode steps: 106, steps per second: 125, episode reward: -54.592, mean reward: -0.515 [-100.000, 18.813], mean action: 1.670 [0.000, 3.000],  loss: 14.694106, mse: 7398.329309, mean_q: 76.176697, mean_eps: 0.808279
  32078/150000: episode: 346, duration: 0.592s, episode steps:  71, steps per second: 120, episode reward: -57.660, mean reward: -0.812 [-100.000,  7.536], mean action: 1.634 [0.000, 3.000],  loss: 19.379452, mse: 7581.715470, mean_q: 77.866072, mean_eps: 0.807748
  32174/150000: episode: 347, duration: 0.786s, episode steps:  96, steps per second: 122, episode reward: -112.845, mean reward: -1.175 [-100.000,  6.679], mean action: 1.750 [0.000, 3.000],  loss: 17.557418, mse: 7518.737503, mean_q: 76.802535, mean_eps: 0.807247
  32245/150000: episode: 348, duration: 0.565s, episode steps:  71, steps per second: 126, episode reward: -67.743, mean reward: -0.954 [-100.000, 11.871], mean action: 1.915 [0.000, 3.000],  loss: 15.878591, mse: 7196.041751, mean_q: 75.059814, mean_eps: 0.806746
  32334/150000: episode: 349, duration: 0.729s, episode steps:  89, steps per second: 122, episode reward: -119.799, mean reward: -1.346 [-100.000,  6.397], mean action: 1.449 [0.000, 3.000],  loss: 9.117121, mse: 7451.326474, mean_q: 75.061342, mean_eps: 0.806266
  32433/150000: episode: 350, duration: 0.814s, episode steps:  99, steps per second: 122, episode reward: -235.758, mean reward: -2.381 [-100.000, 96.189], mean action: 1.444 [0.000, 3.000],  loss: 15.517398, mse: 7524.454688, mean_q: 75.812182, mean_eps: 0.805702
  32529/150000: episode: 351, duration: 0.766s, episode steps:  96, steps per second: 125, episode reward: -279.451, mean reward: -2.911 [-100.000, 113.603], mean action: 1.542 [0.000, 3.000],  loss: 14.999534, mse: 7534.402903, mean_q: 75.726468, mean_eps: 0.805117
  32637/150000: episode: 352, duration: 0.927s, episode steps: 108, steps per second: 117, episode reward: -67.867, mean reward: -0.628 [-100.000, 13.667], mean action: 1.583 [0.000, 3.000],  loss: 12.722117, mse: 7467.486486, mean_q: 76.990461, mean_eps: 0.804505
  32745/150000: episode: 353, duration: 0.895s, episode steps: 108, steps per second: 121, episode reward: -150.817, mean reward: -1.396 [-100.000,  7.126], mean action: 1.528 [0.000, 3.000],  loss: 22.963991, mse: 7525.270001, mean_q: 76.317033, mean_eps: 0.803857
  32882/150000: episode: 354, duration: 1.132s, episode steps: 137, steps per second: 121, episode reward: 11.092, mean reward:  0.081 [-100.000, 45.318], mean action: 1.547 [0.000, 3.000],  loss: 22.716291, mse: 7532.765393, mean_q: 77.449374, mean_eps: 0.803122
  32990/150000: episode: 355, duration: 0.889s, episode steps: 108, steps per second: 121, episode reward: -72.637, mean reward: -0.673 [-100.000, 66.076], mean action: 1.472 [0.000, 3.000],  loss: 14.949159, mse: 7681.184946, mean_q: 79.059675, mean_eps: 0.802387
  33076/150000: episode: 356, duration: 0.694s, episode steps:  86, steps per second: 124, episode reward: -29.101, mean reward: -0.338 [-100.000,  9.113], mean action: 1.523 [0.000, 3.000],  loss: 9.782551, mse: 7787.908794, mean_q: 76.949873, mean_eps: 0.801805
  33208/150000: episode: 357, duration: 1.124s, episode steps: 132, steps per second: 117, episode reward: -105.772, mean reward: -0.801 [-100.000,  7.389], mean action: 1.568 [0.000, 3.000],  loss: 16.415803, mse: 7831.652203, mean_q: 77.674283, mean_eps: 0.801151
  33279/150000: episode: 358, duration: 0.596s, episode steps:  71, steps per second: 119, episode reward: -77.902, mean reward: -1.097 [-100.000, 12.431], mean action: 1.521 [0.000, 3.000],  loss: 26.635844, mse: 7801.968557, mean_q: 77.451936, mean_eps: 0.800542
  33390/150000: episode: 359, duration: 0.900s, episode steps: 111, steps per second: 123, episode reward: -117.507, mean reward: -1.059 [-100.000, 12.760], mean action: 1.586 [0.000, 3.000],  loss: 29.418355, mse: 7812.872515, mean_q: 78.362801, mean_eps: 0.799996
  33475/150000: episode: 360, duration: 0.723s, episode steps:  85, steps per second: 117, episode reward: -101.069, mean reward: -1.189 [-100.000, 10.694], mean action: 1.412 [0.000, 3.000],  loss: 23.633129, mse: 7913.612971, mean_q: 78.151819, mean_eps: 0.799408
  33576/150000: episode: 361, duration: 0.811s, episode steps: 101, steps per second: 125, episode reward: -49.267, mean reward: -0.488 [-100.000, 13.581], mean action: 1.604 [0.000, 3.000],  loss: 20.342327, mse: 7877.457336, mean_q: 78.225506, mean_eps: 0.798850
  33658/150000: episode: 362, duration: 0.694s, episode steps:  82, steps per second: 118, episode reward: -61.615, mean reward: -0.751 [-100.000, 17.253], mean action: 1.598 [0.000, 3.000],  loss: 21.527765, mse: 7939.243599, mean_q: 78.938327, mean_eps: 0.798301
  33740/150000: episode: 363, duration: 0.667s, episode steps:  82, steps per second: 123, episode reward: -98.030, mean reward: -1.195 [-100.000,  7.728], mean action: 1.537 [0.000, 3.000],  loss: 15.061955, mse: 8059.057212, mean_q: 79.310237, mean_eps: 0.797809
  33827/150000: episode: 364, duration: 0.692s, episode steps:  87, steps per second: 126, episode reward: -77.477, mean reward: -0.891 [-100.000,  7.078], mean action: 1.540 [0.000, 3.000],  loss: 23.754825, mse: 7936.902192, mean_q: 80.044589, mean_eps: 0.797302
  33957/150000: episode: 365, duration: 1.099s, episode steps: 130, steps per second: 118, episode reward: -72.819, mean reward: -0.560 [-100.000, 10.359], mean action: 1.731 [0.000, 3.000],  loss: 23.793510, mse: 7954.664600, mean_q: 79.212695, mean_eps: 0.796651
  34038/150000: episode: 366, duration: 0.684s, episode steps:  81, steps per second: 118, episode reward: -109.279, mean reward: -1.349 [-100.000, 33.063], mean action: 1.617 [0.000, 3.000],  loss: 18.111128, mse: 7915.194728, mean_q: 78.493083, mean_eps: 0.796018
  34147/150000: episode: 367, duration: 0.885s, episode steps: 109, steps per second: 123, episode reward: 14.582, mean reward:  0.134 [-100.000, 85.487], mean action: 1.642 [0.000, 3.000],  loss: 17.347332, mse: 7778.071262, mean_q: 77.570035, mean_eps: 0.795448
  34225/150000: episode: 368, duration: 0.666s, episode steps:  78, steps per second: 117, episode reward: -67.533, mean reward: -0.866 [-100.000, 13.593], mean action: 1.577 [0.000, 3.000],  loss: 19.064203, mse: 7989.948849, mean_q: 79.089918, mean_eps: 0.794887
  34344/150000: episode: 369, duration: 0.972s, episode steps: 119, steps per second: 122, episode reward: -99.345, mean reward: -0.835 [-100.000,  7.719], mean action: 1.597 [0.000, 3.000],  loss: 11.779095, mse: 7895.218085, mean_q: 78.128856, mean_eps: 0.794296
  34477/150000: episode: 370, duration: 1.112s, episode steps: 133, steps per second: 120, episode reward: -201.309, mean reward: -1.514 [-100.000, 78.609], mean action: 1.707 [0.000, 3.000],  loss: 17.392138, mse: 8001.961657, mean_q: 79.784202, mean_eps: 0.793540
  34570/150000: episode: 371, duration: 0.767s, episode steps:  93, steps per second: 121, episode reward: -102.776, mean reward: -1.105 [-100.000, 10.427], mean action: 1.774 [0.000, 3.000],  loss: 15.819596, mse: 7980.256993, mean_q: 77.840661, mean_eps: 0.792862
  34702/150000: episode: 372, duration: 1.095s, episode steps: 132, steps per second: 121, episode reward: -92.451, mean reward: -0.700 [-100.000, 13.885], mean action: 1.455 [0.000, 3.000],  loss: 16.548348, mse: 7927.282874, mean_q: 77.989974, mean_eps: 0.792187
  34782/150000: episode: 373, duration: 0.691s, episode steps:  80, steps per second: 116, episode reward: -68.363, mean reward: -0.855 [-100.000, 14.501], mean action: 1.675 [0.000, 3.000],  loss: 22.104194, mse: 8024.798315, mean_q: 79.346042, mean_eps: 0.791551
  34891/150000: episode: 374, duration: 0.973s, episode steps: 109, steps per second: 112, episode reward: -281.969, mean reward: -2.587 [-100.000, 35.291], mean action: 1.404 [0.000, 3.000],  loss: 19.216843, mse: 7977.251286, mean_q: 78.761887, mean_eps: 0.790984
  34986/150000: episode: 375, duration: 0.856s, episode steps:  95, steps per second: 111, episode reward: -55.767, mean reward: -0.587 [-100.000, 11.401], mean action: 1.600 [0.000, 3.000],  loss: 16.723881, mse: 8165.406486, mean_q: 79.049892, mean_eps: 0.790372
  35092/150000: episode: 376, duration: 0.851s, episode steps: 106, steps per second: 125, episode reward: -92.358, mean reward: -0.871 [-100.000,  7.177], mean action: 1.604 [0.000, 3.000],  loss: 21.293705, mse: 8011.824509, mean_q: 77.759964, mean_eps: 0.789769
  35218/150000: episode: 377, duration: 1.042s, episode steps: 126, steps per second: 121, episode reward: -123.897, mean reward: -0.983 [-100.000, 29.564], mean action: 1.730 [0.000, 3.000],  loss: 10.521473, mse: 8306.485979, mean_q: 80.752092, mean_eps: 0.789073
  35324/150000: episode: 378, duration: 0.858s, episode steps: 106, steps per second: 123, episode reward: -83.703, mean reward: -0.790 [-100.000, 12.550], mean action: 1.594 [0.000, 3.000],  loss: 16.328589, mse: 8175.231012, mean_q: 78.572025, mean_eps: 0.788377
  35434/150000: episode: 379, duration: 0.888s, episode steps: 110, steps per second: 124, episode reward: 19.564, mean reward:  0.178 [-100.000, 39.829], mean action: 1.773 [0.000, 3.000],  loss: 22.673847, mse: 8268.010001, mean_q: 78.548872, mean_eps: 0.787729
  35515/150000: episode: 380, duration: 0.708s, episode steps:  81, steps per second: 114, episode reward: -62.623, mean reward: -0.773 [-100.000,  9.466], mean action: 1.519 [0.000, 3.000],  loss: 20.614555, mse: 8301.041215, mean_q: 80.189578, mean_eps: 0.787156
  35664/150000: episode: 381, duration: 1.175s, episode steps: 149, steps per second: 127, episode reward: -28.456, mean reward: -0.191 [-100.000, 94.768], mean action: 1.597 [0.000, 3.000],  loss: 15.518959, mse: 8079.777331, mean_q: 77.793641, mean_eps: 0.786466
  35772/150000: episode: 382, duration: 0.899s, episode steps: 108, steps per second: 120, episode reward: -102.113, mean reward: -0.945 [-100.000,  6.172], mean action: 1.537 [0.000, 3.000],  loss: 18.006441, mse: 8382.270707, mean_q: 81.329687, mean_eps: 0.785695
  35872/150000: episode: 383, duration: 0.800s, episode steps: 100, steps per second: 125, episode reward: -80.863, mean reward: -0.809 [-100.000,  7.922], mean action: 1.510 [0.000, 3.000],  loss: 16.221197, mse: 8303.389883, mean_q: 79.654684, mean_eps: 0.785071
  36002/150000: episode: 384, duration: 1.046s, episode steps: 130, steps per second: 124, episode reward: -151.264, mean reward: -1.164 [-100.000,  5.697], mean action: 1.615 [0.000, 3.000],  loss: 14.485266, mse: 8363.357966, mean_q: 80.060639, mean_eps: 0.784381
  36084/150000: episode: 385, duration: 0.690s, episode steps:  82, steps per second: 119, episode reward: -109.838, mean reward: -1.339 [-100.000,  7.601], mean action: 1.488 [0.000, 3.000],  loss: 25.420939, mse: 8627.532328, mean_q: 81.898710, mean_eps: 0.783745
  36167/150000: episode: 386, duration: 0.666s, episode steps:  83, steps per second: 125, episode reward: -134.615, mean reward: -1.622 [-100.000,  8.781], mean action: 1.687 [0.000, 3.000],  loss: 20.247406, mse: 8511.984675, mean_q: 80.599516, mean_eps: 0.783250
  36239/150000: episode: 387, duration: 0.577s, episode steps:  72, steps per second: 125, episode reward: -84.481, mean reward: -1.173 [-100.000, 10.919], mean action: 1.542 [0.000, 3.000],  loss: 16.368968, mse: 8716.447367, mean_q: 82.807351, mean_eps: 0.782785
  36303/150000: episode: 388, duration: 0.546s, episode steps:  64, steps per second: 117, episode reward: -51.216, mean reward: -0.800 [-100.000, 19.968], mean action: 1.500 [0.000, 3.000],  loss: 17.938168, mse: 8437.560822, mean_q: 79.995161, mean_eps: 0.782377
  36407/150000: episode: 389, duration: 0.827s, episode steps: 104, steps per second: 126, episode reward: -129.006, mean reward: -1.240 [-100.000,  7.582], mean action: 1.606 [0.000, 3.000],  loss: 17.299734, mse: 8658.697345, mean_q: 81.879556, mean_eps: 0.781873
  36506/150000: episode: 390, duration: 0.796s, episode steps:  99, steps per second: 124, episode reward: -36.103, mean reward: -0.365 [-100.000, 14.425], mean action: 1.737 [0.000, 3.000],  loss: 12.677161, mse: 8639.888756, mean_q: 82.967335, mean_eps: 0.781264
  36577/150000: episode: 391, duration: 0.625s, episode steps:  71, steps per second: 114, episode reward: -77.665, mean reward: -1.094 [-100.000, 14.598], mean action: 1.676 [0.000, 3.000],  loss: 26.633267, mse: 8607.250241, mean_q: 81.176880, mean_eps: 0.780754
  36685/150000: episode: 392, duration: 0.970s, episode steps: 108, steps per second: 111, episode reward: -143.841, mean reward: -1.332 [-100.000, 16.620], mean action: 1.583 [0.000, 3.000],  loss: 18.799211, mse: 8488.533637, mean_q: 79.976868, mean_eps: 0.780217
  36768/150000: episode: 393, duration: 0.741s, episode steps:  83, steps per second: 112, episode reward: -62.800, mean reward: -0.757 [-100.000,  9.978], mean action: 1.651 [0.000, 3.000],  loss: 26.078484, mse: 8581.516672, mean_q: 82.115061, mean_eps: 0.779644
  36876/150000: episode: 394, duration: 0.952s, episode steps: 108, steps per second: 113, episode reward: -213.880, mean reward: -1.980 [-100.000,  4.099], mean action: 1.509 [0.000, 3.000],  loss: 24.962041, mse: 8617.403687, mean_q: 81.278383, mean_eps: 0.779071
  36949/150000: episode: 395, duration: 0.637s, episode steps:  73, steps per second: 115, episode reward: -42.008, mean reward: -0.575 [-100.000, 17.517], mean action: 1.493 [0.000, 3.000],  loss: 10.157257, mse: 8772.452496, mean_q: 81.592151, mean_eps: 0.778528
  37043/150000: episode: 396, duration: 0.875s, episode steps:  94, steps per second: 107, episode reward: -59.101, mean reward: -0.629 [-100.000,  8.527], mean action: 1.638 [0.000, 3.000],  loss: 13.214189, mse: 8712.122740, mean_q: 81.528917, mean_eps: 0.778027
  37136/150000: episode: 397, duration: 0.769s, episode steps:  93, steps per second: 121, episode reward: -98.237, mean reward: -1.056 [-100.000,  8.323], mean action: 1.452 [0.000, 3.000],  loss: 14.139776, mse: 8940.026960, mean_q: 82.513851, mean_eps: 0.777466
  37213/150000: episode: 398, duration: 0.631s, episode steps:  77, steps per second: 122, episode reward: -97.288, mean reward: -1.263 [-100.000, 11.701], mean action: 1.455 [0.000, 3.000],  loss: 29.483956, mse: 8882.209320, mean_q: 82.749017, mean_eps: 0.776956
  37302/150000: episode: 399, duration: 0.751s, episode steps:  89, steps per second: 118, episode reward: -57.538, mean reward: -0.646 [-100.000, 13.263], mean action: 1.573 [0.000, 3.000],  loss: 18.563651, mse: 8957.393407, mean_q: 83.178125, mean_eps: 0.776458
  37407/150000: episode: 400, duration: 0.897s, episode steps: 105, steps per second: 117, episode reward: -59.461, mean reward: -0.566 [-100.000, 16.436], mean action: 1.581 [0.000, 3.000],  loss: 14.301922, mse: 8834.507003, mean_q: 81.704658, mean_eps: 0.775876
  37501/150000: episode: 401, duration: 0.764s, episode steps:  94, steps per second: 123, episode reward: -96.191, mean reward: -1.023 [-100.000,  9.452], mean action: 1.553 [0.000, 3.000],  loss: 18.573454, mse: 8941.824827, mean_q: 83.396191, mean_eps: 0.775279
  37583/150000: episode: 402, duration: 0.718s, episode steps:  82, steps per second: 114, episode reward: -77.793, mean reward: -0.949 [-100.000, 10.602], mean action: 1.671 [0.000, 3.000],  loss: 25.167963, mse: 8965.345471, mean_q: 82.613454, mean_eps: 0.774751
  37667/150000: episode: 403, duration: 0.690s, episode steps:  84, steps per second: 122, episode reward: -31.928, mean reward: -0.380 [-100.000, 17.330], mean action: 1.607 [0.000, 3.000],  loss: 13.086254, mse: 8987.197748, mean_q: 82.018529, mean_eps: 0.774253
  37733/150000: episode: 404, duration: 0.609s, episode steps:  66, steps per second: 108, episode reward: -86.591, mean reward: -1.312 [-100.000,  6.947], mean action: 1.652 [0.000, 3.000],  loss: 17.124236, mse: 8816.849624, mean_q: 82.187120, mean_eps: 0.773803
  37846/150000: episode: 405, duration: 1.008s, episode steps: 113, steps per second: 112, episode reward: -103.493, mean reward: -0.916 [-100.000, 11.656], mean action: 1.611 [0.000, 3.000],  loss: 16.932190, mse: 9013.719852, mean_q: 82.812591, mean_eps: 0.773266
  37970/150000: episode: 406, duration: 1.028s, episode steps: 124, steps per second: 121, episode reward: -303.639, mean reward: -2.449 [-100.000, 102.623], mean action: 1.387 [0.000, 3.000],  loss: 23.477691, mse: 8915.046773, mean_q: 82.599486, mean_eps: 0.772555
  38081/150000: episode: 407, duration: 0.953s, episode steps: 111, steps per second: 116, episode reward: -122.158, mean reward: -1.101 [-100.000,  9.298], mean action: 1.486 [0.000, 3.000],  loss: 13.496598, mse: 8928.420463, mean_q: 82.898066, mean_eps: 0.771850
  38197/150000: episode: 408, duration: 0.926s, episode steps: 116, steps per second: 125, episode reward: 11.544, mean reward:  0.100 [-100.000, 72.392], mean action: 1.810 [0.000, 3.000],  loss: 15.650491, mse: 8823.935324, mean_q: 82.703510, mean_eps: 0.771169
  38272/150000: episode: 409, duration: 0.636s, episode steps:  75, steps per second: 118, episode reward: -23.310, mean reward: -0.311 [-100.000, 14.679], mean action: 1.680 [0.000, 3.000],  loss: 15.767200, mse: 8967.805651, mean_q: 82.161023, mean_eps: 0.770596
  38404/150000: episode: 410, duration: 1.314s, episode steps: 132, steps per second: 100, episode reward: -66.509, mean reward: -0.504 [-100.000, 10.648], mean action: 1.636 [0.000, 3.000],  loss: 20.138088, mse: 8957.381248, mean_q: 82.901174, mean_eps: 0.769975
  38509/150000: episode: 411, duration: 0.931s, episode steps: 105, steps per second: 113, episode reward: -71.355, mean reward: -0.680 [-100.000,  8.565], mean action: 1.752 [0.000, 3.000],  loss: 14.122748, mse: 9169.007092, mean_q: 84.052160, mean_eps: 0.769264
  38603/150000: episode: 412, duration: 0.870s, episode steps:  94, steps per second: 108, episode reward: -3.575, mean reward: -0.038 [-100.000, 14.471], mean action: 1.670 [0.000, 3.000],  loss: 11.620126, mse: 8989.359115, mean_q: 81.622600, mean_eps: 0.768667
  38689/150000: episode: 413, duration: 0.744s, episode steps:  86, steps per second: 116, episode reward: -130.408, mean reward: -1.516 [-100.000, 15.880], mean action: 1.558 [0.000, 3.000],  loss: 13.680541, mse: 9106.614644, mean_q: 84.154622, mean_eps: 0.768127
  38798/150000: episode: 414, duration: 0.960s, episode steps: 109, steps per second: 113, episode reward: -51.506, mean reward: -0.473 [-100.000, 28.675], mean action: 1.624 [0.000, 3.000],  loss: 17.081583, mse: 9125.702950, mean_q: 85.135247, mean_eps: 0.767542
  38900/150000: episode: 415, duration: 0.828s, episode steps: 102, steps per second: 123, episode reward: -115.235, mean reward: -1.130 [-100.000, 10.733], mean action: 1.627 [0.000, 3.000],  loss: 14.306165, mse: 9143.634378, mean_q: 83.302447, mean_eps: 0.766909
  39028/150000: episode: 416, duration: 1.029s, episode steps: 128, steps per second: 124, episode reward: -25.333, mean reward: -0.198 [-100.000, 19.761], mean action: 1.508 [0.000, 3.000],  loss: 15.660585, mse: 9096.223320, mean_q: 82.240119, mean_eps: 0.766219
  39105/150000: episode: 417, duration: 0.647s, episode steps:  77, steps per second: 119, episode reward: -98.991, mean reward: -1.286 [-100.000,  7.904], mean action: 1.662 [0.000, 3.000],  loss: 22.388637, mse: 9313.760869, mean_q: 82.722826, mean_eps: 0.765604
  39178/150000: episode: 418, duration: 0.588s, episode steps:  73, steps per second: 124, episode reward: -99.256, mean reward: -1.360 [-100.000,  9.947], mean action: 1.507 [0.000, 3.000],  loss: 12.348506, mse: 9508.903514, mean_q: 87.762814, mean_eps: 0.765154
  39266/150000: episode: 419, duration: 0.697s, episode steps:  88, steps per second: 126, episode reward: -106.827, mean reward: -1.214 [-100.000,  8.242], mean action: 1.602 [0.000, 3.000],  loss: 21.818347, mse: 9301.971230, mean_q: 83.787431, mean_eps: 0.764671
  39335/150000: episode: 420, duration: 0.566s, episode steps:  69, steps per second: 122, episode reward: -81.328, mean reward: -1.179 [-100.000,  5.599], mean action: 1.580 [0.000, 3.000],  loss: 11.917951, mse: 9195.296118, mean_q: 83.563525, mean_eps: 0.764200
  39427/150000: episode: 421, duration: 0.748s, episode steps:  92, steps per second: 123, episode reward: -71.674, mean reward: -0.779 [-100.000,  6.535], mean action: 1.424 [0.000, 3.000],  loss: 16.828767, mse: 9313.526930, mean_q: 82.600820, mean_eps: 0.763717
  39533/150000: episode: 422, duration: 0.852s, episode steps: 106, steps per second: 124, episode reward: -48.681, mean reward: -0.459 [-100.000, 12.567], mean action: 1.358 [0.000, 3.000],  loss: 18.336494, mse: 9349.598117, mean_q: 83.399363, mean_eps: 0.763123
  39645/150000: episode: 423, duration: 0.946s, episode steps: 112, steps per second: 118, episode reward: -66.576, mean reward: -0.594 [-100.000, 12.652], mean action: 1.580 [0.000, 3.000],  loss: 13.934503, mse: 9398.403608, mean_q: 83.670889, mean_eps: 0.762469
  39753/150000: episode: 424, duration: 0.863s, episode steps: 108, steps per second: 125, episode reward: -83.287, mean reward: -0.771 [-100.000, 11.382], mean action: 1.583 [0.000, 3.000],  loss: 21.651011, mse: 9345.148962, mean_q: 84.034899, mean_eps: 0.761809
  39861/150000: episode: 425, duration: 0.893s, episode steps: 108, steps per second: 121, episode reward: -266.713, mean reward: -2.470 [-100.000, 51.837], mean action: 1.630 [0.000, 3.000],  loss: 22.178939, mse: 9627.108322, mean_q: 86.242547, mean_eps: 0.761161
  39944/150000: episode: 426, duration: 0.656s, episode steps:  83, steps per second: 127, episode reward: -82.387, mean reward: -0.993 [-100.000,  6.582], mean action: 1.831 [0.000, 3.000],  loss: 14.467636, mse: 9280.876394, mean_q: 84.152893, mean_eps: 0.760588
  40069/150000: episode: 427, duration: 0.992s, episode steps: 125, steps per second: 126, episode reward: -149.050, mean reward: -1.192 [-100.000,  4.833], mean action: 1.752 [0.000, 3.000],  loss: 19.917083, mse: 9497.317512, mean_q: 85.291127, mean_eps: 0.759964
  40176/150000: episode: 428, duration: 0.949s, episode steps: 107, steps per second: 113, episode reward: -93.515, mean reward: -0.874 [-100.000, 12.270], mean action: 1.794 [0.000, 3.000],  loss: 15.559139, mse: 9579.090492, mean_q: 84.762112, mean_eps: 0.759268
  40321/150000: episode: 429, duration: 1.215s, episode steps: 145, steps per second: 119, episode reward: -75.962, mean reward: -0.524 [-100.000, 11.144], mean action: 1.662 [0.000, 3.000],  loss: 18.529579, mse: 9524.318252, mean_q: 84.777302, mean_eps: 0.758512
  40439/150000: episode: 430, duration: 1.039s, episode steps: 118, steps per second: 114, episode reward: -179.533, mean reward: -1.521 [-100.000,  8.597], mean action: 1.381 [0.000, 3.000],  loss: 19.135445, mse: 9401.596084, mean_q: 82.801722, mean_eps: 0.757723
  40540/150000: episode: 431, duration: 0.834s, episode steps: 101, steps per second: 121, episode reward: -24.233, mean reward: -0.240 [-100.000,  7.440], mean action: 1.693 [0.000, 3.000],  loss: 12.350573, mse: 9583.169105, mean_q: 85.292656, mean_eps: 0.757066
  40611/150000: episode: 432, duration: 0.586s, episode steps:  71, steps per second: 121, episode reward: -38.483, mean reward: -0.542 [-100.000, 13.141], mean action: 1.507 [0.000, 3.000],  loss: 14.978958, mse: 9654.482037, mean_q: 86.176096, mean_eps: 0.756550
  40730/150000: episode: 433, duration: 1.065s, episode steps: 119, steps per second: 112, episode reward: -41.585, mean reward: -0.349 [-100.000, 10.909], mean action: 1.681 [0.000, 3.000],  loss: 21.180503, mse: 9528.197307, mean_q: 84.037704, mean_eps: 0.755980
  40828/150000: episode: 434, duration: 0.859s, episode steps:  98, steps per second: 114, episode reward: -38.465, mean reward: -0.392 [-100.000, 17.116], mean action: 1.612 [0.000, 3.000],  loss: 10.873690, mse: 9753.693419, mean_q: 86.050842, mean_eps: 0.755329
  40917/150000: episode: 435, duration: 0.767s, episode steps:  89, steps per second: 116, episode reward: -68.368, mean reward: -0.768 [-100.000, 11.602], mean action: 1.652 [0.000, 3.000],  loss: 9.343885, mse: 9884.690512, mean_q: 87.297300, mean_eps: 0.754768
  41008/150000: episode: 436, duration: 0.735s, episode steps:  91, steps per second: 124, episode reward: -66.330, mean reward: -0.729 [-100.000, 13.588], mean action: 1.484 [0.000, 3.000],  loss: 17.873628, mse: 9619.637271, mean_q: 84.845057, mean_eps: 0.754228
  41095/150000: episode: 437, duration: 0.690s, episode steps:  87, steps per second: 126, episode reward: -94.835, mean reward: -1.090 [-100.000,  6.326], mean action: 1.437 [0.000, 3.000],  loss: 17.198042, mse: 9747.087559, mean_q: 84.848006, mean_eps: 0.753694
  41191/150000: episode: 438, duration: 0.864s, episode steps:  96, steps per second: 111, episode reward: -90.441, mean reward: -0.942 [-100.000,  6.062], mean action: 1.427 [0.000, 3.000],  loss: 15.590029, mse: 9760.562673, mean_q: 85.631013, mean_eps: 0.753145
  41307/150000: episode: 439, duration: 0.935s, episode steps: 116, steps per second: 124, episode reward: -74.947, mean reward: -0.646 [-100.000,  9.657], mean action: 1.603 [0.000, 3.000],  loss: 15.471255, mse: 9809.753229, mean_q: 86.128349, mean_eps: 0.752509
  41424/150000: episode: 440, duration: 0.987s, episode steps: 117, steps per second: 118, episode reward: -35.623, mean reward: -0.304 [-100.000,  7.599], mean action: 1.521 [0.000, 3.000],  loss: 13.233059, mse: 9723.381143, mean_q: 84.821062, mean_eps: 0.751810
  41543/150000: episode: 441, duration: 0.981s, episode steps: 119, steps per second: 121, episode reward: -52.800, mean reward: -0.444 [-100.000, 12.995], mean action: 1.555 [0.000, 3.000],  loss: 14.519870, mse: 9851.710507, mean_q: 84.766464, mean_eps: 0.751102
  41679/150000: episode: 442, duration: 1.161s, episode steps: 136, steps per second: 117, episode reward: -42.118, mean reward: -0.310 [-100.000,  6.693], mean action: 1.610 [0.000, 3.000],  loss: 21.141305, mse: 9918.587815, mean_q: 86.921611, mean_eps: 0.750337
  41772/150000: episode: 443, duration: 0.757s, episode steps:  93, steps per second: 123, episode reward: -80.545, mean reward: -0.866 [-100.000, 10.757], mean action: 1.613 [0.000, 3.000],  loss: 17.840648, mse: 9877.752300, mean_q: 85.619263, mean_eps: 0.749650
  41866/150000: episode: 444, duration: 0.739s, episode steps:  94, steps per second: 127, episode reward: -80.406, mean reward: -0.855 [-100.000,  9.785], mean action: 1.585 [0.000, 3.000],  loss: 19.421664, mse: 9937.103287, mean_q: 86.964711, mean_eps: 0.749089
  41932/150000: episode: 445, duration: 0.597s, episode steps:  66, steps per second: 111, episode reward: -71.867, mean reward: -1.089 [-100.000,  9.712], mean action: 1.848 [0.000, 3.000],  loss: 25.586382, mse: 9836.558601, mean_q: 85.039513, mean_eps: 0.748609
  42034/150000: episode: 446, duration: 0.845s, episode steps: 102, steps per second: 121, episode reward: -96.896, mean reward: -0.950 [-100.000,  7.270], mean action: 1.647 [0.000, 3.000],  loss: 16.752100, mse: 9810.924891, mean_q: 85.561857, mean_eps: 0.748105
  42143/150000: episode: 447, duration: 0.891s, episode steps: 109, steps per second: 122, episode reward: -80.185, mean reward: -0.736 [-100.000, 10.854], mean action: 1.587 [0.000, 3.000],  loss: 14.231317, mse: 9750.651125, mean_q: 85.753268, mean_eps: 0.747472
  42226/150000: episode: 448, duration: 0.726s, episode steps:  83, steps per second: 114, episode reward: -177.102, mean reward: -2.134 [-100.000, 24.248], mean action: 1.542 [0.000, 3.000],  loss: 18.410228, mse: 9854.702131, mean_q: 86.346661, mean_eps: 0.746896
  42354/150000: episode: 449, duration: 1.031s, episode steps: 128, steps per second: 124, episode reward: -113.675, mean reward: -0.888 [-100.000, 10.370], mean action: 1.578 [0.000, 3.000],  loss: 24.497341, mse: 9805.874969, mean_q: 85.238199, mean_eps: 0.746263
  42467/150000: episode: 450, duration: 0.955s, episode steps: 113, steps per second: 118, episode reward: -78.029, mean reward: -0.691 [-100.000, 12.378], mean action: 1.646 [0.000, 3.000],  loss: 17.253977, mse: 9805.822961, mean_q: 86.314016, mean_eps: 0.745540
  42583/150000: episode: 451, duration: 0.962s, episode steps: 116, steps per second: 121, episode reward: -78.372, mean reward: -0.676 [-100.000, 18.077], mean action: 1.560 [0.000, 3.000],  loss: 19.734683, mse: 9906.531519, mean_q: 86.527657, mean_eps: 0.744853
  42683/150000: episode: 452, duration: 0.838s, episode steps: 100, steps per second: 119, episode reward: -109.466, mean reward: -1.095 [-100.000, 11.601], mean action: 1.550 [0.000, 3.000],  loss: 13.277089, mse: 9682.951060, mean_q: 84.940789, mean_eps: 0.744205
  42804/150000: episode: 453, duration: 0.999s, episode steps: 121, steps per second: 121, episode reward: -70.229, mean reward: -0.580 [-100.000, 58.683], mean action: 1.612 [0.000, 3.000],  loss: 19.073234, mse: 9787.607434, mean_q: 85.746329, mean_eps: 0.743542
  42918/150000: episode: 454, duration: 0.934s, episode steps: 114, steps per second: 122, episode reward: -66.635, mean reward: -0.585 [-100.000, 11.568], mean action: 1.588 [0.000, 3.000],  loss: 24.311725, mse: 9774.482854, mean_q: 85.364152, mean_eps: 0.742837
  43064/150000: episode: 455, duration: 1.208s, episode steps: 146, steps per second: 121, episode reward: -24.906, mean reward: -0.171 [-100.000,  9.570], mean action: 1.575 [0.000, 3.000],  loss: 20.301589, mse: 9807.930079, mean_q: 85.341577, mean_eps: 0.742057
  43135/150000: episode: 456, duration: 0.590s, episode steps:  71, steps per second: 120, episode reward: -67.561, mean reward: -0.952 [-100.000,  6.198], mean action: 1.352 [0.000, 3.000],  loss: 9.888758, mse: 9849.519497, mean_q: 85.781029, mean_eps: 0.741406
  43226/150000: episode: 457, duration: 0.753s, episode steps:  91, steps per second: 121, episode reward: -2.883, mean reward: -0.032 [-100.000, 12.008], mean action: 1.626 [0.000, 3.000],  loss: 12.843477, mse: 10074.706945, mean_q: 89.115066, mean_eps: 0.740920
  43292/150000: episode: 458, duration: 0.556s, episode steps:  66, steps per second: 119, episode reward: -76.384, mean reward: -1.157 [-100.000, 15.265], mean action: 1.697 [0.000, 3.000],  loss: 17.121979, mse: 10101.340354, mean_q: 87.686234, mean_eps: 0.740449
  43404/150000: episode: 459, duration: 0.919s, episode steps: 112, steps per second: 122, episode reward: -36.394, mean reward: -0.325 [-100.000, 11.645], mean action: 1.616 [0.000, 3.000],  loss: 26.480953, mse: 10092.034555, mean_q: 88.478429, mean_eps: 0.739915
  43502/150000: episode: 460, duration: 0.823s, episode steps:  98, steps per second: 119, episode reward: -57.250, mean reward: -0.584 [-100.000, 21.071], mean action: 1.582 [0.000, 3.000],  loss: 12.192280, mse: 9976.553432, mean_q: 86.591124, mean_eps: 0.739285
  43629/150000: episode: 461, duration: 1.021s, episode steps: 127, steps per second: 124, episode reward: -56.114, mean reward: -0.442 [-100.000, 11.532], mean action: 1.535 [0.000, 3.000],  loss: 19.004952, mse: 10059.625765, mean_q: 88.005608, mean_eps: 0.738610
  43700/150000: episode: 462, duration: 0.599s, episode steps:  71, steps per second: 119, episode reward: -77.925, mean reward: -1.098 [-100.000, 36.411], mean action: 1.394 [0.000, 3.000],  loss: 15.430548, mse: 10081.925052, mean_q: 87.235776, mean_eps: 0.738016
  43843/150000: episode: 463, duration: 1.203s, episode steps: 143, steps per second: 119, episode reward: -72.661, mean reward: -0.508 [-100.000, 13.677], mean action: 1.531 [0.000, 3.000],  loss: 18.254834, mse: 10042.543030, mean_q: 86.921848, mean_eps: 0.737374
  43956/150000: episode: 464, duration: 0.914s, episode steps: 113, steps per second: 124, episode reward: -72.955, mean reward: -0.646 [-100.000, 24.011], mean action: 1.593 [0.000, 3.000],  loss: 12.332650, mse: 10095.073808, mean_q: 88.448561, mean_eps: 0.736606
  44066/150000: episode: 465, duration: 0.932s, episode steps: 110, steps per second: 118, episode reward: -59.923, mean reward: -0.545 [-100.000, 20.328], mean action: 1.500 [0.000, 3.000],  loss: 21.182971, mse: 10198.418879, mean_q: 88.545945, mean_eps: 0.735937
  44163/150000: episode: 466, duration: 0.794s, episode steps:  97, steps per second: 122, episode reward: -60.533, mean reward: -0.624 [-100.000, 14.230], mean action: 1.464 [0.000, 3.000],  loss: 15.068210, mse: 10213.174513, mean_q: 87.174398, mean_eps: 0.735316
  44275/150000: episode: 467, duration: 0.931s, episode steps: 112, steps per second: 120, episode reward: -76.709, mean reward: -0.685 [-100.000, 14.715], mean action: 1.473 [0.000, 3.000],  loss: 16.755990, mse: 10362.223275, mean_q: 89.261564, mean_eps: 0.734689
  44420/150000: episode: 468, duration: 1.236s, episode steps: 145, steps per second: 117, episode reward: -165.775, mean reward: -1.143 [-100.000,  3.828], mean action: 1.669 [0.000, 3.000],  loss: 13.719636, mse: 10330.269686, mean_q: 88.179380, mean_eps: 0.733918
  44511/150000: episode: 469, duration: 0.846s, episode steps:  91, steps per second: 108, episode reward: -43.058, mean reward: -0.473 [-100.000, 20.295], mean action: 1.670 [0.000, 3.000],  loss: 13.961772, mse: 10100.139627, mean_q: 84.958703, mean_eps: 0.733210
  44661/150000: episode: 470, duration: 1.399s, episode steps: 150, steps per second: 107, episode reward: -83.189, mean reward: -0.555 [-100.000,  6.777], mean action: 1.567 [0.000, 3.000],  loss: 17.851772, mse: 10357.342806, mean_q: 88.177678, mean_eps: 0.732487
  44772/150000: episode: 471, duration: 1.068s, episode steps: 111, steps per second: 104, episode reward:  5.287, mean reward:  0.048 [-100.000, 23.746], mean action: 1.739 [0.000, 3.000],  loss: 21.157012, mse: 10423.004065, mean_q: 87.529574, mean_eps: 0.731704
  44895/150000: episode: 472, duration: 1.067s, episode steps: 123, steps per second: 115, episode reward: -25.222, mean reward: -0.205 [-100.000, 12.756], mean action: 1.553 [0.000, 3.000],  loss: 13.666837, mse: 10493.242434, mean_q: 89.583893, mean_eps: 0.731002
  44992/150000: episode: 473, duration: 0.816s, episode steps:  97, steps per second: 119, episode reward: -26.742, mean reward: -0.276 [-100.000, 47.496], mean action: 1.742 [0.000, 3.000],  loss: 22.467023, mse: 10452.365557, mean_q: 89.727149, mean_eps: 0.730342
  45072/150000: episode: 474, duration: 0.699s, episode steps:  80, steps per second: 114, episode reward: -144.507, mean reward: -1.806 [-100.000,  6.873], mean action: 1.275 [0.000, 3.000],  loss: 18.646612, mse: 10689.834167, mean_q: 89.536295, mean_eps: 0.729811
  45176/150000: episode: 475, duration: 0.857s, episode steps: 104, steps per second: 121, episode reward: -74.612, mean reward: -0.717 [-100.000, 16.315], mean action: 1.712 [0.000, 3.000],  loss: 19.574392, mse: 10625.758855, mean_q: 89.982236, mean_eps: 0.729259
  45257/150000: episode: 476, duration: 0.688s, episode steps:  81, steps per second: 118, episode reward: -67.447, mean reward: -0.833 [-100.000, 10.895], mean action: 1.580 [0.000, 3.000],  loss: 20.900276, mse: 10682.173623, mean_q: 89.047252, mean_eps: 0.728704
  45335/150000: episode: 477, duration: 0.643s, episode steps:  78, steps per second: 121, episode reward: -42.566, mean reward: -0.546 [-100.000, 12.260], mean action: 1.577 [0.000, 3.000],  loss: 28.402036, mse: 10600.876941, mean_q: 87.607433, mean_eps: 0.728227
  45424/150000: episode: 478, duration: 0.729s, episode steps:  89, steps per second: 122, episode reward: -59.047, mean reward: -0.663 [-100.000, 10.457], mean action: 1.640 [0.000, 3.000],  loss: 20.078103, mse: 10374.827351, mean_q: 85.596312, mean_eps: 0.727726
  45508/150000: episode: 479, duration: 0.713s, episode steps:  84, steps per second: 118, episode reward: -81.999, mean reward: -0.976 [-100.000,  8.528], mean action: 1.595 [0.000, 3.000],  loss: 16.191512, mse: 10800.721529, mean_q: 90.941529, mean_eps: 0.727207
  45580/150000: episode: 480, duration: 0.611s, episode steps:  72, steps per second: 118, episode reward: -86.454, mean reward: -1.201 [-100.000, 14.510], mean action: 1.583 [0.000, 3.000],  loss: 17.823721, mse: 10747.792236, mean_q: 89.888769, mean_eps: 0.726739
  45692/150000: episode: 481, duration: 0.916s, episode steps: 112, steps per second: 122, episode reward: -153.740, mean reward: -1.373 [-100.000,  2.037], mean action: 1.848 [0.000, 3.000],  loss: 20.961810, mse: 10645.879456, mean_q: 87.099090, mean_eps: 0.726187
  45814/150000: episode: 482, duration: 1.026s, episode steps: 122, steps per second: 119, episode reward: -10.110, mean reward: -0.083 [-100.000, 13.432], mean action: 1.484 [0.000, 3.000],  loss: 20.728971, mse: 10702.986656, mean_q: 88.989301, mean_eps: 0.725485
  45907/150000: episode: 483, duration: 0.747s, episode steps:  93, steps per second: 124, episode reward: -58.661, mean reward: -0.631 [-100.000, 12.578], mean action: 1.333 [0.000, 3.000],  loss: 20.839230, mse: 10858.467322, mean_q: 90.088553, mean_eps: 0.724840
  45978/150000: episode: 484, duration: 0.585s, episode steps:  71, steps per second: 121, episode reward: -99.136, mean reward: -1.396 [-100.000,  7.829], mean action: 1.394 [0.000, 3.000],  loss: 13.363539, mse: 10809.502421, mean_q: 89.804145, mean_eps: 0.724348
  46086/150000: episode: 485, duration: 0.969s, episode steps: 108, steps per second: 111, episode reward: -37.121, mean reward: -0.344 [-100.000, 13.130], mean action: 1.472 [0.000, 3.000],  loss: 10.575211, mse: 10985.564182, mean_q: 91.153120, mean_eps: 0.723811
  46197/150000: episode: 486, duration: 1.081s, episode steps: 111, steps per second: 103, episode reward: -34.904, mean reward: -0.314 [-100.000, 18.315], mean action: 1.559 [0.000, 3.000],  loss: 14.880765, mse: 10751.549330, mean_q: 89.161863, mean_eps: 0.723154
  46275/150000: episode: 487, duration: 0.760s, episode steps:  78, steps per second: 103, episode reward: -86.754, mean reward: -1.112 [-100.000,  8.252], mean action: 1.538 [0.000, 3.000],  loss: 12.041529, mse: 10917.198480, mean_q: 88.791155, mean_eps: 0.722587
  46390/150000: episode: 488, duration: 1.062s, episode steps: 115, steps per second: 108, episode reward: -46.806, mean reward: -0.407 [-100.000, 48.571], mean action: 1.696 [0.000, 3.000],  loss: 18.690048, mse: 10836.129395, mean_q: 89.742015, mean_eps: 0.722008
  46516/150000: episode: 489, duration: 1.110s, episode steps: 126, steps per second: 113, episode reward: -75.214, mean reward: -0.597 [-100.000,  7.420], mean action: 1.571 [0.000, 3.000],  loss: 21.816103, mse: 10940.983530, mean_q: 90.000758, mean_eps: 0.721285
  46605/150000: episode: 490, duration: 0.787s, episode steps:  89, steps per second: 113, episode reward: -95.156, mean reward: -1.069 [-100.000,  6.831], mean action: 1.494 [0.000, 3.000],  loss: 13.066762, mse: 11100.473841, mean_q: 91.509912, mean_eps: 0.720640
  46688/150000: episode: 491, duration: 0.672s, episode steps:  83, steps per second: 124, episode reward: -56.417, mean reward: -0.680 [-100.000,  7.345], mean action: 1.494 [0.000, 3.000],  loss: 17.547959, mse: 10927.005601, mean_q: 87.849510, mean_eps: 0.720124
  46793/150000: episode: 492, duration: 0.912s, episode steps: 105, steps per second: 115, episode reward: -128.032, mean reward: -1.219 [-100.000, 14.161], mean action: 1.552 [0.000, 3.000],  loss: 14.396995, mse: 11160.007464, mean_q: 91.619375, mean_eps: 0.719560
  46887/150000: episode: 493, duration: 0.778s, episode steps:  94, steps per second: 121, episode reward: -78.946, mean reward: -0.840 [-100.000,  7.462], mean action: 1.787 [0.000, 3.000],  loss: 19.419602, mse: 11012.546678, mean_q: 89.720131, mean_eps: 0.718963
  46960/150000: episode: 494, duration: 0.572s, episode steps:  73, steps per second: 128, episode reward: -77.559, mean reward: -1.062 [-100.000, 12.382], mean action: 1.479 [0.000, 3.000],  loss: 11.736821, mse: 10991.985566, mean_q: 90.782690, mean_eps: 0.718462
  47039/150000: episode: 495, duration: 0.669s, episode steps:  79, steps per second: 118, episode reward: -48.044, mean reward: -0.608 [-100.000, 16.109], mean action: 1.633 [0.000, 3.000],  loss: 18.998382, mse: 10932.525823, mean_q: 87.963372, mean_eps: 0.718006
  47172/150000: episode: 496, duration: 1.098s, episode steps: 133, steps per second: 121, episode reward: -75.206, mean reward: -0.565 [-100.000,  5.560], mean action: 1.571 [0.000, 3.000],  loss: 19.661955, mse: 10937.400618, mean_q: 89.436908, mean_eps: 0.717370
  47242/150000: episode: 497, duration: 0.553s, episode steps:  70, steps per second: 126, episode reward: -104.264, mean reward: -1.489 [-100.000, 27.861], mean action: 1.329 [0.000, 3.000],  loss: 20.301677, mse: 10865.238295, mean_q: 90.094112, mean_eps: 0.716761
  47377/150000: episode: 498, duration: 1.151s, episode steps: 135, steps per second: 117, episode reward: -65.702, mean reward: -0.487 [-100.000, 18.397], mean action: 1.704 [0.000, 3.000],  loss: 15.393847, mse: 11084.626924, mean_q: 91.289611, mean_eps: 0.716146
  47476/150000: episode: 499, duration: 0.791s, episode steps:  99, steps per second: 125, episode reward: -61.978, mean reward: -0.626 [-100.000, 13.308], mean action: 1.646 [0.000, 3.000],  loss: 22.996176, mse: 11114.902304, mean_q: 89.991970, mean_eps: 0.715444
  47552/150000: episode: 500, duration: 0.638s, episode steps:  76, steps per second: 119, episode reward: -74.879, mean reward: -0.985 [-100.000, 15.226], mean action: 1.605 [0.000, 3.000],  loss: 22.459626, mse: 11094.426771, mean_q: 90.920577, mean_eps: 0.714919
  47639/150000: episode: 501, duration: 0.736s, episode steps:  87, steps per second: 118, episode reward: -100.099, mean reward: -1.151 [-100.000, 10.964], mean action: 1.494 [0.000, 3.000],  loss: 22.022692, mse: 11145.980951, mean_q: 91.705271, mean_eps: 0.714430
  47728/150000: episode: 502, duration: 0.705s, episode steps:  89, steps per second: 126, episode reward: -106.721, mean reward: -1.199 [-100.000, 15.171], mean action: 1.438 [0.000, 3.000],  loss: 18.975207, mse: 11103.001009, mean_q: 90.628392, mean_eps: 0.713902
  47846/150000: episode: 503, duration: 1.002s, episode steps: 118, steps per second: 118, episode reward: -104.264, mean reward: -0.884 [-100.000,  9.543], mean action: 1.729 [0.000, 3.000],  loss: 18.211604, mse: 11098.748345, mean_q: 90.944896, mean_eps: 0.713281
  47924/150000: episode: 504, duration: 0.639s, episode steps:  78, steps per second: 122, episode reward: -72.294, mean reward: -0.927 [-100.000,  7.579], mean action: 1.615 [0.000, 3.000],  loss: 16.755586, mse: 10995.232015, mean_q: 90.616021, mean_eps: 0.712693
  48050/150000: episode: 505, duration: 1.054s, episode steps: 126, steps per second: 120, episode reward:  2.782, mean reward:  0.022 [-100.000, 13.114], mean action: 1.579 [0.000, 3.000],  loss: 17.619691, mse: 11218.241598, mean_q: 90.498521, mean_eps: 0.712081
  48122/150000: episode: 506, duration: 0.620s, episode steps:  72, steps per second: 116, episode reward: -67.130, mean reward: -0.932 [-100.000, 12.225], mean action: 1.597 [0.000, 3.000],  loss: 18.959301, mse: 11419.742255, mean_q: 91.810285, mean_eps: 0.711487
  48198/150000: episode: 507, duration: 0.617s, episode steps:  76, steps per second: 123, episode reward: -90.379, mean reward: -1.189 [-100.000, 12.356], mean action: 1.447 [0.000, 3.000],  loss: 15.060039, mse: 11246.517231, mean_q: 90.191632, mean_eps: 0.711043
  48336/150000: episode: 508, duration: 1.176s, episode steps: 138, steps per second: 117, episode reward: -84.982, mean reward: -0.616 [-100.000,  7.522], mean action: 1.746 [0.000, 3.000],  loss: 22.777596, mse: 11370.782580, mean_q: 90.592891, mean_eps: 0.710401
  48412/150000: episode: 509, duration: 0.629s, episode steps:  76, steps per second: 121, episode reward: -74.814, mean reward: -0.984 [-100.000,  8.660], mean action: 1.645 [0.000, 3.000],  loss: 15.098343, mse: 11473.962736, mean_q: 90.939920, mean_eps: 0.709759
  48491/150000: episode: 510, duration: 0.630s, episode steps:  79, steps per second: 125, episode reward: -77.543, mean reward: -0.982 [-100.000,  7.892], mean action: 1.532 [0.000, 3.000],  loss: 23.381836, mse: 11322.007479, mean_q: 91.204892, mean_eps: 0.709294
  48576/150000: episode: 511, duration: 0.738s, episode steps:  85, steps per second: 115, episode reward: -199.901, mean reward: -2.352 [-100.000, 70.012], mean action: 1.353 [0.000, 3.000],  loss: 18.863342, mse: 11202.654492, mean_q: 89.015872, mean_eps: 0.708802
  48666/150000: episode: 512, duration: 0.752s, episode steps:  90, steps per second: 120, episode reward: -59.713, mean reward: -0.663 [-100.000,  6.847], mean action: 1.733 [0.000, 3.000],  loss: 16.210067, mse: 11378.168641, mean_q: 91.399753, mean_eps: 0.708277
  48778/150000: episode: 513, duration: 0.900s, episode steps: 112, steps per second: 124, episode reward: -162.336, mean reward: -1.449 [-100.000, 11.399], mean action: 1.661 [0.000, 3.000],  loss: 27.554231, mse: 11397.381321, mean_q: 92.577460, mean_eps: 0.707671
  48895/150000: episode: 514, duration: 1.171s, episode steps: 117, steps per second: 100, episode reward: -182.636, mean reward: -1.561 [-100.000, 46.857], mean action: 1.590 [0.000, 3.000],  loss: 22.738109, mse: 11452.899297, mean_q: 91.864833, mean_eps: 0.706984
  49009/150000: episode: 515, duration: 1.116s, episode steps: 114, steps per second: 102, episode reward: -70.919, mean reward: -0.622 [-100.000,  9.189], mean action: 1.719 [0.000, 3.000],  loss: 16.170191, mse: 11454.786698, mean_q: 90.469769, mean_eps: 0.706291
  49083/150000: episode: 516, duration: 0.715s, episode steps:  74, steps per second: 103, episode reward: -63.281, mean reward: -0.855 [-100.000,  6.903], mean action: 1.635 [0.000, 3.000],  loss: 19.606082, mse: 11742.776948, mean_q: 94.669870, mean_eps: 0.705727
  49205/150000: episode: 517, duration: 1.031s, episode steps: 122, steps per second: 118, episode reward: -91.491, mean reward: -0.750 [-100.000, 12.478], mean action: 1.664 [0.000, 3.000],  loss: 22.860652, mse: 11726.125088, mean_q: 94.469176, mean_eps: 0.705139
  49285/150000: episode: 518, duration: 0.656s, episode steps:  80, steps per second: 122, episode reward: -107.611, mean reward: -1.345 [-100.000,  8.619], mean action: 1.375 [0.000, 3.000],  loss: 15.487749, mse: 11566.085999, mean_q: 92.279691, mean_eps: 0.704533
  49368/150000: episode: 519, duration: 0.722s, episode steps:  83, steps per second: 115, episode reward: -67.487, mean reward: -0.813 [-100.000, 12.970], mean action: 1.578 [0.000, 3.000],  loss: 14.830741, mse: 11585.533568, mean_q: 91.737745, mean_eps: 0.704044
  49504/150000: episode: 520, duration: 1.118s, episode steps: 136, steps per second: 122, episode reward: -56.993, mean reward: -0.419 [-100.000,  8.947], mean action: 1.662 [0.000, 3.000],  loss: 26.233179, mse: 11603.902638, mean_q: 91.235172, mean_eps: 0.703387
  49604/150000: episode: 521, duration: 0.981s, episode steps: 100, steps per second: 102, episode reward: -72.727, mean reward: -0.727 [-100.000, 11.782], mean action: 1.770 [0.000, 3.000],  loss: 21.360126, mse: 11690.660127, mean_q: 93.089550, mean_eps: 0.702679
  49737/150000: episode: 522, duration: 1.122s, episode steps: 133, steps per second: 119, episode reward: -24.851, mean reward: -0.187 [-100.000, 10.699], mean action: 1.632 [0.000, 3.000],  loss: 21.639194, mse: 11647.297815, mean_q: 92.581955, mean_eps: 0.701980
  49847/150000: episode: 523, duration: 0.917s, episode steps: 110, steps per second: 120, episode reward: -71.529, mean reward: -0.650 [-100.000, 11.988], mean action: 1.609 [0.000, 3.000],  loss: 25.292694, mse: 11702.878356, mean_q: 92.004664, mean_eps: 0.701251
  49916/150000: episode: 524, duration: 0.559s, episode steps:  69, steps per second: 123, episode reward: -73.212, mean reward: -1.061 [-100.000, 13.989], mean action: 1.652 [0.000, 3.000],  loss: 22.966167, mse: 11736.540450, mean_q: 93.669394, mean_eps: 0.700714
  49984/150000: episode: 525, duration: 0.549s, episode steps:  68, steps per second: 124, episode reward: -119.693, mean reward: -1.760 [-100.000,  4.929], mean action: 1.426 [0.000, 3.000],  loss: 26.440396, mse: 11772.450109, mean_q: 92.898670, mean_eps: 0.700303
  50071/150000: episode: 526, duration: 0.729s, episode steps:  87, steps per second: 119, episode reward: -96.607, mean reward: -1.110 [-100.000,  5.538], mean action: 1.747 [0.000, 3.000],  loss: 21.525746, mse: 11646.332042, mean_q: 92.805700, mean_eps: 0.699838
  50210/150000: episode: 527, duration: 1.161s, episode steps: 139, steps per second: 120, episode reward: -120.819, mean reward: -0.869 [-100.000,  5.564], mean action: 1.712 [0.000, 3.000],  loss: 28.251598, mse: 11553.650131, mean_q: 92.514456, mean_eps: 0.699160
  50284/150000: episode: 528, duration: 0.586s, episode steps:  74, steps per second: 126, episode reward: -58.886, mean reward: -0.796 [-100.000, 15.113], mean action: 1.635 [0.000, 3.000],  loss: 20.278905, mse: 11362.801771, mean_q: 90.667383, mean_eps: 0.698521
  50409/150000: episode: 529, duration: 1.073s, episode steps: 125, steps per second: 116, episode reward: 18.044, mean reward:  0.144 [-100.000, 63.717], mean action: 1.680 [0.000, 3.000],  loss: 20.240642, mse: 11411.093477, mean_q: 92.466345, mean_eps: 0.697924
  50522/150000: episode: 530, duration: 0.950s, episode steps: 113, steps per second: 119, episode reward: -57.563, mean reward: -0.509 [-100.000, 21.605], mean action: 1.743 [0.000, 3.000],  loss: 19.432653, mse: 11512.151117, mean_q: 92.045232, mean_eps: 0.697210
  50597/150000: episode: 531, duration: 0.659s, episode steps:  75, steps per second: 114, episode reward: -50.139, mean reward: -0.669 [-100.000,  8.415], mean action: 1.667 [0.000, 3.000],  loss: 23.016171, mse: 11375.415286, mean_q: 90.282952, mean_eps: 0.696646
  50696/150000: episode: 532, duration: 0.854s, episode steps:  99, steps per second: 116, episode reward: -111.791, mean reward: -1.129 [-100.000, 12.809], mean action: 1.525 [0.000, 3.000],  loss: 28.890898, mse: 11526.165266, mean_q: 92.267379, mean_eps: 0.696124
  50819/150000: episode: 533, duration: 0.984s, episode steps: 123, steps per second: 125, episode reward: -32.528, mean reward: -0.264 [-100.000, 23.434], mean action: 1.512 [0.000, 3.000],  loss: 28.356806, mse: 11461.419914, mean_q: 91.835214, mean_eps: 0.695458
  50930/150000: episode: 534, duration: 0.984s, episode steps: 111, steps per second: 113, episode reward: -39.619, mean reward: -0.357 [-100.000,  7.078], mean action: 1.802 [0.000, 3.000],  loss: 29.548202, mse: 11609.102803, mean_q: 94.049608, mean_eps: 0.694756
  51047/150000: episode: 535, duration: 0.948s, episode steps: 117, steps per second: 123, episode reward: -177.200, mean reward: -1.515 [-100.000,  1.389], mean action: 1.564 [0.000, 3.000],  loss: 17.293424, mse: 11228.505951, mean_q: 89.049668, mean_eps: 0.694072
  51168/150000: episode: 536, duration: 1.004s, episode steps: 121, steps per second: 120, episode reward: -166.893, mean reward: -1.379 [-100.000, 52.654], mean action: 1.736 [0.000, 3.000],  loss: 29.659168, mse: 11297.414054, mean_q: 90.489921, mean_eps: 0.693358
  51274/150000: episode: 537, duration: 0.873s, episode steps: 106, steps per second: 121, episode reward: -113.038, mean reward: -1.066 [-100.000, 10.046], mean action: 1.547 [0.000, 3.000],  loss: 24.510618, mse: 11383.799795, mean_q: 91.362402, mean_eps: 0.692677
  51349/150000: episode: 538, duration: 0.639s, episode steps:  75, steps per second: 117, episode reward: -72.561, mean reward: -0.967 [-100.000, 12.899], mean action: 1.747 [0.000, 3.000],  loss: 18.648391, mse: 11155.179232, mean_q: 89.431238, mean_eps: 0.692134
  51481/150000: episode: 539, duration: 1.128s, episode steps: 132, steps per second: 117, episode reward: -240.806, mean reward: -1.824 [-100.000, 82.334], mean action: 1.606 [0.000, 3.000],  loss: 27.085498, mse: 11408.759181, mean_q: 92.953448, mean_eps: 0.691513
  51574/150000: episode: 540, duration: 0.770s, episode steps:  93, steps per second: 121, episode reward: -60.155, mean reward: -0.647 [-100.000,  7.742], mean action: 1.495 [0.000, 3.000],  loss: 23.147084, mse: 11503.569241, mean_q: 92.181332, mean_eps: 0.690838
  51656/150000: episode: 541, duration: 0.700s, episode steps:  82, steps per second: 117, episode reward: -75.953, mean reward: -0.926 [-100.000,  5.952], mean action: 1.695 [0.000, 3.000],  loss: 25.338752, mse: 11425.809916, mean_q: 93.317144, mean_eps: 0.690313
  51792/150000: episode: 542, duration: 1.133s, episode steps: 136, steps per second: 120, episode reward: -14.154, mean reward: -0.104 [-100.000, 18.093], mean action: 1.618 [0.000, 3.000],  loss: 15.779720, mse: 11482.635627, mean_q: 92.935266, mean_eps: 0.689659
  51923/150000: episode: 543, duration: 1.182s, episode steps: 131, steps per second: 111, episode reward: -141.675, mean reward: -1.081 [-100.000, 25.394], mean action: 1.710 [0.000, 3.000],  loss: 20.374484, mse: 11385.575091, mean_q: 91.854493, mean_eps: 0.688858
  52007/150000: episode: 544, duration: 0.739s, episode steps:  84, steps per second: 114, episode reward: -72.697, mean reward: -0.865 [-100.000, 10.303], mean action: 1.488 [0.000, 3.000],  loss: 22.374157, mse: 11388.298991, mean_q: 91.552215, mean_eps: 0.688213
  52110/150000: episode: 545, duration: 0.947s, episode steps: 103, steps per second: 109, episode reward: -67.692, mean reward: -0.657 [-100.000,  8.447], mean action: 1.456 [0.000, 3.000],  loss: 21.149732, mse: 11389.830666, mean_q: 91.844336, mean_eps: 0.687652
  52192/150000: episode: 546, duration: 0.768s, episode steps:  82, steps per second: 107, episode reward: -72.354, mean reward: -0.882 [-100.000, 12.382], mean action: 1.402 [0.000, 3.000],  loss: 20.879933, mse: 11664.655976, mean_q: 94.205549, mean_eps: 0.687097
  52317/150000: episode: 547, duration: 1.141s, episode steps: 125, steps per second: 110, episode reward: -64.880, mean reward: -0.519 [-100.000, 16.577], mean action: 1.624 [0.000, 3.000],  loss: 31.819299, mse: 11304.186758, mean_q: 90.058576, mean_eps: 0.686476
  52406/150000: episode: 548, duration: 0.845s, episode steps:  89, steps per second: 105, episode reward: -127.971, mean reward: -1.438 [-100.000, 47.401], mean action: 1.596 [0.000, 3.000],  loss: 27.617868, mse: 11528.761861, mean_q: 92.568199, mean_eps: 0.685834
  52503/150000: episode: 549, duration: 0.869s, episode steps:  97, steps per second: 112, episode reward: -81.298, mean reward: -0.838 [-100.000, 11.023], mean action: 1.546 [0.000, 3.000],  loss: 14.688216, mse: 11536.999406, mean_q: 93.264346, mean_eps: 0.685276
  52577/150000: episode: 550, duration: 0.602s, episode steps:  74, steps per second: 123, episode reward: -43.202, mean reward: -0.584 [-100.000, 19.474], mean action: 1.622 [0.000, 3.000],  loss: 11.628800, mse: 11599.222326, mean_q: 92.279065, mean_eps: 0.684763
  52681/150000: episode: 551, duration: 0.895s, episode steps: 104, steps per second: 116, episode reward: -51.506, mean reward: -0.495 [-100.000, 22.183], mean action: 1.567 [0.000, 3.000],  loss: 31.450236, mse: 11485.943500, mean_q: 92.859158, mean_eps: 0.684229
  52762/150000: episode: 552, duration: 0.688s, episode steps:  81, steps per second: 118, episode reward: -69.587, mean reward: -0.859 [-100.000, 17.022], mean action: 1.321 [0.000, 3.000],  loss: 22.081515, mse: 11354.746371, mean_q: 91.046638, mean_eps: 0.683674
  52891/150000: episode: 553, duration: 1.078s, episode steps: 129, steps per second: 120, episode reward: -112.719, mean reward: -0.874 [-100.000, 14.046], mean action: 1.628 [0.000, 3.000],  loss: 25.270617, mse: 11506.422518, mean_q: 92.756844, mean_eps: 0.683044
  52991/150000: episode: 554, duration: 0.831s, episode steps: 100, steps per second: 120, episode reward: -58.779, mean reward: -0.588 [-100.000, 12.698], mean action: 1.650 [0.000, 3.000],  loss: 25.352833, mse: 11687.272334, mean_q: 94.668204, mean_eps: 0.682357
  53139/150000: episode: 555, duration: 1.235s, episode steps: 148, steps per second: 120, episode reward: -195.666, mean reward: -1.322 [-100.000, 62.356], mean action: 1.797 [0.000, 3.000],  loss: 21.963977, mse: 11329.903175, mean_q: 91.966334, mean_eps: 0.681613
  53224/150000: episode: 556, duration: 0.715s, episode steps:  85, steps per second: 119, episode reward: -75.005, mean reward: -0.882 [-100.000,  7.032], mean action: 1.565 [0.000, 3.000],  loss: 16.277965, mse: 11378.122495, mean_q: 91.286459, mean_eps: 0.680914
  53343/150000: episode: 557, duration: 0.980s, episode steps: 119, steps per second: 121, episode reward: -237.957, mean reward: -2.000 [-100.000, 68.593], mean action: 1.597 [0.000, 3.000],  loss: 18.605705, mse: 11251.657883, mean_q: 92.002471, mean_eps: 0.680302
  53452/150000: episode: 558, duration: 0.934s, episode steps: 109, steps per second: 117, episode reward: -81.677, mean reward: -0.749 [-100.000, 10.967], mean action: 1.817 [0.000, 3.000],  loss: 15.912134, mse: 11345.433486, mean_q: 92.773212, mean_eps: 0.679618
  53607/150000: episode: 559, duration: 1.248s, episode steps: 155, steps per second: 124, episode reward: -180.556, mean reward: -1.165 [-100.000, 57.452], mean action: 1.729 [0.000, 3.000],  loss: 18.024988, mse: 11469.257901, mean_q: 93.176987, mean_eps: 0.678826
  53735/150000: episode: 560, duration: 1.164s, episode steps: 128, steps per second: 110, episode reward: 15.269, mean reward:  0.119 [-100.000, 26.434], mean action: 1.664 [0.000, 3.000],  loss: 26.659417, mse: 11278.970871, mean_q: 92.147115, mean_eps: 0.677977
  53844/150000: episode: 561, duration: 1.055s, episode steps: 109, steps per second: 103, episode reward: -142.133, mean reward: -1.304 [-100.000,  7.968], mean action: 1.697 [0.000, 3.000],  loss: 14.180896, mse: 11338.808289, mean_q: 92.806252, mean_eps: 0.677266
  53973/150000: episode: 562, duration: 1.221s, episode steps: 129, steps per second: 106, episode reward: -64.336, mean reward: -0.499 [-100.000, 12.924], mean action: 1.426 [0.000, 3.000],  loss: 22.553369, mse: 11506.990287, mean_q: 94.824848, mean_eps: 0.676552
  54058/150000: episode: 563, duration: 0.805s, episode steps:  85, steps per second: 106, episode reward: -110.638, mean reward: -1.302 [-100.000, 10.299], mean action: 1.576 [0.000, 3.000],  loss: 24.610580, mse: 11119.085903, mean_q: 90.044517, mean_eps: 0.675910
  54156/150000: episode: 564, duration: 1.000s, episode steps:  98, steps per second:  98, episode reward: -76.178, mean reward: -0.777 [-100.000, 13.128], mean action: 1.694 [0.000, 3.000],  loss: 19.567361, mse: 11083.732452, mean_q: 91.279403, mean_eps: 0.675361
  54232/150000: episode: 565, duration: 0.714s, episode steps:  76, steps per second: 106, episode reward: -15.601, mean reward: -0.205 [-100.000, 12.135], mean action: 1.737 [0.000, 3.000],  loss: 23.219241, mse: 10932.599571, mean_q: 88.742279, mean_eps: 0.674839
  54330/150000: episode: 566, duration: 0.835s, episode steps:  98, steps per second: 117, episode reward: -66.059, mean reward: -0.674 [-100.000, 23.818], mean action: 1.500 [0.000, 3.000],  loss: 30.878992, mse: 11144.232342, mean_q: 91.145422, mean_eps: 0.674317
  54435/150000: episode: 567, duration: 0.885s, episode steps: 105, steps per second: 119, episode reward: -14.606, mean reward: -0.139 [-100.000, 17.519], mean action: 1.705 [0.000, 3.000],  loss: 26.002381, mse: 11252.160677, mean_q: 92.484146, mean_eps: 0.673708
  54538/150000: episode: 568, duration: 0.838s, episode steps: 103, steps per second: 123, episode reward: -21.077, mean reward: -0.205 [-100.000, 10.665], mean action: 1.524 [0.000, 3.000],  loss: 23.513345, mse: 11065.190316, mean_q: 91.088462, mean_eps: 0.673084
  54651/150000: episode: 569, duration: 0.957s, episode steps: 113, steps per second: 118, episode reward: -124.835, mean reward: -1.105 [-100.000, 15.451], mean action: 1.566 [0.000, 3.000],  loss: 32.895726, mse: 11182.860982, mean_q: 92.676875, mean_eps: 0.672436
  54762/150000: episode: 570, duration: 0.902s, episode steps: 111, steps per second: 123, episode reward: -51.557, mean reward: -0.464 [-100.000,  9.153], mean action: 1.631 [0.000, 3.000],  loss: 30.992466, mse: 11225.536696, mean_q: 92.158401, mean_eps: 0.671764
  54843/150000: episode: 571, duration: 0.711s, episode steps:  81, steps per second: 114, episode reward: -60.179, mean reward: -0.743 [-100.000,  8.131], mean action: 1.531 [0.000, 3.000],  loss: 23.961784, mse: 11187.454753, mean_q: 92.357522, mean_eps: 0.671188
  54936/150000: episode: 572, duration: 0.814s, episode steps:  93, steps per second: 114, episode reward: -69.164, mean reward: -0.744 [-100.000, 11.632], mean action: 1.710 [0.000, 3.000],  loss: 17.338176, mse: 11172.028898, mean_q: 91.798897, mean_eps: 0.670666
  55064/150000: episode: 573, duration: 1.048s, episode steps: 128, steps per second: 122, episode reward: -40.199, mean reward: -0.314 [-100.000, 18.947], mean action: 1.555 [0.000, 3.000],  loss: 21.330623, mse: 11128.867752, mean_q: 91.920856, mean_eps: 0.670003
  55155/150000: episode: 574, duration: 0.791s, episode steps:  91, steps per second: 115, episode reward: -74.661, mean reward: -0.820 [-100.000, 13.409], mean action: 1.516 [0.000, 3.000],  loss: 21.517206, mse: 11531.653589, mean_q: 94.985361, mean_eps: 0.669346
  55231/150000: episode: 575, duration: 0.604s, episode steps:  76, steps per second: 126, episode reward: -63.025, mean reward: -0.829 [-100.000,  9.576], mean action: 1.579 [0.000, 3.000],  loss: 26.111027, mse: 11492.057848, mean_q: 94.465887, mean_eps: 0.668845
  55373/150000: episode: 576, duration: 1.305s, episode steps: 142, steps per second: 109, episode reward: -164.547, mean reward: -1.159 [-100.000,  8.385], mean action: 1.627 [0.000, 3.000],  loss: 20.176468, mse: 11365.549626, mean_q: 93.824496, mean_eps: 0.668191
  55521/150000: episode: 577, duration: 1.393s, episode steps: 148, steps per second: 106, episode reward: -140.609, mean reward: -0.950 [-100.000, 20.476], mean action: 1.709 [0.000, 3.000],  loss: 17.309224, mse: 11190.213392, mean_q: 92.720328, mean_eps: 0.667321
  55601/150000: episode: 578, duration: 0.685s, episode steps:  80, steps per second: 117, episode reward: 33.625, mean reward:  0.420 [-100.000, 73.652], mean action: 1.762 [0.000, 3.000],  loss: 25.215113, mse: 11388.440845, mean_q: 94.197138, mean_eps: 0.666637
  55701/150000: episode: 579, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: -43.546, mean reward: -0.435 [-100.000,  8.033], mean action: 1.590 [0.000, 3.000],  loss: 14.377204, mse: 11269.518359, mean_q: 92.916800, mean_eps: 0.666097
  55798/150000: episode: 580, duration: 0.780s, episode steps:  97, steps per second: 124, episode reward: -97.758, mean reward: -1.008 [-100.000,  8.202], mean action: 1.351 [0.000, 3.000],  loss: 21.137008, mse: 11307.785619, mean_q: 92.515096, mean_eps: 0.665506
  55915/150000: episode: 581, duration: 0.984s, episode steps: 117, steps per second: 119, episode reward: -55.113, mean reward: -0.471 [-100.000, 11.925], mean action: 1.521 [0.000, 3.000],  loss: 24.325495, mse: 11292.880943, mean_q: 93.179011, mean_eps: 0.664864
  55987/150000: episode: 582, duration: 0.600s, episode steps:  72, steps per second: 120, episode reward: 12.266, mean reward:  0.170 [-100.000, 22.125], mean action: 1.569 [0.000, 3.000],  loss: 24.001275, mse: 11247.718682, mean_q: 92.647404, mean_eps: 0.664297
  56127/150000: episode: 583, duration: 1.174s, episode steps: 140, steps per second: 119, episode reward: -7.951, mean reward: -0.057 [-100.000, 10.661], mean action: 1.714 [0.000, 3.000],  loss: 22.115624, mse: 11428.002107, mean_q: 94.236145, mean_eps: 0.663661
  56221/150000: episode: 584, duration: 0.778s, episode steps:  94, steps per second: 121, episode reward: -80.248, mean reward: -0.854 [-100.000, 17.183], mean action: 1.809 [0.000, 3.000],  loss: 17.397510, mse: 11493.216994, mean_q: 94.751467, mean_eps: 0.662959
  56363/150000: episode: 585, duration: 1.140s, episode steps: 142, steps per second: 125, episode reward: -58.243, mean reward: -0.410 [-100.000,  8.414], mean action: 1.711 [0.000, 3.000],  loss: 20.679577, mse: 11563.808972, mean_q: 95.600311, mean_eps: 0.662251
  56435/150000: episode: 586, duration: 0.601s, episode steps:  72, steps per second: 120, episode reward: -64.196, mean reward: -0.892 [-100.000, 10.027], mean action: 1.611 [0.000, 3.000],  loss: 26.178701, mse: 11294.368178, mean_q: 93.558927, mean_eps: 0.661609
  56549/150000: episode: 587, duration: 0.915s, episode steps: 114, steps per second: 125, episode reward: -65.604, mean reward: -0.575 [-100.000, 18.678], mean action: 1.588 [0.000, 3.000],  loss: 26.260323, mse: 11379.807189, mean_q: 94.057004, mean_eps: 0.661051
  56649/150000: episode: 588, duration: 0.826s, episode steps: 100, steps per second: 121, episode reward: -44.903, mean reward: -0.449 [-100.000, 12.127], mean action: 1.610 [0.000, 3.000],  loss: 24.059536, mse: 11324.367637, mean_q: 93.811671, mean_eps: 0.660409
  56771/150000: episode: 589, duration: 1.109s, episode steps: 122, steps per second: 110, episode reward: -57.111, mean reward: -0.468 [-100.000, 11.668], mean action: 1.402 [0.000, 3.000],  loss: 22.088253, mse: 11572.316558, mean_q: 95.143793, mean_eps: 0.659743
  56880/150000: episode: 590, duration: 0.963s, episode steps: 109, steps per second: 113, episode reward: -104.663, mean reward: -0.960 [-100.000, 30.603], mean action: 1.606 [0.000, 3.000],  loss: 23.440371, mse: 11455.640777, mean_q: 94.279479, mean_eps: 0.659050
  56943/150000: episode: 591, duration: 0.555s, episode steps:  63, steps per second: 114, episode reward: -54.234, mean reward: -0.861 [-100.000,  6.783], mean action: 1.825 [0.000, 3.000],  loss: 25.372195, mse: 11333.316995, mean_q: 93.635287, mean_eps: 0.658534
  57031/150000: episode: 592, duration: 0.749s, episode steps:  88, steps per second: 117, episode reward: -89.350, mean reward: -1.015 [-100.000, 16.491], mean action: 1.466 [0.000, 3.000],  loss: 19.515860, mse: 11576.005637, mean_q: 94.555415, mean_eps: 0.658081
  57133/150000: episode: 593, duration: 0.853s, episode steps: 102, steps per second: 120, episode reward: -44.273, mean reward: -0.434 [-100.000, 12.197], mean action: 1.627 [0.000, 3.000],  loss: 19.887150, mse: 11495.970828, mean_q: 94.614333, mean_eps: 0.657511
  57247/150000: episode: 594, duration: 0.968s, episode steps: 114, steps per second: 118, episode reward: -133.552, mean reward: -1.172 [-100.000,  7.856], mean action: 1.614 [0.000, 3.000],  loss: 20.765951, mse: 11569.783563, mean_q: 95.000549, mean_eps: 0.656863
  57347/150000: episode: 595, duration: 0.856s, episode steps: 100, steps per second: 117, episode reward: -169.931, mean reward: -1.699 [-100.000, 25.244], mean action: 1.580 [0.000, 3.000],  loss: 22.264708, mse: 11507.017900, mean_q: 94.691994, mean_eps: 0.656221
  57473/150000: episode: 596, duration: 1.066s, episode steps: 126, steps per second: 118, episode reward: -9.234, mean reward: -0.073 [-100.000, 15.757], mean action: 1.611 [0.000, 3.000],  loss: 15.496430, mse: 11749.676665, mean_q: 96.005048, mean_eps: 0.655543
  57555/150000: episode: 597, duration: 0.683s, episode steps:  82, steps per second: 120, episode reward: -90.773, mean reward: -1.107 [-100.000,  7.218], mean action: 1.476 [0.000, 3.000],  loss: 18.856683, mse: 11459.503013, mean_q: 92.209798, mean_eps: 0.654919
  57679/150000: episode: 598, duration: 1.055s, episode steps: 124, steps per second: 118, episode reward: -41.612, mean reward: -0.336 [-100.000, 17.501], mean action: 1.694 [0.000, 3.000],  loss: 22.577375, mse: 11526.780982, mean_q: 94.298548, mean_eps: 0.654301
  57750/150000: episode: 599, duration: 0.579s, episode steps:  71, steps per second: 123, episode reward: -54.963, mean reward: -0.774 [-100.000,  8.547], mean action: 1.634 [0.000, 3.000],  loss: 26.802767, mse: 11612.938146, mean_q: 95.411811, mean_eps: 0.653716
  57833/150000: episode: 600, duration: 0.661s, episode steps:  83, steps per second: 126, episode reward: -117.193, mean reward: -1.412 [-100.000, 23.052], mean action: 1.494 [0.000, 3.000],  loss: 28.301330, mse: 11414.435806, mean_q: 93.867805, mean_eps: 0.653254
  57928/150000: episode: 601, duration: 0.797s, episode steps:  95, steps per second: 119, episode reward: -90.268, mean reward: -0.950 [-100.000, 16.348], mean action: 1.432 [0.000, 3.000],  loss: 14.959860, mse: 11703.613867, mean_q: 95.444624, mean_eps: 0.652720
  58017/150000: episode: 602, duration: 0.759s, episode steps:  89, steps per second: 117, episode reward: -32.375, mean reward: -0.364 [-100.000,  8.581], mean action: 1.539 [0.000, 3.000],  loss: 17.786566, mse: 11413.935481, mean_q: 92.943156, mean_eps: 0.652168
  58114/150000: episode: 603, duration: 0.806s, episode steps:  97, steps per second: 120, episode reward: -60.123, mean reward: -0.620 [-100.000, 16.387], mean action: 1.649 [0.000, 3.000],  loss: 16.008695, mse: 11434.999779, mean_q: 93.209309, mean_eps: 0.651610
  59114/150000: episode: 604, duration: 8.889s, episode steps: 1000, steps per second: 112, episode reward: 88.787, mean reward:  0.089 [-23.747, 35.361], mean action: 1.331 [0.000, 3.000],  loss: 24.601463, mse: 11557.930381, mean_q: 94.439783, mean_eps: 0.648319
  59189/150000: episode: 605, duration: 0.616s, episode steps:  75, steps per second: 122, episode reward: -5.140, mean reward: -0.069 [-100.000, 13.007], mean action: 1.667 [0.000, 3.000],  loss: 20.573157, mse: 11584.053971, mean_q: 94.912452, mean_eps: 0.645094
  59338/150000: episode: 606, duration: 1.236s, episode steps: 149, steps per second: 121, episode reward: -25.782, mean reward: -0.173 [-100.000,  8.997], mean action: 1.591 [0.000, 3.000],  loss: 20.149819, mse: 11580.018371, mean_q: 94.349656, mean_eps: 0.644422
  59409/150000: episode: 607, duration: 0.609s, episode steps:  71, steps per second: 117, episode reward: -46.712, mean reward: -0.658 [-100.000,  6.021], mean action: 1.859 [0.000, 3.000],  loss: 20.677200, mse: 11453.069666, mean_q: 92.970682, mean_eps: 0.643762
  59527/150000: episode: 608, duration: 0.973s, episode steps: 118, steps per second: 121, episode reward: -194.685, mean reward: -1.650 [-100.000, 54.283], mean action: 1.551 [0.000, 3.000],  loss: 22.743561, mse: 11589.997037, mean_q: 94.266587, mean_eps: 0.643195
  59626/150000: episode: 609, duration: 0.790s, episode steps:  99, steps per second: 125, episode reward: -56.996, mean reward: -0.576 [-100.000, 10.330], mean action: 1.778 [0.000, 3.000],  loss: 17.427708, mse: 11733.892696, mean_q: 96.229445, mean_eps: 0.642544
  59722/150000: episode: 610, duration: 0.906s, episode steps:  96, steps per second: 106, episode reward: -92.735, mean reward: -0.966 [-100.000, 15.143], mean action: 1.719 [0.000, 3.000],  loss: 29.237693, mse: 11669.990529, mean_q: 95.350704, mean_eps: 0.641959
  59865/150000: episode: 611, duration: 1.280s, episode steps: 143, steps per second: 112, episode reward: -124.613, mean reward: -0.871 [-100.000,  5.110], mean action: 1.608 [0.000, 3.000],  loss: 18.373199, mse: 11713.569254, mean_q: 95.906371, mean_eps: 0.641242
  59954/150000: episode: 612, duration: 0.830s, episode steps:  89, steps per second: 107, episode reward: -29.712, mean reward: -0.334 [-100.000, 18.238], mean action: 1.461 [0.000, 3.000],  loss: 21.206480, mse: 11751.152827, mean_q: 95.356005, mean_eps: 0.640546
  60053/150000: episode: 613, duration: 0.843s, episode steps:  99, steps per second: 117, episode reward: -52.714, mean reward: -0.532 [-100.000, 12.644], mean action: 1.687 [0.000, 3.000],  loss: 18.914447, mse: 11385.550614, mean_q: 92.113348, mean_eps: 0.639982
  60136/150000: episode: 614, duration: 0.729s, episode steps:  83, steps per second: 114, episode reward: -81.671, mean reward: -0.984 [-100.000, 10.760], mean action: 1.687 [0.000, 3.000],  loss: 18.676228, mse: 11563.510589, mean_q: 94.984826, mean_eps: 0.639436
  60221/150000: episode: 615, duration: 0.756s, episode steps:  85, steps per second: 112, episode reward: -33.710, mean reward: -0.397 [-100.000,  9.218], mean action: 1.600 [0.000, 3.000],  loss: 19.378005, mse: 11604.700460, mean_q: 93.779338, mean_eps: 0.638932
  60323/150000: episode: 616, duration: 0.832s, episode steps: 102, steps per second: 123, episode reward: -62.714, mean reward: -0.615 [-100.000,  6.808], mean action: 1.578 [0.000, 3.000],  loss: 18.230471, mse: 11766.895077, mean_q: 95.687061, mean_eps: 0.638371
  60419/150000: episode: 617, duration: 0.828s, episode steps:  96, steps per second: 116, episode reward: -46.749, mean reward: -0.487 [-100.000, 16.062], mean action: 1.635 [0.000, 3.000],  loss: 18.475962, mse: 11476.217794, mean_q: 92.246419, mean_eps: 0.637777
  60548/150000: episode: 618, duration: 1.073s, episode steps: 129, steps per second: 120, episode reward: -29.154, mean reward: -0.226 [-100.000, 13.491], mean action: 1.806 [0.000, 3.000],  loss: 25.014731, mse: 11488.831244, mean_q: 93.190381, mean_eps: 0.637102
  60620/150000: episode: 619, duration: 0.574s, episode steps:  72, steps per second: 125, episode reward: -44.671, mean reward: -0.620 [-100.000, 12.320], mean action: 1.347 [0.000, 3.000],  loss: 15.568186, mse: 11648.712660, mean_q: 94.820320, mean_eps: 0.636499
  60753/150000: episode: 620, duration: 1.109s, episode steps: 133, steps per second: 120, episode reward: 21.388, mean reward:  0.161 [-100.000, 15.861], mean action: 1.707 [0.000, 3.000],  loss: 13.963393, mse: 11553.110212, mean_q: 93.017018, mean_eps: 0.635884
  60913/150000: episode: 621, duration: 1.327s, episode steps: 160, steps per second: 121, episode reward: -52.738, mean reward: -0.330 [-100.000, 12.136], mean action: 1.725 [0.000, 3.000],  loss: 20.033531, mse: 11723.846387, mean_q: 94.970546, mean_eps: 0.635005
  60978/150000: episode: 622, duration: 0.548s, episode steps:  65, steps per second: 119, episode reward: -66.184, mean reward: -1.018 [-100.000, 10.718], mean action: 1.815 [0.000, 3.000],  loss: 24.409336, mse: 11502.999730, mean_q: 95.018393, mean_eps: 0.634330
  61119/150000: episode: 623, duration: 1.136s, episode steps: 141, steps per second: 124, episode reward: 11.622, mean reward:  0.082 [-100.000, 65.170], mean action: 1.489 [0.000, 3.000],  loss: 24.960445, mse: 11571.047962, mean_q: 94.832636, mean_eps: 0.633712
  61248/150000: episode: 624, duration: 1.080s, episode steps: 129, steps per second: 119, episode reward: -31.263, mean reward: -0.242 [-100.000, 15.603], mean action: 1.643 [0.000, 3.000],  loss: 22.453466, mse: 11709.941331, mean_q: 96.219042, mean_eps: 0.632902
  61363/150000: episode: 625, duration: 0.938s, episode steps: 115, steps per second: 123, episode reward: -90.775, mean reward: -0.789 [-100.000,  7.040], mean action: 1.522 [0.000, 3.000],  loss: 21.741349, mse: 11761.089394, mean_q: 96.580957, mean_eps: 0.632170
  61449/150000: episode: 626, duration: 0.869s, episode steps:  86, steps per second:  99, episode reward: -60.458, mean reward: -0.703 [-100.000, 10.125], mean action: 1.779 [0.000, 3.000],  loss: 24.096796, mse: 11753.336460, mean_q: 97.172421, mean_eps: 0.631567
  61527/150000: episode: 627, duration: 0.712s, episode steps:  78, steps per second: 110, episode reward: -44.475, mean reward: -0.570 [-100.000, 10.595], mean action: 1.718 [0.000, 3.000],  loss: 19.459915, mse: 11923.773926, mean_q: 97.644876, mean_eps: 0.631075
  61604/150000: episode: 628, duration: 0.784s, episode steps:  77, steps per second:  98, episode reward: -55.080, mean reward: -0.715 [-100.000, 10.985], mean action: 1.455 [0.000, 3.000],  loss: 26.228380, mse: 11798.761922, mean_q: 95.744014, mean_eps: 0.630610
  61704/150000: episode: 629, duration: 0.974s, episode steps: 100, steps per second: 103, episode reward: -133.774, mean reward: -1.338 [-100.000,  8.689], mean action: 1.720 [0.000, 3.000],  loss: 17.477074, mse: 11986.015127, mean_q: 97.770165, mean_eps: 0.630079
  61810/150000: episode: 630, duration: 0.946s, episode steps: 106, steps per second: 112, episode reward: -40.863, mean reward: -0.386 [-100.000, 19.697], mean action: 1.481 [0.000, 3.000],  loss: 14.430678, mse: 11784.271742, mean_q: 96.181719, mean_eps: 0.629461
  61936/150000: episode: 631, duration: 1.133s, episode steps: 126, steps per second: 111, episode reward: -41.940, mean reward: -0.333 [-100.000, 17.512], mean action: 1.778 [0.000, 3.000],  loss: 23.745953, mse: 11926.916318, mean_q: 97.672471, mean_eps: 0.628765
  62023/150000: episode: 632, duration: 0.756s, episode steps:  87, steps per second: 115, episode reward: -54.089, mean reward: -0.622 [-100.000, 43.443], mean action: 1.540 [0.000, 3.000],  loss: 21.243466, mse: 11536.883621, mean_q: 95.084606, mean_eps: 0.628126
  62154/150000: episode: 633, duration: 1.062s, episode steps: 131, steps per second: 123, episode reward: -44.391, mean reward: -0.339 [-100.000, 11.577], mean action: 1.702 [0.000, 3.000],  loss: 22.249030, mse: 11789.251297, mean_q: 96.029704, mean_eps: 0.627472
  62266/150000: episode: 634, duration: 0.972s, episode steps: 112, steps per second: 115, episode reward: -103.779, mean reward: -0.927 [-100.000,  7.277], mean action: 1.580 [0.000, 3.000],  loss: 23.013734, mse: 11901.180150, mean_q: 97.309074, mean_eps: 0.626743
  62394/150000: episode: 635, duration: 1.010s, episode steps: 128, steps per second: 127, episode reward: -81.371, mean reward: -0.636 [-100.000,  7.035], mean action: 1.508 [0.000, 3.000],  loss: 23.846602, mse: 11891.201416, mean_q: 96.486849, mean_eps: 0.626023
  62524/150000: episode: 636, duration: 1.128s, episode steps: 130, steps per second: 115, episode reward: -60.766, mean reward: -0.467 [-100.000,  9.743], mean action: 1.731 [0.000, 3.000],  loss: 20.143500, mse: 11867.043239, mean_q: 96.740440, mean_eps: 0.625249
  62629/150000: episode: 637, duration: 0.847s, episode steps: 105, steps per second: 124, episode reward: -174.198, mean reward: -1.659 [-100.000,  5.656], mean action: 1.571 [0.000, 3.000],  loss: 26.508933, mse: 11804.695982, mean_q: 97.006011, mean_eps: 0.624544
  62727/150000: episode: 638, duration: 0.823s, episode steps:  98, steps per second: 119, episode reward: -40.444, mean reward: -0.413 [-100.000, 13.093], mean action: 1.663 [0.000, 3.000],  loss: 18.232380, mse: 12056.006846, mean_q: 98.950016, mean_eps: 0.623935
  62816/150000: episode: 639, duration: 0.715s, episode steps:  89, steps per second: 124, episode reward: -104.347, mean reward: -1.172 [-100.000, 11.561], mean action: 1.416 [0.000, 3.000],  loss: 19.302517, mse: 11632.305434, mean_q: 93.978970, mean_eps: 0.623374
  62921/150000: episode: 640, duration: 0.853s, episode steps: 105, steps per second: 123, episode reward: -38.872, mean reward: -0.370 [-100.000, 23.804], mean action: 1.600 [0.000, 3.000],  loss: 19.275994, mse: 11769.390485, mean_q: 96.425801, mean_eps: 0.622792
  63029/150000: episode: 641, duration: 0.909s, episode steps: 108, steps per second: 119, episode reward: -173.279, mean reward: -1.604 [-100.000,  2.332], mean action: 1.648 [0.000, 3.000],  loss: 24.823042, mse: 11828.053015, mean_q: 96.094649, mean_eps: 0.622153
  63135/150000: episode: 642, duration: 0.845s, episode steps: 106, steps per second: 125, episode reward: -62.441, mean reward: -0.589 [-100.000, 11.370], mean action: 1.632 [0.000, 3.000],  loss: 22.190377, mse: 11878.454544, mean_q: 97.341822, mean_eps: 0.621511
  63294/150000: episode: 643, duration: 1.334s, episode steps: 159, steps per second: 119, episode reward: -56.982, mean reward: -0.358 [-100.000, 19.646], mean action: 1.673 [0.000, 3.000],  loss: 20.126076, mse: 11945.550437, mean_q: 97.973991, mean_eps: 0.620716
  63407/150000: episode: 644, duration: 0.889s, episode steps: 113, steps per second: 127, episode reward: -14.873, mean reward: -0.132 [-100.000, 19.367], mean action: 1.637 [0.000, 3.000],  loss: 16.864130, mse: 11774.206297, mean_q: 96.811217, mean_eps: 0.619900
  63490/150000: episode: 645, duration: 0.688s, episode steps:  83, steps per second: 121, episode reward: -27.424, mean reward: -0.330 [-100.000, 11.282], mean action: 1.651 [0.000, 3.000],  loss: 18.339404, mse: 11789.784992, mean_q: 95.805911, mean_eps: 0.619312
  63568/150000: episode: 646, duration: 0.662s, episode steps:  78, steps per second: 118, episode reward: -86.719, mean reward: -1.112 [-100.000, 10.737], mean action: 1.487 [0.000, 3.000],  loss: 29.483774, mse: 12100.588279, mean_q: 99.901495, mean_eps: 0.618829
  63697/150000: episode: 647, duration: 1.034s, episode steps: 129, steps per second: 125, episode reward: -30.133, mean reward: -0.234 [-100.000, 30.699], mean action: 1.628 [0.000, 3.000],  loss: 24.763955, mse: 11800.217342, mean_q: 96.712000, mean_eps: 0.618208
  63804/150000: episode: 648, duration: 0.922s, episode steps: 107, steps per second: 116, episode reward: -54.035, mean reward: -0.505 [-100.000,  7.250], mean action: 1.617 [0.000, 3.000],  loss: 16.614338, mse: 11972.756918, mean_q: 98.679102, mean_eps: 0.617500
  63898/150000: episode: 649, duration: 0.763s, episode steps:  94, steps per second: 123, episode reward: -94.878, mean reward: -1.009 [-100.000,  7.748], mean action: 1.691 [0.000, 3.000],  loss: 20.045473, mse: 11917.251953, mean_q: 98.039973, mean_eps: 0.616897
  63984/150000: episode: 650, duration: 0.702s, episode steps:  86, steps per second: 122, episode reward: -151.176, mean reward: -1.758 [-100.000, 33.444], mean action: 1.605 [0.000, 3.000],  loss: 20.394146, mse: 11651.422170, mean_q: 95.708895, mean_eps: 0.616357
  64120/150000: episode: 651, duration: 1.132s, episode steps: 136, steps per second: 120, episode reward: -115.833, mean reward: -0.852 [-100.000,  9.701], mean action: 1.640 [0.000, 3.000],  loss: 22.406866, mse: 12004.474329, mean_q: 99.014759, mean_eps: 0.615691
  64194/150000: episode: 652, duration: 0.603s, episode steps:  74, steps per second: 123, episode reward: -31.557, mean reward: -0.426 [-100.000, 12.850], mean action: 1.703 [0.000, 3.000],  loss: 24.239035, mse: 11736.473976, mean_q: 97.096002, mean_eps: 0.615061
  64281/150000: episode: 653, duration: 0.760s, episode steps:  87, steps per second: 114, episode reward: -65.065, mean reward: -0.748 [-100.000, 12.031], mean action: 1.552 [0.000, 3.000],  loss: 21.802584, mse: 11829.280880, mean_q: 97.581571, mean_eps: 0.614578
  64417/150000: episode: 654, duration: 1.124s, episode steps: 136, steps per second: 121, episode reward: -72.498, mean reward: -0.533 [-100.000,  6.806], mean action: 1.699 [0.000, 3.000],  loss: 21.792348, mse: 11881.083029, mean_q: 97.709867, mean_eps: 0.613909
  64560/150000: episode: 655, duration: 1.333s, episode steps: 143, steps per second: 107, episode reward:  6.821, mean reward:  0.048 [-100.000, 47.564], mean action: 1.594 [0.000, 3.000],  loss: 19.797755, mse: 11908.905683, mean_q: 97.564640, mean_eps: 0.613072
  65560/150000: episode: 656, duration: 9.074s, episode steps: 1000, steps per second: 110, episode reward: 81.062, mean reward:  0.081 [-22.829, 31.850], mean action: 1.638 [0.000, 3.000],  loss: 19.136448, mse: 11999.119104, mean_q: 98.543686, mean_eps: 0.609643
  65653/150000: episode: 657, duration: 0.740s, episode steps:  93, steps per second: 126, episode reward: -54.326, mean reward: -0.584 [-100.000,  9.738], mean action: 1.570 [0.000, 3.000],  loss: 17.497288, mse: 11827.091776, mean_q: 97.198060, mean_eps: 0.606364
  65769/150000: episode: 658, duration: 1.068s, episode steps: 116, steps per second: 109, episode reward: -58.790, mean reward: -0.507 [-100.000,  9.352], mean action: 1.836 [0.000, 3.000],  loss: 18.940654, mse: 11914.091098, mean_q: 97.615161, mean_eps: 0.605737
  65898/150000: episode: 659, duration: 1.131s, episode steps: 129, steps per second: 114, episode reward: -129.676, mean reward: -1.005 [-100.000, 48.864], mean action: 1.698 [0.000, 3.000],  loss: 14.099446, mse: 11763.697856, mean_q: 96.361779, mean_eps: 0.605002
  66010/150000: episode: 660, duration: 0.982s, episode steps: 112, steps per second: 114, episode reward: -238.290, mean reward: -2.128 [-100.000, 30.926], mean action: 1.589 [0.000, 3.000],  loss: 19.761677, mse: 12052.802769, mean_q: 98.313236, mean_eps: 0.604279
  66165/150000: episode: 661, duration: 1.264s, episode steps: 155, steps per second: 123, episode reward: -178.929, mean reward: -1.154 [-100.000, 58.165], mean action: 1.858 [0.000, 3.000],  loss: 19.660708, mse: 12190.305507, mean_q: 98.575028, mean_eps: 0.603478
  66283/150000: episode: 662, duration: 1.012s, episode steps: 118, steps per second: 117, episode reward: -81.632, mean reward: -0.692 [-100.000, 19.276], mean action: 1.568 [0.000, 3.000],  loss: 17.714567, mse: 12214.935994, mean_q: 99.924908, mean_eps: 0.602659
  66400/150000: episode: 663, duration: 0.962s, episode steps: 117, steps per second: 122, episode reward: -53.678, mean reward: -0.459 [-100.000,  6.627], mean action: 1.744 [0.000, 3.000],  loss: 20.530536, mse: 12385.158579, mean_q: 101.212221, mean_eps: 0.601954
  66538/150000: episode: 664, duration: 1.145s, episode steps: 138, steps per second: 121, episode reward: -79.877, mean reward: -0.579 [-100.000, 24.637], mean action: 1.703 [0.000, 3.000],  loss: 18.941205, mse: 12402.537803, mean_q: 100.762744, mean_eps: 0.601189
  66623/150000: episode: 665, duration: 0.687s, episode steps:  85, steps per second: 124, episode reward: -27.489, mean reward: -0.323 [-100.000,  7.350], mean action: 1.624 [0.000, 3.000],  loss: 22.816609, mse: 12202.517153, mean_q: 100.041868, mean_eps: 0.600520
  66759/150000: episode: 666, duration: 1.133s, episode steps: 136, steps per second: 120, episode reward: -46.126, mean reward: -0.339 [-100.000, 19.073], mean action: 1.610 [0.000, 3.000],  loss: 17.288305, mse: 12281.103243, mean_q: 99.371043, mean_eps: 0.599857
  66836/150000: episode: 667, duration: 0.632s, episode steps:  77, steps per second: 122, episode reward: -51.974, mean reward: -0.675 [-100.000,  9.991], mean action: 1.740 [0.000, 3.000],  loss: 23.424541, mse: 12353.202022, mean_q: 100.960595, mean_eps: 0.599218
  66935/150000: episode: 668, duration: 0.782s, episode steps:  99, steps per second: 127, episode reward: -23.237, mean reward: -0.235 [-100.000, 54.095], mean action: 1.697 [0.000, 3.000],  loss: 16.204373, mse: 12179.267026, mean_q: 98.577966, mean_eps: 0.598690
  67035/150000: episode: 669, duration: 0.867s, episode steps: 100, steps per second: 115, episode reward: 11.703, mean reward:  0.117 [-100.000, 18.865], mean action: 1.800 [0.000, 3.000],  loss: 27.319155, mse: 12444.268086, mean_q: 100.713692, mean_eps: 0.598093
  67148/150000: episode: 670, duration: 0.932s, episode steps: 113, steps per second: 121, episode reward: -56.516, mean reward: -0.500 [-100.000,  8.497], mean action: 1.743 [0.000, 3.000],  loss: 21.289672, mse: 12043.204179, mean_q: 98.448745, mean_eps: 0.597454
  67255/150000: episode: 671, duration: 0.863s, episode steps: 107, steps per second: 124, episode reward: -96.956, mean reward: -0.906 [-100.000,  5.594], mean action: 1.561 [0.000, 3.000],  loss: 25.638289, mse: 11903.120291, mean_q: 98.147899, mean_eps: 0.596794
  67375/150000: episode: 672, duration: 1.151s, episode steps: 120, steps per second: 104, episode reward: -79.925, mean reward: -0.666 [-100.000, 19.899], mean action: 1.642 [0.000, 3.000],  loss: 17.439900, mse: 12373.409440, mean_q: 100.984334, mean_eps: 0.596113
  67463/150000: episode: 673, duration: 0.779s, episode steps:  88, steps per second: 113, episode reward: -48.076, mean reward: -0.546 [-100.000, 12.053], mean action: 1.693 [0.000, 3.000],  loss: 17.856993, mse: 12171.535400, mean_q: 99.464993, mean_eps: 0.595489
  67861/150000: episode: 674, duration: 3.595s, episode steps: 398, steps per second: 111, episode reward: -279.262, mean reward: -0.702 [-100.000, 19.158], mean action: 1.601 [0.000, 3.000],  loss: 21.194785, mse: 12165.966115, mean_q: 99.614112, mean_eps: 0.594031
  67968/150000: episode: 675, duration: 0.866s, episode steps: 107, steps per second: 124, episode reward: -63.023, mean reward: -0.589 [-100.000, 13.790], mean action: 1.598 [0.000, 3.000],  loss: 17.535177, mse: 12355.546930, mean_q: 101.180256, mean_eps: 0.592516
  68053/150000: episode: 676, duration: 0.740s, episode steps:  85, steps per second: 115, episode reward: -91.535, mean reward: -1.077 [-100.000, 12.019], mean action: 1.894 [0.000, 3.000],  loss: 16.397067, mse: 12259.501367, mean_q: 99.709942, mean_eps: 0.591940
  68182/150000: episode: 677, duration: 1.074s, episode steps: 129, steps per second: 120, episode reward: -192.503, mean reward: -1.492 [-100.000, 67.953], mean action: 1.605 [0.000, 3.000],  loss: 26.605132, mse: 12288.417098, mean_q: 99.386753, mean_eps: 0.591298
  68279/150000: episode: 678, duration: 0.792s, episode steps:  97, steps per second: 122, episode reward: -128.947, mean reward: -1.329 [-100.000,  7.439], mean action: 1.536 [0.000, 3.000],  loss: 16.258778, mse: 12325.939906, mean_q: 98.819053, mean_eps: 0.590620
  68398/150000: episode: 679, duration: 1.023s, episode steps: 119, steps per second: 116, episode reward: -53.334, mean reward: -0.448 [-100.000, 10.664], mean action: 1.790 [0.000, 3.000],  loss: 19.947228, mse: 12410.058741, mean_q: 100.389020, mean_eps: 0.589972
  68971/150000: episode: 680, duration: 5.138s, episode steps: 573, steps per second: 112, episode reward: -23.830, mean reward: -0.042 [-100.000, 23.981], mean action: 1.841 [0.000, 3.000],  loss: 16.354890, mse: 12201.967588, mean_q: 99.121859, mean_eps: 0.587896
  69120/150000: episode: 681, duration: 1.462s, episode steps: 149, steps per second: 102, episode reward: -39.308, mean reward: -0.264 [-100.000, 24.983], mean action: 1.537 [0.000, 3.000],  loss: 21.267826, mse: 12038.413551, mean_q: 97.725661, mean_eps: 0.585730
  69253/150000: episode: 682, duration: 1.245s, episode steps: 133, steps per second: 107, episode reward: -115.713, mean reward: -0.870 [-100.000,  6.078], mean action: 1.579 [0.000, 3.000],  loss: 19.358534, mse: 11949.544459, mean_q: 98.182153, mean_eps: 0.584884
  69343/150000: episode: 683, duration: 0.890s, episode steps:  90, steps per second: 101, episode reward: -111.437, mean reward: -1.238 [-100.000,  5.843], mean action: 1.578 [0.000, 3.000],  loss: 17.856144, mse: 12118.246300, mean_q: 98.773431, mean_eps: 0.584215
  69472/150000: episode: 684, duration: 1.184s, episode steps: 129, steps per second: 109, episode reward: -110.297, mean reward: -0.855 [-100.000, 15.943], mean action: 1.512 [0.000, 3.000],  loss: 16.060218, mse: 12334.456532, mean_q: 100.452181, mean_eps: 0.583558
  69612/150000: episode: 685, duration: 1.174s, episode steps: 140, steps per second: 119, episode reward: -178.521, mean reward: -1.275 [-100.000, 64.190], mean action: 1.607 [0.000, 3.000],  loss: 22.527892, mse: 12163.306731, mean_q: 98.151908, mean_eps: 0.582751
  69733/150000: episode: 686, duration: 1.001s, episode steps: 121, steps per second: 121, episode reward: -254.920, mean reward: -2.107 [-100.000, 95.008], mean action: 1.587 [0.000, 3.000],  loss: 23.385526, mse: 12159.430834, mean_q: 98.764250, mean_eps: 0.581968
  69866/150000: episode: 687, duration: 1.263s, episode steps: 133, steps per second: 105, episode reward: -30.719, mean reward: -0.231 [-100.000, 65.306], mean action: 1.797 [0.000, 3.000],  loss: 21.512512, mse: 12088.446810, mean_q: 98.774395, mean_eps: 0.581206
  70018/150000: episode: 688, duration: 1.369s, episode steps: 152, steps per second: 111, episode reward: -27.506, mean reward: -0.181 [-100.000,  9.866], mean action: 1.737 [0.000, 3.000],  loss: 19.231655, mse: 11938.815359, mean_q: 96.590991, mean_eps: 0.580351
  70111/150000: episode: 689, duration: 0.744s, episode steps:  93, steps per second: 125, episode reward: -90.908, mean reward: -0.978 [-100.000, 12.423], mean action: 1.677 [0.000, 3.000],  loss: 13.675280, mse: 12154.047799, mean_q: 99.304080, mean_eps: 0.579616
  70212/150000: episode: 690, duration: 0.844s, episode steps: 101, steps per second: 120, episode reward:  2.515, mean reward:  0.025 [-100.000,  9.287], mean action: 1.644 [0.000, 3.000],  loss: 16.128627, mse: 12420.108698, mean_q: 101.013226, mean_eps: 0.579034
  70383/150000: episode: 691, duration: 1.407s, episode steps: 171, steps per second: 122, episode reward: -9.802, mean reward: -0.057 [-100.000, 11.527], mean action: 1.602 [0.000, 3.000],  loss: 18.545647, mse: 12372.352813, mean_q: 99.911636, mean_eps: 0.578218
  70482/150000: episode: 692, duration: 0.824s, episode steps:  99, steps per second: 120, episode reward: -79.128, mean reward: -0.799 [-100.000, 12.492], mean action: 1.616 [0.000, 3.000],  loss: 14.872326, mse: 12283.693261, mean_q: 99.930199, mean_eps: 0.577408
  70616/150000: episode: 693, duration: 1.116s, episode steps: 134, steps per second: 120, episode reward: -55.702, mean reward: -0.416 [-100.000,  9.298], mean action: 1.672 [0.000, 3.000],  loss: 20.826911, mse: 12442.373462, mean_q: 100.006010, mean_eps: 0.576709
  70745/150000: episode: 694, duration: 1.059s, episode steps: 129, steps per second: 122, episode reward: -75.652, mean reward: -0.586 [-100.000, 13.506], mean action: 1.744 [0.000, 3.000],  loss: 20.715933, mse: 12102.786791, mean_q: 98.396687, mean_eps: 0.575920
  70865/150000: episode: 695, duration: 0.962s, episode steps: 120, steps per second: 125, episode reward: -119.736, mean reward: -0.998 [-100.000, 10.324], mean action: 1.600 [0.000, 3.000],  loss: 19.156892, mse: 12243.759749, mean_q: 99.482916, mean_eps: 0.575173
  70975/150000: episode: 696, duration: 0.885s, episode steps: 110, steps per second: 124, episode reward: -48.085, mean reward: -0.437 [-100.000, 20.347], mean action: 1.636 [0.000, 3.000],  loss: 22.701109, mse: 12344.027397, mean_q: 99.895016, mean_eps: 0.574483
  71057/150000: episode: 697, duration: 0.701s, episode steps:  82, steps per second: 117, episode reward: -128.884, mean reward: -1.572 [-100.000,  6.233], mean action: 1.427 [0.000, 3.000],  loss: 13.080259, mse: 12266.020139, mean_q: 99.136836, mean_eps: 0.573907
  71149/150000: episode: 698, duration: 0.739s, episode steps:  92, steps per second: 125, episode reward: -50.054, mean reward: -0.544 [-100.000, 19.126], mean action: 1.478 [0.000, 3.000],  loss: 13.306859, mse: 12241.995542, mean_q: 98.391909, mean_eps: 0.573385
  71233/150000: episode: 699, duration: 0.694s, episode steps:  84, steps per second: 121, episode reward: -108.019, mean reward: -1.286 [-100.000, 14.854], mean action: 1.464 [0.000, 3.000],  loss: 19.459967, mse: 12163.642880, mean_q: 97.779062, mean_eps: 0.572857
  71337/150000: episode: 700, duration: 0.901s, episode steps: 104, steps per second: 115, episode reward: -162.074, mean reward: -1.558 [-100.000,  4.819], mean action: 1.567 [0.000, 3.000],  loss: 16.933260, mse: 12151.784565, mean_q: 98.657947, mean_eps: 0.572293
  71425/150000: episode: 701, duration: 0.690s, episode steps:  88, steps per second: 127, episode reward: -142.151, mean reward: -1.615 [-100.000, 28.095], mean action: 1.557 [0.000, 3.000],  loss: 16.743923, mse: 12232.414440, mean_q: 98.198950, mean_eps: 0.571717
  71566/150000: episode: 702, duration: 1.175s, episode steps: 141, steps per second: 120, episode reward: -20.205, mean reward: -0.143 [-100.000,  7.123], mean action: 1.624 [0.000, 3.000],  loss: 22.631135, mse: 12209.225510, mean_q: 98.559063, mean_eps: 0.571030
  71672/150000: episode: 703, duration: 0.876s, episode steps: 106, steps per second: 121, episode reward: -119.263, mean reward: -1.125 [-100.000, 18.208], mean action: 1.623 [0.000, 3.000],  loss: 15.800072, mse: 12287.018997, mean_q: 98.929842, mean_eps: 0.570289
  71792/150000: episode: 704, duration: 0.988s, episode steps: 120, steps per second: 121, episode reward: -103.697, mean reward: -0.864 [-100.000,  5.999], mean action: 1.683 [0.000, 3.000],  loss: 18.668617, mse: 12359.761296, mean_q: 99.280656, mean_eps: 0.569611
  71906/150000: episode: 705, duration: 0.922s, episode steps: 114, steps per second: 124, episode reward:  4.933, mean reward:  0.043 [-100.000, 17.967], mean action: 1.623 [0.000, 3.000],  loss: 12.886799, mse: 12354.121059, mean_q: 99.033122, mean_eps: 0.568909
  72007/150000: episode: 706, duration: 0.808s, episode steps: 101, steps per second: 125, episode reward: -22.718, mean reward: -0.225 [-100.000,  7.681], mean action: 1.574 [0.000, 3.000],  loss: 22.472392, mse: 12186.609501, mean_q: 98.338612, mean_eps: 0.568264
  72151/150000: episode: 707, duration: 1.214s, episode steps: 144, steps per second: 119, episode reward: -174.885, mean reward: -1.214 [-100.000, 36.546], mean action: 1.556 [0.000, 3.000],  loss: 18.233499, mse: 12270.504313, mean_q: 100.352401, mean_eps: 0.567529
  72264/150000: episode: 708, duration: 0.916s, episode steps: 113, steps per second: 123, episode reward: -33.700, mean reward: -0.298 [-100.000, 19.890], mean action: 1.690 [0.000, 3.000],  loss: 22.120314, mse: 12284.879166, mean_q: 98.901525, mean_eps: 0.566758
  72412/150000: episode: 709, duration: 1.256s, episode steps: 148, steps per second: 118, episode reward: -206.461, mean reward: -1.395 [-100.000, 13.617], mean action: 1.791 [0.000, 3.000],  loss: 15.996335, mse: 12322.653657, mean_q: 99.469933, mean_eps: 0.565975
  72534/150000: episode: 710, duration: 0.969s, episode steps: 122, steps per second: 126, episode reward: -34.913, mean reward: -0.286 [-100.000,  6.660], mean action: 1.639 [0.000, 3.000],  loss: 17.056343, mse: 12355.762423, mean_q: 100.332244, mean_eps: 0.565165
  72637/150000: episode: 711, duration: 0.916s, episode steps: 103, steps per second: 112, episode reward: -61.180, mean reward: -0.594 [-100.000, 18.181], mean action: 1.738 [0.000, 3.000],  loss: 20.990696, mse: 12424.422340, mean_q: 100.950370, mean_eps: 0.564490
  72736/150000: episode: 712, duration: 0.811s, episode steps:  99, steps per second: 122, episode reward: -55.801, mean reward: -0.564 [-100.000,  9.562], mean action: 1.636 [0.000, 3.000],  loss: 19.104484, mse: 12264.573913, mean_q: 99.050299, mean_eps: 0.563884
  72880/150000: episode: 713, duration: 1.215s, episode steps: 144, steps per second: 119, episode reward: -56.430, mean reward: -0.392 [-100.000, 14.113], mean action: 1.715 [0.000, 3.000],  loss: 19.615002, mse: 12252.483209, mean_q: 99.366974, mean_eps: 0.563155
  72979/150000: episode: 714, duration: 0.835s, episode steps:  99, steps per second: 119, episode reward: -124.806, mean reward: -1.261 [-100.000,  5.536], mean action: 1.889 [0.000, 3.000],  loss: 17.240772, mse: 12310.924972, mean_q: 99.787668, mean_eps: 0.562426
  73123/150000: episode: 715, duration: 1.268s, episode steps: 144, steps per second: 114, episode reward: -46.514, mean reward: -0.323 [-100.000,  7.095], mean action: 1.694 [0.000, 3.000],  loss: 22.636818, mse: 12264.707330, mean_q: 100.060359, mean_eps: 0.561697
  73232/150000: episode: 716, duration: 0.911s, episode steps: 109, steps per second: 120, episode reward: -50.743, mean reward: -0.466 [-100.000, 10.805], mean action: 1.615 [0.000, 3.000],  loss: 21.845775, mse: 12220.438575, mean_q: 99.336647, mean_eps: 0.560938
  73321/150000: episode: 717, duration: 0.727s, episode steps:  89, steps per second: 122, episode reward: -49.697, mean reward: -0.558 [-100.000, 10.898], mean action: 1.528 [0.000, 3.000],  loss: 28.755108, mse: 12436.149107, mean_q: 100.887860, mean_eps: 0.560344
  73476/150000: episode: 718, duration: 1.304s, episode steps: 155, steps per second: 119, episode reward: -31.402, mean reward: -0.203 [-100.000, 17.059], mean action: 1.716 [0.000, 3.000],  loss: 18.057004, mse: 12392.140203, mean_q: 100.787175, mean_eps: 0.559612
  73565/150000: episode: 719, duration: 0.715s, episode steps:  89, steps per second: 125, episode reward: -35.789, mean reward: -0.402 [-100.000, 22.994], mean action: 1.685 [0.000, 3.000],  loss: 21.484551, mse: 12592.606522, mean_q: 101.196726, mean_eps: 0.558880
  73644/150000: episode: 720, duration: 0.688s, episode steps:  79, steps per second: 115, episode reward: -99.004, mean reward: -1.253 [-100.000, 22.112], mean action: 1.658 [0.000, 3.000],  loss: 18.958409, mse: 12365.137015, mean_q: 99.398865, mean_eps: 0.558376
  73760/150000: episode: 721, duration: 0.933s, episode steps: 116, steps per second: 124, episode reward: -153.422, mean reward: -1.323 [-100.000,  3.026], mean action: 1.491 [0.000, 3.000],  loss: 17.496858, mse: 12356.152453, mean_q: 100.965893, mean_eps: 0.557791
  73859/150000: episode: 722, duration: 0.793s, episode steps:  99, steps per second: 125, episode reward: -30.240, mean reward: -0.305 [-100.000,  7.292], mean action: 1.667 [0.000, 3.000],  loss: 21.051851, mse: 12265.209734, mean_q: 99.515247, mean_eps: 0.557146
  73943/150000: episode: 723, duration: 0.742s, episode steps:  84, steps per second: 113, episode reward: -41.864, mean reward: -0.498 [-100.000,  8.537], mean action: 1.857 [0.000, 3.000],  loss: 18.504980, mse: 12435.707775, mean_q: 101.160014, mean_eps: 0.556597
  74041/150000: episode: 724, duration: 0.819s, episode steps:  98, steps per second: 120, episode reward: -38.606, mean reward: -0.394 [-100.000, 12.014], mean action: 1.571 [0.000, 3.000],  loss: 20.360715, mse: 12572.261998, mean_q: 101.643974, mean_eps: 0.556051
  74147/150000: episode: 725, duration: 0.893s, episode steps: 106, steps per second: 119, episode reward: -158.184, mean reward: -1.492 [-100.000,  7.179], mean action: 1.698 [0.000, 3.000],  loss: 21.031545, mse: 12322.160092, mean_q: 100.903840, mean_eps: 0.555439
  74276/150000: episode: 726, duration: 1.083s, episode steps: 129, steps per second: 119, episode reward: -6.609, mean reward: -0.051 [-100.000, 18.936], mean action: 1.620 [0.000, 3.000],  loss: 17.767827, mse: 12459.271870, mean_q: 101.425771, mean_eps: 0.554734
  74377/150000: episode: 727, duration: 0.843s, episode steps: 101, steps per second: 120, episode reward: -76.858, mean reward: -0.761 [-100.000, 33.483], mean action: 1.673 [0.000, 3.000],  loss: 21.357661, mse: 12308.336189, mean_q: 99.526032, mean_eps: 0.554044
  74499/150000: episode: 728, duration: 1.012s, episode steps: 122, steps per second: 121, episode reward:  9.540, mean reward:  0.078 [-100.000, 18.537], mean action: 1.746 [0.000, 3.000],  loss: 18.427278, mse: 12136.710169, mean_q: 99.444739, mean_eps: 0.553375
  74625/150000: episode: 729, duration: 1.021s, episode steps: 126, steps per second: 123, episode reward: -202.947, mean reward: -1.611 [-100.000, 28.622], mean action: 1.722 [0.000, 3.000],  loss: 15.716075, mse: 12421.403940, mean_q: 101.228664, mean_eps: 0.552631
  74801/150000: episode: 730, duration: 1.507s, episode steps: 176, steps per second: 117, episode reward: -114.065, mean reward: -0.648 [-100.000, 14.818], mean action: 1.739 [0.000, 3.000],  loss: 20.315005, mse: 12436.690463, mean_q: 101.718427, mean_eps: 0.551725
  74923/150000: episode: 731, duration: 1.033s, episode steps: 122, steps per second: 118, episode reward: -93.286, mean reward: -0.765 [-100.000, 10.147], mean action: 1.590 [0.000, 3.000],  loss: 22.424895, mse: 12359.655562, mean_q: 101.091839, mean_eps: 0.550831
  75130/150000: episode: 732, duration: 1.971s, episode steps: 207, steps per second: 105, episode reward: -147.981, mean reward: -0.715 [-100.000, 20.096], mean action: 1.671 [0.000, 3.000],  loss: 18.909514, mse: 12481.097269, mean_q: 101.316838, mean_eps: 0.549844
  75286/150000: episode: 733, duration: 1.401s, episode steps: 156, steps per second: 111, episode reward: 23.999, mean reward:  0.154 [-100.000, 17.010], mean action: 1.724 [0.000, 3.000],  loss: 25.132360, mse: 12604.168732, mean_q: 102.737922, mean_eps: 0.548755
  75422/150000: episode: 734, duration: 1.227s, episode steps: 136, steps per second: 111, episode reward: -30.763, mean reward: -0.226 [-100.000,  7.703], mean action: 1.765 [0.000, 3.000],  loss: 24.457821, mse: 12508.806016, mean_q: 102.476866, mean_eps: 0.547879
  75586/150000: episode: 735, duration: 1.389s, episode steps: 164, steps per second: 118, episode reward: -1.210, mean reward: -0.007 [-100.000, 19.662], mean action: 1.726 [0.000, 3.000],  loss: 22.973656, mse: 12443.901427, mean_q: 101.857165, mean_eps: 0.546979
  75683/150000: episode: 736, duration: 0.858s, episode steps:  97, steps per second: 113, episode reward: -175.240, mean reward: -1.807 [-100.000, 47.118], mean action: 1.577 [0.000, 3.000],  loss: 22.311533, mse: 12418.849136, mean_q: 101.832896, mean_eps: 0.546196
  75779/150000: episode: 737, duration: 0.777s, episode steps:  96, steps per second: 124, episode reward: -44.971, mean reward: -0.468 [-100.000, 12.555], mean action: 1.708 [0.000, 3.000],  loss: 24.188115, mse: 12506.226359, mean_q: 101.933563, mean_eps: 0.545617
  75920/150000: episode: 738, duration: 1.189s, episode steps: 141, steps per second: 119, episode reward: -13.525, mean reward: -0.096 [-100.000, 47.202], mean action: 1.574 [0.000, 3.000],  loss: 18.937671, mse: 12588.977110, mean_q: 103.354571, mean_eps: 0.544906
  76022/150000: episode: 739, duration: 0.844s, episode steps: 102, steps per second: 121, episode reward: -11.111, mean reward: -0.109 [-100.000, 16.291], mean action: 1.549 [0.000, 3.000],  loss: 28.078234, mse: 12538.560987, mean_q: 103.637695, mean_eps: 0.544177
  76194/150000: episode: 740, duration: 1.389s, episode steps: 172, steps per second: 124, episode reward: -11.152, mean reward: -0.065 [-100.000, 38.609], mean action: 1.651 [0.000, 3.000],  loss: 18.236861, mse: 12457.316327, mean_q: 102.449408, mean_eps: 0.543355
  76419/150000: episode: 741, duration: 1.882s, episode steps: 225, steps per second: 120, episode reward: -181.635, mean reward: -0.807 [-100.000, 32.146], mean action: 1.689 [0.000, 3.000],  loss: 19.356724, mse: 12414.029362, mean_q: 102.386434, mean_eps: 0.542164
  76499/150000: episode: 742, duration: 0.651s, episode steps:  80, steps per second: 123, episode reward: -34.004, mean reward: -0.425 [-100.000, 11.791], mean action: 1.512 [0.000, 3.000],  loss: 18.041914, mse: 12451.014978, mean_q: 102.557412, mean_eps: 0.541249
  76585/150000: episode: 743, duration: 0.717s, episode steps:  86, steps per second: 120, episode reward: -73.013, mean reward: -0.849 [-100.000, 14.836], mean action: 1.709 [0.000, 3.000],  loss: 17.841628, mse: 12241.838935, mean_q: 101.480189, mean_eps: 0.540751
  76729/150000: episode: 744, duration: 1.228s, episode steps: 144, steps per second: 117, episode reward: -206.467, mean reward: -1.434 [-100.000, 77.293], mean action: 1.597 [0.000, 3.000],  loss: 21.678944, mse: 12522.922099, mean_q: 102.541344, mean_eps: 0.540061
  76874/150000: episode: 745, duration: 1.341s, episode steps: 145, steps per second: 108, episode reward: -54.022, mean reward: -0.373 [-100.000,  7.520], mean action: 1.634 [0.000, 3.000],  loss: 18.251549, mse: 12135.838652, mean_q: 100.479305, mean_eps: 0.539194
  77004/150000: episode: 746, duration: 1.259s, episode steps: 130, steps per second: 103, episode reward: -178.852, mean reward: -1.376 [-100.000, 90.007], mean action: 1.608 [0.000, 3.000],  loss: 19.884318, mse: 12249.714814, mean_q: 101.543325, mean_eps: 0.538369
  77124/150000: episode: 747, duration: 1.025s, episode steps: 120, steps per second: 117, episode reward: -32.918, mean reward: -0.274 [-100.000, 14.298], mean action: 1.550 [0.000, 3.000],  loss: 27.875231, mse: 12310.654045, mean_q: 102.182942, mean_eps: 0.537619
  77247/150000: episode: 748, duration: 1.131s, episode steps: 123, steps per second: 109, episode reward: -24.013, mean reward: -0.195 [-100.000, 16.596], mean action: 1.715 [0.000, 3.000],  loss: 22.404959, mse: 12287.567740, mean_q: 101.696598, mean_eps: 0.536890
  77419/150000: episode: 749, duration: 1.449s, episode steps: 172, steps per second: 119, episode reward: -145.245, mean reward: -0.844 [-100.000, 12.449], mean action: 1.703 [0.000, 3.000],  loss: 22.016400, mse: 12273.688346, mean_q: 101.367153, mean_eps: 0.536005
  77546/150000: episode: 750, duration: 1.046s, episode steps: 127, steps per second: 121, episode reward: -63.603, mean reward: -0.501 [-100.000, 11.944], mean action: 1.661 [0.000, 3.000],  loss: 21.255249, mse: 12244.085668, mean_q: 101.117057, mean_eps: 0.535108
  77643/150000: episode: 751, duration: 0.772s, episode steps:  97, steps per second: 126, episode reward: -97.907, mean reward: -1.009 [-100.000,  8.003], mean action: 1.763 [0.000, 3.000],  loss: 25.741070, mse: 12414.428832, mean_q: 102.344291, mean_eps: 0.534436
  77732/150000: episode: 752, duration: 0.795s, episode steps:  89, steps per second: 112, episode reward: -16.980, mean reward: -0.191 [-100.000, 15.124], mean action: 1.798 [0.000, 3.000],  loss: 18.238268, mse: 12279.383987, mean_q: 101.619093, mean_eps: 0.533878
  77840/150000: episode: 753, duration: 0.874s, episode steps: 108, steps per second: 124, episode reward: -12.342, mean reward: -0.114 [-100.000, 11.267], mean action: 1.639 [0.000, 3.000],  loss: 18.707985, mse: 12389.310610, mean_q: 102.052491, mean_eps: 0.533287
  77963/150000: episode: 754, duration: 1.028s, episode steps: 123, steps per second: 120, episode reward: -4.694, mean reward: -0.038 [-100.000, 20.369], mean action: 1.650 [0.000, 3.000],  loss: 18.567923, mse: 12219.545462, mean_q: 101.323317, mean_eps: 0.532594
  78119/150000: episode: 755, duration: 1.258s, episode steps: 156, steps per second: 124, episode reward: -18.138, mean reward: -0.116 [-100.000, 11.011], mean action: 1.750 [0.000, 3.000],  loss: 16.590475, mse: 12428.220797, mean_q: 102.479187, mean_eps: 0.531757
  78259/150000: episode: 756, duration: 1.167s, episode steps: 140, steps per second: 120, episode reward: -165.713, mean reward: -1.184 [-100.000, 55.716], mean action: 1.479 [0.000, 3.000],  loss: 22.264727, mse: 12511.502009, mean_q: 103.419254, mean_eps: 0.530869
  78370/150000: episode: 757, duration: 0.901s, episode steps: 111, steps per second: 123, episode reward: -1.045, mean reward: -0.009 [-100.000, 13.553], mean action: 1.685 [0.000, 3.000],  loss: 17.684884, mse: 12461.408757, mean_q: 102.138512, mean_eps: 0.530116
  78475/150000: episode: 758, duration: 0.899s, episode steps: 105, steps per second: 117, episode reward: -37.368, mean reward: -0.356 [-100.000, 27.288], mean action: 1.619 [0.000, 3.000],  loss: 21.018103, mse: 12196.041685, mean_q: 101.277039, mean_eps: 0.529468
  79099/150000: episode: 759, duration: 5.576s, episode steps: 624, steps per second: 112, episode reward: -234.667, mean reward: -0.376 [-100.000, 20.602], mean action: 1.737 [0.000, 3.000],  loss: 20.898616, mse: 12275.202857, mean_q: 101.839970, mean_eps: 0.527281
  79207/150000: episode: 760, duration: 0.925s, episode steps: 108, steps per second: 117, episode reward: -76.518, mean reward: -0.708 [-100.000, 19.411], mean action: 1.741 [0.000, 3.000],  loss: 31.804580, mse: 12372.657697, mean_q: 101.798292, mean_eps: 0.525085
  79295/150000: episode: 761, duration: 0.731s, episode steps:  88, steps per second: 120, episode reward: -32.249, mean reward: -0.366 [-100.000, 12.751], mean action: 1.659 [0.000, 3.000],  loss: 24.707710, mse: 12310.061978, mean_q: 101.750837, mean_eps: 0.524497
  79401/150000: episode: 762, duration: 0.889s, episode steps: 106, steps per second: 119, episode reward: -60.834, mean reward: -0.574 [-100.000, 10.509], mean action: 1.726 [0.000, 3.000],  loss: 31.103557, mse: 12219.615050, mean_q: 101.698842, mean_eps: 0.523915
  79500/150000: episode: 763, duration: 0.850s, episode steps:  99, steps per second: 116, episode reward: -9.275, mean reward: -0.094 [-100.000, 14.654], mean action: 1.808 [0.000, 3.000],  loss: 25.866143, mse: 12165.518505, mean_q: 101.230309, mean_eps: 0.523300
  79570/150000: episode: 764, duration: 0.578s, episode steps:  70, steps per second: 121, episode reward: -47.111, mean reward: -0.673 [-100.000, 11.280], mean action: 1.757 [0.000, 3.000],  loss: 19.895633, mse: 12134.649400, mean_q: 101.264282, mean_eps: 0.522793
  79722/150000: episode: 765, duration: 1.232s, episode steps: 152, steps per second: 123, episode reward: -11.005, mean reward: -0.072 [-100.000, 11.506], mean action: 1.651 [0.000, 3.000],  loss: 26.661858, mse: 12143.549985, mean_q: 101.385990, mean_eps: 0.522127
  79862/150000: episode: 766, duration: 1.155s, episode steps: 140, steps per second: 121, episode reward: -163.080, mean reward: -1.165 [-100.000, 45.974], mean action: 1.593 [0.000, 3.000],  loss: 25.636858, mse: 12183.912228, mean_q: 100.544113, mean_eps: 0.521251
  79956/150000: episode: 767, duration: 0.861s, episode steps:  94, steps per second: 109, episode reward: -36.010, mean reward: -0.383 [-100.000, 13.642], mean action: 1.596 [0.000, 3.000],  loss: 21.376530, mse: 12050.704818, mean_q: 100.361059, mean_eps: 0.520549
  80099/150000: episode: 768, duration: 1.199s, episode steps: 143, steps per second: 119, episode reward: -151.414, mean reward: -1.059 [-100.000, 33.160], mean action: 1.741 [0.000, 3.000],  loss: 21.696486, mse: 12262.936544, mean_q: 102.301558, mean_eps: 0.519838
  80249/150000: episode: 769, duration: 1.253s, episode steps: 150, steps per second: 120, episode reward: -22.800, mean reward: -0.152 [-100.000,  8.087], mean action: 1.687 [0.000, 3.000],  loss: 26.938431, mse: 12196.842715, mean_q: 101.415909, mean_eps: 0.518959
  80388/150000: episode: 770, duration: 1.118s, episode steps: 139, steps per second: 124, episode reward: -48.706, mean reward: -0.350 [-100.000,  9.140], mean action: 1.734 [0.000, 3.000],  loss: 21.917246, mse: 11984.123433, mean_q: 100.080191, mean_eps: 0.518092
  80478/150000: episode: 771, duration: 0.749s, episode steps:  90, steps per second: 120, episode reward: -95.971, mean reward: -1.066 [-100.000,  7.589], mean action: 1.756 [0.000, 3.000],  loss: 37.829733, mse: 11968.575380, mean_q: 98.994159, mean_eps: 0.517405
  80763/150000: episode: 772, duration: 2.406s, episode steps: 285, steps per second: 118, episode reward: -143.830, mean reward: -0.505 [-100.000, 20.327], mean action: 1.512 [0.000, 3.000],  loss: 23.326988, mse: 12061.351902, mean_q: 100.440462, mean_eps: 0.516280
  80863/150000: episode: 773, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: -49.149, mean reward: -0.491 [-100.000, 10.914], mean action: 1.790 [0.000, 3.000],  loss: 37.760787, mse: 11942.577578, mean_q: 98.997408, mean_eps: 0.515125
  81024/150000: episode: 774, duration: 1.367s, episode steps: 161, steps per second: 118, episode reward: -25.714, mean reward: -0.160 [-100.000, 15.313], mean action: 1.776 [0.000, 3.000],  loss: 20.556207, mse: 12136.302601, mean_q: 101.275499, mean_eps: 0.514342
  81093/150000: episode: 775, duration: 0.583s, episode steps:  69, steps per second: 118, episode reward: -38.344, mean reward: -0.556 [-100.000, 12.300], mean action: 1.768 [0.000, 3.000],  loss: 21.085260, mse: 12452.108087, mean_q: 103.681437, mean_eps: 0.513652
  81215/150000: episode: 776, duration: 0.970s, episode steps: 122, steps per second: 126, episode reward: -39.488, mean reward: -0.324 [-100.000,  9.581], mean action: 1.525 [0.000, 3.000],  loss: 24.218460, mse: 12563.048188, mean_q: 103.142373, mean_eps: 0.513079
  81334/150000: episode: 777, duration: 1.008s, episode steps: 119, steps per second: 118, episode reward: -11.065, mean reward: -0.093 [-100.000, 18.473], mean action: 1.655 [0.000, 3.000],  loss: 20.377061, mse: 12404.036830, mean_q: 102.402831, mean_eps: 0.512356
  81506/150000: episode: 778, duration: 1.404s, episode steps: 172, steps per second: 122, episode reward: -94.920, mean reward: -0.552 [-100.000,  5.155], mean action: 1.785 [0.000, 3.000],  loss: 17.912260, mse: 12433.215820, mean_q: 102.908569, mean_eps: 0.511483
  81622/150000: episode: 779, duration: 0.969s, episode steps: 116, steps per second: 120, episode reward:  6.800, mean reward:  0.059 [-100.000, 17.819], mean action: 1.741 [0.000, 3.000],  loss: 22.556493, mse: 12407.748333, mean_q: 103.083066, mean_eps: 0.510619
  81788/150000: episode: 780, duration: 1.363s, episode steps: 166, steps per second: 122, episode reward: -13.220, mean reward: -0.080 [-100.000, 10.530], mean action: 1.657 [0.000, 3.000],  loss: 21.405435, mse: 12312.117288, mean_q: 102.192014, mean_eps: 0.509773
  81913/150000: episode: 781, duration: 1.070s, episode steps: 125, steps per second: 117, episode reward: -37.650, mean reward: -0.301 [-100.000, 11.081], mean action: 1.744 [0.000, 3.000],  loss: 28.525135, mse: 12167.590141, mean_q: 100.957258, mean_eps: 0.508900
  82028/150000: episode: 782, duration: 0.964s, episode steps: 115, steps per second: 119, episode reward: -62.218, mean reward: -0.541 [-100.000, 16.266], mean action: 1.757 [0.000, 3.000],  loss: 18.710121, mse: 12251.958025, mean_q: 100.722017, mean_eps: 0.508180
  82476/150000: episode: 783, duration: 3.778s, episode steps: 448, steps per second: 119, episode reward: -23.369, mean reward: -0.052 [-100.000, 47.250], mean action: 1.752 [0.000, 3.000],  loss: 21.160985, mse: 12470.808212, mean_q: 103.694090, mean_eps: 0.506491
  82615/150000: episode: 784, duration: 1.156s, episode steps: 139, steps per second: 120, episode reward: -30.105, mean reward: -0.217 [-100.000,  9.862], mean action: 1.755 [0.000, 3.000],  loss: 22.647943, mse: 12311.671636, mean_q: 101.332790, mean_eps: 0.504730
  82703/150000: episode: 785, duration: 0.689s, episode steps:  88, steps per second: 128, episode reward: -66.228, mean reward: -0.753 [-100.000,  9.128], mean action: 1.659 [0.000, 3.000],  loss: 23.304246, mse: 12069.213113, mean_q: 99.471430, mean_eps: 0.504049
  82823/150000: episode: 786, duration: 1.071s, episode steps: 120, steps per second: 112, episode reward: -74.400, mean reward: -0.620 [-100.000, 21.049], mean action: 1.692 [0.000, 3.000],  loss: 20.079189, mse: 12229.467554, mean_q: 101.689200, mean_eps: 0.503425
  82974/150000: episode: 787, duration: 1.335s, episode steps: 151, steps per second: 113, episode reward: -9.438, mean reward: -0.063 [-100.000, 19.803], mean action: 1.887 [0.000, 3.000],  loss: 25.286797, mse: 12388.906127, mean_q: 102.776404, mean_eps: 0.502612
  83077/150000: episode: 788, duration: 0.961s, episode steps: 103, steps per second: 107, episode reward: -56.204, mean reward: -0.546 [-100.000, 20.981], mean action: 1.631 [0.000, 3.000],  loss: 17.337507, mse: 12504.325063, mean_q: 103.156277, mean_eps: 0.501850
  83173/150000: episode: 789, duration: 0.842s, episode steps:  96, steps per second: 114, episode reward: -97.865, mean reward: -1.019 [-100.000, 16.544], mean action: 1.719 [0.000, 3.000],  loss: 18.566674, mse: 12431.748505, mean_q: 103.046084, mean_eps: 0.501253
  83311/150000: episode: 790, duration: 1.194s, episode steps: 138, steps per second: 116, episode reward: 23.564, mean reward:  0.171 [-100.000, 18.154], mean action: 1.565 [0.000, 3.000],  loss: 23.546208, mse: 12396.681117, mean_q: 102.787820, mean_eps: 0.500551
  83428/150000: episode: 791, duration: 0.965s, episode steps: 117, steps per second: 121, episode reward: -53.240, mean reward: -0.455 [-100.000,  8.428], mean action: 1.769 [0.000, 3.000],  loss: 22.686157, mse: 12490.791107, mean_q: 104.519077, mean_eps: 0.499786
  83558/150000: episode: 792, duration: 1.064s, episode steps: 130, steps per second: 122, episode reward: -20.147, mean reward: -0.155 [-100.000, 10.571], mean action: 1.654 [0.000, 3.000],  loss: 20.765335, mse: 12496.402794, mean_q: 102.962177, mean_eps: 0.499045
  83684/150000: episode: 793, duration: 1.079s, episode steps: 126, steps per second: 117, episode reward: -40.015, mean reward: -0.318 [-100.000, 11.173], mean action: 1.865 [0.000, 3.000],  loss: 20.012373, mse: 12557.243420, mean_q: 104.405364, mean_eps: 0.498277
  83781/150000: episode: 794, duration: 0.757s, episode steps:  97, steps per second: 128, episode reward: 17.365, mean reward:  0.179 [-100.000, 20.108], mean action: 1.732 [0.000, 3.000],  loss: 21.848215, mse: 12776.956105, mean_q: 106.050598, mean_eps: 0.497608
  83941/150000: episode: 795, duration: 1.308s, episode steps: 160, steps per second: 122, episode reward: -55.196, mean reward: -0.345 [-100.000, 20.653], mean action: 1.812 [0.000, 3.000],  loss: 21.180241, mse: 12332.797925, mean_q: 102.377704, mean_eps: 0.496837
  84072/150000: episode: 796, duration: 1.013s, episode steps: 131, steps per second: 129, episode reward: -54.835, mean reward: -0.419 [-100.000,  9.164], mean action: 1.504 [0.000, 3.000],  loss: 30.709392, mse: 12416.128616, mean_q: 102.639394, mean_eps: 0.495964
  84234/150000: episode: 797, duration: 1.310s, episode steps: 162, steps per second: 124, episode reward: 23.638, mean reward:  0.146 [-100.000, 23.222], mean action: 1.611 [0.000, 3.000],  loss: 23.095229, mse: 12499.653712, mean_q: 102.068545, mean_eps: 0.495085
  84331/150000: episode: 798, duration: 0.767s, episode steps:  97, steps per second: 126, episode reward: -60.141, mean reward: -0.620 [-100.000, 11.219], mean action: 1.732 [0.000, 3.000],  loss: 23.258966, mse: 12594.033364, mean_q: 103.635004, mean_eps: 0.494308
  84458/150000: episode: 799, duration: 1.037s, episode steps: 127, steps per second: 122, episode reward: 18.828, mean reward:  0.148 [-100.000, 12.969], mean action: 1.740 [0.000, 3.000],  loss: 22.141220, mse: 12342.710230, mean_q: 101.639646, mean_eps: 0.493636
  84573/150000: episode: 800, duration: 0.879s, episode steps: 115, steps per second: 131, episode reward: -19.900, mean reward: -0.173 [-100.000, 13.221], mean action: 1.661 [0.000, 3.000],  loss: 25.721225, mse: 12416.548200, mean_q: 102.039863, mean_eps: 0.492910
  85181/150000: episode: 801, duration: 5.728s, episode steps: 608, steps per second: 106, episode reward: -34.632, mean reward: -0.057 [-100.000, 64.375], mean action: 1.786 [0.000, 3.000],  loss: 20.809309, mse: 12546.760411, mean_q: 103.075650, mean_eps: 0.490741
  85353/150000: episode: 802, duration: 1.366s, episode steps: 172, steps per second: 126, episode reward: -20.724, mean reward: -0.120 [-100.000, 18.012], mean action: 1.616 [0.000, 3.000],  loss: 24.774463, mse: 12619.526072, mean_q: 103.726538, mean_eps: 0.488401
  85528/150000: episode: 803, duration: 1.358s, episode steps: 175, steps per second: 129, episode reward: 54.702, mean reward:  0.313 [-100.000, 17.365], mean action: 1.783 [0.000, 3.000],  loss: 21.416632, mse: 12520.099944, mean_q: 102.891146, mean_eps: 0.487360
  85640/150000: episode: 804, duration: 0.889s, episode steps: 112, steps per second: 126, episode reward: -5.278, mean reward: -0.047 [-100.000, 18.217], mean action: 1.714 [0.000, 3.000],  loss: 23.486457, mse: 12639.566616, mean_q: 103.317785, mean_eps: 0.486499
  85741/150000: episode: 805, duration: 0.850s, episode steps: 101, steps per second: 119, episode reward: -35.166, mean reward: -0.348 [-100.000,  6.402], mean action: 1.525 [0.000, 3.000],  loss: 20.456171, mse: 12828.082283, mean_q: 104.657257, mean_eps: 0.485860
  85880/150000: episode: 806, duration: 1.106s, episode steps: 139, steps per second: 126, episode reward: -86.678, mean reward: -0.624 [-100.000,  9.547], mean action: 1.784 [0.000, 3.000],  loss: 17.458500, mse: 12873.764164, mean_q: 105.326119, mean_eps: 0.485140
  86047/150000: episode: 807, duration: 1.349s, episode steps: 167, steps per second: 124, episode reward: -212.156, mean reward: -1.270 [-100.000, 39.066], mean action: 1.509 [0.000, 3.000],  loss: 18.059438, mse: 12772.969136, mean_q: 104.495052, mean_eps: 0.484222
  86210/150000: episode: 808, duration: 1.305s, episode steps: 163, steps per second: 125, episode reward: 31.219, mean reward:  0.192 [-100.000, 15.155], mean action: 1.779 [0.000, 3.000],  loss: 21.435345, mse: 12767.420755, mean_q: 104.726527, mean_eps: 0.483232
  86732/150000: episode: 809, duration: 4.390s, episode steps: 522, steps per second: 119, episode reward: -111.472, mean reward: -0.214 [-100.000, 16.160], mean action: 1.831 [0.000, 3.000],  loss: 23.105699, mse: 12808.972673, mean_q: 104.859197, mean_eps: 0.481177
  87251/150000: episode: 810, duration: 4.414s, episode steps: 519, steps per second: 118, episode reward: -359.188, mean reward: -0.692 [-100.000, 22.340], mean action: 1.707 [0.000, 3.000],  loss: 23.233705, mse: 12751.346546, mean_q: 104.301194, mean_eps: 0.478054
  87364/150000: episode: 811, duration: 0.878s, episode steps: 113, steps per second: 129, episode reward: -26.151, mean reward: -0.231 [-100.000, 20.321], mean action: 1.770 [0.000, 3.000],  loss: 24.574037, mse: 12513.444716, mean_q: 101.595361, mean_eps: 0.476158
  87456/150000: episode: 812, duration: 0.728s, episode steps:  92, steps per second: 126, episode reward: -35.279, mean reward: -0.383 [-100.000, 21.085], mean action: 1.685 [0.000, 3.000],  loss: 19.187208, mse: 12715.588390, mean_q: 102.994221, mean_eps: 0.475543
  87575/150000: episode: 813, duration: 1.005s, episode steps: 119, steps per second: 118, episode reward: 10.293, mean reward:  0.086 [-100.000, 15.022], mean action: 1.849 [0.000, 3.000],  loss: 23.908619, mse: 12721.673804, mean_q: 103.858127, mean_eps: 0.474910
  87699/150000: episode: 814, duration: 0.962s, episode steps: 124, steps per second: 129, episode reward:  6.671, mean reward:  0.054 [-100.000, 14.973], mean action: 1.669 [0.000, 3.000],  loss: 25.361404, mse: 12936.262089, mean_q: 104.662323, mean_eps: 0.474181
  87799/150000: episode: 815, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: -137.466, mean reward: -1.375 [-100.000,  8.197], mean action: 1.650 [0.000, 3.000],  loss: 22.234133, mse: 12801.058574, mean_q: 105.101255, mean_eps: 0.473509
  87898/150000: episode: 816, duration: 0.823s, episode steps:  99, steps per second: 120, episode reward: -74.147, mean reward: -0.749 [-100.000, 21.117], mean action: 1.879 [0.000, 3.000],  loss: 29.314739, mse: 12608.845486, mean_q: 102.681957, mean_eps: 0.472912
  88020/150000: episode: 817, duration: 0.990s, episode steps: 122, steps per second: 123, episode reward: -6.529, mean reward: -0.054 [-100.000, 20.086], mean action: 1.623 [0.000, 3.000],  loss: 21.332946, mse: 12786.650847, mean_q: 104.027448, mean_eps: 0.472249
  89020/150000: episode: 818, duration: 8.749s, episode steps: 1000, steps per second: 114, episode reward: 60.912, mean reward:  0.061 [-24.402, 23.298], mean action: 2.067 [0.000, 3.000],  loss: 23.289587, mse: 12733.540589, mean_q: 103.262068, mean_eps: 0.468883
  89404/150000: episode: 819, duration: 3.217s, episode steps: 384, steps per second: 119, episode reward: -156.719, mean reward: -0.408 [-100.000, 16.907], mean action: 1.557 [0.000, 3.000],  loss: 21.511358, mse: 12874.811826, mean_q: 103.679574, mean_eps: 0.464731
  89550/150000: episode: 820, duration: 1.176s, episode steps: 146, steps per second: 124, episode reward:  6.099, mean reward:  0.042 [-100.000, 26.929], mean action: 1.760 [0.000, 3.000],  loss: 30.668602, mse: 12888.501672, mean_q: 103.681698, mean_eps: 0.463141
  89677/150000: episode: 821, duration: 1.025s, episode steps: 127, steps per second: 124, episode reward: 20.536, mean reward:  0.162 [-100.000,  7.968], mean action: 1.748 [0.000, 3.000],  loss: 23.552849, mse: 12892.890840, mean_q: 104.671051, mean_eps: 0.462322
  89881/150000: episode: 822, duration: 1.643s, episode steps: 204, steps per second: 124, episode reward: -156.079, mean reward: -0.765 [-100.000, 37.668], mean action: 1.490 [0.000, 3.000],  loss: 22.577932, mse: 12662.210612, mean_q: 101.699723, mean_eps: 0.461329
  90133/150000: episode: 823, duration: 2.527s, episode steps: 252, steps per second: 100, episode reward: -108.244, mean reward: -0.430 [-100.000, 18.969], mean action: 1.583 [0.000, 3.000],  loss: 25.353408, mse: 12876.049468, mean_q: 103.910517, mean_eps: 0.459961
  90233/150000: episode: 824, duration: 0.977s, episode steps: 100, steps per second: 102, episode reward: 19.846, mean reward:  0.198 [-100.000, 14.863], mean action: 1.590 [0.000, 3.000],  loss: 26.037349, mse: 13110.825957, mean_q: 104.960775, mean_eps: 0.458905
  90567/150000: episode: 825, duration: 3.176s, episode steps: 334, steps per second: 105, episode reward: -366.820, mean reward: -1.098 [-100.000, 12.567], mean action: 1.734 [0.000, 3.000],  loss: 22.907776, mse: 13035.648072, mean_q: 104.630144, mean_eps: 0.457603
  90719/150000: episode: 826, duration: 1.609s, episode steps: 152, steps per second:  94, episode reward: 23.713, mean reward:  0.156 [-100.000, 13.437], mean action: 1.684 [0.000, 3.000],  loss: 28.938562, mse: 13085.949765, mean_q: 104.605181, mean_eps: 0.456145
  90848/150000: episode: 827, duration: 1.190s, episode steps: 129, steps per second: 108, episode reward: 27.493, mean reward:  0.213 [-100.000, 17.950], mean action: 1.651 [0.000, 3.000],  loss: 19.592233, mse: 13074.021711, mean_q: 105.107342, mean_eps: 0.455302
  90967/150000: episode: 828, duration: 1.157s, episode steps: 119, steps per second: 103, episode reward: -41.433, mean reward: -0.348 [-100.000, 10.151], mean action: 1.672 [0.000, 3.000],  loss: 22.771874, mse: 12940.601325, mean_q: 104.242369, mean_eps: 0.454558
  91083/150000: episode: 829, duration: 1.024s, episode steps: 116, steps per second: 113, episode reward:  1.115, mean reward:  0.010 [-100.000, 14.948], mean action: 1.552 [0.000, 3.000],  loss: 26.840602, mse: 12884.117928, mean_q: 103.821643, mean_eps: 0.453853
  91169/150000: episode: 830, duration: 0.774s, episode steps:  86, steps per second: 111, episode reward:  2.420, mean reward:  0.028 [-100.000, 20.560], mean action: 1.919 [0.000, 3.000],  loss: 24.156322, mse: 12670.990598, mean_q: 103.498957, mean_eps: 0.453247
  91309/150000: episode: 831, duration: 1.132s, episode steps: 140, steps per second: 124, episode reward: -197.925, mean reward: -1.414 [-100.000, 22.167], mean action: 1.650 [0.000, 3.000],  loss: 26.242366, mse: 12870.665067, mean_q: 103.847039, mean_eps: 0.452569
  91468/150000: episode: 832, duration: 1.279s, episode steps: 159, steps per second: 124, episode reward: -352.134, mean reward: -2.215 [-100.000, 15.535], mean action: 1.730 [0.000, 3.000],  loss: 29.617633, mse: 12607.016743, mean_q: 102.502253, mean_eps: 0.451672
  91597/150000: episode: 833, duration: 1.014s, episode steps: 129, steps per second: 127, episode reward: -442.209, mean reward: -3.428 [-100.000, 79.432], mean action: 1.628 [0.000, 3.000],  loss: 28.327223, mse: 12927.697356, mean_q: 104.640888, mean_eps: 0.450808
  91734/150000: episode: 834, duration: 1.121s, episode steps: 137, steps per second: 122, episode reward: -47.357, mean reward: -0.346 [-100.000, 15.122], mean action: 1.788 [0.000, 3.000],  loss: 23.391212, mse: 12838.883768, mean_q: 103.680477, mean_eps: 0.450010
  91852/150000: episode: 835, duration: 0.932s, episode steps: 118, steps per second: 127, episode reward: 28.114, mean reward:  0.238 [-100.000, 27.546], mean action: 1.831 [0.000, 3.000],  loss: 29.127192, mse: 12792.123924, mean_q: 103.871262, mean_eps: 0.449245
  91971/150000: episode: 836, duration: 0.946s, episode steps: 119, steps per second: 126, episode reward: -24.590, mean reward: -0.207 [-100.000, 11.897], mean action: 1.723 [0.000, 3.000],  loss: 34.254355, mse: 12859.047917, mean_q: 103.477553, mean_eps: 0.448534
  92123/150000: episode: 837, duration: 1.235s, episode steps: 152, steps per second: 123, episode reward: -31.762, mean reward: -0.209 [-100.000, 14.819], mean action: 1.757 [0.000, 3.000],  loss: 25.211231, mse: 13026.532342, mean_q: 104.877800, mean_eps: 0.447721
  92222/150000: episode: 838, duration: 0.805s, episode steps:  99, steps per second: 123, episode reward:  3.147, mean reward:  0.032 [-100.000, 21.183], mean action: 1.545 [0.000, 3.000],  loss: 21.192884, mse: 12977.147540, mean_q: 104.338730, mean_eps: 0.446968
  92671/150000: episode: 839, duration: 4.384s, episode steps: 449, steps per second: 102, episode reward: -223.841, mean reward: -0.499 [-100.000, 17.418], mean action: 1.822 [0.000, 3.000],  loss: 27.691938, mse: 13158.949739, mean_q: 106.003671, mean_eps: 0.445324
  92993/150000: episode: 840, duration: 2.778s, episode steps: 322, steps per second: 116, episode reward: -285.776, mean reward: -0.888 [-100.000, 17.134], mean action: 1.689 [0.000, 3.000],  loss: 29.323858, mse: 13004.117800, mean_q: 105.129991, mean_eps: 0.443011
  93264/150000: episode: 841, duration: 2.343s, episode steps: 271, steps per second: 116, episode reward: -42.527, mean reward: -0.157 [-100.000, 13.051], mean action: 1.712 [0.000, 3.000],  loss: 24.608153, mse: 12990.130137, mean_q: 105.101285, mean_eps: 0.441232
  94160/150000: episode: 842, duration: 7.901s, episode steps: 896, steps per second: 113, episode reward: -250.553, mean reward: -0.280 [-100.000, 23.016], mean action: 1.929 [0.000, 3.000],  loss: 22.361155, mse: 12918.012383, mean_q: 104.322809, mean_eps: 0.437731
  94254/150000: episode: 843, duration: 0.771s, episode steps:  94, steps per second: 122, episode reward: -145.395, mean reward: -1.547 [-100.000, 12.618], mean action: 1.681 [0.000, 3.000],  loss: 27.348110, mse: 12763.914405, mean_q: 103.693582, mean_eps: 0.434761
  94379/150000: episode: 844, duration: 0.985s, episode steps: 125, steps per second: 127, episode reward: -142.754, mean reward: -1.142 [-100.000, 67.009], mean action: 1.752 [0.000, 3.000],  loss: 31.093316, mse: 12889.568742, mean_q: 104.652183, mean_eps: 0.434104
  95379/150000: episode: 845, duration: 9.470s, episode steps: 1000, steps per second: 106, episode reward: 50.877, mean reward:  0.051 [-20.794, 22.694], mean action: 1.905 [0.000, 3.000],  loss: 27.016813, mse: 12841.468159, mean_q: 104.580069, mean_eps: 0.430729
  95482/150000: episode: 846, duration: 0.830s, episode steps: 103, steps per second: 124, episode reward: -25.100, mean reward: -0.244 [-100.000, 15.428], mean action: 1.621 [0.000, 3.000],  loss: 28.693641, mse: 12972.519380, mean_q: 104.800792, mean_eps: 0.427420
  95818/150000: episode: 847, duration: 2.772s, episode steps: 336, steps per second: 121, episode reward: -144.207, mean reward: -0.429 [-100.000, 19.085], mean action: 1.857 [0.000, 3.000],  loss: 25.360576, mse: 12828.574910, mean_q: 104.343418, mean_eps: 0.426103
  95931/150000: episode: 848, duration: 0.917s, episode steps: 113, steps per second: 123, episode reward: -9.846, mean reward: -0.087 [-100.000, 17.576], mean action: 1.743 [0.000, 3.000],  loss: 25.956381, mse: 12763.070917, mean_q: 104.186732, mean_eps: 0.424756
  96081/150000: episode: 849, duration: 1.150s, episode steps: 150, steps per second: 130, episode reward: -90.128, mean reward: -0.601 [-100.000, 14.044], mean action: 1.847 [0.000, 3.000],  loss: 27.477468, mse: 12823.094674, mean_q: 104.881254, mean_eps: 0.423967
  96215/150000: episode: 850, duration: 1.121s, episode steps: 134, steps per second: 119, episode reward: -42.190, mean reward: -0.315 [-100.000, 10.181], mean action: 1.724 [0.000, 3.000],  loss: 20.195087, mse: 13165.731722, mean_q: 107.382484, mean_eps: 0.423115
  96349/150000: episode: 851, duration: 1.040s, episode steps: 134, steps per second: 129, episode reward: 26.455, mean reward:  0.197 [-100.000, 15.630], mean action: 1.567 [0.000, 3.000],  loss: 22.801884, mse: 13140.101854, mean_q: 106.449950, mean_eps: 0.422311
  97349/150000: episode: 852, duration: 9.281s, episode steps: 1000, steps per second: 108, episode reward: 43.995, mean reward:  0.044 [-22.157, 21.947], mean action: 1.593 [0.000, 3.000],  loss: 25.444765, mse: 12900.379149, mean_q: 105.094778, mean_eps: 0.418909
  97444/150000: episode: 853, duration: 0.746s, episode steps:  95, steps per second: 127, episode reward: -2.692, mean reward: -0.028 [-100.000,  9.098], mean action: 1.821 [0.000, 3.000],  loss: 29.251443, mse: 12680.588271, mean_q: 104.009642, mean_eps: 0.415624
  97566/150000: episode: 854, duration: 0.971s, episode steps: 122, steps per second: 126, episode reward:  8.856, mean reward:  0.073 [-100.000, 21.059], mean action: 1.852 [0.000, 3.000],  loss: 27.881445, mse: 12591.794578, mean_q: 103.691224, mean_eps: 0.414973
  97697/150000: episode: 855, duration: 1.084s, episode steps: 131, steps per second: 121, episode reward: -11.916, mean reward: -0.091 [-100.000, 12.665], mean action: 1.763 [0.000, 3.000],  loss: 27.773172, mse: 12555.002870, mean_q: 104.291710, mean_eps: 0.414214
  97839/150000: episode: 856, duration: 1.101s, episode steps: 142, steps per second: 129, episode reward: -41.551, mean reward: -0.293 [-100.000, 14.651], mean action: 1.761 [0.000, 3.000],  loss: 22.039988, mse: 12680.505158, mean_q: 104.468105, mean_eps: 0.413395
  97944/150000: episode: 857, duration: 0.893s, episode steps: 105, steps per second: 118, episode reward: -30.820, mean reward: -0.294 [-100.000, 11.910], mean action: 1.610 [0.000, 3.000],  loss: 31.322482, mse: 12948.497489, mean_q: 107.578088, mean_eps: 0.412654
  98944/150000: episode: 858, duration: 9.661s, episode steps: 1000, steps per second: 104, episode reward: 76.325, mean reward:  0.076 [-20.045, 16.959], mean action: 1.789 [0.000, 3.000],  loss: 25.420903, mse: 12784.234698, mean_q: 105.567319, mean_eps: 0.409339
  99944/150000: episode: 859, duration: 10.002s, episode steps: 1000, steps per second: 100, episode reward: 83.865, mean reward:  0.084 [-23.684, 23.189], mean action: 1.414 [0.000, 3.000],  loss: 24.945292, mse: 12550.167115, mean_q: 105.448088, mean_eps: 0.403339
 100944/150000: episode: 860, duration: 9.294s, episode steps: 1000, steps per second: 108, episode reward: 43.311, mean reward:  0.043 [-21.005, 20.213], mean action: 1.720 [0.000, 3.000],  loss: 25.767591, mse: 12546.111260, mean_q: 105.721615, mean_eps: 0.397339
 101944/150000: episode: 861, duration: 9.268s, episode steps: 1000, steps per second: 108, episode reward: 97.203, mean reward:  0.097 [-24.334, 21.087], mean action: 1.595 [0.000, 3.000],  loss: 27.973083, mse: 12571.972573, mean_q: 106.521888, mean_eps: 0.391339
 102023/150000: episode: 862, duration: 0.663s, episode steps:  79, steps per second: 119, episode reward: -26.853, mean reward: -0.340 [-100.000, 11.579], mean action: 1.785 [0.000, 3.000],  loss: 26.595949, mse: 12445.730568, mean_q: 105.660522, mean_eps: 0.388102
 102188/150000: episode: 863, duration: 1.338s, episode steps: 165, steps per second: 123, episode reward: -30.818, mean reward: -0.187 [-100.000,  9.960], mean action: 1.830 [0.000, 3.000],  loss: 23.762098, mse: 12489.815956, mean_q: 104.880506, mean_eps: 0.387370
 102297/150000: episode: 864, duration: 0.956s, episode steps: 109, steps per second: 114, episode reward:  8.852, mean reward:  0.081 [-100.000, 17.243], mean action: 1.752 [0.000, 3.000],  loss: 28.049479, mse: 12498.454415, mean_q: 104.430449, mean_eps: 0.386548
 102420/150000: episode: 865, duration: 0.987s, episode steps: 123, steps per second: 125, episode reward:  9.839, mean reward:  0.080 [-100.000, 18.801], mean action: 1.634 [0.000, 3.000],  loss: 25.769192, mse: 12699.171605, mean_q: 106.345028, mean_eps: 0.385852
 102581/150000: episode: 866, duration: 1.358s, episode steps: 161, steps per second: 119, episode reward: -10.619, mean reward: -0.066 [-100.000, 15.177], mean action: 1.882 [0.000, 3.000],  loss: 23.042195, mse: 12684.221431, mean_q: 106.209643, mean_eps: 0.385000
 103581/150000: episode: 867, duration: 8.795s, episode steps: 1000, steps per second: 114, episode reward: 54.117, mean reward:  0.054 [-24.063, 25.522], mean action: 1.428 [0.000, 3.000],  loss: 24.742182, mse: 12558.558779, mean_q: 106.047037, mean_eps: 0.381517
 103697/150000: episode: 868, duration: 0.926s, episode steps: 116, steps per second: 125, episode reward:  7.668, mean reward:  0.066 [-100.000, 13.727], mean action: 1.853 [0.000, 3.000],  loss: 27.278997, mse: 12669.656107, mean_q: 107.273495, mean_eps: 0.378169
 103807/150000: episode: 869, duration: 0.906s, episode steps: 110, steps per second: 121, episode reward: -0.140, mean reward: -0.001 [-100.000, 15.230], mean action: 1.809 [0.000, 3.000],  loss: 30.550826, mse: 12867.530700, mean_q: 107.068028, mean_eps: 0.377491
 103890/150000: episode: 870, duration: 0.690s, episode steps:  83, steps per second: 120, episode reward: -18.800, mean reward: -0.227 [-100.000,  9.515], mean action: 1.687 [0.000, 3.000],  loss: 16.894776, mse: 12558.251224, mean_q: 105.873177, mean_eps: 0.376912
 104890/150000: episode: 871, duration: 9.752s, episode steps: 1000, steps per second: 103, episode reward: 48.704, mean reward:  0.049 [-23.543, 23.463], mean action: 1.315 [0.000, 3.000],  loss: 25.998425, mse: 12567.765746, mean_q: 106.202517, mean_eps: 0.373663
 105524/150000: episode: 872, duration: 5.703s, episode steps: 634, steps per second: 111, episode reward: -91.302, mean reward: -0.144 [-100.000, 39.497], mean action: 1.762 [0.000, 3.000],  loss: 27.898884, mse: 12652.895609, mean_q: 106.646140, mean_eps: 0.368761
 106524/150000: episode: 873, duration: 9.005s, episode steps: 1000, steps per second: 111, episode reward: 108.997, mean reward:  0.109 [-20.390, 23.946], mean action: 1.591 [0.000, 3.000],  loss: 26.468360, mse: 12388.179166, mean_q: 105.099490, mean_eps: 0.363859
 107241/150000: episode: 874, duration: 6.791s, episode steps: 717, steps per second: 106, episode reward: -307.377, mean reward: -0.429 [-100.000, 21.764], mean action: 1.842 [0.000, 3.000],  loss: 25.480507, mse: 12324.798238, mean_q: 104.776464, mean_eps: 0.358708
 107369/150000: episode: 875, duration: 1.131s, episode steps: 128, steps per second: 113, episode reward: -70.714, mean reward: -0.552 [-100.000,  8.141], mean action: 1.844 [0.000, 3.000],  loss: 28.080658, mse: 12159.607124, mean_q: 103.799436, mean_eps: 0.356173
 107469/150000: episode: 876, duration: 0.784s, episode steps: 100, steps per second: 128, episode reward: -84.599, mean reward: -0.846 [-100.000, 11.367], mean action: 1.630 [0.000, 3.000],  loss: 18.494511, mse: 11971.947900, mean_q: 103.127553, mean_eps: 0.355489
 108469/150000: episode: 877, duration: 8.907s, episode steps: 1000, steps per second: 112, episode reward: 110.822, mean reward:  0.111 [-23.823, 23.262], mean action: 1.352 [0.000, 3.000],  loss: 24.658480, mse: 12248.967023, mean_q: 105.357528, mean_eps: 0.352189
 109469/150000: episode: 878, duration: 9.344s, episode steps: 1000, steps per second: 107, episode reward: 126.527, mean reward:  0.127 [-24.347, 22.677], mean action: 1.497 [0.000, 3.000],  loss: 23.572182, mse: 12343.612625, mean_q: 106.955490, mean_eps: 0.346189
 110469/150000: episode: 879, duration: 9.111s, episode steps: 1000, steps per second: 110, episode reward: 91.435, mean reward:  0.091 [-20.378, 21.866], mean action: 1.390 [0.000, 3.000],  loss: 24.102491, mse: 12069.661634, mean_q: 105.672140, mean_eps: 0.340189
 111469/150000: episode: 880, duration: 8.720s, episode steps: 1000, steps per second: 115, episode reward: 114.137, mean reward:  0.114 [-23.429, 24.429], mean action: 1.093 [0.000, 3.000],  loss: 24.939942, mse: 11807.059187, mean_q: 104.542083, mean_eps: 0.334189
 112469/150000: episode: 881, duration: 8.476s, episode steps: 1000, steps per second: 118, episode reward: 125.767, mean reward:  0.126 [-20.280, 23.014], mean action: 0.948 [0.000, 3.000],  loss: 21.656549, mse: 11730.761869, mean_q: 103.948996, mean_eps: 0.328189
 113469/150000: episode: 882, duration: 8.882s, episode steps: 1000, steps per second: 113, episode reward: 66.248, mean reward:  0.066 [-23.554, 24.727], mean action: 1.215 [0.000, 3.000],  loss: 22.464368, mse: 11273.913518, mean_q: 101.923014, mean_eps: 0.322189
 113607/150000: episode: 883, duration: 1.094s, episode steps: 138, steps per second: 126, episode reward: -14.228, mean reward: -0.103 [-100.000,  8.543], mean action: 1.833 [0.000, 3.000],  loss: 21.141121, mse: 11353.522776, mean_q: 101.704707, mean_eps: 0.318775
 114607/150000: episode: 884, duration: 9.216s, episode steps: 1000, steps per second: 109, episode reward: 133.911, mean reward:  0.134 [-21.399, 23.897], mean action: 1.327 [0.000, 3.000],  loss: 25.126507, mse: 11348.369108, mean_q: 101.932072, mean_eps: 0.315361
 114754/150000: episode: 885, duration: 1.226s, episode steps: 147, steps per second: 120, episode reward: -10.988, mean reward: -0.075 [-100.000, 11.162], mean action: 1.646 [0.000, 3.000],  loss: 18.753761, mse: 11500.633364, mean_q: 102.220467, mean_eps: 0.311920
 114890/150000: episode: 886, duration: 1.110s, episode steps: 136, steps per second: 123, episode reward:  6.456, mean reward:  0.047 [-100.000, 18.760], mean action: 1.735 [0.000, 3.000],  loss: 21.832910, mse: 11539.855160, mean_q: 102.891087, mean_eps: 0.311071
 115114/150000: episode: 887, duration: 1.820s, episode steps: 224, steps per second: 123, episode reward: -5.466, mean reward: -0.024 [-100.000, 14.141], mean action: 1.607 [0.000, 3.000],  loss: 23.418241, mse: 11577.892426, mean_q: 103.431473, mean_eps: 0.309991
 115262/150000: episode: 888, duration: 1.195s, episode steps: 148, steps per second: 124, episode reward: -170.824, mean reward: -1.154 [-100.000, 10.157], mean action: 1.736 [0.000, 3.000],  loss: 24.655931, mse: 11517.500600, mean_q: 103.076820, mean_eps: 0.308875
 116262/150000: episode: 889, duration: 8.754s, episode steps: 1000, steps per second: 114, episode reward: 78.346, mean reward:  0.078 [-20.367, 24.421], mean action: 1.123 [0.000, 3.000],  loss: 24.121055, mse: 11519.434587, mean_q: 102.740275, mean_eps: 0.305431
 116957/150000: episode: 890, duration: 5.989s, episode steps: 695, steps per second: 116, episode reward: -178.886, mean reward: -0.257 [-100.000, 22.512], mean action: 1.518 [0.000, 3.000],  loss: 25.421375, mse: 11516.636721, mean_q: 102.881099, mean_eps: 0.300346
 117098/150000: episode: 891, duration: 1.118s, episode steps: 141, steps per second: 126, episode reward: 37.359, mean reward:  0.265 [-100.000, 18.667], mean action: 1.837 [0.000, 3.000],  loss: 22.934787, mse: 11195.885057, mean_q: 101.241012, mean_eps: 0.297838
 117214/150000: episode: 892, duration: 0.985s, episode steps: 116, steps per second: 118, episode reward: 30.716, mean reward:  0.265 [-100.000, 18.106], mean action: 1.819 [0.000, 3.000],  loss: 23.530069, mse: 11184.141804, mean_q: 101.183113, mean_eps: 0.297067
 117360/150000: episode: 893, duration: 1.174s, episode steps: 146, steps per second: 124, episode reward: 19.469, mean reward:  0.133 [-100.000, 14.776], mean action: 1.705 [0.000, 3.000],  loss: 25.253311, mse: 11504.055791, mean_q: 102.911406, mean_eps: 0.296281
 117476/150000: episode: 894, duration: 0.966s, episode steps: 116, steps per second: 120, episode reward: -72.228, mean reward: -0.623 [-100.000,  9.249], mean action: 1.810 [0.000, 3.000],  loss: 24.526474, mse: 11345.420452, mean_q: 101.494433, mean_eps: 0.295495
 118476/150000: episode: 895, duration: 8.719s, episode steps: 1000, steps per second: 115, episode reward: 99.091, mean reward:  0.099 [-23.299, 22.890], mean action: 1.209 [0.000, 3.000],  loss: 24.246559, mse: 11365.809960, mean_q: 102.198756, mean_eps: 0.292147
 119476/150000: episode: 896, duration: 8.814s, episode steps: 1000, steps per second: 113, episode reward: 125.148, mean reward:  0.125 [-20.112, 23.517], mean action: 1.287 [0.000, 3.000],  loss: 24.166102, mse: 11084.168122, mean_q: 100.864297, mean_eps: 0.286147
 120476/150000: episode: 897, duration: 9.087s, episode steps: 1000, steps per second: 110, episode reward: 97.948, mean reward:  0.098 [-22.053, 21.359], mean action: 1.237 [0.000, 3.000],  loss: 21.984103, mse: 11041.678915, mean_q: 100.862813, mean_eps: 0.280147
 121476/150000: episode: 898, duration: 8.586s, episode steps: 1000, steps per second: 116, episode reward: 138.293, mean reward:  0.138 [-23.968, 22.858], mean action: 1.145 [0.000, 3.000],  loss: 21.026767, mse: 10915.804140, mean_q: 100.355353, mean_eps: 0.274147
 121634/150000: episode: 899, duration: 1.274s, episode steps: 158, steps per second: 124, episode reward: 13.646, mean reward:  0.086 [-100.000, 13.218], mean action: 1.747 [0.000, 3.000],  loss: 20.110429, mse: 10726.479535, mean_q: 98.937079, mean_eps: 0.270673
 122634/150000: episode: 900, duration: 9.325s, episode steps: 1000, steps per second: 107, episode reward: 107.626, mean reward:  0.108 [-21.334, 23.425], mean action: 1.109 [0.000, 3.000],  loss: 19.152760, mse: 10757.803250, mean_q: 99.527909, mean_eps: 0.267199
 122775/150000: episode: 901, duration: 1.122s, episode steps: 141, steps per second: 126, episode reward: 19.571, mean reward:  0.139 [-100.000, 14.961], mean action: 1.596 [0.000, 3.000],  loss: 17.909610, mse: 10417.205999, mean_q: 98.208985, mean_eps: 0.263776
 123775/150000: episode: 902, duration: 8.704s, episode steps: 1000, steps per second: 115, episode reward: 74.062, mean reward:  0.074 [-21.202, 23.124], mean action: 1.219 [0.000, 3.000],  loss: 20.104797, mse: 10676.831172, mean_q: 99.173655, mean_eps: 0.260353
 124775/150000: episode: 903, duration: 8.749s, episode steps: 1000, steps per second: 114, episode reward: 137.873, mean reward:  0.138 [-22.748, 23.366], mean action: 1.175 [0.000, 3.000],  loss: 20.338688, mse: 10474.860021, mean_q: 98.605888, mean_eps: 0.254353
 125775/150000: episode: 904, duration: 8.494s, episode steps: 1000, steps per second: 118, episode reward: 38.963, mean reward:  0.039 [-23.381, 22.952], mean action: 1.446 [0.000, 3.000],  loss: 19.412109, mse: 10316.148951, mean_q: 98.145291, mean_eps: 0.248353
 125956/150000: episode: 905, duration: 1.479s, episode steps: 181, steps per second: 122, episode reward: -59.217, mean reward: -0.327 [-100.000,  9.965], mean action: 1.746 [0.000, 3.000],  loss: 18.279858, mse: 10379.254969, mean_q: 97.976093, mean_eps: 0.244810
 126956/150000: episode: 906, duration: 8.830s, episode steps: 1000, steps per second: 113, episode reward: 176.835, mean reward:  0.177 [-24.308, 22.869], mean action: 1.353 [0.000, 3.000],  loss: 19.564645, mse: 10029.504800, mean_q: 96.990811, mean_eps: 0.241267
 127956/150000: episode: 907, duration: 8.775s, episode steps: 1000, steps per second: 114, episode reward: 160.140, mean reward:  0.160 [-20.857, 23.317], mean action: 0.906 [0.000, 3.000],  loss: 19.158446, mse: 10086.088926, mean_q: 97.250610, mean_eps: 0.235267
 128956/150000: episode: 908, duration: 8.757s, episode steps: 1000, steps per second: 114, episode reward: 152.289, mean reward:  0.152 [-21.981, 22.917], mean action: 1.073 [0.000, 3.000],  loss: 16.185440, mse: 9986.408897, mean_q: 96.406439, mean_eps: 0.229267
 129254/150000: episode: 909, duration: 3.015s, episode steps: 298, steps per second:  99, episode reward: 290.505, mean reward:  0.975 [-19.319, 100.000], mean action: 1.299 [0.000, 3.000],  loss: 16.999087, mse: 9818.283229, mean_q: 95.592633, mean_eps: 0.225373
 130254/150000: episode: 910, duration: 9.283s, episode steps: 1000, steps per second: 108, episode reward: 88.227, mean reward:  0.088 [-24.117, 23.026], mean action: 1.131 [0.000, 3.000],  loss: 16.438598, mse: 9765.969371, mean_q: 95.347387, mean_eps: 0.221479
 131254/150000: episode: 911, duration: 9.954s, episode steps: 1000, steps per second: 100, episode reward: 156.895, mean reward:  0.157 [-20.976, 23.437], mean action: 1.134 [0.000, 3.000],  loss: 16.028581, mse: 9677.523154, mean_q: 95.009072, mean_eps: 0.215479
 132254/150000: episode: 912, duration: 10.364s, episode steps: 1000, steps per second:  96, episode reward: 139.116, mean reward:  0.139 [-22.232, 23.110], mean action: 0.885 [0.000, 3.000],  loss: 17.834644, mse: 9395.685856, mean_q: 93.878019, mean_eps: 0.209479
 132643/150000: episode: 913, duration: 3.999s, episode steps: 389, steps per second:  97, episode reward: -243.931, mean reward: -0.627 [-100.000, 32.148], mean action: 1.946 [0.000, 3.000],  loss: 17.528350, mse: 9032.490866, mean_q: 92.364718, mean_eps: 0.205312
 133002/150000: episode: 914, duration: 3.334s, episode steps: 359, steps per second: 108, episode reward: -234.825, mean reward: -0.654 [-100.000, 23.545], mean action: 2.067 [0.000, 3.000],  loss: 13.116356, mse: 8956.850443, mean_q: 91.928343, mean_eps: 0.203068
 134002/150000: episode: 915, duration: 9.130s, episode steps: 1000, steps per second: 110, episode reward: 109.928, mean reward:  0.110 [-22.544, 22.438], mean action: 1.309 [0.000, 3.000],  loss: 15.967901, mse: 9077.269672, mean_q: 93.463956, mean_eps: 0.198991
 134923/150000: episode: 916, duration: 8.192s, episode steps: 921, steps per second: 112, episode reward: 273.093, mean reward:  0.297 [-22.356, 100.000], mean action: 1.204 [0.000, 3.000],  loss: 15.927786, mse: 8622.257071, mean_q: 90.950732, mean_eps: 0.193228
 135148/150000: episode: 917, duration: 1.897s, episode steps: 225, steps per second: 119, episode reward: 272.910, mean reward:  1.213 [-3.043, 100.000], mean action: 1.640 [0.000, 3.000],  loss: 12.053667, mse: 8612.420054, mean_q: 91.347008, mean_eps: 0.189790
 135709/150000: episode: 918, duration: 4.746s, episode steps: 561, steps per second: 118, episode reward: 291.030, mean reward:  0.519 [-18.023, 100.000], mean action: 1.041 [0.000, 3.000],  loss: 14.048614, mse: 8522.208285, mean_q: 90.469468, mean_eps: 0.187432
 136709/150000: episode: 919, duration: 9.994s, episode steps: 1000, steps per second: 100, episode reward: 164.691, mean reward:  0.165 [-19.937, 23.287], mean action: 1.259 [0.000, 3.000],  loss: 13.872348, mse: 8513.945321, mean_q: 90.886844, mean_eps: 0.182749
 136790/150000: episode: 920, duration: 0.650s, episode steps:  81, steps per second: 125, episode reward: -2.691, mean reward: -0.033 [-100.000, 21.147], mean action: 1.840 [0.000, 3.000],  loss: 16.072040, mse: 8410.951087, mean_q: 90.028910, mean_eps: 0.179506
 137221/150000: episode: 921, duration: 3.666s, episode steps: 431, steps per second: 118, episode reward: 264.684, mean reward:  0.614 [-18.740, 100.000], mean action: 1.137 [0.000, 3.000],  loss: 15.913213, mse: 8317.079038, mean_q: 90.150808, mean_eps: 0.177970
 137743/150000: episode: 922, duration: 4.471s, episode steps: 522, steps per second: 117, episode reward: 245.710, mean reward:  0.471 [-19.417, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 14.630198, mse: 8275.200113, mean_q: 89.867961, mean_eps: 0.175111
 138621/150000: episode: 923, duration: 7.114s, episode steps: 878, steps per second: 123, episode reward: 266.136, mean reward:  0.303 [-20.407, 100.000], mean action: 0.946 [0.000, 3.000],  loss: 11.105711, mse: 8212.694993, mean_q: 89.806576, mean_eps: 0.170911
 138978/150000: episode: 924, duration: 2.826s, episode steps: 357, steps per second: 126, episode reward: 167.678, mean reward:  0.470 [-21.855, 100.000], mean action: 1.793 [0.000, 3.000],  loss: 10.550649, mse: 8284.277378, mean_q: 89.955035, mean_eps: 0.167206
 139978/150000: episode: 925, duration: 8.457s, episode steps: 1000, steps per second: 118, episode reward: 127.555, mean reward:  0.128 [-19.706, 24.231], mean action: 1.157 [0.000, 3.000],  loss: 12.530245, mse: 8151.424292, mean_q: 89.265314, mean_eps: 0.163135
 140554/150000: episode: 926, duration: 4.596s, episode steps: 576, steps per second: 125, episode reward: 226.806, mean reward:  0.394 [-20.101, 100.000], mean action: 1.439 [0.000, 3.000],  loss: 12.271180, mse: 8017.863344, mean_q: 88.627749, mean_eps: 0.158407
 141253/150000: episode: 927, duration: 5.829s, episode steps: 699, steps per second: 120, episode reward: 267.263, mean reward:  0.382 [-18.585, 100.000], mean action: 0.863 [0.000, 3.000],  loss: 12.009842, mse: 7828.653706, mean_q: 87.864551, mean_eps: 0.154582
 141499/150000: episode: 928, duration: 1.941s, episode steps: 246, steps per second: 127, episode reward: 281.203, mean reward:  1.143 [-8.898, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 12.090669, mse: 7683.758384, mean_q: 87.611196, mean_eps: 0.151747
 141917/150000: episode: 929, duration: 3.213s, episode steps: 418, steps per second: 130, episode reward: 271.331, mean reward:  0.649 [-17.763, 100.000], mean action: 0.983 [0.000, 3.000],  loss: 10.610598, mse: 7735.898142, mean_q: 87.965698, mean_eps: 0.149755
 142550/150000: episode: 930, duration: 5.239s, episode steps: 633, steps per second: 121, episode reward: -438.063, mean reward: -0.692 [-100.000, 120.036], mean action: 1.731 [0.000, 3.000],  loss: 10.834427, mse: 7720.318410, mean_q: 88.015926, mean_eps: 0.146602
 142774/150000: episode: 931, duration: 1.972s, episode steps: 224, steps per second: 114, episode reward: 295.742, mean reward:  1.320 [-10.034, 100.000], mean action: 1.375 [0.000, 3.000],  loss: 14.549895, mse: 7848.443752, mean_q: 88.117981, mean_eps: 0.144031
 143119/150000: episode: 932, duration: 2.784s, episode steps: 345, steps per second: 124, episode reward: -275.505, mean reward: -0.799 [-100.000, 29.547], mean action: 1.849 [0.000, 3.000],  loss: 10.012287, mse: 7983.141960, mean_q: 88.397573, mean_eps: 0.142324
 143676/150000: episode: 933, duration: 4.555s, episode steps: 557, steps per second: 122, episode reward: 231.679, mean reward:  0.416 [-20.828, 100.000], mean action: 1.294 [0.000, 3.000],  loss: 8.263821, mse: 7864.590693, mean_q: 88.378379, mean_eps: 0.139618
 144213/150000: episode: 934, duration: 4.137s, episode steps: 537, steps per second: 130, episode reward: 260.954, mean reward:  0.486 [-21.319, 100.000], mean action: 0.924 [0.000, 3.000],  loss: 10.362236, mse: 8019.517852, mean_q: 88.682655, mean_eps: 0.136336
 144522/150000: episode: 935, duration: 2.784s, episode steps: 309, steps per second: 111, episode reward: 263.077, mean reward:  0.851 [-8.133, 100.000], mean action: 1.515 [0.000, 3.000],  loss: 9.068243, mse: 7878.899838, mean_q: 88.012525, mean_eps: 0.133798
 145139/150000: episode: 936, duration: 4.999s, episode steps: 617, steps per second: 123, episode reward: 211.575, mean reward:  0.343 [-22.617, 100.000], mean action: 1.185 [0.000, 3.000],  loss: 10.149077, mse: 7906.545441, mean_q: 88.189643, mean_eps: 0.131020
 145649/150000: episode: 937, duration: 3.986s, episode steps: 510, steps per second: 128, episode reward: 231.013, mean reward:  0.453 [-19.721, 100.000], mean action: 1.384 [0.000, 3.000],  loss: 9.038264, mse: 7849.942526, mean_q: 88.101529, mean_eps: 0.127639
 146252/150000: episode: 938, duration: 4.680s, episode steps: 603, steps per second: 129, episode reward: 262.979, mean reward:  0.436 [-19.775, 100.000], mean action: 1.357 [0.000, 3.000],  loss: 9.418600, mse: 7899.835431, mean_q: 88.119066, mean_eps: 0.124300
 146646/150000: episode: 939, duration: 2.934s, episode steps: 394, steps per second: 134, episode reward: 250.695, mean reward:  0.636 [-18.938, 100.000], mean action: 1.404 [0.000, 3.000],  loss: 9.702403, mse: 7619.979137, mean_q: 86.503868, mean_eps: 0.121309
 147095/150000: episode: 940, duration: 3.496s, episode steps: 449, steps per second: 128, episode reward: 300.658, mean reward:  0.670 [-18.848, 100.000], mean action: 1.038 [0.000, 3.000],  loss: 8.702461, mse: 7806.111660, mean_q: 87.506846, mean_eps: 0.118780
 147507/150000: episode: 941, duration: 3.268s, episode steps: 412, steps per second: 126, episode reward: 237.335, mean reward:  0.576 [-11.509, 100.000], mean action: 1.614 [0.000, 3.000],  loss: 9.327006, mse: 7704.300957, mean_q: 86.879925, mean_eps: 0.116197
 148063/150000: episode: 942, duration: 4.287s, episode steps: 556, steps per second: 130, episode reward: 278.609, mean reward:  0.501 [-19.229, 100.000], mean action: 1.201 [0.000, 3.000],  loss: 7.638709, mse: 7546.648798, mean_q: 86.475847, mean_eps: 0.113293
 148359/150000: episode: 943, duration: 2.203s, episode steps: 296, steps per second: 134, episode reward: 278.498, mean reward:  0.941 [-7.582, 100.000], mean action: 1.409 [0.000, 3.000],  loss: 8.092106, mse: 7620.925860, mean_q: 86.844407, mean_eps: 0.110737
 149359/150000: episode: 944, duration: 8.347s, episode steps: 1000, steps per second: 120, episode reward: 135.869, mean reward:  0.136 [-20.011, 22.917], mean action: 1.238 [0.000, 3.000],  loss: 8.160576, mse: 7658.945193, mean_q: 87.075802, mean_eps: 0.106849
 149734/150000: episode: 945, duration: 2.950s, episode steps: 375, steps per second: 127, episode reward: 189.878, mean reward:  0.506 [-10.253, 100.000], mean action: 1.712 [0.000, 3.000],  loss: 5.867792, mse: 7662.448573, mean_q: 87.220212, mean_eps: 0.102724
done, took 1300.273 seconds
Testing for 5 episodes ...
Episode 1: reward: 186.956, steps: 444
Episode 2: reward: 214.801, steps: 440
Episode 3: reward: 235.327, steps: 380
Episode 4: reward: 179.768, steps: 437
Episode 5: reward: 207.736, steps: 324
Testing for 5 episodes ...
Episode 1: reward: 237.999, steps: 461
Episode 2: reward: 174.927, steps: 497
Episode 3: reward: 272.777, steps: 284
Episode 4: reward: 290.249, steps: 307
Episode 5: reward: 212.818, steps: 233
Testing for 5 episodes ...
Episode 1: reward: 215.685, steps: 444
Episode 2: reward: 187.913, steps: 467
Episode 3: reward: -129.220, steps: 365
Episode 4: reward: 268.805, steps: 324
Episode 5: reward: 266.000, steps: 405