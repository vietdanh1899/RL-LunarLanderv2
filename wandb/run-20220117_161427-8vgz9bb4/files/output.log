Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 150000 steps ...
     96/150000: episode: 1, duration: 1.149s, episode steps:  96, steps per second:  84, episode reward: -329.923, mean reward: -3.437 [-100.000,  5.616], mean action: 1.448 [0.000, 3.000],  loss: 1.735906, mse: 1.853438, mean_q: -0.498404, mean_eps: 0.999682
    178/150000: episode: 2, duration: 0.519s, episode steps:  82, steps per second: 158, episode reward: -55.320, mean reward: -0.675 [-100.000, 17.487], mean action: 1.378 [0.000, 3.000],  loss: 18.238595, mse: 81.563557, mean_q: -6.735515, mean_eps: 0.999181
    272/150000: episode: 3, duration: 0.609s, episode steps:  94, steps per second: 154, episode reward: -331.944, mean reward: -3.531 [-100.000, -0.100], mean action: 1.383 [0.000, 3.000],  loss: 18.208615, mse: 1127.742169, mean_q: -30.530256, mean_eps: 0.998653
    380/150000: episode: 4, duration: 0.702s, episode steps: 108, steps per second: 154, episode reward: -266.675, mean reward: -2.469 [-100.000,  1.060], mean action: 1.444 [0.000, 3.000],  loss: 16.851891, mse: 4341.207144, mean_q: -68.442130, mean_eps: 0.998047
    510/150000: episode: 5, duration: 0.855s, episode steps: 130, steps per second: 152, episode reward: -199.849, mean reward: -1.537 [-100.000, 31.442], mean action: 1.362 [0.000, 3.000],  loss: 19.429526, mse: 8329.123505, mean_q: -96.091356, mean_eps: 0.997333
    593/150000: episode: 6, duration: 0.521s, episode steps:  83, steps per second: 159, episode reward: -245.771, mean reward: -2.961 [-100.000, 27.008], mean action: 1.313 [0.000, 3.000],  loss: 14.309552, mse: 6702.048093, mean_q: -85.049022, mean_eps: 0.996694
    716/150000: episode: 7, duration: 0.857s, episode steps: 123, steps per second: 143, episode reward: -471.310, mean reward: -3.832 [-100.000,  2.523], mean action: 1.577 [0.000, 3.000],  loss: 16.872044, mse: 4792.925630, mean_q: -64.206165, mean_eps: 0.996076
    865/150000: episode: 8, duration: 0.964s, episode steps: 149, steps per second: 155, episode reward: -302.681, mean reward: -2.031 [-100.000, 77.968], mean action: 1.483 [0.000, 3.000],  loss: 13.706067, mse: 2496.064959, mean_q: -40.821344, mean_eps: 0.995260
    997/150000: episode: 9, duration: 0.899s, episode steps: 132, steps per second: 147, episode reward: -82.982, mean reward: -0.629 [-100.000, 15.453], mean action: 1.386 [0.000, 3.000],  loss: 14.407546, mse: 2838.140240, mean_q: -48.544869, mean_eps: 0.994417
   1094/150000: episode: 10, duration: 0.649s, episode steps:  97, steps per second: 150, episode reward: -247.570, mean reward: -2.552 [-100.000, 21.805], mean action: 1.392 [0.000, 3.000],  loss: 16.545007, mse: 2559.244613, mean_q: -43.454907, mean_eps: 0.993730
   1160/150000: episode: 11, duration: 0.433s, episode steps:  66, steps per second: 152, episode reward: -8.199, mean reward: -0.124 [-100.000, 43.613], mean action: 1.530 [0.000, 3.000],  loss: 12.813357, mse: 1896.736186, mean_q: -35.320341, mean_eps: 0.993241
   1216/150000: episode: 12, duration: 0.383s, episode steps:  56, steps per second: 146, episode reward: -118.839, mean reward: -2.122 [-100.000, 13.135], mean action: 1.411 [0.000, 3.000],  loss: 8.291135, mse: 1443.678258, mean_q: -28.427630, mean_eps: 0.992875
   1284/150000: episode: 13, duration: 0.456s, episode steps:  68, steps per second: 149, episode reward: -93.226, mean reward: -1.371 [-100.000, 70.460], mean action: 1.471 [0.000, 3.000],  loss: 14.411841, mse: 1091.073761, mean_q: -21.348536, mean_eps: 0.992503
   1405/150000: episode: 14, duration: 0.831s, episode steps: 121, steps per second: 146, episode reward: -275.489, mean reward: -2.277 [-100.000, 109.476], mean action: 1.446 [0.000, 3.000],  loss: 21.815133, mse: 930.348385, mean_q: -17.574855, mean_eps: 0.991936
   1461/150000: episode: 15, duration: 0.383s, episode steps:  56, steps per second: 146, episode reward: -107.838, mean reward: -1.926 [-100.000, 12.482], mean action: 1.339 [0.000, 3.000],  loss: 22.194985, mse: 1132.528469, mean_q: -19.964285, mean_eps: 0.991405
   1524/150000: episode: 16, duration: 0.403s, episode steps:  63, steps per second: 156, episode reward: -70.197, mean reward: -1.114 [-100.000,  9.067], mean action: 1.508 [0.000, 3.000],  loss: 25.749985, mse: 873.570859, mean_q: -16.800245, mean_eps: 0.991048
   1618/150000: episode: 17, duration: 0.648s, episode steps:  94, steps per second: 145, episode reward: -152.126, mean reward: -1.618 [-100.000, 12.705], mean action: 1.532 [0.000, 3.000],  loss: 25.313915, mse: 900.279699, mean_q: -17.117118, mean_eps: 0.990577
   1680/150000: episode: 18, duration: 0.448s, episode steps:  62, steps per second: 138, episode reward: -153.415, mean reward: -2.474 [-100.000, 30.212], mean action: 1.581 [0.000, 3.000],  loss: 26.422093, mse: 859.371431, mean_q: -16.852657, mean_eps: 0.990109
   1835/150000: episode: 19, duration: 1.087s, episode steps: 155, steps per second: 143, episode reward: -176.395, mean reward: -1.138 [-100.000, 12.151], mean action: 1.561 [0.000, 3.000],  loss: 23.039404, mse: 1043.700107, mean_q: -20.019831, mean_eps: 0.989458
   1944/150000: episode: 20, duration: 0.750s, episode steps: 109, steps per second: 145, episode reward: -247.831, mean reward: -2.274 [-100.000, 53.103], mean action: 1.165 [0.000, 3.000],  loss: 19.832801, mse: 1239.549834, mean_q: -23.254107, mean_eps: 0.988666
   2021/150000: episode: 21, duration: 0.544s, episode steps:  77, steps per second: 142, episode reward: -116.215, mean reward: -1.509 [-100.000,  9.767], mean action: 1.519 [0.000, 3.000],  loss: 22.941180, mse: 1355.022314, mean_q: -23.395036, mean_eps: 0.988108
   2153/150000: episode: 22, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -159.459, mean reward: -1.208 [-100.000, 12.653], mean action: 1.417 [0.000, 3.000],  loss: 21.196451, mse: 1273.187653, mean_q: -22.619323, mean_eps: 0.987481
   2229/150000: episode: 23, duration: 0.579s, episode steps:  76, steps per second: 131, episode reward: -291.343, mean reward: -3.833 [-100.000,  5.046], mean action: 1.368 [0.000, 3.000],  loss: 14.509588, mse: 1147.134644, mean_q: -20.891126, mean_eps: 0.986857
   2352/150000: episode: 24, duration: 0.862s, episode steps: 123, steps per second: 143, episode reward: -291.537, mean reward: -2.370 [-100.000,  2.412], mean action: 1.447 [0.000, 3.000],  loss: 18.291911, mse: 1249.594819, mean_q: -22.245668, mean_eps: 0.986260
   2445/150000: episode: 25, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: -207.651, mean reward: -2.233 [-100.000,  6.592], mean action: 1.785 [0.000, 3.000],  loss: 17.808246, mse: 1288.309724, mean_q: -22.199044, mean_eps: 0.985612
   2543/150000: episode: 26, duration: 0.721s, episode steps:  98, steps per second: 136, episode reward: -78.604, mean reward: -0.802 [-100.000, 23.184], mean action: 1.459 [0.000, 3.000],  loss: 11.472903, mse: 1236.150745, mean_q: -21.851042, mean_eps: 0.985039
   2666/150000: episode: 27, duration: 0.800s, episode steps: 123, steps per second: 154, episode reward: -332.357, mean reward: -2.702 [-100.000, 98.854], mean action: 1.480 [0.000, 3.000],  loss: 14.586123, mse: 1184.971669, mean_q: -21.133230, mean_eps: 0.984376
   2757/150000: episode: 28, duration: 0.589s, episode steps:  91, steps per second: 155, episode reward: -195.040, mean reward: -2.143 [-100.000, 32.170], mean action: 1.516 [0.000, 3.000],  loss: 16.296298, mse: 1499.997178, mean_q: -23.834091, mean_eps: 0.983734
   2829/150000: episode: 29, duration: 0.471s, episode steps:  72, steps per second: 153, episode reward: -92.026, mean reward: -1.278 [-100.000,  8.501], mean action: 1.542 [0.000, 3.000],  loss: 23.266844, mse: 1493.692714, mean_q: -23.863736, mean_eps: 0.983245
   2930/150000: episode: 30, duration: 0.751s, episode steps: 101, steps per second: 134, episode reward: -152.843, mean reward: -1.513 [-100.000,  6.019], mean action: 1.644 [0.000, 3.000],  loss: 21.546209, mse: 1637.721576, mean_q: -25.808029, mean_eps: 0.982726
   3022/150000: episode: 31, duration: 0.595s, episode steps:  92, steps per second: 155, episode reward: -159.153, mean reward: -1.730 [-100.000,  7.683], mean action: 1.337 [0.000, 3.000],  loss: 19.249817, mse: 1776.293697, mean_q: -27.926365, mean_eps: 0.982147
   3118/150000: episode: 32, duration: 0.635s, episode steps:  96, steps per second: 151, episode reward: -184.785, mean reward: -1.925 [-100.000, 34.988], mean action: 1.542 [0.000, 3.000],  loss: 20.569889, mse: 2229.896005, mean_q: -31.926253, mean_eps: 0.981583
   3212/150000: episode: 33, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -218.989, mean reward: -2.330 [-100.000,  7.145], mean action: 1.521 [0.000, 3.000],  loss: 20.870405, mse: 2050.243198, mean_q: -29.760376, mean_eps: 0.981013
   3290/150000: episode: 34, duration: 0.575s, episode steps:  78, steps per second: 136, episode reward: -132.248, mean reward: -1.695 [-100.000,  6.777], mean action: 1.526 [0.000, 3.000],  loss: 16.914407, mse: 1862.311954, mean_q: -28.452048, mean_eps: 0.980497
   3369/150000: episode: 35, duration: 0.537s, episode steps:  79, steps per second: 147, episode reward: -103.002, mean reward: -1.304 [-100.000,  5.266], mean action: 1.570 [0.000, 3.000],  loss: 17.909557, mse: 1530.592624, mean_q: -25.257297, mean_eps: 0.980026
   3432/150000: episode: 36, duration: 0.414s, episode steps:  63, steps per second: 152, episode reward: -141.444, mean reward: -2.245 [-100.000, 44.178], mean action: 1.524 [0.000, 3.000],  loss: 13.452974, mse: 1443.062492, mean_q: -24.487030, mean_eps: 0.979600
   3527/150000: episode: 37, duration: 0.650s, episode steps:  95, steps per second: 146, episode reward: -302.802, mean reward: -3.187 [-100.000,  0.539], mean action: 1.516 [0.000, 3.000],  loss: 17.796085, mse: 1606.490232, mean_q: -26.156652, mean_eps: 0.979126
   3640/150000: episode: 38, duration: 0.842s, episode steps: 113, steps per second: 134, episode reward: -157.375, mean reward: -1.393 [-100.000, 14.284], mean action: 1.504 [0.000, 3.000],  loss: 12.399408, mse: 1821.085707, mean_q: -28.426059, mean_eps: 0.978502
   3758/150000: episode: 39, duration: 0.971s, episode steps: 118, steps per second: 122, episode reward: -229.383, mean reward: -1.944 [-100.000,  1.328], mean action: 1.559 [0.000, 3.000],  loss: 12.427003, mse: 1819.986415, mean_q: -28.812485, mean_eps: 0.977809
   3830/150000: episode: 40, duration: 0.589s, episode steps:  72, steps per second: 122, episode reward: -93.534, mean reward: -1.299 [-100.000, 16.172], mean action: 1.611 [0.000, 3.000],  loss: 20.392731, mse: 2186.109495, mean_q: -32.284563, mean_eps: 0.977239
   3965/150000: episode: 41, duration: 1.243s, episode steps: 135, steps per second: 109, episode reward: -123.945, mean reward: -0.918 [-100.000, 15.948], mean action: 1.622 [0.000, 3.000],  loss: 15.542299, mse: 2121.917792, mean_q: -31.746209, mean_eps: 0.976618
   4047/150000: episode: 42, duration: 0.714s, episode steps:  82, steps per second: 115, episode reward: -71.195, mean reward: -0.868 [-100.000, 10.755], mean action: 1.512 [0.000, 3.000],  loss: 14.959055, mse: 2107.576233, mean_q: -31.451693, mean_eps: 0.975967
   4155/150000: episode: 43, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: 13.350, mean reward:  0.124 [-100.000, 129.668], mean action: 1.519 [0.000, 3.000],  loss: 15.166100, mse: 1848.267879, mean_q: -28.737268, mean_eps: 0.975397
   4232/150000: episode: 44, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward: -98.676, mean reward: -1.282 [-100.000,  9.801], mean action: 1.325 [0.000, 3.000],  loss: 11.374289, mse: 1788.274646, mean_q: -28.476829, mean_eps: 0.974842
   4358/150000: episode: 45, duration: 0.888s, episode steps: 126, steps per second: 142, episode reward: -10.923, mean reward: -0.087 [-100.000, 124.381], mean action: 1.484 [0.000, 3.000],  loss: 17.429952, mse: 2166.781673, mean_q: -31.291689, mean_eps: 0.974233
   4419/150000: episode: 46, duration: 0.402s, episode steps:  61, steps per second: 152, episode reward: -180.915, mean reward: -2.966 [-100.000,  6.535], mean action: 1.574 [0.000, 3.000],  loss: 14.105096, mse: 2277.337821, mean_q: -33.421069, mean_eps: 0.973672
   4550/150000: episode: 47, duration: 0.862s, episode steps: 131, steps per second: 152, episode reward: -76.786, mean reward: -0.586 [-100.000,  7.070], mean action: 1.466 [0.000, 3.000],  loss: 15.065913, mse: 1787.706293, mean_q: -28.201386, mean_eps: 0.973096
   4655/150000: episode: 48, duration: 0.714s, episode steps: 105, steps per second: 147, episode reward: -346.479, mean reward: -3.300 [-100.000, 13.110], mean action: 1.571 [0.000, 3.000],  loss: 18.235172, mse: 1573.599955, mean_q: -25.331635, mean_eps: 0.972388
   4739/150000: episode: 49, duration: 0.572s, episode steps:  84, steps per second: 147, episode reward: -370.803, mean reward: -4.414 [-100.000, 74.993], mean action: 1.679 [0.000, 3.000],  loss: 14.836308, mse: 1503.010917, mean_q: -24.607748, mean_eps: 0.971821
   4799/150000: episode: 50, duration: 0.406s, episode steps:  60, steps per second: 148, episode reward: -100.618, mean reward: -1.677 [-100.000,  7.376], mean action: 1.600 [0.000, 3.000],  loss: 21.003757, mse: 1381.586097, mean_q: -23.219735, mean_eps: 0.971389
   4901/150000: episode: 51, duration: 0.666s, episode steps: 102, steps per second: 153, episode reward: -114.200, mean reward: -1.120 [-100.000, 11.614], mean action: 1.588 [0.000, 3.000],  loss: 16.641339, mse: 1452.504991, mean_q: -23.663648, mean_eps: 0.970903
   4984/150000: episode: 52, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -399.238, mean reward: -4.810 [-100.000, 108.392], mean action: 1.494 [0.000, 3.000],  loss: 21.053503, mse: 1779.146186, mean_q: -27.096715, mean_eps: 0.970348
   5072/150000: episode: 53, duration: 0.628s, episode steps:  88, steps per second: 140, episode reward: -251.737, mean reward: -2.861 [-100.000, 25.553], mean action: 1.477 [0.000, 3.000],  loss: 24.004060, mse: 2026.875024, mean_q: -29.665058, mean_eps: 0.969835
   5172/150000: episode: 54, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: -135.810, mean reward: -1.358 [-100.000, 10.317], mean action: 1.520 [0.000, 3.000],  loss: 23.105948, mse: 1824.783527, mean_q: -27.627859, mean_eps: 0.969271
   5273/150000: episode: 55, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -132.572, mean reward: -1.313 [-100.000, 18.249], mean action: 1.426 [0.000, 3.000],  loss: 19.244861, mse: 1700.596595, mean_q: -25.483249, mean_eps: 0.968668
   5371/150000: episode: 56, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -207.681, mean reward: -2.119 [-100.000, 21.524], mean action: 1.520 [0.000, 3.000],  loss: 21.605021, mse: 1966.963348, mean_q: -26.687750, mean_eps: 0.968071
   5437/150000: episode: 57, duration: 0.456s, episode steps:  66, steps per second: 145, episode reward: -109.211, mean reward: -1.655 [-100.000,  8.673], mean action: 1.545 [0.000, 3.000],  loss: 29.353411, mse: 2182.006779, mean_q: -26.974507, mean_eps: 0.967579
   5565/150000: episode: 58, duration: 0.877s, episode steps: 128, steps per second: 146, episode reward: -132.415, mean reward: -1.034 [-100.000, 19.116], mean action: 1.469 [0.000, 3.000],  loss: 26.976864, mse: 2091.195951, mean_q: -27.167929, mean_eps: 0.966997
   5653/150000: episode: 59, duration: 0.650s, episode steps:  88, steps per second: 135, episode reward: -194.462, mean reward: -2.210 [-100.000, 21.311], mean action: 1.500 [0.000, 3.000],  loss: 21.355039, mse: 1742.976916, mean_q: -23.228201, mean_eps: 0.966349
   5750/150000: episode: 60, duration: 0.658s, episode steps:  97, steps per second: 147, episode reward: -287.250, mean reward: -2.961 [-100.000,  5.601], mean action: 1.371 [0.000, 3.000],  loss: 20.577372, mse: 1976.636550, mean_q: -24.472463, mean_eps: 0.965794
   5809/150000: episode: 61, duration: 0.408s, episode steps:  59, steps per second: 144, episode reward: -48.896, mean reward: -0.829 [-100.000, 11.543], mean action: 1.525 [0.000, 3.000],  loss: 31.074146, mse: 2350.015037, mean_q: -23.563469, mean_eps: 0.965326
   5907/150000: episode: 62, duration: 0.692s, episode steps:  98, steps per second: 142, episode reward: -89.427, mean reward: -0.913 [-100.000, 12.179], mean action: 1.755 [0.000, 3.000],  loss: 23.247580, mse: 1922.407706, mean_q: -16.889999, mean_eps: 0.964855
   6014/150000: episode: 63, duration: 0.780s, episode steps: 107, steps per second: 137, episode reward: -285.293, mean reward: -2.666 [-100.000,  0.576], mean action: 1.533 [0.000, 3.000],  loss: 17.241925, mse: 1973.778349, mean_q: -10.440513, mean_eps: 0.964240
   6127/150000: episode: 64, duration: 0.788s, episode steps: 113, steps per second: 143, episode reward: -104.975, mean reward: -0.929 [-100.000, 10.138], mean action: 1.513 [0.000, 3.000],  loss: 17.222001, mse: 2481.169018, mean_q: -7.190120, mean_eps: 0.963580
   6238/150000: episode: 65, duration: 0.793s, episode steps: 111, steps per second: 140, episode reward: -219.890, mean reward: -1.981 [-100.000,  5.604], mean action: 1.306 [0.000, 3.000],  loss: 17.066598, mse: 2814.393670, mean_q: -4.041500, mean_eps: 0.962908
   6318/150000: episode: 66, duration: 0.555s, episode steps:  80, steps per second: 144, episode reward: -95.854, mean reward: -1.198 [-100.000, 17.223], mean action: 1.387 [0.000, 3.000],  loss: 19.323338, mse: 2950.118031, mean_q: 0.271082, mean_eps: 0.962335
   6384/150000: episode: 67, duration: 0.460s, episode steps:  66, steps per second: 144, episode reward: -145.102, mean reward: -2.199 [-100.000,  8.554], mean action: 1.500 [0.000, 3.000],  loss: 13.124346, mse: 2985.907959, mean_q: 8.881460, mean_eps: 0.961897
   6473/150000: episode: 68, duration: 0.603s, episode steps:  89, steps per second: 148, episode reward: -145.143, mean reward: -1.631 [-100.000, 14.219], mean action: 1.618 [0.000, 3.000],  loss: 11.966719, mse: 3285.371607, mean_q: 13.455450, mean_eps: 0.961432
   6587/150000: episode: 69, duration: 0.824s, episode steps: 114, steps per second: 138, episode reward: -187.993, mean reward: -1.649 [-100.000,  7.605], mean action: 1.570 [0.000, 3.000],  loss: 20.541762, mse: 3656.525513, mean_q: 20.785583, mean_eps: 0.960823
   6693/150000: episode: 70, duration: 0.720s, episode steps: 106, steps per second: 147, episode reward: -325.554, mean reward: -3.071 [-100.000,  0.672], mean action: 1.538 [0.000, 3.000],  loss: 13.636189, mse: 3962.961617, mean_q: 27.407888, mean_eps: 0.960163
   6784/150000: episode: 71, duration: 0.636s, episode steps:  91, steps per second: 143, episode reward: -122.275, mean reward: -1.344 [-100.000,  8.963], mean action: 1.637 [0.000, 3.000],  loss: 9.277317, mse: 4721.231370, mean_q: 25.943356, mean_eps: 0.959572
   6857/150000: episode: 72, duration: 0.540s, episode steps:  73, steps per second: 135, episode reward: -190.382, mean reward: -2.608 [-100.000, 76.187], mean action: 1.438 [0.000, 3.000],  loss: 12.674423, mse: 5114.695774, mean_q: 22.876407, mean_eps: 0.959080
   6918/150000: episode: 73, duration: 0.426s, episode steps:  61, steps per second: 143, episode reward: -113.434, mean reward: -1.860 [-100.000, 10.051], mean action: 1.361 [0.000, 3.000],  loss: 17.785804, mse: 5030.818896, mean_q: 21.722956, mean_eps: 0.958678
   7001/150000: episode: 74, duration: 0.578s, episode steps:  83, steps per second: 144, episode reward: -99.563, mean reward: -1.200 [-100.000,  5.666], mean action: 1.627 [0.000, 3.000],  loss: 12.183230, mse: 4753.631212, mean_q: 26.343970, mean_eps: 0.958246
   7100/150000: episode: 75, duration: 0.696s, episode steps:  99, steps per second: 142, episode reward: -198.325, mean reward: -2.003 [-100.000, 19.142], mean action: 1.576 [0.000, 3.000],  loss: 11.950924, mse: 5405.615323, mean_q: 31.165680, mean_eps: 0.957700
   7181/150000: episode: 76, duration: 0.599s, episode steps:  81, steps per second: 135, episode reward: -103.630, mean reward: -1.279 [-100.000, 14.797], mean action: 1.309 [0.000, 3.000],  loss: 17.318178, mse: 5831.902931, mean_q: 34.596378, mean_eps: 0.957160
   7258/150000: episode: 77, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -272.144, mean reward: -3.534 [-100.000,  4.882], mean action: 1.390 [0.000, 3.000],  loss: 16.111181, mse: 6082.416200, mean_q: 37.204281, mean_eps: 0.956686
   7354/150000: episode: 78, duration: 0.752s, episode steps:  96, steps per second: 128, episode reward: 16.920, mean reward:  0.176 [-100.000, 100.793], mean action: 1.510 [0.000, 3.000],  loss: 13.660581, mse: 6872.505625, mean_q: 45.110639, mean_eps: 0.956167
   7442/150000: episode: 79, duration: 0.755s, episode steps:  88, steps per second: 117, episode reward: -241.231, mean reward: -2.741 [-100.000,  7.804], mean action: 1.727 [0.000, 3.000],  loss: 14.372218, mse: 7180.821428, mean_q: 42.758850, mean_eps: 0.955615
   7520/150000: episode: 80, duration: 0.609s, episode steps:  78, steps per second: 128, episode reward: -97.923, mean reward: -1.255 [-100.000, 10.145], mean action: 1.590 [0.000, 3.000],  loss: 12.592402, mse: 7673.022085, mean_q: 45.567804, mean_eps: 0.955117
   7595/150000: episode: 81, duration: 0.593s, episode steps:  75, steps per second: 126, episode reward: -138.287, mean reward: -1.844 [-100.000, 21.350], mean action: 1.520 [0.000, 3.000],  loss: 12.692312, mse: 8412.128848, mean_q: 51.389770, mean_eps: 0.954658
   7666/150000: episode: 82, duration: 0.570s, episode steps:  71, steps per second: 125, episode reward: -38.816, mean reward: -0.547 [-100.000, 16.024], mean action: 1.634 [0.000, 3.000],  loss: 16.366727, mse: 8879.850620, mean_q: 53.927499, mean_eps: 0.954220
   7726/150000: episode: 83, duration: 0.539s, episode steps:  60, steps per second: 111, episode reward: -117.495, mean reward: -1.958 [-100.000,  7.366], mean action: 1.500 [0.000, 3.000],  loss: 12.717511, mse: 9080.277865, mean_q: 58.219178, mean_eps: 0.953827
   7809/150000: episode: 84, duration: 0.659s, episode steps:  83, steps per second: 126, episode reward: -118.364, mean reward: -1.426 [-100.000,  9.364], mean action: 1.494 [0.000, 3.000],  loss: 11.071567, mse: 9672.648020, mean_q: 57.432373, mean_eps: 0.953398
   7902/150000: episode: 85, duration: 0.737s, episode steps:  93, steps per second: 126, episode reward: -380.614, mean reward: -4.093 [-100.000,  1.245], mean action: 1.656 [0.000, 3.000],  loss: 11.579162, mse: 10989.544770, mean_q: 63.249803, mean_eps: 0.952870
   8020/150000: episode: 86, duration: 1.128s, episode steps: 118, steps per second: 105, episode reward: -167.088, mean reward: -1.416 [-100.000, 11.203], mean action: 1.364 [0.000, 3.000],  loss: 14.777216, mse: 12701.621061, mean_q: 72.004491, mean_eps: 0.952237
   8101/150000: episode: 87, duration: 0.690s, episode steps:  81, steps per second: 117, episode reward: -335.216, mean reward: -4.138 [-100.000,  6.603], mean action: 1.383 [0.000, 3.000],  loss: 14.619380, mse: 13768.487570, mean_q: 74.838619, mean_eps: 0.951640
   8159/150000: episode: 88, duration: 0.433s, episode steps:  58, steps per second: 134, episode reward: -90.058, mean reward: -1.553 [-100.000,  9.108], mean action: 1.207 [0.000, 3.000],  loss: 9.389914, mse: 14723.348768, mean_q: 75.981781, mean_eps: 0.951223
   8287/150000: episode: 89, duration: 0.951s, episode steps: 128, steps per second: 135, episode reward: -64.293, mean reward: -0.502 [-100.000,  7.074], mean action: 1.508 [0.000, 3.000],  loss: 18.661187, mse: 15422.963928, mean_q: 74.146734, mean_eps: 0.950665
   8379/150000: episode: 90, duration: 0.653s, episode steps:  92, steps per second: 141, episode reward: -149.104, mean reward: -1.621 [-100.000, 20.010], mean action: 1.413 [0.000, 3.000],  loss: 15.130506, mse: 17372.767599, mean_q: 88.151136, mean_eps: 0.950005
   8451/150000: episode: 91, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -13.530, mean reward: -0.188 [-100.000, 81.625], mean action: 1.514 [0.000, 3.000],  loss: 16.288494, mse: 19398.149875, mean_q: 95.701600, mean_eps: 0.949513
   8561/150000: episode: 92, duration: 0.799s, episode steps: 110, steps per second: 138, episode reward: -294.145, mean reward: -2.674 [-100.000,  4.772], mean action: 1.464 [0.000, 3.000],  loss: 15.740143, mse: 20874.451127, mean_q: 97.690613, mean_eps: 0.948967
   8631/150000: episode: 93, duration: 0.498s, episode steps:  70, steps per second: 141, episode reward: -114.651, mean reward: -1.638 [-100.000, 15.299], mean action: 1.500 [0.000, 3.000],  loss: 12.415270, mse: 23008.137779, mean_q: 104.605754, mean_eps: 0.948427
   8746/150000: episode: 94, duration: 0.813s, episode steps: 115, steps per second: 141, episode reward: -153.589, mean reward: -1.336 [-100.000,  7.788], mean action: 1.565 [0.000, 3.000],  loss: 12.951186, mse: 26978.042527, mean_q: 114.519401, mean_eps: 0.947872
   8825/150000: episode: 95, duration: 0.602s, episode steps:  79, steps per second: 131, episode reward: -272.925, mean reward: -3.455 [-100.000,  1.739], mean action: 1.671 [0.000, 3.000],  loss: 11.602525, mse: 29732.883060, mean_q: 121.759577, mean_eps: 0.947290
   8921/150000: episode: 96, duration: 0.683s, episode steps:  96, steps per second: 141, episode reward: -117.061, mean reward: -1.219 [-100.000,  6.870], mean action: 1.521 [0.000, 3.000],  loss: 14.163716, mse: 32089.579000, mean_q: 126.135154, mean_eps: 0.946765
   9043/150000: episode: 97, duration: 0.830s, episode steps: 122, steps per second: 147, episode reward: -82.735, mean reward: -0.678 [-100.000, 19.904], mean action: 1.664 [0.000, 3.000],  loss: 14.102922, mse: 39083.202517, mean_q: 140.748980, mean_eps: 0.946111
   9159/150000: episode: 98, duration: 0.846s, episode steps: 116, steps per second: 137, episode reward: -140.265, mean reward: -1.209 [-100.000, 10.685], mean action: 1.716 [0.000, 3.000],  loss: 14.151812, mse: 47590.820548, mean_q: 159.708427, mean_eps: 0.945397
   9244/150000: episode: 99, duration: 0.651s, episode steps:  85, steps per second: 131, episode reward: -201.518, mean reward: -2.371 [-100.000,  7.208], mean action: 1.471 [0.000, 3.000],  loss: 13.702229, mse: 55970.826930, mean_q: 174.042274, mean_eps: 0.944794
   9347/150000: episode: 100, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -31.273, mean reward: -0.304 [-100.000, 101.451], mean action: 1.699 [0.000, 3.000],  loss: 17.233248, mse: 68415.573422, mean_q: 189.603875, mean_eps: 0.944230
   9465/150000: episode: 101, duration: 0.872s, episode steps: 118, steps per second: 135, episode reward: -266.538, mean reward: -2.259 [-100.000,  9.401], mean action: 1.729 [0.000, 3.000],  loss: 21.978320, mse: 84196.867585, mean_q: 210.537401, mean_eps: 0.943567
   9622/150000: episode: 102, duration: 1.101s, episode steps: 157, steps per second: 143, episode reward:  0.932, mean reward:  0.006 [-100.000, 93.276], mean action: 1.471 [0.000, 3.000],  loss: 24.819774, mse: 114969.390127, mean_q: 251.584096, mean_eps: 0.942742
   9710/150000: episode: 103, duration: 0.705s, episode steps:  88, steps per second: 125, episode reward: -178.326, mean reward: -2.026 [-100.000, 28.481], mean action: 1.443 [0.000, 3.000],  loss: 31.357704, mse: 171987.347301, mean_q: 316.237349, mean_eps: 0.942007
   9789/150000: episode: 104, duration: 0.606s, episode steps:  79, steps per second: 130, episode reward: -139.778, mean reward: -1.769 [-100.000,  5.456], mean action: 1.519 [0.000, 3.000],  loss: 40.553250, mse: 228754.230024, mean_q: 361.741152, mean_eps: 0.941506
   9906/150000: episode: 105, duration: 0.911s, episode steps: 117, steps per second: 128, episode reward: -100.634, mean reward: -0.860 [-100.000, 13.815], mean action: 1.573 [0.000, 3.000],  loss: 61.924121, mse: 325650.883814, mean_q: 429.293780, mean_eps: 0.940918
   9994/150000: episode: 106, duration: 0.824s, episode steps:  88, steps per second: 107, episode reward: -378.417, mean reward: -4.300 [-100.000,  1.002], mean action: 1.534 [0.000, 3.000],  loss: 102.452799, mse: 501868.754616, mean_q: 533.045983, mean_eps: 0.940303
  10110/150000: episode: 107, duration: 0.839s, episode steps: 116, steps per second: 138, episode reward: -260.696, mean reward: -2.247 [-100.000, 16.519], mean action: 1.397 [0.000, 3.000],  loss: 172.388850, mse: 891852.140356, mean_q: 724.898061, mean_eps: 0.939691
  10229/150000: episode: 108, duration: 0.812s, episode steps: 119, steps per second: 147, episode reward: -103.880, mean reward: -0.873 [-100.000,  7.642], mean action: 1.555 [0.000, 3.000],  loss: 294.337989, mse: 1759144.662815, mean_q: 1028.082788, mean_eps: 0.938986
  10305/150000: episode: 109, duration: 0.580s, episode steps:  76, steps per second: 131, episode reward: -156.054, mean reward: -2.053 [-100.000, 50.004], mean action: 1.618 [0.000, 3.000],  loss: 479.947475, mse: 3063220.389803, mean_q: 1380.711397, mean_eps: 0.938401
  10426/150000: episode: 110, duration: 0.836s, episode steps: 121, steps per second: 145, episode reward: -353.940, mean reward: -2.925 [-100.000, 76.033], mean action: 1.322 [0.000, 3.000],  loss: 986.219873, mse: 6096206.510331, mean_q: 2001.066980, mean_eps: 0.937810
  10496/150000: episode: 111, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -243.336, mean reward: -3.476 [-100.000,  7.118], mean action: 1.471 [0.000, 3.000],  loss: 1886.247057, mse: 11734740.700000, mean_q: 2860.188009, mean_eps: 0.937237
  10588/150000: episode: 112, duration: 0.651s, episode steps:  92, steps per second: 141, episode reward: -121.831, mean reward: -1.324 [-100.000, 35.214], mean action: 1.446 [0.000, 3.000],  loss: 2994.019493, mse: 21527226.456522, mean_q: 3928.097003, mean_eps: 0.936751
  10659/150000: episode: 113, duration: 0.482s, episode steps:  71, steps per second: 147, episode reward: -158.077, mean reward: -2.226 [-100.000, 26.844], mean action: 1.366 [0.000, 3.000],  loss: 5421.842772, mse: 34246927.154930, mean_q: 4995.749574, mean_eps: 0.936262
  10725/150000: episode: 114, duration: 0.431s, episode steps:  66, steps per second: 153, episode reward: -145.260, mean reward: -2.201 [-100.000,  6.934], mean action: 1.455 [0.000, 3.000],  loss: 9441.075872, mse: 56429253.121212, mean_q: 6636.128196, mean_eps: 0.935851
  10797/150000: episode: 115, duration: 0.485s, episode steps:  72, steps per second: 149, episode reward: -127.078, mean reward: -1.765 [-100.000, 12.397], mean action: 1.639 [0.000, 3.000],  loss: 13763.716983, mse: 89607809.666667, mean_q: 8367.794556, mean_eps: 0.935437
  10938/150000: episode: 116, duration: 0.988s, episode steps: 141, steps per second: 143, episode reward: -248.745, mean reward: -1.764 [-100.000, 89.255], mean action: 1.617 [0.000, 3.000],  loss: 31761.266172, mse: 173376938.609929, mean_q: 11708.013225, mean_eps: 0.934798
  11040/150000: episode: 117, duration: 0.680s, episode steps: 102, steps per second: 150, episode reward: -137.809, mean reward: -1.351 [-100.000, 11.293], mean action: 1.559 [0.000, 3.000],  loss: 87671.304400, mse: 354833834.352941, mean_q: 17196.155369, mean_eps: 0.934069
  11126/150000: episode: 118, duration: 0.554s, episode steps:  86, steps per second: 155, episode reward: -118.842, mean reward: -1.382 [-100.000,  8.373], mean action: 1.593 [0.000, 3.000],  loss: 191119.251499, mse: 614964142.883721, mean_q: 22929.545104, mean_eps: 0.933505
  11194/150000: episode: 119, duration: 0.474s, episode steps:  68, steps per second: 144, episode reward: -65.380, mean reward: -0.961 [-100.000,  5.953], mean action: 1.632 [0.000, 3.000],  loss: 228023.779929, mse: 829784960.941176, mean_q: 26199.829935, mean_eps: 0.933043
  11318/150000: episode: 120, duration: 0.848s, episode steps: 124, steps per second: 146, episode reward: -179.205, mean reward: -1.445 [-100.000,  7.832], mean action: 1.435 [0.000, 3.000],  loss: 438706.431562, mse: 1360054375.741935, mean_q: 33794.358493, mean_eps: 0.932467
  11433/150000: episode: 121, duration: 0.756s, episode steps: 115, steps per second: 152, episode reward: -208.478, mean reward: -1.813 [-100.000,  1.185], mean action: 1.435 [0.000, 3.000],  loss: 715984.473234, mse: 2392203437.634783, mean_q: 45179.481386, mean_eps: 0.931750
  11525/150000: episode: 122, duration: 0.683s, episode steps:  92, steps per second: 135, episode reward: -176.694, mean reward: -1.921 [-100.000, 10.938], mean action: 1.467 [0.000, 3.000],  loss: 1243397.564538, mse: 3731177132.521739, mean_q: 56481.824304, mean_eps: 0.931129
  11627/150000: episode: 123, duration: 0.703s, episode steps: 102, steps per second: 145, episode reward: -131.626, mean reward: -1.290 [-100.000, 10.221], mean action: 1.441 [0.000, 3.000],  loss: 1351225.450061, mse: 5444280558.431373, mean_q: 67316.709329, mean_eps: 0.930547
  11771/150000: episode: 124, duration: 0.939s, episode steps: 144, steps per second: 153, episode reward: -525.743, mean reward: -3.651 [-100.000,  2.267], mean action: 1.750 [0.000, 3.000],  loss: 2809901.879449, mse: 8985175736.888889, mean_q: 87055.056722, mean_eps: 0.929809
  11856/150000: episode: 125, duration: 0.592s, episode steps:  85, steps per second: 144, episode reward: -129.948, mean reward: -1.529 [-100.000, 11.945], mean action: 1.635 [0.000, 3.000],  loss: 5043213.680882, mse: 13636967568.564707, mean_q: 108172.907261, mean_eps: 0.929122
  11957/150000: episode: 126, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -407.266, mean reward: -4.032 [-100.000,  0.571], mean action: 1.653 [0.000, 3.000],  loss: 4884150.739480, mse: 19078164632.079208, mean_q: 127589.315903, mean_eps: 0.928564
  12052/150000: episode: 127, duration: 0.627s, episode steps:  95, steps per second: 152, episode reward: -95.076, mean reward: -1.001 [-100.000, 16.596], mean action: 1.579 [0.000, 3.000],  loss: 9082246.296053, mse: 28078038781.305264, mean_q: 155884.489638, mean_eps: 0.927976
  12162/150000: episode: 128, duration: 0.759s, episode steps: 110, steps per second: 145, episode reward: -239.315, mean reward: -2.176 [-100.000,  6.961], mean action: 1.564 [0.000, 3.000],  loss: 13288778.986364, mse: 41175009540.654549, mean_q: 189489.585938, mean_eps: 0.927361
  12273/150000: episode: 129, duration: 0.784s, episode steps: 111, steps per second: 142, episode reward: -250.174, mean reward: -2.254 [-100.000,  0.811], mean action: 1.568 [0.000, 3.000],  loss: 14446170.752252, mse: 58991450278.054054, mean_q: 226278.740569, mean_eps: 0.926698
  12400/150000: episode: 130, duration: 0.849s, episode steps: 127, steps per second: 150, episode reward: -255.614, mean reward: -2.013 [-100.000,  6.031], mean action: 1.528 [0.000, 3.000],  loss: 22699766.874016, mse: 84592358891.842514, mean_q: 269662.282849, mean_eps: 0.925984
  12503/150000: episode: 131, duration: 0.731s, episode steps: 103, steps per second: 141, episode reward: -115.121, mean reward: -1.118 [-100.000,  5.326], mean action: 1.398 [0.000, 3.000],  loss: 39204651.876214, mse: 118569176968.699036, mean_q: 320387.643204, mean_eps: 0.925294
  12602/150000: episode: 132, duration: 0.684s, episode steps:  99, steps per second: 145, episode reward: -324.751, mean reward: -3.280 [-100.000,  0.880], mean action: 1.515 [0.000, 3.000],  loss: 57948556.782828, mse: 165085004768.969696, mean_q: 378934.050505, mean_eps: 0.924688
  12677/150000: episode: 133, duration: 0.498s, episode steps:  75, steps per second: 151, episode reward: -202.246, mean reward: -2.697 [-100.000,  5.742], mean action: 1.693 [0.000, 3.000],  loss: 59988368.433333, mse: 207819834108.586670, mean_q: 422627.688333, mean_eps: 0.924166
  12770/150000: episode: 134, duration: 0.621s, episode steps:  93, steps per second: 150, episode reward: -268.839, mean reward: -2.891 [-100.000, 44.610], mean action: 1.742 [0.000, 3.000],  loss: 68551019.193548, mse: 256801908361.634399, mean_q: 470809.929099, mean_eps: 0.923662
  12890/150000: episode: 135, duration: 0.843s, episode steps: 120, steps per second: 142, episode reward: -79.007, mean reward: -0.658 [-100.000,  8.457], mean action: 1.767 [0.000, 3.000],  loss: 68052377.475000, mse: 339989201988.266663, mean_q: 539301.500521, mean_eps: 0.923023
  12982/150000: episode: 136, duration: 0.633s, episode steps:  92, steps per second: 145, episode reward: -106.294, mean reward: -1.155 [-100.000,  8.527], mean action: 1.652 [0.000, 3.000],  loss: 116550045.304348, mse: 438442724129.391296, mean_q: 617976.872283, mean_eps: 0.922387
  13072/150000: episode: 137, duration: 0.639s, episode steps:  90, steps per second: 141, episode reward: -148.680, mean reward: -1.652 [-100.000, 12.612], mean action: 1.411 [0.000, 3.000],  loss: 244685014.533333, mse: 566903751748.266724, mean_q: 703272.940278, mean_eps: 0.921841
  13188/150000: episode: 138, duration: 0.921s, episode steps: 116, steps per second: 126, episode reward: -154.460, mean reward: -1.332 [-100.000, 49.112], mean action: 1.517 [0.000, 3.000],  loss: 162020122.793103, mse: 716661247858.758667, mean_q: 792373.858297, mean_eps: 0.921223
  13308/150000: episode: 139, duration: 0.885s, episode steps: 120, steps per second: 136, episode reward: -559.255, mean reward: -4.660 [-100.000,  2.310], mean action: 1.425 [0.000, 3.000],  loss: 324329540.583333, mse: 950724863590.400024, mean_q: 912246.583854, mean_eps: 0.920515
  13415/150000: episode: 140, duration: 0.799s, episode steps: 107, steps per second: 134, episode reward: -482.687, mean reward: -4.511 [-100.000,  1.582], mean action: 1.598 [0.000, 3.000],  loss: 368304866.467290, mse: 1195875610279.476562, mean_q: 1024071.147780, mean_eps: 0.919834
  13498/150000: episode: 141, duration: 0.601s, episode steps:  83, steps per second: 138, episode reward: -106.869, mean reward: -1.288 [-100.000, 14.825], mean action: 1.434 [0.000, 3.000],  loss: 363090144.289157, mse: 1420608448931.469971, mean_q: 1114720.578313, mean_eps: 0.919264
  13577/150000: episode: 142, duration: 0.581s, episode steps:  79, steps per second: 136, episode reward: -186.390, mean reward: -2.359 [-100.000,  6.287], mean action: 1.557 [0.000, 3.000],  loss: 512479093.367089, mse: 1653887450928.607666, mean_q: 1202096.772152, mean_eps: 0.918778
  13660/150000: episode: 143, duration: 0.603s, episode steps:  83, steps per second: 138, episode reward: -343.081, mean reward: -4.134 [-100.000,  4.063], mean action: 1.422 [0.000, 3.000],  loss: 438459267.180723, mse: 2024170096565.975830, mean_q: 1333905.808735, mean_eps: 0.918292
  13733/150000: episode: 144, duration: 0.543s, episode steps:  73, steps per second: 134, episode reward: -93.985, mean reward: -1.287 [-100.000,  7.065], mean action: 1.397 [0.000, 3.000],  loss: 648099355.068493, mse: 2439171578445.150879, mean_q: 1475283.303082, mean_eps: 0.917824
  13851/150000: episode: 145, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -171.663, mean reward: -1.455 [-100.000,  3.523], mean action: 1.500 [0.000, 3.000],  loss: 690262005.627119, mse: 2697734066523.118652, mean_q: 1529524.639831, mean_eps: 0.917251
  13917/150000: episode: 146, duration: 0.456s, episode steps:  66, steps per second: 145, episode reward: -193.935, mean reward: -2.938 [-100.000,  5.606], mean action: 1.545 [0.000, 3.000],  loss: 930568453.090909, mse: 3089178494913.939453, mean_q: 1627609.935606, mean_eps: 0.916699
  13996/150000: episode: 147, duration: 0.558s, episode steps:  79, steps per second: 142, episode reward: -68.099, mean reward: -0.862 [-100.000, 10.234], mean action: 1.671 [0.000, 3.000],  loss: 1489991519.797468, mse: 3486053262452.658203, mean_q: 1730023.469937, mean_eps: 0.916264
  14086/150000: episode: 148, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -285.013, mean reward: -3.167 [-100.000,  3.154], mean action: 1.567 [0.000, 3.000],  loss: 951780830.933333, mse: 4082806944927.289062, mean_q: 1888948.376389, mean_eps: 0.915757
  14193/150000: episode: 149, duration: 0.752s, episode steps: 107, steps per second: 142, episode reward: -95.132, mean reward: -0.889 [-100.000, 21.183], mean action: 1.514 [0.000, 3.000],  loss: 1444042499.738318, mse: 4722925353017.420898, mean_q: 2013666.477804, mean_eps: 0.915166
  14317/150000: episode: 150, duration: 0.870s, episode steps: 124, steps per second: 143, episode reward: -191.080, mean reward: -1.541 [-100.000,  5.936], mean action: 1.540 [0.000, 3.000],  loss: 1121557984.903226, mse: 5754243423463.225586, mean_q: 2227651.841734, mean_eps: 0.914473
  14452/150000: episode: 151, duration: 0.918s, episode steps: 135, steps per second: 147, episode reward: -229.430, mean reward: -1.699 [-100.000, 15.299], mean action: 1.615 [0.000, 3.000],  loss: 2242543807.051852, mse: 7012938041359.169922, mean_q: 2459062.515741, mean_eps: 0.913696
  14543/150000: episode: 152, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -421.411, mean reward: -4.631 [-100.000,  0.671], mean action: 1.681 [0.000, 3.000],  loss: 1195146863.824176, mse: 8603152001946.725586, mean_q: 2738059.829670, mean_eps: 0.913018
  14675/150000: episode: 153, duration: 0.934s, episode steps: 132, steps per second: 141, episode reward: -74.143, mean reward: -0.562 [-100.000, 12.358], mean action: 1.591 [0.000, 3.000],  loss: 2443415778.909091, mse: 10120214804542.060547, mean_q: 2980047.043561, mean_eps: 0.912349
  14737/150000: episode: 154, duration: 0.409s, episode steps:  62, steps per second: 151, episode reward: -111.706, mean reward: -1.802 [-100.000, 16.130], mean action: 1.629 [0.000, 3.000],  loss: 2889956139.870968, mse: 11599693014841.806641, mean_q: 3195065.729839, mean_eps: 0.911767
  14884/150000: episode: 155, duration: 1.011s, episode steps: 147, steps per second: 145, episode reward: -293.450, mean reward: -1.996 [-100.000, 84.074], mean action: 1.612 [0.000, 3.000],  loss: 6546211327.564626, mse: 13508613774301.169922, mean_q: 3435262.568027, mean_eps: 0.911140
  14949/150000: episode: 156, duration: 0.491s, episode steps:  65, steps per second: 132, episode reward: -57.497, mean reward: -0.885 [-100.000,  7.713], mean action: 1.492 [0.000, 3.000],  loss: 4441735738.092308, mse: 16399905462177.476562, mean_q: 3807776.476923, mean_eps: 0.910504
  15018/150000: episode: 157, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -67.342, mean reward: -0.976 [-100.000, 22.051], mean action: 1.551 [0.000, 3.000],  loss: 4251453966.840580, mse: 17154527852692.406250, mean_q: 3845991.865942, mean_eps: 0.910102
  15163/150000: episode: 158, duration: 0.986s, episode steps: 145, steps per second: 147, episode reward: -312.886, mean reward: -2.158 [-100.000,  4.215], mean action: 1.462 [0.000, 3.000],  loss: 7518480572.910345, mse: 19930214486100.746094, mean_q: 4164532.815517, mean_eps: 0.909460
  15334/150000: episode: 159, duration: 1.197s, episode steps: 171, steps per second: 143, episode reward: -97.262, mean reward: -0.569 [-100.000, 10.622], mean action: 1.532 [0.000, 3.000],  loss: 7782137661.380117, mse: 25708601129762.433594, mean_q: 4767712.595029, mean_eps: 0.908512
  15422/150000: episode: 160, duration: 0.584s, episode steps:  88, steps per second: 151, episode reward: -49.130, mean reward: -0.558 [-100.000, 15.156], mean action: 1.648 [0.000, 3.000],  loss: 13060603095.272728, mse: 29784457251746.910156, mean_q: 5106229.417614, mean_eps: 0.907735
  15518/150000: episode: 161, duration: 0.654s, episode steps:  96, steps per second: 147, episode reward: -219.913, mean reward: -2.291 [-100.000,  3.547], mean action: 1.542 [0.000, 3.000],  loss: 9009306409.333334, mse: 33291687428096.000000, mean_q: 5424447.208333, mean_eps: 0.907183
  15612/150000: episode: 162, duration: 0.686s, episode steps:  94, steps per second: 137, episode reward: -399.707, mean reward: -4.252 [-100.000, -0.110], mean action: 1.564 [0.000, 3.000],  loss: 8908450215.489361, mse: 37970025359577.875000, mean_q: 5819550.398936, mean_eps: 0.906613
  15744/150000: episode: 163, duration: 0.886s, episode steps: 132, steps per second: 149, episode reward: -462.501, mean reward: -3.504 [-100.000,  1.651], mean action: 1.500 [0.000, 3.000],  loss: 16185189028.848484, mse: 42721699764037.820312, mean_q: 6117642.132576, mean_eps: 0.905935
  15812/150000: episode: 164, duration: 0.461s, episode steps:  68, steps per second: 147, episode reward: -134.994, mean reward: -1.985 [-100.000,  6.227], mean action: 1.515 [0.000, 3.000],  loss: 17093262411.294117, mse: 49310493927785.414062, mean_q: 6614087.676471, mean_eps: 0.905335
  15878/150000: episode: 165, duration: 0.501s, episode steps:  66, steps per second: 132, episode reward: -57.410, mean reward: -0.870 [-100.000, 12.257], mean action: 1.348 [0.000, 3.000],  loss: 10991525279.030304, mse: 52057819276505.210938, mean_q: 6805202.515152, mean_eps: 0.904933
  15959/150000: episode: 166, duration: 0.614s, episode steps:  81, steps per second: 132, episode reward: -368.387, mean reward: -4.548 [-100.000, 11.364], mean action: 1.543 [0.000, 3.000],  loss: 11332081961.086420, mse: 55022876360704.000000, mean_q: 6921798.308642, mean_eps: 0.904492
  16077/150000: episode: 167, duration: 0.965s, episode steps: 118, steps per second: 122, episode reward: -302.849, mean reward: -2.567 [-100.000, 27.461], mean action: 1.576 [0.000, 3.000],  loss: 16110588175.186441, mse: 60982051849025.085938, mean_q: 7312671.546610, mean_eps: 0.903895
  16168/150000: episode: 168, duration: 0.792s, episode steps:  91, steps per second: 115, episode reward: -489.919, mean reward: -5.384 [-100.000,  0.102], mean action: 1.725 [0.000, 3.000],  loss: 23096525933.714287, mse: 69207499539152.179688, mean_q: 7758114.175824, mean_eps: 0.903268
  16280/150000: episode: 169, duration: 0.893s, episode steps: 112, steps per second: 125, episode reward: -98.857, mean reward: -0.883 [-100.000,  8.391], mean action: 1.625 [0.000, 3.000],  loss: 40043630043.428574, mse: 77797583599323.421875, mean_q: 8297667.187500, mean_eps: 0.902659
  16371/150000: episode: 170, duration: 0.616s, episode steps:  91, steps per second: 148, episode reward: -5.160, mean reward: -0.057 [-100.000, 95.653], mean action: 1.484 [0.000, 3.000],  loss: 37855216184.263733, mse: 83702708808602.718750, mean_q: 8542849.318681, mean_eps: 0.902050
  16495/150000: episode: 171, duration: 0.912s, episode steps: 124, steps per second: 136, episode reward: -325.876, mean reward: -2.628 [-100.000,  1.652], mean action: 1.516 [0.000, 3.000],  loss: 27178126856.258064, mse: 93946539028876.390625, mean_q: 9096733.822581, mean_eps: 0.901405
  16576/150000: episode: 172, duration: 0.677s, episode steps:  81, steps per second: 120, episode reward: -104.873, mean reward: -1.295 [-100.000,  5.901], mean action: 1.679 [0.000, 3.000],  loss: 30423895943.901234, mse: 103875949246691.562500, mean_q: 9566011.450617, mean_eps: 0.900790
  16650/150000: episode: 173, duration: 0.591s, episode steps:  74, steps per second: 125, episode reward: -175.686, mean reward: -2.374 [-100.000,  6.320], mean action: 1.378 [0.000, 3.000],  loss: 38755534647.351349, mse: 110619476658784.859375, mean_q: 9874102.972973, mean_eps: 0.900325
  16746/150000: episode: 174, duration: 0.774s, episode steps:  96, steps per second: 124, episode reward: -221.161, mean reward: -2.304 [-100.000,  8.904], mean action: 1.375 [0.000, 3.000],  loss: 23723722730.666668, mse: 121468155680085.328125, mean_q: 10343353.567708, mean_eps: 0.899815
  16870/150000: episode: 175, duration: 0.968s, episode steps: 124, steps per second: 128, episode reward: -164.225, mean reward: -1.324 [-100.000, 20.312], mean action: 1.645 [0.000, 3.000],  loss: 76469984000.000000, mse: 137968693052449.031250, mean_q: 11112708.612903, mean_eps: 0.899155
  16947/150000: episode: 176, duration: 0.595s, episode steps:  77, steps per second: 129, episode reward: -84.958, mean reward: -1.103 [-100.000, 11.747], mean action: 1.351 [0.000, 3.000],  loss: 48018693279.584419, mse: 150156411336198.656250, mean_q: 11518305.688312, mean_eps: 0.898552
  17065/150000: episode: 177, duration: 0.911s, episode steps: 118, steps per second: 130, episode reward: -158.459, mean reward: -1.343 [-100.000,  6.631], mean action: 1.525 [0.000, 3.000],  loss: 61395616551.050850, mse: 161715989809880.937500, mean_q: 11880115.118644, mean_eps: 0.897967
  17175/150000: episode: 178, duration: 0.762s, episode steps: 110, steps per second: 144, episode reward: -105.613, mean reward: -0.960 [-100.000,  7.825], mean action: 1.655 [0.000, 3.000],  loss: 66360184692.363640, mse: 175267848340945.468750, mean_q: 12365347.563636, mean_eps: 0.897283
  17268/150000: episode: 179, duration: 0.690s, episode steps:  93, steps per second: 135, episode reward: -199.869, mean reward: -2.149 [-100.000, 14.379], mean action: 1.387 [0.000, 3.000],  loss: 111165854048.344086, mse: 191064673976672.343750, mean_q: 12883830.591398, mean_eps: 0.896674
  17349/150000: episode: 180, duration: 0.577s, episode steps:  81, steps per second: 140, episode reward: -211.059, mean reward: -2.606 [-100.000, 71.203], mean action: 1.765 [0.000, 3.000],  loss: 61949003156.543213, mse: 202096819384168.281250, mean_q: 13279990.654321, mean_eps: 0.896152
  17474/150000: episode: 181, duration: 0.848s, episode steps: 125, steps per second: 147, episode reward: -391.672, mean reward: -3.133 [-100.000,  1.903], mean action: 1.656 [0.000, 3.000],  loss: 99052985171.968002, mse: 224833985379827.718750, mean_q: 14086085.160000, mean_eps: 0.895534
  17545/150000: episode: 182, duration: 0.504s, episode steps:  71, steps per second: 141, episode reward: -53.738, mean reward: -0.757 [-100.000, 10.247], mean action: 1.563 [0.000, 3.000],  loss: 93323515067.492950, mse: 240160547330134.531250, mean_q: 14460725.154930, mean_eps: 0.894946
  17637/150000: episode: 183, duration: 0.653s, episode steps:  92, steps per second: 141, episode reward: -123.462, mean reward: -1.342 [-100.000,  7.467], mean action: 1.630 [0.000, 3.000],  loss: 75702797901.913040, mse: 262780335913583.312500, mean_q: 15303225.010870, mean_eps: 0.894457
  17738/150000: episode: 184, duration: 0.703s, episode steps: 101, steps per second: 144, episode reward: -228.354, mean reward: -2.261 [-100.000, 28.386], mean action: 1.475 [0.000, 3.000],  loss: 95950917773.940598, mse: 279583568195462.343750, mean_q: 15559270.514851, mean_eps: 0.893878
  17849/150000: episode: 185, duration: 0.783s, episode steps: 111, steps per second: 142, episode reward: -294.522, mean reward: -2.653 [-100.000,  0.458], mean action: 1.432 [0.000, 3.000],  loss: 148631180832.288300, mse: 297382615095351.375000, mean_q: 16033543.270270, mean_eps: 0.893242
  17917/150000: episode: 186, duration: 0.503s, episode steps:  68, steps per second: 135, episode reward: -63.139, mean reward: -0.929 [-100.000, 11.202], mean action: 1.441 [0.000, 3.000],  loss: 161330809268.705872, mse: 325305852216259.750000, mean_q: 16948599.470588, mean_eps: 0.892705
  17988/150000: episode: 187, duration: 0.519s, episode steps:  71, steps per second: 137, episode reward: -63.463, mean reward: -0.894 [-100.000, 17.609], mean action: 1.732 [0.000, 3.000],  loss: 111274104731.042252, mse: 337543351751867.500000, mean_q: 17093838.549296, mean_eps: 0.892288
  18081/150000: episode: 188, duration: 0.775s, episode steps:  93, steps per second: 120, episode reward: -377.410, mean reward: -4.058 [-100.000,  0.656], mean action: 1.710 [0.000, 3.000],  loss: 223212433209.806458, mse: 359763152058478.125000, mean_q: 17676670.946237, mean_eps: 0.891796
  18170/150000: episode: 189, duration: 0.698s, episode steps:  89, steps per second: 127, episode reward: -76.187, mean reward: -0.856 [-100.000, 31.104], mean action: 1.427 [0.000, 3.000],  loss: 130973451689.707870, mse: 387299514023567.812500, mean_q: 18365058.179775, mean_eps: 0.891250
  18284/150000: episode: 190, duration: 0.803s, episode steps: 114, steps per second: 142, episode reward: -183.954, mean reward: -1.614 [-100.000,  2.838], mean action: 1.500 [0.000, 3.000],  loss: 205900187576.140350, mse: 411283398332919.000000, mean_q: 19006413.473684, mean_eps: 0.890641
  18368/150000: episode: 191, duration: 0.666s, episode steps:  84, steps per second: 126, episode reward: -84.550, mean reward: -1.007 [-100.000, 15.588], mean action: 1.619 [0.000, 3.000],  loss: 120361375207.619049, mse: 446347243658873.875000, mean_q: 19667840.809524, mean_eps: 0.890047
  18447/150000: episode: 192, duration: 0.642s, episode steps:  79, steps per second: 123, episode reward: -169.453, mean reward: -2.145 [-100.000, 30.299], mean action: 1.671 [0.000, 3.000],  loss: 131301654061.367096, mse: 475037466879507.437500, mean_q: 20380529.721519, mean_eps: 0.889558
  18514/150000: episode: 193, duration: 0.476s, episode steps:  67, steps per second: 141, episode reward: -93.613, mean reward: -1.397 [-100.000,  6.696], mean action: 1.522 [0.000, 3.000],  loss: 115875391763.104477, mse: 495113429010584.812500, mean_q: 20739756.388060, mean_eps: 0.889120
  18604/150000: episode: 194, duration: 0.666s, episode steps:  90, steps per second: 135, episode reward: -91.042, mean reward: -1.012 [-100.000, 18.449], mean action: 1.489 [0.000, 3.000],  loss: 246506855469.511108, mse: 518324760032051.187500, mean_q: 21307852.966667, mean_eps: 0.888649
  18732/150000: episode: 195, duration: 0.973s, episode steps: 128, steps per second: 132, episode reward: -359.891, mean reward: -2.812 [-100.000,  4.360], mean action: 1.320 [0.000, 3.000],  loss: 234537815872.000000, mse: 561169396662272.000000, mean_q: 22145358.992188, mean_eps: 0.887995
  18832/150000: episode: 196, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: -358.290, mean reward: -3.583 [-100.000, 65.589], mean action: 1.790 [0.000, 3.000],  loss: 177985781432.320007, mse: 618500602074234.875000, mean_q: 23392104.620000, mean_eps: 0.887311
  18935/150000: episode: 197, duration: 0.784s, episode steps: 103, steps per second: 131, episode reward: -200.215, mean reward: -1.944 [-100.000, 67.196], mean action: 1.767 [0.000, 3.000],  loss: 195127884829.825256, mse: 670967214980642.750000, mean_q: 24310955.456311, mean_eps: 0.886702
  19042/150000: episode: 198, duration: 0.786s, episode steps: 107, steps per second: 136, episode reward: -132.596, mean reward: -1.239 [-100.000,  9.533], mean action: 1.467 [0.000, 3.000],  loss: 291715777009.644836, mse: 704034609728196.125000, mean_q: 24695723.700935, mean_eps: 0.886072
  19111/150000: episode: 199, duration: 0.477s, episode steps:  69, steps per second: 145, episode reward: -188.857, mean reward: -2.737 [-100.000, 13.043], mean action: 1.377 [0.000, 3.000],  loss: 173009586621.217377, mse: 732471559808044.500000, mean_q: 25178888.521739, mean_eps: 0.885544
  19184/150000: episode: 200, duration: 0.505s, episode steps:  73, steps per second: 144, episode reward: -244.664, mean reward: -3.352 [-100.000, 11.309], mean action: 1.521 [0.000, 3.000],  loss: 299037214691.945190, mse: 781519376049586.875000, mean_q: 26253592.849315, mean_eps: 0.885118
  19269/150000: episode: 201, duration: 0.634s, episode steps:  85, steps per second: 134, episode reward: -477.764, mean reward: -5.621 [-100.000,  1.156], mean action: 1.482 [0.000, 3.000],  loss: 371599563498.917664, mse: 807061215054089.000000, mean_q: 26400374.494118, mean_eps: 0.884644
  19362/150000: episode: 202, duration: 0.728s, episode steps:  93, steps per second: 128, episode reward: -519.660, mean reward: -5.588 [-100.000,  1.009], mean action: 1.677 [0.000, 3.000],  loss: 257608675812.473114, mse: 884009319423383.375000, mean_q: 27720765.677419, mean_eps: 0.884110
  19453/150000: episode: 203, duration: 0.733s, episode steps:  91, steps per second: 124, episode reward: -432.631, mean reward: -4.754 [-100.000,  0.258], mean action: 1.571 [0.000, 3.000],  loss: 302869204766.945068, mse: 947293913076477.125000, mean_q: 28728302.131868, mean_eps: 0.883558
  19565/150000: episode: 204, duration: 1.045s, episode steps: 112, steps per second: 107, episode reward: -151.531, mean reward: -1.353 [-100.000,  8.481], mean action: 1.545 [0.000, 3.000],  loss: 180544898450.285706, mse: 990609883454317.750000, mean_q: 29491651.392857, mean_eps: 0.882949
  19674/150000: episode: 205, duration: 0.780s, episode steps: 109, steps per second: 140, episode reward: -246.597, mean reward: -2.262 [-100.000, 40.123], mean action: 1.523 [0.000, 3.000],  loss: 380588321049.834839, mse: 1060516150073062.125000, mean_q: 30338003.339450, mean_eps: 0.882286
  19764/150000: episode: 206, duration: 0.616s, episode steps:  90, steps per second: 146, episode reward: -521.713, mean reward: -5.797 [-100.000, 18.812], mean action: 1.544 [0.000, 3.000],  loss: 533708757856.711121, mse: 1148334661056739.500000, mean_q: 31905271.022222, mean_eps: 0.881689
  19888/150000: episode: 207, duration: 1.062s, episode steps: 124, steps per second: 117, episode reward: -93.045, mean reward: -0.750 [-100.000, 35.117], mean action: 1.500 [0.000, 3.000],  loss: 410283964779.354858, mse: 1216527278970021.250000, mean_q: 32535212.903226, mean_eps: 0.881047
  19985/150000: episode: 208, duration: 0.774s, episode steps:  97, steps per second: 125, episode reward: -191.987, mean reward: -1.979 [-100.000,  7.286], mean action: 1.536 [0.000, 3.000],  loss: 716807811958.762939, mse: 1296810967034595.000000, mean_q: 33967938.082474, mean_eps: 0.880384
  20064/150000: episode: 209, duration: 0.565s, episode steps:  79, steps per second: 140, episode reward: -156.828, mean reward: -1.985 [-100.000, 34.092], mean action: 1.494 [0.000, 3.000],  loss: 712650011816.506348, mse: 1374068195392706.500000, mean_q: 34747758.632911, mean_eps: 0.879856
  20168/150000: episode: 210, duration: 0.797s, episode steps: 104, steps per second: 130, episode reward: -171.725, mean reward: -1.651 [-100.000,  2.258], mean action: 1.510 [0.000, 3.000],  loss: 326698041344.000000, mse: 1442278858676381.500000, mean_q: 35514689.384615, mean_eps: 0.879307
  20257/150000: episode: 211, duration: 0.658s, episode steps:  89, steps per second: 135, episode reward: -282.517, mean reward: -3.174 [-100.000,  3.565], mean action: 1.742 [0.000, 3.000],  loss: 684922861222.831421, mse: 1537457343748763.250000, mean_q: 36649256.696629, mean_eps: 0.878728
  20363/150000: episode: 212, duration: 0.799s, episode steps: 106, steps per second: 133, episode reward: -322.039, mean reward: -3.038 [-100.000,  3.108], mean action: 1.670 [0.000, 3.000],  loss: 642955703122.113159, mse: 1649628578519078.750000, mean_q: 38196349.283019, mean_eps: 0.878143
  20497/150000: episode: 213, duration: 1.025s, episode steps: 134, steps per second: 131, episode reward: -230.345, mean reward: -1.719 [-100.000, 17.228], mean action: 1.627 [0.000, 3.000],  loss: 664595340822.925415, mse: 1816601977750482.250000, mean_q: 40366466.238806, mean_eps: 0.877423
  20597/150000: episode: 214, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: -211.401, mean reward: -2.114 [-100.000,  1.208], mean action: 1.600 [0.000, 3.000],  loss: 845373449093.119995, mse: 1908697066624778.250000, mean_q: 41160164.180000, mean_eps: 0.876721
  20722/150000: episode: 215, duration: 0.887s, episode steps: 125, steps per second: 141, episode reward: -114.617, mean reward: -0.917 [-100.000, 11.251], mean action: 1.520 [0.000, 3.000],  loss: 854249766322.176025, mse: 2046686678659104.750000, mean_q: 42587021.296000, mean_eps: 0.876046
  20814/150000: episode: 216, duration: 0.667s, episode steps:  92, steps per second: 138, episode reward: -374.136, mean reward: -4.067 [-100.000,  0.236], mean action: 1.457 [0.000, 3.000],  loss: 869262074746.434814, mse: 2161595629630686.500000, mean_q: 43459433.173913, mean_eps: 0.875395
  20910/150000: episode: 217, duration: 0.667s, episode steps:  96, steps per second: 144, episode reward: -135.709, mean reward: -1.414 [-100.000, 18.567], mean action: 1.562 [0.000, 3.000],  loss: 438008486997.333313, mse: 2264813749141504.000000, mean_q: 44494417.833333, mean_eps: 0.874831
  21028/150000: episode: 218, duration: 0.955s, episode steps: 118, steps per second: 124, episode reward: -274.993, mean reward: -2.330 [-100.000,  9.932], mean action: 1.602 [0.000, 3.000],  loss: 651779128996.881348, mse: 2367327875477990.000000, mean_q: 45428440.067797, mean_eps: 0.874189
  21177/150000: episode: 219, duration: 1.106s, episode steps: 149, steps per second: 135, episode reward: -243.654, mean reward: -1.635 [-100.000, 25.915], mean action: 1.564 [0.000, 3.000],  loss: 785926021834.738281, mse: 2560266252940061.000000, mean_q: 47329152.255034, mean_eps: 0.873388
  21268/150000: episode: 220, duration: 0.635s, episode steps:  91, steps per second: 143, episode reward: -62.870, mean reward: -0.691 [-100.000, 68.408], mean action: 1.692 [0.000, 3.000],  loss: 1112351037417.494385, mse: 2779938995225836.500000, mean_q: 49355167.120879, mean_eps: 0.872668
  21392/150000: episode: 221, duration: 0.909s, episode steps: 124, steps per second: 136, episode reward: -114.471, mean reward: -0.923 [-100.000, 11.653], mean action: 1.589 [0.000, 3.000],  loss: 1418969005419.354736, mse: 2908690114429456.500000, mean_q: 50446205.096774, mean_eps: 0.872023
  21483/150000: episode: 222, duration: 0.638s, episode steps:  91, steps per second: 143, episode reward: -116.514, mean reward: -1.280 [-100.000,  6.178], mean action: 1.681 [0.000, 3.000],  loss: 1015152645221.274780, mse: 3119992327349754.500000, mean_q: 52341884.351648, mean_eps: 0.871378
  21597/150000: episode: 223, duration: 0.783s, episode steps: 114, steps per second: 146, episode reward: -360.997, mean reward: -3.167 [-100.000,  6.216], mean action: 1.623 [0.000, 3.000],  loss: 1144632542621.192871, mse: 3189631820901609.500000, mean_q: 52709725.929825, mean_eps: 0.870763
  21666/150000: episode: 224, duration: 0.505s, episode steps:  69, steps per second: 137, episode reward: -226.591, mean reward: -3.284 [-100.000,  5.550], mean action: 1.507 [0.000, 3.000],  loss: 1908220344705.854980, mse: 3494127219362831.000000, mean_q: 55583426.724638, mean_eps: 0.870214
  21744/150000: episode: 225, duration: 0.559s, episode steps:  78, steps per second: 140, episode reward: -238.207, mean reward: -3.054 [-100.000, 60.513], mean action: 1.551 [0.000, 3.000],  loss: 1171326031819.487061, mse: 3533108011859968.000000, mean_q: 55462123.333333, mean_eps: 0.869773
  21843/150000: episode: 226, duration: 0.685s, episode steps:  99, steps per second: 144, episode reward: -167.591, mean reward: -1.693 [-100.000,  8.714], mean action: 1.717 [0.000, 3.000],  loss: 1390385829039.838379, mse: 3701429588740199.500000, mean_q: 56646328.161616, mean_eps: 0.869242
  21973/150000: episode: 227, duration: 1.302s, episode steps: 130, steps per second: 100, episode reward: -101.785, mean reward: -0.783 [-100.000, 15.045], mean action: 1.585 [0.000, 3.000],  loss: 2063163985604.923096, mse: 3988861307507885.500000, mean_q: 59174726.923077, mean_eps: 0.868555
  22060/150000: episode: 228, duration: 0.794s, episode steps:  87, steps per second: 110, episode reward: -373.144, mean reward: -4.289 [-100.000,  1.059], mean action: 1.425 [0.000, 3.000],  loss: 1075484908343.908081, mse: 4203583498996489.000000, mean_q: 60808271.586207, mean_eps: 0.867904
  22203/150000: episode: 229, duration: 1.159s, episode steps: 143, steps per second: 123, episode reward: -125.080, mean reward: -0.875 [-100.000, 10.671], mean action: 1.608 [0.000, 3.000],  loss: 1813144074683.971924, mse: 4438231163710886.500000, mean_q: 62097936.839161, mean_eps: 0.867214
  22328/150000: episode: 230, duration: 0.898s, episode steps: 125, steps per second: 139, episode reward: -78.553, mean reward: -0.628 [-100.000,  9.026], mean action: 1.664 [0.000, 3.000],  loss: 2086711175938.048096, mse: 4672429820351611.000000, mean_q: 63692536.096000, mean_eps: 0.866410
  22514/150000: episode: 231, duration: 1.451s, episode steps: 186, steps per second: 128, episode reward: -258.468, mean reward: -1.390 [-100.000, 13.502], mean action: 1.581 [0.000, 3.000],  loss: 1218539307008.000000, mse: 5143068578146810.000000, mean_q: 66894545.569892, mean_eps: 0.865477
  22619/150000: episode: 232, duration: 0.732s, episode steps: 105, steps per second: 143, episode reward: -387.405, mean reward: -3.690 [-100.000,  0.533], mean action: 1.629 [0.000, 3.000],  loss: 2001562555109.180908, mse: 5616134784773559.000000, mean_q: 70350568.495238, mean_eps: 0.864604
  22692/150000: episode: 233, duration: 0.582s, episode steps:  73, steps per second: 125, episode reward: -226.408, mean reward: -3.101 [-100.000, 15.664], mean action: 1.699 [0.000, 3.000],  loss: 3190984984996.821777, mse: 5715664373830698.000000, mean_q: 70613865.095890, mean_eps: 0.864070
  22761/150000: episode: 234, duration: 0.524s, episode steps:  69, steps per second: 132, episode reward: -139.918, mean reward: -2.028 [-100.000,  6.088], mean action: 1.493 [0.000, 3.000],  loss: 2660658691858.550781, mse: 6017522390634674.000000, mean_q: 72682743.420290, mean_eps: 0.863644
  22860/150000: episode: 235, duration: 0.706s, episode steps:  99, steps per second: 140, episode reward: -305.610, mean reward: -3.087 [-100.000,  1.018], mean action: 1.646 [0.000, 3.000],  loss: 2597223801576.727051, mse: 6261075868785116.000000, mean_q: 73642848.646465, mean_eps: 0.863140
  22929/150000: episode: 236, duration: 0.480s, episode steps:  69, steps per second: 144, episode reward: -79.096, mean reward: -1.146 [-100.000,  9.128], mean action: 1.536 [0.000, 3.000],  loss: 2864711572910.376953, mse: 6533354899412072.000000, mean_q: 75897764.695652, mean_eps: 0.862636
  23054/150000: episode: 237, duration: 0.898s, episode steps: 125, steps per second: 139, episode reward: -248.113, mean reward: -1.985 [-100.000,  5.116], mean action: 1.624 [0.000, 3.000],  loss: 3462285408600.063965, mse: 7034351674235814.000000, mean_q: 79070704.768000, mean_eps: 0.862054
  23147/150000: episode: 238, duration: 0.661s, episode steps:  93, steps per second: 141, episode reward: -191.642, mean reward: -2.061 [-100.000, 101.575], mean action: 1.398 [0.000, 3.000],  loss: 2638788577169.892578, mse: 7138472735074106.000000, mean_q: 78902657.505376, mean_eps: 0.861400
  23267/150000: episode: 239, duration: 0.826s, episode steps: 120, steps per second: 145, episode reward: -322.577, mean reward: -2.688 [-100.000,  7.904], mean action: 1.583 [0.000, 3.000],  loss: 3855878724539.733398, mse: 7643785853438089.000000, mean_q: 81912398.133333, mean_eps: 0.860761
  23380/150000: episode: 240, duration: 0.842s, episode steps: 113, steps per second: 134, episode reward: -179.185, mean reward: -1.586 [-100.000, 14.057], mean action: 1.469 [0.000, 3.000],  loss: 2519687434493.734375, mse: 8117548461514245.000000, mean_q: 84501800.566372, mean_eps: 0.860062
  23471/150000: episode: 241, duration: 0.639s, episode steps:  91, steps per second: 143, episode reward: -91.134, mean reward: -1.001 [-100.000,  4.788], mean action: 1.648 [0.000, 3.000],  loss: 2470629861758.593262, mse: 8414037881893179.000000, mean_q: 85959390.769231, mean_eps: 0.859450
  23591/150000: episode: 242, duration: 0.832s, episode steps: 120, steps per second: 144, episode reward: -358.674, mean reward: -2.989 [-100.000,  1.915], mean action: 1.558 [0.000, 3.000],  loss: 3635434204364.799805, mse: 8761118119253333.000000, mean_q: 87537281.233333, mean_eps: 0.858817
  23744/150000: episode: 243, duration: 1.100s, episode steps: 153, steps per second: 139, episode reward: -521.791, mean reward: -3.410 [-100.000,  2.501], mean action: 1.510 [0.000, 3.000],  loss: 4260238404909.176270, mse: 9849614655995582.000000, mean_q: 93523632.418301, mean_eps: 0.857998
  23829/150000: episode: 244, duration: 0.597s, episode steps:  85, steps per second: 142, episode reward: -165.682, mean reward: -1.949 [-100.000,  8.009], mean action: 1.494 [0.000, 3.000],  loss: 4561352229635.011719, mse: 10116682956533230.000000, mean_q: 94149082.635294, mean_eps: 0.857284
  23988/150000: episode: 245, duration: 1.137s, episode steps: 159, steps per second: 140, episode reward: -498.040, mean reward: -3.132 [-100.000,  3.635], mean action: 1.591 [0.000, 3.000],  loss: 4191134850917.434082, mse: 10642978574463980.000000, mean_q: 96797798.742138, mean_eps: 0.856552
  24054/150000: episode: 246, duration: 0.485s, episode steps:  66, steps per second: 136, episode reward: -86.662, mean reward: -1.313 [-100.000,  5.648], mean action: 1.470 [0.000, 3.000],  loss: 2991389690290.424316, mse: 11288028996412136.000000, mean_q: 99722047.515152, mean_eps: 0.855877
  24136/150000: episode: 247, duration: 0.567s, episode steps:  82, steps per second: 145, episode reward: -137.460, mean reward: -1.676 [-100.000,  8.025], mean action: 1.341 [0.000, 3.000],  loss: 2843885977200.390137, mse: 11451949642703848.000000, mean_q: 99881933.268293, mean_eps: 0.855433
  24259/150000: episode: 248, duration: 0.884s, episode steps: 123, steps per second: 139, episode reward: -82.142, mean reward: -0.668 [-100.000,  8.344], mean action: 1.512 [0.000, 3.000],  loss: 3033061266107.316895, mse: 12348059102798024.000000, mean_q: 104154783.219512, mean_eps: 0.854818
  24354/150000: episode: 249, duration: 0.654s, episode steps:  95, steps per second: 145, episode reward: -236.275, mean reward: -2.487 [-100.000, 20.299], mean action: 1.537 [0.000, 3.000],  loss: 7739630054346.105469, mse: 12887506723501024.000000, mean_q: 106409551.747368, mean_eps: 0.854164
  24463/150000: episode: 250, duration: 0.766s, episode steps: 109, steps per second: 142, episode reward: -294.478, mean reward: -2.702 [-100.000,  0.692], mean action: 1.468 [0.000, 3.000],  loss: 6664765553635.816406, mse: 13364474518029040.000000, mean_q: 108682014.311927, mean_eps: 0.853552
  24552/150000: episode: 251, duration: 0.636s, episode steps:  89, steps per second: 140, episode reward: -146.072, mean reward: -1.641 [-100.000, 15.509], mean action: 1.371 [0.000, 3.000],  loss: 3376707284485.752930, mse: 14086429859183904.000000, mean_q: 111727263.910112, mean_eps: 0.852958
  24630/150000: episode: 252, duration: 0.561s, episode steps:  78, steps per second: 139, episode reward: -355.911, mean reward: -4.563 [-100.000, 18.283], mean action: 1.513 [0.000, 3.000],  loss: 6277508158805.333008, mse: 14390194482177470.000000, mean_q: 112262069.743590, mean_eps: 0.852457
  24772/150000: episode: 253, duration: 0.980s, episode steps: 142, steps per second: 145, episode reward: -332.119, mean reward: -2.339 [-100.000, 48.770], mean action: 1.458 [0.000, 3.000],  loss: 4449090203590.309570, mse: 15214166692154296.000000, mean_q: 116115987.718310, mean_eps: 0.851797
  24872/150000: episode: 254, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -200.929, mean reward: -2.009 [-100.000,  6.862], mean action: 1.440 [0.000, 3.000],  loss: 6154255489433.599609, mse: 15942984444600648.000000, mean_q: 118596428.800000, mean_eps: 0.851071
  24979/150000: episode: 255, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: -263.012, mean reward: -2.458 [-100.000,  4.519], mean action: 1.561 [0.000, 3.000],  loss: 5128944902067.439453, mse: 16597921285682338.000000, mean_q: 120954008.672897, mean_eps: 0.850450
  25071/150000: episode: 256, duration: 0.651s, episode steps:  92, steps per second: 141, episode reward: -290.437, mean reward: -3.157 [-100.000,  0.482], mean action: 1.576 [0.000, 3.000],  loss: 6305808127554.782227, mse: 17287613083720660.000000, mean_q: 123706772.869565, mean_eps: 0.849853
  25220/150000: episode: 257, duration: 1.073s, episode steps: 149, steps per second: 139, episode reward: -447.936, mean reward: -3.006 [-100.000, 70.979], mean action: 1.779 [0.000, 3.000],  loss: 6376275505289.449219, mse: 18114938181045804.000000, mean_q: 126225931.060403, mean_eps: 0.849130
  25305/150000: episode: 258, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -401.802, mean reward: -4.727 [-100.000,  1.317], mean action: 1.553 [0.000, 3.000],  loss: 8630194354489.223633, mse: 19161677209756516.000000, mean_q: 129494178.541176, mean_eps: 0.848428
  25433/150000: episode: 259, duration: 0.992s, episode steps: 128, steps per second: 129, episode reward: -345.661, mean reward: -2.700 [-100.000,  8.318], mean action: 1.453 [0.000, 3.000],  loss: 8099330439168.000000, mse: 19410254273970176.000000, mean_q: 130069011.687500, mean_eps: 0.847789
  25514/150000: episode: 260, duration: 0.692s, episode steps:  81, steps per second: 117, episode reward: -100.202, mean reward: -1.237 [-100.000, 11.761], mean action: 1.568 [0.000, 3.000],  loss: 9721975435719.111328, mse: 20661565333758864.000000, mean_q: 135063759.407407, mean_eps: 0.847162
  25593/150000: episode: 261, duration: 0.632s, episode steps:  79, steps per second: 125, episode reward: -363.379, mean reward: -4.600 [-100.000,  7.333], mean action: 1.430 [0.000, 3.000],  loss: 10061952652974.988281, mse: 21079684102999080.000000, mean_q: 135655202.632911, mean_eps: 0.846682
  25688/150000: episode: 262, duration: 0.711s, episode steps:  95, steps per second: 134, episode reward: -53.920, mean reward: -0.568 [-100.000, 15.233], mean action: 1.495 [0.000, 3.000],  loss: 8301939738818.021484, mse: 21483150123866868.000000, mean_q: 136733508.715789, mean_eps: 0.846160
  25793/150000: episode: 263, duration: 0.860s, episode steps: 105, steps per second: 122, episode reward: -206.325, mean reward: -1.965 [-100.000, 24.626], mean action: 1.648 [0.000, 3.000],  loss: 8971747245611.884766, mse: 22474491785000356.000000, mean_q: 139591236.952381, mean_eps: 0.845560
  25918/150000: episode: 264, duration: 0.926s, episode steps: 125, steps per second: 135, episode reward: -276.440, mean reward: -2.212 [-100.000,  4.182], mean action: 1.480 [0.000, 3.000],  loss: 11261550462500.863281, mse: 24021515717297832.000000, mean_q: 145131524.736000, mean_eps: 0.844870
  26015/150000: episode: 265, duration: 0.771s, episode steps:  97, steps per second: 126, episode reward: -194.519, mean reward: -2.005 [-100.000, 34.742], mean action: 1.546 [0.000, 3.000],  loss: 10309857504646.597656, mse: 24656179571998552.000000, mean_q: 146570524.701031, mean_eps: 0.844204
  26094/150000: episode: 266, duration: 0.570s, episode steps:  79, steps per second: 139, episode reward: -473.103, mean reward: -5.989 [-100.000,  2.205], mean action: 1.886 [0.000, 3.000],  loss: 8010813466961.012695, mse: 25360817825822072.000000, mean_q: 148528643.037975, mean_eps: 0.843676
  26190/150000: episode: 267, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -320.216, mean reward: -3.336 [-100.000,  5.372], mean action: 1.677 [0.000, 3.000],  loss: 7764064205482.666992, mse: 27020026119367340.000000, mean_q: 154317883.500000, mean_eps: 0.843151
  26292/150000: episode: 268, duration: 0.710s, episode steps: 102, steps per second: 144, episode reward: -264.225, mean reward: -2.590 [-100.000,  7.119], mean action: 1.716 [0.000, 3.000],  loss: 7614736162012.862305, mse: 28403742746653876.000000, mean_q: 158924762.352941, mean_eps: 0.842557
  26388/150000: episode: 269, duration: 0.680s, episode steps:  96, steps per second: 141, episode reward: -353.470, mean reward: -3.682 [-100.000, 76.002], mean action: 1.604 [0.000, 3.000],  loss: 8244027038378.666992, mse: 28736695802615124.000000, mean_q: 158109939.916667, mean_eps: 0.841963
  26498/150000: episode: 270, duration: 0.764s, episode steps: 110, steps per second: 144, episode reward: -184.248, mean reward: -1.675 [-100.000,  6.714], mean action: 1.545 [0.000, 3.000],  loss: 15627471807506.619141, mse: 30037055452813124.000000, mean_q: 162305514.472727, mean_eps: 0.841345
  26644/150000: episode: 271, duration: 1.048s, episode steps: 146, steps per second: 139, episode reward: -245.249, mean reward: -1.680 [-100.000,  8.207], mean action: 1.459 [0.000, 3.000],  loss: 15776572568954.740234, mse: 32022094233986032.000000, mean_q: 168025216.657534, mean_eps: 0.840577
  26752/150000: episode: 272, duration: 0.778s, episode steps: 108, steps per second: 139, episode reward: -91.581, mean reward: -0.848 [-100.000,  6.919], mean action: 1.657 [0.000, 3.000],  loss: 12189480370782.814453, mse: 33638363356113580.000000, mean_q: 172221527.851852, mean_eps: 0.839815
  26862/150000: episode: 273, duration: 0.751s, episode steps: 110, steps per second: 146, episode reward: -301.077, mean reward: -2.737 [-100.000,  1.194], mean action: 1.518 [0.000, 3.000],  loss: 12955027067848.144531, mse: 34805982658803080.000000, mean_q: 174096036.218182, mean_eps: 0.839161
  26951/150000: episode: 274, duration: 0.661s, episode steps:  89, steps per second: 135, episode reward: -424.586, mean reward: -4.771 [-100.000,  0.778], mean action: 1.517 [0.000, 3.000],  loss: 9447936672664.449219, mse: 36185214540142888.000000, mean_q: 178921677.662921, mean_eps: 0.838564
  27101/150000: episode: 275, duration: 1.062s, episode steps: 150, steps per second: 141, episode reward: -270.413, mean reward: -1.803 [-100.000,  5.535], mean action: 1.760 [0.000, 3.000],  loss: 11776522460856.320312, mse: 38183748927896944.000000, mean_q: 183947912.000000, mean_eps: 0.837847
  27200/150000: episode: 276, duration: 0.667s, episode steps:  99, steps per second: 148, episode reward: -102.911, mean reward: -1.040 [-100.000, 39.646], mean action: 1.455 [0.000, 3.000],  loss: 11771613531208.404297, mse: 39588598537272848.000000, mean_q: 186496015.676768, mean_eps: 0.837100
  27322/150000: episode: 277, duration: 0.883s, episode steps: 122, steps per second: 138, episode reward: -224.068, mean reward: -1.837 [-100.000, 15.018], mean action: 1.615 [0.000, 3.000],  loss: 15458366957534.425781, mse: 41652068780188592.000000, mean_q: 192341422.295082, mean_eps: 0.836437
  27495/150000: episode: 278, duration: 1.211s, episode steps: 173, steps per second: 143, episode reward: -263.257, mean reward: -1.522 [-100.000,  7.460], mean action: 1.549 [0.000, 3.000],  loss: 14503941617154.958984, mse: 43846530317113928.000000, mean_q: 196183291.375723, mean_eps: 0.835552
  27592/150000: episode: 279, duration: 0.728s, episode steps:  97, steps per second: 133, episode reward: -318.368, mean reward: -3.282 [-100.000,  0.519], mean action: 1.485 [0.000, 3.000],  loss: 16473721554078.349609, mse: 45637325479269152.000000, mean_q: 199855934.185567, mean_eps: 0.834742
  27701/150000: episode: 280, duration: 0.745s, episode steps: 109, steps per second: 146, episode reward: -151.411, mean reward: -1.389 [-100.000,  6.832], mean action: 1.550 [0.000, 3.000],  loss: 13674236505097.394531, mse: 47423446462807008.000000, mean_q: 203255198.972477, mean_eps: 0.834124
  27791/150000: episode: 281, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -34.788, mean reward: -0.387 [-100.000,  7.877], mean action: 1.544 [0.000, 3.000],  loss: 20520525685646.222656, mse: 48923364730863616.000000, mean_q: 206810194.311111, mean_eps: 0.833527
  27899/150000: episode: 282, duration: 0.771s, episode steps: 108, steps per second: 140, episode reward: -249.246, mean reward: -2.308 [-100.000,  0.922], mean action: 1.546 [0.000, 3.000],  loss: 24826485419880.296875, mse: 50103144425795432.000000, mean_q: 210055057.629630, mean_eps: 0.832933
  27999/150000: episode: 283, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -457.427, mean reward: -4.574 [-100.000,  5.314], mean action: 1.610 [0.000, 3.000],  loss: 20808811958763.519531, mse: 52860285600105760.000000, mean_q: 216507946.400000, mean_eps: 0.832309
  28097/150000: episode: 284, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -367.775, mean reward: -3.753 [-100.000,  3.783], mean action: 1.571 [0.000, 3.000],  loss: 16101874378543.019531, mse: 53707425174835448.000000, mean_q: 216580789.714286, mean_eps: 0.831715
  28189/150000: episode: 285, duration: 0.675s, episode steps:  92, steps per second: 136, episode reward: -150.904, mean reward: -1.640 [-100.000,  8.751], mean action: 1.587 [0.000, 3.000],  loss: 17095769339814.957031, mse: 57414923563793008.000000, mean_q: 224335759.478261, mean_eps: 0.831145
  28305/150000: episode: 286, duration: 0.801s, episode steps: 116, steps per second: 145, episode reward: -411.786, mean reward: -3.550 [-100.000,  2.506], mean action: 1.603 [0.000, 3.000],  loss: 18782647226791.722656, mse: 59075335793617952.000000, mean_q: 228809738.068966, mean_eps: 0.830521
  28406/150000: episode: 287, duration: 0.722s, episode steps: 101, steps per second: 140, episode reward: -241.615, mean reward: -2.392 [-100.000, 21.820], mean action: 1.574 [0.000, 3.000],  loss: 16137897063738.296875, mse: 61839386102077784.000000, mean_q: 233200240.000000, mean_eps: 0.829870
  28511/150000: episode: 288, duration: 0.771s, episode steps: 105, steps per second: 136, episode reward: -241.664, mean reward: -2.302 [-100.000, 18.285], mean action: 1.505 [0.000, 3.000],  loss: 22286926837623.464844, mse: 62650286633188752.000000, mean_q: 235212158.476190, mean_eps: 0.829252
  28650/150000: episode: 289, duration: 0.974s, episode steps: 139, steps per second: 143, episode reward: -234.948, mean reward: -1.690 [-100.000,  4.198], mean action: 1.504 [0.000, 3.000],  loss: 21173257588529.726562, mse: 66567474102625608.000000, mean_q: 243004252.661871, mean_eps: 0.828520
  28805/150000: episode: 290, duration: 1.149s, episode steps: 155, steps per second: 135, episode reward: -351.677, mean reward: -2.269 [-100.000,  2.421], mean action: 1.645 [0.000, 3.000],  loss: 24004265265323.769531, mse: 70111639926060040.000000, mean_q: 249734052.025806, mean_eps: 0.827638
  28915/150000: episode: 291, duration: 0.763s, episode steps: 110, steps per second: 144, episode reward: -367.323, mean reward: -3.339 [-100.000,  5.650], mean action: 1.545 [0.000, 3.000],  loss: 21069177500467.199219, mse: 72534018517287808.000000, mean_q: 252379866.618182, mean_eps: 0.826843
  29014/150000: episode: 292, duration: 0.672s, episode steps:  99, steps per second: 147, episode reward: -109.753, mean reward: -1.109 [-100.000, 54.528], mean action: 1.687 [0.000, 3.000],  loss: 32558268658284.605469, mse: 74165542034749664.000000, mean_q: 255208055.111111, mean_eps: 0.826216
  29108/150000: episode: 293, duration: 0.693s, episode steps:  94, steps per second: 136, episode reward: -417.933, mean reward: -4.446 [-100.000,  1.069], mean action: 1.713 [0.000, 3.000],  loss: 23703546985798.808594, mse: 75781018567372720.000000, mean_q: 256252670.468085, mean_eps: 0.825637
  29256/150000: episode: 294, duration: 1.025s, episode steps: 148, steps per second: 144, episode reward: -14.580, mean reward: -0.099 [-100.000, 116.759], mean action: 1.514 [0.000, 3.000],  loss: 36699238085604.328125, mse: 79991464075729840.000000, mean_q: 264476738.594595, mean_eps: 0.824911
  29438/150000: episode: 295, duration: 1.320s, episode steps: 182, steps per second: 138, episode reward: -220.143, mean reward: -1.210 [-100.000,  2.093], mean action: 1.588 [0.000, 3.000],  loss: 34344792144299.605469, mse: 84078522735332880.000000, mean_q: 270906750.769231, mean_eps: 0.823921
  29539/150000: episode: 296, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -347.835, mean reward: -3.444 [-100.000,  0.885], mean action: 1.475 [0.000, 3.000],  loss: 38061731232980.914062, mse: 88891428975862816.000000, mean_q: 279384727.920792, mean_eps: 0.823072
  29618/150000: episode: 297, duration: 0.543s, episode steps:  79, steps per second: 146, episode reward: -53.820, mean reward: -0.681 [-100.000,  8.226], mean action: 1.544 [0.000, 3.000],  loss: 44825945644330.125000, mse: 91187130788475664.000000, mean_q: 282291383.898734, mean_eps: 0.822532
  29705/150000: episode: 298, duration: 0.654s, episode steps:  87, steps per second: 133, episode reward: -217.780, mean reward: -2.503 [-100.000, 21.429], mean action: 1.655 [0.000, 3.000],  loss: 31081098870407.355469, mse: 95457288288065232.000000, mean_q: 289903375.448276, mean_eps: 0.822034
  29802/150000: episode: 299, duration: 0.692s, episode steps:  97, steps per second: 140, episode reward: -503.960, mean reward: -5.195 [-100.000,  0.838], mean action: 1.732 [0.000, 3.000],  loss: 27602933138590.351562, mse: 99076954699803872.000000, mean_q: 295084187.876289, mean_eps: 0.821482
  29867/150000: episode: 300, duration: 0.444s, episode steps:  65, steps per second: 146, episode reward: -66.413, mean reward: -1.022 [-100.000, 10.970], mean action: 1.585 [0.000, 3.000],  loss: 17985689104194.953125, mse: 99824737466583008.000000, mean_q: 296504345.846154, mean_eps: 0.820996
  29964/150000: episode: 301, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -452.720, mean reward: -4.667 [-100.000,  1.163], mean action: 1.670 [0.000, 3.000],  loss: 44764221615325.687500, mse: 102193929581856416.000000, mean_q: 298614449.979381, mean_eps: 0.820510
  30170/150000: episode: 302, duration: 1.466s, episode steps: 206, steps per second: 141, episode reward: -95.272, mean reward: -0.462 [-100.000,  9.352], mean action: 1.699 [0.000, 3.000],  loss: 35911510490817.867188, mse: 108356927585873888.000000, mean_q: 308301755.572816, mean_eps: 0.819601
  30249/150000: episode: 303, duration: 0.553s, episode steps:  79, steps per second: 143, episode reward: -37.797, mean reward: -0.478 [-100.000, 17.338], mean action: 1.557 [0.000, 3.000],  loss: 33015339812475.140625, mse: 114563974942106528.000000, mean_q: 316487606.683544, mean_eps: 0.818746
  30326/150000: episode: 304, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: -54.275, mean reward: -0.705 [-100.000, 14.377], mean action: 1.442 [0.000, 3.000],  loss: 35141486338154.390625, mse: 118700529902355280.000000, mean_q: 323152030.337662, mean_eps: 0.818278
  30445/150000: episode: 305, duration: 0.820s, episode steps: 119, steps per second: 145, episode reward: -147.572, mean reward: -1.240 [-100.000,  8.078], mean action: 1.588 [0.000, 3.000],  loss: 45639792427834.085938, mse: 120630599247841296.000000, mean_q: 325789302.857143, mean_eps: 0.817690
  30623/150000: episode: 306, duration: 1.250s, episode steps: 178, steps per second: 142, episode reward: -283.592, mean reward: -1.593 [-100.000,  2.136], mean action: 1.539 [0.000, 3.000],  loss: 43661625189916.765625, mse: 129972812901641920.000000, mean_q: 340386827.595506, mean_eps: 0.816799
  30744/150000: episode: 307, duration: 0.853s, episode steps: 121, steps per second: 142, episode reward: -262.441, mean reward: -2.169 [-100.000, 25.974], mean action: 1.529 [0.000, 3.000],  loss: 74534086889844.359375, mse: 131467995660793872.000000, mean_q: 339866846.545455, mean_eps: 0.815902
  30862/150000: episode: 308, duration: 0.802s, episode steps: 118, steps per second: 147, episode reward: -157.810, mean reward: -1.337 [-100.000,  2.547], mean action: 1.542 [0.000, 3.000],  loss: 32806477905086.914062, mse: 140259453545151408.000000, mean_q: 352520237.288136, mean_eps: 0.815185
  30968/150000: episode: 309, duration: 0.796s, episode steps: 106, steps per second: 133, episode reward: -329.099, mean reward: -3.105 [-100.000,  0.913], mean action: 1.613 [0.000, 3.000],  loss: 34716827907670.945312, mse: 145091645973265184.000000, mean_q: 357032541.886792, mean_eps: 0.814513
  31102/150000: episode: 310, duration: 0.926s, episode steps: 134, steps per second: 145, episode reward: -132.449, mean reward: -0.988 [-100.000, 13.236], mean action: 1.694 [0.000, 3.000],  loss: 51942763582968.359375, mse: 149340368427486080.000000, mean_q: 362678339.462687, mean_eps: 0.813793
  31169/150000: episode: 311, duration: 0.467s, episode steps:  67, steps per second: 143, episode reward: -52.509, mean reward: -0.784 [-100.000, 12.426], mean action: 1.478 [0.000, 3.000],  loss: 52169197263061.968750, mse: 153782558548354368.000000, mean_q: 367464020.059702, mean_eps: 0.813190
  31281/150000: episode: 312, duration: 0.916s, episode steps: 112, steps per second: 122, episode reward: -282.929, mean reward: -2.526 [-100.000,  1.425], mean action: 1.616 [0.000, 3.000],  loss: 51499003815058.289062, mse: 162932811964987104.000000, mean_q: 381299058.571429, mean_eps: 0.812653
  31422/150000: episode: 313, duration: 1.094s, episode steps: 141, steps per second: 129, episode reward: -285.343, mean reward: -2.024 [-100.000,  4.411], mean action: 1.376 [0.000, 3.000],  loss: 66540379778403.859375, mse: 169064338560502144.000000, mean_q: 388282014.411348, mean_eps: 0.811894
  31527/150000: episode: 314, duration: 0.830s, episode steps: 105, steps per second: 127, episode reward: -95.780, mean reward: -0.912 [-100.000,  7.555], mean action: 1.524 [0.000, 3.000],  loss: 69706903448702.781250, mse: 174026949833515712.000000, mean_q: 390529877.638095, mean_eps: 0.811156
  31609/150000: episode: 315, duration: 0.601s, episode steps:  82, steps per second: 136, episode reward: -122.366, mean reward: -1.492 [-100.000,  8.856], mean action: 1.671 [0.000, 3.000],  loss: 71275734604824.968750, mse: 179910533823530560.000000, mean_q: 398774052.682927, mean_eps: 0.810595
  31721/150000: episode: 316, duration: 0.872s, episode steps: 112, steps per second: 128, episode reward: -173.862, mean reward: -1.552 [-100.000,  7.244], mean action: 1.348 [0.000, 3.000],  loss: 90771497119158.859375, mse: 185012308666520128.000000, mean_q: 403559143.714286, mean_eps: 0.810013
  31785/150000: episode: 317, duration: 0.529s, episode steps:  64, steps per second: 121, episode reward: -114.406, mean reward: -1.788 [-100.000, 12.151], mean action: 1.500 [0.000, 3.000],  loss: 118133347155968.000000, mse: 185649266390204416.000000, mean_q: 405198177.500000, mean_eps: 0.809485
  31906/150000: episode: 318, duration: 0.858s, episode steps: 121, steps per second: 141, episode reward: -136.293, mean reward: -1.126 [-100.000,  8.906], mean action: 1.587 [0.000, 3.000],  loss: 86435008014522.187500, mse: 194300669535864800.000000, mean_q: 415637625.917355, mean_eps: 0.808930
  32025/150000: episode: 319, duration: 0.848s, episode steps: 119, steps per second: 140, episode reward: -283.468, mean reward: -2.382 [-100.000, 17.286], mean action: 1.588 [0.000, 3.000],  loss: 106552402982344.062500, mse: 201600534831206976.000000, mean_q: 424327390.386555, mean_eps: 0.808210
  32121/150000: episode: 320, duration: 0.711s, episode steps:  96, steps per second: 135, episode reward: -439.345, mean reward: -4.577 [-100.000,  3.876], mean action: 1.469 [0.000, 3.000],  loss: 63234167384746.664062, mse: 201433255007398560.000000, mean_q: 421457333.666667, mean_eps: 0.807565
  32203/150000: episode: 321, duration: 0.569s, episode steps:  82, steps per second: 144, episode reward: -363.467, mean reward: -4.433 [-100.000, 12.435], mean action: 1.768 [0.000, 3.000],  loss: 49500383711132.093750, mse: 212825404652855104.000000, mean_q: 433292654.048781, mean_eps: 0.807031
  32279/150000: episode: 322, duration: 0.546s, episode steps:  76, steps per second: 139, episode reward: -81.911, mean reward: -1.078 [-100.000,  7.556], mean action: 1.526 [0.000, 3.000],  loss: 125993252883833.265625, mse: 212025420184896352.000000, mean_q: 431938139.368421, mean_eps: 0.806557
  32346/150000: episode: 323, duration: 0.483s, episode steps:  67, steps per second: 139, episode reward: -126.268, mean reward: -1.885 [-100.000,  6.085], mean action: 1.567 [0.000, 3.000],  loss: 89209209211521.906250, mse: 215773535543993408.000000, mean_q: 437070770.149254, mean_eps: 0.806128
  32430/150000: episode: 324, duration: 0.619s, episode steps:  84, steps per second: 136, episode reward: -98.881, mean reward: -1.177 [-100.000,  8.442], mean action: 1.690 [0.000, 3.000],  loss: 119201874225834.671875, mse: 216331786507093536.000000, mean_q: 434083319.238095, mean_eps: 0.805675
  32516/150000: episode: 325, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -109.917, mean reward: -1.278 [-100.000,  6.823], mean action: 1.640 [0.000, 3.000],  loss: 99806823853984.750000, mse: 225877152445966752.000000, mean_q: 446411747.348837, mean_eps: 0.805165
  32614/150000: episode: 326, duration: 0.713s, episode steps:  98, steps per second: 137, episode reward: -261.892, mean reward: -2.672 [-100.000, 40.573], mean action: 1.490 [0.000, 3.000],  loss: 145046181596766.031250, mse: 233633093729295552.000000, mean_q: 453077054.040816, mean_eps: 0.804613
  32681/150000: episode: 327, duration: 0.527s, episode steps:  67, steps per second: 127, episode reward: -234.323, mean reward: -3.497 [-100.000,  7.772], mean action: 1.403 [0.000, 3.000],  loss: 61735563510004.539062, mse: 226757271055874880.000000, mean_q: 443211233.432836, mean_eps: 0.804118
  32786/150000: episode: 328, duration: 0.747s, episode steps: 105, steps per second: 141, episode reward: -332.205, mean reward: -3.164 [-100.000,  1.387], mean action: 1.629 [0.000, 3.000],  loss: 127041060805329.671875, mse: 241628810631975808.000000, mean_q: 461410078.476190, mean_eps: 0.803602
  32978/150000: episode: 329, duration: 1.365s, episode steps: 192, steps per second: 141, episode reward: -300.918, mean reward: -1.567 [-100.000,  3.103], mean action: 1.589 [0.000, 3.000],  loss: 64853194113024.000000, mse: 254822823443300352.000000, mean_q: 473287877.333333, mean_eps: 0.802711
  33069/150000: episode: 330, duration: 0.655s, episode steps:  91, steps per second: 139, episode reward: -338.641, mean reward: -3.721 [-100.000,  0.244], mean action: 1.560 [0.000, 3.000],  loss: 92565317350591.296875, mse: 263658803685193984.000000, mean_q: 482652379.780220, mean_eps: 0.801862
  33205/150000: episode: 331, duration: 0.943s, episode steps: 136, steps per second: 144, episode reward: -414.647, mean reward: -3.049 [-100.000, 39.412], mean action: 1.485 [0.000, 3.000],  loss: 107822207328737.875000, mse: 280909846303396576.000000, mean_q: 501220252.705882, mean_eps: 0.801181
  33291/150000: episode: 332, duration: 0.642s, episode steps:  86, steps per second: 134, episode reward: -461.209, mean reward: -5.363 [-100.000,  0.078], mean action: 1.674 [0.000, 3.000],  loss: 98786839595246.140625, mse: 289647048740514240.000000, mean_q: 507206626.976744, mean_eps: 0.800515
  33380/150000: episode: 333, duration: 0.629s, episode steps:  89, steps per second: 141, episode reward: -292.334, mean reward: -3.285 [-100.000,  6.921], mean action: 1.640 [0.000, 3.000],  loss: 181090098798684.031250, mse: 295962058721337984.000000, mean_q: 510277974.292135, mean_eps: 0.799990
  33512/150000: episode: 334, duration: 0.922s, episode steps: 132, steps per second: 143, episode reward: -303.035, mean reward: -2.296 [-100.000, 13.175], mean action: 1.485 [0.000, 3.000],  loss: 112226872145485.578125, mse: 307153752299031936.000000, mean_q: 523478925.090909, mean_eps: 0.799327
  33680/150000: episode: 335, duration: 1.246s, episode steps: 168, steps per second: 135, episode reward: -140.849, mean reward: -0.838 [-100.000, 10.388], mean action: 1.619 [0.000, 3.000],  loss: 107284669948294.093750, mse: 321707944087587904.000000, mean_q: 535250317.523810, mean_eps: 0.798427
  33821/150000: episode: 336, duration: 0.977s, episode steps: 141, steps per second: 144, episode reward: -496.296, mean reward: -3.520 [-100.000,  3.054], mean action: 1.674 [0.000, 3.000],  loss: 290967617188333.875000, mse: 333073507071293632.000000, mean_q: 542958524.368794, mean_eps: 0.797500
  33907/150000: episode: 337, duration: 0.673s, episode steps:  86, steps per second: 128, episode reward: -418.083, mean reward: -4.861 [-100.000, -0.545], mean action: 1.488 [0.000, 3.000],  loss: 79548082071218.609375, mse: 333612009219381568.000000, mean_q: 540663922.976744, mean_eps: 0.796819
  34056/150000: episode: 338, duration: 1.048s, episode steps: 149, steps per second: 142, episode reward: -420.789, mean reward: -2.824 [-100.000,  4.051], mean action: 1.456 [0.000, 3.000],  loss: 184812801936940.656250, mse: 353148504972379072.000000, mean_q: 558335327.140940, mean_eps: 0.796114
  34147/150000: episode: 339, duration: 0.651s, episode steps:  91, steps per second: 140, episode reward: -328.493, mean reward: -3.610 [-100.000,  1.191], mean action: 1.516 [0.000, 3.000],  loss: 109234965020311.906250, mse: 359615165167434304.000000, mean_q: 565172305.934066, mean_eps: 0.795394
  34241/150000: episode: 340, duration: 0.685s, episode steps:  94, steps per second: 137, episode reward: -194.215, mean reward: -2.066 [-100.000, 16.150], mean action: 1.596 [0.000, 3.000],  loss: 316863764094801.687500, mse: 361379116134945920.000000, mean_q: 562678801.021277, mean_eps: 0.794839
  34318/150000: episode: 341, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward: -153.445, mean reward: -1.993 [-100.000,  6.823], mean action: 1.468 [0.000, 3.000],  loss: 141847844655689.156250, mse: 376500946295780864.000000, mean_q: 578359003.844156, mean_eps: 0.794326
  34441/150000: episode: 342, duration: 0.863s, episode steps: 123, steps per second: 143, episode reward: -92.715, mean reward: -0.754 [-100.000,  7.374], mean action: 1.553 [0.000, 3.000],  loss: 182978202389762.093750, mse: 384755740173625920.000000, mean_q: 581313122.601626, mean_eps: 0.793726
  34620/150000: episode: 343, duration: 1.423s, episode steps: 179, steps per second: 126, episode reward: -242.628, mean reward: -1.355 [-100.000,  2.373], mean action: 1.486 [0.000, 3.000],  loss: 173897734697852.437500, mse: 396861085449929536.000000, mean_q: 591685329.519553, mean_eps: 0.792820
  34713/150000: episode: 344, duration: 0.738s, episode steps:  93, steps per second: 126, episode reward: -326.208, mean reward: -3.508 [-100.000, 80.252], mean action: 1.602 [0.000, 3.000],  loss: 239016039280507.875000, mse: 400194833390062144.000000, mean_q: 594359212.731183, mean_eps: 0.792004
  34801/150000: episode: 345, duration: 0.757s, episode steps:  88, steps per second: 116, episode reward: -91.775, mean reward: -1.043 [-100.000, 18.549], mean action: 1.625 [0.000, 3.000],  loss: 320230370884142.562500, mse: 416236962185501952.000000, mean_q: 606021805.090909, mean_eps: 0.791461
  34895/150000: episode: 346, duration: 0.727s, episode steps:  94, steps per second: 129, episode reward: -412.005, mean reward: -4.383 [-100.000,  0.637], mean action: 1.649 [0.000, 3.000],  loss: 202474694661926.125000, mse: 412649991969584768.000000, mean_q: 604027484.936170, mean_eps: 0.790915
  34979/150000: episode: 347, duration: 0.660s, episode steps:  84, steps per second: 127, episode reward: -155.179, mean reward: -1.847 [-100.000,  8.242], mean action: 1.548 [0.000, 3.000],  loss: 159529454434791.625000, mse: 418224414087733632.000000, mean_q: 604641809.523810, mean_eps: 0.790381
  35066/150000: episode: 348, duration: 0.677s, episode steps:  87, steps per second: 128, episode reward: -371.545, mean reward: -4.271 [-100.000,  3.710], mean action: 1.770 [0.000, 3.000],  loss: 400709546033340.312500, mse: 428644630600026304.000000, mean_q: 613544221.425287, mean_eps: 0.789868
  35247/150000: episode: 349, duration: 1.320s, episode steps: 181, steps per second: 137, episode reward: -112.111, mean reward: -0.619 [-100.000, 52.873], mean action: 1.597 [0.000, 3.000],  loss: 288646209490740.312500, mse: 437068672467248128.000000, mean_q: 617639151.204420, mean_eps: 0.789064
  35378/150000: episode: 350, duration: 0.974s, episode steps: 131, steps per second: 135, episode reward: -279.773, mean reward: -2.136 [-100.000,  4.847], mean action: 1.817 [0.000, 3.000],  loss: 152022364907160.437500, mse: 438232117357730304.000000, mean_q: 617304380.091603, mean_eps: 0.788128
  35491/150000: episode: 351, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -343.308, mean reward: -3.038 [-100.000,  1.326], mean action: 1.522 [0.000, 3.000],  loss: 165884251990360.343750, mse: 445500159046771328.000000, mean_q: 621204645.946903, mean_eps: 0.787396
  35593/150000: episode: 352, duration: 0.751s, episode steps: 102, steps per second: 136, episode reward: -34.289, mean reward: -0.336 [-100.000, 29.183], mean action: 1.667 [0.000, 3.000],  loss: 138327805626247.531250, mse: 455332389035915072.000000, mean_q: 629257913.411765, mean_eps: 0.786751
  35688/150000: episode: 353, duration: 0.685s, episode steps:  95, steps per second: 139, episode reward: -232.853, mean reward: -2.451 [-100.000, 56.646], mean action: 1.505 [0.000, 3.000],  loss: 181084845001587.875000, mse: 461095770047842304.000000, mean_q: 633352830.652632, mean_eps: 0.786160
  35785/150000: episode: 354, duration: 0.671s, episode steps:  97, steps per second: 144, episode reward: -228.274, mean reward: -2.353 [-100.000, 12.694], mean action: 1.577 [0.000, 3.000],  loss: 224413492683005.375000, mse: 471727776779187712.000000, mean_q: 634870456.082474, mean_eps: 0.785584
  35913/150000: episode: 355, duration: 0.953s, episode steps: 128, steps per second: 134, episode reward: -399.492, mean reward: -3.121 [-100.000,  2.289], mean action: 1.648 [0.000, 3.000],  loss: 152748462948352.000000, mse: 484318012946513920.000000, mean_q: 646693783.000000, mean_eps: 0.784909
  36011/150000: episode: 356, duration: 0.697s, episode steps:  98, steps per second: 141, episode reward: -439.640, mean reward: -4.486 [-100.000,  0.964], mean action: 1.551 [0.000, 3.000],  loss: 171266417970782.031250, mse: 496268794886592384.000000, mean_q: 655971847.836735, mean_eps: 0.784231
  36100/150000: episode: 357, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: -91.139, mean reward: -1.024 [-100.000, 16.010], mean action: 1.663 [0.000, 3.000],  loss: 423433823793382.125000, mse: 482764511153729408.000000, mean_q: 641562765.303371, mean_eps: 0.783670
  36205/150000: episode: 358, duration: 0.760s, episode steps: 105, steps per second: 138, episode reward: -313.607, mean reward: -2.987 [-100.000, 57.289], mean action: 1.581 [0.000, 3.000],  loss: 349951787055435.562500, mse: 501415510949392448.000000, mean_q: 659020302.323810, mean_eps: 0.783088
  36408/150000: episode: 359, duration: 1.467s, episode steps: 203, steps per second: 138, episode reward: -126.848, mean reward: -0.625 [-100.000,  9.611], mean action: 1.709 [0.000, 3.000],  loss: 156690367438323.375000, mse: 510946128031681856.000000, mean_q: 662602326.699507, mean_eps: 0.782164
  36518/150000: episode: 360, duration: 0.804s, episode steps: 110, steps per second: 137, episode reward: -90.196, mean reward: -0.820 [-100.000,  6.333], mean action: 1.664 [0.000, 3.000],  loss: 161965718329455.718750, mse: 540245003323276736.000000, mean_q: 688354865.163636, mean_eps: 0.781225
  36684/150000: episode: 361, duration: 1.166s, episode steps: 166, steps per second: 142, episode reward: -473.061, mean reward: -2.850 [-100.000, 22.450], mean action: 1.560 [0.000, 3.000],  loss: 225658636851964.906250, mse: 548886400941633088.000000, mean_q: 690833210.409639, mean_eps: 0.780397
  36754/150000: episode: 362, duration: 0.527s, episode steps:  70, steps per second: 133, episode reward: -40.592, mean reward: -0.580 [-100.000,  6.387], mean action: 1.600 [0.000, 3.000],  loss: 553891286512903.312500, mse: 549867021144641664.000000, mean_q: 688798096.457143, mean_eps: 0.779689
  36869/150000: episode: 363, duration: 0.849s, episode steps: 115, steps per second: 135, episode reward: -574.959, mean reward: -5.000 [-100.000,  2.839], mean action: 1.626 [0.000, 3.000],  loss: 433101492332561.812500, mse: 557504920566304640.000000, mean_q: 688598996.591304, mean_eps: 0.779134
  36951/150000: episode: 364, duration: 0.577s, episode steps:  82, steps per second: 142, episode reward: -109.273, mean reward: -1.333 [-100.000,  5.088], mean action: 1.561 [0.000, 3.000],  loss: 184489864196995.125000, mse: 576479446934244096.000000, mean_q: 701949118.439024, mean_eps: 0.778543
  37037/150000: episode: 365, duration: 0.604s, episode steps:  86, steps per second: 142, episode reward: -349.327, mean reward: -4.062 [-100.000,  0.396], mean action: 1.593 [0.000, 3.000],  loss: 218526528758331.531250, mse: 579928541659824896.000000, mean_q: 708002595.720930, mean_eps: 0.778039
  37128/150000: episode: 366, duration: 0.670s, episode steps:  91, steps per second: 136, episode reward: -381.912, mean reward: -4.197 [-100.000, -0.031], mean action: 1.637 [0.000, 3.000],  loss: 233523368816414.937500, mse: 590945217439691648.000000, mean_q: 714616026.021978, mean_eps: 0.777508
  37229/150000: episode: 367, duration: 0.732s, episode steps: 101, steps per second: 138, episode reward: -59.445, mean reward: -0.589 [-100.000,  8.325], mean action: 1.653 [0.000, 3.000],  loss: 400560322710700.375000, mse: 589497432382244096.000000, mean_q: 708430417.108911, mean_eps: 0.776932
  37363/150000: episode: 368, duration: 0.927s, episode steps: 134, steps per second: 145, episode reward: -426.622, mean reward: -3.184 [-100.000,  2.638], mean action: 1.761 [0.000, 3.000],  loss: 175399891753418.500000, mse: 608306792726586752.000000, mean_q: 724768268.179104, mean_eps: 0.776227
  37449/150000: episode: 369, duration: 0.645s, episode steps:  86, steps per second: 133, episode reward: -92.568, mean reward: -1.076 [-100.000,  7.173], mean action: 1.733 [0.000, 3.000],  loss: 364752288844014.125000, mse: 619706070435956224.000000, mean_q: 734818961.116279, mean_eps: 0.775567
  37561/150000: episode: 370, duration: 0.800s, episode steps: 112, steps per second: 140, episode reward: -57.738, mean reward: -0.516 [-100.000, 13.203], mean action: 1.634 [0.000, 3.000],  loss: 246283839246921.156250, mse: 628917639088663936.000000, mean_q: 733298518.857143, mean_eps: 0.774973
  37664/150000: episode: 371, duration: 0.720s, episode steps: 103, steps per second: 143, episode reward: -89.617, mean reward: -0.870 [-100.000,  7.523], mean action: 1.398 [0.000, 3.000],  loss: 187836512780765.218750, mse: 637442289194866560.000000, mean_q: 737669202.640777, mean_eps: 0.774328
  37787/150000: episode: 372, duration: 0.923s, episode steps: 123, steps per second: 133, episode reward: -302.850, mean reward: -2.462 [-100.000,  1.354], mean action: 1.724 [0.000, 3.000],  loss: 176574488093804.218750, mse: 660344903199657344.000000, mean_q: 753470528.000000, mean_eps: 0.773650
  37920/150000: episode: 373, duration: 0.919s, episode steps: 133, steps per second: 145, episode reward: -328.397, mean reward: -2.469 [-100.000,  2.749], mean action: 1.662 [0.000, 3.000],  loss: 224661256605049.250000, mse: 670646716928930048.000000, mean_q: 756117337.984962, mean_eps: 0.772882
  38023/150000: episode: 374, duration: 0.776s, episode steps: 103, steps per second: 133, episode reward: -412.757, mean reward: -4.007 [-100.000,  2.410], mean action: 1.641 [0.000, 3.000],  loss: 702744814221451.125000, mse: 701228293951901952.000000, mean_q: 782383036.893204, mean_eps: 0.772174
  38150/150000: episode: 375, duration: 0.915s, episode steps: 127, steps per second: 139, episode reward: -107.710, mean reward: -0.848 [-100.000,  9.806], mean action: 1.732 [0.000, 3.000],  loss: 446804742301131.562500, mse: 681710919838925312.000000, mean_q: 760757418.330709, mean_eps: 0.771484
  38301/150000: episode: 376, duration: 1.081s, episode steps: 151, steps per second: 140, episode reward: -540.966, mean reward: -3.583 [-100.000,  2.527], mean action: 1.629 [0.000, 3.000],  loss: 640037942969079.500000, mse: 719756337060673152.000000, mean_q: 788697329.165563, mean_eps: 0.770650
  38461/150000: episode: 377, duration: 1.164s, episode steps: 160, steps per second: 137, episode reward: -231.941, mean reward: -1.450 [-100.000,  2.238], mean action: 1.500 [0.000, 3.000],  loss: 275830138653900.812500, mse: 723313299989672704.000000, mean_q: 788858806.800000, mean_eps: 0.769717
  38543/150000: episode: 378, duration: 0.708s, episode steps:  82, steps per second: 116, episode reward: -399.775, mean reward: -4.875 [-100.000,  1.538], mean action: 1.476 [0.000, 3.000],  loss: 538286190986015.250000, mse: 752265797598535040.000000, mean_q: 807970581.853659, mean_eps: 0.768991
  38631/150000: episode: 379, duration: 0.663s, episode steps:  88, steps per second: 133, episode reward: -306.817, mean reward: -3.487 [-100.000, 69.457], mean action: 1.659 [0.000, 3.000],  loss: 362162386036922.187500, mse: 742455793837098496.000000, mean_q: 801174509.818182, mean_eps: 0.768481
  38700/150000: episode: 380, duration: 0.484s, episode steps:  69, steps per second: 143, episode reward: -140.088, mean reward: -2.030 [-100.000, 39.284], mean action: 1.478 [0.000, 3.000],  loss: 320522531221088.437500, mse: 740606521029309440.000000, mean_q: 795639492.637681, mean_eps: 0.768010
  38794/150000: episode: 381, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -183.128, mean reward: -1.948 [-100.000,  7.196], mean action: 1.585 [0.000, 3.000],  loss: 550844625355449.187500, mse: 767826350512621312.000000, mean_q: 815691477.787234, mean_eps: 0.767521
  38867/150000: episode: 382, duration: 0.539s, episode steps:  73, steps per second: 135, episode reward: -184.661, mean reward: -2.530 [-100.000, 19.016], mean action: 1.644 [0.000, 3.000],  loss: 498448273010744.125000, mse: 785029793977208320.000000, mean_q: 827589835.397260, mean_eps: 0.767020
  38977/150000: episode: 383, duration: 0.814s, episode steps: 110, steps per second: 135, episode reward: -248.422, mean reward: -2.258 [-100.000,  3.842], mean action: 1.555 [0.000, 3.000],  loss: 295547479990420.937500, mse: 752783630251856512.000000, mean_q: 802502624.000000, mean_eps: 0.766471
  39076/150000: episode: 384, duration: 0.691s, episode steps:  99, steps per second: 143, episode reward: -246.603, mean reward: -2.491 [-100.000,  2.391], mean action: 1.545 [0.000, 3.000],  loss: 245901086557483.968750, mse: 790802383526958208.000000, mean_q: 823788099.878788, mean_eps: 0.765844
  39210/150000: episode: 385, duration: 0.984s, episode steps: 134, steps per second: 136, episode reward: -73.379, mean reward: -0.548 [-100.000,  9.164], mean action: 1.687 [0.000, 3.000],  loss: 694974206854342.625000, mse: 801275850686765312.000000, mean_q: 826996981.492537, mean_eps: 0.765145
  39320/150000: episode: 386, duration: 0.795s, episode steps: 110, steps per second: 138, episode reward: -260.115, mean reward: -2.365 [-100.000,  0.671], mean action: 1.545 [0.000, 3.000],  loss: 253919366484638.250000, mse: 833957250831497216.000000, mean_q: 847179006.836364, mean_eps: 0.764413
  39403/150000: episode: 387, duration: 0.583s, episode steps:  83, steps per second: 142, episode reward: -337.589, mean reward: -4.067 [-100.000,  0.949], mean action: 1.627 [0.000, 3.000],  loss: 188315860486662.156250, mse: 818955086043975168.000000, mean_q: 838049767.325301, mean_eps: 0.763834
  39504/150000: episode: 388, duration: 0.753s, episode steps: 101, steps per second: 134, episode reward: -428.891, mean reward: -4.246 [-100.000,  0.867], mean action: 1.644 [0.000, 3.000],  loss: 236285655504754.062500, mse: 832185202071890176.000000, mean_q: 837815167.366337, mean_eps: 0.763282
  39597/150000: episode: 389, duration: 0.667s, episode steps:  93, steps per second: 139, episode reward: -326.316, mean reward: -3.509 [-100.000,  0.193], mean action: 1.796 [0.000, 3.000],  loss: 380645802604125.562500, mse: 847764215665000448.000000, mean_q: 847704456.258065, mean_eps: 0.762700
  39692/150000: episode: 390, duration: 0.682s, episode steps:  95, steps per second: 139, episode reward: -51.127, mean reward: -0.538 [-100.000, 23.458], mean action: 1.589 [0.000, 3.000],  loss: 212650456677365.218750, mse: 852821661723532032.000000, mean_q: 848859916.800000, mean_eps: 0.762136
  39904/150000: episode: 391, duration: 1.695s, episode steps: 212, steps per second: 125, episode reward: -269.688, mean reward: -1.272 [-100.000, 26.969], mean action: 1.575 [0.000, 3.000],  loss: 478974832165791.375000, mse: 899275159415638144.000000, mean_q: 881904367.396226, mean_eps: 0.761215
  39995/150000: episode: 392, duration: 0.821s, episode steps:  91, steps per second: 111, episode reward: -76.338, mean reward: -0.839 [-100.000,  9.584], mean action: 1.648 [0.000, 3.000],  loss: 444613895755033.312500, mse: 919230606982659584.000000, mean_q: 887003137.406593, mean_eps: 0.760306
  40074/150000: episode: 393, duration: 1.102s, episode steps:  79, steps per second:  72, episode reward: -78.521, mean reward: -0.994 [-100.000,  7.642], mean action: 1.532 [0.000, 3.000],  loss: 180324946650436.062500, mse: 896934685331721856.000000, mean_q: 871230149.670886, mean_eps: 0.759796
  40160/150000: episode: 394, duration: 0.709s, episode steps:  86, steps per second: 121, episode reward: -333.479, mean reward: -3.878 [-100.000,  1.339], mean action: 1.593 [0.000, 3.000],  loss: 436281590549480.187500, mse: 905999606913860352.000000, mean_q: 873594472.186046, mean_eps: 0.759301
  40310/150000: episode: 395, duration: 1.280s, episode steps: 150, steps per second: 117, episode reward: -80.561, mean reward: -0.537 [-100.000,  9.896], mean action: 1.687 [0.000, 3.000],  loss: 618145796000317.500000, mse: 945703784280804608.000000, mean_q: 902485074.346667, mean_eps: 0.758593
  40437/150000: episode: 396, duration: 1.303s, episode steps: 127, steps per second:  97, episode reward: -383.385, mean reward: -3.019 [-100.000,  4.553], mean action: 1.638 [0.000, 3.000],  loss: 696378762892586.375000, mse: 965814068338014592.000000, mean_q: 910784329.574803, mean_eps: 0.757762
  40527/150000: episode: 397, duration: 0.919s, episode steps:  90, steps per second:  98, episode reward: -47.331, mean reward: -0.526 [-100.000, 10.475], mean action: 1.700 [0.000, 3.000],  loss: 248877461047432.531250, mse: 1015729974847285248.000000, mean_q: 936489654.044444, mean_eps: 0.757111
  40683/150000: episode: 398, duration: 1.289s, episode steps: 156, steps per second: 121, episode reward: -464.864, mean reward: -2.980 [-100.000,  6.083], mean action: 1.724 [0.000, 3.000],  loss: 308011967030298.250000, mse: 1021610263351320320.000000, mean_q: 936473665.641026, mean_eps: 0.756373
  40781/150000: episode: 399, duration: 0.823s, episode steps:  98, steps per second: 119, episode reward: -120.368, mean reward: -1.228 [-100.000,  9.331], mean action: 1.724 [0.000, 3.000],  loss: 195609277625741.062500, mse: 1034139899586958208.000000, mean_q: 941855298.612245, mean_eps: 0.755611
  40867/150000: episode: 400, duration: 0.658s, episode steps:  86, steps per second: 131, episode reward: -128.383, mean reward: -1.493 [-100.000,  7.117], mean action: 1.616 [0.000, 3.000],  loss: 548061726162563.000000, mse: 1079404904984553856.000000, mean_q: 970007873.488372, mean_eps: 0.755059
  40963/150000: episode: 401, duration: 0.862s, episode steps:  96, steps per second: 111, episode reward: -81.435, mean reward: -0.848 [-100.000, 11.727], mean action: 1.542 [0.000, 3.000],  loss: 741458512489130.625000, mse: 1083673326702471808.000000, mean_q: 972863634.666667, mean_eps: 0.754513
  41054/150000: episode: 402, duration: 0.816s, episode steps:  91, steps per second: 112, episode reward: -256.785, mean reward: -2.822 [-100.000,  0.612], mean action: 1.659 [0.000, 3.000],  loss: 274625609971419.437500, mse: 1127102454641436160.000000, mean_q: 992954058.549451, mean_eps: 0.753952
  41201/150000: episode: 403, duration: 1.070s, episode steps: 147, steps per second: 137, episode reward: -324.768, mean reward: -2.209 [-100.000,  2.734], mean action: 1.660 [0.000, 3.000],  loss: 337821009696308.250000, mse: 1140981265991565056.000000, mean_q: 996629608.054422, mean_eps: 0.753238
  41314/150000: episode: 404, duration: 0.875s, episode steps: 113, steps per second: 129, episode reward: -381.492, mean reward: -3.376 [-100.000,  1.145], mean action: 1.655 [0.000, 3.000],  loss: 601973637278910.250000, mse: 1180004062628304384.000000, mean_q: 1010319959.787611, mean_eps: 0.752458
  41393/150000: episode: 405, duration: 0.648s, episode steps:  79, steps per second: 122, episode reward: -65.811, mean reward: -0.833 [-100.000, 11.133], mean action: 1.722 [0.000, 3.000],  loss: 949030831063869.625000, mse: 1183698562623750400.000000, mean_q: 1008012699.544304, mean_eps: 0.751882
  41473/150000: episode: 406, duration: 0.699s, episode steps:  80, steps per second: 114, episode reward: -180.593, mean reward: -2.257 [-100.000,  5.880], mean action: 1.650 [0.000, 3.000],  loss: 618257105603788.750000, mse: 1196884748617016832.000000, mean_q: 1014341696.800000, mean_eps: 0.751405
  41664/150000: episode: 407, duration: 1.478s, episode steps: 191, steps per second: 129, episode reward: -184.147, mean reward: -0.964 [-100.000,  6.034], mean action: 1.623 [0.000, 3.000],  loss: 512569262544585.062500, mse: 1228112236541234432.000000, mean_q: 1029458873.298429, mean_eps: 0.750592
  41786/150000: episode: 408, duration: 0.929s, episode steps: 122, steps per second: 131, episode reward: -61.697, mean reward: -0.506 [-100.000,  8.530], mean action: 1.525 [0.000, 3.000],  loss: 861279428211560.875000, mse: 1274014072783043584.000000, mean_q: 1052146944.000000, mean_eps: 0.749653
  41938/150000: episode: 409, duration: 1.339s, episode steps: 152, steps per second: 114, episode reward: -315.365, mean reward: -2.075 [-100.000,  6.233], mean action: 1.645 [0.000, 3.000],  loss: 849542053915594.125000, mse: 1289668973769412864.000000, mean_q: 1057367064.842105, mean_eps: 0.748831
  42078/150000: episode: 410, duration: 1.230s, episode steps: 140, steps per second: 114, episode reward: -354.350, mean reward: -2.531 [-100.000,  4.615], mean action: 1.693 [0.000, 3.000],  loss: 811120425083114.000000, mse: 1359883364650278656.000000, mean_q: 1093248297.600000, mean_eps: 0.747955
  42191/150000: episode: 411, duration: 0.923s, episode steps: 113, steps per second: 122, episode reward: -15.037, mean reward: -0.133 [-100.000, 11.166], mean action: 1.743 [0.000, 3.000],  loss: 944818322818039.000000, mse: 1394908915663982336.000000, mean_q: 1105484552.495575, mean_eps: 0.747196
  42296/150000: episode: 412, duration: 0.817s, episode steps: 105, steps per second: 129, episode reward: -274.018, mean reward: -2.610 [-100.000,  0.607], mean action: 1.410 [0.000, 3.000],  loss: 350815957783737.312500, mse: 1363988878566394112.000000, mean_q: 1088632265.142857, mean_eps: 0.746542
  42429/150000: episode: 413, duration: 1.032s, episode steps: 133, steps per second: 129, episode reward: -92.151, mean reward: -0.693 [-100.000,  9.454], mean action: 1.677 [0.000, 3.000],  loss: 668066134344296.000000, mse: 1444395344989943552.000000, mean_q: 1123933408.721805, mean_eps: 0.745828
  42522/150000: episode: 414, duration: 0.704s, episode steps:  93, steps per second: 132, episode reward: -397.841, mean reward: -4.278 [-100.000,  0.377], mean action: 1.613 [0.000, 3.000],  loss: 803612480605283.125000, mse: 1462858121578897408.000000, mean_q: 1129846391.053763, mean_eps: 0.745150
  42631/150000: episode: 415, duration: 0.886s, episode steps: 109, steps per second: 123, episode reward: -294.626, mean reward: -2.703 [-100.000,  1.167], mean action: 1.596 [0.000, 3.000],  loss: 641287899364323.875000, mse: 1504292907074751744.000000, mean_q: 1147893417.100917, mean_eps: 0.744544
  42850/150000: episode: 416, duration: 1.629s, episode steps: 219, steps per second: 134, episode reward: -55.317, mean reward: -0.253 [-100.000, 16.621], mean action: 1.584 [0.000, 3.000],  loss: 1086012170873514.625000, mse: 1541184170283079424.000000, mean_q: 1157524286.246575, mean_eps: 0.743560
  42934/150000: episode: 417, duration: 0.665s, episode steps:  84, steps per second: 126, episode reward: -399.115, mean reward: -4.751 [-100.000,  1.339], mean action: 1.536 [0.000, 3.000],  loss: 1386023545558357.250000, mse: 1615541444127527424.000000, mean_q: 1188390300.952381, mean_eps: 0.742651
  43036/150000: episode: 418, duration: 0.745s, episode steps: 102, steps per second: 137, episode reward: -96.773, mean reward: -0.949 [-100.000, 12.408], mean action: 1.647 [0.000, 3.000],  loss: 2402953799524834.000000, mse: 1604700211258527488.000000, mean_q: 1182800422.274510, mean_eps: 0.742093
  43157/150000: episode: 419, duration: 1.068s, episode steps: 121, steps per second: 113, episode reward: -42.934, mean reward: -0.355 [-100.000, 29.549], mean action: 1.521 [0.000, 3.000],  loss: 902516590367092.375000, mse: 1662208554948110848.000000, mean_q: 1206096892.826446, mean_eps: 0.741424
  43227/150000: episode: 420, duration: 0.597s, episode steps:  70, steps per second: 117, episode reward: -30.549, mean reward: -0.436 [-100.000, 14.878], mean action: 1.529 [0.000, 3.000],  loss: 747769792763377.375000, mse: 1679317669731309056.000000, mean_q: 1225617139.200000, mean_eps: 0.740851
  43371/150000: episode: 421, duration: 1.188s, episode steps: 144, steps per second: 121, episode reward: -334.614, mean reward: -2.324 [-100.000,  2.361], mean action: 1.660 [0.000, 3.000],  loss: 466265697564899.562500, mse: 1715827092924865536.000000, mean_q: 1227430583.555556, mean_eps: 0.740209
  43456/150000: episode: 422, duration: 0.723s, episode steps:  85, steps per second: 118, episode reward: -296.110, mean reward: -3.484 [-100.000,  5.420], mean action: 1.624 [0.000, 3.000],  loss: 596565499391277.125000, mse: 1760449637063204352.000000, mean_q: 1242786797.929412, mean_eps: 0.739522
  43536/150000: episode: 423, duration: 0.647s, episode steps:  80, steps per second: 124, episode reward: -26.865, mean reward: -0.336 [-100.000, 11.989], mean action: 1.538 [0.000, 3.000],  loss: 991511254768025.625000, mse: 1748620520094642944.000000, mean_q: 1240040359.200000, mean_eps: 0.739027
  43643/150000: episode: 424, duration: 0.833s, episode steps: 107, steps per second: 128, episode reward: -83.602, mean reward: -0.781 [-100.000, 44.655], mean action: 1.598 [0.000, 3.000],  loss: 1029017694991675.875000, mse: 1784778052119168256.000000, mean_q: 1243313113.719626, mean_eps: 0.738466
  43738/150000: episode: 425, duration: 0.771s, episode steps:  95, steps per second: 123, episode reward: -44.709, mean reward: -0.471 [-100.000,  5.894], mean action: 1.642 [0.000, 3.000],  loss: 1167049895524503.000000, mse: 1781987997231788544.000000, mean_q: 1245926216.757895, mean_eps: 0.737860
  43810/150000: episode: 426, duration: 0.561s, episode steps:  72, steps per second: 128, episode reward: -190.293, mean reward: -2.643 [-100.000, 12.869], mean action: 1.528 [0.000, 3.000],  loss: 1985686366322688.000000, mse: 1849878574194083584.000000, mean_q: 1278877037.333333, mean_eps: 0.737359
  43954/150000: episode: 427, duration: 1.107s, episode steps: 144, steps per second: 130, episode reward: -71.182, mean reward: -0.494 [-100.000,  7.396], mean action: 1.639 [0.000, 3.000],  loss: 499221398669084.437500, mse: 1860761991734595072.000000, mean_q: 1276496980.444444, mean_eps: 0.736711
  44046/150000: episode: 428, duration: 0.702s, episode steps:  92, steps per second: 131, episode reward: -232.110, mean reward: -2.523 [-100.000,  1.253], mean action: 1.793 [0.000, 3.000],  loss: 688466749224158.625000, mse: 1934018405800106240.000000, mean_q: 1307793874.086957, mean_eps: 0.736003
  44192/150000: episode: 429, duration: 1.117s, episode steps: 146, steps per second: 131, episode reward: -297.152, mean reward: -2.035 [-100.000, 25.432], mean action: 1.644 [0.000, 3.000],  loss: 860649887122979.125000, mse: 1994218982170316032.000000, mean_q: 1326087848.328767, mean_eps: 0.735289
  44272/150000: episode: 430, duration: 0.632s, episode steps:  80, steps per second: 127, episode reward: -301.616, mean reward: -3.770 [-100.000,  0.235], mean action: 1.788 [0.000, 3.000],  loss: 1041148959680102.375000, mse: 2067253306723401728.000000, mean_q: 1358388963.200000, mean_eps: 0.734611
  44426/150000: episode: 431, duration: 1.170s, episode steps: 154, steps per second: 132, episode reward: -465.496, mean reward: -3.023 [-100.000,  3.812], mean action: 1.747 [0.000, 3.000],  loss: 1731422778723235.000000, mse: 2045201033532627456.000000, mean_q: 1340286981.402597, mean_eps: 0.733909
  44615/150000: episode: 432, duration: 1.531s, episode steps: 189, steps per second: 123, episode reward: -56.970, mean reward: -0.301 [-100.000, 40.490], mean action: 1.709 [0.000, 3.000],  loss: 1298431289842195.000000, mse: 2154417369029259008.000000, mean_q: 1381537334.179894, mean_eps: 0.732880
  44806/150000: episode: 433, duration: 1.544s, episode steps: 191, steps per second: 124, episode reward: -50.767, mean reward: -0.266 [-100.000, 58.902], mean action: 1.613 [0.000, 3.000],  loss: 1747703232844398.000000, mse: 2227660782097398528.000000, mean_q: 1404503075.518325, mean_eps: 0.731740
  44900/150000: episode: 434, duration: 0.701s, episode steps:  94, steps per second: 134, episode reward: -406.944, mean reward: -4.329 [-100.000, -0.128], mean action: 1.457 [0.000, 3.000],  loss: 2325675019387926.000000, mse: 2322369188030534144.000000, mean_q: 1440069260.936170, mean_eps: 0.730885
  45003/150000: episode: 435, duration: 0.759s, episode steps: 103, steps per second: 136, episode reward: -381.503, mean reward: -3.704 [-100.000,  1.052], mean action: 1.602 [0.000, 3.000],  loss: 1963472572721728.500000, mse: 2314029609848528384.000000, mean_q: 1430274002.640777, mean_eps: 0.730294
  45096/150000: episode: 436, duration: 0.761s, episode steps:  93, steps per second: 122, episode reward: -103.376, mean reward: -1.112 [-100.000, 17.963], mean action: 1.548 [0.000, 3.000],  loss: 2206641262929898.000000, mse: 2417140559447622656.000000, mean_q: 1470533066.322581, mean_eps: 0.729706
  45224/150000: episode: 437, duration: 0.999s, episode steps: 128, steps per second: 128, episode reward: -206.002, mean reward: -1.609 [-100.000, 26.205], mean action: 1.562 [0.000, 3.000],  loss: 2071305855434752.000000, mse: 2431267266269020160.000000, mean_q: 1473466244.000000, mean_eps: 0.729043
  45410/150000: episode: 438, duration: 1.530s, episode steps: 186, steps per second: 122, episode reward: -430.708, mean reward: -2.316 [-100.000, 54.960], mean action: 1.624 [0.000, 3.000],  loss: 1197128822492611.500000, mse: 2540545997490421248.000000, mean_q: 1512112125.935484, mean_eps: 0.728101
  45490/150000: episode: 439, duration: 0.573s, episode steps:  80, steps per second: 140, episode reward: -118.640, mean reward: -1.483 [-100.000,  8.371], mean action: 1.650 [0.000, 3.000],  loss: 1151062664177254.500000, mse: 2578077487482063872.000000, mean_q: 1525241057.600000, mean_eps: 0.727303
  45591/150000: episode: 440, duration: 0.795s, episode steps: 101, steps per second: 127, episode reward: -149.626, mean reward: -1.481 [-100.000, 15.886], mean action: 1.426 [0.000, 3.000],  loss: 960371312609989.750000, mse: 2599517553233597952.000000, mean_q: 1523491687.920792, mean_eps: 0.726760
  45714/150000: episode: 441, duration: 0.993s, episode steps: 123, steps per second: 124, episode reward: -500.883, mean reward: -4.072 [-100.000,  2.750], mean action: 1.472 [0.000, 3.000],  loss: 1837251294509372.250000, mse: 2690862303386018304.000000, mean_q: 1557608273.170732, mean_eps: 0.726088
  45842/150000: episode: 442, duration: 0.967s, episode steps: 128, steps per second: 132, episode reward: -315.522, mean reward: -2.465 [-100.000,  7.207], mean action: 1.688 [0.000, 3.000],  loss: 1097691735523328.000000, mse: 2757597091491479552.000000, mean_q: 1579294864.000000, mean_eps: 0.725335
  45926/150000: episode: 443, duration: 0.700s, episode steps:  84, steps per second: 120, episode reward: -65.404, mean reward: -0.779 [-100.000, 59.680], mean action: 1.726 [0.000, 3.000],  loss: 2707773669294860.000000, mse: 2793681036555221504.000000, mean_q: 1589756064.000000, mean_eps: 0.724699
  46043/150000: episode: 444, duration: 0.903s, episode steps: 117, steps per second: 130, episode reward: -432.507, mean reward: -3.697 [-100.000,  1.300], mean action: 1.624 [0.000, 3.000],  loss: 1702132131516661.000000, mse: 2843037615792342528.000000, mean_q: 1594140226.735043, mean_eps: 0.724096
  46124/150000: episode: 445, duration: 0.582s, episode steps:  81, steps per second: 139, episode reward: -152.923, mean reward: -1.888 [-100.000,  6.829], mean action: 1.778 [0.000, 3.000],  loss: 2224695142524397.000000, mse: 2951686348964852736.000000, mean_q: 1636893394.172839, mean_eps: 0.723502
  46209/150000: episode: 446, duration: 0.641s, episode steps:  85, steps per second: 133, episode reward: -463.776, mean reward: -5.456 [-100.000,  0.044], mean action: 1.671 [0.000, 3.000],  loss: 2425160171029636.500000, mse: 2836523557564294656.000000, mean_q: 1586550351.811765, mean_eps: 0.723004
  46288/150000: episode: 447, duration: 0.563s, episode steps:  79, steps per second: 140, episode reward: -384.667, mean reward: -4.869 [-100.000, -0.146], mean action: 1.532 [0.000, 3.000],  loss: 1884263255375872.000000, mse: 2932754174061519872.000000, mean_q: 1617070460.759494, mean_eps: 0.722512
  46429/150000: episode: 448, duration: 1.311s, episode steps: 141, steps per second: 108, episode reward: -563.302, mean reward: -3.995 [-100.000,  5.281], mean action: 1.539 [0.000, 3.000],  loss: 2317172553644453.000000, mse: 3005091637746243584.000000, mean_q: 1648622354.156028, mean_eps: 0.721852
  46560/150000: episode: 449, duration: 1.037s, episode steps: 131, steps per second: 126, episode reward: -392.973, mean reward: -3.000 [-100.000,  2.789], mean action: 1.695 [0.000, 3.000],  loss: 2503979732764125.000000, mse: 3118903207702584832.000000, mean_q: 1675829213.801527, mean_eps: 0.721036
  46659/150000: episode: 450, duration: 0.777s, episode steps:  99, steps per second: 127, episode reward: -379.693, mean reward: -3.835 [-100.000,  0.704], mean action: 1.737 [0.000, 3.000],  loss: 1412895317171975.750000, mse: 3117781373133798912.000000, mean_q: 1670082602.666667, mean_eps: 0.720346
  46784/150000: episode: 451, duration: 1.011s, episode steps: 125, steps per second: 124, episode reward: -232.035, mean reward: -1.856 [-100.000,  1.648], mean action: 1.640 [0.000, 3.000],  loss: 2823393504888094.500000, mse: 3253935322801778176.000000, mean_q: 1714143102.976000, mean_eps: 0.719674
  46868/150000: episode: 452, duration: 0.684s, episode steps:  84, steps per second: 123, episode reward: -395.956, mean reward: -4.714 [-100.000,  0.342], mean action: 1.476 [0.000, 3.000],  loss: 2188564924702330.000000, mse: 3384264032374438912.000000, mean_q: 1747147082.666667, mean_eps: 0.719047
  46969/150000: episode: 453, duration: 0.867s, episode steps: 101, steps per second: 116, episode reward: -37.082, mean reward: -0.367 [-100.000, 10.564], mean action: 1.525 [0.000, 3.000],  loss: 2621492629296037.000000, mse: 3395626188044329472.000000, mean_q: 1745119357.465347, mean_eps: 0.718492
  47050/150000: episode: 454, duration: 0.593s, episode steps:  81, steps per second: 137, episode reward: -343.339, mean reward: -4.239 [-100.000, -0.075], mean action: 1.778 [0.000, 3.000],  loss: 1099953106751842.000000, mse: 3469075682962508288.000000, mean_q: 1770217411.950617, mean_eps: 0.717946
  47185/150000: episode: 455, duration: 0.988s, episode steps: 135, steps per second: 137, episode reward: -181.401, mean reward: -1.344 [-100.000, 45.731], mean action: 1.511 [0.000, 3.000],  loss: 2470691572955363.500000, mse: 3533578376841840128.000000, mean_q: 1789443559.348148, mean_eps: 0.717298
  47293/150000: episode: 456, duration: 0.868s, episode steps: 108, steps per second: 124, episode reward: -363.994, mean reward: -3.370 [-100.000,  1.246], mean action: 1.694 [0.000, 3.000],  loss: 1228669242106766.250000, mse: 3581947688198631936.000000, mean_q: 1800559985.777778, mean_eps: 0.716569
  47380/150000: episode: 457, duration: 0.934s, episode steps:  87, steps per second:  93, episode reward: -347.333, mean reward: -3.992 [-100.000,  0.117], mean action: 1.690 [0.000, 3.000],  loss: 3243831045013692.500000, mse: 3692565358284871168.000000, mean_q: 1822397363.494253, mean_eps: 0.715984
  47477/150000: episode: 458, duration: 0.712s, episode steps:  97, steps per second: 136, episode reward: -126.369, mean reward: -1.303 [-100.000, 13.257], mean action: 1.505 [0.000, 3.000],  loss: 4412977296933202.000000, mse: 3695926054759683584.000000, mean_q: 1819748990.680412, mean_eps: 0.715432
  47585/150000: episode: 459, duration: 1.022s, episode steps: 108, steps per second: 106, episode reward: -391.594, mean reward: -3.626 [-100.000,  0.756], mean action: 1.463 [0.000, 3.000],  loss: 2216539094389494.500000, mse: 3738904609605268480.000000, mean_q: 1842431863.703704, mean_eps: 0.714817
  47678/150000: episode: 460, duration: 0.675s, episode steps:  93, steps per second: 138, episode reward: -375.633, mean reward: -4.039 [-100.000,  0.461], mean action: 1.613 [0.000, 3.000],  loss: 2986931827352168.500000, mse: 3937364918429320192.000000, mean_q: 1896868576.344086, mean_eps: 0.714214
  47760/150000: episode: 461, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -470.834, mean reward: -5.742 [-100.000,  0.270], mean action: 1.744 [0.000, 3.000],  loss: 1384993530717558.750000, mse: 3952760030234468352.000000, mean_q: 1882665262.829268, mean_eps: 0.713689
  47862/150000: episode: 462, duration: 0.779s, episode steps: 102, steps per second: 131, episode reward: -441.284, mean reward: -4.326 [-100.000,  2.573], mean action: 1.529 [0.000, 3.000],  loss: 2609337431207855.500000, mse: 4138219789188930048.000000, mean_q: 1941231545.725490, mean_eps: 0.713137
  47973/150000: episode: 463, duration: 0.832s, episode steps: 111, steps per second: 133, episode reward: -33.026, mean reward: -0.298 [-100.000, 85.632], mean action: 1.685 [0.000, 3.000],  loss: 2734599151479614.500000, mse: 4113780723767917056.000000, mean_q: 1929520626.162162, mean_eps: 0.712498
  48104/150000: episode: 464, duration: 1.062s, episode steps: 131, steps per second: 123, episode reward: -50.140, mean reward: -0.383 [-100.000, 12.225], mean action: 1.634 [0.000, 3.000],  loss: 3802685189938637.000000, mse: 4269619971173754368.000000, mean_q: 1975461031.083969, mean_eps: 0.711772
  48218/150000: episode: 465, duration: 1.021s, episode steps: 114, steps per second: 112, episode reward: -267.107, mean reward: -2.343 [-100.000, 83.354], mean action: 1.658 [0.000, 3.000],  loss: 2667669683736504.000000, mse: 4180402743983302656.000000, mean_q: 1939151014.175439, mean_eps: 0.711037
  48371/150000: episode: 466, duration: 1.416s, episode steps: 153, steps per second: 108, episode reward: -290.588, mean reward: -1.899 [-100.000,  3.501], mean action: 1.680 [0.000, 3.000],  loss: 2153467560085919.000000, mse: 4321922987743711232.000000, mean_q: 1977413319.111111, mean_eps: 0.710236
  48454/150000: episode: 467, duration: 0.646s, episode steps:  83, steps per second: 129, episode reward: -439.307, mean reward: -5.293 [-100.000, -0.533], mean action: 1.831 [0.000, 3.000],  loss: 3330896397055482.000000, mse: 4457283312730104320.000000, mean_q: 2009215073.156626, mean_eps: 0.709528
  48598/150000: episode: 468, duration: 1.223s, episode steps: 144, steps per second: 118, episode reward: -594.772, mean reward: -4.130 [-100.000,  2.147], mean action: 1.729 [0.000, 3.000],  loss: 3364012972339655.000000, mse: 4574955051521758720.000000, mean_q: 2044769993.777778, mean_eps: 0.708847
  48780/150000: episode: 469, duration: 1.424s, episode steps: 182, steps per second: 128, episode reward: -238.150, mean reward: -1.309 [-100.000,  4.697], mean action: 1.626 [0.000, 3.000],  loss: 7510792734619355.000000, mse: 4620119757424562176.000000, mean_q: 2042066602.197802, mean_eps: 0.707869
  48876/150000: episode: 470, duration: 0.796s, episode steps:  96, steps per second: 121, episode reward: -79.945, mean reward: -0.833 [-100.000, 62.161], mean action: 1.677 [0.000, 3.000],  loss: 2020447071764480.000000, mse: 4845326297958362112.000000, mean_q: 2097010453.333333, mean_eps: 0.707035
  48985/150000: episode: 471, duration: 0.839s, episode steps: 109, steps per second: 130, episode reward: -279.349, mean reward: -2.563 [-100.000,  1.600], mean action: 1.661 [0.000, 3.000],  loss: 2661858649532848.000000, mse: 4792872519594251264.000000, mean_q: 2088250624.000000, mean_eps: 0.706420
  49151/150000: episode: 472, duration: 1.272s, episode steps: 166, steps per second: 130, episode reward: -470.332, mean reward: -2.833 [-100.000,  2.668], mean action: 1.639 [0.000, 3.000],  loss: 2218120762344016.250000, mse: 5087362748064301056.000000, mean_q: 2156340344.289156, mean_eps: 0.705595
  49265/150000: episode: 473, duration: 0.861s, episode steps: 114, steps per second: 132, episode reward: -196.997, mean reward: -1.728 [-100.000, 30.804], mean action: 1.632 [0.000, 3.000],  loss: 3343669059598947.000000, mse: 5207448557506255872.000000, mean_q: 2175245388.350877, mean_eps: 0.704755
  49360/150000: episode: 474, duration: 0.705s, episode steps:  95, steps per second: 135, episode reward: -296.051, mean reward: -3.116 [-100.000,  1.201], mean action: 1.589 [0.000, 3.000],  loss: 3034729426797406.500000, mse: 5142706038440018944.000000, mean_q: 2157521643.789474, mean_eps: 0.704128
  49483/150000: episode: 475, duration: 0.954s, episode steps: 123, steps per second: 129, episode reward: -247.631, mean reward: -2.013 [-100.000,  1.727], mean action: 1.691 [0.000, 3.000],  loss: 8505253748543655.000000, mse: 5256278815501433856.000000, mean_q: 2195951010.341464, mean_eps: 0.703474
  49600/150000: episode: 476, duration: 0.854s, episode steps: 117, steps per second: 137, episode reward: -53.703, mean reward: -0.459 [-100.000, 13.018], mean action: 1.598 [0.000, 3.000],  loss: 5424811580814327.000000, mse: 5517626333484198912.000000, mean_q: 2254724923.076923, mean_eps: 0.702754
  49725/150000: episode: 477, duration: 1.020s, episode steps: 125, steps per second: 123, episode reward: -64.166, mean reward: -0.513 [-100.000, 14.446], mean action: 1.480 [0.000, 3.000],  loss: 9185790521514132.000000, mse: 5308104599827916800.000000, mean_q: 2185297623.040000, mean_eps: 0.702028
  49831/150000: episode: 478, duration: 0.815s, episode steps: 106, steps per second: 130, episode reward: -190.585, mean reward: -1.798 [-100.000,  8.830], mean action: 1.594 [0.000, 3.000],  loss: 2886424004390062.000000, mse: 5478767913037295616.000000, mean_q: 2223660102.037736, mean_eps: 0.701335
  49926/150000: episode: 479, duration: 0.711s, episode steps:  95, steps per second: 134, episode reward: -287.820, mean reward: -3.030 [-100.000,  0.630], mean action: 1.653 [0.000, 3.000],  loss: 5144864268774346.000000, mse: 5311869904016991232.000000, mean_q: 2175135071.663158, mean_eps: 0.700732
  50014/150000: episode: 480, duration: 0.673s, episode steps:  88, steps per second: 131, episode reward: -366.517, mean reward: -4.165 [-100.000,  1.277], mean action: 1.557 [0.000, 3.000],  loss: 4839406153903756.000000, mse: 5517462966463912960.000000, mean_q: 2235635419.636364, mean_eps: 0.700183
  50168/150000: episode: 481, duration: 1.125s, episode steps: 154, steps per second: 137, episode reward: -344.743, mean reward: -2.239 [-100.000,  2.387], mean action: 1.591 [0.000, 3.000],  loss: 5310800430012655.000000, mse: 5659421825713405952.000000, mean_q: 2269629231.376623, mean_eps: 0.699457
  50272/150000: episode: 482, duration: 0.809s, episode steps: 104, steps per second: 129, episode reward: -195.514, mean reward: -1.880 [-100.000,  5.666], mean action: 1.683 [0.000, 3.000],  loss: 3474520934691131.000000, mse: 5727253563413404672.000000, mean_q: 2275309227.076923, mean_eps: 0.698683
  50440/150000: episode: 483, duration: 1.266s, episode steps: 168, steps per second: 133, episode reward: -286.444, mean reward: -1.705 [-100.000,  2.768], mean action: 1.482 [0.000, 3.000],  loss: 8679576399935976.000000, mse: 5812486897281818624.000000, mean_q: 2297020100.571429, mean_eps: 0.697867
  50532/150000: episode: 484, duration: 0.790s, episode steps:  92, steps per second: 116, episode reward: -351.061, mean reward: -3.816 [-100.000,  2.517], mean action: 1.674 [0.000, 3.000],  loss: 2505505667672509.000000, mse: 5983197479876370432.000000, mean_q: 2339281448.347826, mean_eps: 0.697087
  50653/150000: episode: 485, duration: 0.952s, episode steps: 121, steps per second: 127, episode reward: -131.504, mean reward: -1.087 [-100.000, 10.467], mean action: 1.554 [0.000, 3.000],  loss: 9786895693077242.000000, mse: 6025492740592913408.000000, mean_q: 2336890325.685950, mean_eps: 0.696448
  50806/150000: episode: 486, duration: 1.236s, episode steps: 153, steps per second: 124, episode reward: -279.204, mean reward: -1.825 [-100.000,  8.090], mean action: 1.686 [0.000, 3.000],  loss: 3177254947773232.500000, mse: 6118034659241335808.000000, mean_q: 2364153446.065360, mean_eps: 0.695626
  50949/150000: episode: 487, duration: 1.144s, episode steps: 143, steps per second: 125, episode reward: -299.919, mean reward: -2.097 [-100.000,  6.829], mean action: 1.573 [0.000, 3.000],  loss: 6400184680491996.000000, mse: 6283760635553014784.000000, mean_q: 2390967938.685315, mean_eps: 0.694738
  51040/150000: episode: 488, duration: 0.734s, episode steps:  91, steps per second: 124, episode reward: -385.122, mean reward: -4.232 [-100.000,  0.917], mean action: 1.571 [0.000, 3.000],  loss: 4583631359176479.000000, mse: 6133281707039193088.000000, mean_q: 2357464014.769231, mean_eps: 0.694036
  51171/150000: episode: 489, duration: 1.004s, episode steps: 131, steps per second: 131, episode reward: -361.890, mean reward: -2.763 [-100.000,  1.465], mean action: 1.725 [0.000, 3.000],  loss: 8030327393336719.000000, mse: 6396248055340138496.000000, mean_q: 2417041995.236641, mean_eps: 0.693370
  51339/150000: episode: 490, duration: 1.337s, episode steps: 168, steps per second: 126, episode reward: -154.976, mean reward: -0.922 [-100.000,  6.538], mean action: 1.518 [0.000, 3.000],  loss: 5507553646658316.000000, mse: 6401158287420754944.000000, mean_q: 2413220569.904762, mean_eps: 0.692473
  51424/150000: episode: 491, duration: 0.800s, episode steps:  85, steps per second: 106, episode reward: -447.624, mean reward: -5.266 [-100.000,  0.317], mean action: 1.565 [0.000, 3.000],  loss: 4283638948289500.000000, mse: 6424095163299382272.000000, mean_q: 2422865127.905882, mean_eps: 0.691714
  51595/150000: episode: 492, duration: 1.645s, episode steps: 171, steps per second: 104, episode reward: -495.784, mean reward: -2.899 [-100.000,  7.918], mean action: 1.673 [0.000, 3.000],  loss: 4794242077556736.000000, mse: 6715691228037762048.000000, mean_q: 2472526886.923976, mean_eps: 0.690946
  51659/150000: episode: 493, duration: 0.606s, episode steps:  64, steps per second: 106, episode reward: -323.349, mean reward: -5.052 [-100.000,  5.773], mean action: 1.891 [0.000, 3.000],  loss: 9501209072238592.000000, mse: 6704015946975019008.000000, mean_q: 2471737062.000000, mean_eps: 0.690241
  51832/150000: episode: 494, duration: 1.519s, episode steps: 173, steps per second: 114, episode reward: -442.046, mean reward: -2.555 [-100.000,  2.049], mean action: 1.699 [0.000, 3.000],  loss: 5383640259983324.000000, mse: 6931288063517618176.000000, mean_q: 2517580315.375722, mean_eps: 0.689530
  52004/150000: episode: 495, duration: 1.565s, episode steps: 172, steps per second: 110, episode reward: -222.627, mean reward: -1.294 [-100.000, 22.327], mean action: 1.791 [0.000, 3.000],  loss: 11680029254200486.000000, mse: 7103621118138046464.000000, mean_q: 2549974585.302326, mean_eps: 0.688495
  52105/150000: episode: 496, duration: 0.839s, episode steps: 101, steps per second: 120, episode reward: -310.390, mean reward: -3.073 [-100.000,  9.134], mean action: 1.762 [0.000, 3.000],  loss: 2626832329446795.500000, mse: 7046433098306246656.000000, mean_q: 2542013209.346535, mean_eps: 0.687676
  52227/150000: episode: 497, duration: 0.953s, episode steps: 122, steps per second: 128, episode reward: -345.458, mean reward: -2.832 [-100.000,  0.799], mean action: 1.648 [0.000, 3.000],  loss: 6337781286224141.000000, mse: 7342158516673800192.000000, mean_q: 2595854445.114754, mean_eps: 0.687007
  52378/150000: episode: 498, duration: 1.164s, episode steps: 151, steps per second: 130, episode reward: -303.071, mean reward: -2.007 [-100.000,  2.315], mean action: 1.675 [0.000, 3.000],  loss: 7721000059672508.000000, mse: 7478950004561728512.000000, mean_q: 2621007606.675497, mean_eps: 0.686188
  52546/150000: episode: 499, duration: 1.235s, episode steps: 168, steps per second: 136, episode reward: -19.524, mean reward: -0.116 [-100.000, 14.604], mean action: 1.500 [0.000, 3.000],  loss: 8723583169924145.000000, mse: 7492821817173188608.000000, mean_q: 2607442662.857143, mean_eps: 0.685231
  52676/150000: episode: 500, duration: 1.019s, episode steps: 130, steps per second: 128, episode reward: -601.106, mean reward: -4.624 [-100.000,  1.887], mean action: 1.638 [0.000, 3.000],  loss: 5123763379851957.000000, mse: 7536787373530820608.000000, mean_q: 2618020859.076923, mean_eps: 0.684337
  52793/150000: episode: 501, duration: 0.878s, episode steps: 117, steps per second: 133, episode reward: -301.847, mean reward: -2.580 [-100.000, 58.116], mean action: 1.590 [0.000, 3.000],  loss: 7869356007749439.000000, mse: 7713998537285520384.000000, mean_q: 2650898994.324786, mean_eps: 0.683596
  52903/150000: episode: 502, duration: 0.850s, episode steps: 110, steps per second: 129, episode reward: -362.926, mean reward: -3.299 [-100.000,  1.315], mean action: 1.545 [0.000, 3.000],  loss: 6530653300190562.000000, mse: 7881168885845952512.000000, mean_q: 2688277613.381818, mean_eps: 0.682915
  52999/150000: episode: 503, duration: 0.717s, episode steps:  96, steps per second: 134, episode reward: -257.809, mean reward: -2.686 [-100.000,  0.863], mean action: 1.667 [0.000, 3.000],  loss: 9463286798308694.000000, mse: 7796905714433130496.000000, mean_q: 2658259222.666667, mean_eps: 0.682297
  53126/150000: episode: 504, duration: 0.950s, episode steps: 127, steps per second: 134, episode reward: -58.325, mean reward: -0.459 [-100.000, 10.616], mean action: 1.669 [0.000, 3.000],  loss: 6439164615532092.000000, mse: 7857115412165211136.000000, mean_q: 2677643678.236220, mean_eps: 0.681628
  53217/150000: episode: 505, duration: 0.730s, episode steps:  91, steps per second: 125, episode reward: -239.306, mean reward: -2.630 [-100.000,  0.937], mean action: 1.637 [0.000, 3.000],  loss: 8943947417389889.000000, mse: 8200398645907890176.000000, mean_q: 2734185955.868132, mean_eps: 0.680974
  53367/150000: episode: 506, duration: 1.121s, episode steps: 150, steps per second: 134, episode reward: -161.798, mean reward: -1.079 [-100.000,  4.714], mean action: 1.727 [0.000, 3.000],  loss: 5516050578031684.000000, mse: 8168642850641038336.000000, mean_q: 2739958512.640000, mean_eps: 0.680251
  53474/150000: episode: 507, duration: 0.820s, episode steps: 107, steps per second: 130, episode reward: -337.240, mean reward: -3.152 [-100.000,  0.918], mean action: 1.617 [0.000, 3.000],  loss: 3817590935173790.000000, mse: 8384823605892505600.000000, mean_q: 2769049616.747663, mean_eps: 0.679480
  53600/150000: episode: 508, duration: 0.965s, episode steps: 126, steps per second: 131, episode reward: -218.976, mean reward: -1.738 [-100.000,  1.781], mean action: 1.611 [0.000, 3.000],  loss: 5467683186827654.000000, mse: 8594317014101822464.000000, mean_q: 2811181761.015873, mean_eps: 0.678781
  53701/150000: episode: 509, duration: 0.746s, episode steps: 101, steps per second: 135, episode reward: -205.544, mean reward: -2.035 [-100.000,  1.872], mean action: 1.673 [0.000, 3.000],  loss: 12266035486252164.000000, mse: 8747273780603177984.000000, mean_q: 2837608351.683168, mean_eps: 0.678100
  53814/150000: episode: 510, duration: 0.893s, episode steps: 113, steps per second: 127, episode reward: -424.957, mean reward: -3.761 [-100.000,  0.418], mean action: 1.743 [0.000, 3.000],  loss: 13019568052276342.000000, mse: 8847911745165296640.000000, mean_q: 2845092057.486726, mean_eps: 0.677458
  53903/150000: episode: 511, duration: 0.660s, episode steps:  89, steps per second: 135, episode reward: -279.187, mean reward: -3.137 [-100.000,  1.478], mean action: 1.573 [0.000, 3.000],  loss: 6247446857537099.000000, mse: 9117912613803842560.000000, mean_q: 2909306595.235955, mean_eps: 0.676852
  54005/150000: episode: 512, duration: 0.777s, episode steps: 102, steps per second: 131, episode reward: -193.089, mean reward: -1.893 [-100.000, 42.760], mean action: 1.667 [0.000, 3.000],  loss: 7018073204349530.000000, mse: 8872564950620459008.000000, mean_q: 2846716621.803922, mean_eps: 0.676279
  54117/150000: episode: 513, duration: 0.981s, episode steps: 112, steps per second: 114, episode reward: -486.291, mean reward: -4.342 [-100.000,  2.796], mean action: 1.848 [0.000, 3.000],  loss: 7515682202564315.000000, mse: 9133100694013888512.000000, mean_q: 2898970233.142857, mean_eps: 0.675637
  54252/150000: episode: 514, duration: 1.052s, episode steps: 135, steps per second: 128, episode reward: -501.257, mean reward: -3.713 [-100.000,  1.319], mean action: 1.674 [0.000, 3.000],  loss: 12858985717306102.000000, mse: 9251994413597878272.000000, mean_q: 2904208166.874074, mean_eps: 0.674896
  54331/150000: episode: 515, duration: 0.628s, episode steps:  79, steps per second: 126, episode reward: -311.097, mean reward: -3.938 [-100.000,  2.793], mean action: 1.506 [0.000, 3.000],  loss: 5863677384740294.000000, mse: 9494832592070303744.000000, mean_q: 2956339556.455696, mean_eps: 0.674254
  54482/150000: episode: 516, duration: 1.155s, episode steps: 151, steps per second: 131, episode reward: -330.097, mean reward: -2.186 [-100.000,  5.186], mean action: 1.616 [0.000, 3.000],  loss: 10692527151603006.000000, mse: 9658202318963535872.000000, mean_q: 2985283760.317881, mean_eps: 0.673564
  54849/150000: episode: 517, duration: 3.055s, episode steps: 367, steps per second: 120, episode reward: -561.620, mean reward: -1.530 [-100.000, 15.394], mean action: 1.719 [0.000, 3.000],  loss: 5764437175020262.000000, mse: 9799484690367924224.000000, mean_q: 2997059623.062670, mean_eps: 0.672010
  55024/150000: episode: 518, duration: 1.438s, episode steps: 175, steps per second: 122, episode reward: -203.230, mean reward: -1.161 [-100.000, 15.100], mean action: 1.680 [0.000, 3.000],  loss: 7961048077465588.000000, mse: 10290408872451123200.000000, mean_q: 3086801470.902857, mean_eps: 0.670384
  55146/150000: episode: 519, duration: 1.032s, episode steps: 122, steps per second: 118, episode reward: -16.514, mean reward: -0.135 [-100.000, 21.989], mean action: 1.730 [0.000, 3.000],  loss: 10398501211669554.000000, mse: 10477765572480892928.000000, mean_q: 3117415291.803279, mean_eps: 0.669493
  55238/150000: episode: 520, duration: 0.684s, episode steps:  92, steps per second: 135, episode reward: -33.324, mean reward: -0.362 [-100.000, 11.370], mean action: 1.522 [0.000, 3.000],  loss: 10420648044863844.000000, mse: 10531536223429859328.000000, mean_q: 3106530323.478261, mean_eps: 0.668851
  55365/150000: episode: 521, duration: 1.023s, episode steps: 127, steps per second: 124, episode reward: -351.919, mean reward: -2.771 [-100.000,  1.732], mean action: 1.583 [0.000, 3.000],  loss: 14939365545411076.000000, mse: 10961130109289502720.000000, mean_q: 3188240025.196850, mean_eps: 0.668194
  55442/150000: episode: 522, duration: 0.576s, episode steps:  77, steps per second: 134, episode reward: -70.894, mean reward: -0.921 [-100.000, 17.904], mean action: 1.610 [0.000, 3.000],  loss: 4956594647716212.000000, mse: 10886851144666734592.000000, mean_q: 3172966788.987013, mean_eps: 0.667582
  55537/150000: episode: 523, duration: 0.701s, episode steps:  95, steps per second: 136, episode reward: -400.793, mean reward: -4.219 [-100.000,  1.012], mean action: 1.547 [0.000, 3.000],  loss: 6391392176403424.000000, mse: 10674923341638778880.000000, mean_q: 3118476942.821053, mean_eps: 0.667066
  55621/150000: episode: 524, duration: 0.651s, episode steps:  84, steps per second: 129, episode reward: -303.887, mean reward: -3.618 [-100.000,  1.836], mean action: 1.738 [0.000, 3.000],  loss: 19418376406252788.000000, mse: 11206255004788420608.000000, mean_q: 3214928374.857143, mean_eps: 0.666529
  55718/150000: episode: 525, duration: 0.734s, episode steps:  97, steps per second: 132, episode reward: -425.259, mean reward: -4.384 [-100.000,  0.635], mean action: 1.608 [0.000, 3.000],  loss: 6318747534626774.000000, mse: 11397787615745478656.000000, mean_q: 3247515059.463918, mean_eps: 0.665986
  55825/150000: episode: 526, duration: 0.781s, episode steps: 107, steps per second: 137, episode reward: -302.705, mean reward: -2.829 [-100.000,  1.100], mean action: 1.776 [0.000, 3.000],  loss: 12541221629890446.000000, mse: 11559570553190586368.000000, mean_q: 3275626113.196262, mean_eps: 0.665374
  55906/150000: episode: 527, duration: 0.691s, episode steps:  81, steps per second: 117, episode reward: -159.594, mean reward: -1.970 [-100.000, 41.399], mean action: 1.481 [0.000, 3.000],  loss: 21577024049454192.000000, mse: 11579390851975497728.000000, mean_q: 3262968522.271605, mean_eps: 0.664810
  56002/150000: episode: 528, duration: 0.740s, episode steps:  96, steps per second: 130, episode reward: -163.465, mean reward: -1.703 [-100.000,  2.917], mean action: 1.646 [0.000, 3.000],  loss: 23939031677708972.000000, mse: 11980594662674704384.000000, mean_q: 3346778293.333333, mean_eps: 0.664279
  56140/150000: episode: 529, duration: 1.040s, episode steps: 138, steps per second: 133, episode reward: -216.096, mean reward: -1.566 [-100.000, 35.423], mean action: 1.746 [0.000, 3.000],  loss: 13731475072559312.000000, mse: 11754358484610322432.000000, mean_q: 3280712038.028986, mean_eps: 0.663577
  56228/150000: episode: 530, duration: 0.687s, episode steps:  88, steps per second: 128, episode reward: -376.029, mean reward: -4.273 [-100.000,  0.525], mean action: 1.659 [0.000, 3.000],  loss: 9380986477537466.000000, mse: 12114664693256921088.000000, mean_q: 3343174219.636364, mean_eps: 0.662899
  56329/150000: episode: 531, duration: 0.746s, episode steps: 101, steps per second: 135, episode reward: -38.594, mean reward: -0.382 [-100.000,  8.090], mean action: 1.683 [0.000, 3.000],  loss: 13594103845860342.000000, mse: 11959316572381270016.000000, mean_q: 3317852945.742574, mean_eps: 0.662332
  56415/150000: episode: 532, duration: 0.647s, episode steps:  86, steps per second: 133, episode reward: -289.295, mean reward: -3.364 [-100.000, 19.789], mean action: 1.721 [0.000, 3.000],  loss: 17464480600375678.000000, mse: 12059667034755551232.000000, mean_q: 3335442146.232558, mean_eps: 0.661771
  56503/150000: episode: 533, duration: 0.749s, episode steps:  88, steps per second: 117, episode reward: -315.257, mean reward: -3.582 [-100.000,  1.870], mean action: 1.705 [0.000, 3.000],  loss: 12382470117880926.000000, mse: 12384908027498117120.000000, mean_q: 3390794234.181818, mean_eps: 0.661249
  56599/150000: episode: 534, duration: 0.787s, episode steps:  96, steps per second: 122, episode reward: -401.516, mean reward: -4.182 [-100.000,  0.991], mean action: 1.792 [0.000, 3.000],  loss: 11366562683770198.000000, mse: 12681910916000077824.000000, mean_q: 3427068872.000000, mean_eps: 0.660697
  56724/150000: episode: 535, duration: 1.013s, episode steps: 125, steps per second: 123, episode reward: -329.777, mean reward: -2.638 [-100.000,  4.896], mean action: 1.632 [0.000, 3.000],  loss: 11610971806910906.000000, mse: 12799161870110957568.000000, mean_q: 3446784346.112000, mean_eps: 0.660034
  56851/150000: episode: 536, duration: 1.039s, episode steps: 127, steps per second: 122, episode reward: -592.431, mean reward: -4.665 [-100.000,  2.136], mean action: 1.433 [0.000, 3.000],  loss: 23028025856762952.000000, mse: 12748100651272110080.000000, mean_q: 3423505083.464567, mean_eps: 0.659278
  56969/150000: episode: 537, duration: 0.906s, episode steps: 118, steps per second: 130, episode reward: 16.789, mean reward:  0.142 [-100.000, 11.443], mean action: 1.932 [0.000, 3.000],  loss: 21108293732307848.000000, mse: 12941417959722475520.000000, mean_q: 3468558307.796610, mean_eps: 0.658543
  57103/150000: episode: 538, duration: 1.098s, episode steps: 134, steps per second: 122, episode reward: -237.297, mean reward: -1.771 [-100.000,  1.697], mean action: 1.664 [0.000, 3.000],  loss: 10154026062702868.000000, mse: 13165204539156127744.000000, mean_q: 3478646130.626866, mean_eps: 0.657787
  57215/150000: episode: 539, duration: 0.838s, episode steps: 112, steps per second: 134, episode reward: -377.432, mean reward: -3.370 [-100.000,  1.309], mean action: 1.679 [0.000, 3.000],  loss: 15680124513194570.000000, mse: 13046773678376896512.000000, mean_q: 3471135090.285714, mean_eps: 0.657049
  57381/150000: episode: 540, duration: 1.286s, episode steps: 166, steps per second: 129, episode reward: -519.590, mean reward: -3.130 [-100.000,  3.935], mean action: 1.627 [0.000, 3.000],  loss: 19220640124693096.000000, mse: 13397293812939663360.000000, mean_q: 3515385618.506024, mean_eps: 0.656215
  57570/150000: episode: 541, duration: 1.417s, episode steps: 189, steps per second: 133, episode reward: -641.617, mean reward: -3.395 [-100.000,  2.241], mean action: 1.820 [0.000, 3.000],  loss: 7292366279318122.000000, mse: 13820803119728920576.000000, mean_q: 3570294373.587302, mean_eps: 0.655150
  57672/150000: episode: 542, duration: 0.780s, episode steps: 102, steps per second: 131, episode reward: -513.232, mean reward: -5.032 [-100.000,  2.862], mean action: 1.833 [0.000, 3.000],  loss: 18450850801110476.000000, mse: 14218514266161702912.000000, mean_q: 3633098945.254902, mean_eps: 0.654277
  57757/150000: episode: 543, duration: 0.623s, episode steps:  85, steps per second: 137, episode reward: -94.775, mean reward: -1.115 [-100.000, 24.393], mean action: 1.812 [0.000, 3.000],  loss: 25490720039700744.000000, mse: 14247046510978093056.000000, mean_q: 3651481259.670588, mean_eps: 0.653716
  57935/150000: episode: 544, duration: 1.382s, episode steps: 178, steps per second: 129, episode reward: -354.987, mean reward: -1.994 [-100.000,  3.200], mean action: 1.725 [0.000, 3.000],  loss: 22153562453315908.000000, mse: 14387387875307040768.000000, mean_q: 3642806447.460674, mean_eps: 0.652927
  58040/150000: episode: 545, duration: 0.860s, episode steps: 105, steps per second: 122, episode reward: -43.020, mean reward: -0.410 [-100.000, 12.737], mean action: 1.781 [0.000, 3.000],  loss: 6592616065166151.000000, mse: 14574521312159524864.000000, mean_q: 3672671578.209524, mean_eps: 0.652078
  58143/150000: episode: 546, duration: 0.926s, episode steps: 103, steps per second: 111, episode reward: -162.191, mean reward: -1.575 [-100.000,  2.471], mean action: 1.612 [0.000, 3.000],  loss: 40045934098190696.000000, mse: 14449617333832572928.000000, mean_q: 3643242247.456311, mean_eps: 0.651454
  58245/150000: episode: 547, duration: 0.880s, episode steps: 102, steps per second: 116, episode reward: -187.278, mean reward: -1.836 [-100.000,  2.278], mean action: 1.667 [0.000, 3.000],  loss: 7706101923693508.000000, mse: 14695189961756993536.000000, mean_q: 3655829797.647059, mean_eps: 0.650839
  58341/150000: episode: 548, duration: 0.906s, episode steps:  96, steps per second: 106, episode reward: -354.121, mean reward: -3.689 [-100.000,  0.638], mean action: 1.604 [0.000, 3.000],  loss: 29043373211211092.000000, mse: 14984348136407128064.000000, mean_q: 3725615954.666667, mean_eps: 0.650245
  58430/150000: episode: 549, duration: 0.873s, episode steps:  89, steps per second: 102, episode reward: -249.581, mean reward: -2.804 [-100.000,  0.960], mean action: 1.764 [0.000, 3.000],  loss: 8057682485266409.000000, mse: 15190283114684090368.000000, mean_q: 3761678839.370787, mean_eps: 0.649690
  58547/150000: episode: 550, duration: 1.075s, episode steps: 117, steps per second: 109, episode reward: -503.185, mean reward: -4.301 [-100.000,  1.130], mean action: 1.684 [0.000, 3.000],  loss: 12151279761382374.000000, mse: 15152261877471111168.000000, mean_q: 3731590003.965812, mean_eps: 0.649072
  58651/150000: episode: 551, duration: 0.982s, episode steps: 104, steps per second: 106, episode reward: -313.328, mean reward: -3.013 [-100.000,  0.732], mean action: 1.673 [0.000, 3.000],  loss: 20311493152160532.000000, mse: 14822642518157588480.000000, mean_q: 3663485297.230769, mean_eps: 0.648409
  58753/150000: episode: 552, duration: 0.828s, episode steps: 102, steps per second: 123, episode reward: 11.443, mean reward:  0.112 [-100.000, 11.069], mean action: 1.647 [0.000, 3.000],  loss: 18619829852184096.000000, mse: 15468444442041761792.000000, mean_q: 3757959454.117647, mean_eps: 0.647791
  58838/150000: episode: 553, duration: 0.666s, episode steps:  85, steps per second: 128, episode reward: -301.687, mean reward: -3.549 [-100.000,  0.776], mean action: 1.776 [0.000, 3.000],  loss: 26114194493037148.000000, mse: 15446840467921080320.000000, mean_q: 3749789771.294117, mean_eps: 0.647230
  58942/150000: episode: 554, duration: 0.810s, episode steps: 104, steps per second: 128, episode reward: -396.892, mean reward: -3.816 [-100.000,  0.972], mean action: 1.683 [0.000, 3.000],  loss: 32528799620439432.000000, mse: 15197059836525553664.000000, mean_q: 3728442062.769231, mean_eps: 0.646663
  59074/150000: episode: 555, duration: 1.015s, episode steps: 132, steps per second: 130, episode reward: -62.298, mean reward: -0.472 [-100.000, 17.165], mean action: 1.652 [0.000, 3.000],  loss: 9612023002999466.000000, mse: 15549482822556442624.000000, mean_q: 3771490856.727273, mean_eps: 0.645955
  59386/150000: episode: 556, duration: 2.501s, episode steps: 312, steps per second: 125, episode reward: -675.884, mean reward: -2.166 [-100.000,  3.531], mean action: 1.808 [0.000, 3.000],  loss: 22634750104379604.000000, mse: 15970146256748234752.000000, mean_q: 3812735148.307693, mean_eps: 0.644623
  59518/150000: episode: 557, duration: 1.004s, episode steps: 132, steps per second: 131, episode reward: -360.426, mean reward: -2.730 [-100.000,  1.581], mean action: 1.545 [0.000, 3.000],  loss: 13828965721726232.000000, mse: 16312350644734771200.000000, mean_q: 3878734642.424242, mean_eps: 0.643291
  59640/150000: episode: 558, duration: 0.973s, episode steps: 122, steps per second: 125, episode reward: -222.590, mean reward: -1.825 [-100.000,  1.108], mean action: 1.828 [0.000, 3.000],  loss: 19226914215376528.000000, mse: 16491338045874147328.000000, mean_q: 3901753963.016394, mean_eps: 0.642529
  59920/150000: episode: 559, duration: 2.556s, episode steps: 280, steps per second: 110, episode reward: -382.267, mean reward: -1.365 [-100.000, 41.872], mean action: 1.614 [0.000, 3.000],  loss: 26326085640980304.000000, mse: 16616225560473206784.000000, mean_q: 3897960739.657143, mean_eps: 0.641323
  60047/150000: episode: 560, duration: 1.071s, episode steps: 127, steps per second: 119, episode reward: -185.830, mean reward: -1.463 [-100.000,  2.634], mean action: 1.669 [0.000, 3.000],  loss: 19859052015776744.000000, mse: 16786136390322675712.000000, mean_q: 3931218163.905512, mean_eps: 0.640102
  60133/150000: episode: 561, duration: 0.707s, episode steps:  86, steps per second: 122, episode reward: -306.100, mean reward: -3.559 [-100.000,  0.740], mean action: 1.570 [0.000, 3.000],  loss: 17433061929041538.000000, mse: 16885461022777364480.000000, mean_q: 3907446804.837209, mean_eps: 0.639463
  60236/150000: episode: 562, duration: 0.861s, episode steps: 103, steps per second: 120, episode reward: -418.955, mean reward: -4.068 [-100.000,  1.449], mean action: 1.816 [0.000, 3.000],  loss: 12108295803026660.000000, mse: 16992741325217820672.000000, mean_q: 3945960858.097087, mean_eps: 0.638896
  60330/150000: episode: 563, duration: 0.714s, episode steps:  94, steps per second: 132, episode reward: -385.908, mean reward: -4.105 [-100.000,  5.673], mean action: 1.755 [0.000, 3.000],  loss: 14267607314696236.000000, mse: 17748290857036939264.000000, mean_q: 4052889940.425532, mean_eps: 0.638305
  60502/150000: episode: 564, duration: 1.354s, episode steps: 172, steps per second: 127, episode reward: -444.829, mean reward: -2.586 [-100.000,  3.172], mean action: 1.733 [0.000, 3.000],  loss: 12272289725199002.000000, mse: 17463125797845786624.000000, mean_q: 3995649172.837209, mean_eps: 0.637507
  60632/150000: episode: 565, duration: 0.977s, episode steps: 130, steps per second: 133, episode reward: -299.279, mean reward: -2.302 [-100.000, 12.895], mean action: 1.677 [0.000, 3.000],  loss: 18111194505424708.000000, mse: 17798908405680105472.000000, mean_q: 4030379112.369231, mean_eps: 0.636601
  60823/150000: episode: 566, duration: 1.500s, episode steps: 191, steps per second: 127, episode reward: -180.022, mean reward: -0.943 [-100.000, 47.146], mean action: 1.634 [0.000, 3.000],  loss: 20721434080277660.000000, mse: 17862809100407748608.000000, mean_q: 4040621298.596859, mean_eps: 0.635638
  60931/150000: episode: 567, duration: 0.815s, episode steps: 108, steps per second: 132, episode reward: -362.637, mean reward: -3.358 [-100.000,  0.530], mean action: 1.731 [0.000, 3.000],  loss: 25180162125075192.000000, mse: 18072637431201359872.000000, mean_q: 4076784933.925926, mean_eps: 0.634741
  61045/150000: episode: 568, duration: 0.918s, episode steps: 114, steps per second: 124, episode reward: -233.357, mean reward: -2.047 [-100.000,  5.044], mean action: 1.526 [0.000, 3.000],  loss: 20968240328594200.000000, mse: 18419483930146295808.000000, mean_q: 4145159657.543859, mean_eps: 0.634075
  61155/150000: episode: 569, duration: 0.856s, episode steps: 110, steps per second: 128, episode reward: -173.424, mean reward: -1.577 [-100.000,  2.174], mean action: 1.636 [0.000, 3.000],  loss: 16098353691263572.000000, mse: 18227139236018587648.000000, mean_q: 4091732689.454545, mean_eps: 0.633403
  61275/150000: episode: 570, duration: 0.941s, episode steps: 120, steps per second: 128, episode reward: -335.673, mean reward: -2.797 [-100.000, 84.427], mean action: 1.592 [0.000, 3.000],  loss: 12651047408159948.000000, mse: 18646867980950052864.000000, mean_q: 4157923259.733333, mean_eps: 0.632713
  61386/150000: episode: 571, duration: 0.860s, episode steps: 111, steps per second: 129, episode reward: -425.849, mean reward: -3.836 [-100.000,  1.725], mean action: 1.694 [0.000, 3.000],  loss: 18461429361449016.000000, mse: 18867979221521096704.000000, mean_q: 4174540982.198198, mean_eps: 0.632020
  61485/150000: episode: 572, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -277.598, mean reward: -2.804 [-100.000,  1.035], mean action: 1.768 [0.000, 3.000],  loss: 19137465154651932.000000, mse: 19385028502973632512.000000, mean_q: 4240969699.555555, mean_eps: 0.631390
  61617/150000: episode: 573, duration: 1.071s, episode steps: 132, steps per second: 123, episode reward: -481.048, mean reward: -3.644 [-100.000,  1.787], mean action: 1.879 [0.000, 3.000],  loss: 17065934282468010.000000, mse: 19699696580684328960.000000, mean_q: 4261657479.757576, mean_eps: 0.630697
  61719/150000: episode: 574, duration: 0.783s, episode steps: 102, steps per second: 130, episode reward: -451.692, mean reward: -4.428 [-100.000,  1.088], mean action: 1.676 [0.000, 3.000],  loss: 7272388270458760.000000, mse: 19461229378577530880.000000, mean_q: 4229651518.745098, mean_eps: 0.629995
  61825/150000: episode: 575, duration: 0.815s, episode steps: 106, steps per second: 130, episode reward: -229.244, mean reward: -2.163 [-100.000, 39.936], mean action: 1.651 [0.000, 3.000],  loss: 26949828968967340.000000, mse: 19231757515399716864.000000, mean_q: 4180922413.886793, mean_eps: 0.629371
  61978/150000: episode: 576, duration: 1.190s, episode steps: 153, steps per second: 129, episode reward: -212.272, mean reward: -1.387 [-100.000,  4.259], mean action: 1.810 [0.000, 3.000],  loss: 38701705369183760.000000, mse: 19579771043668819968.000000, mean_q: 4236579000.052288, mean_eps: 0.628594
  62125/150000: episode: 577, duration: 1.165s, episode steps: 147, steps per second: 126, episode reward: -447.915, mean reward: -3.047 [-100.000,  4.084], mean action: 1.531 [0.000, 3.000],  loss: 35166264209972188.000000, mse: 20047114580279074816.000000, mean_q: 4308797184.000000, mean_eps: 0.627694
  62302/150000: episode: 578, duration: 1.364s, episode steps: 177, steps per second: 130, episode reward: -543.707, mean reward: -3.072 [-100.000,  2.084], mean action: 1.734 [0.000, 3.000],  loss: 32508271284413388.000000, mse: 20333978295170981888.000000, mean_q: 4323332340.429379, mean_eps: 0.626722
  62384/150000: episode: 579, duration: 0.653s, episode steps:  82, steps per second: 126, episode reward: -21.829, mean reward: -0.266 [-100.000, 17.805], mean action: 1.622 [0.000, 3.000],  loss: 16802030560656210.000000, mse: 20578277428333686784.000000, mean_q: 4360079512.975610, mean_eps: 0.625945
  62641/150000: episode: 580, duration: 1.995s, episode steps: 257, steps per second: 129, episode reward: -353.155, mean reward: -1.374 [-100.000,  3.038], mean action: 1.700 [0.000, 3.000],  loss: 29535437067211436.000000, mse: 20406073768736849920.000000, mean_q: 4313499766.536965, mean_eps: 0.624928
  62797/150000: episode: 581, duration: 1.503s, episode steps: 156, steps per second: 104, episode reward: -563.631, mean reward: -3.613 [-100.000,  3.451], mean action: 1.577 [0.000, 3.000],  loss: 35430548308976248.000000, mse: 21236925295782367232.000000, mean_q: 4423339815.384615, mean_eps: 0.623689
  62927/150000: episode: 582, duration: 1.149s, episode steps: 130, steps per second: 113, episode reward: -519.443, mean reward: -3.996 [-100.000,  2.539], mean action: 1.554 [0.000, 3.000],  loss: 20327059198094668.000000, mse: 21737713099117879296.000000, mean_q: 4469629780.676923, mean_eps: 0.622831
  63025/150000: episode: 583, duration: 0.808s, episode steps:  98, steps per second: 121, episode reward: -104.093, mean reward: -1.062 [-100.000, 10.154], mean action: 1.602 [0.000, 3.000],  loss: 30202643517233360.000000, mse: 21529841837958799360.000000, mean_q: 4448658669.714286, mean_eps: 0.622147
  63131/150000: episode: 584, duration: 0.845s, episode steps: 106, steps per second: 125, episode reward: -417.850, mean reward: -3.942 [-100.000,  3.697], mean action: 1.604 [0.000, 3.000],  loss: 37684013420209520.000000, mse: 21400286718384701440.000000, mean_q: 4436479983.094339, mean_eps: 0.621535
  63279/150000: episode: 585, duration: 1.235s, episode steps: 148, steps per second: 120, episode reward: -469.172, mean reward: -3.170 [-100.000,  2.092], mean action: 1.709 [0.000, 3.000],  loss: 42031009789499008.000000, mse: 21171692974463672320.000000, mean_q: 4377702073.081081, mean_eps: 0.620773
  63379/150000: episode: 586, duration: 0.814s, episode steps: 100, steps per second: 123, episode reward: -420.477, mean reward: -4.205 [-100.000,  1.057], mean action: 1.670 [0.000, 3.000],  loss: 45806648730289440.000000, mse: 21482604562635587584.000000, mean_q: 4431331072.000000, mean_eps: 0.620029
  63473/150000: episode: 587, duration: 0.861s, episode steps:  94, steps per second: 109, episode reward: -501.465, mean reward: -5.335 [-100.000,  0.587], mean action: 1.723 [0.000, 3.000],  loss: 49404541412640112.000000, mse: 22084873369788784640.000000, mean_q: 4508638513.021276, mean_eps: 0.619447
  63626/150000: episode: 588, duration: 1.239s, episode steps: 153, steps per second: 123, episode reward: -369.026, mean reward: -2.412 [-100.000, 42.621], mean action: 1.667 [0.000, 3.000],  loss: 50566619637195344.000000, mse: 22392749583779241984.000000, mean_q: 4511819944.993464, mean_eps: 0.618706
  63738/150000: episode: 589, duration: 0.929s, episode steps: 112, steps per second: 121, episode reward: -256.008, mean reward: -2.286 [-100.000,  0.600], mean action: 1.554 [0.000, 3.000],  loss: 41864215193827032.000000, mse: 22523880661500035072.000000, mean_q: 4546320004.571428, mean_eps: 0.617911
  63828/150000: episode: 590, duration: 0.805s, episode steps:  90, steps per second: 112, episode reward: -335.800, mean reward: -3.731 [-100.000,  1.074], mean action: 1.733 [0.000, 3.000],  loss: 34891933890449680.000000, mse: 21744224427652149248.000000, mean_q: 4437121137.777778, mean_eps: 0.617305
  63941/150000: episode: 591, duration: 1.140s, episode steps: 113, steps per second:  99, episode reward: -489.013, mean reward: -4.328 [-100.000,  0.904], mean action: 1.761 [0.000, 3.000],  loss: 23634063719533976.000000, mse: 22732019320663576576.000000, mean_q: 4562325610.477876, mean_eps: 0.616696
  64076/150000: episode: 592, duration: 1.246s, episode steps: 135, steps per second: 108, episode reward: -603.802, mean reward: -4.473 [-100.000,  2.362], mean action: 1.667 [0.000, 3.000],  loss: 36542644084702504.000000, mse: 22722665735172112384.000000, mean_q: 4565430886.400000, mean_eps: 0.615952
  64156/150000: episode: 593, duration: 0.740s, episode steps:  80, steps per second: 108, episode reward: -271.547, mean reward: -3.394 [-100.000,  1.890], mean action: 1.738 [0.000, 3.000],  loss: 33658148857525044.000000, mse: 23219280603101790208.000000, mean_q: 4586088681.600000, mean_eps: 0.615307
  64306/150000: episode: 594, duration: 1.328s, episode steps: 150, steps per second: 113, episode reward: -264.394, mean reward: -1.763 [-100.000,  2.013], mean action: 1.640 [0.000, 3.000],  loss: 28020401584031664.000000, mse: 23201249439788765184.000000, mean_q: 4592345422.506667, mean_eps: 0.614617
  64381/150000: episode: 595, duration: 0.681s, episode steps:  75, steps per second: 110, episode reward: -320.314, mean reward: -4.271 [-100.000,  0.983], mean action: 1.747 [0.000, 3.000],  loss: 28340689146719764.000000, mse: 22570073059965657088.000000, mean_q: 4518695860.906667, mean_eps: 0.613942
  64467/150000: episode: 596, duration: 0.771s, episode steps:  86, steps per second: 112, episode reward: -499.495, mean reward: -5.808 [-100.000,  0.727], mean action: 1.756 [0.000, 3.000],  loss: 42869944540999584.000000, mse: 23315143389384077312.000000, mean_q: 4613822359.813953, mean_eps: 0.613459
  64584/150000: episode: 597, duration: 1.173s, episode steps: 117, steps per second: 100, episode reward: -131.031, mean reward: -1.120 [-100.000,  8.249], mean action: 1.650 [0.000, 3.000],  loss: 57097271393890616.000000, mse: 23359603242548289536.000000, mean_q: 4614791399.931623, mean_eps: 0.612850
  64682/150000: episode: 598, duration: 1.190s, episode steps:  98, steps per second:  82, episode reward: -399.840, mean reward: -4.080 [-100.000,  1.152], mean action: 1.837 [0.000, 3.000],  loss: 35855414451107212.000000, mse: 23679600038931476480.000000, mean_q: 4641395994.122449, mean_eps: 0.612205
  64780/150000: episode: 599, duration: 1.040s, episode steps:  98, steps per second:  94, episode reward: -263.601, mean reward: -2.690 [-100.000, 34.726], mean action: 1.704 [0.000, 3.000],  loss: 22330363894429068.000000, mse: 23866099369527439360.000000, mean_q: 4673347764.244898, mean_eps: 0.611617
  64859/150000: episode: 600, duration: 0.770s, episode steps:  79, steps per second: 103, episode reward: -300.645, mean reward: -3.806 [-100.000,  0.520], mean action: 1.848 [0.000, 3.000],  loss: 56863283000377344.000000, mse: 23868106126374019072.000000, mean_q: 4649055069.974684, mean_eps: 0.611086
  64963/150000: episode: 601, duration: 1.048s, episode steps: 104, steps per second:  99, episode reward: -190.490, mean reward: -1.832 [-100.000,  2.905], mean action: 1.731 [0.000, 3.000],  loss: 62829530488201848.000000, mse: 23698991336261439488.000000, mean_q: 4633659293.538462, mean_eps: 0.610537
  65045/150000: episode: 602, duration: 0.857s, episode steps:  82, steps per second:  96, episode reward: -188.227, mean reward: -2.295 [-100.000,  7.098], mean action: 1.610 [0.000, 3.000],  loss: 61729832597192704.000000, mse: 24145425804741144576.000000, mean_q: 4668235910.243902, mean_eps: 0.609979
  65128/150000: episode: 603, duration: 0.764s, episode steps:  83, steps per second: 109, episode reward: -103.819, mean reward: -1.251 [-100.000,  7.922], mean action: 1.530 [0.000, 3.000],  loss: 34998498444724744.000000, mse: 23822573266715009024.000000, mean_q: 4637831097.060241, mean_eps: 0.609484
  65238/150000: episode: 604, duration: 0.880s, episode steps: 110, steps per second: 125, episode reward: -311.846, mean reward: -2.835 [-100.000,  0.943], mean action: 1.691 [0.000, 3.000],  loss: 37408709596108912.000000, mse: 24834214768394113024.000000, mean_q: 4770443424.581819, mean_eps: 0.608905
  65383/150000: episode: 605, duration: 1.495s, episode steps: 145, steps per second:  97, episode reward: -391.983, mean reward: -2.703 [-100.000,  2.769], mean action: 1.572 [0.000, 3.000],  loss: 28233304672411816.000000, mse: 24450140680916783104.000000, mean_q: 4731091005.793103, mean_eps: 0.608140
  65534/150000: episode: 606, duration: 1.469s, episode steps: 151, steps per second: 103, episode reward: -252.238, mean reward: -1.670 [-100.000,  1.703], mean action: 1.589 [0.000, 3.000],  loss: 73458887920656432.000000, mse: 24455348241470304256.000000, mean_q: 4741073730.119205, mean_eps: 0.607252
  65622/150000: episode: 607, duration: 0.701s, episode steps:  88, steps per second: 126, episode reward: -443.549, mean reward: -5.040 [-100.000,  0.568], mean action: 1.807 [0.000, 3.000],  loss: 47557851591022408.000000, mse: 24990281043132497920.000000, mean_q: 4769709169.454545, mean_eps: 0.606535
  65757/150000: episode: 608, duration: 1.090s, episode steps: 135, steps per second: 124, episode reward: -256.335, mean reward: -1.899 [-100.000,  1.736], mean action: 1.726 [0.000, 3.000],  loss: 51982698105620008.000000, mse: 24819560869467201536.000000, mean_q: 4780379443.200000, mean_eps: 0.605866
  65870/150000: episode: 609, duration: 0.937s, episode steps: 113, steps per second: 121, episode reward: -375.607, mean reward: -3.324 [-100.000,  1.281], mean action: 1.779 [0.000, 3.000],  loss: 35845442951604280.000000, mse: 25185695229481803776.000000, mean_q: 4804817521.274336, mean_eps: 0.605122
  65972/150000: episode: 610, duration: 0.797s, episode steps: 102, steps per second: 128, episode reward: -383.015, mean reward: -3.755 [-100.000,  0.852], mean action: 1.735 [0.000, 3.000],  loss: 64003884295630768.000000, mse: 25373247390713221120.000000, mean_q: 4799261783.843137, mean_eps: 0.604477
  66089/150000: episode: 611, duration: 0.990s, episode steps: 117, steps per second: 118, episode reward: -313.624, mean reward: -2.681 [-100.000,  1.266], mean action: 1.778 [0.000, 3.000],  loss: 53194843578021272.000000, mse: 25767800268465950720.000000, mean_q: 4873701596.991453, mean_eps: 0.603820
  66317/150000: episode: 612, duration: 1.811s, episode steps: 228, steps per second: 126, episode reward: -649.738, mean reward: -2.850 [-100.000,  2.753], mean action: 1.754 [0.000, 3.000],  loss: 59776944128310432.000000, mse: 26025827961624133632.000000, mean_q: 4876184952.140351, mean_eps: 0.602785
  66446/150000: episode: 613, duration: 1.018s, episode steps: 129, steps per second: 127, episode reward: -134.057, mean reward: -1.039 [-100.000,  2.203], mean action: 1.744 [0.000, 3.000],  loss: 28658673655094216.000000, mse: 26655508872965341184.000000, mean_q: 4973365535.751938, mean_eps: 0.601714
  66549/150000: episode: 614, duration: 0.807s, episode steps: 103, steps per second: 128, episode reward: -431.785, mean reward: -4.192 [-100.000,  1.287], mean action: 1.728 [0.000, 3.000],  loss: 60135460844140680.000000, mse: 26868799508650299392.000000, mean_q: 4975708453.281553, mean_eps: 0.601018
  66652/150000: episode: 615, duration: 0.799s, episode steps: 103, steps per second: 129, episode reward: -226.444, mean reward: -2.198 [-100.000,  1.393], mean action: 1.680 [0.000, 3.000],  loss: 24324589562086488.000000, mse: 26269487435388882944.000000, mean_q: 4901987303.145631, mean_eps: 0.600400
  66832/150000: episode: 616, duration: 1.450s, episode steps: 180, steps per second: 124, episode reward: -125.320, mean reward: -0.696 [-100.000,  4.566], mean action: 1.656 [0.000, 3.000],  loss: 46298524900369656.000000, mse: 27444339485319389184.000000, mean_q: 5031627680.711111, mean_eps: 0.599551
  66989/150000: episode: 617, duration: 1.334s, episode steps: 157, steps per second: 118, episode reward: -668.143, mean reward: -4.256 [-100.000,  1.790], mean action: 1.707 [0.000, 3.000],  loss: 37082694975005456.000000, mse: 27363303833831510016.000000, mean_q: 5000088364.025477, mean_eps: 0.598540
  67162/150000: episode: 618, duration: 1.438s, episode steps: 173, steps per second: 120, episode reward: -348.363, mean reward: -2.014 [-100.000,  3.321], mean action: 1.653 [0.000, 3.000],  loss: 54832591128031920.000000, mse: 27814724464560812032.000000, mean_q: 5050902467.329480, mean_eps: 0.597550
  67279/150000: episode: 619, duration: 1.146s, episode steps: 117, steps per second: 102, episode reward: -361.327, mean reward: -3.088 [-100.000,  2.290], mean action: 1.735 [0.000, 3.000],  loss: 40698230090905824.000000, mse: 28285230767391866880.000000, mean_q: 5123626736.683761, mean_eps: 0.596680
  67366/150000: episode: 620, duration: 1.111s, episode steps:  87, steps per second:  78, episode reward: -467.589, mean reward: -5.375 [-100.000,  0.183], mean action: 1.747 [0.000, 3.000],  loss: 57004624766256048.000000, mse: 28646972186312167424.000000, mean_q: 5116878774.436782, mean_eps: 0.596068
  67476/150000: episode: 621, duration: 1.934s, episode steps: 110, steps per second:  57, episode reward: -237.030, mean reward: -2.155 [-100.000,  1.558], mean action: 1.609 [0.000, 3.000],  loss: 38450811594237544.000000, mse: 28455208774424715264.000000, mean_q: 5097788199.563637, mean_eps: 0.595477
  67570/150000: episode: 622, duration: 0.923s, episode steps:  94, steps per second: 102, episode reward: -249.382, mean reward: -2.653 [-100.000,  1.000], mean action: 1.872 [0.000, 3.000],  loss: 18162297900066684.000000, mse: 29154192363818516480.000000, mean_q: 5190150696.851064, mean_eps: 0.594865
  67697/150000: episode: 623, duration: 1.319s, episode steps: 127, steps per second:  96, episode reward: -129.093, mean reward: -1.016 [-100.000, 14.088], mean action: 1.661 [0.000, 3.000],  loss: 48639957603298136.000000, mse: 28953815848408113152.000000, mean_q: 5164600328.062992, mean_eps: 0.594202
  67790/150000: episode: 624, duration: 0.790s, episode steps:  93, steps per second: 118, episode reward: -374.605, mean reward: -4.028 [-100.000,  1.924], mean action: 1.806 [0.000, 3.000],  loss: 48949988777079888.000000, mse: 29149202632142073856.000000, mean_q: 5181529110.021505, mean_eps: 0.593542
  67938/150000: episode: 625, duration: 1.410s, episode steps: 148, steps per second: 105, episode reward: -145.890, mean reward: -0.986 [-100.000,  3.045], mean action: 1.811 [0.000, 3.000],  loss: 56034319512675856.000000, mse: 29371721330930765824.000000, mean_q: 5184401193.513514, mean_eps: 0.592819
  68031/150000: episode: 626, duration: 0.797s, episode steps:  93, steps per second: 117, episode reward: -375.692, mean reward: -4.040 [-100.000,  1.991], mean action: 1.656 [0.000, 3.000],  loss: 17931415537277654.000000, mse: 29309194318860197888.000000, mean_q: 5202191046.193548, mean_eps: 0.592096
  68108/150000: episode: 627, duration: 0.644s, episode steps:  77, steps per second: 120, episode reward: -305.006, mean reward: -3.961 [-100.000,  0.052], mean action: 1.766 [0.000, 3.000],  loss: 66525627967800144.000000, mse: 29214422287363756032.000000, mean_q: 5159251130.181818, mean_eps: 0.591586
  68286/150000: episode: 628, duration: 1.589s, episode steps: 178, steps per second: 112, episode reward: -542.852, mean reward: -3.050 [-100.000,  1.718], mean action: 1.798 [0.000, 3.000],  loss: 63185120035920536.000000, mse: 30133768243369361408.000000, mean_q: 5287521715.775281, mean_eps: 0.590821
  68617/150000: episode: 629, duration: 2.833s, episode steps: 331, steps per second: 117, episode reward: -726.471, mean reward: -2.195 [-100.000,  3.202], mean action: 1.689 [0.000, 3.000],  loss: 47981229790663904.000000, mse: 31108503750749270016.000000, mean_q: 5341869419.504532, mean_eps: 0.589294
  68716/150000: episode: 630, duration: 0.804s, episode steps:  99, steps per second: 123, episode reward: -203.477, mean reward: -2.055 [-100.000,  1.266], mean action: 1.778 [0.000, 3.000],  loss: 68822660724987384.000000, mse: 32208557884252966912.000000, mean_q: 5429991263.676767, mean_eps: 0.588004
  68841/150000: episode: 631, duration: 0.972s, episode steps: 125, steps per second: 129, episode reward: -364.969, mean reward: -2.920 [-100.000,  1.246], mean action: 1.656 [0.000, 3.000],  loss: 109673433644805392.000000, mse: 31897491694506889216.000000, mean_q: 5396005945.344000, mean_eps: 0.587332
  69091/150000: episode: 632, duration: 2.115s, episode steps: 250, steps per second: 118, episode reward: -814.947, mean reward: -3.260 [-100.000,  2.012], mean action: 1.812 [0.000, 3.000],  loss: 39077637079210920.000000, mse: 32110975774866575360.000000, mean_q: 5400147449.856000, mean_eps: 0.586207
  69221/150000: episode: 633, duration: 1.116s, episode steps: 130, steps per second: 116, episode reward: -557.150, mean reward: -4.286 [-100.000,  1.755], mean action: 1.677 [0.000, 3.000],  loss: 45077585291733592.000000, mse: 33074286028334166016.000000, mean_q: 5540296440.123077, mean_eps: 0.585067
  69309/150000: episode: 634, duration: 0.706s, episode steps:  88, steps per second: 125, episode reward: -333.105, mean reward: -3.785 [-100.000,  3.354], mean action: 1.636 [0.000, 3.000],  loss: 89203581092769424.000000, mse: 33219546059441319936.000000, mean_q: 5466973099.636364, mean_eps: 0.584413
  69385/150000: episode: 635, duration: 0.623s, episode steps:  76, steps per second: 122, episode reward: -411.262, mean reward: -5.411 [-100.000,  0.003], mean action: 1.934 [0.000, 3.000],  loss: 63552006715701568.000000, mse: 33555419108264722432.000000, mean_q: 5532542160.842105, mean_eps: 0.583921
  69558/150000: episode: 636, duration: 1.483s, episode steps: 173, steps per second: 117, episode reward: -550.915, mean reward: -3.184 [-100.000,  2.716], mean action: 1.751 [0.000, 3.000],  loss: 44817838301481376.000000, mse: 33992217670500560896.000000, mean_q: 5588540713.433526, mean_eps: 0.583174
  69690/150000: episode: 637, duration: 1.060s, episode steps: 132, steps per second: 124, episode reward: -305.284, mean reward: -2.313 [-100.000,  1.892], mean action: 1.598 [0.000, 3.000],  loss: 82105283327343152.000000, mse: 33744765788236750848.000000, mean_q: 5537347281.454545, mean_eps: 0.582259
  69833/150000: episode: 638, duration: 1.156s, episode steps: 143, steps per second: 124, episode reward: -684.276, mean reward: -4.785 [-100.000,  2.905], mean action: 1.594 [0.000, 3.000],  loss: 52082577885404552.000000, mse: 34615184275323936768.000000, mean_q: 5633491208.951049, mean_eps: 0.581434
  70019/150000: episode: 639, duration: 1.471s, episode steps: 186, steps per second: 126, episode reward: -474.266, mean reward: -2.550 [-100.000,  4.234], mean action: 1.667 [0.000, 3.000],  loss: 59939458977568968.000000, mse: 33857660582245605376.000000, mean_q: 5564030587.870968, mean_eps: 0.580447
  70116/150000: episode: 640, duration: 0.763s, episode steps:  97, steps per second: 127, episode reward: -258.914, mean reward: -2.669 [-100.000, 39.823], mean action: 1.577 [0.000, 3.000],  loss: 70043718037471904.000000, mse: 34763234674093867008.000000, mean_q: 5650266653.030928, mean_eps: 0.579598
  70236/150000: episode: 641, duration: 0.961s, episode steps: 120, steps per second: 125, episode reward: -506.730, mean reward: -4.223 [-100.000,  3.867], mean action: 1.817 [0.000, 3.000],  loss: 33384158905150124.000000, mse: 35327022819045625856.000000, mean_q: 5699672401.066667, mean_eps: 0.578947
  70379/150000: episode: 642, duration: 1.145s, episode steps: 143, steps per second: 125, episode reward: -400.541, mean reward: -2.801 [-100.000,  2.415], mean action: 1.741 [0.000, 3.000],  loss: 56433679207266744.000000, mse: 35572764422005768192.000000, mean_q: 5703000919.720280, mean_eps: 0.578158
  70516/150000: episode: 643, duration: 1.164s, episode steps: 137, steps per second: 118, episode reward: -292.074, mean reward: -2.132 [-100.000,  1.830], mean action: 1.708 [0.000, 3.000],  loss: 59172065488335592.000000, mse: 36217560773938933760.000000, mean_q: 5785749702.072992, mean_eps: 0.577318
  70787/150000: episode: 644, duration: 2.191s, episode steps: 271, steps per second: 124, episode reward: -818.514, mean reward: -3.020 [-100.000,  2.322], mean action: 1.852 [0.000, 3.000],  loss: 65928805398996848.000000, mse: 36233993015403438080.000000, mean_q: 5733003256.442804, mean_eps: 0.576094
  70906/150000: episode: 645, duration: 1.094s, episode steps: 119, steps per second: 109, episode reward: -432.337, mean reward: -3.633 [-100.000,  3.503], mean action: 1.840 [0.000, 3.000],  loss: 145225202082944672.000000, mse: 37522290550189228032.000000, mean_q: 5875404520.336135, mean_eps: 0.574924
  71114/150000: episode: 646, duration: 1.959s, episode steps: 208, steps per second: 106, episode reward: -595.801, mean reward: -2.864 [-100.000,  2.664], mean action: 1.740 [0.000, 3.000],  loss: 68974380290973064.000000, mse: 38567390178820104192.000000, mean_q: 5942334402.461538, mean_eps: 0.573943
  71225/150000: episode: 647, duration: 1.022s, episode steps: 111, steps per second: 109, episode reward: -553.878, mean reward: -4.990 [-100.000,  1.520], mean action: 1.685 [0.000, 3.000],  loss: 70167669369657544.000000, mse: 38480109576846696448.000000, mean_q: 5930825437.405405, mean_eps: 0.572986
  71349/150000: episode: 648, duration: 1.229s, episode steps: 124, steps per second: 101, episode reward: -548.099, mean reward: -4.420 [-100.000,  2.166], mean action: 1.782 [0.000, 3.000],  loss: 80364643422137184.000000, mse: 38216423375408545792.000000, mean_q: 5888972139.354838, mean_eps: 0.572281
  71439/150000: episode: 649, duration: 0.865s, episode steps:  90, steps per second: 104, episode reward: -375.357, mean reward: -4.171 [-100.000,  0.695], mean action: 1.678 [0.000, 3.000],  loss: 52077155822099208.000000, mse: 38947542138696384512.000000, mean_q: 6009241702.400000, mean_eps: 0.571639
  71536/150000: episode: 650, duration: 0.790s, episode steps:  97, steps per second: 123, episode reward: -425.148, mean reward: -4.383 [-100.000,  2.871], mean action: 1.701 [0.000, 3.000],  loss: 81117866391727152.000000, mse: 38972572049863360512.000000, mean_q: 5995098402.309278, mean_eps: 0.571078
  71711/150000: episode: 651, duration: 1.460s, episode steps: 175, steps per second: 120, episode reward: -356.606, mean reward: -2.038 [-100.000,  3.233], mean action: 1.657 [0.000, 3.000],  loss: 76065446756840336.000000, mse: 38874055657742639104.000000, mean_q: 5966015008.182858, mean_eps: 0.570262
  71794/150000: episode: 652, duration: 0.704s, episode steps:  83, steps per second: 118, episode reward: -469.244, mean reward: -5.654 [-100.000,  0.554], mean action: 1.771 [0.000, 3.000],  loss: 66552122586747328.000000, mse: 39444632863146024960.000000, mean_q: 5997573539.469879, mean_eps: 0.569488
  71944/150000: episode: 653, duration: 1.398s, episode steps: 150, steps per second: 107, episode reward: -388.033, mean reward: -2.587 [-100.000,  2.205], mean action: 1.667 [0.000, 3.000],  loss: 85656532078877296.000000, mse: 39269332165481512960.000000, mean_q: 6004813796.693334, mean_eps: 0.568789
  72141/150000: episode: 654, duration: 2.031s, episode steps: 197, steps per second:  97, episode reward: -482.311, mean reward: -2.448 [-100.000,  3.837], mean action: 1.797 [0.000, 3.000],  loss: 44448569375926344.000000, mse: 40913730720211836928.000000, mean_q: 6170127323.614213, mean_eps: 0.567748
  72225/150000: episode: 655, duration: 0.855s, episode steps:  84, steps per second:  98, episode reward: -348.617, mean reward: -4.150 [-100.000,  0.718], mean action: 1.821 [0.000, 3.000],  loss: 87193435221824080.000000, mse: 41253281994521567232.000000, mean_q: 6147080064.000000, mean_eps: 0.566905
  72357/150000: episode: 656, duration: 1.247s, episode steps: 132, steps per second: 106, episode reward: -372.766, mean reward: -2.824 [-100.000,  4.731], mean action: 1.576 [0.000, 3.000],  loss: 107302891600366992.000000, mse: 42038312644022239232.000000, mean_q: 6220312859.151515, mean_eps: 0.566257
  72522/150000: episode: 657, duration: 1.519s, episode steps: 165, steps per second: 109, episode reward: -527.328, mean reward: -3.196 [-100.000,  2.268], mean action: 1.842 [0.000, 3.000],  loss: 64395728820223704.000000, mse: 41538833428679409664.000000, mean_q: 6175858542.157576, mean_eps: 0.565366
  72631/150000: episode: 658, duration: 0.901s, episode steps: 109, steps per second: 121, episode reward: -384.683, mean reward: -3.529 [-100.000,  0.905], mean action: 1.725 [0.000, 3.000],  loss: 100724099795367136.000000, mse: 43340257620038811648.000000, mean_q: 6331553853.064220, mean_eps: 0.564544
  72716/150000: episode: 659, duration: 0.737s, episode steps:  85, steps per second: 115, episode reward: -391.190, mean reward: -4.602 [-100.000,  0.487], mean action: 1.706 [0.000, 3.000],  loss: 76134309513775568.000000, mse: 42297217767001391104.000000, mean_q: 6246566315.670588, mean_eps: 0.563962
  72804/150000: episode: 660, duration: 0.796s, episode steps:  88, steps per second: 111, episode reward: -317.272, mean reward: -3.605 [-100.000,  1.226], mean action: 1.614 [0.000, 3.000],  loss: 121981507135864832.000000, mse: 43084800914952716288.000000, mean_q: 6299742609.454545, mean_eps: 0.563443
  72942/150000: episode: 661, duration: 1.232s, episode steps: 138, steps per second: 112, episode reward: -238.193, mean reward: -1.726 [-100.000,  3.094], mean action: 1.746 [0.000, 3.000],  loss: 174903263107751328.000000, mse: 42946809321802833920.000000, mean_q: 6311667103.536232, mean_eps: 0.562765
  73087/150000: episode: 662, duration: 1.232s, episode steps: 145, steps per second: 118, episode reward: -396.010, mean reward: -2.731 [-100.000,  1.766], mean action: 1.717 [0.000, 3.000],  loss: 97123321666503328.000000, mse: 42667895200535568384.000000, mean_q: 6252300856.496552, mean_eps: 0.561916
  73174/150000: episode: 663, duration: 0.766s, episode steps:  87, steps per second: 114, episode reward: -424.270, mean reward: -4.877 [-100.000,  0.153], mean action: 1.862 [0.000, 3.000],  loss: 45348020038383968.000000, mse: 42913508607099199488.000000, mean_q: 6301980360.091954, mean_eps: 0.561220
  73327/150000: episode: 664, duration: 1.278s, episode steps: 153, steps per second: 120, episode reward: -415.156, mean reward: -2.713 [-100.000,  1.948], mean action: 1.784 [0.000, 3.000],  loss: 88762814681910192.000000, mse: 43308563098779557888.000000, mean_q: 6343666048.836601, mean_eps: 0.560500
  73425/150000: episode: 665, duration: 0.863s, episode steps:  98, steps per second: 114, episode reward: -379.742, mean reward: -3.875 [-100.000,  3.894], mean action: 1.857 [0.000, 3.000],  loss: 97930064728606336.000000, mse: 43793732897149575168.000000, mean_q: 6346582392.163265, mean_eps: 0.559747
  73509/150000: episode: 666, duration: 0.720s, episode steps:  84, steps per second: 117, episode reward: -447.690, mean reward: -5.330 [-100.000,  1.794], mean action: 1.750 [0.000, 3.000],  loss: 58348415617622408.000000, mse: 44139982388891983872.000000, mean_q: 6382472508.952381, mean_eps: 0.559201
  73585/150000: episode: 667, duration: 0.620s, episode steps:  76, steps per second: 123, episode reward: -367.202, mean reward: -4.832 [-100.000, -0.228], mean action: 1.763 [0.000, 3.000],  loss: 84245009950696720.000000, mse: 44071389109360869376.000000, mean_q: 6399921664.000000, mean_eps: 0.558721
  73778/150000: episode: 668, duration: 1.900s, episode steps: 193, steps per second: 102, episode reward: -462.896, mean reward: -2.398 [-100.000,  4.239], mean action: 1.777 [0.000, 3.000],  loss: 72483846498332432.000000, mse: 44779773717882970112.000000, mean_q: 6464559271.129534, mean_eps: 0.557914
  73907/150000: episode: 669, duration: 1.437s, episode steps: 129, steps per second:  90, episode reward: -374.251, mean reward: -2.901 [-100.000,  1.641], mean action: 1.667 [0.000, 3.000],  loss: 139811122172560128.000000, mse: 45478496807999823872.000000, mean_q: 6507032635.534883, mean_eps: 0.556948
  74020/150000: episode: 670, duration: 0.972s, episode steps: 113, steps per second: 116, episode reward: -512.467, mean reward: -4.535 [-100.000,  1.930], mean action: 1.823 [0.000, 3.000],  loss: 104669401051557344.000000, mse: 45363903821060423680.000000, mean_q: 6468088437.805309, mean_eps: 0.556222
  74128/150000: episode: 671, duration: 0.955s, episode steps: 108, steps per second: 113, episode reward: -543.944, mean reward: -5.037 [-100.000,  1.331], mean action: 1.806 [0.000, 3.000],  loss: 112300910570683808.000000, mse: 44777409606575128576.000000, mean_q: 6459215094.518518, mean_eps: 0.555559
  74246/150000: episode: 672, duration: 0.954s, episode steps: 118, steps per second: 124, episode reward: -401.702, mean reward: -3.404 [-100.000,  1.211], mean action: 1.686 [0.000, 3.000],  loss: 64497290345185280.000000, mse: 45355061316674445312.000000, mean_q: 6479675109.966102, mean_eps: 0.554881
  74358/150000: episode: 673, duration: 0.901s, episode steps: 112, steps per second: 124, episode reward: 11.601, mean reward:  0.104 [-100.000, 14.323], mean action: 1.759 [0.000, 3.000],  loss: 46418324516203960.000000, mse: 46318720800236126208.000000, mean_q: 6535705321.142858, mean_eps: 0.554191
  74485/150000: episode: 674, duration: 1.006s, episode steps: 127, steps per second: 126, episode reward: -158.928, mean reward: -1.251 [-100.000, 32.263], mean action: 1.685 [0.000, 3.000],  loss: 83802273152846480.000000, mse: 46294856762556645376.000000, mean_q: 6498008402.645669, mean_eps: 0.553474
  74620/150000: episode: 675, duration: 1.072s, episode steps: 135, steps per second: 126, episode reward: -348.322, mean reward: -2.580 [-100.000,  2.134], mean action: 1.674 [0.000, 3.000],  loss: 62375335308721448.000000, mse: 46914882742815506432.000000, mean_q: 6569027534.696297, mean_eps: 0.552688
  74765/150000: episode: 676, duration: 1.291s, episode steps: 145, steps per second: 112, episode reward: -4.715, mean reward: -0.033 [-100.000, 12.302], mean action: 1.655 [0.000, 3.000],  loss: 63885815450451856.000000, mse: 46185116622540644352.000000, mean_q: 6539988730.703448, mean_eps: 0.551848
  74861/150000: episode: 677, duration: 0.883s, episode steps:  96, steps per second: 109, episode reward: -394.405, mean reward: -4.108 [-100.000,  0.667], mean action: 1.792 [0.000, 3.000],  loss: 55991592018422440.000000, mse: 46648050629794422784.000000, mean_q: 6574949349.333333, mean_eps: 0.551125
  74965/150000: episode: 678, duration: 0.954s, episode steps: 104, steps per second: 109, episode reward: -256.832, mean reward: -2.470 [-100.000,  0.719], mean action: 1.644 [0.000, 3.000],  loss: 180714114189320832.000000, mse: 47695488951655596032.000000, mean_q: 6637601595.076923, mean_eps: 0.550525
  75057/150000: episode: 679, duration: 0.839s, episode steps:  92, steps per second: 110, episode reward: -387.773, mean reward: -4.215 [-100.000,  3.116], mean action: 1.717 [0.000, 3.000],  loss: 94577651993827600.000000, mse: 47289941401892986880.000000, mean_q: 6616674910.608696, mean_eps: 0.549937
  75161/150000: episode: 680, duration: 0.991s, episode steps: 104, steps per second: 105, episode reward: -560.220, mean reward: -5.387 [-100.000,  2.112], mean action: 1.827 [0.000, 3.000],  loss: 105859798576620144.000000, mse: 48334495845794144256.000000, mean_q: 6659122121.846154, mean_eps: 0.549349
  75274/150000: episode: 681, duration: 1.095s, episode steps: 113, steps per second: 103, episode reward: -516.837, mean reward: -4.574 [-100.000,  0.941], mean action: 1.708 [0.000, 3.000],  loss: 150570686663708480.000000, mse: 47858261913091342336.000000, mean_q: 6626994737.840708, mean_eps: 0.548698
  75356/150000: episode: 682, duration: 0.874s, episode steps:  82, steps per second:  94, episode reward: -365.800, mean reward: -4.461 [-100.000,  0.792], mean action: 1.866 [0.000, 3.000],  loss: 51440853071065184.000000, mse: 48198381474805276672.000000, mean_q: 6689016782.048780, mean_eps: 0.548113
  75435/150000: episode: 683, duration: 0.720s, episode steps:  79, steps per second: 110, episode reward: -426.440, mean reward: -5.398 [-100.000, -0.223], mean action: 1.608 [0.000, 3.000],  loss: 134309706906106656.000000, mse: 48834456825592774656.000000, mean_q: 6750751698.632912, mean_eps: 0.547630
  75625/150000: episode: 684, duration: 1.922s, episode steps: 190, steps per second:  99, episode reward: -687.550, mean reward: -3.619 [-100.000,  2.694], mean action: 1.800 [0.000, 3.000],  loss: 115893839077903744.000000, mse: 49436796980419895296.000000, mean_q: 6752346343.747369, mean_eps: 0.546823
  75828/150000: episode: 685, duration: 2.777s, episode steps: 203, steps per second:  73, episode reward: -691.749, mean reward: -3.408 [-100.000,  3.396], mean action: 1.862 [0.000, 3.000],  loss: 96957032821856736.000000, mse: 49064503892092526592.000000, mean_q: 6738158392.748769, mean_eps: 0.545644
  75930/150000: episode: 686, duration: 1.079s, episode steps: 102, steps per second:  95, episode reward: -244.315, mean reward: -2.395 [-100.000,  2.299], mean action: 1.755 [0.000, 3.000],  loss: 97864462228998992.000000, mse: 50787636393750773760.000000, mean_q: 6862204330.666667, mean_eps: 0.544729
  76064/150000: episode: 687, duration: 1.372s, episode steps: 134, steps per second:  98, episode reward: -600.083, mean reward: -4.478 [-100.000,  2.092], mean action: 1.567 [0.000, 3.000],  loss: 68027882906481312.000000, mse: 50957409383916625920.000000, mean_q: 6890251053.850746, mean_eps: 0.544021
  76209/150000: episode: 688, duration: 1.717s, episode steps: 145, steps per second:  84, episode reward: -439.525, mean reward: -3.031 [-100.000,  2.005], mean action: 1.634 [0.000, 3.000],  loss: 104062407904172448.000000, mse: 50268622445174980608.000000, mean_q: 6797390399.558620, mean_eps: 0.543184
  76360/150000: episode: 689, duration: 2.165s, episode steps: 151, steps per second:  70, episode reward: -699.228, mean reward: -4.631 [-100.000,  2.051], mean action: 1.901 [0.000, 3.000],  loss: 175261157387069952.000000, mse: 51320204928994254848.000000, mean_q: 6921813192.052980, mean_eps: 0.542296
  76458/150000: episode: 690, duration: 1.067s, episode steps:  98, steps per second:  92, episode reward: -170.588, mean reward: -1.741 [-100.000,  1.198], mean action: 1.816 [0.000, 3.000],  loss: 105183607709424496.000000, mse: 50659008756631240704.000000, mean_q: 6820159075.265306, mean_eps: 0.541549
  76582/150000: episode: 691, duration: 1.127s, episode steps: 124, steps per second: 110, episode reward: -453.647, mean reward: -3.658 [-100.000,  1.691], mean action: 1.823 [0.000, 3.000],  loss: 198737632341642880.000000, mse: 51964169797552947200.000000, mean_q: 6920174071.741936, mean_eps: 0.540883
  76694/150000: episode: 692, duration: 0.984s, episode steps: 112, steps per second: 114, episode reward: -339.466, mean reward: -3.031 [-100.000,  7.031], mean action: 1.750 [0.000, 3.000],  loss: 57436071606990552.000000, mse: 52262742644961198080.000000, mean_q: 6965821088.000000, mean_eps: 0.540175
  76815/150000: episode: 693, duration: 1.014s, episode steps: 121, steps per second: 119, episode reward: -395.721, mean reward: -3.270 [-100.000,  4.171], mean action: 1.711 [0.000, 3.000],  loss: 153880301600755008.000000, mse: 51161842199318880256.000000, mean_q: 6858786629.818182, mean_eps: 0.539476
  76914/150000: episode: 694, duration: 0.820s, episode steps:  99, steps per second: 121, episode reward: -499.956, mean reward: -5.050 [-100.000,  1.559], mean action: 1.889 [0.000, 3.000],  loss: 184929149085196384.000000, mse: 52914487890940755968.000000, mean_q: 6988060506.505051, mean_eps: 0.538816
  77015/150000: episode: 695, duration: 0.909s, episode steps: 101, steps per second: 111, episode reward: -249.282, mean reward: -2.468 [-100.000,  2.265], mean action: 1.693 [0.000, 3.000],  loss: 113452012111352736.000000, mse: 52448750346173071360.000000, mean_q: 6888671637.544555, mean_eps: 0.538216
  77115/150000: episode: 696, duration: 0.869s, episode steps: 100, steps per second: 115, episode reward: -426.483, mean reward: -4.265 [-100.000,  2.148], mean action: 1.740 [0.000, 3.000],  loss: 24954368468514244.000000, mse: 50876235419293908992.000000, mean_q: 6872305075.200000, mean_eps: 0.537613
  77261/150000: episode: 697, duration: 1.329s, episode steps: 146, steps per second: 110, episode reward: -659.043, mean reward: -4.514 [-100.000,  2.171], mean action: 1.651 [0.000, 3.000],  loss: 75924408061272848.000000, mse: 52946265343106727936.000000, mean_q: 6972326845.369863, mean_eps: 0.536875
  77395/150000: episode: 698, duration: 1.214s, episode steps: 134, steps per second: 110, episode reward: -580.329, mean reward: -4.331 [-100.000,  2.278], mean action: 1.746 [0.000, 3.000],  loss: 115259224086720448.000000, mse: 53762472026906329088.000000, mean_q: 7067060006.208955, mean_eps: 0.536035
  77494/150000: episode: 699, duration: 0.882s, episode steps:  99, steps per second: 112, episode reward: -305.534, mean reward: -3.086 [-100.000,  3.308], mean action: 1.828 [0.000, 3.000],  loss: 154779329114184480.000000, mse: 52695738919764557824.000000, mean_q: 6980849110.626263, mean_eps: 0.535336
  77586/150000: episode: 700, duration: 0.791s, episode steps:  92, steps per second: 116, episode reward: -347.247, mean reward: -3.774 [-100.000,  0.549], mean action: 1.663 [0.000, 3.000],  loss: 58296863860309304.000000, mse: 51322948857454018560.000000, mean_q: 6868772680.347826, mean_eps: 0.534763
  77818/150000: episode: 701, duration: 2.469s, episode steps: 232, steps per second:  94, episode reward: -825.126, mean reward: -3.557 [-100.000,  2.042], mean action: 1.698 [0.000, 3.000],  loss: 93962033833759712.000000, mse: 54095768639013863424.000000, mean_q: 7062487331.310345, mean_eps: 0.533791
  77992/150000: episode: 702, duration: 1.572s, episode steps: 174, steps per second: 111, episode reward: -411.156, mean reward: -2.363 [-100.000,  3.215], mean action: 1.759 [0.000, 3.000],  loss: 123589020105337328.000000, mse: 55689262531884728320.000000, mean_q: 7144068669.793103, mean_eps: 0.532573
  78081/150000: episode: 703, duration: 1.099s, episode steps:  89, steps per second:  81, episode reward: -285.802, mean reward: -3.211 [-100.000, 22.403], mean action: 1.820 [0.000, 3.000],  loss: 147222943817572640.000000, mse: 56366081758550220800.000000, mean_q: 7284043735.730337, mean_eps: 0.531784
  78165/150000: episode: 704, duration: 0.820s, episode steps:  84, steps per second: 102, episode reward: -250.443, mean reward: -2.981 [-100.000,  0.709], mean action: 1.488 [0.000, 3.000],  loss: 111996830469349760.000000, mse: 55151430000826671104.000000, mean_q: 7151337575.619047, mean_eps: 0.531265
  78306/150000: episode: 705, duration: 1.249s, episode steps: 141, steps per second: 113, episode reward: -186.349, mean reward: -1.322 [-100.000,  1.660], mean action: 1.730 [0.000, 3.000],  loss: 125076289793723808.000000, mse: 55732415978714128384.000000, mean_q: 7136833448.851064, mean_eps: 0.530590
  78401/150000: episode: 706, duration: 0.849s, episode steps:  95, steps per second: 112, episode reward: -194.735, mean reward: -2.050 [-100.000,  1.160], mean action: 1.800 [0.000, 3.000],  loss: 184968132796543616.000000, mse: 56237667824353943552.000000, mean_q: 7226885729.010527, mean_eps: 0.529882
  78603/150000: episode: 707, duration: 1.796s, episode steps: 202, steps per second: 112, episode reward: -493.223, mean reward: -2.442 [-100.000,  4.334], mean action: 1.619 [0.000, 3.000],  loss: 128197990338571360.000000, mse: 55477902844966502400.000000, mean_q: 7117942958.891089, mean_eps: 0.528991
  78814/150000: episode: 708, duration: 1.876s, episode steps: 211, steps per second: 112, episode reward: -549.205, mean reward: -2.603 [-100.000,  2.505], mean action: 1.701 [0.000, 3.000],  loss: 92449493523892656.000000, mse: 56286844323722387456.000000, mean_q: 7212166059.071090, mean_eps: 0.527752
  78960/150000: episode: 709, duration: 1.898s, episode steps: 146, steps per second:  77, episode reward: -260.536, mean reward: -1.784 [-100.000,  1.562], mean action: 1.664 [0.000, 3.000],  loss: 192589840583732192.000000, mse: 56673298433796874240.000000, mean_q: 7206316259.945206, mean_eps: 0.526681
  79103/150000: episode: 710, duration: 1.668s, episode steps: 143, steps per second:  86, episode reward: -557.271, mean reward: -3.897 [-100.000,  2.071], mean action: 1.783 [0.000, 3.000],  loss: 192005695080511808.000000, mse: 57150553327272411136.000000, mean_q: 7253676544.000000, mean_eps: 0.525814
  79206/150000: episode: 711, duration: 1.111s, episode steps: 103, steps per second:  93, episode reward: -495.220, mean reward: -4.808 [-100.000,  0.755], mean action: 1.689 [0.000, 3.000],  loss: 186710774468972256.000000, mse: 56984903789587513344.000000, mean_q: 7231963916.427184, mean_eps: 0.525076
  79316/150000: episode: 712, duration: 0.970s, episode steps: 110, steps per second: 113, episode reward: -399.347, mean reward: -3.630 [-100.000,  0.829], mean action: 1.600 [0.000, 3.000],  loss: 136267714211439040.000000, mse: 57558784122603913216.000000, mean_q: 7220692819.781818, mean_eps: 0.524437
  79506/150000: episode: 713, duration: 1.652s, episode steps: 190, steps per second: 115, episode reward: -476.280, mean reward: -2.507 [-100.000,  4.457], mean action: 1.800 [0.000, 3.000],  loss: 170880681612408128.000000, mse: 56710068805509906432.000000, mean_q: 7190576397.473684, mean_eps: 0.523537
  79594/150000: episode: 714, duration: 0.732s, episode steps:  88, steps per second: 120, episode reward: -327.844, mean reward: -3.726 [-100.000,  0.707], mean action: 1.727 [0.000, 3.000],  loss: 229620912243462528.000000, mse: 58699139088914579456.000000, mean_q: 7358154891.636364, mean_eps: 0.522703
  79753/150000: episode: 715, duration: 1.445s, episode steps: 159, steps per second: 110, episode reward: -582.805, mean reward: -3.665 [-100.000,  2.532], mean action: 1.780 [0.000, 3.000],  loss: 194020287993249184.000000, mse: 58263106219088502784.000000, mean_q: 7338035171.018867, mean_eps: 0.521962
  79963/150000: episode: 716, duration: 1.866s, episode steps: 210, steps per second: 113, episode reward: -77.169, mean reward: -0.367 [-100.000, 16.183], mean action: 1.762 [0.000, 3.000],  loss: 114666461470177616.000000, mse: 57511000750447034368.000000, mean_q: 7257726819.961905, mean_eps: 0.520855
  80098/150000: episode: 717, duration: 1.188s, episode steps: 135, steps per second: 114, episode reward: -344.450, mean reward: -2.551 [-100.000,  4.381], mean action: 1.756 [0.000, 3.000],  loss: 113294637655535952.000000, mse: 58035365320085471232.000000, mean_q: 7273971010.370370, mean_eps: 0.519820
  80561/150000: episode: 718, duration: 4.143s, episode steps: 463, steps per second: 112, episode reward: -863.044, mean reward: -1.864 [-100.000,  4.715], mean action: 1.840 [0.000, 3.000],  loss: 155596128931695136.000000, mse: 58612950143980552192.000000, mean_q: 7351232786.246221, mean_eps: 0.518026
  80639/150000: episode: 719, duration: 0.650s, episode steps:  78, steps per second: 120, episode reward: -303.448, mean reward: -3.890 [-100.000,  0.671], mean action: 1.769 [0.000, 3.000],  loss: 93012291411596624.000000, mse: 58785361550290116608.000000, mean_q: 7330192167.384615, mean_eps: 0.516403
  80807/150000: episode: 720, duration: 1.396s, episode steps: 168, steps per second: 120, episode reward: -524.087, mean reward: -3.120 [-100.000,  2.726], mean action: 1.833 [0.000, 3.000],  loss: 124084575087411984.000000, mse: 58829712416904839168.000000, mean_q: 7338435547.428572, mean_eps: 0.515665
  80914/150000: episode: 721, duration: 0.952s, episode steps: 107, steps per second: 112, episode reward: -615.744, mean reward: -5.755 [-100.000,  1.616], mean action: 1.907 [0.000, 3.000],  loss: 101156220846767216.000000, mse: 60072053346201157632.000000, mean_q: 7396957035.663551, mean_eps: 0.514840
  81075/150000: episode: 722, duration: 1.342s, episode steps: 161, steps per second: 120, episode reward: -661.860, mean reward: -4.111 [-100.000,  2.352], mean action: 1.807 [0.000, 3.000],  loss: 90434053226036032.000000, mse: 58181584514996166656.000000, mean_q: 7313302070.062112, mean_eps: 0.514036
  81164/150000: episode: 723, duration: 0.822s, episode steps:  89, steps per second: 108, episode reward: -465.964, mean reward: -5.236 [-100.000,  0.483], mean action: 1.865 [0.000, 3.000],  loss: 156407019600836064.000000, mse: 58241534258751168512.000000, mean_q: 7273128833.438202, mean_eps: 0.513286
  81259/150000: episode: 724, duration: 0.821s, episode steps:  95, steps per second: 116, episode reward: -362.578, mean reward: -3.817 [-100.000,  3.077], mean action: 1.779 [0.000, 3.000],  loss: 72838243984828704.000000, mse: 60174423467922194432.000000, mean_q: 7458644792.589474, mean_eps: 0.512734
  81343/150000: episode: 725, duration: 0.742s, episode steps:  84, steps per second: 113, episode reward: -426.971, mean reward: -5.083 [-100.000,  0.708], mean action: 1.774 [0.000, 3.000],  loss: 175968104382601312.000000, mse: 62010320973846831104.000000, mean_q: 7582503795.809524, mean_eps: 0.512197
  81461/150000: episode: 726, duration: 1.005s, episode steps: 118, steps per second: 117, episode reward: -36.705, mean reward: -0.311 [-100.000, 24.089], mean action: 1.695 [0.000, 3.000],  loss: 243187855429130496.000000, mse: 59720970220188205056.000000, mean_q: 7380234617.491526, mean_eps: 0.511591
  81544/150000: episode: 727, duration: 0.780s, episode steps:  83, steps per second: 106, episode reward: -210.828, mean reward: -2.540 [-100.000,  4.018], mean action: 1.723 [0.000, 3.000],  loss: 229260925775652096.000000, mse: 59434812410207723520.000000, mean_q: 7417870397.686747, mean_eps: 0.510988
  81627/150000: episode: 728, duration: 0.809s, episode steps:  83, steps per second: 103, episode reward: -419.894, mean reward: -5.059 [-100.000,  0.732], mean action: 1.711 [0.000, 3.000],  loss: 109741848254710944.000000, mse: 60769975980049235968.000000, mean_q: 7481738789.012048, mean_eps: 0.510490
  81724/150000: episode: 729, duration: 0.898s, episode steps:  97, steps per second: 108, episode reward: -237.722, mean reward: -2.451 [-100.000,  4.622], mean action: 1.794 [0.000, 3.000],  loss: 100236156432580272.000000, mse: 59644349290169524224.000000, mean_q: 7386602047.340206, mean_eps: 0.509950
  81807/150000: episode: 730, duration: 0.730s, episode steps:  83, steps per second: 114, episode reward: -479.508, mean reward: -5.777 [-100.000,  0.524], mean action: 1.687 [0.000, 3.000],  loss: 205945836328808992.000000, mse: 59531796224284147712.000000, mean_q: 7316192638.457831, mean_eps: 0.509410
  81907/150000: episode: 731, duration: 0.919s, episode steps: 100, steps per second: 109, episode reward: -269.607, mean reward: -2.696 [-100.000,  2.303], mean action: 1.730 [0.000, 3.000],  loss: 128923186962985120.000000, mse: 60496171613473538048.000000, mean_q: 7466430791.680000, mean_eps: 0.508861
  82093/150000: episode: 732, duration: 1.583s, episode steps: 186, steps per second: 118, episode reward: -582.839, mean reward: -3.134 [-100.000,  2.401], mean action: 1.710 [0.000, 3.000],  loss: 181203079997939456.000000, mse: 60372785422588682240.000000, mean_q: 7423066695.569893, mean_eps: 0.508003
  82360/150000: episode: 733, duration: 2.212s, episode steps: 267, steps per second: 121, episode reward: -908.089, mean reward: -3.401 [-100.000,  2.027], mean action: 1.667 [0.000, 3.000],  loss: 184732754049342048.000000, mse: 61269690436716027904.000000, mean_q: 7500025570.277154, mean_eps: 0.506644
  82538/150000: episode: 734, duration: 1.484s, episode steps: 178, steps per second: 120, episode reward: -644.164, mean reward: -3.619 [-100.000,  1.978], mean action: 1.820 [0.000, 3.000],  loss: 78568653492272224.000000, mse: 62110081864034779136.000000, mean_q: 7566111680.719101, mean_eps: 0.505309
  82652/150000: episode: 735, duration: 1.005s, episode steps: 114, steps per second: 113, episode reward: -364.170, mean reward: -3.194 [-100.000,  2.248], mean action: 1.667 [0.000, 3.000],  loss: 164993060368754784.000000, mse: 63024539246438490112.000000, mean_q: 7597482419.649123, mean_eps: 0.504433
  82842/150000: episode: 736, duration: 1.523s, episode steps: 190, steps per second: 125, episode reward: -606.126, mean reward: -3.190 [-100.000,  3.270], mean action: 1.811 [0.000, 3.000],  loss: 184856162213609984.000000, mse: 63207015770221895680.000000, mean_q: 7585054755.031579, mean_eps: 0.503521
  83054/150000: episode: 737, duration: 1.795s, episode steps: 212, steps per second: 118, episode reward: -662.161, mean reward: -3.123 [-100.000,  2.572], mean action: 1.731 [0.000, 3.000],  loss: 122877977243240560.000000, mse: 62762230668169928704.000000, mean_q: 7590629202.113208, mean_eps: 0.502315
  83162/150000: episode: 738, duration: 0.888s, episode steps: 108, steps per second: 122, episode reward: -310.218, mean reward: -2.872 [-100.000, 18.750], mean action: 1.759 [0.000, 3.000],  loss: 132048448742606624.000000, mse: 62794853050619625472.000000, mean_q: 7527458280.296296, mean_eps: 0.501355
  83472/150000: episode: 739, duration: 2.598s, episode steps: 310, steps per second: 119, episode reward: -174.994, mean reward: -0.564 [-100.000, 12.432], mean action: 1.748 [0.000, 3.000],  loss: 149529585959097184.000000, mse: 64689940130229444608.000000, mean_q: 7699661108.851613, mean_eps: 0.500101
  83573/150000: episode: 740, duration: 0.807s, episode steps: 101, steps per second: 125, episode reward: -314.852, mean reward: -3.117 [-100.000,  4.522], mean action: 1.673 [0.000, 3.000],  loss: 101754213895220960.000000, mse: 62845536345855598592.000000, mean_q: 7586941009.108911, mean_eps: 0.498868
  83706/150000: episode: 741, duration: 1.103s, episode steps: 133, steps per second: 121, episode reward: -212.793, mean reward: -1.600 [-100.000,  4.440], mean action: 1.699 [0.000, 3.000],  loss: 182451664346385536.000000, mse: 63469517108024582144.000000, mean_q: 7622614674.285714, mean_eps: 0.498166
  83814/150000: episode: 742, duration: 0.869s, episode steps: 108, steps per second: 124, episode reward: -401.113, mean reward: -3.714 [-100.000,  1.205], mean action: 1.787 [0.000, 3.000],  loss: 153089585236187072.000000, mse: 64433478726464905216.000000, mean_q: 7685905981.629630, mean_eps: 0.497443
  84024/150000: episode: 743, duration: 1.752s, episode steps: 210, steps per second: 120, episode reward: -752.343, mean reward: -3.583 [-100.000,  2.432], mean action: 1.624 [0.000, 3.000],  loss: 110688978539538016.000000, mse: 64358759880300634112.000000, mean_q: 7677164934.095238, mean_eps: 0.496489
  84127/150000: episode: 744, duration: 0.913s, episode steps: 103, steps per second: 113, episode reward: -554.042, mean reward: -5.379 [-100.000,  0.674], mean action: 1.709 [0.000, 3.000],  loss: 61428331878081704.000000, mse: 64833269412324753408.000000, mean_q: 7682303875.728155, mean_eps: 0.495550
  84220/150000: episode: 745, duration: 0.906s, episode steps:  93, steps per second: 103, episode reward: -245.432, mean reward: -2.639 [-100.000,  0.720], mean action: 1.742 [0.000, 3.000],  loss: 219512468923186144.000000, mse: 63247893270651289600.000000, mean_q: 7555390056.602151, mean_eps: 0.494962
  84315/150000: episode: 746, duration: 0.853s, episode steps:  95, steps per second: 111, episode reward: -244.040, mean reward: -2.569 [-100.000,  1.589], mean action: 1.537 [0.000, 3.000],  loss: 98626422715965360.000000, mse: 65885031498121732096.000000, mean_q: 7731434862.484211, mean_eps: 0.494398
  84446/150000: episode: 747, duration: 1.666s, episode steps: 131, steps per second:  79, episode reward: -487.219, mean reward: -3.719 [-100.000,  1.496], mean action: 1.641 [0.000, 3.000],  loss: 234088500727179904.000000, mse: 65600463027378823168.000000, mean_q: 7703005160.549619, mean_eps: 0.493720
  84690/150000: episode: 748, duration: 3.523s, episode steps: 244, steps per second:  69, episode reward: -879.055, mean reward: -3.603 [-100.000,  1.887], mean action: 1.836 [0.000, 3.000],  loss: 156554674050188064.000000, mse: 65522073688484962304.000000, mean_q: 7733101803.016394, mean_eps: 0.492595
  84799/150000: episode: 749, duration: 1.126s, episode steps: 109, steps per second:  97, episode reward: -321.855, mean reward: -2.953 [-100.000,  4.747], mean action: 1.835 [0.000, 3.000],  loss: 373669616454833152.000000, mse: 66287436220545736704.000000, mean_q: 7811362111.412844, mean_eps: 0.491536
  84899/150000: episode: 750, duration: 1.305s, episode steps: 100, steps per second:  77, episode reward: -312.876, mean reward: -3.129 [-100.000,  1.078], mean action: 1.770 [0.000, 3.000],  loss: 138309160183498016.000000, mse: 66592520178442207232.000000, mean_q: 7791811363.840000, mean_eps: 0.490909
  85073/150000: episode: 751, duration: 1.956s, episode steps: 174, steps per second:  89, episode reward: -192.737, mean reward: -1.108 [-100.000,  4.292], mean action: 1.690 [0.000, 3.000],  loss: 123918212337635744.000000, mse: 67038465003454988288.000000, mean_q: 7822113515.402299, mean_eps: 0.490087
  85218/150000: episode: 752, duration: 1.235s, episode steps: 145, steps per second: 117, episode reward: -352.329, mean reward: -2.430 [-100.000,  1.734], mean action: 1.724 [0.000, 3.000],  loss: 80925735188362144.000000, mse: 67615811289785360384.000000, mean_q: 7887323997.572413, mean_eps: 0.489130
  85307/150000: episode: 753, duration: 0.713s, episode steps:  89, steps per second: 125, episode reward: -488.269, mean reward: -5.486 [-100.000,  0.504], mean action: 1.764 [0.000, 3.000],  loss: 192893094163031776.000000, mse: 70930806961096392704.000000, mean_q: 8091531637.932584, mean_eps: 0.488428
  85458/150000: episode: 754, duration: 1.535s, episode steps: 151, steps per second:  98, episode reward: -254.296, mean reward: -1.684 [-100.000,  3.327], mean action: 1.722 [0.000, 3.000],  loss: 222442676832198400.000000, mse: 69375579965004857344.000000, mean_q: 7976184828.609271, mean_eps: 0.487708
  85548/150000: episode: 755, duration: 0.854s, episode steps:  90, steps per second: 105, episode reward: -503.229, mean reward: -5.591 [-100.000,  0.176], mean action: 1.689 [0.000, 3.000],  loss: 232611487504446976.000000, mse: 69907219676124143616.000000, mean_q: 8020637513.955556, mean_eps: 0.486985
  85630/150000: episode: 756, duration: 0.777s, episode steps:  82, steps per second: 106, episode reward: -431.894, mean reward: -5.267 [-100.000,  0.937], mean action: 1.793 [0.000, 3.000],  loss: 206712471021062496.000000, mse: 70018452323577536512.000000, mean_q: 8010778112.000000, mean_eps: 0.486469
  85722/150000: episode: 757, duration: 0.937s, episode steps:  92, steps per second:  98, episode reward: -334.342, mean reward: -3.634 [-100.000,  9.569], mean action: 1.641 [0.000, 3.000],  loss: 88800799595361504.000000, mse: 70074321447634362368.000000, mean_q: 8074534956.521739, mean_eps: 0.485947
  85876/150000: episode: 758, duration: 1.501s, episode steps: 154, steps per second: 103, episode reward: -489.146, mean reward: -3.176 [-100.000,  4.283], mean action: 1.812 [0.000, 3.000],  loss: 159941532925521344.000000, mse: 68252745216477831168.000000, mean_q: 7863262999.272727, mean_eps: 0.485209
  85980/150000: episode: 759, duration: 1.090s, episode steps: 104, steps per second:  95, episode reward: -266.196, mean reward: -2.560 [-100.000,  2.883], mean action: 1.663 [0.000, 3.000],  loss: 238401013258810912.000000, mse: 68616301017606209536.000000, mean_q: 7892111158.153846, mean_eps: 0.484435
  86061/150000: episode: 760, duration: 0.826s, episode steps:  81, steps per second:  98, episode reward: -425.691, mean reward: -5.255 [-100.000,  0.602], mean action: 1.827 [0.000, 3.000],  loss: 157232512834906784.000000, mse: 69817721726380802048.000000, mean_q: 7973991809.580247, mean_eps: 0.483880
  86152/150000: episode: 761, duration: 0.833s, episode steps:  91, steps per second: 109, episode reward: -363.292, mean reward: -3.992 [-100.000,  0.926], mean action: 1.857 [0.000, 3.000],  loss: 169403979617847936.000000, mse: 70908594962516631552.000000, mean_q: 8026947280.175824, mean_eps: 0.483364
  86227/150000: episode: 762, duration: 0.698s, episode steps:  75, steps per second: 107, episode reward: -418.220, mean reward: -5.576 [-100.000, -0.156], mean action: 1.747 [0.000, 3.000],  loss: 125798147319370432.000000, mse: 69555100471375806464.000000, mean_q: 7991027179.520000, mean_eps: 0.482866
  86316/150000: episode: 763, duration: 0.820s, episode steps:  89, steps per second: 108, episode reward: -468.613, mean reward: -5.265 [-100.000,  0.670], mean action: 1.663 [0.000, 3.000],  loss: 189175582816124384.000000, mse: 72345037904997957632.000000, mean_q: 8204392568.808989, mean_eps: 0.482374
  86433/150000: episode: 764, duration: 0.968s, episode steps: 117, steps per second: 121, episode reward: -146.144, mean reward: -1.249 [-100.000,  3.057], mean action: 1.786 [0.000, 3.000],  loss: 258673651268553184.000000, mse: 70633315243814158336.000000, mean_q: 8078686803.145299, mean_eps: 0.481756
  86513/150000: episode: 765, duration: 0.705s, episode steps:  80, steps per second: 114, episode reward: -491.213, mean reward: -6.140 [-100.000,  0.510], mean action: 1.800 [0.000, 3.000],  loss: 258535041226558656.000000, mse: 71097624564441128960.000000, mean_q: 8111292851.200000, mean_eps: 0.481165
  86600/150000: episode: 766, duration: 0.709s, episode steps:  87, steps per second: 123, episode reward: -421.947, mean reward: -4.850 [-100.000,  1.139], mean action: 1.805 [0.000, 3.000],  loss: 229417442915433568.000000, mse: 70819112683732779008.000000, mean_q: 8047358146.206897, mean_eps: 0.480664
  86781/150000: episode: 767, duration: 1.514s, episode steps: 181, steps per second: 120, episode reward: -537.983, mean reward: -2.972 [-100.000,  4.785], mean action: 1.685 [0.000, 3.000],  loss: 185935910085688064.000000, mse: 70138013448522268672.000000, mean_q: 8023931892.685082, mean_eps: 0.479860
  86893/150000: episode: 768, duration: 0.906s, episode steps: 112, steps per second: 124, episode reward: -487.843, mean reward: -4.356 [-100.000,  2.001], mean action: 1.821 [0.000, 3.000],  loss: 103518185064646944.000000, mse: 69675909033458327552.000000, mean_q: 7964866482.285714, mean_eps: 0.478981
  87232/150000: episode: 769, duration: 2.881s, episode steps: 339, steps per second: 118, episode reward: -547.350, mean reward: -1.615 [-100.000,  3.598], mean action: 1.723 [0.000, 3.000],  loss: 152918364732829504.000000, mse: 71511910555637448704.000000, mean_q: 8127307659.705015, mean_eps: 0.477628
  87322/150000: episode: 770, duration: 0.736s, episode steps:  90, steps per second: 122, episode reward: -344.073, mean reward: -3.823 [-100.000,  1.249], mean action: 1.900 [0.000, 3.000],  loss: 141690099720741040.000000, mse: 72835994076686893056.000000, mean_q: 8205578535.822222, mean_eps: 0.476341
  87449/150000: episode: 771, duration: 1.029s, episode steps: 127, steps per second: 123, episode reward: -502.375, mean reward: -3.956 [-100.000,  3.776], mean action: 1.835 [0.000, 3.000],  loss: 272259543296758688.000000, mse: 71325168305775665152.000000, mean_q: 8075951410.393701, mean_eps: 0.475690
  87560/150000: episode: 772, duration: 0.951s, episode steps: 111, steps per second: 117, episode reward: -323.840, mean reward: -2.917 [-100.000,  1.559], mean action: 1.712 [0.000, 3.000],  loss: 209671089118787424.000000, mse: 73958523485775233024.000000, mean_q: 8254976013.837838, mean_eps: 0.474976
  87644/150000: episode: 773, duration: 0.703s, episode steps:  84, steps per second: 120, episode reward: -236.478, mean reward: -2.815 [-100.000,  1.661], mean action: 1.702 [0.000, 3.000],  loss: 172714412048046944.000000, mse: 74304705481674653696.000000, mean_q: 8325276470.857142, mean_eps: 0.474391
  87819/150000: episode: 774, duration: 1.495s, episode steps: 175, steps per second: 117, episode reward: -734.015, mean reward: -4.194 [-100.000,  2.016], mean action: 1.880 [0.000, 3.000],  loss: 233729292236307456.000000, mse: 71841217261851123712.000000, mean_q: 8120715682.377143, mean_eps: 0.473614
  87995/150000: episode: 775, duration: 1.802s, episode steps: 176, steps per second:  98, episode reward: -731.716, mean reward: -4.157 [-100.000,  1.888], mean action: 1.898 [0.000, 3.000],  loss: 180760237286883328.000000, mse: 74656141760918077440.000000, mean_q: 8278130213.818182, mean_eps: 0.472561
  88157/150000: episode: 776, duration: 1.635s, episode steps: 162, steps per second:  99, episode reward: -509.391, mean reward: -3.144 [-100.000,  1.697], mean action: 1.753 [0.000, 3.000],  loss: 168203087543604768.000000, mse: 75656746476851478528.000000, mean_q: 8383003960.888889, mean_eps: 0.471547
  88295/150000: episode: 777, duration: 1.620s, episode steps: 138, steps per second:  85, episode reward: -545.697, mean reward: -3.954 [-100.000,  1.754], mean action: 1.833 [0.000, 3.000],  loss: 198385063224854624.000000, mse: 76601302415334096896.000000, mean_q: 8412690276.173913, mean_eps: 0.470647
  88390/150000: episode: 778, duration: 1.028s, episode steps:  95, steps per second:  92, episode reward: -278.511, mean reward: -2.932 [-100.000,  1.830], mean action: 1.747 [0.000, 3.000],  loss: 186482258137173888.000000, mse: 75217830589495672832.000000, mean_q: 8323402951.410526, mean_eps: 0.469948
  88527/150000: episode: 779, duration: 1.363s, episode steps: 137, steps per second: 100, episode reward: -373.529, mean reward: -2.726 [-100.000,  5.055], mean action: 1.774 [0.000, 3.000],  loss: 269773276547077152.000000, mse: 75862995723925815296.000000, mean_q: 8373377573.372263, mean_eps: 0.469252
  88680/150000: episode: 780, duration: 1.488s, episode steps: 153, steps per second: 103, episode reward: -329.799, mean reward: -2.156 [-100.000,  4.096], mean action: 1.791 [0.000, 3.000],  loss: 280949283726497216.000000, mse: 76834636285573496832.000000, mean_q: 8384089874.405229, mean_eps: 0.468382
  88926/150000: episode: 781, duration: 2.504s, episode steps: 246, steps per second:  98, episode reward: -635.458, mean reward: -2.583 [-100.000,  2.177], mean action: 1.691 [0.000, 3.000],  loss: 150974417754405440.000000, mse: 78407780661525053440.000000, mean_q: 8505828106.406504, mean_eps: 0.467185
  89016/150000: episode: 782, duration: 0.851s, episode steps:  90, steps per second: 106, episode reward: -474.551, mean reward: -5.273 [-100.000,  0.633], mean action: 1.722 [0.000, 3.000],  loss: 280626296204639904.000000, mse: 78461403476511866880.000000, mean_q: 8522761278.577778, mean_eps: 0.466177
  89099/150000: episode: 783, duration: 0.788s, episode steps:  83, steps per second: 105, episode reward: -476.982, mean reward: -5.747 [-100.000,  0.888], mean action: 1.807 [0.000, 3.000],  loss: 153545893208246400.000000, mse: 78645290613675556864.000000, mean_q: 8497908624.963856, mean_eps: 0.465658
  89171/150000: episode: 784, duration: 0.594s, episode steps:  72, steps per second: 121, episode reward: -506.205, mean reward: -7.031 [-100.000, -0.911], mean action: 1.694 [0.000, 3.000],  loss: 283246502722841248.000000, mse: 79820013089119535104.000000, mean_q: 8585102755.555555, mean_eps: 0.465193
  89326/150000: episode: 785, duration: 1.292s, episode steps: 155, steps per second: 120, episode reward: -827.773, mean reward: -5.340 [-100.000,  1.691], mean action: 1.723 [0.000, 3.000],  loss: 277691782813418112.000000, mse: 79229632200395407360.000000, mean_q: 8512819949.832258, mean_eps: 0.464512
  89423/150000: episode: 786, duration: 0.802s, episode steps:  97, steps per second: 121, episode reward: -309.039, mean reward: -3.186 [-100.000,  2.897], mean action: 1.866 [0.000, 3.000],  loss: 244517673224572224.000000, mse: 80897460998423691264.000000, mean_q: 8598194814.680412, mean_eps: 0.463756
  89587/150000: episode: 787, duration: 1.379s, episode steps: 164, steps per second: 119, episode reward: -649.577, mean reward: -3.961 [-100.000,  2.128], mean action: 1.720 [0.000, 3.000],  loss: 229997251869131136.000000, mse: 81117358342309298176.000000, mean_q: 8634157224.585365, mean_eps: 0.462973
  89687/150000: episode: 788, duration: 0.824s, episode steps: 100, steps per second: 121, episode reward: -62.371, mean reward: -0.624 [-100.000,  5.623], mean action: 1.670 [0.000, 3.000],  loss: 102193648531815792.000000, mse: 82359633029177933824.000000, mean_q: 8649125565.440001, mean_eps: 0.462181
  89776/150000: episode: 789, duration: 0.730s, episode steps:  89, steps per second: 122, episode reward: -415.067, mean reward: -4.664 [-100.000,  0.834], mean action: 1.809 [0.000, 3.000],  loss: 186573239727673856.000000, mse: 84274689897195880448.000000, mean_q: 8787675239.550562, mean_eps: 0.461614
  89922/150000: episode: 790, duration: 1.234s, episode steps: 146, steps per second: 118, episode reward: -257.777, mean reward: -1.766 [-100.000,  4.022], mean action: 1.856 [0.000, 3.000],  loss: 187900740126516000.000000, mse: 81396173746602049536.000000, mean_q: 8693694250.082191, mean_eps: 0.460909
  90042/150000: episode: 791, duration: 1.029s, episode steps: 120, steps per second: 117, episode reward: -254.305, mean reward: -2.119 [-100.000,  2.111], mean action: 1.742 [0.000, 3.000],  loss: 306260917358410688.000000, mse: 80671552958994841600.000000, mean_q: 8616198438.400000, mean_eps: 0.460111
  90148/150000: episode: 792, duration: 1.053s, episode steps: 106, steps per second: 101, episode reward: -570.131, mean reward: -5.379 [-100.000,  0.915], mean action: 1.689 [0.000, 3.000],  loss: 204711853073762720.000000, mse: 81001962758720290816.000000, mean_q: 8646649401.962265, mean_eps: 0.459433
  90355/150000: episode: 793, duration: 1.772s, episode steps: 207, steps per second: 117, episode reward: -807.350, mean reward: -3.900 [-100.000,  2.215], mean action: 1.744 [0.000, 3.000],  loss: 211566176944522496.000000, mse: 82628731641885622272.000000, mean_q: 8702833698.628019, mean_eps: 0.458494
  90557/150000: episode: 794, duration: 1.699s, episode steps: 202, steps per second: 119, episode reward: -726.706, mean reward: -3.598 [-100.000,  2.054], mean action: 1.713 [0.000, 3.000],  loss: 283711583040322784.000000, mse: 84197944383353487360.000000, mean_q: 8819482778.613861, mean_eps: 0.457267
  90672/150000: episode: 795, duration: 1.095s, episode steps: 115, steps per second: 105, episode reward: -511.435, mean reward: -4.447 [-100.000,  3.944], mean action: 1.809 [0.000, 3.000],  loss: 421146375837311168.000000, mse: 85037935351803314176.000000, mean_q: 8795485500.104347, mean_eps: 0.456316
  90818/150000: episode: 796, duration: 1.592s, episode steps: 146, steps per second:  92, episode reward: -532.479, mean reward: -3.647 [-100.000,  1.754], mean action: 1.767 [0.000, 3.000],  loss: 193038691164972480.000000, mse: 86447078632492892160.000000, mean_q: 8908647936.000000, mean_eps: 0.455533
  90906/150000: episode: 797, duration: 1.020s, episode steps:  88, steps per second:  86, episode reward: -305.088, mean reward: -3.467 [-100.000,  1.741], mean action: 1.682 [0.000, 3.000],  loss: 217536946276996928.000000, mse: 85822454672935157760.000000, mean_q: 8840341562.181818, mean_eps: 0.454831
  91012/150000: episode: 798, duration: 0.984s, episode steps: 106, steps per second: 108, episode reward: -485.878, mean reward: -4.584 [-100.000,  2.541], mean action: 1.868 [0.000, 3.000],  loss: 390196597649621312.000000, mse: 85822624363601068032.000000, mean_q: 8861951772.981133, mean_eps: 0.454249
  91094/150000: episode: 799, duration: 0.854s, episode steps:  82, steps per second:  96, episode reward: -422.630, mean reward: -5.154 [-100.000,  0.272], mean action: 1.634 [0.000, 3.000],  loss: 410190465469897344.000000, mse: 85901673237490368512.000000, mean_q: 8856475935.219513, mean_eps: 0.453685
  91179/150000: episode: 800, duration: 0.825s, episode steps:  85, steps per second: 103, episode reward: -560.666, mean reward: -6.596 [-100.000,  0.089], mean action: 1.788 [0.000, 3.000],  loss: 262104123464566080.000000, mse: 86830490185953099776.000000, mean_q: 8974918427.105883, mean_eps: 0.453184
  91304/150000: episode: 801, duration: 1.100s, episode steps: 125, steps per second: 114, episode reward: -442.848, mean reward: -3.543 [-100.000,  4.342], mean action: 1.776 [0.000, 3.000],  loss: 116995873461982192.000000, mse: 86420829859461595136.000000, mean_q: 8978311413.760000, mean_eps: 0.452554
  91590/150000: episode: 802, duration: 2.658s, episode steps: 286, steps per second: 108, episode reward: -318.950, mean reward: -1.115 [-100.000, 10.304], mean action: 1.839 [0.000, 3.000],  loss: 249181794236449344.000000, mse: 84974181452105973760.000000, mean_q: 8849763618.013987, mean_eps: 0.451321
  91833/150000: episode: 803, duration: 2.398s, episode steps: 243, steps per second: 101, episode reward: -794.704, mean reward: -3.270 [-100.000,  2.468], mean action: 1.695 [0.000, 3.000],  loss: 243277479733754272.000000, mse: 86956242667057184768.000000, mean_q: 8947307783.374485, mean_eps: 0.449734
  91973/150000: episode: 804, duration: 1.317s, episode steps: 140, steps per second: 106, episode reward: -419.197, mean reward: -2.994 [-100.000,  1.404], mean action: 1.793 [0.000, 3.000],  loss: 168232016733656096.000000, mse: 89726615871184617472.000000, mean_q: 9059915585.828571, mean_eps: 0.448585
  92241/150000: episode: 805, duration: 2.292s, episode steps: 268, steps per second: 117, episode reward: -974.214, mean reward: -3.635 [-100.000,  1.659], mean action: 1.761 [0.000, 3.000],  loss: 352886586558022144.000000, mse: 89883544410487898112.000000, mean_q: 9066516405.492537, mean_eps: 0.447361
  92337/150000: episode: 806, duration: 0.793s, episode steps:  96, steps per second: 121, episode reward: -621.120, mean reward: -6.470 [-100.000,  0.950], mean action: 1.625 [0.000, 3.000],  loss: 588673671390123392.000000, mse: 91363128206999339008.000000, mean_q: 9107529562.666666, mean_eps: 0.446269
  92446/150000: episode: 807, duration: 0.955s, episode steps: 109, steps per second: 114, episode reward: -481.259, mean reward: -4.415 [-100.000,  1.064], mean action: 1.771 [0.000, 3.000],  loss: 115286824456444224.000000, mse: 90160949857086537728.000000, mean_q: 9022840273.027523, mean_eps: 0.445654
  92550/150000: episode: 808, duration: 0.874s, episode steps: 104, steps per second: 119, episode reward: -588.890, mean reward: -5.662 [-100.000,  1.004], mean action: 1.817 [0.000, 3.000],  loss: 88748279796415248.000000, mse: 89905079958377037824.000000, mean_q: 9061742670.769230, mean_eps: 0.445015
  92668/150000: episode: 809, duration: 1.011s, episode steps: 118, steps per second: 117, episode reward: -173.156, mean reward: -1.467 [-100.000,  3.279], mean action: 1.780 [0.000, 3.000],  loss: 214551233271808832.000000, mse: 92384226700907446272.000000, mean_q: 9207883463.593220, mean_eps: 0.444349
  92763/150000: episode: 810, duration: 0.783s, episode steps:  95, steps per second: 121, episode reward: -467.583, mean reward: -4.922 [-100.000,  0.883], mean action: 1.863 [0.000, 3.000],  loss: 397152539983630528.000000, mse: 93280789041592188928.000000, mean_q: 9206382597.389473, mean_eps: 0.443710
  92908/150000: episode: 811, duration: 1.282s, episode steps: 145, steps per second: 113, episode reward: -754.044, mean reward: -5.200 [-100.000,  2.238], mean action: 1.924 [0.000, 3.000],  loss: 166767207834723136.000000, mse: 89932546507498389504.000000, mean_q: 9084576361.931034, mean_eps: 0.442990
  93020/150000: episode: 812, duration: 0.974s, episode steps: 112, steps per second: 115, episode reward: -427.696, mean reward: -3.819 [-100.000,  0.598], mean action: 1.777 [0.000, 3.000],  loss: 366416855206274176.000000, mse: 92027193719749263360.000000, mean_q: 9170752256.000000, mean_eps: 0.442219
  93208/150000: episode: 813, duration: 1.611s, episode steps: 188, steps per second: 117, episode reward: -836.514, mean reward: -4.450 [-100.000,  2.397], mean action: 1.846 [0.000, 3.000],  loss: 306611214797024832.000000, mse: 92151262447234973696.000000, mean_q: 9190357169.021276, mean_eps: 0.441319
  93446/150000: episode: 814, duration: 2.307s, episode steps: 238, steps per second: 103, episode reward: -876.740, mean reward: -3.684 [-100.000,  1.702], mean action: 1.790 [0.000, 3.000],  loss: 333255868019303424.000000, mse: 92249470475313135616.000000, mean_q: 9130565270.588236, mean_eps: 0.440041
  93525/150000: episode: 815, duration: 0.740s, episode steps:  79, steps per second: 107, episode reward: -506.064, mean reward: -6.406 [-100.000,  0.793], mean action: 1.937 [0.000, 3.000],  loss: 339240844473590464.000000, mse: 96366205204319240192.000000, mean_q: 9351282940.759493, mean_eps: 0.439090
  93620/150000: episode: 816, duration: 0.933s, episode steps:  95, steps per second: 102, episode reward: -413.661, mean reward: -4.354 [-100.000,  0.952], mean action: 1.832 [0.000, 3.000],  loss: 221196821942117824.000000, mse: 92921340820514799616.000000, mean_q: 9140114949.389473, mean_eps: 0.438568
  93807/150000: episode: 817, duration: 1.555s, episode steps: 187, steps per second: 120, episode reward: -725.881, mean reward: -3.882 [-100.000,  1.942], mean action: 1.690 [0.000, 3.000],  loss: 443758679900314880.000000, mse: 94403034749360357376.000000, mean_q: 9287795613.433155, mean_eps: 0.437722
  93886/150000: episode: 818, duration: 0.682s, episode steps:  79, steps per second: 116, episode reward: -427.781, mean reward: -5.415 [-100.000,  0.192], mean action: 1.747 [0.000, 3.000],  loss: 84087742537274224.000000, mse: 92583295810865840128.000000, mean_q: 9205654333.569620, mean_eps: 0.436924
  94054/150000: episode: 819, duration: 1.399s, episode steps: 168, steps per second: 120, episode reward: -799.915, mean reward: -4.761 [-100.000,  2.028], mean action: 1.756 [0.000, 3.000],  loss: 213272276115203232.000000, mse: 94335719524245307392.000000, mean_q: 9298837074.285715, mean_eps: 0.436183
  94190/150000: episode: 820, duration: 1.283s, episode steps: 136, steps per second: 106, episode reward: -436.590, mean reward: -3.210 [-100.000,  1.579], mean action: 1.757 [0.000, 3.000],  loss: 454105956582193280.000000, mse: 95687309351782055936.000000, mean_q: 9355483267.764706, mean_eps: 0.435271
  94347/150000: episode: 821, duration: 1.567s, episode steps: 157, steps per second: 100, episode reward: -400.874, mean reward: -2.553 [-100.000,  4.714], mean action: 1.803 [0.000, 3.000],  loss: 284333603937179040.000000, mse: 96382730325277032448.000000, mean_q: 9393353023.592358, mean_eps: 0.434392
  94455/150000: episode: 822, duration: 1.031s, episode steps: 108, steps per second: 105, episode reward: -402.555, mean reward: -3.727 [-100.000,  1.039], mean action: 1.824 [0.000, 3.000],  loss: 242506862406489440.000000, mse: 98922193636332404736.000000, mean_q: 9542956093.629629, mean_eps: 0.433597
  94591/150000: episode: 823, duration: 1.319s, episode steps: 136, steps per second: 103, episode reward: -392.870, mean reward: -2.889 [-100.000,  1.689], mean action: 1.728 [0.000, 3.000],  loss: 238523159575080224.000000, mse: 99273431267605200896.000000, mean_q: 9514782456.470589, mean_eps: 0.432865
  94772/150000: episode: 824, duration: 1.595s, episode steps: 181, steps per second: 113, episode reward: -281.425, mean reward: -1.555 [-100.000,  2.882], mean action: 1.901 [0.000, 3.000],  loss: 305577969275212224.000000, mse: 98605199313114742784.000000, mean_q: 9479366777.635359, mean_eps: 0.431914
  94892/150000: episode: 825, duration: 1.470s, episode steps: 120, steps per second:  82, episode reward: -535.426, mean reward: -4.462 [-100.000,  2.492], mean action: 1.667 [0.000, 3.000],  loss: 218003470047772672.000000, mse: 96529400367984885760.000000, mean_q: 9366357137.066668, mean_eps: 0.431011
  94985/150000: episode: 826, duration: 0.982s, episode steps:  93, steps per second:  95, episode reward: -366.305, mean reward: -3.939 [-100.000,  1.219], mean action: 1.817 [0.000, 3.000],  loss: 264194828104903680.000000, mse: 98961667061408727040.000000, mean_q: 9525644695.397850, mean_eps: 0.430372
  95167/150000: episode: 827, duration: 1.756s, episode steps: 182, steps per second: 104, episode reward: -613.721, mean reward: -3.372 [-100.000,  2.602], mean action: 1.797 [0.000, 3.000],  loss: 171788443803195008.000000, mse: 98300477335823532032.000000, mean_q: 9481503940.923077, mean_eps: 0.429547
  95266/150000: episode: 828, duration: 1.122s, episode steps:  99, steps per second:  88, episode reward: -373.413, mean reward: -3.772 [-100.000,  2.641], mean action: 1.737 [0.000, 3.000],  loss: 180323500790840672.000000, mse: 101277588358287441920.000000, mean_q: 9673492071.434343, mean_eps: 0.428704
  95396/150000: episode: 829, duration: 1.853s, episode steps: 130, steps per second:  70, episode reward: -253.393, mean reward: -1.949 [-100.000,  3.393], mean action: 1.685 [0.000, 3.000],  loss: 347894864361953216.000000, mse: 100577030916338139136.000000, mean_q: 9615872114.215385, mean_eps: 0.428017
  95508/150000: episode: 830, duration: 1.402s, episode steps: 112, steps per second:  80, episode reward: -490.590, mean reward: -4.380 [-100.000,  4.033], mean action: 1.875 [0.000, 3.000],  loss: 583185970654571904.000000, mse: 99238711894229090304.000000, mean_q: 9541115186.285715, mean_eps: 0.427291
  95615/150000: episode: 831, duration: 1.057s, episode steps: 107, steps per second: 101, episode reward: -493.255, mean reward: -4.610 [-100.000,  1.560], mean action: 1.664 [0.000, 3.000],  loss: 303138521379474752.000000, mse: 102246330853429362688.000000, mean_q: 9695498885.981308, mean_eps: 0.426634
  95832/150000: episode: 832, duration: 2.404s, episode steps: 217, steps per second:  90, episode reward: -457.364, mean reward: -2.108 [-100.000,  3.897], mean action: 1.682 [0.000, 3.000],  loss: 273537179702852128.000000, mse: 102557295053435666432.000000, mean_q: 9702206145.474655, mean_eps: 0.425662
  95951/150000: episode: 833, duration: 1.372s, episode steps: 119, steps per second:  87, episode reward: -358.989, mean reward: -3.017 [-100.000,  2.748], mean action: 1.765 [0.000, 3.000],  loss: 388661079068520256.000000, mse: 100446991806612553728.000000, mean_q: 9645375040.537815, mean_eps: 0.424654
  96090/150000: episode: 834, duration: 1.599s, episode steps: 139, steps per second:  87, episode reward: -536.753, mean reward: -3.862 [-100.000,  3.308], mean action: 1.791 [0.000, 3.000],  loss: 207591470541528544.000000, mse: 103850521814791176192.000000, mean_q: 9783700513.151079, mean_eps: 0.423880
  96165/150000: episode: 835, duration: 0.744s, episode steps:  75, steps per second: 101, episode reward: -401.690, mean reward: -5.356 [-100.000,  1.051], mean action: 1.867 [0.000, 3.000],  loss: 419452789156276224.000000, mse: 105490338902440214528.000000, mean_q: 9947922377.386667, mean_eps: 0.423238
  96248/150000: episode: 836, duration: 0.708s, episode steps:  83, steps per second: 117, episode reward: -467.601, mean reward: -5.634 [-100.000,  0.281], mean action: 1.855 [0.000, 3.000],  loss: 198552449492786112.000000, mse: 105481528044698976256.000000, mean_q: 9934423539.662651, mean_eps: 0.422764
  96327/150000: episode: 837, duration: 0.670s, episode steps:  79, steps per second: 118, episode reward: -280.288, mean reward: -3.548 [-100.000,  2.267], mean action: 1.734 [0.000, 3.000],  loss: 483653610826449920.000000, mse: 102077320790768746496.000000, mean_q: 9763148495.392405, mean_eps: 0.422278
  96457/150000: episode: 838, duration: 1.234s, episode steps: 130, steps per second: 105, episode reward: -391.319, mean reward: -3.010 [-100.000,  2.806], mean action: 1.808 [0.000, 3.000],  loss: 364781864177937728.000000, mse: 103302614350167310336.000000, mean_q: 9718667807.507692, mean_eps: 0.421651
  96549/150000: episode: 839, duration: 0.977s, episode steps:  92, steps per second:  94, episode reward: -357.473, mean reward: -3.886 [-100.000,  1.845], mean action: 1.761 [0.000, 3.000],  loss: 385397629141567552.000000, mse: 101661655539848331264.000000, mean_q: 9730552971.130434, mean_eps: 0.420985
  96640/150000: episode: 840, duration: 1.111s, episode steps:  91, steps per second:  82, episode reward: -386.466, mean reward: -4.247 [-100.000,  0.926], mean action: 1.868 [0.000, 3.000],  loss: 242633678800140000.000000, mse: 102396656795936145408.000000, mean_q: 9743007310.769230, mean_eps: 0.420436
  96770/150000: episode: 841, duration: 2.538s, episode steps: 130, steps per second:  51, episode reward: -474.462, mean reward: -3.650 [-100.000,  1.635], mean action: 1.854 [0.000, 3.000],  loss: 342240616718218048.000000, mse: 104019397652053295104.000000, mean_q: 9823966436.430769, mean_eps: 0.419773
  96930/150000: episode: 842, duration: 2.394s, episode steps: 160, steps per second:  67, episode reward: -687.172, mean reward: -4.295 [-100.000,  1.955], mean action: 1.750 [0.000, 3.000],  loss: 276780156465446912.000000, mse: 105445229079458267136.000000, mean_q: 9913743625.600000, mean_eps: 0.418903
  97019/150000: episode: 843, duration: 1.485s, episode steps:  89, steps per second:  60, episode reward: -448.993, mean reward: -5.045 [-100.000,  0.359], mean action: 1.854 [0.000, 3.000],  loss: 399695246960340864.000000, mse: 107687332379473018880.000000, mean_q: 9985001448.988764, mean_eps: 0.418156
  97141/150000: episode: 844, duration: 1.719s, episode steps: 122, steps per second:  71, episode reward: -442.003, mean reward: -3.623 [-100.000,  1.456], mean action: 1.762 [0.000, 3.000],  loss: 432709321373627520.000000, mse: 105788948532811792384.000000, mean_q: 9930073440.524590, mean_eps: 0.417523
  97248/150000: episode: 845, duration: 1.192s, episode steps: 107, steps per second:  90, episode reward: -371.517, mean reward: -3.472 [-100.000,  1.654], mean action: 1.729 [0.000, 3.000],  loss: 411415527951923712.000000, mse: 104777938743094312960.000000, mean_q: 9837127689.570093, mean_eps: 0.416836
  97352/150000: episode: 846, duration: 1.016s, episode steps: 104, steps per second: 102, episode reward: -453.879, mean reward: -4.364 [-100.000,  1.096], mean action: 1.837 [0.000, 3.000],  loss: 297615516690808832.000000, mse: 106523441790500274176.000000, mean_q: 9890736851.692308, mean_eps: 0.416203
  97438/150000: episode: 847, duration: 0.877s, episode steps:  86, steps per second:  98, episode reward: -368.145, mean reward: -4.281 [-100.000,  0.841], mean action: 1.860 [0.000, 3.000],  loss: 367053484160066496.000000, mse: 105895667221946810368.000000, mean_q: 9924890320.372093, mean_eps: 0.415633
  97582/150000: episode: 848, duration: 1.432s, episode steps: 144, steps per second: 101, episode reward: -502.510, mean reward: -3.490 [-100.000,  3.662], mean action: 1.812 [0.000, 3.000],  loss: 377415566926107968.000000, mse: 107556758046685609984.000000, mean_q: 9993235879.111111, mean_eps: 0.414943
  97725/150000: episode: 849, duration: 1.721s, episode steps: 143, steps per second:  83, episode reward: -293.293, mean reward: -2.051 [-100.000,  1.600], mean action: 1.720 [0.000, 3.000],  loss: 408409974281516672.000000, mse: 107778499075412230144.000000, mean_q: 10017267876.699301, mean_eps: 0.414082
  97915/150000: episode: 850, duration: 2.245s, episode steps: 190, steps per second:  85, episode reward: -711.977, mean reward: -3.747 [-100.000,  1.608], mean action: 1.700 [0.000, 3.000],  loss: 460161681278165568.000000, mse: 110126164566714040320.000000, mean_q: 10104476238.147367, mean_eps: 0.413083
  98007/150000: episode: 851, duration: 0.839s, episode steps:  92, steps per second: 110, episode reward: -292.636, mean reward: -3.181 [-100.000,  0.977], mean action: 1.609 [0.000, 3.000],  loss: 169298549962075968.000000, mse: 110179318802310938624.000000, mean_q: 10141692672.000000, mean_eps: 0.412237
  98121/150000: episode: 852, duration: 1.054s, episode steps: 114, steps per second: 108, episode reward: -658.695, mean reward: -5.778 [-100.000,  2.031], mean action: 1.886 [0.000, 3.000],  loss: 289417013658809536.000000, mse: 110797842285259243520.000000, mean_q: 10109206429.192982, mean_eps: 0.411619
  98197/150000: episode: 853, duration: 0.701s, episode steps:  76, steps per second: 108, episode reward: -446.676, mean reward: -5.877 [-100.000,  0.884], mean action: 1.697 [0.000, 3.000],  loss: 141420620574580304.000000, mse: 108528714341927337984.000000, mean_q: 10028071376.842106, mean_eps: 0.411049
  98412/150000: episode: 854, duration: 1.991s, episode steps: 215, steps per second: 108, episode reward: -643.969, mean reward: -2.995 [-100.000,  3.317], mean action: 1.698 [0.000, 3.000],  loss: 409237178857412480.000000, mse: 109515121156452597760.000000, mean_q: 10071515850.418604, mean_eps: 0.410176
  98743/150000: episode: 855, duration: 2.923s, episode steps: 331, steps per second: 113, episode reward: -1384.297, mean reward: -4.182 [-100.000,  3.442], mean action: 1.798 [0.000, 3.000],  loss: 445351636206536576.000000, mse: 111777683770987708416.000000, mean_q: 10135261940.398792, mean_eps: 0.408538
  98890/150000: episode: 856, duration: 1.285s, episode steps: 147, steps per second: 114, episode reward: -672.194, mean reward: -4.573 [-100.000,  1.794], mean action: 1.755 [0.000, 3.000],  loss: 368661742959902592.000000, mse: 115086379387675197440.000000, mean_q: 10254536690.068027, mean_eps: 0.407104
  99086/150000: episode: 857, duration: 1.714s, episode steps: 196, steps per second: 114, episode reward: -433.123, mean reward: -2.210 [-100.000,  4.680], mean action: 1.862 [0.000, 3.000],  loss: 752588452684601216.000000, mse: 114011672772140498944.000000, mean_q: 10160493502.693878, mean_eps: 0.406075
  99175/150000: episode: 858, duration: 0.799s, episode steps:  89, steps per second: 111, episode reward: -455.126, mean reward: -5.114 [-100.000,  0.202], mean action: 1.753 [0.000, 3.000],  loss: 318210454793763200.000000, mse: 115196904388790108160.000000, mean_q: 10272051832.808989, mean_eps: 0.405220
  99420/150000: episode: 859, duration: 2.152s, episode steps: 245, steps per second: 114, episode reward: -725.652, mean reward: -2.962 [-100.000,  3.612], mean action: 1.824 [0.000, 3.000],  loss: 475389874004588864.000000, mse: 116205315673005342720.000000, mean_q: 10315276496.979591, mean_eps: 0.404218
  99553/150000: episode: 860, duration: 1.311s, episode steps: 133, steps per second: 101, episode reward: -413.819, mean reward: -3.111 [-100.000,  2.018], mean action: 1.774 [0.000, 3.000],  loss: 321916552047171840.000000, mse: 119058360118251798528.000000, mean_q: 10445997286.977444, mean_eps: 0.403084
  99639/150000: episode: 861, duration: 0.870s, episode steps:  86, steps per second:  99, episode reward: -538.203, mean reward: -6.258 [-100.000,  0.606], mean action: 1.860 [0.000, 3.000],  loss: 368119935514303552.000000, mse: 121532249100181487616.000000, mean_q: 10392732076.651163, mean_eps: 0.402427
  99769/150000: episode: 862, duration: 1.287s, episode steps: 130, steps per second: 101, episode reward: -364.991, mean reward: -2.808 [-100.000,  2.180], mean action: 1.738 [0.000, 3.000],  loss: 516308733938929088.000000, mse: 117724004674640445440.000000, mean_q: 10357679387.569231, mean_eps: 0.401779
  99998/150000: episode: 863, duration: 2.202s, episode steps: 229, steps per second: 104, episode reward: -507.557, mean reward: -2.216 [-100.000,  3.901], mean action: 1.799 [0.000, 3.000],  loss: 365002754036490304.000000, mse: 119360950221820002304.000000, mean_q: 10409034720.698690, mean_eps: 0.400702
 100143/150000: episode: 864, duration: 1.398s, episode steps: 145, steps per second: 104, episode reward: -440.567, mean reward: -3.038 [-100.000,  4.685], mean action: 1.752 [0.000, 3.000],  loss: 470052880951424256.000000, mse: 119409592686961819648.000000, mean_q: 10439924750.124138, mean_eps: 0.399580
 100235/150000: episode: 865, duration: 0.876s, episode steps:  92, steps per second: 105, episode reward: -451.730, mean reward: -4.910 [-100.000,  2.548], mean action: 1.957 [0.000, 3.000],  loss: 564359158287724352.000000, mse: 120008792130321022976.000000, mean_q: 10506939698.086956, mean_eps: 0.398869
 100405/150000: episode: 866, duration: 1.645s, episode steps: 170, steps per second: 103, episode reward: -907.374, mean reward: -5.337 [-100.000,  1.597], mean action: 1.859 [0.000, 3.000],  loss: 369369441508524032.000000, mse: 120751080660541112320.000000, mean_q: 10482885086.870588, mean_eps: 0.398083
 100559/150000: episode: 867, duration: 1.359s, episode steps: 154, steps per second: 113, episode reward: -445.205, mean reward: -2.891 [-100.000,  5.180], mean action: 1.675 [0.000, 3.000],  loss: 447431832108443200.000000, mse: 122655371238837469184.000000, mean_q: 10593934455.688313, mean_eps: 0.397111
 100644/150000: episode: 868, duration: 0.875s, episode steps:  85, steps per second:  97, episode reward: -546.477, mean reward: -6.429 [-100.000,  0.146], mean action: 1.776 [0.000, 3.000],  loss: 313728036916987008.000000, mse: 121766428631452729344.000000, mean_q: 10608498495.247059, mean_eps: 0.396394
 100788/150000: episode: 869, duration: 1.345s, episode steps: 144, steps per second: 107, episode reward: -404.440, mean reward: -2.809 [-100.000,  5.217], mean action: 1.833 [0.000, 3.000],  loss: 381311913468071488.000000, mse: 122813523150131101696.000000, mean_q: 10521118702.222221, mean_eps: 0.395707
 100998/150000: episode: 870, duration: 1.833s, episode steps: 210, steps per second: 115, episode reward: -403.009, mean reward: -1.919 [-100.000,  2.793], mean action: 1.738 [0.000, 3.000],  loss: 411018685137304192.000000, mse: 120354723830455058432.000000, mean_q: 10483428790.857143, mean_eps: 0.394645
 101121/150000: episode: 871, duration: 1.184s, episode steps: 123, steps per second: 104, episode reward: -743.393, mean reward: -6.044 [-100.000,  0.788], mean action: 1.943 [0.000, 3.000],  loss: 421075896512755776.000000, mse: 120457530654259593216.000000, mean_q: 10418275061.593496, mean_eps: 0.393646
 101263/150000: episode: 872, duration: 1.534s, episode steps: 142, steps per second:  93, episode reward: -354.067, mean reward: -2.493 [-100.000,  4.214], mean action: 1.789 [0.000, 3.000],  loss: 387290126016898752.000000, mse: 124043358506863984640.000000, mean_q: 10628736591.323944, mean_eps: 0.392851
 101339/150000: episode: 873, duration: 0.676s, episode steps:  76, steps per second: 112, episode reward: -545.193, mean reward: -7.174 [-100.000, -0.766], mean action: 2.026 [0.000, 3.000],  loss: 601175401206755072.000000, mse: 122059422312559099904.000000, mean_q: 10562658681.263159, mean_eps: 0.392197
 101474/150000: episode: 874, duration: 1.244s, episode steps: 135, steps per second: 109, episode reward: -629.628, mean reward: -4.664 [-100.000,  2.562], mean action: 1.941 [0.000, 3.000],  loss: 638010365552565376.000000, mse: 123495121627280326656.000000, mean_q: 10631585598.577778, mean_eps: 0.391564
 101547/150000: episode: 875, duration: 0.722s, episode steps:  73, steps per second: 101, episode reward: -500.126, mean reward: -6.851 [-100.000, -0.893], mean action: 1.849 [0.000, 3.000],  loss: 271829136609877440.000000, mse: 121241389301693759488.000000, mean_q: 10528645568.876713, mean_eps: 0.390940
 101683/150000: episode: 876, duration: 1.310s, episode steps: 136, steps per second: 104, episode reward: -601.722, mean reward: -4.424 [-100.000,  2.919], mean action: 1.757 [0.000, 3.000],  loss: 635861442054062080.000000, mse: 125166853718010445824.000000, mean_q: 10758488022.588236, mean_eps: 0.390313
 101933/150000: episode: 877, duration: 2.275s, episode steps: 250, steps per second: 110, episode reward: -973.765, mean reward: -3.895 [-100.000,  2.026], mean action: 1.820 [0.000, 3.000],  loss: 665889232026882560.000000, mse: 119853247438944714752.000000, mean_q: 10488366575.615999, mean_eps: 0.389155
 102006/150000: episode: 878, duration: 0.642s, episode steps:  73, steps per second: 114, episode reward: -511.220, mean reward: -7.003 [-100.000, -0.580], mean action: 1.945 [0.000, 3.000],  loss: 496686345868879488.000000, mse: 121861038268264087552.000000, mean_q: 10568899408.657534, mean_eps: 0.388186
 102142/150000: episode: 879, duration: 1.563s, episode steps: 136, steps per second:  87, episode reward: -375.462, mean reward: -2.761 [-100.000,  4.690], mean action: 1.860 [0.000, 3.000],  loss: 554904705804117440.000000, mse: 121000357176551915520.000000, mean_q: 10469176726.588236, mean_eps: 0.387559
 102233/150000: episode: 880, duration: 0.963s, episode steps:  91, steps per second:  94, episode reward: -502.423, mean reward: -5.521 [-100.000,  0.757], mean action: 1.769 [0.000, 3.000],  loss: 511504931096325568.000000, mse: 125809652903091421184.000000, mean_q: 10691215236.219780, mean_eps: 0.386878
 102359/150000: episode: 881, duration: 1.281s, episode steps: 126, steps per second:  98, episode reward: -315.768, mean reward: -2.506 [-100.000,  1.624], mean action: 1.849 [0.000, 3.000],  loss: 475199377607965952.000000, mse: 129034036877972799488.000000, mean_q: 10878908724.825397, mean_eps: 0.386227
 102469/150000: episode: 882, duration: 1.033s, episode steps: 110, steps per second: 107, episode reward: -365.888, mean reward: -3.326 [-100.000,  1.881], mean action: 1.727 [0.000, 3.000],  loss: 671666812244840832.000000, mse: 130580738898542133248.000000, mean_q: 10915675145.309092, mean_eps: 0.385519
 102580/150000: episode: 883, duration: 1.032s, episode steps: 111, steps per second: 108, episode reward: -450.973, mean reward: -4.063 [-100.000,  0.735], mean action: 1.874 [0.000, 3.000],  loss: 357831927273836736.000000, mse: 125411703005061464064.000000, mean_q: 10717090207.135136, mean_eps: 0.384856
 102704/150000: episode: 884, duration: 1.054s, episode steps: 124, steps per second: 118, episode reward: -411.545, mean reward: -3.319 [-100.000,  1.936], mean action: 1.766 [0.000, 3.000],  loss: 327492379280763712.000000, mse: 126522416381891559424.000000, mean_q: 10799355668.645161, mean_eps: 0.384151
 102881/150000: episode: 885, duration: 1.541s, episode steps: 177, steps per second: 115, episode reward: -720.452, mean reward: -4.070 [-100.000,  2.142], mean action: 1.881 [0.000, 3.000],  loss: 397140541666024384.000000, mse: 127221889589371076608.000000, mean_q: 10794058083.796610, mean_eps: 0.383248
 102982/150000: episode: 886, duration: 0.859s, episode steps: 101, steps per second: 118, episode reward: -549.213, mean reward: -5.438 [-100.000,  1.219], mean action: 1.842 [0.000, 3.000],  loss: 682046150458391424.000000, mse: 133759546628639997952.000000, mean_q: 11016897606.970297, mean_eps: 0.382414
 103128/150000: episode: 887, duration: 1.273s, episode steps: 146, steps per second: 115, episode reward: -561.097, mean reward: -3.843 [-100.000,  3.463], mean action: 1.884 [0.000, 3.000],  loss: 456679700247260992.000000, mse: 126446318114202746880.000000, mean_q: 10750292497.534246, mean_eps: 0.381673
 103244/150000: episode: 888, duration: 0.993s, episode steps: 116, steps per second: 117, episode reward: -690.472, mean reward: -5.952 [-100.000,  1.462], mean action: 1.750 [0.000, 3.000],  loss: 400428911862815360.000000, mse: 127777295929962463232.000000, mean_q: 10924702415.448277, mean_eps: 0.380887
 103451/150000: episode: 889, duration: 1.814s, episode steps: 207, steps per second: 114, episode reward: -842.466, mean reward: -4.070 [-100.000,  2.069], mean action: 1.783 [0.000, 3.000],  loss: 607368842979122944.000000, mse: 128190327002214400000.000000, mean_q: 10898617299.478260, mean_eps: 0.379918
 103542/150000: episode: 890, duration: 0.828s, episode steps:  91, steps per second: 110, episode reward: -390.745, mean reward: -4.294 [-100.000,  0.642], mean action: 1.780 [0.000, 3.000],  loss: 456921060016917312.000000, mse: 130621168272816242688.000000, mean_q: 10941162850.461538, mean_eps: 0.379024
 103628/150000: episode: 891, duration: 0.742s, episode steps:  86, steps per second: 116, episode reward: -433.473, mean reward: -5.040 [-100.000,  2.379], mean action: 1.628 [0.000, 3.000],  loss: 331282819950660992.000000, mse: 130759057545563717632.000000, mean_q: 11041176206.883720, mean_eps: 0.378493
 103719/150000: episode: 892, duration: 0.781s, episode steps:  91, steps per second: 117, episode reward: -289.109, mean reward: -3.177 [-100.000,  0.866], mean action: 1.857 [0.000, 3.000],  loss: 482221283590764800.000000, mse: 133795282710326198272.000000, mean_q: 11128718240.351648, mean_eps: 0.377962
 103817/150000: episode: 893, duration: 0.896s, episode steps:  98, steps per second: 109, episode reward: -246.114, mean reward: -2.511 [-100.000,  2.107], mean action: 1.776 [0.000, 3.000],  loss: 671312716376374016.000000, mse: 127696730694090752000.000000, mean_q: 10986444653.714285, mean_eps: 0.377395
 103955/150000: episode: 894, duration: 1.165s, episode steps: 138, steps per second: 119, episode reward: -289.689, mean reward: -2.099 [-100.000,  1.614], mean action: 1.804 [0.000, 3.000],  loss: 227928718309159680.000000, mse: 133546350416510877696.000000, mean_q: 11124555827.942028, mean_eps: 0.376687
 104046/150000: episode: 895, duration: 0.802s, episode steps:  91, steps per second: 113, episode reward: -510.425, mean reward: -5.609 [-100.000,  0.880], mean action: 1.780 [0.000, 3.000],  loss: 390852743460509504.000000, mse: 129436673530617446400.000000, mean_q: 10980831760.879122, mean_eps: 0.376000
 104187/150000: episode: 896, duration: 1.209s, episode steps: 141, steps per second: 117, episode reward: -707.103, mean reward: -5.015 [-100.000,  1.864], mean action: 1.766 [0.000, 3.000],  loss: 416103834987358976.000000, mse: 134732513491759644672.000000, mean_q: 11183899364.765957, mean_eps: 0.375304
 104300/150000: episode: 897, duration: 0.988s, episode steps: 113, steps per second: 114, episode reward: -432.272, mean reward: -3.825 [-100.000,  2.902], mean action: 1.735 [0.000, 3.000],  loss: 636079960732028800.000000, mse: 131376235069421109248.000000, mean_q: 11078821534.584070, mean_eps: 0.374542
 104429/150000: episode: 898, duration: 1.096s, episode steps: 129, steps per second: 118, episode reward: -706.270, mean reward: -5.475 [-100.000,  1.788], mean action: 1.806 [0.000, 3.000],  loss: 243983548613896352.000000, mse: 134977821236354465792.000000, mean_q: 11197642247.937984, mean_eps: 0.373816
 104560/150000: episode: 899, duration: 1.142s, episode steps: 131, steps per second: 115, episode reward: -543.671, mean reward: -4.150 [-100.000,  1.838], mean action: 1.794 [0.000, 3.000],  loss: 484518052087231808.000000, mse: 135925793685057224704.000000, mean_q: 11148750597.862595, mean_eps: 0.373036
 104658/150000: episode: 900, duration: 0.831s, episode steps:  98, steps per second: 118, episode reward: -601.324, mean reward: -6.136 [-100.000,  1.204], mean action: 1.776 [0.000, 3.000],  loss: 514546596514493824.000000, mse: 136311117347619880960.000000, mean_q: 11337017427.591837, mean_eps: 0.372349
 104773/150000: episode: 901, duration: 0.997s, episode steps: 115, steps per second: 115, episode reward: -412.819, mean reward: -3.590 [-100.000,  1.420], mean action: 1.861 [0.000, 3.000],  loss: 438241967366341760.000000, mse: 134933447182397554688.000000, mean_q: 11266004609.113043, mean_eps: 0.371710
 104929/150000: episode: 902, duration: 1.323s, episode steps: 156, steps per second: 118, episode reward: -560.816, mean reward: -3.595 [-100.000,  3.105], mean action: 1.744 [0.000, 3.000],  loss: 502123584216393984.000000, mse: 139391353337410830336.000000, mean_q: 11374697360.410257, mean_eps: 0.370897
 105116/150000: episode: 903, duration: 1.600s, episode steps: 187, steps per second: 117, episode reward: -492.981, mean reward: -2.636 [-100.000,  4.703], mean action: 1.791 [0.000, 3.000],  loss: 686011054985579264.000000, mse: 135028624778278912000.000000, mean_q: 11275040592.770054, mean_eps: 0.369868
 105196/150000: episode: 904, duration: 0.684s, episode steps:  80, steps per second: 117, episode reward: -362.962, mean reward: -4.537 [-100.000,  0.480], mean action: 1.788 [0.000, 3.000],  loss: 751571537406355072.000000, mse: 140482556468921155584.000000, mean_q: 11296890368.000000, mean_eps: 0.369067
 105468/150000: episode: 905, duration: 2.377s, episode steps: 272, steps per second: 114, episode reward: -1117.035, mean reward: -4.107 [-100.000,  3.221], mean action: 1.860 [0.000, 3.000],  loss: 641240225154072576.000000, mse: 139880497171479330816.000000, mean_q: 11400601692.235294, mean_eps: 0.368011
 105704/150000: episode: 906, duration: 2.067s, episode steps: 236, steps per second: 114, episode reward: -943.242, mean reward: -3.997 [-100.000,  2.489], mean action: 1.792 [0.000, 3.000],  loss: 607061053645685248.000000, mse: 141516318956214337536.000000, mean_q: 11438374755.796610, mean_eps: 0.366487
 105800/150000: episode: 907, duration: 0.881s, episode steps:  96, steps per second: 109, episode reward: -576.025, mean reward: -6.000 [-100.000,  0.408], mean action: 1.729 [0.000, 3.000],  loss: 388411679071447744.000000, mse: 144893466478086799360.000000, mean_q: 11478102602.666666, mean_eps: 0.365491
 105885/150000: episode: 908, duration: 0.725s, episode steps:  85, steps per second: 117, episode reward: -413.930, mean reward: -4.870 [-100.000,  0.113], mean action: 1.718 [0.000, 3.000],  loss: 834258329745449856.000000, mse: 143347678873679724544.000000, mean_q: 11601958899.952942, mean_eps: 0.364948
 105972/150000: episode: 909, duration: 0.759s, episode steps:  87, steps per second: 115, episode reward: -518.188, mean reward: -5.956 [-100.000, -0.161], mean action: 1.862 [0.000, 3.000],  loss: 810349136192600576.000000, mse: 138493361060162027520.000000, mean_q: 11278170194.390804, mean_eps: 0.364432
 106343/150000: episode: 910, duration: 3.347s, episode steps: 371, steps per second: 111, episode reward: -1565.348, mean reward: -4.219 [-100.000,  2.444], mean action: 1.809 [0.000, 3.000],  loss: 577495379155525248.000000, mse: 145173326272463224832.000000, mean_q: 11542669127.072777, mean_eps: 0.363058
 106442/150000: episode: 911, duration: 0.836s, episode steps:  99, steps per second: 118, episode reward: -351.696, mean reward: -3.552 [-100.000,  2.141], mean action: 1.646 [0.000, 3.000],  loss: 666781856641599616.000000, mse: 150904127996948971520.000000, mean_q: 11763067779.878788, mean_eps: 0.361648
 106637/150000: episode: 912, duration: 1.688s, episode steps: 195, steps per second: 115, episode reward: -341.247, mean reward: -1.750 [-100.000,  4.879], mean action: 1.738 [0.000, 3.000],  loss: 1551111943892265216.000000, mse: 155574493533802430464.000000, mean_q: 11715008716.799999, mean_eps: 0.360766
 106836/150000: episode: 913, duration: 1.872s, episode steps: 199, steps per second: 106, episode reward: -853.630, mean reward: -4.290 [-100.000,  2.422], mean action: 1.754 [0.000, 3.000],  loss: 516184986559880448.000000, mse: 146548497744336846848.000000, mean_q: 11563330431.356785, mean_eps: 0.359584
 106917/150000: episode: 914, duration: 0.824s, episode steps:  81, steps per second:  98, episode reward: -432.723, mean reward: -5.342 [-100.000,  0.082], mean action: 1.840 [0.000, 3.000],  loss: 778310396622111104.000000, mse: 148963201766135660544.000000, mean_q: 11674260530.567902, mean_eps: 0.358744
 107117/150000: episode: 915, duration: 1.882s, episode steps: 200, steps per second: 106, episode reward: -827.479, mean reward: -4.137 [-100.000,  2.207], mean action: 1.825 [0.000, 3.000],  loss: 878619654686077056.000000, mse: 153142938170490224640.000000, mean_q: 11786750981.120001, mean_eps: 0.357901
 107293/150000: episode: 916, duration: 1.697s, episode steps: 176, steps per second: 104, episode reward: -926.212, mean reward: -5.263 [-100.000,  1.612], mean action: 1.756 [0.000, 3.000],  loss: 728344440009701888.000000, mse: 155220185297582686208.000000, mean_q: 11799454551.272728, mean_eps: 0.356773
 107440/150000: episode: 917, duration: 1.324s, episode steps: 147, steps per second: 111, episode reward: -426.627, mean reward: -2.902 [-100.000,  4.550], mean action: 1.830 [0.000, 3.000],  loss: 445619192163238976.000000, mse: 154659791985103798272.000000, mean_q: 11844025030.530613, mean_eps: 0.355804
 107531/150000: episode: 918, duration: 0.782s, episode steps:  91, steps per second: 116, episode reward: -504.178, mean reward: -5.540 [-100.000,  0.196], mean action: 1.835 [0.000, 3.000],  loss: 517141455273739136.000000, mse: 145654360910763753472.000000, mean_q: 11492646113.054945, mean_eps: 0.355090
 107745/150000: episode: 919, duration: 1.866s, episode steps: 214, steps per second: 115, episode reward: -616.778, mean reward: -2.882 [-100.000,  3.094], mean action: 1.827 [0.000, 3.000],  loss: 554184767761476224.000000, mse: 152626303385581420544.000000, mean_q: 11780295852.261683, mean_eps: 0.354175
 107845/150000: episode: 920, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: -414.068, mean reward: -4.141 [-100.000,  0.978], mean action: 1.910 [0.000, 3.000],  loss: 308167952348301248.000000, mse: 150945089721122291712.000000, mean_q: 11817419356.160000, mean_eps: 0.353233
 107971/150000: episode: 921, duration: 1.098s, episode steps: 126, steps per second: 115, episode reward: -551.995, mean reward: -4.381 [-100.000,  1.690], mean action: 1.833 [0.000, 3.000],  loss: 748205976922166400.000000, mse: 155251298255063810048.000000, mean_q: 11875470149.079365, mean_eps: 0.352555
 108088/150000: episode: 922, duration: 1.020s, episode steps: 117, steps per second: 115, episode reward: -586.553, mean reward: -5.013 [-100.000,  1.507], mean action: 1.889 [0.000, 3.000],  loss: 660370138925100672.000000, mse: 153791130375745601536.000000, mean_q: 11923179756.307692, mean_eps: 0.351826
 108171/150000: episode: 923, duration: 0.749s, episode steps:  83, steps per second: 111, episode reward: -443.132, mean reward: -5.339 [-100.000,  0.248], mean action: 1.771 [0.000, 3.000],  loss: 835768443307610496.000000, mse: 156738436267425857536.000000, mean_q: 11985718395.373493, mean_eps: 0.351226
 108289/150000: episode: 924, duration: 1.034s, episode steps: 118, steps per second: 114, episode reward: -352.861, mean reward: -2.990 [-100.000,  1.233], mean action: 1.839 [0.000, 3.000],  loss: 1031723042056095616.000000, mse: 154839957403639414784.000000, mean_q: 11917095050.847458, mean_eps: 0.350623
 108407/150000: episode: 925, duration: 1.032s, episode steps: 118, steps per second: 114, episode reward: -446.537, mean reward: -3.784 [-100.000,  4.768], mean action: 1.831 [0.000, 3.000],  loss: 718633334869954304.000000, mse: 157738905013104082944.000000, mean_q: 12003428438.779661, mean_eps: 0.349915
 108550/150000: episode: 926, duration: 1.224s, episode steps: 143, steps per second: 117, episode reward: -569.983, mean reward: -3.986 [-100.000,  3.759], mean action: 1.944 [0.000, 3.000],  loss: 401477726136211072.000000, mse: 155514310874482376704.000000, mean_q: 11937646763.860140, mean_eps: 0.349132
 108796/150000: episode: 927, duration: 2.180s, episode steps: 246, steps per second: 113, episode reward: -796.721, mean reward: -3.239 [-100.000,  2.553], mean action: 1.862 [0.000, 3.000],  loss: 381505067659462976.000000, mse: 157194551467559321600.000000, mean_q: 12039238031.609756, mean_eps: 0.347965
 108886/150000: episode: 928, duration: 0.858s, episode steps:  90, steps per second: 105, episode reward: -364.364, mean reward: -4.048 [-100.000,  1.182], mean action: 1.722 [0.000, 3.000],  loss: 1095769475265039872.000000, mse: 160632215551105892352.000000, mean_q: 12006091878.400000, mean_eps: 0.346957
 109112/150000: episode: 929, duration: 2.365s, episode steps: 226, steps per second:  96, episode reward: -1365.743, mean reward: -6.043 [-100.000,  1.309], mean action: 1.885 [0.000, 3.000],  loss: 564974722565523904.000000, mse: 160373519512047976448.000000, mean_q: 12182059846.230089, mean_eps: 0.346009
 109186/150000: episode: 930, duration: 0.747s, episode steps:  74, steps per second:  99, episode reward: -500.002, mean reward: -6.757 [-100.000,  0.303], mean action: 1.676 [0.000, 3.000],  loss: 1113049349812961664.000000, mse: 163299101206234365952.000000, mean_q: 12132487624.648649, mean_eps: 0.345109
 109275/150000: episode: 931, duration: 0.946s, episode steps:  89, steps per second:  94, episode reward: -333.889, mean reward: -3.752 [-100.000,  2.803], mean action: 1.888 [0.000, 3.000],  loss: 1576802098674504448.000000, mse: 163220648330359701504.000000, mean_q: 12176875531.505617, mean_eps: 0.344620
 109386/150000: episode: 932, duration: 1.267s, episode steps: 111, steps per second:  88, episode reward: -644.598, mean reward: -5.807 [-100.000,  3.081], mean action: 1.694 [0.000, 3.000],  loss: 680038891857735936.000000, mse: 159970974324499677184.000000, mean_q: 12124923553.441441, mean_eps: 0.344020
 109604/150000: episode: 933, duration: 2.322s, episode steps: 218, steps per second:  94, episode reward: -801.012, mean reward: -3.674 [-100.000,  2.064], mean action: 1.858 [0.000, 3.000],  loss: 1051512918676081024.000000, mse: 165415928066284552192.000000, mean_q: 12239288935.339449, mean_eps: 0.343033
 109700/150000: episode: 934, duration: 1.025s, episode steps:  96, steps per second:  94, episode reward: -547.422, mean reward: -5.702 [-100.000,  0.929], mean action: 1.781 [0.000, 3.000],  loss: 501382158257662656.000000, mse: 158920925232708452352.000000, mean_q: 12222788608.000000, mean_eps: 0.342091
 109792/150000: episode: 935, duration: 1.000s, episode steps:  92, steps per second:  92, episode reward: -479.171, mean reward: -5.208 [-100.000,  0.723], mean action: 1.717 [0.000, 3.000],  loss: 460354505419180096.000000, mse: 163338478107893530624.000000, mean_q: 12281290974.608696, mean_eps: 0.341527
 109919/150000: episode: 936, duration: 1.344s, episode steps: 127, steps per second:  95, episode reward: -459.932, mean reward: -3.622 [-100.000,  3.386], mean action: 1.858 [0.000, 3.000],  loss: 494424060000830208.000000, mse: 164971070380301320192.000000, mean_q: 12204863770.204725, mean_eps: 0.340870
 109994/150000: episode: 937, duration: 0.702s, episode steps:  75, steps per second: 107, episode reward: -401.268, mean reward: -5.350 [-100.000,  0.202], mean action: 1.800 [0.000, 3.000],  loss: 830780428459802240.000000, mse: 166313599406869839872.000000, mean_q: 12320264628.906666, mean_eps: 0.340264
 110152/150000: episode: 938, duration: 1.500s, episode steps: 158, steps per second: 105, episode reward: -493.885, mean reward: -3.126 [-100.000,  4.951], mean action: 1.899 [0.000, 3.000],  loss: 623780460347808384.000000, mse: 157791345545763979264.000000, mean_q: 12097483925.063292, mean_eps: 0.339565
 110290/150000: episode: 939, duration: 1.268s, episode steps: 138, steps per second: 109, episode reward: -323.205, mean reward: -2.342 [-100.000,  4.011], mean action: 1.913 [0.000, 3.000],  loss: 717877859629001088.000000, mse: 163573042532549787648.000000, mean_q: 12283315860.405798, mean_eps: 0.338677
 110378/150000: episode: 940, duration: 0.765s, episode steps:  88, steps per second: 115, episode reward: -548.008, mean reward: -6.227 [-100.000,  0.830], mean action: 1.932 [0.000, 3.000],  loss: 589760309954370816.000000, mse: 165015740136886239232.000000, mean_q: 12293382085.818182, mean_eps: 0.337999
 110494/150000: episode: 941, duration: 1.025s, episode steps: 116, steps per second: 113, episode reward: -566.283, mean reward: -4.882 [-100.000,  1.605], mean action: 1.914 [0.000, 3.000],  loss: 730412274267681536.000000, mse: 158384929636228202496.000000, mean_q: 12091058096.551723, mean_eps: 0.337387
 110603/150000: episode: 942, duration: 0.926s, episode steps: 109, steps per second: 118, episode reward: -550.637, mean reward: -5.052 [-100.000,  3.417], mean action: 1.872 [0.000, 3.000],  loss: 520387655560064320.000000, mse: 165654456804748132352.000000, mean_q: 12212545442.055046, mean_eps: 0.336712
 110705/150000: episode: 943, duration: 0.890s, episode steps: 102, steps per second: 115, episode reward: -434.942, mean reward: -4.264 [-100.000,  1.159], mean action: 1.853 [0.000, 3.000],  loss: 1098941864225404416.000000, mse: 165244874618639351808.000000, mean_q: 12235716678.274509, mean_eps: 0.336079
 110785/150000: episode: 944, duration: 0.687s, episode steps:  80, steps per second: 116, episode reward: -450.237, mean reward: -5.628 [-100.000,  1.141], mean action: 2.000 [0.000, 3.000],  loss: 998178907080150272.000000, mse: 162551367162236436480.000000, mean_q: 12144670502.400000, mean_eps: 0.335533
 110890/150000: episode: 945, duration: 0.925s, episode steps: 105, steps per second: 114, episode reward: -467.601, mean reward: -4.453 [-100.000,  3.663], mean action: 1.771 [0.000, 3.000],  loss: 724994603050413696.000000, mse: 167299150729949937664.000000, mean_q: 12384842664.228571, mean_eps: 0.334978
 111108/150000: episode: 946, duration: 1.906s, episode steps: 218, steps per second: 114, episode reward: -958.115, mean reward: -4.395 [-100.000,  2.143], mean action: 1.757 [0.000, 3.000],  loss: 1160796313429227520.000000, mse: 164955597490012520448.000000, mean_q: 12271893006.091743, mean_eps: 0.334009
 111240/150000: episode: 947, duration: 1.148s, episode steps: 132, steps per second: 115, episode reward: -549.104, mean reward: -4.160 [-100.000,  2.821], mean action: 1.811 [0.000, 3.000],  loss: 633613370648206080.000000, mse: 164696957431222304768.000000, mean_q: 12272811931.151516, mean_eps: 0.332959
 111423/150000: episode: 948, duration: 1.581s, episode steps: 183, steps per second: 116, episode reward: -621.691, mean reward: -3.397 [-100.000,  2.646], mean action: 1.814 [0.000, 3.000],  loss: 750435398648498432.000000, mse: 172335141376185171968.000000, mean_q: 12378439344.262295, mean_eps: 0.332014
 111501/150000: episode: 949, duration: 0.683s, episode steps:  78, steps per second: 114, episode reward: -544.323, mean reward: -6.978 [-100.000, -0.716], mean action: 1.821 [0.000, 3.000],  loss: 369376196810667584.000000, mse: 164744170316993003520.000000, mean_q: 12278456556.307692, mean_eps: 0.331231
 111588/150000: episode: 950, duration: 0.752s, episode steps:  87, steps per second: 116, episode reward: -323.058, mean reward: -3.713 [-100.000,  2.327], mean action: 1.851 [0.000, 3.000],  loss: 441970829066375616.000000, mse: 172942156937980870656.000000, mean_q: 12514344606.896551, mean_eps: 0.330736
 111688/150000: episode: 951, duration: 0.875s, episode steps: 100, steps per second: 114, episode reward: -389.632, mean reward: -3.896 [-100.000,  1.477], mean action: 1.870 [0.000, 3.000],  loss: 778024786003031552.000000, mse: 171045355554846932992.000000, mean_q: 12499642961.920000, mean_eps: 0.330175
 111810/150000: episode: 952, duration: 1.032s, episode steps: 122, steps per second: 118, episode reward: -528.905, mean reward: -4.335 [-100.000,  1.180], mean action: 1.803 [0.000, 3.000],  loss: 783917703451124096.000000, mse: 168143586849865826304.000000, mean_q: 12475323005.901640, mean_eps: 0.329509
 111897/150000: episode: 953, duration: 0.769s, episode steps:  87, steps per second: 113, episode reward: -511.234, mean reward: -5.876 [-100.000,  0.868], mean action: 1.839 [0.000, 3.000],  loss: 1111532661602046976.000000, mse: 168963059440327098368.000000, mean_q: 12440406981.149426, mean_eps: 0.328882
 112029/150000: episode: 954, duration: 1.128s, episode steps: 132, steps per second: 117, episode reward: -570.668, mean reward: -4.323 [-100.000,  1.903], mean action: 1.826 [0.000, 3.000],  loss: 596540049772264960.000000, mse: 168179314799092170752.000000, mean_q: 12449918712.242424, mean_eps: 0.328225
 112227/150000: episode: 955, duration: 1.724s, episode steps: 198, steps per second: 115, episode reward: -819.111, mean reward: -4.137 [-100.000,  2.182], mean action: 1.803 [0.000, 3.000],  loss: 1169286533292297472.000000, mse: 172394564014738505728.000000, mean_q: 12531816608.323233, mean_eps: 0.327235
 112324/150000: episode: 956, duration: 0.862s, episode steps:  97, steps per second: 113, episode reward: -491.004, mean reward: -5.062 [-100.000,  1.176], mean action: 1.804 [0.000, 3.000],  loss: 833530305545232512.000000, mse: 178181802628861296640.000000, mean_q: 12750300666.721649, mean_eps: 0.326350
 112459/150000: episode: 957, duration: 1.179s, episode steps: 135, steps per second: 114, episode reward: -506.569, mean reward: -3.752 [-100.000,  4.230], mean action: 1.770 [0.000, 3.000],  loss: 748780080071090944.000000, mse: 173801810336136232960.000000, mean_q: 12569768724.859259, mean_eps: 0.325654
 112650/150000: episode: 958, duration: 1.667s, episode steps: 191, steps per second: 115, episode reward: -843.577, mean reward: -4.417 [-100.000,  1.879], mean action: 1.843 [0.000, 3.000],  loss: 565864090612101632.000000, mse: 169540041527752785920.000000, mean_q: 12603938998.282722, mean_eps: 0.324676
 112828/150000: episode: 959, duration: 1.528s, episode steps: 178, steps per second: 116, episode reward: -644.868, mean reward: -3.623 [-100.000,  2.149], mean action: 1.860 [0.000, 3.000],  loss: 585391019313732864.000000, mse: 174838614373702172672.000000, mean_q: 12677832209.258427, mean_eps: 0.323569
 112941/150000: episode: 960, duration: 0.998s, episode steps: 113, steps per second: 113, episode reward: -555.128, mean reward: -4.913 [-100.000,  3.693], mean action: 1.947 [0.000, 3.000],  loss: 781867428038175104.000000, mse: 174685596814424571904.000000, mean_q: 12707685312.566372, mean_eps: 0.322696
 113027/150000: episode: 961, duration: 0.736s, episode steps:  86, steps per second: 117, episode reward: -420.862, mean reward: -4.894 [-100.000,  2.193], mean action: 1.756 [0.000, 3.000],  loss: 1321859718893334016.000000, mse: 175789547771877228544.000000, mean_q: 12661648753.116280, mean_eps: 0.322099
 113127/150000: episode: 962, duration: 0.880s, episode steps: 100, steps per second: 114, episode reward: -225.849, mean reward: -2.258 [-100.000,  3.022], mean action: 1.910 [0.000, 3.000],  loss: 1082055772868035200.000000, mse: 174792379823769911296.000000, mean_q: 12737179863.040001, mean_eps: 0.321541
 113235/150000: episode: 963, duration: 0.939s, episode steps: 108, steps per second: 115, episode reward: -298.122, mean reward: -2.760 [-100.000,  2.891], mean action: 1.704 [0.000, 3.000],  loss: 1639347134176554240.000000, mse: 170713347134489329664.000000, mean_q: 12544967964.444445, mean_eps: 0.320917
 113350/150000: episode: 964, duration: 0.979s, episode steps: 115, steps per second: 117, episode reward: -339.701, mean reward: -2.954 [-100.000,  3.948], mean action: 1.861 [0.000, 3.000],  loss: 853371314843965696.000000, mse: 175102633337513803776.000000, mean_q: 12738022533.565218, mean_eps: 0.320248
 113439/150000: episode: 965, duration: 0.794s, episode steps:  89, steps per second: 112, episode reward: -556.628, mean reward: -6.254 [-100.000,  0.780], mean action: 1.888 [0.000, 3.000],  loss: 438353471022943296.000000, mse: 176097144241946263552.000000, mean_q: 12727954282.426966, mean_eps: 0.319636
 113561/150000: episode: 966, duration: 1.157s, episode steps: 122, steps per second: 105, episode reward: -473.024, mean reward: -3.877 [-100.000,  3.742], mean action: 1.934 [0.000, 3.000],  loss: 722084804371909248.000000, mse: 175612876407513612288.000000, mean_q: 12821220226.098360, mean_eps: 0.319003
 113683/150000: episode: 967, duration: 1.117s, episode steps: 122, steps per second: 109, episode reward: -591.354, mean reward: -4.847 [-100.000,  2.324], mean action: 1.893 [0.000, 3.000],  loss: 756168991081973632.000000, mse: 170538524403814629376.000000, mean_q: 12587994758.295082, mean_eps: 0.318271
 113893/150000: episode: 968, duration: 2.052s, episode steps: 210, steps per second: 102, episode reward: -925.828, mean reward: -4.409 [-100.000,  1.679], mean action: 1.924 [0.000, 3.000],  loss: 758002644120984064.000000, mse: 175699295583133138944.000000, mean_q: 12756355320.685715, mean_eps: 0.317275
 114008/150000: episode: 969, duration: 1.113s, episode steps: 115, steps per second: 103, episode reward: -552.978, mean reward: -4.809 [-100.000,  1.575], mean action: 1.817 [0.000, 3.000],  loss: 1305294992612079616.000000, mse: 174767098032015245312.000000, mean_q: 12647514592.834784, mean_eps: 0.316300
 114088/150000: episode: 970, duration: 0.769s, episode steps:  80, steps per second: 104, episode reward: -466.942, mean reward: -5.837 [-100.000,  0.465], mean action: 1.913 [0.000, 3.000],  loss: 654562411003720448.000000, mse: 173107392042078994432.000000, mean_q: 12736706764.799999, mean_eps: 0.315715
 114167/150000: episode: 971, duration: 0.844s, episode steps:  79, steps per second:  94, episode reward: -377.153, mean reward: -4.774 [-100.000,  1.557], mean action: 1.911 [0.000, 3.000],  loss: 552189723699361152.000000, mse: 179673011766442622976.000000, mean_q: 12819823084.556963, mean_eps: 0.315238
 114348/150000: episode: 972, duration: 1.944s, episode steps: 181, steps per second:  93, episode reward: -685.805, mean reward: -3.789 [-100.000,  4.234], mean action: 1.812 [0.000, 3.000],  loss: 1370924093676148224.000000, mse: 178243317684820901888.000000, mean_q: 12787641615.558010, mean_eps: 0.314458
 114428/150000: episode: 973, duration: 0.717s, episode steps:  80, steps per second: 112, episode reward: -374.580, mean reward: -4.682 [-100.000,  0.564], mean action: 1.837 [0.000, 3.000],  loss: 667294708495967872.000000, mse: 173275452174482210816.000000, mean_q: 12704450342.400000, mean_eps: 0.313675
 114533/150000: episode: 974, duration: 1.170s, episode steps: 105, steps per second:  90, episode reward: -428.660, mean reward: -4.082 [-100.000,  1.103], mean action: 1.924 [0.000, 3.000],  loss: 1510169200877965568.000000, mse: 176563404494002520064.000000, mean_q: 12737230623.695238, mean_eps: 0.313120
 114662/150000: episode: 975, duration: 1.186s, episode steps: 129, steps per second: 109, episode reward: -448.007, mean reward: -3.473 [-100.000,  1.256], mean action: 1.868 [0.000, 3.000],  loss: 857228376287997952.000000, mse: 175357031740730605568.000000, mean_q: 12785542636.155039, mean_eps: 0.312418
 114807/150000: episode: 976, duration: 1.299s, episode steps: 145, steps per second: 112, episode reward: -564.174, mean reward: -3.891 [-100.000,  1.981], mean action: 1.793 [0.000, 3.000],  loss: 1083577851459147008.000000, mse: 171193667504494215168.000000, mean_q: 12591706203.806896, mean_eps: 0.311596
 114943/150000: episode: 977, duration: 1.228s, episode steps: 136, steps per second: 111, episode reward: -565.222, mean reward: -4.156 [-100.000,  3.312], mean action: 1.912 [0.000, 3.000],  loss: 753471164791251456.000000, mse: 172661518714151665664.000000, mean_q: 12750088598.588236, mean_eps: 0.310753
 115025/150000: episode: 978, duration: 0.742s, episode steps:  82, steps per second: 111, episode reward: -316.712, mean reward: -3.862 [-100.000,  1.971], mean action: 1.780 [0.000, 3.000],  loss: 845599071523953280.000000, mse: 174859872734677893120.000000, mean_q: 12737587000.195122, mean_eps: 0.310099
 115178/150000: episode: 979, duration: 1.320s, episode steps: 153, steps per second: 116, episode reward: -819.651, mean reward: -5.357 [-100.000,  1.026], mean action: 1.810 [0.000, 3.000],  loss: 1131126035189399168.000000, mse: 183671129745905385472.000000, mean_q: 13033256190.326797, mean_eps: 0.309394
 115379/150000: episode: 980, duration: 1.767s, episode steps: 201, steps per second: 114, episode reward: -1245.891, mean reward: -6.198 [-100.000,  1.234], mean action: 1.871 [0.000, 3.000],  loss: 859669720102444160.000000, mse: 178497664388079943680.000000, mean_q: 12883715010.865671, mean_eps: 0.308332
 115610/150000: episode: 981, duration: 2.051s, episode steps: 231, steps per second: 113, episode reward: -952.496, mean reward: -4.123 [-100.000,  1.708], mean action: 1.879 [0.000, 3.000],  loss: 816361620671399424.000000, mse: 180339757071732375552.000000, mean_q: 12822616148.225109, mean_eps: 0.307036
 115875/150000: episode: 982, duration: 2.338s, episode steps: 265, steps per second: 113, episode reward: -1337.355, mean reward: -5.047 [-100.000,  1.723], mean action: 1.834 [0.000, 3.000],  loss: 1829803407939768064.000000, mse: 184353698259068518400.000000, mean_q: 12989376260.830189, mean_eps: 0.305548
 115995/150000: episode: 983, duration: 1.053s, episode steps: 120, steps per second: 114, episode reward: -627.309, mean reward: -5.228 [-100.000,  1.271], mean action: 1.842 [0.000, 3.000],  loss: 637114677084343552.000000, mse: 186875422145963720704.000000, mean_q: 13115002555.733334, mean_eps: 0.304393
 116073/150000: episode: 984, duration: 0.676s, episode steps:  78, steps per second: 115, episode reward: -331.162, mean reward: -4.246 [-100.000,  2.671], mean action: 1.846 [0.000, 3.000],  loss: 982137859278695552.000000, mse: 181882151378317017088.000000, mean_q: 12958998580.512821, mean_eps: 0.303799
 116183/150000: episode: 985, duration: 0.997s, episode steps: 110, steps per second: 110, episode reward: -414.604, mean reward: -3.769 [-100.000,  1.423], mean action: 1.927 [0.000, 3.000],  loss: 1176371948821447168.000000, mse: 183259104566129098752.000000, mean_q: 12948712997.236364, mean_eps: 0.303235
 116287/150000: episode: 986, duration: 0.899s, episode steps: 104, steps per second: 116, episode reward: -368.037, mean reward: -3.539 [-100.000,  2.260], mean action: 1.702 [0.000, 3.000],  loss: 774875922663896576.000000, mse: 181181440117168537600.000000, mean_q: 12871403460.923077, mean_eps: 0.302593
 116426/150000: episode: 987, duration: 1.254s, episode steps: 139, steps per second: 111, episode reward: -639.753, mean reward: -4.603 [-100.000,  2.405], mean action: 1.856 [0.000, 3.000],  loss: 1070901475891782144.000000, mse: 191090095680540835840.000000, mean_q: 13320234160.805756, mean_eps: 0.301864
 116512/150000: episode: 988, duration: 0.876s, episode steps:  86, steps per second:  98, episode reward: -365.946, mean reward: -4.255 [-100.000,  2.434], mean action: 1.802 [0.000, 3.000],  loss: 935060363595005568.000000, mse: 191664384307796869120.000000, mean_q: 13262927467.162790, mean_eps: 0.301189
 116949/150000: episode: 989, duration: 4.520s, episode steps: 437, steps per second:  97, episode reward: -3186.795, mean reward: -7.292 [-100.000,  1.802], mean action: 1.863 [0.000, 3.000],  loss: 803498506792589056.000000, mse: 191488089603901784064.000000, mean_q: 13178994516.942791, mean_eps: 0.299620
 117092/150000: episode: 990, duration: 1.302s, episode steps: 143, steps per second: 110, episode reward: -716.827, mean reward: -5.013 [-100.000,  1.922], mean action: 1.748 [0.000, 3.000],  loss: 1150101618239062784.000000, mse: 227255925815308255232.000000, mean_q: 13508230086.713287, mean_eps: 0.297880
 117178/150000: episode: 991, duration: 0.747s, episode steps:  86, steps per second: 115, episode reward: -485.076, mean reward: -5.640 [-100.000,  1.000], mean action: 1.663 [0.000, 3.000],  loss: 1064469302291543424.000000, mse: 226154812980317814784.000000, mean_q: 13639408913.860466, mean_eps: 0.297193
 117412/150000: episode: 992, duration: 2.147s, episode steps: 234, steps per second: 109, episode reward: -768.649, mean reward: -3.285 [-100.000,  2.847], mean action: 1.880 [0.000, 3.000],  loss: 1288590201245558016.000000, mse: 214443976264097464320.000000, mean_q: 13431898895.316238, mean_eps: 0.296233
 117503/150000: episode: 993, duration: 0.846s, episode steps:  91, steps per second: 108, episode reward: -455.548, mean reward: -5.006 [-100.000,  1.024], mean action: 1.813 [0.000, 3.000],  loss: 1235077060454722048.000000, mse: 216579791160353259520.000000, mean_q: 13410809518.417582, mean_eps: 0.295258
 117615/150000: episode: 994, duration: 1.046s, episode steps: 112, steps per second: 107, episode reward: -506.603, mean reward: -4.523 [-100.000,  1.235], mean action: 1.804 [0.000, 3.000],  loss: 632541466363241600.000000, mse: 219199139019506352128.000000, mean_q: 13393345792.000000, mean_eps: 0.294649
 117760/150000: episode: 995, duration: 1.341s, episode steps: 145, steps per second: 108, episode reward: -681.970, mean reward: -4.703 [-100.000,  2.425], mean action: 1.855 [0.000, 3.000],  loss: 851424565631906304.000000, mse: 237794169981147578368.000000, mean_q: 13661003931.365517, mean_eps: 0.293878
 117862/150000: episode: 996, duration: 0.907s, episode steps: 102, steps per second: 112, episode reward: -477.223, mean reward: -4.679 [-100.000,  3.828], mean action: 1.853 [0.000, 3.000],  loss: 1157455766511033856.000000, mse: 221028354212698324992.000000, mean_q: 13515968080.313726, mean_eps: 0.293137
 117939/150000: episode: 997, duration: 0.656s, episode steps:  77, steps per second: 117, episode reward: -504.583, mean reward: -6.553 [-100.000, -0.063], mean action: 1.805 [0.000, 3.000],  loss: 629192033247026944.000000, mse: 213484651077617156096.000000, mean_q: 13435981996.883118, mean_eps: 0.292600
 118224/150000: episode: 998, duration: 2.502s, episode steps: 285, steps per second: 114, episode reward: -1384.513, mean reward: -4.858 [-100.000,  2.710], mean action: 1.870 [0.000, 3.000],  loss: 1425360673443838976.000000, mse: 231706716772924620800.000000, mean_q: 13600260660.098246, mean_eps: 0.291514
 118378/150000: episode: 999, duration: 1.328s, episode steps: 154, steps per second: 116, episode reward: -956.107, mean reward: -6.208 [-100.000,  1.796], mean action: 1.903 [0.000, 3.000],  loss: 938338446324432512.000000, mse: 226223167271750565888.000000, mean_q: 13645992507.844156, mean_eps: 0.290197
 118472/150000: episode: 1000, duration: 0.845s, episode steps:  94, steps per second: 111, episode reward: -417.282, mean reward: -4.439 [-100.000,  0.883], mean action: 1.830 [0.000, 3.000],  loss: 710488019047725312.000000, mse: 238247966399957729280.000000, mean_q: 13823588733.276596, mean_eps: 0.289453
 118648/150000: episode: 1001, duration: 2.122s, episode steps: 176, steps per second:  83, episode reward: -712.630, mean reward: -4.049 [-100.000,  4.272], mean action: 1.875 [0.000, 3.000],  loss: 993852593418630272.000000, mse: 245964024779429511168.000000, mean_q: 13878983360.000000, mean_eps: 0.288643
 118737/150000: episode: 1002, duration: 0.942s, episode steps:  89, steps per second:  95, episode reward: -491.746, mean reward: -5.525 [-100.000,  0.893], mean action: 1.888 [0.000, 3.000],  loss: 1086165387157609344.000000, mse: 228342422902948200448.000000, mean_q: 13733008844.224720, mean_eps: 0.287848
 118862/150000: episode: 1003, duration: 1.497s, episode steps: 125, steps per second:  84, episode reward: -629.642, mean reward: -5.037 [-100.000,  1.465], mean action: 1.840 [0.000, 3.000],  loss: 1182104635690318848.000000, mse: 234600270976917798912.000000, mean_q: 13850388119.552000, mean_eps: 0.287206
 118987/150000: episode: 1004, duration: 1.111s, episode steps: 125, steps per second: 113, episode reward: -592.288, mean reward: -4.738 [-100.000,  1.324], mean action: 1.888 [0.000, 3.000],  loss: 1234475706115770368.000000, mse: 235818898100131889152.000000, mean_q: 13909346803.712000, mean_eps: 0.286456
 119154/150000: episode: 1005, duration: 1.932s, episode steps: 167, steps per second:  86, episode reward: -610.127, mean reward: -3.653 [-100.000,  2.039], mean action: 1.814 [0.000, 3.000],  loss: 1109692964480406400.000000, mse: 227525950573969342464.000000, mean_q: 13617509210.443113, mean_eps: 0.285580
 119253/150000: episode: 1006, duration: 1.019s, episode steps:  99, steps per second:  97, episode reward: -524.513, mean reward: -5.298 [-100.000,  0.838], mean action: 1.848 [0.000, 3.000],  loss: 1486879172544377088.000000, mse: 236362465533835902976.000000, mean_q: 13801381339.797979, mean_eps: 0.284782
 119630/150000: episode: 1007, duration: 3.470s, episode steps: 377, steps per second: 109, episode reward: -2882.805, mean reward: -7.647 [-100.000,  1.723], mean action: 1.881 [0.000, 3.000],  loss: 764202949951939584.000000, mse: 230359955682229125120.000000, mean_q: 13802448241.400530, mean_eps: 0.283354
 119724/150000: episode: 1008, duration: 0.803s, episode steps:  94, steps per second: 117, episode reward: -520.219, mean reward: -5.534 [-100.000,  1.324], mean action: 1.745 [0.000, 3.000],  loss: 1017169716139981440.000000, mse: 253366141143564025856.000000, mean_q: 14044868302.978724, mean_eps: 0.281941
 119893/150000: episode: 1009, duration: 1.507s, episode steps: 169, steps per second: 112, episode reward: -1024.037, mean reward: -6.059 [-100.000,  2.067], mean action: 1.882 [0.000, 3.000],  loss: 996198339915099904.000000, mse: 279484667098287079424.000000, mean_q: 14277810303.242603, mean_eps: 0.281152
 119978/150000: episode: 1010, duration: 0.730s, episode steps:  85, steps per second: 116, episode reward: -277.639, mean reward: -3.266 [-100.000,  2.928], mean action: 1.871 [0.000, 3.000],  loss: 1628732605004473088.000000, mse: 254876079853370343424.000000, mean_q: 14274727707.105883, mean_eps: 0.280390
 120071/150000: episode: 1011, duration: 0.851s, episode steps:  93, steps per second: 109, episode reward: -470.245, mean reward: -5.056 [-100.000,  1.908], mean action: 1.806 [0.000, 3.000],  loss: 793033980247793408.000000, mse: 276758057167341912064.000000, mean_q: 14407581409.720430, mean_eps: 0.279856
 120252/150000: episode: 1012, duration: 2.161s, episode steps: 181, steps per second:  84, episode reward: -565.767, mean reward: -3.126 [-100.000,  4.288], mean action: 1.845 [0.000, 3.000],  loss: 1045707921934600192.000000, mse: 266362433927980351488.000000, mean_q: 14249773287.955801, mean_eps: 0.279034
 120366/150000: episode: 1013, duration: 1.379s, episode steps: 114, steps per second:  83, episode reward: -602.341, mean reward: -5.284 [-100.000,  1.683], mean action: 1.991 [0.000, 3.000],  loss: 1552681680678613760.000000, mse: 265046540469992030208.000000, mean_q: 14276211532.350878, mean_eps: 0.278149
 120460/150000: episode: 1014, duration: 1.027s, episode steps:  94, steps per second:  92, episode reward: -409.690, mean reward: -4.358 [-100.000,  3.440], mean action: 1.840 [0.000, 3.000],  loss: 1297179360844528640.000000, mse: 240661538061253574656.000000, mean_q: 13975129371.234043, mean_eps: 0.277525
 120550/150000: episode: 1015, duration: 0.907s, episode steps:  90, steps per second:  99, episode reward: -352.156, mean reward: -3.913 [-100.000,  3.097], mean action: 1.900 [0.000, 3.000],  loss: 1253184139213930496.000000, mse: 249655008950014443520.000000, mean_q: 13954076740.266666, mean_eps: 0.276973
 120707/150000: episode: 1016, duration: 1.508s, episode steps: 157, steps per second: 104, episode reward: -544.287, mean reward: -3.467 [-100.000,  1.573], mean action: 1.841 [0.000, 3.000],  loss: 1549611313501308672.000000, mse: 260981268393912565760.000000, mean_q: 14062651248.509554, mean_eps: 0.276232
 120835/150000: episode: 1017, duration: 1.269s, episode steps: 128, steps per second: 101, episode reward: -445.888, mean reward: -3.483 [-100.000,  1.794], mean action: 1.719 [0.000, 3.000],  loss: 818545112467374080.000000, mse: 250876963596063997952.000000, mean_q: 14063193528.000000, mean_eps: 0.275377
 120927/150000: episode: 1018, duration: 0.844s, episode steps:  92, steps per second: 109, episode reward: -248.298, mean reward: -2.699 [-100.000,  2.529], mean action: 1.837 [0.000, 3.000],  loss: 4354094986579684352.000000, mse: 273560121606413811712.000000, mean_q: 14318276830.608696, mean_eps: 0.274717
 121024/150000: episode: 1019, duration: 0.853s, episode steps:  97, steps per second: 114, episode reward: -373.794, mean reward: -3.854 [-100.000,  1.100], mean action: 2.010 [0.000, 3.000],  loss: 3443436822406187008.000000, mse: 259378882450162745344.000000, mean_q: 14067990665.237114, mean_eps: 0.274150
 121112/150000: episode: 1020, duration: 0.839s, episode steps:  88, steps per second: 105, episode reward: -372.067, mean reward: -4.228 [-100.000,  1.813], mean action: 1.852 [0.000, 3.000],  loss: 2561290835992867840.000000, mse: 264737021240979685376.000000, mean_q: 13992663133.090910, mean_eps: 0.273595
 121204/150000: episode: 1021, duration: 0.871s, episode steps:  92, steps per second: 106, episode reward: -285.873, mean reward: -3.107 [-100.000,  0.924], mean action: 1.815 [0.000, 3.000],  loss: 2008548328190509056.000000, mse: 272175215577608093696.000000, mean_q: 14337402568.347826, mean_eps: 0.273055
 121324/150000: episode: 1022, duration: 1.106s, episode steps: 120, steps per second: 108, episode reward: -502.928, mean reward: -4.191 [-100.000,  1.078], mean action: 1.783 [0.000, 3.000],  loss: 3032193298773818368.000000, mse: 264837691079594475520.000000, mean_q: 14149605632.000000, mean_eps: 0.272419
 121433/150000: episode: 1023, duration: 0.947s, episode steps: 109, steps per second: 115, episode reward: -469.864, mean reward: -4.311 [-100.000,  2.324], mean action: 1.862 [0.000, 3.000],  loss: 1081272729449370624.000000, mse: 246212388471500898304.000000, mean_q: 14013796755.963303, mean_eps: 0.271732
 121589/150000: episode: 1024, duration: 1.358s, episode steps: 156, steps per second: 115, episode reward: -792.778, mean reward: -5.082 [-100.000,  1.871], mean action: 1.833 [0.000, 3.000],  loss: 1276046499899405824.000000, mse: 256120631748222484480.000000, mean_q: 14221606721.641026, mean_eps: 0.270937
 121668/150000: episode: 1025, duration: 0.688s, episode steps:  79, steps per second: 115, episode reward: -474.644, mean reward: -6.008 [-100.000, -0.049], mean action: 1.759 [0.000, 3.000],  loss: 932763195521338240.000000, mse: 275259205662817091584.000000, mean_q: 14272924010.936708, mean_eps: 0.270232
 121756/150000: episode: 1026, duration: 0.910s, episode steps:  88, steps per second:  97, episode reward: -426.259, mean reward: -4.844 [-100.000,  0.724], mean action: 1.977 [0.000, 3.000],  loss: 3714452970547255296.000000, mse: 261765563079346323456.000000, mean_q: 14152299357.090910, mean_eps: 0.269731
 121835/150000: episode: 1027, duration: 0.755s, episode steps:  79, steps per second: 105, episode reward: -362.856, mean reward: -4.593 [-100.000,  3.274], mean action: 1.911 [0.000, 3.000],  loss: 1022736496475345408.000000, mse: 262685733929822093312.000000, mean_q: 14448548215.898735, mean_eps: 0.269230
 121956/150000: episode: 1028, duration: 1.082s, episode steps: 121, steps per second: 112, episode reward: -721.115, mean reward: -5.960 [-100.000,  2.354], mean action: 1.826 [0.000, 3.000],  loss: 710941752560939008.000000, mse: 249222510642318114816.000000, mean_q: 14222940413.884298, mean_eps: 0.268630
 122117/150000: episode: 1029, duration: 1.528s, episode steps: 161, steps per second: 105, episode reward: -724.350, mean reward: -4.499 [-100.000,  3.437], mean action: 1.789 [0.000, 3.000],  loss: 706559315242357760.000000, mse: 243994986988797460480.000000, mean_q: 14076124338.086956, mean_eps: 0.267784
 122215/150000: episode: 1030, duration: 1.058s, episode steps:  98, steps per second:  93, episode reward: -547.489, mean reward: -5.587 [-100.000,  0.964], mean action: 1.857 [0.000, 3.000],  loss: 1115075292264232960.000000, mse: 257275511956559101952.000000, mean_q: 14172173458.285715, mean_eps: 0.267007
 122310/150000: episode: 1031, duration: 1.581s, episode steps:  95, steps per second:  60, episode reward: -429.463, mean reward: -4.521 [-100.000,  1.238], mean action: 1.895 [0.000, 3.000],  loss: 879654485327615360.000000, mse: 259725335817383968768.000000, mean_q: 14204062763.115789, mean_eps: 0.266428
 122418/150000: episode: 1032, duration: 1.403s, episode steps: 108, steps per second:  77, episode reward: -480.177, mean reward: -4.446 [-100.000,  0.824], mean action: 1.824 [0.000, 3.000],  loss: 908706839703274112.000000, mse: 269142647829670264832.000000, mean_q: 14394275963.259260, mean_eps: 0.265819
 122497/150000: episode: 1033, duration: 0.694s, episode steps:  79, steps per second: 114, episode reward: -402.465, mean reward: -5.094 [-100.000,  0.414], mean action: 1.861 [0.000, 3.000],  loss: 1324734780470417152.000000, mse: 249074905209804357632.000000, mean_q: 14169931827.848101, mean_eps: 0.265258
 122585/150000: episode: 1034, duration: 0.794s, episode steps:  88, steps per second: 111, episode reward: -547.113, mean reward: -6.217 [-100.000,  0.321], mean action: 2.011 [0.000, 3.000],  loss: 4456249790285289472.000000, mse: 260173295719844675584.000000, mean_q: 14510631610.181818, mean_eps: 0.264757
 122669/150000: episode: 1035, duration: 0.767s, episode steps:  84, steps per second: 109, episode reward: -523.318, mean reward: -6.230 [-100.000,  0.193], mean action: 1.893 [0.000, 3.000],  loss: 819273805747017088.000000, mse: 250956424205164085248.000000, mean_q: 13994981741.714285, mean_eps: 0.264241
 122804/150000: episode: 1036, duration: 1.172s, episode steps: 135, steps per second: 115, episode reward: -476.347, mean reward: -3.528 [-100.000,  4.862], mean action: 1.911 [0.000, 3.000],  loss: 1392552537312330752.000000, mse: 274825216978515853312.000000, mean_q: 14392406713.837036, mean_eps: 0.263584
 122955/150000: episode: 1037, duration: 1.561s, episode steps: 151, steps per second:  97, episode reward: -625.827, mean reward: -4.145 [-100.000,  3.513], mean action: 1.868 [0.000, 3.000],  loss: 1035192754773920896.000000, mse: 243546667131691597824.000000, mean_q: 14028810931.708609, mean_eps: 0.262726
 123054/150000: episode: 1038, duration: 1.399s, episode steps:  99, steps per second:  71, episode reward: -476.093, mean reward: -4.809 [-100.000,  3.512], mean action: 1.768 [0.000, 3.000],  loss: 719470781118964352.000000, mse: 266340371433353412608.000000, mean_q: 14259073106.747475, mean_eps: 0.261976
 123152/150000: episode: 1039, duration: 0.935s, episode steps:  98, steps per second: 105, episode reward: -471.516, mean reward: -4.811 [-100.000,  0.659], mean action: 1.898 [0.000, 3.000],  loss: 1030166198068878464.000000, mse: 242602949342208229376.000000, mean_q: 14141744180.244898, mean_eps: 0.261385
 123286/150000: episode: 1040, duration: 2.008s, episode steps: 134, steps per second:  67, episode reward: -513.957, mean reward: -3.835 [-100.000,  4.343], mean action: 1.784 [0.000, 3.000],  loss: 1413538728706963456.000000, mse: 264388872434138054656.000000, mean_q: 14372544618.985075, mean_eps: 0.260689
 123370/150000: episode: 1041, duration: 1.304s, episode steps:  84, steps per second:  64, episode reward: -497.992, mean reward: -5.928 [-100.000, -0.344], mean action: 1.810 [0.000, 3.000],  loss: 2526075998504085504.000000, mse: 242455050353157963776.000000, mean_q: 14249867861.333334, mean_eps: 0.260035
 123466/150000: episode: 1042, duration: 1.585s, episode steps:  96, steps per second:  61, episode reward: -608.497, mean reward: -6.339 [-100.000,  0.339], mean action: 1.927 [0.000, 3.000],  loss: 1097195299406173568.000000, mse: 269129097387285577728.000000, mean_q: 14435610293.333334, mean_eps: 0.259495
 123657/150000: episode: 1043, duration: 2.093s, episode steps: 191, steps per second:  91, episode reward: -761.147, mean reward: -3.985 [-100.000,  1.986], mean action: 1.901 [0.000, 3.000],  loss: 2674169261686354432.000000, mse: 246744029124948852736.000000, mean_q: 14100988075.560209, mean_eps: 0.258634
 123757/150000: episode: 1044, duration: 1.181s, episode steps: 100, steps per second:  85, episode reward: -360.045, mean reward: -3.600 [-100.000,  3.133], mean action: 1.770 [0.000, 3.000],  loss: 1343474218035136512.000000, mse: 269772287141104943104.000000, mean_q: 14294298245.120001, mean_eps: 0.257761
 123854/150000: episode: 1045, duration: 0.885s, episode steps:  97, steps per second: 110, episode reward: -569.637, mean reward: -5.873 [-100.000,  0.869], mean action: 1.907 [0.000, 3.000],  loss: 900746992933612800.000000, mse: 260172533157522538496.000000, mean_q: 14276007091.463917, mean_eps: 0.257170
 123943/150000: episode: 1046, duration: 0.829s, episode steps:  89, steps per second: 107, episode reward: -413.809, mean reward: -4.650 [-100.000,  0.583], mean action: 1.921 [0.000, 3.000],  loss: 700040167951198080.000000, mse: 251510373643467161600.000000, mean_q: 14344511798.651686, mean_eps: 0.256612
 124151/150000: episode: 1047, duration: 1.810s, episode steps: 208, steps per second: 115, episode reward: -919.833, mean reward: -4.422 [-100.000,  3.586], mean action: 1.865 [0.000, 3.000],  loss: 1671200217610523648.000000, mse: 254323339833082675200.000000, mean_q: 14344181504.000000, mean_eps: 0.255721
 124413/150000: episode: 1048, duration: 2.522s, episode steps: 262, steps per second: 104, episode reward: -1061.979, mean reward: -4.053 [-100.000,  3.589], mean action: 1.771 [0.000, 3.000],  loss: 1984789420981784832.000000, mse: 268829896421239554048.000000, mean_q: 14504063562.259542, mean_eps: 0.254311
 124517/150000: episode: 1049, duration: 1.102s, episode steps: 104, steps per second:  94, episode reward: -464.193, mean reward: -4.463 [-100.000,  3.256], mean action: 1.875 [0.000, 3.000],  loss: 1412299580678401024.000000, mse: 262398160289717387264.000000, mean_q: 14515048812.307692, mean_eps: 0.253213
 124608/150000: episode: 1050, duration: 1.206s, episode steps:  91, steps per second:  75, episode reward: -542.547, mean reward: -5.962 [-100.000,  0.370], mean action: 1.846 [0.000, 3.000],  loss: 690395088164227968.000000, mse: 261754512936960163840.000000, mean_q: 14451769872.879122, mean_eps: 0.252628
 124703/150000: episode: 1051, duration: 1.240s, episode steps:  95, steps per second:  77, episode reward: -514.265, mean reward: -5.413 [-100.000,  0.501], mean action: 1.821 [0.000, 2.000],  loss: 1334241933676549888.000000, mse: 272442936363315757056.000000, mean_q: 14581300806.063158, mean_eps: 0.252070
 124792/150000: episode: 1052, duration: 1.096s, episode steps:  89, steps per second:  81, episode reward: -359.113, mean reward: -4.035 [-100.000,  1.059], mean action: 1.843 [0.000, 3.000],  loss: 1196409534018757376.000000, mse: 257795283905315831808.000000, mean_q: 14366013911.730337, mean_eps: 0.251518
 124913/150000: episode: 1053, duration: 1.437s, episode steps: 121, steps per second:  84, episode reward: -560.372, mean reward: -4.631 [-100.000,  1.501], mean action: 1.884 [0.000, 3.000],  loss: 1116312099138204160.000000, mse: 255826802510210236416.000000, mean_q: 14392368568.066116, mean_eps: 0.250888
 124997/150000: episode: 1054, duration: 0.853s, episode steps:  84, steps per second:  99, episode reward: -418.306, mean reward: -4.980 [-100.000,  0.772], mean action: 1.929 [0.000, 3.000],  loss: 879640149724096384.000000, mse: 258837254218670505984.000000, mean_q: 14371848265.142857, mean_eps: 0.250273
 125078/150000: episode: 1055, duration: 0.695s, episode steps:  81, steps per second: 117, episode reward: -306.591, mean reward: -3.785 [-100.000,  2.297], mean action: 1.778 [0.000, 3.000],  loss: 1389700042208743680.000000, mse: 258660142578051350528.000000, mean_q: 14582933200.592592, mean_eps: 0.249778
 125363/150000: episode: 1056, duration: 2.652s, episode steps: 285, steps per second: 107, episode reward: -1642.162, mean reward: -5.762 [-100.000,  3.023], mean action: 1.867 [0.000, 3.000],  loss: 1138271732858601344.000000, mse: 269406773101872775168.000000, mean_q: 14581735506.638596, mean_eps: 0.248680
 125440/150000: episode: 1057, duration: 0.673s, episode steps:  77, steps per second: 114, episode reward: -336.838, mean reward: -4.375 [-100.000,  1.288], mean action: 1.857 [0.000, 3.000],  loss: 1889897102780082944.000000, mse: 282370672239254831104.000000, mean_q: 14605210703.792208, mean_eps: 0.247594
 125519/150000: episode: 1058, duration: 0.684s, episode steps:  79, steps per second: 115, episode reward: -464.331, mean reward: -5.878 [-100.000,  1.422], mean action: 1.848 [0.000, 3.000],  loss: 1323610503170958336.000000, mse: 259000503980279332864.000000, mean_q: 14429009790.379747, mean_eps: 0.247126
 125794/150000: episode: 1059, duration: 2.806s, episode steps: 275, steps per second:  98, episode reward: -1143.292, mean reward: -4.157 [-100.000,  1.559], mean action: 1.785 [0.000, 3.000],  loss: 1178888558914224896.000000, mse: 256422058146353446912.000000, mean_q: 14476154790.632727, mean_eps: 0.246064
 125900/150000: episode: 1060, duration: 0.967s, episode steps: 106, steps per second: 110, episode reward: -262.343, mean reward: -2.475 [-100.000,  3.275], mean action: 1.783 [0.000, 3.000],  loss: 1721441069860314624.000000, mse: 262243092212036042752.000000, mean_q: 14468723209.660378, mean_eps: 0.244921
 125997/150000: episode: 1061, duration: 0.884s, episode steps:  97, steps per second: 110, episode reward: -479.182, mean reward: -4.940 [-100.000,  0.714], mean action: 1.866 [0.000, 3.000],  loss: 1203704900122972160.000000, mse: 268886209236509491200.000000, mean_q: 14661302293.113401, mean_eps: 0.244312
 126092/150000: episode: 1062, duration: 0.955s, episode steps:  95, steps per second:  99, episode reward: -282.754, mean reward: -2.976 [-100.000,  2.808], mean action: 1.811 [0.000, 3.000],  loss: 1870749588520871680.000000, mse: 280043280537305874432.000000, mean_q: 14712093663.663158, mean_eps: 0.243736
 126175/150000: episode: 1063, duration: 0.856s, episode steps:  83, steps per second:  97, episode reward: -600.431, mean reward: -7.234 [-100.000, -0.148], mean action: 1.916 [0.000, 3.000],  loss: 659228100231278592.000000, mse: 238187626031602925568.000000, mean_q: 14313549651.277109, mean_eps: 0.243202
 126278/150000: episode: 1064, duration: 0.984s, episode steps: 103, steps per second: 105, episode reward: -411.071, mean reward: -3.991 [-100.000,  1.754], mean action: 1.835 [0.000, 3.000],  loss: 892708791516573184.000000, mse: 258251212094312054784.000000, mean_q: 14373077777.398058, mean_eps: 0.242644
 126371/150000: episode: 1065, duration: 0.907s, episode steps:  93, steps per second: 103, episode reward: -571.556, mean reward: -6.146 [-100.000,  1.062], mean action: 1.935 [0.000, 3.000],  loss: 837298272517102464.000000, mse: 271019733788982738944.000000, mean_q: 14605415126.709677, mean_eps: 0.242056
 126536/150000: episode: 1066, duration: 1.737s, episode steps: 165, steps per second:  95, episode reward: -698.954, mean reward: -4.236 [-100.000,  3.937], mean action: 1.903 [0.000, 3.000],  loss: 1370488816853732864.000000, mse: 270793976378070695936.000000, mean_q: 14812691493.236364, mean_eps: 0.241282
 126685/150000: episode: 1067, duration: 1.630s, episode steps: 149, steps per second:  91, episode reward: -732.740, mean reward: -4.918 [-100.000,  1.215], mean action: 1.906 [0.000, 3.000],  loss: 2533715981535962112.000000, mse: 286031946780024209408.000000, mean_q: 14908646599.302013, mean_eps: 0.240340
 126782/150000: episode: 1068, duration: 1.082s, episode steps:  97, steps per second:  90, episode reward: -601.677, mean reward: -6.203 [-100.000,  0.662], mean action: 1.825 [0.000, 3.000],  loss: 4082954510600689664.000000, mse: 255610657146845626368.000000, mean_q: 14441404120.412371, mean_eps: 0.239602
 127017/150000: episode: 1069, duration: 2.997s, episode steps: 235, steps per second:  78, episode reward: -1572.267, mean reward: -6.690 [-100.000,  2.065], mean action: 1.940 [0.000, 3.000],  loss: 994188174675972096.000000, mse: 261860702393897418752.000000, mean_q: 14650175640.510639, mean_eps: 0.238606
 127115/150000: episode: 1070, duration: 1.420s, episode steps:  98, steps per second:  69, episode reward: -430.338, mean reward: -4.391 [-100.000,  0.998], mean action: 1.888 [0.000, 3.000],  loss: 4344558992872905216.000000, mse: 287116566025947873280.000000, mean_q: 14881543230.693878, mean_eps: 0.237607
 127213/150000: episode: 1071, duration: 1.441s, episode steps:  98, steps per second:  68, episode reward: -370.329, mean reward: -3.779 [-100.000,  2.614], mean action: 1.765 [0.000, 3.000],  loss: 1533783184812636160.000000, mse: 286513805853068591104.000000, mean_q: 14955938335.346939, mean_eps: 0.237019
 127287/150000: episode: 1072, duration: 0.793s, episode steps:  74, steps per second:  93, episode reward: -348.352, mean reward: -4.707 [-100.000,  1.906], mean action: 1.770 [0.000, 3.000],  loss: 956675146076779008.000000, mse: 276868969402337296384.000000, mean_q: 14667875494.054054, mean_eps: 0.236503
 127369/150000: episode: 1073, duration: 0.840s, episode steps:  82, steps per second:  98, episode reward: -613.860, mean reward: -7.486 [-100.000, -0.545], mean action: 2.012 [0.000, 3.000],  loss: 1976121702178958336.000000, mse: 295120331144039038976.000000, mean_q: 15044734551.414635, mean_eps: 0.236035
 127450/150000: episode: 1074, duration: 0.787s, episode steps:  81, steps per second: 103, episode reward: -527.591, mean reward: -6.513 [-100.000, -0.296], mean action: 1.864 [0.000, 3.000],  loss: 1617452692410854656.000000, mse: 276545599920076980224.000000, mean_q: 14778137031.111111, mean_eps: 0.235546
 127523/150000: episode: 1075, duration: 0.635s, episode steps:  73, steps per second: 115, episode reward: -514.207, mean reward: -7.044 [-100.000, -1.233], mean action: 2.000 [0.000, 3.000],  loss: 884188375896419072.000000, mse: 270282594069519695872.000000, mean_q: 14734357321.643835, mean_eps: 0.235084
 127723/150000: episode: 1076, duration: 1.776s, episode steps: 200, steps per second: 113, episode reward: -876.991, mean reward: -4.385 [-100.000,  2.711], mean action: 1.880 [0.000, 3.000],  loss: 2690805980022198784.000000, mse: 286776836337325834240.000000, mean_q: 15026083005.440001, mean_eps: 0.234265
 127815/150000: episode: 1077, duration: 0.801s, episode steps:  92, steps per second: 115, episode reward: -551.352, mean reward: -5.993 [-100.000,  2.448], mean action: 1.880 [0.000, 3.000],  loss: 3590277091402369024.000000, mse: 283659186676290650112.000000, mean_q: 14887962445.913044, mean_eps: 0.233389
 127954/150000: episode: 1078, duration: 1.262s, episode steps: 139, steps per second: 110, episode reward: -602.402, mean reward: -4.334 [-100.000,  2.022], mean action: 1.906 [0.000, 3.000],  loss: 5181805029199636480.000000, mse: 274859359717409619968.000000, mean_q: 14647177459.107914, mean_eps: 0.232696
 128100/150000: episode: 1079, duration: 1.257s, episode steps: 146, steps per second: 116, episode reward: -820.050, mean reward: -5.617 [-100.000,  1.317], mean action: 1.863 [0.000, 3.000],  loss: 1354716970173936896.000000, mse: 265509419374353350656.000000, mean_q: 14591520185.863014, mean_eps: 0.231841
 128236/150000: episode: 1080, duration: 1.202s, episode steps: 136, steps per second: 113, episode reward: -554.421, mean reward: -4.077 [-100.000,  4.309], mean action: 1.846 [0.000, 3.000],  loss: 2029627548948326656.000000, mse: 279201044568706646016.000000, mean_q: 14883749541.647058, mean_eps: 0.230995
 128336/150000: episode: 1081, duration: 0.854s, episode steps: 100, steps per second: 117, episode reward: -502.183, mean reward: -5.022 [-100.000,  3.352], mean action: 1.890 [0.000, 3.000],  loss: 2118450734735588096.000000, mse: 260664419152591749120.000000, mean_q: 14673101332.480000, mean_eps: 0.230287
 128482/150000: episode: 1082, duration: 1.279s, episode steps: 146, steps per second: 114, episode reward: -989.237, mean reward: -6.776 [-100.000,  1.297], mean action: 1.979 [0.000, 3.000],  loss: 3049772163066017280.000000, mse: 274158835737999638528.000000, mean_q: 14893109297.095890, mean_eps: 0.229549
 128567/150000: episode: 1083, duration: 0.730s, episode steps:  85, steps per second: 117, episode reward: -490.977, mean reward: -5.776 [-100.000,  0.271], mean action: 1.871 [0.000, 3.000],  loss: 678037880865110272.000000, mse: 275277141944423022592.000000, mean_q: 14871454189.929411, mean_eps: 0.228856
 128845/150000: episode: 1084, duration: 2.420s, episode steps: 278, steps per second: 115, episode reward: -1948.253, mean reward: -7.008 [-100.000,  1.795], mean action: 1.896 [0.000, 3.000],  loss: 1383454103668232448.000000, mse: 247117015786258923520.000000, mean_q: 14570510391.251799, mean_eps: 0.227767
 128920/150000: episode: 1085, duration: 0.647s, episode steps:  75, steps per second: 116, episode reward: -503.789, mean reward: -6.717 [-100.000,  0.612], mean action: 1.800 [0.000, 3.000],  loss: 1298108168711022336.000000, mse: 281744716057337659392.000000, mean_q: 15103609309.866667, mean_eps: 0.226708
 129040/150000: episode: 1086, duration: 1.019s, episode steps: 120, steps per second: 118, episode reward: -604.157, mean reward: -5.035 [-100.000,  0.658], mean action: 1.917 [0.000, 3.000],  loss: 1036468776753902336.000000, mse: 283364206994223202304.000000, mean_q: 15029158272.000000, mean_eps: 0.226123
 129269/150000: episode: 1087, duration: 2.042s, episode steps: 229, steps per second: 112, episode reward: -1539.023, mean reward: -6.721 [-100.000,  1.268], mean action: 1.795 [0.000, 3.000],  loss: 1588468763336497664.000000, mse: 278084551002480214016.000000, mean_q: 14959088921.711790, mean_eps: 0.225076
 129427/150000: episode: 1088, duration: 1.500s, episode steps: 158, steps per second: 105, episode reward: -837.882, mean reward: -5.303 [-100.000,  1.652], mean action: 1.880 [0.000, 3.000],  loss: 1573664861973898752.000000, mse: 283546229143739858944.000000, mean_q: 15070808789.873417, mean_eps: 0.223915
 129527/150000: episode: 1089, duration: 0.930s, episode steps: 100, steps per second: 108, episode reward: -297.327, mean reward: -2.973 [-100.000,  3.205], mean action: 1.820 [0.000, 3.000],  loss: 2106339860718970368.000000, mse: 293221029963275370496.000000, mean_q: 15193419612.160000, mean_eps: 0.223141
 129640/150000: episode: 1090, duration: 1.150s, episode steps: 113, steps per second:  98, episode reward: -567.769, mean reward: -5.025 [-100.000,  3.913], mean action: 1.947 [0.000, 3.000],  loss: 2736031879150375936.000000, mse: 293547569235548405760.000000, mean_q: 15253927120.424778, mean_eps: 0.222502
 129716/150000: episode: 1091, duration: 0.751s, episode steps:  76, steps per second: 101, episode reward: -511.819, mean reward: -6.734 [-100.000, -0.210], mean action: 1.895 [0.000, 3.000],  loss: 960776306693077376.000000, mse: 277788804489171009536.000000, mean_q: 15009819701.894737, mean_eps: 0.221935
 129805/150000: episode: 1092, duration: 0.869s, episode steps:  89, steps per second: 102, episode reward: -339.475, mean reward: -3.814 [-100.000,  2.501], mean action: 1.899 [0.000, 3.000],  loss: 1079463357794528384.000000, mse: 284864541866508288000.000000, mean_q: 15185299962.247190, mean_eps: 0.221440
 129909/150000: episode: 1093, duration: 1.007s, episode steps: 104, steps per second: 103, episode reward: -620.288, mean reward: -5.964 [-100.000,  1.499], mean action: 1.933 [0.000, 3.000],  loss: 1304893911047198976.000000, mse: 276026181912467865600.000000, mean_q: 15086949996.307692, mean_eps: 0.220861
 130022/150000: episode: 1094, duration: 1.021s, episode steps: 113, steps per second: 111, episode reward: -515.811, mean reward: -4.565 [-100.000,  2.334], mean action: 1.965 [0.000, 3.000],  loss: 1229448292391385344.000000, mse: 289749158457407766528.000000, mean_q: 15181304523.893805, mean_eps: 0.220210
 130127/150000: episode: 1095, duration: 0.940s, episode steps: 105, steps per second: 112, episode reward: -576.918, mean reward: -5.494 [-100.000,  0.839], mean action: 1.886 [0.000, 3.000],  loss: 1372931293736010240.000000, mse: 250171625916264906752.000000, mean_q: 14763701257.752380, mean_eps: 0.219556
 130225/150000: episode: 1096, duration: 0.850s, episode steps:  98, steps per second: 115, episode reward: -608.804, mean reward: -6.212 [-100.000,  0.234], mean action: 1.918 [0.000, 3.000],  loss: 1667097316479633408.000000, mse: 301380334974162370560.000000, mean_q: 15489226825.142857, mean_eps: 0.218947
 130309/150000: episode: 1097, duration: 0.753s, episode steps:  84, steps per second: 112, episode reward: -422.347, mean reward: -5.028 [-100.000,  1.417], mean action: 1.857 [0.000, 3.000],  loss: 1120710824823232640.000000, mse: 263854088281700794368.000000, mean_q: 14882166467.047619, mean_eps: 0.218401
 130397/150000: episode: 1098, duration: 0.755s, episode steps:  88, steps per second: 117, episode reward: -395.123, mean reward: -4.490 [-100.000,  0.802], mean action: 1.989 [1.000, 3.000],  loss: 825527636377445120.000000, mse: 272458705985207828480.000000, mean_q: 15116831220.363636, mean_eps: 0.217885
 130497/150000: episode: 1099, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: -540.356, mean reward: -5.404 [-100.000,  1.599], mean action: 1.890 [0.000, 3.000],  loss: 2644643802215277056.000000, mse: 280854788380915924992.000000, mean_q: 15129735004.160000, mean_eps: 0.217321
 130624/150000: episode: 1100, duration: 1.111s, episode steps: 127, steps per second: 114, episode reward: -792.146, mean reward: -6.237 [-100.000,  0.755], mean action: 1.929 [0.000, 3.000],  loss: 1979859571446543616.000000, mse: 275595677969613815808.000000, mean_q: 15005979309.354330, mean_eps: 0.216640
 130701/150000: episode: 1101, duration: 0.659s, episode steps:  77, steps per second: 117, episode reward: -506.472, mean reward: -6.578 [-100.000, -0.360], mean action: 1.987 [0.000, 3.000],  loss: 965398366042425472.000000, mse: 310568687014896730112.000000, mean_q: 15455764466.701298, mean_eps: 0.216028
 130781/150000: episode: 1102, duration: 0.700s, episode steps:  80, steps per second: 114, episode reward: -392.568, mean reward: -4.907 [-100.000,  1.617], mean action: 1.925 [0.000, 3.000],  loss: 1083321725079047424.000000, mse: 268693360965848334336.000000, mean_q: 15027025779.200001, mean_eps: 0.215557
 130943/150000: episode: 1103, duration: 1.393s, episode steps: 162, steps per second: 116, episode reward: -671.724, mean reward: -4.146 [-100.000,  2.076], mean action: 1.914 [0.000, 3.000],  loss: 1231862951782134528.000000, mse: 290691547847292223488.000000, mean_q: 15312614621.234568, mean_eps: 0.214831
 131069/150000: episode: 1104, duration: 1.102s, episode steps: 126, steps per second: 114, episode reward: -715.750, mean reward: -5.681 [-100.000,  2.897], mean action: 1.897 [0.000, 3.000],  loss: 1633888606860470272.000000, mse: 283999317951623659520.000000, mean_q: 15164184754.793652, mean_eps: 0.213967
 131275/150000: episode: 1105, duration: 1.779s, episode steps: 206, steps per second: 116, episode reward: -1260.701, mean reward: -6.120 [-100.000,  1.606], mean action: 1.922 [0.000, 3.000],  loss: 2236760593730414848.000000, mse: 291421709377222770688.000000, mean_q: 15291758765.980583, mean_eps: 0.212971
 131509/150000: episode: 1106, duration: 2.036s, episode steps: 234, steps per second: 115, episode reward: -1267.406, mean reward: -5.416 [-100.000,  1.777], mean action: 1.893 [0.000, 3.000],  loss: 1254005359995899904.000000, mse: 279001685072298278912.000000, mean_q: 15130595669.333334, mean_eps: 0.211651
 131596/150000: episode: 1107, duration: 0.766s, episode steps:  87, steps per second: 114, episode reward: -482.057, mean reward: -5.541 [-100.000,  0.627], mean action: 1.908 [0.000, 3.000],  loss: 2048648469815685376.000000, mse: 283671342737279156224.000000, mean_q: 15087211496.459770, mean_eps: 0.210688
 131733/150000: episode: 1108, duration: 1.164s, episode steps: 137, steps per second: 118, episode reward: -816.030, mean reward: -5.956 [-100.000,  2.155], mean action: 1.869 [0.000, 3.000],  loss: 2622874646460264448.000000, mse: 290179526609809899520.000000, mean_q: 15417364480.000000, mean_eps: 0.210016
 131819/150000: episode: 1109, duration: 0.745s, episode steps:  86, steps per second: 115, episode reward: -374.394, mean reward: -4.353 [-100.000,  3.029], mean action: 1.860 [0.000, 3.000],  loss: 1476453597621573120.000000, mse: 300613204772155555840.000000, mean_q: 15442045761.488373, mean_eps: 0.209347
 131908/150000: episode: 1110, duration: 0.770s, episode steps:  89, steps per second: 116, episode reward: -278.584, mean reward: -3.130 [-100.000,  2.975], mean action: 1.921 [0.000, 3.000],  loss: 353060189904597760.000000, mse: 268840405971907084288.000000, mean_q: 15009490932.494383, mean_eps: 0.208822
 131993/150000: episode: 1111, duration: 0.749s, episode steps:  85, steps per second: 114, episode reward: -580.868, mean reward: -6.834 [-100.000, -0.311], mean action: 1.871 [0.000, 3.000],  loss: 947090838347693440.000000, mse: 308426823112948056064.000000, mean_q: 15674365699.011765, mean_eps: 0.208300
 132100/150000: episode: 1112, duration: 0.999s, episode steps: 107, steps per second: 107, episode reward: -432.193, mean reward: -4.039 [-100.000,  4.085], mean action: 1.963 [0.000, 3.000],  loss: 1906337294346271488.000000, mse: 293630185199584903168.000000, mean_q: 15285911293.607477, mean_eps: 0.207724
 132216/150000: episode: 1113, duration: 0.994s, episode steps: 116, steps per second: 117, episode reward: -348.052, mean reward: -3.000 [-100.000,  1.526], mean action: 1.905 [0.000, 3.000],  loss: 1084483991321408256.000000, mse: 279357496877182484480.000000, mean_q: 15152045347.310345, mean_eps: 0.207055
 132482/150000: episode: 1114, duration: 2.595s, episode steps: 266, steps per second: 103, episode reward: -1325.781, mean reward: -4.984 [-100.000,  1.577], mean action: 1.868 [0.000, 3.000],  loss: 2669790202659133440.000000, mse: 300595992106994827264.000000, mean_q: 15492233577.864662, mean_eps: 0.205909
 132603/150000: episode: 1115, duration: 1.087s, episode steps: 121, steps per second: 111, episode reward: -767.839, mean reward: -6.346 [-100.000,  1.280], mean action: 1.909 [0.000, 3.000],  loss: 1697319684329597440.000000, mse: 293676784791582638080.000000, mean_q: 15409098633.520660, mean_eps: 0.204748
 132742/150000: episode: 1116, duration: 1.229s, episode steps: 139, steps per second: 113, episode reward: -814.697, mean reward: -5.861 [-100.000,  1.845], mean action: 2.029 [0.000, 3.000],  loss: 1732000678925200128.000000, mse: 292440159251048988672.000000, mean_q: 15401988847.424461, mean_eps: 0.203968
 132848/150000: episode: 1117, duration: 0.910s, episode steps: 106, steps per second: 117, episode reward: -549.343, mean reward: -5.182 [-100.000,  0.174], mean action: 1.991 [0.000, 3.000],  loss: 1067619059921168640.000000, mse: 295945988640235388928.000000, mean_q: 15440605155.018867, mean_eps: 0.203233
 132924/150000: episode: 1118, duration: 0.689s, episode steps:  76, steps per second: 110, episode reward: -511.957, mean reward: -6.736 [-100.000, -0.394], mean action: 1.882 [0.000, 3.000],  loss: 1796234503551271680.000000, mse: 287274054501911330816.000000, mean_q: 15429157712.842106, mean_eps: 0.202687
 133003/150000: episode: 1119, duration: 0.950s, episode steps:  79, steps per second:  83, episode reward: -327.690, mean reward: -4.148 [-100.000,  2.328], mean action: 1.962 [0.000, 3.000],  loss: 4629153294875759616.000000, mse: 318500074117389418496.000000, mean_q: 15547683684.455696, mean_eps: 0.202222
 133082/150000: episode: 1120, duration: 0.756s, episode steps:  79, steps per second: 104, episode reward: -440.540, mean reward: -5.576 [-100.000,  1.027], mean action: 1.886 [0.000, 3.000],  loss: 1714129807199124992.000000, mse: 285109132869794463744.000000, mean_q: 15343738854.075949, mean_eps: 0.201748
 133165/150000: episode: 1121, duration: 0.751s, episode steps:  83, steps per second: 111, episode reward: -468.267, mean reward: -5.642 [-100.000,  0.540], mean action: 1.880 [0.000, 3.000],  loss: 875719477702117120.000000, mse: 295533210509821935616.000000, mean_q: 15464826374.168674, mean_eps: 0.201262
 133246/150000: episode: 1122, duration: 0.804s, episode steps:  81, steps per second: 101, episode reward: -515.007, mean reward: -6.358 [-100.000,  0.209], mean action: 1.938 [0.000, 3.000],  loss: 955145027406674176.000000, mse: 261074285147835531264.000000, mean_q: 14858663025.777779, mean_eps: 0.200770
 133329/150000: episode: 1123, duration: 0.716s, episode steps:  83, steps per second: 116, episode reward: -504.114, mean reward: -6.074 [-100.000,  0.347], mean action: 1.892 [0.000, 3.000],  loss: 913834313557885952.000000, mse: 296901937056268025856.000000, mean_q: 15576617478.168674, mean_eps: 0.200278
 133449/150000: episode: 1124, duration: 1.130s, episode steps: 120, steps per second: 106, episode reward: -765.272, mean reward: -6.377 [-100.000,  0.701], mean action: 1.917 [0.000, 3.000],  loss: 1338870709537088000.000000, mse: 293221678308631904256.000000, mean_q: 15521094545.066668, mean_eps: 0.199669
 133547/150000: episode: 1125, duration: 1.003s, episode steps:  98, steps per second:  98, episode reward: -704.437, mean reward: -7.188 [-100.000,  0.744], mean action: 1.796 [0.000, 3.000],  loss: 1919819986014707456.000000, mse: 294543990025606070272.000000, mean_q: 15582813309.387754, mean_eps: 0.199015
 133666/150000: episode: 1126, duration: 1.150s, episode steps: 119, steps per second: 104, episode reward: -688.599, mean reward: -5.787 [-100.000,  1.463], mean action: 1.983 [0.000, 3.000],  loss: 1101472335973668864.000000, mse: 266689886797582761984.000000, mean_q: 15207634255.596638, mean_eps: 0.198364
 133782/150000: episode: 1127, duration: 1.121s, episode steps: 116, steps per second: 104, episode reward: -445.551, mean reward: -3.841 [-100.000,  1.772], mean action: 1.931 [0.000, 3.000],  loss: 1663388226148901632.000000, mse: 332038215510682501120.000000, mean_q: 15910045254.620689, mean_eps: 0.197659
 133855/150000: episode: 1128, duration: 0.731s, episode steps:  73, steps per second: 100, episode reward: -578.401, mean reward: -7.923 [-100.000, -1.083], mean action: 1.849 [0.000, 3.000],  loss: 1781308885774494464.000000, mse: 290533630458699546624.000000, mean_q: 15443121194.082191, mean_eps: 0.197092
 134191/150000: episode: 1129, duration: 3.538s, episode steps: 336, steps per second:  95, episode reward: -1925.092, mean reward: -5.729 [-100.000,  3.863], mean action: 1.908 [0.000, 3.000],  loss: 1881964376783086080.000000, mse: 294156188100833083392.000000, mean_q: 15495011449.904762, mean_eps: 0.195865
 134276/150000: episode: 1130, duration: 0.894s, episode steps:  85, steps per second:  95, episode reward: -550.931, mean reward: -6.482 [-100.000,  0.366], mean action: 1.882 [0.000, 3.000],  loss: 1816115450544442624.000000, mse: 275068976297529081856.000000, mean_q: 15420129713.694118, mean_eps: 0.194602
 134391/150000: episode: 1131, duration: 1.135s, episode steps: 115, steps per second: 101, episode reward: -551.108, mean reward: -4.792 [-100.000,  1.100], mean action: 1.870 [0.000, 3.000],  loss: 2091432303136756992.000000, mse: 305191836280800673792.000000, mean_q: 15648161097.460869, mean_eps: 0.194002
 134540/150000: episode: 1132, duration: 1.535s, episode steps: 149, steps per second:  97, episode reward: -712.500, mean reward: -4.782 [-100.000,  3.979], mean action: 1.919 [0.000, 3.000],  loss: 996895075456137856.000000, mse: 308323748484973527040.000000, mean_q: 15576200789.906040, mean_eps: 0.193210
 134808/150000: episode: 1133, duration: 2.487s, episode steps: 268, steps per second: 108, episode reward: -1803.837, mean reward: -6.731 [-100.000,  1.322], mean action: 1.888 [0.000, 3.000],  loss: 1703421922122861312.000000, mse: 303504583466812768256.000000, mean_q: 15660028568.835821, mean_eps: 0.191959
 134991/150000: episode: 1134, duration: 1.956s, episode steps: 183, steps per second:  94, episode reward: -840.575, mean reward: -4.593 [-100.000,  3.471], mean action: 1.956 [0.000, 3.000],  loss: 1931736118349083648.000000, mse: 296667901056213319680.000000, mean_q: 15654904596.983606, mean_eps: 0.190606
 135152/150000: episode: 1135, duration: 1.411s, episode steps: 161, steps per second: 114, episode reward: -551.260, mean reward: -3.424 [-100.000,  4.429], mean action: 1.907 [0.000, 3.000],  loss: 2492292544363741184.000000, mse: 310526496065492025344.000000, mean_q: 15846868069.763975, mean_eps: 0.189574
 135243/150000: episode: 1136, duration: 0.798s, episode steps:  91, steps per second: 114, episode reward: -467.642, mean reward: -5.139 [-100.000,  0.161], mean action: 1.857 [0.000, 3.000],  loss: 1228005716009963264.000000, mse: 305067466326994059264.000000, mean_q: 15600168554.901098, mean_eps: 0.188818
 135394/150000: episode: 1137, duration: 1.308s, episode steps: 151, steps per second: 115, episode reward: -576.451, mean reward: -3.818 [-100.000,  4.405], mean action: 1.947 [0.000, 3.000],  loss: 2566917222009275392.000000, mse: 323266609801072738304.000000, mean_q: 15954933522.649006, mean_eps: 0.188092
 135661/150000: episode: 1138, duration: 2.914s, episode steps: 267, steps per second:  92, episode reward: -1774.347, mean reward: -6.645 [-100.000,  1.077], mean action: 1.914 [0.000, 3.000],  loss: 1890500369468046336.000000, mse: 304614462437606752256.000000, mean_q: 15683651503.460674, mean_eps: 0.186838
 135788/150000: episode: 1139, duration: 1.143s, episode steps: 127, steps per second: 111, episode reward: -754.716, mean reward: -5.943 [-100.000,  0.510], mean action: 1.874 [0.000, 3.000],  loss: 2881517016585842688.000000, mse: 299203022221131644928.000000, mean_q: 15677249576.314960, mean_eps: 0.185656
 135894/150000: episode: 1140, duration: 1.014s, episode steps: 106, steps per second: 105, episode reward: -363.581, mean reward: -3.430 [-100.000,  3.817], mean action: 2.000 [0.000, 3.000],  loss: 1267277207450528000.000000, mse: 294049298055481950208.000000, mean_q: 15474483750.641510, mean_eps: 0.184957
 136119/150000: episode: 1141, duration: 2.076s, episode steps: 225, steps per second: 108, episode reward: -966.452, mean reward: -4.295 [-100.000,  4.112], mean action: 1.920 [0.000, 3.000],  loss: 3326623314301458944.000000, mse: 302541756796082782208.000000, mean_q: 15657894038.186666, mean_eps: 0.183964
 136386/150000: episode: 1142, duration: 2.678s, episode steps: 267, steps per second: 100, episode reward: -1901.450, mean reward: -7.122 [-100.000,  2.371], mean action: 1.903 [0.000, 3.000],  loss: 1923849246277657600.000000, mse: 302261911812059234304.000000, mean_q: 15717753016.089888, mean_eps: 0.182488
 136475/150000: episode: 1143, duration: 0.783s, episode steps:  89, steps per second: 114, episode reward: -494.562, mean reward: -5.557 [-100.000,  0.739], mean action: 1.933 [0.000, 3.000],  loss: 2525318781189762560.000000, mse: 287637136011892162560.000000, mean_q: 15576628189.483147, mean_eps: 0.181420
 136568/150000: episode: 1144, duration: 1.021s, episode steps:  93, steps per second:  91, episode reward: -482.775, mean reward: -5.191 [-100.000,  0.761], mean action: 1.989 [0.000, 3.000],  loss: 3149081458798107648.000000, mse: 326526869132340035584.000000, mean_q: 16094238015.311829, mean_eps: 0.180874
 136683/150000: episode: 1145, duration: 1.537s, episode steps: 115, steps per second:  75, episode reward: -633.882, mean reward: -5.512 [-100.000,  3.892], mean action: 1.939 [0.000, 3.000],  loss: 2074414184442560000.000000, mse: 298323692955676639232.000000, mean_q: 15681736855.373913, mean_eps: 0.180250
 136773/150000: episode: 1146, duration: 0.956s, episode steps:  90, steps per second:  94, episode reward: -506.749, mean reward: -5.631 [-100.000,  0.259], mean action: 1.933 [0.000, 3.000],  loss: 2184412031741177856.000000, mse: 322750469054725226496.000000, mean_q: 16157652400.355556, mean_eps: 0.179635
 136848/150000: episode: 1147, duration: 0.819s, episode steps:  75, steps per second:  92, episode reward: -483.654, mean reward: -6.449 [-100.000,  0.052], mean action: 1.853 [0.000, 3.000],  loss: 985816147814378368.000000, mse: 295926210699205541888.000000, mean_q: 15464435807.573334, mean_eps: 0.179140
 136944/150000: episode: 1148, duration: 1.078s, episode steps:  96, steps per second:  89, episode reward: -442.243, mean reward: -4.607 [-100.000,  3.107], mean action: 1.948 [0.000, 3.000],  loss: 4307844810018564608.000000, mse: 325922811962322518016.000000, mean_q: 16106874805.333334, mean_eps: 0.178627
 137130/150000: episode: 1149, duration: 1.670s, episode steps: 186, steps per second: 111, episode reward: -999.398, mean reward: -5.373 [-100.000,  1.884], mean action: 1.892 [0.000, 3.000],  loss: 2564924783500265472.000000, mse: 321380064850048450560.000000, mean_q: 15940853154.408602, mean_eps: 0.177781
 137315/150000: episode: 1150, duration: 1.576s, episode steps: 185, steps per second: 117, episode reward: -909.594, mean reward: -4.917 [-100.000,  3.334], mean action: 1.930 [0.000, 3.000],  loss: 1170788736218639872.000000, mse: 308644223790491828224.000000, mean_q: 15755826054.227028, mean_eps: 0.176668
 137450/150000: episode: 1151, duration: 1.167s, episode steps: 135, steps per second: 116, episode reward: -602.647, mean reward: -4.464 [-100.000,  1.842], mean action: 1.941 [0.000, 3.000],  loss: 4613270926595392512.000000, mse: 307875467360644628480.000000, mean_q: 15776946782.814816, mean_eps: 0.175708
 137614/150000: episode: 1152, duration: 1.410s, episode steps: 164, steps per second: 116, episode reward: -650.180, mean reward: -3.965 [-100.000,  4.445], mean action: 1.817 [0.000, 3.000],  loss: 2646181315521364992.000000, mse: 301770566856848637952.000000, mean_q: 15753704423.024391, mean_eps: 0.174811
 137825/150000: episode: 1153, duration: 1.811s, episode steps: 211, steps per second: 116, episode reward: -1625.628, mean reward: -7.704 [-100.000,  2.526], mean action: 1.934 [0.000, 3.000],  loss: 2443856804686033920.000000, mse: 313397445099644256256.000000, mean_q: 15910531290.388626, mean_eps: 0.173686
 137977/150000: episode: 1154, duration: 1.321s, episode steps: 152, steps per second: 115, episode reward: -931.893, mean reward: -6.131 [-100.000,  1.996], mean action: 1.842 [0.000, 3.000],  loss: 2237586973612434688.000000, mse: 335367041472412516352.000000, mean_q: 16136887019.789474, mean_eps: 0.172597
 138075/150000: episode: 1155, duration: 0.848s, episode steps:  98, steps per second: 116, episode reward: -494.201, mean reward: -5.043 [-100.000,  0.920], mean action: 1.888 [0.000, 2.000],  loss: 2030747225601915904.000000, mse: 303452929740804915200.000000, mean_q: 15781174898.938776, mean_eps: 0.171847
 138166/150000: episode: 1156, duration: 0.814s, episode steps:  91, steps per second: 112, episode reward: -434.101, mean reward: -4.770 [-100.000,  0.841], mean action: 1.967 [0.000, 3.000],  loss: 2366378052930966528.000000, mse: 308511838499423059968.000000, mean_q: 15832996954.021978, mean_eps: 0.171280
 138417/150000: episode: 1157, duration: 2.175s, episode steps: 251, steps per second: 115, episode reward: -1771.308, mean reward: -7.057 [-100.000,  1.804], mean action: 1.936 [0.000, 3.000],  loss: 1723819495076170496.000000, mse: 326584394358303490048.000000, mean_q: 16167453577.689243, mean_eps: 0.170254
 138631/150000: episode: 1158, duration: 1.868s, episode steps: 214, steps per second: 115, episode reward: -854.341, mean reward: -3.992 [-100.000,  4.474], mean action: 1.907 [0.000, 3.000],  loss: 3611937903054875136.000000, mse: 328567554364673228800.000000, mean_q: 16234216916.934580, mean_eps: 0.168859
 138757/150000: episode: 1159, duration: 1.069s, episode steps: 126, steps per second: 118, episode reward: -590.219, mean reward: -4.684 [-100.000,  1.472], mean action: 1.960 [0.000, 3.000],  loss: 1292441413731608576.000000, mse: 301829557478407733248.000000, mean_q: 15744408957.968254, mean_eps: 0.167839
 138888/150000: episode: 1160, duration: 1.148s, episode steps: 131, steps per second: 114, episode reward: -705.007, mean reward: -5.382 [-100.000,  1.355], mean action: 1.908 [0.000, 3.000],  loss: 768741539579408896.000000, mse: 306205475866234454016.000000, mean_q: 15787842208.244274, mean_eps: 0.167068
 139040/150000: episode: 1161, duration: 1.297s, episode steps: 152, steps per second: 117, episode reward: -837.528, mean reward: -5.510 [-100.000,  2.173], mean action: 1.921 [0.000, 3.000],  loss: 1730587336503032064.000000, mse: 329381891592317632512.000000, mean_q: 16061638056.421053, mean_eps: 0.166219
 139257/150000: episode: 1162, duration: 1.869s, episode steps: 217, steps per second: 116, episode reward: -1345.683, mean reward: -6.201 [-100.000,  2.221], mean action: 1.876 [0.000, 3.000],  loss: 2129224702642618368.000000, mse: 313965277658892009472.000000, mean_q: 15963529801.142857, mean_eps: 0.165112
 139335/150000: episode: 1163, duration: 0.666s, episode steps:  78, steps per second: 117, episode reward: -495.972, mean reward: -6.359 [-100.000,  0.734], mean action: 1.885 [0.000, 3.000],  loss: 4875796928792894464.000000, mse: 324133085800789704704.000000, mean_q: 16198722704.410257, mean_eps: 0.164227
 139434/150000: episode: 1164, duration: 0.887s, episode steps:  99, steps per second: 112, episode reward: -383.306, mean reward: -3.872 [-100.000,  2.932], mean action: 1.960 [0.000, 3.000],  loss: 2199890614273449728.000000, mse: 301267281701726519296.000000, mean_q: 15956341925.494949, mean_eps: 0.163696
 139586/150000: episode: 1165, duration: 1.286s, episode steps: 152, steps per second: 118, episode reward: -790.763, mean reward: -5.202 [-100.000,  2.386], mean action: 1.875 [0.000, 3.000],  loss: 2398138866026690048.000000, mse: 335444586034556239872.000000, mean_q: 16141088518.736841, mean_eps: 0.162943
 139746/150000: episode: 1166, duration: 1.385s, episode steps: 160, steps per second: 116, episode reward: -1023.318, mean reward: -6.396 [-100.000,  1.969], mean action: 1.900 [0.000, 3.000],  loss: 2144920950274418176.000000, mse: 328638876969098149888.000000, mean_q: 16299539475.200001, mean_eps: 0.162007
 139844/150000: episode: 1167, duration: 0.844s, episode steps:  98, steps per second: 116, episode reward: -633.465, mean reward: -6.464 [-100.000,  0.293], mean action: 1.898 [0.000, 3.000],  loss: 1501545253178484480.000000, mse: 336387077593675268096.000000, mean_q: 16304087301.224489, mean_eps: 0.161233
 139992/150000: episode: 1168, duration: 1.272s, episode steps: 148, steps per second: 116, episode reward: -952.290, mean reward: -6.434 [-100.000,  1.375], mean action: 1.905 [0.000, 3.000],  loss: 1643130703851304704.000000, mse: 313365303965049487360.000000, mean_q: 16069054941.405405, mean_eps: 0.160495
 140078/150000: episode: 1169, duration: 0.738s, episode steps:  86, steps per second: 117, episode reward: -515.284, mean reward: -5.992 [-100.000,  0.458], mean action: 1.872 [0.000, 3.000],  loss: 1960273663977074688.000000, mse: 331490749925729894400.000000, mean_q: 16389336468.837210, mean_eps: 0.159793
 140175/150000: episode: 1170, duration: 0.849s, episode steps:  97, steps per second: 114, episode reward: -443.809, mean reward: -4.575 [-100.000,  2.151], mean action: 1.959 [0.000, 3.000],  loss: 1592276257084917248.000000, mse: 340063696584894709760.000000, mean_q: 16503105409.319588, mean_eps: 0.159244
 140259/150000: episode: 1171, duration: 0.723s, episode steps:  84, steps per second: 116, episode reward: -500.294, mean reward: -5.956 [-100.000, -0.439], mean action: 1.929 [0.000, 3.000],  loss: 1750009041679823104.000000, mse: 326781701439137120256.000000, mean_q: 16335694543.238094, mean_eps: 0.158701
 140352/150000: episode: 1172, duration: 0.808s, episode steps:  93, steps per second: 115, episode reward: -353.296, mean reward: -3.799 [-100.000,  1.441], mean action: 1.860 [0.000, 3.000],  loss: 1784374754570409472.000000, mse: 316613151718935363584.000000, mean_q: 16227062806.021505, mean_eps: 0.158170
 140451/150000: episode: 1173, duration: 0.848s, episode steps:  99, steps per second: 117, episode reward: -477.269, mean reward: -4.821 [-100.000,  1.905], mean action: 1.788 [0.000, 3.000],  loss: 1066804454994095744.000000, mse: 322648819200294846464.000000, mean_q: 16164055691.636364, mean_eps: 0.157594
 140561/150000: episode: 1174, duration: 0.945s, episode steps: 110, steps per second: 116, episode reward: -552.297, mean reward: -5.021 [-100.000,  0.386], mean action: 1.909 [0.000, 3.000],  loss: 1372873185114961408.000000, mse: 332699584656645292032.000000, mean_q: 16435951997.672728, mean_eps: 0.156967
 140642/150000: episode: 1175, duration: 0.701s, episode steps:  81, steps per second: 115, episode reward: -524.820, mean reward: -6.479 [-100.000,  0.781], mean action: 1.852 [0.000, 3.000],  loss: 4145143513862936576.000000, mse: 335155127942577848320.000000, mean_q: 16464828276.938272, mean_eps: 0.156394
 140727/150000: episode: 1176, duration: 0.729s, episode steps:  85, steps per second: 117, episode reward: -424.005, mean reward: -4.988 [-100.000,  0.380], mean action: 1.741 [0.000, 3.000],  loss: 1837112257353881600.000000, mse: 306743210136019140608.000000, mean_q: 16200906944.752941, mean_eps: 0.155896
 140840/150000: episode: 1177, duration: 0.960s, episode steps: 113, steps per second: 118, episode reward: -616.529, mean reward: -5.456 [-100.000,  0.862], mean action: 1.973 [0.000, 3.000],  loss: 1884113629915927040.000000, mse: 314624323015903281152.000000, mean_q: 16102391363.964602, mean_eps: 0.155302
 140965/150000: episode: 1178, duration: 1.073s, episode steps: 125, steps per second: 116, episode reward: -722.239, mean reward: -5.778 [-100.000,  0.559], mean action: 1.928 [0.000, 3.000],  loss: 3286261829553136640.000000, mse: 331160945229120536576.000000, mean_q: 16317405290.496000, mean_eps: 0.154588
 141077/150000: episode: 1179, duration: 0.953s, episode steps: 112, steps per second: 118, episode reward: -668.419, mean reward: -5.968 [-100.000,  4.540], mean action: 1.920 [0.000, 3.000],  loss: 2015530715521894656.000000, mse: 317455707636373192704.000000, mean_q: 16039515181.714285, mean_eps: 0.153877
 141184/150000: episode: 1180, duration: 0.944s, episode steps: 107, steps per second: 113, episode reward: -731.250, mean reward: -6.834 [-100.000,  1.466], mean action: 1.963 [0.000, 3.000],  loss: 2697383129938912256.000000, mse: 320706314301395828736.000000, mean_q: 16089127342.654205, mean_eps: 0.153220
 141288/150000: episode: 1181, duration: 0.926s, episode steps: 104, steps per second: 112, episode reward: -533.280, mean reward: -5.128 [-100.000,  1.374], mean action: 1.856 [0.000, 3.000],  loss: 3332978444714316800.000000, mse: 324467877900658016256.000000, mean_q: 16146195692.307692, mean_eps: 0.152587
 141416/150000: episode: 1182, duration: 1.185s, episode steps: 128, steps per second: 108, episode reward: -669.952, mean reward: -5.234 [-100.000,  1.845], mean action: 1.898 [0.000, 3.000],  loss: 1586051065143361536.000000, mse: 314538556758324936704.000000, mean_q: 16215269288.000000, mean_eps: 0.151891
 141501/150000: episode: 1183, duration: 0.771s, episode steps:  85, steps per second: 110, episode reward: -606.322, mean reward: -7.133 [-100.000, -0.429], mean action: 1.918 [0.000, 2.000],  loss: 2502795191038206976.000000, mse: 357692078600104837120.000000, mean_q: 16709463690.541176, mean_eps: 0.151252
 141607/150000: episode: 1184, duration: 0.958s, episode steps: 106, steps per second: 111, episode reward: -592.280, mean reward: -5.588 [-100.000,  1.340], mean action: 1.962 [0.000, 3.000],  loss: 3775432203537684480.000000, mse: 328154533753221808128.000000, mean_q: 16288350536.452829, mean_eps: 0.150679
 141680/150000: episode: 1185, duration: 0.667s, episode steps:  73, steps per second: 110, episode reward: -480.602, mean reward: -6.584 [-100.000, -0.207], mean action: 2.000 [0.000, 3.000],  loss: 4686997515136954368.000000, mse: 316334849601422819328.000000, mean_q: 16008559980.712328, mean_eps: 0.150142
 141772/150000: episode: 1186, duration: 0.797s, episode steps:  92, steps per second: 115, episode reward: -663.768, mean reward: -7.215 [-100.000,  0.009], mean action: 1.967 [0.000, 3.000],  loss: 2609562974243281408.000000, mse: 313766755521329037312.000000, mean_q: 16004440442.434782, mean_eps: 0.149647
 141851/150000: episode: 1187, duration: 0.695s, episode steps:  79, steps per second: 114, episode reward: -576.262, mean reward: -7.294 [-100.000, -0.246], mean action: 1.911 [0.000, 3.000],  loss: 1809546646626271744.000000, mse: 320139098341714624512.000000, mean_q: 16243707696.607595, mean_eps: 0.149134
 141933/150000: episode: 1188, duration: 0.701s, episode steps:  82, steps per second: 117, episode reward: -560.639, mean reward: -6.837 [-100.000,  0.495], mean action: 1.951 [0.000, 3.000],  loss: 3124226438230039040.000000, mse: 302978708838946373632.000000, mean_q: 15916177720.195122, mean_eps: 0.148651
 142125/150000: episode: 1189, duration: 1.663s, episode steps: 192, steps per second: 115, episode reward: -1271.050, mean reward: -6.620 [-100.000,  1.463], mean action: 1.839 [0.000, 3.000],  loss: 2807799421650075648.000000, mse: 316785797676110512128.000000, mean_q: 16139769050.666666, mean_eps: 0.147829
 142231/150000: episode: 1190, duration: 0.901s, episode steps: 106, steps per second: 118, episode reward: -478.530, mean reward: -4.514 [-100.000,  3.034], mean action: 1.915 [0.000, 3.000],  loss: 1767019039867288064.000000, mse: 322121361594537213952.000000, mean_q: 16276214069.132076, mean_eps: 0.146935
 142406/150000: episode: 1191, duration: 1.517s, episode steps: 175, steps per second: 115, episode reward: -1048.585, mean reward: -5.992 [-100.000,  2.122], mean action: 1.954 [0.000, 3.000],  loss: 1294879063775875328.000000, mse: 320253955650238545920.000000, mean_q: 16280334897.737143, mean_eps: 0.146092
 142536/150000: episode: 1192, duration: 1.101s, episode steps: 130, steps per second: 118, episode reward: -784.374, mean reward: -6.034 [-100.000,  1.374], mean action: 1.892 [0.000, 3.000],  loss: 3208524824778907648.000000, mse: 331269117740694306816.000000, mean_q: 16321990112.492308, mean_eps: 0.145177
 142693/150000: episode: 1193, duration: 1.361s, episode steps: 157, steps per second: 115, episode reward: -786.442, mean reward: -5.009 [-100.000,  3.575], mean action: 1.936 [0.000, 3.000],  loss: 2692784759389552640.000000, mse: 315013338825597845504.000000, mean_q: 16201207834.089172, mean_eps: 0.144316
 142783/150000: episode: 1194, duration: 0.757s, episode steps:  90, steps per second: 119, episode reward: -555.396, mean reward: -6.171 [-100.000,  0.838], mean action: 1.911 [0.000, 3.000],  loss: 2195463872618383872.000000, mse: 317528690910995808256.000000, mean_q: 16390979117.511110, mean_eps: 0.143575
 142866/150000: episode: 1195, duration: 0.747s, episode steps:  83, steps per second: 111, episode reward: -497.187, mean reward: -5.990 [-100.000,  0.448], mean action: 2.012 [1.000, 3.000],  loss: 2950577395447201280.000000, mse: 313201886955250909184.000000, mean_q: 16094757468.530121, mean_eps: 0.143056
 142949/150000: episode: 1196, duration: 0.709s, episode steps:  83, steps per second: 117, episode reward: -457.137, mean reward: -5.508 [-100.000,  0.814], mean action: 1.928 [0.000, 3.000],  loss: 2140269718613685504.000000, mse: 316085872572144746496.000000, mean_q: 16223612903.325302, mean_eps: 0.142558
 143086/150000: episode: 1197, duration: 1.168s, episode steps: 137, steps per second: 117, episode reward: -983.879, mean reward: -7.182 [-100.000,  1.369], mean action: 1.920 [0.000, 3.000],  loss: 2053785987974061312.000000, mse: 336014216412383870976.000000, mean_q: 16479108529.518248, mean_eps: 0.141898
 143272/150000: episode: 1198, duration: 1.656s, episode steps: 186, steps per second: 112, episode reward: -1037.079, mean reward: -5.576 [-100.000,  2.125], mean action: 1.855 [0.000, 3.000],  loss: 3136206496451580416.000000, mse: 320395112308232290304.000000, mean_q: 16209477202.580645, mean_eps: 0.140929
 143452/150000: episode: 1199, duration: 1.573s, episode steps: 180, steps per second: 114, episode reward: -1057.268, mean reward: -5.874 [-100.000,  2.029], mean action: 1.900 [0.000, 3.000],  loss: 2252002282935401472.000000, mse: 310761076335560097792.000000, mean_q: 16055379928.177778, mean_eps: 0.139831
 143560/150000: episode: 1200, duration: 0.929s, episode steps: 108, steps per second: 116, episode reward: -589.469, mean reward: -5.458 [-100.000,  1.418], mean action: 1.935 [0.000, 3.000],  loss: 3961875099013834752.000000, mse: 313048902502905151488.000000, mean_q: 16066108842.666666, mean_eps: 0.138967
 143657/150000: episode: 1201, duration: 0.858s, episode steps:  97, steps per second: 113, episode reward: -506.410, mean reward: -5.221 [-100.000,  1.103], mean action: 1.928 [0.000, 3.000],  loss: 1670444066786687232.000000, mse: 308916535502856781824.000000, mean_q: 16187350406.597939, mean_eps: 0.138352
 143777/150000: episode: 1202, duration: 1.020s, episode steps: 120, steps per second: 118, episode reward: -750.220, mean reward: -6.252 [-100.000,  1.194], mean action: 1.942 [0.000, 3.000],  loss: 2018375009343862528.000000, mse: 310123263665168908288.000000, mean_q: 16225143483.733334, mean_eps: 0.137701
 143934/150000: episode: 1203, duration: 1.363s, episode steps: 157, steps per second: 115, episode reward: -1007.111, mean reward: -6.415 [-100.000,  1.441], mean action: 1.962 [0.000, 3.000],  loss: 2875371853945742336.000000, mse: 320156076354375254016.000000, mean_q: 16294077798.726114, mean_eps: 0.136870
 144013/150000: episode: 1204, duration: 0.675s, episode steps:  79, steps per second: 117, episode reward: -488.842, mean reward: -6.188 [-100.000,  0.551], mean action: 1.835 [0.000, 2.000],  loss: 1361381965888467456.000000, mse: 325575916674702573568.000000, mean_q: 16446084044.151899, mean_eps: 0.136162
 144109/150000: episode: 1205, duration: 0.898s, episode steps:  96, steps per second: 107, episode reward: -597.147, mean reward: -6.220 [-100.000,  0.226], mean action: 1.979 [0.000, 3.000],  loss: 2067687072751157248.000000, mse: 308314945416078884864.000000, mean_q: 16100014250.666666, mean_eps: 0.135637
 144233/150000: episode: 1206, duration: 1.200s, episode steps: 124, steps per second: 103, episode reward: -806.719, mean reward: -6.506 [-100.000,  1.934], mean action: 1.952 [0.000, 3.000],  loss: 3392558254429619712.000000, mse: 322097268860187574272.000000, mean_q: 16267901902.451612, mean_eps: 0.134977
 144325/150000: episode: 1207, duration: 0.878s, episode steps:  92, steps per second: 105, episode reward: -553.695, mean reward: -6.018 [-100.000,  1.619], mean action: 1.902 [0.000, 3.000],  loss: 3530575423774153216.000000, mse: 319721017709428670464.000000, mean_q: 16426812816.695652, mean_eps: 0.134329
 144399/150000: episode: 1208, duration: 0.699s, episode steps:  74, steps per second: 106, episode reward: -443.954, mean reward: -5.999 [-100.000,  0.991], mean action: 1.932 [0.000, 3.000],  loss: 3937245787363241472.000000, mse: 319191635574678487040.000000, mean_q: 16149418426.810810, mean_eps: 0.133831
 144480/150000: episode: 1209, duration: 0.752s, episode steps:  81, steps per second: 108, episode reward: -500.319, mean reward: -6.177 [-100.000,  1.183], mean action: 1.975 [1.000, 3.000],  loss: 2656151192386886656.000000, mse: 332485502199401218048.000000, mean_q: 16533485770.271605, mean_eps: 0.133366
 144554/150000: episode: 1210, duration: 0.724s, episode steps:  74, steps per second: 102, episode reward: -512.441, mean reward: -6.925 [-100.000,  0.166], mean action: 1.986 [0.000, 3.000],  loss: 1180943416302395648.000000, mse: 298051884415934595072.000000, mean_q: 15906351173.189190, mean_eps: 0.132901
 144632/150000: episode: 1211, duration: 0.721s, episode steps:  78, steps per second: 108, episode reward: -556.058, mean reward: -7.129 [-100.000, -0.423], mean action: 1.949 [0.000, 3.000],  loss: 2454855323694627840.000000, mse: 318136203796282867712.000000, mean_q: 16201681211.076923, mean_eps: 0.132445
 144714/150000: episode: 1212, duration: 0.717s, episode steps:  82, steps per second: 114, episode reward: -452.065, mean reward: -5.513 [-100.000,  1.857], mean action: 1.927 [0.000, 3.000],  loss: 1682058553036879104.000000, mse: 330280929753359384576.000000, mean_q: 16389727893.853659, mean_eps: 0.131965
 144800/150000: episode: 1213, duration: 0.776s, episode steps:  86, steps per second: 111, episode reward: -506.255, mean reward: -5.887 [-100.000,  1.778], mean action: 1.930 [0.000, 3.000],  loss: 1830053388054301696.000000, mse: 326175935119712911360.000000, mean_q: 16292985867.906977, mean_eps: 0.131461
 145115/150000: episode: 1214, duration: 2.778s, episode steps: 315, steps per second: 113, episode reward: -2548.814, mean reward: -8.091 [-100.000,  1.905], mean action: 1.902 [0.000, 3.000],  loss: 3029296979066439168.000000, mse: 301738346936334024704.000000, mean_q: 15988962915.149206, mean_eps: 0.130258
 145198/150000: episode: 1215, duration: 0.706s, episode steps:  83, steps per second: 117, episode reward: -430.942, mean reward: -5.192 [-100.000,  0.613], mean action: 1.988 [0.000, 3.000],  loss: 3106353957534047744.000000, mse: 332065773553180934144.000000, mean_q: 16479782763.951807, mean_eps: 0.129064
 145303/150000: episode: 1216, duration: 0.922s, episode steps: 105, steps per second: 114, episode reward: -551.122, mean reward: -5.249 [-100.000,  0.765], mean action: 1.962 [0.000, 3.000],  loss: 1858233055420195584.000000, mse: 306674840110565294080.000000, mean_q: 16039305947.428572, mean_eps: 0.128500
 145469/150000: episode: 1217, duration: 1.416s, episode steps: 166, steps per second: 117, episode reward: -689.643, mean reward: -4.154 [-100.000,  4.706], mean action: 1.867 [0.000, 3.000],  loss: 3531282202365229056.000000, mse: 332010170587807875072.000000, mean_q: 16342681322.409639, mean_eps: 0.127687
 145614/150000: episode: 1218, duration: 1.246s, episode steps: 145, steps per second: 116, episode reward: -991.545, mean reward: -6.838 [-100.000,  1.332], mean action: 1.945 [0.000, 3.000],  loss: 3501770072976452608.000000, mse: 320007195065600180224.000000, mean_q: 16261530362.703447, mean_eps: 0.126754
 145699/150000: episode: 1219, duration: 0.724s, episode steps:  85, steps per second: 117, episode reward: -665.133, mean reward: -7.825 [-100.000,  0.230], mean action: 1.882 [0.000, 2.000],  loss: 1728491354585260800.000000, mse: 303913581725823795200.000000, mean_q: 16047502769.694118, mean_eps: 0.126064
 145842/150000: episode: 1220, duration: 1.233s, episode steps: 143, steps per second: 116, episode reward: -847.019, mean reward: -5.923 [-100.000,  1.967], mean action: 1.958 [0.000, 3.000],  loss: 1574594149109444864.000000, mse: 309226691408186048512.000000, mean_q: 16212039529.622377, mean_eps: 0.125380
 145918/150000: episode: 1221, duration: 0.641s, episode steps:  76, steps per second: 119, episode reward: -486.329, mean reward: -6.399 [-100.000, -0.515], mean action: 1.895 [0.000, 3.000],  loss: 4231659648146141696.000000, mse: 318625457298715246592.000000, mean_q: 16057643722.105263, mean_eps: 0.124723
 146015/150000: episode: 1222, duration: 0.849s, episode steps:  97, steps per second: 114, episode reward: -568.271, mean reward: -5.858 [-100.000,  1.618], mean action: 1.887 [0.000, 3.000],  loss: 1404652624877428992.000000, mse: 323076984978912444416.000000, mean_q: 16125566025.896908, mean_eps: 0.124204
 146147/150000: episode: 1223, duration: 1.150s, episode steps: 132, steps per second: 115, episode reward: -821.657, mean reward: -6.225 [-100.000,  1.781], mean action: 1.894 [0.000, 3.000],  loss: 2541676472979979264.000000, mse: 331662280251499282432.000000, mean_q: 16444682899.393940, mean_eps: 0.123517
 146223/150000: episode: 1224, duration: 0.667s, episode steps:  76, steps per second: 114, episode reward: -445.495, mean reward: -5.862 [-100.000,  0.260], mean action: 1.908 [0.000, 3.000],  loss: 835343610154168064.000000, mse: 334825297973363212288.000000, mean_q: 16435220102.736841, mean_eps: 0.122893
 146307/150000: episode: 1225, duration: 0.767s, episode steps:  84, steps per second: 109, episode reward: -507.138, mean reward: -6.037 [-100.000,  0.662], mean action: 1.905 [0.000, 3.000],  loss: 4478652602605283840.000000, mse: 305354598109791453184.000000, mean_q: 16011688533.333334, mean_eps: 0.122413
 146434/150000: episode: 1226, duration: 1.224s, episode steps: 127, steps per second: 104, episode reward: -480.930, mean reward: -3.787 [-100.000,  4.305], mean action: 1.906 [0.000, 3.000],  loss: 3405023613968239104.000000, mse: 294900030723902406656.000000, mean_q: 15889543087.370079, mean_eps: 0.121780
 146517/150000: episode: 1227, duration: 1.009s, episode steps:  83, steps per second:  82, episode reward: -513.992, mean reward: -6.193 [-100.000,  1.407], mean action: 1.904 [0.000, 3.000],  loss: 3457729583740180992.000000, mse: 331748848837912428544.000000, mean_q: 16335949663.614458, mean_eps: 0.121150
 146634/150000: episode: 1228, duration: 1.389s, episode steps: 117, steps per second:  84, episode reward: -660.383, mean reward: -5.644 [-100.000,  1.459], mean action: 1.957 [0.000, 3.000],  loss: 1231082541900755712.000000, mse: 301483989152338477056.000000, mean_q: 15937801259.760683, mean_eps: 0.120550
 146767/150000: episode: 1229, duration: 1.248s, episode steps: 133, steps per second: 107, episode reward: -892.142, mean reward: -6.708 [-100.000,  0.798], mean action: 1.985 [0.000, 3.000],  loss: 1660607446972268288.000000, mse: 310477307007843958784.000000, mean_q: 16052823163.187969, mean_eps: 0.119800
 146849/150000: episode: 1230, duration: 0.756s, episode steps:  82, steps per second: 108, episode reward: -439.364, mean reward: -5.358 [-100.000,  1.565], mean action: 1.963 [0.000, 3.000],  loss: 2551906180088393216.000000, mse: 309438052706412855296.000000, mean_q: 16233111576.975609, mean_eps: 0.119155
 146939/150000: episode: 1231, duration: 0.834s, episode steps:  90, steps per second: 108, episode reward: -554.977, mean reward: -6.166 [-100.000,  0.713], mean action: 1.944 [0.000, 3.000],  loss: 1959764764066502656.000000, mse: 314974211574620028928.000000, mean_q: 16184727028.622223, mean_eps: 0.118639
 147030/150000: episode: 1232, duration: 0.908s, episode steps:  91, steps per second: 100, episode reward: -394.990, mean reward: -4.341 [-100.000,  3.162], mean action: 1.934 [0.000, 3.000],  loss: 3156187417148472320.000000, mse: 317482496270580318208.000000, mean_q: 16289033508.571428, mean_eps: 0.118096
 147118/150000: episode: 1233, duration: 0.821s, episode steps:  88, steps per second: 107, episode reward: -383.322, mean reward: -4.356 [-100.000,  3.345], mean action: 1.886 [0.000, 3.000],  loss: 2096860209634392576.000000, mse: 315262367699307659264.000000, mean_q: 16225438731.636364, mean_eps: 0.117559
 147514/150000: episode: 1234, duration: 4.177s, episode steps: 396, steps per second:  95, episode reward: -4273.072, mean reward: -10.791 [-100.000,  1.793], mean action: 1.949 [0.000, 3.000],  loss: 2195719164515682560.000000, mse: 320200230572458704896.000000, mean_q: 16136306874.181818, mean_eps: 0.116107
 147634/150000: episode: 1235, duration: 1.239s, episode steps: 120, steps per second:  97, episode reward: -761.771, mean reward: -6.348 [-100.000,  1.049], mean action: 1.925 [0.000, 3.000],  loss: 3418358068002842112.000000, mse: 366237311352506155008.000000, mean_q: 16476373478.400000, mean_eps: 0.114559
 147722/150000: episode: 1236, duration: 0.767s, episode steps:  88, steps per second: 115, episode reward: -513.793, mean reward: -5.839 [-100.000,  0.419], mean action: 1.977 [0.000, 3.000],  loss: 2056493615710327040.000000, mse: 329402315930925596672.000000, mean_q: 16142512128.000000, mean_eps: 0.113935
 147828/150000: episode: 1237, duration: 1.018s, episode steps: 106, steps per second: 104, episode reward: -449.352, mean reward: -4.239 [-100.000,  2.040], mean action: 1.953 [0.000, 3.000],  loss: 9704011592245661696.000000, mse: 389513246832083664896.000000, mean_q: 16898230455.547171, mean_eps: 0.113353
 147930/150000: episode: 1238, duration: 0.888s, episode steps: 102, steps per second: 115, episode reward: -470.924, mean reward: -4.617 [-100.000,  2.784], mean action: 1.990 [0.000, 3.000],  loss: 3239369890443821056.000000, mse: 331183244594328764416.000000, mean_q: 16179405944.470589, mean_eps: 0.112729
 148027/150000: episode: 1239, duration: 0.856s, episode steps:  97, steps per second: 113, episode reward: -552.832, mean reward: -5.699 [-100.000,  0.818], mean action: 1.907 [0.000, 3.000],  loss: 3477197871080624640.000000, mse: 358882049472557547520.000000, mean_q: 16515597512.577320, mean_eps: 0.112132
 148390/150000: episode: 1240, duration: 3.436s, episode steps: 363, steps per second: 106, episode reward: -3280.328, mean reward: -9.037 [-100.000,  0.764], mean action: 1.909 [0.000, 3.000],  loss: 3039359556902131712.000000, mse: 348922172303707013120.000000, mean_q: 16382996437.685951, mean_eps: 0.110752
 148510/150000: episode: 1241, duration: 1.228s, episode steps: 120, steps per second:  98, episode reward: -821.689, mean reward: -6.847 [-100.000,  0.957], mean action: 1.958 [0.000, 3.000],  loss: 1515635490089057280.000000, mse: 382079592682178674688.000000, mean_q: 16611777092.266666, mean_eps: 0.109303
 148596/150000: episode: 1242, duration: 0.782s, episode steps:  86, steps per second: 110, episode reward: -408.527, mean reward: -4.750 [-100.000,  1.665], mean action: 2.000 [0.000, 3.000],  loss: 2949790516978604544.000000, mse: 371389841580005654528.000000, mean_q: 16687448873.674419, mean_eps: 0.108685
 148691/150000: episode: 1243, duration: 0.896s, episode steps:  95, steps per second: 106, episode reward: -390.880, mean reward: -4.115 [-100.000,  1.728], mean action: 1.926 [0.000, 3.000],  loss: 1344050234085508864.000000, mse: 371046985627296595968.000000, mean_q: 16588977701.726316, mean_eps: 0.108142
 148783/150000: episode: 1244, duration: 0.838s, episode steps:  92, steps per second: 110, episode reward: -560.253, mean reward: -6.090 [-100.000, -0.192], mean action: 2.000 [0.000, 3.000],  loss: 4857652088107582464.000000, mse: 354150461672680194048.000000, mean_q: 16493347083.130434, mean_eps: 0.107581
 148859/150000: episode: 1245, duration: 0.671s, episode steps:  76, steps per second: 113, episode reward: -464.807, mean reward: -6.116 [-100.000,  0.368], mean action: 1.855 [0.000, 3.000],  loss: 3303681610298659840.000000, mse: 347443117028907876352.000000, mean_q: 16280368640.000000, mean_eps: 0.107077
 148951/150000: episode: 1246, duration: 0.855s, episode steps:  92, steps per second: 108, episode reward: -529.468, mean reward: -5.755 [-100.000,  0.799], mean action: 1.946 [0.000, 3.000],  loss: 1951679924894683904.000000, mse: 364052732040200323072.000000, mean_q: 16676198900.869566, mean_eps: 0.106573
 149031/150000: episode: 1247, duration: 0.782s, episode steps:  80, steps per second: 102, episode reward: -502.232, mean reward: -6.278 [-100.000,  0.237], mean action: 1.900 [0.000, 3.000],  loss: 1437790383345814784.000000, mse: 345958044380732719104.000000, mean_q: 16176950361.600000, mean_eps: 0.106057
 149128/150000: episode: 1248, duration: 0.894s, episode steps:  97, steps per second: 108, episode reward: -448.991, mean reward: -4.629 [-100.000,  2.924], mean action: 1.979 [0.000, 3.000],  loss: 4932271413967740928.000000, mse: 364038306144058408960.000000, mean_q: 16573990700.865980, mean_eps: 0.105526
 149253/150000: episode: 1249, duration: 1.146s, episode steps: 125, steps per second: 109, episode reward: -769.948, mean reward: -6.160 [-100.000,  1.710], mean action: 1.888 [0.000, 3.000],  loss: 3333360301927341056.000000, mse: 349491954392231051264.000000, mean_q: 16465471668.224001, mean_eps: 0.104860
 149360/150000: episode: 1250, duration: 0.937s, episode steps: 107, steps per second: 114, episode reward: -512.271, mean reward: -4.788 [-100.000,  1.035], mean action: 1.944 [0.000, 3.000],  loss: 3116325842824832512.000000, mse: 377132568067450535936.000000, mean_q: 16561835888.448599, mean_eps: 0.104164
 149470/150000: episode: 1251, duration: 0.993s, episode steps: 110, steps per second: 111, episode reward: -636.855, mean reward: -5.790 [-100.000,  0.824], mean action: 1.900 [0.000, 3.000],  loss: 3209797721411656192.000000, mse: 347459855758244315136.000000, mean_q: 16545012047.127274, mean_eps: 0.103513
 149609/150000: episode: 1252, duration: 1.249s, episode steps: 139, steps per second: 111, episode reward: -780.445, mean reward: -5.615 [-100.000,  1.225], mean action: 2.029 [0.000, 3.000],  loss: 1834453748962103552.000000, mse: 347312965823352602624.000000, mean_q: 16374235275.971224, mean_eps: 0.102766
 149718/150000: episode: 1253, duration: 1.043s, episode steps: 109, steps per second: 104, episode reward: -611.195, mean reward: -5.607 [-100.000, -0.019], mean action: 1.945 [0.000, 3.000],  loss: 1501678424155868160.000000, mse: 365972512284665511936.000000, mean_q: 16724255199.119267, mean_eps: 0.102022
 149809/150000: episode: 1254, duration: 0.799s, episode steps:  91, steps per second: 114, episode reward: -281.638, mean reward: -3.095 [-100.000,  2.738], mean action: 1.923 [0.000, 3.000],  loss: 4942360939479652352.000000, mse: 364110147123219660800.000000, mean_q: 16404208786.285715, mean_eps: 0.101422
 149882/150000: episode: 1255, duration: 0.654s, episode steps:  73, steps per second: 112, episode reward: -485.849, mean reward: -6.655 [-100.000, -0.180], mean action: 1.945 [0.000, 3.000],  loss: 1494670029096377088.000000, mse: 338476330132998717440.000000, mean_q: 16336465218.630136, mean_eps: 0.100930
 149959/150000: episode: 1256, duration: 0.717s, episode steps:  77, steps per second: 107, episode reward: -523.400, mean reward: -6.797 [-100.000, -0.617], mean action: 1.883 [0.000, 3.000],  loss: 4801486141210806272.000000, mse: 358961239486203625472.000000, mean_q: 16472572435.948051, mean_eps: 0.100480
done, took 1294.856 seconds
Testing for 5 episodes ...
Episode 1: reward: -596.308, steps: 94
Episode 2: reward: -487.737, steps: 89
Episode 3: reward: -884.921, steps: 144
Episode 4: reward: -773.738, steps: 115
Episode 5: reward: -525.687, steps: 90
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_4 (Dense)              (None, 64)                576
_________________________________________________________________
activation_4 (Activation)    (None, 64)                0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_5 (Activation)    (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_6 (Activation)    (None, 32)                0
_________________________________________________________________
dense_7 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_7 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None