Training for 150000 steps ...
     57/150000: episode: 1, duration: 0.285s, episode steps:  57, steps per second: 200, episode reward: -90.955, mean reward: -1.596 [-100.000,  6.351], mean action: 1.421 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    118/150000: episode: 2, duration: 1.310s, episode steps:  61, steps per second:  47, episode reward: -84.969, mean reward: -1.393 [-100.000, 21.202], mean action: 1.508 [0.000, 3.000],  loss: 51.874045, mse: 25.942000, mean_q: 0.057868, mean_eps: 0.999346
    201/150000: episode: 3, duration: 0.722s, episode steps:  83, steps per second: 115, episode reward: -99.890, mean reward: -1.203 [-100.000, 17.389], mean action: 1.349 [0.000, 3.000],  loss: 57.193351, mse: 29.219210, mean_q: 1.160269, mean_eps: 0.999046
    321/150000: episode: 4, duration: 0.802s, episode steps: 120, steps per second: 150, episode reward: -184.456, mean reward: -1.537 [-100.000,  8.410], mean action: 1.475 [0.000, 3.000],  loss: 57.371696, mse: 32.542435, mean_q: 2.476016, mean_eps: 0.998437
    422/150000: episode: 5, duration: 0.707s, episode steps: 101, steps per second: 143, episode reward: -133.795, mean reward: -1.325 [-100.000, 15.542], mean action: 1.248 [0.000, 3.000],  loss: 32.623806, mse: 24.510601, mean_q: 1.931141, mean_eps: 0.997774
    515/150000: episode: 6, duration: 0.619s, episode steps:  93, steps per second: 150, episode reward: -356.109, mean reward: -3.829 [-100.000,  5.356], mean action: 1.419 [0.000, 3.000],  loss: 20.581798, mse: 27.833579, mean_q: 1.653467, mean_eps: 0.997192
    585/150000: episode: 7, duration: 0.474s, episode steps:  70, steps per second: 148, episode reward: -225.025, mean reward: -3.215 [-100.000, 15.567], mean action: 1.400 [0.000, 3.000],  loss: 16.127024, mse: 44.966772, mean_q: 0.614518, mean_eps: 0.996703
    668/150000: episode: 8, duration: 0.544s, episode steps:  83, steps per second: 153, episode reward: -222.850, mean reward: -2.685 [-100.000, 22.778], mean action: 1.301 [0.000, 3.000],  loss: 13.871601, mse: 53.397903, mean_q: 0.244923, mean_eps: 0.996244
    769/150000: episode: 9, duration: 0.734s, episode steps: 101, steps per second: 138, episode reward: -302.514, mean reward: -2.995 [-100.000,  3.528], mean action: 1.297 [0.000, 3.000],  loss: 22.573914, mse: 84.579554, mean_q: -0.050379, mean_eps: 0.995692
    852/150000: episode: 10, duration: 0.642s, episode steps:  83, steps per second: 129, episode reward: -196.773, mean reward: -2.371 [-100.000, 13.385], mean action: 1.614 [0.000, 3.000],  loss: 20.618735, mse: 64.480306, mean_q: -0.642231, mean_eps: 0.995140
    932/150000: episode: 11, duration: 0.572s, episode steps:  80, steps per second: 140, episode reward: -126.707, mean reward: -1.584 [-100.000,  7.037], mean action: 1.562 [0.000, 3.000],  loss: 20.569978, mse: 71.780142, mean_q: -0.564859, mean_eps: 0.994651
   1041/150000: episode: 12, duration: 0.789s, episode steps: 109, steps per second: 138, episode reward: -70.474, mean reward: -0.647 [-100.000, 14.274], mean action: 1.495 [0.000, 3.000],  loss: 19.641164, mse: 78.543579, mean_q: -0.438977, mean_eps: 0.994084
   1123/150000: episode: 13, duration: 0.602s, episode steps:  82, steps per second: 136, episode reward: -73.193, mean reward: -0.893 [-100.000, 16.240], mean action: 1.500 [0.000, 3.000],  loss: 12.501985, mse: 104.576485, mean_q: -0.480734, mean_eps: 0.993511
   1180/150000: episode: 14, duration: 0.536s, episode steps:  57, steps per second: 106, episode reward: -114.602, mean reward: -2.011 [-100.000,  8.422], mean action: 1.491 [0.000, 3.000],  loss: 7.690018, mse: 90.479338, mean_q: 0.001266, mean_eps: 0.993094
   1248/150000: episode: 15, duration: 0.741s, episode steps:  68, steps per second:  92, episode reward: -211.080, mean reward: -3.104 [-100.000,  7.257], mean action: 1.618 [0.000, 3.000],  loss: 9.837823, mse: 96.047286, mean_q: 0.075224, mean_eps: 0.992719
   1364/150000: episode: 16, duration: 1.147s, episode steps: 116, steps per second: 101, episode reward: -87.009, mean reward: -0.750 [-100.000,  8.480], mean action: 1.431 [0.000, 3.000],  loss: 13.885158, mse: 104.778943, mean_q: -0.156357, mean_eps: 0.992167
   1463/150000: episode: 17, duration: 1.090s, episode steps:  99, steps per second:  91, episode reward: -359.276, mean reward: -3.629 [-100.000,  0.437], mean action: 1.465 [0.000, 3.000],  loss: 11.851276, mse: 97.189662, mean_q: 0.036728, mean_eps: 0.991522
   1568/150000: episode: 18, duration: 1.257s, episode steps: 105, steps per second:  84, episode reward: 25.757, mean reward:  0.245 [-100.000, 88.789], mean action: 1.714 [0.000, 3.000],  loss: 16.310249, mse: 81.674518, mean_q: 0.086078, mean_eps: 0.990910
   1709/150000: episode: 19, duration: 1.492s, episode steps: 141, steps per second:  95, episode reward: -44.347, mean reward: -0.315 [-100.000, 88.067], mean action: 1.418 [0.000, 3.000],  loss: 21.206826, mse: 86.859028, mean_q: 0.286074, mean_eps: 0.990172
   1799/150000: episode: 20, duration: 0.920s, episode steps:  90, steps per second:  98, episode reward: -295.615, mean reward: -3.285 [-100.000, 29.563], mean action: 1.456 [0.000, 3.000],  loss: 24.090891, mse: 90.302517, mean_q: 0.057578, mean_eps: 0.989479
   1917/150000: episode: 21, duration: 1.124s, episode steps: 118, steps per second: 105, episode reward: -155.520, mean reward: -1.318 [-100.000,  8.240], mean action: 1.441 [0.000, 3.000],  loss: 24.691209, mse: 90.899107, mean_q: -0.052739, mean_eps: 0.988855
   2006/150000: episode: 22, duration: 0.628s, episode steps:  89, steps per second: 142, episode reward: -76.973, mean reward: -0.865 [-100.000,  9.887], mean action: 1.483 [0.000, 3.000],  loss: 19.203295, mse: 94.372964, mean_q: -0.134045, mean_eps: 0.988234
   2133/150000: episode: 23, duration: 0.841s, episode steps: 127, steps per second: 151, episode reward: -180.965, mean reward: -1.425 [-100.000, 28.374], mean action: 1.598 [0.000, 3.000],  loss: 19.830568, mse: 110.581899, mean_q: 0.148648, mean_eps: 0.987586
   2225/150000: episode: 24, duration: 0.718s, episode steps:  92, steps per second: 128, episode reward: -410.748, mean reward: -4.465 [-100.000,  0.040], mean action: 1.630 [0.000, 3.000],  loss: 19.802100, mse: 127.716659, mean_q: -0.059698, mean_eps: 0.986929
   2328/150000: episode: 25, duration: 0.707s, episode steps: 103, steps per second: 146, episode reward: -194.756, mean reward: -1.891 [-100.000, 35.155], mean action: 1.359 [0.000, 3.000],  loss: 25.109820, mse: 125.145375, mean_q: 0.024613, mean_eps: 0.986344
   2423/150000: episode: 26, duration: 0.607s, episode steps:  95, steps per second: 156, episode reward: -95.492, mean reward: -1.005 [-100.000, 21.066], mean action: 1.389 [0.000, 3.000],  loss: 18.999315, mse: 104.936732, mean_q: 0.255518, mean_eps: 0.985750
   2514/150000: episode: 27, duration: 0.655s, episode steps:  91, steps per second: 139, episode reward: -122.263, mean reward: -1.344 [-100.000, 20.764], mean action: 1.495 [0.000, 3.000],  loss: 15.006702, mse: 117.929488, mean_q: 0.001990, mean_eps: 0.985192
   2610/150000: episode: 28, duration: 0.633s, episode steps:  96, steps per second: 152, episode reward: -329.457, mean reward: -3.432 [-100.000,  0.492], mean action: 1.385 [0.000, 3.000],  loss: 19.184384, mse: 111.527271, mean_q: 0.046675, mean_eps: 0.984631
   2731/150000: episode: 29, duration: 0.745s, episode steps: 121, steps per second: 162, episode reward: -188.005, mean reward: -1.554 [-100.000,  1.705], mean action: 1.413 [0.000, 3.000],  loss: 22.096556, mse: 104.641173, mean_q: 0.129578, mean_eps: 0.983980
   2829/150000: episode: 30, duration: 0.658s, episode steps:  98, steps per second: 149, episode reward: -172.883, mean reward: -1.764 [-100.000, 13.629], mean action: 1.684 [0.000, 3.000],  loss: 18.264833, mse: 90.844918, mean_q: 0.449486, mean_eps: 0.983323
   2894/150000: episode: 31, duration: 0.429s, episode steps:  65, steps per second: 151, episode reward: -101.965, mean reward: -1.569 [-100.000,  5.982], mean action: 1.538 [0.000, 3.000],  loss: 15.060428, mse: 84.894822, mean_q: 0.388312, mean_eps: 0.982834
   2978/150000: episode: 32, duration: 0.536s, episode steps:  84, steps per second: 157, episode reward: -322.770, mean reward: -3.842 [-100.000,  4.079], mean action: 1.369 [0.000, 3.000],  loss: 14.432115, mse: 95.885260, mean_q: 0.123283, mean_eps: 0.982387
   3067/150000: episode: 33, duration: 0.599s, episode steps:  89, steps per second: 149, episode reward: -311.099, mean reward: -3.495 [-100.000,  3.767], mean action: 1.528 [0.000, 3.000],  loss: 17.405785, mse: 133.833763, mean_q: -0.282099, mean_eps: 0.981868
   3188/150000: episode: 34, duration: 0.852s, episode steps: 121, steps per second: 142, episode reward: -87.651, mean reward: -0.724 [-100.000,  8.210], mean action: 1.322 [0.000, 3.000],  loss: 20.120881, mse: 144.824209, mean_q: -0.266787, mean_eps: 0.981238
   3275/150000: episode: 35, duration: 0.542s, episode steps:  87, steps per second: 161, episode reward: -128.697, mean reward: -1.479 [-100.000, 11.417], mean action: 1.667 [0.000, 3.000],  loss: 15.821804, mse: 133.690426, mean_q: 0.068013, mean_eps: 0.980614
   3391/150000: episode: 36, duration: 0.740s, episode steps: 116, steps per second: 157, episode reward: -341.051, mean reward: -2.940 [-100.000, 18.133], mean action: 1.552 [0.000, 3.000],  loss: 18.393157, mse: 125.927583, mean_q: 0.045871, mean_eps: 0.980005
   3456/150000: episode: 37, duration: 0.458s, episode steps:  65, steps per second: 142, episode reward: -181.802, mean reward: -2.797 [-100.000, 32.198], mean action: 1.708 [0.000, 3.000],  loss: 19.346674, mse: 156.626113, mean_q: -0.287112, mean_eps: 0.979462
   3588/150000: episode: 38, duration: 0.824s, episode steps: 132, steps per second: 160, episode reward: -95.589, mean reward: -0.724 [-100.000, 12.456], mean action: 1.659 [0.000, 3.000],  loss: 21.765356, mse: 147.101553, mean_q: -0.082919, mean_eps: 0.978871
   3658/150000: episode: 39, duration: 0.520s, episode steps:  70, steps per second: 135, episode reward: -95.964, mean reward: -1.371 [-100.000,  9.556], mean action: 1.314 [0.000, 3.000],  loss: 15.032457, mse: 125.102615, mean_q: 0.324302, mean_eps: 0.978265
   3714/150000: episode: 40, duration: 0.396s, episode steps:  56, steps per second: 141, episode reward: -106.959, mean reward: -1.910 [-100.000, 11.574], mean action: 1.357 [0.000, 3.000],  loss: 14.734579, mse: 131.192351, mean_q: 0.284004, mean_eps: 0.977887
   3799/150000: episode: 41, duration: 0.665s, episode steps:  85, steps per second: 128, episode reward: -226.980, mean reward: -2.670 [-100.000, 71.907], mean action: 1.388 [0.000, 3.000],  loss: 17.292211, mse: 133.302690, mean_q: 0.082325, mean_eps: 0.977464
   3869/150000: episode: 42, duration: 0.491s, episode steps:  70, steps per second: 143, episode reward: -73.492, mean reward: -1.050 [-100.000,  8.350], mean action: 1.343 [0.000, 3.000],  loss: 14.958752, mse: 145.934741, mean_q: 0.194049, mean_eps: 0.976999
   3942/150000: episode: 43, duration: 0.525s, episode steps:  73, steps per second: 139, episode reward: -133.994, mean reward: -1.836 [-100.000,  6.920], mean action: 1.356 [0.000, 3.000],  loss: 26.751895, mse: 168.033069, mean_q: -0.355527, mean_eps: 0.976570
   4024/150000: episode: 44, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -117.990, mean reward: -1.439 [-100.000,  8.731], mean action: 1.598 [0.000, 3.000],  loss: 20.850154, mse: 141.333492, mean_q: 0.476323, mean_eps: 0.976105
   4111/150000: episode: 45, duration: 0.661s, episode steps:  87, steps per second: 132, episode reward: -115.880, mean reward: -1.332 [-100.000, 14.842], mean action: 1.517 [0.000, 3.000],  loss: 19.786972, mse: 194.205910, mean_q: 0.621060, mean_eps: 0.975598
   4213/150000: episode: 46, duration: 0.794s, episode steps: 102, steps per second: 128, episode reward: -393.192, mean reward: -3.855 [-100.000,  3.988], mean action: 1.245 [0.000, 3.000],  loss: 15.437135, mse: 186.722277, mean_q: 0.640599, mean_eps: 0.975031
   4305/150000: episode: 47, duration: 0.612s, episode steps:  92, steps per second: 150, episode reward: -53.391, mean reward: -0.580 [-100.000, 57.515], mean action: 1.565 [0.000, 3.000],  loss: 19.069647, mse: 201.575355, mean_q: 0.228641, mean_eps: 0.974449
   4381/150000: episode: 48, duration: 0.539s, episode steps:  76, steps per second: 141, episode reward: -168.079, mean reward: -2.212 [-100.000,  4.794], mean action: 1.395 [0.000, 3.000],  loss: 14.516202, mse: 174.694253, mean_q: 0.727307, mean_eps: 0.973945
   4484/150000: episode: 49, duration: 0.669s, episode steps: 103, steps per second: 154, episode reward: -207.237, mean reward: -2.012 [-100.000,  1.045], mean action: 1.544 [0.000, 3.000],  loss: 17.216588, mse: 200.001623, mean_q: 0.152813, mean_eps: 0.973408
   4584/150000: episode: 50, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: -95.190, mean reward: -0.952 [-100.000, 17.708], mean action: 1.260 [0.000, 3.000],  loss: 15.417382, mse: 206.788149, mean_q: 0.411199, mean_eps: 0.972799
   4658/150000: episode: 51, duration: 0.539s, episode steps:  74, steps per second: 137, episode reward: -194.207, mean reward: -2.624 [-100.000, 24.540], mean action: 1.581 [0.000, 3.000],  loss: 14.866108, mse: 193.589574, mean_q: 0.541749, mean_eps: 0.972277
   4732/150000: episode: 52, duration: 0.558s, episode steps:  74, steps per second: 133, episode reward: -140.945, mean reward: -1.905 [-100.000, 17.883], mean action: 1.608 [0.000, 3.000],  loss: 21.189041, mse: 196.148730, mean_q: 0.205664, mean_eps: 0.971833
   4803/150000: episode: 53, duration: 0.485s, episode steps:  71, steps per second: 146, episode reward: -79.285, mean reward: -1.117 [-100.000, 12.824], mean action: 1.577 [0.000, 3.000],  loss: 15.148001, mse: 167.858300, mean_q: 0.944806, mean_eps: 0.971398
   4888/150000: episode: 54, duration: 0.595s, episode steps:  85, steps per second: 143, episode reward: -212.824, mean reward: -2.504 [-100.000,  4.832], mean action: 1.341 [0.000, 3.000],  loss: 14.023862, mse: 196.113887, mean_q: 0.461975, mean_eps: 0.970930
   4953/150000: episode: 55, duration: 0.466s, episode steps:  65, steps per second: 139, episode reward: -59.614, mean reward: -0.917 [-100.000, 24.476], mean action: 1.446 [0.000, 3.000],  loss: 20.509391, mse: 186.287923, mean_q: 0.491911, mean_eps: 0.970480
   5047/150000: episode: 56, duration: 0.622s, episode steps:  94, steps per second: 151, episode reward: -110.052, mean reward: -1.171 [-100.000,  7.035], mean action: 1.479 [0.000, 3.000],  loss: 13.213498, mse: 201.491285, mean_q: 0.452290, mean_eps: 0.970003
   5169/150000: episode: 57, duration: 0.817s, episode steps: 122, steps per second: 149, episode reward: -66.437, mean reward: -0.545 [-100.000, 11.941], mean action: 1.279 [0.000, 3.000],  loss: 12.416266, mse: 225.697970, mean_q: 0.253021, mean_eps: 0.969355
   5250/150000: episode: 58, duration: 0.553s, episode steps:  81, steps per second: 147, episode reward: 22.361, mean reward:  0.276 [-100.000, 116.681], mean action: 1.457 [0.000, 3.000],  loss: 18.138996, mse: 250.124542, mean_q: -0.035254, mean_eps: 0.968746
   5321/150000: episode: 59, duration: 0.515s, episode steps:  71, steps per second: 138, episode reward: -218.645, mean reward: -3.080 [-100.000, 60.964], mean action: 1.423 [0.000, 3.000],  loss: 17.998899, mse: 250.000812, mean_q: 0.168516, mean_eps: 0.968290
   5411/150000: episode: 60, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -118.956, mean reward: -1.322 [-100.000,  6.319], mean action: 1.511 [0.000, 3.000],  loss: 20.966258, mse: 250.077189, mean_q: -0.262126, mean_eps: 0.967807
   5515/150000: episode: 61, duration: 0.731s, episode steps: 104, steps per second: 142, episode reward: -94.746, mean reward: -0.911 [-100.000,  9.555], mean action: 1.510 [0.000, 3.000],  loss: 15.807337, mse: 230.921868, mean_q: 0.350757, mean_eps: 0.967225
   5600/150000: episode: 62, duration: 0.609s, episode steps:  85, steps per second: 140, episode reward: -288.070, mean reward: -3.389 [-100.000,  2.195], mean action: 1.365 [0.000, 3.000],  loss: 21.205572, mse: 267.872874, mean_q: -0.259812, mean_eps: 0.966658
   5673/150000: episode: 63, duration: 0.498s, episode steps:  73, steps per second: 147, episode reward: -107.486, mean reward: -1.472 [-100.000,  6.713], mean action: 1.616 [0.000, 3.000],  loss: 12.505441, mse: 239.218347, mean_q: 0.249199, mean_eps: 0.966184
   5803/150000: episode: 64, duration: 0.917s, episode steps: 130, steps per second: 142, episode reward: 25.676, mean reward:  0.198 [-100.000, 91.221], mean action: 1.554 [0.000, 3.000],  loss: 13.498718, mse: 250.617960, mean_q: -0.003493, mean_eps: 0.965575
   5898/150000: episode: 65, duration: 0.630s, episode steps:  95, steps per second: 151, episode reward: -100.391, mean reward: -1.057 [-100.000,  7.622], mean action: 1.400 [0.000, 3.000],  loss: 13.012545, mse: 251.649186, mean_q: 0.094829, mean_eps: 0.964900
   5969/150000: episode: 66, duration: 0.476s, episode steps:  71, steps per second: 149, episode reward: -108.904, mean reward: -1.534 [-100.000,  5.716], mean action: 1.690 [0.000, 3.000],  loss: 20.846329, mse: 210.452667, mean_q: 1.045167, mean_eps: 0.964402
   6085/150000: episode: 67, duration: 0.789s, episode steps: 116, steps per second: 147, episode reward: -148.952, mean reward: -1.284 [-100.000,  7.535], mean action: 1.509 [0.000, 3.000],  loss: 17.342513, mse: 255.357690, mean_q: 0.220832, mean_eps: 0.963841
   6154/150000: episode: 68, duration: 0.485s, episode steps:  69, steps per second: 142, episode reward: -100.164, mean reward: -1.452 [-100.000,  7.635], mean action: 1.522 [0.000, 3.000],  loss: 13.086309, mse: 282.548523, mean_q: -0.113839, mean_eps: 0.963286
   6228/150000: episode: 69, duration: 0.650s, episode steps:  74, steps per second: 114, episode reward: -167.870, mean reward: -2.269 [-100.000, 21.365], mean action: 1.527 [0.000, 3.000],  loss: 15.709033, mse: 307.330863, mean_q: -0.230579, mean_eps: 0.962857
   6353/150000: episode: 70, duration: 0.835s, episode steps: 125, steps per second: 150, episode reward: -82.270, mean reward: -0.658 [-100.000, 38.532], mean action: 1.520 [0.000, 3.000],  loss: 19.101407, mse: 284.275537, mean_q: 0.423858, mean_eps: 0.962260
   6451/150000: episode: 71, duration: 0.625s, episode steps:  98, steps per second: 157, episode reward: -93.346, mean reward: -0.953 [-100.000,  7.399], mean action: 1.602 [0.000, 3.000],  loss: 12.928607, mse: 308.805258, mean_q: -0.140928, mean_eps: 0.961591
   6563/150000: episode: 72, duration: 0.791s, episode steps: 112, steps per second: 142, episode reward: -255.263, mean reward: -2.279 [-100.000,  6.377], mean action: 1.491 [0.000, 3.000],  loss: 19.086617, mse: 296.760632, mean_q: 0.329577, mean_eps: 0.960961
   6649/150000: episode: 73, duration: 0.561s, episode steps:  86, steps per second: 153, episode reward: -345.221, mean reward: -4.014 [-100.000,  0.378], mean action: 1.663 [0.000, 3.000],  loss: 16.426368, mse: 297.628578, mean_q: -0.144554, mean_eps: 0.960367
   6712/150000: episode: 74, duration: 0.406s, episode steps:  63, steps per second: 155, episode reward: -121.621, mean reward: -1.930 [-100.000, 19.717], mean action: 1.365 [0.000, 3.000],  loss: 21.405613, mse: 308.554496, mean_q: 0.013653, mean_eps: 0.959920
   6834/150000: episode: 75, duration: 0.861s, episode steps: 122, steps per second: 142, episode reward: -131.737, mean reward: -1.080 [-100.000, 16.042], mean action: 1.566 [0.000, 3.000],  loss: 16.907425, mse: 306.344348, mean_q: -0.086858, mean_eps: 0.959365
   6911/150000: episode: 76, duration: 0.633s, episode steps:  77, steps per second: 122, episode reward: -68.299, mean reward: -0.887 [-100.000, 11.590], mean action: 1.610 [0.000, 3.000],  loss: 12.923664, mse: 272.641417, mean_q: 0.853750, mean_eps: 0.958768
   6998/150000: episode: 77, duration: 0.706s, episode steps:  87, steps per second: 123, episode reward: -112.674, mean reward: -1.295 [-100.000, 11.638], mean action: 1.414 [0.000, 3.000],  loss: 18.092779, mse: 258.481718, mean_q: 0.761612, mean_eps: 0.958276
   7076/150000: episode: 78, duration: 0.637s, episode steps:  78, steps per second: 122, episode reward: -92.428, mean reward: -1.185 [-100.000,  8.663], mean action: 1.359 [0.000, 3.000],  loss: 16.372780, mse: 324.226514, mean_q: 0.594116, mean_eps: 0.957781
   7203/150000: episode: 79, duration: 0.913s, episode steps: 127, steps per second: 139, episode reward: -200.883, mean reward: -1.582 [-100.000, 23.499], mean action: 1.465 [0.000, 3.000],  loss: 12.846088, mse: 320.331449, mean_q: 0.553864, mean_eps: 0.957166
   7307/150000: episode: 80, duration: 0.868s, episode steps: 104, steps per second: 120, episode reward:  8.704, mean reward:  0.084 [-100.000, 93.519], mean action: 1.596 [0.000, 3.000],  loss: 16.776408, mse: 340.865167, mean_q: 0.437604, mean_eps: 0.956473
   7382/150000: episode: 81, duration: 0.601s, episode steps:  75, steps per second: 125, episode reward: -143.395, mean reward: -1.912 [-100.000, 25.905], mean action: 1.480 [0.000, 3.000],  loss: 21.933704, mse: 330.178000, mean_q: 1.069360, mean_eps: 0.955936
   7467/150000: episode: 82, duration: 0.606s, episode steps:  85, steps per second: 140, episode reward: -124.046, mean reward: -1.459 [-100.000,  7.620], mean action: 1.529 [0.000, 3.000],  loss: 12.713063, mse: 345.343120, mean_q: 0.228285, mean_eps: 0.955456
   7535/150000: episode: 83, duration: 0.432s, episode steps:  68, steps per second: 157, episode reward: -163.239, mean reward: -2.401 [-100.000, 19.761], mean action: 1.456 [0.000, 3.000],  loss: 24.946716, mse: 364.963144, mean_q: 0.067589, mean_eps: 0.954997
   7594/150000: episode: 84, duration: 0.422s, episode steps:  59, steps per second: 140, episode reward: -74.979, mean reward: -1.271 [-100.000,  7.742], mean action: 1.390 [0.000, 3.000],  loss: 15.915174, mse: 396.474645, mean_q: -0.359411, mean_eps: 0.954616
   7661/150000: episode: 85, duration: 0.530s, episode steps:  67, steps per second: 126, episode reward: -118.391, mean reward: -1.767 [-100.000, 37.685], mean action: 1.328 [0.000, 3.000],  loss: 14.014121, mse: 339.723139, mean_q: 0.641623, mean_eps: 0.954238
   7771/150000: episode: 86, duration: 0.998s, episode steps: 110, steps per second: 110, episode reward: -18.668, mean reward: -0.170 [-100.000, 104.011], mean action: 1.445 [0.000, 3.000],  loss: 13.241066, mse: 341.681668, mean_q: 0.222373, mean_eps: 0.953707
   7852/150000: episode: 87, duration: 0.768s, episode steps:  81, steps per second: 106, episode reward: -87.658, mean reward: -1.082 [-100.000, 10.934], mean action: 1.580 [0.000, 3.000],  loss: 15.374493, mse: 320.301406, mean_q: 0.835666, mean_eps: 0.953134
   7943/150000: episode: 88, duration: 0.744s, episode steps:  91, steps per second: 122, episode reward: -215.943, mean reward: -2.373 [-100.000, 29.814], mean action: 1.451 [0.000, 3.000],  loss: 12.632499, mse: 341.688424, mean_q: 0.259130, mean_eps: 0.952618
   8049/150000: episode: 89, duration: 0.864s, episode steps: 106, steps per second: 123, episode reward: -317.910, mean reward: -2.999 [-100.000,  0.732], mean action: 1.462 [0.000, 3.000],  loss: 16.346317, mse: 360.247679, mean_q: 0.510841, mean_eps: 0.952027
   8151/150000: episode: 90, duration: 0.759s, episode steps: 102, steps per second: 134, episode reward: -56.657, mean reward: -0.555 [-100.000, 17.407], mean action: 1.578 [0.000, 3.000],  loss: 18.069387, mse: 415.246927, mean_q: 0.218277, mean_eps: 0.951403
   8222/150000: episode: 91, duration: 0.525s, episode steps:  71, steps per second: 135, episode reward: -126.982, mean reward: -1.788 [-100.000, 39.221], mean action: 1.493 [0.000, 3.000],  loss: 14.256657, mse: 385.508974, mean_q: 0.107500, mean_eps: 0.950884
   8301/150000: episode: 92, duration: 0.570s, episode steps:  79, steps per second: 138, episode reward: -123.150, mean reward: -1.559 [-100.000, 19.100], mean action: 1.532 [0.000, 3.000],  loss: 16.674513, mse: 395.103128, mean_q: 0.195750, mean_eps: 0.950434
   8391/150000: episode: 93, duration: 0.605s, episode steps:  90, steps per second: 149, episode reward: -219.435, mean reward: -2.438 [-100.000, 21.239], mean action: 1.511 [0.000, 3.000],  loss: 16.224666, mse: 376.486067, mean_q: 0.887963, mean_eps: 0.949927
   8462/150000: episode: 94, duration: 0.576s, episode steps:  71, steps per second: 123, episode reward: -75.709, mean reward: -1.066 [-100.000, 13.400], mean action: 1.634 [0.000, 3.000],  loss: 14.414950, mse: 423.301715, mean_q: 0.085635, mean_eps: 0.949444
   8544/150000: episode: 95, duration: 0.757s, episode steps:  82, steps per second: 108, episode reward: -72.508, mean reward: -0.884 [-100.000, 19.005], mean action: 1.512 [0.000, 3.000],  loss: 14.020131, mse: 431.189830, mean_q: -0.308806, mean_eps: 0.948985
   8626/150000: episode: 96, duration: 0.884s, episode steps:  82, steps per second:  93, episode reward: -121.416, mean reward: -1.481 [-100.000,  6.110], mean action: 1.659 [0.000, 3.000],  loss: 16.912607, mse: 411.216137, mean_q: 0.074258, mean_eps: 0.948493
   8720/150000: episode: 97, duration: 1.057s, episode steps:  94, steps per second:  89, episode reward: -120.717, mean reward: -1.284 [-100.000,  6.098], mean action: 1.553 [0.000, 3.000],  loss: 14.724296, mse: 396.200844, mean_q: 0.631994, mean_eps: 0.947965
   8798/150000: episode: 98, duration: 0.919s, episode steps:  78, steps per second:  85, episode reward: -106.202, mean reward: -1.362 [-100.000,  7.485], mean action: 1.500 [0.000, 3.000],  loss: 13.005676, mse: 401.134206, mean_q: 0.383403, mean_eps: 0.947449
   8865/150000: episode: 99, duration: 1.396s, episode steps:  67, steps per second:  48, episode reward: -109.238, mean reward: -1.630 [-100.000, 38.578], mean action: 1.418 [0.000, 3.000],  loss: 14.171278, mse: 406.314312, mean_q: 0.501571, mean_eps: 0.947014
   8955/150000: episode: 100, duration: 0.908s, episode steps:  90, steps per second:  99, episode reward: -418.478, mean reward: -4.650 [-100.000,  0.029], mean action: 1.444 [0.000, 3.000],  loss: 15.277829, mse: 401.866801, mean_q: 0.414944, mean_eps: 0.946543
   9018/150000: episode: 101, duration: 0.519s, episode steps:  63, steps per second: 121, episode reward: -158.932, mean reward: -2.523 [-100.000,  8.705], mean action: 1.524 [0.000, 3.000],  loss: 13.607506, mse: 418.464912, mean_q: 0.514834, mean_eps: 0.946084
   9080/150000: episode: 102, duration: 0.622s, episode steps:  62, steps per second: 100, episode reward: -111.246, mean reward: -1.794 [-100.000,  5.687], mean action: 1.419 [0.000, 3.000],  loss: 20.495106, mse: 476.912144, mean_q: 0.239792, mean_eps: 0.945709
   9139/150000: episode: 103, duration: 0.490s, episode steps:  59, steps per second: 120, episode reward: -165.224, mean reward: -2.800 [-100.000,  6.756], mean action: 1.390 [0.000, 3.000],  loss: 20.646714, mse: 472.565723, mean_q: 0.467413, mean_eps: 0.945346
   9254/150000: episode: 104, duration: 0.877s, episode steps: 115, steps per second: 131, episode reward:  9.990, mean reward:  0.087 [-100.000, 90.708], mean action: 1.539 [0.000, 3.000],  loss: 13.879553, mse: 506.046884, mean_q: 0.122336, mean_eps: 0.944824
   9368/150000: episode: 105, duration: 0.992s, episode steps: 114, steps per second: 115, episode reward: -128.383, mean reward: -1.126 [-100.000, 10.729], mean action: 1.693 [0.000, 3.000],  loss: 12.591282, mse: 455.150272, mean_q: 0.721188, mean_eps: 0.944137
   9493/150000: episode: 106, duration: 0.820s, episode steps: 125, steps per second: 152, episode reward: -89.374, mean reward: -0.715 [-100.000, 13.051], mean action: 1.560 [0.000, 3.000],  loss: 17.441612, mse: 476.720864, mean_q: 0.623022, mean_eps: 0.943420
   9583/150000: episode: 107, duration: 0.583s, episode steps:  90, steps per second: 154, episode reward: -182.316, mean reward: -2.026 [-100.000,  7.488], mean action: 1.556 [0.000, 3.000],  loss: 18.615745, mse: 490.792360, mean_q: 0.790345, mean_eps: 0.942775
   9662/150000: episode: 108, duration: 0.539s, episode steps:  79, steps per second: 147, episode reward: -101.763, mean reward: -1.288 [-100.000, 21.991], mean action: 1.684 [0.000, 3.000],  loss: 18.146584, mse: 460.123487, mean_q: 0.742544, mean_eps: 0.942268
   9765/150000: episode: 109, duration: 0.819s, episode steps: 103, steps per second: 126, episode reward: -106.689, mean reward: -1.036 [-100.000,  6.039], mean action: 1.612 [0.000, 3.000],  loss: 13.074638, mse: 443.425608, mean_q: 1.002596, mean_eps: 0.941722
   9871/150000: episode: 110, duration: 0.829s, episode steps: 106, steps per second: 128, episode reward: -152.293, mean reward: -1.437 [-100.000, 11.562], mean action: 1.642 [0.000, 3.000],  loss: 13.545735, mse: 469.433108, mean_q: 0.926935, mean_eps: 0.941095
   9986/150000: episode: 111, duration: 0.892s, episode steps: 115, steps per second: 129, episode reward: -84.088, mean reward: -0.731 [-100.000,  7.248], mean action: 1.487 [0.000, 3.000],  loss: 17.838527, mse: 483.352796, mean_q: 0.560532, mean_eps: 0.940432
  10107/150000: episode: 112, duration: 0.838s, episode steps: 121, steps per second: 144, episode reward: -109.296, mean reward: -0.903 [-100.000,  6.316], mean action: 1.628 [0.000, 3.000],  loss: 11.798894, mse: 492.150985, mean_q: 1.127433, mean_eps: 0.939724
  10231/150000: episode: 113, duration: 0.919s, episode steps: 124, steps per second: 135, episode reward: -271.022, mean reward: -2.186 [-100.000,  8.794], mean action: 1.371 [0.000, 3.000],  loss: 13.413681, mse: 484.669811, mean_q: 1.669885, mean_eps: 0.938989
  10304/150000: episode: 114, duration: 0.494s, episode steps:  73, steps per second: 148, episode reward: -72.141, mean reward: -0.988 [-100.000,  6.776], mean action: 1.466 [0.000, 3.000],  loss: 22.917535, mse: 528.926245, mean_q: 0.904719, mean_eps: 0.938398
  10449/150000: episode: 115, duration: 1.024s, episode steps: 145, steps per second: 142, episode reward: -64.607, mean reward: -0.446 [-100.000, 67.192], mean action: 1.697 [0.000, 3.000],  loss: 12.432976, mse: 498.615275, mean_q: 1.366284, mean_eps: 0.937744
  10559/150000: episode: 116, duration: 0.724s, episode steps: 110, steps per second: 152, episode reward: -88.080, mean reward: -0.801 [-100.000, 20.042], mean action: 1.455 [0.000, 3.000],  loss: 11.840876, mse: 537.455376, mean_q: 1.073206, mean_eps: 0.936979
  10678/150000: episode: 117, duration: 0.790s, episode steps: 119, steps per second: 151, episode reward: -80.097, mean reward: -0.673 [-100.000, 18.377], mean action: 1.571 [0.000, 3.000],  loss: 17.284363, mse: 500.726749, mean_q: 1.271029, mean_eps: 0.936292
  10757/150000: episode: 118, duration: 0.510s, episode steps:  79, steps per second: 155, episode reward: -87.679, mean reward: -1.110 [-100.000,  7.122], mean action: 1.582 [0.000, 3.000],  loss: 14.257646, mse: 513.079477, mean_q: 1.280670, mean_eps: 0.935698
  10856/150000: episode: 119, duration: 0.637s, episode steps:  99, steps per second: 155, episode reward: -171.010, mean reward: -1.727 [-100.000,  8.077], mean action: 1.576 [0.000, 3.000],  loss: 17.484822, mse: 499.848852, mean_q: 1.700064, mean_eps: 0.935164
  10961/150000: episode: 120, duration: 0.728s, episode steps: 105, steps per second: 144, episode reward: -234.898, mean reward: -2.237 [-100.000,  1.043], mean action: 1.467 [0.000, 3.000],  loss: 15.919992, mse: 507.948009, mean_q: 1.480822, mean_eps: 0.934552
  11068/150000: episode: 121, duration: 0.715s, episode steps: 107, steps per second: 150, episode reward: -160.925, mean reward: -1.504 [-100.000, 15.443], mean action: 1.430 [0.000, 3.000],  loss: 12.740488, mse: 573.788525, mean_q: 0.929146, mean_eps: 0.933916
  11210/150000: episode: 122, duration: 0.919s, episode steps: 142, steps per second: 154, episode reward: -202.066, mean reward: -1.423 [-100.000, 27.758], mean action: 1.359 [0.000, 3.000],  loss: 15.896365, mse: 556.899640, mean_q: 1.758854, mean_eps: 0.933169
  11277/150000: episode: 123, duration: 0.449s, episode steps:  67, steps per second: 149, episode reward: -92.834, mean reward: -1.386 [-100.000,  6.836], mean action: 1.507 [0.000, 3.000],  loss: 15.738844, mse: 540.225079, mean_q: 1.936780, mean_eps: 0.932542
  11424/150000: episode: 124, duration: 0.911s, episode steps: 147, steps per second: 161, episode reward: -81.289, mean reward: -0.553 [-100.000,  9.101], mean action: 1.510 [0.000, 3.000],  loss: 12.682658, mse: 557.669492, mean_q: 1.805540, mean_eps: 0.931900
  11532/150000: episode: 125, duration: 0.712s, episode steps: 108, steps per second: 152, episode reward: -349.567, mean reward: -3.237 [-100.000, 13.494], mean action: 1.574 [0.000, 3.000],  loss: 11.776726, mse: 578.260607, mean_q: 1.615055, mean_eps: 0.931135
  11675/150000: episode: 126, duration: 0.951s, episode steps: 143, steps per second: 150, episode reward:  7.157, mean reward:  0.050 [-100.000, 76.651], mean action: 1.573 [0.000, 3.000],  loss: 14.890106, mse: 591.731532, mean_q: 1.429363, mean_eps: 0.930382
  11772/150000: episode: 127, duration: 0.704s, episode steps:  97, steps per second: 138, episode reward: -371.073, mean reward: -3.825 [-100.000, 103.310], mean action: 1.557 [0.000, 3.000],  loss: 17.591473, mse: 590.176905, mean_q: 1.206916, mean_eps: 0.929662
  11877/150000: episode: 128, duration: 0.768s, episode steps: 105, steps per second: 137, episode reward: -102.957, mean reward: -0.981 [-100.000,  9.890], mean action: 1.533 [0.000, 3.000],  loss: 14.916212, mse: 588.582457, mean_q: 1.354800, mean_eps: 0.929056
  11989/150000: episode: 129, duration: 1.134s, episode steps: 112, steps per second:  99, episode reward:  8.008, mean reward:  0.071 [-100.000, 102.185], mean action: 1.482 [0.000, 3.000],  loss: 26.984695, mse: 561.028912, mean_q: 2.052731, mean_eps: 0.928405
  12081/150000: episode: 130, duration: 1.701s, episode steps:  92, steps per second:  54, episode reward: -246.869, mean reward: -2.683 [-100.000,  0.988], mean action: 1.652 [0.000, 3.000],  loss: 21.078594, mse: 585.432436, mean_q: 2.209344, mean_eps: 0.927793
  12174/150000: episode: 131, duration: 1.358s, episode steps:  93, steps per second:  69, episode reward: -206.086, mean reward: -2.216 [-100.000, 20.926], mean action: 1.634 [0.000, 3.000],  loss: 19.556700, mse: 664.285401, mean_q: 1.239164, mean_eps: 0.927238
  12231/150000: episode: 132, duration: 0.818s, episode steps:  57, steps per second:  70, episode reward: -149.599, mean reward: -2.625 [-100.000,  7.593], mean action: 1.579 [0.000, 3.000],  loss: 17.577567, mse: 632.349055, mean_q: 2.066681, mean_eps: 0.926788
  12341/150000: episode: 133, duration: 1.300s, episode steps: 110, steps per second:  85, episode reward: -124.718, mean reward: -1.134 [-100.000, 13.141], mean action: 1.482 [0.000, 3.000],  loss: 18.261908, mse: 659.957929, mean_q: 1.481654, mean_eps: 0.926287
  12469/150000: episode: 134, duration: 1.262s, episode steps: 128, steps per second: 101, episode reward: -159.230, mean reward: -1.244 [-100.000,  4.300], mean action: 1.453 [0.000, 3.000],  loss: 21.822082, mse: 668.501083, mean_q: 1.562817, mean_eps: 0.925573
  12566/150000: episode: 135, duration: 0.843s, episode steps:  97, steps per second: 115, episode reward: -126.135, mean reward: -1.300 [-100.000, 13.286], mean action: 1.660 [0.000, 3.000],  loss: 14.926946, mse: 614.447414, mean_q: 1.982913, mean_eps: 0.924898
  12672/150000: episode: 136, duration: 0.964s, episode steps: 106, steps per second: 110, episode reward: -125.260, mean reward: -1.182 [-100.000, 10.682], mean action: 1.481 [0.000, 3.000],  loss: 12.534633, mse: 660.615426, mean_q: 1.930622, mean_eps: 0.924289
  12739/150000: episode: 137, duration: 0.641s, episode steps:  67, steps per second: 105, episode reward: -135.123, mean reward: -2.017 [-100.000, 16.032], mean action: 1.627 [0.000, 3.000],  loss: 22.952142, mse: 664.268213, mean_q: 2.314221, mean_eps: 0.923770
  12838/150000: episode: 138, duration: 1.017s, episode steps:  99, steps per second:  97, episode reward: -75.086, mean reward: -0.758 [-100.000, 11.082], mean action: 1.667 [0.000, 3.000],  loss: 21.345372, mse: 645.473810, mean_q: 1.908492, mean_eps: 0.923272
  12943/150000: episode: 139, duration: 1.018s, episode steps: 105, steps per second: 103, episode reward: -265.667, mean reward: -2.530 [-100.000,  2.842], mean action: 1.295 [0.000, 3.000],  loss: 17.382269, mse: 652.895492, mean_q: 2.034622, mean_eps: 0.922660
  13020/150000: episode: 140, duration: 0.809s, episode steps:  77, steps per second:  95, episode reward: -117.625, mean reward: -1.528 [-100.000,  7.984], mean action: 1.481 [0.000, 3.000],  loss: 13.907155, mse: 690.371560, mean_q: 1.565992, mean_eps: 0.922114
  13132/150000: episode: 141, duration: 0.906s, episode steps: 112, steps per second: 124, episode reward: -101.635, mean reward: -0.907 [-100.000,  8.207], mean action: 1.580 [0.000, 3.000],  loss: 19.985313, mse: 730.212768, mean_q: 1.868382, mean_eps: 0.921547
  13222/150000: episode: 142, duration: 0.662s, episode steps:  90, steps per second: 136, episode reward: -140.150, mean reward: -1.557 [-100.000, 16.607], mean action: 1.356 [0.000, 3.000],  loss: 20.535217, mse: 748.217688, mean_q: 1.028643, mean_eps: 0.920941
  13319/150000: episode: 143, duration: 0.706s, episode steps:  97, steps per second: 137, episode reward: -48.254, mean reward: -0.497 [-100.000, 49.085], mean action: 1.515 [0.000, 3.000],  loss: 17.669493, mse: 718.358856, mean_q: 2.159428, mean_eps: 0.920380
  13374/150000: episode: 144, duration: 0.454s, episode steps:  55, steps per second: 121, episode reward: -101.119, mean reward: -1.839 [-100.000, 35.863], mean action: 1.655 [0.000, 3.000],  loss: 17.424872, mse: 749.651863, mean_q: 1.527007, mean_eps: 0.919924
  13504/150000: episode: 145, duration: 0.919s, episode steps: 130, steps per second: 141, episode reward: -127.616, mean reward: -0.982 [-100.000, 21.646], mean action: 1.492 [0.000, 3.000],  loss: 20.997348, mse: 693.109520, mean_q: 2.575658, mean_eps: 0.919369
  13623/150000: episode: 146, duration: 1.042s, episode steps: 119, steps per second: 114, episode reward: -131.077, mean reward: -1.101 [-100.000, 11.169], mean action: 1.370 [0.000, 3.000],  loss: 17.749031, mse: 713.665972, mean_q: 2.511800, mean_eps: 0.918622
  13744/150000: episode: 147, duration: 0.892s, episode steps: 121, steps per second: 136, episode reward: -109.663, mean reward: -0.906 [-100.000,  6.852], mean action: 1.479 [0.000, 3.000],  loss: 18.132388, mse: 727.410192, mean_q: 1.953748, mean_eps: 0.917902
  13885/150000: episode: 148, duration: 1.064s, episode steps: 141, steps per second: 132, episode reward: -50.424, mean reward: -0.358 [-100.000, 12.657], mean action: 1.468 [0.000, 3.000],  loss: 17.055094, mse: 708.658652, mean_q: 2.141390, mean_eps: 0.917116
  13979/150000: episode: 149, duration: 0.729s, episode steps:  94, steps per second: 129, episode reward: -104.946, mean reward: -1.116 [-100.000, 12.273], mean action: 1.457 [0.000, 3.000],  loss: 16.084711, mse: 713.742550, mean_q: 2.110229, mean_eps: 0.916411
  14979/150000: episode: 150, duration: 10.440s, episode steps: 1000, steps per second:  96, episode reward: 47.252, mean reward:  0.047 [-25.507, 125.331], mean action: 1.523 [0.000, 3.000],  loss: 20.674680, mse: 782.760514, mean_q: 1.818536, mean_eps: 0.913129
  15070/150000: episode: 151, duration: 0.893s, episode steps:  91, steps per second: 102, episode reward: -96.290, mean reward: -1.058 [-100.000, 20.002], mean action: 1.637 [0.000, 3.000],  loss: 21.333582, mse: 820.145255, mean_q: 1.079017, mean_eps: 0.909856
  15149/150000: episode: 152, duration: 0.789s, episode steps:  79, steps per second: 100, episode reward: -191.317, mean reward: -2.422 [-100.000,  7.078], mean action: 1.354 [0.000, 3.000],  loss: 19.725228, mse: 821.857875, mean_q: 1.587027, mean_eps: 0.909346
  15260/150000: episode: 153, duration: 1.028s, episode steps: 111, steps per second: 108, episode reward: -172.603, mean reward: -1.555 [-100.000, 36.094], mean action: 1.423 [0.000, 3.000],  loss: 22.508638, mse: 853.900183, mean_q: 1.875358, mean_eps: 0.908776
  15326/150000: episode: 154, duration: 0.575s, episode steps:  66, steps per second: 115, episode reward: -60.609, mean reward: -0.918 [-100.000,  7.329], mean action: 1.606 [0.000, 3.000],  loss: 19.052603, mse: 778.460697, mean_q: 2.022854, mean_eps: 0.908245
  15444/150000: episode: 155, duration: 1.282s, episode steps: 118, steps per second:  92, episode reward: -119.641, mean reward: -1.014 [-100.000, 16.289], mean action: 1.347 [0.000, 3.000],  loss: 23.706458, mse: 845.709896, mean_q: 1.380599, mean_eps: 0.907693
  15543/150000: episode: 156, duration: 1.030s, episode steps:  99, steps per second:  96, episode reward: -120.679, mean reward: -1.219 [-100.000,  4.990], mean action: 1.657 [0.000, 3.000],  loss: 23.496090, mse: 827.142754, mean_q: 1.652848, mean_eps: 0.907042
  15653/150000: episode: 157, duration: 1.218s, episode steps: 110, steps per second:  90, episode reward: -78.214, mean reward: -0.711 [-100.000,  9.265], mean action: 1.655 [0.000, 3.000],  loss: 19.333350, mse: 816.215743, mean_q: 1.920228, mean_eps: 0.906415
  15749/150000: episode: 158, duration: 1.031s, episode steps:  96, steps per second:  93, episode reward: -53.384, mean reward: -0.556 [-100.000, 18.021], mean action: 1.604 [0.000, 3.000],  loss: 22.694639, mse: 809.347008, mean_q: 2.175082, mean_eps: 0.905797
  15902/150000: episode: 159, duration: 1.426s, episode steps: 153, steps per second: 107, episode reward: -60.236, mean reward: -0.394 [-100.000,  7.741], mean action: 1.523 [0.000, 3.000],  loss: 25.929647, mse: 800.727175, mean_q: 2.344296, mean_eps: 0.905050
  15992/150000: episode: 160, duration: 0.906s, episode steps:  90, steps per second:  99, episode reward: -144.177, mean reward: -1.602 [-100.000,  8.956], mean action: 1.589 [0.000, 3.000],  loss: 16.383004, mse: 801.683378, mean_q: 2.920669, mean_eps: 0.904321
  16085/150000: episode: 161, duration: 0.932s, episode steps:  93, steps per second: 100, episode reward: -212.289, mean reward: -2.283 [-100.000, 31.644], mean action: 1.505 [0.000, 3.000],  loss: 21.144757, mse: 805.054080, mean_q: 3.009211, mean_eps: 0.903772
  16149/150000: episode: 162, duration: 0.720s, episode steps:  64, steps per second:  89, episode reward: -63.857, mean reward: -0.998 [-100.000, 23.488], mean action: 1.375 [0.000, 3.000],  loss: 31.616562, mse: 788.627321, mean_q: 3.471753, mean_eps: 0.903301
  16246/150000: episode: 163, duration: 0.913s, episode steps:  97, steps per second: 106, episode reward: -283.207, mean reward: -2.920 [-100.000, 96.370], mean action: 1.536 [0.000, 3.000],  loss: 17.516957, mse: 806.152017, mean_q: 3.731345, mean_eps: 0.902818
  16320/150000: episode: 164, duration: 0.736s, episode steps:  74, steps per second: 101, episode reward: -131.242, mean reward: -1.774 [-100.000,  6.796], mean action: 1.432 [0.000, 3.000],  loss: 25.330912, mse: 828.012829, mean_q: 3.464697, mean_eps: 0.902305
  16390/150000: episode: 165, duration: 0.676s, episode steps:  70, steps per second: 103, episode reward: -67.732, mean reward: -0.968 [-100.000, 16.279], mean action: 1.514 [0.000, 3.000],  loss: 22.879445, mse: 843.099430, mean_q: 3.175888, mean_eps: 0.901873
  16456/150000: episode: 166, duration: 0.645s, episode steps:  66, steps per second: 102, episode reward: -70.405, mean reward: -1.067 [-100.000,  9.389], mean action: 1.576 [0.000, 3.000],  loss: 12.376279, mse: 840.885058, mean_q: 2.322233, mean_eps: 0.901465
  16533/150000: episode: 167, duration: 0.834s, episode steps:  77, steps per second:  92, episode reward: -174.590, mean reward: -2.267 [-100.000, 17.689], mean action: 1.675 [0.000, 3.000],  loss: 15.469856, mse: 829.594086, mean_q: 3.348944, mean_eps: 0.901036
  16606/150000: episode: 168, duration: 0.627s, episode steps:  73, steps per second: 116, episode reward: -126.951, mean reward: -1.739 [-100.000, 25.289], mean action: 1.781 [0.000, 3.000],  loss: 15.539924, mse: 861.348665, mean_q: 2.976447, mean_eps: 0.900586
  16678/150000: episode: 169, duration: 0.689s, episode steps:  72, steps per second: 105, episode reward: -75.224, mean reward: -1.045 [-100.000, 19.259], mean action: 1.583 [0.000, 3.000],  loss: 14.447450, mse: 822.689223, mean_q: 3.353644, mean_eps: 0.900151
  16807/150000: episode: 170, duration: 1.151s, episode steps: 129, steps per second: 112, episode reward: -137.929, mean reward: -1.069 [-100.000,  6.362], mean action: 1.403 [0.000, 3.000],  loss: 14.431562, mse: 833.253642, mean_q: 3.399124, mean_eps: 0.899548
  16900/150000: episode: 171, duration: 0.800s, episode steps:  93, steps per second: 116, episode reward: -85.231, mean reward: -0.916 [-100.000,  7.213], mean action: 1.559 [0.000, 3.000],  loss: 16.518120, mse: 814.408894, mean_q: 3.523845, mean_eps: 0.898882
  16970/150000: episode: 172, duration: 0.647s, episode steps:  70, steps per second: 108, episode reward: -129.775, mean reward: -1.854 [-100.000,  9.217], mean action: 1.500 [0.000, 3.000],  loss: 12.903592, mse: 830.607799, mean_q: 4.088049, mean_eps: 0.898393
  17063/150000: episode: 173, duration: 0.793s, episode steps:  93, steps per second: 117, episode reward: -75.990, mean reward: -0.817 [-100.000, 19.682], mean action: 1.505 [0.000, 3.000],  loss: 20.257388, mse: 844.387286, mean_q: 3.992226, mean_eps: 0.897904
  17165/150000: episode: 174, duration: 1.071s, episode steps: 102, steps per second:  95, episode reward: -425.698, mean reward: -4.174 [-100.000, 76.745], mean action: 1.716 [0.000, 3.000],  loss: 20.415219, mse: 823.986717, mean_q: 4.642002, mean_eps: 0.897319
  17281/150000: episode: 175, duration: 1.259s, episode steps: 116, steps per second:  92, episode reward: -14.597, mean reward: -0.126 [-100.000, 87.314], mean action: 1.578 [0.000, 3.000],  loss: 15.011979, mse: 821.131716, mean_q: 3.803172, mean_eps: 0.896665
  17351/150000: episode: 176, duration: 0.697s, episode steps:  70, steps per second: 100, episode reward: -49.452, mean reward: -0.706 [-100.000, 25.970], mean action: 1.443 [0.000, 3.000],  loss: 14.287270, mse: 779.130585, mean_q: 4.739527, mean_eps: 0.896107
  17442/150000: episode: 177, duration: 0.930s, episode steps:  91, steps per second:  98, episode reward: -74.718, mean reward: -0.821 [-100.000, 10.454], mean action: 1.736 [0.000, 3.000],  loss: 18.225734, mse: 812.383960, mean_q: 4.153578, mean_eps: 0.895624
  17517/150000: episode: 178, duration: 0.713s, episode steps:  75, steps per second: 105, episode reward: -65.855, mean reward: -0.878 [-100.000,  9.816], mean action: 1.547 [0.000, 3.000],  loss: 22.228721, mse: 831.779881, mean_q: 4.929746, mean_eps: 0.895126
  17589/150000: episode: 179, duration: 0.599s, episode steps:  72, steps per second: 120, episode reward: -92.394, mean reward: -1.283 [-100.000, 19.618], mean action: 1.764 [0.000, 3.000],  loss: 24.321936, mse: 828.880761, mean_q: 5.004122, mean_eps: 0.894685
  17691/150000: episode: 180, duration: 0.870s, episode steps: 102, steps per second: 117, episode reward: -187.506, mean reward: -1.838 [-100.000, 23.747], mean action: 1.314 [0.000, 3.000],  loss: 17.996966, mse: 838.399429, mean_q: 4.386779, mean_eps: 0.894163
  17767/150000: episode: 181, duration: 0.709s, episode steps:  76, steps per second: 107, episode reward: -112.658, mean reward: -1.482 [-100.000,  6.537], mean action: 1.500 [0.000, 3.000],  loss: 19.256916, mse: 807.588043, mean_q: 5.178111, mean_eps: 0.893629
  17863/150000: episode: 182, duration: 0.823s, episode steps:  96, steps per second: 117, episode reward: -171.920, mean reward: -1.791 [-100.000,  9.565], mean action: 1.771 [0.000, 3.000],  loss: 16.344186, mse: 831.783993, mean_q: 5.346785, mean_eps: 0.893113
  17961/150000: episode: 183, duration: 0.845s, episode steps:  98, steps per second: 116, episode reward: -124.742, mean reward: -1.273 [-100.000,  5.556], mean action: 1.449 [0.000, 3.000],  loss: 22.200117, mse: 819.634043, mean_q: 4.524252, mean_eps: 0.892531
  18060/150000: episode: 184, duration: 0.830s, episode steps:  99, steps per second: 119, episode reward: -76.833, mean reward: -0.776 [-100.000, 10.198], mean action: 1.556 [0.000, 3.000],  loss: 19.657577, mse: 846.748007, mean_q: 5.096395, mean_eps: 0.891940
  18162/150000: episode: 185, duration: 0.899s, episode steps: 102, steps per second: 113, episode reward: -351.753, mean reward: -3.449 [-100.000,  2.291], mean action: 1.696 [0.000, 3.000],  loss: 25.553296, mse: 844.119241, mean_q: 6.032259, mean_eps: 0.891337
  18244/150000: episode: 186, duration: 0.689s, episode steps:  82, steps per second: 119, episode reward: -198.420, mean reward: -2.420 [-100.000,  5.826], mean action: 1.512 [0.000, 3.000],  loss: 16.493768, mse: 822.448280, mean_q: 6.710596, mean_eps: 0.890785
  18329/150000: episode: 187, duration: 0.786s, episode steps:  85, steps per second: 108, episode reward: -84.508, mean reward: -0.994 [-100.000, 11.127], mean action: 1.412 [0.000, 3.000],  loss: 26.692551, mse: 831.966357, mean_q: 6.102178, mean_eps: 0.890284
  18431/150000: episode: 188, duration: 0.978s, episode steps: 102, steps per second: 104, episode reward: -127.531, mean reward: -1.250 [-100.000, 90.591], mean action: 1.725 [0.000, 3.000],  loss: 27.625214, mse: 845.759456, mean_q: 5.202570, mean_eps: 0.889723
  18495/150000: episode: 189, duration: 0.618s, episode steps:  64, steps per second: 104, episode reward: -79.866, mean reward: -1.248 [-100.000, 10.625], mean action: 1.641 [0.000, 3.000],  loss: 24.359957, mse: 825.195265, mean_q: 5.930266, mean_eps: 0.889225
  18591/150000: episode: 190, duration: 0.907s, episode steps:  96, steps per second: 106, episode reward: -112.575, mean reward: -1.173 [-100.000, 10.233], mean action: 1.500 [0.000, 3.000],  loss: 15.975018, mse: 854.467888, mean_q: 6.076227, mean_eps: 0.888745
  18657/150000: episode: 191, duration: 0.634s, episode steps:  66, steps per second: 104, episode reward: -81.819, mean reward: -1.240 [-100.000, 10.180], mean action: 1.197 [0.000, 3.000],  loss: 19.795658, mse: 800.038191, mean_q: 6.057941, mean_eps: 0.888259
  18794/150000: episode: 192, duration: 1.230s, episode steps: 137, steps per second: 111, episode reward: -94.879, mean reward: -0.693 [-100.000, 19.163], mean action: 1.715 [0.000, 3.000],  loss: 19.172444, mse: 874.250093, mean_q: 4.838897, mean_eps: 0.887650
  18913/150000: episode: 193, duration: 1.177s, episode steps: 119, steps per second: 101, episode reward: -227.521, mean reward: -1.912 [-100.000,  7.549], mean action: 1.597 [0.000, 3.000],  loss: 18.319337, mse: 843.069941, mean_q: 5.826081, mean_eps: 0.886882
  18988/150000: episode: 194, duration: 0.665s, episode steps:  75, steps per second: 113, episode reward: -275.324, mean reward: -3.671 [-100.000, 118.977], mean action: 1.440 [0.000, 3.000],  loss: 24.498103, mse: 823.426483, mean_q: 6.293679, mean_eps: 0.886300
  19107/150000: episode: 195, duration: 1.094s, episode steps: 119, steps per second: 109, episode reward: -130.026, mean reward: -1.093 [-100.000,  5.532], mean action: 1.723 [0.000, 3.000],  loss: 18.128623, mse: 871.094915, mean_q: 7.445499, mean_eps: 0.885718
  19195/150000: episode: 196, duration: 0.784s, episode steps:  88, steps per second: 112, episode reward: -90.067, mean reward: -1.023 [-100.000,  8.024], mean action: 1.534 [0.000, 3.000],  loss: 24.803465, mse: 915.872561, mean_q: 6.636101, mean_eps: 0.885097
  19276/150000: episode: 197, duration: 0.706s, episode steps:  81, steps per second: 115, episode reward: -152.723, mean reward: -1.885 [-100.000,  5.947], mean action: 1.481 [0.000, 3.000],  loss: 19.344356, mse: 890.184458, mean_q: 7.532735, mean_eps: 0.884590
  19358/150000: episode: 198, duration: 0.760s, episode steps:  82, steps per second: 108, episode reward: -62.842, mean reward: -0.766 [-100.000, 12.802], mean action: 1.488 [0.000, 3.000],  loss: 25.788819, mse: 898.454099, mean_q: 7.368662, mean_eps: 0.884101
  19446/150000: episode: 199, duration: 0.855s, episode steps:  88, steps per second: 103, episode reward: -169.118, mean reward: -1.922 [-100.000, 14.319], mean action: 1.420 [0.000, 3.000],  loss: 16.802751, mse: 885.349177, mean_q: 7.898538, mean_eps: 0.883591
  19548/150000: episode: 200, duration: 0.912s, episode steps: 102, steps per second: 112, episode reward: -118.734, mean reward: -1.164 [-100.000, 19.262], mean action: 1.598 [0.000, 3.000],  loss: 19.839924, mse: 921.506405, mean_q: 6.974626, mean_eps: 0.883021
  19637/150000: episode: 201, duration: 0.838s, episode steps:  89, steps per second: 106, episode reward: -61.332, mean reward: -0.689 [-100.000, 16.983], mean action: 1.708 [0.000, 3.000],  loss: 23.452989, mse: 912.836652, mean_q: 7.247712, mean_eps: 0.882448
  19752/150000: episode: 202, duration: 1.097s, episode steps: 115, steps per second: 105, episode reward: -144.819, mean reward: -1.259 [-100.000,  8.642], mean action: 1.574 [0.000, 3.000],  loss: 18.452644, mse: 915.404841, mean_q: 6.882552, mean_eps: 0.881836
  19880/150000: episode: 203, duration: 1.257s, episode steps: 128, steps per second: 102, episode reward: -121.643, mean reward: -0.950 [-100.000,  6.826], mean action: 1.453 [0.000, 3.000],  loss: 24.235560, mse: 894.101408, mean_q: 7.651449, mean_eps: 0.881107
  19984/150000: episode: 204, duration: 1.060s, episode steps: 104, steps per second:  98, episode reward: -84.241, mean reward: -0.810 [-100.000,  9.190], mean action: 1.538 [0.000, 3.000],  loss: 19.787872, mse: 899.239241, mean_q: 7.250220, mean_eps: 0.880411
  20060/150000: episode: 205, duration: 0.732s, episode steps:  76, steps per second: 104, episode reward: -99.126, mean reward: -1.304 [-100.000,  6.206], mean action: 1.566 [0.000, 3.000],  loss: 26.259007, mse: 927.534624, mean_q: 8.944766, mean_eps: 0.879871
  20183/150000: episode: 206, duration: 1.122s, episode steps: 123, steps per second: 110, episode reward: -33.592, mean reward: -0.273 [-100.000, 23.126], mean action: 1.602 [0.000, 3.000],  loss: 18.178485, mse: 932.769657, mean_q: 9.143946, mean_eps: 0.879274
  20286/150000: episode: 207, duration: 0.955s, episode steps: 103, steps per second: 108, episode reward: -76.393, mean reward: -0.742 [-100.000, 17.939], mean action: 1.534 [0.000, 3.000],  loss: 21.852030, mse: 947.068807, mean_q: 9.001279, mean_eps: 0.878596
  20401/150000: episode: 208, duration: 0.944s, episode steps: 115, steps per second: 122, episode reward: -67.788, mean reward: -0.589 [-100.000, 13.809], mean action: 1.652 [0.000, 3.000],  loss: 21.967396, mse: 950.382593, mean_q: 8.887066, mean_eps: 0.877942
  20504/150000: episode: 209, duration: 1.036s, episode steps: 103, steps per second:  99, episode reward: -104.389, mean reward: -1.013 [-100.000, 13.689], mean action: 1.602 [0.000, 3.000],  loss: 20.965365, mse: 934.231169, mean_q: 9.038056, mean_eps: 0.877288
  20596/150000: episode: 210, duration: 0.771s, episode steps:  92, steps per second: 119, episode reward: -121.329, mean reward: -1.319 [-100.000, 15.679], mean action: 1.609 [0.000, 3.000],  loss: 14.704350, mse: 939.068026, mean_q: 9.191174, mean_eps: 0.876703
  20704/150000: episode: 211, duration: 0.902s, episode steps: 108, steps per second: 120, episode reward: -98.279, mean reward: -0.910 [-100.000,  8.084], mean action: 1.639 [0.000, 3.000],  loss: 18.557254, mse: 949.390474, mean_q: 8.879182, mean_eps: 0.876103
  20785/150000: episode: 212, duration: 0.732s, episode steps:  81, steps per second: 111, episode reward: -36.884, mean reward: -0.455 [-100.000, 10.891], mean action: 1.469 [0.000, 3.000],  loss: 19.078923, mse: 951.595181, mean_q: 8.998522, mean_eps: 0.875536
  20862/150000: episode: 213, duration: 0.654s, episode steps:  77, steps per second: 118, episode reward: -92.253, mean reward: -1.198 [-100.000, 22.870], mean action: 1.649 [0.000, 3.000],  loss: 23.388330, mse: 974.692325, mean_q: 8.234643, mean_eps: 0.875062
  20975/150000: episode: 214, duration: 0.996s, episode steps: 113, steps per second: 113, episode reward: -76.066, mean reward: -0.673 [-100.000, 17.212], mean action: 1.708 [0.000, 3.000],  loss: 25.518415, mse: 956.823190, mean_q: 8.860658, mean_eps: 0.874492
  21075/150000: episode: 215, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: -344.212, mean reward: -3.442 [-100.000,  5.958], mean action: 1.480 [0.000, 3.000],  loss: 18.970252, mse: 1020.472708, mean_q: 9.715154, mean_eps: 0.873853
  21186/150000: episode: 216, duration: 0.921s, episode steps: 111, steps per second: 121, episode reward: -73.957, mean reward: -0.666 [-100.000, 24.149], mean action: 1.396 [0.000, 3.000],  loss: 23.330599, mse: 1002.271327, mean_q: 10.774775, mean_eps: 0.873220
  21269/150000: episode: 217, duration: 0.736s, episode steps:  83, steps per second: 113, episode reward: -98.671, mean reward: -1.189 [-100.000, 21.996], mean action: 1.687 [0.000, 3.000],  loss: 25.678918, mse: 1035.523940, mean_q: 10.591346, mean_eps: 0.872638
  21411/150000: episode: 218, duration: 1.178s, episode steps: 142, steps per second: 121, episode reward: -93.586, mean reward: -0.659 [-100.000, 19.127], mean action: 1.493 [0.000, 3.000],  loss: 20.597586, mse: 1063.698972, mean_q: 9.998035, mean_eps: 0.871963
  21480/150000: episode: 219, duration: 0.614s, episode steps:  69, steps per second: 112, episode reward: -94.689, mean reward: -1.372 [-100.000,  9.484], mean action: 1.507 [0.000, 3.000],  loss: 28.852990, mse: 1031.920029, mean_q: 10.190283, mean_eps: 0.871330
  21559/150000: episode: 220, duration: 0.690s, episode steps:  79, steps per second: 114, episode reward: -100.239, mean reward: -1.269 [-100.000, 11.716], mean action: 1.658 [0.000, 3.000],  loss: 17.739158, mse: 1023.759854, mean_q: 10.436811, mean_eps: 0.870886
  21625/150000: episode: 221, duration: 0.640s, episode steps:  66, steps per second: 103, episode reward: -142.899, mean reward: -2.165 [-100.000,  4.856], mean action: 1.788 [0.000, 3.000],  loss: 18.265214, mse: 1099.773026, mean_q: 10.323583, mean_eps: 0.870451
  21761/150000: episode: 222, duration: 1.190s, episode steps: 136, steps per second: 114, episode reward: -205.136, mean reward: -1.508 [-100.000, 77.846], mean action: 1.537 [0.000, 3.000],  loss: 27.584971, mse: 1047.120904, mean_q: 9.929684, mean_eps: 0.869845
  21822/150000: episode: 223, duration: 0.524s, episode steps:  61, steps per second: 117, episode reward: -96.587, mean reward: -1.583 [-100.000,  6.170], mean action: 1.902 [0.000, 3.000],  loss: 19.645579, mse: 964.857181, mean_q: 11.560078, mean_eps: 0.869254
  21916/150000: episode: 224, duration: 0.812s, episode steps:  94, steps per second: 116, episode reward: -112.995, mean reward: -1.202 [-100.000, 16.428], mean action: 1.500 [0.000, 3.000],  loss: 25.170323, mse: 1069.238081, mean_q: 10.910118, mean_eps: 0.868789
  21993/150000: episode: 225, duration: 0.677s, episode steps:  77, steps per second: 114, episode reward: -79.436, mean reward: -1.032 [-100.000, 10.762], mean action: 1.649 [0.000, 3.000],  loss: 23.530713, mse: 1039.621944, mean_q: 10.830546, mean_eps: 0.868276
  22100/150000: episode: 226, duration: 0.875s, episode steps: 107, steps per second: 122, episode reward: -111.618, mean reward: -1.043 [-100.000,  5.780], mean action: 1.645 [0.000, 3.000],  loss: 17.820461, mse: 1105.624491, mean_q: 10.998157, mean_eps: 0.867724
  22190/150000: episode: 227, duration: 0.863s, episode steps:  90, steps per second: 104, episode reward: -128.700, mean reward: -1.430 [-100.000, 12.813], mean action: 1.489 [0.000, 3.000],  loss: 20.675671, mse: 1113.318808, mean_q: 11.832265, mean_eps: 0.867133
  22259/150000: episode: 228, duration: 0.679s, episode steps:  69, steps per second: 102, episode reward: -64.064, mean reward: -0.928 [-100.000,  7.277], mean action: 1.420 [0.000, 3.000],  loss: 13.603848, mse: 1101.080305, mean_q: 12.430909, mean_eps: 0.866656
  22327/150000: episode: 229, duration: 0.634s, episode steps:  68, steps per second: 107, episode reward: -72.122, mean reward: -1.061 [-100.000, 12.256], mean action: 1.721 [0.000, 3.000],  loss: 20.596668, mse: 1130.146580, mean_q: 10.966567, mean_eps: 0.866245
  22486/150000: episode: 230, duration: 1.582s, episode steps: 159, steps per second: 101, episode reward: -90.997, mean reward: -0.572 [-100.000,  6.716], mean action: 1.579 [0.000, 3.000],  loss: 15.465540, mse: 1115.530015, mean_q: 11.203184, mean_eps: 0.865564
  22585/150000: episode: 231, duration: 0.916s, episode steps:  99, steps per second: 108, episode reward: -99.867, mean reward: -1.009 [-100.000,  9.798], mean action: 1.636 [0.000, 3.000],  loss: 14.087165, mse: 1104.831157, mean_q: 11.640401, mean_eps: 0.864790
  22689/150000: episode: 232, duration: 1.175s, episode steps: 104, steps per second:  89, episode reward: -112.163, mean reward: -1.078 [-100.000, 10.247], mean action: 1.558 [0.000, 3.000],  loss: 20.755375, mse: 1130.035734, mean_q: 12.301447, mean_eps: 0.864181
  22784/150000: episode: 233, duration: 1.113s, episode steps:  95, steps per second:  85, episode reward: -201.475, mean reward: -2.121 [-100.000, 23.541], mean action: 1.337 [0.000, 3.000],  loss: 17.326022, mse: 1105.159059, mean_q: 12.081986, mean_eps: 0.863584
  22906/150000: episode: 234, duration: 1.264s, episode steps: 122, steps per second:  97, episode reward: -165.707, mean reward: -1.358 [-100.000, 17.970], mean action: 1.484 [0.000, 3.000],  loss: 16.106073, mse: 1130.060956, mean_q: 11.472139, mean_eps: 0.862933
  23014/150000: episode: 235, duration: 0.930s, episode steps: 108, steps per second: 116, episode reward: -214.232, mean reward: -1.984 [-100.000, 20.459], mean action: 1.657 [0.000, 3.000],  loss: 11.674983, mse: 1096.818971, mean_q: 11.883028, mean_eps: 0.862243
  23074/150000: episode: 236, duration: 0.513s, episode steps:  60, steps per second: 117, episode reward: -112.559, mean reward: -1.876 [-100.000,  3.897], mean action: 1.367 [0.000, 3.000],  loss: 19.904414, mse: 1179.791429, mean_q: 11.970520, mean_eps: 0.861739
  23198/150000: episode: 237, duration: 1.139s, episode steps: 124, steps per second: 109, episode reward: -111.509, mean reward: -0.899 [-100.000,  5.586], mean action: 1.468 [0.000, 3.000],  loss: 19.862763, mse: 1181.755139, mean_q: 12.296416, mean_eps: 0.861187
  23317/150000: episode: 238, duration: 1.183s, episode steps: 119, steps per second: 101, episode reward: -118.063, mean reward: -0.992 [-100.000,  8.679], mean action: 1.613 [0.000, 3.000],  loss: 20.083355, mse: 1185.234352, mean_q: 12.393743, mean_eps: 0.860458
  23404/150000: episode: 239, duration: 0.743s, episode steps:  87, steps per second: 117, episode reward: -117.820, mean reward: -1.354 [-100.000, 28.172], mean action: 1.448 [0.000, 3.000],  loss: 12.150991, mse: 1171.385897, mean_q: 12.209020, mean_eps: 0.859840
  23517/150000: episode: 240, duration: 1.081s, episode steps: 113, steps per second: 104, episode reward: -34.780, mean reward: -0.308 [-100.000, 23.402], mean action: 1.513 [0.000, 3.000],  loss: 12.393908, mse: 1195.239887, mean_q: 11.568772, mean_eps: 0.859240
  23605/150000: episode: 241, duration: 0.791s, episode steps:  88, steps per second: 111, episode reward: -70.291, mean reward: -0.799 [-100.000, 11.733], mean action: 1.432 [0.000, 3.000],  loss: 15.481340, mse: 1203.146525, mean_q: 12.625218, mean_eps: 0.858637
  23670/150000: episode: 242, duration: 0.554s, episode steps:  65, steps per second: 117, episode reward: -103.487, mean reward: -1.592 [-100.000,  7.189], mean action: 1.446 [0.000, 3.000],  loss: 10.335140, mse: 1196.191550, mean_q: 13.052273, mean_eps: 0.858178
  23783/150000: episode: 243, duration: 1.062s, episode steps: 113, steps per second: 106, episode reward: -168.635, mean reward: -1.492 [-100.000,  3.494], mean action: 1.442 [0.000, 3.000],  loss: 13.523761, mse: 1180.110727, mean_q: 12.379776, mean_eps: 0.857644
  23876/150000: episode: 244, duration: 0.835s, episode steps:  93, steps per second: 111, episode reward: -81.676, mean reward: -0.878 [-100.000,  8.958], mean action: 1.581 [0.000, 3.000],  loss: 12.811472, mse: 1185.147224, mean_q: 12.910324, mean_eps: 0.857026
  23963/150000: episode: 245, duration: 0.742s, episode steps:  87, steps per second: 117, episode reward: -91.513, mean reward: -1.052 [-100.000, 17.303], mean action: 1.425 [0.000, 3.000],  loss: 12.982181, mse: 1185.207859, mean_q: 12.678857, mean_eps: 0.856486
  24069/150000: episode: 246, duration: 1.020s, episode steps: 106, steps per second: 104, episode reward: -152.757, mean reward: -1.441 [-100.000, 11.589], mean action: 1.368 [0.000, 3.000],  loss: 16.892980, mse: 1241.963909, mean_q: 13.540145, mean_eps: 0.855907
  24163/150000: episode: 247, duration: 0.884s, episode steps:  94, steps per second: 106, episode reward: -73.273, mean reward: -0.779 [-100.000,  6.950], mean action: 1.649 [0.000, 3.000],  loss: 18.593749, mse: 1312.256068, mean_q: 12.973890, mean_eps: 0.855307
  24239/150000: episode: 248, duration: 0.898s, episode steps:  76, steps per second:  85, episode reward: -73.475, mean reward: -0.967 [-100.000,  9.109], mean action: 1.658 [0.000, 3.000],  loss: 16.496870, mse: 1305.771725, mean_q: 13.053109, mean_eps: 0.854797
  24352/150000: episode: 249, duration: 1.040s, episode steps: 113, steps per second: 109, episode reward: -318.821, mean reward: -2.821 [-100.000,  0.676], mean action: 1.646 [0.000, 3.000],  loss: 17.981474, mse: 1291.032473, mean_q: 14.578097, mean_eps: 0.854230
  24431/150000: episode: 250, duration: 0.628s, episode steps:  79, steps per second: 126, episode reward: -101.174, mean reward: -1.281 [-100.000,  9.275], mean action: 1.456 [0.000, 3.000],  loss: 17.149315, mse: 1344.057444, mean_q: 13.326042, mean_eps: 0.853654
  24530/150000: episode: 251, duration: 1.201s, episode steps:  99, steps per second:  82, episode reward: -7.958, mean reward: -0.080 [-100.000, 110.917], mean action: 1.576 [0.000, 3.000],  loss: 14.174384, mse: 1284.401090, mean_q: 12.994234, mean_eps: 0.853120
  24653/150000: episode: 252, duration: 1.219s, episode steps: 123, steps per second: 101, episode reward: -72.273, mean reward: -0.588 [-100.000, 15.315], mean action: 1.480 [0.000, 3.000],  loss: 14.391386, mse: 1291.463972, mean_q: 13.320157, mean_eps: 0.852454
  24764/150000: episode: 253, duration: 1.080s, episode steps: 111, steps per second: 103, episode reward: -193.393, mean reward: -1.742 [-100.000,  6.294], mean action: 1.369 [0.000, 3.000],  loss: 11.887520, mse: 1241.161355, mean_q: 13.954051, mean_eps: 0.851752
  24848/150000: episode: 254, duration: 0.699s, episode steps:  84, steps per second: 120, episode reward: -80.410, mean reward: -0.957 [-100.000, 16.355], mean action: 1.833 [0.000, 3.000],  loss: 13.961153, mse: 1262.605493, mean_q: 13.241333, mean_eps: 0.851167
  24950/150000: episode: 255, duration: 0.831s, episode steps: 102, steps per second: 123, episode reward: -112.899, mean reward: -1.107 [-100.000, 10.490], mean action: 1.324 [0.000, 3.000],  loss: 17.970082, mse: 1273.654518, mean_q: 13.988420, mean_eps: 0.850609
  25073/150000: episode: 256, duration: 0.930s, episode steps: 123, steps per second: 132, episode reward: -131.105, mean reward: -1.066 [-100.000,  9.875], mean action: 1.650 [0.000, 3.000],  loss: 19.866915, mse: 1321.479910, mean_q: 13.856531, mean_eps: 0.849934
  25201/150000: episode: 257, duration: 0.966s, episode steps: 128, steps per second: 132, episode reward: -52.700, mean reward: -0.412 [-100.000, 14.975], mean action: 1.648 [0.000, 3.000],  loss: 16.124947, mse: 1385.460627, mean_q: 13.620669, mean_eps: 0.849181
  25309/150000: episode: 258, duration: 0.960s, episode steps: 108, steps per second: 113, episode reward: -69.892, mean reward: -0.647 [-100.000, 56.576], mean action: 1.491 [0.000, 3.000],  loss: 11.325260, mse: 1307.460235, mean_q: 14.242447, mean_eps: 0.848473
  25414/150000: episode: 259, duration: 1.044s, episode steps: 105, steps per second: 101, episode reward: -140.668, mean reward: -1.340 [-100.000, 27.592], mean action: 1.390 [0.000, 3.000],  loss: 14.384169, mse: 1322.155323, mean_q: 15.132828, mean_eps: 0.847834
  25474/150000: episode: 260, duration: 0.491s, episode steps:  60, steps per second: 122, episode reward: -103.569, mean reward: -1.726 [-100.000,  9.349], mean action: 1.350 [0.000, 3.000],  loss: 11.685805, mse: 1333.619669, mean_q: 14.242816, mean_eps: 0.847339
  25538/150000: episode: 261, duration: 0.463s, episode steps:  64, steps per second: 138, episode reward: -181.433, mean reward: -2.835 [-100.000, 48.277], mean action: 1.656 [0.000, 3.000],  loss: 15.965052, mse: 1349.103177, mean_q: 14.499745, mean_eps: 0.846967
  25600/150000: episode: 262, duration: 0.455s, episode steps:  62, steps per second: 136, episode reward: -91.660, mean reward: -1.478 [-100.000, 17.238], mean action: 1.403 [0.000, 3.000],  loss: 14.735655, mse: 1311.227304, mean_q: 14.313945, mean_eps: 0.846589
  25661/150000: episode: 263, duration: 0.446s, episode steps:  61, steps per second: 137, episode reward: -72.958, mean reward: -1.196 [-100.000, 11.497], mean action: 1.492 [0.000, 3.000],  loss: 16.875237, mse: 1344.630260, mean_q: 13.750205, mean_eps: 0.846220
  25732/150000: episode: 264, duration: 0.635s, episode steps:  71, steps per second: 112, episode reward: -80.293, mean reward: -1.131 [-100.000, 12.366], mean action: 1.507 [0.000, 3.000],  loss: 16.326516, mse: 1364.077910, mean_q: 14.930221, mean_eps: 0.845824
  25800/150000: episode: 265, duration: 0.599s, episode steps:  68, steps per second: 114, episode reward: -77.907, mean reward: -1.146 [-100.000, 11.936], mean action: 1.662 [0.000, 3.000],  loss: 13.979585, mse: 1352.630735, mean_q: 13.926883, mean_eps: 0.845407
  25933/150000: episode: 266, duration: 1.025s, episode steps: 133, steps per second: 130, episode reward: -144.877, mean reward: -1.089 [-100.000, 21.008], mean action: 1.564 [0.000, 3.000],  loss: 14.361193, mse: 1340.976800, mean_q: 14.783176, mean_eps: 0.844804
  26002/150000: episode: 267, duration: 0.616s, episode steps:  69, steps per second: 112, episode reward: -91.159, mean reward: -1.321 [-100.000,  7.355], mean action: 1.478 [0.000, 3.000],  loss: 11.844736, mse: 1314.889353, mean_q: 13.814981, mean_eps: 0.844198
  26083/150000: episode: 268, duration: 0.620s, episode steps:  81, steps per second: 131, episode reward: -106.843, mean reward: -1.319 [-100.000,  8.132], mean action: 1.617 [0.000, 3.000],  loss: 15.983801, mse: 1447.346219, mean_q: 15.862300, mean_eps: 0.843748
  26177/150000: episode: 269, duration: 0.674s, episode steps:  94, steps per second: 139, episode reward: -226.090, mean reward: -2.405 [-100.000,  7.039], mean action: 1.553 [0.000, 3.000],  loss: 12.303785, mse: 1442.700789, mean_q: 17.328497, mean_eps: 0.843223
  26291/150000: episode: 270, duration: 0.870s, episode steps: 114, steps per second: 131, episode reward: -254.601, mean reward: -2.233 [-100.000, 67.019], mean action: 1.693 [0.000, 3.000],  loss: 13.845386, mse: 1427.058569, mean_q: 16.592384, mean_eps: 0.842599
  26397/150000: episode: 271, duration: 1.311s, episode steps: 106, steps per second:  81, episode reward: -173.007, mean reward: -1.632 [-100.000,  5.441], mean action: 1.736 [0.000, 3.000],  loss: 20.406731, mse: 1444.971493, mean_q: 15.847726, mean_eps: 0.841939
  26456/150000: episode: 272, duration: 0.809s, episode steps:  59, steps per second:  73, episode reward: -97.282, mean reward: -1.649 [-100.000, 11.032], mean action: 1.492 [0.000, 3.000],  loss: 22.543290, mse: 1479.551488, mean_q: 16.353438, mean_eps: 0.841444
  26573/150000: episode: 273, duration: 1.141s, episode steps: 117, steps per second: 103, episode reward: -241.768, mean reward: -2.066 [-100.000,  8.661], mean action: 1.778 [0.000, 3.000],  loss: 20.614236, mse: 1460.070492, mean_q: 16.374748, mean_eps: 0.840916
  26665/150000: episode: 274, duration: 0.785s, episode steps:  92, steps per second: 117, episode reward: -85.753, mean reward: -0.932 [-100.000,  8.259], mean action: 1.402 [0.000, 3.000],  loss: 15.670867, mse: 1433.744724, mean_q: 16.810813, mean_eps: 0.840289
  26765/150000: episode: 275, duration: 0.935s, episode steps: 100, steps per second: 107, episode reward: -60.939, mean reward: -0.609 [-100.000, 12.653], mean action: 1.500 [0.000, 3.000],  loss: 14.710638, mse: 1420.361566, mean_q: 16.029494, mean_eps: 0.839713
  26854/150000: episode: 276, duration: 0.733s, episode steps:  89, steps per second: 121, episode reward: -111.987, mean reward: -1.258 [-100.000, 11.850], mean action: 1.494 [0.000, 3.000],  loss: 20.895739, mse: 1422.096423, mean_q: 15.841196, mean_eps: 0.839146
  26934/150000: episode: 277, duration: 0.769s, episode steps:  80, steps per second: 104, episode reward: -44.793, mean reward: -0.560 [-100.000, 17.393], mean action: 1.512 [0.000, 3.000],  loss: 17.746286, mse: 1444.226825, mean_q: 16.622877, mean_eps: 0.838639
  27050/150000: episode: 278, duration: 1.005s, episode steps: 116, steps per second: 115, episode reward: -197.961, mean reward: -1.707 [-100.000, 27.048], mean action: 1.698 [0.000, 3.000],  loss: 18.248379, mse: 1471.750634, mean_q: 16.918731, mean_eps: 0.838051
  27192/150000: episode: 279, duration: 1.097s, episode steps: 142, steps per second: 129, episode reward: -81.910, mean reward: -0.577 [-100.000, 11.254], mean action: 1.324 [0.000, 3.000],  loss: 13.647775, mse: 1474.697910, mean_q: 16.970458, mean_eps: 0.837277
  27315/150000: episode: 280, duration: 1.037s, episode steps: 123, steps per second: 119, episode reward: -121.252, mean reward: -0.986 [-100.000, 11.898], mean action: 1.650 [0.000, 3.000],  loss: 17.150865, mse: 1508.417070, mean_q: 17.391513, mean_eps: 0.836482
  27424/150000: episode: 281, duration: 0.801s, episode steps: 109, steps per second: 136, episode reward: -109.119, mean reward: -1.001 [-100.000,  7.109], mean action: 1.523 [0.000, 3.000],  loss: 14.383396, mse: 1532.317084, mean_q: 17.887148, mean_eps: 0.835786
  27494/150000: episode: 282, duration: 0.542s, episode steps:  70, steps per second: 129, episode reward: -58.312, mean reward: -0.833 [-100.000,  6.236], mean action: 1.800 [0.000, 3.000],  loss: 18.559224, mse: 1512.289676, mean_q: 19.147176, mean_eps: 0.835249
  27603/150000: episode: 283, duration: 0.791s, episode steps: 109, steps per second: 138, episode reward: -110.696, mean reward: -1.016 [-100.000,  8.368], mean action: 1.633 [0.000, 3.000],  loss: 18.738393, mse: 1511.488295, mean_q: 18.097616, mean_eps: 0.834712
  27683/150000: episode: 284, duration: 0.568s, episode steps:  80, steps per second: 141, episode reward: -79.696, mean reward: -0.996 [-100.000, 18.970], mean action: 1.613 [0.000, 3.000],  loss: 20.023686, mse: 1521.657216, mean_q: 17.852758, mean_eps: 0.834145
  27763/150000: episode: 285, duration: 0.739s, episode steps:  80, steps per second: 108, episode reward: -68.167, mean reward: -0.852 [-100.000, 13.907], mean action: 1.675 [0.000, 3.000],  loss: 12.779290, mse: 1539.436703, mean_q: 16.896942, mean_eps: 0.833665
  27879/150000: episode: 286, duration: 0.898s, episode steps: 116, steps per second: 129, episode reward: -72.103, mean reward: -0.622 [-100.000,  9.539], mean action: 1.543 [0.000, 3.000],  loss: 15.410179, mse: 1555.209521, mean_q: 17.583401, mean_eps: 0.833077
  28015/150000: episode: 287, duration: 1.008s, episode steps: 136, steps per second: 135, episode reward: -123.679, mean reward: -0.909 [-100.000,  5.823], mean action: 1.662 [0.000, 3.000],  loss: 16.703708, mse: 1542.327549, mean_q: 18.509852, mean_eps: 0.832321
  28083/150000: episode: 288, duration: 0.574s, episode steps:  68, steps per second: 118, episode reward: -98.107, mean reward: -1.443 [-100.000,  6.383], mean action: 1.632 [0.000, 3.000],  loss: 22.177083, mse: 1631.632746, mean_q: 19.157661, mean_eps: 0.831709
  28179/150000: episode: 289, duration: 0.874s, episode steps:  96, steps per second: 110, episode reward: -357.602, mean reward: -3.725 [-100.000,  6.555], mean action: 1.490 [0.000, 3.000],  loss: 16.734213, mse: 1633.505652, mean_q: 19.775661, mean_eps: 0.831217
  28246/150000: episode: 290, duration: 0.643s, episode steps:  67, steps per second: 104, episode reward: -74.835, mean reward: -1.117 [-100.000,  7.792], mean action: 1.433 [0.000, 3.000],  loss: 15.189984, mse: 1692.513932, mean_q: 19.514880, mean_eps: 0.830728
  28354/150000: episode: 291, duration: 0.984s, episode steps: 108, steps per second: 110, episode reward: -75.599, mean reward: -0.700 [-100.000,  7.795], mean action: 1.537 [0.000, 3.000],  loss: 17.951234, mse: 1635.557955, mean_q: 20.197856, mean_eps: 0.830203
  28465/150000: episode: 292, duration: 0.783s, episode steps: 111, steps per second: 142, episode reward: -110.403, mean reward: -0.995 [-100.000,  6.556], mean action: 1.658 [0.000, 3.000],  loss: 15.815219, mse: 1641.445746, mean_q: 20.147248, mean_eps: 0.829546
  28540/150000: episode: 293, duration: 0.549s, episode steps:  75, steps per second: 137, episode reward: -85.248, mean reward: -1.137 [-100.000, 11.999], mean action: 1.640 [0.000, 3.000],  loss: 14.471902, mse: 1661.850050, mean_q: 19.721200, mean_eps: 0.828988
  28644/150000: episode: 294, duration: 0.900s, episode steps: 104, steps per second: 116, episode reward: -129.158, mean reward: -1.242 [-100.000,  5.525], mean action: 1.519 [0.000, 3.000],  loss: 18.331895, mse: 1592.616442, mean_q: 19.923088, mean_eps: 0.828451
  28710/150000: episode: 295, duration: 0.482s, episode steps:  66, steps per second: 137, episode reward: -108.576, mean reward: -1.645 [-100.000, 16.041], mean action: 1.773 [0.000, 3.000],  loss: 17.900756, mse: 1653.051832, mean_q: 19.899026, mean_eps: 0.827941
  28775/150000: episode: 296, duration: 0.460s, episode steps:  65, steps per second: 141, episode reward: -52.009, mean reward: -0.800 [-100.000, 21.643], mean action: 1.492 [0.000, 3.000],  loss: 15.820131, mse: 1664.601514, mean_q: 19.194290, mean_eps: 0.827548
  28896/150000: episode: 297, duration: 0.914s, episode steps: 121, steps per second: 132, episode reward: -180.064, mean reward: -1.488 [-100.000, 16.535], mean action: 1.612 [0.000, 3.000],  loss: 13.860181, mse: 1642.741997, mean_q: 20.693503, mean_eps: 0.826990
  28989/150000: episode: 298, duration: 0.773s, episode steps:  93, steps per second: 120, episode reward: -122.089, mean reward: -1.313 [-100.000,  6.144], mean action: 1.796 [0.000, 3.000],  loss: 13.913254, mse: 1654.939850, mean_q: 19.830092, mean_eps: 0.826348
  29065/150000: episode: 299, duration: 0.536s, episode steps:  76, steps per second: 142, episode reward: -23.655, mean reward: -0.311 [-100.000, 24.677], mean action: 1.750 [0.000, 3.000],  loss: 12.532945, mse: 1751.334789, mean_q: 19.651621, mean_eps: 0.825841
  29134/150000: episode: 300, duration: 0.517s, episode steps:  69, steps per second: 134, episode reward: -43.798, mean reward: -0.635 [-100.000, 16.038], mean action: 1.812 [0.000, 3.000],  loss: 14.193225, mse: 1755.331900, mean_q: 20.217902, mean_eps: 0.825406
  29210/150000: episode: 301, duration: 0.564s, episode steps:  76, steps per second: 135, episode reward: -90.592, mean reward: -1.192 [-100.000, 12.633], mean action: 1.645 [0.000, 3.000],  loss: 15.782063, mse: 1783.657206, mean_q: 21.693397, mean_eps: 0.824971
  29298/150000: episode: 302, duration: 0.620s, episode steps:  88, steps per second: 142, episode reward: -93.342, mean reward: -1.061 [-100.000,  6.792], mean action: 1.545 [0.000, 3.000],  loss: 21.009157, mse: 1761.779667, mean_q: 20.921266, mean_eps: 0.824479
  29423/150000: episode: 303, duration: 0.902s, episode steps: 125, steps per second: 139, episode reward: -245.417, mean reward: -1.963 [-100.000,  3.833], mean action: 1.760 [0.000, 3.000],  loss: 16.107461, mse: 1749.963479, mean_q: 22.017727, mean_eps: 0.823840
  29516/150000: episode: 304, duration: 0.685s, episode steps:  93, steps per second: 136, episode reward: -165.910, mean reward: -1.784 [-100.000, 38.341], mean action: 1.796 [0.000, 3.000],  loss: 18.328195, mse: 1786.304050, mean_q: 21.843509, mean_eps: 0.823186
  29614/150000: episode: 305, duration: 0.791s, episode steps:  98, steps per second: 124, episode reward: -127.532, mean reward: -1.301 [-100.000, 11.548], mean action: 1.643 [0.000, 3.000],  loss: 18.718343, mse: 1762.398558, mean_q: 21.156504, mean_eps: 0.822613
  29717/150000: episode: 306, duration: 0.770s, episode steps: 103, steps per second: 134, episode reward: -93.693, mean reward: -0.910 [-100.000, 17.735], mean action: 1.650 [0.000, 3.000],  loss: 18.609309, mse: 1774.966812, mean_q: 20.598708, mean_eps: 0.822010
  29809/150000: episode: 307, duration: 0.676s, episode steps:  92, steps per second: 136, episode reward: -136.939, mean reward: -1.488 [-100.000,  6.437], mean action: 1.750 [0.000, 3.000],  loss: 16.601373, mse: 1746.672874, mean_q: 22.145476, mean_eps: 0.821425
  29895/150000: episode: 308, duration: 0.616s, episode steps:  86, steps per second: 140, episode reward: -74.309, mean reward: -0.864 [-100.000, 18.976], mean action: 1.674 [0.000, 3.000],  loss: 10.530799, mse: 1734.860654, mean_q: 22.243886, mean_eps: 0.820891
  29990/150000: episode: 309, duration: 0.725s, episode steps:  95, steps per second: 131, episode reward: -129.595, mean reward: -1.364 [-100.000,  9.046], mean action: 1.474 [0.000, 3.000],  loss: 15.384049, mse: 1758.072801, mean_q: 21.483723, mean_eps: 0.820348
  30069/150000: episode: 310, duration: 0.655s, episode steps:  79, steps per second: 121, episode reward: -38.461, mean reward: -0.487 [-100.000, 22.128], mean action: 1.608 [0.000, 3.000],  loss: 20.413690, mse: 1864.696779, mean_q: 23.121126, mean_eps: 0.819826
  30163/150000: episode: 311, duration: 0.762s, episode steps:  94, steps per second: 123, episode reward: -85.289, mean reward: -0.907 [-100.000, 23.051], mean action: 1.596 [0.000, 3.000],  loss: 22.070345, mse: 1925.818288, mean_q: 22.620331, mean_eps: 0.819307
  30243/150000: episode: 312, duration: 0.694s, episode steps:  80, steps per second: 115, episode reward: -126.252, mean reward: -1.578 [-100.000,  6.000], mean action: 1.288 [0.000, 3.000],  loss: 12.879395, mse: 1902.890636, mean_q: 23.034392, mean_eps: 0.818785
  30348/150000: episode: 313, duration: 0.884s, episode steps: 105, steps per second: 119, episode reward: -89.592, mean reward: -0.853 [-100.000,  8.624], mean action: 1.610 [0.000, 3.000],  loss: 12.882579, mse: 1886.763000, mean_q: 23.007432, mean_eps: 0.818230
  30433/150000: episode: 314, duration: 0.643s, episode steps:  85, steps per second: 132, episode reward: -56.944, mean reward: -0.670 [-100.000, 12.709], mean action: 1.600 [0.000, 3.000],  loss: 13.135648, mse: 1858.298893, mean_q: 23.794348, mean_eps: 0.817660
  30523/150000: episode: 315, duration: 0.687s, episode steps:  90, steps per second: 131, episode reward: -169.452, mean reward: -1.883 [-100.000,  6.062], mean action: 1.267 [0.000, 3.000],  loss: 18.979683, mse: 1926.768334, mean_q: 23.665332, mean_eps: 0.817135
  30594/150000: episode: 316, duration: 0.579s, episode steps:  71, steps per second: 123, episode reward: -73.693, mean reward: -1.038 [-100.000,  7.161], mean action: 1.634 [0.000, 3.000],  loss: 13.357854, mse: 1927.038612, mean_q: 22.263307, mean_eps: 0.816652
  30665/150000: episode: 317, duration: 0.513s, episode steps:  71, steps per second: 138, episode reward: -59.033, mean reward: -0.831 [-100.000, 15.609], mean action: 1.592 [0.000, 3.000],  loss: 15.048151, mse: 1937.658961, mean_q: 23.220176, mean_eps: 0.816226
  30811/150000: episode: 318, duration: 1.059s, episode steps: 146, steps per second: 138, episode reward: -89.566, mean reward: -0.613 [-100.000,  7.489], mean action: 1.610 [0.000, 3.000],  loss: 13.904406, mse: 1902.683744, mean_q: 23.658328, mean_eps: 0.815575
  30920/150000: episode: 319, duration: 0.911s, episode steps: 109, steps per second: 120, episode reward: -101.098, mean reward: -0.928 [-100.000, 19.448], mean action: 1.495 [0.000, 3.000],  loss: 21.974003, mse: 1886.017452, mean_q: 23.082725, mean_eps: 0.814810
  31036/150000: episode: 320, duration: 0.849s, episode steps: 116, steps per second: 137, episode reward: -49.066, mean reward: -0.423 [-100.000, 16.962], mean action: 1.526 [0.000, 3.000],  loss: 13.645045, mse: 1878.111664, mean_q: 24.192637, mean_eps: 0.814135
  31153/150000: episode: 321, duration: 0.865s, episode steps: 117, steps per second: 135, episode reward: -182.996, mean reward: -1.564 [-100.000, 53.353], mean action: 1.581 [0.000, 3.000],  loss: 8.074506, mse: 1974.088908, mean_q: 25.186553, mean_eps: 0.813436
  31274/150000: episode: 322, duration: 0.905s, episode steps: 121, steps per second: 134, episode reward: -106.742, mean reward: -0.882 [-100.000, 14.183], mean action: 1.512 [0.000, 3.000],  loss: 14.365026, mse: 1996.779839, mean_q: 23.901566, mean_eps: 0.812722
  31346/150000: episode: 323, duration: 0.523s, episode steps:  72, steps per second: 138, episode reward: -131.243, mean reward: -1.823 [-100.000, 13.646], mean action: 1.611 [0.000, 3.000],  loss: 18.508116, mse: 1957.770014, mean_q: 23.807313, mean_eps: 0.812143
  31436/150000: episode: 324, duration: 0.694s, episode steps:  90, steps per second: 130, episode reward: -110.074, mean reward: -1.223 [-100.000, 15.456], mean action: 1.433 [0.000, 3.000],  loss: 14.179415, mse: 1967.213026, mean_q: 24.663394, mean_eps: 0.811657
  31535/150000: episode: 325, duration: 0.809s, episode steps:  99, steps per second: 122, episode reward: -79.412, mean reward: -0.802 [-100.000,  7.485], mean action: 1.636 [0.000, 3.000],  loss: 11.579180, mse: 1985.107952, mean_q: 25.574544, mean_eps: 0.811090
  31610/150000: episode: 326, duration: 0.538s, episode steps:  75, steps per second: 139, episode reward: -23.001, mean reward: -0.307 [-100.000, 16.724], mean action: 1.640 [0.000, 3.000],  loss: 14.550783, mse: 1998.409476, mean_q: 24.957196, mean_eps: 0.810568
  31716/150000: episode: 327, duration: 0.887s, episode steps: 106, steps per second: 120, episode reward: -91.879, mean reward: -0.867 [-100.000, 11.510], mean action: 1.623 [0.000, 3.000],  loss: 14.956725, mse: 1965.811551, mean_q: 25.448347, mean_eps: 0.810025
  31791/150000: episode: 328, duration: 0.872s, episode steps:  75, steps per second:  86, episode reward: -102.335, mean reward: -1.364 [-100.000,  6.188], mean action: 1.827 [0.000, 3.000],  loss: 13.755648, mse: 2024.975721, mean_q: 25.344746, mean_eps: 0.809482
  31884/150000: episode: 329, duration: 0.941s, episode steps:  93, steps per second:  99, episode reward: -114.540, mean reward: -1.232 [-100.000,  6.078], mean action: 1.753 [0.000, 3.000],  loss: 12.000504, mse: 1971.764431, mean_q: 25.329246, mean_eps: 0.808978
  31992/150000: episode: 330, duration: 1.020s, episode steps: 108, steps per second: 106, episode reward: -113.351, mean reward: -1.050 [-100.000, 14.945], mean action: 1.602 [0.000, 3.000],  loss: 13.923113, mse: 1988.548239, mean_q: 24.645032, mean_eps: 0.808375
  32060/150000: episode: 331, duration: 0.645s, episode steps:  68, steps per second: 105, episode reward: -117.483, mean reward: -1.728 [-100.000, 25.718], mean action: 1.544 [0.000, 3.000],  loss: 13.796372, mse: 2055.285408, mean_q: 26.051456, mean_eps: 0.807847
  32186/150000: episode: 332, duration: 1.199s, episode steps: 126, steps per second: 105, episode reward: -80.143, mean reward: -0.636 [-100.000,  7.009], mean action: 1.524 [0.000, 3.000],  loss: 18.179496, mse: 2085.316668, mean_q: 25.628052, mean_eps: 0.807265
  32287/150000: episode: 333, duration: 0.856s, episode steps: 101, steps per second: 118, episode reward: -77.072, mean reward: -0.763 [-100.000,  9.095], mean action: 1.604 [0.000, 3.000],  loss: 13.873197, mse: 2116.423896, mean_q: 24.914755, mean_eps: 0.806584
  32405/150000: episode: 334, duration: 1.164s, episode steps: 118, steps per second: 101, episode reward: -78.764, mean reward: -0.667 [-100.000, 12.478], mean action: 1.441 [0.000, 3.000],  loss: 12.236966, mse: 2091.065395, mean_q: 25.228418, mean_eps: 0.805927
  32518/150000: episode: 335, duration: 1.148s, episode steps: 113, steps per second:  98, episode reward: -71.165, mean reward: -0.630 [-100.000, 10.876], mean action: 1.363 [0.000, 3.000],  loss: 14.036080, mse: 2146.698176, mean_q: 25.527428, mean_eps: 0.805234
  32607/150000: episode: 336, duration: 1.120s, episode steps:  89, steps per second:  79, episode reward: -86.894, mean reward: -0.976 [-100.000, 11.928], mean action: 1.640 [0.000, 3.000],  loss: 16.726105, mse: 2124.225419, mean_q: 26.373112, mean_eps: 0.804628
  32677/150000: episode: 337, duration: 0.689s, episode steps:  70, steps per second: 102, episode reward:  0.961, mean reward:  0.014 [-100.000, 17.383], mean action: 1.571 [0.000, 3.000],  loss: 21.019956, mse: 2084.099369, mean_q: 25.800850, mean_eps: 0.804151
  32762/150000: episode: 338, duration: 0.830s, episode steps:  85, steps per second: 102, episode reward: -62.329, mean reward: -0.733 [-100.000, 10.834], mean action: 1.376 [0.000, 3.000],  loss: 19.188151, mse: 2117.811578, mean_q: 24.815791, mean_eps: 0.803686
  32872/150000: episode: 339, duration: 1.046s, episode steps: 110, steps per second: 105, episode reward: -107.536, mean reward: -0.978 [-100.000,  7.630], mean action: 1.682 [0.000, 3.000],  loss: 11.669262, mse: 2116.790510, mean_q: 26.672875, mean_eps: 0.803101
  32982/150000: episode: 340, duration: 0.957s, episode steps: 110, steps per second: 115, episode reward: -259.306, mean reward: -2.357 [-100.000, 71.768], mean action: 1.827 [0.000, 3.000],  loss: 17.412216, mse: 2078.485348, mean_q: 25.972869, mean_eps: 0.802441
  33118/150000: episode: 341, duration: 1.315s, episode steps: 136, steps per second: 103, episode reward: -160.199, mean reward: -1.178 [-100.000, 13.943], mean action: 1.603 [0.000, 3.000],  loss: 21.052018, mse: 2185.745939, mean_q: 27.554385, mean_eps: 0.801703
  33194/150000: episode: 342, duration: 0.652s, episode steps:  76, steps per second: 117, episode reward: -85.775, mean reward: -1.129 [-100.000,  8.896], mean action: 1.447 [0.000, 3.000],  loss: 12.813104, mse: 2320.631047, mean_q: 27.811487, mean_eps: 0.801067
  33277/150000: episode: 343, duration: 0.753s, episode steps:  83, steps per second: 110, episode reward: -122.973, mean reward: -1.482 [-100.000, 10.231], mean action: 1.578 [0.000, 3.000],  loss: 17.437598, mse: 2213.372209, mean_q: 27.359718, mean_eps: 0.800590
  33370/150000: episode: 344, duration: 0.788s, episode steps:  93, steps per second: 118, episode reward: -114.346, mean reward: -1.230 [-100.000,  6.016], mean action: 1.387 [0.000, 3.000],  loss: 13.836944, mse: 2242.968254, mean_q: 26.405493, mean_eps: 0.800062
  33460/150000: episode: 345, duration: 0.775s, episode steps:  90, steps per second: 116, episode reward: -119.939, mean reward: -1.333 [-100.000,  7.870], mean action: 1.867 [0.000, 3.000],  loss: 13.429192, mse: 2229.383187, mean_q: 26.997427, mean_eps: 0.799513
  33578/150000: episode: 346, duration: 1.060s, episode steps: 118, steps per second: 111, episode reward: -96.011, mean reward: -0.814 [-100.000,  6.059], mean action: 1.678 [0.000, 3.000],  loss: 13.617645, mse: 2217.319313, mean_q: 26.793873, mean_eps: 0.798889
  33692/150000: episode: 347, duration: 1.095s, episode steps: 114, steps per second: 104, episode reward: -37.595, mean reward: -0.330 [-100.000, 13.075], mean action: 1.614 [0.000, 3.000],  loss: 20.673754, mse: 2210.999975, mean_q: 26.563473, mean_eps: 0.798193
  33783/150000: episode: 348, duration: 0.834s, episode steps:  91, steps per second: 109, episode reward: -70.107, mean reward: -0.770 [-100.000,  9.817], mean action: 1.681 [0.000, 3.000],  loss: 14.741638, mse: 2191.386817, mean_q: 26.899869, mean_eps: 0.797578
  33919/150000: episode: 349, duration: 1.188s, episode steps: 136, steps per second: 114, episode reward: -59.420, mean reward: -0.437 [-100.000, 46.247], mean action: 1.434 [0.000, 3.000],  loss: 17.534532, mse: 2195.073061, mean_q: 27.270394, mean_eps: 0.796897
  34037/150000: episode: 350, duration: 1.089s, episode steps: 118, steps per second: 108, episode reward: -54.970, mean reward: -0.466 [-100.000, 21.020], mean action: 1.449 [0.000, 3.000],  loss: 13.917077, mse: 2247.428520, mean_q: 27.997830, mean_eps: 0.796135
  34151/150000: episode: 351, duration: 1.063s, episode steps: 114, steps per second: 107, episode reward: -30.560, mean reward: -0.268 [-100.000, 86.385], mean action: 1.482 [0.000, 3.000],  loss: 15.167348, mse: 2354.638179, mean_q: 27.875372, mean_eps: 0.795439
  34214/150000: episode: 352, duration: 0.721s, episode steps:  63, steps per second:  87, episode reward: -51.032, mean reward: -0.810 [-100.000, 22.620], mean action: 1.508 [0.000, 3.000],  loss: 12.272933, mse: 2352.948986, mean_q: 27.982899, mean_eps: 0.794908
  34295/150000: episode: 353, duration: 0.741s, episode steps:  81, steps per second: 109, episode reward: -100.096, mean reward: -1.236 [-100.000, 11.462], mean action: 1.506 [0.000, 3.000],  loss: 12.224665, mse: 2314.550950, mean_q: 27.746486, mean_eps: 0.794476
  34357/150000: episode: 354, duration: 0.530s, episode steps:  62, steps per second: 117, episode reward: -96.813, mean reward: -1.561 [-100.000,  8.011], mean action: 1.500 [0.000, 3.000],  loss: 12.019119, mse: 2328.401783, mean_q: 27.815635, mean_eps: 0.794047
  34478/150000: episode: 355, duration: 1.070s, episode steps: 121, steps per second: 113, episode reward: -108.912, mean reward: -0.900 [-100.000,  5.766], mean action: 1.570 [0.000, 3.000],  loss: 19.434535, mse: 2368.437570, mean_q: 26.893329, mean_eps: 0.793498
  34605/150000: episode: 356, duration: 1.100s, episode steps: 127, steps per second: 116, episode reward: -117.604, mean reward: -0.926 [-100.000, 14.235], mean action: 1.606 [0.000, 3.000],  loss: 14.875859, mse: 2362.933262, mean_q: 27.041950, mean_eps: 0.792754
  34679/150000: episode: 357, duration: 0.722s, episode steps:  74, steps per second: 103, episode reward: -94.874, mean reward: -1.282 [-100.000, 17.328], mean action: 1.486 [0.000, 3.000],  loss: 11.487939, mse: 2391.445126, mean_q: 28.761335, mean_eps: 0.792151
  34777/150000: episode: 358, duration: 1.037s, episode steps:  98, steps per second:  94, episode reward: -94.424, mean reward: -0.964 [-100.000,  8.516], mean action: 1.480 [0.000, 3.000],  loss: 12.612551, mse: 2417.015358, mean_q: 26.969387, mean_eps: 0.791635
  34893/150000: episode: 359, duration: 1.003s, episode steps: 116, steps per second: 116, episode reward: -84.440, mean reward: -0.728 [-100.000, 14.445], mean action: 1.638 [0.000, 3.000],  loss: 16.582955, mse: 2352.546367, mean_q: 27.681576, mean_eps: 0.790993
  34970/150000: episode: 360, duration: 0.703s, episode steps:  77, steps per second: 110, episode reward: -6.104, mean reward: -0.079 [-100.000, 22.125], mean action: 1.610 [0.000, 3.000],  loss: 15.161727, mse: 2351.430764, mean_q: 26.663276, mean_eps: 0.790414
  35059/150000: episode: 361, duration: 0.773s, episode steps:  89, steps per second: 115, episode reward: -96.371, mean reward: -1.083 [-100.000, 10.460], mean action: 1.607 [0.000, 3.000],  loss: 22.827642, mse: 2375.244260, mean_q: 27.673617, mean_eps: 0.789916
  35123/150000: episode: 362, duration: 0.557s, episode steps:  64, steps per second: 115, episode reward: -102.732, mean reward: -1.605 [-100.000,  9.972], mean action: 1.312 [0.000, 3.000],  loss: 17.939155, mse: 2416.259348, mean_q: 28.676112, mean_eps: 0.789457
  35232/150000: episode: 363, duration: 1.062s, episode steps: 109, steps per second: 103, episode reward: -71.182, mean reward: -0.653 [-100.000,  7.613], mean action: 1.615 [0.000, 3.000],  loss: 22.079882, mse: 2448.743153, mean_q: 27.866770, mean_eps: 0.788938
  35354/150000: episode: 364, duration: 1.177s, episode steps: 122, steps per second: 104, episode reward: -67.001, mean reward: -0.549 [-100.000, 11.530], mean action: 1.705 [0.000, 3.000],  loss: 16.028445, mse: 2413.372029, mean_q: 27.172575, mean_eps: 0.788245
  35418/150000: episode: 365, duration: 0.612s, episode steps:  64, steps per second: 105, episode reward: -44.709, mean reward: -0.699 [-100.000, 10.618], mean action: 1.828 [0.000, 3.000],  loss: 26.318134, mse: 2465.410999, mean_q: 28.095657, mean_eps: 0.787687
  35530/150000: episode: 366, duration: 0.970s, episode steps: 112, steps per second: 115, episode reward: -96.336, mean reward: -0.860 [-100.000,  7.119], mean action: 1.509 [0.000, 3.000],  loss: 13.031955, mse: 2490.449916, mean_q: 28.536087, mean_eps: 0.787159
  35632/150000: episode: 367, duration: 0.902s, episode steps: 102, steps per second: 113, episode reward: -80.813, mean reward: -0.792 [-100.000, 23.085], mean action: 1.598 [0.000, 3.000],  loss: 14.479522, mse: 2424.894936, mean_q: 27.528573, mean_eps: 0.786517
  35767/150000: episode: 368, duration: 1.222s, episode steps: 135, steps per second: 110, episode reward: -120.290, mean reward: -0.891 [-100.000,  4.626], mean action: 1.607 [0.000, 3.000],  loss: 18.422562, mse: 2488.563857, mean_q: 27.829082, mean_eps: 0.785806
  35843/150000: episode: 369, duration: 0.796s, episode steps:  76, steps per second:  95, episode reward: -109.465, mean reward: -1.440 [-100.000, 12.851], mean action: 1.697 [0.000, 3.000],  loss: 12.601873, mse: 2427.912545, mean_q: 28.998950, mean_eps: 0.785173
  35919/150000: episode: 370, duration: 0.711s, episode steps:  76, steps per second: 107, episode reward: -74.117, mean reward: -0.975 [-100.000, 11.008], mean action: 1.592 [0.000, 3.000],  loss: 10.068829, mse: 2417.367340, mean_q: 28.156842, mean_eps: 0.784717
  35996/150000: episode: 371, duration: 0.658s, episode steps:  77, steps per second: 117, episode reward: -67.437, mean reward: -0.876 [-100.000, 11.439], mean action: 1.519 [0.000, 3.000],  loss: 19.617161, mse: 2368.374773, mean_q: 29.596736, mean_eps: 0.784258
  36101/150000: episode: 372, duration: 0.931s, episode steps: 105, steps per second: 113, episode reward: -74.108, mean reward: -0.706 [-100.000, 12.472], mean action: 1.400 [0.000, 3.000],  loss: 18.159176, mse: 2424.458604, mean_q: 28.977175, mean_eps: 0.783712
  36220/150000: episode: 373, duration: 1.082s, episode steps: 119, steps per second: 110, episode reward: -43.551, mean reward: -0.366 [-100.000, 13.183], mean action: 1.521 [0.000, 3.000],  loss: 12.407667, mse: 2460.373037, mean_q: 29.711887, mean_eps: 0.783040
  36296/150000: episode: 374, duration: 0.652s, episode steps:  76, steps per second: 117, episode reward: -142.912, mean reward: -1.880 [-100.000, 10.726], mean action: 1.316 [0.000, 3.000],  loss: 13.863227, mse: 2461.742469, mean_q: 28.829405, mean_eps: 0.782455
  36371/150000: episode: 375, duration: 0.778s, episode steps:  75, steps per second:  96, episode reward: -58.398, mean reward: -0.779 [-100.000, 16.905], mean action: 1.560 [0.000, 3.000],  loss: 18.483385, mse: 2460.230916, mean_q: 28.343635, mean_eps: 0.782002
  36448/150000: episode: 376, duration: 0.696s, episode steps:  77, steps per second: 111, episode reward: -66.926, mean reward: -0.869 [-100.000, 10.691], mean action: 1.403 [0.000, 3.000],  loss: 23.091915, mse: 2462.965979, mean_q: 29.451153, mean_eps: 0.781546
  36517/150000: episode: 377, duration: 0.601s, episode steps:  69, steps per second: 115, episode reward: -61.270, mean reward: -0.888 [-100.000, 10.293], mean action: 1.536 [0.000, 3.000],  loss: 12.374668, mse: 2400.070279, mean_q: 29.328586, mean_eps: 0.781108
  36591/150000: episode: 378, duration: 0.674s, episode steps:  74, steps per second: 110, episode reward: -86.971, mean reward: -1.175 [-100.000,  8.535], mean action: 1.541 [0.000, 3.000],  loss: 12.286025, mse: 2425.931113, mean_q: 30.561877, mean_eps: 0.780679
  36676/150000: episode: 379, duration: 0.835s, episode steps:  85, steps per second: 102, episode reward: -139.321, mean reward: -1.639 [-100.000, 10.170], mean action: 1.365 [0.000, 3.000],  loss: 12.330225, mse: 2407.484050, mean_q: 29.836174, mean_eps: 0.780202
  36748/150000: episode: 380, duration: 0.641s, episode steps:  72, steps per second: 112, episode reward: -47.874, mean reward: -0.665 [-100.000, 11.503], mean action: 1.639 [0.000, 3.000],  loss: 18.424013, mse: 2501.533522, mean_q: 28.492840, mean_eps: 0.779731
  36846/150000: episode: 381, duration: 0.889s, episode steps:  98, steps per second: 110, episode reward: -40.639, mean reward: -0.415 [-100.000, 12.093], mean action: 1.449 [0.000, 3.000],  loss: 13.279269, mse: 2438.172311, mean_q: 30.461460, mean_eps: 0.779221
  36939/150000: episode: 382, duration: 0.962s, episode steps:  93, steps per second:  97, episode reward: -143.282, mean reward: -1.541 [-100.000,  4.856], mean action: 1.806 [0.000, 3.000],  loss: 16.846672, mse: 2423.458094, mean_q: 29.249560, mean_eps: 0.778648
  37043/150000: episode: 383, duration: 0.962s, episode steps: 104, steps per second: 108, episode reward: -135.924, mean reward: -1.307 [-100.000,  9.005], mean action: 1.654 [0.000, 3.000],  loss: 15.372450, mse: 2493.015373, mean_q: 31.644372, mean_eps: 0.778057
  37191/150000: episode: 384, duration: 1.283s, episode steps: 148, steps per second: 115, episode reward: -42.758, mean reward: -0.289 [-100.000, 24.415], mean action: 1.459 [0.000, 3.000],  loss: 15.797727, mse: 2470.086653, mean_q: 30.228915, mean_eps: 0.777301
  37276/150000: episode: 385, duration: 0.748s, episode steps:  85, steps per second: 114, episode reward: -111.541, mean reward: -1.312 [-100.000,  6.353], mean action: 1.471 [0.000, 3.000],  loss: 14.630777, mse: 2475.618230, mean_q: 30.713308, mean_eps: 0.776602
  37341/150000: episode: 386, duration: 0.587s, episode steps:  65, steps per second: 111, episode reward: -52.558, mean reward: -0.809 [-100.000, 10.278], mean action: 1.846 [0.000, 3.000],  loss: 17.422099, mse: 2512.298924, mean_q: 28.960688, mean_eps: 0.776152
  37402/150000: episode: 387, duration: 0.597s, episode steps:  61, steps per second: 102, episode reward: -68.347, mean reward: -1.120 [-100.000,  7.606], mean action: 1.770 [0.000, 3.000],  loss: 11.049178, mse: 2511.095251, mean_q: 29.826624, mean_eps: 0.775774
  37487/150000: episode: 388, duration: 1.013s, episode steps:  85, steps per second:  84, episode reward: -78.980, mean reward: -0.929 [-100.000, 11.554], mean action: 1.518 [0.000, 3.000],  loss: 23.057896, mse: 2485.795246, mean_q: 29.574646, mean_eps: 0.775336
  37635/150000: episode: 389, duration: 1.413s, episode steps: 148, steps per second: 105, episode reward: -14.484, mean reward: -0.098 [-100.000, 88.645], mean action: 1.682 [0.000, 3.000],  loss: 11.597959, mse: 2472.464067, mean_q: 31.092320, mean_eps: 0.774637
  37723/150000: episode: 390, duration: 0.859s, episode steps:  88, steps per second: 102, episode reward: -117.251, mean reward: -1.332 [-100.000,  7.396], mean action: 1.375 [0.000, 3.000],  loss: 10.582835, mse: 2544.450629, mean_q: 31.154191, mean_eps: 0.773929
  37813/150000: episode: 391, duration: 0.849s, episode steps:  90, steps per second: 106, episode reward: -82.383, mean reward: -0.915 [-100.000, 14.034], mean action: 1.656 [0.000, 3.000],  loss: 18.372291, mse: 2465.785049, mean_q: 29.408192, mean_eps: 0.773395
  37896/150000: episode: 392, duration: 0.759s, episode steps:  83, steps per second: 109, episode reward: -93.686, mean reward: -1.129 [-100.000,  7.003], mean action: 1.349 [0.000, 3.000],  loss: 16.828594, mse: 2483.728080, mean_q: 30.112984, mean_eps: 0.772876
  37967/150000: episode: 393, duration: 0.826s, episode steps:  71, steps per second:  86, episode reward: -91.665, mean reward: -1.291 [-100.000, 15.610], mean action: 1.408 [0.000, 3.000],  loss: 15.393808, mse: 2509.420455, mean_q: 29.966468, mean_eps: 0.772414
  38058/150000: episode: 394, duration: 0.844s, episode steps:  91, steps per second: 108, episode reward: -121.249, mean reward: -1.332 [-100.000,  5.500], mean action: 1.670 [0.000, 3.000],  loss: 18.872898, mse: 2517.519451, mean_q: 30.278251, mean_eps: 0.771928
  38178/150000: episode: 395, duration: 1.137s, episode steps: 120, steps per second: 106, episode reward: -66.129, mean reward: -0.551 [-100.000, 13.731], mean action: 1.517 [0.000, 3.000],  loss: 13.730007, mse: 2566.187376, mean_q: 30.495038, mean_eps: 0.771295
  38295/150000: episode: 396, duration: 1.020s, episode steps: 117, steps per second: 115, episode reward: -145.570, mean reward: -1.244 [-100.000,  6.032], mean action: 1.538 [0.000, 3.000],  loss: 15.965254, mse: 2557.942049, mean_q: 31.059069, mean_eps: 0.770584
  38367/150000: episode: 397, duration: 0.631s, episode steps:  72, steps per second: 114, episode reward: -39.421, mean reward: -0.548 [-100.000, 16.492], mean action: 1.542 [0.000, 3.000],  loss: 12.684474, mse: 2665.912706, mean_q: 31.511038, mean_eps: 0.770017
  38461/150000: episode: 398, duration: 0.948s, episode steps:  94, steps per second:  99, episode reward: -89.379, mean reward: -0.951 [-100.000, 11.465], mean action: 1.372 [0.000, 3.000],  loss: 15.702851, mse: 2521.797101, mean_q: 32.154087, mean_eps: 0.769519
  38551/150000: episode: 399, duration: 0.821s, episode steps:  90, steps per second: 110, episode reward: -49.054, mean reward: -0.545 [-100.000, 16.436], mean action: 1.400 [0.000, 3.000],  loss: 12.663936, mse: 2584.040889, mean_q: 31.610630, mean_eps: 0.768967
  38652/150000: episode: 400, duration: 0.936s, episode steps: 101, steps per second: 108, episode reward: -105.957, mean reward: -1.049 [-100.000, 10.308], mean action: 1.574 [0.000, 3.000],  loss: 11.268637, mse: 2555.318630, mean_q: 31.626630, mean_eps: 0.768394
  38721/150000: episode: 401, duration: 0.622s, episode steps:  69, steps per second: 111, episode reward: -69.002, mean reward: -1.000 [-100.000, 16.367], mean action: 1.319 [0.000, 3.000],  loss: 7.672345, mse: 2561.814874, mean_q: 30.607414, mean_eps: 0.767884
  38834/150000: episode: 402, duration: 0.977s, episode steps: 113, steps per second: 116, episode reward: -29.678, mean reward: -0.263 [-100.000,  8.959], mean action: 1.681 [0.000, 3.000],  loss: 12.316808, mse: 2535.550026, mean_q: 30.619913, mean_eps: 0.767338
  38937/150000: episode: 403, duration: 0.944s, episode steps: 103, steps per second: 109, episode reward: -106.560, mean reward: -1.035 [-100.000,  7.741], mean action: 1.612 [0.000, 3.000],  loss: 13.308945, mse: 2546.545394, mean_q: 31.801725, mean_eps: 0.766690
  39010/150000: episode: 404, duration: 0.635s, episode steps:  73, steps per second: 115, episode reward: -84.433, mean reward: -1.157 [-100.000, 16.448], mean action: 1.548 [0.000, 3.000],  loss: 20.070846, mse: 2563.178201, mean_q: 31.310853, mean_eps: 0.766162
  39092/150000: episode: 405, duration: 0.869s, episode steps:  82, steps per second:  94, episode reward: -44.756, mean reward: -0.546 [-100.000, 13.997], mean action: 1.500 [0.000, 3.000],  loss: 17.593041, mse: 2672.512088, mean_q: 33.565435, mean_eps: 0.765697
  39224/150000: episode: 406, duration: 1.254s, episode steps: 132, steps per second: 105, episode reward: -52.743, mean reward: -0.400 [-100.000, 11.558], mean action: 1.447 [0.000, 3.000],  loss: 19.726560, mse: 2650.821030, mean_q: 32.427364, mean_eps: 0.765055
  39298/150000: episode: 407, duration: 0.634s, episode steps:  74, steps per second: 117, episode reward: -70.006, mean reward: -0.946 [-100.000,  6.088], mean action: 1.716 [0.000, 3.000],  loss: 17.772726, mse: 2665.049051, mean_q: 33.070088, mean_eps: 0.764437
  39440/150000: episode: 408, duration: 1.286s, episode steps: 142, steps per second: 110, episode reward: -1.971, mean reward: -0.014 [-100.000, 10.556], mean action: 1.648 [0.000, 3.000],  loss: 14.362672, mse: 2620.484057, mean_q: 31.912151, mean_eps: 0.763789
  39509/150000: episode: 409, duration: 0.638s, episode steps:  69, steps per second: 108, episode reward: -45.660, mean reward: -0.662 [-100.000,  7.129], mean action: 1.580 [0.000, 3.000],  loss: 17.613721, mse: 2641.461143, mean_q: 33.150986, mean_eps: 0.763156
  39612/150000: episode: 410, duration: 1.280s, episode steps: 103, steps per second:  80, episode reward: -84.772, mean reward: -0.823 [-100.000, 14.504], mean action: 1.476 [0.000, 3.000],  loss: 17.688650, mse: 2650.166126, mean_q: 31.383449, mean_eps: 0.762640
  39690/150000: episode: 411, duration: 0.818s, episode steps:  78, steps per second:  95, episode reward: -40.664, mean reward: -0.521 [-100.000, 20.250], mean action: 1.577 [0.000, 3.000],  loss: 12.170719, mse: 2578.851951, mean_q: 31.497373, mean_eps: 0.762097
  39765/150000: episode: 412, duration: 0.840s, episode steps:  75, steps per second:  89, episode reward: -119.274, mean reward: -1.590 [-100.000, 14.848], mean action: 1.640 [0.000, 3.000],  loss: 12.615370, mse: 2615.894108, mean_q: 32.065015, mean_eps: 0.761638
  39927/150000: episode: 413, duration: 1.628s, episode steps: 162, steps per second: 100, episode reward: -59.402, mean reward: -0.367 [-100.000,  9.912], mean action: 1.574 [0.000, 3.000],  loss: 11.117890, mse: 2598.535898, mean_q: 32.835906, mean_eps: 0.760927
  40030/150000: episode: 414, duration: 1.114s, episode steps: 103, steps per second:  92, episode reward: -78.952, mean reward: -0.767 [-100.000, 10.810], mean action: 1.718 [0.000, 3.000],  loss: 13.842944, mse: 2660.627662, mean_q: 33.064553, mean_eps: 0.760132
  40135/150000: episode: 415, duration: 0.968s, episode steps: 105, steps per second: 108, episode reward: -93.077, mean reward: -0.886 [-100.000,  7.773], mean action: 1.619 [0.000, 3.000],  loss: 14.502666, mse: 2829.492726, mean_q: 34.348345, mean_eps: 0.759508
  40248/150000: episode: 416, duration: 1.023s, episode steps: 113, steps per second: 110, episode reward: -104.142, mean reward: -0.922 [-100.000,  6.587], mean action: 1.735 [0.000, 3.000],  loss: 15.471943, mse: 2792.900665, mean_q: 33.635139, mean_eps: 0.758854
  40315/150000: episode: 417, duration: 0.592s, episode steps:  67, steps per second: 113, episode reward: -38.800, mean reward: -0.579 [-100.000, 14.261], mean action: 1.776 [0.000, 3.000],  loss: 10.920354, mse: 2787.939974, mean_q: 35.337385, mean_eps: 0.758314
  40411/150000: episode: 418, duration: 0.829s, episode steps:  96, steps per second: 116, episode reward: -73.825, mean reward: -0.769 [-100.000, 12.050], mean action: 1.573 [0.000, 3.000],  loss: 13.861343, mse: 2760.010965, mean_q: 33.665692, mean_eps: 0.757825
  40518/150000: episode: 419, duration: 0.999s, episode steps: 107, steps per second: 107, episode reward: -126.539, mean reward: -1.183 [-100.000,  5.288], mean action: 1.654 [0.000, 3.000],  loss: 13.537268, mse: 2778.468635, mean_q: 32.299847, mean_eps: 0.757216
  40648/150000: episode: 420, duration: 1.331s, episode steps: 130, steps per second:  98, episode reward: -105.257, mean reward: -0.810 [-100.000, 21.943], mean action: 1.500 [0.000, 3.000],  loss: 12.253681, mse: 2762.014925, mean_q: 33.124191, mean_eps: 0.756505
  40711/150000: episode: 421, duration: 0.607s, episode steps:  63, steps per second: 104, episode reward: -63.685, mean reward: -1.011 [-100.000,  7.190], mean action: 1.413 [0.000, 3.000],  loss: 10.256470, mse: 2794.791072, mean_q: 33.420881, mean_eps: 0.755926
  40804/150000: episode: 422, duration: 0.811s, episode steps:  93, steps per second: 115, episode reward: -76.936, mean reward: -0.827 [-100.000,  7.073], mean action: 1.516 [0.000, 3.000],  loss: 14.766061, mse: 2803.224053, mean_q: 33.126674, mean_eps: 0.755458
  40902/150000: episode: 423, duration: 0.891s, episode steps:  98, steps per second: 110, episode reward: -126.409, mean reward: -1.290 [-100.000, 12.210], mean action: 1.469 [0.000, 3.000],  loss: 15.693743, mse: 2808.272085, mean_q: 33.287403, mean_eps: 0.754885
  41008/150000: episode: 424, duration: 0.977s, episode steps: 106, steps per second: 109, episode reward: -53.736, mean reward: -0.507 [-100.000, 13.432], mean action: 1.670 [0.000, 3.000],  loss: 11.048462, mse: 2787.417470, mean_q: 34.492287, mean_eps: 0.754273
  41101/150000: episode: 425, duration: 0.934s, episode steps:  93, steps per second: 100, episode reward: -90.242, mean reward: -0.970 [-100.000,  7.013], mean action: 1.495 [0.000, 3.000],  loss: 14.337300, mse: 2865.345471, mean_q: 34.749981, mean_eps: 0.753676
  41187/150000: episode: 426, duration: 0.860s, episode steps:  86, steps per second: 100, episode reward: -72.429, mean reward: -0.842 [-100.000,  6.164], mean action: 1.674 [0.000, 3.000],  loss: 16.375422, mse: 2845.616728, mean_q: 34.691173, mean_eps: 0.753139
  41258/150000: episode: 427, duration: 0.651s, episode steps:  71, steps per second: 109, episode reward: -76.948, mean reward: -1.084 [-100.000,  6.733], mean action: 1.507 [0.000, 3.000],  loss: 9.094762, mse: 2894.516051, mean_q: 33.831424, mean_eps: 0.752668
  41380/150000: episode: 428, duration: 1.108s, episode steps: 122, steps per second: 110, episode reward: -128.124, mean reward: -1.050 [-100.000, 23.184], mean action: 1.623 [0.000, 3.000],  loss: 11.047269, mse: 2870.854999, mean_q: 33.822579, mean_eps: 0.752089
  41447/150000: episode: 429, duration: 0.631s, episode steps:  67, steps per second: 106, episode reward: -85.476, mean reward: -1.276 [-100.000, 11.120], mean action: 1.552 [0.000, 3.000],  loss: 6.184748, mse: 2836.704284, mean_q: 36.446209, mean_eps: 0.751522
  41548/150000: episode: 430, duration: 0.901s, episode steps: 101, steps per second: 112, episode reward: -101.126, mean reward: -1.001 [-100.000,  7.853], mean action: 1.574 [0.000, 3.000],  loss: 13.264225, mse: 2886.227225, mean_q: 34.357225, mean_eps: 0.751018
  41621/150000: episode: 431, duration: 0.763s, episode steps:  73, steps per second:  96, episode reward: -41.147, mean reward: -0.564 [-100.000,  7.354], mean action: 1.630 [0.000, 3.000],  loss: 23.141095, mse: 2815.904872, mean_q: 34.647024, mean_eps: 0.750496
  41713/150000: episode: 432, duration: 0.891s, episode steps:  92, steps per second: 103, episode reward: -90.976, mean reward: -0.989 [-100.000, 12.466], mean action: 1.587 [0.000, 3.000],  loss: 17.772629, mse: 2910.711625, mean_q: 35.975522, mean_eps: 0.750001
  41784/150000: episode: 433, duration: 0.642s, episode steps:  71, steps per second: 111, episode reward: -85.628, mean reward: -1.206 [-100.000, 11.265], mean action: 1.563 [0.000, 3.000],  loss: 8.727497, mse: 2874.513080, mean_q: 34.494443, mean_eps: 0.749512
  41899/150000: episode: 434, duration: 1.114s, episode steps: 115, steps per second: 103, episode reward: -79.133, mean reward: -0.688 [-100.000,  7.656], mean action: 1.522 [0.000, 3.000],  loss: 11.563189, mse: 2854.555560, mean_q: 34.034840, mean_eps: 0.748954
  41962/150000: episode: 435, duration: 0.549s, episode steps:  63, steps per second: 115, episode reward: -68.487, mean reward: -1.087 [-100.000,  7.821], mean action: 1.698 [0.000, 3.000],  loss: 11.272234, mse: 2870.575308, mean_q: 34.367366, mean_eps: 0.748420
  42049/150000: episode: 436, duration: 0.781s, episode steps:  87, steps per second: 111, episode reward: -60.734, mean reward: -0.698 [-100.000,  9.614], mean action: 1.701 [0.000, 3.000],  loss: 13.326481, mse: 2914.754016, mean_q: 34.062745, mean_eps: 0.747970
  42176/150000: episode: 437, duration: 1.251s, episode steps: 127, steps per second: 101, episode reward: -61.329, mean reward: -0.483 [-100.000, 12.699], mean action: 1.638 [0.000, 3.000],  loss: 7.791309, mse: 2966.858439, mean_q: 34.529350, mean_eps: 0.747328
  42287/150000: episode: 438, duration: 1.040s, episode steps: 111, steps per second: 107, episode reward: -40.925, mean reward: -0.369 [-100.000, 17.693], mean action: 1.703 [0.000, 3.000],  loss: 8.190210, mse: 2963.094575, mean_q: 33.884598, mean_eps: 0.746614
  42354/150000: episode: 439, duration: 0.608s, episode steps:  67, steps per second: 110, episode reward: -58.290, mean reward: -0.870 [-100.000,  8.662], mean action: 1.627 [0.000, 3.000],  loss: 15.545132, mse: 3009.188440, mean_q: 35.691245, mean_eps: 0.746080
  42502/150000: episode: 440, duration: 1.299s, episode steps: 148, steps per second: 114, episode reward: -147.321, mean reward: -0.995 [-100.000,  8.758], mean action: 1.520 [0.000, 3.000],  loss: 13.061288, mse: 3020.578212, mean_q: 35.110825, mean_eps: 0.745435
  42601/150000: episode: 441, duration: 0.921s, episode steps:  99, steps per second: 107, episode reward: -83.157, mean reward: -0.840 [-100.000, 20.963], mean action: 1.747 [0.000, 3.000],  loss: 13.374307, mse: 3046.224242, mean_q: 34.871013, mean_eps: 0.744694
  42673/150000: episode: 442, duration: 0.735s, episode steps:  72, steps per second:  98, episode reward: -52.533, mean reward: -0.730 [-100.000,  8.409], mean action: 1.556 [0.000, 3.000],  loss: 9.284350, mse: 2977.957638, mean_q: 34.566555, mean_eps: 0.744181
  42798/150000: episode: 443, duration: 1.270s, episode steps: 125, steps per second:  98, episode reward: -91.712, mean reward: -0.734 [-100.000,  5.535], mean action: 1.432 [0.000, 3.000],  loss: 13.069812, mse: 2983.732176, mean_q: 34.470404, mean_eps: 0.743590
  42875/150000: episode: 444, duration: 0.685s, episode steps:  77, steps per second: 112, episode reward: -56.976, mean reward: -0.740 [-100.000,  9.804], mean action: 1.584 [0.000, 3.000],  loss: 10.764988, mse: 2922.211036, mean_q: 34.566381, mean_eps: 0.742984
  42996/150000: episode: 445, duration: 1.121s, episode steps: 121, steps per second: 108, episode reward: -130.518, mean reward: -1.079 [-100.000,  5.935], mean action: 1.463 [0.000, 3.000],  loss: 8.307627, mse: 3028.150685, mean_q: 36.065477, mean_eps: 0.742390
  43092/150000: episode: 446, duration: 0.855s, episode steps:  96, steps per second: 112, episode reward: -98.045, mean reward: -1.021 [-100.000,  7.996], mean action: 1.625 [0.000, 3.000],  loss: 12.092931, mse: 3067.795858, mean_q: 34.418214, mean_eps: 0.741739
  43201/150000: episode: 447, duration: 0.992s, episode steps: 109, steps per second: 110, episode reward: -50.847, mean reward: -0.466 [-100.000, 22.737], mean action: 1.349 [0.000, 3.000],  loss: 14.200986, mse: 3150.222495, mean_q: 35.087853, mean_eps: 0.741124
  43319/150000: episode: 448, duration: 1.176s, episode steps: 118, steps per second: 100, episode reward: -60.571, mean reward: -0.513 [-100.000, 11.360], mean action: 1.517 [0.000, 3.000],  loss: 9.134136, mse: 3128.557205, mean_q: 35.962775, mean_eps: 0.740443
  43381/150000: episode: 449, duration: 0.572s, episode steps:  62, steps per second: 108, episode reward: -40.639, mean reward: -0.655 [-100.000,  8.151], mean action: 1.532 [0.000, 3.000],  loss: 12.423124, mse: 3140.178353, mean_q: 37.565837, mean_eps: 0.739903
  43450/150000: episode: 450, duration: 0.643s, episode steps:  69, steps per second: 107, episode reward: -56.280, mean reward: -0.816 [-100.000,  7.451], mean action: 1.493 [0.000, 3.000],  loss: 13.252947, mse: 3071.804539, mean_q: 34.789483, mean_eps: 0.739510
  43513/150000: episode: 451, duration: 0.599s, episode steps:  63, steps per second: 105, episode reward: -64.553, mean reward: -1.025 [-100.000,  6.273], mean action: 1.524 [0.000, 3.000],  loss: 7.468483, mse: 3132.246877, mean_q: 35.596846, mean_eps: 0.739114
  43641/150000: episode: 452, duration: 1.233s, episode steps: 128, steps per second: 104, episode reward: -112.518, mean reward: -0.879 [-100.000,  6.034], mean action: 1.820 [0.000, 3.000],  loss: 10.728800, mse: 3111.261250, mean_q: 35.240588, mean_eps: 0.738541
  43716/150000: episode: 453, duration: 0.769s, episode steps:  75, steps per second:  97, episode reward: -90.785, mean reward: -1.210 [-100.000,  6.922], mean action: 1.507 [0.000, 3.000],  loss: 9.922271, mse: 3196.571637, mean_q: 37.477060, mean_eps: 0.737932
  43823/150000: episode: 454, duration: 1.102s, episode steps: 107, steps per second:  97, episode reward: -114.774, mean reward: -1.073 [-100.000,  7.048], mean action: 1.673 [0.000, 3.000],  loss: 11.928066, mse: 3053.599965, mean_q: 34.923561, mean_eps: 0.737386
  43917/150000: episode: 455, duration: 0.995s, episode steps:  94, steps per second:  94, episode reward: -41.042, mean reward: -0.437 [-100.000, 11.107], mean action: 1.415 [0.000, 3.000],  loss: 8.454152, mse: 3155.196697, mean_q: 35.577950, mean_eps: 0.736783
  43982/150000: episode: 456, duration: 0.686s, episode steps:  65, steps per second:  95, episode reward: -89.641, mean reward: -1.379 [-100.000, 12.932], mean action: 1.446 [0.000, 3.000],  loss: 5.705954, mse: 3232.021402, mean_q: 37.311214, mean_eps: 0.736306
  44090/150000: episode: 457, duration: 1.126s, episode steps: 108, steps per second:  96, episode reward: -143.229, mean reward: -1.326 [-100.000,  6.807], mean action: 1.648 [0.000, 3.000],  loss: 17.272097, mse: 3176.227064, mean_q: 37.068125, mean_eps: 0.735787
  44175/150000: episode: 458, duration: 0.921s, episode steps:  85, steps per second:  92, episode reward: -71.190, mean reward: -0.838 [-100.000, 18.736], mean action: 1.400 [0.000, 3.000],  loss: 12.025203, mse: 3273.672737, mean_q: 36.925437, mean_eps: 0.735208
  44293/150000: episode: 459, duration: 1.392s, episode steps: 118, steps per second:  85, episode reward: -68.534, mean reward: -0.581 [-100.000, 77.627], mean action: 1.737 [0.000, 3.000],  loss: 8.945529, mse: 3245.990164, mean_q: 37.695077, mean_eps: 0.734599
  44408/150000: episode: 460, duration: 1.355s, episode steps: 115, steps per second:  85, episode reward: -63.011, mean reward: -0.548 [-100.000, 13.577], mean action: 1.574 [0.000, 3.000],  loss: 17.974249, mse: 3250.646580, mean_q: 37.369651, mean_eps: 0.733900
  44500/150000: episode: 461, duration: 1.075s, episode steps:  92, steps per second:  86, episode reward: -77.462, mean reward: -0.842 [-100.000, 24.110], mean action: 1.402 [0.000, 3.000],  loss: 14.461650, mse: 3214.887318, mean_q: 38.421870, mean_eps: 0.733279
  44580/150000: episode: 462, duration: 0.872s, episode steps:  80, steps per second:  92, episode reward: -46.985, mean reward: -0.587 [-100.000, 17.127], mean action: 1.725 [0.000, 3.000],  loss: 8.653370, mse: 3200.599036, mean_q: 36.033042, mean_eps: 0.732763
  44693/150000: episode: 463, duration: 1.291s, episode steps: 113, steps per second:  88, episode reward: -106.562, mean reward: -0.943 [-100.000, 18.611], mean action: 1.566 [0.000, 3.000],  loss: 12.390722, mse: 3279.483146, mean_q: 37.557234, mean_eps: 0.732184
  44762/150000: episode: 464, duration: 0.668s, episode steps:  69, steps per second: 103, episode reward: -56.768, mean reward: -0.823 [-100.000, 11.909], mean action: 1.623 [0.000, 3.000],  loss: 9.344792, mse: 3254.286218, mean_q: 37.373729, mean_eps: 0.731638
  44891/150000: episode: 465, duration: 1.196s, episode steps: 129, steps per second: 108, episode reward: -27.815, mean reward: -0.216 [-100.000, 17.162], mean action: 1.698 [0.000, 3.000],  loss: 11.838599, mse: 3312.542704, mean_q: 37.302076, mean_eps: 0.731044
  45009/150000: episode: 466, duration: 1.107s, episode steps: 118, steps per second: 107, episode reward: -219.632, mean reward: -1.861 [-100.000,  4.396], mean action: 1.780 [0.000, 3.000],  loss: 13.557454, mse: 3265.922856, mean_q: 36.314616, mean_eps: 0.730303
  45095/150000: episode: 467, duration: 0.789s, episode steps:  86, steps per second: 109, episode reward: -80.646, mean reward: -0.938 [-100.000,  9.475], mean action: 1.605 [0.000, 3.000],  loss: 10.833846, mse: 3417.910179, mean_q: 37.859293, mean_eps: 0.729691
  45187/150000: episode: 468, duration: 1.018s, episode steps:  92, steps per second:  90, episode reward: -46.828, mean reward: -0.509 [-100.000, 15.092], mean action: 1.609 [0.000, 3.000],  loss: 10.317984, mse: 3426.400807, mean_q: 39.296211, mean_eps: 0.729157
  45310/150000: episode: 469, duration: 1.145s, episode steps: 123, steps per second: 107, episode reward: -71.190, mean reward: -0.579 [-100.000, 17.571], mean action: 1.341 [0.000, 3.000],  loss: 7.536599, mse: 3429.033568, mean_q: 37.601502, mean_eps: 0.728512
  45407/150000: episode: 470, duration: 0.900s, episode steps:  97, steps per second: 108, episode reward: -112.087, mean reward: -1.156 [-100.000,  9.850], mean action: 1.351 [0.000, 3.000],  loss: 12.961215, mse: 3419.748080, mean_q: 36.824625, mean_eps: 0.727852
  45486/150000: episode: 471, duration: 0.695s, episode steps:  79, steps per second: 114, episode reward: -43.087, mean reward: -0.545 [-100.000,  6.142], mean action: 1.785 [0.000, 3.000],  loss: 10.667492, mse: 3442.689691, mean_q: 37.051533, mean_eps: 0.727324
  45612/150000: episode: 472, duration: 1.171s, episode steps: 126, steps per second: 108, episode reward: -68.855, mean reward: -0.546 [-100.000, 10.614], mean action: 1.516 [0.000, 3.000],  loss: 11.542019, mse: 3401.715937, mean_q: 38.389037, mean_eps: 0.726709
  45714/150000: episode: 473, duration: 1.027s, episode steps: 102, steps per second:  99, episode reward: -184.765, mean reward: -1.811 [-100.000, 18.591], mean action: 1.657 [0.000, 3.000],  loss: 13.133252, mse: 3405.065054, mean_q: 38.191839, mean_eps: 0.726025
  45790/150000: episode: 474, duration: 0.711s, episode steps:  76, steps per second: 107, episode reward: -72.702, mean reward: -0.957 [-100.000,  9.901], mean action: 1.539 [0.000, 3.000],  loss: 6.868975, mse: 3382.610573, mean_q: 38.624828, mean_eps: 0.725491
  45854/150000: episode: 475, duration: 0.611s, episode steps:  64, steps per second: 105, episode reward: -24.275, mean reward: -0.379 [-100.000, 16.230], mean action: 1.781 [0.000, 3.000],  loss: 12.523949, mse: 3487.514153, mean_q: 39.037316, mean_eps: 0.725071
  45945/150000: episode: 476, duration: 0.820s, episode steps:  91, steps per second: 111, episode reward: -82.088, mean reward: -0.902 [-100.000,  7.672], mean action: 1.516 [0.000, 3.000],  loss: 8.966018, mse: 3406.430160, mean_q: 37.876489, mean_eps: 0.724606
  46081/150000: episode: 477, duration: 1.230s, episode steps: 136, steps per second: 111, episode reward: -188.997, mean reward: -1.390 [-100.000, 35.006], mean action: 1.537 [0.000, 3.000],  loss: 8.566346, mse: 3466.988782, mean_q: 37.999387, mean_eps: 0.723925
  46193/150000: episode: 478, duration: 1.039s, episode steps: 112, steps per second: 108, episode reward: -92.661, mean reward: -0.827 [-100.000,  8.408], mean action: 1.545 [0.000, 3.000],  loss: 9.398062, mse: 3492.871961, mean_q: 37.348311, mean_eps: 0.723181
  46275/150000: episode: 479, duration: 0.898s, episode steps:  82, steps per second:  91, episode reward: -28.881, mean reward: -0.352 [-100.000, 11.525], mean action: 1.695 [0.000, 3.000],  loss: 8.551666, mse: 3500.785430, mean_q: 37.313958, mean_eps: 0.722599
  46359/150000: episode: 480, duration: 0.902s, episode steps:  84, steps per second:  93, episode reward: -102.330, mean reward: -1.218 [-100.000, 10.641], mean action: 1.619 [0.000, 3.000],  loss: 18.426249, mse: 3563.636103, mean_q: 39.396765, mean_eps: 0.722101
  46462/150000: episode: 481, duration: 1.039s, episode steps: 103, steps per second:  99, episode reward: -92.913, mean reward: -0.902 [-100.000, 10.497], mean action: 1.631 [0.000, 3.000],  loss: 12.245312, mse: 3478.879653, mean_q: 38.070612, mean_eps: 0.721540
  46563/150000: episode: 482, duration: 1.119s, episode steps: 101, steps per second:  90, episode reward: -78.253, mean reward: -0.775 [-100.000,  6.789], mean action: 1.782 [0.000, 3.000],  loss: 10.788025, mse: 3514.905672, mean_q: 39.102019, mean_eps: 0.720928
  46640/150000: episode: 483, duration: 0.827s, episode steps:  77, steps per second:  93, episode reward: -90.157, mean reward: -1.171 [-100.000, 19.758], mean action: 1.727 [0.000, 3.000],  loss: 7.715221, mse: 3507.521424, mean_q: 38.438441, mean_eps: 0.720394
  46767/150000: episode: 484, duration: 1.401s, episode steps: 127, steps per second:  91, episode reward: -114.569, mean reward: -0.902 [-100.000, 10.535], mean action: 1.677 [0.000, 3.000],  loss: 13.357085, mse: 3560.831501, mean_q: 38.716712, mean_eps: 0.719782
  46823/150000: episode: 485, duration: 0.515s, episode steps:  56, steps per second: 109, episode reward: -65.220, mean reward: -1.165 [-100.000, 10.197], mean action: 1.607 [0.000, 3.000],  loss: 12.628721, mse: 3520.325636, mean_q: 38.837211, mean_eps: 0.719233
  46925/150000: episode: 486, duration: 0.922s, episode steps: 102, steps per second: 111, episode reward: -205.847, mean reward: -2.018 [-100.000,  5.069], mean action: 1.402 [0.000, 3.000],  loss: 9.845943, mse: 3617.358813, mean_q: 38.970782, mean_eps: 0.718759
  47029/150000: episode: 487, duration: 0.930s, episode steps: 104, steps per second: 112, episode reward: -78.390, mean reward: -0.754 [-100.000, 11.100], mean action: 1.462 [0.000, 3.000],  loss: 12.728500, mse: 3478.762773, mean_q: 37.018097, mean_eps: 0.718141
  47141/150000: episode: 488, duration: 1.058s, episode steps: 112, steps per second: 106, episode reward: -70.538, mean reward: -0.630 [-100.000,  8.560], mean action: 1.446 [0.000, 3.000],  loss: 13.700658, mse: 3468.448975, mean_q: 38.814035, mean_eps: 0.717493
  47262/150000: episode: 489, duration: 1.241s, episode steps: 121, steps per second:  98, episode reward: -179.621, mean reward: -1.484 [-100.000,  5.322], mean action: 1.736 [0.000, 3.000],  loss: 7.709946, mse: 3474.136854, mean_q: 38.429266, mean_eps: 0.716794
  47364/150000: episode: 490, duration: 0.969s, episode steps: 102, steps per second: 105, episode reward: -42.716, mean reward: -0.419 [-100.000, 16.301], mean action: 1.627 [0.000, 3.000],  loss: 10.404428, mse: 3544.770946, mean_q: 39.417725, mean_eps: 0.716125
  47452/150000: episode: 491, duration: 0.798s, episode steps:  88, steps per second: 110, episode reward: -89.871, mean reward: -1.021 [-100.000, 18.226], mean action: 1.670 [0.000, 3.000],  loss: 12.655464, mse: 3483.387937, mean_q: 37.568086, mean_eps: 0.715555
  47524/150000: episode: 492, duration: 0.739s, episode steps:  72, steps per second:  97, episode reward: -116.845, mean reward: -1.623 [-100.000,  6.567], mean action: 1.764 [0.000, 3.000],  loss: 9.049795, mse: 3447.842495, mean_q: 37.225150, mean_eps: 0.715075
  47591/150000: episode: 493, duration: 0.746s, episode steps:  67, steps per second:  90, episode reward: -64.807, mean reward: -0.967 [-100.000,  9.482], mean action: 1.522 [0.000, 3.000],  loss: 14.896663, mse: 3469.557286, mean_q: 38.211651, mean_eps: 0.714658
  47679/150000: episode: 494, duration: 0.930s, episode steps:  88, steps per second:  95, episode reward: -67.040, mean reward: -0.762 [-100.000, 14.785], mean action: 1.557 [0.000, 3.000],  loss: 11.847806, mse: 3427.887412, mean_q: 38.812630, mean_eps: 0.714193
  47806/150000: episode: 495, duration: 1.253s, episode steps: 127, steps per second: 101, episode reward: -103.824, mean reward: -0.818 [-100.000, 12.988], mean action: 1.480 [0.000, 3.000],  loss: 11.265850, mse: 3403.313271, mean_q: 37.752810, mean_eps: 0.713548
  47897/150000: episode: 496, duration: 0.936s, episode steps:  91, steps per second:  97, episode reward: -73.243, mean reward: -0.805 [-100.000, 17.416], mean action: 1.758 [0.000, 3.000],  loss: 8.616768, mse: 3571.074683, mean_q: 40.082003, mean_eps: 0.712894
  48024/150000: episode: 497, duration: 1.331s, episode steps: 127, steps per second:  95, episode reward: -104.368, mean reward: -0.822 [-100.000, 50.887], mean action: 1.472 [0.000, 3.000],  loss: 12.061189, mse: 3483.821326, mean_q: 39.642522, mean_eps: 0.712240
  48142/150000: episode: 498, duration: 1.071s, episode steps: 118, steps per second: 110, episode reward: -70.383, mean reward: -0.596 [-100.000, 11.628], mean action: 1.517 [0.000, 3.000],  loss: 13.842574, mse: 3514.922765, mean_q: 37.932020, mean_eps: 0.711505
  48207/150000: episode: 499, duration: 0.675s, episode steps:  65, steps per second:  96, episode reward: -78.341, mean reward: -1.205 [-100.000, 10.749], mean action: 1.462 [0.000, 3.000],  loss: 15.924087, mse: 3553.921499, mean_q: 41.117451, mean_eps: 0.710956
  48329/150000: episode: 500, duration: 1.227s, episode steps: 122, steps per second:  99, episode reward: -49.970, mean reward: -0.410 [-100.000, 11.730], mean action: 1.434 [0.000, 3.000],  loss: 10.606212, mse: 3521.029307, mean_q: 38.556570, mean_eps: 0.710395
  48419/150000: episode: 501, duration: 0.815s, episode steps:  90, steps per second: 110, episode reward: -80.194, mean reward: -0.891 [-100.000, 17.532], mean action: 1.589 [0.000, 3.000],  loss: 10.831764, mse: 3504.562690, mean_q: 38.658309, mean_eps: 0.709759
  48505/150000: episode: 502, duration: 0.821s, episode steps:  86, steps per second: 105, episode reward:  2.495, mean reward:  0.029 [-100.000, 15.936], mean action: 1.733 [0.000, 3.000],  loss: 15.306438, mse: 3633.730773, mean_q: 38.706694, mean_eps: 0.709231
  48577/150000: episode: 503, duration: 0.645s, episode steps:  72, steps per second: 112, episode reward: -86.833, mean reward: -1.206 [-100.000, 14.130], mean action: 1.806 [0.000, 3.000],  loss: 15.209876, mse: 3571.293454, mean_q: 39.130533, mean_eps: 0.708757
  48667/150000: episode: 504, duration: 0.832s, episode steps:  90, steps per second: 108, episode reward: -67.992, mean reward: -0.755 [-100.000, 14.950], mean action: 1.478 [0.000, 3.000],  loss: 18.309944, mse: 3607.394805, mean_q: 39.312942, mean_eps: 0.708271
  48770/150000: episode: 505, duration: 1.075s, episode steps: 103, steps per second:  96, episode reward: -61.898, mean reward: -0.601 [-100.000, 12.174], mean action: 1.680 [0.000, 3.000],  loss: 8.442394, mse: 3547.989471, mean_q: 37.708748, mean_eps: 0.707692
  48874/150000: episode: 506, duration: 1.019s, episode steps: 104, steps per second: 102, episode reward: -64.456, mean reward: -0.620 [-100.000,  8.555], mean action: 1.865 [0.000, 3.000],  loss: 9.870469, mse: 3578.889670, mean_q: 39.113024, mean_eps: 0.707071
  48956/150000: episode: 507, duration: 0.822s, episode steps:  82, steps per second: 100, episode reward: -117.112, mean reward: -1.428 [-100.000, 11.151], mean action: 1.573 [0.000, 3.000],  loss: 9.848100, mse: 3519.664250, mean_q: 37.945583, mean_eps: 0.706513
  49029/150000: episode: 508, duration: 0.673s, episode steps:  73, steps per second: 109, episode reward: -9.814, mean reward: -0.134 [-100.000, 22.226], mean action: 1.658 [0.000, 3.000],  loss: 7.853459, mse: 3581.270591, mean_q: 39.062500, mean_eps: 0.706048
  49138/150000: episode: 509, duration: 0.998s, episode steps: 109, steps per second: 109, episode reward: -90.429, mean reward: -0.830 [-100.000, 21.338], mean action: 1.486 [0.000, 3.000],  loss: 11.639428, mse: 3645.253017, mean_q: 39.799002, mean_eps: 0.705502
  49217/150000: episode: 510, duration: 0.800s, episode steps:  79, steps per second:  99, episode reward: -39.668, mean reward: -0.502 [-100.000, 18.047], mean action: 1.785 [0.000, 3.000],  loss: 12.004613, mse: 3588.033478, mean_q: 40.347519, mean_eps: 0.704938
  49336/150000: episode: 511, duration: 1.109s, episode steps: 119, steps per second: 107, episode reward: -20.527, mean reward: -0.172 [-100.000, 22.061], mean action: 1.563 [0.000, 3.000],  loss: 7.939484, mse: 3644.343358, mean_q: 39.655451, mean_eps: 0.704344
  49416/150000: episode: 512, duration: 0.860s, episode steps:  80, steps per second:  93, episode reward: -85.639, mean reward: -1.070 [-100.000,  7.063], mean action: 1.800 [0.000, 3.000],  loss: 11.621961, mse: 3620.614810, mean_q: 39.433239, mean_eps: 0.703747
  49538/150000: episode: 513, duration: 1.150s, episode steps: 122, steps per second: 106, episode reward: -47.298, mean reward: -0.388 [-100.000, 12.052], mean action: 1.730 [0.000, 3.000],  loss: 9.122770, mse: 3635.951476, mean_q: 38.895959, mean_eps: 0.703141
  49639/150000: episode: 514, duration: 0.962s, episode steps: 101, steps per second: 105, episode reward: -63.903, mean reward: -0.633 [-100.000,  8.066], mean action: 1.554 [0.000, 3.000],  loss: 11.916575, mse: 3634.284098, mean_q: 38.759453, mean_eps: 0.702472
  49705/150000: episode: 515, duration: 0.586s, episode steps:  66, steps per second: 113, episode reward: -99.819, mean reward: -1.512 [-100.000, 16.691], mean action: 1.561 [0.000, 3.000],  loss: 7.072892, mse: 3653.982278, mean_q: 38.651720, mean_eps: 0.701971
  49772/150000: episode: 516, duration: 0.684s, episode steps:  67, steps per second:  98, episode reward: -75.154, mean reward: -1.122 [-100.000, 13.762], mean action: 1.478 [0.000, 3.000],  loss: 9.825305, mse: 3608.851810, mean_q: 39.374662, mean_eps: 0.701572
  49859/150000: episode: 517, duration: 0.851s, episode steps:  87, steps per second: 102, episode reward: -123.126, mean reward: -1.415 [-100.000, 10.409], mean action: 1.402 [0.000, 3.000],  loss: 12.810536, mse: 3555.339075, mean_q: 38.603234, mean_eps: 0.701110
  49994/150000: episode: 518, duration: 1.285s, episode steps: 135, steps per second: 105, episode reward: -28.331, mean reward: -0.210 [-100.000, 77.773], mean action: 1.741 [0.000, 3.000],  loss: 11.828628, mse: 3622.428751, mean_q: 39.792344, mean_eps: 0.700444
  50092/150000: episode: 519, duration: 0.945s, episode steps:  98, steps per second: 104, episode reward: -122.706, mean reward: -1.252 [-100.000,  9.260], mean action: 1.786 [0.000, 3.000],  loss: 11.932284, mse: 3712.316304, mean_q: 41.985776, mean_eps: 0.699745
  50200/150000: episode: 520, duration: 0.996s, episode steps: 108, steps per second: 108, episode reward: -93.345, mean reward: -0.864 [-100.000,  6.652], mean action: 1.694 [0.000, 3.000],  loss: 13.218032, mse: 3700.658813, mean_q: 41.640194, mean_eps: 0.699127
  50356/150000: episode: 521, duration: 1.581s, episode steps: 156, steps per second:  99, episode reward: -98.507, mean reward: -0.631 [-100.000,  5.750], mean action: 1.596 [0.000, 3.000],  loss: 9.116450, mse: 3748.161809, mean_q: 41.699375, mean_eps: 0.698335
  50471/150000: episode: 522, duration: 1.123s, episode steps: 115, steps per second: 102, episode reward: -72.476, mean reward: -0.630 [-100.000, 16.607], mean action: 1.487 [0.000, 3.000],  loss: 10.229431, mse: 3686.677819, mean_q: 39.778028, mean_eps: 0.697522
  50570/150000: episode: 523, duration: 0.951s, episode steps:  99, steps per second: 104, episode reward: -89.601, mean reward: -0.905 [-100.000, 10.929], mean action: 1.556 [0.000, 3.000],  loss: 10.587029, mse: 3727.755591, mean_q: 41.080061, mean_eps: 0.696880
  50638/150000: episode: 524, duration: 0.611s, episode steps:  68, steps per second: 111, episode reward: -65.408, mean reward: -0.962 [-100.000, 12.141], mean action: 1.897 [0.000, 3.000],  loss: 10.527935, mse: 3719.297812, mean_q: 40.993902, mean_eps: 0.696379
  50732/150000: episode: 525, duration: 0.874s, episode steps:  94, steps per second: 108, episode reward: -90.757, mean reward: -0.966 [-100.000, 21.285], mean action: 1.564 [0.000, 3.000],  loss: 14.967046, mse: 3727.700439, mean_q: 41.749175, mean_eps: 0.695893
  50814/150000: episode: 526, duration: 0.850s, episode steps:  82, steps per second:  96, episode reward: -95.910, mean reward: -1.170 [-100.000, 11.137], mean action: 1.549 [0.000, 3.000],  loss: 13.074201, mse: 3766.928705, mean_q: 41.237926, mean_eps: 0.695365
  50935/150000: episode: 527, duration: 1.194s, episode steps: 121, steps per second: 101, episode reward: -59.437, mean reward: -0.491 [-100.000, 15.802], mean action: 1.587 [0.000, 3.000],  loss: 11.060994, mse: 3745.352727, mean_q: 42.271022, mean_eps: 0.694756
  51009/150000: episode: 528, duration: 0.774s, episode steps:  74, steps per second:  96, episode reward: -73.847, mean reward: -0.998 [-100.000, 13.284], mean action: 1.635 [0.000, 3.000],  loss: 9.331850, mse: 3734.465956, mean_q: 42.138550, mean_eps: 0.694171
  51104/150000: episode: 529, duration: 1.031s, episode steps:  95, steps per second:  92, episode reward: -148.957, mean reward: -1.568 [-100.000, 14.400], mean action: 1.800 [0.000, 3.000],  loss: 12.653214, mse: 3908.493809, mean_q: 42.647816, mean_eps: 0.693664
  51198/150000: episode: 530, duration: 1.005s, episode steps:  94, steps per second:  93, episode reward: -36.425, mean reward: -0.387 [-100.000,  9.618], mean action: 1.681 [0.000, 3.000],  loss: 7.988996, mse: 3859.324040, mean_q: 42.690321, mean_eps: 0.693097
  51297/150000: episode: 531, duration: 1.039s, episode steps:  99, steps per second:  95, episode reward: -95.577, mean reward: -0.965 [-100.000,  7.406], mean action: 1.606 [0.000, 3.000],  loss: 10.017768, mse: 3912.132953, mean_q: 42.266649, mean_eps: 0.692518
  51367/150000: episode: 532, duration: 0.751s, episode steps:  70, steps per second:  93, episode reward: -41.918, mean reward: -0.599 [-100.000, 11.728], mean action: 1.600 [0.000, 3.000],  loss: 7.851998, mse: 3907.484933, mean_q: 42.463496, mean_eps: 0.692011
  51474/150000: episode: 533, duration: 1.050s, episode steps: 107, steps per second: 102, episode reward: -104.536, mean reward: -0.977 [-100.000, 11.361], mean action: 1.402 [0.000, 3.000],  loss: 8.572140, mse: 3874.834972, mean_q: 42.233172, mean_eps: 0.691480
  51543/150000: episode: 534, duration: 0.719s, episode steps:  69, steps per second:  96, episode reward: -44.461, mean reward: -0.644 [-100.000, 15.727], mean action: 1.725 [0.000, 3.000],  loss: 12.420575, mse: 3914.727160, mean_q: 42.012243, mean_eps: 0.690952
  51627/150000: episode: 535, duration: 0.846s, episode steps:  84, steps per second:  99, episode reward: -68.340, mean reward: -0.814 [-100.000, 13.544], mean action: 1.702 [0.000, 3.000],  loss: 10.509628, mse: 3838.494431, mean_q: 42.154567, mean_eps: 0.690493
  51689/150000: episode: 536, duration: 0.564s, episode steps:  62, steps per second: 110, episode reward: -71.124, mean reward: -1.147 [-100.000, 11.978], mean action: 1.694 [0.000, 3.000],  loss: 11.401573, mse: 3875.301021, mean_q: 41.860437, mean_eps: 0.690055
  51789/150000: episode: 537, duration: 1.086s, episode steps: 100, steps per second:  92, episode reward: -72.943, mean reward: -0.729 [-100.000, 24.878], mean action: 1.630 [0.000, 3.000],  loss: 7.781786, mse: 3863.821360, mean_q: 42.128586, mean_eps: 0.689569
  51910/150000: episode: 538, duration: 1.128s, episode steps: 121, steps per second: 107, episode reward: -111.748, mean reward: -0.924 [-100.000, 15.467], mean action: 1.612 [0.000, 3.000],  loss: 13.029976, mse: 3907.495474, mean_q: 42.678296, mean_eps: 0.688906
  52023/150000: episode: 539, duration: 1.133s, episode steps: 113, steps per second: 100, episode reward: -118.247, mean reward: -1.046 [-100.000,  6.096], mean action: 1.584 [0.000, 3.000],  loss: 11.730591, mse: 3895.672856, mean_q: 42.234558, mean_eps: 0.688204
  52094/150000: episode: 540, duration: 0.651s, episode steps:  71, steps per second: 109, episode reward: -5.149, mean reward: -0.073 [-100.000, 13.489], mean action: 1.521 [0.000, 3.000],  loss: 17.928399, mse: 3940.882672, mean_q: 41.849048, mean_eps: 0.687652
  52207/150000: episode: 541, duration: 1.003s, episode steps: 113, steps per second: 113, episode reward: -89.623, mean reward: -0.793 [-100.000,  7.952], mean action: 1.345 [0.000, 3.000],  loss: 8.319171, mse: 4011.784342, mean_q: 43.187632, mean_eps: 0.687100
  52279/150000: episode: 542, duration: 0.781s, episode steps:  72, steps per second:  92, episode reward: -39.014, mean reward: -0.542 [-100.000,  9.708], mean action: 1.667 [0.000, 3.000],  loss: 6.488984, mse: 4050.786563, mean_q: 43.365967, mean_eps: 0.686545
  52402/150000: episode: 543, duration: 1.146s, episode steps: 123, steps per second: 107, episode reward: -78.599, mean reward: -0.639 [-100.000,  7.141], mean action: 1.642 [0.000, 3.000],  loss: 13.358720, mse: 4048.533781, mean_q: 44.198472, mean_eps: 0.685960
  52514/150000: episode: 544, duration: 1.110s, episode steps: 112, steps per second: 101, episode reward: -109.763, mean reward: -0.980 [-100.000,  8.428], mean action: 1.438 [0.000, 3.000],  loss: 9.286022, mse: 4009.067625, mean_q: 43.256227, mean_eps: 0.685255
  52611/150000: episode: 545, duration: 0.913s, episode steps:  97, steps per second: 106, episode reward: -44.308, mean reward: -0.457 [-100.000, 11.220], mean action: 1.804 [0.000, 3.000],  loss: 14.409933, mse: 4028.605990, mean_q: 45.520305, mean_eps: 0.684628
  52673/150000: episode: 546, duration: 0.602s, episode steps:  62, steps per second: 103, episode reward: -43.880, mean reward: -0.708 [-100.000, 14.456], mean action: 1.710 [0.000, 3.000],  loss: 4.678151, mse: 3916.318761, mean_q: 42.160959, mean_eps: 0.684151
  52741/150000: episode: 547, duration: 0.692s, episode steps:  68, steps per second:  98, episode reward: -48.408, mean reward: -0.712 [-100.000,  7.887], mean action: 1.853 [0.000, 3.000],  loss: 7.351435, mse: 4083.501831, mean_q: 43.366875, mean_eps: 0.683761
  52853/150000: episode: 548, duration: 1.122s, episode steps: 112, steps per second: 100, episode reward: -64.465, mean reward: -0.576 [-100.000, 13.701], mean action: 1.429 [0.000, 3.000],  loss: 7.684756, mse: 4040.556514, mean_q: 43.397053, mean_eps: 0.683221
  52981/150000: episode: 549, duration: 1.411s, episode steps: 128, steps per second:  91, episode reward: -81.796, mean reward: -0.639 [-100.000, 11.820], mean action: 1.680 [0.000, 3.000],  loss: 9.986762, mse: 4009.098757, mean_q: 44.732454, mean_eps: 0.682501
  53091/150000: episode: 550, duration: 1.209s, episode steps: 110, steps per second:  91, episode reward:  3.540, mean reward:  0.032 [-100.000, 21.816], mean action: 1.691 [0.000, 3.000],  loss: 10.040900, mse: 4085.147625, mean_q: 44.300907, mean_eps: 0.681787
  53476/150000: episode: 551, duration: 4.224s, episode steps: 385, steps per second:  91, episode reward: -190.908, mean reward: -0.496 [-100.000, 61.978], mean action: 1.579 [0.000, 3.000],  loss: 11.438668, mse: 4098.549424, mean_q: 45.183105, mean_eps: 0.680302
  53568/150000: episode: 552, duration: 0.863s, episode steps:  92, steps per second: 107, episode reward: -31.849, mean reward: -0.346 [-100.000,  9.733], mean action: 1.783 [0.000, 3.000],  loss: 7.716142, mse: 4076.212564, mean_q: 43.232666, mean_eps: 0.678871
  53658/150000: episode: 553, duration: 0.816s, episode steps:  90, steps per second: 110, episode reward: -71.892, mean reward: -0.799 [-100.000, 16.867], mean action: 1.478 [0.000, 3.000],  loss: 8.992272, mse: 4111.701587, mean_q: 45.662591, mean_eps: 0.678325
  53793/150000: episode: 554, duration: 1.350s, episode steps: 135, steps per second: 100, episode reward: -56.612, mean reward: -0.419 [-100.000, 19.360], mean action: 1.541 [0.000, 3.000],  loss: 12.590820, mse: 4047.861693, mean_q: 44.513312, mean_eps: 0.677650
  53897/150000: episode: 555, duration: 0.952s, episode steps: 104, steps per second: 109, episode reward: -95.288, mean reward: -0.916 [-100.000, 10.389], mean action: 1.462 [0.000, 3.000],  loss: 8.647967, mse: 4035.107513, mean_q: 44.282639, mean_eps: 0.676933
  54009/150000: episode: 556, duration: 1.083s, episode steps: 112, steps per second: 103, episode reward: -77.508, mean reward: -0.692 [-100.000,  6.360], mean action: 1.625 [0.000, 3.000],  loss: 6.778599, mse: 4096.151539, mean_q: 45.075099, mean_eps: 0.676285
  54134/150000: episode: 557, duration: 1.128s, episode steps: 125, steps per second: 111, episode reward: -69.510, mean reward: -0.556 [-100.000, 11.529], mean action: 1.664 [0.000, 3.000],  loss: 8.245395, mse: 4172.223621, mean_q: 45.656768, mean_eps: 0.675574
  54210/150000: episode: 558, duration: 0.736s, episode steps:  76, steps per second: 103, episode reward: -31.926, mean reward: -0.420 [-100.000, 19.510], mean action: 1.553 [0.000, 3.000],  loss: 11.175978, mse: 4105.669443, mean_q: 45.965141, mean_eps: 0.674971
  54350/150000: episode: 559, duration: 1.349s, episode steps: 140, steps per second: 104, episode reward: -86.364, mean reward: -0.617 [-100.000, 13.419], mean action: 1.629 [0.000, 3.000],  loss: 16.797955, mse: 4227.312891, mean_q: 46.739038, mean_eps: 0.674323
  54443/150000: episode: 560, duration: 0.879s, episode steps:  93, steps per second: 106, episode reward: -75.064, mean reward: -0.807 [-100.000, 12.264], mean action: 1.505 [0.000, 3.000],  loss: 12.260869, mse: 4210.161143, mean_q: 47.647594, mean_eps: 0.673624
  54526/150000: episode: 561, duration: 0.785s, episode steps:  83, steps per second: 106, episode reward: -40.755, mean reward: -0.491 [-100.000, 11.895], mean action: 1.482 [0.000, 3.000],  loss: 10.261135, mse: 4132.986322, mean_q: 46.704238, mean_eps: 0.673096
  54620/150000: episode: 562, duration: 0.853s, episode steps:  94, steps per second: 110, episode reward: -120.263, mean reward: -1.279 [-100.000,  6.384], mean action: 1.851 [0.000, 3.000],  loss: 12.516524, mse: 4176.278645, mean_q: 45.661763, mean_eps: 0.672565
  54713/150000: episode: 563, duration: 0.867s, episode steps:  93, steps per second: 107, episode reward: -74.265, mean reward: -0.799 [-100.000,  7.505], mean action: 1.559 [0.000, 3.000],  loss: 9.557781, mse: 4226.884655, mean_q: 46.795768, mean_eps: 0.672004
  54793/150000: episode: 564, duration: 0.814s, episode steps:  80, steps per second:  98, episode reward: -86.793, mean reward: -1.085 [-100.000,  9.324], mean action: 1.875 [0.000, 3.000],  loss: 7.967883, mse: 4262.819485, mean_q: 47.318683, mean_eps: 0.671485
  54935/150000: episode: 565, duration: 1.342s, episode steps: 142, steps per second: 106, episode reward: -197.356, mean reward: -1.390 [-100.000,  4.629], mean action: 1.472 [0.000, 3.000],  loss: 12.979809, mse: 4178.790533, mean_q: 46.594323, mean_eps: 0.670819
  55015/150000: episode: 566, duration: 0.729s, episode steps:  80, steps per second: 110, episode reward: -55.597, mean reward: -0.695 [-100.000, 21.010], mean action: 1.550 [0.000, 3.000],  loss: 11.440780, mse: 4153.022290, mean_q: 46.349469, mean_eps: 0.670153
  55123/150000: episode: 567, duration: 1.066s, episode steps: 108, steps per second: 101, episode reward: -74.109, mean reward: -0.686 [-100.000, 14.087], mean action: 1.620 [0.000, 3.000],  loss: 10.548177, mse: 4356.796615, mean_q: 47.708192, mean_eps: 0.669589
  55234/150000: episode: 568, duration: 1.032s, episode steps: 111, steps per second: 108, episode reward: -50.571, mean reward: -0.456 [-100.000,  7.517], mean action: 1.441 [0.000, 3.000],  loss: 7.550894, mse: 4404.567306, mean_q: 48.798804, mean_eps: 0.668932
  55339/150000: episode: 569, duration: 1.157s, episode steps: 105, steps per second:  91, episode reward: -66.842, mean reward: -0.637 [-100.000, 12.346], mean action: 1.714 [0.000, 3.000],  loss: 8.647386, mse: 4367.620959, mean_q: 48.635769, mean_eps: 0.668284
  55437/150000: episode: 570, duration: 0.898s, episode steps:  98, steps per second: 109, episode reward: -93.927, mean reward: -0.958 [-100.000,  6.770], mean action: 1.673 [0.000, 3.000],  loss: 10.275121, mse: 4479.260483, mean_q: 48.361483, mean_eps: 0.667675
  55556/150000: episode: 571, duration: 1.178s, episode steps: 119, steps per second: 101, episode reward: -180.938, mean reward: -1.520 [-100.000,  3.458], mean action: 1.706 [0.000, 3.000],  loss: 12.144752, mse: 4405.970198, mean_q: 48.299421, mean_eps: 0.667024
  55666/150000: episode: 572, duration: 1.072s, episode steps: 110, steps per second: 103, episode reward: -128.084, mean reward: -1.164 [-100.000, 13.073], mean action: 1.582 [0.000, 3.000],  loss: 9.694997, mse: 4393.827517, mean_q: 47.252368, mean_eps: 0.666337
  55786/150000: episode: 573, duration: 1.116s, episode steps: 120, steps per second: 108, episode reward: -159.367, mean reward: -1.328 [-100.000,  4.652], mean action: 1.508 [0.000, 3.000],  loss: 8.960743, mse: 4473.042601, mean_q: 49.795814, mean_eps: 0.665647
  55884/150000: episode: 574, duration: 0.975s, episode steps:  98, steps per second: 101, episode reward: -102.809, mean reward: -1.049 [-100.000, 11.425], mean action: 1.612 [0.000, 3.000],  loss: 11.541560, mse: 4420.553116, mean_q: 48.520909, mean_eps: 0.664993
  55977/150000: episode: 575, duration: 0.905s, episode steps:  93, steps per second: 103, episode reward: -106.702, mean reward: -1.147 [-100.000,  8.083], mean action: 1.591 [0.000, 3.000],  loss: 8.360883, mse: 4395.999732, mean_q: 48.268939, mean_eps: 0.664420
  56038/150000: episode: 576, duration: 0.589s, episode steps:  61, steps per second: 104, episode reward: -89.886, mean reward: -1.474 [-100.000, 11.663], mean action: 1.639 [0.000, 3.000],  loss: 8.022278, mse: 4433.729256, mean_q: 49.296426, mean_eps: 0.663958
  56133/150000: episode: 577, duration: 0.860s, episode steps:  95, steps per second: 110, episode reward: -145.738, mean reward: -1.534 [-100.000,  6.256], mean action: 1.747 [0.000, 3.000],  loss: 9.594436, mse: 4375.570976, mean_q: 47.785374, mean_eps: 0.663490
  56224/150000: episode: 578, duration: 0.862s, episode steps:  91, steps per second: 106, episode reward: -111.392, mean reward: -1.224 [-100.000,  7.162], mean action: 1.824 [0.000, 3.000],  loss: 11.828958, mse: 4450.593938, mean_q: 49.219452, mean_eps: 0.662932
  56362/150000: episode: 579, duration: 1.337s, episode steps: 138, steps per second: 103, episode reward: -82.647, mean reward: -0.599 [-100.000, 17.696], mean action: 1.580 [0.000, 3.000],  loss: 8.924823, mse: 4390.659597, mean_q: 48.705553, mean_eps: 0.662245
  56445/150000: episode: 580, duration: 0.800s, episode steps:  83, steps per second: 104, episode reward: -71.539, mean reward: -0.862 [-100.000,  7.057], mean action: 1.518 [0.000, 3.000],  loss: 13.333581, mse: 4430.965585, mean_q: 48.799588, mean_eps: 0.661582
  56518/150000: episode: 581, duration: 0.674s, episode steps:  73, steps per second: 108, episode reward: -64.087, mean reward: -0.878 [-100.000, 16.896], mean action: 1.630 [0.000, 3.000],  loss: 10.293969, mse: 4441.355556, mean_q: 47.124250, mean_eps: 0.661114
  56654/150000: episode: 582, duration: 1.319s, episode steps: 136, steps per second: 103, episode reward: -45.534, mean reward: -0.335 [-100.000,  8.559], mean action: 1.618 [0.000, 3.000],  loss: 9.345447, mse: 4483.686453, mean_q: 47.935801, mean_eps: 0.660487
  56771/150000: episode: 583, duration: 1.091s, episode steps: 117, steps per second: 107, episode reward: -65.627, mean reward: -0.561 [-100.000, 12.480], mean action: 1.692 [0.000, 3.000],  loss: 7.780120, mse: 4408.301075, mean_q: 48.543844, mean_eps: 0.659728
  56895/150000: episode: 584, duration: 1.203s, episode steps: 124, steps per second: 103, episode reward: -125.903, mean reward: -1.015 [-100.000, 12.553], mean action: 1.661 [0.000, 3.000],  loss: 8.461007, mse: 4441.778482, mean_q: 49.098019, mean_eps: 0.659005
  56983/150000: episode: 585, duration: 0.832s, episode steps:  88, steps per second: 106, episode reward: -69.675, mean reward: -0.792 [-100.000, 11.269], mean action: 1.659 [0.000, 3.000],  loss: 7.166410, mse: 4381.488645, mean_q: 48.779792, mean_eps: 0.658369
  57174/150000: episode: 586, duration: 1.779s, episode steps: 191, steps per second: 107, episode reward: 55.859, mean reward:  0.292 [-100.000, 39.397], mean action: 1.623 [0.000, 3.000],  loss: 8.780107, mse: 4648.550945, mean_q: 50.091122, mean_eps: 0.657532
  57241/150000: episode: 587, duration: 0.619s, episode steps:  67, steps per second: 108, episode reward: -88.372, mean reward: -1.319 [-100.000, 20.101], mean action: 1.522 [0.000, 3.000],  loss: 10.305736, mse: 4701.386252, mean_q: 51.330619, mean_eps: 0.656758
  57355/150000: episode: 588, duration: 1.036s, episode steps: 114, steps per second: 110, episode reward: -107.593, mean reward: -0.944 [-100.000,  8.005], mean action: 1.395 [0.000, 3.000],  loss: 9.093884, mse: 4591.534822, mean_q: 49.549094, mean_eps: 0.656215
  57455/150000: episode: 589, duration: 0.996s, episode steps: 100, steps per second: 100, episode reward: -94.149, mean reward: -0.941 [-100.000, 14.085], mean action: 1.780 [0.000, 3.000],  loss: 14.982401, mse: 4617.283696, mean_q: 50.179660, mean_eps: 0.655573
  57562/150000: episode: 590, duration: 0.955s, episode steps: 107, steps per second: 112, episode reward: -84.726, mean reward: -0.792 [-100.000, 10.703], mean action: 1.692 [0.000, 3.000],  loss: 6.807960, mse: 4627.145166, mean_q: 49.262943, mean_eps: 0.654952
  57634/150000: episode: 591, duration: 0.693s, episode steps:  72, steps per second: 104, episode reward: -72.949, mean reward: -1.013 [-100.000, 10.353], mean action: 1.875 [0.000, 3.000],  loss: 6.768636, mse: 4606.503489, mean_q: 48.973543, mean_eps: 0.654415
  57754/150000: episode: 592, duration: 1.249s, episode steps: 120, steps per second:  96, episode reward: -96.203, mean reward: -0.802 [-100.000,  6.687], mean action: 1.500 [0.000, 3.000],  loss: 11.957131, mse: 4629.577055, mean_q: 49.459752, mean_eps: 0.653839
  57830/150000: episode: 593, duration: 0.809s, episode steps:  76, steps per second:  94, episode reward: -57.966, mean reward: -0.763 [-100.000,  9.049], mean action: 1.737 [0.000, 3.000],  loss: 9.843884, mse: 4579.675164, mean_q: 50.206522, mean_eps: 0.653251
  57934/150000: episode: 594, duration: 1.176s, episode steps: 104, steps per second:  88, episode reward: -97.655, mean reward: -0.939 [-100.000,  9.255], mean action: 1.577 [0.000, 3.000],  loss: 11.447397, mse: 4668.548802, mean_q: 51.421379, mean_eps: 0.652711
  58060/150000: episode: 595, duration: 1.269s, episode steps: 126, steps per second:  99, episode reward: -31.101, mean reward: -0.247 [-100.000, 19.433], mean action: 1.405 [0.000, 3.000],  loss: 8.458277, mse: 4768.736871, mean_q: 51.355060, mean_eps: 0.652021
  58137/150000: episode: 596, duration: 0.729s, episode steps:  77, steps per second: 106, episode reward: -51.501, mean reward: -0.669 [-100.000,  8.767], mean action: 1.662 [0.000, 3.000],  loss: 13.903344, mse: 4818.002594, mean_q: 52.049687, mean_eps: 0.651412
  58222/150000: episode: 597, duration: 0.796s, episode steps:  85, steps per second: 107, episode reward: -93.184, mean reward: -1.096 [-100.000, 11.761], mean action: 1.482 [0.000, 3.000],  loss: 7.653047, mse: 4774.960794, mean_q: 49.144842, mean_eps: 0.650926
  58345/150000: episode: 598, duration: 1.239s, episode steps: 123, steps per second:  99, episode reward: -106.613, mean reward: -0.867 [-100.000,  9.963], mean action: 1.707 [0.000, 3.000],  loss: 13.586457, mse: 4721.251882, mean_q: 50.610829, mean_eps: 0.650302
  58430/150000: episode: 599, duration: 0.868s, episode steps:  85, steps per second:  98, episode reward: -57.902, mean reward: -0.681 [-100.000, 12.977], mean action: 1.741 [0.000, 3.000],  loss: 8.855493, mse: 4699.672992, mean_q: 49.947279, mean_eps: 0.649678
  58535/150000: episode: 600, duration: 1.146s, episode steps: 105, steps per second:  92, episode reward: -65.825, mean reward: -0.627 [-100.000,  8.915], mean action: 1.467 [0.000, 3.000],  loss: 9.488435, mse: 4801.883582, mean_q: 49.924939, mean_eps: 0.649108
  58661/150000: episode: 601, duration: 1.170s, episode steps: 126, steps per second: 108, episode reward: -250.986, mean reward: -1.992 [-100.000, 30.823], mean action: 1.754 [0.000, 3.000],  loss: 10.944629, mse: 4720.154491, mean_q: 50.198468, mean_eps: 0.648415
  58751/150000: episode: 602, duration: 1.003s, episode steps:  90, steps per second:  90, episode reward:  9.363, mean reward:  0.104 [-100.000, 17.270], mean action: 1.800 [0.000, 3.000],  loss: 9.709066, mse: 4710.706348, mean_q: 50.718113, mean_eps: 0.647767
  58834/150000: episode: 603, duration: 1.055s, episode steps:  83, steps per second:  79, episode reward: -24.712, mean reward: -0.298 [-100.000, 11.818], mean action: 1.904 [0.000, 3.000],  loss: 13.518501, mse: 4700.205937, mean_q: 49.137343, mean_eps: 0.647248
  58919/150000: episode: 604, duration: 1.177s, episode steps:  85, steps per second:  72, episode reward: -57.922, mean reward: -0.681 [-100.000, 10.858], mean action: 1.671 [0.000, 3.000],  loss: 11.336932, mse: 4714.381681, mean_q: 49.742008, mean_eps: 0.646744
  59041/150000: episode: 605, duration: 1.531s, episode steps: 122, steps per second:  80, episode reward: -167.334, mean reward: -1.372 [-100.000, 13.762], mean action: 1.549 [0.000, 3.000],  loss: 11.638464, mse: 4802.348245, mean_q: 50.716046, mean_eps: 0.646123
  59147/150000: episode: 606, duration: 1.266s, episode steps: 106, steps per second:  84, episode reward: 21.785, mean reward:  0.206 [-100.000, 24.157], mean action: 1.745 [0.000, 3.000],  loss: 10.709071, mse: 4768.796440, mean_q: 50.191688, mean_eps: 0.645439
  59239/150000: episode: 607, duration: 0.807s, episode steps:  92, steps per second: 114, episode reward: -93.992, mean reward: -1.022 [-100.000, 10.012], mean action: 1.880 [0.000, 3.000],  loss: 9.366750, mse: 4871.305876, mean_q: 52.118103, mean_eps: 0.644845
  59351/150000: episode: 608, duration: 1.019s, episode steps: 112, steps per second: 110, episode reward: -168.414, mean reward: -1.504 [-100.000,  2.297], mean action: 1.420 [0.000, 3.000],  loss: 16.410179, mse: 4770.857779, mean_q: 51.386079, mean_eps: 0.644233
  59441/150000: episode: 609, duration: 0.825s, episode steps:  90, steps per second: 109, episode reward: -43.747, mean reward: -0.486 [-100.000, 12.628], mean action: 1.844 [0.000, 3.000],  loss: 10.244864, mse: 4775.194577, mean_q: 50.957120, mean_eps: 0.643627
  59546/150000: episode: 610, duration: 0.998s, episode steps: 105, steps per second: 105, episode reward: -126.338, mean reward: -1.203 [-100.000, 10.535], mean action: 1.781 [0.000, 3.000],  loss: 7.362871, mse: 4879.055727, mean_q: 52.176328, mean_eps: 0.643042
  59636/150000: episode: 611, duration: 0.917s, episode steps:  90, steps per second:  98, episode reward: -53.729, mean reward: -0.597 [-100.000, 16.081], mean action: 1.578 [0.000, 3.000],  loss: 13.526353, mse: 4861.999395, mean_q: 50.822206, mean_eps: 0.642457
  59711/150000: episode: 612, duration: 0.739s, episode steps:  75, steps per second: 102, episode reward: -94.300, mean reward: -1.257 [-100.000,  9.702], mean action: 1.520 [0.000, 3.000],  loss: 8.320268, mse: 4816.729339, mean_q: 51.236077, mean_eps: 0.641962
  59793/150000: episode: 613, duration: 0.727s, episode steps:  82, steps per second: 113, episode reward: -91.224, mean reward: -1.112 [-100.000,  5.236], mean action: 1.537 [0.000, 3.000],  loss: 17.268413, mse: 4763.964305, mean_q: 49.726537, mean_eps: 0.641491
  59926/150000: episode: 614, duration: 1.240s, episode steps: 133, steps per second: 107, episode reward: -68.129, mean reward: -0.512 [-100.000, 11.590], mean action: 1.556 [0.000, 3.000],  loss: 12.115547, mse: 4839.792927, mean_q: 51.341308, mean_eps: 0.640846
  60070/150000: episode: 615, duration: 1.159s, episode steps: 144, steps per second: 124, episode reward: -83.494, mean reward: -0.580 [-100.000, 17.737], mean action: 1.625 [0.000, 3.000],  loss: 12.589913, mse: 4847.002925, mean_q: 51.237081, mean_eps: 0.640015
  60199/150000: episode: 616, duration: 1.040s, episode steps: 129, steps per second: 124, episode reward: -15.475, mean reward: -0.120 [-100.000, 13.157], mean action: 1.566 [0.000, 3.000],  loss: 15.202376, mse: 4930.331492, mean_q: 51.643924, mean_eps: 0.639196
  60275/150000: episode: 617, duration: 0.596s, episode steps:  76, steps per second: 127, episode reward: -50.867, mean reward: -0.669 [-100.000, 12.684], mean action: 1.447 [0.000, 3.000],  loss: 6.373282, mse: 4982.081691, mean_q: 51.898015, mean_eps: 0.638581
  60344/150000: episode: 618, duration: 0.520s, episode steps:  69, steps per second: 133, episode reward: -76.665, mean reward: -1.111 [-100.000, 10.209], mean action: 1.870 [0.000, 3.000],  loss: 16.545414, mse: 4970.923655, mean_q: 51.981954, mean_eps: 0.638146
  60467/150000: episode: 619, duration: 1.059s, episode steps: 123, steps per second: 116, episode reward: -60.193, mean reward: -0.489 [-100.000, 14.010], mean action: 1.724 [0.000, 3.000],  loss: 7.572304, mse: 4955.878426, mean_q: 50.603486, mean_eps: 0.637570
  60578/150000: episode: 620, duration: 0.843s, episode steps: 111, steps per second: 132, episode reward: -98.499, mean reward: -0.887 [-100.000, 15.693], mean action: 1.559 [0.000, 3.000],  loss: 10.098373, mse: 4968.131543, mean_q: 51.942386, mean_eps: 0.636868
  60700/150000: episode: 621, duration: 0.952s, episode steps: 122, steps per second: 128, episode reward: -152.933, mean reward: -1.254 [-100.000, 38.910], mean action: 1.705 [0.000, 3.000],  loss: 7.259334, mse: 4916.954782, mean_q: 51.114075, mean_eps: 0.636169
  60806/150000: episode: 622, duration: 0.835s, episode steps: 106, steps per second: 127, episode reward: 10.786, mean reward:  0.102 [-100.000, 53.613], mean action: 1.425 [0.000, 3.000],  loss: 7.489618, mse: 4997.020800, mean_q: 51.705056, mean_eps: 0.635485
  60900/150000: episode: 623, duration: 0.739s, episode steps:  94, steps per second: 127, episode reward: -97.675, mean reward: -1.039 [-100.000,  9.055], mean action: 1.415 [0.000, 3.000],  loss: 9.278054, mse: 4887.508597, mean_q: 50.365383, mean_eps: 0.634885
  60980/150000: episode: 624, duration: 0.625s, episode steps:  80, steps per second: 128, episode reward: -84.278, mean reward: -1.053 [-100.000, 10.968], mean action: 1.762 [0.000, 3.000],  loss: 7.752491, mse: 4928.386505, mean_q: 51.487565, mean_eps: 0.634363
  61064/150000: episode: 625, duration: 0.710s, episode steps:  84, steps per second: 118, episode reward: -101.876, mean reward: -1.213 [-100.000,  8.444], mean action: 1.619 [0.000, 3.000],  loss: 13.182335, mse: 5022.607454, mean_q: 52.088514, mean_eps: 0.633871
  61144/150000: episode: 626, duration: 0.672s, episode steps:  80, steps per second: 119, episode reward: -73.321, mean reward: -0.917 [-100.000, 12.330], mean action: 1.650 [0.000, 3.000],  loss: 9.251857, mse: 4928.683215, mean_q: 49.569848, mean_eps: 0.633379
  61238/150000: episode: 627, duration: 0.733s, episode steps:  94, steps per second: 128, episode reward: -54.005, mean reward: -0.575 [-100.000, 10.046], mean action: 1.734 [0.000, 3.000],  loss: 8.372614, mse: 4971.963379, mean_q: 51.098972, mean_eps: 0.632857
  61342/150000: episode: 628, duration: 0.832s, episode steps: 104, steps per second: 125, episode reward: -123.589, mean reward: -1.188 [-100.000,  6.457], mean action: 1.615 [0.000, 3.000],  loss: 4.677239, mse: 4924.867274, mean_q: 50.888748, mean_eps: 0.632263
  61454/150000: episode: 629, duration: 0.857s, episode steps: 112, steps per second: 131, episode reward: -67.713, mean reward: -0.605 [-100.000, 10.031], mean action: 1.598 [0.000, 3.000],  loss: 6.751115, mse: 4944.595490, mean_q: 51.137109, mean_eps: 0.631615
  61544/150000: episode: 630, duration: 0.720s, episode steps:  90, steps per second: 125, episode reward: -106.027, mean reward: -1.178 [-100.000, 10.973], mean action: 1.622 [0.000, 3.000],  loss: 8.951339, mse: 4936.807389, mean_q: 50.122793, mean_eps: 0.631009
  61653/150000: episode: 631, duration: 0.835s, episode steps: 109, steps per second: 130, episode reward: -169.237, mean reward: -1.553 [-100.000,  7.425], mean action: 1.761 [0.000, 3.000],  loss: 9.610605, mse: 5001.259154, mean_q: 51.407876, mean_eps: 0.630412
  61793/150000: episode: 632, duration: 1.145s, episode steps: 140, steps per second: 122, episode reward: -31.836, mean reward: -0.227 [-100.000, 19.320], mean action: 1.686 [0.000, 3.000],  loss: 12.569987, mse: 5077.737277, mean_q: 53.413974, mean_eps: 0.629665
  61910/150000: episode: 633, duration: 0.910s, episode steps: 117, steps per second: 129, episode reward: -59.002, mean reward: -0.504 [-100.000,  7.480], mean action: 1.376 [0.000, 3.000],  loss: 7.975134, mse: 4981.200517, mean_q: 52.113922, mean_eps: 0.628894
  62033/150000: episode: 634, duration: 0.915s, episode steps: 123, steps per second: 134, episode reward: -124.678, mean reward: -1.014 [-100.000, 11.947], mean action: 1.520 [0.000, 3.000],  loss: 8.844921, mse: 5048.470592, mean_q: 53.454403, mean_eps: 0.628174
  62146/150000: episode: 635, duration: 0.918s, episode steps: 113, steps per second: 123, episode reward: 17.270, mean reward:  0.153 [-100.000, 38.109], mean action: 1.496 [0.000, 3.000],  loss: 8.691506, mse: 5108.960575, mean_q: 54.310171, mean_eps: 0.627466
  62261/150000: episode: 636, duration: 0.865s, episode steps: 115, steps per second: 133, episode reward: -76.013, mean reward: -0.661 [-100.000,  9.106], mean action: 1.635 [0.000, 3.000],  loss: 7.960271, mse: 5094.578894, mean_q: 53.060928, mean_eps: 0.626782
  62365/150000: episode: 637, duration: 0.887s, episode steps: 104, steps per second: 117, episode reward: -75.362, mean reward: -0.725 [-100.000,  7.854], mean action: 1.760 [0.000, 3.000],  loss: 10.937204, mse: 5052.460747, mean_q: 53.002620, mean_eps: 0.626125
  62510/150000: episode: 638, duration: 1.152s, episode steps: 145, steps per second: 126, episode reward: -179.641, mean reward: -1.239 [-100.000, 19.554], mean action: 1.779 [0.000, 3.000],  loss: 9.270527, mse: 5143.873549, mean_q: 53.914732, mean_eps: 0.625378
  62610/150000: episode: 639, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: -134.102, mean reward: -1.341 [-100.000,  7.624], mean action: 1.620 [0.000, 3.000],  loss: 8.165407, mse: 5017.742456, mean_q: 53.032828, mean_eps: 0.624643
  62708/150000: episode: 640, duration: 0.790s, episode steps:  98, steps per second: 124, episode reward: -86.312, mean reward: -0.881 [-100.000, 12.666], mean action: 1.663 [0.000, 3.000],  loss: 7.179519, mse: 5052.815362, mean_q: 52.508035, mean_eps: 0.624049
  62776/150000: episode: 641, duration: 0.536s, episode steps:  68, steps per second: 127, episode reward: -52.384, mean reward: -0.770 [-100.000, 10.186], mean action: 1.721 [0.000, 3.000],  loss: 9.521149, mse: 5177.009170, mean_q: 53.310792, mean_eps: 0.623551
  62867/150000: episode: 642, duration: 0.673s, episode steps:  91, steps per second: 135, episode reward: -1.891, mean reward: -0.021 [-100.000, 14.524], mean action: 1.758 [0.000, 3.000],  loss: 8.928064, mse: 5077.707294, mean_q: 52.856483, mean_eps: 0.623074
  62981/150000: episode: 643, duration: 1.024s, episode steps: 114, steps per second: 111, episode reward: -105.152, mean reward: -0.922 [-100.000, 21.053], mean action: 1.763 [0.000, 3.000],  loss: 5.649948, mse: 5105.329922, mean_q: 53.894714, mean_eps: 0.622459
  63074/150000: episode: 644, duration: 0.756s, episode steps:  93, steps per second: 123, episode reward: -98.273, mean reward: -1.057 [-100.000,  6.234], mean action: 1.430 [0.000, 3.000],  loss: 9.131028, mse: 5071.515601, mean_q: 52.392052, mean_eps: 0.621838
  63221/150000: episode: 645, duration: 1.135s, episode steps: 147, steps per second: 129, episode reward: -38.019, mean reward: -0.259 [-100.000, 68.978], mean action: 1.626 [0.000, 3.000],  loss: 16.283972, mse: 5160.034572, mean_q: 53.885691, mean_eps: 0.621118
  63320/150000: episode: 646, duration: 0.774s, episode steps:  99, steps per second: 128, episode reward: -56.109, mean reward: -0.567 [-100.000, 17.370], mean action: 1.556 [0.000, 3.000],  loss: 10.604242, mse: 5289.294046, mean_q: 54.319024, mean_eps: 0.620380
  63408/150000: episode: 647, duration: 0.662s, episode steps:  88, steps per second: 133, episode reward: -48.494, mean reward: -0.551 [-100.000, 12.141], mean action: 1.591 [0.000, 3.000],  loss: 10.281859, mse: 5118.389576, mean_q: 53.781761, mean_eps: 0.619819
  63482/150000: episode: 648, duration: 0.596s, episode steps:  74, steps per second: 124, episode reward: -57.074, mean reward: -0.771 [-100.000,  6.380], mean action: 1.595 [0.000, 3.000],  loss: 12.735587, mse: 5067.872948, mean_q: 54.730537, mean_eps: 0.619333
  63600/150000: episode: 649, duration: 1.007s, episode steps: 118, steps per second: 117, episode reward: -54.074, mean reward: -0.458 [-100.000, 13.781], mean action: 1.678 [0.000, 3.000],  loss: 8.722105, mse: 5128.783890, mean_q: 53.851365, mean_eps: 0.618757
  63692/150000: episode: 650, duration: 0.686s, episode steps:  92, steps per second: 134, episode reward: -41.465, mean reward: -0.451 [-100.000,  8.878], mean action: 1.511 [0.000, 3.000],  loss: 7.373652, mse: 5088.847033, mean_q: 52.789815, mean_eps: 0.618127
  63779/150000: episode: 651, duration: 0.683s, episode steps:  87, steps per second: 127, episode reward: -65.570, mean reward: -0.754 [-100.000, 16.578], mean action: 1.310 [0.000, 3.000],  loss: 8.450359, mse: 5096.047759, mean_q: 53.062838, mean_eps: 0.617590
  63850/150000: episode: 652, duration: 0.535s, episode steps:  71, steps per second: 133, episode reward: -33.423, mean reward: -0.471 [-100.000, 26.763], mean action: 1.380 [0.000, 3.000],  loss: 8.221676, mse: 5170.645649, mean_q: 53.645691, mean_eps: 0.617116
  64008/150000: episode: 653, duration: 1.242s, episode steps: 158, steps per second: 127, episode reward: -62.250, mean reward: -0.394 [-100.000,  9.360], mean action: 1.506 [0.000, 3.000],  loss: 10.542434, mse: 5131.034005, mean_q: 52.854162, mean_eps: 0.616429
  64110/150000: episode: 654, duration: 0.780s, episode steps: 102, steps per second: 131, episode reward: -117.388, mean reward: -1.151 [-100.000,  8.273], mean action: 1.637 [0.000, 3.000],  loss: 8.657902, mse: 5243.802538, mean_q: 54.627098, mean_eps: 0.615649
  64205/150000: episode: 655, duration: 0.884s, episode steps:  95, steps per second: 108, episode reward: -0.022, mean reward: -0.000 [-100.000, 15.035], mean action: 1.547 [0.000, 3.000],  loss: 14.437998, mse: 5237.459203, mean_q: 54.433909, mean_eps: 0.615058
  64308/150000: episode: 656, duration: 1.230s, episode steps: 103, steps per second:  84, episode reward: -45.892, mean reward: -0.446 [-100.000, 12.425], mean action: 1.602 [0.000, 3.000],  loss: 7.571947, mse: 5261.183845, mean_q: 53.527608, mean_eps: 0.614464
  64428/150000: episode: 657, duration: 1.126s, episode steps: 120, steps per second: 107, episode reward: -74.403, mean reward: -0.620 [-100.000, 13.065], mean action: 1.458 [0.000, 3.000],  loss: 8.639884, mse: 5262.349910, mean_q: 55.234380, mean_eps: 0.613795
  64537/150000: episode: 658, duration: 1.074s, episode steps: 109, steps per second: 101, episode reward: -116.958, mean reward: -1.073 [-100.000,  8.511], mean action: 1.752 [0.000, 3.000],  loss: 8.890106, mse: 5233.922762, mean_q: 54.094489, mean_eps: 0.613108
  64639/150000: episode: 659, duration: 0.977s, episode steps: 102, steps per second: 104, episode reward: -56.289, mean reward: -0.552 [-100.000, 15.356], mean action: 1.529 [0.000, 3.000],  loss: 14.866062, mse: 5396.782806, mean_q: 55.096938, mean_eps: 0.612475
  64739/150000: episode: 660, duration: 1.053s, episode steps: 100, steps per second:  95, episode reward: -25.555, mean reward: -0.256 [-100.000, 14.198], mean action: 1.680 [0.000, 3.000],  loss: 11.212497, mse: 5232.725525, mean_q: 53.363690, mean_eps: 0.611869
  64880/150000: episode: 661, duration: 1.319s, episode steps: 141, steps per second: 107, episode reward: -211.751, mean reward: -1.502 [-100.000, 35.130], mean action: 1.702 [0.000, 3.000],  loss: 9.483684, mse: 5293.227686, mean_q: 53.866804, mean_eps: 0.611146
  64983/150000: episode: 662, duration: 0.976s, episode steps: 103, steps per second: 106, episode reward: -83.866, mean reward: -0.814 [-100.000, 11.529], mean action: 1.505 [0.000, 3.000],  loss: 7.281329, mse: 5191.928901, mean_q: 53.093019, mean_eps: 0.610414
  65066/150000: episode: 663, duration: 0.855s, episode steps:  83, steps per second:  97, episode reward: -127.543, mean reward: -1.537 [-100.000,  5.276], mean action: 1.554 [0.000, 3.000],  loss: 9.790495, mse: 5236.234534, mean_q: 53.392395, mean_eps: 0.609856
  65200/150000: episode: 664, duration: 1.643s, episode steps: 134, steps per second:  82, episode reward: -32.496, mean reward: -0.243 [-100.000, 21.372], mean action: 1.769 [0.000, 3.000],  loss: 7.250331, mse: 5372.674790, mean_q: 55.986288, mean_eps: 0.609205
  65310/150000: episode: 665, duration: 1.200s, episode steps: 110, steps per second:  92, episode reward: -56.781, mean reward: -0.516 [-100.000,  9.277], mean action: 1.455 [0.000, 3.000],  loss: 8.097148, mse: 5251.168182, mean_q: 54.071495, mean_eps: 0.608473
  65402/150000: episode: 666, duration: 0.958s, episode steps:  92, steps per second:  96, episode reward: -46.147, mean reward: -0.502 [-100.000,  7.078], mean action: 1.554 [0.000, 3.000],  loss: 7.305451, mse: 5331.537486, mean_q: 55.133147, mean_eps: 0.607867
  65496/150000: episode: 667, duration: 0.917s, episode steps:  94, steps per second: 102, episode reward: -57.783, mean reward: -0.615 [-100.000, 16.414], mean action: 1.617 [0.000, 3.000],  loss: 7.473686, mse: 5464.548355, mean_q: 56.777603, mean_eps: 0.607309
  65602/150000: episode: 668, duration: 1.116s, episode steps: 106, steps per second:  95, episode reward: -44.179, mean reward: -0.417 [-100.000, 12.405], mean action: 1.821 [0.000, 3.000],  loss: 9.550720, mse: 5340.686464, mean_q: 54.449401, mean_eps: 0.606709
  65676/150000: episode: 669, duration: 0.795s, episode steps:  74, steps per second:  93, episode reward: -126.206, mean reward: -1.705 [-100.000,  6.486], mean action: 1.473 [0.000, 3.000],  loss: 5.913787, mse: 5238.137458, mean_q: 53.729318, mean_eps: 0.606169
  65779/150000: episode: 670, duration: 1.038s, episode steps: 103, steps per second:  99, episode reward: -13.043, mean reward: -0.127 [-100.000, 13.689], mean action: 1.670 [0.000, 3.000],  loss: 10.610357, mse: 5335.926957, mean_q: 54.653491, mean_eps: 0.605638
  65903/150000: episode: 671, duration: 1.131s, episode steps: 124, steps per second: 110, episode reward: -82.895, mean reward: -0.669 [-100.000, 16.467], mean action: 1.597 [0.000, 3.000],  loss: 9.628484, mse: 5265.685817, mean_q: 54.860737, mean_eps: 0.604957
  66006/150000: episode: 672, duration: 1.040s, episode steps: 103, steps per second:  99, episode reward:  8.431, mean reward:  0.082 [-100.000, 15.044], mean action: 1.592 [0.000, 3.000],  loss: 10.388134, mse: 5361.504413, mean_q: 55.956275, mean_eps: 0.604276
  66134/150000: episode: 673, duration: 2.204s, episode steps: 128, steps per second:  58, episode reward: -235.429, mean reward: -1.839 [-100.000, 17.584], mean action: 1.539 [0.000, 3.000],  loss: 7.747282, mse: 5560.135899, mean_q: 56.812705, mean_eps: 0.603583
  66290/150000: episode: 674, duration: 1.542s, episode steps: 156, steps per second: 101, episode reward: -3.476, mean reward: -0.022 [-100.000, 22.248], mean action: 1.718 [0.000, 3.000],  loss: 8.270070, mse: 5532.244648, mean_q: 56.454224, mean_eps: 0.602731
  66438/150000: episode: 675, duration: 1.492s, episode steps: 148, steps per second:  99, episode reward: -3.823, mean reward: -0.026 [-100.000, 17.199], mean action: 1.696 [0.000, 3.000],  loss: 7.859532, mse: 5515.495886, mean_q: 56.392420, mean_eps: 0.601819
  66508/150000: episode: 676, duration: 0.677s, episode steps:  70, steps per second: 103, episode reward: -59.132, mean reward: -0.845 [-100.000, 19.859], mean action: 1.643 [0.000, 3.000],  loss: 4.780483, mse: 5441.251416, mean_q: 55.060290, mean_eps: 0.601165
  66641/150000: episode: 677, duration: 1.478s, episode steps: 133, steps per second:  90, episode reward: -85.025, mean reward: -0.639 [-100.000, 11.440], mean action: 1.496 [0.000, 3.000],  loss: 6.471713, mse: 5496.764704, mean_q: 55.593567, mean_eps: 0.600556
  66721/150000: episode: 678, duration: 0.781s, episode steps:  80, steps per second: 102, episode reward: -39.215, mean reward: -0.490 [-100.000, 24.096], mean action: 1.688 [0.000, 3.000],  loss: 10.574567, mse: 5516.165524, mean_q: 56.452232, mean_eps: 0.599917
  66842/150000: episode: 679, duration: 1.298s, episode steps: 121, steps per second:  93, episode reward: -83.707, mean reward: -0.692 [-100.000, 11.882], mean action: 1.529 [0.000, 3.000],  loss: 8.669208, mse: 5563.319969, mean_q: 57.181732, mean_eps: 0.599314
  66973/150000: episode: 680, duration: 2.224s, episode steps: 131, steps per second:  59, episode reward: -39.426, mean reward: -0.301 [-100.000,  7.789], mean action: 1.489 [0.000, 3.000],  loss: 6.450647, mse: 5423.625503, mean_q: 55.573087, mean_eps: 0.598558
  67081/150000: episode: 681, duration: 1.490s, episode steps: 108, steps per second:  72, episode reward: -196.480, mean reward: -1.819 [-100.000,  9.176], mean action: 1.759 [0.000, 3.000],  loss: 5.998494, mse: 5651.070724, mean_q: 57.287682, mean_eps: 0.597841
  67168/150000: episode: 682, duration: 0.855s, episode steps:  87, steps per second: 102, episode reward: -76.066, mean reward: -0.874 [-100.000, 11.134], mean action: 1.782 [0.000, 3.000],  loss: 6.192442, mse: 5552.205575, mean_q: 55.501745, mean_eps: 0.597256
  67259/150000: episode: 683, duration: 0.837s, episode steps:  91, steps per second: 109, episode reward: -87.445, mean reward: -0.961 [-100.000, 19.397], mean action: 1.802 [0.000, 3.000],  loss: 4.999749, mse: 5603.520513, mean_q: 58.074784, mean_eps: 0.596722
  67371/150000: episode: 684, duration: 0.966s, episode steps: 112, steps per second: 116, episode reward: 27.188, mean reward:  0.243 [-100.000, 18.859], mean action: 1.571 [0.000, 3.000],  loss: 8.349887, mse: 5577.471361, mean_q: 56.654546, mean_eps: 0.596113
  67467/150000: episode: 685, duration: 0.889s, episode steps:  96, steps per second: 108, episode reward: -120.933, mean reward: -1.260 [-100.000, 11.414], mean action: 1.677 [0.000, 3.000],  loss: 8.143483, mse: 5619.722366, mean_q: 58.416914, mean_eps: 0.595489
  67542/150000: episode: 686, duration: 0.668s, episode steps:  75, steps per second: 112, episode reward: -42.181, mean reward: -0.562 [-100.000,  9.111], mean action: 1.587 [0.000, 3.000],  loss: 5.714883, mse: 5714.974079, mean_q: 58.330508, mean_eps: 0.594976
  67690/150000: episode: 687, duration: 1.215s, episode steps: 148, steps per second: 122, episode reward: -187.857, mean reward: -1.269 [-100.000, 36.351], mean action: 1.784 [0.000, 3.000],  loss: 10.990281, mse: 5538.216576, mean_q: 56.897236, mean_eps: 0.594307
  67783/150000: episode: 688, duration: 0.777s, episode steps:  93, steps per second: 120, episode reward: 51.349, mean reward:  0.552 [-100.000, 16.720], mean action: 1.742 [0.000, 3.000],  loss: 7.957209, mse: 5608.690057, mean_q: 58.821711, mean_eps: 0.593584
  67867/150000: episode: 689, duration: 0.676s, episode steps:  84, steps per second: 124, episode reward: -1.855, mean reward: -0.022 [-100.000, 13.213], mean action: 1.821 [0.000, 3.000],  loss: 4.851738, mse: 5752.937390, mean_q: 59.705250, mean_eps: 0.593053
  67995/150000: episode: 690, duration: 1.357s, episode steps: 128, steps per second:  94, episode reward:  4.306, mean reward:  0.034 [-100.000, 19.904], mean action: 1.781 [0.000, 3.000],  loss: 7.385616, mse: 5689.008999, mean_q: 57.821620, mean_eps: 0.592417
  68066/150000: episode: 691, duration: 0.764s, episode steps:  71, steps per second:  93, episode reward: -39.514, mean reward: -0.557 [-100.000,  8.301], mean action: 1.563 [0.000, 3.000],  loss: 7.418667, mse: 5863.853798, mean_q: 58.884465, mean_eps: 0.591820
  68150/150000: episode: 692, duration: 1.010s, episode steps:  84, steps per second:  83, episode reward: -14.770, mean reward: -0.176 [-100.000, 17.041], mean action: 1.738 [0.000, 3.000],  loss: 7.134561, mse: 5821.476388, mean_q: 57.756220, mean_eps: 0.591355
  68263/150000: episode: 693, duration: 1.246s, episode steps: 113, steps per second:  91, episode reward: -26.276, mean reward: -0.233 [-100.000, 18.562], mean action: 1.735 [0.000, 3.000],  loss: 7.855851, mse: 5888.955955, mean_q: 59.937136, mean_eps: 0.590764
  68352/150000: episode: 694, duration: 0.915s, episode steps:  89, steps per second:  97, episode reward: -68.196, mean reward: -0.766 [-100.000,  8.093], mean action: 1.787 [0.000, 3.000],  loss: 13.485018, mse: 5871.509253, mean_q: 60.010179, mean_eps: 0.590158
  68454/150000: episode: 695, duration: 1.235s, episode steps: 102, steps per second:  83, episode reward: -69.283, mean reward: -0.679 [-100.000, 12.659], mean action: 1.500 [0.000, 3.000],  loss: 8.546195, mse: 5909.619758, mean_q: 59.224625, mean_eps: 0.589585
  68561/150000: episode: 696, duration: 1.107s, episode steps: 107, steps per second:  97, episode reward: 26.063, mean reward:  0.244 [-100.000, 17.582], mean action: 1.748 [0.000, 3.000],  loss: 5.987416, mse: 5804.261915, mean_q: 58.727619, mean_eps: 0.588958
  68665/150000: episode: 697, duration: 1.058s, episode steps: 104, steps per second:  98, episode reward: -30.705, mean reward: -0.295 [-100.000,  9.389], mean action: 1.663 [0.000, 3.000],  loss: 5.599327, mse: 5941.636376, mean_q: 60.471824, mean_eps: 0.588325
  68780/150000: episode: 698, duration: 1.121s, episode steps: 115, steps per second: 103, episode reward: -62.109, mean reward: -0.540 [-100.000, 10.979], mean action: 1.843 [0.000, 3.000],  loss: 8.047020, mse: 5816.227259, mean_q: 58.228618, mean_eps: 0.587668
  68880/150000: episode: 699, duration: 1.064s, episode steps: 100, steps per second:  94, episode reward:  2.080, mean reward:  0.021 [-100.000, 17.985], mean action: 1.690 [0.000, 3.000],  loss: 7.345606, mse: 5849.026069, mean_q: 58.530906, mean_eps: 0.587023
  68982/150000: episode: 700, duration: 1.088s, episode steps: 102, steps per second:  94, episode reward: -116.468, mean reward: -1.142 [-100.000,  5.283], mean action: 1.775 [0.000, 3.000],  loss: 6.597174, mse: 5822.455145, mean_q: 58.800731, mean_eps: 0.586417
  69094/150000: episode: 701, duration: 1.149s, episode steps: 112, steps per second:  97, episode reward: -213.528, mean reward: -1.907 [-100.000, 58.554], mean action: 1.500 [0.000, 3.000],  loss: 7.233558, mse: 5923.267661, mean_q: 58.875121, mean_eps: 0.585775
  69206/150000: episode: 702, duration: 0.843s, episode steps: 112, steps per second: 133, episode reward: -123.787, mean reward: -1.105 [-100.000, 11.374], mean action: 1.750 [0.000, 3.000],  loss: 5.382781, mse: 6033.219238, mean_q: 60.594106, mean_eps: 0.585103
  69313/150000: episode: 703, duration: 0.851s, episode steps: 107, steps per second: 126, episode reward: -56.591, mean reward: -0.529 [-100.000, 23.161], mean action: 1.664 [0.000, 3.000],  loss: 7.667537, mse: 6041.392300, mean_q: 60.496087, mean_eps: 0.584446
  69425/150000: episode: 704, duration: 1.215s, episode steps: 112, steps per second:  92, episode reward: -145.704, mean reward: -1.301 [-100.000, 10.949], mean action: 1.536 [0.000, 3.000],  loss: 8.897965, mse: 5959.506659, mean_q: 59.827000, mean_eps: 0.583789
  69545/150000: episode: 705, duration: 1.315s, episode steps: 120, steps per second:  91, episode reward: -56.575, mean reward: -0.471 [-100.000, 10.412], mean action: 1.767 [0.000, 3.000],  loss: 7.646421, mse: 5840.167049, mean_q: 58.641005, mean_eps: 0.583093
  69649/150000: episode: 706, duration: 0.849s, episode steps: 104, steps per second: 123, episode reward: -73.457, mean reward: -0.706 [-100.000, 18.589], mean action: 1.702 [0.000, 3.000],  loss: 4.776005, mse: 5986.616511, mean_q: 60.073793, mean_eps: 0.582421
  69726/150000: episode: 707, duration: 0.621s, episode steps:  77, steps per second: 124, episode reward: -68.319, mean reward: -0.887 [-100.000, 21.222], mean action: 1.662 [0.000, 3.000],  loss: 7.911657, mse: 5869.397347, mean_q: 58.940962, mean_eps: 0.581878
  69860/150000: episode: 708, duration: 1.287s, episode steps: 134, steps per second: 104, episode reward: -100.846, mean reward: -0.753 [-100.000, 47.402], mean action: 1.507 [0.000, 3.000],  loss: 5.367126, mse: 5825.514415, mean_q: 57.765355, mean_eps: 0.581245
  69945/150000: episode: 709, duration: 0.670s, episode steps:  85, steps per second: 127, episode reward: -54.060, mean reward: -0.636 [-100.000, 12.652], mean action: 1.718 [0.000, 3.000],  loss: 6.744407, mse: 5999.723949, mean_q: 59.029329, mean_eps: 0.580588
  70049/150000: episode: 710, duration: 0.901s, episode steps: 104, steps per second: 115, episode reward: -37.207, mean reward: -0.358 [-100.000, 15.252], mean action: 1.683 [0.000, 3.000],  loss: 6.371544, mse: 6036.322303, mean_q: 60.635945, mean_eps: 0.580021
  70140/150000: episode: 711, duration: 0.703s, episode steps:  91, steps per second: 129, episode reward: -87.601, mean reward: -0.963 [-100.000, 12.096], mean action: 1.593 [0.000, 3.000],  loss: 11.172219, mse: 6061.565848, mean_q: 61.025647, mean_eps: 0.579436
  70234/150000: episode: 712, duration: 0.736s, episode steps:  94, steps per second: 128, episode reward: -151.069, mean reward: -1.607 [-100.000, 11.917], mean action: 1.415 [0.000, 3.000],  loss: 4.887513, mse: 5895.020887, mean_q: 58.389733, mean_eps: 0.578881
  70365/150000: episode: 713, duration: 1.049s, episode steps: 131, steps per second: 125, episode reward: -34.788, mean reward: -0.266 [-100.000, 17.832], mean action: 1.603 [0.000, 3.000],  loss: 9.317449, mse: 5923.203941, mean_q: 58.972163, mean_eps: 0.578206
  70462/150000: episode: 714, duration: 0.744s, episode steps:  97, steps per second: 130, episode reward: -79.818, mean reward: -0.823 [-100.000, 20.422], mean action: 1.680 [0.000, 3.000],  loss: 9.212395, mse: 6003.536822, mean_q: 59.985653, mean_eps: 0.577522
  70575/150000: episode: 715, duration: 0.929s, episode steps: 113, steps per second: 122, episode reward: -182.957, mean reward: -1.619 [-100.000, 25.910], mean action: 1.558 [0.000, 3.000],  loss: 5.397813, mse: 6017.703181, mean_q: 60.215812, mean_eps: 0.576892
  70668/150000: episode: 716, duration: 0.871s, episode steps:  93, steps per second: 107, episode reward: -90.068, mean reward: -0.968 [-100.000,  6.144], mean action: 1.548 [0.000, 3.000],  loss: 5.965344, mse: 6110.635317, mean_q: 60.940809, mean_eps: 0.576274
  70760/150000: episode: 717, duration: 0.858s, episode steps:  92, steps per second: 107, episode reward: -54.097, mean reward: -0.588 [-100.000, 11.649], mean action: 1.511 [0.000, 3.000],  loss: 5.444213, mse: 6121.119316, mean_q: 61.749848, mean_eps: 0.575719
  70870/150000: episode: 718, duration: 1.035s, episode steps: 110, steps per second: 106, episode reward: -60.468, mean reward: -0.550 [-100.000,  7.592], mean action: 1.682 [0.000, 3.000],  loss: 7.811202, mse: 6040.052388, mean_q: 59.169934, mean_eps: 0.575113
  70979/150000: episode: 719, duration: 1.005s, episode steps: 109, steps per second: 108, episode reward: -104.371, mean reward: -0.958 [-100.000,  7.371], mean action: 1.716 [0.000, 3.000],  loss: 7.527660, mse: 6067.948551, mean_q: 60.991939, mean_eps: 0.574456
  71103/150000: episode: 720, duration: 1.084s, episode steps: 124, steps per second: 114, episode reward: -77.552, mean reward: -0.625 [-100.000, 13.410], mean action: 1.734 [0.000, 3.000],  loss: 6.744887, mse: 6166.487848, mean_q: 61.004279, mean_eps: 0.573757
  71183/150000: episode: 721, duration: 0.705s, episode steps:  80, steps per second: 114, episode reward: -59.515, mean reward: -0.744 [-100.000,  4.639], mean action: 1.587 [0.000, 3.000],  loss: 7.854893, mse: 6395.139136, mean_q: 62.640687, mean_eps: 0.573145
  71298/150000: episode: 722, duration: 0.960s, episode steps: 115, steps per second: 120, episode reward: -55.485, mean reward: -0.482 [-100.000, 16.389], mean action: 1.765 [0.000, 3.000],  loss: 6.990510, mse: 6229.465793, mean_q: 60.705643, mean_eps: 0.572560
  71373/150000: episode: 723, duration: 0.578s, episode steps:  75, steps per second: 130, episode reward: -46.639, mean reward: -0.622 [-100.000,  9.417], mean action: 1.747 [0.000, 3.000],  loss: 9.272787, mse: 6158.104095, mean_q: 60.368008, mean_eps: 0.571990
  71474/150000: episode: 724, duration: 0.766s, episode steps: 101, steps per second: 132, episode reward: -94.705, mean reward: -0.938 [-100.000, 19.976], mean action: 1.693 [0.000, 3.000],  loss: 6.945795, mse: 6185.899124, mean_q: 59.845009, mean_eps: 0.571462
  71608/150000: episode: 725, duration: 1.189s, episode steps: 134, steps per second: 113, episode reward: -55.087, mean reward: -0.411 [-100.000, 10.030], mean action: 1.627 [0.000, 3.000],  loss: 6.518956, mse: 6156.406301, mean_q: 60.551358, mean_eps: 0.570757
  71711/150000: episode: 726, duration: 0.905s, episode steps: 103, steps per second: 114, episode reward: -64.119, mean reward: -0.623 [-100.000,  7.155], mean action: 1.854 [0.000, 3.000],  loss: 5.917492, mse: 6233.557176, mean_q: 61.736640, mean_eps: 0.570046
  71856/150000: episode: 727, duration: 1.367s, episode steps: 145, steps per second: 106, episode reward: 31.090, mean reward:  0.214 [-100.000, 24.453], mean action: 1.607 [0.000, 3.000],  loss: 5.695109, mse: 6164.917255, mean_q: 60.300428, mean_eps: 0.569302
  71955/150000: episode: 728, duration: 0.793s, episode steps:  99, steps per second: 125, episode reward: -66.833, mean reward: -0.675 [-100.000, 10.586], mean action: 1.646 [0.000, 3.000],  loss: 6.895989, mse: 6333.905599, mean_q: 62.279644, mean_eps: 0.568570
  72038/150000: episode: 729, duration: 0.734s, episode steps:  83, steps per second: 113, episode reward: -22.715, mean reward: -0.274 [-100.000,  6.558], mean action: 1.795 [0.000, 3.000],  loss: 6.003342, mse: 6244.846191, mean_q: 61.179666, mean_eps: 0.568024
  72147/150000: episode: 730, duration: 0.989s, episode steps: 109, steps per second: 110, episode reward: -108.731, mean reward: -0.998 [-100.000,  8.963], mean action: 1.862 [0.000, 3.000],  loss: 6.006280, mse: 6223.464512, mean_q: 61.072880, mean_eps: 0.567448
  73147/150000: episode: 731, duration: 10.434s, episode steps: 1000, steps per second:  96, episode reward: 14.835, mean reward:  0.015 [-22.917, 16.061], mean action: 1.728 [0.000, 3.000],  loss: 7.126587, mse: 6221.334563, mean_q: 60.884843, mean_eps: 0.564121
  73209/150000: episode: 732, duration: 0.738s, episode steps:  62, steps per second:  84, episode reward: -52.531, mean reward: -0.847 [-100.000, 21.053], mean action: 1.774 [0.000, 3.000],  loss: 6.781595, mse: 6118.092025, mean_q: 60.468768, mean_eps: 0.560935
  73307/150000: episode: 733, duration: 1.030s, episode steps:  98, steps per second:  95, episode reward: -36.705, mean reward: -0.375 [-100.000, 16.424], mean action: 1.765 [0.000, 3.000],  loss: 11.140531, mse: 6204.887232, mean_q: 59.514550, mean_eps: 0.560455
  73392/150000: episode: 734, duration: 0.661s, episode steps:  85, steps per second: 129, episode reward: -107.860, mean reward: -1.269 [-100.000, 23.895], mean action: 1.835 [0.000, 3.000],  loss: 12.128780, mse: 6220.878119, mean_q: 60.170964, mean_eps: 0.559906
  73498/150000: episode: 735, duration: 0.873s, episode steps: 106, steps per second: 121, episode reward: -76.988, mean reward: -0.726 [-100.000, 10.435], mean action: 1.698 [0.000, 3.000],  loss: 10.754826, mse: 6202.448675, mean_q: 61.163662, mean_eps: 0.559333
  73597/150000: episode: 736, duration: 0.879s, episode steps:  99, steps per second: 113, episode reward: 13.437, mean reward:  0.136 [-100.000, 17.737], mean action: 1.697 [0.000, 3.000],  loss: 7.288708, mse: 6175.611442, mean_q: 60.087002, mean_eps: 0.558718
  73681/150000: episode: 737, duration: 0.751s, episode steps:  84, steps per second: 112, episode reward: -10.918, mean reward: -0.130 [-100.000, 24.570], mean action: 1.702 [0.000, 3.000],  loss: 6.421659, mse: 6120.505697, mean_q: 59.378851, mean_eps: 0.558169
  73757/150000: episode: 738, duration: 0.822s, episode steps:  76, steps per second:  92, episode reward: -86.673, mean reward: -1.140 [-100.000, 11.301], mean action: 1.461 [0.000, 3.000],  loss: 7.013508, mse: 6180.466026, mean_q: 59.572090, mean_eps: 0.557689
  73848/150000: episode: 739, duration: 1.029s, episode steps:  91, steps per second:  88, episode reward: -22.324, mean reward: -0.245 [-100.000, 12.010], mean action: 1.582 [0.000, 3.000],  loss: 9.264979, mse: 6231.635334, mean_q: 62.300897, mean_eps: 0.557188
  73972/150000: episode: 740, duration: 1.097s, episode steps: 124, steps per second: 113, episode reward: -175.574, mean reward: -1.416 [-100.000, 62.281], mean action: 1.766 [0.000, 3.000],  loss: 11.117132, mse: 6202.202416, mean_q: 60.809006, mean_eps: 0.556543
  74083/150000: episode: 741, duration: 0.945s, episode steps: 111, steps per second: 117, episode reward: -11.471, mean reward: -0.103 [-100.000, 12.581], mean action: 1.640 [0.000, 3.000],  loss: 8.839467, mse: 6192.426837, mean_q: 61.010977, mean_eps: 0.555838
  74197/150000: episode: 742, duration: 0.948s, episode steps: 114, steps per second: 120, episode reward: -49.819, mean reward: -0.437 [-100.000, 11.227], mean action: 1.807 [0.000, 3.000],  loss: 7.648175, mse: 6160.385194, mean_q: 61.690178, mean_eps: 0.555163
  74307/150000: episode: 743, duration: 0.874s, episode steps: 110, steps per second: 126, episode reward: -109.154, mean reward: -0.992 [-100.000, 26.069], mean action: 1.473 [0.000, 3.000],  loss: 9.803361, mse: 6109.346911, mean_q: 60.275678, mean_eps: 0.554491
  74391/150000: episode: 744, duration: 0.753s, episode steps:  84, steps per second: 112, episode reward: -66.212, mean reward: -0.788 [-100.000,  8.638], mean action: 1.726 [0.000, 3.000],  loss: 11.926833, mse: 6221.050985, mean_q: 61.392184, mean_eps: 0.553909
  74514/150000: episode: 745, duration: 0.990s, episode steps: 123, steps per second: 124, episode reward: -96.270, mean reward: -0.783 [-100.000, 10.440], mean action: 1.691 [0.000, 3.000],  loss: 8.875655, mse: 6169.972557, mean_q: 60.083269, mean_eps: 0.553288
  74597/150000: episode: 746, duration: 0.640s, episode steps:  83, steps per second: 130, episode reward: -6.637, mean reward: -0.080 [-100.000, 11.630], mean action: 1.747 [0.000, 3.000],  loss: 7.384862, mse: 6211.841303, mean_q: 60.661001, mean_eps: 0.552670
  74696/150000: episode: 747, duration: 0.723s, episode steps:  99, steps per second: 137, episode reward: -59.929, mean reward: -0.605 [-100.000,  9.530], mean action: 1.465 [0.000, 3.000],  loss: 5.922251, mse: 6164.006422, mean_q: 60.890218, mean_eps: 0.552124
  74848/150000: episode: 748, duration: 1.213s, episode steps: 152, steps per second: 125, episode reward: -14.530, mean reward: -0.096 [-100.000, 42.670], mean action: 1.704 [0.000, 3.000],  loss: 7.503956, mse: 6102.852305, mean_q: 59.982706, mean_eps: 0.551371
  74948/150000: episode: 749, duration: 0.746s, episode steps: 100, steps per second: 134, episode reward: -36.224, mean reward: -0.362 [-100.000, 12.988], mean action: 1.520 [0.000, 3.000],  loss: 7.232581, mse: 6190.753403, mean_q: 60.114014, mean_eps: 0.550615
  75069/150000: episode: 750, duration: 1.062s, episode steps: 121, steps per second: 114, episode reward: -122.410, mean reward: -1.012 [-100.000, 16.146], mean action: 1.686 [0.000, 3.000],  loss: 11.626121, mse: 6132.640887, mean_q: 60.720769, mean_eps: 0.549952
  75169/150000: episode: 751, duration: 1.017s, episode steps: 100, steps per second:  98, episode reward: -1.430, mean reward: -0.014 [-100.000, 10.916], mean action: 1.610 [0.000, 3.000],  loss: 7.088215, mse: 6045.102803, mean_q: 59.482837, mean_eps: 0.549289
  75275/150000: episode: 752, duration: 1.044s, episode steps: 106, steps per second: 102, episode reward: 37.237, mean reward:  0.351 [-100.000, 55.461], mean action: 1.670 [0.000, 3.000],  loss: 7.128143, mse: 6090.947966, mean_q: 59.929292, mean_eps: 0.548671
  75390/150000: episode: 753, duration: 0.880s, episode steps: 115, steps per second: 131, episode reward: -78.949, mean reward: -0.687 [-100.000, 10.059], mean action: 1.696 [0.000, 3.000],  loss: 8.821788, mse: 6094.723960, mean_q: 60.516096, mean_eps: 0.548008
  75461/150000: episode: 754, duration: 0.517s, episode steps:  71, steps per second: 137, episode reward: -17.965, mean reward: -0.253 [-100.000, 16.695], mean action: 1.662 [0.000, 3.000],  loss: 6.356236, mse: 6126.095696, mean_q: 61.236715, mean_eps: 0.547450
  75793/150000: episode: 755, duration: 2.891s, episode steps: 332, steps per second: 115, episode reward: 215.637, mean reward:  0.650 [-24.796, 100.000], mean action: 2.102 [0.000, 3.000],  loss: 7.549746, mse: 6128.539377, mean_q: 61.369580, mean_eps: 0.546241
  75947/150000: episode: 756, duration: 1.303s, episode steps: 154, steps per second: 118, episode reward: -117.523, mean reward: -0.763 [-100.000, 18.196], mean action: 1.695 [0.000, 3.000],  loss: 9.897969, mse: 6157.437652, mean_q: 61.132238, mean_eps: 0.544783
  76079/150000: episode: 757, duration: 1.105s, episode steps: 132, steps per second: 119, episode reward: -63.755, mean reward: -0.483 [-100.000, 18.300], mean action: 1.682 [0.000, 3.000],  loss: 6.338706, mse: 6170.487911, mean_q: 61.563606, mean_eps: 0.543925
  76170/150000: episode: 758, duration: 0.826s, episode steps:  91, steps per second: 110, episode reward: -30.708, mean reward: -0.337 [-100.000, 19.512], mean action: 1.604 [0.000, 3.000],  loss: 7.923673, mse: 6178.938863, mean_q: 60.093058, mean_eps: 0.543256
  76283/150000: episode: 759, duration: 0.939s, episode steps: 113, steps per second: 120, episode reward: -26.007, mean reward: -0.230 [-100.000, 11.667], mean action: 1.637 [0.000, 3.000],  loss: 6.655287, mse: 6189.558244, mean_q: 60.833584, mean_eps: 0.542644
  76406/150000: episode: 760, duration: 1.000s, episode steps: 123, steps per second: 123, episode reward: -217.288, mean reward: -1.767 [-100.000, 111.260], mean action: 1.634 [0.000, 3.000],  loss: 11.727374, mse: 6194.408346, mean_q: 62.232661, mean_eps: 0.541936
  76513/150000: episode: 761, duration: 0.823s, episode steps: 107, steps per second: 130, episode reward: -2.661, mean reward: -0.025 [-100.000, 17.726], mean action: 1.636 [0.000, 3.000],  loss: 9.564295, mse: 6335.417636, mean_q: 62.655394, mean_eps: 0.541246
  76611/150000: episode: 762, duration: 0.800s, episode steps:  98, steps per second: 122, episode reward: 18.528, mean reward:  0.189 [-100.000, 19.358], mean action: 1.786 [0.000, 3.000],  loss: 10.949335, mse: 6310.024559, mean_q: 62.840772, mean_eps: 0.540631
  76700/150000: episode: 763, duration: 0.711s, episode steps:  89, steps per second: 125, episode reward: -102.568, mean reward: -1.152 [-100.000,  7.412], mean action: 1.551 [0.000, 3.000],  loss: 6.519349, mse: 6196.510627, mean_q: 62.014992, mean_eps: 0.540070
  76793/150000: episode: 764, duration: 0.817s, episode steps:  93, steps per second: 114, episode reward: -30.022, mean reward: -0.323 [-100.000, 16.184], mean action: 1.720 [0.000, 3.000],  loss: 9.257015, mse: 6253.969601, mean_q: 61.400297, mean_eps: 0.539524
  76922/150000: episode: 765, duration: 1.020s, episode steps: 129, steps per second: 126, episode reward: -55.587, mean reward: -0.431 [-100.000, 45.294], mean action: 1.674 [0.000, 3.000],  loss: 7.712764, mse: 6193.100075, mean_q: 61.517167, mean_eps: 0.538858
  77028/150000: episode: 766, duration: 0.781s, episode steps: 106, steps per second: 136, episode reward: -59.759, mean reward: -0.564 [-100.000, 12.585], mean action: 1.736 [0.000, 3.000],  loss: 7.647000, mse: 6461.878713, mean_q: 64.221105, mean_eps: 0.538153
  77124/150000: episode: 767, duration: 0.769s, episode steps:  96, steps per second: 125, episode reward: -52.178, mean reward: -0.544 [-100.000, 19.364], mean action: 1.729 [0.000, 3.000],  loss: 9.906218, mse: 6382.993566, mean_q: 60.707891, mean_eps: 0.537547
  77211/150000: episode: 768, duration: 0.686s, episode steps:  87, steps per second: 127, episode reward: 25.277, mean reward:  0.291 [-100.000, 18.153], mean action: 1.678 [0.000, 3.000],  loss: 8.180086, mse: 6416.805810, mean_q: 62.750047, mean_eps: 0.536998
  77291/150000: episode: 769, duration: 0.588s, episode steps:  80, steps per second: 136, episode reward: -21.473, mean reward: -0.268 [-100.000, 37.781], mean action: 1.700 [0.000, 3.000],  loss: 10.891920, mse: 6384.627631, mean_q: 61.957512, mean_eps: 0.536497
  77359/150000: episode: 770, duration: 0.543s, episode steps:  68, steps per second: 125, episode reward: -35.961, mean reward: -0.529 [-100.000, 15.170], mean action: 1.897 [0.000, 3.000],  loss: 6.170507, mse: 6296.228738, mean_q: 61.249344, mean_eps: 0.536053
  77752/150000: episode: 771, duration: 3.320s, episode steps: 393, steps per second: 118, episode reward: -145.804, mean reward: -0.371 [-100.000, 19.063], mean action: 1.723 [0.000, 3.000],  loss: 8.296798, mse: 6361.111931, mean_q: 61.630912, mean_eps: 0.534670
  77838/150000: episode: 772, duration: 0.649s, episode steps:  86, steps per second: 132, episode reward: -18.916, mean reward: -0.220 [-100.000, 11.148], mean action: 1.674 [0.000, 3.000],  loss: 16.478209, mse: 6382.068950, mean_q: 63.310949, mean_eps: 0.533233
  77937/150000: episode: 773, duration: 0.802s, episode steps:  99, steps per second: 123, episode reward: -128.918, mean reward: -1.302 [-100.000,  9.742], mean action: 1.697 [0.000, 3.000],  loss: 9.302801, mse: 6282.509169, mean_q: 61.704265, mean_eps: 0.532678
  78045/150000: episode: 774, duration: 0.968s, episode steps: 108, steps per second: 112, episode reward: -47.334, mean reward: -0.438 [-100.000,  8.598], mean action: 1.667 [0.000, 3.000],  loss: 9.568845, mse: 6315.513559, mean_q: 62.250391, mean_eps: 0.532057
  78172/150000: episode: 775, duration: 1.049s, episode steps: 127, steps per second: 121, episode reward: -81.086, mean reward: -0.638 [-100.000,  9.891], mean action: 1.693 [0.000, 3.000],  loss: 9.149251, mse: 6379.255383, mean_q: 61.819479, mean_eps: 0.531352
  78328/150000: episode: 776, duration: 1.350s, episode steps: 156, steps per second: 116, episode reward: -22.594, mean reward: -0.145 [-100.000, 11.217], mean action: 1.667 [0.000, 3.000],  loss: 9.677713, mse: 6411.357710, mean_q: 62.365409, mean_eps: 0.530503
  78432/150000: episode: 777, duration: 0.951s, episode steps: 104, steps per second: 109, episode reward: -63.464, mean reward: -0.610 [-100.000, 10.457], mean action: 1.740 [0.000, 3.000],  loss: 11.546905, mse: 6399.069139, mean_q: 62.248507, mean_eps: 0.529723
  78815/150000: episode: 778, duration: 3.308s, episode steps: 383, steps per second: 116, episode reward: -54.685, mean reward: -0.143 [-100.000, 36.920], mean action: 1.791 [0.000, 3.000],  loss: 8.239506, mse: 6451.629434, mean_q: 62.895681, mean_eps: 0.528262
  78917/150000: episode: 779, duration: 0.903s, episode steps: 102, steps per second: 113, episode reward: -47.670, mean reward: -0.467 [-100.000,  9.842], mean action: 1.608 [0.000, 3.000],  loss: 9.583289, mse: 6513.187845, mean_q: 62.757347, mean_eps: 0.526807
  79057/150000: episode: 780, duration: 1.203s, episode steps: 140, steps per second: 116, episode reward: -217.325, mean reward: -1.552 [-100.000, 35.889], mean action: 1.793 [0.000, 3.000],  loss: 7.936996, mse: 6453.106934, mean_q: 63.148931, mean_eps: 0.526081
  79243/150000: episode: 781, duration: 1.799s, episode steps: 186, steps per second: 103, episode reward: -207.503, mean reward: -1.116 [-100.000, 53.514], mean action: 1.651 [0.000, 3.000],  loss: 9.007596, mse: 6557.201469, mean_q: 63.682738, mean_eps: 0.525103
  79374/150000: episode: 782, duration: 1.207s, episode steps: 131, steps per second: 109, episode reward: -37.162, mean reward: -0.284 [-100.000,  8.186], mean action: 1.779 [0.000, 3.000],  loss: 14.445558, mse: 6576.649485, mean_q: 64.069890, mean_eps: 0.524152
  79491/150000: episode: 783, duration: 0.998s, episode steps: 117, steps per second: 117, episode reward: -238.694, mean reward: -2.040 [-100.000, 19.872], mean action: 1.735 [0.000, 3.000],  loss: 8.136185, mse: 6469.427688, mean_q: 63.155180, mean_eps: 0.523408
  79599/150000: episode: 784, duration: 1.028s, episode steps: 108, steps per second: 105, episode reward: -26.080, mean reward: -0.241 [-100.000, 16.151], mean action: 1.806 [0.000, 3.000],  loss: 8.318076, mse: 6445.542313, mean_q: 62.491321, mean_eps: 0.522733
  79701/150000: episode: 785, duration: 0.892s, episode steps: 102, steps per second: 114, episode reward: -49.048, mean reward: -0.481 [-100.000, 14.615], mean action: 1.510 [0.000, 3.000],  loss: 6.836355, mse: 6488.272155, mean_q: 63.014497, mean_eps: 0.522103
  79795/150000: episode: 786, duration: 0.756s, episode steps:  94, steps per second: 124, episode reward:  3.708, mean reward:  0.039 [-100.000, 11.609], mean action: 1.596 [0.000, 3.000],  loss: 9.382718, mse: 6561.926592, mean_q: 62.827939, mean_eps: 0.521515
  79883/150000: episode: 787, duration: 0.740s, episode steps:  88, steps per second: 119, episode reward: -36.557, mean reward: -0.415 [-100.000, 14.513], mean action: 1.659 [0.000, 3.000],  loss: 8.684316, mse: 6562.242287, mean_q: 63.397450, mean_eps: 0.520969
  80005/150000: episode: 788, duration: 1.053s, episode steps: 122, steps per second: 116, episode reward: -44.148, mean reward: -0.362 [-100.000, 17.168], mean action: 1.721 [0.000, 3.000],  loss: 8.723450, mse: 6638.959301, mean_q: 64.309003, mean_eps: 0.520339
  80114/150000: episode: 789, duration: 0.884s, episode steps: 109, steps per second: 123, episode reward:  5.678, mean reward:  0.052 [-100.000, 17.527], mean action: 1.798 [0.000, 3.000],  loss: 8.148846, mse: 6567.890052, mean_q: 63.325438, mean_eps: 0.519646
  80241/150000: episode: 790, duration: 1.025s, episode steps: 127, steps per second: 124, episode reward: 35.598, mean reward:  0.280 [-100.000, 17.829], mean action: 1.795 [0.000, 3.000],  loss: 11.114164, mse: 6572.425604, mean_q: 62.912563, mean_eps: 0.518938
  81059/150000: episode: 791, duration: 7.965s, episode steps: 818, steps per second: 103, episode reward: -158.971, mean reward: -0.194 [-100.000, 20.046], mean action: 1.736 [0.000, 3.000],  loss: 9.107247, mse: 6602.746822, mean_q: 63.831581, mean_eps: 0.516103
  81172/150000: episode: 792, duration: 1.156s, episode steps: 113, steps per second:  98, episode reward: -40.170, mean reward: -0.355 [-100.000, 20.678], mean action: 1.611 [0.000, 3.000],  loss: 8.408834, mse: 6596.416997, mean_q: 63.644336, mean_eps: 0.513310
  81313/150000: episode: 793, duration: 1.400s, episode steps: 141, steps per second: 101, episode reward:  5.034, mean reward:  0.036 [-100.000, 22.206], mean action: 1.716 [0.000, 3.000],  loss: 8.045458, mse: 6621.378356, mean_q: 63.876278, mean_eps: 0.512548
  81426/150000: episode: 794, duration: 1.097s, episode steps: 113, steps per second: 103, episode reward: 16.172, mean reward:  0.143 [-100.000, 13.622], mean action: 1.779 [0.000, 3.000],  loss: 8.740278, mse: 6544.609833, mean_q: 63.878400, mean_eps: 0.511786
  82426/150000: episode: 795, duration: 12.010s, episode steps: 1000, steps per second:  83, episode reward: -31.982, mean reward: -0.032 [-21.869, 23.509], mean action: 1.726 [0.000, 3.000],  loss: 9.496833, mse: 6541.375710, mean_q: 63.362006, mean_eps: 0.508447
  82525/150000: episode: 796, duration: 0.892s, episode steps:  99, steps per second: 111, episode reward: -13.246, mean reward: -0.134 [-100.000,  6.920], mean action: 1.586 [0.000, 3.000],  loss: 8.620398, mse: 6416.247869, mean_q: 62.325011, mean_eps: 0.505150
  82667/150000: episode: 797, duration: 1.434s, episode steps: 142, steps per second:  99, episode reward:  5.266, mean reward:  0.037 [-100.000, 17.620], mean action: 1.655 [0.000, 3.000],  loss: 9.553235, mse: 6391.653197, mean_q: 61.747079, mean_eps: 0.504427
  83581/150000: episode: 798, duration: 9.389s, episode steps: 914, steps per second:  97, episode reward: -168.508, mean reward: -0.184 [-100.000, 25.451], mean action: 1.765 [0.000, 3.000],  loss: 10.282968, mse: 6450.614277, mean_q: 62.702211, mean_eps: 0.501259
  83749/150000: episode: 799, duration: 1.604s, episode steps: 168, steps per second: 105, episode reward: -187.124, mean reward: -1.114 [-100.000, 56.352], mean action: 1.619 [0.000, 3.000],  loss: 9.173630, mse: 6487.481120, mean_q: 63.403948, mean_eps: 0.498013
  84714/150000: episode: 800, duration: 10.245s, episode steps: 965, steps per second:  94, episode reward: -106.489, mean reward: -0.110 [-100.000, 22.613], mean action: 1.756 [0.000, 3.000],  loss: 9.503242, mse: 6461.856616, mean_q: 62.262981, mean_eps: 0.494614
  84873/150000: episode: 801, duration: 1.459s, episode steps: 159, steps per second: 109, episode reward: -145.046, mean reward: -0.912 [-100.000, 73.810], mean action: 1.912 [0.000, 3.000],  loss: 9.990682, mse: 6462.564063, mean_q: 62.428752, mean_eps: 0.491242
  85873/150000: episode: 802, duration: 11.440s, episode steps: 1000, steps per second:  87, episode reward: -83.824, mean reward: -0.084 [-16.742, 20.611], mean action: 1.720 [0.000, 3.000],  loss: 11.635626, mse: 6421.231188, mean_q: 61.970779, mean_eps: 0.487765
  86067/150000: episode: 803, duration: 1.972s, episode steps: 194, steps per second:  98, episode reward: -8.111, mean reward: -0.042 [-100.000,  9.347], mean action: 1.778 [0.000, 3.000],  loss: 11.242137, mse: 6476.999464, mean_q: 62.478991, mean_eps: 0.484183
  86171/150000: episode: 804, duration: 0.966s, episode steps: 104, steps per second: 108, episode reward:  4.907, mean reward:  0.047 [-100.000, 15.842], mean action: 1.731 [0.000, 3.000],  loss: 14.507119, mse: 6667.752122, mean_q: 64.680235, mean_eps: 0.483289
  87171/150000: episode: 805, duration: 12.594s, episode steps: 1000, steps per second:  79, episode reward: -40.436, mean reward: -0.040 [-19.036, 20.028], mean action: 1.721 [0.000, 3.000],  loss: 11.498194, mse: 6499.490036, mean_q: 63.105564, mean_eps: 0.479977
  87835/150000: episode: 806, duration: 8.672s, episode steps: 664, steps per second:  77, episode reward: -134.636, mean reward: -0.203 [-100.000, 46.423], mean action: 1.770 [0.000, 3.000],  loss: 13.155126, mse: 6559.172822, mean_q: 64.090087, mean_eps: 0.474985
  87941/150000: episode: 807, duration: 1.222s, episode steps: 106, steps per second:  87, episode reward: -28.544, mean reward: -0.269 [-100.000, 15.706], mean action: 1.736 [0.000, 3.000],  loss: 12.013851, mse: 6471.270535, mean_q: 63.429155, mean_eps: 0.472675
  88030/150000: episode: 808, duration: 1.003s, episode steps:  89, steps per second:  89, episode reward: -54.397, mean reward: -0.611 [-100.000,  9.304], mean action: 1.618 [0.000, 3.000],  loss: 18.873461, mse: 6449.019131, mean_q: 63.570623, mean_eps: 0.472090
  88118/150000: episode: 809, duration: 1.234s, episode steps:  88, steps per second:  71, episode reward: -41.551, mean reward: -0.472 [-100.000, 10.266], mean action: 1.761 [0.000, 3.000],  loss: 11.986207, mse: 6204.072804, mean_q: 61.790826, mean_eps: 0.471559
  88230/150000: episode: 810, duration: 2.080s, episode steps: 112, steps per second:  54, episode reward: -4.980, mean reward: -0.044 [-100.000, 17.805], mean action: 1.643 [0.000, 3.000],  loss: 10.595683, mse: 6347.245893, mean_q: 62.933260, mean_eps: 0.470959
  88351/150000: episode: 811, duration: 4.283s, episode steps: 121, steps per second:  28, episode reward: -32.172, mean reward: -0.266 [-100.000, 10.798], mean action: 1.826 [0.000, 3.000],  loss: 13.694879, mse: 6378.861937, mean_q: 62.798774, mean_eps: 0.470260
  88452/150000: episode: 812, duration: 2.117s, episode steps: 101, steps per second:  48, episode reward: -38.929, mean reward: -0.385 [-100.000, 17.895], mean action: 1.644 [0.000, 3.000],  loss: 14.770954, mse: 6460.291025, mean_q: 63.473871, mean_eps: 0.469594
  88548/150000: episode: 813, duration: 1.193s, episode steps:  96, steps per second:  80, episode reward: -51.666, mean reward: -0.538 [-100.000, 11.744], mean action: 1.708 [0.000, 3.000],  loss: 17.492499, mse: 6439.555791, mean_q: 63.083443, mean_eps: 0.469003
  88651/150000: episode: 814, duration: 1.197s, episode steps: 103, steps per second:  86, episode reward: -92.592, mean reward: -0.899 [-100.000,  8.754], mean action: 1.718 [0.000, 3.000],  loss: 12.527515, mse: 6493.869762, mean_q: 64.452614, mean_eps: 0.468406
  88866/150000: episode: 815, duration: 2.408s, episode steps: 215, steps per second:  89, episode reward: -154.912, mean reward: -0.721 [-100.000, 21.902], mean action: 1.767 [0.000, 3.000],  loss: 14.211264, mse: 6459.592219, mean_q: 63.180494, mean_eps: 0.467452
  89866/150000: episode: 816, duration: 12.109s, episode steps: 1000, steps per second:  83, episode reward: -61.064, mean reward: -0.061 [-25.296, 27.231], mean action: 1.582 [0.000, 3.000],  loss: 13.421619, mse: 6456.907766, mean_q: 63.589356, mean_eps: 0.463807
  89987/150000: episode: 817, duration: 1.133s, episode steps: 121, steps per second: 107, episode reward: 10.990, mean reward:  0.091 [-100.000, 20.248], mean action: 1.744 [0.000, 3.000],  loss: 14.042182, mse: 6469.041064, mean_q: 62.794713, mean_eps: 0.460444
  90094/150000: episode: 818, duration: 1.116s, episode steps: 107, steps per second:  96, episode reward: -233.294, mean reward: -2.180 [-100.000, 41.772], mean action: 1.636 [0.000, 3.000],  loss: 12.859988, mse: 6589.682635, mean_q: 64.726135, mean_eps: 0.459760
  91094/150000: episode: 819, duration: 12.105s, episode steps: 1000, steps per second:  83, episode reward: -9.338, mean reward: -0.009 [-19.058, 16.616], mean action: 1.809 [0.000, 3.000],  loss: 12.991145, mse: 6506.532991, mean_q: 65.005736, mean_eps: 0.456439
  91225/150000: episode: 820, duration: 1.555s, episode steps: 131, steps per second:  84, episode reward: -25.103, mean reward: -0.192 [-100.000,  8.504], mean action: 1.740 [0.000, 3.000],  loss: 18.332700, mse: 6479.005949, mean_q: 65.449606, mean_eps: 0.453046
  91311/150000: episode: 821, duration: 0.773s, episode steps:  86, steps per second: 111, episode reward: -40.480, mean reward: -0.471 [-100.000, 20.174], mean action: 1.872 [0.000, 3.000],  loss: 17.606301, mse: 6627.830714, mean_q: 67.223305, mean_eps: 0.452395
  91439/150000: episode: 822, duration: 1.250s, episode steps: 128, steps per second: 102, episode reward: -37.153, mean reward: -0.290 [-100.000, 12.417], mean action: 1.586 [0.000, 3.000],  loss: 15.080375, mse: 6616.591515, mean_q: 66.350169, mean_eps: 0.451753
  91568/150000: episode: 823, duration: 1.123s, episode steps: 129, steps per second: 115, episode reward: 13.197, mean reward:  0.102 [-100.000, 34.304], mean action: 1.884 [0.000, 3.000],  loss: 12.667265, mse: 6577.521477, mean_q: 66.095894, mean_eps: 0.450982
  91694/150000: episode: 824, duration: 1.189s, episode steps: 126, steps per second: 106, episode reward: -14.895, mean reward: -0.118 [-100.000, 16.276], mean action: 1.754 [0.000, 3.000],  loss: 14.326623, mse: 6559.266493, mean_q: 66.368816, mean_eps: 0.450217
  91797/150000: episode: 825, duration: 1.030s, episode steps: 103, steps per second: 100, episode reward: -143.722, mean reward: -1.395 [-100.000, 52.448], mean action: 1.641 [0.000, 3.000],  loss: 13.922504, mse: 6526.028880, mean_q: 65.147211, mean_eps: 0.449530
  91916/150000: episode: 826, duration: 1.355s, episode steps: 119, steps per second:  88, episode reward:  3.063, mean reward:  0.026 [-100.000, 16.310], mean action: 1.798 [0.000, 3.000],  loss: 13.566616, mse: 6568.273971, mean_q: 65.653694, mean_eps: 0.448864
  92024/150000: episode: 827, duration: 1.009s, episode steps: 108, steps per second: 107, episode reward: -72.107, mean reward: -0.668 [-100.000, 12.800], mean action: 1.685 [0.000, 3.000],  loss: 13.984558, mse: 6605.039890, mean_q: 66.540135, mean_eps: 0.448183
  92150/150000: episode: 828, duration: 1.187s, episode steps: 126, steps per second: 106, episode reward: -43.382, mean reward: -0.344 [-100.000, 13.043], mean action: 1.563 [0.000, 3.000],  loss: 18.576821, mse: 6537.005875, mean_q: 66.532526, mean_eps: 0.447481
  92284/150000: episode: 829, duration: 1.299s, episode steps: 134, steps per second: 103, episode reward: -44.180, mean reward: -0.330 [-100.000,  8.947], mean action: 1.851 [0.000, 3.000],  loss: 19.719235, mse: 6579.667743, mean_q: 67.285571, mean_eps: 0.446701
  93284/150000: episode: 830, duration: 12.039s, episode steps: 1000, steps per second:  83, episode reward: -75.677, mean reward: -0.076 [-21.215, 18.757], mean action: 1.728 [0.000, 3.000],  loss: 16.256349, mse: 6611.043034, mean_q: 67.581141, mean_eps: 0.443299
  93417/150000: episode: 831, duration: 1.218s, episode steps: 133, steps per second: 109, episode reward: -21.097, mean reward: -0.159 [-100.000, 11.994], mean action: 1.752 [0.000, 3.000],  loss: 16.954774, mse: 6649.095725, mean_q: 67.903812, mean_eps: 0.439900
  93544/150000: episode: 832, duration: 1.125s, episode steps: 127, steps per second: 113, episode reward: 17.042, mean reward:  0.134 [-100.000, 17.130], mean action: 1.756 [0.000, 3.000],  loss: 19.934419, mse: 6834.422179, mean_q: 70.222960, mean_eps: 0.439120
  94544/150000: episode: 833, duration: 11.534s, episode steps: 1000, steps per second:  87, episode reward: -18.461, mean reward: -0.018 [-24.409, 18.351], mean action: 1.783 [0.000, 3.000],  loss: 16.036718, mse: 6751.714014, mean_q: 69.525943, mean_eps: 0.435739
  94624/150000: episode: 834, duration: 0.849s, episode steps:  80, steps per second:  94, episode reward: -23.836, mean reward: -0.298 [-100.000, 13.461], mean action: 1.812 [0.000, 3.000],  loss: 16.968975, mse: 6778.620142, mean_q: 70.035440, mean_eps: 0.432499
  94748/150000: episode: 835, duration: 1.503s, episode steps: 124, steps per second:  83, episode reward: -58.625, mean reward: -0.473 [-100.000,  7.250], mean action: 1.782 [0.000, 3.000],  loss: 16.497360, mse: 6682.451322, mean_q: 68.714469, mean_eps: 0.431887
  94858/150000: episode: 836, duration: 1.293s, episode steps: 110, steps per second:  85, episode reward: 17.766, mean reward:  0.162 [-100.000, 16.216], mean action: 1.655 [0.000, 3.000],  loss: 22.027163, mse: 6766.891113, mean_q: 68.978002, mean_eps: 0.431185
  94942/150000: episode: 837, duration: 1.417s, episode steps:  84, steps per second:  59, episode reward: -100.256, mean reward: -1.194 [-100.000, 10.155], mean action: 1.750 [0.000, 3.000],  loss: 15.362045, mse: 6846.766142, mean_q: 70.026471, mean_eps: 0.430603
  95084/150000: episode: 838, duration: 1.267s, episode steps: 142, steps per second: 112, episode reward: -44.156, mean reward: -0.311 [-100.000, 11.768], mean action: 1.768 [0.000, 3.000],  loss: 15.779069, mse: 6926.260825, mean_q: 71.149890, mean_eps: 0.429925
  95224/150000: episode: 839, duration: 1.179s, episode steps: 140, steps per second: 119, episode reward: 13.220, mean reward:  0.094 [-100.000, 19.683], mean action: 1.657 [0.000, 3.000],  loss: 14.752867, mse: 6817.227616, mean_q: 70.566613, mean_eps: 0.429079
  95333/150000: episode: 840, duration: 0.944s, episode steps: 109, steps per second: 115, episode reward: -49.790, mean reward: -0.457 [-100.000,  9.697], mean action: 1.697 [0.000, 3.000],  loss: 16.960792, mse: 6835.731392, mean_q: 70.578520, mean_eps: 0.428332
  95428/150000: episode: 841, duration: 0.869s, episode steps:  95, steps per second: 109, episode reward: -54.990, mean reward: -0.579 [-100.000,  9.451], mean action: 1.737 [0.000, 3.000],  loss: 18.394207, mse: 6806.981553, mean_q: 70.286384, mean_eps: 0.427720
  96428/150000: episode: 842, duration: 9.936s, episode steps: 1000, steps per second: 101, episode reward: -36.458, mean reward: -0.036 [-25.005, 18.520], mean action: 1.796 [0.000, 3.000],  loss: 15.973037, mse: 6909.148210, mean_q: 71.487566, mean_eps: 0.424435
  96546/150000: episode: 843, duration: 1.110s, episode steps: 118, steps per second: 106, episode reward: -14.036, mean reward: -0.119 [-100.000,  5.905], mean action: 1.797 [0.000, 3.000],  loss: 16.621967, mse: 6938.986730, mean_q: 72.837920, mean_eps: 0.421081
  96711/150000: episode: 844, duration: 1.294s, episode steps: 165, steps per second: 127, episode reward:  3.039, mean reward:  0.018 [-100.000,  8.667], mean action: 1.788 [0.000, 3.000],  loss: 18.717536, mse: 6913.936663, mean_q: 71.955673, mean_eps: 0.420232
  96855/150000: episode: 845, duration: 1.169s, episode steps: 144, steps per second: 123, episode reward: -48.470, mean reward: -0.337 [-100.000, 11.636], mean action: 1.764 [0.000, 3.000],  loss: 14.906844, mse: 7007.903056, mean_q: 73.693716, mean_eps: 0.419305
  96954/150000: episode: 846, duration: 0.778s, episode steps:  99, steps per second: 127, episode reward: -98.958, mean reward: -1.000 [-100.000, 12.411], mean action: 1.616 [0.000, 3.000],  loss: 16.425935, mse: 6923.736811, mean_q: 72.198146, mean_eps: 0.418576
  97954/150000: episode: 847, duration: 10.545s, episode steps: 1000, steps per second:  95, episode reward: -11.386, mean reward: -0.011 [-24.005, 23.354], mean action: 1.724 [0.000, 3.000],  loss: 18.197858, mse: 6980.487170, mean_q: 73.640263, mean_eps: 0.415279
  98129/150000: episode: 848, duration: 1.616s, episode steps: 175, steps per second: 108, episode reward: -14.992, mean reward: -0.086 [-100.000, 12.475], mean action: 1.743 [0.000, 3.000],  loss: 20.373361, mse: 7039.929277, mean_q: 74.039387, mean_eps: 0.411754
  98309/150000: episode: 849, duration: 1.481s, episode steps: 180, steps per second: 122, episode reward:  4.905, mean reward:  0.027 [-100.000,  9.572], mean action: 1.678 [0.000, 3.000],  loss: 19.340204, mse: 7166.166455, mean_q: 76.544665, mean_eps: 0.410689
  98433/150000: episode: 850, duration: 0.999s, episode steps: 124, steps per second: 124, episode reward:  9.404, mean reward:  0.076 [-100.000, 20.082], mean action: 1.726 [0.000, 3.000],  loss: 13.424530, mse: 7163.663444, mean_q: 76.248275, mean_eps: 0.409777
  99433/150000: episode: 851, duration: 10.575s, episode steps: 1000, steps per second:  95, episode reward: -51.735, mean reward: -0.052 [-23.080, 16.943], mean action: 1.707 [0.000, 3.000],  loss: 18.537180, mse: 7099.159906, mean_q: 76.274974, mean_eps: 0.406405
 100433/150000: episode: 852, duration: 10.096s, episode steps: 1000, steps per second:  99, episode reward: -42.235, mean reward: -0.042 [-22.343, 22.818], mean action: 1.674 [0.000, 3.000],  loss: 18.333022, mse: 7186.728323, mean_q: 77.944929, mean_eps: 0.400405
 101433/150000: episode: 853, duration: 9.459s, episode steps: 1000, steps per second: 106, episode reward: -120.435, mean reward: -0.120 [-23.262, 21.458], mean action: 1.790 [0.000, 3.000],  loss: 18.597478, mse: 7279.798135, mean_q: 79.239065, mean_eps: 0.394405
 102433/150000: episode: 854, duration: 10.801s, episode steps: 1000, steps per second:  93, episode reward: -7.699, mean reward: -0.008 [-23.359, 14.580], mean action: 1.771 [0.000, 3.000],  loss: 17.214430, mse: 7414.332326, mean_q: 80.507172, mean_eps: 0.388405
 103433/150000: episode: 855, duration: 9.737s, episode steps: 1000, steps per second: 103, episode reward: -26.661, mean reward: -0.027 [-17.060, 21.285], mean action: 1.790 [0.000, 3.000],  loss: 15.272658, mse: 7484.519207, mean_q: 80.914821, mean_eps: 0.382405
 104433/150000: episode: 856, duration: 11.774s, episode steps: 1000, steps per second:  85, episode reward: -25.344, mean reward: -0.025 [-24.848, 14.738], mean action: 1.762 [0.000, 3.000],  loss: 16.562314, mse: 7501.783415, mean_q: 81.337683, mean_eps: 0.376405
 105433/150000: episode: 857, duration: 9.988s, episode steps: 1000, steps per second: 100, episode reward: 57.500, mean reward:  0.058 [-24.267, 23.734], mean action: 1.485 [0.000, 3.000],  loss: 18.167265, mse: 7660.558805, mean_q: 83.294859, mean_eps: 0.370405
 106424/150000: episode: 858, duration: 14.178s, episode steps: 991, steps per second:  70, episode reward: -311.690, mean reward: -0.315 [-100.000, 27.302], mean action: 1.915 [0.000, 3.000],  loss: 16.489422, mse: 7868.948983, mean_q: 85.554717, mean_eps: 0.364432
 106612/150000: episode: 859, duration: 4.007s, episode steps: 188, steps per second:  47, episode reward: 57.778, mean reward:  0.307 [-100.000, 16.588], mean action: 1.851 [0.000, 3.000],  loss: 19.188979, mse: 8075.732858, mean_q: 87.066012, mean_eps: 0.360895
 107612/150000: episode: 860, duration: 12.084s, episode steps: 1000, steps per second:  83, episode reward: -46.243, mean reward: -0.046 [-23.279, 19.096], mean action: 1.777 [0.000, 3.000],  loss: 19.525233, mse: 7948.874643, mean_q: 86.415072, mean_eps: 0.357331
 108612/150000: episode: 861, duration: 10.426s, episode steps: 1000, steps per second:  96, episode reward: -9.847, mean reward: -0.010 [-23.471, 24.196], mean action: 1.599 [0.000, 3.000],  loss: 20.871773, mse: 8001.959021, mean_q: 87.136664, mean_eps: 0.351331
 109057/150000: episode: 862, duration: 5.566s, episode steps: 445, steps per second:  80, episode reward: -91.323, mean reward: -0.205 [-100.000,  3.861], mean action: 1.775 [0.000, 3.000],  loss: 18.593724, mse: 8213.307761, mean_q: 88.727334, mean_eps: 0.346996
 110057/150000: episode: 863, duration: 15.377s, episode steps: 1000, steps per second:  65, episode reward: 12.407, mean reward:  0.012 [-18.908, 17.661], mean action: 1.829 [0.000, 3.000],  loss: 18.548832, mse: 8258.155935, mean_q: 89.604745, mean_eps: 0.342661
 111057/150000: episode: 864, duration: 11.601s, episode steps: 1000, steps per second:  86, episode reward: 27.150, mean reward:  0.027 [-21.479, 23.896], mean action: 1.713 [0.000, 3.000],  loss: 18.451526, mse: 8366.604506, mean_q: 90.520879, mean_eps: 0.336661
 112057/150000: episode: 865, duration: 12.468s, episode steps: 1000, steps per second:  80, episode reward: 33.013, mean reward:  0.033 [-22.478, 25.813], mean action: 1.743 [0.000, 3.000],  loss: 22.810387, mse: 8436.262384, mean_q: 91.906946, mean_eps: 0.330661
 113057/150000: episode: 866, duration: 11.454s, episode steps: 1000, steps per second:  87, episode reward: -27.053, mean reward: -0.027 [-24.361, 23.301], mean action: 2.128 [0.000, 3.000],  loss: 19.247688, mse: 8414.391132, mean_q: 92.008913, mean_eps: 0.324661
 113784/150000: episode: 867, duration: 7.993s, episode steps: 727, steps per second:  91, episode reward: -263.538, mean reward: -0.363 [-100.000, 31.481], mean action: 1.816 [0.000, 3.000],  loss: 22.480717, mse: 8500.525925, mean_q: 92.811549, mean_eps: 0.319480
 114554/150000: episode: 868, duration: 8.068s, episode steps: 770, steps per second:  95, episode reward: -151.910, mean reward: -0.197 [-100.000, 32.614], mean action: 1.782 [0.000, 3.000],  loss: 21.457183, mse: 8435.729476, mean_q: 92.149938, mean_eps: 0.314989
 115554/150000: episode: 869, duration: 10.278s, episode steps: 1000, steps per second:  97, episode reward: -111.396, mean reward: -0.111 [-12.461, 10.475], mean action: 1.806 [0.000, 3.000],  loss: 22.726590, mse: 8594.213365, mean_q: 93.841257, mean_eps: 0.309679
 115693/150000: episode: 870, duration: 1.241s, episode steps: 139, steps per second: 112, episode reward:  7.453, mean reward:  0.054 [-100.000, 17.681], mean action: 1.827 [0.000, 3.000],  loss: 16.609853, mse: 8581.411319, mean_q: 94.314004, mean_eps: 0.306262
 116693/150000: episode: 871, duration: 10.574s, episode steps: 1000, steps per second:  95, episode reward: -18.791, mean reward: -0.019 [-24.229, 16.330], mean action: 1.740 [0.000, 3.000],  loss: 22.099492, mse: 8644.290454, mean_q: 95.010782, mean_eps: 0.302845
 117693/150000: episode: 872, duration: 9.702s, episode steps: 1000, steps per second: 103, episode reward:  2.130, mean reward:  0.002 [-22.628, 23.019], mean action: 2.185 [0.000, 3.000],  loss: 21.422456, mse: 8642.332710, mean_q: 95.297013, mean_eps: 0.296845
 118693/150000: episode: 873, duration: 8.783s, episode steps: 1000, steps per second: 114, episode reward: -32.258, mean reward: -0.032 [-11.325, 11.523], mean action: 1.770 [0.000, 3.000],  loss: 22.115409, mse: 8670.355451, mean_q: 95.469014, mean_eps: 0.290845
 119693/150000: episode: 874, duration: 10.773s, episode steps: 1000, steps per second:  93, episode reward: -21.605, mean reward: -0.022 [-4.567,  5.345], mean action: 1.799 [0.000, 3.000],  loss: 18.815407, mse: 8681.148282, mean_q: 96.277401, mean_eps: 0.284845
 120693/150000: episode: 875, duration: 8.976s, episode steps: 1000, steps per second: 111, episode reward: -23.900, mean reward: -0.024 [-3.561,  5.299], mean action: 1.684 [0.000, 3.000],  loss: 18.424534, mse: 8829.155646, mean_q: 97.865707, mean_eps: 0.278845
 121693/150000: episode: 876, duration: 9.567s, episode steps: 1000, steps per second: 105, episode reward: -42.890, mean reward: -0.043 [-4.699,  6.228], mean action: 1.694 [0.000, 3.000],  loss: 20.330047, mse: 8838.199528, mean_q: 98.667249, mean_eps: 0.272845
 121828/150000: episode: 877, duration: 1.123s, episode steps: 135, steps per second: 120, episode reward: -53.484, mean reward: -0.396 [-100.000, 15.152], mean action: 1.533 [0.000, 3.000],  loss: 21.037750, mse: 8805.224287, mean_q: 98.713802, mean_eps: 0.269440
 122828/150000: episode: 878, duration: 9.759s, episode steps: 1000, steps per second: 102, episode reward:  7.413, mean reward:  0.007 [-22.838, 21.260], mean action: 1.746 [0.000, 3.000],  loss: 22.179020, mse: 8854.460609, mean_q: 98.929291, mean_eps: 0.266035
 123828/150000: episode: 879, duration: 11.386s, episode steps: 1000, steps per second:  88, episode reward: -58.315, mean reward: -0.058 [-4.052,  5.493], mean action: 1.733 [0.000, 3.000],  loss: 20.993054, mse: 8966.965560, mean_q: 99.446318, mean_eps: 0.260035
 124828/150000: episode: 880, duration: 11.269s, episode steps: 1000, steps per second:  89, episode reward: -27.527, mean reward: -0.028 [-20.042, 12.251], mean action: 1.633 [0.000, 3.000],  loss: 19.941411, mse: 8872.064032, mean_q: 99.471762, mean_eps: 0.254035
 124932/150000: episode: 881, duration: 1.066s, episode steps: 104, steps per second:  98, episode reward: -57.017, mean reward: -0.548 [-100.000, 10.546], mean action: 1.683 [0.000, 3.000],  loss: 21.459095, mse: 8962.227412, mean_q: 100.202421, mean_eps: 0.250723
 125932/150000: episode: 882, duration: 10.341s, episode steps: 1000, steps per second:  97, episode reward: -0.635, mean reward: -0.001 [-18.642, 14.506], mean action: 1.707 [0.000, 3.000],  loss: 16.618662, mse: 9032.094503, mean_q: 101.091563, mean_eps: 0.247411
 126932/150000: episode: 883, duration: 11.174s, episode steps: 1000, steps per second:  89, episode reward: -11.854, mean reward: -0.012 [-18.201, 12.941], mean action: 1.607 [0.000, 3.000],  loss: 17.626598, mse: 9063.009578, mean_q: 101.505929, mean_eps: 0.241411
 127932/150000: episode: 884, duration: 12.353s, episode steps: 1000, steps per second:  81, episode reward: -19.099, mean reward: -0.019 [-17.867, 16.009], mean action: 1.754 [0.000, 3.000],  loss: 19.238565, mse: 9323.034313, mean_q: 103.555341, mean_eps: 0.235411
 128932/150000: episode: 885, duration: 9.600s, episode steps: 1000, steps per second: 104, episode reward: 14.176, mean reward:  0.014 [-5.391,  5.963], mean action: 1.760 [0.000, 3.000],  loss: 21.054247, mse: 9500.156311, mean_q: 104.866211, mean_eps: 0.229411
 129070/150000: episode: 886, duration: 1.411s, episode steps: 138, steps per second:  98, episode reward: -30.715, mean reward: -0.223 [-100.000, 15.915], mean action: 1.268 [0.000, 3.000],  loss: 14.225772, mse: 9485.679804, mean_q: 105.084684, mean_eps: 0.225997
 130070/150000: episode: 887, duration: 13.983s, episode steps: 1000, steps per second:  72, episode reward: -33.545, mean reward: -0.034 [-11.966, 12.158], mean action: 1.679 [0.000, 3.000],  loss: 17.901923, mse: 9450.836592, mean_q: 105.347328, mean_eps: 0.222583
 131070/150000: episode: 888, duration: 16.155s, episode steps: 1000, steps per second:  62, episode reward: -44.686, mean reward: -0.045 [-5.257,  5.328], mean action: 1.636 [0.000, 3.000],  loss: 14.198819, mse: 9536.775577, mean_q: 106.210680, mean_eps: 0.216583
 132070/150000: episode: 889, duration: 9.917s, episode steps: 1000, steps per second: 101, episode reward:  5.096, mean reward:  0.005 [-5.453,  5.329], mean action: 1.639 [0.000, 3.000],  loss: 15.892042, mse: 9590.524946, mean_q: 106.557336, mean_eps: 0.210583
 133070/150000: episode: 890, duration: 9.244s, episode steps: 1000, steps per second: 108, episode reward: -27.107, mean reward: -0.027 [-4.819,  5.350], mean action: 1.715 [0.000, 3.000],  loss: 17.505304, mse: 9368.642895, mean_q: 105.395098, mean_eps: 0.204583
 134070/150000: episode: 891, duration: 9.350s, episode steps: 1000, steps per second: 107, episode reward: -26.844, mean reward: -0.027 [-3.775,  5.108], mean action: 1.629 [0.000, 3.000],  loss: 15.644056, mse: 9450.906612, mean_q: 106.118489, mean_eps: 0.198583
 135070/150000: episode: 892, duration: 9.727s, episode steps: 1000, steps per second: 103, episode reward: 17.541, mean reward:  0.018 [-4.715,  5.135], mean action: 1.692 [0.000, 3.000],  loss: 16.211540, mse: 9555.374641, mean_q: 106.715280, mean_eps: 0.192583
 136070/150000: episode: 893, duration: 10.081s, episode steps: 1000, steps per second:  99, episode reward:  9.051, mean reward:  0.009 [-4.324,  5.366], mean action: 1.647 [0.000, 3.000],  loss: 16.531283, mse: 9643.916091, mean_q: 107.458746, mean_eps: 0.186583
 137070/150000: episode: 894, duration: 10.039s, episode steps: 1000, steps per second: 100, episode reward: -46.799, mean reward: -0.047 [-4.486,  5.007], mean action: 1.755 [0.000, 3.000],  loss: 13.559019, mse: 9502.964256, mean_q: 106.716860, mean_eps: 0.180583
 138070/150000: episode: 895, duration: 9.174s, episode steps: 1000, steps per second: 109, episode reward: -26.296, mean reward: -0.026 [-5.205,  5.609], mean action: 1.694 [0.000, 3.000],  loss: 12.725658, mse: 9517.818827, mean_q: 106.881499, mean_eps: 0.174583
 139070/150000: episode: 896, duration: 12.509s, episode steps: 1000, steps per second:  80, episode reward: -21.318, mean reward: -0.021 [-4.026,  5.584], mean action: 1.613 [0.000, 3.000],  loss: 11.475649, mse: 9467.227462, mean_q: 106.647145, mean_eps: 0.168583
 140070/150000: episode: 897, duration: 14.553s, episode steps: 1000, steps per second:  69, episode reward: 22.063, mean reward:  0.022 [-5.327,  5.826], mean action: 1.604 [0.000, 3.000],  loss: 12.161489, mse: 9428.510752, mean_q: 106.503708, mean_eps: 0.162583
 141070/150000: episode: 898, duration: 10.262s, episode steps: 1000, steps per second:  97, episode reward:  0.049, mean reward:  0.000 [-4.405,  5.012], mean action: 1.623 [0.000, 3.000],  loss: 10.909254, mse: 9404.473991, mean_q: 106.551424, mean_eps: 0.156583
 142070/150000: episode: 899, duration: 10.347s, episode steps: 1000, steps per second:  97, episode reward: -1.868, mean reward: -0.002 [-4.220,  4.717], mean action: 1.630 [0.000, 3.000],  loss: 10.256597, mse: 9436.629669, mean_q: 106.949221, mean_eps: 0.150583
 143070/150000: episode: 900, duration: 10.101s, episode steps: 1000, steps per second:  99, episode reward: 36.753, mean reward:  0.037 [-4.988,  6.482], mean action: 1.596 [0.000, 3.000],  loss: 9.633737, mse: 9408.908937, mean_q: 107.173514, mean_eps: 0.144583
 144070/150000: episode: 901, duration: 10.048s, episode steps: 1000, steps per second: 100, episode reward: 45.006, mean reward:  0.045 [-20.352, 14.815], mean action: 1.600 [0.000, 3.000],  loss: 7.973784, mse: 9295.263338, mean_q: 106.646986, mean_eps: 0.138583
 145070/150000: episode: 902, duration: 10.390s, episode steps: 1000, steps per second:  96, episode reward: 24.089, mean reward:  0.024 [-11.144, 11.695], mean action: 1.626 [0.000, 3.000],  loss: 9.278025, mse: 9396.104334, mean_q: 107.271891, mean_eps: 0.132583
 146070/150000: episode: 903, duration: 9.866s, episode steps: 1000, steps per second: 101, episode reward: 53.896, mean reward:  0.054 [-18.346, 21.755], mean action: 1.492 [0.000, 3.000],  loss: 8.572084, mse: 9410.862986, mean_q: 107.553591, mean_eps: 0.126583
 147070/150000: episode: 904, duration: 9.505s, episode steps: 1000, steps per second: 105, episode reward: 91.325, mean reward:  0.091 [-17.462, 13.793], mean action: 1.746 [0.000, 3.000],  loss: 8.268182, mse: 9341.226062, mean_q: 107.135963, mean_eps: 0.120583
 148070/150000: episode: 905, duration: 10.418s, episode steps: 1000, steps per second:  96, episode reward: -15.926, mean reward: -0.016 [-4.111,  4.792], mean action: 1.582 [0.000, 3.000],  loss: 8.259343, mse: 9319.074583, mean_q: 106.994918, mean_eps: 0.114583
 149070/150000: episode: 906, duration: 9.414s, episode steps: 1000, steps per second: 106, episode reward: 63.739, mean reward:  0.064 [-17.534, 12.056], mean action: 1.367 [0.000, 3.000],  loss: 9.332689, mse: 9237.064544, mean_q: 106.439255, mean_eps: 0.108583
 149785/150000: episode: 907, duration: 6.388s, episode steps: 715, steps per second: 112, episode reward: 162.656, mean reward:  0.227 [-17.113, 100.000], mean action: 1.439 [0.000, 3.000],  loss: 9.489178, mse: 9230.639947, mean_q: 106.339934, mean_eps: 0.103438
done, took 1489.905 seconds
Testing for 5 episodes ...
Episode 1: reward: -10.657, steps: 1000
Episode 2: reward: -54.249, steps: 1000
Episode 3: reward: 0.770, steps: 1000
Episode 4: reward: 2.706, steps: 1000
Episode 5: reward: 4.937, steps: 1000