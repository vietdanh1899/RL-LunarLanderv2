
4
Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 16)                144
_________________________________________________________________
activation (Activation)      (None, 16)                0
_________________________________________________________________
dense_1 (Dense)              (None, 32)                544
_________________________________________________________________
activation_1 (Activation)    (None, 32)                0
_________________________________________________________________
dense_2 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_2 (Activation)    (None, 4)                 0
=================================================================
Total params: 820
Trainable params: 820
Non-trainable params: 0
_________________________________________________________________
None
Training for 20000 steps ...
   120/20000: episode: 1, duration: 1.004s, episode steps: 120, steps per second: 120, episode reward: -49.619, mean reward: -0.413 [-100.000, 118.012], mean action: 1.533 [0.000, 3.000],  loss: 17.364729, mae: 0.814912, mean_q: 0.407262, mean_eps: 0.997075
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
   241/20000: episode: 2, duration: 0.698s, episode steps: 121, steps per second: 173, episode reward: -77.328, mean reward: -0.639 [-100.000, 19.408], mean action: 1.463 [0.000, 3.000],  loss: 74.912377, mae: 1.225024, mean_q: 0.586493, mean_eps: 0.991900
   313/20000: episode: 3, duration: 0.422s, episode steps:  72, steps per second: 171, episode reward: -68.012, mean reward: -0.945 [-100.000,  7.729], mean action: 1.694 [0.000, 3.000],  loss: 61.454553, mae: 1.262913, mean_q: 1.006663, mean_eps: 0.987557
   387/20000: episode: 4, duration: 0.397s, episode steps:  74, steps per second: 186, episode reward: -82.069, mean reward: -1.109 [-100.000, 22.298], mean action: 1.405 [0.000, 3.000],  loss: 67.000986, mae: 1.433757, mean_q: 1.444556, mean_eps: 0.984272
   486/20000: episode: 5, duration: 0.543s, episode steps:  99, steps per second: 182, episode reward: -76.327, mean reward: -0.771 [-100.000, 15.684], mean action: 1.545 [0.000, 3.000],  loss: 78.285740, mae: 1.760770, mean_q: 2.298849, mean_eps: 0.980380
   560/20000: episode: 6, duration: 0.392s, episode steps:  74, steps per second: 189, episode reward: -74.641, mean reward: -1.009 [-100.000, 12.002], mean action: 1.459 [0.000, 3.000],  loss: 88.840095, mae: 2.355618, mean_q: 3.567814, mean_eps: 0.976487
   653/20000: episode: 7, duration: 0.526s, episode steps:  93, steps per second: 177, episode reward: -117.866, mean reward: -1.267 [-100.000,  6.778], mean action: 1.290 [0.000, 3.000],  loss: 74.271714, mae: 3.146507, mean_q: 5.179649, mean_eps: 0.972730
   738/20000: episode: 8, duration: 0.531s, episode steps:  85, steps per second: 160, episode reward: -77.519, mean reward: -0.912 [-100.000, 12.631], mean action: 1.518 [0.000, 3.000],  loss: 73.837827, mae: 4.284546, mean_q: 7.063340, mean_eps: 0.968725
   850/20000: episode: 9, duration: 0.656s, episode steps: 112, steps per second: 171, episode reward: -178.243, mean reward: -1.591 [-100.000,  6.589], mean action: 1.723 [0.000, 3.000],  loss: 71.526325, mae: 5.662945, mean_q: 9.249889, mean_eps: 0.964292
   938/20000: episode: 10, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -111.183, mean reward: -1.263 [-100.000,  5.320], mean action: 1.511 [0.000, 3.000],  loss: 52.119921, mae: 7.044736, mean_q: 11.223369, mean_eps: 0.959792
  1037/20000: episode: 11, duration: 0.587s, episode steps:  99, steps per second: 169, episode reward: -352.939, mean reward: -3.565 [-100.000,  3.314], mean action: 1.333 [0.000, 3.000],  loss: 62.827148, mae: 8.142647, mean_q: 12.370736, mean_eps: 0.955585
  1114/20000: episode: 12, duration: 0.456s, episode steps:  77, steps per second: 169, episode reward: -135.813, mean reward: -1.764 [-100.000,  7.751], mean action: 1.351 [0.000, 3.000],  loss: 53.927705, mae: 8.568588, mean_q: 12.777001, mean_eps: 0.951625
  1238/20000: episode: 13, duration: 0.721s, episode steps: 124, steps per second: 172, episode reward: -88.574, mean reward: -0.714 [-100.000,  7.376], mean action: 1.339 [0.000, 3.000],  loss: 58.393201, mae: 9.980727, mean_q: 14.235186, mean_eps: 0.947102
  1309/20000: episode: 14, duration: 0.473s, episode steps:  71, steps per second: 150, episode reward: -92.869, mean reward: -1.308 [-100.000, 22.647], mean action: 1.437 [0.000, 3.000],  loss: 44.785580, mae: 10.686816, mean_q: 14.978679, mean_eps: 0.942715
  1379/20000: episode: 15, duration: 0.453s, episode steps:  70, steps per second: 155, episode reward: -72.256, mean reward: -1.032 [-100.000,  8.557], mean action: 1.557 [0.000, 3.000],  loss: 39.634876, mae: 11.712517, mean_q: 16.428754, mean_eps: 0.939543
  1480/20000: episode: 16, duration: 0.658s, episode steps: 101, steps per second: 153, episode reward: -97.201, mean reward: -0.962 [-100.000, 16.873], mean action: 1.386 [0.000, 3.000],  loss: 40.699219, mae: 12.252892, mean_q: 16.643156, mean_eps: 0.935695
  1584/20000: episode: 17, duration: 0.748s, episode steps: 104, steps per second: 139, episode reward: -67.517, mean reward: -0.649 [-100.000, 49.998], mean action: 1.577 [0.000, 3.000],  loss: 58.338381, mae: 13.163213, mean_q: 17.550265, mean_eps: 0.931083
  1703/20000: episode: 18, duration: 0.751s, episode steps: 119, steps per second: 158, episode reward: -135.874, mean reward: -1.142 [-100.000,  7.071], mean action: 1.513 [0.000, 3.000],  loss: 34.788639, mae: 13.370707, mean_q: 17.603833, mean_eps: 0.926065
  1830/20000: episode: 19, duration: 0.776s, episode steps: 127, steps per second: 164, episode reward: -183.500, mean reward: -1.445 [-100.000,  4.946], mean action: 1.441 [0.000, 3.000],  loss: 36.561419, mae: 13.732557, mean_q: 17.785789, mean_eps: 0.920530
  1953/20000: episode: 20, duration: 0.700s, episode steps: 123, steps per second: 176, episode reward: -202.286, mean reward: -1.645 [-100.000, 15.745], mean action: 1.732 [0.000, 3.000],  loss: 32.000723, mae: 14.229936, mean_q: 18.512939, mean_eps: 0.914905
  2032/20000: episode: 21, duration: 0.612s, episode steps:  79, steps per second: 129, episode reward: -84.601, mean reward: -1.071 [-100.000, 11.064], mean action: 1.633 [0.000, 3.000],  loss: 45.476566, mae: 15.007054, mean_q: 18.581105, mean_eps: 0.910360
  2152/20000: episode: 22, duration: 0.791s, episode steps: 120, steps per second: 152, episode reward: -155.112, mean reward: -1.293 [-100.000,  5.821], mean action: 1.600 [0.000, 3.000],  loss: 27.417366, mae: 15.676567, mean_q: 19.603479, mean_eps: 0.905883
  2275/20000: episode: 23, duration: 0.670s, episode steps: 123, steps per second: 184, episode reward: -98.092, mean reward: -0.797 [-100.000,  9.561], mean action: 1.512 [0.000, 3.000],  loss: 33.665024, mae: 15.929327, mean_q: 19.868520, mean_eps: 0.900415
  2373/20000: episode: 24, duration: 0.563s, episode steps:  98, steps per second: 174, episode reward: -295.508, mean reward: -3.015 [-100.000,  0.639], mean action: 1.459 [0.000, 3.000],  loss: 23.519596, mae: 16.450470, mean_q: 20.383395, mean_eps: 0.895442
  2484/20000: episode: 25, duration: 0.734s, episode steps: 111, steps per second: 151, episode reward: -76.380, mean reward: -0.688 [-100.000, 16.637], mean action: 1.640 [0.000, 3.000],  loss: 27.475260, mae: 16.881088, mean_q: 20.460656, mean_eps: 0.890740
  2580/20000: episode: 26, duration: 0.701s, episode steps:  96, steps per second: 137, episode reward: -96.345, mean reward: -1.004 [-100.000, 12.226], mean action: 1.604 [0.000, 3.000],  loss: 22.765126, mae: 17.822270, mean_q: 20.781489, mean_eps: 0.886082
  2701/20000: episode: 27, duration: 0.829s, episode steps: 121, steps per second: 146, episode reward: -161.061, mean reward: -1.331 [-100.000, 10.744], mean action: 1.521 [0.000, 3.000],  loss: 26.337664, mae: 18.661035, mean_q: 20.840871, mean_eps: 0.881200
  2795/20000: episode: 28, duration: 0.698s, episode steps:  94, steps per second: 135, episode reward: -404.367, mean reward: -4.302 [-100.000,  2.483], mean action: 1.851 [0.000, 3.000],  loss: 23.997572, mae: 19.313662, mean_q: 20.917659, mean_eps: 0.876362
  2904/20000: episode: 29, duration: 0.894s, episode steps: 109, steps per second: 122, episode reward: -239.131, mean reward: -2.194 [-100.000, 13.936], mean action: 1.523 [0.000, 3.000],  loss: 22.768915, mae: 19.536307, mean_q: 21.543479, mean_eps: 0.871795
  3028/20000: episode: 30, duration: 0.939s, episode steps: 124, steps per second: 132, episode reward: -274.086, mean reward: -2.210 [-100.000,  3.975], mean action: 1.790 [0.000, 3.000],  loss: 15.198980, mae: 19.927533, mean_q: 21.850557, mean_eps: 0.866552
  3124/20000: episode: 31, duration: 0.662s, episode steps:  96, steps per second: 145, episode reward: -145.054, mean reward: -1.511 [-100.000, 15.017], mean action: 1.583 [0.000, 3.000],  loss: 21.507181, mae: 20.914957, mean_q: 21.974487, mean_eps: 0.861603
  3205/20000: episode: 32, duration: 0.511s, episode steps:  81, steps per second: 159, episode reward: -77.078, mean reward: -0.952 [-100.000, 14.151], mean action: 1.642 [0.000, 3.000],  loss: 13.212694, mae: 20.666998, mean_q: 21.551389, mean_eps: 0.857620
  3327/20000: episode: 33, duration: 0.816s, episode steps: 122, steps per second: 149, episode reward: -101.653, mean reward: -0.833 [-100.000, 17.770], mean action: 1.582 [0.000, 3.000],  loss: 18.130483, mae: 21.249751, mean_q: 20.994784, mean_eps: 0.853052
  3465/20000: episode: 34, duration: 0.899s, episode steps: 138, steps per second: 154, episode reward: -296.722, mean reward: -2.150 [-100.000, 89.548], mean action: 1.529 [0.000, 3.000],  loss: 13.595624, mae: 21.457444, mean_q: 22.102442, mean_eps: 0.847202
  3540/20000: episode: 35, duration: 0.464s, episode steps:  75, steps per second: 162, episode reward: -58.136, mean reward: -0.775 [-100.000, 16.908], mean action: 1.600 [0.000, 3.000],  loss: 17.894524, mae: 21.960793, mean_q: 21.219889, mean_eps: 0.842410
  3625/20000: episode: 36, duration: 0.545s, episode steps:  85, steps per second: 156, episode reward: -207.313, mean reward: -2.439 [-100.000,  3.476], mean action: 1.729 [0.000, 3.000],  loss: 17.958940, mae: 22.587825, mean_q: 21.006146, mean_eps: 0.838810
  3718/20000: episode: 37, duration: 0.612s, episode steps:  93, steps per second: 152, episode reward: -113.230, mean reward: -1.218 [-100.000, 31.030], mean action: 1.548 [0.000, 3.000],  loss: 19.064168, mae: 23.406013, mean_q: 20.238650, mean_eps: 0.834805
  3817/20000: episode: 38, duration: 0.653s, episode steps:  99, steps per second: 152, episode reward: -161.400, mean reward: -1.630 [-100.000,  7.549], mean action: 1.707 [0.000, 3.000],  loss: 16.012518, mae: 23.703548, mean_q: 20.197772, mean_eps: 0.830485
  3940/20000: episode: 39, duration: 0.885s, episode steps: 123, steps per second: 139, episode reward: -126.998, mean reward: -1.033 [-100.000, 17.442], mean action: 1.520 [0.000, 3.000],  loss: 14.492393, mae: 24.176363, mean_q: 18.748580, mean_eps: 0.825490
  4014/20000: episode: 40, duration: 0.501s, episode steps:  74, steps per second: 148, episode reward: -98.639, mean reward: -1.333 [-100.000,  7.899], mean action: 1.554 [0.000, 3.000],  loss: 19.443314, mae: 24.669960, mean_q: 18.884667, mean_eps: 0.821057
  4169/20000: episode: 41, duration: 1.047s, episode steps: 155, steps per second: 148, episode reward: -198.476, mean reward: -1.280 [-100.000, 18.581], mean action: 1.574 [0.000, 3.000],  loss: 13.998765, mae: 25.482628, mean_q: 19.892673, mean_eps: 0.815905
  4274/20000: episode: 42, duration: 0.629s, episode steps: 105, steps per second: 167, episode reward: -44.342, mean reward: -0.422 [-100.000, 21.843], mean action: 1.590 [0.000, 3.000],  loss: 9.870616, mae: 26.177751, mean_q: 19.113045, mean_eps: 0.810055
  4375/20000: episode: 43, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: -321.509, mean reward: -3.183 [-100.000,  0.137], mean action: 1.703 [0.000, 3.000],  loss: 8.016373, mae: 26.143373, mean_q: 17.594345, mean_eps: 0.805420
  4462/20000: episode: 44, duration: 0.558s, episode steps:  87, steps per second: 156, episode reward: -454.317, mean reward: -5.222 [-100.000,  1.148], mean action: 1.644 [0.000, 3.000],  loss: 9.479511, mae: 26.695822, mean_q: 16.739328, mean_eps: 0.801190
  4598/20000: episode: 45, duration: 0.879s, episode steps: 136, steps per second: 155, episode reward: -342.504, mean reward: -2.518 [-100.000,  5.006], mean action: 1.529 [0.000, 3.000],  loss: 9.270718, mae: 27.376921, mean_q: 15.087954, mean_eps: 0.796172
  4723/20000: episode: 46, duration: 0.889s, episode steps: 125, steps per second: 141, episode reward: -325.046, mean reward: -2.600 [-100.000,  1.455], mean action: 1.512 [0.000, 3.000],  loss: 14.997545, mae: 27.692989, mean_q: 15.958449, mean_eps: 0.790300
  4797/20000: episode: 47, duration: 0.457s, episode steps:  74, steps per second: 162, episode reward: -162.199, mean reward: -2.192 [-100.000, 10.482], mean action: 1.730 [0.000, 3.000],  loss: 16.398230, mae: 28.902542, mean_q: 13.494771, mean_eps: 0.785822
  4930/20000: episode: 48, duration: 0.813s, episode steps: 133, steps per second: 164, episode reward: -210.898, mean reward: -1.586 [-100.000, 16.885], mean action: 1.617 [0.000, 3.000],  loss: 13.455671, mae: 28.184306, mean_q: 11.834618, mean_eps: 0.781165
  5018/20000: episode: 49, duration: 0.536s, episode steps:  88, steps per second: 164, episode reward: -239.826, mean reward: -2.725 [-100.000,  5.181], mean action: 1.636 [0.000, 3.000],  loss: 7.529692, mae: 28.091596, mean_q: 11.754660, mean_eps: 0.776192
  5118/20000: episode: 50, duration: 0.613s, episode steps: 100, steps per second: 163, episode reward: -79.989, mean reward: -0.800 [-100.000, 14.837], mean action: 1.610 [0.000, 3.000],  loss: 10.020327, mae: 28.583084, mean_q: 11.016948, mean_eps: 0.771962
  5220/20000: episode: 51, duration: 0.564s, episode steps: 102, steps per second: 181, episode reward: -202.684, mean reward: -1.987 [-100.000,  7.682], mean action: 1.520 [0.000, 3.000],  loss: 11.547670, mae: 28.755986, mean_q: 9.605884, mean_eps: 0.767418
  5318/20000: episode: 52, duration: 0.577s, episode steps:  98, steps per second: 170, episode reward: -389.199, mean reward: -3.971 [-100.000,  4.899], mean action: 1.429 [0.000, 3.000],  loss: 9.370868, mae: 29.103082, mean_q: 9.940476, mean_eps: 0.762917
  5429/20000: episode: 53, duration: 0.629s, episode steps: 111, steps per second: 177, episode reward: -73.782, mean reward: -0.665 [-100.000, 11.343], mean action: 1.649 [0.000, 3.000],  loss: 11.519093, mae: 30.161079, mean_q: 9.161529, mean_eps: 0.758215
  5513/20000: episode: 54, duration: 0.486s, episode steps:  84, steps per second: 173, episode reward: -200.606, mean reward: -2.388 [-100.000, 11.665], mean action: 1.631 [0.000, 3.000],  loss: 12.916604, mae: 30.633177, mean_q: 8.216530, mean_eps: 0.753827
  5619/20000: episode: 55, duration: 0.598s, episode steps: 106, steps per second: 177, episode reward: -256.038, mean reward: -2.415 [-100.000,  1.073], mean action: 1.557 [0.000, 3.000],  loss: 11.886322, mae: 30.662065, mean_q: 7.246385, mean_eps: 0.749553
  5744/20000: episode: 56, duration: 0.681s, episode steps: 125, steps per second: 184, episode reward: -79.159, mean reward: -0.633 [-100.000, 11.482], mean action: 1.744 [0.000, 3.000],  loss: 18.928153, mae: 30.999261, mean_q: 6.331114, mean_eps: 0.744355
  5836/20000: episode: 57, duration: 0.509s, episode steps:  92, steps per second: 181, episode reward: -13.415, mean reward: -0.146 [-100.000, 11.547], mean action: 1.696 [0.000, 3.000],  loss: 13.715638, mae: 30.555376, mean_q: 8.048868, mean_eps: 0.739472
  5952/20000: episode: 58, duration: 0.684s, episode steps: 116, steps per second: 170, episode reward: -75.991, mean reward: -0.655 [-100.000,  9.373], mean action: 1.664 [0.000, 3.000],  loss: 6.560078, mae: 30.501756, mean_q: 7.665735, mean_eps: 0.734792
  6084/20000: episode: 59, duration: 0.760s, episode steps: 132, steps per second: 174, episode reward: -192.620, mean reward: -1.459 [-100.000, 26.391], mean action: 1.674 [0.000, 3.000],  loss: 16.529780, mae: 31.162318, mean_q: 6.930816, mean_eps: 0.729212
  6198/20000: episode: 60, duration: 0.632s, episode steps: 114, steps per second: 180, episode reward: -90.705, mean reward: -0.796 [-100.000,  6.619], mean action: 1.596 [0.000, 3.000],  loss: 16.499245, mae: 31.356843, mean_q: 6.350276, mean_eps: 0.723677
  6337/20000: episode: 61, duration: 0.779s, episode steps: 139, steps per second: 179, episode reward: -344.818, mean reward: -2.481 [-100.000, 86.460], mean action: 1.568 [0.000, 3.000],  loss: 9.797690, mae: 32.008330, mean_q: 3.955123, mean_eps: 0.717985
  6443/20000: episode: 62, duration: 0.573s, episode steps: 106, steps per second: 185, episode reward: -232.167, mean reward: -2.190 [-100.000,  9.256], mean action: 1.689 [0.000, 3.000],  loss: 11.423785, mae: 32.286640, mean_q: 6.396581, mean_eps: 0.712473
  6547/20000: episode: 63, duration: 0.594s, episode steps: 104, steps per second: 175, episode reward: -313.544, mean reward: -3.015 [-100.000, 120.383], mean action: 1.740 [0.000, 3.000],  loss: 8.164385, mae: 32.792431, mean_q: 4.142000, mean_eps: 0.707747
  6636/20000: episode: 64, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -72.031, mean reward: -0.809 [-100.000, 15.877], mean action: 1.596 [0.000, 3.000],  loss: 14.002104, mae: 33.249880, mean_q: 2.154156, mean_eps: 0.703405
  6808/20000: episode: 65, duration: 1.162s, episode steps: 172, steps per second: 148, episode reward: -35.904, mean reward: -0.209 [-100.000, 12.178], mean action: 1.500 [0.000, 3.000],  loss: 13.895627, mae: 32.907468, mean_q: 2.950914, mean_eps: 0.697532
  6978/20000: episode: 66, duration: 1.011s, episode steps: 170, steps per second: 168, episode reward: -190.638, mean reward: -1.121 [-100.000,  6.433], mean action: 1.706 [0.000, 3.000],  loss: 18.076949, mae: 33.701888, mean_q: 2.147065, mean_eps: 0.689837
  7064/20000: episode: 67, duration: 0.514s, episode steps:  86, steps per second: 167, episode reward: -87.470, mean reward: -1.017 [-100.000, 11.623], mean action: 1.605 [0.000, 3.000],  loss: 9.437552, mae: 33.735056, mean_q: 1.924988, mean_eps: 0.684077
  7131/20000: episode: 68, duration: 0.385s, episode steps:  67, steps per second: 174, episode reward: -239.383, mean reward: -3.573 [-100.000,  5.943], mean action: 1.627 [0.000, 3.000],  loss: 13.587210, mae: 33.783512, mean_q: 2.512092, mean_eps: 0.680635
  7226/20000: episode: 69, duration: 0.555s, episode steps:  95, steps per second: 171, episode reward: -95.745, mean reward: -1.008 [-100.000,  6.377], mean action: 1.716 [0.000, 3.000],  loss: 11.198849, mae: 34.145604, mean_q: -1.194461, mean_eps: 0.676990
  7367/20000: episode: 70, duration: 0.897s, episode steps: 141, steps per second: 157, episode reward: -16.753, mean reward: -0.119 [-100.000, 21.192], mean action: 1.787 [0.000, 3.000],  loss: 10.124963, mae: 33.517218, mean_q: 0.862751, mean_eps: 0.671680
  7521/20000: episode: 71, duration: 1.007s, episode steps: 154, steps per second: 153, episode reward: -6.887, mean reward: -0.045 [-100.000, 18.431], mean action: 1.584 [0.000, 3.000],  loss: 20.816497, mae: 34.893159, mean_q: -1.171863, mean_eps: 0.665042
  7744/20000: episode: 72, duration: 1.325s, episode steps: 223, steps per second: 168, episode reward: -90.408, mean reward: -0.405 [-100.000, 10.491], mean action: 1.646 [0.000, 3.000],  loss: 17.332333, mae: 34.588001, mean_q: -0.906487, mean_eps: 0.656560
  7849/20000: episode: 73, duration: 0.665s, episode steps: 105, steps per second: 158, episode reward: -91.454, mean reward: -0.871 [-100.000, 13.438], mean action: 1.695 [0.000, 3.000],  loss: 12.167773, mae: 34.606511, mean_q: 0.543235, mean_eps: 0.649180
  7954/20000: episode: 74, duration: 0.741s, episode steps: 105, steps per second: 142, episode reward: -44.265, mean reward: -0.422 [-100.000,  8.295], mean action: 1.810 [0.000, 3.000],  loss: 10.234631, mae: 34.862728, mean_q: -0.812709, mean_eps: 0.644455
  8056/20000: episode: 75, duration: 0.631s, episode steps: 102, steps per second: 162, episode reward: -186.872, mean reward: -1.832 [-100.000,  6.742], mean action: 1.755 [0.000, 3.000],  loss: 10.866267, mae: 34.774283, mean_q: -0.557929, mean_eps: 0.639797
  8175/20000: episode: 76, duration: 0.670s, episode steps: 119, steps per second: 178, episode reward: 15.790, mean reward:  0.133 [-100.000, 91.481], mean action: 1.748 [0.000, 3.000],  loss: 17.451315, mae: 33.967729, mean_q: -1.194162, mean_eps: 0.634825
  8328/20000: episode: 77, duration: 0.963s, episode steps: 153, steps per second: 159, episode reward: -404.319, mean reward: -2.643 [-100.000, 28.633], mean action: 1.673 [0.000, 3.000],  loss: 15.773686, mae: 34.815173, mean_q: -1.036089, mean_eps: 0.628705
  8441/20000: episode: 78, duration: 0.692s, episode steps: 113, steps per second: 163, episode reward: -127.408, mean reward: -1.128 [-100.000,  6.918], mean action: 1.513 [0.000, 3.000],  loss: 12.885585, mae: 35.425154, mean_q: -1.891483, mean_eps: 0.622720
  8556/20000: episode: 79, duration: 0.673s, episode steps: 115, steps per second: 171, episode reward: 22.765, mean reward:  0.198 [-100.000, 78.570], mean action: 1.739 [0.000, 3.000],  loss: 13.094706, mae: 35.394607, mean_q: -3.456193, mean_eps: 0.617590
  8644/20000: episode: 80, duration: 0.514s, episode steps:  88, steps per second: 171, episode reward: -45.365, mean reward: -0.516 [-100.000, 91.744], mean action: 1.705 [0.000, 3.000],  loss: 10.114146, mae: 36.188664, mean_q: -2.728112, mean_eps: 0.613023
  8756/20000: episode: 81, duration: 0.715s, episode steps: 112, steps per second: 157, episode reward: -108.524, mean reward: -0.969 [-100.000, 11.137], mean action: 1.723 [0.000, 3.000],  loss: 21.884021, mae: 35.554651, mean_q: -1.116480, mean_eps: 0.608522
  8915/20000: episode: 82, duration: 1.062s, episode steps: 159, steps per second: 150, episode reward: 18.934, mean reward:  0.119 [-100.000, 78.280], mean action: 1.686 [0.000, 3.000],  loss: 14.368459, mae: 35.982887, mean_q: -1.314407, mean_eps: 0.602425
  9008/20000: episode: 83, duration: 0.619s, episode steps:  93, steps per second: 150, episode reward: -147.250, mean reward: -1.583 [-100.000,  5.327], mean action: 1.785 [0.000, 3.000],  loss: 14.850733, mae: 35.770004, mean_q: -2.656550, mean_eps: 0.596755
  9147/20000: episode: 84, duration: 1.030s, episode steps: 139, steps per second: 135, episode reward: -90.783, mean reward: -0.653 [-100.000,  8.186], mean action: 1.734 [0.000, 3.000],  loss: 12.081535, mae: 35.742571, mean_q: -1.873188, mean_eps: 0.591535
  9288/20000: episode: 85, duration: 0.946s, episode steps: 141, steps per second: 149, episode reward: -4.935, mean reward: -0.035 [-100.000, 14.832], mean action: 1.801 [0.000, 3.000],  loss: 13.925960, mae: 35.693008, mean_q: -1.381735, mean_eps: 0.585235
  9386/20000: episode: 86, duration: 0.600s, episode steps:  98, steps per second: 163, episode reward: -131.991, mean reward: -1.347 [-100.000, 11.847], mean action: 1.806 [0.000, 3.000],  loss: 11.614305, mae: 35.794931, mean_q: -1.109359, mean_eps: 0.579857
  9489/20000: episode: 87, duration: 0.650s, episode steps: 103, steps per second: 158, episode reward: -68.959, mean reward: -0.670 [-100.000, 11.093], mean action: 1.738 [0.000, 3.000],  loss: 11.530508, mae: 35.959851, mean_q: -2.529775, mean_eps: 0.575335
  9613/20000: episode: 88, duration: 0.808s, episode steps: 124, steps per second: 153, episode reward: -15.162, mean reward: -0.122 [-100.000, 16.252], mean action: 1.734 [0.000, 3.000],  loss: 15.361331, mae: 36.188094, mean_q: -3.820073, mean_eps: 0.570227
  9749/20000: episode: 89, duration: 0.997s, episode steps: 136, steps per second: 136, episode reward: -1.776, mean reward: -0.013 [-100.000,  6.585], mean action: 1.588 [0.000, 3.000],  loss: 17.933301, mae: 35.355007, mean_q: -2.454997, mean_eps: 0.564377
  9887/20000: episode: 90, duration: 0.860s, episode steps: 138, steps per second: 161, episode reward: -27.439, mean reward: -0.199 [-100.000, 14.423], mean action: 1.667 [0.000, 3.000],  loss: 15.854370, mae: 34.977586, mean_q: -3.146225, mean_eps: 0.558212
 10096/20000: episode: 91, duration: 1.463s, episode steps: 209, steps per second: 143, episode reward: -60.360, mean reward: -0.289 [-100.000, 11.169], mean action: 1.670 [0.000, 3.000],  loss: 12.874159, mae: 34.440216, mean_q: -3.616636, mean_eps: 0.550405
 10353/20000: episode: 92, duration: 1.799s, episode steps: 257, steps per second: 143, episode reward: -38.180, mean reward: -0.149 [-100.000, 10.803], mean action: 1.778 [0.000, 3.000],  loss: 10.853386, mae: 34.977975, mean_q: -3.765467, mean_eps: 0.539920
 10542/20000: episode: 93, duration: 1.209s, episode steps: 189, steps per second: 156, episode reward: -20.468, mean reward: -0.108 [-100.000, 13.620], mean action: 1.884 [0.000, 3.000],  loss: 12.419217, mae: 34.849008, mean_q: -3.720757, mean_eps: 0.529885
 10656/20000: episode: 94, duration: 0.753s, episode steps: 114, steps per second: 151, episode reward: -103.340, mean reward: -0.906 [-100.000,  8.538], mean action: 1.737 [0.000, 3.000],  loss: 16.888318, mae: 34.378638, mean_q: -3.525174, mean_eps: 0.523068
 10746/20000: episode: 95, duration: 0.595s, episode steps:  90, steps per second: 151, episode reward: -86.662, mean reward: -0.963 [-100.000, 10.485], mean action: 1.811 [0.000, 3.000],  loss: 18.040347, mae: 34.471437, mean_q: -3.825451, mean_eps: 0.518478
 10852/20000: episode: 96, duration: 0.675s, episode steps: 106, steps per second: 157, episode reward: -113.324, mean reward: -1.069 [-100.000,  7.999], mean action: 1.849 [0.000, 3.000],  loss: 18.742377, mae: 34.022361, mean_q: -2.621961, mean_eps: 0.514067
 10970/20000: episode: 97, duration: 0.798s, episode steps: 118, steps per second: 148, episode reward: -84.991, mean reward: -0.720 [-100.000, 13.467], mean action: 1.881 [0.000, 3.000],  loss: 13.821468, mae: 33.566675, mean_q: -2.135516, mean_eps: 0.509027
 11199/20000: episode: 98, duration: 1.644s, episode steps: 229, steps per second: 139, episode reward: -80.118, mean reward: -0.350 [-100.000,  9.292], mean action: 1.620 [0.000, 3.000],  loss: 17.793925, mae: 33.389573, mean_q: -1.025145, mean_eps: 0.501220
 11375/20000: episode: 99, duration: 1.376s, episode steps: 176, steps per second: 128, episode reward: -85.251, mean reward: -0.484 [-100.000, 11.557], mean action: 1.693 [0.000, 3.000],  loss: 14.650763, mae: 33.179747, mean_q: -0.282877, mean_eps: 0.492107
 11577/20000: episode: 100, duration: 1.359s, episode steps: 202, steps per second: 149, episode reward: -96.664, mean reward: -0.479 [-100.000, 23.475], mean action: 1.688 [0.000, 3.000],  loss: 22.163185, mae: 32.876703, mean_q: 0.447062, mean_eps: 0.483602
 11716/20000: episode: 101, duration: 0.905s, episode steps: 139, steps per second: 154, episode reward: -195.180, mean reward: -1.404 [-100.000,  6.962], mean action: 1.806 [0.000, 3.000],  loss: 13.246776, mae: 31.942070, mean_q: 4.475345, mean_eps: 0.475930
 11895/20000: episode: 102, duration: 1.177s, episode steps: 179, steps per second: 152, episode reward: -69.017, mean reward: -0.386 [-100.000,  8.681], mean action: 1.536 [0.000, 3.000],  loss: 12.993375, mae: 31.855267, mean_q: 3.931836, mean_eps: 0.468775
 12133/20000: episode: 103, duration: 1.519s, episode steps: 238, steps per second: 157, episode reward: 15.540, mean reward:  0.065 [-100.000, 12.823], mean action: 1.752 [0.000, 3.000],  loss: 13.750473, mae: 31.924112, mean_q: 4.879899, mean_eps: 0.459392
 12325/20000: episode: 104, duration: 1.318s, episode steps: 192, steps per second: 146, episode reward: -73.176, mean reward: -0.381 [-100.000,  9.934], mean action: 1.661 [0.000, 3.000],  loss: 11.627617, mae: 32.171546, mean_q: 4.260430, mean_eps: 0.449717
 12669/20000: episode: 105, duration: 2.296s, episode steps: 344, steps per second: 150, episode reward: -144.600, mean reward: -0.420 [-100.000,  8.060], mean action: 1.849 [0.000, 3.000],  loss: 10.124760, mae: 32.384702, mean_q: 6.071084, mean_eps: 0.437657
 13116/20000: episode: 106, duration: 2.990s, episode steps: 447, steps per second: 150, episode reward: -154.150, mean reward: -0.345 [-100.000, 19.482], mean action: 1.729 [0.000, 3.000],  loss: 13.270329, mae: 32.515065, mean_q: 7.809011, mean_eps: 0.419860
 13239/20000: episode: 107, duration: 0.768s, episode steps: 123, steps per second: 160, episode reward: -6.734, mean reward: -0.055 [-100.000, 17.816], mean action: 1.943 [0.000, 3.000],  loss: 8.726813, mae: 32.590250, mean_q: 6.691292, mean_eps: 0.407035
 13411/20000: episode: 108, duration: 1.067s, episode steps: 172, steps per second: 161, episode reward: -155.932, mean reward: -0.907 [-100.000,  6.071], mean action: 1.843 [0.000, 3.000],  loss: 11.356208, mae: 32.222813, mean_q: 8.373384, mean_eps: 0.400398
 13579/20000: episode: 109, duration: 1.038s, episode steps: 168, steps per second: 162, episode reward: -292.321, mean reward: -1.740 [-100.000, 88.195], mean action: 1.792 [0.000, 3.000],  loss: 12.200936, mae: 32.641927, mean_q: 7.152539, mean_eps: 0.392747
 13779/20000: episode: 110, duration: 1.245s, episode steps: 200, steps per second: 161, episode reward: -145.031, mean reward: -0.725 [-100.000, 11.691], mean action: 1.630 [0.000, 3.000],  loss: 12.948067, mae: 33.163504, mean_q: 8.601877, mean_eps: 0.384467
 14163/20000: episode: 111, duration: 2.758s, episode steps: 384, steps per second: 139, episode reward: -223.063, mean reward: -0.581 [-100.000,  5.798], mean action: 1.711 [0.000, 3.000],  loss: 12.375486, mae: 33.409629, mean_q: 9.336902, mean_eps: 0.371327
 14558/20000: episode: 112, duration: 2.978s, episode steps: 395, steps per second: 133, episode reward: -104.898, mean reward: -0.266 [-100.000,  6.004], mean action: 1.810 [0.000, 3.000],  loss: 13.057159, mae: 33.615123, mean_q: 10.217340, mean_eps: 0.353800
 15558/20000: episode: 113, duration: 11.862s, episode steps: 1000, steps per second:  84, episode reward: -81.843, mean reward: -0.082 [-5.132,  5.017], mean action: 1.724 [0.000, 3.000],  loss: 11.365851, mae: 33.660380, mean_q: 12.229410, mean_eps: 0.322412
 15789/20000: episode: 114, duration: 2.089s, episode steps: 231, steps per second: 111, episode reward: -170.189, mean reward: -0.737 [-100.000,  3.249], mean action: 1.779 [0.000, 3.000],  loss: 9.421967, mae: 33.645554, mean_q: 12.589803, mean_eps: 0.294715
 16789/20000: episode: 115, duration: 10.859s, episode steps: 1000, steps per second:  92, episode reward: -148.540, mean reward: -0.149 [-5.119,  4.006], mean action: 1.638 [0.000, 3.000],  loss: 12.189481, mae: 33.639380, mean_q: 14.265492, mean_eps: 0.267017
 16940/20000: episode: 116, duration: 0.971s, episode steps: 151, steps per second: 155, episode reward: -285.977, mean reward: -1.894 [-100.000,  5.342], mean action: 1.675 [0.000, 3.000],  loss: 13.178213, mae: 34.005578, mean_q: 16.153781, mean_eps: 0.241120
 17940/20000: episode: 117, duration: 7.510s, episode steps: 1000, steps per second: 133, episode reward: -125.673, mean reward: -0.126 [-4.707,  4.105], mean action: 1.710 [0.000, 3.000],  loss: 9.973783, mae: 33.101956, mean_q: 15.569317, mean_eps: 0.215222
 18940/20000: episode: 118, duration: 8.356s, episode steps: 1000, steps per second: 120, episode reward: -186.146, mean reward: -0.186 [-4.702,  3.857], mean action: 1.478 [0.000, 3.000],  loss: 8.726761, mae: 33.078618, mean_q: 17.377673, mean_eps: 0.170222
 19940/20000: episode: 119, duration: 7.385s, episode steps: 1000, steps per second: 135, episode reward: -145.539, mean reward: -0.146 [-4.942,  3.574], mean action: 1.549 [0.000, 3.000],  loss: 9.369869, mae: 32.932312, mean_q: 18.061404, mean_eps: 0.125222
done, took 143.634 seconds
4
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_3 (Dense)              (None, 16)                144
_________________________________________________________________
activation_3 (Activation)    (None, 16)                0
_________________________________________________________________
dense_4 (Dense)              (None, 32)                544
_________________________________________________________________
activation_4 (Activation)    (None, 32)                0
_________________________________________________________________
dense_5 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_5 (Activation)    (None, 4)                 0
=================================================================
Total params: 820
Trainable params: 820
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 150000 steps ...
    118/150000: episode: 1, duration: 1.131s, episode steps: 118, steps per second: 104, episode reward: -55.905, mean reward: -0.474 [-100.000, 79.624], mean action: 1.500 [0.000, 3.000],  loss: 1.850291, mae: 0.549182, mean_q: 0.736841, mean_eps: 0.999616
    208/150000: episode: 2, duration: 0.492s, episode steps:  90, steps per second: 183, episode reward: -132.400, mean reward: -1.471 [-100.000,  6.466], mean action: 1.378 [0.000, 3.000],  loss: 54.357732, mae: 1.077460, mean_q: 1.548232, mean_eps: 0.999025
    321/150000: episode: 3, duration: 0.660s, episode steps: 113, steps per second: 171, episode reward: -151.979, mean reward: -1.345 [-100.000, 48.878], mean action: 1.522 [0.000, 3.000],  loss: 48.708992, mae: 1.500671, mean_q: 2.773741, mean_eps: 0.998416
    390/150000: episode: 4, duration: 0.392s, episode steps:  69, steps per second: 176, episode reward: -91.730, mean reward: -1.329 [-100.000,  8.500], mean action: 1.493 [0.000, 3.000],  loss: 59.387635, mae: 2.165279, mean_q: 3.309706, mean_eps: 0.997870
    464/150000: episode: 5, duration: 0.409s, episode steps:  74, steps per second: 181, episode reward: -216.123, mean reward: -2.921 [-100.000,  4.356], mean action: 1.541 [0.000, 3.000],  loss: 52.060253, mae: 2.206605, mean_q: 3.425191, mean_eps: 0.997441
    546/150000: episode: 6, duration: 0.452s, episode steps:  82, steps per second: 181, episode reward: -116.632, mean reward: -1.422 [-100.000, 17.239], mean action: 1.500 [0.000, 3.000],  loss: 61.745001, mae: 2.210836, mean_q: 3.347973, mean_eps: 0.996973
    624/150000: episode: 7, duration: 0.448s, episode steps:  78, steps per second: 174, episode reward: -422.175, mean reward: -5.412 [-100.000,  2.686], mean action: 1.821 [0.000, 3.000],  loss: 69.557416, mae: 2.327687, mean_q: 3.507505, mean_eps: 0.996493
    743/150000: episode: 8, duration: 0.673s, episode steps: 119, steps per second: 177, episode reward: -189.957, mean reward: -1.596 [-100.000,  9.507], mean action: 1.454 [0.000, 3.000],  loss: 63.396784, mae: 2.499649, mean_q: 3.592939, mean_eps: 0.995902
    840/150000: episode: 9, duration: 0.590s, episode steps:  97, steps per second: 164, episode reward: -97.294, mean reward: -1.003 [-100.000, 29.318], mean action: 1.289 [0.000, 3.000],  loss: 51.796643, mae: 2.596187, mean_q: 3.741195, mean_eps: 0.995254
    944/150000: episode: 10, duration: 0.638s, episode steps: 104, steps per second: 163, episode reward: -258.824, mean reward: -2.489 [-100.000,  0.859], mean action: 1.433 [0.000, 3.000],  loss: 68.196642, mae: 3.115844, mean_q: 4.415493, mean_eps: 0.994651
   1003/150000: episode: 11, duration: 0.361s, episode steps:  59, steps per second: 163, episode reward: -159.845, mean reward: -2.709 [-100.000,  5.971], mean action: 1.254 [0.000, 3.000],  loss: 73.911883, mae: 3.497042, mean_q: 5.016988, mean_eps: 0.994162
   1118/150000: episode: 12, duration: 0.669s, episode steps: 115, steps per second: 172, episode reward: -231.543, mean reward: -2.013 [-100.000,  4.290], mean action: 1.548 [0.000, 3.000],  loss: 52.893503, mae: 3.761829, mean_q: 5.732066, mean_eps: 0.993640
   1210/150000: episode: 13, duration: 0.512s, episode steps:  92, steps per second: 180, episode reward: -193.932, mean reward: -2.108 [-100.000, 11.592], mean action: 1.467 [0.000, 3.000],  loss: 57.569108, mae: 4.290692, mean_q: 6.496717, mean_eps: 0.993019
   1275/150000: episode: 14, duration: 0.352s, episode steps:  65, steps per second: 185, episode reward: -56.766, mean reward: -0.873 [-100.000, 13.036], mean action: 1.415 [0.000, 3.000],  loss: 55.389612, mae: 4.792916, mean_q: 7.181661, mean_eps: 0.992548
   1361/150000: episode: 15, duration: 0.502s, episode steps:  86, steps per second: 171, episode reward: -118.358, mean reward: -1.376 [-100.000, 18.567], mean action: 1.453 [0.000, 3.000],  loss: 52.953323, mae: 5.205847, mean_q: 7.785500, mean_eps: 0.992095
   1424/150000: episode: 16, duration: 0.406s, episode steps:  63, steps per second: 155, episode reward: -91.504, mean reward: -1.452 [-100.000,  8.890], mean action: 1.317 [0.000, 3.000],  loss: 76.671627, mae: 5.448720, mean_q: 8.071149, mean_eps: 0.991648
   1513/150000: episode: 17, duration: 0.627s, episode steps:  89, steps per second: 142, episode reward: -82.764, mean reward: -0.930 [-100.000, 16.597], mean action: 1.584 [0.000, 3.000],  loss: 67.811723, mae: 6.151501, mean_q: 8.805786, mean_eps: 0.991192
   1601/150000: episode: 18, duration: 0.529s, episode steps:  88, steps per second: 166, episode reward: -85.862, mean reward: -0.976 [-100.000, 18.991], mean action: 1.557 [0.000, 3.000],  loss: 37.211492, mae: 6.353107, mean_q: 9.319300, mean_eps: 0.990661
   1680/150000: episode: 19, duration: 0.522s, episode steps:  79, steps per second: 151, episode reward: -202.308, mean reward: -2.561 [-100.000,  7.422], mean action: 1.646 [0.000, 3.000],  loss: 48.513619, mae: 6.866871, mean_q: 9.846856, mean_eps: 0.990160
   1795/150000: episode: 20, duration: 0.798s, episode steps: 115, steps per second: 144, episode reward: -224.056, mean reward: -1.948 [-100.000, 25.373], mean action: 1.487 [0.000, 3.000],  loss: 42.590146, mae: 7.461738, mean_q: 10.628795, mean_eps: 0.989578
   1866/150000: episode: 21, duration: 0.471s, episode steps:  71, steps per second: 151, episode reward: -83.153, mean reward: -1.171 [-100.000, 11.623], mean action: 1.310 [0.000, 3.000],  loss: 41.650771, mae: 8.178101, mean_q: 11.481734, mean_eps: 0.989020
   1938/150000: episode: 22, duration: 0.437s, episode steps:  72, steps per second: 165, episode reward: -131.449, mean reward: -1.826 [-100.000, 19.543], mean action: 1.528 [0.000, 3.000],  loss: 60.712236, mae: 8.887850, mean_q: 11.969670, mean_eps: 0.988591
   2030/150000: episode: 23, duration: 0.542s, episode steps:  92, steps per second: 170, episode reward: -96.990, mean reward: -1.054 [-100.000,  5.954], mean action: 1.478 [0.000, 3.000],  loss: 38.761896, mae: 9.300995, mean_q: 12.611271, mean_eps: 0.988099
   2114/150000: episode: 24, duration: 0.498s, episode steps:  84, steps per second: 169, episode reward: -116.610, mean reward: -1.388 [-100.000, 27.143], mean action: 1.429 [0.000, 3.000],  loss: 39.158151, mae: 9.953969, mean_q: 13.649375, mean_eps: 0.987571
   2211/150000: episode: 25, duration: 0.544s, episode steps:  97, steps per second: 178, episode reward: -167.674, mean reward: -1.729 [-100.000,  9.486], mean action: 1.505 [0.000, 3.000],  loss: 44.605384, mae: 10.682190, mean_q: 13.795735, mean_eps: 0.987028
   2289/150000: episode: 26, duration: 0.450s, episode steps:  78, steps per second: 173, episode reward: -70.651, mean reward: -0.906 [-100.000,  8.100], mean action: 1.410 [0.000, 3.000],  loss: 38.253883, mae: 11.079883, mean_q: 14.557858, mean_eps: 0.986503
   2358/150000: episode: 27, duration: 0.402s, episode steps:  69, steps per second: 172, episode reward: -326.100, mean reward: -4.726 [-100.000,  3.143], mean action: 1.362 [0.000, 3.000],  loss: 46.968113, mae: 11.425565, mean_q: 14.700240, mean_eps: 0.986062
   2463/150000: episode: 28, duration: 0.595s, episode steps: 105, steps per second: 176, episode reward: -127.346, mean reward: -1.213 [-100.000,  8.477], mean action: 1.543 [0.000, 3.000],  loss: 28.782876, mae: 11.708651, mean_q: 15.005956, mean_eps: 0.985540
   2522/150000: episode: 29, duration: 0.355s, episode steps:  59, steps per second: 166, episode reward: -125.519, mean reward: -2.127 [-100.000,  5.694], mean action: 1.525 [0.000, 3.000],  loss: 34.375531, mae: 12.207389, mean_q: 15.896169, mean_eps: 0.985048
   2606/150000: episode: 30, duration: 0.548s, episode steps:  84, steps per second: 153, episode reward: -325.088, mean reward: -3.870 [-100.000,  4.677], mean action: 1.643 [0.000, 3.000],  loss: 31.793686, mae: 12.855010, mean_q: 16.074225, mean_eps: 0.984619
   2663/150000: episode: 31, duration: 0.434s, episode steps:  57, steps per second: 131, episode reward: -78.468, mean reward: -1.377 [-100.000, 13.830], mean action: 1.632 [0.000, 3.000],  loss: 36.648630, mae: 14.077169, mean_q: 15.525739, mean_eps: 0.984196
   2727/150000: episode: 32, duration: 0.430s, episode steps:  64, steps per second: 149, episode reward: -152.946, mean reward: -2.390 [-100.000,  8.154], mean action: 1.484 [0.000, 3.000],  loss: 23.845054, mae: 14.036685, mean_q: 16.660821, mean_eps: 0.983833
   2832/150000: episode: 33, duration: 0.675s, episode steps: 105, steps per second: 156, episode reward: -134.319, mean reward: -1.279 [-100.000,  8.286], mean action: 1.514 [0.000, 3.000],  loss: 24.735354, mae: 14.528552, mean_q: 16.829272, mean_eps: 0.983326
   2956/150000: episode: 34, duration: 0.749s, episode steps: 124, steps per second: 165, episode reward: -400.388, mean reward: -3.229 [-100.000,  2.256], mean action: 1.395 [0.000, 3.000],  loss: 26.083250, mae: 15.282711, mean_q: 16.020853, mean_eps: 0.982639
   3027/150000: episode: 35, duration: 0.427s, episode steps:  71, steps per second: 166, episode reward: -126.804, mean reward: -1.786 [-100.000, 10.013], mean action: 1.479 [0.000, 3.000],  loss: 19.988504, mae: 16.041544, mean_q: 15.901609, mean_eps: 0.982054
   3120/150000: episode: 36, duration: 0.502s, episode steps:  93, steps per second: 185, episode reward: -432.364, mean reward: -4.649 [-100.000,  0.141], mean action: 1.280 [0.000, 3.000],  loss: 22.437377, mae: 16.161810, mean_q: 15.507770, mean_eps: 0.981562
   3227/150000: episode: 37, duration: 0.598s, episode steps: 107, steps per second: 179, episode reward: -104.491, mean reward: -0.977 [-100.000,  6.825], mean action: 1.514 [0.000, 3.000],  loss: 26.100083, mae: 17.301413, mean_q: 16.860191, mean_eps: 0.980962
   3308/150000: episode: 38, duration: 0.442s, episode steps:  81, steps per second: 183, episode reward: -81.323, mean reward: -1.004 [-100.000,  8.413], mean action: 1.481 [0.000, 3.000],  loss: 25.573304, mae: 18.125930, mean_q: 17.480011, mean_eps: 0.980398
   3375/150000: episode: 39, duration: 0.377s, episode steps:  67, steps per second: 178, episode reward: -101.446, mean reward: -1.514 [-100.000,  6.694], mean action: 1.552 [0.000, 3.000],  loss: 16.149775, mae: 18.615569, mean_q: 16.670059, mean_eps: 0.979954
   3506/150000: episode: 40, duration: 0.761s, episode steps: 131, steps per second: 172, episode reward: -31.231, mean reward: -0.238 [-100.000, 106.293], mean action: 1.557 [0.000, 3.000],  loss: 16.579763, mae: 19.133845, mean_q: 16.992147, mean_eps: 0.979360
   3561/150000: episode: 41, duration: 0.325s, episode steps:  55, steps per second: 169, episode reward: -164.203, mean reward: -2.986 [-100.000,  4.218], mean action: 1.855 [0.000, 3.000],  loss: 20.450202, mae: 20.022085, mean_q: 16.862839, mean_eps: 0.978802
   3667/150000: episode: 42, duration: 0.669s, episode steps: 106, steps per second: 158, episode reward: -330.451, mean reward: -3.117 [-100.000, 12.844], mean action: 1.425 [0.000, 3.000],  loss: 20.327155, mae: 20.604907, mean_q: 16.688530, mean_eps: 0.978319
   3765/150000: episode: 43, duration: 0.549s, episode steps:  98, steps per second: 179, episode reward: -126.477, mean reward: -1.291 [-100.000,  5.239], mean action: 1.643 [0.000, 3.000],  loss: 16.021412, mae: 21.406526, mean_q: 15.471852, mean_eps: 0.977707
   3833/150000: episode: 44, duration: 0.365s, episode steps:  68, steps per second: 186, episode reward: -115.346, mean reward: -1.696 [-100.000,  8.686], mean action: 1.412 [0.000, 3.000],  loss: 11.631508, mae: 21.679474, mean_q: 15.744423, mean_eps: 0.977209
   3896/150000: episode: 45, duration: 0.346s, episode steps:  63, steps per second: 182, episode reward: -66.891, mean reward: -1.062 [-100.000, 11.566], mean action: 1.476 [0.000, 3.000],  loss: 13.477067, mae: 22.275090, mean_q: 15.686109, mean_eps: 0.976816
   3966/150000: episode: 46, duration: 0.506s, episode steps:  70, steps per second: 138, episode reward: -124.086, mean reward: -1.773 [-100.000, 10.343], mean action: 1.686 [0.000, 3.000],  loss: 15.587689, mae: 22.997004, mean_q: 15.331608, mean_eps: 0.976417
   4079/150000: episode: 47, duration: 0.753s, episode steps: 113, steps per second: 150, episode reward: -117.336, mean reward: -1.038 [-100.000, 10.896], mean action: 1.558 [0.000, 3.000],  loss: 18.568405, mae: 23.272443, mean_q: 14.830392, mean_eps: 0.975868
   4143/150000: episode: 48, duration: 0.376s, episode steps:  64, steps per second: 170, episode reward: -119.467, mean reward: -1.867 [-100.000, 88.956], mean action: 1.484 [0.000, 3.000],  loss: 20.183380, mae: 24.320844, mean_q: 15.501897, mean_eps: 0.975337
   4267/150000: episode: 49, duration: 0.704s, episode steps: 124, steps per second: 176, episode reward: -111.285, mean reward: -0.897 [-100.000,  6.617], mean action: 1.492 [0.000, 3.000],  loss: 14.481483, mae: 23.923494, mean_q: 15.800904, mean_eps: 0.974773
   4346/150000: episode: 50, duration: 0.468s, episode steps:  79, steps per second: 169, episode reward: -84.408, mean reward: -1.068 [-100.000, 19.413], mean action: 1.329 [0.000, 3.000],  loss: 12.703751, mae: 24.931181, mean_q: 13.394963, mean_eps: 0.974164
   4428/150000: episode: 51, duration: 0.478s, episode steps:  82, steps per second: 172, episode reward: -128.473, mean reward: -1.567 [-100.000,  6.475], mean action: 1.500 [0.000, 3.000],  loss: 15.023998, mae: 25.312733, mean_q: 14.729981, mean_eps: 0.973681
   4499/150000: episode: 52, duration: 0.398s, episode steps:  71, steps per second: 179, episode reward: -123.868, mean reward: -1.745 [-100.000, 10.697], mean action: 1.437 [0.000, 3.000],  loss: 9.519160, mae: 25.458909, mean_q: 13.471179, mean_eps: 0.973222
   4630/150000: episode: 53, duration: 0.742s, episode steps: 131, steps per second: 176, episode reward: -144.847, mean reward: -1.106 [-100.000, 31.156], mean action: 1.557 [0.000, 3.000],  loss: 12.663813, mae: 25.676303, mean_q: 14.297237, mean_eps: 0.972616
   4719/150000: episode: 54, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -75.493, mean reward: -0.848 [-100.000,  8.285], mean action: 1.416 [0.000, 3.000],  loss: 10.646745, mae: 26.145372, mean_q: 13.957761, mean_eps: 0.971956
   4814/150000: episode: 55, duration: 0.576s, episode steps:  95, steps per second: 165, episode reward: -316.312, mean reward: -3.330 [-100.000,  0.090], mean action: 1.442 [0.000, 3.000],  loss: 15.485780, mae: 26.546643, mean_q: 13.589012, mean_eps: 0.971404
   4927/150000: episode: 56, duration: 0.846s, episode steps: 113, steps per second: 134, episode reward: -322.988, mean reward: -2.858 [-100.000,  4.352], mean action: 1.460 [0.000, 3.000],  loss: 9.293388, mae: 26.728336, mean_q: 12.740131, mean_eps: 0.970780
   5051/150000: episode: 57, duration: 0.890s, episode steps: 124, steps per second: 139, episode reward: -328.156, mean reward: -2.646 [-100.000, 34.058], mean action: 1.605 [0.000, 3.000],  loss: 12.538841, mae: 28.060028, mean_q: 12.568330, mean_eps: 0.970069
   5158/150000: episode: 58, duration: 0.841s, episode steps: 107, steps per second: 127, episode reward: -374.944, mean reward: -3.504 [-100.000,  0.735], mean action: 1.467 [0.000, 3.000],  loss: 10.665675, mae: 28.048415, mean_q: 12.593197, mean_eps: 0.969376
   5237/150000: episode: 59, duration: 0.606s, episode steps:  79, steps per second: 130, episode reward: -144.645, mean reward: -1.831 [-100.000,  6.296], mean action: 1.329 [0.000, 3.000],  loss: 11.512671, mae: 28.751359, mean_q: 11.230199, mean_eps: 0.968818
   5337/150000: episode: 60, duration: 0.774s, episode steps: 100, steps per second: 129, episode reward: -140.685, mean reward: -1.407 [-100.000,  6.710], mean action: 1.760 [0.000, 3.000],  loss: 12.741419, mae: 29.514060, mean_q: 11.505241, mean_eps: 0.968281
   5421/150000: episode: 61, duration: 0.699s, episode steps:  84, steps per second: 120, episode reward: -124.678, mean reward: -1.484 [-100.000,  9.576], mean action: 1.583 [0.000, 3.000],  loss: 11.572125, mae: 30.115064, mean_q: 10.452561, mean_eps: 0.967729
   5501/150000: episode: 62, duration: 0.603s, episode steps:  80, steps per second: 133, episode reward: -206.371, mean reward: -2.580 [-100.000,  9.207], mean action: 1.363 [0.000, 3.000],  loss: 13.016402, mae: 29.602976, mean_q: 12.198580, mean_eps: 0.967237
   5590/150000: episode: 63, duration: 0.805s, episode steps:  89, steps per second: 111, episode reward: -502.346, mean reward: -5.644 [-100.000,  4.381], mean action: 1.663 [0.000, 3.000],  loss: 10.065605, mae: 30.571440, mean_q: 9.629093, mean_eps: 0.966730
   5686/150000: episode: 64, duration: 0.744s, episode steps:  96, steps per second: 129, episode reward: -222.668, mean reward: -2.319 [-100.000,  3.009], mean action: 1.615 [0.000, 3.000],  loss: 8.912035, mae: 31.234956, mean_q: 10.704460, mean_eps: 0.966175
   5763/150000: episode: 65, duration: 0.596s, episode steps:  77, steps per second: 129, episode reward: -147.648, mean reward: -1.918 [-100.000, 38.358], mean action: 1.584 [0.000, 3.000],  loss: 13.677250, mae: 32.061308, mean_q: 9.696894, mean_eps: 0.965656
   5846/150000: episode: 66, duration: 0.664s, episode steps:  83, steps per second: 125, episode reward: -100.064, mean reward: -1.206 [-100.000,  6.736], mean action: 1.711 [0.000, 3.000],  loss: 14.412853, mae: 32.205178, mean_q: 11.221451, mean_eps: 0.965176
   5967/150000: episode: 67, duration: 0.933s, episode steps: 121, steps per second: 130, episode reward: -81.068, mean reward: -0.670 [-100.000, 11.418], mean action: 1.430 [0.000, 3.000],  loss: 10.129826, mae: 32.652763, mean_q: 9.491638, mean_eps: 0.964564
   6097/150000: episode: 68, duration: 0.950s, episode steps: 130, steps per second: 137, episode reward: -278.177, mean reward: -2.140 [-100.000,  9.934], mean action: 1.662 [0.000, 3.000],  loss: 17.019162, mae: 32.875097, mean_q: 8.894015, mean_eps: 0.963811
   6176/150000: episode: 69, duration: 0.689s, episode steps:  79, steps per second: 115, episode reward: 47.028, mean reward:  0.595 [-100.000, 76.047], mean action: 1.747 [0.000, 3.000],  loss: 14.045297, mae: 32.825299, mean_q: 9.910544, mean_eps: 0.963184
   6263/150000: episode: 70, duration: 0.674s, episode steps:  87, steps per second: 129, episode reward: -88.170, mean reward: -1.013 [-100.000,  6.694], mean action: 1.667 [0.000, 3.000],  loss: 15.025170, mae: 33.049836, mean_q: 9.530602, mean_eps: 0.962686
   6382/150000: episode: 71, duration: 1.099s, episode steps: 119, steps per second: 108, episode reward: -241.776, mean reward: -2.032 [-100.000, 68.142], mean action: 1.496 [0.000, 3.000],  loss: 11.193475, mae: 34.012505, mean_q: 9.489087, mean_eps: 0.962068
   6482/150000: episode: 72, duration: 0.822s, episode steps: 100, steps per second: 122, episode reward: -85.495, mean reward: -0.855 [-100.000, 16.962], mean action: 1.490 [0.000, 3.000],  loss: 7.892070, mae: 33.371062, mean_q: 10.876350, mean_eps: 0.961411
   6595/150000: episode: 73, duration: 0.926s, episode steps: 113, steps per second: 122, episode reward: -80.540, mean reward: -0.713 [-100.000,  9.063], mean action: 1.451 [0.000, 3.000],  loss: 9.846278, mae: 34.089026, mean_q: 9.707631, mean_eps: 0.960772
   6718/150000: episode: 74, duration: 0.967s, episode steps: 123, steps per second: 127, episode reward: -49.875, mean reward: -0.405 [-100.000, 32.601], mean action: 1.439 [0.000, 3.000],  loss: 11.971202, mae: 34.569541, mean_q: 11.726263, mean_eps: 0.960064
   6790/150000: episode: 75, duration: 0.526s, episode steps:  72, steps per second: 137, episode reward: -83.177, mean reward: -1.155 [-100.000, 42.429], mean action: 1.458 [0.000, 3.000],  loss: 14.754373, mae: 35.190238, mean_q: 11.333006, mean_eps: 0.959479
   6879/150000: episode: 76, duration: 0.645s, episode steps:  89, steps per second: 138, episode reward: -351.237, mean reward: -3.946 [-100.000,  0.167], mean action: 1.539 [0.000, 3.000],  loss: 21.602432, mae: 35.423039, mean_q: 11.435759, mean_eps: 0.958996
   6982/150000: episode: 77, duration: 0.702s, episode steps: 103, steps per second: 147, episode reward: -164.163, mean reward: -1.594 [-100.000, 11.203], mean action: 1.573 [0.000, 3.000],  loss: 16.930622, mae: 35.261078, mean_q: 10.299370, mean_eps: 0.958420
   7086/150000: episode: 78, duration: 0.689s, episode steps: 104, steps per second: 151, episode reward: -146.037, mean reward: -1.404 [-100.000, 11.802], mean action: 1.317 [0.000, 3.000],  loss: 18.209752, mae: 35.343621, mean_q: 10.447707, mean_eps: 0.957799
   7207/150000: episode: 79, duration: 0.769s, episode steps: 121, steps per second: 157, episode reward: -355.629, mean reward: -2.939 [-100.000, 27.225], mean action: 1.545 [0.000, 3.000],  loss: 18.459241, mae: 35.647913, mean_q: 11.843904, mean_eps: 0.957124
   7313/150000: episode: 80, duration: 0.752s, episode steps: 106, steps per second: 141, episode reward: -118.392, mean reward: -1.117 [-100.000, 39.145], mean action: 1.604 [0.000, 3.000],  loss: 19.085341, mae: 36.110438, mean_q: 10.658530, mean_eps: 0.956443
   7388/150000: episode: 81, duration: 0.500s, episode steps:  75, steps per second: 150, episode reward: -104.619, mean reward: -1.395 [-100.000, 13.334], mean action: 1.347 [0.000, 3.000],  loss: 13.296905, mae: 35.804077, mean_q: 11.378674, mean_eps: 0.955900
   7502/150000: episode: 82, duration: 0.720s, episode steps: 114, steps per second: 158, episode reward: -190.295, mean reward: -1.669 [-100.000,  4.779], mean action: 1.395 [0.000, 3.000],  loss: 11.965227, mae: 36.004180, mean_q: 11.777407, mean_eps: 0.955333
   7608/150000: episode: 83, duration: 0.734s, episode steps: 106, steps per second: 144, episode reward: -300.940, mean reward: -2.839 [-100.000,  0.977], mean action: 1.670 [0.000, 3.000],  loss: 9.586795, mae: 36.872407, mean_q: 12.031629, mean_eps: 0.954673
   7708/150000: episode: 84, duration: 0.656s, episode steps: 100, steps per second: 152, episode reward: -102.179, mean reward: -1.022 [-100.000, 69.877], mean action: 1.530 [0.000, 3.000],  loss: 15.055349, mae: 36.520461, mean_q: 12.673867, mean_eps: 0.954055
   7792/150000: episode: 85, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -149.418, mean reward: -1.779 [-100.000,  9.839], mean action: 1.631 [0.000, 3.000],  loss: 15.986588, mae: 36.605138, mean_q: 12.202051, mean_eps: 0.953503
   7866/150000: episode: 86, duration: 0.461s, episode steps:  74, steps per second: 161, episode reward: -126.632, mean reward: -1.711 [-100.000,  6.630], mean action: 1.649 [0.000, 3.000],  loss: 11.441070, mae: 36.694345, mean_q: 13.583810, mean_eps: 0.953029
   7946/150000: episode: 87, duration: 0.557s, episode steps:  80, steps per second: 144, episode reward: -175.151, mean reward: -2.189 [-100.000,  9.056], mean action: 1.450 [0.000, 3.000],  loss: 12.319999, mae: 37.235024, mean_q: 12.980666, mean_eps: 0.952567
   8035/150000: episode: 88, duration: 0.620s, episode steps:  89, steps per second: 144, episode reward: 23.116, mean reward:  0.260 [-100.000, 117.243], mean action: 1.494 [0.000, 3.000],  loss: 16.725611, mae: 37.470148, mean_q: 10.623113, mean_eps: 0.952060
   8103/150000: episode: 89, duration: 0.460s, episode steps:  68, steps per second: 148, episode reward: -27.050, mean reward: -0.398 [-100.000, 12.115], mean action: 1.559 [0.000, 3.000],  loss: 22.575954, mae: 37.207653, mean_q: 13.721016, mean_eps: 0.951589
   8183/150000: episode: 90, duration: 0.526s, episode steps:  80, steps per second: 152, episode reward: -84.740, mean reward: -1.059 [-100.000, 18.183], mean action: 1.312 [0.000, 3.000],  loss: 13.935635, mae: 37.078532, mean_q: 12.770004, mean_eps: 0.951145
   8249/150000: episode: 91, duration: 0.476s, episode steps:  66, steps per second: 139, episode reward: -238.754, mean reward: -3.617 [-100.000,  4.592], mean action: 1.394 [0.000, 3.000],  loss: 15.548978, mae: 37.578721, mean_q: 13.213920, mean_eps: 0.950707
   8362/150000: episode: 92, duration: 0.763s, episode steps: 113, steps per second: 148, episode reward: -108.583, mean reward: -0.961 [-100.000,  9.562], mean action: 1.575 [0.000, 3.000],  loss: 12.564311, mae: 37.625567, mean_q: 12.281904, mean_eps: 0.950170
   8445/150000: episode: 93, duration: 0.529s, episode steps:  83, steps per second: 157, episode reward: -137.139, mean reward: -1.652 [-100.000, 27.678], mean action: 1.482 [0.000, 3.000],  loss: 19.702047, mae: 37.725364, mean_q: 14.499559, mean_eps: 0.949582
   8531/150000: episode: 94, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -31.304, mean reward: -0.364 [-100.000, 62.746], mean action: 1.605 [0.000, 3.000],  loss: 18.500711, mae: 37.805966, mean_q: 14.469894, mean_eps: 0.949075
   8623/150000: episode: 95, duration: 0.635s, episode steps:  92, steps per second: 145, episode reward: -321.096, mean reward: -3.490 [-100.000, 93.673], mean action: 1.663 [0.000, 3.000],  loss: 14.970868, mae: 38.547920, mean_q: 12.310729, mean_eps: 0.948541
   8735/150000: episode: 96, duration: 0.720s, episode steps: 112, steps per second: 156, episode reward: -236.730, mean reward: -2.114 [-100.000,  6.953], mean action: 1.562 [0.000, 3.000],  loss: 17.033253, mae: 37.934308, mean_q: 12.964082, mean_eps: 0.947929
   8817/150000: episode: 97, duration: 0.462s, episode steps:  82, steps per second: 177, episode reward: -132.127, mean reward: -1.611 [-100.000,  9.214], mean action: 1.610 [0.000, 3.000],  loss: 11.639776, mae: 38.493909, mean_q: 15.519007, mean_eps: 0.947347
   8899/150000: episode: 98, duration: 0.477s, episode steps:  82, steps per second: 172, episode reward: -66.989, mean reward: -0.817 [-100.000, 13.738], mean action: 1.439 [0.000, 3.000],  loss: 14.479139, mae: 37.394593, mean_q: 13.137634, mean_eps: 0.946855
   9020/150000: episode: 99, duration: 0.722s, episode steps: 121, steps per second: 168, episode reward: -85.645, mean reward: -0.708 [-100.000,  8.260], mean action: 1.587 [0.000, 3.000],  loss: 13.728642, mae: 38.208490, mean_q: 15.699039, mean_eps: 0.946246
   9089/150000: episode: 100, duration: 0.389s, episode steps:  69, steps per second: 178, episode reward: -82.828, mean reward: -1.200 [-100.000, 10.792], mean action: 1.464 [0.000, 3.000],  loss: 11.702772, mae: 38.037644, mean_q: 13.519699, mean_eps: 0.945676
   9194/150000: episode: 101, duration: 0.599s, episode steps: 105, steps per second: 175, episode reward: -187.829, mean reward: -1.789 [-100.000,  5.111], mean action: 1.552 [0.000, 3.000],  loss: 14.697129, mae: 38.460337, mean_q: 13.804905, mean_eps: 0.945154
   9321/150000: episode: 102, duration: 0.745s, episode steps: 127, steps per second: 170, episode reward: -173.775, mean reward: -1.368 [-100.000, 21.289], mean action: 1.543 [0.000, 3.000],  loss: 16.334629, mae: 37.851619, mean_q: 15.160613, mean_eps: 0.944458
   9387/150000: episode: 103, duration: 0.380s, episode steps:  66, steps per second: 174, episode reward: -182.821, mean reward: -2.770 [-100.000,  4.766], mean action: 1.621 [0.000, 3.000],  loss: 9.464451, mae: 38.706240, mean_q: 13.749383, mean_eps: 0.943879
   9460/150000: episode: 104, duration: 0.404s, episode steps:  73, steps per second: 181, episode reward: -86.611, mean reward: -1.186 [-100.000, 19.083], mean action: 1.616 [0.000, 3.000],  loss: 16.938365, mae: 37.900305, mean_q: 14.692128, mean_eps: 0.943462
   9580/150000: episode: 105, duration: 0.649s, episode steps: 120, steps per second: 185, episode reward: -376.874, mean reward: -3.141 [-100.000,  4.981], mean action: 1.558 [0.000, 3.000],  loss: 13.036832, mae: 38.519636, mean_q: 16.122158, mean_eps: 0.942883
   9703/150000: episode: 106, duration: 0.711s, episode steps: 123, steps per second: 173, episode reward: -186.755, mean reward: -1.518 [-100.000, 27.629], mean action: 1.642 [0.000, 3.000],  loss: 10.879793, mae: 39.018181, mean_q: 13.320386, mean_eps: 0.942154
   9806/150000: episode: 107, duration: 0.666s, episode steps: 103, steps per second: 155, episode reward: -259.351, mean reward: -2.518 [-100.000,  8.913], mean action: 1.495 [0.000, 3.000],  loss: 12.818168, mae: 39.123497, mean_q: 14.912124, mean_eps: 0.941476
   9952/150000: episode: 108, duration: 0.928s, episode steps: 146, steps per second: 157, episode reward: -190.252, mean reward: -1.303 [-100.000,  4.597], mean action: 1.479 [0.000, 3.000],  loss: 14.297195, mae: 39.399227, mean_q: 15.810207, mean_eps: 0.940729
  10047/150000: episode: 109, duration: 0.641s, episode steps:  95, steps per second: 148, episode reward: -120.106, mean reward: -1.264 [-100.000, 13.409], mean action: 1.368 [0.000, 3.000],  loss: 8.502488, mae: 39.125767, mean_q: 18.312160, mean_eps: 0.940006
  10124/150000: episode: 110, duration: 0.529s, episode steps:  77, steps per second: 145, episode reward: -220.940, mean reward: -2.869 [-100.000, 19.711], mean action: 1.688 [0.000, 3.000],  loss: 13.587645, mae: 40.522217, mean_q: 15.602470, mean_eps: 0.939490
  10187/150000: episode: 111, duration: 0.437s, episode steps:  63, steps per second: 144, episode reward: -88.744, mean reward: -1.409 [-100.000, 29.276], mean action: 1.603 [0.000, 3.000],  loss: 11.568358, mae: 39.620343, mean_q: 16.438691, mean_eps: 0.939070
  10255/150000: episode: 112, duration: 0.422s, episode steps:  68, steps per second: 161, episode reward: -74.250, mean reward: -1.092 [-100.000, 10.081], mean action: 1.603 [0.000, 3.000],  loss: 21.763382, mae: 39.839341, mean_q: 16.776312, mean_eps: 0.938677
  10327/150000: episode: 113, duration: 0.499s, episode steps:  72, steps per second: 144, episode reward: -112.589, mean reward: -1.564 [-100.000,  9.979], mean action: 1.639 [0.000, 3.000],  loss: 20.016811, mae: 39.907790, mean_q: 17.091075, mean_eps: 0.938257
  10390/150000: episode: 114, duration: 0.416s, episode steps:  63, steps per second: 151, episode reward: -60.958, mean reward: -0.968 [-100.000, 12.118], mean action: 1.413 [0.000, 3.000],  loss: 11.244733, mae: 40.534525, mean_q: 16.992452, mean_eps: 0.937852
  10499/150000: episode: 115, duration: 0.700s, episode steps: 109, steps per second: 156, episode reward: -110.246, mean reward: -1.011 [-100.000, 10.372], mean action: 1.532 [0.000, 3.000],  loss: 22.797808, mae: 39.278249, mean_q: 16.546401, mean_eps: 0.937336
  10580/150000: episode: 116, duration: 0.529s, episode steps:  81, steps per second: 153, episode reward: -46.366, mean reward: -0.572 [-100.000, 104.444], mean action: 1.370 [0.000, 3.000],  loss: 13.144305, mae: 39.912799, mean_q: 15.744041, mean_eps: 0.936766
  10663/150000: episode: 117, duration: 0.596s, episode steps:  83, steps per second: 139, episode reward: 46.923, mean reward:  0.565 [-100.000, 108.670], mean action: 1.614 [0.000, 3.000],  loss: 14.790362, mae: 39.670206, mean_q: 18.305145, mean_eps: 0.936274
  10759/150000: episode: 118, duration: 0.653s, episode steps:  96, steps per second: 147, episode reward: -266.076, mean reward: -2.772 [-100.000, 72.623], mean action: 1.562 [0.000, 3.000],  loss: 18.820186, mae: 40.155699, mean_q: 16.419962, mean_eps: 0.935737
  10819/150000: episode: 119, duration: 0.405s, episode steps:  60, steps per second: 148, episode reward: -105.484, mean reward: -1.758 [-100.000,  9.435], mean action: 1.767 [0.000, 3.000],  loss: 8.078431, mae: 39.596035, mean_q: 14.397462, mean_eps: 0.935269
  10900/150000: episode: 120, duration: 0.510s, episode steps:  81, steps per second: 159, episode reward: -172.343, mean reward: -2.128 [-100.000,  7.248], mean action: 1.568 [0.000, 3.000],  loss: 11.955131, mae: 39.907125, mean_q: 15.258901, mean_eps: 0.934846
  11018/150000: episode: 121, duration: 0.825s, episode steps: 118, steps per second: 143, episode reward: -67.420, mean reward: -0.571 [-100.000, 23.279], mean action: 1.602 [0.000, 3.000],  loss: 14.925215, mae: 40.875642, mean_q: 16.323152, mean_eps: 0.934249
  11112/150000: episode: 122, duration: 0.616s, episode steps:  94, steps per second: 153, episode reward: -171.237, mean reward: -1.822 [-100.000,  6.516], mean action: 1.638 [0.000, 3.000],  loss: 25.739185, mae: 40.823729, mean_q: 17.503290, mean_eps: 0.933613
  11232/150000: episode: 123, duration: 0.785s, episode steps: 120, steps per second: 153, episode reward: -56.197, mean reward: -0.468 [-100.000, 68.084], mean action: 1.575 [0.000, 3.000],  loss: 13.841439, mae: 41.085550, mean_q: 19.193481, mean_eps: 0.932971
  11305/150000: episode: 124, duration: 0.504s, episode steps:  73, steps per second: 145, episode reward: -126.722, mean reward: -1.736 [-100.000, 12.660], mean action: 1.589 [0.000, 3.000],  loss: 19.876161, mae: 40.828326, mean_q: 17.562138, mean_eps: 0.932392
  11400/150000: episode: 125, duration: 0.625s, episode steps:  95, steps per second: 152, episode reward: -354.778, mean reward: -3.735 [-100.000,  0.625], mean action: 1.516 [0.000, 3.000],  loss: 15.895657, mae: 41.968208, mean_q: 21.034220, mean_eps: 0.931888
  11472/150000: episode: 126, duration: 0.494s, episode steps:  72, steps per second: 146, episode reward: -89.321, mean reward: -1.241 [-100.000, 13.171], mean action: 1.292 [0.000, 3.000],  loss: 12.058470, mae: 41.212300, mean_q: 20.649628, mean_eps: 0.931387
  11533/150000: episode: 127, duration: 0.362s, episode steps:  61, steps per second: 169, episode reward: -75.756, mean reward: -1.242 [-100.000,  7.606], mean action: 1.557 [0.000, 3.000],  loss: 20.796110, mae: 41.552213, mean_q: 19.800596, mean_eps: 0.930988
  11619/150000: episode: 128, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -111.085, mean reward: -1.292 [-100.000,  9.693], mean action: 1.651 [0.000, 3.000],  loss: 25.214370, mae: 41.788159, mean_q: 19.672161, mean_eps: 0.930547
  11741/150000: episode: 129, duration: 0.820s, episode steps: 122, steps per second: 149, episode reward:  5.264, mean reward:  0.043 [-100.000, 46.693], mean action: 1.443 [0.000, 3.000],  loss: 14.958340, mae: 41.608572, mean_q: 22.492377, mean_eps: 0.929923
  11843/150000: episode: 130, duration: 0.693s, episode steps: 102, steps per second: 147, episode reward: -166.163, mean reward: -1.629 [-100.000, 13.151], mean action: 1.539 [0.000, 3.000],  loss: 14.373114, mae: 41.599865, mean_q: 22.759309, mean_eps: 0.929251
  11961/150000: episode: 131, duration: 0.807s, episode steps: 118, steps per second: 146, episode reward: -413.875, mean reward: -3.507 [-100.000,  3.646], mean action: 1.627 [0.000, 3.000],  loss: 17.629168, mae: 42.043868, mean_q: 21.612309, mean_eps: 0.928591
  12083/150000: episode: 132, duration: 0.811s, episode steps: 122, steps per second: 151, episode reward: -47.109, mean reward: -0.386 [-100.000, 11.224], mean action: 1.779 [0.000, 3.000],  loss: 17.168917, mae: 43.265826, mean_q: 22.044282, mean_eps: 0.927871
  12185/150000: episode: 133, duration: 0.673s, episode steps: 102, steps per second: 152, episode reward: -100.005, mean reward: -0.980 [-100.000,  6.931], mean action: 1.588 [0.000, 3.000],  loss: 19.887569, mae: 43.947533, mean_q: 25.493863, mean_eps: 0.927199
  12263/150000: episode: 134, duration: 0.526s, episode steps:  78, steps per second: 148, episode reward: -56.241, mean reward: -0.721 [-100.000, 14.855], mean action: 1.692 [0.000, 3.000],  loss: 13.551083, mae: 43.723701, mean_q: 22.479493, mean_eps: 0.926659
  12377/150000: episode: 135, duration: 0.731s, episode steps: 114, steps per second: 156, episode reward: -122.147, mean reward: -1.071 [-100.000, 16.406], mean action: 1.535 [0.000, 3.000],  loss: 13.172969, mae: 43.388909, mean_q: 23.375685, mean_eps: 0.926083
  12475/150000: episode: 136, duration: 0.651s, episode steps:  98, steps per second: 151, episode reward: -169.328, mean reward: -1.728 [-100.000,  7.270], mean action: 1.602 [0.000, 3.000],  loss: 13.261639, mae: 44.027483, mean_q: 24.737618, mean_eps: 0.925447
  12541/150000: episode: 137, duration: 0.438s, episode steps:  66, steps per second: 151, episode reward: -30.550, mean reward: -0.463 [-100.000, 16.499], mean action: 1.591 [0.000, 3.000],  loss: 13.113988, mae: 44.123756, mean_q: 23.666815, mean_eps: 0.924955
  12650/150000: episode: 138, duration: 0.745s, episode steps: 109, steps per second: 146, episode reward: -110.807, mean reward: -1.017 [-100.000,  6.767], mean action: 1.505 [0.000, 3.000],  loss: 20.862164, mae: 44.355511, mean_q: 26.833216, mean_eps: 0.924430
  12730/150000: episode: 139, duration: 0.516s, episode steps:  80, steps per second: 155, episode reward: -178.866, mean reward: -2.236 [-100.000,  6.000], mean action: 1.438 [0.000, 3.000],  loss: 15.221260, mae: 44.435269, mean_q: 25.304365, mean_eps: 0.923863
  12812/150000: episode: 140, duration: 0.553s, episode steps:  82, steps per second: 148, episode reward: -128.799, mean reward: -1.571 [-100.000,  7.288], mean action: 1.720 [0.000, 3.000],  loss: 10.884540, mae: 44.822265, mean_q: 26.389572, mean_eps: 0.923377
  12867/150000: episode: 141, duration: 0.359s, episode steps:  55, steps per second: 153, episode reward: -76.653, mean reward: -1.394 [-100.000,  9.002], mean action: 1.527 [0.000, 3.000],  loss: 12.687334, mae: 44.953324, mean_q: 27.053953, mean_eps: 0.922966
  12944/150000: episode: 142, duration: 0.531s, episode steps:  77, steps per second: 145, episode reward: -89.597, mean reward: -1.164 [-100.000, 13.455], mean action: 1.481 [0.000, 3.000],  loss: 18.923987, mae: 44.111091, mean_q: 27.221478, mean_eps: 0.922570
  13020/150000: episode: 143, duration: 0.509s, episode steps:  76, steps per second: 149, episode reward: -171.200, mean reward: -2.253 [-100.000,  5.416], mean action: 1.566 [0.000, 3.000],  loss: 17.978643, mae: 45.172483, mean_q: 30.457574, mean_eps: 0.922111
  13088/150000: episode: 144, duration: 0.439s, episode steps:  68, steps per second: 155, episode reward: -59.448, mean reward: -0.874 [-100.000,  7.043], mean action: 1.456 [0.000, 3.000],  loss: 16.010380, mae: 45.280050, mean_q: 27.210228, mean_eps: 0.921679
  13196/150000: episode: 145, duration: 0.689s, episode steps: 108, steps per second: 157, episode reward: -115.383, mean reward: -1.068 [-100.000,  6.099], mean action: 1.639 [0.000, 3.000],  loss: 19.095038, mae: 44.742283, mean_q: 28.648641, mean_eps: 0.921151
  13266/150000: episode: 146, duration: 0.485s, episode steps:  70, steps per second: 144, episode reward: -92.137, mean reward: -1.316 [-100.000, 12.576], mean action: 1.400 [0.000, 3.000],  loss: 12.724000, mae: 45.344286, mean_q: 30.894262, mean_eps: 0.920617
  13376/150000: episode: 147, duration: 0.730s, episode steps: 110, steps per second: 151, episode reward: -125.117, mean reward: -1.137 [-100.000, 15.976], mean action: 1.664 [0.000, 3.000],  loss: 17.460084, mae: 45.463324, mean_q: 27.664953, mean_eps: 0.920077
  13453/150000: episode: 148, duration: 0.497s, episode steps:  77, steps per second: 155, episode reward: -66.332, mean reward: -0.861 [-100.000, 10.802], mean action: 1.455 [0.000, 3.000],  loss: 15.368429, mae: 44.559137, mean_q: 30.517744, mean_eps: 0.919516
  13525/150000: episode: 149, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -70.630, mean reward: -0.981 [-100.000,  8.145], mean action: 1.319 [0.000, 3.000],  loss: 13.005633, mae: 45.019242, mean_q: 31.046868, mean_eps: 0.919069
  13602/150000: episode: 150, duration: 0.529s, episode steps:  77, steps per second: 145, episode reward: -91.252, mean reward: -1.185 [-100.000,  8.925], mean action: 1.727 [0.000, 3.000],  loss: 21.250638, mae: 45.389333, mean_q: 27.908316, mean_eps: 0.918622
  13670/150000: episode: 151, duration: 0.430s, episode steps:  68, steps per second: 158, episode reward: -58.111, mean reward: -0.855 [-100.000, 17.272], mean action: 1.456 [0.000, 3.000],  loss: 26.666496, mae: 44.537434, mean_q: 31.469577, mean_eps: 0.918187
  13760/150000: episode: 152, duration: 0.568s, episode steps:  90, steps per second: 158, episode reward: -60.996, mean reward: -0.678 [-100.000, 20.498], mean action: 1.578 [0.000, 3.000],  loss: 11.627812, mae: 45.733465, mean_q: 30.536723, mean_eps: 0.917713
  13835/150000: episode: 153, duration: 0.497s, episode steps:  75, steps per second: 151, episode reward: -80.833, mean reward: -1.078 [-100.000,  6.452], mean action: 1.547 [0.000, 3.000],  loss: 13.268154, mae: 45.035206, mean_q: 31.321544, mean_eps: 0.917218
  13917/150000: episode: 154, duration: 0.492s, episode steps:  82, steps per second: 167, episode reward: -120.245, mean reward: -1.466 [-100.000,  9.891], mean action: 1.427 [0.000, 3.000],  loss: 16.536964, mae: 44.281487, mean_q: 26.886309, mean_eps: 0.916747
  13999/150000: episode: 155, duration: 0.470s, episode steps:  82, steps per second: 174, episode reward: -191.861, mean reward: -2.340 [-100.000,  7.529], mean action: 1.329 [0.000, 3.000],  loss: 17.486463, mae: 45.906517, mean_q: 29.114486, mean_eps: 0.916255
  14109/150000: episode: 156, duration: 0.604s, episode steps: 110, steps per second: 182, episode reward: -23.511, mean reward: -0.214 [-100.000, 95.201], mean action: 1.645 [0.000, 3.000],  loss: 17.164434, mae: 45.865895, mean_q: 30.819604, mean_eps: 0.915679
  14194/150000: episode: 157, duration: 0.480s, episode steps:  85, steps per second: 177, episode reward: -98.594, mean reward: -1.160 [-100.000,  6.204], mean action: 1.800 [0.000, 3.000],  loss: 15.756007, mae: 46.080943, mean_q: 30.939975, mean_eps: 0.915094
  14278/150000: episode: 158, duration: 0.491s, episode steps:  84, steps per second: 171, episode reward: -86.533, mean reward: -1.030 [-100.000,  7.614], mean action: 1.500 [0.000, 3.000],  loss: 13.937050, mae: 46.678212, mean_q: 31.959629, mean_eps: 0.914587
  14354/150000: episode: 159, duration: 0.445s, episode steps:  76, steps per second: 171, episode reward: -64.191, mean reward: -0.845 [-100.000, 12.791], mean action: 1.763 [0.000, 3.000],  loss: 17.052676, mae: 46.350241, mean_q: 31.154912, mean_eps: 0.914107
  14431/150000: episode: 160, duration: 0.431s, episode steps:  77, steps per second: 179, episode reward: -80.978, mean reward: -1.052 [-100.000, 11.775], mean action: 1.364 [0.000, 3.000],  loss: 9.425480, mae: 47.527738, mean_q: 33.004322, mean_eps: 0.913648
  14527/150000: episode: 161, duration: 0.522s, episode steps:  96, steps per second: 184, episode reward: -132.411, mean reward: -1.379 [-100.000,  5.750], mean action: 1.427 [0.000, 3.000],  loss: 19.364540, mae: 47.306420, mean_q: 31.651227, mean_eps: 0.913129
  14614/150000: episode: 162, duration: 0.474s, episode steps:  87, steps per second: 183, episode reward: -145.057, mean reward: -1.667 [-100.000, 34.275], mean action: 1.540 [0.000, 3.000],  loss: 12.499381, mae: 46.519941, mean_q: 30.985353, mean_eps: 0.912580
  14714/150000: episode: 163, duration: 0.611s, episode steps: 100, steps per second: 164, episode reward: -88.591, mean reward: -0.886 [-100.000, 13.372], mean action: 1.530 [0.000, 3.000],  loss: 14.708547, mae: 47.026036, mean_q: 32.261279, mean_eps: 0.912019
  14781/150000: episode: 164, duration: 0.377s, episode steps:  67, steps per second: 178, episode reward: -118.527, mean reward: -1.769 [-100.000, 21.405], mean action: 1.627 [0.000, 3.000],  loss: 8.527887, mae: 46.776152, mean_q: 31.450381, mean_eps: 0.911518
  14903/150000: episode: 165, duration: 0.761s, episode steps: 122, steps per second: 160, episode reward: -235.466, mean reward: -1.930 [-100.000, 58.176], mean action: 1.426 [0.000, 3.000],  loss: 15.271758, mae: 47.789272, mean_q: 32.430402, mean_eps: 0.910951
  14978/150000: episode: 166, duration: 0.630s, episode steps:  75, steps per second: 119, episode reward: -203.514, mean reward: -2.714 [-100.000,  8.367], mean action: 1.560 [0.000, 3.000],  loss: 14.546806, mae: 48.554494, mean_q: 33.076500, mean_eps: 0.910360
  15063/150000: episode: 167, duration: 0.647s, episode steps:  85, steps per second: 131, episode reward: -97.794, mean reward: -1.151 [-100.000,  9.835], mean action: 1.659 [0.000, 3.000],  loss: 13.665049, mae: 47.910317, mean_q: 32.706767, mean_eps: 0.909880
  15158/150000: episode: 168, duration: 0.684s, episode steps:  95, steps per second: 139, episode reward:  0.817, mean reward:  0.009 [-100.000, 50.624], mean action: 1.537 [0.000, 3.000],  loss: 9.540853, mae: 47.174664, mean_q: 31.001041, mean_eps: 0.909340
  15288/150000: episode: 169, duration: 0.933s, episode steps: 130, steps per second: 139, episode reward: -308.450, mean reward: -2.373 [-100.000, 125.733], mean action: 1.492 [0.000, 3.000],  loss: 16.839195, mae: 48.002664, mean_q: 32.018039, mean_eps: 0.908665
  15414/150000: episode: 170, duration: 0.954s, episode steps: 126, steps per second: 132, episode reward: -156.680, mean reward: -1.243 [-100.000,  6.101], mean action: 1.571 [0.000, 3.000],  loss: 8.898284, mae: 48.586320, mean_q: 31.602285, mean_eps: 0.907897
  15513/150000: episode: 171, duration: 0.653s, episode steps:  99, steps per second: 152, episode reward: -105.084, mean reward: -1.061 [-100.000,  7.537], mean action: 1.424 [0.000, 3.000],  loss: 15.563133, mae: 48.555337, mean_q: 33.236555, mean_eps: 0.907222
  15632/150000: episode: 172, duration: 0.841s, episode steps: 119, steps per second: 141, episode reward: -83.174, mean reward: -0.699 [-100.000, 19.371], mean action: 1.521 [0.000, 3.000],  loss: 12.134463, mae: 49.124535, mean_q: 34.237189, mean_eps: 0.906568
  15718/150000: episode: 173, duration: 0.649s, episode steps:  86, steps per second: 133, episode reward: -136.029, mean reward: -1.582 [-100.000, 24.010], mean action: 1.581 [0.000, 3.000],  loss: 8.921315, mae: 48.786251, mean_q: 34.017397, mean_eps: 0.905953
  15809/150000: episode: 174, duration: 0.588s, episode steps:  91, steps per second: 155, episode reward: -137.644, mean reward: -1.513 [-100.000, 27.991], mean action: 1.571 [0.000, 3.000],  loss: 14.383070, mae: 49.412728, mean_q: 33.440142, mean_eps: 0.905422
  15908/150000: episode: 175, duration: 0.684s, episode steps:  99, steps per second: 145, episode reward: -90.158, mean reward: -0.911 [-100.000,  8.578], mean action: 1.515 [0.000, 3.000],  loss: 15.871276, mae: 50.206899, mean_q: 35.430656, mean_eps: 0.904852
  15991/150000: episode: 176, duration: 0.607s, episode steps:  83, steps per second: 137, episode reward: -163.584, mean reward: -1.971 [-100.000, 17.196], mean action: 1.145 [0.000, 3.000],  loss: 9.942511, mae: 49.703369, mean_q: 32.584314, mean_eps: 0.904306
  16058/150000: episode: 177, duration: 0.437s, episode steps:  67, steps per second: 153, episode reward: -172.096, mean reward: -2.569 [-100.000, 10.509], mean action: 1.642 [0.000, 3.000],  loss: 21.813201, mae: 50.570502, mean_q: 34.743053, mean_eps: 0.903856
  16145/150000: episode: 178, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -263.575, mean reward: -3.030 [-100.000, 26.629], mean action: 1.632 [0.000, 3.000],  loss: 21.751341, mae: 49.369207, mean_q: 33.405589, mean_eps: 0.903394
  16240/150000: episode: 179, duration: 0.764s, episode steps:  95, steps per second: 124, episode reward: -135.299, mean reward: -1.424 [-100.000,  6.155], mean action: 1.547 [0.000, 3.000],  loss: 14.481221, mae: 50.413367, mean_q: 36.344156, mean_eps: 0.902848
  16313/150000: episode: 180, duration: 0.622s, episode steps:  73, steps per second: 117, episode reward: -137.988, mean reward: -1.890 [-100.000, 15.311], mean action: 1.329 [0.000, 3.000],  loss: 9.976971, mae: 50.720791, mean_q: 34.987726, mean_eps: 0.902344
  16426/150000: episode: 181, duration: 0.893s, episode steps: 113, steps per second: 126, episode reward: -103.113, mean reward: -0.913 [-100.000,  7.159], mean action: 1.522 [0.000, 3.000],  loss: 12.791256, mae: 50.484961, mean_q: 36.077181, mean_eps: 0.901786
  16491/150000: episode: 182, duration: 0.558s, episode steps:  65, steps per second: 117, episode reward: -79.630, mean reward: -1.225 [-100.000,  6.588], mean action: 1.462 [0.000, 3.000],  loss: 15.602413, mae: 50.588745, mean_q: 32.987117, mean_eps: 0.901252
  16560/150000: episode: 183, duration: 0.527s, episode steps:  69, steps per second: 131, episode reward: -113.940, mean reward: -1.651 [-100.000,  6.621], mean action: 1.420 [0.000, 3.000],  loss: 14.275691, mae: 51.417086, mean_q: 36.811480, mean_eps: 0.900850
  16681/150000: episode: 184, duration: 0.891s, episode steps: 121, steps per second: 136, episode reward: -144.914, mean reward: -1.198 [-100.000, 38.615], mean action: 1.736 [0.000, 3.000],  loss: 21.023646, mae: 50.801232, mean_q: 36.235855, mean_eps: 0.900280
  16774/150000: episode: 185, duration: 0.700s, episode steps:  93, steps per second: 133, episode reward: -133.023, mean reward: -1.430 [-100.000,  6.268], mean action: 1.430 [0.000, 3.000],  loss: 11.275126, mae: 50.583463, mean_q: 37.064998, mean_eps: 0.899638
  16903/150000: episode: 186, duration: 0.934s, episode steps: 129, steps per second: 138, episode reward: -131.578, mean reward: -1.020 [-100.000,  7.977], mean action: 1.636 [0.000, 3.000],  loss: 15.406444, mae: 50.900910, mean_q: 38.528009, mean_eps: 0.898972
  17012/150000: episode: 187, duration: 0.720s, episode steps: 109, steps per second: 151, episode reward: -224.297, mean reward: -2.058 [-100.000, 43.972], mean action: 1.578 [0.000, 3.000],  loss: 14.841135, mae: 50.933198, mean_q: 37.839346, mean_eps: 0.898258
  17088/150000: episode: 188, duration: 0.504s, episode steps:  76, steps per second: 151, episode reward: -118.540, mean reward: -1.560 [-100.000, 18.648], mean action: 1.579 [0.000, 3.000],  loss: 9.744155, mae: 51.421782, mean_q: 41.586285, mean_eps: 0.897703
  17184/150000: episode: 189, duration: 0.656s, episode steps:  96, steps per second: 146, episode reward: -75.604, mean reward: -0.788 [-100.000, 20.826], mean action: 1.448 [0.000, 3.000],  loss: 10.702508, mae: 51.615225, mean_q: 39.611821, mean_eps: 0.897187
  17249/150000: episode: 190, duration: 0.435s, episode steps:  65, steps per second: 150, episode reward: -64.762, mean reward: -0.996 [-100.000, 19.252], mean action: 1.538 [0.000, 3.000],  loss: 15.579590, mae: 52.654206, mean_q: 41.351248, mean_eps: 0.896704
  17328/150000: episode: 191, duration: 0.539s, episode steps:  79, steps per second: 147, episode reward: -100.274, mean reward: -1.269 [-100.000,  4.431], mean action: 1.544 [0.000, 3.000],  loss: 12.121186, mae: 51.467617, mean_q: 40.032980, mean_eps: 0.896272
  17419/150000: episode: 192, duration: 0.648s, episode steps:  91, steps per second: 140, episode reward: -57.417, mean reward: -0.631 [-100.000,  8.554], mean action: 1.582 [0.000, 3.000],  loss: 14.248586, mae: 52.514636, mean_q: 40.709977, mean_eps: 0.895762
  17542/150000: episode: 193, duration: 0.812s, episode steps: 123, steps per second: 151, episode reward: -62.502, mean reward: -0.508 [-100.000, 88.349], mean action: 1.528 [0.000, 3.000],  loss: 17.709907, mae: 52.283481, mean_q: 40.725807, mean_eps: 0.895120
  17642/150000: episode: 194, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -163.411, mean reward: -1.634 [-100.000,  2.685], mean action: 1.750 [0.000, 3.000],  loss: 15.393156, mae: 51.579735, mean_q: 39.661921, mean_eps: 0.894451
  17753/150000: episode: 195, duration: 0.851s, episode steps: 111, steps per second: 131, episode reward: -137.431, mean reward: -1.238 [-100.000,  8.854], mean action: 1.577 [0.000, 3.000],  loss: 14.030444, mae: 52.134131, mean_q: 41.240300, mean_eps: 0.893818
  17827/150000: episode: 196, duration: 0.540s, episode steps:  74, steps per second: 137, episode reward: -50.725, mean reward: -0.685 [-100.000, 16.629], mean action: 1.446 [0.000, 3.000],  loss: 10.241478, mae: 52.669841, mean_q: 43.520834, mean_eps: 0.893263
  17898/150000: episode: 197, duration: 0.475s, episode steps:  71, steps per second: 150, episode reward: -103.558, mean reward: -1.459 [-100.000,  8.776], mean action: 1.394 [0.000, 3.000],  loss: 22.526129, mae: 52.782838, mean_q: 41.361100, mean_eps: 0.892828
  17995/150000: episode: 198, duration: 0.676s, episode steps:  97, steps per second: 144, episode reward: -78.523, mean reward: -0.810 [-100.000, 14.587], mean action: 1.546 [0.000, 3.000],  loss: 10.643757, mae: 52.482340, mean_q: 41.571753, mean_eps: 0.892324
  18114/150000: episode: 199, duration: 0.834s, episode steps: 119, steps per second: 143, episode reward: -194.483, mean reward: -1.634 [-100.000, 25.323], mean action: 1.546 [0.000, 3.000],  loss: 17.655712, mae: 51.176998, mean_q: 41.774666, mean_eps: 0.891676
  18195/150000: episode: 200, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -84.463, mean reward: -1.043 [-100.000, 19.140], mean action: 1.506 [0.000, 3.000],  loss: 11.347999, mae: 51.373171, mean_q: 40.808853, mean_eps: 0.891076
  18332/150000: episode: 201, duration: 0.936s, episode steps: 137, steps per second: 146, episode reward: -69.481, mean reward: -0.507 [-100.000,  8.967], mean action: 1.679 [0.000, 3.000],  loss: 11.401578, mae: 51.035680, mean_q: 40.861058, mean_eps: 0.890422
  18413/150000: episode: 202, duration: 0.560s, episode steps:  81, steps per second: 145, episode reward: -69.057, mean reward: -0.853 [-100.000, 14.074], mean action: 1.568 [0.000, 3.000],  loss: 11.907349, mae: 52.336411, mean_q: 41.862290, mean_eps: 0.889768
  18481/150000: episode: 203, duration: 0.467s, episode steps:  68, steps per second: 146, episode reward: -100.948, mean reward: -1.485 [-100.000,  8.094], mean action: 1.574 [0.000, 3.000],  loss: 10.830872, mae: 51.501140, mean_q: 41.538802, mean_eps: 0.889321
  18606/150000: episode: 204, duration: 0.852s, episode steps: 125, steps per second: 147, episode reward: -130.523, mean reward: -1.044 [-100.000,  8.174], mean action: 1.400 [0.000, 3.000],  loss: 14.991081, mae: 51.476887, mean_q: 40.947243, mean_eps: 0.888742
  18683/150000: episode: 205, duration: 0.541s, episode steps:  77, steps per second: 142, episode reward: -89.848, mean reward: -1.167 [-100.000,  9.231], mean action: 1.416 [0.000, 3.000],  loss: 12.540749, mae: 51.228542, mean_q: 42.314518, mean_eps: 0.888136
  18797/150000: episode: 206, duration: 0.767s, episode steps: 114, steps per second: 149, episode reward: -155.073, mean reward: -1.360 [-100.000,  4.737], mean action: 1.596 [0.000, 3.000],  loss: 16.054688, mae: 51.658415, mean_q: 41.923486, mean_eps: 0.887563
  18877/150000: episode: 207, duration: 0.524s, episode steps:  80, steps per second: 153, episode reward: -88.636, mean reward: -1.108 [-100.000, 22.798], mean action: 1.350 [0.000, 3.000],  loss: 9.677293, mae: 51.326295, mean_q: 43.896086, mean_eps: 0.886981
  18996/150000: episode: 208, duration: 0.701s, episode steps: 119, steps per second: 170, episode reward: -87.510, mean reward: -0.735 [-100.000, 17.544], mean action: 1.471 [0.000, 3.000],  loss: 8.885819, mae: 52.160517, mean_q: 44.779163, mean_eps: 0.886384
  19097/150000: episode: 209, duration: 0.580s, episode steps: 101, steps per second: 174, episode reward: -243.378, mean reward: -2.410 [-100.000,  1.015], mean action: 1.624 [0.000, 3.000],  loss: 9.844247, mae: 53.066820, mean_q: 45.749619, mean_eps: 0.885724
  19185/150000: episode: 210, duration: 0.490s, episode steps:  88, steps per second: 180, episode reward: -101.609, mean reward: -1.155 [-100.000, 12.232], mean action: 1.670 [0.000, 3.000],  loss: 12.078127, mae: 53.458276, mean_q: 46.749781, mean_eps: 0.885157
  19339/150000: episode: 211, duration: 0.907s, episode steps: 154, steps per second: 170, episode reward: -71.031, mean reward: -0.461 [-100.000,  7.998], mean action: 1.526 [0.000, 3.000],  loss: 13.181834, mae: 52.804915, mean_q: 44.523633, mean_eps: 0.884431
  19464/150000: episode: 212, duration: 0.730s, episode steps: 125, steps per second: 171, episode reward: -78.844, mean reward: -0.631 [-100.000,  7.172], mean action: 1.544 [0.000, 3.000],  loss: 13.787150, mae: 52.717541, mean_q: 46.241346, mean_eps: 0.883594
  19574/150000: episode: 213, duration: 0.614s, episode steps: 110, steps per second: 179, episode reward: -165.170, mean reward: -1.502 [-100.000,  3.982], mean action: 1.673 [0.000, 3.000],  loss: 13.865437, mae: 52.719212, mean_q: 44.897420, mean_eps: 0.882889
  19678/150000: episode: 214, duration: 0.632s, episode steps: 104, steps per second: 165, episode reward: -113.397, mean reward: -1.090 [-100.000, 19.168], mean action: 1.423 [0.000, 3.000],  loss: 16.370891, mae: 53.128624, mean_q: 46.890019, mean_eps: 0.882247
  19772/150000: episode: 215, duration: 0.560s, episode steps:  94, steps per second: 168, episode reward: -115.484, mean reward: -1.229 [-100.000,  7.472], mean action: 1.628 [0.000, 3.000],  loss: 12.939429, mae: 52.474789, mean_q: 44.876236, mean_eps: 0.881653
  19890/150000: episode: 216, duration: 0.695s, episode steps: 118, steps per second: 170, episode reward: -68.391, mean reward: -0.580 [-100.000,  9.895], mean action: 1.390 [0.000, 3.000],  loss: 15.597234, mae: 53.357833, mean_q: 46.500874, mean_eps: 0.881017
  19988/150000: episode: 217, duration: 0.595s, episode steps:  98, steps per second: 165, episode reward: -145.707, mean reward: -1.487 [-100.000, 14.256], mean action: 1.541 [0.000, 3.000],  loss: 10.673161, mae: 51.743721, mean_q: 44.487620, mean_eps: 0.880369
  20121/150000: episode: 218, duration: 0.817s, episode steps: 133, steps per second: 163, episode reward: -72.556, mean reward: -0.546 [-100.000, 12.558], mean action: 1.534 [0.000, 3.000],  loss: 12.491507, mae: 53.334608, mean_q: 47.149954, mean_eps: 0.879676
  20204/150000: episode: 219, duration: 0.485s, episode steps:  83, steps per second: 171, episode reward: -110.061, mean reward: -1.326 [-100.000,  7.062], mean action: 1.735 [0.000, 3.000],  loss: 12.512316, mae: 53.876880, mean_q: 47.278445, mean_eps: 0.879028
  20330/150000: episode: 220, duration: 0.729s, episode steps: 126, steps per second: 173, episode reward: -86.789, mean reward: -0.689 [-100.000, 13.694], mean action: 1.460 [0.000, 3.000],  loss: 8.573779, mae: 52.845448, mean_q: 45.323430, mean_eps: 0.878401
  20408/150000: episode: 221, duration: 0.455s, episode steps:  78, steps per second: 171, episode reward: -108.287, mean reward: -1.388 [-100.000, 20.002], mean action: 1.513 [0.000, 3.000],  loss: 14.212408, mae: 53.877324, mean_q: 45.484555, mean_eps: 0.877789
  20516/150000: episode: 222, duration: 0.772s, episode steps: 108, steps per second: 140, episode reward: -204.483, mean reward: -1.893 [-100.000, 12.753], mean action: 1.657 [0.000, 3.000],  loss: 11.085775, mae: 53.850789, mean_q: 47.272985, mean_eps: 0.877231
  20638/150000: episode: 223, duration: 0.865s, episode steps: 122, steps per second: 141, episode reward: -130.389, mean reward: -1.069 [-100.000,  8.643], mean action: 1.533 [0.000, 3.000],  loss: 12.761265, mae: 52.815737, mean_q: 46.617812, mean_eps: 0.876541
  20763/150000: episode: 224, duration: 0.762s, episode steps: 125, steps per second: 164, episode reward: -109.832, mean reward: -0.879 [-100.000, 29.780], mean action: 1.440 [0.000, 3.000],  loss: 18.742692, mae: 52.597965, mean_q: 44.463447, mean_eps: 0.875800
  20890/150000: episode: 225, duration: 0.741s, episode steps: 127, steps per second: 171, episode reward: -90.464, mean reward: -0.712 [-100.000,  8.225], mean action: 1.520 [0.000, 3.000],  loss: 9.977176, mae: 52.691172, mean_q: 45.081026, mean_eps: 0.875044
  20973/150000: episode: 226, duration: 0.459s, episode steps:  83, steps per second: 181, episode reward: -122.992, mean reward: -1.482 [-100.000,  5.932], mean action: 1.602 [0.000, 3.000],  loss: 7.874280, mae: 52.730912, mean_q: 47.225900, mean_eps: 0.874414
  21128/150000: episode: 227, duration: 0.923s, episode steps: 155, steps per second: 168, episode reward: -58.926, mean reward: -0.380 [-100.000, 76.011], mean action: 1.561 [0.000, 3.000],  loss: 8.385227, mae: 53.220096, mean_q: 46.452199, mean_eps: 0.873700
  21217/150000: episode: 228, duration: 0.524s, episode steps:  89, steps per second: 170, episode reward: -87.532, mean reward: -0.984 [-100.000, 13.618], mean action: 1.562 [0.000, 3.000],  loss: 18.984636, mae: 52.919972, mean_q: 47.358585, mean_eps: 0.872968
  21306/150000: episode: 229, duration: 0.508s, episode steps:  89, steps per second: 175, episode reward: -112.493, mean reward: -1.264 [-100.000,  9.929], mean action: 1.315 [0.000, 3.000],  loss: 11.851816, mae: 53.182036, mean_q: 45.486254, mean_eps: 0.872434
  21375/150000: episode: 230, duration: 0.380s, episode steps:  69, steps per second: 182, episode reward: -89.038, mean reward: -1.290 [-100.000, 10.464], mean action: 1.507 [0.000, 3.000],  loss: 9.858700, mae: 52.741865, mean_q: 45.443952, mean_eps: 0.871960
  21485/150000: episode: 231, duration: 0.633s, episode steps: 110, steps per second: 174, episode reward: -132.985, mean reward: -1.209 [-100.000, 11.379], mean action: 1.400 [0.000, 3.000],  loss: 11.318773, mae: 51.982898, mean_q: 44.767804, mean_eps: 0.871423
  21601/150000: episode: 232, duration: 0.697s, episode steps: 116, steps per second: 166, episode reward: -154.113, mean reward: -1.329 [-100.000,  6.502], mean action: 1.638 [0.000, 3.000],  loss: 15.372550, mae: 52.451320, mean_q: 43.206664, mean_eps: 0.870745
  21669/150000: episode: 233, duration: 0.384s, episode steps:  68, steps per second: 177, episode reward: -103.327, mean reward: -1.520 [-100.000,  7.751], mean action: 1.926 [0.000, 3.000],  loss: 13.833448, mae: 52.154260, mean_q: 44.572480, mean_eps: 0.870193
  21801/150000: episode: 234, duration: 0.735s, episode steps: 132, steps per second: 180, episode reward: -89.262, mean reward: -0.676 [-100.000, 18.390], mean action: 1.758 [0.000, 3.000],  loss: 20.864168, mae: 52.985595, mean_q: 46.337223, mean_eps: 0.869593
  21912/150000: episode: 235, duration: 0.704s, episode steps: 111, steps per second: 158, episode reward: -244.578, mean reward: -2.203 [-100.000,  6.874], mean action: 1.532 [0.000, 3.000],  loss: 12.143862, mae: 53.046310, mean_q: 47.528940, mean_eps: 0.868864
  21997/150000: episode: 236, duration: 0.520s, episode steps:  85, steps per second: 164, episode reward: -124.035, mean reward: -1.459 [-100.000, 13.757], mean action: 1.518 [0.000, 3.000],  loss: 14.099460, mae: 52.157940, mean_q: 45.607985, mean_eps: 0.868276
  22080/150000: episode: 237, duration: 0.490s, episode steps:  83, steps per second: 169, episode reward: -107.842, mean reward: -1.299 [-100.000, 21.339], mean action: 1.590 [0.000, 3.000],  loss: 15.510114, mae: 51.649993, mean_q: 44.708799, mean_eps: 0.867772
  22163/150000: episode: 238, duration: 0.654s, episode steps:  83, steps per second: 127, episode reward: -47.610, mean reward: -0.574 [-100.000, 14.848], mean action: 1.458 [0.000, 3.000],  loss: 6.058439, mae: 51.990168, mean_q: 46.941225, mean_eps: 0.867274
  22245/150000: episode: 239, duration: 0.587s, episode steps:  82, steps per second: 140, episode reward: -258.390, mean reward: -3.151 [-100.000, 38.438], mean action: 1.707 [0.000, 3.000],  loss: 13.938129, mae: 51.780152, mean_q: 45.072710, mean_eps: 0.866779
  22358/150000: episode: 240, duration: 0.817s, episode steps: 113, steps per second: 138, episode reward: -119.427, mean reward: -1.057 [-100.000,  9.897], mean action: 1.460 [0.000, 3.000],  loss: 9.487305, mae: 51.666170, mean_q: 47.336260, mean_eps: 0.866194
  22476/150000: episode: 241, duration: 0.848s, episode steps: 118, steps per second: 139, episode reward: -72.532, mean reward: -0.615 [-100.000, 11.779], mean action: 1.407 [0.000, 3.000],  loss: 13.341342, mae: 52.529829, mean_q: 48.025733, mean_eps: 0.865501
  22543/150000: episode: 242, duration: 0.507s, episode steps:  67, steps per second: 132, episode reward: -109.893, mean reward: -1.640 [-100.000,  9.488], mean action: 1.627 [0.000, 3.000],  loss: 6.824269, mae: 52.410347, mean_q: 49.082294, mean_eps: 0.864946
  22653/150000: episode: 243, duration: 0.802s, episode steps: 110, steps per second: 137, episode reward: -51.897, mean reward: -0.472 [-100.000, 13.246], mean action: 1.400 [0.000, 3.000],  loss: 12.514734, mae: 53.476897, mean_q: 48.558878, mean_eps: 0.864415
  22731/150000: episode: 244, duration: 0.474s, episode steps:  78, steps per second: 164, episode reward: -51.678, mean reward: -0.663 [-100.000, 13.524], mean action: 1.744 [0.000, 3.000],  loss: 6.762089, mae: 53.470411, mean_q: 48.134123, mean_eps: 0.863851
  22850/150000: episode: 245, duration: 0.725s, episode steps: 119, steps per second: 164, episode reward: -12.061, mean reward: -0.101 [-100.000, 57.362], mean action: 1.513 [0.000, 3.000],  loss: 12.113544, mae: 52.441856, mean_q: 47.638739, mean_eps: 0.863260
  22938/150000: episode: 246, duration: 0.529s, episode steps:  88, steps per second: 166, episode reward: -121.763, mean reward: -1.384 [-100.000, 11.636], mean action: 1.625 [0.000, 3.000],  loss: 12.731420, mae: 53.264821, mean_q: 49.477501, mean_eps: 0.862639
  23016/150000: episode: 247, duration: 0.451s, episode steps:  78, steps per second: 173, episode reward: -112.884, mean reward: -1.447 [-100.000,  9.696], mean action: 1.833 [0.000, 3.000],  loss: 11.925613, mae: 52.383431, mean_q: 46.260049, mean_eps: 0.862141
  23082/150000: episode: 248, duration: 0.375s, episode steps:  66, steps per second: 176, episode reward: -123.876, mean reward: -1.877 [-100.000, 13.405], mean action: 1.318 [0.000, 3.000],  loss: 14.123170, mae: 51.359267, mean_q: 45.827727, mean_eps: 0.861709
  23154/150000: episode: 249, duration: 0.405s, episode steps:  72, steps per second: 178, episode reward: -103.088, mean reward: -1.432 [-100.000, 12.765], mean action: 1.597 [0.000, 3.000],  loss: 6.606770, mae: 52.732392, mean_q: 47.729180, mean_eps: 0.861295
  23255/150000: episode: 250, duration: 0.610s, episode steps: 101, steps per second: 166, episode reward: -64.812, mean reward: -0.642 [-100.000, 16.106], mean action: 1.574 [0.000, 3.000],  loss: 13.113203, mae: 52.598562, mean_q: 47.534501, mean_eps: 0.860776
  23346/150000: episode: 251, duration: 0.524s, episode steps:  91, steps per second: 174, episode reward: -113.276, mean reward: -1.245 [-100.000, 13.028], mean action: 1.571 [0.000, 3.000],  loss: 13.081766, mae: 52.431707, mean_q: 45.198407, mean_eps: 0.860200
  23421/150000: episode: 252, duration: 0.437s, episode steps:  75, steps per second: 171, episode reward: -113.305, mean reward: -1.511 [-100.000,  7.729], mean action: 1.653 [0.000, 3.000],  loss: 13.359134, mae: 52.221800, mean_q: 47.812165, mean_eps: 0.859702
  23560/150000: episode: 253, duration: 0.798s, episode steps: 139, steps per second: 174, episode reward: -45.075, mean reward: -0.324 [-100.000, 17.610], mean action: 1.504 [0.000, 3.000],  loss: 9.975696, mae: 52.069973, mean_q: 47.157340, mean_eps: 0.859060
  23695/150000: episode: 254, duration: 0.792s, episode steps: 135, steps per second: 170, episode reward: -116.926, mean reward: -0.866 [-100.000, 11.045], mean action: 1.481 [0.000, 3.000],  loss: 12.792690, mae: 52.892753, mean_q: 47.234024, mean_eps: 0.858238
  23786/150000: episode: 255, duration: 0.525s, episode steps:  91, steps per second: 173, episode reward: -139.058, mean reward: -1.528 [-100.000, 10.076], mean action: 1.495 [0.000, 3.000],  loss: 14.503843, mae: 52.411801, mean_q: 47.441821, mean_eps: 0.857560
  23885/150000: episode: 256, duration: 0.557s, episode steps:  99, steps per second: 178, episode reward: -78.658, mean reward: -0.795 [-100.000, 18.133], mean action: 1.465 [0.000, 3.000],  loss: 11.413759, mae: 52.717111, mean_q: 46.725422, mean_eps: 0.856990
  23998/150000: episode: 257, duration: 0.667s, episode steps: 113, steps per second: 169, episode reward: -112.871, mean reward: -0.999 [-100.000, 13.390], mean action: 1.345 [0.000, 3.000],  loss: 11.908607, mae: 52.023318, mean_q: 47.243871, mean_eps: 0.856354
  24129/150000: episode: 258, duration: 0.787s, episode steps: 131, steps per second: 166, episode reward: -107.829, mean reward: -0.823 [-100.000, 39.354], mean action: 1.435 [0.000, 3.000],  loss: 10.077309, mae: 52.623014, mean_q: 48.198223, mean_eps: 0.855622
  24221/150000: episode: 259, duration: 0.556s, episode steps:  92, steps per second: 165, episode reward: -52.433, mean reward: -0.570 [-100.000, 72.920], mean action: 1.772 [0.000, 3.000],  loss: 14.435370, mae: 51.904473, mean_q: 48.025447, mean_eps: 0.854953
  24310/150000: episode: 260, duration: 0.525s, episode steps:  89, steps per second: 170, episode reward: -131.941, mean reward: -1.482 [-100.000, 18.391], mean action: 1.584 [0.000, 3.000],  loss: 9.464661, mae: 52.055819, mean_q: 48.957073, mean_eps: 0.854410
  24398/150000: episode: 261, duration: 0.524s, episode steps:  88, steps per second: 168, episode reward: -79.777, mean reward: -0.907 [-100.000, 23.959], mean action: 1.534 [0.000, 3.000],  loss: 8.565859, mae: 51.715911, mean_q: 46.083593, mean_eps: 0.853879
  24534/150000: episode: 262, duration: 0.867s, episode steps: 136, steps per second: 157, episode reward: -171.044, mean reward: -1.258 [-100.000,  8.456], mean action: 1.456 [0.000, 3.000],  loss: 10.292678, mae: 52.166249, mean_q: 48.366879, mean_eps: 0.853207
  24619/150000: episode: 263, duration: 0.494s, episode steps:  85, steps per second: 172, episode reward: -50.262, mean reward: -0.591 [-100.000,  6.725], mean action: 1.471 [0.000, 3.000],  loss: 12.829928, mae: 51.210509, mean_q: 47.118383, mean_eps: 0.852544
  24741/150000: episode: 264, duration: 1.025s, episode steps: 122, steps per second: 119, episode reward: -76.127, mean reward: -0.624 [-100.000,  6.845], mean action: 1.631 [0.000, 3.000],  loss: 8.775181, mae: 52.392926, mean_q: 49.203450, mean_eps: 0.851923
  24825/150000: episode: 265, duration: 0.712s, episode steps:  84, steps per second: 118, episode reward: -140.502, mean reward: -1.673 [-100.000, 16.983], mean action: 1.464 [0.000, 3.000],  loss: 8.561223, mae: 53.412561, mean_q: 49.299218, mean_eps: 0.851305
  24987/150000: episode: 266, duration: 1.303s, episode steps: 162, steps per second: 124, episode reward: -190.015, mean reward: -1.173 [-100.000, 22.639], mean action: 1.605 [0.000, 3.000],  loss: 14.184737, mae: 52.857829, mean_q: 48.564601, mean_eps: 0.850567
  25097/150000: episode: 267, duration: 0.858s, episode steps: 110, steps per second: 128, episode reward: -149.436, mean reward: -1.359 [-100.000,  2.727], mean action: 1.509 [0.000, 3.000],  loss: 15.047592, mae: 53.101588, mean_q: 49.763914, mean_eps: 0.849751
  25188/150000: episode: 268, duration: 0.698s, episode steps:  91, steps per second: 130, episode reward: -146.642, mean reward: -1.611 [-100.000, 24.694], mean action: 1.670 [0.000, 3.000],  loss: 7.439643, mae: 51.624461, mean_q: 48.477011, mean_eps: 0.849148
  25264/150000: episode: 269, duration: 0.630s, episode steps:  76, steps per second: 121, episode reward: -98.021, mean reward: -1.290 [-100.000, 10.876], mean action: 1.671 [0.000, 3.000],  loss: 13.498318, mae: 52.209073, mean_q: 49.200765, mean_eps: 0.848647
  25357/150000: episode: 270, duration: 0.689s, episode steps:  93, steps per second: 135, episode reward: -142.547, mean reward: -1.533 [-100.000,  9.975], mean action: 1.645 [0.000, 3.000],  loss: 12.515469, mae: 52.226548, mean_q: 48.906466, mean_eps: 0.848140
  25474/150000: episode: 271, duration: 0.949s, episode steps: 117, steps per second: 123, episode reward: -123.573, mean reward: -1.056 [-100.000,  6.479], mean action: 1.530 [0.000, 3.000],  loss: 9.840474, mae: 53.052536, mean_q: 50.534613, mean_eps: 0.847510
  25559/150000: episode: 272, duration: 0.625s, episode steps:  85, steps per second: 136, episode reward: -113.976, mean reward: -1.341 [-100.000,  9.290], mean action: 1.682 [0.000, 3.000],  loss: 11.274478, mae: 51.271009, mean_q: 45.974590, mean_eps: 0.846904
  25647/150000: episode: 273, duration: 0.622s, episode steps:  88, steps per second: 141, episode reward: -93.457, mean reward: -1.062 [-100.000, 19.862], mean action: 1.580 [0.000, 3.000],  loss: 8.388111, mae: 51.735341, mean_q: 49.366413, mean_eps: 0.846385
  25733/150000: episode: 274, duration: 0.684s, episode steps:  86, steps per second: 126, episode reward: -47.192, mean reward: -0.549 [-100.000, 19.938], mean action: 1.430 [0.000, 3.000],  loss: 11.134802, mae: 52.902304, mean_q: 50.247237, mean_eps: 0.845863
  25850/150000: episode: 275, duration: 1.084s, episode steps: 117, steps per second: 108, episode reward: -117.072, mean reward: -1.001 [-100.000, 17.894], mean action: 1.487 [0.000, 3.000],  loss: 7.213202, mae: 51.104921, mean_q: 46.888439, mean_eps: 0.845254
  25921/150000: episode: 276, duration: 0.513s, episode steps:  71, steps per second: 138, episode reward: -56.766, mean reward: -0.800 [-100.000,  7.752], mean action: 1.493 [0.000, 3.000],  loss: 11.221366, mae: 51.986229, mean_q: 49.702737, mean_eps: 0.844690
  26039/150000: episode: 277, duration: 0.814s, episode steps: 118, steps per second: 145, episode reward: -205.275, mean reward: -1.740 [-100.000,  1.420], mean action: 1.593 [0.000, 3.000],  loss: 7.916125, mae: 51.058520, mean_q: 48.481119, mean_eps: 0.844123
  26145/150000: episode: 278, duration: 0.815s, episode steps: 106, steps per second: 130, episode reward: -262.420, mean reward: -2.476 [-100.000, 49.151], mean action: 1.632 [0.000, 3.000],  loss: 13.642592, mae: 51.223074, mean_q: 48.020007, mean_eps: 0.843451
  26220/150000: episode: 279, duration: 0.577s, episode steps:  75, steps per second: 130, episode reward: -52.196, mean reward: -0.696 [-100.000, 12.328], mean action: 1.747 [0.000, 3.000],  loss: 8.377763, mae: 52.093594, mean_q: 48.753868, mean_eps: 0.842908
  26286/150000: episode: 280, duration: 0.484s, episode steps:  66, steps per second: 136, episode reward: -103.822, mean reward: -1.573 [-100.000,  7.471], mean action: 1.576 [0.000, 3.000],  loss: 12.945058, mae: 51.483266, mean_q: 49.712084, mean_eps: 0.842485
  26386/150000: episode: 281, duration: 0.806s, episode steps: 100, steps per second: 124, episode reward: -83.760, mean reward: -0.838 [-100.000, 19.487], mean action: 1.550 [0.000, 3.000],  loss: 9.247818, mae: 51.036897, mean_q: 48.476576, mean_eps: 0.841987
  26524/150000: episode: 282, duration: 1.051s, episode steps: 138, steps per second: 131, episode reward: -97.378, mean reward: -0.706 [-100.000, 16.402], mean action: 1.601 [0.000, 3.000],  loss: 9.151148, mae: 51.336090, mean_q: 49.452417, mean_eps: 0.841273
  26646/150000: episode: 283, duration: 0.953s, episode steps: 122, steps per second: 128, episode reward: -138.245, mean reward: -1.133 [-100.000,  6.865], mean action: 1.525 [0.000, 3.000],  loss: 7.525962, mae: 51.383366, mean_q: 48.694184, mean_eps: 0.840493
  26727/150000: episode: 284, duration: 0.640s, episode steps:  81, steps per second: 127, episode reward: -26.222, mean reward: -0.324 [-100.000, 18.951], mean action: 1.543 [0.000, 3.000],  loss: 12.034143, mae: 51.401863, mean_q: 47.249140, mean_eps: 0.839884
  26846/150000: episode: 285, duration: 0.920s, episode steps: 119, steps per second: 129, episode reward: -154.941, mean reward: -1.302 [-100.000, 14.355], mean action: 1.571 [0.000, 3.000],  loss: 8.909180, mae: 50.685547, mean_q: 47.930938, mean_eps: 0.839284
  26952/150000: episode: 286, duration: 0.797s, episode steps: 106, steps per second: 133, episode reward: -255.206, mean reward: -2.408 [-100.000,  8.236], mean action: 1.434 [0.000, 3.000],  loss: 9.529648, mae: 51.017913, mean_q: 46.067598, mean_eps: 0.838609
  27046/150000: episode: 287, duration: 0.635s, episode steps:  94, steps per second: 148, episode reward: -99.373, mean reward: -1.057 [-100.000,  7.735], mean action: 1.660 [0.000, 3.000],  loss: 21.313135, mae: 51.741949, mean_q: 48.062977, mean_eps: 0.838009
  27163/150000: episode: 288, duration: 0.770s, episode steps: 117, steps per second: 152, episode reward: -120.829, mean reward: -1.033 [-100.000,  6.763], mean action: 1.453 [0.000, 3.000],  loss: 8.096825, mae: 50.855965, mean_q: 48.172339, mean_eps: 0.837376
  27226/150000: episode: 289, duration: 0.454s, episode steps:  63, steps per second: 139, episode reward: -207.223, mean reward: -3.289 [-100.000,  7.329], mean action: 1.619 [0.000, 3.000],  loss: 8.589822, mae: 50.677913, mean_q: 45.506375, mean_eps: 0.836836
  27343/150000: episode: 290, duration: 0.807s, episode steps: 117, steps per second: 145, episode reward: -132.818, mean reward: -1.135 [-100.000, 10.478], mean action: 1.556 [0.000, 3.000],  loss: 8.889878, mae: 50.642117, mean_q: 47.371419, mean_eps: 0.836296
  27426/150000: episode: 291, duration: 0.540s, episode steps:  83, steps per second: 154, episode reward: -69.883, mean reward: -0.842 [-100.000, 43.199], mean action: 1.783 [0.000, 3.000],  loss: 11.772820, mae: 50.474710, mean_q: 45.535214, mean_eps: 0.835696
  27525/150000: episode: 292, duration: 0.706s, episode steps:  99, steps per second: 140, episode reward: -129.625, mean reward: -1.309 [-100.000,  8.445], mean action: 1.687 [0.000, 3.000],  loss: 10.694559, mae: 50.792160, mean_q: 46.906008, mean_eps: 0.835150
  27623/150000: episode: 293, duration: 0.673s, episode steps:  98, steps per second: 146, episode reward: -131.830, mean reward: -1.345 [-100.000, 23.501], mean action: 1.571 [0.000, 3.000],  loss: 10.691734, mae: 51.756880, mean_q: 47.746659, mean_eps: 0.834559
  27756/150000: episode: 294, duration: 0.897s, episode steps: 133, steps per second: 148, episode reward: -23.723, mean reward: -0.178 [-100.000, 71.940], mean action: 1.398 [0.000, 3.000],  loss: 11.787915, mae: 51.723144, mean_q: 49.558567, mean_eps: 0.833866
  27903/150000: episode: 295, duration: 1.048s, episode steps: 147, steps per second: 140, episode reward: -181.229, mean reward: -1.233 [-100.000, 11.118], mean action: 1.646 [0.000, 3.000],  loss: 9.092148, mae: 51.101359, mean_q: 46.871690, mean_eps: 0.833026
  28020/150000: episode: 296, duration: 0.765s, episode steps: 117, steps per second: 153, episode reward: -139.510, mean reward: -1.192 [-100.000,  9.086], mean action: 1.632 [0.000, 3.000],  loss: 6.344456, mae: 51.258032, mean_q: 47.817259, mean_eps: 0.832234
  28175/150000: episode: 297, duration: 1.096s, episode steps: 155, steps per second: 141, episode reward: -65.007, mean reward: -0.419 [-100.000, 14.391], mean action: 1.645 [0.000, 3.000],  loss: 14.812478, mae: 50.698061, mean_q: 47.779582, mean_eps: 0.831418
  28270/150000: episode: 298, duration: 0.688s, episode steps:  95, steps per second: 138, episode reward: -130.375, mean reward: -1.372 [-100.000,  7.371], mean action: 1.368 [0.000, 3.000],  loss: 6.060100, mae: 50.382433, mean_q: 47.190271, mean_eps: 0.830668
  28427/150000: episode: 299, duration: 1.199s, episode steps: 157, steps per second: 131, episode reward: -136.845, mean reward: -0.872 [-100.000,  7.654], mean action: 1.580 [0.000, 3.000],  loss: 8.606232, mae: 51.267473, mean_q: 46.738359, mean_eps: 0.829912
  28511/150000: episode: 300, duration: 0.584s, episode steps:  84, steps per second: 144, episode reward: -66.544, mean reward: -0.792 [-100.000, 10.642], mean action: 1.702 [0.000, 3.000],  loss: 5.994987, mae: 51.439035, mean_q: 49.462146, mean_eps: 0.829189
  28620/150000: episode: 301, duration: 0.763s, episode steps: 109, steps per second: 143, episode reward: -49.151, mean reward: -0.451 [-100.000, 72.759], mean action: 1.578 [0.000, 3.000],  loss: 6.469494, mae: 51.344636, mean_q: 49.182977, mean_eps: 0.828610
  28766/150000: episode: 302, duration: 0.988s, episode steps: 146, steps per second: 148, episode reward: -83.992, mean reward: -0.575 [-100.000,  6.318], mean action: 1.466 [0.000, 3.000],  loss: 8.896774, mae: 50.670789, mean_q: 47.374454, mean_eps: 0.827845
  28897/150000: episode: 303, duration: 0.880s, episode steps: 131, steps per second: 149, episode reward: -110.786, mean reward: -0.846 [-100.000, 19.099], mean action: 1.473 [0.000, 3.000],  loss: 7.744829, mae: 50.658195, mean_q: 47.004677, mean_eps: 0.827014
  29023/150000: episode: 304, duration: 0.851s, episode steps: 126, steps per second: 148, episode reward: -71.343, mean reward: -0.566 [-100.000, 10.093], mean action: 1.627 [0.000, 3.000],  loss: 5.349180, mae: 50.271972, mean_q: 45.594757, mean_eps: 0.826243
  29105/150000: episode: 305, duration: 0.790s, episode steps:  82, steps per second: 104, episode reward: -80.719, mean reward: -0.984 [-100.000, 16.436], mean action: 1.354 [0.000, 3.000],  loss: 5.757366, mae: 49.735692, mean_q: 45.889723, mean_eps: 0.825619
  29212/150000: episode: 306, duration: 0.780s, episode steps: 107, steps per second: 137, episode reward: -122.429, mean reward: -1.144 [-100.000,  5.499], mean action: 1.523 [0.000, 3.000],  loss: 9.558097, mae: 50.263209, mean_q: 45.575797, mean_eps: 0.825052
  29344/150000: episode: 307, duration: 0.868s, episode steps: 132, steps per second: 152, episode reward: -98.483, mean reward: -0.746 [-100.000, 52.072], mean action: 1.545 [0.000, 3.000],  loss: 12.477002, mae: 50.737139, mean_q: 47.156943, mean_eps: 0.824335
  29496/150000: episode: 308, duration: 1.029s, episode steps: 152, steps per second: 148, episode reward: -20.805, mean reward: -0.137 [-100.000,  8.875], mean action: 1.605 [0.000, 3.000],  loss: 7.906563, mae: 50.464918, mean_q: 45.948708, mean_eps: 0.823483
  29614/150000: episode: 309, duration: 0.844s, episode steps: 118, steps per second: 140, episode reward: -95.745, mean reward: -0.811 [-100.000, 11.606], mean action: 1.627 [0.000, 3.000],  loss: 14.295133, mae: 51.001952, mean_q: 46.525244, mean_eps: 0.822673
  29716/150000: episode: 310, duration: 0.711s, episode steps: 102, steps per second: 143, episode reward: -74.349, mean reward: -0.729 [-100.000,  8.942], mean action: 1.696 [0.000, 3.000],  loss: 10.434691, mae: 50.121902, mean_q: 45.441525, mean_eps: 0.822013
  29820/150000: episode: 311, duration: 0.692s, episode steps: 104, steps per second: 150, episode reward: -137.516, mean reward: -1.322 [-100.000, 10.229], mean action: 1.500 [0.000, 3.000],  loss: 15.927006, mae: 49.225442, mean_q: 44.276269, mean_eps: 0.821395
  29940/150000: episode: 312, duration: 0.795s, episode steps: 120, steps per second: 151, episode reward: -63.818, mean reward: -0.532 [-100.000, 22.320], mean action: 1.425 [0.000, 3.000],  loss: 11.566661, mae: 49.724340, mean_q: 44.620443, mean_eps: 0.820723
  30012/150000: episode: 313, duration: 0.522s, episode steps:  72, steps per second: 138, episode reward: -61.949, mean reward: -0.860 [-100.000,  7.197], mean action: 1.458 [0.000, 3.000],  loss: 10.839207, mae: 49.648833, mean_q: 44.039006, mean_eps: 0.820147
  30140/150000: episode: 314, duration: 0.914s, episode steps: 128, steps per second: 140, episode reward: -109.704, mean reward: -0.857 [-100.000,  6.660], mean action: 1.383 [0.000, 3.000],  loss: 11.891910, mae: 49.607489, mean_q: 45.017210, mean_eps: 0.819547
  30257/150000: episode: 315, duration: 0.780s, episode steps: 117, steps per second: 150, episode reward: -123.096, mean reward: -1.052 [-100.000,  9.646], mean action: 1.436 [0.000, 3.000],  loss: 11.604490, mae: 48.816707, mean_q: 44.151680, mean_eps: 0.818812
  30365/150000: episode: 316, duration: 0.789s, episode steps: 108, steps per second: 137, episode reward: -146.634, mean reward: -1.358 [-100.000,  5.524], mean action: 1.435 [0.000, 3.000],  loss: 11.576094, mae: 49.294830, mean_q: 45.312609, mean_eps: 0.818137
  30464/150000: episode: 317, duration: 0.695s, episode steps:  99, steps per second: 142, episode reward: -307.914, mean reward: -3.110 [-100.000, 57.986], mean action: 1.606 [0.000, 3.000],  loss: 10.583608, mae: 49.616383, mean_q: 44.029683, mean_eps: 0.817516
  30582/150000: episode: 318, duration: 0.784s, episode steps: 118, steps per second: 151, episode reward: -75.560, mean reward: -0.640 [-100.000,  9.951], mean action: 1.593 [0.000, 3.000],  loss: 11.436044, mae: 49.754238, mean_q: 46.358740, mean_eps: 0.816865
  30684/150000: episode: 319, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -86.101, mean reward: -0.844 [-100.000, 18.945], mean action: 1.402 [0.000, 3.000],  loss: 7.425419, mae: 49.699995, mean_q: 47.099167, mean_eps: 0.816205
  30756/150000: episode: 320, duration: 0.475s, episode steps:  72, steps per second: 151, episode reward: -35.432, mean reward: -0.492 [-100.000, 10.205], mean action: 1.611 [0.000, 3.000],  loss: 7.771745, mae: 49.528338, mean_q: 47.701652, mean_eps: 0.815683
  30908/150000: episode: 321, duration: 1.046s, episode steps: 152, steps per second: 145, episode reward: -98.539, mean reward: -0.648 [-100.000,  8.223], mean action: 1.658 [0.000, 3.000],  loss: 6.446558, mae: 49.789180, mean_q: 46.536130, mean_eps: 0.815011
  31019/150000: episode: 322, duration: 0.759s, episode steps: 111, steps per second: 146, episode reward: -62.806, mean reward: -0.566 [-100.000, 12.147], mean action: 1.505 [0.000, 3.000],  loss: 9.551530, mae: 48.566789, mean_q: 45.208882, mean_eps: 0.814222
  31095/150000: episode: 323, duration: 0.647s, episode steps:  76, steps per second: 118, episode reward: -234.540, mean reward: -3.086 [-100.000,  6.655], mean action: 1.526 [0.000, 3.000],  loss: 6.867916, mae: 49.148589, mean_q: 48.098981, mean_eps: 0.813661
  31222/150000: episode: 324, duration: 1.105s, episode steps: 127, steps per second: 115, episode reward: -106.694, mean reward: -0.840 [-100.000,  8.091], mean action: 1.394 [0.000, 3.000],  loss: 5.949789, mae: 48.926384, mean_q: 45.148367, mean_eps: 0.813052
  31336/150000: episode: 325, duration: 0.893s, episode steps: 114, steps per second: 128, episode reward: -82.475, mean reward: -0.723 [-100.000,  9.976], mean action: 1.640 [0.000, 3.000],  loss: 9.330154, mae: 49.859751, mean_q: 47.506788, mean_eps: 0.812329
  31422/150000: episode: 326, duration: 0.544s, episode steps:  86, steps per second: 158, episode reward: -78.493, mean reward: -0.913 [-100.000, 11.903], mean action: 1.570 [0.000, 3.000],  loss: 4.711963, mae: 49.432524, mean_q: 46.096185, mean_eps: 0.811729
  31520/150000: episode: 327, duration: 0.676s, episode steps:  98, steps per second: 145, episode reward: -137.794, mean reward: -1.406 [-100.000,  8.926], mean action: 1.347 [0.000, 3.000],  loss: 8.380226, mae: 49.410943, mean_q: 46.131586, mean_eps: 0.811177
  31631/150000: episode: 328, duration: 0.702s, episode steps: 111, steps per second: 158, episode reward: -121.345, mean reward: -1.093 [-100.000,  5.476], mean action: 1.523 [0.000, 3.000],  loss: 8.472269, mae: 49.418066, mean_q: 46.436570, mean_eps: 0.810550
  31740/150000: episode: 329, duration: 0.692s, episode steps: 109, steps per second: 158, episode reward: -88.552, mean reward: -0.812 [-100.000, 19.045], mean action: 1.578 [0.000, 3.000],  loss: 9.266568, mae: 50.039789, mean_q: 46.670452, mean_eps: 0.809890
  31863/150000: episode: 330, duration: 0.758s, episode steps: 123, steps per second: 162, episode reward: -142.458, mean reward: -1.158 [-100.000, 35.605], mean action: 1.610 [0.000, 3.000],  loss: 7.767996, mae: 49.082248, mean_q: 45.555028, mean_eps: 0.809194
  31964/150000: episode: 331, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: -61.451, mean reward: -0.608 [-100.000, 17.000], mean action: 1.574 [0.000, 3.000],  loss: 7.558270, mae: 50.058368, mean_q: 47.008664, mean_eps: 0.808522
  32085/150000: episode: 332, duration: 0.697s, episode steps: 121, steps per second: 174, episode reward: -105.255, mean reward: -0.870 [-100.000,  9.436], mean action: 1.322 [0.000, 3.000],  loss: 6.717169, mae: 48.862257, mean_q: 44.697743, mean_eps: 0.807856
  32175/150000: episode: 333, duration: 0.565s, episode steps:  90, steps per second: 159, episode reward: -115.990, mean reward: -1.289 [-100.000,  9.139], mean action: 1.622 [0.000, 3.000],  loss: 6.512143, mae: 49.586298, mean_q: 46.136087, mean_eps: 0.807223
  32293/150000: episode: 334, duration: 0.709s, episode steps: 118, steps per second: 166, episode reward: -126.518, mean reward: -1.072 [-100.000,  8.665], mean action: 1.500 [0.000, 3.000],  loss: 6.193767, mae: 49.367217, mean_q: 44.305198, mean_eps: 0.806599
  32383/150000: episode: 335, duration: 0.525s, episode steps:  90, steps per second: 171, episode reward: -177.040, mean reward: -1.967 [-100.000,  7.768], mean action: 1.722 [0.000, 3.000],  loss: 6.524251, mae: 49.020113, mean_q: 44.365232, mean_eps: 0.805975
  32491/150000: episode: 336, duration: 0.623s, episode steps: 108, steps per second: 173, episode reward: -156.946, mean reward: -1.453 [-100.000,  7.060], mean action: 1.519 [0.000, 3.000],  loss: 7.289308, mae: 49.399762, mean_q: 45.388123, mean_eps: 0.805381
  32630/150000: episode: 337, duration: 0.913s, episode steps: 139, steps per second: 152, episode reward: -239.651, mean reward: -1.724 [-100.000, 61.171], mean action: 1.547 [0.000, 3.000],  loss: 7.247961, mae: 49.886988, mean_q: 45.288936, mean_eps: 0.804640
  32707/150000: episode: 338, duration: 0.550s, episode steps:  77, steps per second: 140, episode reward: -157.331, mean reward: -2.043 [-100.000,  7.207], mean action: 1.532 [0.000, 3.000],  loss: 11.498094, mae: 49.896474, mean_q: 45.741823, mean_eps: 0.803992
  32836/150000: episode: 339, duration: 0.924s, episode steps: 129, steps per second: 140, episode reward: -97.094, mean reward: -0.753 [-100.000, 65.509], mean action: 1.705 [0.000, 3.000],  loss: 8.149591, mae: 50.517483, mean_q: 45.732168, mean_eps: 0.803374
  32957/150000: episode: 340, duration: 0.847s, episode steps: 121, steps per second: 143, episode reward: -132.204, mean reward: -1.093 [-100.000,  6.363], mean action: 1.595 [0.000, 3.000],  loss: 9.529810, mae: 49.534237, mean_q: 45.284776, mean_eps: 0.802624
  33074/150000: episode: 341, duration: 0.864s, episode steps: 117, steps per second: 135, episode reward: -93.411, mean reward: -0.798 [-100.000, 16.575], mean action: 1.547 [0.000, 3.000],  loss: 12.042091, mae: 50.502031, mean_q: 47.918250, mean_eps: 0.801910
  33191/150000: episode: 342, duration: 0.867s, episode steps: 117, steps per second: 135, episode reward: -84.982, mean reward: -0.726 [-100.000, 14.521], mean action: 1.650 [0.000, 3.000],  loss: 14.125363, mae: 50.016829, mean_q: 47.354817, mean_eps: 0.801208
  33325/150000: episode: 343, duration: 1.305s, episode steps: 134, steps per second: 103, episode reward: -90.577, mean reward: -0.676 [-100.000, 15.994], mean action: 1.642 [0.000, 3.000],  loss: 8.724459, mae: 50.972109, mean_q: 49.483554, mean_eps: 0.800455
  33406/150000: episode: 344, duration: 0.737s, episode steps:  81, steps per second: 110, episode reward: -148.844, mean reward: -1.838 [-100.000, 13.359], mean action: 1.728 [0.000, 3.000],  loss: 7.200644, mae: 49.109718, mean_q: 45.268283, mean_eps: 0.799810
  33531/150000: episode: 345, duration: 1.530s, episode steps: 125, steps per second:  82, episode reward: -55.913, mean reward: -0.447 [-100.000, 14.828], mean action: 1.568 [0.000, 3.000],  loss: 7.283266, mae: 49.827666, mean_q: 46.418397, mean_eps: 0.799192
  33616/150000: episode: 346, duration: 0.878s, episode steps:  85, steps per second:  97, episode reward: -64.104, mean reward: -0.754 [-100.000, 14.930], mean action: 1.647 [0.000, 3.000],  loss: 17.368045, mae: 50.133889, mean_q: 47.045446, mean_eps: 0.798562
  34616/150000: episode: 347, duration: 8.759s, episode steps: 1000, steps per second: 114, episode reward: -34.482, mean reward: -0.034 [-23.942, 23.455], mean action: 1.622 [0.000, 3.000],  loss: 9.840508, mae: 50.245636, mean_q: 46.930036, mean_eps: 0.795307
  34742/150000: episode: 348, duration: 0.936s, episode steps: 126, steps per second: 135, episode reward: -64.784, mean reward: -0.514 [-100.000, 14.132], mean action: 1.571 [0.000, 3.000],  loss: 10.796255, mae: 49.405851, mean_q: 44.366461, mean_eps: 0.791929
  34825/150000: episode: 349, duration: 0.587s, episode steps:  83, steps per second: 141, episode reward: -102.394, mean reward: -1.234 [-100.000, 10.283], mean action: 1.434 [0.000, 3.000],  loss: 8.788076, mae: 50.700615, mean_q: 46.647503, mean_eps: 0.791302
  34923/150000: episode: 350, duration: 0.745s, episode steps:  98, steps per second: 132, episode reward: -116.822, mean reward: -1.192 [-100.000, 10.147], mean action: 1.602 [0.000, 3.000],  loss: 12.851336, mae: 50.785982, mean_q: 46.174861, mean_eps: 0.790759
  35027/150000: episode: 351, duration: 0.857s, episode steps: 104, steps per second: 121, episode reward: -68.546, mean reward: -0.659 [-100.000, 13.786], mean action: 1.519 [0.000, 3.000],  loss: 9.360244, mae: 50.210694, mean_q: 45.006135, mean_eps: 0.790153
  35149/150000: episode: 352, duration: 1.056s, episode steps: 122, steps per second: 116, episode reward: -74.359, mean reward: -0.609 [-100.000, 13.784], mean action: 1.615 [0.000, 3.000],  loss: 12.483017, mae: 50.370919, mean_q: 46.267567, mean_eps: 0.789475
  35260/150000: episode: 353, duration: 0.903s, episode steps: 111, steps per second: 123, episode reward: -301.053, mean reward: -2.712 [-100.000,  8.755], mean action: 1.559 [0.000, 3.000],  loss: 10.233326, mae: 50.518617, mean_q: 47.739085, mean_eps: 0.788776
  35322/150000: episode: 354, duration: 0.443s, episode steps:  62, steps per second: 140, episode reward: -57.371, mean reward: -0.925 [-100.000, 22.309], mean action: 1.403 [0.000, 3.000],  loss: 6.615136, mae: 50.767567, mean_q: 47.335508, mean_eps: 0.788257
  35437/150000: episode: 355, duration: 0.974s, episode steps: 115, steps per second: 118, episode reward: -139.543, mean reward: -1.213 [-100.000, 12.376], mean action: 1.617 [0.000, 3.000],  loss: 8.012348, mae: 49.515766, mean_q: 46.022577, mean_eps: 0.787726
  35513/150000: episode: 356, duration: 0.588s, episode steps:  76, steps per second: 129, episode reward: -86.435, mean reward: -1.137 [-100.000, 17.849], mean action: 1.763 [0.000, 3.000],  loss: 5.446749, mae: 50.383026, mean_q: 46.052745, mean_eps: 0.787153
  35581/150000: episode: 357, duration: 0.605s, episode steps:  68, steps per second: 112, episode reward: -87.575, mean reward: -1.288 [-100.000,  9.391], mean action: 1.397 [0.000, 3.000],  loss: 6.497975, mae: 50.599397, mean_q: 47.397441, mean_eps: 0.786721
  35645/150000: episode: 358, duration: 0.509s, episode steps:  64, steps per second: 126, episode reward: -128.258, mean reward: -2.004 [-100.000,  7.129], mean action: 1.469 [0.000, 3.000],  loss: 5.209081, mae: 49.625232, mean_q: 46.078148, mean_eps: 0.786325
  35724/150000: episode: 359, duration: 0.592s, episode steps:  79, steps per second: 133, episode reward: -89.972, mean reward: -1.139 [-100.000,  7.360], mean action: 1.481 [0.000, 3.000],  loss: 7.851490, mae: 50.661313, mean_q: 47.037241, mean_eps: 0.785896
  35829/150000: episode: 360, duration: 0.757s, episode steps: 105, steps per second: 139, episode reward: -74.614, mean reward: -0.711 [-100.000, 13.504], mean action: 1.657 [0.000, 3.000],  loss: 9.955129, mae: 50.632838, mean_q: 44.818464, mean_eps: 0.785344
  35903/150000: episode: 361, duration: 0.561s, episode steps:  74, steps per second: 132, episode reward: -35.508, mean reward: -0.480 [-100.000, 13.707], mean action: 1.635 [0.000, 3.000],  loss: 10.670949, mae: 51.115689, mean_q: 46.896146, mean_eps: 0.784807
  35978/150000: episode: 362, duration: 0.631s, episode steps:  75, steps per second: 119, episode reward: -82.931, mean reward: -1.106 [-100.000, 24.412], mean action: 1.493 [0.000, 3.000],  loss: 11.741822, mae: 51.202728, mean_q: 46.577118, mean_eps: 0.784360
  36075/150000: episode: 363, duration: 0.755s, episode steps:  97, steps per second: 128, episode reward: -63.772, mean reward: -0.657 [-100.000, 15.119], mean action: 1.670 [0.000, 3.000],  loss: 10.026665, mae: 51.624689, mean_q: 47.103525, mean_eps: 0.783844
  36156/150000: episode: 364, duration: 0.640s, episode steps:  81, steps per second: 127, episode reward: -150.317, mean reward: -1.856 [-100.000, 32.129], mean action: 1.506 [0.000, 3.000],  loss: 10.713328, mae: 51.383806, mean_q: 48.884691, mean_eps: 0.783310
  36268/150000: episode: 365, duration: 0.868s, episode steps: 112, steps per second: 129, episode reward: -57.760, mean reward: -0.516 [-100.000, 24.679], mean action: 1.455 [0.000, 3.000],  loss: 9.520432, mae: 50.972088, mean_q: 47.016495, mean_eps: 0.782731
  36350/150000: episode: 366, duration: 0.612s, episode steps:  82, steps per second: 134, episode reward: -77.901, mean reward: -0.950 [-100.000, 12.233], mean action: 1.549 [0.000, 3.000],  loss: 8.044988, mae: 50.753799, mean_q: 46.995878, mean_eps: 0.782149
  36414/150000: episode: 367, duration: 0.493s, episode steps:  64, steps per second: 130, episode reward: -81.051, mean reward: -1.266 [-100.000, 15.229], mean action: 1.547 [0.000, 3.000],  loss: 8.363441, mae: 51.161476, mean_q: 46.579480, mean_eps: 0.781711
  36552/150000: episode: 368, duration: 1.104s, episode steps: 138, steps per second: 125, episode reward: -17.623, mean reward: -0.128 [-100.000, 15.496], mean action: 1.594 [0.000, 3.000],  loss: 9.532180, mae: 50.880479, mean_q: 45.379426, mean_eps: 0.781105
  36658/150000: episode: 369, duration: 0.846s, episode steps: 106, steps per second: 125, episode reward: -177.150, mean reward: -1.671 [-100.000,  7.253], mean action: 1.670 [0.000, 3.000],  loss: 9.547817, mae: 51.071476, mean_q: 46.747484, mean_eps: 0.780373
  36775/150000: episode: 370, duration: 0.989s, episode steps: 117, steps per second: 118, episode reward: -75.995, mean reward: -0.650 [-100.000, 17.042], mean action: 1.487 [0.000, 3.000],  loss: 9.988884, mae: 50.853577, mean_q: 46.131655, mean_eps: 0.779704
  36940/150000: episode: 371, duration: 1.244s, episode steps: 165, steps per second: 133, episode reward: -18.491, mean reward: -0.112 [-100.000, 93.401], mean action: 1.527 [0.000, 3.000],  loss: 9.714427, mae: 49.872018, mean_q: 44.766194, mean_eps: 0.778858
  37042/150000: episode: 372, duration: 0.677s, episode steps: 102, steps per second: 151, episode reward: -64.172, mean reward: -0.629 [-100.000, 17.659], mean action: 1.529 [0.000, 3.000],  loss: 9.108305, mae: 50.091363, mean_q: 45.164572, mean_eps: 0.778057
  38042/150000: episode: 373, duration: 8.703s, episode steps: 1000, steps per second: 115, episode reward: 18.169, mean reward:  0.018 [-24.530, 46.782], mean action: 1.496 [0.000, 3.000],  loss: 8.282497, mae: 50.015944, mean_q: 43.773385, mean_eps: 0.774751
  38108/150000: episode: 374, duration: 0.529s, episode steps:  66, steps per second: 125, episode reward: -86.143, mean reward: -1.305 [-100.000,  7.915], mean action: 1.636 [0.000, 3.000],  loss: 14.244997, mae: 49.747388, mean_q: 41.155291, mean_eps: 0.771553
  38232/150000: episode: 375, duration: 0.883s, episode steps: 124, steps per second: 140, episode reward: -46.365, mean reward: -0.374 [-100.000, 11.879], mean action: 1.710 [0.000, 3.000],  loss: 8.428680, mae: 49.317510, mean_q: 43.826672, mean_eps: 0.770983
  38344/150000: episode: 376, duration: 0.937s, episode steps: 112, steps per second: 120, episode reward: -392.892, mean reward: -3.508 [-100.000, 60.069], mean action: 1.607 [0.000, 3.000],  loss: 10.729162, mae: 48.679272, mean_q: 41.084901, mean_eps: 0.770275
  38439/150000: episode: 377, duration: 0.764s, episode steps:  95, steps per second: 124, episode reward: -112.708, mean reward: -1.186 [-100.000, 15.289], mean action: 1.526 [0.000, 3.000],  loss: 10.138280, mae: 48.098035, mean_q: 41.413868, mean_eps: 0.769654
  38542/150000: episode: 378, duration: 0.792s, episode steps: 103, steps per second: 130, episode reward: -92.795, mean reward: -0.901 [-100.000, 15.803], mean action: 1.641 [0.000, 3.000],  loss: 9.077809, mae: 48.697908, mean_q: 41.071342, mean_eps: 0.769060
  38633/150000: episode: 379, duration: 0.691s, episode steps:  91, steps per second: 132, episode reward: -90.166, mean reward: -0.991 [-100.000,  9.022], mean action: 1.758 [0.000, 3.000],  loss: 6.654843, mae: 48.762110, mean_q: 41.901786, mean_eps: 0.768478
  38694/150000: episode: 380, duration: 0.484s, episode steps:  61, steps per second: 126, episode reward: -107.796, mean reward: -1.767 [-100.000,  9.114], mean action: 1.918 [0.000, 3.000],  loss: 7.038281, mae: 50.016568, mean_q: 42.516150, mean_eps: 0.768022
  38832/150000: episode: 381, duration: 1.063s, episode steps: 138, steps per second: 130, episode reward: -116.234, mean reward: -0.842 [-100.000, 22.360], mean action: 1.391 [0.000, 3.000],  loss: 7.915037, mae: 48.747391, mean_q: 40.936081, mean_eps: 0.767425
  38950/150000: episode: 382, duration: 0.910s, episode steps: 118, steps per second: 130, episode reward: -128.685, mean reward: -1.091 [-100.000,  5.505], mean action: 1.458 [0.000, 3.000],  loss: 9.996209, mae: 48.251437, mean_q: 42.447656, mean_eps: 0.766657
  39007/150000: episode: 383, duration: 0.453s, episode steps:  57, steps per second: 126, episode reward: -140.549, mean reward: -2.466 [-100.000,  4.191], mean action: 1.491 [0.000, 3.000],  loss: 6.933803, mae: 49.077589, mean_q: 43.665454, mean_eps: 0.766132
  39099/150000: episode: 384, duration: 0.674s, episode steps:  92, steps per second: 137, episode reward: -149.862, mean reward: -1.629 [-100.000,  7.155], mean action: 1.663 [0.000, 3.000],  loss: 6.950227, mae: 47.699286, mean_q: 40.966457, mean_eps: 0.765685
  39180/150000: episode: 385, duration: 0.639s, episode steps:  81, steps per second: 127, episode reward: -62.256, mean reward: -0.769 [-100.000,  6.753], mean action: 1.691 [0.000, 3.000],  loss: 6.364214, mae: 48.865299, mean_q: 42.913332, mean_eps: 0.765166
  39268/150000: episode: 386, duration: 0.731s, episode steps:  88, steps per second: 120, episode reward: -52.213, mean reward: -0.593 [-100.000, 15.425], mean action: 1.705 [0.000, 3.000],  loss: 8.093151, mae: 47.968690, mean_q: 40.710604, mean_eps: 0.764659
  39387/150000: episode: 387, duration: 0.918s, episode steps: 119, steps per second: 130, episode reward: -53.857, mean reward: -0.453 [-100.000, 11.095], mean action: 1.748 [0.000, 3.000],  loss: 8.244057, mae: 47.360662, mean_q: 40.547326, mean_eps: 0.764038
  39459/150000: episode: 388, duration: 0.629s, episode steps:  72, steps per second: 114, episode reward: -103.515, mean reward: -1.438 [-100.000,  5.635], mean action: 1.819 [0.000, 3.000],  loss: 9.126874, mae: 48.851916, mean_q: 43.762508, mean_eps: 0.763465
  39555/150000: episode: 389, duration: 0.803s, episode steps:  96, steps per second: 119, episode reward: -116.759, mean reward: -1.216 [-100.000,  5.925], mean action: 1.583 [0.000, 3.000],  loss: 9.533617, mae: 47.788211, mean_q: 40.932173, mean_eps: 0.762961
  39695/150000: episode: 390, duration: 1.296s, episode steps: 140, steps per second: 108, episode reward: -241.268, mean reward: -1.723 [-100.000, 17.460], mean action: 1.693 [0.000, 3.000],  loss: 8.977682, mae: 48.233435, mean_q: 42.462532, mean_eps: 0.762253
  39795/150000: episode: 391, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: -124.194, mean reward: -1.242 [-100.000,  5.836], mean action: 1.530 [0.000, 3.000],  loss: 10.160172, mae: 47.800474, mean_q: 40.695152, mean_eps: 0.761533
  39881/150000: episode: 392, duration: 0.738s, episode steps:  86, steps per second: 117, episode reward: -146.512, mean reward: -1.704 [-100.000, 16.139], mean action: 1.628 [0.000, 3.000],  loss: 8.966727, mae: 48.385160, mean_q: 42.020060, mean_eps: 0.760975
  40003/150000: episode: 393, duration: 1.068s, episode steps: 122, steps per second: 114, episode reward: -113.945, mean reward: -0.934 [-100.000, 15.787], mean action: 1.557 [0.000, 3.000],  loss: 11.900111, mae: 48.480475, mean_q: 41.799119, mean_eps: 0.760351
  40138/150000: episode: 394, duration: 1.081s, episode steps: 135, steps per second: 125, episode reward: -184.427, mean reward: -1.366 [-100.000, 18.204], mean action: 1.504 [0.000, 3.000],  loss: 9.709537, mae: 47.831284, mean_q: 40.165871, mean_eps: 0.759580
  40234/150000: episode: 395, duration: 0.851s, episode steps:  96, steps per second: 113, episode reward: -55.237, mean reward: -0.575 [-100.000, 35.463], mean action: 1.625 [0.000, 3.000],  loss: 9.094751, mae: 48.400579, mean_q: 42.094197, mean_eps: 0.758887
  40349/150000: episode: 396, duration: 1.074s, episode steps: 115, steps per second: 107, episode reward: -201.786, mean reward: -1.755 [-100.000, 21.179], mean action: 1.391 [0.000, 3.000],  loss: 8.342705, mae: 47.534582, mean_q: 40.745814, mean_eps: 0.758254
  40421/150000: episode: 397, duration: 0.629s, episode steps:  72, steps per second: 114, episode reward: -1.484, mean reward: -0.021 [-100.000, 22.540], mean action: 1.583 [0.000, 3.000],  loss: 9.564585, mae: 47.121838, mean_q: 39.631470, mean_eps: 0.757693
  40497/150000: episode: 398, duration: 0.620s, episode steps:  76, steps per second: 123, episode reward: -77.162, mean reward: -1.015 [-100.000, 18.705], mean action: 1.579 [0.000, 3.000],  loss: 8.719891, mae: 48.264237, mean_q: 41.972607, mean_eps: 0.757249
  40585/150000: episode: 399, duration: 0.715s, episode steps:  88, steps per second: 123, episode reward: -54.529, mean reward: -0.620 [-100.000, 17.711], mean action: 1.477 [0.000, 3.000],  loss: 11.610291, mae: 48.268900, mean_q: 40.999015, mean_eps: 0.756757
  40687/150000: episode: 400, duration: 0.822s, episode steps: 102, steps per second: 124, episode reward: -267.109, mean reward: -2.619 [-100.000, 16.212], mean action: 1.696 [0.000, 3.000],  loss: 10.890014, mae: 47.575000, mean_q: 39.050562, mean_eps: 0.756187
  40787/150000: episode: 401, duration: 0.823s, episode steps: 100, steps per second: 122, episode reward: -98.501, mean reward: -0.985 [-100.000,  8.080], mean action: 1.570 [0.000, 3.000],  loss: 8.496826, mae: 47.867400, mean_q: 38.186201, mean_eps: 0.755581
  40910/150000: episode: 402, duration: 0.973s, episode steps: 123, steps per second: 126, episode reward: -68.387, mean reward: -0.556 [-100.000, 12.102], mean action: 1.650 [0.000, 3.000],  loss: 9.656393, mae: 47.972847, mean_q: 37.993991, mean_eps: 0.754912
  41007/150000: episode: 403, duration: 1.004s, episode steps:  97, steps per second:  97, episode reward: -188.153, mean reward: -1.940 [-100.000,  2.057], mean action: 1.546 [0.000, 3.000],  loss: 8.528371, mae: 46.477520, mean_q: 37.005496, mean_eps: 0.754252
  41111/150000: episode: 404, duration: 1.013s, episode steps: 104, steps per second: 103, episode reward: -63.062, mean reward: -0.606 [-100.000, 16.710], mean action: 1.615 [0.000, 3.000],  loss: 7.431678, mae: 47.183119, mean_q: 37.963262, mean_eps: 0.753649
  41226/150000: episode: 405, duration: 0.980s, episode steps: 115, steps per second: 117, episode reward: -94.194, mean reward: -0.819 [-100.000, 12.364], mean action: 1.548 [0.000, 3.000],  loss: 8.621454, mae: 47.877915, mean_q: 38.445622, mean_eps: 0.752992
  41312/150000: episode: 406, duration: 0.707s, episode steps:  86, steps per second: 122, episode reward: -52.339, mean reward: -0.609 [-100.000, 14.893], mean action: 1.500 [0.000, 3.000],  loss: 9.232517, mae: 48.687344, mean_q: 40.418860, mean_eps: 0.752389
  41445/150000: episode: 407, duration: 1.005s, episode steps: 133, steps per second: 132, episode reward: -57.599, mean reward: -0.433 [-100.000, 10.725], mean action: 1.466 [0.000, 3.000],  loss: 8.509387, mae: 48.051712, mean_q: 39.326004, mean_eps: 0.751732
  41575/150000: episode: 408, duration: 0.990s, episode steps: 130, steps per second: 131, episode reward: -137.100, mean reward: -1.055 [-100.000,  5.794], mean action: 1.500 [0.000, 3.000],  loss: 9.094780, mae: 48.307474, mean_q: 39.410139, mean_eps: 0.750943
  41635/150000: episode: 409, duration: 0.519s, episode steps:  60, steps per second: 116, episode reward: -64.396, mean reward: -1.073 [-100.000,  5.125], mean action: 1.717 [0.000, 3.000],  loss: 8.583274, mae: 48.271291, mean_q: 38.004712, mean_eps: 0.750373
  41734/150000: episode: 410, duration: 0.821s, episode steps:  99, steps per second: 121, episode reward: -96.577, mean reward: -0.976 [-100.000, 18.873], mean action: 1.677 [0.000, 3.000],  loss: 11.655096, mae: 48.190062, mean_q: 41.396094, mean_eps: 0.749896
  41838/150000: episode: 411, duration: 0.955s, episode steps: 104, steps per second: 109, episode reward: -61.655, mean reward: -0.593 [-100.000, 12.100], mean action: 1.606 [0.000, 3.000],  loss: 11.550110, mae: 48.205659, mean_q: 40.855548, mean_eps: 0.749287
  41940/150000: episode: 412, duration: 1.019s, episode steps: 102, steps per second: 100, episode reward: -75.486, mean reward: -0.740 [-100.000, 11.485], mean action: 1.706 [0.000, 3.000],  loss: 10.292627, mae: 47.890053, mean_q: 40.819490, mean_eps: 0.748669
  42062/150000: episode: 413, duration: 1.050s, episode steps: 122, steps per second: 116, episode reward: -86.863, mean reward: -0.712 [-100.000, 12.099], mean action: 1.393 [0.000, 3.000],  loss: 10.677412, mae: 48.410028, mean_q: 41.056708, mean_eps: 0.747997
  42151/150000: episode: 414, duration: 0.655s, episode steps:  89, steps per second: 136, episode reward: -133.940, mean reward: -1.505 [-100.000,  7.289], mean action: 1.697 [0.000, 3.000],  loss: 6.227380, mae: 47.253649, mean_q: 38.239309, mean_eps: 0.747364
  42245/150000: episode: 415, duration: 0.731s, episode steps:  94, steps per second: 129, episode reward: -110.269, mean reward: -1.173 [-100.000,  5.365], mean action: 1.489 [0.000, 3.000],  loss: 9.787567, mae: 47.536261, mean_q: 41.703177, mean_eps: 0.746815
  42357/150000: episode: 416, duration: 1.113s, episode steps: 112, steps per second: 101, episode reward: -112.693, mean reward: -1.006 [-100.000,  5.870], mean action: 1.634 [0.000, 3.000],  loss: 9.942591, mae: 47.938390, mean_q: 41.461569, mean_eps: 0.746197
  42473/150000: episode: 417, duration: 0.833s, episode steps: 116, steps per second: 139, episode reward: 15.062, mean reward:  0.130 [-100.000, 76.181], mean action: 1.819 [0.000, 3.000],  loss: 11.052828, mae: 47.996610, mean_q: 41.396469, mean_eps: 0.745513
  42589/150000: episode: 418, duration: 0.813s, episode steps: 116, steps per second: 143, episode reward: -92.906, mean reward: -0.801 [-100.000,  8.512], mean action: 1.578 [0.000, 3.000],  loss: 8.505846, mae: 47.754336, mean_q: 42.185616, mean_eps: 0.744817
  42662/150000: episode: 419, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -77.193, mean reward: -1.057 [-100.000, 15.610], mean action: 1.671 [0.000, 3.000],  loss: 9.119632, mae: 48.563923, mean_q: 42.489831, mean_eps: 0.744250
  42822/150000: episode: 420, duration: 1.098s, episode steps: 160, steps per second: 146, episode reward: -228.466, mean reward: -1.428 [-100.000, 25.292], mean action: 1.725 [0.000, 3.000],  loss: 9.652204, mae: 47.971847, mean_q: 41.147824, mean_eps: 0.743551
  42900/150000: episode: 421, duration: 0.562s, episode steps:  78, steps per second: 139, episode reward: -96.783, mean reward: -1.241 [-100.000,  7.046], mean action: 1.603 [0.000, 3.000],  loss: 8.256224, mae: 48.322730, mean_q: 40.755032, mean_eps: 0.742837
  43040/150000: episode: 422, duration: 1.067s, episode steps: 140, steps per second: 131, episode reward: -66.351, mean reward: -0.474 [-100.000, 16.598], mean action: 1.629 [0.000, 3.000],  loss: 8.909264, mae: 48.880682, mean_q: 42.888229, mean_eps: 0.742183
  43148/150000: episode: 423, duration: 0.905s, episode steps: 108, steps per second: 119, episode reward: -90.039, mean reward: -0.834 [-100.000, 12.256], mean action: 1.565 [0.000, 3.000],  loss: 11.302499, mae: 48.005423, mean_q: 41.760278, mean_eps: 0.741439
  43237/150000: episode: 424, duration: 0.759s, episode steps:  89, steps per second: 117, episode reward: -68.901, mean reward: -0.774 [-100.000,  7.027], mean action: 1.506 [0.000, 3.000],  loss: 9.008008, mae: 48.120090, mean_q: 41.739402, mean_eps: 0.740848
  43308/150000: episode: 425, duration: 0.587s, episode steps:  71, steps per second: 121, episode reward: -53.115, mean reward: -0.748 [-100.000, 17.399], mean action: 1.577 [0.000, 3.000],  loss: 10.770244, mae: 48.845171, mean_q: 42.398406, mean_eps: 0.740368
  43416/150000: episode: 426, duration: 0.850s, episode steps: 108, steps per second: 127, episode reward: -154.257, mean reward: -1.428 [-100.000,  5.704], mean action: 1.546 [0.000, 3.000],  loss: 12.567492, mae: 48.001755, mean_q: 41.957017, mean_eps: 0.739831
  43508/150000: episode: 427, duration: 0.688s, episode steps:  92, steps per second: 134, episode reward: -100.423, mean reward: -1.092 [-100.000, 22.885], mean action: 1.783 [0.000, 3.000],  loss: 9.395056, mae: 47.269740, mean_q: 41.459186, mean_eps: 0.739231
  43583/150000: episode: 428, duration: 0.552s, episode steps:  75, steps per second: 136, episode reward: -107.790, mean reward: -1.437 [-100.000, 10.157], mean action: 1.547 [0.000, 3.000],  loss: 8.769175, mae: 47.736349, mean_q: 40.461602, mean_eps: 0.738730
  43654/150000: episode: 429, duration: 0.484s, episode steps:  71, steps per second: 147, episode reward: -85.648, mean reward: -1.206 [-100.000, 12.432], mean action: 1.437 [0.000, 3.000],  loss: 9.673358, mae: 48.534195, mean_q: 43.113416, mean_eps: 0.738292
  43789/150000: episode: 430, duration: 0.923s, episode steps: 135, steps per second: 146, episode reward: -31.701, mean reward: -0.235 [-100.000, 22.473], mean action: 1.674 [0.000, 3.000],  loss: 11.080303, mae: 48.603621, mean_q: 41.267242, mean_eps: 0.737674
  43874/150000: episode: 431, duration: 0.549s, episode steps:  85, steps per second: 155, episode reward: -72.249, mean reward: -0.850 [-100.000,  8.240], mean action: 1.612 [0.000, 3.000],  loss: 9.273915, mae: 48.050701, mean_q: 41.546079, mean_eps: 0.737014
  44001/150000: episode: 432, duration: 0.836s, episode steps: 127, steps per second: 152, episode reward: -171.827, mean reward: -1.353 [-100.000,  4.121], mean action: 1.465 [0.000, 3.000],  loss: 8.161935, mae: 47.228409, mean_q: 39.788402, mean_eps: 0.736378
  44080/150000: episode: 433, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -36.241, mean reward: -0.459 [-100.000,  8.009], mean action: 1.570 [0.000, 3.000],  loss: 11.124905, mae: 47.723684, mean_q: 41.745466, mean_eps: 0.735760
  44201/150000: episode: 434, duration: 1.004s, episode steps: 121, steps per second: 121, episode reward: -87.436, mean reward: -0.723 [-100.000,  9.193], mean action: 1.620 [0.000, 3.000],  loss: 9.077957, mae: 48.547644, mean_q: 42.541716, mean_eps: 0.735160
  44288/150000: episode: 435, duration: 0.720s, episode steps:  87, steps per second: 121, episode reward: -78.554, mean reward: -0.903 [-100.000, 24.397], mean action: 1.425 [0.000, 3.000],  loss: 9.837985, mae: 48.621090, mean_q: 42.398692, mean_eps: 0.734536
  44421/150000: episode: 436, duration: 0.920s, episode steps: 133, steps per second: 145, episode reward: -98.545, mean reward: -0.741 [-100.000, 12.578], mean action: 1.429 [0.000, 3.000],  loss: 10.336672, mae: 48.006908, mean_q: 41.711598, mean_eps: 0.733876
  44560/150000: episode: 437, duration: 0.877s, episode steps: 139, steps per second: 158, episode reward: -94.685, mean reward: -0.681 [-100.000, 19.960], mean action: 1.590 [0.000, 3.000],  loss: 8.298451, mae: 48.108145, mean_q: 43.257358, mean_eps: 0.733060
  44681/150000: episode: 438, duration: 0.885s, episode steps: 121, steps per second: 137, episode reward: -67.535, mean reward: -0.558 [-100.000, 13.075], mean action: 1.521 [0.000, 3.000],  loss: 7.790289, mae: 48.904005, mean_q: 41.427950, mean_eps: 0.732280
  44809/150000: episode: 439, duration: 0.858s, episode steps: 128, steps per second: 149, episode reward: -65.320, mean reward: -0.510 [-100.000, 11.720], mean action: 1.516 [0.000, 3.000],  loss: 10.809688, mae: 47.777429, mean_q: 40.867442, mean_eps: 0.731533
  44882/150000: episode: 440, duration: 0.490s, episode steps:  73, steps per second: 149, episode reward: -110.996, mean reward: -1.520 [-100.000,  7.044], mean action: 1.630 [0.000, 3.000],  loss: 10.508448, mae: 48.366262, mean_q: 41.061526, mean_eps: 0.730930
  45006/150000: episode: 441, duration: 0.837s, episode steps: 124, steps per second: 148, episode reward: -104.123, mean reward: -0.840 [-100.000,  7.110], mean action: 1.669 [0.000, 3.000],  loss: 9.128816, mae: 47.353340, mean_q: 41.257373, mean_eps: 0.730339
  45101/150000: episode: 442, duration: 0.725s, episode steps:  95, steps per second: 131, episode reward: -37.817, mean reward: -0.398 [-100.000, 11.990], mean action: 1.589 [0.000, 3.000],  loss: 10.408591, mae: 46.819226, mean_q: 40.194375, mean_eps: 0.729682
  45203/150000: episode: 443, duration: 0.796s, episode steps: 102, steps per second: 128, episode reward: -107.546, mean reward: -1.054 [-100.000,  9.595], mean action: 1.578 [0.000, 3.000],  loss: 11.533716, mae: 46.886902, mean_q: 40.945125, mean_eps: 0.729091
  45296/150000: episode: 444, duration: 0.901s, episode steps:  93, steps per second: 103, episode reward: -54.708, mean reward: -0.588 [-100.000,  8.499], mean action: 1.613 [0.000, 3.000],  loss: 9.760327, mae: 47.704468, mean_q: 41.868299, mean_eps: 0.728506
  45388/150000: episode: 445, duration: 0.746s, episode steps:  92, steps per second: 123, episode reward: -73.337, mean reward: -0.797 [-100.000, 15.261], mean action: 1.467 [0.000, 3.000],  loss: 7.589998, mae: 47.098437, mean_q: 40.528664, mean_eps: 0.727951
  45466/150000: episode: 446, duration: 0.488s, episode steps:  78, steps per second: 160, episode reward: -62.550, mean reward: -0.802 [-100.000, 24.212], mean action: 1.423 [0.000, 3.000],  loss: 6.815804, mae: 46.681171, mean_q: 42.487829, mean_eps: 0.727441
  45571/150000: episode: 447, duration: 0.672s, episode steps: 105, steps per second: 156, episode reward: -87.682, mean reward: -0.835 [-100.000, 15.982], mean action: 1.648 [0.000, 3.000],  loss: 9.783758, mae: 46.886580, mean_q: 40.583570, mean_eps: 0.726892
  45670/150000: episode: 448, duration: 0.579s, episode steps:  99, steps per second: 171, episode reward: -80.909, mean reward: -0.817 [-100.000,  7.488], mean action: 1.535 [0.000, 3.000],  loss: 10.152757, mae: 47.624538, mean_q: 42.689460, mean_eps: 0.726280
  45780/150000: episode: 449, duration: 0.643s, episode steps: 110, steps per second: 171, episode reward: -59.132, mean reward: -0.538 [-100.000, 13.190], mean action: 1.673 [0.000, 3.000],  loss: 12.414593, mae: 46.861864, mean_q: 42.702464, mean_eps: 0.725653
  45856/150000: episode: 450, duration: 0.517s, episode steps:  76, steps per second: 147, episode reward: -48.049, mean reward: -0.632 [-100.000, 12.059], mean action: 1.474 [0.000, 3.000],  loss: 14.600253, mae: 46.699373, mean_q: 42.530114, mean_eps: 0.725095
  45937/150000: episode: 451, duration: 0.566s, episode steps:  81, steps per second: 143, episode reward: -39.224, mean reward: -0.484 [-100.000, 12.346], mean action: 1.753 [0.000, 3.000],  loss: 16.645986, mae: 46.663591, mean_q: 40.914099, mean_eps: 0.724624
  46065/150000: episode: 452, duration: 0.866s, episode steps: 128, steps per second: 148, episode reward: -52.820, mean reward: -0.413 [-100.000,  7.210], mean action: 1.531 [0.000, 3.000],  loss: 8.873248, mae: 46.632186, mean_q: 41.801181, mean_eps: 0.723997
  46142/150000: episode: 453, duration: 0.452s, episode steps:  77, steps per second: 170, episode reward: -96.148, mean reward: -1.249 [-100.000, 10.563], mean action: 1.649 [0.000, 3.000],  loss: 9.043988, mae: 46.127700, mean_q: 40.557325, mean_eps: 0.723382
  46244/150000: episode: 454, duration: 0.635s, episode steps: 102, steps per second: 161, episode reward: -86.701, mean reward: -0.850 [-100.000, 11.291], mean action: 1.422 [0.000, 3.000],  loss: 7.953473, mae: 45.961430, mean_q: 41.979598, mean_eps: 0.722845
  46319/150000: episode: 455, duration: 0.440s, episode steps:  75, steps per second: 170, episode reward: -39.310, mean reward: -0.524 [-100.000, 12.340], mean action: 1.787 [0.000, 3.000],  loss: 12.766156, mae: 45.323025, mean_q: 40.012465, mean_eps: 0.722314
  46458/150000: episode: 456, duration: 0.805s, episode steps: 139, steps per second: 173, episode reward: 44.896, mean reward:  0.323 [-100.000, 38.826], mean action: 1.755 [0.000, 3.000],  loss: 9.162793, mae: 46.284774, mean_q: 41.694834, mean_eps: 0.721672
  46571/150000: episode: 457, duration: 0.705s, episode steps: 113, steps per second: 160, episode reward: -101.951, mean reward: -0.902 [-100.000, 13.427], mean action: 1.531 [0.000, 3.000],  loss: 11.408387, mae: 46.090624, mean_q: 41.971615, mean_eps: 0.720916
  46679/150000: episode: 458, duration: 0.633s, episode steps: 108, steps per second: 171, episode reward: -90.848, mean reward: -0.841 [-100.000, 10.175], mean action: 1.509 [0.000, 3.000],  loss: 12.568687, mae: 46.097480, mean_q: 40.922539, mean_eps: 0.720253
  46777/150000: episode: 459, duration: 0.544s, episode steps:  98, steps per second: 180, episode reward: -147.251, mean reward: -1.503 [-100.000,  6.800], mean action: 1.439 [0.000, 3.000],  loss: 11.696312, mae: 45.342646, mean_q: 41.535877, mean_eps: 0.719635
  46940/150000: episode: 460, duration: 0.956s, episode steps: 163, steps per second: 170, episode reward: -104.612, mean reward: -0.642 [-100.000, 41.396], mean action: 1.564 [0.000, 3.000],  loss: 9.195092, mae: 45.755247, mean_q: 41.161133, mean_eps: 0.718852
  47041/150000: episode: 461, duration: 0.588s, episode steps: 101, steps per second: 172, episode reward: -77.826, mean reward: -0.771 [-100.000,  8.952], mean action: 1.584 [0.000, 3.000],  loss: 11.136853, mae: 44.701362, mean_q: 41.198599, mean_eps: 0.718060
  47150/150000: episode: 462, duration: 0.640s, episode steps: 109, steps per second: 170, episode reward: -135.983, mean reward: -1.248 [-100.000,  6.350], mean action: 1.440 [0.000, 3.000],  loss: 8.944997, mae: 44.429583, mean_q: 40.458327, mean_eps: 0.717430
  47255/150000: episode: 463, duration: 0.648s, episode steps: 105, steps per second: 162, episode reward: -84.845, mean reward: -0.808 [-100.000,  5.544], mean action: 1.457 [0.000, 3.000],  loss: 13.616923, mae: 43.774893, mean_q: 39.048811, mean_eps: 0.716788
  47380/150000: episode: 464, duration: 0.793s, episode steps: 125, steps per second: 158, episode reward: -51.238, mean reward: -0.410 [-100.000, 13.078], mean action: 1.432 [0.000, 3.000],  loss: 10.848029, mae: 44.647399, mean_q: 41.616956, mean_eps: 0.716098
  47474/150000: episode: 465, duration: 0.543s, episode steps:  94, steps per second: 173, episode reward: -71.794, mean reward: -0.764 [-100.000, 13.129], mean action: 1.670 [0.000, 3.000],  loss: 8.690404, mae: 44.829214, mean_q: 42.750480, mean_eps: 0.715441
  47544/150000: episode: 466, duration: 0.402s, episode steps:  70, steps per second: 174, episode reward: -53.749, mean reward: -0.768 [-100.000, 13.955], mean action: 1.600 [0.000, 3.000],  loss: 8.558319, mae: 44.558772, mean_q: 40.800570, mean_eps: 0.714949
  47646/150000: episode: 467, duration: 0.607s, episode steps: 102, steps per second: 168, episode reward: -64.031, mean reward: -0.628 [-100.000, 10.005], mean action: 1.794 [0.000, 3.000],  loss: 8.709589, mae: 44.352655, mean_q: 41.602982, mean_eps: 0.714433
  47730/150000: episode: 468, duration: 0.494s, episode steps:  84, steps per second: 170, episode reward: -132.690, mean reward: -1.580 [-100.000,  6.531], mean action: 1.548 [0.000, 3.000],  loss: 9.521954, mae: 44.329856, mean_q: 42.821255, mean_eps: 0.713875
  47848/150000: episode: 469, duration: 0.683s, episode steps: 118, steps per second: 173, episode reward: -46.408, mean reward: -0.393 [-100.000, 11.564], mean action: 1.661 [0.000, 3.000],  loss: 9.539378, mae: 43.612528, mean_q: 42.011506, mean_eps: 0.713269
  47989/150000: episode: 470, duration: 0.810s, episode steps: 141, steps per second: 174, episode reward: -41.760, mean reward: -0.296 [-100.000, 10.609], mean action: 1.518 [0.000, 3.000],  loss: 10.519233, mae: 44.597981, mean_q: 42.618062, mean_eps: 0.712492
  48141/150000: episode: 471, duration: 0.915s, episode steps: 152, steps per second: 166, episode reward: -36.325, mean reward: -0.239 [-100.000, 18.873], mean action: 1.717 [0.000, 3.000],  loss: 9.937686, mae: 44.262613, mean_q: 42.904736, mean_eps: 0.711613
  48215/150000: episode: 472, duration: 0.448s, episode steps:  74, steps per second: 165, episode reward: -117.145, mean reward: -1.583 [-100.000, 16.170], mean action: 1.568 [0.000, 3.000],  loss: 13.267100, mae: 42.840737, mean_q: 40.837697, mean_eps: 0.710935
  48324/150000: episode: 473, duration: 0.776s, episode steps: 109, steps per second: 140, episode reward: -78.417, mean reward: -0.719 [-100.000, 10.248], mean action: 1.505 [0.000, 3.000],  loss: 11.882961, mae: 43.250431, mean_q: 41.520162, mean_eps: 0.710386
  48446/150000: episode: 474, duration: 0.912s, episode steps: 122, steps per second: 134, episode reward: -66.031, mean reward: -0.541 [-100.000, 12.196], mean action: 1.607 [0.000, 3.000],  loss: 7.184329, mae: 44.165736, mean_q: 45.041884, mean_eps: 0.709693
  48541/150000: episode: 475, duration: 0.734s, episode steps:  95, steps per second: 130, episode reward: -84.863, mean reward: -0.893 [-100.000,  9.142], mean action: 1.611 [0.000, 3.000],  loss: 11.051175, mae: 44.314101, mean_q: 43.592613, mean_eps: 0.709042
  48619/150000: episode: 476, duration: 0.620s, episode steps:  78, steps per second: 126, episode reward: -43.480, mean reward: -0.557 [-100.000, 11.082], mean action: 1.615 [0.000, 3.000],  loss: 9.274631, mae: 44.206199, mean_q: 42.808827, mean_eps: 0.708523
  48719/150000: episode: 477, duration: 0.777s, episode steps: 100, steps per second: 129, episode reward: -82.465, mean reward: -0.825 [-100.000, 14.746], mean action: 1.670 [0.000, 3.000],  loss: 7.420113, mae: 43.824092, mean_q: 42.292279, mean_eps: 0.707989
  48839/150000: episode: 478, duration: 0.847s, episode steps: 120, steps per second: 142, episode reward: -27.618, mean reward: -0.230 [-100.000, 11.451], mean action: 1.525 [0.000, 3.000],  loss: 7.857433, mae: 43.850844, mean_q: 41.770829, mean_eps: 0.707329
  48932/150000: episode: 479, duration: 0.695s, episode steps:  93, steps per second: 134, episode reward: -80.438, mean reward: -0.865 [-100.000,  7.127], mean action: 1.462 [0.000, 3.000],  loss: 13.942278, mae: 44.943473, mean_q: 42.004326, mean_eps: 0.706690
  49068/150000: episode: 480, duration: 1.157s, episode steps: 136, steps per second: 118, episode reward: -79.425, mean reward: -0.584 [-100.000,  9.040], mean action: 1.588 [0.000, 3.000],  loss: 8.703873, mae: 44.766114, mean_q: 43.652053, mean_eps: 0.706003
  49156/150000: episode: 481, duration: 0.725s, episode steps:  88, steps per second: 121, episode reward: -59.939, mean reward: -0.681 [-100.000, 14.617], mean action: 1.705 [0.000, 3.000],  loss: 10.825325, mae: 44.931433, mean_q: 43.820352, mean_eps: 0.705331
  49283/150000: episode: 482, duration: 0.949s, episode steps: 127, steps per second: 134, episode reward: -79.112, mean reward: -0.623 [-100.000, 12.495], mean action: 1.512 [0.000, 3.000],  loss: 11.706825, mae: 45.312877, mean_q: 43.648677, mean_eps: 0.704686
  49377/150000: episode: 483, duration: 0.707s, episode steps:  94, steps per second: 133, episode reward: -43.229, mean reward: -0.460 [-100.000, 14.886], mean action: 1.638 [0.000, 3.000],  loss: 11.580807, mae: 45.199209, mean_q: 43.923401, mean_eps: 0.704023
  49478/150000: episode: 484, duration: 0.722s, episode steps: 101, steps per second: 140, episode reward: -86.820, mean reward: -0.860 [-100.000, 17.910], mean action: 1.515 [0.000, 3.000],  loss: 10.545199, mae: 45.780557, mean_q: 43.992832, mean_eps: 0.703438
  49598/150000: episode: 485, duration: 0.829s, episode steps: 120, steps per second: 145, episode reward: -119.826, mean reward: -0.999 [-100.000,  6.785], mean action: 1.458 [0.000, 3.000],  loss: 7.717798, mae: 45.835656, mean_q: 46.060581, mean_eps: 0.702775
  49673/150000: episode: 486, duration: 0.513s, episode steps:  75, steps per second: 146, episode reward: -69.764, mean reward: -0.930 [-100.000, 14.039], mean action: 1.427 [0.000, 3.000],  loss: 9.170626, mae: 44.950160, mean_q: 43.602951, mean_eps: 0.702190
  49821/150000: episode: 487, duration: 1.044s, episode steps: 148, steps per second: 142, episode reward: -178.779, mean reward: -1.208 [-100.000, 43.662], mean action: 1.608 [0.000, 3.000],  loss: 8.016076, mae: 44.878641, mean_q: 44.087638, mean_eps: 0.701521
  49908/150000: episode: 488, duration: 0.620s, episode steps:  87, steps per second: 140, episode reward: -68.350, mean reward: -0.786 [-100.000,  7.465], mean action: 1.885 [0.000, 3.000],  loss: 13.084804, mae: 45.138812, mean_q: 45.029712, mean_eps: 0.700816
  50000/150000: episode: 489, duration: 0.671s, episode steps:  92, steps per second: 137, episode reward: -101.015, mean reward: -1.098 [-100.000,  9.260], mean action: 1.391 [0.000, 3.000],  loss: 14.763095, mae: 44.058454, mean_q: 42.566107, mean_eps: 0.700279
  50123/150000: episode: 490, duration: 0.942s, episode steps: 123, steps per second: 131, episode reward: -92.753, mean reward: -0.754 [-100.000,  7.288], mean action: 1.553 [0.000, 3.000],  loss: 9.633457, mae: 44.923660, mean_q: 43.186016, mean_eps: 0.699634
  50227/150000: episode: 491, duration: 0.852s, episode steps: 104, steps per second: 122, episode reward: -34.030, mean reward: -0.327 [-100.000, 17.747], mean action: 1.625 [0.000, 3.000],  loss: 7.426797, mae: 45.138656, mean_q: 43.248126, mean_eps: 0.698953
  50328/150000: episode: 492, duration: 0.911s, episode steps: 101, steps per second: 111, episode reward: -114.053, mean reward: -1.129 [-100.000, 11.950], mean action: 1.495 [0.000, 3.000],  loss: 11.678040, mae: 45.232832, mean_q: 43.535306, mean_eps: 0.698338
  50456/150000: episode: 493, duration: 1.032s, episode steps: 128, steps per second: 124, episode reward: -163.400, mean reward: -1.277 [-100.000,  5.639], mean action: 1.242 [0.000, 3.000],  loss: 8.431552, mae: 44.694189, mean_q: 42.349378, mean_eps: 0.697651
  50555/150000: episode: 494, duration: 0.740s, episode steps:  99, steps per second: 134, episode reward: -68.259, mean reward: -0.689 [-100.000, 11.364], mean action: 1.717 [0.000, 3.000],  loss: 10.355567, mae: 44.754268, mean_q: 42.199426, mean_eps: 0.696970
  50653/150000: episode: 495, duration: 0.759s, episode steps:  98, steps per second: 129, episode reward: -24.373, mean reward: -0.249 [-100.000, 17.685], mean action: 1.806 [0.000, 3.000],  loss: 9.021585, mae: 44.698718, mean_q: 42.007861, mean_eps: 0.696379
  50774/150000: episode: 496, duration: 0.860s, episode steps: 121, steps per second: 141, episode reward: -114.412, mean reward: -0.946 [-100.000,  9.807], mean action: 1.372 [0.000, 3.000],  loss: 9.176501, mae: 45.262661, mean_q: 43.696826, mean_eps: 0.695722
  50854/150000: episode: 497, duration: 0.609s, episode steps:  80, steps per second: 131, episode reward: -52.912, mean reward: -0.661 [-100.000, 10.862], mean action: 1.663 [0.000, 3.000],  loss: 9.137832, mae: 44.226012, mean_q: 41.430577, mean_eps: 0.695119
  50939/150000: episode: 498, duration: 0.628s, episode steps:  85, steps per second: 135, episode reward: -76.529, mean reward: -0.900 [-100.000,  9.393], mean action: 1.729 [0.000, 3.000],  loss: 9.165712, mae: 45.132794, mean_q: 42.385967, mean_eps: 0.694624
  51029/150000: episode: 499, duration: 0.656s, episode steps:  90, steps per second: 137, episode reward: -58.600, mean reward: -0.651 [-100.000,  7.531], mean action: 1.589 [0.000, 3.000],  loss: 10.060046, mae: 43.410512, mean_q: 41.826693, mean_eps: 0.694099
  51110/150000: episode: 500, duration: 0.644s, episode steps:  81, steps per second: 126, episode reward: -139.726, mean reward: -1.725 [-100.000, 16.783], mean action: 1.593 [0.000, 3.000],  loss: 8.288645, mae: 44.772986, mean_q: 42.191971, mean_eps: 0.693586
  51225/150000: episode: 501, duration: 0.895s, episode steps: 115, steps per second: 128, episode reward: -71.415, mean reward: -0.621 [-100.000, 10.758], mean action: 1.635 [0.000, 3.000],  loss: 9.500610, mae: 44.192542, mean_q: 43.658304, mean_eps: 0.692998
  51338/150000: episode: 502, duration: 0.951s, episode steps: 113, steps per second: 119, episode reward: -136.754, mean reward: -1.210 [-100.000, 15.937], mean action: 1.558 [0.000, 3.000],  loss: 12.141624, mae: 43.744001, mean_q: 42.368696, mean_eps: 0.692314
  51447/150000: episode: 503, duration: 0.794s, episode steps: 109, steps per second: 137, episode reward: -75.507, mean reward: -0.693 [-100.000,  7.104], mean action: 1.734 [0.000, 3.000],  loss: 13.006522, mae: 44.371984, mean_q: 42.606198, mean_eps: 0.691648
  51531/150000: episode: 504, duration: 0.692s, episode steps:  84, steps per second: 121, episode reward: -95.267, mean reward: -1.134 [-100.000,  7.172], mean action: 1.631 [0.000, 3.000],  loss: 9.281616, mae: 43.466285, mean_q: 40.249217, mean_eps: 0.691069
  51651/150000: episode: 505, duration: 0.847s, episode steps: 120, steps per second: 142, episode reward: -59.542, mean reward: -0.496 [-100.000, 15.072], mean action: 1.550 [0.000, 3.000],  loss: 9.789486, mae: 43.090817, mean_q: 40.616255, mean_eps: 0.690457
  51814/150000: episode: 506, duration: 1.188s, episode steps: 163, steps per second: 137, episode reward: 34.815, mean reward:  0.214 [-100.000, 30.831], mean action: 1.761 [0.000, 3.000],  loss: 9.420418, mae: 43.141265, mean_q: 41.112259, mean_eps: 0.689608
  51953/150000: episode: 507, duration: 0.945s, episode steps: 139, steps per second: 147, episode reward: -14.503, mean reward: -0.104 [-100.000, 18.156], mean action: 1.518 [0.000, 3.000],  loss: 9.788471, mae: 43.938820, mean_q: 41.330376, mean_eps: 0.688702
  52068/150000: episode: 508, duration: 0.810s, episode steps: 115, steps per second: 142, episode reward: -41.807, mean reward: -0.364 [-100.000, 17.504], mean action: 1.574 [0.000, 3.000],  loss: 9.863103, mae: 43.865577, mean_q: 41.940865, mean_eps: 0.687940
  52239/150000: episode: 509, duration: 1.208s, episode steps: 171, steps per second: 142, episode reward: -132.485, mean reward: -0.775 [-100.000,  4.589], mean action: 1.673 [0.000, 3.000],  loss: 9.848872, mae: 43.322703, mean_q: 41.886191, mean_eps: 0.687082
  52378/150000: episode: 510, duration: 1.164s, episode steps: 139, steps per second: 119, episode reward: -18.218, mean reward: -0.131 [-100.000,  7.305], mean action: 1.669 [0.000, 3.000],  loss: 12.141143, mae: 44.348561, mean_q: 43.239343, mean_eps: 0.686152
  52519/150000: episode: 511, duration: 1.231s, episode steps: 141, steps per second: 115, episode reward: -287.205, mean reward: -2.037 [-100.000, 94.049], mean action: 1.340 [0.000, 3.000],  loss: 11.008670, mae: 44.145771, mean_q: 42.215032, mean_eps: 0.685312
  52612/150000: episode: 512, duration: 0.838s, episode steps:  93, steps per second: 111, episode reward:  8.648, mean reward:  0.093 [-100.000, 22.503], mean action: 1.624 [0.000, 3.000],  loss: 6.599858, mae: 43.663781, mean_q: 41.610168, mean_eps: 0.684610
  52709/150000: episode: 513, duration: 0.779s, episode steps:  97, steps per second: 125, episode reward: -36.699, mean reward: -0.378 [-100.000,  8.279], mean action: 1.608 [0.000, 3.000],  loss: 7.448120, mae: 43.192261, mean_q: 41.942738, mean_eps: 0.684040
  52795/150000: episode: 514, duration: 0.660s, episode steps:  86, steps per second: 130, episode reward: -124.832, mean reward: -1.452 [-100.000, 11.107], mean action: 1.407 [0.000, 3.000],  loss: 17.174113, mae: 43.414964, mean_q: 39.325703, mean_eps: 0.683491
  52891/150000: episode: 515, duration: 0.704s, episode steps:  96, steps per second: 136, episode reward: -77.584, mean reward: -0.808 [-100.000, 20.280], mean action: 1.594 [0.000, 3.000],  loss: 12.582448, mae: 43.392797, mean_q: 40.466807, mean_eps: 0.682945
  53043/150000: episode: 516, duration: 1.215s, episode steps: 152, steps per second: 125, episode reward: -15.699, mean reward: -0.103 [-100.000, 16.048], mean action: 1.691 [0.000, 3.000],  loss: 12.745968, mae: 44.274851, mean_q: 42.642670, mean_eps: 0.682201
  53142/150000: episode: 517, duration: 0.737s, episode steps:  99, steps per second: 134, episode reward: -115.483, mean reward: -1.166 [-100.000, 13.159], mean action: 1.657 [0.000, 3.000],  loss: 16.414092, mae: 43.289861, mean_q: 41.060284, mean_eps: 0.681448
  53246/150000: episode: 518, duration: 0.692s, episode steps: 104, steps per second: 150, episode reward: -138.299, mean reward: -1.330 [-100.000, 17.610], mean action: 1.481 [0.000, 3.000],  loss: 8.953609, mae: 43.799619, mean_q: 41.772468, mean_eps: 0.680839
  53355/150000: episode: 519, duration: 0.681s, episode steps: 109, steps per second: 160, episode reward: -116.603, mean reward: -1.070 [-100.000, 15.710], mean action: 1.633 [0.000, 3.000],  loss: 8.812896, mae: 42.979000, mean_q: 40.867182, mean_eps: 0.680200
  53496/150000: episode: 520, duration: 0.909s, episode steps: 141, steps per second: 155, episode reward: -62.933, mean reward: -0.446 [-100.000,  4.060], mean action: 1.574 [0.000, 3.000],  loss: 6.398202, mae: 43.142835, mean_q: 42.223541, mean_eps: 0.679450
  53609/150000: episode: 521, duration: 0.895s, episode steps: 113, steps per second: 126, episode reward: -43.414, mean reward: -0.384 [-100.000, 18.919], mean action: 1.566 [0.000, 3.000],  loss: 9.681615, mae: 43.459774, mean_q: 41.115238, mean_eps: 0.678688
  53692/150000: episode: 522, duration: 0.610s, episode steps:  83, steps per second: 136, episode reward: -82.101, mean reward: -0.989 [-100.000,  7.267], mean action: 1.458 [0.000, 3.000],  loss: 12.248608, mae: 43.702638, mean_q: 42.258759, mean_eps: 0.678100
  53775/150000: episode: 523, duration: 0.565s, episode steps:  83, steps per second: 147, episode reward: -32.399, mean reward: -0.390 [-100.000,  7.214], mean action: 1.602 [0.000, 3.000],  loss: 13.306139, mae: 44.044349, mean_q: 41.526596, mean_eps: 0.677602
  53884/150000: episode: 524, duration: 0.857s, episode steps: 109, steps per second: 127, episode reward: -124.324, mean reward: -1.141 [-100.000,  6.091], mean action: 1.835 [0.000, 3.000],  loss: 12.079392, mae: 43.709430, mean_q: 42.462695, mean_eps: 0.677026
  54001/150000: episode: 525, duration: 0.846s, episode steps: 117, steps per second: 138, episode reward: -161.460, mean reward: -1.380 [-100.000,  7.271], mean action: 1.838 [0.000, 3.000],  loss: 10.463038, mae: 44.522695, mean_q: 43.092033, mean_eps: 0.676348
  54069/150000: episode: 526, duration: 0.479s, episode steps:  68, steps per second: 142, episode reward: -89.398, mean reward: -1.315 [-100.000, 17.813], mean action: 1.500 [0.000, 3.000],  loss: 9.774234, mae: 43.977178, mean_q: 42.094923, mean_eps: 0.675793
  54142/150000: episode: 527, duration: 0.571s, episode steps:  73, steps per second: 128, episode reward: -39.838, mean reward: -0.546 [-100.000, 15.919], mean action: 1.699 [0.000, 3.000],  loss: 13.525525, mae: 44.826002, mean_q: 44.251413, mean_eps: 0.675370
  54211/150000: episode: 528, duration: 0.466s, episode steps:  69, steps per second: 148, episode reward: -96.220, mean reward: -1.394 [-100.000, 10.825], mean action: 1.696 [0.000, 3.000],  loss: 10.142851, mae: 45.729362, mean_q: 44.842603, mean_eps: 0.674944
  54362/150000: episode: 529, duration: 1.083s, episode steps: 151, steps per second: 139, episode reward: -25.977, mean reward: -0.172 [-100.000,  9.775], mean action: 1.649 [0.000, 3.000],  loss: 12.335479, mae: 44.796785, mean_q: 42.271864, mean_eps: 0.674284
  54472/150000: episode: 530, duration: 0.819s, episode steps: 110, steps per second: 134, episode reward: -81.059, mean reward: -0.737 [-100.000, 17.109], mean action: 1.436 [0.000, 3.000],  loss: 7.024263, mae: 45.525571, mean_q: 43.629651, mean_eps: 0.673501
  54598/150000: episode: 531, duration: 0.891s, episode steps: 126, steps per second: 141, episode reward: -85.949, mean reward: -0.682 [-100.000, 10.563], mean action: 1.603 [0.000, 3.000],  loss: 11.809199, mae: 45.158151, mean_q: 43.338485, mean_eps: 0.672793
  54709/150000: episode: 532, duration: 0.877s, episode steps: 111, steps per second: 127, episode reward: -71.723, mean reward: -0.646 [-100.000, 13.873], mean action: 1.640 [0.000, 3.000],  loss: 7.632130, mae: 44.538019, mean_q: 43.879292, mean_eps: 0.672082
  54786/150000: episode: 533, duration: 0.548s, episode steps:  77, steps per second: 141, episode reward: -111.403, mean reward: -1.447 [-100.000,  8.975], mean action: 1.558 [0.000, 3.000],  loss: 12.050394, mae: 44.271882, mean_q: 43.592973, mean_eps: 0.671518
  54925/150000: episode: 534, duration: 0.965s, episode steps: 139, steps per second: 144, episode reward: -65.450, mean reward: -0.471 [-100.000, 11.232], mean action: 1.482 [0.000, 3.000],  loss: 7.867731, mae: 44.846843, mean_q: 42.991217, mean_eps: 0.670870
  55037/150000: episode: 535, duration: 0.791s, episode steps: 112, steps per second: 142, episode reward: -126.836, mean reward: -1.132 [-100.000,  3.479], mean action: 1.446 [0.000, 3.000],  loss: 10.714525, mae: 45.022069, mean_q: 44.222785, mean_eps: 0.670117
  55153/150000: episode: 536, duration: 0.769s, episode steps: 116, steps per second: 151, episode reward: -10.748, mean reward: -0.093 [-100.000, 11.837], mean action: 1.724 [0.000, 3.000],  loss: 9.302075, mae: 45.795826, mean_q: 44.680400, mean_eps: 0.669433
  55286/150000: episode: 537, duration: 0.917s, episode steps: 133, steps per second: 145, episode reward: -60.745, mean reward: -0.457 [-100.000, 16.394], mean action: 1.617 [0.000, 3.000],  loss: 7.599841, mae: 45.206192, mean_q: 44.501683, mean_eps: 0.668686
  55388/150000: episode: 538, duration: 0.768s, episode steps: 102, steps per second: 133, episode reward: -26.696, mean reward: -0.262 [-100.000, 38.254], mean action: 1.510 [0.000, 3.000],  loss: 7.972001, mae: 45.850297, mean_q: 45.267038, mean_eps: 0.667981
  55513/150000: episode: 539, duration: 0.864s, episode steps: 125, steps per second: 145, episode reward: -18.176, mean reward: -0.145 [-100.000, 17.403], mean action: 1.400 [0.000, 3.000],  loss: 9.720698, mae: 44.943380, mean_q: 44.370405, mean_eps: 0.667300
  55610/150000: episode: 540, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -74.201, mean reward: -0.765 [-100.000, 15.743], mean action: 1.608 [0.000, 3.000],  loss: 8.869315, mae: 46.621989, mean_q: 46.537105, mean_eps: 0.666634
  55731/150000: episode: 541, duration: 0.848s, episode steps: 121, steps per second: 143, episode reward: -7.372, mean reward: -0.061 [-100.000, 16.220], mean action: 1.645 [0.000, 3.000],  loss: 9.115279, mae: 46.499225, mean_q: 44.562478, mean_eps: 0.665980
  55882/150000: episode: 542, duration: 1.119s, episode steps: 151, steps per second: 135, episode reward:  6.795, mean reward:  0.045 [-100.000, 16.056], mean action: 1.788 [0.000, 3.000],  loss: 9.213795, mae: 46.027761, mean_q: 45.683858, mean_eps: 0.665164
  55991/150000: episode: 543, duration: 0.910s, episode steps: 109, steps per second: 120, episode reward: -55.418, mean reward: -0.508 [-100.000, 44.704], mean action: 1.716 [0.000, 3.000],  loss: 8.084851, mae: 45.713841, mean_q: 44.952666, mean_eps: 0.664384
  56071/150000: episode: 544, duration: 0.616s, episode steps:  80, steps per second: 130, episode reward: -109.857, mean reward: -1.373 [-100.000,  8.224], mean action: 1.512 [0.000, 3.000],  loss: 5.359331, mae: 46.711633, mean_q: 47.337750, mean_eps: 0.663817
  56140/150000: episode: 545, duration: 0.506s, episode steps:  69, steps per second: 136, episode reward: -89.149, mean reward: -1.292 [-100.000,  5.548], mean action: 1.522 [0.000, 3.000],  loss: 9.660173, mae: 45.422687, mean_q: 45.132355, mean_eps: 0.663370
  56276/150000: episode: 546, duration: 1.066s, episode steps: 136, steps per second: 128, episode reward: -84.411, mean reward: -0.621 [-100.000,  6.271], mean action: 1.662 [0.000, 3.000],  loss: 11.819791, mae: 46.671779, mean_q: 46.828302, mean_eps: 0.662755
  56371/150000: episode: 547, duration: 0.710s, episode steps:  95, steps per second: 134, episode reward: -46.707, mean reward: -0.492 [-100.000, 12.446], mean action: 1.474 [0.000, 3.000],  loss: 6.266353, mae: 46.300445, mean_q: 44.872546, mean_eps: 0.662062
  56517/150000: episode: 548, duration: 1.133s, episode steps: 146, steps per second: 129, episode reward: -79.621, mean reward: -0.545 [-100.000,  7.784], mean action: 1.644 [0.000, 3.000],  loss: 11.545834, mae: 45.957308, mean_q: 45.582610, mean_eps: 0.661339
  56656/150000: episode: 549, duration: 1.038s, episode steps: 139, steps per second: 134, episode reward: -27.804, mean reward: -0.200 [-100.000,  9.559], mean action: 1.633 [0.000, 3.000],  loss: 7.874937, mae: 46.206424, mean_q: 46.050694, mean_eps: 0.660484
  56788/150000: episode: 550, duration: 0.925s, episode steps: 132, steps per second: 143, episode reward: -79.342, mean reward: -0.601 [-100.000, 10.783], mean action: 1.705 [0.000, 3.000],  loss: 8.594509, mae: 45.452994, mean_q: 45.061987, mean_eps: 0.659671
  56906/150000: episode: 551, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -50.226, mean reward: -0.426 [-100.000, 18.452], mean action: 1.653 [0.000, 3.000],  loss: 12.091142, mae: 46.085713, mean_q: 46.069081, mean_eps: 0.658921
  57026/150000: episode: 552, duration: 0.844s, episode steps: 120, steps per second: 142, episode reward: -108.837, mean reward: -0.907 [-100.000,  7.101], mean action: 1.667 [0.000, 3.000],  loss: 9.052959, mae: 45.929955, mean_q: 45.910131, mean_eps: 0.658207
  57123/150000: episode: 553, duration: 0.709s, episode steps:  97, steps per second: 137, episode reward: -137.679, mean reward: -1.419 [-100.000,  8.276], mean action: 1.691 [0.000, 3.000],  loss: 9.488805, mae: 46.929240, mean_q: 46.130423, mean_eps: 0.657556
  57241/150000: episode: 554, duration: 0.824s, episode steps: 118, steps per second: 143, episode reward: -94.422, mean reward: -0.800 [-100.000, 13.247], mean action: 1.661 [0.000, 3.000],  loss: 7.600437, mae: 46.610737, mean_q: 48.149879, mean_eps: 0.656911
  57346/150000: episode: 555, duration: 0.787s, episode steps: 105, steps per second: 133, episode reward: -177.006, mean reward: -1.686 [-100.000, 51.167], mean action: 1.695 [0.000, 3.000],  loss: 10.380046, mae: 46.793958, mean_q: 46.846467, mean_eps: 0.656242
  57468/150000: episode: 556, duration: 0.918s, episode steps: 122, steps per second: 133, episode reward: -46.144, mean reward: -0.378 [-100.000, 19.919], mean action: 1.697 [0.000, 3.000],  loss: 8.703193, mae: 47.793594, mean_q: 48.884109, mean_eps: 0.655561
  57605/150000: episode: 557, duration: 0.969s, episode steps: 137, steps per second: 141, episode reward: -134.457, mean reward: -0.981 [-100.000, 35.728], mean action: 1.642 [0.000, 3.000],  loss: 10.729659, mae: 46.646006, mean_q: 46.309173, mean_eps: 0.654784
  57728/150000: episode: 558, duration: 0.855s, episode steps: 123, steps per second: 144, episode reward: -49.015, mean reward: -0.398 [-100.000, 23.124], mean action: 1.626 [0.000, 3.000],  loss: 10.282965, mae: 47.493090, mean_q: 46.799823, mean_eps: 0.654004
  58109/150000: episode: 559, duration: 2.661s, episode steps: 381, steps per second: 143, episode reward: -135.869, mean reward: -0.357 [-100.000, 58.946], mean action: 1.795 [0.000, 3.000],  loss: 8.988408, mae: 47.144719, mean_q: 46.448296, mean_eps: 0.652492
  58182/150000: episode: 560, duration: 0.503s, episode steps:  73, steps per second: 145, episode reward: -110.719, mean reward: -1.517 [-100.000, 17.653], mean action: 1.616 [0.000, 3.000],  loss: 8.831359, mae: 46.678282, mean_q: 46.403230, mean_eps: 0.651130
  58304/150000: episode: 561, duration: 0.780s, episode steps: 122, steps per second: 156, episode reward: -69.274, mean reward: -0.568 [-100.000,  7.796], mean action: 1.541 [0.000, 3.000],  loss: 8.473149, mae: 46.274266, mean_q: 45.880614, mean_eps: 0.650545
  58399/150000: episode: 562, duration: 0.579s, episode steps:  95, steps per second: 164, episode reward: -77.348, mean reward: -0.814 [-100.000, 17.669], mean action: 1.632 [0.000, 3.000],  loss: 11.200371, mae: 45.915469, mean_q: 44.677661, mean_eps: 0.649894
  58526/150000: episode: 563, duration: 0.724s, episode steps: 127, steps per second: 175, episode reward: 28.048, mean reward:  0.221 [-100.000, 18.036], mean action: 1.661 [0.000, 3.000],  loss: 9.444388, mae: 46.008632, mean_q: 45.772140, mean_eps: 0.649228
  58623/150000: episode: 564, duration: 0.547s, episode steps:  97, steps per second: 177, episode reward: -41.844, mean reward: -0.431 [-100.000, 21.140], mean action: 1.443 [0.000, 3.000],  loss: 7.392347, mae: 45.323267, mean_q: 44.337786, mean_eps: 0.648556
  58727/150000: episode: 565, duration: 0.653s, episode steps: 104, steps per second: 159, episode reward: -26.096, mean reward: -0.251 [-100.000, 13.499], mean action: 1.558 [0.000, 3.000],  loss: 10.565163, mae: 46.336141, mean_q: 46.231877, mean_eps: 0.647953
  58824/150000: episode: 566, duration: 0.659s, episode steps:  97, steps per second: 147, episode reward: -94.082, mean reward: -0.970 [-100.000,  6.308], mean action: 1.660 [0.000, 3.000],  loss: 10.685057, mae: 45.803448, mean_q: 46.465472, mean_eps: 0.647350
  58944/150000: episode: 567, duration: 0.822s, episode steps: 120, steps per second: 146, episode reward: -82.654, mean reward: -0.689 [-100.000,  6.819], mean action: 1.717 [0.000, 3.000],  loss: 6.940643, mae: 45.854567, mean_q: 46.121089, mean_eps: 0.646699
  59128/150000: episode: 568, duration: 1.321s, episode steps: 184, steps per second: 139, episode reward: -39.017, mean reward: -0.212 [-100.000, 16.551], mean action: 1.652 [0.000, 3.000],  loss: 10.172589, mae: 46.117457, mean_q: 46.050953, mean_eps: 0.645787
  59278/150000: episode: 569, duration: 1.087s, episode steps: 150, steps per second: 138, episode reward: -50.732, mean reward: -0.338 [-100.000, 23.122], mean action: 1.520 [0.000, 3.000],  loss: 9.629119, mae: 45.605383, mean_q: 46.296434, mean_eps: 0.644785
  59381/150000: episode: 570, duration: 0.806s, episode steps: 103, steps per second: 128, episode reward: -91.146, mean reward: -0.885 [-100.000,  7.817], mean action: 1.699 [0.000, 3.000],  loss: 10.049486, mae: 44.988649, mean_q: 44.265414, mean_eps: 0.644026
  59492/150000: episode: 571, duration: 0.796s, episode steps: 111, steps per second: 140, episode reward: -79.824, mean reward: -0.719 [-100.000,  7.183], mean action: 1.523 [0.000, 3.000],  loss: 10.834468, mae: 45.203494, mean_q: 44.344930, mean_eps: 0.643384
  59616/150000: episode: 572, duration: 0.877s, episode steps: 124, steps per second: 141, episode reward: -19.257, mean reward: -0.155 [-100.000, 16.692], mean action: 1.540 [0.000, 3.000],  loss: 9.381723, mae: 45.148172, mean_q: 43.170304, mean_eps: 0.642679
  59751/150000: episode: 573, duration: 0.950s, episode steps: 135, steps per second: 142, episode reward: -119.610, mean reward: -0.886 [-100.000, 13.064], mean action: 1.652 [0.000, 3.000],  loss: 11.751430, mae: 45.201815, mean_q: 43.109742, mean_eps: 0.641902
  59900/150000: episode: 574, duration: 1.016s, episode steps: 149, steps per second: 147, episode reward: -52.183, mean reward: -0.350 [-100.000, 12.367], mean action: 1.691 [0.000, 3.000],  loss: 8.805347, mae: 45.641513, mean_q: 44.466266, mean_eps: 0.641050
  59982/150000: episode: 575, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -49.373, mean reward: -0.602 [-100.000,  7.445], mean action: 1.671 [0.000, 3.000],  loss: 9.002536, mae: 45.443328, mean_q: 43.960788, mean_eps: 0.640357
  60090/150000: episode: 576, duration: 0.844s, episode steps: 108, steps per second: 128, episode reward: -73.155, mean reward: -0.677 [-100.000,  7.963], mean action: 1.731 [0.000, 3.000],  loss: 8.537338, mae: 47.015908, mean_q: 45.717279, mean_eps: 0.639787
  60165/150000: episode: 577, duration: 0.555s, episode steps:  75, steps per second: 135, episode reward: -64.313, mean reward: -0.858 [-100.000, 11.125], mean action: 1.547 [0.000, 3.000],  loss: 9.907676, mae: 45.421663, mean_q: 45.483596, mean_eps: 0.639238
  60274/150000: episode: 578, duration: 0.833s, episode steps: 109, steps per second: 131, episode reward: -102.822, mean reward: -0.943 [-100.000,  9.203], mean action: 1.477 [0.000, 3.000],  loss: 7.996134, mae: 46.049781, mean_q: 46.465242, mean_eps: 0.638686
  60391/150000: episode: 579, duration: 0.812s, episode steps: 117, steps per second: 144, episode reward: -20.431, mean reward: -0.175 [-100.000, 11.388], mean action: 1.726 [0.000, 3.000],  loss: 8.809433, mae: 45.297846, mean_q: 45.206316, mean_eps: 0.638008
  60631/150000: episode: 580, duration: 1.765s, episode steps: 240, steps per second: 136, episode reward: -80.473, mean reward: -0.335 [-100.000, 86.768], mean action: 1.679 [0.000, 3.000],  loss: 10.235037, mae: 45.798581, mean_q: 45.670382, mean_eps: 0.636937
  60757/150000: episode: 581, duration: 0.856s, episode steps: 126, steps per second: 147, episode reward: -31.598, mean reward: -0.251 [-100.000, 14.103], mean action: 1.690 [0.000, 3.000],  loss: 9.299097, mae: 45.104538, mean_q: 45.402031, mean_eps: 0.635839
  60896/150000: episode: 582, duration: 0.953s, episode steps: 139, steps per second: 146, episode reward: -68.383, mean reward: -0.492 [-100.000, 18.172], mean action: 1.633 [0.000, 3.000],  loss: 11.693616, mae: 45.328959, mean_q: 45.181945, mean_eps: 0.635044
  61007/150000: episode: 583, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward: -23.908, mean reward: -0.215 [-100.000, 11.113], mean action: 1.631 [0.000, 3.000],  loss: 12.255427, mae: 45.714028, mean_q: 47.037048, mean_eps: 0.634294
  61132/150000: episode: 584, duration: 0.879s, episode steps: 125, steps per second: 142, episode reward:  5.951, mean reward:  0.048 [-100.000, 18.809], mean action: 1.840 [0.000, 3.000],  loss: 10.754890, mae: 46.280473, mean_q: 46.441077, mean_eps: 0.633586
  61214/150000: episode: 585, duration: 0.589s, episode steps:  82, steps per second: 139, episode reward: -89.210, mean reward: -1.088 [-100.000, 10.365], mean action: 1.524 [0.000, 3.000],  loss: 10.106669, mae: 45.966900, mean_q: 46.467760, mean_eps: 0.632965
  61423/150000: episode: 586, duration: 1.491s, episode steps: 209, steps per second: 140, episode reward: -120.633, mean reward: -0.577 [-100.000, 13.346], mean action: 1.646 [0.000, 3.000],  loss: 11.150054, mae: 45.983619, mean_q: 47.771788, mean_eps: 0.632092
  61583/150000: episode: 587, duration: 1.334s, episode steps: 160, steps per second: 120, episode reward: 39.741, mean reward:  0.248 [-100.000, 26.240], mean action: 1.738 [0.000, 3.000],  loss: 9.120205, mae: 45.393448, mean_q: 46.708175, mean_eps: 0.630985
  61722/150000: episode: 588, duration: 1.205s, episode steps: 139, steps per second: 115, episode reward: -68.679, mean reward: -0.494 [-100.000,  6.399], mean action: 1.547 [0.000, 3.000],  loss: 12.446679, mae: 46.166856, mean_q: 46.243407, mean_eps: 0.630088
  61829/150000: episode: 589, duration: 0.908s, episode steps: 107, steps per second: 118, episode reward: -73.201, mean reward: -0.684 [-100.000, 13.999], mean action: 1.645 [0.000, 3.000],  loss: 7.860669, mae: 45.210938, mean_q: 45.877037, mean_eps: 0.629350
  61928/150000: episode: 590, duration: 0.800s, episode steps:  99, steps per second: 124, episode reward: -71.879, mean reward: -0.726 [-100.000, 11.692], mean action: 1.737 [0.000, 3.000],  loss: 10.129058, mae: 45.708150, mean_q: 46.586551, mean_eps: 0.628732
  62083/150000: episode: 591, duration: 1.201s, episode steps: 155, steps per second: 129, episode reward: -49.174, mean reward: -0.317 [-100.000, 12.599], mean action: 1.658 [0.000, 3.000],  loss: 10.605178, mae: 46.003275, mean_q: 47.822932, mean_eps: 0.627970
  62242/150000: episode: 592, duration: 1.129s, episode steps: 159, steps per second: 141, episode reward: -55.101, mean reward: -0.347 [-100.000, 13.688], mean action: 1.585 [0.000, 3.000],  loss: 9.054250, mae: 44.950895, mean_q: 46.595175, mean_eps: 0.627028
  62406/150000: episode: 593, duration: 1.389s, episode steps: 164, steps per second: 118, episode reward: -68.742, mean reward: -0.419 [-100.000, 12.281], mean action: 1.707 [0.000, 3.000],  loss: 10.893987, mae: 45.279546, mean_q: 46.088255, mean_eps: 0.626059
  62551/150000: episode: 594, duration: 1.099s, episode steps: 145, steps per second: 132, episode reward: -66.551, mean reward: -0.459 [-100.000, 12.949], mean action: 1.572 [0.000, 3.000],  loss: 12.547982, mae: 45.102767, mean_q: 46.480827, mean_eps: 0.625132
  62678/150000: episode: 595, duration: 0.941s, episode steps: 127, steps per second: 135, episode reward: -35.840, mean reward: -0.282 [-100.000, 19.178], mean action: 1.512 [0.000, 3.000],  loss: 12.759674, mae: 45.151723, mean_q: 46.407320, mean_eps: 0.624316
  62767/150000: episode: 596, duration: 0.738s, episode steps:  89, steps per second: 121, episode reward: -14.037, mean reward: -0.158 [-100.000, 15.235], mean action: 1.618 [0.000, 3.000],  loss: 15.087929, mae: 45.538565, mean_q: 46.495787, mean_eps: 0.623668
  62888/150000: episode: 597, duration: 0.921s, episode steps: 121, steps per second: 131, episode reward: -78.932, mean reward: -0.652 [-100.000, 11.737], mean action: 1.612 [0.000, 3.000],  loss: 11.032389, mae: 46.171577, mean_q: 49.361724, mean_eps: 0.623038
  63057/150000: episode: 598, duration: 1.345s, episode steps: 169, steps per second: 126, episode reward: -62.980, mean reward: -0.373 [-100.000, 11.104], mean action: 1.704 [0.000, 3.000],  loss: 11.604100, mae: 45.301075, mean_q: 46.585071, mean_eps: 0.622168
  63165/150000: episode: 599, duration: 0.778s, episode steps: 108, steps per second: 139, episode reward: -33.014, mean reward: -0.306 [-100.000, 17.128], mean action: 1.722 [0.000, 3.000],  loss: 9.006181, mae: 44.886000, mean_q: 47.568133, mean_eps: 0.621337
  63247/150000: episode: 600, duration: 0.502s, episode steps:  82, steps per second: 163, episode reward: -98.144, mean reward: -1.197 [-100.000, 11.084], mean action: 1.768 [0.000, 3.000],  loss: 8.942818, mae: 44.767214, mean_q: 47.042808, mean_eps: 0.620767
  63422/150000: episode: 601, duration: 1.093s, episode steps: 175, steps per second: 160, episode reward: -255.872, mean reward: -1.462 [-100.000, 39.737], mean action: 1.611 [0.000, 3.000],  loss: 11.330283, mae: 44.657109, mean_q: 47.303948, mean_eps: 0.619996
  63535/150000: episode: 602, duration: 0.713s, episode steps: 113, steps per second: 158, episode reward: -100.822, mean reward: -0.892 [-100.000, 13.605], mean action: 1.531 [0.000, 3.000],  loss: 16.087932, mae: 44.776208, mean_q: 46.868170, mean_eps: 0.619132
  63621/150000: episode: 603, duration: 0.540s, episode steps:  86, steps per second: 159, episode reward: -61.319, mean reward: -0.713 [-100.000,  7.873], mean action: 1.686 [0.000, 3.000],  loss: 11.119327, mae: 43.367646, mean_q: 46.109809, mean_eps: 0.618535
  63754/150000: episode: 604, duration: 0.783s, episode steps: 133, steps per second: 170, episode reward: -147.347, mean reward: -1.108 [-100.000, 11.340], mean action: 1.752 [0.000, 3.000],  loss: 10.567738, mae: 44.026276, mean_q: 46.127986, mean_eps: 0.617878
  63839/150000: episode: 605, duration: 0.508s, episode steps:  85, steps per second: 167, episode reward: -67.627, mean reward: -0.796 [-100.000,  7.678], mean action: 1.565 [0.000, 3.000],  loss: 11.499694, mae: 44.441544, mean_q: 46.649411, mean_eps: 0.617224
  63916/150000: episode: 606, duration: 0.441s, episode steps:  77, steps per second: 175, episode reward: -83.645, mean reward: -1.086 [-100.000,  9.083], mean action: 1.610 [0.000, 3.000],  loss: 13.160528, mae: 43.870278, mean_q: 46.036749, mean_eps: 0.616738
  64038/150000: episode: 607, duration: 0.687s, episode steps: 122, steps per second: 178, episode reward: -14.291, mean reward: -0.117 [-100.000, 15.020], mean action: 1.607 [0.000, 3.000],  loss: 14.270475, mae: 44.296752, mean_q: 46.877993, mean_eps: 0.616141
  64154/150000: episode: 608, duration: 0.685s, episode steps: 116, steps per second: 169, episode reward: -64.423, mean reward: -0.555 [-100.000, 11.410], mean action: 1.612 [0.000, 3.000],  loss: 9.502467, mae: 44.181534, mean_q: 46.484329, mean_eps: 0.615427
  64290/150000: episode: 609, duration: 0.802s, episode steps: 136, steps per second: 170, episode reward: -67.498, mean reward: -0.496 [-100.000, 25.960], mean action: 1.544 [0.000, 3.000],  loss: 9.788684, mae: 44.211004, mean_q: 45.930844, mean_eps: 0.614671
  64414/150000: episode: 610, duration: 0.738s, episode steps: 124, steps per second: 168, episode reward: -45.379, mean reward: -0.366 [-100.000, 14.181], mean action: 1.419 [0.000, 3.000],  loss: 11.920819, mae: 44.375256, mean_q: 46.281294, mean_eps: 0.613891
  64547/150000: episode: 611, duration: 0.829s, episode steps: 133, steps per second: 160, episode reward: -50.151, mean reward: -0.377 [-100.000, 12.513], mean action: 1.549 [0.000, 3.000],  loss: 11.585507, mae: 43.737688, mean_q: 46.392329, mean_eps: 0.613120
  64711/150000: episode: 612, duration: 1.131s, episode steps: 164, steps per second: 145, episode reward: -190.391, mean reward: -1.161 [-100.000, 41.549], mean action: 1.671 [0.000, 3.000],  loss: 11.685146, mae: 44.067517, mean_q: 46.611735, mean_eps: 0.612229
  65078/150000: episode: 613, duration: 2.402s, episode steps: 367, steps per second: 153, episode reward: -143.583, mean reward: -0.391 [-100.000, 16.225], mean action: 1.564 [0.000, 3.000],  loss: 14.974066, mae: 43.831938, mean_q: 47.079668, mean_eps: 0.610636
  65180/150000: episode: 614, duration: 0.612s, episode steps: 102, steps per second: 167, episode reward:  7.093, mean reward:  0.070 [-100.000, 16.684], mean action: 1.559 [0.000, 3.000],  loss: 17.765398, mae: 43.686109, mean_q: 46.754329, mean_eps: 0.609229
  65412/150000: episode: 615, duration: 1.412s, episode steps: 232, steps per second: 164, episode reward: -143.307, mean reward: -0.618 [-100.000, 19.933], mean action: 1.754 [0.000, 3.000],  loss: 16.607600, mae: 43.452613, mean_q: 47.463540, mean_eps: 0.608227
  65538/150000: episode: 616, duration: 0.816s, episode steps: 126, steps per second: 154, episode reward: -50.540, mean reward: -0.401 [-100.000, 11.630], mean action: 1.698 [0.000, 3.000],  loss: 14.511822, mae: 44.373219, mean_q: 47.025975, mean_eps: 0.607153
  65643/150000: episode: 617, duration: 0.696s, episode steps: 105, steps per second: 151, episode reward: -2.024, mean reward: -0.019 [-100.000, 22.448], mean action: 1.657 [0.000, 3.000],  loss: 16.625278, mae: 44.115118, mean_q: 47.129475, mean_eps: 0.606460
  65758/150000: episode: 618, duration: 0.670s, episode steps: 115, steps per second: 172, episode reward: -94.587, mean reward: -0.822 [-100.000,  8.185], mean action: 1.713 [0.000, 3.000],  loss: 10.315463, mae: 43.188626, mean_q: 46.113816, mean_eps: 0.605800
  65880/150000: episode: 619, duration: 0.756s, episode steps: 122, steps per second: 161, episode reward: -1.900, mean reward: -0.016 [-100.000, 10.421], mean action: 1.631 [0.000, 3.000],  loss: 8.779200, mae: 43.847745, mean_q: 47.301123, mean_eps: 0.605089
  66034/150000: episode: 620, duration: 0.938s, episode steps: 154, steps per second: 164, episode reward: -48.839, mean reward: -0.317 [-100.000,  7.811], mean action: 1.701 [0.000, 3.000],  loss: 12.064975, mae: 44.002186, mean_q: 47.652835, mean_eps: 0.604261
  66124/150000: episode: 621, duration: 0.554s, episode steps:  90, steps per second: 163, episode reward: -122.412, mean reward: -1.360 [-100.000, 11.873], mean action: 1.667 [0.000, 3.000],  loss: 9.571256, mae: 43.973004, mean_q: 47.253661, mean_eps: 0.603529
  66251/150000: episode: 622, duration: 0.796s, episode steps: 127, steps per second: 160, episode reward: -33.345, mean reward: -0.263 [-100.000,  6.497], mean action: 1.457 [0.000, 3.000],  loss: 8.869199, mae: 43.946256, mean_q: 47.700142, mean_eps: 0.602878
  66362/150000: episode: 623, duration: 0.706s, episode steps: 111, steps per second: 157, episode reward: -4.004, mean reward: -0.036 [-100.000, 12.437], mean action: 1.613 [0.000, 3.000],  loss: 13.049955, mae: 44.294175, mean_q: 47.278464, mean_eps: 0.602164
  66509/150000: episode: 624, duration: 0.882s, episode steps: 147, steps per second: 167, episode reward: -0.293, mean reward: -0.002 [-100.000, 24.781], mean action: 1.660 [0.000, 3.000],  loss: 10.852128, mae: 43.765720, mean_q: 46.504982, mean_eps: 0.601390
  66613/150000: episode: 625, duration: 0.631s, episode steps: 104, steps per second: 165, episode reward: -77.868, mean reward: -0.749 [-100.000,  6.864], mean action: 1.529 [0.000, 3.000],  loss: 10.018858, mae: 44.031682, mean_q: 46.906739, mean_eps: 0.600637
  66739/150000: episode: 626, duration: 0.717s, episode steps: 126, steps per second: 176, episode reward: -17.186, mean reward: -0.136 [-100.000, 13.036], mean action: 1.516 [0.000, 3.000],  loss: 10.316732, mae: 44.021666, mean_q: 48.085264, mean_eps: 0.599947
  66882/150000: episode: 627, duration: 0.818s, episode steps: 143, steps per second: 175, episode reward: -362.353, mean reward: -2.534 [-100.000, 43.416], mean action: 1.839 [0.000, 3.000],  loss: 14.621194, mae: 44.072088, mean_q: 47.197429, mean_eps: 0.599140
  67034/150000: episode: 628, duration: 0.904s, episode steps: 152, steps per second: 168, episode reward: -29.233, mean reward: -0.192 [-100.000, 18.564], mean action: 1.625 [0.000, 3.000],  loss: 12.531148, mae: 44.586269, mean_q: 48.644985, mean_eps: 0.598255
  67136/150000: episode: 629, duration: 0.586s, episode steps: 102, steps per second: 174, episode reward: -132.976, mean reward: -1.304 [-100.000, 10.024], mean action: 1.765 [0.000, 3.000],  loss: 19.717725, mae: 44.404530, mean_q: 48.242991, mean_eps: 0.597493
  67296/150000: episode: 630, duration: 0.928s, episode steps: 160, steps per second: 173, episode reward:  1.334, mean reward:  0.008 [-100.000, 17.677], mean action: 1.700 [0.000, 3.000],  loss: 15.494411, mae: 44.266868, mean_q: 46.559267, mean_eps: 0.596707
  67413/150000: episode: 631, duration: 0.784s, episode steps: 117, steps per second: 149, episode reward:  7.117, mean reward:  0.061 [-100.000, 12.744], mean action: 1.615 [0.000, 3.000],  loss: 8.456331, mae: 45.089295, mean_q: 48.861428, mean_eps: 0.595876
  67563/150000: episode: 632, duration: 0.974s, episode steps: 150, steps per second: 154, episode reward: -73.839, mean reward: -0.492 [-100.000, 10.480], mean action: 1.680 [0.000, 3.000],  loss: 13.927256, mae: 45.494895, mean_q: 48.122785, mean_eps: 0.595075
  67647/150000: episode: 633, duration: 0.543s, episode steps:  84, steps per second: 155, episode reward: -72.134, mean reward: -0.859 [-100.000, 13.403], mean action: 1.595 [0.000, 3.000],  loss: 11.877858, mae: 45.604131, mean_q: 47.513881, mean_eps: 0.594373
  67784/150000: episode: 634, duration: 0.816s, episode steps: 137, steps per second: 168, episode reward: -25.369, mean reward: -0.185 [-100.000, 13.407], mean action: 1.504 [0.000, 3.000],  loss: 10.680057, mae: 44.708989, mean_q: 46.728536, mean_eps: 0.593710
  68103/150000: episode: 635, duration: 1.951s, episode steps: 319, steps per second: 164, episode reward: -284.210, mean reward: -0.891 [-100.000, 12.990], mean action: 1.690 [0.000, 3.000],  loss: 12.025296, mae: 44.891120, mean_q: 47.562910, mean_eps: 0.592342
  68254/150000: episode: 636, duration: 0.926s, episode steps: 151, steps per second: 163, episode reward: -11.048, mean reward: -0.073 [-100.000, 18.509], mean action: 1.675 [0.000, 3.000],  loss: 11.393719, mae: 45.145142, mean_q: 47.707658, mean_eps: 0.590932
  68369/150000: episode: 637, duration: 0.687s, episode steps: 115, steps per second: 168, episode reward: -90.282, mean reward: -0.785 [-100.000, 15.892], mean action: 1.843 [0.000, 3.000],  loss: 11.050366, mae: 44.408934, mean_q: 46.935649, mean_eps: 0.590134
  68491/150000: episode: 638, duration: 0.708s, episode steps: 122, steps per second: 172, episode reward:  0.431, mean reward:  0.004 [-100.000,  8.271], mean action: 1.664 [0.000, 3.000],  loss: 10.100802, mae: 44.921539, mean_q: 48.451037, mean_eps: 0.589423
  68632/150000: episode: 639, duration: 0.805s, episode steps: 141, steps per second: 175, episode reward: -39.151, mean reward: -0.278 [-100.000,  8.410], mean action: 1.631 [0.000, 3.000],  loss: 12.051794, mae: 44.489913, mean_q: 47.627493, mean_eps: 0.588634
  68764/150000: episode: 640, duration: 0.788s, episode steps: 132, steps per second: 168, episode reward: -185.647, mean reward: -1.406 [-100.000, 51.937], mean action: 1.568 [0.000, 3.000],  loss: 12.016240, mae: 45.190470, mean_q: 48.250268, mean_eps: 0.587815
  69764/150000: episode: 641, duration: 8.065s, episode steps: 1000, steps per second: 124, episode reward:  8.645, mean reward:  0.009 [-24.422, 24.259], mean action: 1.468 [0.000, 3.000],  loss: 11.969718, mae: 45.611539, mean_q: 48.997297, mean_eps: 0.584419
  69888/150000: episode: 642, duration: 0.959s, episode steps: 124, steps per second: 129, episode reward: -8.015, mean reward: -0.065 [-100.000, 19.284], mean action: 1.605 [0.000, 3.000],  loss: 12.750026, mae: 45.787058, mean_q: 48.211956, mean_eps: 0.581047
  69988/150000: episode: 643, duration: 0.765s, episode steps: 100, steps per second: 131, episode reward: -0.242, mean reward: -0.002 [-100.000, 12.631], mean action: 1.680 [0.000, 3.000],  loss: 10.710495, mae: 46.047023, mean_q: 49.238098, mean_eps: 0.580375
  70096/150000: episode: 644, duration: 0.838s, episode steps: 108, steps per second: 129, episode reward: -121.892, mean reward: -1.129 [-100.000,  6.151], mean action: 1.630 [0.000, 3.000],  loss: 11.511251, mae: 45.079473, mean_q: 48.548633, mean_eps: 0.579751
  70200/150000: episode: 645, duration: 0.823s, episode steps: 104, steps per second: 126, episode reward: -95.638, mean reward: -0.920 [-100.000,  9.367], mean action: 1.644 [0.000, 3.000],  loss: 14.487690, mae: 45.585312, mean_q: 48.156071, mean_eps: 0.579115
  70334/150000: episode: 646, duration: 0.991s, episode steps: 134, steps per second: 135, episode reward: 25.127, mean reward:  0.188 [-100.000, 16.596], mean action: 1.672 [0.000, 3.000],  loss: 11.664868, mae: 45.663139, mean_q: 49.381027, mean_eps: 0.578401
  70498/150000: episode: 647, duration: 1.265s, episode steps: 164, steps per second: 130, episode reward: 20.327, mean reward:  0.124 [-100.000, 48.883], mean action: 1.573 [0.000, 3.000],  loss: 10.993505, mae: 45.341022, mean_q: 49.689521, mean_eps: 0.577507
  70565/150000: episode: 648, duration: 0.470s, episode steps:  67, steps per second: 143, episode reward: -8.016, mean reward: -0.120 [-100.000, 21.994], mean action: 1.776 [0.000, 3.000],  loss: 11.402463, mae: 44.638054, mean_q: 47.365682, mean_eps: 0.576814
  70661/150000: episode: 649, duration: 0.694s, episode steps:  96, steps per second: 138, episode reward: -87.474, mean reward: -0.911 [-100.000,  7.747], mean action: 1.583 [0.000, 3.000],  loss: 16.419343, mae: 45.290736, mean_q: 48.667716, mean_eps: 0.576325
  70764/150000: episode: 650, duration: 0.734s, episode steps: 103, steps per second: 140, episode reward: -46.504, mean reward: -0.451 [-100.000, 13.174], mean action: 1.505 [0.000, 3.000],  loss: 13.988314, mae: 44.896225, mean_q: 48.138768, mean_eps: 0.575728
  70893/150000: episode: 651, duration: 0.960s, episode steps: 129, steps per second: 134, episode reward: -61.299, mean reward: -0.475 [-100.000,  7.826], mean action: 1.760 [0.000, 3.000],  loss: 8.538915, mae: 45.019759, mean_q: 48.159564, mean_eps: 0.575032
  71018/150000: episode: 652, duration: 0.902s, episode steps: 125, steps per second: 139, episode reward: -0.777, mean reward: -0.006 [-100.000,  8.132], mean action: 1.696 [0.000, 3.000],  loss: 14.411154, mae: 44.696882, mean_q: 47.809402, mean_eps: 0.574270
  71148/150000: episode: 653, duration: 0.998s, episode steps: 130, steps per second: 130, episode reward: -92.465, mean reward: -0.711 [-100.000,  6.621], mean action: 1.623 [0.000, 3.000],  loss: 18.072484, mae: 45.388010, mean_q: 48.821524, mean_eps: 0.573505
  71245/150000: episode: 654, duration: 0.815s, episode steps:  97, steps per second: 119, episode reward: -69.608, mean reward: -0.718 [-100.000, 16.043], mean action: 1.608 [0.000, 3.000],  loss: 11.651010, mae: 45.856344, mean_q: 49.929089, mean_eps: 0.572824
  71346/150000: episode: 655, duration: 0.763s, episode steps: 101, steps per second: 132, episode reward: -61.209, mean reward: -0.606 [-100.000,  8.335], mean action: 1.604 [0.000, 3.000],  loss: 12.435752, mae: 45.872227, mean_q: 50.208607, mean_eps: 0.572230
  71446/150000: episode: 656, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: -76.693, mean reward: -0.767 [-100.000,  9.469], mean action: 1.640 [0.000, 3.000],  loss: 8.469372, mae: 45.759543, mean_q: 48.339151, mean_eps: 0.571627
  71572/150000: episode: 657, duration: 0.967s, episode steps: 126, steps per second: 130, episode reward: -7.212, mean reward: -0.057 [-100.000, 17.873], mean action: 1.754 [0.000, 3.000],  loss: 14.225542, mae: 45.491740, mean_q: 49.321858, mean_eps: 0.570949
  71728/150000: episode: 658, duration: 1.131s, episode steps: 156, steps per second: 138, episode reward: 10.741, mean reward:  0.069 [-100.000, 13.461], mean action: 1.596 [0.000, 3.000],  loss: 13.633004, mae: 46.228861, mean_q: 49.227483, mean_eps: 0.570103
  71873/150000: episode: 659, duration: 0.986s, episode steps: 145, steps per second: 147, episode reward: -49.392, mean reward: -0.341 [-100.000, 26.923], mean action: 1.676 [0.000, 3.000],  loss: 9.470109, mae: 46.592826, mean_q: 50.199419, mean_eps: 0.569200
  71971/150000: episode: 660, duration: 0.682s, episode steps:  98, steps per second: 144, episode reward: 32.474, mean reward:  0.331 [-100.000, 16.265], mean action: 1.673 [0.000, 3.000],  loss: 9.993137, mae: 46.566826, mean_q: 50.583111, mean_eps: 0.568471
  72093/150000: episode: 661, duration: 0.825s, episode steps: 122, steps per second: 148, episode reward: -8.887, mean reward: -0.073 [-100.000, 69.215], mean action: 1.689 [0.000, 3.000],  loss: 14.181915, mae: 47.677167, mean_q: 51.840392, mean_eps: 0.567811
  72483/150000: episode: 662, duration: 2.826s, episode steps: 390, steps per second: 138, episode reward: 33.006, mean reward:  0.085 [-100.000, 22.090], mean action: 1.726 [0.000, 3.000],  loss: 14.252167, mae: 47.192591, mean_q: 52.122000, mean_eps: 0.566275
  72594/150000: episode: 663, duration: 0.753s, episode steps: 111, steps per second: 147, episode reward: -32.694, mean reward: -0.295 [-100.000, 17.293], mean action: 1.523 [0.000, 3.000],  loss: 14.082111, mae: 48.024285, mean_q: 54.200760, mean_eps: 0.564772
  72748/150000: episode: 664, duration: 1.043s, episode steps: 154, steps per second: 148, episode reward: 20.748, mean reward:  0.135 [-100.000, 18.606], mean action: 1.721 [0.000, 3.000],  loss: 10.941801, mae: 47.844162, mean_q: 54.163174, mean_eps: 0.563977
  72897/150000: episode: 665, duration: 1.029s, episode steps: 149, steps per second: 145, episode reward: -39.033, mean reward: -0.262 [-100.000, 15.802], mean action: 1.698 [0.000, 3.000],  loss: 15.871957, mae: 48.611377, mean_q: 54.099983, mean_eps: 0.563068
  72986/150000: episode: 666, duration: 0.585s, episode steps:  89, steps per second: 152, episode reward: -8.694, mean reward: -0.098 [-100.000, 19.208], mean action: 1.685 [0.000, 3.000],  loss: 13.820984, mae: 48.065736, mean_q: 54.265988, mean_eps: 0.562354
  73067/150000: episode: 667, duration: 0.563s, episode steps:  81, steps per second: 144, episode reward: -72.993, mean reward: -0.901 [-100.000, 21.167], mean action: 1.481 [0.000, 3.000],  loss: 11.059963, mae: 48.284854, mean_q: 54.857769, mean_eps: 0.561844
  73176/150000: episode: 668, duration: 0.767s, episode steps: 109, steps per second: 142, episode reward: -74.562, mean reward: -0.684 [-100.000, 14.254], mean action: 1.615 [0.000, 3.000],  loss: 16.418628, mae: 48.255935, mean_q: 53.644260, mean_eps: 0.561274
  73293/150000: episode: 669, duration: 0.743s, episode steps: 117, steps per second: 158, episode reward: -30.954, mean reward: -0.265 [-100.000, 16.869], mean action: 1.573 [0.000, 3.000],  loss: 11.656096, mae: 48.681855, mean_q: 54.701949, mean_eps: 0.560596
  73405/150000: episode: 670, duration: 0.653s, episode steps: 112, steps per second: 172, episode reward: -39.673, mean reward: -0.354 [-100.000, 16.861], mean action: 1.688 [0.000, 3.000],  loss: 11.560815, mae: 49.117581, mean_q: 55.596379, mean_eps: 0.559909
  73519/150000: episode: 671, duration: 0.668s, episode steps: 114, steps per second: 171, episode reward: -57.581, mean reward: -0.505 [-100.000,  7.876], mean action: 1.632 [0.000, 3.000],  loss: 15.058062, mae: 49.290072, mean_q: 55.942733, mean_eps: 0.559231
  73670/150000: episode: 672, duration: 0.846s, episode steps: 151, steps per second: 178, episode reward: -90.260, mean reward: -0.598 [-100.000,  7.018], mean action: 1.689 [0.000, 3.000],  loss: 17.004388, mae: 49.079749, mean_q: 55.063896, mean_eps: 0.558436
  73866/150000: episode: 673, duration: 1.163s, episode steps: 196, steps per second: 169, episode reward: -59.853, mean reward: -0.305 [-100.000, 10.455], mean action: 1.643 [0.000, 3.000],  loss: 14.894129, mae: 49.300328, mean_q: 56.092254, mean_eps: 0.557395
  73958/150000: episode: 674, duration: 0.520s, episode steps:  92, steps per second: 177, episode reward: -97.332, mean reward: -1.058 [-100.000,  5.047], mean action: 1.630 [0.000, 3.000],  loss: 16.425128, mae: 50.196363, mean_q: 56.746199, mean_eps: 0.556531
  74097/150000: episode: 675, duration: 0.812s, episode steps: 139, steps per second: 171, episode reward: -41.249, mean reward: -0.297 [-100.000, 15.342], mean action: 1.698 [0.000, 3.000],  loss: 12.860274, mae: 50.129118, mean_q: 57.178386, mean_eps: 0.555838
  74211/150000: episode: 676, duration: 0.700s, episode steps: 114, steps per second: 163, episode reward: -5.846, mean reward: -0.051 [-100.000, 14.088], mean action: 1.860 [0.000, 3.000],  loss: 15.788396, mae: 50.246586, mean_q: 57.864123, mean_eps: 0.555079
  74334/150000: episode: 677, duration: 0.693s, episode steps: 123, steps per second: 177, episode reward: -13.346, mean reward: -0.109 [-100.000, 17.411], mean action: 1.650 [0.000, 3.000],  loss: 11.268894, mae: 49.637969, mean_q: 56.584022, mean_eps: 0.554368
  74435/150000: episode: 678, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: -136.579, mean reward: -1.352 [-100.000, 24.667], mean action: 1.683 [0.000, 3.000],  loss: 12.723317, mae: 50.409404, mean_q: 56.471168, mean_eps: 0.553696
  74531/150000: episode: 679, duration: 0.623s, episode steps:  96, steps per second: 154, episode reward: -67.971, mean reward: -0.708 [-100.000,  6.639], mean action: 1.615 [0.000, 3.000],  loss: 12.955477, mae: 50.465883, mean_q: 57.420988, mean_eps: 0.553105
  74627/150000: episode: 680, duration: 0.654s, episode steps:  96, steps per second: 147, episode reward: -42.045, mean reward: -0.438 [-100.000,  9.914], mean action: 1.583 [0.000, 3.000],  loss: 15.959159, mae: 50.985567, mean_q: 57.943550, mean_eps: 0.552529
  74764/150000: episode: 681, duration: 0.913s, episode steps: 137, steps per second: 150, episode reward: -17.936, mean reward: -0.131 [-100.000,  6.152], mean action: 1.686 [0.000, 3.000],  loss: 17.320758, mae: 51.262941, mean_q: 58.292940, mean_eps: 0.551830
  74874/150000: episode: 682, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: -80.155, mean reward: -0.729 [-100.000, 16.227], mean action: 1.827 [0.000, 3.000],  loss: 14.587629, mae: 49.878180, mean_q: 56.674397, mean_eps: 0.551089
  74967/150000: episode: 683, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -42.717, mean reward: -0.459 [-100.000,  7.753], mean action: 1.699 [0.000, 3.000],  loss: 12.110561, mae: 50.438721, mean_q: 57.321244, mean_eps: 0.550480
  75106/150000: episode: 684, duration: 0.894s, episode steps: 139, steps per second: 155, episode reward: -193.571, mean reward: -1.393 [-100.000,  4.790], mean action: 1.763 [0.000, 3.000],  loss: 12.971850, mae: 50.669015, mean_q: 57.511331, mean_eps: 0.549784
  75201/150000: episode: 685, duration: 0.678s, episode steps:  95, steps per second: 140, episode reward: -64.130, mean reward: -0.675 [-100.000, 18.134], mean action: 1.653 [0.000, 3.000],  loss: 13.876100, mae: 50.734742, mean_q: 56.947524, mean_eps: 0.549082
  75305/150000: episode: 686, duration: 0.716s, episode steps: 104, steps per second: 145, episode reward: -88.990, mean reward: -0.856 [-100.000,  8.649], mean action: 1.721 [0.000, 3.000],  loss: 9.374109, mae: 50.719289, mean_q: 57.895227, mean_eps: 0.548485
  75401/150000: episode: 687, duration: 0.638s, episode steps:  96, steps per second: 150, episode reward: -116.834, mean reward: -1.217 [-100.000, 15.303], mean action: 1.594 [0.000, 3.000],  loss: 15.661906, mae: 49.849481, mean_q: 55.828985, mean_eps: 0.547885
  75506/150000: episode: 688, duration: 0.717s, episode steps: 105, steps per second: 146, episode reward: -20.514, mean reward: -0.195 [-100.000, 19.656], mean action: 1.629 [0.000, 3.000],  loss: 14.341091, mae: 51.284646, mean_q: 58.561658, mean_eps: 0.547282
  75588/150000: episode: 689, duration: 0.574s, episode steps:  82, steps per second: 143, episode reward: -66.017, mean reward: -0.805 [-100.000, 14.028], mean action: 1.610 [0.000, 3.000],  loss: 13.273817, mae: 50.631441, mean_q: 57.444653, mean_eps: 0.546721
  75710/150000: episode: 690, duration: 0.824s, episode steps: 122, steps per second: 148, episode reward: -70.667, mean reward: -0.579 [-100.000,  9.411], mean action: 1.795 [0.000, 3.000],  loss: 14.905671, mae: 51.435998, mean_q: 57.660423, mean_eps: 0.546109
  75811/150000: episode: 691, duration: 0.704s, episode steps: 101, steps per second: 144, episode reward: -96.885, mean reward: -0.959 [-100.000,  7.901], mean action: 1.832 [0.000, 3.000],  loss: 17.091400, mae: 50.861278, mean_q: 57.653912, mean_eps: 0.545440
  76533/150000: episode: 692, duration: 5.778s, episode steps: 722, steps per second: 125, episode reward: -115.065, mean reward: -0.159 [-100.000, 15.839], mean action: 1.673 [0.000, 3.000],  loss: 14.542381, mae: 50.795302, mean_q: 58.070960, mean_eps: 0.542971
  76641/150000: episode: 693, duration: 0.835s, episode steps: 108, steps per second: 129, episode reward: -26.644, mean reward: -0.247 [-100.000, 18.547], mean action: 1.537 [0.000, 3.000],  loss: 14.643228, mae: 51.192055, mean_q: 58.022337, mean_eps: 0.540481
  76778/150000: episode: 694, duration: 0.999s, episode steps: 137, steps per second: 137, episode reward: -20.204, mean reward: -0.147 [-100.000, 11.704], mean action: 1.445 [0.000, 3.000],  loss: 13.254141, mae: 52.002438, mean_q: 60.428797, mean_eps: 0.539746
  76859/150000: episode: 695, duration: 0.750s, episode steps:  81, steps per second: 108, episode reward: -49.851, mean reward: -0.615 [-100.000,  9.999], mean action: 1.815 [0.000, 3.000],  loss: 8.455002, mae: 52.457294, mean_q: 61.308369, mean_eps: 0.539092
  76975/150000: episode: 696, duration: 1.197s, episode steps: 116, steps per second:  97, episode reward: -71.600, mean reward: -0.617 [-100.000, 10.591], mean action: 1.879 [0.000, 3.000],  loss: 14.969029, mae: 52.128744, mean_q: 58.809219, mean_eps: 0.538501
  77056/150000: episode: 697, duration: 0.589s, episode steps:  81, steps per second: 137, episode reward: -61.076, mean reward: -0.754 [-100.000,  8.253], mean action: 1.667 [0.000, 3.000],  loss: 10.303325, mae: 52.220503, mean_q: 59.803801, mean_eps: 0.537910
  77170/150000: episode: 698, duration: 0.902s, episode steps: 114, steps per second: 126, episode reward: -58.767, mean reward: -0.516 [-100.000,  9.907], mean action: 1.842 [0.000, 3.000],  loss: 12.462012, mae: 52.418484, mean_q: 59.772882, mean_eps: 0.537325
  77276/150000: episode: 699, duration: 0.892s, episode steps: 106, steps per second: 119, episode reward: -36.306, mean reward: -0.343 [-100.000, 16.559], mean action: 1.632 [0.000, 3.000],  loss: 16.927838, mae: 52.603803, mean_q: 59.239295, mean_eps: 0.536665
  77391/150000: episode: 700, duration: 0.838s, episode steps: 115, steps per second: 137, episode reward: -56.712, mean reward: -0.493 [-100.000, 19.128], mean action: 1.617 [0.000, 3.000],  loss: 13.648066, mae: 52.522868, mean_q: 60.142614, mean_eps: 0.536002
  77884/150000: episode: 701, duration: 3.699s, episode steps: 493, steps per second: 133, episode reward: 15.200, mean reward:  0.031 [-100.000, 18.003], mean action: 1.801 [0.000, 3.000],  loss: 14.044663, mae: 53.064663, mean_q: 60.457307, mean_eps: 0.534178
  78120/150000: episode: 702, duration: 2.136s, episode steps: 236, steps per second: 110, episode reward: -186.313, mean reward: -0.789 [-100.000, 17.453], mean action: 1.712 [0.000, 3.000],  loss: 11.796427, mae: 54.060973, mean_q: 62.307090, mean_eps: 0.531991
  78204/150000: episode: 703, duration: 0.538s, episode steps:  84, steps per second: 156, episode reward: -74.480, mean reward: -0.887 [-100.000,  7.075], mean action: 1.845 [0.000, 3.000],  loss: 11.076564, mae: 54.168089, mean_q: 62.987993, mean_eps: 0.531031
  78354/150000: episode: 704, duration: 1.026s, episode steps: 150, steps per second: 146, episode reward: -87.477, mean reward: -0.583 [-100.000, 33.489], mean action: 1.453 [0.000, 3.000],  loss: 13.770024, mae: 53.961098, mean_q: 62.881592, mean_eps: 0.530329
  78494/150000: episode: 705, duration: 1.030s, episode steps: 140, steps per second: 136, episode reward: -12.872, mean reward: -0.092 [-100.000, 10.608], mean action: 1.864 [0.000, 3.000],  loss: 14.768663, mae: 54.582117, mean_q: 62.955276, mean_eps: 0.529459
  78589/150000: episode: 706, duration: 0.719s, episode steps:  95, steps per second: 132, episode reward: -41.206, mean reward: -0.434 [-100.000, 11.251], mean action: 1.505 [0.000, 3.000],  loss: 14.206839, mae: 54.103551, mean_q: 61.941668, mean_eps: 0.528754
  78949/150000: episode: 707, duration: 2.799s, episode steps: 360, steps per second: 129, episode reward: -184.581, mean reward: -0.513 [-100.000, 17.901], mean action: 1.692 [0.000, 3.000],  loss: 13.402893, mae: 54.495249, mean_q: 63.450780, mean_eps: 0.527389
  79410/150000: episode: 708, duration: 3.805s, episode steps: 461, steps per second: 121, episode reward: -221.844, mean reward: -0.481 [-100.000, 13.068], mean action: 1.714 [0.000, 3.000],  loss: 15.593338, mae: 54.334788, mean_q: 62.900499, mean_eps: 0.524926
  80410/150000: episode: 709, duration: 8.139s, episode steps: 1000, steps per second: 123, episode reward: -91.561, mean reward: -0.092 [-21.021, 19.406], mean action: 1.653 [0.000, 3.000],  loss: 15.442214, mae: 55.146493, mean_q: 64.399458, mean_eps: 0.520543
  80580/150000: episode: 710, duration: 1.447s, episode steps: 170, steps per second: 117, episode reward: -15.827, mean reward: -0.093 [-100.000,  9.912], mean action: 1.476 [0.000, 3.000],  loss: 17.308684, mae: 56.126673, mean_q: 66.852348, mean_eps: 0.517033
  80665/150000: episode: 711, duration: 0.675s, episode steps:  85, steps per second: 126, episode reward: -17.570, mean reward: -0.207 [-100.000, 10.142], mean action: 1.741 [0.000, 3.000],  loss: 15.739404, mae: 56.971842, mean_q: 66.248869, mean_eps: 0.516268
  80737/150000: episode: 712, duration: 0.606s, episode steps:  72, steps per second: 119, episode reward: -7.939, mean reward: -0.110 [-100.000, 13.073], mean action: 1.583 [0.000, 3.000],  loss: 12.862071, mae: 57.208090, mean_q: 68.076726, mean_eps: 0.515797
  80822/150000: episode: 713, duration: 0.717s, episode steps:  85, steps per second: 119, episode reward: -33.373, mean reward: -0.393 [-100.000, 16.556], mean action: 1.635 [0.000, 3.000],  loss: 20.009998, mae: 56.308418, mean_q: 63.556710, mean_eps: 0.515326
  80941/150000: episode: 714, duration: 0.992s, episode steps: 119, steps per second: 120, episode reward: -28.045, mean reward: -0.236 [-100.000, 12.227], mean action: 1.622 [0.000, 3.000],  loss: 14.513409, mae: 56.201840, mean_q: 64.984655, mean_eps: 0.514714
  81048/150000: episode: 715, duration: 0.861s, episode steps: 107, steps per second: 124, episode reward: -16.977, mean reward: -0.159 [-100.000, 17.809], mean action: 1.729 [0.000, 3.000],  loss: 14.318818, mae: 57.781435, mean_q: 67.855665, mean_eps: 0.514036
  81213/150000: episode: 716, duration: 1.233s, episode steps: 165, steps per second: 134, episode reward: 37.388, mean reward:  0.227 [-100.000, 12.429], mean action: 1.739 [0.000, 3.000],  loss: 14.307268, mae: 57.283214, mean_q: 67.333977, mean_eps: 0.513220
  81287/150000: episode: 717, duration: 0.568s, episode steps:  74, steps per second: 130, episode reward: -78.778, mean reward: -1.065 [-100.000, 10.310], mean action: 1.730 [0.000, 3.000],  loss: 13.964645, mae: 57.975352, mean_q: 67.579573, mean_eps: 0.512503
  82287/150000: episode: 718, duration: 8.079s, episode steps: 1000, steps per second: 124, episode reward: 24.628, mean reward:  0.025 [-22.820, 54.236], mean action: 1.627 [0.000, 3.000],  loss: 13.514204, mae: 58.099218, mean_q: 67.991303, mean_eps: 0.509281
  82462/150000: episode: 719, duration: 1.465s, episode steps: 175, steps per second: 119, episode reward:  1.120, mean reward:  0.006 [-100.000, 18.819], mean action: 1.771 [0.000, 3.000],  loss: 14.378911, mae: 57.998851, mean_q: 68.181091, mean_eps: 0.505756
  82592/150000: episode: 720, duration: 1.006s, episode steps: 130, steps per second: 129, episode reward: -51.596, mean reward: -0.397 [-100.000, 12.523], mean action: 1.538 [0.000, 3.000],  loss: 14.803585, mae: 58.363299, mean_q: 68.181267, mean_eps: 0.504841
  83118/150000: episode: 721, duration: 4.176s, episode steps: 526, steps per second: 126, episode reward: -102.393, mean reward: -0.195 [-100.000, 18.766], mean action: 1.812 [0.000, 3.000],  loss: 13.956997, mae: 58.751195, mean_q: 69.147328, mean_eps: 0.502873
  83211/150000: episode: 722, duration: 0.665s, episode steps:  93, steps per second: 140, episode reward: -15.812, mean reward: -0.170 [-100.000, 19.570], mean action: 1.634 [0.000, 3.000],  loss: 20.028402, mae: 59.616874, mean_q: 71.269104, mean_eps: 0.501016
  83434/150000: episode: 723, duration: 1.622s, episode steps: 223, steps per second: 138, episode reward: -0.965, mean reward: -0.004 [-100.000, 19.113], mean action: 1.646 [0.000, 3.000],  loss: 15.859235, mae: 59.692668, mean_q: 71.013283, mean_eps: 0.500068
  83556/150000: episode: 724, duration: 0.901s, episode steps: 122, steps per second: 135, episode reward: -72.892, mean reward: -0.597 [-100.000, 13.474], mean action: 1.566 [0.000, 3.000],  loss: 21.227331, mae: 59.423408, mean_q: 70.827055, mean_eps: 0.499033
  84556/150000: episode: 725, duration: 8.490s, episode steps: 1000, steps per second: 118, episode reward: -28.798, mean reward: -0.029 [-20.866, 25.218], mean action: 1.724 [0.000, 3.000],  loss: 16.589107, mae: 59.127337, mean_q: 71.353764, mean_eps: 0.495667
  84702/150000: episode: 726, duration: 1.159s, episode steps: 146, steps per second: 126, episode reward: -68.506, mean reward: -0.469 [-100.000,  7.294], mean action: 1.616 [0.000, 3.000],  loss: 19.506757, mae: 59.304910, mean_q: 72.600203, mean_eps: 0.492229
  84809/150000: episode: 727, duration: 0.836s, episode steps: 107, steps per second: 128, episode reward: -110.928, mean reward: -1.037 [-100.000,  6.560], mean action: 1.421 [0.000, 3.000],  loss: 15.569060, mae: 59.641572, mean_q: 73.463118, mean_eps: 0.491470
  85809/150000: episode: 728, duration: 8.438s, episode steps: 1000, steps per second: 119, episode reward: -43.601, mean reward: -0.044 [-22.625, 22.609], mean action: 1.648 [0.000, 3.000],  loss: 17.032116, mae: 59.589554, mean_q: 72.835746, mean_eps: 0.488149
  85938/150000: episode: 729, duration: 1.169s, episode steps: 129, steps per second: 110, episode reward: 33.055, mean reward:  0.256 [-100.000, 17.646], mean action: 1.628 [0.000, 3.000],  loss: 15.362896, mae: 58.444031, mean_q: 72.336486, mean_eps: 0.484762
  86019/150000: episode: 730, duration: 0.691s, episode steps:  81, steps per second: 117, episode reward: -37.155, mean reward: -0.459 [-100.000, 12.621], mean action: 2.000 [0.000, 3.000],  loss: 15.735926, mae: 59.369219, mean_q: 72.427295, mean_eps: 0.484132
  86130/150000: episode: 731, duration: 0.933s, episode steps: 111, steps per second: 119, episode reward: -52.266, mean reward: -0.471 [-100.000, 10.868], mean action: 1.712 [0.000, 3.000],  loss: 18.122649, mae: 58.637068, mean_q: 70.770836, mean_eps: 0.483556
  86256/150000: episode: 732, duration: 1.108s, episode steps: 126, steps per second: 114, episode reward: -112.937, mean reward: -0.896 [-100.000,  8.090], mean action: 1.611 [0.000, 3.000],  loss: 18.227858, mae: 58.096830, mean_q: 71.801697, mean_eps: 0.482845
  86444/150000: episode: 733, duration: 1.690s, episode steps: 188, steps per second: 111, episode reward: -38.375, mean reward: -0.204 [-100.000, 12.208], mean action: 1.739 [0.000, 3.000],  loss: 14.677784, mae: 59.214434, mean_q: 72.063661, mean_eps: 0.481903
  87444/150000: episode: 734, duration: 8.426s, episode steps: 1000, steps per second: 119, episode reward: 28.552, mean reward:  0.029 [-24.618, 25.527], mean action: 1.537 [0.000, 3.000],  loss: 18.223088, mae: 59.577457, mean_q: 73.950853, mean_eps: 0.478339
  87574/150000: episode: 735, duration: 0.785s, episode steps: 130, steps per second: 166, episode reward: -73.041, mean reward: -0.562 [-100.000, 15.277], mean action: 1.769 [0.000, 3.000],  loss: 15.345937, mae: 60.564417, mean_q: 76.601862, mean_eps: 0.474949
  87825/150000: episode: 736, duration: 1.824s, episode steps: 251, steps per second: 138, episode reward: -187.038, mean reward: -0.745 [-100.000, 14.627], mean action: 1.825 [0.000, 3.000],  loss: 17.637420, mae: 61.770502, mean_q: 78.354681, mean_eps: 0.473806
  87993/150000: episode: 737, duration: 1.120s, episode steps: 168, steps per second: 150, episode reward: -40.177, mean reward: -0.239 [-100.000, 15.200], mean action: 1.560 [0.000, 3.000],  loss: 19.326781, mae: 61.403055, mean_q: 77.063103, mean_eps: 0.472549
  88112/150000: episode: 738, duration: 0.725s, episode steps: 119, steps per second: 164, episode reward: -8.220, mean reward: -0.069 [-100.000, 13.467], mean action: 1.697 [0.000, 3.000],  loss: 17.519919, mae: 61.903627, mean_q: 78.689277, mean_eps: 0.471688
  88232/150000: episode: 739, duration: 0.878s, episode steps: 120, steps per second: 137, episode reward: -109.746, mean reward: -0.915 [-100.000,  9.630], mean action: 1.525 [0.000, 3.000],  loss: 16.148537, mae: 61.910208, mean_q: 78.130273, mean_eps: 0.470971
  88387/150000: episode: 740, duration: 1.203s, episode steps: 155, steps per second: 129, episode reward: -183.353, mean reward: -1.183 [-100.000,  8.107], mean action: 1.729 [0.000, 3.000],  loss: 20.115425, mae: 62.226378, mean_q: 78.022638, mean_eps: 0.470146
  88488/150000: episode: 741, duration: 0.697s, episode steps: 101, steps per second: 145, episode reward: -12.002, mean reward: -0.119 [-100.000, 37.075], mean action: 1.584 [0.000, 3.000],  loss: 21.237148, mae: 62.833293, mean_q: 79.036899, mean_eps: 0.469378
  89488/150000: episode: 742, duration: 7.741s, episode steps: 1000, steps per second: 129, episode reward: 36.505, mean reward:  0.037 [-26.045, 24.260], mean action: 1.658 [0.000, 3.000],  loss: 20.029647, mae: 63.867749, mean_q: 80.905034, mean_eps: 0.466075
  89619/150000: episode: 743, duration: 0.727s, episode steps: 131, steps per second: 180, episode reward: 13.629, mean reward:  0.104 [-100.000, 17.770], mean action: 1.473 [0.000, 3.000],  loss: 24.163248, mae: 64.710772, mean_q: 82.578020, mean_eps: 0.462682
  89764/150000: episode: 744, duration: 0.878s, episode steps: 145, steps per second: 165, episode reward: -101.765, mean reward: -0.702 [-100.000,  7.572], mean action: 1.690 [0.000, 3.000],  loss: 18.688839, mae: 64.149713, mean_q: 80.902882, mean_eps: 0.461854
  90041/150000: episode: 745, duration: 1.804s, episode steps: 277, steps per second: 154, episode reward: -37.472, mean reward: -0.135 [-100.000, 13.505], mean action: 1.664 [0.000, 3.000],  loss: 20.615711, mae: 65.693494, mean_q: 83.570281, mean_eps: 0.460588
  90276/150000: episode: 746, duration: 1.746s, episode steps: 235, steps per second: 135, episode reward: -49.788, mean reward: -0.212 [-100.000, 13.880], mean action: 1.838 [0.000, 3.000],  loss: 21.707329, mae: 65.819625, mean_q: 83.512703, mean_eps: 0.459052
  91276/150000: episode: 747, duration: 9.678s, episode steps: 1000, steps per second: 103, episode reward: -35.226, mean reward: -0.035 [-22.629, 22.007], mean action: 1.740 [0.000, 3.000],  loss: 19.503746, mae: 65.073003, mean_q: 82.791019, mean_eps: 0.455347
  91525/150000: episode: 748, duration: 2.155s, episode steps: 249, steps per second: 116, episode reward: -196.535, mean reward: -0.789 [-100.000, 13.982], mean action: 1.779 [0.000, 3.000],  loss: 21.250505, mae: 65.345375, mean_q: 83.266713, mean_eps: 0.451600
  91639/150000: episode: 749, duration: 1.033s, episode steps: 114, steps per second: 110, episode reward: -103.881, mean reward: -0.911 [-100.000, 16.820], mean action: 1.737 [0.000, 3.000],  loss: 22.443003, mae: 64.975288, mean_q: 83.055190, mean_eps: 0.450511
  91865/150000: episode: 750, duration: 1.947s, episode steps: 226, steps per second: 116, episode reward: -272.227, mean reward: -1.205 [-100.000, 15.405], mean action: 1.518 [0.000, 3.000],  loss: 21.636428, mae: 65.109058, mean_q: 82.906368, mean_eps: 0.449491
  91996/150000: episode: 751, duration: 1.063s, episode steps: 131, steps per second: 123, episode reward: 29.183, mean reward:  0.223 [-100.000, 17.038], mean action: 1.779 [0.000, 3.000],  loss: 18.892401, mae: 65.484763, mean_q: 82.414437, mean_eps: 0.448420
  92996/150000: episode: 752, duration: 7.427s, episode steps: 1000, steps per second: 135, episode reward: 17.951, mean reward:  0.018 [-21.784, 19.833], mean action: 1.748 [0.000, 3.000],  loss: 21.501275, mae: 65.365329, mean_q: 82.495356, mean_eps: 0.445027
  93126/150000: episode: 753, duration: 0.812s, episode steps: 130, steps per second: 160, episode reward: -63.153, mean reward: -0.486 [-100.000, 10.014], mean action: 1.469 [0.000, 3.000],  loss: 19.887966, mae: 66.716025, mean_q: 84.799305, mean_eps: 0.441637
  93267/150000: episode: 754, duration: 0.919s, episode steps: 141, steps per second: 153, episode reward: -18.042, mean reward: -0.128 [-100.000, 17.318], mean action: 1.546 [0.000, 3.000],  loss: 15.170435, mae: 66.687438, mean_q: 85.418325, mean_eps: 0.440824
  94267/150000: episode: 755, duration: 8.659s, episode steps: 1000, steps per second: 115, episode reward: -66.378, mean reward: -0.066 [-21.034, 24.119], mean action: 1.704 [0.000, 3.000],  loss: 17.664599, mae: 65.753699, mean_q: 83.422906, mean_eps: 0.437401
  94467/150000: episode: 756, duration: 1.932s, episode steps: 200, steps per second: 104, episode reward: -57.002, mean reward: -0.285 [-100.000, 18.800], mean action: 1.625 [0.000, 3.000],  loss: 18.702676, mae: 66.032404, mean_q: 84.338136, mean_eps: 0.433801
  94553/150000: episode: 757, duration: 0.507s, episode steps:  86, steps per second: 170, episode reward: -3.039, mean reward: -0.035 [-100.000, 20.763], mean action: 1.895 [0.000, 3.000],  loss: 17.156284, mae: 66.799260, mean_q: 85.533126, mean_eps: 0.432943
  95553/150000: episode: 758, duration: 7.421s, episode steps: 1000, steps per second: 135, episode reward: -42.817, mean reward: -0.043 [-25.342, 21.035], mean action: 1.700 [0.000, 3.000],  loss: 20.416928, mae: 66.793977, mean_q: 86.058931, mean_eps: 0.429685
  96553/150000: episode: 759, duration: 8.303s, episode steps: 1000, steps per second: 120, episode reward: 51.541, mean reward:  0.052 [-24.289, 23.986], mean action: 1.640 [0.000, 3.000],  loss: 21.956144, mae: 68.230558, mean_q: 88.777353, mean_eps: 0.423685
  96663/150000: episode: 760, duration: 0.933s, episode steps: 110, steps per second: 118, episode reward: 17.661, mean reward:  0.161 [-100.000, 19.515], mean action: 1.736 [0.000, 3.000],  loss: 23.479436, mae: 67.437379, mean_q: 87.477437, mean_eps: 0.420355
  97663/150000: episode: 761, duration: 9.171s, episode steps: 1000, steps per second: 109, episode reward: 37.368, mean reward:  0.037 [-24.672, 25.863], mean action: 1.713 [0.000, 3.000],  loss: 20.249861, mae: 68.149465, mean_q: 88.921102, mean_eps: 0.417025
  97777/150000: episode: 762, duration: 0.719s, episode steps: 114, steps per second: 159, episode reward:  5.954, mean reward:  0.052 [-100.000, 15.715], mean action: 1.798 [0.000, 3.000],  loss: 13.466569, mae: 68.591049, mean_q: 90.115720, mean_eps: 0.413683
  97912/150000: episode: 763, duration: 1.053s, episode steps: 135, steps per second: 128, episode reward: -22.346, mean reward: -0.166 [-100.000,  7.467], mean action: 1.756 [0.000, 3.000],  loss: 19.141120, mae: 69.037436, mean_q: 90.577693, mean_eps: 0.412936
  98121/150000: episode: 764, duration: 1.365s, episode steps: 209, steps per second: 153, episode reward: -23.131, mean reward: -0.111 [-100.000, 11.469], mean action: 1.785 [0.000, 3.000],  loss: 15.505164, mae: 69.469979, mean_q: 90.952680, mean_eps: 0.411904
  99121/150000: episode: 765, duration: 9.715s, episode steps: 1000, steps per second: 103, episode reward: 61.570, mean reward:  0.062 [-24.205, 24.178], mean action: 1.666 [0.000, 3.000],  loss: 19.555971, mae: 68.207136, mean_q: 89.630659, mean_eps: 0.408277
  99245/150000: episode: 766, duration: 0.979s, episode steps: 124, steps per second: 127, episode reward: 25.824, mean reward:  0.208 [-100.000, 10.832], mean action: 1.782 [0.000, 3.000],  loss: 13.685508, mae: 67.134092, mean_q: 87.886400, mean_eps: 0.404905
  99326/150000: episode: 767, duration: 0.684s, episode steps:  81, steps per second: 119, episode reward: -60.778, mean reward: -0.750 [-100.000,  6.775], mean action: 1.580 [0.000, 3.000],  loss: 17.473812, mae: 67.460117, mean_q: 88.708385, mean_eps: 0.404290
  99610/150000: episode: 768, duration: 2.201s, episode steps: 284, steps per second: 129, episode reward: 11.927, mean reward:  0.042 [-100.000,  9.881], mean action: 1.877 [0.000, 3.000],  loss: 19.100488, mae: 67.858398, mean_q: 89.065236, mean_eps: 0.403195
  99697/150000: episode: 769, duration: 0.515s, episode steps:  87, steps per second: 169, episode reward: 42.381, mean reward:  0.487 [-100.000, 17.033], mean action: 1.690 [0.000, 3.000],  loss: 29.831231, mae: 68.040720, mean_q: 89.727740, mean_eps: 0.402082
  99775/150000: episode: 770, duration: 0.443s, episode steps:  78, steps per second: 176, episode reward: -83.057, mean reward: -1.065 [-100.000, 18.777], mean action: 1.910 [0.000, 3.000],  loss: 20.302924, mae: 67.717784, mean_q: 89.501332, mean_eps: 0.401587
  99904/150000: episode: 771, duration: 0.758s, episode steps: 129, steps per second: 170, episode reward: -28.752, mean reward: -0.223 [-100.000, 12.161], mean action: 1.636 [0.000, 3.000],  loss: 24.477935, mae: 67.667830, mean_q: 88.802819, mean_eps: 0.400966
 100904/150000: episode: 772, duration: 6.787s, episode steps: 1000, steps per second: 147, episode reward: 66.123, mean reward:  0.066 [-24.430, 25.739], mean action: 1.518 [0.000, 3.000],  loss: 19.714804, mae: 66.415050, mean_q: 86.762315, mean_eps: 0.397579
 101025/150000: episode: 773, duration: 0.784s, episode steps: 121, steps per second: 154, episode reward: -5.175, mean reward: -0.043 [-100.000, 29.584], mean action: 1.479 [0.000, 3.000],  loss: 20.373934, mae: 64.858744, mean_q: 85.488239, mean_eps: 0.394216
 102025/150000: episode: 774, duration: 7.237s, episode steps: 1000, steps per second: 138, episode reward:  8.758, mean reward:  0.009 [-23.978, 26.438], mean action: 1.631 [0.000, 3.000],  loss: 17.658433, mae: 65.085552, mean_q: 85.674904, mean_eps: 0.390853
 103025/150000: episode: 775, duration: 6.364s, episode steps: 1000, steps per second: 157, episode reward: 17.144, mean reward:  0.017 [-24.767, 22.927], mean action: 1.638 [0.000, 3.000],  loss: 15.792460, mae: 64.340553, mean_q: 84.666703, mean_eps: 0.384853
 103290/150000: episode: 776, duration: 1.547s, episode steps: 265, steps per second: 171, episode reward: 22.084, mean reward:  0.083 [-100.000, 21.013], mean action: 1.766 [0.000, 3.000],  loss: 16.271880, mae: 63.082716, mean_q: 83.014023, mean_eps: 0.381058
 103472/150000: episode: 777, duration: 1.008s, episode steps: 182, steps per second: 181, episode reward:  8.354, mean reward:  0.046 [-100.000, 17.580], mean action: 1.670 [0.000, 3.000],  loss: 16.774095, mae: 64.031038, mean_q: 84.744737, mean_eps: 0.379717
 103608/150000: episode: 778, duration: 0.788s, episode steps: 136, steps per second: 173, episode reward: -38.788, mean reward: -0.285 [-100.000, 19.918], mean action: 1.794 [0.000, 3.000],  loss: 13.604049, mae: 63.522123, mean_q: 83.726667, mean_eps: 0.378763
 104608/150000: episode: 779, duration: 8.738s, episode steps: 1000, steps per second: 114, episode reward: 63.684, mean reward:  0.064 [-24.721, 23.686], mean action: 1.639 [0.000, 3.000],  loss: 13.915060, mae: 62.475022, mean_q: 82.119469, mean_eps: 0.375355
 104767/150000: episode: 780, duration: 1.198s, episode steps: 159, steps per second: 133, episode reward: -57.954, mean reward: -0.364 [-100.000, 10.037], mean action: 1.673 [0.000, 3.000],  loss: 11.968296, mae: 62.117836, mean_q: 81.734958, mean_eps: 0.371878
 105767/150000: episode: 781, duration: 8.603s, episode steps: 1000, steps per second: 116, episode reward: 40.615, mean reward:  0.041 [-24.203, 23.014], mean action: 1.529 [0.000, 3.000],  loss: 14.557390, mae: 59.654360, mean_q: 78.432421, mean_eps: 0.368401
 106767/150000: episode: 782, duration: 6.523s, episode steps: 1000, steps per second: 153, episode reward: 58.617, mean reward:  0.059 [-24.686, 27.874], mean action: 1.580 [0.000, 3.000],  loss: 13.171212, mae: 57.360076, mean_q: 75.370404, mean_eps: 0.362401
 107767/150000: episode: 783, duration: 6.606s, episode steps: 1000, steps per second: 151, episode reward: 41.463, mean reward:  0.041 [-24.561, 23.671], mean action: 1.625 [0.000, 3.000],  loss: 14.181172, mae: 56.541804, mean_q: 74.151955, mean_eps: 0.356401
 108767/150000: episode: 784, duration: 7.546s, episode steps: 1000, steps per second: 133, episode reward: 83.147, mean reward:  0.083 [-24.204, 27.869], mean action: 1.379 [0.000, 3.000],  loss: 10.654295, mae: 55.998615, mean_q: 73.882982, mean_eps: 0.350401
 109767/150000: episode: 785, duration: 7.416s, episode steps: 1000, steps per second: 135, episode reward: 71.065, mean reward:  0.071 [-23.477, 25.861], mean action: 1.630 [0.000, 3.000],  loss: 12.864591, mae: 54.880009, mean_q: 72.541201, mean_eps: 0.344401
 110767/150000: episode: 786, duration: 6.553s, episode steps: 1000, steps per second: 153, episode reward: 43.203, mean reward:  0.043 [-24.894, 25.585], mean action: 1.558 [0.000, 3.000],  loss: 9.948287, mae: 51.860442, mean_q: 68.240894, mean_eps: 0.338401
 111340/150000: episode: 787, duration: 3.549s, episode steps: 573, steps per second: 161, episode reward: -141.585, mean reward: -0.247 [-100.000, 11.217], mean action: 1.750 [0.000, 3.000],  loss: 10.699814, mae: 51.564446, mean_q: 67.551596, mean_eps: 0.333682
 111443/150000: episode: 788, duration: 0.569s, episode steps: 103, steps per second: 181, episode reward: -127.496, mean reward: -1.238 [-100.000, 18.393], mean action: 1.913 [0.000, 3.000],  loss: 12.819460, mae: 50.789572, mean_q: 66.842718, mean_eps: 0.331654
 112443/150000: episode: 789, duration: 6.149s, episode steps: 1000, steps per second: 163, episode reward: 101.818, mean reward:  0.102 [-23.668, 22.906], mean action: 1.385 [0.000, 3.000],  loss: 9.173079, mae: 49.311630, mean_q: 65.304675, mean_eps: 0.328345
 113443/150000: episode: 790, duration: 7.183s, episode steps: 1000, steps per second: 139, episode reward: 103.495, mean reward:  0.103 [-21.237, 22.825], mean action: 1.253 [0.000, 3.000],  loss: 8.652113, mae: 46.936104, mean_q: 62.159366, mean_eps: 0.322345
 113601/150000: episode: 791, duration: 1.267s, episode steps: 158, steps per second: 125, episode reward: -19.203, mean reward: -0.122 [-100.000,  7.937], mean action: 1.570 [0.000, 3.000],  loss: 11.874387, mae: 46.208437, mean_q: 61.071674, mean_eps: 0.318871
 114266/150000: episode: 792, duration: 5.089s, episode steps: 665, steps per second: 131, episode reward: 197.326, mean reward:  0.297 [-19.649, 100.000], mean action: 1.364 [0.000, 3.000],  loss: 7.965512, mae: 45.805115, mean_q: 60.820333, mean_eps: 0.316402
 115266/150000: episode: 793, duration: 7.889s, episode steps: 1000, steps per second: 127, episode reward: -40.898, mean reward: -0.041 [-19.698, 12.061], mean action: 1.603 [0.000, 3.000],  loss: 8.214393, mae: 44.238414, mean_q: 58.503873, mean_eps: 0.311407
 115395/150000: episode: 794, duration: 0.725s, episode steps: 129, steps per second: 178, episode reward: -6.535, mean reward: -0.051 [-100.000,  4.772], mean action: 1.822 [0.000, 3.000],  loss: 5.376417, mae: 43.503086, mean_q: 57.478523, mean_eps: 0.308020
 115596/150000: episode: 795, duration: 1.254s, episode steps: 201, steps per second: 160, episode reward: -70.464, mean reward: -0.351 [-100.000, 12.303], mean action: 1.746 [0.000, 3.000],  loss: 7.711681, mae: 44.461537, mean_q: 58.693542, mean_eps: 0.307030
 115932/150000: episode: 796, duration: 1.976s, episode steps: 336, steps per second: 170, episode reward: -57.345, mean reward: -0.171 [-100.000, 11.859], mean action: 1.821 [0.000, 3.000],  loss: 6.530047, mae: 43.532328, mean_q: 57.100125, mean_eps: 0.305419
 116143/150000: episode: 797, duration: 1.184s, episode steps: 211, steps per second: 178, episode reward: -33.151, mean reward: -0.157 [-100.000, 12.016], mean action: 1.725 [0.000, 3.000],  loss: 11.181604, mae: 43.010162, mean_q: 56.233391, mean_eps: 0.303778
 117143/150000: episode: 798, duration: 7.989s, episode steps: 1000, steps per second: 125, episode reward: 95.622, mean reward:  0.096 [-20.151, 22.288], mean action: 1.251 [0.000, 3.000],  loss: 7.288683, mae: 43.270848, mean_q: 56.766346, mean_eps: 0.300145
 118143/150000: episode: 799, duration: 6.919s, episode steps: 1000, steps per second: 145, episode reward: 31.967, mean reward:  0.032 [-19.289, 21.019], mean action: 1.685 [0.000, 3.000],  loss: 6.911339, mae: 43.016792, mean_q: 56.657009, mean_eps: 0.294145
 119143/150000: episode: 800, duration: 7.491s, episode steps: 1000, steps per second: 134, episode reward: 71.087, mean reward:  0.071 [-19.788, 23.467], mean action: 1.117 [0.000, 3.000],  loss: 6.830184, mae: 42.547380, mean_q: 56.241835, mean_eps: 0.288145
 120143/150000: episode: 801, duration: 6.855s, episode steps: 1000, steps per second: 146, episode reward: 138.126, mean reward:  0.138 [-20.079, 23.369], mean action: 1.264 [0.000, 3.000],  loss: 5.914794, mae: 41.061001, mean_q: 54.388234, mean_eps: 0.282145
 120312/150000: episode: 802, duration: 0.957s, episode steps: 169, steps per second: 177, episode reward: 22.373, mean reward:  0.132 [-100.000, 18.237], mean action: 1.574 [0.000, 3.000],  loss: 4.316862, mae: 40.088019, mean_q: 53.368196, mean_eps: 0.278638
 120461/150000: episode: 803, duration: 0.851s, episode steps: 149, steps per second: 175, episode reward: 10.009, mean reward:  0.067 [-100.000, 14.176], mean action: 1.839 [0.000, 3.000],  loss: 6.261705, mae: 40.214639, mean_q: 53.647758, mean_eps: 0.277684
 121461/150000: episode: 804, duration: 7.012s, episode steps: 1000, steps per second: 143, episode reward: 91.008, mean reward:  0.091 [-19.981, 23.949], mean action: 1.212 [0.000, 3.000],  loss: 6.073841, mae: 39.965915, mean_q: 53.177964, mean_eps: 0.274237
 121612/150000: episode: 805, duration: 0.879s, episode steps: 151, steps per second: 172, episode reward: -23.746, mean reward: -0.157 [-100.000, 11.645], mean action: 1.742 [0.000, 3.000],  loss: 4.901801, mae: 39.990984, mean_q: 53.203927, mean_eps: 0.270784
 121869/150000: episode: 806, duration: 1.674s, episode steps: 257, steps per second: 153, episode reward: -77.750, mean reward: -0.303 [-100.000, 19.667], mean action: 1.899 [0.000, 3.000],  loss: 7.086418, mae: 39.372020, mean_q: 52.615362, mean_eps: 0.269560
 122123/150000: episode: 807, duration: 1.835s, episode steps: 254, steps per second: 138, episode reward: -328.911, mean reward: -1.295 [-100.000, 11.130], mean action: 1.701 [0.000, 3.000],  loss: 8.416476, mae: 39.651784, mean_q: 52.674848, mean_eps: 0.268027
 122324/150000: episode: 808, duration: 1.917s, episode steps: 201, steps per second: 105, episode reward: -230.456, mean reward: -1.147 [-100.000, 10.287], mean action: 1.547 [0.000, 3.000],  loss: 7.991910, mae: 40.062634, mean_q: 53.003057, mean_eps: 0.266662
 123324/150000: episode: 809, duration: 8.302s, episode steps: 1000, steps per second: 120, episode reward:  7.695, mean reward:  0.008 [-22.761, 29.380], mean action: 1.505 [0.000, 3.000],  loss: 10.244959, mae: 39.740999, mean_q: 51.993451, mean_eps: 0.263059
 124324/150000: episode: 810, duration: 8.193s, episode steps: 1000, steps per second: 122, episode reward: 76.762, mean reward:  0.077 [-19.850, 23.034], mean action: 0.895 [0.000, 3.000],  loss: 9.498289, mae: 38.882576, mean_q: 50.984138, mean_eps: 0.257059
 125324/150000: episode: 811, duration: 6.940s, episode steps: 1000, steps per second: 144, episode reward: 80.877, mean reward:  0.081 [-23.250, 20.519], mean action: 1.385 [0.000, 3.000],  loss: 8.341698, mae: 37.364602, mean_q: 48.714506, mean_eps: 0.251059
 126324/150000: episode: 812, duration: 7.371s, episode steps: 1000, steps per second: 136, episode reward: 52.546, mean reward:  0.053 [-22.002, 13.001], mean action: 1.440 [0.000, 3.000],  loss: 9.079216, mae: 37.148711, mean_q: 48.219991, mean_eps: 0.245059
 126908/150000: episode: 813, duration: 3.856s, episode steps: 584, steps per second: 151, episode reward: 205.758, mean reward:  0.352 [-19.773, 100.000], mean action: 1.305 [0.000, 3.000],  loss: 11.260739, mae: 37.449894, mean_q: 48.681813, mean_eps: 0.240307
 127908/150000: episode: 814, duration: 7.055s, episode steps: 1000, steps per second: 142, episode reward: -14.620, mean reward: -0.015 [-18.229, 23.059], mean action: 1.568 [0.000, 3.000],  loss: 9.634060, mae: 36.448518, mean_q: 47.409056, mean_eps: 0.235555
 128690/150000: episode: 815, duration: 5.166s, episode steps: 782, steps per second: 151, episode reward: 190.706, mean reward:  0.244 [-20.216, 100.000], mean action: 0.969 [0.000, 3.000],  loss: 8.449196, mae: 36.213262, mean_q: 47.366059, mean_eps: 0.230209
 129690/150000: episode: 816, duration: 7.661s, episode steps: 1000, steps per second: 131, episode reward: 61.711, mean reward:  0.062 [-19.046, 14.429], mean action: 1.455 [0.000, 3.000],  loss: 8.127148, mae: 36.516194, mean_q: 47.761460, mean_eps: 0.224863
 130690/150000: episode: 817, duration: 7.097s, episode steps: 1000, steps per second: 141, episode reward:  7.208, mean reward:  0.007 [-24.453, 23.145], mean action: 1.558 [0.000, 3.000],  loss: 9.681587, mae: 35.929194, mean_q: 46.822951, mean_eps: 0.218863
 131690/150000: episode: 818, duration: 6.463s, episode steps: 1000, steps per second: 155, episode reward: 91.304, mean reward:  0.091 [-19.564, 22.389], mean action: 1.168 [0.000, 3.000],  loss: 9.389217, mae: 34.558603, mean_q: 45.370555, mean_eps: 0.212863
 131862/150000: episode: 819, duration: 1.113s, episode steps: 172, steps per second: 154, episode reward: -568.780, mean reward: -3.307 [-100.000,  2.456], mean action: 1.453 [0.000, 3.000],  loss: 8.994129, mae: 35.155981, mean_q: 46.405890, mean_eps: 0.209347
 132862/150000: episode: 820, duration: 7.109s, episode steps: 1000, steps per second: 141, episode reward: 96.454, mean reward:  0.096 [-20.688, 21.774], mean action: 1.089 [0.000, 3.000],  loss: 6.040868, mae: 35.682300, mean_q: 47.464314, mean_eps: 0.205831
 133862/150000: episode: 821, duration: 7.064s, episode steps: 1000, steps per second: 142, episode reward: 70.885, mean reward:  0.071 [-19.678, 14.110], mean action: 1.184 [0.000, 3.000],  loss: 8.891834, mae: 34.248632, mean_q: 45.557324, mean_eps: 0.199831
 134452/150000: episode: 822, duration: 3.744s, episode steps: 590, steps per second: 158, episode reward: 264.370, mean reward:  0.448 [-20.454, 100.000], mean action: 0.888 [0.000, 3.000],  loss: 12.477471, mae: 34.118686, mean_q: 45.485504, mean_eps: 0.195061
 134766/150000: episode: 823, duration: 1.940s, episode steps: 314, steps per second: 162, episode reward: 220.967, mean reward:  0.704 [-11.708, 100.000], mean action: 1.334 [0.000, 3.000],  loss: 8.885157, mae: 33.730774, mean_q: 45.160777, mean_eps: 0.192349
 134984/150000: episode: 824, duration: 1.320s, episode steps: 218, steps per second: 165, episode reward: -73.716, mean reward: -0.338 [-100.000, 14.152], mean action: 1.688 [0.000, 3.000],  loss: 5.179518, mae: 34.645099, mean_q: 46.101769, mean_eps: 0.190753
 135303/150000: episode: 825, duration: 2.054s, episode steps: 319, steps per second: 155, episode reward: 47.232, mean reward:  0.148 [-100.000, 14.741], mean action: 1.345 [0.000, 3.000],  loss: 4.477542, mae: 34.246439, mean_q: 45.783710, mean_eps: 0.189142
 136220/150000: episode: 826, duration: 5.716s, episode steps: 917, steps per second: 160, episode reward: 225.973, mean reward:  0.246 [-20.400, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 8.038982, mae: 33.386199, mean_q: 44.713195, mean_eps: 0.185434
 137151/150000: episode: 827, duration: 6.529s, episode steps: 931, steps per second: 143, episode reward: -185.376, mean reward: -0.199 [-100.000, 26.643], mean action: 1.380 [0.000, 3.000],  loss: 8.219797, mae: 33.165493, mean_q: 44.450899, mean_eps: 0.179890
 137786/150000: episode: 828, duration: 4.083s, episode steps: 635, steps per second: 156, episode reward: 232.466, mean reward:  0.366 [-20.400, 100.000], mean action: 0.942 [0.000, 3.000],  loss: 4.799152, mae: 32.515020, mean_q: 43.296685, mean_eps: 0.175192
 138169/150000: episode: 829, duration: 3.078s, episode steps: 383, steps per second: 124, episode reward: 228.760, mean reward:  0.597 [-9.213, 100.000], mean action: 1.287 [0.000, 3.000],  loss: 6.932274, mae: 32.299664, mean_q: 43.183605, mean_eps: 0.172138
 139169/150000: episode: 830, duration: 7.212s, episode steps: 1000, steps per second: 139, episode reward: 120.521, mean reward:  0.121 [-20.262, 23.190], mean action: 1.064 [0.000, 3.000],  loss: 6.500621, mae: 32.798800, mean_q: 43.647495, mean_eps: 0.167989
 139406/150000: episode: 831, duration: 1.703s, episode steps: 237, steps per second: 139, episode reward: -146.995, mean reward: -0.620 [-100.000, 17.479], mean action: 1.688 [0.000, 3.000],  loss: 7.082629, mae: 33.154234, mean_q: 44.183935, mean_eps: 0.164278
 140070/150000: episode: 832, duration: 4.438s, episode steps: 664, steps per second: 150, episode reward: 260.373, mean reward:  0.392 [-18.301, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 7.355407, mae: 34.134766, mean_q: 45.071225, mean_eps: 0.161575
 140679/150000: episode: 833, duration: 3.788s, episode steps: 609, steps per second: 161, episode reward: 244.589, mean reward:  0.402 [-17.729, 100.000], mean action: 1.041 [0.000, 3.000],  loss: 6.220910, mae: 33.397560, mean_q: 44.225991, mean_eps: 0.157756
 141247/150000: episode: 834, duration: 3.528s, episode steps: 568, steps per second: 161, episode reward: 267.716, mean reward:  0.471 [-19.025, 100.000], mean action: 1.218 [0.000, 3.000],  loss: 7.474198, mae: 33.525992, mean_q: 44.505712, mean_eps: 0.154225
 141637/150000: episode: 835, duration: 2.930s, episode steps: 390, steps per second: 133, episode reward: 220.360, mean reward:  0.565 [-9.970, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 8.648917, mae: 33.516244, mean_q: 44.440250, mean_eps: 0.151351
 142145/150000: episode: 836, duration: 3.618s, episode steps: 508, steps per second: 140, episode reward: 224.441, mean reward:  0.442 [-17.543, 100.000], mean action: 1.299 [0.000, 3.000],  loss: 5.693346, mae: 33.217476, mean_q: 44.244148, mean_eps: 0.148657
 142658/150000: episode: 837, duration: 3.135s, episode steps: 513, steps per second: 164, episode reward: 201.441, mean reward:  0.393 [-18.587, 100.000], mean action: 1.405 [0.000, 3.000],  loss: 5.254660, mae: 32.507582, mean_q: 43.715781, mean_eps: 0.145594
 143195/150000: episode: 838, duration: 3.901s, episode steps: 537, steps per second: 138, episode reward: 227.237, mean reward:  0.423 [-17.173, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 4.768819, mae: 32.303672, mean_q: 43.527121, mean_eps: 0.142444
 143774/150000: episode: 839, duration: 3.919s, episode steps: 579, steps per second: 148, episode reward: 220.276, mean reward:  0.380 [-19.599, 100.000], mean action: 1.320 [0.000, 3.000],  loss: 6.536227, mae: 32.513833, mean_q: 43.870113, mean_eps: 0.139096
 144295/150000: episode: 840, duration: 3.291s, episode steps: 521, steps per second: 158, episode reward: 241.206, mean reward:  0.463 [-17.765, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 6.507889, mae: 32.202972, mean_q: 43.408356, mean_eps: 0.135796
 144840/150000: episode: 841, duration: 3.307s, episode steps: 545, steps per second: 165, episode reward: 244.593, mean reward:  0.449 [-17.560, 100.000], mean action: 1.479 [0.000, 3.000],  loss: 6.339080, mae: 32.044927, mean_q: 43.372291, mean_eps: 0.132598
 145725/150000: episode: 842, duration: 6.158s, episode steps: 885, steps per second: 144, episode reward: 203.343, mean reward:  0.230 [-19.661, 100.000], mean action: 1.021 [0.000, 3.000],  loss: 5.445152, mae: 32.659598, mean_q: 44.318344, mean_eps: 0.128308
 146208/150000: episode: 843, duration: 3.328s, episode steps: 483, steps per second: 145, episode reward: 190.904, mean reward:  0.395 [-12.314, 100.000], mean action: 1.404 [0.000, 3.000],  loss: 8.450733, mae: 32.548714, mean_q: 44.073058, mean_eps: 0.124204
 146719/150000: episode: 844, duration: 3.478s, episode steps: 511, steps per second: 147, episode reward: 232.837, mean reward:  0.456 [-17.599, 100.000], mean action: 1.174 [0.000, 3.000],  loss: 7.162698, mae: 33.112972, mean_q: 44.883539, mean_eps: 0.121222
 147719/150000: episode: 845, duration: 7.862s, episode steps: 1000, steps per second: 127, episode reward: 125.652, mean reward:  0.126 [-20.774, 23.340], mean action: 0.831 [0.000, 3.000],  loss: 6.246725, mae: 33.097216, mean_q: 44.963548, mean_eps: 0.116689
 148115/150000: episode: 846, duration: 2.829s, episode steps: 396, steps per second: 140, episode reward: 260.090, mean reward:  0.657 [-2.851, 100.000], mean action: 1.465 [0.000, 3.000],  loss: 3.168181, mae: 33.128010, mean_q: 44.973798, mean_eps: 0.112501
 148480/150000: episode: 847, duration: 2.342s, episode steps: 365, steps per second: 156, episode reward: 239.326, mean reward:  0.656 [-7.422, 100.000], mean action: 1.285 [0.000, 3.000],  loss: 6.812096, mae: 33.133784, mean_q: 45.091621, mean_eps: 0.110218
 148907/150000: episode: 848, duration: 3.163s, episode steps: 427, steps per second: 135, episode reward: 224.994, mean reward:  0.527 [-11.723, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 6.789412, mae: 33.602852, mean_q: 45.665404, mean_eps: 0.107842
 149371/150000: episode: 849, duration: 3.507s, episode steps: 464, steps per second: 132, episode reward: 231.767, mean reward:  0.499 [-18.815, 100.000], mean action: 1.315 [0.000, 3.000],  loss: 6.812945, mae: 33.708007, mean_q: 45.783607, mean_eps: 0.105169
 149904/150000: episode: 850, duration: 4.063s, episode steps: 533, steps per second: 131, episode reward: 211.419, mean reward:  0.397 [-19.305, 100.000], mean action: 1.298 [0.000, 3.000],  loss: 5.076005, mae: 34.338659, mean_q: 46.626147, mean_eps: 0.102178
done, took 1081.057 seconds
Testing for 5 episodes ...
Episode 1: reward: 217.249, steps: 552
Episode 2: reward: 217.063, steps: 498
Episode 3: reward: 208.375, steps: 585
Episode 4: reward: 204.176, steps: 437
Episode 5: reward: 183.917, steps: 485
Testing for 5 episodes ...
Episode 1: reward: 125.835, steps: 1000
Episode 2: reward: 208.659, steps: 528
Episode 3: reward: 171.259, steps: 524
Episode 4: reward: 246.136, steps: 518
Episode 5: reward: 234.293, steps: 450
Testing for 5 episodes ...
Episode 1: reward: 233.277, steps: 495
Episode 2: reward: 244.062, steps: 539
Episode 3: reward: 203.760, steps: 736
Episode 4: reward: 232.413, steps: 409
Episode 5: reward: 244.472, steps: 498
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Testing for 5 episodes ...
Episode 1: reward: 227.290, steps: 433
Episode 2: reward: 203.973, steps: 585
Episode 3: reward: 220.219, steps: 529
Episode 4: reward: 205.124, steps: 518
Episode 5: reward: 217.539, steps: 453
4
Model: "sequential_2"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_2 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_6 (Dense)              (None, 64)                576
_________________________________________________________________
activation_6 (Activation)    (None, 64)                0
_________________________________________________________________
dense_7 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_7 (Activation)    (None, 64)                0
_________________________________________________________________
dense_8 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_8 (Activation)    (None, 32)                0
_________________________________________________________________
dense_9 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_9 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 1000 steps ...
 112/1000: episode: 1, duration: 1.486s, episode steps: 112, steps per second:  75, episode reward: -163.802, mean reward: -1.463 [-100.000,  6.097], mean action: 1.509 [0.000, 3.000],  loss: 0.323055, mae: 5.569778, mean_q: 8.255569, mean_eps: 0.945100
 168/1000: episode: 2, duration: 0.390s, episode steps:  56, steps per second: 144, episode reward: -105.312, mean reward: -1.881 [-100.000,  9.602], mean action: 1.393 [0.000, 3.000],  loss: 31.603168, mae: 10.702860, mean_q: 14.761018, mean_eps: 0.874450
 261/1000: episode: 3, duration: 0.686s, episode steps:  93, steps per second: 136, episode reward: -140.121, mean reward: -1.507 [-100.000, 18.984], mean action: 1.634 [0.000, 3.000],  loss: 17.917283, mae: 18.666870, mean_q: 24.684052, mean_eps: 0.807400
 383/1000: episode: 4, duration: 0.870s, episode steps: 122, steps per second: 140, episode reward: -255.732, mean reward: -2.096 [-100.000, 30.805], mean action: 1.574 [0.000, 3.000],  loss: 10.389740, mae: 32.152209, mean_q: 38.969138, mean_eps: 0.710650
 535/1000: episode: 5, duration: 1.080s, episode steps: 152, steps per second: 141, episode reward: -81.546, mean reward: -0.536 [-100.000, 14.871], mean action: 1.553 [0.000, 3.000],  loss: 15.573657, mae: 24.193232, mean_q: 29.340877, mean_eps: 0.587350
 651/1000: episode: 6, duration: 0.822s, episode steps: 116, steps per second: 141, episode reward: -161.817, mean reward: -1.395 [-100.000,  3.829], mean action: 1.629 [0.000, 3.000],  loss: 3.729729, mae: 23.716255, mean_q: 29.878164, mean_eps: 0.466750
 798/1000: episode: 7, duration: 1.186s, episode steps: 147, steps per second: 124, episode reward: -130.971, mean reward: -0.891 [-100.000,  4.464], mean action: 1.646 [0.000, 3.000],  loss: 10.164038, mae: 32.407621, mean_q: 35.912476, mean_eps: 0.348400
done, took 7.981 seconds
Testing for 5 episodes ...
Episode 1: reward: -347.440, steps: 461
Episode 2: reward: -391.502, steps: 605
Episode 3: reward: -343.745, steps: 331
Episode 4: reward: -392.200, steps: 700
Episode 5: reward: -134.538, steps: 241
4
Model: "sequential_3"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_3 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_10 (Dense)             (None, 64)                576
_________________________________________________________________
activation_10 (Activation)   (None, 64)                0
_________________________________________________________________
dense_11 (Dense)             (None, 64)                4160
_________________________________________________________________
activation_11 (Activation)   (None, 64)                0
_________________________________________________________________
dense_12 (Dense)             (None, 32)                2080
_________________________________________________________________
activation_12 (Activation)   (None, 32)                0
_________________________________________________________________
dense_13 (Dense)             (None, 4)                 132
_________________________________________________________________
activation_13 (Activation)   (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 100000 steps ...
    88/100000: episode: 1, duration: 1.521s, episode steps:  88, steps per second:  58, episode reward: -254.428, mean reward: -2.891 [-100.000, 23.038], mean action: 1.477 [0.000, 3.000],  loss: 0.428155, mae: 1.490533, mean_q: -0.700649, mean_eps: 0.999559
   154/100000: episode: 2, duration: 0.484s, episode steps:  66, steps per second: 136, episode reward: -69.551, mean reward: -1.054 [-100.000,  7.988], mean action: 1.636 [0.000, 3.000],  loss: 9.086438, mae: 9.178504, mean_q: -9.108554, mean_eps: 0.998915
   235/100000: episode: 3, duration: 0.638s, episode steps:  81, steps per second: 127, episode reward: -268.150, mean reward: -3.310 [-100.000,  6.308], mean action: 1.383 [0.000, 3.000],  loss: 9.624062, mae: 14.231822, mean_q: 4.242065, mean_eps: 0.998254
   302/100000: episode: 4, duration: 0.479s, episode steps:  67, steps per second: 140, episode reward: -65.953, mean reward: -0.984 [-100.000, 17.497], mean action: 1.731 [0.000, 3.000],  loss: 11.112758, mae: 31.658462, mean_q: 4.748802, mean_eps: 0.997588
   396/100000: episode: 5, duration: 0.678s, episode steps:  94, steps per second: 139, episode reward: -170.193, mean reward: -1.811 [-100.000,  6.091], mean action: 1.638 [0.000, 3.000],  loss: 13.136009, mae: 42.383523, mean_q: 19.331980, mean_eps: 0.996863
   494/100000: episode: 6, duration: 0.757s, episode steps:  98, steps per second: 129, episode reward: -419.085, mean reward: -4.276 [-100.000,  5.466], mean action: 1.735 [0.000, 3.000],  loss: 18.695237, mae: 95.848650, mean_q: 95.791064, mean_eps: 0.996000
   605/100000: episode: 7, duration: 0.831s, episode steps: 111, steps per second: 134, episode reward: -271.594, mean reward: -2.447 [-100.000,  9.511], mean action: 1.486 [0.000, 3.000],  loss: 109.439692, mae: 147.898226, mean_q: 161.643330, mean_eps: 0.995059
   700/100000: episode: 8, duration: 0.685s, episode steps:  95, steps per second: 139, episode reward: -296.931, mean reward: -3.126 [-100.000, 27.037], mean action: 1.716 [0.000, 3.000],  loss: 41.550047, mae: 143.166611, mean_q: 142.578443, mean_eps: 0.994132
   781/100000: episode: 9, duration: 0.589s, episode steps:  81, steps per second: 138, episode reward: -353.405, mean reward: -4.363 [-100.000, 23.009], mean action: 1.444 [0.000, 3.000],  loss: 107.174584, mae: 144.979937, mean_q: 147.202859, mean_eps: 0.993340
   851/100000: episode: 10, duration: 0.495s, episode steps:  70, steps per second: 141, episode reward: -250.431, mean reward: -3.578 [-100.000, 18.871], mean action: 1.514 [0.000, 3.000],  loss: 681.101808, mae: 155.186631, mean_q: 169.893632, mean_eps: 0.992660
   937/100000: episode: 11, duration: 0.686s, episode steps:  86, steps per second: 125, episode reward: -279.810, mean reward: -3.254 [-100.000,  4.537], mean action: 1.512 [0.000, 3.000],  loss: 12535.723326, mae: 420.499400, mean_q: 618.129797, mean_eps: 0.991958
  1019/100000: episode: 12, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -128.803, mean reward: -1.571 [-100.000,  8.026], mean action: 1.451 [0.000, 3.000],  loss: 238448.092642, mae: 1929.184341, mean_q: 2997.048512, mean_eps: 0.991202
  1119/100000: episode: 13, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: -462.424, mean reward: -4.624 [-100.000,  0.480], mean action: 1.520 [0.000, 3.000],  loss: 1710679.881641, mae: 14342.341362, mean_q: 20358.781021, mean_eps: 0.990384
  1203/100000: episode: 14, duration: 0.591s, episode steps:  84, steps per second: 142, episode reward: -139.307, mean reward: -1.658 [-100.000, 27.842], mean action: 1.488 [0.000, 3.000],  loss: 4915107.486607, mae: 35099.464658, mean_q: 49441.424944, mean_eps: 0.989555
  1283/100000: episode: 15, duration: 0.570s, episode steps:  80, steps per second: 140, episode reward: -195.187, mean reward: -2.440 [-100.000,  5.801], mean action: 1.562 [0.000, 3.000],  loss: 10298685.856250, mae: 72321.768848, mean_q: 101476.033740, mean_eps: 0.988818
  1400/100000: episode: 16, duration: 0.985s, episode steps: 117, steps per second: 119, episode reward: -184.308, mean reward: -1.575 [-100.000,  5.624], mean action: 1.453 [0.000, 3.000],  loss: 15306491.786325, mae: 123855.629541, mean_q: 173935.391159, mean_eps: 0.987931
  1485/100000: episode: 17, duration: 0.767s, episode steps:  85, steps per second: 111, episode reward: -167.383, mean reward: -1.969 [-100.000,  6.805], mean action: 1.506 [0.000, 3.000],  loss: 30160224.564706, mae: 185402.462316, mean_q: 260305.441912, mean_eps: 0.987022
  1578/100000: episode: 18, duration: 0.751s, episode steps:  93, steps per second: 124, episode reward: -321.002, mean reward: -3.452 [-100.000,  5.878], mean action: 1.688 [0.000, 3.000],  loss: 50623910.107527, mae: 259059.579805, mean_q: 363924.252520, mean_eps: 0.986221
  1654/100000: episode: 19, duration: 0.623s, episode steps:  76, steps per second: 122, episode reward: -214.134, mean reward: -2.818 [-100.000, 17.542], mean action: 1.408 [0.000, 3.000],  loss: 78564941.421053, mae: 333165.692229, mean_q: 467677.708882, mean_eps: 0.985460
  1748/100000: episode: 20, duration: 0.743s, episode steps:  94, steps per second: 127, episode reward: -122.648, mean reward: -1.305 [-100.000,  8.704], mean action: 1.574 [0.000, 3.000],  loss: 160981778.297872, mae: 453769.783078, mean_q: 638139.374003, mean_eps: 0.984696
  1829/100000: episode: 21, duration: 0.605s, episode steps:  81, steps per second: 134, episode reward: -49.598, mean reward: -0.612 [-100.000, 13.433], mean action: 1.704 [0.000, 3.000],  loss: 408403953.580247, mae: 702339.825231, mean_q: 982954.039352, mean_eps: 0.983908
  1893/100000: episode: 22, duration: 0.484s, episode steps:  64, steps per second: 132, episode reward: -103.691, mean reward: -1.620 [-100.000,  9.916], mean action: 1.469 [0.000, 3.000],  loss: 500254887.000000, mae: 861309.534180, mean_q: 1206773.924805, mean_eps: 0.983256
  2028/100000: episode: 23, duration: 1.019s, episode steps: 135, steps per second: 132, episode reward: -283.489, mean reward: -2.100 [-100.000, 94.413], mean action: 1.689 [0.000, 3.000],  loss: 969389763.555556, mae: 1285451.134722, mean_q: 1794383.918519, mean_eps: 0.982360
  2090/100000: episode: 24, duration: 0.436s, episode steps:  62, steps per second: 142, episode reward: -81.597, mean reward: -1.316 [-100.000,  7.158], mean action: 1.258 [0.000, 3.000],  loss: 1467488077.419355, mae: 1678866.941532, mean_q: 2336188.243952, mean_eps: 0.981473
  2198/100000: episode: 25, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: -320.162, mean reward: -2.964 [-100.000,  0.385], mean action: 1.426 [0.000, 3.000],  loss: 4062326768.000000, mae: 2027024.307870, mean_q: 2823382.782407, mean_eps: 0.980708
  2295/100000: episode: 26, duration: 0.678s, episode steps:  97, steps per second: 143, episode reward: -392.772, mean reward: -4.049 [-100.000,  0.126], mean action: 1.464 [0.000, 3.000],  loss: 4715855930.721649, mae: 2603068.577320, mean_q: 3616686.561856, mean_eps: 0.979786
  2426/100000: episode: 27, duration: 0.979s, episode steps: 131, steps per second: 134, episode reward: -213.797, mean reward: -1.632 [-100.000, 15.558], mean action: 1.443 [0.000, 3.000],  loss: 6737371226.870229, mae: 3102140.748092, mean_q: 4298672.660305, mean_eps: 0.978760
  2552/100000: episode: 28, duration: 0.908s, episode steps: 126, steps per second: 139, episode reward: -532.120, mean reward: -4.223 [-100.000, 49.330], mean action: 1.437 [0.000, 3.000],  loss: 7194267242.666667, mae: 3593206.398810, mean_q: 4988085.488095, mean_eps: 0.977604
  2623/100000: episode: 29, duration: 0.533s, episode steps:  71, steps per second: 133, episode reward: -274.770, mean reward: -3.870 [-100.000,  2.896], mean action: 1.563 [0.000, 3.000],  loss: 28547763456.000000, mae: 4160750.345070, mean_q: 5780728.253521, mean_eps: 0.976717
  2716/100000: episode: 30, duration: 0.674s, episode steps:  93, steps per second: 138, episode reward: -100.250, mean reward: -1.078 [-100.000,  6.461], mean action: 1.796 [0.000, 3.000],  loss: 34578716696.774193, mae: 4843551.967742, mean_q: 6741139.360215, mean_eps: 0.975979
  2802/100000: episode: 31, duration: 0.636s, episode steps:  86, steps per second: 135, episode reward: -80.114, mean reward: -0.932 [-100.000, 20.276], mean action: 1.395 [0.000, 3.000],  loss: 37146063925.581398, mae: 5461424.843023, mean_q: 7592229.651163, mean_eps: 0.975173
  2865/100000: episode: 32, duration: 0.448s, episode steps:  63, steps per second: 141, episode reward: -108.421, mean reward: -1.721 [-100.000,  8.326], mean action: 1.397 [0.000, 3.000],  loss: 40361960407.365082, mae: 6320027.619048, mean_q: 8788271.849206, mean_eps: 0.974503
  2962/100000: episode: 33, duration: 0.669s, episode steps:  97, steps per second: 145, episode reward: -441.292, mean reward: -4.549 [-100.000,  0.816], mean action: 1.588 [0.000, 3.000],  loss: 31513971791.175259, mae: 7094365.211340, mean_q: 9877921.489691, mean_eps: 0.973783
  3084/100000: episode: 34, duration: 0.871s, episode steps: 122, steps per second: 140, episode reward: -254.598, mean reward: -2.087 [-100.000, 21.405], mean action: 1.533 [0.000, 3.000],  loss: 35699310021.245903, mae: 8561948.852459, mean_q: 11943948.442623, mean_eps: 0.972798
  3211/100000: episode: 35, duration: 0.891s, episode steps: 127, steps per second: 143, episode reward: -228.890, mean reward: -1.802 [-100.000,  2.214], mean action: 1.528 [0.000, 3.000],  loss: 72521654062.362198, mae: 11552700.125984, mean_q: 16135317.874016, mean_eps: 0.971677
  3313/100000: episode: 36, duration: 0.729s, episode steps: 102, steps per second: 140, episode reward: -423.582, mean reward: -4.153 [-100.000,  1.223], mean action: 1.382 [0.000, 3.000],  loss: 148444405719.843140, mae: 16378527.156863, mean_q: 22905034.813725, mean_eps: 0.970646
  3394/100000: episode: 37, duration: 0.583s, episode steps:  81, steps per second: 139, episode reward: -128.576, mean reward: -1.587 [-100.000,  6.578], mean action: 1.420 [0.000, 3.000],  loss: 246623985562.864197, mae: 21008692.012346, mean_q: 29411738.864198, mean_eps: 0.969823
  3456/100000: episode: 38, duration: 0.442s, episode steps:  62, steps per second: 140, episode reward: -239.703, mean reward: -3.866 [-100.000,  5.717], mean action: 1.629 [0.000, 3.000],  loss: 445889517501.935486, mae: 27028398.419355, mean_q: 37848374.258065, mean_eps: 0.969179
  3533/100000: episode: 39, duration: 0.542s, episode steps:  77, steps per second: 142, episode reward: -386.996, mean reward: -5.026 [-100.000,  0.262], mean action: 1.688 [0.000, 3.000],  loss: 657708365956.987061, mae: 32783909.662338, mean_q: 45884723.740260, mean_eps: 0.968554
  3603/100000: episode: 40, duration: 0.513s, episode steps:  70, steps per second: 136, episode reward: -199.905, mean reward: -2.856 [-100.000, 16.827], mean action: 1.557 [0.000, 3.000],  loss: 1129486359873.828613, mae: 40735538.057143, mean_q: 57034266.628571, mean_eps: 0.967892
  3666/100000: episode: 41, duration: 0.477s, episode steps:  63, steps per second: 132, episode reward: -176.344, mean reward: -2.799 [-100.000,  6.165], mean action: 1.635 [0.000, 3.000],  loss: 1625448147366.603271, mae: 50890965.460317, mean_q: 71221143.619048, mean_eps: 0.967294
  3786/100000: episode: 42, duration: 0.939s, episode steps: 120, steps per second: 128, episode reward: -77.238, mean reward: -0.644 [-100.000,  7.373], mean action: 1.450 [0.000, 3.000],  loss: 2439833843575.466797, mae: 67365920.966667, mean_q: 94095056.633333, mean_eps: 0.966471
  3859/100000: episode: 43, duration: 0.577s, episode steps:  73, steps per second: 127, episode reward: -132.169, mean reward: -1.811 [-100.000, 15.534], mean action: 1.247 [0.000, 3.000],  loss: 4205321858454.794434, mae: 88048461.260274, mean_q: 123012435.397260, mean_eps: 0.965602
  3995/100000: episode: 44, duration: 1.033s, episode steps: 136, steps per second: 132, episode reward: -226.427, mean reward: -1.665 [-100.000, 26.470], mean action: 1.603 [0.000, 3.000],  loss: 7618631437131.293945, mae: 121422891.852941, mean_q: 169487676.882353, mean_eps: 0.964661
  4076/100000: episode: 45, duration: 0.586s, episode steps:  81, steps per second: 138, episode reward: -91.190, mean reward: -1.126 [-100.000, 12.059], mean action: 1.531 [0.000, 3.000],  loss: 12059462692510.025391, mae: 162142894.123457, mean_q: 226154592.000000, mean_eps: 0.963685
  4189/100000: episode: 46, duration: 0.819s, episode steps: 113, steps per second: 138, episode reward: -221.615, mean reward: -1.961 [-100.000,  0.815], mean action: 1.460 [0.000, 3.000],  loss: 23905140831114.195312, mae: 209355566.938053, mean_q: 291707844.247788, mean_eps: 0.962812
  4293/100000: episode: 47, duration: 0.868s, episode steps: 104, steps per second: 120, episode reward: -329.178, mean reward: -3.165 [-100.000,  0.968], mean action: 1.577 [0.000, 3.000],  loss: 36688307522008.617188, mae: 291705066.153846, mean_q: 406225774.769231, mean_eps: 0.961835
  4393/100000: episode: 48, duration: 0.968s, episode steps: 100, steps per second: 103, episode reward: -123.238, mean reward: -1.232 [-100.000,  9.292], mean action: 1.500 [0.000, 3.000],  loss: 66416794885160.960938, mae: 382919928.640000, mean_q: 533372180.480000, mean_eps: 0.960917
  4509/100000: episode: 49, duration: 0.939s, episode steps: 116, steps per second: 124, episode reward: -339.047, mean reward: -2.923 [-100.000, 10.835], mean action: 1.448 [0.000, 3.000],  loss: 153833038350547.875000, mae: 509076239.448276, mean_q: 708758647.172414, mean_eps: 0.959946
  4591/100000: episode: 50, duration: 0.644s, episode steps:  82, steps per second: 127, episode reward: -98.038, mean reward: -1.196 [-100.000, 11.760], mean action: 1.402 [0.000, 3.000],  loss: 256815106129120.781250, mae: 651045045.853659, mean_q: 907884194.341463, mean_eps: 0.959054
  4676/100000: episode: 51, duration: 0.793s, episode steps:  85, steps per second: 107, episode reward: -142.453, mean reward: -1.676 [-100.000, 28.295], mean action: 1.529 [0.000, 3.000],  loss: 533495602701155.375000, mae: 825315867.858824, mean_q: 1150638828.423529, mean_eps: 0.958303
  4783/100000: episode: 52, duration: 0.999s, episode steps: 107, steps per second: 107, episode reward: -272.473, mean reward: -2.546 [-100.000,  0.728], mean action: 1.467 [0.000, 3.000],  loss: 1023807598455721.875000, mae: 1074941733.682243, mean_q: 1498752767.401869, mean_eps: 0.957439
  4848/100000: episode: 53, duration: 0.513s, episode steps:  65, steps per second: 127, episode reward: -55.009, mean reward: -0.846 [-100.000, 15.089], mean action: 1.662 [0.000, 3.000],  loss: 1947145524139181.250000, mae: 1423459918.769231, mean_q: 1983208067.938462, mean_eps: 0.956665
  4939/100000: episode: 54, duration: 0.655s, episode steps:  91, steps per second: 139, episode reward: -185.458, mean reward: -2.038 [-100.000,  2.540], mean action: 1.659 [0.000, 3.000],  loss: 3101532566761067.000000, mae: 1679504082.989011, mean_q: 2337327006.945055, mean_eps: 0.955963
  5015/100000: episode: 55, duration: 0.558s, episode steps:  76, steps per second: 136, episode reward: -199.743, mean reward: -2.628 [-100.000, 55.826], mean action: 1.566 [0.000, 3.000],  loss: 5156160359477032.000000, mae: 2108565364.210526, mean_q: 2934206000.842105, mean_eps: 0.955211
  5106/100000: episode: 56, duration: 0.736s, episode steps:  91, steps per second: 124, episode reward: -192.075, mean reward: -2.111 [-100.000,  9.810], mean action: 1.593 [0.000, 3.000],  loss: 9968106773148864.000000, mae: 2476863413.450550, mean_q: 3446795691.604395, mean_eps: 0.954460
  5223/100000: episode: 57, duration: 0.865s, episode steps: 117, steps per second: 135, episode reward: -367.899, mean reward: -3.144 [-100.000,  3.336], mean action: 1.513 [0.000, 3.000],  loss: 15876195758115008.000000, mae: 3200774814.632479, mean_q: 4458826878.905983, mean_eps: 0.953524
  5364/100000: episode: 58, duration: 1.169s, episode steps: 141, steps per second: 121, episode reward: -137.197, mean reward: -0.973 [-100.000,  5.873], mean action: 1.496 [0.000, 3.000],  loss: 38405565002086128.000000, mae: 4436823168.000000, mean_q: 6182240317.730496, mean_eps: 0.952363
  5458/100000: episode: 59, duration: 0.711s, episode steps:  94, steps per second: 132, episode reward: -259.084, mean reward: -2.756 [-100.000,  4.542], mean action: 1.585 [0.000, 3.000],  loss: 58518743913388512.000000, mae: 5632345409.361702, mean_q: 7833614374.127660, mean_eps: 0.951306
  5544/100000: episode: 60, duration: 0.631s, episode steps:  86, steps per second: 136, episode reward: -98.165, mean reward: -1.141 [-100.000, 13.325], mean action: 1.581 [0.000, 3.000],  loss: 97665949219178352.000000, mae: 6719479147.162790, mean_q: 9354143892.837210, mean_eps: 0.950495
  5622/100000: episode: 61, duration: 0.602s, episode steps:  78, steps per second: 130, episode reward: -104.464, mean reward: -1.339 [-100.000,  7.088], mean action: 1.538 [0.000, 3.000],  loss: 99869140082928304.000000, mae: 8071004330.666667, mean_q: 11226639661.948717, mean_eps: 0.949758
  5714/100000: episode: 62, duration: 0.660s, episode steps:  92, steps per second: 139, episode reward: -387.638, mean reward: -4.213 [-100.000,  3.947], mean action: 1.522 [0.000, 3.000],  loss: 213190755181658112.000000, mae: 9342587842.782608, mean_q: 12985195531.130434, mean_eps: 0.948993
  5770/100000: episode: 63, duration: 0.405s, episode steps:  56, steps per second: 138, episode reward: -63.730, mean reward: -1.138 [-100.000, 11.708], mean action: 1.571 [0.000, 3.000],  loss: 372293229642814016.000000, mae: 10852747044.571428, mean_q: 15078752676.571428, mean_eps: 0.948327
  5858/100000: episode: 64, duration: 0.639s, episode steps:  88, steps per second: 138, episode reward: -335.554, mean reward: -3.813 [-100.000,  2.751], mean action: 1.386 [0.000, 3.000],  loss: 423620762488989888.000000, mae: 12069108840.727272, mean_q: 16791810292.363636, mean_eps: 0.947678
  5972/100000: episode: 65, duration: 0.868s, episode steps: 114, steps per second: 131, episode reward: -305.135, mean reward: -2.677 [-100.000,  5.049], mean action: 1.544 [0.000, 3.000],  loss: 459732709144913152.000000, mae: 13402435049.543859, mean_q: 18624618990.035088, mean_eps: 0.946770
  6039/100000: episode: 66, duration: 0.514s, episode steps:  67, steps per second: 130, episode reward: -111.694, mean reward: -1.667 [-100.000, 11.952], mean action: 1.582 [0.000, 3.000],  loss: 665312462845349248.000000, mae: 15022060360.597015, mean_q: 20891078579.582088, mean_eps: 0.945955
  6109/100000: episode: 67, duration: 0.516s, episode steps:  70, steps per second: 136, episode reward: -136.291, mean reward: -1.947 [-100.000,  8.405], mean action: 1.414 [0.000, 3.000],  loss: 916382111515916032.000000, mae: 16704989476.571428, mean_q: 23252102875.428570, mean_eps: 0.945339
  6199/100000: episode: 68, duration: 0.795s, episode steps:  90, steps per second: 113, episode reward: -132.419, mean reward: -1.471 [-100.000,  6.717], mean action: 1.600 [0.000, 3.000],  loss: 1138505507198252544.000000, mae: 18506699138.844444, mean_q: 25726632914.488888, mean_eps: 0.944619
  6302/100000: episode: 69, duration: 0.783s, episode steps: 103, steps per second: 132, episode reward: -414.049, mean reward: -4.020 [-100.000,  0.412], mean action: 1.524 [0.000, 3.000],  loss: 988444970421258112.000000, mae: 20392418522.718445, mean_q: 28335085369.165047, mean_eps: 0.943750
  6379/100000: episode: 70, duration: 0.579s, episode steps:  77, steps per second: 133, episode reward: -367.826, mean reward: -4.777 [-100.000,  1.663], mean action: 1.714 [0.000, 3.000],  loss: 1404082365194009344.000000, mae: 21765483586.493507, mean_q: 30283529841.038960, mean_eps: 0.942940
  6471/100000: episode: 71, duration: 0.863s, episode steps:  92, steps per second: 107, episode reward: -105.969, mean reward: -1.152 [-100.000, 28.565], mean action: 1.413 [0.000, 3.000],  loss: 1775689808503464960.000000, mae: 24107000364.521740, mean_q: 33543492407.652172, mean_eps: 0.942179
  6550/100000: episode: 72, duration: 1.097s, episode steps:  79, steps per second:  72, episode reward: -108.864, mean reward: -1.378 [-100.000, 14.787], mean action: 1.481 [0.000, 3.000],  loss: 2247246056668661248.000000, mae: 27053974644.658226, mean_q: 37648822401.620255, mean_eps: 0.941410
  6628/100000: episode: 73, duration: 0.908s, episode steps:  78, steps per second:  86, episode reward: -105.194, mean reward: -1.349 [-100.000,  5.120], mean action: 1.513 [0.000, 3.000],  loss: 3003532444143142912.000000, mae: 28214478296.615383, mean_q: 39254982078.358971, mean_eps: 0.940704
  6770/100000: episode: 74, duration: 1.096s, episode steps: 142, steps per second: 130, episode reward: -97.898, mean reward: -0.689 [-100.000,  6.916], mean action: 1.507 [0.000, 3.000],  loss: 4224904144431740928.000000, mae: 33871683295.549297, mean_q: 47125257504.450706, mean_eps: 0.939714
  6918/100000: episode: 75, duration: 1.110s, episode steps: 148, steps per second: 133, episode reward: -192.531, mean reward: -1.301 [-100.000,  7.606], mean action: 1.453 [0.000, 3.000],  loss: 5810852212254625792.000000, mae: 40153956462.702705, mean_q: 55780182943.135132, mean_eps: 0.938408
  7000/100000: episode: 76, duration: 0.617s, episode steps:  82, steps per second: 133, episode reward: -128.630, mean reward: -1.569 [-100.000,  5.503], mean action: 1.610 [0.000, 3.000],  loss: 7230055816733284352.000000, mae: 44024880527.609756, mean_q: 61194778923.707314, mean_eps: 0.937373
  7079/100000: episode: 77, duration: 0.690s, episode steps:  79, steps per second: 114, episode reward: -163.372, mean reward: -2.068 [-100.000, 25.233], mean action: 1.582 [0.000, 3.000],  loss: 9366930894663514112.000000, mae: 45172483292.354431, mean_q: 62755526993.012657, mean_eps: 0.936649
  7156/100000: episode: 78, duration: 0.646s, episode steps:  77, steps per second: 119, episode reward: -100.227, mean reward: -1.302 [-100.000, 25.880], mean action: 1.468 [0.000, 3.000],  loss: 9184180198341549056.000000, mae: 49886822213.818184, mean_q: 69394598180.571426, mean_eps: 0.935947
  7246/100000: episode: 79, duration: 0.795s, episode steps:  90, steps per second: 113, episode reward: -360.178, mean reward: -4.002 [-100.000,  5.000], mean action: 1.478 [0.000, 3.000],  loss: 10788065166278205440.000000, mae: 52954134118.400002, mean_q: 73672563143.111115, mean_eps: 0.935196
  7322/100000: episode: 80, duration: 0.682s, episode steps:  76, steps per second: 111, episode reward: -60.044, mean reward: -0.790 [-100.000,  8.734], mean action: 1.684 [0.000, 3.000],  loss: 12295389972459835392.000000, mae: 56214564567.578949, mean_q: 78207434266.947372, mean_eps: 0.934448
  7410/100000: episode: 81, duration: 0.799s, episode steps:  88, steps per second: 110, episode reward: -106.020, mean reward: -1.205 [-100.000, 10.947], mean action: 1.420 [0.000, 3.000],  loss: 13140604786944698368.000000, mae: 59684024785.454544, mean_q: 82985621317.818176, mean_eps: 0.933710
  7542/100000: episode: 82, duration: 1.139s, episode steps: 132, steps per second: 116, episode reward: -314.834, mean reward: -2.385 [-100.000,  3.853], mean action: 1.417 [0.000, 3.000],  loss: 19300593659340574720.000000, mae: 68789445694.060608, mean_q: 95638417066.666672, mean_eps: 0.932721
  7617/100000: episode: 83, duration: 0.574s, episode steps:  75, steps per second: 131, episode reward: -290.635, mean reward: -3.875 [-100.000,  7.718], mean action: 1.467 [0.000, 3.000],  loss: 19720412142079688704.000000, mae: 72281633751.039993, mean_q: 100570402433.706665, mean_eps: 0.931789
  7721/100000: episode: 84, duration: 0.836s, episode steps: 104, steps per second: 124, episode reward: -197.818, mean reward: -1.902 [-100.000, 13.643], mean action: 1.433 [0.000, 3.000],  loss: 20345259917282480128.000000, mae: 73484442742.153839, mean_q: 102188404539.076920, mean_eps: 0.930983
  7807/100000: episode: 85, duration: 0.775s, episode steps:  86, steps per second: 111, episode reward: -398.287, mean reward: -4.631 [-100.000, -0.767], mean action: 1.430 [0.000, 3.000],  loss: 28404616336515366912.000000, mae: 81732790986.418610, mean_q: 113600014240.744186, mean_eps: 0.930129
  7903/100000: episode: 86, duration: 0.720s, episode steps:  96, steps per second: 133, episode reward: -81.571, mean reward: -0.850 [-100.000, 18.062], mean action: 1.333 [0.000, 3.000],  loss: 27811716154338398208.000000, mae: 86407337088.000000, mean_q: 120173121920.000000, mean_eps: 0.929310
  7964/100000: episode: 87, duration: 0.507s, episode steps:  61, steps per second: 120, episode reward: -128.186, mean reward: -2.101 [-100.000,  8.090], mean action: 1.426 [0.000, 3.000],  loss: 34803801752090828800.000000, mae: 88306770591.475403, mean_q: 122854742049.573776, mean_eps: 0.928603
  8061/100000: episode: 88, duration: 0.705s, episode steps:  97, steps per second: 138, episode reward:  5.220, mean reward:  0.054 [-100.000, 120.028], mean action: 1.557 [0.000, 3.000],  loss: 41392657298353061888.000000, mae: 91183117554.804123, mean_q: 126780842575.175262, mean_eps: 0.927892
  8136/100000: episode: 89, duration: 0.539s, episode steps:  75, steps per second: 139, episode reward: -90.783, mean reward: -1.210 [-100.000,  8.062], mean action: 1.533 [0.000, 3.000],  loss: 44343003077844254720.000000, mae: 100023393594.026672, mean_q: 139151963805.013336, mean_eps: 0.927118
  8209/100000: episode: 90, duration: 0.509s, episode steps:  73, steps per second: 143, episode reward: -97.067, mean reward: -1.330 [-100.000, 17.939], mean action: 1.082 [0.000, 3.000],  loss: 49202788528860078080.000000, mae: 106322920995.068497, mean_q: 147948813101.589050, mean_eps: 0.926452
  8307/100000: episode: 91, duration: 0.759s, episode steps:  98, steps per second: 129, episode reward: -1.343, mean reward: -0.014 [-100.000, 101.203], mean action: 1.490 [0.000, 3.000],  loss: 70374479263972999168.000000, mae: 110706319610.775513, mean_q: 154166082079.346924, mean_eps: 0.925683
  8398/100000: episode: 92, duration: 0.653s, episode steps:  91, steps per second: 139, episode reward: -294.050, mean reward: -3.231 [-100.000,  5.058], mean action: 1.352 [0.000, 3.000],  loss: 75412122867897876480.000000, mae: 120503514933.450546, mean_q: 167666945012.747253, mean_eps: 0.924832
  8490/100000: episode: 93, duration: 0.677s, episode steps:  92, steps per second: 136, episode reward: -129.931, mean reward: -1.412 [-100.000, 35.742], mean action: 1.533 [0.000, 3.000],  loss: 87966188971073814528.000000, mae: 130317452332.521744, mean_q: 181391261606.956512, mean_eps: 0.924009
  8609/100000: episode: 94, duration: 0.906s, episode steps: 119, steps per second: 131, episode reward: -309.146, mean reward: -2.598 [-100.000, 10.058], mean action: 1.504 [0.000, 3.000],  loss: 91840677170840813568.000000, mae: 137789965045.243683, mean_q: 191819182527.462189, mean_eps: 0.923059
  8728/100000: episode: 95, duration: 0.965s, episode steps: 119, steps per second: 123, episode reward: -142.737, mean reward: -1.199 [-100.000,  5.514], mean action: 1.261 [0.000, 3.000],  loss: 120989803897635831808.000000, mae: 147720515962.621857, mean_q: 205563795369.949585, mean_eps: 0.921988
  8837/100000: episode: 96, duration: 0.889s, episode steps: 109, steps per second: 123, episode reward: -158.915, mean reward: -1.458 [-100.000, 19.321], mean action: 1.450 [0.000, 3.000],  loss: 147553375058079121408.000000, mae: 155487648589.504578, mean_q: 216165454988.917419, mean_eps: 0.920962
  8896/100000: episode: 97, duration: 0.441s, episode steps:  59, steps per second: 134, episode reward: -145.097, mean reward: -2.459 [-100.000,  7.913], mean action: 1.559 [0.000, 3.000],  loss: 127868933670765543424.000000, mae: 160393573948.745758, mean_q: 223279267006.915253, mean_eps: 0.920206
  8999/100000: episode: 98, duration: 0.746s, episode steps: 103, steps per second: 138, episode reward: -239.625, mean reward: -2.326 [-100.000, 55.668], mean action: 1.573 [0.000, 3.000],  loss: 167815299174589661184.000000, mae: 172828422979.106781, mean_q: 240554734005.436890, mean_eps: 0.919477
  9072/100000: episode: 99, duration: 0.542s, episode steps:  73, steps per second: 135, episode reward: -90.719, mean reward: -1.243 [-100.000,  6.481], mean action: 1.562 [0.000, 3.000],  loss: 213598554160209592320.000000, mae: 189755589954.630127, mean_q: 263935720349.808228, mean_eps: 0.918685
  9155/100000: episode: 100, duration: 0.626s, episode steps:  83, steps per second: 132, episode reward: -213.506, mean reward: -2.572 [-100.000,  6.430], mean action: 1.446 [0.000, 3.000],  loss: 199886501343357370368.000000, mae: 195280096342.361450, mean_q: 271626052077.493988, mean_eps: 0.917983
  9279/100000: episode: 101, duration: 0.905s, episode steps: 124, steps per second: 137, episode reward: -141.291, mean reward: -1.139 [-100.000,  5.733], mean action: 1.387 [0.000, 3.000],  loss: 213631844259364044800.000000, mae: 208515538019.096771, mean_q: 289924613153.032288, mean_eps: 0.917052
  9371/100000: episode: 102, duration: 0.698s, episode steps:  92, steps per second: 132, episode reward: -360.063, mean reward: -3.914 [-100.000,  0.628], mean action: 1.663 [0.000, 3.000],  loss: 246232666860500254720.000000, mae: 210735818395.826080, mean_q: 292920817574.956543, mean_eps: 0.916080
  9453/100000: episode: 103, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -101.046, mean reward: -1.232 [-100.000, 19.254], mean action: 1.451 [0.000, 3.000],  loss: 277434504789583134720.000000, mae: 213221783701.853668, mean_q: 296681329938.731689, mean_eps: 0.915296
  9544/100000: episode: 104, duration: 0.640s, episode steps:  91, steps per second: 142, episode reward: -141.491, mean reward: -1.555 [-100.000, 11.196], mean action: 1.099 [0.000, 3.000],  loss: 277037369722854440960.000000, mae: 233610194898.989014, mean_q: 325306971507.340637, mean_eps: 0.914518
  9659/100000: episode: 105, duration: 0.827s, episode steps: 115, steps per second: 139, episode reward: -183.997, mean reward: -1.600 [-100.000,  6.880], mean action: 1.339 [0.000, 3.000],  loss: 311642783743928631296.000000, mae: 244885497303.930420, mean_q: 340683336125.217407, mean_eps: 0.913591
  9735/100000: episode: 106, duration: 0.601s, episode steps:  76, steps per second: 127, episode reward: -174.906, mean reward: -2.301 [-100.000, 49.217], mean action: 1.474 [0.000, 3.000],  loss: 408366085107994525696.000000, mae: 251632887053.473694, mean_q: 350124020574.315796, mean_eps: 0.912731
  9833/100000: episode: 107, duration: 0.763s, episode steps:  98, steps per second: 129, episode reward: -434.016, mean reward: -4.429 [-100.000,  0.164], mean action: 1.714 [0.000, 3.000],  loss: 469174412221023846400.000000, mae: 263695389089.959198, mean_q: 366821313306.122437, mean_eps: 0.911948
  9922/100000: episode: 108, duration: 0.643s, episode steps:  89, steps per second: 138, episode reward: -71.412, mean reward: -0.802 [-100.000,  8.294], mean action: 1.562 [0.000, 3.000],  loss: 385337512362045341696.000000, mae: 283160796539.685364, mean_q: 394224474963.415710, mean_eps: 0.911107
 10030/100000: episode: 109, duration: 0.878s, episode steps: 108, steps per second: 123, episode reward: -94.027, mean reward: -0.871 [-100.000, 17.917], mean action: 1.620 [0.000, 3.000],  loss: 424679138934937354240.000000, mae: 280316552381.629639, mean_q: 390120071774.814819, mean_eps: 0.910220
 10106/100000: episode: 110, duration: 0.553s, episode steps:  76, steps per second: 137, episode reward: -236.070, mean reward: -3.106 [-100.000,  5.600], mean action: 1.329 [0.000, 3.000],  loss: 539310926098389860352.000000, mae: 304276022541.473694, mean_q: 423632292378.947388, mean_eps: 0.909393
 10169/100000: episode: 111, duration: 0.454s, episode steps:  63, steps per second: 139, episode reward: -104.282, mean reward: -1.655 [-100.000,  7.616], mean action: 1.667 [0.000, 3.000],  loss: 528971007135197954048.000000, mae: 293494218882.031738, mean_q: 408558112833.015869, mean_eps: 0.908767
 10269/100000: episode: 112, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: -365.219, mean reward: -3.652 [-100.000, 105.257], mean action: 1.690 [0.000, 3.000],  loss: 702336854645559918592.000000, mae: 325778919915.520020, mean_q: 453297110712.320007, mean_eps: 0.908034
 10359/100000: episode: 113, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -127.844, mean reward: -1.420 [-100.000,  8.254], mean action: 1.256 [0.000, 3.000],  loss: 744713040156598599680.000000, mae: 342566675921.454529, mean_q: 476251246219.636353, mean_eps: 0.907179
 10436/100000: episode: 114, duration: 0.554s, episode steps:  77, steps per second: 139, episode reward: -102.458, mean reward: -1.331 [-100.000,  5.600], mean action: 1.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.906427
 10568/100000: episode: 115, duration: 1.081s, episode steps: 132, steps per second: 122, episode reward: -221.012, mean reward: -1.674 [-100.000, 78.889], mean action: 1.341 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.905487
 10665/100000: episode: 116, duration: 0.898s, episode steps:  97, steps per second: 108, episode reward: -267.734, mean reward: -2.760 [-100.000,  0.744], mean action: 1.227 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.904456
 10757/100000: episode: 117, duration: 0.774s, episode steps:  92, steps per second: 119, episode reward: -228.461, mean reward: -2.483 [-100.000, 47.888], mean action: 1.261 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.903605
 10835/100000: episode: 118, duration: 0.663s, episode steps:  78, steps per second: 118, episode reward: -117.827, mean reward: -1.511 [-100.000, 16.991], mean action: 1.487 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.902841
 10943/100000: episode: 119, duration: 0.810s, episode steps: 108, steps per second: 133, episode reward: -15.332, mean reward: -0.142 [-100.000, 76.909], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.902004
 11010/100000: episode: 120, duration: 0.505s, episode steps:  67, steps per second: 133, episode reward: -13.418, mean reward: -0.200 [-100.000, 82.726], mean action: 1.060 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.901216
 11070/100000: episode: 121, duration: 0.542s, episode steps:  60, steps per second: 111, episode reward: -107.654, mean reward: -1.794 [-100.000, 10.834], mean action: 1.217 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.900644
 11176/100000: episode: 122, duration: 0.880s, episode steps: 106, steps per second: 120, episode reward: -400.228, mean reward: -3.776 [-100.000,  0.729], mean action: 1.198 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.899897
 11258/100000: episode: 123, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -135.497, mean reward: -1.652 [-100.000,  6.289], mean action: 1.232 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.899051
 11385/100000: episode: 124, duration: 1.009s, episode steps: 127, steps per second: 126, episode reward: -117.695, mean reward: -0.927 [-100.000, 41.686], mean action: 1.276 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.898111
 11488/100000: episode: 125, duration: 0.788s, episode steps: 103, steps per second: 131, episode reward: -395.148, mean reward: -3.836 [-100.000,  1.130], mean action: 1.359 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.897076
 11558/100000: episode: 126, duration: 0.518s, episode steps:  70, steps per second: 135, episode reward: -176.714, mean reward: -2.524 [-100.000,  6.087], mean action: 1.329 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.896297
 11658/100000: episode: 127, duration: 0.750s, episode steps: 100, steps per second: 133, episode reward: -213.836, mean reward: -2.138 [-100.000,  7.406], mean action: 1.280 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.895533
 11720/100000: episode: 128, duration: 0.540s, episode steps:  62, steps per second: 115, episode reward: -86.007, mean reward: -1.387 [-100.000,  6.600], mean action: 0.968 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.894803
 11846/100000: episode: 129, duration: 1.192s, episode steps: 126, steps per second: 106, episode reward: -130.284, mean reward: -1.034 [-100.000,  6.367], mean action: 1.405 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.893958
 11943/100000: episode: 130, duration: 0.752s, episode steps:  97, steps per second: 129, episode reward: -145.984, mean reward: -1.505 [-100.000,  6.162], mean action: 1.485 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.892954
 12073/100000: episode: 131, duration: 0.972s, episode steps: 130, steps per second: 134, episode reward: -13.751, mean reward: -0.106 [-100.000, 93.991], mean action: 1.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.891932
 12169/100000: episode: 132, duration: 0.753s, episode steps:  96, steps per second: 127, episode reward: -325.712, mean reward: -3.393 [-100.000,  4.837], mean action: 1.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.890916
 12252/100000: episode: 133, duration: 0.690s, episode steps:  83, steps per second: 120, episode reward: -170.552, mean reward: -2.055 [-100.000,  7.214], mean action: 1.482 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.890110
 12317/100000: episode: 134, duration: 0.564s, episode steps:  65, steps per second: 115, episode reward: -78.922, mean reward: -1.214 [-100.000, 12.709], mean action: 1.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.889444
 12375/100000: episode: 135, duration: 0.427s, episode steps:  58, steps per second: 136, episode reward: -126.196, mean reward: -2.176 [-100.000, 13.000], mean action: 1.069 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.888891
 12499/100000: episode: 136, duration: 1.054s, episode steps: 124, steps per second: 118, episode reward: -110.006, mean reward: -0.887 [-100.000, 18.268], mean action: 1.355 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.888072
 12562/100000: episode: 137, duration: 0.570s, episode steps:  63, steps per second: 111, episode reward: -78.064, mean reward: -1.239 [-100.000,  8.044], mean action: 1.492 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.887230
 12661/100000: episode: 138, duration: 0.812s, episode steps:  99, steps per second: 122, episode reward: -442.438, mean reward: -4.469 [-100.000,  1.681], mean action: 1.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.886501
 12744/100000: episode: 139, duration: 0.681s, episode steps:  83, steps per second: 122, episode reward: -269.975, mean reward: -3.253 [-100.000, 104.098], mean action: 1.349 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.885682
 12828/100000: episode: 140, duration: 0.681s, episode steps:  84, steps per second: 123, episode reward: -290.525, mean reward: -3.459 [-100.000,  5.251], mean action: 1.345 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.884931
 12923/100000: episode: 141, duration: 0.788s, episode steps:  95, steps per second: 121, episode reward: -113.372, mean reward: -1.193 [-100.000, 19.146], mean action: 1.274 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.884125
 13000/100000: episode: 142, duration: 0.591s, episode steps:  77, steps per second: 130, episode reward: -42.866, mean reward: -0.557 [-100.000, 67.668], mean action: 1.377 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.883351
 13089/100000: episode: 143, duration: 0.630s, episode steps:  89, steps per second: 141, episode reward: -201.137, mean reward: -2.260 [-100.000, 68.792], mean action: 1.303 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.882604
 13163/100000: episode: 144, duration: 0.577s, episode steps:  74, steps per second: 128, episode reward: -184.168, mean reward: -2.489 [-100.000, 24.579], mean action: 1.297 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.881870
 13222/100000: episode: 145, duration: 0.451s, episode steps:  59, steps per second: 131, episode reward: -102.878, mean reward: -1.744 [-100.000,  6.702], mean action: 1.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.881272
 13291/100000: episode: 146, duration: 0.555s, episode steps:  69, steps per second: 124, episode reward: -83.231, mean reward: -1.206 [-100.000, 22.953], mean action: 1.116 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.880696
 13354/100000: episode: 147, duration: 0.488s, episode steps:  63, steps per second: 129, episode reward: -93.964, mean reward: -1.491 [-100.000, 11.487], mean action: 1.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.880102
 13456/100000: episode: 148, duration: 0.750s, episode steps: 102, steps per second: 136, episode reward: -188.815, mean reward: -1.851 [-100.000, 14.477], mean action: 1.294 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.879360
 13522/100000: episode: 149, duration: 0.512s, episode steps:  66, steps per second: 129, episode reward: -82.177, mean reward: -1.245 [-100.000, 17.145], mean action: 1.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.878603
 13583/100000: episode: 150, duration: 0.454s, episode steps:  61, steps per second: 134, episode reward: -109.558, mean reward: -1.796 [-100.000,  9.249], mean action: 1.295 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.878032
 13672/100000: episode: 151, duration: 0.665s, episode steps:  89, steps per second: 134, episode reward: -84.619, mean reward: -0.951 [-100.000, 45.631], mean action: 1.573 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.877357
 13756/100000: episode: 152, duration: 0.620s, episode steps:  84, steps per second: 135, episode reward: -118.592, mean reward: -1.412 [-100.000,  8.621], mean action: 1.214 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.876579
 13817/100000: episode: 153, duration: 0.486s, episode steps:  61, steps per second: 125, episode reward: -133.302, mean reward: -2.185 [-100.000,  8.487], mean action: 1.082 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.875926
 13892/100000: episode: 154, duration: 0.542s, episode steps:  75, steps per second: 138, episode reward: -209.166, mean reward: -2.789 [-100.000,  4.847], mean action: 1.013 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.875314
 13972/100000: episode: 155, duration: 0.569s, episode steps:  80, steps per second: 141, episode reward: -101.035, mean reward: -1.263 [-100.000,  8.860], mean action: 1.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.874617
 14034/100000: episode: 156, duration: 0.449s, episode steps:  62, steps per second: 138, episode reward: -239.648, mean reward: -3.865 [-100.000, 47.904], mean action: 1.161 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.873977
 14113/100000: episode: 157, duration: 0.625s, episode steps:  79, steps per second: 126, episode reward: -134.412, mean reward: -1.701 [-100.000, 65.116], mean action: 1.468 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.873343
 14189/100000: episode: 158, duration: 0.560s, episode steps:  76, steps per second: 136, episode reward: -110.629, mean reward: -1.456 [-100.000, 11.097], mean action: 1.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.872645
 14298/100000: episode: 159, duration: 0.781s, episode steps: 109, steps per second: 139, episode reward: -137.297, mean reward: -1.260 [-100.000, 34.694], mean action: 1.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.871813
 14360/100000: episode: 160, duration: 0.469s, episode steps:  62, steps per second: 132, episode reward: -122.893, mean reward: -1.982 [-100.000,  8.675], mean action: 1.226 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.871044
 14492/100000: episode: 161, duration: 1.003s, episode steps: 132, steps per second: 132, episode reward: -119.377, mean reward: -0.904 [-100.000,  5.569], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.870170
 14555/100000: episode: 162, duration: 0.465s, episode steps:  63, steps per second: 136, episode reward: -89.610, mean reward: -1.422 [-100.000, 12.885], mean action: 1.159 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.869293
 14625/100000: episode: 163, duration: 0.596s, episode steps:  70, steps per second: 117, episode reward: -100.716, mean reward: -1.439 [-100.000,  9.567], mean action: 1.314 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.868695
 14683/100000: episode: 164, duration: 0.545s, episode steps:  58, steps per second: 107, episode reward: -78.429, mean reward: -1.352 [-100.000, 18.391], mean action: 1.138 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.868119
 14758/100000: episode: 165, duration: 0.595s, episode steps:  75, steps per second: 126, episode reward: -143.939, mean reward: -1.919 [-100.000,  8.737], mean action: 1.120 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.867520
 14872/100000: episode: 166, duration: 0.850s, episode steps: 114, steps per second: 134, episode reward: -279.439, mean reward: -2.451 [-100.000, 49.929], mean action: 1.325 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.866670
 14940/100000: episode: 167, duration: 0.540s, episode steps:  68, steps per second: 126, episode reward: -75.893, mean reward: -1.116 [-100.000,  7.805], mean action: 1.221 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.865850
 15029/100000: episode: 168, duration: 0.925s, episode steps:  89, steps per second:  96, episode reward: -135.237, mean reward: -1.520 [-100.000,  6.673], mean action: 1.247 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.865144
 15105/100000: episode: 169, duration: 0.546s, episode steps:  76, steps per second: 139, episode reward: -86.486, mean reward: -1.138 [-100.000, 18.733], mean action: 1.382 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.864401
 15201/100000: episode: 170, duration: 0.720s, episode steps:  96, steps per second: 133, episode reward: -481.265, mean reward: -5.013 [-100.000,  5.309], mean action: 1.312 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.863627
 15296/100000: episode: 171, duration: 0.704s, episode steps:  95, steps per second: 135, episode reward: -222.943, mean reward: -2.347 [-100.000, 18.385], mean action: 1.242 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.862768
 15385/100000: episode: 172, duration: 0.644s, episode steps:  89, steps per second: 138, episode reward: -276.119, mean reward: -3.102 [-100.000, 124.658], mean action: 1.416 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.861940
 15469/100000: episode: 173, duration: 0.627s, episode steps:  84, steps per second: 134, episode reward: -285.421, mean reward: -3.398 [-100.000, 23.070], mean action: 1.262 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.861162
 15556/100000: episode: 174, duration: 0.667s, episode steps:  87, steps per second: 130, episode reward: -119.329, mean reward: -1.372 [-100.000, 11.691], mean action: 1.230 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.860392
 15623/100000: episode: 175, duration: 0.510s, episode steps:  67, steps per second: 131, episode reward: -148.403, mean reward: -2.215 [-100.000,  7.591], mean action: 1.179 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.859699
 15735/100000: episode: 176, duration: 1.018s, episode steps: 112, steps per second: 110, episode reward: -6.897, mean reward: -0.062 [-100.000, 100.033], mean action: 1.304 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.858893
 15833/100000: episode: 177, duration: 0.862s, episode steps:  98, steps per second: 114, episode reward: -5.561, mean reward: -0.057 [-100.000, 105.544], mean action: 1.327 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.857949
 15902/100000: episode: 178, duration: 0.590s, episode steps:  69, steps per second: 117, episode reward: -102.646, mean reward: -1.488 [-100.000,  6.693], mean action: 1.232 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.857197
 15976/100000: episode: 179, duration: 0.634s, episode steps:  74, steps per second: 117, episode reward: -160.575, mean reward: -2.170 [-100.000, 14.838], mean action: 1.216 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.856554
 16044/100000: episode: 180, duration: 0.555s, episode steps:  68, steps per second: 123, episode reward: -91.219, mean reward: -1.341 [-100.000, 16.302], mean action: 1.176 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.855914
 16124/100000: episode: 181, duration: 0.674s, episode steps:  80, steps per second: 119, episode reward: -100.410, mean reward: -1.255 [-100.000, 14.731], mean action: 1.113 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.855249
 16220/100000: episode: 182, duration: 0.841s, episode steps:  96, steps per second: 114, episode reward: -20.894, mean reward: -0.218 [-100.000, 113.156], mean action: 1.312 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.854456
 16293/100000: episode: 183, duration: 0.609s, episode steps:  73, steps per second: 120, episode reward: -89.264, mean reward: -1.223 [-100.000, 13.725], mean action: 1.233 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.853696
 16382/100000: episode: 184, duration: 0.719s, episode steps:  89, steps per second: 124, episode reward: -116.483, mean reward: -1.309 [-100.000, 13.162], mean action: 1.169 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.852967
 16483/100000: episode: 185, duration: 0.870s, episode steps: 101, steps per second: 116, episode reward: -337.345, mean reward: -3.340 [-100.000, 15.536], mean action: 1.040 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.852112
 16648/100000: episode: 186, duration: 1.260s, episode steps: 165, steps per second: 131, episode reward:  0.348, mean reward:  0.002 [-100.000, 101.216], mean action: 1.527 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.850915
 16726/100000: episode: 187, duration: 0.618s, episode steps:  78, steps per second: 126, episode reward: -143.016, mean reward: -1.834 [-100.000,  9.948], mean action: 1.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.849822
 16801/100000: episode: 188, duration: 0.638s, episode steps:  75, steps per second: 117, episode reward: -125.472, mean reward: -1.673 [-100.000,  8.979], mean action: 1.267 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.849133
 16914/100000: episode: 189, duration: 0.859s, episode steps: 113, steps per second: 132, episode reward: -159.301, mean reward: -1.410 [-100.000,  7.292], mean action: 1.345 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.848287
 17004/100000: episode: 190, duration: 0.663s, episode steps:  90, steps per second: 136, episode reward: -115.939, mean reward: -1.288 [-100.000,  9.764], mean action: 1.344 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.847374
 17091/100000: episode: 191, duration: 0.673s, episode steps:  87, steps per second: 129, episode reward: -307.398, mean reward: -3.533 [-100.000, 33.366], mean action: 1.299 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.846577
 17163/100000: episode: 192, duration: 0.538s, episode steps:  72, steps per second: 134, episode reward: -141.114, mean reward: -1.960 [-100.000,  8.392], mean action: 1.208 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.845862
 17232/100000: episode: 193, duration: 0.502s, episode steps:  69, steps per second: 137, episode reward: -199.433, mean reward: -2.890 [-100.000,  4.513], mean action: 1.188 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.845227
 17322/100000: episode: 194, duration: 0.654s, episode steps:  90, steps per second: 138, episode reward: -97.240, mean reward: -1.080 [-100.000, 16.025], mean action: 1.200 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.844511
 17382/100000: episode: 195, duration: 0.498s, episode steps:  60, steps per second: 121, episode reward: -135.574, mean reward: -2.260 [-100.000,  7.982], mean action: 1.150 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.843837
 17483/100000: episode: 196, duration: 0.749s, episode steps: 101, steps per second: 135, episode reward: -295.120, mean reward: -2.922 [-100.000,  5.760], mean action: 1.307 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.843112
 17593/100000: episode: 197, duration: 0.824s, episode steps: 110, steps per second: 134, episode reward: -86.540, mean reward: -0.787 [-100.000, 98.919], mean action: 1.218 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.842162
 17664/100000: episode: 198, duration: 0.553s, episode steps:  71, steps per second: 128, episode reward: -153.298, mean reward: -2.159 [-100.000,  6.488], mean action: 1.352 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.841348
 17764/100000: episode: 199, duration: 0.727s, episode steps: 100, steps per second: 138, episode reward: -102.946, mean reward: -1.029 [-100.000, 14.993], mean action: 1.380 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.840579
 17828/100000: episode: 200, duration: 0.457s, episode steps:  64, steps per second: 140, episode reward: -38.729, mean reward: -0.605 [-100.000, 12.982], mean action: 1.281 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.839840
 17937/100000: episode: 201, duration: 0.903s, episode steps: 109, steps per second: 121, episode reward: -101.633, mean reward: -0.932 [-100.000, 15.502], mean action: 1.560 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.839062
 18006/100000: episode: 202, duration: 0.600s, episode steps:  69, steps per second: 115, episode reward: -98.987, mean reward: -1.435 [-100.000,  7.307], mean action: 0.942 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.838261
 18093/100000: episode: 203, duration: 0.684s, episode steps:  87, steps per second: 127, episode reward: -63.867, mean reward: -0.734 [-100.000, 102.154], mean action: 1.069 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.837559
 18202/100000: episode: 204, duration: 0.926s, episode steps: 109, steps per second: 118, episode reward: -338.700, mean reward: -3.107 [-100.000, 92.661], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.836677
 18272/100000: episode: 205, duration: 0.570s, episode steps:  70, steps per second: 123, episode reward: -85.996, mean reward: -1.229 [-100.000, 14.039], mean action: 1.571 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.835871
 18361/100000: episode: 206, duration: 0.676s, episode steps:  89, steps per second: 132, episode reward: -358.022, mean reward: -4.023 [-100.000,  0.090], mean action: 1.281 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.835156
 18464/100000: episode: 207, duration: 0.825s, episode steps: 103, steps per second: 125, episode reward: -311.299, mean reward: -3.022 [-100.000,  0.321], mean action: 1.301 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.834292
 18581/100000: episode: 208, duration: 0.873s, episode steps: 117, steps per second: 134, episode reward: -131.719, mean reward: -1.126 [-100.000, 18.315], mean action: 1.282 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.833302
 18680/100000: episode: 209, duration: 0.713s, episode steps:  99, steps per second: 139, episode reward: -222.376, mean reward: -2.246 [-100.000,  6.252], mean action: 1.141 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.832330
 18775/100000: episode: 210, duration: 0.744s, episode steps:  95, steps per second: 128, episode reward: -121.875, mean reward: -1.283 [-100.000, 13.062], mean action: 1.021 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.831457
 18847/100000: episode: 211, duration: 0.539s, episode steps:  72, steps per second: 134, episode reward: -114.278, mean reward: -1.587 [-100.000,  9.214], mean action: 1.222 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.830706
 18924/100000: episode: 212, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward: -156.749, mean reward: -2.036 [-100.000,  9.428], mean action: 1.091 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.830035
 19018/100000: episode: 213, duration: 0.677s, episode steps:  94, steps per second: 139, episode reward: -419.858, mean reward: -4.467 [-100.000,  4.443], mean action: 1.330 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.829265
 19099/100000: episode: 214, duration: 0.703s, episode steps:  81, steps per second: 115, episode reward: -184.835, mean reward: -2.282 [-100.000,  8.380], mean action: 1.284 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.828478
 19189/100000: episode: 215, duration: 0.678s, episode steps:  90, steps per second: 133, episode reward: -124.360, mean reward: -1.382 [-100.000, 11.945], mean action: 1.367 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.827708
 19248/100000: episode: 216, duration: 0.428s, episode steps:  59, steps per second: 138, episode reward: -89.978, mean reward: -1.525 [-100.000, 10.058], mean action: 1.153 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.827038
 19347/100000: episode: 217, duration: 0.901s, episode steps:  99, steps per second: 110, episode reward: -167.566, mean reward: -1.693 [-100.000, 10.764], mean action: 1.313 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.826327
 19419/100000: episode: 218, duration: 0.585s, episode steps:  72, steps per second: 123, episode reward: -180.455, mean reward: -2.506 [-100.000,  6.314], mean action: 1.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.825557
 19485/100000: episode: 219, duration: 0.496s, episode steps:  66, steps per second: 133, episode reward: -95.604, mean reward: -1.449 [-100.000,  7.667], mean action: 1.045 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.824936
 19573/100000: episode: 220, duration: 0.677s, episode steps:  88, steps per second: 130, episode reward: -143.325, mean reward: -1.629 [-100.000, 10.903], mean action: 1.034 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.824243
 19648/100000: episode: 221, duration: 0.632s, episode steps:  75, steps per second: 119, episode reward: -98.258, mean reward: -1.310 [-100.000,  9.326], mean action: 1.160 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.823510
 19722/100000: episode: 222, duration: 0.554s, episode steps:  74, steps per second: 134, episode reward: -263.083, mean reward: -3.555 [-100.000,  6.805], mean action: 1.095 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.822839
 19800/100000: episode: 223, duration: 0.596s, episode steps:  78, steps per second: 131, episode reward: -282.042, mean reward: -3.616 [-100.000, 115.269], mean action: 1.128 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.822156
 19873/100000: episode: 224, duration: 0.573s, episode steps:  73, steps per second: 127, episode reward: -104.252, mean reward: -1.428 [-100.000,  9.209], mean action: 1.151 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.821476
 19950/100000: episode: 225, duration: 0.619s, episode steps:  77, steps per second: 124, episode reward: -148.216, mean reward: -1.925 [-100.000,  7.746], mean action: 1.247 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.820801
 20070/100000: episode: 226, duration: 0.897s, episode steps: 120, steps per second: 134, episode reward: -221.711, mean reward: -1.848 [-100.000,  7.645], mean action: 1.200 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.819914
 20150/100000: episode: 227, duration: 0.609s, episode steps:  80, steps per second: 131, episode reward: -113.469, mean reward: -1.418 [-100.000,  6.869], mean action: 1.300 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.819015
 20249/100000: episode: 228, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -114.658, mean reward: -1.158 [-100.000, 22.326], mean action: 0.980 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.818209
 20308/100000: episode: 229, duration: 0.443s, episode steps:  59, steps per second: 133, episode reward: -102.923, mean reward: -1.744 [-100.000,  7.753], mean action: 1.322 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.817498
 20402/100000: episode: 230, duration: 0.728s, episode steps:  94, steps per second: 129, episode reward: -108.207, mean reward: -1.151 [-100.000,  7.376], mean action: 1.181 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.816809
 20507/100000: episode: 231, duration: 0.802s, episode steps: 105, steps per second: 131, episode reward: -232.137, mean reward: -2.211 [-100.000,  6.337], mean action: 1.267 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.815914
 20585/100000: episode: 232, duration: 0.583s, episode steps:  78, steps per second: 134, episode reward: -192.145, mean reward: -2.463 [-100.000,  6.779], mean action: 1.244 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.815091
 20687/100000: episode: 233, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -110.412, mean reward: -1.082 [-100.000, 13.218], mean action: 1.127 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.814281
 20760/100000: episode: 234, duration: 0.584s, episode steps:  73, steps per second: 125, episode reward: -62.210, mean reward: -0.852 [-100.000, 52.492], mean action: 1.315 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.813493
 20865/100000: episode: 235, duration: 0.864s, episode steps: 105, steps per second: 121, episode reward: -171.835, mean reward: -1.637 [-100.000, 59.673], mean action: 1.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.812692
 20940/100000: episode: 236, duration: 0.635s, episode steps:  75, steps per second: 118, episode reward: -88.237, mean reward: -1.176 [-100.000, 10.766], mean action: 0.920 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.811882
 21023/100000: episode: 237, duration: 0.749s, episode steps:  83, steps per second: 111, episode reward: -93.635, mean reward: -1.128 [-100.000, 58.123], mean action: 1.349 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.811171
 21089/100000: episode: 238, duration: 0.615s, episode steps:  66, steps per second: 107, episode reward: -112.220, mean reward: -1.700 [-100.000,  6.814], mean action: 1.409 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.810501
 21146/100000: episode: 239, duration: 0.486s, episode steps:  57, steps per second: 117, episode reward: -87.728, mean reward: -1.539 [-100.000, 26.518], mean action: 1.070 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.809947
 21243/100000: episode: 240, duration: 0.821s, episode steps:  97, steps per second: 118, episode reward: -168.181, mean reward: -1.734 [-100.000,  7.974], mean action: 1.227 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.809254
 21370/100000: episode: 241, duration: 1.023s, episode steps: 127, steps per second: 124, episode reward: -261.050, mean reward: -2.056 [-100.000, 17.923], mean action: 1.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.808246
 21478/100000: episode: 242, duration: 0.805s, episode steps: 108, steps per second: 134, episode reward: -340.895, mean reward: -3.156 [-100.000, 25.542], mean action: 1.157 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.807189
 21539/100000: episode: 243, duration: 0.464s, episode steps:  61, steps per second: 131, episode reward: -144.312, mean reward: -2.366 [-100.000,  8.716], mean action: 1.148 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.806428
 21639/100000: episode: 244, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward: -124.766, mean reward: -1.248 [-100.000,  8.060], mean action: 1.290 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.805703
 21743/100000: episode: 245, duration: 0.758s, episode steps: 104, steps per second: 137, episode reward: -108.740, mean reward: -1.046 [-100.000, 21.786], mean action: 1.260 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.804786
 21842/100000: episode: 246, duration: 0.780s, episode steps:  99, steps per second: 127, episode reward: -128.860, mean reward: -1.302 [-100.000,  6.201], mean action: 1.091 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.803872
 21948/100000: episode: 247, duration: 0.774s, episode steps: 106, steps per second: 137, episode reward: -152.732, mean reward: -1.441 [-100.000, 102.343], mean action: 1.358 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.802949
 22008/100000: episode: 248, duration: 0.432s, episode steps:  60, steps per second: 139, episode reward: -80.004, mean reward: -1.333 [-100.000,  7.110], mean action: 1.367 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.802203
 22070/100000: episode: 249, duration: 0.476s, episode steps:  62, steps per second: 130, episode reward: -84.418, mean reward: -1.362 [-100.000,  7.664], mean action: 1.097 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.801654
 22133/100000: episode: 250, duration: 0.511s, episode steps:  63, steps per second: 123, episode reward: -47.181, mean reward: -0.749 [-100.000, 28.165], mean action: 1.460 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.801091
 22222/100000: episode: 251, duration: 0.684s, episode steps:  89, steps per second: 130, episode reward: -138.042, mean reward: -1.551 [-100.000,  6.326], mean action: 1.180 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.800407
 22317/100000: episode: 252, duration: 0.690s, episode steps:  95, steps per second: 138, episode reward: -117.190, mean reward: -1.234 [-100.000,  6.596], mean action: 0.968 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.799579
 22450/100000: episode: 253, duration: 1.037s, episode steps: 133, steps per second: 128, episode reward: 13.973, mean reward:  0.105 [-100.000, 85.543], mean action: 1.338 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.798553
 22537/100000: episode: 254, duration: 0.669s, episode steps:  87, steps per second: 130, episode reward: -181.873, mean reward: -2.090 [-100.000, 26.849], mean action: 1.540 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.797563
 22621/100000: episode: 255, duration: 0.668s, episode steps:  84, steps per second: 126, episode reward: -101.201, mean reward: -1.205 [-100.000,  6.554], mean action: 1.369 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.796794
 22701/100000: episode: 256, duration: 0.620s, episode steps:  80, steps per second: 129, episode reward: -104.991, mean reward: -1.312 [-100.000, 18.284], mean action: 1.100 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.796055
 22782/100000: episode: 257, duration: 0.585s, episode steps:  81, steps per second: 138, episode reward: -287.217, mean reward: -3.546 [-100.000,  4.144], mean action: 1.160 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.795331
 22915/100000: episode: 258, duration: 1.069s, episode steps: 133, steps per second: 124, episode reward: -119.290, mean reward: -0.897 [-100.000, 28.720], mean action: 1.195 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.794368
 22997/100000: episode: 259, duration: 0.691s, episode steps:  82, steps per second: 119, episode reward: -117.320, mean reward: -1.431 [-100.000,  8.395], mean action: 1.061 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.793401
 23107/100000: episode: 260, duration: 0.824s, episode steps: 110, steps per second: 134, episode reward: -171.872, mean reward: -1.562 [-100.000, 38.989], mean action: 1.227 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.792536
 23171/100000: episode: 261, duration: 0.535s, episode steps:  64, steps per second: 120, episode reward: -75.054, mean reward: -1.173 [-100.000, 19.062], mean action: 1.188 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.791753
 23254/100000: episode: 262, duration: 0.702s, episode steps:  83, steps per second: 118, episode reward: -104.647, mean reward: -1.261 [-100.000,  5.287], mean action: 1.205 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.791092
 23376/100000: episode: 263, duration: 1.001s, episode steps: 122, steps per second: 122, episode reward: -320.252, mean reward: -2.625 [-100.000, 104.033], mean action: 0.951 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.790169
 23437/100000: episode: 264, duration: 0.526s, episode steps:  61, steps per second: 116, episode reward: -93.630, mean reward: -1.535 [-100.000,  6.156], mean action: 1.180 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.789346
 23538/100000: episode: 265, duration: 0.783s, episode steps: 101, steps per second: 129, episode reward: -181.672, mean reward: -1.799 [-100.000,  7.314], mean action: 1.277 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.788617
 23643/100000: episode: 266, duration: 0.805s, episode steps: 105, steps per second: 130, episode reward: -123.965, mean reward: -1.181 [-100.000,  9.821], mean action: 1.114 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.787690
 23707/100000: episode: 267, duration: 0.511s, episode steps:  64, steps per second: 125, episode reward: -149.037, mean reward: -2.329 [-100.000, 36.754], mean action: 0.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.786930
 23835/100000: episode: 268, duration: 1.110s, episode steps: 128, steps per second: 115, episode reward: -276.956, mean reward: -2.164 [-100.000,  8.576], mean action: 1.133 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.786066
 23908/100000: episode: 269, duration: 0.610s, episode steps:  73, steps per second: 120, episode reward: -81.683, mean reward: -1.119 [-100.000, 18.607], mean action: 1.356 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.785161
 23971/100000: episode: 270, duration: 0.531s, episode steps:  63, steps per second: 119, episode reward: -58.574, mean reward: -0.930 [-100.000, 14.096], mean action: 0.841 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.784549
 24052/100000: episode: 271, duration: 0.847s, episode steps:  81, steps per second:  96, episode reward: -245.274, mean reward: -3.028 [-100.000, 99.089], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.783901
 24139/100000: episode: 272, duration: 0.853s, episode steps:  87, steps per second: 102, episode reward: -353.744, mean reward: -4.066 [-100.000,  0.215], mean action: 1.264 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.783145
 24247/100000: episode: 273, duration: 0.935s, episode steps: 108, steps per second: 115, episode reward: -145.637, mean reward: -1.348 [-100.000,  6.268], mean action: 1.074 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.782267
 24315/100000: episode: 274, duration: 0.613s, episode steps:  68, steps per second: 111, episode reward: -79.948, mean reward: -1.176 [-100.000, 17.183], mean action: 1.265 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.781475
 24432/100000: episode: 275, duration: 1.037s, episode steps: 117, steps per second: 113, episode reward: -338.527, mean reward: -2.893 [-100.000, 83.658], mean action: 1.179 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.780643
 24496/100000: episode: 276, duration: 0.510s, episode steps:  64, steps per second: 126, episode reward: -167.413, mean reward: -2.616 [-100.000,  7.878], mean action: 1.141 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.779828
 24574/100000: episode: 277, duration: 0.658s, episode steps:  78, steps per second: 118, episode reward: -112.282, mean reward: -1.440 [-100.000, 15.907], mean action: 1.038 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.779189
 24674/100000: episode: 278, duration: 0.757s, episode steps: 100, steps per second: 132, episode reward: -114.175, mean reward: -1.142 [-100.000, 15.544], mean action: 1.160 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.778388
 24741/100000: episode: 279, duration: 0.525s, episode steps:  67, steps per second: 128, episode reward: -75.724, mean reward: -1.130 [-100.000, 15.813], mean action: 1.075 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.777637
 24812/100000: episode: 280, duration: 0.537s, episode steps:  71, steps per second: 132, episode reward: -146.798, mean reward: -2.068 [-100.000, 11.019], mean action: 1.155 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.777016
 24900/100000: episode: 281, duration: 0.648s, episode steps:  88, steps per second: 136, episode reward: -317.686, mean reward: -3.610 [-100.000,  7.134], mean action: 1.227 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.776301
 24971/100000: episode: 282, duration: 0.509s, episode steps:  71, steps per second: 139, episode reward: -148.032, mean reward: -2.085 [-100.000,  5.479], mean action: 0.930 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.775585
 25092/100000: episode: 283, duration: 0.952s, episode steps: 121, steps per second: 127, episode reward: -189.509, mean reward: -1.566 [-100.000,  6.270], mean action: 1.298 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.774721
 25181/100000: episode: 284, duration: 0.654s, episode steps:  89, steps per second: 136, episode reward: -224.278, mean reward: -2.520 [-100.000,  6.447], mean action: 0.978 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.773776
 25282/100000: episode: 285, duration: 0.747s, episode steps: 101, steps per second: 135, episode reward: -494.226, mean reward: -4.893 [-100.000,  0.836], mean action: 1.356 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.772921
 25373/100000: episode: 286, duration: 0.681s, episode steps:  91, steps per second: 134, episode reward: -140.514, mean reward: -1.544 [-100.000, 11.659], mean action: 1.165 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.772057
 25454/100000: episode: 287, duration: 0.597s, episode steps:  81, steps per second: 136, episode reward: -95.120, mean reward: -1.174 [-100.000,  7.873], mean action: 0.988 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.771283
 25519/100000: episode: 288, duration: 0.480s, episode steps:  65, steps per second: 136, episode reward: -99.489, mean reward: -1.531 [-100.000, 10.824], mean action: 1.046 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.770626
 25614/100000: episode: 289, duration: 0.737s, episode steps:  95, steps per second: 129, episode reward: -331.880, mean reward: -3.493 [-100.000,  3.940], mean action: 1.211 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.769906
 25706/100000: episode: 290, duration: 0.676s, episode steps:  92, steps per second: 136, episode reward: -146.605, mean reward: -1.594 [-100.000, 16.924], mean action: 1.283 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.769065
 25776/100000: episode: 291, duration: 0.506s, episode steps:  70, steps per second: 138, episode reward: -93.154, mean reward: -1.331 [-100.000,  7.597], mean action: 0.971 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.768336
 25834/100000: episode: 292, duration: 0.438s, episode steps:  58, steps per second: 132, episode reward: -123.212, mean reward: -2.124 [-100.000,  9.419], mean action: 1.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.767759
 25916/100000: episode: 293, duration: 0.659s, episode steps:  82, steps per second: 124, episode reward: -200.424, mean reward: -2.444 [-100.000,  6.271], mean action: 1.159 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.767130
 26032/100000: episode: 294, duration: 0.852s, episode steps: 116, steps per second: 136, episode reward: -390.618, mean reward: -3.367 [-100.000, 12.627], mean action: 1.293 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.766239
 26158/100000: episode: 295, duration: 0.936s, episode steps: 126, steps per second: 135, episode reward: -275.540, mean reward: -2.187 [-100.000, 20.515], mean action: 1.024 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.765150
 26236/100000: episode: 296, duration: 0.598s, episode steps:  78, steps per second: 130, episode reward: -229.067, mean reward: -2.937 [-100.000, 29.953], mean action: 0.974 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.764232
 26341/100000: episode: 297, duration: 0.850s, episode steps: 105, steps per second: 124, episode reward: -134.774, mean reward: -1.284 [-100.000,  5.273], mean action: 1.114 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.763408
 26457/100000: episode: 298, duration: 0.896s, episode steps: 116, steps per second: 129, episode reward: -246.499, mean reward: -2.125 [-100.000, 27.156], mean action: 1.293 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.762413
 26516/100000: episode: 299, duration: 0.440s, episode steps:  59, steps per second: 134, episode reward: -70.472, mean reward: -1.194 [-100.000, 13.779], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.761626
 26617/100000: episode: 300, duration: 0.730s, episode steps: 101, steps per second: 138, episode reward: -354.415, mean reward: -3.509 [-100.000,  0.873], mean action: 1.139 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.760906
 26695/100000: episode: 301, duration: 0.587s, episode steps:  78, steps per second: 133, episode reward: -118.068, mean reward: -1.514 [-100.000,  7.213], mean action: 1.141 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.760100
 26757/100000: episode: 302, duration: 0.500s, episode steps:  62, steps per second: 124, episode reward: -129.908, mean reward: -2.095 [-100.000,  7.222], mean action: 1.210 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.759471
 26836/100000: episode: 303, duration: 0.594s, episode steps:  79, steps per second: 133, episode reward: -247.388, mean reward: -3.131 [-100.000,  8.159], mean action: 1.025 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.758836
 26938/100000: episode: 304, duration: 0.827s, episode steps: 102, steps per second: 123, episode reward:  9.590, mean reward:  0.094 [-100.000, 122.245], mean action: 0.941 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.758021
 27017/100000: episode: 305, duration: 0.622s, episode steps:  79, steps per second: 127, episode reward: -139.472, mean reward: -1.765 [-100.000, 12.319], mean action: 0.975 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.757207
 27102/100000: episode: 306, duration: 0.655s, episode steps:  85, steps per second: 130, episode reward: -319.901, mean reward: -3.764 [-100.000,  4.123], mean action: 1.247 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.756469
 27194/100000: episode: 307, duration: 0.688s, episode steps:  92, steps per second: 134, episode reward: -284.798, mean reward: -3.096 [-100.000,  7.442], mean action: 1.087 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.755673
 27277/100000: episode: 308, duration: 0.622s, episode steps:  83, steps per second: 133, episode reward: -104.225, mean reward: -1.256 [-100.000, 13.352], mean action: 0.940 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.754885
 27365/100000: episode: 309, duration: 0.665s, episode steps:  88, steps per second: 132, episode reward: -121.178, mean reward: -1.377 [-100.000,  7.129], mean action: 1.057 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.754116
 27458/100000: episode: 310, duration: 0.677s, episode steps:  93, steps per second: 137, episode reward: -161.017, mean reward: -1.731 [-100.000, 10.336], mean action: 1.290 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.753301
 27526/100000: episode: 311, duration: 0.515s, episode steps:  68, steps per second: 132, episode reward: -106.874, mean reward: -1.572 [-100.000, 17.591], mean action: 1.235 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.752576
 27605/100000: episode: 312, duration: 0.637s, episode steps:  79, steps per second: 124, episode reward: -142.607, mean reward: -1.805 [-100.000, 12.075], mean action: 1.253 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.751915
 27674/100000: episode: 313, duration: 0.518s, episode steps:  69, steps per second: 133, episode reward: -76.959, mean reward: -1.115 [-100.000,  8.681], mean action: 1.087 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.751249
 27786/100000: episode: 314, duration: 0.861s, episode steps: 112, steps per second: 130, episode reward: -396.277, mean reward: -3.538 [-100.000,  1.446], mean action: 1.188 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.750435
 27881/100000: episode: 315, duration: 0.727s, episode steps:  95, steps per second: 131, episode reward: -211.089, mean reward: -2.222 [-100.000,  7.211], mean action: 1.021 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.749503
 28002/100000: episode: 316, duration: 0.913s, episode steps: 121, steps per second: 132, episode reward: -171.495, mean reward: -1.417 [-100.000, 16.157], mean action: 1.289 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.748531
 28109/100000: episode: 317, duration: 0.788s, episode steps: 107, steps per second: 136, episode reward:  1.989, mean reward:  0.019 [-100.000, 108.527], mean action: 0.935 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.747505
 28190/100000: episode: 318, duration: 0.639s, episode steps:  81, steps per second: 127, episode reward: -111.572, mean reward: -1.377 [-100.000,  8.245], mean action: 1.111 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.746659
 28278/100000: episode: 319, duration: 0.647s, episode steps:  88, steps per second: 136, episode reward: -319.071, mean reward: -3.626 [-100.000,  5.747], mean action: 0.989 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.745899
 28371/100000: episode: 320, duration: 0.727s, episode steps:  93, steps per second: 128, episode reward: -278.188, mean reward: -2.991 [-100.000,  7.260], mean action: 1.086 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.745084
 28438/100000: episode: 321, duration: 0.551s, episode steps:  67, steps per second: 122, episode reward: -110.907, mean reward: -1.655 [-100.000,  6.049], mean action: 1.134 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.744364
 28524/100000: episode: 322, duration: 0.648s, episode steps:  86, steps per second: 133, episode reward: -31.178, mean reward: -0.363 [-100.000, 105.328], mean action: 1.209 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.743676
 28596/100000: episode: 323, duration: 0.526s, episode steps:  72, steps per second: 137, episode reward: -244.474, mean reward: -3.395 [-100.000,  4.671], mean action: 1.056 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.742964
 28658/100000: episode: 324, duration: 0.480s, episode steps:  62, steps per second: 129, episode reward: -82.342, mean reward: -1.328 [-100.000,  7.462], mean action: 0.887 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.742362
 28724/100000: episode: 325, duration: 0.531s, episode steps:  66, steps per second: 124, episode reward: -115.066, mean reward: -1.743 [-100.000,  8.330], mean action: 1.197 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.741785
 28782/100000: episode: 326, duration: 0.465s, episode steps:  58, steps per second: 125, episode reward: -90.503, mean reward: -1.560 [-100.000,  4.537], mean action: 1.103 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.741227
 28861/100000: episode: 327, duration: 0.581s, episode steps:  79, steps per second: 136, episode reward: -362.012, mean reward: -4.582 [-100.000,  2.270], mean action: 0.899 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.740611
 28944/100000: episode: 328, duration: 0.615s, episode steps:  83, steps per second: 135, episode reward: -135.670, mean reward: -1.635 [-100.000,  8.416], mean action: 0.964 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.739882
 29017/100000: episode: 329, duration: 0.570s, episode steps:  73, steps per second: 128, episode reward: -97.721, mean reward: -1.339 [-100.000, 17.069], mean action: 1.137 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.739180
 29092/100000: episode: 330, duration: 0.564s, episode steps:  75, steps per second: 133, episode reward: -24.371, mean reward: -0.325 [-100.000, 87.500], mean action: 1.253 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.738514
 29177/100000: episode: 331, duration: 0.642s, episode steps:  85, steps per second: 132, episode reward: -126.451, mean reward: -1.488 [-100.000,  9.382], mean action: 1.024 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.737794
 29286/100000: episode: 332, duration: 0.918s, episode steps: 109, steps per second: 119, episode reward: -129.580, mean reward: -1.189 [-100.000, 21.832], mean action: 1.156 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.736921
 29358/100000: episode: 333, duration: 0.601s, episode steps:  72, steps per second: 120, episode reward: -108.082, mean reward: -1.501 [-100.000,  8.249], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.736106
 29425/100000: episode: 334, duration: 0.552s, episode steps:  67, steps per second: 121, episode reward: -113.363, mean reward: -1.692 [-100.000,  6.548], mean action: 0.970 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.735481
 29488/100000: episode: 335, duration: 0.513s, episode steps:  63, steps per second: 123, episode reward: -89.471, mean reward: -1.420 [-100.000, 21.999], mean action: 1.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.734896
 29601/100000: episode: 336, duration: 1.053s, episode steps: 113, steps per second: 107, episode reward: -111.826, mean reward: -0.990 [-100.000,  7.619], mean action: 0.929 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.734104
 29701/100000: episode: 337, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: -276.682, mean reward: -2.767 [-100.000,  5.737], mean action: 1.090 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.733146
 29792/100000: episode: 338, duration: 0.758s, episode steps:  91, steps per second: 120, episode reward: -143.927, mean reward: -1.582 [-100.000,  5.692], mean action: 1.231 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.732286
 29880/100000: episode: 339, duration: 0.708s, episode steps:  88, steps per second: 124, episode reward: -152.057, mean reward: -1.728 [-100.000,  6.085], mean action: 1.011 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.731480
 29974/100000: episode: 340, duration: 0.697s, episode steps:  94, steps per second: 135, episode reward: -155.722, mean reward: -1.657 [-100.000, 22.794], mean action: 1.096 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.730661
 30065/100000: episode: 341, duration: 0.726s, episode steps:  91, steps per second: 125, episode reward: -287.416, mean reward: -3.158 [-100.000,  6.208], mean action: 1.055 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.729829
 30166/100000: episode: 342, duration: 0.761s, episode steps: 101, steps per second: 133, episode reward: -149.643, mean reward: -1.482 [-100.000, 15.485], mean action: 1.079 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.728965
 30263/100000: episode: 343, duration: 0.716s, episode steps:  97, steps per second: 136, episode reward: -102.967, mean reward: -1.062 [-100.000,  8.188], mean action: 1.103 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.728074
 30337/100000: episode: 344, duration: 0.580s, episode steps:  74, steps per second: 128, episode reward: -134.198, mean reward: -1.813 [-100.000,  9.451], mean action: 1.068 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.727305
 30426/100000: episode: 345, duration: 0.695s, episode steps:  89, steps per second: 128, episode reward: -317.852, mean reward: -3.571 [-100.000,  4.019], mean action: 1.360 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.726571
 30508/100000: episode: 346, duration: 0.616s, episode steps:  82, steps per second: 133, episode reward: -78.962, mean reward: -0.963 [-100.000, 14.462], mean action: 1.146 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.725801
 30605/100000: episode: 347, duration: 0.742s, episode steps:  97, steps per second: 131, episode reward: -256.989, mean reward: -2.649 [-100.000,  4.162], mean action: 1.010 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.724996
 30673/100000: episode: 348, duration: 0.525s, episode steps:  68, steps per second: 130, episode reward: -158.623, mean reward: -2.333 [-100.000,  6.446], mean action: 0.971 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.724253
 30759/100000: episode: 349, duration: 0.656s, episode steps:  86, steps per second: 131, episode reward: -223.814, mean reward: -2.602 [-100.000, 22.253], mean action: 1.256 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.723560
 30845/100000: episode: 350, duration: 0.663s, episode steps:  86, steps per second: 130, episode reward: -255.416, mean reward: -2.970 [-100.000,  5.028], mean action: 1.140 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.722786
 30951/100000: episode: 351, duration: 0.842s, episode steps: 106, steps per second: 126, episode reward: -50.756, mean reward: -0.479 [-100.000, 42.906], mean action: 0.934 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.721922
 31047/100000: episode: 352, duration: 0.723s, episode steps:  96, steps per second: 133, episode reward: -177.130, mean reward: -1.845 [-100.000,  6.261], mean action: 1.188 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.721013
 31117/100000: episode: 353, duration: 0.527s, episode steps:  70, steps per second: 133, episode reward: -125.432, mean reward: -1.792 [-100.000,  8.841], mean action: 0.986 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.720267
 31216/100000: episode: 354, duration: 0.784s, episode steps:  99, steps per second: 126, episode reward: -287.932, mean reward: -2.908 [-100.000,  0.332], mean action: 1.101 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.719506
 31329/100000: episode: 355, duration: 0.837s, episode steps: 113, steps per second: 135, episode reward: -188.001, mean reward: -1.664 [-100.000,  5.899], mean action: 1.018 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.718552
 31435/100000: episode: 356, duration: 0.785s, episode steps: 106, steps per second: 135, episode reward: -97.260, mean reward: -0.918 [-100.000,  9.188], mean action: 1.094 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.717566
 31501/100000: episode: 357, duration: 0.542s, episode steps:  66, steps per second: 122, episode reward: -108.428, mean reward: -1.643 [-100.000, 16.312], mean action: 1.258 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.716793
 31608/100000: episode: 358, duration: 0.807s, episode steps: 107, steps per second: 133, episode reward: -189.435, mean reward: -1.770 [-100.000,  7.390], mean action: 0.944 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.716014
 31672/100000: episode: 359, duration: 0.468s, episode steps:  64, steps per second: 137, episode reward: -157.855, mean reward: -2.466 [-100.000,  8.280], mean action: 1.172 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.715244
 31764/100000: episode: 360, duration: 0.708s, episode steps:  92, steps per second: 130, episode reward: -132.155, mean reward: -1.436 [-100.000,  7.543], mean action: 1.076 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.714542
 31862/100000: episode: 361, duration: 0.747s, episode steps:  98, steps per second: 131, episode reward: -128.414, mean reward: -1.310 [-100.000,  8.007], mean action: 1.010 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.713688
 31953/100000: episode: 362, duration: 0.674s, episode steps:  91, steps per second: 135, episode reward: -191.050, mean reward: -2.099 [-100.000,  9.297], mean action: 0.890 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.712837
 32015/100000: episode: 363, duration: 0.494s, episode steps:  62, steps per second: 126, episode reward: -194.553, mean reward: -3.138 [-100.000,  4.068], mean action: 1.048 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.712149
 32097/100000: episode: 364, duration: 0.649s, episode steps:  82, steps per second: 126, episode reward: -371.783, mean reward: -4.534 [-100.000, 43.407], mean action: 1.061 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.711501
 32190/100000: episode: 365, duration: 0.708s, episode steps:  93, steps per second: 131, episode reward: -160.862, mean reward: -1.730 [-100.000, 11.626], mean action: 0.871 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.710713
 32288/100000: episode: 366, duration: 0.731s, episode steps:  98, steps per second: 134, episode reward: -167.959, mean reward: -1.714 [-100.000,  7.120], mean action: 1.092 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.709853
 32377/100000: episode: 367, duration: 0.695s, episode steps:  89, steps per second: 128, episode reward: -376.127, mean reward: -4.226 [-100.000,  6.970], mean action: 1.169 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.709012
 32453/100000: episode: 368, duration: 0.557s, episode steps:  76, steps per second: 136, episode reward: -89.108, mean reward: -1.172 [-100.000, 17.926], mean action: 1.158 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.708269
 32540/100000: episode: 369, duration: 0.640s, episode steps:  87, steps per second: 136, episode reward: -205.466, mean reward: -2.362 [-100.000,  7.049], mean action: 1.011 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.707536
 32614/100000: episode: 370, duration: 0.699s, episode steps:  74, steps per second: 106, episode reward: -162.505, mean reward: -2.196 [-100.000, 19.618], mean action: 1.176 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.706811
 32674/100000: episode: 371, duration: 0.548s, episode steps:  60, steps per second: 110, episode reward: -96.594, mean reward: -1.610 [-100.000, 10.858], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.706209
 32773/100000: episode: 372, duration: 0.923s, episode steps:  99, steps per second: 107, episode reward: -312.552, mean reward: -3.157 [-100.000,  0.915], mean action: 1.040 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.705493
 32871/100000: episode: 373, duration: 0.961s, episode steps:  98, steps per second: 102, episode reward: -127.194, mean reward: -1.298 [-100.000, 35.095], mean action: 1.020 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.704606
 32978/100000: episode: 374, duration: 0.953s, episode steps: 107, steps per second: 112, episode reward: -237.759, mean reward: -2.222 [-100.000, 72.122], mean action: 0.972 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.703684
 33039/100000: episode: 375, duration: 0.511s, episode steps:  61, steps per second: 119, episode reward: -93.598, mean reward: -1.534 [-100.000, 27.949], mean action: 1.131 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.702928
 33125/100000: episode: 376, duration: 0.839s, episode steps:  86, steps per second: 103, episode reward: -112.052, mean reward: -1.303 [-100.000, 11.032], mean action: 1.093 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.702267
 33224/100000: episode: 377, duration: 0.763s, episode steps:  99, steps per second: 130, episode reward: -342.616, mean reward: -3.461 [-100.000,  4.087], mean action: 1.030 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.701434
 33298/100000: episode: 378, duration: 0.587s, episode steps:  74, steps per second: 126, episode reward: -132.060, mean reward: -1.785 [-100.000,  6.404], mean action: 1.108 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.700656
 33396/100000: episode: 379, duration: 0.838s, episode steps:  98, steps per second: 117, episode reward: -162.480, mean reward: -1.658 [-100.000,  9.185], mean action: 1.051 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.699881
 33453/100000: episode: 380, duration: 0.434s, episode steps:  57, steps per second: 131, episode reward: -134.065, mean reward: -2.352 [-100.000,  6.945], mean action: 1.088 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.699184
 33554/100000: episode: 381, duration: 0.739s, episode steps: 101, steps per second: 137, episode reward: -81.759, mean reward: -0.809 [-100.000, 68.659], mean action: 1.238 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.698473
 33613/100000: episode: 382, duration: 0.467s, episode steps:  59, steps per second: 126, episode reward: -124.452, mean reward: -2.109 [-100.000, 45.879], mean action: 1.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.697753
 33712/100000: episode: 383, duration: 0.764s, episode steps:  99, steps per second: 130, episode reward: -276.434, mean reward: -2.792 [-100.000,  4.168], mean action: 1.242 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.697042
 33787/100000: episode: 384, duration: 0.567s, episode steps:  75, steps per second: 132, episode reward: -151.419, mean reward: -2.019 [-100.000,  6.727], mean action: 1.053 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.696259
 33880/100000: episode: 385, duration: 0.761s, episode steps:  93, steps per second: 122, episode reward: -39.119, mean reward: -0.421 [-100.000, 77.898], mean action: 1.118 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.695503
 33972/100000: episode: 386, duration: 0.798s, episode steps:  92, steps per second: 115, episode reward: -90.800, mean reward: -0.987 [-100.000, 29.255], mean action: 1.054 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.694670
 34045/100000: episode: 387, duration: 0.572s, episode steps:  73, steps per second: 128, episode reward: 39.681, mean reward:  0.544 [-100.000, 113.083], mean action: 0.959 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.693928
 34145/100000: episode: 388, duration: 0.811s, episode steps: 100, steps per second: 123, episode reward: -164.623, mean reward: -1.646 [-100.000,  6.133], mean action: 1.090 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.693149
 34249/100000: episode: 389, duration: 0.792s, episode steps: 104, steps per second: 131, episode reward: -107.677, mean reward: -1.035 [-100.000,  9.995], mean action: 1.048 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.692231
 34320/100000: episode: 390, duration: 0.530s, episode steps:  71, steps per second: 134, episode reward: -108.784, mean reward: -1.532 [-100.000,  7.572], mean action: 0.775 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.691444
 34435/100000: episode: 391, duration: 0.879s, episode steps: 115, steps per second: 131, episode reward: -608.290, mean reward: -5.289 [-100.000, 46.305], mean action: 0.965 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.690607
 34537/100000: episode: 392, duration: 0.782s, episode steps: 102, steps per second: 130, episode reward: -142.848, mean reward: -1.400 [-100.000, 13.118], mean action: 1.186 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.689630
 34630/100000: episode: 393, duration: 0.702s, episode steps:  93, steps per second: 132, episode reward: -146.656, mean reward: -1.577 [-100.000,  5.727], mean action: 1.011 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.688753
 34693/100000: episode: 394, duration: 0.487s, episode steps:  63, steps per second: 129, episode reward: -108.241, mean reward: -1.718 [-100.000,  6.110], mean action: 1.238 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.688051
 34783/100000: episode: 395, duration: 0.695s, episode steps:  90, steps per second: 129, episode reward: -282.309, mean reward: -3.137 [-100.000,  4.887], mean action: 1.244 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.687362
 34866/100000: episode: 396, duration: 0.623s, episode steps:  83, steps per second: 133, episode reward: -163.880, mean reward: -1.974 [-100.000, 22.828], mean action: 0.940 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.686584
 34934/100000: episode: 397, duration: 0.524s, episode steps:  68, steps per second: 130, episode reward: -92.332, mean reward: -1.358 [-100.000,  7.594], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.685905
 35037/100000: episode: 398, duration: 0.819s, episode steps: 103, steps per second: 126, episode reward: -310.136, mean reward: -3.011 [-100.000,  4.392], mean action: 1.175 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.685135
 35119/100000: episode: 399, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -225.264, mean reward: -2.747 [-100.000,  5.346], mean action: 0.988 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.684303
 35226/100000: episode: 400, duration: 0.792s, episode steps: 107, steps per second: 135, episode reward: -152.412, mean reward: -1.424 [-100.000,  8.571], mean action: 1.009 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.683452
 35307/100000: episode: 401, duration: 0.659s, episode steps:  81, steps per second: 123, episode reward: -118.104, mean reward: -1.458 [-100.000, 70.263], mean action: 1.037 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.682606
 35412/100000: episode: 402, duration: 0.810s, episode steps: 105, steps per second: 130, episode reward: -211.310, mean reward: -2.012 [-100.000,  6.363], mean action: 0.971 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.681769
 35515/100000: episode: 403, duration: 0.769s, episode steps: 103, steps per second: 134, episode reward: -197.249, mean reward: -1.915 [-100.000, 14.991], mean action: 1.097 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.680833
 35605/100000: episode: 404, duration: 0.714s, episode steps:  90, steps per second: 126, episode reward: -137.134, mean reward: -1.524 [-100.000, 16.836], mean action: 1.067 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.679964
 35696/100000: episode: 405, duration: 0.690s, episode steps:  91, steps per second: 132, episode reward: -218.949, mean reward: -2.406 [-100.000, 66.959], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.679150
 35763/100000: episode: 406, duration: 0.520s, episode steps:  67, steps per second: 129, episode reward: -117.913, mean reward: -1.760 [-100.000, 27.472], mean action: 1.060 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.678439
 35834/100000: episode: 407, duration: 0.558s, episode steps:  71, steps per second: 127, episode reward: -93.539, mean reward: -1.317 [-100.000, 16.490], mean action: 0.859 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.677818
 35891/100000: episode: 408, duration: 0.448s, episode steps:  57, steps per second: 127, episode reward: -95.742, mean reward: -1.680 [-100.000,  7.778], mean action: 1.088 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.677242
 35968/100000: episode: 409, duration: 0.592s, episode steps:  77, steps per second: 130, episode reward: -128.419, mean reward: -1.668 [-100.000,  5.847], mean action: 0.883 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.676639
 36042/100000: episode: 410, duration: 0.560s, episode steps:  74, steps per second: 132, episode reward: -195.325, mean reward: -2.640 [-100.000,  7.632], mean action: 0.932 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.675960
 36138/100000: episode: 411, duration: 0.750s, episode steps:  96, steps per second: 128, episode reward: -337.154, mean reward: -3.512 [-100.000,  0.321], mean action: 0.844 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.675195
 36240/100000: episode: 412, duration: 0.761s, episode steps: 102, steps per second: 134, episode reward: -330.212, mean reward: -3.237 [-100.000, 92.682], mean action: 0.990 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.674304
 36311/100000: episode: 413, duration: 0.527s, episode steps:  71, steps per second: 135, episode reward: -201.851, mean reward: -2.843 [-100.000,  5.492], mean action: 0.845 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.673525
 36408/100000: episode: 414, duration: 0.829s, episode steps:  97, steps per second: 117, episode reward: -119.883, mean reward: -1.236 [-100.000,  7.668], mean action: 1.113 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.672769
 36471/100000: episode: 415, duration: 0.485s, episode steps:  63, steps per second: 130, episode reward: -93.889, mean reward: -1.490 [-100.000, 17.040], mean action: 1.063 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.672049
 36574/100000: episode: 416, duration: 0.768s, episode steps: 103, steps per second: 134, episode reward: -206.063, mean reward: -2.001 [-100.000,  5.183], mean action: 0.796 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.671302
 36641/100000: episode: 417, duration: 0.504s, episode steps:  67, steps per second: 133, episode reward: -72.860, mean reward: -1.087 [-100.000, 27.619], mean action: 1.075 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.670537
 36704/100000: episode: 418, duration: 0.510s, episode steps:  63, steps per second: 123, episode reward: -127.005, mean reward: -2.016 [-100.000,  5.488], mean action: 0.841 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.669952
 36789/100000: episode: 419, duration: 0.681s, episode steps:  85, steps per second: 125, episode reward: -145.563, mean reward: -1.713 [-100.000, 21.969], mean action: 1.082 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.669286
 36884/100000: episode: 420, duration: 0.713s, episode steps:  95, steps per second: 133, episode reward: -88.074, mean reward: -0.927 [-100.000, 12.821], mean action: 0.958 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.668476
 36970/100000: episode: 421, duration: 0.693s, episode steps:  86, steps per second: 124, episode reward: -158.730, mean reward: -1.846 [-100.000,  7.472], mean action: 1.012 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.667662
 37057/100000: episode: 422, duration: 0.674s, episode steps:  87, steps per second: 129, episode reward: -134.602, mean reward: -1.547 [-100.000,  6.850], mean action: 0.931 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.666883
 37150/100000: episode: 423, duration: 0.714s, episode steps:  93, steps per second: 130, episode reward: -136.703, mean reward: -1.470 [-100.000, 35.866], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.666073
 37217/100000: episode: 424, duration: 0.549s, episode steps:  67, steps per second: 122, episode reward: -103.105, mean reward: -1.539 [-100.000, 17.056], mean action: 0.881 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.665353
 37279/100000: episode: 425, duration: 0.492s, episode steps:  62, steps per second: 126, episode reward: -102.392, mean reward: -1.651 [-100.000, 25.443], mean action: 0.839 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.664773
 37377/100000: episode: 426, duration: 0.734s, episode steps:  98, steps per second: 134, episode reward: -264.413, mean reward: -2.698 [-100.000,  5.892], mean action: 1.143 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.664053
 37486/100000: episode: 427, duration: 0.827s, episode steps: 109, steps per second: 132, episode reward: -162.705, mean reward: -1.493 [-100.000,  7.787], mean action: 1.119 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.663121
 37583/100000: episode: 428, duration: 0.786s, episode steps:  97, steps per second: 123, episode reward: -171.269, mean reward: -1.766 [-100.000,  6.404], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.662194
 37674/100000: episode: 429, duration: 0.700s, episode steps:  91, steps per second: 130, episode reward: -107.727, mean reward: -1.184 [-100.000, 12.291], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.661348
 37780/100000: episode: 430, duration: 0.846s, episode steps: 106, steps per second: 125, episode reward: -181.742, mean reward: -1.715 [-100.000,  9.652], mean action: 0.991 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.660461
 37848/100000: episode: 431, duration: 0.598s, episode steps:  68, steps per second: 114, episode reward: -151.279, mean reward: -2.225 [-100.000,  9.311], mean action: 0.882 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.659678
 37936/100000: episode: 432, duration: 0.773s, episode steps:  88, steps per second: 114, episode reward: -351.112, mean reward: -3.990 [-100.000,  0.563], mean action: 1.216 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.658976
 37995/100000: episode: 433, duration: 0.492s, episode steps:  59, steps per second: 120, episode reward: -113.563, mean reward: -1.925 [-100.000,  6.061], mean action: 1.051 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.658315
 38056/100000: episode: 434, duration: 0.532s, episode steps:  61, steps per second: 115, episode reward: -148.239, mean reward: -2.430 [-100.000,  5.171], mean action: 0.803 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.657775
 38139/100000: episode: 435, duration: 0.677s, episode steps:  83, steps per second: 123, episode reward: -114.780, mean reward: -1.383 [-100.000,  7.942], mean action: 1.108 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.657127
 38226/100000: episode: 436, duration: 0.680s, episode steps:  87, steps per second: 128, episode reward: -182.969, mean reward: -2.103 [-100.000, 28.846], mean action: 0.977 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.656362
 38294/100000: episode: 437, duration: 0.584s, episode steps:  68, steps per second: 117, episode reward: -117.304, mean reward: -1.725 [-100.000,  9.412], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.655664
 38411/100000: episode: 438, duration: 0.899s, episode steps: 117, steps per second: 130, episode reward: -184.570, mean reward: -1.578 [-100.000,  6.575], mean action: 1.068 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.654832
 38492/100000: episode: 439, duration: 0.599s, episode steps:  81, steps per second: 135, episode reward: -178.396, mean reward: -2.202 [-100.000,  6.397], mean action: 0.864 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.653941
 38576/100000: episode: 440, duration: 0.655s, episode steps:  84, steps per second: 128, episode reward: -243.826, mean reward: -2.903 [-100.000,  5.787], mean action: 0.964 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.653198
 38664/100000: episode: 441, duration: 0.700s, episode steps:  88, steps per second: 126, episode reward: -1.915, mean reward: -0.022 [-100.000, 100.595], mean action: 0.807 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.652424
 38766/100000: episode: 442, duration: 0.769s, episode steps: 102, steps per second: 133, episode reward: -87.460, mean reward: -0.857 [-100.000,  6.672], mean action: 1.098 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.651569
 38841/100000: episode: 443, duration: 0.585s, episode steps:  75, steps per second: 128, episode reward: -139.643, mean reward: -1.862 [-100.000,  7.817], mean action: 0.960 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.650773
 38944/100000: episode: 444, duration: 0.810s, episode steps: 103, steps per second: 127, episode reward: -5.551, mean reward: -0.054 [-100.000, 102.000], mean action: 0.932 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.649972
 39021/100000: episode: 445, duration: 0.581s, episode steps:  77, steps per second: 132, episode reward: -119.821, mean reward: -1.556 [-100.000, 38.243], mean action: 1.052 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.649162
 39117/100000: episode: 446, duration: 0.750s, episode steps:  96, steps per second: 128, episode reward: -187.121, mean reward: -1.949 [-100.000, 31.033], mean action: 0.958 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.648384
 39205/100000: episode: 447, duration: 0.688s, episode steps:  88, steps per second: 128, episode reward: -103.349, mean reward: -1.174 [-100.000,  7.835], mean action: 0.932 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.647556
 39298/100000: episode: 448, duration: 0.699s, episode steps:  93, steps per second: 133, episode reward: 34.283, mean reward:  0.369 [-100.000, 121.421], mean action: 0.849 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.646741
 39359/100000: episode: 449, duration: 0.471s, episode steps:  61, steps per second: 130, episode reward: -95.450, mean reward: -1.565 [-100.000, 16.336], mean action: 0.934 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.646048
 39462/100000: episode: 450, duration: 0.855s, episode steps: 103, steps per second: 120, episode reward: -449.893, mean reward: -4.368 [-100.000, 61.805], mean action: 1.136 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.645310
 39534/100000: episode: 451, duration: 0.547s, episode steps:  72, steps per second: 132, episode reward: -78.060, mean reward: -1.084 [-100.000, 21.061], mean action: 0.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.644522
 39612/100000: episode: 452, duration: 0.583s, episode steps:  78, steps per second: 134, episode reward: -134.417, mean reward: -1.723 [-100.000,  6.386], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.643848
 39670/100000: episode: 453, duration: 0.453s, episode steps:  58, steps per second: 128, episode reward: -50.705, mean reward: -0.874 [-100.000, 17.119], mean action: 0.741 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.643236
 39739/100000: episode: 454, duration: 0.554s, episode steps:  69, steps per second: 125, episode reward: -82.953, mean reward: -1.202 [-100.000, 12.995], mean action: 0.812 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.642664
 39798/100000: episode: 455, duration: 0.480s, episode steps:  59, steps per second: 123, episode reward: -71.131, mean reward: -1.206 [-100.000, 16.490], mean action: 0.814 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.642088
 39873/100000: episode: 456, duration: 0.581s, episode steps:  75, steps per second: 129, episode reward: -128.654, mean reward: -1.715 [-100.000,  6.299], mean action: 1.120 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.641485
 39940/100000: episode: 457, duration: 0.528s, episode steps:  67, steps per second: 127, episode reward: -101.736, mean reward: -1.518 [-100.000,  7.453], mean action: 0.806 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.640846
 40000/100000: episode: 458, duration: 0.478s, episode steps:  60, steps per second: 126, episode reward: -127.376, mean reward: -2.123 [-100.000,  5.602], mean action: 1.067 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.640275
 40056/100000: episode: 459, duration: 0.451s, episode steps:  56, steps per second: 124, episode reward: -90.710, mean reward: -1.620 [-100.000,  7.880], mean action: 1.089 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.639752
 40132/100000: episode: 460, duration: 0.591s, episode steps:  76, steps per second: 128, episode reward: -89.979, mean reward: -1.184 [-100.000, 11.438], mean action: 1.053 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.639158
 40208/100000: episode: 461, duration: 0.627s, episode steps:  76, steps per second: 121, episode reward: -169.712, mean reward: -2.233 [-100.000, 16.234], mean action: 0.776 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.638474
 40268/100000: episode: 462, duration: 0.527s, episode steps:  60, steps per second: 114, episode reward: -89.687, mean reward: -1.495 [-100.000,  6.832], mean action: 0.950 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.637862
 40328/100000: episode: 463, duration: 0.471s, episode steps:  60, steps per second: 127, episode reward: -239.196, mean reward: -3.987 [-100.000,  6.474], mean action: 1.100 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.637323
 40445/100000: episode: 464, duration: 0.884s, episode steps: 117, steps per second: 132, episode reward: -171.219, mean reward: -1.463 [-100.000,  7.891], mean action: 0.897 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.636526
 40518/100000: episode: 465, duration: 0.586s, episode steps:  73, steps per second: 125, episode reward:  8.552, mean reward:  0.117 [-100.000, 123.151], mean action: 0.753 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.635671
 40581/100000: episode: 466, duration: 0.494s, episode steps:  63, steps per second: 128, episode reward: -104.887, mean reward: -1.665 [-100.000,  7.422], mean action: 0.746 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.635059
 40722/100000: episode: 467, duration: 1.075s, episode steps: 141, steps per second: 131, episode reward: -371.651, mean reward: -2.636 [-100.000, 117.197], mean action: 1.092 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.634141
 40816/100000: episode: 468, duration: 0.824s, episode steps:  94, steps per second: 114, episode reward: -298.536, mean reward: -3.176 [-100.000, -0.139], mean action: 0.830 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.633084
 40883/100000: episode: 469, duration: 0.656s, episode steps:  67, steps per second: 102, episode reward: -131.278, mean reward: -1.959 [-100.000,  6.538], mean action: 0.791 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.632359
 40989/100000: episode: 470, duration: 0.966s, episode steps: 106, steps per second: 110, episode reward: -252.000, mean reward: -2.377 [-100.000,  4.497], mean action: 1.113 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.631580
 41052/100000: episode: 471, duration: 0.592s, episode steps:  63, steps per second: 106, episode reward: -84.661, mean reward: -1.344 [-100.000, 10.431], mean action: 1.048 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.630820
 41126/100000: episode: 472, duration: 0.650s, episode steps:  74, steps per second: 114, episode reward: -56.365, mean reward: -0.762 [-100.000, 73.513], mean action: 0.905 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.630204
 41204/100000: episode: 473, duration: 0.645s, episode steps:  78, steps per second: 121, episode reward: -324.097, mean reward: -4.155 [-100.000,  5.314], mean action: 0.821 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.629520
 41268/100000: episode: 474, duration: 0.558s, episode steps:  64, steps per second: 115, episode reward: -110.869, mean reward: -1.732 [-100.000,  6.785], mean action: 1.078 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.628880
 41341/100000: episode: 475, duration: 0.648s, episode steps:  73, steps per second: 113, episode reward: -135.206, mean reward: -1.852 [-100.000,  7.133], mean action: 1.014 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.628264
 41407/100000: episode: 476, duration: 0.510s, episode steps:  66, steps per second: 129, episode reward: -131.870, mean reward: -1.998 [-100.000, 17.243], mean action: 0.636 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.627639
 41468/100000: episode: 477, duration: 0.467s, episode steps:  61, steps per second: 131, episode reward: -96.765, mean reward: -1.586 [-100.000,  6.903], mean action: 0.918 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.627067
 41548/100000: episode: 478, duration: 0.655s, episode steps:  80, steps per second: 122, episode reward: -105.932, mean reward: -1.324 [-100.000, 17.672], mean action: 1.125 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.626432
 41647/100000: episode: 479, duration: 0.763s, episode steps:  99, steps per second: 130, episode reward: -233.337, mean reward: -2.357 [-100.000, 49.795], mean action: 0.788 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.625627
 41706/100000: episode: 480, duration: 0.439s, episode steps:  59, steps per second: 134, episode reward: -113.056, mean reward: -1.916 [-100.000, 14.012], mean action: 1.085 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.624916
 41790/100000: episode: 481, duration: 0.664s, episode steps:  84, steps per second: 126, episode reward: -127.926, mean reward: -1.523 [-100.000,  7.591], mean action: 0.905 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.624273
 41857/100000: episode: 482, duration: 0.529s, episode steps:  67, steps per second: 127, episode reward: -63.332, mean reward: -0.945 [-100.000, 17.343], mean action: 1.104 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.623593
 41964/100000: episode: 483, duration: 0.832s, episode steps: 107, steps per second: 129, episode reward: -137.272, mean reward: -1.283 [-100.000,  6.641], mean action: 0.991 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.622810
 42046/100000: episode: 484, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -253.517, mean reward: -3.092 [-100.000,  4.842], mean action: 0.829 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.621959
 42138/100000: episode: 485, duration: 0.771s, episode steps:  92, steps per second: 119, episode reward: -158.868, mean reward: -1.727 [-100.000,  7.434], mean action: 0.913 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.621177
 42235/100000: episode: 486, duration: 0.738s, episode steps:  97, steps per second: 131, episode reward: -227.306, mean reward: -2.343 [-100.000, 29.191], mean action: 0.959 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.620326
 42341/100000: episode: 487, duration: 0.843s, episode steps: 106, steps per second: 126, episode reward: -173.662, mean reward: -1.638 [-100.000,  8.182], mean action: 1.019 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.619413
 42422/100000: episode: 488, duration: 0.649s, episode steps:  81, steps per second: 125, episode reward: -99.666, mean reward: -1.230 [-100.000,  8.467], mean action: 0.951 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.618571
 42536/100000: episode: 489, duration: 0.856s, episode steps: 114, steps per second: 133, episode reward: -314.960, mean reward: -2.763 [-100.000,  5.152], mean action: 0.912 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.617694
 42595/100000: episode: 490, duration: 0.444s, episode steps:  59, steps per second: 133, episode reward: -200.813, mean reward: -3.404 [-100.000,  6.605], mean action: 1.322 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.616915
 42664/100000: episode: 491, duration: 0.576s, episode steps:  69, steps per second: 120, episode reward: -205.381, mean reward: -2.977 [-100.000,  4.749], mean action: 0.797 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.616339
 42725/100000: episode: 492, duration: 0.473s, episode steps:  61, steps per second: 129, episode reward: -131.500, mean reward: -2.156 [-100.000,  9.439], mean action: 0.869 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.615754
 42822/100000: episode: 493, duration: 0.920s, episode steps:  97, steps per second: 105, episode reward: -111.690, mean reward: -1.151 [-100.000,  9.309], mean action: 0.928 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.615043
 42915/100000: episode: 494, duration: 0.793s, episode steps:  93, steps per second: 117, episode reward: -361.073, mean reward: -3.883 [-100.000,  4.405], mean action: 0.871 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.614188
 43017/100000: episode: 495, duration: 0.790s, episode steps: 102, steps per second: 129, episode reward: -375.734, mean reward: -3.684 [-100.000,  3.800], mean action: 0.843 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.613310
 43116/100000: episode: 496, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -117.705, mean reward: -1.189 [-100.000,  8.652], mean action: 0.970 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.612406
 43197/100000: episode: 497, duration: 0.661s, episode steps:  81, steps per second: 123, episode reward: -112.496, mean reward: -1.389 [-100.000,  8.986], mean action: 0.778 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.611596
 43270/100000: episode: 498, duration: 0.585s, episode steps:  73, steps per second: 125, episode reward: -161.694, mean reward: -2.215 [-100.000,  8.412], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.610903
 43329/100000: episode: 499, duration: 0.466s, episode steps:  59, steps per second: 127, episode reward: -95.627, mean reward: -1.621 [-100.000,  9.410], mean action: 1.068 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.610309
 43412/100000: episode: 500, duration: 0.732s, episode steps:  83, steps per second: 113, episode reward: -87.397, mean reward: -1.053 [-100.000, 16.862], mean action: 0.976 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.609670
 43504/100000: episode: 501, duration: 0.748s, episode steps:  92, steps per second: 123, episode reward: -122.788, mean reward: -1.335 [-100.000, 17.059], mean action: 0.761 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.608882
 43565/100000: episode: 502, duration: 0.469s, episode steps:  61, steps per second: 130, episode reward: -97.141, mean reward: -1.592 [-100.000,  7.532], mean action: 0.869 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.608194
 43635/100000: episode: 503, duration: 0.549s, episode steps:  70, steps per second: 128, episode reward: -146.716, mean reward: -2.096 [-100.000, 84.576], mean action: 1.071 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.607605
 43712/100000: episode: 504, duration: 0.623s, episode steps:  77, steps per second: 124, episode reward: -191.993, mean reward: -2.493 [-100.000,  5.408], mean action: 1.130 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.606943
 43791/100000: episode: 505, duration: 0.621s, episode steps:  79, steps per second: 127, episode reward: -139.218, mean reward: -1.762 [-100.000, 11.957], mean action: 1.063 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.606241
 43878/100000: episode: 506, duration: 0.658s, episode steps:  87, steps per second: 132, episode reward: -106.743, mean reward: -1.227 [-100.000, 16.692], mean action: 0.874 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.605494
 43941/100000: episode: 507, duration: 0.496s, episode steps:  63, steps per second: 127, episode reward: -106.261, mean reward: -1.687 [-100.000, 10.436], mean action: 1.000 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.604819
 44026/100000: episode: 508, duration: 0.719s, episode steps:  85, steps per second: 118, episode reward: -101.323, mean reward: -1.192 [-100.000, 17.234], mean action: 0.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.604153
 44092/100000: episode: 509, duration: 0.515s, episode steps:  66, steps per second: 128, episode reward: -174.753, mean reward: -2.648 [-100.000,  5.825], mean action: 0.773 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.603473
 44196/100000: episode: 510, duration: 0.789s, episode steps: 104, steps per second: 132, episode reward: -96.204, mean reward: -0.925 [-100.000, 13.750], mean action: 1.077 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.602709
 44276/100000: episode: 511, duration: 0.644s, episode steps:  80, steps per second: 124, episode reward: -139.715, mean reward: -1.746 [-100.000,  8.346], mean action: 0.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.601881
 44372/100000: episode: 512, duration: 0.737s, episode steps:  96, steps per second: 130, episode reward: -328.510, mean reward: -3.422 [-100.000, 107.823], mean action: 0.656 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.601089
 44495/100000: episode: 513, duration: 0.941s, episode steps: 123, steps per second: 131, episode reward: -261.454, mean reward: -2.126 [-100.000, 89.887], mean action: 1.130 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.600103
 44577/100000: episode: 514, duration: 0.679s, episode steps:  82, steps per second: 121, episode reward: -124.413, mean reward: -1.517 [-100.000,  6.968], mean action: 0.854 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.599180
 44659/100000: episode: 515, duration: 0.630s, episode steps:  82, steps per second: 130, episode reward: -198.247, mean reward: -2.418 [-100.000, 26.098], mean action: 0.878 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.598442
 44751/100000: episode: 516, duration: 0.696s, episode steps:  92, steps per second: 132, episode reward: -279.878, mean reward: -3.042 [-100.000, 20.450], mean action: 0.750 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.597660
 44832/100000: episode: 517, duration: 0.662s, episode steps:  81, steps per second: 122, episode reward: -131.257, mean reward: -1.620 [-100.000,  7.158], mean action: 0.802 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.596881
 44928/100000: episode: 518, duration: 0.760s, episode steps:  96, steps per second: 126, episode reward: -118.577, mean reward: -1.235 [-100.000, 77.012], mean action: 0.927 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.596085
 44985/100000: episode: 519, duration: 0.433s, episode steps:  57, steps per second: 132, episode reward: -108.022, mean reward: -1.895 [-100.000, 32.828], mean action: 0.912 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.595396
 45043/100000: episode: 520, duration: 0.452s, episode steps:  58, steps per second: 128, episode reward: -138.573, mean reward: -2.389 [-100.000,  7.435], mean action: 0.776 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.594878
 45135/100000: episode: 521, duration: 0.766s, episode steps:  92, steps per second: 120, episode reward: -151.358, mean reward: -1.645 [-100.000, 27.824], mean action: 0.957 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.594203
 45230/100000: episode: 522, duration: 0.730s, episode steps:  95, steps per second: 130, episode reward: -152.623, mean reward: -1.607 [-100.000,  6.102], mean action: 0.779 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.593362
 45331/100000: episode: 523, duration: 0.806s, episode steps: 101, steps per second: 125, episode reward: -99.487, mean reward: -0.985 [-100.000, 11.335], mean action: 0.802 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.592480
 45391/100000: episode: 524, duration: 0.477s, episode steps:  60, steps per second: 126, episode reward: -177.398, mean reward: -2.957 [-100.000,  5.316], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.591755
 45504/100000: episode: 525, duration: 0.868s, episode steps: 113, steps per second: 130, episode reward: -131.231, mean reward: -1.161 [-100.000, 12.099], mean action: 0.929 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.590977
 45563/100000: episode: 526, duration: 0.455s, episode steps:  59, steps per second: 130, episode reward: -91.746, mean reward: -1.555 [-100.000, 16.673], mean action: 1.017 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.590203
 45661/100000: episode: 527, duration: 0.814s, episode steps:  98, steps per second: 120, episode reward: -141.763, mean reward: -1.447 [-100.000,  4.603], mean action: 0.857 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.589496
 45763/100000: episode: 528, duration: 0.784s, episode steps: 102, steps per second: 130, episode reward: -295.366, mean reward: -2.896 [-100.000, 27.537], mean action: 0.961 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.588596
 45854/100000: episode: 529, duration: 0.692s, episode steps:  91, steps per second: 132, episode reward: -258.354, mean reward: -2.839 [-100.000, 60.005], mean action: 0.989 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.587728
 45915/100000: episode: 530, duration: 0.515s, episode steps:  61, steps per second: 118, episode reward: -104.700, mean reward: -1.716 [-100.000, 11.426], mean action: 0.918 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.587044
 45977/100000: episode: 531, duration: 0.494s, episode steps:  62, steps per second: 125, episode reward: -142.264, mean reward: -2.295 [-100.000,  7.959], mean action: 0.790 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.586491
 46066/100000: episode: 532, duration: 0.787s, episode steps:  89, steps per second: 113, episode reward: -139.151, mean reward: -1.563 [-100.000,  6.266], mean action: 0.854 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.585811
 46158/100000: episode: 533, duration: 0.804s, episode steps:  92, steps per second: 114, episode reward: -217.330, mean reward: -2.362 [-100.000, 12.756], mean action: 0.804 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.584996
 46234/100000: episode: 534, duration: 0.640s, episode steps:  76, steps per second: 119, episode reward: -218.570, mean reward: -2.876 [-100.000,  5.835], mean action: 0.763 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.584240
 46298/100000: episode: 535, duration: 0.552s, episode steps:  64, steps per second: 116, episode reward: -179.945, mean reward: -2.812 [-100.000, 70.126], mean action: 0.625 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.583610
 46359/100000: episode: 536, duration: 0.496s, episode steps:  61, steps per second: 123, episode reward: -46.481, mean reward: -0.762 [-100.000, 95.801], mean action: 0.885 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.583048
 46463/100000: episode: 537, duration: 0.893s, episode steps: 104, steps per second: 116, episode reward: -70.267, mean reward: -0.676 [-100.000, 80.149], mean action: 0.865 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.582306
 46526/100000: episode: 538, duration: 0.506s, episode steps:  63, steps per second: 125, episode reward: -123.275, mean reward: -1.957 [-100.000,  7.717], mean action: 0.857 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.581554
 46615/100000: episode: 539, duration: 0.677s, episode steps:  89, steps per second: 131, episode reward: -171.977, mean reward: -1.932 [-100.000,  8.849], mean action: 0.708 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.580870
 46681/100000: episode: 540, duration: 0.533s, episode steps:  66, steps per second: 124, episode reward: -156.330, mean reward: -2.369 [-100.000,  6.306], mean action: 1.076 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.580173
 46785/100000: episode: 541, duration: 0.843s, episode steps: 104, steps per second: 123, episode reward: -98.093, mean reward: -0.943 [-100.000,  6.307], mean action: 0.971 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.579408
 46874/100000: episode: 542, duration: 0.679s, episode steps:  89, steps per second: 131, episode reward: -178.353, mean reward: -2.004 [-100.000,  6.859], mean action: 0.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.578539
 46950/100000: episode: 543, duration: 0.613s, episode steps:  76, steps per second: 124, episode reward: -122.770, mean reward: -1.615 [-100.000, 12.049], mean action: 0.658 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.577797
 47032/100000: episode: 544, duration: 0.653s, episode steps:  82, steps per second: 125, episode reward: 50.591, mean reward:  0.617 [-100.000, 131.779], mean action: 1.134 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.577086
 47155/100000: episode: 545, duration: 0.988s, episode steps: 123, steps per second: 125, episode reward: -462.418, mean reward: -3.759 [-100.000, 99.888], mean action: 0.756 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.576163
 47217/100000: episode: 546, duration: 0.507s, episode steps:  62, steps per second: 122, episode reward: -76.591, mean reward: -1.235 [-100.000, 16.051], mean action: 0.952 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.575330
 47284/100000: episode: 547, duration: 0.535s, episode steps:  67, steps per second: 125, episode reward: -106.481, mean reward: -1.589 [-100.000, 14.575], mean action: 0.806 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.574750
 47394/100000: episode: 548, duration: 0.847s, episode steps: 110, steps per second: 130, episode reward: -195.586, mean reward: -1.778 [-100.000, 39.568], mean action: 0.773 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.573954
 47460/100000: episode: 549, duration: 0.509s, episode steps:  66, steps per second: 130, episode reward: -130.891, mean reward: -1.983 [-100.000,  8.998], mean action: 0.924 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.573161
 47573/100000: episode: 550, duration: 0.928s, episode steps: 113, steps per second: 122, episode reward: -211.434, mean reward: -1.871 [-100.000,  6.143], mean action: 0.938 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.572356
 47670/100000: episode: 551, duration: 0.788s, episode steps:  97, steps per second: 123, episode reward: -123.034, mean reward: -1.268 [-100.000,  8.955], mean action: 0.701 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.571411
 47773/100000: episode: 552, duration: 0.869s, episode steps: 103, steps per second: 119, episode reward: -171.430, mean reward: -1.664 [-100.000, 23.611], mean action: 0.874 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.570511
 47873/100000: episode: 553, duration: 0.791s, episode steps: 100, steps per second: 127, episode reward: -115.485, mean reward: -1.155 [-100.000, 14.676], mean action: 0.860 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.569597
 47943/100000: episode: 554, duration: 0.532s, episode steps:  70, steps per second: 132, episode reward: -102.543, mean reward: -1.465 [-100.000, 14.780], mean action: 0.700 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.568832
 48000/100000: episode: 555, duration: 0.458s, episode steps:  57, steps per second: 124, episode reward: -176.277, mean reward: -3.093 [-100.000,  6.418], mean action: 0.965 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.568261
 48067/100000: episode: 556, duration: 0.549s, episode steps:  67, steps per second: 122, episode reward: -126.211, mean reward: -1.884 [-100.000, 12.091], mean action: 0.910 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.567703
 48159/100000: episode: 557, duration: 0.713s, episode steps:  92, steps per second: 129, episode reward: -154.596, mean reward: -1.680 [-100.000, 16.380], mean action: 0.891 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.566987
 48262/100000: episode: 558, duration: 0.815s, episode steps: 103, steps per second: 126, episode reward: -133.825, mean reward: -1.299 [-100.000, 11.385], mean action: 0.816 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.566110
 48339/100000: episode: 559, duration: 0.635s, episode steps:  77, steps per second: 121, episode reward: -148.764, mean reward: -1.932 [-100.000,  9.101], mean action: 0.714 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.565300
 48415/100000: episode: 560, duration: 0.593s, episode steps:  76, steps per second: 128, episode reward: -134.187, mean reward: -1.766 [-100.000, 19.257], mean action: 0.868 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.564612
 48516/100000: episode: 561, duration: 0.781s, episode steps: 101, steps per second: 129, episode reward: -306.455, mean reward: -3.034 [-100.000, 139.169], mean action: 0.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.563815
 48598/100000: episode: 562, duration: 0.687s, episode steps:  82, steps per second: 119, episode reward: -120.623, mean reward: -1.471 [-100.000,  9.064], mean action: 0.927 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.562991
 48658/100000: episode: 563, duration: 0.499s, episode steps:  60, steps per second: 120, episode reward: -126.786, mean reward: -2.113 [-100.000, 14.615], mean action: 0.867 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.562352
 48730/100000: episode: 564, duration: 0.551s, episode steps:  72, steps per second: 131, episode reward: -144.002, mean reward: -2.000 [-100.000, 16.249], mean action: 0.750 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.561759
 48812/100000: episode: 565, duration: 0.631s, episode steps:  82, steps per second: 130, episode reward: -118.170, mean reward: -1.441 [-100.000,  7.034], mean action: 0.915 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.561065
 48874/100000: episode: 566, duration: 0.507s, episode steps:  62, steps per second: 122, episode reward: -140.934, mean reward: -2.273 [-100.000,  7.650], mean action: 0.758 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.560418
 49008/100000: episode: 567, duration: 1.127s, episode steps: 134, steps per second: 119, episode reward: -296.168, mean reward: -2.210 [-100.000, 115.173], mean action: 0.724 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.559535
 49079/100000: episode: 568, duration: 0.685s, episode steps:  71, steps per second: 104, episode reward: -162.146, mean reward: -2.284 [-100.000,  7.507], mean action: 0.972 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.558613
 49167/100000: episode: 569, duration: 0.834s, episode steps:  88, steps per second: 106, episode reward: -275.668, mean reward: -3.133 [-100.000, 109.146], mean action: 0.875 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.557898
 49259/100000: episode: 570, duration: 0.804s, episode steps:  92, steps per second: 114, episode reward: -132.089, mean reward: -1.436 [-100.000,  9.005], mean action: 0.837 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.557087
 49343/100000: episode: 571, duration: 0.793s, episode steps:  84, steps per second: 106, episode reward: -128.031, mean reward: -1.524 [-100.000, 19.330], mean action: 0.917 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.556295
 49399/100000: episode: 572, duration: 0.639s, episode steps:  56, steps per second:  88, episode reward: -85.800, mean reward: -1.532 [-100.000, 17.484], mean action: 0.714 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.555666
 49489/100000: episode: 573, duration: 0.852s, episode steps:  90, steps per second: 106, episode reward: -134.670, mean reward: -1.496 [-100.000,  6.980], mean action: 0.833 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.555009
 49585/100000: episode: 574, duration: 0.815s, episode steps:  96, steps per second: 118, episode reward: -100.723, mean reward: -1.049 [-100.000, 12.102], mean action: 0.823 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.554171
 49688/100000: episode: 575, duration: 0.822s, episode steps: 103, steps per second: 125, episode reward: -94.742, mean reward: -0.920 [-100.000, 23.633], mean action: 0.767 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.553276
 49776/100000: episode: 576, duration: 0.685s, episode steps:  88, steps per second: 128, episode reward: -143.026, mean reward: -1.625 [-100.000, 13.408], mean action: 0.761 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.552416
 49857/100000: episode: 577, duration: 0.711s, episode steps:  81, steps per second: 114, episode reward: -103.786, mean reward: -1.281 [-100.000,  6.247], mean action: 0.593 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.551656
 49948/100000: episode: 578, duration: 0.739s, episode steps:  91, steps per second: 123, episode reward: -164.885, mean reward: -1.812 [-100.000,  6.021], mean action: 0.846 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.550882
 50050/100000: episode: 579, duration: 0.822s, episode steps: 102, steps per second: 124, episode reward: -335.968, mean reward: -3.294 [-100.000, 105.317], mean action: 0.971 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.550014
 50151/100000: episode: 580, duration: 0.910s, episode steps: 101, steps per second: 111, episode reward: -129.534, mean reward: -1.283 [-100.000,  6.889], mean action: 0.663 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.549100
 50273/100000: episode: 581, duration: 1.185s, episode steps: 122, steps per second: 103, episode reward: -187.036, mean reward: -1.533 [-100.000, 54.149], mean action: 1.008 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.548096
 50372/100000: episode: 582, duration: 1.112s, episode steps:  99, steps per second:  89, episode reward: -140.185, mean reward: -1.416 [-100.000,  5.127], mean action: 0.838 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.547102
 50470/100000: episode: 583, duration: 0.918s, episode steps:  98, steps per second: 107, episode reward: -105.377, mean reward: -1.075 [-100.000,  7.658], mean action: 0.704 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.546215
 50590/100000: episode: 584, duration: 1.076s, episode steps: 120, steps per second: 112, episode reward: -301.159, mean reward: -2.510 [-100.000, 101.479], mean action: 0.800 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.545234
 50660/100000: episode: 585, duration: 0.641s, episode steps:  70, steps per second: 109, episode reward: -142.682, mean reward: -2.038 [-100.000, 13.729], mean action: 0.757 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.544380
 50780/100000: episode: 586, duration: 1.455s, episode steps: 120, steps per second:  82, episode reward: -10.772, mean reward: -0.090 [-100.000, 93.754], mean action: 0.825 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.543524
 50843/100000: episode: 587, duration: 0.728s, episode steps:  63, steps per second:  87, episode reward: -105.419, mean reward: -1.673 [-100.000, 52.591], mean action: 0.810 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.542701
 50930/100000: episode: 588, duration: 0.941s, episode steps:  87, steps per second:  92, episode reward: -108.180, mean reward: -1.243 [-100.000, 13.675], mean action: 0.908 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.542026
 51050/100000: episode: 589, duration: 1.072s, episode steps: 120, steps per second: 112, episode reward: -331.150, mean reward: -2.760 [-100.000, 123.483], mean action: 0.783 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.541095
 51136/100000: episode: 590, duration: 0.711s, episode steps:  86, steps per second: 121, episode reward: -118.535, mean reward: -1.378 [-100.000, 16.804], mean action: 0.942 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.540168
 51194/100000: episode: 591, duration: 0.496s, episode steps:  58, steps per second: 117, episode reward: -78.625, mean reward: -1.356 [-100.000, 10.294], mean action: 0.724 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.539520
 51300/100000: episode: 592, duration: 0.889s, episode steps: 106, steps per second: 119, episode reward: -74.952, mean reward: -0.707 [-100.000, 86.643], mean action: 0.868 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.538781
 51417/100000: episode: 593, duration: 0.954s, episode steps: 117, steps per second: 123, episode reward: -377.242, mean reward: -3.224 [-100.000,  1.589], mean action: 0.769 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.537778
 51492/100000: episode: 594, duration: 0.729s, episode steps:  75, steps per second: 103, episode reward: 34.327, mean reward:  0.458 [-100.000, 116.927], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.536914
 51556/100000: episode: 595, duration: 0.764s, episode steps:  64, steps per second:  84, episode reward: -108.730, mean reward: -1.699 [-100.000, 17.026], mean action: 0.781 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.536288
 51619/100000: episode: 596, duration: 0.558s, episode steps:  63, steps per second: 113, episode reward: -89.016, mean reward: -1.413 [-100.000, 22.002], mean action: 0.746 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.535717
 51688/100000: episode: 597, duration: 0.628s, episode steps:  69, steps per second: 110, episode reward: -130.632, mean reward: -1.893 [-100.000, 15.985], mean action: 0.841 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.535123
 51772/100000: episode: 598, duration: 0.691s, episode steps:  84, steps per second: 122, episode reward: -217.288, mean reward: -2.587 [-100.000, 58.576], mean action: 1.060 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.534434
 51864/100000: episode: 599, duration: 0.759s, episode steps:  92, steps per second: 121, episode reward: -112.711, mean reward: -1.225 [-100.000, 46.665], mean action: 0.848 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.533643
 51965/100000: episode: 600, duration: 0.923s, episode steps: 101, steps per second: 109, episode reward: -127.428, mean reward: -1.262 [-100.000,  7.773], mean action: 0.772 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.532774
 52026/100000: episode: 601, duration: 0.610s, episode steps:  61, steps per second: 100, episode reward: -83.218, mean reward: -1.364 [-100.000, 17.031], mean action: 0.787 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.532045
 52098/100000: episode: 602, duration: 0.721s, episode steps:  72, steps per second: 100, episode reward: -113.608, mean reward: -1.578 [-100.000, 12.487], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.531446
 52156/100000: episode: 603, duration: 0.631s, episode steps:  58, steps per second:  92, episode reward: -78.427, mean reward: -1.352 [-100.000, 16.950], mean action: 0.776 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.530861
 52222/100000: episode: 604, duration: 0.669s, episode steps:  66, steps per second:  99, episode reward: -19.919, mean reward: -0.302 [-100.000, 85.330], mean action: 0.742 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.530304
 52277/100000: episode: 605, duration: 0.495s, episode steps:  55, steps per second: 111, episode reward: -97.362, mean reward: -1.770 [-100.000,  6.877], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.529759
 52357/100000: episode: 606, duration: 0.781s, episode steps:  80, steps per second: 102, episode reward: -229.349, mean reward: -2.867 [-100.000,  5.724], mean action: 0.775 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.529151
 52423/100000: episode: 607, duration: 0.587s, episode steps:  66, steps per second: 113, episode reward: -152.233, mean reward: -2.307 [-100.000, 10.026], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.528494
 52509/100000: episode: 608, duration: 0.768s, episode steps:  86, steps per second: 112, episode reward: -116.307, mean reward: -1.352 [-100.000, 11.359], mean action: 0.616 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.527810
 52576/100000: episode: 609, duration: 0.627s, episode steps:  67, steps per second: 107, episode reward: -129.423, mean reward: -1.932 [-100.000, 10.536], mean action: 0.806 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.527122
 52703/100000: episode: 610, duration: 1.106s, episode steps: 127, steps per second: 115, episode reward: -191.777, mean reward: -1.510 [-100.000,  5.648], mean action: 0.843 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.526249
 52793/100000: episode: 611, duration: 0.805s, episode steps:  90, steps per second: 112, episode reward: -100.911, mean reward: -1.121 [-100.000, 12.180], mean action: 0.922 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.525272
 52866/100000: episode: 612, duration: 0.660s, episode steps:  73, steps per second: 111, episode reward: -229.967, mean reward: -3.150 [-100.000, 85.060], mean action: 0.740 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.524539
 52949/100000: episode: 613, duration: 0.718s, episode steps:  83, steps per second: 116, episode reward: -145.137, mean reward: -1.749 [-100.000,  8.956], mean action: 0.831 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.523837
 53012/100000: episode: 614, duration: 0.586s, episode steps:  63, steps per second: 107, episode reward: -117.068, mean reward: -1.858 [-100.000,  7.125], mean action: 0.714 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.523180
 53110/100000: episode: 615, duration: 0.899s, episode steps:  98, steps per second: 109, episode reward: -148.089, mean reward: -1.511 [-100.000,  6.495], mean action: 0.745 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.522455
 53166/100000: episode: 616, duration: 0.484s, episode steps:  56, steps per second: 116, episode reward: -14.726, mean reward: -0.263 [-100.000, 65.484], mean action: 0.786 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.521763
 53250/100000: episode: 617, duration: 0.674s, episode steps:  84, steps per second: 125, episode reward: -17.949, mean reward: -0.214 [-100.000, 79.460], mean action: 0.893 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.521132
 53366/100000: episode: 618, duration: 0.961s, episode steps: 116, steps per second: 121, episode reward: 11.638, mean reward:  0.100 [-100.000, 104.612], mean action: 0.638 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.520232
 53451/100000: episode: 619, duration: 0.670s, episode steps:  85, steps per second: 127, episode reward: -189.937, mean reward: -2.235 [-100.000,  6.587], mean action: 0.812 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.519328
 53553/100000: episode: 620, duration: 1.051s, episode steps: 102, steps per second:  97, episode reward: -113.468, mean reward: -1.112 [-100.000,  8.202], mean action: 0.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.518486
 53618/100000: episode: 621, duration: 0.966s, episode steps:  65, steps per second:  67, episode reward: -95.582, mean reward: -1.470 [-100.000, 10.268], mean action: 0.723 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.517735
 53694/100000: episode: 622, duration: 0.846s, episode steps:  76, steps per second:  90, episode reward: -96.981, mean reward: -1.276 [-100.000, 14.515], mean action: 0.974 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.517100
 53768/100000: episode: 623, duration: 0.825s, episode steps:  74, steps per second:  90, episode reward: -139.430, mean reward: -1.884 [-100.000, 15.906], mean action: 0.919 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.516426
 53836/100000: episode: 624, duration: 0.621s, episode steps:  68, steps per second: 110, episode reward: -96.480, mean reward: -1.419 [-100.000, 10.997], mean action: 1.147 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.515786
 53909/100000: episode: 625, duration: 0.643s, episode steps:  73, steps per second: 114, episode reward: -123.650, mean reward: -1.694 [-100.000,  7.318], mean action: 0.959 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.515152
 53990/100000: episode: 626, duration: 0.733s, episode steps:  81, steps per second: 111, episode reward: -108.129, mean reward: -1.335 [-100.000,  7.230], mean action: 0.815 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.514459
 54071/100000: episode: 627, duration: 0.733s, episode steps:  81, steps per second: 110, episode reward: -130.679, mean reward: -1.613 [-100.000,  8.188], mean action: 0.716 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.513730
 54149/100000: episode: 628, duration: 0.717s, episode steps:  78, steps per second: 109, episode reward: -99.412, mean reward: -1.275 [-100.000, 17.379], mean action: 0.603 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.513014
 54236/100000: episode: 629, duration: 0.991s, episode steps:  87, steps per second:  88, episode reward: -150.210, mean reward: -1.727 [-100.000,  6.058], mean action: 0.747 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.512272
 54329/100000: episode: 630, duration: 0.844s, episode steps:  93, steps per second: 110, episode reward: -248.147, mean reward: -2.668 [-100.000,  4.357], mean action: 0.882 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.511462
 54396/100000: episode: 631, duration: 0.595s, episode steps:  67, steps per second: 113, episode reward: -162.123, mean reward: -2.420 [-100.000, 19.531], mean action: 0.881 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.510742
 54462/100000: episode: 632, duration: 0.673s, episode steps:  66, steps per second:  98, episode reward: -150.428, mean reward: -2.279 [-100.000,  9.296], mean action: 0.864 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.510143
 54541/100000: episode: 633, duration: 0.714s, episode steps:  79, steps per second: 111, episode reward: -254.089, mean reward: -3.216 [-100.000, 10.808], mean action: 0.747 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.509491
 54600/100000: episode: 634, duration: 0.524s, episode steps:  59, steps per second: 113, episode reward: -98.247, mean reward: -1.665 [-100.000, 41.128], mean action: 0.644 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.508870
 54702/100000: episode: 635, duration: 0.969s, episode steps: 102, steps per second: 105, episode reward: -75.661, mean reward: -0.742 [-100.000, 17.641], mean action: 0.784 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.508146
 54762/100000: episode: 636, duration: 0.539s, episode steps:  60, steps per second: 111, episode reward: -139.082, mean reward: -2.318 [-100.000,  5.865], mean action: 0.833 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.507417
 54840/100000: episode: 637, duration: 0.703s, episode steps:  78, steps per second: 111, episode reward: -136.606, mean reward: -1.751 [-100.000, 13.154], mean action: 1.051 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.506796
 54903/100000: episode: 638, duration: 0.613s, episode steps:  63, steps per second: 103, episode reward: -106.156, mean reward: -1.685 [-100.000, 19.979], mean action: 0.698 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.506161
 54967/100000: episode: 639, duration: 0.617s, episode steps:  64, steps per second: 104, episode reward: -98.959, mean reward: -1.546 [-100.000,  6.308], mean action: 0.797 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.505590
 55041/100000: episode: 640, duration: 0.671s, episode steps:  74, steps per second: 110, episode reward: -97.539, mean reward: -1.318 [-100.000, 11.760], mean action: 0.865 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.504969
 55101/100000: episode: 641, duration: 0.535s, episode steps:  60, steps per second: 112, episode reward: -66.643, mean reward: -1.111 [-100.000, 17.066], mean action: 0.817 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.504366
 55192/100000: episode: 642, duration: 0.837s, episode steps:  91, steps per second: 109, episode reward: -292.332, mean reward: -3.212 [-100.000,  0.503], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.503686
 55251/100000: episode: 643, duration: 0.557s, episode steps:  59, steps per second: 106, episode reward: -132.706, mean reward: -2.249 [-100.000, 14.604], mean action: 0.864 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.503011
 55309/100000: episode: 644, duration: 0.519s, episode steps:  58, steps per second: 112, episode reward: -105.752, mean reward: -1.823 [-100.000, 24.841], mean action: 0.690 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.502485
 55398/100000: episode: 645, duration: 0.830s, episode steps:  89, steps per second: 107, episode reward: -213.178, mean reward: -2.395 [-100.000, 22.676], mean action: 0.674 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.501823
 55487/100000: episode: 646, duration: 0.878s, episode steps:  89, steps per second: 101, episode reward: -109.046, mean reward: -1.225 [-100.000, 10.098], mean action: 0.719 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.501022
 55594/100000: episode: 647, duration: 0.993s, episode steps: 107, steps per second: 108, episode reward: -129.013, mean reward: -1.206 [-100.000,  6.901], mean action: 0.729 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.500140
 55717/100000: episode: 648, duration: 1.116s, episode steps: 123, steps per second: 110, episode reward: -109.032, mean reward: -0.886 [-100.000, 100.951], mean action: 0.797 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.499105
 55805/100000: episode: 649, duration: 0.773s, episode steps:  88, steps per second: 114, episode reward: -224.625, mean reward: -2.553 [-100.000, 24.673], mean action: 0.750 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.498156
 55870/100000: episode: 650, duration: 0.645s, episode steps:  65, steps per second: 101, episode reward: -64.689, mean reward: -0.995 [-100.000, 12.586], mean action: 0.769 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.497467
 55986/100000: episode: 651, duration: 1.188s, episode steps: 116, steps per second:  98, episode reward: -29.153, mean reward: -0.251 [-100.000, 54.780], mean action: 0.741 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.496652
 56072/100000: episode: 652, duration: 1.078s, episode steps:  86, steps per second:  80, episode reward: -154.145, mean reward: -1.792 [-100.000, 10.098], mean action: 0.686 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.495744
 56135/100000: episode: 653, duration: 0.701s, episode steps:  63, steps per second:  90, episode reward: -143.087, mean reward: -2.271 [-100.000, 18.590], mean action: 0.905 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.495073
 56243/100000: episode: 654, duration: 1.122s, episode steps: 108, steps per second:  96, episode reward: -167.237, mean reward: -1.548 [-100.000,  5.911], mean action: 0.630 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.494303
 56306/100000: episode: 655, duration: 0.607s, episode steps:  63, steps per second: 104, episode reward: -95.589, mean reward: -1.517 [-100.000,  7.047], mean action: 0.921 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.493534
 56393/100000: episode: 656, duration: 0.809s, episode steps:  87, steps per second: 108, episode reward: -167.305, mean reward: -1.923 [-100.000, 20.639], mean action: 0.575 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.492859
 56452/100000: episode: 657, duration: 0.594s, episode steps:  59, steps per second:  99, episode reward: -139.088, mean reward: -2.357 [-100.000,  8.276], mean action: 0.492 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.492202
 56517/100000: episode: 658, duration: 0.572s, episode steps:  65, steps per second: 114, episode reward: -94.468, mean reward: -1.453 [-100.000,  6.806], mean action: 0.831 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.491644
 56587/100000: episode: 659, duration: 0.584s, episode steps:  70, steps per second: 120, episode reward: -121.182, mean reward: -1.731 [-100.000, 10.068], mean action: 0.957 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.491036
 56655/100000: episode: 660, duration: 0.696s, episode steps:  68, steps per second:  98, episode reward: -140.298, mean reward: -2.063 [-100.000, 22.129], mean action: 0.676 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.490415
 56715/100000: episode: 661, duration: 0.601s, episode steps:  60, steps per second: 100, episode reward: -112.658, mean reward: -1.878 [-100.000,  7.586], mean action: 0.817 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.489840
 56801/100000: episode: 662, duration: 0.825s, episode steps:  86, steps per second: 104, episode reward: -143.320, mean reward: -1.667 [-100.000,  7.978], mean action: 0.814 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.489182
 56864/100000: episode: 663, duration: 0.581s, episode steps:  63, steps per second: 108, episode reward: -122.176, mean reward: -1.939 [-100.000,  7.360], mean action: 0.730 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.488512
 56938/100000: episode: 664, duration: 0.735s, episode steps:  74, steps per second: 101, episode reward: -143.897, mean reward: -1.945 [-100.000,  8.630], mean action: 0.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.487895
 57052/100000: episode: 665, duration: 1.067s, episode steps: 114, steps per second: 107, episode reward: -281.559, mean reward: -2.470 [-100.000, 119.144], mean action: 0.605 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.487050
 57148/100000: episode: 666, duration: 0.792s, episode steps:  96, steps per second: 121, episode reward: -118.298, mean reward: -1.232 [-100.000,  6.354], mean action: 0.771 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.486104
 57216/100000: episode: 667, duration: 0.607s, episode steps:  68, steps per second: 112, episode reward: -95.444, mean reward: -1.404 [-100.000, 16.962], mean action: 0.912 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.485367
 57301/100000: episode: 668, duration: 0.730s, episode steps:  85, steps per second: 116, episode reward: -128.906, mean reward: -1.517 [-100.000,  7.046], mean action: 0.706 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.484678
 57370/100000: episode: 669, duration: 0.580s, episode steps:  69, steps per second: 119, episode reward: -103.602, mean reward: -1.501 [-100.000, 16.982], mean action: 0.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.483985
 57424/100000: episode: 670, duration: 0.521s, episode steps:  54, steps per second: 104, episode reward: -194.014, mean reward: -3.593 [-100.000,  5.916], mean action: 0.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.483431
 57494/100000: episode: 671, duration: 0.745s, episode steps:  70, steps per second:  94, episode reward: -148.746, mean reward: -2.125 [-100.000, 11.470], mean action: 0.743 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.482873
 57577/100000: episode: 672, duration: 0.813s, episode steps:  83, steps per second: 102, episode reward: -156.806, mean reward: -1.889 [-100.000,  7.367], mean action: 0.795 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.482185
 57645/100000: episode: 673, duration: 0.783s, episode steps:  68, steps per second:  87, episode reward: -210.219, mean reward: -3.091 [-100.000,  6.113], mean action: 0.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.481505
 57740/100000: episode: 674, duration: 1.137s, episode steps:  95, steps per second:  84, episode reward: -428.570, mean reward: -4.511 [-100.000, 83.312], mean action: 0.737 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.480772
 57801/100000: episode: 675, duration: 0.677s, episode steps:  61, steps per second:  90, episode reward: -96.621, mean reward: -1.584 [-100.000, 11.993], mean action: 0.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.480070
 57895/100000: episode: 676, duration: 1.028s, episode steps:  94, steps per second:  91, episode reward: -107.240, mean reward: -1.141 [-100.000, 22.324], mean action: 0.649 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.479373
 57959/100000: episode: 677, duration: 0.803s, episode steps:  64, steps per second:  80, episode reward: -120.021, mean reward: -1.875 [-100.000,  9.335], mean action: 0.688 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.478661
 58040/100000: episode: 678, duration: 0.883s, episode steps:  81, steps per second:  92, episode reward: -126.487, mean reward: -1.562 [-100.000,  8.234], mean action: 0.889 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.478009
 58136/100000: episode: 679, duration: 1.805s, episode steps:  96, steps per second:  53, episode reward: -268.003, mean reward: -2.792 [-100.000,  5.154], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.477212
 58209/100000: episode: 680, duration: 1.269s, episode steps:  73, steps per second:  58, episode reward: -105.678, mean reward: -1.448 [-100.000, 20.351], mean action: 0.849 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.476452
 58298/100000: episode: 681, duration: 1.221s, episode steps:  89, steps per second:  73, episode reward: -120.418, mean reward: -1.353 [-100.000,  7.676], mean action: 0.775 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.475723
 58363/100000: episode: 682, duration: 1.024s, episode steps:  65, steps per second:  63, episode reward: -146.834, mean reward: -2.259 [-100.000,  6.919], mean action: 0.662 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.475030
 58454/100000: episode: 683, duration: 1.113s, episode steps:  91, steps per second:  82, episode reward: -230.349, mean reward: -2.531 [-100.000,  4.460], mean action: 0.824 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.474328
 58508/100000: episode: 684, duration: 0.697s, episode steps:  54, steps per second:  77, episode reward: -84.137, mean reward: -1.558 [-100.000, 23.839], mean action: 0.611 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.473675
 58567/100000: episode: 685, duration: 0.585s, episode steps:  59, steps per second: 101, episode reward: -88.391, mean reward: -1.498 [-100.000, 80.946], mean action: 0.678 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.473167
 58625/100000: episode: 686, duration: 0.606s, episode steps:  58, steps per second:  96, episode reward: -110.648, mean reward: -1.908 [-100.000,  6.993], mean action: 0.638 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.472641
 58724/100000: episode: 687, duration: 0.981s, episode steps:  99, steps per second: 101, episode reward: -217.493, mean reward: -2.197 [-100.000,  7.726], mean action: 0.636 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.471934
 58777/100000: episode: 688, duration: 0.497s, episode steps:  53, steps per second: 107, episode reward: -153.022, mean reward: -2.887 [-100.000,  7.027], mean action: 0.528 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.471250
 58864/100000: episode: 689, duration: 1.029s, episode steps:  87, steps per second:  85, episode reward: -146.724, mean reward: -1.686 [-100.000,  8.063], mean action: 0.701 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.470620
 58936/100000: episode: 690, duration: 0.778s, episode steps:  72, steps per second:  93, episode reward: -119.199, mean reward: -1.656 [-100.000, 13.141], mean action: 0.792 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.469904
 59001/100000: episode: 691, duration: 0.598s, episode steps:  65, steps per second: 109, episode reward: -109.485, mean reward: -1.684 [-100.000,  7.480], mean action: 0.585 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.469288
 59110/100000: episode: 692, duration: 1.271s, episode steps: 109, steps per second:  86, episode reward: -160.467, mean reward: -1.472 [-100.000,  6.837], mean action: 0.725 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.468505
 59174/100000: episode: 693, duration: 0.699s, episode steps:  64, steps per second:  91, episode reward: -130.732, mean reward: -2.043 [-100.000,  8.492], mean action: 0.703 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.467726
 59242/100000: episode: 694, duration: 0.654s, episode steps:  68, steps per second: 104, episode reward: -121.784, mean reward: -1.791 [-100.000,  6.768], mean action: 0.676 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.467133
 59312/100000: episode: 695, duration: 0.717s, episode steps:  70, steps per second:  98, episode reward: -119.481, mean reward: -1.707 [-100.000,  6.235], mean action: 0.829 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.466511
 59401/100000: episode: 696, duration: 0.931s, episode steps:  89, steps per second:  96, episode reward: -206.233, mean reward: -2.317 [-100.000, 87.649], mean action: 0.775 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.465796
 59463/100000: episode: 697, duration: 0.756s, episode steps:  62, steps per second:  82, episode reward: -140.385, mean reward: -2.264 [-100.000,  2.685], mean action: 0.629 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.465117
 59544/100000: episode: 698, duration: 1.016s, episode steps:  81, steps per second:  80, episode reward: -119.975, mean reward: -1.481 [-100.000,  8.940], mean action: 0.728 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.464473
 59628/100000: episode: 699, duration: 1.121s, episode steps:  84, steps per second:  75, episode reward: -162.156, mean reward: -1.930 [-100.000,  8.730], mean action: 0.869 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.463730
 59733/100000: episode: 700, duration: 1.571s, episode steps: 105, steps per second:  67, episode reward: -36.970, mean reward: -0.352 [-100.000, 110.774], mean action: 0.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.462880
 59829/100000: episode: 701, duration: 1.359s, episode steps:  96, steps per second:  71, episode reward: -84.206, mean reward: -0.877 [-100.000, 104.512], mean action: 0.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.461976
 59882/100000: episode: 702, duration: 0.632s, episode steps:  53, steps per second:  84, episode reward: -144.260, mean reward: -2.722 [-100.000,  8.641], mean action: 0.887 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.461305
 59953/100000: episode: 703, duration: 0.918s, episode steps:  71, steps per second:  77, episode reward: -132.878, mean reward: -1.872 [-100.000, 21.255], mean action: 0.662 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.460747
 60075/100000: episode: 704, duration: 1.400s, episode steps: 122, steps per second:  87, episode reward: -305.028, mean reward: -2.500 [-100.000, 102.100], mean action: 0.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.459878
 60139/100000: episode: 705, duration: 0.712s, episode steps:  64, steps per second:  90, episode reward: -114.054, mean reward: -1.782 [-100.000,  6.547], mean action: 1.047 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.459041
 60194/100000: episode: 706, duration: 0.796s, episode steps:  55, steps per second:  69, episode reward: -96.451, mean reward: -1.754 [-100.000,  6.762], mean action: 0.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.458506
 60304/100000: episode: 707, duration: 1.544s, episode steps: 110, steps per second:  71, episode reward: -372.309, mean reward: -3.385 [-100.000, 74.700], mean action: 0.664 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.457763
 60364/100000: episode: 708, duration: 0.793s, episode steps:  60, steps per second:  76, episode reward: -146.683, mean reward: -2.445 [-100.000,  7.985], mean action: 0.650 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.456998
 60441/100000: episode: 709, duration: 0.994s, episode steps:  77, steps per second:  77, episode reward: -187.369, mean reward: -2.433 [-100.000,  6.854], mean action: 0.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.456382
 60521/100000: episode: 710, duration: 0.975s, episode steps:  80, steps per second:  82, episode reward: -89.999, mean reward: -1.125 [-100.000, 14.611], mean action: 0.775 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.455675
 60625/100000: episode: 711, duration: 1.155s, episode steps: 104, steps per second:  90, episode reward: -131.378, mean reward: -1.263 [-100.000, 16.330], mean action: 0.740 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.454847
 60683/100000: episode: 712, duration: 0.672s, episode steps:  58, steps per second:  86, episode reward: -124.297, mean reward: -2.143 [-100.000,  7.148], mean action: 0.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.454118
 60766/100000: episode: 713, duration: 1.033s, episode steps:  83, steps per second:  80, episode reward: -153.647, mean reward: -1.851 [-100.000,  6.035], mean action: 0.747 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.453484
 60827/100000: episode: 714, duration: 0.770s, episode steps:  61, steps per second:  79, episode reward: -121.250, mean reward: -1.988 [-100.000, 11.457], mean action: 0.787 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.452836
 60918/100000: episode: 715, duration: 0.974s, episode steps:  91, steps per second:  93, episode reward: -134.442, mean reward: -1.477 [-100.000, 10.944], mean action: 0.912 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.452152
 60989/100000: episode: 716, duration: 0.600s, episode steps:  71, steps per second: 118, episode reward: -165.644, mean reward: -2.333 [-100.000,  8.009], mean action: 0.662 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.451423
 61084/100000: episode: 717, duration: 0.845s, episode steps:  95, steps per second: 112, episode reward: -317.202, mean reward: -3.339 [-100.000, 116.293], mean action: 0.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.450676
 61160/100000: episode: 718, duration: 0.654s, episode steps:  76, steps per second: 116, episode reward: -155.250, mean reward: -2.043 [-100.000,  7.075], mean action: 0.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.449907
 61251/100000: episode: 719, duration: 0.867s, episode steps:  91, steps per second: 105, episode reward: -114.778, mean reward: -1.261 [-100.000,  8.264], mean action: 0.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.449155
 61325/100000: episode: 720, duration: 0.859s, episode steps:  74, steps per second:  86, episode reward: -167.602, mean reward: -2.265 [-100.000,  6.747], mean action: 0.757 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.448413
 61399/100000: episode: 721, duration: 1.004s, episode steps:  74, steps per second:  74, episode reward: -16.995, mean reward: -0.230 [-100.000, 100.406], mean action: 0.811 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.447746
 61489/100000: episode: 722, duration: 1.013s, episode steps:  90, steps per second:  89, episode reward: -209.604, mean reward: -2.329 [-100.000,  4.550], mean action: 0.767 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.447008
 61571/100000: episode: 723, duration: 0.722s, episode steps:  82, steps per second: 114, episode reward: -141.637, mean reward: -1.727 [-100.000,  8.276], mean action: 0.634 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.446234
 61674/100000: episode: 724, duration: 0.962s, episode steps: 103, steps per second: 107, episode reward: -115.209, mean reward: -1.119 [-100.000, 12.432], mean action: 0.563 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.445402
 61729/100000: episode: 725, duration: 0.487s, episode steps:  55, steps per second: 113, episode reward: -142.706, mean reward: -2.595 [-100.000,  5.658], mean action: 0.364 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.444691
 61815/100000: episode: 726, duration: 0.924s, episode steps:  86, steps per second:  93, episode reward: -9.781, mean reward: -0.114 [-100.000, 111.827], mean action: 0.814 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.444057
 61885/100000: episode: 727, duration: 0.773s, episode steps:  70, steps per second:  91, episode reward: -125.175, mean reward: -1.788 [-100.000,  7.920], mean action: 0.743 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.443354
 61967/100000: episode: 728, duration: 0.987s, episode steps:  82, steps per second:  83, episode reward: -19.215, mean reward: -0.234 [-100.000, 84.797], mean action: 0.585 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.442671
 62052/100000: episode: 729, duration: 0.981s, episode steps:  85, steps per second:  87, episode reward: -94.069, mean reward: -1.107 [-100.000, 15.237], mean action: 0.588 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.441919
 62163/100000: episode: 730, duration: 1.181s, episode steps: 111, steps per second:  94, episode reward: -147.773, mean reward: -1.331 [-100.000,  8.806], mean action: 0.622 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.441037
 62220/100000: episode: 731, duration: 0.564s, episode steps:  57, steps per second: 101, episode reward: -72.329, mean reward: -1.269 [-100.000, 15.091], mean action: 0.474 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.440281
 62280/100000: episode: 732, duration: 0.624s, episode steps:  60, steps per second:  96, episode reward: -139.094, mean reward: -2.318 [-100.000, 19.655], mean action: 0.550 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.439754
 62361/100000: episode: 733, duration: 0.846s, episode steps:  81, steps per second:  96, episode reward: -188.226, mean reward: -2.324 [-100.000,  5.455], mean action: 0.778 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.439120
 62437/100000: episode: 734, duration: 0.778s, episode steps:  76, steps per second:  98, episode reward: -131.335, mean reward: -1.728 [-100.000, 12.698], mean action: 0.658 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.438414
 62518/100000: episode: 735, duration: 0.865s, episode steps:  81, steps per second:  94, episode reward: -126.325, mean reward: -1.560 [-100.000,  6.450], mean action: 0.852 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.437707
 62620/100000: episode: 736, duration: 1.194s, episode steps: 102, steps per second:  85, episode reward: -109.878, mean reward: -1.077 [-100.000, 12.485], mean action: 0.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.436883
 62675/100000: episode: 737, duration: 0.605s, episode steps:  55, steps per second:  91, episode reward: -90.209, mean reward: -1.640 [-100.000, 64.429], mean action: 0.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.436177
 62768/100000: episode: 738, duration: 0.951s, episode steps:  93, steps per second:  98, episode reward: -172.786, mean reward: -1.858 [-100.000,  8.453], mean action: 0.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.435511
 62823/100000: episode: 739, duration: 0.546s, episode steps:  55, steps per second: 101, episode reward: -143.509, mean reward: -2.609 [-100.000,  6.057], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.434845
 62879/100000: episode: 740, duration: 0.580s, episode steps:  56, steps per second:  97, episode reward: -102.735, mean reward: -1.835 [-100.000,  6.659], mean action: 0.643 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.434345
 62929/100000: episode: 741, duration: 0.512s, episode steps:  50, steps per second:  98, episode reward: -145.134, mean reward: -2.903 [-100.000,  7.319], mean action: 0.560 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.433869
 62989/100000: episode: 742, duration: 0.620s, episode steps:  60, steps per second:  97, episode reward: -121.446, mean reward: -2.024 [-100.000, 31.357], mean action: 0.783 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.433374
 63063/100000: episode: 743, duration: 0.723s, episode steps:  74, steps per second: 102, episode reward: -292.462, mean reward: -3.952 [-100.000, 117.763], mean action: 0.378 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.432771
 63167/100000: episode: 744, duration: 1.092s, episode steps: 104, steps per second:  95, episode reward: -374.296, mean reward: -3.599 [-100.000, 103.796], mean action: 0.558 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.431970
 63233/100000: episode: 745, duration: 0.696s, episode steps:  66, steps per second:  95, episode reward: 15.690, mean reward:  0.238 [-100.000, 96.470], mean action: 0.606 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.431204
 63329/100000: episode: 746, duration: 0.927s, episode steps:  96, steps per second: 104, episode reward: -393.257, mean reward: -4.096 [-100.000, 84.305], mean action: 0.719 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.430476
 63421/100000: episode: 747, duration: 0.920s, episode steps:  92, steps per second: 100, episode reward: -56.178, mean reward: -0.611 [-100.000, 67.238], mean action: 0.696 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.429629
 63490/100000: episode: 748, duration: 0.694s, episode steps:  69, steps per second:  99, episode reward: -89.414, mean reward: -1.296 [-100.000, 16.457], mean action: 0.536 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.428905
 63559/100000: episode: 749, duration: 0.689s, episode steps:  69, steps per second: 100, episode reward: -167.368, mean reward: -2.426 [-100.000,  5.967], mean action: 0.826 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.428284
 63634/100000: episode: 750, duration: 0.736s, episode steps:  75, steps per second: 102, episode reward: -128.303, mean reward: -1.711 [-100.000,  6.823], mean action: 0.773 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.427636
 63699/100000: episode: 751, duration: 0.637s, episode steps:  65, steps per second: 102, episode reward: -173.130, mean reward: -2.664 [-100.000, 35.744], mean action: 0.892 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.427006
 63796/100000: episode: 752, duration: 0.999s, episode steps:  97, steps per second:  97, episode reward: -90.519, mean reward: -0.933 [-100.000, 16.651], mean action: 0.577 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.426277
 63886/100000: episode: 753, duration: 0.934s, episode steps:  90, steps per second:  96, episode reward: -110.665, mean reward: -1.230 [-100.000,  7.707], mean action: 0.633 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.425435
 63954/100000: episode: 754, duration: 0.692s, episode steps:  68, steps per second:  98, episode reward: -152.417, mean reward: -2.241 [-100.000,  7.240], mean action: 0.721 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.424725
 64108/100000: episode: 755, duration: 1.628s, episode steps: 154, steps per second:  95, episode reward: -187.252, mean reward: -1.216 [-100.000, 117.475], mean action: 0.649 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.423725
 64219/100000: episode: 756, duration: 1.436s, episode steps: 111, steps per second:  77, episode reward: -6.693, mean reward: -0.060 [-100.000, 129.233], mean action: 0.712 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.422533
 64304/100000: episode: 757, duration: 0.962s, episode steps:  85, steps per second:  88, episode reward: -115.892, mean reward: -1.363 [-100.000,  7.094], mean action: 0.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.421651
 64364/100000: episode: 758, duration: 0.673s, episode steps:  60, steps per second:  89, episode reward: -121.322, mean reward: -2.022 [-100.000,  6.372], mean action: 0.517 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.420998
 64432/100000: episode: 759, duration: 0.716s, episode steps:  68, steps per second:  95, episode reward: -135.803, mean reward: -1.997 [-100.000,  5.791], mean action: 0.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.420422
 64489/100000: episode: 760, duration: 0.577s, episode steps:  57, steps per second:  99, episode reward: -129.526, mean reward: -2.272 [-100.000,  6.096], mean action: 0.614 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.419860
 64549/100000: episode: 761, duration: 0.628s, episode steps:  60, steps per second:  96, episode reward: -124.189, mean reward: -2.070 [-100.000,  7.213], mean action: 0.700 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.419333
 64634/100000: episode: 762, duration: 0.894s, episode steps:  85, steps per second:  95, episode reward: -143.574, mean reward: -1.689 [-100.000,  6.940], mean action: 0.776 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.418681
 64709/100000: episode: 763, duration: 0.795s, episode steps:  75, steps per second:  94, episode reward: -206.913, mean reward: -2.759 [-100.000,  4.780], mean action: 0.640 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.417961
 64800/100000: episode: 764, duration: 1.002s, episode steps:  91, steps per second:  91, episode reward: -183.325, mean reward: -2.015 [-100.000,  7.110], mean action: 0.484 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.417214
 64900/100000: episode: 765, duration: 1.078s, episode steps: 100, steps per second:  93, episode reward: -288.042, mean reward: -2.880 [-100.000, 125.324], mean action: 0.530 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.416354
 64998/100000: episode: 766, duration: 1.283s, episode steps:  98, steps per second:  76, episode reward: -227.989, mean reward: -2.326 [-100.000, 57.170], mean action: 0.888 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.415463
 65077/100000: episode: 767, duration: 0.888s, episode steps:  79, steps per second:  89, episode reward: -156.886, mean reward: -1.986 [-100.000, 13.595], mean action: 0.570 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.414667
 65172/100000: episode: 768, duration: 0.983s, episode steps:  95, steps per second:  97, episode reward: -134.523, mean reward: -1.416 [-100.000,  9.545], mean action: 0.632 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.413884
 65236/100000: episode: 769, duration: 0.662s, episode steps:  64, steps per second:  97, episode reward: -114.393, mean reward: -1.787 [-100.000,  8.595], mean action: 0.641 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.413168
 65335/100000: episode: 770, duration: 0.953s, episode steps:  99, steps per second: 104, episode reward: -191.833, mean reward: -1.938 [-100.000,  7.807], mean action: 0.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.412435
 65433/100000: episode: 771, duration: 1.104s, episode steps:  98, steps per second:  89, episode reward: -3.010, mean reward: -0.031 [-100.000, 118.516], mean action: 0.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.411548
 65519/100000: episode: 772, duration: 0.870s, episode steps:  86, steps per second:  99, episode reward: -144.143, mean reward: -1.676 [-100.000,  7.722], mean action: 0.802 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.410720
 65617/100000: episode: 773, duration: 1.120s, episode steps:  98, steps per second:  88, episode reward: -273.468, mean reward: -2.790 [-100.000, 14.077], mean action: 0.561 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.409892
 65705/100000: episode: 774, duration: 1.039s, episode steps:  88, steps per second:  85, episode reward: -170.985, mean reward: -1.943 [-100.000, 20.653], mean action: 0.670 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.409056
 65784/100000: episode: 775, duration: 0.845s, episode steps:  79, steps per second:  93, episode reward: -164.928, mean reward: -2.088 [-100.000, 21.119], mean action: 0.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.408304
 65860/100000: episode: 776, duration: 0.831s, episode steps:  76, steps per second:  92, episode reward: -137.166, mean reward: -1.805 [-100.000, 20.343], mean action: 0.658 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.407606
 65962/100000: episode: 777, duration: 1.120s, episode steps: 102, steps per second:  91, episode reward: -137.135, mean reward: -1.344 [-100.000, 18.430], mean action: 0.539 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.406805
 66024/100000: episode: 778, duration: 0.691s, episode steps:  62, steps per second:  90, episode reward: -108.314, mean reward: -1.747 [-100.000,  9.572], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.406068
 66101/100000: episode: 779, duration: 0.786s, episode steps:  77, steps per second:  98, episode reward: -145.107, mean reward: -1.885 [-100.000,  9.662], mean action: 0.831 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.405442
 66200/100000: episode: 780, duration: 0.905s, episode steps:  99, steps per second: 109, episode reward: -131.963, mean reward: -1.333 [-100.000,  8.293], mean action: 0.889 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.404650
 66269/100000: episode: 781, duration: 0.633s, episode steps:  69, steps per second: 109, episode reward: -168.014, mean reward: -2.435 [-100.000,  5.937], mean action: 0.420 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.403894
 66338/100000: episode: 782, duration: 0.570s, episode steps:  69, steps per second: 121, episode reward: -168.853, mean reward: -2.447 [-100.000, 21.355], mean action: 0.478 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.403273
 66430/100000: episode: 783, duration: 0.860s, episode steps:  92, steps per second: 107, episode reward: -256.791, mean reward: -2.791 [-100.000,  5.822], mean action: 0.467 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.402548
 66494/100000: episode: 784, duration: 0.550s, episode steps:  64, steps per second: 116, episode reward: -143.332, mean reward: -2.240 [-100.000, 11.325], mean action: 0.531 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.401846
 66581/100000: episode: 785, duration: 0.857s, episode steps:  87, steps per second: 102, episode reward: -142.513, mean reward: -1.638 [-100.000,  9.663], mean action: 0.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.401167
 66639/100000: episode: 786, duration: 0.598s, episode steps:  58, steps per second:  97, episode reward: -111.894, mean reward: -1.929 [-100.000,  6.885], mean action: 0.776 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.400514
 66701/100000: episode: 787, duration: 0.579s, episode steps:  62, steps per second: 107, episode reward: -137.568, mean reward: -2.219 [-100.000,  6.421], mean action: 0.548 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.399975
 66762/100000: episode: 788, duration: 0.546s, episode steps:  61, steps per second: 112, episode reward: -90.005, mean reward: -1.475 [-100.000, 16.672], mean action: 0.656 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.399421
 66851/100000: episode: 789, duration: 0.768s, episode steps:  89, steps per second: 116, episode reward: -119.508, mean reward: -1.343 [-100.000, 17.153], mean action: 0.348 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.398746
 66927/100000: episode: 790, duration: 0.778s, episode steps:  76, steps per second:  98, episode reward: -135.606, mean reward: -1.784 [-100.000,  9.366], mean action: 0.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.398004
 66991/100000: episode: 791, duration: 0.653s, episode steps:  64, steps per second:  98, episode reward: -145.140, mean reward: -2.268 [-100.000, 104.044], mean action: 0.672 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.397373
 67093/100000: episode: 792, duration: 1.220s, episode steps: 102, steps per second:  84, episode reward: -130.850, mean reward: -1.283 [-100.000,  6.584], mean action: 0.706 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.396626
 67180/100000: episode: 793, duration: 0.920s, episode steps:  87, steps per second:  95, episode reward: -329.319, mean reward: -3.785 [-100.000, -0.546], mean action: 0.621 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.395776
 67262/100000: episode: 794, duration: 0.931s, episode steps:  82, steps per second:  88, episode reward: -110.326, mean reward: -1.345 [-100.000, 16.214], mean action: 0.402 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.395016
 67368/100000: episode: 795, duration: 1.103s, episode steps: 106, steps per second:  96, episode reward: -87.261, mean reward: -0.823 [-100.000, 13.086], mean action: 0.679 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.394170
 67452/100000: episode: 796, duration: 0.997s, episode steps:  84, steps per second:  84, episode reward: -107.513, mean reward: -1.280 [-100.000, 16.090], mean action: 0.738 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.393314
 67550/100000: episode: 797, duration: 1.000s, episode steps:  98, steps per second:  98, episode reward: -103.606, mean reward: -1.057 [-100.000,  7.085], mean action: 0.480 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.392495
 67643/100000: episode: 798, duration: 0.816s, episode steps:  93, steps per second: 114, episode reward: -122.655, mean reward: -1.319 [-100.000, 19.883], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.391636
 67727/100000: episode: 799, duration: 0.841s, episode steps:  84, steps per second: 100, episode reward: -112.121, mean reward: -1.335 [-100.000,  7.308], mean action: 0.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.390839
 67787/100000: episode: 800, duration: 0.578s, episode steps:  60, steps per second: 104, episode reward: -131.966, mean reward: -2.199 [-100.000, 11.781], mean action: 0.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.390191
 67881/100000: episode: 801, duration: 0.963s, episode steps:  94, steps per second:  98, episode reward: -77.347, mean reward: -0.823 [-100.000, 16.962], mean action: 0.777 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.389499
 67978/100000: episode: 802, duration: 0.955s, episode steps:  97, steps per second: 102, episode reward: -171.682, mean reward: -1.770 [-100.000,  7.497], mean action: 0.598 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.388639
 68057/100000: episode: 803, duration: 0.737s, episode steps:  79, steps per second: 107, episode reward: -58.375, mean reward: -0.739 [-100.000, 69.593], mean action: 0.608 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.387847
 68153/100000: episode: 804, duration: 1.179s, episode steps:  96, steps per second:  81, episode reward: -327.522, mean reward: -3.412 [-100.000,  3.117], mean action: 0.802 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.387059
 68244/100000: episode: 805, duration: 1.019s, episode steps:  91, steps per second:  89, episode reward: -117.438, mean reward: -1.291 [-100.000, 16.891], mean action: 0.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.386218
 68321/100000: episode: 806, duration: 0.915s, episode steps:  77, steps per second:  84, episode reward: -175.530, mean reward: -2.280 [-100.000,  5.702], mean action: 0.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.385462
 68424/100000: episode: 807, duration: 1.105s, episode steps: 103, steps per second:  93, episode reward: -30.631, mean reward: -0.297 [-100.000, 103.737], mean action: 0.592 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.384652
 68516/100000: episode: 808, duration: 0.985s, episode steps:  92, steps per second:  93, episode reward: -143.207, mean reward: -1.557 [-100.000,  8.204], mean action: 0.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.383775
 68598/100000: episode: 809, duration: 1.036s, episode steps:  82, steps per second:  79, episode reward: -192.472, mean reward: -2.347 [-100.000,  6.693], mean action: 0.646 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.382991
 68708/100000: episode: 810, duration: 1.081s, episode steps: 110, steps per second: 102, episode reward: -267.057, mean reward: -2.428 [-100.000, 127.280], mean action: 0.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.382128
 68782/100000: episode: 811, duration: 0.945s, episode steps:  74, steps per second:  78, episode reward: -166.245, mean reward: -2.247 [-100.000, 34.074], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.381299
 68844/100000: episode: 812, duration: 0.610s, episode steps:  62, steps per second: 102, episode reward: -175.919, mean reward: -2.837 [-100.000,  5.873], mean action: 0.597 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.380688
 68907/100000: episode: 813, duration: 0.563s, episode steps:  63, steps per second: 112, episode reward: -172.366, mean reward: -2.736 [-100.000,  5.884], mean action: 0.492 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.380125
 68987/100000: episode: 814, duration: 0.900s, episode steps:  80, steps per second:  89, episode reward: -105.085, mean reward: -1.314 [-100.000, 10.953], mean action: 0.675 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.379481
 69062/100000: episode: 815, duration: 0.773s, episode steps:  75, steps per second:  97, episode reward: -97.425, mean reward: -1.299 [-100.000, 17.029], mean action: 0.840 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.378784
 69121/100000: episode: 816, duration: 0.663s, episode steps:  59, steps per second:  89, episode reward: -90.982, mean reward: -1.542 [-100.000,  6.562], mean action: 0.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.378181
 69229/100000: episode: 817, duration: 1.246s, episode steps: 108, steps per second:  87, episode reward: -103.398, mean reward: -0.957 [-100.000, 93.907], mean action: 0.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.377429
 69303/100000: episode: 818, duration: 0.825s, episode steps:  74, steps per second:  90, episode reward: -134.870, mean reward: -1.823 [-100.000, 16.238], mean action: 0.595 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.376610
 69400/100000: episode: 819, duration: 0.938s, episode steps:  97, steps per second: 103, episode reward: -172.388, mean reward: -1.777 [-100.000,  6.071], mean action: 0.392 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.375841
 69475/100000: episode: 820, duration: 0.786s, episode steps:  75, steps per second:  95, episode reward: -97.345, mean reward: -1.298 [-100.000, 14.625], mean action: 0.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.375067
 69566/100000: episode: 821, duration: 0.885s, episode steps:  91, steps per second: 103, episode reward: -148.305, mean reward: -1.630 [-100.000,  8.419], mean action: 0.681 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.374320
 69635/100000: episode: 822, duration: 0.657s, episode steps:  69, steps per second: 105, episode reward: -218.965, mean reward: -3.173 [-100.000, 32.388], mean action: 0.609 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.373600
 69695/100000: episode: 823, duration: 0.606s, episode steps:  60, steps per second:  99, episode reward: -116.608, mean reward: -1.943 [-100.000,  9.672], mean action: 0.483 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.373019
 69764/100000: episode: 824, duration: 0.721s, episode steps:  69, steps per second:  96, episode reward: -88.852, mean reward: -1.288 [-100.000,  8.229], mean action: 0.551 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.372439
 69858/100000: episode: 825, duration: 0.903s, episode steps:  94, steps per second: 104, episode reward: -118.597, mean reward: -1.262 [-100.000,  8.952], mean action: 0.649 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.371706
 69951/100000: episode: 826, duration: 0.833s, episode steps:  93, steps per second: 112, episode reward: -37.318, mean reward: -0.401 [-100.000, 108.670], mean action: 0.871 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.370864
 70033/100000: episode: 827, duration: 0.782s, episode steps:  82, steps per second: 105, episode reward: -112.143, mean reward: -1.368 [-100.000, 17.329], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.370076
 70116/100000: episode: 828, duration: 0.731s, episode steps:  83, steps per second: 114, episode reward: -142.520, mean reward: -1.717 [-100.000, 15.970], mean action: 0.663 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.369334
 70200/100000: episode: 829, duration: 0.833s, episode steps:  84, steps per second: 101, episode reward: -108.715, mean reward: -1.294 [-100.000,  7.010], mean action: 0.702 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.368583
 70285/100000: episode: 830, duration: 0.846s, episode steps:  85, steps per second: 100, episode reward: -249.881, mean reward: -2.940 [-100.000,  6.910], mean action: 0.741 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.367822
 70343/100000: episode: 831, duration: 0.565s, episode steps:  58, steps per second: 103, episode reward: -78.649, mean reward: -1.356 [-100.000, 12.057], mean action: 0.586 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.367179
 70409/100000: episode: 832, duration: 0.614s, episode steps:  66, steps per second: 107, episode reward: -94.166, mean reward: -1.427 [-100.000, 84.842], mean action: 0.424 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.366621
 70485/100000: episode: 833, duration: 0.691s, episode steps:  76, steps per second: 110, episode reward: -145.076, mean reward: -1.909 [-100.000,  6.266], mean action: 0.447 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.365981
 70564/100000: episode: 834, duration: 0.685s, episode steps:  79, steps per second: 115, episode reward: -119.591, mean reward: -1.514 [-100.000,  8.751], mean action: 0.506 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.365284
 70631/100000: episode: 835, duration: 0.580s, episode steps:  67, steps per second: 116, episode reward: -124.707, mean reward: -1.861 [-100.000,  7.540], mean action: 0.672 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.364627
 70706/100000: episode: 836, duration: 0.838s, episode steps:  75, steps per second:  89, episode reward: -158.812, mean reward: -2.117 [-100.000,  6.208], mean action: 0.533 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.363988
 70760/100000: episode: 837, duration: 0.513s, episode steps:  54, steps per second: 105, episode reward: -118.339, mean reward: -2.191 [-100.000,  8.273], mean action: 0.481 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.363407
 70848/100000: episode: 838, duration: 0.844s, episode steps:  88, steps per second: 104, episode reward: -162.885, mean reward: -1.851 [-100.000,  9.337], mean action: 0.545 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.362768
 70918/100000: episode: 839, duration: 0.749s, episode steps:  70, steps per second:  93, episode reward: -116.153, mean reward: -1.659 [-100.000,  8.293], mean action: 0.657 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.362057
 70977/100000: episode: 840, duration: 0.585s, episode steps:  59, steps per second: 101, episode reward: -144.486, mean reward: -2.449 [-100.000,  8.302], mean action: 0.441 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.361477
 71046/100000: episode: 841, duration: 0.721s, episode steps:  69, steps per second:  96, episode reward: -189.825, mean reward: -2.751 [-100.000,  6.184], mean action: 0.826 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.360901
 71108/100000: episode: 842, duration: 0.678s, episode steps:  62, steps per second:  91, episode reward: -128.245, mean reward: -2.068 [-100.000,  7.583], mean action: 0.419 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.360311
 71173/100000: episode: 843, duration: 0.747s, episode steps:  65, steps per second:  87, episode reward: -164.658, mean reward: -2.533 [-100.000,  8.073], mean action: 0.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.359740
 71268/100000: episode: 844, duration: 0.937s, episode steps:  95, steps per second: 101, episode reward: -233.173, mean reward: -2.454 [-100.000,  4.379], mean action: 0.484 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.359020
 71326/100000: episode: 845, duration: 0.651s, episode steps:  58, steps per second:  89, episode reward: -123.324, mean reward: -2.126 [-100.000,  9.065], mean action: 0.466 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.358332
 71381/100000: episode: 846, duration: 0.671s, episode steps:  55, steps per second:  82, episode reward: -123.742, mean reward: -2.250 [-100.000, 10.811], mean action: 0.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.357823
 71477/100000: episode: 847, duration: 1.140s, episode steps:  96, steps per second:  84, episode reward: -164.315, mean reward: -1.712 [-100.000,  7.486], mean action: 0.677 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.357144
 71536/100000: episode: 848, duration: 0.711s, episode steps:  59, steps per second:  83, episode reward: -107.735, mean reward: -1.826 [-100.000, 14.891], mean action: 0.610 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.356446
 71615/100000: episode: 849, duration: 0.912s, episode steps:  79, steps per second:  87, episode reward: -126.661, mean reward: -1.603 [-100.000, 10.248], mean action: 0.734 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.355825
 71673/100000: episode: 850, duration: 0.630s, episode steps:  58, steps per second:  92, episode reward: -91.682, mean reward: -1.581 [-100.000,  8.990], mean action: 0.345 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.355208
 71748/100000: episode: 851, duration: 0.728s, episode steps:  75, steps per second: 103, episode reward: -164.989, mean reward: -2.200 [-100.000, 13.653], mean action: 0.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.354610
 71829/100000: episode: 852, duration: 0.710s, episode steps:  81, steps per second: 114, episode reward: -195.724, mean reward: -2.416 [-100.000, 88.093], mean action: 0.642 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.353908
 71891/100000: episode: 853, duration: 0.570s, episode steps:  62, steps per second: 109, episode reward: -140.890, mean reward: -2.272 [-100.000, 20.063], mean action: 0.484 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.353264
 71988/100000: episode: 854, duration: 0.942s, episode steps:  97, steps per second: 103, episode reward: -182.113, mean reward: -1.877 [-100.000,  9.856], mean action: 0.443 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.352549
 72047/100000: episode: 855, duration: 0.536s, episode steps:  59, steps per second: 110, episode reward: -104.636, mean reward: -1.773 [-100.000, 26.407], mean action: 0.339 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.351847
 72123/100000: episode: 856, duration: 0.752s, episode steps:  76, steps per second: 101, episode reward: -153.826, mean reward: -2.024 [-100.000,  9.848], mean action: 0.579 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.351239
 72208/100000: episode: 857, duration: 0.855s, episode steps:  85, steps per second:  99, episode reward: -110.157, mean reward: -1.296 [-100.000, 14.835], mean action: 0.541 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.350515
 72304/100000: episode: 858, duration: 0.896s, episode steps:  96, steps per second: 107, episode reward: -120.143, mean reward: -1.251 [-100.000, 10.050], mean action: 0.406 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.349700
 72363/100000: episode: 859, duration: 0.628s, episode steps:  59, steps per second:  94, episode reward: -114.895, mean reward: -1.947 [-100.000,  6.387], mean action: 0.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.349003
 72437/100000: episode: 860, duration: 0.812s, episode steps:  74, steps per second:  91, episode reward: -154.185, mean reward: -2.084 [-100.000,  7.727], mean action: 0.216 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.348405
 72493/100000: episode: 861, duration: 0.831s, episode steps:  56, steps per second:  67, episode reward: -139.548, mean reward: -2.492 [-100.000,  5.157], mean action: 0.518 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.347820
 72575/100000: episode: 862, duration: 0.944s, episode steps:  82, steps per second:  87, episode reward: -141.841, mean reward: -1.730 [-100.000,  9.143], mean action: 0.476 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.347198
 72668/100000: episode: 863, duration: 1.134s, episode steps:  93, steps per second:  82, episode reward: -143.299, mean reward: -1.541 [-100.000,  8.026], mean action: 0.387 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.346411
 72758/100000: episode: 864, duration: 0.974s, episode steps:  90, steps per second:  92, episode reward: -103.726, mean reward: -1.153 [-100.000, 17.184], mean action: 0.389 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.345587
 72835/100000: episode: 865, duration: 0.705s, episode steps:  77, steps per second: 109, episode reward: -144.801, mean reward: -1.881 [-100.000,  9.258], mean action: 0.558 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.344836
 72901/100000: episode: 866, duration: 0.708s, episode steps:  66, steps per second:  93, episode reward: -102.176, mean reward: -1.548 [-100.000, 14.896], mean action: 0.591 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.344192
 72984/100000: episode: 867, duration: 1.226s, episode steps:  83, steps per second:  68, episode reward: -140.136, mean reward: -1.688 [-100.000, 18.653], mean action: 0.602 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.343522
 73075/100000: episode: 868, duration: 1.451s, episode steps:  91, steps per second:  63, episode reward: -184.907, mean reward: -2.032 [-100.000, 13.506], mean action: 0.560 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.342739
 73195/100000: episode: 869, duration: 1.210s, episode steps: 120, steps per second:  99, episode reward: -531.971, mean reward: -4.433 [-100.000, 66.662], mean action: 0.758 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.341789
 73263/100000: episode: 870, duration: 0.687s, episode steps:  68, steps per second:  99, episode reward: -303.135, mean reward: -4.458 [-100.000, 115.409], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.340944
 73368/100000: episode: 871, duration: 1.037s, episode steps: 105, steps per second: 101, episode reward: -163.766, mean reward: -1.560 [-100.000,  6.688], mean action: 0.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.340165
 73463/100000: episode: 872, duration: 0.920s, episode steps:  95, steps per second: 103, episode reward: -96.401, mean reward: -1.015 [-100.000, 12.210], mean action: 0.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.339265
 73547/100000: episode: 873, duration: 0.814s, episode steps:  84, steps per second: 103, episode reward: -195.458, mean reward: -2.327 [-100.000, 12.996], mean action: 0.524 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.338460
 73630/100000: episode: 874, duration: 0.718s, episode steps:  83, steps per second: 116, episode reward: -139.389, mean reward: -1.679 [-100.000,  6.091], mean action: 0.627 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.337708
 73692/100000: episode: 875, duration: 0.565s, episode steps:  62, steps per second: 110, episode reward: -80.452, mean reward: -1.298 [-100.000, 11.387], mean action: 0.290 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.337056
 73768/100000: episode: 876, duration: 0.725s, episode steps:  76, steps per second: 105, episode reward: -136.702, mean reward: -1.799 [-100.000,  7.323], mean action: 0.658 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.336434
 73837/100000: episode: 877, duration: 0.683s, episode steps:  69, steps per second: 101, episode reward: -136.092, mean reward: -1.972 [-100.000, 13.146], mean action: 0.449 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.335782
 73939/100000: episode: 878, duration: 1.125s, episode steps: 102, steps per second:  91, episode reward: -104.884, mean reward: -1.028 [-100.000,  7.115], mean action: 0.647 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.335012
 73994/100000: episode: 879, duration: 0.584s, episode steps:  55, steps per second:  94, episode reward: -103.459, mean reward: -1.881 [-100.000,  7.946], mean action: 0.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.334306
 74079/100000: episode: 880, duration: 0.872s, episode steps:  85, steps per second:  97, episode reward: -125.181, mean reward: -1.473 [-100.000, 27.086], mean action: 0.459 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.333676
 74172/100000: episode: 881, duration: 0.950s, episode steps:  93, steps per second:  98, episode reward: -277.762, mean reward: -2.987 [-100.000, 120.464], mean action: 0.398 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.332875
 74247/100000: episode: 882, duration: 0.693s, episode steps:  75, steps per second: 108, episode reward: -204.160, mean reward: -2.722 [-100.000, 15.284], mean action: 0.520 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.332119
 74322/100000: episode: 883, duration: 0.674s, episode steps:  75, steps per second: 111, episode reward: -163.662, mean reward: -2.182 [-100.000, 24.003], mean action: 0.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.331444
 74417/100000: episode: 884, duration: 0.901s, episode steps:  95, steps per second: 105, episode reward: -121.057, mean reward: -1.274 [-100.000, 10.932], mean action: 0.589 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.330679
 74516/100000: episode: 885, duration: 0.965s, episode steps:  99, steps per second: 103, episode reward: -133.347, mean reward: -1.347 [-100.000,  9.134], mean action: 0.343 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.329806
 74612/100000: episode: 886, duration: 0.944s, episode steps:  96, steps per second: 102, episode reward: -215.879, mean reward: -2.249 [-100.000,  6.361], mean action: 0.604 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.328929
 74708/100000: episode: 887, duration: 1.010s, episode steps:  96, steps per second:  95, episode reward: -93.363, mean reward: -0.973 [-100.000, 17.058], mean action: 0.552 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.328064
 74790/100000: episode: 888, duration: 0.951s, episode steps:  82, steps per second:  86, episode reward: -123.498, mean reward: -1.506 [-100.000,  8.152], mean action: 0.549 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.327263
 74850/100000: episode: 889, duration: 0.699s, episode steps:  60, steps per second:  86, episode reward: -99.318, mean reward: -1.655 [-100.000, 11.360], mean action: 0.567 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.326625
 74911/100000: episode: 890, duration: 0.663s, episode steps:  61, steps per second:  92, episode reward: -157.541, mean reward: -2.583 [-100.000,  6.969], mean action: 0.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.326080
 74969/100000: episode: 891, duration: 0.600s, episode steps:  58, steps per second:  97, episode reward: -90.279, mean reward: -1.557 [-100.000,  7.138], mean action: 0.690 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.325544
 75024/100000: episode: 892, duration: 0.617s, episode steps:  55, steps per second:  89, episode reward: -102.495, mean reward: -1.864 [-100.000,  8.340], mean action: 0.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.325036
 75086/100000: episode: 893, duration: 0.749s, episode steps:  62, steps per second:  83, episode reward: -150.204, mean reward: -2.423 [-100.000,  6.442], mean action: 0.806 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.324510
 75166/100000: episode: 894, duration: 0.766s, episode steps:  80, steps per second: 104, episode reward: -213.931, mean reward: -2.674 [-100.000,  5.211], mean action: 0.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.323871
 75227/100000: episode: 895, duration: 0.612s, episode steps:  61, steps per second: 100, episode reward: -152.062, mean reward: -2.493 [-100.000,  7.840], mean action: 0.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.323236
 75302/100000: episode: 896, duration: 0.686s, episode steps:  75, steps per second: 109, episode reward: -125.866, mean reward: -1.678 [-100.000,  7.302], mean action: 0.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.322624
 75366/100000: episode: 897, duration: 0.558s, episode steps:  64, steps per second: 115, episode reward: -123.885, mean reward: -1.936 [-100.000,  8.712], mean action: 0.516 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.321998
 75493/100000: episode: 898, duration: 1.141s, episode steps: 127, steps per second: 111, episode reward: -289.502, mean reward: -2.280 [-100.000, 121.310], mean action: 0.512 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.321139
 75583/100000: episode: 899, duration: 0.844s, episode steps:  90, steps per second: 107, episode reward: -125.515, mean reward: -1.395 [-100.000, 14.077], mean action: 0.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.320163
 75640/100000: episode: 900, duration: 0.532s, episode steps:  57, steps per second: 107, episode reward: -127.693, mean reward: -2.240 [-100.000,  6.306], mean action: 0.474 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.319501
 75712/100000: episode: 901, duration: 0.702s, episode steps:  72, steps per second: 103, episode reward: -145.143, mean reward: -2.016 [-100.000,  7.820], mean action: 0.347 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.318920
 75792/100000: episode: 902, duration: 0.725s, episode steps:  80, steps per second: 110, episode reward: -193.356, mean reward: -2.417 [-100.000,  4.651], mean action: 0.688 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.318236
 75885/100000: episode: 903, duration: 0.869s, episode steps:  93, steps per second: 107, episode reward: -179.690, mean reward: -1.932 [-100.000,  9.111], mean action: 0.280 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.317458
 75961/100000: episode: 904, duration: 0.738s, episode steps:  76, steps per second: 103, episode reward: -147.934, mean reward: -1.947 [-100.000, 17.653], mean action: 0.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.316697
 76017/100000: episode: 905, duration: 0.487s, episode steps:  56, steps per second: 115, episode reward: -150.209, mean reward: -2.682 [-100.000,  7.572], mean action: 0.232 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.316104
 76110/100000: episode: 906, duration: 0.855s, episode steps:  93, steps per second: 109, episode reward: -148.082, mean reward: -1.592 [-100.000,  7.384], mean action: 0.344 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.315433
 76173/100000: episode: 907, duration: 0.622s, episode steps:  63, steps per second: 101, episode reward: -142.796, mean reward: -2.267 [-100.000,  7.649], mean action: 0.603 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.314731
 76247/100000: episode: 908, duration: 0.639s, episode steps:  74, steps per second: 116, episode reward: -60.766, mean reward: -0.821 [-100.000, 54.374], mean action: 0.622 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.314114
 76308/100000: episode: 909, duration: 0.544s, episode steps:  61, steps per second: 112, episode reward: -152.286, mean reward: -2.496 [-100.000,  7.321], mean action: 0.525 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.313507
 76387/100000: episode: 910, duration: 0.711s, episode steps:  79, steps per second: 111, episode reward: -136.364, mean reward: -1.726 [-100.000,  8.045], mean action: 0.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.312877
 76473/100000: episode: 911, duration: 0.878s, episode steps:  86, steps per second:  98, episode reward: -196.739, mean reward: -2.288 [-100.000, 12.416], mean action: 0.314 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.312134
 76543/100000: episode: 912, duration: 0.781s, episode steps:  70, steps per second:  90, episode reward: -116.418, mean reward: -1.663 [-100.000,  7.873], mean action: 0.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.311433
 76612/100000: episode: 913, duration: 0.732s, episode steps:  69, steps per second:  94, episode reward: -146.410, mean reward: -2.122 [-100.000,  8.928], mean action: 0.594 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.310807
 76675/100000: episode: 914, duration: 0.616s, episode steps:  63, steps per second: 102, episode reward: -149.910, mean reward: -2.380 [-100.000, 15.322], mean action: 0.492 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.310213
 76754/100000: episode: 915, duration: 0.744s, episode steps:  79, steps per second: 106, episode reward: -313.538, mean reward: -3.969 [-100.000, 46.219], mean action: 0.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.309574
 76835/100000: episode: 916, duration: 0.759s, episode steps:  81, steps per second: 107, episode reward: -166.233, mean reward: -2.052 [-100.000,  9.231], mean action: 0.617 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.308854
 76904/100000: episode: 917, duration: 0.680s, episode steps:  69, steps per second: 101, episode reward: -125.550, mean reward: -1.820 [-100.000,  9.771], mean action: 0.507 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.308179
 76983/100000: episode: 918, duration: 0.783s, episode steps:  79, steps per second: 101, episode reward: -127.707, mean reward: -1.617 [-100.000,  7.753], mean action: 0.519 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.307513
 77045/100000: episode: 919, duration: 0.672s, episode steps:  62, steps per second:  92, episode reward: -96.084, mean reward: -1.550 [-100.000, 19.624], mean action: 0.581 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.306878
 77148/100000: episode: 920, duration: 1.072s, episode steps: 103, steps per second:  96, episode reward: -220.825, mean reward: -2.144 [-100.000,  5.166], mean action: 0.466 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.306136
 77241/100000: episode: 921, duration: 0.992s, episode steps:  93, steps per second:  94, episode reward: -95.928, mean reward: -1.031 [-100.000, 17.763], mean action: 0.462 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.305254
 77315/100000: episode: 922, duration: 0.730s, episode steps:  74, steps per second: 101, episode reward: -143.472, mean reward: -1.939 [-100.000, 12.875], mean action: 0.405 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.304502
 77373/100000: episode: 923, duration: 0.672s, episode steps:  58, steps per second:  86, episode reward: -105.488, mean reward: -1.819 [-100.000,  6.816], mean action: 0.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.303908
 77433/100000: episode: 924, duration: 0.745s, episode steps:  60, steps per second:  81, episode reward: -78.554, mean reward: -1.309 [-100.000, 15.744], mean action: 0.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.303377
 77512/100000: episode: 925, duration: 0.879s, episode steps:  79, steps per second:  90, episode reward: -140.072, mean reward: -1.773 [-100.000,  8.374], mean action: 0.418 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.302752
 77580/100000: episode: 926, duration: 0.720s, episode steps:  68, steps per second:  94, episode reward: -127.660, mean reward: -1.877 [-100.000,  6.204], mean action: 0.515 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.302090
 77657/100000: episode: 927, duration: 0.959s, episode steps:  77, steps per second:  80, episode reward: -258.613, mean reward: -3.359 [-100.000,  5.530], mean action: 0.429 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.301438
 77724/100000: episode: 928, duration: 0.661s, episode steps:  67, steps per second: 101, episode reward: -111.631, mean reward: -1.666 [-100.000,  6.922], mean action: 0.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.300790
 77781/100000: episode: 929, duration: 0.568s, episode steps:  57, steps per second: 100, episode reward: -107.160, mean reward: -1.880 [-100.000,  7.100], mean action: 0.298 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.300232
 77841/100000: episode: 930, duration: 0.643s, episode steps:  60, steps per second:  93, episode reward: -112.011, mean reward: -1.867 [-100.000,  5.757], mean action: 0.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.299706
 77922/100000: episode: 931, duration: 0.870s, episode steps:  81, steps per second:  93, episode reward: -166.989, mean reward: -2.062 [-100.000,  6.805], mean action: 0.370 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.299071
 78003/100000: episode: 932, duration: 0.802s, episode steps:  81, steps per second: 101, episode reward: -53.135, mean reward: -0.656 [-100.000, 72.391], mean action: 0.457 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.298342
 78080/100000: episode: 933, duration: 0.806s, episode steps:  77, steps per second:  96, episode reward: -10.628, mean reward: -0.138 [-100.000, 89.987], mean action: 0.584 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.297631
 78141/100000: episode: 934, duration: 0.618s, episode steps:  61, steps per second:  99, episode reward: -160.669, mean reward: -2.634 [-100.000,  7.077], mean action: 0.344 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.297010
 78205/100000: episode: 935, duration: 0.669s, episode steps:  64, steps per second:  96, episode reward: -98.900, mean reward: -1.545 [-100.000,  7.170], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.296447
 78267/100000: episode: 936, duration: 0.657s, episode steps:  62, steps per second:  94, episode reward: -113.904, mean reward: -1.837 [-100.000,  7.082], mean action: 0.419 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.295880
 78337/100000: episode: 937, duration: 0.743s, episode steps:  70, steps per second:  94, episode reward: -6.487, mean reward: -0.093 [-100.000, 80.060], mean action: 0.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.295286
 78392/100000: episode: 938, duration: 0.568s, episode steps:  55, steps per second:  97, episode reward: -109.149, mean reward: -1.985 [-100.000,  9.485], mean action: 0.345 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.294724
 78463/100000: episode: 939, duration: 0.973s, episode steps:  71, steps per second:  73, episode reward: -137.141, mean reward: -1.932 [-100.000, 15.560], mean action: 0.493 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.294157
 78517/100000: episode: 940, duration: 0.637s, episode steps:  54, steps per second:  85, episode reward: -113.934, mean reward: -2.110 [-100.000,  6.812], mean action: 0.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.293594
 78569/100000: episode: 941, duration: 0.614s, episode steps:  52, steps per second:  85, episode reward: -97.955, mean reward: -1.884 [-100.000,  8.586], mean action: 0.423 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.293118
 78642/100000: episode: 942, duration: 0.868s, episode steps:  73, steps per second:  84, episode reward: -170.149, mean reward: -2.331 [-100.000, 20.894], mean action: 0.521 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.292555
 78713/100000: episode: 943, duration: 0.755s, episode steps:  71, steps per second:  94, episode reward: -119.460, mean reward: -1.683 [-100.000, 65.181], mean action: 0.408 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.291907
 78793/100000: episode: 944, duration: 0.904s, episode steps:  80, steps per second:  88, episode reward: -109.993, mean reward: -1.375 [-100.000, 49.401], mean action: 0.438 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.291227
 78848/100000: episode: 945, duration: 0.647s, episode steps:  55, steps per second:  85, episode reward: -116.181, mean reward: -2.112 [-100.000,  6.909], mean action: 0.436 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.290620
 78907/100000: episode: 946, duration: 0.538s, episode steps:  59, steps per second: 110, episode reward: -137.267, mean reward: -2.327 [-100.000,  8.683], mean action: 0.593 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.290107
 78967/100000: episode: 947, duration: 0.573s, episode steps:  60, steps per second: 105, episode reward: -125.351, mean reward: -2.089 [-100.000,  7.071], mean action: 0.233 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.289571
 79035/100000: episode: 948, duration: 0.746s, episode steps:  68, steps per second:  91, episode reward: -107.525, mean reward: -1.581 [-100.000, 16.255], mean action: 0.456 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.288995
 79094/100000: episode: 949, duration: 0.574s, episode steps:  59, steps per second: 103, episode reward: -101.607, mean reward: -1.722 [-100.000,  8.484], mean action: 0.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.288424
 79149/100000: episode: 950, duration: 0.571s, episode steps:  55, steps per second:  96, episode reward: -104.160, mean reward: -1.894 [-100.000,  6.509], mean action: 0.509 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.287911
 79227/100000: episode: 951, duration: 0.711s, episode steps:  78, steps per second: 110, episode reward: -165.749, mean reward: -2.125 [-100.000,  6.567], mean action: 0.410 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.287313
 79308/100000: episode: 952, duration: 0.919s, episode steps:  81, steps per second:  88, episode reward: -129.855, mean reward: -1.603 [-100.000, 34.227], mean action: 0.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.286597
 79371/100000: episode: 953, duration: 0.640s, episode steps:  63, steps per second:  98, episode reward: -119.201, mean reward: -1.892 [-100.000,  8.438], mean action: 0.714 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.285949
 79446/100000: episode: 954, duration: 0.700s, episode steps:  75, steps per second: 107, episode reward: -112.759, mean reward: -1.503 [-100.000,  7.687], mean action: 0.600 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.285328
 79510/100000: episode: 955, duration: 0.577s, episode steps:  64, steps per second: 111, episode reward: -122.790, mean reward: -1.919 [-100.000, 15.849], mean action: 0.406 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.284702
 79563/100000: episode: 956, duration: 0.468s, episode steps:  53, steps per second: 113, episode reward: -102.485, mean reward: -1.934 [-100.000,  8.012], mean action: 0.283 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.284176
 79656/100000: episode: 957, duration: 0.815s, episode steps:  93, steps per second: 114, episode reward: -99.654, mean reward: -1.072 [-100.000, 15.481], mean action: 0.419 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.283519
 79711/100000: episode: 958, duration: 0.496s, episode steps:  55, steps per second: 111, episode reward: -124.720, mean reward: -2.268 [-100.000, 15.112], mean action: 0.473 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.282853
 79789/100000: episode: 959, duration: 0.719s, episode steps:  78, steps per second: 108, episode reward: -332.981, mean reward: -4.269 [-100.000, 123.975], mean action: 0.372 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.282255
 79883/100000: episode: 960, duration: 0.860s, episode steps:  94, steps per second: 109, episode reward: -111.183, mean reward: -1.183 [-100.000,  6.688], mean action: 0.383 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.281481
 79942/100000: episode: 961, duration: 0.644s, episode steps:  59, steps per second:  92, episode reward: -101.030, mean reward: -1.712 [-100.000,  8.253], mean action: 0.390 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.280792
 80024/100000: episode: 962, duration: 0.807s, episode steps:  82, steps per second: 102, episode reward: -177.152, mean reward: -2.160 [-100.000,  6.135], mean action: 0.268 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.280158
 80081/100000: episode: 963, duration: 0.536s, episode steps:  57, steps per second: 106, episode reward: -111.763, mean reward: -1.961 [-100.000, 11.004], mean action: 0.281 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.279532
 80167/100000: episode: 964, duration: 0.930s, episode steps:  86, steps per second:  92, episode reward: -146.676, mean reward: -1.706 [-100.000,  7.088], mean action: 0.488 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.278889
 80221/100000: episode: 965, duration: 0.559s, episode steps:  54, steps per second:  97, episode reward: -160.844, mean reward: -2.979 [-100.000,  7.088], mean action: 0.370 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.278258
 80297/100000: episode: 966, duration: 0.719s, episode steps:  76, steps per second: 106, episode reward: -160.259, mean reward: -2.109 [-100.000,  6.931], mean action: 0.395 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.277674
 80366/100000: episode: 967, duration: 0.672s, episode steps:  69, steps per second: 103, episode reward: -56.278, mean reward: -0.816 [-100.000, 64.910], mean action: 0.580 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.277021
 80435/100000: episode: 968, duration: 0.666s, episode steps:  69, steps per second: 104, episode reward: -162.605, mean reward: -2.357 [-100.000,  8.304], mean action: 0.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.276400
 80496/100000: episode: 969, duration: 0.581s, episode steps:  61, steps per second: 105, episode reward: -112.766, mean reward: -1.849 [-100.000,  6.816], mean action: 0.344 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.275815
 80557/100000: episode: 970, duration: 0.608s, episode steps:  61, steps per second: 100, episode reward: -138.018, mean reward: -2.263 [-100.000,  6.040], mean action: 0.623 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.275266
 80638/100000: episode: 971, duration: 0.894s, episode steps:  81, steps per second:  91, episode reward: -119.329, mean reward: -1.473 [-100.000,  7.868], mean action: 0.420 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.274627
 80708/100000: episode: 972, duration: 0.632s, episode steps:  70, steps per second: 111, episode reward: -145.487, mean reward: -2.078 [-100.000,  7.465], mean action: 0.357 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.273948
 80763/100000: episode: 973, duration: 0.532s, episode steps:  55, steps per second: 103, episode reward: -80.346, mean reward: -1.461 [-100.000, 13.490], mean action: 0.618 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.273385
 80820/100000: episode: 974, duration: 0.569s, episode steps:  57, steps per second: 100, episode reward: -109.315, mean reward: -1.918 [-100.000, 10.129], mean action: 0.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.272881
 80888/100000: episode: 975, duration: 0.660s, episode steps:  68, steps per second: 103, episode reward: -187.570, mean reward: -2.758 [-100.000, 15.807], mean action: 0.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.272318
 80944/100000: episode: 976, duration: 0.679s, episode steps:  56, steps per second:  82, episode reward: -97.834, mean reward: -1.747 [-100.000, 16.969], mean action: 0.571 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.271761
 81015/100000: episode: 977, duration: 0.824s, episode steps:  71, steps per second:  86, episode reward: -133.372, mean reward: -1.878 [-100.000,  7.180], mean action: 0.451 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.271189
 81076/100000: episode: 978, duration: 0.762s, episode steps:  61, steps per second:  80, episode reward: -133.621, mean reward: -2.191 [-100.000, 11.134], mean action: 0.344 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.270595
 81154/100000: episode: 979, duration: 0.890s, episode steps:  78, steps per second:  88, episode reward: -164.451, mean reward: -2.108 [-100.000,  6.816], mean action: 0.346 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.269969
 81233/100000: episode: 980, duration: 0.977s, episode steps:  79, steps per second:  81, episode reward: -162.582, mean reward: -2.058 [-100.000,  6.813], mean action: 0.418 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.269263
 81331/100000: episode: 981, duration: 1.017s, episode steps:  98, steps per second:  96, episode reward: -100.062, mean reward: -1.021 [-100.000, 17.135], mean action: 0.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.268467
 81423/100000: episode: 982, duration: 0.930s, episode steps:  92, steps per second:  99, episode reward: -117.973, mean reward: -1.282 [-100.000,  8.705], mean action: 0.380 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.267612
 81485/100000: episode: 983, duration: 0.596s, episode steps:  62, steps per second: 104, episode reward: -102.721, mean reward: -1.657 [-100.000, 25.398], mean action: 0.290 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.266918
 81550/100000: episode: 984, duration: 0.598s, episode steps:  65, steps per second: 109, episode reward: -140.033, mean reward: -2.154 [-100.000,  8.594], mean action: 0.585 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.266347
 81667/100000: episode: 985, duration: 1.186s, episode steps: 117, steps per second:  99, episode reward: 35.337, mean reward:  0.302 [-100.000, 109.007], mean action: 0.419 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.265528
 81732/100000: episode: 986, duration: 0.641s, episode steps:  65, steps per second: 101, episode reward: -137.899, mean reward: -2.122 [-100.000,  8.552], mean action: 0.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.264709
 81793/100000: episode: 987, duration: 0.534s, episode steps:  61, steps per second: 114, episode reward: -194.797, mean reward: -3.193 [-100.000,  6.272], mean action: 0.328 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.264142
 81877/100000: episode: 988, duration: 1.001s, episode steps:  84, steps per second:  84, episode reward: -110.827, mean reward: -1.319 [-100.000, 68.865], mean action: 0.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.263489
 81954/100000: episode: 989, duration: 0.742s, episode steps:  77, steps per second: 104, episode reward: -166.486, mean reward: -2.162 [-100.000,  7.871], mean action: 0.364 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.262765
 82015/100000: episode: 990, duration: 0.600s, episode steps:  61, steps per second: 102, episode reward: -121.962, mean reward: -1.999 [-100.000,  6.950], mean action: 0.393 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.262144
 82107/100000: episode: 991, duration: 0.965s, episode steps:  92, steps per second:  95, episode reward: -130.803, mean reward: -1.422 [-100.000,  6.972], mean action: 0.489 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.261455
 82171/100000: episode: 992, duration: 0.674s, episode steps:  64, steps per second:  95, episode reward: -144.182, mean reward: -2.253 [-100.000,  7.898], mean action: 0.156 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.260753
 82229/100000: episode: 993, duration: 0.553s, episode steps:  58, steps per second: 105, episode reward: -109.396, mean reward: -1.886 [-100.000,  6.837], mean action: 0.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.260205
 82286/100000: episode: 994, duration: 0.659s, episode steps:  57, steps per second:  86, episode reward: -136.197, mean reward: -2.389 [-100.000, 15.016], mean action: 0.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.259687
 82352/100000: episode: 995, duration: 0.663s, episode steps:  66, steps per second: 100, episode reward: -172.973, mean reward: -2.621 [-100.000,  7.274], mean action: 0.242 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.259133
 82432/100000: episode: 996, duration: 0.787s, episode steps:  80, steps per second: 102, episode reward: -121.571, mean reward: -1.520 [-100.000,  8.469], mean action: 0.425 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.258476
 82491/100000: episode: 997, duration: 0.610s, episode steps:  59, steps per second:  97, episode reward: -164.864, mean reward: -2.794 [-100.000,  5.459], mean action: 0.271 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.257851
 82545/100000: episode: 998, duration: 0.592s, episode steps:  54, steps per second:  91, episode reward: -92.384, mean reward: -1.711 [-100.000, 14.380], mean action: 0.574 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.257343
 82608/100000: episode: 999, duration: 0.698s, episode steps:  63, steps per second:  90, episode reward: -134.322, mean reward: -2.132 [-100.000,  7.125], mean action: 0.381 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.256816
 82670/100000: episode: 1000, duration: 0.691s, episode steps:  62, steps per second:  90, episode reward: -88.402, mean reward: -1.426 [-100.000, 17.875], mean action: 0.403 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.256254
 82742/100000: episode: 1001, duration: 0.705s, episode steps:  72, steps per second: 102, episode reward: -131.208, mean reward: -1.822 [-100.000, 16.722], mean action: 0.444 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.255651
 82837/100000: episode: 1002, duration: 0.845s, episode steps:  95, steps per second: 112, episode reward: -240.243, mean reward: -2.529 [-100.000,  6.144], mean action: 0.463 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.254899
 82927/100000: episode: 1003, duration: 0.844s, episode steps:  90, steps per second: 107, episode reward: -118.428, mean reward: -1.316 [-100.000,  7.692], mean action: 0.356 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.254066
 82990/100000: episode: 1004, duration: 0.572s, episode steps:  63, steps per second: 110, episode reward: -164.745, mean reward: -2.615 [-100.000,  6.961], mean action: 0.159 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.253378
 83055/100000: episode: 1005, duration: 0.678s, episode steps:  65, steps per second:  96, episode reward: -112.596, mean reward: -1.732 [-100.000, 16.073], mean action: 0.415 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.252802
 83131/100000: episode: 1006, duration: 0.816s, episode steps:  76, steps per second:  93, episode reward: -200.354, mean reward: -2.636 [-100.000, 45.759], mean action: 0.250 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.252167
 83208/100000: episode: 1007, duration: 0.746s, episode steps:  77, steps per second: 103, episode reward: -122.165, mean reward: -1.587 [-100.000, 20.712], mean action: 0.558 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.251479
 83317/100000: episode: 1008, duration: 0.994s, episode steps: 109, steps per second: 110, episode reward: -145.601, mean reward: -1.336 [-100.000,  6.504], mean action: 0.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.250642
 83376/100000: episode: 1009, duration: 0.558s, episode steps:  59, steps per second: 106, episode reward: -89.532, mean reward: -1.517 [-100.000, 16.735], mean action: 0.220 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.249886
 83447/100000: episode: 1010, duration: 0.624s, episode steps:  71, steps per second: 114, episode reward: -160.060, mean reward: -2.254 [-100.000,  6.266], mean action: 0.394 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.249301
 83511/100000: episode: 1011, duration: 0.591s, episode steps:  64, steps per second: 108, episode reward: -104.261, mean reward: -1.629 [-100.000,  7.981], mean action: 0.422 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.248693
 83606/100000: episode: 1012, duration: 0.862s, episode steps:  95, steps per second: 110, episode reward: -237.780, mean reward: -2.503 [-100.000,  4.061], mean action: 0.484 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.247978
 83691/100000: episode: 1013, duration: 0.742s, episode steps:  85, steps per second: 115, episode reward: -127.828, mean reward: -1.504 [-100.000,  8.671], mean action: 0.400 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.247168
 83763/100000: episode: 1014, duration: 0.656s, episode steps:  72, steps per second: 110, episode reward: -145.212, mean reward: -2.017 [-100.000, 87.449], mean action: 0.556 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.246462
 83857/100000: episode: 1015, duration: 0.888s, episode steps:  94, steps per second: 106, episode reward: -208.856, mean reward: -2.222 [-100.000,  7.074], mean action: 0.287 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.245714
 83928/100000: episode: 1016, duration: 0.634s, episode steps:  71, steps per second: 112, episode reward: -153.603, mean reward: -2.163 [-100.000,  8.095], mean action: 0.310 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.244972
 84005/100000: episode: 1017, duration: 0.672s, episode steps:  77, steps per second: 115, episode reward: -148.771, mean reward: -1.932 [-100.000,  7.141], mean action: 0.416 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.244306
 84089/100000: episode: 1018, duration: 0.788s, episode steps:  84, steps per second: 107, episode reward: -245.571, mean reward: -2.923 [-100.000,  5.762], mean action: 0.405 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.243581
 84162/100000: episode: 1019, duration: 0.656s, episode steps:  73, steps per second: 111, episode reward: -90.037, mean reward: -1.233 [-100.000, 10.142], mean action: 0.562 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.242875
 84217/100000: episode: 1020, duration: 0.482s, episode steps:  55, steps per second: 114, episode reward: -98.423, mean reward: -1.790 [-100.000, 20.473], mean action: 0.236 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.242299
 84301/100000: episode: 1021, duration: 0.747s, episode steps:  84, steps per second: 112, episode reward: -165.315, mean reward: -1.968 [-100.000,  6.545], mean action: 0.464 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.241673
 84402/100000: episode: 1022, duration: 0.930s, episode steps: 101, steps per second: 109, episode reward: -140.611, mean reward: -1.392 [-100.000, 17.039], mean action: 0.455 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.240841
 84530/100000: episode: 1023, duration: 1.174s, episode steps: 128, steps per second: 109, episode reward: 33.516, mean reward:  0.262 [-100.000, 139.828], mean action: 0.234 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.239810
 84600/100000: episode: 1024, duration: 0.658s, episode steps:  70, steps per second: 106, episode reward: -149.699, mean reward: -2.139 [-100.000,  6.732], mean action: 0.200 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.238920
 84676/100000: episode: 1025, duration: 0.863s, episode steps:  76, steps per second:  88, episode reward: -142.833, mean reward: -1.879 [-100.000,  9.893], mean action: 0.263 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.238262
 84728/100000: episode: 1026, duration: 0.763s, episode steps:  52, steps per second:  68, episode reward: -116.554, mean reward: -2.241 [-100.000,  6.901], mean action: 0.327 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.237686
 84807/100000: episode: 1027, duration: 0.873s, episode steps:  79, steps per second:  90, episode reward: -144.211, mean reward: -1.825 [-100.000,  8.246], mean action: 0.228 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.237097
 84869/100000: episode: 1028, duration: 0.611s, episode steps:  62, steps per second: 101, episode reward: -152.150, mean reward: -2.454 [-100.000,  6.286], mean action: 0.258 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.236462
 84949/100000: episode: 1029, duration: 0.799s, episode steps:  80, steps per second: 100, episode reward: -170.724, mean reward: -2.134 [-100.000,  6.458], mean action: 0.300 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.235823
 85011/100000: episode: 1030, duration: 0.673s, episode steps:  62, steps per second:  92, episode reward: -100.031, mean reward: -1.613 [-100.000, 14.137], mean action: 0.323 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.235184
 85078/100000: episode: 1031, duration: 0.794s, episode steps:  67, steps per second:  84, episode reward: -102.831, mean reward: -1.535 [-100.000, 11.373], mean action: 0.612 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.234604
 85156/100000: episode: 1032, duration: 0.925s, episode steps:  78, steps per second:  84, episode reward: -200.678, mean reward: -2.573 [-100.000,  6.215], mean action: 0.564 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.233952
 85209/100000: episode: 1033, duration: 0.573s, episode steps:  53, steps per second:  92, episode reward: -80.228, mean reward: -1.514 [-100.000, 11.689], mean action: 0.132 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.233362
 85282/100000: episode: 1034, duration: 0.745s, episode steps:  73, steps per second:  98, episode reward: -147.813, mean reward: -2.025 [-100.000,  6.843], mean action: 0.247 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.232795
 85363/100000: episode: 1035, duration: 0.903s, episode steps:  81, steps per second:  90, episode reward: -145.980, mean reward: -1.802 [-100.000, 42.369], mean action: 0.469 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.232102
 85459/100000: episode: 1036, duration: 0.931s, episode steps:  96, steps per second: 103, episode reward: -125.649, mean reward: -1.309 [-100.000, 12.226], mean action: 0.229 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.231306
 85517/100000: episode: 1037, duration: 0.776s, episode steps:  58, steps per second:  75, episode reward: -84.940, mean reward: -1.464 [-100.000, 20.059], mean action: 0.259 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.230612
 85580/100000: episode: 1038, duration: 0.664s, episode steps:  63, steps per second:  95, episode reward: -115.500, mean reward: -1.833 [-100.000,  7.664], mean action: 0.302 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.230068
 85634/100000: episode: 1039, duration: 0.627s, episode steps:  54, steps per second:  86, episode reward: -126.162, mean reward: -2.336 [-100.000,  7.218], mean action: 0.389 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.229541
 85714/100000: episode: 1040, duration: 0.836s, episode steps:  80, steps per second:  96, episode reward: -178.726, mean reward: -2.234 [-100.000,  9.579], mean action: 0.375 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.228938
 85767/100000: episode: 1041, duration: 0.491s, episode steps:  53, steps per second: 108, episode reward: -114.355, mean reward: -2.158 [-100.000, 33.380], mean action: 0.340 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.228340
 85828/100000: episode: 1042, duration: 0.637s, episode steps:  61, steps per second:  96, episode reward: -168.839, mean reward: -2.768 [-100.000,  5.079], mean action: 0.295 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.227827
 85902/100000: episode: 1043, duration: 0.694s, episode steps:  74, steps per second: 107, episode reward: -157.169, mean reward: -2.124 [-100.000,  7.734], mean action: 0.203 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.227219
 85966/100000: episode: 1044, duration: 0.694s, episode steps:  64, steps per second:  92, episode reward: -122.580, mean reward: -1.915 [-100.000, 19.339], mean action: 0.453 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.226598
 86050/100000: episode: 1045, duration: 0.871s, episode steps:  84, steps per second:  96, episode reward: -170.731, mean reward: -2.033 [-100.000,  6.782], mean action: 0.310 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.225933
 86112/100000: episode: 1046, duration: 0.675s, episode steps:  62, steps per second:  92, episode reward: -123.937, mean reward: -1.999 [-100.000, 22.551], mean action: 0.339 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.225276
 86212/100000: episode: 1047, duration: 1.018s, episode steps: 100, steps per second:  98, episode reward: -142.026, mean reward: -1.420 [-100.000,  8.243], mean action: 0.330 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.224547
 86291/100000: episode: 1048, duration: 0.742s, episode steps:  79, steps per second: 106, episode reward: -124.070, mean reward: -1.571 [-100.000,  7.835], mean action: 0.177 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.223741
 86374/100000: episode: 1049, duration: 0.800s, episode steps:  83, steps per second: 104, episode reward: -125.751, mean reward: -1.515 [-100.000, 13.142], mean action: 0.313 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.223012
 86447/100000: episode: 1050, duration: 0.680s, episode steps:  73, steps per second: 107, episode reward: -138.994, mean reward: -1.904 [-100.000, 37.336], mean action: 0.370 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.222310
 86526/100000: episode: 1051, duration: 0.844s, episode steps:  79, steps per second:  94, episode reward: -140.312, mean reward: -1.776 [-100.000,  8.408], mean action: 0.329 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.221626
 86588/100000: episode: 1052, duration: 0.655s, episode steps:  62, steps per second:  95, episode reward: -117.002, mean reward: -1.887 [-100.000, 22.280], mean action: 0.258 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.220992
 86642/100000: episode: 1053, duration: 0.640s, episode steps:  54, steps per second:  84, episode reward: -93.209, mean reward: -1.726 [-100.000,  8.417], mean action: 0.296 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.220469
 86717/100000: episode: 1054, duration: 0.742s, episode steps:  75, steps per second: 101, episode reward: -183.458, mean reward: -2.446 [-100.000,  5.598], mean action: 0.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.219889
 86808/100000: episode: 1055, duration: 0.983s, episode steps:  91, steps per second:  93, episode reward: -114.096, mean reward: -1.254 [-100.000, 24.945], mean action: 0.374 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.219142
 86903/100000: episode: 1056, duration: 0.900s, episode steps:  95, steps per second: 106, episode reward: -346.339, mean reward: -3.646 [-100.000, 134.372], mean action: 0.305 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.218305
 86960/100000: episode: 1057, duration: 0.575s, episode steps:  57, steps per second:  99, episode reward: -33.424, mean reward: -0.586 [-100.000, 65.651], mean action: 0.298 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.217621
 87031/100000: episode: 1058, duration: 0.729s, episode steps:  71, steps per second:  97, episode reward: -131.477, mean reward: -1.852 [-100.000,  6.787], mean action: 0.254 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.217045
 87106/100000: episode: 1059, duration: 0.712s, episode steps:  75, steps per second: 105, episode reward: -125.495, mean reward: -1.673 [-100.000, 24.811], mean action: 0.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.216388
 87226/100000: episode: 1060, duration: 1.153s, episode steps: 120, steps per second: 104, episode reward: -455.259, mean reward: -3.794 [-100.000, 96.086], mean action: 0.458 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.215510
 87295/100000: episode: 1061, duration: 0.737s, episode steps:  69, steps per second:  94, episode reward: -186.387, mean reward: -2.701 [-100.000,  6.370], mean action: 0.377 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.214660
 87350/100000: episode: 1062, duration: 0.576s, episode steps:  55, steps per second:  95, episode reward: -177.509, mean reward: -3.227 [-100.000,  6.013], mean action: 0.255 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.214102
 87423/100000: episode: 1063, duration: 0.930s, episode steps:  73, steps per second:  79, episode reward: -200.868, mean reward: -2.752 [-100.000,  7.528], mean action: 0.301 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.213526
 87507/100000: episode: 1064, duration: 0.986s, episode steps:  84, steps per second:  85, episode reward: -142.137, mean reward: -1.692 [-100.000, 13.832], mean action: 0.440 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.212819
 87578/100000: episode: 1065, duration: 0.717s, episode steps:  71, steps per second:  99, episode reward: -126.385, mean reward: -1.780 [-100.000, 14.215], mean action: 0.324 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.212122
 87649/100000: episode: 1066, duration: 0.793s, episode steps:  71, steps per second:  90, episode reward: -147.145, mean reward: -2.072 [-100.000, 15.606], mean action: 0.310 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.211483
 87729/100000: episode: 1067, duration: 0.792s, episode steps:  80, steps per second: 101, episode reward: -175.223, mean reward: -2.190 [-100.000, 56.594], mean action: 0.263 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.210803
 87801/100000: episode: 1068, duration: 0.662s, episode steps:  72, steps per second: 109, episode reward: -127.397, mean reward: -1.769 [-100.000,  7.598], mean action: 0.333 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.210119
 87863/100000: episode: 1069, duration: 0.663s, episode steps:  62, steps per second:  94, episode reward: -139.078, mean reward: -2.243 [-100.000,  8.252], mean action: 0.161 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.209516
 87931/100000: episode: 1070, duration: 0.679s, episode steps:  68, steps per second: 100, episode reward: -109.727, mean reward: -1.614 [-100.000,  8.443], mean action: 0.221 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.208931
 88012/100000: episode: 1071, duration: 0.773s, episode steps:  81, steps per second: 105, episode reward: -29.509, mean reward: -0.364 [-100.000, 100.078], mean action: 0.346 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.208261
 88088/100000: episode: 1072, duration: 0.861s, episode steps:  76, steps per second:  88, episode reward: -151.680, mean reward: -1.996 [-100.000,  6.459], mean action: 0.289 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.207555
 88147/100000: episode: 1073, duration: 0.621s, episode steps:  59, steps per second:  95, episode reward: -132.538, mean reward: -2.246 [-100.000,  7.461], mean action: 0.559 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.206947
 88233/100000: episode: 1074, duration: 0.871s, episode steps:  86, steps per second:  99, episode reward: -116.688, mean reward: -1.357 [-100.000,  7.607], mean action: 0.326 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.206294
 88308/100000: episode: 1075, duration: 0.736s, episode steps:  75, steps per second: 102, episode reward: -182.294, mean reward: -2.431 [-100.000,  5.490], mean action: 0.133 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.205570
 88384/100000: episode: 1076, duration: 0.740s, episode steps:  76, steps per second: 103, episode reward: -122.165, mean reward: -1.607 [-100.000, 17.356], mean action: 0.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.204890
 88455/100000: episode: 1077, duration: 0.658s, episode steps:  71, steps per second: 108, episode reward: -111.692, mean reward: -1.573 [-100.000, 16.985], mean action: 0.296 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.204229
 88534/100000: episode: 1078, duration: 0.934s, episode steps:  79, steps per second:  85, episode reward: -185.345, mean reward: -2.346 [-100.000,  5.812], mean action: 0.241 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.203554
 88621/100000: episode: 1079, duration: 0.940s, episode steps:  87, steps per second:  93, episode reward: -174.285, mean reward: -2.003 [-100.000, 15.032], mean action: 0.299 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.202807
 88700/100000: episode: 1080, duration: 0.797s, episode steps:  79, steps per second:  99, episode reward: -144.027, mean reward: -1.823 [-100.000,  8.122], mean action: 0.329 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.202060
 88781/100000: episode: 1081, duration: 0.796s, episode steps:  81, steps per second: 102, episode reward: -116.847, mean reward: -1.443 [-100.000, 13.745], mean action: 0.321 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.201340
 88875/100000: episode: 1082, duration: 0.915s, episode steps:  94, steps per second: 103, episode reward: -132.303, mean reward: -1.407 [-100.000, 14.506], mean action: 0.340 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.200552
 88972/100000: episode: 1083, duration: 1.028s, episode steps:  97, steps per second:  94, episode reward: -145.445, mean reward: -1.499 [-100.000, 15.710], mean action: 0.340 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.199693
 89040/100000: episode: 1084, duration: 0.702s, episode steps:  68, steps per second:  97, episode reward: -110.998, mean reward: -1.632 [-100.000, 17.237], mean action: 0.221 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.198951
 89096/100000: episode: 1085, duration: 0.554s, episode steps:  56, steps per second: 101, episode reward: -115.733, mean reward: -2.067 [-100.000,  7.247], mean action: 0.161 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.198392
 89171/100000: episode: 1086, duration: 0.748s, episode steps:  75, steps per second: 100, episode reward: -111.524, mean reward: -1.487 [-100.000, 10.544], mean action: 0.373 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.197803
 89224/100000: episode: 1087, duration: 0.522s, episode steps:  53, steps per second: 102, episode reward: -116.248, mean reward: -2.193 [-100.000,  7.619], mean action: 0.434 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.197227
 89283/100000: episode: 1088, duration: 0.548s, episode steps:  59, steps per second: 108, episode reward: -131.033, mean reward: -2.221 [-100.000,  6.799], mean action: 0.203 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.196723
 89342/100000: episode: 1089, duration: 0.616s, episode steps:  59, steps per second:  96, episode reward: -127.088, mean reward: -2.154 [-100.000, 16.061], mean action: 0.305 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.196192
 89403/100000: episode: 1090, duration: 0.640s, episode steps:  61, steps per second:  95, episode reward: -117.302, mean reward: -1.923 [-100.000, 21.082], mean action: 0.279 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.195652
 89459/100000: episode: 1091, duration: 0.616s, episode steps:  56, steps per second:  91, episode reward: -129.719, mean reward: -2.316 [-100.000,  8.803], mean action: 0.143 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.195126
 89519/100000: episode: 1092, duration: 0.677s, episode steps:  60, steps per second:  89, episode reward: -81.895, mean reward: -1.365 [-100.000, 24.845], mean action: 0.450 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.194603
 89594/100000: episode: 1093, duration: 0.834s, episode steps:  75, steps per second:  90, episode reward: -116.194, mean reward: -1.549 [-100.000, 44.471], mean action: 0.240 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.193996
 89668/100000: episode: 1094, duration: 0.734s, episode steps:  74, steps per second: 101, episode reward: -145.453, mean reward: -1.966 [-100.000, 15.566], mean action: 0.324 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.193325
 89774/100000: episode: 1095, duration: 1.028s, episode steps: 106, steps per second: 103, episode reward: -249.619, mean reward: -2.355 [-100.000, 126.276], mean action: 0.425 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.192516
 89844/100000: episode: 1096, duration: 0.643s, episode steps:  70, steps per second: 109, episode reward: -122.533, mean reward: -1.750 [-100.000,  8.402], mean action: 0.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.191724
 89942/100000: episode: 1097, duration: 0.907s, episode steps:  98, steps per second: 108, episode reward: -139.670, mean reward: -1.425 [-100.000, 10.043], mean action: 0.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.190967
 90026/100000: episode: 1098, duration: 0.785s, episode steps:  84, steps per second: 107, episode reward: -146.360, mean reward: -1.742 [-100.000, 16.336], mean action: 0.345 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.190148
 90086/100000: episode: 1099, duration: 0.536s, episode steps:  60, steps per second: 112, episode reward: -101.718, mean reward: -1.695 [-100.000, 12.716], mean action: 0.350 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.189500
 90154/100000: episode: 1100, duration: 0.658s, episode steps:  68, steps per second: 103, episode reward: -131.238, mean reward: -1.930 [-100.000,  7.871], mean action: 0.118 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.188924
 90215/100000: episode: 1101, duration: 0.574s, episode steps:  61, steps per second: 106, episode reward: -136.281, mean reward: -2.234 [-100.000, 17.487], mean action: 0.230 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.188344
 90274/100000: episode: 1102, duration: 0.541s, episode steps:  59, steps per second: 109, episode reward: -99.992, mean reward: -1.695 [-100.000, 14.344], mean action: 0.508 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.187804
 90355/100000: episode: 1103, duration: 0.731s, episode steps:  81, steps per second: 111, episode reward: -140.951, mean reward: -1.740 [-100.000,  8.227], mean action: 0.185 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.187174
 90423/100000: episode: 1104, duration: 0.624s, episode steps:  68, steps per second: 109, episode reward: -169.683, mean reward: -2.495 [-100.000,  5.709], mean action: 0.176 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.186503
 90490/100000: episode: 1105, duration: 0.643s, episode steps:  67, steps per second: 104, episode reward: -145.220, mean reward: -2.167 [-100.000,  6.008], mean action: 0.284 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.185896
 90553/100000: episode: 1106, duration: 0.605s, episode steps:  63, steps per second: 104, episode reward: -149.255, mean reward: -2.369 [-100.000,  7.875], mean action: 0.159 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.185311
 90623/100000: episode: 1107, duration: 0.669s, episode steps:  70, steps per second: 105, episode reward: -134.136, mean reward: -1.916 [-100.000,  8.190], mean action: 0.386 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.184712
 90704/100000: episode: 1108, duration: 0.786s, episode steps:  81, steps per second: 103, episode reward: -184.744, mean reward: -2.281 [-100.000, 36.254], mean action: 0.259 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.184033
 90779/100000: episode: 1109, duration: 0.689s, episode steps:  75, steps per second: 109, episode reward: -125.425, mean reward: -1.672 [-100.000,  6.602], mean action: 0.227 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.183331
 90835/100000: episode: 1110, duration: 0.567s, episode steps:  56, steps per second:  99, episode reward: -82.617, mean reward: -1.475 [-100.000, 15.968], mean action: 0.214 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.182741
 90902/100000: episode: 1111, duration: 0.703s, episode steps:  67, steps per second:  95, episode reward: -111.483, mean reward: -1.664 [-100.000, 16.346], mean action: 0.269 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.182188
 90966/100000: episode: 1112, duration: 0.659s, episode steps:  64, steps per second:  97, episode reward: -210.472, mean reward: -3.289 [-100.000,  5.065], mean action: 0.297 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.181598
 91040/100000: episode: 1113, duration: 0.741s, episode steps:  74, steps per second: 100, episode reward: -132.130, mean reward: -1.786 [-100.000, 13.306], mean action: 0.365 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.180977
 91134/100000: episode: 1114, duration: 1.130s, episode steps:  94, steps per second:  83, episode reward: -128.022, mean reward: -1.362 [-100.000,  6.373], mean action: 0.287 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.180221
 91228/100000: episode: 1115, duration: 0.964s, episode steps:  94, steps per second:  98, episode reward: -327.009, mean reward: -3.479 [-100.000, 126.052], mean action: 0.298 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.179375
 91293/100000: episode: 1116, duration: 0.723s, episode steps:  65, steps per second:  90, episode reward: -157.203, mean reward: -2.419 [-100.000, 16.263], mean action: 0.246 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.178660
 91355/100000: episode: 1117, duration: 0.657s, episode steps:  62, steps per second:  94, episode reward: -103.834, mean reward: -1.675 [-100.000, 16.672], mean action: 0.210 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.178088
 91439/100000: episode: 1118, duration: 0.870s, episode steps:  84, steps per second:  97, episode reward: -218.619, mean reward: -2.603 [-100.000,  5.716], mean action: 0.167 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.177431
 91495/100000: episode: 1119, duration: 0.619s, episode steps:  56, steps per second:  90, episode reward: -88.410, mean reward: -1.579 [-100.000, 17.508], mean action: 0.107 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.176802
 91566/100000: episode: 1120, duration: 0.823s, episode steps:  71, steps per second:  86, episode reward: -123.873, mean reward: -1.745 [-100.000, 17.098], mean action: 0.282 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.176230
 91630/100000: episode: 1121, duration: 0.783s, episode steps:  64, steps per second:  82, episode reward: -111.309, mean reward: -1.739 [-100.000, 16.995], mean action: 0.062 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.175622
 91716/100000: episode: 1122, duration: 1.235s, episode steps:  86, steps per second:  70, episode reward: -156.086, mean reward: -1.815 [-100.000, 22.500], mean action: 0.209 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.174948
 91802/100000: episode: 1123, duration: 0.911s, episode steps:  86, steps per second:  94, episode reward: -99.127, mean reward: -1.153 [-100.000, 12.075], mean action: 0.244 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.174173
 91892/100000: episode: 1124, duration: 1.005s, episode steps:  90, steps per second:  90, episode reward: -168.814, mean reward: -1.876 [-100.000,  4.992], mean action: 0.189 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.173381
 91975/100000: episode: 1125, duration: 1.128s, episode steps:  83, steps per second:  74, episode reward: -178.602, mean reward: -2.152 [-100.000, 17.771], mean action: 0.241 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.172603
 92045/100000: episode: 1126, duration: 0.821s, episode steps:  70, steps per second:  85, episode reward: -132.524, mean reward: -1.893 [-100.000, 75.941], mean action: 0.271 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.171914
 92121/100000: episode: 1127, duration: 0.800s, episode steps:  76, steps per second:  95, episode reward: -116.349, mean reward: -1.531 [-100.000, 51.516], mean action: 0.197 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.171257
 92196/100000: episode: 1128, duration: 0.785s, episode steps:  75, steps per second:  95, episode reward: -117.047, mean reward: -1.561 [-100.000, 10.671], mean action: 0.267 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.170578
 92279/100000: episode: 1129, duration: 0.913s, episode steps:  83, steps per second:  91, episode reward: -214.085, mean reward: -2.579 [-100.000, 52.866], mean action: 0.361 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.169867
 92378/100000: episode: 1130, duration: 1.010s, episode steps:  99, steps per second:  98, episode reward: -355.671, mean reward: -3.593 [-100.000, 114.613], mean action: 0.131 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.169048
 92459/100000: episode: 1131, duration: 0.929s, episode steps:  81, steps per second:  87, episode reward: -223.153, mean reward: -2.755 [-100.000,  4.452], mean action: 0.407 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.168238
 92528/100000: episode: 1132, duration: 0.709s, episode steps:  69, steps per second:  97, episode reward: -123.866, mean reward: -1.795 [-100.000,  8.068], mean action: 0.246 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.167563
 92583/100000: episode: 1133, duration: 0.598s, episode steps:  55, steps per second:  92, episode reward: -85.232, mean reward: -1.550 [-100.000,  8.295], mean action: 0.291 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.167005
 92655/100000: episode: 1134, duration: 0.735s, episode steps:  72, steps per second:  98, episode reward: -186.555, mean reward: -2.591 [-100.000,  5.725], mean action: 0.250 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.166433
 92731/100000: episode: 1135, duration: 0.779s, episode steps:  76, steps per second:  98, episode reward: -138.870, mean reward: -1.827 [-100.000,  7.314], mean action: 0.224 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.165767
 92796/100000: episode: 1136, duration: 0.578s, episode steps:  65, steps per second: 112, episode reward: -154.052, mean reward: -2.370 [-100.000,  5.824], mean action: 0.308 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.165133
 92878/100000: episode: 1137, duration: 0.809s, episode steps:  82, steps per second: 101, episode reward: -132.692, mean reward: -1.618 [-100.000,  7.494], mean action: 0.134 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.164471
 92972/100000: episode: 1138, duration: 0.970s, episode steps:  94, steps per second:  97, episode reward: -210.298, mean reward: -2.237 [-100.000,  6.265], mean action: 0.234 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.163679
 93033/100000: episode: 1139, duration: 0.557s, episode steps:  61, steps per second: 110, episode reward: -129.869, mean reward: -2.129 [-100.000, 10.943], mean action: 0.230 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.162982
 93109/100000: episode: 1140, duration: 0.721s, episode steps:  76, steps per second: 105, episode reward: -170.517, mean reward: -2.244 [-100.000, 110.741], mean action: 0.171 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.162365
 93198/100000: episode: 1141, duration: 0.895s, episode steps:  89, steps per second:  99, episode reward: -155.359, mean reward: -1.746 [-100.000,  7.354], mean action: 0.258 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.161623
 93250/100000: episode: 1142, duration: 0.635s, episode steps:  52, steps per second:  82, episode reward: -125.552, mean reward: -2.414 [-100.000,  6.752], mean action: 0.115 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.160988
 93336/100000: episode: 1143, duration: 0.921s, episode steps:  86, steps per second:  93, episode reward: -187.683, mean reward: -2.182 [-100.000,  5.585], mean action: 0.360 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.160367
 93411/100000: episode: 1144, duration: 0.856s, episode steps:  75, steps per second:  88, episode reward: -113.030, mean reward: -1.507 [-100.000, 24.813], mean action: 0.347 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.159643
 93464/100000: episode: 1145, duration: 0.517s, episode steps:  53, steps per second: 103, episode reward: -153.865, mean reward: -2.903 [-100.000,  4.824], mean action: 0.358 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.159067
 93517/100000: episode: 1146, duration: 0.552s, episode steps:  53, steps per second:  96, episode reward: -112.865, mean reward: -2.130 [-100.000,  7.400], mean action: 0.264 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.158590
 93585/100000: episode: 1147, duration: 0.658s, episode steps:  68, steps per second: 103, episode reward: -141.044, mean reward: -2.074 [-100.000,  9.256], mean action: 0.294 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.158045
 93639/100000: episode: 1148, duration: 0.513s, episode steps:  54, steps per second: 105, episode reward: -88.809, mean reward: -1.645 [-100.000, 16.951], mean action: 0.037 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.157496
 93696/100000: episode: 1149, duration: 0.530s, episode steps:  57, steps per second: 107, episode reward: -82.694, mean reward: -1.451 [-100.000, 53.629], mean action: 0.211 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.156997
 93785/100000: episode: 1150, duration: 0.877s, episode steps:  89, steps per second: 102, episode reward: -131.071, mean reward: -1.473 [-100.000,  6.426], mean action: 0.270 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.156340
 93848/100000: episode: 1151, duration: 0.688s, episode steps:  63, steps per second:  92, episode reward: -137.867, mean reward: -2.188 [-100.000,  7.158], mean action: 0.206 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.155656
 93926/100000: episode: 1152, duration: 0.888s, episode steps:  78, steps per second:  88, episode reward: -7.619, mean reward: -0.098 [-100.000, 131.685], mean action: 0.256 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.155021
 94003/100000: episode: 1153, duration: 0.856s, episode steps:  77, steps per second:  90, episode reward: -180.122, mean reward: -2.339 [-100.000, 11.309], mean action: 0.182 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.154324
 94064/100000: episode: 1154, duration: 0.675s, episode steps:  61, steps per second:  90, episode reward: -105.294, mean reward: -1.726 [-100.000, 11.726], mean action: 0.197 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.153703
 94153/100000: episode: 1155, duration: 1.071s, episode steps:  89, steps per second:  83, episode reward: -274.139, mean reward: -3.080 [-100.000, 95.935], mean action: 0.337 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.153028
 94214/100000: episode: 1156, duration: 0.656s, episode steps:  61, steps per second:  93, episode reward: -114.155, mean reward: -1.871 [-100.000,  8.224], mean action: 0.148 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.152353
 94277/100000: episode: 1157, duration: 0.656s, episode steps:  63, steps per second:  96, episode reward: -121.040, mean reward: -1.921 [-100.000,  7.202], mean action: 0.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.151795
 94362/100000: episode: 1158, duration: 0.921s, episode steps:  85, steps per second:  92, episode reward: -293.083, mean reward: -3.448 [-100.000, 119.664], mean action: 0.106 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.151129
 94419/100000: episode: 1159, duration: 0.552s, episode steps:  57, steps per second: 103, episode reward: -162.192, mean reward: -2.845 [-100.000,  6.299], mean action: 0.263 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.150490
 94473/100000: episode: 1160, duration: 0.555s, episode steps:  54, steps per second:  97, episode reward: -153.564, mean reward: -2.844 [-100.000,  5.813], mean action: 0.241 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.149990
 94536/100000: episode: 1161, duration: 0.692s, episode steps:  63, steps per second:  91, episode reward: -183.213, mean reward: -2.908 [-100.000,  5.912], mean action: 0.143 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.149464
 94591/100000: episode: 1162, duration: 0.597s, episode steps:  55, steps per second:  92, episode reward: -145.215, mean reward: -2.640 [-100.000,  6.619], mean action: 0.364 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.148933
 94658/100000: episode: 1163, duration: 0.657s, episode steps:  67, steps per second: 102, episode reward: -145.056, mean reward: -2.165 [-100.000, 45.454], mean action: 0.448 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.148384
 94743/100000: episode: 1164, duration: 0.871s, episode steps:  85, steps per second:  98, episode reward: -130.088, mean reward: -1.530 [-100.000,  6.294], mean action: 0.212 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.147700
 94812/100000: episode: 1165, duration: 0.703s, episode steps:  69, steps per second:  98, episode reward: -153.491, mean reward: -2.225 [-100.000,  5.987], mean action: 0.203 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.147007
 94864/100000: episode: 1166, duration: 0.607s, episode steps:  52, steps per second:  86, episode reward: -97.381, mean reward: -1.873 [-100.000,  7.584], mean action: 0.231 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.146462
 94924/100000: episode: 1167, duration: 0.685s, episode steps:  60, steps per second:  88, episode reward: -93.602, mean reward: -1.560 [-100.000,  8.665], mean action: 0.217 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.145958
 94985/100000: episode: 1168, duration: 0.607s, episode steps:  61, steps per second: 100, episode reward: -103.490, mean reward: -1.697 [-100.000, 16.558], mean action: 0.098 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.145414
 95065/100000: episode: 1169, duration: 0.753s, episode steps:  80, steps per second: 106, episode reward: -111.858, mean reward: -1.398 [-100.000, 16.097], mean action: 0.175 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.144780
 95132/100000: episode: 1170, duration: 0.854s, episode steps:  67, steps per second:  78, episode reward: -154.278, mean reward: -2.303 [-100.000,  7.176], mean action: 0.254 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.144118
 95196/100000: episode: 1171, duration: 0.706s, episode steps:  64, steps per second:  91, episode reward: -140.294, mean reward: -2.192 [-100.000,  7.462], mean action: 0.219 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.143529
 95272/100000: episode: 1172, duration: 1.147s, episode steps:  76, steps per second:  66, episode reward: -146.818, mean reward: -1.932 [-100.000, 10.861], mean action: 0.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.142898
 95351/100000: episode: 1173, duration: 1.258s, episode steps:  79, steps per second:  63, episode reward: -124.575, mean reward: -1.577 [-100.000,  8.637], mean action: 0.215 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.142201
 95407/100000: episode: 1174, duration: 0.933s, episode steps:  56, steps per second:  60, episode reward: -118.184, mean reward: -2.110 [-100.000,  7.629], mean action: 0.232 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.141593
 95483/100000: episode: 1175, duration: 1.098s, episode steps:  76, steps per second:  69, episode reward: -176.390, mean reward: -2.321 [-100.000,  7.383], mean action: 0.118 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.140999
 95581/100000: episode: 1176, duration: 1.477s, episode steps:  98, steps per second:  66, episode reward: -182.367, mean reward: -1.861 [-100.000,  5.195], mean action: 0.184 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.140216
 95640/100000: episode: 1177, duration: 1.116s, episode steps:  59, steps per second:  53, episode reward: -110.324, mean reward: -1.870 [-100.000,  7.365], mean action: 0.254 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.139510
 95695/100000: episode: 1178, duration: 0.938s, episode steps:  55, steps per second:  59, episode reward: -132.610, mean reward: -2.411 [-100.000,  6.133], mean action: 0.127 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.138997
 95762/100000: episode: 1179, duration: 0.760s, episode steps:  67, steps per second:  88, episode reward: -110.969, mean reward: -1.656 [-100.000, 41.968], mean action: 0.149 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.138448
 95856/100000: episode: 1180, duration: 0.891s, episode steps:  94, steps per second: 105, episode reward: -109.246, mean reward: -1.162 [-100.000,  7.458], mean action: 0.223 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.137723
 95910/100000: episode: 1181, duration: 0.523s, episode steps:  54, steps per second: 103, episode reward: -128.124, mean reward: -2.373 [-100.000, 16.339], mean action: 0.111 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.137057
 95982/100000: episode: 1182, duration: 0.658s, episode steps:  72, steps per second: 109, episode reward: -136.909, mean reward: -1.902 [-100.000,  7.573], mean action: 0.181 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.136490
 96071/100000: episode: 1183, duration: 0.788s, episode steps:  89, steps per second: 113, episode reward: -174.200, mean reward: -1.957 [-100.000,  6.520], mean action: 0.225 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.135766
 96162/100000: episode: 1184, duration: 0.927s, episode steps:  91, steps per second:  98, episode reward: -119.440, mean reward: -1.313 [-100.000, 16.333], mean action: 0.110 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.134956
 96228/100000: episode: 1185, duration: 0.656s, episode steps:  66, steps per second: 101, episode reward: -175.195, mean reward: -2.654 [-100.000,  5.713], mean action: 0.045 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.134249
 96286/100000: episode: 1186, duration: 0.537s, episode steps:  58, steps per second: 108, episode reward: -124.759, mean reward: -2.151 [-100.000,  6.929], mean action: 0.241 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.133691
 96345/100000: episode: 1187, duration: 0.532s, episode steps:  59, steps per second: 111, episode reward: -151.709, mean reward: -2.571 [-100.000,  6.971], mean action: 0.203 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.133165
 96441/100000: episode: 1188, duration: 0.893s, episode steps:  96, steps per second: 108, episode reward: -153.484, mean reward: -1.599 [-100.000, 118.066], mean action: 0.188 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.132467
 96539/100000: episode: 1189, duration: 0.863s, episode steps:  98, steps per second: 114, episode reward: -255.199, mean reward: -2.604 [-100.000,  7.387], mean action: 0.327 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.131594
 96627/100000: episode: 1190, duration: 0.831s, episode steps:  88, steps per second: 106, episode reward: -162.349, mean reward: -1.845 [-100.000,  5.621], mean action: 0.148 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.130757
 96690/100000: episode: 1191, duration: 0.571s, episode steps:  63, steps per second: 110, episode reward: -146.208, mean reward: -2.321 [-100.000,  6.092], mean action: 0.127 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.130078
 96752/100000: episode: 1192, duration: 0.762s, episode steps:  62, steps per second:  81, episode reward: -130.205, mean reward: -2.100 [-100.000,  7.605], mean action: 0.226 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.129515
 96817/100000: episode: 1193, duration: 0.947s, episode steps:  65, steps per second:  69, episode reward: -131.747, mean reward: -2.027 [-100.000,  6.844], mean action: 0.062 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.128944
 96904/100000: episode: 1194, duration: 1.219s, episode steps:  87, steps per second:  71, episode reward: -128.531, mean reward: -1.477 [-100.000, 16.354], mean action: 0.218 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.128260
 96961/100000: episode: 1195, duration: 0.721s, episode steps:  57, steps per second:  79, episode reward: -122.209, mean reward: -2.144 [-100.000,  7.608], mean action: 0.316 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.127612
 97019/100000: episode: 1196, duration: 0.697s, episode steps:  58, steps per second:  83, episode reward: -101.494, mean reward: -1.750 [-100.000, 16.788], mean action: 0.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.127094
 97106/100000: episode: 1197, duration: 0.973s, episode steps:  87, steps per second:  89, episode reward: -123.384, mean reward: -1.418 [-100.000,  8.164], mean action: 0.184 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.126442
 97191/100000: episode: 1198, duration: 0.934s, episode steps:  85, steps per second:  91, episode reward: -101.393, mean reward: -1.193 [-100.000, 15.573], mean action: 0.165 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.125668
 97275/100000: episode: 1199, duration: 1.064s, episode steps:  84, steps per second:  79, episode reward: -129.332, mean reward: -1.540 [-100.000,  7.879], mean action: 0.083 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.124907
 97342/100000: episode: 1200, duration: 1.269s, episode steps:  67, steps per second:  53, episode reward: -122.528, mean reward: -1.829 [-100.000, 23.269], mean action: 0.060 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.124228
 97420/100000: episode: 1201, duration: 1.004s, episode steps:  78, steps per second:  78, episode reward: -218.226, mean reward: -2.798 [-100.000,  5.586], mean action: 0.244 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.123575
 97509/100000: episode: 1202, duration: 1.084s, episode steps:  89, steps per second:  82, episode reward: -148.603, mean reward: -1.670 [-100.000,  7.578], mean action: 0.202 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.122824
 97598/100000: episode: 1203, duration: 0.919s, episode steps:  89, steps per second:  97, episode reward: -89.730, mean reward: -1.008 [-100.000, 16.909], mean action: 0.202 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.122023
 97661/100000: episode: 1204, duration: 0.643s, episode steps:  63, steps per second:  98, episode reward: -150.565, mean reward: -2.390 [-100.000,  6.997], mean action: 0.222 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.121339
 97729/100000: episode: 1205, duration: 0.713s, episode steps:  68, steps per second:  95, episode reward: -145.666, mean reward: -2.142 [-100.000,  6.389], mean action: 0.353 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.120749
 97820/100000: episode: 1206, duration: 0.860s, episode steps:  91, steps per second: 106, episode reward: -129.305, mean reward: -1.421 [-100.000,  6.711], mean action: 0.165 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.120034
 97881/100000: episode: 1207, duration: 0.568s, episode steps:  61, steps per second: 107, episode reward: -125.102, mean reward: -2.051 [-100.000,  6.312], mean action: 0.246 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.119350
 97947/100000: episode: 1208, duration: 0.603s, episode steps:  66, steps per second: 109, episode reward: -138.411, mean reward: -2.097 [-100.000,  7.089], mean action: 0.152 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.118778
 98024/100000: episode: 1209, duration: 0.705s, episode steps:  77, steps per second: 109, episode reward: -127.747, mean reward: -1.659 [-100.000, 12.661], mean action: 0.221 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.118135
 98115/100000: episode: 1210, duration: 0.876s, episode steps:  91, steps per second: 104, episode reward: -158.064, mean reward: -1.737 [-100.000,  6.672], mean action: 0.154 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.117379
 98181/100000: episode: 1211, duration: 0.656s, episode steps:  66, steps per second: 101, episode reward: -114.531, mean reward: -1.735 [-100.000,  8.603], mean action: 0.197 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.116672
 98262/100000: episode: 1212, duration: 0.738s, episode steps:  81, steps per second: 110, episode reward: -46.242, mean reward: -0.571 [-100.000, 73.126], mean action: 0.173 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.116011
 98338/100000: episode: 1213, duration: 0.741s, episode steps:  76, steps per second: 103, episode reward: -106.936, mean reward: -1.407 [-100.000, 14.627], mean action: 0.066 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.115304
 98426/100000: episode: 1214, duration: 0.847s, episode steps:  88, steps per second: 104, episode reward: -130.915, mean reward: -1.488 [-100.000,  9.899], mean action: 0.182 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.114566
 98486/100000: episode: 1215, duration: 0.598s, episode steps:  60, steps per second: 100, episode reward: -142.356, mean reward: -2.373 [-100.000,  7.399], mean action: 0.017 [0.000, 1.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.113900
 98538/100000: episode: 1216, duration: 0.490s, episode steps:  52, steps per second: 106, episode reward: -118.644, mean reward: -2.282 [-100.000,  7.418], mean action: 0.115 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.113396
 98631/100000: episode: 1217, duration: 0.957s, episode steps:  93, steps per second:  97, episode reward: -200.624, mean reward: -2.157 [-100.000,  6.320], mean action: 0.183 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.112744
 98691/100000: episode: 1218, duration: 0.570s, episode steps:  60, steps per second: 105, episode reward: 11.218, mean reward:  0.187 [-100.000, 81.320], mean action: 0.200 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.112055
 98751/100000: episode: 1219, duration: 0.567s, episode steps:  60, steps per second: 106, episode reward: -41.594, mean reward: -0.693 [-100.000, 98.562], mean action: 0.267 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.111516
 98805/100000: episode: 1220, duration: 0.506s, episode steps:  54, steps per second: 107, episode reward: -149.544, mean reward: -2.769 [-100.000,  7.471], mean action: 0.056 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.111002
 98885/100000: episode: 1221, duration: 0.745s, episode steps:  80, steps per second: 107, episode reward: -113.825, mean reward: -1.423 [-100.000, 16.980], mean action: 0.125 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.110399
 98962/100000: episode: 1222, duration: 0.702s, episode steps:  77, steps per second: 110, episode reward: -190.161, mean reward: -2.470 [-100.000, 30.597], mean action: 0.221 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.109693
 99019/100000: episode: 1223, duration: 0.587s, episode steps:  57, steps per second:  97, episode reward: -97.710, mean reward: -1.714 [-100.000, 16.356], mean action: 0.140 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.109090
 99105/100000: episode: 1224, duration: 0.802s, episode steps:  86, steps per second: 107, episode reward: -178.260, mean reward: -2.073 [-100.000, 14.479], mean action: 0.198 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.108446
 99192/100000: episode: 1225, duration: 0.783s, episode steps:  87, steps per second: 111, episode reward: -161.922, mean reward: -1.861 [-100.000,  6.121], mean action: 0.092 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.107668
 99269/100000: episode: 1226, duration: 0.742s, episode steps:  77, steps per second: 104, episode reward: -155.870, mean reward: -2.024 [-100.000,  6.440], mean action: 0.065 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.106930
 99362/100000: episode: 1227, duration: 0.865s, episode steps:  93, steps per second: 107, episode reward: -173.473, mean reward: -1.865 [-100.000,  6.311], mean action: 0.054 [0.000, 2.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.106165
 99434/100000: episode: 1228, duration: 0.654s, episode steps:  72, steps per second: 110, episode reward: -170.813, mean reward: -2.372 [-100.000,  5.851], mean action: 0.139 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.105422
 99497/100000: episode: 1229, duration: 0.633s, episode steps:  63, steps per second: 100, episode reward: -129.082, mean reward: -2.049 [-100.000,  8.022], mean action: 0.143 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.104815
 99589/100000: episode: 1230, duration: 0.863s, episode steps:  92, steps per second: 107, episode reward: -153.863, mean reward: -1.672 [-100.000, 16.146], mean action: 0.054 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.104117
 99642/100000: episode: 1231, duration: 0.488s, episode steps:  53, steps per second: 109, episode reward: -118.389, mean reward: -2.234 [-100.000,  6.672], mean action: 0.094 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.103465
 99732/100000: episode: 1232, duration: 0.859s, episode steps:  90, steps per second: 105, episode reward: -127.213, mean reward: -1.413 [-100.000, 15.918], mean action: 0.167 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.102821
 99811/100000: episode: 1233, duration: 0.734s, episode steps:  79, steps per second: 108, episode reward: -135.427, mean reward: -1.714 [-100.000,  7.138], mean action: 0.190 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.102061
 99882/100000: episode: 1234, duration: 0.694s, episode steps:  71, steps per second: 102, episode reward: -144.912, mean reward: -2.041 [-100.000, 11.204], mean action: 0.254 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.101386
 99947/100000: episode: 1235, duration: 0.780s, episode steps:  65, steps per second:  83, episode reward: -129.091, mean reward: -1.986 [-100.000,  6.492], mean action: 0.154 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: 0.100774
