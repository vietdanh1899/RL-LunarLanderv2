Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 300000 steps ...
     70/300000: episode: 1, duration: 0.115s, episode steps:  70, steps per second: 610, episode reward: -273.002, mean reward: -3.900 [-100.000, 119.001], mean action: 1.343 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
    189/300000: episode: 2, duration: 1.125s, episode steps: 119, steps per second: 106, episode reward: -262.873, mean reward: -2.209 [-100.000,  1.496], mean action: 1.555 [0.000, 3.000],  loss: 107.255871, mse: 55.407277, mean_q: -0.049680, mean_eps: 0.999523
    282/300000: episode: 3, duration: 0.593s, episode steps:  93, steps per second: 157, episode reward: -84.070, mean reward: -0.904 [-100.000, 14.699], mean action: 1.398 [0.000, 3.000],  loss: 61.978859, mse: 45.721432, mean_q: -0.301496, mean_eps: 0.999224
    357/300000: episode: 4, duration: 0.490s, episode steps:  75, steps per second: 153, episode reward: -68.719, mean reward: -0.916 [-100.000, 17.692], mean action: 1.547 [0.000, 3.000],  loss: 50.646972, mse: 40.980636, mean_q: 0.289971, mean_eps: 0.998947
    431/300000: episode: 5, duration: 0.496s, episode steps:  74, steps per second: 149, episode reward: -116.992, mean reward: -1.581 [-100.000,  7.146], mean action: 1.500 [0.000, 3.000],  loss: 51.110041, mse: 42.991972, mean_q: 1.607649, mean_eps: 0.998701
    553/300000: episode: 6, duration: 0.817s, episode steps: 122, steps per second: 149, episode reward: -33.457, mean reward: -0.274 [-100.000, 92.076], mean action: 1.492 [0.000, 3.000],  loss: 36.328497, mse: 41.265115, mean_q: 1.846919, mean_eps: 0.998378
    630/300000: episode: 7, duration: 0.561s, episode steps:  77, steps per second: 137, episode reward: -111.062, mean reward: -1.442 [-100.000, 17.372], mean action: 1.532 [0.000, 3.000],  loss: 44.104563, mse: 41.442899, mean_q: 1.947991, mean_eps: 0.998050
    736/300000: episode: 8, duration: 0.754s, episode steps: 106, steps per second: 141, episode reward: -191.526, mean reward: -1.807 [-100.000,  4.002], mean action: 1.594 [0.000, 3.000],  loss: 47.976459, mse: 44.013828, mean_q: 1.655128, mean_eps: 0.997748
    812/300000: episode: 9, duration: 0.470s, episode steps:  76, steps per second: 162, episode reward: -79.223, mean reward: -1.042 [-100.000, 11.231], mean action: 1.513 [0.000, 3.000],  loss: 33.945567, mse: 36.480346, mean_q: 1.594641, mean_eps: 0.997447
    924/300000: episode: 10, duration: 0.694s, episode steps: 112, steps per second: 161, episode reward: -74.588, mean reward: -0.666 [-100.000, 11.070], mean action: 1.491 [0.000, 3.000],  loss: 32.454969, mse: 46.244303, mean_q: 2.010332, mean_eps: 0.997137
   1031/300000: episode: 11, duration: 0.686s, episode steps: 107, steps per second: 156, episode reward: -399.426, mean reward: -3.733 [-100.000,  1.645], mean action: 1.308 [0.000, 3.000],  loss: 26.528327, mse: 48.062023, mean_q: 2.334801, mean_eps: 0.996776
   1142/300000: episode: 12, duration: 0.758s, episode steps: 111, steps per second: 146, episode reward:  6.466, mean reward:  0.058 [-100.000, 101.510], mean action: 1.369 [0.000, 3.000],  loss: 35.945083, mse: 62.803402, mean_q: 3.667007, mean_eps: 0.996416
   1240/300000: episode: 13, duration: 0.760s, episode steps:  98, steps per second: 129, episode reward: -546.181, mean reward: -5.573 [-100.000,  0.654], mean action: 1.684 [0.000, 3.000],  loss: 35.018495, mse: 62.661451, mean_q: 3.738449, mean_eps: 0.996071
   1344/300000: episode: 14, duration: 0.774s, episode steps: 104, steps per second: 134, episode reward: -305.537, mean reward: -2.938 [-100.000,  0.995], mean action: 1.375 [0.000, 3.000],  loss: 37.004409, mse: 75.394066, mean_q: 3.483150, mean_eps: 0.995738
   1448/300000: episode: 15, duration: 0.838s, episode steps: 104, steps per second: 124, episode reward: -435.371, mean reward: -4.186 [-100.000,  1.078], mean action: 1.337 [0.000, 3.000],  loss: 38.185348, mse: 75.064112, mean_q: 3.441407, mean_eps: 0.995395
   1580/300000: episode: 16, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -372.267, mean reward: -2.820 [-100.000,  2.408], mean action: 1.508 [0.000, 3.000],  loss: 35.960886, mse: 67.306558, mean_q: 3.112643, mean_eps: 0.995005
   1697/300000: episode: 17, duration: 0.965s, episode steps: 117, steps per second: 121, episode reward: -399.917, mean reward: -3.418 [-100.000,  3.094], mean action: 1.393 [0.000, 3.000],  loss: 33.280033, mse: 60.758983, mean_q: 2.962264, mean_eps: 0.994595
   1805/300000: episode: 18, duration: 0.990s, episode steps: 108, steps per second: 109, episode reward: -190.476, mean reward: -1.764 [-100.000, 37.037], mean action: 1.370 [0.000, 3.000],  loss: 33.867756, mse: 57.277855, mean_q: 2.946643, mean_eps: 0.994223
   1890/300000: episode: 19, duration: 0.793s, episode steps:  85, steps per second: 107, episode reward: -89.189, mean reward: -1.049 [-100.000,  6.652], mean action: 1.365 [0.000, 3.000],  loss: 27.425347, mse: 69.779853, mean_q: 2.953438, mean_eps: 0.993905
   1977/300000: episode: 20, duration: 0.577s, episode steps:  87, steps per second: 151, episode reward: -184.969, mean reward: -2.126 [-100.000,  8.423], mean action: 1.448 [0.000, 3.000],  loss: 42.083859, mse: 72.386058, mean_q: 2.937125, mean_eps: 0.993621
   2046/300000: episode: 21, duration: 0.530s, episode steps:  69, steps per second: 130, episode reward: -114.787, mean reward: -1.664 [-100.000,  7.867], mean action: 1.696 [0.000, 3.000],  loss: 39.968238, mse: 64.058501, mean_q: 2.927950, mean_eps: 0.993364
   2144/300000: episode: 22, duration: 0.680s, episode steps:  98, steps per second: 144, episode reward: -124.702, mean reward: -1.272 [-100.000,  5.687], mean action: 1.490 [0.000, 3.000],  loss: 40.013792, mse: 74.656788, mean_q: 3.283983, mean_eps: 0.993088
   2242/300000: episode: 23, duration: 0.671s, episode steps:  98, steps per second: 146, episode reward: -266.241, mean reward: -2.717 [-100.000,  0.176], mean action: 1.480 [0.000, 3.000],  loss: 42.859617, mse: 77.857202, mean_q: 3.197423, mean_eps: 0.992765
   2366/300000: episode: 24, duration: 0.843s, episode steps: 124, steps per second: 147, episode reward: -10.134, mean reward: -0.082 [-100.000, 102.732], mean action: 1.532 [0.000, 3.000],  loss: 41.499656, mse: 76.548611, mean_q: 2.931410, mean_eps: 0.992398
   2428/300000: episode: 25, duration: 0.408s, episode steps:  62, steps per second: 152, episode reward: -71.376, mean reward: -1.151 [-100.000, 22.832], mean action: 1.452 [0.000, 3.000],  loss: 39.117540, mse: 67.067245, mean_q: 3.232542, mean_eps: 0.992092
   2532/300000: episode: 26, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -377.067, mean reward: -3.626 [-100.000, 27.418], mean action: 1.404 [0.000, 3.000],  loss: 43.902315, mse: 73.172090, mean_q: 3.549231, mean_eps: 0.991818
   2605/300000: episode: 27, duration: 0.479s, episode steps:  73, steps per second: 152, episode reward: -196.034, mean reward: -2.685 [-100.000,  7.111], mean action: 1.726 [0.000, 3.000],  loss: 41.764138, mse: 62.257939, mean_q: 3.138300, mean_eps: 0.991526
   2683/300000: episode: 28, duration: 0.546s, episode steps:  78, steps per second: 143, episode reward: -115.830, mean reward: -1.485 [-100.000, 14.223], mean action: 1.628 [0.000, 3.000],  loss: 42.722606, mse: 68.373489, mean_q: 3.284560, mean_eps: 0.991276
   2742/300000: episode: 29, duration: 0.397s, episode steps:  59, steps per second: 148, episode reward: -99.946, mean reward: -1.694 [-100.000,  9.004], mean action: 1.254 [0.000, 3.000],  loss: 59.336706, mse: 83.153728, mean_q: 3.475296, mean_eps: 0.991050
   2817/300000: episode: 30, duration: 0.552s, episode steps:  75, steps per second: 136, episode reward: -211.794, mean reward: -2.824 [-100.000,  6.159], mean action: 1.440 [0.000, 3.000],  loss: 42.849037, mse: 73.614584, mean_q: 3.978477, mean_eps: 0.990829
   2927/300000: episode: 31, duration: 0.691s, episode steps: 110, steps per second: 159, episode reward: -39.000, mean reward: -0.355 [-100.000, 106.596], mean action: 1.309 [0.000, 3.000],  loss: 61.719305, mse: 77.090244, mean_q: 3.656764, mean_eps: 0.990524
   3009/300000: episode: 32, duration: 0.549s, episode steps:  82, steps per second: 149, episode reward: -103.932, mean reward: -1.267 [-100.000,  7.357], mean action: 1.390 [0.000, 3.000],  loss: 52.087526, mse: 65.792944, mean_q: 3.309555, mean_eps: 0.990207
   3099/300000: episode: 33, duration: 0.605s, episode steps:  90, steps per second: 149, episode reward: -82.686, mean reward: -0.919 [-100.000, 11.747], mean action: 1.656 [0.000, 3.000],  loss: 44.918656, mse: 81.985037, mean_q: 3.473177, mean_eps: 0.989923
   3227/300000: episode: 34, duration: 0.844s, episode steps: 128, steps per second: 152, episode reward: -270.827, mean reward: -2.116 [-100.000,  1.331], mean action: 1.469 [0.000, 3.000],  loss: 50.599695, mse: 96.446610, mean_q: 3.410463, mean_eps: 0.989564
   3286/300000: episode: 35, duration: 0.393s, episode steps:  59, steps per second: 150, episode reward: -75.411, mean reward: -1.278 [-100.000, 17.444], mean action: 1.492 [0.000, 3.000],  loss: 51.215039, mse: 90.960738, mean_q: 3.653230, mean_eps: 0.989255
   3411/300000: episode: 36, duration: 0.794s, episode steps: 125, steps per second: 158, episode reward: -328.214, mean reward: -2.626 [-100.000,  4.871], mean action: 1.488 [0.000, 3.000],  loss: 49.854052, mse: 101.896423, mean_q: 3.169632, mean_eps: 0.988952
   3496/300000: episode: 37, duration: 0.578s, episode steps:  85, steps per second: 147, episode reward: -70.292, mean reward: -0.827 [-100.000, 12.256], mean action: 1.529 [0.000, 3.000],  loss: 59.124960, mse: 97.869101, mean_q: 3.088033, mean_eps: 0.988605
   3563/300000: episode: 38, duration: 0.449s, episode steps:  67, steps per second: 149, episode reward: -49.751, mean reward: -0.743 [-100.000,  8.810], mean action: 1.522 [0.000, 3.000],  loss: 60.908207, mse: 99.456163, mean_q: 2.870720, mean_eps: 0.988354
   3633/300000: episode: 39, duration: 0.481s, episode steps:  70, steps per second: 146, episode reward: -113.926, mean reward: -1.628 [-100.000, 15.724], mean action: 1.314 [0.000, 3.000],  loss: 48.442975, mse: 93.017355, mean_q: 3.269411, mean_eps: 0.988128
   3741/300000: episode: 40, duration: 0.798s, episode steps: 108, steps per second: 135, episode reward: -129.487, mean reward: -1.199 [-100.000,  9.838], mean action: 1.389 [0.000, 3.000],  loss: 45.443531, mse: 93.897490, mean_q: 3.630635, mean_eps: 0.987835
   3832/300000: episode: 41, duration: 0.667s, episode steps:  91, steps per second: 137, episode reward: -116.733, mean reward: -1.283 [-100.000,  6.343], mean action: 1.352 [0.000, 3.000],  loss: 50.400048, mse: 100.343922, mean_q: 3.771245, mean_eps: 0.987506
   3938/300000: episode: 42, duration: 0.748s, episode steps: 106, steps per second: 142, episode reward: -225.973, mean reward: -2.132 [-100.000,  1.148], mean action: 1.594 [0.000, 3.000],  loss: 39.818301, mse: 91.364468, mean_q: 3.455912, mean_eps: 0.987181
   4061/300000: episode: 43, duration: 1.004s, episode steps: 123, steps per second: 122, episode reward: -225.078, mean reward: -1.830 [-100.000,  7.127], mean action: 1.496 [0.000, 3.000],  loss: 41.681671, mse: 114.922231, mean_q: 3.744602, mean_eps: 0.986803
   4157/300000: episode: 44, duration: 0.853s, episode steps:  96, steps per second: 112, episode reward: 12.127, mean reward:  0.126 [-100.000, 84.683], mean action: 1.896 [0.000, 3.000],  loss: 39.232535, mse: 140.210687, mean_q: 3.715799, mean_eps: 0.986442
   4261/300000: episode: 45, duration: 0.829s, episode steps: 104, steps per second: 126, episode reward: -137.433, mean reward: -1.321 [-100.000,  9.778], mean action: 1.596 [0.000, 3.000],  loss: 36.955659, mse: 125.998450, mean_q: 3.788964, mean_eps: 0.986112
   4320/300000: episode: 46, duration: 0.531s, episode steps:  59, steps per second: 111, episode reward: -182.596, mean reward: -3.095 [-100.000,  6.367], mean action: 1.610 [0.000, 3.000],  loss: 35.451220, mse: 133.084659, mean_q: 3.628198, mean_eps: 0.985843
   4404/300000: episode: 47, duration: 0.680s, episode steps:  84, steps per second: 124, episode reward: -93.129, mean reward: -1.109 [-100.000, 17.375], mean action: 1.500 [0.000, 3.000],  loss: 32.127679, mse: 112.366641, mean_q: 4.207437, mean_eps: 0.985607
   4476/300000: episode: 48, duration: 0.526s, episode steps:  72, steps per second: 137, episode reward: -112.172, mean reward: -1.558 [-100.000, 24.957], mean action: 1.472 [0.000, 3.000],  loss: 37.932085, mse: 126.299226, mean_q: 3.865207, mean_eps: 0.985350
   4580/300000: episode: 49, duration: 0.838s, episode steps: 104, steps per second: 124, episode reward: -35.081, mean reward: -0.337 [-100.000, 64.973], mean action: 1.413 [0.000, 3.000],  loss: 33.831646, mse: 132.642244, mean_q: 3.784873, mean_eps: 0.985059
   4658/300000: episode: 50, duration: 0.678s, episode steps:  78, steps per second: 115, episode reward: -142.009, mean reward: -1.821 [-100.000, 22.459], mean action: 1.346 [0.000, 3.000],  loss: 33.394601, mse: 137.221173, mean_q: 4.005025, mean_eps: 0.984759
   4751/300000: episode: 51, duration: 0.698s, episode steps:  93, steps per second: 133, episode reward: -404.604, mean reward: -4.351 [-100.000,  0.941], mean action: 1.312 [0.000, 3.000],  loss: 42.594727, mse: 148.866566, mean_q: 4.077105, mean_eps: 0.984477
   4834/300000: episode: 52, duration: 0.574s, episode steps:  83, steps per second: 145, episode reward: -141.950, mean reward: -1.710 [-100.000, 10.229], mean action: 1.373 [0.000, 3.000],  loss: 28.133644, mse: 144.026385, mean_q: 3.880991, mean_eps: 0.984186
   4930/300000: episode: 53, duration: 0.739s, episode steps:  96, steps per second: 130, episode reward: -293.224, mean reward: -3.054 [-100.000,  0.658], mean action: 1.771 [0.000, 3.000],  loss: 32.119441, mse: 137.797280, mean_q: 3.637524, mean_eps: 0.983891
   5001/300000: episode: 54, duration: 0.510s, episode steps:  71, steps per second: 139, episode reward: -141.315, mean reward: -1.990 [-100.000, 21.114], mean action: 1.535 [0.000, 3.000],  loss: 39.976469, mse: 130.456581, mean_q: 3.937334, mean_eps: 0.983616
   5079/300000: episode: 55, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -174.295, mean reward: -2.235 [-100.000, 19.895], mean action: 1.577 [0.000, 3.000],  loss: 43.460513, mse: 174.444583, mean_q: 3.687666, mean_eps: 0.983370
   5160/300000: episode: 56, duration: 0.613s, episode steps:  81, steps per second: 132, episode reward: -185.900, mean reward: -2.295 [-100.000, 21.617], mean action: 1.605 [0.000, 3.000],  loss: 42.819521, mse: 160.100129, mean_q: 4.195185, mean_eps: 0.983107
   5250/300000: episode: 57, duration: 0.647s, episode steps:  90, steps per second: 139, episode reward: -82.412, mean reward: -0.916 [-100.000, 21.233], mean action: 1.578 [0.000, 3.000],  loss: 36.469833, mse: 159.095103, mean_q: 4.679177, mean_eps: 0.982825
   5367/300000: episode: 58, duration: 0.802s, episode steps: 117, steps per second: 146, episode reward: -176.117, mean reward: -1.505 [-100.000,  3.505], mean action: 1.530 [0.000, 3.000],  loss: 39.557488, mse: 158.041699, mean_q: 4.497619, mean_eps: 0.982484
   5457/300000: episode: 59, duration: 0.653s, episode steps:  90, steps per second: 138, episode reward: -337.185, mean reward: -3.746 [-100.000,  4.621], mean action: 1.633 [0.000, 3.000],  loss: 31.583050, mse: 153.916538, mean_q: 4.623314, mean_eps: 0.982142
   5595/300000: episode: 60, duration: 1.005s, episode steps: 138, steps per second: 137, episode reward: -164.271, mean reward: -1.190 [-100.000,  7.173], mean action: 1.674 [0.000, 3.000],  loss: 37.114035, mse: 185.555790, mean_q: 4.073972, mean_eps: 0.981766
   5698/300000: episode: 61, duration: 0.828s, episode steps: 103, steps per second: 124, episode reward: -85.107, mean reward: -0.826 [-100.000, 15.862], mean action: 1.456 [0.000, 3.000],  loss: 32.959564, mse: 168.644938, mean_q: 4.543690, mean_eps: 0.981368
   5829/300000: episode: 62, duration: 1.011s, episode steps: 131, steps per second: 130, episode reward: -55.227, mean reward: -0.422 [-100.000, 83.183], mean action: 1.565 [0.000, 3.000],  loss: 31.193692, mse: 179.312444, mean_q: 4.192513, mean_eps: 0.980982
   5923/300000: episode: 63, duration: 0.664s, episode steps:  94, steps per second: 142, episode reward: -89.153, mean reward: -0.948 [-100.000,  7.876], mean action: 1.702 [0.000, 3.000],  loss: 33.429128, mse: 171.428124, mean_q: 4.959077, mean_eps: 0.980611
   5991/300000: episode: 64, duration: 0.493s, episode steps:  68, steps per second: 138, episode reward: -73.542, mean reward: -1.082 [-100.000, 12.644], mean action: 1.544 [0.000, 3.000],  loss: 37.043920, mse: 166.071416, mean_q: 4.415158, mean_eps: 0.980344
   6066/300000: episode: 65, duration: 0.550s, episode steps:  75, steps per second: 136, episode reward: -113.690, mean reward: -1.516 [-100.000, 15.236], mean action: 1.507 [0.000, 3.000],  loss: 36.890864, mse: 195.472187, mean_q: 6.350273, mean_eps: 0.980108
   6145/300000: episode: 66, duration: 0.672s, episode steps:  79, steps per second: 118, episode reward: -271.139, mean reward: -3.432 [-100.000, 101.620], mean action: 1.354 [0.000, 3.000],  loss: 30.442866, mse: 190.346252, mean_q: 6.117841, mean_eps: 0.979854
   6219/300000: episode: 67, duration: 0.612s, episode steps:  74, steps per second: 121, episode reward: -78.854, mean reward: -1.066 [-100.000, 10.018], mean action: 1.554 [0.000, 3.000],  loss: 33.293613, mse: 210.899803, mean_q: 6.238486, mean_eps: 0.979601
   6335/300000: episode: 68, duration: 1.000s, episode steps: 116, steps per second: 116, episode reward: -230.506, mean reward: -1.987 [-100.000,  2.580], mean action: 1.250 [0.000, 3.000],  loss: 38.250634, mse: 208.572168, mean_q: 5.912329, mean_eps: 0.979288
   6420/300000: episode: 69, duration: 0.662s, episode steps:  85, steps per second: 128, episode reward: -200.169, mean reward: -2.355 [-100.000,  6.860], mean action: 1.471 [0.000, 3.000],  loss: 30.961308, mse: 197.459756, mean_q: 6.336907, mean_eps: 0.978956
   6527/300000: episode: 70, duration: 0.797s, episode steps: 107, steps per second: 134, episode reward: -296.792, mean reward: -2.774 [-100.000,  1.141], mean action: 1.561 [0.000, 3.000],  loss: 41.079250, mse: 199.555525, mean_q: 5.921620, mean_eps: 0.978639
   6649/300000: episode: 71, duration: 1.053s, episode steps: 122, steps per second: 116, episode reward: -95.128, mean reward: -0.780 [-100.000, 16.784], mean action: 1.377 [0.000, 3.000],  loss: 36.528670, mse: 197.743868, mean_q: 5.705303, mean_eps: 0.978261
   6723/300000: episode: 72, duration: 0.569s, episode steps:  74, steps per second: 130, episode reward: -106.802, mean reward: -1.443 [-100.000, 11.467], mean action: 1.486 [0.000, 3.000],  loss: 36.945367, mse: 202.395500, mean_q: 6.262800, mean_eps: 0.977938
   6846/300000: episode: 73, duration: 0.913s, episode steps: 123, steps per second: 135, episode reward: -97.988, mean reward: -0.797 [-100.000,  9.160], mean action: 1.528 [0.000, 3.000],  loss: 36.034129, mse: 217.187446, mean_q: 6.105995, mean_eps: 0.977613
   6923/300000: episode: 74, duration: 0.638s, episode steps:  77, steps per second: 121, episode reward: -110.350, mean reward: -1.433 [-100.000, 39.809], mean action: 1.571 [0.000, 3.000],  loss: 37.244661, mse: 236.558354, mean_q: 5.760903, mean_eps: 0.977283
   6990/300000: episode: 75, duration: 0.501s, episode steps:  67, steps per second: 134, episode reward: -37.055, mean reward: -0.553 [-100.000, 15.294], mean action: 1.612 [0.000, 3.000],  loss: 27.245172, mse: 206.868805, mean_q: 5.922623, mean_eps: 0.977045
   7070/300000: episode: 76, duration: 0.651s, episode steps:  80, steps per second: 123, episode reward: -146.394, mean reward: -1.830 [-100.000,  9.703], mean action: 1.525 [0.000, 3.000],  loss: 37.811487, mse: 258.766116, mean_q: 6.974277, mean_eps: 0.976803
   7133/300000: episode: 77, duration: 0.490s, episode steps:  63, steps per second: 128, episode reward: -87.260, mean reward: -1.385 [-100.000,  8.581], mean action: 1.476 [0.000, 3.000],  loss: 39.602538, mse: 257.065939, mean_q: 6.871410, mean_eps: 0.976567
   7220/300000: episode: 78, duration: 0.659s, episode steps:  87, steps per second: 132, episode reward: -413.236, mean reward: -4.750 [-100.000,  5.456], mean action: 1.621 [0.000, 3.000],  loss: 29.965402, mse: 256.421556, mean_q: 6.741557, mean_eps: 0.976319
   7292/300000: episode: 79, duration: 0.509s, episode steps:  72, steps per second: 142, episode reward: -72.941, mean reward: -1.013 [-100.000, 17.652], mean action: 1.514 [0.000, 3.000],  loss: 37.413003, mse: 265.946642, mean_q: 6.620202, mean_eps: 0.976057
   7364/300000: episode: 80, duration: 0.513s, episode steps:  72, steps per second: 140, episode reward: -78.010, mean reward: -1.083 [-100.000,  6.493], mean action: 1.583 [0.000, 3.000],  loss: 34.876308, mse: 254.495076, mean_q: 6.965693, mean_eps: 0.975819
   7477/300000: episode: 81, duration: 0.860s, episode steps: 113, steps per second: 131, episode reward: -247.069, mean reward: -2.186 [-100.000, 123.483], mean action: 1.681 [0.000, 3.000],  loss: 39.475867, mse: 257.058272, mean_q: 6.979686, mean_eps: 0.975514
   7553/300000: episode: 82, duration: 0.555s, episode steps:  76, steps per second: 137, episode reward: -107.620, mean reward: -1.416 [-100.000, 14.418], mean action: 1.421 [0.000, 3.000],  loss: 32.304137, mse: 274.009221, mean_q: 6.745055, mean_eps: 0.975202
   7619/300000: episode: 83, duration: 0.473s, episode steps:  66, steps per second: 140, episode reward: -70.120, mean reward: -1.062 [-100.000, 15.055], mean action: 1.424 [0.000, 3.000],  loss: 40.864780, mse: 271.675983, mean_q: 7.335987, mean_eps: 0.974968
   7702/300000: episode: 84, duration: 0.568s, episode steps:  83, steps per second: 146, episode reward: -365.506, mean reward: -4.404 [-100.000, -0.218], mean action: 1.530 [0.000, 3.000],  loss: 41.435294, mse: 303.006677, mean_q: 6.864237, mean_eps: 0.974722
   7798/300000: episode: 85, duration: 0.735s, episode steps:  96, steps per second: 131, episode reward: -139.202, mean reward: -1.450 [-100.000,  8.500], mean action: 1.469 [0.000, 3.000],  loss: 37.190135, mse: 267.098778, mean_q: 6.945329, mean_eps: 0.974427
   7912/300000: episode: 86, duration: 0.894s, episode steps: 114, steps per second: 127, episode reward: -265.760, mean reward: -2.331 [-100.000,  5.422], mean action: 1.491 [0.000, 3.000],  loss: 27.645659, mse: 260.152796, mean_q: 6.812682, mean_eps: 0.974080
   8030/300000: episode: 87, duration: 0.982s, episode steps: 118, steps per second: 120, episode reward: -325.615, mean reward: -2.759 [-100.000, 107.936], mean action: 1.441 [0.000, 3.000],  loss: 32.432625, mse: 257.090389, mean_q: 7.433065, mean_eps: 0.973697
   8129/300000: episode: 88, duration: 0.646s, episode steps:  99, steps per second: 153, episode reward: -202.689, mean reward: -2.047 [-100.000, 10.332], mean action: 1.646 [0.000, 3.000],  loss: 39.302686, mse: 341.221312, mean_q: 8.386655, mean_eps: 0.973339
   8237/300000: episode: 89, duration: 0.706s, episode steps: 108, steps per second: 153, episode reward: -91.477, mean reward: -0.847 [-100.000, 14.032], mean action: 1.639 [0.000, 3.000],  loss: 35.137733, mse: 341.158608, mean_q: 7.931065, mean_eps: 0.972998
   8309/300000: episode: 90, duration: 0.466s, episode steps:  72, steps per second: 154, episode reward: -108.892, mean reward: -1.512 [-100.000,  9.223], mean action: 1.389 [0.000, 3.000],  loss: 29.547027, mse: 340.538850, mean_q: 8.106654, mean_eps: 0.972701
   8386/300000: episode: 91, duration: 0.508s, episode steps:  77, steps per second: 152, episode reward: -80.594, mean reward: -1.047 [-100.000,  7.416], mean action: 1.442 [0.000, 3.000],  loss: 22.752963, mse: 308.628551, mean_q: 8.784645, mean_eps: 0.972455
   8444/300000: episode: 92, duration: 0.358s, episode steps:  58, steps per second: 162, episode reward: -153.311, mean reward: -2.643 [-100.000,  5.755], mean action: 1.690 [0.000, 3.000],  loss: 23.760325, mse: 328.243355, mean_q: 8.167277, mean_eps: 0.972232
   8518/300000: episode: 93, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -132.075, mean reward: -1.785 [-100.000,  9.736], mean action: 1.446 [0.000, 3.000],  loss: 34.635132, mse: 317.480426, mean_q: 9.068905, mean_eps: 0.972014
   8631/300000: episode: 94, duration: 0.690s, episode steps: 113, steps per second: 164, episode reward: -155.117, mean reward: -1.373 [-100.000, 22.566], mean action: 1.531 [0.000, 3.000],  loss: 26.956356, mse: 325.723054, mean_q: 8.873167, mean_eps: 0.971706
   8731/300000: episode: 95, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -114.918, mean reward: -1.149 [-100.000,  6.748], mean action: 1.630 [0.000, 3.000],  loss: 31.788837, mse: 325.502449, mean_q: 8.831308, mean_eps: 0.971354
   8829/300000: episode: 96, duration: 0.621s, episode steps:  98, steps per second: 158, episode reward: -25.880, mean reward: -0.264 [-100.000, 93.566], mean action: 1.469 [0.000, 3.000],  loss: 31.934756, mse: 340.574292, mean_q: 8.719212, mean_eps: 0.971028
   8931/300000: episode: 97, duration: 0.655s, episode steps: 102, steps per second: 156, episode reward: -53.912, mean reward: -0.529 [-100.000, 21.832], mean action: 1.588 [0.000, 3.000],  loss: 26.453124, mse: 347.217499, mean_q: 8.650089, mean_eps: 0.970698
   9013/300000: episode: 98, duration: 0.616s, episode steps:  82, steps per second: 133, episode reward: -357.612, mean reward: -4.361 [-100.000, 68.175], mean action: 1.427 [0.000, 3.000],  loss: 39.352605, mse: 326.775199, mean_q: 9.181507, mean_eps: 0.970394
   9086/300000: episode: 99, duration: 0.562s, episode steps:  73, steps per second: 130, episode reward: -93.010, mean reward: -1.274 [-100.000,  9.410], mean action: 1.219 [0.000, 3.000],  loss: 35.911038, mse: 397.604328, mean_q: 10.133486, mean_eps: 0.970138
   9150/300000: episode: 100, duration: 0.549s, episode steps:  64, steps per second: 117, episode reward: -76.968, mean reward: -1.203 [-100.000,  7.653], mean action: 1.703 [0.000, 3.000],  loss: 27.859254, mse: 362.447769, mean_q: 10.093170, mean_eps: 0.969912
   9232/300000: episode: 101, duration: 0.610s, episode steps:  82, steps per second: 134, episode reward: -118.990, mean reward: -1.451 [-100.000, 29.043], mean action: 1.634 [0.000, 3.000],  loss: 38.822414, mse: 421.543256, mean_q: 9.648442, mean_eps: 0.969671
   9298/300000: episode: 102, duration: 0.577s, episode steps:  66, steps per second: 114, episode reward: -96.917, mean reward: -1.468 [-100.000,  7.092], mean action: 1.576 [0.000, 3.000],  loss: 34.348468, mse: 403.705571, mean_q: 10.484897, mean_eps: 0.969427
   9417/300000: episode: 103, duration: 0.902s, episode steps: 119, steps per second: 132, episode reward: -236.926, mean reward: -1.991 [-100.000,  1.080], mean action: 1.622 [0.000, 3.000],  loss: 36.684994, mse: 392.823290, mean_q: 10.387005, mean_eps: 0.969122
   9524/300000: episode: 104, duration: 0.775s, episode steps: 107, steps per second: 138, episode reward: -147.742, mean reward: -1.381 [-100.000, 24.619], mean action: 1.439 [0.000, 3.000],  loss: 34.946379, mse: 381.129529, mean_q: 10.303164, mean_eps: 0.968749
   9607/300000: episode: 105, duration: 0.702s, episode steps:  83, steps per second: 118, episode reward: -84.573, mean reward: -1.019 [-100.000, 15.854], mean action: 1.675 [0.000, 3.000],  loss: 33.640065, mse: 389.110639, mean_q: 9.815502, mean_eps: 0.968435
   9728/300000: episode: 106, duration: 0.932s, episode steps: 121, steps per second: 130, episode reward: -120.206, mean reward: -0.993 [-100.000,  6.757], mean action: 1.603 [0.000, 3.000],  loss: 30.809791, mse: 409.185543, mean_q: 9.645278, mean_eps: 0.968099
   9840/300000: episode: 107, duration: 0.930s, episode steps: 112, steps per second: 120, episode reward: -259.613, mean reward: -2.318 [-100.000,  8.274], mean action: 1.500 [0.000, 3.000],  loss: 30.552245, mse: 387.322008, mean_q: 9.915242, mean_eps: 0.967714
   9943/300000: episode: 108, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -228.811, mean reward: -2.221 [-100.000,  4.525], mean action: 1.553 [0.000, 3.000],  loss: 34.540027, mse: 378.961273, mean_q: 9.945582, mean_eps: 0.967360
  10010/300000: episode: 109, duration: 0.475s, episode steps:  67, steps per second: 141, episode reward: -244.411, mean reward: -3.648 [-100.000,  4.192], mean action: 1.388 [0.000, 3.000],  loss: 23.442963, mse: 387.849027, mean_q: 10.059325, mean_eps: 0.967079
  10109/300000: episode: 110, duration: 0.667s, episode steps:  99, steps per second: 148, episode reward: -4.191, mean reward: -0.042 [-100.000, 98.269], mean action: 1.545 [0.000, 3.000],  loss: 34.641424, mse: 424.697631, mean_q: 10.815628, mean_eps: 0.966805
  10237/300000: episode: 111, duration: 0.888s, episode steps: 128, steps per second: 144, episode reward: -237.804, mean reward: -1.858 [-100.000,  1.689], mean action: 1.484 [0.000, 3.000],  loss: 25.733783, mse: 440.660023, mean_q: 10.624127, mean_eps: 0.966431
  10303/300000: episode: 112, duration: 0.455s, episode steps:  66, steps per second: 145, episode reward: -141.318, mean reward: -2.141 [-100.000,  5.502], mean action: 1.576 [0.000, 3.000],  loss: 27.680823, mse: 431.785056, mean_q: 10.642716, mean_eps: 0.966111
  10390/300000: episode: 113, duration: 0.597s, episode steps:  87, steps per second: 146, episode reward: -270.870, mean reward: -3.113 [-100.000, 32.347], mean action: 1.598 [0.000, 3.000],  loss: 29.172773, mse: 440.482800, mean_q: 11.223195, mean_eps: 0.965858
  10470/300000: episode: 114, duration: 0.606s, episode steps:  80, steps per second: 132, episode reward: -78.939, mean reward: -0.987 [-100.000,  5.697], mean action: 1.650 [0.000, 3.000],  loss: 28.587327, mse: 414.334072, mean_q: 10.706160, mean_eps: 0.965583
  10531/300000: episode: 115, duration: 0.438s, episode steps:  61, steps per second: 139, episode reward: -236.298, mean reward: -3.874 [-100.000, 27.308], mean action: 1.934 [0.000, 3.000],  loss: 24.821408, mse: 451.262584, mean_q: 10.661792, mean_eps: 0.965350
  10633/300000: episode: 116, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -135.671, mean reward: -1.330 [-100.000, 21.157], mean action: 1.608 [0.000, 3.000],  loss: 26.576091, mse: 425.324667, mean_q: 11.049277, mean_eps: 0.965081
  10730/300000: episode: 117, duration: 0.795s, episode steps:  97, steps per second: 122, episode reward: -130.917, mean reward: -1.350 [-100.000,  8.780], mean action: 1.309 [0.000, 3.000],  loss: 29.278604, mse: 450.570529, mean_q: 10.547065, mean_eps: 0.964753
  10803/300000: episode: 118, duration: 0.615s, episode steps:  73, steps per second: 119, episode reward: -107.314, mean reward: -1.470 [-100.000, 18.989], mean action: 1.740 [0.000, 3.000],  loss: 27.989814, mse: 481.349208, mean_q: 10.805424, mean_eps: 0.964472
  10940/300000: episode: 119, duration: 1.063s, episode steps: 137, steps per second: 129, episode reward: -186.175, mean reward: -1.359 [-100.000,  8.531], mean action: 1.562 [0.000, 3.000],  loss: 26.899902, mse: 441.597859, mean_q: 10.682104, mean_eps: 0.964126
  11052/300000: episode: 120, duration: 0.862s, episode steps: 112, steps per second: 130, episode reward: -173.515, mean reward: -1.549 [-100.000, 13.144], mean action: 1.616 [0.000, 3.000],  loss: 25.963349, mse: 476.968079, mean_q: 10.784888, mean_eps: 0.963715
  11151/300000: episode: 121, duration: 0.709s, episode steps:  99, steps per second: 140, episode reward: -163.857, mean reward: -1.655 [-100.000,  4.534], mean action: 1.404 [0.000, 3.000],  loss: 30.565354, mse: 511.896197, mean_q: 11.410867, mean_eps: 0.963367
  11271/300000: episode: 122, duration: 0.902s, episode steps: 120, steps per second: 133, episode reward: -179.114, mean reward: -1.493 [-100.000, 49.633], mean action: 1.617 [0.000, 3.000],  loss: 26.677964, mse: 482.996305, mean_q: 11.523999, mean_eps: 0.963005
  11380/300000: episode: 123, duration: 0.840s, episode steps: 109, steps per second: 130, episode reward: -137.458, mean reward: -1.261 [-100.000,  9.646], mean action: 1.431 [0.000, 3.000],  loss: 30.809221, mse: 503.886968, mean_q: 11.347097, mean_eps: 0.962628
  11463/300000: episode: 124, duration: 0.692s, episode steps:  83, steps per second: 120, episode reward: -118.420, mean reward: -1.427 [-100.000, 11.807], mean action: 1.554 [0.000, 3.000],  loss: 31.379351, mse: 489.224913, mean_q: 11.563008, mean_eps: 0.962311
  11560/300000: episode: 125, duration: 0.800s, episode steps:  97, steps per second: 121, episode reward: -295.906, mean reward: -3.051 [-100.000, 114.542], mean action: 1.608 [0.000, 3.000],  loss: 25.783090, mse: 452.033852, mean_q: 11.729680, mean_eps: 0.962014
  11660/300000: episode: 126, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: -158.746, mean reward: -1.587 [-100.000,  3.273], mean action: 1.430 [0.000, 3.000],  loss: 28.404945, mse: 488.012214, mean_q: 11.357986, mean_eps: 0.961689
  11727/300000: episode: 127, duration: 0.505s, episode steps:  67, steps per second: 133, episode reward: -57.981, mean reward: -0.865 [-100.000, 12.901], mean action: 1.642 [0.000, 3.000],  loss: 26.461590, mse: 538.054027, mean_q: 10.698926, mean_eps: 0.961413
  11844/300000: episode: 128, duration: 1.086s, episode steps: 117, steps per second: 108, episode reward: -118.459, mean reward: -1.012 [-100.000,  6.487], mean action: 1.496 [0.000, 3.000],  loss: 27.605284, mse: 507.666024, mean_q: 11.313377, mean_eps: 0.961109
  11930/300000: episode: 129, duration: 0.696s, episode steps:  86, steps per second: 124, episode reward: -121.936, mean reward: -1.418 [-100.000,  6.397], mean action: 1.360 [0.000, 3.000],  loss: 25.336736, mse: 484.550749, mean_q: 10.926985, mean_eps: 0.960775
  12009/300000: episode: 130, duration: 0.584s, episode steps:  79, steps per second: 135, episode reward: -135.084, mean reward: -1.710 [-100.000, 12.334], mean action: 1.797 [0.000, 3.000],  loss: 22.834903, mse: 499.137093, mean_q: 11.884661, mean_eps: 0.960502
  12093/300000: episode: 131, duration: 0.579s, episode steps:  84, steps per second: 145, episode reward: -261.611, mean reward: -3.114 [-100.000, 71.617], mean action: 1.429 [0.000, 3.000],  loss: 33.145798, mse: 550.573525, mean_q: 12.152940, mean_eps: 0.960233
  12203/300000: episode: 132, duration: 0.751s, episode steps: 110, steps per second: 146, episode reward: -139.495, mean reward: -1.268 [-100.000,  6.499], mean action: 1.464 [0.000, 3.000],  loss: 35.678643, mse: 532.642731, mean_q: 12.173480, mean_eps: 0.959913
  12271/300000: episode: 133, duration: 0.442s, episode steps:  68, steps per second: 154, episode reward: 41.413, mean reward:  0.609 [-100.000, 110.092], mean action: 1.485 [0.000, 3.000],  loss: 27.901933, mse: 518.301344, mean_q: 12.167756, mean_eps: 0.959620
  12370/300000: episode: 134, duration: 0.655s, episode steps:  99, steps per second: 151, episode reward: -99.031, mean reward: -1.000 [-100.000,  6.568], mean action: 1.505 [0.000, 3.000],  loss: 31.558607, mse: 513.050100, mean_q: 12.899448, mean_eps: 0.959344
  12458/300000: episode: 135, duration: 0.601s, episode steps:  88, steps per second: 146, episode reward: -107.713, mean reward: -1.224 [-100.000, 12.099], mean action: 1.386 [0.000, 3.000],  loss: 27.015028, mse: 544.688022, mean_q: 12.577780, mean_eps: 0.959035
  12544/300000: episode: 136, duration: 0.570s, episode steps:  86, steps per second: 151, episode reward: -362.894, mean reward: -4.220 [-100.000, 85.443], mean action: 1.570 [0.000, 3.000],  loss: 31.647757, mse: 561.139406, mean_q: 12.027655, mean_eps: 0.958748
  12645/300000: episode: 137, duration: 0.662s, episode steps: 101, steps per second: 153, episode reward: -88.289, mean reward: -0.874 [-100.000, 12.724], mean action: 1.663 [0.000, 3.000],  loss: 30.915625, mse: 541.469527, mean_q: 12.672397, mean_eps: 0.958440
  12725/300000: episode: 138, duration: 0.527s, episode steps:  80, steps per second: 152, episode reward: -136.864, mean reward: -1.711 [-100.000, 11.791], mean action: 1.438 [0.000, 3.000],  loss: 30.110660, mse: 530.833535, mean_q: 13.117625, mean_eps: 0.958141
  12794/300000: episode: 139, duration: 0.490s, episode steps:  69, steps per second: 141, episode reward: -86.544, mean reward: -1.254 [-100.000,  7.206], mean action: 1.696 [0.000, 3.000],  loss: 44.087416, mse: 568.059642, mean_q: 12.538928, mean_eps: 0.957895
  12924/300000: episode: 140, duration: 0.827s, episode steps: 130, steps per second: 157, episode reward: -100.260, mean reward: -0.771 [-100.000, 17.735], mean action: 1.492 [0.000, 3.000],  loss: 32.399155, mse: 537.057951, mean_q: 12.713677, mean_eps: 0.957567
  13009/300000: episode: 141, duration: 0.587s, episode steps:  85, steps per second: 145, episode reward: -334.182, mean reward: -3.932 [-100.000,  1.454], mean action: 1.576 [0.000, 3.000],  loss: 31.368720, mse: 512.672630, mean_q: 12.993067, mean_eps: 0.957212
  13129/300000: episode: 142, duration: 0.958s, episode steps: 120, steps per second: 125, episode reward: -137.804, mean reward: -1.148 [-100.000, 16.214], mean action: 1.550 [0.000, 3.000],  loss: 30.922202, mse: 566.313999, mean_q: 12.134361, mean_eps: 0.956874
  13248/300000: episode: 143, duration: 0.872s, episode steps: 119, steps per second: 137, episode reward: -46.674, mean reward: -0.392 [-100.000, 40.726], mean action: 1.529 [0.000, 3.000],  loss: 29.476320, mse: 603.868655, mean_q: 12.096893, mean_eps: 0.956480
  13336/300000: episode: 144, duration: 0.583s, episode steps:  88, steps per second: 151, episode reward: -337.446, mean reward: -3.835 [-100.000, -0.207], mean action: 1.477 [0.000, 3.000],  loss: 25.348201, mse: 587.843770, mean_q: 11.759322, mean_eps: 0.956138
  13436/300000: episode: 145, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -76.849, mean reward: -0.768 [-100.000, 13.268], mean action: 1.510 [0.000, 3.000],  loss: 32.424159, mse: 572.256574, mean_q: 12.037887, mean_eps: 0.955828
  13541/300000: episode: 146, duration: 0.722s, episode steps: 105, steps per second: 145, episode reward: -155.862, mean reward: -1.484 [-100.000, 10.957], mean action: 1.514 [0.000, 3.000],  loss: 30.149330, mse: 574.747884, mean_q: 12.043591, mean_eps: 0.955490
  13619/300000: episode: 147, duration: 0.495s, episode steps:  78, steps per second: 158, episode reward: -124.545, mean reward: -1.597 [-100.000,  8.710], mean action: 1.615 [0.000, 3.000],  loss: 31.951809, mse: 564.083365, mean_q: 12.486808, mean_eps: 0.955188
  13726/300000: episode: 148, duration: 0.718s, episode steps: 107, steps per second: 149, episode reward: -252.028, mean reward: -2.355 [-100.000,  4.338], mean action: 1.505 [0.000, 3.000],  loss: 36.016991, mse: 590.538621, mean_q: 12.304457, mean_eps: 0.954882
  13817/300000: episode: 149, duration: 0.630s, episode steps:  91, steps per second: 145, episode reward: -164.910, mean reward: -1.812 [-100.000, 10.462], mean action: 1.143 [0.000, 3.000],  loss: 27.821863, mse: 598.563509, mean_q: 11.622284, mean_eps: 0.954556
  13883/300000: episode: 150, duration: 0.416s, episode steps:  66, steps per second: 159, episode reward: -86.401, mean reward: -1.309 [-100.000,  9.027], mean action: 1.682 [0.000, 3.000],  loss: 28.491267, mse: 561.059078, mean_q: 12.346266, mean_eps: 0.954297
  13978/300000: episode: 151, duration: 0.612s, episode steps:  95, steps per second: 155, episode reward: -211.125, mean reward: -2.222 [-100.000,  6.370], mean action: 1.558 [0.000, 3.000],  loss: 23.925392, mse: 590.705561, mean_q: 11.740324, mean_eps: 0.954031
  14050/300000: episode: 152, duration: 0.495s, episode steps:  72, steps per second: 146, episode reward: -38.473, mean reward: -0.534 [-100.000, 49.350], mean action: 1.250 [0.000, 3.000],  loss: 25.682015, mse: 645.649595, mean_q: 12.988151, mean_eps: 0.953755
  14125/300000: episode: 153, duration: 0.525s, episode steps:  75, steps per second: 143, episode reward: -69.990, mean reward: -0.933 [-100.000,  8.630], mean action: 1.293 [0.000, 3.000],  loss: 34.822482, mse: 653.227559, mean_q: 13.875326, mean_eps: 0.953513
  14195/300000: episode: 154, duration: 0.465s, episode steps:  70, steps per second: 151, episode reward: -125.284, mean reward: -1.790 [-100.000,  6.242], mean action: 1.300 [0.000, 3.000],  loss: 31.204183, mse: 675.676824, mean_q: 13.680010, mean_eps: 0.953274
  14286/300000: episode: 155, duration: 0.587s, episode steps:  91, steps per second: 155, episode reward: -324.972, mean reward: -3.571 [-100.000, 77.674], mean action: 1.560 [0.000, 3.000],  loss: 32.661533, mse: 649.022877, mean_q: 13.846626, mean_eps: 0.953008
  14365/300000: episode: 156, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -123.940, mean reward: -1.569 [-100.000, 12.519], mean action: 1.608 [0.000, 3.000],  loss: 32.640629, mse: 688.808074, mean_q: 13.931026, mean_eps: 0.952727
  14479/300000: episode: 157, duration: 0.736s, episode steps: 114, steps per second: 155, episode reward: -131.444, mean reward: -1.153 [-100.000, 58.132], mean action: 1.333 [0.000, 3.000],  loss: 26.090108, mse: 651.944685, mean_q: 13.767602, mean_eps: 0.952409
  14574/300000: episode: 158, duration: 0.615s, episode steps:  95, steps per second: 154, episode reward: -105.414, mean reward: -1.110 [-100.000, 11.567], mean action: 1.642 [0.000, 3.000],  loss: 23.872671, mse: 681.444055, mean_q: 14.586502, mean_eps: 0.952064
  14650/300000: episode: 159, duration: 0.562s, episode steps:  76, steps per second: 135, episode reward: -364.629, mean reward: -4.798 [-100.000, 79.029], mean action: 1.382 [0.000, 3.000],  loss: 22.759235, mse: 653.617842, mean_q: 14.240013, mean_eps: 0.951782
  14791/300000: episode: 160, duration: 1.080s, episode steps: 141, steps per second: 131, episode reward: -121.475, mean reward: -0.862 [-100.000,  6.197], mean action: 1.539 [0.000, 3.000],  loss: 34.005748, mse: 675.314836, mean_q: 13.574595, mean_eps: 0.951424
  14883/300000: episode: 161, duration: 0.707s, episode steps:  92, steps per second: 130, episode reward: -85.546, mean reward: -0.930 [-100.000,  7.648], mean action: 1.543 [0.000, 3.000],  loss: 32.191257, mse: 700.388525, mean_q: 13.170969, mean_eps: 0.951040
  14953/300000: episode: 162, duration: 0.550s, episode steps:  70, steps per second: 127, episode reward: -115.427, mean reward: -1.649 [-100.000,  5.834], mean action: 1.486 [0.000, 3.000],  loss: 27.218343, mse: 706.230797, mean_q: 13.595319, mean_eps: 0.950772
  15025/300000: episode: 163, duration: 0.508s, episode steps:  72, steps per second: 142, episode reward: -110.379, mean reward: -1.533 [-100.000,  9.059], mean action: 1.556 [0.000, 3.000],  loss: 22.753689, mse: 700.465187, mean_q: 13.877521, mean_eps: 0.950538
  15146/300000: episode: 164, duration: 0.862s, episode steps: 121, steps per second: 140, episode reward: -123.168, mean reward: -1.018 [-100.000, 11.429], mean action: 1.463 [0.000, 3.000],  loss: 36.784106, mse: 802.480401, mean_q: 13.901653, mean_eps: 0.950220
  15216/300000: episode: 165, duration: 0.580s, episode steps:  70, steps per second: 121, episode reward: -146.417, mean reward: -2.092 [-100.000, 29.179], mean action: 1.657 [0.000, 3.000],  loss: 25.274985, mse: 824.804422, mean_q: 13.906212, mean_eps: 0.949904
  15311/300000: episode: 166, duration: 0.824s, episode steps:  95, steps per second: 115, episode reward: -135.080, mean reward: -1.422 [-100.000,  9.433], mean action: 1.611 [0.000, 3.000],  loss: 32.736118, mse: 811.068582, mean_q: 13.834085, mean_eps: 0.949632
  15405/300000: episode: 167, duration: 0.673s, episode steps:  94, steps per second: 140, episode reward: -115.172, mean reward: -1.225 [-100.000,  6.399], mean action: 1.457 [0.000, 3.000],  loss: 23.500800, mse: 793.441717, mean_q: 14.797173, mean_eps: 0.949320
  15502/300000: episode: 168, duration: 0.821s, episode steps:  97, steps per second: 118, episode reward: -130.959, mean reward: -1.350 [-100.000,  5.795], mean action: 1.598 [0.000, 3.000],  loss: 26.272916, mse: 807.725795, mean_q: 14.832708, mean_eps: 0.949005
  15632/300000: episode: 169, duration: 1.153s, episode steps: 130, steps per second: 113, episode reward: -123.820, mean reward: -0.952 [-100.000,  8.224], mean action: 1.492 [0.000, 3.000],  loss: 36.343849, mse: 779.585635, mean_q: 15.408754, mean_eps: 0.948631
  15725/300000: episode: 170, duration: 0.726s, episode steps:  93, steps per second: 128, episode reward: -183.328, mean reward: -1.971 [-100.000,  9.593], mean action: 1.602 [0.000, 3.000],  loss: 29.390943, mse: 822.573899, mean_q: 14.331823, mean_eps: 0.948263
  15845/300000: episode: 171, duration: 1.012s, episode steps: 120, steps per second: 119, episode reward: -366.661, mean reward: -3.056 [-100.000, 87.030], mean action: 1.442 [0.000, 3.000],  loss: 33.554391, mse: 809.005407, mean_q: 15.284850, mean_eps: 0.947911
  15975/300000: episode: 172, duration: 0.891s, episode steps: 130, steps per second: 146, episode reward: -119.811, mean reward: -0.922 [-100.000, 11.557], mean action: 1.462 [0.000, 3.000],  loss: 27.092060, mse: 816.396365, mean_q: 14.355339, mean_eps: 0.947499
  16063/300000: episode: 173, duration: 0.575s, episode steps:  88, steps per second: 153, episode reward: -182.515, mean reward: -2.074 [-100.000, 28.451], mean action: 1.398 [0.000, 3.000],  loss: 29.762587, mse: 805.634536, mean_q: 15.754827, mean_eps: 0.947139
  16174/300000: episode: 174, duration: 0.755s, episode steps: 111, steps per second: 147, episode reward: -164.704, mean reward: -1.484 [-100.000,  7.508], mean action: 1.640 [0.000, 3.000],  loss: 21.400078, mse: 836.290859, mean_q: 16.711977, mean_eps: 0.946811
  16256/300000: episode: 175, duration: 0.545s, episode steps:  82, steps per second: 150, episode reward: -76.608, mean reward: -0.934 [-100.000, 18.235], mean action: 1.524 [0.000, 3.000],  loss: 15.113578, mse: 835.323702, mean_q: 15.908187, mean_eps: 0.946492
  16359/300000: episode: 176, duration: 0.715s, episode steps: 103, steps per second: 144, episode reward: -310.061, mean reward: -3.010 [-100.000,  0.761], mean action: 1.631 [0.000, 3.000],  loss: 19.970071, mse: 815.515448, mean_q: 15.190002, mean_eps: 0.946187
  16417/300000: episode: 177, duration: 0.425s, episode steps:  58, steps per second: 136, episode reward: -81.033, mean reward: -1.397 [-100.000, 11.060], mean action: 1.483 [0.000, 3.000],  loss: 25.057547, mse: 811.024739, mean_q: 16.116747, mean_eps: 0.945921
  16547/300000: episode: 178, duration: 0.954s, episode steps: 130, steps per second: 136, episode reward: -135.212, mean reward: -1.040 [-100.000, 10.902], mean action: 1.508 [0.000, 3.000],  loss: 21.426463, mse: 820.204088, mean_q: 16.179863, mean_eps: 0.945611
  16619/300000: episode: 179, duration: 0.659s, episode steps:  72, steps per second: 109, episode reward: -82.639, mean reward: -1.148 [-100.000, 10.579], mean action: 1.569 [0.000, 3.000],  loss: 23.328373, mse: 847.009286, mean_q: 15.664045, mean_eps: 0.945278
  16724/300000: episode: 180, duration: 0.852s, episode steps: 105, steps per second: 123, episode reward: -371.547, mean reward: -3.539 [-100.000,  6.099], mean action: 1.457 [0.000, 3.000],  loss: 29.866662, mse: 807.737405, mean_q: 16.073173, mean_eps: 0.944986
  16790/300000: episode: 181, duration: 0.458s, episode steps:  66, steps per second: 144, episode reward: -106.627, mean reward: -1.616 [-100.000, 13.952], mean action: 1.606 [0.000, 3.000],  loss: 28.067124, mse: 855.551051, mean_q: 15.048574, mean_eps: 0.944704
  16914/300000: episode: 182, duration: 0.907s, episode steps: 124, steps per second: 137, episode reward: -198.446, mean reward: -1.600 [-100.000, 50.420], mean action: 1.702 [0.000, 3.000],  loss: 25.055725, mse: 835.256907, mean_q: 16.013973, mean_eps: 0.944390
  16995/300000: episode: 183, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -87.461, mean reward: -1.080 [-100.000, 17.794], mean action: 1.432 [0.000, 3.000],  loss: 27.668123, mse: 797.011785, mean_q: 16.624328, mean_eps: 0.944052
  17085/300000: episode: 184, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: 31.326, mean reward:  0.348 [-100.000, 101.141], mean action: 1.322 [0.000, 3.000],  loss: 19.209034, mse: 921.892601, mean_q: 17.238205, mean_eps: 0.943770
  17183/300000: episode: 185, duration: 0.789s, episode steps:  98, steps per second: 124, episode reward: -222.572, mean reward: -2.271 [-100.000,  1.019], mean action: 1.531 [0.000, 3.000],  loss: 18.152068, mse: 941.546269, mean_q: 16.865106, mean_eps: 0.943459
  17246/300000: episode: 186, duration: 0.533s, episode steps:  63, steps per second: 118, episode reward: -89.917, mean reward: -1.427 [-100.000, 13.874], mean action: 1.381 [0.000, 3.000],  loss: 23.715909, mse: 908.128153, mean_q: 18.119847, mean_eps: 0.943194
  17346/300000: episode: 187, duration: 0.918s, episode steps: 100, steps per second: 109, episode reward: -174.764, mean reward: -1.748 [-100.000, 16.786], mean action: 1.550 [0.000, 3.000],  loss: 16.703834, mse: 958.188808, mean_q: 17.464061, mean_eps: 0.942925
  17448/300000: episode: 188, duration: 0.696s, episode steps: 102, steps per second: 146, episode reward: -64.497, mean reward: -0.632 [-100.000, 80.713], mean action: 1.569 [0.000, 3.000],  loss: 21.916329, mse: 934.491253, mean_q: 17.432057, mean_eps: 0.942592
  17541/300000: episode: 189, duration: 0.643s, episode steps:  93, steps per second: 145, episode reward: -105.553, mean reward: -1.135 [-100.000,  6.333], mean action: 1.516 [0.000, 3.000],  loss: 19.050136, mse: 951.866951, mean_q: 16.967072, mean_eps: 0.942270
  17626/300000: episode: 190, duration: 0.572s, episode steps:  85, steps per second: 148, episode reward: -133.485, mean reward: -1.570 [-100.000,  6.932], mean action: 1.588 [0.000, 3.000],  loss: 18.226532, mse: 936.689830, mean_q: 17.165441, mean_eps: 0.941976
  17739/300000: episode: 191, duration: 0.756s, episode steps: 113, steps per second: 149, episode reward: -208.960, mean reward: -1.849 [-100.000,  9.436], mean action: 1.540 [0.000, 3.000],  loss: 20.941119, mse: 954.910320, mean_q: 17.017268, mean_eps: 0.941649
  17825/300000: episode: 192, duration: 0.550s, episode steps:  86, steps per second: 156, episode reward: -138.631, mean reward: -1.612 [-100.000,  6.228], mean action: 1.430 [0.000, 3.000],  loss: 34.340261, mse: 939.824935, mean_q: 17.325247, mean_eps: 0.941321
  17889/300000: episode: 193, duration: 0.470s, episode steps:  64, steps per second: 136, episode reward: -54.102, mean reward: -0.845 [-100.000, 12.515], mean action: 1.672 [0.000, 3.000],  loss: 22.607729, mse: 984.278676, mean_q: 16.337118, mean_eps: 0.941074
  18003/300000: episode: 194, duration: 0.836s, episode steps: 114, steps per second: 136, episode reward: -74.486, mean reward: -0.653 [-100.000, 13.417], mean action: 1.482 [0.000, 3.000],  loss: 18.751095, mse: 952.102543, mean_q: 16.547693, mean_eps: 0.940780
  18089/300000: episode: 195, duration: 0.586s, episode steps:  86, steps per second: 147, episode reward: -83.575, mean reward: -0.972 [-100.000, 10.931], mean action: 1.488 [0.000, 3.000],  loss: 21.362095, mse: 1014.390439, mean_q: 18.810535, mean_eps: 0.940450
  18147/300000: episode: 196, duration: 0.457s, episode steps:  58, steps per second: 127, episode reward: -83.612, mean reward: -1.442 [-100.000,  9.033], mean action: 1.655 [0.000, 3.000],  loss: 26.900118, mse: 1045.377801, mean_q: 18.901276, mean_eps: 0.940212
  18207/300000: episode: 197, duration: 0.446s, episode steps:  60, steps per second: 134, episode reward: -67.206, mean reward: -1.120 [-100.000, 17.076], mean action: 1.300 [0.000, 3.000],  loss: 19.246889, mse: 1000.914936, mean_q: 19.073929, mean_eps: 0.940018
  18312/300000: episode: 198, duration: 0.705s, episode steps: 105, steps per second: 149, episode reward: -85.013, mean reward: -0.810 [-100.000, 13.222], mean action: 1.552 [0.000, 3.000],  loss: 25.643026, mse: 1014.895586, mean_q: 18.647699, mean_eps: 0.939745
  18390/300000: episode: 199, duration: 0.505s, episode steps:  78, steps per second: 154, episode reward: -121.890, mean reward: -1.563 [-100.000,  9.203], mean action: 1.641 [0.000, 3.000],  loss: 22.957831, mse: 1012.227448, mean_q: 19.536025, mean_eps: 0.939443
  18471/300000: episode: 200, duration: 0.552s, episode steps:  81, steps per second: 147, episode reward: -113.152, mean reward: -1.397 [-100.000,  6.123], mean action: 1.519 [0.000, 3.000],  loss: 22.345948, mse: 1045.506775, mean_q: 18.637981, mean_eps: 0.939181
  18568/300000: episode: 201, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -118.724, mean reward: -1.224 [-100.000, 11.597], mean action: 1.557 [0.000, 3.000],  loss: 20.054979, mse: 998.343097, mean_q: 19.084105, mean_eps: 0.938887
  18682/300000: episode: 202, duration: 0.758s, episode steps: 114, steps per second: 150, episode reward: -104.342, mean reward: -0.915 [-100.000, 37.814], mean action: 1.491 [0.000, 3.000],  loss: 19.993674, mse: 1026.861755, mean_q: 18.605391, mean_eps: 0.938539
  18750/300000: episode: 203, duration: 0.481s, episode steps:  68, steps per second: 141, episode reward: -81.954, mean reward: -1.205 [-100.000,  7.975], mean action: 1.574 [0.000, 3.000],  loss: 13.843254, mse: 1014.068241, mean_q: 19.702920, mean_eps: 0.938239
  18816/300000: episode: 204, duration: 0.527s, episode steps:  66, steps per second: 125, episode reward: -75.058, mean reward: -1.137 [-100.000, 20.040], mean action: 1.500 [0.000, 3.000],  loss: 24.291720, mse: 1047.809769, mean_q: 19.278331, mean_eps: 0.938018
  18901/300000: episode: 205, duration: 0.605s, episode steps:  85, steps per second: 141, episode reward: -95.465, mean reward: -1.123 [-100.000,  6.641], mean action: 1.600 [0.000, 3.000],  loss: 21.982434, mse: 1024.470476, mean_q: 18.818139, mean_eps: 0.937769
  18991/300000: episode: 206, duration: 0.657s, episode steps:  90, steps per second: 137, episode reward: -131.541, mean reward: -1.462 [-100.000, 11.750], mean action: 1.589 [0.000, 3.000],  loss: 21.991784, mse: 1043.246010, mean_q: 19.126045, mean_eps: 0.937480
  19106/300000: episode: 207, duration: 0.874s, episode steps: 115, steps per second: 132, episode reward: -410.750, mean reward: -3.572 [-100.000, 112.557], mean action: 1.330 [0.000, 3.000],  loss: 26.785833, mse: 1116.446627, mean_q: 19.899232, mean_eps: 0.937142
  19193/300000: episode: 208, duration: 0.633s, episode steps:  87, steps per second: 137, episode reward: -136.094, mean reward: -1.564 [-100.000, 22.574], mean action: 1.586 [0.000, 3.000],  loss: 19.710494, mse: 1151.177271, mean_q: 20.308799, mean_eps: 0.936808
  19292/300000: episode: 209, duration: 0.774s, episode steps:  99, steps per second: 128, episode reward: -228.235, mean reward: -2.305 [-100.000, 61.148], mean action: 1.586 [0.000, 3.000],  loss: 28.388503, mse: 1122.462396, mean_q: 19.398491, mean_eps: 0.936501
  19395/300000: episode: 210, duration: 0.996s, episode steps: 103, steps per second: 103, episode reward: -96.770, mean reward: -0.940 [-100.000,  9.148], mean action: 1.524 [0.000, 3.000],  loss: 23.784283, mse: 1131.389678, mean_q: 19.136270, mean_eps: 0.936168
  19480/300000: episode: 211, duration: 0.857s, episode steps:  85, steps per second:  99, episode reward: -265.140, mean reward: -3.119 [-100.000, 35.907], mean action: 1.541 [0.000, 3.000],  loss: 21.378627, mse: 1134.882921, mean_q: 19.746768, mean_eps: 0.935858
  19568/300000: episode: 212, duration: 0.875s, episode steps:  88, steps per second: 101, episode reward: -199.777, mean reward: -2.270 [-100.000,  5.444], mean action: 1.580 [0.000, 3.000],  loss: 20.015010, mse: 1104.622566, mean_q: 19.769217, mean_eps: 0.935572
  19655/300000: episode: 213, duration: 0.625s, episode steps:  87, steps per second: 139, episode reward: -115.602, mean reward: -1.329 [-100.000,  6.609], mean action: 1.391 [0.000, 3.000],  loss: 26.298542, mse: 1133.047664, mean_q: 19.543411, mean_eps: 0.935284
  19813/300000: episode: 214, duration: 1.040s, episode steps: 158, steps per second: 152, episode reward: -123.995, mean reward: -0.785 [-100.000, 10.064], mean action: 1.620 [0.000, 3.000],  loss: 23.063329, mse: 1116.264170, mean_q: 19.894748, mean_eps: 0.934879
  19876/300000: episode: 215, duration: 0.411s, episode steps:  63, steps per second: 153, episode reward: -103.024, mean reward: -1.635 [-100.000,  9.918], mean action: 1.698 [0.000, 3.000],  loss: 14.577247, mse: 1060.728840, mean_q: 19.896948, mean_eps: 0.934515
  19977/300000: episode: 216, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -88.685, mean reward: -0.878 [-100.000, 10.771], mean action: 1.683 [0.000, 3.000],  loss: 22.037069, mse: 1129.029239, mean_q: 19.708207, mean_eps: 0.934244
  20055/300000: episode: 217, duration: 0.537s, episode steps:  78, steps per second: 145, episode reward: -118.765, mean reward: -1.523 [-100.000, 24.471], mean action: 1.551 [0.000, 3.000],  loss: 29.645847, mse: 1227.825944, mean_q: 19.698252, mean_eps: 0.933949
  20158/300000: episode: 218, duration: 0.674s, episode steps: 103, steps per second: 153, episode reward: -59.474, mean reward: -0.577 [-100.000,  8.103], mean action: 1.563 [0.000, 3.000],  loss: 29.637757, mse: 1230.320215, mean_q: 21.131263, mean_eps: 0.933650
  20266/300000: episode: 219, duration: 0.794s, episode steps: 108, steps per second: 136, episode reward: -118.793, mean reward: -1.100 [-100.000,  9.165], mean action: 1.500 [0.000, 3.000],  loss: 31.759043, mse: 1245.825916, mean_q: 20.702647, mean_eps: 0.933302
  20336/300000: episode: 220, duration: 0.487s, episode steps:  70, steps per second: 144, episode reward: -108.564, mean reward: -1.551 [-100.000,  5.551], mean action: 1.586 [0.000, 3.000],  loss: 37.327915, mse: 1229.046242, mean_q: 21.009980, mean_eps: 0.933008
  20471/300000: episode: 221, duration: 0.886s, episode steps: 135, steps per second: 152, episode reward: -320.946, mean reward: -2.377 [-100.000, 88.493], mean action: 1.570 [0.000, 3.000],  loss: 32.032133, mse: 1287.645575, mean_q: 20.598086, mean_eps: 0.932670
  20539/300000: episode: 222, duration: 0.488s, episode steps:  68, steps per second: 139, episode reward: -162.872, mean reward: -2.395 [-100.000,  8.082], mean action: 1.618 [0.000, 3.000],  loss: 28.981082, mse: 1262.435682, mean_q: 20.777854, mean_eps: 0.932335
  20683/300000: episode: 223, duration: 0.972s, episode steps: 144, steps per second: 148, episode reward: -42.510, mean reward: -0.295 [-100.000, 31.068], mean action: 1.688 [0.000, 3.000],  loss: 28.033810, mse: 1273.156141, mean_q: 21.477365, mean_eps: 0.931985
  20797/300000: episode: 224, duration: 0.739s, episode steps: 114, steps per second: 154, episode reward: -277.516, mean reward: -2.434 [-100.000, 98.367], mean action: 1.526 [0.000, 3.000],  loss: 28.975065, mse: 1232.230106, mean_q: 21.710402, mean_eps: 0.931560
  20861/300000: episode: 225, duration: 0.437s, episode steps:  64, steps per second: 146, episode reward: -189.051, mean reward: -2.954 [-100.000,  4.691], mean action: 1.766 [0.000, 3.000],  loss: 23.132878, mse: 1215.309290, mean_q: 20.228620, mean_eps: 0.931266
  20927/300000: episode: 226, duration: 0.434s, episode steps:  66, steps per second: 152, episode reward: -83.988, mean reward: -1.273 [-100.000, 10.923], mean action: 1.530 [0.000, 3.000],  loss: 33.141913, mse: 1329.561645, mean_q: 20.896830, mean_eps: 0.931051
  20984/300000: episode: 227, duration: 0.376s, episode steps:  57, steps per second: 152, episode reward: -138.961, mean reward: -2.438 [-100.000,  7.941], mean action: 1.404 [0.000, 3.000],  loss: 42.948915, mse: 1238.249126, mean_q: 22.046593, mean_eps: 0.930848
  21066/300000: episode: 228, duration: 0.554s, episode steps:  82, steps per second: 148, episode reward: -383.372, mean reward: -4.675 [-100.000, -0.111], mean action: 1.500 [0.000, 3.000],  loss: 24.898035, mse: 1288.361421, mean_q: 20.724945, mean_eps: 0.930619
  21168/300000: episode: 229, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -214.737, mean reward: -2.105 [-100.000, 16.702], mean action: 1.588 [0.000, 3.000],  loss: 18.374602, mse: 1359.296455, mean_q: 20.346659, mean_eps: 0.930316
  21257/300000: episode: 230, duration: 0.612s, episode steps:  89, steps per second: 145, episode reward: -193.533, mean reward: -2.175 [-100.000, 135.897], mean action: 1.708 [0.000, 3.000],  loss: 24.348527, mse: 1244.248068, mean_q: 21.702830, mean_eps: 0.930000
  21320/300000: episode: 231, duration: 0.419s, episode steps:  63, steps per second: 150, episode reward: -62.276, mean reward: -0.989 [-100.000, 59.434], mean action: 1.778 [0.000, 3.000],  loss: 27.877671, mse: 1271.621185, mean_q: 20.859654, mean_eps: 0.929750
  21394/300000: episode: 232, duration: 0.481s, episode steps:  74, steps per second: 154, episode reward: -41.951, mean reward: -0.567 [-100.000, 62.142], mean action: 1.514 [0.000, 3.000],  loss: 32.538707, mse: 1296.128574, mean_q: 21.675598, mean_eps: 0.929524
  21489/300000: episode: 233, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -124.195, mean reward: -1.307 [-100.000, 16.809], mean action: 1.505 [0.000, 3.000],  loss: 32.088904, mse: 1276.073979, mean_q: 21.040230, mean_eps: 0.929245
  21588/300000: episode: 234, duration: 0.673s, episode steps:  99, steps per second: 147, episode reward: -218.404, mean reward: -2.206 [-100.000,  1.030], mean action: 1.465 [0.000, 3.000],  loss: 30.233283, mse: 1262.073964, mean_q: 21.327035, mean_eps: 0.928925
  21683/300000: episode: 235, duration: 0.623s, episode steps:  95, steps per second: 152, episode reward: -109.921, mean reward: -1.157 [-100.000,  8.518], mean action: 1.421 [0.000, 3.000],  loss: 23.512161, mse: 1322.160876, mean_q: 20.842300, mean_eps: 0.928604
  21773/300000: episode: 236, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -111.698, mean reward: -1.241 [-100.000, 10.993], mean action: 1.467 [0.000, 3.000],  loss: 24.058974, mse: 1292.342897, mean_q: 20.613303, mean_eps: 0.928299
  21891/300000: episode: 237, duration: 1.003s, episode steps: 118, steps per second: 118, episode reward: -179.275, mean reward: -1.519 [-100.000,  8.186], mean action: 1.500 [0.000, 3.000],  loss: 26.522811, mse: 1330.543377, mean_q: 21.063864, mean_eps: 0.927956
  21980/300000: episode: 238, duration: 0.780s, episode steps:  89, steps per second: 114, episode reward: -132.645, mean reward: -1.490 [-100.000, 14.732], mean action: 1.551 [0.000, 3.000],  loss: 22.069216, mse: 1310.028654, mean_q: 19.551400, mean_eps: 0.927614
  22086/300000: episode: 239, duration: 0.769s, episode steps: 106, steps per second: 138, episode reward: -171.814, mean reward: -1.621 [-100.000,  9.006], mean action: 1.547 [0.000, 3.000],  loss: 28.137466, mse: 1336.075831, mean_q: 22.158520, mean_eps: 0.927293
  22170/300000: episode: 240, duration: 0.637s, episode steps:  84, steps per second: 132, episode reward: -149.915, mean reward: -1.785 [-100.000, 21.293], mean action: 1.476 [0.000, 3.000],  loss: 22.486880, mse: 1389.838866, mean_q: 21.914305, mean_eps: 0.926979
  22281/300000: episode: 241, duration: 0.795s, episode steps: 111, steps per second: 140, episode reward: -76.061, mean reward: -0.685 [-100.000, 16.511], mean action: 1.342 [0.000, 3.000],  loss: 24.870774, mse: 1421.082608, mean_q: 22.227738, mean_eps: 0.926657
  22363/300000: episode: 242, duration: 0.580s, episode steps:  82, steps per second: 141, episode reward: -256.163, mean reward: -3.124 [-100.000, 49.640], mean action: 1.500 [0.000, 3.000],  loss: 27.612651, mse: 1313.223778, mean_q: 22.415174, mean_eps: 0.926339
  22424/300000: episode: 243, duration: 0.442s, episode steps:  61, steps per second: 138, episode reward: -102.453, mean reward: -1.680 [-100.000,  6.648], mean action: 1.361 [0.000, 3.000],  loss: 15.763222, mse: 1428.775725, mean_q: 21.343304, mean_eps: 0.926103
  22521/300000: episode: 244, duration: 0.663s, episode steps:  97, steps per second: 146, episode reward: -84.342, mean reward: -0.870 [-100.000, 15.723], mean action: 1.546 [0.000, 3.000],  loss: 20.076726, mse: 1381.666476, mean_q: 22.279129, mean_eps: 0.925842
  22637/300000: episode: 245, duration: 0.786s, episode steps: 116, steps per second: 148, episode reward: -204.603, mean reward: -1.764 [-100.000, 103.932], mean action: 1.578 [0.000, 3.000],  loss: 20.673493, mse: 1371.468048, mean_q: 22.638318, mean_eps: 0.925491
  22724/300000: episode: 246, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -111.183, mean reward: -1.278 [-100.000,  6.526], mean action: 1.448 [0.000, 3.000],  loss: 18.935239, mse: 1471.452435, mean_q: 21.647771, mean_eps: 0.925156
  22826/300000: episode: 247, duration: 0.667s, episode steps: 102, steps per second: 153, episode reward: -247.630, mean reward: -2.428 [-100.000, 49.970], mean action: 1.588 [0.000, 3.000],  loss: 25.174249, mse: 1407.020813, mean_q: 21.725934, mean_eps: 0.924844
  22901/300000: episode: 248, duration: 0.478s, episode steps:  75, steps per second: 157, episode reward: -60.148, mean reward: -0.802 [-100.000, 14.064], mean action: 1.467 [0.000, 3.000],  loss: 24.583200, mse: 1405.397955, mean_q: 22.579406, mean_eps: 0.924552
  23021/300000: episode: 249, duration: 0.793s, episode steps: 120, steps per second: 151, episode reward: -113.710, mean reward: -0.948 [-100.000,  6.615], mean action: 1.383 [0.000, 3.000],  loss: 18.554445, mse: 1400.599802, mean_q: 23.056318, mean_eps: 0.924230
  23091/300000: episode: 250, duration: 0.458s, episode steps:  70, steps per second: 153, episode reward: -113.478, mean reward: -1.621 [-100.000, 12.324], mean action: 1.800 [0.000, 3.000],  loss: 22.000623, mse: 1535.487763, mean_q: 24.071019, mean_eps: 0.923917
  23208/300000: episode: 251, duration: 0.742s, episode steps: 117, steps per second: 158, episode reward: -122.040, mean reward: -1.043 [-100.000,  6.262], mean action: 1.513 [0.000, 3.000],  loss: 20.873038, mse: 1539.423849, mean_q: 24.016573, mean_eps: 0.923608
  23278/300000: episode: 252, duration: 0.436s, episode steps:  70, steps per second: 160, episode reward: -81.252, mean reward: -1.161 [-100.000,  6.858], mean action: 1.586 [0.000, 3.000],  loss: 25.989499, mse: 1531.615388, mean_q: 23.508707, mean_eps: 0.923300
  23391/300000: episode: 253, duration: 0.770s, episode steps: 113, steps per second: 147, episode reward: -158.609, mean reward: -1.404 [-100.000,  7.512], mean action: 1.584 [0.000, 3.000],  loss: 20.902184, mse: 1567.538973, mean_q: 23.316974, mean_eps: 0.922998
  23470/300000: episode: 254, duration: 0.514s, episode steps:  79, steps per second: 154, episode reward: -65.231, mean reward: -0.826 [-100.000, 19.123], mean action: 1.316 [0.000, 3.000],  loss: 28.030682, mse: 1571.742086, mean_q: 24.134997, mean_eps: 0.922681
  23563/300000: episode: 255, duration: 0.600s, episode steps:  93, steps per second: 155, episode reward: -144.328, mean reward: -1.552 [-100.000, 22.201], mean action: 1.602 [0.000, 3.000],  loss: 34.594212, mse: 1557.073484, mean_q: 23.394760, mean_eps: 0.922397
  23683/300000: episode: 256, duration: 0.799s, episode steps: 120, steps per second: 150, episode reward: -143.304, mean reward: -1.194 [-100.000, 13.985], mean action: 1.500 [0.000, 3.000],  loss: 31.104595, mse: 1563.375016, mean_q: 24.103293, mean_eps: 0.922046
  23779/300000: episode: 257, duration: 0.629s, episode steps:  96, steps per second: 153, episode reward: -131.990, mean reward: -1.375 [-100.000,  8.966], mean action: 1.562 [0.000, 3.000],  loss: 22.804702, mse: 1535.927026, mean_q: 24.377451, mean_eps: 0.921689
  23895/300000: episode: 258, duration: 0.742s, episode steps: 116, steps per second: 156, episode reward: -86.421, mean reward: -0.745 [-100.000,  7.869], mean action: 1.534 [0.000, 3.000],  loss: 22.525159, mse: 1562.403564, mean_q: 23.778061, mean_eps: 0.921340
  23976/300000: episode: 259, duration: 0.525s, episode steps:  81, steps per second: 154, episode reward: -100.695, mean reward: -1.243 [-100.000, 10.124], mean action: 1.556 [0.000, 3.000],  loss: 23.401226, mse: 1561.411054, mean_q: 24.603253, mean_eps: 0.921015
  24128/300000: episode: 260, duration: 1.009s, episode steps: 152, steps per second: 151, episode reward: -125.978, mean reward: -0.829 [-100.000, 13.574], mean action: 1.599 [0.000, 3.000],  loss: 24.516808, mse: 1623.782526, mean_q: 23.939444, mean_eps: 0.920630
  24214/300000: episode: 261, duration: 0.545s, episode steps:  86, steps per second: 158, episode reward: -147.562, mean reward: -1.716 [-100.000,  7.951], mean action: 1.535 [0.000, 3.000],  loss: 22.488364, mse: 1684.866154, mean_q: 24.185621, mean_eps: 0.920237
  24331/300000: episode: 262, duration: 0.764s, episode steps: 117, steps per second: 153, episode reward: -243.018, mean reward: -2.077 [-100.000,  5.948], mean action: 1.718 [0.000, 3.000],  loss: 18.441197, mse: 1666.434052, mean_q: 23.734614, mean_eps: 0.919902
  24403/300000: episode: 263, duration: 0.578s, episode steps:  72, steps per second: 125, episode reward: -237.504, mean reward: -3.299 [-100.000,  5.512], mean action: 1.597 [0.000, 3.000],  loss: 30.992530, mse: 1685.291295, mean_q: 24.296813, mean_eps: 0.919591
  24492/300000: episode: 264, duration: 0.685s, episode steps:  89, steps per second: 130, episode reward: -88.617, mean reward: -0.996 [-100.000,  7.581], mean action: 1.449 [0.000, 3.000],  loss: 25.503590, mse: 1655.910409, mean_q: 24.875176, mean_eps: 0.919325
  24567/300000: episode: 265, duration: 0.548s, episode steps:  75, steps per second: 137, episode reward: -123.109, mean reward: -1.641 [-100.000, 24.303], mean action: 1.213 [0.000, 3.000],  loss: 25.291888, mse: 1657.470688, mean_q: 24.349512, mean_eps: 0.919054
  24639/300000: episode: 266, duration: 0.548s, episode steps:  72, steps per second: 131, episode reward: -57.806, mean reward: -0.803 [-100.000,  6.337], mean action: 1.528 [0.000, 3.000],  loss: 21.021875, mse: 1736.769962, mean_q: 23.264382, mean_eps: 0.918812
  24715/300000: episode: 267, duration: 0.547s, episode steps:  76, steps per second: 139, episode reward: -193.150, mean reward: -2.541 [-100.000, 65.585], mean action: 1.632 [0.000, 3.000],  loss: 24.201178, mse: 1692.322971, mean_q: 24.671581, mean_eps: 0.918568
  24826/300000: episode: 268, duration: 0.785s, episode steps: 111, steps per second: 141, episode reward: -154.233, mean reward: -1.389 [-100.000,  6.203], mean action: 1.369 [0.000, 3.000],  loss: 26.679215, mse: 1682.566194, mean_q: 24.433519, mean_eps: 0.918259
  24936/300000: episode: 269, duration: 0.771s, episode steps: 110, steps per second: 143, episode reward: -115.282, mean reward: -1.048 [-100.000,  6.788], mean action: 1.564 [0.000, 3.000],  loss: 21.450297, mse: 1660.942070, mean_q: 24.439840, mean_eps: 0.917894
  25019/300000: episode: 270, duration: 0.633s, episode steps:  83, steps per second: 131, episode reward: -127.956, mean reward: -1.542 [-100.000,  8.652], mean action: 1.759 [0.000, 3.000],  loss: 20.684704, mse: 1755.108029, mean_q: 24.818286, mean_eps: 0.917576
  25140/300000: episode: 271, duration: 0.835s, episode steps: 121, steps per second: 145, episode reward: -220.740, mean reward: -1.824 [-100.000,  1.997], mean action: 1.587 [0.000, 3.000],  loss: 19.783492, mse: 1782.547316, mean_q: 26.209838, mean_eps: 0.917239
  25242/300000: episode: 272, duration: 0.723s, episode steps: 102, steps per second: 141, episode reward: -117.189, mean reward: -1.149 [-100.000, 17.935], mean action: 1.539 [0.000, 3.000],  loss: 20.120535, mse: 1738.441859, mean_q: 26.852005, mean_eps: 0.916871
  25336/300000: episode: 273, duration: 0.837s, episode steps:  94, steps per second: 112, episode reward: -107.494, mean reward: -1.144 [-100.000,  6.295], mean action: 1.489 [0.000, 3.000],  loss: 18.709532, mse: 1789.459777, mean_q: 25.790249, mean_eps: 0.916548
  25403/300000: episode: 274, duration: 0.505s, episode steps:  67, steps per second: 133, episode reward: -75.249, mean reward: -1.123 [-100.000, 21.049], mean action: 1.701 [0.000, 3.000],  loss: 21.268440, mse: 1794.240610, mean_q: 25.640882, mean_eps: 0.916282
  25469/300000: episode: 275, duration: 0.458s, episode steps:  66, steps per second: 144, episode reward: -79.218, mean reward: -1.200 [-100.000, 13.819], mean action: 1.636 [0.000, 3.000],  loss: 21.862110, mse: 1801.601250, mean_q: 25.655441, mean_eps: 0.916063
  25578/300000: episode: 276, duration: 0.825s, episode steps: 109, steps per second: 132, episode reward: -29.072, mean reward: -0.267 [-100.000, 117.032], mean action: 1.413 [0.000, 3.000],  loss: 17.144371, mse: 1758.662608, mean_q: 26.059289, mean_eps: 0.915774
  25663/300000: episode: 277, duration: 0.584s, episode steps:  85, steps per second: 146, episode reward: -111.325, mean reward: -1.310 [-100.000, 10.543], mean action: 1.247 [0.000, 3.000],  loss: 17.521970, mse: 1767.398986, mean_q: 25.908190, mean_eps: 0.915454
  25753/300000: episode: 278, duration: 0.640s, episode steps:  90, steps per second: 141, episode reward: -39.050, mean reward: -0.434 [-100.000, 76.204], mean action: 1.633 [0.000, 3.000],  loss: 21.072775, mse: 1780.970022, mean_q: 27.032902, mean_eps: 0.915165
  25847/300000: episode: 279, duration: 0.676s, episode steps:  94, steps per second: 139, episode reward: -88.882, mean reward: -0.946 [-100.000, 13.468], mean action: 1.489 [0.000, 3.000],  loss: 16.232078, mse: 1782.684701, mean_q: 25.792624, mean_eps: 0.914862
  25951/300000: episode: 280, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -88.930, mean reward: -0.855 [-100.000, 13.362], mean action: 1.615 [0.000, 3.000],  loss: 18.014644, mse: 1755.624188, mean_q: 26.139129, mean_eps: 0.914535
  26031/300000: episode: 281, duration: 0.533s, episode steps:  80, steps per second: 150, episode reward: -2.239, mean reward: -0.028 [-100.000, 114.927], mean action: 1.538 [0.000, 3.000],  loss: 21.216629, mse: 1799.910194, mean_q: 27.781722, mean_eps: 0.914231
  26103/300000: episode: 282, duration: 0.467s, episode steps:  72, steps per second: 154, episode reward: -60.347, mean reward: -0.838 [-100.000,  9.530], mean action: 1.569 [0.000, 3.000],  loss: 31.461830, mse: 1999.641703, mean_q: 29.246658, mean_eps: 0.913981
  26228/300000: episode: 283, duration: 0.853s, episode steps: 125, steps per second: 146, episode reward: -103.511, mean reward: -0.828 [-100.000,  8.653], mean action: 1.568 [0.000, 3.000],  loss: 21.956203, mse: 1934.500974, mean_q: 29.127520, mean_eps: 0.913656
  26324/300000: episode: 284, duration: 0.615s, episode steps:  96, steps per second: 156, episode reward: -161.707, mean reward: -1.684 [-100.000,  8.988], mean action: 1.427 [0.000, 3.000],  loss: 21.169285, mse: 1891.240100, mean_q: 28.936219, mean_eps: 0.913291
  26402/300000: episode: 285, duration: 0.503s, episode steps:  78, steps per second: 155, episode reward: -74.544, mean reward: -0.956 [-100.000, 15.644], mean action: 1.692 [0.000, 3.000],  loss: 29.666104, mse: 1954.757792, mean_q: 27.997283, mean_eps: 0.913004
  26496/300000: episode: 286, duration: 0.667s, episode steps:  94, steps per second: 141, episode reward: -93.670, mean reward: -0.996 [-100.000,  7.587], mean action: 1.574 [0.000, 3.000],  loss: 25.913320, mse: 1895.530863, mean_q: 28.398885, mean_eps: 0.912720
  26596/300000: episode: 287, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -141.284, mean reward: -1.413 [-100.000,  6.726], mean action: 1.610 [0.000, 3.000],  loss: 23.191774, mse: 1948.360558, mean_q: 28.693442, mean_eps: 0.912400
  26688/300000: episode: 288, duration: 0.591s, episode steps:  92, steps per second: 156, episode reward: -141.913, mean reward: -1.543 [-100.000,  6.803], mean action: 1.380 [0.000, 3.000],  loss: 24.522077, mse: 1920.380765, mean_q: 27.956081, mean_eps: 0.912083
  26796/300000: episode: 289, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: -153.489, mean reward: -1.421 [-100.000,  5.627], mean action: 1.713 [0.000, 3.000],  loss: 23.875012, mse: 2004.513305, mean_q: 28.672016, mean_eps: 0.911753
  26913/300000: episode: 290, duration: 0.771s, episode steps: 117, steps per second: 152, episode reward: -102.841, mean reward: -0.879 [-100.000,  9.390], mean action: 1.393 [0.000, 3.000],  loss: 27.364214, mse: 1918.709907, mean_q: 29.645268, mean_eps: 0.911382
  27006/300000: episode: 291, duration: 0.605s, episode steps:  93, steps per second: 154, episode reward: -77.279, mean reward: -0.831 [-100.000,  8.617], mean action: 1.452 [0.000, 3.000],  loss: 26.867161, mse: 1896.098725, mean_q: 28.793545, mean_eps: 0.911035
  27096/300000: episode: 292, duration: 0.580s, episode steps:  90, steps per second: 155, episode reward: -91.300, mean reward: -1.014 [-100.000, 13.084], mean action: 1.556 [0.000, 3.000],  loss: 19.588930, mse: 2022.242875, mean_q: 28.492956, mean_eps: 0.910733
  27162/300000: episode: 293, duration: 0.452s, episode steps:  66, steps per second: 146, episode reward: -86.842, mean reward: -1.316 [-100.000,  7.796], mean action: 1.667 [0.000, 3.000],  loss: 14.851137, mse: 1984.769396, mean_q: 29.898174, mean_eps: 0.910476
  27242/300000: episode: 294, duration: 0.544s, episode steps:  80, steps per second: 147, episode reward: -127.816, mean reward: -1.598 [-100.000,  7.712], mean action: 1.575 [0.000, 3.000],  loss: 16.351566, mse: 1956.288562, mean_q: 28.556155, mean_eps: 0.910235
  27357/300000: episode: 295, duration: 0.766s, episode steps: 115, steps per second: 150, episode reward: -115.679, mean reward: -1.006 [-100.000, 95.128], mean action: 1.513 [0.000, 3.000],  loss: 17.257177, mse: 1932.937775, mean_q: 28.098643, mean_eps: 0.909913
  27461/300000: episode: 296, duration: 0.715s, episode steps: 104, steps per second: 145, episode reward: -62.903, mean reward: -0.605 [-100.000, 12.054], mean action: 1.577 [0.000, 3.000],  loss: 20.539865, mse: 2018.746426, mean_q: 29.613764, mean_eps: 0.909552
  27547/300000: episode: 297, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -45.010, mean reward: -0.523 [-100.000, 17.314], mean action: 1.616 [0.000, 3.000],  loss: 21.306776, mse: 2035.105733, mean_q: 29.275325, mean_eps: 0.909238
  27626/300000: episode: 298, duration: 0.549s, episode steps:  79, steps per second: 144, episode reward: -91.738, mean reward: -1.161 [-100.000,  5.816], mean action: 1.646 [0.000, 3.000],  loss: 23.031827, mse: 1972.117472, mean_q: 29.317516, mean_eps: 0.908966
  27743/300000: episode: 299, duration: 0.805s, episode steps: 117, steps per second: 145, episode reward: -285.502, mean reward: -2.440 [-100.000,  6.254], mean action: 1.521 [0.000, 3.000],  loss: 15.984233, mse: 2028.779497, mean_q: 28.457844, mean_eps: 0.908643
  27838/300000: episode: 300, duration: 0.762s, episode steps:  95, steps per second: 125, episode reward: -135.356, mean reward: -1.425 [-100.000,  6.744], mean action: 1.695 [0.000, 3.000],  loss: 17.272462, mse: 1979.234560, mean_q: 28.091877, mean_eps: 0.908293
  27926/300000: episode: 301, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -147.264, mean reward: -1.673 [-100.000, 23.756], mean action: 1.409 [0.000, 3.000],  loss: 20.671551, mse: 1961.178503, mean_q: 28.470097, mean_eps: 0.907991
  28040/300000: episode: 302, duration: 0.833s, episode steps: 114, steps per second: 137, episode reward: -103.076, mean reward: -0.904 [-100.000,  6.052], mean action: 1.684 [0.000, 3.000],  loss: 24.033199, mse: 2005.461851, mean_q: 28.630393, mean_eps: 0.907658
  28145/300000: episode: 303, duration: 0.735s, episode steps: 105, steps per second: 143, episode reward: -197.870, mean reward: -1.884 [-100.000, 10.765], mean action: 1.590 [0.000, 3.000],  loss: 16.095149, mse: 2040.373712, mean_q: 28.353645, mean_eps: 0.907296
  28271/300000: episode: 304, duration: 0.908s, episode steps: 126, steps per second: 139, episode reward: -107.905, mean reward: -0.856 [-100.000,  9.471], mean action: 1.484 [0.000, 3.000],  loss: 15.409653, mse: 2121.721346, mean_q: 27.914901, mean_eps: 0.906915
  28352/300000: episode: 305, duration: 0.686s, episode steps:  81, steps per second: 118, episode reward: -199.023, mean reward: -2.457 [-100.000,  3.733], mean action: 1.469 [0.000, 3.000],  loss: 19.838698, mse: 2081.203749, mean_q: 29.922501, mean_eps: 0.906574
  28443/300000: episode: 306, duration: 0.642s, episode steps:  91, steps per second: 142, episode reward: -62.681, mean reward: -0.689 [-100.000, 20.463], mean action: 1.560 [0.000, 3.000],  loss: 21.681326, mse: 2006.802801, mean_q: 29.138971, mean_eps: 0.906290
  28513/300000: episode: 307, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -201.790, mean reward: -2.883 [-100.000,  6.305], mean action: 1.543 [0.000, 3.000],  loss: 19.588285, mse: 2109.188194, mean_q: 28.188468, mean_eps: 0.906024
  28615/300000: episode: 308, duration: 0.663s, episode steps: 102, steps per second: 154, episode reward: -135.950, mean reward: -1.333 [-100.000,  6.488], mean action: 1.373 [0.000, 3.000],  loss: 16.038840, mse: 2104.913737, mean_q: 29.439408, mean_eps: 0.905740
  28714/300000: episode: 309, duration: 0.740s, episode steps:  99, steps per second: 134, episode reward: -457.065, mean reward: -4.617 [-100.000, 85.221], mean action: 1.404 [0.000, 3.000],  loss: 27.561008, mse: 2069.455288, mean_q: 29.753873, mean_eps: 0.905409
  28826/300000: episode: 310, duration: 0.729s, episode steps: 112, steps per second: 154, episode reward: -72.538, mean reward: -0.648 [-100.000, 16.749], mean action: 1.589 [0.000, 3.000],  loss: 17.916758, mse: 2098.714967, mean_q: 28.874849, mean_eps: 0.905061
  28913/300000: episode: 311, duration: 0.572s, episode steps:  87, steps per second: 152, episode reward: -88.245, mean reward: -1.014 [-100.000, 10.567], mean action: 1.598 [0.000, 3.000],  loss: 26.906850, mse: 2101.238444, mean_q: 29.426787, mean_eps: 0.904732
  28991/300000: episode: 312, duration: 0.552s, episode steps:  78, steps per second: 141, episode reward: -50.016, mean reward: -0.641 [-100.000, 18.239], mean action: 1.679 [0.000, 3.000],  loss: 28.443335, mse: 2077.777075, mean_q: 29.539539, mean_eps: 0.904460
  29095/300000: episode: 313, duration: 0.919s, episode steps: 104, steps per second: 113, episode reward: -76.919, mean reward: -0.740 [-100.000,  7.981], mean action: 1.394 [0.000, 3.000],  loss: 16.539270, mse: 2128.940271, mean_q: 27.660033, mean_eps: 0.904160
  29158/300000: episode: 314, duration: 0.428s, episode steps:  63, steps per second: 147, episode reward: -102.532, mean reward: -1.627 [-100.000, 11.405], mean action: 1.444 [0.000, 3.000],  loss: 18.048445, mse: 2189.157405, mean_q: 29.162934, mean_eps: 0.903884
  29224/300000: episode: 315, duration: 0.464s, episode steps:  66, steps per second: 142, episode reward: -98.087, mean reward: -1.486 [-100.000,  6.488], mean action: 1.500 [0.000, 3.000],  loss: 29.817718, mse: 2119.560671, mean_q: 29.761396, mean_eps: 0.903671
  29298/300000: episode: 316, duration: 0.543s, episode steps:  74, steps per second: 136, episode reward: -223.776, mean reward: -3.024 [-100.000, 27.947], mean action: 1.689 [0.000, 3.000],  loss: 26.635571, mse: 2161.272425, mean_q: 29.817417, mean_eps: 0.903440
  29408/300000: episode: 317, duration: 0.854s, episode steps: 110, steps per second: 129, episode reward: -327.840, mean reward: -2.980 [-100.000,  1.234], mean action: 1.700 [0.000, 3.000],  loss: 27.155521, mse: 2219.613561, mean_q: 30.276146, mean_eps: 0.903137
  29478/300000: episode: 318, duration: 0.528s, episode steps:  70, steps per second: 133, episode reward: -67.110, mean reward: -0.959 [-100.000, 11.719], mean action: 1.414 [0.000, 3.000],  loss: 21.482756, mse: 2186.048601, mean_q: 29.179799, mean_eps: 0.902840
  29551/300000: episode: 319, duration: 0.679s, episode steps:  73, steps per second: 107, episode reward: -185.180, mean reward: -2.537 [-100.000, 13.656], mean action: 1.562 [0.000, 3.000],  loss: 15.393569, mse: 2223.205374, mean_q: 29.702568, mean_eps: 0.902604
  29638/300000: episode: 320, duration: 0.605s, episode steps:  87, steps per second: 144, episode reward: -241.180, mean reward: -2.772 [-100.000, 82.853], mean action: 1.713 [0.000, 3.000],  loss: 25.942058, mse: 2165.440441, mean_q: 29.448113, mean_eps: 0.902340
  29738/300000: episode: 321, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: -303.144, mean reward: -3.031 [-100.000,  0.688], mean action: 1.600 [0.000, 3.000],  loss: 14.112529, mse: 2099.584106, mean_q: 29.775064, mean_eps: 0.902031
  29863/300000: episode: 322, duration: 1.082s, episode steps: 125, steps per second: 116, episode reward: -82.777, mean reward: -0.662 [-100.000, 11.270], mean action: 1.512 [0.000, 3.000],  loss: 17.794762, mse: 2148.484552, mean_q: 29.784090, mean_eps: 0.901660
  29935/300000: episode: 323, duration: 0.719s, episode steps:  72, steps per second: 100, episode reward: -46.976, mean reward: -0.652 [-100.000, 16.143], mean action: 1.375 [0.000, 3.000],  loss: 19.172232, mse: 2191.552580, mean_q: 29.258748, mean_eps: 0.901335
  30014/300000: episode: 324, duration: 0.740s, episode steps:  79, steps per second: 107, episode reward: -107.722, mean reward: -1.364 [-100.000, 12.331], mean action: 1.620 [0.000, 3.000],  loss: 23.407418, mse: 2151.848546, mean_q: 29.838136, mean_eps: 0.901086
  30091/300000: episode: 325, duration: 0.715s, episode steps:  77, steps per second: 108, episode reward: -90.957, mean reward: -1.181 [-100.000,  5.860], mean action: 1.610 [0.000, 3.000],  loss: 26.131092, mse: 2204.609814, mean_q: 29.700837, mean_eps: 0.900828
  30151/300000: episode: 326, duration: 0.515s, episode steps:  60, steps per second: 116, episode reward: -122.352, mean reward: -2.039 [-100.000,  7.150], mean action: 1.417 [0.000, 3.000],  loss: 20.564275, mse: 2272.968943, mean_q: 29.694954, mean_eps: 0.900602
  30227/300000: episode: 327, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: -109.104, mean reward: -1.436 [-100.000,  6.338], mean action: 1.632 [0.000, 3.000],  loss: 19.264217, mse: 2208.571426, mean_q: 30.312826, mean_eps: 0.900378
  30322/300000: episode: 328, duration: 0.710s, episode steps:  95, steps per second: 134, episode reward: -355.349, mean reward: -3.741 [-100.000,  0.441], mean action: 1.653 [0.000, 3.000],  loss: 25.552574, mse: 2199.462393, mean_q: 30.433421, mean_eps: 0.900096
  30426/300000: episode: 329, duration: 0.818s, episode steps: 104, steps per second: 127, episode reward: -158.332, mean reward: -1.522 [-100.000,  6.815], mean action: 1.365 [0.000, 3.000],  loss: 21.132782, mse: 2199.690207, mean_q: 30.109347, mean_eps: 0.899767
  30545/300000: episode: 330, duration: 0.942s, episode steps: 119, steps per second: 126, episode reward: -182.610, mean reward: -1.535 [-100.000,  2.992], mean action: 1.487 [0.000, 3.000],  loss: 23.552900, mse: 2246.919663, mean_q: 30.247644, mean_eps: 0.899399
  30619/300000: episode: 331, duration: 0.602s, episode steps:  74, steps per second: 123, episode reward: -131.841, mean reward: -1.782 [-100.000, 14.032], mean action: 1.297 [0.000, 3.000],  loss: 18.666093, mse: 2221.828429, mean_q: 30.686668, mean_eps: 0.899081
  30702/300000: episode: 332, duration: 0.610s, episode steps:  83, steps per second: 136, episode reward: -86.403, mean reward: -1.041 [-100.000,  7.936], mean action: 1.361 [0.000, 3.000],  loss: 20.449762, mse: 2207.326123, mean_q: 29.072320, mean_eps: 0.898822
  30809/300000: episode: 333, duration: 0.766s, episode steps: 107, steps per second: 140, episode reward: -136.538, mean reward: -1.276 [-100.000,  5.106], mean action: 1.495 [0.000, 3.000],  loss: 22.454018, mse: 2202.029544, mean_q: 28.986277, mean_eps: 0.898509
  30909/300000: episode: 334, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: -123.243, mean reward: -1.232 [-100.000, 11.142], mean action: 1.650 [0.000, 3.000],  loss: 21.481603, mse: 2193.224934, mean_q: 30.778309, mean_eps: 0.898167
  30993/300000: episode: 335, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -63.193, mean reward: -0.752 [-100.000, 12.411], mean action: 1.190 [0.000, 3.000],  loss: 19.707442, mse: 2153.931875, mean_q: 31.438291, mean_eps: 0.897863
  31104/300000: episode: 336, duration: 0.767s, episode steps: 111, steps per second: 145, episode reward: -381.557, mean reward: -3.437 [-100.000, 11.337], mean action: 1.532 [0.000, 3.000],  loss: 19.783803, mse: 2288.744115, mean_q: 30.646416, mean_eps: 0.897542
  31169/300000: episode: 337, duration: 0.473s, episode steps:  65, steps per second: 137, episode reward: -87.593, mean reward: -1.348 [-100.000, 10.299], mean action: 1.523 [0.000, 3.000],  loss: 17.006813, mse: 2275.611493, mean_q: 31.026795, mean_eps: 0.897251
  31256/300000: episode: 338, duration: 0.916s, episode steps:  87, steps per second:  95, episode reward: -80.377, mean reward: -0.924 [-100.000, 20.548], mean action: 1.506 [0.000, 3.000],  loss: 24.175728, mse: 2331.316730, mean_q: 30.679928, mean_eps: 0.897000
  31318/300000: episode: 339, duration: 0.467s, episode steps:  62, steps per second: 133, episode reward: -94.100, mean reward: -1.518 [-100.000, 14.426], mean action: 1.500 [0.000, 3.000],  loss: 19.038867, mse: 2300.563563, mean_q: 30.978702, mean_eps: 0.896755
  31379/300000: episode: 340, duration: 0.420s, episode steps:  61, steps per second: 145, episode reward: -100.628, mean reward: -1.650 [-100.000,  6.517], mean action: 1.557 [0.000, 3.000],  loss: 14.542810, mse: 2332.784248, mean_q: 29.251681, mean_eps: 0.896552
  31442/300000: episode: 341, duration: 0.467s, episode steps:  63, steps per second: 135, episode reward: -81.744, mean reward: -1.298 [-100.000, 12.941], mean action: 1.444 [0.000, 3.000],  loss: 17.190888, mse: 2341.490438, mean_q: 31.042061, mean_eps: 0.896347
  31521/300000: episode: 342, duration: 0.593s, episode steps:  79, steps per second: 133, episode reward: -136.070, mean reward: -1.722 [-100.000,  8.481], mean action: 1.570 [0.000, 3.000],  loss: 21.707592, mse: 2385.521970, mean_q: 30.758459, mean_eps: 0.896113
  31634/300000: episode: 343, duration: 0.808s, episode steps: 113, steps per second: 140, episode reward: -152.798, mean reward: -1.352 [-100.000,  3.551], mean action: 1.513 [0.000, 3.000],  loss: 22.326426, mse: 2317.932806, mean_q: 30.267173, mean_eps: 0.895796
  31739/300000: episode: 344, duration: 0.763s, episode steps: 105, steps per second: 138, episode reward: -90.572, mean reward: -0.863 [-100.000, 12.680], mean action: 1.771 [0.000, 3.000],  loss: 18.992757, mse: 2331.174450, mean_q: 30.287829, mean_eps: 0.895436
  31826/300000: episode: 345, duration: 0.679s, episode steps:  87, steps per second: 128, episode reward: -40.459, mean reward: -0.465 [-100.000, 85.289], mean action: 1.609 [0.000, 3.000],  loss: 21.737430, mse: 2320.870015, mean_q: 29.489152, mean_eps: 0.895119
  31934/300000: episode: 346, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: -145.196, mean reward: -1.344 [-100.000, 10.637], mean action: 1.509 [0.000, 3.000],  loss: 18.472084, mse: 2279.642242, mean_q: 31.507879, mean_eps: 0.894798
  32040/300000: episode: 347, duration: 0.716s, episode steps: 106, steps per second: 148, episode reward: -207.327, mean reward: -1.956 [-100.000, 34.130], mean action: 1.604 [0.000, 3.000],  loss: 19.275800, mse: 2387.585066, mean_q: 31.215920, mean_eps: 0.894445
  32136/300000: episode: 348, duration: 0.661s, episode steps:  96, steps per second: 145, episode reward: -134.714, mean reward: -1.403 [-100.000, 12.075], mean action: 1.583 [0.000, 3.000],  loss: 16.509278, mse: 2373.732012, mean_q: 31.346300, mean_eps: 0.894111
  32215/300000: episode: 349, duration: 0.588s, episode steps:  79, steps per second: 134, episode reward: -197.279, mean reward: -2.497 [-100.000, 13.253], mean action: 1.646 [0.000, 3.000],  loss: 20.470322, mse: 2406.217429, mean_q: 31.365474, mean_eps: 0.893822
  32305/300000: episode: 350, duration: 0.758s, episode steps:  90, steps per second: 119, episode reward: -88.914, mean reward: -0.988 [-100.000,  7.768], mean action: 1.700 [0.000, 3.000],  loss: 24.940370, mse: 2347.857677, mean_q: 31.236958, mean_eps: 0.893544
  32395/300000: episode: 351, duration: 0.805s, episode steps:  90, steps per second: 112, episode reward: -120.989, mean reward: -1.344 [-100.000, 10.918], mean action: 1.644 [0.000, 3.000],  loss: 21.485806, mse: 2338.536013, mean_q: 31.803614, mean_eps: 0.893247
  32473/300000: episode: 352, duration: 0.647s, episode steps:  78, steps per second: 121, episode reward: -88.248, mean reward: -1.131 [-100.000, 10.948], mean action: 1.372 [0.000, 3.000],  loss: 17.325447, mse: 2443.029273, mean_q: 32.295520, mean_eps: 0.892969
  32562/300000: episode: 353, duration: 0.733s, episode steps:  89, steps per second: 121, episode reward: -100.115, mean reward: -1.125 [-100.000, 20.192], mean action: 1.449 [0.000, 3.000],  loss: 37.464570, mse: 2448.548087, mean_q: 31.190933, mean_eps: 0.892694
  32654/300000: episode: 354, duration: 0.746s, episode steps:  92, steps per second: 123, episode reward: -83.007, mean reward: -0.902 [-100.000, 10.739], mean action: 1.348 [0.000, 3.000],  loss: 30.327792, mse: 2394.101195, mean_q: 30.939406, mean_eps: 0.892395
  32728/300000: episode: 355, duration: 0.514s, episode steps:  74, steps per second: 144, episode reward: -99.792, mean reward: -1.349 [-100.000, 10.536], mean action: 1.595 [0.000, 3.000],  loss: 24.028537, mse: 2419.246328, mean_q: 30.061362, mean_eps: 0.892121
  32826/300000: episode: 356, duration: 0.746s, episode steps:  98, steps per second: 131, episode reward: -176.843, mean reward: -1.805 [-100.000,  6.989], mean action: 1.510 [0.000, 3.000],  loss: 20.032498, mse: 2397.074702, mean_q: 30.701504, mean_eps: 0.891838
  32910/300000: episode: 357, duration: 0.697s, episode steps:  84, steps per second: 121, episode reward: -101.074, mean reward: -1.203 [-100.000,  8.537], mean action: 1.488 [0.000, 3.000],  loss: 16.698508, mse: 2431.702951, mean_q: 32.046672, mean_eps: 0.891537
  33022/300000: episode: 358, duration: 0.915s, episode steps: 112, steps per second: 122, episode reward: -122.575, mean reward: -1.094 [-100.000,  6.578], mean action: 1.839 [0.000, 3.000],  loss: 26.846421, mse: 2412.763670, mean_q: 31.090512, mean_eps: 0.891214
  33115/300000: episode: 359, duration: 0.647s, episode steps:  93, steps per second: 144, episode reward: -134.769, mean reward: -1.449 [-100.000, 15.553], mean action: 1.581 [0.000, 3.000],  loss: 18.158078, mse: 2503.621103, mean_q: 31.916859, mean_eps: 0.890876
  33204/300000: episode: 360, duration: 0.633s, episode steps:  89, steps per second: 141, episode reward: -157.737, mean reward: -1.772 [-100.000,  7.944], mean action: 1.663 [0.000, 3.000],  loss: 29.310730, mse: 2430.258236, mean_q: 31.767990, mean_eps: 0.890575
  33294/300000: episode: 361, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -119.928, mean reward: -1.333 [-100.000,  9.620], mean action: 1.467 [0.000, 3.000],  loss: 24.466794, mse: 2437.481289, mean_q: 32.107528, mean_eps: 0.890280
  33413/300000: episode: 362, duration: 0.930s, episode steps: 119, steps per second: 128, episode reward: -159.611, mean reward: -1.341 [-100.000,  5.647], mean action: 1.412 [0.000, 3.000],  loss: 19.634518, mse: 2491.656726, mean_q: 31.010227, mean_eps: 0.889935
  33494/300000: episode: 363, duration: 0.721s, episode steps:  81, steps per second: 112, episode reward: -96.988, mean reward: -1.197 [-100.000,  6.091], mean action: 1.420 [0.000, 3.000],  loss: 26.529263, mse: 2454.516990, mean_q: 30.743575, mean_eps: 0.889605
  33604/300000: episode: 364, duration: 0.811s, episode steps: 110, steps per second: 136, episode reward: -99.644, mean reward: -0.906 [-100.000,  7.626], mean action: 1.473 [0.000, 3.000],  loss: 21.043188, mse: 2482.902572, mean_q: 30.616591, mean_eps: 0.889290
  33696/300000: episode: 365, duration: 0.676s, episode steps:  92, steps per second: 136, episode reward: -111.147, mean reward: -1.208 [-100.000, 14.417], mean action: 1.402 [0.000, 3.000],  loss: 26.393137, mse: 2493.249502, mean_q: 32.257608, mean_eps: 0.888957
  33785/300000: episode: 366, duration: 0.673s, episode steps:  89, steps per second: 132, episode reward: -91.121, mean reward: -1.024 [-100.000, 11.131], mean action: 1.416 [0.000, 3.000],  loss: 26.802577, mse: 2408.312666, mean_q: 30.482480, mean_eps: 0.888658
  33915/300000: episode: 367, duration: 0.998s, episode steps: 130, steps per second: 130, episode reward: -91.357, mean reward: -0.703 [-100.000,  7.501], mean action: 1.615 [0.000, 3.000],  loss: 21.311715, mse: 2515.918584, mean_q: 31.195340, mean_eps: 0.888297
  34048/300000: episode: 368, duration: 0.933s, episode steps: 133, steps per second: 143, episode reward: -27.394, mean reward: -0.206 [-100.000, 64.131], mean action: 1.459 [0.000, 3.000],  loss: 27.569337, mse: 2519.380953, mean_q: 31.969061, mean_eps: 0.887863
  34106/300000: episode: 369, duration: 0.408s, episode steps:  58, steps per second: 142, episode reward: -50.849, mean reward: -0.877 [-100.000, 23.628], mean action: 1.414 [0.000, 3.000],  loss: 26.841453, mse: 2599.941615, mean_q: 34.206548, mean_eps: 0.887548
  34187/300000: episode: 370, duration: 0.535s, episode steps:  81, steps per second: 151, episode reward: -78.642, mean reward: -0.971 [-100.000, 15.999], mean action: 1.370 [0.000, 3.000],  loss: 20.275035, mse: 2662.455039, mean_q: 34.063436, mean_eps: 0.887318
  34289/300000: episode: 371, duration: 0.669s, episode steps: 102, steps per second: 152, episode reward: -399.618, mean reward: -3.918 [-100.000,  4.480], mean action: 1.735 [0.000, 3.000],  loss: 24.100073, mse: 2647.847486, mean_q: 33.374327, mean_eps: 0.887016
  34360/300000: episode: 372, duration: 0.459s, episode steps:  71, steps per second: 155, episode reward:  6.058, mean reward:  0.085 [-100.000, 113.267], mean action: 1.507 [0.000, 3.000],  loss: 22.383902, mse: 2647.532386, mean_q: 33.212869, mean_eps: 0.886731
  34478/300000: episode: 373, duration: 0.820s, episode steps: 118, steps per second: 144, episode reward: -129.409, mean reward: -1.097 [-100.000,  5.938], mean action: 1.576 [0.000, 3.000],  loss: 18.253549, mse: 2583.773375, mean_q: 32.951367, mean_eps: 0.886419
  34566/300000: episode: 374, duration: 0.583s, episode steps:  88, steps per second: 151, episode reward: -20.827, mean reward: -0.237 [-100.000, 97.186], mean action: 1.375 [0.000, 3.000],  loss: 21.418492, mse: 2610.918593, mean_q: 33.399672, mean_eps: 0.886079
  34656/300000: episode: 375, duration: 0.598s, episode steps:  90, steps per second: 150, episode reward: -89.272, mean reward: -0.992 [-100.000,  6.633], mean action: 1.444 [0.000, 3.000],  loss: 18.288035, mse: 2591.852644, mean_q: 32.658191, mean_eps: 0.885785
  34744/300000: episode: 376, duration: 0.622s, episode steps:  88, steps per second: 141, episode reward: -131.710, mean reward: -1.497 [-100.000,  5.595], mean action: 1.534 [0.000, 3.000],  loss: 26.176710, mse: 2515.086780, mean_q: 33.430247, mean_eps: 0.885492
  34856/300000: episode: 377, duration: 0.740s, episode steps: 112, steps per second: 151, episode reward: -421.573, mean reward: -3.764 [-100.000, 100.688], mean action: 1.554 [0.000, 3.000],  loss: 20.868336, mse: 2591.278878, mean_q: 34.451285, mean_eps: 0.885162
  34970/300000: episode: 378, duration: 0.743s, episode steps: 114, steps per second: 153, episode reward: -108.962, mean reward: -0.956 [-100.000, 17.373], mean action: 1.570 [0.000, 3.000],  loss: 24.353249, mse: 2625.406942, mean_q: 33.180329, mean_eps: 0.884789
  35064/300000: episode: 379, duration: 0.675s, episode steps:  94, steps per second: 139, episode reward: -168.807, mean reward: -1.796 [-100.000,  7.543], mean action: 1.489 [0.000, 3.000],  loss: 16.072821, mse: 2695.382574, mean_q: 35.091860, mean_eps: 0.884446
  35136/300000: episode: 380, duration: 0.483s, episode steps:  72, steps per second: 149, episode reward: -98.638, mean reward: -1.370 [-100.000, 47.264], mean action: 1.653 [0.000, 3.000],  loss: 16.540272, mse: 2674.173316, mean_q: 35.751938, mean_eps: 0.884172
  35221/300000: episode: 381, duration: 0.569s, episode steps:  85, steps per second: 150, episode reward: -95.840, mean reward: -1.128 [-100.000, 18.539], mean action: 1.282 [0.000, 3.000],  loss: 15.159631, mse: 2666.303797, mean_q: 34.450258, mean_eps: 0.883913
  35336/300000: episode: 382, duration: 0.769s, episode steps: 115, steps per second: 150, episode reward: -75.739, mean reward: -0.659 [-100.000, 19.796], mean action: 1.409 [0.000, 3.000],  loss: 19.833056, mse: 2773.502630, mean_q: 35.616897, mean_eps: 0.883583
  35437/300000: episode: 383, duration: 0.692s, episode steps: 101, steps per second: 146, episode reward: -70.855, mean reward: -0.702 [-100.000, 11.485], mean action: 1.574 [0.000, 3.000],  loss: 25.246781, mse: 2695.835058, mean_q: 34.394049, mean_eps: 0.883226
  35525/300000: episode: 384, duration: 0.573s, episode steps:  88, steps per second: 153, episode reward: -117.949, mean reward: -1.340 [-100.000, 12.404], mean action: 1.455 [0.000, 3.000],  loss: 30.028089, mse: 2683.024017, mean_q: 35.358919, mean_eps: 0.882914
  35599/300000: episode: 385, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -96.139, mean reward: -1.299 [-100.000,  6.532], mean action: 1.473 [0.000, 3.000],  loss: 22.194641, mse: 2776.217453, mean_q: 35.399038, mean_eps: 0.882647
  35671/300000: episode: 386, duration: 0.500s, episode steps:  72, steps per second: 144, episode reward: -53.592, mean reward: -0.744 [-100.000, 24.332], mean action: 1.681 [0.000, 3.000],  loss: 28.829639, mse: 2674.223324, mean_q: 34.866667, mean_eps: 0.882406
  35762/300000: episode: 387, duration: 0.615s, episode steps:  91, steps per second: 148, episode reward: -155.446, mean reward: -1.708 [-100.000,  5.610], mean action: 1.341 [0.000, 3.000],  loss: 18.974613, mse: 2721.689931, mean_q: 34.274135, mean_eps: 0.882137
  35846/300000: episode: 388, duration: 0.555s, episode steps:  84, steps per second: 151, episode reward: -51.600, mean reward: -0.614 [-100.000,  9.401], mean action: 1.417 [0.000, 3.000],  loss: 18.743411, mse: 2701.867641, mean_q: 35.182527, mean_eps: 0.881848
  35922/300000: episode: 389, duration: 0.498s, episode steps:  76, steps per second: 153, episode reward: -115.817, mean reward: -1.524 [-100.000,  6.510], mean action: 1.316 [0.000, 3.000],  loss: 21.346240, mse: 2722.065182, mean_q: 34.820563, mean_eps: 0.881584
  35985/300000: episode: 390, duration: 0.438s, episode steps:  63, steps per second: 144, episode reward: -126.173, mean reward: -2.003 [-100.000, 18.031], mean action: 1.429 [0.000, 3.000],  loss: 21.993711, mse: 2754.712800, mean_q: 34.989961, mean_eps: 0.881355
  36052/300000: episode: 391, duration: 0.474s, episode steps:  67, steps per second: 141, episode reward: -47.533, mean reward: -0.709 [-100.000, 12.936], mean action: 1.522 [0.000, 3.000],  loss: 18.455916, mse: 2819.829856, mean_q: 36.009849, mean_eps: 0.881141
  36127/300000: episode: 392, duration: 0.507s, episode steps:  75, steps per second: 148, episode reward: -83.631, mean reward: -1.115 [-100.000, 19.530], mean action: 1.867 [0.000, 3.000],  loss: 26.485873, mse: 2927.857520, mean_q: 36.646289, mean_eps: 0.880906
  36241/300000: episode: 393, duration: 0.748s, episode steps: 114, steps per second: 152, episode reward: -288.575, mean reward: -2.531 [-100.000, 48.560], mean action: 1.561 [0.000, 3.000],  loss: 15.696828, mse: 2896.663255, mean_q: 36.127978, mean_eps: 0.880594
  36329/300000: episode: 394, duration: 0.608s, episode steps:  88, steps per second: 145, episode reward: -90.756, mean reward: -1.031 [-100.000, 13.097], mean action: 1.602 [0.000, 3.000],  loss: 17.774564, mse: 2921.585213, mean_q: 34.205807, mean_eps: 0.880261
  36416/300000: episode: 395, duration: 0.591s, episode steps:  87, steps per second: 147, episode reward: -142.190, mean reward: -1.634 [-100.000, 11.741], mean action: 1.391 [0.000, 3.000],  loss: 16.628675, mse: 2919.416423, mean_q: 35.255047, mean_eps: 0.879972
  36510/300000: episode: 396, duration: 0.627s, episode steps:  94, steps per second: 150, episode reward: -301.429, mean reward: -3.207 [-100.000,  0.220], mean action: 1.606 [0.000, 3.000],  loss: 21.426297, mse: 2956.574188, mean_q: 35.770226, mean_eps: 0.879674
  36586/300000: episode: 397, duration: 0.496s, episode steps:  76, steps per second: 153, episode reward: -263.696, mean reward: -3.470 [-100.000, 112.361], mean action: 1.579 [0.000, 3.000],  loss: 30.635749, mse: 2896.910285, mean_q: 35.890312, mean_eps: 0.879393
  36689/300000: episode: 398, duration: 0.723s, episode steps: 103, steps per second: 143, episode reward: -147.099, mean reward: -1.428 [-100.000,  4.848], mean action: 1.379 [0.000, 3.000],  loss: 21.590551, mse: 2936.702960, mean_q: 35.149178, mean_eps: 0.879098
  36814/300000: episode: 399, duration: 0.824s, episode steps: 125, steps per second: 152, episode reward: -194.816, mean reward: -1.559 [-100.000,  5.785], mean action: 1.584 [0.000, 3.000],  loss: 18.514950, mse: 2967.144234, mean_q: 35.825217, mean_eps: 0.878722
  36911/300000: episode: 400, duration: 0.658s, episode steps:  97, steps per second: 147, episode reward: -212.224, mean reward: -2.188 [-100.000,  8.017], mean action: 1.258 [0.000, 3.000],  loss: 23.596854, mse: 2948.691419, mean_q: 36.760361, mean_eps: 0.878355
  36978/300000: episode: 401, duration: 0.482s, episode steps:  67, steps per second: 139, episode reward: -105.123, mean reward: -1.569 [-100.000,  9.385], mean action: 1.388 [0.000, 3.000],  loss: 23.594204, mse: 2986.529854, mean_q: 35.270423, mean_eps: 0.878085
  37102/300000: episode: 402, duration: 0.827s, episode steps: 124, steps per second: 150, episode reward: -97.881, mean reward: -0.789 [-100.000,  9.816], mean action: 1.629 [0.000, 3.000],  loss: 17.914549, mse: 2972.427033, mean_q: 36.141129, mean_eps: 0.877770
  37168/300000: episode: 403, duration: 0.429s, episode steps:  66, steps per second: 154, episode reward: -97.075, mean reward: -1.471 [-100.000,  6.860], mean action: 1.379 [0.000, 3.000],  loss: 15.041549, mse: 2965.047411, mean_q: 35.256568, mean_eps: 0.877456
  37245/300000: episode: 404, duration: 0.511s, episode steps:  77, steps per second: 151, episode reward: -27.844, mean reward: -0.362 [-100.000, 27.512], mean action: 1.468 [0.000, 3.000],  loss: 18.146407, mse: 3047.290053, mean_q: 35.214715, mean_eps: 0.877220
  37318/300000: episode: 405, duration: 0.508s, episode steps:  73, steps per second: 144, episode reward: -70.174, mean reward: -0.961 [-100.000, 16.557], mean action: 1.575 [0.000, 3.000],  loss: 12.290975, mse: 3089.866402, mean_q: 36.497667, mean_eps: 0.876973
  37392/300000: episode: 406, duration: 0.508s, episode steps:  74, steps per second: 146, episode reward: -88.333, mean reward: -1.194 [-100.000,  6.887], mean action: 1.527 [0.000, 3.000],  loss: 25.058058, mse: 2999.011151, mean_q: 36.916332, mean_eps: 0.876730
  37481/300000: episode: 407, duration: 0.591s, episode steps:  89, steps per second: 151, episode reward: -140.636, mean reward: -1.580 [-100.000, 16.734], mean action: 1.472 [0.000, 3.000],  loss: 19.106358, mse: 3053.684088, mean_q: 34.864138, mean_eps: 0.876461
  37562/300000: episode: 408, duration: 0.549s, episode steps:  81, steps per second: 148, episode reward: -95.122, mean reward: -1.174 [-100.000,  9.989], mean action: 1.346 [0.000, 3.000],  loss: 18.035915, mse: 3034.277886, mean_q: 36.959004, mean_eps: 0.876181
  37694/300000: episode: 409, duration: 0.920s, episode steps: 132, steps per second: 143, episode reward: -17.503, mean reward: -0.133 [-100.000, 105.155], mean action: 1.530 [0.000, 3.000],  loss: 13.881396, mse: 3079.886571, mean_q: 37.104157, mean_eps: 0.875829
  37807/300000: episode: 410, duration: 0.766s, episode steps: 113, steps per second: 147, episode reward: -65.416, mean reward: -0.579 [-100.000,  6.717], mean action: 1.540 [0.000, 3.000],  loss: 17.141385, mse: 3044.625763, mean_q: 36.892571, mean_eps: 0.875425
  37913/300000: episode: 411, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -115.456, mean reward: -1.089 [-100.000,  6.234], mean action: 1.651 [0.000, 3.000],  loss: 13.970287, mse: 3088.675749, mean_q: 36.421458, mean_eps: 0.875064
  38010/300000: episode: 412, duration: 0.670s, episode steps:  97, steps per second: 145, episode reward: -66.570, mean reward: -0.686 [-100.000, 16.801], mean action: 1.443 [0.000, 3.000],  loss: 17.507793, mse: 3050.975090, mean_q: 36.748042, mean_eps: 0.874729
  38103/300000: episode: 413, duration: 0.621s, episode steps:  93, steps per second: 150, episode reward: -120.625, mean reward: -1.297 [-100.000, 10.553], mean action: 1.581 [0.000, 3.000],  loss: 22.646573, mse: 3159.784300, mean_q: 37.699395, mean_eps: 0.874415
  38209/300000: episode: 414, duration: 0.693s, episode steps: 106, steps per second: 153, episode reward: -172.324, mean reward: -1.626 [-100.000, 15.598], mean action: 1.575 [0.000, 3.000],  loss: 22.344437, mse: 3098.250944, mean_q: 37.421889, mean_eps: 0.874087
  38291/300000: episode: 415, duration: 0.580s, episode steps:  82, steps per second: 141, episode reward: -69.111, mean reward: -0.843 [-100.000, 13.510], mean action: 1.512 [0.000, 3.000],  loss: 24.140867, mse: 3133.157658, mean_q: 37.777813, mean_eps: 0.873777
  38361/300000: episode: 416, duration: 0.481s, episode steps:  70, steps per second: 146, episode reward: -75.097, mean reward: -1.073 [-100.000,  6.367], mean action: 1.771 [0.000, 3.000],  loss: 24.073819, mse: 3066.648490, mean_q: 35.951145, mean_eps: 0.873526
  38495/300000: episode: 417, duration: 0.900s, episode steps: 134, steps per second: 149, episode reward: -291.312, mean reward: -2.174 [-100.000, 93.737], mean action: 1.493 [0.000, 3.000],  loss: 22.177047, mse: 3117.523638, mean_q: 37.434975, mean_eps: 0.873189
  38564/300000: episode: 418, duration: 0.495s, episode steps:  69, steps per second: 139, episode reward: -69.984, mean reward: -1.014 [-100.000,  8.356], mean action: 1.667 [0.000, 3.000],  loss: 17.604307, mse: 3011.096050, mean_q: 36.201440, mean_eps: 0.872854
  38630/300000: episode: 419, duration: 0.449s, episode steps:  66, steps per second: 147, episode reward: -98.268, mean reward: -1.489 [-100.000, 15.131], mean action: 1.485 [0.000, 3.000],  loss: 16.290459, mse: 3066.621464, mean_q: 38.042022, mean_eps: 0.872632
  38716/300000: episode: 420, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -120.237, mean reward: -1.398 [-100.000, 17.510], mean action: 1.523 [0.000, 3.000],  loss: 17.943900, mse: 3146.654061, mean_q: 36.832742, mean_eps: 0.872381
  38775/300000: episode: 421, duration: 0.510s, episode steps:  59, steps per second: 116, episode reward: -168.600, mean reward: -2.858 [-100.000,  8.159], mean action: 1.780 [0.000, 3.000],  loss: 21.353560, mse: 3150.622352, mean_q: 36.579710, mean_eps: 0.872141
  38852/300000: episode: 422, duration: 0.525s, episode steps:  77, steps per second: 147, episode reward: -66.220, mean reward: -0.860 [-100.000, 10.731], mean action: 1.714 [0.000, 3.000],  loss: 27.487279, mse: 3099.654490, mean_q: 35.367322, mean_eps: 0.871917
  38924/300000: episode: 423, duration: 0.505s, episode steps:  72, steps per second: 143, episode reward: -123.322, mean reward: -1.713 [-100.000,  8.138], mean action: 1.611 [0.000, 3.000],  loss: 18.018111, mse: 3116.216743, mean_q: 37.058342, mean_eps: 0.871671
  39038/300000: episode: 424, duration: 0.767s, episode steps: 114, steps per second: 149, episode reward: -94.457, mean reward: -0.829 [-100.000,  9.797], mean action: 1.535 [0.000, 3.000],  loss: 23.702233, mse: 3113.407456, mean_q: 37.043975, mean_eps: 0.871364
  39132/300000: episode: 425, duration: 0.626s, episode steps:  94, steps per second: 150, episode reward: -78.777, mean reward: -0.838 [-100.000, 14.192], mean action: 1.670 [0.000, 3.000],  loss: 27.583868, mse: 3160.761755, mean_q: 36.448564, mean_eps: 0.871021
  39200/300000: episode: 426, duration: 0.493s, episode steps:  68, steps per second: 138, episode reward: -72.556, mean reward: -1.067 [-100.000, 17.939], mean action: 1.544 [0.000, 3.000],  loss: 29.140856, mse: 3083.711864, mean_q: 36.052740, mean_eps: 0.870754
  39308/300000: episode: 427, duration: 0.727s, episode steps: 108, steps per second: 148, episode reward: -89.848, mean reward: -0.832 [-100.000, 17.159], mean action: 1.583 [0.000, 3.000],  loss: 18.903874, mse: 3150.038192, mean_q: 35.856319, mean_eps: 0.870463
  39400/300000: episode: 428, duration: 0.594s, episode steps:  92, steps per second: 155, episode reward: -163.866, mean reward: -1.781 [-100.000,  7.706], mean action: 1.522 [0.000, 3.000],  loss: 24.183804, mse: 3167.978003, mean_q: 37.644808, mean_eps: 0.870133
  39498/300000: episode: 429, duration: 0.697s, episode steps:  98, steps per second: 141, episode reward: 14.143, mean reward:  0.144 [-100.000, 86.388], mean action: 1.694 [0.000, 3.000],  loss: 22.168867, mse: 3075.194929, mean_q: 35.220859, mean_eps: 0.869820
  39602/300000: episode: 430, duration: 0.712s, episode steps: 104, steps per second: 146, episode reward: -106.416, mean reward: -1.023 [-100.000, 17.644], mean action: 1.529 [0.000, 3.000],  loss: 24.851029, mse: 3059.329484, mean_q: 35.885176, mean_eps: 0.869487
  39659/300000: episode: 431, duration: 0.397s, episode steps:  57, steps per second: 144, episode reward: -109.595, mean reward: -1.923 [-100.000, 29.257], mean action: 1.526 [0.000, 3.000],  loss: 21.194948, mse: 3140.041662, mean_q: 35.983100, mean_eps: 0.869221
  39732/300000: episode: 432, duration: 0.489s, episode steps:  73, steps per second: 149, episode reward: -129.835, mean reward: -1.779 [-100.000,  9.622], mean action: 1.712 [0.000, 3.000],  loss: 35.485349, mse: 3161.552855, mean_q: 37.728467, mean_eps: 0.869007
  39822/300000: episode: 433, duration: 0.636s, episode steps:  90, steps per second: 142, episode reward: -152.730, mean reward: -1.697 [-100.000,  6.926], mean action: 1.500 [0.000, 3.000],  loss: 19.043193, mse: 3161.098736, mean_q: 35.837305, mean_eps: 0.868738
  39927/300000: episode: 434, duration: 0.710s, episode steps: 105, steps per second: 148, episode reward: -78.551, mean reward: -0.748 [-100.000, 13.164], mean action: 1.562 [0.000, 3.000],  loss: 18.539921, mse: 3123.972017, mean_q: 37.447080, mean_eps: 0.868416
  40010/300000: episode: 435, duration: 0.563s, episode steps:  83, steps per second: 147, episode reward: -35.749, mean reward: -0.431 [-100.000, 16.847], mean action: 1.590 [0.000, 3.000],  loss: 23.443543, mse: 3101.069221, mean_q: 35.245149, mean_eps: 0.868106
  40096/300000: episode: 436, duration: 0.575s, episode steps:  86, steps per second: 150, episode reward: -187.175, mean reward: -2.176 [-100.000, 54.451], mean action: 1.395 [0.000, 3.000],  loss: 26.392584, mse: 3319.081532, mean_q: 37.116308, mean_eps: 0.867827
  40184/300000: episode: 437, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -58.169, mean reward: -0.661 [-100.000,  7.080], mean action: 1.602 [0.000, 3.000],  loss: 24.665836, mse: 3290.579540, mean_q: 36.950739, mean_eps: 0.867540
  40269/300000: episode: 438, duration: 0.568s, episode steps:  85, steps per second: 150, episode reward: -207.677, mean reward: -2.443 [-100.000, 41.730], mean action: 1.471 [0.000, 3.000],  loss: 28.091086, mse: 3291.195646, mean_q: 37.440364, mean_eps: 0.867254
  40379/300000: episode: 439, duration: 0.740s, episode steps: 110, steps per second: 149, episode reward: -73.010, mean reward: -0.664 [-100.000, 12.320], mean action: 1.673 [0.000, 3.000],  loss: 27.362762, mse: 3290.291917, mean_q: 38.481254, mean_eps: 0.866932
  40452/300000: episode: 440, duration: 0.548s, episode steps:  73, steps per second: 133, episode reward: -91.936, mean reward: -1.259 [-100.000, 45.680], mean action: 1.699 [0.000, 3.000],  loss: 25.814499, mse: 3286.617552, mean_q: 38.323598, mean_eps: 0.866630
  40552/300000: episode: 441, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: -58.140, mean reward: -0.581 [-100.000, 16.706], mean action: 1.360 [0.000, 3.000],  loss: 23.496634, mse: 3284.286504, mean_q: 37.668588, mean_eps: 0.866345
  40617/300000: episode: 442, duration: 0.428s, episode steps:  65, steps per second: 152, episode reward: -111.039, mean reward: -1.708 [-100.000, 102.585], mean action: 1.477 [0.000, 3.000],  loss: 30.696780, mse: 3217.356475, mean_q: 38.249947, mean_eps: 0.866073
  40679/300000: episode: 443, duration: 0.409s, episode steps:  62, steps per second: 152, episode reward: -67.947, mean reward: -1.096 [-100.000, 18.128], mean action: 1.387 [0.000, 3.000],  loss: 17.404129, mse: 3300.543075, mean_q: 37.783776, mean_eps: 0.865863
  40770/300000: episode: 444, duration: 0.627s, episode steps:  91, steps per second: 145, episode reward: -72.570, mean reward: -0.797 [-100.000, 10.846], mean action: 1.484 [0.000, 3.000],  loss: 25.756418, mse: 3211.033799, mean_q: 36.948616, mean_eps: 0.865611
  40873/300000: episode: 445, duration: 0.744s, episode steps: 103, steps per second: 139, episode reward: -33.964, mean reward: -0.330 [-100.000, 21.657], mean action: 1.641 [0.000, 3.000],  loss: 28.963918, mse: 3208.317895, mean_q: 38.372388, mean_eps: 0.865291
  40947/300000: episode: 446, duration: 0.555s, episode steps:  74, steps per second: 133, episode reward: -90.317, mean reward: -1.220 [-100.000,  6.846], mean action: 1.541 [0.000, 3.000],  loss: 32.348617, mse: 3282.641397, mean_q: 36.622633, mean_eps: 0.864999
  41058/300000: episode: 447, duration: 0.843s, episode steps: 111, steps per second: 132, episode reward: -108.195, mean reward: -0.975 [-100.000,  6.522], mean action: 1.450 [0.000, 3.000],  loss: 26.414833, mse: 3321.933453, mean_q: 37.549861, mean_eps: 0.864693
  41157/300000: episode: 448, duration: 0.723s, episode steps:  99, steps per second: 137, episode reward: -123.303, mean reward: -1.245 [-100.000,  5.401], mean action: 1.465 [0.000, 3.000],  loss: 20.430011, mse: 3307.979662, mean_q: 38.485529, mean_eps: 0.864347
  41293/300000: episode: 449, duration: 0.971s, episode steps: 136, steps per second: 140, episode reward: -97.557, mean reward: -0.717 [-100.000,  8.101], mean action: 1.404 [0.000, 3.000],  loss: 22.926593, mse: 3363.705290, mean_q: 38.693943, mean_eps: 0.863959
  41405/300000: episode: 450, duration: 0.826s, episode steps: 112, steps per second: 136, episode reward: -143.089, mean reward: -1.278 [-100.000,  5.080], mean action: 1.571 [0.000, 3.000],  loss: 18.658142, mse: 3326.645735, mean_q: 38.120879, mean_eps: 0.863550
  41502/300000: episode: 451, duration: 0.659s, episode steps:  97, steps per second: 147, episode reward: -149.845, mean reward: -1.545 [-100.000,  9.021], mean action: 1.454 [0.000, 3.000],  loss: 17.459348, mse: 3275.027480, mean_q: 37.196163, mean_eps: 0.863205
  41582/300000: episode: 452, duration: 0.525s, episode steps:  80, steps per second: 152, episode reward: -74.677, mean reward: -0.933 [-100.000, 18.027], mean action: 1.637 [0.000, 3.000],  loss: 23.277833, mse: 3343.028760, mean_q: 37.956412, mean_eps: 0.862913
  41643/300000: episode: 453, duration: 0.418s, episode steps:  61, steps per second: 146, episode reward: -119.228, mean reward: -1.955 [-100.000, 35.770], mean action: 1.492 [0.000, 3.000],  loss: 17.353319, mse: 3337.893471, mean_q: 37.946015, mean_eps: 0.862680
  41764/300000: episode: 454, duration: 0.863s, episode steps: 121, steps per second: 140, episode reward: -150.174, mean reward: -1.241 [-100.000,  5.458], mean action: 1.380 [0.000, 3.000],  loss: 22.224204, mse: 3322.082296, mean_q: 38.480450, mean_eps: 0.862380
  41841/300000: episode: 455, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -200.255, mean reward: -2.601 [-100.000, 10.426], mean action: 1.519 [0.000, 3.000],  loss: 19.200876, mse: 3288.506627, mean_q: 38.844796, mean_eps: 0.862053
  41943/300000: episode: 456, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: -117.524, mean reward: -1.152 [-100.000,  9.747], mean action: 1.422 [0.000, 3.000],  loss: 16.104421, mse: 3198.717352, mean_q: 37.623303, mean_eps: 0.861758
  42045/300000: episode: 457, duration: 0.717s, episode steps: 102, steps per second: 142, episode reward: -331.909, mean reward: -3.254 [-100.000,  5.049], mean action: 1.500 [0.000, 3.000],  loss: 22.140912, mse: 3342.152945, mean_q: 38.329673, mean_eps: 0.861421
  42130/300000: episode: 458, duration: 0.569s, episode steps:  85, steps per second: 149, episode reward: -117.587, mean reward: -1.383 [-100.000,  5.401], mean action: 1.529 [0.000, 3.000],  loss: 25.868083, mse: 3358.773920, mean_q: 38.145238, mean_eps: 0.861113
  42216/300000: episode: 459, duration: 0.582s, episode steps:  86, steps per second: 148, episode reward: -131.172, mean reward: -1.525 [-100.000,  8.541], mean action: 1.570 [0.000, 3.000],  loss: 16.648905, mse: 3493.652531, mean_q: 37.319541, mean_eps: 0.860831
  42278/300000: episode: 460, duration: 0.428s, episode steps:  62, steps per second: 145, episode reward: -75.597, mean reward: -1.219 [-100.000, 11.605], mean action: 1.968 [0.000, 3.000],  loss: 21.027939, mse: 3316.401694, mean_q: 38.114607, mean_eps: 0.860587
  42397/300000: episode: 461, duration: 0.843s, episode steps: 119, steps per second: 141, episode reward: -139.210, mean reward: -1.170 [-100.000, 12.210], mean action: 1.353 [0.000, 3.000],  loss: 16.660844, mse: 3468.537077, mean_q: 39.053458, mean_eps: 0.860288
  42507/300000: episode: 462, duration: 0.959s, episode steps: 110, steps per second: 115, episode reward: -90.257, mean reward: -0.821 [-100.000,  7.023], mean action: 1.527 [0.000, 3.000],  loss: 14.264916, mse: 3440.978456, mean_q: 39.345229, mean_eps: 0.859910
  42634/300000: episode: 463, duration: 1.084s, episode steps: 127, steps per second: 117, episode reward: -98.233, mean reward: -0.773 [-100.000, 38.614], mean action: 1.504 [0.000, 3.000],  loss: 16.742948, mse: 3421.188382, mean_q: 37.415333, mean_eps: 0.859519
  42705/300000: episode: 464, duration: 0.578s, episode steps:  71, steps per second: 123, episode reward: -74.731, mean reward: -1.053 [-100.000, 19.573], mean action: 1.408 [0.000, 3.000],  loss: 20.739972, mse: 3427.693043, mean_q: 37.324940, mean_eps: 0.859192
  42802/300000: episode: 465, duration: 0.733s, episode steps:  97, steps per second: 132, episode reward: -119.477, mean reward: -1.232 [-100.000, 12.875], mean action: 1.433 [0.000, 3.000],  loss: 20.448515, mse: 3428.229583, mean_q: 39.036514, mean_eps: 0.858915
  42920/300000: episode: 466, duration: 1.123s, episode steps: 118, steps per second: 105, episode reward: -100.419, mean reward: -0.851 [-100.000,  9.029], mean action: 1.678 [0.000, 3.000],  loss: 21.054353, mse: 3438.536770, mean_q: 39.471812, mean_eps: 0.858560
  42985/300000: episode: 467, duration: 0.631s, episode steps:  65, steps per second: 103, episode reward: -151.330, mean reward: -2.328 [-100.000,  6.123], mean action: 1.677 [0.000, 3.000],  loss: 15.681376, mse: 3349.058733, mean_q: 39.123190, mean_eps: 0.858258
  43087/300000: episode: 468, duration: 0.944s, episode steps: 102, steps per second: 108, episode reward: -211.616, mean reward: -2.075 [-100.000,  6.530], mean action: 1.392 [0.000, 3.000],  loss: 21.563974, mse: 3453.718317, mean_q: 40.532995, mean_eps: 0.857983
  43186/300000: episode: 469, duration: 0.872s, episode steps:  99, steps per second: 114, episode reward: -77.900, mean reward: -0.787 [-100.000, 19.438], mean action: 1.747 [0.000, 3.000],  loss: 20.591598, mse: 3494.265132, mean_q: 40.458853, mean_eps: 0.857651
  43265/300000: episode: 470, duration: 0.725s, episode steps:  79, steps per second: 109, episode reward: -66.101, mean reward: -0.837 [-100.000, 11.823], mean action: 1.570 [0.000, 3.000],  loss: 18.468793, mse: 3528.445773, mean_q: 39.797101, mean_eps: 0.857358
  43329/300000: episode: 471, duration: 0.600s, episode steps:  64, steps per second: 107, episode reward: -82.776, mean reward: -1.293 [-100.000, 15.176], mean action: 1.328 [0.000, 3.000],  loss: 14.389098, mse: 3546.357468, mean_q: 40.049842, mean_eps: 0.857122
  43407/300000: episode: 472, duration: 0.639s, episode steps:  78, steps per second: 122, episode reward: -106.701, mean reward: -1.368 [-100.000, 12.516], mean action: 1.474 [0.000, 3.000],  loss: 27.685820, mse: 3496.635376, mean_q: 39.953735, mean_eps: 0.856887
  43508/300000: episode: 473, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -267.892, mean reward: -2.652 [-100.000, 113.375], mean action: 1.653 [0.000, 3.000],  loss: 14.198473, mse: 3465.978869, mean_q: 38.956444, mean_eps: 0.856592
  43616/300000: episode: 474, duration: 0.855s, episode steps: 108, steps per second: 126, episode reward: -120.789, mean reward: -1.118 [-100.000, 39.142], mean action: 1.435 [0.000, 3.000],  loss: 24.830610, mse: 3521.606418, mean_q: 38.976447, mean_eps: 0.856247
  43702/300000: episode: 475, duration: 0.681s, episode steps:  86, steps per second: 126, episode reward: -123.693, mean reward: -1.438 [-100.000,  8.550], mean action: 1.430 [0.000, 3.000],  loss: 19.673194, mse: 3510.162041, mean_q: 38.771720, mean_eps: 0.855927
  43776/300000: episode: 476, duration: 0.571s, episode steps:  74, steps per second: 130, episode reward: -73.540, mean reward: -0.994 [-100.000, 18.094], mean action: 1.676 [0.000, 3.000],  loss: 20.435137, mse: 3577.795344, mean_q: 40.631596, mean_eps: 0.855663
  43839/300000: episode: 477, duration: 0.459s, episode steps:  63, steps per second: 137, episode reward: -163.093, mean reward: -2.589 [-100.000,  7.895], mean action: 1.603 [0.000, 3.000],  loss: 14.517406, mse: 3511.021271, mean_q: 39.250777, mean_eps: 0.855437
  43902/300000: episode: 478, duration: 0.471s, episode steps:  63, steps per second: 134, episode reward: -88.123, mean reward: -1.399 [-100.000, 10.742], mean action: 1.444 [0.000, 3.000],  loss: 26.332630, mse: 3547.654126, mean_q: 40.934609, mean_eps: 0.855229
  44012/300000: episode: 479, duration: 0.764s, episode steps: 110, steps per second: 144, episode reward: -121.354, mean reward: -1.103 [-100.000, 16.341], mean action: 1.627 [0.000, 3.000],  loss: 27.414282, mse: 3534.837744, mean_q: 40.627694, mean_eps: 0.854944
  44094/300000: episode: 480, duration: 0.608s, episode steps:  82, steps per second: 135, episode reward: -81.163, mean reward: -0.990 [-100.000,  7.515], mean action: 1.622 [0.000, 3.000],  loss: 21.545796, mse: 3679.668073, mean_q: 39.719935, mean_eps: 0.854627
  44171/300000: episode: 481, duration: 0.596s, episode steps:  77, steps per second: 129, episode reward: -79.890, mean reward: -1.038 [-100.000, 11.570], mean action: 1.675 [0.000, 3.000],  loss: 20.585889, mse: 3703.107856, mean_q: 41.913229, mean_eps: 0.854364
  44250/300000: episode: 482, duration: 0.602s, episode steps:  79, steps per second: 131, episode reward: -112.588, mean reward: -1.425 [-100.000, 14.914], mean action: 1.608 [0.000, 3.000],  loss: 34.748295, mse: 3729.236702, mean_q: 41.245378, mean_eps: 0.854107
  44384/300000: episode: 483, duration: 1.085s, episode steps: 134, steps per second: 123, episode reward: -195.608, mean reward: -1.460 [-100.000, 27.353], mean action: 1.627 [0.000, 3.000],  loss: 29.980910, mse: 3698.248344, mean_q: 41.595235, mean_eps: 0.853756
  44454/300000: episode: 484, duration: 0.629s, episode steps:  70, steps per second: 111, episode reward: -82.908, mean reward: -1.184 [-100.000,  6.668], mean action: 1.529 [0.000, 3.000],  loss: 34.495860, mse: 3672.200579, mean_q: 39.349375, mean_eps: 0.853419
  44527/300000: episode: 485, duration: 0.574s, episode steps:  73, steps per second: 127, episode reward: -30.480, mean reward: -0.418 [-100.000, 16.918], mean action: 1.507 [0.000, 3.000],  loss: 30.551275, mse: 3664.001726, mean_q: 41.386724, mean_eps: 0.853183
  44595/300000: episode: 486, duration: 0.581s, episode steps:  68, steps per second: 117, episode reward: -50.387, mean reward: -0.741 [-100.000,  6.222], mean action: 1.353 [0.000, 3.000],  loss: 28.266706, mse: 3721.684445, mean_q: 40.445212, mean_eps: 0.852950
  44709/300000: episode: 487, duration: 1.065s, episode steps: 114, steps per second: 107, episode reward: -127.508, mean reward: -1.118 [-100.000, 16.430], mean action: 1.667 [0.000, 3.000],  loss: 26.112897, mse: 3768.537112, mean_q: 42.198639, mean_eps: 0.852650
  44783/300000: episode: 488, duration: 0.592s, episode steps:  74, steps per second: 125, episode reward: -47.185, mean reward: -0.638 [-100.000, 18.020], mean action: 1.473 [0.000, 3.000],  loss: 27.822939, mse: 3731.003794, mean_q: 41.315313, mean_eps: 0.852340
  44851/300000: episode: 489, duration: 0.522s, episode steps:  68, steps per second: 130, episode reward: -70.568, mean reward: -1.038 [-100.000,  7.949], mean action: 1.691 [0.000, 3.000],  loss: 23.093925, mse: 3742.059692, mean_q: 41.186330, mean_eps: 0.852106
  44917/300000: episode: 490, duration: 0.520s, episode steps:  66, steps per second: 127, episode reward: -102.999, mean reward: -1.561 [-100.000, 10.133], mean action: 1.591 [0.000, 3.000],  loss: 20.259633, mse: 3701.838878, mean_q: 40.654124, mean_eps: 0.851884
  45040/300000: episode: 491, duration: 1.023s, episode steps: 123, steps per second: 120, episode reward: -103.350, mean reward: -0.840 [-100.000, 10.318], mean action: 1.585 [0.000, 3.000],  loss: 24.735018, mse: 3711.044973, mean_q: 39.736472, mean_eps: 0.851573
  45153/300000: episode: 492, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -94.531, mean reward: -0.837 [-100.000, 10.379], mean action: 1.575 [0.000, 3.000],  loss: 21.026573, mse: 3722.625901, mean_q: 39.933751, mean_eps: 0.851183
  45258/300000: episode: 493, duration: 1.507s, episode steps: 105, steps per second:  70, episode reward: -110.832, mean reward: -1.056 [-100.000,  5.197], mean action: 1.581 [0.000, 3.000],  loss: 24.307349, mse: 3761.060210, mean_q: 42.366041, mean_eps: 0.850823
  45323/300000: episode: 494, duration: 0.609s, episode steps:  65, steps per second: 107, episode reward: -58.572, mean reward: -0.901 [-100.000, 15.131], mean action: 1.431 [0.000, 3.000],  loss: 32.950325, mse: 3729.199542, mean_q: 40.227569, mean_eps: 0.850543
  45404/300000: episode: 495, duration: 0.605s, episode steps:  81, steps per second: 134, episode reward: -137.231, mean reward: -1.694 [-100.000, 16.124], mean action: 1.407 [0.000, 3.000],  loss: 26.492007, mse: 3719.719458, mean_q: 39.572735, mean_eps: 0.850302
  45479/300000: episode: 496, duration: 0.717s, episode steps:  75, steps per second: 105, episode reward: -68.526, mean reward: -0.914 [-100.000, 11.468], mean action: 1.400 [0.000, 3.000],  loss: 27.584023, mse: 3715.821107, mean_q: 39.662966, mean_eps: 0.850045
  45553/300000: episode: 497, duration: 0.870s, episode steps:  74, steps per second:  85, episode reward: -127.233, mean reward: -1.719 [-100.000,  6.434], mean action: 1.432 [0.000, 3.000],  loss: 38.235199, mse: 3774.232491, mean_q: 42.184480, mean_eps: 0.849799
  45641/300000: episode: 498, duration: 0.865s, episode steps:  88, steps per second: 102, episode reward: -65.354, mean reward: -0.743 [-100.000, 12.814], mean action: 1.739 [0.000, 3.000],  loss: 21.288754, mse: 3633.941040, mean_q: 39.171119, mean_eps: 0.849532
  45754/300000: episode: 499, duration: 1.033s, episode steps: 113, steps per second: 109, episode reward: -119.473, mean reward: -1.057 [-100.000,  8.262], mean action: 1.690 [0.000, 3.000],  loss: 28.559839, mse: 3763.508413, mean_q: 40.340732, mean_eps: 0.849200
  45857/300000: episode: 500, duration: 0.727s, episode steps: 103, steps per second: 142, episode reward: -105.900, mean reward: -1.028 [-100.000,  9.592], mean action: 1.553 [0.000, 3.000],  loss: 31.205828, mse: 3701.879110, mean_q: 41.522426, mean_eps: 0.848844
  45935/300000: episode: 501, duration: 0.622s, episode steps:  78, steps per second: 125, episode reward: -104.625, mean reward: -1.341 [-100.000,  6.521], mean action: 1.500 [0.000, 3.000],  loss: 21.725355, mse: 3799.995928, mean_q: 41.514901, mean_eps: 0.848545
  46017/300000: episode: 502, duration: 0.614s, episode steps:  82, steps per second: 133, episode reward: -142.494, mean reward: -1.738 [-100.000, 15.021], mean action: 1.427 [0.000, 3.000],  loss: 24.195936, mse: 3812.115398, mean_q: 40.239286, mean_eps: 0.848281
  46095/300000: episode: 503, duration: 0.594s, episode steps:  78, steps per second: 131, episode reward: -152.340, mean reward: -1.953 [-100.000,  7.731], mean action: 1.551 [0.000, 3.000],  loss: 23.063081, mse: 3778.150485, mean_q: 40.727549, mean_eps: 0.848017
  46163/300000: episode: 504, duration: 0.530s, episode steps:  68, steps per second: 128, episode reward: -185.958, mean reward: -2.735 [-100.000,  6.944], mean action: 1.544 [0.000, 3.000],  loss: 18.491887, mse: 3773.136532, mean_q: 40.862835, mean_eps: 0.847776
  46259/300000: episode: 505, duration: 0.887s, episode steps:  96, steps per second: 108, episode reward: -154.006, mean reward: -1.604 [-100.000,  5.143], mean action: 1.594 [0.000, 3.000],  loss: 20.072592, mse: 3860.809959, mean_q: 40.675384, mean_eps: 0.847505
  46338/300000: episode: 506, duration: 0.628s, episode steps:  79, steps per second: 126, episode reward: -84.724, mean reward: -1.072 [-100.000, 20.596], mean action: 1.646 [0.000, 3.000],  loss: 21.535692, mse: 3747.319954, mean_q: 40.248573, mean_eps: 0.847217
  46404/300000: episode: 507, duration: 0.636s, episode steps:  66, steps per second: 104, episode reward: -139.356, mean reward: -2.111 [-100.000,  7.375], mean action: 1.606 [0.000, 3.000],  loss: 17.543085, mse: 3800.685081, mean_q: 41.752191, mean_eps: 0.846977
  46498/300000: episode: 508, duration: 0.883s, episode steps:  94, steps per second: 107, episode reward: -338.866, mean reward: -3.605 [-100.000,  0.485], mean action: 1.553 [0.000, 3.000],  loss: 32.048082, mse: 3804.449917, mean_q: 41.803764, mean_eps: 0.846713
  46598/300000: episode: 509, duration: 0.927s, episode steps: 100, steps per second: 108, episode reward: -222.456, mean reward: -2.225 [-100.000,  6.265], mean action: 1.510 [0.000, 3.000],  loss: 22.309607, mse: 3796.696157, mean_q: 39.346762, mean_eps: 0.846393
  46707/300000: episode: 510, duration: 0.839s, episode steps: 109, steps per second: 130, episode reward: -149.428, mean reward: -1.371 [-100.000,  7.640], mean action: 1.275 [0.000, 3.000],  loss: 21.409984, mse: 3682.355187, mean_q: 39.303072, mean_eps: 0.846048
  46777/300000: episode: 511, duration: 0.495s, episode steps:  70, steps per second: 141, episode reward: -127.671, mean reward: -1.824 [-100.000,  7.574], mean action: 1.529 [0.000, 3.000],  loss: 35.236909, mse: 3918.018813, mean_q: 42.579863, mean_eps: 0.845753
  46917/300000: episode: 512, duration: 0.955s, episode steps: 140, steps per second: 147, episode reward: -57.876, mean reward: -0.413 [-100.000, 13.914], mean action: 1.500 [0.000, 3.000],  loss: 23.741286, mse: 3825.110582, mean_q: 40.356612, mean_eps: 0.845407
  46986/300000: episode: 513, duration: 0.688s, episode steps:  69, steps per second: 100, episode reward: -132.056, mean reward: -1.914 [-100.000, 15.545], mean action: 1.464 [0.000, 3.000],  loss: 18.649367, mse: 3773.766131, mean_q: 40.551068, mean_eps: 0.845062
  47094/300000: episode: 514, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: -66.091, mean reward: -0.612 [-100.000,  7.205], mean action: 1.454 [0.000, 3.000],  loss: 25.201520, mse: 3584.200340, mean_q: 39.208305, mean_eps: 0.844770
  47185/300000: episode: 515, duration: 0.626s, episode steps:  91, steps per second: 145, episode reward: -228.604, mean reward: -2.512 [-100.000,  4.719], mean action: 1.429 [0.000, 3.000],  loss: 21.162651, mse: 3660.013447, mean_q: 38.803576, mean_eps: 0.844441
  47286/300000: episode: 516, duration: 0.733s, episode steps: 101, steps per second: 138, episode reward: 17.444, mean reward:  0.173 [-100.000, 119.709], mean action: 1.446 [0.000, 3.000],  loss: 30.038936, mse: 3632.870925, mean_q: 37.899092, mean_eps: 0.844124
  47403/300000: episode: 517, duration: 0.823s, episode steps: 117, steps per second: 142, episode reward: -63.735, mean reward: -0.545 [-100.000, 13.419], mean action: 1.453 [0.000, 3.000],  loss: 20.524343, mse: 3746.516631, mean_q: 40.940733, mean_eps: 0.843765
  47466/300000: episode: 518, duration: 0.427s, episode steps:  63, steps per second: 147, episode reward: -106.957, mean reward: -1.698 [-100.000, 12.242], mean action: 1.698 [0.000, 3.000],  loss: 23.460752, mse: 3668.238440, mean_q: 37.391198, mean_eps: 0.843468
  47545/300000: episode: 519, duration: 0.605s, episode steps:  79, steps per second: 131, episode reward: -39.702, mean reward: -0.503 [-100.000, 20.443], mean action: 1.557 [0.000, 3.000],  loss: 25.598955, mse: 3654.863359, mean_q: 38.476745, mean_eps: 0.843233
  47626/300000: episode: 520, duration: 0.687s, episode steps:  81, steps per second: 118, episode reward: -122.393, mean reward: -1.511 [-100.000,  8.440], mean action: 1.568 [0.000, 3.000],  loss: 23.413455, mse: 3667.642008, mean_q: 40.124581, mean_eps: 0.842970
  47740/300000: episode: 521, duration: 0.807s, episode steps: 114, steps per second: 141, episode reward: -167.862, mean reward: -1.472 [-100.000,  7.996], mean action: 1.509 [0.000, 3.000],  loss: 16.286592, mse: 3674.728145, mean_q: 40.582867, mean_eps: 0.842648
  47811/300000: episode: 522, duration: 0.491s, episode steps:  71, steps per second: 144, episode reward: -67.419, mean reward: -0.950 [-100.000, 18.981], mean action: 1.563 [0.000, 3.000],  loss: 16.135147, mse: 3783.120048, mean_q: 39.877407, mean_eps: 0.842342
  47889/300000: episode: 523, duration: 0.568s, episode steps:  78, steps per second: 137, episode reward: -110.571, mean reward: -1.418 [-100.000, 16.198], mean action: 1.628 [0.000, 3.000],  loss: 29.457083, mse: 3729.659859, mean_q: 40.811172, mean_eps: 0.842097
  48030/300000: episode: 524, duration: 1.019s, episode steps: 141, steps per second: 138, episode reward: -3.651, mean reward: -0.026 [-100.000, 98.325], mean action: 1.645 [0.000, 3.000],  loss: 16.212254, mse: 3718.144852, mean_q: 39.404553, mean_eps: 0.841735
  48129/300000: episode: 525, duration: 0.920s, episode steps:  99, steps per second: 108, episode reward: -108.528, mean reward: -1.096 [-100.000,  5.057], mean action: 1.525 [0.000, 3.000],  loss: 29.946747, mse: 3761.127979, mean_q: 41.459056, mean_eps: 0.841339
  48236/300000: episode: 526, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -218.070, mean reward: -2.038 [-100.000, 13.624], mean action: 1.495 [0.000, 3.000],  loss: 18.868647, mse: 3750.942892, mean_q: 40.263370, mean_eps: 0.840999
  48330/300000: episode: 527, duration: 0.659s, episode steps:  94, steps per second: 143, episode reward: -205.548, mean reward: -2.187 [-100.000,  8.138], mean action: 1.511 [0.000, 3.000],  loss: 15.589923, mse: 3764.435620, mean_q: 40.790653, mean_eps: 0.840668
  48434/300000: episode: 528, duration: 0.774s, episode steps: 104, steps per second: 134, episode reward: -98.342, mean reward: -0.946 [-100.000,  6.933], mean action: 1.269 [0.000, 3.000],  loss: 21.083103, mse: 3838.651304, mean_q: 40.611021, mean_eps: 0.840341
  48516/300000: episode: 529, duration: 0.636s, episode steps:  82, steps per second: 129, episode reward: -92.392, mean reward: -1.127 [-100.000, 16.673], mean action: 1.537 [0.000, 3.000],  loss: 21.357355, mse: 3756.952518, mean_q: 41.226834, mean_eps: 0.840034
  48625/300000: episode: 530, duration: 0.765s, episode steps: 109, steps per second: 142, episode reward: -106.522, mean reward: -0.977 [-100.000,  5.070], mean action: 1.358 [0.000, 3.000],  loss: 25.377760, mse: 3790.629278, mean_q: 40.653643, mean_eps: 0.839719
  48732/300000: episode: 531, duration: 0.756s, episode steps: 107, steps per second: 142, episode reward: -177.963, mean reward: -1.663 [-100.000,  5.681], mean action: 1.411 [0.000, 3.000],  loss: 31.605582, mse: 3760.202721, mean_q: 41.515511, mean_eps: 0.839363
  48814/300000: episode: 532, duration: 0.583s, episode steps:  82, steps per second: 141, episode reward: -94.613, mean reward: -1.154 [-100.000,  7.153], mean action: 1.524 [0.000, 3.000],  loss: 31.334231, mse: 3804.718917, mean_q: 40.650934, mean_eps: 0.839051
  48880/300000: episode: 533, duration: 0.455s, episode steps:  66, steps per second: 145, episode reward: -52.616, mean reward: -0.797 [-100.000,  7.480], mean action: 1.576 [0.000, 3.000],  loss: 12.514977, mse: 3750.824652, mean_q: 41.105726, mean_eps: 0.838807
  48989/300000: episode: 534, duration: 0.747s, episode steps: 109, steps per second: 146, episode reward: -10.108, mean reward: -0.093 [-100.000, 118.331], mean action: 1.532 [0.000, 3.000],  loss: 20.799354, mse: 3643.918811, mean_q: 40.008589, mean_eps: 0.838518
  49089/300000: episode: 535, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: -61.128, mean reward: -0.611 [-100.000, 86.650], mean action: 1.610 [0.000, 3.000],  loss: 28.835467, mse: 3787.971960, mean_q: 40.223173, mean_eps: 0.838173
  49216/300000: episode: 536, duration: 0.865s, episode steps: 127, steps per second: 147, episode reward: -82.250, mean reward: -0.648 [-100.000, 29.542], mean action: 1.512 [0.000, 3.000],  loss: 32.966035, mse: 3746.753704, mean_q: 41.101702, mean_eps: 0.837798
  49294/300000: episode: 537, duration: 0.592s, episode steps:  78, steps per second: 132, episode reward: -116.709, mean reward: -1.496 [-100.000, 14.484], mean action: 1.513 [0.000, 3.000],  loss: 29.373761, mse: 3808.936076, mean_q: 42.097491, mean_eps: 0.837460
  49385/300000: episode: 538, duration: 0.742s, episode steps:  91, steps per second: 123, episode reward: -126.643, mean reward: -1.392 [-100.000,  8.076], mean action: 1.231 [0.000, 3.000],  loss: 27.049757, mse: 3759.850626, mean_q: 41.896699, mean_eps: 0.837181
  49465/300000: episode: 539, duration: 0.590s, episode steps:  80, steps per second: 136, episode reward: -95.823, mean reward: -1.198 [-100.000,  6.488], mean action: 1.712 [0.000, 3.000],  loss: 18.504764, mse: 3662.513400, mean_q: 39.704787, mean_eps: 0.836899
  49588/300000: episode: 540, duration: 0.902s, episode steps: 123, steps per second: 136, episode reward: -80.275, mean reward: -0.653 [-100.000,  9.680], mean action: 1.374 [0.000, 3.000],  loss: 35.449261, mse: 3751.817391, mean_q: 39.201361, mean_eps: 0.836564
  49676/300000: episode: 541, duration: 0.697s, episode steps:  88, steps per second: 126, episode reward: -155.370, mean reward: -1.766 [-100.000,  6.192], mean action: 1.534 [0.000, 3.000],  loss: 25.340068, mse: 3686.787975, mean_q: 41.922698, mean_eps: 0.836216
  49791/300000: episode: 542, duration: 0.883s, episode steps: 115, steps per second: 130, episode reward: -190.854, mean reward: -1.660 [-100.000,  2.599], mean action: 1.617 [0.000, 3.000],  loss: 20.247213, mse: 3755.571705, mean_q: 40.879541, mean_eps: 0.835881
  49870/300000: episode: 543, duration: 0.581s, episode steps:  79, steps per second: 136, episode reward: -107.947, mean reward: -1.366 [-100.000, 10.371], mean action: 1.304 [0.000, 3.000],  loss: 25.032065, mse: 3700.518966, mean_q: 40.266214, mean_eps: 0.835561
  49965/300000: episode: 544, duration: 0.693s, episode steps:  95, steps per second: 137, episode reward: -130.554, mean reward: -1.374 [-100.000,  5.047], mean action: 1.411 [0.000, 3.000],  loss: 37.814502, mse: 3790.693038, mean_q: 40.926597, mean_eps: 0.835274
  50065/300000: episode: 545, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: -157.729, mean reward: -1.577 [-100.000,  4.688], mean action: 1.540 [0.000, 3.000],  loss: 19.804134, mse: 3917.658118, mean_q: 40.844405, mean_eps: 0.834952
  50130/300000: episode: 546, duration: 0.445s, episode steps:  65, steps per second: 146, episode reward: -84.487, mean reward: -1.300 [-100.000, 16.726], mean action: 1.492 [0.000, 3.000],  loss: 22.564149, mse: 3984.484687, mean_q: 42.317602, mean_eps: 0.834680
  50268/300000: episode: 547, duration: 1.161s, episode steps: 138, steps per second: 119, episode reward: -96.180, mean reward: -0.697 [-100.000, 11.924], mean action: 1.514 [0.000, 3.000],  loss: 20.092119, mse: 3961.025534, mean_q: 41.581981, mean_eps: 0.834345
  50358/300000: episode: 548, duration: 0.638s, episode steps:  90, steps per second: 141, episode reward: -17.354, mean reward: -0.193 [-100.000, 12.636], mean action: 1.533 [0.000, 3.000],  loss: 29.546367, mse: 3983.743405, mean_q: 42.589973, mean_eps: 0.833969
  50453/300000: episode: 549, duration: 0.637s, episode steps:  95, steps per second: 149, episode reward: -140.772, mean reward: -1.482 [-100.000,  7.016], mean action: 1.421 [0.000, 3.000],  loss: 22.154944, mse: 3932.546451, mean_q: 41.387005, mean_eps: 0.833664
  50547/300000: episode: 550, duration: 0.679s, episode steps:  94, steps per second: 138, episode reward: -58.842, mean reward: -0.626 [-100.000, 110.581], mean action: 1.468 [0.000, 3.000],  loss: 33.858643, mse: 3927.089841, mean_q: 41.623978, mean_eps: 0.833352
  50653/300000: episode: 551, duration: 0.752s, episode steps: 106, steps per second: 141, episode reward: -126.036, mean reward: -1.189 [-100.000, 12.172], mean action: 1.642 [0.000, 3.000],  loss: 22.020205, mse: 3980.042784, mean_q: 42.202332, mean_eps: 0.833022
  50715/300000: episode: 552, duration: 0.420s, episode steps:  62, steps per second: 148, episode reward: -97.563, mean reward: -1.574 [-100.000, 10.465], mean action: 1.516 [0.000, 3.000],  loss: 18.286005, mse: 3924.495271, mean_q: 41.561672, mean_eps: 0.832744
  50775/300000: episode: 553, duration: 0.417s, episode steps:  60, steps per second: 144, episode reward: -104.763, mean reward: -1.746 [-100.000,  8.614], mean action: 1.533 [0.000, 3.000],  loss: 26.262121, mse: 3878.865401, mean_q: 41.155751, mean_eps: 0.832543
  50852/300000: episode: 554, duration: 0.537s, episode steps:  77, steps per second: 143, episode reward: -108.856, mean reward: -1.414 [-100.000,  6.608], mean action: 1.571 [0.000, 3.000],  loss: 20.800038, mse: 3921.719819, mean_q: 41.579606, mean_eps: 0.832317
  50920/300000: episode: 555, duration: 0.479s, episode steps:  68, steps per second: 142, episode reward: -73.007, mean reward: -1.074 [-100.000, 64.934], mean action: 1.500 [0.000, 3.000],  loss: 21.564336, mse: 3971.712779, mean_q: 43.245641, mean_eps: 0.832078
  50986/300000: episode: 556, duration: 0.444s, episode steps:  66, steps per second: 149, episode reward: -138.930, mean reward: -2.105 [-100.000,  9.962], mean action: 1.621 [0.000, 3.000],  loss: 20.829759, mse: 3911.525335, mean_q: 41.208013, mean_eps: 0.831857
  51110/300000: episode: 557, duration: 0.856s, episode steps: 124, steps per second: 145, episode reward: -95.174, mean reward: -0.768 [-100.000,  7.039], mean action: 1.532 [0.000, 3.000],  loss: 19.210084, mse: 4078.805894, mean_q: 44.298695, mean_eps: 0.831543
  51201/300000: episode: 558, duration: 0.655s, episode steps:  91, steps per second: 139, episode reward: -71.348, mean reward: -0.784 [-100.000, 21.435], mean action: 1.407 [0.000, 3.000],  loss: 14.053745, mse: 3945.220397, mean_q: 42.395114, mean_eps: 0.831188
  51316/300000: episode: 559, duration: 0.784s, episode steps: 115, steps per second: 147, episode reward: -80.973, mean reward: -0.704 [-100.000, 16.297], mean action: 1.583 [0.000, 3.000],  loss: 14.188881, mse: 4011.291493, mean_q: 43.095425, mean_eps: 0.830849
  51446/300000: episode: 560, duration: 1.069s, episode steps: 130, steps per second: 122, episode reward: -89.005, mean reward: -0.685 [-100.000, 18.844], mean action: 1.554 [0.000, 3.000],  loss: 22.196715, mse: 3997.097322, mean_q: 42.364101, mean_eps: 0.830444
  51509/300000: episode: 561, duration: 0.500s, episode steps:  63, steps per second: 126, episode reward: -88.440, mean reward: -1.404 [-100.000, 11.541], mean action: 1.571 [0.000, 3.000],  loss: 14.655824, mse: 4078.560504, mean_q: 43.842842, mean_eps: 0.830126
  51594/300000: episode: 562, duration: 0.711s, episode steps:  85, steps per second: 119, episode reward: -86.934, mean reward: -1.023 [-100.000, 15.961], mean action: 1.824 [0.000, 3.000],  loss: 17.662633, mse: 3993.262983, mean_q: 41.229490, mean_eps: 0.829882
  51692/300000: episode: 563, duration: 0.783s, episode steps:  98, steps per second: 125, episode reward: -221.483, mean reward: -2.260 [-100.000, 13.469], mean action: 1.643 [0.000, 3.000],  loss: 25.476411, mse: 3986.206919, mean_q: 41.936240, mean_eps: 0.829580
  51754/300000: episode: 564, duration: 0.506s, episode steps:  62, steps per second: 122, episode reward: -92.008, mean reward: -1.484 [-100.000, 16.946], mean action: 1.645 [0.000, 3.000],  loss: 30.484545, mse: 4048.885955, mean_q: 42.026467, mean_eps: 0.829316
  51841/300000: episode: 565, duration: 0.720s, episode steps:  87, steps per second: 121, episode reward: -62.625, mean reward: -0.720 [-100.000, 22.472], mean action: 1.793 [0.000, 3.000],  loss: 21.589436, mse: 4025.966039, mean_q: 42.917621, mean_eps: 0.829070
  51975/300000: episode: 566, duration: 1.088s, episode steps: 134, steps per second: 123, episode reward: -113.571, mean reward: -0.848 [-100.000,  6.118], mean action: 1.627 [0.000, 3.000],  loss: 21.844785, mse: 4017.458478, mean_q: 43.296836, mean_eps: 0.828705
  52044/300000: episode: 567, duration: 0.567s, episode steps:  69, steps per second: 122, episode reward: -136.867, mean reward: -1.984 [-100.000,  9.092], mean action: 1.449 [0.000, 3.000],  loss: 28.645183, mse: 4145.665315, mean_q: 45.550098, mean_eps: 0.828370
  52114/300000: episode: 568, duration: 0.547s, episode steps:  70, steps per second: 128, episode reward: -109.475, mean reward: -1.564 [-100.000,  8.237], mean action: 1.629 [0.000, 3.000],  loss: 13.192897, mse: 4220.613215, mean_q: 46.486169, mean_eps: 0.828141
  52217/300000: episode: 569, duration: 0.815s, episode steps: 103, steps per second: 126, episode reward: -143.076, mean reward: -1.389 [-100.000,  5.277], mean action: 1.369 [0.000, 3.000],  loss: 16.294777, mse: 4229.871392, mean_q: 44.644649, mean_eps: 0.827856
  52291/300000: episode: 570, duration: 0.625s, episode steps:  74, steps per second: 118, episode reward: -88.596, mean reward: -1.197 [-100.000,  9.041], mean action: 1.419 [0.000, 3.000],  loss: 17.416348, mse: 4105.616798, mean_q: 43.642281, mean_eps: 0.827563
  52417/300000: episode: 571, duration: 0.988s, episode steps: 126, steps per second: 127, episode reward: -194.867, mean reward: -1.547 [-100.000,  5.566], mean action: 1.492 [0.000, 3.000],  loss: 23.200589, mse: 4213.232445, mean_q: 46.377856, mean_eps: 0.827233
  52515/300000: episode: 572, duration: 0.700s, episode steps:  98, steps per second: 140, episode reward: -137.260, mean reward: -1.401 [-100.000,  8.581], mean action: 1.469 [0.000, 3.000],  loss: 18.151972, mse: 4137.260989, mean_q: 44.016233, mean_eps: 0.826864
  52612/300000: episode: 573, duration: 0.836s, episode steps:  97, steps per second: 116, episode reward: -104.991, mean reward: -1.082 [-100.000,  7.808], mean action: 1.433 [0.000, 3.000],  loss: 22.218976, mse: 4217.894103, mean_q: 44.437338, mean_eps: 0.826542
  52672/300000: episode: 574, duration: 0.539s, episode steps:  60, steps per second: 111, episode reward: -92.110, mean reward: -1.535 [-100.000, 26.642], mean action: 1.683 [0.000, 3.000],  loss: 10.057807, mse: 4147.844189, mean_q: 44.549900, mean_eps: 0.826283
  52771/300000: episode: 575, duration: 0.991s, episode steps:  99, steps per second: 100, episode reward: -138.785, mean reward: -1.402 [-100.000,  6.648], mean action: 1.576 [0.000, 3.000],  loss: 20.635890, mse: 4246.979273, mean_q: 45.038824, mean_eps: 0.826021
  52908/300000: episode: 576, duration: 1.185s, episode steps: 137, steps per second: 116, episode reward: -139.439, mean reward: -1.018 [-100.000, 35.276], mean action: 1.358 [0.000, 3.000],  loss: 16.146177, mse: 4181.268589, mean_q: 44.333867, mean_eps: 0.825631
  52974/300000: episode: 577, duration: 0.634s, episode steps:  66, steps per second: 104, episode reward: -108.183, mean reward: -1.639 [-100.000,  8.918], mean action: 1.227 [0.000, 3.000],  loss: 30.288944, mse: 4179.634906, mean_q: 44.527456, mean_eps: 0.825296
  53060/300000: episode: 578, duration: 0.781s, episode steps:  86, steps per second: 110, episode reward: -124.051, mean reward: -1.442 [-100.000, 16.196], mean action: 1.314 [0.000, 3.000],  loss: 29.509392, mse: 4252.114897, mean_q: 46.132255, mean_eps: 0.825046
  53189/300000: episode: 579, duration: 1.145s, episode steps: 129, steps per second: 113, episode reward: -108.490, mean reward: -0.841 [-100.000, 21.094], mean action: 1.674 [0.000, 3.000],  loss: 22.148325, mse: 4234.265890, mean_q: 44.681075, mean_eps: 0.824691
  53264/300000: episode: 580, duration: 0.647s, episode steps:  75, steps per second: 116, episode reward: -121.437, mean reward: -1.619 [-100.000, 18.805], mean action: 1.253 [0.000, 3.000],  loss: 25.214148, mse: 4146.229170, mean_q: 44.165346, mean_eps: 0.824354
  53364/300000: episode: 581, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -140.373, mean reward: -1.404 [-100.000,  6.881], mean action: 1.670 [0.000, 3.000],  loss: 19.759762, mse: 4238.750195, mean_q: 45.306367, mean_eps: 0.824065
  53441/300000: episode: 582, duration: 0.607s, episode steps:  77, steps per second: 127, episode reward: -128.502, mean reward: -1.669 [-100.000, 26.222], mean action: 1.649 [0.000, 3.000],  loss: 16.141302, mse: 4259.735434, mean_q: 45.702401, mean_eps: 0.823773
  53518/300000: episode: 583, duration: 0.617s, episode steps:  77, steps per second: 125, episode reward: -106.658, mean reward: -1.385 [-100.000, 12.647], mean action: 1.494 [0.000, 3.000],  loss: 16.809916, mse: 4220.978240, mean_q: 45.592122, mean_eps: 0.823519
  53592/300000: episode: 584, duration: 0.627s, episode steps:  74, steps per second: 118, episode reward: -103.045, mean reward: -1.392 [-100.000,  8.601], mean action: 1.459 [0.000, 3.000],  loss: 29.843310, mse: 4096.813417, mean_q: 44.665551, mean_eps: 0.823270
  53662/300000: episode: 585, duration: 0.514s, episode steps:  70, steps per second: 136, episode reward: -29.865, mean reward: -0.427 [-100.000, 17.534], mean action: 1.686 [0.000, 3.000],  loss: 21.772157, mse: 4137.005580, mean_q: 43.643015, mean_eps: 0.823033
  53735/300000: episode: 586, duration: 0.519s, episode steps:  73, steps per second: 141, episode reward: -137.112, mean reward: -1.878 [-100.000, 22.146], mean action: 1.493 [0.000, 3.000],  loss: 20.670853, mse: 4182.324342, mean_q: 43.269652, mean_eps: 0.822797
  53837/300000: episode: 587, duration: 0.738s, episode steps: 102, steps per second: 138, episode reward: -105.522, mean reward: -1.035 [-100.000,  7.244], mean action: 1.588 [0.000, 3.000],  loss: 19.361322, mse: 4293.111795, mean_q: 45.511504, mean_eps: 0.822508
  53934/300000: episode: 588, duration: 0.666s, episode steps:  97, steps per second: 146, episode reward: -93.128, mean reward: -0.960 [-100.000,  6.481], mean action: 1.381 [0.000, 3.000],  loss: 20.728098, mse: 4211.272625, mean_q: 44.892974, mean_eps: 0.822180
  54043/300000: episode: 589, duration: 0.757s, episode steps: 109, steps per second: 144, episode reward: -127.580, mean reward: -1.170 [-100.000,  7.157], mean action: 1.376 [0.000, 3.000],  loss: 19.044399, mse: 4244.784516, mean_q: 45.044916, mean_eps: 0.821840
  54110/300000: episode: 590, duration: 0.495s, episode steps:  67, steps per second: 135, episode reward: -80.242, mean reward: -1.198 [-100.000, 10.452], mean action: 1.507 [0.000, 3.000],  loss: 15.559862, mse: 4374.119399, mean_q: 46.936582, mean_eps: 0.821549
  54209/300000: episode: 591, duration: 0.709s, episode steps:  99, steps per second: 140, episode reward: -481.973, mean reward: -4.868 [-100.000, 82.570], mean action: 1.596 [0.000, 3.000],  loss: 25.113055, mse: 4279.989090, mean_q: 45.337334, mean_eps: 0.821275
  54291/300000: episode: 592, duration: 0.561s, episode steps:  82, steps per second: 146, episode reward: -158.841, mean reward: -1.937 [-100.000,  6.016], mean action: 1.610 [0.000, 3.000],  loss: 15.681229, mse: 4323.892781, mean_q: 45.881245, mean_eps: 0.820977
  54365/300000: episode: 593, duration: 0.579s, episode steps:  74, steps per second: 128, episode reward: -100.749, mean reward: -1.361 [-100.000,  6.144], mean action: 1.541 [0.000, 3.000],  loss: 21.173023, mse: 4402.050999, mean_q: 47.746420, mean_eps: 0.820719
  54437/300000: episode: 594, duration: 0.595s, episode steps:  72, steps per second: 121, episode reward: -99.835, mean reward: -1.387 [-100.000,  6.078], mean action: 1.583 [0.000, 3.000],  loss: 30.479805, mse: 4253.961395, mean_q: 45.032727, mean_eps: 0.820478
  54541/300000: episode: 595, duration: 0.777s, episode steps: 104, steps per second: 134, episode reward: -117.895, mean reward: -1.134 [-100.000, 15.639], mean action: 1.481 [0.000, 3.000],  loss: 19.991610, mse: 4273.758625, mean_q: 46.453211, mean_eps: 0.820188
  54641/300000: episode: 596, duration: 0.749s, episode steps: 100, steps per second: 133, episode reward: -98.030, mean reward: -0.980 [-100.000, 11.571], mean action: 1.390 [0.000, 3.000],  loss: 13.867860, mse: 4220.899775, mean_q: 45.273246, mean_eps: 0.819851
  54703/300000: episode: 597, duration: 0.478s, episode steps:  62, steps per second: 130, episode reward: -205.085, mean reward: -3.308 [-100.000,  3.160], mean action: 1.500 [0.000, 3.000],  loss: 14.213388, mse: 4227.412527, mean_q: 45.801532, mean_eps: 0.819584
  54822/300000: episode: 598, duration: 0.852s, episode steps: 119, steps per second: 140, episode reward: -43.189, mean reward: -0.363 [-100.000, 87.159], mean action: 1.538 [0.000, 3.000],  loss: 18.046834, mse: 4279.700852, mean_q: 44.491000, mean_eps: 0.819285
  54935/300000: episode: 599, duration: 0.773s, episode steps: 113, steps per second: 146, episode reward: -151.393, mean reward: -1.340 [-100.000,  7.168], mean action: 1.487 [0.000, 3.000],  loss: 23.009286, mse: 4302.099227, mean_q: 45.764578, mean_eps: 0.818903
  55040/300000: episode: 600, duration: 0.789s, episode steps: 105, steps per second: 133, episode reward: -170.607, mean reward: -1.625 [-100.000,  3.566], mean action: 1.562 [0.000, 3.000],  loss: 27.285421, mse: 4428.336768, mean_q: 45.888771, mean_eps: 0.818543
  55133/300000: episode: 601, duration: 0.654s, episode steps:  93, steps per second: 142, episode reward: -138.493, mean reward: -1.489 [-100.000,  7.819], mean action: 1.333 [0.000, 3.000],  loss: 23.933487, mse: 4477.709874, mean_q: 47.508974, mean_eps: 0.818216
  55196/300000: episode: 602, duration: 0.452s, episode steps:  63, steps per second: 139, episode reward: -77.278, mean reward: -1.227 [-100.000,  6.562], mean action: 1.524 [0.000, 3.000],  loss: 32.465850, mse: 4522.930897, mean_q: 48.536037, mean_eps: 0.817959
  55269/300000: episode: 603, duration: 0.515s, episode steps:  73, steps per second: 142, episode reward: -62.285, mean reward: -0.853 [-100.000, 11.844], mean action: 1.507 [0.000, 3.000],  loss: 22.373508, mse: 4471.177698, mean_q: 48.170084, mean_eps: 0.817734
  55379/300000: episode: 604, duration: 0.844s, episode steps: 110, steps per second: 130, episode reward: -197.609, mean reward: -1.796 [-100.000,  2.110], mean action: 1.464 [0.000, 3.000],  loss: 24.706899, mse: 4460.595390, mean_q: 48.460038, mean_eps: 0.817432
  55458/300000: episode: 605, duration: 0.549s, episode steps:  79, steps per second: 144, episode reward: -103.178, mean reward: -1.306 [-100.000,  9.811], mean action: 1.241 [0.000, 3.000],  loss: 14.748796, mse: 4413.041856, mean_q: 47.122876, mean_eps: 0.817121
  55573/300000: episode: 606, duration: 0.792s, episode steps: 115, steps per second: 145, episode reward: -109.416, mean reward: -0.951 [-100.000, 32.460], mean action: 1.600 [0.000, 3.000],  loss: 22.222395, mse: 4430.149775, mean_q: 46.548203, mean_eps: 0.816801
  55678/300000: episode: 607, duration: 0.789s, episode steps: 105, steps per second: 133, episode reward: -183.256, mean reward: -1.745 [-100.000,  2.995], mean action: 1.457 [0.000, 3.000],  loss: 23.218976, mse: 4464.453276, mean_q: 47.264333, mean_eps: 0.816438
  55754/300000: episode: 608, duration: 0.534s, episode steps:  76, steps per second: 142, episode reward: -99.740, mean reward: -1.312 [-100.000, 12.855], mean action: 1.579 [0.000, 3.000],  loss: 21.526551, mse: 4345.661493, mean_q: 47.098878, mean_eps: 0.816139
  55847/300000: episode: 609, duration: 0.766s, episode steps:  93, steps per second: 121, episode reward: -188.178, mean reward: -2.023 [-100.000,  6.150], mean action: 1.387 [0.000, 3.000],  loss: 23.732343, mse: 4344.062119, mean_q: 46.523409, mean_eps: 0.815860
  55960/300000: episode: 610, duration: 0.871s, episode steps: 113, steps per second: 130, episode reward: -0.862, mean reward: -0.008 [-100.000, 94.081], mean action: 1.779 [0.000, 3.000],  loss: 24.092021, mse: 4405.719027, mean_q: 47.232063, mean_eps: 0.815520
  56071/300000: episode: 611, duration: 0.778s, episode steps: 111, steps per second: 143, episode reward: -116.330, mean reward: -1.048 [-100.000,  9.747], mean action: 1.387 [0.000, 3.000],  loss: 25.970488, mse: 4478.010208, mean_q: 48.747166, mean_eps: 0.815151
  56177/300000: episode: 612, duration: 0.811s, episode steps: 106, steps per second: 131, episode reward: -88.036, mean reward: -0.831 [-100.000, 11.357], mean action: 1.651 [0.000, 3.000],  loss: 27.864701, mse: 4432.715823, mean_q: 47.938657, mean_eps: 0.814792
  56298/300000: episode: 613, duration: 1.393s, episode steps: 121, steps per second:  87, episode reward: -109.700, mean reward: -0.907 [-100.000, 10.264], mean action: 1.702 [0.000, 3.000],  loss: 23.163425, mse: 4439.370650, mean_q: 47.421771, mean_eps: 0.814418
  56430/300000: episode: 614, duration: 1.449s, episode steps: 132, steps per second:  91, episode reward: -50.471, mean reward: -0.382 [-100.000, 16.408], mean action: 1.576 [0.000, 3.000],  loss: 25.870517, mse: 4308.861602, mean_q: 47.581615, mean_eps: 0.814000
  56513/300000: episode: 615, duration: 0.624s, episode steps:  83, steps per second: 133, episode reward: -128.427, mean reward: -1.547 [-100.000,  4.699], mean action: 1.410 [0.000, 3.000],  loss: 18.976304, mse: 4360.674643, mean_q: 47.945325, mean_eps: 0.813646
  56627/300000: episode: 616, duration: 0.981s, episode steps: 114, steps per second: 116, episode reward: -143.583, mean reward: -1.259 [-100.000,  5.071], mean action: 1.482 [0.000, 3.000],  loss: 21.618471, mse: 4352.743959, mean_q: 46.060195, mean_eps: 0.813321
  56748/300000: episode: 617, duration: 0.874s, episode steps: 121, steps per second: 138, episode reward: -300.926, mean reward: -2.487 [-100.000, 69.967], mean action: 1.711 [0.000, 3.000],  loss: 21.156288, mse: 4380.540098, mean_q: 47.159727, mean_eps: 0.812933
  56829/300000: episode: 618, duration: 0.583s, episode steps:  81, steps per second: 139, episode reward: -111.212, mean reward: -1.373 [-100.000,  7.740], mean action: 1.543 [0.000, 3.000],  loss: 22.706988, mse: 4281.220058, mean_q: 47.497726, mean_eps: 0.812600
  56929/300000: episode: 619, duration: 1.211s, episode steps: 100, steps per second:  83, episode reward: -226.817, mean reward: -2.268 [-100.000, 89.335], mean action: 1.470 [0.000, 3.000],  loss: 19.362779, mse: 4387.981165, mean_q: 47.157833, mean_eps: 0.812301
  57014/300000: episode: 620, duration: 0.647s, episode steps:  85, steps per second: 131, episode reward: -90.748, mean reward: -1.068 [-100.000, 18.832], mean action: 1.659 [0.000, 3.000],  loss: 14.955452, mse: 4378.296309, mean_q: 46.758958, mean_eps: 0.811996
  57090/300000: episode: 621, duration: 0.606s, episode steps:  76, steps per second: 125, episode reward: 32.601, mean reward:  0.429 [-100.000, 112.666], mean action: 1.263 [0.000, 3.000],  loss: 25.268370, mse: 4422.946238, mean_q: 48.420295, mean_eps: 0.811730
  57205/300000: episode: 622, duration: 1.051s, episode steps: 115, steps per second: 109, episode reward: -89.187, mean reward: -0.776 [-100.000, 11.163], mean action: 1.496 [0.000, 3.000],  loss: 25.031844, mse: 4476.094578, mean_q: 48.816698, mean_eps: 0.811415
  57304/300000: episode: 623, duration: 0.812s, episode steps:  99, steps per second: 122, episode reward: -124.476, mean reward: -1.257 [-100.000,  6.359], mean action: 1.657 [0.000, 3.000],  loss: 16.743923, mse: 4537.059896, mean_q: 49.070657, mean_eps: 0.811062
  57372/300000: episode: 624, duration: 0.574s, episode steps:  68, steps per second: 119, episode reward: -108.699, mean reward: -1.599 [-100.000, 15.409], mean action: 1.471 [0.000, 3.000],  loss: 25.354753, mse: 4360.294053, mean_q: 48.325188, mean_eps: 0.810786
  57476/300000: episode: 625, duration: 0.952s, episode steps: 104, steps per second: 109, episode reward: -189.783, mean reward: -1.825 [-100.000,  6.342], mean action: 1.577 [0.000, 3.000],  loss: 19.606248, mse: 4579.439129, mean_q: 48.722808, mean_eps: 0.810502
  57568/300000: episode: 626, duration: 0.716s, episode steps:  92, steps per second: 128, episode reward: -298.646, mean reward: -3.246 [-100.000, 124.629], mean action: 1.435 [0.000, 3.000],  loss: 16.706085, mse: 4523.755262, mean_q: 47.659877, mean_eps: 0.810179
  57673/300000: episode: 627, duration: 0.978s, episode steps: 105, steps per second: 107, episode reward: -56.487, mean reward: -0.538 [-100.000, 29.910], mean action: 1.495 [0.000, 3.000],  loss: 23.761795, mse: 4522.251486, mean_q: 48.723035, mean_eps: 0.809854
  57773/300000: episode: 628, duration: 0.839s, episode steps: 100, steps per second: 119, episode reward: -138.937, mean reward: -1.389 [-100.000,  9.327], mean action: 1.660 [0.000, 3.000],  loss: 17.396775, mse: 4495.052600, mean_q: 48.223542, mean_eps: 0.809516
  57852/300000: episode: 629, duration: 0.604s, episode steps:  79, steps per second: 131, episode reward: -78.094, mean reward: -0.989 [-100.000, 10.793], mean action: 1.658 [0.000, 3.000],  loss: 28.104058, mse: 4523.440572, mean_q: 49.495044, mean_eps: 0.809220
  57958/300000: episode: 630, duration: 0.809s, episode steps: 106, steps per second: 131, episode reward: -77.149, mean reward: -0.728 [-100.000,  8.055], mean action: 1.575 [0.000, 3.000],  loss: 16.296920, mse: 4540.046875, mean_q: 49.776427, mean_eps: 0.808915
  58032/300000: episode: 631, duration: 0.538s, episode steps:  74, steps per second: 138, episode reward: -75.004, mean reward: -1.014 [-100.000,  7.889], mean action: 1.622 [0.000, 3.000],  loss: 17.180092, mse: 4444.435547, mean_q: 47.244395, mean_eps: 0.808618
  58109/300000: episode: 632, duration: 0.594s, episode steps:  77, steps per second: 130, episode reward: -75.545, mean reward: -0.981 [-100.000,  6.506], mean action: 1.364 [0.000, 3.000],  loss: 15.687650, mse: 4383.128120, mean_q: 46.277168, mean_eps: 0.808369
  58188/300000: episode: 633, duration: 0.647s, episode steps:  79, steps per second: 122, episode reward: -97.265, mean reward: -1.231 [-100.000, 15.171], mean action: 1.671 [0.000, 3.000],  loss: 35.836327, mse: 4557.047966, mean_q: 48.173795, mean_eps: 0.808112
  58294/300000: episode: 634, duration: 0.823s, episode steps: 106, steps per second: 129, episode reward: -116.882, mean reward: -1.103 [-100.000,  5.316], mean action: 1.396 [0.000, 3.000],  loss: 20.643711, mse: 4489.435358, mean_q: 47.673387, mean_eps: 0.807806
  58370/300000: episode: 635, duration: 0.620s, episode steps:  76, steps per second: 123, episode reward: -58.228, mean reward: -0.766 [-100.000, 10.446], mean action: 1.421 [0.000, 3.000],  loss: 14.982363, mse: 4460.913536, mean_q: 47.724354, mean_eps: 0.807506
  58482/300000: episode: 636, duration: 0.891s, episode steps: 112, steps per second: 126, episode reward: -105.454, mean reward: -0.942 [-100.000,  7.109], mean action: 1.509 [0.000, 3.000],  loss: 13.032719, mse: 4545.007797, mean_q: 49.039205, mean_eps: 0.807196
  58569/300000: episode: 637, duration: 0.661s, episode steps:  87, steps per second: 132, episode reward: -25.763, mean reward: -0.296 [-100.000,  7.208], mean action: 1.598 [0.000, 3.000],  loss: 22.969920, mse: 4477.820607, mean_q: 48.199789, mean_eps: 0.806867
  58685/300000: episode: 638, duration: 0.859s, episode steps: 116, steps per second: 135, episode reward: -155.460, mean reward: -1.340 [-100.000, 10.011], mean action: 1.440 [0.000, 3.000],  loss: 16.620015, mse: 4458.827611, mean_q: 46.177761, mean_eps: 0.806533
  58811/300000: episode: 639, duration: 1.054s, episode steps: 126, steps per second: 120, episode reward: -132.834, mean reward: -1.054 [-100.000,  7.250], mean action: 1.460 [0.000, 3.000],  loss: 11.130685, mse: 4434.173098, mean_q: 47.215517, mean_eps: 0.806133
  58889/300000: episode: 640, duration: 0.669s, episode steps:  78, steps per second: 117, episode reward: -91.688, mean reward: -1.175 [-100.000, 17.297], mean action: 1.385 [0.000, 3.000],  loss: 16.187069, mse: 4450.145921, mean_q: 47.203114, mean_eps: 0.805797
  58972/300000: episode: 641, duration: 0.645s, episode steps:  83, steps per second: 129, episode reward: -86.096, mean reward: -1.037 [-100.000, 12.980], mean action: 1.434 [0.000, 3.000],  loss: 19.224738, mse: 4522.950942, mean_q: 49.457398, mean_eps: 0.805531
  59119/300000: episode: 642, duration: 1.161s, episode steps: 147, steps per second: 127, episode reward: -262.498, mean reward: -1.786 [-100.000, 56.949], mean action: 1.435 [0.000, 3.000],  loss: 15.932506, mse: 4445.221502, mean_q: 48.687327, mean_eps: 0.805152
  59186/300000: episode: 643, duration: 0.509s, episode steps:  67, steps per second: 132, episode reward: -70.591, mean reward: -1.054 [-100.000,  8.618], mean action: 1.657 [0.000, 3.000],  loss: 21.603057, mse: 4576.862939, mean_q: 49.050019, mean_eps: 0.804798
  59255/300000: episode: 644, duration: 0.601s, episode steps:  69, steps per second: 115, episode reward: -118.560, mean reward: -1.718 [-100.000,  6.171], mean action: 1.362 [0.000, 3.000],  loss: 24.738372, mse: 4504.207863, mean_q: 49.973999, mean_eps: 0.804574
  59337/300000: episode: 645, duration: 0.856s, episode steps:  82, steps per second:  96, episode reward: -146.973, mean reward: -1.792 [-100.000, 17.933], mean action: 1.646 [0.000, 3.000],  loss: 18.637404, mse: 4526.538750, mean_q: 48.210170, mean_eps: 0.804325
  59448/300000: episode: 646, duration: 0.910s, episode steps: 111, steps per second: 122, episode reward: -143.543, mean reward: -1.293 [-100.000,  6.060], mean action: 1.595 [0.000, 3.000],  loss: 27.845765, mse: 4469.381838, mean_q: 48.298384, mean_eps: 0.804006
  59510/300000: episode: 647, duration: 0.482s, episode steps:  62, steps per second: 129, episode reward: -126.544, mean reward: -2.041 [-100.000, 10.095], mean action: 1.532 [0.000, 3.000],  loss: 25.479485, mse: 4676.665354, mean_q: 50.470946, mean_eps: 0.803721
  59592/300000: episode: 648, duration: 0.712s, episode steps:  82, steps per second: 115, episode reward: -155.334, mean reward: -1.894 [-100.000, 25.895], mean action: 1.463 [0.000, 3.000],  loss: 19.378822, mse: 4496.518224, mean_q: 48.561807, mean_eps: 0.803483
  59660/300000: episode: 649, duration: 0.589s, episode steps:  68, steps per second: 115, episode reward: -62.308, mean reward: -0.916 [-100.000, 10.480], mean action: 1.603 [0.000, 3.000],  loss: 19.026409, mse: 4557.070858, mean_q: 48.871358, mean_eps: 0.803236
  59774/300000: episode: 650, duration: 0.961s, episode steps: 114, steps per second: 119, episode reward: -50.955, mean reward: -0.447 [-100.000, 11.264], mean action: 1.579 [0.000, 3.000],  loss: 16.174318, mse: 4519.521915, mean_q: 47.698469, mean_eps: 0.802936
  59874/300000: episode: 651, duration: 0.809s, episode steps: 100, steps per second: 124, episode reward: -135.506, mean reward: -1.355 [-100.000, 24.895], mean action: 1.510 [0.000, 3.000],  loss: 15.353406, mse: 4580.915657, mean_q: 49.024998, mean_eps: 0.802582
  59937/300000: episode: 652, duration: 0.533s, episode steps:  63, steps per second: 118, episode reward: -72.409, mean reward: -1.149 [-100.000, 14.706], mean action: 1.444 [0.000, 3.000],  loss: 20.245787, mse: 4516.003007, mean_q: 49.659679, mean_eps: 0.802314
  60020/300000: episode: 653, duration: 0.650s, episode steps:  83, steps per second: 128, episode reward: -47.466, mean reward: -0.572 [-100.000, 17.227], mean action: 1.494 [0.000, 3.000],  loss: 15.471869, mse: 4513.041451, mean_q: 48.845598, mean_eps: 0.802073
  60092/300000: episode: 654, duration: 0.816s, episode steps:  72, steps per second:  88, episode reward: -80.499, mean reward: -1.118 [-100.000,  8.440], mean action: 1.375 [0.000, 3.000],  loss: 13.756052, mse: 4458.738261, mean_q: 47.137436, mean_eps: 0.801817
  60160/300000: episode: 655, duration: 0.496s, episode steps:  68, steps per second: 137, episode reward: -84.501, mean reward: -1.243 [-100.000,  9.836], mean action: 1.485 [0.000, 3.000],  loss: 13.370484, mse: 4669.445862, mean_q: 48.624502, mean_eps: 0.801586
  60260/300000: episode: 656, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -98.127, mean reward: -0.981 [-100.000, 10.930], mean action: 1.580 [0.000, 3.000],  loss: 17.112623, mse: 4547.633328, mean_q: 47.660739, mean_eps: 0.801309
  60342/300000: episode: 657, duration: 0.636s, episode steps:  82, steps per second: 129, episode reward: -84.291, mean reward: -1.028 [-100.000, 11.331], mean action: 1.415 [0.000, 3.000],  loss: 14.800611, mse: 4572.802976, mean_q: 48.148943, mean_eps: 0.801008
  60446/300000: episode: 658, duration: 1.010s, episode steps: 104, steps per second: 103, episode reward: -111.079, mean reward: -1.068 [-100.000, 14.759], mean action: 1.442 [0.000, 3.000],  loss: 15.133321, mse: 4615.146630, mean_q: 48.917925, mean_eps: 0.800701
  60529/300000: episode: 659, duration: 1.107s, episode steps:  83, steps per second:  75, episode reward: -56.597, mean reward: -0.682 [-100.000, 10.972], mean action: 1.566 [0.000, 3.000],  loss: 12.021993, mse: 4525.111999, mean_q: 46.864818, mean_eps: 0.800393
  60622/300000: episode: 660, duration: 1.079s, episode steps:  93, steps per second:  86, episode reward: -129.547, mean reward: -1.393 [-100.000, 11.405], mean action: 1.538 [0.000, 3.000],  loss: 21.969270, mse: 4578.740192, mean_q: 48.794221, mean_eps: 0.800102
  60720/300000: episode: 661, duration: 1.105s, episode steps:  98, steps per second:  89, episode reward: -167.415, mean reward: -1.708 [-100.000,  8.534], mean action: 1.469 [0.000, 3.000],  loss: 23.980736, mse: 4596.978217, mean_q: 49.373230, mean_eps: 0.799787
  60801/300000: episode: 662, duration: 0.619s, episode steps:  81, steps per second: 131, episode reward: -114.888, mean reward: -1.418 [-100.000,  7.515], mean action: 1.593 [0.000, 3.000],  loss: 10.287302, mse: 4602.173671, mean_q: 50.060187, mean_eps: 0.799492
  60892/300000: episode: 663, duration: 0.653s, episode steps:  91, steps per second: 139, episode reward: -88.919, mean reward: -0.977 [-100.000, 22.514], mean action: 1.758 [0.000, 3.000],  loss: 22.021749, mse: 4661.377082, mean_q: 48.504324, mean_eps: 0.799208
  60960/300000: episode: 664, duration: 0.543s, episode steps:  68, steps per second: 125, episode reward: -91.331, mean reward: -1.343 [-100.000,  7.852], mean action: 1.485 [0.000, 3.000],  loss: 20.377971, mse: 4542.899540, mean_q: 48.415328, mean_eps: 0.798946
  61026/300000: episode: 665, duration: 0.542s, episode steps:  66, steps per second: 122, episode reward: -16.759, mean reward: -0.254 [-100.000, 20.025], mean action: 1.773 [0.000, 3.000],  loss: 15.781244, mse: 4614.877238, mean_q: 48.565920, mean_eps: 0.798725
  61106/300000: episode: 666, duration: 0.606s, episode steps:  80, steps per second: 132, episode reward: -119.696, mean reward: -1.496 [-100.000, 13.891], mean action: 1.613 [0.000, 3.000],  loss: 18.144181, mse: 4567.979501, mean_q: 48.823517, mean_eps: 0.798484
  61171/300000: episode: 667, duration: 0.470s, episode steps:  65, steps per second: 138, episode reward: -91.869, mean reward: -1.413 [-100.000,  9.402], mean action: 1.754 [0.000, 3.000],  loss: 21.874754, mse: 4652.499388, mean_q: 46.730691, mean_eps: 0.798245
  61265/300000: episode: 668, duration: 0.654s, episode steps:  94, steps per second: 144, episode reward: -195.931, mean reward: -2.084 [-100.000,  6.213], mean action: 1.468 [0.000, 3.000],  loss: 25.455926, mse: 4597.960174, mean_q: 48.149453, mean_eps: 0.797982
  61342/300000: episode: 669, duration: 0.603s, episode steps:  77, steps per second: 128, episode reward: -111.981, mean reward: -1.454 [-100.000,  5.039], mean action: 1.494 [0.000, 3.000],  loss: 21.035916, mse: 4721.514433, mean_q: 50.255050, mean_eps: 0.797700
  61404/300000: episode: 670, duration: 0.450s, episode steps:  62, steps per second: 138, episode reward: -118.802, mean reward: -1.916 [-100.000,  7.999], mean action: 1.726 [0.000, 3.000],  loss: 16.078080, mse: 4577.811827, mean_q: 45.894503, mean_eps: 0.797471
  61512/300000: episode: 671, duration: 0.850s, episode steps: 108, steps per second: 127, episode reward: -81.728, mean reward: -0.757 [-100.000,  8.135], mean action: 1.574 [0.000, 3.000],  loss: 15.166311, mse: 4640.783086, mean_q: 48.803272, mean_eps: 0.797190
  61579/300000: episode: 672, duration: 0.515s, episode steps:  67, steps per second: 130, episode reward: -57.981, mean reward: -0.865 [-100.000, 20.066], mean action: 1.731 [0.000, 3.000],  loss: 23.427077, mse: 4622.936946, mean_q: 47.971303, mean_eps: 0.796902
  61669/300000: episode: 673, duration: 0.665s, episode steps:  90, steps per second: 135, episode reward: -122.661, mean reward: -1.363 [-100.000,  9.983], mean action: 1.600 [0.000, 3.000],  loss: 16.263544, mse: 4689.722876, mean_q: 49.033739, mean_eps: 0.796642
  61752/300000: episode: 674, duration: 0.624s, episode steps:  83, steps per second: 133, episode reward: 49.280, mean reward:  0.594 [-100.000, 119.190], mean action: 1.482 [0.000, 3.000],  loss: 13.573634, mse: 4667.228498, mean_q: 48.322633, mean_eps: 0.796357
  61864/300000: episode: 675, duration: 0.849s, episode steps: 112, steps per second: 132, episode reward: -115.939, mean reward: -1.035 [-100.000, 22.615], mean action: 1.670 [0.000, 3.000],  loss: 17.888846, mse: 4685.976501, mean_q: 49.201648, mean_eps: 0.796035
  61945/300000: episode: 676, duration: 0.585s, episode steps:  81, steps per second: 138, episode reward: -67.267, mean reward: -0.830 [-100.000,  8.387], mean action: 1.605 [0.000, 3.000],  loss: 18.737876, mse: 4666.934700, mean_q: 49.875139, mean_eps: 0.795717
  62025/300000: episode: 677, duration: 0.575s, episode steps:  80, steps per second: 139, episode reward: -47.904, mean reward: -0.599 [-100.000, 16.742], mean action: 1.700 [0.000, 3.000],  loss: 19.488583, mse: 4709.513574, mean_q: 48.597772, mean_eps: 0.795451
  62108/300000: episode: 678, duration: 0.663s, episode steps:  83, steps per second: 125, episode reward: -83.533, mean reward: -1.006 [-100.000,  8.641], mean action: 1.458 [0.000, 3.000],  loss: 14.827226, mse: 4702.840964, mean_q: 49.301267, mean_eps: 0.795182
  62214/300000: episode: 679, duration: 0.826s, episode steps: 106, steps per second: 128, episode reward: -186.139, mean reward: -1.756 [-100.000,  6.491], mean action: 1.651 [0.000, 3.000],  loss: 13.071136, mse: 4840.562562, mean_q: 48.963328, mean_eps: 0.794870
  62326/300000: episode: 680, duration: 0.898s, episode steps: 112, steps per second: 125, episode reward: -283.320, mean reward: -2.530 [-100.000,  1.344], mean action: 1.714 [0.000, 3.000],  loss: 14.800284, mse: 4755.902919, mean_q: 48.661978, mean_eps: 0.794511
  62411/300000: episode: 681, duration: 0.746s, episode steps:  85, steps per second: 114, episode reward: -81.049, mean reward: -0.954 [-100.000,  7.077], mean action: 1.471 [0.000, 3.000],  loss: 25.775771, mse: 4661.546964, mean_q: 48.788865, mean_eps: 0.794186
  62492/300000: episode: 682, duration: 0.649s, episode steps:  81, steps per second: 125, episode reward: -289.860, mean reward: -3.579 [-100.000, 123.007], mean action: 1.506 [0.000, 3.000],  loss: 18.845214, mse: 4797.193634, mean_q: 50.744322, mean_eps: 0.793912
  62573/300000: episode: 683, duration: 0.632s, episode steps:  81, steps per second: 128, episode reward: -84.843, mean reward: -1.047 [-100.000, 12.175], mean action: 1.580 [0.000, 3.000],  loss: 16.375324, mse: 4737.859297, mean_q: 48.843057, mean_eps: 0.793644
  62675/300000: episode: 684, duration: 0.895s, episode steps: 102, steps per second: 114, episode reward: -166.800, mean reward: -1.635 [-100.000,  6.320], mean action: 1.627 [0.000, 3.000],  loss: 13.161130, mse: 4720.495608, mean_q: 49.159264, mean_eps: 0.793342
  62785/300000: episode: 685, duration: 0.982s, episode steps: 110, steps per second: 112, episode reward: -25.550, mean reward: -0.232 [-100.000, 58.912], mean action: 1.700 [0.000, 3.000],  loss: 12.490132, mse: 4821.273404, mean_q: 48.601033, mean_eps: 0.792993
  62885/300000: episode: 686, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: -106.531, mean reward: -1.065 [-100.000,  6.903], mean action: 1.520 [0.000, 3.000],  loss: 14.200309, mse: 4724.561729, mean_q: 50.006882, mean_eps: 0.792646
  62989/300000: episode: 687, duration: 0.803s, episode steps: 104, steps per second: 129, episode reward: -97.183, mean reward: -0.934 [-100.000, 13.048], mean action: 1.490 [0.000, 3.000],  loss: 17.095325, mse: 4719.571646, mean_q: 48.277014, mean_eps: 0.792310
  63078/300000: episode: 688, duration: 0.679s, episode steps:  89, steps per second: 131, episode reward: -57.884, mean reward: -0.650 [-100.000, 12.716], mean action: 1.685 [0.000, 3.000],  loss: 26.991104, mse: 4706.748286, mean_q: 49.539703, mean_eps: 0.791991
  63161/300000: episode: 689, duration: 0.594s, episode steps:  83, steps per second: 140, episode reward: -30.610, mean reward: -0.369 [-100.000, 12.088], mean action: 1.554 [0.000, 3.000],  loss: 19.224519, mse: 4720.929185, mean_q: 48.046954, mean_eps: 0.791707
  63265/300000: episode: 690, duration: 0.763s, episode steps: 104, steps per second: 136, episode reward: -105.435, mean reward: -1.014 [-100.000, 21.776], mean action: 1.548 [0.000, 3.000],  loss: 17.869534, mse: 4649.561397, mean_q: 48.032205, mean_eps: 0.791399
  63372/300000: episode: 691, duration: 0.796s, episode steps: 107, steps per second: 134, episode reward: -128.502, mean reward: -1.201 [-100.000, 11.346], mean action: 1.523 [0.000, 3.000],  loss: 16.586773, mse: 4655.766891, mean_q: 48.492986, mean_eps: 0.791051
  63437/300000: episode: 692, duration: 0.473s, episode steps:  65, steps per second: 137, episode reward: -71.746, mean reward: -1.104 [-100.000, 14.427], mean action: 1.523 [0.000, 3.000],  loss: 11.474382, mse: 4733.201097, mean_q: 50.009669, mean_eps: 0.790767
  63526/300000: episode: 693, duration: 0.653s, episode steps:  89, steps per second: 136, episode reward: -139.821, mean reward: -1.571 [-100.000,  7.503], mean action: 1.640 [0.000, 3.000],  loss: 13.229004, mse: 4538.186576, mean_q: 48.184620, mean_eps: 0.790513
  63595/300000: episode: 694, duration: 0.512s, episode steps:  69, steps per second: 135, episode reward: -71.675, mean reward: -1.039 [-100.000, 13.156], mean action: 1.406 [0.000, 3.000],  loss: 16.408391, mse: 4653.391842, mean_q: 47.496440, mean_eps: 0.790252
  63719/300000: episode: 695, duration: 0.939s, episode steps: 124, steps per second: 132, episode reward: -177.217, mean reward: -1.429 [-100.000,  7.963], mean action: 1.484 [0.000, 3.000],  loss: 11.173117, mse: 4597.214781, mean_q: 49.031410, mean_eps: 0.789934
  63870/300000: episode: 696, duration: 1.253s, episode steps: 151, steps per second: 120, episode reward: -264.188, mean reward: -1.750 [-100.000, 53.200], mean action: 1.411 [0.000, 3.000],  loss: 21.675831, mse: 4628.223993, mean_q: 48.457341, mean_eps: 0.789480
  63946/300000: episode: 697, duration: 0.541s, episode steps:  76, steps per second: 140, episode reward: -173.361, mean reward: -2.281 [-100.000,  7.067], mean action: 1.579 [0.000, 3.000],  loss: 15.932124, mse: 4587.308295, mean_q: 48.536010, mean_eps: 0.789105
  64061/300000: episode: 698, duration: 0.879s, episode steps: 115, steps per second: 131, episode reward: -92.822, mean reward: -0.807 [-100.000,  8.576], mean action: 1.504 [0.000, 3.000],  loss: 14.435600, mse: 4838.030726, mean_q: 49.638418, mean_eps: 0.788790
  64144/300000: episode: 699, duration: 0.644s, episode steps:  83, steps per second: 129, episode reward: -113.353, mean reward: -1.366 [-100.000, 20.097], mean action: 1.627 [0.000, 3.000],  loss: 17.947667, mse: 4629.931776, mean_q: 47.055781, mean_eps: 0.788463
  64224/300000: episode: 700, duration: 0.586s, episode steps:  80, steps per second: 137, episode reward: -105.071, mean reward: -1.313 [-100.000, 13.104], mean action: 1.762 [0.000, 3.000],  loss: 11.018814, mse: 4905.535727, mean_q: 48.917429, mean_eps: 0.788194
  64313/300000: episode: 701, duration: 0.629s, episode steps:  89, steps per second: 142, episode reward: -240.397, mean reward: -2.701 [-100.000, 21.256], mean action: 1.584 [0.000, 3.000],  loss: 19.838973, mse: 4780.963906, mean_q: 48.707525, mean_eps: 0.787916
  64396/300000: episode: 702, duration: 0.625s, episode steps:  83, steps per second: 133, episode reward: -235.185, mean reward: -2.834 [-100.000, 112.078], mean action: 1.410 [0.000, 3.000],  loss: 18.121181, mse: 4798.482304, mean_q: 48.749226, mean_eps: 0.787632
  64499/300000: episode: 703, duration: 0.750s, episode steps: 103, steps per second: 137, episode reward: -173.689, mean reward: -1.686 [-100.000,  7.132], mean action: 1.621 [0.000, 3.000],  loss: 15.180510, mse: 4707.791762, mean_q: 48.953870, mean_eps: 0.787325
  64608/300000: episode: 704, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -111.969, mean reward: -1.027 [-100.000,  6.297], mean action: 1.486 [0.000, 3.000],  loss: 22.768186, mse: 4804.619665, mean_q: 48.772319, mean_eps: 0.786975
  64701/300000: episode: 705, duration: 0.691s, episode steps:  93, steps per second: 135, episode reward: -68.374, mean reward: -0.735 [-100.000, 13.917], mean action: 1.527 [0.000, 3.000],  loss: 19.638373, mse: 4800.120886, mean_q: 49.354222, mean_eps: 0.786642
  64806/300000: episode: 706, duration: 0.763s, episode steps: 105, steps per second: 138, episode reward: -72.055, mean reward: -0.686 [-100.000, 20.568], mean action: 1.705 [0.000, 3.000],  loss: 15.970385, mse: 4859.132141, mean_q: 50.831201, mean_eps: 0.786315
  64906/300000: episode: 707, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward: -194.376, mean reward: -1.944 [-100.000,  1.273], mean action: 1.590 [0.000, 3.000],  loss: 16.511149, mse: 4807.313799, mean_q: 49.099515, mean_eps: 0.785977
  64988/300000: episode: 708, duration: 0.653s, episode steps:  82, steps per second: 126, episode reward: -63.803, mean reward: -0.778 [-100.000,  7.278], mean action: 1.549 [0.000, 3.000],  loss: 19.418539, mse: 4813.096766, mean_q: 48.682002, mean_eps: 0.785677
  65078/300000: episode: 709, duration: 0.735s, episode steps:  90, steps per second: 122, episode reward: -69.973, mean reward: -0.777 [-100.000, 83.489], mean action: 1.444 [0.000, 3.000],  loss: 20.561166, mse: 4778.147477, mean_q: 50.756912, mean_eps: 0.785393
  65185/300000: episode: 710, duration: 0.778s, episode steps: 107, steps per second: 138, episode reward: -224.974, mean reward: -2.103 [-100.000,  9.101], mean action: 1.607 [0.000, 3.000],  loss: 29.765365, mse: 4887.365264, mean_q: 50.497037, mean_eps: 0.785068
  65303/300000: episode: 711, duration: 0.949s, episode steps: 118, steps per second: 124, episode reward: -114.503, mean reward: -0.970 [-100.000, 36.607], mean action: 1.314 [0.000, 3.000],  loss: 15.383938, mse: 4820.456725, mean_q: 49.950769, mean_eps: 0.784696
  65370/300000: episode: 712, duration: 0.491s, episode steps:  67, steps per second: 136, episode reward: -131.880, mean reward: -1.968 [-100.000, 11.402], mean action: 1.627 [0.000, 3.000],  loss: 27.681849, mse: 4875.321996, mean_q: 50.401906, mean_eps: 0.784391
  65487/300000: episode: 713, duration: 0.842s, episode steps: 117, steps per second: 139, episode reward: -147.628, mean reward: -1.262 [-100.000, 14.294], mean action: 1.462 [0.000, 3.000],  loss: 28.265383, mse: 4833.426683, mean_q: 48.821263, mean_eps: 0.784088
  65592/300000: episode: 714, duration: 0.798s, episode steps: 105, steps per second: 132, episode reward: -173.315, mean reward: -1.651 [-100.000, 23.140], mean action: 1.590 [0.000, 3.000],  loss: 15.026300, mse: 4884.895027, mean_q: 51.625205, mean_eps: 0.783721
  65687/300000: episode: 715, duration: 0.701s, episode steps:  95, steps per second: 135, episode reward: -71.398, mean reward: -0.752 [-100.000,  7.450], mean action: 1.547 [0.000, 3.000],  loss: 25.140439, mse: 4751.787891, mean_q: 48.742878, mean_eps: 0.783391
  65772/300000: episode: 716, duration: 0.619s, episode steps:  85, steps per second: 137, episode reward: -117.751, mean reward: -1.385 [-100.000,  6.384], mean action: 1.506 [0.000, 3.000],  loss: 45.408637, mse: 4794.201583, mean_q: 49.941669, mean_eps: 0.783094
  65868/300000: episode: 717, duration: 0.719s, episode steps:  96, steps per second: 134, episode reward: -106.837, mean reward: -1.113 [-100.000,  7.444], mean action: 1.427 [0.000, 3.000],  loss: 20.668807, mse: 4859.159739, mean_q: 51.306943, mean_eps: 0.782796
  65983/300000: episode: 718, duration: 0.871s, episode steps: 115, steps per second: 132, episode reward: -97.406, mean reward: -0.847 [-100.000, 15.347], mean action: 1.513 [0.000, 3.000],  loss: 27.402596, mse: 4800.080857, mean_q: 50.377208, mean_eps: 0.782448
  66045/300000: episode: 719, duration: 0.494s, episode steps:  62, steps per second: 126, episode reward: -80.190, mean reward: -1.293 [-100.000, 53.582], mean action: 1.532 [0.000, 3.000],  loss: 20.819098, mse: 4875.625224, mean_q: 49.743080, mean_eps: 0.782155
  66124/300000: episode: 720, duration: 0.631s, episode steps:  79, steps per second: 125, episode reward: -152.910, mean reward: -1.936 [-100.000,  8.103], mean action: 1.570 [0.000, 3.000],  loss: 23.769874, mse: 4922.478605, mean_q: 50.079561, mean_eps: 0.781923
  66209/300000: episode: 721, duration: 0.682s, episode steps:  85, steps per second: 125, episode reward: -141.482, mean reward: -1.664 [-100.000, 23.797], mean action: 1.553 [0.000, 3.000],  loss: 10.897642, mse: 5015.632959, mean_q: 52.212970, mean_eps: 0.781652
  66288/300000: episode: 722, duration: 0.603s, episode steps:  79, steps per second: 131, episode reward: -97.305, mean reward: -1.232 [-100.000, 34.542], mean action: 1.684 [0.000, 3.000],  loss: 13.345599, mse: 4992.098302, mean_q: 50.554768, mean_eps: 0.781382
  66389/300000: episode: 723, duration: 0.800s, episode steps: 101, steps per second: 126, episode reward: -177.708, mean reward: -1.759 [-100.000,  9.200], mean action: 1.653 [0.000, 3.000],  loss: 17.244717, mse: 4955.289454, mean_q: 51.684690, mean_eps: 0.781085
  66494/300000: episode: 724, duration: 0.825s, episode steps: 105, steps per second: 127, episode reward: -268.184, mean reward: -2.554 [-100.000, 53.099], mean action: 1.505 [0.000, 3.000],  loss: 25.766923, mse: 4901.313416, mean_q: 49.636802, mean_eps: 0.780745
  66569/300000: episode: 725, duration: 0.556s, episode steps:  75, steps per second: 135, episode reward: -147.651, mean reward: -1.969 [-100.000,  7.845], mean action: 1.707 [0.000, 3.000],  loss: 19.435937, mse: 5001.654036, mean_q: 49.977125, mean_eps: 0.780448
  66638/300000: episode: 726, duration: 0.624s, episode steps:  69, steps per second: 111, episode reward: -42.595, mean reward: -0.617 [-100.000, 12.559], mean action: 1.696 [0.000, 3.000],  loss: 12.715815, mse: 4939.452683, mean_q: 49.348317, mean_eps: 0.780210
  66750/300000: episode: 727, duration: 0.916s, episode steps: 112, steps per second: 122, episode reward: -175.461, mean reward: -1.567 [-100.000,  8.418], mean action: 1.679 [0.000, 3.000],  loss: 14.118714, mse: 4992.851759, mean_q: 49.652026, mean_eps: 0.779911
  66837/300000: episode: 728, duration: 0.636s, episode steps:  87, steps per second: 137, episode reward: -45.100, mean reward: -0.518 [-100.000, 72.453], mean action: 1.540 [0.000, 3.000],  loss: 16.402036, mse: 5056.476265, mean_q: 50.978504, mean_eps: 0.779583
  66923/300000: episode: 729, duration: 0.657s, episode steps:  86, steps per second: 131, episode reward: -164.498, mean reward: -1.913 [-100.000, 13.714], mean action: 1.395 [0.000, 3.000],  loss: 14.376175, mse: 4950.131742, mean_q: 50.198036, mean_eps: 0.779298
  67010/300000: episode: 730, duration: 0.675s, episode steps:  87, steps per second: 129, episode reward: -152.333, mean reward: -1.751 [-100.000,  9.378], mean action: 1.425 [0.000, 3.000],  loss: 17.242539, mse: 4966.088794, mean_q: 50.984268, mean_eps: 0.779012
  67092/300000: episode: 731, duration: 0.610s, episode steps:  82, steps per second: 134, episode reward: -134.216, mean reward: -1.637 [-100.000,  8.713], mean action: 1.354 [0.000, 3.000],  loss: 17.658240, mse: 5216.888871, mean_q: 51.401126, mean_eps: 0.778733
  67180/300000: episode: 732, duration: 0.655s, episode steps:  88, steps per second: 134, episode reward: -95.193, mean reward: -1.082 [-100.000, 11.002], mean action: 1.670 [0.000, 3.000],  loss: 24.762195, mse: 5152.069577, mean_q: 50.823100, mean_eps: 0.778453
  67244/300000: episode: 733, duration: 0.494s, episode steps:  64, steps per second: 130, episode reward: -106.768, mean reward: -1.668 [-100.000, 10.289], mean action: 1.781 [0.000, 3.000],  loss: 20.848997, mse: 5099.820923, mean_q: 51.415916, mean_eps: 0.778202
  67350/300000: episode: 734, duration: 0.785s, episode steps: 106, steps per second: 135, episode reward: -51.370, mean reward: -0.485 [-100.000, 23.372], mean action: 1.670 [0.000, 3.000],  loss: 11.762822, mse: 5203.892841, mean_q: 51.335251, mean_eps: 0.777922
  67502/300000: episode: 735, duration: 1.080s, episode steps: 152, steps per second: 141, episode reward: -52.827, mean reward: -0.348 [-100.000, 53.263], mean action: 1.441 [0.000, 3.000],  loss: 13.801198, mse: 5205.439199, mean_q: 52.796864, mean_eps: 0.777496
  67597/300000: episode: 736, duration: 0.730s, episode steps:  95, steps per second: 130, episode reward: -169.607, mean reward: -1.785 [-100.000,  6.187], mean action: 1.189 [0.000, 3.000],  loss: 14.124422, mse: 5090.704528, mean_q: 51.233009, mean_eps: 0.777088
  67683/300000: episode: 737, duration: 0.673s, episode steps:  86, steps per second: 128, episode reward: -55.183, mean reward: -0.642 [-100.000,  8.205], mean action: 1.698 [0.000, 3.000],  loss: 15.394818, mse: 5040.748793, mean_q: 50.367484, mean_eps: 0.776790
  67783/300000: episode: 738, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: -228.065, mean reward: -2.281 [-100.000,  5.752], mean action: 1.570 [0.000, 3.000],  loss: 32.385065, mse: 5069.140510, mean_q: 50.396439, mean_eps: 0.776483
  67848/300000: episode: 739, duration: 0.528s, episode steps:  65, steps per second: 123, episode reward: -58.808, mean reward: -0.905 [-100.000,  7.567], mean action: 1.646 [0.000, 3.000],  loss: 15.530419, mse: 5220.427787, mean_q: 52.289479, mean_eps: 0.776211
  67921/300000: episode: 740, duration: 0.604s, episode steps:  73, steps per second: 121, episode reward: -71.625, mean reward: -0.981 [-100.000,  7.704], mean action: 1.562 [0.000, 3.000],  loss: 14.805057, mse: 5164.795377, mean_q: 51.408532, mean_eps: 0.775983
  68011/300000: episode: 741, duration: 0.792s, episode steps:  90, steps per second: 114, episode reward: -87.022, mean reward: -0.967 [-100.000,  6.340], mean action: 1.600 [0.000, 3.000],  loss: 13.615079, mse: 5099.863512, mean_q: 50.450247, mean_eps: 0.775714
  68101/300000: episode: 742, duration: 0.795s, episode steps:  90, steps per second: 113, episode reward: -103.838, mean reward: -1.154 [-100.000, 17.800], mean action: 1.556 [0.000, 3.000],  loss: 28.229007, mse: 5324.253196, mean_q: 51.483303, mean_eps: 0.775417
  68226/300000: episode: 743, duration: 1.074s, episode steps: 125, steps per second: 116, episode reward: -74.785, mean reward: -0.598 [-100.000,  8.828], mean action: 1.664 [0.000, 3.000],  loss: 18.314811, mse: 5203.459572, mean_q: 51.831016, mean_eps: 0.775062
  68295/300000: episode: 744, duration: 0.561s, episode steps:  69, steps per second: 123, episode reward: -68.858, mean reward: -0.998 [-100.000, 10.782], mean action: 1.710 [0.000, 3.000],  loss: 15.347004, mse: 5109.412775, mean_q: 49.049728, mean_eps: 0.774742
  68416/300000: episode: 745, duration: 1.078s, episode steps: 121, steps per second: 112, episode reward: -43.718, mean reward: -0.361 [-100.000, 28.705], mean action: 1.529 [0.000, 3.000],  loss: 19.581681, mse: 5220.200522, mean_q: 52.065421, mean_eps: 0.774428
  68480/300000: episode: 746, duration: 0.554s, episode steps:  64, steps per second: 115, episode reward: -87.887, mean reward: -1.373 [-100.000,  8.421], mean action: 1.547 [0.000, 3.000],  loss: 25.571869, mse: 5252.655083, mean_q: 51.938755, mean_eps: 0.774123
  68571/300000: episode: 747, duration: 0.995s, episode steps:  91, steps per second:  91, episode reward: -82.311, mean reward: -0.905 [-100.000,  7.393], mean action: 1.297 [0.000, 3.000],  loss: 25.464444, mse: 5274.262121, mean_q: 51.330536, mean_eps: 0.773868
  68652/300000: episode: 748, duration: 0.621s, episode steps:  81, steps per second: 130, episode reward: -150.313, mean reward: -1.856 [-100.000,  5.383], mean action: 1.481 [0.000, 3.000],  loss: 16.069192, mse: 5222.658285, mean_q: 51.153584, mean_eps: 0.773584
  68762/300000: episode: 749, duration: 0.825s, episode steps: 110, steps per second: 133, episode reward: -110.295, mean reward: -1.003 [-100.000, 11.482], mean action: 1.564 [0.000, 3.000],  loss: 19.916871, mse: 5379.765858, mean_q: 53.755700, mean_eps: 0.773269
  68852/300000: episode: 750, duration: 0.678s, episode steps:  90, steps per second: 133, episode reward: -119.089, mean reward: -1.323 [-100.000,  7.588], mean action: 1.311 [0.000, 3.000],  loss: 22.279614, mse: 5256.910845, mean_q: 52.032510, mean_eps: 0.772939
  68966/300000: episode: 751, duration: 0.827s, episode steps: 114, steps per second: 138, episode reward: -95.834, mean reward: -0.841 [-100.000,  9.136], mean action: 1.632 [0.000, 3.000],  loss: 14.769868, mse: 5283.407494, mean_q: 53.080137, mean_eps: 0.772602
  69063/300000: episode: 752, duration: 0.710s, episode steps:  97, steps per second: 137, episode reward: -136.031, mean reward: -1.402 [-100.000, 15.751], mean action: 1.753 [0.000, 3.000],  loss: 23.596057, mse: 5449.714844, mean_q: 53.770726, mean_eps: 0.772254
  69169/300000: episode: 753, duration: 0.861s, episode steps: 106, steps per second: 123, episode reward: -130.168, mean reward: -1.228 [-100.000, 72.018], mean action: 1.632 [0.000, 3.000],  loss: 23.415547, mse: 5336.439398, mean_q: 53.016461, mean_eps: 0.771919
  69283/300000: episode: 754, duration: 0.872s, episode steps: 114, steps per second: 131, episode reward: -17.786, mean reward: -0.156 [-100.000, 17.382], mean action: 1.667 [0.000, 3.000],  loss: 19.525052, mse: 5372.455776, mean_q: 53.354762, mean_eps: 0.771556
  69352/300000: episode: 755, duration: 0.539s, episode steps:  69, steps per second: 128, episode reward: -71.701, mean reward: -1.039 [-100.000,  6.860], mean action: 1.681 [0.000, 3.000],  loss: 14.323614, mse: 5371.073914, mean_q: 54.492496, mean_eps: 0.771254
  69446/300000: episode: 756, duration: 0.765s, episode steps:  94, steps per second: 123, episode reward: -136.980, mean reward: -1.457 [-100.000, 12.992], mean action: 1.638 [0.000, 3.000],  loss: 21.429235, mse: 5340.429373, mean_q: 51.716388, mean_eps: 0.770985
  69538/300000: episode: 757, duration: 0.678s, episode steps:  92, steps per second: 136, episode reward: -154.579, mean reward: -1.680 [-100.000, 17.250], mean action: 1.457 [0.000, 3.000],  loss: 19.974223, mse: 5228.650348, mean_q: 52.366545, mean_eps: 0.770678
  69612/300000: episode: 758, duration: 0.540s, episode steps:  74, steps per second: 137, episode reward: -41.391, mean reward: -0.559 [-100.000, 13.536], mean action: 1.595 [0.000, 3.000],  loss: 15.174781, mse: 5396.932974, mean_q: 53.702903, mean_eps: 0.770404
  69694/300000: episode: 759, duration: 0.639s, episode steps:  82, steps per second: 128, episode reward: -72.461, mean reward: -0.884 [-100.000,  6.511], mean action: 1.561 [0.000, 3.000],  loss: 26.678001, mse: 5473.109107, mean_q: 54.765906, mean_eps: 0.770147
  69773/300000: episode: 760, duration: 0.560s, episode steps:  79, steps per second: 141, episode reward: -166.658, mean reward: -2.110 [-100.000,  7.877], mean action: 1.570 [0.000, 3.000],  loss: 37.402214, mse: 5405.334720, mean_q: 54.116025, mean_eps: 0.769881
  69859/300000: episode: 761, duration: 0.594s, episode steps:  86, steps per second: 145, episode reward: -127.827, mean reward: -1.486 [-100.000,  5.744], mean action: 1.535 [0.000, 3.000],  loss: 19.612173, mse: 5361.632500, mean_q: 52.989545, mean_eps: 0.769609
  69947/300000: episode: 762, duration: 0.607s, episode steps:  88, steps per second: 145, episode reward: -114.179, mean reward: -1.297 [-100.000,  4.916], mean action: 1.477 [0.000, 3.000],  loss: 14.070759, mse: 5362.436224, mean_q: 54.417358, mean_eps: 0.769322
  70052/300000: episode: 763, duration: 0.767s, episode steps: 105, steps per second: 137, episode reward: -74.158, mean reward: -0.706 [-100.000, 17.009], mean action: 1.343 [0.000, 3.000],  loss: 15.903020, mse: 5295.328692, mean_q: 52.595656, mean_eps: 0.769003
  70160/300000: episode: 764, duration: 0.757s, episode steps: 108, steps per second: 143, episode reward: -101.889, mean reward: -0.943 [-100.000, 10.481], mean action: 1.676 [0.000, 3.000],  loss: 19.204676, mse: 5398.379526, mean_q: 52.420255, mean_eps: 0.768652
  70230/300000: episode: 765, duration: 0.478s, episode steps:  70, steps per second: 146, episode reward: -82.788, mean reward: -1.183 [-100.000,  7.528], mean action: 1.714 [0.000, 3.000],  loss: 11.294685, mse: 5346.316577, mean_q: 52.925776, mean_eps: 0.768358
  70324/300000: episode: 766, duration: 0.681s, episode steps:  94, steps per second: 138, episode reward: -125.371, mean reward: -1.334 [-100.000, 17.561], mean action: 1.702 [0.000, 3.000],  loss: 17.684930, mse: 5277.458623, mean_q: 52.251910, mean_eps: 0.768088
  70416/300000: episode: 767, duration: 0.655s, episode steps:  92, steps per second: 141, episode reward: -15.687, mean reward: -0.171 [-100.000, 17.237], mean action: 1.522 [0.000, 3.000],  loss: 14.660026, mse: 5343.806025, mean_q: 53.670628, mean_eps: 0.767781
  70513/300000: episode: 768, duration: 0.720s, episode steps:  97, steps per second: 135, episode reward: -91.703, mean reward: -0.945 [-100.000,  7.325], mean action: 1.660 [0.000, 3.000],  loss: 12.591255, mse: 5475.150602, mean_q: 54.742436, mean_eps: 0.767469
  70595/300000: episode: 769, duration: 0.785s, episode steps:  82, steps per second: 104, episode reward: -74.069, mean reward: -0.903 [-100.000, 11.307], mean action: 1.573 [0.000, 3.000],  loss: 14.308763, mse: 5426.833401, mean_q: 54.476178, mean_eps: 0.767173
  70671/300000: episode: 770, duration: 0.697s, episode steps:  76, steps per second: 109, episode reward: -99.991, mean reward: -1.316 [-100.000,  9.648], mean action: 1.763 [0.000, 3.000],  loss: 14.900083, mse: 5547.464516, mean_q: 55.765006, mean_eps: 0.766913
  70776/300000: episode: 771, duration: 0.895s, episode steps: 105, steps per second: 117, episode reward: -163.343, mean reward: -1.556 [-100.000,  7.211], mean action: 1.552 [0.000, 3.000],  loss: 9.904804, mse: 5442.511526, mean_q: 55.062008, mean_eps: 0.766614
  70904/300000: episode: 772, duration: 1.532s, episode steps: 128, steps per second:  84, episode reward: -195.400, mean reward: -1.527 [-100.000,  9.595], mean action: 1.516 [0.000, 3.000],  loss: 17.445803, mse: 5408.211500, mean_q: 54.877495, mean_eps: 0.766230
  70978/300000: episode: 773, duration: 0.962s, episode steps:  74, steps per second:  77, episode reward: -59.734, mean reward: -0.807 [-100.000,  9.474], mean action: 1.649 [0.000, 3.000],  loss: 16.481879, mse: 5310.026394, mean_q: 52.921157, mean_eps: 0.765896
  71102/300000: episode: 774, duration: 1.047s, episode steps: 124, steps per second: 118, episode reward: -30.212, mean reward: -0.244 [-100.000, 79.290], mean action: 1.500 [0.000, 3.000],  loss: 9.672711, mse: 5420.715625, mean_q: 52.794613, mean_eps: 0.765570
  71167/300000: episode: 775, duration: 0.531s, episode steps:  65, steps per second: 122, episode reward: -85.336, mean reward: -1.313 [-100.000, 16.210], mean action: 1.800 [0.000, 3.000],  loss: 21.688889, mse: 5585.353110, mean_q: 52.650278, mean_eps: 0.765258
  71240/300000: episode: 776, duration: 0.601s, episode steps:  73, steps per second: 122, episode reward: -62.641, mean reward: -0.858 [-100.000,  8.057], mean action: 1.740 [0.000, 3.000],  loss: 16.717624, mse: 5510.470402, mean_q: 54.571224, mean_eps: 0.765030
  71342/300000: episode: 777, duration: 0.782s, episode steps: 102, steps per second: 131, episode reward: -111.677, mean reward: -1.095 [-100.000, 14.961], mean action: 1.510 [0.000, 3.000],  loss: 12.465444, mse: 5488.783241, mean_q: 54.044326, mean_eps: 0.764741
  71470/300000: episode: 778, duration: 1.051s, episode steps: 128, steps per second: 122, episode reward:  3.830, mean reward:  0.030 [-100.000, 92.576], mean action: 1.680 [0.000, 3.000],  loss: 16.203005, mse: 5496.385818, mean_q: 53.346102, mean_eps: 0.764362
  71568/300000: episode: 779, duration: 0.854s, episode steps:  98, steps per second: 115, episode reward: 14.196, mean reward:  0.145 [-100.000, 35.219], mean action: 1.673 [0.000, 3.000],  loss: 16.389574, mse: 5507.213852, mean_q: 52.367395, mean_eps: 0.763989
  71706/300000: episode: 780, duration: 1.077s, episode steps: 138, steps per second: 128, episode reward: -168.502, mean reward: -1.221 [-100.000,  6.850], mean action: 1.500 [0.000, 3.000],  loss: 15.117959, mse: 5539.193803, mean_q: 53.276393, mean_eps: 0.763600
  71800/300000: episode: 781, duration: 0.739s, episode steps:  94, steps per second: 127, episode reward: -133.039, mean reward: -1.415 [-100.000,  8.616], mean action: 1.564 [0.000, 3.000],  loss: 18.553088, mse: 5639.439484, mean_q: 54.636577, mean_eps: 0.763217
  71868/300000: episode: 782, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -50.501, mean reward: -0.743 [-100.000, 12.804], mean action: 1.691 [0.000, 3.000],  loss: 26.067898, mse: 5489.157614, mean_q: 52.839013, mean_eps: 0.762949
  71934/300000: episode: 783, duration: 0.472s, episode steps:  66, steps per second: 140, episode reward: -93.952, mean reward: -1.424 [-100.000, 22.478], mean action: 1.803 [0.000, 3.000],  loss: 13.788565, mse: 5425.621020, mean_q: 54.159929, mean_eps: 0.762728
  72017/300000: episode: 784, duration: 0.579s, episode steps:  83, steps per second: 143, episode reward: -55.008, mean reward: -0.663 [-100.000, 12.138], mean action: 1.373 [0.000, 3.000],  loss: 11.800301, mse: 5452.935600, mean_q: 53.364868, mean_eps: 0.762482
  72084/300000: episode: 785, duration: 0.488s, episode steps:  67, steps per second: 137, episode reward: -91.648, mean reward: -1.368 [-100.000, 12.454], mean action: 1.552 [0.000, 3.000],  loss: 13.299713, mse: 5409.644535, mean_q: 51.303506, mean_eps: 0.762235
  72150/300000: episode: 786, duration: 0.538s, episode steps:  66, steps per second: 123, episode reward: -89.663, mean reward: -1.359 [-100.000,  5.852], mean action: 1.500 [0.000, 3.000],  loss: 10.969264, mse: 5379.661458, mean_q: 51.617872, mean_eps: 0.762016
  72241/300000: episode: 787, duration: 0.667s, episode steps:  91, steps per second: 136, episode reward: -270.247, mean reward: -2.970 [-100.000, 30.334], mean action: 1.418 [0.000, 3.000],  loss: 24.763439, mse: 5458.305669, mean_q: 52.111411, mean_eps: 0.761756
  72346/300000: episode: 788, duration: 0.792s, episode steps: 105, steps per second: 133, episode reward: -93.568, mean reward: -0.891 [-100.000, 11.402], mean action: 1.410 [0.000, 3.000],  loss: 30.069008, mse: 5485.204594, mean_q: 54.426153, mean_eps: 0.761433
  72444/300000: episode: 789, duration: 0.771s, episode steps:  98, steps per second: 127, episode reward: -99.497, mean reward: -1.015 [-100.000, 14.154], mean action: 1.367 [0.000, 3.000],  loss: 21.176261, mse: 5488.547099, mean_q: 53.482348, mean_eps: 0.761098
  72588/300000: episode: 790, duration: 1.065s, episode steps: 144, steps per second: 135, episode reward: -270.276, mean reward: -1.877 [-100.000,  8.929], mean action: 1.681 [0.000, 3.000],  loss: 16.024996, mse: 5506.987044, mean_q: 53.011411, mean_eps: 0.760699
  72674/300000: episode: 791, duration: 0.691s, episode steps:  86, steps per second: 124, episode reward: 16.889, mean reward:  0.196 [-100.000, 81.048], mean action: 1.849 [0.000, 3.000],  loss: 12.376260, mse: 5598.947762, mean_q: 54.047832, mean_eps: 0.760319
  73674/300000: episode: 792, duration: 10.240s, episode steps: 1000, steps per second:  98, episode reward: 106.133, mean reward:  0.106 [-25.033, 56.417], mean action: 1.466 [0.000, 3.000],  loss: 18.137202, mse: 5475.842700, mean_q: 52.390748, mean_eps: 0.758527
  73743/300000: episode: 793, duration: 0.712s, episode steps:  69, steps per second:  97, episode reward: -131.858, mean reward: -1.911 [-100.000,  9.939], mean action: 1.493 [0.000, 3.000],  loss: 14.505442, mse: 5441.587169, mean_q: 50.505887, mean_eps: 0.756764
  73839/300000: episode: 794, duration: 0.839s, episode steps:  96, steps per second: 114, episode reward: -236.708, mean reward: -2.466 [-100.000,  5.324], mean action: 1.698 [0.000, 3.000],  loss: 21.498681, mse: 5405.882044, mean_q: 50.216579, mean_eps: 0.756491
  73960/300000: episode: 795, duration: 0.949s, episode steps: 121, steps per second: 127, episode reward: -322.895, mean reward: -2.669 [-100.000, 69.718], mean action: 1.711 [0.000, 3.000],  loss: 16.469686, mse: 5462.060323, mean_q: 52.885963, mean_eps: 0.756133
  74050/300000: episode: 796, duration: 0.778s, episode steps:  90, steps per second: 116, episode reward: -0.583, mean reward: -0.006 [-100.000, 98.863], mean action: 1.556 [0.000, 3.000],  loss: 13.545467, mse: 5483.236144, mean_q: 51.400183, mean_eps: 0.755785
  74142/300000: episode: 797, duration: 0.767s, episode steps:  92, steps per second: 120, episode reward: -138.462, mean reward: -1.505 [-100.000,  6.537], mean action: 1.717 [0.000, 3.000],  loss: 21.382969, mse: 5462.720828, mean_q: 52.990537, mean_eps: 0.755485
  74234/300000: episode: 798, duration: 0.786s, episode steps:  92, steps per second: 117, episode reward: -91.624, mean reward: -0.996 [-100.000, 17.774], mean action: 1.750 [0.000, 3.000],  loss: 11.146772, mse: 5319.573993, mean_q: 50.531909, mean_eps: 0.755181
  74326/300000: episode: 799, duration: 0.984s, episode steps:  92, steps per second:  93, episode reward: -58.654, mean reward: -0.638 [-100.000, 22.857], mean action: 1.543 [0.000, 3.000],  loss: 30.358581, mse: 5275.894083, mean_q: 49.905272, mean_eps: 0.754878
  74412/300000: episode: 800, duration: 0.875s, episode steps:  86, steps per second:  98, episode reward: -108.637, mean reward: -1.263 [-100.000, 16.164], mean action: 1.686 [0.000, 3.000],  loss: 18.056046, mse: 5328.812466, mean_q: 50.789915, mean_eps: 0.754584
  74479/300000: episode: 801, duration: 0.652s, episode steps:  67, steps per second: 103, episode reward: -78.712, mean reward: -1.175 [-100.000,  8.258], mean action: 1.537 [0.000, 3.000],  loss: 14.884354, mse: 5217.906192, mean_q: 49.424796, mean_eps: 0.754332
  74574/300000: episode: 802, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -84.238, mean reward: -0.887 [-100.000, 14.130], mean action: 1.474 [0.000, 3.000],  loss: 15.539026, mse: 5394.050077, mean_q: 52.134877, mean_eps: 0.754064
  74673/300000: episode: 803, duration: 0.855s, episode steps:  99, steps per second: 116, episode reward: -81.060, mean reward: -0.819 [-100.000,  9.555], mean action: 1.707 [0.000, 3.000],  loss: 18.074548, mse: 5425.425332, mean_q: 52.411385, mean_eps: 0.753744
  74749/300000: episode: 804, duration: 0.579s, episode steps:  76, steps per second: 131, episode reward: -91.058, mean reward: -1.198 [-100.000,  8.158], mean action: 1.618 [0.000, 3.000],  loss: 15.731408, mse: 5366.978753, mean_q: 51.635520, mean_eps: 0.753455
  74849/300000: episode: 805, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: -113.669, mean reward: -1.137 [-100.000, 15.186], mean action: 1.710 [0.000, 3.000],  loss: 19.191539, mse: 5496.255786, mean_q: 52.917983, mean_eps: 0.753165
  74942/300000: episode: 806, duration: 0.692s, episode steps:  93, steps per second: 134, episode reward: -123.224, mean reward: -1.325 [-100.000, 31.939], mean action: 1.731 [0.000, 3.000],  loss: 16.972578, mse: 5360.943354, mean_q: 51.526620, mean_eps: 0.752846
  75055/300000: episode: 807, duration: 0.830s, episode steps: 113, steps per second: 136, episode reward: -78.729, mean reward: -0.697 [-100.000,  7.256], mean action: 1.522 [0.000, 3.000],  loss: 19.517327, mse: 5457.744912, mean_q: 52.278599, mean_eps: 0.752507
  75172/300000: episode: 808, duration: 0.826s, episode steps: 117, steps per second: 142, episode reward: -118.470, mean reward: -1.013 [-100.000, 26.809], mean action: 1.496 [0.000, 3.000],  loss: 12.274638, mse: 5559.060228, mean_q: 52.995459, mean_eps: 0.752127
  75286/300000: episode: 809, duration: 0.811s, episode steps: 114, steps per second: 141, episode reward: -126.956, mean reward: -1.114 [-100.000, 14.087], mean action: 1.596 [0.000, 3.000],  loss: 16.669409, mse: 5544.717309, mean_q: 51.963797, mean_eps: 0.751746
  75367/300000: episode: 810, duration: 0.659s, episode steps:  81, steps per second: 123, episode reward: -34.384, mean reward: -0.424 [-100.000, 19.265], mean action: 1.556 [0.000, 3.000],  loss: 17.831477, mse: 5456.742025, mean_q: 51.775011, mean_eps: 0.751424
  75465/300000: episode: 811, duration: 0.711s, episode steps:  98, steps per second: 138, episode reward: -112.487, mean reward: -1.148 [-100.000, 15.552], mean action: 1.551 [0.000, 3.000],  loss: 14.514998, mse: 5466.298761, mean_q: 52.009278, mean_eps: 0.751129
  75544/300000: episode: 812, duration: 0.558s, episode steps:  79, steps per second: 142, episode reward: -87.082, mean reward: -1.102 [-100.000,  9.306], mean action: 1.443 [0.000, 3.000],  loss: 12.396880, mse: 5574.385090, mean_q: 53.073590, mean_eps: 0.750837
  75625/300000: episode: 813, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -80.012, mean reward: -0.988 [-100.000, 11.499], mean action: 1.691 [0.000, 3.000],  loss: 23.973870, mse: 5550.290449, mean_q: 51.646960, mean_eps: 0.750573
  75717/300000: episode: 814, duration: 0.639s, episode steps:  92, steps per second: 144, episode reward: -106.624, mean reward: -1.159 [-100.000, 11.391], mean action: 1.728 [0.000, 3.000],  loss: 16.968528, mse: 5568.286717, mean_q: 51.465100, mean_eps: 0.750287
  75843/300000: episode: 815, duration: 1.016s, episode steps: 126, steps per second: 124, episode reward: -223.546, mean reward: -1.774 [-100.000,  1.766], mean action: 1.333 [0.000, 3.000],  loss: 12.372722, mse: 5563.984571, mean_q: 52.661547, mean_eps: 0.749928
  75905/300000: episode: 816, duration: 0.506s, episode steps:  62, steps per second: 122, episode reward: -78.551, mean reward: -1.267 [-100.000,  9.867], mean action: 1.323 [0.000, 3.000],  loss: 18.928603, mse: 5580.232225, mean_q: 52.335884, mean_eps: 0.749617
  75988/300000: episode: 817, duration: 0.672s, episode steps:  83, steps per second: 123, episode reward: -61.976, mean reward: -0.747 [-100.000, 12.011], mean action: 1.590 [0.000, 3.000],  loss: 17.437547, mse: 5446.356028, mean_q: 50.866399, mean_eps: 0.749378
  76080/300000: episode: 818, duration: 0.765s, episode steps:  92, steps per second: 120, episode reward: -90.899, mean reward: -0.988 [-100.000,  6.467], mean action: 1.522 [0.000, 3.000],  loss: 25.554994, mse: 5752.678796, mean_q: 53.263628, mean_eps: 0.749089
  76160/300000: episode: 819, duration: 0.649s, episode steps:  80, steps per second: 123, episode reward: -31.711, mean reward: -0.396 [-100.000, 16.727], mean action: 1.663 [0.000, 3.000],  loss: 23.029451, mse: 5577.020203, mean_q: 51.350233, mean_eps: 0.748806
  76243/300000: episode: 820, duration: 0.687s, episode steps:  83, steps per second: 121, episode reward: -233.687, mean reward: -2.816 [-100.000,  7.595], mean action: 1.494 [0.000, 3.000],  loss: 14.350707, mse: 5584.664674, mean_q: 50.460939, mean_eps: 0.748537
  76336/300000: episode: 821, duration: 0.748s, episode steps:  93, steps per second: 124, episode reward: -82.819, mean reward: -0.891 [-100.000,  6.756], mean action: 1.710 [0.000, 3.000],  loss: 13.018613, mse: 5558.533466, mean_q: 51.715049, mean_eps: 0.748246
  76433/300000: episode: 822, duration: 0.765s, episode steps:  97, steps per second: 127, episode reward: -72.167, mean reward: -0.744 [-100.000, 12.218], mean action: 1.722 [0.000, 3.000],  loss: 8.228962, mse: 5689.870117, mean_q: 54.175147, mean_eps: 0.747933
  76515/300000: episode: 823, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -104.817, mean reward: -1.278 [-100.000, 11.051], mean action: 1.598 [0.000, 3.000],  loss: 12.064131, mse: 5605.444235, mean_q: 51.791251, mean_eps: 0.747637
  76606/300000: episode: 824, duration: 0.714s, episode steps:  91, steps per second: 127, episode reward: -67.760, mean reward: -0.745 [-100.000,  7.817], mean action: 1.593 [0.000, 3.000],  loss: 18.808728, mse: 5618.904517, mean_q: 50.843924, mean_eps: 0.747352
  76672/300000: episode: 825, duration: 0.522s, episode steps:  66, steps per second: 126, episode reward: -84.928, mean reward: -1.287 [-100.000, 25.609], mean action: 1.515 [0.000, 3.000],  loss: 11.363222, mse: 5518.949685, mean_q: 51.699309, mean_eps: 0.747093
  76761/300000: episode: 826, duration: 0.775s, episode steps:  89, steps per second: 115, episode reward: -159.470, mean reward: -1.792 [-100.000,  8.097], mean action: 1.539 [0.000, 3.000],  loss: 11.573196, mse: 5487.421228, mean_q: 50.505885, mean_eps: 0.746837
  76849/300000: episode: 827, duration: 0.665s, episode steps:  88, steps per second: 132, episode reward: -76.220, mean reward: -0.866 [-100.000, 11.661], mean action: 1.466 [0.000, 3.000],  loss: 12.923731, mse: 5703.977744, mean_q: 52.114006, mean_eps: 0.746545
  76946/300000: episode: 828, duration: 0.850s, episode steps:  97, steps per second: 114, episode reward: -206.204, mean reward: -2.126 [-100.000,  6.103], mean action: 1.773 [0.000, 3.000],  loss: 21.540522, mse: 5601.240280, mean_q: 52.986126, mean_eps: 0.746240
  77053/300000: episode: 829, duration: 0.880s, episode steps: 107, steps per second: 122, episode reward: -177.423, mean reward: -1.658 [-100.000, 22.327], mean action: 1.458 [0.000, 3.000],  loss: 14.102798, mse: 5729.133794, mean_q: 53.749324, mean_eps: 0.745903
  77166/300000: episode: 830, duration: 0.832s, episode steps: 113, steps per second: 136, episode reward: -22.791, mean reward: -0.202 [-100.000, 21.044], mean action: 1.673 [0.000, 3.000],  loss: 20.150581, mse: 5679.288313, mean_q: 52.845200, mean_eps: 0.745540
  77265/300000: episode: 831, duration: 0.731s, episode steps:  99, steps per second: 136, episode reward: -41.256, mean reward: -0.417 [-100.000, 14.290], mean action: 1.394 [0.000, 3.000],  loss: 34.617988, mse: 5690.175475, mean_q: 51.564127, mean_eps: 0.745190
  77358/300000: episode: 832, duration: 0.732s, episode steps:  93, steps per second: 127, episode reward: -252.445, mean reward: -2.714 [-100.000, 89.179], mean action: 1.753 [0.000, 3.000],  loss: 22.053078, mse: 5720.102991, mean_q: 51.655595, mean_eps: 0.744874
  77472/300000: episode: 833, duration: 0.851s, episode steps: 114, steps per second: 134, episode reward: -106.010, mean reward: -0.930 [-100.000, 21.633], mean action: 1.281 [0.000, 3.000],  loss: 19.444758, mse: 5743.760312, mean_q: 53.032296, mean_eps: 0.744532
  77566/300000: episode: 834, duration: 0.685s, episode steps:  94, steps per second: 137, episode reward: -193.278, mean reward: -2.056 [-100.000,  6.137], mean action: 1.479 [0.000, 3.000],  loss: 11.342565, mse: 5718.500161, mean_q: 51.249120, mean_eps: 0.744189
  77641/300000: episode: 835, duration: 0.562s, episode steps:  75, steps per second: 133, episode reward: -98.784, mean reward: -1.317 [-100.000, 12.806], mean action: 1.573 [0.000, 3.000],  loss: 7.775966, mse: 5738.509759, mean_q: 53.212957, mean_eps: 0.743910
  77777/300000: episode: 836, duration: 1.050s, episode steps: 136, steps per second: 130, episode reward: -80.626, mean reward: -0.593 [-100.000, 19.592], mean action: 1.632 [0.000, 3.000],  loss: 12.127182, mse: 5618.097940, mean_q: 51.064996, mean_eps: 0.743562
  77858/300000: episode: 837, duration: 0.644s, episode steps:  81, steps per second: 126, episode reward: -15.671, mean reward: -0.193 [-100.000, 25.785], mean action: 1.568 [0.000, 3.000],  loss: 20.971949, mse: 5735.581664, mean_q: 52.283356, mean_eps: 0.743204
  77953/300000: episode: 838, duration: 0.754s, episode steps:  95, steps per second: 126, episode reward: -87.064, mean reward: -0.916 [-100.000, 11.589], mean action: 1.505 [0.000, 3.000],  loss: 20.606214, mse: 5698.826264, mean_q: 54.454149, mean_eps: 0.742913
  78051/300000: episode: 839, duration: 0.809s, episode steps:  98, steps per second: 121, episode reward: -108.114, mean reward: -1.103 [-100.000,  5.832], mean action: 1.561 [0.000, 3.000],  loss: 14.782244, mse: 5637.756542, mean_q: 51.966706, mean_eps: 0.742595
  78160/300000: episode: 840, duration: 0.845s, episode steps: 109, steps per second: 129, episode reward: -239.837, mean reward: -2.200 [-100.000,  4.264], mean action: 1.697 [0.000, 3.000],  loss: 16.113314, mse: 5834.735844, mean_q: 52.981181, mean_eps: 0.742254
  78259/300000: episode: 841, duration: 0.741s, episode steps:  99, steps per second: 134, episode reward: -59.921, mean reward: -0.605 [-100.000, 17.463], mean action: 1.606 [0.000, 3.000],  loss: 14.854618, mse: 5731.853422, mean_q: 50.920145, mean_eps: 0.741910
  78365/300000: episode: 842, duration: 0.869s, episode steps: 106, steps per second: 122, episode reward: -113.646, mean reward: -1.072 [-100.000, 14.719], mean action: 1.651 [0.000, 3.000],  loss: 15.673792, mse: 5778.454581, mean_q: 53.834663, mean_eps: 0.741572
  78468/300000: episode: 843, duration: 0.819s, episode steps: 103, steps per second: 126, episode reward: -139.685, mean reward: -1.356 [-100.000,  8.630], mean action: 1.583 [0.000, 3.000],  loss: 13.124362, mse: 5820.578879, mean_q: 53.323517, mean_eps: 0.741227
  78567/300000: episode: 844, duration: 0.838s, episode steps:  99, steps per second: 118, episode reward: -77.424, mean reward: -0.782 [-100.000, 10.807], mean action: 1.374 [0.000, 3.000],  loss: 13.066640, mse: 5773.782700, mean_q: 52.181089, mean_eps: 0.740894
  78653/300000: episode: 845, duration: 0.726s, episode steps:  86, steps per second: 118, episode reward: -95.153, mean reward: -1.106 [-100.000, 27.818], mean action: 1.640 [0.000, 3.000],  loss: 19.377279, mse: 5698.560007, mean_q: 51.384635, mean_eps: 0.740589
  78769/300000: episode: 846, duration: 0.913s, episode steps: 116, steps per second: 127, episode reward: -66.121, mean reward: -0.570 [-100.000,  8.199], mean action: 1.560 [0.000, 3.000],  loss: 12.888434, mse: 5860.336123, mean_q: 53.528287, mean_eps: 0.740255
  78871/300000: episode: 847, duration: 0.750s, episode steps: 102, steps per second: 136, episode reward: -168.681, mean reward: -1.654 [-100.000,  3.923], mean action: 1.549 [0.000, 3.000],  loss: 13.096404, mse: 5875.731622, mean_q: 52.478979, mean_eps: 0.739896
  79065/300000: episode: 848, duration: 1.452s, episode steps: 194, steps per second: 134, episode reward: -101.579, mean reward: -0.524 [-100.000, 49.415], mean action: 1.448 [0.000, 3.000],  loss: 15.674600, mse: 5788.871476, mean_q: 52.493493, mean_eps: 0.739407
  79192/300000: episode: 849, duration: 0.937s, episode steps: 127, steps per second: 136, episode reward: -170.750, mean reward: -1.344 [-100.000, 14.838], mean action: 1.583 [0.000, 3.000],  loss: 16.462920, mse: 5842.381794, mean_q: 52.047943, mean_eps: 0.738878
  79310/300000: episode: 850, duration: 0.904s, episode steps: 118, steps per second: 130, episode reward: -233.272, mean reward: -1.977 [-100.000,  6.461], mean action: 1.356 [0.000, 3.000],  loss: 12.442673, mse: 5826.555751, mean_q: 52.697119, mean_eps: 0.738473
  79400/300000: episode: 851, duration: 0.639s, episode steps:  90, steps per second: 141, episode reward: -89.186, mean reward: -0.991 [-100.000,  8.522], mean action: 1.500 [0.000, 3.000],  loss: 10.011447, mse: 5889.190571, mean_q: 53.929339, mean_eps: 0.738130
  79476/300000: episode: 852, duration: 0.541s, episode steps:  76, steps per second: 141, episode reward: -81.297, mean reward: -1.070 [-100.000, 19.483], mean action: 1.368 [0.000, 3.000],  loss: 21.328881, mse: 5792.208040, mean_q: 51.934922, mean_eps: 0.737856
  79556/300000: episode: 853, duration: 0.632s, episode steps:  80, steps per second: 127, episode reward: -74.501, mean reward: -0.931 [-100.000,  8.322], mean action: 1.613 [0.000, 3.000],  loss: 14.722454, mse: 5877.319293, mean_q: 51.470115, mean_eps: 0.737599
  79678/300000: episode: 854, duration: 0.836s, episode steps: 122, steps per second: 146, episode reward: -130.946, mean reward: -1.073 [-100.000, 21.470], mean action: 1.648 [0.000, 3.000],  loss: 15.067689, mse: 5871.100990, mean_q: 53.804513, mean_eps: 0.737266
  79783/300000: episode: 855, duration: 0.733s, episode steps: 105, steps per second: 143, episode reward: -63.268, mean reward: -0.603 [-100.000, 10.391], mean action: 1.762 [0.000, 3.000],  loss: 11.894688, mse: 5930.564453, mean_q: 53.467711, mean_eps: 0.736891
  79861/300000: episode: 856, duration: 0.584s, episode steps:  78, steps per second: 133, episode reward: -180.622, mean reward: -2.316 [-100.000, 10.077], mean action: 1.295 [0.000, 3.000],  loss: 11.745665, mse: 5712.304105, mean_q: 51.076293, mean_eps: 0.736589
  79979/300000: episode: 857, duration: 0.821s, episode steps: 118, steps per second: 144, episode reward: -310.970, mean reward: -2.635 [-100.000, 95.765], mean action: 1.610 [0.000, 3.000],  loss: 17.327608, mse: 5933.743317, mean_q: 53.421530, mean_eps: 0.736266
  80091/300000: episode: 858, duration: 0.795s, episode steps: 112, steps per second: 141, episode reward: -153.782, mean reward: -1.373 [-100.000, 10.213], mean action: 1.518 [0.000, 3.000],  loss: 15.074918, mse: 5846.854135, mean_q: 52.849360, mean_eps: 0.735886
  80171/300000: episode: 859, duration: 0.593s, episode steps:  80, steps per second: 135, episode reward: -4.246, mean reward: -0.053 [-100.000, 14.454], mean action: 1.538 [0.000, 3.000],  loss: 22.486082, mse: 5867.540631, mean_q: 52.355332, mean_eps: 0.735569
  80264/300000: episode: 860, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward: -57.853, mean reward: -0.622 [-100.000, 15.598], mean action: 1.538 [0.000, 3.000],  loss: 21.187385, mse: 5800.680150, mean_q: 51.777717, mean_eps: 0.735284
  80378/300000: episode: 861, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: -68.324, mean reward: -0.599 [-100.000,  8.678], mean action: 1.684 [0.000, 3.000],  loss: 16.274277, mse: 5822.909133, mean_q: 52.029677, mean_eps: 0.734942
  80450/300000: episode: 862, duration: 0.557s, episode steps:  72, steps per second: 129, episode reward: -61.526, mean reward: -0.855 [-100.000,  8.942], mean action: 1.722 [0.000, 3.000],  loss: 16.105511, mse: 5785.254408, mean_q: 51.461007, mean_eps: 0.734635
  80533/300000: episode: 863, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -155.094, mean reward: -1.869 [-100.000,  7.532], mean action: 1.675 [0.000, 3.000],  loss: 19.058920, mse: 5758.582320, mean_q: 50.596268, mean_eps: 0.734380
  80604/300000: episode: 864, duration: 0.512s, episode steps:  71, steps per second: 139, episode reward: -124.496, mean reward: -1.753 [-100.000,  5.455], mean action: 1.479 [0.000, 3.000],  loss: 20.386893, mse: 5833.761409, mean_q: 53.553919, mean_eps: 0.734126
  80727/300000: episode: 865, duration: 0.868s, episode steps: 123, steps per second: 142, episode reward: -136.997, mean reward: -1.114 [-100.000,  7.632], mean action: 1.374 [0.000, 3.000],  loss: 13.906202, mse: 5967.127116, mean_q: 53.312237, mean_eps: 0.733805
  80808/300000: episode: 866, duration: 0.582s, episode steps:  81, steps per second: 139, episode reward: -39.985, mean reward: -0.494 [-100.000,  9.934], mean action: 1.667 [0.000, 3.000],  loss: 17.087323, mse: 5874.924564, mean_q: 53.453470, mean_eps: 0.733469
  80878/300000: episode: 867, duration: 0.479s, episode steps:  70, steps per second: 146, episode reward: -48.199, mean reward: -0.689 [-100.000, 13.766], mean action: 1.571 [0.000, 3.000],  loss: 19.004633, mse: 6008.806396, mean_q: 54.419198, mean_eps: 0.733220
  80971/300000: episode: 868, duration: 0.652s, episode steps:  93, steps per second: 143, episode reward: -135.484, mean reward: -1.457 [-100.000,  9.181], mean action: 1.699 [0.000, 3.000],  loss: 16.535895, mse: 5826.495542, mean_q: 52.924270, mean_eps: 0.732951
  81038/300000: episode: 869, duration: 0.490s, episode steps:  67, steps per second: 137, episode reward: -66.342, mean reward: -0.990 [-100.000, 12.149], mean action: 1.433 [0.000, 3.000],  loss: 14.400300, mse: 5950.205238, mean_q: 53.774548, mean_eps: 0.732687
  81117/300000: episode: 870, duration: 0.572s, episode steps:  79, steps per second: 138, episode reward: -79.272, mean reward: -1.003 [-100.000,  6.824], mean action: 1.684 [0.000, 3.000],  loss: 9.516739, mse: 5837.480005, mean_q: 51.927579, mean_eps: 0.732446
  81242/300000: episode: 871, duration: 0.877s, episode steps: 125, steps per second: 142, episode reward: -131.761, mean reward: -1.054 [-100.000, 73.180], mean action: 1.688 [0.000, 3.000],  loss: 20.769210, mse: 5942.861469, mean_q: 53.714093, mean_eps: 0.732109
  81370/300000: episode: 872, duration: 0.921s, episode steps: 128, steps per second: 139, episode reward: -16.261, mean reward: -0.127 [-100.000, 17.364], mean action: 1.688 [0.000, 3.000],  loss: 17.891438, mse: 5943.544865, mean_q: 53.441848, mean_eps: 0.731692
  81493/300000: episode: 873, duration: 0.869s, episode steps: 123, steps per second: 142, episode reward: -52.107, mean reward: -0.424 [-100.000, 16.425], mean action: 1.642 [0.000, 3.000],  loss: 14.378322, mse: 5851.586636, mean_q: 52.922897, mean_eps: 0.731278
  81593/300000: episode: 874, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: -200.551, mean reward: -2.006 [-100.000, 15.562], mean action: 1.720 [0.000, 3.000],  loss: 13.065573, mse: 5916.692466, mean_q: 53.383526, mean_eps: 0.730910
  81677/300000: episode: 875, duration: 0.663s, episode steps:  84, steps per second: 127, episode reward: -85.542, mean reward: -1.018 [-100.000,  9.830], mean action: 1.500 [0.000, 3.000],  loss: 18.962772, mse: 5903.440308, mean_q: 53.934911, mean_eps: 0.730606
  81775/300000: episode: 876, duration: 0.959s, episode steps:  98, steps per second: 102, episode reward: -53.000, mean reward: -0.541 [-100.000, 13.469], mean action: 1.612 [0.000, 3.000],  loss: 18.026630, mse: 6065.657685, mean_q: 54.749021, mean_eps: 0.730306
  81890/300000: episode: 877, duration: 0.879s, episode steps: 115, steps per second: 131, episode reward: -86.295, mean reward: -0.750 [-100.000,  6.559], mean action: 1.643 [0.000, 3.000],  loss: 14.256125, mse: 5914.697223, mean_q: 53.613357, mean_eps: 0.729954
  82039/300000: episode: 878, duration: 1.187s, episode steps: 149, steps per second: 126, episode reward: -85.420, mean reward: -0.573 [-100.000,  6.740], mean action: 1.564 [0.000, 3.000],  loss: 13.436594, mse: 5781.509074, mean_q: 52.682475, mean_eps: 0.729519
  82109/300000: episode: 879, duration: 0.483s, episode steps:  70, steps per second: 145, episode reward: -64.687, mean reward: -0.924 [-100.000, 12.639], mean action: 1.586 [0.000, 3.000],  loss: 15.671796, mse: 6122.065597, mean_q: 55.503394, mean_eps: 0.729157
  82190/300000: episode: 880, duration: 0.583s, episode steps:  81, steps per second: 139, episode reward: -51.521, mean reward: -0.636 [-100.000, 11.246], mean action: 1.753 [0.000, 3.000],  loss: 18.291694, mse: 6073.930495, mean_q: 55.905395, mean_eps: 0.728908
  82285/300000: episode: 881, duration: 0.692s, episode steps:  95, steps per second: 137, episode reward: -55.837, mean reward: -0.588 [-100.000, 39.148], mean action: 1.568 [0.000, 3.000],  loss: 16.970021, mse: 6093.693873, mean_q: 54.814300, mean_eps: 0.728618
  82384/300000: episode: 882, duration: 0.681s, episode steps:  99, steps per second: 145, episode reward: -109.393, mean reward: -1.105 [-100.000, 12.669], mean action: 1.485 [0.000, 3.000],  loss: 18.355310, mse: 5991.195677, mean_q: 54.717740, mean_eps: 0.728298
  82511/300000: episode: 883, duration: 0.895s, episode steps: 127, steps per second: 142, episode reward: -86.282, mean reward: -0.679 [-100.000,  6.697], mean action: 1.701 [0.000, 3.000],  loss: 25.009922, mse: 5911.755713, mean_q: 54.080800, mean_eps: 0.727925
  82640/300000: episode: 884, duration: 1.017s, episode steps: 129, steps per second: 127, episode reward: -87.809, mean reward: -0.681 [-100.000, 18.722], mean action: 1.667 [0.000, 3.000],  loss: 23.580273, mse: 5983.305725, mean_q: 54.272718, mean_eps: 0.727503
  82711/300000: episode: 885, duration: 0.543s, episode steps:  71, steps per second: 131, episode reward: -67.047, mean reward: -0.944 [-100.000, 29.147], mean action: 1.521 [0.000, 3.000],  loss: 18.005843, mse: 5977.773926, mean_q: 52.897557, mean_eps: 0.727172
  82818/300000: episode: 886, duration: 0.852s, episode steps: 107, steps per second: 126, episode reward: -96.351, mean reward: -0.900 [-100.000, 10.665], mean action: 1.514 [0.000, 3.000],  loss: 18.158908, mse: 5987.409586, mean_q: 53.682666, mean_eps: 0.726879
  82936/300000: episode: 887, duration: 0.856s, episode steps: 118, steps per second: 138, episode reward: -124.358, mean reward: -1.054 [-100.000,  6.141], mean action: 1.585 [0.000, 3.000],  loss: 18.334650, mse: 5951.752826, mean_q: 54.227136, mean_eps: 0.726508
  83012/300000: episode: 888, duration: 0.536s, episode steps:  76, steps per second: 142, episode reward: -97.662, mean reward: -1.285 [-100.000, 12.792], mean action: 1.789 [0.000, 3.000],  loss: 17.776160, mse: 6045.008744, mean_q: 53.480986, mean_eps: 0.726187
  83118/300000: episode: 889, duration: 0.817s, episode steps: 106, steps per second: 130, episode reward: 12.132, mean reward:  0.114 [-100.000, 48.822], mean action: 1.509 [0.000, 3.000],  loss: 17.208350, mse: 6114.484260, mean_q: 54.459028, mean_eps: 0.725887
  83201/300000: episode: 890, duration: 0.580s, episode steps:  83, steps per second: 143, episode reward: -71.284, mean reward: -0.859 [-100.000, 10.550], mean action: 1.723 [0.000, 3.000],  loss: 17.293258, mse: 6011.654120, mean_q: 53.368134, mean_eps: 0.725575
  83280/300000: episode: 891, duration: 0.535s, episode steps:  79, steps per second: 148, episode reward: -37.280, mean reward: -0.472 [-100.000, 16.842], mean action: 1.797 [0.000, 3.000],  loss: 13.866256, mse: 6164.000729, mean_q: 54.555337, mean_eps: 0.725308
  83369/300000: episode: 892, duration: 0.627s, episode steps:  89, steps per second: 142, episode reward: -88.121, mean reward: -0.990 [-100.000, 11.005], mean action: 1.584 [0.000, 3.000],  loss: 22.805007, mse: 6210.036808, mean_q: 55.239987, mean_eps: 0.725031
  83469/300000: episode: 893, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -97.542, mean reward: -0.975 [-100.000, 40.121], mean action: 1.640 [0.000, 3.000],  loss: 17.121802, mse: 6119.250879, mean_q: 54.052355, mean_eps: 0.724719
  83615/300000: episode: 894, duration: 0.999s, episode steps: 146, steps per second: 146, episode reward: -275.379, mean reward: -1.886 [-100.000, 58.192], mean action: 1.658 [0.000, 3.000],  loss: 15.037833, mse: 6062.610726, mean_q: 54.764888, mean_eps: 0.724313
  83703/300000: episode: 895, duration: 0.637s, episode steps:  88, steps per second: 138, episode reward: -55.503, mean reward: -0.631 [-100.000, 22.503], mean action: 1.784 [0.000, 3.000],  loss: 21.311881, mse: 6195.810569, mean_q: 54.468404, mean_eps: 0.723927
  83820/300000: episode: 896, duration: 0.797s, episode steps: 117, steps per second: 147, episode reward: -87.107, mean reward: -0.745 [-100.000,  7.490], mean action: 1.564 [0.000, 3.000],  loss: 17.562178, mse: 6141.058402, mean_q: 55.148328, mean_eps: 0.723589
  83901/300000: episode: 897, duration: 0.545s, episode steps:  81, steps per second: 149, episode reward: -141.214, mean reward: -1.743 [-100.000, 21.269], mean action: 1.753 [0.000, 3.000],  loss: 26.645935, mse: 6173.116322, mean_q: 56.247734, mean_eps: 0.723262
  83995/300000: episode: 898, duration: 0.710s, episode steps:  94, steps per second: 132, episode reward: -86.965, mean reward: -0.925 [-100.000, 14.939], mean action: 1.713 [0.000, 3.000],  loss: 17.076086, mse: 6157.029510, mean_q: 53.436517, mean_eps: 0.722973
  84069/300000: episode: 899, duration: 0.555s, episode steps:  74, steps per second: 133, episode reward: -73.405, mean reward: -0.992 [-100.000, 13.048], mean action: 1.595 [0.000, 3.000],  loss: 18.330792, mse: 6145.596897, mean_q: 54.105070, mean_eps: 0.722696
  84154/300000: episode: 900, duration: 0.592s, episode steps:  85, steps per second: 144, episode reward: 17.725, mean reward:  0.209 [-100.000, 30.548], mean action: 1.765 [0.000, 3.000],  loss: 17.056391, mse: 6231.791119, mean_q: 54.788139, mean_eps: 0.722434
  84237/300000: episode: 901, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -79.907, mean reward: -0.963 [-100.000,  9.004], mean action: 1.542 [0.000, 3.000],  loss: 14.676454, mse: 5992.155609, mean_q: 54.266060, mean_eps: 0.722156
  84334/300000: episode: 902, duration: 0.772s, episode steps:  97, steps per second: 126, episode reward: -44.484, mean reward: -0.459 [-100.000, 20.316], mean action: 1.753 [0.000, 3.000],  loss: 15.142288, mse: 6208.493370, mean_q: 55.826782, mean_eps: 0.721859
  84473/300000: episode: 903, duration: 1.038s, episode steps: 139, steps per second: 134, episode reward: -69.021, mean reward: -0.497 [-100.000,  6.458], mean action: 1.676 [0.000, 3.000],  loss: 13.430070, mse: 6101.877684, mean_q: 55.203329, mean_eps: 0.721470
  84583/300000: episode: 904, duration: 0.846s, episode steps: 110, steps per second: 130, episode reward: -111.422, mean reward: -1.013 [-100.000, 12.036], mean action: 1.745 [0.000, 3.000],  loss: 12.675071, mse: 6122.297874, mean_q: 55.735454, mean_eps: 0.721059
  84647/300000: episode: 905, duration: 0.535s, episode steps:  64, steps per second: 120, episode reward: -46.610, mean reward: -0.728 [-100.000, 53.556], mean action: 1.922 [0.000, 3.000],  loss: 26.634362, mse: 6014.547012, mean_q: 53.058458, mean_eps: 0.720772
  84747/300000: episode: 906, duration: 0.868s, episode steps: 100, steps per second: 115, episode reward: -75.575, mean reward: -0.756 [-100.000, 11.504], mean action: 1.560 [0.000, 3.000],  loss: 15.538562, mse: 6123.045322, mean_q: 55.955411, mean_eps: 0.720502
  84884/300000: episode: 907, duration: 1.212s, episode steps: 137, steps per second: 113, episode reward: -68.536, mean reward: -0.500 [-100.000, 12.942], mean action: 1.577 [0.000, 3.000],  loss: 14.863783, mse: 6104.954233, mean_q: 54.244669, mean_eps: 0.720110
  84989/300000: episode: 908, duration: 0.868s, episode steps: 105, steps per second: 121, episode reward: -99.178, mean reward: -0.945 [-100.000, 13.021], mean action: 1.543 [0.000, 3.000],  loss: 15.083181, mse: 6194.120215, mean_q: 54.962771, mean_eps: 0.719711
  85098/300000: episode: 909, duration: 1.062s, episode steps: 109, steps per second: 103, episode reward: -13.176, mean reward: -0.121 [-100.000, 36.624], mean action: 1.376 [0.000, 3.000],  loss: 16.070329, mse: 6132.941160, mean_q: 55.065230, mean_eps: 0.719358
  85223/300000: episode: 910, duration: 0.921s, episode steps: 125, steps per second: 136, episode reward: -153.728, mean reward: -1.230 [-100.000,  5.746], mean action: 1.736 [0.000, 3.000],  loss: 16.716944, mse: 6420.779066, mean_q: 57.602569, mean_eps: 0.718972
  85313/300000: episode: 911, duration: 0.668s, episode steps:  90, steps per second: 135, episode reward: -101.130, mean reward: -1.124 [-100.000, 23.089], mean action: 1.667 [0.000, 3.000],  loss: 16.638718, mse: 6232.434863, mean_q: 54.877411, mean_eps: 0.718617
  85387/300000: episode: 912, duration: 0.569s, episode steps:  74, steps per second: 130, episode reward: -47.634, mean reward: -0.644 [-100.000, 12.297], mean action: 1.581 [0.000, 3.000],  loss: 18.708449, mse: 6270.308567, mean_q: 55.547960, mean_eps: 0.718347
  85470/300000: episode: 913, duration: 0.606s, episode steps:  83, steps per second: 137, episode reward: -89.361, mean reward: -1.077 [-100.000,  8.251], mean action: 1.735 [0.000, 3.000],  loss: 18.881931, mse: 6156.507660, mean_q: 55.286889, mean_eps: 0.718088
  85559/300000: episode: 914, duration: 0.647s, episode steps:  89, steps per second: 138, episode reward: -57.580, mean reward: -0.647 [-100.000, 12.089], mean action: 1.494 [0.000, 3.000],  loss: 14.459634, mse: 6165.292689, mean_q: 54.872812, mean_eps: 0.717804
  85682/300000: episode: 915, duration: 0.904s, episode steps: 123, steps per second: 136, episode reward: -37.933, mean reward: -0.308 [-100.000,  7.857], mean action: 1.659 [0.000, 3.000],  loss: 12.186293, mse: 6258.071551, mean_q: 55.864905, mean_eps: 0.717454
  85771/300000: episode: 916, duration: 0.674s, episode steps:  89, steps per second: 132, episode reward: -97.790, mean reward: -1.099 [-100.000,  6.281], mean action: 1.674 [0.000, 3.000],  loss: 13.762471, mse: 6198.871352, mean_q: 54.629544, mean_eps: 0.717104
  85879/300000: episode: 917, duration: 0.780s, episode steps: 108, steps per second: 139, episode reward: -113.358, mean reward: -1.050 [-100.000, 14.057], mean action: 1.861 [0.000, 3.000],  loss: 14.430653, mse: 6146.670935, mean_q: 55.769034, mean_eps: 0.716779
  85976/300000: episode: 918, duration: 0.752s, episode steps:  97, steps per second: 129, episode reward: -34.066, mean reward: -0.351 [-100.000, 13.688], mean action: 1.619 [0.000, 3.000],  loss: 21.450329, mse: 6179.765177, mean_q: 56.236866, mean_eps: 0.716441
  86086/300000: episode: 919, duration: 0.804s, episode steps: 110, steps per second: 137, episode reward: -79.721, mean reward: -0.725 [-100.000, 14.330], mean action: 1.600 [0.000, 3.000],  loss: 20.542490, mse: 6262.874765, mean_q: 55.178544, mean_eps: 0.716099
  86199/300000: episode: 920, duration: 0.790s, episode steps: 113, steps per second: 143, episode reward: -70.281, mean reward: -0.622 [-100.000, 46.064], mean action: 1.593 [0.000, 3.000],  loss: 15.775850, mse: 6456.179990, mean_q: 56.842000, mean_eps: 0.715731
  86316/300000: episode: 921, duration: 0.945s, episode steps: 117, steps per second: 124, episode reward: -153.463, mean reward: -1.312 [-100.000,  8.008], mean action: 1.607 [0.000, 3.000],  loss: 15.443732, mse: 6242.364521, mean_q: 54.552109, mean_eps: 0.715352
  86448/300000: episode: 922, duration: 0.995s, episode steps: 132, steps per second: 133, episode reward: -71.252, mean reward: -0.540 [-100.000,  8.476], mean action: 1.636 [0.000, 3.000],  loss: 19.961716, mse: 6356.708881, mean_q: 55.104007, mean_eps: 0.714941
  86567/300000: episode: 923, duration: 0.875s, episode steps: 119, steps per second: 136, episode reward: -78.793, mean reward: -0.662 [-100.000, 18.786], mean action: 1.597 [0.000, 3.000],  loss: 18.183561, mse: 6519.703991, mean_q: 57.924346, mean_eps: 0.714527
  86654/300000: episode: 924, duration: 0.612s, episode steps:  87, steps per second: 142, episode reward: -64.497, mean reward: -0.741 [-100.000, 10.880], mean action: 1.621 [0.000, 3.000],  loss: 14.863960, mse: 6296.207261, mean_q: 55.222736, mean_eps: 0.714187
  86778/300000: episode: 925, duration: 0.899s, episode steps: 124, steps per second: 138, episode reward: -103.430, mean reward: -0.834 [-100.000, 18.929], mean action: 1.677 [0.000, 3.000],  loss: 16.776838, mse: 6548.909735, mean_q: 57.332093, mean_eps: 0.713839
  86903/300000: episode: 926, duration: 0.965s, episode steps: 125, steps per second: 129, episode reward: -45.825, mean reward: -0.367 [-100.000, 20.229], mean action: 1.736 [0.000, 3.000],  loss: 17.444533, mse: 6456.283012, mean_q: 57.159000, mean_eps: 0.713428
  87023/300000: episode: 927, duration: 0.823s, episode steps: 120, steps per second: 146, episode reward: -121.900, mean reward: -1.016 [-100.000, 18.362], mean action: 1.708 [0.000, 3.000],  loss: 11.375027, mse: 6561.835567, mean_q: 57.841758, mean_eps: 0.713024
  87104/300000: episode: 928, duration: 0.584s, episode steps:  81, steps per second: 139, episode reward: -20.626, mean reward: -0.255 [-100.000, 11.384], mean action: 1.531 [0.000, 3.000],  loss: 18.610926, mse: 6542.385163, mean_q: 56.634852, mean_eps: 0.712692
  87202/300000: episode: 929, duration: 0.702s, episode steps:  98, steps per second: 140, episode reward: -40.284, mean reward: -0.411 [-100.000, 11.276], mean action: 1.663 [0.000, 3.000],  loss: 11.803227, mse: 6648.930256, mean_q: 56.485072, mean_eps: 0.712397
  87333/300000: episode: 930, duration: 0.896s, episode steps: 131, steps per second: 146, episode reward: -81.615, mean reward: -0.623 [-100.000, 11.601], mean action: 1.550 [0.000, 3.000],  loss: 13.749351, mse: 6514.570588, mean_q: 57.017184, mean_eps: 0.712019
  87416/300000: episode: 931, duration: 0.613s, episode steps:  83, steps per second: 135, episode reward: -175.068, mean reward: -2.109 [-100.000, 15.600], mean action: 1.675 [0.000, 3.000],  loss: 17.890599, mse: 6677.778797, mean_q: 57.899821, mean_eps: 0.711666
  87514/300000: episode: 932, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -89.670, mean reward: -0.915 [-100.000, 16.514], mean action: 1.704 [0.000, 3.000],  loss: 11.857343, mse: 6577.251046, mean_q: 56.816899, mean_eps: 0.711367
  87606/300000: episode: 933, duration: 0.642s, episode steps:  92, steps per second: 143, episode reward: -76.534, mean reward: -0.832 [-100.000, 11.199], mean action: 1.717 [0.000, 3.000],  loss: 12.355900, mse: 6728.624963, mean_q: 58.397158, mean_eps: 0.711054
  87684/300000: episode: 934, duration: 0.564s, episode steps:  78, steps per second: 138, episode reward: -83.304, mean reward: -1.068 [-100.000, 11.133], mean action: 1.526 [0.000, 3.000],  loss: 14.644249, mse: 6603.102126, mean_q: 57.327525, mean_eps: 0.710773
  87780/300000: episode: 935, duration: 0.727s, episode steps:  96, steps per second: 132, episode reward: -26.677, mean reward: -0.278 [-100.000, 12.813], mean action: 1.656 [0.000, 3.000],  loss: 11.718599, mse: 6723.086253, mean_q: 58.407721, mean_eps: 0.710486
  87874/300000: episode: 936, duration: 0.643s, episode steps:  94, steps per second: 146, episode reward: -32.996, mean reward: -0.351 [-100.000,  7.502], mean action: 1.723 [0.000, 3.000],  loss: 18.721113, mse: 6687.626361, mean_q: 58.664692, mean_eps: 0.710173
  88014/300000: episode: 937, duration: 0.963s, episode steps: 140, steps per second: 145, episode reward: -84.742, mean reward: -0.605 [-100.000,  6.002], mean action: 1.579 [0.000, 3.000],  loss: 14.005664, mse: 6614.442104, mean_q: 57.313852, mean_eps: 0.709786
  88111/300000: episode: 938, duration: 0.706s, episode steps:  97, steps per second: 137, episode reward: -79.642, mean reward: -0.821 [-100.000, 16.612], mean action: 1.619 [0.000, 3.000],  loss: 10.661235, mse: 6626.897982, mean_q: 57.280459, mean_eps: 0.709395
  88197/300000: episode: 939, duration: 0.685s, episode steps:  86, steps per second: 125, episode reward: -123.280, mean reward: -1.433 [-100.000, 12.608], mean action: 1.779 [0.000, 3.000],  loss: 11.467028, mse: 6698.953994, mean_q: 57.070860, mean_eps: 0.709093
  88291/300000: episode: 940, duration: 0.712s, episode steps:  94, steps per second: 132, episode reward: -47.687, mean reward: -0.507 [-100.000, 15.871], mean action: 1.521 [0.000, 3.000],  loss: 15.256881, mse: 6889.799862, mean_q: 58.877185, mean_eps: 0.708796
  88427/300000: episode: 941, duration: 1.023s, episode steps: 136, steps per second: 133, episode reward: -37.276, mean reward: -0.274 [-100.000, 11.067], mean action: 1.662 [0.000, 3.000],  loss: 10.789742, mse: 6817.787967, mean_q: 57.489232, mean_eps: 0.708417
  88556/300000: episode: 942, duration: 0.973s, episode steps: 129, steps per second: 133, episode reward: -93.540, mean reward: -0.725 [-100.000, 20.057], mean action: 1.504 [0.000, 3.000],  loss: 18.802199, mse: 6646.062523, mean_q: 56.824005, mean_eps: 0.707980
  88669/300000: episode: 943, duration: 0.876s, episode steps: 113, steps per second: 129, episode reward: -284.872, mean reward: -2.521 [-100.000, 123.566], mean action: 1.673 [0.000, 3.000],  loss: 10.330853, mse: 6638.123107, mean_q: 56.609998, mean_eps: 0.707580
  88762/300000: episode: 944, duration: 0.694s, episode steps:  93, steps per second: 134, episode reward: -94.194, mean reward: -1.013 [-100.000,  8.625], mean action: 1.613 [0.000, 3.000],  loss: 14.298581, mse: 6578.046381, mean_q: 55.490737, mean_eps: 0.707241
  88845/300000: episode: 945, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -87.946, mean reward: -1.060 [-100.000, 26.805], mean action: 1.675 [0.000, 3.000],  loss: 22.518827, mse: 6662.846438, mean_q: 57.827901, mean_eps: 0.706950
  88944/300000: episode: 946, duration: 0.781s, episode steps:  99, steps per second: 127, episode reward: -36.501, mean reward: -0.369 [-100.000, 10.160], mean action: 1.687 [0.000, 3.000],  loss: 13.799753, mse: 6675.722607, mean_q: 56.510731, mean_eps: 0.706650
  89106/300000: episode: 947, duration: 1.163s, episode steps: 162, steps per second: 139, episode reward: -0.577, mean reward: -0.004 [-100.000, 29.645], mean action: 1.673 [0.000, 3.000],  loss: 11.865151, mse: 6708.264329, mean_q: 56.579905, mean_eps: 0.706219
  89227/300000: episode: 948, duration: 0.924s, episode steps: 121, steps per second: 131, episode reward: -108.432, mean reward: -0.896 [-100.000, 24.376], mean action: 1.793 [0.000, 3.000],  loss: 12.060177, mse: 6842.806750, mean_q: 56.981927, mean_eps: 0.705752
  89319/300000: episode: 949, duration: 0.687s, episode steps:  92, steps per second: 134, episode reward: -42.287, mean reward: -0.460 [-100.000, 10.208], mean action: 1.793 [0.000, 3.000],  loss: 11.977889, mse: 6797.438508, mean_q: 57.475671, mean_eps: 0.705401
  89424/300000: episode: 950, duration: 0.757s, episode steps: 105, steps per second: 139, episode reward: -46.027, mean reward: -0.438 [-100.000,  7.480], mean action: 1.552 [0.000, 3.000],  loss: 16.624874, mse: 6851.288853, mean_q: 57.443973, mean_eps: 0.705076
  89503/300000: episode: 951, duration: 0.613s, episode steps:  79, steps per second: 129, episode reward: -117.605, mean reward: -1.489 [-100.000, 16.088], mean action: 1.620 [0.000, 3.000],  loss: 16.363007, mse: 6874.729529, mean_q: 57.848549, mean_eps: 0.704772
  89631/300000: episode: 952, duration: 0.936s, episode steps: 128, steps per second: 137, episode reward: -89.054, mean reward: -0.696 [-100.000, 30.166], mean action: 1.625 [0.000, 3.000],  loss: 14.262099, mse: 6821.363350, mean_q: 57.797594, mean_eps: 0.704431
  89734/300000: episode: 953, duration: 0.757s, episode steps: 103, steps per second: 136, episode reward: -188.473, mean reward: -1.830 [-100.000, 33.520], mean action: 1.786 [0.000, 3.000],  loss: 12.835700, mse: 6904.092954, mean_q: 57.981161, mean_eps: 0.704049
  89812/300000: episode: 954, duration: 0.588s, episode steps:  78, steps per second: 133, episode reward: -49.359, mean reward: -0.633 [-100.000,  9.376], mean action: 1.641 [0.000, 3.000],  loss: 8.363943, mse: 6764.455585, mean_q: 56.727059, mean_eps: 0.703751
  89936/300000: episode: 955, duration: 0.888s, episode steps: 124, steps per second: 140, episode reward: -203.729, mean reward: -1.643 [-100.000, 20.518], mean action: 1.774 [0.000, 3.000],  loss: 13.732152, mse: 7046.264550, mean_q: 58.678159, mean_eps: 0.703417
  90047/300000: episode: 956, duration: 0.836s, episode steps: 111, steps per second: 133, episode reward: -14.140, mean reward: -0.127 [-100.000, 18.106], mean action: 1.775 [0.000, 3.000],  loss: 16.500730, mse: 6813.446043, mean_q: 56.548429, mean_eps: 0.703030
  90165/300000: episode: 957, duration: 0.868s, episode steps: 118, steps per second: 136, episode reward: -72.037, mean reward: -0.610 [-100.000, 18.907], mean action: 1.602 [0.000, 3.000],  loss: 10.348908, mse: 6888.706452, mean_q: 59.059247, mean_eps: 0.702652
  90257/300000: episode: 958, duration: 0.669s, episode steps:  92, steps per second: 137, episode reward: -76.038, mean reward: -0.826 [-100.000, 20.923], mean action: 1.609 [0.000, 3.000],  loss: 11.040515, mse: 6899.333034, mean_q: 57.660011, mean_eps: 0.702305
  90365/300000: episode: 959, duration: 0.794s, episode steps: 108, steps per second: 136, episode reward: -89.491, mean reward: -0.829 [-100.000, 20.363], mean action: 1.509 [0.000, 3.000],  loss: 10.001673, mse: 7087.519319, mean_q: 59.185030, mean_eps: 0.701975
  90448/300000: episode: 960, duration: 0.595s, episode steps:  83, steps per second: 140, episode reward: -59.029, mean reward: -0.711 [-100.000,  8.791], mean action: 1.578 [0.000, 3.000],  loss: 10.991412, mse: 7147.214150, mean_q: 59.650203, mean_eps: 0.701660
  90558/300000: episode: 961, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: -83.158, mean reward: -0.756 [-100.000,  8.503], mean action: 1.673 [0.000, 3.000],  loss: 12.367093, mse: 7035.515079, mean_q: 58.506702, mean_eps: 0.701342
  90647/300000: episode: 962, duration: 0.628s, episode steps:  89, steps per second: 142, episode reward: -106.412, mean reward: -1.196 [-100.000, 17.385], mean action: 1.730 [0.000, 3.000],  loss: 10.795372, mse: 6957.403260, mean_q: 58.334271, mean_eps: 0.701013
  90787/300000: episode: 963, duration: 1.093s, episode steps: 140, steps per second: 128, episode reward: -77.296, mean reward: -0.552 [-100.000,  8.401], mean action: 1.579 [0.000, 3.000],  loss: 12.178674, mse: 7051.725757, mean_q: 59.182955, mean_eps: 0.700636
  90863/300000: episode: 964, duration: 0.562s, episode steps:  76, steps per second: 135, episode reward: -51.474, mean reward: -0.677 [-100.000, 10.098], mean action: 1.500 [0.000, 3.000],  loss: 10.102504, mse: 6972.423899, mean_q: 58.856703, mean_eps: 0.700279
  90990/300000: episode: 965, duration: 0.978s, episode steps: 127, steps per second: 130, episode reward: -141.079, mean reward: -1.111 [-100.000,  7.744], mean action: 1.575 [0.000, 3.000],  loss: 11.784126, mse: 7031.342047, mean_q: 59.013866, mean_eps: 0.699944
  91127/300000: episode: 966, duration: 0.967s, episode steps: 137, steps per second: 142, episode reward: -54.995, mean reward: -0.401 [-100.000, 11.034], mean action: 1.555 [0.000, 3.000],  loss: 10.538422, mse: 7001.510186, mean_q: 58.527005, mean_eps: 0.699509
  91198/300000: episode: 967, duration: 0.503s, episode steps:  71, steps per second: 141, episode reward: 13.628, mean reward:  0.192 [-100.000, 76.494], mean action: 1.507 [0.000, 3.000],  loss: 13.759177, mse: 7004.054323, mean_q: 59.859924, mean_eps: 0.699165
  91277/300000: episode: 968, duration: 0.608s, episode steps:  79, steps per second: 130, episode reward: -47.279, mean reward: -0.598 [-100.000,  7.776], mean action: 1.646 [0.000, 3.000],  loss: 12.187689, mse: 7026.395743, mean_q: 59.111553, mean_eps: 0.698918
  91368/300000: episode: 969, duration: 0.652s, episode steps:  91, steps per second: 139, episode reward: -83.303, mean reward: -0.915 [-100.000, 11.126], mean action: 1.736 [0.000, 3.000],  loss: 10.809815, mse: 6982.640373, mean_q: 58.525507, mean_eps: 0.698637
  91434/300000: episode: 970, duration: 0.486s, episode steps:  66, steps per second: 136, episode reward: -83.634, mean reward: -1.267 [-100.000,  7.532], mean action: 1.697 [0.000, 3.000],  loss: 10.702723, mse: 7146.084635, mean_q: 58.617819, mean_eps: 0.698378
  91515/300000: episode: 971, duration: 0.712s, episode steps:  81, steps per second: 114, episode reward: -84.734, mean reward: -1.046 [-100.000,  9.748], mean action: 1.667 [0.000, 3.000],  loss: 16.623069, mse: 7070.597409, mean_q: 58.142851, mean_eps: 0.698136
  91626/300000: episode: 972, duration: 0.968s, episode steps: 111, steps per second: 115, episode reward: -132.496, mean reward: -1.194 [-100.000,  9.173], mean action: 1.586 [0.000, 3.000],  loss: 11.873780, mse: 7173.681522, mean_q: 59.929376, mean_eps: 0.697819
  91738/300000: episode: 973, duration: 0.887s, episode steps: 112, steps per second: 126, episode reward: -30.376, mean reward: -0.271 [-100.000, 11.959], mean action: 1.616 [0.000, 3.000],  loss: 13.448907, mse: 7065.740204, mean_q: 58.666048, mean_eps: 0.697451
  91901/300000: episode: 974, duration: 1.360s, episode steps: 163, steps per second: 120, episode reward: -119.906, mean reward: -0.736 [-100.000,  6.368], mean action: 1.571 [0.000, 3.000],  loss: 10.321484, mse: 7000.443818, mean_q: 57.205385, mean_eps: 0.696997
  92049/300000: episode: 975, duration: 1.157s, episode steps: 148, steps per second: 128, episode reward: -71.114, mean reward: -0.481 [-100.000, 44.372], mean action: 1.601 [0.000, 3.000],  loss: 11.788964, mse: 7023.905699, mean_q: 58.249075, mean_eps: 0.696484
  92165/300000: episode: 976, duration: 0.853s, episode steps: 116, steps per second: 136, episode reward: -165.094, mean reward: -1.423 [-100.000, 42.935], mean action: 1.733 [0.000, 3.000],  loss: 17.164939, mse: 7210.142326, mean_q: 60.181842, mean_eps: 0.696049
  92283/300000: episode: 977, duration: 0.849s, episode steps: 118, steps per second: 139, episode reward: -69.579, mean reward: -0.590 [-100.000, 15.662], mean action: 1.602 [0.000, 3.000],  loss: 12.632159, mse: 7080.344756, mean_q: 59.596027, mean_eps: 0.695662
  92392/300000: episode: 978, duration: 0.813s, episode steps: 109, steps per second: 134, episode reward: -132.407, mean reward: -1.215 [-100.000, 11.217], mean action: 1.706 [0.000, 3.000],  loss: 13.323479, mse: 7146.388882, mean_q: 61.075951, mean_eps: 0.695288
  92493/300000: episode: 979, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: -42.387, mean reward: -0.420 [-100.000, 29.689], mean action: 1.653 [0.000, 3.000],  loss: 14.589303, mse: 7239.847381, mean_q: 59.651585, mean_eps: 0.694941
  92592/300000: episode: 980, duration: 0.695s, episode steps:  99, steps per second: 142, episode reward: -44.219, mean reward: -0.447 [-100.000,  9.085], mean action: 1.818 [0.000, 3.000],  loss: 10.414966, mse: 7028.385811, mean_q: 59.152100, mean_eps: 0.694611
  92691/300000: episode: 981, duration: 0.744s, episode steps:  99, steps per second: 133, episode reward: -75.405, mean reward: -0.762 [-100.000, 21.709], mean action: 1.515 [0.000, 3.000],  loss: 11.085316, mse: 7109.603831, mean_q: 60.041212, mean_eps: 0.694285
  92818/300000: episode: 982, duration: 0.895s, episode steps: 127, steps per second: 142, episode reward:  2.607, mean reward:  0.021 [-100.000, 83.448], mean action: 1.709 [0.000, 3.000],  loss: 14.810678, mse: 7063.118310, mean_q: 59.197556, mean_eps: 0.693912
  92921/300000: episode: 983, duration: 0.712s, episode steps: 103, steps per second: 145, episode reward: -260.627, mean reward: -2.530 [-100.000, 59.919], mean action: 1.650 [0.000, 3.000],  loss: 13.022999, mse: 6863.258604, mean_q: 57.993509, mean_eps: 0.693532
  93071/300000: episode: 984, duration: 1.119s, episode steps: 150, steps per second: 134, episode reward:  4.606, mean reward:  0.031 [-100.000, 32.043], mean action: 1.673 [0.000, 3.000],  loss: 14.481405, mse: 7164.484056, mean_q: 59.795563, mean_eps: 0.693115
  93215/300000: episode: 985, duration: 1.111s, episode steps: 144, steps per second: 130, episode reward: -212.056, mean reward: -1.473 [-100.000, 71.409], mean action: 1.625 [0.000, 3.000],  loss: 16.867446, mse: 7198.669512, mean_q: 59.409817, mean_eps: 0.692630
  93299/300000: episode: 986, duration: 0.696s, episode steps:  84, steps per second: 121, episode reward:  8.504, mean reward:  0.101 [-100.000, 24.597], mean action: 1.524 [0.000, 3.000],  loss: 14.030068, mse: 7280.708851, mean_q: 60.911556, mean_eps: 0.692254
  93384/300000: episode: 987, duration: 0.786s, episode steps:  85, steps per second: 108, episode reward: -75.980, mean reward: -0.894 [-100.000,  8.190], mean action: 1.471 [0.000, 3.000],  loss: 10.533442, mse: 7289.382640, mean_q: 60.235993, mean_eps: 0.691975
  93502/300000: episode: 988, duration: 1.107s, episode steps: 118, steps per second: 107, episode reward: -68.723, mean reward: -0.582 [-100.000, 11.424], mean action: 1.669 [0.000, 3.000],  loss: 11.172186, mse: 7359.801096, mean_q: 59.780156, mean_eps: 0.691640
  93591/300000: episode: 989, duration: 0.787s, episode steps:  89, steps per second: 113, episode reward: -86.593, mean reward: -0.973 [-100.000,  7.440], mean action: 1.719 [0.000, 3.000],  loss: 15.300485, mse: 7224.967741, mean_q: 59.324045, mean_eps: 0.691298
  93676/300000: episode: 990, duration: 0.718s, episode steps:  85, steps per second: 118, episode reward: -57.304, mean reward: -0.674 [-100.000, 11.411], mean action: 1.600 [0.000, 3.000],  loss: 16.595213, mse: 7329.737684, mean_q: 60.521280, mean_eps: 0.691011
  93766/300000: episode: 991, duration: 0.771s, episode steps:  90, steps per second: 117, episode reward: -81.889, mean reward: -0.910 [-100.000, 10.595], mean action: 1.578 [0.000, 3.000],  loss: 12.321572, mse: 7356.457004, mean_q: 61.126998, mean_eps: 0.690722
  93899/300000: episode: 992, duration: 1.032s, episode steps: 133, steps per second: 129, episode reward: -86.728, mean reward: -0.652 [-100.000,  7.314], mean action: 1.714 [0.000, 3.000],  loss: 12.545322, mse: 7215.403698, mean_q: 58.528412, mean_eps: 0.690354
  93987/300000: episode: 993, duration: 0.635s, episode steps:  88, steps per second: 139, episode reward: -138.403, mean reward: -1.573 [-100.000,  6.856], mean action: 1.557 [0.000, 3.000],  loss: 15.985178, mse: 7283.620938, mean_q: 60.814771, mean_eps: 0.689990
  94069/300000: episode: 994, duration: 0.608s, episode steps:  82, steps per second: 135, episode reward: -55.218, mean reward: -0.673 [-100.000, 18.252], mean action: 1.561 [0.000, 3.000],  loss: 16.759745, mse: 7346.792945, mean_q: 60.812979, mean_eps: 0.689709
  94143/300000: episode: 995, duration: 0.515s, episode steps:  74, steps per second: 144, episode reward: -47.698, mean reward: -0.645 [-100.000, 13.127], mean action: 1.622 [0.000, 3.000],  loss: 13.709272, mse: 7434.558811, mean_q: 61.532420, mean_eps: 0.689452
  94261/300000: episode: 996, duration: 0.865s, episode steps: 118, steps per second: 136, episode reward: -107.715, mean reward: -0.913 [-100.000, 19.377], mean action: 1.627 [0.000, 3.000],  loss: 16.159878, mse: 7335.667795, mean_q: 60.043083, mean_eps: 0.689135
  94354/300000: episode: 997, duration: 0.712s, episode steps:  93, steps per second: 131, episode reward: -19.513, mean reward: -0.210 [-100.000, 11.838], mean action: 1.613 [0.000, 3.000],  loss: 10.233244, mse: 7196.527218, mean_q: 59.840981, mean_eps: 0.688787
  94423/300000: episode: 998, duration: 0.530s, episode steps:  69, steps per second: 130, episode reward: -63.464, mean reward: -0.920 [-100.000, 33.551], mean action: 1.536 [0.000, 3.000],  loss: 14.831109, mse: 7751.901863, mean_q: 63.301507, mean_eps: 0.688520
  94567/300000: episode: 999, duration: 0.993s, episode steps: 144, steps per second: 145, episode reward: -67.663, mean reward: -0.470 [-100.000, 10.608], mean action: 1.681 [0.000, 3.000],  loss: 14.108269, mse: 7384.901679, mean_q: 60.805908, mean_eps: 0.688168
  94694/300000: episode: 1000, duration: 0.918s, episode steps: 127, steps per second: 138, episode reward: -65.262, mean reward: -0.514 [-100.000, 11.257], mean action: 1.630 [0.000, 3.000],  loss: 13.883073, mse: 7392.443156, mean_q: 60.466327, mean_eps: 0.687721
  94797/300000: episode: 1001, duration: 0.712s, episode steps: 103, steps per second: 145, episode reward: -76.220, mean reward: -0.740 [-100.000, 23.774], mean action: 1.670 [0.000, 3.000],  loss: 16.038719, mse: 7474.135548, mean_q: 60.398022, mean_eps: 0.687342
  94910/300000: episode: 1002, duration: 0.803s, episode steps: 113, steps per second: 141, episode reward: -96.409, mean reward: -0.853 [-100.000, 19.627], mean action: 1.602 [0.000, 3.000],  loss: 15.204877, mse: 7348.491220, mean_q: 59.619287, mean_eps: 0.686985
  95009/300000: episode: 1003, duration: 0.696s, episode steps:  99, steps per second: 142, episode reward: -194.344, mean reward: -1.963 [-100.000, 55.878], mean action: 1.758 [0.000, 3.000],  loss: 13.001433, mse: 7323.630489, mean_q: 60.899730, mean_eps: 0.686635
  95126/300000: episode: 1004, duration: 0.792s, episode steps: 117, steps per second: 148, episode reward: -100.082, mean reward: -0.855 [-100.000, 37.637], mean action: 1.658 [0.000, 3.000],  loss: 14.122167, mse: 7429.245781, mean_q: 60.518274, mean_eps: 0.686279
  95218/300000: episode: 1005, duration: 0.675s, episode steps:  92, steps per second: 136, episode reward: -88.465, mean reward: -0.962 [-100.000, 15.189], mean action: 1.630 [0.000, 3.000],  loss: 14.261237, mse: 7639.630642, mean_q: 61.574018, mean_eps: 0.685934
  95324/300000: episode: 1006, duration: 0.795s, episode steps: 106, steps per second: 133, episode reward: -80.818, mean reward: -0.762 [-100.000,  7.799], mean action: 1.613 [0.000, 3.000],  loss: 11.385941, mse: 7766.458017, mean_q: 62.347143, mean_eps: 0.685607
  95402/300000: episode: 1007, duration: 0.577s, episode steps:  78, steps per second: 135, episode reward: -136.177, mean reward: -1.746 [-100.000,  9.262], mean action: 1.628 [0.000, 3.000],  loss: 12.782698, mse: 7548.009753, mean_q: 60.450335, mean_eps: 0.685304
  95475/300000: episode: 1008, duration: 0.530s, episode steps:  73, steps per second: 138, episode reward: -28.998, mean reward: -0.397 [-100.000, 17.030], mean action: 1.507 [0.000, 3.000],  loss: 14.288349, mse: 7824.261144, mean_q: 65.015922, mean_eps: 0.685055
  95572/300000: episode: 1009, duration: 0.762s, episode steps:  97, steps per second: 127, episode reward: -74.860, mean reward: -0.772 [-100.000,  9.320], mean action: 1.732 [0.000, 3.000],  loss: 12.996028, mse: 7603.910494, mean_q: 62.131397, mean_eps: 0.684774
  95699/300000: episode: 1010, duration: 0.922s, episode steps: 127, steps per second: 138, episode reward: -50.795, mean reward: -0.400 [-100.000, 20.256], mean action: 1.724 [0.000, 3.000],  loss: 13.875369, mse: 7609.627907, mean_q: 61.708770, mean_eps: 0.684404
  95836/300000: episode: 1011, duration: 1.026s, episode steps: 137, steps per second: 133, episode reward: -100.578, mean reward: -0.734 [-100.000, 11.726], mean action: 1.635 [0.000, 3.000],  loss: 11.689482, mse: 7564.290991, mean_q: 61.511452, mean_eps: 0.683969
  95929/300000: episode: 1012, duration: 0.678s, episode steps:  93, steps per second: 137, episode reward: -132.396, mean reward: -1.424 [-100.000, 10.284], mean action: 1.710 [0.000, 3.000],  loss: 11.383976, mse: 7691.736381, mean_q: 63.125029, mean_eps: 0.683589
  96314/300000: episode: 1013, duration: 2.854s, episode steps: 385, steps per second: 135, episode reward: -99.002, mean reward: -0.257 [-100.000, 68.263], mean action: 1.769 [0.000, 3.000],  loss: 12.891225, mse: 7632.959754, mean_q: 61.761431, mean_eps: 0.682801
  96431/300000: episode: 1014, duration: 0.830s, episode steps: 117, steps per second: 141, episode reward: -139.121, mean reward: -1.189 [-100.000, 14.896], mean action: 1.726 [0.000, 3.000],  loss: 16.865450, mse: 7528.649660, mean_q: 61.464624, mean_eps: 0.681972
  96555/300000: episode: 1015, duration: 0.877s, episode steps: 124, steps per second: 141, episode reward: -165.380, mean reward: -1.334 [-100.000, 11.522], mean action: 1.565 [0.000, 3.000],  loss: 15.923090, mse: 7319.207015, mean_q: 58.621392, mean_eps: 0.681575
  96656/300000: episode: 1016, duration: 0.686s, episode steps: 101, steps per second: 147, episode reward: -53.482, mean reward: -0.530 [-100.000,  6.922], mean action: 1.733 [0.000, 3.000],  loss: 17.215617, mse: 7804.961000, mean_q: 63.473205, mean_eps: 0.681203
  96788/300000: episode: 1017, duration: 0.971s, episode steps: 132, steps per second: 136, episode reward: -99.439, mean reward: -0.753 [-100.000,  9.194], mean action: 1.765 [0.000, 3.000],  loss: 14.005199, mse: 7662.883367, mean_q: 62.432740, mean_eps: 0.680819
  96918/300000: episode: 1018, duration: 0.924s, episode steps: 130, steps per second: 141, episode reward: -87.286, mean reward: -0.671 [-100.000,  9.839], mean action: 1.746 [0.000, 3.000],  loss: 14.194869, mse: 7591.475691, mean_q: 62.211970, mean_eps: 0.680387
  97033/300000: episode: 1019, duration: 0.885s, episode steps: 115, steps per second: 130, episode reward: -47.390, mean reward: -0.412 [-100.000,  9.781], mean action: 1.678 [0.000, 3.000],  loss: 17.227363, mse: 7425.636613, mean_q: 59.316938, mean_eps: 0.679982
  97117/300000: episode: 1020, duration: 0.613s, episode steps:  84, steps per second: 137, episode reward: -54.213, mean reward: -0.645 [-100.000,  7.673], mean action: 1.690 [0.000, 3.000],  loss: 17.193988, mse: 7903.922904, mean_q: 62.290798, mean_eps: 0.679654
  97262/300000: episode: 1021, duration: 0.995s, episode steps: 145, steps per second: 146, episode reward: -58.336, mean reward: -0.402 [-100.000, 12.108], mean action: 1.676 [0.000, 3.000],  loss: 13.524138, mse: 7703.745821, mean_q: 61.353910, mean_eps: 0.679276
  97384/300000: episode: 1022, duration: 0.900s, episode steps: 122, steps per second: 135, episode reward: -15.448, mean reward: -0.127 [-100.000, 12.852], mean action: 1.615 [0.000, 3.000],  loss: 15.400800, mse: 7597.650903, mean_q: 59.379667, mean_eps: 0.678836
  97470/300000: episode: 1023, duration: 0.608s, episode steps:  86, steps per second: 141, episode reward: -33.398, mean reward: -0.388 [-100.000, 12.753], mean action: 1.744 [0.000, 3.000],  loss: 19.540653, mse: 7773.935036, mean_q: 62.746750, mean_eps: 0.678493
  97575/300000: episode: 1024, duration: 0.721s, episode steps: 105, steps per second: 146, episode reward: -176.585, mean reward: -1.682 [-100.000, 25.167], mean action: 1.514 [0.000, 3.000],  loss: 15.983774, mse: 7778.072484, mean_q: 61.564755, mean_eps: 0.678177
  97701/300000: episode: 1025, duration: 0.938s, episode steps: 126, steps per second: 134, episode reward: -215.579, mean reward: -1.711 [-100.000,  4.984], mean action: 1.762 [0.000, 3.000],  loss: 13.442408, mse: 7781.240296, mean_q: 61.454699, mean_eps: 0.677796
  97777/300000: episode: 1026, duration: 0.526s, episode steps:  76, steps per second: 145, episode reward: -70.876, mean reward: -0.933 [-100.000, 16.949], mean action: 1.526 [0.000, 3.000],  loss: 13.092707, mse: 7490.443192, mean_q: 60.450558, mean_eps: 0.677463
  98393/300000: episode: 1027, duration: 4.617s, episode steps: 616, steps per second: 133, episode reward: -51.661, mean reward: -0.084 [-100.000, 76.040], mean action: 1.534 [0.000, 3.000],  loss: 13.067148, mse: 7718.146306, mean_q: 60.613726, mean_eps: 0.676321
  98483/300000: episode: 1028, duration: 0.665s, episode steps:  90, steps per second: 135, episode reward: -35.937, mean reward: -0.399 [-100.000,  9.454], mean action: 1.556 [0.000, 3.000],  loss: 12.700194, mse: 7687.195253, mean_q: 61.031156, mean_eps: 0.675156
  98597/300000: episode: 1029, duration: 1.052s, episode steps: 114, steps per second: 108, episode reward: -79.745, mean reward: -0.700 [-100.000,  9.889], mean action: 1.658 [0.000, 3.000],  loss: 12.817837, mse: 7824.582468, mean_q: 61.887937, mean_eps: 0.674820
  98683/300000: episode: 1030, duration: 0.817s, episode steps:  86, steps per second: 105, episode reward: -102.071, mean reward: -1.187 [-100.000, 12.575], mean action: 1.465 [0.000, 3.000],  loss: 16.839520, mse: 7661.075474, mean_q: 59.794349, mean_eps: 0.674490
  98773/300000: episode: 1031, duration: 0.693s, episode steps:  90, steps per second: 130, episode reward: -17.698, mean reward: -0.197 [-100.000, 13.340], mean action: 1.533 [0.000, 3.000],  loss: 9.989690, mse: 7802.268072, mean_q: 61.650718, mean_eps: 0.674199
  98861/300000: episode: 1032, duration: 0.637s, episode steps:  88, steps per second: 138, episode reward: -31.946, mean reward: -0.363 [-100.000, 18.051], mean action: 1.591 [0.000, 3.000],  loss: 14.645144, mse: 7766.027377, mean_q: 60.425418, mean_eps: 0.673906
  98941/300000: episode: 1033, duration: 0.630s, episode steps:  80, steps per second: 127, episode reward: -93.870, mean reward: -1.173 [-100.000, 23.540], mean action: 1.875 [0.000, 3.000],  loss: 22.068728, mse: 7703.590460, mean_q: 60.552469, mean_eps: 0.673628
  99082/300000: episode: 1034, duration: 1.118s, episode steps: 141, steps per second: 126, episode reward: -129.408, mean reward: -0.918 [-100.000,  5.663], mean action: 1.674 [0.000, 3.000],  loss: 14.084042, mse: 7757.610466, mean_q: 60.559403, mean_eps: 0.673264
  99198/300000: episode: 1035, duration: 0.832s, episode steps: 116, steps per second: 139, episode reward: -41.904, mean reward: -0.361 [-100.000, 24.334], mean action: 1.578 [0.000, 3.000],  loss: 17.180336, mse: 7737.107973, mean_q: 58.788949, mean_eps: 0.672840
  99300/300000: episode: 1036, duration: 0.769s, episode steps: 102, steps per second: 133, episode reward: -85.025, mean reward: -0.834 [-100.000, 12.213], mean action: 1.627 [0.000, 3.000],  loss: 13.936530, mse: 7790.980521, mean_q: 59.805587, mean_eps: 0.672480
  99372/300000: episode: 1037, duration: 0.543s, episode steps:  72, steps per second: 133, episode reward: -12.003, mean reward: -0.167 [-100.000, 14.518], mean action: 1.611 [0.000, 3.000],  loss: 13.342472, mse: 7845.395766, mean_q: 59.315553, mean_eps: 0.672193
  99506/300000: episode: 1038, duration: 0.961s, episode steps: 134, steps per second: 139, episode reward: -97.879, mean reward: -0.730 [-100.000, 25.625], mean action: 1.754 [0.000, 3.000],  loss: 10.339168, mse: 7864.165502, mean_q: 60.279680, mean_eps: 0.671853
  99663/300000: episode: 1039, duration: 1.152s, episode steps: 157, steps per second: 136, episode reward: -85.108, mean reward: -0.542 [-100.000,  9.703], mean action: 1.656 [0.000, 3.000],  loss: 16.719883, mse: 7939.590509, mean_q: 61.337425, mean_eps: 0.671373
  99756/300000: episode: 1040, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -182.471, mean reward: -1.962 [-100.000, 29.162], mean action: 1.677 [0.000, 3.000],  loss: 17.141088, mse: 7903.372401, mean_q: 61.185321, mean_eps: 0.670960
  99861/300000: episode: 1041, duration: 0.770s, episode steps: 105, steps per second: 136, episode reward: -70.579, mean reward: -0.672 [-100.000, 22.402], mean action: 1.676 [0.000, 3.000],  loss: 15.929302, mse: 8102.279269, mean_q: 62.386604, mean_eps: 0.670634
  99956/300000: episode: 1042, duration: 0.711s, episode steps:  95, steps per second: 134, episode reward: -75.338, mean reward: -0.793 [-100.000, 12.712], mean action: 1.611 [0.000, 3.000],  loss: 10.590114, mse: 8149.448504, mean_q: 62.822693, mean_eps: 0.670304
 100086/300000: episode: 1043, duration: 0.888s, episode steps: 130, steps per second: 146, episode reward: -35.936, mean reward: -0.276 [-100.000, 51.481], mean action: 1.631 [0.000, 3.000],  loss: 13.915637, mse: 8032.329999, mean_q: 62.843370, mean_eps: 0.669932
 100172/300000: episode: 1044, duration: 0.587s, episode steps:  86, steps per second: 146, episode reward: -30.227, mean reward: -0.351 [-100.000, 12.079], mean action: 1.802 [0.000, 3.000],  loss: 17.315488, mse: 7969.212777, mean_q: 61.837098, mean_eps: 0.669576
 100244/300000: episode: 1045, duration: 0.595s, episode steps:  72, steps per second: 121, episode reward: -56.809, mean reward: -0.789 [-100.000,  8.531], mean action: 1.444 [0.000, 3.000],  loss: 10.963407, mse: 7952.362305, mean_q: 62.313709, mean_eps: 0.669315
 100346/300000: episode: 1046, duration: 0.891s, episode steps: 102, steps per second: 114, episode reward: -97.603, mean reward: -0.957 [-100.000, 11.063], mean action: 1.735 [0.000, 3.000],  loss: 12.987801, mse: 8112.512408, mean_q: 63.395394, mean_eps: 0.669028
 100470/300000: episode: 1047, duration: 1.067s, episode steps: 124, steps per second: 116, episode reward: -71.783, mean reward: -0.579 [-100.000,  9.939], mean action: 1.694 [0.000, 3.000],  loss: 15.432867, mse: 7867.885014, mean_q: 61.270393, mean_eps: 0.668655
 100576/300000: episode: 1048, duration: 0.818s, episode steps: 106, steps per second: 130, episode reward: -216.147, mean reward: -2.039 [-100.000, 50.293], mean action: 1.792 [0.000, 3.000],  loss: 14.053877, mse: 7899.773603, mean_q: 61.234719, mean_eps: 0.668276
 100659/300000: episode: 1049, duration: 0.653s, episode steps:  83, steps per second: 127, episode reward: -59.911, mean reward: -0.722 [-100.000,  8.751], mean action: 1.554 [0.000, 3.000],  loss: 14.699129, mse: 7957.052899, mean_q: 62.413505, mean_eps: 0.667964
 100769/300000: episode: 1050, duration: 0.852s, episode steps: 110, steps per second: 129, episode reward: -101.736, mean reward: -0.925 [-100.000, 15.839], mean action: 1.700 [0.000, 3.000],  loss: 13.890173, mse: 8030.276966, mean_q: 62.907980, mean_eps: 0.667645
 100868/300000: episode: 1051, duration: 0.757s, episode steps:  99, steps per second: 131, episode reward: -38.589, mean reward: -0.390 [-100.000, 24.339], mean action: 1.596 [0.000, 3.000],  loss: 15.034448, mse: 8149.101286, mean_q: 63.401297, mean_eps: 0.667301
 100976/300000: episode: 1052, duration: 0.876s, episode steps: 108, steps per second: 123, episode reward: -35.767, mean reward: -0.331 [-100.000, 12.171], mean action: 1.648 [0.000, 3.000],  loss: 11.067130, mse: 8024.376867, mean_q: 62.240400, mean_eps: 0.666959
 101098/300000: episode: 1053, duration: 1.019s, episode steps: 122, steps per second: 120, episode reward: -207.676, mean reward: -1.702 [-100.000, 60.425], mean action: 1.770 [0.000, 3.000],  loss: 11.426708, mse: 8067.588351, mean_q: 62.850021, mean_eps: 0.666580
 101204/300000: episode: 1054, duration: 0.803s, episode steps: 106, steps per second: 132, episode reward: -210.106, mean reward: -1.982 [-100.000,  5.085], mean action: 1.632 [0.000, 3.000],  loss: 15.965050, mse: 8031.052205, mean_q: 60.674231, mean_eps: 0.666203
 101286/300000: episode: 1055, duration: 0.680s, episode steps:  82, steps per second: 121, episode reward: -1.899, mean reward: -0.023 [-100.000, 59.347], mean action: 1.793 [0.000, 3.000],  loss: 16.363496, mse: 8136.228194, mean_q: 63.785215, mean_eps: 0.665893
 101388/300000: episode: 1056, duration: 0.799s, episode steps: 102, steps per second: 128, episode reward: -103.341, mean reward: -1.013 [-100.000,  9.780], mean action: 1.647 [0.000, 3.000],  loss: 14.334532, mse: 7993.151736, mean_q: 61.834013, mean_eps: 0.665590
 101850/300000: episode: 1057, duration: 3.817s, episode steps: 462, steps per second: 121, episode reward: -231.208, mean reward: -0.500 [-100.000, 29.081], mean action: 1.630 [0.000, 3.000],  loss: 14.286475, mse: 7984.782324, mean_q: 61.695150, mean_eps: 0.664659
 101969/300000: episode: 1058, duration: 0.931s, episode steps: 119, steps per second: 128, episode reward: -124.706, mean reward: -1.048 [-100.000, 17.284], mean action: 1.739 [0.000, 3.000],  loss: 14.926841, mse: 8191.068790, mean_q: 63.248339, mean_eps: 0.663700
 102053/300000: episode: 1059, duration: 0.781s, episode steps:  84, steps per second: 108, episode reward: -90.049, mean reward: -1.072 [-100.000, 20.492], mean action: 1.488 [0.000, 3.000],  loss: 14.986571, mse: 8053.421352, mean_q: 61.682882, mean_eps: 0.663365
 102182/300000: episode: 1060, duration: 1.141s, episode steps: 129, steps per second: 113, episode reward: -27.731, mean reward: -0.215 [-100.000, 10.742], mean action: 1.674 [0.000, 3.000],  loss: 16.252784, mse: 8119.542480, mean_q: 62.898832, mean_eps: 0.663014
 102312/300000: episode: 1061, duration: 1.141s, episode steps: 130, steps per second: 114, episode reward: -28.119, mean reward: -0.216 [-100.000, 11.673], mean action: 1.608 [0.000, 3.000],  loss: 18.292623, mse: 8000.325300, mean_q: 61.107172, mean_eps: 0.662587
 102409/300000: episode: 1062, duration: 1.060s, episode steps:  97, steps per second:  91, episode reward: -85.819, mean reward: -0.885 [-100.000, 17.278], mean action: 1.639 [0.000, 3.000],  loss: 16.531687, mse: 8113.995897, mean_q: 63.205846, mean_eps: 0.662212
 102515/300000: episode: 1063, duration: 1.096s, episode steps: 106, steps per second:  97, episode reward: -134.683, mean reward: -1.271 [-100.000, 16.673], mean action: 1.736 [0.000, 3.000],  loss: 12.966098, mse: 8029.817562, mean_q: 61.836531, mean_eps: 0.661877
 102597/300000: episode: 1064, duration: 0.653s, episode steps:  82, steps per second: 125, episode reward: -51.089, mean reward: -0.623 [-100.000,  8.521], mean action: 1.756 [0.000, 3.000],  loss: 16.622585, mse: 8087.661198, mean_q: 61.414861, mean_eps: 0.661567
 102681/300000: episode: 1065, duration: 0.597s, episode steps:  84, steps per second: 141, episode reward: -41.747, mean reward: -0.497 [-100.000, 19.097], mean action: 1.798 [0.000, 3.000],  loss: 19.289787, mse: 8380.339908, mean_q: 64.622958, mean_eps: 0.661293
 102803/300000: episode: 1066, duration: 0.903s, episode steps: 122, steps per second: 135, episode reward: -47.921, mean reward: -0.393 [-100.000, 11.798], mean action: 1.549 [0.000, 3.000],  loss: 14.569063, mse: 8132.840992, mean_q: 62.350851, mean_eps: 0.660953
 102962/300000: episode: 1067, duration: 1.118s, episode steps: 159, steps per second: 142, episode reward: -75.549, mean reward: -0.475 [-100.000, 21.962], mean action: 1.654 [0.000, 3.000],  loss: 17.884009, mse: 8235.114823, mean_q: 63.890254, mean_eps: 0.660489
 103049/300000: episode: 1068, duration: 0.605s, episode steps:  87, steps per second: 144, episode reward: -12.900, mean reward: -0.148 [-100.000, 29.351], mean action: 1.713 [0.000, 3.000],  loss: 13.585536, mse: 8304.016669, mean_q: 62.906014, mean_eps: 0.660084
 103200/300000: episode: 1069, duration: 1.127s, episode steps: 151, steps per second: 134, episode reward: -40.010, mean reward: -0.265 [-100.000,  9.212], mean action: 1.576 [0.000, 3.000],  loss: 15.900710, mse: 8260.222601, mean_q: 63.930879, mean_eps: 0.659691
 103265/300000: episode: 1070, duration: 0.455s, episode steps:  65, steps per second: 143, episode reward: -53.689, mean reward: -0.826 [-100.000, 13.441], mean action: 1.308 [0.000, 3.000],  loss: 14.346378, mse: 7921.558759, mean_q: 61.587672, mean_eps: 0.659334
 103370/300000: episode: 1071, duration: 0.822s, episode steps: 105, steps per second: 128, episode reward: -92.049, mean reward: -0.877 [-100.000, 10.212], mean action: 1.667 [0.000, 3.000],  loss: 16.010327, mse: 8233.211277, mean_q: 63.377655, mean_eps: 0.659054
 103510/300000: episode: 1072, duration: 1.648s, episode steps: 140, steps per second:  85, episode reward: -240.008, mean reward: -1.714 [-100.000, 71.142], mean action: 1.600 [0.000, 3.000],  loss: 13.686348, mse: 8121.659382, mean_q: 62.359170, mean_eps: 0.658650
 103607/300000: episode: 1073, duration: 1.004s, episode steps:  97, steps per second:  97, episode reward: -57.997, mean reward: -0.598 [-100.000, 10.445], mean action: 1.763 [0.000, 3.000],  loss: 13.718564, mse: 8124.590443, mean_q: 61.557060, mean_eps: 0.658259
 103717/300000: episode: 1074, duration: 0.842s, episode steps: 110, steps per second: 131, episode reward: -55.450, mean reward: -0.504 [-100.000, 10.439], mean action: 1.691 [0.000, 3.000],  loss: 17.870818, mse: 7923.260218, mean_q: 60.594068, mean_eps: 0.657917
 103793/300000: episode: 1075, duration: 0.637s, episode steps:  76, steps per second: 119, episode reward: -86.269, mean reward: -1.135 [-100.000,  6.919], mean action: 1.816 [0.000, 3.000],  loss: 19.223798, mse: 8139.143940, mean_q: 62.833594, mean_eps: 0.657610
 103869/300000: episode: 1076, duration: 0.804s, episode steps:  76, steps per second:  95, episode reward: -21.608, mean reward: -0.284 [-100.000, 15.772], mean action: 1.684 [0.000, 3.000],  loss: 11.937342, mse: 8057.099956, mean_q: 62.674328, mean_eps: 0.657359
 103978/300000: episode: 1077, duration: 1.294s, episode steps: 109, steps per second:  84, episode reward: -48.470, mean reward: -0.445 [-100.000, 10.043], mean action: 1.514 [0.000, 3.000],  loss: 12.979051, mse: 8254.703524, mean_q: 63.590417, mean_eps: 0.657054
 104085/300000: episode: 1078, duration: 0.898s, episode steps: 107, steps per second: 119, episode reward: -229.449, mean reward: -2.144 [-100.000,  6.815], mean action: 1.682 [0.000, 3.000],  loss: 18.195543, mse: 8139.346150, mean_q: 62.680418, mean_eps: 0.656698
 104201/300000: episode: 1079, duration: 0.928s, episode steps: 116, steps per second: 125, episode reward: -21.465, mean reward: -0.185 [-100.000, 11.543], mean action: 1.621 [0.000, 3.000],  loss: 14.236625, mse: 8330.281275, mean_q: 63.447846, mean_eps: 0.656330
 104337/300000: episode: 1080, duration: 1.154s, episode steps: 136, steps per second: 118, episode reward: -98.645, mean reward: -0.725 [-100.000, 11.016], mean action: 1.728 [0.000, 3.000],  loss: 13.687376, mse: 8232.963594, mean_q: 63.682814, mean_eps: 0.655914
 104436/300000: episode: 1081, duration: 0.839s, episode steps:  99, steps per second: 118, episode reward: -43.623, mean reward: -0.441 [-100.000, 21.446], mean action: 1.556 [0.000, 3.000],  loss: 14.186478, mse: 8545.632497, mean_q: 64.938735, mean_eps: 0.655526
 104562/300000: episode: 1082, duration: 1.087s, episode steps: 126, steps per second: 116, episode reward: -14.571, mean reward: -0.116 [-100.000, 21.418], mean action: 1.595 [0.000, 3.000],  loss: 15.388432, mse: 8272.874554, mean_q: 63.712207, mean_eps: 0.655155
 104688/300000: episode: 1083, duration: 0.954s, episode steps: 126, steps per second: 132, episode reward: -116.295, mean reward: -0.923 [-100.000, 45.819], mean action: 1.635 [0.000, 3.000],  loss: 17.696881, mse: 8188.476733, mean_q: 64.040746, mean_eps: 0.654739
 104778/300000: episode: 1084, duration: 0.694s, episode steps:  90, steps per second: 130, episode reward: -87.967, mean reward: -0.977 [-100.000, 10.933], mean action: 1.578 [0.000, 3.000],  loss: 18.735910, mse: 8304.446951, mean_q: 64.334168, mean_eps: 0.654383
 104852/300000: episode: 1085, duration: 0.610s, episode steps:  74, steps per second: 121, episode reward: -39.424, mean reward: -0.533 [-100.000, 12.751], mean action: 1.743 [0.000, 3.000],  loss: 17.158681, mse: 8242.570788, mean_q: 63.241576, mean_eps: 0.654112
 104926/300000: episode: 1086, duration: 0.570s, episode steps:  74, steps per second: 130, episode reward: -78.785, mean reward: -1.065 [-100.000, 30.736], mean action: 1.743 [0.000, 3.000],  loss: 15.442650, mse: 8201.122862, mean_q: 63.004221, mean_eps: 0.653868
 105112/300000: episode: 1087, duration: 1.409s, episode steps: 186, steps per second: 132, episode reward: -116.551, mean reward: -0.627 [-100.000, 12.027], mean action: 1.742 [0.000, 3.000],  loss: 15.836024, mse: 8435.247412, mean_q: 65.995519, mean_eps: 0.653439
 105212/300000: episode: 1088, duration: 0.724s, episode steps: 100, steps per second: 138, episode reward: -32.345, mean reward: -0.323 [-100.000, 11.661], mean action: 1.670 [0.000, 3.000],  loss: 17.207916, mse: 8288.799678, mean_q: 64.570360, mean_eps: 0.652967
 105325/300000: episode: 1089, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -38.579, mean reward: -0.341 [-100.000, 12.605], mean action: 1.673 [0.000, 3.000],  loss: 17.106086, mse: 8248.335752, mean_q: 64.675873, mean_eps: 0.652616
 105452/300000: episode: 1090, duration: 0.916s, episode steps: 127, steps per second: 139, episode reward: -71.510, mean reward: -0.563 [-100.000, 21.035], mean action: 1.567 [0.000, 3.000],  loss: 16.804647, mse: 8175.041512, mean_q: 64.000131, mean_eps: 0.652220
 105534/300000: episode: 1091, duration: 0.578s, episode steps:  82, steps per second: 142, episode reward: -63.466, mean reward: -0.774 [-100.000, 25.260], mean action: 1.671 [0.000, 3.000],  loss: 16.035787, mse: 8331.636278, mean_q: 65.705470, mean_eps: 0.651875
 105662/300000: episode: 1092, duration: 0.885s, episode steps: 128, steps per second: 145, episode reward: -17.673, mean reward: -0.138 [-100.000, 10.773], mean action: 1.539 [0.000, 3.000],  loss: 14.178866, mse: 8139.901588, mean_q: 64.070245, mean_eps: 0.651528
 105760/300000: episode: 1093, duration: 0.718s, episode steps:  98, steps per second: 137, episode reward: -41.080, mean reward: -0.419 [-100.000, 14.650], mean action: 1.612 [0.000, 3.000],  loss: 15.415650, mse: 8259.873436, mean_q: 64.542256, mean_eps: 0.651155
 105878/300000: episode: 1094, duration: 0.813s, episode steps: 118, steps per second: 145, episode reward: -152.234, mean reward: -1.290 [-100.000,  7.296], mean action: 1.712 [0.000, 3.000],  loss: 21.820789, mse: 8116.041409, mean_q: 63.696761, mean_eps: 0.650799
 106179/300000: episode: 1095, duration: 2.190s, episode steps: 301, steps per second: 137, episode reward: -182.011, mean reward: -0.605 [-100.000, 68.980], mean action: 1.708 [0.000, 3.000],  loss: 16.415685, mse: 8425.804670, mean_q: 65.120714, mean_eps: 0.650108
 106366/300000: episode: 1096, duration: 1.520s, episode steps: 187, steps per second: 123, episode reward: -65.898, mean reward: -0.352 [-100.000, 25.913], mean action: 1.668 [0.000, 3.000],  loss: 14.083339, mse: 8188.573613, mean_q: 62.606773, mean_eps: 0.649302
 106452/300000: episode: 1097, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -48.532, mean reward: -0.564 [-100.000,  9.428], mean action: 1.663 [0.000, 3.000],  loss: 16.506545, mse: 8546.393367, mean_q: 66.451910, mean_eps: 0.648852
 106674/300000: episode: 1098, duration: 1.646s, episode steps: 222, steps per second: 135, episode reward: -84.028, mean reward: -0.379 [-100.000, 16.078], mean action: 1.626 [0.000, 3.000],  loss: 22.502341, mse: 8389.914408, mean_q: 64.958651, mean_eps: 0.648344
 106790/300000: episode: 1099, duration: 0.790s, episode steps: 116, steps per second: 147, episode reward: -100.168, mean reward: -0.864 [-100.000, 55.052], mean action: 1.698 [0.000, 3.000],  loss: 17.112538, mse: 8515.039951, mean_q: 65.975313, mean_eps: 0.647786
 106909/300000: episode: 1100, duration: 0.837s, episode steps: 119, steps per second: 142, episode reward: -79.647, mean reward: -0.669 [-100.000, 10.588], mean action: 1.849 [0.000, 3.000],  loss: 18.026469, mse: 8174.060526, mean_q: 64.340022, mean_eps: 0.647398
 107004/300000: episode: 1101, duration: 0.720s, episode steps:  95, steps per second: 132, episode reward:  9.846, mean reward:  0.104 [-100.000, 25.935], mean action: 1.695 [0.000, 3.000],  loss: 14.108848, mse: 8437.298057, mean_q: 64.145886, mean_eps: 0.647045
 107121/300000: episode: 1102, duration: 0.805s, episode steps: 117, steps per second: 145, episode reward: -68.450, mean reward: -0.585 [-100.000,  8.328], mean action: 1.718 [0.000, 3.000],  loss: 14.306273, mse: 8580.706284, mean_q: 65.040797, mean_eps: 0.646695
 107232/300000: episode: 1103, duration: 0.839s, episode steps: 111, steps per second: 132, episode reward: -174.202, mean reward: -1.569 [-100.000, 26.169], mean action: 1.649 [0.000, 3.000],  loss: 15.693949, mse: 8631.645288, mean_q: 66.206691, mean_eps: 0.646319
 107359/300000: episode: 1104, duration: 0.925s, episode steps: 127, steps per second: 137, episode reward: -117.793, mean reward: -0.928 [-100.000, 10.678], mean action: 1.638 [0.000, 3.000],  loss: 14.037936, mse: 8582.830682, mean_q: 65.375263, mean_eps: 0.645926
 107468/300000: episode: 1105, duration: 0.756s, episode steps: 109, steps per second: 144, episode reward: -66.616, mean reward: -0.611 [-100.000, 11.414], mean action: 1.817 [0.000, 3.000],  loss: 15.360430, mse: 8389.036841, mean_q: 63.949052, mean_eps: 0.645537
 107561/300000: episode: 1106, duration: 0.689s, episode steps:  93, steps per second: 135, episode reward: -69.774, mean reward: -0.750 [-100.000, 14.593], mean action: 1.699 [0.000, 3.000],  loss: 15.107720, mse: 8434.206837, mean_q: 64.200779, mean_eps: 0.645204
 107641/300000: episode: 1107, duration: 0.585s, episode steps:  80, steps per second: 137, episode reward: -95.025, mean reward: -1.188 [-100.000, 35.818], mean action: 1.462 [0.000, 3.000],  loss: 12.915517, mse: 8599.758325, mean_q: 66.034867, mean_eps: 0.644918
 107759/300000: episode: 1108, duration: 0.862s, episode steps: 118, steps per second: 137, episode reward: -137.426, mean reward: -1.165 [-100.000,  6.091], mean action: 1.864 [0.000, 3.000],  loss: 15.089672, mse: 8457.488852, mean_q: 64.244507, mean_eps: 0.644592
 107868/300000: episode: 1109, duration: 0.866s, episode steps: 109, steps per second: 126, episode reward: -77.070, mean reward: -0.707 [-100.000, 12.630], mean action: 1.633 [0.000, 3.000],  loss: 15.434990, mse: 8508.229734, mean_q: 64.840638, mean_eps: 0.644217
 108484/300000: episode: 1110, duration: 4.956s, episode steps: 616, steps per second: 124, episode reward: -321.361, mean reward: -0.522 [-100.000, 42.306], mean action: 1.721 [0.000, 3.000],  loss: 15.646875, mse: 8737.812231, mean_q: 64.860418, mean_eps: 0.643021
 108572/300000: episode: 1111, duration: 0.689s, episode steps:  88, steps per second: 128, episode reward: -44.137, mean reward: -0.502 [-100.000, 10.991], mean action: 1.534 [0.000, 3.000],  loss: 20.559493, mse: 8746.670005, mean_q: 63.728642, mean_eps: 0.641859
 108705/300000: episode: 1112, duration: 1.049s, episode steps: 133, steps per second: 127, episode reward: -59.086, mean reward: -0.444 [-100.000, 11.111], mean action: 1.617 [0.000, 3.000],  loss: 12.392325, mse: 8681.435114, mean_q: 63.973599, mean_eps: 0.641495
 108814/300000: episode: 1113, duration: 0.829s, episode steps: 109, steps per second: 131, episode reward: -61.467, mean reward: -0.564 [-100.000, 21.842], mean action: 1.743 [0.000, 3.000],  loss: 13.828425, mse: 8691.046364, mean_q: 64.476824, mean_eps: 0.641095
 108935/300000: episode: 1114, duration: 1.063s, episode steps: 121, steps per second: 114, episode reward: -16.259, mean reward: -0.134 [-100.000, 16.224], mean action: 1.504 [0.000, 3.000],  loss: 20.329407, mse: 8741.774023, mean_q: 64.594373, mean_eps: 0.640716
 109052/300000: episode: 1115, duration: 0.843s, episode steps: 117, steps per second: 139, episode reward: -101.725, mean reward: -0.869 [-100.000,  6.636], mean action: 1.744 [0.000, 3.000],  loss: 15.775433, mse: 8774.282218, mean_q: 65.801538, mean_eps: 0.640323
 109145/300000: episode: 1116, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -34.360, mean reward: -0.369 [-100.000, 15.711], mean action: 1.753 [0.000, 3.000],  loss: 16.865172, mse: 8931.571903, mean_q: 66.853024, mean_eps: 0.639977
 109224/300000: episode: 1117, duration: 0.573s, episode steps:  79, steps per second: 138, episode reward: -87.703, mean reward: -1.110 [-100.000,  7.131], mean action: 1.392 [0.000, 3.000],  loss: 12.911341, mse: 8809.672400, mean_q: 65.784786, mean_eps: 0.639693
 109373/300000: episode: 1118, duration: 1.035s, episode steps: 149, steps per second: 144, episode reward: -26.423, mean reward: -0.177 [-100.000,  7.967], mean action: 1.651 [0.000, 3.000],  loss: 13.107106, mse: 8899.617489, mean_q: 66.063949, mean_eps: 0.639317
 109489/300000: episode: 1119, duration: 0.822s, episode steps: 116, steps per second: 141, episode reward: -77.607, mean reward: -0.669 [-100.000,  7.738], mean action: 1.560 [0.000, 3.000],  loss: 18.330333, mse: 8843.097042, mean_q: 67.176263, mean_eps: 0.638879
 109622/300000: episode: 1120, duration: 0.935s, episode steps: 133, steps per second: 142, episode reward: -80.812, mean reward: -0.608 [-100.000, 11.052], mean action: 1.624 [0.000, 3.000],  loss: 19.683689, mse: 9007.318870, mean_q: 68.227545, mean_eps: 0.638469
 109747/300000: episode: 1121, duration: 0.920s, episode steps: 125, steps per second: 136, episode reward: -71.265, mean reward: -0.570 [-100.000, 17.690], mean action: 1.512 [0.000, 3.000],  loss: 22.467129, mse: 8822.213562, mean_q: 66.057685, mean_eps: 0.638043
 109912/300000: episode: 1122, duration: 1.240s, episode steps: 165, steps per second: 133, episode reward: -66.967, mean reward: -0.406 [-100.000, 13.291], mean action: 1.848 [0.000, 3.000],  loss: 20.538200, mse: 9050.738471, mean_q: 67.597674, mean_eps: 0.637564
 110001/300000: episode: 1123, duration: 0.611s, episode steps:  89, steps per second: 146, episode reward: -21.339, mean reward: -0.240 [-100.000, 17.638], mean action: 1.674 [0.000, 3.000],  loss: 16.483289, mse: 8849.724050, mean_q: 66.072391, mean_eps: 0.637145
 110128/300000: episode: 1124, duration: 0.988s, episode steps: 127, steps per second: 129, episode reward: -81.923, mean reward: -0.645 [-100.000,  5.398], mean action: 1.669 [0.000, 3.000],  loss: 10.530688, mse: 9126.578033, mean_q: 66.841151, mean_eps: 0.636789
 110222/300000: episode: 1125, duration: 0.794s, episode steps:  94, steps per second: 118, episode reward: -169.542, mean reward: -1.804 [-100.000, 11.699], mean action: 1.713 [0.000, 3.000],  loss: 18.177017, mse: 8914.292148, mean_q: 64.638034, mean_eps: 0.636424
 110320/300000: episode: 1126, duration: 0.754s, episode steps:  98, steps per second: 130, episode reward: -92.348, mean reward: -0.942 [-100.000, 13.042], mean action: 1.663 [0.000, 3.000],  loss: 16.993772, mse: 8898.629140, mean_q: 66.186718, mean_eps: 0.636107
 110419/300000: episode: 1127, duration: 0.939s, episode steps:  99, steps per second: 105, episode reward: -72.592, mean reward: -0.733 [-100.000,  6.873], mean action: 1.657 [0.000, 3.000],  loss: 21.092451, mse: 9025.884120, mean_q: 67.641845, mean_eps: 0.635782
 110517/300000: episode: 1128, duration: 0.842s, episode steps:  98, steps per second: 116, episode reward: -37.891, mean reward: -0.387 [-100.000,  7.620], mean action: 1.704 [0.000, 3.000],  loss: 17.530614, mse: 9048.023109, mean_q: 66.344461, mean_eps: 0.635457
 110606/300000: episode: 1129, duration: 0.746s, episode steps:  89, steps per second: 119, episode reward: -60.551, mean reward: -0.680 [-100.000, 12.329], mean action: 1.449 [0.000, 3.000],  loss: 17.738389, mse: 9025.165462, mean_q: 66.024475, mean_eps: 0.635149
 110737/300000: episode: 1130, duration: 1.004s, episode steps: 131, steps per second: 130, episode reward: -77.933, mean reward: -0.595 [-100.000, 14.744], mean action: 1.580 [0.000, 3.000],  loss: 14.466227, mse: 9044.450781, mean_q: 67.438232, mean_eps: 0.634786
 110847/300000: episode: 1131, duration: 0.887s, episode steps: 110, steps per second: 124, episode reward: -91.515, mean reward: -0.832 [-100.000,  7.368], mean action: 1.727 [0.000, 3.000],  loss: 15.333061, mse: 9046.956871, mean_q: 66.516409, mean_eps: 0.634388
 110998/300000: episode: 1132, duration: 1.366s, episode steps: 151, steps per second: 111, episode reward: -30.038, mean reward: -0.199 [-100.000, 56.271], mean action: 1.907 [0.000, 3.000],  loss: 12.842436, mse: 9142.439589, mean_q: 67.043697, mean_eps: 0.633957
 111093/300000: episode: 1133, duration: 0.677s, episode steps:  95, steps per second: 140, episode reward: -64.797, mean reward: -0.682 [-100.000, 27.079], mean action: 1.621 [0.000, 3.000],  loss: 13.487089, mse: 8799.577241, mean_q: 66.414486, mean_eps: 0.633552
 111202/300000: episode: 1134, duration: 0.790s, episode steps: 109, steps per second: 138, episode reward: -111.810, mean reward: -1.026 [-100.000, 10.081], mean action: 1.596 [0.000, 3.000],  loss: 12.094255, mse: 8962.024441, mean_q: 66.267752, mean_eps: 0.633215
 111345/300000: episode: 1135, duration: 1.084s, episode steps: 143, steps per second: 132, episode reward: -138.077, mean reward: -0.966 [-100.000,  9.902], mean action: 1.580 [0.000, 3.000],  loss: 18.752116, mse: 8896.407968, mean_q: 66.041740, mean_eps: 0.632799
 111456/300000: episode: 1136, duration: 0.852s, episode steps: 111, steps per second: 130, episode reward: -112.724, mean reward: -1.016 [-100.000,  5.705], mean action: 1.703 [0.000, 3.000],  loss: 16.526769, mse: 8983.611588, mean_q: 66.887567, mean_eps: 0.632380
 111586/300000: episode: 1137, duration: 0.972s, episode steps: 130, steps per second: 134, episode reward: -98.597, mean reward: -0.758 [-100.000,  9.840], mean action: 1.738 [0.000, 3.000],  loss: 16.667402, mse: 9148.151979, mean_q: 67.923687, mean_eps: 0.631982
 111710/300000: episode: 1138, duration: 0.908s, episode steps: 124, steps per second: 137, episode reward: -199.093, mean reward: -1.606 [-100.000, 15.007], mean action: 1.669 [0.000, 3.000],  loss: 12.783883, mse: 9094.120239, mean_q: 67.627295, mean_eps: 0.631563
 111810/300000: episode: 1139, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: -111.017, mean reward: -1.110 [-100.000,  9.485], mean action: 1.660 [0.000, 3.000],  loss: 12.433236, mse: 8964.785728, mean_q: 65.479798, mean_eps: 0.631194
 111920/300000: episode: 1140, duration: 0.805s, episode steps: 110, steps per second: 137, episode reward: -40.093, mean reward: -0.364 [-100.000, 10.001], mean action: 1.518 [0.000, 3.000],  loss: 13.829109, mse: 9177.127193, mean_q: 68.794112, mean_eps: 0.630847
 112043/300000: episode: 1141, duration: 1.077s, episode steps: 123, steps per second: 114, episode reward: -208.686, mean reward: -1.697 [-100.000, 14.721], mean action: 1.911 [0.000, 3.000],  loss: 14.632381, mse: 8968.026411, mean_q: 66.736745, mean_eps: 0.630463
 112172/300000: episode: 1142, duration: 1.024s, episode steps: 129, steps per second: 126, episode reward: -36.999, mean reward: -0.287 [-100.000, 16.508], mean action: 1.581 [0.000, 3.000],  loss: 16.067826, mse: 9109.138679, mean_q: 68.137946, mean_eps: 0.630047
 112284/300000: episode: 1143, duration: 0.925s, episode steps: 112, steps per second: 121, episode reward: -104.131, mean reward: -0.930 [-100.000,  9.936], mean action: 1.634 [0.000, 3.000],  loss: 13.407480, mse: 9134.597142, mean_q: 67.568322, mean_eps: 0.629649
 112410/300000: episode: 1144, duration: 0.999s, episode steps: 126, steps per second: 126, episode reward: -42.235, mean reward: -0.335 [-100.000, 14.831], mean action: 1.619 [0.000, 3.000],  loss: 18.681776, mse: 8905.229085, mean_q: 65.335123, mean_eps: 0.629257
 112540/300000: episode: 1145, duration: 1.075s, episode steps: 130, steps per second: 121, episode reward: -94.224, mean reward: -0.725 [-100.000,  7.486], mean action: 1.692 [0.000, 3.000],  loss: 12.536243, mse: 9343.715554, mean_q: 68.584763, mean_eps: 0.628834
 112660/300000: episode: 1146, duration: 1.013s, episode steps: 120, steps per second: 118, episode reward: -28.905, mean reward: -0.241 [-100.000, 15.456], mean action: 1.817 [0.000, 3.000],  loss: 14.921880, mse: 9329.440910, mean_q: 69.562664, mean_eps: 0.628422
 112768/300000: episode: 1147, duration: 0.925s, episode steps: 108, steps per second: 117, episode reward: -50.055, mean reward: -0.463 [-100.000, 13.816], mean action: 1.676 [0.000, 3.000],  loss: 18.417000, mse: 9225.493282, mean_q: 68.178319, mean_eps: 0.628045
 112904/300000: episode: 1148, duration: 1.175s, episode steps: 136, steps per second: 116, episode reward: -77.220, mean reward: -0.568 [-100.000, 11.063], mean action: 1.772 [0.000, 3.000],  loss: 11.746811, mse: 9302.608485, mean_q: 68.130668, mean_eps: 0.627643
 112992/300000: episode: 1149, duration: 0.684s, episode steps:  88, steps per second: 129, episode reward: -70.726, mean reward: -0.804 [-100.000,  6.374], mean action: 1.682 [0.000, 3.000],  loss: 17.858029, mse: 9130.897822, mean_q: 67.547982, mean_eps: 0.627273
 113101/300000: episode: 1150, duration: 0.813s, episode steps: 109, steps per second: 134, episode reward: -35.243, mean reward: -0.323 [-100.000, 18.710], mean action: 1.844 [0.000, 3.000],  loss: 14.012051, mse: 9079.851854, mean_q: 66.479005, mean_eps: 0.626948
 113217/300000: episode: 1151, duration: 0.834s, episode steps: 116, steps per second: 139, episode reward: -5.399, mean reward: -0.047 [-100.000, 16.160], mean action: 1.707 [0.000, 3.000],  loss: 13.894589, mse: 9059.174409, mean_q: 67.678274, mean_eps: 0.626577
 113356/300000: episode: 1152, duration: 1.010s, episode steps: 139, steps per second: 138, episode reward: -59.697, mean reward: -0.429 [-100.000, 14.783], mean action: 1.741 [0.000, 3.000],  loss: 15.030940, mse: 9188.536604, mean_q: 67.480047, mean_eps: 0.626156
 113445/300000: episode: 1153, duration: 0.659s, episode steps:  89, steps per second: 135, episode reward: -39.460, mean reward: -0.443 [-100.000, 12.160], mean action: 1.730 [0.000, 3.000],  loss: 12.871004, mse: 9322.131178, mean_q: 68.109949, mean_eps: 0.625780
 113527/300000: episode: 1154, duration: 0.647s, episode steps:  82, steps per second: 127, episode reward: -70.369, mean reward: -0.858 [-100.000, 10.517], mean action: 1.683 [0.000, 3.000],  loss: 15.210712, mse: 9347.541409, mean_q: 68.976061, mean_eps: 0.625498
 113625/300000: episode: 1155, duration: 0.790s, episode steps:  98, steps per second: 124, episode reward: -150.504, mean reward: -1.536 [-100.000, 17.049], mean action: 1.765 [0.000, 3.000],  loss: 12.247780, mse: 9252.714360, mean_q: 67.662728, mean_eps: 0.625201
 113737/300000: episode: 1156, duration: 0.892s, episode steps: 112, steps per second: 126, episode reward: -145.154, mean reward: -1.296 [-100.000, 12.227], mean action: 1.661 [0.000, 3.000],  loss: 16.599140, mse: 9619.910304, mean_q: 71.657937, mean_eps: 0.624854
 113819/300000: episode: 1157, duration: 0.612s, episode steps:  82, steps per second: 134, episode reward: -52.323, mean reward: -0.638 [-100.000, 12.012], mean action: 1.720 [0.000, 3.000],  loss: 15.522442, mse: 9298.356797, mean_q: 69.234808, mean_eps: 0.624534
 113972/300000: episode: 1158, duration: 1.128s, episode steps: 153, steps per second: 136, episode reward: -51.362, mean reward: -0.336 [-100.000, 15.537], mean action: 1.516 [0.000, 3.000],  loss: 16.261980, mse: 9365.524373, mean_q: 69.143803, mean_eps: 0.624147
 114074/300000: episode: 1159, duration: 0.740s, episode steps: 102, steps per second: 138, episode reward: -193.249, mean reward: -1.895 [-100.000, 17.350], mean action: 1.824 [0.000, 3.000],  loss: 21.289336, mse: 9424.826660, mean_q: 68.715225, mean_eps: 0.623726
 114157/300000: episode: 1160, duration: 0.590s, episode steps:  83, steps per second: 141, episode reward: -143.216, mean reward: -1.725 [-100.000,  8.109], mean action: 1.663 [0.000, 3.000],  loss: 14.677305, mse: 9100.484546, mean_q: 67.052880, mean_eps: 0.623420
 114282/300000: episode: 1161, duration: 0.899s, episode steps: 125, steps per second: 139, episode reward: -169.850, mean reward: -1.359 [-100.000,  6.614], mean action: 1.544 [0.000, 3.000],  loss: 23.488696, mse: 9349.729113, mean_q: 69.150413, mean_eps: 0.623077
 114397/300000: episode: 1162, duration: 0.896s, episode steps: 115, steps per second: 128, episode reward: -44.257, mean reward: -0.385 [-100.000, 19.103], mean action: 1.783 [0.000, 3.000],  loss: 15.554422, mse: 9447.689763, mean_q: 69.020746, mean_eps: 0.622681
 114532/300000: episode: 1163, duration: 0.990s, episode steps: 135, steps per second: 136, episode reward: -72.331, mean reward: -0.536 [-100.000, 21.261], mean action: 1.504 [0.000, 3.000],  loss: 18.459450, mse: 9371.364012, mean_q: 69.065179, mean_eps: 0.622269
 114659/300000: episode: 1164, duration: 0.930s, episode steps: 127, steps per second: 137, episode reward: -65.006, mean reward: -0.512 [-100.000, 18.131], mean action: 1.638 [0.000, 3.000],  loss: 20.667764, mse: 9393.430310, mean_q: 68.940885, mean_eps: 0.621837
 114742/300000: episode: 1165, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -25.125, mean reward: -0.303 [-100.000, 31.305], mean action: 1.759 [0.000, 3.000],  loss: 21.958585, mse: 8937.663327, mean_q: 65.054105, mean_eps: 0.621490
 114863/300000: episode: 1166, duration: 0.928s, episode steps: 121, steps per second: 130, episode reward: -128.680, mean reward: -1.063 [-100.000,  7.380], mean action: 1.686 [0.000, 3.000],  loss: 16.567126, mse: 9264.460869, mean_q: 68.718250, mean_eps: 0.621153
 114937/300000: episode: 1167, duration: 0.540s, episode steps:  74, steps per second: 137, episode reward: -39.249, mean reward: -0.530 [-100.000,  6.405], mean action: 1.568 [0.000, 3.000],  loss: 15.263626, mse: 9727.928599, mean_q: 71.357952, mean_eps: 0.620832
 115046/300000: episode: 1168, duration: 0.836s, episode steps: 109, steps per second: 130, episode reward: -73.487, mean reward: -0.674 [-100.000,  8.406], mean action: 1.578 [0.000, 3.000],  loss: 16.057718, mse: 9554.071580, mean_q: 69.829507, mean_eps: 0.620530
 115140/300000: episode: 1169, duration: 0.758s, episode steps:  94, steps per second: 124, episode reward: -45.172, mean reward: -0.481 [-100.000, 26.024], mean action: 1.777 [0.000, 3.000],  loss: 16.635040, mse: 9449.169512, mean_q: 68.250966, mean_eps: 0.620195
 115301/300000: episode: 1170, duration: 1.193s, episode steps: 161, steps per second: 135, episode reward: -136.197, mean reward: -0.846 [-100.000, 23.506], mean action: 1.801 [0.000, 3.000],  loss: 20.376659, mse: 9538.224982, mean_q: 69.553238, mean_eps: 0.619774
 115393/300000: episode: 1171, duration: 0.695s, episode steps:  92, steps per second: 132, episode reward: -105.642, mean reward: -1.148 [-100.000,  6.042], mean action: 1.761 [0.000, 3.000],  loss: 15.625523, mse: 9671.094387, mean_q: 71.777640, mean_eps: 0.619357
 115495/300000: episode: 1172, duration: 0.752s, episode steps: 102, steps per second: 136, episode reward: -49.778, mean reward: -0.488 [-100.000, 20.362], mean action: 1.608 [0.000, 3.000],  loss: 19.391617, mse: 9718.942000, mean_q: 71.260669, mean_eps: 0.619036
 115592/300000: episode: 1173, duration: 0.694s, episode steps:  97, steps per second: 140, episode reward: -77.223, mean reward: -0.796 [-100.000, 18.222], mean action: 1.753 [0.000, 3.000],  loss: 17.662349, mse: 9445.438285, mean_q: 69.244158, mean_eps: 0.618708
 115723/300000: episode: 1174, duration: 1.049s, episode steps: 131, steps per second: 125, episode reward: -51.590, mean reward: -0.394 [-100.000, 11.825], mean action: 1.626 [0.000, 3.000],  loss: 15.412920, mse: 9609.569168, mean_q: 70.779962, mean_eps: 0.618332
 115821/300000: episode: 1175, duration: 0.744s, episode steps:  98, steps per second: 132, episode reward: -73.979, mean reward: -0.755 [-100.000,  8.817], mean action: 1.684 [0.000, 3.000],  loss: 18.034643, mse: 9510.364871, mean_q: 70.684526, mean_eps: 0.617954
 115913/300000: episode: 1176, duration: 0.713s, episode steps:  92, steps per second: 129, episode reward: -83.763, mean reward: -0.910 [-100.000, 11.229], mean action: 1.674 [0.000, 3.000],  loss: 15.548940, mse: 9612.526261, mean_q: 70.310848, mean_eps: 0.617641
 116084/300000: episode: 1177, duration: 1.315s, episode steps: 171, steps per second: 130, episode reward: -91.544, mean reward: -0.535 [-100.000,  9.297], mean action: 1.596 [0.000, 3.000],  loss: 14.823632, mse: 9699.372561, mean_q: 70.894407, mean_eps: 0.617207
 116188/300000: episode: 1178, duration: 0.743s, episode steps: 104, steps per second: 140, episode reward: -98.435, mean reward: -0.946 [-100.000,  6.329], mean action: 1.721 [0.000, 3.000],  loss: 12.379309, mse: 9554.368934, mean_q: 70.311815, mean_eps: 0.616753
 116303/300000: episode: 1179, duration: 0.865s, episode steps: 115, steps per second: 133, episode reward: -61.663, mean reward: -0.536 [-100.000,  6.885], mean action: 1.765 [0.000, 3.000],  loss: 18.418385, mse: 9661.972346, mean_q: 70.844697, mean_eps: 0.616391
 117190/300000: episode: 1180, duration: 7.510s, episode steps: 887, steps per second: 118, episode reward: -89.344, mean reward: -0.101 [-100.000, 25.497], mean action: 1.652 [0.000, 3.000],  loss: 17.602523, mse: 9712.183753, mean_q: 71.102363, mean_eps: 0.614738
 117308/300000: episode: 1181, duration: 0.980s, episode steps: 118, steps per second: 120, episode reward: -69.074, mean reward: -0.585 [-100.000, 13.266], mean action: 1.475 [0.000, 3.000],  loss: 15.097913, mse: 9988.878075, mean_q: 70.667580, mean_eps: 0.613080
 117505/300000: episode: 1182, duration: 1.578s, episode steps: 197, steps per second: 125, episode reward: -41.976, mean reward: -0.213 [-100.000,  9.284], mean action: 1.640 [0.000, 3.000],  loss: 14.443794, mse: 9784.396398, mean_q: 71.033608, mean_eps: 0.612560
 117614/300000: episode: 1183, duration: 0.895s, episode steps: 109, steps per second: 122, episode reward: -10.635, mean reward: -0.098 [-100.000, 16.240], mean action: 1.761 [0.000, 3.000],  loss: 17.472830, mse: 9753.056923, mean_q: 70.289688, mean_eps: 0.612055
 117710/300000: episode: 1184, duration: 0.689s, episode steps:  96, steps per second: 139, episode reward: -46.776, mean reward: -0.487 [-100.000, 14.777], mean action: 1.781 [0.000, 3.000],  loss: 16.132748, mse: 9967.558690, mean_q: 73.273567, mean_eps: 0.611717
 117804/300000: episode: 1185, duration: 0.659s, episode steps:  94, steps per second: 143, episode reward: -16.506, mean reward: -0.176 [-100.000, 18.156], mean action: 1.574 [0.000, 3.000],  loss: 16.174814, mse: 9748.057383, mean_q: 71.602289, mean_eps: 0.611404
 117947/300000: episode: 1186, duration: 1.089s, episode steps: 143, steps per second: 131, episode reward: -123.948, mean reward: -0.867 [-100.000,  3.173], mean action: 1.601 [0.000, 3.000],  loss: 13.205929, mse: 9923.882741, mean_q: 72.194717, mean_eps: 0.611012
 118026/300000: episode: 1187, duration: 0.572s, episode steps:  79, steps per second: 138, episode reward:  4.251, mean reward:  0.054 [-100.000, 21.514], mean action: 1.797 [0.000, 3.000],  loss: 19.118199, mse: 9628.992948, mean_q: 69.873587, mean_eps: 0.610646
 118114/300000: episode: 1188, duration: 0.663s, episode steps:  88, steps per second: 133, episode reward: -76.553, mean reward: -0.870 [-100.000,  8.934], mean action: 1.705 [0.000, 3.000],  loss: 16.288942, mse: 9994.544861, mean_q: 72.004575, mean_eps: 0.610371
 118253/300000: episode: 1189, duration: 1.027s, episode steps: 139, steps per second: 135, episode reward: -80.533, mean reward: -0.579 [-100.000, 10.594], mean action: 1.691 [0.000, 3.000],  loss: 19.898897, mse: 9865.012780, mean_q: 70.821593, mean_eps: 0.609996
 118392/300000: episode: 1190, duration: 0.963s, episode steps: 139, steps per second: 144, episode reward: -74.724, mean reward: -0.538 [-100.000, 13.025], mean action: 1.705 [0.000, 3.000],  loss: 16.880229, mse: 9911.788817, mean_q: 71.643227, mean_eps: 0.609537
 118470/300000: episode: 1191, duration: 0.585s, episode steps:  78, steps per second: 133, episode reward: -59.752, mean reward: -0.766 [-100.000, 11.453], mean action: 1.795 [0.000, 3.000],  loss: 16.862950, mse: 9676.767929, mean_q: 71.124520, mean_eps: 0.609179
 118583/300000: episode: 1192, duration: 0.797s, episode steps: 113, steps per second: 142, episode reward: -98.052, mean reward: -0.868 [-100.000,  5.816], mean action: 1.354 [0.000, 3.000],  loss: 17.872400, mse: 9867.653584, mean_q: 71.264774, mean_eps: 0.608864
 118676/300000: episode: 1193, duration: 0.668s, episode steps:  93, steps per second: 139, episode reward: -58.307, mean reward: -0.627 [-100.000,  6.720], mean action: 1.505 [0.000, 3.000],  loss: 21.581646, mse: 9870.315036, mean_q: 72.384223, mean_eps: 0.608524
 118768/300000: episode: 1194, duration: 0.747s, episode steps:  92, steps per second: 123, episode reward: -41.107, mean reward: -0.447 [-100.000, 10.318], mean action: 1.793 [0.000, 3.000],  loss: 16.586277, mse: 9652.114720, mean_q: 70.542509, mean_eps: 0.608219
 118900/300000: episode: 1195, duration: 1.177s, episode steps: 132, steps per second: 112, episode reward: -67.549, mean reward: -0.512 [-100.000, 10.332], mean action: 1.644 [0.000, 3.000],  loss: 21.653050, mse: 9462.717699, mean_q: 69.573454, mean_eps: 0.607849
 118980/300000: episode: 1196, duration: 0.654s, episode steps:  80, steps per second: 122, episode reward: -45.467, mean reward: -0.568 [-100.000, 13.300], mean action: 1.637 [0.000, 3.000],  loss: 18.191079, mse: 9916.758173, mean_q: 71.052467, mean_eps: 0.607500
 119111/300000: episode: 1197, duration: 1.061s, episode steps: 131, steps per second: 123, episode reward: -60.292, mean reward: -0.460 [-100.000, 13.972], mean action: 1.511 [0.000, 3.000],  loss: 17.388672, mse: 9663.389313, mean_q: 70.545470, mean_eps: 0.607151
 119234/300000: episode: 1198, duration: 0.973s, episode steps: 123, steps per second: 126, episode reward: -46.884, mean reward: -0.381 [-100.000, 14.778], mean action: 1.797 [0.000, 3.000],  loss: 21.160349, mse: 9701.539642, mean_q: 70.422079, mean_eps: 0.606732
 119381/300000: episode: 1199, duration: 1.171s, episode steps: 147, steps per second: 126, episode reward: -142.004, mean reward: -0.966 [-100.000,  3.145], mean action: 1.694 [0.000, 3.000],  loss: 14.773657, mse: 9756.272501, mean_q: 70.712464, mean_eps: 0.606287
 120002/300000: episode: 1200, duration: 4.900s, episode steps: 621, steps per second: 127, episode reward: -311.970, mean reward: -0.502 [-100.000, 21.271], mean action: 1.681 [0.000, 3.000],  loss: 16.382285, mse: 9818.372116, mean_q: 71.796645, mean_eps: 0.605020
 120136/300000: episode: 1201, duration: 1.045s, episode steps: 134, steps per second: 128, episode reward: -100.655, mean reward: -0.751 [-100.000, 17.558], mean action: 1.590 [0.000, 3.000],  loss: 15.538510, mse: 10062.085289, mean_q: 72.538974, mean_eps: 0.603774
 120271/300000: episode: 1202, duration: 0.951s, episode steps: 135, steps per second: 142, episode reward: -273.249, mean reward: -2.024 [-100.000, 55.627], mean action: 1.756 [0.000, 3.000],  loss: 14.482925, mse: 10293.087692, mean_q: 74.635244, mean_eps: 0.603330
 120372/300000: episode: 1203, duration: 0.748s, episode steps: 101, steps per second: 135, episode reward: -60.871, mean reward: -0.603 [-100.000, 11.276], mean action: 1.792 [0.000, 3.000],  loss: 12.823860, mse: 10175.919163, mean_q: 74.702288, mean_eps: 0.602941
 120480/300000: episode: 1204, duration: 0.831s, episode steps: 108, steps per second: 130, episode reward: -90.837, mean reward: -0.841 [-100.000,  6.524], mean action: 1.778 [0.000, 3.000],  loss: 19.284104, mse: 10216.509047, mean_q: 74.673891, mean_eps: 0.602596
 120576/300000: episode: 1205, duration: 0.703s, episode steps:  96, steps per second: 136, episode reward: -72.934, mean reward: -0.760 [-100.000,  8.622], mean action: 1.500 [0.000, 3.000],  loss: 13.856525, mse: 9921.809174, mean_q: 73.242407, mean_eps: 0.602259
 120736/300000: episode: 1206, duration: 1.203s, episode steps: 160, steps per second: 133, episode reward: -269.458, mean reward: -1.684 [-100.000, 69.177], mean action: 1.488 [0.000, 3.000],  loss: 17.937964, mse: 9956.386417, mean_q: 72.404697, mean_eps: 0.601837
 120842/300000: episode: 1207, duration: 0.772s, episode steps: 106, steps per second: 137, episode reward: -68.185, mean reward: -0.643 [-100.000,  7.802], mean action: 1.840 [0.000, 3.000],  loss: 17.985304, mse: 9920.344634, mean_q: 71.391722, mean_eps: 0.601398
 120955/300000: episode: 1208, duration: 0.852s, episode steps: 113, steps per second: 133, episode reward: -43.859, mean reward: -0.388 [-100.000, 10.547], mean action: 1.717 [0.000, 3.000],  loss: 14.841734, mse: 10253.521541, mean_q: 75.086138, mean_eps: 0.601037
 121055/300000: episode: 1209, duration: 0.751s, episode steps: 100, steps per second: 133, episode reward: -0.272, mean reward: -0.003 [-100.000, 20.661], mean action: 1.630 [0.000, 3.000],  loss: 19.917436, mse: 9788.141694, mean_q: 71.281427, mean_eps: 0.600685
 121159/300000: episode: 1210, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -84.959, mean reward: -0.817 [-100.000,  9.429], mean action: 1.702 [0.000, 3.000],  loss: 15.287650, mse: 10414.033630, mean_q: 75.151546, mean_eps: 0.600349
 121242/300000: episode: 1211, duration: 0.605s, episode steps:  83, steps per second: 137, episode reward: -100.768, mean reward: -1.214 [-100.000,  8.888], mean action: 1.735 [0.000, 3.000],  loss: 17.254185, mse: 10195.517455, mean_q: 73.470488, mean_eps: 0.600040
 121344/300000: episode: 1212, duration: 0.748s, episode steps: 102, steps per second: 136, episode reward: -127.802, mean reward: -1.253 [-100.000, 20.097], mean action: 1.745 [0.000, 3.000],  loss: 19.133005, mse: 10482.752642, mean_q: 74.960539, mean_eps: 0.599735
 121422/300000: episode: 1213, duration: 0.554s, episode steps:  78, steps per second: 141, episode reward: -87.625, mean reward: -1.123 [-100.000,  5.728], mean action: 1.679 [0.000, 3.000],  loss: 17.752023, mse: 10280.696696, mean_q: 74.571391, mean_eps: 0.599438
 121516/300000: episode: 1214, duration: 0.673s, episode steps:  94, steps per second: 140, episode reward: -38.696, mean reward: -0.412 [-100.000, 12.900], mean action: 1.681 [0.000, 3.000],  loss: 19.793685, mse: 10244.068754, mean_q: 73.982535, mean_eps: 0.599154
 121600/300000: episode: 1215, duration: 0.678s, episode steps:  84, steps per second: 124, episode reward: -27.026, mean reward: -0.322 [-100.000,  9.912], mean action: 1.786 [0.000, 3.000],  loss: 15.822665, mse: 10342.577736, mean_q: 74.350108, mean_eps: 0.598860
 121693/300000: episode: 1216, duration: 0.715s, episode steps:  93, steps per second: 130, episode reward: -111.042, mean reward: -1.194 [-100.000,  9.937], mean action: 1.613 [0.000, 3.000],  loss: 17.526080, mse: 10201.385585, mean_q: 74.249051, mean_eps: 0.598568
 121812/300000: episode: 1217, duration: 0.886s, episode steps: 119, steps per second: 134, episode reward: -71.581, mean reward: -0.602 [-100.000, 14.868], mean action: 1.765 [0.000, 3.000],  loss: 14.947641, mse: 10260.787315, mean_q: 74.487918, mean_eps: 0.598218
 121913/300000: episode: 1218, duration: 0.876s, episode steps: 101, steps per second: 115, episode reward: -139.147, mean reward: -1.378 [-100.000, 37.512], mean action: 1.634 [0.000, 3.000],  loss: 18.751640, mse: 10657.252910, mean_q: 75.528665, mean_eps: 0.597855
 121998/300000: episode: 1219, duration: 0.605s, episode steps:  85, steps per second: 141, episode reward: -47.333, mean reward: -0.557 [-100.000, 11.457], mean action: 1.718 [0.000, 3.000],  loss: 15.186521, mse: 10226.774121, mean_q: 73.183004, mean_eps: 0.597548
 122110/300000: episode: 1220, duration: 0.814s, episode steps: 112, steps per second: 138, episode reward: -15.781, mean reward: -0.141 [-100.000, 13.542], mean action: 1.670 [0.000, 3.000],  loss: 17.648544, mse: 10253.518515, mean_q: 73.458636, mean_eps: 0.597223
 122237/300000: episode: 1221, duration: 0.930s, episode steps: 127, steps per second: 137, episode reward: -72.513, mean reward: -0.571 [-100.000, 15.900], mean action: 1.764 [0.000, 3.000],  loss: 15.119729, mse: 10222.291896, mean_q: 74.201333, mean_eps: 0.596829
 122350/300000: episode: 1222, duration: 0.800s, episode steps: 113, steps per second: 141, episode reward: -16.030, mean reward: -0.142 [-100.000,  7.854], mean action: 1.779 [0.000, 3.000],  loss: 15.787895, mse: 10372.817958, mean_q: 74.873445, mean_eps: 0.596433
 122454/300000: episode: 1223, duration: 0.796s, episode steps: 104, steps per second: 131, episode reward: -47.936, mean reward: -0.461 [-100.000, 10.393], mean action: 1.740 [0.000, 3.000],  loss: 14.451516, mse: 10239.897874, mean_q: 75.350473, mean_eps: 0.596075
 122555/300000: episode: 1224, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -100.449, mean reward: -0.995 [-100.000,  6.998], mean action: 1.861 [0.000, 3.000],  loss: 14.371156, mse: 10285.595041, mean_q: 75.203994, mean_eps: 0.595737
 122659/300000: episode: 1225, duration: 0.736s, episode steps: 104, steps per second: 141, episode reward: -28.774, mean reward: -0.277 [-100.000, 19.824], mean action: 1.654 [0.000, 3.000],  loss: 20.863912, mse: 10612.094553, mean_q: 75.802458, mean_eps: 0.595399
 122860/300000: episode: 1226, duration: 1.554s, episode steps: 201, steps per second: 129, episode reward: -123.309, mean reward: -0.613 [-100.000,  9.542], mean action: 1.637 [0.000, 3.000],  loss: 16.107309, mse: 10190.583902, mean_q: 74.020702, mean_eps: 0.594895
 122962/300000: episode: 1227, duration: 0.718s, episode steps: 102, steps per second: 142, episode reward: -37.145, mean reward: -0.364 [-100.000, 12.924], mean action: 1.804 [0.000, 3.000],  loss: 21.472202, mse: 10682.431291, mean_q: 76.880012, mean_eps: 0.594395
 123072/300000: episode: 1228, duration: 0.851s, episode steps: 110, steps per second: 129, episode reward: -111.748, mean reward: -1.016 [-100.000,  6.557], mean action: 1.636 [0.000, 3.000],  loss: 18.454956, mse: 10471.574703, mean_q: 75.947109, mean_eps: 0.594046
 123220/300000: episode: 1229, duration: 1.041s, episode steps: 148, steps per second: 142, episode reward: -3.764, mean reward: -0.025 [-100.000, 22.162], mean action: 1.480 [0.000, 3.000],  loss: 19.827631, mse: 10455.157415, mean_q: 75.260833, mean_eps: 0.593620
 123563/300000: episode: 1230, duration: 2.537s, episode steps: 343, steps per second: 135, episode reward: -120.350, mean reward: -0.351 [-100.000, 15.111], mean action: 1.668 [0.000, 3.000],  loss: 18.617909, mse: 10753.277416, mean_q: 77.240335, mean_eps: 0.592810
 123660/300000: episode: 1231, duration: 0.744s, episode steps:  97, steps per second: 130, episode reward: -97.510, mean reward: -1.005 [-100.000,  9.316], mean action: 1.722 [0.000, 3.000],  loss: 11.769823, mse: 10610.937284, mean_q: 76.054977, mean_eps: 0.592084
 123761/300000: episode: 1232, duration: 0.752s, episode steps: 101, steps per second: 134, episode reward: -77.204, mean reward: -0.764 [-100.000,  8.016], mean action: 1.545 [0.000, 3.000],  loss: 20.403899, mse: 10604.133175, mean_q: 76.254933, mean_eps: 0.591757
 123878/300000: episode: 1233, duration: 1.102s, episode steps: 117, steps per second: 106, episode reward: -23.267, mean reward: -0.199 [-100.000, 14.451], mean action: 1.692 [0.000, 3.000],  loss: 18.226432, mse: 10704.898116, mean_q: 76.577274, mean_eps: 0.591397
 123987/300000: episode: 1234, duration: 0.843s, episode steps: 109, steps per second: 129, episode reward: -27.404, mean reward: -0.251 [-100.000, 11.892], mean action: 1.817 [0.000, 3.000],  loss: 15.653793, mse: 10589.385684, mean_q: 76.359578, mean_eps: 0.591024
 124072/300000: episode: 1235, duration: 0.609s, episode steps:  85, steps per second: 140, episode reward: -31.850, mean reward: -0.375 [-100.000, 11.219], mean action: 1.624 [0.000, 3.000],  loss: 19.507084, mse: 10686.470875, mean_q: 77.385537, mean_eps: 0.590704
 124222/300000: episode: 1236, duration: 1.124s, episode steps: 150, steps per second: 133, episode reward: 11.760, mean reward:  0.078 [-100.000, 10.496], mean action: 1.680 [0.000, 3.000],  loss: 16.209449, mse: 10567.149355, mean_q: 76.910232, mean_eps: 0.590317
 124339/300000: episode: 1237, duration: 0.856s, episode steps: 117, steps per second: 137, episode reward: -82.424, mean reward: -0.704 [-100.000, 16.902], mean action: 1.615 [0.000, 3.000],  loss: 15.092991, mse: 10744.133351, mean_q: 76.734650, mean_eps: 0.589876
 124430/300000: episode: 1238, duration: 0.687s, episode steps:  91, steps per second: 132, episode reward: -29.826, mean reward: -0.328 [-100.000,  8.404], mean action: 1.912 [0.000, 3.000],  loss: 16.110694, mse: 10441.011633, mean_q: 75.726467, mean_eps: 0.589533
 124554/300000: episode: 1239, duration: 0.949s, episode steps: 124, steps per second: 131, episode reward: -48.177, mean reward: -0.389 [-100.000,  9.980], mean action: 1.718 [0.000, 3.000],  loss: 17.642947, mse: 10678.007521, mean_q: 76.356809, mean_eps: 0.589178
 124641/300000: episode: 1240, duration: 0.623s, episode steps:  87, steps per second: 140, episode reward: -62.036, mean reward: -0.713 [-100.000,  7.274], mean action: 1.839 [0.000, 3.000],  loss: 19.954648, mse: 10968.098066, mean_q: 78.946901, mean_eps: 0.588830
 124735/300000: episode: 1241, duration: 0.711s, episode steps:  94, steps per second: 132, episode reward: -62.594, mean reward: -0.666 [-100.000,  8.933], mean action: 1.574 [0.000, 3.000],  loss: 15.477006, mse: 10737.794704, mean_q: 77.785027, mean_eps: 0.588531
 124825/300000: episode: 1242, duration: 0.653s, episode steps:  90, steps per second: 138, episode reward: -140.054, mean reward: -1.556 [-100.000,  7.850], mean action: 1.667 [0.000, 3.000],  loss: 16.915600, mse: 10665.059972, mean_q: 75.954695, mean_eps: 0.588228
 124971/300000: episode: 1243, duration: 1.058s, episode steps: 146, steps per second: 138, episode reward: -79.617, mean reward: -0.545 [-100.000, 12.035], mean action: 1.623 [0.000, 3.000],  loss: 15.222702, mse: 10940.577603, mean_q: 77.501849, mean_eps: 0.587838
 125049/300000: episode: 1244, duration: 0.587s, episode steps:  78, steps per second: 133, episode reward: -87.747, mean reward: -1.125 [-100.000, 10.173], mean action: 1.692 [0.000, 3.000],  loss: 17.592431, mse: 10103.743239, mean_q: 73.684466, mean_eps: 0.587469
 125162/300000: episode: 1245, duration: 0.815s, episode steps: 113, steps per second: 139, episode reward:  2.483, mean reward:  0.022 [-100.000, 21.873], mean action: 1.717 [0.000, 3.000],  loss: 20.512651, mse: 10519.231467, mean_q: 77.488892, mean_eps: 0.587153
 125262/300000: episode: 1246, duration: 0.704s, episode steps: 100, steps per second: 142, episode reward: -86.863, mean reward: -0.869 [-100.000, 12.341], mean action: 1.590 [0.000, 3.000],  loss: 17.034104, mse: 10558.121694, mean_q: 76.166738, mean_eps: 0.586802
 125396/300000: episode: 1247, duration: 1.002s, episode steps: 134, steps per second: 134, episode reward: -65.593, mean reward: -0.490 [-100.000, 16.758], mean action: 1.552 [0.000, 3.000],  loss: 17.251079, mse: 10599.181378, mean_q: 76.575558, mean_eps: 0.586416
 125476/300000: episode: 1248, duration: 0.566s, episode steps:  80, steps per second: 141, episode reward: -20.241, mean reward: -0.253 [-100.000, 13.357], mean action: 1.725 [0.000, 3.000],  loss: 15.269533, mse: 10300.591791, mean_q: 75.627845, mean_eps: 0.586063
 125572/300000: episode: 1249, duration: 0.784s, episode steps:  96, steps per second: 122, episode reward: -81.775, mean reward: -0.852 [-100.000,  9.494], mean action: 1.635 [0.000, 3.000],  loss: 22.247383, mse: 10599.483139, mean_q: 76.560682, mean_eps: 0.585772
 125748/300000: episode: 1250, duration: 1.510s, episode steps: 176, steps per second: 117, episode reward: -57.255, mean reward: -0.325 [-100.000,  4.320], mean action: 1.636 [0.000, 3.000],  loss: 18.783426, mse: 10570.768008, mean_q: 76.767648, mean_eps: 0.585324
 125876/300000: episode: 1251, duration: 1.040s, episode steps: 128, steps per second: 123, episode reward:  6.641, mean reward:  0.052 [-100.000, 19.397], mean action: 1.719 [0.000, 3.000],  loss: 16.995270, mse: 10515.127190, mean_q: 76.990697, mean_eps: 0.584822
 126005/300000: episode: 1252, duration: 0.975s, episode steps: 129, steps per second: 132, episode reward: -80.401, mean reward: -0.623 [-100.000, 11.785], mean action: 1.760 [0.000, 3.000],  loss: 20.547656, mse: 10789.860333, mean_q: 78.773887, mean_eps: 0.584398
 126099/300000: episode: 1253, duration: 0.666s, episode steps:  94, steps per second: 141, episode reward: -33.085, mean reward: -0.352 [-100.000, 18.418], mean action: 1.723 [0.000, 3.000],  loss: 15.387500, mse: 10850.675090, mean_q: 78.436228, mean_eps: 0.584030
 127099/300000: episode: 1254, duration: 8.626s, episode steps: 1000, steps per second: 116, episode reward: 47.279, mean reward:  0.047 [-24.156, 22.521], mean action: 1.717 [0.000, 3.000],  loss: 17.021301, mse: 10643.409384, mean_q: 77.823948, mean_eps: 0.582225
 127205/300000: episode: 1255, duration: 0.970s, episode steps: 106, steps per second: 109, episode reward: -42.380, mean reward: -0.400 [-100.000,  8.937], mean action: 1.726 [0.000, 3.000],  loss: 20.606672, mse: 10978.675901, mean_q: 80.260303, mean_eps: 0.580400
 127362/300000: episode: 1256, duration: 1.553s, episode steps: 157, steps per second: 101, episode reward: -25.344, mean reward: -0.161 [-100.000, 17.218], mean action: 1.771 [0.000, 3.000],  loss: 17.781719, mse: 10754.143608, mean_q: 78.465026, mean_eps: 0.579966
 127487/300000: episode: 1257, duration: 1.191s, episode steps: 125, steps per second: 105, episode reward: -24.567, mean reward: -0.197 [-100.000,  8.243], mean action: 1.624 [0.000, 3.000],  loss: 19.125871, mse: 10502.325992, mean_q: 77.297828, mean_eps: 0.579501
 127564/300000: episode: 1258, duration: 0.670s, episode steps:  77, steps per second: 115, episode reward: -0.875, mean reward: -0.011 [-100.000, 10.851], mean action: 1.455 [0.000, 3.000],  loss: 32.188421, mse: 10431.135717, mean_q: 77.225874, mean_eps: 0.579167
 127702/300000: episode: 1259, duration: 1.415s, episode steps: 138, steps per second:  98, episode reward: -14.747, mean reward: -0.107 [-100.000, 18.119], mean action: 1.761 [0.000, 3.000],  loss: 17.607745, mse: 10568.657977, mean_q: 77.439518, mean_eps: 0.578813
 127833/300000: episode: 1260, duration: 1.023s, episode steps: 131, steps per second: 128, episode reward: -18.490, mean reward: -0.141 [-100.000, 13.966], mean action: 1.832 [0.000, 3.000],  loss: 16.012716, mse: 10393.086869, mean_q: 77.150997, mean_eps: 0.578369
 127966/300000: episode: 1261, duration: 1.017s, episode steps: 133, steps per second: 131, episode reward: -105.059, mean reward: -0.790 [-100.000,  3.469], mean action: 1.722 [0.000, 3.000],  loss: 14.871305, mse: 10498.761928, mean_q: 77.456690, mean_eps: 0.577933
 128082/300000: episode: 1262, duration: 0.836s, episode steps: 116, steps per second: 139, episode reward: -28.780, mean reward: -0.248 [-100.000, 17.540], mean action: 1.690 [0.000, 3.000],  loss: 19.698893, mse: 10429.132859, mean_q: 76.823438, mean_eps: 0.577522
 128230/300000: episode: 1263, duration: 1.144s, episode steps: 148, steps per second: 129, episode reward: -5.437, mean reward: -0.037 [-100.000, 15.376], mean action: 1.601 [0.000, 3.000],  loss: 17.756540, mse: 10456.561629, mean_q: 77.656860, mean_eps: 0.577087
 128313/300000: episode: 1264, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -113.172, mean reward: -1.364 [-100.000,  7.311], mean action: 1.771 [0.000, 3.000],  loss: 12.255797, mse: 10210.931417, mean_q: 76.393637, mean_eps: 0.576706
 128406/300000: episode: 1265, duration: 0.774s, episode steps:  93, steps per second: 120, episode reward: -114.198, mean reward: -1.228 [-100.000,  7.401], mean action: 1.634 [0.000, 3.000],  loss: 22.364324, mse: 10389.397870, mean_q: 77.813010, mean_eps: 0.576415
 128566/300000: episode: 1266, duration: 1.214s, episode steps: 160, steps per second: 132, episode reward: -68.217, mean reward: -0.426 [-100.000, 11.947], mean action: 1.688 [0.000, 3.000],  loss: 16.597328, mse: 10467.511154, mean_q: 79.126514, mean_eps: 0.575998
 128674/300000: episode: 1267, duration: 0.788s, episode steps: 108, steps per second: 137, episode reward: -41.212, mean reward: -0.382 [-100.000, 13.683], mean action: 1.731 [0.000, 3.000],  loss: 18.671836, mse: 10329.893514, mean_q: 77.320865, mean_eps: 0.575556
 128759/300000: episode: 1268, duration: 0.649s, episode steps:  85, steps per second: 131, episode reward: -44.529, mean reward: -0.524 [-100.000, 10.118], mean action: 1.671 [0.000, 3.000],  loss: 14.575503, mse: 10437.227585, mean_q: 77.086002, mean_eps: 0.575237
 128861/300000: episode: 1269, duration: 0.750s, episode steps: 102, steps per second: 136, episode reward: -10.041, mean reward: -0.098 [-100.000, 11.209], mean action: 1.765 [0.000, 3.000],  loss: 17.178313, mse: 10724.281609, mean_q: 78.968588, mean_eps: 0.574929
 128965/300000: episode: 1270, duration: 0.733s, episode steps: 104, steps per second: 142, episode reward: 21.518, mean reward:  0.207 [-100.000, 15.212], mean action: 1.769 [0.000, 3.000],  loss: 23.888684, mse: 10726.630888, mean_q: 80.051356, mean_eps: 0.574589
 129063/300000: episode: 1271, duration: 0.776s, episode steps:  98, steps per second: 126, episode reward: -55.149, mean reward: -0.563 [-100.000,  7.723], mean action: 1.520 [0.000, 3.000],  loss: 13.640705, mse: 10487.459866, mean_q: 78.497300, mean_eps: 0.574255
 129282/300000: episode: 1272, duration: 1.682s, episode steps: 219, steps per second: 130, episode reward: -210.670, mean reward: -0.962 [-100.000, 13.587], mean action: 1.804 [0.000, 3.000],  loss: 16.228151, mse: 10547.412587, mean_q: 79.846312, mean_eps: 0.573732
 129397/300000: episode: 1273, duration: 0.906s, episode steps: 115, steps per second: 127, episode reward:  7.905, mean reward:  0.069 [-100.000, 14.804], mean action: 1.730 [0.000, 3.000],  loss: 19.332640, mse: 10594.146654, mean_q: 79.143715, mean_eps: 0.573181
 129534/300000: episode: 1274, duration: 0.948s, episode steps: 137, steps per second: 145, episode reward: -40.080, mean reward: -0.293 [-100.000, 18.092], mean action: 1.759 [0.000, 3.000],  loss: 19.571614, mse: 10596.248646, mean_q: 79.210773, mean_eps: 0.572766
 129622/300000: episode: 1275, duration: 0.656s, episode steps:  88, steps per second: 134, episode reward: -94.821, mean reward: -1.078 [-100.000,  9.475], mean action: 1.807 [0.000, 3.000],  loss: 19.042775, mse: 10388.847751, mean_q: 78.232796, mean_eps: 0.572394
 129724/300000: episode: 1276, duration: 0.736s, episode steps: 102, steps per second: 139, episode reward: -50.074, mean reward: -0.491 [-100.000,  6.624], mean action: 1.627 [0.000, 3.000],  loss: 19.144208, mse: 10495.145958, mean_q: 79.877191, mean_eps: 0.572081
 129828/300000: episode: 1277, duration: 0.707s, episode steps: 104, steps per second: 147, episode reward: -54.725, mean reward: -0.526 [-100.000, 14.734], mean action: 1.683 [0.000, 3.000],  loss: 18.651815, mse: 10540.007428, mean_q: 79.384615, mean_eps: 0.571741
 129943/300000: episode: 1278, duration: 0.830s, episode steps: 115, steps per second: 138, episode reward: -43.910, mean reward: -0.382 [-100.000, 14.503], mean action: 1.730 [0.000, 3.000],  loss: 16.975536, mse: 10453.952514, mean_q: 78.357751, mean_eps: 0.571380
 130064/300000: episode: 1279, duration: 0.898s, episode steps: 121, steps per second: 135, episode reward: -42.504, mean reward: -0.351 [-100.000, 73.342], mean action: 1.727 [0.000, 3.000],  loss: 14.768631, mse: 10455.690567, mean_q: 78.478261, mean_eps: 0.570990
 130193/300000: episode: 1280, duration: 0.986s, episode steps: 129, steps per second: 131, episode reward: 22.654, mean reward:  0.176 [-100.000, 21.447], mean action: 1.605 [0.000, 3.000],  loss: 17.278821, mse: 10480.906981, mean_q: 79.070418, mean_eps: 0.570578
 130355/300000: episode: 1281, duration: 1.145s, episode steps: 162, steps per second: 142, episode reward: -59.728, mean reward: -0.369 [-100.000,  9.833], mean action: 1.642 [0.000, 3.000],  loss: 19.093646, mse: 10521.957426, mean_q: 79.484967, mean_eps: 0.570097
 130446/300000: episode: 1282, duration: 0.648s, episode steps:  91, steps per second: 140, episode reward: -49.205, mean reward: -0.541 [-100.000, 10.357], mean action: 1.736 [0.000, 3.000],  loss: 17.961214, mse: 10956.808068, mean_q: 81.961470, mean_eps: 0.569680
 130551/300000: episode: 1283, duration: 0.780s, episode steps: 105, steps per second: 135, episode reward: -64.619, mean reward: -0.615 [-100.000, 13.139], mean action: 1.743 [0.000, 3.000],  loss: 15.322305, mse: 10439.491313, mean_q: 78.964241, mean_eps: 0.569357
 130881/300000: episode: 1284, duration: 2.382s, episode steps: 330, steps per second: 139, episode reward: -36.577, mean reward: -0.111 [-100.000, 14.787], mean action: 1.676 [0.000, 3.000],  loss: 18.481028, mse: 10509.957993, mean_q: 79.552829, mean_eps: 0.568639
 131026/300000: episode: 1285, duration: 1.028s, episode steps: 145, steps per second: 141, episode reward: -123.182, mean reward: -0.850 [-100.000, 15.075], mean action: 1.607 [0.000, 3.000],  loss: 17.961678, mse: 10276.717326, mean_q: 79.251304, mean_eps: 0.567855
 131163/300000: episode: 1286, duration: 1.003s, episode steps: 137, steps per second: 137, episode reward: -11.681, mean reward: -0.085 [-100.000, 19.134], mean action: 1.803 [0.000, 3.000],  loss: 18.593565, mse: 10268.982518, mean_q: 79.282267, mean_eps: 0.567390
 131272/300000: episode: 1287, duration: 0.803s, episode steps: 109, steps per second: 136, episode reward: -50.254, mean reward: -0.461 [-100.000, 19.779], mean action: 1.651 [0.000, 3.000],  loss: 19.188708, mse: 10527.452422, mean_q: 80.608846, mean_eps: 0.566984
 131368/300000: episode: 1288, duration: 0.723s, episode steps:  96, steps per second: 133, episode reward: -81.506, mean reward: -0.849 [-100.000, 17.811], mean action: 1.854 [0.000, 3.000],  loss: 16.362812, mse: 10631.642426, mean_q: 80.173740, mean_eps: 0.566646
 131512/300000: episode: 1289, duration: 1.038s, episode steps: 144, steps per second: 139, episode reward: -9.384, mean reward: -0.065 [-100.000, 10.747], mean action: 1.750 [0.000, 3.000],  loss: 20.483306, mse: 10668.691193, mean_q: 79.534927, mean_eps: 0.566250
 131625/300000: episode: 1290, duration: 0.799s, episode steps: 113, steps per second: 141, episode reward: -47.730, mean reward: -0.422 [-100.000, 10.585], mean action: 1.646 [0.000, 3.000],  loss: 20.633812, mse: 10769.723330, mean_q: 80.427473, mean_eps: 0.565826
 131723/300000: episode: 1291, duration: 0.724s, episode steps:  98, steps per second: 135, episode reward: -3.246, mean reward: -0.033 [-100.000, 17.336], mean action: 1.704 [0.000, 3.000],  loss: 14.732809, mse: 10893.106435, mean_q: 81.514751, mean_eps: 0.565477
 131824/300000: episode: 1292, duration: 0.762s, episode steps: 101, steps per second: 133, episode reward: -90.063, mean reward: -0.892 [-100.000, 11.141], mean action: 1.574 [0.000, 3.000],  loss: 20.479693, mse: 10682.271247, mean_q: 80.395055, mean_eps: 0.565149
 131935/300000: episode: 1293, duration: 0.795s, episode steps: 111, steps per second: 140, episode reward: -76.404, mean reward: -0.688 [-100.000,  7.307], mean action: 1.721 [0.000, 3.000],  loss: 13.746378, mse: 10641.222586, mean_q: 80.476208, mean_eps: 0.564799
 132024/300000: episode: 1294, duration: 0.695s, episode steps:  89, steps per second: 128, episode reward: -49.681, mean reward: -0.558 [-100.000, 11.855], mean action: 1.775 [0.000, 3.000],  loss: 23.492274, mse: 10857.829661, mean_q: 81.393841, mean_eps: 0.564469
 132141/300000: episode: 1295, duration: 0.900s, episode steps: 117, steps per second: 130, episode reward: -82.451, mean reward: -0.705 [-100.000, 17.428], mean action: 1.709 [0.000, 3.000],  loss: 17.688270, mse: 10776.706130, mean_q: 80.830516, mean_eps: 0.564129
 132292/300000: episode: 1296, duration: 1.162s, episode steps: 151, steps per second: 130, episode reward: -3.016, mean reward: -0.020 [-100.000, 22.974], mean action: 1.735 [0.000, 3.000],  loss: 18.917382, mse: 10773.346272, mean_q: 80.832352, mean_eps: 0.563687
 132423/300000: episode: 1297, duration: 0.930s, episode steps: 131, steps per second: 141, episode reward: 19.136, mean reward:  0.146 [-100.000, 16.904], mean action: 1.595 [0.000, 3.000],  loss: 16.615381, mse: 10812.940590, mean_q: 81.387446, mean_eps: 0.563222
 132554/300000: episode: 1298, duration: 1.012s, episode steps: 131, steps per second: 129, episode reward: -183.020, mean reward: -1.397 [-100.000, 31.563], mean action: 1.634 [0.000, 3.000],  loss: 16.167406, mse: 10767.966726, mean_q: 80.587629, mean_eps: 0.562790
 132692/300000: episode: 1299, duration: 1.052s, episode steps: 138, steps per second: 131, episode reward: -145.553, mean reward: -1.055 [-100.000, 10.634], mean action: 1.696 [0.000, 3.000],  loss: 15.022834, mse: 10652.310791, mean_q: 80.578611, mean_eps: 0.562346
 132814/300000: episode: 1300, duration: 0.880s, episode steps: 122, steps per second: 139, episode reward: -51.599, mean reward: -0.423 [-100.000,  9.229], mean action: 1.705 [0.000, 3.000],  loss: 17.296606, mse: 10704.799096, mean_q: 80.736270, mean_eps: 0.561917
 132915/300000: episode: 1301, duration: 0.780s, episode steps: 101, steps per second: 130, episode reward: -39.220, mean reward: -0.388 [-100.000, 15.437], mean action: 1.743 [0.000, 3.000],  loss: 16.828649, mse: 10606.875242, mean_q: 80.182537, mean_eps: 0.561549
 132995/300000: episode: 1302, duration: 0.574s, episode steps:  80, steps per second: 139, episode reward: -35.180, mean reward: -0.440 [-100.000, 14.021], mean action: 1.650 [0.000, 3.000],  loss: 20.622950, mse: 11305.115442, mean_q: 83.034110, mean_eps: 0.561250
 133136/300000: episode: 1303, duration: 1.027s, episode steps: 141, steps per second: 137, episode reward: -47.173, mean reward: -0.335 [-100.000, 12.419], mean action: 1.652 [0.000, 3.000],  loss: 23.062886, mse: 10786.986581, mean_q: 81.373478, mean_eps: 0.560886
 133242/300000: episode: 1304, duration: 0.823s, episode steps: 106, steps per second: 129, episode reward: -14.896, mean reward: -0.141 [-100.000, 15.487], mean action: 1.830 [0.000, 3.000],  loss: 21.113631, mse: 10662.494532, mean_q: 80.963258, mean_eps: 0.560478
 133338/300000: episode: 1305, duration: 0.729s, episode steps:  96, steps per second: 132, episode reward: -81.965, mean reward: -0.854 [-100.000,  9.597], mean action: 1.625 [0.000, 3.000],  loss: 25.106025, mse: 10913.953695, mean_q: 81.725282, mean_eps: 0.560145
 133448/300000: episode: 1306, duration: 0.868s, episode steps: 110, steps per second: 127, episode reward: -5.707, mean reward: -0.052 [-100.000, 15.773], mean action: 1.773 [0.000, 3.000],  loss: 16.848706, mse: 11228.395517, mean_q: 83.377639, mean_eps: 0.559805
 134448/300000: episode: 1307, duration: 8.448s, episode steps: 1000, steps per second: 118, episode reward: 85.076, mean reward:  0.085 [-23.964, 27.352], mean action: 1.672 [0.000, 3.000],  loss: 19.491800, mse: 10712.974225, mean_q: 81.348051, mean_eps: 0.557973
 134567/300000: episode: 1308, duration: 0.897s, episode steps: 119, steps per second: 133, episode reward: -7.617, mean reward: -0.064 [-100.000, 15.002], mean action: 1.597 [0.000, 3.000],  loss: 18.803770, mse: 10696.074826, mean_q: 82.466420, mean_eps: 0.556127
 134696/300000: episode: 1309, duration: 0.928s, episode steps: 129, steps per second: 139, episode reward: -31.584, mean reward: -0.245 [-100.000, 13.656], mean action: 1.527 [0.000, 3.000],  loss: 20.258543, mse: 10845.697595, mean_q: 83.641961, mean_eps: 0.555718
 134806/300000: episode: 1310, duration: 0.826s, episode steps: 110, steps per second: 133, episode reward: -38.327, mean reward: -0.348 [-100.000, 16.748], mean action: 1.791 [0.000, 3.000],  loss: 18.654187, mse: 10728.523260, mean_q: 82.610817, mean_eps: 0.555323
 134902/300000: episode: 1311, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: 23.995, mean reward:  0.250 [-100.000,  9.933], mean action: 1.865 [0.000, 3.000],  loss: 17.360824, mse: 10604.999273, mean_q: 81.459058, mean_eps: 0.554983
 134975/300000: episode: 1312, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -22.863, mean reward: -0.313 [-100.000,  9.751], mean action: 1.534 [0.000, 3.000],  loss: 14.408860, mse: 11091.804721, mean_q: 84.821861, mean_eps: 0.554705
 135975/300000: episode: 1313, duration: 9.170s, episode steps: 1000, steps per second: 109, episode reward: 16.917, mean reward:  0.017 [-24.786, 22.682], mean action: 1.441 [0.000, 3.000],  loss: 18.838567, mse: 10710.142394, mean_q: 82.799517, mean_eps: 0.552934
 136975/300000: episode: 1314, duration: 8.449s, episode steps: 1000, steps per second: 118, episode reward: 39.595, mean reward:  0.040 [-23.919, 25.521], mean action: 1.897 [0.000, 3.000],  loss: 17.756135, mse: 10736.275963, mean_q: 83.741119, mean_eps: 0.549634
 137092/300000: episode: 1315, duration: 0.982s, episode steps: 117, steps per second: 119, episode reward: -58.893, mean reward: -0.503 [-100.000, 23.281], mean action: 1.487 [0.000, 3.000],  loss: 17.208658, mse: 10547.835328, mean_q: 82.621660, mean_eps: 0.547791
 137303/300000: episode: 1316, duration: 1.625s, episode steps: 211, steps per second: 130, episode reward: -58.164, mean reward: -0.276 [-100.000, 13.156], mean action: 1.673 [0.000, 3.000],  loss: 20.610800, mse: 10742.338513, mean_q: 83.788616, mean_eps: 0.547250
 137457/300000: episode: 1317, duration: 1.118s, episode steps: 154, steps per second: 138, episode reward: -132.052, mean reward: -0.857 [-100.000, 67.935], mean action: 1.825 [0.000, 3.000],  loss: 16.229342, mse: 10620.419136, mean_q: 83.201235, mean_eps: 0.546648
 137596/300000: episode: 1318, duration: 1.102s, episode steps: 139, steps per second: 126, episode reward: -55.524, mean reward: -0.399 [-100.000,  9.457], mean action: 1.683 [0.000, 3.000],  loss: 18.516866, mse: 10567.252982, mean_q: 83.358672, mean_eps: 0.546164
 137711/300000: episode: 1319, duration: 0.891s, episode steps: 115, steps per second: 129, episode reward: -44.571, mean reward: -0.388 [-100.000, 13.914], mean action: 1.843 [0.000, 3.000],  loss: 17.206790, mse: 10974.596421, mean_q: 85.613626, mean_eps: 0.545745
 137818/300000: episode: 1320, duration: 0.802s, episode steps: 107, steps per second: 133, episode reward: -48.578, mean reward: -0.454 [-100.000, 12.113], mean action: 1.850 [0.000, 3.000],  loss: 22.023249, mse: 10872.279571, mean_q: 84.398162, mean_eps: 0.545379
 137948/300000: episode: 1321, duration: 1.005s, episode steps: 130, steps per second: 129, episode reward: -7.075, mean reward: -0.054 [-100.000, 14.010], mean action: 1.577 [0.000, 3.000],  loss: 17.891856, mse: 10534.723505, mean_q: 82.085611, mean_eps: 0.544988
 138070/300000: episode: 1322, duration: 0.841s, episode steps: 122, steps per second: 145, episode reward: -71.758, mean reward: -0.588 [-100.000,  9.909], mean action: 1.705 [0.000, 3.000],  loss: 29.543109, mse: 10846.058938, mean_q: 85.053419, mean_eps: 0.544572
 138165/300000: episode: 1323, duration: 0.716s, episode steps:  95, steps per second: 133, episode reward: -8.186, mean reward: -0.086 [-100.000, 15.917], mean action: 1.832 [0.000, 3.000],  loss: 26.454486, mse: 10560.239885, mean_q: 84.852336, mean_eps: 0.544214
 138285/300000: episode: 1324, duration: 0.889s, episode steps: 120, steps per second: 135, episode reward: -192.879, mean reward: -1.607 [-100.000, 62.753], mean action: 1.758 [0.000, 3.000],  loss: 28.510639, mse: 10608.095597, mean_q: 84.096524, mean_eps: 0.543859
 138415/300000: episode: 1325, duration: 0.974s, episode steps: 130, steps per second: 134, episode reward: -33.252, mean reward: -0.256 [-100.000, 17.327], mean action: 1.715 [0.000, 3.000],  loss: 25.569012, mse: 11031.274827, mean_q: 86.075215, mean_eps: 0.543447
 138511/300000: episode: 1326, duration: 0.744s, episode steps:  96, steps per second: 129, episode reward: -13.450, mean reward: -0.140 [-100.000, 15.222], mean action: 1.615 [0.000, 3.000],  loss: 26.196026, mse: 10647.624837, mean_q: 85.721296, mean_eps: 0.543074
 138608/300000: episode: 1327, duration: 0.708s, episode steps:  97, steps per second: 137, episode reward: -73.390, mean reward: -0.757 [-100.000, 22.489], mean action: 1.546 [0.000, 3.000],  loss: 21.554751, mse: 10965.092909, mean_q: 85.835317, mean_eps: 0.542755
 138742/300000: episode: 1328, duration: 0.994s, episode steps: 134, steps per second: 135, episode reward: -3.506, mean reward: -0.026 [-100.000, 25.124], mean action: 1.619 [0.000, 3.000],  loss: 24.209624, mse: 10916.761959, mean_q: 86.537610, mean_eps: 0.542374
 138857/300000: episode: 1329, duration: 0.886s, episode steps: 115, steps per second: 130, episode reward: -49.056, mean reward: -0.427 [-100.000, 20.917], mean action: 1.609 [0.000, 3.000],  loss: 16.168716, mse: 10549.945194, mean_q: 83.830858, mean_eps: 0.541963
 138993/300000: episode: 1330, duration: 1.048s, episode steps: 136, steps per second: 130, episode reward: -74.844, mean reward: -0.550 [-100.000,  5.957], mean action: 1.721 [0.000, 3.000],  loss: 25.307170, mse: 10637.127542, mean_q: 83.985747, mean_eps: 0.541549
 139085/300000: episode: 1331, duration: 0.687s, episode steps:  92, steps per second: 134, episode reward: -57.233, mean reward: -0.622 [-100.000, 20.174], mean action: 1.707 [0.000, 3.000],  loss: 20.745277, mse: 10800.202929, mean_q: 86.976626, mean_eps: 0.541173
 139190/300000: episode: 1332, duration: 0.732s, episode steps: 105, steps per second: 143, episode reward: -28.896, mean reward: -0.275 [-100.000, 19.017], mean action: 1.781 [0.000, 3.000],  loss: 29.749246, mse: 10811.704385, mean_q: 85.841536, mean_eps: 0.540848
 139307/300000: episode: 1333, duration: 0.883s, episode steps: 117, steps per second: 133, episode reward: -78.428, mean reward: -0.670 [-100.000, 12.767], mean action: 1.761 [0.000, 3.000],  loss: 19.466558, mse: 10843.302801, mean_q: 86.644602, mean_eps: 0.540482
 139409/300000: episode: 1334, duration: 0.816s, episode steps: 102, steps per second: 125, episode reward: -13.711, mean reward: -0.134 [-100.000, 11.788], mean action: 1.696 [0.000, 3.000],  loss: 24.552822, mse: 10944.491886, mean_q: 86.581497, mean_eps: 0.540120
 139496/300000: episode: 1335, duration: 0.655s, episode steps:  87, steps per second: 133, episode reward: -41.415, mean reward: -0.476 [-100.000,  7.721], mean action: 1.667 [0.000, 3.000],  loss: 23.128363, mse: 10319.317456, mean_q: 83.885776, mean_eps: 0.539808
 139632/300000: episode: 1336, duration: 1.039s, episode steps: 136, steps per second: 131, episode reward: -1.422, mean reward: -0.010 [-100.000, 18.091], mean action: 1.544 [0.000, 3.000],  loss: 21.220758, mse: 10606.000797, mean_q: 85.761632, mean_eps: 0.539440
 139750/300000: episode: 1337, duration: 0.891s, episode steps: 118, steps per second: 132, episode reward: -81.455, mean reward: -0.690 [-100.000,  6.831], mean action: 1.839 [0.000, 3.000],  loss: 19.328855, mse: 10772.786571, mean_q: 86.643774, mean_eps: 0.539021
 139869/300000: episode: 1338, duration: 0.895s, episode steps: 119, steps per second: 133, episode reward: -63.049, mean reward: -0.530 [-100.000,  7.777], mean action: 1.731 [0.000, 3.000],  loss: 21.688788, mse: 10690.778480, mean_q: 86.221458, mean_eps: 0.538630
 140869/300000: episode: 1339, duration: 8.091s, episode steps: 1000, steps per second: 124, episode reward: 126.272, mean reward:  0.126 [-23.052, 24.440], mean action: 1.539 [0.000, 3.000],  loss: 22.880658, mse: 10634.575289, mean_q: 87.045897, mean_eps: 0.536784
 141045/300000: episode: 1340, duration: 1.272s, episode steps: 176, steps per second: 138, episode reward: 45.645, mean reward:  0.259 [-100.000, 20.357], mean action: 1.773 [0.000, 3.000],  loss: 20.244791, mse: 10657.948170, mean_q: 87.470980, mean_eps: 0.534844
 141181/300000: episode: 1341, duration: 0.947s, episode steps: 136, steps per second: 144, episode reward: -3.763, mean reward: -0.028 [-100.000, 16.591], mean action: 1.743 [0.000, 3.000],  loss: 23.444803, mse: 10790.766505, mean_q: 87.255325, mean_eps: 0.534329
 141314/300000: episode: 1342, duration: 0.982s, episode steps: 133, steps per second: 135, episode reward: -12.503, mean reward: -0.094 [-100.000, 14.944], mean action: 1.707 [0.000, 3.000],  loss: 22.802595, mse: 10576.249079, mean_q: 87.344214, mean_eps: 0.533885
 141401/300000: episode: 1343, duration: 0.595s, episode steps:  87, steps per second: 146, episode reward: -71.514, mean reward: -0.822 [-100.000, 16.528], mean action: 1.862 [0.000, 3.000],  loss: 21.744605, mse: 10692.024094, mean_q: 88.686418, mean_eps: 0.533522
 141493/300000: episode: 1344, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -56.025, mean reward: -0.609 [-100.000, 10.284], mean action: 1.717 [0.000, 3.000],  loss: 17.979062, mse: 10873.796185, mean_q: 89.575406, mean_eps: 0.533227
 141616/300000: episode: 1345, duration: 0.886s, episode steps: 123, steps per second: 139, episode reward: -5.069, mean reward: -0.041 [-100.000, 12.530], mean action: 1.780 [0.000, 3.000],  loss: 21.262452, mse: 10858.732882, mean_q: 88.565720, mean_eps: 0.532872
 141714/300000: episode: 1346, duration: 0.845s, episode steps:  98, steps per second: 116, episode reward: -74.809, mean reward: -0.763 [-100.000,  7.616], mean action: 1.561 [0.000, 3.000],  loss: 28.510635, mse: 10679.983369, mean_q: 88.393457, mean_eps: 0.532507
 141841/300000: episode: 1347, duration: 0.928s, episode steps: 127, steps per second: 137, episode reward: -36.524, mean reward: -0.288 [-100.000, 27.943], mean action: 1.787 [0.000, 3.000],  loss: 22.888340, mse: 10908.718473, mean_q: 88.608462, mean_eps: 0.532136
 141946/300000: episode: 1348, duration: 0.743s, episode steps: 105, steps per second: 141, episode reward: -51.525, mean reward: -0.491 [-100.000, 14.695], mean action: 1.810 [0.000, 3.000],  loss: 17.974200, mse: 10761.817541, mean_q: 89.753929, mean_eps: 0.531753
 142084/300000: episode: 1349, duration: 0.984s, episode steps: 138, steps per second: 140, episode reward: -95.786, mean reward: -0.694 [-100.000, 15.703], mean action: 1.717 [0.000, 3.000],  loss: 19.221539, mse: 10624.612634, mean_q: 88.222230, mean_eps: 0.531352
 142183/300000: episode: 1350, duration: 0.798s, episode steps:  99, steps per second: 124, episode reward: -49.874, mean reward: -0.504 [-100.000, 14.233], mean action: 1.576 [0.000, 3.000],  loss: 22.059642, mse: 10525.918117, mean_q: 88.588563, mean_eps: 0.530961
 142296/300000: episode: 1351, duration: 0.871s, episode steps: 113, steps per second: 130, episode reward: -19.092, mean reward: -0.169 [-100.000, 11.930], mean action: 1.549 [0.000, 3.000],  loss: 20.849656, mse: 10643.479302, mean_q: 88.675547, mean_eps: 0.530611
 142422/300000: episode: 1352, duration: 0.985s, episode steps: 126, steps per second: 128, episode reward: -69.016, mean reward: -0.548 [-100.000, 30.593], mean action: 1.841 [0.000, 3.000],  loss: 17.078285, mse: 10756.567360, mean_q: 89.212329, mean_eps: 0.530217
 142609/300000: episode: 1353, duration: 1.417s, episode steps: 187, steps per second: 132, episode reward: -29.230, mean reward: -0.156 [-100.000, 25.522], mean action: 1.743 [0.000, 3.000],  loss: 19.292194, mse: 10588.427285, mean_q: 88.275825, mean_eps: 0.529700
 143609/300000: episode: 1354, duration: 7.857s, episode steps: 1000, steps per second: 127, episode reward: 33.378, mean reward:  0.033 [-22.578, 26.329], mean action: 1.567 [0.000, 3.000],  loss: 20.959955, mse: 10659.120452, mean_q: 89.134176, mean_eps: 0.527742
 143702/300000: episode: 1355, duration: 0.647s, episode steps:  93, steps per second: 144, episode reward: -79.866, mean reward: -0.859 [-100.000, 18.958], mean action: 1.667 [0.000, 3.000],  loss: 19.348346, mse: 10309.817677, mean_q: 87.741657, mean_eps: 0.525938
 143786/300000: episode: 1356, duration: 0.601s, episode steps:  84, steps per second: 140, episode reward: -8.601, mean reward: -0.102 [-100.000, 15.527], mean action: 1.619 [0.000, 3.000],  loss: 20.605016, mse: 10475.939395, mean_q: 87.760346, mean_eps: 0.525646
 143916/300000: episode: 1357, duration: 0.953s, episode steps: 130, steps per second: 136, episode reward: -126.146, mean reward: -0.970 [-100.000, 10.973], mean action: 1.600 [0.000, 3.000],  loss: 25.056261, mse: 10750.739896, mean_q: 90.089075, mean_eps: 0.525293
 144012/300000: episode: 1358, duration: 0.671s, episode steps:  96, steps per second: 143, episode reward:  6.734, mean reward:  0.070 [-100.000, 26.952], mean action: 1.635 [0.000, 3.000],  loss: 24.870866, mse: 10307.324834, mean_q: 88.556091, mean_eps: 0.524920
 144117/300000: episode: 1359, duration: 0.749s, episode steps: 105, steps per second: 140, episode reward: -8.181, mean reward: -0.078 [-100.000, 19.936], mean action: 1.743 [0.000, 3.000],  loss: 20.926951, mse: 10226.692220, mean_q: 87.652908, mean_eps: 0.524589
 144241/300000: episode: 1360, duration: 0.994s, episode steps: 124, steps per second: 125, episode reward: 12.052, mean reward:  0.097 [-100.000, 18.799], mean action: 1.750 [0.000, 3.000],  loss: 20.556056, mse: 10916.138774, mean_q: 91.377864, mean_eps: 0.524211
 144338/300000: episode: 1361, duration: 0.792s, episode steps:  97, steps per second: 122, episode reward: -58.387, mean reward: -0.602 [-100.000, 14.354], mean action: 1.856 [0.000, 3.000],  loss: 24.679369, mse: 10713.322200, mean_q: 90.334111, mean_eps: 0.523846
 144447/300000: episode: 1362, duration: 0.965s, episode steps: 109, steps per second: 113, episode reward: -45.752, mean reward: -0.420 [-100.000, 15.397], mean action: 1.706 [0.000, 3.000],  loss: 27.643307, mse: 10777.888287, mean_q: 90.395465, mean_eps: 0.523506
 144616/300000: episode: 1363, duration: 1.300s, episode steps: 169, steps per second: 130, episode reward: -87.914, mean reward: -0.520 [-100.000,  8.545], mean action: 1.704 [0.000, 3.000],  loss: 23.666637, mse: 10596.835776, mean_q: 89.793931, mean_eps: 0.523048
 144741/300000: episode: 1364, duration: 0.972s, episode steps: 125, steps per second: 129, episode reward: -48.858, mean reward: -0.391 [-100.000, 16.354], mean action: 1.832 [0.000, 3.000],  loss: 15.975461, mse: 10609.671090, mean_q: 89.126216, mean_eps: 0.522563
 144878/300000: episode: 1365, duration: 0.992s, episode steps: 137, steps per second: 138, episode reward:  9.805, mean reward:  0.072 [-100.000, 17.492], mean action: 1.847 [0.000, 3.000],  loss: 16.968859, mse: 10921.429203, mean_q: 91.703807, mean_eps: 0.522130
 145007/300000: episode: 1366, duration: 0.933s, episode steps: 129, steps per second: 138, episode reward: 30.180, mean reward:  0.234 [-100.000, 70.166], mean action: 1.667 [0.000, 3.000],  loss: 18.888526, mse: 11081.794979, mean_q: 92.328641, mean_eps: 0.521691
 145139/300000: episode: 1367, duration: 0.906s, episode steps: 132, steps per second: 146, episode reward: -39.382, mean reward: -0.298 [-100.000, 13.128], mean action: 1.788 [0.000, 3.000],  loss: 21.816489, mse: 11016.052124, mean_q: 91.881061, mean_eps: 0.521261
 145230/300000: episode: 1368, duration: 0.619s, episode steps:  91, steps per second: 147, episode reward: 17.897, mean reward:  0.197 [-100.000, 17.610], mean action: 1.648 [0.000, 3.000],  loss: 18.159211, mse: 10801.318402, mean_q: 90.120400, mean_eps: 0.520893
 145318/300000: episode: 1369, duration: 0.654s, episode steps:  88, steps per second: 134, episode reward:  6.958, mean reward:  0.079 [-100.000, 13.180], mean action: 1.898 [0.000, 3.000],  loss: 23.054376, mse: 11002.917708, mean_q: 91.379626, mean_eps: 0.520597
 145431/300000: episode: 1370, duration: 0.768s, episode steps: 113, steps per second: 147, episode reward: -46.885, mean reward: -0.415 [-100.000,  4.959], mean action: 1.947 [0.000, 3.000],  loss: 16.743089, mse: 11072.016610, mean_q: 92.528841, mean_eps: 0.520266
 145534/300000: episode: 1371, duration: 0.705s, episode steps: 103, steps per second: 146, episode reward: -45.920, mean reward: -0.446 [-100.000, 11.076], mean action: 1.641 [0.000, 3.000],  loss: 23.054074, mse: 10862.474330, mean_q: 91.218288, mean_eps: 0.519909
 145681/300000: episode: 1372, duration: 1.069s, episode steps: 147, steps per second: 138, episode reward: -26.022, mean reward: -0.177 [-100.000, 17.048], mean action: 1.755 [0.000, 3.000],  loss: 23.010470, mse: 11030.412518, mean_q: 91.863179, mean_eps: 0.519497
 145810/300000: episode: 1373, duration: 0.952s, episode steps: 129, steps per second: 135, episode reward:  6.425, mean reward:  0.050 [-100.000, 24.663], mean action: 1.581 [0.000, 3.000],  loss: 19.932052, mse: 10916.971581, mean_q: 90.958735, mean_eps: 0.519041
 145930/300000: episode: 1374, duration: 0.888s, episode steps: 120, steps per second: 135, episode reward: -49.383, mean reward: -0.412 [-100.000,  9.827], mean action: 1.842 [0.000, 3.000],  loss: 17.797976, mse: 10940.918953, mean_q: 91.111683, mean_eps: 0.518631
 146102/300000: episode: 1375, duration: 1.202s, episode steps: 172, steps per second: 143, episode reward: -141.492, mean reward: -0.823 [-100.000,  7.506], mean action: 1.674 [0.000, 3.000],  loss: 16.721697, mse: 10904.739692, mean_q: 91.964414, mean_eps: 0.518149
 146328/300000: episode: 1376, duration: 1.618s, episode steps: 226, steps per second: 140, episode reward: -113.647, mean reward: -0.503 [-100.000, 15.206], mean action: 1.540 [0.000, 3.000],  loss: 18.031733, mse: 10825.873706, mean_q: 91.098964, mean_eps: 0.517492
 146434/300000: episode: 1377, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -32.374, mean reward: -0.305 [-100.000, 17.388], mean action: 1.642 [0.000, 3.000],  loss: 14.936968, mse: 11221.318240, mean_q: 92.809960, mean_eps: 0.516944
 146511/300000: episode: 1378, duration: 0.570s, episode steps:  77, steps per second: 135, episode reward: -37.923, mean reward: -0.493 [-100.000, 14.906], mean action: 1.701 [0.000, 3.000],  loss: 15.775051, mse: 10875.575443, mean_q: 90.033922, mean_eps: 0.516642
 146613/300000: episode: 1379, duration: 0.716s, episode steps: 102, steps per second: 142, episode reward: -59.259, mean reward: -0.581 [-100.000, 14.450], mean action: 1.608 [0.000, 3.000],  loss: 19.557367, mse: 10853.780130, mean_q: 90.203725, mean_eps: 0.516347
 146691/300000: episode: 1380, duration: 0.532s, episode steps:  78, steps per second: 147, episode reward: -39.661, mean reward: -0.508 [-100.000, 11.875], mean action: 1.500 [0.000, 3.000],  loss: 20.138394, mse: 10984.644644, mean_q: 90.644433, mean_eps: 0.516050
 146889/300000: episode: 1381, duration: 1.409s, episode steps: 198, steps per second: 141, episode reward: 29.417, mean reward:  0.149 [-100.000, 15.760], mean action: 1.697 [0.000, 3.000],  loss: 21.643532, mse: 10928.563938, mean_q: 91.284906, mean_eps: 0.515595
 147033/300000: episode: 1382, duration: 1.003s, episode steps: 144, steps per second: 144, episode reward: -18.413, mean reward: -0.128 [-100.000, 40.105], mean action: 1.688 [0.000, 3.000],  loss: 22.619569, mse: 10956.323866, mean_q: 92.242895, mean_eps: 0.515030
 147165/300000: episode: 1383, duration: 1.001s, episode steps: 132, steps per second: 132, episode reward: -55.081, mean reward: -0.417 [-100.000, 14.372], mean action: 1.606 [0.000, 3.000],  loss: 20.648054, mse: 11199.737098, mean_q: 93.822058, mean_eps: 0.514575
 147270/300000: episode: 1384, duration: 0.735s, episode steps: 105, steps per second: 143, episode reward: -41.923, mean reward: -0.399 [-100.000, 12.649], mean action: 1.733 [0.000, 3.000],  loss: 23.936793, mse: 11261.201981, mean_q: 94.182423, mean_eps: 0.514184
 147411/300000: episode: 1385, duration: 1.018s, episode steps: 141, steps per second: 139, episode reward: -51.234, mean reward: -0.363 [-100.000,  9.770], mean action: 1.773 [0.000, 3.000],  loss: 22.907851, mse: 11031.705026, mean_q: 93.720285, mean_eps: 0.513778
 147517/300000: episode: 1386, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -84.779, mean reward: -0.800 [-100.000, 11.878], mean action: 1.660 [0.000, 3.000],  loss: 21.830516, mse: 11173.365796, mean_q: 94.919102, mean_eps: 0.513370
 147654/300000: episode: 1387, duration: 0.935s, episode steps: 137, steps per second: 146, episode reward: 55.254, mean reward:  0.403 [-100.000, 85.916], mean action: 1.861 [0.000, 3.000],  loss: 20.120946, mse: 10820.233302, mean_q: 92.253800, mean_eps: 0.512969
 147768/300000: episode: 1388, duration: 0.836s, episode steps: 114, steps per second: 136, episode reward: -28.974, mean reward: -0.254 [-100.000, 23.773], mean action: 1.798 [0.000, 3.000],  loss: 20.835900, mse: 11187.123689, mean_q: 93.588201, mean_eps: 0.512555
 147903/300000: episode: 1389, duration: 1.038s, episode steps: 135, steps per second: 130, episode reward: 25.210, mean reward:  0.187 [-100.000, 13.760], mean action: 1.674 [0.000, 3.000],  loss: 21.764479, mse: 11458.113824, mean_q: 95.416395, mean_eps: 0.512144
 148045/300000: episode: 1390, duration: 1.039s, episode steps: 142, steps per second: 137, episode reward: -135.152, mean reward: -0.952 [-100.000,  7.180], mean action: 1.697 [0.000, 3.000],  loss: 21.360017, mse: 11460.128628, mean_q: 96.313113, mean_eps: 0.511687
 148135/300000: episode: 1391, duration: 0.612s, episode steps:  90, steps per second: 147, episode reward: -39.716, mean reward: -0.441 [-100.000, 16.146], mean action: 1.767 [0.000, 3.000],  loss: 28.599214, mse: 11432.631033, mean_q: 96.028817, mean_eps: 0.511305
 148267/300000: episode: 1392, duration: 0.899s, episode steps: 132, steps per second: 147, episode reward: -76.674, mean reward: -0.581 [-100.000, 11.458], mean action: 1.712 [0.000, 3.000],  loss: 21.692639, mse: 11268.000618, mean_q: 95.004758, mean_eps: 0.510938
 148378/300000: episode: 1393, duration: 0.823s, episode steps: 111, steps per second: 135, episode reward: -48.623, mean reward: -0.438 [-100.000, 17.903], mean action: 1.730 [0.000, 3.000],  loss: 22.450093, mse: 11440.265291, mean_q: 95.060252, mean_eps: 0.510537
 148466/300000: episode: 1394, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -43.005, mean reward: -0.489 [-100.000, 16.760], mean action: 1.693 [0.000, 3.000],  loss: 18.019098, mse: 11211.300881, mean_q: 95.134946, mean_eps: 0.510209
 148592/300000: episode: 1395, duration: 0.896s, episode steps: 126, steps per second: 141, episode reward: -33.820, mean reward: -0.268 [-100.000,  9.333], mean action: 1.754 [0.000, 3.000],  loss: 25.723775, mse: 11523.691476, mean_q: 95.344291, mean_eps: 0.509856
 148761/300000: episode: 1396, duration: 1.194s, episode steps: 169, steps per second: 141, episode reward: -29.364, mean reward: -0.174 [-100.000,  8.913], mean action: 1.657 [0.000, 3.000],  loss: 20.596591, mse: 11225.377811, mean_q: 94.761981, mean_eps: 0.509369
 148894/300000: episode: 1397, duration: 0.943s, episode steps: 133, steps per second: 141, episode reward: -45.876, mean reward: -0.345 [-100.000, 15.198], mean action: 1.797 [0.000, 3.000],  loss: 17.738969, mse: 11274.917752, mean_q: 94.799343, mean_eps: 0.508871
 148987/300000: episode: 1398, duration: 0.672s, episode steps:  93, steps per second: 138, episode reward: -9.270, mean reward: -0.100 [-100.000,  8.740], mean action: 1.667 [0.000, 3.000],  loss: 27.145653, mse: 11349.592569, mean_q: 94.387276, mean_eps: 0.508498
 149132/300000: episode: 1399, duration: 0.989s, episode steps: 145, steps per second: 147, episode reward: -24.819, mean reward: -0.171 [-100.000,  9.949], mean action: 1.621 [0.000, 3.000],  loss: 20.773575, mse: 11580.834675, mean_q: 95.604491, mean_eps: 0.508105
 149223/300000: episode: 1400, duration: 0.661s, episode steps:  91, steps per second: 138, episode reward: -40.378, mean reward: -0.444 [-100.000, 17.310], mean action: 1.703 [0.000, 3.000],  loss: 23.102799, mse: 11555.877608, mean_q: 95.076106, mean_eps: 0.507716
 149453/300000: episode: 1401, duration: 1.611s, episode steps: 230, steps per second: 143, episode reward: 24.947, mean reward:  0.108 [-100.000, 17.630], mean action: 1.670 [0.000, 3.000],  loss: 20.677073, mse: 11448.893459, mean_q: 95.583230, mean_eps: 0.507186
 149742/300000: episode: 1402, duration: 2.069s, episode steps: 289, steps per second: 140, episode reward: -168.974, mean reward: -0.585 [-100.000, 14.508], mean action: 1.706 [0.000, 3.000],  loss: 19.680437, mse: 11709.702819, mean_q: 96.510209, mean_eps: 0.506330
 149955/300000: episode: 1403, duration: 1.530s, episode steps: 213, steps per second: 139, episode reward: -36.769, mean reward: -0.173 [-100.000, 19.385], mean action: 1.709 [0.000, 3.000],  loss: 24.703518, mse: 11609.636517, mean_q: 95.944641, mean_eps: 0.505502
 150099/300000: episode: 1404, duration: 0.993s, episode steps: 144, steps per second: 145, episode reward: -32.898, mean reward: -0.228 [-100.000, 14.160], mean action: 1.722 [0.000, 3.000],  loss: 20.289232, mse: 11489.963603, mean_q: 95.882106, mean_eps: 0.504913
 150220/300000: episode: 1405, duration: 0.886s, episode steps: 121, steps per second: 136, episode reward: -178.563, mean reward: -1.476 [-100.000, 25.335], mean action: 1.711 [0.000, 3.000],  loss: 21.292314, mse: 11481.831035, mean_q: 95.605709, mean_eps: 0.504475
 151220/300000: episode: 1406, duration: 7.999s, episode steps: 1000, steps per second: 125, episode reward: 22.707, mean reward:  0.023 [-21.590, 39.968], mean action: 1.498 [0.000, 3.000],  loss: 19.919366, mse: 11525.566951, mean_q: 96.251231, mean_eps: 0.502626
 151314/300000: episode: 1407, duration: 0.727s, episode steps:  94, steps per second: 129, episode reward: -59.893, mean reward: -0.637 [-100.000, 12.594], mean action: 1.798 [0.000, 3.000],  loss: 20.229798, mse: 11576.096472, mean_q: 96.782791, mean_eps: 0.500821
 151598/300000: episode: 1408, duration: 2.192s, episode steps: 284, steps per second: 130, episode reward: -23.989, mean reward: -0.084 [-100.000, 14.437], mean action: 1.803 [0.000, 3.000],  loss: 22.813758, mse: 11754.742107, mean_q: 97.792900, mean_eps: 0.500197
 152598/300000: episode: 1409, duration: 7.971s, episode steps: 1000, steps per second: 125, episode reward: 107.192, mean reward:  0.107 [-22.681, 27.175], mean action: 1.436 [0.000, 3.000],  loss: 21.343397, mse: 11504.538966, mean_q: 96.875631, mean_eps: 0.498078
 153277/300000: episode: 1410, duration: 5.412s, episode steps: 679, steps per second: 125, episode reward: -84.241, mean reward: -0.124 [-100.000, 22.307], mean action: 1.399 [0.000, 3.000],  loss: 23.863978, mse: 11371.477674, mean_q: 96.378991, mean_eps: 0.495308
 153363/300000: episode: 1411, duration: 0.664s, episode steps:  86, steps per second: 130, episode reward: 38.034, mean reward:  0.442 [-100.000, 18.676], mean action: 1.814 [0.000, 3.000],  loss: 16.582842, mse: 11536.166788, mean_q: 97.702538, mean_eps: 0.494046
 153516/300000: episode: 1412, duration: 1.218s, episode steps: 153, steps per second: 126, episode reward: -37.247, mean reward: -0.243 [-100.000, 10.286], mean action: 1.856 [0.000, 3.000],  loss: 25.299588, mse: 11433.285051, mean_q: 97.198617, mean_eps: 0.493651
 154516/300000: episode: 1413, duration: 8.013s, episode steps: 1000, steps per second: 125, episode reward: 111.077, mean reward:  0.111 [-24.150, 24.608], mean action: 1.205 [0.000, 3.000],  loss: 23.564199, mse: 11386.823933, mean_q: 97.157182, mean_eps: 0.491749
 155516/300000: episode: 1414, duration: 7.621s, episode steps: 1000, steps per second: 131, episode reward: 53.045, mean reward:  0.053 [-25.090, 23.484], mean action: 1.298 [0.000, 3.000],  loss: 23.610017, mse: 11152.704497, mean_q: 96.747770, mean_eps: 0.488449
 155664/300000: episode: 1415, duration: 1.061s, episode steps: 148, steps per second: 139, episode reward: -46.749, mean reward: -0.316 [-100.000, 11.674], mean action: 1.919 [0.000, 3.000],  loss: 24.288733, mse: 11161.152789, mean_q: 96.364442, mean_eps: 0.486555
 155770/300000: episode: 1416, duration: 0.763s, episode steps: 106, steps per second: 139, episode reward: -90.508, mean reward: -0.854 [-100.000,  9.605], mean action: 1.783 [0.000, 3.000],  loss: 26.782629, mse: 11442.975918, mean_q: 98.085920, mean_eps: 0.486136
 156770/300000: episode: 1417, duration: 7.875s, episode steps: 1000, steps per second: 127, episode reward: 125.894, mean reward:  0.126 [-19.411, 46.014], mean action: 1.238 [0.000, 3.000],  loss: 22.015051, mse: 11168.442002, mean_q: 97.127927, mean_eps: 0.484311
 156861/300000: episode: 1418, duration: 0.639s, episode steps:  91, steps per second: 142, episode reward: -64.109, mean reward: -0.704 [-100.000,  9.770], mean action: 1.758 [0.000, 3.000],  loss: 21.459083, mse: 11018.618507, mean_q: 97.073415, mean_eps: 0.482511
 157861/300000: episode: 1419, duration: 8.120s, episode steps: 1000, steps per second: 123, episode reward: 75.473, mean reward:  0.075 [-24.713, 26.881], mean action: 1.293 [0.000, 3.000],  loss: 20.684255, mse: 10980.779045, mean_q: 97.000595, mean_eps: 0.480710
 157974/300000: episode: 1420, duration: 0.777s, episode steps: 113, steps per second: 145, episode reward: -60.202, mean reward: -0.533 [-100.000, 17.192], mean action: 1.673 [0.000, 3.000],  loss: 22.405065, mse: 10882.676218, mean_q: 96.833352, mean_eps: 0.478874
 158419/300000: episode: 1421, duration: 3.269s, episode steps: 445, steps per second: 136, episode reward: -51.003, mean reward: -0.115 [-100.000, 25.648], mean action: 1.746 [0.000, 3.000],  loss: 17.362772, mse: 10791.743081, mean_q: 96.347712, mean_eps: 0.477953
 158547/300000: episode: 1422, duration: 0.906s, episode steps: 128, steps per second: 141, episode reward:  1.246, mean reward:  0.010 [-100.000, 19.358], mean action: 1.664 [0.000, 3.000],  loss: 27.549262, mse: 11000.573242, mean_q: 97.882930, mean_eps: 0.477008
 158650/300000: episode: 1423, duration: 0.803s, episode steps: 103, steps per second: 128, episode reward: -42.428, mean reward: -0.412 [-100.000, 17.464], mean action: 1.845 [0.000, 3.000],  loss: 18.748562, mse: 10906.564183, mean_q: 97.709764, mean_eps: 0.476627
 158965/300000: episode: 1424, duration: 2.306s, episode steps: 315, steps per second: 137, episode reward: -100.635, mean reward: -0.319 [-100.000, 17.592], mean action: 1.711 [0.000, 3.000],  loss: 22.309531, mse: 10740.286097, mean_q: 96.675328, mean_eps: 0.475937
 159107/300000: episode: 1425, duration: 1.036s, episode steps: 142, steps per second: 137, episode reward: -11.987, mean reward: -0.084 [-100.000, 27.427], mean action: 1.648 [0.000, 3.000],  loss: 28.275166, mse: 10857.395663, mean_q: 96.826345, mean_eps: 0.475183
 159222/300000: episode: 1426, duration: 0.785s, episode steps: 115, steps per second: 147, episode reward: -59.242, mean reward: -0.515 [-100.000,  8.590], mean action: 1.609 [0.000, 3.000],  loss: 18.884957, mse: 10962.860228, mean_q: 98.052654, mean_eps: 0.474759
 159373/300000: episode: 1427, duration: 1.076s, episode steps: 151, steps per second: 140, episode reward: -257.005, mean reward: -1.702 [-100.000, 28.868], mean action: 1.709 [0.000, 3.000],  loss: 25.307392, mse: 10560.640292, mean_q: 96.051837, mean_eps: 0.474320
 159606/300000: episode: 1428, duration: 1.624s, episode steps: 233, steps per second: 144, episode reward:  4.253, mean reward:  0.018 [-100.000, 21.751], mean action: 1.747 [0.000, 3.000],  loss: 21.069629, mse: 10823.725611, mean_q: 96.701256, mean_eps: 0.473686
 159719/300000: episode: 1429, duration: 0.819s, episode steps: 113, steps per second: 138, episode reward: -79.422, mean reward: -0.703 [-100.000, 10.500], mean action: 1.708 [0.000, 3.000],  loss: 24.776610, mse: 10774.300284, mean_q: 96.514098, mean_eps: 0.473115
 160719/300000: episode: 1430, duration: 7.860s, episode steps: 1000, steps per second: 127, episode reward: 97.268, mean reward:  0.097 [-22.149, 23.249], mean action: 1.192 [0.000, 3.000],  loss: 24.052280, mse: 10803.875469, mean_q: 96.874654, mean_eps: 0.471279
 160921/300000: episode: 1431, duration: 1.446s, episode steps: 202, steps per second: 140, episode reward: 82.130, mean reward:  0.407 [-100.000, 19.006], mean action: 1.673 [0.000, 3.000],  loss: 26.250800, mse: 10831.717979, mean_q: 97.165570, mean_eps: 0.469296
 161153/300000: episode: 1432, duration: 1.628s, episode steps: 232, steps per second: 143, episode reward: -353.508, mean reward: -1.524 [-100.000, 24.686], mean action: 1.728 [0.000, 3.000],  loss: 24.020766, mse: 10739.296534, mean_q: 96.833398, mean_eps: 0.468580
 162153/300000: episode: 1433, duration: 7.765s, episode steps: 1000, steps per second: 129, episode reward: 101.399, mean reward:  0.101 [-22.918, 24.401], mean action: 1.023 [0.000, 3.000],  loss: 24.287803, mse: 10604.148512, mean_q: 96.647078, mean_eps: 0.466547
 162314/300000: episode: 1434, duration: 1.230s, episode steps: 161, steps per second: 131, episode reward: -86.399, mean reward: -0.537 [-100.000,  4.657], mean action: 1.702 [0.000, 3.000],  loss: 24.679993, mse: 10702.342228, mean_q: 98.023945, mean_eps: 0.464631
 163314/300000: episode: 1435, duration: 7.632s, episode steps: 1000, steps per second: 131, episode reward: 29.478, mean reward:  0.029 [-22.659, 23.095], mean action: 1.121 [0.000, 3.000],  loss: 22.427445, mse: 10474.245223, mean_q: 96.739539, mean_eps: 0.462715
 163444/300000: episode: 1436, duration: 0.913s, episode steps: 130, steps per second: 142, episode reward: -168.275, mean reward: -1.294 [-100.000,  2.228], mean action: 1.800 [0.000, 3.000],  loss: 21.502584, mse: 10378.364592, mean_q: 96.234416, mean_eps: 0.460851
 163634/300000: episode: 1437, duration: 1.342s, episode steps: 190, steps per second: 142, episode reward:  9.760, mean reward:  0.051 [-100.000,  9.193], mean action: 1.689 [0.000, 3.000],  loss: 23.008299, mse: 10570.325563, mean_q: 97.804073, mean_eps: 0.460323
 163709/300000: episode: 1438, duration: 0.535s, episode steps:  75, steps per second: 140, episode reward: -55.773, mean reward: -0.744 [-100.000,  9.464], mean action: 1.787 [0.000, 3.000],  loss: 26.798999, mse: 10452.676003, mean_q: 96.812791, mean_eps: 0.459886
 163860/300000: episode: 1439, duration: 1.040s, episode steps: 151, steps per second: 145, episode reward: -62.085, mean reward: -0.411 [-100.000, 12.797], mean action: 1.642 [0.000, 3.000],  loss: 22.719394, mse: 10417.196331, mean_q: 96.906643, mean_eps: 0.459513
 164305/300000: episode: 1440, duration: 3.347s, episode steps: 445, steps per second: 133, episode reward: -197.723, mean reward: -0.444 [-100.000, 43.855], mean action: 1.461 [0.000, 3.000],  loss: 24.030933, mse: 10465.160371, mean_q: 98.171706, mean_eps: 0.458529
 164462/300000: episode: 1441, duration: 1.076s, episode steps: 157, steps per second: 146, episode reward: -79.265, mean reward: -0.505 [-100.000, 27.227], mean action: 1.669 [0.000, 3.000],  loss: 21.706526, mse: 10464.448429, mean_q: 98.323160, mean_eps: 0.457536
 164568/300000: episode: 1442, duration: 0.776s, episode steps: 106, steps per second: 137, episode reward: 22.243, mean reward:  0.210 [-100.000, 18.720], mean action: 1.717 [0.000, 3.000],  loss: 32.272560, mse: 10454.677417, mean_q: 98.239833, mean_eps: 0.457102
 164676/300000: episode: 1443, duration: 0.751s, episode steps: 108, steps per second: 144, episode reward: -97.129, mean reward: -0.899 [-100.000, 11.589], mean action: 1.778 [0.000, 3.000],  loss: 35.147892, mse: 10399.019061, mean_q: 97.385863, mean_eps: 0.456749
 165676/300000: episode: 1444, duration: 7.665s, episode steps: 1000, steps per second: 130, episode reward: 138.331, mean reward:  0.138 [-22.695, 23.142], mean action: 1.205 [0.000, 3.000],  loss: 21.745845, mse: 10593.720546, mean_q: 99.041122, mean_eps: 0.454921
 165908/300000: episode: 1445, duration: 1.663s, episode steps: 232, steps per second: 140, episode reward: -38.356, mean reward: -0.165 [-100.000,  8.710], mean action: 1.823 [0.000, 3.000],  loss: 20.359499, mse: 10575.494604, mean_q: 99.368467, mean_eps: 0.452888
 166024/300000: episode: 1446, duration: 0.858s, episode steps: 116, steps per second: 135, episode reward: -38.829, mean reward: -0.335 [-100.000, 16.756], mean action: 1.750 [0.000, 3.000],  loss: 25.250999, mse: 10238.458993, mean_q: 98.545897, mean_eps: 0.452314
 166153/300000: episode: 1447, duration: 0.907s, episode steps: 129, steps per second: 142, episode reward: -30.328, mean reward: -0.235 [-100.000, 11.098], mean action: 1.752 [0.000, 3.000],  loss: 22.195613, mse: 10703.447364, mean_q: 100.659203, mean_eps: 0.451910
 167153/300000: episode: 1448, duration: 8.071s, episode steps: 1000, steps per second: 124, episode reward: 125.240, mean reward:  0.125 [-21.700, 23.139], mean action: 1.190 [0.000, 3.000],  loss: 22.398008, mse: 10656.942035, mean_q: 100.225111, mean_eps: 0.450047
 167255/300000: episode: 1449, duration: 0.698s, episode steps: 102, steps per second: 146, episode reward: -122.771, mean reward: -1.204 [-100.000, 12.463], mean action: 1.784 [0.000, 3.000],  loss: 16.315930, mse: 10728.002633, mean_q: 101.231855, mean_eps: 0.448228
 167393/300000: episode: 1450, duration: 0.988s, episode steps: 138, steps per second: 140, episode reward:  9.601, mean reward:  0.070 [-100.000, 13.945], mean action: 1.739 [0.000, 3.000],  loss: 24.421852, mse: 10579.179496, mean_q: 100.334326, mean_eps: 0.447832
 167504/300000: episode: 1451, duration: 0.783s, episode steps: 111, steps per second: 142, episode reward: -67.968, mean reward: -0.612 [-100.000, 12.042], mean action: 1.865 [0.000, 3.000],  loss: 28.919880, mse: 10512.645270, mean_q: 100.525824, mean_eps: 0.447422
 167645/300000: episode: 1452, duration: 0.974s, episode steps: 141, steps per second: 145, episode reward: -15.201, mean reward: -0.108 [-100.000, 15.273], mean action: 1.638 [0.000, 3.000],  loss: 15.336025, mse: 10766.617665, mean_q: 101.867018, mean_eps: 0.447006
 167804/300000: episode: 1453, duration: 1.154s, episode steps: 159, steps per second: 138, episode reward:  3.341, mean reward:  0.021 [-100.000, 22.282], mean action: 1.597 [0.000, 3.000],  loss: 24.244039, mse: 10532.974204, mean_q: 99.921181, mean_eps: 0.446511
 167987/300000: episode: 1454, duration: 1.302s, episode steps: 183, steps per second: 141, episode reward: -215.328, mean reward: -1.177 [-100.000,  6.820], mean action: 1.727 [0.000, 3.000],  loss: 21.515384, mse: 10662.328632, mean_q: 100.736778, mean_eps: 0.445946
 168114/300000: episode: 1455, duration: 0.878s, episode steps: 127, steps per second: 145, episode reward: -64.964, mean reward: -0.512 [-100.000, 10.923], mean action: 1.795 [0.000, 3.000],  loss: 18.268324, mse: 10874.138799, mean_q: 102.458883, mean_eps: 0.445435
 168275/300000: episode: 1456, duration: 1.139s, episode steps: 161, steps per second: 141, episode reward: -33.437, mean reward: -0.208 [-100.000, 18.155], mean action: 1.640 [0.000, 3.000],  loss: 24.081654, mse: 10608.412795, mean_q: 100.847280, mean_eps: 0.444960
 168420/300000: episode: 1457, duration: 1.028s, episode steps: 145, steps per second: 141, episode reward: -40.982, mean reward: -0.283 [-100.000, 14.015], mean action: 1.607 [0.000, 3.000],  loss: 20.712838, mse: 10921.891797, mean_q: 102.038601, mean_eps: 0.444455
 168570/300000: episode: 1458, duration: 1.037s, episode steps: 150, steps per second: 145, episode reward: -205.846, mean reward: -1.372 [-100.000, 10.676], mean action: 1.807 [0.000, 3.000],  loss: 22.603758, mse: 10872.989834, mean_q: 101.473311, mean_eps: 0.443968
 169570/300000: episode: 1459, duration: 8.246s, episode steps: 1000, steps per second: 121, episode reward: 23.276, mean reward:  0.023 [-24.669, 27.969], mean action: 1.798 [0.000, 3.000],  loss: 23.208368, mse: 10960.050171, mean_q: 103.067872, mean_eps: 0.442071
 169663/300000: episode: 1460, duration: 0.693s, episode steps:  93, steps per second: 134, episode reward: -40.363, mean reward: -0.434 [-100.000,  9.463], mean action: 1.828 [0.000, 3.000],  loss: 21.665295, mse: 10952.079564, mean_q: 103.219734, mean_eps: 0.440267
 169805/300000: episode: 1461, duration: 0.999s, episode steps: 142, steps per second: 142, episode reward: -31.751, mean reward: -0.224 [-100.000, 18.142], mean action: 1.796 [0.000, 3.000],  loss: 21.168174, mse: 10921.605737, mean_q: 103.215568, mean_eps: 0.439879
 170085/300000: episode: 1462, duration: 1.996s, episode steps: 280, steps per second: 140, episode reward: -272.629, mean reward: -0.974 [-100.000, 41.942], mean action: 1.793 [0.000, 3.000],  loss: 22.415847, mse: 10973.476041, mean_q: 103.101375, mean_eps: 0.439183
 170261/300000: episode: 1463, duration: 1.248s, episode steps: 176, steps per second: 141, episode reward: -355.475, mean reward: -2.020 [-100.000, 100.137], mean action: 1.716 [0.000, 3.000],  loss: 20.129999, mse: 11123.815763, mean_q: 104.259959, mean_eps: 0.438431
 170411/300000: episode: 1464, duration: 1.035s, episode steps: 150, steps per second: 145, episode reward: -19.318, mean reward: -0.129 [-100.000, 18.807], mean action: 1.687 [0.000, 3.000],  loss: 24.284754, mse: 10889.006419, mean_q: 102.859423, mean_eps: 0.437893
 170604/300000: episode: 1465, duration: 1.727s, episode steps: 193, steps per second: 112, episode reward: -253.680, mean reward: -1.314 [-100.000, 12.269], mean action: 1.720 [0.000, 3.000],  loss: 18.528054, mse: 11023.627307, mean_q: 103.401724, mean_eps: 0.437327
 170775/300000: episode: 1466, duration: 1.331s, episode steps: 171, steps per second: 128, episode reward: -7.607, mean reward: -0.044 [-100.000, 32.647], mean action: 1.626 [0.000, 3.000],  loss: 25.504750, mse: 11098.809191, mean_q: 103.936442, mean_eps: 0.436726
 170928/300000: episode: 1467, duration: 1.193s, episode steps: 153, steps per second: 128, episode reward:  2.761, mean reward:  0.018 [-100.000, 15.977], mean action: 1.556 [0.000, 3.000],  loss: 30.956428, mse: 10969.019203, mean_q: 103.786888, mean_eps: 0.436192
 171928/300000: episode: 1468, duration: 7.651s, episode steps: 1000, steps per second: 131, episode reward: 99.917, mean reward:  0.100 [-20.968, 24.304], mean action: 1.267 [0.000, 3.000],  loss: 23.977888, mse: 10997.318771, mean_q: 104.341340, mean_eps: 0.434289
 172047/300000: episode: 1469, duration: 0.886s, episode steps: 119, steps per second: 134, episode reward: -288.773, mean reward: -2.427 [-100.000,  3.812], mean action: 1.622 [0.000, 3.000],  loss: 20.381957, mse: 10994.715074, mean_q: 104.353281, mean_eps: 0.432443
 172172/300000: episode: 1470, duration: 0.887s, episode steps: 125, steps per second: 141, episode reward: -51.712, mean reward: -0.414 [-100.000, 11.023], mean action: 1.816 [0.000, 3.000],  loss: 24.171409, mse: 11031.868957, mean_q: 104.733382, mean_eps: 0.432040
 172258/300000: episode: 1471, duration: 0.649s, episode steps:  86, steps per second: 132, episode reward: -404.466, mean reward: -4.703 [-100.000,  1.532], mean action: 1.872 [0.000, 3.000],  loss: 25.896601, mse: 11297.032692, mean_q: 106.043754, mean_eps: 0.431692
 172596/300000: episode: 1472, duration: 2.453s, episode steps: 338, steps per second: 138, episode reward: -147.798, mean reward: -0.437 [-100.000,  6.087], mean action: 1.769 [0.000, 3.000],  loss: 24.990530, mse: 11301.470738, mean_q: 105.710074, mean_eps: 0.430993
 172869/300000: episode: 1473, duration: 1.942s, episode steps: 273, steps per second: 141, episode reward: -416.120, mean reward: -1.524 [-100.000,  4.806], mean action: 1.711 [0.000, 3.000],  loss: 37.472925, mse: 11282.184733, mean_q: 105.582169, mean_eps: 0.429984
 173869/300000: episode: 1474, duration: 8.121s, episode steps: 1000, steps per second: 123, episode reward: 118.970, mean reward:  0.119 [-21.221, 23.707], mean action: 1.276 [0.000, 3.000],  loss: 25.695381, mse: 11441.685211, mean_q: 106.861274, mean_eps: 0.427884
 174086/300000: episode: 1475, duration: 1.705s, episode steps: 217, steps per second: 127, episode reward: -213.728, mean reward: -0.985 [-100.000,  4.475], mean action: 1.654 [0.000, 3.000],  loss: 25.223474, mse: 11429.580490, mean_q: 106.811671, mean_eps: 0.425876
 174302/300000: episode: 1476, duration: 1.775s, episode steps: 216, steps per second: 122, episode reward: -241.914, mean reward: -1.120 [-100.000, 10.943], mean action: 1.713 [0.000, 3.000],  loss: 26.648568, mse: 11499.485892, mean_q: 107.428073, mean_eps: 0.425161
 174477/300000: episode: 1477, duration: 1.378s, episode steps: 175, steps per second: 127, episode reward: -115.772, mean reward: -0.662 [-100.000,  5.655], mean action: 1.829 [0.000, 3.000],  loss: 27.386215, mse: 11564.486956, mean_q: 107.804118, mean_eps: 0.424516
 175477/300000: episode: 1478, duration: 7.649s, episode steps: 1000, steps per second: 131, episode reward: 158.734, mean reward:  0.159 [-23.772, 23.317], mean action: 0.871 [0.000, 3.000],  loss: 25.965482, mse: 11388.172974, mean_q: 107.250662, mean_eps: 0.422578
 176477/300000: episode: 1479, duration: 7.884s, episode steps: 1000, steps per second: 127, episode reward: 98.787, mean reward:  0.099 [-24.301, 23.167], mean action: 0.941 [0.000, 3.000],  loss: 25.195389, mse: 11384.868074, mean_q: 107.997404, mean_eps: 0.419278
 176780/300000: episode: 1480, duration: 2.181s, episode steps: 303, steps per second: 139, episode reward: -246.393, mean reward: -0.813 [-100.000, 22.167], mean action: 1.657 [0.000, 3.000],  loss: 27.707335, mse: 11363.028388, mean_q: 107.848398, mean_eps: 0.417128
 177016/300000: episode: 1481, duration: 1.688s, episode steps: 236, steps per second: 140, episode reward: -96.995, mean reward: -0.411 [-100.000,  8.645], mean action: 1.839 [0.000, 3.000],  loss: 25.916359, mse: 11194.155838, mean_q: 106.593636, mean_eps: 0.416238
 177115/300000: episode: 1482, duration: 0.719s, episode steps:  99, steps per second: 138, episode reward: -385.423, mean reward: -3.893 [-100.000, 13.895], mean action: 1.788 [0.000, 3.000],  loss: 24.118264, mse: 11218.837743, mean_q: 106.555783, mean_eps: 0.415685
 177215/300000: episode: 1483, duration: 0.832s, episode steps: 100, steps per second: 120, episode reward: -395.496, mean reward: -3.955 [-100.000,  0.816], mean action: 1.720 [0.000, 3.000],  loss: 41.332645, mse: 11544.347192, mean_q: 108.149923, mean_eps: 0.415357
 177353/300000: episode: 1484, duration: 1.120s, episode steps: 138, steps per second: 123, episode reward: -128.206, mean reward: -0.929 [-100.000, 16.148], mean action: 1.768 [0.000, 3.000],  loss: 26.102231, mse: 11360.315971, mean_q: 107.360011, mean_eps: 0.414964
 178353/300000: episode: 1485, duration: 7.904s, episode steps: 1000, steps per second: 127, episode reward: 134.239, mean reward:  0.134 [-23.204, 24.798], mean action: 1.205 [0.000, 3.000],  loss: 25.004699, mse: 11433.820069, mean_q: 107.698564, mean_eps: 0.413087
 178438/300000: episode: 1486, duration: 0.622s, episode steps:  85, steps per second: 137, episode reward: -271.446, mean reward: -3.193 [-100.000,  1.947], mean action: 1.894 [0.000, 3.000],  loss: 25.756686, mse: 11320.101631, mean_q: 106.929855, mean_eps: 0.411296
 178597/300000: episode: 1487, duration: 1.117s, episode steps: 159, steps per second: 142, episode reward: -5.712, mean reward: -0.036 [-100.000, 15.395], mean action: 1.748 [0.000, 3.000],  loss: 24.195311, mse: 11239.878642, mean_q: 106.420200, mean_eps: 0.410894
 178802/300000: episode: 1488, duration: 1.460s, episode steps: 205, steps per second: 140, episode reward: -172.817, mean reward: -0.843 [-100.000,  3.851], mean action: 1.795 [0.000, 3.000],  loss: 22.410097, mse: 11239.990687, mean_q: 106.774453, mean_eps: 0.410293
 178898/300000: episode: 1489, duration: 0.688s, episode steps:  96, steps per second: 140, episode reward: -34.501, mean reward: -0.359 [-100.000,  7.774], mean action: 2.010 [0.000, 3.000],  loss: 27.795070, mse: 11123.106028, mean_q: 106.866177, mean_eps: 0.409797
 179246/300000: episode: 1490, duration: 2.972s, episode steps: 348, steps per second: 117, episode reward: -293.858, mean reward: -0.844 [-100.000,  7.211], mean action: 1.747 [0.000, 3.000],  loss: 22.411811, mse: 11213.883653, mean_q: 107.098054, mean_eps: 0.409064
 179399/300000: episode: 1491, duration: 1.308s, episode steps: 153, steps per second: 117, episode reward: -197.800, mean reward: -1.293 [-100.000,  2.969], mean action: 1.824 [0.000, 3.000],  loss: 22.554515, mse: 11294.010404, mean_q: 106.815745, mean_eps: 0.408237
 179496/300000: episode: 1492, duration: 0.813s, episode steps:  97, steps per second: 119, episode reward: -263.111, mean reward: -2.712 [-100.000,  1.067], mean action: 1.763 [0.000, 3.000],  loss: 28.569334, mse: 11178.034210, mean_q: 105.909126, mean_eps: 0.407825
 180496/300000: episode: 1493, duration: 7.917s, episode steps: 1000, steps per second: 126, episode reward: 60.145, mean reward:  0.060 [-24.502, 22.791], mean action: 1.385 [0.000, 3.000],  loss: 23.773184, mse: 11410.194656, mean_q: 107.690787, mean_eps: 0.406015
 180589/300000: episode: 1494, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: -53.006, mean reward: -0.570 [-100.000, 13.894], mean action: 1.688 [0.000, 3.000],  loss: 20.105692, mse: 11391.569619, mean_q: 107.928587, mean_eps: 0.404211
 181589/300000: episode: 1495, duration: 7.634s, episode steps: 1000, steps per second: 131, episode reward: 143.592, mean reward:  0.144 [-19.577, 42.195], mean action: 0.952 [0.000, 3.000],  loss: 21.099734, mse: 11543.459370, mean_q: 108.040990, mean_eps: 0.402408
 181695/300000: episode: 1496, duration: 0.718s, episode steps: 106, steps per second: 148, episode reward: -36.381, mean reward: -0.343 [-100.000, 15.689], mean action: 1.755 [0.000, 3.000],  loss: 19.750773, mse: 11502.266896, mean_q: 107.064173, mean_eps: 0.400583
 182021/300000: episode: 1497, duration: 2.339s, episode steps: 326, steps per second: 139, episode reward: -310.214, mean reward: -0.952 [-100.000,  9.682], mean action: 1.764 [0.000, 3.000],  loss: 22.401885, mse: 11447.271794, mean_q: 107.523035, mean_eps: 0.399870
 182324/300000: episode: 1498, duration: 2.134s, episode steps: 303, steps per second: 142, episode reward: -286.502, mean reward: -0.946 [-100.000, 19.704], mean action: 1.871 [0.000, 3.000],  loss: 21.330119, mse: 11330.596370, mean_q: 106.806057, mean_eps: 0.398832
 182479/300000: episode: 1499, duration: 1.113s, episode steps: 155, steps per second: 139, episode reward: -171.615, mean reward: -1.107 [-100.000, 35.808], mean action: 1.871 [0.000, 3.000],  loss: 20.948777, mse: 11270.678790, mean_q: 107.018820, mean_eps: 0.398077
 182566/300000: episode: 1500, duration: 0.604s, episode steps:  87, steps per second: 144, episode reward: -592.192, mean reward: -6.807 [-100.000, -1.666], mean action: 1.425 [0.000, 3.000],  loss: 31.489523, mse: 11207.232658, mean_q: 105.674042, mean_eps: 0.397677
 182944/300000: episode: 1501, duration: 2.722s, episode steps: 378, steps per second: 139, episode reward: -316.079, mean reward: -0.836 [-100.000, 11.148], mean action: 1.696 [0.000, 3.000],  loss: 21.700667, mse: 11396.689281, mean_q: 106.869990, mean_eps: 0.396910
 183144/300000: episode: 1502, duration: 1.406s, episode steps: 200, steps per second: 142, episode reward: -143.422, mean reward: -0.717 [-100.000, 28.145], mean action: 1.890 [0.000, 3.000],  loss: 23.573698, mse: 11223.938396, mean_q: 106.417042, mean_eps: 0.395956
 183392/300000: episode: 1503, duration: 1.859s, episode steps: 248, steps per second: 133, episode reward: -124.130, mean reward: -0.501 [-100.000,  5.469], mean action: 1.859 [0.000, 3.000],  loss: 23.109451, mse: 11146.140229, mean_q: 105.847226, mean_eps: 0.395217
 183628/300000: episode: 1504, duration: 1.688s, episode steps: 236, steps per second: 140, episode reward: -97.958, mean reward: -0.415 [-100.000, 21.141], mean action: 1.860 [0.000, 3.000],  loss: 23.815840, mse: 11356.915633, mean_q: 106.486129, mean_eps: 0.394419
 184624/300000: episode: 1505, duration: 7.981s, episode steps: 996, steps per second: 125, episode reward: -386.574, mean reward: -0.388 [-100.000, 38.263], mean action: 1.735 [0.000, 3.000],  loss: 22.085449, mse: 11597.780703, mean_q: 107.660298, mean_eps: 0.392386
 184692/300000: episode: 1506, duration: 0.502s, episode steps:  68, steps per second: 135, episode reward: -29.287, mean reward: -0.431 [-100.000, 12.687], mean action: 1.515 [0.000, 3.000],  loss: 19.055668, mse: 11487.946504, mean_q: 107.908164, mean_eps: 0.390630
 184859/300000: episode: 1507, duration: 1.156s, episode steps: 167, steps per second: 144, episode reward: -157.013, mean reward: -0.940 [-100.000, 12.896], mean action: 1.886 [0.000, 3.000],  loss: 24.060514, mse: 11561.434760, mean_q: 107.621286, mean_eps: 0.390242
 184981/300000: episode: 1508, duration: 0.886s, episode steps: 122, steps per second: 138, episode reward: -36.931, mean reward: -0.303 [-100.000, 12.374], mean action: 1.820 [0.000, 3.000],  loss: 24.811631, mse: 11577.983118, mean_q: 107.310243, mean_eps: 0.389766
 185097/300000: episode: 1509, duration: 0.809s, episode steps: 116, steps per second: 143, episode reward: -19.671, mean reward: -0.170 [-100.000,  7.738], mean action: 1.612 [0.000, 3.000],  loss: 17.058952, mse: 11464.844600, mean_q: 107.291351, mean_eps: 0.389373
 185256/300000: episode: 1510, duration: 1.135s, episode steps: 159, steps per second: 140, episode reward: -346.460, mean reward: -2.179 [-100.000,  2.574], mean action: 1.698 [0.000, 3.000],  loss: 25.337085, mse: 11448.437088, mean_q: 106.560061, mean_eps: 0.388919
 186256/300000: episode: 1511, duration: 8.131s, episode steps: 1000, steps per second: 123, episode reward: 60.587, mean reward:  0.061 [-23.189, 23.601], mean action: 1.349 [0.000, 3.000],  loss: 23.621027, mse: 11682.428684, mean_q: 108.181552, mean_eps: 0.387007
 186365/300000: episode: 1512, duration: 0.787s, episode steps: 109, steps per second: 139, episode reward:  6.087, mean reward:  0.056 [-100.000, 21.142], mean action: 1.798 [0.000, 3.000],  loss: 18.711614, mse: 11811.460946, mean_q: 108.606429, mean_eps: 0.385177
 187365/300000: episode: 1513, duration: 8.107s, episode steps: 1000, steps per second: 123, episode reward: -23.301, mean reward: -0.023 [-21.438, 20.320], mean action: 1.484 [0.000, 3.000],  loss: 25.487804, mse: 11742.122792, mean_q: 107.993220, mean_eps: 0.383347
 187626/300000: episode: 1514, duration: 1.990s, episode steps: 261, steps per second: 131, episode reward: -245.098, mean reward: -0.939 [-100.000,  6.293], mean action: 1.824 [0.000, 3.000],  loss: 25.334212, mse: 11730.423244, mean_q: 107.697417, mean_eps: 0.381266
 188626/300000: episode: 1515, duration: 8.095s, episode steps: 1000, steps per second: 124, episode reward: 81.205, mean reward:  0.081 [-25.765, 22.337], mean action: 1.309 [0.000, 3.000],  loss: 20.979205, mse: 11822.230465, mean_q: 108.243035, mean_eps: 0.379186
 189626/300000: episode: 1516, duration: 7.883s, episode steps: 1000, steps per second: 127, episode reward: 60.528, mean reward:  0.061 [-23.548, 22.703], mean action: 1.399 [0.000, 3.000],  loss: 22.765856, mse: 11930.211631, mean_q: 108.908026, mean_eps: 0.375886
 189786/300000: episode: 1517, duration: 1.104s, episode steps: 160, steps per second: 145, episode reward: -177.534, mean reward: -1.110 [-100.000, 21.100], mean action: 1.781 [0.000, 3.000],  loss: 27.003603, mse: 11491.059509, mean_q: 106.924774, mean_eps: 0.373972
 190017/300000: episode: 1518, duration: 1.633s, episode steps: 231, steps per second: 141, episode reward: -239.538, mean reward: -1.037 [-100.000, 15.989], mean action: 1.602 [0.000, 3.000],  loss: 22.674282, mse: 11915.974573, mean_q: 108.682421, mean_eps: 0.373327
 190208/300000: episode: 1519, duration: 1.328s, episode steps: 191, steps per second: 144, episode reward: -219.529, mean reward: -1.149 [-100.000,  8.906], mean action: 1.843 [0.000, 3.000],  loss: 15.920988, mse: 11945.868686, mean_q: 109.272930, mean_eps: 0.372630
 190458/300000: episode: 1520, duration: 1.742s, episode steps: 250, steps per second: 144, episode reward: -278.901, mean reward: -1.116 [-100.000, 17.786], mean action: 1.836 [0.000, 3.000],  loss: 22.433619, mse: 11780.344770, mean_q: 107.993677, mean_eps: 0.371903
 190720/300000: episode: 1521, duration: 1.892s, episode steps: 262, steps per second: 138, episode reward: -19.383, mean reward: -0.074 [-100.000, 15.895], mean action: 1.763 [0.000, 3.000],  loss: 28.635298, mse: 12017.667238, mean_q: 108.873488, mean_eps: 0.371058
 190920/300000: episode: 1522, duration: 1.412s, episode steps: 200, steps per second: 142, episode reward: -226.335, mean reward: -1.132 [-100.000,  4.977], mean action: 1.755 [0.000, 3.000],  loss: 20.571958, mse: 11835.220393, mean_q: 109.034493, mean_eps: 0.370296
 191327/300000: episode: 1523, duration: 2.920s, episode steps: 407, steps per second: 139, episode reward: -242.349, mean reward: -0.595 [-100.000, 39.340], mean action: 1.671 [0.000, 3.000],  loss: 23.897257, mse: 11941.688816, mean_q: 109.662491, mean_eps: 0.369294
 191506/300000: episode: 1524, duration: 1.283s, episode steps: 179, steps per second: 140, episode reward: -138.842, mean reward: -0.776 [-100.000,  9.802], mean action: 1.821 [0.000, 3.000],  loss: 24.970125, mse: 11815.271348, mean_q: 108.836788, mean_eps: 0.368327
 192141/300000: episode: 1525, duration: 4.897s, episode steps: 635, steps per second: 130, episode reward: 170.175, mean reward:  0.268 [-20.265, 100.000], mean action: 1.479 [0.000, 3.000],  loss: 20.951699, mse: 11739.530789, mean_q: 108.537821, mean_eps: 0.366984
 192337/300000: episode: 1526, duration: 1.401s, episode steps: 196, steps per second: 140, episode reward: -151.132, mean reward: -0.771 [-100.000,  3.543], mean action: 1.796 [0.000, 3.000],  loss: 19.411654, mse: 11359.436538, mean_q: 107.105014, mean_eps: 0.365613
 192570/300000: episode: 1527, duration: 1.623s, episode steps: 233, steps per second: 144, episode reward: -285.359, mean reward: -1.225 [-100.000,  5.634], mean action: 1.755 [0.000, 3.000],  loss: 19.617612, mse: 11596.317257, mean_q: 107.857906, mean_eps: 0.364905
 192828/300000: episode: 1528, duration: 1.821s, episode steps: 258, steps per second: 142, episode reward: -97.751, mean reward: -0.379 [-100.000,  9.896], mean action: 1.806 [0.000, 3.000],  loss: 23.009001, mse: 11680.004268, mean_q: 107.939809, mean_eps: 0.364095
 192980/300000: episode: 1529, duration: 1.105s, episode steps: 152, steps per second: 138, episode reward: -264.394, mean reward: -1.739 [-100.000,  8.655], mean action: 1.954 [0.000, 3.000],  loss: 21.538462, mse: 11738.172614, mean_q: 108.305626, mean_eps: 0.363418
 193156/300000: episode: 1530, duration: 1.210s, episode steps: 176, steps per second: 145, episode reward: -134.571, mean reward: -0.765 [-100.000,  3.596], mean action: 1.869 [0.000, 3.000],  loss: 24.543777, mse: 12077.819075, mean_q: 109.202818, mean_eps: 0.362877
 193354/300000: episode: 1531, duration: 1.405s, episode steps: 198, steps per second: 141, episode reward: -254.921, mean reward: -1.287 [-100.000,  3.669], mean action: 1.854 [0.000, 3.000],  loss: 23.848124, mse: 12042.751832, mean_q: 109.310579, mean_eps: 0.362260
 193601/300000: episode: 1532, duration: 1.742s, episode steps: 247, steps per second: 142, episode reward: -271.390, mean reward: -1.099 [-100.000,  7.439], mean action: 1.652 [0.000, 3.000],  loss: 25.563996, mse: 11994.369524, mean_q: 109.149808, mean_eps: 0.361526
 193779/300000: episode: 1533, duration: 1.231s, episode steps: 178, steps per second: 145, episode reward: -96.941, mean reward: -0.545 [-100.000,  5.515], mean action: 1.815 [0.000, 3.000],  loss: 33.012615, mse: 12107.700700, mean_q: 109.105917, mean_eps: 0.360825
 194039/300000: episode: 1534, duration: 1.844s, episode steps: 260, steps per second: 141, episode reward: -478.895, mean reward: -1.842 [-100.000,  3.961], mean action: 1.808 [0.000, 3.000],  loss: 24.150493, mse: 12323.534961, mean_q: 109.916549, mean_eps: 0.360102
 195039/300000: episode: 1535, duration: 8.310s, episode steps: 1000, steps per second: 120, episode reward: 110.229, mean reward:  0.110 [-22.119, 22.654], mean action: 1.102 [0.000, 3.000],  loss: 23.804115, mse: 12251.950514, mean_q: 109.167954, mean_eps: 0.358023
 195176/300000: episode: 1536, duration: 0.990s, episode steps: 137, steps per second: 138, episode reward: -229.153, mean reward: -1.673 [-100.000, 35.940], mean action: 1.701 [0.000, 3.000],  loss: 28.454962, mse: 11808.226117, mean_q: 106.833254, mean_eps: 0.356147
 195355/300000: episode: 1537, duration: 1.256s, episode steps: 179, steps per second: 143, episode reward: -194.347, mean reward: -1.086 [-100.000,  4.398], mean action: 1.771 [0.000, 3.000],  loss: 23.765302, mse: 11968.076862, mean_q: 107.267514, mean_eps: 0.355625
 195462/300000: episode: 1538, duration: 0.743s, episode steps: 107, steps per second: 144, episode reward: -139.842, mean reward: -1.307 [-100.000,  2.118], mean action: 1.972 [0.000, 3.000],  loss: 26.891459, mse: 12047.957342, mean_q: 107.580951, mean_eps: 0.355154
 196462/300000: episode: 1539, duration: 7.800s, episode steps: 1000, steps per second: 128, episode reward: 62.514, mean reward:  0.063 [-22.414, 18.420], mean action: 1.328 [0.000, 3.000],  loss: 23.806093, mse: 12094.401830, mean_q: 108.171648, mean_eps: 0.353327
 196630/300000: episode: 1540, duration: 1.322s, episode steps: 168, steps per second: 127, episode reward: -112.916, mean reward: -0.672 [-100.000, 16.745], mean action: 1.810 [0.000, 3.000],  loss: 22.938339, mse: 12002.823155, mean_q: 107.747209, mean_eps: 0.351400
 196816/300000: episode: 1541, duration: 1.426s, episode steps: 186, steps per second: 130, episode reward: -209.033, mean reward: -1.124 [-100.000, 37.161], mean action: 1.801 [0.000, 3.000],  loss: 27.664333, mse: 12085.611688, mean_q: 108.151539, mean_eps: 0.350816
 197217/300000: episode: 1542, duration: 3.120s, episode steps: 401, steps per second: 129, episode reward: -64.069, mean reward: -0.160 [-100.000,  8.844], mean action: 1.793 [0.000, 3.000],  loss: 20.546491, mse: 12314.698702, mean_q: 109.420706, mean_eps: 0.349847
 197641/300000: episode: 1543, duration: 3.015s, episode steps: 424, steps per second: 141, episode reward: -136.524, mean reward: -0.322 [-100.000,  8.652], mean action: 1.698 [0.000, 3.000],  loss: 23.979664, mse: 12323.562537, mean_q: 109.499964, mean_eps: 0.348486
 197840/300000: episode: 1544, duration: 1.425s, episode steps: 199, steps per second: 140, episode reward: -148.883, mean reward: -0.748 [-100.000, 15.497], mean action: 1.874 [0.000, 3.000],  loss: 21.408445, mse: 12416.502966, mean_q: 109.758908, mean_eps: 0.347458
 198012/300000: episode: 1545, duration: 1.182s, episode steps: 172, steps per second: 146, episode reward: -29.073, mean reward: -0.169 [-100.000, 11.956], mean action: 1.779 [0.000, 3.000],  loss: 20.359056, mse: 12125.311339, mean_q: 108.499087, mean_eps: 0.346846
 198737/300000: episode: 1546, duration: 5.414s, episode steps: 725, steps per second: 134, episode reward: 234.712, mean reward:  0.324 [-19.052, 100.000], mean action: 1.274 [0.000, 3.000],  loss: 21.613830, mse: 12183.692016, mean_q: 109.173071, mean_eps: 0.345366
 198947/300000: episode: 1547, duration: 1.444s, episode steps: 210, steps per second: 145, episode reward: -130.166, mean reward: -0.620 [-100.000,  6.165], mean action: 1.743 [0.000, 3.000],  loss: 22.170590, mse: 11863.105480, mean_q: 107.823175, mean_eps: 0.343823
 199139/300000: episode: 1548, duration: 1.368s, episode steps: 192, steps per second: 140, episode reward: -91.690, mean reward: -0.478 [-100.000, 19.676], mean action: 1.766 [0.000, 3.000],  loss: 22.036012, mse: 12099.668381, mean_q: 108.048450, mean_eps: 0.343160
 199315/300000: episode: 1549, duration: 1.252s, episode steps: 176, steps per second: 141, episode reward: -123.579, mean reward: -0.702 [-100.000,  5.557], mean action: 1.875 [0.000, 3.000],  loss: 19.075833, mse: 12136.551247, mean_q: 108.187496, mean_eps: 0.342553
 199516/300000: episode: 1550, duration: 1.384s, episode steps: 201, steps per second: 145, episode reward: -166.043, mean reward: -0.826 [-100.000,  9.986], mean action: 1.746 [0.000, 3.000],  loss: 25.574818, mse: 12054.299914, mean_q: 107.798731, mean_eps: 0.341930
 199880/300000: episode: 1551, duration: 2.714s, episode steps: 364, steps per second: 134, episode reward: -39.303, mean reward: -0.108 [-100.000, 18.185], mean action: 1.659 [0.000, 3.000],  loss: 22.367224, mse: 11915.370647, mean_q: 107.269859, mean_eps: 0.340998
 200880/300000: episode: 1552, duration: 7.492s, episode steps: 1000, steps per second: 133, episode reward: 25.692, mean reward:  0.026 [-24.169, 23.582], mean action: 1.247 [0.000, 3.000],  loss: 23.225561, mse: 12335.320356, mean_q: 109.396134, mean_eps: 0.338748
 201049/300000: episode: 1553, duration: 1.180s, episode steps: 169, steps per second: 143, episode reward: -150.218, mean reward: -0.889 [-100.000,  3.921], mean action: 1.751 [0.000, 3.000],  loss: 17.941305, mse: 11985.573967, mean_q: 108.364006, mean_eps: 0.336819
 201367/300000: episode: 1554, duration: 2.269s, episode steps: 318, steps per second: 140, episode reward: 22.076, mean reward:  0.069 [-100.000, 17.176], mean action: 1.692 [0.000, 3.000],  loss: 24.447689, mse: 12300.852769, mean_q: 109.093367, mean_eps: 0.336015
 201608/300000: episode: 1555, duration: 1.668s, episode steps: 241, steps per second: 144, episode reward: -143.444, mean reward: -0.595 [-100.000,  3.567], mean action: 1.851 [0.000, 3.000],  loss: 20.670326, mse: 12433.121248, mean_q: 109.476837, mean_eps: 0.335093
 201858/300000: episode: 1556, duration: 1.768s, episode steps: 250, steps per second: 141, episode reward: -158.258, mean reward: -0.633 [-100.000, 22.336], mean action: 1.848 [0.000, 3.000],  loss: 22.209799, mse: 12454.727928, mean_q: 109.442982, mean_eps: 0.334283
 202858/300000: episode: 1557, duration: 7.807s, episode steps: 1000, steps per second: 128, episode reward: 120.978, mean reward:  0.121 [-22.050, 22.718], mean action: 1.287 [0.000, 3.000],  loss: 24.577222, mse: 12652.734340, mean_q: 110.215239, mean_eps: 0.332220
 203081/300000: episode: 1558, duration: 1.554s, episode steps: 223, steps per second: 144, episode reward: -176.583, mean reward: -0.792 [-100.000,  7.835], mean action: 1.798 [0.000, 3.000],  loss: 20.414512, mse: 12697.029949, mean_q: 110.455504, mean_eps: 0.330202
 203455/300000: episode: 1559, duration: 2.857s, episode steps: 374, steps per second: 131, episode reward: -224.909, mean reward: -0.601 [-100.000,  3.750], mean action: 1.765 [0.000, 3.000],  loss: 25.796718, mse: 12557.802373, mean_q: 110.060337, mean_eps: 0.329217
 203655/300000: episode: 1560, duration: 1.534s, episode steps: 200, steps per second: 130, episode reward: -243.970, mean reward: -1.220 [-100.000, 29.631], mean action: 1.850 [0.000, 3.000],  loss: 21.099704, mse: 12471.626641, mean_q: 109.423979, mean_eps: 0.328270
 203815/300000: episode: 1561, duration: 1.256s, episode steps: 160, steps per second: 127, episode reward: -122.315, mean reward: -0.764 [-100.000, 36.140], mean action: 1.919 [0.000, 3.000],  loss: 21.987337, mse: 12444.568066, mean_q: 109.192179, mean_eps: 0.327676
 204160/300000: episode: 1562, duration: 2.569s, episode steps: 345, steps per second: 134, episode reward: -200.248, mean reward: -0.580 [-100.000, 25.558], mean action: 1.803 [0.000, 3.000],  loss: 19.739948, mse: 12531.805981, mean_q: 109.491864, mean_eps: 0.326843
 204380/300000: episode: 1563, duration: 1.569s, episode steps: 220, steps per second: 140, episode reward: -352.324, mean reward: -1.601 [-100.000,  3.691], mean action: 1.691 [0.000, 3.000],  loss: 19.492298, mse: 12712.632586, mean_q: 110.337902, mean_eps: 0.325911
 204755/300000: episode: 1564, duration: 2.697s, episode steps: 375, steps per second: 139, episode reward: -169.160, mean reward: -0.451 [-100.000,  6.528], mean action: 1.784 [0.000, 3.000],  loss: 25.215678, mse: 12946.433732, mean_q: 110.870165, mean_eps: 0.324929
 204848/300000: episode: 1565, duration: 0.677s, episode steps:  93, steps per second: 137, episode reward: -182.996, mean reward: -1.968 [-100.000,  2.089], mean action: 1.720 [0.000, 3.000],  loss: 22.200516, mse: 13277.806924, mean_q: 112.148144, mean_eps: 0.324157
 205343/300000: episode: 1566, duration: 3.758s, episode steps: 495, steps per second: 132, episode reward: -130.359, mean reward: -0.263 [-100.000, 19.567], mean action: 1.584 [0.000, 3.000],  loss: 24.380342, mse: 12935.405325, mean_q: 110.833003, mean_eps: 0.323186
 205477/300000: episode: 1567, duration: 1.107s, episode steps: 134, steps per second: 121, episode reward: -99.757, mean reward: -0.744 [-100.000,  7.744], mean action: 1.769 [0.000, 3.000],  loss: 21.535317, mse: 12839.298952, mean_q: 109.700495, mean_eps: 0.322149
 205805/300000: episode: 1568, duration: 2.610s, episode steps: 328, steps per second: 126, episode reward: -222.403, mean reward: -0.678 [-100.000,  5.295], mean action: 1.835 [0.000, 3.000],  loss: 19.673274, mse: 13072.967638, mean_q: 111.515547, mean_eps: 0.321386
 206217/300000: episode: 1569, duration: 3.218s, episode steps: 412, steps per second: 128, episode reward: 205.721, mean reward:  0.499 [-20.818, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 23.181719, mse: 12979.063749, mean_q: 110.708147, mean_eps: 0.320165
 206542/300000: episode: 1570, duration: 2.319s, episode steps: 325, steps per second: 140, episode reward: -130.239, mean reward: -0.401 [-100.000, 14.443], mean action: 1.926 [0.000, 3.000],  loss: 20.413305, mse: 12953.889549, mean_q: 111.065642, mean_eps: 0.318949
 206715/300000: episode: 1571, duration: 1.209s, episode steps: 173, steps per second: 143, episode reward: -72.434, mean reward: -0.419 [-100.000, 10.772], mean action: 1.728 [0.000, 3.000],  loss: 18.401488, mse: 13018.454655, mean_q: 111.170188, mean_eps: 0.318128
 206959/300000: episode: 1572, duration: 1.717s, episode steps: 244, steps per second: 142, episode reward: -186.112, mean reward: -0.763 [-100.000,  4.033], mean action: 1.697 [0.000, 3.000],  loss: 19.013541, mse: 13300.898778, mean_q: 112.464589, mean_eps: 0.317440
 207395/300000: episode: 1573, duration: 3.175s, episode steps: 436, steps per second: 137, episode reward: -134.445, mean reward: -0.308 [-100.000, 22.907], mean action: 1.844 [0.000, 3.000],  loss: 23.527388, mse: 13117.549767, mean_q: 111.353869, mean_eps: 0.316318
 207573/300000: episode: 1574, duration: 1.278s, episode steps: 178, steps per second: 139, episode reward: -71.975, mean reward: -0.404 [-100.000,  4.027], mean action: 1.787 [0.000, 3.000],  loss: 18.827859, mse: 13229.535754, mean_q: 111.360042, mean_eps: 0.315304
 208248/300000: episode: 1575, duration: 5.037s, episode steps: 675, steps per second: 134, episode reward: -172.116, mean reward: -0.255 [-100.000, 17.241], mean action: 1.753 [0.000, 3.000],  loss: 20.812703, mse: 13242.111085, mean_q: 111.995971, mean_eps: 0.313897
 208700/300000: episode: 1576, duration: 3.225s, episode steps: 452, steps per second: 140, episode reward: -239.351, mean reward: -0.530 [-100.000, 22.185], mean action: 1.783 [0.000, 3.000],  loss: 22.526275, mse: 13319.829541, mean_q: 112.494070, mean_eps: 0.312037
 208985/300000: episode: 1577, duration: 2.002s, episode steps: 285, steps per second: 142, episode reward: -146.547, mean reward: -0.514 [-100.000,  4.829], mean action: 1.821 [0.000, 3.000],  loss: 24.879832, mse: 13300.388178, mean_q: 112.383725, mean_eps: 0.310821
 209181/300000: episode: 1578, duration: 1.363s, episode steps: 196, steps per second: 144, episode reward: -162.584, mean reward: -0.830 [-100.000, 18.243], mean action: 1.755 [0.000, 3.000],  loss: 25.108236, mse: 13463.864866, mean_q: 113.660022, mean_eps: 0.310028
 209751/300000: episode: 1579, duration: 4.169s, episode steps: 570, steps per second: 137, episode reward: 129.944, mean reward:  0.228 [-17.951, 100.000], mean action: 1.633 [0.000, 3.000],  loss: 18.808367, mse: 13628.839118, mean_q: 114.718290, mean_eps: 0.308764
 209853/300000: episode: 1580, duration: 0.721s, episode steps: 102, steps per second: 141, episode reward:  3.687, mean reward:  0.036 [-100.000, 15.829], mean action: 1.853 [0.000, 3.000],  loss: 22.834720, mse: 13559.277171, mean_q: 114.167921, mean_eps: 0.307655
 210773/300000: episode: 1581, duration: 6.917s, episode steps: 920, steps per second: 133, episode reward: -163.724, mean reward: -0.178 [-100.000, 21.830], mean action: 1.673 [0.000, 3.000],  loss: 20.659088, mse: 13866.803767, mean_q: 115.317748, mean_eps: 0.305969
 211205/300000: episode: 1582, duration: 3.193s, episode steps: 432, steps per second: 135, episode reward: -116.625, mean reward: -0.270 [-100.000, 20.558], mean action: 1.546 [0.000, 3.000],  loss: 29.992124, mse: 13659.570428, mean_q: 114.472725, mean_eps: 0.303738
 211638/300000: episode: 1583, duration: 3.189s, episode steps: 433, steps per second: 136, episode reward: -188.228, mean reward: -0.435 [-100.000,  8.435], mean action: 1.783 [0.000, 3.000],  loss: 21.329489, mse: 13888.422994, mean_q: 115.570473, mean_eps: 0.302311
 212344/300000: episode: 1584, duration: 5.362s, episode steps: 706, steps per second: 132, episode reward: -92.288, mean reward: -0.131 [-100.000, 21.121], mean action: 1.722 [0.000, 3.000],  loss: 20.909235, mse: 13990.041985, mean_q: 116.427652, mean_eps: 0.300431
 213146/300000: episode: 1585, duration: 6.302s, episode steps: 802, steps per second: 127, episode reward: -149.647, mean reward: -0.187 [-100.000, 12.245], mean action: 1.736 [0.000, 3.000],  loss: 21.834812, mse: 14185.890806, mean_q: 117.116020, mean_eps: 0.297943
 214146/300000: episode: 1586, duration: 7.619s, episode steps: 1000, steps per second: 131, episode reward: 84.140, mean reward:  0.084 [-22.560, 22.516], mean action: 1.141 [0.000, 3.000],  loss: 21.314921, mse: 14108.371041, mean_q: 116.225469, mean_eps: 0.294970
 214466/300000: episode: 1587, duration: 2.551s, episode steps: 320, steps per second: 125, episode reward: -201.828, mean reward: -0.631 [-100.000, 32.668], mean action: 1.975 [0.000, 3.000],  loss: 24.310146, mse: 13829.787720, mean_q: 114.926737, mean_eps: 0.292792
 214648/300000: episode: 1588, duration: 1.375s, episode steps: 182, steps per second: 132, episode reward: -149.257, mean reward: -0.820 [-100.000, 12.905], mean action: 1.813 [0.000, 3.000],  loss: 18.613105, mse: 13740.868475, mean_q: 114.364886, mean_eps: 0.291964
 214830/300000: episode: 1589, duration: 1.292s, episode steps: 182, steps per second: 141, episode reward: -77.675, mean reward: -0.427 [-100.000,  9.837], mean action: 1.830 [0.000, 3.000],  loss: 26.513813, mse: 13658.217832, mean_q: 114.005969, mean_eps: 0.291363
 215830/300000: episode: 1590, duration: 7.760s, episode steps: 1000, steps per second: 129, episode reward: 50.720, mean reward:  0.051 [-24.251, 24.186], mean action: 1.306 [0.000, 3.000],  loss: 23.403406, mse: 14043.251699, mean_q: 115.754890, mean_eps: 0.289413
 216830/300000: episode: 1591, duration: 7.953s, episode steps: 1000, steps per second: 126, episode reward: 126.248, mean reward:  0.126 [-21.290, 23.060], mean action: 1.457 [0.000, 3.000],  loss: 23.457655, mse: 13834.916142, mean_q: 115.088485, mean_eps: 0.286113
 217181/300000: episode: 1592, duration: 2.596s, episode steps: 351, steps per second: 135, episode reward: -104.321, mean reward: -0.297 [-100.000, 18.121], mean action: 1.769 [0.000, 3.000],  loss: 24.741324, mse: 13825.652878, mean_q: 114.864748, mean_eps: 0.283883
 217430/300000: episode: 1593, duration: 1.736s, episode steps: 249, steps per second: 143, episode reward: -221.498, mean reward: -0.890 [-100.000, 15.793], mean action: 1.783 [0.000, 3.000],  loss: 25.738563, mse: 13711.779775, mean_q: 114.621385, mean_eps: 0.282894
 217751/300000: episode: 1594, duration: 2.301s, episode steps: 321, steps per second: 139, episode reward: -77.124, mean reward: -0.240 [-100.000, 14.652], mean action: 1.847 [0.000, 3.000],  loss: 24.461398, mse: 13868.455522, mean_q: 115.129584, mean_eps: 0.281953
 218549/300000: episode: 1595, duration: 5.885s, episode steps: 798, steps per second: 136, episode reward: 89.774, mean reward:  0.112 [-19.786, 100.000], mean action: 1.784 [0.000, 3.000],  loss: 23.698030, mse: 13832.147107, mean_q: 115.273778, mean_eps: 0.280107
 219043/300000: episode: 1596, duration: 3.653s, episode steps: 494, steps per second: 135, episode reward: -77.750, mean reward: -0.157 [-100.000, 22.899], mean action: 1.820 [0.000, 3.000],  loss: 23.026341, mse: 13997.243550, mean_q: 115.676520, mean_eps: 0.277975
 219365/300000: episode: 1597, duration: 2.476s, episode steps: 322, steps per second: 130, episode reward: -129.234, mean reward: -0.401 [-100.000, 16.216], mean action: 1.826 [0.000, 3.000],  loss: 25.160715, mse: 14093.398246, mean_q: 116.603133, mean_eps: 0.276628
 219662/300000: episode: 1598, duration: 2.249s, episode steps: 297, steps per second: 132, episode reward: -243.294, mean reward: -0.819 [-100.000,  5.191], mean action: 1.774 [0.000, 3.000],  loss: 21.192356, mse: 14419.666328, mean_q: 117.882108, mean_eps: 0.275607
 220662/300000: episode: 1599, duration: 7.636s, episode steps: 1000, steps per second: 131, episode reward: 103.143, mean reward:  0.103 [-21.422, 22.540], mean action: 1.280 [0.000, 3.000],  loss: 22.395406, mse: 14416.790951, mean_q: 117.540215, mean_eps: 0.273467
 221662/300000: episode: 1600, duration: 8.160s, episode steps: 1000, steps per second: 123, episode reward: 40.687, mean reward:  0.041 [-20.338, 23.294], mean action: 2.007 [0.000, 3.000],  loss: 21.034274, mse: 14529.687845, mean_q: 117.975083, mean_eps: 0.270167
 221893/300000: episode: 1601, duration: 1.639s, episode steps: 231, steps per second: 141, episode reward: -271.660, mean reward: -1.176 [-100.000,  2.572], mean action: 1.831 [0.000, 3.000],  loss: 21.042423, mse: 14652.425037, mean_q: 118.163989, mean_eps: 0.268136
 222893/300000: episode: 1602, duration: 8.268s, episode steps: 1000, steps per second: 121, episode reward: -18.748, mean reward: -0.019 [-24.016, 16.768], mean action: 1.656 [0.000, 3.000],  loss: 25.865128, mse: 14740.864462, mean_q: 118.994527, mean_eps: 0.266105
 223433/300000: episode: 1603, duration: 4.273s, episode steps: 540, steps per second: 126, episode reward: -233.613, mean reward: -0.433 [-100.000, 25.209], mean action: 1.683 [0.000, 3.000],  loss: 32.788306, mse: 15210.772911, mean_q: 122.059410, mean_eps: 0.263564
 223594/300000: episode: 1604, duration: 1.129s, episode steps: 161, steps per second: 143, episode reward: -45.754, mean reward: -0.284 [-100.000, 18.392], mean action: 1.770 [0.000, 3.000],  loss: 21.822664, mse: 15227.489573, mean_q: 121.405625, mean_eps: 0.262407
 223989/300000: episode: 1605, duration: 2.855s, episode steps: 395, steps per second: 138, episode reward: -280.187, mean reward: -0.709 [-100.000, 11.420], mean action: 1.630 [0.000, 3.000],  loss: 30.062222, mse: 15170.396277, mean_q: 121.298221, mean_eps: 0.261490
 224120/300000: episode: 1606, duration: 0.924s, episode steps: 131, steps per second: 142, episode reward: -355.323, mean reward: -2.712 [-100.000,  2.111], mean action: 1.878 [0.000, 3.000],  loss: 18.845173, mse: 15113.549969, mean_q: 121.440737, mean_eps: 0.260622
 225120/300000: episode: 1607, duration: 8.218s, episode steps: 1000, steps per second: 122, episode reward: 124.662, mean reward:  0.125 [-21.105, 24.000], mean action: 1.250 [0.000, 3.000],  loss: 23.597802, mse: 15165.469129, mean_q: 121.520131, mean_eps: 0.258756
 225692/300000: episode: 1608, duration: 4.146s, episode steps: 572, steps per second: 138, episode reward: -275.508, mean reward: -0.482 [-100.000, 17.325], mean action: 1.837 [0.000, 3.000],  loss: 21.567648, mse: 15192.588773, mean_q: 121.649221, mean_eps: 0.256162
 225910/300000: episode: 1609, duration: 1.524s, episode steps: 218, steps per second: 143, episode reward: -61.679, mean reward: -0.283 [-100.000,  7.847], mean action: 1.798 [0.000, 3.000],  loss: 29.686809, mse: 15763.507096, mean_q: 124.096115, mean_eps: 0.254858
 226020/300000: episode: 1610, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: -180.218, mean reward: -1.638 [-100.000,  3.065], mean action: 1.645 [0.000, 3.000],  loss: 26.829293, mse: 15323.875533, mean_q: 123.273827, mean_eps: 0.254317
 227020/300000: episode: 1611, duration: 7.610s, episode steps: 1000, steps per second: 131, episode reward: 29.251, mean reward:  0.029 [-21.009, 22.622], mean action: 1.582 [0.000, 3.000],  loss: 23.419328, mse: 15790.210559, mean_q: 124.259476, mean_eps: 0.252486
 228020/300000: episode: 1612, duration: 7.867s, episode steps: 1000, steps per second: 127, episode reward: 115.906, mean reward:  0.116 [-23.012, 22.463], mean action: 1.446 [0.000, 3.000],  loss: 27.689599, mse: 15670.854859, mean_q: 123.818500, mean_eps: 0.249186
 228120/300000: episode: 1613, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -249.600, mean reward: -2.496 [-100.000,  2.894], mean action: 1.770 [0.000, 3.000],  loss: 35.208720, mse: 15551.189326, mean_q: 122.687707, mean_eps: 0.247371
 228413/300000: episode: 1614, duration: 2.101s, episode steps: 293, steps per second: 139, episode reward: -267.311, mean reward: -0.912 [-100.000,  4.435], mean action: 1.816 [0.000, 3.000],  loss: 27.423720, mse: 15509.904693, mean_q: 122.836913, mean_eps: 0.246722
 228744/300000: episode: 1615, duration: 2.374s, episode steps: 331, steps per second: 139, episode reward: -277.422, mean reward: -0.838 [-100.000, 35.285], mean action: 1.770 [0.000, 3.000],  loss: 24.394155, mse: 16136.202532, mean_q: 124.860256, mean_eps: 0.245693
 228884/300000: episode: 1616, duration: 1.002s, episode steps: 140, steps per second: 140, episode reward: -351.522, mean reward: -2.511 [-100.000,  3.167], mean action: 1.757 [0.000, 3.000],  loss: 21.382553, mse: 16012.111823, mean_q: 123.850099, mean_eps: 0.244915
 229160/300000: episode: 1617, duration: 1.970s, episode steps: 276, steps per second: 140, episode reward: -258.633, mean reward: -0.937 [-100.000, 20.422], mean action: 1.649 [0.000, 3.000],  loss: 24.196458, mse: 15954.402860, mean_q: 124.334952, mean_eps: 0.244229
 229335/300000: episode: 1618, duration: 1.237s, episode steps: 175, steps per second: 141, episode reward: -98.822, mean reward: -0.565 [-100.000,  8.926], mean action: 1.749 [0.000, 3.000],  loss: 28.133861, mse: 15558.805709, mean_q: 122.520147, mean_eps: 0.243485
 229504/300000: episode: 1619, duration: 1.252s, episode steps: 169, steps per second: 135, episode reward: -166.623, mean reward: -0.986 [-100.000,  2.954], mean action: 1.893 [0.000, 3.000],  loss: 32.691905, mse: 16143.850309, mean_q: 125.195243, mean_eps: 0.242917
 230504/300000: episode: 1620, duration: 7.444s, episode steps: 1000, steps per second: 134, episode reward: 69.440, mean reward:  0.069 [-22.707, 23.702], mean action: 1.123 [0.000, 3.000],  loss: 34.768196, mse: 16027.679006, mean_q: 124.148593, mean_eps: 0.240988
 230599/300000: episode: 1621, duration: 0.692s, episode steps:  95, steps per second: 137, episode reward: -281.607, mean reward: -2.964 [-100.000,  3.634], mean action: 1.800 [0.000, 3.000],  loss: 31.986276, mse: 15971.593997, mean_q: 124.562721, mean_eps: 0.239182
 231051/300000: episode: 1622, duration: 3.308s, episode steps: 452, steps per second: 137, episode reward: -278.488, mean reward: -0.616 [-100.000, 11.971], mean action: 1.838 [0.000, 3.000],  loss: 35.139219, mse: 16155.533292, mean_q: 124.612053, mean_eps: 0.238279
 231260/300000: episode: 1623, duration: 1.482s, episode steps: 209, steps per second: 141, episode reward: -38.958, mean reward: -0.186 [-100.000, 12.112], mean action: 1.727 [0.000, 3.000],  loss: 34.811839, mse: 16042.374626, mean_q: 124.781424, mean_eps: 0.237188
 231341/300000: episode: 1624, duration: 0.642s, episode steps:  81, steps per second: 126, episode reward: -160.421, mean reward: -1.981 [-100.000, 13.952], mean action: 1.605 [0.000, 3.000],  loss: 46.325434, mse: 15616.995419, mean_q: 122.321268, mean_eps: 0.236710
 232247/300000: episode: 1625, duration: 7.861s, episode steps: 906, steps per second: 115, episode reward: -306.571, mean reward: -0.338 [-100.000, 12.816], mean action: 1.599 [0.000, 3.000],  loss: 28.197225, mse: 16071.272103, mean_q: 124.225448, mean_eps: 0.235081
 233171/300000: episode: 1626, duration: 6.944s, episode steps: 924, steps per second: 133, episode reward: 150.986, mean reward:  0.163 [-18.963, 100.000], mean action: 1.596 [0.000, 3.000],  loss: 30.929126, mse: 15813.524734, mean_q: 122.877876, mean_eps: 0.232062
 234171/300000: episode: 1627, duration: 7.634s, episode steps: 1000, steps per second: 131, episode reward: 72.563, mean reward:  0.073 [-24.435, 33.907], mean action: 1.450 [0.000, 3.000],  loss: 31.281028, mse: 15573.292499, mean_q: 122.080537, mean_eps: 0.228887
 234628/300000: episode: 1628, duration: 3.421s, episode steps: 457, steps per second: 134, episode reward: -299.109, mean reward: -0.655 [-100.000, 22.602], mean action: 1.790 [0.000, 3.000],  loss: 32.874156, mse: 15566.060130, mean_q: 121.513459, mean_eps: 0.226483
 234961/300000: episode: 1629, duration: 2.380s, episode steps: 333, steps per second: 140, episode reward: -390.008, mean reward: -1.171 [-100.000,  4.928], mean action: 1.778 [0.000, 3.000],  loss: 29.891470, mse: 15517.019514, mean_q: 121.625965, mean_eps: 0.225180
 235182/300000: episode: 1630, duration: 1.569s, episode steps: 221, steps per second: 141, episode reward: -359.372, mean reward: -1.626 [-100.000,  5.059], mean action: 1.900 [0.000, 3.000],  loss: 40.331036, mse: 15725.536804, mean_q: 121.197254, mean_eps: 0.224266
 236182/300000: episode: 1631, duration: 7.758s, episode steps: 1000, steps per second: 129, episode reward: 49.247, mean reward:  0.049 [-19.496, 16.916], mean action: 1.569 [0.000, 3.000],  loss: 27.435665, mse: 15812.317767, mean_q: 122.318926, mean_eps: 0.222251
 237182/300000: episode: 1632, duration: 8.018s, episode steps: 1000, steps per second: 125, episode reward: -260.014, mean reward: -0.260 [-7.259,  7.459], mean action: 1.730 [0.000, 3.000],  loss: 26.903244, mse: 15896.093761, mean_q: 122.935169, mean_eps: 0.218951
 237405/300000: episode: 1633, duration: 1.582s, episode steps: 223, steps per second: 141, episode reward: -168.325, mean reward: -0.755 [-100.000, 29.262], mean action: 1.709 [0.000, 3.000],  loss: 36.638702, mse: 15923.632313, mean_q: 123.153009, mean_eps: 0.216933
 237654/300000: episode: 1634, duration: 1.784s, episode steps: 249, steps per second: 140, episode reward: -245.137, mean reward: -0.984 [-100.000,  8.973], mean action: 1.867 [0.000, 3.000],  loss: 30.904540, mse: 16196.390170, mean_q: 124.180006, mean_eps: 0.216154
 237849/300000: episode: 1635, duration: 1.405s, episode steps: 195, steps per second: 139, episode reward: -112.142, mean reward: -0.575 [-100.000, 18.650], mean action: 1.928 [0.000, 3.000],  loss: 29.004448, mse: 16017.963792, mean_q: 123.847870, mean_eps: 0.215422
 238082/300000: episode: 1636, duration: 1.821s, episode steps: 233, steps per second: 128, episode reward: -173.962, mean reward: -0.747 [-100.000, 10.042], mean action: 1.622 [0.000, 3.000],  loss: 17.716367, mse: 16403.863147, mean_q: 124.783627, mean_eps: 0.214715
 238414/300000: episode: 1637, duration: 2.511s, episode steps: 332, steps per second: 132, episode reward: -349.486, mean reward: -1.053 [-100.000,  5.422], mean action: 1.617 [0.000, 3.000],  loss: 28.705179, mse: 16403.867055, mean_q: 125.620649, mean_eps: 0.213783
 238538/300000: episode: 1638, duration: 0.941s, episode steps: 124, steps per second: 132, episode reward: -317.676, mean reward: -2.562 [-100.000,  2.940], mean action: 1.774 [0.000, 3.000],  loss: 21.827425, mse: 16693.693824, mean_q: 127.600912, mean_eps: 0.213031
 238766/300000: episode: 1639, duration: 1.583s, episode steps: 228, steps per second: 144, episode reward: -338.409, mean reward: -1.484 [-100.000,  4.486], mean action: 1.548 [0.000, 3.000],  loss: 20.984661, mse: 16956.775485, mean_q: 127.882826, mean_eps: 0.212450
 238934/300000: episode: 1640, duration: 1.320s, episode steps: 168, steps per second: 127, episode reward: -210.739, mean reward: -1.254 [-100.000,  6.560], mean action: 1.744 [0.000, 3.000],  loss: 35.014825, mse: 16891.074393, mean_q: 127.178077, mean_eps: 0.211797
 239167/300000: episode: 1641, duration: 1.648s, episode steps: 233, steps per second: 141, episode reward: -336.795, mean reward: -1.445 [-100.000,  3.813], mean action: 1.734 [0.000, 3.000],  loss: 35.799593, mse: 17110.809998, mean_q: 128.706999, mean_eps: 0.211135
 240167/300000: episode: 1642, duration: 8.453s, episode steps: 1000, steps per second: 118, episode reward: 61.639, mean reward:  0.062 [-21.408, 22.909], mean action: 1.600 [0.000, 3.000],  loss: 26.495615, mse: 17147.248788, mean_q: 128.770980, mean_eps: 0.209101
 240288/300000: episode: 1643, duration: 0.913s, episode steps: 121, steps per second: 132, episode reward: -161.812, mean reward: -1.337 [-100.000,  2.677], mean action: 1.793 [0.000, 3.000],  loss: 23.377529, mse: 16914.717677, mean_q: 128.090363, mean_eps: 0.207251
 241288/300000: episode: 1644, duration: 7.944s, episode steps: 1000, steps per second: 126, episode reward: 11.369, mean reward:  0.011 [-20.625, 29.135], mean action: 1.745 [0.000, 3.000],  loss: 26.323115, mse: 16925.777136, mean_q: 128.517794, mean_eps: 0.205401
 241452/300000: episode: 1645, duration: 1.159s, episode steps: 164, steps per second: 141, episode reward: -75.773, mean reward: -0.462 [-100.000, 15.408], mean action: 1.872 [0.000, 3.000],  loss: 27.351686, mse: 16696.158709, mean_q: 127.425512, mean_eps: 0.203481
 242452/300000: episode: 1646, duration: 8.085s, episode steps: 1000, steps per second: 124, episode reward: 45.506, mean reward:  0.046 [-19.886, 22.401], mean action: 1.611 [0.000, 3.000],  loss: 29.429576, mse: 16882.962149, mean_q: 128.733309, mean_eps: 0.201560
 242888/300000: episode: 1647, duration: 3.229s, episode steps: 436, steps per second: 135, episode reward: -89.670, mean reward: -0.206 [-100.000, 15.165], mean action: 1.670 [0.000, 3.000],  loss: 31.227978, mse: 16775.136275, mean_q: 128.473728, mean_eps: 0.199191
 243600/300000: episode: 1648, duration: 5.630s, episode steps: 712, steps per second: 126, episode reward: -82.713, mean reward: -0.116 [-100.000, 18.221], mean action: 1.705 [0.000, 3.000],  loss: 26.356455, mse: 16850.255707, mean_q: 129.748702, mean_eps: 0.197296
 243793/300000: episode: 1649, duration: 1.363s, episode steps: 193, steps per second: 142, episode reward: -97.371, mean reward: -0.505 [-100.000, 15.122], mean action: 1.699 [0.000, 3.000],  loss: 35.032200, mse: 16640.379807, mean_q: 129.340435, mean_eps: 0.195803
 244049/300000: episode: 1650, duration: 1.820s, episode steps: 256, steps per second: 141, episode reward: 241.135, mean reward:  0.942 [-10.644, 100.000], mean action: 1.652 [0.000, 3.000],  loss: 26.711240, mse: 16610.750885, mean_q: 129.467273, mean_eps: 0.195062
 244510/300000: episode: 1651, duration: 3.343s, episode steps: 461, steps per second: 138, episode reward: 180.842, mean reward:  0.392 [-20.056, 100.000], mean action: 1.284 [0.000, 3.000],  loss: 24.784631, mse: 16420.119772, mean_q: 129.007976, mean_eps: 0.193879
 245201/300000: episode: 1652, duration: 5.311s, episode steps: 691, steps per second: 130, episode reward: 17.275, mean reward:  0.025 [-100.000, 19.523], mean action: 1.812 [0.000, 3.000],  loss: 29.482680, mse: 16414.370900, mean_q: 129.016947, mean_eps: 0.191978
 246201/300000: episode: 1653, duration: 7.944s, episode steps: 1000, steps per second: 126, episode reward: -8.419, mean reward: -0.008 [-5.153,  6.207], mean action: 1.827 [0.000, 3.000],  loss: 27.823607, mse: 16441.318293, mean_q: 129.432002, mean_eps: 0.189188
 247201/300000: episode: 1654, duration: 9.164s, episode steps: 1000, steps per second: 109, episode reward: -177.504, mean reward: -0.178 [-22.279, 16.220], mean action: 1.787 [0.000, 3.000],  loss: 28.650717, mse: 16552.730946, mean_q: 130.157967, mean_eps: 0.185888
 247330/300000: episode: 1655, duration: 0.925s, episode steps: 129, steps per second: 139, episode reward: 11.865, mean reward:  0.092 [-100.000, 20.986], mean action: 1.814 [0.000, 3.000],  loss: 26.775950, mse: 16662.232664, mean_q: 131.394060, mean_eps: 0.184025
 247884/300000: episode: 1656, duration: 4.229s, episode steps: 554, steps per second: 131, episode reward: 232.571, mean reward:  0.420 [-23.487, 100.000], mean action: 1.287 [0.000, 3.000],  loss: 26.490600, mse: 16476.579022, mean_q: 130.099163, mean_eps: 0.182899
 248038/300000: episode: 1657, duration: 1.088s, episode steps: 154, steps per second: 142, episode reward: -87.055, mean reward: -0.565 [-100.000, 14.754], mean action: 1.636 [0.000, 3.000],  loss: 26.223219, mse: 16679.541751, mean_q: 131.646248, mean_eps: 0.181730
 248331/300000: episode: 1658, duration: 2.372s, episode steps: 293, steps per second: 124, episode reward: -165.489, mean reward: -0.565 [-100.000, 20.780], mean action: 1.809 [0.000, 3.000],  loss: 29.086795, mse: 16210.603536, mean_q: 129.364953, mean_eps: 0.180993
 248479/300000: episode: 1659, duration: 1.162s, episode steps: 148, steps per second: 127, episode reward: -139.794, mean reward: -0.945 [-100.000,  9.324], mean action: 1.824 [0.000, 3.000],  loss: 30.674359, mse: 16379.765229, mean_q: 130.525484, mean_eps: 0.180265
 249479/300000: episode: 1660, duration: 8.353s, episode steps: 1000, steps per second: 120, episode reward: 51.711, mean reward:  0.052 [-20.052, 18.126], mean action: 1.214 [0.000, 3.000],  loss: 25.748114, mse: 16198.581967, mean_q: 129.714116, mean_eps: 0.178371
 250479/300000: episode: 1661, duration: 8.576s, episode steps: 1000, steps per second: 117, episode reward: -26.835, mean reward: -0.027 [-11.246, 12.980], mean action: 1.673 [0.000, 3.000],  loss: 25.424304, mse: 16111.443587, mean_q: 129.896394, mean_eps: 0.175071
 250797/300000: episode: 1662, duration: 2.297s, episode steps: 318, steps per second: 138, episode reward: -340.253, mean reward: -1.070 [-100.000,  5.788], mean action: 1.767 [0.000, 3.000],  loss: 24.329368, mse: 16238.561735, mean_q: 130.424733, mean_eps: 0.172896
 250949/300000: episode: 1663, duration: 1.058s, episode steps: 152, steps per second: 144, episode reward: -139.260, mean reward: -0.916 [-100.000, 17.040], mean action: 1.632 [0.000, 3.000],  loss: 23.747357, mse: 16278.125231, mean_q: 130.745640, mean_eps: 0.172121
 251083/300000: episode: 1664, duration: 0.955s, episode steps: 134, steps per second: 140, episode reward: -29.304, mean reward: -0.219 [-100.000, 17.536], mean action: 1.933 [0.000, 3.000],  loss: 23.729762, mse: 16067.507397, mean_q: 130.243765, mean_eps: 0.171649
 252017/300000: episode: 1665, duration: 7.423s, episode steps: 934, steps per second: 126, episode reward: 143.085, mean reward:  0.153 [-19.748, 100.000], mean action: 1.529 [0.000, 3.000],  loss: 25.760811, mse: 16227.404336, mean_q: 130.798592, mean_eps: 0.169887
 252359/300000: episode: 1666, duration: 2.476s, episode steps: 342, steps per second: 138, episode reward: -49.918, mean reward: -0.146 [-100.000, 27.969], mean action: 1.766 [0.000, 3.000],  loss: 29.558810, mse: 16007.069267, mean_q: 129.929185, mean_eps: 0.167781
 252775/300000: episode: 1667, duration: 3.128s, episode steps: 416, steps per second: 133, episode reward: -57.383, mean reward: -0.138 [-100.000, 14.576], mean action: 1.781 [0.000, 3.000],  loss: 21.261742, mse: 16058.828489, mean_q: 130.356824, mean_eps: 0.166531
 253316/300000: episode: 1668, duration: 4.111s, episode steps: 541, steps per second: 132, episode reward: 211.973, mean reward:  0.392 [-22.727, 100.000], mean action: 1.336 [0.000, 3.000],  loss: 25.845094, mse: 16308.194641, mean_q: 131.467259, mean_eps: 0.164951
 253593/300000: episode: 1669, duration: 2.015s, episode steps: 277, steps per second: 137, episode reward: 224.713, mean reward:  0.811 [-12.633, 100.000], mean action: 1.812 [0.000, 3.000],  loss: 20.662866, mse: 16444.494060, mean_q: 132.051188, mean_eps: 0.163602
 253835/300000: episode: 1670, duration: 1.704s, episode steps: 242, steps per second: 142, episode reward: -38.415, mean reward: -0.159 [-100.000, 14.945], mean action: 1.674 [0.000, 3.000],  loss: 15.966640, mse: 16196.702968, mean_q: 131.544511, mean_eps: 0.162745
 253968/300000: episode: 1671, duration: 0.966s, episode steps: 133, steps per second: 138, episode reward: -7.518, mean reward: -0.057 [-100.000, 13.471], mean action: 1.594 [0.000, 3.000],  loss: 26.411095, mse: 16347.640720, mean_q: 132.071050, mean_eps: 0.162127
 254283/300000: episode: 1672, duration: 2.268s, episode steps: 315, steps per second: 139, episode reward: -260.731, mean reward: -0.828 [-100.000,  2.942], mean action: 1.775 [0.000, 3.000],  loss: 23.320829, mse: 16418.008966, mean_q: 132.339420, mean_eps: 0.161387
 254622/300000: episode: 1673, duration: 2.434s, episode steps: 339, steps per second: 139, episode reward: -2.698, mean reward: -0.008 [-100.000, 11.057], mean action: 1.773 [0.000, 3.000],  loss: 20.330951, mse: 16460.793026, mean_q: 132.605393, mean_eps: 0.160308
 254809/300000: episode: 1674, duration: 1.398s, episode steps: 187, steps per second: 134, episode reward: -85.885, mean reward: -0.459 [-100.000, 16.532], mean action: 1.561 [0.000, 3.000],  loss: 30.505718, mse: 16185.499927, mean_q: 131.442156, mean_eps: 0.159440
 255148/300000: episode: 1675, duration: 2.579s, episode steps: 339, steps per second: 131, episode reward: -43.047, mean reward: -0.127 [-100.000, 11.453], mean action: 1.605 [0.000, 3.000],  loss: 24.320152, mse: 16585.183507, mean_q: 133.216615, mean_eps: 0.158573
 255636/300000: episode: 1676, duration: 3.702s, episode steps: 488, steps per second: 132, episode reward: -101.469, mean reward: -0.208 [-100.000, 10.612], mean action: 1.555 [0.000, 3.000],  loss: 24.010688, mse: 16403.653000, mean_q: 132.517202, mean_eps: 0.157208
 255719/300000: episode: 1677, duration: 0.582s, episode steps:  83, steps per second: 143, episode reward: -381.312, mean reward: -4.594 [-100.000,  0.296], mean action: 1.373 [0.000, 3.000],  loss: 27.700084, mse: 16369.129647, mean_q: 132.242212, mean_eps: 0.156266
 256282/300000: episode: 1678, duration: 4.248s, episode steps: 563, steps per second: 133, episode reward: -60.375, mean reward: -0.107 [-100.000, 11.542], mean action: 1.540 [0.000, 3.000],  loss: 22.286303, mse: 16571.357935, mean_q: 133.528479, mean_eps: 0.155200
 257282/300000: episode: 1679, duration: 8.407s, episode steps: 1000, steps per second: 119, episode reward: -76.739, mean reward: -0.077 [-12.304, 19.927], mean action: 1.671 [0.000, 3.000],  loss: 22.569727, mse: 16369.971393, mean_q: 132.913971, mean_eps: 0.152621
 257843/300000: episode: 1680, duration: 4.402s, episode steps: 561, steps per second: 127, episode reward: 207.437, mean reward:  0.370 [-17.294, 100.000], mean action: 1.531 [0.000, 3.000],  loss: 25.365405, mse: 16256.299598, mean_q: 132.413302, mean_eps: 0.150045
 258352/300000: episode: 1681, duration: 3.743s, episode steps: 509, steps per second: 136, episode reward: -319.612, mean reward: -0.628 [-100.000, 15.949], mean action: 1.904 [0.000, 3.000],  loss: 22.422025, mse: 15984.708188, mean_q: 131.670105, mean_eps: 0.148280
 258440/300000: episode: 1682, duration: 0.635s, episode steps:  88, steps per second: 139, episode reward: -383.743, mean reward: -4.361 [-100.000,  0.571], mean action: 1.307 [0.000, 3.000],  loss: 29.946034, mse: 15863.409468, mean_q: 131.113948, mean_eps: 0.147295
 259440/300000: episode: 1683, duration: 8.447s, episode steps: 1000, steps per second: 118, episode reward: -10.273, mean reward: -0.010 [-19.033, 12.439], mean action: 1.759 [0.000, 3.000],  loss: 23.397499, mse: 16096.737168, mean_q: 131.808218, mean_eps: 0.145500
 260440/300000: episode: 1684, duration: 8.154s, episode steps: 1000, steps per second: 123, episode reward: -49.103, mean reward: -0.049 [-4.989,  4.672], mean action: 1.718 [0.000, 3.000],  loss: 22.191979, mse: 16469.378495, mean_q: 133.602625, mean_eps: 0.142200
 261440/300000: episode: 1685, duration: 7.999s, episode steps: 1000, steps per second: 125, episode reward: -66.805, mean reward: -0.067 [-6.009,  5.297], mean action: 1.718 [0.000, 3.000],  loss: 22.730475, mse: 16656.263980, mean_q: 134.569175, mean_eps: 0.138900
 262440/300000: episode: 1686, duration: 7.943s, episode steps: 1000, steps per second: 126, episode reward: -77.937, mean reward: -0.078 [-7.475,  5.271], mean action: 1.700 [0.000, 3.000],  loss: 20.045555, mse: 16695.929281, mean_q: 135.166958, mean_eps: 0.135600
 263440/300000: episode: 1687, duration: 8.211s, episode steps: 1000, steps per second: 122, episode reward: -54.615, mean reward: -0.055 [-4.272,  4.711], mean action: 1.780 [0.000, 3.000],  loss: 22.443934, mse: 16725.964582, mean_q: 135.573155, mean_eps: 0.132300
 264440/300000: episode: 1688, duration: 7.589s, episode steps: 1000, steps per second: 132, episode reward: -64.485, mean reward: -0.064 [-5.930,  5.157], mean action: 1.679 [0.000, 3.000],  loss: 19.307941, mse: 16746.473529, mean_q: 136.179913, mean_eps: 0.129000
 265440/300000: episode: 1689, duration: 8.883s, episode steps: 1000, steps per second: 113, episode reward: -81.616, mean reward: -0.082 [-4.416,  5.288], mean action: 1.800 [0.000, 3.000],  loss: 19.919664, mse: 16796.739670, mean_q: 136.965150, mean_eps: 0.125700
 266077/300000: episode: 1690, duration: 5.546s, episode steps: 637, steps per second: 115, episode reward: -501.328, mean reward: -0.787 [-100.000,  5.096], mean action: 1.779 [0.000, 3.000],  loss: 19.253108, mse: 16917.012432, mean_q: 137.791489, mean_eps: 0.122999
 266462/300000: episode: 1691, duration: 3.261s, episode steps: 385, steps per second: 118, episode reward: -128.365, mean reward: -0.333 [-100.000, 11.367], mean action: 1.868 [0.000, 3.000],  loss: 23.059733, mse: 17201.852077, mean_q: 138.722726, mean_eps: 0.121312
 266613/300000: episode: 1692, duration: 1.178s, episode steps: 151, steps per second: 128, episode reward: -93.036, mean reward: -0.616 [-100.000, 113.409], mean action: 1.841 [0.000, 3.000],  loss: 21.774395, mse: 17267.992731, mean_q: 139.313800, mean_eps: 0.120428
 267613/300000: episode: 1693, duration: 7.865s, episode steps: 1000, steps per second: 127, episode reward: -64.988, mean reward: -0.065 [-6.436,  4.527], mean action: 1.820 [0.000, 3.000],  loss: 18.172028, mse: 17582.790560, mean_q: 140.035994, mean_eps: 0.118529
 268613/300000: episode: 1694, duration: 7.552s, episode steps: 1000, steps per second: 132, episode reward: -88.625, mean reward: -0.089 [-8.660, 12.771], mean action: 1.756 [0.000, 3.000],  loss: 20.517941, mse: 17342.347847, mean_q: 138.865446, mean_eps: 0.115229
 269613/300000: episode: 1695, duration: 8.268s, episode steps: 1000, steps per second: 121, episode reward: -58.095, mean reward: -0.058 [-5.097,  5.893], mean action: 1.607 [0.000, 3.000],  loss: 18.302366, mse: 17362.336256, mean_q: 139.097397, mean_eps: 0.111929
 270613/300000: episode: 1696, duration: 8.472s, episode steps: 1000, steps per second: 118, episode reward: -89.735, mean reward: -0.090 [-4.995,  4.950], mean action: 1.572 [0.000, 3.000],  loss: 17.325719, mse: 17276.050707, mean_q: 139.307593, mean_eps: 0.108629
 271613/300000: episode: 1697, duration: 8.061s, episode steps: 1000, steps per second: 124, episode reward: -147.623, mean reward: -0.148 [-5.892,  4.346], mean action: 1.639 [0.000, 3.000],  loss: 18.480865, mse: 17635.187517, mean_q: 141.036383, mean_eps: 0.105329
 272199/300000: episode: 1698, duration: 4.422s, episode steps: 586, steps per second: 133, episode reward: -393.845, mean reward: -0.672 [-100.000,  4.382], mean action: 1.706 [0.000, 3.000],  loss: 20.315429, mse: 17828.533058, mean_q: 142.022430, mean_eps: 0.102712
 273199/300000: episode: 1699, duration: 8.316s, episode steps: 1000, steps per second: 120, episode reward: -71.772, mean reward: -0.072 [-7.448,  6.674], mean action: 1.745 [0.000, 3.000],  loss: 19.037608, mse: 18030.218082, mean_q: 142.934429, mean_eps: 0.100095
 274199/300000: episode: 1700, duration: 8.016s, episode steps: 1000, steps per second: 125, episode reward: -218.516, mean reward: -0.219 [-8.176,  5.148], mean action: 1.790 [0.000, 3.000],  loss: 20.302455, mse: 17979.241060, mean_q: 143.458124, mean_eps: 0.096795
 275199/300000: episode: 1701, duration: 8.620s, episode steps: 1000, steps per second: 116, episode reward: -276.924, mean reward: -0.277 [-6.507,  5.761], mean action: 1.732 [0.000, 3.000],  loss: 17.504023, mse: 18452.009142, mean_q: 145.771332, mean_eps: 0.093495
 275490/300000: episode: 1702, duration: 2.107s, episode steps: 291, steps per second: 138, episode reward: -202.531, mean reward: -0.696 [-100.000, 13.397], mean action: 1.832 [0.000, 3.000],  loss: 20.439887, mse: 18574.260444, mean_q: 146.675470, mean_eps: 0.091365
 276490/300000: episode: 1703, duration: 8.049s, episode steps: 1000, steps per second: 124, episode reward: -84.861, mean reward: -0.085 [-23.507, 11.608], mean action: 1.629 [0.000, 3.000],  loss: 20.165992, mse: 18447.721969, mean_q: 146.133936, mean_eps: 0.089235
 276637/300000: episode: 1704, duration: 1.062s, episode steps: 147, steps per second: 138, episode reward: -39.529, mean reward: -0.269 [-100.000, 19.246], mean action: 1.973 [0.000, 3.000],  loss: 15.052582, mse: 18933.562772, mean_q: 148.258279, mean_eps: 0.087342
 277637/300000: episode: 1705, duration: 8.295s, episode steps: 1000, steps per second: 121, episode reward: -227.426, mean reward: -0.227 [-6.640,  5.359], mean action: 1.701 [0.000, 3.000],  loss: 20.687750, mse: 18724.918979, mean_q: 147.524834, mean_eps: 0.085450
 277934/300000: episode: 1706, duration: 2.132s, episode steps: 297, steps per second: 139, episode reward: 213.363, mean reward:  0.718 [-10.673, 100.000], mean action: 1.273 [0.000, 3.000],  loss: 17.307725, mse: 19017.796918, mean_q: 148.916255, mean_eps: 0.083309
 278934/300000: episode: 1707, duration: 7.965s, episode steps: 1000, steps per second: 126, episode reward: -25.144, mean reward: -0.025 [-5.529,  4.564], mean action: 1.695 [0.000, 3.000],  loss: 22.055065, mse: 18972.229184, mean_q: 149.020327, mean_eps: 0.081169
 279934/300000: episode: 1708, duration: 8.474s, episode steps: 1000, steps per second: 118, episode reward: -33.341, mean reward: -0.033 [-5.531,  6.282], mean action: 1.590 [0.000, 3.000],  loss: 20.677907, mse: 18765.855900, mean_q: 148.663698, mean_eps: 0.077869
 280086/300000: episode: 1709, duration: 1.100s, episode steps: 152, steps per second: 138, episode reward: 51.608, mean reward:  0.340 [-100.000, 15.849], mean action: 1.974 [0.000, 3.000],  loss: 12.930046, mse: 18587.791697, mean_q: 147.720047, mean_eps: 0.075969
 280753/300000: episode: 1710, duration: 5.257s, episode steps: 667, steps per second: 127, episode reward: 180.566, mean reward:  0.271 [-14.618, 100.000], mean action: 1.436 [0.000, 3.000],  loss: 24.277565, mse: 18843.929620, mean_q: 149.068684, mean_eps: 0.074617
 281753/300000: episode: 1711, duration: 8.731s, episode steps: 1000, steps per second: 115, episode reward: -50.977, mean reward: -0.051 [-5.382,  5.213], mean action: 1.585 [0.000, 3.000],  loss: 17.069811, mse: 18837.412264, mean_q: 149.286839, mean_eps: 0.071867
 282535/300000: episode: 1712, duration: 6.442s, episode steps: 782, steps per second: 121, episode reward: -64.342, mean reward: -0.082 [-100.000, 19.533], mean action: 1.753 [0.000, 3.000],  loss: 18.718179, mse: 18564.379006, mean_q: 148.568938, mean_eps: 0.068926
 283535/300000: episode: 1713, duration: 8.342s, episode steps: 1000, steps per second: 120, episode reward: -52.742, mean reward: -0.053 [-5.769,  6.506], mean action: 1.705 [0.000, 3.000],  loss: 21.788188, mse: 18695.280768, mean_q: 149.258502, mean_eps: 0.065986
 284078/300000: episode: 1714, duration: 4.059s, episode steps: 543, steps per second: 134, episode reward: -577.758, mean reward: -1.064 [-100.000,  5.406], mean action: 1.926 [0.000, 3.000],  loss: 20.776748, mse: 18986.777191, mean_q: 150.187896, mean_eps: 0.063440
 285078/300000: episode: 1715, duration: 7.948s, episode steps: 1000, steps per second: 126, episode reward: -141.225, mean reward: -0.141 [-11.116, 28.112], mean action: 1.751 [0.000, 3.000],  loss: 18.018946, mse: 19107.730530, mean_q: 150.521199, mean_eps: 0.060894
 285906/300000: episode: 1716, duration: 6.472s, episode steps: 828, steps per second: 128, episode reward: -486.712, mean reward: -0.588 [-100.000, 26.277], mean action: 1.810 [0.000, 3.000],  loss: 20.269368, mse: 18845.525310, mean_q: 149.661750, mean_eps: 0.057878
 286396/300000: episode: 1717, duration: 3.638s, episode steps: 490, steps per second: 135, episode reward: -647.624, mean reward: -1.322 [-100.000,  5.225], mean action: 1.543 [0.000, 3.000],  loss: 21.667997, mse: 19008.953974, mean_q: 150.284360, mean_eps: 0.055703
 286669/300000: episode: 1718, duration: 1.965s, episode steps: 273, steps per second: 139, episode reward: -55.426, mean reward: -0.203 [-100.000, 19.784], mean action: 1.868 [0.000, 3.000],  loss: 15.532536, mse: 18924.778921, mean_q: 150.007710, mean_eps: 0.054444
 287588/300000: episode: 1719, duration: 7.452s, episode steps: 919, steps per second: 123, episode reward: -281.378, mean reward: -0.306 [-100.000,  6.695], mean action: 1.639 [0.000, 3.000],  loss: 24.508598, mse: 18774.300692, mean_q: 149.082504, mean_eps: 0.052478
 287714/300000: episode: 1720, duration: 0.972s, episode steps: 126, steps per second: 130, episode reward: -117.245, mean reward: -0.931 [-100.000, 12.611], mean action: 0.873 [0.000, 3.000],  loss: 15.396849, mse: 18642.514114, mean_q: 148.359415, mean_eps: 0.050753
 288171/300000: episode: 1721, duration: 3.620s, episode steps: 457, steps per second: 126, episode reward: -118.745, mean reward: -0.260 [-100.000, 10.881], mean action: 1.803 [0.000, 3.000],  loss: 18.592970, mse: 18673.949578, mean_q: 148.544750, mean_eps: 0.049791
 288278/300000: episode: 1722, duration: 0.761s, episode steps: 107, steps per second: 141, episode reward: -233.709, mean reward: -2.184 [-100.000,  1.368], mean action: 1.579 [0.000, 3.000],  loss: 19.085696, mse: 18403.086376, mean_q: 148.043381, mean_eps: 0.048861
 289278/300000: episode: 1723, duration: 7.993s, episode steps: 1000, steps per second: 125, episode reward: -277.260, mean reward: -0.277 [-10.325, 11.082], mean action: 1.711 [0.000, 3.000],  loss: 18.265098, mse: 18373.986201, mean_q: 147.742611, mean_eps: 0.047034
 289398/300000: episode: 1724, duration: 0.894s, episode steps: 120, steps per second: 134, episode reward: -175.416, mean reward: -1.462 [-100.000, 11.501], mean action: 1.292 [0.000, 3.000],  loss: 18.176249, mse: 18047.782511, mean_q: 146.788518, mean_eps: 0.045186
 290396/300000: episode: 1725, duration: 8.087s, episode steps: 998, steps per second: 123, episode reward: -362.497, mean reward: -0.363 [-100.000, 31.127], mean action: 1.741 [0.000, 3.000],  loss: 18.832310, mse: 18309.228433, mean_q: 147.984940, mean_eps: 0.043342
 291089/300000: episode: 1726, duration: 6.319s, episode steps: 693, steps per second: 110, episode reward: -436.613, mean reward: -0.630 [-100.000, 76.996], mean action: 1.714 [0.000, 3.000],  loss: 18.639974, mse: 18277.885666, mean_q: 147.964194, mean_eps: 0.040551
 291352/300000: episode: 1727, duration: 1.904s, episode steps: 263, steps per second: 138, episode reward: -194.251, mean reward: -0.739 [-100.000,  5.602], mean action: 1.471 [0.000, 3.000],  loss: 20.535989, mse: 18341.262105, mean_q: 148.157500, mean_eps: 0.038974
 291598/300000: episode: 1728, duration: 1.722s, episode steps: 246, steps per second: 143, episode reward: -260.754, mean reward: -1.060 [-100.000, 26.091], mean action: 1.569 [0.000, 3.000],  loss: 15.156500, mse: 18315.276653, mean_q: 148.372188, mean_eps: 0.038134
 292144/300000: episode: 1729, duration: 4.155s, episode steps: 546, steps per second: 131, episode reward: 194.991, mean reward:  0.357 [-8.278, 100.000], mean action: 1.601 [0.000, 3.000],  loss: 18.377402, mse: 18460.858114, mean_q: 148.579032, mean_eps: 0.036827
 292375/300000: episode: 1730, duration: 1.675s, episode steps: 231, steps per second: 138, episode reward: -166.310, mean reward: -0.720 [-100.000,  6.383], mean action: 1.706 [0.000, 3.000],  loss: 18.185794, mse: 18589.260222, mean_q: 149.135324, mean_eps: 0.035545
 293375/300000: episode: 1731, duration: 7.832s, episode steps: 1000, steps per second: 128, episode reward: -80.566, mean reward: -0.081 [-8.298,  7.330], mean action: 1.686 [0.000, 3.000],  loss: 17.132725, mse: 18643.716961, mean_q: 149.257003, mean_eps: 0.033514
 294375/300000: episode: 1732, duration: 8.293s, episode steps: 1000, steps per second: 121, episode reward: 62.280, mean reward:  0.062 [-10.225, 17.061], mean action: 1.645 [0.000, 3.000],  loss: 16.994703, mse: 18720.871854, mean_q: 149.583659, mean_eps: 0.030214
 294928/300000: episode: 1733, duration: 4.090s, episode steps: 553, steps per second: 135, episode reward: -223.975, mean reward: -0.405 [-100.000, 16.164], mean action: 1.624 [0.000, 3.000],  loss: 14.508566, mse: 18717.941445, mean_q: 149.852102, mean_eps: 0.027652
 295120/300000: episode: 1734, duration: 1.371s, episode steps: 192, steps per second: 140, episode reward: -234.149, mean reward: -1.220 [-100.000,  6.797], mean action: 1.307 [0.000, 3.000],  loss: 15.803653, mse: 18773.486074, mean_q: 149.729133, mean_eps: 0.026422
 296120/300000: episode: 1735, duration: 7.793s, episode steps: 1000, steps per second: 128, episode reward: -129.808, mean reward: -0.130 [-6.760,  6.495], mean action: 1.759 [0.000, 3.000],  loss: 18.237336, mse: 18659.095782, mean_q: 149.345264, mean_eps: 0.024456
 296400/300000: episode: 1736, duration: 2.138s, episode steps: 280, steps per second: 131, episode reward: -104.779, mean reward: -0.374 [-100.000, 10.803], mean action: 1.546 [0.000, 3.000],  loss: 14.509986, mse: 18973.489617, mean_q: 150.390035, mean_eps: 0.022344
 297400/300000: episode: 1737, duration: 7.709s, episode steps: 1000, steps per second: 130, episode reward: -199.319, mean reward: -0.199 [-6.482,  6.294], mean action: 1.761 [0.000, 3.000],  loss: 16.565370, mse: 18727.251749, mean_q: 149.589387, mean_eps: 0.020232
 298400/300000: episode: 1738, duration: 10.798s, episode steps: 1000, steps per second:  93, episode reward: -232.318, mean reward: -0.232 [-6.868,  7.286], mean action: 1.707 [0.000, 3.000],  loss: 17.322459, mse: 18747.530378, mean_q: 149.683688, mean_eps: 0.016932
 299130/300000: episode: 1739, duration: 5.828s, episode steps: 730, steps per second: 125, episode reward: -205.446, mean reward: -0.281 [-100.000,  9.403], mean action: 1.547 [0.000, 3.000],  loss: 16.511904, mse: 18945.517250, mean_q: 150.573451, mean_eps: 0.014077
 299353/300000: episode: 1740, duration: 1.594s, episode steps: 223, steps per second: 140, episode reward: -202.706, mean reward: -0.909 [-100.000, 35.512], mean action: 1.735 [0.000, 3.000],  loss: 12.594244, mse: 18709.295754, mean_q: 150.072166, mean_eps: 0.012505
 299410/300000: episode: 1741, duration: 0.406s, episode steps:  57, steps per second: 140, episode reward: -110.524, mean reward: -1.939 [-100.000, 19.724], mean action: 1.614 [0.000, 3.000],  loss: 22.309073, mse: 18943.908032, mean_q: 150.494724, mean_eps: 0.012043
 299797/300000: episode: 1742, duration: 2.798s, episode steps: 387, steps per second: 138, episode reward: -26.891, mean reward: -0.069 [-100.000, 64.507], mean action: 1.540 [0.000, 3.000],  loss: 15.716001, mse: 19111.572621, mean_q: 151.398128, mean_eps: 0.011310
done, took 2310.758 seconds
Testing for 5 episodes ...
Episode 1: reward: -137.930, steps: 173
Episode 2: reward: -236.116, steps: 82
Episode 3: reward: -170.830, steps: 65
Episode 4: reward: -228.085, steps: 80
Episode 5: reward: -278.964, steps: 908
Testing for 5 episodes ...
Episode 1: reward: -190.469, steps: 270
Episode 2: reward: -226.344, steps: 98
Episode 3: reward: -96.989, steps: 1000
Episode 4: reward: -157.083, steps: 106
Episode 5: reward: -207.593, steps: 87