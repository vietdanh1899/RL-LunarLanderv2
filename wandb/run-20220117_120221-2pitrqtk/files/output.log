Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_4 (Dense)              (None, 64)                576
_________________________________________________________________
activation_4 (Activation)    (None, 64)                0
_________________________________________________________________
dense_5 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_5 (Activation)    (None, 64)                0
_________________________________________________________________
dense_6 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_6 (Activation)    (None, 64)                0
_________________________________________________________________
dense_7 (Dense)              (None, 4)                 260
_________________________________________________________________
activation_7 (Activation)    (None, 4)                 0
=================================================================
Total params: 9,156
Trainable params: 9,156
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 150000 steps ...
    114/150000: episode: 1, duration: 0.177s, episode steps: 114, steps per second: 645, episode reward: -97.566, mean reward: -0.856 [-100.000,  6.885], mean action: 1.430 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    181/150000: episode: 2, duration: 0.056s, episode steps:  67, steps per second: 1204, episode reward: -169.993, mean reward: -2.537 [-100.000, 13.271], mean action: 1.522 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    299/150000: episode: 3, duration: 0.096s, episode steps: 118, steps per second: 1227, episode reward: -146.663, mean reward: -1.243 [-100.000,  3.155], mean action: 1.585 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    385/150000: episode: 4, duration: 0.063s, episode steps:  86, steps per second: 1360, episode reward: -113.189, mean reward: -1.316 [-100.000, 15.481], mean action: 1.337 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    452/150000: episode: 5, duration: 0.050s, episode steps:  67, steps per second: 1347, episode reward: -109.168, mean reward: -1.629 [-100.000,  7.600], mean action: 1.299 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    539/150000: episode: 6, duration: 0.069s, episode steps:  87, steps per second: 1264, episode reward:  4.773, mean reward:  0.055 [-100.000, 113.333], mean action: 1.414 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    617/150000: episode: 7, duration: 0.060s, episode steps:  78, steps per second: 1299, episode reward: -81.974, mean reward: -1.051 [-100.000, 15.004], mean action: 1.372 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    692/150000: episode: 8, duration: 0.060s, episode steps:  75, steps per second: 1243, episode reward: -101.448, mean reward: -1.353 [-100.000,  5.737], mean action: 1.653 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    786/150000: episode: 9, duration: 0.083s, episode steps:  94, steps per second: 1134, episode reward: -357.752, mean reward: -3.806 [-100.000,  5.808], mean action: 1.543 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    901/150000: episode: 10, duration: 0.092s, episode steps: 115, steps per second: 1249, episode reward: -72.622, mean reward: -0.631 [-100.000, 19.961], mean action: 1.565 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    996/150000: episode: 11, duration: 0.073s, episode steps:  95, steps per second: 1306, episode reward: -106.144, mean reward: -1.117 [-100.000,  6.242], mean action: 1.695 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1062/150000: episode: 12, duration: 0.923s, episode steps:  66, steps per second:  72, episode reward: -167.068, mean reward: -2.531 [-100.000,  6.314], mean action: 1.379 [0.000, 3.000],  loss: 86.929723, mae: 1.028200, mean_q: 0.059500, mean_eps: 0.993814
   1123/150000: episode: 13, duration: 0.345s, episode steps:  61, steps per second: 177, episode reward: -137.376, mean reward: -2.252 [-100.000,  8.836], mean action: 1.410 [0.000, 3.000],  loss: 51.606481, mae: 1.258663, mean_q: 0.154797, mean_eps: 0.993448
   1206/150000: episode: 14, duration: 0.521s, episode steps:  83, steps per second: 159, episode reward: -109.812, mean reward: -1.323 [-100.000,  6.989], mean action: 1.482 [0.000, 3.000],  loss: 54.598714, mae: 1.763324, mean_q: 0.788931, mean_eps: 0.993016
   1263/150000: episode: 15, duration: 0.331s, episode steps:  57, steps per second: 172, episode reward: -92.228, mean reward: -1.618 [-100.000, 13.096], mean action: 1.386 [0.000, 3.000],  loss: 33.496162, mae: 1.925401, mean_q: 1.331828, mean_eps: 0.992596
   1362/150000: episode: 16, duration: 0.610s, episode steps:  99, steps per second: 162, episode reward: -213.759, mean reward: -2.159 [-100.000,  4.494], mean action: 1.636 [0.000, 3.000],  loss: 42.140390, mae: 2.387799, mean_q: 1.627964, mean_eps: 0.992128
   1439/150000: episode: 17, duration: 0.424s, episode steps:  77, steps per second: 182, episode reward: -175.345, mean reward: -2.277 [-100.000,  7.040], mean action: 1.416 [0.000, 3.000],  loss: 46.532599, mae: 2.137267, mean_q: 1.456681, mean_eps: 0.991600
   1508/150000: episode: 18, duration: 0.441s, episode steps:  69, steps per second: 156, episode reward: -82.701, mean reward: -1.199 [-100.000, 16.272], mean action: 1.319 [0.000, 3.000],  loss: 42.367562, mae: 2.133920, mean_q: 1.424257, mean_eps: 0.991162
   1596/150000: episode: 19, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -366.933, mean reward: -4.170 [-100.000,  0.037], mean action: 1.625 [0.000, 3.000],  loss: 51.012118, mae: 2.025816, mean_q: 1.076399, mean_eps: 0.990691
   1663/150000: episode: 20, duration: 0.465s, episode steps:  67, steps per second: 144, episode reward: -81.698, mean reward: -1.219 [-100.000,  6.815], mean action: 1.627 [0.000, 3.000],  loss: 32.451507, mae: 2.001059, mean_q: 0.618198, mean_eps: 0.990226
   1744/150000: episode: 21, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -190.266, mean reward: -2.349 [-100.000,  4.611], mean action: 1.728 [0.000, 3.000],  loss: 39.958937, mae: 2.073550, mean_q: 0.839253, mean_eps: 0.989782
   1831/150000: episode: 22, duration: 0.614s, episode steps:  87, steps per second: 142, episode reward: -101.232, mean reward: -1.164 [-100.000,  5.525], mean action: 1.391 [0.000, 3.000],  loss: 34.787110, mae: 2.119748, mean_q: 0.553939, mean_eps: 0.989278
   1926/150000: episode: 23, duration: 0.683s, episode steps:  95, steps per second: 139, episode reward: -234.170, mean reward: -2.465 [-100.000,  9.830], mean action: 1.632 [0.000, 3.000],  loss: 34.919621, mae: 2.070147, mean_q: 0.820771, mean_eps: 0.988732
   2002/150000: episode: 24, duration: 0.481s, episode steps:  76, steps per second: 158, episode reward: -155.247, mean reward: -2.043 [-100.000,  7.425], mean action: 1.461 [0.000, 3.000],  loss: 31.824432, mae: 2.079628, mean_q: 0.866921, mean_eps: 0.988219
   2061/150000: episode: 25, duration: 0.476s, episode steps:  59, steps per second: 124, episode reward: -132.001, mean reward: -2.237 [-100.000, 17.093], mean action: 1.508 [0.000, 3.000],  loss: 20.330958, mae: 2.419556, mean_q: 0.875394, mean_eps: 0.987814
   2181/150000: episode: 26, duration: 0.828s, episode steps: 120, steps per second: 145, episode reward: -94.601, mean reward: -0.788 [-100.000,  7.693], mean action: 1.683 [0.000, 3.000],  loss: 23.814462, mae: 2.347017, mean_q: 0.872966, mean_eps: 0.987277
   2294/150000: episode: 27, duration: 0.756s, episode steps: 113, steps per second: 149, episode reward: -140.931, mean reward: -1.247 [-100.000, 19.920], mean action: 1.407 [0.000, 3.000],  loss: 28.263789, mae: 2.447852, mean_q: 1.170551, mean_eps: 0.986578
   2373/150000: episode: 28, duration: 0.460s, episode steps:  79, steps per second: 172, episode reward: -105.233, mean reward: -1.332 [-100.000,  6.318], mean action: 1.418 [0.000, 3.000],  loss: 25.380462, mae: 2.654591, mean_q: 1.097933, mean_eps: 0.986002
   2484/150000: episode: 29, duration: 0.624s, episode steps: 111, steps per second: 178, episode reward: -84.463, mean reward: -0.761 [-100.000, 18.385], mean action: 1.586 [0.000, 3.000],  loss: 23.280017, mae: 2.618454, mean_q: 1.132241, mean_eps: 0.985432
   2558/150000: episode: 30, duration: 0.425s, episode steps:  74, steps per second: 174, episode reward: -93.507, mean reward: -1.264 [-100.000,  9.860], mean action: 1.541 [0.000, 3.000],  loss: 15.008101, mae: 2.584575, mean_q: 1.131836, mean_eps: 0.984877
   2616/150000: episode: 31, duration: 0.336s, episode steps:  58, steps per second: 173, episode reward: -183.171, mean reward: -3.158 [-100.000, 32.985], mean action: 1.483 [0.000, 3.000],  loss: 29.257746, mae: 2.824290, mean_q: 0.849551, mean_eps: 0.984481
   2677/150000: episode: 32, duration: 0.418s, episode steps:  61, steps per second: 146, episode reward: -224.965, mean reward: -3.688 [-100.000, 12.687], mean action: 1.344 [0.000, 3.000],  loss: 33.449391, mae: 3.424965, mean_q: 0.277565, mean_eps: 0.984124
   2759/150000: episode: 33, duration: 0.519s, episode steps:  82, steps per second: 158, episode reward: -172.154, mean reward: -2.099 [-100.000, 27.828], mean action: 1.341 [0.000, 3.000],  loss: 30.699666, mae: 2.986832, mean_q: 0.701349, mean_eps: 0.983695
   2853/150000: episode: 34, duration: 0.641s, episode steps:  94, steps per second: 147, episode reward: -292.035, mean reward: -3.107 [-100.000,  4.782], mean action: 1.372 [0.000, 3.000],  loss: 20.737922, mae: 2.910625, mean_q: 0.748905, mean_eps: 0.983167
   2973/150000: episode: 35, duration: 0.710s, episode steps: 120, steps per second: 169, episode reward: -137.556, mean reward: -1.146 [-100.000,  7.470], mean action: 1.492 [0.000, 3.000],  loss: 24.956079, mae: 2.866277, mean_q: 0.663139, mean_eps: 0.982525
   3070/150000: episode: 36, duration: 0.546s, episode steps:  97, steps per second: 178, episode reward: -104.923, mean reward: -1.082 [-100.000,  7.144], mean action: 1.598 [0.000, 3.000],  loss: 18.897778, mae: 3.067683, mean_q: 1.552941, mean_eps: 0.981874
   3168/150000: episode: 37, duration: 0.551s, episode steps:  98, steps per second: 178, episode reward: -538.463, mean reward: -5.495 [-100.000, 79.819], mean action: 1.500 [0.000, 3.000],  loss: 15.417552, mae: 3.693917, mean_q: 1.311422, mean_eps: 0.981289
   3261/150000: episode: 38, duration: 0.519s, episode steps:  93, steps per second: 179, episode reward: -127.608, mean reward: -1.372 [-100.000, 53.527], mean action: 1.484 [0.000, 3.000],  loss: 19.356752, mae: 3.833673, mean_q: 1.400213, mean_eps: 0.980716
   3342/150000: episode: 39, duration: 0.446s, episode steps:  81, steps per second: 182, episode reward: -209.832, mean reward: -2.591 [-100.000,  4.285], mean action: 1.296 [0.000, 3.000],  loss: 14.855180, mae: 3.803859, mean_q: 1.153388, mean_eps: 0.980194
   3406/150000: episode: 40, duration: 0.350s, episode steps:  64, steps per second: 183, episode reward: -167.846, mean reward: -2.623 [-100.000,  8.065], mean action: 1.516 [0.000, 3.000],  loss: 16.976964, mae: 3.807715, mean_q: 1.344652, mean_eps: 0.979759
   3471/150000: episode: 41, duration: 0.359s, episode steps:  65, steps per second: 181, episode reward: -77.623, mean reward: -1.194 [-100.000,  8.035], mean action: 1.262 [0.000, 3.000],  loss: 11.187080, mae: 3.828367, mean_q: 1.122574, mean_eps: 0.979372
   3592/150000: episode: 42, duration: 0.699s, episode steps: 121, steps per second: 173, episode reward: -449.899, mean reward: -3.718 [-100.000,  1.948], mean action: 1.612 [0.000, 3.000],  loss: 14.944316, mae: 3.771494, mean_q: 1.377018, mean_eps: 0.978814
   3705/150000: episode: 43, duration: 0.638s, episode steps: 113, steps per second: 177, episode reward: -142.637, mean reward: -1.262 [-100.000, 11.735], mean action: 1.566 [0.000, 3.000],  loss: 20.028561, mae: 4.172452, mean_q: 0.870696, mean_eps: 0.978112
   3776/150000: episode: 44, duration: 0.412s, episode steps:  71, steps per second: 172, episode reward: -106.325, mean reward: -1.498 [-100.000, 21.843], mean action: 1.507 [0.000, 3.000],  loss: 21.992460, mae: 4.106689, mean_q: 0.893943, mean_eps: 0.977560
   3897/150000: episode: 45, duration: 0.682s, episode steps: 121, steps per second: 177, episode reward: -229.001, mean reward: -1.893 [-100.000,  7.451], mean action: 1.562 [0.000, 3.000],  loss: 22.174763, mae: 3.851198, mean_q: 1.056571, mean_eps: 0.976984
   4018/150000: episode: 46, duration: 0.695s, episode steps: 121, steps per second: 174, episode reward: -268.800, mean reward: -2.221 [-100.000,  6.968], mean action: 1.430 [0.000, 3.000],  loss: 16.656609, mae: 3.885104, mean_q: 0.740591, mean_eps: 0.976258
   4080/150000: episode: 47, duration: 0.342s, episode steps:  62, steps per second: 181, episode reward: -74.372, mean reward: -1.200 [-100.000,  7.177], mean action: 1.484 [0.000, 3.000],  loss: 20.128564, mae: 4.848483, mean_q: 0.891686, mean_eps: 0.975709
   4209/150000: episode: 48, duration: 0.709s, episode steps: 129, steps per second: 182, episode reward: -184.313, mean reward: -1.429 [-100.000, 80.725], mean action: 1.581 [0.000, 3.000],  loss: 15.710721, mae: 5.262572, mean_q: 0.581021, mean_eps: 0.975136
   4306/150000: episode: 49, duration: 0.620s, episode steps:  97, steps per second: 156, episode reward: -275.117, mean reward: -2.836 [-100.000, 75.661], mean action: 1.598 [0.000, 3.000],  loss: 14.089327, mae: 5.003879, mean_q: 0.788332, mean_eps: 0.974458
   4386/150000: episode: 50, duration: 0.539s, episode steps:  80, steps per second: 148, episode reward: -84.945, mean reward: -1.062 [-100.000,  7.490], mean action: 1.425 [0.000, 3.000],  loss: 14.668016, mae: 4.716007, mean_q: 1.397210, mean_eps: 0.973927
   4480/150000: episode: 51, duration: 0.542s, episode steps:  94, steps per second: 173, episode reward: -84.185, mean reward: -0.896 [-100.000, 16.636], mean action: 1.351 [0.000, 3.000],  loss: 20.038684, mae: 5.164190, mean_q: 0.981596, mean_eps: 0.973405
   4593/150000: episode: 52, duration: 0.674s, episode steps: 113, steps per second: 168, episode reward: -126.447, mean reward: -1.119 [-100.000,  7.041], mean action: 1.478 [0.000, 3.000],  loss: 18.654975, mae: 5.066616, mean_q: 0.507692, mean_eps: 0.972784
   4705/150000: episode: 53, duration: 0.665s, episode steps: 112, steps per second: 168, episode reward: -117.861, mean reward: -1.052 [-100.000, 14.994], mean action: 1.607 [0.000, 3.000],  loss: 15.024187, mae: 5.247478, mean_q: 1.147290, mean_eps: 0.972109
   4803/150000: episode: 54, duration: 0.569s, episode steps:  98, steps per second: 172, episode reward: -73.898, mean reward: -0.754 [-100.000, 12.670], mean action: 1.378 [0.000, 3.000],  loss: 20.144896, mae: 5.280005, mean_q: 0.706035, mean_eps: 0.971479
   4906/150000: episode: 55, duration: 0.614s, episode steps: 103, steps per second: 168, episode reward: -403.392, mean reward: -3.916 [-100.000, 70.823], mean action: 1.184 [0.000, 3.000],  loss: 14.461074, mae: 5.317086, mean_q: 0.839119, mean_eps: 0.970876
   5016/150000: episode: 56, duration: 0.639s, episode steps: 110, steps per second: 172, episode reward: -89.827, mean reward: -0.817 [-100.000, 13.183], mean action: 1.618 [0.000, 3.000],  loss: 18.777399, mae: 5.279352, mean_q: 1.011338, mean_eps: 0.970237
   5086/150000: episode: 57, duration: 0.408s, episode steps:  70, steps per second: 172, episode reward: -89.344, mean reward: -1.276 [-100.000,  6.058], mean action: 1.529 [0.000, 3.000],  loss: 13.255121, mae: 6.416783, mean_q: 0.876043, mean_eps: 0.969697
   5186/150000: episode: 58, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -185.544, mean reward: -1.855 [-100.000,  7.364], mean action: 1.370 [0.000, 3.000],  loss: 14.917135, mae: 6.351728, mean_q: 1.317495, mean_eps: 0.969187
   5308/150000: episode: 59, duration: 0.669s, episode steps: 122, steps per second: 182, episode reward: -342.407, mean reward: -2.807 [-100.000,  3.448], mean action: 1.656 [0.000, 3.000],  loss: 10.562892, mae: 6.588068, mean_q: 0.770606, mean_eps: 0.968521
   5414/150000: episode: 60, duration: 0.581s, episode steps: 106, steps per second: 183, episode reward: -117.483, mean reward: -1.108 [-100.000, 21.620], mean action: 1.717 [0.000, 3.000],  loss: 12.784419, mae: 6.621489, mean_q: 0.660882, mean_eps: 0.967837
   5502/150000: episode: 61, duration: 0.508s, episode steps:  88, steps per second: 173, episode reward: -105.468, mean reward: -1.199 [-100.000, 11.493], mean action: 1.284 [0.000, 3.000],  loss: 12.816081, mae: 6.523640, mean_q: 0.597055, mean_eps: 0.967255
   5581/150000: episode: 62, duration: 0.434s, episode steps:  79, steps per second: 182, episode reward: -156.771, mean reward: -1.984 [-100.000, 21.685], mean action: 1.468 [0.000, 3.000],  loss: 17.734985, mae: 6.827656, mean_q: 0.386733, mean_eps: 0.966754
   5650/150000: episode: 63, duration: 0.379s, episode steps:  69, steps per second: 182, episode reward: -123.398, mean reward: -1.788 [-100.000,  4.429], mean action: 1.391 [0.000, 3.000],  loss: 17.314574, mae: 6.232468, mean_q: 1.255806, mean_eps: 0.966310
   5752/150000: episode: 64, duration: 0.558s, episode steps: 102, steps per second: 183, episode reward: -100.134, mean reward: -0.982 [-100.000,  8.828], mean action: 1.461 [0.000, 3.000],  loss: 16.047078, mae: 6.249698, mean_q: 0.798895, mean_eps: 0.965797
   5874/150000: episode: 65, duration: 0.704s, episode steps: 122, steps per second: 173, episode reward: -18.430, mean reward: -0.151 [-100.000, 105.184], mean action: 1.648 [0.000, 3.000],  loss: 10.186511, mae: 5.960472, mean_q: 0.943928, mean_eps: 0.965125
   5950/150000: episode: 66, duration: 0.415s, episode steps:  76, steps per second: 183, episode reward: -109.693, mean reward: -1.443 [-100.000, 15.423], mean action: 1.263 [0.000, 3.000],  loss: 12.181650, mae: 5.941364, mean_q: 0.829869, mean_eps: 0.964531
   6032/150000: episode: 67, duration: 0.461s, episode steps:  82, steps per second: 178, episode reward: -150.930, mean reward: -1.841 [-100.000,  8.084], mean action: 1.451 [0.000, 3.000],  loss: 13.399281, mae: 6.566444, mean_q: 1.534227, mean_eps: 0.964057
   6132/150000: episode: 68, duration: 0.542s, episode steps: 100, steps per second: 185, episode reward: -113.564, mean reward: -1.136 [-100.000,  5.989], mean action: 1.340 [0.000, 3.000],  loss: 12.099667, mae: 7.878085, mean_q: 2.491643, mean_eps: 0.963511
   6255/150000: episode: 69, duration: 0.730s, episode steps: 123, steps per second: 169, episode reward: -320.597, mean reward: -2.606 [-100.000, 42.743], mean action: 1.561 [0.000, 3.000],  loss: 17.190573, mae: 7.584828, mean_q: 2.969096, mean_eps: 0.962842
   6376/150000: episode: 70, duration: 0.712s, episode steps: 121, steps per second: 170, episode reward: -442.440, mean reward: -3.657 [-100.000,  1.247], mean action: 1.504 [0.000, 3.000],  loss: 11.034539, mae: 7.941120, mean_q: 2.439070, mean_eps: 0.962110
   6516/150000: episode: 71, duration: 0.766s, episode steps: 140, steps per second: 183, episode reward: -198.428, mean reward: -1.417 [-100.000,  6.094], mean action: 1.479 [0.000, 3.000],  loss: 9.331248, mae: 7.752668, mean_q: 2.533389, mean_eps: 0.961327
   6634/150000: episode: 72, duration: 0.672s, episode steps: 118, steps per second: 175, episode reward: -200.421, mean reward: -1.698 [-100.000,  8.822], mean action: 1.559 [0.000, 3.000],  loss: 15.272083, mae: 7.504668, mean_q: 2.713833, mean_eps: 0.960553
   6702/150000: episode: 73, duration: 0.403s, episode steps:  68, steps per second: 169, episode reward: -279.693, mean reward: -4.113 [-100.000, 24.370], mean action: 1.706 [0.000, 3.000],  loss: 7.374064, mae: 7.359271, mean_q: 2.594401, mean_eps: 0.959995
   6765/150000: episode: 74, duration: 0.356s, episode steps:  63, steps per second: 177, episode reward: -60.191, mean reward: -0.955 [-100.000, 12.028], mean action: 1.476 [0.000, 3.000],  loss: 11.558107, mae: 7.488877, mean_q: 3.126691, mean_eps: 0.959602
   6905/150000: episode: 75, duration: 0.766s, episode steps: 140, steps per second: 183, episode reward: -349.652, mean reward: -2.498 [-100.000,  2.694], mean action: 1.371 [0.000, 3.000],  loss: 12.261824, mae: 7.566006, mean_q: 2.787838, mean_eps: 0.958993
   7054/150000: episode: 76, duration: 0.882s, episode steps: 149, steps per second: 169, episode reward: -88.185, mean reward: -0.592 [-100.000, 107.055], mean action: 1.503 [0.000, 3.000],  loss: 11.030904, mae: 8.290235, mean_q: 2.186703, mean_eps: 0.958126
   7158/150000: episode: 77, duration: 0.598s, episode steps: 104, steps per second: 174, episode reward: -121.261, mean reward: -1.166 [-100.000, 14.972], mean action: 1.510 [0.000, 3.000],  loss: 21.171723, mae: 9.185525, mean_q: 2.469015, mean_eps: 0.957367
   7235/150000: episode: 78, duration: 0.429s, episode steps:  77, steps per second: 179, episode reward: -164.804, mean reward: -2.140 [-100.000,  6.278], mean action: 1.506 [0.000, 3.000],  loss: 12.668961, mae: 9.041330, mean_q: 2.656164, mean_eps: 0.956824
   7325/150000: episode: 79, duration: 0.507s, episode steps:  90, steps per second: 178, episode reward: -96.359, mean reward: -1.071 [-100.000, 20.508], mean action: 1.589 [0.000, 3.000],  loss: 8.781264, mae: 8.794572, mean_q: 2.528815, mean_eps: 0.956323
   7460/150000: episode: 80, duration: 0.809s, episode steps: 135, steps per second: 167, episode reward: -196.492, mean reward: -1.455 [-100.000,  7.947], mean action: 1.585 [0.000, 3.000],  loss: 10.891135, mae: 8.927687, mean_q: 2.472165, mean_eps: 0.955648
   7567/150000: episode: 81, duration: 0.605s, episode steps: 107, steps per second: 177, episode reward: -223.850, mean reward: -2.092 [-100.000,  0.541], mean action: 1.589 [0.000, 3.000],  loss: 13.384092, mae: 8.974308, mean_q: 2.577123, mean_eps: 0.954922
   7625/150000: episode: 82, duration: 0.322s, episode steps:  58, steps per second: 180, episode reward: -183.283, mean reward: -3.160 [-100.000,  5.499], mean action: 1.534 [0.000, 3.000],  loss: 12.867700, mae: 8.740519, mean_q: 2.357048, mean_eps: 0.954427
   7729/150000: episode: 83, duration: 0.574s, episode steps: 104, steps per second: 181, episode reward: -243.459, mean reward: -2.341 [-100.000, 12.839], mean action: 1.404 [0.000, 3.000],  loss: 9.417887, mae: 8.657131, mean_q: 2.414645, mean_eps: 0.953941
   7847/150000: episode: 84, duration: 0.678s, episode steps: 118, steps per second: 174, episode reward: -231.668, mean reward: -1.963 [-100.000, 12.137], mean action: 1.458 [0.000, 3.000],  loss: 12.953861, mae: 8.737715, mean_q: 2.393349, mean_eps: 0.953275
   7918/150000: episode: 85, duration: 0.430s, episode steps:  71, steps per second: 165, episode reward: -97.150, mean reward: -1.368 [-100.000, 38.079], mean action: 1.803 [0.000, 3.000],  loss: 11.301180, mae: 9.599941, mean_q: 1.501249, mean_eps: 0.952708
   8027/150000: episode: 86, duration: 0.594s, episode steps: 109, steps per second: 184, episode reward: -129.874, mean reward: -1.192 [-100.000, 12.003], mean action: 1.349 [0.000, 3.000],  loss: 9.907726, mae: 8.663852, mean_q: 2.194066, mean_eps: 0.952168
   8144/150000: episode: 87, duration: 0.701s, episode steps: 117, steps per second: 167, episode reward: -171.871, mean reward: -1.469 [-100.000, 34.954], mean action: 1.581 [0.000, 3.000],  loss: 12.602060, mae: 9.434329, mean_q: 2.370425, mean_eps: 0.951490
   8209/150000: episode: 88, duration: 0.367s, episode steps:  65, steps per second: 177, episode reward: -89.714, mean reward: -1.380 [-100.000, 20.003], mean action: 1.292 [0.000, 3.000],  loss: 8.028983, mae: 9.871395, mean_q: 1.892231, mean_eps: 0.950944
   8288/150000: episode: 89, duration: 0.459s, episode steps:  79, steps per second: 172, episode reward: -96.162, mean reward: -1.217 [-100.000, 14.227], mean action: 1.570 [0.000, 3.000],  loss: 12.086084, mae: 9.737315, mean_q: 2.335566, mean_eps: 0.950512
   8371/150000: episode: 90, duration: 0.463s, episode steps:  83, steps per second: 179, episode reward: -146.427, mean reward: -1.764 [-100.000, 27.921], mean action: 1.518 [0.000, 3.000],  loss: 11.185787, mae: 9.436090, mean_q: 2.324586, mean_eps: 0.950026
   8470/150000: episode: 91, duration: 0.559s, episode steps:  99, steps per second: 177, episode reward: -223.800, mean reward: -2.261 [-100.000,  7.857], mean action: 1.586 [0.000, 3.000],  loss: 16.337180, mae: 10.263542, mean_q: 1.415903, mean_eps: 0.949480
   8553/150000: episode: 92, duration: 0.500s, episode steps:  83, steps per second: 166, episode reward: 24.847, mean reward:  0.299 [-100.000, 124.583], mean action: 1.410 [0.000, 3.000],  loss: 13.914588, mae: 9.365445, mean_q: 2.259490, mean_eps: 0.948934
   8646/150000: episode: 93, duration: 0.522s, episode steps:  93, steps per second: 178, episode reward: -262.063, mean reward: -2.818 [-100.000,  2.370], mean action: 1.538 [0.000, 3.000],  loss: 10.180622, mae: 9.338147, mean_q: 2.093724, mean_eps: 0.948406
   8774/150000: episode: 94, duration: 0.717s, episode steps: 128, steps per second: 178, episode reward: -220.488, mean reward: -1.723 [-100.000,  4.277], mean action: 1.516 [0.000, 3.000],  loss: 9.194307, mae: 9.591048, mean_q: 2.163985, mean_eps: 0.947743
   8868/150000: episode: 95, duration: 0.534s, episode steps:  94, steps per second: 176, episode reward: -100.472, mean reward: -1.069 [-100.000,  7.926], mean action: 1.500 [0.000, 3.000],  loss: 19.625277, mae: 9.730731, mean_q: 2.156775, mean_eps: 0.947077
   8924/150000: episode: 96, duration: 0.337s, episode steps:  56, steps per second: 166, episode reward: -110.124, mean reward: -1.966 [-100.000, 10.680], mean action: 1.500 [0.000, 3.000],  loss: 8.229672, mae: 9.820334, mean_q: 1.690754, mean_eps: 0.946627
   9019/150000: episode: 97, duration: 0.533s, episode steps:  95, steps per second: 178, episode reward: -212.988, mean reward: -2.242 [-100.000,  7.341], mean action: 1.663 [0.000, 3.000],  loss: 12.004174, mae: 9.854806, mean_q: 2.015957, mean_eps: 0.946174
   9131/150000: episode: 98, duration: 0.641s, episode steps: 112, steps per second: 175, episode reward: -92.543, mean reward: -0.826 [-100.000, 12.063], mean action: 1.562 [0.000, 3.000],  loss: 9.913259, mae: 10.930795, mean_q: 1.225426, mean_eps: 0.945553
   9251/150000: episode: 99, duration: 0.694s, episode steps: 120, steps per second: 173, episode reward: -181.076, mean reward: -1.509 [-100.000, 20.915], mean action: 1.517 [0.000, 3.000],  loss: 12.989040, mae: 10.748015, mean_q: 2.308224, mean_eps: 0.944857
   9360/150000: episode: 100, duration: 0.623s, episode steps: 109, steps per second: 175, episode reward: -339.815, mean reward: -3.118 [-100.000, 78.110], mean action: 1.578 [0.000, 3.000],  loss: 10.738230, mae: 10.615107, mean_q: 2.108748, mean_eps: 0.944170
   9464/150000: episode: 101, duration: 0.584s, episode steps: 104, steps per second: 178, episode reward: -19.481, mean reward: -0.187 [-100.000, 96.071], mean action: 1.471 [0.000, 3.000],  loss: 9.080080, mae: 10.623139, mean_q: 2.303881, mean_eps: 0.943531
   9551/150000: episode: 102, duration: 0.486s, episode steps:  87, steps per second: 179, episode reward: -125.211, mean reward: -1.439 [-100.000,  8.721], mean action: 1.299 [0.000, 3.000],  loss: 17.941932, mae: 11.419690, mean_q: 1.326138, mean_eps: 0.942958
   9655/150000: episode: 103, duration: 0.595s, episode steps: 104, steps per second: 175, episode reward: -66.891, mean reward: -0.643 [-100.000, 96.169], mean action: 1.548 [0.000, 3.000],  loss: 8.829450, mae: 10.627547, mean_q: 1.955003, mean_eps: 0.942385
   9724/150000: episode: 104, duration: 0.412s, episode steps:  69, steps per second: 168, episode reward: -104.655, mean reward: -1.517 [-100.000, 43.566], mean action: 1.478 [0.000, 3.000],  loss: 14.625495, mae: 10.893506, mean_q: 1.734789, mean_eps: 0.941866
   9820/150000: episode: 105, duration: 0.551s, episode steps:  96, steps per second: 174, episode reward: -156.033, mean reward: -1.625 [-100.000,  7.459], mean action: 1.406 [0.000, 3.000],  loss: 9.547080, mae: 10.614347, mean_q: 1.601263, mean_eps: 0.941371
   9904/150000: episode: 106, duration: 0.482s, episode steps:  84, steps per second: 174, episode reward: -64.905, mean reward: -0.773 [-100.000, 58.203], mean action: 1.631 [0.000, 3.000],  loss: 9.220379, mae: 10.692306, mean_q: 2.227641, mean_eps: 0.940831
   9974/150000: episode: 107, duration: 0.390s, episode steps:  70, steps per second: 179, episode reward: -76.863, mean reward: -1.098 [-100.000, 13.326], mean action: 1.514 [0.000, 3.000],  loss: 6.587692, mae: 10.370038, mean_q: 1.964800, mean_eps: 0.940369
  10082/150000: episode: 108, duration: 0.646s, episode steps: 108, steps per second: 167, episode reward: -77.973, mean reward: -0.722 [-100.000,  7.851], mean action: 1.454 [0.000, 3.000],  loss: 19.020258, mae: 11.410620, mean_q: 1.559470, mean_eps: 0.939835
  10148/150000: episode: 109, duration: 0.372s, episode steps:  66, steps per second: 177, episode reward: -153.053, mean reward: -2.319 [-100.000,  5.414], mean action: 1.561 [0.000, 3.000],  loss: 17.880380, mae: 11.787726, mean_q: 1.077814, mean_eps: 0.939313
  10275/150000: episode: 110, duration: 0.708s, episode steps: 127, steps per second: 180, episode reward: -124.582, mean reward: -0.981 [-100.000,  6.331], mean action: 1.465 [0.000, 3.000],  loss: 14.523519, mae: 11.303063, mean_q: 2.567113, mean_eps: 0.938734
  10382/150000: episode: 111, duration: 0.627s, episode steps: 107, steps per second: 171, episode reward: -224.975, mean reward: -2.103 [-100.000, 75.174], mean action: 1.402 [0.000, 3.000],  loss: 13.883353, mae: 11.465918, mean_q: 1.699511, mean_eps: 0.938032
  10474/150000: episode: 112, duration: 0.530s, episode steps:  92, steps per second: 174, episode reward: -43.420, mean reward: -0.472 [-100.000,  8.880], mean action: 1.554 [0.000, 3.000],  loss: 8.882032, mae: 11.675922, mean_q: 1.520571, mean_eps: 0.937435
  10581/150000: episode: 113, duration: 0.604s, episode steps: 107, steps per second: 177, episode reward: -113.063, mean reward: -1.057 [-100.000, 38.771], mean action: 1.477 [0.000, 3.000],  loss: 14.606969, mae: 12.275734, mean_q: 1.082910, mean_eps: 0.936838
  10698/150000: episode: 114, duration: 0.654s, episode steps: 117, steps per second: 179, episode reward: -98.939, mean reward: -0.846 [-100.000, 17.412], mean action: 1.436 [0.000, 3.000],  loss: 15.814212, mae: 12.133789, mean_q: 1.075577, mean_eps: 0.936166
  10804/150000: episode: 115, duration: 0.615s, episode steps: 106, steps per second: 172, episode reward: -122.438, mean reward: -1.155 [-100.000,  6.409], mean action: 1.538 [0.000, 3.000],  loss: 12.069058, mae: 11.751516, mean_q: 1.872402, mean_eps: 0.935497
  10887/150000: episode: 116, duration: 0.481s, episode steps:  83, steps per second: 173, episode reward: -102.090, mean reward: -1.230 [-100.000,  5.740], mean action: 1.554 [0.000, 3.000],  loss: 20.220106, mae: 11.497025, mean_q: 1.765265, mean_eps: 0.934930
  10988/150000: episode: 117, duration: 0.565s, episode steps: 101, steps per second: 179, episode reward: -132.211, mean reward: -1.309 [-100.000,  4.578], mean action: 1.485 [0.000, 3.000],  loss: 8.530084, mae: 12.024222, mean_q: 1.439618, mean_eps: 0.934378
  11083/150000: episode: 118, duration: 0.538s, episode steps:  95, steps per second: 176, episode reward: -100.943, mean reward: -1.063 [-100.000,  9.086], mean action: 1.389 [0.000, 3.000],  loss: 18.620543, mae: 12.534366, mean_q: 1.813981, mean_eps: 0.933790
  11174/150000: episode: 119, duration: 0.535s, episode steps:  91, steps per second: 170, episode reward: -118.935, mean reward: -1.307 [-100.000,  9.404], mean action: 1.429 [0.000, 3.000],  loss: 17.826837, mae: 12.356590, mean_q: 2.524850, mean_eps: 0.933232
  11285/150000: episode: 120, duration: 0.632s, episode steps: 111, steps per second: 176, episode reward: -158.604, mean reward: -1.429 [-100.000,  8.653], mean action: 1.667 [0.000, 3.000],  loss: 12.319347, mae: 12.496566, mean_q: 2.620456, mean_eps: 0.932626
  11406/150000: episode: 121, duration: 0.671s, episode steps: 121, steps per second: 180, episode reward: -495.556, mean reward: -4.096 [-100.000, 18.412], mean action: 1.529 [0.000, 3.000],  loss: 14.121925, mae: 12.972589, mean_q: 1.792105, mean_eps: 0.931930
  11524/150000: episode: 122, duration: 0.803s, episode steps: 118, steps per second: 147, episode reward: -226.856, mean reward: -1.923 [-100.000,  5.651], mean action: 1.475 [0.000, 3.000],  loss: 15.251460, mae: 12.678295, mean_q: 2.191026, mean_eps: 0.931213
  11629/150000: episode: 123, duration: 0.682s, episode steps: 105, steps per second: 154, episode reward: -91.334, mean reward: -0.870 [-100.000,  9.058], mean action: 1.505 [0.000, 3.000],  loss: 12.821341, mae: 12.481468, mean_q: 1.948127, mean_eps: 0.930544
  11740/150000: episode: 124, duration: 0.693s, episode steps: 111, steps per second: 160, episode reward: -160.511, mean reward: -1.446 [-100.000, 27.859], mean action: 1.495 [0.000, 3.000],  loss: 17.765675, mae: 12.411679, mean_q: 2.482895, mean_eps: 0.929896
  11819/150000: episode: 125, duration: 0.492s, episode steps:  79, steps per second: 161, episode reward: -208.153, mean reward: -2.635 [-100.000,  5.484], mean action: 1.646 [0.000, 3.000],  loss: 16.765061, mae: 13.207483, mean_q: 1.830927, mean_eps: 0.929326
  11953/150000: episode: 126, duration: 0.872s, episode steps: 134, steps per second: 154, episode reward: -179.815, mean reward: -1.342 [-100.000,  8.387], mean action: 1.604 [0.000, 3.000],  loss: 9.689168, mae: 12.403786, mean_q: 2.382016, mean_eps: 0.928687
  12018/150000: episode: 127, duration: 0.380s, episode steps:  65, steps per second: 171, episode reward: -53.882, mean reward: -0.829 [-100.000, 16.625], mean action: 1.277 [0.000, 3.000],  loss: 11.913895, mae: 12.574863, mean_q: 3.114086, mean_eps: 0.928090
  12148/150000: episode: 128, duration: 0.741s, episode steps: 130, steps per second: 175, episode reward: -127.887, mean reward: -0.984 [-100.000,  8.925], mean action: 1.546 [0.000, 3.000],  loss: 15.050425, mae: 13.883506, mean_q: 2.215030, mean_eps: 0.927505
  12237/150000: episode: 129, duration: 0.553s, episode steps:  89, steps per second: 161, episode reward: -362.745, mean reward: -4.076 [-100.000, 21.584], mean action: 1.270 [0.000, 3.000],  loss: 10.817808, mae: 13.153245, mean_q: 3.089001, mean_eps: 0.926848
  12356/150000: episode: 130, duration: 0.685s, episode steps: 119, steps per second: 174, episode reward: -303.920, mean reward: -2.554 [-100.000,  4.477], mean action: 1.546 [0.000, 3.000],  loss: 14.292453, mae: 13.584590, mean_q: 2.862130, mean_eps: 0.926224
  12432/150000: episode: 131, duration: 0.430s, episode steps:  76, steps per second: 177, episode reward: -93.939, mean reward: -1.236 [-100.000,  6.248], mean action: 1.553 [0.000, 3.000],  loss: 15.182224, mae: 13.763318, mean_q: 1.735739, mean_eps: 0.925639
  12498/150000: episode: 132, duration: 0.377s, episode steps:  66, steps per second: 175, episode reward: -104.891, mean reward: -1.589 [-100.000, 19.217], mean action: 1.455 [0.000, 3.000],  loss: 10.108020, mae: 14.088191, mean_q: 1.786630, mean_eps: 0.925213
  12601/150000: episode: 133, duration: 0.600s, episode steps: 103, steps per second: 172, episode reward: -148.138, mean reward: -1.438 [-100.000, 61.234], mean action: 1.553 [0.000, 3.000],  loss: 9.302338, mae: 13.932851, mean_q: 2.071524, mean_eps: 0.924706
  12716/150000: episode: 134, duration: 0.666s, episode steps: 115, steps per second: 173, episode reward: -378.464, mean reward: -3.291 [-100.000,  1.847], mean action: 1.678 [0.000, 3.000],  loss: 15.583263, mae: 13.272434, mean_q: 2.480015, mean_eps: 0.924052
  12800/150000: episode: 135, duration: 0.467s, episode steps:  84, steps per second: 180, episode reward: -119.791, mean reward: -1.426 [-100.000,  6.154], mean action: 1.393 [0.000, 3.000],  loss: 12.786739, mae: 13.725545, mean_q: 2.044907, mean_eps: 0.923455
  12887/150000: episode: 136, duration: 0.481s, episode steps:  87, steps per second: 181, episode reward: -130.201, mean reward: -1.497 [-100.000,  6.174], mean action: 1.598 [0.000, 3.000],  loss: 9.486730, mae: 13.734886, mean_q: 2.226889, mean_eps: 0.922942
  12976/150000: episode: 137, duration: 0.522s, episode steps:  89, steps per second: 170, episode reward: -61.640, mean reward: -0.693 [-100.000, 30.072], mean action: 1.539 [0.000, 3.000],  loss: 14.583147, mae: 13.807184, mean_q: 2.927577, mean_eps: 0.922414
  13087/150000: episode: 138, duration: 0.656s, episode steps: 111, steps per second: 169, episode reward: -252.509, mean reward: -2.275 [-100.000,  8.929], mean action: 1.369 [0.000, 3.000],  loss: 12.124745, mae: 13.986604, mean_q: 2.592155, mean_eps: 0.921814
  13151/150000: episode: 139, duration: 0.364s, episode steps:  64, steps per second: 176, episode reward: -34.065, mean reward: -0.532 [-100.000, 56.612], mean action: 1.531 [0.000, 3.000],  loss: 16.361288, mae: 15.102970, mean_q: 2.302466, mean_eps: 0.921289
  13266/150000: episode: 140, duration: 0.642s, episode steps: 115, steps per second: 179, episode reward: -110.960, mean reward: -0.965 [-100.000, 16.914], mean action: 1.539 [0.000, 3.000],  loss: 9.990568, mae: 14.108250, mean_q: 2.694543, mean_eps: 0.920752
  13374/150000: episode: 141, duration: 0.653s, episode steps: 108, steps per second: 165, episode reward: -108.317, mean reward: -1.003 [-100.000, 14.695], mean action: 1.389 [0.000, 3.000],  loss: 19.164465, mae: 14.360909, mean_q: 3.469427, mean_eps: 0.920083
  13463/150000: episode: 142, duration: 0.508s, episode steps:  89, steps per second: 175, episode reward: -116.001, mean reward: -1.303 [-100.000,  6.457], mean action: 1.506 [0.000, 3.000],  loss: 15.096517, mae: 14.617381, mean_q: 2.576389, mean_eps: 0.919492
  13542/150000: episode: 143, duration: 0.451s, episode steps:  79, steps per second: 175, episode reward: -83.596, mean reward: -1.058 [-100.000, 15.814], mean action: 1.342 [0.000, 3.000],  loss: 17.896156, mae: 13.981942, mean_q: 3.055023, mean_eps: 0.918988
  13611/150000: episode: 144, duration: 0.398s, episode steps:  69, steps per second: 173, episode reward: -84.609, mean reward: -1.226 [-100.000,  7.572], mean action: 1.507 [0.000, 3.000],  loss: 17.149902, mae: 14.257034, mean_q: 3.105455, mean_eps: 0.918544
  13731/150000: episode: 145, duration: 0.702s, episode steps: 120, steps per second: 171, episode reward: -117.812, mean reward: -0.982 [-100.000,  6.273], mean action: 1.383 [0.000, 3.000],  loss: 10.412401, mae: 13.961672, mean_q: 3.368485, mean_eps: 0.917977
  13814/150000: episode: 146, duration: 0.473s, episode steps:  83, steps per second: 175, episode reward: -170.178, mean reward: -2.050 [-100.000, 12.961], mean action: 1.566 [0.000, 3.000],  loss: 15.869254, mae: 14.407797, mean_q: 2.637310, mean_eps: 0.917368
  13921/150000: episode: 147, duration: 0.598s, episode steps: 107, steps per second: 179, episode reward: -172.903, mean reward: -1.616 [-100.000,  4.384], mean action: 1.495 [0.000, 3.000],  loss: 13.387121, mae: 14.471449, mean_q: 2.036209, mean_eps: 0.916798
  14016/150000: episode: 148, duration: 0.533s, episode steps:  95, steps per second: 178, episode reward: -115.572, mean reward: -1.217 [-100.000, 48.637], mean action: 1.421 [0.000, 3.000],  loss: 8.604846, mae: 14.151080, mean_q: 3.455945, mean_eps: 0.916192
  14126/150000: episode: 149, duration: 0.653s, episode steps: 110, steps per second: 168, episode reward: -102.412, mean reward: -0.931 [-100.000,  6.298], mean action: 1.655 [0.000, 3.000],  loss: 19.217375, mae: 15.634106, mean_q: 2.447961, mean_eps: 0.915577
  14193/150000: episode: 150, duration: 0.377s, episode steps:  67, steps per second: 178, episode reward: -113.126, mean reward: -1.688 [-100.000, 14.462], mean action: 1.642 [0.000, 3.000],  loss: 12.637739, mae: 14.830844, mean_q: 3.588672, mean_eps: 0.915046
  14262/150000: episode: 151, duration: 0.405s, episode steps:  69, steps per second: 170, episode reward: -146.333, mean reward: -2.121 [-100.000,  6.596], mean action: 1.493 [0.000, 3.000],  loss: 15.640746, mae: 15.152783, mean_q: 2.673979, mean_eps: 0.914638
  14383/150000: episode: 152, duration: 0.668s, episode steps: 121, steps per second: 181, episode reward: -39.709, mean reward: -0.328 [-100.000, 22.146], mean action: 1.562 [0.000, 3.000],  loss: 18.046849, mae: 15.123835, mean_q: 3.190719, mean_eps: 0.914068
  14516/150000: episode: 153, duration: 0.776s, episode steps: 133, steps per second: 171, episode reward: -125.502, mean reward: -0.944 [-100.000,  7.121], mean action: 1.481 [0.000, 3.000],  loss: 12.855591, mae: 15.433910, mean_q: 2.580640, mean_eps: 0.913306
  14590/150000: episode: 154, duration: 0.416s, episode steps:  74, steps per second: 178, episode reward: -186.867, mean reward: -2.525 [-100.000, 25.551], mean action: 1.419 [0.000, 3.000],  loss: 16.171714, mae: 15.604099, mean_q: 1.955497, mean_eps: 0.912685
  14649/150000: episode: 155, duration: 0.343s, episode steps:  59, steps per second: 172, episode reward: -110.633, mean reward: -1.875 [-100.000,  7.870], mean action: 1.712 [0.000, 3.000],  loss: 16.660694, mae: 15.617083, mean_q: 2.433429, mean_eps: 0.912286
  14729/150000: episode: 156, duration: 0.454s, episode steps:  80, steps per second: 176, episode reward: -69.681, mean reward: -0.871 [-100.000, 19.562], mean action: 1.337 [0.000, 3.000],  loss: 13.833236, mae: 15.169047, mean_q: 2.709858, mean_eps: 0.911869
  14831/150000: episode: 157, duration: 0.598s, episode steps: 102, steps per second: 171, episode reward: -380.998, mean reward: -3.735 [-100.000,  0.418], mean action: 1.392 [0.000, 3.000],  loss: 11.667879, mae: 15.366947, mean_q: 2.021707, mean_eps: 0.911323
  14940/150000: episode: 158, duration: 0.654s, episode steps: 109, steps per second: 167, episode reward: -166.090, mean reward: -1.524 [-100.000, 23.728], mean action: 1.734 [0.000, 3.000],  loss: 16.638248, mae: 15.507489, mean_q: 2.513300, mean_eps: 0.910690
  15046/150000: episode: 159, duration: 0.625s, episode steps: 106, steps per second: 170, episode reward: -224.406, mean reward: -2.117 [-100.000,  0.940], mean action: 1.434 [0.000, 3.000],  loss: 14.538429, mae: 15.836266, mean_q: 2.579202, mean_eps: 0.910045
  15166/150000: episode: 160, duration: 0.715s, episode steps: 120, steps per second: 168, episode reward: -39.988, mean reward: -0.333 [-100.000, 13.631], mean action: 1.617 [0.000, 3.000],  loss: 17.353826, mae: 15.794821, mean_q: 3.104627, mean_eps: 0.909367
  15237/150000: episode: 161, duration: 0.451s, episode steps:  71, steps per second: 157, episode reward: -101.483, mean reward: -1.429 [-100.000, 10.257], mean action: 1.662 [0.000, 3.000],  loss: 13.402959, mae: 15.945398, mean_q: 2.940302, mean_eps: 0.908794
  15329/150000: episode: 162, duration: 0.522s, episode steps:  92, steps per second: 176, episode reward: -103.297, mean reward: -1.123 [-100.000,  7.364], mean action: 1.620 [0.000, 3.000],  loss: 13.776705, mae: 16.351069, mean_q: 3.126481, mean_eps: 0.908305
  15424/150000: episode: 163, duration: 0.528s, episode steps:  95, steps per second: 180, episode reward: -148.398, mean reward: -1.562 [-100.000,  8.474], mean action: 1.600 [0.000, 3.000],  loss: 11.622550, mae: 15.843279, mean_q: 3.585875, mean_eps: 0.907744
  15539/150000: episode: 164, duration: 0.711s, episode steps: 115, steps per second: 162, episode reward: -143.666, mean reward: -1.249 [-100.000, 11.986], mean action: 1.400 [0.000, 3.000],  loss: 12.150883, mae: 15.508565, mean_q: 4.290125, mean_eps: 0.907114
  15613/150000: episode: 165, duration: 0.533s, episode steps:  74, steps per second: 139, episode reward: -80.225, mean reward: -1.084 [-100.000,  9.742], mean action: 1.662 [0.000, 3.000],  loss: 15.358909, mae: 15.808561, mean_q: 3.735328, mean_eps: 0.906547
  15692/150000: episode: 166, duration: 0.533s, episode steps:  79, steps per second: 148, episode reward: -102.672, mean reward: -1.300 [-100.000,  7.333], mean action: 1.291 [0.000, 3.000],  loss: 14.009677, mae: 15.591630, mean_q: 3.471421, mean_eps: 0.906088
  15772/150000: episode: 167, duration: 0.517s, episode steps:  80, steps per second: 155, episode reward: -65.856, mean reward: -0.823 [-100.000, 17.617], mean action: 1.575 [0.000, 3.000],  loss: 16.113491, mae: 16.134021, mean_q: 3.255635, mean_eps: 0.905611
  15892/150000: episode: 168, duration: 0.790s, episode steps: 120, steps per second: 152, episode reward: -206.172, mean reward: -1.718 [-100.000, 19.476], mean action: 1.592 [0.000, 3.000],  loss: 13.792513, mae: 16.115513, mean_q: 3.361705, mean_eps: 0.905011
  15994/150000: episode: 169, duration: 0.671s, episode steps: 102, steps per second: 152, episode reward: -60.420, mean reward: -0.592 [-100.000, 85.460], mean action: 1.451 [0.000, 3.000],  loss: 19.175332, mae: 16.151307, mean_q: 3.885852, mean_eps: 0.904345
  16133/150000: episode: 170, duration: 0.838s, episode steps: 139, steps per second: 166, episode reward: -274.144, mean reward: -1.972 [-100.000,  6.650], mean action: 1.468 [0.000, 3.000],  loss: 11.275907, mae: 15.755767, mean_q: 4.259936, mean_eps: 0.903622
  16223/150000: episode: 171, duration: 0.621s, episode steps:  90, steps per second: 145, episode reward: -212.736, mean reward: -2.364 [-100.000, 59.841], mean action: 1.667 [0.000, 3.000],  loss: 12.136667, mae: 15.067387, mean_q: 4.514397, mean_eps: 0.902935
  16360/150000: episode: 172, duration: 0.881s, episode steps: 137, steps per second: 156, episode reward: -79.322, mean reward: -0.579 [-100.000, 12.212], mean action: 1.672 [0.000, 3.000],  loss: 10.601626, mae: 15.344320, mean_q: 3.890377, mean_eps: 0.902254
  16439/150000: episode: 173, duration: 0.451s, episode steps:  79, steps per second: 175, episode reward: -129.329, mean reward: -1.637 [-100.000, 13.947], mean action: 1.557 [0.000, 3.000],  loss: 15.205743, mae: 15.565132, mean_q: 4.831924, mean_eps: 0.901606
  16569/150000: episode: 174, duration: 0.784s, episode steps: 130, steps per second: 166, episode reward: -339.308, mean reward: -2.610 [-100.000, 84.632], mean action: 1.592 [0.000, 3.000],  loss: 10.513568, mae: 15.620316, mean_q: 4.486613, mean_eps: 0.900979
  16649/150000: episode: 175, duration: 0.483s, episode steps:  80, steps per second: 166, episode reward: -121.734, mean reward: -1.522 [-100.000,  7.377], mean action: 1.512 [0.000, 3.000],  loss: 10.148584, mae: 15.785851, mean_q: 3.555903, mean_eps: 0.900349
  16731/150000: episode: 176, duration: 0.462s, episode steps:  82, steps per second: 177, episode reward: -171.843, mean reward: -2.096 [-100.000, 14.437], mean action: 1.573 [0.000, 3.000],  loss: 12.009752, mae: 15.787753, mean_q: 4.569716, mean_eps: 0.899863
  16804/150000: episode: 177, duration: 0.406s, episode steps:  73, steps per second: 180, episode reward: -76.229, mean reward: -1.044 [-100.000,  8.746], mean action: 1.425 [0.000, 3.000],  loss: 10.303969, mae: 15.634545, mean_q: 4.640450, mean_eps: 0.899398
  16876/150000: episode: 178, duration: 0.403s, episode steps:  72, steps per second: 178, episode reward: -151.836, mean reward: -2.109 [-100.000,  6.620], mean action: 1.569 [0.000, 3.000],  loss: 13.873663, mae: 16.001277, mean_q: 3.617122, mean_eps: 0.898963
  16951/150000: episode: 179, duration: 0.471s, episode steps:  75, steps per second: 159, episode reward: -89.781, mean reward: -1.197 [-100.000, 16.763], mean action: 1.547 [0.000, 3.000],  loss: 9.210815, mae: 15.545441, mean_q: 4.299441, mean_eps: 0.898522
  17077/150000: episode: 180, duration: 0.734s, episode steps: 126, steps per second: 172, episode reward: -119.534, mean reward: -0.949 [-100.000, 11.718], mean action: 1.476 [0.000, 3.000],  loss: 11.563819, mae: 16.601503, mean_q: 3.525347, mean_eps: 0.897919
  17191/150000: episode: 181, duration: 0.633s, episode steps: 114, steps per second: 180, episode reward: -99.628, mean reward: -0.874 [-100.000,  8.150], mean action: 1.728 [0.000, 3.000],  loss: 12.329399, mae: 16.981885, mean_q: 3.284736, mean_eps: 0.897199
  17268/150000: episode: 182, duration: 0.440s, episode steps:  77, steps per second: 175, episode reward: -85.730, mean reward: -1.113 [-100.000,  9.854], mean action: 1.519 [0.000, 3.000],  loss: 7.662126, mae: 16.412007, mean_q: 4.522744, mean_eps: 0.896626
  17351/150000: episode: 183, duration: 0.510s, episode steps:  83, steps per second: 163, episode reward: -130.900, mean reward: -1.577 [-100.000, 18.860], mean action: 1.482 [0.000, 3.000],  loss: 16.567679, mae: 16.482143, mean_q: 5.099200, mean_eps: 0.896146
  17413/150000: episode: 184, duration: 0.380s, episode steps:  62, steps per second: 163, episode reward: -174.023, mean reward: -2.807 [-100.000,  5.894], mean action: 1.629 [0.000, 3.000],  loss: 14.179042, mae: 16.412913, mean_q: 5.044257, mean_eps: 0.895711
  17487/150000: episode: 185, duration: 0.423s, episode steps:  74, steps per second: 175, episode reward: -111.240, mean reward: -1.503 [-100.000,  6.343], mean action: 1.649 [0.000, 3.000],  loss: 8.564450, mae: 16.697058, mean_q: 5.000349, mean_eps: 0.895303
  17564/150000: episode: 186, duration: 0.447s, episode steps:  77, steps per second: 172, episode reward: -100.711, mean reward: -1.308 [-100.000,  7.419], mean action: 1.675 [0.000, 3.000],  loss: 13.715442, mae: 16.657186, mean_q: 5.651095, mean_eps: 0.894850
  17652/150000: episode: 187, duration: 0.497s, episode steps:  88, steps per second: 177, episode reward: -95.746, mean reward: -1.088 [-100.000, 16.604], mean action: 1.614 [0.000, 3.000],  loss: 6.268374, mae: 16.777237, mean_q: 5.034370, mean_eps: 0.894355
  17717/150000: episode: 188, duration: 0.403s, episode steps:  65, steps per second: 161, episode reward: -94.464, mean reward: -1.453 [-100.000, 10.493], mean action: 1.538 [0.000, 3.000],  loss: 11.245443, mae: 16.517580, mean_q: 4.737832, mean_eps: 0.893896
  17782/150000: episode: 189, duration: 0.380s, episode steps:  65, steps per second: 171, episode reward: -48.420, mean reward: -0.745 [-100.000, 11.124], mean action: 1.662 [0.000, 3.000],  loss: 12.250523, mae: 17.264695, mean_q: 3.751806, mean_eps: 0.893506
  17862/150000: episode: 190, duration: 0.447s, episode steps:  80, steps per second: 179, episode reward: -122.618, mean reward: -1.533 [-100.000, 10.601], mean action: 1.575 [0.000, 3.000],  loss: 9.358223, mae: 16.414673, mean_q: 5.733037, mean_eps: 0.893071
  17965/150000: episode: 191, duration: 0.582s, episode steps: 103, steps per second: 177, episode reward: -201.553, mean reward: -1.957 [-100.000, 71.079], mean action: 1.592 [0.000, 3.000],  loss: 10.500726, mae: 16.948308, mean_q: 4.756562, mean_eps: 0.892522
  18106/150000: episode: 192, duration: 0.838s, episode steps: 141, steps per second: 168, episode reward: -129.639, mean reward: -0.919 [-100.000, 11.095], mean action: 1.596 [0.000, 3.000],  loss: 11.097096, mae: 17.300077, mean_q: 5.237399, mean_eps: 0.891790
  18210/150000: episode: 193, duration: 0.594s, episode steps: 104, steps per second: 175, episode reward: -149.301, mean reward: -1.436 [-100.000,  3.209], mean action: 1.510 [0.000, 3.000],  loss: 13.812378, mae: 17.458465, mean_q: 5.868962, mean_eps: 0.891055
  18284/150000: episode: 194, duration: 0.413s, episode steps:  74, steps per second: 179, episode reward: -104.008, mean reward: -1.406 [-100.000,  8.530], mean action: 1.446 [0.000, 3.000],  loss: 11.655244, mae: 17.796103, mean_q: 5.012987, mean_eps: 0.890521
  18366/150000: episode: 195, duration: 0.467s, episode steps:  82, steps per second: 175, episode reward: -78.822, mean reward: -0.961 [-100.000, 15.308], mean action: 1.671 [0.000, 3.000],  loss: 13.877974, mae: 17.660322, mean_q: 5.276613, mean_eps: 0.890053
  18433/150000: episode: 196, duration: 0.414s, episode steps:  67, steps per second: 162, episode reward: -115.408, mean reward: -1.723 [-100.000,  9.335], mean action: 1.313 [0.000, 3.000],  loss: 10.270654, mae: 17.589035, mean_q: 5.829964, mean_eps: 0.889606
  18508/150000: episode: 197, duration: 0.463s, episode steps:  75, steps per second: 162, episode reward: -105.489, mean reward: -1.407 [-100.000,  7.016], mean action: 1.453 [0.000, 3.000],  loss: 13.037814, mae: 17.631817, mean_q: 6.107973, mean_eps: 0.889180
  18592/150000: episode: 198, duration: 0.483s, episode steps:  84, steps per second: 174, episode reward: -82.976, mean reward: -0.988 [-100.000,  6.601], mean action: 1.702 [0.000, 3.000],  loss: 17.396770, mae: 17.236434, mean_q: 6.310981, mean_eps: 0.888703
  18675/150000: episode: 199, duration: 0.464s, episode steps:  83, steps per second: 179, episode reward: -91.461, mean reward: -1.102 [-100.000,  8.462], mean action: 1.398 [0.000, 3.000],  loss: 18.025618, mae: 17.869819, mean_q: 5.634835, mean_eps: 0.888202
  18794/150000: episode: 200, duration: 0.666s, episode steps: 119, steps per second: 179, episode reward: -243.026, mean reward: -2.042 [-100.000, 17.062], mean action: 1.655 [0.000, 3.000],  loss: 10.099155, mae: 17.593336, mean_q: 5.302852, mean_eps: 0.887596
  18872/150000: episode: 201, duration: 0.484s, episode steps:  78, steps per second: 161, episode reward: 30.514, mean reward:  0.391 [-100.000, 94.017], mean action: 1.410 [0.000, 3.000],  loss: 14.947676, mae: 17.751156, mean_q: 5.252030, mean_eps: 0.887005
  18972/150000: episode: 202, duration: 0.579s, episode steps: 100, steps per second: 173, episode reward: -119.333, mean reward: -1.193 [-100.000, 11.954], mean action: 1.520 [0.000, 3.000],  loss: 17.425921, mae: 18.084964, mean_q: 5.390075, mean_eps: 0.886471
  19096/150000: episode: 203, duration: 0.698s, episode steps: 124, steps per second: 178, episode reward: -110.475, mean reward: -0.891 [-100.000,  7.787], mean action: 1.532 [0.000, 3.000],  loss: 16.063415, mae: 17.994093, mean_q: 6.165969, mean_eps: 0.885799
  19197/150000: episode: 204, duration: 0.596s, episode steps: 101, steps per second: 170, episode reward: -84.847, mean reward: -0.840 [-100.000, 18.730], mean action: 1.376 [0.000, 3.000],  loss: 16.175383, mae: 17.821550, mean_q: 6.050917, mean_eps: 0.885124
  19294/150000: episode: 205, duration: 0.580s, episode steps:  97, steps per second: 167, episode reward: -88.875, mean reward: -0.916 [-100.000, 13.660], mean action: 1.464 [0.000, 3.000],  loss: 9.533054, mae: 18.482435, mean_q: 6.312460, mean_eps: 0.884530
  19398/150000: episode: 206, duration: 0.596s, episode steps: 104, steps per second: 174, episode reward: -173.078, mean reward: -1.664 [-100.000,  9.788], mean action: 1.548 [0.000, 3.000],  loss: 15.361905, mae: 18.084743, mean_q: 6.466353, mean_eps: 0.883927
  19473/150000: episode: 207, duration: 0.423s, episode steps:  75, steps per second: 177, episode reward: -119.735, mean reward: -1.596 [-100.000, 26.443], mean action: 1.440 [0.000, 3.000],  loss: 12.347878, mae: 18.099387, mean_q: 7.472497, mean_eps: 0.883390
  19581/150000: episode: 208, duration: 0.637s, episode steps: 108, steps per second: 170, episode reward: -122.600, mean reward: -1.135 [-100.000,  7.388], mean action: 1.574 [0.000, 3.000],  loss: 11.182231, mae: 18.443965, mean_q: 5.405875, mean_eps: 0.882841
  19665/150000: episode: 209, duration: 0.486s, episode steps:  84, steps per second: 173, episode reward: -55.783, mean reward: -0.664 [-100.000, 17.141], mean action: 1.571 [0.000, 3.000],  loss: 15.695853, mae: 18.397418, mean_q: 7.048233, mean_eps: 0.882265
  19752/150000: episode: 210, duration: 0.508s, episode steps:  87, steps per second: 171, episode reward: -93.711, mean reward: -1.077 [-100.000, 17.289], mean action: 1.598 [0.000, 3.000],  loss: 13.363226, mae: 18.088096, mean_q: 6.941122, mean_eps: 0.881752
  19816/150000: episode: 211, duration: 0.374s, episode steps:  64, steps per second: 171, episode reward: -123.539, mean reward: -1.930 [-100.000,  7.916], mean action: 1.609 [0.000, 3.000],  loss: 12.215472, mae: 17.791767, mean_q: 6.530602, mean_eps: 0.881299
  19934/150000: episode: 212, duration: 0.674s, episode steps: 118, steps per second: 175, episode reward: -86.110, mean reward: -0.730 [-100.000,  7.756], mean action: 1.610 [0.000, 3.000],  loss: 18.436141, mae: 18.307457, mean_q: 6.680564, mean_eps: 0.880753
  20043/150000: episode: 213, duration: 0.648s, episode steps: 109, steps per second: 168, episode reward: -138.100, mean reward: -1.267 [-100.000, 16.703], mean action: 1.661 [0.000, 3.000],  loss: 17.883248, mae: 17.837006, mean_q: 6.864551, mean_eps: 0.880072
  20213/150000: episode: 214, duration: 1.057s, episode steps: 170, steps per second: 161, episode reward: -141.244, mean reward: -0.831 [-100.000,  6.955], mean action: 1.724 [0.000, 3.000],  loss: 11.462300, mae: 18.706665, mean_q: 7.081758, mean_eps: 0.879235
  20340/150000: episode: 215, duration: 0.781s, episode steps: 127, steps per second: 163, episode reward: -32.666, mean reward: -0.257 [-100.000, 32.811], mean action: 1.488 [0.000, 3.000],  loss: 10.512262, mae: 18.876781, mean_q: 6.902675, mean_eps: 0.878344
  20410/150000: episode: 216, duration: 0.403s, episode steps:  70, steps per second: 174, episode reward: -104.182, mean reward: -1.488 [-100.000,  6.603], mean action: 1.700 [0.000, 3.000],  loss: 10.506359, mae: 18.878932, mean_q: 7.759856, mean_eps: 0.877753
  20508/150000: episode: 217, duration: 0.589s, episode steps:  98, steps per second: 166, episode reward: -99.992, mean reward: -1.020 [-100.000,  6.584], mean action: 1.541 [0.000, 3.000],  loss: 11.531226, mae: 19.157916, mean_q: 7.581433, mean_eps: 0.877249
  20642/150000: episode: 218, duration: 0.772s, episode steps: 134, steps per second: 174, episode reward: -111.779, mean reward: -0.834 [-100.000, 16.977], mean action: 1.485 [0.000, 3.000],  loss: 6.881166, mae: 19.062941, mean_q: 6.144966, mean_eps: 0.876553
  20755/150000: episode: 219, duration: 0.665s, episode steps: 113, steps per second: 170, episode reward: -147.493, mean reward: -1.305 [-100.000,  8.608], mean action: 1.425 [0.000, 3.000],  loss: 11.722444, mae: 19.015347, mean_q: 7.100096, mean_eps: 0.875812
  20848/150000: episode: 220, duration: 0.535s, episode steps:  93, steps per second: 174, episode reward: -117.021, mean reward: -1.258 [-100.000, 15.159], mean action: 1.473 [0.000, 3.000],  loss: 13.690584, mae: 18.619209, mean_q: 6.681070, mean_eps: 0.875194
  20920/150000: episode: 221, duration: 0.409s, episode steps:  72, steps per second: 176, episode reward: -161.127, mean reward: -2.238 [-100.000, 29.244], mean action: 1.764 [0.000, 3.000],  loss: 17.954281, mae: 19.100806, mean_q: 6.631889, mean_eps: 0.874699
  21046/150000: episode: 222, duration: 0.751s, episode steps: 126, steps per second: 168, episode reward: -61.141, mean reward: -0.485 [-100.000,  8.365], mean action: 1.563 [0.000, 3.000],  loss: 9.180515, mae: 19.157291, mean_q: 8.271747, mean_eps: 0.874105
  21112/150000: episode: 223, duration: 0.381s, episode steps:  66, steps per second: 173, episode reward: -65.056, mean reward: -0.986 [-100.000,  8.687], mean action: 1.742 [0.000, 3.000],  loss: 11.457480, mae: 19.642458, mean_q: 6.938176, mean_eps: 0.873529
  21173/150000: episode: 224, duration: 0.356s, episode steps:  61, steps per second: 172, episode reward: -64.846, mean reward: -1.063 [-100.000,  7.837], mean action: 1.459 [0.000, 3.000],  loss: 8.302366, mae: 19.328425, mean_q: 8.241640, mean_eps: 0.873148
  21253/150000: episode: 225, duration: 0.460s, episode steps:  80, steps per second: 174, episode reward: -72.097, mean reward: -0.901 [-100.000, 18.162], mean action: 1.688 [0.000, 3.000],  loss: 15.267885, mae: 19.714911, mean_q: 7.506219, mean_eps: 0.872725
  21354/150000: episode: 226, duration: 0.572s, episode steps: 101, steps per second: 177, episode reward: -159.103, mean reward: -1.575 [-100.000,  9.751], mean action: 1.485 [0.000, 3.000],  loss: 18.711370, mae: 19.885884, mean_q: 8.315097, mean_eps: 0.872182
  21444/150000: episode: 227, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -247.826, mean reward: -2.754 [-100.000, 40.172], mean action: 1.278 [0.000, 3.000],  loss: 13.917164, mae: 19.920674, mean_q: 7.468799, mean_eps: 0.871609
  21528/150000: episode: 228, duration: 0.480s, episode steps:  84, steps per second: 175, episode reward: -63.788, mean reward: -0.759 [-100.000, 23.595], mean action: 1.619 [0.000, 3.000],  loss: 19.346923, mae: 19.877850, mean_q: 8.234785, mean_eps: 0.871087
  21587/150000: episode: 229, duration: 0.329s, episode steps:  59, steps per second: 179, episode reward: -52.472, mean reward: -0.889 [-100.000, 13.629], mean action: 1.627 [0.000, 3.000],  loss: 11.962952, mae: 19.802102, mean_q: 7.599237, mean_eps: 0.870658
  21704/150000: episode: 230, duration: 0.685s, episode steps: 117, steps per second: 171, episode reward: -175.842, mean reward: -1.503 [-100.000,  5.104], mean action: 1.470 [0.000, 3.000],  loss: 11.998757, mae: 20.202796, mean_q: 7.671882, mean_eps: 0.870130
  21808/150000: episode: 231, duration: 0.636s, episode steps: 104, steps per second: 163, episode reward: -141.640, mean reward: -1.362 [-100.000, 37.783], mean action: 1.529 [0.000, 3.000],  loss: 13.507788, mae: 20.240297, mean_q: 7.560175, mean_eps: 0.869467
  21895/150000: episode: 232, duration: 0.506s, episode steps:  87, steps per second: 172, episode reward: -197.252, mean reward: -2.267 [-100.000, 29.414], mean action: 1.506 [0.000, 3.000],  loss: 13.227573, mae: 20.662230, mean_q: 7.536707, mean_eps: 0.868894
  21988/150000: episode: 233, duration: 0.545s, episode steps:  93, steps per second: 171, episode reward: -246.712, mean reward: -2.653 [-100.000, 21.231], mean action: 1.677 [0.000, 3.000],  loss: 6.643826, mae: 19.757329, mean_q: 7.094273, mean_eps: 0.868354
  22074/150000: episode: 234, duration: 0.501s, episode steps:  86, steps per second: 172, episode reward: -121.189, mean reward: -1.409 [-100.000, 18.693], mean action: 1.558 [0.000, 3.000],  loss: 8.148490, mae: 21.016895, mean_q: 8.437192, mean_eps: 0.867817
  22202/150000: episode: 235, duration: 0.773s, episode steps: 128, steps per second: 166, episode reward: -43.108, mean reward: -0.337 [-100.000, 53.336], mean action: 1.367 [0.000, 3.000],  loss: 14.067519, mae: 21.585562, mean_q: 8.190762, mean_eps: 0.867175
  22300/150000: episode: 236, duration: 0.555s, episode steps:  98, steps per second: 177, episode reward: -151.382, mean reward: -1.545 [-100.000, 15.253], mean action: 1.663 [0.000, 3.000],  loss: 17.360710, mae: 21.082869, mean_q: 8.073888, mean_eps: 0.866497
  22414/150000: episode: 237, duration: 0.662s, episode steps: 114, steps per second: 172, episode reward: -103.254, mean reward: -0.906 [-100.000,  6.425], mean action: 1.535 [0.000, 3.000],  loss: 17.597430, mae: 20.708740, mean_q: 9.284858, mean_eps: 0.865861
  22481/150000: episode: 238, duration: 0.402s, episode steps:  67, steps per second: 167, episode reward: -85.868, mean reward: -1.282 [-100.000, 12.906], mean action: 1.433 [0.000, 3.000],  loss: 14.990747, mae: 20.772906, mean_q: 9.193294, mean_eps: 0.865318
  22559/150000: episode: 239, duration: 0.549s, episode steps:  78, steps per second: 142, episode reward: -144.933, mean reward: -1.858 [-100.000,  5.854], mean action: 1.769 [0.000, 3.000],  loss: 10.686098, mae: 21.121259, mean_q: 8.037327, mean_eps: 0.864883
  22658/150000: episode: 240, duration: 0.658s, episode steps:  99, steps per second: 150, episode reward: 39.694, mean reward:  0.401 [-100.000, 90.857], mean action: 1.556 [0.000, 3.000],  loss: 15.100635, mae: 21.324890, mean_q: 8.005726, mean_eps: 0.864352
  22719/150000: episode: 241, duration: 0.404s, episode steps:  61, steps per second: 151, episode reward: -73.954, mean reward: -1.212 [-100.000,  9.109], mean action: 1.492 [0.000, 3.000],  loss: 14.064776, mae: 21.528696, mean_q: 7.860981, mean_eps: 0.863872
  22791/150000: episode: 242, duration: 0.454s, episode steps:  72, steps per second: 159, episode reward: -51.740, mean reward: -0.719 [-100.000, 10.547], mean action: 1.736 [0.000, 3.000],  loss: 10.447082, mae: 21.725296, mean_q: 8.107884, mean_eps: 0.863473
  22888/150000: episode: 243, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -64.555, mean reward: -0.666 [-100.000, 10.977], mean action: 1.629 [0.000, 3.000],  loss: 13.556340, mae: 21.157325, mean_q: 9.419173, mean_eps: 0.862966
  22949/150000: episode: 244, duration: 0.369s, episode steps:  61, steps per second: 165, episode reward: -105.305, mean reward: -1.726 [-100.000,  7.536], mean action: 1.639 [0.000, 3.000],  loss: 17.380860, mae: 21.093548, mean_q: 8.939791, mean_eps: 0.862492
  23023/150000: episode: 245, duration: 0.459s, episode steps:  74, steps per second: 161, episode reward: -107.163, mean reward: -1.448 [-100.000,  6.447], mean action: 1.622 [0.000, 3.000],  loss: 16.007071, mae: 21.075486, mean_q: 9.633385, mean_eps: 0.862087
  23104/150000: episode: 246, duration: 0.468s, episode steps:  81, steps per second: 173, episode reward: -113.228, mean reward: -1.398 [-100.000,  9.684], mean action: 1.778 [0.000, 3.000],  loss: 14.639033, mae: 21.897229, mean_q: 9.447961, mean_eps: 0.861622
  23202/150000: episode: 247, duration: 0.596s, episode steps:  98, steps per second: 164, episode reward: -180.098, mean reward: -1.838 [-100.000, 18.655], mean action: 1.520 [0.000, 3.000],  loss: 12.448760, mae: 22.146420, mean_q: 7.757078, mean_eps: 0.861085
  23342/150000: episode: 248, duration: 0.824s, episode steps: 140, steps per second: 170, episode reward: -220.399, mean reward: -1.574 [-100.000,  4.945], mean action: 1.643 [0.000, 3.000],  loss: 8.952288, mae: 21.264410, mean_q: 9.597901, mean_eps: 0.860371
  23433/150000: episode: 249, duration: 0.525s, episode steps:  91, steps per second: 173, episode reward: -80.648, mean reward: -0.886 [-100.000,  7.827], mean action: 1.615 [0.000, 3.000],  loss: 11.193602, mae: 21.542604, mean_q: 8.459860, mean_eps: 0.859678
  23496/150000: episode: 250, duration: 0.376s, episode steps:  63, steps per second: 168, episode reward: -54.973, mean reward: -0.873 [-100.000, 12.263], mean action: 1.571 [0.000, 3.000],  loss: 10.408857, mae: 21.297417, mean_q: 9.738334, mean_eps: 0.859216
  23561/150000: episode: 251, duration: 0.423s, episode steps:  65, steps per second: 154, episode reward: -98.526, mean reward: -1.516 [-100.000, 10.644], mean action: 1.662 [0.000, 3.000],  loss: 6.731772, mae: 21.722318, mean_q: 8.935649, mean_eps: 0.858832
  23653/150000: episode: 252, duration: 0.543s, episode steps:  92, steps per second: 169, episode reward: -187.398, mean reward: -2.037 [-100.000,  5.479], mean action: 1.587 [0.000, 3.000],  loss: 8.567309, mae: 21.574857, mean_q: 9.274110, mean_eps: 0.858361
  23754/150000: episode: 253, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: -59.407, mean reward: -0.588 [-100.000, 20.628], mean action: 1.574 [0.000, 3.000],  loss: 16.228860, mae: 21.697817, mean_q: 8.896596, mean_eps: 0.857782
  23824/150000: episode: 254, duration: 0.396s, episode steps:  70, steps per second: 177, episode reward: -89.682, mean reward: -1.281 [-100.000, 13.755], mean action: 1.600 [0.000, 3.000],  loss: 10.242349, mae: 21.280799, mean_q: 9.289455, mean_eps: 0.857269
  23910/150000: episode: 255, duration: 0.502s, episode steps:  86, steps per second: 171, episode reward: -119.636, mean reward: -1.391 [-100.000,  7.237], mean action: 1.674 [0.000, 3.000],  loss: 14.935471, mae: 21.207486, mean_q: 8.492281, mean_eps: 0.856801
  23982/150000: episode: 256, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: -40.618, mean reward: -0.564 [-100.000, 64.899], mean action: 1.556 [0.000, 3.000],  loss: 9.119484, mae: 21.732031, mean_q: 8.865472, mean_eps: 0.856327
  24126/150000: episode: 257, duration: 0.824s, episode steps: 144, steps per second: 175, episode reward: -300.652, mean reward: -2.088 [-100.000, 75.018], mean action: 1.521 [0.000, 3.000],  loss: 18.934943, mae: 21.507084, mean_q: 10.066947, mean_eps: 0.855679
  24226/150000: episode: 258, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -115.385, mean reward: -1.154 [-100.000,  7.055], mean action: 1.600 [0.000, 3.000],  loss: 11.990028, mae: 21.677968, mean_q: 10.440062, mean_eps: 0.854947
  24297/150000: episode: 259, duration: 0.435s, episode steps:  71, steps per second: 163, episode reward: -68.123, mean reward: -0.959 [-100.000, 21.840], mean action: 1.493 [0.000, 3.000],  loss: 17.912294, mae: 21.583170, mean_q: 10.739314, mean_eps: 0.854434
  24383/150000: episode: 260, duration: 0.519s, episode steps:  86, steps per second: 166, episode reward: -62.923, mean reward: -0.732 [-100.000, 17.266], mean action: 1.500 [0.000, 3.000],  loss: 11.673177, mae: 22.436341, mean_q: 9.126998, mean_eps: 0.853963
  24526/150000: episode: 261, duration: 0.816s, episode steps: 143, steps per second: 175, episode reward: -59.489, mean reward: -0.416 [-100.000, 12.882], mean action: 1.601 [0.000, 3.000],  loss: 11.759360, mae: 22.151364, mean_q: 9.752222, mean_eps: 0.853276
  24652/150000: episode: 262, duration: 0.735s, episode steps: 126, steps per second: 171, episode reward: -142.527, mean reward: -1.131 [-100.000, 10.249], mean action: 1.683 [0.000, 3.000],  loss: 9.738142, mae: 22.006473, mean_q: 10.405026, mean_eps: 0.852469
  24730/150000: episode: 263, duration: 0.473s, episode steps:  78, steps per second: 165, episode reward: -26.638, mean reward: -0.342 [-100.000, 16.092], mean action: 1.564 [0.000, 3.000],  loss: 11.425164, mae: 22.313308, mean_q: 8.185466, mean_eps: 0.851857
  24828/150000: episode: 264, duration: 0.568s, episode steps:  98, steps per second: 173, episode reward: -46.446, mean reward: -0.474 [-100.000,  8.438], mean action: 1.612 [0.000, 3.000],  loss: 13.537711, mae: 22.399336, mean_q: 9.809932, mean_eps: 0.851329
  24963/150000: episode: 265, duration: 0.776s, episode steps: 135, steps per second: 174, episode reward: -48.791, mean reward: -0.361 [-100.000, 17.731], mean action: 1.674 [0.000, 3.000],  loss: 10.082907, mae: 22.227389, mean_q: 10.367179, mean_eps: 0.850630
  25055/150000: episode: 266, duration: 0.561s, episode steps:  92, steps per second: 164, episode reward: -160.015, mean reward: -1.739 [-100.000,  5.966], mean action: 1.576 [0.000, 3.000],  loss: 11.810511, mae: 22.964548, mean_q: 9.288137, mean_eps: 0.849949
  25186/150000: episode: 267, duration: 0.767s, episode steps: 131, steps per second: 171, episode reward: -53.868, mean reward: -0.411 [-100.000, 95.633], mean action: 1.580 [0.000, 3.000],  loss: 13.777298, mae: 23.295648, mean_q: 11.472517, mean_eps: 0.849280
  25251/150000: episode: 268, duration: 0.380s, episode steps:  65, steps per second: 171, episode reward: -7.281, mean reward: -0.112 [-100.000, 56.060], mean action: 1.662 [0.000, 3.000],  loss: 11.949356, mae: 23.194270, mean_q: 9.853422, mean_eps: 0.848692
  25369/150000: episode: 269, duration: 0.691s, episode steps: 118, steps per second: 171, episode reward: -96.918, mean reward: -0.821 [-100.000, 18.998], mean action: 1.653 [0.000, 3.000],  loss: 9.543525, mae: 22.527242, mean_q: 10.984551, mean_eps: 0.848143
  25433/150000: episode: 270, duration: 0.376s, episode steps:  64, steps per second: 170, episode reward: -62.320, mean reward: -0.974 [-100.000, 20.324], mean action: 1.391 [0.000, 3.000],  loss: 14.491315, mae: 23.686339, mean_q: 10.971073, mean_eps: 0.847597
  25521/150000: episode: 271, duration: 0.514s, episode steps:  88, steps per second: 171, episode reward: -116.769, mean reward: -1.327 [-100.000,  6.437], mean action: 1.693 [0.000, 3.000],  loss: 15.895752, mae: 24.254103, mean_q: 9.880541, mean_eps: 0.847141
  25594/150000: episode: 272, duration: 0.415s, episode steps:  73, steps per second: 176, episode reward: -71.445, mean reward: -0.979 [-100.000, 22.160], mean action: 1.452 [0.000, 3.000],  loss: 13.397746, mae: 23.313218, mean_q: 11.509942, mean_eps: 0.846658
  25677/150000: episode: 273, duration: 0.490s, episode steps:  83, steps per second: 170, episode reward: -64.488, mean reward: -0.777 [-100.000, 18.012], mean action: 1.675 [0.000, 3.000],  loss: 12.267668, mae: 22.987082, mean_q: 11.929793, mean_eps: 0.846190
  25772/150000: episode: 274, duration: 0.565s, episode steps:  95, steps per second: 168, episode reward: -109.377, mean reward: -1.151 [-100.000, 45.395], mean action: 1.642 [0.000, 3.000],  loss: 13.128912, mae: 22.952117, mean_q: 10.798875, mean_eps: 0.845656
  25892/150000: episode: 275, duration: 0.719s, episode steps: 120, steps per second: 167, episode reward: -92.485, mean reward: -0.771 [-100.000, 25.639], mean action: 1.592 [0.000, 3.000],  loss: 9.124951, mae: 23.290767, mean_q: 10.518763, mean_eps: 0.845011
  26026/150000: episode: 276, duration: 0.776s, episode steps: 134, steps per second: 173, episode reward: -196.738, mean reward: -1.468 [-100.000,  8.529], mean action: 1.560 [0.000, 3.000],  loss: 11.295522, mae: 23.524993, mean_q: 10.092717, mean_eps: 0.844249
  26115/150000: episode: 277, duration: 0.527s, episode steps:  89, steps per second: 169, episode reward: -122.262, mean reward: -1.374 [-100.000,  5.626], mean action: 1.517 [0.000, 3.000],  loss: 12.894456, mae: 23.832377, mean_q: 9.494461, mean_eps: 0.843580
  26228/150000: episode: 278, duration: 0.681s, episode steps: 113, steps per second: 166, episode reward: -73.240, mean reward: -0.648 [-100.000, 16.406], mean action: 1.504 [0.000, 3.000],  loss: 16.123419, mae: 23.166714, mean_q: 10.497308, mean_eps: 0.842974
  26338/150000: episode: 279, duration: 0.665s, episode steps: 110, steps per second: 165, episode reward: -117.256, mean reward: -1.066 [-100.000, 24.969], mean action: 1.555 [0.000, 3.000],  loss: 17.192548, mae: 23.601349, mean_q: 10.616129, mean_eps: 0.842305
  26449/150000: episode: 280, duration: 0.648s, episode steps: 111, steps per second: 171, episode reward: -101.295, mean reward: -0.913 [-100.000,  8.695], mean action: 1.486 [0.000, 3.000],  loss: 13.692868, mae: 23.808982, mean_q: 10.458470, mean_eps: 0.841642
  26552/150000: episode: 281, duration: 0.640s, episode steps: 103, steps per second: 161, episode reward: -78.130, mean reward: -0.759 [-100.000, 11.649], mean action: 1.573 [0.000, 3.000],  loss: 20.913453, mae: 23.483774, mean_q: 9.977969, mean_eps: 0.841000
  26649/150000: episode: 282, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -73.963, mean reward: -0.763 [-100.000,  8.458], mean action: 1.464 [0.000, 3.000],  loss: 13.789145, mae: 23.711405, mean_q: 11.115807, mean_eps: 0.840400
  26732/150000: episode: 283, duration: 0.565s, episode steps:  83, steps per second: 147, episode reward: -247.790, mean reward: -2.985 [-100.000, 95.039], mean action: 1.361 [0.000, 3.000],  loss: 18.460828, mae: 23.516432, mean_q: 11.714121, mean_eps: 0.839860
  26830/150000: episode: 284, duration: 0.698s, episode steps:  98, steps per second: 140, episode reward: -118.017, mean reward: -1.204 [-100.000, 24.908], mean action: 1.571 [0.000, 3.000],  loss: 22.767271, mae: 23.580406, mean_q: 11.439592, mean_eps: 0.839317
  26934/150000: episode: 285, duration: 0.663s, episode steps: 104, steps per second: 157, episode reward: -63.009, mean reward: -0.606 [-100.000, 12.099], mean action: 1.462 [0.000, 3.000],  loss: 9.796547, mae: 23.459492, mean_q: 11.675550, mean_eps: 0.838711
  27022/150000: episode: 286, duration: 0.577s, episode steps:  88, steps per second: 152, episode reward: -118.958, mean reward: -1.352 [-100.000,  9.640], mean action: 1.523 [0.000, 3.000],  loss: 16.975471, mae: 23.753144, mean_q: 9.900505, mean_eps: 0.838135
  27125/150000: episode: 287, duration: 0.673s, episode steps: 103, steps per second: 153, episode reward: -62.954, mean reward: -0.611 [-100.000, 16.420], mean action: 1.612 [0.000, 3.000],  loss: 14.439722, mae: 24.135715, mean_q: 11.902533, mean_eps: 0.837562
  27238/150000: episode: 288, duration: 0.742s, episode steps: 113, steps per second: 152, episode reward: -149.543, mean reward: -1.323 [-100.000, 15.592], mean action: 1.602 [0.000, 3.000],  loss: 9.726676, mae: 24.426446, mean_q: 10.831495, mean_eps: 0.836914
  27307/150000: episode: 289, duration: 0.401s, episode steps:  69, steps per second: 172, episode reward: -85.324, mean reward: -1.237 [-100.000,  7.904], mean action: 1.391 [0.000, 3.000],  loss: 16.705081, mae: 23.593996, mean_q: 11.519201, mean_eps: 0.836368
  27406/150000: episode: 290, duration: 0.570s, episode steps:  99, steps per second: 174, episode reward: -145.996, mean reward: -1.475 [-100.000, 13.382], mean action: 1.354 [0.000, 3.000],  loss: 14.319021, mae: 24.575293, mean_q: 11.078820, mean_eps: 0.835864
  27493/150000: episode: 291, duration: 0.521s, episode steps:  87, steps per second: 167, episode reward: -104.806, mean reward: -1.205 [-100.000,  9.761], mean action: 1.414 [0.000, 3.000],  loss: 10.475469, mae: 24.515371, mean_q: 12.084346, mean_eps: 0.835306
  27567/150000: episode: 292, duration: 0.440s, episode steps:  74, steps per second: 168, episode reward: -104.305, mean reward: -1.410 [-100.000,  5.743], mean action: 1.500 [0.000, 3.000],  loss: 15.909831, mae: 24.722130, mean_q: 10.463408, mean_eps: 0.834823
  27659/150000: episode: 293, duration: 0.538s, episode steps:  92, steps per second: 171, episode reward: -81.786, mean reward: -0.889 [-100.000,  6.839], mean action: 1.652 [0.000, 3.000],  loss: 11.284874, mae: 24.678199, mean_q: 10.049634, mean_eps: 0.834325
  27787/150000: episode: 294, duration: 0.768s, episode steps: 128, steps per second: 167, episode reward: -67.717, mean reward: -0.529 [-100.000, 13.617], mean action: 1.586 [0.000, 3.000],  loss: 10.832146, mae: 24.678192, mean_q: 10.997261, mean_eps: 0.833665
  27846/150000: episode: 295, duration: 0.376s, episode steps:  59, steps per second: 157, episode reward: -80.612, mean reward: -1.366 [-100.000, 18.615], mean action: 1.475 [0.000, 3.000],  loss: 15.367742, mae: 24.649946, mean_q: 10.172513, mean_eps: 0.833104
  27972/150000: episode: 296, duration: 0.746s, episode steps: 126, steps per second: 169, episode reward: -78.724, mean reward: -0.625 [-100.000,  7.444], mean action: 1.500 [0.000, 3.000],  loss: 13.431882, mae: 24.730417, mean_q: 11.067702, mean_eps: 0.832549
  28055/150000: episode: 297, duration: 0.468s, episode steps:  83, steps per second: 177, episode reward: -42.570, mean reward: -0.513 [-100.000,  9.608], mean action: 1.566 [0.000, 3.000],  loss: 14.843699, mae: 25.126399, mean_q: 11.282479, mean_eps: 0.831922
  28163/150000: episode: 298, duration: 0.630s, episode steps: 108, steps per second: 171, episode reward: -52.934, mean reward: -0.490 [-100.000, 15.465], mean action: 1.528 [0.000, 3.000],  loss: 15.244133, mae: 25.584708, mean_q: 11.355516, mean_eps: 0.831349
  28299/150000: episode: 299, duration: 0.853s, episode steps: 136, steps per second: 159, episode reward: -80.397, mean reward: -0.591 [-100.000, 10.446], mean action: 1.522 [0.000, 3.000],  loss: 11.296651, mae: 24.919914, mean_q: 12.532474, mean_eps: 0.830617
  28389/150000: episode: 300, duration: 0.511s, episode steps:  90, steps per second: 176, episode reward: -123.191, mean reward: -1.369 [-100.000,  3.372], mean action: 1.444 [0.000, 3.000],  loss: 10.989213, mae: 24.693013, mean_q: 13.036452, mean_eps: 0.829939
  28465/150000: episode: 301, duration: 0.435s, episode steps:  76, steps per second: 175, episode reward: -31.116, mean reward: -0.409 [-100.000, 17.075], mean action: 1.513 [0.000, 3.000],  loss: 18.860445, mae: 25.309718, mean_q: 12.932576, mean_eps: 0.829441
  28537/150000: episode: 302, duration: 0.414s, episode steps:  72, steps per second: 174, episode reward: -98.436, mean reward: -1.367 [-100.000,  8.978], mean action: 1.625 [0.000, 3.000],  loss: 13.899977, mae: 25.246565, mean_q: 12.300854, mean_eps: 0.828997
  28618/150000: episode: 303, duration: 0.513s, episode steps:  81, steps per second: 158, episode reward: -33.603, mean reward: -0.415 [-100.000, 13.387], mean action: 1.568 [0.000, 3.000],  loss: 15.279436, mae: 25.753579, mean_q: 13.691877, mean_eps: 0.828538
  28703/150000: episode: 304, duration: 0.498s, episode steps:  85, steps per second: 171, episode reward: -97.323, mean reward: -1.145 [-100.000,  9.772], mean action: 1.565 [0.000, 3.000],  loss: 13.730453, mae: 25.439824, mean_q: 13.037366, mean_eps: 0.828040
  28810/150000: episode: 305, duration: 0.613s, episode steps: 107, steps per second: 174, episode reward: -92.643, mean reward: -0.866 [-100.000,  7.611], mean action: 1.570 [0.000, 3.000],  loss: 9.787888, mae: 25.269661, mean_q: 12.457537, mean_eps: 0.827464
  28875/150000: episode: 306, duration: 0.367s, episode steps:  65, steps per second: 177, episode reward: -54.020, mean reward: -0.831 [-100.000,  7.047], mean action: 1.492 [0.000, 3.000],  loss: 10.018225, mae: 24.959812, mean_q: 13.752275, mean_eps: 0.826948
  28953/150000: episode: 307, duration: 0.483s, episode steps:  78, steps per second: 161, episode reward: -67.402, mean reward: -0.864 [-100.000,  7.358], mean action: 1.679 [0.000, 3.000],  loss: 15.310254, mae: 25.619729, mean_q: 11.746639, mean_eps: 0.826519
  29116/150000: episode: 308, duration: 0.950s, episode steps: 163, steps per second: 172, episode reward: -151.302, mean reward: -0.928 [-100.000, 19.953], mean action: 1.546 [0.000, 3.000],  loss: 8.847471, mae: 25.639242, mean_q: 13.124634, mean_eps: 0.825796
  29184/150000: episode: 309, duration: 0.384s, episode steps:  68, steps per second: 177, episode reward: -48.109, mean reward: -0.707 [-100.000, 11.276], mean action: 1.529 [0.000, 3.000],  loss: 9.018070, mae: 25.395512, mean_q: 13.208190, mean_eps: 0.825103
  29301/150000: episode: 310, duration: 0.706s, episode steps: 117, steps per second: 166, episode reward: -128.662, mean reward: -1.100 [-100.000,  8.824], mean action: 1.462 [0.000, 3.000],  loss: 11.572555, mae: 25.466350, mean_q: 13.330651, mean_eps: 0.824548
  29378/150000: episode: 311, duration: 0.484s, episode steps:  77, steps per second: 159, episode reward: -131.296, mean reward: -1.705 [-100.000,  6.553], mean action: 1.558 [0.000, 3.000],  loss: 15.853160, mae: 26.042409, mean_q: 12.239637, mean_eps: 0.823966
  29501/150000: episode: 312, duration: 0.713s, episode steps: 123, steps per second: 173, episode reward: -168.030, mean reward: -1.366 [-100.000, 22.535], mean action: 1.707 [0.000, 3.000],  loss: 15.545615, mae: 25.499126, mean_q: 13.857441, mean_eps: 0.823366
  29582/150000: episode: 313, duration: 0.456s, episode steps:  81, steps per second: 178, episode reward: -71.226, mean reward: -0.879 [-100.000,  7.932], mean action: 1.494 [0.000, 3.000],  loss: 13.230063, mae: 25.518191, mean_q: 14.675341, mean_eps: 0.822754
  29656/150000: episode: 314, duration: 0.426s, episode steps:  74, steps per second: 174, episode reward: -114.929, mean reward: -1.553 [-100.000,  9.317], mean action: 1.595 [0.000, 3.000],  loss: 10.579846, mae: 25.455751, mean_q: 14.390143, mean_eps: 0.822289
  29752/150000: episode: 315, duration: 0.616s, episode steps:  96, steps per second: 156, episode reward: -85.681, mean reward: -0.893 [-100.000, 18.710], mean action: 1.635 [0.000, 3.000],  loss: 9.475669, mae: 25.682823, mean_q: 14.152501, mean_eps: 0.821779
  29867/150000: episode: 316, duration: 0.655s, episode steps: 115, steps per second: 176, episode reward: -86.898, mean reward: -0.756 [-100.000,  6.417], mean action: 1.748 [0.000, 3.000],  loss: 10.038496, mae: 25.453813, mean_q: 13.369796, mean_eps: 0.821146
  30002/150000: episode: 317, duration: 0.790s, episode steps: 135, steps per second: 171, episode reward: -70.856, mean reward: -0.525 [-100.000,  7.179], mean action: 1.511 [0.000, 3.000],  loss: 12.352705, mae: 25.702606, mean_q: 13.596609, mean_eps: 0.820396
  30070/150000: episode: 318, duration: 0.421s, episode steps:  68, steps per second: 162, episode reward: -73.735, mean reward: -1.084 [-100.000, 51.954], mean action: 1.500 [0.000, 3.000],  loss: 19.725725, mae: 26.117203, mean_q: 12.267801, mean_eps: 0.819787
  30174/150000: episode: 319, duration: 0.626s, episode steps: 104, steps per second: 166, episode reward: -80.121, mean reward: -0.770 [-100.000, 10.988], mean action: 1.712 [0.000, 3.000],  loss: 11.552757, mae: 25.760464, mean_q: 14.614977, mean_eps: 0.819271
  30297/150000: episode: 320, duration: 0.705s, episode steps: 123, steps per second: 174, episode reward: -51.385, mean reward: -0.418 [-100.000, 12.975], mean action: 1.577 [0.000, 3.000],  loss: 12.260732, mae: 25.691413, mean_q: 13.396803, mean_eps: 0.818590
  30368/150000: episode: 321, duration: 0.416s, episode steps:  71, steps per second: 171, episode reward: -54.239, mean reward: -0.764 [-100.000, 11.063], mean action: 1.296 [0.000, 3.000],  loss: 13.370341, mae: 26.162910, mean_q: 14.135757, mean_eps: 0.818008
  30468/150000: episode: 322, duration: 0.635s, episode steps: 100, steps per second: 157, episode reward: -135.854, mean reward: -1.359 [-100.000,  6.992], mean action: 1.670 [0.000, 3.000],  loss: 7.460064, mae: 26.041864, mean_q: 13.921361, mean_eps: 0.817495
  30591/150000: episode: 323, duration: 0.712s, episode steps: 123, steps per second: 173, episode reward: -27.348, mean reward: -0.222 [-100.000, 21.000], mean action: 1.569 [0.000, 3.000],  loss: 11.450482, mae: 26.233933, mean_q: 12.942906, mean_eps: 0.816826
  30711/150000: episode: 324, duration: 0.681s, episode steps: 120, steps per second: 176, episode reward: -143.481, mean reward: -1.196 [-100.000, 11.856], mean action: 1.733 [0.000, 3.000],  loss: 11.116850, mae: 26.290103, mean_q: 13.612621, mean_eps: 0.816097
  30785/150000: episode: 325, duration: 0.461s, episode steps:  74, steps per second: 160, episode reward: -159.708, mean reward: -2.158 [-100.000,  8.049], mean action: 1.662 [0.000, 3.000],  loss: 12.742024, mae: 26.634840, mean_q: 13.001708, mean_eps: 0.815515
  30883/150000: episode: 326, duration: 0.581s, episode steps:  98, steps per second: 169, episode reward: -79.268, mean reward: -0.809 [-100.000,  8.368], mean action: 1.602 [0.000, 3.000],  loss: 14.292556, mae: 25.798489, mean_q: 14.708213, mean_eps: 0.814999
  30971/150000: episode: 327, duration: 0.567s, episode steps:  88, steps per second: 155, episode reward: -110.774, mean reward: -1.259 [-100.000, 10.368], mean action: 1.455 [0.000, 3.000],  loss: 16.658297, mae: 26.519243, mean_q: 13.076979, mean_eps: 0.814441
  31110/150000: episode: 328, duration: 0.962s, episode steps: 139, steps per second: 144, episode reward: -70.655, mean reward: -0.508 [-100.000, 22.035], mean action: 1.525 [0.000, 3.000],  loss: 18.456098, mae: 26.262581, mean_q: 14.011362, mean_eps: 0.813760
  31186/150000: episode: 329, duration: 0.480s, episode steps:  76, steps per second: 158, episode reward: -48.579, mean reward: -0.639 [-100.000, 13.222], mean action: 1.434 [0.000, 3.000],  loss: 12.150999, mae: 26.566132, mean_q: 13.325825, mean_eps: 0.813115
  31328/150000: episode: 330, duration: 0.843s, episode steps: 142, steps per second: 168, episode reward: -50.646, mean reward: -0.357 [-100.000, 12.025], mean action: 1.592 [0.000, 3.000],  loss: 8.732519, mae: 26.963124, mean_q: 13.215570, mean_eps: 0.812461
  31685/150000: episode: 331, duration: 2.161s, episode steps: 357, steps per second: 165, episode reward: -184.606, mean reward: -0.517 [-100.000, 129.224], mean action: 1.535 [0.000, 3.000],  loss: 10.717341, mae: 26.867221, mean_q: 13.296749, mean_eps: 0.810964
  31753/150000: episode: 332, duration: 0.387s, episode steps:  68, steps per second: 176, episode reward: -55.920, mean reward: -0.822 [-100.000, 15.810], mean action: 1.779 [0.000, 3.000],  loss: 15.625208, mae: 26.381916, mean_q: 14.503224, mean_eps: 0.809689
  31845/150000: episode: 333, duration: 0.572s, episode steps:  92, steps per second: 161, episode reward: -119.778, mean reward: -1.302 [-100.000,  6.693], mean action: 1.543 [0.000, 3.000],  loss: 9.160337, mae: 27.399447, mean_q: 12.840653, mean_eps: 0.809209
  31968/150000: episode: 334, duration: 0.748s, episode steps: 123, steps per second: 164, episode reward: -59.802, mean reward: -0.486 [-100.000, 15.492], mean action: 1.626 [0.000, 3.000],  loss: 11.591953, mae: 27.366316, mean_q: 13.451204, mean_eps: 0.808564
  32070/150000: episode: 335, duration: 0.598s, episode steps: 102, steps per second: 170, episode reward: -74.495, mean reward: -0.730 [-100.000,  7.606], mean action: 1.637 [0.000, 3.000],  loss: 15.233067, mae: 27.687614, mean_q: 14.416731, mean_eps: 0.807889
  32144/150000: episode: 336, duration: 0.424s, episode steps:  74, steps per second: 175, episode reward: -40.948, mean reward: -0.553 [-100.000, 20.448], mean action: 1.649 [0.000, 3.000],  loss: 9.619676, mae: 27.638410, mean_q: 15.374854, mean_eps: 0.807361
  32213/150000: episode: 337, duration: 0.448s, episode steps:  69, steps per second: 154, episode reward: -117.193, mean reward: -1.698 [-100.000, 18.528], mean action: 1.884 [0.000, 3.000],  loss: 14.613484, mae: 27.727546, mean_q: 14.164192, mean_eps: 0.806932
  32330/150000: episode: 338, duration: 0.696s, episode steps: 117, steps per second: 168, episode reward: -165.797, mean reward: -1.417 [-100.000,  3.889], mean action: 1.692 [0.000, 3.000],  loss: 16.927662, mae: 27.775190, mean_q: 14.090330, mean_eps: 0.806374
  32466/150000: episode: 339, duration: 0.790s, episode steps: 136, steps per second: 172, episode reward: -113.390, mean reward: -0.834 [-100.000, 31.605], mean action: 1.676 [0.000, 3.000],  loss: 15.266930, mae: 27.507417, mean_q: 14.205893, mean_eps: 0.805615
  32573/150000: episode: 340, duration: 0.641s, episode steps: 107, steps per second: 167, episode reward: -110.840, mean reward: -1.036 [-100.000, 23.039], mean action: 1.542 [0.000, 3.000],  loss: 11.835924, mae: 27.167358, mean_q: 15.452536, mean_eps: 0.804886
  32643/150000: episode: 341, duration: 0.430s, episode steps:  70, steps per second: 163, episode reward: -47.445, mean reward: -0.678 [-100.000, 22.574], mean action: 1.557 [0.000, 3.000],  loss: 13.281184, mae: 27.445287, mean_q: 15.124413, mean_eps: 0.804355
  32733/150000: episode: 342, duration: 0.529s, episode steps:  90, steps per second: 170, episode reward: -52.845, mean reward: -0.587 [-100.000, 12.359], mean action: 1.500 [0.000, 3.000],  loss: 15.055760, mae: 27.728091, mean_q: 12.661610, mean_eps: 0.803875
  32834/150000: episode: 343, duration: 0.593s, episode steps: 101, steps per second: 170, episode reward: -101.608, mean reward: -1.006 [-100.000,  7.088], mean action: 1.703 [0.000, 3.000],  loss: 8.446361, mae: 27.210760, mean_q: 14.369501, mean_eps: 0.803302
  32924/150000: episode: 344, duration: 0.569s, episode steps:  90, steps per second: 158, episode reward: -83.186, mean reward: -0.924 [-100.000, 18.073], mean action: 1.667 [0.000, 3.000],  loss: 13.919944, mae: 27.054988, mean_q: 15.317135, mean_eps: 0.802729
  33016/150000: episode: 345, duration: 0.536s, episode steps:  92, steps per second: 172, episode reward: -122.634, mean reward: -1.333 [-100.000, 13.328], mean action: 1.565 [0.000, 3.000],  loss: 13.488680, mae: 27.780447, mean_q: 15.377464, mean_eps: 0.802183
  33106/150000: episode: 346, duration: 0.516s, episode steps:  90, steps per second: 174, episode reward: -54.182, mean reward: -0.602 [-100.000, 10.664], mean action: 1.556 [0.000, 3.000],  loss: 15.679843, mae: 28.519791, mean_q: 15.674579, mean_eps: 0.801637
  33167/150000: episode: 347, duration: 0.366s, episode steps:  61, steps per second: 167, episode reward: -56.419, mean reward: -0.925 [-100.000, 13.718], mean action: 1.574 [0.000, 3.000],  loss: 22.767553, mae: 28.271345, mean_q: 17.048802, mean_eps: 0.801184
  33233/150000: episode: 348, duration: 0.394s, episode steps:  66, steps per second: 167, episode reward: -65.678, mean reward: -0.995 [-100.000, 16.500], mean action: 1.439 [0.000, 3.000],  loss: 10.594045, mae: 28.557319, mean_q: 17.066130, mean_eps: 0.800803
  33300/150000: episode: 349, duration: 0.436s, episode steps:  67, steps per second: 154, episode reward: -56.989, mean reward: -0.851 [-100.000,  7.203], mean action: 1.448 [0.000, 3.000],  loss: 18.421384, mae: 28.936060, mean_q: 15.348977, mean_eps: 0.800404
  33399/150000: episode: 350, duration: 0.634s, episode steps:  99, steps per second: 156, episode reward: -144.034, mean reward: -1.455 [-100.000, 15.248], mean action: 1.717 [0.000, 3.000],  loss: 20.122327, mae: 28.841882, mean_q: 17.077067, mean_eps: 0.799906
  33472/150000: episode: 351, duration: 0.468s, episode steps:  73, steps per second: 156, episode reward: -70.347, mean reward: -0.964 [-100.000,  6.886], mean action: 1.507 [0.000, 3.000],  loss: 24.575130, mae: 28.916360, mean_q: 17.071338, mean_eps: 0.799390
  33548/150000: episode: 352, duration: 0.479s, episode steps:  76, steps per second: 159, episode reward: -112.182, mean reward: -1.476 [-100.000, 33.273], mean action: 1.487 [0.000, 3.000],  loss: 15.444778, mae: 28.422276, mean_q: 15.948161, mean_eps: 0.798943
  33649/150000: episode: 353, duration: 0.684s, episode steps: 101, steps per second: 148, episode reward: -90.478, mean reward: -0.896 [-100.000, 12.922], mean action: 1.554 [0.000, 3.000],  loss: 13.762680, mae: 28.678183, mean_q: 16.496240, mean_eps: 0.798412
  33734/150000: episode: 354, duration: 0.520s, episode steps:  85, steps per second: 163, episode reward: -62.800, mean reward: -0.739 [-100.000, 16.681], mean action: 1.753 [0.000, 3.000],  loss: 11.036378, mae: 28.308889, mean_q: 17.562099, mean_eps: 0.797854
  33849/150000: episode: 355, duration: 0.695s, episode steps: 115, steps per second: 166, episode reward: -80.488, mean reward: -0.700 [-100.000, 14.421], mean action: 1.591 [0.000, 3.000],  loss: 31.783922, mae: 29.186330, mean_q: 16.431317, mean_eps: 0.797254
  33976/150000: episode: 356, duration: 0.873s, episode steps: 127, steps per second: 145, episode reward: -138.722, mean reward: -1.092 [-100.000, 14.524], mean action: 1.748 [0.000, 3.000],  loss: 23.376428, mae: 29.301471, mean_q: 16.787696, mean_eps: 0.796528
  34071/150000: episode: 357, duration: 0.573s, episode steps:  95, steps per second: 166, episode reward: -93.483, mean reward: -0.984 [-100.000, 18.746], mean action: 1.558 [0.000, 3.000],  loss: 21.403855, mae: 28.621542, mean_q: 16.384661, mean_eps: 0.795862
  34202/150000: episode: 358, duration: 0.756s, episode steps: 131, steps per second: 173, episode reward: -115.769, mean reward: -0.884 [-100.000, 11.169], mean action: 1.656 [0.000, 3.000],  loss: 16.731480, mae: 28.531789, mean_q: 18.395255, mean_eps: 0.795184
  34262/150000: episode: 359, duration: 0.364s, episode steps:  60, steps per second: 165, episode reward: -76.530, mean reward: -1.276 [-100.000, 12.066], mean action: 1.333 [0.000, 3.000],  loss: 10.423331, mae: 29.226039, mean_q: 17.540894, mean_eps: 0.794611
  34370/150000: episode: 360, duration: 0.669s, episode steps: 108, steps per second: 161, episode reward: -103.770, mean reward: -0.961 [-100.000, 34.834], mean action: 1.380 [0.000, 3.000],  loss: 18.134975, mae: 29.219856, mean_q: 16.511831, mean_eps: 0.794107
  34481/150000: episode: 361, duration: 0.646s, episode steps: 111, steps per second: 172, episode reward: -62.628, mean reward: -0.564 [-100.000, 10.938], mean action: 1.676 [0.000, 3.000],  loss: 19.614248, mae: 28.917723, mean_q: 18.547425, mean_eps: 0.793450
  34549/150000: episode: 362, duration: 0.382s, episode steps:  68, steps per second: 178, episode reward: -51.298, mean reward: -0.754 [-100.000, 17.126], mean action: 1.368 [0.000, 3.000],  loss: 12.796972, mae: 28.695861, mean_q: 18.195430, mean_eps: 0.792913
  34623/150000: episode: 363, duration: 0.425s, episode steps:  74, steps per second: 174, episode reward: -98.963, mean reward: -1.337 [-100.000, 15.430], mean action: 1.784 [0.000, 3.000],  loss: 9.615579, mae: 28.865996, mean_q: 18.977930, mean_eps: 0.792487
  34741/150000: episode: 364, duration: 0.733s, episode steps: 118, steps per second: 161, episode reward: -58.619, mean reward: -0.497 [-100.000,  6.559], mean action: 1.415 [0.000, 3.000],  loss: 9.817282, mae: 29.098616, mean_q: 16.952630, mean_eps: 0.791911
  34810/150000: episode: 365, duration: 0.407s, episode steps:  69, steps per second: 169, episode reward: -77.493, mean reward: -1.123 [-100.000, 22.043], mean action: 1.768 [0.000, 3.000],  loss: 18.483115, mae: 29.134985, mean_q: 19.449199, mean_eps: 0.791350
  34884/150000: episode: 366, duration: 0.422s, episode steps:  74, steps per second: 175, episode reward: -75.841, mean reward: -1.025 [-100.000,  7.133], mean action: 1.622 [0.000, 3.000],  loss: 9.077827, mae: 29.071621, mean_q: 18.146414, mean_eps: 0.790921
  34985/150000: episode: 367, duration: 0.583s, episode steps: 101, steps per second: 173, episode reward: -292.195, mean reward: -2.893 [-100.000, 69.496], mean action: 1.663 [0.000, 3.000],  loss: 26.750195, mae: 28.791138, mean_q: 17.486668, mean_eps: 0.790396
  35134/150000: episode: 368, duration: 0.922s, episode steps: 149, steps per second: 162, episode reward: -47.680, mean reward: -0.320 [-100.000, 22.175], mean action: 1.584 [0.000, 3.000],  loss: 14.493560, mae: 29.447823, mean_q: 17.809077, mean_eps: 0.789646
  35251/150000: episode: 369, duration: 0.687s, episode steps: 117, steps per second: 170, episode reward: -99.553, mean reward: -0.851 [-100.000,  6.786], mean action: 1.513 [0.000, 3.000],  loss: 14.534734, mae: 29.505297, mean_q: 18.311448, mean_eps: 0.788848
  35332/150000: episode: 370, duration: 0.469s, episode steps:  81, steps per second: 173, episode reward: -136.819, mean reward: -1.689 [-100.000, 26.667], mean action: 1.580 [0.000, 3.000],  loss: 9.751883, mae: 28.886211, mean_q: 18.104308, mean_eps: 0.788254
  35416/150000: episode: 371, duration: 0.541s, episode steps:  84, steps per second: 155, episode reward: -83.376, mean reward: -0.993 [-100.000,  5.996], mean action: 1.417 [0.000, 3.000],  loss: 10.199074, mae: 29.768234, mean_q: 17.792363, mean_eps: 0.787759
  35524/150000: episode: 372, duration: 0.639s, episode steps: 108, steps per second: 169, episode reward: -77.584, mean reward: -0.718 [-100.000, 12.898], mean action: 1.500 [0.000, 3.000],  loss: 17.446549, mae: 29.152236, mean_q: 17.968454, mean_eps: 0.787183
  35619/150000: episode: 373, duration: 0.547s, episode steps:  95, steps per second: 174, episode reward: -24.121, mean reward: -0.254 [-100.000, 19.041], mean action: 1.600 [0.000, 3.000],  loss: 12.709337, mae: 29.359212, mean_q: 18.445825, mean_eps: 0.786574
  35694/150000: episode: 374, duration: 0.428s, episode steps:  75, steps per second: 175, episode reward: -90.038, mean reward: -1.201 [-100.000, 16.425], mean action: 1.800 [0.000, 3.000],  loss: 10.705808, mae: 29.193223, mean_q: 18.680667, mean_eps: 0.786064
  35836/150000: episode: 375, duration: 0.881s, episode steps: 142, steps per second: 161, episode reward: -121.468, mean reward: -0.855 [-100.000,  4.474], mean action: 1.535 [0.000, 3.000],  loss: 12.689404, mae: 29.459857, mean_q: 17.568554, mean_eps: 0.785413
  35896/150000: episode: 376, duration: 0.359s, episode steps:  60, steps per second: 167, episode reward: -140.047, mean reward: -2.334 [-100.000,  5.240], mean action: 1.483 [0.000, 3.000],  loss: 6.581198, mae: 29.106217, mean_q: 20.555599, mean_eps: 0.784807
  35966/150000: episode: 377, duration: 0.430s, episode steps:  70, steps per second: 163, episode reward: -59.271, mean reward: -0.847 [-100.000, 18.187], mean action: 1.800 [0.000, 3.000],  loss: 11.849304, mae: 29.259419, mean_q: 18.210101, mean_eps: 0.784417
  36034/150000: episode: 378, duration: 0.400s, episode steps:  68, steps per second: 170, episode reward: -60.303, mean reward: -0.887 [-100.000,  9.641], mean action: 1.515 [0.000, 3.000],  loss: 21.581829, mae: 29.702432, mean_q: 17.824589, mean_eps: 0.784003
  36148/150000: episode: 379, duration: 0.714s, episode steps: 114, steps per second: 160, episode reward: -33.707, mean reward: -0.296 [-100.000, 50.476], mean action: 1.588 [0.000, 3.000],  loss: 10.700733, mae: 29.934110, mean_q: 19.592414, mean_eps: 0.783457
  36246/150000: episode: 380, duration: 0.588s, episode steps:  98, steps per second: 167, episode reward: -114.383, mean reward: -1.167 [-100.000,  5.716], mean action: 1.531 [0.000, 3.000],  loss: 7.791695, mae: 29.702550, mean_q: 18.595877, mean_eps: 0.782821
  36365/150000: episode: 381, duration: 0.731s, episode steps: 119, steps per second: 163, episode reward: -75.965, mean reward: -0.638 [-100.000,  8.696], mean action: 1.605 [0.000, 3.000],  loss: 14.959544, mae: 29.914969, mean_q: 16.587004, mean_eps: 0.782170
  36452/150000: episode: 382, duration: 0.513s, episode steps:  87, steps per second: 170, episode reward: -111.286, mean reward: -1.279 [-100.000, 10.114], mean action: 1.598 [0.000, 3.000],  loss: 19.002458, mae: 30.118183, mean_q: 18.702411, mean_eps: 0.781552
  36533/150000: episode: 383, duration: 0.483s, episode steps:  81, steps per second: 168, episode reward: -99.362, mean reward: -1.227 [-100.000, 12.447], mean action: 1.457 [0.000, 3.000],  loss: 9.838809, mae: 29.780962, mean_q: 17.913806, mean_eps: 0.781048
  36623/150000: episode: 384, duration: 0.532s, episode steps:  90, steps per second: 169, episode reward: -79.471, mean reward: -0.883 [-100.000,  9.547], mean action: 1.500 [0.000, 3.000],  loss: 21.095750, mae: 30.050008, mean_q: 17.628983, mean_eps: 0.780535
  36693/150000: episode: 385, duration: 0.433s, episode steps:  70, steps per second: 162, episode reward: -62.291, mean reward: -0.890 [-100.000,  8.494], mean action: 1.286 [0.000, 3.000],  loss: 8.421790, mae: 29.746896, mean_q: 17.097739, mean_eps: 0.780055
  36767/150000: episode: 386, duration: 0.473s, episode steps:  74, steps per second: 156, episode reward: -78.161, mean reward: -1.056 [-100.000, 13.774], mean action: 1.473 [0.000, 3.000],  loss: 13.508404, mae: 30.031037, mean_q: 17.408512, mean_eps: 0.779623
  36860/150000: episode: 387, duration: 0.574s, episode steps:  93, steps per second: 162, episode reward: -75.616, mean reward: -0.813 [-100.000,  7.532], mean action: 1.613 [0.000, 3.000],  loss: 19.409242, mae: 30.160025, mean_q: 18.351391, mean_eps: 0.779122
  36972/150000: episode: 388, duration: 0.657s, episode steps: 112, steps per second: 170, episode reward: -82.152, mean reward: -0.734 [-100.000,  6.886], mean action: 1.688 [0.000, 3.000],  loss: 14.563877, mae: 30.340920, mean_q: 17.377709, mean_eps: 0.778507
  37107/150000: episode: 389, duration: 0.795s, episode steps: 135, steps per second: 170, episode reward: -87.834, mean reward: -0.651 [-100.000, 13.707], mean action: 1.489 [0.000, 3.000],  loss: 12.589971, mae: 30.597431, mean_q: 18.975693, mean_eps: 0.777766
  37199/150000: episode: 390, duration: 0.557s, episode steps:  92, steps per second: 165, episode reward: -51.020, mean reward: -0.555 [-100.000, 10.746], mean action: 1.500 [0.000, 3.000],  loss: 10.304636, mae: 30.424948, mean_q: 19.352255, mean_eps: 0.777085
  37318/150000: episode: 391, duration: 0.704s, episode steps: 119, steps per second: 169, episode reward: -2.914, mean reward: -0.024 [-100.000, 87.494], mean action: 1.571 [0.000, 3.000],  loss: 11.768478, mae: 30.103450, mean_q: 19.035195, mean_eps: 0.776452
  37400/150000: episode: 392, duration: 0.530s, episode steps:  82, steps per second: 155, episode reward: -32.099, mean reward: -0.391 [-100.000, 20.863], mean action: 1.500 [0.000, 3.000],  loss: 16.367511, mae: 30.823010, mean_q: 18.283273, mean_eps: 0.775849
  37501/150000: episode: 393, duration: 0.735s, episode steps: 101, steps per second: 137, episode reward: -125.065, mean reward: -1.238 [-100.000,  7.133], mean action: 1.564 [0.000, 3.000],  loss: 11.243603, mae: 30.132924, mean_q: 18.963226, mean_eps: 0.775300
  37588/150000: episode: 394, duration: 0.634s, episode steps:  87, steps per second: 137, episode reward: -74.618, mean reward: -0.858 [-100.000,  6.037], mean action: 1.816 [0.000, 3.000],  loss: 11.003505, mae: 30.467028, mean_q: 18.541198, mean_eps: 0.774736
  37687/150000: episode: 395, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -49.922, mean reward: -0.504 [-100.000, 16.166], mean action: 1.596 [0.000, 3.000],  loss: 7.999895, mae: 30.628762, mean_q: 19.086714, mean_eps: 0.774178
  37752/150000: episode: 396, duration: 0.397s, episode steps:  65, steps per second: 164, episode reward: -115.942, mean reward: -1.784 [-100.000,  3.924], mean action: 1.492 [0.000, 3.000],  loss: 14.649107, mae: 30.825545, mean_q: 19.285734, mean_eps: 0.773686
  37873/150000: episode: 397, duration: 0.802s, episode steps: 121, steps per second: 151, episode reward: -89.357, mean reward: -0.738 [-100.000, 25.363], mean action: 1.694 [0.000, 3.000],  loss: 8.310769, mae: 29.935401, mean_q: 19.207166, mean_eps: 0.773128
  37970/150000: episode: 398, duration: 0.622s, episode steps:  97, steps per second: 156, episode reward: -82.585, mean reward: -0.851 [-100.000,  5.238], mean action: 1.598 [0.000, 3.000],  loss: 15.838989, mae: 30.572519, mean_q: 18.374703, mean_eps: 0.772474
  38091/150000: episode: 399, duration: 0.822s, episode steps: 121, steps per second: 147, episode reward: -126.103, mean reward: -1.042 [-100.000,  6.808], mean action: 1.851 [0.000, 3.000],  loss: 18.147722, mae: 30.990507, mean_q: 20.956148, mean_eps: 0.771820
  38204/150000: episode: 400, duration: 0.684s, episode steps: 113, steps per second: 165, episode reward: -99.783, mean reward: -0.883 [-100.000, 21.481], mean action: 1.531 [0.000, 3.000],  loss: 7.253028, mae: 31.986936, mean_q: 20.528931, mean_eps: 0.771118
  38290/150000: episode: 401, duration: 0.514s, episode steps:  86, steps per second: 167, episode reward: -91.870, mean reward: -1.068 [-100.000,  9.612], mean action: 1.558 [0.000, 3.000],  loss: 10.457022, mae: 31.844897, mean_q: 21.867411, mean_eps: 0.770521
  38406/150000: episode: 402, duration: 0.675s, episode steps: 116, steps per second: 172, episode reward: -59.568, mean reward: -0.514 [-100.000, 15.169], mean action: 1.595 [0.000, 3.000],  loss: 10.521342, mae: 32.089616, mean_q: 22.886033, mean_eps: 0.769915
  38499/150000: episode: 403, duration: 0.551s, episode steps:  93, steps per second: 169, episode reward: 52.168, mean reward:  0.561 [-100.000, 135.595], mean action: 1.462 [0.000, 3.000],  loss: 5.743675, mae: 31.563414, mean_q: 22.424538, mean_eps: 0.769288
  38579/150000: episode: 404, duration: 0.513s, episode steps:  80, steps per second: 156, episode reward: -92.242, mean reward: -1.153 [-100.000, 17.504], mean action: 1.750 [0.000, 3.000],  loss: 7.967226, mae: 32.041345, mean_q: 22.517234, mean_eps: 0.768769
  38674/150000: episode: 405, duration: 0.561s, episode steps:  95, steps per second: 169, episode reward: -71.780, mean reward: -0.756 [-100.000, 61.821], mean action: 1.705 [0.000, 3.000],  loss: 10.933109, mae: 31.077971, mean_q: 21.365736, mean_eps: 0.768244
  38780/150000: episode: 406, duration: 0.632s, episode steps: 106, steps per second: 168, episode reward: -49.053, mean reward: -0.463 [-100.000, 16.720], mean action: 1.368 [0.000, 3.000],  loss: 21.362657, mae: 31.600158, mean_q: 20.817365, mean_eps: 0.767641
  38842/150000: episode: 407, duration: 0.376s, episode steps:  62, steps per second: 165, episode reward: -93.666, mean reward: -1.511 [-100.000,  9.741], mean action: 1.516 [0.000, 3.000],  loss: 7.044279, mae: 32.530239, mean_q: 21.536872, mean_eps: 0.767137
  38926/150000: episode: 408, duration: 0.526s, episode steps:  84, steps per second: 160, episode reward: -80.257, mean reward: -0.955 [-100.000,  6.874], mean action: 1.476 [0.000, 3.000],  loss: 20.061749, mae: 31.399899, mean_q: 23.162884, mean_eps: 0.766699
  38993/150000: episode: 409, duration: 0.405s, episode steps:  67, steps per second: 165, episode reward: -39.184, mean reward: -0.585 [-100.000, 12.660], mean action: 1.567 [0.000, 3.000],  loss: 6.833141, mae: 32.571210, mean_q: 21.695838, mean_eps: 0.766246
  39086/150000: episode: 410, duration: 0.535s, episode steps:  93, steps per second: 174, episode reward: -88.753, mean reward: -0.954 [-100.000,  6.004], mean action: 1.602 [0.000, 3.000],  loss: 8.804042, mae: 32.025460, mean_q: 20.964350, mean_eps: 0.765766
  39171/150000: episode: 411, duration: 0.515s, episode steps:  85, steps per second: 165, episode reward: -60.159, mean reward: -0.708 [-100.000,  6.862], mean action: 1.482 [0.000, 3.000],  loss: 16.935324, mae: 32.048167, mean_q: 21.986837, mean_eps: 0.765232
  39274/150000: episode: 412, duration: 0.647s, episode steps: 103, steps per second: 159, episode reward: -112.704, mean reward: -1.094 [-100.000, 10.342], mean action: 1.738 [0.000, 3.000],  loss: 10.014724, mae: 31.727449, mean_q: 21.904897, mean_eps: 0.764668
  39411/150000: episode: 413, duration: 0.805s, episode steps: 137, steps per second: 170, episode reward: -77.314, mean reward: -0.564 [-100.000,  9.591], mean action: 1.664 [0.000, 3.000],  loss: 11.912717, mae: 31.802665, mean_q: 21.841873, mean_eps: 0.763948
  39504/150000: episode: 414, duration: 0.538s, episode steps:  93, steps per second: 173, episode reward: -64.078, mean reward: -0.689 [-100.000,  7.188], mean action: 1.398 [0.000, 3.000],  loss: 19.031283, mae: 31.408772, mean_q: 20.394058, mean_eps: 0.763258
  39599/150000: episode: 415, duration: 0.593s, episode steps:  95, steps per second: 160, episode reward: -70.794, mean reward: -0.745 [-100.000, 13.139], mean action: 1.516 [0.000, 3.000],  loss: 21.135345, mae: 31.908033, mean_q: 21.391208, mean_eps: 0.762694
  39667/150000: episode: 416, duration: 0.424s, episode steps:  68, steps per second: 160, episode reward: -116.834, mean reward: -1.718 [-100.000,  7.467], mean action: 1.574 [0.000, 3.000],  loss: 19.569918, mae: 31.184220, mean_q: 21.971199, mean_eps: 0.762205
  39739/150000: episode: 417, duration: 0.426s, episode steps:  72, steps per second: 169, episode reward: -61.697, mean reward: -0.857 [-100.000, 11.145], mean action: 1.653 [0.000, 3.000],  loss: 11.227647, mae: 31.554214, mean_q: 21.781513, mean_eps: 0.761785
  39854/150000: episode: 418, duration: 0.661s, episode steps: 115, steps per second: 174, episode reward: -75.516, mean reward: -0.657 [-100.000,  7.039], mean action: 1.504 [0.000, 3.000],  loss: 15.461467, mae: 31.487079, mean_q: 22.373096, mean_eps: 0.761224
  40854/150000: episode: 419, duration: 6.661s, episode steps: 1000, steps per second: 150, episode reward: 55.901, mean reward:  0.056 [-24.244, 45.438], mean action: 1.683 [0.000, 3.000],  loss: 10.375741, mae: 31.860881, mean_q: 20.531193, mean_eps: 0.757879
  40929/150000: episode: 420, duration: 0.457s, episode steps:  75, steps per second: 164, episode reward: -76.611, mean reward: -1.021 [-100.000,  6.630], mean action: 1.360 [0.000, 3.000],  loss: 12.505760, mae: 31.634233, mean_q: 20.526621, mean_eps: 0.754654
  41049/150000: episode: 421, duration: 0.746s, episode steps: 120, steps per second: 161, episode reward: -116.289, mean reward: -0.969 [-100.000, 10.268], mean action: 1.583 [0.000, 3.000],  loss: 11.376257, mae: 32.319388, mean_q: 20.013494, mean_eps: 0.754069
  41116/150000: episode: 422, duration: 0.396s, episode steps:  67, steps per second: 169, episode reward: -115.191, mean reward: -1.719 [-100.000,  7.039], mean action: 1.507 [0.000, 3.000],  loss: 8.695183, mae: 32.046923, mean_q: 19.429961, mean_eps: 0.753508
  41194/150000: episode: 423, duration: 0.454s, episode steps:  78, steps per second: 172, episode reward: -90.491, mean reward: -1.160 [-100.000,  8.402], mean action: 1.551 [0.000, 3.000],  loss: 15.948421, mae: 32.952504, mean_q: 20.739071, mean_eps: 0.753073
  41266/150000: episode: 424, duration: 0.422s, episode steps:  72, steps per second: 171, episode reward: -76.899, mean reward: -1.068 [-100.000,  7.803], mean action: 1.583 [0.000, 3.000],  loss: 10.857144, mae: 32.434307, mean_q: 20.715231, mean_eps: 0.752623
  41375/150000: episode: 425, duration: 0.710s, episode steps: 109, steps per second: 153, episode reward: -171.739, mean reward: -1.576 [-100.000,  7.456], mean action: 1.725 [0.000, 3.000],  loss: 15.469107, mae: 32.897106, mean_q: 20.529155, mean_eps: 0.752080
  41470/150000: episode: 426, duration: 0.578s, episode steps:  95, steps per second: 164, episode reward: -49.017, mean reward: -0.516 [-100.000, 17.409], mean action: 1.579 [0.000, 3.000],  loss: 13.889739, mae: 32.863992, mean_q: 20.588606, mean_eps: 0.751468
  41550/150000: episode: 427, duration: 0.480s, episode steps:  80, steps per second: 167, episode reward: -79.684, mean reward: -0.996 [-100.000,  7.640], mean action: 1.688 [0.000, 3.000],  loss: 13.223561, mae: 32.904629, mean_q: 21.067037, mean_eps: 0.750943
  41674/150000: episode: 428, duration: 0.771s, episode steps: 124, steps per second: 161, episode reward: -100.192, mean reward: -0.808 [-100.000, 10.344], mean action: 1.548 [0.000, 3.000],  loss: 8.742951, mae: 32.408223, mean_q: 20.613500, mean_eps: 0.750331
  41745/150000: episode: 429, duration: 0.451s, episode steps:  71, steps per second: 157, episode reward: -88.694, mean reward: -1.249 [-100.000,  6.733], mean action: 1.746 [0.000, 3.000],  loss: 14.230624, mae: 32.696614, mean_q: 23.026699, mean_eps: 0.749746
  41819/150000: episode: 430, duration: 0.449s, episode steps:  74, steps per second: 165, episode reward: -96.302, mean reward: -1.301 [-100.000,  9.849], mean action: 1.635 [0.000, 3.000],  loss: 11.179721, mae: 33.189321, mean_q: 21.901511, mean_eps: 0.749311
  41897/150000: episode: 431, duration: 0.466s, episode steps:  78, steps per second: 167, episode reward: -25.527, mean reward: -0.327 [-100.000,  7.543], mean action: 1.744 [0.000, 3.000],  loss: 12.314652, mae: 32.606564, mean_q: 19.816412, mean_eps: 0.748855
  42003/150000: episode: 432, duration: 0.651s, episode steps: 106, steps per second: 163, episode reward: -91.481, mean reward: -0.863 [-100.000, 10.262], mean action: 1.670 [0.000, 3.000],  loss: 10.213252, mae: 32.663590, mean_q: 19.765051, mean_eps: 0.748303
  42134/150000: episode: 433, duration: 0.792s, episode steps: 131, steps per second: 165, episode reward: -90.089, mean reward: -0.688 [-100.000,  7.302], mean action: 1.458 [0.000, 3.000],  loss: 7.720194, mae: 33.005584, mean_q: 21.579915, mean_eps: 0.747592
  42204/150000: episode: 434, duration: 0.422s, episode steps:  70, steps per second: 166, episode reward: -54.469, mean reward: -0.778 [-100.000,  9.663], mean action: 1.486 [0.000, 3.000],  loss: 8.609252, mae: 32.376726, mean_q: 20.086853, mean_eps: 0.746989
  42290/150000: episode: 435, duration: 0.514s, episode steps:  86, steps per second: 167, episode reward: -65.689, mean reward: -0.764 [-100.000, 11.191], mean action: 1.709 [0.000, 3.000],  loss: 5.201783, mae: 32.165636, mean_q: 21.326639, mean_eps: 0.746521
  42387/150000: episode: 436, duration: 0.610s, episode steps:  97, steps per second: 159, episode reward: -120.726, mean reward: -1.245 [-100.000, 30.720], mean action: 1.557 [0.000, 3.000],  loss: 13.020872, mae: 33.402661, mean_q: 21.477982, mean_eps: 0.745972
  42480/150000: episode: 437, duration: 0.675s, episode steps:  93, steps per second: 138, episode reward: -50.084, mean reward: -0.539 [-100.000, 13.395], mean action: 1.581 [0.000, 3.000],  loss: 9.061795, mae: 33.011112, mean_q: 20.109511, mean_eps: 0.745402
  42569/150000: episode: 438, duration: 0.511s, episode steps:  89, steps per second: 174, episode reward: -107.963, mean reward: -1.213 [-100.000,  6.688], mean action: 1.438 [0.000, 3.000],  loss: 7.575170, mae: 32.808152, mean_q: 21.008328, mean_eps: 0.744856
  42662/150000: episode: 439, duration: 0.537s, episode steps:  93, steps per second: 173, episode reward: -108.365, mean reward: -1.165 [-100.000,  7.209], mean action: 1.667 [0.000, 3.000],  loss: 13.523143, mae: 32.894329, mean_q: 20.649586, mean_eps: 0.744310
  42723/150000: episode: 440, duration: 0.424s, episode steps:  61, steps per second: 144, episode reward: -91.390, mean reward: -1.498 [-100.000,  7.359], mean action: 1.377 [0.000, 3.000],  loss: 11.119619, mae: 32.564862, mean_q: 20.764367, mean_eps: 0.743848
  42813/150000: episode: 441, duration: 0.550s, episode steps:  90, steps per second: 164, episode reward: -58.136, mean reward: -0.646 [-100.000, 12.271], mean action: 1.611 [0.000, 3.000],  loss: 14.006939, mae: 32.808208, mean_q: 19.960042, mean_eps: 0.743395
  42879/150000: episode: 442, duration: 0.394s, episode steps:  66, steps per second: 167, episode reward: -78.569, mean reward: -1.190 [-100.000, 17.705], mean action: 1.561 [0.000, 3.000],  loss: 6.584854, mae: 32.562083, mean_q: 20.544348, mean_eps: 0.742927
  42988/150000: episode: 443, duration: 0.637s, episode steps: 109, steps per second: 171, episode reward: -82.769, mean reward: -0.759 [-100.000, 11.612], mean action: 1.431 [0.000, 3.000],  loss: 8.525426, mae: 33.385209, mean_q: 21.021430, mean_eps: 0.742402
  43108/150000: episode: 444, duration: 0.775s, episode steps: 120, steps per second: 155, episode reward: -110.340, mean reward: -0.920 [-100.000, 10.961], mean action: 1.608 [0.000, 3.000],  loss: 12.919095, mae: 33.628667, mean_q: 20.764050, mean_eps: 0.741715
  43242/150000: episode: 445, duration: 1.075s, episode steps: 134, steps per second: 125, episode reward: -71.172, mean reward: -0.531 [-100.000, 23.468], mean action: 1.567 [0.000, 3.000],  loss: 13.868429, mae: 32.881491, mean_q: 19.515353, mean_eps: 0.740953
  43308/150000: episode: 446, duration: 0.693s, episode steps:  66, steps per second:  95, episode reward: -46.851, mean reward: -0.710 [-100.000,  6.483], mean action: 1.500 [0.000, 3.000],  loss: 15.607028, mae: 32.996491, mean_q: 22.082972, mean_eps: 0.740353
  43408/150000: episode: 447, duration: 1.084s, episode steps: 100, steps per second:  92, episode reward: -52.166, mean reward: -0.522 [-100.000, 13.119], mean action: 1.700 [0.000, 3.000],  loss: 5.089909, mae: 33.225180, mean_q: 19.819566, mean_eps: 0.739855
  43510/150000: episode: 448, duration: 0.911s, episode steps: 102, steps per second: 112, episode reward: -106.307, mean reward: -1.042 [-100.000,  6.458], mean action: 1.402 [0.000, 3.000],  loss: 3.502373, mae: 32.915776, mean_q: 22.387290, mean_eps: 0.739249
  43617/150000: episode: 449, duration: 0.942s, episode steps: 107, steps per second: 114, episode reward: -75.577, mean reward: -0.706 [-100.000, 14.085], mean action: 1.757 [0.000, 3.000],  loss: 6.719785, mae: 33.241915, mean_q: 20.376002, mean_eps: 0.738622
  43734/150000: episode: 450, duration: 0.916s, episode steps: 117, steps per second: 128, episode reward: -109.010, mean reward: -0.932 [-100.000, 10.289], mean action: 1.513 [0.000, 3.000],  loss: 9.049188, mae: 33.055101, mean_q: 22.895608, mean_eps: 0.737950
  43838/150000: episode: 451, duration: 0.894s, episode steps: 104, steps per second: 116, episode reward: -87.093, mean reward: -0.837 [-100.000, 11.278], mean action: 1.558 [0.000, 3.000],  loss: 13.955248, mae: 34.188265, mean_q: 22.932960, mean_eps: 0.737287
  43951/150000: episode: 452, duration: 0.860s, episode steps: 113, steps per second: 131, episode reward: -68.765, mean reward: -0.609 [-100.000, 10.436], mean action: 1.593 [0.000, 3.000],  loss: 15.039743, mae: 33.699861, mean_q: 22.092699, mean_eps: 0.736636
  44023/150000: episode: 453, duration: 0.592s, episode steps:  72, steps per second: 122, episode reward: -137.171, mean reward: -1.905 [-100.000,  6.112], mean action: 1.833 [0.000, 3.000],  loss: 6.253960, mae: 33.579909, mean_q: 22.213230, mean_eps: 0.736081
  44115/150000: episode: 454, duration: 0.754s, episode steps:  92, steps per second: 122, episode reward: -73.430, mean reward: -0.798 [-100.000,  7.193], mean action: 1.783 [0.000, 3.000],  loss: 10.500887, mae: 33.359004, mean_q: 21.676164, mean_eps: 0.735589
  44196/150000: episode: 455, duration: 0.642s, episode steps:  81, steps per second: 126, episode reward: -157.399, mean reward: -1.943 [-100.000,  7.531], mean action: 1.827 [0.000, 3.000],  loss: 10.376024, mae: 33.166532, mean_q: 22.000756, mean_eps: 0.735070
  44306/150000: episode: 456, duration: 0.828s, episode steps: 110, steps per second: 133, episode reward: -97.477, mean reward: -0.886 [-100.000,  9.643], mean action: 1.282 [0.000, 3.000],  loss: 11.163722, mae: 33.459738, mean_q: 21.645325, mean_eps: 0.734497
  44387/150000: episode: 457, duration: 0.633s, episode steps:  81, steps per second: 128, episode reward: -79.190, mean reward: -0.978 [-100.000, 10.216], mean action: 1.519 [0.000, 3.000],  loss: 18.533015, mae: 32.641110, mean_q: 21.903019, mean_eps: 0.733924
  44475/150000: episode: 458, duration: 0.664s, episode steps:  88, steps per second: 133, episode reward: -94.547, mean reward: -1.074 [-100.000,  6.423], mean action: 1.693 [0.000, 3.000],  loss: 28.429294, mae: 33.365208, mean_q: 21.297638, mean_eps: 0.733417
  44603/150000: episode: 459, duration: 0.934s, episode steps: 128, steps per second: 137, episode reward: -69.453, mean reward: -0.543 [-100.000, 13.567], mean action: 1.578 [0.000, 3.000],  loss: 13.282361, mae: 33.264828, mean_q: 21.724777, mean_eps: 0.732769
  44704/150000: episode: 460, duration: 0.769s, episode steps: 101, steps per second: 131, episode reward: -82.672, mean reward: -0.819 [-100.000, 13.037], mean action: 1.535 [0.000, 3.000],  loss: 9.392592, mae: 33.131495, mean_q: 21.649286, mean_eps: 0.732082
  44793/150000: episode: 461, duration: 0.657s, episode steps:  89, steps per second: 135, episode reward: -189.438, mean reward: -2.129 [-100.000, 63.614], mean action: 1.337 [0.000, 3.000],  loss: 11.770960, mae: 32.815772, mean_q: 21.476589, mean_eps: 0.731512
  44898/150000: episode: 462, duration: 0.717s, episode steps: 105, steps per second: 146, episode reward: -118.836, mean reward: -1.132 [-100.000,  7.727], mean action: 1.486 [0.000, 3.000],  loss: 14.177934, mae: 33.581643, mean_q: 21.797742, mean_eps: 0.730930
  44984/150000: episode: 463, duration: 0.700s, episode steps:  86, steps per second: 123, episode reward: -33.284, mean reward: -0.387 [-100.000, 22.012], mean action: 1.581 [0.000, 3.000],  loss: 12.763701, mae: 33.238097, mean_q: 19.992469, mean_eps: 0.730357
  45098/150000: episode: 464, duration: 1.052s, episode steps: 114, steps per second: 108, episode reward: -94.329, mean reward: -0.827 [-100.000,  8.219], mean action: 1.509 [0.000, 3.000],  loss: 6.543464, mae: 32.844835, mean_q: 23.232531, mean_eps: 0.729757
  45207/150000: episode: 465, duration: 1.059s, episode steps: 109, steps per second: 103, episode reward: -84.511, mean reward: -0.775 [-100.000,  4.892], mean action: 1.661 [0.000, 3.000],  loss: 13.907697, mae: 33.103963, mean_q: 21.702523, mean_eps: 0.729088
  45364/150000: episode: 466, duration: 1.355s, episode steps: 157, steps per second: 116, episode reward: -56.670, mean reward: -0.361 [-100.000, 13.690], mean action: 1.561 [0.000, 3.000],  loss: 7.039236, mae: 32.372139, mean_q: 21.854719, mean_eps: 0.728290
  45471/150000: episode: 467, duration: 0.958s, episode steps: 107, steps per second: 112, episode reward: -102.676, mean reward: -0.960 [-100.000, 16.158], mean action: 1.411 [0.000, 3.000],  loss: 9.290567, mae: 32.895537, mean_q: 22.374754, mean_eps: 0.727498
  46471/150000: episode: 468, duration: 9.501s, episode steps: 1000, steps per second: 105, episode reward:  7.434, mean reward:  0.007 [-26.343, 111.192], mean action: 1.685 [0.000, 3.000],  loss: 10.048740, mae: 32.670461, mean_q: 21.115081, mean_eps: 0.724177
  46567/150000: episode: 469, duration: 0.945s, episode steps:  96, steps per second: 102, episode reward: -78.233, mean reward: -0.815 [-100.000,  6.585], mean action: 1.688 [0.000, 3.000],  loss: 11.028223, mae: 33.082872, mean_q: 20.034424, mean_eps: 0.720889
  46642/150000: episode: 470, duration: 0.787s, episode steps:  75, steps per second:  95, episode reward: -46.614, mean reward: -0.622 [-100.000, 10.632], mean action: 1.613 [0.000, 3.000],  loss: 7.098382, mae: 33.260468, mean_q: 21.241725, mean_eps: 0.720376
  46744/150000: episode: 471, duration: 1.051s, episode steps: 102, steps per second:  97, episode reward: -58.190, mean reward: -0.570 [-100.000, 13.955], mean action: 1.490 [0.000, 3.000],  loss: 11.748388, mae: 32.897395, mean_q: 21.549637, mean_eps: 0.719845
  46833/150000: episode: 472, duration: 0.996s, episode steps:  89, steps per second:  89, episode reward: -75.618, mean reward: -0.850 [-100.000,  9.438], mean action: 1.764 [0.000, 3.000],  loss: 9.098993, mae: 32.585003, mean_q: 22.036841, mean_eps: 0.719272
  46929/150000: episode: 473, duration: 0.944s, episode steps:  96, steps per second: 102, episode reward: -53.854, mean reward: -0.561 [-100.000, 12.447], mean action: 1.469 [0.000, 3.000],  loss: 11.669605, mae: 32.280194, mean_q: 21.313956, mean_eps: 0.718717
  47030/150000: episode: 474, duration: 1.039s, episode steps: 101, steps per second:  97, episode reward: -33.734, mean reward: -0.334 [-100.000, 28.466], mean action: 1.673 [0.000, 3.000],  loss: 12.683541, mae: 32.509634, mean_q: 20.838750, mean_eps: 0.718126
  47134/150000: episode: 475, duration: 0.980s, episode steps: 104, steps per second: 106, episode reward: -37.776, mean reward: -0.363 [-100.000, 17.748], mean action: 1.577 [0.000, 3.000],  loss: 11.149528, mae: 34.106384, mean_q: 21.733755, mean_eps: 0.717511
  47252/150000: episode: 476, duration: 1.096s, episode steps: 118, steps per second: 108, episode reward: -94.126, mean reward: -0.798 [-100.000,  6.628], mean action: 1.746 [0.000, 3.000],  loss: 18.714512, mae: 33.200806, mean_q: 22.643840, mean_eps: 0.716845
  47374/150000: episode: 477, duration: 1.087s, episode steps: 122, steps per second: 112, episode reward: -76.358, mean reward: -0.626 [-100.000, 10.907], mean action: 1.434 [0.000, 3.000],  loss: 11.016598, mae: 33.805138, mean_q: 22.279424, mean_eps: 0.716125
  47496/150000: episode: 478, duration: 1.169s, episode steps: 122, steps per second: 104, episode reward: 28.941, mean reward:  0.237 [-100.000, 39.913], mean action: 1.557 [0.000, 3.000],  loss: 10.572865, mae: 33.679777, mean_q: 21.369826, mean_eps: 0.715393
  47577/150000: episode: 479, duration: 0.767s, episode steps:  81, steps per second: 106, episode reward: -117.576, mean reward: -1.452 [-100.000, 18.606], mean action: 1.728 [0.000, 3.000],  loss: 16.371541, mae: 33.526616, mean_q: 23.395336, mean_eps: 0.714784
  47687/150000: episode: 480, duration: 0.990s, episode steps: 110, steps per second: 111, episode reward: -90.211, mean reward: -0.820 [-100.000, 19.693], mean action: 1.409 [0.000, 3.000],  loss: 20.144372, mae: 33.253403, mean_q: 21.744673, mean_eps: 0.714211
  47797/150000: episode: 481, duration: 1.037s, episode steps: 110, steps per second: 106, episode reward: -22.729, mean reward: -0.207 [-100.000, 10.613], mean action: 1.500 [0.000, 3.000],  loss: 13.450885, mae: 33.590908, mean_q: 22.070449, mean_eps: 0.713551
  47898/150000: episode: 482, duration: 0.929s, episode steps: 101, steps per second: 109, episode reward: -72.180, mean reward: -0.715 [-100.000,  7.884], mean action: 1.386 [0.000, 3.000],  loss: 20.111901, mae: 33.443536, mean_q: 22.589380, mean_eps: 0.712918
  47993/150000: episode: 483, duration: 0.895s, episode steps:  95, steps per second: 106, episode reward: -94.272, mean reward: -0.992 [-100.000, 11.132], mean action: 1.547 [0.000, 3.000],  loss: 13.157082, mae: 34.079830, mean_q: 22.814573, mean_eps: 0.712330
  48125/150000: episode: 484, duration: 1.231s, episode steps: 132, steps per second: 107, episode reward: -87.768, mean reward: -0.665 [-100.000, 13.697], mean action: 1.477 [0.000, 3.000],  loss: 7.512234, mae: 33.133263, mean_q: 19.958216, mean_eps: 0.711649
  48235/150000: episode: 485, duration: 0.978s, episode steps: 110, steps per second: 112, episode reward: -82.689, mean reward: -0.752 [-100.000, 12.034], mean action: 1.509 [0.000, 3.000],  loss: 13.156084, mae: 33.842864, mean_q: 21.044260, mean_eps: 0.710923
  48348/150000: episode: 486, duration: 0.954s, episode steps: 113, steps per second: 118, episode reward: -68.368, mean reward: -0.605 [-100.000,  7.112], mean action: 1.496 [0.000, 3.000],  loss: 11.915091, mae: 33.495557, mean_q: 22.004871, mean_eps: 0.710254
  48422/150000: episode: 487, duration: 0.648s, episode steps:  74, steps per second: 114, episode reward: -58.942, mean reward: -0.797 [-100.000, 10.630], mean action: 1.838 [0.000, 3.000],  loss: 10.809650, mae: 33.583831, mean_q: 21.766827, mean_eps: 0.709693
  48505/150000: episode: 488, duration: 0.753s, episode steps:  83, steps per second: 110, episode reward: -5.001, mean reward: -0.060 [-100.000, 20.518], mean action: 1.614 [0.000, 3.000],  loss: 12.740204, mae: 33.726684, mean_q: 21.294624, mean_eps: 0.709222
  48612/150000: episode: 489, duration: 0.940s, episode steps: 107, steps per second: 114, episode reward: -39.292, mean reward: -0.367 [-100.000, 12.414], mean action: 1.561 [0.000, 3.000],  loss: 8.281821, mae: 33.460200, mean_q: 22.052408, mean_eps: 0.708652
  48688/150000: episode: 490, duration: 0.682s, episode steps:  76, steps per second: 111, episode reward: -95.478, mean reward: -1.256 [-100.000,  8.464], mean action: 1.579 [0.000, 3.000],  loss: 5.462698, mae: 33.656135, mean_q: 20.594269, mean_eps: 0.708103
  48800/150000: episode: 491, duration: 0.965s, episode steps: 112, steps per second: 116, episode reward: -100.309, mean reward: -0.896 [-100.000,  9.686], mean action: 1.598 [0.000, 3.000],  loss: 9.372355, mae: 33.735638, mean_q: 21.771667, mean_eps: 0.707539
  48925/150000: episode: 492, duration: 1.105s, episode steps: 125, steps per second: 113, episode reward: -74.138, mean reward: -0.593 [-100.000, 22.358], mean action: 1.568 [0.000, 3.000],  loss: 17.373002, mae: 33.278218, mean_q: 22.196818, mean_eps: 0.706828
  49022/150000: episode: 493, duration: 0.854s, episode steps:  97, steps per second: 114, episode reward: -63.890, mean reward: -0.659 [-100.000, 11.059], mean action: 1.722 [0.000, 3.000],  loss: 13.366730, mae: 33.383069, mean_q: 21.163721, mean_eps: 0.706162
  49113/150000: episode: 494, duration: 0.791s, episode steps:  91, steps per second: 115, episode reward: -93.423, mean reward: -1.027 [-100.000,  6.378], mean action: 1.308 [0.000, 3.000],  loss: 14.033123, mae: 34.322569, mean_q: 22.294138, mean_eps: 0.705598
  49197/150000: episode: 495, duration: 0.774s, episode steps:  84, steps per second: 109, episode reward: -54.340, mean reward: -0.647 [-100.000, 13.339], mean action: 1.619 [0.000, 3.000],  loss: 10.685867, mae: 34.373517, mean_q: 20.641239, mean_eps: 0.705073
  49262/150000: episode: 496, duration: 0.580s, episode steps:  65, steps per second: 112, episode reward: -80.425, mean reward: -1.237 [-100.000, 14.709], mean action: 1.677 [0.000, 3.000],  loss: 13.164085, mae: 33.985715, mean_q: 23.199167, mean_eps: 0.704626
  49374/150000: episode: 497, duration: 0.988s, episode steps: 112, steps per second: 113, episode reward: -60.151, mean reward: -0.537 [-100.000,  7.201], mean action: 1.634 [0.000, 3.000],  loss: 12.847584, mae: 33.882165, mean_q: 21.003207, mean_eps: 0.704095
  49465/150000: episode: 498, duration: 0.819s, episode steps:  91, steps per second: 111, episode reward: -73.980, mean reward: -0.813 [-100.000, 12.600], mean action: 1.703 [0.000, 3.000],  loss: 8.958681, mae: 34.075289, mean_q: 22.136726, mean_eps: 0.703486
  49601/150000: episode: 499, duration: 1.201s, episode steps: 136, steps per second: 113, episode reward: -20.852, mean reward: -0.153 [-100.000, 10.095], mean action: 1.610 [0.000, 3.000],  loss: 15.024547, mae: 34.595760, mean_q: 21.575838, mean_eps: 0.702805
  49679/150000: episode: 500, duration: 0.786s, episode steps:  78, steps per second:  99, episode reward: -92.996, mean reward: -1.192 [-100.000, 11.731], mean action: 1.667 [0.000, 3.000],  loss: 24.409406, mae: 34.003755, mean_q: 21.803038, mean_eps: 0.702163
  49772/150000: episode: 501, duration: 0.831s, episode steps:  93, steps per second: 112, episode reward: -87.738, mean reward: -0.943 [-100.000, 11.842], mean action: 1.688 [0.000, 3.000],  loss: 13.459017, mae: 34.012727, mean_q: 21.469232, mean_eps: 0.701650
  49895/150000: episode: 502, duration: 1.122s, episode steps: 123, steps per second: 110, episode reward: -163.744, mean reward: -1.331 [-100.000,  7.532], mean action: 1.398 [0.000, 3.000],  loss: 14.726482, mae: 34.100559, mean_q: 21.719268, mean_eps: 0.701002
  49977/150000: episode: 503, duration: 0.722s, episode steps:  82, steps per second: 114, episode reward: -42.665, mean reward: -0.520 [-100.000,  9.224], mean action: 1.610 [0.000, 3.000],  loss: 10.923967, mae: 34.029688, mean_q: 21.875729, mean_eps: 0.700387
  50044/150000: episode: 504, duration: 0.609s, episode steps:  67, steps per second: 110, episode reward: -46.841, mean reward: -0.699 [-100.000,  9.781], mean action: 1.731 [0.000, 3.000],  loss: 15.375334, mae: 33.964188, mean_q: 21.761854, mean_eps: 0.699940
  50138/150000: episode: 505, duration: 0.844s, episode steps:  94, steps per second: 111, episode reward: -29.694, mean reward: -0.316 [-100.000, 12.407], mean action: 1.766 [0.000, 3.000],  loss: 10.571818, mae: 33.970156, mean_q: 20.389788, mean_eps: 0.699457
  50238/150000: episode: 506, duration: 0.873s, episode steps: 100, steps per second: 115, episode reward: 46.308, mean reward:  0.463 [-100.000, 18.672], mean action: 1.650 [0.000, 3.000],  loss: 11.759296, mae: 34.515921, mean_q: 20.970234, mean_eps: 0.698875
  50342/150000: episode: 507, duration: 0.895s, episode steps: 104, steps per second: 116, episode reward: -103.939, mean reward: -0.999 [-100.000,  5.125], mean action: 1.750 [0.000, 3.000],  loss: 8.180468, mae: 33.536806, mean_q: 20.662952, mean_eps: 0.698263
  50442/150000: episode: 508, duration: 0.828s, episode steps: 100, steps per second: 121, episode reward: -39.536, mean reward: -0.395 [-100.000, 15.757], mean action: 1.570 [0.000, 3.000],  loss: 16.657103, mae: 33.914178, mean_q: 20.762097, mean_eps: 0.697651
  50565/150000: episode: 509, duration: 0.919s, episode steps: 123, steps per second: 134, episode reward: -78.743, mean reward: -0.640 [-100.000,  8.450], mean action: 1.520 [0.000, 3.000],  loss: 13.955372, mae: 34.083484, mean_q: 22.094772, mean_eps: 0.696982
  50643/150000: episode: 510, duration: 0.574s, episode steps:  78, steps per second: 136, episode reward: -86.241, mean reward: -1.106 [-100.000,  7.560], mean action: 1.667 [0.000, 3.000],  loss: 17.681470, mae: 33.774270, mean_q: 21.419488, mean_eps: 0.696379
  50708/150000: episode: 511, duration: 0.517s, episode steps:  65, steps per second: 126, episode reward: -75.093, mean reward: -1.155 [-100.000, 13.273], mean action: 1.385 [0.000, 3.000],  loss: 14.482177, mae: 33.736949, mean_q: 21.538787, mean_eps: 0.695950
  50845/150000: episode: 512, duration: 1.019s, episode steps: 137, steps per second: 134, episode reward: -91.844, mean reward: -0.670 [-100.000,  9.342], mean action: 1.489 [0.000, 3.000],  loss: 10.666786, mae: 33.632577, mean_q: 20.497519, mean_eps: 0.695344
  50917/150000: episode: 513, duration: 0.540s, episode steps:  72, steps per second: 133, episode reward: -38.752, mean reward: -0.538 [-100.000, 47.461], mean action: 1.819 [0.000, 3.000],  loss: 5.585616, mae: 34.694196, mean_q: 22.417926, mean_eps: 0.694717
  51022/150000: episode: 514, duration: 0.900s, episode steps: 105, steps per second: 117, episode reward: -13.307, mean reward: -0.127 [-100.000, 14.667], mean action: 1.724 [0.000, 3.000],  loss: 8.758612, mae: 33.149034, mean_q: 21.417934, mean_eps: 0.694186
  51090/150000: episode: 515, duration: 0.523s, episode steps:  68, steps per second: 130, episode reward: -47.865, mean reward: -0.704 [-100.000,  9.250], mean action: 1.632 [0.000, 3.000],  loss: 14.402402, mae: 34.246644, mean_q: 21.525943, mean_eps: 0.693667
  51195/150000: episode: 516, duration: 0.773s, episode steps: 105, steps per second: 136, episode reward: -46.853, mean reward: -0.446 [-100.000, 13.496], mean action: 1.590 [0.000, 3.000],  loss: 18.293224, mae: 33.666731, mean_q: 21.520984, mean_eps: 0.693148
  51300/150000: episode: 517, duration: 0.822s, episode steps: 105, steps per second: 128, episode reward: -78.019, mean reward: -0.743 [-100.000,  6.450], mean action: 1.695 [0.000, 3.000],  loss: 6.792836, mae: 34.065181, mean_q: 21.590690, mean_eps: 0.692518
  51394/150000: episode: 518, duration: 0.805s, episode steps:  94, steps per second: 117, episode reward: -112.425, mean reward: -1.196 [-100.000, 22.208], mean action: 1.606 [0.000, 3.000],  loss: 11.837090, mae: 33.339179, mean_q: 22.001976, mean_eps: 0.691921
  51475/150000: episode: 519, duration: 0.635s, episode steps:  81, steps per second: 127, episode reward: -32.156, mean reward: -0.397 [-100.000, 15.729], mean action: 1.580 [0.000, 3.000],  loss: 16.313753, mae: 34.023684, mean_q: 22.583056, mean_eps: 0.691396
  51581/150000: episode: 520, duration: 0.871s, episode steps: 106, steps per second: 122, episode reward: -151.862, mean reward: -1.433 [-100.000,  3.079], mean action: 1.462 [0.000, 3.000],  loss: 8.476843, mae: 33.401447, mean_q: 21.612092, mean_eps: 0.690835
  51715/150000: episode: 521, duration: 1.044s, episode steps: 134, steps per second: 128, episode reward: -32.731, mean reward: -0.244 [-100.000, 12.444], mean action: 1.627 [0.000, 3.000],  loss: 6.047989, mae: 33.544183, mean_q: 21.278148, mean_eps: 0.690115
  51790/150000: episode: 522, duration: 0.635s, episode steps:  75, steps per second: 118, episode reward: -70.875, mean reward: -0.945 [-100.000,  9.538], mean action: 1.480 [0.000, 3.000],  loss: 10.487337, mae: 34.143676, mean_q: 23.605816, mean_eps: 0.689488
  51885/150000: episode: 523, duration: 0.727s, episode steps:  95, steps per second: 131, episode reward: -50.634, mean reward: -0.533 [-100.000, 11.804], mean action: 1.589 [0.000, 3.000],  loss: 11.629569, mae: 33.751495, mean_q: 21.843150, mean_eps: 0.688978
  51966/150000: episode: 524, duration: 0.587s, episode steps:  81, steps per second: 138, episode reward: -55.340, mean reward: -0.683 [-100.000, 11.984], mean action: 1.457 [0.000, 3.000],  loss: 8.959932, mae: 33.373507, mean_q: 21.562470, mean_eps: 0.688450
  52055/150000: episode: 525, duration: 0.682s, episode steps:  89, steps per second: 130, episode reward: -71.137, mean reward: -0.799 [-100.000,  7.972], mean action: 1.697 [0.000, 3.000],  loss: 15.691698, mae: 34.381334, mean_q: 21.602762, mean_eps: 0.687940
  52147/150000: episode: 526, duration: 0.712s, episode steps:  92, steps per second: 129, episode reward: -54.455, mean reward: -0.592 [-100.000, 14.715], mean action: 1.587 [0.000, 3.000],  loss: 7.325001, mae: 35.081108, mean_q: 21.256180, mean_eps: 0.687397
  52229/150000: episode: 527, duration: 0.597s, episode steps:  82, steps per second: 137, episode reward: -121.443, mean reward: -1.481 [-100.000, 12.316], mean action: 1.561 [0.000, 3.000],  loss: 10.359837, mae: 34.464192, mean_q: 20.391978, mean_eps: 0.686875
  52298/150000: episode: 528, duration: 0.521s, episode steps:  69, steps per second: 133, episode reward: -50.216, mean reward: -0.728 [-100.000, 13.454], mean action: 1.406 [0.000, 3.000],  loss: 12.905527, mae: 34.772842, mean_q: 20.986030, mean_eps: 0.686422
  52381/150000: episode: 529, duration: 0.646s, episode steps:  83, steps per second: 129, episode reward: -81.465, mean reward: -0.982 [-100.000,  8.374], mean action: 1.361 [0.000, 3.000],  loss: 8.304990, mae: 34.339168, mean_q: 22.534674, mean_eps: 0.685966
  52481/150000: episode: 530, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: -94.957, mean reward: -0.950 [-100.000,  6.659], mean action: 1.470 [0.000, 3.000],  loss: 11.036327, mae: 34.795472, mean_q: 22.238840, mean_eps: 0.685417
  52595/150000: episode: 531, duration: 0.828s, episode steps: 114, steps per second: 138, episode reward: -114.879, mean reward: -1.008 [-100.000, 11.698], mean action: 1.439 [0.000, 3.000],  loss: 8.952078, mae: 34.669509, mean_q: 22.662267, mean_eps: 0.684775
  52662/150000: episode: 532, duration: 0.518s, episode steps:  67, steps per second: 129, episode reward: -69.089, mean reward: -1.031 [-100.000, 13.401], mean action: 1.537 [0.000, 3.000],  loss: 10.349341, mae: 34.370420, mean_q: 22.109670, mean_eps: 0.684232
  52766/150000: episode: 533, duration: 0.768s, episode steps: 104, steps per second: 135, episode reward: -60.310, mean reward: -0.580 [-100.000, 10.096], mean action: 1.606 [0.000, 3.000],  loss: 13.928605, mae: 34.297046, mean_q: 19.607138, mean_eps: 0.683719
  52862/150000: episode: 534, duration: 0.704s, episode steps:  96, steps per second: 136, episode reward: -73.564, mean reward: -0.766 [-100.000,  5.649], mean action: 1.542 [0.000, 3.000],  loss: 10.235449, mae: 34.799807, mean_q: 21.458818, mean_eps: 0.683119
  52989/150000: episode: 535, duration: 0.975s, episode steps: 127, steps per second: 130, episode reward: -5.708, mean reward: -0.045 [-100.000, 14.609], mean action: 1.512 [0.000, 3.000],  loss: 11.205402, mae: 34.566963, mean_q: 19.577070, mean_eps: 0.682450
  53061/150000: episode: 536, duration: 0.528s, episode steps:  72, steps per second: 136, episode reward: -104.034, mean reward: -1.445 [-100.000,  7.321], mean action: 1.514 [0.000, 3.000],  loss: 9.748672, mae: 34.831384, mean_q: 19.789929, mean_eps: 0.681853
  53172/150000: episode: 537, duration: 0.798s, episode steps: 111, steps per second: 139, episode reward: -38.311, mean reward: -0.345 [-100.000, 20.854], mean action: 1.550 [0.000, 3.000],  loss: 13.129013, mae: 34.745592, mean_q: 21.540804, mean_eps: 0.681304
  53279/150000: episode: 538, duration: 0.805s, episode steps: 107, steps per second: 133, episode reward: -56.330, mean reward: -0.526 [-100.000, 12.107], mean action: 1.383 [0.000, 3.000],  loss: 19.472802, mae: 34.237471, mean_q: 21.758191, mean_eps: 0.680650
  53372/150000: episode: 539, duration: 0.720s, episode steps:  93, steps per second: 129, episode reward: -26.245, mean reward: -0.282 [-100.000, 11.909], mean action: 1.602 [0.000, 3.000],  loss: 9.724828, mae: 34.501248, mean_q: 22.250571, mean_eps: 0.680050
  53465/150000: episode: 540, duration: 0.751s, episode steps:  93, steps per second: 124, episode reward: -76.349, mean reward: -0.821 [-100.000, 11.025], mean action: 1.484 [0.000, 3.000],  loss: 7.490915, mae: 34.718865, mean_q: 21.562937, mean_eps: 0.679492
  53525/150000: episode: 541, duration: 0.519s, episode steps:  60, steps per second: 116, episode reward: -103.185, mean reward: -1.720 [-100.000,  8.848], mean action: 1.717 [0.000, 3.000],  loss: 11.674175, mae: 34.437838, mean_q: 21.581559, mean_eps: 0.679033
  53647/150000: episode: 542, duration: 0.954s, episode steps: 122, steps per second: 128, episode reward: -129.038, mean reward: -1.058 [-100.000, 20.683], mean action: 1.459 [0.000, 3.000],  loss: 21.465372, mae: 34.571507, mean_q: 22.469909, mean_eps: 0.678487
  53803/150000: episode: 543, duration: 1.023s, episode steps: 156, steps per second: 153, episode reward: -47.997, mean reward: -0.308 [-100.000,  9.638], mean action: 1.500 [0.000, 3.000],  loss: 12.208660, mae: 33.972179, mean_q: 21.170388, mean_eps: 0.677653
  53917/150000: episode: 544, duration: 0.717s, episode steps: 114, steps per second: 159, episode reward: -26.634, mean reward: -0.234 [-100.000, 16.195], mean action: 1.728 [0.000, 3.000],  loss: 15.030925, mae: 34.508045, mean_q: 23.328577, mean_eps: 0.676843
  53994/150000: episode: 545, duration: 0.463s, episode steps:  77, steps per second: 166, episode reward: -72.440, mean reward: -0.941 [-100.000,  9.807], mean action: 1.532 [0.000, 3.000],  loss: 7.367373, mae: 34.331479, mean_q: 22.924535, mean_eps: 0.676270
  54102/150000: episode: 546, duration: 0.649s, episode steps: 108, steps per second: 166, episode reward: -166.683, mean reward: -1.543 [-100.000, 15.849], mean action: 1.426 [0.000, 3.000],  loss: 9.384621, mae: 34.364506, mean_q: 22.676168, mean_eps: 0.675715
  54205/150000: episode: 547, duration: 0.660s, episode steps: 103, steps per second: 156, episode reward: -60.711, mean reward: -0.589 [-100.000, 16.663], mean action: 1.408 [0.000, 3.000],  loss: 10.471481, mae: 34.425881, mean_q: 20.873036, mean_eps: 0.675082
  54295/150000: episode: 548, duration: 0.568s, episode steps:  90, steps per second: 158, episode reward: -48.482, mean reward: -0.539 [-100.000,  8.052], mean action: 1.433 [0.000, 3.000],  loss: 11.032684, mae: 34.040553, mean_q: 21.081421, mean_eps: 0.674503
  54421/150000: episode: 549, duration: 0.781s, episode steps: 126, steps per second: 161, episode reward: -12.080, mean reward: -0.096 [-100.000, 13.904], mean action: 1.563 [0.000, 3.000],  loss: 15.679842, mae: 34.577286, mean_q: 22.043667, mean_eps: 0.673855
  54488/150000: episode: 550, duration: 0.416s, episode steps:  67, steps per second: 161, episode reward: -35.024, mean reward: -0.523 [-100.000, 16.618], mean action: 1.687 [0.000, 3.000],  loss: 3.421253, mae: 34.738199, mean_q: 22.914589, mean_eps: 0.673276
  54589/150000: episode: 551, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: -64.724, mean reward: -0.641 [-100.000, 15.612], mean action: 1.624 [0.000, 3.000],  loss: 7.722454, mae: 34.597192, mean_q: 21.928085, mean_eps: 0.672772
  54683/150000: episode: 552, duration: 0.604s, episode steps:  94, steps per second: 156, episode reward: -68.341, mean reward: -0.727 [-100.000, 10.000], mean action: 1.543 [0.000, 3.000],  loss: 11.313315, mae: 34.272164, mean_q: 20.956232, mean_eps: 0.672187
  54777/150000: episode: 553, duration: 0.684s, episode steps:  94, steps per second: 138, episode reward: -105.213, mean reward: -1.119 [-100.000,  9.318], mean action: 1.702 [0.000, 3.000],  loss: 6.714873, mae: 34.203798, mean_q: 23.455685, mean_eps: 0.671623
  54850/150000: episode: 554, duration: 0.522s, episode steps:  73, steps per second: 140, episode reward: -82.625, mean reward: -1.132 [-100.000,  7.980], mean action: 1.164 [0.000, 3.000],  loss: 10.184980, mae: 34.811312, mean_q: 22.628436, mean_eps: 0.671122
  54941/150000: episode: 555, duration: 0.629s, episode steps:  91, steps per second: 145, episode reward: -80.119, mean reward: -0.880 [-100.000, 16.900], mean action: 1.582 [0.000, 3.000],  loss: 16.214979, mae: 34.129149, mean_q: 21.900854, mean_eps: 0.670630
  55051/150000: episode: 556, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -65.734, mean reward: -0.598 [-100.000, 14.418], mean action: 1.318 [0.000, 3.000],  loss: 18.240574, mae: 34.336994, mean_q: 21.589581, mean_eps: 0.670027
  55150/150000: episode: 557, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -12.791, mean reward: -0.129 [-100.000, 15.517], mean action: 1.455 [0.000, 3.000],  loss: 14.410117, mae: 34.394234, mean_q: 21.423135, mean_eps: 0.669400
  55267/150000: episode: 558, duration: 0.769s, episode steps: 117, steps per second: 152, episode reward: -153.084, mean reward: -1.308 [-100.000, 15.250], mean action: 1.427 [0.000, 3.000],  loss: 12.794315, mae: 35.150071, mean_q: 23.734280, mean_eps: 0.668752
  55373/150000: episode: 559, duration: 0.700s, episode steps: 106, steps per second: 152, episode reward: -38.146, mean reward: -0.360 [-100.000, 17.586], mean action: 1.726 [0.000, 3.000],  loss: 11.625623, mae: 34.734405, mean_q: 22.714766, mean_eps: 0.668083
  55457/150000: episode: 560, duration: 0.527s, episode steps:  84, steps per second: 159, episode reward: -79.139, mean reward: -0.942 [-100.000,  7.213], mean action: 1.560 [0.000, 3.000],  loss: 10.456155, mae: 34.888303, mean_q: 21.780434, mean_eps: 0.667513
  55584/150000: episode: 561, duration: 0.783s, episode steps: 127, steps per second: 162, episode reward: -103.192, mean reward: -0.813 [-100.000, 11.199], mean action: 1.425 [0.000, 3.000],  loss: 14.676791, mae: 34.002941, mean_q: 21.397158, mean_eps: 0.666880
  55660/150000: episode: 562, duration: 0.453s, episode steps:  76, steps per second: 168, episode reward: -50.478, mean reward: -0.664 [-100.000, 11.692], mean action: 1.395 [0.000, 3.000],  loss: 13.534257, mae: 34.470311, mean_q: 22.421181, mean_eps: 0.666271
  55741/150000: episode: 563, duration: 0.478s, episode steps:  81, steps per second: 169, episode reward: -69.802, mean reward: -0.862 [-100.000,  7.199], mean action: 1.494 [0.000, 3.000],  loss: 17.218911, mae: 34.987567, mean_q: 22.348550, mean_eps: 0.665800
  55869/150000: episode: 564, duration: 0.825s, episode steps: 128, steps per second: 155, episode reward: -37.421, mean reward: -0.292 [-100.000, 16.112], mean action: 1.492 [0.000, 3.000],  loss: 13.942735, mae: 34.285295, mean_q: 22.866771, mean_eps: 0.665173
  55964/150000: episode: 565, duration: 0.593s, episode steps:  95, steps per second: 160, episode reward: -89.106, mean reward: -0.938 [-100.000, 12.762], mean action: 1.568 [0.000, 3.000],  loss: 7.703927, mae: 34.855132, mean_q: 22.019628, mean_eps: 0.664504
  56057/150000: episode: 566, duration: 0.566s, episode steps:  93, steps per second: 164, episode reward: -52.423, mean reward: -0.564 [-100.000, 11.582], mean action: 1.484 [0.000, 3.000],  loss: 8.925284, mae: 35.191673, mean_q: 23.802059, mean_eps: 0.663940
  56147/150000: episode: 567, duration: 0.560s, episode steps:  90, steps per second: 161, episode reward: -44.001, mean reward: -0.489 [-100.000, 15.224], mean action: 1.811 [0.000, 3.000],  loss: 12.513720, mae: 34.848045, mean_q: 22.889742, mean_eps: 0.663391
  56218/150000: episode: 568, duration: 0.502s, episode steps:  71, steps per second: 141, episode reward: -88.928, mean reward: -1.253 [-100.000, 19.748], mean action: 1.620 [0.000, 3.000],  loss: 9.464082, mae: 35.492992, mean_q: 21.997527, mean_eps: 0.662908
  56332/150000: episode: 569, duration: 0.716s, episode steps: 114, steps per second: 159, episode reward: -127.860, mean reward: -1.122 [-100.000, 12.383], mean action: 1.561 [0.000, 3.000],  loss: 12.149710, mae: 34.764795, mean_q: 22.456390, mean_eps: 0.662353
  56422/150000: episode: 570, duration: 0.540s, episode steps:  90, steps per second: 167, episode reward: -22.029, mean reward: -0.245 [-100.000,  8.293], mean action: 1.478 [0.000, 3.000],  loss: 6.615274, mae: 34.282096, mean_q: 21.477586, mean_eps: 0.661741
  56547/150000: episode: 571, duration: 0.778s, episode steps: 125, steps per second: 161, episode reward: -55.478, mean reward: -0.444 [-100.000, 22.049], mean action: 1.336 [0.000, 3.000],  loss: 8.425246, mae: 34.702817, mean_q: 22.728795, mean_eps: 0.661096
  56620/150000: episode: 572, duration: 0.447s, episode steps:  73, steps per second: 163, episode reward: -32.838, mean reward: -0.450 [-100.000, 16.627], mean action: 1.589 [0.000, 3.000],  loss: 13.514365, mae: 34.586626, mean_q: 22.761184, mean_eps: 0.660502
  56707/150000: episode: 573, duration: 0.518s, episode steps:  87, steps per second: 168, episode reward: -71.044, mean reward: -0.817 [-100.000, 17.177], mean action: 1.770 [0.000, 3.000],  loss: 15.924451, mae: 35.129510, mean_q: 22.266058, mean_eps: 0.660022
  56809/150000: episode: 574, duration: 0.604s, episode steps: 102, steps per second: 169, episode reward: -86.481, mean reward: -0.848 [-100.000,  7.292], mean action: 1.667 [0.000, 3.000],  loss: 25.015638, mae: 35.074686, mean_q: 22.507584, mean_eps: 0.659455
  56878/150000: episode: 575, duration: 0.431s, episode steps:  69, steps per second: 160, episode reward: -67.663, mean reward: -0.981 [-100.000, 10.371], mean action: 1.493 [0.000, 3.000],  loss: 7.494679, mae: 34.942287, mean_q: 22.988658, mean_eps: 0.658942
  56982/150000: episode: 576, duration: 0.645s, episode steps: 104, steps per second: 161, episode reward: -47.533, mean reward: -0.457 [-100.000, 15.314], mean action: 1.567 [0.000, 3.000],  loss: 6.969177, mae: 34.629785, mean_q: 22.813533, mean_eps: 0.658423
  57084/150000: episode: 577, duration: 0.611s, episode steps: 102, steps per second: 167, episode reward: -55.411, mean reward: -0.543 [-100.000, 10.002], mean action: 1.647 [0.000, 3.000],  loss: 9.373093, mae: 34.849347, mean_q: 22.652381, mean_eps: 0.657805
  57170/150000: episode: 578, duration: 0.505s, episode steps:  86, steps per second: 170, episode reward: -130.081, mean reward: -1.513 [-100.000,  6.968], mean action: 1.326 [0.000, 3.000],  loss: 10.138025, mae: 34.602696, mean_q: 20.459704, mean_eps: 0.657241
  57258/150000: episode: 579, duration: 0.563s, episode steps:  88, steps per second: 156, episode reward: -37.186, mean reward: -0.423 [-100.000, 11.023], mean action: 1.670 [0.000, 3.000],  loss: 10.967870, mae: 34.337601, mean_q: 21.464783, mean_eps: 0.656719
  57335/150000: episode: 580, duration: 0.483s, episode steps:  77, steps per second: 159, episode reward: -69.845, mean reward: -0.907 [-100.000,  6.873], mean action: 1.714 [0.000, 3.000],  loss: 14.334683, mae: 34.925000, mean_q: 23.384728, mean_eps: 0.656224
  57457/150000: episode: 581, duration: 0.729s, episode steps: 122, steps per second: 167, episode reward: -78.259, mean reward: -0.641 [-100.000, 12.166], mean action: 1.705 [0.000, 3.000],  loss: 11.648378, mae: 35.278034, mean_q: 21.552683, mean_eps: 0.655627
  57632/150000: episode: 582, duration: 1.079s, episode steps: 175, steps per second: 162, episode reward: -288.310, mean reward: -1.647 [-100.000, 96.620], mean action: 1.600 [0.000, 3.000],  loss: 10.138302, mae: 34.911609, mean_q: 22.388143, mean_eps: 0.654736
  57710/150000: episode: 583, duration: 0.488s, episode steps:  78, steps per second: 160, episode reward: -71.860, mean reward: -0.921 [-100.000,  5.288], mean action: 1.731 [0.000, 3.000],  loss: 6.821444, mae: 35.137629, mean_q: 23.103821, mean_eps: 0.653977
  57801/150000: episode: 584, duration: 0.546s, episode steps:  91, steps per second: 167, episode reward: -69.564, mean reward: -0.764 [-100.000,  8.773], mean action: 1.538 [0.000, 3.000],  loss: 8.313726, mae: 35.009548, mean_q: 22.963488, mean_eps: 0.653470
  57890/150000: episode: 585, duration: 0.532s, episode steps:  89, steps per second: 167, episode reward: -72.777, mean reward: -0.818 [-100.000, 10.967], mean action: 1.416 [0.000, 3.000],  loss: 8.556456, mae: 34.645046, mean_q: 21.355413, mean_eps: 0.652930
  57964/150000: episode: 586, duration: 0.470s, episode steps:  74, steps per second: 157, episode reward: -58.745, mean reward: -0.794 [-100.000,  7.268], mean action: 1.486 [0.000, 3.000],  loss: 7.307508, mae: 34.709944, mean_q: 21.804104, mean_eps: 0.652441
  58044/150000: episode: 587, duration: 0.493s, episode steps:  80, steps per second: 162, episode reward: -105.507, mean reward: -1.319 [-100.000,  6.272], mean action: 1.625 [0.000, 3.000],  loss: 9.295760, mae: 35.212911, mean_q: 22.806183, mean_eps: 0.651979
  58190/150000: episode: 588, duration: 0.885s, episode steps: 146, steps per second: 165, episode reward: 33.617, mean reward:  0.230 [-100.000, 86.132], mean action: 1.623 [0.000, 3.000],  loss: 7.514838, mae: 35.342247, mean_q: 23.385826, mean_eps: 0.651301
  58259/150000: episode: 589, duration: 0.407s, episode steps:  69, steps per second: 169, episode reward: -19.784, mean reward: -0.287 [-100.000, 13.263], mean action: 1.725 [0.000, 3.000],  loss: 12.118769, mae: 34.452874, mean_q: 23.008879, mean_eps: 0.650656
  58373/150000: episode: 590, duration: 0.711s, episode steps: 114, steps per second: 160, episode reward: -89.605, mean reward: -0.786 [-100.000,  9.881], mean action: 1.588 [0.000, 3.000],  loss: 11.284736, mae: 34.898351, mean_q: 23.335658, mean_eps: 0.650107
  58467/150000: episode: 591, duration: 0.561s, episode steps:  94, steps per second: 168, episode reward: -66.712, mean reward: -0.710 [-100.000, 19.385], mean action: 1.628 [0.000, 3.000],  loss: 11.326327, mae: 35.192377, mean_q: 22.878938, mean_eps: 0.649483
  58542/150000: episode: 592, duration: 0.447s, episode steps:  75, steps per second: 168, episode reward: -81.990, mean reward: -1.093 [-100.000, 11.632], mean action: 1.480 [0.000, 3.000],  loss: 6.871447, mae: 34.956875, mean_q: 24.605852, mean_eps: 0.648976
  58652/150000: episode: 593, duration: 0.670s, episode steps: 110, steps per second: 164, episode reward: -118.981, mean reward: -1.082 [-100.000,  8.559], mean action: 1.609 [0.000, 3.000],  loss: 13.557320, mae: 35.035836, mean_q: 20.938568, mean_eps: 0.648421
  58740/150000: episode: 594, duration: 0.546s, episode steps:  88, steps per second: 161, episode reward: -28.011, mean reward: -0.318 [-100.000, 12.022], mean action: 1.864 [0.000, 3.000],  loss: 12.130186, mae: 35.667732, mean_q: 24.227616, mean_eps: 0.647827
  58836/150000: episode: 595, duration: 0.585s, episode steps:  96, steps per second: 164, episode reward: -113.436, mean reward: -1.182 [-100.000, 10.282], mean action: 1.740 [0.000, 3.000],  loss: 17.859886, mae: 35.061669, mean_q: 22.994019, mean_eps: 0.647275
  58959/150000: episode: 596, duration: 0.787s, episode steps: 123, steps per second: 156, episode reward: -109.820, mean reward: -0.893 [-100.000,  5.669], mean action: 1.553 [0.000, 3.000],  loss: 10.224248, mae: 35.041758, mean_q: 22.436504, mean_eps: 0.646618
  59065/150000: episode: 597, duration: 0.683s, episode steps: 106, steps per second: 155, episode reward: -70.024, mean reward: -0.661 [-100.000, 16.051], mean action: 1.519 [0.000, 3.000],  loss: 10.603040, mae: 35.377081, mean_q: 26.396159, mean_eps: 0.645931
  59181/150000: episode: 598, duration: 0.687s, episode steps: 116, steps per second: 169, episode reward: -113.075, mean reward: -0.975 [-100.000, 12.407], mean action: 1.595 [0.000, 3.000],  loss: 10.846314, mae: 34.664125, mean_q: 23.979190, mean_eps: 0.645265
  59291/150000: episode: 599, duration: 0.656s, episode steps: 110, steps per second: 168, episode reward: -129.770, mean reward: -1.180 [-100.000, 13.119], mean action: 1.691 [0.000, 3.000],  loss: 8.701019, mae: 34.549929, mean_q: 22.804598, mean_eps: 0.644587
  59452/150000: episode: 600, duration: 0.991s, episode steps: 161, steps per second: 162, episode reward: -213.227, mean reward: -1.324 [-100.000, 61.830], mean action: 1.553 [0.000, 3.000],  loss: 13.287337, mae: 35.175150, mean_q: 24.072194, mean_eps: 0.643774
  59531/150000: episode: 601, duration: 0.465s, episode steps:  79, steps per second: 170, episode reward: -56.762, mean reward: -0.719 [-100.000, 14.691], mean action: 1.658 [0.000, 3.000],  loss: 6.414778, mae: 33.839479, mean_q: 23.549969, mean_eps: 0.643054
  59643/150000: episode: 602, duration: 0.663s, episode steps: 112, steps per second: 169, episode reward: -74.033, mean reward: -0.661 [-100.000,  7.563], mean action: 1.723 [0.000, 3.000],  loss: 8.917641, mae: 34.489834, mean_q: 24.236279, mean_eps: 0.642481
  59763/150000: episode: 603, duration: 0.734s, episode steps: 120, steps per second: 164, episode reward: -207.268, mean reward: -1.727 [-100.000, 11.135], mean action: 1.533 [0.000, 3.000],  loss: 13.535764, mae: 34.850259, mean_q: 23.270005, mean_eps: 0.641785
  59901/150000: episode: 604, duration: 0.816s, episode steps: 138, steps per second: 169, episode reward: -30.198, mean reward: -0.219 [-100.000, 22.048], mean action: 1.558 [0.000, 3.000],  loss: 11.766906, mae: 34.776578, mean_q: 24.565962, mean_eps: 0.641011
  60004/150000: episode: 605, duration: 0.610s, episode steps: 103, steps per second: 169, episode reward: -47.975, mean reward: -0.466 [-100.000, 12.307], mean action: 1.524 [0.000, 3.000],  loss: 17.424475, mae: 34.093249, mean_q: 22.529728, mean_eps: 0.640288
  60143/150000: episode: 606, duration: 0.881s, episode steps: 139, steps per second: 158, episode reward: -89.305, mean reward: -0.642 [-100.000,  7.866], mean action: 1.619 [0.000, 3.000],  loss: 9.163703, mae: 34.822923, mean_q: 22.923215, mean_eps: 0.639562
  60233/150000: episode: 607, duration: 0.535s, episode steps:  90, steps per second: 168, episode reward: -22.228, mean reward: -0.247 [-100.000, 16.299], mean action: 1.467 [0.000, 3.000],  loss: 12.914460, mae: 35.278120, mean_q: 24.525869, mean_eps: 0.638875
  60332/150000: episode: 608, duration: 0.619s, episode steps:  99, steps per second: 160, episode reward: -88.492, mean reward: -0.894 [-100.000, 13.198], mean action: 1.556 [0.000, 3.000],  loss: 14.262005, mae: 34.653031, mean_q: 23.207604, mean_eps: 0.638308
  60421/150000: episode: 609, duration: 0.678s, episode steps:  89, steps per second: 131, episode reward: -78.870, mean reward: -0.886 [-100.000,  6.344], mean action: 1.517 [0.000, 3.000],  loss: 14.371389, mae: 34.207076, mean_q: 23.313433, mean_eps: 0.637744
  60502/150000: episode: 610, duration: 0.521s, episode steps:  81, steps per second: 155, episode reward: -66.383, mean reward: -0.820 [-100.000, 12.386], mean action: 1.481 [0.000, 3.000],  loss: 14.643782, mae: 35.218644, mean_q: 24.421274, mean_eps: 0.637234
  60592/150000: episode: 611, duration: 0.539s, episode steps:  90, steps per second: 167, episode reward: -106.921, mean reward: -1.188 [-100.000,  6.910], mean action: 1.656 [0.000, 3.000],  loss: 13.814046, mae: 35.196950, mean_q: 23.169838, mean_eps: 0.636721
  60666/150000: episode: 612, duration: 0.443s, episode steps:  74, steps per second: 167, episode reward: -53.419, mean reward: -0.722 [-100.000,  6.493], mean action: 1.595 [0.000, 3.000],  loss: 20.073683, mae: 34.716011, mean_q: 23.709354, mean_eps: 0.636229
  61043/150000: episode: 613, duration: 2.375s, episode steps: 377, steps per second: 159, episode reward: -45.526, mean reward: -0.121 [-100.000, 56.884], mean action: 1.642 [0.000, 3.000],  loss: 8.817539, mae: 34.978317, mean_q: 24.861786, mean_eps: 0.634876
  61108/150000: episode: 614, duration: 0.420s, episode steps:  65, steps per second: 155, episode reward: -81.400, mean reward: -1.252 [-100.000,  9.194], mean action: 1.615 [0.000, 3.000],  loss: 9.672559, mae: 34.247267, mean_q: 23.740866, mean_eps: 0.633550
  61240/150000: episode: 615, duration: 0.927s, episode steps: 132, steps per second: 142, episode reward: -115.034, mean reward: -0.871 [-100.000, 18.627], mean action: 1.727 [0.000, 3.000],  loss: 10.005150, mae: 35.229651, mean_q: 25.723897, mean_eps: 0.632959
  61394/150000: episode: 616, duration: 1.001s, episode steps: 154, steps per second: 154, episode reward: -30.705, mean reward: -0.199 [-100.000, 13.928], mean action: 1.649 [0.000, 3.000],  loss: 8.372537, mae: 34.189919, mean_q: 23.030384, mean_eps: 0.632101
  61543/150000: episode: 617, duration: 0.980s, episode steps: 149, steps per second: 152, episode reward: -90.551, mean reward: -0.608 [-100.000, 17.565], mean action: 1.591 [0.000, 3.000],  loss: 12.236758, mae: 34.534673, mean_q: 25.456171, mean_eps: 0.631192
  61621/150000: episode: 618, duration: 0.511s, episode steps:  78, steps per second: 153, episode reward: -60.139, mean reward: -0.771 [-100.000,  8.108], mean action: 1.628 [0.000, 3.000],  loss: 8.398836, mae: 34.462406, mean_q: 22.586982, mean_eps: 0.630511
  61737/150000: episode: 619, duration: 0.736s, episode steps: 116, steps per second: 158, episode reward: -108.695, mean reward: -0.937 [-100.000, 18.871], mean action: 1.690 [0.000, 3.000],  loss: 11.204153, mae: 34.845820, mean_q: 23.982280, mean_eps: 0.629929
  61805/150000: episode: 620, duration: 0.460s, episode steps:  68, steps per second: 148, episode reward: -47.171, mean reward: -0.694 [-100.000, 12.667], mean action: 1.721 [0.000, 3.000],  loss: 10.615010, mae: 33.889892, mean_q: 24.560071, mean_eps: 0.629377
  61936/150000: episode: 621, duration: 0.812s, episode steps: 131, steps per second: 161, episode reward: -86.167, mean reward: -0.658 [-100.000, 14.474], mean action: 1.641 [0.000, 3.000],  loss: 7.720613, mae: 34.429209, mean_q: 23.973616, mean_eps: 0.628780
  62086/150000: episode: 622, duration: 0.919s, episode steps: 150, steps per second: 163, episode reward: -260.079, mean reward: -1.734 [-100.000, 44.777], mean action: 1.607 [0.000, 3.000],  loss: 9.772221, mae: 34.164132, mean_q: 24.085752, mean_eps: 0.627937
  62184/150000: episode: 623, duration: 0.609s, episode steps:  98, steps per second: 161, episode reward: -83.214, mean reward: -0.849 [-100.000, 11.218], mean action: 1.469 [0.000, 3.000],  loss: 5.753889, mae: 34.213872, mean_q: 24.594090, mean_eps: 0.627193
  62315/150000: episode: 624, duration: 0.785s, episode steps: 131, steps per second: 167, episode reward: -100.273, mean reward: -0.765 [-100.000, 11.919], mean action: 1.458 [0.000, 3.000],  loss: 7.447219, mae: 34.529221, mean_q: 24.336601, mean_eps: 0.626506
  62442/150000: episode: 625, duration: 0.765s, episode steps: 127, steps per second: 166, episode reward: -96.933, mean reward: -0.763 [-100.000,  6.116], mean action: 1.748 [0.000, 3.000],  loss: 13.111576, mae: 33.703357, mean_q: 24.837553, mean_eps: 0.625732
  62534/150000: episode: 626, duration: 0.568s, episode steps:  92, steps per second: 162, episode reward: -64.228, mean reward: -0.698 [-100.000, 12.450], mean action: 1.728 [0.000, 3.000],  loss: 10.816373, mae: 34.306942, mean_q: 24.689151, mean_eps: 0.625075
  62611/150000: episode: 627, duration: 0.477s, episode steps:  77, steps per second: 161, episode reward: -54.493, mean reward: -0.708 [-100.000,  7.561], mean action: 1.597 [0.000, 3.000],  loss: 5.992488, mae: 34.546079, mean_q: 24.712564, mean_eps: 0.624568
  62712/150000: episode: 628, duration: 0.594s, episode steps: 101, steps per second: 170, episode reward: -70.124, mean reward: -0.694 [-100.000, 14.238], mean action: 1.584 [0.000, 3.000],  loss: 11.875431, mae: 34.011342, mean_q: 24.134327, mean_eps: 0.624034
  62810/150000: episode: 629, duration: 0.603s, episode steps:  98, steps per second: 163, episode reward: -43.558, mean reward: -0.444 [-100.000, 10.916], mean action: 1.796 [0.000, 3.000],  loss: 12.606424, mae: 34.564483, mean_q: 24.566435, mean_eps: 0.623437
  62907/150000: episode: 630, duration: 0.616s, episode steps:  97, steps per second: 157, episode reward: -71.481, mean reward: -0.737 [-100.000, 22.837], mean action: 1.505 [0.000, 3.000],  loss: 13.305701, mae: 33.817995, mean_q: 23.299535, mean_eps: 0.622852
  63019/150000: episode: 631, duration: 0.680s, episode steps: 112, steps per second: 165, episode reward: -70.079, mean reward: -0.626 [-100.000, 10.087], mean action: 1.473 [0.000, 3.000],  loss: 11.849660, mae: 34.142178, mean_q: 24.363901, mean_eps: 0.622225
  63165/150000: episode: 632, duration: 0.881s, episode steps: 146, steps per second: 166, episode reward:  2.776, mean reward:  0.019 [-100.000, 17.561], mean action: 1.589 [0.000, 3.000],  loss: 9.725567, mae: 34.249602, mean_q: 23.187352, mean_eps: 0.621451
  63262/150000: episode: 633, duration: 0.588s, episode steps:  97, steps per second: 165, episode reward: -64.448, mean reward: -0.664 [-100.000, 12.178], mean action: 1.526 [0.000, 3.000],  loss: 7.015793, mae: 34.127132, mean_q: 23.401948, mean_eps: 0.620722
  63352/150000: episode: 634, duration: 0.563s, episode steps:  90, steps per second: 160, episode reward: 16.949, mean reward:  0.188 [-100.000, 14.383], mean action: 1.722 [0.000, 3.000],  loss: 6.387230, mae: 35.003383, mean_q: 24.921684, mean_eps: 0.620161
  63427/150000: episode: 635, duration: 0.463s, episode steps:  75, steps per second: 162, episode reward: -54.049, mean reward: -0.721 [-100.000, 10.479], mean action: 1.507 [0.000, 3.000],  loss: 10.029671, mae: 35.015457, mean_q: 27.059218, mean_eps: 0.619666
  63512/150000: episode: 636, duration: 0.520s, episode steps:  85, steps per second: 164, episode reward: -168.417, mean reward: -1.981 [-100.000,  4.939], mean action: 1.800 [0.000, 3.000],  loss: 10.823418, mae: 34.738159, mean_q: 25.202602, mean_eps: 0.619186
  63646/150000: episode: 637, duration: 0.808s, episode steps: 134, steps per second: 166, episode reward: -87.402, mean reward: -0.652 [-100.000, 41.036], mean action: 1.545 [0.000, 3.000],  loss: 7.195316, mae: 34.676605, mean_q: 24.957312, mean_eps: 0.618529
  63730/150000: episode: 638, duration: 0.508s, episode steps:  84, steps per second: 166, episode reward: -30.692, mean reward: -0.365 [-100.000, 12.577], mean action: 1.333 [0.000, 3.000],  loss: 12.722603, mae: 34.549132, mean_q: 24.213226, mean_eps: 0.617875
  63834/150000: episode: 639, duration: 0.615s, episode steps: 104, steps per second: 169, episode reward: -49.952, mean reward: -0.480 [-100.000,  9.535], mean action: 1.769 [0.000, 3.000],  loss: 9.856413, mae: 34.626966, mean_q: 24.398989, mean_eps: 0.617311
  63957/150000: episode: 640, duration: 0.790s, episode steps: 123, steps per second: 156, episode reward: -86.877, mean reward: -0.706 [-100.000,  7.044], mean action: 1.504 [0.000, 3.000],  loss: 7.382262, mae: 34.325090, mean_q: 25.015571, mean_eps: 0.616630
  64057/150000: episode: 641, duration: 0.619s, episode steps: 100, steps per second: 162, episode reward: -59.673, mean reward: -0.597 [-100.000,  5.974], mean action: 1.620 [0.000, 3.000],  loss: 8.458719, mae: 34.263042, mean_q: 25.574592, mean_eps: 0.615961
  64137/150000: episode: 642, duration: 0.493s, episode steps:  80, steps per second: 162, episode reward: -68.539, mean reward: -0.857 [-100.000,  7.745], mean action: 1.425 [0.000, 3.000],  loss: 13.803887, mae: 34.517287, mean_q: 25.310258, mean_eps: 0.615421
  64248/150000: episode: 643, duration: 0.752s, episode steps: 111, steps per second: 148, episode reward: -49.252, mean reward: -0.444 [-100.000, 21.551], mean action: 1.622 [0.000, 3.000],  loss: 10.207415, mae: 35.131149, mean_q: 26.359251, mean_eps: 0.614848
  64344/150000: episode: 644, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: -83.152, mean reward: -0.866 [-100.000,  8.065], mean action: 1.469 [0.000, 3.000],  loss: 6.925763, mae: 34.846708, mean_q: 25.550565, mean_eps: 0.614227
  64433/150000: episode: 645, duration: 0.609s, episode steps:  89, steps per second: 146, episode reward: -53.631, mean reward: -0.603 [-100.000,  9.510], mean action: 1.708 [0.000, 3.000],  loss: 4.876468, mae: 34.799713, mean_q: 25.576701, mean_eps: 0.613672
  64537/150000: episode: 646, duration: 0.699s, episode steps: 104, steps per second: 149, episode reward: -71.741, mean reward: -0.690 [-100.000, 22.361], mean action: 1.769 [0.000, 3.000],  loss: 4.810384, mae: 34.751698, mean_q: 25.783113, mean_eps: 0.613093
  64705/150000: episode: 647, duration: 1.025s, episode steps: 168, steps per second: 164, episode reward: -57.824, mean reward: -0.344 [-100.000, 11.442], mean action: 1.607 [0.000, 3.000],  loss: 8.255073, mae: 34.689769, mean_q: 25.084155, mean_eps: 0.612277
  64821/150000: episode: 648, duration: 0.705s, episode steps: 116, steps per second: 164, episode reward: -53.364, mean reward: -0.460 [-100.000, 18.143], mean action: 1.526 [0.000, 3.000],  loss: 8.261465, mae: 35.363355, mean_q: 25.371371, mean_eps: 0.611425
  64914/150000: episode: 649, duration: 0.607s, episode steps:  93, steps per second: 153, episode reward: -86.091, mean reward: -0.926 [-100.000,  8.774], mean action: 1.495 [0.000, 3.000],  loss: 7.991823, mae: 34.594229, mean_q: 24.717779, mean_eps: 0.610798
  65031/150000: episode: 650, duration: 0.698s, episode steps: 117, steps per second: 168, episode reward: -103.436, mean reward: -0.884 [-100.000, 18.959], mean action: 1.667 [0.000, 3.000],  loss: 6.266060, mae: 34.984836, mean_q: 23.979598, mean_eps: 0.610168
  65196/150000: episode: 651, duration: 1.056s, episode steps: 165, steps per second: 156, episode reward: -59.753, mean reward: -0.362 [-100.000, 18.051], mean action: 1.667 [0.000, 3.000],  loss: 15.865522, mae: 35.291800, mean_q: 25.123412, mean_eps: 0.609322
  65285/150000: episode: 652, duration: 0.690s, episode steps:  89, steps per second: 129, episode reward: -67.914, mean reward: -0.763 [-100.000, 12.074], mean action: 1.719 [0.000, 3.000],  loss: 22.273293, mae: 34.634257, mean_q: 25.096385, mean_eps: 0.608560
  65371/150000: episode: 653, duration: 0.589s, episode steps:  86, steps per second: 146, episode reward: -36.542, mean reward: -0.425 [-100.000, 16.612], mean action: 1.605 [0.000, 3.000],  loss: 5.280711, mae: 35.122757, mean_q: 24.009652, mean_eps: 0.608035
  65508/150000: episode: 654, duration: 0.948s, episode steps: 137, steps per second: 145, episode reward: -22.177, mean reward: -0.162 [-100.000, 25.262], mean action: 1.723 [0.000, 3.000],  loss: 10.148470, mae: 35.416248, mean_q: 26.671606, mean_eps: 0.607366
  65685/150000: episode: 655, duration: 1.214s, episode steps: 177, steps per second: 146, episode reward: -213.541, mean reward: -1.206 [-100.000, 52.329], mean action: 1.525 [0.000, 3.000],  loss: 7.968918, mae: 34.596034, mean_q: 24.382025, mean_eps: 0.606424
  65769/150000: episode: 656, duration: 0.514s, episode steps:  84, steps per second: 163, episode reward: -48.013, mean reward: -0.572 [-100.000, 11.923], mean action: 1.548 [0.000, 3.000],  loss: 6.475617, mae: 34.859150, mean_q: 24.213392, mean_eps: 0.605641
  65854/150000: episode: 657, duration: 0.614s, episode steps:  85, steps per second: 138, episode reward: -42.497, mean reward: -0.500 [-100.000,  8.261], mean action: 1.624 [0.000, 3.000],  loss: 5.934523, mae: 35.016527, mean_q: 24.604165, mean_eps: 0.605134
  66854/150000: episode: 658, duration: 6.874s, episode steps: 1000, steps per second: 145, episode reward: 33.927, mean reward:  0.034 [-23.709, 24.976], mean action: 1.531 [0.000, 3.000],  loss: 9.447798, mae: 34.722531, mean_q: 24.493798, mean_eps: 0.601879
  66965/150000: episode: 659, duration: 0.670s, episode steps: 111, steps per second: 166, episode reward:  3.960, mean reward:  0.036 [-100.000, 54.076], mean action: 1.748 [0.000, 3.000],  loss: 8.874591, mae: 34.287715, mean_q: 24.055441, mean_eps: 0.598546
  67078/150000: episode: 660, duration: 0.672s, episode steps: 113, steps per second: 168, episode reward: -69.834, mean reward: -0.618 [-100.000, 12.254], mean action: 1.504 [0.000, 3.000],  loss: 8.902870, mae: 33.901487, mean_q: 24.298595, mean_eps: 0.597874
  67190/150000: episode: 661, duration: 0.707s, episode steps: 112, steps per second: 158, episode reward: -34.227, mean reward: -0.306 [-100.000, 16.111], mean action: 1.518 [0.000, 3.000],  loss: 11.827491, mae: 34.289746, mean_q: 24.175566, mean_eps: 0.597199
  67281/150000: episode: 662, duration: 0.561s, episode steps:  91, steps per second: 162, episode reward: -97.225, mean reward: -1.068 [-100.000,  7.835], mean action: 1.681 [0.000, 3.000],  loss: 4.416098, mae: 34.405291, mean_q: 24.081333, mean_eps: 0.596590
  67374/150000: episode: 663, duration: 0.553s, episode steps:  93, steps per second: 168, episode reward: -61.807, mean reward: -0.665 [-100.000, 19.873], mean action: 1.419 [0.000, 3.000],  loss: 10.486804, mae: 34.151766, mean_q: 23.030103, mean_eps: 0.596038
  67478/150000: episode: 664, duration: 0.621s, episode steps: 104, steps per second: 167, episode reward: -107.178, mean reward: -1.031 [-100.000,  7.892], mean action: 1.577 [0.000, 3.000],  loss: 9.935423, mae: 33.595889, mean_q: 23.117031, mean_eps: 0.595447
  67619/150000: episode: 665, duration: 0.870s, episode steps: 141, steps per second: 162, episode reward: -195.270, mean reward: -1.385 [-100.000,  4.076], mean action: 1.801 [0.000, 3.000],  loss: 6.688171, mae: 34.466336, mean_q: 23.688164, mean_eps: 0.594712
  67760/150000: episode: 666, duration: 0.846s, episode steps: 141, steps per second: 167, episode reward: -243.973, mean reward: -1.730 [-100.000, 13.414], mean action: 1.780 [0.000, 3.000],  loss: 8.498413, mae: 34.463663, mean_q: 24.408515, mean_eps: 0.593866
  67876/150000: episode: 667, duration: 0.745s, episode steps: 116, steps per second: 156, episode reward: -81.271, mean reward: -0.701 [-100.000, 10.044], mean action: 1.552 [0.000, 3.000],  loss: 8.058553, mae: 34.250912, mean_q: 24.168567, mean_eps: 0.593095
  67941/150000: episode: 668, duration: 0.387s, episode steps:  65, steps per second: 168, episode reward: -58.724, mean reward: -0.903 [-100.000, 24.615], mean action: 1.538 [0.000, 3.000],  loss: 7.072323, mae: 33.352804, mean_q: 21.578752, mean_eps: 0.592552
  68031/150000: episode: 669, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -139.429, mean reward: -1.549 [-100.000, 39.580], mean action: 1.544 [0.000, 3.000],  loss: 6.094627, mae: 34.356236, mean_q: 24.580379, mean_eps: 0.592087
  68120/150000: episode: 670, duration: 0.582s, episode steps:  89, steps per second: 153, episode reward: 28.080, mean reward:  0.316 [-100.000, 16.765], mean action: 1.764 [0.000, 3.000],  loss: 8.666303, mae: 34.255744, mean_q: 24.386765, mean_eps: 0.591550
  68256/150000: episode: 671, duration: 0.865s, episode steps: 136, steps per second: 157, episode reward: -81.755, mean reward: -0.601 [-100.000, 14.018], mean action: 1.500 [0.000, 3.000],  loss: 8.500506, mae: 34.169767, mean_q: 23.760523, mean_eps: 0.590875
  68356/150000: episode: 672, duration: 0.587s, episode steps: 100, steps per second: 170, episode reward:  3.827, mean reward:  0.038 [-100.000, 16.946], mean action: 1.700 [0.000, 3.000],  loss: 7.258060, mae: 34.494395, mean_q: 22.145057, mean_eps: 0.590167
  68460/150000: episode: 673, duration: 0.620s, episode steps: 104, steps per second: 168, episode reward: -21.196, mean reward: -0.204 [-100.000, 19.176], mean action: 1.596 [0.000, 3.000],  loss: 7.693116, mae: 34.513087, mean_q: 23.832741, mean_eps: 0.589555
  68578/150000: episode: 674, duration: 0.725s, episode steps: 118, steps per second: 163, episode reward: -66.955, mean reward: -0.567 [-100.000,  8.980], mean action: 1.466 [0.000, 3.000],  loss: 8.751436, mae: 34.100249, mean_q: 23.077142, mean_eps: 0.588889
  68673/150000: episode: 675, duration: 0.578s, episode steps:  95, steps per second: 164, episode reward: -59.508, mean reward: -0.626 [-100.000,  8.638], mean action: 1.800 [0.000, 3.000],  loss: 7.714905, mae: 34.389746, mean_q: 24.001040, mean_eps: 0.588250
  68770/150000: episode: 676, duration: 0.573s, episode steps:  97, steps per second: 169, episode reward: -9.020, mean reward: -0.093 [-100.000, 22.299], mean action: 1.763 [0.000, 3.000],  loss: 10.210375, mae: 33.806181, mean_q: 22.383797, mean_eps: 0.587674
  68857/150000: episode: 677, duration: 0.529s, episode steps:  87, steps per second: 165, episode reward: -33.748, mean reward: -0.388 [-100.000, 16.801], mean action: 1.655 [0.000, 3.000],  loss: 4.777493, mae: 34.671423, mean_q: 24.688578, mean_eps: 0.587122
  68973/150000: episode: 678, duration: 0.720s, episode steps: 116, steps per second: 161, episode reward: -35.844, mean reward: -0.309 [-100.000, 10.789], mean action: 1.759 [0.000, 3.000],  loss: 9.372851, mae: 34.278457, mean_q: 23.412705, mean_eps: 0.586513
  69101/150000: episode: 679, duration: 0.761s, episode steps: 128, steps per second: 168, episode reward: -59.687, mean reward: -0.466 [-100.000, 20.362], mean action: 1.625 [0.000, 3.000],  loss: 8.398918, mae: 33.961686, mean_q: 23.115684, mean_eps: 0.585781
  69214/150000: episode: 680, duration: 0.685s, episode steps: 113, steps per second: 165, episode reward: -184.848, mean reward: -1.636 [-100.000, 63.341], mean action: 1.673 [0.000, 3.000],  loss: 6.459258, mae: 34.258488, mean_q: 24.033362, mean_eps: 0.585058
  69325/150000: episode: 681, duration: 0.701s, episode steps: 111, steps per second: 158, episode reward: -38.182, mean reward: -0.344 [-100.000, 17.043], mean action: 1.577 [0.000, 3.000],  loss: 17.291210, mae: 34.427884, mean_q: 23.312813, mean_eps: 0.584386
  69401/150000: episode: 682, duration: 0.445s, episode steps:  76, steps per second: 171, episode reward: -52.219, mean reward: -0.687 [-100.000, 20.352], mean action: 1.592 [0.000, 3.000],  loss: 5.109412, mae: 34.127877, mean_q: 25.655469, mean_eps: 0.583825
  69503/150000: episode: 683, duration: 0.608s, episode steps: 102, steps per second: 168, episode reward: -9.879, mean reward: -0.097 [-100.000, 11.707], mean action: 1.745 [0.000, 3.000],  loss: 10.262203, mae: 34.489995, mean_q: 24.511403, mean_eps: 0.583291
  69625/150000: episode: 684, duration: 0.757s, episode steps: 122, steps per second: 161, episode reward: -28.355, mean reward: -0.232 [-100.000, 11.788], mean action: 1.557 [0.000, 3.000],  loss: 10.317846, mae: 34.583026, mean_q: 23.911172, mean_eps: 0.582619
  69712/150000: episode: 685, duration: 0.522s, episode steps:  87, steps per second: 167, episode reward: -63.544, mean reward: -0.730 [-100.000, 13.681], mean action: 1.747 [0.000, 3.000],  loss: 4.601233, mae: 33.480831, mean_q: 23.059693, mean_eps: 0.581992
  69827/150000: episode: 686, duration: 0.694s, episode steps: 115, steps per second: 166, episode reward: -50.445, mean reward: -0.439 [-100.000, 23.774], mean action: 1.774 [0.000, 3.000],  loss: 8.138107, mae: 34.356623, mean_q: 22.540152, mean_eps: 0.581386
  69914/150000: episode: 687, duration: 0.511s, episode steps:  87, steps per second: 170, episode reward: -10.912, mean reward: -0.125 [-100.000, 11.924], mean action: 1.747 [0.000, 3.000],  loss: 13.701731, mae: 34.374723, mean_q: 22.237415, mean_eps: 0.580780
  70020/150000: episode: 688, duration: 0.686s, episode steps: 106, steps per second: 154, episode reward: -63.949, mean reward: -0.603 [-100.000,  8.689], mean action: 1.613 [0.000, 3.000],  loss: 11.399223, mae: 34.095725, mean_q: 24.434440, mean_eps: 0.580201
  70161/150000: episode: 689, duration: 0.843s, episode steps: 141, steps per second: 167, episode reward: -48.650, mean reward: -0.345 [-100.000,  8.858], mean action: 1.745 [0.000, 3.000],  loss: 7.849746, mae: 34.280602, mean_q: 24.445979, mean_eps: 0.579460
  70264/150000: episode: 690, duration: 0.605s, episode steps: 103, steps per second: 170, episode reward: -26.309, mean reward: -0.255 [-100.000, 15.908], mean action: 1.660 [0.000, 3.000],  loss: 8.330141, mae: 34.634042, mean_q: 25.395689, mean_eps: 0.578728
  70420/150000: episode: 691, duration: 0.966s, episode steps: 156, steps per second: 161, episode reward: -82.054, mean reward: -0.526 [-100.000,  9.387], mean action: 1.641 [0.000, 3.000],  loss: 10.109342, mae: 34.541927, mean_q: 24.134392, mean_eps: 0.577951
  70549/150000: episode: 692, duration: 0.769s, episode steps: 129, steps per second: 168, episode reward: -10.722, mean reward: -0.083 [-100.000, 14.710], mean action: 1.736 [0.000, 3.000],  loss: 6.469054, mae: 34.372666, mean_q: 25.101532, mean_eps: 0.577096
  70648/150000: episode: 693, duration: 0.597s, episode steps:  99, steps per second: 166, episode reward: -54.609, mean reward: -0.552 [-100.000, 10.613], mean action: 1.747 [0.000, 3.000],  loss: 6.364616, mae: 34.182849, mean_q: 24.150236, mean_eps: 0.576412
  70736/150000: episode: 694, duration: 0.572s, episode steps:  88, steps per second: 154, episode reward: -84.183, mean reward: -0.957 [-100.000,  8.241], mean action: 1.636 [0.000, 3.000],  loss: 7.030514, mae: 34.161832, mean_q: 24.186268, mean_eps: 0.575851
  71736/150000: episode: 695, duration: 7.485s, episode steps: 1000, steps per second: 134, episode reward: -21.215, mean reward: -0.021 [-23.713, 26.232], mean action: 1.601 [0.000, 3.000],  loss: 9.242950, mae: 34.056007, mean_q: 24.913973, mean_eps: 0.572587
  71865/150000: episode: 696, duration: 0.857s, episode steps: 129, steps per second: 151, episode reward:  9.328, mean reward:  0.072 [-100.000, 15.861], mean action: 1.628 [0.000, 3.000],  loss: 5.229857, mae: 33.860543, mean_q: 25.161778, mean_eps: 0.569200
  71939/150000: episode: 697, duration: 0.483s, episode steps:  74, steps per second: 153, episode reward: -29.439, mean reward: -0.398 [-100.000, 10.797], mean action: 1.757 [0.000, 3.000],  loss: 4.970703, mae: 34.134179, mean_q: 25.446945, mean_eps: 0.568591
  72102/150000: episode: 698, duration: 1.038s, episode steps: 163, steps per second: 157, episode reward:  0.668, mean reward:  0.004 [-100.000, 21.037], mean action: 1.773 [0.000, 3.000],  loss: 11.003359, mae: 33.050622, mean_q: 25.903188, mean_eps: 0.567880
  72202/150000: episode: 699, duration: 0.659s, episode steps: 100, steps per second: 152, episode reward: -10.922, mean reward: -0.109 [-100.000, 14.249], mean action: 1.650 [0.000, 3.000],  loss: 9.088569, mae: 33.221960, mean_q: 25.669153, mean_eps: 0.567091
  72292/150000: episode: 700, duration: 0.542s, episode steps:  90, steps per second: 166, episode reward: -73.400, mean reward: -0.816 [-100.000, 10.505], mean action: 1.533 [0.000, 3.000],  loss: 10.436051, mae: 32.802427, mean_q: 24.293958, mean_eps: 0.566521
  72394/150000: episode: 701, duration: 0.635s, episode steps: 102, steps per second: 161, episode reward: -25.020, mean reward: -0.245 [-100.000, 11.859], mean action: 1.755 [0.000, 3.000],  loss: 10.075679, mae: 33.147773, mean_q: 24.857854, mean_eps: 0.565945
  72464/150000: episode: 702, duration: 0.416s, episode steps:  70, steps per second: 168, episode reward: -54.739, mean reward: -0.782 [-100.000, 11.559], mean action: 1.786 [0.000, 3.000],  loss: 6.462081, mae: 32.609754, mean_q: 24.024959, mean_eps: 0.565429
  72566/150000: episode: 703, duration: 0.635s, episode steps: 102, steps per second: 161, episode reward: -69.131, mean reward: -0.678 [-100.000, 16.714], mean action: 1.559 [0.000, 3.000],  loss: 5.470295, mae: 32.826095, mean_q: 25.822374, mean_eps: 0.564913
  72663/150000: episode: 704, duration: 0.584s, episode steps:  97, steps per second: 166, episode reward: -65.532, mean reward: -0.676 [-100.000,  8.349], mean action: 1.629 [0.000, 3.000],  loss: 7.105831, mae: 32.422009, mean_q: 25.764401, mean_eps: 0.564316
  72737/150000: episode: 705, duration: 0.454s, episode steps:  74, steps per second: 163, episode reward: -22.620, mean reward: -0.306 [-100.000, 13.455], mean action: 1.865 [0.000, 3.000],  loss: 12.906797, mae: 32.541466, mean_q: 26.782318, mean_eps: 0.563803
  72888/150000: episode: 706, duration: 0.960s, episode steps: 151, steps per second: 157, episode reward: -53.762, mean reward: -0.356 [-100.000, 10.075], mean action: 1.543 [0.000, 3.000],  loss: 9.644598, mae: 32.319598, mean_q: 25.078864, mean_eps: 0.563128
  72981/150000: episode: 707, duration: 0.580s, episode steps:  93, steps per second: 160, episode reward: -21.146, mean reward: -0.227 [-100.000, 23.085], mean action: 1.688 [0.000, 3.000],  loss: 7.651544, mae: 32.173530, mean_q: 24.513119, mean_eps: 0.562396
  73073/150000: episode: 708, duration: 0.548s, episode steps:  92, steps per second: 168, episode reward: -59.942, mean reward: -0.652 [-100.000, 26.492], mean action: 1.761 [0.000, 3.000],  loss: 11.862134, mae: 32.869126, mean_q: 23.338211, mean_eps: 0.561841
  73215/150000: episode: 709, duration: 0.862s, episode steps: 142, steps per second: 165, episode reward: -53.218, mean reward: -0.375 [-100.000,  6.529], mean action: 1.662 [0.000, 3.000],  loss: 8.390398, mae: 32.910887, mean_q: 25.527866, mean_eps: 0.561139
  73294/150000: episode: 710, duration: 0.524s, episode steps:  79, steps per second: 151, episode reward: -42.336, mean reward: -0.536 [-100.000,  8.903], mean action: 1.633 [0.000, 3.000],  loss: 6.875627, mae: 32.509531, mean_q: 24.774192, mean_eps: 0.560476
  73409/150000: episode: 711, duration: 0.677s, episode steps: 115, steps per second: 170, episode reward: 13.493, mean reward:  0.117 [-100.000, 18.160], mean action: 1.617 [0.000, 3.000],  loss: 5.053332, mae: 32.870387, mean_q: 23.509613, mean_eps: 0.559894
  73549/150000: episode: 712, duration: 0.838s, episode steps: 140, steps per second: 167, episode reward: -28.002, mean reward: -0.200 [-100.000, 17.820], mean action: 1.514 [0.000, 3.000],  loss: 5.738934, mae: 32.665910, mean_q: 23.779932, mean_eps: 0.559129
  73628/150000: episode: 713, duration: 0.517s, episode steps:  79, steps per second: 153, episode reward: -52.939, mean reward: -0.670 [-100.000, 11.428], mean action: 1.709 [0.000, 3.000],  loss: 7.315495, mae: 32.721185, mean_q: 24.215916, mean_eps: 0.558472
  73701/150000: episode: 714, duration: 0.442s, episode steps:  73, steps per second: 165, episode reward: -77.926, mean reward: -1.067 [-100.000,  7.088], mean action: 1.685 [0.000, 3.000],  loss: 7.649149, mae: 32.448917, mean_q: 22.049031, mean_eps: 0.558016
  73777/150000: episode: 715, duration: 0.448s, episode steps:  76, steps per second: 170, episode reward: -52.238, mean reward: -0.687 [-100.000,  6.540], mean action: 1.474 [0.000, 3.000],  loss: 8.328691, mae: 32.420546, mean_q: 24.208276, mean_eps: 0.557569
  73870/150000: episode: 716, duration: 0.559s, episode steps:  93, steps per second: 166, episode reward: -69.860, mean reward: -0.751 [-100.000, 16.625], mean action: 1.688 [0.000, 3.000],  loss: 12.497256, mae: 32.810616, mean_q: 24.062170, mean_eps: 0.557062
  73946/150000: episode: 717, duration: 0.506s, episode steps:  76, steps per second: 150, episode reward: -39.746, mean reward: -0.523 [-100.000, 17.046], mean action: 1.750 [0.000, 3.000],  loss: 6.423693, mae: 32.634927, mean_q: 24.277742, mean_eps: 0.556555
  74042/150000: episode: 718, duration: 0.595s, episode steps:  96, steps per second: 161, episode reward: -117.846, mean reward: -1.228 [-100.000,  6.942], mean action: 1.312 [0.000, 3.000],  loss: 17.420770, mae: 32.843164, mean_q: 25.715193, mean_eps: 0.556039
  74142/150000: episode: 719, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -21.213, mean reward: -0.212 [-100.000, 18.007], mean action: 1.480 [0.000, 3.000],  loss: 8.043230, mae: 32.947645, mean_q: 26.336386, mean_eps: 0.555451
  74247/150000: episode: 720, duration: 0.627s, episode steps: 105, steps per second: 167, episode reward: -87.614, mean reward: -0.834 [-100.000,  7.595], mean action: 1.762 [0.000, 3.000],  loss: 8.592522, mae: 32.615966, mean_q: 22.842341, mean_eps: 0.554836
  74378/150000: episode: 721, duration: 0.822s, episode steps: 131, steps per second: 159, episode reward: -37.617, mean reward: -0.287 [-100.000, 18.862], mean action: 1.679 [0.000, 3.000],  loss: 10.973802, mae: 33.540828, mean_q: 25.114313, mean_eps: 0.554128
  74459/150000: episode: 722, duration: 0.485s, episode steps:  81, steps per second: 167, episode reward: -38.745, mean reward: -0.478 [-100.000, 12.375], mean action: 1.531 [0.000, 3.000],  loss: 11.613695, mae: 33.135987, mean_q: 25.141901, mean_eps: 0.553492
  74544/150000: episode: 723, duration: 0.527s, episode steps:  85, steps per second: 161, episode reward: -40.812, mean reward: -0.480 [-100.000,  8.173], mean action: 1.518 [0.000, 3.000],  loss: 15.344627, mae: 33.225768, mean_q: 26.343879, mean_eps: 0.552994
  74714/150000: episode: 724, duration: 1.061s, episode steps: 170, steps per second: 160, episode reward: -135.821, mean reward: -0.799 [-100.000, 26.158], mean action: 1.606 [0.000, 3.000],  loss: 8.057167, mae: 32.603436, mean_q: 24.481983, mean_eps: 0.552229
  74948/150000: episode: 725, duration: 1.406s, episode steps: 234, steps per second: 166, episode reward: -22.182, mean reward: -0.095 [-100.000, 21.285], mean action: 1.645 [0.000, 3.000],  loss: 11.608880, mae: 33.061850, mean_q: 24.837359, mean_eps: 0.551017
  75070/150000: episode: 726, duration: 0.802s, episode steps: 122, steps per second: 152, episode reward: -53.592, mean reward: -0.439 [-100.000, 17.576], mean action: 1.730 [0.000, 3.000],  loss: 9.613360, mae: 33.110038, mean_q: 24.033810, mean_eps: 0.549949
  75211/150000: episode: 727, duration: 0.850s, episode steps: 141, steps per second: 166, episode reward: -264.378, mean reward: -1.875 [-100.000, 51.791], mean action: 1.787 [0.000, 3.000],  loss: 8.585436, mae: 33.056244, mean_q: 24.086724, mean_eps: 0.549160
  75292/150000: episode: 728, duration: 0.483s, episode steps:  81, steps per second: 168, episode reward: -66.838, mean reward: -0.825 [-100.000, 16.433], mean action: 1.864 [0.000, 3.000],  loss: 9.025463, mae: 32.792914, mean_q: 25.586670, mean_eps: 0.548494
  75422/150000: episode: 729, duration: 0.884s, episode steps: 130, steps per second: 147, episode reward: -41.380, mean reward: -0.318 [-100.000, 10.692], mean action: 1.715 [0.000, 3.000],  loss: 13.123806, mae: 33.107957, mean_q: 24.311867, mean_eps: 0.547861
  75536/150000: episode: 730, duration: 0.837s, episode steps: 114, steps per second: 136, episode reward: -17.154, mean reward: -0.150 [-100.000, 13.354], mean action: 1.579 [0.000, 3.000],  loss: 7.843006, mae: 32.940285, mean_q: 24.123364, mean_eps: 0.547129
  75667/150000: episode: 731, duration: 0.936s, episode steps: 131, steps per second: 140, episode reward: -0.505, mean reward: -0.004 [-100.000, 16.109], mean action: 1.725 [0.000, 3.000],  loss: 12.626470, mae: 33.111741, mean_q: 23.153776, mean_eps: 0.546394
  75800/150000: episode: 732, duration: 0.929s, episode steps: 133, steps per second: 143, episode reward: 16.766, mean reward:  0.126 [-100.000, 24.350], mean action: 1.556 [0.000, 3.000],  loss: 12.016478, mae: 33.351285, mean_q: 25.253582, mean_eps: 0.545602
  75918/150000: episode: 733, duration: 0.761s, episode steps: 118, steps per second: 155, episode reward: -23.588, mean reward: -0.200 [-100.000,  7.982], mean action: 1.593 [0.000, 3.000],  loss: 8.248771, mae: 33.834463, mean_q: 25.425652, mean_eps: 0.544849
  75994/150000: episode: 734, duration: 0.511s, episode steps:  76, steps per second: 149, episode reward: -48.946, mean reward: -0.644 [-100.000, 11.035], mean action: 1.645 [0.000, 3.000],  loss: 9.429927, mae: 32.370668, mean_q: 24.203524, mean_eps: 0.544267
  76151/150000: episode: 735, duration: 1.049s, episode steps: 157, steps per second: 150, episode reward: -36.893, mean reward: -0.235 [-100.000, 17.463], mean action: 1.548 [0.000, 3.000],  loss: 12.441933, mae: 33.527206, mean_q: 25.212343, mean_eps: 0.543568
  76251/150000: episode: 736, duration: 0.603s, episode steps: 100, steps per second: 166, episode reward: -60.644, mean reward: -0.606 [-100.000, 10.525], mean action: 1.660 [0.000, 3.000],  loss: 13.137079, mae: 33.457926, mean_q: 24.942343, mean_eps: 0.542797
  76350/150000: episode: 737, duration: 0.624s, episode steps:  99, steps per second: 159, episode reward: -33.639, mean reward: -0.340 [-100.000, 16.870], mean action: 1.616 [0.000, 3.000],  loss: 15.381950, mae: 33.668243, mean_q: 23.828899, mean_eps: 0.542200
  76510/150000: episode: 738, duration: 0.988s, episode steps: 160, steps per second: 162, episode reward: -18.768, mean reward: -0.117 [-100.000, 33.596], mean action: 1.675 [0.000, 3.000],  loss: 10.872759, mae: 33.525624, mean_q: 23.284811, mean_eps: 0.541423
  76595/150000: episode: 739, duration: 0.503s, episode steps:  85, steps per second: 169, episode reward: -85.480, mean reward: -1.006 [-100.000, 10.240], mean action: 1.741 [0.000, 3.000],  loss: 7.874933, mae: 33.197194, mean_q: 24.878010, mean_eps: 0.540688
  76686/150000: episode: 740, duration: 0.571s, episode steps:  91, steps per second: 159, episode reward: -69.278, mean reward: -0.761 [-100.000, 11.118], mean action: 1.593 [0.000, 3.000],  loss: 9.102402, mae: 33.079244, mean_q: 24.125965, mean_eps: 0.540160
  76818/150000: episode: 741, duration: 0.806s, episode steps: 132, steps per second: 164, episode reward: -118.475, mean reward: -0.898 [-100.000,  7.856], mean action: 1.788 [0.000, 3.000],  loss: 11.109179, mae: 33.627888, mean_q: 24.564655, mean_eps: 0.539491
  76900/150000: episode: 742, duration: 0.486s, episode steps:  82, steps per second: 169, episode reward: -69.108, mean reward: -0.843 [-100.000, 20.877], mean action: 1.573 [0.000, 3.000],  loss: 9.222288, mae: 33.602929, mean_q: 25.221396, mean_eps: 0.538849
  77007/150000: episode: 743, duration: 0.657s, episode steps: 107, steps per second: 163, episode reward: -77.238, mean reward: -0.722 [-100.000, 12.526], mean action: 1.589 [0.000, 3.000],  loss: 7.813053, mae: 32.740062, mean_q: 23.227708, mean_eps: 0.538282
  77125/150000: episode: 744, duration: 0.733s, episode steps: 118, steps per second: 161, episode reward: -17.458, mean reward: -0.148 [-100.000, 13.432], mean action: 1.797 [0.000, 3.000],  loss: 6.893837, mae: 33.542747, mean_q: 23.822705, mean_eps: 0.537607
  77214/150000: episode: 745, duration: 0.529s, episode steps:  89, steps per second: 168, episode reward: 15.949, mean reward:  0.179 [-100.000,  7.074], mean action: 1.697 [0.000, 3.000],  loss: 6.927464, mae: 33.205130, mean_q: 23.229013, mean_eps: 0.536986
  77355/150000: episode: 746, duration: 0.851s, episode steps: 141, steps per second: 166, episode reward: -47.894, mean reward: -0.340 [-100.000, 10.647], mean action: 1.617 [0.000, 3.000],  loss: 5.948315, mae: 33.327602, mean_q: 24.156805, mean_eps: 0.536296
  77437/150000: episode: 747, duration: 0.517s, episode steps:  82, steps per second: 159, episode reward: -30.844, mean reward: -0.376 [-100.000, 12.623], mean action: 1.720 [0.000, 3.000],  loss: 18.220829, mae: 33.932649, mean_q: 23.920192, mean_eps: 0.535627
  77526/150000: episode: 748, duration: 0.531s, episode steps:  89, steps per second: 168, episode reward: -3.847, mean reward: -0.043 [-100.000, 12.477], mean action: 1.730 [0.000, 3.000],  loss: 17.627392, mae: 33.761713, mean_q: 23.452009, mean_eps: 0.535114
  77652/150000: episode: 749, duration: 0.759s, episode steps: 126, steps per second: 166, episode reward: -110.233, mean reward: -0.875 [-100.000, 10.571], mean action: 1.540 [0.000, 3.000],  loss: 6.276308, mae: 33.013493, mean_q: 24.552114, mean_eps: 0.534469
  77720/150000: episode: 750, duration: 0.439s, episode steps:  68, steps per second: 155, episode reward: -24.892, mean reward: -0.366 [-100.000, 11.220], mean action: 1.574 [0.000, 3.000],  loss: 10.275342, mae: 33.138838, mean_q: 22.050846, mean_eps: 0.533887
  77840/150000: episode: 751, duration: 0.781s, episode steps: 120, steps per second: 154, episode reward: -38.100, mean reward: -0.318 [-100.000,  8.935], mean action: 1.542 [0.000, 3.000],  loss: 17.084982, mae: 33.669122, mean_q: 24.763148, mean_eps: 0.533323
  77954/150000: episode: 752, duration: 0.692s, episode steps: 114, steps per second: 165, episode reward: -26.650, mean reward: -0.234 [-100.000, 17.314], mean action: 1.702 [0.000, 3.000],  loss: 10.312383, mae: 33.591795, mean_q: 24.146861, mean_eps: 0.532621
  78042/150000: episode: 753, duration: 0.525s, episode steps:  88, steps per second: 167, episode reward: -91.227, mean reward: -1.037 [-100.000, 21.243], mean action: 1.659 [0.000, 3.000],  loss: 7.942698, mae: 33.273676, mean_q: 23.878562, mean_eps: 0.532015
  78183/150000: episode: 754, duration: 0.875s, episode steps: 141, steps per second: 161, episode reward: -69.556, mean reward: -0.493 [-100.000, 15.267], mean action: 1.745 [0.000, 3.000],  loss: 12.309190, mae: 34.046456, mean_q: 24.349771, mean_eps: 0.531328
  78330/150000: episode: 755, duration: 0.874s, episode steps: 147, steps per second: 168, episode reward: -55.068, mean reward: -0.375 [-100.000,  8.747], mean action: 1.667 [0.000, 3.000],  loss: 5.978595, mae: 34.169123, mean_q: 24.916194, mean_eps: 0.530464
  78427/150000: episode: 756, duration: 0.601s, episode steps:  97, steps per second: 161, episode reward: -44.497, mean reward: -0.459 [-100.000, 13.905], mean action: 1.557 [0.000, 3.000],  loss: 7.203827, mae: 33.763114, mean_q: 24.490947, mean_eps: 0.529732
  78544/150000: episode: 757, duration: 0.703s, episode steps: 117, steps per second: 166, episode reward: -36.769, mean reward: -0.314 [-100.000,  8.408], mean action: 1.658 [0.000, 3.000],  loss: 12.024985, mae: 33.875485, mean_q: 24.249340, mean_eps: 0.529090
  78669/150000: episode: 758, duration: 0.741s, episode steps: 125, steps per second: 169, episode reward: -61.861, mean reward: -0.495 [-100.000, 10.966], mean action: 1.744 [0.000, 3.000],  loss: 12.749981, mae: 34.063416, mean_q: 24.431044, mean_eps: 0.528364
  78788/150000: episode: 759, duration: 0.726s, episode steps: 119, steps per second: 164, episode reward: -18.047, mean reward: -0.152 [-100.000, 22.701], mean action: 1.765 [0.000, 3.000],  loss: 10.633575, mae: 33.963509, mean_q: 22.953892, mean_eps: 0.527632
  79312/150000: episode: 760, duration: 3.393s, episode steps: 524, steps per second: 154, episode reward: -147.202, mean reward: -0.281 [-100.000, 24.839], mean action: 1.366 [0.000, 3.000],  loss: 8.264197, mae: 33.857960, mean_q: 24.279194, mean_eps: 0.525703
  79404/150000: episode: 761, duration: 0.551s, episode steps:  92, steps per second: 167, episode reward: -39.217, mean reward: -0.426 [-100.000,  7.070], mean action: 1.717 [0.000, 3.000],  loss: 4.563949, mae: 33.998927, mean_q: 24.056523, mean_eps: 0.523855
  79502/150000: episode: 762, duration: 0.607s, episode steps:  98, steps per second: 161, episode reward: -166.977, mean reward: -1.704 [-100.000,  4.073], mean action: 1.357 [0.000, 3.000],  loss: 6.457463, mae: 34.193406, mean_q: 24.758827, mean_eps: 0.523285
  79600/150000: episode: 763, duration: 0.598s, episode steps:  98, steps per second: 164, episode reward: -50.379, mean reward: -0.514 [-100.000, 12.569], mean action: 1.490 [0.000, 3.000],  loss: 10.123385, mae: 33.900717, mean_q: 23.949668, mean_eps: 0.522697
  79769/150000: episode: 764, duration: 1.040s, episode steps: 169, steps per second: 163, episode reward: -5.173, mean reward: -0.031 [-100.000, 17.612], mean action: 1.746 [0.000, 3.000],  loss: 7.414335, mae: 33.583721, mean_q: 24.257374, mean_eps: 0.521896
  79925/150000: episode: 765, duration: 0.981s, episode steps: 156, steps per second: 159, episode reward: 17.681, mean reward:  0.113 [-100.000, 10.337], mean action: 1.795 [0.000, 3.000],  loss: 10.001562, mae: 34.141218, mean_q: 23.933073, mean_eps: 0.520921
  80571/150000: episode: 766, duration: 4.157s, episode steps: 646, steps per second: 155, episode reward: -170.862, mean reward: -0.264 [-100.000, 44.653], mean action: 1.659 [0.000, 3.000],  loss: 8.659197, mae: 33.837397, mean_q: 24.401268, mean_eps: 0.518515
  80690/150000: episode: 767, duration: 0.699s, episode steps: 119, steps per second: 170, episode reward: -39.519, mean reward: -0.332 [-100.000, 11.129], mean action: 1.420 [0.000, 3.000],  loss: 9.029961, mae: 33.242637, mean_q: 23.276924, mean_eps: 0.516220
  81391/150000: episode: 768, duration: 4.606s, episode steps: 701, steps per second: 152, episode reward: -276.328, mean reward: -0.394 [-100.000, 50.843], mean action: 2.080 [0.000, 3.000],  loss: 8.959680, mae: 33.699290, mean_q: 23.935505, mean_eps: 0.513760
  81504/150000: episode: 769, duration: 0.686s, episode steps: 113, steps per second: 165, episode reward: -26.403, mean reward: -0.234 [-100.000, 42.874], mean action: 1.655 [0.000, 3.000],  loss: 11.054618, mae: 33.160714, mean_q: 23.398534, mean_eps: 0.511318
  81665/150000: episode: 770, duration: 0.984s, episode steps: 161, steps per second: 164, episode reward: -33.681, mean reward: -0.209 [-100.000, 10.959], mean action: 1.795 [0.000, 3.000],  loss: 9.273631, mae: 33.482916, mean_q: 22.480909, mean_eps: 0.510496
  81785/150000: episode: 771, duration: 0.707s, episode steps: 120, steps per second: 170, episode reward: -11.308, mean reward: -0.094 [-100.000,  9.588], mean action: 1.733 [0.000, 3.000],  loss: 7.939626, mae: 33.989217, mean_q: 25.286408, mean_eps: 0.509653
  81913/150000: episode: 772, duration: 0.897s, episode steps: 128, steps per second: 143, episode reward: -110.113, mean reward: -0.860 [-100.000, 10.102], mean action: 1.422 [0.000, 3.000],  loss: 11.282589, mae: 33.739656, mean_q: 23.704867, mean_eps: 0.508909
  82025/150000: episode: 773, duration: 0.749s, episode steps: 112, steps per second: 149, episode reward: -1.061, mean reward: -0.009 [-100.000, 10.934], mean action: 1.821 [0.000, 3.000],  loss: 12.463870, mae: 33.624116, mean_q: 23.961220, mean_eps: 0.508189
  82111/150000: episode: 774, duration: 0.555s, episode steps:  86, steps per second: 155, episode reward: -15.499, mean reward: -0.180 [-100.000, 20.080], mean action: 1.721 [0.000, 3.000],  loss: 17.674302, mae: 33.884017, mean_q: 22.823187, mean_eps: 0.507595
  82227/150000: episode: 775, duration: 0.820s, episode steps: 116, steps per second: 141, episode reward: 61.181, mean reward:  0.527 [-100.000, 28.673], mean action: 1.897 [0.000, 3.000],  loss: 16.398221, mae: 33.968367, mean_q: 24.684448, mean_eps: 0.506989
  82318/150000: episode: 776, duration: 0.594s, episode steps:  91, steps per second: 153, episode reward: -45.749, mean reward: -0.503 [-100.000, 12.890], mean action: 1.659 [0.000, 3.000],  loss: 10.008467, mae: 33.618819, mean_q: 24.269519, mean_eps: 0.506368
  82393/150000: episode: 777, duration: 0.469s, episode steps:  75, steps per second: 160, episode reward: -67.123, mean reward: -0.895 [-100.000,  6.333], mean action: 1.347 [0.000, 3.000],  loss: 8.161650, mae: 33.715959, mean_q: 24.368128, mean_eps: 0.505870
  82490/150000: episode: 778, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -26.556, mean reward: -0.274 [-100.000, 12.628], mean action: 1.845 [0.000, 3.000],  loss: 7.496339, mae: 33.725756, mean_q: 24.478350, mean_eps: 0.505354
  82589/150000: episode: 779, duration: 0.679s, episode steps:  99, steps per second: 146, episode reward: -16.362, mean reward: -0.165 [-100.000, 10.402], mean action: 1.768 [0.000, 3.000],  loss: 11.529725, mae: 34.076653, mean_q: 24.337856, mean_eps: 0.504766
  82709/150000: episode: 780, duration: 0.741s, episode steps: 120, steps per second: 162, episode reward: -32.478, mean reward: -0.271 [-100.000, 17.170], mean action: 1.458 [0.000, 3.000],  loss: 10.337903, mae: 33.798391, mean_q: 24.173626, mean_eps: 0.504109
  82860/150000: episode: 781, duration: 0.922s, episode steps: 151, steps per second: 164, episode reward: -47.256, mean reward: -0.313 [-100.000, 20.645], mean action: 1.669 [0.000, 3.000],  loss: 10.094009, mae: 33.976540, mean_q: 23.874885, mean_eps: 0.503296
  82990/150000: episode: 782, duration: 0.786s, episode steps: 130, steps per second: 165, episode reward: -4.654, mean reward: -0.036 [-100.000, 18.473], mean action: 1.692 [0.000, 3.000],  loss: 13.541440, mae: 34.000271, mean_q: 23.908460, mean_eps: 0.502453
  83081/150000: episode: 783, duration: 0.541s, episode steps:  91, steps per second: 168, episode reward: -75.776, mean reward: -0.833 [-100.000,  8.223], mean action: 1.505 [0.000, 3.000],  loss: 16.694160, mae: 33.622935, mean_q: 25.184684, mean_eps: 0.501790
  83170/150000: episode: 784, duration: 0.531s, episode steps:  89, steps per second: 168, episode reward: -16.689, mean reward: -0.188 [-100.000, 22.403], mean action: 1.685 [0.000, 3.000],  loss: 11.946522, mae: 34.056041, mean_q: 24.752487, mean_eps: 0.501250
  83266/150000: episode: 785, duration: 0.633s, episode steps:  96, steps per second: 152, episode reward: -25.336, mean reward: -0.264 [-100.000, 11.082], mean action: 1.604 [0.000, 3.000],  loss: 8.517109, mae: 33.802680, mean_q: 24.862490, mean_eps: 0.500695
  83354/150000: episode: 786, duration: 0.530s, episode steps:  88, steps per second: 166, episode reward: -47.852, mean reward: -0.544 [-100.000,  8.913], mean action: 1.773 [0.000, 3.000],  loss: 8.164041, mae: 33.620934, mean_q: 23.853491, mean_eps: 0.500143
  83481/150000: episode: 787, duration: 0.751s, episode steps: 127, steps per second: 169, episode reward: -6.090, mean reward: -0.048 [-100.000, 13.198], mean action: 1.756 [0.000, 3.000],  loss: 6.277225, mae: 33.551602, mean_q: 23.135787, mean_eps: 0.499498
  84028/150000: episode: 788, duration: 3.598s, episode steps: 547, steps per second: 152, episode reward: -114.842, mean reward: -0.210 [-100.000, 26.921], mean action: 1.735 [0.000, 3.000],  loss: 8.391358, mae: 33.617533, mean_q: 24.112434, mean_eps: 0.497476
  84144/150000: episode: 789, duration: 0.693s, episode steps: 116, steps per second: 167, episode reward: -42.050, mean reward: -0.362 [-100.000, 11.401], mean action: 1.655 [0.000, 3.000],  loss: 6.426086, mae: 33.170407, mean_q: 23.352375, mean_eps: 0.495487
  84236/150000: episode: 790, duration: 0.575s, episode steps:  92, steps per second: 160, episode reward: -15.581, mean reward: -0.169 [-100.000, 14.371], mean action: 1.967 [0.000, 3.000],  loss: 11.559421, mae: 33.406263, mean_q: 24.580740, mean_eps: 0.494863
  84367/150000: episode: 791, duration: 0.777s, episode steps: 131, steps per second: 169, episode reward: -97.775, mean reward: -0.746 [-100.000,  7.120], mean action: 1.672 [0.000, 3.000],  loss: 10.237358, mae: 33.720328, mean_q: 25.120910, mean_eps: 0.494194
  84480/150000: episode: 792, duration: 0.675s, episode steps: 113, steps per second: 167, episode reward:  7.395, mean reward:  0.065 [-100.000, 14.606], mean action: 1.743 [0.000, 3.000],  loss: 8.942072, mae: 33.556136, mean_q: 24.476566, mean_eps: 0.493462
  84600/150000: episode: 793, duration: 0.771s, episode steps: 120, steps per second: 156, episode reward:  2.229, mean reward:  0.019 [-100.000, 13.743], mean action: 1.700 [0.000, 3.000],  loss: 6.445369, mae: 32.623641, mean_q: 23.643801, mean_eps: 0.492763
  85600/150000: episode: 794, duration: 7.388s, episode steps: 1000, steps per second: 135, episode reward: -3.358, mean reward: -0.003 [-22.151, 24.431], mean action: 1.713 [0.000, 3.000],  loss: 10.492578, mae: 33.277689, mean_q: 24.742444, mean_eps: 0.489403
  85722/150000: episode: 795, duration: 0.858s, episode steps: 122, steps per second: 142, episode reward: 33.860, mean reward:  0.278 [-100.000, 17.787], mean action: 1.713 [0.000, 3.000],  loss: 7.564501, mae: 32.973613, mean_q: 24.875713, mean_eps: 0.486037
  85838/150000: episode: 796, duration: 0.870s, episode steps: 116, steps per second: 133, episode reward: -2.097, mean reward: -0.018 [-100.000, 16.420], mean action: 1.810 [0.000, 3.000],  loss: 6.683536, mae: 32.816184, mean_q: 25.295688, mean_eps: 0.485323
  85983/150000: episode: 797, duration: 0.942s, episode steps: 145, steps per second: 154, episode reward: -7.600, mean reward: -0.052 [-100.000,  8.934], mean action: 1.759 [0.000, 3.000],  loss: 7.104330, mae: 32.896914, mean_q: 24.918784, mean_eps: 0.484540
  86090/150000: episode: 798, duration: 0.740s, episode steps: 107, steps per second: 145, episode reward: -68.755, mean reward: -0.643 [-100.000, 21.647], mean action: 1.682 [0.000, 3.000],  loss: 10.828231, mae: 33.135182, mean_q: 23.539890, mean_eps: 0.483784
  86241/150000: episode: 799, duration: 1.003s, episode steps: 151, steps per second: 151, episode reward: -83.607, mean reward: -0.554 [-100.000, 11.006], mean action: 1.576 [0.000, 3.000],  loss: 8.617732, mae: 32.711368, mean_q: 24.213261, mean_eps: 0.483010
  86356/150000: episode: 800, duration: 0.755s, episode steps: 115, steps per second: 152, episode reward: -3.273, mean reward: -0.028 [-100.000, 13.061], mean action: 1.722 [0.000, 3.000],  loss: 10.990843, mae: 33.246161, mean_q: 25.000232, mean_eps: 0.482212
  86497/150000: episode: 801, duration: 0.893s, episode steps: 141, steps per second: 158, episode reward: 17.280, mean reward:  0.123 [-100.000, 19.878], mean action: 1.660 [0.000, 3.000],  loss: 8.277857, mae: 33.021360, mean_q: 24.474144, mean_eps: 0.481444
  86598/150000: episode: 802, duration: 0.625s, episode steps: 101, steps per second: 162, episode reward: 11.501, mean reward:  0.114 [-100.000, 10.256], mean action: 1.723 [0.000, 3.000],  loss: 8.362346, mae: 32.958668, mean_q: 23.933756, mean_eps: 0.480718
  86733/150000: episode: 803, duration: 0.803s, episode steps: 135, steps per second: 168, episode reward: -121.594, mean reward: -0.901 [-100.000, 12.388], mean action: 1.563 [0.000, 3.000],  loss: 8.908522, mae: 33.136389, mean_q: 25.016115, mean_eps: 0.480010
  86857/150000: episode: 804, duration: 0.849s, episode steps: 124, steps per second: 146, episode reward: -49.683, mean reward: -0.401 [-100.000, 11.928], mean action: 1.653 [0.000, 3.000],  loss: 7.658505, mae: 32.846292, mean_q: 25.467041, mean_eps: 0.479233
  86996/150000: episode: 805, duration: 0.856s, episode steps: 139, steps per second: 162, episode reward: -38.229, mean reward: -0.275 [-100.000, 11.006], mean action: 1.755 [0.000, 3.000],  loss: 11.193885, mae: 33.390777, mean_q: 25.454840, mean_eps: 0.478444
  87143/150000: episode: 806, duration: 0.939s, episode steps: 147, steps per second: 157, episode reward: -23.758, mean reward: -0.162 [-100.000, 13.015], mean action: 1.748 [0.000, 3.000],  loss: 6.222948, mae: 33.053010, mean_q: 25.973141, mean_eps: 0.477586
  88143/150000: episode: 807, duration: 7.115s, episode steps: 1000, steps per second: 141, episode reward: 24.688, mean reward:  0.025 [-23.861, 21.634], mean action: 1.532 [0.000, 3.000],  loss: 9.313927, mae: 32.858655, mean_q: 25.255907, mean_eps: 0.474145
  88287/150000: episode: 808, duration: 0.855s, episode steps: 144, steps per second: 168, episode reward: -1.763, mean reward: -0.012 [-100.000, 12.673], mean action: 1.590 [0.000, 3.000],  loss: 7.370788, mae: 33.143683, mean_q: 24.646914, mean_eps: 0.470713
  88395/150000: episode: 809, duration: 0.680s, episode steps: 108, steps per second: 159, episode reward: -24.716, mean reward: -0.229 [-100.000, 10.553], mean action: 1.639 [0.000, 3.000],  loss: 7.376350, mae: 33.097504, mean_q: 25.252300, mean_eps: 0.469957
  88510/150000: episode: 810, duration: 0.678s, episode steps: 115, steps per second: 170, episode reward: -13.099, mean reward: -0.114 [-100.000, 18.783], mean action: 1.626 [0.000, 3.000],  loss: 9.762322, mae: 33.520743, mean_q: 24.970583, mean_eps: 0.469288
  88657/150000: episode: 811, duration: 0.891s, episode steps: 147, steps per second: 165, episode reward: -4.768, mean reward: -0.032 [-100.000, 13.899], mean action: 1.605 [0.000, 3.000],  loss: 6.540428, mae: 32.759948, mean_q: 24.363907, mean_eps: 0.468502
  88833/150000: episode: 812, duration: 1.091s, episode steps: 176, steps per second: 161, episode reward: -32.607, mean reward: -0.185 [-100.000, 11.139], mean action: 1.676 [0.000, 3.000],  loss: 8.246458, mae: 33.013218, mean_q: 25.230854, mean_eps: 0.467533
  89037/150000: episode: 813, duration: 1.278s, episode steps: 204, steps per second: 160, episode reward: -39.432, mean reward: -0.193 [-100.000, 16.554], mean action: 1.725 [0.000, 3.000],  loss: 7.730897, mae: 32.633938, mean_q: 24.483012, mean_eps: 0.466393
  89284/150000: episode: 814, duration: 1.505s, episode steps: 247, steps per second: 164, episode reward: -98.895, mean reward: -0.400 [-100.000,  7.198], mean action: 1.745 [0.000, 3.000],  loss: 8.192324, mae: 32.121589, mean_q: 24.888476, mean_eps: 0.465040
  90284/150000: episode: 815, duration: 7.516s, episode steps: 1000, steps per second: 133, episode reward: -14.133, mean reward: -0.014 [-24.511, 21.947], mean action: 1.848 [0.000, 3.000],  loss: 7.739128, mae: 32.165587, mean_q: 25.262434, mean_eps: 0.461299
  90448/150000: episode: 816, duration: 0.976s, episode steps: 164, steps per second: 168, episode reward: -98.907, mean reward: -0.603 [-100.000, 11.326], mean action: 1.707 [0.000, 3.000],  loss: 7.776942, mae: 32.447644, mean_q: 24.821419, mean_eps: 0.457807
  90549/150000: episode: 817, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: -91.371, mean reward: -0.905 [-100.000,  9.692], mean action: 1.634 [0.000, 3.000],  loss: 6.702886, mae: 32.743806, mean_q: 25.607265, mean_eps: 0.457012
  90656/150000: episode: 818, duration: 0.687s, episode steps: 107, steps per second: 156, episode reward: 24.436, mean reward:  0.228 [-100.000, 12.081], mean action: 1.748 [0.000, 3.000],  loss: 8.774468, mae: 32.794503, mean_q: 25.461651, mean_eps: 0.456388
  90854/150000: episode: 819, duration: 1.191s, episode steps: 198, steps per second: 166, episode reward: -45.251, mean reward: -0.229 [-100.000, 15.883], mean action: 1.657 [0.000, 3.000],  loss: 6.361017, mae: 32.441715, mean_q: 26.445431, mean_eps: 0.455473
  91251/150000: episode: 820, duration: 2.517s, episode steps: 397, steps per second: 158, episode reward: -125.499, mean reward: -0.316 [-100.000, 20.972], mean action: 1.605 [0.000, 3.000],  loss: 7.635239, mae: 32.664527, mean_q: 25.400440, mean_eps: 0.453688
  91363/150000: episode: 821, duration: 0.699s, episode steps: 112, steps per second: 160, episode reward: -45.328, mean reward: -0.405 [-100.000, 14.173], mean action: 1.732 [0.000, 3.000],  loss: 7.958932, mae: 33.325026, mean_q: 27.648000, mean_eps: 0.452161
  91484/150000: episode: 822, duration: 0.773s, episode steps: 121, steps per second: 157, episode reward: -61.199, mean reward: -0.506 [-100.000, 16.159], mean action: 1.612 [0.000, 3.000],  loss: 14.110130, mae: 32.654569, mean_q: 25.707527, mean_eps: 0.451462
  91587/150000: episode: 823, duration: 0.619s, episode steps: 103, steps per second: 166, episode reward: 13.036, mean reward:  0.127 [-100.000, 11.293], mean action: 1.592 [0.000, 3.000],  loss: 9.505219, mae: 32.485691, mean_q: 25.263126, mean_eps: 0.450790
  92208/150000: episode: 824, duration: 4.518s, episode steps: 621, steps per second: 137, episode reward: -60.391, mean reward: -0.097 [-100.000, 21.661], mean action: 1.671 [0.000, 3.000],  loss: 10.862424, mae: 32.679000, mean_q: 25.666433, mean_eps: 0.448618
  93208/150000: episode: 825, duration: 6.792s, episode steps: 1000, steps per second: 147, episode reward: 39.038, mean reward:  0.039 [-23.973, 24.178], mean action: 1.745 [0.000, 3.000],  loss: 9.307181, mae: 31.848480, mean_q: 25.415786, mean_eps: 0.443755
  94208/150000: episode: 826, duration: 7.209s, episode steps: 1000, steps per second: 139, episode reward: -37.639, mean reward: -0.038 [-21.565, 24.084], mean action: 1.727 [0.000, 3.000],  loss: 9.493889, mae: 31.640735, mean_q: 26.067538, mean_eps: 0.437755
  95208/150000: episode: 827, duration: 7.468s, episode steps: 1000, steps per second: 134, episode reward:  6.000, mean reward:  0.006 [-24.561, 17.732], mean action: 1.672 [0.000, 3.000],  loss: 9.972127, mae: 30.977943, mean_q: 25.815697, mean_eps: 0.431755
  95380/150000: episode: 828, duration: 1.434s, episode steps: 172, steps per second: 120, episode reward: -2.443, mean reward: -0.014 [-100.000, 19.077], mean action: 1.645 [0.000, 3.000],  loss: 7.622260, mae: 31.360012, mean_q: 26.382037, mean_eps: 0.428239
  95515/150000: episode: 829, duration: 1.118s, episode steps: 135, steps per second: 121, episode reward: 10.608, mean reward:  0.079 [-100.000, 21.872], mean action: 1.896 [0.000, 3.000],  loss: 10.757961, mae: 31.259175, mean_q: 24.901088, mean_eps: 0.427318
  95763/150000: episode: 830, duration: 1.889s, episode steps: 248, steps per second: 131, episode reward: -57.026, mean reward: -0.230 [-100.000, 21.272], mean action: 1.758 [0.000, 3.000],  loss: 8.130372, mae: 30.937927, mean_q: 24.903832, mean_eps: 0.426169
  96763/150000: episode: 831, duration: 7.594s, episode steps: 1000, steps per second: 132, episode reward: 26.490, mean reward:  0.026 [-24.287, 20.992], mean action: 1.757 [0.000, 3.000],  loss: 7.868112, mae: 30.888967, mean_q: 25.109080, mean_eps: 0.422425
  96899/150000: episode: 832, duration: 0.832s, episode steps: 136, steps per second: 163, episode reward: -36.126, mean reward: -0.266 [-100.000, 16.181], mean action: 1.743 [0.000, 3.000],  loss: 6.013384, mae: 30.973143, mean_q: 25.004210, mean_eps: 0.419017
  97899/150000: episode: 833, duration: 7.280s, episode steps: 1000, steps per second: 137, episode reward: 85.257, mean reward:  0.085 [-19.613, 22.788], mean action: 1.405 [0.000, 3.000],  loss: 9.173872, mae: 30.359218, mean_q: 25.160481, mean_eps: 0.415609
  98706/150000: episode: 834, duration: 5.264s, episode steps: 807, steps per second: 153, episode reward: 166.347, mean reward:  0.206 [-20.278, 100.000], mean action: 1.316 [0.000, 3.000],  loss: 8.309538, mae: 30.345566, mean_q: 25.459851, mean_eps: 0.410188
  98814/150000: episode: 835, duration: 0.664s, episode steps: 108, steps per second: 163, episode reward: -9.215, mean reward: -0.085 [-100.000, 17.798], mean action: 1.620 [0.000, 3.000],  loss: 11.541409, mae: 30.424693, mean_q: 24.212710, mean_eps: 0.407443
  98896/150000: episode: 836, duration: 0.520s, episode steps:  82, steps per second: 158, episode reward: -46.924, mean reward: -0.572 [-100.000,  7.138], mean action: 1.890 [0.000, 3.000],  loss: 6.861879, mae: 29.740295, mean_q: 25.472671, mean_eps: 0.406873
  99896/150000: episode: 837, duration: 6.722s, episode steps: 1000, steps per second: 149, episode reward: 88.313, mean reward:  0.088 [-21.799, 23.403], mean action: 1.319 [0.000, 3.000],  loss: 7.533671, mae: 29.396043, mean_q: 24.521251, mean_eps: 0.403627
 100111/150000: episode: 838, duration: 1.306s, episode steps: 215, steps per second: 165, episode reward:  0.416, mean reward:  0.002 [-100.000, 16.516], mean action: 1.791 [0.000, 3.000],  loss: 6.257644, mae: 28.815993, mean_q: 24.921729, mean_eps: 0.399982
 100576/150000: episode: 839, duration: 3.087s, episode steps: 465, steps per second: 151, episode reward: -209.007, mean reward: -0.449 [-100.000, 12.136], mean action: 1.738 [0.000, 3.000],  loss: 9.988378, mae: 28.615792, mean_q: 25.040491, mean_eps: 0.397942
 100684/150000: episode: 840, duration: 0.656s, episode steps: 108, steps per second: 165, episode reward: -75.717, mean reward: -0.701 [-100.000, 12.507], mean action: 1.731 [0.000, 3.000],  loss: 8.035414, mae: 28.164369, mean_q: 24.507281, mean_eps: 0.396223
 101684/150000: episode: 841, duration: 7.118s, episode steps: 1000, steps per second: 140, episode reward: 101.744, mean reward:  0.102 [-22.978, 22.179], mean action: 1.375 [0.000, 3.000],  loss: 9.796078, mae: 28.435006, mean_q: 24.938317, mean_eps: 0.392899
 102684/150000: episode: 842, duration: 6.820s, episode steps: 1000, steps per second: 147, episode reward: 31.454, mean reward:  0.031 [-22.389, 24.255], mean action: 1.419 [0.000, 3.000],  loss: 7.454726, mae: 28.436343, mean_q: 25.298180, mean_eps: 0.386899
 102831/150000: episode: 843, duration: 0.901s, episode steps: 147, steps per second: 163, episode reward: -58.447, mean reward: -0.398 [-100.000, 16.389], mean action: 1.810 [0.000, 3.000],  loss: 7.120990, mae: 28.170148, mean_q: 25.234522, mean_eps: 0.383458
 103146/150000: episode: 844, duration: 2.018s, episode steps: 315, steps per second: 156, episode reward: -221.427, mean reward: -0.703 [-100.000, 16.525], mean action: 1.886 [0.000, 3.000],  loss: 8.008704, mae: 27.847715, mean_q: 24.862751, mean_eps: 0.382072
 103456/150000: episode: 845, duration: 1.998s, episode steps: 310, steps per second: 155, episode reward: -141.716, mean reward: -0.457 [-100.000, 11.413], mean action: 1.774 [0.000, 3.000],  loss: 10.775655, mae: 27.705427, mean_q: 25.261139, mean_eps: 0.380197
 103549/150000: episode: 846, duration: 0.561s, episode steps:  93, steps per second: 166, episode reward: -9.193, mean reward: -0.099 [-100.000, 11.132], mean action: 1.763 [0.000, 3.000],  loss: 7.476450, mae: 26.941485, mean_q: 24.062128, mean_eps: 0.378988
 103662/150000: episode: 847, duration: 0.680s, episode steps: 113, steps per second: 166, episode reward: -50.524, mean reward: -0.447 [-100.000,  6.218], mean action: 1.558 [0.000, 3.000],  loss: 7.363520, mae: 26.963453, mean_q: 24.772574, mean_eps: 0.378370
 103999/150000: episode: 848, duration: 2.182s, episode steps: 337, steps per second: 154, episode reward: -219.570, mean reward: -0.652 [-100.000, 44.748], mean action: 1.593 [0.000, 3.000],  loss: 10.732520, mae: 27.306322, mean_q: 23.978986, mean_eps: 0.377020
 104144/150000: episode: 849, duration: 0.920s, episode steps: 145, steps per second: 158, episode reward: -71.743, mean reward: -0.495 [-100.000, 22.033], mean action: 1.745 [0.000, 3.000],  loss: 7.017715, mae: 26.969769, mean_q: 24.162845, mean_eps: 0.375574
 104684/150000: episode: 850, duration: 3.525s, episode steps: 540, steps per second: 153, episode reward: -157.513, mean reward: -0.292 [-100.000, 10.931], mean action: 1.704 [0.000, 3.000],  loss: 7.728086, mae: 26.980469, mean_q: 24.479282, mean_eps: 0.373519
 104913/150000: episode: 851, duration: 1.524s, episode steps: 229, steps per second: 150, episode reward:  8.632, mean reward:  0.038 [-100.000, 15.275], mean action: 1.886 [0.000, 3.000],  loss: 11.585555, mae: 26.611821, mean_q: 24.168826, mean_eps: 0.371212
 105021/150000: episode: 852, duration: 0.797s, episode steps: 108, steps per second: 135, episode reward: -10.947, mean reward: -0.101 [-100.000,  8.503], mean action: 1.824 [0.000, 3.000],  loss: 13.877724, mae: 26.898595, mean_q: 23.838790, mean_eps: 0.370201
 105190/150000: episode: 853, duration: 1.219s, episode steps: 169, steps per second: 139, episode reward: -15.583, mean reward: -0.092 [-100.000, 16.171], mean action: 1.811 [0.000, 3.000],  loss: 7.696679, mae: 27.057005, mean_q: 25.237230, mean_eps: 0.369370
 106190/150000: episode: 854, duration: 7.406s, episode steps: 1000, steps per second: 135, episode reward: 62.708, mean reward:  0.063 [-21.316, 24.232], mean action: 1.361 [0.000, 3.000],  loss: 7.944271, mae: 26.872648, mean_q: 25.102131, mean_eps: 0.365863
 107190/150000: episode: 855, duration: 6.866s, episode steps: 1000, steps per second: 146, episode reward: 120.885, mean reward:  0.121 [-22.938, 23.576], mean action: 1.518 [0.000, 3.000],  loss: 8.262714, mae: 26.795856, mean_q: 24.750256, mean_eps: 0.359863
 108190/150000: episode: 856, duration: 6.726s, episode steps: 1000, steps per second: 149, episode reward: 23.613, mean reward:  0.024 [-24.240, 38.175], mean action: 1.848 [0.000, 3.000],  loss: 8.177718, mae: 26.475994, mean_q: 25.262614, mean_eps: 0.353863
 109190/150000: episode: 857, duration: 6.822s, episode steps: 1000, steps per second: 147, episode reward: 110.478, mean reward:  0.110 [-19.960, 24.023], mean action: 1.243 [0.000, 3.000],  loss: 8.981805, mae: 26.078491, mean_q: 25.418796, mean_eps: 0.347863
 109499/150000: episode: 858, duration: 1.945s, episode steps: 309, steps per second: 159, episode reward:  0.726, mean reward:  0.002 [-100.000, 19.546], mean action: 1.612 [0.000, 3.000],  loss: 9.082887, mae: 25.534484, mean_q: 26.147540, mean_eps: 0.343936
 110499/150000: episode: 859, duration: 7.220s, episode steps: 1000, steps per second: 138, episode reward: 49.006, mean reward:  0.049 [-19.240, 27.420], mean action: 1.750 [0.000, 3.000],  loss: 6.746586, mae: 25.523227, mean_q: 26.018130, mean_eps: 0.340009
 110610/150000: episode: 860, duration: 0.694s, episode steps: 111, steps per second: 160, episode reward: -22.116, mean reward: -0.199 [-100.000, 14.683], mean action: 1.486 [0.000, 3.000],  loss: 11.432699, mae: 25.906537, mean_q: 27.250680, mean_eps: 0.336676
 110900/150000: episode: 861, duration: 1.825s, episode steps: 290, steps per second: 159, episode reward: -189.296, mean reward: -0.653 [-100.000, 16.750], mean action: 1.555 [0.000, 3.000],  loss: 11.103707, mae: 25.685089, mean_q: 26.915405, mean_eps: 0.335473
 111900/150000: episode: 862, duration: 7.152s, episode steps: 1000, steps per second: 140, episode reward: 59.754, mean reward:  0.060 [-19.145, 16.682], mean action: 1.625 [0.000, 3.000],  loss: 10.972293, mae: 25.259309, mean_q: 27.296034, mean_eps: 0.331603
 112900/150000: episode: 863, duration: 7.082s, episode steps: 1000, steps per second: 141, episode reward: 120.975, mean reward:  0.121 [-20.287, 24.373], mean action: 1.465 [0.000, 3.000],  loss: 9.474750, mae: 25.321947, mean_q: 27.878074, mean_eps: 0.325603
 113169/150000: episode: 864, duration: 1.703s, episode steps: 269, steps per second: 158, episode reward: -218.292, mean reward: -0.811 [-100.000, 19.779], mean action: 1.710 [0.000, 3.000],  loss: 7.379512, mae: 24.414727, mean_q: 26.600358, mean_eps: 0.321796
 113387/150000: episode: 865, duration: 1.351s, episode steps: 218, steps per second: 161, episode reward: -28.708, mean reward: -0.132 [-100.000, 15.159], mean action: 1.647 [0.000, 3.000],  loss: 12.134025, mae: 24.926480, mean_q: 28.054594, mean_eps: 0.320335
 114387/150000: episode: 866, duration: 6.962s, episode steps: 1000, steps per second: 144, episode reward: 132.801, mean reward:  0.133 [-20.353, 23.753], mean action: 2.047 [0.000, 3.000],  loss: 10.468580, mae: 24.674627, mean_q: 27.809189, mean_eps: 0.316681
 115387/150000: episode: 867, duration: 8.102s, episode steps: 1000, steps per second: 123, episode reward: 77.229, mean reward:  0.077 [-23.010, 22.811], mean action: 1.473 [0.000, 3.000],  loss: 8.651478, mae: 24.485923, mean_q: 27.360458, mean_eps: 0.310681
 116387/150000: episode: 868, duration: 7.695s, episode steps: 1000, steps per second: 130, episode reward: 112.625, mean reward:  0.113 [-22.008, 22.491], mean action: 1.361 [0.000, 3.000],  loss: 8.970930, mae: 24.455528, mean_q: 27.906580, mean_eps: 0.304681
 117387/150000: episode: 869, duration: 6.882s, episode steps: 1000, steps per second: 145, episode reward: -22.629, mean reward: -0.023 [-23.384, 23.909], mean action: 1.423 [0.000, 3.000],  loss: 9.341579, mae: 24.565008, mean_q: 28.048339, mean_eps: 0.298681
 118387/150000: episode: 870, duration: 6.600s, episode steps: 1000, steps per second: 152, episode reward: 112.896, mean reward:  0.113 [-20.364, 21.428], mean action: 1.795 [0.000, 3.000],  loss: 8.887399, mae: 24.183582, mean_q: 28.107920, mean_eps: 0.292681
 119387/150000: episode: 871, duration: 7.067s, episode steps: 1000, steps per second: 141, episode reward: 79.823, mean reward:  0.080 [-20.873, 20.968], mean action: 1.331 [0.000, 3.000],  loss: 7.779584, mae: 24.047606, mean_q: 28.321112, mean_eps: 0.286681
 120387/150000: episode: 872, duration: 6.647s, episode steps: 1000, steps per second: 150, episode reward: 140.027, mean reward:  0.140 [-22.713, 24.624], mean action: 1.270 [0.000, 3.000],  loss: 7.016664, mae: 23.420102, mean_q: 28.126384, mean_eps: 0.280681
 121387/150000: episode: 873, duration: 7.597s, episode steps: 1000, steps per second: 132, episode reward: 96.168, mean reward:  0.096 [-21.563, 22.904], mean action: 1.276 [0.000, 3.000],  loss: 8.360909, mae: 23.505908, mean_q: 28.602750, mean_eps: 0.274681
 122387/150000: episode: 874, duration: 6.661s, episode steps: 1000, steps per second: 150, episode reward: 122.024, mean reward:  0.122 [-21.256, 23.763], mean action: 1.049 [0.000, 3.000],  loss: 8.182091, mae: 23.640487, mean_q: 28.578711, mean_eps: 0.268681
 123387/150000: episode: 875, duration: 6.725s, episode steps: 1000, steps per second: 149, episode reward: 136.247, mean reward:  0.136 [-21.376, 22.693], mean action: 1.173 [0.000, 3.000],  loss: 7.708102, mae: 23.491909, mean_q: 28.940591, mean_eps: 0.262681
 123516/150000: episode: 876, duration: 0.826s, episode steps: 129, steps per second: 156, episode reward: -9.153, mean reward: -0.071 [-100.000, 14.735], mean action: 1.713 [0.000, 3.000],  loss: 12.632293, mae: 23.771835, mean_q: 29.297572, mean_eps: 0.259294
 124516/150000: episode: 877, duration: 7.495s, episode steps: 1000, steps per second: 133, episode reward: 107.625, mean reward:  0.108 [-20.882, 22.678], mean action: 1.258 [0.000, 3.000],  loss: 7.962221, mae: 23.529356, mean_q: 29.090680, mean_eps: 0.255907
 125152/150000: episode: 878, duration: 4.100s, episode steps: 636, steps per second: 155, episode reward: 273.657, mean reward:  0.430 [-23.995, 100.000], mean action: 1.186 [0.000, 3.000],  loss: 8.259413, mae: 23.238930, mean_q: 29.145035, mean_eps: 0.250999
 126152/150000: episode: 879, duration: 7.426s, episode steps: 1000, steps per second: 135, episode reward: 104.433, mean reward:  0.104 [-23.792, 22.048], mean action: 1.237 [0.000, 3.000],  loss: 8.193020, mae: 23.030373, mean_q: 28.988819, mean_eps: 0.246091
 127152/150000: episode: 880, duration: 6.963s, episode steps: 1000, steps per second: 144, episode reward: 116.550, mean reward:  0.117 [-20.984, 22.029], mean action: 1.197 [0.000, 3.000],  loss: 5.766455, mae: 22.715171, mean_q: 28.895965, mean_eps: 0.240091
 127445/150000: episode: 881, duration: 1.845s, episode steps: 293, steps per second: 159, episode reward: -124.487, mean reward: -0.425 [-100.000, 14.968], mean action: 1.666 [0.000, 3.000],  loss: 5.499511, mae: 22.807910, mean_q: 29.336700, mean_eps: 0.236212
 128445/150000: episode: 882, duration: 6.807s, episode steps: 1000, steps per second: 147, episode reward: 142.797, mean reward:  0.143 [-21.440, 23.956], mean action: 1.234 [0.000, 3.000],  loss: 5.481129, mae: 22.554256, mean_q: 29.154561, mean_eps: 0.232333
 129445/150000: episode: 883, duration: 7.559s, episode steps: 1000, steps per second: 132, episode reward: 53.757, mean reward:  0.054 [-21.081, 19.403], mean action: 1.665 [0.000, 3.000],  loss: 5.815067, mae: 22.212114, mean_q: 28.823072, mean_eps: 0.226333
 129920/150000: episode: 884, duration: 3.342s, episode steps: 475, steps per second: 142, episode reward: 232.748, mean reward:  0.490 [-20.704, 100.000], mean action: 1.495 [0.000, 3.000],  loss: 5.419607, mae: 22.174061, mean_q: 28.882879, mean_eps: 0.221908
 130547/150000: episode: 885, duration: 4.091s, episode steps: 627, steps per second: 153, episode reward: 241.689, mean reward:  0.385 [-19.578, 100.000], mean action: 1.144 [0.000, 3.000],  loss: 7.375714, mae: 21.985601, mean_q: 28.745488, mean_eps: 0.218602
 131547/150000: episode: 886, duration: 6.942s, episode steps: 1000, steps per second: 144, episode reward: 103.838, mean reward:  0.104 [-21.436, 22.610], mean action: 1.368 [0.000, 3.000],  loss: 6.401263, mae: 22.242452, mean_q: 29.109862, mean_eps: 0.213721
 132547/150000: episode: 887, duration: 7.443s, episode steps: 1000, steps per second: 134, episode reward: 81.828, mean reward:  0.082 [-25.036, 23.242], mean action: 1.297 [0.000, 3.000],  loss: 5.593329, mae: 22.491111, mean_q: 29.756757, mean_eps: 0.207721
 133531/150000: episode: 888, duration: 7.146s, episode steps: 984, steps per second: 138, episode reward: 193.748, mean reward:  0.197 [-18.014, 100.000], mean action: 1.480 [0.000, 3.000],  loss: 5.712031, mae: 22.500387, mean_q: 29.721267, mean_eps: 0.201769
 134531/150000: episode: 889, duration: 6.922s, episode steps: 1000, steps per second: 144, episode reward: 100.229, mean reward:  0.100 [-18.003, 14.153], mean action: 1.138 [0.000, 3.000],  loss: 5.526620, mae: 22.637137, mean_q: 29.960025, mean_eps: 0.195817
 135257/150000: episode: 890, duration: 4.850s, episode steps: 726, steps per second: 150, episode reward: 214.849, mean reward:  0.296 [-20.085, 100.000], mean action: 1.241 [0.000, 3.000],  loss: 5.309659, mae: 22.824990, mean_q: 30.334610, mean_eps: 0.190639
 136257/150000: episode: 891, duration: 6.986s, episode steps: 1000, steps per second: 143, episode reward: 60.166, mean reward:  0.060 [-19.692, 23.392], mean action: 1.331 [0.000, 3.000],  loss: 3.867114, mae: 23.060205, mean_q: 30.687106, mean_eps: 0.185461
 136888/150000: episode: 892, duration: 4.196s, episode steps: 631, steps per second: 150, episode reward: 227.178, mean reward:  0.360 [-18.590, 100.000], mean action: 1.189 [0.000, 3.000],  loss: 4.285237, mae: 23.325214, mean_q: 31.115193, mean_eps: 0.180568
 137459/150000: episode: 893, duration: 3.767s, episode steps: 571, steps per second: 152, episode reward: 236.685, mean reward:  0.415 [-19.045, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 5.454739, mae: 22.838193, mean_q: 30.507906, mean_eps: 0.176962
 138365/150000: episode: 894, duration: 6.183s, episode steps: 906, steps per second: 147, episode reward: 107.914, mean reward:  0.119 [-17.641, 100.000], mean action: 1.643 [0.000, 3.000],  loss: 5.834666, mae: 22.950591, mean_q: 30.696801, mean_eps: 0.172531
 139282/150000: episode: 895, duration: 6.815s, episode steps: 917, steps per second: 135, episode reward: 221.923, mean reward:  0.242 [-22.306, 100.000], mean action: 1.043 [0.000, 3.000],  loss: 5.191397, mae: 23.191973, mean_q: 30.947996, mean_eps: 0.167062
 140282/150000: episode: 896, duration: 7.654s, episode steps: 1000, steps per second: 131, episode reward: 56.930, mean reward:  0.057 [-20.056, 22.969], mean action: 1.046 [0.000, 3.000],  loss: 5.496192, mae: 23.407605, mean_q: 31.387146, mean_eps: 0.161311
 140716/150000: episode: 897, duration: 3.096s, episode steps: 434, steps per second: 140, episode reward: 218.801, mean reward:  0.504 [-17.728, 100.000], mean action: 1.472 [0.000, 3.000],  loss: 5.084499, mae: 23.653904, mean_q: 31.880741, mean_eps: 0.157009
 141716/150000: episode: 898, duration: 7.510s, episode steps: 1000, steps per second: 133, episode reward: 133.962, mean reward:  0.134 [-19.394, 13.798], mean action: 0.827 [0.000, 3.000],  loss: 4.054502, mae: 23.387258, mean_q: 31.395704, mean_eps: 0.152707
 142716/150000: episode: 899, duration: 7.765s, episode steps: 1000, steps per second: 129, episode reward: 115.814, mean reward:  0.116 [-20.508, 23.503], mean action: 1.585 [0.000, 3.000],  loss: 5.079377, mae: 23.714338, mean_q: 31.787728, mean_eps: 0.146707
 143594/150000: episode: 900, duration: 6.301s, episode steps: 878, steps per second: 139, episode reward: 182.468, mean reward:  0.208 [-18.036, 100.000], mean action: 0.944 [0.000, 3.000],  loss: 3.670784, mae: 23.860523, mean_q: 32.170701, mean_eps: 0.141073
 144045/150000: episode: 901, duration: 2.973s, episode steps: 451, steps per second: 152, episode reward: 248.125, mean reward:  0.550 [-8.399, 100.000], mean action: 1.534 [0.000, 3.000],  loss: 5.279875, mae: 23.869749, mean_q: 32.042187, mean_eps: 0.137086
 145045/150000: episode: 902, duration: 6.714s, episode steps: 1000, steps per second: 149, episode reward: 146.470, mean reward:  0.146 [-18.552, 24.131], mean action: 1.370 [0.000, 3.000],  loss: 3.194778, mae: 24.002484, mean_q: 32.290904, mean_eps: 0.132733
 145644/150000: episode: 903, duration: 4.156s, episode steps: 599, steps per second: 144, episode reward: 225.590, mean reward:  0.377 [-17.986, 100.000], mean action: 0.928 [0.000, 3.000],  loss: 6.135130, mae: 23.975988, mean_q: 32.286257, mean_eps: 0.127936
 146020/150000: episode: 904, duration: 2.641s, episode steps: 376, steps per second: 142, episode reward: 217.741, mean reward:  0.579 [-18.339, 100.000], mean action: 1.234 [0.000, 3.000],  loss: 6.265956, mae: 23.803584, mean_q: 32.038032, mean_eps: 0.125011
 147020/150000: episode: 905, duration: 7.562s, episode steps: 1000, steps per second: 132, episode reward: 128.925, mean reward:  0.129 [-20.045, 21.275], mean action: 1.582 [0.000, 3.000],  loss: 4.212740, mae: 23.896995, mean_q: 32.220093, mean_eps: 0.120883
 147367/150000: episode: 906, duration: 2.319s, episode steps: 347, steps per second: 150, episode reward: -18.520, mean reward: -0.053 [-100.000,  8.957], mean action: 1.744 [0.000, 3.000],  loss: 3.057608, mae: 24.382435, mean_q: 32.967450, mean_eps: 0.116842
 148367/150000: episode: 907, duration: 7.544s, episode steps: 1000, steps per second: 133, episode reward: 29.151, mean reward:  0.029 [-13.598, 12.686], mean action: 1.476 [0.000, 3.000],  loss: 3.837492, mae: 24.183289, mean_q: 32.484752, mean_eps: 0.112801
 149367/150000: episode: 908, duration: 8.198s, episode steps: 1000, steps per second: 122, episode reward: -23.186, mean reward: -0.023 [-4.996,  5.044], mean action: 1.509 [0.000, 3.000],  loss: 4.062860, mae: 24.347159, mean_q: 32.728579, mean_eps: 0.106801
 149789/150000: episode: 909, duration: 4.155s, episode steps: 422, steps per second: 102, episode reward: 272.942, mean reward:  0.647 [-18.588, 100.000], mean action: 1.159 [0.000, 3.000],  loss: 4.254482, mae: 24.279676, mean_q: 32.764871, mean_eps: 0.102535
done, took 1003.174 seconds
Testing for 5 episodes ...
Episode 1: reward: 260.566, steps: 448
Episode 2: reward: -52.583, steps: 1000
Episode 3: reward: -42.633, steps: 1000
Episode 4: reward: 160.669, steps: 610
Episode 5: reward: 253.002, steps: 290
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 150000 steps ...
    102/150000: episode: 1, duration: 0.172s, episode steps: 102, steps per second: 592, episode reward: -171.186, mean reward: -1.678 [-100.000,  2.526], mean action: 1.608 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    206/150000: episode: 2, duration: 0.110s, episode steps: 104, steps per second: 947, episode reward: -194.217, mean reward: -1.867 [-100.000, 24.797], mean action: 1.567 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    273/150000: episode: 3, duration: 0.051s, episode steps:  67, steps per second: 1325, episode reward: -103.243, mean reward: -1.541 [-100.000,  6.194], mean action: 1.388 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    352/150000: episode: 4, duration: 0.070s, episode steps:  79, steps per second: 1123, episode reward: -193.305, mean reward: -2.447 [-100.000,  6.495], mean action: 1.544 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    458/150000: episode: 5, duration: 0.123s, episode steps: 106, steps per second: 864, episode reward: -288.991, mean reward: -2.726 [-100.000,  5.711], mean action: 1.472 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    571/150000: episode: 6, duration: 0.101s, episode steps: 113, steps per second: 1123, episode reward: -44.062, mean reward: -0.390 [-100.000, 22.042], mean action: 1.513 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    666/150000: episode: 7, duration: 0.087s, episode steps:  95, steps per second: 1098, episode reward: -256.308, mean reward: -2.698 [-100.000,  0.474], mean action: 1.547 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    755/150000: episode: 8, duration: 0.093s, episode steps:  89, steps per second: 953, episode reward: -136.593, mean reward: -1.535 [-100.000, 18.915], mean action: 1.652 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    822/150000: episode: 9, duration: 0.050s, episode steps:  67, steps per second: 1328, episode reward: -202.064, mean reward: -3.016 [-100.000,  3.009], mean action: 1.582 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    883/150000: episode: 10, duration: 0.051s, episode steps:  61, steps per second: 1193, episode reward: -124.459, mean reward: -2.040 [-100.000,  9.537], mean action: 1.590 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1006/150000: episode: 11, duration: 0.681s, episode steps: 123, steps per second: 181, episode reward: -286.118, mean reward: -2.326 [-100.000,  9.077], mean action: 1.398 [0.000, 3.000],  loss: 2.414910, mae: 24.636517, mean_q: 33.133535, mean_eps: 0.993982
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
   1068/150000: episode: 12, duration: 0.442s, episode steps:  62, steps per second: 140, episode reward: -203.889, mean reward: -3.289 [-100.000,  5.923], mean action: 1.468 [0.000, 3.000],  loss: 5.867843, mae: 25.161465, mean_q: 33.645988, mean_eps: 0.993781
   1142/150000: episode: 13, duration: 0.524s, episode steps:  74, steps per second: 141, episode reward: -218.322, mean reward: -2.950 [-100.000,  5.358], mean action: 1.338 [0.000, 3.000],  loss: 7.552654, mae: 25.136262, mean_q: 33.410381, mean_eps: 0.993373
   1224/150000: episode: 14, duration: 0.560s, episode steps:  82, steps per second: 146, episode reward: -95.510, mean reward: -1.165 [-100.000,  7.522], mean action: 1.415 [0.000, 3.000],  loss: 3.140328, mae: 24.774181, mean_q: 33.023198, mean_eps: 0.992905
   1303/150000: episode: 15, duration: 0.529s, episode steps:  79, steps per second: 149, episode reward: -197.351, mean reward: -2.498 [-100.000,  6.266], mean action: 1.380 [0.000, 3.000],  loss: 3.784504, mae: 25.230965, mean_q: 33.493719, mean_eps: 0.992422
   1379/150000: episode: 16, duration: 0.524s, episode steps:  76, steps per second: 145, episode reward: -272.361, mean reward: -3.584 [-100.000,  5.569], mean action: 1.671 [0.000, 3.000],  loss: 10.844174, mae: 25.414132, mean_q: 33.490724, mean_eps: 0.991957
   1451/150000: episode: 17, duration: 0.450s, episode steps:  72, steps per second: 160, episode reward: -135.637, mean reward: -1.884 [-100.000,  8.635], mean action: 1.611 [0.000, 3.000],  loss: 8.410356, mae: 24.422486, mean_q: 32.091397, mean_eps: 0.991513
   1523/150000: episode: 18, duration: 0.565s, episode steps:  72, steps per second: 127, episode reward: -109.197, mean reward: -1.517 [-100.000,  8.206], mean action: 1.486 [0.000, 3.000],  loss: 9.835215, mae: 25.036107, mean_q: 33.123527, mean_eps: 0.991081
   1602/150000: episode: 19, duration: 0.507s, episode steps:  79, steps per second: 156, episode reward: -82.269, mean reward: -1.041 [-100.000,  7.879], mean action: 1.608 [0.000, 3.000],  loss: 5.826380, mae: 24.779306, mean_q: 32.812157, mean_eps: 0.990628
   1669/150000: episode: 20, duration: 0.421s, episode steps:  67, steps per second: 159, episode reward: -81.509, mean reward: -1.217 [-100.000,  7.599], mean action: 1.552 [0.000, 3.000],  loss: 5.193328, mae: 25.475329, mean_q: 33.711613, mean_eps: 0.990190
   1801/150000: episode: 21, duration: 0.893s, episode steps: 132, steps per second: 148, episode reward: -34.196, mean reward: -0.259 [-100.000, 17.154], mean action: 1.485 [0.000, 3.000],  loss: 8.665460, mae: 24.991706, mean_q: 33.117673, mean_eps: 0.989593
   1920/150000: episode: 22, duration: 0.765s, episode steps: 119, steps per second: 155, episode reward: -143.141, mean reward: -1.203 [-100.000,  9.383], mean action: 1.345 [0.000, 3.000],  loss: 5.796862, mae: 24.856605, mean_q: 33.133916, mean_eps: 0.988840
   1998/150000: episode: 23, duration: 0.591s, episode steps:  78, steps per second: 132, episode reward: -97.131, mean reward: -1.245 [-100.000, 95.426], mean action: 1.410 [0.000, 3.000],  loss: 3.468747, mae: 24.663532, mean_q: 32.288945, mean_eps: 0.988249
   2132/150000: episode: 24, duration: 0.864s, episode steps: 134, steps per second: 155, episode reward: -368.691, mean reward: -2.751 [-100.000, 81.935], mean action: 1.634 [0.000, 3.000],  loss: 4.948598, mae: 25.669759, mean_q: 33.702226, mean_eps: 0.987613
   2190/150000: episode: 25, duration: 0.372s, episode steps:  58, steps per second: 156, episode reward: -117.736, mean reward: -2.030 [-100.000,  7.648], mean action: 1.621 [0.000, 3.000],  loss: 5.536391, mae: 24.777991, mean_q: 32.636155, mean_eps: 0.987037
   2286/150000: episode: 26, duration: 0.611s, episode steps:  96, steps per second: 157, episode reward: -237.175, mean reward: -2.471 [-100.000,  9.058], mean action: 1.500 [0.000, 3.000],  loss: 7.650402, mae: 24.742257, mean_q: 32.391490, mean_eps: 0.986575
   2369/150000: episode: 27, duration: 0.519s, episode steps:  83, steps per second: 160, episode reward: -104.515, mean reward: -1.259 [-100.000, 18.391], mean action: 1.590 [0.000, 3.000],  loss: 11.676277, mae: 26.146718, mean_q: 34.193475, mean_eps: 0.986038
   2468/150000: episode: 28, duration: 0.632s, episode steps:  99, steps per second: 157, episode reward: -107.498, mean reward: -1.086 [-100.000, 16.799], mean action: 1.505 [0.000, 3.000],  loss: 2.796317, mae: 25.631483, mean_q: 33.925903, mean_eps: 0.985492
   2562/150000: episode: 29, duration: 0.584s, episode steps:  94, steps per second: 161, episode reward: -53.534, mean reward: -0.570 [-100.000, 68.584], mean action: 1.543 [0.000, 3.000],  loss: 4.595740, mae: 24.815509, mean_q: 32.615640, mean_eps: 0.984913
   2654/150000: episode: 30, duration: 0.562s, episode steps:  92, steps per second: 164, episode reward: -105.462, mean reward: -1.146 [-100.000, 20.570], mean action: 1.435 [0.000, 3.000],  loss: 8.197771, mae: 25.978281, mean_q: 34.077229, mean_eps: 0.984355
   2770/150000: episode: 31, duration: 0.745s, episode steps: 116, steps per second: 156, episode reward: -109.042, mean reward: -0.940 [-100.000,  6.610], mean action: 1.491 [0.000, 3.000],  loss: 4.519281, mae: 25.401004, mean_q: 32.810120, mean_eps: 0.983731
   2862/150000: episode: 32, duration: 0.662s, episode steps:  92, steps per second: 139, episode reward: -97.133, mean reward: -1.056 [-100.000, 14.115], mean action: 1.413 [0.000, 3.000],  loss: 3.228482, mae: 25.615557, mean_q: 33.339578, mean_eps: 0.983107
   2941/150000: episode: 33, duration: 0.527s, episode steps:  79, steps per second: 150, episode reward: -122.607, mean reward: -1.552 [-100.000,  9.018], mean action: 1.316 [0.000, 3.000],  loss: 4.119328, mae: 25.945672, mean_q: 33.861439, mean_eps: 0.982594
   3014/150000: episode: 34, duration: 0.463s, episode steps:  73, steps per second: 158, episode reward: -103.937, mean reward: -1.424 [-100.000, 59.934], mean action: 1.671 [0.000, 3.000],  loss: 7.303019, mae: 25.419841, mean_q: 33.589987, mean_eps: 0.982138
   3117/150000: episode: 35, duration: 0.722s, episode steps: 103, steps per second: 143, episode reward: -369.090, mean reward: -3.583 [-100.000,  1.072], mean action: 1.650 [0.000, 3.000],  loss: 2.272179, mae: 26.140168, mean_q: 34.315798, mean_eps: 0.981610
   3197/150000: episode: 36, duration: 0.563s, episode steps:  80, steps per second: 142, episode reward: -142.696, mean reward: -1.784 [-100.000, 12.692], mean action: 1.587 [0.000, 3.000],  loss: 4.512221, mae: 25.900592, mean_q: 33.701350, mean_eps: 0.981061
   3264/150000: episode: 37, duration: 0.420s, episode steps:  67, steps per second: 160, episode reward: -88.608, mean reward: -1.323 [-100.000, 17.345], mean action: 1.478 [0.000, 3.000],  loss: 3.925979, mae: 26.043597, mean_q: 34.124649, mean_eps: 0.980620
   3330/150000: episode: 38, duration: 0.406s, episode steps:  66, steps per second: 163, episode reward: -124.969, mean reward: -1.893 [-100.000, 35.522], mean action: 1.545 [0.000, 3.000],  loss: 4.344746, mae: 26.104652, mean_q: 34.804711, mean_eps: 0.980221
   3397/150000: episode: 39, duration: 0.415s, episode steps:  67, steps per second: 161, episode reward: -91.864, mean reward: -1.371 [-100.000, 14.035], mean action: 1.269 [0.000, 3.000],  loss: 9.399253, mae: 26.630633, mean_q: 34.665478, mean_eps: 0.979822
   3513/150000: episode: 40, duration: 0.747s, episode steps: 116, steps per second: 155, episode reward: -109.314, mean reward: -0.942 [-100.000, 16.053], mean action: 1.526 [0.000, 3.000],  loss: 8.407184, mae: 26.311509, mean_q: 34.311628, mean_eps: 0.979273
   3624/150000: episode: 41, duration: 0.680s, episode steps: 111, steps per second: 163, episode reward: -245.464, mean reward: -2.211 [-100.000,  8.271], mean action: 1.532 [0.000, 3.000],  loss: 4.368985, mae: 25.348884, mean_q: 33.355624, mean_eps: 0.978592
   3725/150000: episode: 42, duration: 0.621s, episode steps: 101, steps per second: 163, episode reward: -127.865, mean reward: -1.266 [-100.000, 17.967], mean action: 1.624 [0.000, 3.000],  loss: 8.745477, mae: 26.261670, mean_q: 33.928050, mean_eps: 0.977956
   3862/150000: episode: 43, duration: 0.958s, episode steps: 137, steps per second: 143, episode reward: -327.736, mean reward: -2.392 [-100.000,  3.665], mean action: 1.248 [0.000, 3.000],  loss: 8.336607, mae: 26.064875, mean_q: 34.001987, mean_eps: 0.977242
   3979/150000: episode: 44, duration: 0.789s, episode steps: 117, steps per second: 148, episode reward: -128.560, mean reward: -1.099 [-100.000, 12.458], mean action: 1.556 [0.000, 3.000],  loss: 8.141172, mae: 26.251044, mean_q: 34.348694, mean_eps: 0.976480
   4087/150000: episode: 45, duration: 0.747s, episode steps: 108, steps per second: 144, episode reward: -177.058, mean reward: -1.639 [-100.000, 34.627], mean action: 1.593 [0.000, 3.000],  loss: 6.234106, mae: 26.077530, mean_q: 34.252619, mean_eps: 0.975805
   4171/150000: episode: 46, duration: 0.623s, episode steps:  84, steps per second: 135, episode reward: -511.307, mean reward: -6.087 [-100.000, 87.699], mean action: 1.655 [0.000, 3.000],  loss: 10.148557, mae: 26.529005, mean_q: 33.861087, mean_eps: 0.975229
   4230/150000: episode: 47, duration: 0.411s, episode steps:  59, steps per second: 143, episode reward: -102.981, mean reward: -1.745 [-100.000,  6.486], mean action: 1.254 [0.000, 3.000],  loss: 7.215485, mae: 26.330632, mean_q: 33.902661, mean_eps: 0.974800
   4323/150000: episode: 48, duration: 0.633s, episode steps:  93, steps per second: 147, episode reward: -72.791, mean reward: -0.783 [-100.000, 59.608], mean action: 1.527 [0.000, 3.000],  loss: 4.616512, mae: 25.563552, mean_q: 33.430984, mean_eps: 0.974344
   4388/150000: episode: 49, duration: 0.462s, episode steps:  65, steps per second: 141, episode reward: -85.531, mean reward: -1.316 [-100.000,  7.682], mean action: 1.677 [0.000, 3.000],  loss: 7.969471, mae: 25.866181, mean_q: 33.553664, mean_eps: 0.973870
   4464/150000: episode: 50, duration: 0.509s, episode steps:  76, steps per second: 149, episode reward: -186.287, mean reward: -2.451 [-100.000, 12.351], mean action: 1.329 [0.000, 3.000],  loss: 11.656667, mae: 26.133272, mean_q: 33.767546, mean_eps: 0.973447
   4619/150000: episode: 51, duration: 1.098s, episode steps: 155, steps per second: 141, episode reward: 36.622, mean reward:  0.236 [-100.000, 70.560], mean action: 1.594 [0.000, 3.000],  loss: 10.044569, mae: 26.168431, mean_q: 34.021902, mean_eps: 0.972754
   4716/150000: episode: 52, duration: 0.671s, episode steps:  97, steps per second: 145, episode reward: -126.969, mean reward: -1.309 [-100.000, 13.532], mean action: 1.546 [0.000, 3.000],  loss: 7.093652, mae: 25.298234, mean_q: 32.955874, mean_eps: 0.971998
   4832/150000: episode: 53, duration: 0.812s, episode steps: 116, steps per second: 143, episode reward: -97.029, mean reward: -0.836 [-100.000, 25.700], mean action: 1.578 [0.000, 3.000],  loss: 7.380909, mae: 26.526853, mean_q: 34.490256, mean_eps: 0.971359
   4940/150000: episode: 54, duration: 0.694s, episode steps: 108, steps per second: 156, episode reward: -209.789, mean reward: -1.942 [-100.000,  1.400], mean action: 1.333 [0.000, 3.000],  loss: 17.534744, mae: 26.164029, mean_q: 33.666118, mean_eps: 0.970687
   5026/150000: episode: 55, duration: 0.633s, episode steps:  86, steps per second: 136, episode reward: -87.324, mean reward: -1.015 [-100.000, 13.946], mean action: 1.686 [0.000, 3.000],  loss: 25.701909, mae: 26.485707, mean_q: 34.101747, mean_eps: 0.970105
   5122/150000: episode: 56, duration: 0.651s, episode steps:  96, steps per second: 147, episode reward: -166.736, mean reward: -1.737 [-100.000,  8.966], mean action: 1.521 [0.000, 3.000],  loss: 5.375978, mae: 26.377584, mean_q: 34.368709, mean_eps: 0.969559
   5194/150000: episode: 57, duration: 0.479s, episode steps:  72, steps per second: 150, episode reward: -86.145, mean reward: -1.196 [-100.000, 12.480], mean action: 1.389 [0.000, 3.000],  loss: 4.779566, mae: 27.378280, mean_q: 35.104877, mean_eps: 0.969055
   5284/150000: episode: 58, duration: 0.584s, episode steps:  90, steps per second: 154, episode reward: -284.211, mean reward: -3.158 [-100.000, 42.514], mean action: 1.456 [0.000, 3.000],  loss: 5.810326, mae: 26.286231, mean_q: 33.889105, mean_eps: 0.968569
   5419/150000: episode: 59, duration: 0.898s, episode steps: 135, steps per second: 150, episode reward: -109.214, mean reward: -0.809 [-100.000, 10.133], mean action: 1.459 [0.000, 3.000],  loss: 5.621509, mae: 26.179215, mean_q: 33.976529, mean_eps: 0.967894
   5507/150000: episode: 60, duration: 0.554s, episode steps:  88, steps per second: 159, episode reward: -382.145, mean reward: -4.343 [-100.000,  0.105], mean action: 1.511 [0.000, 3.000],  loss: 9.017177, mae: 26.295525, mean_q: 34.505346, mean_eps: 0.967225
   5614/150000: episode: 61, duration: 0.684s, episode steps: 107, steps per second: 157, episode reward: -99.878, mean reward: -0.933 [-100.000,  6.488], mean action: 1.570 [0.000, 3.000],  loss: 9.999217, mae: 26.409734, mean_q: 34.106290, mean_eps: 0.966640
   5691/150000: episode: 62, duration: 0.480s, episode steps:  77, steps per second: 160, episode reward: -252.134, mean reward: -3.274 [-100.000, 52.781], mean action: 1.519 [0.000, 3.000],  loss: 2.144770, mae: 26.572710, mean_q: 34.710338, mean_eps: 0.966088
   5771/150000: episode: 63, duration: 0.532s, episode steps:  80, steps per second: 150, episode reward: -74.772, mean reward: -0.935 [-100.000,  8.453], mean action: 1.462 [0.000, 3.000],  loss: 5.696293, mae: 26.380626, mean_q: 33.810978, mean_eps: 0.965617
   5838/150000: episode: 64, duration: 0.421s, episode steps:  67, steps per second: 159, episode reward: -155.433, mean reward: -2.320 [-100.000, 31.024], mean action: 1.463 [0.000, 3.000],  loss: 6.447625, mae: 26.702180, mean_q: 33.967830, mean_eps: 0.965176
   5915/150000: episode: 65, duration: 0.495s, episode steps:  77, steps per second: 156, episode reward: -117.304, mean reward: -1.523 [-100.000, 11.142], mean action: 1.545 [0.000, 3.000],  loss: 11.075851, mae: 26.889302, mean_q: 35.281743, mean_eps: 0.964744
   6006/150000: episode: 66, duration: 0.562s, episode steps:  91, steps per second: 162, episode reward: -138.079, mean reward: -1.517 [-100.000,  5.655], mean action: 1.374 [0.000, 3.000],  loss: 6.795541, mae: 26.352166, mean_q: 34.611711, mean_eps: 0.964240
   6075/150000: episode: 67, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -89.273, mean reward: -1.294 [-100.000, 11.982], mean action: 1.246 [0.000, 3.000],  loss: 10.548987, mae: 27.601336, mean_q: 35.769528, mean_eps: 0.963760
   6177/150000: episode: 68, duration: 0.626s, episode steps: 102, steps per second: 163, episode reward: -80.009, mean reward: -0.784 [-100.000, 18.072], mean action: 1.422 [0.000, 3.000],  loss: 4.030475, mae: 27.939417, mean_q: 35.728573, mean_eps: 0.963247
   6243/150000: episode: 69, duration: 0.430s, episode steps:  66, steps per second: 154, episode reward: -96.215, mean reward: -1.458 [-100.000,  6.130], mean action: 1.530 [0.000, 3.000],  loss: 3.127267, mae: 27.378964, mean_q: 35.445718, mean_eps: 0.962743
   6306/150000: episode: 70, duration: 0.424s, episode steps:  63, steps per second: 149, episode reward: -85.582, mean reward: -1.358 [-100.000, 15.515], mean action: 1.762 [0.000, 3.000],  loss: 7.797511, mae: 28.092584, mean_q: 35.065499, mean_eps: 0.962356
   6415/150000: episode: 71, duration: 0.705s, episode steps: 109, steps per second: 155, episode reward: -90.298, mean reward: -0.828 [-100.000,  7.480], mean action: 1.321 [0.000, 3.000],  loss: 7.129225, mae: 27.268564, mean_q: 35.479866, mean_eps: 0.961840
   6484/150000: episode: 72, duration: 0.433s, episode steps:  69, steps per second: 160, episode reward: -102.618, mean reward: -1.487 [-100.000, 22.361], mean action: 1.522 [0.000, 3.000],  loss: 7.274411, mae: 27.568491, mean_q: 35.567782, mean_eps: 0.961306
   6542/150000: episode: 73, duration: 0.370s, episode steps:  58, steps per second: 157, episode reward: -85.393, mean reward: -1.472 [-100.000, 10.640], mean action: 1.431 [0.000, 3.000],  loss: 7.554237, mae: 26.737671, mean_q: 34.848041, mean_eps: 0.960925
   6638/150000: episode: 74, duration: 0.600s, episode steps:  96, steps per second: 160, episode reward: -115.855, mean reward: -1.207 [-100.000,  6.048], mean action: 1.521 [0.000, 3.000],  loss: 12.324278, mae: 27.829938, mean_q: 35.482089, mean_eps: 0.960463
   6735/150000: episode: 75, duration: 0.606s, episode steps:  97, steps per second: 160, episode reward: -137.787, mean reward: -1.420 [-100.000,  6.592], mean action: 1.701 [0.000, 3.000],  loss: 18.749439, mae: 27.474130, mean_q: 35.474370, mean_eps: 0.959884
   6868/150000: episode: 76, duration: 0.836s, episode steps: 133, steps per second: 159, episode reward: -87.469, mean reward: -0.658 [-100.000, 15.625], mean action: 1.617 [0.000, 3.000],  loss: 8.040918, mae: 27.626737, mean_q: 35.412551, mean_eps: 0.959194
   6959/150000: episode: 77, duration: 0.583s, episode steps:  91, steps per second: 156, episode reward: -275.880, mean reward: -3.032 [-100.000,  6.380], mean action: 1.407 [0.000, 3.000],  loss: 8.261640, mae: 27.817446, mean_q: 35.695005, mean_eps: 0.958522
   7032/150000: episode: 78, duration: 0.480s, episode steps:  73, steps per second: 152, episode reward: -331.803, mean reward: -4.545 [-100.000,  3.197], mean action: 1.247 [0.000, 3.000],  loss: 4.220219, mae: 27.654385, mean_q: 35.728374, mean_eps: 0.958030
   7178/150000: episode: 79, duration: 0.965s, episode steps: 146, steps per second: 151, episode reward: -162.988, mean reward: -1.116 [-100.000,  6.469], mean action: 1.507 [0.000, 3.000],  loss: 5.643392, mae: 27.619175, mean_q: 35.158218, mean_eps: 0.957373
   7295/150000: episode: 80, duration: 0.740s, episode steps: 117, steps per second: 158, episode reward: -199.869, mean reward: -1.708 [-100.000, 39.384], mean action: 1.624 [0.000, 3.000],  loss: 8.106837, mae: 28.509007, mean_q: 36.024615, mean_eps: 0.956584
   7360/150000: episode: 81, duration: 0.431s, episode steps:  65, steps per second: 151, episode reward: -126.180, mean reward: -1.941 [-100.000,  5.999], mean action: 1.262 [0.000, 3.000],  loss: 6.307039, mae: 27.225537, mean_q: 34.395934, mean_eps: 0.956038
   7450/150000: episode: 82, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -309.759, mean reward: -3.442 [-100.000,  5.130], mean action: 1.356 [0.000, 3.000],  loss: 7.244347, mae: 27.548865, mean_q: 35.297002, mean_eps: 0.955573
   7575/150000: episode: 83, duration: 0.876s, episode steps: 125, steps per second: 143, episode reward: -154.555, mean reward: -1.236 [-100.000, 15.323], mean action: 1.528 [0.000, 3.000],  loss: 9.669627, mae: 28.014392, mean_q: 35.795148, mean_eps: 0.954928
   7647/150000: episode: 84, duration: 0.503s, episode steps:  72, steps per second: 143, episode reward: -125.037, mean reward: -1.737 [-100.000,  7.669], mean action: 1.347 [0.000, 3.000],  loss: 15.731593, mae: 28.853196, mean_q: 36.516795, mean_eps: 0.954337
   7769/150000: episode: 85, duration: 0.974s, episode steps: 122, steps per second: 125, episode reward: -307.110, mean reward: -2.517 [-100.000,  5.992], mean action: 1.738 [0.000, 3.000],  loss: 10.750398, mae: 28.234405, mean_q: 35.866632, mean_eps: 0.953755
   7835/150000: episode: 86, duration: 0.483s, episode steps:  66, steps per second: 137, episode reward: -174.574, mean reward: -2.645 [-100.000,  5.747], mean action: 1.318 [0.000, 3.000],  loss: 6.444045, mae: 28.004199, mean_q: 35.688844, mean_eps: 0.953191
   7937/150000: episode: 87, duration: 0.754s, episode steps: 102, steps per second: 135, episode reward: -386.643, mean reward: -3.791 [-100.000,  1.295], mean action: 1.745 [0.000, 3.000],  loss: 12.595702, mae: 27.999756, mean_q: 35.661096, mean_eps: 0.952687
   8056/150000: episode: 88, duration: 1.025s, episode steps: 119, steps per second: 116, episode reward: -309.509, mean reward: -2.601 [-100.000,  6.527], mean action: 1.647 [0.000, 3.000],  loss: 9.004316, mae: 28.581305, mean_q: 36.153042, mean_eps: 0.952024
   8178/150000: episode: 89, duration: 0.909s, episode steps: 122, steps per second: 134, episode reward: -89.005, mean reward: -0.730 [-100.000, 13.813], mean action: 1.434 [0.000, 3.000],  loss: 5.320286, mae: 27.956851, mean_q: 36.010760, mean_eps: 0.951301
   8293/150000: episode: 90, duration: 0.872s, episode steps: 115, steps per second: 132, episode reward: -196.521, mean reward: -1.709 [-100.000, 22.603], mean action: 1.383 [0.000, 3.000],  loss: 6.443011, mae: 28.656385, mean_q: 36.364606, mean_eps: 0.950590
   8390/150000: episode: 91, duration: 0.667s, episode steps:  97, steps per second: 145, episode reward: -157.052, mean reward: -1.619 [-100.000,  5.549], mean action: 1.402 [0.000, 3.000],  loss: 10.996665, mae: 28.977978, mean_q: 37.560543, mean_eps: 0.949954
   8472/150000: episode: 92, duration: 0.524s, episode steps:  82, steps per second: 157, episode reward: -50.906, mean reward: -0.621 [-100.000, 12.171], mean action: 1.646 [0.000, 3.000],  loss: 4.807256, mae: 28.047312, mean_q: 36.328491, mean_eps: 0.949417
   8602/150000: episode: 93, duration: 0.842s, episode steps: 130, steps per second: 154, episode reward: -99.034, mean reward: -0.762 [-100.000, 17.131], mean action: 1.523 [0.000, 3.000],  loss: 5.921017, mae: 28.354639, mean_q: 35.980451, mean_eps: 0.948781
   8664/150000: episode: 94, duration: 0.405s, episode steps:  62, steps per second: 153, episode reward: -113.437, mean reward: -1.830 [-100.000,  5.676], mean action: 1.597 [0.000, 3.000],  loss: 15.229974, mae: 28.971905, mean_q: 37.203802, mean_eps: 0.948205
   8739/150000: episode: 95, duration: 0.477s, episode steps:  75, steps per second: 157, episode reward: -84.081, mean reward: -1.121 [-100.000, 10.009], mean action: 1.320 [0.000, 3.000],  loss: 7.091566, mae: 29.416407, mean_q: 37.645200, mean_eps: 0.947794
   8843/150000: episode: 96, duration: 0.645s, episode steps: 104, steps per second: 161, episode reward: -288.815, mean reward: -2.777 [-100.000,  3.561], mean action: 1.644 [0.000, 3.000],  loss: 8.601281, mae: 28.563720, mean_q: 37.053393, mean_eps: 0.947257
   8919/150000: episode: 97, duration: 0.500s, episode steps:  76, steps per second: 152, episode reward: -142.299, mean reward: -1.872 [-100.000,  4.937], mean action: 1.855 [0.000, 3.000],  loss: 7.071693, mae: 28.899308, mean_q: 36.692369, mean_eps: 0.946717
   8984/150000: episode: 98, duration: 0.442s, episode steps:  65, steps per second: 147, episode reward: -204.733, mean reward: -3.150 [-100.000, 28.686], mean action: 1.554 [0.000, 3.000],  loss: 8.596756, mae: 28.600536, mean_q: 36.232658, mean_eps: 0.946294
   9056/150000: episode: 99, duration: 0.457s, episode steps:  72, steps per second: 158, episode reward: -127.368, mean reward: -1.769 [-100.000, 30.822], mean action: 1.403 [0.000, 3.000],  loss: 6.130893, mae: 28.375354, mean_q: 36.403729, mean_eps: 0.945883
   9146/150000: episode: 100, duration: 0.554s, episode steps:  90, steps per second: 162, episode reward: -164.906, mean reward: -1.832 [-100.000, 16.508], mean action: 1.689 [0.000, 3.000],  loss: 4.028495, mae: 28.899461, mean_q: 36.592337, mean_eps: 0.945397
   9246/150000: episode: 101, duration: 0.616s, episode steps: 100, steps per second: 162, episode reward: -155.518, mean reward: -1.555 [-100.000, 10.624], mean action: 1.590 [0.000, 3.000],  loss: 4.004665, mae: 28.581635, mean_q: 36.510881, mean_eps: 0.944827
   9320/150000: episode: 102, duration: 0.514s, episode steps:  74, steps per second: 144, episode reward: -101.533, mean reward: -1.372 [-100.000,  6.034], mean action: 1.297 [0.000, 3.000],  loss: 7.732549, mae: 28.458932, mean_q: 35.673722, mean_eps: 0.944305
   9403/150000: episode: 103, duration: 0.539s, episode steps:  83, steps per second: 154, episode reward: -318.462, mean reward: -3.837 [-100.000,  0.345], mean action: 1.337 [0.000, 3.000],  loss: 8.528829, mae: 28.045900, mean_q: 35.702260, mean_eps: 0.943834
   9469/150000: episode: 104, duration: 0.411s, episode steps:  66, steps per second: 161, episode reward: -97.746, mean reward: -1.481 [-100.000, 13.814], mean action: 1.712 [0.000, 3.000],  loss: 10.409526, mae: 28.007956, mean_q: 36.150593, mean_eps: 0.943387
   9558/150000: episode: 105, duration: 0.550s, episode steps:  89, steps per second: 162, episode reward: -82.435, mean reward: -0.926 [-100.000, 19.657], mean action: 1.292 [0.000, 3.000],  loss: 5.541976, mae: 27.896500, mean_q: 35.770279, mean_eps: 0.942922
   9634/150000: episode: 106, duration: 0.502s, episode steps:  76, steps per second: 152, episode reward: -165.362, mean reward: -2.176 [-100.000,  8.061], mean action: 1.316 [0.000, 3.000],  loss: 10.145586, mae: 28.533319, mean_q: 36.637855, mean_eps: 0.942427
   9702/150000: episode: 107, duration: 0.458s, episode steps:  68, steps per second: 148, episode reward: -154.705, mean reward: -2.275 [-100.000, 17.601], mean action: 1.338 [0.000, 3.000],  loss: 9.541275, mae: 28.182977, mean_q: 35.914684, mean_eps: 0.941995
   9767/150000: episode: 108, duration: 0.409s, episode steps:  65, steps per second: 159, episode reward: -83.033, mean reward: -1.277 [-100.000,  8.065], mean action: 1.600 [0.000, 3.000],  loss: 4.182864, mae: 28.354351, mean_q: 35.384968, mean_eps: 0.941596
   9850/150000: episode: 109, duration: 0.522s, episode steps:  83, steps per second: 159, episode reward: -120.707, mean reward: -1.454 [-100.000, 40.524], mean action: 1.446 [0.000, 3.000],  loss: 4.255261, mae: 27.992210, mean_q: 36.156298, mean_eps: 0.941152
   9940/150000: episode: 110, duration: 0.578s, episode steps:  90, steps per second: 156, episode reward: -139.888, mean reward: -1.554 [-100.000, 16.410], mean action: 1.467 [0.000, 3.000],  loss: 6.595039, mae: 28.681800, mean_q: 36.326046, mean_eps: 0.940633
  10031/150000: episode: 111, duration: 0.604s, episode steps:  91, steps per second: 151, episode reward: -193.573, mean reward: -2.127 [-100.000, 30.814], mean action: 1.769 [0.000, 3.000],  loss: 4.843323, mae: 28.615989, mean_q: 36.190110, mean_eps: 0.940090
  10135/150000: episode: 112, duration: 0.657s, episode steps: 104, steps per second: 158, episode reward: -107.321, mean reward: -1.032 [-100.000, 14.225], mean action: 1.538 [0.000, 3.000],  loss: 6.480749, mae: 28.358862, mean_q: 35.662557, mean_eps: 0.939505
  10263/150000: episode: 113, duration: 0.813s, episode steps: 128, steps per second: 157, episode reward: -96.912, mean reward: -0.757 [-100.000, 15.908], mean action: 1.500 [0.000, 3.000],  loss: 8.426982, mae: 28.263322, mean_q: 35.458470, mean_eps: 0.938809
  10356/150000: episode: 114, duration: 0.613s, episode steps:  93, steps per second: 152, episode reward: -227.266, mean reward: -2.444 [-100.000, 24.610], mean action: 1.505 [0.000, 3.000],  loss: 12.286952, mae: 27.968063, mean_q: 34.649361, mean_eps: 0.938146
  10482/150000: episode: 115, duration: 0.808s, episode steps: 126, steps per second: 156, episode reward: -105.972, mean reward: -0.841 [-100.000, 21.729], mean action: 1.548 [0.000, 3.000],  loss: 3.653163, mae: 28.696722, mean_q: 36.599960, mean_eps: 0.937489
  10578/150000: episode: 116, duration: 0.590s, episode steps:  96, steps per second: 163, episode reward: -144.609, mean reward: -1.506 [-100.000, 11.147], mean action: 1.302 [0.000, 3.000],  loss: 4.348684, mae: 27.501054, mean_q: 34.334934, mean_eps: 0.936823
  10674/150000: episode: 117, duration: 0.635s, episode steps:  96, steps per second: 151, episode reward: -74.248, mean reward: -0.773 [-100.000, 20.398], mean action: 1.438 [0.000, 3.000],  loss: 6.655810, mae: 28.613743, mean_q: 35.991896, mean_eps: 0.936247
  10769/150000: episode: 118, duration: 0.732s, episode steps:  95, steps per second: 130, episode reward: -110.459, mean reward: -1.163 [-100.000, 12.596], mean action: 1.663 [0.000, 3.000],  loss: 9.512253, mae: 28.309502, mean_q: 35.559668, mean_eps: 0.935674
  10893/150000: episode: 119, duration: 0.933s, episode steps: 124, steps per second: 133, episode reward: -159.366, mean reward: -1.285 [-100.000, 13.663], mean action: 1.645 [0.000, 3.000],  loss: 5.418445, mae: 28.020577, mean_q: 34.991084, mean_eps: 0.935017
  10959/150000: episode: 120, duration: 0.482s, episode steps:  66, steps per second: 137, episode reward: -162.935, mean reward: -2.469 [-100.000,  6.248], mean action: 1.515 [0.000, 3.000],  loss: 8.443547, mae: 28.395792, mean_q: 35.180917, mean_eps: 0.934447
  11048/150000: episode: 121, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: -104.879, mean reward: -1.178 [-100.000,  7.512], mean action: 1.517 [0.000, 3.000],  loss: 5.609830, mae: 28.210401, mean_q: 35.776174, mean_eps: 0.933982
  11122/150000: episode: 122, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -87.378, mean reward: -1.181 [-100.000,  7.666], mean action: 1.473 [0.000, 3.000],  loss: 8.724155, mae: 28.450681, mean_q: 35.329521, mean_eps: 0.933493
  11193/150000: episode: 123, duration: 0.514s, episode steps:  71, steps per second: 138, episode reward: -95.772, mean reward: -1.349 [-100.000, 17.099], mean action: 1.690 [0.000, 3.000],  loss: 10.827639, mae: 28.455689, mean_q: 36.460027, mean_eps: 0.933058
  11262/150000: episode: 124, duration: 0.508s, episode steps:  69, steps per second: 136, episode reward: -99.431, mean reward: -1.441 [-100.000, 14.519], mean action: 1.304 [0.000, 3.000],  loss: 4.423338, mae: 28.071376, mean_q: 35.765127, mean_eps: 0.932638
  11328/150000: episode: 125, duration: 0.449s, episode steps:  66, steps per second: 147, episode reward: -56.165, mean reward: -0.851 [-100.000, 17.363], mean action: 1.500 [0.000, 3.000],  loss: 6.529467, mae: 29.055011, mean_q: 36.773113, mean_eps: 0.932233
  11403/150000: episode: 126, duration: 0.492s, episode steps:  75, steps per second: 152, episode reward:  6.370, mean reward:  0.085 [-100.000, 99.909], mean action: 1.520 [0.000, 3.000],  loss: 12.399898, mae: 28.512557, mean_q: 35.098961, mean_eps: 0.931810
  11475/150000: episode: 127, duration: 0.451s, episode steps:  72, steps per second: 160, episode reward: -66.275, mean reward: -0.920 [-100.000, 13.796], mean action: 1.458 [0.000, 3.000],  loss: 6.896708, mae: 28.356299, mean_q: 35.531187, mean_eps: 0.931369
  11543/150000: episode: 128, duration: 0.434s, episode steps:  68, steps per second: 157, episode reward: -109.321, mean reward: -1.608 [-100.000, 56.173], mean action: 1.750 [0.000, 3.000],  loss: 6.413837, mae: 29.142368, mean_q: 36.388172, mean_eps: 0.930949
  11642/150000: episode: 129, duration: 0.641s, episode steps:  99, steps per second: 155, episode reward: -287.283, mean reward: -2.902 [-100.000,  0.485], mean action: 1.495 [0.000, 3.000],  loss: 5.731303, mae: 28.484651, mean_q: 35.602363, mean_eps: 0.930448
  11699/150000: episode: 130, duration: 0.353s, episode steps:  57, steps per second: 161, episode reward: -113.621, mean reward: -1.993 [-100.000, 24.523], mean action: 1.684 [0.000, 3.000],  loss: 3.083818, mae: 28.474091, mean_q: 35.875711, mean_eps: 0.929980
  11780/150000: episode: 131, duration: 0.535s, episode steps:  81, steps per second: 151, episode reward: -92.636, mean reward: -1.144 [-100.000,  9.840], mean action: 1.543 [0.000, 3.000],  loss: 5.788156, mae: 29.331165, mean_q: 35.856918, mean_eps: 0.929566
  11848/150000: episode: 132, duration: 0.493s, episode steps:  68, steps per second: 138, episode reward: -108.278, mean reward: -1.592 [-100.000, 10.446], mean action: 1.544 [0.000, 3.000],  loss: 4.752957, mae: 28.199752, mean_q: 35.306149, mean_eps: 0.929119
  11937/150000: episode: 133, duration: 0.605s, episode steps:  89, steps per second: 147, episode reward: -355.268, mean reward: -3.992 [-100.000, -0.071], mean action: 1.584 [0.000, 3.000],  loss: 5.532335, mae: 28.832383, mean_q: 36.415118, mean_eps: 0.928648
  12009/150000: episode: 134, duration: 0.535s, episode steps:  72, steps per second: 135, episode reward: -184.024, mean reward: -2.556 [-100.000,  5.349], mean action: 1.500 [0.000, 3.000],  loss: 6.422993, mae: 28.435434, mean_q: 35.501644, mean_eps: 0.928165
  12101/150000: episode: 135, duration: 0.592s, episode steps:  92, steps per second: 155, episode reward: -154.147, mean reward: -1.676 [-100.000, 10.806], mean action: 1.413 [0.000, 3.000],  loss: 7.642102, mae: 29.754705, mean_q: 36.316486, mean_eps: 0.927673
  12215/150000: episode: 136, duration: 0.836s, episode steps: 114, steps per second: 136, episode reward: -325.699, mean reward: -2.857 [-100.000,  5.051], mean action: 1.500 [0.000, 3.000],  loss: 6.665958, mae: 28.943354, mean_q: 35.976083, mean_eps: 0.927055
  12290/150000: episode: 137, duration: 0.465s, episode steps:  75, steps per second: 161, episode reward: -32.231, mean reward: -0.430 [-100.000, 81.953], mean action: 1.693 [0.000, 3.000],  loss: 8.357590, mae: 29.763242, mean_q: 36.367808, mean_eps: 0.926488
  12374/150000: episode: 138, duration: 0.511s, episode steps:  84, steps per second: 164, episode reward: -130.420, mean reward: -1.553 [-100.000,  5.039], mean action: 1.679 [0.000, 3.000],  loss: 7.968134, mae: 29.006653, mean_q: 35.907684, mean_eps: 0.926011
  12458/150000: episode: 139, duration: 0.571s, episode steps:  84, steps per second: 147, episode reward: -111.710, mean reward: -1.330 [-100.000,  8.779], mean action: 1.476 [0.000, 3.000],  loss: 4.931791, mae: 29.196614, mean_q: 35.855394, mean_eps: 0.925507
  12549/150000: episode: 140, duration: 0.619s, episode steps:  91, steps per second: 147, episode reward: -133.345, mean reward: -1.465 [-100.000,  7.820], mean action: 1.495 [0.000, 3.000],  loss: 3.988112, mae: 29.552554, mean_q: 37.158594, mean_eps: 0.924982
  12630/150000: episode: 141, duration: 0.514s, episode steps:  81, steps per second: 158, episode reward: -78.197, mean reward: -0.965 [-100.000,  8.350], mean action: 1.531 [0.000, 3.000],  loss: 4.873496, mae: 29.406169, mean_q: 36.169864, mean_eps: 0.924466
  13630/150000: episode: 142, duration: 7.777s, episode steps: 1000, steps per second: 129, episode reward: 65.986, mean reward:  0.066 [-24.361, 121.117], mean action: 1.471 [0.000, 3.000],  loss: 9.406054, mae: 29.335582, mean_q: 36.540628, mean_eps: 0.921223
  13695/150000: episode: 143, duration: 0.441s, episode steps:  65, steps per second: 147, episode reward: -145.465, mean reward: -2.238 [-100.000,  8.110], mean action: 1.446 [0.000, 3.000],  loss: 16.646468, mae: 29.845796, mean_q: 36.397102, mean_eps: 0.918028
  13763/150000: episode: 144, duration: 0.424s, episode steps:  68, steps per second: 160, episode reward: -63.401, mean reward: -0.932 [-100.000, 22.959], mean action: 1.662 [0.000, 3.000],  loss: 10.731240, mae: 29.628397, mean_q: 36.082967, mean_eps: 0.917629
  13847/150000: episode: 145, duration: 0.543s, episode steps:  84, steps per second: 155, episode reward: -101.568, mean reward: -1.209 [-100.000,  9.576], mean action: 1.702 [0.000, 3.000],  loss: 10.523576, mae: 29.607490, mean_q: 36.852329, mean_eps: 0.917173
  13936/150000: episode: 146, duration: 0.543s, episode steps:  89, steps per second: 164, episode reward: -157.474, mean reward: -1.769 [-100.000,  8.824], mean action: 1.393 [0.000, 3.000],  loss: 11.397901, mae: 29.850646, mean_q: 36.608894, mean_eps: 0.916654
  13997/150000: episode: 147, duration: 0.411s, episode steps:  61, steps per second: 148, episode reward: -86.173, mean reward: -1.413 [-100.000,  9.080], mean action: 1.492 [0.000, 3.000],  loss: 4.226110, mae: 29.956137, mean_q: 36.831763, mean_eps: 0.916204
  14057/150000: episode: 148, duration: 0.385s, episode steps:  60, steps per second: 156, episode reward: -63.774, mean reward: -1.063 [-100.000, 14.861], mean action: 1.483 [0.000, 3.000],  loss: 3.420387, mae: 30.425717, mean_q: 38.422571, mean_eps: 0.915841
  14122/150000: episode: 149, duration: 0.404s, episode steps:  65, steps per second: 161, episode reward: -108.030, mean reward: -1.662 [-100.000, 13.709], mean action: 1.615 [0.000, 3.000],  loss: 6.384214, mae: 29.653722, mean_q: 36.937011, mean_eps: 0.915466
  14206/150000: episode: 150, duration: 0.515s, episode steps:  84, steps per second: 163, episode reward: -141.479, mean reward: -1.684 [-100.000, 13.157], mean action: 1.488 [0.000, 3.000],  loss: 10.523564, mae: 30.898746, mean_q: 36.727583, mean_eps: 0.915019
  14297/150000: episode: 151, duration: 0.558s, episode steps:  91, steps per second: 163, episode reward: -54.516, mean reward: -0.599 [-100.000, 55.422], mean action: 1.484 [0.000, 3.000],  loss: 6.219492, mae: 30.517933, mean_q: 37.238901, mean_eps: 0.914494
  14361/150000: episode: 152, duration: 0.419s, episode steps:  64, steps per second: 153, episode reward: -70.549, mean reward: -1.102 [-100.000,  9.183], mean action: 1.625 [0.000, 3.000],  loss: 8.807427, mae: 30.534265, mean_q: 38.341245, mean_eps: 0.914029
  14431/150000: episode: 153, duration: 0.438s, episode steps:  70, steps per second: 160, episode reward: -120.600, mean reward: -1.723 [-100.000, 22.652], mean action: 1.600 [0.000, 3.000],  loss: 4.930598, mae: 29.731260, mean_q: 36.793804, mean_eps: 0.913627
  14495/150000: episode: 154, duration: 0.394s, episode steps:  64, steps per second: 163, episode reward: -121.666, mean reward: -1.901 [-100.000, 11.594], mean action: 1.703 [0.000, 3.000],  loss: 9.738354, mae: 31.104246, mean_q: 38.812053, mean_eps: 0.913225
  14618/150000: episode: 155, duration: 0.903s, episode steps: 123, steps per second: 136, episode reward: -67.037, mean reward: -0.545 [-100.000, 11.180], mean action: 1.634 [0.000, 3.000],  loss: 8.618636, mae: 30.950090, mean_q: 37.916464, mean_eps: 0.912664
  14728/150000: episode: 156, duration: 1.063s, episode steps: 110, steps per second: 104, episode reward: -249.809, mean reward: -2.271 [-100.000,  0.944], mean action: 1.564 [0.000, 3.000],  loss: 5.381917, mae: 29.979175, mean_q: 36.619847, mean_eps: 0.911965
  14807/150000: episode: 157, duration: 0.737s, episode steps:  79, steps per second: 107, episode reward: -84.025, mean reward: -1.064 [-100.000, 17.481], mean action: 1.519 [0.000, 3.000],  loss: 10.512852, mae: 30.508764, mean_q: 37.538239, mean_eps: 0.911398
  14947/150000: episode: 158, duration: 0.916s, episode steps: 140, steps per second: 153, episode reward: -183.360, mean reward: -1.310 [-100.000,  6.944], mean action: 1.607 [0.000, 3.000],  loss: 4.914347, mae: 30.958331, mean_q: 38.543320, mean_eps: 0.910741
  15052/150000: episode: 159, duration: 0.651s, episode steps: 105, steps per second: 161, episode reward: -120.505, mean reward: -1.148 [-100.000,  6.573], mean action: 1.571 [0.000, 3.000],  loss: 8.461222, mae: 31.332582, mean_q: 38.102254, mean_eps: 0.910006
  15133/150000: episode: 160, duration: 0.503s, episode steps:  81, steps per second: 161, episode reward: -97.747, mean reward: -1.207 [-100.000, 12.842], mean action: 1.457 [0.000, 3.000],  loss: 7.233656, mae: 31.492007, mean_q: 38.430810, mean_eps: 0.909448
  15204/150000: episode: 161, duration: 0.464s, episode steps:  71, steps per second: 153, episode reward: -99.361, mean reward: -1.399 [-100.000, 10.526], mean action: 1.746 [0.000, 3.000],  loss: 7.347977, mae: 30.313831, mean_q: 36.877907, mean_eps: 0.908992
  15279/150000: episode: 162, duration: 0.520s, episode steps:  75, steps per second: 144, episode reward: -62.618, mean reward: -0.835 [-100.000,  6.461], mean action: 1.667 [0.000, 3.000],  loss: 9.394841, mae: 31.403715, mean_q: 39.125484, mean_eps: 0.908554
  15384/150000: episode: 163, duration: 0.649s, episode steps: 105, steps per second: 162, episode reward: -177.233, mean reward: -1.688 [-100.000,  5.983], mean action: 1.505 [0.000, 3.000],  loss: 6.612064, mae: 31.047635, mean_q: 38.520728, mean_eps: 0.908014
  15449/150000: episode: 164, duration: 0.399s, episode steps:  65, steps per second: 163, episode reward: -23.839, mean reward: -0.367 [-100.000, 11.823], mean action: 1.554 [0.000, 3.000],  loss: 3.723049, mae: 30.715508, mean_q: 37.355164, mean_eps: 0.907504
  15536/150000: episode: 165, duration: 0.535s, episode steps:  87, steps per second: 163, episode reward: -220.464, mean reward: -2.534 [-100.000,  7.149], mean action: 1.506 [0.000, 3.000],  loss: 7.678476, mae: 31.112799, mean_q: 38.455991, mean_eps: 0.907048
  15625/150000: episode: 166, duration: 0.586s, episode steps:  89, steps per second: 152, episode reward: -89.787, mean reward: -1.009 [-100.000, 10.479], mean action: 1.618 [0.000, 3.000],  loss: 5.010227, mae: 30.591666, mean_q: 37.951827, mean_eps: 0.906520
  15720/150000: episode: 167, duration: 0.583s, episode steps:  95, steps per second: 163, episode reward: -114.916, mean reward: -1.210 [-100.000, 19.507], mean action: 1.705 [0.000, 3.000],  loss: 3.893477, mae: 31.214207, mean_q: 38.826327, mean_eps: 0.905968
  15834/150000: episode: 168, duration: 0.694s, episode steps: 114, steps per second: 164, episode reward: -138.530, mean reward: -1.215 [-100.000,  9.620], mean action: 1.684 [0.000, 3.000],  loss: 5.980396, mae: 31.634500, mean_q: 38.840646, mean_eps: 0.905341
  15895/150000: episode: 169, duration: 0.387s, episode steps:  61, steps per second: 157, episode reward: -142.280, mean reward: -2.332 [-100.000,  6.313], mean action: 1.492 [0.000, 3.000],  loss: 19.472195, mae: 30.773709, mean_q: 38.144346, mean_eps: 0.904816
  15973/150000: episode: 170, duration: 0.505s, episode steps:  78, steps per second: 155, episode reward: -104.215, mean reward: -1.336 [-100.000,  5.624], mean action: 1.321 [0.000, 3.000],  loss: 5.833993, mae: 31.031690, mean_q: 39.013599, mean_eps: 0.904399
  16065/150000: episode: 171, duration: 0.595s, episode steps:  92, steps per second: 155, episode reward: -98.503, mean reward: -1.071 [-100.000,  8.041], mean action: 1.522 [0.000, 3.000],  loss: 4.200167, mae: 31.339939, mean_q: 38.626652, mean_eps: 0.903889
  16164/150000: episode: 172, duration: 0.611s, episode steps:  99, steps per second: 162, episode reward: -124.460, mean reward: -1.257 [-100.000, 14.385], mean action: 1.586 [0.000, 3.000],  loss: 8.470161, mae: 32.021385, mean_q: 39.440627, mean_eps: 0.903316
  16237/150000: episode: 173, duration: 0.455s, episode steps:  73, steps per second: 160, episode reward: -66.281, mean reward: -0.908 [-100.000,  8.865], mean action: 1.685 [0.000, 3.000],  loss: 7.389454, mae: 31.472232, mean_q: 38.770784, mean_eps: 0.902800
  16314/150000: episode: 174, duration: 0.505s, episode steps:  77, steps per second: 152, episode reward: -97.739, mean reward: -1.269 [-100.000,  6.319], mean action: 1.455 [0.000, 3.000],  loss: 12.664627, mae: 31.679222, mean_q: 38.623345, mean_eps: 0.902350
  16392/150000: episode: 175, duration: 0.491s, episode steps:  78, steps per second: 159, episode reward: -77.134, mean reward: -0.989 [-100.000, 17.400], mean action: 1.462 [0.000, 3.000],  loss: 5.281859, mae: 31.532235, mean_q: 38.364218, mean_eps: 0.901885
  16482/150000: episode: 176, duration: 0.558s, episode steps:  90, steps per second: 161, episode reward: -154.805, mean reward: -1.720 [-100.000, 17.959], mean action: 1.589 [0.000, 3.000],  loss: 9.422043, mae: 32.498560, mean_q: 39.804990, mean_eps: 0.901381
  16563/150000: episode: 177, duration: 0.498s, episode steps:  81, steps per second: 163, episode reward: -188.035, mean reward: -2.321 [-100.000,  8.078], mean action: 1.667 [0.000, 3.000],  loss: 4.216112, mae: 32.332798, mean_q: 40.256375, mean_eps: 0.900868
  16677/150000: episode: 178, duration: 0.716s, episode steps: 114, steps per second: 159, episode reward: -98.086, mean reward: -0.860 [-100.000,  7.352], mean action: 1.544 [0.000, 3.000],  loss: 11.929887, mae: 32.204090, mean_q: 39.429407, mean_eps: 0.900283
  16786/150000: episode: 179, duration: 0.663s, episode steps: 109, steps per second: 165, episode reward: -116.101, mean reward: -1.065 [-100.000, 11.133], mean action: 1.697 [0.000, 3.000],  loss: 10.901058, mae: 32.384702, mean_q: 39.641818, mean_eps: 0.899614
  16886/150000: episode: 180, duration: 0.620s, episode steps: 100, steps per second: 161, episode reward: -220.187, mean reward: -2.202 [-100.000, 43.703], mean action: 1.600 [0.000, 3.000],  loss: 10.483957, mae: 33.060197, mean_q: 40.566228, mean_eps: 0.898987
  16992/150000: episode: 181, duration: 0.698s, episode steps: 106, steps per second: 152, episode reward: -346.944, mean reward: -3.273 [-100.000,  0.464], mean action: 1.519 [0.000, 3.000],  loss: 4.838535, mae: 32.255151, mean_q: 39.218268, mean_eps: 0.898369
  17089/150000: episode: 182, duration: 0.684s, episode steps:  97, steps per second: 142, episode reward: -184.532, mean reward: -1.902 [-100.000,  8.593], mean action: 1.351 [0.000, 3.000],  loss: 17.772240, mae: 33.703625, mean_q: 40.794229, mean_eps: 0.897760
  17165/150000: episode: 183, duration: 0.511s, episode steps:  76, steps per second: 149, episode reward: -112.766, mean reward: -1.484 [-100.000, 11.431], mean action: 1.487 [0.000, 3.000],  loss: 6.470068, mae: 33.506283, mean_q: 40.577357, mean_eps: 0.897241
  17282/150000: episode: 184, duration: 0.802s, episode steps: 117, steps per second: 146, episode reward: -81.064, mean reward: -0.693 [-100.000,  6.250], mean action: 1.368 [0.000, 3.000],  loss: 11.264913, mae: 32.533888, mean_q: 39.490159, mean_eps: 0.896662
  17406/150000: episode: 185, duration: 0.825s, episode steps: 124, steps per second: 150, episode reward: -92.369, mean reward: -0.745 [-100.000, 10.716], mean action: 1.516 [0.000, 3.000],  loss: 9.943345, mae: 32.624868, mean_q: 39.541000, mean_eps: 0.895939
  17475/150000: episode: 186, duration: 0.433s, episode steps:  69, steps per second: 159, episode reward: -126.506, mean reward: -1.833 [-100.000,  6.430], mean action: 1.348 [0.000, 3.000],  loss: 9.075302, mae: 33.396854, mean_q: 41.660993, mean_eps: 0.895360
  17546/150000: episode: 187, duration: 0.472s, episode steps:  71, steps per second: 150, episode reward: -107.205, mean reward: -1.510 [-100.000, 10.005], mean action: 1.577 [0.000, 3.000],  loss: 18.689964, mae: 32.696716, mean_q: 40.171286, mean_eps: 0.894940
  17658/150000: episode: 188, duration: 0.716s, episode steps: 112, steps per second: 156, episode reward: -105.356, mean reward: -0.941 [-100.000, 10.020], mean action: 1.429 [0.000, 3.000],  loss: 15.058663, mae: 33.434496, mean_q: 40.224153, mean_eps: 0.894391
  17740/150000: episode: 189, duration: 0.500s, episode steps:  82, steps per second: 164, episode reward: -102.647, mean reward: -1.252 [-100.000,  7.810], mean action: 1.683 [0.000, 3.000],  loss: 7.655979, mae: 33.921938, mean_q: 40.811729, mean_eps: 0.893809
  17809/150000: episode: 190, duration: 0.432s, episode steps:  69, steps per second: 160, episode reward: -51.361, mean reward: -0.744 [-100.000, 13.883], mean action: 1.725 [0.000, 3.000],  loss: 9.553914, mae: 33.847599, mean_q: 40.770294, mean_eps: 0.893356
  17893/150000: episode: 191, duration: 0.512s, episode steps:  84, steps per second: 164, episode reward: -122.025, mean reward: -1.453 [-100.000,  7.118], mean action: 1.595 [0.000, 3.000],  loss: 8.104661, mae: 34.060332, mean_q: 42.143990, mean_eps: 0.892897
  17955/150000: episode: 192, duration: 0.405s, episode steps:  62, steps per second: 153, episode reward: -102.172, mean reward: -1.648 [-100.000, 21.284], mean action: 1.387 [0.000, 3.000],  loss: 14.116255, mae: 34.027174, mean_q: 41.887177, mean_eps: 0.892459
  18101/150000: episode: 193, duration: 0.912s, episode steps: 146, steps per second: 160, episode reward: -122.895, mean reward: -0.842 [-100.000, 50.249], mean action: 1.548 [0.000, 3.000],  loss: 6.329465, mae: 33.494208, mean_q: 40.366530, mean_eps: 0.891835
  18217/150000: episode: 194, duration: 0.704s, episode steps: 116, steps per second: 165, episode reward: -280.645, mean reward: -2.419 [-100.000, 62.883], mean action: 1.517 [0.000, 3.000],  loss: 4.709188, mae: 33.306729, mean_q: 40.148210, mean_eps: 0.891049
  18278/150000: episode: 195, duration: 0.392s, episode steps:  61, steps per second: 156, episode reward: -81.992, mean reward: -1.344 [-100.000, 34.601], mean action: 1.721 [0.000, 3.000],  loss: 11.707507, mae: 33.217663, mean_q: 39.949770, mean_eps: 0.890518
  18396/150000: episode: 196, duration: 0.763s, episode steps: 118, steps per second: 155, episode reward: -87.796, mean reward: -0.744 [-100.000,  6.901], mean action: 1.483 [0.000, 3.000],  loss: 12.890325, mae: 33.932582, mean_q: 40.642186, mean_eps: 0.889981
  18485/150000: episode: 197, duration: 0.553s, episode steps:  89, steps per second: 161, episode reward: -91.904, mean reward: -1.033 [-100.000, 19.812], mean action: 1.618 [0.000, 3.000],  loss: 15.248197, mae: 34.394390, mean_q: 42.334606, mean_eps: 0.889360
  18557/150000: episode: 198, duration: 0.438s, episode steps:  72, steps per second: 164, episode reward: -127.919, mean reward: -1.777 [-100.000,  8.359], mean action: 1.569 [0.000, 3.000],  loss: 12.940127, mae: 33.236940, mean_q: 39.690963, mean_eps: 0.888877
  18662/150000: episode: 199, duration: 0.676s, episode steps: 105, steps per second: 155, episode reward: -192.022, mean reward: -1.829 [-100.000, 10.021], mean action: 1.476 [0.000, 3.000],  loss: 9.506139, mae: 33.506390, mean_q: 39.276472, mean_eps: 0.888346
  18761/150000: episode: 200, duration: 0.614s, episode steps:  99, steps per second: 161, episode reward: -126.478, mean reward: -1.278 [-100.000,  9.187], mean action: 1.667 [0.000, 3.000],  loss: 12.396600, mae: 34.092033, mean_q: 40.282769, mean_eps: 0.887734
  18892/150000: episode: 201, duration: 0.801s, episode steps: 131, steps per second: 164, episode reward: -159.554, mean reward: -1.218 [-100.000, 10.448], mean action: 1.626 [0.000, 3.000],  loss: 7.360423, mae: 34.410300, mean_q: 41.396470, mean_eps: 0.887044
  19014/150000: episode: 202, duration: 0.781s, episode steps: 122, steps per second: 156, episode reward: -104.379, mean reward: -0.856 [-100.000, 17.923], mean action: 1.631 [0.000, 3.000],  loss: 15.670629, mae: 33.846474, mean_q: 41.623950, mean_eps: 0.886285
  19134/150000: episode: 203, duration: 0.749s, episode steps: 120, steps per second: 160, episode reward: -96.764, mean reward: -0.806 [-100.000, 11.694], mean action: 1.483 [0.000, 3.000],  loss: 9.185725, mae: 33.653792, mean_q: 40.207828, mean_eps: 0.885559
  19236/150000: episode: 204, duration: 0.619s, episode steps: 102, steps per second: 165, episode reward: -97.267, mean reward: -0.954 [-100.000,  7.440], mean action: 1.500 [0.000, 3.000],  loss: 10.558683, mae: 34.777624, mean_q: 42.646672, mean_eps: 0.884893
  19328/150000: episode: 205, duration: 0.587s, episode steps:  92, steps per second: 157, episode reward: -104.775, mean reward: -1.139 [-100.000,  6.102], mean action: 1.370 [0.000, 3.000],  loss: 6.692577, mae: 34.809170, mean_q: 41.010107, mean_eps: 0.884311
  19414/150000: episode: 206, duration: 0.551s, episode steps:  86, steps per second: 156, episode reward: -102.142, mean reward: -1.188 [-100.000, 11.702], mean action: 1.698 [0.000, 3.000],  loss: 10.943231, mae: 34.784293, mean_q: 40.371919, mean_eps: 0.883777
  19532/150000: episode: 207, duration: 0.721s, episode steps: 118, steps per second: 164, episode reward: -109.605, mean reward: -0.929 [-100.000, 25.497], mean action: 1.542 [0.000, 3.000],  loss: 9.867510, mae: 34.732980, mean_q: 41.246984, mean_eps: 0.883165
  19615/150000: episode: 208, duration: 0.506s, episode steps:  83, steps per second: 164, episode reward: -152.132, mean reward: -1.833 [-100.000,  7.765], mean action: 1.554 [0.000, 3.000],  loss: 14.073491, mae: 34.566672, mean_q: 40.707302, mean_eps: 0.882562
  19678/150000: episode: 209, duration: 0.409s, episode steps:  63, steps per second: 154, episode reward: -76.764, mean reward: -1.218 [-100.000,  7.823], mean action: 1.460 [0.000, 3.000],  loss: 11.694915, mae: 35.122967, mean_q: 42.661036, mean_eps: 0.882124
  19774/150000: episode: 210, duration: 0.597s, episode steps:  96, steps per second: 161, episode reward: -106.823, mean reward: -1.113 [-100.000, 12.658], mean action: 1.677 [0.000, 3.000],  loss: 5.064983, mae: 35.021723, mean_q: 41.522737, mean_eps: 0.881647
  19873/150000: episode: 211, duration: 0.607s, episode steps:  99, steps per second: 163, episode reward: -76.544, mean reward: -0.773 [-100.000, 30.347], mean action: 1.475 [0.000, 3.000],  loss: 6.063095, mae: 35.431454, mean_q: 41.015434, mean_eps: 0.881062
  19967/150000: episode: 212, duration: 0.578s, episode steps:  94, steps per second: 163, episode reward: -111.210, mean reward: -1.183 [-100.000, 13.803], mean action: 1.585 [0.000, 3.000],  loss: 16.370697, mae: 35.371462, mean_q: 42.542886, mean_eps: 0.880483
  20082/150000: episode: 213, duration: 0.827s, episode steps: 115, steps per second: 139, episode reward: -34.306, mean reward: -0.298 [-100.000, 130.139], mean action: 1.443 [0.000, 3.000],  loss: 12.690103, mae: 34.470672, mean_q: 39.390455, mean_eps: 0.879856
  20227/150000: episode: 214, duration: 0.888s, episode steps: 145, steps per second: 163, episode reward: -138.328, mean reward: -0.954 [-100.000,  5.525], mean action: 1.593 [0.000, 3.000],  loss: 5.333186, mae: 35.033520, mean_q: 41.250478, mean_eps: 0.879076
  20314/150000: episode: 215, duration: 0.533s, episode steps:  87, steps per second: 163, episode reward: -104.473, mean reward: -1.201 [-100.000,  8.389], mean action: 1.333 [0.000, 3.000],  loss: 6.321173, mae: 34.376196, mean_q: 41.839875, mean_eps: 0.878380
  20417/150000: episode: 216, duration: 0.656s, episode steps: 103, steps per second: 157, episode reward: -71.019, mean reward: -0.690 [-100.000, 29.343], mean action: 1.612 [0.000, 3.000],  loss: 6.043879, mae: 34.535255, mean_q: 40.897104, mean_eps: 0.877810
  20483/150000: episode: 217, duration: 0.405s, episode steps:  66, steps per second: 163, episode reward: -116.137, mean reward: -1.760 [-100.000, 16.367], mean action: 1.455 [0.000, 3.000],  loss: 5.027003, mae: 34.557553, mean_q: 40.677231, mean_eps: 0.877303
  20564/150000: episode: 218, duration: 0.493s, episode steps:  81, steps per second: 164, episode reward: -103.472, mean reward: -1.277 [-100.000, 12.954], mean action: 1.679 [0.000, 3.000],  loss: 7.860136, mae: 34.915538, mean_q: 40.769702, mean_eps: 0.876862
  20711/150000: episode: 219, duration: 0.904s, episode steps: 147, steps per second: 163, episode reward: -94.058, mean reward: -0.640 [-100.000,  9.713], mean action: 1.639 [0.000, 3.000],  loss: 6.175847, mae: 35.144798, mean_q: 41.515596, mean_eps: 0.876178
  20805/150000: episode: 220, duration: 0.588s, episode steps:  94, steps per second: 160, episode reward: -73.661, mean reward: -0.784 [-100.000, 14.688], mean action: 1.468 [0.000, 3.000],  loss: 9.763432, mae: 35.066403, mean_q: 41.418873, mean_eps: 0.875455
  20887/150000: episode: 221, duration: 0.591s, episode steps:  82, steps per second: 139, episode reward: -235.939, mean reward: -2.877 [-100.000, 16.430], mean action: 1.695 [0.000, 3.000],  loss: 8.254540, mae: 34.993515, mean_q: 41.997244, mean_eps: 0.874927
  20953/150000: episode: 222, duration: 0.482s, episode steps:  66, steps per second: 137, episode reward: -99.738, mean reward: -1.511 [-100.000, 13.027], mean action: 1.470 [0.000, 3.000],  loss: 5.459533, mae: 34.255884, mean_q: 40.976416, mean_eps: 0.874483
  21077/150000: episode: 223, duration: 0.918s, episode steps: 124, steps per second: 135, episode reward: -108.269, mean reward: -0.873 [-100.000, 11.409], mean action: 1.540 [0.000, 3.000],  loss: 13.861375, mae: 35.466676, mean_q: 43.257170, mean_eps: 0.873913
  21153/150000: episode: 224, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -94.709, mean reward: -1.246 [-100.000, 20.037], mean action: 1.382 [0.000, 3.000],  loss: 15.026066, mae: 35.856792, mean_q: 44.032697, mean_eps: 0.873313
  21221/150000: episode: 225, duration: 0.523s, episode steps:  68, steps per second: 130, episode reward: -95.990, mean reward: -1.412 [-100.000, 10.783], mean action: 1.691 [0.000, 3.000],  loss: 13.547495, mae: 35.741497, mean_q: 43.695905, mean_eps: 0.872881
  21296/150000: episode: 226, duration: 0.526s, episode steps:  75, steps per second: 143, episode reward: -97.380, mean reward: -1.298 [-100.000, 16.319], mean action: 1.533 [0.000, 3.000],  loss: 7.787974, mae: 35.004826, mean_q: 42.490517, mean_eps: 0.872452
  21426/150000: episode: 227, duration: 0.949s, episode steps: 130, steps per second: 137, episode reward: -234.825, mean reward: -1.806 [-100.000,  3.323], mean action: 1.477 [0.000, 3.000],  loss: 8.118154, mae: 35.204128, mean_q: 42.346848, mean_eps: 0.871837
  21533/150000: episode: 228, duration: 0.716s, episode steps: 107, steps per second: 150, episode reward: -103.778, mean reward: -0.970 [-100.000, 12.490], mean action: 1.393 [0.000, 3.000],  loss: 5.800930, mae: 36.277339, mean_q: 44.232058, mean_eps: 0.871126
  21593/150000: episode: 229, duration: 0.377s, episode steps:  60, steps per second: 159, episode reward: -71.778, mean reward: -1.196 [-100.000,  8.669], mean action: 1.767 [0.000, 3.000],  loss: 13.742075, mae: 34.950659, mean_q: 42.413847, mean_eps: 0.870625
  21718/150000: episode: 230, duration: 0.811s, episode steps: 125, steps per second: 154, episode reward: -131.295, mean reward: -1.050 [-100.000,  8.711], mean action: 1.488 [0.000, 3.000],  loss: 11.149404, mae: 35.866556, mean_q: 43.617056, mean_eps: 0.870070
  21782/150000: episode: 231, duration: 0.393s, episode steps:  64, steps per second: 163, episode reward: -98.848, mean reward: -1.544 [-100.000, 18.364], mean action: 1.500 [0.000, 3.000],  loss: 5.330649, mae: 36.226225, mean_q: 43.265040, mean_eps: 0.869503
  21853/150000: episode: 232, duration: 0.443s, episode steps:  71, steps per second: 160, episode reward: -102.081, mean reward: -1.438 [-100.000, 12.508], mean action: 1.479 [0.000, 3.000],  loss: 23.863500, mae: 35.626617, mean_q: 44.116186, mean_eps: 0.869098
  21958/150000: episode: 233, duration: 0.634s, episode steps: 105, steps per second: 166, episode reward: -90.316, mean reward: -0.860 [-100.000, 10.191], mean action: 1.486 [0.000, 3.000],  loss: 14.469868, mae: 36.258144, mean_q: 43.567943, mean_eps: 0.868570
  22075/150000: episode: 234, duration: 0.739s, episode steps: 117, steps per second: 158, episode reward: -70.107, mean reward: -0.599 [-100.000,  6.441], mean action: 1.538 [0.000, 3.000],  loss: 6.932959, mae: 36.809552, mean_q: 44.634477, mean_eps: 0.867904
  22158/150000: episode: 235, duration: 0.516s, episode steps:  83, steps per second: 161, episode reward: -97.468, mean reward: -1.174 [-100.000, 20.635], mean action: 1.530 [0.000, 3.000],  loss: 21.565266, mae: 36.229146, mean_q: 44.627008, mean_eps: 0.867304
  22223/150000: episode: 236, duration: 0.399s, episode steps:  65, steps per second: 163, episode reward: -58.626, mean reward: -0.902 [-100.000, 17.080], mean action: 1.492 [0.000, 3.000],  loss: 17.179792, mae: 36.157800, mean_q: 44.296660, mean_eps: 0.866860
  22322/150000: episode: 237, duration: 0.622s, episode steps:  99, steps per second: 159, episode reward: -120.549, mean reward: -1.218 [-100.000, 11.335], mean action: 1.616 [0.000, 3.000],  loss: 6.098278, mae: 36.609251, mean_q: 44.384166, mean_eps: 0.866368
  22400/150000: episode: 238, duration: 0.493s, episode steps:  78, steps per second: 158, episode reward: -75.053, mean reward: -0.962 [-100.000,  8.822], mean action: 1.551 [0.000, 3.000],  loss: 7.988422, mae: 36.863309, mean_q: 45.067690, mean_eps: 0.865837
  22494/150000: episode: 239, duration: 0.578s, episode steps:  94, steps per second: 163, episode reward: -91.826, mean reward: -0.977 [-100.000,  7.566], mean action: 1.489 [0.000, 3.000],  loss: 15.577089, mae: 36.620722, mean_q: 44.357570, mean_eps: 0.865321
  22600/150000: episode: 240, duration: 0.651s, episode steps: 106, steps per second: 163, episode reward: -76.363, mean reward: -0.720 [-100.000,  8.188], mean action: 1.547 [0.000, 3.000],  loss: 5.900417, mae: 36.248725, mean_q: 43.534156, mean_eps: 0.864721
  22685/150000: episode: 241, duration: 0.554s, episode steps:  85, steps per second: 153, episode reward: -89.570, mean reward: -1.054 [-100.000,  6.873], mean action: 1.671 [0.000, 3.000],  loss: 5.171797, mae: 36.960262, mean_q: 44.977549, mean_eps: 0.864148
  22810/150000: episode: 242, duration: 0.777s, episode steps: 125, steps per second: 161, episode reward: -152.445, mean reward: -1.220 [-100.000,  6.801], mean action: 1.480 [0.000, 3.000],  loss: 12.745790, mae: 36.546714, mean_q: 44.696723, mean_eps: 0.863518
  22905/150000: episode: 243, duration: 0.574s, episode steps:  95, steps per second: 166, episode reward: -75.182, mean reward: -0.791 [-100.000, 17.208], mean action: 1.358 [0.000, 3.000],  loss: 5.505462, mae: 36.704966, mean_q: 44.825760, mean_eps: 0.862858
  22988/150000: episode: 244, duration: 0.520s, episode steps:  83, steps per second: 160, episode reward: -108.592, mean reward: -1.308 [-100.000, 17.461], mean action: 1.446 [0.000, 3.000],  loss: 4.244125, mae: 37.136336, mean_q: 45.526292, mean_eps: 0.862324
  23094/150000: episode: 245, duration: 0.699s, episode steps: 106, steps per second: 152, episode reward: -85.357, mean reward: -0.805 [-100.000,  9.563], mean action: 1.736 [0.000, 3.000],  loss: 8.441591, mae: 37.896008, mean_q: 46.369825, mean_eps: 0.861757
  23215/150000: episode: 246, duration: 0.736s, episode steps: 121, steps per second: 164, episode reward: -83.553, mean reward: -0.691 [-100.000,  6.275], mean action: 1.645 [0.000, 3.000],  loss: 8.347161, mae: 37.864187, mean_q: 45.438027, mean_eps: 0.861076
  23291/150000: episode: 247, duration: 0.470s, episode steps:  76, steps per second: 162, episode reward: -52.138, mean reward: -0.686 [-100.000,  9.606], mean action: 1.789 [0.000, 3.000],  loss: 18.480984, mae: 37.286195, mean_q: 45.186658, mean_eps: 0.860485
  23385/150000: episode: 248, duration: 0.590s, episode steps:  94, steps per second: 159, episode reward: -120.281, mean reward: -1.280 [-100.000,  7.091], mean action: 1.426 [0.000, 3.000],  loss: 22.473483, mae: 37.180722, mean_q: 45.597487, mean_eps: 0.859975
  23464/150000: episode: 249, duration: 0.494s, episode steps:  79, steps per second: 160, episode reward: -80.472, mean reward: -1.019 [-100.000, 11.227], mean action: 1.722 [0.000, 3.000],  loss: 22.560034, mae: 37.887266, mean_q: 46.508356, mean_eps: 0.859456
  23567/150000: episode: 250, duration: 0.628s, episode steps: 103, steps per second: 164, episode reward: -149.657, mean reward: -1.453 [-100.000, 28.981], mean action: 1.505 [0.000, 3.000],  loss: 7.504385, mae: 37.924476, mean_q: 46.465753, mean_eps: 0.858910
  23634/150000: episode: 251, duration: 0.406s, episode steps:  67, steps per second: 165, episode reward: -103.931, mean reward: -1.551 [-100.000,  8.015], mean action: 1.478 [0.000, 3.000],  loss: 7.966095, mae: 38.978909, mean_q: 47.556147, mean_eps: 0.858400
  23720/150000: episode: 252, duration: 0.554s, episode steps:  86, steps per second: 155, episode reward: -160.376, mean reward: -1.865 [-100.000, 18.381], mean action: 1.512 [0.000, 3.000],  loss: 7.766502, mae: 37.747596, mean_q: 44.879563, mean_eps: 0.857941
  23814/150000: episode: 253, duration: 0.586s, episode steps:  94, steps per second: 160, episode reward: -96.006, mean reward: -1.021 [-100.000, 13.542], mean action: 1.670 [0.000, 3.000],  loss: 8.892404, mae: 37.719333, mean_q: 45.539345, mean_eps: 0.857401
  23894/150000: episode: 254, duration: 0.492s, episode steps:  80, steps per second: 163, episode reward: -95.188, mean reward: -1.190 [-100.000,  6.680], mean action: 1.637 [0.000, 3.000],  loss: 6.359232, mae: 37.479341, mean_q: 45.207473, mean_eps: 0.856879
  24005/150000: episode: 255, duration: 0.675s, episode steps: 111, steps per second: 165, episode reward: -91.007, mean reward: -0.820 [-100.000, 13.747], mean action: 1.640 [0.000, 3.000],  loss: 10.060208, mae: 37.183296, mean_q: 44.838066, mean_eps: 0.856306
  24128/150000: episode: 256, duration: 0.792s, episode steps: 123, steps per second: 155, episode reward: -192.819, mean reward: -1.568 [-100.000, 57.513], mean action: 1.488 [0.000, 3.000],  loss: 8.552554, mae: 38.703966, mean_q: 46.518078, mean_eps: 0.855604
  24214/150000: episode: 257, duration: 0.533s, episode steps:  86, steps per second: 161, episode reward: -156.286, mean reward: -1.817 [-100.000,  7.653], mean action: 1.442 [0.000, 3.000],  loss: 40.558302, mae: 38.538234, mean_q: 46.127617, mean_eps: 0.854977
  24289/150000: episode: 258, duration: 0.456s, episode steps:  75, steps per second: 165, episode reward: -103.384, mean reward: -1.378 [-100.000, 20.902], mean action: 1.600 [0.000, 3.000],  loss: 4.911190, mae: 38.629435, mean_q: 45.900349, mean_eps: 0.854494
  24361/150000: episode: 259, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: -92.286, mean reward: -1.282 [-100.000, 27.295], mean action: 1.569 [0.000, 3.000],  loss: 26.814388, mae: 39.146405, mean_q: 46.536868, mean_eps: 0.854053
  24430/150000: episode: 260, duration: 0.452s, episode steps:  69, steps per second: 153, episode reward: -97.306, mean reward: -1.410 [-100.000,  7.042], mean action: 1.435 [0.000, 3.000],  loss: 11.557383, mae: 39.485534, mean_q: 47.312093, mean_eps: 0.853630
  24549/150000: episode: 261, duration: 0.742s, episode steps: 119, steps per second: 160, episode reward: -85.656, mean reward: -0.720 [-100.000, 10.863], mean action: 1.513 [0.000, 3.000],  loss: 15.394746, mae: 38.508880, mean_q: 46.034943, mean_eps: 0.853066
  24613/150000: episode: 262, duration: 0.391s, episode steps:  64, steps per second: 164, episode reward: -85.589, mean reward: -1.337 [-100.000, 10.707], mean action: 1.688 [0.000, 3.000],  loss: 6.781473, mae: 39.045708, mean_q: 46.221583, mean_eps: 0.852517
  24738/150000: episode: 263, duration: 0.781s, episode steps: 125, steps per second: 160, episode reward: -96.350, mean reward: -0.771 [-100.000, 12.784], mean action: 1.632 [0.000, 3.000],  loss: 22.840692, mae: 38.567221, mean_q: 46.237541, mean_eps: 0.851950
  24826/150000: episode: 264, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: -117.350, mean reward: -1.334 [-100.000, 27.143], mean action: 1.716 [0.000, 3.000],  loss: 8.867358, mae: 39.290999, mean_q: 47.971690, mean_eps: 0.851311
  24904/150000: episode: 265, duration: 0.489s, episode steps:  78, steps per second: 160, episode reward: -110.238, mean reward: -1.413 [-100.000,  6.191], mean action: 1.449 [0.000, 3.000],  loss: 4.029293, mae: 38.439708, mean_q: 45.754310, mean_eps: 0.850813
  24998/150000: episode: 266, duration: 0.581s, episode steps:  94, steps per second: 162, episode reward: -95.433, mean reward: -1.015 [-100.000,  7.626], mean action: 1.521 [0.000, 3.000],  loss: 18.099982, mae: 39.609178, mean_q: 46.514265, mean_eps: 0.850297
  25099/150000: episode: 267, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: -133.705, mean reward: -1.324 [-100.000, 13.775], mean action: 1.693 [0.000, 3.000],  loss: 14.117957, mae: 39.581435, mean_q: 46.721824, mean_eps: 0.849712
  25168/150000: episode: 268, duration: 0.442s, episode steps:  69, steps per second: 156, episode reward: -115.576, mean reward: -1.675 [-100.000,  8.942], mean action: 1.638 [0.000, 3.000],  loss: 4.051661, mae: 39.385177, mean_q: 47.417041, mean_eps: 0.849202
  25249/150000: episode: 269, duration: 0.494s, episode steps:  81, steps per second: 164, episode reward: -100.782, mean reward: -1.244 [-100.000,  6.722], mean action: 1.420 [0.000, 3.000],  loss: 4.628197, mae: 39.562462, mean_q: 47.196600, mean_eps: 0.848752
  25338/150000: episode: 270, duration: 0.546s, episode steps:  89, steps per second: 163, episode reward: -15.115, mean reward: -0.170 [-100.000, 111.464], mean action: 1.607 [0.000, 3.000],  loss: 23.073339, mae: 39.800488, mean_q: 47.034381, mean_eps: 0.848242
  25405/150000: episode: 271, duration: 0.411s, episode steps:  67, steps per second: 163, episode reward: -62.190, mean reward: -0.928 [-100.000,  7.116], mean action: 1.463 [0.000, 3.000],  loss: 8.971409, mae: 39.606618, mean_q: 47.090352, mean_eps: 0.847774
  25477/150000: episode: 272, duration: 0.475s, episode steps:  72, steps per second: 151, episode reward: -119.069, mean reward: -1.654 [-100.000,  7.362], mean action: 1.736 [0.000, 3.000],  loss: 5.853876, mae: 40.556182, mean_q: 47.698830, mean_eps: 0.847357
  25574/150000: episode: 273, duration: 0.604s, episode steps:  97, steps per second: 161, episode reward: -104.554, mean reward: -1.078 [-100.000, 10.458], mean action: 1.485 [0.000, 3.000],  loss: 6.433475, mae: 40.322987, mean_q: 47.628636, mean_eps: 0.846850
  25674/150000: episode: 274, duration: 0.614s, episode steps: 100, steps per second: 163, episode reward: -92.515, mean reward: -0.925 [-100.000, 11.906], mean action: 1.520 [0.000, 3.000],  loss: 19.618203, mae: 41.057325, mean_q: 47.252604, mean_eps: 0.846259
  25769/150000: episode: 275, duration: 0.582s, episode steps:  95, steps per second: 163, episode reward: -103.672, mean reward: -1.091 [-100.000,  8.860], mean action: 1.537 [0.000, 3.000],  loss: 12.988549, mae: 40.385320, mean_q: 48.134989, mean_eps: 0.845674
  25881/150000: episode: 276, duration: 0.727s, episode steps: 112, steps per second: 154, episode reward: -81.762, mean reward: -0.730 [-100.000, 15.439], mean action: 1.357 [0.000, 3.000],  loss: 9.295170, mae: 40.450232, mean_q: 48.312080, mean_eps: 0.845053
  25968/150000: episode: 277, duration: 0.536s, episode steps:  87, steps per second: 162, episode reward: -87.627, mean reward: -1.007 [-100.000, 17.160], mean action: 1.563 [0.000, 3.000],  loss: 15.120069, mae: 40.809847, mean_q: 48.476346, mean_eps: 0.844456
  26063/150000: episode: 278, duration: 0.586s, episode steps:  95, steps per second: 162, episode reward: -74.846, mean reward: -0.788 [-100.000,  9.902], mean action: 1.695 [0.000, 3.000],  loss: 11.973256, mae: 40.728631, mean_q: 47.587135, mean_eps: 0.843910
  26143/150000: episode: 279, duration: 0.510s, episode steps:  80, steps per second: 157, episode reward: -24.771, mean reward: -0.310 [-100.000, 13.252], mean action: 1.663 [0.000, 3.000],  loss: 13.955655, mae: 40.297691, mean_q: 48.631586, mean_eps: 0.843385
  26260/150000: episode: 280, duration: 0.729s, episode steps: 117, steps per second: 160, episode reward: -144.974, mean reward: -1.239 [-100.000, 19.343], mean action: 1.684 [0.000, 3.000],  loss: 8.780834, mae: 40.921270, mean_q: 48.416853, mean_eps: 0.842794
  26326/150000: episode: 281, duration: 0.413s, episode steps:  66, steps per second: 160, episode reward: -98.407, mean reward: -1.491 [-100.000, 18.729], mean action: 1.561 [0.000, 3.000],  loss: 6.064160, mae: 39.857104, mean_q: 47.847701, mean_eps: 0.842245
  26387/150000: episode: 282, duration: 0.372s, episode steps:  61, steps per second: 164, episode reward: -63.834, mean reward: -1.046 [-100.000,  9.169], mean action: 1.672 [0.000, 3.000],  loss: 20.951402, mae: 40.078187, mean_q: 48.652258, mean_eps: 0.841864
  26499/150000: episode: 283, duration: 0.708s, episode steps: 112, steps per second: 158, episode reward: -154.316, mean reward: -1.378 [-100.000, 24.985], mean action: 1.598 [0.000, 3.000],  loss: 10.318027, mae: 40.080349, mean_q: 47.396786, mean_eps: 0.841345
  26564/150000: episode: 284, duration: 0.423s, episode steps:  65, steps per second: 154, episode reward: -94.657, mean reward: -1.456 [-100.000,  6.365], mean action: 1.723 [0.000, 3.000],  loss: 24.182463, mae: 40.503722, mean_q: 49.029676, mean_eps: 0.840814
  26643/150000: episode: 285, duration: 0.497s, episode steps:  79, steps per second: 159, episode reward: -72.870, mean reward: -0.922 [-100.000, 68.548], mean action: 1.430 [0.000, 3.000],  loss: 33.444218, mae: 40.438241, mean_q: 47.816375, mean_eps: 0.840382
  26714/150000: episode: 286, duration: 0.438s, episode steps:  71, steps per second: 162, episode reward: -86.143, mean reward: -1.213 [-100.000,  5.880], mean action: 1.634 [0.000, 3.000],  loss: 9.170535, mae: 40.627667, mean_q: 48.652987, mean_eps: 0.839932
  26805/150000: episode: 287, duration: 0.584s, episode steps:  91, steps per second: 156, episode reward: -263.690, mean reward: -2.898 [-100.000,  5.074], mean action: 1.527 [0.000, 3.000],  loss: 25.841831, mae: 40.271576, mean_q: 48.152899, mean_eps: 0.839446
  26896/150000: episode: 288, duration: 0.576s, episode steps:  91, steps per second: 158, episode reward: -20.118, mean reward: -0.221 [-100.000, 14.987], mean action: 1.593 [0.000, 3.000],  loss: 13.185224, mae: 41.909728, mean_q: 49.743420, mean_eps: 0.838900
  26993/150000: episode: 289, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -89.365, mean reward: -0.921 [-100.000, 23.958], mean action: 1.433 [0.000, 3.000],  loss: 6.691954, mae: 39.992193, mean_q: 48.794087, mean_eps: 0.838336
  27096/150000: episode: 290, duration: 0.635s, episode steps: 103, steps per second: 162, episode reward: -101.766, mean reward: -0.988 [-100.000, 16.316], mean action: 1.563 [0.000, 3.000],  loss: 11.502677, mae: 41.112492, mean_q: 49.104848, mean_eps: 0.837736
  27192/150000: episode: 291, duration: 0.625s, episode steps:  96, steps per second: 154, episode reward: -124.622, mean reward: -1.298 [-100.000,  5.436], mean action: 1.500 [0.000, 3.000],  loss: 15.467481, mae: 41.333842, mean_q: 48.845421, mean_eps: 0.837139
  27283/150000: episode: 292, duration: 0.624s, episode steps:  91, steps per second: 146, episode reward: -81.189, mean reward: -0.892 [-100.000, 19.194], mean action: 1.462 [0.000, 3.000],  loss: 24.238786, mae: 40.733153, mean_q: 47.979640, mean_eps: 0.836578
  27376/150000: episode: 293, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -143.502, mean reward: -1.543 [-100.000,  8.765], mean action: 1.763 [0.000, 3.000],  loss: 23.745172, mae: 40.801098, mean_q: 48.216528, mean_eps: 0.836026
  27504/150000: episode: 294, duration: 0.908s, episode steps: 128, steps per second: 141, episode reward: -13.275, mean reward: -0.104 [-100.000, 63.748], mean action: 1.570 [0.000, 3.000],  loss: 18.899674, mae: 40.946905, mean_q: 48.525508, mean_eps: 0.835363
  27593/150000: episode: 295, duration: 0.590s, episode steps:  89, steps per second: 151, episode reward: -52.143, mean reward: -0.586 [-100.000, 67.363], mean action: 1.584 [0.000, 3.000],  loss: 5.698996, mae: 40.749843, mean_q: 48.949989, mean_eps: 0.834712
  27713/150000: episode: 296, duration: 0.775s, episode steps: 120, steps per second: 155, episode reward: -151.527, mean reward: -1.263 [-100.000,  3.601], mean action: 1.458 [0.000, 3.000],  loss: 5.611958, mae: 40.726771, mean_q: 47.696892, mean_eps: 0.834085
  27798/150000: episode: 297, duration: 0.547s, episode steps:  85, steps per second: 155, episode reward: -96.320, mean reward: -1.133 [-100.000, 15.096], mean action: 1.612 [0.000, 3.000],  loss: 16.474936, mae: 40.671631, mean_q: 48.100978, mean_eps: 0.833470
  27897/150000: episode: 298, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward: -63.342, mean reward: -0.640 [-100.000, 27.530], mean action: 1.576 [0.000, 3.000],  loss: 8.682215, mae: 41.574963, mean_q: 48.624900, mean_eps: 0.832918
  27978/150000: episode: 299, duration: 0.560s, episode steps:  81, steps per second: 145, episode reward: -62.904, mean reward: -0.777 [-100.000, 15.862], mean action: 1.790 [0.000, 3.000],  loss: 6.362227, mae: 41.620762, mean_q: 48.578050, mean_eps: 0.832378
  28092/150000: episode: 300, duration: 0.709s, episode steps: 114, steps per second: 161, episode reward: -84.974, mean reward: -0.745 [-100.000,  8.059], mean action: 1.553 [0.000, 3.000],  loss: 34.238131, mae: 41.699391, mean_q: 48.839437, mean_eps: 0.831793
  28166/150000: episode: 301, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward: -63.548, mean reward: -0.859 [-100.000, 11.291], mean action: 1.716 [0.000, 3.000],  loss: 6.160120, mae: 41.516821, mean_q: 47.031690, mean_eps: 0.831229
  28243/150000: episode: 302, duration: 0.472s, episode steps:  77, steps per second: 163, episode reward: -111.129, mean reward: -1.443 [-100.000, 13.687], mean action: 1.442 [0.000, 3.000],  loss: 7.276474, mae: 41.177139, mean_q: 47.521016, mean_eps: 0.830776
  28324/150000: episode: 303, duration: 0.498s, episode steps:  81, steps per second: 163, episode reward: -89.844, mean reward: -1.109 [-100.000, 12.337], mean action: 1.580 [0.000, 3.000],  loss: 14.488567, mae: 42.140478, mean_q: 49.642898, mean_eps: 0.830302
  28392/150000: episode: 304, duration: 0.418s, episode steps:  68, steps per second: 163, episode reward: -177.113, mean reward: -2.605 [-100.000, 24.109], mean action: 1.500 [0.000, 3.000],  loss: 6.977205, mae: 41.720875, mean_q: 49.453245, mean_eps: 0.829855
  28508/150000: episode: 305, duration: 0.771s, episode steps: 116, steps per second: 150, episode reward: -315.492, mean reward: -2.720 [-100.000, 99.321], mean action: 1.500 [0.000, 3.000],  loss: 16.347358, mae: 41.444025, mean_q: 49.304138, mean_eps: 0.829303
  28614/150000: episode: 306, duration: 0.650s, episode steps: 106, steps per second: 163, episode reward: -93.780, mean reward: -0.885 [-100.000,  6.618], mean action: 1.623 [0.000, 3.000],  loss: 23.970527, mae: 41.375809, mean_q: 47.776444, mean_eps: 0.828637
  28717/150000: episode: 307, duration: 0.629s, episode steps: 103, steps per second: 164, episode reward: -92.342, mean reward: -0.897 [-100.000,  9.582], mean action: 1.660 [0.000, 3.000],  loss: 19.759774, mae: 42.004854, mean_q: 49.164898, mean_eps: 0.828010
  28811/150000: episode: 308, duration: 0.609s, episode steps:  94, steps per second: 154, episode reward: -166.582, mean reward: -1.772 [-100.000, 21.593], mean action: 1.500 [0.000, 3.000],  loss: 13.891580, mae: 42.044944, mean_q: 49.715344, mean_eps: 0.827419
  28913/150000: episode: 309, duration: 0.637s, episode steps: 102, steps per second: 160, episode reward: -43.961, mean reward: -0.431 [-100.000, 14.502], mean action: 1.667 [0.000, 3.000],  loss: 22.707477, mae: 41.368114, mean_q: 47.108859, mean_eps: 0.826831
  28985/150000: episode: 310, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: -87.335, mean reward: -1.213 [-100.000, 17.849], mean action: 1.347 [0.000, 3.000],  loss: 34.475598, mae: 41.999482, mean_q: 49.348176, mean_eps: 0.826309
  29099/150000: episode: 311, duration: 0.709s, episode steps: 114, steps per second: 161, episode reward: -88.011, mean reward: -0.772 [-100.000, 11.780], mean action: 1.570 [0.000, 3.000],  loss: 11.472607, mae: 41.602776, mean_q: 46.992688, mean_eps: 0.825751
  29217/150000: episode: 312, duration: 0.750s, episode steps: 118, steps per second: 157, episode reward: -87.234, mean reward: -0.739 [-100.000,  7.094], mean action: 1.636 [0.000, 3.000],  loss: 17.776939, mae: 42.336352, mean_q: 48.423933, mean_eps: 0.825055
  29333/150000: episode: 313, duration: 0.720s, episode steps: 116, steps per second: 161, episode reward: -26.297, mean reward: -0.227 [-100.000, 68.989], mean action: 1.612 [0.000, 3.000],  loss: 12.849032, mae: 42.526340, mean_q: 48.823596, mean_eps: 0.824353
  29404/150000: episode: 314, duration: 0.438s, episode steps:  71, steps per second: 162, episode reward: -97.819, mean reward: -1.378 [-100.000,  7.512], mean action: 1.704 [0.000, 3.000],  loss: 17.000491, mae: 42.039055, mean_q: 47.875493, mean_eps: 0.823792
  29524/150000: episode: 315, duration: 0.765s, episode steps: 120, steps per second: 157, episode reward: -110.034, mean reward: -0.917 [-100.000, 12.147], mean action: 1.583 [0.000, 3.000],  loss: 8.201855, mae: 42.533784, mean_q: 48.681510, mean_eps: 0.823219
  29633/150000: episode: 316, duration: 0.710s, episode steps: 109, steps per second: 154, episode reward: -101.174, mean reward: -0.928 [-100.000,  7.281], mean action: 1.651 [0.000, 3.000],  loss: 13.143925, mae: 41.835509, mean_q: 47.830389, mean_eps: 0.822532
  29733/150000: episode: 317, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -79.650, mean reward: -0.796 [-100.000,  6.666], mean action: 1.720 [0.000, 3.000],  loss: 21.859619, mae: 42.445330, mean_q: 49.738334, mean_eps: 0.821905
  29834/150000: episode: 318, duration: 0.624s, episode steps: 101, steps per second: 162, episode reward: -110.038, mean reward: -1.089 [-100.000,  9.991], mean action: 1.535 [0.000, 3.000],  loss: 8.528103, mae: 42.959244, mean_q: 49.532286, mean_eps: 0.821302
  29902/150000: episode: 319, duration: 0.446s, episode steps:  68, steps per second: 153, episode reward: -47.117, mean reward: -0.693 [-100.000,  7.867], mean action: 1.632 [0.000, 3.000],  loss: 6.376726, mae: 42.310978, mean_q: 48.446708, mean_eps: 0.820795
  29996/150000: episode: 320, duration: 0.571s, episode steps:  94, steps per second: 164, episode reward: -74.430, mean reward: -0.792 [-100.000, 12.183], mean action: 1.468 [0.000, 3.000],  loss: 22.677946, mae: 42.229724, mean_q: 47.833739, mean_eps: 0.820309
  30072/150000: episode: 321, duration: 0.512s, episode steps:  76, steps per second: 149, episode reward: -65.341, mean reward: -0.860 [-100.000,  9.963], mean action: 1.487 [0.000, 3.000],  loss: 24.182384, mae: 43.763798, mean_q: 49.755086, mean_eps: 0.819799
  30192/150000: episode: 322, duration: 0.771s, episode steps: 120, steps per second: 156, episode reward: -81.749, mean reward: -0.681 [-100.000,  6.847], mean action: 1.508 [0.000, 3.000],  loss: 22.321330, mae: 44.087871, mean_q: 48.706395, mean_eps: 0.819211
  30279/150000: episode: 323, duration: 0.556s, episode steps:  87, steps per second: 157, episode reward: -82.829, mean reward: -0.952 [-100.000, 11.726], mean action: 1.529 [0.000, 3.000],  loss: 10.351535, mae: 44.449891, mean_q: 50.387557, mean_eps: 0.818590
  30384/150000: episode: 324, duration: 0.643s, episode steps: 105, steps per second: 163, episode reward: -98.558, mean reward: -0.939 [-100.000,  9.392], mean action: 1.429 [0.000, 3.000],  loss: 6.384248, mae: 44.225188, mean_q: 50.590505, mean_eps: 0.818014
  30458/150000: episode: 325, duration: 0.452s, episode steps:  74, steps per second: 164, episode reward: -73.989, mean reward: -1.000 [-100.000, 11.874], mean action: 1.608 [0.000, 3.000],  loss: 5.987052, mae: 43.722203, mean_q: 49.421590, mean_eps: 0.817477
  30579/150000: episode: 326, duration: 0.779s, episode steps: 121, steps per second: 155, episode reward: -145.164, mean reward: -1.200 [-100.000, 16.152], mean action: 1.603 [0.000, 3.000],  loss: 13.716821, mae: 44.177530, mean_q: 50.073610, mean_eps: 0.816892
  30648/150000: episode: 327, duration: 0.423s, episode steps:  69, steps per second: 163, episode reward: -76.288, mean reward: -1.106 [-100.000,  6.344], mean action: 1.797 [0.000, 3.000],  loss: 17.108936, mae: 44.711784, mean_q: 51.120661, mean_eps: 0.816322
  30751/150000: episode: 328, duration: 0.641s, episode steps: 103, steps per second: 161, episode reward: -64.791, mean reward: -0.629 [-100.000, 13.187], mean action: 1.573 [0.000, 3.000],  loss: 16.907725, mae: 43.867065, mean_q: 51.058761, mean_eps: 0.815806
  30849/150000: episode: 329, duration: 0.598s, episode steps:  98, steps per second: 164, episode reward: -118.251, mean reward: -1.207 [-100.000,  5.897], mean action: 1.480 [0.000, 3.000],  loss: 18.411799, mae: 43.873232, mean_q: 49.252883, mean_eps: 0.815203
  30964/150000: episode: 330, duration: 0.729s, episode steps: 115, steps per second: 158, episode reward: -107.095, mean reward: -0.931 [-100.000,  6.082], mean action: 1.539 [0.000, 3.000],  loss: 8.159166, mae: 44.759548, mean_q: 51.287285, mean_eps: 0.814564
  31089/150000: episode: 331, duration: 0.789s, episode steps: 125, steps per second: 158, episode reward: -146.652, mean reward: -1.173 [-100.000,  5.447], mean action: 1.560 [0.000, 3.000],  loss: 11.637239, mae: 44.560745, mean_q: 49.770366, mean_eps: 0.813844
  31179/150000: episode: 332, duration: 0.548s, episode steps:  90, steps per second: 164, episode reward: -94.390, mean reward: -1.049 [-100.000, 17.095], mean action: 1.522 [0.000, 3.000],  loss: 21.220909, mae: 44.116090, mean_q: 50.168986, mean_eps: 0.813199
  31321/150000: episode: 333, duration: 0.967s, episode steps: 142, steps per second: 147, episode reward: -44.980, mean reward: -0.317 [-100.000, 16.515], mean action: 1.570 [0.000, 3.000],  loss: 15.502733, mae: 44.767561, mean_q: 50.329581, mean_eps: 0.812503
  31421/150000: episode: 334, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: -85.104, mean reward: -0.851 [-100.000,  8.721], mean action: 1.600 [0.000, 3.000],  loss: 9.069665, mae: 44.655432, mean_q: 50.471025, mean_eps: 0.811777
  31495/150000: episode: 335, duration: 0.523s, episode steps:  74, steps per second: 142, episode reward: -144.647, mean reward: -1.955 [-100.000, 18.018], mean action: 1.419 [0.000, 3.000],  loss: 16.372498, mae: 44.596373, mean_q: 50.003739, mean_eps: 0.811255
  31609/150000: episode: 336, duration: 0.853s, episode steps: 114, steps per second: 134, episode reward: -83.895, mean reward: -0.736 [-100.000, 10.102], mean action: 1.658 [0.000, 3.000],  loss: 4.916568, mae: 44.025599, mean_q: 50.449223, mean_eps: 0.810691
  31709/150000: episode: 337, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -89.298, mean reward: -0.893 [-100.000, 10.150], mean action: 1.530 [0.000, 3.000],  loss: 4.126640, mae: 44.592141, mean_q: 51.036152, mean_eps: 0.810049
  31807/150000: episode: 338, duration: 0.784s, episode steps:  98, steps per second: 125, episode reward: -78.911, mean reward: -0.805 [-100.000, 22.772], mean action: 1.663 [0.000, 3.000],  loss: 16.695726, mae: 44.726063, mean_q: 51.214928, mean_eps: 0.809455
  31915/150000: episode: 339, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -125.567, mean reward: -1.163 [-100.000,  6.297], mean action: 1.519 [0.000, 3.000],  loss: 24.253301, mae: 45.655413, mean_q: 51.828715, mean_eps: 0.808837
  31993/150000: episode: 340, duration: 0.491s, episode steps:  78, steps per second: 159, episode reward: -64.479, mean reward: -0.827 [-100.000, 11.995], mean action: 1.885 [0.000, 3.000],  loss: 22.025220, mae: 45.125985, mean_q: 50.827904, mean_eps: 0.808279
  32081/150000: episode: 341, duration: 0.540s, episode steps:  88, steps per second: 163, episode reward: -250.914, mean reward: -2.851 [-100.000, 59.996], mean action: 1.636 [0.000, 3.000],  loss: 6.972584, mae: 46.802416, mean_q: 52.114257, mean_eps: 0.807781
  32150/150000: episode: 342, duration: 0.440s, episode steps:  69, steps per second: 157, episode reward: -226.912, mean reward: -3.289 [-100.000,  4.536], mean action: 1.580 [0.000, 3.000],  loss: 11.754014, mae: 45.231313, mean_q: 50.520324, mean_eps: 0.807310
  32223/150000: episode: 343, duration: 0.478s, episode steps:  73, steps per second: 153, episode reward: -59.332, mean reward: -0.813 [-100.000, 13.240], mean action: 1.589 [0.000, 3.000],  loss: 12.714202, mae: 47.208078, mean_q: 53.005711, mean_eps: 0.806884
  32314/150000: episode: 344, duration: 0.555s, episode steps:  91, steps per second: 164, episode reward: -88.743, mean reward: -0.975 [-100.000,  6.885], mean action: 1.396 [0.000, 3.000],  loss: 15.501508, mae: 46.063173, mean_q: 51.078314, mean_eps: 0.806392
  32370/150000: episode: 345, duration: 0.344s, episode steps:  56, steps per second: 163, episode reward: -77.093, mean reward: -1.377 [-100.000,  9.017], mean action: 1.750 [0.000, 3.000],  loss: 17.467049, mae: 46.075322, mean_q: 51.285868, mean_eps: 0.805951
  32435/150000: episode: 346, duration: 0.396s, episode steps:  65, steps per second: 164, episode reward: -93.739, mean reward: -1.442 [-100.000,  5.181], mean action: 1.323 [0.000, 3.000],  loss: 13.288016, mae: 46.782871, mean_q: 51.497944, mean_eps: 0.805588
  32495/150000: episode: 347, duration: 0.408s, episode steps:  60, steps per second: 147, episode reward: -48.509, mean reward: -0.808 [-100.000, 91.617], mean action: 1.517 [0.000, 3.000],  loss: 29.879275, mae: 45.754410, mean_q: 50.432734, mean_eps: 0.805213
  32562/150000: episode: 348, duration: 0.403s, episode steps:  67, steps per second: 166, episode reward: -113.648, mean reward: -1.696 [-100.000,  6.552], mean action: 1.537 [0.000, 3.000],  loss: 6.475418, mae: 46.173794, mean_q: 52.265199, mean_eps: 0.804832
  32646/150000: episode: 349, duration: 0.525s, episode steps:  84, steps per second: 160, episode reward: -120.562, mean reward: -1.435 [-100.000,  6.971], mean action: 1.452 [0.000, 3.000],  loss: 21.930062, mae: 45.882921, mean_q: 50.611918, mean_eps: 0.804379
  32729/150000: episode: 350, duration: 0.506s, episode steps:  83, steps per second: 164, episode reward: -52.553, mean reward: -0.633 [-100.000, 13.208], mean action: 1.530 [0.000, 3.000],  loss: 8.450151, mae: 46.910818, mean_q: 51.939553, mean_eps: 0.803878
  32824/150000: episode: 351, duration: 0.590s, episode steps:  95, steps per second: 161, episode reward: -46.178, mean reward: -0.486 [-100.000, 10.805], mean action: 1.705 [0.000, 3.000],  loss: 22.007673, mae: 46.428234, mean_q: 51.223622, mean_eps: 0.803344
  32893/150000: episode: 352, duration: 0.452s, episode steps:  69, steps per second: 153, episode reward: -85.976, mean reward: -1.246 [-100.000, 11.172], mean action: 1.565 [0.000, 3.000],  loss: 12.253547, mae: 46.000334, mean_q: 50.020547, mean_eps: 0.802852
  32985/150000: episode: 353, duration: 0.566s, episode steps:  92, steps per second: 162, episode reward: -32.528, mean reward: -0.354 [-100.000, 13.440], mean action: 1.717 [0.000, 3.000],  loss: 12.192980, mae: 46.708976, mean_q: 52.647518, mean_eps: 0.802369
  33069/150000: episode: 354, duration: 0.517s, episode steps:  84, steps per second: 163, episode reward: -83.754, mean reward: -0.997 [-100.000, 12.027], mean action: 1.679 [0.000, 3.000],  loss: 7.618411, mae: 46.169468, mean_q: 51.210083, mean_eps: 0.801841
  33149/150000: episode: 355, duration: 0.497s, episode steps:  80, steps per second: 161, episode reward: -78.626, mean reward: -0.983 [-100.000, 10.060], mean action: 1.587 [0.000, 3.000],  loss: 8.152918, mae: 46.010728, mean_q: 50.689793, mean_eps: 0.801349
  33240/150000: episode: 356, duration: 0.583s, episode steps:  91, steps per second: 156, episode reward: -38.644, mean reward: -0.425 [-100.000,  9.150], mean action: 1.549 [0.000, 3.000],  loss: 14.286743, mae: 46.454976, mean_q: 51.707874, mean_eps: 0.800836
  33330/150000: episode: 357, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -88.090, mean reward: -0.979 [-100.000,  7.530], mean action: 1.511 [0.000, 3.000],  loss: 8.133560, mae: 46.780036, mean_q: 51.381746, mean_eps: 0.800293
  33435/150000: episode: 358, duration: 0.638s, episode steps: 105, steps per second: 164, episode reward: -85.585, mean reward: -0.815 [-100.000, 13.717], mean action: 1.543 [0.000, 3.000],  loss: 9.638579, mae: 45.923350, mean_q: 50.469293, mean_eps: 0.799708
  33583/150000: episode: 359, duration: 0.942s, episode steps: 148, steps per second: 157, episode reward: -46.393, mean reward: -0.313 [-100.000, 19.870], mean action: 1.486 [0.000, 3.000],  loss: 22.948077, mae: 46.228292, mean_q: 51.217334, mean_eps: 0.798949
  33680/150000: episode: 360, duration: 0.599s, episode steps:  97, steps per second: 162, episode reward: -99.071, mean reward: -1.021 [-100.000,  8.002], mean action: 1.515 [0.000, 3.000],  loss: 26.112122, mae: 46.544421, mean_q: 51.441909, mean_eps: 0.798214
  33759/150000: episode: 361, duration: 0.483s, episode steps:  79, steps per second: 164, episode reward: -96.566, mean reward: -1.222 [-100.000, 11.927], mean action: 1.658 [0.000, 3.000],  loss: 13.033191, mae: 47.172375, mean_q: 52.781692, mean_eps: 0.797686
  33831/150000: episode: 362, duration: 0.443s, episode steps:  72, steps per second: 163, episode reward: -75.470, mean reward: -1.048 [-100.000, 15.202], mean action: 1.583 [0.000, 3.000],  loss: 7.344006, mae: 46.713563, mean_q: 51.829749, mean_eps: 0.797233
  33901/150000: episode: 363, duration: 0.462s, episode steps:  70, steps per second: 152, episode reward: -39.746, mean reward: -0.568 [-100.000, 12.598], mean action: 1.557 [0.000, 3.000],  loss: 21.934560, mae: 47.075689, mean_q: 52.187318, mean_eps: 0.796807
  34014/150000: episode: 364, duration: 0.714s, episode steps: 113, steps per second: 158, episode reward: -97.638, mean reward: -0.864 [-100.000, 10.164], mean action: 1.531 [0.000, 3.000],  loss: 11.322989, mae: 47.139737, mean_q: 52.465898, mean_eps: 0.796258
  34147/150000: episode: 365, duration: 0.830s, episode steps: 133, steps per second: 160, episode reward: -88.082, mean reward: -0.662 [-100.000, 13.993], mean action: 1.699 [0.000, 3.000],  loss: 8.269905, mae: 46.281646, mean_q: 51.126819, mean_eps: 0.795520
  34220/150000: episode: 366, duration: 0.465s, episode steps:  73, steps per second: 157, episode reward: -93.593, mean reward: -1.282 [-100.000,  6.706], mean action: 1.411 [0.000, 3.000],  loss: 17.178204, mae: 47.410393, mean_q: 54.344262, mean_eps: 0.794902
  34294/150000: episode: 367, duration: 0.492s, episode steps:  74, steps per second: 150, episode reward: -43.516, mean reward: -0.588 [-100.000, 11.702], mean action: 1.568 [0.000, 3.000],  loss: 20.550735, mae: 46.569046, mean_q: 51.119452, mean_eps: 0.794461
  34451/150000: episode: 368, duration: 0.986s, episode steps: 157, steps per second: 159, episode reward: -46.218, mean reward: -0.294 [-100.000, 25.798], mean action: 1.503 [0.000, 3.000],  loss: 16.110656, mae: 47.387786, mean_q: 53.388461, mean_eps: 0.793768
  34565/150000: episode: 369, duration: 0.718s, episode steps: 114, steps per second: 159, episode reward: -49.009, mean reward: -0.430 [-100.000, 42.327], mean action: 1.640 [0.000, 3.000],  loss: 5.400476, mae: 47.524398, mean_q: 52.560270, mean_eps: 0.792955
  34657/150000: episode: 370, duration: 0.578s, episode steps:  92, steps per second: 159, episode reward: -84.775, mean reward: -0.921 [-100.000, 10.590], mean action: 1.402 [0.000, 3.000],  loss: 21.950632, mae: 47.392597, mean_q: 52.271148, mean_eps: 0.792337
  34768/150000: episode: 371, duration: 0.683s, episode steps: 111, steps per second: 162, episode reward: -98.683, mean reward: -0.889 [-100.000, 17.035], mean action: 1.486 [0.000, 3.000],  loss: 8.237368, mae: 47.062606, mean_q: 52.224875, mean_eps: 0.791728
  34868/150000: episode: 372, duration: 0.642s, episode steps: 100, steps per second: 156, episode reward: -107.876, mean reward: -1.079 [-100.000, 10.343], mean action: 1.470 [0.000, 3.000],  loss: 15.254472, mae: 46.729736, mean_q: 50.917502, mean_eps: 0.791095
  34966/150000: episode: 373, duration: 0.637s, episode steps:  98, steps per second: 154, episode reward: -87.887, mean reward: -0.897 [-100.000,  6.650], mean action: 1.459 [0.000, 3.000],  loss: 6.153568, mae: 47.199280, mean_q: 53.100106, mean_eps: 0.790501
  35045/150000: episode: 374, duration: 0.500s, episode steps:  79, steps per second: 158, episode reward: -114.398, mean reward: -1.448 [-100.000, 32.668], mean action: 1.532 [0.000, 3.000],  loss: 26.552853, mae: 47.854625, mean_q: 52.490293, mean_eps: 0.789970
  35125/150000: episode: 375, duration: 0.487s, episode steps:  80, steps per second: 164, episode reward: -103.007, mean reward: -1.288 [-100.000,  6.975], mean action: 1.637 [0.000, 3.000],  loss: 8.696569, mae: 47.852771, mean_q: 52.391351, mean_eps: 0.789493
  35235/150000: episode: 376, duration: 0.679s, episode steps: 110, steps per second: 162, episode reward: -58.299, mean reward: -0.530 [-100.000,  7.346], mean action: 1.500 [0.000, 3.000],  loss: 11.394725, mae: 47.651517, mean_q: 53.265115, mean_eps: 0.788923
  35332/150000: episode: 377, duration: 0.620s, episode steps:  97, steps per second: 156, episode reward: -127.207, mean reward: -1.311 [-100.000,  8.409], mean action: 1.443 [0.000, 3.000],  loss: 8.827485, mae: 48.003608, mean_q: 53.313032, mean_eps: 0.788302
  35444/150000: episode: 378, duration: 0.681s, episode steps: 112, steps per second: 164, episode reward: -113.108, mean reward: -1.010 [-100.000, 12.970], mean action: 1.411 [0.000, 3.000],  loss: 30.211126, mae: 47.940557, mean_q: 55.080581, mean_eps: 0.787675
  35564/150000: episode: 379, duration: 0.747s, episode steps: 120, steps per second: 161, episode reward: -64.610, mean reward: -0.538 [-100.000, 16.319], mean action: 1.617 [0.000, 3.000],  loss: 21.203322, mae: 48.151868, mean_q: 53.158011, mean_eps: 0.786979
  35631/150000: episode: 380, duration: 0.435s, episode steps:  67, steps per second: 154, episode reward: -95.041, mean reward: -1.419 [-100.000,  5.474], mean action: 1.642 [0.000, 3.000],  loss: 38.822123, mae: 48.533737, mean_q: 53.365492, mean_eps: 0.786418
  35713/150000: episode: 381, duration: 0.518s, episode steps:  82, steps per second: 158, episode reward: -84.549, mean reward: -1.031 [-100.000,  6.627], mean action: 1.549 [0.000, 3.000],  loss: 7.791857, mae: 48.674818, mean_q: 54.732359, mean_eps: 0.785971
  35817/150000: episode: 382, duration: 0.636s, episode steps: 104, steps per second: 163, episode reward: -102.045, mean reward: -0.981 [-100.000,  7.528], mean action: 1.452 [0.000, 3.000],  loss: 18.918759, mae: 48.386858, mean_q: 55.187434, mean_eps: 0.785413
  35903/150000: episode: 383, duration: 0.524s, episode steps:  86, steps per second: 164, episode reward: -111.526, mean reward: -1.297 [-100.000,  9.094], mean action: 1.477 [0.000, 3.000],  loss: 8.276347, mae: 47.719181, mean_q: 54.554686, mean_eps: 0.784843
  35980/150000: episode: 384, duration: 0.510s, episode steps:  77, steps per second: 151, episode reward: -100.930, mean reward: -1.311 [-100.000, 15.259], mean action: 1.623 [0.000, 3.000],  loss: 26.357942, mae: 48.776405, mean_q: 53.040689, mean_eps: 0.784354
  36066/150000: episode: 385, duration: 0.541s, episode steps:  86, steps per second: 159, episode reward: -87.290, mean reward: -1.015 [-100.000,  8.955], mean action: 1.535 [0.000, 3.000],  loss: 6.334070, mae: 48.776171, mean_q: 55.584821, mean_eps: 0.783865
  36168/150000: episode: 386, duration: 0.625s, episode steps: 102, steps per second: 163, episode reward: -30.032, mean reward: -0.294 [-100.000, 16.409], mean action: 1.539 [0.000, 3.000],  loss: 20.819415, mae: 48.966906, mean_q: 56.103851, mean_eps: 0.783301
  36281/150000: episode: 387, duration: 0.832s, episode steps: 113, steps per second: 136, episode reward: -178.434, mean reward: -1.579 [-100.000,  9.928], mean action: 1.566 [0.000, 3.000],  loss: 12.397604, mae: 48.724868, mean_q: 54.708475, mean_eps: 0.782656
  36376/150000: episode: 388, duration: 0.619s, episode steps:  95, steps per second: 153, episode reward: -23.142, mean reward: -0.244 [-100.000, 47.132], mean action: 1.579 [0.000, 3.000],  loss: 9.594137, mae: 48.333077, mean_q: 53.569811, mean_eps: 0.782032
  36490/150000: episode: 389, duration: 0.710s, episode steps: 114, steps per second: 160, episode reward: -65.840, mean reward: -0.578 [-100.000, 13.874], mean action: 1.535 [0.000, 3.000],  loss: 13.841154, mae: 49.215789, mean_q: 55.708030, mean_eps: 0.781405
  36579/150000: episode: 390, duration: 0.552s, episode steps:  89, steps per second: 161, episode reward: -153.693, mean reward: -1.727 [-100.000, 14.367], mean action: 1.697 [0.000, 3.000],  loss: 19.952834, mae: 49.642287, mean_q: 55.756228, mean_eps: 0.780796
  36677/150000: episode: 391, duration: 0.634s, episode steps:  98, steps per second: 155, episode reward: -94.092, mean reward: -0.960 [-100.000,  8.137], mean action: 1.602 [0.000, 3.000],  loss: 18.784226, mae: 48.262148, mean_q: 54.743552, mean_eps: 0.780235
  36799/150000: episode: 392, duration: 0.750s, episode steps: 122, steps per second: 163, episode reward: -80.614, mean reward: -0.661 [-100.000,  7.138], mean action: 1.639 [0.000, 3.000],  loss: 16.845225, mae: 48.961984, mean_q: 55.913113, mean_eps: 0.779575
  36861/150000: episode: 393, duration: 0.378s, episode steps:  62, steps per second: 164, episode reward: -72.825, mean reward: -1.175 [-100.000, 26.943], mean action: 1.710 [0.000, 3.000],  loss: 30.951264, mae: 48.686719, mean_q: 56.430276, mean_eps: 0.779023
  36944/150000: episode: 394, duration: 0.655s, episode steps:  83, steps per second: 127, episode reward: -86.710, mean reward: -1.045 [-100.000,  7.498], mean action: 1.651 [0.000, 3.000],  loss: 9.843418, mae: 49.925776, mean_q: 56.775660, mean_eps: 0.778588
  37039/150000: episode: 395, duration: 0.743s, episode steps:  95, steps per second: 128, episode reward: -58.108, mean reward: -0.612 [-100.000, 15.481], mean action: 1.621 [0.000, 3.000],  loss: 12.529637, mae: 50.034472, mean_q: 55.923276, mean_eps: 0.778054
  37125/150000: episode: 396, duration: 0.575s, episode steps:  86, steps per second: 150, episode reward: -88.782, mean reward: -1.032 [-100.000, 17.813], mean action: 1.640 [0.000, 3.000],  loss: 12.607923, mae: 49.477432, mean_q: 55.857554, mean_eps: 0.777511
  37209/150000: episode: 397, duration: 0.564s, episode steps:  84, steps per second: 149, episode reward: -89.118, mean reward: -1.061 [-100.000,  6.527], mean action: 1.702 [0.000, 3.000],  loss: 25.029078, mae: 50.576125, mean_q: 56.374459, mean_eps: 0.777001
  37310/150000: episode: 398, duration: 0.711s, episode steps: 101, steps per second: 142, episode reward: -173.553, mean reward: -1.718 [-100.000,  7.812], mean action: 1.515 [0.000, 3.000],  loss: 15.490284, mae: 49.539570, mean_q: 55.562191, mean_eps: 0.776446
  37401/150000: episode: 399, duration: 0.636s, episode steps:  91, steps per second: 143, episode reward: -97.304, mean reward: -1.069 [-100.000, 11.823], mean action: 1.484 [0.000, 3.000],  loss: 23.853275, mae: 50.043411, mean_q: 55.394127, mean_eps: 0.775870
  37511/150000: episode: 400, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -88.832, mean reward: -0.808 [-100.000, 14.092], mean action: 1.418 [0.000, 3.000],  loss: 12.749112, mae: 49.869631, mean_q: 56.564380, mean_eps: 0.775267
  37630/150000: episode: 401, duration: 0.934s, episode steps: 119, steps per second: 127, episode reward: -130.424, mean reward: -1.096 [-100.000, 11.506], mean action: 1.597 [0.000, 3.000],  loss: 18.720495, mae: 49.516084, mean_q: 55.430012, mean_eps: 0.774580
  37725/150000: episode: 402, duration: 0.641s, episode steps:  95, steps per second: 148, episode reward: -127.216, mean reward: -1.339 [-100.000,  8.516], mean action: 1.474 [0.000, 3.000],  loss: 18.732808, mae: 48.504305, mean_q: 54.890145, mean_eps: 0.773938
  37800/150000: episode: 403, duration: 0.530s, episode steps:  75, steps per second: 141, episode reward: -86.495, mean reward: -1.153 [-100.000,  7.056], mean action: 1.653 [0.000, 3.000],  loss: 27.085800, mae: 48.969632, mean_q: 54.203132, mean_eps: 0.773428
  37927/150000: episode: 404, duration: 0.919s, episode steps: 127, steps per second: 138, episode reward: -62.938, mean reward: -0.496 [-100.000, 13.421], mean action: 1.559 [0.000, 3.000],  loss: 10.907882, mae: 50.058748, mean_q: 54.727849, mean_eps: 0.772822
  38016/150000: episode: 405, duration: 0.583s, episode steps:  89, steps per second: 153, episode reward: -178.214, mean reward: -2.002 [-100.000,  5.765], mean action: 1.528 [0.000, 3.000],  loss: 6.786281, mae: 51.231604, mean_q: 57.469756, mean_eps: 0.772174
  38121/150000: episode: 406, duration: 0.683s, episode steps: 105, steps per second: 154, episode reward: -149.188, mean reward: -1.421 [-100.000,  6.023], mean action: 1.848 [0.000, 3.000],  loss: 11.584156, mae: 50.438402, mean_q: 54.604320, mean_eps: 0.771592
  38246/150000: episode: 407, duration: 0.857s, episode steps: 125, steps per second: 146, episode reward: -137.264, mean reward: -1.098 [-100.000, 18.586], mean action: 1.608 [0.000, 3.000],  loss: 17.221498, mae: 50.360724, mean_q: 54.103400, mean_eps: 0.770902
  38365/150000: episode: 408, duration: 0.743s, episode steps: 119, steps per second: 160, episode reward: -126.549, mean reward: -1.063 [-100.000,  7.182], mean action: 1.773 [0.000, 3.000],  loss: 11.375315, mae: 50.240971, mean_q: 56.152672, mean_eps: 0.770170
  38462/150000: episode: 409, duration: 0.594s, episode steps:  97, steps per second: 163, episode reward: -106.580, mean reward: -1.099 [-100.000, 18.238], mean action: 1.680 [0.000, 3.000],  loss: 10.181394, mae: 50.105858, mean_q: 54.670054, mean_eps: 0.769522
  38553/150000: episode: 410, duration: 0.606s, episode steps:  91, steps per second: 150, episode reward: -79.327, mean reward: -0.872 [-100.000, 12.765], mean action: 1.495 [0.000, 3.000],  loss: 6.750225, mae: 50.354734, mean_q: 56.392009, mean_eps: 0.768958
  38664/150000: episode: 411, duration: 0.687s, episode steps: 111, steps per second: 162, episode reward: -81.511, mean reward: -0.734 [-100.000, 11.249], mean action: 1.541 [0.000, 3.000],  loss: 12.295374, mae: 50.356094, mean_q: 55.135086, mean_eps: 0.768352
  38731/150000: episode: 412, duration: 0.411s, episode steps:  67, steps per second: 163, episode reward: -131.025, mean reward: -1.956 [-100.000, 16.136], mean action: 1.537 [0.000, 3.000],  loss: 10.269382, mae: 51.115348, mean_q: 55.421672, mean_eps: 0.767818
  38857/150000: episode: 413, duration: 0.790s, episode steps: 126, steps per second: 159, episode reward: -59.625, mean reward: -0.473 [-100.000,  7.929], mean action: 1.563 [0.000, 3.000],  loss: 17.263788, mae: 50.312594, mean_q: 56.542598, mean_eps: 0.767239
  38986/150000: episode: 414, duration: 0.822s, episode steps: 129, steps per second: 157, episode reward: -116.887, mean reward: -0.906 [-100.000,  8.753], mean action: 1.543 [0.000, 3.000],  loss: 7.064165, mae: 50.155958, mean_q: 54.406058, mean_eps: 0.766474
  39060/150000: episode: 415, duration: 0.471s, episode steps:  74, steps per second: 157, episode reward: -110.582, mean reward: -1.494 [-100.000, 13.427], mean action: 1.635 [0.000, 3.000],  loss: 4.878961, mae: 50.932161, mean_q: 56.257465, mean_eps: 0.765865
  39200/150000: episode: 416, duration: 0.900s, episode steps: 140, steps per second: 156, episode reward: -162.016, mean reward: -1.157 [-100.000,  5.816], mean action: 1.457 [0.000, 3.000],  loss: 18.850839, mae: 51.067101, mean_q: 55.589129, mean_eps: 0.765223
  39295/150000: episode: 417, duration: 0.597s, episode steps:  95, steps per second: 159, episode reward: -144.088, mean reward: -1.517 [-100.000,  8.577], mean action: 1.421 [0.000, 3.000],  loss: 11.792335, mae: 51.963107, mean_q: 56.553264, mean_eps: 0.764518
  39423/150000: episode: 418, duration: 0.781s, episode steps: 128, steps per second: 164, episode reward: -88.023, mean reward: -0.688 [-100.000, 19.856], mean action: 1.398 [0.000, 3.000],  loss: 17.308997, mae: 50.902122, mean_q: 56.226480, mean_eps: 0.763849
  39499/150000: episode: 419, duration: 0.468s, episode steps:  76, steps per second: 162, episode reward: -87.979, mean reward: -1.158 [-100.000, 12.741], mean action: 1.671 [0.000, 3.000],  loss: 7.216069, mae: 50.496647, mean_q: 55.089652, mean_eps: 0.763237
  39593/150000: episode: 420, duration: 0.606s, episode steps:  94, steps per second: 155, episode reward: -134.285, mean reward: -1.429 [-100.000, 10.503], mean action: 1.574 [0.000, 3.000],  loss: 7.783380, mae: 50.959960, mean_q: 56.342087, mean_eps: 0.762727
  39685/150000: episode: 421, duration: 0.571s, episode steps:  92, steps per second: 161, episode reward: -9.741, mean reward: -0.106 [-100.000, 17.709], mean action: 1.500 [0.000, 3.000],  loss: 8.169857, mae: 51.480210, mean_q: 57.568081, mean_eps: 0.762169
  39805/150000: episode: 422, duration: 0.733s, episode steps: 120, steps per second: 164, episode reward: -104.958, mean reward: -0.875 [-100.000, 15.208], mean action: 1.467 [0.000, 3.000],  loss: 18.452353, mae: 51.227193, mean_q: 57.093572, mean_eps: 0.761533
  39915/150000: episode: 423, duration: 0.681s, episode steps: 110, steps per second: 162, episode reward: -101.505, mean reward: -0.923 [-100.000,  6.327], mean action: 1.636 [0.000, 3.000],  loss: 19.278410, mae: 50.927054, mean_q: 56.752162, mean_eps: 0.760843
  40040/150000: episode: 424, duration: 0.772s, episode steps: 125, steps per second: 162, episode reward: -85.090, mean reward: -0.681 [-100.000,  5.777], mean action: 1.432 [0.000, 3.000],  loss: 9.344609, mae: 51.359929, mean_q: 55.043160, mean_eps: 0.760138
  40130/150000: episode: 425, duration: 0.558s, episode steps:  90, steps per second: 161, episode reward: -61.521, mean reward: -0.684 [-100.000,  6.905], mean action: 1.667 [0.000, 3.000],  loss: 6.860638, mae: 51.626899, mean_q: 54.531076, mean_eps: 0.759493
  40202/150000: episode: 426, duration: 0.437s, episode steps:  72, steps per second: 165, episode reward: -65.619, mean reward: -0.911 [-100.000, 19.283], mean action: 1.444 [0.000, 3.000],  loss: 6.398769, mae: 52.515604, mean_q: 57.886077, mean_eps: 0.759007
  40287/150000: episode: 427, duration: 0.552s, episode steps:  85, steps per second: 154, episode reward: -66.817, mean reward: -0.786 [-100.000, 20.266], mean action: 1.612 [0.000, 3.000],  loss: 22.805661, mae: 52.056939, mean_q: 56.244235, mean_eps: 0.758536
  40386/150000: episode: 428, duration: 0.601s, episode steps:  99, steps per second: 165, episode reward: -51.343, mean reward: -0.519 [-100.000, 14.319], mean action: 1.495 [0.000, 3.000],  loss: 8.441054, mae: 52.021229, mean_q: 57.101563, mean_eps: 0.757984
  40463/150000: episode: 429, duration: 0.476s, episode steps:  77, steps per second: 162, episode reward: -80.381, mean reward: -1.044 [-100.000,  6.002], mean action: 1.831 [0.000, 3.000],  loss: 11.166034, mae: 52.822929, mean_q: 56.109786, mean_eps: 0.757456
  40534/150000: episode: 430, duration: 0.432s, episode steps:  71, steps per second: 164, episode reward: -55.891, mean reward: -0.787 [-100.000,  9.622], mean action: 1.718 [0.000, 3.000],  loss: 12.928557, mae: 52.201653, mean_q: 57.125042, mean_eps: 0.757012
  40617/150000: episode: 431, duration: 0.553s, episode steps:  83, steps per second: 150, episode reward: -210.465, mean reward: -2.536 [-100.000,  8.251], mean action: 1.807 [0.000, 3.000],  loss: 22.923939, mae: 52.711773, mean_q: 57.917591, mean_eps: 0.756550
  40743/150000: episode: 432, duration: 0.773s, episode steps: 126, steps per second: 163, episode reward: -49.682, mean reward: -0.394 [-100.000, 11.165], mean action: 1.611 [0.000, 3.000],  loss: 13.348045, mae: 52.836058, mean_q: 57.605166, mean_eps: 0.755923
  40818/150000: episode: 433, duration: 0.456s, episode steps:  75, steps per second: 164, episode reward: -27.512, mean reward: -0.367 [-100.000, 95.750], mean action: 1.600 [0.000, 3.000],  loss: 7.181886, mae: 53.020092, mean_q: 57.445440, mean_eps: 0.755320
  40907/150000: episode: 434, duration: 0.543s, episode steps:  89, steps per second: 164, episode reward: -11.114, mean reward: -0.125 [-100.000, 21.415], mean action: 1.629 [0.000, 3.000],  loss: 5.782507, mae: 53.083084, mean_q: 58.063600, mean_eps: 0.754828
  41012/150000: episode: 435, duration: 0.672s, episode steps: 105, steps per second: 156, episode reward: -49.973, mean reward: -0.476 [-100.000, 13.294], mean action: 1.581 [0.000, 3.000],  loss: 30.753870, mae: 52.928143, mean_q: 58.067289, mean_eps: 0.754246
  41120/150000: episode: 436, duration: 0.667s, episode steps: 108, steps per second: 162, episode reward: -90.774, mean reward: -0.841 [-100.000,  9.216], mean action: 1.481 [0.000, 3.000],  loss: 10.655320, mae: 53.241833, mean_q: 57.279150, mean_eps: 0.753607
  41244/150000: episode: 437, duration: 0.758s, episode steps: 124, steps per second: 164, episode reward: -56.370, mean reward: -0.455 [-100.000, 11.780], mean action: 1.669 [0.000, 3.000],  loss: 17.065739, mae: 52.976675, mean_q: 58.663800, mean_eps: 0.752911
  41341/150000: episode: 438, duration: 0.610s, episode steps:  97, steps per second: 159, episode reward: -60.203, mean reward: -0.621 [-100.000, 13.497], mean action: 1.505 [0.000, 3.000],  loss: 6.551196, mae: 52.896729, mean_q: 57.944834, mean_eps: 0.752248
  41429/150000: episode: 439, duration: 0.547s, episode steps:  88, steps per second: 161, episode reward: -121.161, mean reward: -1.377 [-100.000,  4.946], mean action: 1.602 [0.000, 3.000],  loss: 18.698727, mae: 52.971549, mean_q: 57.782673, mean_eps: 0.751693
  41528/150000: episode: 440, duration: 0.645s, episode steps:  99, steps per second: 153, episode reward: -207.549, mean reward: -2.096 [-100.000,  5.034], mean action: 1.596 [0.000, 3.000],  loss: 8.897395, mae: 53.625489, mean_q: 57.838026, mean_eps: 0.751132
  41636/150000: episode: 441, duration: 0.840s, episode steps: 108, steps per second: 129, episode reward: -103.707, mean reward: -0.960 [-100.000,  7.716], mean action: 1.630 [0.000, 3.000],  loss: 9.395579, mae: 53.341859, mean_q: 58.409726, mean_eps: 0.750511
  41709/150000: episode: 442, duration: 0.546s, episode steps:  73, steps per second: 134, episode reward: -71.891, mean reward: -0.985 [-100.000,  9.083], mean action: 1.548 [0.000, 3.000],  loss: 6.176837, mae: 53.623616, mean_q: 58.659632, mean_eps: 0.749968
  41793/150000: episode: 443, duration: 0.585s, episode steps:  84, steps per second: 144, episode reward: -82.890, mean reward: -0.987 [-100.000,  6.522], mean action: 1.726 [0.000, 3.000],  loss: 12.731334, mae: 52.819366, mean_q: 57.110913, mean_eps: 0.749497
  41905/150000: episode: 444, duration: 0.776s, episode steps: 112, steps per second: 144, episode reward: -89.161, mean reward: -0.796 [-100.000,  8.889], mean action: 1.527 [0.000, 3.000],  loss: 21.763723, mae: 52.823942, mean_q: 57.913603, mean_eps: 0.748909
  41997/150000: episode: 445, duration: 0.649s, episode steps:  92, steps per second: 142, episode reward: -96.186, mean reward: -1.045 [-100.000, 11.209], mean action: 1.609 [0.000, 3.000],  loss: 15.754888, mae: 53.583571, mean_q: 58.989718, mean_eps: 0.748297
  42099/150000: episode: 446, duration: 0.743s, episode steps: 102, steps per second: 137, episode reward: -221.616, mean reward: -2.173 [-100.000,  0.441], mean action: 1.627 [0.000, 3.000],  loss: 6.332875, mae: 55.190773, mean_q: 59.730424, mean_eps: 0.747715
  42247/150000: episode: 447, duration: 1.097s, episode steps: 148, steps per second: 135, episode reward: -64.360, mean reward: -0.435 [-100.000, 27.459], mean action: 1.676 [0.000, 3.000],  loss: 10.794409, mae: 54.371576, mean_q: 58.754625, mean_eps: 0.746965
  42318/150000: episode: 448, duration: 0.452s, episode steps:  71, steps per second: 157, episode reward: -56.716, mean reward: -0.799 [-100.000, 13.049], mean action: 1.634 [0.000, 3.000],  loss: 16.718815, mae: 54.710355, mean_q: 58.593290, mean_eps: 0.746308
  42451/150000: episode: 449, duration: 0.835s, episode steps: 133, steps per second: 159, episode reward: -54.239, mean reward: -0.408 [-100.000, 18.294], mean action: 1.692 [0.000, 3.000],  loss: 17.845556, mae: 54.674417, mean_q: 59.308701, mean_eps: 0.745696
  42576/150000: episode: 450, duration: 0.798s, episode steps: 125, steps per second: 157, episode reward: -14.167, mean reward: -0.113 [-100.000, 93.102], mean action: 1.712 [0.000, 3.000],  loss: 7.865807, mae: 54.879098, mean_q: 59.849121, mean_eps: 0.744922
  42657/150000: episode: 451, duration: 0.498s, episode steps:  81, steps per second: 163, episode reward: -36.451, mean reward: -0.450 [-100.000, 21.699], mean action: 1.617 [0.000, 3.000],  loss: 8.466079, mae: 53.961268, mean_q: 55.745440, mean_eps: 0.744304
  42765/150000: episode: 452, duration: 0.664s, episode steps: 108, steps per second: 163, episode reward: -84.544, mean reward: -0.783 [-100.000,  8.242], mean action: 1.583 [0.000, 3.000],  loss: 8.792255, mae: 53.357298, mean_q: 57.177373, mean_eps: 0.743737
  42830/150000: episode: 453, duration: 0.396s, episode steps:  65, steps per second: 164, episode reward: -31.690, mean reward: -0.488 [-100.000, 16.126], mean action: 1.738 [0.000, 3.000],  loss: 7.386510, mae: 54.071427, mean_q: 58.953611, mean_eps: 0.743218
  42926/150000: episode: 454, duration: 0.624s, episode steps:  96, steps per second: 154, episode reward: -66.884, mean reward: -0.697 [-100.000, 12.762], mean action: 1.615 [0.000, 3.000],  loss: 9.648232, mae: 55.044569, mean_q: 58.904865, mean_eps: 0.742735
  43055/150000: episode: 455, duration: 0.793s, episode steps: 129, steps per second: 163, episode reward: -44.207, mean reward: -0.343 [-100.000, 19.268], mean action: 1.643 [0.000, 3.000],  loss: 8.789360, mae: 55.064932, mean_q: 60.242281, mean_eps: 0.742060
  43121/150000: episode: 456, duration: 0.401s, episode steps:  66, steps per second: 165, episode reward: -65.224, mean reward: -0.988 [-100.000,  9.617], mean action: 1.470 [0.000, 3.000],  loss: 21.283380, mae: 55.368241, mean_q: 62.397369, mean_eps: 0.741475
  43184/150000: episode: 457, duration: 0.389s, episode steps:  63, steps per second: 162, episode reward: -96.353, mean reward: -1.529 [-100.000,  4.810], mean action: 1.714 [0.000, 3.000],  loss: 12.341276, mae: 55.177462, mean_q: 61.202706, mean_eps: 0.741088
  43258/150000: episode: 458, duration: 0.484s, episode steps:  74, steps per second: 153, episode reward: -91.247, mean reward: -1.233 [-100.000, 10.169], mean action: 1.473 [0.000, 3.000],  loss: 9.178973, mae: 55.524229, mean_q: 59.463375, mean_eps: 0.740677
  43381/150000: episode: 459, duration: 0.762s, episode steps: 123, steps per second: 161, episode reward: -62.154, mean reward: -0.505 [-100.000, 10.854], mean action: 1.585 [0.000, 3.000],  loss: 7.258929, mae: 54.555260, mean_q: 58.834099, mean_eps: 0.740086
  43462/150000: episode: 460, duration: 0.493s, episode steps:  81, steps per second: 164, episode reward: -99.777, mean reward: -1.232 [-100.000, 18.611], mean action: 1.272 [0.000, 3.000],  loss: 4.580894, mae: 56.265573, mean_q: 61.719882, mean_eps: 0.739474
  43542/150000: episode: 461, duration: 0.494s, episode steps:  80, steps per second: 162, episode reward: -63.607, mean reward: -0.795 [-100.000,  6.212], mean action: 1.613 [0.000, 3.000],  loss: 19.888477, mae: 54.806309, mean_q: 58.328503, mean_eps: 0.738991
  43616/150000: episode: 462, duration: 0.481s, episode steps:  74, steps per second: 154, episode reward: -24.519, mean reward: -0.331 [-100.000, 19.609], mean action: 1.446 [0.000, 3.000],  loss: 7.120539, mae: 55.403988, mean_q: 58.731298, mean_eps: 0.738529
  43741/150000: episode: 463, duration: 0.776s, episode steps: 125, steps per second: 161, episode reward: -83.355, mean reward: -0.667 [-100.000,  6.740], mean action: 1.776 [0.000, 3.000],  loss: 5.017533, mae: 55.910341, mean_q: 60.531337, mean_eps: 0.737932
  43832/150000: episode: 464, duration: 0.556s, episode steps:  91, steps per second: 164, episode reward: -99.976, mean reward: -1.099 [-100.000,  5.343], mean action: 1.637 [0.000, 3.000],  loss: 13.848933, mae: 56.097035, mean_q: 60.235172, mean_eps: 0.737284
  43918/150000: episode: 465, duration: 0.537s, episode steps:  86, steps per second: 160, episode reward: -15.011, mean reward: -0.175 [-100.000, 21.798], mean action: 1.488 [0.000, 3.000],  loss: 32.641852, mae: 55.169626, mean_q: 59.033005, mean_eps: 0.736753
  44033/150000: episode: 466, duration: 0.736s, episode steps: 115, steps per second: 156, episode reward: -147.092, mean reward: -1.279 [-100.000,  5.167], mean action: 1.704 [0.000, 3.000],  loss: 13.611686, mae: 55.552870, mean_q: 60.761248, mean_eps: 0.736150
  44098/150000: episode: 467, duration: 0.395s, episode steps:  65, steps per second: 165, episode reward: -49.610, mean reward: -0.763 [-100.000, 17.350], mean action: 1.508 [0.000, 3.000],  loss: 7.937898, mae: 56.614418, mean_q: 61.531470, mean_eps: 0.735610
  44179/150000: episode: 468, duration: 0.501s, episode steps:  81, steps per second: 162, episode reward: -95.789, mean reward: -1.183 [-100.000,  7.265], mean action: 1.420 [0.000, 3.000],  loss: 7.546571, mae: 55.457137, mean_q: 59.006504, mean_eps: 0.735172
  44264/150000: episode: 469, duration: 0.539s, episode steps:  85, steps per second: 158, episode reward: -104.989, mean reward: -1.235 [-100.000, 10.781], mean action: 1.635 [0.000, 3.000],  loss: 11.702052, mae: 56.739593, mean_q: 61.636713, mean_eps: 0.734674
  44343/150000: episode: 470, duration: 0.488s, episode steps:  79, steps per second: 162, episode reward: -121.685, mean reward: -1.540 [-100.000,  4.648], mean action: 1.430 [0.000, 3.000],  loss: 16.303735, mae: 56.352887, mean_q: 59.895724, mean_eps: 0.734182
  44422/150000: episode: 471, duration: 0.486s, episode steps:  79, steps per second: 163, episode reward: -96.424, mean reward: -1.221 [-100.000,  8.951], mean action: 1.671 [0.000, 3.000],  loss: 8.660465, mae: 57.016546, mean_q: 62.223457, mean_eps: 0.733708
  44525/150000: episode: 472, duration: 0.627s, episode steps: 103, steps per second: 164, episode reward: -72.173, mean reward: -0.701 [-100.000, 10.485], mean action: 1.534 [0.000, 3.000],  loss: 15.043750, mae: 56.959829, mean_q: 61.568003, mean_eps: 0.733162
  44614/150000: episode: 473, duration: 0.553s, episode steps:  89, steps per second: 161, episode reward: -101.676, mean reward: -1.142 [-100.000, 11.926], mean action: 1.416 [0.000, 3.000],  loss: 27.670803, mae: 55.740680, mean_q: 58.408866, mean_eps: 0.732586
  44728/150000: episode: 474, duration: 0.706s, episode steps: 114, steps per second: 162, episode reward: -112.388, mean reward: -0.986 [-100.000,  6.299], mean action: 1.491 [0.000, 3.000],  loss: 16.148926, mae: 56.546719, mean_q: 61.050291, mean_eps: 0.731977
  44808/150000: episode: 475, duration: 0.487s, episode steps:  80, steps per second: 164, episode reward: -82.907, mean reward: -1.036 [-100.000, 14.191], mean action: 1.462 [0.000, 3.000],  loss: 12.816277, mae: 56.484694, mean_q: 60.133090, mean_eps: 0.731395
  44902/150000: episode: 476, duration: 0.573s, episode steps:  94, steps per second: 164, episode reward: -129.236, mean reward: -1.375 [-100.000,  6.459], mean action: 1.681 [0.000, 3.000],  loss: 23.404224, mae: 56.706746, mean_q: 60.852249, mean_eps: 0.730873
  44974/150000: episode: 477, duration: 0.459s, episode steps:  72, steps per second: 157, episode reward: -80.456, mean reward: -1.117 [-100.000, 10.093], mean action: 1.375 [0.000, 3.000],  loss: 6.666345, mae: 56.478350, mean_q: 60.179876, mean_eps: 0.730375
  45095/150000: episode: 478, duration: 0.746s, episode steps: 121, steps per second: 162, episode reward: -103.839, mean reward: -0.858 [-100.000, 18.616], mean action: 1.463 [0.000, 3.000],  loss: 31.853227, mae: 56.462447, mean_q: 61.166182, mean_eps: 0.729796
  45181/150000: episode: 479, duration: 0.523s, episode steps:  86, steps per second: 165, episode reward: -144.921, mean reward: -1.685 [-100.000, 27.316], mean action: 1.488 [0.000, 3.000],  loss: 12.191134, mae: 56.969328, mean_q: 60.260249, mean_eps: 0.729175
  45271/150000: episode: 480, duration: 0.551s, episode steps:  90, steps per second: 163, episode reward: -108.267, mean reward: -1.203 [-100.000, 10.779], mean action: 1.844 [0.000, 3.000],  loss: 9.675277, mae: 56.451306, mean_q: 59.434957, mean_eps: 0.728647
  45348/150000: episode: 481, duration: 0.498s, episode steps:  77, steps per second: 155, episode reward: -57.195, mean reward: -0.743 [-100.000,  6.414], mean action: 1.844 [0.000, 3.000],  loss: 5.427558, mae: 56.744131, mean_q: 61.735337, mean_eps: 0.728146
  45443/150000: episode: 482, duration: 0.585s, episode steps:  95, steps per second: 162, episode reward: -49.193, mean reward: -0.518 [-100.000,  9.023], mean action: 1.611 [0.000, 3.000],  loss: 8.375705, mae: 57.168906, mean_q: 61.056119, mean_eps: 0.727630
  45578/150000: episode: 483, duration: 0.822s, episode steps: 135, steps per second: 164, episode reward: -125.437, mean reward: -0.929 [-100.000, 15.215], mean action: 1.681 [0.000, 3.000],  loss: 27.267257, mae: 56.821653, mean_q: 61.444088, mean_eps: 0.726940
  45667/150000: episode: 484, duration: 0.564s, episode steps:  89, steps per second: 158, episode reward: -80.405, mean reward: -0.903 [-100.000, 10.625], mean action: 1.674 [0.000, 3.000],  loss: 24.000649, mae: 56.504079, mean_q: 61.602054, mean_eps: 0.726268
  45797/150000: episode: 485, duration: 0.817s, episode steps: 130, steps per second: 159, episode reward: -190.566, mean reward: -1.466 [-100.000, 116.467], mean action: 1.477 [0.000, 3.000],  loss: 8.302056, mae: 56.988669, mean_q: 61.047474, mean_eps: 0.725611
  45908/150000: episode: 486, duration: 0.677s, episode steps: 111, steps per second: 164, episode reward: -73.058, mean reward: -0.658 [-100.000, 11.258], mean action: 1.414 [0.000, 3.000],  loss: 10.576633, mae: 57.493765, mean_q: 61.867736, mean_eps: 0.724888
  45994/150000: episode: 487, duration: 0.527s, episode steps:  86, steps per second: 163, episode reward: -92.763, mean reward: -1.079 [-100.000, 15.847], mean action: 1.302 [0.000, 3.000],  loss: 33.266615, mae: 57.576192, mean_q: 61.796496, mean_eps: 0.724297
  46075/150000: episode: 488, duration: 0.519s, episode steps:  81, steps per second: 156, episode reward: -71.186, mean reward: -0.879 [-100.000,  7.536], mean action: 1.642 [0.000, 3.000],  loss: 11.602640, mae: 57.226749, mean_q: 61.355025, mean_eps: 0.723796
  46198/150000: episode: 489, duration: 0.747s, episode steps: 123, steps per second: 165, episode reward: -30.668, mean reward: -0.249 [-100.000,  8.076], mean action: 1.472 [0.000, 3.000],  loss: 18.762179, mae: 57.008572, mean_q: 60.692561, mean_eps: 0.723184
  46271/150000: episode: 490, duration: 0.454s, episode steps:  73, steps per second: 161, episode reward: -65.067, mean reward: -0.891 [-100.000,  6.364], mean action: 1.452 [0.000, 3.000],  loss: 22.027475, mae: 56.700086, mean_q: 60.549882, mean_eps: 0.722596
  46348/150000: episode: 491, duration: 0.471s, episode steps:  77, steps per second: 164, episode reward: -108.888, mean reward: -1.414 [-100.000, 17.270], mean action: 1.688 [0.000, 3.000],  loss: 9.936868, mae: 56.805321, mean_q: 60.224541, mean_eps: 0.722146
  46413/150000: episode: 492, duration: 0.418s, episode steps:  65, steps per second: 155, episode reward: -23.626, mean reward: -0.363 [-100.000, 23.477], mean action: 1.631 [0.000, 3.000],  loss: 25.079859, mae: 58.058502, mean_q: 62.260091, mean_eps: 0.721720
  46532/150000: episode: 493, duration: 0.730s, episode steps: 119, steps per second: 163, episode reward: -87.684, mean reward: -0.737 [-100.000, 10.157], mean action: 1.513 [0.000, 3.000],  loss: 19.948193, mae: 57.669317, mean_q: 61.127604, mean_eps: 0.721168
  46659/150000: episode: 494, duration: 0.788s, episode steps: 127, steps per second: 161, episode reward: -108.168, mean reward: -0.852 [-100.000, 12.939], mean action: 1.598 [0.000, 3.000],  loss: 7.233159, mae: 58.055046, mean_q: 62.119184, mean_eps: 0.720430
  46725/150000: episode: 495, duration: 0.439s, episode steps:  66, steps per second: 150, episode reward: -107.418, mean reward: -1.628 [-100.000, 13.284], mean action: 1.970 [0.000, 3.000],  loss: 6.688301, mae: 58.068442, mean_q: 62.818956, mean_eps: 0.719851
  46831/150000: episode: 496, duration: 0.679s, episode steps: 106, steps per second: 156, episode reward: -87.733, mean reward: -0.828 [-100.000,  9.837], mean action: 1.670 [0.000, 3.000],  loss: 13.915091, mae: 57.590398, mean_q: 62.673709, mean_eps: 0.719335
  46959/150000: episode: 497, duration: 0.789s, episode steps: 128, steps per second: 162, episode reward: -64.523, mean reward: -0.504 [-100.000,  8.840], mean action: 1.539 [0.000, 3.000],  loss: 22.868553, mae: 57.690432, mean_q: 62.709885, mean_eps: 0.718633
  47054/150000: episode: 498, duration: 0.596s, episode steps:  95, steps per second: 159, episode reward: -73.139, mean reward: -0.770 [-100.000, 16.454], mean action: 1.632 [0.000, 3.000],  loss: 7.512379, mae: 58.021544, mean_q: 63.356429, mean_eps: 0.717964
  47123/150000: episode: 499, duration: 0.435s, episode steps:  69, steps per second: 159, episode reward: -61.937, mean reward: -0.898 [-100.000, 10.243], mean action: 1.493 [0.000, 3.000],  loss: 6.206166, mae: 58.011877, mean_q: 62.584531, mean_eps: 0.717472
  47200/150000: episode: 500, duration: 0.478s, episode steps:  77, steps per second: 161, episode reward: -41.954, mean reward: -0.545 [-100.000, 37.166], mean action: 1.416 [0.000, 3.000],  loss: 12.487714, mae: 57.605033, mean_q: 60.502392, mean_eps: 0.717034
  47272/150000: episode: 501, duration: 0.465s, episode steps:  72, steps per second: 155, episode reward: -23.089, mean reward: -0.321 [-100.000,  8.400], mean action: 1.722 [0.000, 3.000],  loss: 16.869589, mae: 57.675356, mean_q: 62.628085, mean_eps: 0.716587
  47358/150000: episode: 502, duration: 0.531s, episode steps:  86, steps per second: 162, episode reward: -65.682, mean reward: -0.764 [-100.000, 66.059], mean action: 1.488 [0.000, 3.000],  loss: 20.717055, mae: 57.688466, mean_q: 61.986010, mean_eps: 0.716113
  47471/150000: episode: 503, duration: 0.720s, episode steps: 113, steps per second: 157, episode reward: -94.561, mean reward: -0.837 [-100.000, 22.807], mean action: 1.416 [0.000, 3.000],  loss: 17.782271, mae: 57.809598, mean_q: 62.085384, mean_eps: 0.715516
  47537/150000: episode: 504, duration: 0.411s, episode steps:  66, steps per second: 160, episode reward: -65.095, mean reward: -0.986 [-100.000, 16.070], mean action: 1.742 [0.000, 3.000],  loss: 22.959258, mae: 57.894499, mean_q: 62.186241, mean_eps: 0.714979
  47614/150000: episode: 505, duration: 0.470s, episode steps:  77, steps per second: 164, episode reward: -55.014, mean reward: -0.714 [-100.000, 11.533], mean action: 1.636 [0.000, 3.000],  loss: 12.419933, mae: 58.739253, mean_q: 62.815501, mean_eps: 0.714550
  47730/150000: episode: 506, duration: 0.723s, episode steps: 116, steps per second: 160, episode reward: -99.841, mean reward: -0.861 [-100.000, 12.743], mean action: 1.655 [0.000, 3.000],  loss: 24.447075, mae: 58.770313, mean_q: 61.607983, mean_eps: 0.713971
  47859/150000: episode: 507, duration: 0.830s, episode steps: 129, steps per second: 155, episode reward: -209.153, mean reward: -1.621 [-100.000, 116.377], mean action: 1.543 [0.000, 3.000],  loss: 18.707498, mae: 58.304890, mean_q: 61.960112, mean_eps: 0.713236
  47954/150000: episode: 508, duration: 0.576s, episode steps:  95, steps per second: 165, episode reward: -48.483, mean reward: -0.510 [-100.000, 29.557], mean action: 1.600 [0.000, 3.000],  loss: 17.759692, mae: 59.412438, mean_q: 64.243598, mean_eps: 0.712564
  48042/150000: episode: 509, duration: 0.573s, episode steps:  88, steps per second: 154, episode reward: -96.052, mean reward: -1.092 [-100.000, 11.054], mean action: 1.716 [0.000, 3.000],  loss: 29.801021, mae: 58.387962, mean_q: 62.260810, mean_eps: 0.712015
  48144/150000: episode: 510, duration: 0.738s, episode steps: 102, steps per second: 138, episode reward: -144.784, mean reward: -1.419 [-100.000,  5.662], mean action: 1.657 [0.000, 3.000],  loss: 8.376459, mae: 58.543288, mean_q: 62.224572, mean_eps: 0.711445
  48225/150000: episode: 511, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -42.361, mean reward: -0.523 [-100.000, 12.564], mean action: 1.617 [0.000, 3.000],  loss: 10.267945, mae: 59.491646, mean_q: 63.062111, mean_eps: 0.710896
  48356/150000: episode: 512, duration: 0.864s, episode steps: 131, steps per second: 152, episode reward: -91.773, mean reward: -0.701 [-100.000, 12.440], mean action: 1.557 [0.000, 3.000],  loss: 14.993061, mae: 59.233072, mean_q: 64.413763, mean_eps: 0.710260
  48472/150000: episode: 513, duration: 0.791s, episode steps: 116, steps per second: 147, episode reward: -168.026, mean reward: -1.449 [-100.000,  8.165], mean action: 1.621 [0.000, 3.000],  loss: 11.528308, mae: 60.063097, mean_q: 65.008434, mean_eps: 0.709519
  48576/150000: episode: 514, duration: 0.689s, episode steps: 104, steps per second: 151, episode reward: -101.356, mean reward: -0.975 [-100.000, 10.362], mean action: 1.606 [0.000, 3.000],  loss: 5.757682, mae: 59.327678, mean_q: 63.868694, mean_eps: 0.708859
  48690/150000: episode: 515, duration: 0.701s, episode steps: 114, steps per second: 163, episode reward: -52.751, mean reward: -0.463 [-100.000, 12.421], mean action: 1.614 [0.000, 3.000],  loss: 18.475867, mae: 59.835845, mean_q: 63.967988, mean_eps: 0.708205
  48806/150000: episode: 516, duration: 0.741s, episode steps: 116, steps per second: 157, episode reward: -129.438, mean reward: -1.116 [-100.000, 10.820], mean action: 1.483 [0.000, 3.000],  loss: 12.099713, mae: 59.689829, mean_q: 64.461111, mean_eps: 0.707515
  48872/150000: episode: 517, duration: 0.401s, episode steps:  66, steps per second: 164, episode reward: -55.601, mean reward: -0.842 [-100.000, 20.664], mean action: 1.606 [0.000, 3.000],  loss: 15.233586, mae: 59.291218, mean_q: 61.532696, mean_eps: 0.706969
  49004/150000: episode: 518, duration: 0.811s, episode steps: 132, steps per second: 163, episode reward: -64.853, mean reward: -0.491 [-100.000, 20.679], mean action: 1.477 [0.000, 3.000],  loss: 15.754503, mae: 59.387570, mean_q: 64.063983, mean_eps: 0.706375
  49072/150000: episode: 519, duration: 0.428s, episode steps:  68, steps per second: 159, episode reward: -49.279, mean reward: -0.725 [-100.000, 10.417], mean action: 1.456 [0.000, 3.000],  loss: 28.680771, mae: 59.815026, mean_q: 62.506413, mean_eps: 0.705775
  49187/150000: episode: 520, duration: 0.723s, episode steps: 115, steps per second: 159, episode reward: -87.221, mean reward: -0.758 [-100.000, 12.601], mean action: 1.704 [0.000, 3.000],  loss: 12.056143, mae: 60.014970, mean_q: 63.340758, mean_eps: 0.705226
  49254/150000: episode: 521, duration: 0.419s, episode steps:  67, steps per second: 160, episode reward: -83.922, mean reward: -1.253 [-100.000, 18.564], mean action: 1.657 [0.000, 3.000],  loss: 47.494682, mae: 59.690577, mean_q: 61.455199, mean_eps: 0.704680
  49327/150000: episode: 522, duration: 0.443s, episode steps:  73, steps per second: 165, episode reward: -111.271, mean reward: -1.524 [-100.000,  5.302], mean action: 1.781 [0.000, 3.000],  loss: 11.528300, mae: 60.637027, mean_q: 63.758732, mean_eps: 0.704260
  49451/150000: episode: 523, duration: 0.801s, episode steps: 124, steps per second: 155, episode reward: -77.737, mean reward: -0.627 [-100.000, 11.081], mean action: 1.605 [0.000, 3.000],  loss: 10.236941, mae: 61.136806, mean_q: 65.693105, mean_eps: 0.703669
  49565/150000: episode: 524, duration: 0.694s, episode steps: 114, steps per second: 164, episode reward: -129.070, mean reward: -1.132 [-100.000,  5.190], mean action: 1.632 [0.000, 3.000],  loss: 9.198699, mae: 59.715212, mean_q: 63.246499, mean_eps: 0.702955
  49650/150000: episode: 525, duration: 0.516s, episode steps:  85, steps per second: 165, episode reward: -13.038, mean reward: -0.153 [-100.000, 17.283], mean action: 1.612 [0.000, 3.000],  loss: 10.816340, mae: 59.703251, mean_q: 62.184482, mean_eps: 0.702358
  49724/150000: episode: 526, duration: 0.461s, episode steps:  74, steps per second: 161, episode reward: -71.456, mean reward: -0.966 [-100.000, 12.431], mean action: 1.459 [0.000, 3.000],  loss: 27.685168, mae: 60.072547, mean_q: 65.202457, mean_eps: 0.701881
  49879/150000: episode: 527, duration: 1.033s, episode steps: 155, steps per second: 150, episode reward: -2.697, mean reward: -0.017 [-100.000, 59.319], mean action: 1.600 [0.000, 3.000],  loss: 19.437347, mae: 59.571924, mean_q: 62.484889, mean_eps: 0.701194
  49975/150000: episode: 528, duration: 0.602s, episode steps:  96, steps per second: 160, episode reward: -87.018, mean reward: -0.906 [-100.000, 11.306], mean action: 1.615 [0.000, 3.000],  loss: 36.462900, mae: 60.517157, mean_q: 65.556251, mean_eps: 0.700441
  50095/150000: episode: 529, duration: 0.767s, episode steps: 120, steps per second: 156, episode reward: -116.666, mean reward: -0.972 [-100.000,  5.950], mean action: 1.558 [0.000, 3.000],  loss: 18.352191, mae: 60.808076, mean_q: 64.801136, mean_eps: 0.699793
  50187/150000: episode: 530, duration: 0.607s, episode steps:  92, steps per second: 152, episode reward: -122.820, mean reward: -1.335 [-100.000,  5.718], mean action: 1.728 [0.000, 3.000],  loss: 9.389435, mae: 59.923853, mean_q: 66.747893, mean_eps: 0.699157
  50289/150000: episode: 531, duration: 0.613s, episode steps: 102, steps per second: 166, episode reward: -110.527, mean reward: -1.084 [-100.000, 11.019], mean action: 1.588 [0.000, 3.000],  loss: 27.756985, mae: 60.109170, mean_q: 65.445827, mean_eps: 0.698575
  50373/150000: episode: 532, duration: 0.518s, episode steps:  84, steps per second: 162, episode reward: -97.309, mean reward: -1.158 [-100.000,  9.411], mean action: 1.619 [0.000, 3.000],  loss: 15.748963, mae: 60.246659, mean_q: 65.102807, mean_eps: 0.698017
  50461/150000: episode: 533, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -41.258, mean reward: -0.469 [-100.000, 14.616], mean action: 1.580 [0.000, 3.000],  loss: 15.645377, mae: 58.696218, mean_q: 63.431266, mean_eps: 0.697501
  50559/150000: episode: 534, duration: 0.614s, episode steps:  98, steps per second: 160, episode reward: -107.518, mean reward: -1.097 [-100.000, 20.195], mean action: 1.480 [0.000, 3.000],  loss: 32.101649, mae: 59.537005, mean_q: 63.005471, mean_eps: 0.696943
  50678/150000: episode: 535, duration: 0.729s, episode steps: 119, steps per second: 163, episode reward: -119.719, mean reward: -1.006 [-100.000,  9.685], mean action: 1.437 [0.000, 3.000],  loss: 26.033044, mae: 60.360538, mean_q: 65.896261, mean_eps: 0.696292
  50760/150000: episode: 536, duration: 0.500s, episode steps:  82, steps per second: 164, episode reward: -50.758, mean reward: -0.619 [-100.000, 12.591], mean action: 1.524 [0.000, 3.000],  loss: 8.585091, mae: 60.754596, mean_q: 65.979973, mean_eps: 0.695689
  50918/150000: episode: 537, duration: 0.989s, episode steps: 158, steps per second: 160, episode reward: -52.997, mean reward: -0.335 [-100.000, 12.444], mean action: 1.658 [0.000, 3.000],  loss: 23.635676, mae: 60.144887, mean_q: 64.798958, mean_eps: 0.694969
  51016/150000: episode: 538, duration: 0.598s, episode steps:  98, steps per second: 164, episode reward: -87.752, mean reward: -0.895 [-100.000, 11.607], mean action: 1.459 [0.000, 3.000],  loss: 13.725202, mae: 59.623045, mean_q: 64.898549, mean_eps: 0.694201
  51101/150000: episode: 539, duration: 0.510s, episode steps:  85, steps per second: 167, episode reward: -88.623, mean reward: -1.043 [-100.000, 12.764], mean action: 1.659 [0.000, 3.000],  loss: 8.251821, mae: 59.938030, mean_q: 63.666808, mean_eps: 0.693652
  51194/150000: episode: 540, duration: 0.617s, episode steps:  93, steps per second: 151, episode reward: -80.297, mean reward: -0.863 [-100.000,  6.862], mean action: 1.613 [0.000, 3.000],  loss: 8.496215, mae: 59.656144, mean_q: 64.866281, mean_eps: 0.693118
  51316/150000: episode: 541, duration: 0.747s, episode steps: 122, steps per second: 163, episode reward: -95.514, mean reward: -0.783 [-100.000,  6.177], mean action: 1.697 [0.000, 3.000],  loss: 20.597414, mae: 59.725039, mean_q: 63.612793, mean_eps: 0.692473
  51379/150000: episode: 542, duration: 0.387s, episode steps:  63, steps per second: 163, episode reward: -89.001, mean reward: -1.413 [-100.000,  6.764], mean action: 1.413 [0.000, 3.000],  loss: 15.311802, mae: 60.782619, mean_q: 66.721733, mean_eps: 0.691918
  51485/150000: episode: 543, duration: 0.661s, episode steps: 106, steps per second: 160, episode reward: -62.106, mean reward: -0.586 [-100.000, 10.307], mean action: 1.538 [0.000, 3.000],  loss: 13.187191, mae: 60.058285, mean_q: 65.300884, mean_eps: 0.691411
  51573/150000: episode: 544, duration: 0.565s, episode steps:  88, steps per second: 156, episode reward: -94.693, mean reward: -1.076 [-100.000, 11.695], mean action: 1.727 [0.000, 3.000],  loss: 11.725471, mae: 59.049903, mean_q: 63.510306, mean_eps: 0.690829
  51678/150000: episode: 545, duration: 0.652s, episode steps: 105, steps per second: 161, episode reward: -29.397, mean reward: -0.280 [-100.000, 13.395], mean action: 1.448 [0.000, 3.000],  loss: 9.297317, mae: 59.592248, mean_q: 63.904070, mean_eps: 0.690250
  51789/150000: episode: 546, duration: 0.673s, episode steps: 111, steps per second: 165, episode reward: -136.629, mean reward: -1.231 [-100.000,  4.660], mean action: 1.721 [0.000, 3.000],  loss: 37.230375, mae: 59.669449, mean_q: 62.664083, mean_eps: 0.689602
  51871/150000: episode: 547, duration: 0.528s, episode steps:  82, steps per second: 155, episode reward: -82.224, mean reward: -1.003 [-100.000, 13.004], mean action: 1.598 [0.000, 3.000],  loss: 7.949157, mae: 59.825434, mean_q: 64.686165, mean_eps: 0.689023
  51945/150000: episode: 548, duration: 0.542s, episode steps:  74, steps per second: 137, episode reward: -45.497, mean reward: -0.615 [-100.000, 10.407], mean action: 1.838 [0.000, 3.000],  loss: 26.222694, mae: 58.994173, mean_q: 64.517461, mean_eps: 0.688555
  52024/150000: episode: 549, duration: 0.579s, episode steps:  79, steps per second: 137, episode reward: -111.769, mean reward: -1.415 [-100.000,  5.845], mean action: 1.734 [0.000, 3.000],  loss: 21.999603, mae: 59.236136, mean_q: 64.882035, mean_eps: 0.688096
  52157/150000: episode: 550, duration: 0.991s, episode steps: 133, steps per second: 134, episode reward: -133.251, mean reward: -1.002 [-100.000,  8.169], mean action: 1.684 [0.000, 3.000],  loss: 19.552267, mae: 60.087420, mean_q: 66.253937, mean_eps: 0.687460
  52253/150000: episode: 551, duration: 0.667s, episode steps:  96, steps per second: 144, episode reward: -143.541, mean reward: -1.495 [-100.000,  9.482], mean action: 1.760 [0.000, 3.000],  loss: 23.629706, mae: 60.582400, mean_q: 67.793263, mean_eps: 0.686773
  52330/150000: episode: 552, duration: 0.526s, episode steps:  77, steps per second: 146, episode reward: -44.546, mean reward: -0.579 [-100.000, 12.428], mean action: 1.753 [0.000, 3.000],  loss: 7.727047, mae: 60.253864, mean_q: 67.258988, mean_eps: 0.686254
  52409/150000: episode: 553, duration: 0.529s, episode steps:  79, steps per second: 149, episode reward: -71.841, mean reward: -0.909 [-100.000, 13.504], mean action: 1.734 [0.000, 3.000],  loss: 8.859267, mae: 59.504391, mean_q: 65.467262, mean_eps: 0.685786
  52525/150000: episode: 554, duration: 0.829s, episode steps: 116, steps per second: 140, episode reward: -47.961, mean reward: -0.413 [-100.000,  7.742], mean action: 1.500 [0.000, 3.000],  loss: 8.662929, mae: 60.229695, mean_q: 66.358905, mean_eps: 0.685201
  52640/150000: episode: 555, duration: 0.713s, episode steps: 115, steps per second: 161, episode reward: -77.771, mean reward: -0.676 [-100.000, 18.922], mean action: 1.643 [0.000, 3.000],  loss: 19.757629, mae: 60.854989, mean_q: 66.533226, mean_eps: 0.684508
  52739/150000: episode: 556, duration: 0.603s, episode steps:  99, steps per second: 164, episode reward: -59.672, mean reward: -0.603 [-100.000, 16.891], mean action: 1.717 [0.000, 3.000],  loss: 9.043925, mae: 60.606411, mean_q: 66.964679, mean_eps: 0.683866
  52811/150000: episode: 557, duration: 0.481s, episode steps:  72, steps per second: 150, episode reward: -115.269, mean reward: -1.601 [-100.000,  8.029], mean action: 1.486 [0.000, 3.000],  loss: 24.497946, mae: 60.187253, mean_q: 67.048124, mean_eps: 0.683353
  52888/150000: episode: 558, duration: 0.484s, episode steps:  77, steps per second: 159, episode reward: -76.824, mean reward: -0.998 [-100.000, 10.344], mean action: 1.753 [0.000, 3.000],  loss: 8.239272, mae: 59.788211, mean_q: 65.542488, mean_eps: 0.682906
  52996/150000: episode: 559, duration: 0.662s, episode steps: 108, steps per second: 163, episode reward: -8.400, mean reward: -0.078 [-100.000, 16.078], mean action: 1.750 [0.000, 3.000],  loss: 13.604396, mae: 60.139629, mean_q: 64.757091, mean_eps: 0.682351
  53101/150000: episode: 560, duration: 0.641s, episode steps: 105, steps per second: 164, episode reward: -127.922, mean reward: -1.218 [-100.000,  7.363], mean action: 1.514 [0.000, 3.000],  loss: 26.919380, mae: 60.807228, mean_q: 66.279868, mean_eps: 0.681712
  53224/150000: episode: 561, duration: 0.806s, episode steps: 123, steps per second: 153, episode reward: -119.023, mean reward: -0.968 [-100.000, 14.374], mean action: 1.553 [0.000, 3.000],  loss: 9.552040, mae: 60.289637, mean_q: 66.621662, mean_eps: 0.681028
  53287/150000: episode: 562, duration: 0.389s, episode steps:  63, steps per second: 162, episode reward: -85.120, mean reward: -1.351 [-100.000, 11.534], mean action: 1.556 [0.000, 3.000],  loss: 21.511332, mae: 60.571408, mean_q: 66.619085, mean_eps: 0.680470
  53389/150000: episode: 563, duration: 0.614s, episode steps: 102, steps per second: 166, episode reward: -159.152, mean reward: -1.560 [-100.000,  3.806], mean action: 1.863 [0.000, 3.000],  loss: 29.249005, mae: 60.299226, mean_q: 68.055754, mean_eps: 0.679975
  53500/150000: episode: 564, duration: 0.709s, episode steps: 111, steps per second: 156, episode reward: -90.784, mean reward: -0.818 [-100.000,  8.765], mean action: 1.405 [0.000, 3.000],  loss: 30.103589, mae: 60.469090, mean_q: 67.880699, mean_eps: 0.679336
  53598/150000: episode: 565, duration: 0.605s, episode steps:  98, steps per second: 162, episode reward: 21.066, mean reward:  0.215 [-100.000, 16.300], mean action: 1.663 [0.000, 3.000],  loss: 21.546636, mae: 61.519446, mean_q: 68.209917, mean_eps: 0.678709
  53671/150000: episode: 566, duration: 0.445s, episode steps:  73, steps per second: 164, episode reward: -34.838, mean reward: -0.477 [-100.000, 17.804], mean action: 1.534 [0.000, 3.000],  loss: 9.376550, mae: 60.321408, mean_q: 66.168551, mean_eps: 0.678196
  53767/150000: episode: 567, duration: 0.585s, episode steps:  96, steps per second: 164, episode reward: -65.872, mean reward: -0.686 [-100.000,  6.424], mean action: 1.656 [0.000, 3.000],  loss: 15.104098, mae: 59.706577, mean_q: 66.049070, mean_eps: 0.677689
  53869/150000: episode: 568, duration: 0.651s, episode steps: 102, steps per second: 157, episode reward: -113.149, mean reward: -1.109 [-100.000, 11.738], mean action: 1.578 [0.000, 3.000],  loss: 22.984499, mae: 60.232647, mean_q: 65.884990, mean_eps: 0.677095
  53949/150000: episode: 569, duration: 0.498s, episode steps:  80, steps per second: 161, episode reward: -98.526, mean reward: -1.232 [-100.000, 18.647], mean action: 1.550 [0.000, 3.000],  loss: 25.136983, mae: 61.357217, mean_q: 68.244851, mean_eps: 0.676549
  54037/150000: episode: 570, duration: 0.540s, episode steps:  88, steps per second: 163, episode reward: -142.000, mean reward: -1.614 [-100.000,  5.222], mean action: 1.466 [0.000, 3.000],  loss: 9.494506, mae: 60.620580, mean_q: 67.701733, mean_eps: 0.676045
  54125/150000: episode: 571, duration: 0.539s, episode steps:  88, steps per second: 163, episode reward: -57.273, mean reward: -0.651 [-100.000,  7.262], mean action: 1.545 [0.000, 3.000],  loss: 13.378327, mae: 60.794130, mean_q: 66.746752, mean_eps: 0.675517
  54212/150000: episode: 572, duration: 0.558s, episode steps:  87, steps per second: 156, episode reward: -89.412, mean reward: -1.028 [-100.000, 12.899], mean action: 1.632 [0.000, 3.000],  loss: 28.707439, mae: 61.467394, mean_q: 69.360948, mean_eps: 0.674992
  54294/150000: episode: 573, duration: 0.507s, episode steps:  82, steps per second: 162, episode reward: -46.225, mean reward: -0.564 [-100.000, 10.962], mean action: 1.646 [0.000, 3.000],  loss: 13.285877, mae: 60.186505, mean_q: 68.169844, mean_eps: 0.674485
  54397/150000: episode: 574, duration: 0.624s, episode steps: 103, steps per second: 165, episode reward: -69.277, mean reward: -0.673 [-100.000, 16.163], mean action: 1.631 [0.000, 3.000],  loss: 13.907461, mae: 60.301716, mean_q: 65.881466, mean_eps: 0.673930
  54471/150000: episode: 575, duration: 0.461s, episode steps:  74, steps per second: 161, episode reward: -44.676, mean reward: -0.604 [-100.000,  7.405], mean action: 1.743 [0.000, 3.000],  loss: 26.122436, mae: 60.863322, mean_q: 68.015061, mean_eps: 0.673399
  54536/150000: episode: 576, duration: 0.417s, episode steps:  65, steps per second: 156, episode reward: -38.668, mean reward: -0.595 [-100.000,  8.775], mean action: 1.585 [0.000, 3.000],  loss: 11.198971, mae: 61.018588, mean_q: 67.098658, mean_eps: 0.672982
  54625/150000: episode: 577, duration: 0.555s, episode steps:  89, steps per second: 160, episode reward: -70.649, mean reward: -0.794 [-100.000, 11.946], mean action: 1.629 [0.000, 3.000],  loss: 7.299717, mae: 60.900473, mean_q: 66.955508, mean_eps: 0.672520
  54693/150000: episode: 578, duration: 0.415s, episode steps:  68, steps per second: 164, episode reward: -36.431, mean reward: -0.536 [-100.000, 10.518], mean action: 1.412 [0.000, 3.000],  loss: 10.359887, mae: 61.806678, mean_q: 69.140845, mean_eps: 0.672049
  54775/150000: episode: 579, duration: 0.514s, episode steps:  82, steps per second: 159, episode reward: -87.205, mean reward: -1.063 [-100.000, 11.245], mean action: 1.866 [0.000, 3.000],  loss: 9.307768, mae: 61.034315, mean_q: 67.195027, mean_eps: 0.671599
  54855/150000: episode: 580, duration: 0.503s, episode steps:  80, steps per second: 159, episode reward: -48.960, mean reward: -0.612 [-100.000,  8.569], mean action: 1.462 [0.000, 3.000],  loss: 12.096168, mae: 61.957010, mean_q: 69.943725, mean_eps: 0.671113
  54952/150000: episode: 581, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -112.287, mean reward: -1.158 [-100.000, 11.595], mean action: 1.691 [0.000, 3.000],  loss: 8.068275, mae: 60.960347, mean_q: 67.290946, mean_eps: 0.670582
  55031/150000: episode: 582, duration: 0.482s, episode steps:  79, steps per second: 164, episode reward: -132.151, mean reward: -1.673 [-100.000,  7.523], mean action: 1.696 [0.000, 3.000],  loss: 12.988179, mae: 61.191987, mean_q: 67.508057, mean_eps: 0.670054
  55113/150000: episode: 583, duration: 0.504s, episode steps:  82, steps per second: 163, episode reward: -279.702, mean reward: -3.411 [-100.000, 18.608], mean action: 1.537 [0.000, 3.000],  loss: 15.501533, mae: 61.351764, mean_q: 68.651400, mean_eps: 0.669571
  55194/150000: episode: 584, duration: 0.495s, episode steps:  81, steps per second: 163, episode reward: -89.618, mean reward: -1.106 [-100.000, 10.621], mean action: 1.667 [0.000, 3.000],  loss: 15.083257, mae: 61.636869, mean_q: 70.205817, mean_eps: 0.669082
  55350/150000: episode: 585, duration: 0.961s, episode steps: 156, steps per second: 162, episode reward: -43.674, mean reward: -0.280 [-100.000, 13.918], mean action: 1.667 [0.000, 3.000],  loss: 12.671307, mae: 61.824242, mean_q: 68.419219, mean_eps: 0.668371
  55434/150000: episode: 586, duration: 0.519s, episode steps:  84, steps per second: 162, episode reward: -60.562, mean reward: -0.721 [-100.000, 16.878], mean action: 1.774 [0.000, 3.000],  loss: 11.694398, mae: 61.298692, mean_q: 68.060214, mean_eps: 0.667651
  55508/150000: episode: 587, duration: 0.453s, episode steps:  74, steps per second: 163, episode reward: -143.051, mean reward: -1.933 [-100.000,  9.093], mean action: 1.635 [0.000, 3.000],  loss: 8.372473, mae: 61.498896, mean_q: 69.083920, mean_eps: 0.667177
  55634/150000: episode: 588, duration: 0.805s, episode steps: 126, steps per second: 156, episode reward: -83.980, mean reward: -0.667 [-100.000, 11.383], mean action: 1.643 [0.000, 3.000],  loss: 12.016770, mae: 61.654980, mean_q: 68.747429, mean_eps: 0.666577
  55752/150000: episode: 589, duration: 0.718s, episode steps: 118, steps per second: 164, episode reward: -145.514, mean reward: -1.233 [-100.000,  9.673], mean action: 1.517 [0.000, 3.000],  loss: 9.642825, mae: 62.169403, mean_q: 69.767101, mean_eps: 0.665845
  55825/150000: episode: 590, duration: 0.443s, episode steps:  73, steps per second: 165, episode reward: -65.589, mean reward: -0.898 [-100.000,  9.994], mean action: 1.603 [0.000, 3.000],  loss: 46.221486, mae: 61.983566, mean_q: 69.633036, mean_eps: 0.665272
  55931/150000: episode: 591, duration: 0.684s, episode steps: 106, steps per second: 155, episode reward: -89.824, mean reward: -0.847 [-100.000,  6.775], mean action: 1.868 [0.000, 3.000],  loss: 15.126737, mae: 61.531986, mean_q: 68.206730, mean_eps: 0.664735
  56038/150000: episode: 592, duration: 0.671s, episode steps: 107, steps per second: 160, episode reward: -98.177, mean reward: -0.918 [-100.000,  9.405], mean action: 1.449 [0.000, 3.000],  loss: 8.926463, mae: 62.575323, mean_q: 71.822827, mean_eps: 0.664096
  56109/150000: episode: 593, duration: 0.436s, episode steps:  71, steps per second: 163, episode reward: -20.540, mean reward: -0.289 [-100.000, 16.427], mean action: 1.746 [0.000, 3.000],  loss: 21.404833, mae: 61.193505, mean_q: 67.422846, mean_eps: 0.663562
  56179/150000: episode: 594, duration: 0.428s, episode steps:  70, steps per second: 163, episode reward: -96.646, mean reward: -1.381 [-100.000, 12.670], mean action: 1.729 [0.000, 3.000],  loss: 6.940599, mae: 60.881577, mean_q: 66.964055, mean_eps: 0.663139
  56282/150000: episode: 595, duration: 0.643s, episode steps: 103, steps per second: 160, episode reward: -31.393, mean reward: -0.305 [-100.000,  7.819], mean action: 1.534 [0.000, 3.000],  loss: 6.176647, mae: 61.918942, mean_q: 68.418018, mean_eps: 0.662620
  56388/150000: episode: 596, duration: 0.662s, episode steps: 106, steps per second: 160, episode reward: -177.320, mean reward: -1.673 [-100.000,  4.906], mean action: 1.434 [0.000, 3.000],  loss: 8.804944, mae: 63.103883, mean_q: 70.454601, mean_eps: 0.661993
  56483/150000: episode: 597, duration: 0.576s, episode steps:  95, steps per second: 165, episode reward: -135.907, mean reward: -1.431 [-100.000,  9.455], mean action: 1.600 [0.000, 3.000],  loss: 14.201431, mae: 61.770020, mean_q: 68.418546, mean_eps: 0.661390
  56604/150000: episode: 598, duration: 0.760s, episode steps: 121, steps per second: 159, episode reward: -134.304, mean reward: -1.110 [-100.000,  7.336], mean action: 1.636 [0.000, 3.000],  loss: 18.312134, mae: 61.884370, mean_q: 69.938593, mean_eps: 0.660742
  56705/150000: episode: 599, duration: 0.645s, episode steps: 101, steps per second: 157, episode reward: -76.201, mean reward: -0.754 [-100.000, 13.385], mean action: 1.416 [0.000, 3.000],  loss: 11.769441, mae: 62.631467, mean_q: 70.870300, mean_eps: 0.660076
  56781/150000: episode: 600, duration: 0.464s, episode steps:  76, steps per second: 164, episode reward: -65.289, mean reward: -0.859 [-100.000, 16.263], mean action: 1.592 [0.000, 3.000],  loss: 7.694498, mae: 62.187616, mean_q: 69.769932, mean_eps: 0.659545
  56887/150000: episode: 601, duration: 0.667s, episode steps: 106, steps per second: 159, episode reward: -116.516, mean reward: -1.099 [-100.000, 13.015], mean action: 1.481 [0.000, 3.000],  loss: 6.868761, mae: 62.029623, mean_q: 67.887430, mean_eps: 0.658999
  57032/150000: episode: 602, duration: 0.933s, episode steps: 145, steps per second: 155, episode reward: -88.910, mean reward: -0.613 [-100.000,  7.817], mean action: 1.352 [0.000, 3.000],  loss: 12.411413, mae: 61.888749, mean_q: 69.949359, mean_eps: 0.658246
  57097/150000: episode: 603, duration: 0.393s, episode steps:  65, steps per second: 165, episode reward: -76.396, mean reward: -1.175 [-100.000, 10.235], mean action: 1.631 [0.000, 3.000],  loss: 6.383025, mae: 63.077846, mean_q: 69.928335, mean_eps: 0.657616
  57202/150000: episode: 604, duration: 0.642s, episode steps: 105, steps per second: 164, episode reward: -105.246, mean reward: -1.002 [-100.000, 10.293], mean action: 1.533 [0.000, 3.000],  loss: 9.124410, mae: 62.652714, mean_q: 69.898690, mean_eps: 0.657106
  57301/150000: episode: 605, duration: 0.623s, episode steps:  99, steps per second: 159, episode reward: -94.375, mean reward: -0.953 [-100.000, 10.648], mean action: 1.657 [0.000, 3.000],  loss: 28.011943, mae: 63.089399, mean_q: 70.651110, mean_eps: 0.656494
  58301/150000: episode: 606, duration: 7.353s, episode steps: 1000, steps per second: 136, episode reward: 45.730, mean reward:  0.046 [-21.014, 27.598], mean action: 1.527 [0.000, 3.000],  loss: 13.536465, mae: 62.900291, mean_q: 70.863555, mean_eps: 0.653197
  58420/150000: episode: 607, duration: 0.800s, episode steps: 119, steps per second: 149, episode reward: -41.711, mean reward: -0.351 [-100.000, 20.816], mean action: 1.866 [0.000, 3.000],  loss: 9.043058, mae: 61.688599, mean_q: 68.046370, mean_eps: 0.649840
  58514/150000: episode: 608, duration: 0.638s, episode steps:  94, steps per second: 147, episode reward: -53.369, mean reward: -0.568 [-100.000, 15.839], mean action: 1.649 [0.000, 3.000],  loss: 12.991711, mae: 62.408540, mean_q: 70.177080, mean_eps: 0.649201
  58580/150000: episode: 609, duration: 0.489s, episode steps:  66, steps per second: 135, episode reward: -143.492, mean reward: -2.174 [-100.000,  4.577], mean action: 1.894 [0.000, 3.000],  loss: 11.903954, mae: 62.440924, mean_q: 70.387567, mean_eps: 0.648721
  58656/150000: episode: 610, duration: 0.521s, episode steps:  76, steps per second: 146, episode reward: -70.605, mean reward: -0.929 [-100.000,  8.597], mean action: 1.605 [0.000, 3.000],  loss: 36.232464, mae: 62.093441, mean_q: 70.937565, mean_eps: 0.648295
  58772/150000: episode: 611, duration: 0.783s, episode steps: 116, steps per second: 148, episode reward: -29.235, mean reward: -0.252 [-100.000, 20.224], mean action: 1.491 [0.000, 3.000],  loss: 14.843257, mae: 62.471571, mean_q: 70.787310, mean_eps: 0.647719
  58869/150000: episode: 612, duration: 0.676s, episode steps:  97, steps per second: 143, episode reward: -102.781, mean reward: -1.060 [-100.000, 15.268], mean action: 1.495 [0.000, 3.000],  loss: 23.653765, mae: 62.087618, mean_q: 70.016933, mean_eps: 0.647080
  58988/150000: episode: 613, duration: 0.733s, episode steps: 119, steps per second: 162, episode reward: -129.093, mean reward: -1.085 [-100.000,  6.062], mean action: 1.655 [0.000, 3.000],  loss: 18.752148, mae: 62.682361, mean_q: 71.085528, mean_eps: 0.646432
  59101/150000: episode: 614, duration: 0.695s, episode steps: 113, steps per second: 163, episode reward: -69.303, mean reward: -0.613 [-100.000, 10.263], mean action: 1.867 [0.000, 3.000],  loss: 7.722310, mae: 63.036421, mean_q: 71.075383, mean_eps: 0.645736
  59195/150000: episode: 615, duration: 0.613s, episode steps:  94, steps per second: 153, episode reward: -44.696, mean reward: -0.475 [-100.000, 12.433], mean action: 1.553 [0.000, 3.000],  loss: 7.936246, mae: 61.925788, mean_q: 68.683007, mean_eps: 0.645115
  59288/150000: episode: 616, duration: 0.565s, episode steps:  93, steps per second: 165, episode reward: -59.731, mean reward: -0.642 [-100.000,  9.640], mean action: 1.570 [0.000, 3.000],  loss: 20.435702, mae: 62.746589, mean_q: 70.905950, mean_eps: 0.644554
  59426/150000: episode: 617, duration: 0.840s, episode steps: 138, steps per second: 164, episode reward: -31.313, mean reward: -0.227 [-100.000, 13.757], mean action: 1.804 [0.000, 3.000],  loss: 8.248365, mae: 61.893787, mean_q: 68.610811, mean_eps: 0.643861
  59514/150000: episode: 618, duration: 0.561s, episode steps:  88, steps per second: 157, episode reward: -96.897, mean reward: -1.101 [-100.000,  9.436], mean action: 1.580 [0.000, 3.000],  loss: 6.275145, mae: 63.403215, mean_q: 70.999693, mean_eps: 0.643183
  59640/150000: episode: 619, duration: 0.772s, episode steps: 126, steps per second: 163, episode reward: -79.956, mean reward: -0.635 [-100.000,  8.159], mean action: 1.611 [0.000, 3.000],  loss: 12.087539, mae: 62.252589, mean_q: 70.370910, mean_eps: 0.642541
  59753/150000: episode: 620, duration: 0.680s, episode steps: 113, steps per second: 166, episode reward: -86.060, mean reward: -0.762 [-100.000,  7.832], mean action: 1.336 [0.000, 3.000],  loss: 21.129745, mae: 62.305721, mean_q: 69.817988, mean_eps: 0.641824
  59848/150000: episode: 621, duration: 0.591s, episode steps:  95, steps per second: 161, episode reward: -70.316, mean reward: -0.740 [-100.000, 23.703], mean action: 1.789 [0.000, 3.000],  loss: 18.945538, mae: 62.786117, mean_q: 70.009923, mean_eps: 0.641200
  59926/150000: episode: 622, duration: 0.480s, episode steps:  78, steps per second: 163, episode reward: -102.613, mean reward: -1.316 [-100.000, 10.106], mean action: 1.500 [0.000, 3.000],  loss: 9.060790, mae: 64.070721, mean_q: 72.420468, mean_eps: 0.640681
  60036/150000: episode: 623, duration: 0.676s, episode steps: 110, steps per second: 163, episode reward: -75.964, mean reward: -0.691 [-100.000, 10.008], mean action: 1.718 [0.000, 3.000],  loss: 20.992263, mae: 62.644220, mean_q: 68.854375, mean_eps: 0.640117
  60112/150000: episode: 624, duration: 0.462s, episode steps:  76, steps per second: 165, episode reward: -92.505, mean reward: -1.217 [-100.000, 11.110], mean action: 1.645 [0.000, 3.000],  loss: 17.144209, mae: 62.728506, mean_q: 69.862365, mean_eps: 0.639559
  60203/150000: episode: 625, duration: 0.570s, episode steps:  91, steps per second: 160, episode reward: -72.851, mean reward: -0.801 [-100.000,  9.473], mean action: 1.681 [0.000, 3.000],  loss: 30.623600, mae: 63.250204, mean_q: 70.558482, mean_eps: 0.639058
  60302/150000: episode: 626, duration: 0.612s, episode steps:  99, steps per second: 162, episode reward: -106.551, mean reward: -1.076 [-100.000, 10.623], mean action: 1.263 [0.000, 3.000],  loss: 20.074687, mae: 62.459469, mean_q: 70.209513, mean_eps: 0.638488
  60445/150000: episode: 627, duration: 0.877s, episode steps: 143, steps per second: 163, episode reward: -37.550, mean reward: -0.263 [-100.000,  6.290], mean action: 1.538 [0.000, 3.000],  loss: 20.247583, mae: 62.327556, mean_q: 69.166167, mean_eps: 0.637762
  60526/150000: episode: 628, duration: 0.500s, episode steps:  81, steps per second: 162, episode reward: -33.425, mean reward: -0.413 [-100.000, 27.384], mean action: 1.605 [0.000, 3.000],  loss: 18.438508, mae: 63.319804, mean_q: 71.437714, mean_eps: 0.637090
  60606/150000: episode: 629, duration: 0.527s, episode steps:  80, steps per second: 152, episode reward: -86.339, mean reward: -1.079 [-100.000, 13.335], mean action: 1.450 [0.000, 3.000],  loss: 8.805085, mae: 63.638531, mean_q: 71.934211, mean_eps: 0.636607
  60727/150000: episode: 630, duration: 0.740s, episode steps: 121, steps per second: 163, episode reward: -33.506, mean reward: -0.277 [-100.000, 11.529], mean action: 1.529 [0.000, 3.000],  loss: 13.493337, mae: 62.556844, mean_q: 70.385231, mean_eps: 0.636004
  60849/150000: episode: 631, duration: 0.742s, episode steps: 122, steps per second: 164, episode reward: -53.254, mean reward: -0.437 [-100.000,  7.912], mean action: 1.697 [0.000, 3.000],  loss: 18.929767, mae: 63.532279, mean_q: 71.073097, mean_eps: 0.635275
  60959/150000: episode: 632, duration: 0.691s, episode steps: 110, steps per second: 159, episode reward: -60.082, mean reward: -0.546 [-100.000, 12.506], mean action: 1.682 [0.000, 3.000],  loss: 15.766301, mae: 63.669003, mean_q: 71.807124, mean_eps: 0.634579
  61069/150000: episode: 633, duration: 0.673s, episode steps: 110, steps per second: 163, episode reward: -146.344, mean reward: -1.330 [-100.000,  9.602], mean action: 1.473 [0.000, 3.000],  loss: 9.909788, mae: 63.283698, mean_q: 69.543308, mean_eps: 0.633919
  61159/150000: episode: 634, duration: 0.546s, episode steps:  90, steps per second: 165, episode reward: -73.247, mean reward: -0.814 [-100.000,  9.105], mean action: 1.600 [0.000, 3.000],  loss: 7.145512, mae: 63.335373, mean_q: 69.092330, mean_eps: 0.633319
  61253/150000: episode: 635, duration: 0.588s, episode steps:  94, steps per second: 160, episode reward: -86.215, mean reward: -0.917 [-100.000,  6.960], mean action: 1.574 [0.000, 3.000],  loss: 12.042769, mae: 62.762589, mean_q: 69.659786, mean_eps: 0.632767
  61340/150000: episode: 636, duration: 0.552s, episode steps:  87, steps per second: 158, episode reward:  8.690, mean reward:  0.100 [-100.000, 30.892], mean action: 1.540 [0.000, 3.000],  loss: 11.241308, mae: 64.780433, mean_q: 72.119863, mean_eps: 0.632224
  61457/150000: episode: 637, duration: 0.716s, episode steps: 117, steps per second: 163, episode reward: -90.339, mean reward: -0.772 [-100.000,  8.479], mean action: 1.641 [0.000, 3.000],  loss: 11.582542, mae: 64.268056, mean_q: 71.864536, mean_eps: 0.631612
  61548/150000: episode: 638, duration: 0.567s, episode steps:  91, steps per second: 160, episode reward: -5.522, mean reward: -0.061 [-100.000, 59.345], mean action: 1.824 [0.000, 3.000],  loss: 15.969336, mae: 63.809715, mean_q: 70.444800, mean_eps: 0.630988
  62548/150000: episode: 639, duration: 7.645s, episode steps: 1000, steps per second: 131, episode reward: 43.204, mean reward:  0.043 [-24.387, 54.841], mean action: 1.621 [0.000, 3.000],  loss: 13.169422, mae: 63.554468, mean_q: 70.186031, mean_eps: 0.627715
  62679/150000: episode: 640, duration: 0.852s, episode steps: 131, steps per second: 154, episode reward: 19.114, mean reward:  0.146 [-100.000, 22.104], mean action: 1.511 [0.000, 3.000],  loss: 11.290408, mae: 63.031687, mean_q: 70.458623, mean_eps: 0.624322
  62783/150000: episode: 641, duration: 0.688s, episode steps: 104, steps per second: 151, episode reward: -185.328, mean reward: -1.782 [-100.000,  6.506], mean action: 1.548 [0.000, 3.000],  loss: 7.126999, mae: 64.440953, mean_q: 71.331187, mean_eps: 0.623617
  62893/150000: episode: 642, duration: 0.672s, episode steps: 110, steps per second: 164, episode reward: -64.766, mean reward: -0.589 [-100.000,  9.476], mean action: 1.673 [0.000, 3.000],  loss: 19.304887, mae: 63.203765, mean_q: 69.371129, mean_eps: 0.622975
  63012/150000: episode: 643, duration: 0.726s, episode steps: 119, steps per second: 164, episode reward: -129.431, mean reward: -1.088 [-100.000, 15.430], mean action: 1.496 [0.000, 3.000],  loss: 14.398500, mae: 63.924728, mean_q: 71.677251, mean_eps: 0.622288
  63132/150000: episode: 644, duration: 0.774s, episode steps: 120, steps per second: 155, episode reward: -44.105, mean reward: -0.368 [-100.000, 17.369], mean action: 1.608 [0.000, 3.000],  loss: 8.419338, mae: 63.750478, mean_q: 69.557166, mean_eps: 0.621571
  63271/150000: episode: 645, duration: 0.847s, episode steps: 139, steps per second: 164, episode reward: -62.956, mean reward: -0.453 [-100.000, 14.333], mean action: 1.626 [0.000, 3.000],  loss: 22.176429, mae: 63.368424, mean_q: 69.862956, mean_eps: 0.620794
  63351/150000: episode: 646, duration: 0.494s, episode steps:  80, steps per second: 162, episode reward: -102.680, mean reward: -1.284 [-100.000, 14.734], mean action: 1.625 [0.000, 3.000],  loss: 26.811467, mae: 63.879544, mean_q: 71.160698, mean_eps: 0.620137
  63439/150000: episode: 647, duration: 0.549s, episode steps:  88, steps per second: 160, episode reward: -12.806, mean reward: -0.146 [-100.000, 17.595], mean action: 1.466 [0.000, 3.000],  loss: 16.800226, mae: 63.221654, mean_q: 68.650774, mean_eps: 0.619633
  63510/150000: episode: 648, duration: 0.441s, episode steps:  71, steps per second: 161, episode reward: -44.255, mean reward: -0.623 [-100.000,  9.799], mean action: 1.479 [0.000, 3.000],  loss: 28.008497, mae: 63.794833, mean_q: 70.925302, mean_eps: 0.619156
  63588/150000: episode: 649, duration: 0.478s, episode steps:  78, steps per second: 163, episode reward: -69.203, mean reward: -0.887 [-100.000,  8.120], mean action: 1.615 [0.000, 3.000],  loss: 9.729685, mae: 63.989014, mean_q: 69.616526, mean_eps: 0.618709
  63706/150000: episode: 650, duration: 0.720s, episode steps: 118, steps per second: 164, episode reward: -20.575, mean reward: -0.174 [-100.000, 18.377], mean action: 1.585 [0.000, 3.000],  loss: 8.975202, mae: 64.144989, mean_q: 70.461176, mean_eps: 0.618121
  63802/150000: episode: 651, duration: 0.605s, episode steps:  96, steps per second: 159, episode reward: -190.765, mean reward: -1.987 [-100.000, 55.156], mean action: 1.396 [0.000, 3.000],  loss: 10.133348, mae: 63.983992, mean_q: 70.032025, mean_eps: 0.617479
  63907/150000: episode: 652, duration: 0.652s, episode steps: 105, steps per second: 161, episode reward: -49.848, mean reward: -0.475 [-100.000,  9.931], mean action: 1.657 [0.000, 3.000],  loss: 10.112394, mae: 64.305418, mean_q: 70.406987, mean_eps: 0.616876
  64055/150000: episode: 653, duration: 0.900s, episode steps: 148, steps per second: 164, episode reward: -38.401, mean reward: -0.259 [-100.000,  7.096], mean action: 1.642 [0.000, 3.000],  loss: 8.896945, mae: 64.411129, mean_q: 72.233588, mean_eps: 0.616117
  64169/150000: episode: 654, duration: 0.749s, episode steps: 114, steps per second: 152, episode reward: -110.367, mean reward: -0.968 [-100.000,  8.200], mean action: 1.737 [0.000, 3.000],  loss: 7.771617, mae: 63.234736, mean_q: 68.681282, mean_eps: 0.615331
  64289/150000: episode: 655, duration: 0.746s, episode steps: 120, steps per second: 161, episode reward: -68.652, mean reward: -0.572 [-100.000, 29.763], mean action: 1.583 [0.000, 3.000],  loss: 11.805892, mae: 63.489906, mean_q: 69.568666, mean_eps: 0.614629
  64371/150000: episode: 656, duration: 0.533s, episode steps:  82, steps per second: 154, episode reward: -40.707, mean reward: -0.496 [-100.000, 18.087], mean action: 1.720 [0.000, 3.000],  loss: 15.283176, mae: 63.854870, mean_q: 71.375501, mean_eps: 0.614023
  64463/150000: episode: 657, duration: 0.579s, episode steps:  92, steps per second: 159, episode reward: -69.172, mean reward: -0.752 [-100.000, 20.635], mean action: 1.533 [0.000, 3.000],  loss: 17.109155, mae: 63.769417, mean_q: 71.137762, mean_eps: 0.613501
  64570/150000: episode: 658, duration: 0.675s, episode steps: 107, steps per second: 159, episode reward: -60.740, mean reward: -0.568 [-100.000, 10.249], mean action: 1.570 [0.000, 3.000],  loss: 9.914491, mae: 63.751535, mean_q: 71.282851, mean_eps: 0.612904
  64658/150000: episode: 659, duration: 0.535s, episode steps:  88, steps per second: 165, episode reward: -38.454, mean reward: -0.437 [-100.000, 13.333], mean action: 1.364 [0.000, 3.000],  loss: 12.558380, mae: 64.054444, mean_q: 71.395716, mean_eps: 0.612319
  64729/150000: episode: 660, duration: 0.442s, episode steps:  71, steps per second: 161, episode reward: -36.219, mean reward: -0.510 [-100.000, 14.319], mean action: 1.620 [0.000, 3.000],  loss: 10.777729, mae: 64.814623, mean_q: 73.788481, mean_eps: 0.611842
  64817/150000: episode: 661, duration: 0.566s, episode steps:  88, steps per second: 156, episode reward: -107.610, mean reward: -1.223 [-100.000,  7.698], mean action: 1.489 [0.000, 3.000],  loss: 12.639203, mae: 64.077655, mean_q: 71.598759, mean_eps: 0.611365
  64888/150000: episode: 662, duration: 0.453s, episode steps:  71, steps per second: 157, episode reward: -74.892, mean reward: -1.055 [-100.000, 11.381], mean action: 1.761 [0.000, 3.000],  loss: 11.272348, mae: 63.435051, mean_q: 71.825907, mean_eps: 0.610888
  65006/150000: episode: 663, duration: 0.734s, episode steps: 118, steps per second: 161, episode reward: -262.797, mean reward: -2.227 [-100.000, 35.066], mean action: 1.373 [0.000, 3.000],  loss: 13.009544, mae: 61.943867, mean_q: 68.040455, mean_eps: 0.610321
  65096/150000: episode: 664, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -55.329, mean reward: -0.615 [-100.000, 12.155], mean action: 1.622 [0.000, 3.000],  loss: 18.316975, mae: 63.841693, mean_q: 71.348367, mean_eps: 0.609697
  65211/150000: episode: 665, duration: 0.743s, episode steps: 115, steps per second: 155, episode reward: -101.119, mean reward: -0.879 [-100.000,  9.335], mean action: 1.626 [0.000, 3.000],  loss: 13.130863, mae: 64.275710, mean_q: 72.903762, mean_eps: 0.609082
  65309/150000: episode: 666, duration: 0.601s, episode steps:  98, steps per second: 163, episode reward: -35.775, mean reward: -0.365 [-100.000, 14.892], mean action: 1.602 [0.000, 3.000],  loss: 8.850931, mae: 63.439477, mean_q: 72.051972, mean_eps: 0.608443
  65391/150000: episode: 667, duration: 0.500s, episode steps:  82, steps per second: 164, episode reward: -71.323, mean reward: -0.870 [-100.000, 12.635], mean action: 1.646 [0.000, 3.000],  loss: 9.018190, mae: 63.812465, mean_q: 73.489758, mean_eps: 0.607903
  65510/150000: episode: 668, duration: 0.752s, episode steps: 119, steps per second: 158, episode reward: -63.283, mean reward: -0.532 [-100.000, 53.475], mean action: 1.765 [0.000, 3.000],  loss: 16.974028, mae: 63.662331, mean_q: 72.375897, mean_eps: 0.607300
  65680/150000: episode: 669, duration: 1.082s, episode steps: 170, steps per second: 157, episode reward: 11.535, mean reward:  0.068 [-100.000,  8.987], mean action: 1.471 [0.000, 3.000],  loss: 16.782535, mae: 63.850252, mean_q: 71.892642, mean_eps: 0.606433
  65796/150000: episode: 670, duration: 0.704s, episode steps: 116, steps per second: 165, episode reward: -59.839, mean reward: -0.516 [-100.000, 11.617], mean action: 1.534 [0.000, 3.000],  loss: 8.840766, mae: 64.308753, mean_q: 72.789008, mean_eps: 0.605575
  65866/150000: episode: 671, duration: 0.449s, episode steps:  70, steps per second: 156, episode reward: -87.300, mean reward: -1.247 [-100.000,  8.077], mean action: 1.329 [0.000, 3.000],  loss: 15.119909, mae: 63.968841, mean_q: 72.924221, mean_eps: 0.605017
  65992/150000: episode: 672, duration: 0.795s, episode steps: 126, steps per second: 159, episode reward: -34.930, mean reward: -0.277 [-100.000, 12.574], mean action: 1.714 [0.000, 3.000],  loss: 14.525428, mae: 64.019978, mean_q: 74.104560, mean_eps: 0.604429
  66129/150000: episode: 673, duration: 0.848s, episode steps: 137, steps per second: 161, episode reward: -102.709, mean reward: -0.750 [-100.000,  9.788], mean action: 1.445 [0.000, 3.000],  loss: 18.933684, mae: 63.857927, mean_q: 73.965430, mean_eps: 0.603640
  66233/150000: episode: 674, duration: 0.711s, episode steps: 104, steps per second: 146, episode reward: -35.646, mean reward: -0.343 [-100.000,  9.475], mean action: 1.567 [0.000, 3.000],  loss: 19.147924, mae: 63.688941, mean_q: 74.185057, mean_eps: 0.602917
  66349/150000: episode: 675, duration: 0.720s, episode steps: 116, steps per second: 161, episode reward: -45.732, mean reward: -0.394 [-100.000, 13.192], mean action: 1.672 [0.000, 3.000],  loss: 12.567781, mae: 64.347643, mean_q: 75.121694, mean_eps: 0.602257
  66485/150000: episode: 676, duration: 0.833s, episode steps: 136, steps per second: 163, episode reward: -72.780, mean reward: -0.535 [-100.000, 15.115], mean action: 1.529 [0.000, 3.000],  loss: 14.855074, mae: 63.519670, mean_q: 73.254065, mean_eps: 0.601501
  66567/150000: episode: 677, duration: 0.528s, episode steps:  82, steps per second: 155, episode reward: -73.638, mean reward: -0.898 [-100.000, 11.624], mean action: 1.561 [0.000, 3.000],  loss: 9.947029, mae: 64.210522, mean_q: 74.511116, mean_eps: 0.600847
  66649/150000: episode: 678, duration: 0.532s, episode steps:  82, steps per second: 154, episode reward: -21.620, mean reward: -0.264 [-100.000, 19.082], mean action: 1.573 [0.000, 3.000],  loss: 7.017453, mae: 63.386405, mean_q: 74.353078, mean_eps: 0.600355
  66780/150000: episode: 679, duration: 0.796s, episode steps: 131, steps per second: 165, episode reward: -7.041, mean reward: -0.054 [-100.000, 17.494], mean action: 1.641 [0.000, 3.000],  loss: 15.538728, mae: 63.966087, mean_q: 73.426534, mean_eps: 0.599716
  66906/150000: episode: 680, duration: 0.818s, episode steps: 126, steps per second: 154, episode reward: 24.965, mean reward:  0.198 [-100.000, 13.851], mean action: 1.841 [0.000, 3.000],  loss: 18.837848, mae: 63.736158, mean_q: 73.182944, mean_eps: 0.598945
  66979/150000: episode: 681, duration: 0.448s, episode steps:  73, steps per second: 163, episode reward: -87.978, mean reward: -1.205 [-100.000, 14.549], mean action: 1.671 [0.000, 3.000],  loss: 12.726203, mae: 62.942612, mean_q: 73.499414, mean_eps: 0.598348
  67065/150000: episode: 682, duration: 0.527s, episode steps:  86, steps per second: 163, episode reward: -101.523, mean reward: -1.181 [-100.000, 11.363], mean action: 1.558 [0.000, 3.000],  loss: 12.693687, mae: 64.441970, mean_q: 74.188574, mean_eps: 0.597871
  67173/150000: episode: 683, duration: 0.671s, episode steps: 108, steps per second: 161, episode reward: -39.492, mean reward: -0.366 [-100.000, 15.757], mean action: 1.648 [0.000, 3.000],  loss: 12.511887, mae: 64.650733, mean_q: 74.291194, mean_eps: 0.597289
  67273/150000: episode: 684, duration: 0.630s, episode steps: 100, steps per second: 159, episode reward: -90.208, mean reward: -0.902 [-100.000, 16.767], mean action: 1.660 [0.000, 3.000],  loss: 15.648569, mae: 64.873547, mean_q: 73.045066, mean_eps: 0.596665
  67413/150000: episode: 685, duration: 0.863s, episode steps: 140, steps per second: 162, episode reward: 55.507, mean reward:  0.396 [-100.000, 64.899], mean action: 1.750 [0.000, 3.000],  loss: 12.970326, mae: 63.862246, mean_q: 73.401355, mean_eps: 0.595945
  67511/150000: episode: 686, duration: 0.601s, episode steps:  98, steps per second: 163, episode reward: -89.229, mean reward: -0.910 [-100.000, 10.634], mean action: 1.551 [0.000, 3.000],  loss: 17.827269, mae: 65.654972, mean_q: 74.386571, mean_eps: 0.595231
  67599/150000: episode: 687, duration: 0.572s, episode steps:  88, steps per second: 154, episode reward: -75.187, mean reward: -0.854 [-100.000,  7.595], mean action: 1.682 [0.000, 3.000],  loss: 7.108113, mae: 65.538129, mean_q: 75.419209, mean_eps: 0.594673
  67693/150000: episode: 688, duration: 0.573s, episode steps:  94, steps per second: 164, episode reward:  6.555, mean reward:  0.070 [-100.000, 15.972], mean action: 1.649 [0.000, 3.000],  loss: 21.779073, mae: 66.047481, mean_q: 75.897115, mean_eps: 0.594127
  67812/150000: episode: 689, duration: 0.727s, episode steps: 119, steps per second: 164, episode reward: -81.656, mean reward: -0.686 [-100.000, 15.049], mean action: 1.613 [0.000, 3.000],  loss: 25.207465, mae: 64.976896, mean_q: 74.950572, mean_eps: 0.593488
  67929/150000: episode: 690, duration: 0.732s, episode steps: 117, steps per second: 160, episode reward: -45.130, mean reward: -0.386 [-100.000, 10.070], mean action: 1.556 [0.000, 3.000],  loss: 11.180221, mae: 64.930937, mean_q: 74.383292, mean_eps: 0.592780
  68012/150000: episode: 691, duration: 0.517s, episode steps:  83, steps per second: 161, episode reward: -32.062, mean reward: -0.386 [-100.000, 12.599], mean action: 1.735 [0.000, 3.000],  loss: 7.508878, mae: 64.716062, mean_q: 74.648410, mean_eps: 0.592180
  68125/150000: episode: 692, duration: 0.694s, episode steps: 113, steps per second: 163, episode reward: -61.178, mean reward: -0.541 [-100.000, 12.564], mean action: 1.761 [0.000, 3.000],  loss: 12.751527, mae: 65.799296, mean_q: 75.497780, mean_eps: 0.591592
  68272/150000: episode: 693, duration: 0.919s, episode steps: 147, steps per second: 160, episode reward: -24.969, mean reward: -0.170 [-100.000, 15.578], mean action: 1.660 [0.000, 3.000],  loss: 9.781895, mae: 65.492152, mean_q: 74.481024, mean_eps: 0.590812
  68358/150000: episode: 694, duration: 0.531s, episode steps:  86, steps per second: 162, episode reward: -30.134, mean reward: -0.350 [-100.000, 16.247], mean action: 1.512 [0.000, 3.000],  loss: 31.069266, mae: 65.981433, mean_q: 75.247008, mean_eps: 0.590113
  69358/150000: episode: 695, duration: 7.549s, episode steps: 1000, steps per second: 132, episode reward: 71.939, mean reward:  0.072 [-24.589, 25.776], mean action: 1.599 [0.000, 3.000],  loss: 12.318707, mae: 65.272960, mean_q: 74.350577, mean_eps: 0.586855
  69507/150000: episode: 696, duration: 0.924s, episode steps: 149, steps per second: 161, episode reward: -55.280, mean reward: -0.371 [-100.000, 16.485], mean action: 1.638 [0.000, 3.000],  loss: 14.169969, mae: 65.103144, mean_q: 75.397562, mean_eps: 0.583408
  69585/150000: episode: 697, duration: 0.480s, episode steps:  78, steps per second: 162, episode reward: -58.478, mean reward: -0.750 [-100.000, 13.603], mean action: 1.756 [0.000, 3.000],  loss: 23.116228, mae: 65.898337, mean_q: 76.265780, mean_eps: 0.582727
  69675/150000: episode: 698, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -24.641, mean reward: -0.274 [-100.000, 12.916], mean action: 1.589 [0.000, 3.000],  loss: 8.833934, mae: 65.396900, mean_q: 75.052378, mean_eps: 0.582223
  69742/150000: episode: 699, duration: 0.412s, episode steps:  67, steps per second: 163, episode reward: -128.113, mean reward: -1.912 [-100.000,  6.301], mean action: 1.657 [0.000, 3.000],  loss: 21.416448, mae: 66.205017, mean_q: 76.088036, mean_eps: 0.581752
  69857/150000: episode: 700, duration: 0.746s, episode steps: 115, steps per second: 154, episode reward: -103.175, mean reward: -0.897 [-100.000, 10.734], mean action: 1.652 [0.000, 3.000],  loss: 10.064339, mae: 66.533307, mean_q: 75.512414, mean_eps: 0.581206
  69943/150000: episode: 701, duration: 0.523s, episode steps:  86, steps per second: 164, episode reward: -102.786, mean reward: -1.195 [-100.000, 11.835], mean action: 1.663 [0.000, 3.000],  loss: 15.119448, mae: 65.292549, mean_q: 73.846150, mean_eps: 0.580603
  70042/150000: episode: 702, duration: 0.603s, episode steps:  99, steps per second: 164, episode reward: -81.463, mean reward: -0.823 [-100.000,  7.663], mean action: 1.667 [0.000, 3.000],  loss: 10.553409, mae: 66.495053, mean_q: 77.752662, mean_eps: 0.580048
  70152/150000: episode: 703, duration: 0.692s, episode steps: 110, steps per second: 159, episode reward: -73.477, mean reward: -0.668 [-100.000,  9.248], mean action: 1.509 [0.000, 3.000],  loss: 25.441478, mae: 65.882080, mean_q: 75.654464, mean_eps: 0.579421
  70288/150000: episode: 704, duration: 0.835s, episode steps: 136, steps per second: 163, episode reward: -36.970, mean reward: -0.272 [-100.000,  9.451], mean action: 1.360 [0.000, 3.000],  loss: 22.414005, mae: 65.351740, mean_q: 74.076174, mean_eps: 0.578683
  70360/150000: episode: 705, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: -48.779, mean reward: -0.677 [-100.000, 10.337], mean action: 1.847 [0.000, 3.000],  loss: 11.681892, mae: 65.423720, mean_q: 74.206505, mean_eps: 0.578059
  70504/150000: episode: 706, duration: 0.893s, episode steps: 144, steps per second: 161, episode reward: -51.026, mean reward: -0.354 [-100.000, 12.923], mean action: 1.604 [0.000, 3.000],  loss: 20.255942, mae: 66.006156, mean_q: 75.633583, mean_eps: 0.577411
  70609/150000: episode: 707, duration: 0.645s, episode steps: 105, steps per second: 163, episode reward: -36.545, mean reward: -0.348 [-100.000, 13.863], mean action: 1.676 [0.000, 3.000],  loss: 12.921777, mae: 65.360048, mean_q: 76.513177, mean_eps: 0.576664
  70721/150000: episode: 708, duration: 0.681s, episode steps: 112, steps per second: 164, episode reward: -35.559, mean reward: -0.317 [-100.000,  8.882], mean action: 1.723 [0.000, 3.000],  loss: 24.000875, mae: 66.173704, mean_q: 75.925331, mean_eps: 0.576013
  70848/150000: episode: 709, duration: 0.813s, episode steps: 127, steps per second: 156, episode reward: -222.660, mean reward: -1.753 [-100.000, 85.266], mean action: 1.646 [0.000, 3.000],  loss: 10.553084, mae: 66.080413, mean_q: 76.639872, mean_eps: 0.575296
  70988/150000: episode: 710, duration: 0.894s, episode steps: 140, steps per second: 157, episode reward: -25.421, mean reward: -0.182 [-100.000, 12.841], mean action: 1.536 [0.000, 3.000],  loss: 20.874961, mae: 65.520117, mean_q: 74.097406, mean_eps: 0.574495
  71094/150000: episode: 711, duration: 0.649s, episode steps: 106, steps per second: 163, episode reward: -51.032, mean reward: -0.481 [-100.000, 50.758], mean action: 1.887 [0.000, 3.000],  loss: 17.351045, mae: 66.477047, mean_q: 76.941972, mean_eps: 0.573757
  71196/150000: episode: 712, duration: 0.645s, episode steps: 102, steps per second: 158, episode reward: -139.428, mean reward: -1.367 [-100.000,  8.030], mean action: 1.588 [0.000, 3.000],  loss: 22.245148, mae: 65.839370, mean_q: 76.469567, mean_eps: 0.573133
  71271/150000: episode: 713, duration: 0.489s, episode steps:  75, steps per second: 153, episode reward: -84.071, mean reward: -1.121 [-100.000,  7.585], mean action: 1.547 [0.000, 3.000],  loss: 8.796217, mae: 66.703776, mean_q: 77.782104, mean_eps: 0.572602
  71358/150000: episode: 714, duration: 0.535s, episode steps:  87, steps per second: 162, episode reward: -32.316, mean reward: -0.371 [-100.000, 17.219], mean action: 1.540 [0.000, 3.000],  loss: 13.653313, mae: 67.818693, mean_q: 79.597525, mean_eps: 0.572116
  71433/150000: episode: 715, duration: 0.462s, episode steps:  75, steps per second: 162, episode reward: -158.323, mean reward: -2.111 [-100.000,  6.279], mean action: 1.573 [0.000, 3.000],  loss: 12.661392, mae: 66.969638, mean_q: 75.933600, mean_eps: 0.571630
  71525/150000: episode: 716, duration: 0.570s, episode steps:  92, steps per second: 161, episode reward: -80.344, mean reward: -0.873 [-100.000,  5.684], mean action: 1.696 [0.000, 3.000],  loss: 22.258312, mae: 65.551489, mean_q: 74.572623, mean_eps: 0.571129
  71657/150000: episode: 717, duration: 0.823s, episode steps: 132, steps per second: 160, episode reward: -66.243, mean reward: -0.502 [-100.000, 10.887], mean action: 1.621 [0.000, 3.000],  loss: 8.859115, mae: 66.015274, mean_q: 75.583497, mean_eps: 0.570457
  71765/150000: episode: 718, duration: 0.669s, episode steps: 108, steps per second: 161, episode reward: -76.577, mean reward: -0.709 [-100.000, 14.544], mean action: 1.713 [0.000, 3.000],  loss: 10.001382, mae: 66.799166, mean_q: 77.271117, mean_eps: 0.569737
  71867/150000: episode: 719, duration: 0.620s, episode steps: 102, steps per second: 164, episode reward: -36.920, mean reward: -0.362 [-100.000, 13.617], mean action: 1.598 [0.000, 3.000],  loss: 12.749955, mae: 66.483407, mean_q: 76.666633, mean_eps: 0.569107
  71998/150000: episode: 720, duration: 0.820s, episode steps: 131, steps per second: 160, episode reward: -129.456, mean reward: -0.988 [-100.000,  6.134], mean action: 1.595 [0.000, 3.000],  loss: 11.448403, mae: 66.435255, mean_q: 76.313981, mean_eps: 0.568408
  72115/150000: episode: 721, duration: 0.724s, episode steps: 117, steps per second: 162, episode reward: -65.690, mean reward: -0.561 [-100.000,  5.854], mean action: 1.590 [0.000, 3.000],  loss: 19.369789, mae: 66.650415, mean_q: 75.947378, mean_eps: 0.567664
  72209/150000: episode: 722, duration: 0.585s, episode steps:  94, steps per second: 161, episode reward: -98.224, mean reward: -1.045 [-100.000, 20.787], mean action: 1.660 [0.000, 3.000],  loss: 16.890631, mae: 66.710300, mean_q: 77.432077, mean_eps: 0.567031
  72302/150000: episode: 723, duration: 0.604s, episode steps:  93, steps per second: 154, episode reward: -116.896, mean reward: -1.257 [-100.000,  9.391], mean action: 1.462 [0.000, 3.000],  loss: 12.629889, mae: 67.042419, mean_q: 76.830007, mean_eps: 0.566470
  72424/150000: episode: 724, duration: 0.917s, episode steps: 122, steps per second: 133, episode reward: -84.178, mean reward: -0.690 [-100.000,  8.774], mean action: 1.451 [0.000, 3.000],  loss: 8.908253, mae: 66.424755, mean_q: 75.476405, mean_eps: 0.565825
  72534/150000: episode: 725, duration: 0.833s, episode steps: 110, steps per second: 132, episode reward: -61.606, mean reward: -0.560 [-100.000, 26.226], mean action: 1.682 [0.000, 3.000],  loss: 14.981913, mae: 66.031330, mean_q: 76.941409, mean_eps: 0.565129
  72636/150000: episode: 726, duration: 0.775s, episode steps: 102, steps per second: 132, episode reward: -57.154, mean reward: -0.560 [-100.000, 29.407], mean action: 1.735 [0.000, 3.000],  loss: 22.062070, mae: 66.067857, mean_q: 74.762023, mean_eps: 0.564493
  72765/150000: episode: 727, duration: 0.905s, episode steps: 129, steps per second: 143, episode reward: -39.651, mean reward: -0.307 [-100.000,  8.857], mean action: 1.310 [0.000, 3.000],  loss: 13.272587, mae: 67.007180, mean_q: 77.384656, mean_eps: 0.563800
  72881/150000: episode: 728, duration: 0.840s, episode steps: 116, steps per second: 138, episode reward: -91.985, mean reward: -0.793 [-100.000,  6.579], mean action: 1.672 [0.000, 3.000],  loss: 14.365892, mae: 67.161944, mean_q: 78.296618, mean_eps: 0.563065
  73033/150000: episode: 729, duration: 1.009s, episode steps: 152, steps per second: 151, episode reward: -130.938, mean reward: -0.861 [-100.000,  7.808], mean action: 1.724 [0.000, 3.000],  loss: 18.382837, mae: 66.281033, mean_q: 76.505791, mean_eps: 0.562261
  73123/150000: episode: 730, duration: 0.550s, episode steps:  90, steps per second: 164, episode reward: -61.454, mean reward: -0.683 [-100.000,  9.925], mean action: 1.622 [0.000, 3.000],  loss: 13.978877, mae: 67.525019, mean_q: 76.922030, mean_eps: 0.561535
  73228/150000: episode: 731, duration: 0.675s, episode steps: 105, steps per second: 156, episode reward: -12.167, mean reward: -0.116 [-100.000, 18.914], mean action: 1.686 [0.000, 3.000],  loss: 12.342949, mae: 66.547000, mean_q: 76.761616, mean_eps: 0.560950
  73324/150000: episode: 732, duration: 0.589s, episode steps:  96, steps per second: 163, episode reward: -52.803, mean reward: -0.550 [-100.000, 19.285], mean action: 1.646 [0.000, 3.000],  loss: 13.316586, mae: 66.841828, mean_q: 76.726860, mean_eps: 0.560347
  73471/150000: episode: 733, duration: 0.892s, episode steps: 147, steps per second: 165, episode reward: -20.168, mean reward: -0.137 [-100.000,  6.397], mean action: 1.755 [0.000, 3.000],  loss: 15.952253, mae: 65.876965, mean_q: 75.110060, mean_eps: 0.559618
  73548/150000: episode: 734, duration: 0.510s, episode steps:  77, steps per second: 151, episode reward: -28.336, mean reward: -0.368 [-100.000, 10.682], mean action: 1.779 [0.000, 3.000],  loss: 12.314172, mae: 66.105324, mean_q: 75.278368, mean_eps: 0.558946
  73681/150000: episode: 735, duration: 0.814s, episode steps: 133, steps per second: 163, episode reward: 18.242, mean reward:  0.137 [-100.000, 17.913], mean action: 1.729 [0.000, 3.000],  loss: 36.037835, mae: 67.801943, mean_q: 77.757774, mean_eps: 0.558316
  73762/150000: episode: 736, duration: 0.489s, episode steps:  81, steps per second: 166, episode reward: -47.396, mean reward: -0.585 [-100.000, 14.491], mean action: 1.593 [0.000, 3.000],  loss: 8.777095, mae: 66.579567, mean_q: 74.706980, mean_eps: 0.557674
  73916/150000: episode: 737, duration: 0.978s, episode steps: 154, steps per second: 157, episode reward: -40.727, mean reward: -0.264 [-100.000, 21.823], mean action: 1.773 [0.000, 3.000],  loss: 19.393322, mae: 66.735142, mean_q: 76.457543, mean_eps: 0.556969
  74048/150000: episode: 738, duration: 0.806s, episode steps: 132, steps per second: 164, episode reward: 12.928, mean reward:  0.098 [-100.000, 16.805], mean action: 1.538 [0.000, 3.000],  loss: 23.558884, mae: 67.658238, mean_q: 78.247247, mean_eps: 0.556111
  74163/150000: episode: 739, duration: 0.700s, episode steps: 115, steps per second: 164, episode reward: -21.309, mean reward: -0.185 [-100.000, 38.955], mean action: 1.704 [0.000, 3.000],  loss: 13.851438, mae: 68.174497, mean_q: 80.769142, mean_eps: 0.555370
  74302/150000: episode: 740, duration: 0.861s, episode steps: 139, steps per second: 161, episode reward: -11.501, mean reward: -0.083 [-100.000, 17.469], mean action: 1.770 [0.000, 3.000],  loss: 15.200500, mae: 67.496651, mean_q: 78.835083, mean_eps: 0.554608
  74396/150000: episode: 741, duration: 0.575s, episode steps:  94, steps per second: 164, episode reward: -8.789, mean reward: -0.094 [-100.000, 10.829], mean action: 1.670 [0.000, 3.000],  loss: 9.523167, mae: 67.819944, mean_q: 80.441147, mean_eps: 0.553909
  74481/150000: episode: 742, duration: 0.523s, episode steps:  85, steps per second: 162, episode reward: -115.173, mean reward: -1.355 [-100.000, 14.943], mean action: 1.659 [0.000, 3.000],  loss: 18.106741, mae: 68.127728, mean_q: 81.401190, mean_eps: 0.553372
  74582/150000: episode: 743, duration: 0.648s, episode steps: 101, steps per second: 156, episode reward: -55.032, mean reward: -0.545 [-100.000,  9.468], mean action: 1.703 [0.000, 3.000],  loss: 23.018300, mae: 67.864391, mean_q: 79.993808, mean_eps: 0.552814
  74654/150000: episode: 744, duration: 0.444s, episode steps:  72, steps per second: 162, episode reward: -12.099, mean reward: -0.168 [-100.000, 14.281], mean action: 1.500 [0.000, 3.000],  loss: 16.359137, mae: 67.696690, mean_q: 78.157198, mean_eps: 0.552295
  74749/150000: episode: 745, duration: 0.576s, episode steps:  95, steps per second: 165, episode reward: -137.709, mean reward: -1.450 [-100.000,  6.318], mean action: 1.674 [0.000, 3.000],  loss: 26.201897, mae: 66.738517, mean_q: 77.378783, mean_eps: 0.551794
  74869/150000: episode: 746, duration: 0.733s, episode steps: 120, steps per second: 164, episode reward: -75.837, mean reward: -0.632 [-100.000,  6.842], mean action: 1.600 [0.000, 3.000],  loss: 20.551779, mae: 67.252799, mean_q: 78.073805, mean_eps: 0.551149
  74962/150000: episode: 747, duration: 0.601s, episode steps:  93, steps per second: 155, episode reward: -69.928, mean reward: -0.752 [-100.000,  5.822], mean action: 1.667 [0.000, 3.000],  loss: 18.836900, mae: 67.805016, mean_q: 78.195998, mean_eps: 0.550510
  75962/150000: episode: 748, duration: 7.038s, episode steps: 1000, steps per second: 142, episode reward: 59.778, mean reward:  0.060 [-23.503, 22.706], mean action: 1.547 [0.000, 3.000],  loss: 14.484077, mae: 66.897990, mean_q: 77.754064, mean_eps: 0.547231
  76074/150000: episode: 749, duration: 0.686s, episode steps: 112, steps per second: 163, episode reward: -32.953, mean reward: -0.294 [-100.000, 11.354], mean action: 1.616 [0.000, 3.000],  loss: 10.233539, mae: 67.385144, mean_q: 78.178920, mean_eps: 0.543895
  76185/150000: episode: 750, duration: 0.710s, episode steps: 111, steps per second: 156, episode reward: -42.873, mean reward: -0.386 [-100.000, 14.508], mean action: 1.784 [0.000, 3.000],  loss: 23.054782, mae: 67.549694, mean_q: 78.071856, mean_eps: 0.543226
  76330/150000: episode: 751, duration: 0.889s, episode steps: 145, steps per second: 163, episode reward: 12.899, mean reward:  0.089 [-100.000, 22.663], mean action: 1.717 [0.000, 3.000],  loss: 27.410919, mae: 66.433180, mean_q: 76.415296, mean_eps: 0.542458
  76469/150000: episode: 752, duration: 0.851s, episode steps: 139, steps per second: 163, episode reward: -85.017, mean reward: -0.612 [-100.000, 31.430], mean action: 1.727 [0.000, 3.000],  loss: 13.161804, mae: 67.116096, mean_q: 77.730985, mean_eps: 0.541606
  76598/150000: episode: 753, duration: 0.809s, episode steps: 129, steps per second: 160, episode reward: -72.136, mean reward: -0.559 [-100.000,  8.417], mean action: 1.729 [0.000, 3.000],  loss: 25.255253, mae: 68.050177, mean_q: 78.821697, mean_eps: 0.540802
  76710/150000: episode: 754, duration: 0.685s, episode steps: 112, steps per second: 163, episode reward:  5.427, mean reward:  0.048 [-100.000, 21.206], mean action: 1.661 [0.000, 3.000],  loss: 8.342518, mae: 66.933488, mean_q: 77.206070, mean_eps: 0.540079
  76818/150000: episode: 755, duration: 0.665s, episode steps: 108, steps per second: 162, episode reward: -70.799, mean reward: -0.656 [-100.000, 16.786], mean action: 1.593 [0.000, 3.000],  loss: 14.098269, mae: 66.753752, mean_q: 76.751323, mean_eps: 0.539419
  76986/150000: episode: 756, duration: 1.059s, episode steps: 168, steps per second: 159, episode reward:  2.526, mean reward:  0.015 [-100.000,  7.992], mean action: 1.482 [0.000, 3.000],  loss: 10.876998, mae: 66.848914, mean_q: 78.570204, mean_eps: 0.538591
  77065/150000: episode: 757, duration: 0.492s, episode steps:  79, steps per second: 161, episode reward: -73.226, mean reward: -0.927 [-100.000,  5.762], mean action: 1.633 [0.000, 3.000],  loss: 37.044380, mae: 65.979033, mean_q: 77.220800, mean_eps: 0.537850
  77152/150000: episode: 758, duration: 0.566s, episode steps:  87, steps per second: 154, episode reward: -58.497, mean reward: -0.672 [-100.000, 17.135], mean action: 1.701 [0.000, 3.000],  loss: 11.913553, mae: 66.082381, mean_q: 77.131190, mean_eps: 0.537352
  77264/150000: episode: 759, duration: 0.930s, episode steps: 112, steps per second: 120, episode reward: -93.957, mean reward: -0.839 [-100.000, 18.201], mean action: 1.812 [0.000, 3.000],  loss: 17.727126, mae: 66.602846, mean_q: 77.023414, mean_eps: 0.536755
  77397/150000: episode: 760, duration: 1.116s, episode steps: 133, steps per second: 119, episode reward: -34.135, mean reward: -0.257 [-100.000, 31.769], mean action: 1.647 [0.000, 3.000],  loss: 12.835293, mae: 67.164175, mean_q: 77.178384, mean_eps: 0.536020
  77482/150000: episode: 761, duration: 0.599s, episode steps:  85, steps per second: 142, episode reward: -57.977, mean reward: -0.682 [-100.000, 11.979], mean action: 1.647 [0.000, 3.000],  loss: 14.991029, mae: 67.239045, mean_q: 76.978462, mean_eps: 0.535366
  77611/150000: episode: 762, duration: 0.876s, episode steps: 129, steps per second: 147, episode reward: -68.015, mean reward: -0.527 [-100.000,  9.320], mean action: 1.744 [0.000, 3.000],  loss: 19.579428, mae: 66.877899, mean_q: 76.917128, mean_eps: 0.534724
  77767/150000: episode: 763, duration: 1.085s, episode steps: 156, steps per second: 144, episode reward: 26.907, mean reward:  0.172 [-100.000, 17.878], mean action: 1.647 [0.000, 3.000],  loss: 10.502522, mae: 67.153423, mean_q: 77.489557, mean_eps: 0.533869
  77882/150000: episode: 764, duration: 0.837s, episode steps: 115, steps per second: 137, episode reward: -162.524, mean reward: -1.413 [-100.000,  7.593], mean action: 1.678 [0.000, 3.000],  loss: 12.614301, mae: 66.651850, mean_q: 76.085520, mean_eps: 0.533056
  78005/150000: episode: 765, duration: 0.897s, episode steps: 123, steps per second: 137, episode reward: -41.504, mean reward: -0.337 [-100.000, 22.453], mean action: 1.724 [0.000, 3.000],  loss: 9.715734, mae: 67.123517, mean_q: 77.425713, mean_eps: 0.532342
  78115/150000: episode: 766, duration: 0.711s, episode steps: 110, steps per second: 155, episode reward: -4.855, mean reward: -0.044 [-100.000, 14.608], mean action: 1.591 [0.000, 3.000],  loss: 9.934006, mae: 67.507903, mean_q: 77.358459, mean_eps: 0.531643
  78215/150000: episode: 767, duration: 0.628s, episode steps: 100, steps per second: 159, episode reward: -90.693, mean reward: -0.907 [-100.000,  8.547], mean action: 1.630 [0.000, 3.000],  loss: 16.749130, mae: 67.370751, mean_q: 77.172152, mean_eps: 0.531013
  78286/150000: episode: 768, duration: 0.434s, episode steps:  71, steps per second: 164, episode reward: -95.913, mean reward: -1.351 [-100.000,  3.201], mean action: 1.662 [0.000, 3.000],  loss: 49.023605, mae: 67.875562, mean_q: 78.350276, mean_eps: 0.530500
  78388/150000: episode: 769, duration: 0.623s, episode steps: 102, steps per second: 164, episode reward: -103.191, mean reward: -1.012 [-100.000,  6.558], mean action: 1.529 [0.000, 3.000],  loss: 21.528214, mae: 66.794461, mean_q: 76.552182, mean_eps: 0.529981
  78512/150000: episode: 770, duration: 0.788s, episode steps: 124, steps per second: 157, episode reward: -62.142, mean reward: -0.501 [-100.000,  7.408], mean action: 1.782 [0.000, 3.000],  loss: 25.332488, mae: 67.125617, mean_q: 77.569857, mean_eps: 0.529303
  78677/150000: episode: 771, duration: 1.115s, episode steps: 165, steps per second: 148, episode reward: 25.601, mean reward:  0.155 [-100.000, 15.357], mean action: 1.685 [0.000, 3.000],  loss: 15.859942, mae: 67.693331, mean_q: 78.441655, mean_eps: 0.528436
  78748/150000: episode: 772, duration: 0.509s, episode steps:  71, steps per second: 140, episode reward: -64.754, mean reward: -0.912 [-100.000,  8.668], mean action: 1.437 [0.000, 3.000],  loss: 13.717008, mae: 66.278978, mean_q: 75.760138, mean_eps: 0.527728
  78842/150000: episode: 773, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -0.401, mean reward: -0.004 [-100.000,  9.978], mean action: 1.574 [0.000, 3.000],  loss: 15.168253, mae: 68.352579, mean_q: 79.772841, mean_eps: 0.527233
  78913/150000: episode: 774, duration: 0.466s, episode steps:  71, steps per second: 152, episode reward: -24.476, mean reward: -0.345 [-100.000, 10.410], mean action: 1.831 [0.000, 3.000],  loss: 9.836631, mae: 67.457401, mean_q: 79.235636, mean_eps: 0.526738
  79040/150000: episode: 775, duration: 0.840s, episode steps: 127, steps per second: 151, episode reward: -185.695, mean reward: -1.462 [-100.000, 38.183], mean action: 1.402 [0.000, 3.000],  loss: 17.445788, mae: 67.872504, mean_q: 78.878493, mean_eps: 0.526144
  79194/150000: episode: 776, duration: 1.009s, episode steps: 154, steps per second: 153, episode reward: -210.048, mean reward: -1.364 [-100.000, 74.094], mean action: 1.506 [0.000, 3.000],  loss: 13.392157, mae: 66.902728, mean_q: 77.682394, mean_eps: 0.525301
  79335/150000: episode: 777, duration: 0.864s, episode steps: 141, steps per second: 163, episode reward: -206.350, mean reward: -1.463 [-100.000, 17.282], mean action: 1.688 [0.000, 3.000],  loss: 10.293702, mae: 66.968670, mean_q: 78.469759, mean_eps: 0.524416
  79472/150000: episode: 778, duration: 0.857s, episode steps: 137, steps per second: 160, episode reward: -66.930, mean reward: -0.489 [-100.000, 13.351], mean action: 1.562 [0.000, 3.000],  loss: 8.992461, mae: 67.322485, mean_q: 77.729355, mean_eps: 0.523582
  79592/150000: episode: 779, duration: 0.739s, episode steps: 120, steps per second: 162, episode reward: 17.950, mean reward:  0.150 [-100.000, 17.935], mean action: 1.650 [0.000, 3.000],  loss: 10.802903, mae: 66.157242, mean_q: 76.311922, mean_eps: 0.522811
  79667/150000: episode: 780, duration: 0.473s, episode steps:  75, steps per second: 159, episode reward: -82.458, mean reward: -1.099 [-100.000, 25.291], mean action: 1.560 [0.000, 3.000],  loss: 18.107757, mae: 66.604789, mean_q: 77.553832, mean_eps: 0.522226
  79781/150000: episode: 781, duration: 0.733s, episode steps: 114, steps per second: 156, episode reward: -113.986, mean reward: -1.000 [-100.000, 18.554], mean action: 1.667 [0.000, 3.000],  loss: 18.762184, mae: 66.077636, mean_q: 75.712333, mean_eps: 0.521659
  79901/150000: episode: 782, duration: 0.736s, episode steps: 120, steps per second: 163, episode reward: -120.212, mean reward: -1.002 [-100.000,  9.640], mean action: 1.558 [0.000, 3.000],  loss: 16.422625, mae: 66.400886, mean_q: 76.674608, mean_eps: 0.520957
  79983/150000: episode: 783, duration: 0.502s, episode steps:  82, steps per second: 163, episode reward: -143.483, mean reward: -1.750 [-100.000,  8.794], mean action: 1.780 [0.000, 3.000],  loss: 11.287505, mae: 67.222197, mean_q: 77.941026, mean_eps: 0.520351
  80103/150000: episode: 784, duration: 0.760s, episode steps: 120, steps per second: 158, episode reward: -87.427, mean reward: -0.729 [-100.000,  7.604], mean action: 1.758 [0.000, 3.000],  loss: 21.632327, mae: 66.280088, mean_q: 78.071670, mean_eps: 0.519745
  80184/150000: episode: 785, duration: 0.522s, episode steps:  81, steps per second: 155, episode reward: -31.854, mean reward: -0.393 [-100.000,  8.840], mean action: 1.778 [0.000, 3.000],  loss: 22.779268, mae: 65.366267, mean_q: 75.837990, mean_eps: 0.519142
  80254/150000: episode: 786, duration: 0.432s, episode steps:  70, steps per second: 162, episode reward: -58.939, mean reward: -0.842 [-100.000, 11.480], mean action: 1.800 [0.000, 3.000],  loss: 28.257495, mae: 66.161137, mean_q: 77.363657, mean_eps: 0.518689
  80391/150000: episode: 787, duration: 0.838s, episode steps: 137, steps per second: 164, episode reward: -56.383, mean reward: -0.412 [-100.000, 31.674], mean action: 1.832 [0.000, 3.000],  loss: 24.510287, mae: 66.767321, mean_q: 78.694392, mean_eps: 0.518068
  80497/150000: episode: 788, duration: 0.704s, episode steps: 106, steps per second: 151, episode reward: -34.190, mean reward: -0.323 [-100.000, 13.445], mean action: 1.642 [0.000, 3.000],  loss: 14.653470, mae: 66.661461, mean_q: 77.998171, mean_eps: 0.517339
  80604/150000: episode: 789, duration: 0.682s, episode steps: 107, steps per second: 157, episode reward: -57.582, mean reward: -0.538 [-100.000,  8.111], mean action: 1.841 [0.000, 3.000],  loss: 17.142586, mae: 66.619704, mean_q: 77.238069, mean_eps: 0.516700
  80718/150000: episode: 790, duration: 0.698s, episode steps: 114, steps per second: 163, episode reward: 14.065, mean reward:  0.123 [-100.000, 19.882], mean action: 1.816 [0.000, 3.000],  loss: 19.782994, mae: 66.105281, mean_q: 77.349040, mean_eps: 0.516037
  80835/150000: episode: 791, duration: 0.756s, episode steps: 117, steps per second: 155, episode reward: -130.655, mean reward: -1.117 [-100.000,  5.047], mean action: 1.821 [0.000, 3.000],  loss: 18.196431, mae: 65.909643, mean_q: 77.673446, mean_eps: 0.515344
  81835/150000: episode: 792, duration: 7.245s, episode steps: 1000, steps per second: 138, episode reward: 75.694, mean reward:  0.076 [-20.237, 22.804], mean action: 1.763 [0.000, 3.000],  loss: 15.476614, mae: 66.337004, mean_q: 77.663346, mean_eps: 0.511993
  81910/150000: episode: 793, duration: 0.454s, episode steps:  75, steps per second: 165, episode reward: -212.306, mean reward: -2.831 [-100.000, 11.559], mean action: 1.947 [0.000, 3.000],  loss: 13.460634, mae: 66.542008, mean_q: 78.574912, mean_eps: 0.508768
  82028/150000: episode: 794, duration: 0.737s, episode steps: 118, steps per second: 160, episode reward: -12.946, mean reward: -0.110 [-100.000,  7.490], mean action: 1.737 [0.000, 3.000],  loss: 15.980226, mae: 66.780355, mean_q: 78.614491, mean_eps: 0.508189
  82100/150000: episode: 795, duration: 0.443s, episode steps:  72, steps per second: 162, episode reward: -114.054, mean reward: -1.584 [-100.000, 17.625], mean action: 1.653 [0.000, 3.000],  loss: 12.905037, mae: 65.808342, mean_q: 76.285726, mean_eps: 0.507619
  82203/150000: episode: 796, duration: 0.660s, episode steps: 103, steps per second: 156, episode reward: -81.879, mean reward: -0.795 [-100.000, 18.733], mean action: 1.485 [0.000, 3.000],  loss: 9.115237, mae: 66.668432, mean_q: 77.903797, mean_eps: 0.507094
  82288/150000: episode: 797, duration: 0.629s, episode steps:  85, steps per second: 135, episode reward: -45.571, mean reward: -0.536 [-100.000, 11.657], mean action: 1.588 [0.000, 3.000],  loss: 13.735488, mae: 67.432948, mean_q: 78.264649, mean_eps: 0.506530
  82456/150000: episode: 798, duration: 1.271s, episode steps: 168, steps per second: 132, episode reward: -8.463, mean reward: -0.050 [-100.000, 17.700], mean action: 1.690 [0.000, 3.000],  loss: 11.990093, mae: 66.483994, mean_q: 77.324561, mean_eps: 0.505771
  82586/150000: episode: 799, duration: 0.907s, episode steps: 130, steps per second: 143, episode reward: -118.119, mean reward: -0.909 [-100.000, 12.384], mean action: 1.585 [0.000, 3.000],  loss: 24.578451, mae: 65.177683, mean_q: 75.856900, mean_eps: 0.504877
  82712/150000: episode: 800, duration: 0.871s, episode steps: 126, steps per second: 145, episode reward: -68.684, mean reward: -0.545 [-100.000, 13.124], mean action: 1.754 [0.000, 3.000],  loss: 15.685281, mae: 66.778980, mean_q: 78.384231, mean_eps: 0.504109
  82784/150000: episode: 801, duration: 0.492s, episode steps:  72, steps per second: 146, episode reward: -119.072, mean reward: -1.654 [-100.000,  7.157], mean action: 1.472 [0.000, 3.000],  loss: 13.292030, mae: 65.956635, mean_q: 77.555652, mean_eps: 0.503515
  82897/150000: episode: 802, duration: 0.735s, episode steps: 113, steps per second: 154, episode reward: -21.428, mean reward: -0.190 [-100.000, 15.712], mean action: 1.876 [0.000, 3.000],  loss: 16.259967, mae: 66.235504, mean_q: 77.510075, mean_eps: 0.502960
  83042/150000: episode: 803, duration: 0.938s, episode steps: 145, steps per second: 155, episode reward: -44.184, mean reward: -0.305 [-100.000, 11.002], mean action: 1.641 [0.000, 3.000],  loss: 11.947216, mae: 66.090834, mean_q: 76.765210, mean_eps: 0.502186
  83128/150000: episode: 804, duration: 0.651s, episode steps:  86, steps per second: 132, episode reward: -65.132, mean reward: -0.757 [-100.000,  9.656], mean action: 1.721 [0.000, 3.000],  loss: 13.763060, mae: 65.103046, mean_q: 76.290593, mean_eps: 0.501493
  83221/150000: episode: 805, duration: 0.596s, episode steps:  93, steps per second: 156, episode reward: -78.838, mean reward: -0.848 [-100.000, 11.961], mean action: 1.484 [0.000, 3.000],  loss: 29.170199, mae: 66.315368, mean_q: 78.100504, mean_eps: 0.500956
  83367/150000: episode: 806, duration: 0.903s, episode steps: 146, steps per second: 162, episode reward: -23.798, mean reward: -0.163 [-100.000, 13.982], mean action: 1.603 [0.000, 3.000],  loss: 14.451629, mae: 65.757265, mean_q: 77.103440, mean_eps: 0.500239
  83515/150000: episode: 807, duration: 0.903s, episode steps: 148, steps per second: 164, episode reward: -3.980, mean reward: -0.027 [-100.000, 21.139], mean action: 1.622 [0.000, 3.000],  loss: 25.016651, mae: 65.889557, mean_q: 78.517453, mean_eps: 0.499357
  83607/150000: episode: 808, duration: 0.570s, episode steps:  92, steps per second: 161, episode reward: -31.377, mean reward: -0.341 [-100.000, 11.279], mean action: 1.620 [0.000, 3.000],  loss: 12.904216, mae: 66.036041, mean_q: 77.621471, mean_eps: 0.498637
  83716/150000: episode: 809, duration: 0.690s, episode steps: 109, steps per second: 158, episode reward: -12.202, mean reward: -0.112 [-100.000, 22.358], mean action: 1.734 [0.000, 3.000],  loss: 17.881131, mae: 65.110141, mean_q: 76.096216, mean_eps: 0.498034
  83865/150000: episode: 810, duration: 0.936s, episode steps: 149, steps per second: 159, episode reward: -112.654, mean reward: -0.756 [-100.000,  8.707], mean action: 1.617 [0.000, 3.000],  loss: 17.231551, mae: 66.595013, mean_q: 78.213518, mean_eps: 0.497260
  84865/150000: episode: 811, duration: 6.974s, episode steps: 1000, steps per second: 143, episode reward: 105.095, mean reward:  0.105 [-21.423, 24.341], mean action: 1.820 [0.000, 3.000],  loss: 14.851697, mae: 65.904915, mean_q: 77.163017, mean_eps: 0.493813
  84955/150000: episode: 812, duration: 0.589s, episode steps:  90, steps per second: 153, episode reward: -50.842, mean reward: -0.565 [-100.000, 11.548], mean action: 1.722 [0.000, 3.000],  loss: 29.772329, mae: 64.767187, mean_q: 75.744807, mean_eps: 0.490543
  85035/150000: episode: 813, duration: 0.494s, episode steps:  80, steps per second: 162, episode reward: -17.276, mean reward: -0.216 [-100.000, 19.282], mean action: 1.663 [0.000, 3.000],  loss: 11.717185, mae: 67.063372, mean_q: 79.816422, mean_eps: 0.490033
  85204/150000: episode: 814, duration: 1.053s, episode steps: 169, steps per second: 160, episode reward: -36.078, mean reward: -0.213 [-100.000, 13.842], mean action: 1.876 [0.000, 3.000],  loss: 15.840181, mae: 67.210291, mean_q: 79.639643, mean_eps: 0.489286
  85296/150000: episode: 815, duration: 0.615s, episode steps:  92, steps per second: 150, episode reward: -66.207, mean reward: -0.720 [-100.000,  8.825], mean action: 1.478 [0.000, 3.000],  loss: 13.536848, mae: 65.755116, mean_q: 77.373651, mean_eps: 0.488503
  85408/150000: episode: 816, duration: 0.691s, episode steps: 112, steps per second: 162, episode reward: -109.784, mean reward: -0.980 [-100.000, 12.888], mean action: 1.688 [0.000, 3.000],  loss: 18.139621, mae: 66.856691, mean_q: 78.021377, mean_eps: 0.487891
  85492/150000: episode: 817, duration: 0.516s, episode steps:  84, steps per second: 163, episode reward: -87.423, mean reward: -1.041 [-100.000, 10.501], mean action: 1.452 [0.000, 3.000],  loss: 17.155168, mae: 65.595840, mean_q: 74.871077, mean_eps: 0.487303
  86492/150000: episode: 818, duration: 7.363s, episode steps: 1000, steps per second: 136, episode reward:  0.789, mean reward:  0.001 [-25.160, 27.846], mean action: 1.671 [0.000, 3.000],  loss: 16.658756, mae: 66.519761, mean_q: 78.208085, mean_eps: 0.484051
  86595/150000: episode: 819, duration: 0.636s, episode steps: 103, steps per second: 162, episode reward: -18.476, mean reward: -0.179 [-100.000, 13.268], mean action: 1.534 [0.000, 3.000],  loss: 24.831494, mae: 65.820626, mean_q: 77.169936, mean_eps: 0.480742
  86717/150000: episode: 820, duration: 0.759s, episode steps: 122, steps per second: 161, episode reward: -50.990, mean reward: -0.418 [-100.000, 12.725], mean action: 1.770 [0.000, 3.000],  loss: 29.538495, mae: 67.074661, mean_q: 79.643177, mean_eps: 0.480067
  86844/150000: episode: 821, duration: 0.915s, episode steps: 127, steps per second: 139, episode reward: -63.383, mean reward: -0.499 [-100.000,  8.077], mean action: 1.646 [0.000, 3.000],  loss: 16.376510, mae: 67.610625, mean_q: 80.372919, mean_eps: 0.479320
  86934/150000: episode: 822, duration: 0.629s, episode steps:  90, steps per second: 143, episode reward: -97.249, mean reward: -1.081 [-100.000,  8.407], mean action: 1.678 [0.000, 3.000],  loss: 13.704730, mae: 67.125517, mean_q: 79.529394, mean_eps: 0.478669
  87097/150000: episode: 823, duration: 1.085s, episode steps: 163, steps per second: 150, episode reward: -27.074, mean reward: -0.166 [-100.000, 16.439], mean action: 1.785 [0.000, 3.000],  loss: 13.024056, mae: 66.561080, mean_q: 78.451249, mean_eps: 0.477910
  87213/150000: episode: 824, duration: 0.738s, episode steps: 116, steps per second: 157, episode reward: -193.692, mean reward: -1.670 [-100.000, 42.125], mean action: 1.431 [0.000, 3.000],  loss: 17.128608, mae: 66.259696, mean_q: 77.804046, mean_eps: 0.477073
  87341/150000: episode: 825, duration: 0.778s, episode steps: 128, steps per second: 165, episode reward: -68.504, mean reward: -0.535 [-100.000, 12.258], mean action: 1.438 [0.000, 3.000],  loss: 13.357488, mae: 66.067593, mean_q: 77.400845, mean_eps: 0.476341
  87445/150000: episode: 826, duration: 0.663s, episode steps: 104, steps per second: 157, episode reward: 23.220, mean reward:  0.223 [-100.000, 78.314], mean action: 1.538 [0.000, 3.000],  loss: 18.600341, mae: 66.931689, mean_q: 79.329511, mean_eps: 0.475645
  87575/150000: episode: 827, duration: 0.833s, episode steps: 130, steps per second: 156, episode reward: -9.723, mean reward: -0.075 [-100.000, 16.782], mean action: 1.685 [0.000, 3.000],  loss: 22.519708, mae: 66.136268, mean_q: 76.644348, mean_eps: 0.474943
  87677/150000: episode: 828, duration: 0.641s, episode steps: 102, steps per second: 159, episode reward: 33.125, mean reward:  0.325 [-100.000, 20.091], mean action: 1.735 [0.000, 3.000],  loss: 25.964085, mae: 65.624455, mean_q: 77.336705, mean_eps: 0.474247
  87756/150000: episode: 829, duration: 0.502s, episode steps:  79, steps per second: 157, episode reward: -44.294, mean reward: -0.561 [-100.000, 14.152], mean action: 1.734 [0.000, 3.000],  loss: 28.964313, mae: 65.767562, mean_q: 76.483735, mean_eps: 0.473704
  87847/150000: episode: 830, duration: 0.612s, episode steps:  91, steps per second: 149, episode reward: -103.759, mean reward: -1.140 [-100.000, 12.645], mean action: 1.527 [0.000, 3.000],  loss: 12.645980, mae: 66.825500, mean_q: 79.071924, mean_eps: 0.473194
  87941/150000: episode: 831, duration: 0.597s, episode steps:  94, steps per second: 157, episode reward: -187.365, mean reward: -1.993 [-100.000,  6.726], mean action: 1.223 [0.000, 3.000],  loss: 13.613893, mae: 66.513907, mean_q: 79.090533, mean_eps: 0.472639
  88052/150000: episode: 832, duration: 0.683s, episode steps: 111, steps per second: 163, episode reward: -27.301, mean reward: -0.246 [-100.000,  7.250], mean action: 1.595 [0.000, 3.000],  loss: 14.351139, mae: 66.025918, mean_q: 77.955791, mean_eps: 0.472024
  88135/150000: episode: 833, duration: 0.533s, episode steps:  83, steps per second: 156, episode reward: -104.753, mean reward: -1.262 [-100.000,  8.989], mean action: 1.506 [0.000, 3.000],  loss: 19.309243, mae: 65.017141, mean_q: 77.181832, mean_eps: 0.471442
  88236/150000: episode: 834, duration: 0.644s, episode steps: 101, steps per second: 157, episode reward: -46.648, mean reward: -0.462 [-100.000, 11.646], mean action: 1.901 [0.000, 3.000],  loss: 19.476440, mae: 66.854138, mean_q: 79.320756, mean_eps: 0.470890
  88398/150000: episode: 835, duration: 1.051s, episode steps: 162, steps per second: 154, episode reward: -130.326, mean reward: -0.804 [-100.000, 13.328], mean action: 1.549 [0.000, 3.000],  loss: 26.111332, mae: 66.250561, mean_q: 78.569896, mean_eps: 0.470101
  88534/150000: episode: 836, duration: 0.967s, episode steps: 136, steps per second: 141, episode reward: -78.080, mean reward: -0.574 [-100.000,  6.911], mean action: 1.625 [0.000, 3.000],  loss: 26.018363, mae: 66.779902, mean_q: 78.849301, mean_eps: 0.469207
  88654/150000: episode: 837, duration: 0.785s, episode steps: 120, steps per second: 153, episode reward: -30.330, mean reward: -0.253 [-100.000, 21.004], mean action: 1.717 [0.000, 3.000],  loss: 23.257667, mae: 66.755016, mean_q: 79.686640, mean_eps: 0.468439
  88761/150000: episode: 838, duration: 0.697s, episode steps: 107, steps per second: 153, episode reward: -10.172, mean reward: -0.095 [-100.000, 11.773], mean action: 1.729 [0.000, 3.000],  loss: 14.177861, mae: 65.767419, mean_q: 77.697139, mean_eps: 0.467758
  88917/150000: episode: 839, duration: 1.051s, episode steps: 156, steps per second: 148, episode reward: -121.854, mean reward: -0.781 [-100.000,  5.165], mean action: 1.705 [0.000, 3.000],  loss: 17.980453, mae: 66.559768, mean_q: 78.305132, mean_eps: 0.466969
  89087/150000: episode: 840, duration: 1.073s, episode steps: 170, steps per second: 158, episode reward: -12.057, mean reward: -0.071 [-100.000, 15.026], mean action: 1.724 [0.000, 3.000],  loss: 18.150002, mae: 67.111565, mean_q: 79.955835, mean_eps: 0.465991
  89212/150000: episode: 841, duration: 0.781s, episode steps: 125, steps per second: 160, episode reward: -34.333, mean reward: -0.275 [-100.000, 12.866], mean action: 1.680 [0.000, 3.000],  loss: 14.078031, mae: 65.755160, mean_q: 77.889137, mean_eps: 0.465106
  89312/150000: episode: 842, duration: 0.644s, episode steps: 100, steps per second: 155, episode reward: -38.047, mean reward: -0.380 [-100.000, 12.236], mean action: 1.690 [0.000, 3.000],  loss: 29.721054, mae: 66.167993, mean_q: 79.401957, mean_eps: 0.464431
  89392/150000: episode: 843, duration: 0.493s, episode steps:  80, steps per second: 162, episode reward: -56.369, mean reward: -0.705 [-100.000, 20.648], mean action: 1.675 [0.000, 3.000],  loss: 17.367610, mae: 66.106397, mean_q: 78.110653, mean_eps: 0.463891
  89485/150000: episode: 844, duration: 0.616s, episode steps:  93, steps per second: 151, episode reward: -33.216, mean reward: -0.357 [-100.000, 16.566], mean action: 1.656 [0.000, 3.000],  loss: 18.278911, mae: 66.422710, mean_q: 79.008521, mean_eps: 0.463372
  89589/150000: episode: 845, duration: 0.638s, episode steps: 104, steps per second: 163, episode reward: 18.559, mean reward:  0.178 [-100.000, 17.794], mean action: 1.625 [0.000, 3.000],  loss: 19.709802, mae: 66.666190, mean_q: 79.121611, mean_eps: 0.462781
  89679/150000: episode: 846, duration: 0.549s, episode steps:  90, steps per second: 164, episode reward: -62.134, mean reward: -0.690 [-100.000, 10.408], mean action: 1.833 [0.000, 3.000],  loss: 19.002391, mae: 66.833000, mean_q: 79.849049, mean_eps: 0.462199
  89771/150000: episode: 847, duration: 0.575s, episode steps:  92, steps per second: 160, episode reward: -53.180, mean reward: -0.578 [-100.000, 19.929], mean action: 1.924 [0.000, 3.000],  loss: 18.038941, mae: 65.789944, mean_q: 77.139482, mean_eps: 0.461653
  89900/150000: episode: 848, duration: 0.820s, episode steps: 129, steps per second: 157, episode reward: 12.354, mean reward:  0.096 [-100.000, 20.271], mean action: 1.775 [0.000, 3.000],  loss: 12.620868, mae: 66.394276, mean_q: 78.266579, mean_eps: 0.460990
  90001/150000: episode: 849, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: -17.043, mean reward: -0.169 [-100.000, 42.120], mean action: 1.614 [0.000, 3.000],  loss: 20.612956, mae: 66.027007, mean_q: 78.259003, mean_eps: 0.460300
  90166/150000: episode: 850, duration: 1.067s, episode steps: 165, steps per second: 155, episode reward: -3.412, mean reward: -0.021 [-100.000, 14.551], mean action: 1.630 [0.000, 3.000],  loss: 26.545601, mae: 65.741741, mean_q: 79.937207, mean_eps: 0.459502
  90289/150000: episode: 851, duration: 0.771s, episode steps: 123, steps per second: 160, episode reward: -20.246, mean reward: -0.165 [-100.000, 22.152], mean action: 1.545 [0.000, 3.000],  loss: 17.444412, mae: 66.297083, mean_q: 79.714731, mean_eps: 0.458638
  90515/150000: episode: 852, duration: 1.445s, episode steps: 226, steps per second: 156, episode reward: -143.380, mean reward: -0.634 [-100.000, 20.229], mean action: 1.774 [0.000, 3.000],  loss: 16.814645, mae: 66.644849, mean_q: 79.877569, mean_eps: 0.457591
  90632/150000: episode: 853, duration: 0.728s, episode steps: 117, steps per second: 161, episode reward: -10.612, mean reward: -0.091 [-100.000, 19.871], mean action: 1.496 [0.000, 3.000],  loss: 14.543599, mae: 66.695350, mean_q: 80.421488, mean_eps: 0.456562
  90743/150000: episode: 854, duration: 0.675s, episode steps: 111, steps per second: 164, episode reward: -47.402, mean reward: -0.427 [-100.000, 18.071], mean action: 1.676 [0.000, 3.000],  loss: 17.459902, mae: 65.754778, mean_q: 78.618514, mean_eps: 0.455878
  90854/150000: episode: 855, duration: 0.716s, episode steps: 111, steps per second: 155, episode reward: -71.201, mean reward: -0.641 [-100.000,  8.840], mean action: 1.640 [0.000, 3.000],  loss: 23.793992, mae: 66.679599, mean_q: 80.994286, mean_eps: 0.455212
  90954/150000: episode: 856, duration: 0.637s, episode steps: 100, steps per second: 157, episode reward: -17.604, mean reward: -0.176 [-100.000,  6.023], mean action: 1.670 [0.000, 3.000],  loss: 20.726351, mae: 65.682159, mean_q: 77.882463, mean_eps: 0.454579
  91055/150000: episode: 857, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: -7.284, mean reward: -0.072 [-100.000, 15.338], mean action: 1.703 [0.000, 3.000],  loss: 22.982893, mae: 67.733242, mean_q: 82.552640, mean_eps: 0.453976
  91216/150000: episode: 858, duration: 1.043s, episode steps: 161, steps per second: 154, episode reward: -148.018, mean reward: -0.919 [-100.000, 78.695], mean action: 1.863 [0.000, 3.000],  loss: 23.823153, mae: 66.979712, mean_q: 80.582784, mean_eps: 0.453190
  91354/150000: episode: 859, duration: 0.844s, episode steps: 138, steps per second: 164, episode reward: -227.872, mean reward: -1.651 [-100.000, 14.910], mean action: 1.543 [0.000, 3.000],  loss: 19.273835, mae: 66.527242, mean_q: 80.479341, mean_eps: 0.452293
  91505/150000: episode: 860, duration: 0.979s, episode steps: 151, steps per second: 154, episode reward: -43.502, mean reward: -0.288 [-100.000, 15.368], mean action: 1.636 [0.000, 3.000],  loss: 22.308019, mae: 66.704246, mean_q: 81.433097, mean_eps: 0.451426
  91655/150000: episode: 861, duration: 0.998s, episode steps: 150, steps per second: 150, episode reward: 31.799, mean reward:  0.212 [-100.000, 14.341], mean action: 1.547 [0.000, 3.000],  loss: 17.965217, mae: 67.322929, mean_q: 81.778872, mean_eps: 0.450523
  91753/150000: episode: 862, duration: 0.614s, episode steps:  98, steps per second: 160, episode reward: -3.507, mean reward: -0.036 [-100.000, 11.749], mean action: 1.612 [0.000, 3.000],  loss: 32.721116, mae: 66.859070, mean_q: 81.911523, mean_eps: 0.449779
  91868/150000: episode: 863, duration: 0.724s, episode steps: 115, steps per second: 159, episode reward: -98.473, mean reward: -0.856 [-100.000, 14.213], mean action: 1.391 [0.000, 3.000],  loss: 18.538980, mae: 67.037228, mean_q: 81.779705, mean_eps: 0.449140
  92011/150000: episode: 864, duration: 0.891s, episode steps: 143, steps per second: 160, episode reward: 27.053, mean reward:  0.189 [-100.000, 21.120], mean action: 1.769 [0.000, 3.000],  loss: 29.656840, mae: 66.950460, mean_q: 80.106806, mean_eps: 0.448366
  93011/150000: episode: 865, duration: 7.701s, episode steps: 1000, steps per second: 130, episode reward: 18.010, mean reward:  0.018 [-22.852, 23.391], mean action: 1.658 [0.000, 3.000],  loss: 19.884556, mae: 66.568885, mean_q: 81.327667, mean_eps: 0.444937
  93119/150000: episode: 866, duration: 0.662s, episode steps: 108, steps per second: 163, episode reward: -20.953, mean reward: -0.194 [-100.000,  9.568], mean action: 1.806 [0.000, 3.000],  loss: 21.596542, mae: 67.427093, mean_q: 82.109292, mean_eps: 0.441613
  94119/150000: episode: 867, duration: 7.173s, episode steps: 1000, steps per second: 139, episode reward: 37.774, mean reward:  0.038 [-24.157, 24.691], mean action: 1.255 [0.000, 3.000],  loss: 19.786440, mae: 67.452090, mean_q: 82.749272, mean_eps: 0.438289
  94240/150000: episode: 868, duration: 0.762s, episode steps: 121, steps per second: 159, episode reward: -44.426, mean reward: -0.367 [-100.000, 11.778], mean action: 1.769 [0.000, 3.000],  loss: 16.000905, mae: 68.157956, mean_q: 83.610156, mean_eps: 0.434926
  94458/150000: episode: 869, duration: 1.365s, episode steps: 218, steps per second: 160, episode reward: -189.420, mean reward: -0.869 [-100.000, 20.618], mean action: 1.702 [0.000, 3.000],  loss: 24.259377, mae: 68.279731, mean_q: 83.172784, mean_eps: 0.433909
  94599/150000: episode: 870, duration: 0.896s, episode steps: 141, steps per second: 157, episode reward: 30.944, mean reward:  0.219 [-100.000, 13.009], mean action: 1.766 [0.000, 3.000],  loss: 23.470543, mae: 67.899089, mean_q: 83.032974, mean_eps: 0.432832
  94703/150000: episode: 871, duration: 0.633s, episode steps: 104, steps per second: 164, episode reward:  2.636, mean reward:  0.025 [-100.000, 16.294], mean action: 1.654 [0.000, 3.000],  loss: 19.243321, mae: 69.328715, mean_q: 85.348619, mean_eps: 0.432097
  94858/150000: episode: 872, duration: 0.949s, episode steps: 155, steps per second: 163, episode reward: -4.246, mean reward: -0.027 [-100.000,  6.997], mean action: 1.716 [0.000, 3.000],  loss: 22.168070, mae: 68.432572, mean_q: 84.076215, mean_eps: 0.431320
  95255/150000: episode: 873, duration: 2.693s, episode steps: 397, steps per second: 147, episode reward: -87.189, mean reward: -0.220 [-100.000, 12.041], mean action: 1.776 [0.000, 3.000],  loss: 16.684638, mae: 68.131863, mean_q: 83.553740, mean_eps: 0.429664
  95402/150000: episode: 874, duration: 0.920s, episode steps: 147, steps per second: 160, episode reward: -11.818, mean reward: -0.080 [-100.000, 11.006], mean action: 1.694 [0.000, 3.000],  loss: 19.678921, mae: 68.320941, mean_q: 83.389447, mean_eps: 0.428032
  95650/150000: episode: 875, duration: 1.606s, episode steps: 248, steps per second: 154, episode reward: -119.309, mean reward: -0.481 [-100.000, 11.718], mean action: 1.419 [0.000, 3.000],  loss: 26.286974, mae: 67.526206, mean_q: 82.416761, mean_eps: 0.426847
  95799/150000: episode: 876, duration: 0.907s, episode steps: 149, steps per second: 164, episode reward: -16.774, mean reward: -0.113 [-100.000, 13.705], mean action: 1.832 [0.000, 3.000],  loss: 27.844150, mae: 67.984586, mean_q: 84.433267, mean_eps: 0.425656
  95905/150000: episode: 877, duration: 0.688s, episode steps: 106, steps per second: 154, episode reward:  2.828, mean reward:  0.027 [-100.000, 11.486], mean action: 1.792 [0.000, 3.000],  loss: 19.804367, mae: 68.663817, mean_q: 84.883839, mean_eps: 0.424891
  96036/150000: episode: 878, duration: 0.868s, episode steps: 131, steps per second: 151, episode reward: 17.139, mean reward:  0.131 [-100.000, 13.723], mean action: 1.649 [0.000, 3.000],  loss: 25.319836, mae: 67.581748, mean_q: 82.942302, mean_eps: 0.424180
  96190/150000: episode: 879, duration: 1.024s, episode steps: 154, steps per second: 150, episode reward: -42.765, mean reward: -0.278 [-100.000,  9.947], mean action: 1.649 [0.000, 3.000],  loss: 13.908633, mae: 68.118233, mean_q: 83.654157, mean_eps: 0.423325
  96293/150000: episode: 880, duration: 0.670s, episode steps: 103, steps per second: 154, episode reward: -167.372, mean reward: -1.625 [-100.000, 38.663], mean action: 1.738 [0.000, 3.000],  loss: 21.466666, mae: 69.082485, mean_q: 85.822002, mean_eps: 0.422554
  97293/150000: episode: 881, duration: 6.668s, episode steps: 1000, steps per second: 150, episode reward: 54.074, mean reward:  0.054 [-20.612, 23.311], mean action: 1.285 [0.000, 3.000],  loss: 24.083045, mae: 68.040244, mean_q: 83.092899, mean_eps: 0.419245
  97389/150000: episode: 882, duration: 0.595s, episode steps:  96, steps per second: 161, episode reward: -12.657, mean reward: -0.132 [-100.000, 10.596], mean action: 1.573 [0.000, 3.000],  loss: 18.260776, mae: 67.444599, mean_q: 82.085518, mean_eps: 0.415957
  97486/150000: episode: 883, duration: 0.590s, episode steps:  97, steps per second: 164, episode reward:  3.598, mean reward:  0.037 [-100.000, 16.859], mean action: 1.918 [0.000, 3.000],  loss: 25.128363, mae: 67.731654, mean_q: 83.305109, mean_eps: 0.415378
  97605/150000: episode: 884, duration: 0.782s, episode steps: 119, steps per second: 152, episode reward:  4.583, mean reward:  0.039 [-100.000, 19.997], mean action: 1.630 [0.000, 3.000],  loss: 16.809173, mae: 68.465447, mean_q: 84.514717, mean_eps: 0.414730
  98605/150000: episode: 885, duration: 7.108s, episode steps: 1000, steps per second: 141, episode reward: 28.045, mean reward:  0.028 [-24.253, 25.436], mean action: 1.616 [0.000, 3.000],  loss: 19.030362, mae: 67.553285, mean_q: 82.667898, mean_eps: 0.411373
  98739/150000: episode: 886, duration: 0.846s, episode steps: 134, steps per second: 158, episode reward: 13.120, mean reward:  0.098 [-100.000, 10.589], mean action: 1.813 [0.000, 3.000],  loss: 21.729252, mae: 66.481796, mean_q: 81.058744, mean_eps: 0.407971
  98850/150000: episode: 887, duration: 0.765s, episode steps: 111, steps per second: 145, episode reward: -17.947, mean reward: -0.162 [-100.000, 16.663], mean action: 1.802 [0.000, 3.000],  loss: 19.351785, mae: 66.864060, mean_q: 81.153620, mean_eps: 0.407236
  98965/150000: episode: 888, duration: 0.707s, episode steps: 115, steps per second: 163, episode reward: -91.644, mean reward: -0.797 [-100.000, 17.055], mean action: 1.635 [0.000, 3.000],  loss: 15.699988, mae: 67.559755, mean_q: 83.118738, mean_eps: 0.406558
  99126/150000: episode: 889, duration: 1.008s, episode steps: 161, steps per second: 160, episode reward: 11.123, mean reward:  0.069 [-100.000, 15.564], mean action: 1.646 [0.000, 3.000],  loss: 25.650230, mae: 66.695240, mean_q: 82.553317, mean_eps: 0.405730
 100126/150000: episode: 890, duration: 6.970s, episode steps: 1000, steps per second: 143, episode reward: 127.154, mean reward:  0.127 [-20.209, 32.981], mean action: 1.206 [0.000, 3.000],  loss: 21.002173, mae: 66.761472, mean_q: 81.851710, mean_eps: 0.402247
 100279/150000: episode: 891, duration: 0.945s, episode steps: 153, steps per second: 162, episode reward: -216.927, mean reward: -1.418 [-100.000, 11.351], mean action: 1.634 [0.000, 3.000],  loss: 21.102554, mae: 67.290859, mean_q: 83.534956, mean_eps: 0.398788
 101279/150000: episode: 892, duration: 6.815s, episode steps: 1000, steps per second: 147, episode reward: 86.637, mean reward:  0.087 [-24.942, 34.977], mean action: 1.491 [0.000, 3.000],  loss: 18.767375, mae: 67.417297, mean_q: 83.171059, mean_eps: 0.395329
 101392/150000: episode: 893, duration: 0.728s, episode steps: 113, steps per second: 155, episode reward:  5.415, mean reward:  0.048 [-100.000, 10.536], mean action: 1.726 [0.000, 3.000],  loss: 17.525074, mae: 67.653562, mean_q: 83.416139, mean_eps: 0.391990
 101483/150000: episode: 894, duration: 0.556s, episode steps:  91, steps per second: 164, episode reward: -41.554, mean reward: -0.457 [-100.000, 14.751], mean action: 1.703 [0.000, 3.000],  loss: 18.608039, mae: 67.564796, mean_q: 82.556713, mean_eps: 0.391378
 101563/150000: episode: 895, duration: 0.489s, episode steps:  80, steps per second: 164, episode reward: -6.871, mean reward: -0.086 [-100.000, 15.419], mean action: 1.988 [0.000, 3.000],  loss: 25.694708, mae: 67.202497, mean_q: 82.589363, mean_eps: 0.390865
 101655/150000: episode: 896, duration: 0.559s, episode steps:  92, steps per second: 165, episode reward: 16.730, mean reward:  0.182 [-100.000, 20.113], mean action: 1.761 [0.000, 3.000],  loss: 18.365952, mae: 68.340843, mean_q: 84.454418, mean_eps: 0.390349
 101802/150000: episode: 897, duration: 0.933s, episode steps: 147, steps per second: 157, episode reward: 40.029, mean reward:  0.272 [-100.000, 17.186], mean action: 1.680 [0.000, 3.000],  loss: 18.182270, mae: 67.276329, mean_q: 82.471599, mean_eps: 0.389632
 102802/150000: episode: 898, duration: 7.702s, episode steps: 1000, steps per second: 130, episode reward: 61.206, mean reward:  0.061 [-24.013, 22.698], mean action: 1.723 [0.000, 3.000],  loss: 22.602672, mae: 67.330292, mean_q: 83.146608, mean_eps: 0.386191
 102921/150000: episode: 899, duration: 0.742s, episode steps: 119, steps per second: 160, episode reward: -12.685, mean reward: -0.107 [-100.000, 14.053], mean action: 1.689 [0.000, 3.000],  loss: 20.226555, mae: 65.457935, mean_q: 80.513694, mean_eps: 0.382834
 103013/150000: episode: 900, duration: 0.580s, episode steps:  92, steps per second: 159, episode reward:  1.017, mean reward:  0.011 [-100.000,  8.806], mean action: 1.761 [0.000, 3.000],  loss: 21.398938, mae: 68.639608, mean_q: 85.132365, mean_eps: 0.382201
 104013/150000: episode: 901, duration: 7.063s, episode steps: 1000, steps per second: 142, episode reward: 59.035, mean reward:  0.059 [-22.954, 24.059], mean action: 1.682 [0.000, 3.000],  loss: 23.133108, mae: 67.736693, mean_q: 84.127504, mean_eps: 0.378925
 104116/150000: episode: 902, duration: 0.662s, episode steps: 103, steps per second: 156, episode reward: -19.336, mean reward: -0.188 [-100.000, 17.573], mean action: 1.767 [0.000, 3.000],  loss: 22.881789, mae: 68.768498, mean_q: 85.908548, mean_eps: 0.375616
 104804/150000: episode: 903, duration: 4.760s, episode steps: 688, steps per second: 145, episode reward: -273.885, mean reward: -0.398 [-100.000, 42.986], mean action: 1.722 [0.000, 3.000],  loss: 20.965837, mae: 67.713596, mean_q: 84.947666, mean_eps: 0.373243
 105173/150000: episode: 904, duration: 2.471s, episode steps: 369, steps per second: 149, episode reward:  1.574, mean reward:  0.004 [-100.000, 16.826], mean action: 1.745 [0.000, 3.000],  loss: 25.403762, mae: 67.753494, mean_q: 85.733836, mean_eps: 0.370072
 105284/150000: episode: 905, duration: 0.675s, episode steps: 111, steps per second: 164, episode reward: 15.587, mean reward:  0.140 [-100.000, 14.976], mean action: 1.730 [0.000, 3.000],  loss: 23.084399, mae: 66.609311, mean_q: 85.134217, mean_eps: 0.368632
 105841/150000: episode: 906, duration: 3.869s, episode steps: 557, steps per second: 144, episode reward: -196.388, mean reward: -0.353 [-100.000, 16.765], mean action: 1.429 [0.000, 3.000],  loss: 28.328472, mae: 68.089711, mean_q: 86.476124, mean_eps: 0.366628
 106841/150000: episode: 907, duration: 6.723s, episode steps: 1000, steps per second: 149, episode reward: 23.378, mean reward:  0.023 [-23.145, 22.517], mean action: 1.573 [0.000, 3.000],  loss: 24.170590, mae: 66.810777, mean_q: 84.924244, mean_eps: 0.361957
 106992/150000: episode: 908, duration: 0.949s, episode steps: 151, steps per second: 159, episode reward: -30.948, mean reward: -0.205 [-100.000, 20.788], mean action: 1.695 [0.000, 3.000],  loss: 33.461772, mae: 66.144183, mean_q: 83.508709, mean_eps: 0.358504
 107992/150000: episode: 909, duration: 7.611s, episode steps: 1000, steps per second: 131, episode reward: 62.407, mean reward:  0.062 [-21.129, 24.949], mean action: 1.695 [0.000, 3.000],  loss: 19.294546, mae: 65.921697, mean_q: 83.808311, mean_eps: 0.355051
 108115/150000: episode: 910, duration: 0.811s, episode steps: 123, steps per second: 152, episode reward: 59.011, mean reward:  0.480 [-100.000, 21.853], mean action: 1.780 [0.000, 3.000],  loss: 13.303083, mae: 66.447881, mean_q: 84.398541, mean_eps: 0.351682
 108680/150000: episode: 911, duration: 4.043s, episode steps: 565, steps per second: 140, episode reward: -169.311, mean reward: -0.300 [-100.000, 21.944], mean action: 1.704 [0.000, 3.000],  loss: 23.888335, mae: 65.968771, mean_q: 83.918271, mean_eps: 0.349618
 109582/150000: episode: 912, duration: 6.290s, episode steps: 902, steps per second: 143, episode reward: 225.930, mean reward:  0.250 [-20.784, 100.000], mean action: 2.140 [0.000, 3.000],  loss: 26.636554, mae: 66.054824, mean_q: 84.080395, mean_eps: 0.345217
 109724/150000: episode: 913, duration: 0.910s, episode steps: 142, steps per second: 156, episode reward: 38.718, mean reward:  0.273 [-100.000, 17.163], mean action: 1.592 [0.000, 3.000],  loss: 29.240464, mae: 65.338528, mean_q: 83.509468, mean_eps: 0.342085
 110724/150000: episode: 914, duration: 6.798s, episode steps: 1000, steps per second: 147, episode reward:  7.256, mean reward:  0.007 [-21.358, 24.310], mean action: 1.359 [0.000, 3.000],  loss: 24.228468, mae: 65.463328, mean_q: 83.562951, mean_eps: 0.338659
 111724/150000: episode: 915, duration: 6.963s, episode steps: 1000, steps per second: 144, episode reward: 32.057, mean reward:  0.032 [-22.121, 25.291], mean action: 1.331 [0.000, 3.000],  loss: 19.901347, mae: 65.144186, mean_q: 83.576636, mean_eps: 0.332659
 112724/150000: episode: 916, duration: 6.962s, episode steps: 1000, steps per second: 144, episode reward: 159.976, mean reward:  0.160 [-24.296, 25.229], mean action: 1.233 [0.000, 3.000],  loss: 21.283505, mae: 65.461275, mean_q: 84.047627, mean_eps: 0.326659
 112840/150000: episode: 917, duration: 0.727s, episode steps: 116, steps per second: 160, episode reward: 15.095, mean reward:  0.130 [-100.000, 18.241], mean action: 1.664 [0.000, 3.000],  loss: 19.219767, mae: 64.855681, mean_q: 84.213870, mean_eps: 0.323311
 113840/150000: episode: 918, duration: 6.906s, episode steps: 1000, steps per second: 145, episode reward: 110.159, mean reward:  0.110 [-22.538, 24.463], mean action: 1.541 [0.000, 3.000],  loss: 19.461201, mae: 64.195238, mean_q: 83.095243, mean_eps: 0.319963
 114393/150000: episode: 919, duration: 3.720s, episode steps: 553, steps per second: 149, episode reward: 183.113, mean reward:  0.331 [-19.550, 100.000], mean action: 1.673 [0.000, 3.000],  loss: 19.229440, mae: 64.538242, mean_q: 83.284700, mean_eps: 0.315304
 115393/150000: episode: 920, duration: 6.959s, episode steps: 1000, steps per second: 144, episode reward: 110.700, mean reward:  0.111 [-24.254, 24.355], mean action: 1.378 [0.000, 3.000],  loss: 20.737454, mae: 64.276659, mean_q: 83.109818, mean_eps: 0.310645
 116393/150000: episode: 921, duration: 6.934s, episode steps: 1000, steps per second: 144, episode reward: 116.745, mean reward:  0.117 [-21.889, 19.581], mean action: 1.273 [0.000, 3.000],  loss: 17.886500, mae: 62.734486, mean_q: 81.053347, mean_eps: 0.304645
 116515/150000: episode: 922, duration: 0.773s, episode steps: 122, steps per second: 158, episode reward: 37.505, mean reward:  0.307 [-100.000, 16.335], mean action: 1.574 [0.000, 3.000],  loss: 21.722009, mae: 62.094917, mean_q: 80.218176, mean_eps: 0.301279
 117515/150000: episode: 923, duration: 7.175s, episode steps: 1000, steps per second: 139, episode reward: 103.056, mean reward:  0.103 [-21.975, 26.073], mean action: 1.262 [0.000, 3.000],  loss: 18.866971, mae: 61.709416, mean_q: 79.993791, mean_eps: 0.297913
 118515/150000: episode: 924, duration: 7.614s, episode steps: 1000, steps per second: 131, episode reward: 106.722, mean reward:  0.107 [-20.170, 23.561], mean action: 1.322 [0.000, 3.000],  loss: 17.625334, mae: 61.460199, mean_q: 79.616184, mean_eps: 0.291913
 119515/150000: episode: 925, duration: 7.124s, episode steps: 1000, steps per second: 140, episode reward: 99.227, mean reward:  0.099 [-22.926, 23.357], mean action: 1.262 [0.000, 3.000],  loss: 17.349227, mae: 61.406858, mean_q: 79.580974, mean_eps: 0.285913
 119627/150000: episode: 926, duration: 0.715s, episode steps: 112, steps per second: 157, episode reward: 45.022, mean reward:  0.402 [-100.000, 17.649], mean action: 1.875 [0.000, 3.000],  loss: 23.478813, mae: 60.455599, mean_q: 78.341252, mean_eps: 0.282577
 120627/150000: episode: 927, duration: 7.105s, episode steps: 1000, steps per second: 141, episode reward: 135.303, mean reward:  0.135 [-22.572, 23.071], mean action: 1.354 [0.000, 3.000],  loss: 16.325302, mae: 61.472907, mean_q: 79.996842, mean_eps: 0.279241
 120943/150000: episode: 928, duration: 2.524s, episode steps: 316, steps per second: 125, episode reward: 229.564, mean reward:  0.726 [-19.451, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 15.819909, mae: 60.846379, mean_q: 79.421652, mean_eps: 0.275293
 121943/150000: episode: 929, duration: 7.779s, episode steps: 1000, steps per second: 129, episode reward: 108.564, mean reward:  0.109 [-19.735, 24.720], mean action: 0.831 [0.000, 3.000],  loss: 16.899380, mae: 60.542744, mean_q: 78.997700, mean_eps: 0.271345
 122067/150000: episode: 930, duration: 0.819s, episode steps: 124, steps per second: 151, episode reward: -11.502, mean reward: -0.093 [-100.000, 11.872], mean action: 1.887 [0.000, 3.000],  loss: 22.600901, mae: 60.247662, mean_q: 79.304102, mean_eps: 0.267973
 122666/150000: episode: 931, duration: 4.236s, episode steps: 599, steps per second: 141, episode reward: 208.856, mean reward:  0.349 [-20.885, 100.000], mean action: 1.252 [0.000, 3.000],  loss: 16.413159, mae: 59.935057, mean_q: 78.497795, mean_eps: 0.265804
 123439/150000: episode: 932, duration: 5.165s, episode steps: 773, steps per second: 150, episode reward: 263.015, mean reward:  0.340 [-21.451, 100.000], mean action: 1.572 [0.000, 3.000],  loss: 17.106102, mae: 59.454285, mean_q: 78.135212, mean_eps: 0.261688
 124439/150000: episode: 933, duration: 6.947s, episode steps: 1000, steps per second: 144, episode reward: 136.388, mean reward:  0.136 [-20.019, 22.246], mean action: 0.967 [0.000, 3.000],  loss: 17.892910, mae: 59.338935, mean_q: 78.248324, mean_eps: 0.256369
 124583/150000: episode: 934, duration: 0.880s, episode steps: 144, steps per second: 164, episode reward: -185.895, mean reward: -1.291 [-100.000, 50.165], mean action: 1.681 [0.000, 3.000],  loss: 15.233704, mae: 59.148367, mean_q: 77.799670, mean_eps: 0.252937
 124731/150000: episode: 935, duration: 0.923s, episode steps: 148, steps per second: 160, episode reward: -18.838, mean reward: -0.127 [-100.000, 19.444], mean action: 1.865 [0.000, 3.000],  loss: 17.749708, mae: 58.499625, mean_q: 76.859775, mean_eps: 0.252061
 125731/150000: episode: 936, duration: 6.889s, episode steps: 1000, steps per second: 145, episode reward: 120.354, mean reward:  0.120 [-21.118, 23.146], mean action: 1.225 [0.000, 3.000],  loss: 17.485943, mae: 59.149206, mean_q: 78.236241, mean_eps: 0.248617
 126546/150000: episode: 937, duration: 5.242s, episode steps: 815, steps per second: 155, episode reward: 237.231, mean reward:  0.291 [-21.499, 100.000], mean action: 1.107 [0.000, 3.000],  loss: 17.976722, mae: 59.344026, mean_q: 78.359870, mean_eps: 0.243172
 127546/150000: episode: 938, duration: 7.125s, episode steps: 1000, steps per second: 140, episode reward: 105.920, mean reward:  0.106 [-21.051, 23.141], mean action: 1.146 [0.000, 3.000],  loss: 15.016980, mae: 58.857821, mean_q: 78.172962, mean_eps: 0.237727
 128546/150000: episode: 939, duration: 6.919s, episode steps: 1000, steps per second: 145, episode reward: 67.874, mean reward:  0.068 [-19.854, 24.189], mean action: 1.667 [0.000, 3.000],  loss: 15.279686, mae: 58.321839, mean_q: 77.534601, mean_eps: 0.231727
 129546/150000: episode: 940, duration: 6.758s, episode steps: 1000, steps per second: 148, episode reward: 160.035, mean reward:  0.160 [-23.138, 25.437], mean action: 1.038 [0.000, 3.000],  loss: 15.250193, mae: 58.116285, mean_q: 77.336157, mean_eps: 0.225727
 130102/150000: episode: 941, duration: 3.671s, episode steps: 556, steps per second: 151, episode reward: 236.468, mean reward:  0.425 [-20.916, 100.000], mean action: 1.182 [0.000, 3.000],  loss: 15.636130, mae: 57.152347, mean_q: 76.136100, mean_eps: 0.221059
 131102/150000: episode: 942, duration: 7.301s, episode steps: 1000, steps per second: 137, episode reward: 68.188, mean reward:  0.068 [-23.954, 26.977], mean action: 1.172 [0.000, 3.000],  loss: 13.507592, mae: 57.117252, mean_q: 76.249631, mean_eps: 0.216391
 132102/150000: episode: 943, duration: 6.830s, episode steps: 1000, steps per second: 146, episode reward: 171.723, mean reward:  0.172 [-20.013, 22.686], mean action: 1.134 [0.000, 3.000],  loss: 15.563532, mae: 56.368742, mean_q: 75.298835, mean_eps: 0.210391
 132216/150000: episode: 944, duration: 0.700s, episode steps: 114, steps per second: 163, episode reward: 14.077, mean reward:  0.123 [-100.000,  8.875], mean action: 1.860 [0.000, 3.000],  loss: 16.441102, mae: 56.542193, mean_q: 75.261290, mean_eps: 0.207049
 133216/150000: episode: 945, duration: 6.969s, episode steps: 1000, steps per second: 143, episode reward: 126.715, mean reward:  0.127 [-19.668, 22.282], mean action: 1.050 [0.000, 3.000],  loss: 12.299514, mae: 56.662704, mean_q: 75.637908, mean_eps: 0.203707
 134216/150000: episode: 946, duration: 6.392s, episode steps: 1000, steps per second: 156, episode reward: 182.109, mean reward:  0.182 [-19.186, 23.183], mean action: 0.704 [0.000, 3.000],  loss: 13.194292, mae: 55.572885, mean_q: 74.250970, mean_eps: 0.197707
 134304/150000: episode: 947, duration: 0.575s, episode steps:  88, steps per second: 153, episode reward: -615.875, mean reward: -6.999 [-100.000,  1.945], mean action: 1.045 [0.000, 3.000],  loss: 12.118147, mae: 56.136296, mean_q: 74.680143, mean_eps: 0.194443
 135304/150000: episode: 948, duration: 6.857s, episode steps: 1000, steps per second: 146, episode reward: 146.754, mean reward:  0.147 [-23.765, 23.162], mean action: 1.090 [0.000, 3.000],  loss: 14.073492, mae: 55.738073, mean_q: 74.277852, mean_eps: 0.191179
 135981/150000: episode: 949, duration: 4.581s, episode steps: 677, steps per second: 148, episode reward: 246.958, mean reward:  0.365 [-21.322, 100.000], mean action: 0.724 [0.000, 3.000],  loss: 12.764105, mae: 54.949002, mean_q: 73.436991, mean_eps: 0.186148
 136295/150000: episode: 950, duration: 2.178s, episode steps: 314, steps per second: 144, episode reward: 296.153, mean reward:  0.943 [-18.213, 100.000], mean action: 1.350 [0.000, 3.000],  loss: 13.409510, mae: 54.024030, mean_q: 71.898223, mean_eps: 0.183175
 136484/150000: episode: 951, duration: 1.278s, episode steps: 189, steps per second: 148, episode reward: -287.666, mean reward: -1.522 [-100.000,  2.491], mean action: 2.016 [0.000, 3.000],  loss: 11.804484, mae: 53.976089, mean_q: 72.083569, mean_eps: 0.181666
 136638/150000: episode: 952, duration: 1.013s, episode steps: 154, steps per second: 152, episode reward: -244.892, mean reward: -1.590 [-100.000,  4.288], mean action: 1.630 [0.000, 3.000],  loss: 8.378812, mae: 53.999049, mean_q: 72.073265, mean_eps: 0.180637
 136797/150000: episode: 953, duration: 1.066s, episode steps: 159, steps per second: 149, episode reward: 229.469, mean reward:  1.443 [-4.587, 100.000], mean action: 1.560 [0.000, 3.000],  loss: 13.541984, mae: 54.514680, mean_q: 72.373792, mean_eps: 0.179698
 137135/150000: episode: 954, duration: 2.204s, episode steps: 338, steps per second: 153, episode reward: 250.440, mean reward:  0.741 [-19.457, 100.000], mean action: 1.139 [0.000, 3.000],  loss: 12.091490, mae: 54.182355, mean_q: 72.251328, mean_eps: 0.178207
 137371/150000: episode: 955, duration: 1.468s, episode steps: 236, steps per second: 161, episode reward: -388.785, mean reward: -1.647 [-100.000, 17.299], mean action: 1.754 [0.000, 3.000],  loss: 13.033943, mae: 55.394189, mean_q: 74.250545, mean_eps: 0.176485
 138371/150000: episode: 956, duration: 6.712s, episode steps: 1000, steps per second: 149, episode reward: 69.865, mean reward:  0.070 [-23.649, 26.570], mean action: 1.118 [0.000, 3.000],  loss: 13.728689, mae: 53.594048, mean_q: 71.391428, mean_eps: 0.172777
 138841/150000: episode: 957, duration: 3.108s, episode steps: 470, steps per second: 151, episode reward: 255.959, mean reward:  0.545 [-21.363, 100.000], mean action: 1.255 [0.000, 3.000],  loss: 10.402379, mae: 53.316566, mean_q: 71.319643, mean_eps: 0.168367
 139158/150000: episode: 958, duration: 2.033s, episode steps: 317, steps per second: 156, episode reward: 284.765, mean reward:  0.898 [-21.964, 100.000], mean action: 1.464 [0.000, 3.000],  loss: 10.425224, mae: 53.213594, mean_q: 71.195472, mean_eps: 0.166006
 139622/150000: episode: 959, duration: 3.044s, episode steps: 464, steps per second: 152, episode reward: 234.570, mean reward:  0.506 [-20.184, 100.000], mean action: 1.375 [0.000, 3.000],  loss: 10.295500, mae: 52.209654, mean_q: 69.847178, mean_eps: 0.163663
 140622/150000: episode: 960, duration: 7.509s, episode steps: 1000, steps per second: 133, episode reward: 64.000, mean reward:  0.064 [-22.750, 24.707], mean action: 0.961 [0.000, 3.000],  loss: 10.173944, mae: 52.320884, mean_q: 70.118620, mean_eps: 0.159271
 140846/150000: episode: 961, duration: 1.412s, episode steps: 224, steps per second: 159, episode reward: -208.305, mean reward: -0.930 [-100.000, 11.907], mean action: 1.942 [0.000, 3.000],  loss: 14.506667, mae: 51.784365, mean_q: 69.316923, mean_eps: 0.155599
 141295/150000: episode: 962, duration: 2.951s, episode steps: 449, steps per second: 152, episode reward: 235.636, mean reward:  0.525 [-18.585, 100.000], mean action: 0.686 [0.000, 3.000],  loss: 9.542745, mae: 52.212121, mean_q: 69.886424, mean_eps: 0.153580
 141850/150000: episode: 963, duration: 3.688s, episode steps: 555, steps per second: 151, episode reward: 199.147, mean reward:  0.359 [-22.153, 100.000], mean action: 1.202 [0.000, 3.000],  loss: 11.544825, mae: 51.370553, mean_q: 68.704833, mean_eps: 0.150568
 142672/150000: episode: 964, duration: 5.245s, episode steps: 822, steps per second: 157, episode reward: 226.909, mean reward:  0.276 [-23.828, 100.000], mean action: 0.634 [0.000, 3.000],  loss: 10.322543, mae: 50.339314, mean_q: 67.436075, mean_eps: 0.146437
 142911/150000: episode: 965, duration: 1.500s, episode steps: 239, steps per second: 159, episode reward: -212.767, mean reward: -0.890 [-100.000, 11.627], mean action: 1.640 [0.000, 3.000],  loss: 8.912417, mae: 51.072345, mean_q: 68.231836, mean_eps: 0.143254
 143185/150000: episode: 966, duration: 1.693s, episode steps: 274, steps per second: 162, episode reward: -167.308, mean reward: -0.611 [-100.000, 14.048], mean action: 1.697 [0.000, 3.000],  loss: 11.494244, mae: 49.931255, mean_q: 66.839818, mean_eps: 0.141715
 143434/150000: episode: 967, duration: 1.589s, episode steps: 249, steps per second: 157, episode reward: -29.163, mean reward: -0.117 [-100.000, 12.097], mean action: 1.763 [0.000, 3.000],  loss: 10.021040, mae: 50.544619, mean_q: 67.706619, mean_eps: 0.140146
 144048/150000: episode: 968, duration: 4.128s, episode steps: 614, steps per second: 149, episode reward: 250.386, mean reward:  0.408 [-20.854, 100.000], mean action: 1.049 [0.000, 3.000],  loss: 15.576751, mae: 50.935852, mean_q: 68.200356, mean_eps: 0.137557
 144248/150000: episode: 969, duration: 1.278s, episode steps: 200, steps per second: 156, episode reward: 271.307, mean reward:  1.357 [-2.653, 100.000], mean action: 1.355 [0.000, 3.000],  loss: 11.377352, mae: 51.615057, mean_q: 69.107208, mean_eps: 0.135115
 144635/150000: episode: 970, duration: 2.520s, episode steps: 387, steps per second: 154, episode reward: 273.028, mean reward:  0.705 [-18.690, 100.000], mean action: 1.173 [0.000, 3.000],  loss: 11.984984, mae: 50.986827, mean_q: 68.435198, mean_eps: 0.133354
 144858/150000: episode: 971, duration: 1.396s, episode steps: 223, steps per second: 160, episode reward: 300.747, mean reward:  1.349 [-7.706, 100.000], mean action: 1.300 [0.000, 3.000],  loss: 11.727996, mae: 51.235405, mean_q: 68.855719, mean_eps: 0.131524
 145093/150000: episode: 972, duration: 1.504s, episode steps: 235, steps per second: 156, episode reward: -44.104, mean reward: -0.188 [-100.000, 11.704], mean action: 1.660 [0.000, 3.000],  loss: 9.889115, mae: 51.911218, mean_q: 69.845967, mean_eps: 0.130150
 145332/150000: episode: 973, duration: 1.567s, episode steps: 239, steps per second: 153, episode reward: 230.183, mean reward:  0.963 [-9.282, 100.000], mean action: 1.335 [0.000, 3.000],  loss: 8.430526, mae: 51.156237, mean_q: 68.718639, mean_eps: 0.128728
 145602/150000: episode: 974, duration: 1.704s, episode steps: 270, steps per second: 158, episode reward: 250.288, mean reward:  0.927 [-20.868, 100.000], mean action: 1.341 [0.000, 3.000],  loss: 11.226813, mae: 51.678371, mean_q: 69.158888, mean_eps: 0.127201
 146028/150000: episode: 975, duration: 2.796s, episode steps: 426, steps per second: 152, episode reward: 237.609, mean reward:  0.558 [-20.099, 100.000], mean action: 1.718 [0.000, 3.000],  loss: 11.871584, mae: 51.831055, mean_q: 69.509881, mean_eps: 0.125113
 146389/150000: episode: 976, duration: 2.491s, episode steps: 361, steps per second: 145, episode reward: -147.768, mean reward: -0.409 [-100.000, 17.369], mean action: 1.499 [0.000, 3.000],  loss: 14.266697, mae: 50.656561, mean_q: 67.966936, mean_eps: 0.122752
 146720/150000: episode: 977, duration: 2.295s, episode steps: 331, steps per second: 144, episode reward: 277.995, mean reward:  0.840 [-9.440, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 14.489650, mae: 50.310955, mean_q: 67.438955, mean_eps: 0.120676
 147313/150000: episode: 978, duration: 4.401s, episode steps: 593, steps per second: 135, episode reward: 248.611, mean reward:  0.419 [-21.962, 100.000], mean action: 1.236 [0.000, 3.000],  loss: 10.776638, mae: 50.620402, mean_q: 67.899717, mean_eps: 0.117904
 147577/150000: episode: 979, duration: 1.676s, episode steps: 264, steps per second: 157, episode reward:  6.544, mean reward:  0.025 [-100.000, 31.781], mean action: 1.784 [0.000, 3.000],  loss: 13.601453, mae: 50.630512, mean_q: 67.695001, mean_eps: 0.115333
 147912/150000: episode: 980, duration: 2.155s, episode steps: 335, steps per second: 155, episode reward: 276.770, mean reward:  0.826 [-7.790, 100.000], mean action: 1.218 [0.000, 3.000],  loss: 10.796079, mae: 51.113036, mean_q: 68.432525, mean_eps: 0.113536
 148063/150000: episode: 981, duration: 0.922s, episode steps: 151, steps per second: 164, episode reward: -36.693, mean reward: -0.243 [-100.000, 18.321], mean action: 1.550 [0.000, 3.000],  loss: 9.494817, mae: 50.182882, mean_q: 67.448244, mean_eps: 0.112078
 148574/150000: episode: 982, duration: 3.301s, episode steps: 511, steps per second: 155, episode reward: 234.200, mean reward:  0.458 [-19.486, 100.000], mean action: 0.822 [0.000, 3.000],  loss: 7.773913, mae: 50.488434, mean_q: 67.472933, mean_eps: 0.110092
 149058/150000: episode: 983, duration: 3.101s, episode steps: 484, steps per second: 156, episode reward: 224.391, mean reward:  0.464 [-19.418, 100.000], mean action: 1.076 [0.000, 3.000],  loss: 10.614934, mae: 50.375003, mean_q: 67.430913, mean_eps: 0.107107
 149255/150000: episode: 984, duration: 1.262s, episode steps: 197, steps per second: 156, episode reward: -338.076, mean reward: -1.716 [-100.000,  4.983], mean action: 1.716 [0.000, 3.000],  loss: 8.737944, mae: 49.665921, mean_q: 66.695302, mean_eps: 0.105064
 149571/150000: episode: 985, duration: 2.047s, episode steps: 316, steps per second: 154, episode reward: 259.759, mean reward:  0.822 [-22.003, 100.000], mean action: 1.171 [0.000, 3.000],  loss: 6.666687, mae: 50.558948, mean_q: 67.811720, mean_eps: 0.103525
done, took 993.269 seconds
Testing for 5 episodes ...
Episode 1: reward: 227.872, steps: 544
Episode 2: reward: 219.352, steps: 316
Episode 3: reward: -734.850, steps: 506
Episode 4: reward: 131.152, steps: 555
Episode 5: reward: -328.932, steps: 283