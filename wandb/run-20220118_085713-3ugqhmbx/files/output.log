Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 300000 steps ...
     70/300000: episode: 1, duration: 1.011s, episode steps:  70, steps per second:  69, episode reward: -98.879, mean reward: -1.413 [-100.000,  8.360], mean action: 1.429 [0.000, 3.000],  loss: 0.453683, mse: 0.540337, mean_q: 0.400880, mean_eps: 0.999880
    197/300000: episode: 2, duration: 0.832s, episode steps: 127, steps per second: 153, episode reward: -292.801, mean reward: -2.306 [-100.000,  3.682], mean action: 1.677 [0.000, 3.000],  loss: 35.022906, mse: 23.481182, mean_q: 2.098058, mean_eps: 0.999601
    312/300000: episode: 3, duration: 0.728s, episode steps: 115, steps per second: 158, episode reward: -175.167, mean reward: -1.523 [-100.000,  2.377], mean action: 1.435 [0.000, 3.000],  loss: 20.953841, mse: 20.927670, mean_q: 2.346026, mean_eps: 0.999238
    405/300000: episode: 4, duration: 0.569s, episode steps:  93, steps per second: 163, episode reward: -337.609, mean reward: -3.630 [-100.000,  0.361], mean action: 1.495 [0.000, 3.000],  loss: 31.336606, mse: 24.869293, mean_q: 3.598149, mean_eps: 0.998926
    468/300000: episode: 5, duration: 0.404s, episode steps:  63, steps per second: 156, episode reward: -112.288, mean reward: -1.782 [-100.000, 11.246], mean action: 1.714 [0.000, 3.000],  loss: 35.561572, mse: 31.472327, mean_q: 3.913945, mean_eps: 0.998692
    589/300000: episode: 6, duration: 0.728s, episode steps: 121, steps per second: 166, episode reward: -136.322, mean reward: -1.127 [-100.000, 35.813], mean action: 1.471 [0.000, 3.000],  loss: 35.306312, mse: 35.578727, mean_q: 4.819124, mean_eps: 0.998416
    647/300000: episode: 7, duration: 0.359s, episode steps:  58, steps per second: 162, episode reward: -96.230, mean reward: -1.659 [-100.000,  5.726], mean action: 1.517 [0.000, 3.000],  loss: 40.739148, mse: 51.752484, mean_q: 7.136491, mean_eps: 0.998147
    727/300000: episode: 8, duration: 0.515s, episode steps:  80, steps per second: 155, episode reward: -127.935, mean reward: -1.599 [-100.000, 34.777], mean action: 1.650 [0.000, 3.000],  loss: 63.261416, mse: 74.665827, mean_q: 8.221072, mean_eps: 0.997941
    811/300000: episode: 9, duration: 0.529s, episode steps:  84, steps per second: 159, episode reward: -44.178, mean reward: -0.526 [-100.000, 88.580], mean action: 1.417 [0.000, 3.000],  loss: 31.254050, mse: 89.316041, mean_q: 10.639887, mean_eps: 0.997694
    893/300000: episode: 10, duration: 0.497s, episode steps:  82, steps per second: 165, episode reward: -243.302, mean reward: -2.967 [-100.000, 31.624], mean action: 1.683 [0.000, 3.000],  loss: 74.199497, mse: 149.369860, mean_q: 12.244961, mean_eps: 0.997445
    951/300000: episode: 11, duration: 0.367s, episode steps:  58, steps per second: 158, episode reward: -90.639, mean reward: -1.563 [-100.000,  6.178], mean action: 1.379 [0.000, 3.000],  loss: 51.174420, mse: 191.202092, mean_q: 13.614862, mean_eps: 0.997235
   1080/300000: episode: 12, duration: 0.831s, episode steps: 129, steps per second: 155, episode reward: -39.410, mean reward: -0.306 [-100.000, 104.844], mean action: 1.550 [0.000, 3.000],  loss: 54.168459, mse: 240.888622, mean_q: 14.262823, mean_eps: 0.996955
   1176/300000: episode: 13, duration: 0.586s, episode steps:  96, steps per second: 164, episode reward: -363.572, mean reward: -3.787 [-100.000,  1.693], mean action: 1.271 [0.000, 3.000],  loss: 43.720541, mse: 300.762032, mean_q: 14.819517, mean_eps: 0.996617
   1261/300000: episode: 14, duration: 0.514s, episode steps:  85, steps per second: 165, episode reward: -327.004, mean reward: -3.847 [-100.000,  0.530], mean action: 1.400 [0.000, 3.000],  loss: 39.174614, mse: 336.948947, mean_q: 16.423067, mean_eps: 0.996346
   1341/300000: episode: 15, duration: 0.508s, episode steps:  80, steps per second: 157, episode reward: -109.651, mean reward: -1.371 [-100.000, 11.218], mean action: 1.525 [0.000, 3.000],  loss: 58.004473, mse: 386.387297, mean_q: 17.927156, mean_eps: 0.996099
   1426/300000: episode: 16, duration: 0.533s, episode steps:  85, steps per second: 160, episode reward: -97.274, mean reward: -1.144 [-100.000, 11.878], mean action: 1.765 [0.000, 3.000],  loss: 54.156537, mse: 418.748550, mean_q: 18.433633, mean_eps: 0.995851
   1501/300000: episode: 17, duration: 0.503s, episode steps:  75, steps per second: 149, episode reward: -182.349, mean reward: -2.431 [-100.000, 28.729], mean action: 1.600 [0.000, 3.000],  loss: 41.975292, mse: 438.397978, mean_q: 17.591543, mean_eps: 0.995611
   1591/300000: episode: 18, duration: 0.655s, episode steps:  90, steps per second: 137, episode reward: -81.588, mean reward: -0.907 [-100.000,  8.400], mean action: 1.478 [0.000, 3.000],  loss: 34.142080, mse: 455.459456, mean_q: 19.049575, mean_eps: 0.995364
   1669/300000: episode: 19, duration: 0.725s, episode steps:  78, steps per second: 108, episode reward: -136.897, mean reward: -1.755 [-100.000,  7.551], mean action: 1.538 [0.000, 3.000],  loss: 28.199640, mse: 477.691104, mean_q: 19.187355, mean_eps: 0.995112
   1779/300000: episode: 20, duration: 0.841s, episode steps: 110, steps per second: 131, episode reward: -256.112, mean reward: -2.328 [-100.000, 52.759], mean action: 1.555 [0.000, 3.000],  loss: 27.863162, mse: 504.897662, mean_q: 20.027916, mean_eps: 0.994829
   1885/300000: episode: 21, duration: 0.884s, episode steps: 106, steps per second: 120, episode reward: -116.961, mean reward: -1.103 [-100.000,  6.631], mean action: 1.689 [0.000, 3.000],  loss: 26.487722, mse: 549.101550, mean_q: 19.719580, mean_eps: 0.994506
   1961/300000: episode: 22, duration: 0.703s, episode steps:  76, steps per second: 108, episode reward: -217.403, mean reward: -2.861 [-100.000,  5.674], mean action: 1.368 [0.000, 3.000],  loss: 23.610871, mse: 587.601783, mean_q: 20.545899, mean_eps: 0.994232
   2056/300000: episode: 23, duration: 0.793s, episode steps:  95, steps per second: 120, episode reward: -126.732, mean reward: -1.334 [-100.000,  6.115], mean action: 1.547 [0.000, 3.000],  loss: 24.792825, mse: 632.037445, mean_q: 21.013116, mean_eps: 0.993976
   2150/300000: episode: 24, duration: 0.709s, episode steps:  94, steps per second: 132, episode reward: -102.915, mean reward: -1.095 [-100.000, 34.558], mean action: 1.479 [0.000, 3.000],  loss: 28.495283, mse: 695.657901, mean_q: 20.917772, mean_eps: 0.993692
   2264/300000: episode: 25, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: -234.040, mean reward: -2.053 [-100.000,  5.155], mean action: 1.386 [0.000, 3.000],  loss: 24.351955, mse: 738.779317, mean_q: 22.115020, mean_eps: 0.993380
   2327/300000: episode: 26, duration: 0.410s, episode steps:  63, steps per second: 154, episode reward: -208.279, mean reward: -3.306 [-100.000,  8.769], mean action: 1.603 [0.000, 3.000],  loss: 23.781205, mse: 761.290159, mean_q: 22.665189, mean_eps: 0.993115
   2446/300000: episode: 27, duration: 0.770s, episode steps: 119, steps per second: 155, episode reward: -129.703, mean reward: -1.090 [-100.000, 12.405], mean action: 1.647 [0.000, 3.000],  loss: 18.206982, mse: 823.498822, mean_q: 23.441443, mean_eps: 0.992842
   2519/300000: episode: 28, duration: 0.481s, episode steps:  73, steps per second: 152, episode reward: -81.940, mean reward: -1.122 [-100.000, 16.677], mean action: 1.575 [0.000, 3.000],  loss: 26.633023, mse: 891.268791, mean_q: 24.591013, mean_eps: 0.992554
   2606/300000: episode: 29, duration: 0.538s, episode steps:  87, steps per second: 162, episode reward: -126.618, mean reward: -1.455 [-100.000, 12.990], mean action: 1.333 [0.000, 3.000],  loss: 19.345039, mse: 972.407860, mean_q: 26.136962, mean_eps: 0.992314
   2714/300000: episode: 30, duration: 0.693s, episode steps: 108, steps per second: 156, episode reward: -401.736, mean reward: -3.720 [-100.000, 40.527], mean action: 1.380 [0.000, 3.000],  loss: 16.162711, mse: 995.802527, mean_q: 26.814481, mean_eps: 0.992022
   2809/300000: episode: 31, duration: 0.593s, episode steps:  95, steps per second: 160, episode reward: -140.505, mean reward: -1.479 [-100.000, 14.517], mean action: 1.263 [0.000, 3.000],  loss: 17.063773, mse: 1002.914701, mean_q: 26.573355, mean_eps: 0.991717
   2927/300000: episode: 32, duration: 0.741s, episode steps: 118, steps per second: 159, episode reward: -207.506, mean reward: -1.759 [-100.000, 36.119], mean action: 1.542 [0.000, 3.000],  loss: 18.266212, mse: 1091.963403, mean_q: 26.599677, mean_eps: 0.991398
   2996/300000: episode: 33, duration: 0.398s, episode steps:  69, steps per second: 173, episode reward: -143.161, mean reward: -2.075 [-100.000,  6.711], mean action: 1.391 [0.000, 3.000],  loss: 21.171193, mse: 1089.756050, mean_q: 28.045669, mean_eps: 0.991117
   3113/300000: episode: 34, duration: 0.712s, episode steps: 117, steps per second: 164, episode reward: -130.329, mean reward: -1.114 [-100.000,  7.194], mean action: 1.658 [0.000, 3.000],  loss: 24.505424, mse: 1190.240906, mean_q: 27.796491, mean_eps: 0.990838
   3208/300000: episode: 35, duration: 0.606s, episode steps:  95, steps per second: 157, episode reward: -231.092, mean reward: -2.433 [-100.000, 23.648], mean action: 1.505 [0.000, 3.000],  loss: 26.380862, mse: 1241.659048, mean_q: 29.499929, mean_eps: 0.990520
   3308/300000: episode: 36, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -223.007, mean reward: -2.230 [-100.000,  6.117], mean action: 1.570 [0.000, 3.000],  loss: 20.024339, mse: 1352.440726, mean_q: 29.010538, mean_eps: 0.990227
   3406/300000: episode: 37, duration: 0.618s, episode steps:  98, steps per second: 158, episode reward: -119.234, mean reward: -1.217 [-100.000,  7.219], mean action: 1.551 [0.000, 3.000],  loss: 15.451884, mse: 1378.959490, mean_q: 29.833259, mean_eps: 0.989930
   3522/300000: episode: 38, duration: 0.706s, episode steps: 116, steps per second: 164, episode reward: -183.644, mean reward: -1.583 [-100.000,  3.993], mean action: 1.491 [0.000, 3.000],  loss: 21.805039, mse: 1531.294344, mean_q: 30.521642, mean_eps: 0.989609
   3602/300000: episode: 39, duration: 0.474s, episode steps:  80, steps per second: 169, episode reward: -106.887, mean reward: -1.336 [-100.000, 16.655], mean action: 1.462 [0.000, 3.000],  loss: 22.812590, mse: 1623.388696, mean_q: 32.530371, mean_eps: 0.989316
   3670/300000: episode: 40, duration: 0.394s, episode steps:  68, steps per second: 173, episode reward: -260.995, mean reward: -3.838 [-100.000,  5.397], mean action: 1.662 [0.000, 3.000],  loss: 22.318499, mse: 1701.283304, mean_q: 34.034693, mean_eps: 0.989094
   3781/300000: episode: 41, duration: 0.701s, episode steps: 111, steps per second: 158, episode reward: -77.794, mean reward: -0.701 [-100.000,  9.130], mean action: 1.432 [0.000, 3.000],  loss: 16.919482, mse: 1755.428219, mean_q: 33.336215, mean_eps: 0.988825
   3869/300000: episode: 42, duration: 0.552s, episode steps:  88, steps per second: 159, episode reward: -127.525, mean reward: -1.449 [-100.000, 48.742], mean action: 1.455 [0.000, 3.000],  loss: 18.815053, mse: 1889.586946, mean_q: 33.824592, mean_eps: 0.988526
   3935/300000: episode: 43, duration: 0.418s, episode steps:  66, steps per second: 158, episode reward: -226.383, mean reward: -3.430 [-100.000,  5.326], mean action: 1.394 [0.000, 3.000],  loss: 13.950332, mse: 1931.175060, mean_q: 34.762141, mean_eps: 0.988295
   4000/300000: episode: 44, duration: 0.417s, episode steps:  65, steps per second: 156, episode reward: -178.519, mean reward: -2.746 [-100.000,  2.938], mean action: 1.600 [0.000, 3.000],  loss: 18.270437, mse: 2003.063897, mean_q: 35.308874, mean_eps: 0.988099
   4102/300000: episode: 45, duration: 0.648s, episode steps: 102, steps per second: 157, episode reward: -320.236, mean reward: -3.140 [-100.000, 13.217], mean action: 1.569 [0.000, 3.000],  loss: 22.397563, mse: 2000.956719, mean_q: 33.456321, mean_eps: 0.987848
   4169/300000: episode: 46, duration: 0.396s, episode steps:  67, steps per second: 169, episode reward: -28.811, mean reward: -0.430 [-100.000, 105.877], mean action: 1.522 [0.000, 3.000],  loss: 15.029910, mse: 2154.209746, mean_q: 34.675254, mean_eps: 0.987595
   4231/300000: episode: 47, duration: 0.415s, episode steps:  62, steps per second: 149, episode reward: -92.806, mean reward: -1.497 [-100.000, 10.744], mean action: 1.419 [0.000, 3.000],  loss: 18.239211, mse: 2197.685645, mean_q: 34.102144, mean_eps: 0.987401
   4337/300000: episode: 48, duration: 0.662s, episode steps: 106, steps per second: 160, episode reward: 29.322, mean reward:  0.277 [-100.000, 96.957], mean action: 1.340 [0.000, 3.000],  loss: 19.330676, mse: 2141.661344, mean_q: 34.333640, mean_eps: 0.987149
   4435/300000: episode: 49, duration: 0.605s, episode steps:  98, steps per second: 162, episode reward: -97.305, mean reward: -0.993 [-100.000,  9.704], mean action: 1.612 [0.000, 3.000],  loss: 25.604291, mse: 2164.802486, mean_q: 34.087562, mean_eps: 0.986843
   4517/300000: episode: 50, duration: 0.503s, episode steps:  82, steps per second: 163, episode reward: 26.367, mean reward:  0.322 [-100.000, 107.951], mean action: 1.439 [0.000, 3.000],  loss: 29.148103, mse: 2195.720496, mean_q: 34.599038, mean_eps: 0.986573
   4657/300000: episode: 51, duration: 0.863s, episode steps: 140, steps per second: 162, episode reward: -369.176, mean reward: -2.637 [-100.000, 28.039], mean action: 1.350 [0.000, 3.000],  loss: 34.561762, mse: 2280.295823, mean_q: 34.822258, mean_eps: 0.986240
   4748/300000: episode: 52, duration: 0.535s, episode steps:  91, steps per second: 170, episode reward: -164.541, mean reward: -1.808 [-100.000,  8.017], mean action: 1.396 [0.000, 3.000],  loss: 23.469239, mse: 2385.923258, mean_q: 35.780260, mean_eps: 0.985894
   4833/300000: episode: 53, duration: 0.507s, episode steps:  85, steps per second: 168, episode reward: -259.698, mean reward: -3.055 [-100.000,  4.859], mean action: 1.200 [0.000, 3.000],  loss: 27.672915, mse: 2429.194260, mean_q: 35.676562, mean_eps: 0.985630
   4920/300000: episode: 54, duration: 0.535s, episode steps:  87, steps per second: 163, episode reward: -121.051, mean reward: -1.391 [-100.000, 24.393], mean action: 1.575 [0.000, 3.000],  loss: 28.637617, mse: 2428.035159, mean_q: 34.421898, mean_eps: 0.985372
   5041/300000: episode: 55, duration: 0.766s, episode steps: 121, steps per second: 158, episode reward: -187.373, mean reward: -1.549 [-100.000, 42.858], mean action: 1.579 [0.000, 3.000],  loss: 27.034325, mse: 2550.877793, mean_q: 35.283134, mean_eps: 0.985060
   5149/300000: episode: 56, duration: 0.652s, episode steps: 108, steps per second: 166, episode reward: -118.233, mean reward: -1.095 [-100.000, 20.858], mean action: 1.463 [0.000, 3.000],  loss: 24.057373, mse: 2649.537256, mean_q: 37.198741, mean_eps: 0.984717
   5242/300000: episode: 57, duration: 0.560s, episode steps:  93, steps per second: 166, episode reward: -134.462, mean reward: -1.446 [-100.000,  5.829], mean action: 1.333 [0.000, 3.000],  loss: 31.193176, mse: 2631.453957, mean_q: 37.870373, mean_eps: 0.984415
   5308/300000: episode: 58, duration: 0.426s, episode steps:  66, steps per second: 155, episode reward: -114.090, mean reward: -1.729 [-100.000, 21.707], mean action: 1.439 [0.000, 3.000],  loss: 21.180230, mse: 2699.301621, mean_q: 38.080767, mean_eps: 0.984176
   5383/300000: episode: 59, duration: 0.449s, episode steps:  75, steps per second: 167, episode reward: -112.549, mean reward: -1.501 [-100.000,  7.830], mean action: 1.453 [0.000, 3.000],  loss: 31.935729, mse: 2721.063311, mean_q: 37.734176, mean_eps: 0.983965
   5439/300000: episode: 60, duration: 0.335s, episode steps:  56, steps per second: 167, episode reward: -97.866, mean reward: -1.748 [-100.000,  8.682], mean action: 1.339 [0.000, 3.000],  loss: 35.612288, mse: 2833.247781, mean_q: 35.885256, mean_eps: 0.983769
   5552/300000: episode: 61, duration: 0.689s, episode steps: 113, steps per second: 164, episode reward: -191.301, mean reward: -1.693 [-100.000, 11.551], mean action: 1.558 [0.000, 3.000],  loss: 22.187750, mse: 2793.390971, mean_q: 36.790796, mean_eps: 0.983515
   5660/300000: episode: 62, duration: 0.688s, episode steps: 108, steps per second: 157, episode reward: -438.382, mean reward: -4.059 [-100.000,  1.326], mean action: 1.694 [0.000, 3.000],  loss: 21.977512, mse: 2893.839172, mean_q: 36.498481, mean_eps: 0.983183
   5727/300000: episode: 63, duration: 0.425s, episode steps:  67, steps per second: 158, episode reward: -77.499, mean reward: -1.157 [-100.000, 16.793], mean action: 1.627 [0.000, 3.000],  loss: 29.696137, mse: 2987.809384, mean_q: 37.629098, mean_eps: 0.982921
   5847/300000: episode: 64, duration: 0.734s, episode steps: 120, steps per second: 164, episode reward: -135.880, mean reward: -1.132 [-100.000,  8.443], mean action: 1.592 [0.000, 3.000],  loss: 21.100825, mse: 3064.071539, mean_q: 38.571328, mean_eps: 0.982640
   5909/300000: episode: 65, duration: 0.386s, episode steps:  62, steps per second: 161, episode reward: -66.296, mean reward: -1.069 [-100.000, 11.916], mean action: 1.548 [0.000, 3.000],  loss: 23.112453, mse: 3189.088387, mean_q: 39.705300, mean_eps: 0.982367
   6001/300000: episode: 66, duration: 0.637s, episode steps:  92, steps per second: 144, episode reward: -322.627, mean reward: -3.507 [-100.000,  0.503], mean action: 1.446 [0.000, 3.000],  loss: 25.906203, mse: 3297.683050, mean_q: 40.272128, mean_eps: 0.982137
   6101/300000: episode: 67, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -97.742, mean reward: -0.977 [-100.000, 14.445], mean action: 1.520 [0.000, 3.000],  loss: 26.123340, mse: 3461.478933, mean_q: 42.513665, mean_eps: 0.981848
   6191/300000: episode: 68, duration: 0.587s, episode steps:  90, steps per second: 153, episode reward: -112.562, mean reward: -1.251 [-100.000, 27.325], mean action: 1.467 [0.000, 3.000],  loss: 26.225981, mse: 3578.661062, mean_q: 44.341064, mean_eps: 0.981564
   6262/300000: episode: 69, duration: 0.565s, episode steps:  71, steps per second: 126, episode reward: -112.688, mean reward: -1.587 [-100.000,  7.602], mean action: 1.465 [0.000, 3.000],  loss: 23.636364, mse: 3530.412305, mean_q: 44.261013, mean_eps: 0.981322
   6364/300000: episode: 70, duration: 0.681s, episode steps: 102, steps per second: 150, episode reward: -214.136, mean reward: -2.099 [-100.000, 19.077], mean action: 1.549 [0.000, 3.000],  loss: 19.955976, mse: 3589.394153, mean_q: 44.308120, mean_eps: 0.981063
   6446/300000: episode: 71, duration: 0.585s, episode steps:  82, steps per second: 140, episode reward: -85.256, mean reward: -1.040 [-100.000,  9.711], mean action: 1.537 [0.000, 3.000],  loss: 25.640117, mse: 3700.991780, mean_q: 46.152220, mean_eps: 0.980787
   6523/300000: episode: 72, duration: 0.597s, episode steps:  77, steps per second: 129, episode reward: -160.172, mean reward: -2.080 [-100.000, 32.230], mean action: 1.532 [0.000, 3.000],  loss: 23.149795, mse: 3779.851908, mean_q: 45.237636, mean_eps: 0.980548
   6604/300000: episode: 73, duration: 0.593s, episode steps:  81, steps per second: 137, episode reward: -52.988, mean reward: -0.654 [-100.000, 50.380], mean action: 1.420 [0.000, 3.000],  loss: 21.240312, mse: 3883.581118, mean_q: 44.021549, mean_eps: 0.980311
   6700/300000: episode: 74, duration: 0.684s, episode steps:  96, steps per second: 140, episode reward: -102.344, mean reward: -1.066 [-100.000, 10.992], mean action: 1.615 [0.000, 3.000],  loss: 29.865695, mse: 3907.023122, mean_q: 46.364127, mean_eps: 0.980046
   6801/300000: episode: 75, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -150.674, mean reward: -1.492 [-100.000, 11.270], mean action: 1.475 [0.000, 3.000],  loss: 19.483092, mse: 4113.147521, mean_q: 46.841347, mean_eps: 0.979750
   6896/300000: episode: 76, duration: 0.709s, episode steps:  95, steps per second: 134, episode reward: -405.748, mean reward: -4.271 [-100.000,  0.022], mean action: 1.558 [0.000, 3.000],  loss: 18.376524, mse: 4119.060272, mean_q: 47.412608, mean_eps: 0.979456
   6963/300000: episode: 77, duration: 0.423s, episode steps:  67, steps per second: 158, episode reward: -255.818, mean reward: -3.818 [-100.000,  2.683], mean action: 1.388 [0.000, 3.000],  loss: 29.685305, mse: 4227.448424, mean_q: 49.312839, mean_eps: 0.979213
   7034/300000: episode: 78, duration: 0.460s, episode steps:  71, steps per second: 154, episode reward: -124.783, mean reward: -1.758 [-100.000, 26.411], mean action: 1.563 [0.000, 3.000],  loss: 31.705384, mse: 4296.436379, mean_q: 49.200306, mean_eps: 0.979006
   7201/300000: episode: 79, duration: 1.328s, episode steps: 167, steps per second: 126, episode reward: -97.271, mean reward: -0.582 [-100.000, 12.567], mean action: 1.581 [0.000, 3.000],  loss: 23.823636, mse: 4395.465028, mean_q: 49.640433, mean_eps: 0.978649
   7283/300000: episode: 80, duration: 0.556s, episode steps:  82, steps per second: 147, episode reward: -317.675, mean reward: -3.874 [-100.000,  8.527], mean action: 1.451 [0.000, 3.000],  loss: 21.612055, mse: 4537.494822, mean_q: 50.055356, mean_eps: 0.978276
   7390/300000: episode: 81, duration: 0.647s, episode steps: 107, steps per second: 165, episode reward: -354.377, mean reward: -3.312 [-100.000,  1.102], mean action: 1.607 [0.000, 3.000],  loss: 31.692683, mse: 4537.035398, mean_q: 49.191644, mean_eps: 0.977992
   7457/300000: episode: 82, duration: 0.407s, episode steps:  67, steps per second: 165, episode reward: -156.667, mean reward: -2.338 [-100.000,  7.973], mean action: 1.687 [0.000, 3.000],  loss: 22.685852, mse: 4638.589924, mean_q: 51.382843, mean_eps: 0.977731
   7541/300000: episode: 83, duration: 0.552s, episode steps:  84, steps per second: 152, episode reward: -473.720, mean reward: -5.640 [-100.000, -0.334], mean action: 1.321 [0.000, 3.000],  loss: 15.778754, mse: 4681.207822, mean_q: 50.016281, mean_eps: 0.977504
   7605/300000: episode: 84, duration: 0.399s, episode steps:  64, steps per second: 160, episode reward: -139.104, mean reward: -2.174 [-100.000, 28.663], mean action: 1.516 [0.000, 3.000],  loss: 16.498075, mse: 4570.232620, mean_q: 49.532308, mean_eps: 0.977283
   7719/300000: episode: 85, duration: 0.676s, episode steps: 114, steps per second: 169, episode reward: -100.280, mean reward: -0.880 [-100.000, 13.538], mean action: 1.544 [0.000, 3.000],  loss: 28.761780, mse: 4800.440417, mean_q: 51.231538, mean_eps: 0.977016
   7783/300000: episode: 86, duration: 0.386s, episode steps:  64, steps per second: 166, episode reward: -88.070, mean reward: -1.376 [-100.000,  5.891], mean action: 1.453 [0.000, 3.000],  loss: 25.461119, mse: 4870.132206, mean_q: 49.949037, mean_eps: 0.976749
   7862/300000: episode: 87, duration: 0.512s, episode steps:  79, steps per second: 154, episode reward: -87.752, mean reward: -1.111 [-100.000,  9.282], mean action: 1.392 [0.000, 3.000],  loss: 24.658131, mse: 4909.644253, mean_q: 50.245264, mean_eps: 0.976534
   7973/300000: episode: 88, duration: 0.757s, episode steps: 111, steps per second: 147, episode reward: -117.050, mean reward: -1.055 [-100.000, 17.591], mean action: 1.514 [0.000, 3.000],  loss: 25.958123, mse: 4888.198814, mean_q: 49.704506, mean_eps: 0.976249
   8057/300000: episode: 89, duration: 0.511s, episode steps:  84, steps per second: 164, episode reward: -232.682, mean reward: -2.770 [-100.000, 17.683], mean action: 1.452 [0.000, 3.000],  loss: 26.087967, mse: 4971.984634, mean_q: 49.171018, mean_eps: 0.975957
   8135/300000: episode: 90, duration: 0.464s, episode steps:  78, steps per second: 168, episode reward: -195.426, mean reward: -2.505 [-100.000, 16.898], mean action: 1.372 [0.000, 3.000],  loss: 29.177302, mse: 5072.037100, mean_q: 50.911575, mean_eps: 0.975714
   8218/300000: episode: 91, duration: 0.593s, episode steps:  83, steps per second: 140, episode reward: -286.421, mean reward: -3.451 [-100.000,  2.171], mean action: 1.614 [0.000, 3.000],  loss: 24.708521, mse: 5113.573342, mean_q: 52.241011, mean_eps: 0.975472
   8302/300000: episode: 92, duration: 0.549s, episode steps:  84, steps per second: 153, episode reward: -131.779, mean reward: -1.569 [-100.000, 13.354], mean action: 1.571 [0.000, 3.000],  loss: 27.135440, mse: 5023.729728, mean_q: 51.733160, mean_eps: 0.975222
   8396/300000: episode: 93, duration: 0.718s, episode steps:  94, steps per second: 131, episode reward: -124.456, mean reward: -1.324 [-100.000, 10.139], mean action: 1.298 [0.000, 3.000],  loss: 23.566906, mse: 5199.148843, mean_q: 53.023883, mean_eps: 0.974955
   8465/300000: episode: 94, duration: 0.515s, episode steps:  69, steps per second: 134, episode reward: -227.135, mean reward: -3.292 [-100.000,  6.146], mean action: 1.783 [0.000, 3.000],  loss: 26.985867, mse: 5249.332944, mean_q: 52.421763, mean_eps: 0.974710
   8600/300000: episode: 95, duration: 0.991s, episode steps: 135, steps per second: 136, episode reward: -112.277, mean reward: -0.832 [-100.000, 15.708], mean action: 1.533 [0.000, 3.000],  loss: 26.663397, mse: 5348.425183, mean_q: 53.767845, mean_eps: 0.974404
   8671/300000: episode: 96, duration: 0.484s, episode steps:  71, steps per second: 147, episode reward: -130.374, mean reward: -1.836 [-100.000,  5.619], mean action: 1.563 [0.000, 3.000],  loss: 21.873244, mse: 5597.864388, mean_q: 54.026659, mean_eps: 0.974095
   8745/300000: episode: 97, duration: 0.456s, episode steps:  74, steps per second: 162, episode reward: -179.145, mean reward: -2.421 [-100.000, 10.659], mean action: 1.581 [0.000, 3.000],  loss: 17.737948, mse: 5579.380833, mean_q: 53.856741, mean_eps: 0.973878
   8811/300000: episode: 98, duration: 0.417s, episode steps:  66, steps per second: 158, episode reward: -65.465, mean reward: -0.992 [-100.000, 17.383], mean action: 1.485 [0.000, 3.000],  loss: 17.159764, mse: 5583.413341, mean_q: 53.341474, mean_eps: 0.973668
   8901/300000: episode: 99, duration: 0.578s, episode steps:  90, steps per second: 156, episode reward: -88.293, mean reward: -0.981 [-100.000, 21.778], mean action: 1.578 [0.000, 3.000],  loss: 15.504080, mse: 5674.334871, mean_q: 53.623483, mean_eps: 0.973433
   8979/300000: episode: 100, duration: 0.522s, episode steps:  78, steps per second: 149, episode reward: -173.365, mean reward: -2.223 [-100.000, 29.757], mean action: 1.500 [0.000, 3.000],  loss: 34.311330, mse: 5901.773575, mean_q: 54.702446, mean_eps: 0.973181
   9067/300000: episode: 101, duration: 0.642s, episode steps:  88, steps per second: 137, episode reward: -130.569, mean reward: -1.484 [-100.000,  8.172], mean action: 1.273 [0.000, 3.000],  loss: 26.363594, mse: 5858.289640, mean_q: 54.923137, mean_eps: 0.972932
   9190/300000: episode: 102, duration: 0.940s, episode steps: 123, steps per second: 131, episode reward: 11.891, mean reward:  0.097 [-100.000, 95.807], mean action: 1.488 [0.000, 3.000],  loss: 17.036496, mse: 5911.114792, mean_q: 55.338181, mean_eps: 0.972616
   9314/300000: episode: 103, duration: 0.810s, episode steps: 124, steps per second: 153, episode reward: -178.049, mean reward: -1.436 [-100.000,  7.082], mean action: 1.556 [0.000, 3.000],  loss: 28.869021, mse: 5931.686466, mean_q: 55.958754, mean_eps: 0.972245
   9402/300000: episode: 104, duration: 0.530s, episode steps:  88, steps per second: 166, episode reward: -136.045, mean reward: -1.546 [-100.000, 16.690], mean action: 1.534 [0.000, 3.000],  loss: 26.194252, mse: 5883.347218, mean_q: 54.913644, mean_eps: 0.971927
   9465/300000: episode: 105, duration: 0.411s, episode steps:  63, steps per second: 153, episode reward: -74.196, mean reward: -1.178 [-100.000, 12.013], mean action: 1.571 [0.000, 3.000],  loss: 23.257895, mse: 5968.131131, mean_q: 55.301327, mean_eps: 0.971701
   9568/300000: episode: 106, duration: 0.653s, episode steps: 103, steps per second: 158, episode reward: -391.063, mean reward: -3.797 [-100.000,  4.917], mean action: 1.282 [0.000, 3.000],  loss: 22.208764, mse: 5881.675103, mean_q: 54.640845, mean_eps: 0.971452
   9708/300000: episode: 107, duration: 0.903s, episode steps: 140, steps per second: 155, episode reward: -166.467, mean reward: -1.189 [-100.000,  7.716], mean action: 1.521 [0.000, 3.000],  loss: 23.420962, mse: 6028.451859, mean_q: 56.195507, mean_eps: 0.971087
   9808/300000: episode: 108, duration: 0.641s, episode steps: 100, steps per second: 156, episode reward: -382.434, mean reward: -3.824 [-100.000, 84.678], mean action: 1.460 [0.000, 3.000],  loss: 26.964101, mse: 6043.276704, mean_q: 56.245468, mean_eps: 0.970727
   9896/300000: episode: 109, duration: 0.550s, episode steps:  88, steps per second: 160, episode reward: -126.629, mean reward: -1.439 [-100.000, 16.322], mean action: 1.386 [0.000, 3.000],  loss: 22.979195, mse: 6241.489530, mean_q: 55.940090, mean_eps: 0.970445
  10018/300000: episode: 110, duration: 0.775s, episode steps: 122, steps per second: 157, episode reward: -164.518, mean reward: -1.349 [-100.000,  7.388], mean action: 1.484 [0.000, 3.000],  loss: 24.967322, mse: 6323.006320, mean_q: 56.333889, mean_eps: 0.970131
  10111/300000: episode: 111, duration: 0.629s, episode steps:  93, steps per second: 148, episode reward: -255.220, mean reward: -2.744 [-100.000,  0.974], mean action: 1.634 [0.000, 3.000],  loss: 24.752956, mse: 6271.996834, mean_q: 54.971257, mean_eps: 0.969808
  10226/300000: episode: 112, duration: 0.729s, episode steps: 115, steps per second: 158, episode reward: -197.109, mean reward: -1.714 [-100.000, 26.561], mean action: 1.565 [0.000, 3.000],  loss: 22.489237, mse: 6406.065803, mean_q: 55.164709, mean_eps: 0.969496
  10303/300000: episode: 113, duration: 0.475s, episode steps:  77, steps per second: 162, episode reward: -119.187, mean reward: -1.548 [-100.000,  5.760], mean action: 1.377 [0.000, 3.000],  loss: 24.492161, mse: 6711.220342, mean_q: 57.314027, mean_eps: 0.969208
  10380/300000: episode: 114, duration: 0.459s, episode steps:  77, steps per second: 168, episode reward: -72.140, mean reward: -0.937 [-100.000,  7.136], mean action: 1.558 [0.000, 3.000],  loss: 28.175772, mse: 6711.703442, mean_q: 57.175048, mean_eps: 0.968977
  10445/300000: episode: 115, duration: 0.389s, episode steps:  65, steps per second: 167, episode reward: -87.918, mean reward: -1.353 [-100.000,  7.185], mean action: 1.385 [0.000, 3.000],  loss: 21.604437, mse: 6700.696980, mean_q: 56.482093, mean_eps: 0.968764
  10569/300000: episode: 116, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -152.317, mean reward: -1.228 [-100.000,  3.055], mean action: 1.500 [0.000, 3.000],  loss: 31.803481, mse: 6809.898296, mean_q: 56.980391, mean_eps: 0.968480
  10638/300000: episode: 117, duration: 0.515s, episode steps:  69, steps per second: 134, episode reward: -78.567, mean reward: -1.139 [-100.000, 16.319], mean action: 1.623 [0.000, 3.000],  loss: 29.397775, mse: 6861.352355, mean_q: 60.143374, mean_eps: 0.968191
  10714/300000: episode: 118, duration: 0.590s, episode steps:  76, steps per second: 129, episode reward: -120.568, mean reward: -1.586 [-100.000,  9.089], mean action: 1.474 [0.000, 3.000],  loss: 25.852935, mse: 6727.591084, mean_q: 58.917154, mean_eps: 0.967974
  10805/300000: episode: 119, duration: 0.731s, episode steps:  91, steps per second: 124, episode reward: -401.236, mean reward: -4.409 [-100.000, -0.358], mean action: 1.626 [0.000, 3.000],  loss: 26.797460, mse: 6755.832122, mean_q: 56.964568, mean_eps: 0.967723
  10908/300000: episode: 120, duration: 0.703s, episode steps: 103, steps per second: 147, episode reward: -121.612, mean reward: -1.181 [-100.000,  6.124], mean action: 1.534 [0.000, 3.000],  loss: 26.695503, mse: 6799.163456, mean_q: 57.727355, mean_eps: 0.967432
  10986/300000: episode: 121, duration: 0.531s, episode steps:  78, steps per second: 147, episode reward: -121.246, mean reward: -1.554 [-100.000,  7.058], mean action: 1.513 [0.000, 3.000],  loss: 31.624212, mse: 6923.321239, mean_q: 57.566560, mean_eps: 0.967161
  11069/300000: episode: 122, duration: 0.549s, episode steps:  83, steps per second: 151, episode reward: -119.967, mean reward: -1.445 [-100.000,  9.584], mean action: 1.494 [0.000, 3.000],  loss: 22.594726, mse: 6481.022996, mean_q: 54.175015, mean_eps: 0.966919
  11173/300000: episode: 123, duration: 0.757s, episode steps: 104, steps per second: 137, episode reward: -269.354, mean reward: -2.590 [-100.000,  0.617], mean action: 1.365 [0.000, 3.000],  loss: 24.366218, mse: 6497.606088, mean_q: 55.759916, mean_eps: 0.966639
  11271/300000: episode: 124, duration: 0.620s, episode steps:  98, steps per second: 158, episode reward: -26.525, mean reward: -0.271 [-100.000, 84.071], mean action: 1.531 [0.000, 3.000],  loss: 24.754710, mse: 6458.342669, mean_q: 55.402290, mean_eps: 0.966336
  11337/300000: episode: 125, duration: 0.476s, episode steps:  66, steps per second: 139, episode reward: -91.055, mean reward: -1.380 [-100.000, 31.991], mean action: 1.348 [0.000, 3.000],  loss: 22.718677, mse: 6669.702674, mean_q: 57.818194, mean_eps: 0.966090
  11467/300000: episode: 126, duration: 1.207s, episode steps: 130, steps per second: 108, episode reward: -101.997, mean reward: -0.785 [-100.000, 10.777], mean action: 1.546 [0.000, 3.000],  loss: 27.972544, mse: 6536.986168, mean_q: 54.832894, mean_eps: 0.965796
  11550/300000: episode: 127, duration: 0.751s, episode steps:  83, steps per second: 111, episode reward:  9.320, mean reward:  0.112 [-100.000, 114.889], mean action: 1.422 [0.000, 3.000],  loss: 15.548090, mse: 6748.218956, mean_q: 54.877865, mean_eps: 0.965476
  11638/300000: episode: 128, duration: 0.760s, episode steps:  88, steps per second: 116, episode reward: -136.811, mean reward: -1.555 [-100.000, 15.593], mean action: 1.489 [0.000, 3.000],  loss: 23.954231, mse: 6400.580239, mean_q: 52.585793, mean_eps: 0.965219
  11695/300000: episode: 129, duration: 0.377s, episode steps:  57, steps per second: 151, episode reward: -88.942, mean reward: -1.560 [-100.000,  7.567], mean action: 1.368 [0.000, 3.000],  loss: 35.782055, mse: 6422.642056, mean_q: 53.239512, mean_eps: 0.965002
  11777/300000: episode: 130, duration: 0.636s, episode steps:  82, steps per second: 129, episode reward: -178.504, mean reward: -2.177 [-100.000,  5.128], mean action: 1.427 [0.000, 3.000],  loss: 33.912033, mse: 6533.242193, mean_q: 54.623435, mean_eps: 0.964793
  11867/300000: episode: 131, duration: 0.560s, episode steps:  90, steps per second: 161, episode reward: -216.329, mean reward: -2.404 [-100.000, 98.521], mean action: 1.411 [0.000, 3.000],  loss: 21.593959, mse: 6567.419320, mean_q: 53.981051, mean_eps: 0.964536
  11951/300000: episode: 132, duration: 0.575s, episode steps:  84, steps per second: 146, episode reward: -30.418, mean reward: -0.362 [-100.000, 83.326], mean action: 1.393 [0.000, 3.000],  loss: 27.878228, mse: 6760.832676, mean_q: 53.714522, mean_eps: 0.964274
  12070/300000: episode: 133, duration: 0.857s, episode steps: 119, steps per second: 139, episode reward: -143.696, mean reward: -1.208 [-100.000,  6.221], mean action: 1.605 [0.000, 3.000],  loss: 30.115932, mse: 6455.097968, mean_q: 51.743202, mean_eps: 0.963970
  12137/300000: episode: 134, duration: 0.508s, episode steps:  67, steps per second: 132, episode reward: -123.124, mean reward: -1.838 [-100.000, 11.570], mean action: 1.493 [0.000, 3.000],  loss: 29.638124, mse: 6211.138818, mean_q: 50.058442, mean_eps: 0.963691
  12230/300000: episode: 135, duration: 0.739s, episode steps:  93, steps per second: 126, episode reward: -274.190, mean reward: -2.948 [-100.000, 99.329], mean action: 1.538 [0.000, 3.000],  loss: 17.812974, mse: 6300.219590, mean_q: 50.968833, mean_eps: 0.963451
  12359/300000: episode: 136, duration: 0.996s, episode steps: 129, steps per second: 130, episode reward: -256.047, mean reward: -1.985 [-100.000,  7.133], mean action: 1.481 [0.000, 3.000],  loss: 27.404857, mse: 6169.077035, mean_q: 48.574923, mean_eps: 0.963118
  12464/300000: episode: 137, duration: 0.713s, episode steps: 105, steps per second: 147, episode reward: -551.848, mean reward: -5.256 [-100.000,  1.110], mean action: 1.752 [0.000, 3.000],  loss: 32.143186, mse: 6140.247077, mean_q: 48.798195, mean_eps: 0.962767
  12528/300000: episode: 138, duration: 0.458s, episode steps:  64, steps per second: 140, episode reward: -144.332, mean reward: -2.255 [-100.000, 17.097], mean action: 1.297 [0.000, 3.000],  loss: 32.353144, mse: 5891.003361, mean_q: 47.293200, mean_eps: 0.962514
  12627/300000: episode: 139, duration: 0.691s, episode steps:  99, steps per second: 143, episode reward: -96.100, mean reward: -0.971 [-100.000, 12.152], mean action: 1.576 [0.000, 3.000],  loss: 24.636077, mse: 6201.248964, mean_q: 48.487499, mean_eps: 0.962269
  12736/300000: episode: 140, duration: 0.659s, episode steps: 109, steps per second: 165, episode reward: -177.307, mean reward: -1.627 [-100.000,  7.934], mean action: 1.606 [0.000, 3.000],  loss: 28.315618, mse: 6049.658821, mean_q: 45.732179, mean_eps: 0.961957
  12828/300000: episode: 141, duration: 0.555s, episode steps:  92, steps per second: 166, episode reward: -220.017, mean reward: -2.391 [-100.000, 37.546], mean action: 1.587 [0.000, 3.000],  loss: 29.576584, mse: 6167.699797, mean_q: 47.023292, mean_eps: 0.961655
  12912/300000: episode: 142, duration: 0.556s, episode steps:  84, steps per second: 151, episode reward: -289.571, mean reward: -3.447 [-100.000, 61.682], mean action: 1.488 [0.000, 3.000],  loss: 27.787975, mse: 6060.506914, mean_q: 46.643078, mean_eps: 0.961392
  12979/300000: episode: 143, duration: 0.419s, episode steps:  67, steps per second: 160, episode reward: -57.245, mean reward: -0.854 [-100.000, 12.349], mean action: 1.597 [0.000, 3.000],  loss: 29.410860, mse: 6088.870307, mean_q: 47.512283, mean_eps: 0.961165
  13103/300000: episode: 144, duration: 0.892s, episode steps: 124, steps per second: 139, episode reward: -146.538, mean reward: -1.182 [-100.000, 32.886], mean action: 1.597 [0.000, 3.000],  loss: 36.146223, mse: 6034.717559, mean_q: 46.863721, mean_eps: 0.960878
  13183/300000: episode: 145, duration: 0.666s, episode steps:  80, steps per second: 120, episode reward: -150.325, mean reward: -1.879 [-100.000,  7.674], mean action: 1.575 [0.000, 3.000],  loss: 28.243395, mse: 5792.426428, mean_q: 45.434766, mean_eps: 0.960572
  13268/300000: episode: 146, duration: 0.596s, episode steps:  85, steps per second: 143, episode reward: -189.892, mean reward: -2.234 [-100.000, 40.820], mean action: 1.624 [0.000, 3.000],  loss: 34.687171, mse: 5842.634332, mean_q: 45.074282, mean_eps: 0.960325
  13387/300000: episode: 147, duration: 0.746s, episode steps: 119, steps per second: 159, episode reward: -167.001, mean reward: -1.403 [-100.000,  8.770], mean action: 1.345 [0.000, 3.000],  loss: 38.653057, mse: 5752.744432, mean_q: 45.011397, mean_eps: 0.960019
  13469/300000: episode: 148, duration: 0.490s, episode steps:  82, steps per second: 167, episode reward: -150.354, mean reward: -1.834 [-100.000,  9.032], mean action: 1.695 [0.000, 3.000],  loss: 28.332708, mse: 5868.361614, mean_q: 46.916927, mean_eps: 0.959717
  13593/300000: episode: 149, duration: 0.803s, episode steps: 124, steps per second: 154, episode reward: -112.100, mean reward: -0.904 [-100.000,  7.158], mean action: 1.685 [0.000, 3.000],  loss: 25.060618, mse: 5755.865656, mean_q: 45.653110, mean_eps: 0.959408
  13662/300000: episode: 150, duration: 0.464s, episode steps:  69, steps per second: 149, episode reward: -135.703, mean reward: -1.967 [-100.000, 10.014], mean action: 1.681 [0.000, 3.000],  loss: 33.635741, mse: 5871.825167, mean_q: 46.237579, mean_eps: 0.959119
  13776/300000: episode: 151, duration: 0.754s, episode steps: 114, steps per second: 151, episode reward: -138.903, mean reward: -1.218 [-100.000,  8.815], mean action: 1.439 [0.000, 3.000],  loss: 23.664234, mse: 5535.896896, mean_q: 44.169045, mean_eps: 0.958844
  13862/300000: episode: 152, duration: 0.848s, episode steps:  86, steps per second: 101, episode reward: -150.033, mean reward: -1.745 [-100.000, 10.438], mean action: 1.372 [0.000, 3.000],  loss: 20.527138, mse: 5869.180761, mean_q: 46.813961, mean_eps: 0.958545
  13955/300000: episode: 153, duration: 0.793s, episode steps:  93, steps per second: 117, episode reward: -209.152, mean reward: -2.249 [-100.000,  5.504], mean action: 1.398 [0.000, 3.000],  loss: 22.241216, mse: 5840.833984, mean_q: 47.414144, mean_eps: 0.958276
  14022/300000: episode: 154, duration: 0.698s, episode steps:  67, steps per second:  96, episode reward: -80.987, mean reward: -1.209 [-100.000,  5.973], mean action: 1.537 [0.000, 3.000],  loss: 40.136035, mse: 5674.301171, mean_q: 42.464697, mean_eps: 0.958036
  14103/300000: episode: 155, duration: 0.636s, episode steps:  81, steps per second: 127, episode reward: -127.062, mean reward: -1.569 [-100.000,  7.137], mean action: 1.333 [0.000, 3.000],  loss: 21.196162, mse: 5511.813332, mean_q: 43.171458, mean_eps: 0.957814
  14162/300000: episode: 156, duration: 0.568s, episode steps:  59, steps per second: 104, episode reward: -89.975, mean reward: -1.525 [-100.000, 11.548], mean action: 1.407 [0.000, 3.000],  loss: 24.385118, mse: 5396.573445, mean_q: 44.068977, mean_eps: 0.957604
  14244/300000: episode: 157, duration: 0.518s, episode steps:  82, steps per second: 158, episode reward: -128.102, mean reward: -1.562 [-100.000, 10.032], mean action: 1.390 [0.000, 3.000],  loss: 33.763562, mse: 5499.280669, mean_q: 44.496179, mean_eps: 0.957392
  14350/300000: episode: 158, duration: 0.685s, episode steps: 106, steps per second: 155, episode reward: -153.817, mean reward: -1.451 [-100.000,  5.638], mean action: 1.387 [0.000, 3.000],  loss: 20.122993, mse: 5341.087326, mean_q: 42.746068, mean_eps: 0.957111
  14489/300000: episode: 159, duration: 0.864s, episode steps: 139, steps per second: 161, episode reward: -370.741, mean reward: -2.667 [-100.000,  1.165], mean action: 1.626 [0.000, 3.000],  loss: 21.201828, mse: 5319.463288, mean_q: 43.465350, mean_eps: 0.956743
  14553/300000: episode: 160, duration: 0.387s, episode steps:  64, steps per second: 165, episode reward: -53.309, mean reward: -0.833 [-100.000, 14.201], mean action: 1.594 [0.000, 3.000],  loss: 15.301428, mse: 5298.276352, mean_q: 43.927959, mean_eps: 0.956438
  14689/300000: episode: 161, duration: 0.823s, episode steps: 136, steps per second: 165, episode reward: -204.049, mean reward: -1.500 [-100.000,  8.666], mean action: 1.456 [0.000, 3.000],  loss: 35.803759, mse: 5503.493182, mean_q: 45.512111, mean_eps: 0.956138
  14768/300000: episode: 162, duration: 0.496s, episode steps:  79, steps per second: 159, episode reward: -177.418, mean reward: -2.246 [-100.000,  5.923], mean action: 1.506 [0.000, 3.000],  loss: 27.552374, mse: 5627.602493, mean_q: 47.335761, mean_eps: 0.955816
  14857/300000: episode: 163, duration: 0.538s, episode steps:  89, steps per second: 166, episode reward: -75.566, mean reward: -0.849 [-100.000, 15.562], mean action: 1.539 [0.000, 3.000],  loss: 24.007513, mse: 5497.637278, mean_q: 45.672785, mean_eps: 0.955564
  14957/300000: episode: 164, duration: 0.600s, episode steps: 100, steps per second: 167, episode reward: -254.140, mean reward: -2.541 [-100.000,  1.022], mean action: 1.550 [0.000, 3.000],  loss: 35.703520, mse: 5520.672778, mean_q: 44.783840, mean_eps: 0.955281
  15067/300000: episode: 165, duration: 0.690s, episode steps: 110, steps per second: 159, episode reward: -138.943, mean reward: -1.263 [-100.000,  9.603], mean action: 1.527 [0.000, 3.000],  loss: 20.700612, mse: 5817.459286, mean_q: 47.172280, mean_eps: 0.954966
  15148/300000: episode: 166, duration: 0.505s, episode steps:  81, steps per second: 161, episode reward: -90.794, mean reward: -1.121 [-100.000, 35.235], mean action: 1.630 [0.000, 3.000],  loss: 22.951188, mse: 5866.796534, mean_q: 49.035952, mean_eps: 0.954679
  15263/300000: episode: 167, duration: 0.693s, episode steps: 115, steps per second: 166, episode reward: -169.460, mean reward: -1.474 [-100.000,  8.663], mean action: 1.504 [0.000, 3.000],  loss: 21.141456, mse: 6054.854707, mean_q: 49.706208, mean_eps: 0.954385
  15369/300000: episode: 168, duration: 0.647s, episode steps: 106, steps per second: 164, episode reward: -130.542, mean reward: -1.232 [-100.000, 16.815], mean action: 1.538 [0.000, 3.000],  loss: 21.561733, mse: 6074.806562, mean_q: 50.222660, mean_eps: 0.954054
  15441/300000: episode: 169, duration: 0.464s, episode steps:  72, steps per second: 155, episode reward: -99.589, mean reward: -1.383 [-100.000,  6.840], mean action: 1.306 [0.000, 3.000],  loss: 21.494022, mse: 5803.776486, mean_q: 47.841363, mean_eps: 0.953787
  15517/300000: episode: 170, duration: 0.461s, episode steps:  76, steps per second: 165, episode reward: -308.085, mean reward: -4.054 [-100.000, 79.978], mean action: 1.618 [0.000, 3.000],  loss: 16.786377, mse: 5979.090833, mean_q: 48.937132, mean_eps: 0.953565
  15633/300000: episode: 171, duration: 0.692s, episode steps: 116, steps per second: 168, episode reward: -259.377, mean reward: -2.236 [-100.000,  6.508], mean action: 1.569 [0.000, 3.000],  loss: 25.470651, mse: 5894.198562, mean_q: 48.525668, mean_eps: 0.953276
  15740/300000: episode: 172, duration: 0.703s, episode steps: 107, steps per second: 152, episode reward: -406.076, mean reward: -3.795 [-100.000,  0.508], mean action: 1.346 [0.000, 3.000],  loss: 16.789473, mse: 5945.775543, mean_q: 47.784049, mean_eps: 0.952942
  15830/300000: episode: 173, duration: 0.747s, episode steps:  90, steps per second: 121, episode reward: -126.298, mean reward: -1.403 [-100.000, 32.439], mean action: 1.500 [0.000, 3.000],  loss: 16.859053, mse: 6060.585986, mean_q: 46.937732, mean_eps: 0.952647
  15904/300000: episode: 174, duration: 0.478s, episode steps:  74, steps per second: 155, episode reward: -252.075, mean reward: -3.406 [-100.000,  1.473], mean action: 1.419 [0.000, 3.000],  loss: 26.908594, mse: 5875.341968, mean_q: 47.264919, mean_eps: 0.952400
  16009/300000: episode: 175, duration: 0.657s, episode steps: 105, steps per second: 160, episode reward: -363.279, mean reward: -3.460 [-100.000,  1.906], mean action: 1.581 [0.000, 3.000],  loss: 27.335461, mse: 5919.735463, mean_q: 46.318184, mean_eps: 0.952132
  16098/300000: episode: 176, duration: 0.574s, episode steps:  89, steps per second: 155, episode reward: -129.439, mean reward: -1.454 [-100.000, 14.630], mean action: 1.382 [0.000, 3.000],  loss: 30.891080, mse: 5887.741680, mean_q: 43.901872, mean_eps: 0.951841
  16211/300000: episode: 177, duration: 0.714s, episode steps: 113, steps per second: 158, episode reward: -165.240, mean reward: -1.462 [-100.000, 13.718], mean action: 1.504 [0.000, 3.000],  loss: 22.429152, mse: 6080.908715, mean_q: 44.926114, mean_eps: 0.951538
  16347/300000: episode: 178, duration: 0.831s, episode steps: 136, steps per second: 164, episode reward: -155.771, mean reward: -1.145 [-100.000,  6.108], mean action: 1.640 [0.000, 3.000],  loss: 24.714391, mse: 6087.747474, mean_q: 46.070502, mean_eps: 0.951164
  16427/300000: episode: 179, duration: 0.522s, episode steps:  80, steps per second: 153, episode reward: -58.078, mean reward: -0.726 [-100.000, 19.266], mean action: 1.488 [0.000, 3.000],  loss: 16.246617, mse: 5965.989819, mean_q: 46.577918, mean_eps: 0.950840
  16509/300000: episode: 180, duration: 0.501s, episode steps:  82, steps per second: 164, episode reward: -119.816, mean reward: -1.461 [-100.000,  8.687], mean action: 1.537 [0.000, 3.000],  loss: 17.252421, mse: 6128.306036, mean_q: 46.623004, mean_eps: 0.950597
  16584/300000: episode: 181, duration: 0.449s, episode steps:  75, steps per second: 167, episode reward: -208.744, mean reward: -2.783 [-100.000,  4.601], mean action: 1.440 [0.000, 3.000],  loss: 15.807842, mse: 6219.476582, mean_q: 47.389572, mean_eps: 0.950362
  16674/300000: episode: 182, duration: 0.543s, episode steps:  90, steps per second: 166, episode reward: -207.426, mean reward: -2.305 [-100.000, 36.101], mean action: 1.544 [0.000, 3.000],  loss: 26.062008, mse: 6001.295196, mean_q: 46.151738, mean_eps: 0.950114
  16791/300000: episode: 183, duration: 0.742s, episode steps: 117, steps per second: 158, episode reward: -401.764, mean reward: -3.434 [-100.000, 75.430], mean action: 1.641 [0.000, 3.000],  loss: 18.314205, mse: 6091.237138, mean_q: 45.676363, mean_eps: 0.949804
  16864/300000: episode: 184, duration: 0.456s, episode steps:  73, steps per second: 160, episode reward: -234.846, mean reward: -3.217 [-100.000, 76.606], mean action: 1.329 [0.000, 3.000],  loss: 15.819953, mse: 5744.966870, mean_q: 44.121424, mean_eps: 0.949519
  16938/300000: episode: 185, duration: 0.449s, episode steps:  74, steps per second: 165, episode reward: -156.533, mean reward: -2.115 [-100.000,  7.771], mean action: 1.459 [0.000, 3.000],  loss: 22.201707, mse: 6200.055717, mean_q: 46.201587, mean_eps: 0.949299
  17026/300000: episode: 186, duration: 0.533s, episode steps:  88, steps per second: 165, episode reward: -94.477, mean reward: -1.074 [-100.000, 15.794], mean action: 1.523 [0.000, 3.000],  loss: 24.707196, mse: 6139.419595, mean_q: 46.353319, mean_eps: 0.949056
  17131/300000: episode: 187, duration: 0.676s, episode steps: 105, steps per second: 155, episode reward: -317.278, mean reward: -3.022 [-100.000,  1.280], mean action: 1.590 [0.000, 3.000],  loss: 22.834366, mse: 6040.691532, mean_q: 45.448947, mean_eps: 0.948766
  17221/300000: episode: 188, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -82.007, mean reward: -0.911 [-100.000, 10.123], mean action: 1.422 [0.000, 3.000],  loss: 19.755380, mse: 6349.938184, mean_q: 45.834889, mean_eps: 0.948474
  17293/300000: episode: 189, duration: 0.501s, episode steps:  72, steps per second: 144, episode reward: -155.846, mean reward: -2.165 [-100.000,  5.574], mean action: 1.639 [0.000, 3.000],  loss: 32.819607, mse: 6168.893728, mean_q: 45.255304, mean_eps: 0.948231
  17384/300000: episode: 190, duration: 0.625s, episode steps:  91, steps per second: 146, episode reward: -120.720, mean reward: -1.327 [-100.000, 10.883], mean action: 1.418 [0.000, 3.000],  loss: 24.235135, mse: 6226.210650, mean_q: 45.227253, mean_eps: 0.947986
  17498/300000: episode: 191, duration: 0.758s, episode steps: 114, steps per second: 150, episode reward: -126.525, mean reward: -1.110 [-100.000, 10.888], mean action: 1.465 [0.000, 3.000],  loss: 38.255466, mse: 6491.785486, mean_q: 45.640873, mean_eps: 0.947679
  17566/300000: episode: 192, duration: 0.496s, episode steps:  68, steps per second: 137, episode reward: -225.124, mean reward: -3.311 [-100.000,  5.317], mean action: 1.426 [0.000, 3.000],  loss: 42.573528, mse: 6252.158429, mean_q: 45.401325, mean_eps: 0.947405
  17700/300000: episode: 193, duration: 1.142s, episode steps: 134, steps per second: 117, episode reward: -56.921, mean reward: -0.425 [-100.000, 13.915], mean action: 1.410 [0.000, 3.000],  loss: 23.517861, mse: 6504.802344, mean_q: 45.901159, mean_eps: 0.947102
  17768/300000: episode: 194, duration: 0.590s, episode steps:  68, steps per second: 115, episode reward: -76.071, mean reward: -1.119 [-100.000,  7.138], mean action: 1.544 [0.000, 3.000],  loss: 18.882218, mse: 6439.754258, mean_q: 44.341052, mean_eps: 0.946800
  17862/300000: episode: 195, duration: 0.728s, episode steps:  94, steps per second: 129, episode reward: -101.282, mean reward: -1.077 [-100.000,  7.406], mean action: 1.383 [0.000, 3.000],  loss: 25.852244, mse: 6496.354136, mean_q: 45.408412, mean_eps: 0.946556
  17950/300000: episode: 196, duration: 0.743s, episode steps:  88, steps per second: 118, episode reward: -149.648, mean reward: -1.701 [-100.000,  8.538], mean action: 1.443 [0.000, 3.000],  loss: 24.235509, mse: 6184.952298, mean_q: 43.628408, mean_eps: 0.946284
  18039/300000: episode: 197, duration: 0.726s, episode steps:  89, steps per second: 123, episode reward: -167.936, mean reward: -1.887 [-100.000,  7.934], mean action: 1.494 [0.000, 3.000],  loss: 18.527126, mse: 6550.150846, mean_q: 43.910527, mean_eps: 0.946018
  18196/300000: episode: 198, duration: 1.075s, episode steps: 157, steps per second: 146, episode reward: -61.675, mean reward: -0.393 [-100.000, 19.241], mean action: 1.624 [0.000, 3.000],  loss: 19.818935, mse: 6694.357409, mean_q: 45.065492, mean_eps: 0.945649
  18322/300000: episode: 199, duration: 0.942s, episode steps: 126, steps per second: 134, episode reward: -352.260, mean reward: -2.796 [-100.000, 94.877], mean action: 1.579 [0.000, 3.000],  loss: 23.533278, mse: 6553.191554, mean_q: 45.135421, mean_eps: 0.945225
  18420/300000: episode: 200, duration: 0.641s, episode steps:  98, steps per second: 153, episode reward: -118.862, mean reward: -1.213 [-100.000, 18.770], mean action: 1.418 [0.000, 3.000],  loss: 18.108476, mse: 6748.515755, mean_q: 45.394239, mean_eps: 0.944889
  18537/300000: episode: 201, duration: 0.731s, episode steps: 117, steps per second: 160, episode reward: -184.629, mean reward: -1.578 [-100.000,  8.544], mean action: 1.709 [0.000, 3.000],  loss: 28.042407, mse: 6456.887737, mean_q: 44.537469, mean_eps: 0.944566
  18667/300000: episode: 202, duration: 0.902s, episode steps: 130, steps per second: 144, episode reward: -333.044, mean reward: -2.562 [-100.000, 119.925], mean action: 1.392 [0.000, 3.000],  loss: 21.936657, mse: 6528.493957, mean_q: 47.658531, mean_eps: 0.944195
  18756/300000: episode: 203, duration: 0.576s, episode steps:  89, steps per second: 155, episode reward: -89.403, mean reward: -1.005 [-100.000, 17.621], mean action: 1.371 [0.000, 3.000],  loss: 23.752859, mse: 6677.169889, mean_q: 46.066547, mean_eps: 0.943867
  18845/300000: episode: 204, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -70.626, mean reward: -0.794 [-100.000, 14.394], mean action: 1.663 [0.000, 3.000],  loss: 33.915700, mse: 6929.818228, mean_q: 45.815860, mean_eps: 0.943600
  18953/300000: episode: 205, duration: 0.770s, episode steps: 108, steps per second: 140, episode reward: -103.122, mean reward: -0.955 [-100.000, 12.671], mean action: 1.481 [0.000, 3.000],  loss: 26.039185, mse: 6609.697284, mean_q: 47.312628, mean_eps: 0.943304
  19077/300000: episode: 206, duration: 0.795s, episode steps: 124, steps per second: 156, episode reward: -179.443, mean reward: -1.447 [-100.000,  8.649], mean action: 1.540 [0.000, 3.000],  loss: 21.697500, mse: 6522.036901, mean_q: 46.708604, mean_eps: 0.942956
  19188/300000: episode: 207, duration: 0.678s, episode steps: 111, steps per second: 164, episode reward: -122.003, mean reward: -1.099 [-100.000,  6.449], mean action: 1.541 [0.000, 3.000],  loss: 24.040696, mse: 6599.249591, mean_q: 47.724543, mean_eps: 0.942604
  19267/300000: episode: 208, duration: 0.517s, episode steps:  79, steps per second: 153, episode reward: -66.504, mean reward: -0.842 [-100.000, 18.429], mean action: 1.456 [0.000, 3.000],  loss: 19.946438, mse: 6279.662233, mean_q: 46.513585, mean_eps: 0.942319
  19338/300000: episode: 209, duration: 0.451s, episode steps:  71, steps per second: 157, episode reward: -113.502, mean reward: -1.599 [-100.000, 12.747], mean action: 1.521 [0.000, 3.000],  loss: 20.562848, mse: 6566.640243, mean_q: 47.738941, mean_eps: 0.942094
  19446/300000: episode: 210, duration: 0.670s, episode steps: 108, steps per second: 161, episode reward: -393.165, mean reward: -3.640 [-100.000,  1.003], mean action: 1.537 [0.000, 3.000],  loss: 13.504752, mse: 6354.869471, mean_q: 45.928229, mean_eps: 0.941825
  19553/300000: episode: 211, duration: 0.652s, episode steps: 107, steps per second: 164, episode reward: -258.157, mean reward: -2.413 [-100.000,  7.863], mean action: 1.589 [0.000, 3.000],  loss: 23.940645, mse: 6656.024318, mean_q: 47.451022, mean_eps: 0.941503
  19628/300000: episode: 212, duration: 0.500s, episode steps:  75, steps per second: 150, episode reward: -215.581, mean reward: -2.874 [-100.000, 15.181], mean action: 1.680 [0.000, 3.000],  loss: 37.140051, mse: 6425.861156, mean_q: 47.066907, mean_eps: 0.941230
  19705/300000: episode: 213, duration: 0.584s, episode steps:  77, steps per second: 132, episode reward: -174.637, mean reward: -2.268 [-100.000,  6.695], mean action: 1.494 [0.000, 3.000],  loss: 14.439228, mse: 6344.917341, mean_q: 47.171345, mean_eps: 0.941002
  19791/300000: episode: 214, duration: 0.568s, episode steps:  86, steps per second: 151, episode reward: -100.103, mean reward: -1.164 [-100.000, 25.800], mean action: 1.523 [0.000, 3.000],  loss: 23.652823, mse: 6856.410735, mean_q: 51.397594, mean_eps: 0.940758
  19922/300000: episode: 215, duration: 0.869s, episode steps: 131, steps per second: 151, episode reward: -112.944, mean reward: -0.862 [-100.000,  7.168], mean action: 1.450 [0.000, 3.000],  loss: 23.805380, mse: 6553.248464, mean_q: 47.853928, mean_eps: 0.940432
  19995/300000: episode: 216, duration: 0.502s, episode steps:  73, steps per second: 145, episode reward: -180.067, mean reward: -2.467 [-100.000, 22.694], mean action: 1.630 [0.000, 3.000],  loss: 15.889469, mse: 6495.567356, mean_q: 48.955074, mean_eps: 0.940126
  20066/300000: episode: 217, duration: 0.525s, episode steps:  71, steps per second: 135, episode reward: -111.451, mean reward: -1.570 [-100.000, 13.877], mean action: 1.732 [0.000, 3.000],  loss: 13.752159, mse: 6775.832471, mean_q: 51.525699, mean_eps: 0.939910
  20211/300000: episode: 218, duration: 0.966s, episode steps: 145, steps per second: 150, episode reward: -101.793, mean reward: -0.702 [-100.000,  6.814], mean action: 1.434 [0.000, 3.000],  loss: 21.896537, mse: 6868.042470, mean_q: 51.564465, mean_eps: 0.939586
  20288/300000: episode: 219, duration: 0.491s, episode steps:  77, steps per second: 157, episode reward: -85.817, mean reward: -1.115 [-100.000,  9.942], mean action: 1.610 [0.000, 3.000],  loss: 22.779353, mse: 6925.325956, mean_q: 49.949888, mean_eps: 0.939253
  20347/300000: episode: 220, duration: 0.372s, episode steps:  59, steps per second: 159, episode reward: -127.289, mean reward: -2.157 [-100.000,  9.731], mean action: 1.492 [0.000, 3.000],  loss: 26.928514, mse: 7089.832966, mean_q: 53.608697, mean_eps: 0.939049
  20451/300000: episode: 221, duration: 0.644s, episode steps: 104, steps per second: 162, episode reward: -323.423, mean reward: -3.110 [-100.000,  4.844], mean action: 1.240 [0.000, 3.000],  loss: 16.266286, mse: 7158.839618, mean_q: 52.986017, mean_eps: 0.938805
  20519/300000: episode: 222, duration: 0.425s, episode steps:  68, steps per second: 160, episode reward: -288.871, mean reward: -4.248 [-100.000, 16.019], mean action: 1.265 [0.000, 3.000],  loss: 16.525739, mse: 7137.105605, mean_q: 52.426306, mean_eps: 0.938546
  20594/300000: episode: 223, duration: 0.503s, episode steps:  75, steps per second: 149, episode reward: -59.600, mean reward: -0.795 [-100.000, 14.469], mean action: 1.653 [0.000, 3.000],  loss: 22.141141, mse: 6800.953678, mean_q: 49.614585, mean_eps: 0.938332
  20666/300000: episode: 224, duration: 0.465s, episode steps:  72, steps per second: 155, episode reward: -105.907, mean reward: -1.471 [-100.000, 10.375], mean action: 1.583 [0.000, 3.000],  loss: 22.394871, mse: 6936.589227, mean_q: 51.585736, mean_eps: 0.938111
  20771/300000: episode: 225, duration: 0.664s, episode steps: 105, steps per second: 158, episode reward: -95.665, mean reward: -0.911 [-100.000, 11.845], mean action: 1.524 [0.000, 3.000],  loss: 20.629849, mse: 6951.368727, mean_q: 50.023961, mean_eps: 0.937846
  20868/300000: episode: 226, duration: 0.655s, episode steps:  97, steps per second: 148, episode reward: -148.645, mean reward: -1.532 [-100.000, 10.889], mean action: 1.577 [0.000, 3.000],  loss: 38.023962, mse: 7037.327445, mean_q: 51.316408, mean_eps: 0.937543
  20953/300000: episode: 227, duration: 0.646s, episode steps:  85, steps per second: 132, episode reward: -78.698, mean reward: -0.926 [-100.000, 11.923], mean action: 1.553 [0.000, 3.000],  loss: 28.961745, mse: 7194.188936, mean_q: 52.491421, mean_eps: 0.937270
  21041/300000: episode: 228, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -68.590, mean reward: -0.779 [-100.000, 11.718], mean action: 1.602 [0.000, 3.000],  loss: 36.391066, mse: 7203.018438, mean_q: 52.911172, mean_eps: 0.937010
  21142/300000: episode: 229, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: -202.820, mean reward: -2.008 [-100.000,  6.839], mean action: 1.446 [0.000, 3.000],  loss: 30.695041, mse: 7540.107451, mean_q: 56.590475, mean_eps: 0.936727
  21222/300000: episode: 230, duration: 0.532s, episode steps:  80, steps per second: 150, episode reward: -106.261, mean reward: -1.328 [-100.000, 126.875], mean action: 1.575 [0.000, 3.000],  loss: 33.749961, mse: 7070.900958, mean_q: 52.077148, mean_eps: 0.936455
  21329/300000: episode: 231, duration: 0.673s, episode steps: 107, steps per second: 159, episode reward: -139.436, mean reward: -1.303 [-100.000, 12.944], mean action: 1.607 [0.000, 3.000],  loss: 19.368222, mse: 7374.544324, mean_q: 53.374531, mean_eps: 0.936175
  21444/300000: episode: 232, duration: 0.742s, episode steps: 115, steps per second: 155, episode reward: -93.764, mean reward: -0.815 [-100.000,  7.806], mean action: 1.400 [0.000, 3.000],  loss: 31.857300, mse: 7411.796153, mean_q: 53.436928, mean_eps: 0.935842
  21536/300000: episode: 233, duration: 0.759s, episode steps:  92, steps per second: 121, episode reward: -97.803, mean reward: -1.063 [-100.000, 17.441], mean action: 1.652 [0.000, 3.000],  loss: 33.261329, mse: 7335.649414, mean_q: 52.667902, mean_eps: 0.935532
  21644/300000: episode: 234, duration: 0.715s, episode steps: 108, steps per second: 151, episode reward: -147.248, mean reward: -1.363 [-100.000, 11.909], mean action: 1.565 [0.000, 3.000],  loss: 29.289652, mse: 7736.934191, mean_q: 55.853446, mean_eps: 0.935231
  21756/300000: episode: 235, duration: 0.739s, episode steps: 112, steps per second: 151, episode reward: -246.901, mean reward: -2.204 [-100.000, 12.542], mean action: 1.429 [0.000, 3.000],  loss: 16.154759, mse: 7565.824977, mean_q: 55.102365, mean_eps: 0.934901
  21893/300000: episode: 236, duration: 1.235s, episode steps: 137, steps per second: 111, episode reward: -125.748, mean reward: -0.918 [-100.000, 12.220], mean action: 1.613 [0.000, 3.000],  loss: 21.253172, mse: 7585.858972, mean_q: 54.421138, mean_eps: 0.934528
  22018/300000: episode: 237, duration: 0.982s, episode steps: 125, steps per second: 127, episode reward: -286.066, mean reward: -2.289 [-100.000, 56.990], mean action: 1.560 [0.000, 3.000],  loss: 11.193889, mse: 7504.183707, mean_q: 54.630203, mean_eps: 0.934135
  22082/300000: episode: 238, duration: 0.480s, episode steps:  64, steps per second: 133, episode reward: -110.346, mean reward: -1.724 [-100.000, 18.732], mean action: 1.500 [0.000, 3.000],  loss: 40.295976, mse: 7067.642250, mean_q: 51.546815, mean_eps: 0.933852
  22176/300000: episode: 239, duration: 0.669s, episode steps:  94, steps per second: 141, episode reward: -105.792, mean reward: -1.125 [-100.000, 23.454], mean action: 1.404 [0.000, 3.000],  loss: 36.142437, mse: 7657.277437, mean_q: 55.739588, mean_eps: 0.933614
  22243/300000: episode: 240, duration: 0.441s, episode steps:  67, steps per second: 152, episode reward: -256.721, mean reward: -3.832 [-100.000,  4.746], mean action: 1.224 [0.000, 3.000],  loss: 36.291612, mse: 7997.421307, mean_q: 59.316052, mean_eps: 0.933373
  22326/300000: episode: 241, duration: 0.699s, episode steps:  83, steps per second: 119, episode reward: -239.511, mean reward: -2.886 [-100.000,  6.539], mean action: 1.735 [0.000, 3.000],  loss: 30.303160, mse: 7801.638766, mean_q: 58.489955, mean_eps: 0.933148
  22424/300000: episode: 242, duration: 0.778s, episode steps:  98, steps per second: 126, episode reward: -246.625, mean reward: -2.517 [-100.000, 107.282], mean action: 1.520 [0.000, 3.000],  loss: 26.365297, mse: 7677.248156, mean_q: 57.535423, mean_eps: 0.932876
  22496/300000: episode: 243, duration: 0.517s, episode steps:  72, steps per second: 139, episode reward: -76.831, mean reward: -1.067 [-100.000,  7.232], mean action: 1.778 [0.000, 3.000],  loss: 38.098631, mse: 7710.201965, mean_q: 55.483357, mean_eps: 0.932621
  22599/300000: episode: 244, duration: 0.727s, episode steps: 103, steps per second: 142, episode reward: -171.467, mean reward: -1.665 [-100.000, 12.352], mean action: 1.398 [0.000, 3.000],  loss: 28.867592, mse: 8028.311310, mean_q: 59.392444, mean_eps: 0.932359
  22680/300000: episode: 245, duration: 0.627s, episode steps:  81, steps per second: 129, episode reward: -78.035, mean reward: -0.963 [-100.000, 13.499], mean action: 1.543 [0.000, 3.000],  loss: 33.794935, mse: 8007.148498, mean_q: 59.439120, mean_eps: 0.932083
  22807/300000: episode: 246, duration: 0.852s, episode steps: 127, steps per second: 149, episode reward: -127.143, mean reward: -1.001 [-100.000,  8.041], mean action: 1.598 [0.000, 3.000],  loss: 32.857213, mse: 7817.454317, mean_q: 58.733666, mean_eps: 0.931771
  22875/300000: episode: 247, duration: 0.451s, episode steps:  68, steps per second: 151, episode reward: -85.836, mean reward: -1.262 [-100.000, 15.226], mean action: 1.574 [0.000, 3.000],  loss: 18.438473, mse: 8231.710133, mean_q: 60.484851, mean_eps: 0.931479
  22933/300000: episode: 248, duration: 0.383s, episode steps:  58, steps per second: 151, episode reward: -70.662, mean reward: -1.218 [-100.000,  8.943], mean action: 1.328 [0.000, 3.000],  loss: 33.652253, mse: 8115.190581, mean_q: 61.310225, mean_eps: 0.931289
  23048/300000: episode: 249, duration: 0.778s, episode steps: 115, steps per second: 148, episode reward: -205.680, mean reward: -1.789 [-100.000, 14.861], mean action: 1.530 [0.000, 3.000],  loss: 41.242678, mse: 7573.757995, mean_q: 55.840298, mean_eps: 0.931030
  23129/300000: episode: 250, duration: 0.507s, episode steps:  81, steps per second: 160, episode reward: -225.634, mean reward: -2.786 [-100.000,  8.440], mean action: 1.444 [0.000, 3.000],  loss: 27.921341, mse: 7762.618695, mean_q: 57.550727, mean_eps: 0.930736
  23229/300000: episode: 251, duration: 0.639s, episode steps: 100, steps per second: 156, episode reward: -135.241, mean reward: -1.352 [-100.000,  6.345], mean action: 1.620 [0.000, 3.000],  loss: 21.404252, mse: 7903.189126, mean_q: 57.318290, mean_eps: 0.930464
  23291/300000: episode: 252, duration: 0.425s, episode steps:  62, steps per second: 146, episode reward: -155.157, mean reward: -2.503 [-100.000,  8.251], mean action: 1.468 [0.000, 3.000],  loss: 21.841666, mse: 8046.178963, mean_q: 59.484886, mean_eps: 0.930221
  23409/300000: episode: 253, duration: 0.852s, episode steps: 118, steps per second: 138, episode reward: -80.394, mean reward: -0.681 [-100.000, 11.841], mean action: 1.661 [0.000, 3.000],  loss: 32.764649, mse: 7912.012935, mean_q: 57.000636, mean_eps: 0.929951
  23473/300000: episode: 254, duration: 0.459s, episode steps:  64, steps per second: 139, episode reward: -81.028, mean reward: -1.266 [-100.000,  9.056], mean action: 1.578 [0.000, 3.000],  loss: 21.340051, mse: 7568.160797, mean_q: 55.264752, mean_eps: 0.929678
  23576/300000: episode: 255, duration: 0.717s, episode steps: 103, steps per second: 144, episode reward: -125.455, mean reward: -1.218 [-100.000,  8.434], mean action: 1.505 [0.000, 3.000],  loss: 33.262146, mse: 7383.620056, mean_q: 53.590379, mean_eps: 0.929428
  23676/300000: episode: 256, duration: 0.723s, episode steps: 100, steps per second: 138, episode reward: -120.722, mean reward: -1.207 [-100.000, 55.038], mean action: 1.520 [0.000, 3.000],  loss: 23.976295, mse: 8142.361665, mean_q: 57.125147, mean_eps: 0.929123
  23783/300000: episode: 257, duration: 0.771s, episode steps: 107, steps per second: 139, episode reward: -235.684, mean reward: -2.203 [-100.000, 20.211], mean action: 1.439 [0.000, 3.000],  loss: 20.747945, mse: 7983.536813, mean_q: 56.854697, mean_eps: 0.928813
  23868/300000: episode: 258, duration: 0.619s, episode steps:  85, steps per second: 137, episode reward: -131.936, mean reward: -1.552 [-100.000,  6.740], mean action: 1.706 [0.000, 3.000],  loss: 16.254486, mse: 8051.376649, mean_q: 57.021388, mean_eps: 0.928525
  24012/300000: episode: 259, duration: 0.957s, episode steps: 144, steps per second: 151, episode reward: -381.326, mean reward: -2.648 [-100.000, 124.642], mean action: 1.535 [0.000, 3.000],  loss: 21.519867, mse: 8224.048699, mean_q: 56.959127, mean_eps: 0.928182
  24110/300000: episode: 260, duration: 0.664s, episode steps:  98, steps per second: 148, episode reward: -191.535, mean reward: -1.954 [-100.000, 17.907], mean action: 1.592 [0.000, 3.000],  loss: 41.095770, mse: 8222.568728, mean_q: 57.729439, mean_eps: 0.927818
  24192/300000: episode: 261, duration: 0.520s, episode steps:  82, steps per second: 158, episode reward: -91.060, mean reward: -1.110 [-100.000,  7.604], mean action: 1.317 [0.000, 3.000],  loss: 18.801924, mse: 8597.750095, mean_q: 58.727783, mean_eps: 0.927548
  24287/300000: episode: 262, duration: 0.642s, episode steps:  95, steps per second: 148, episode reward: -123.026, mean reward: -1.295 [-100.000,  8.190], mean action: 1.600 [0.000, 3.000],  loss: 36.986880, mse: 8378.855160, mean_q: 57.106665, mean_eps: 0.927283
  24370/300000: episode: 263, duration: 0.590s, episode steps:  83, steps per second: 141, episode reward: -206.778, mean reward: -2.491 [-100.000, 17.519], mean action: 1.639 [0.000, 3.000],  loss: 33.952027, mse: 8353.946595, mean_q: 57.865286, mean_eps: 0.927016
  24446/300000: episode: 264, duration: 0.530s, episode steps:  76, steps per second: 143, episode reward: -87.017, mean reward: -1.145 [-100.000, 10.055], mean action: 1.526 [0.000, 3.000],  loss: 29.039765, mse: 8568.612754, mean_q: 58.374598, mean_eps: 0.926778
  24538/300000: episode: 265, duration: 0.642s, episode steps:  92, steps per second: 143, episode reward: -127.800, mean reward: -1.389 [-100.000, 11.734], mean action: 1.620 [0.000, 3.000],  loss: 37.007500, mse: 8382.417401, mean_q: 59.603096, mean_eps: 0.926526
  24607/300000: episode: 266, duration: 0.524s, episode steps:  69, steps per second: 132, episode reward: -104.040, mean reward: -1.508 [-100.000,  7.921], mean action: 1.406 [0.000, 3.000],  loss: 32.387050, mse: 8498.869034, mean_q: 59.384926, mean_eps: 0.926284
  24718/300000: episode: 267, duration: 0.706s, episode steps: 111, steps per second: 157, episode reward: -163.480, mean reward: -1.473 [-100.000, 20.733], mean action: 1.523 [0.000, 3.000],  loss: 38.379467, mse: 9045.489482, mean_q: 62.527572, mean_eps: 0.926014
  24786/300000: episode: 268, duration: 0.422s, episode steps:  68, steps per second: 161, episode reward: -67.765, mean reward: -0.997 [-100.000, 13.105], mean action: 1.750 [0.000, 3.000],  loss: 22.032785, mse: 8725.355742, mean_q: 61.259518, mean_eps: 0.925746
  24919/300000: episode: 269, duration: 0.881s, episode steps: 133, steps per second: 151, episode reward: -345.114, mean reward: -2.595 [-100.000, 80.637], mean action: 1.692 [0.000, 3.000],  loss: 40.004388, mse: 9147.079204, mean_q: 63.398791, mean_eps: 0.925444
  25003/300000: episode: 270, duration: 0.535s, episode steps:  84, steps per second: 157, episode reward:  2.648, mean reward:  0.032 [-100.000, 93.062], mean action: 1.536 [0.000, 3.000],  loss: 21.515162, mse: 8949.865380, mean_q: 64.090303, mean_eps: 0.925119
  25083/300000: episode: 271, duration: 0.499s, episode steps:  80, steps per second: 160, episode reward: -80.597, mean reward: -1.007 [-100.000,  8.157], mean action: 1.438 [0.000, 3.000],  loss: 23.712239, mse: 9286.989838, mean_q: 64.731465, mean_eps: 0.924872
  25187/300000: episode: 272, duration: 0.645s, episode steps: 104, steps per second: 161, episode reward: -127.237, mean reward: -1.223 [-100.000, 12.641], mean action: 1.625 [0.000, 3.000],  loss: 23.601634, mse: 9347.714759, mean_q: 65.745996, mean_eps: 0.924597
  25329/300000: episode: 273, duration: 0.928s, episode steps: 142, steps per second: 153, episode reward: -236.425, mean reward: -1.665 [-100.000, 71.357], mean action: 1.549 [0.000, 3.000],  loss: 26.815335, mse: 9430.642637, mean_q: 66.233927, mean_eps: 0.924228
  25421/300000: episode: 274, duration: 0.624s, episode steps:  92, steps per second: 148, episode reward: -108.057, mean reward: -1.175 [-100.000, 10.702], mean action: 1.522 [0.000, 3.000],  loss: 16.777144, mse: 9532.062182, mean_q: 68.042899, mean_eps: 0.923876
  25545/300000: episode: 275, duration: 0.785s, episode steps: 124, steps per second: 158, episode reward: -162.531, mean reward: -1.311 [-100.000,  6.089], mean action: 1.565 [0.000, 3.000],  loss: 20.544376, mse: 9810.462194, mean_q: 69.746204, mean_eps: 0.923552
  25646/300000: episode: 276, duration: 0.672s, episode steps: 101, steps per second: 150, episode reward: -119.585, mean reward: -1.184 [-100.000,  5.957], mean action: 1.426 [0.000, 3.000],  loss: 18.482460, mse: 10127.053919, mean_q: 71.596574, mean_eps: 0.923215
  25758/300000: episode: 277, duration: 0.693s, episode steps: 112, steps per second: 162, episode reward: -246.702, mean reward: -2.203 [-100.000,  7.145], mean action: 1.446 [0.000, 3.000],  loss: 19.385865, mse: 10278.626648, mean_q: 71.005591, mean_eps: 0.922895
  25841/300000: episode: 278, duration: 0.510s, episode steps:  83, steps per second: 163, episode reward: -148.809, mean reward: -1.793 [-100.000,  7.182], mean action: 1.386 [0.000, 3.000],  loss: 19.791583, mse: 10404.787756, mean_q: 71.991083, mean_eps: 0.922603
  25941/300000: episode: 279, duration: 0.669s, episode steps: 100, steps per second: 150, episode reward: -95.898, mean reward: -0.959 [-100.000,  6.697], mean action: 1.450 [0.000, 3.000],  loss: 23.214139, mse: 10220.990376, mean_q: 70.831409, mean_eps: 0.922328
  26037/300000: episode: 280, duration: 0.609s, episode steps:  96, steps per second: 158, episode reward: -129.420, mean reward: -1.348 [-100.000, 18.230], mean action: 1.646 [0.000, 3.000],  loss: 24.732976, mse: 10372.923218, mean_q: 73.380508, mean_eps: 0.922034
  26141/300000: episode: 281, duration: 0.639s, episode steps: 104, steps per second: 163, episode reward: -114.263, mean reward: -1.099 [-100.000, 11.706], mean action: 1.577 [0.000, 3.000],  loss: 18.694003, mse: 10599.297819, mean_q: 73.836965, mean_eps: 0.921734
  26217/300000: episode: 282, duration: 0.490s, episode steps:  76, steps per second: 155, episode reward: -113.663, mean reward: -1.496 [-100.000,  8.380], mean action: 1.868 [0.000, 3.000],  loss: 24.828471, mse: 10453.434300, mean_q: 72.170269, mean_eps: 0.921465
  26335/300000: episode: 283, duration: 0.758s, episode steps: 118, steps per second: 156, episode reward: -281.339, mean reward: -2.384 [-100.000, 10.156], mean action: 1.466 [0.000, 3.000],  loss: 15.985584, mse: 10799.634364, mean_q: 74.623736, mean_eps: 0.921173
  26422/300000: episode: 284, duration: 0.545s, episode steps:  87, steps per second: 160, episode reward: -195.641, mean reward: -2.249 [-100.000,  7.134], mean action: 1.540 [0.000, 3.000],  loss: 18.281256, mse: 11256.358937, mean_q: 76.498398, mean_eps: 0.920866
  26504/300000: episode: 285, duration: 0.505s, episode steps:  82, steps per second: 162, episode reward: -97.620, mean reward: -1.190 [-100.000,  8.764], mean action: 1.512 [0.000, 3.000],  loss: 24.873625, mse: 11552.769120, mean_q: 77.231619, mean_eps: 0.920612
  26613/300000: episode: 286, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -107.234, mean reward: -0.984 [-100.000, 15.838], mean action: 1.486 [0.000, 3.000],  loss: 25.223872, mse: 11482.218750, mean_q: 77.674816, mean_eps: 0.920326
  26690/300000: episode: 287, duration: 0.499s, episode steps:  77, steps per second: 154, episode reward: -96.569, mean reward: -1.254 [-100.000,  6.795], mean action: 1.701 [0.000, 3.000],  loss: 30.370291, mse: 11999.703093, mean_q: 79.394223, mean_eps: 0.920047
  26768/300000: episode: 288, duration: 0.490s, episode steps:  78, steps per second: 159, episode reward: -99.539, mean reward: -1.276 [-100.000,  8.815], mean action: 1.564 [0.000, 3.000],  loss: 33.479610, mse: 12035.293739, mean_q: 79.133774, mean_eps: 0.919814
  26887/300000: episode: 289, duration: 0.752s, episode steps: 119, steps per second: 158, episode reward: -161.668, mean reward: -1.359 [-100.000,  9.829], mean action: 1.555 [0.000, 3.000],  loss: 22.833986, mse: 11466.104340, mean_q: 75.783784, mean_eps: 0.919519
  26972/300000: episode: 290, duration: 0.601s, episode steps:  85, steps per second: 141, episode reward: -269.783, mean reward: -3.174 [-100.000, 21.403], mean action: 1.812 [0.000, 3.000],  loss: 23.070308, mse: 11827.096352, mean_q: 79.692876, mean_eps: 0.919213
  27092/300000: episode: 291, duration: 0.757s, episode steps: 120, steps per second: 159, episode reward: -131.492, mean reward: -1.096 [-100.000,  7.967], mean action: 1.508 [0.000, 3.000],  loss: 22.195249, mse: 11738.354480, mean_q: 77.171266, mean_eps: 0.918905
  27169/300000: episode: 292, duration: 0.481s, episode steps:  77, steps per second: 160, episode reward: -73.300, mean reward: -0.952 [-100.000,  6.839], mean action: 1.558 [0.000, 3.000],  loss: 27.814748, mse: 11793.218725, mean_q: 78.171770, mean_eps: 0.918610
  27239/300000: episode: 293, duration: 0.454s, episode steps:  70, steps per second: 154, episode reward: -129.339, mean reward: -1.848 [-100.000,  6.834], mean action: 1.500 [0.000, 3.000],  loss: 25.373935, mse: 11991.648647, mean_q: 78.323249, mean_eps: 0.918390
  27356/300000: episode: 294, duration: 0.750s, episode steps: 117, steps per second: 156, episode reward: -146.796, mean reward: -1.255 [-100.000,  7.682], mean action: 1.573 [0.000, 3.000],  loss: 16.020290, mse: 12269.220920, mean_q: 78.903154, mean_eps: 0.918109
  27475/300000: episode: 295, duration: 0.752s, episode steps: 119, steps per second: 158, episode reward: -77.666, mean reward: -0.653 [-100.000,  9.402], mean action: 1.613 [0.000, 3.000],  loss: 17.433216, mse: 12351.568478, mean_q: 78.316739, mean_eps: 0.917755
  27581/300000: episode: 296, duration: 0.733s, episode steps: 106, steps per second: 145, episode reward: -235.223, mean reward: -2.219 [-100.000,  4.417], mean action: 1.443 [0.000, 3.000],  loss: 15.339417, mse: 12195.135857, mean_q: 78.151906, mean_eps: 0.917417
  27695/300000: episode: 297, duration: 0.767s, episode steps: 114, steps per second: 149, episode reward: -117.930, mean reward: -1.034 [-100.000,  7.308], mean action: 1.509 [0.000, 3.000],  loss: 14.342192, mse: 12841.028475, mean_q: 80.259179, mean_eps: 0.917087
  27788/300000: episode: 298, duration: 0.597s, episode steps:  93, steps per second: 156, episode reward: -102.428, mean reward: -1.101 [-100.000,  7.223], mean action: 1.538 [0.000, 3.000],  loss: 20.450993, mse: 13240.989930, mean_q: 81.238964, mean_eps: 0.916777
  27868/300000: episode: 299, duration: 0.497s, episode steps:  80, steps per second: 161, episode reward: -128.008, mean reward: -1.600 [-100.000, 10.084], mean action: 1.538 [0.000, 3.000],  loss: 15.540154, mse: 13504.029260, mean_q: 80.954905, mean_eps: 0.916518
  27937/300000: episode: 300, duration: 0.459s, episode steps:  69, steps per second: 150, episode reward: -88.826, mean reward: -1.287 [-100.000, 11.129], mean action: 1.348 [0.000, 3.000],  loss: 18.819900, mse: 13622.781788, mean_q: 81.718786, mean_eps: 0.916294
  28019/300000: episode: 301, duration: 0.572s, episode steps:  82, steps per second: 143, episode reward: -81.051, mean reward: -0.988 [-100.000,  7.534], mean action: 1.476 [0.000, 3.000],  loss: 14.564534, mse: 13951.973663, mean_q: 81.486349, mean_eps: 0.916068
  28079/300000: episode: 302, duration: 0.501s, episode steps:  60, steps per second: 120, episode reward: -86.929, mean reward: -1.449 [-100.000, 24.388], mean action: 1.567 [0.000, 3.000],  loss: 15.858338, mse: 14063.250749, mean_q: 82.142889, mean_eps: 0.915854
  28155/300000: episode: 303, duration: 0.558s, episode steps:  76, steps per second: 136, episode reward: -99.130, mean reward: -1.304 [-100.000, 11.760], mean action: 1.618 [0.000, 3.000],  loss: 14.383052, mse: 13420.241841, mean_q: 78.785778, mean_eps: 0.915651
  28256/300000: episode: 304, duration: 0.989s, episode steps: 101, steps per second: 102, episode reward: -96.194, mean reward: -0.952 [-100.000,  7.279], mean action: 1.376 [0.000, 3.000],  loss: 26.990414, mse: 13344.368111, mean_q: 78.771583, mean_eps: 0.915385
  28316/300000: episode: 305, duration: 0.502s, episode steps:  60, steps per second: 120, episode reward: -67.939, mean reward: -1.132 [-100.000,  7.733], mean action: 1.417 [0.000, 3.000],  loss: 16.056105, mse: 14708.282422, mean_q: 84.504817, mean_eps: 0.915143
  28413/300000: episode: 306, duration: 0.783s, episode steps:  97, steps per second: 124, episode reward: -374.371, mean reward: -3.859 [-100.000,  0.811], mean action: 1.515 [0.000, 3.000],  loss: 22.990368, mse: 13791.144209, mean_q: 80.128142, mean_eps: 0.914908
  28573/300000: episode: 307, duration: 1.280s, episode steps: 160, steps per second: 125, episode reward: -95.944, mean reward: -0.600 [-100.000,  8.022], mean action: 1.569 [0.000, 3.000],  loss: 38.686676, mse: 13918.501672, mean_q: 78.830545, mean_eps: 0.914523
  28674/300000: episode: 308, duration: 0.787s, episode steps: 101, steps per second: 128, episode reward: -255.444, mean reward: -2.529 [-100.000, 70.348], mean action: 1.446 [0.000, 3.000],  loss: 22.426108, mse: 13472.590550, mean_q: 77.831012, mean_eps: 0.914131
  28754/300000: episode: 309, duration: 0.602s, episode steps:  80, steps per second: 133, episode reward: -125.637, mean reward: -1.570 [-100.000,  6.320], mean action: 1.475 [0.000, 3.000],  loss: 40.420695, mse: 13623.690393, mean_q: 77.736794, mean_eps: 0.913859
  28844/300000: episode: 310, duration: 0.633s, episode steps:  90, steps per second: 142, episode reward: -393.551, mean reward: -4.373 [-100.000, -0.074], mean action: 1.400 [0.000, 3.000],  loss: 44.548939, mse: 13777.339605, mean_q: 79.845249, mean_eps: 0.913604
  28941/300000: episode: 311, duration: 0.636s, episode steps:  97, steps per second: 152, episode reward: -135.475, mean reward: -1.397 [-100.000, 13.007], mean action: 1.680 [0.000, 3.000],  loss: 26.775091, mse: 13836.691316, mean_q: 79.623871, mean_eps: 0.913324
  29064/300000: episode: 312, duration: 0.853s, episode steps: 123, steps per second: 144, episode reward: -167.864, mean reward: -1.365 [-100.000,  9.373], mean action: 1.398 [0.000, 3.000],  loss: 25.455387, mse: 14149.365953, mean_q: 80.757300, mean_eps: 0.912994
  29166/300000: episode: 313, duration: 0.735s, episode steps: 102, steps per second: 139, episode reward: -101.955, mean reward: -1.000 [-100.000, 10.237], mean action: 1.559 [0.000, 3.000],  loss: 29.914757, mse: 14146.849265, mean_q: 80.296904, mean_eps: 0.912656
  29259/300000: episode: 314, duration: 0.651s, episode steps:  93, steps per second: 143, episode reward: -99.654, mean reward: -1.072 [-100.000, 10.457], mean action: 1.538 [0.000, 3.000],  loss: 30.332663, mse: 14915.201214, mean_q: 82.967425, mean_eps: 0.912364
  29332/300000: episode: 315, duration: 0.509s, episode steps:  73, steps per second: 143, episode reward: -210.451, mean reward: -2.883 [-100.000, 18.413], mean action: 1.479 [0.000, 3.000],  loss: 28.701680, mse: 14987.777986, mean_q: 84.781061, mean_eps: 0.912115
  29408/300000: episode: 316, duration: 0.501s, episode steps:  76, steps per second: 152, episode reward: -153.894, mean reward: -2.025 [-100.000, 18.342], mean action: 1.592 [0.000, 3.000],  loss: 35.373359, mse: 15136.996569, mean_q: 86.080089, mean_eps: 0.911891
  29537/300000: episode: 317, duration: 0.820s, episode steps: 129, steps per second: 157, episode reward: -147.284, mean reward: -1.142 [-100.000,  5.505], mean action: 1.705 [0.000, 3.000],  loss: 27.226356, mse: 15577.370155, mean_q: 87.063322, mean_eps: 0.911584
  29607/300000: episode: 318, duration: 0.456s, episode steps:  70, steps per second: 153, episode reward: -88.428, mean reward: -1.263 [-100.000,  8.234], mean action: 1.514 [0.000, 3.000],  loss: 18.933706, mse: 16069.556306, mean_q: 86.433472, mean_eps: 0.911286
  29685/300000: episode: 319, duration: 0.547s, episode steps:  78, steps per second: 143, episode reward: -220.740, mean reward: -2.830 [-100.000,  5.760], mean action: 1.603 [0.000, 3.000],  loss: 29.835423, mse: 15895.998397, mean_q: 84.927798, mean_eps: 0.911063
  29827/300000: episode: 320, duration: 0.933s, episode steps: 142, steps per second: 152, episode reward: -155.505, mean reward: -1.095 [-100.000,  7.144], mean action: 1.606 [0.000, 3.000],  loss: 15.312342, mse: 16434.834259, mean_q: 88.731100, mean_eps: 0.910734
  29924/300000: episode: 321, duration: 0.621s, episode steps:  97, steps per second: 156, episode reward: -99.379, mean reward: -1.025 [-100.000,  7.992], mean action: 1.485 [0.000, 3.000],  loss: 20.787284, mse: 17465.295486, mean_q: 92.479293, mean_eps: 0.910375
  29995/300000: episode: 322, duration: 0.476s, episode steps:  71, steps per second: 149, episode reward: -72.982, mean reward: -1.028 [-100.000,  8.232], mean action: 1.535 [0.000, 3.000],  loss: 19.273955, mse: 17408.140680, mean_q: 91.492325, mean_eps: 0.910123
  30080/300000: episode: 323, duration: 0.553s, episode steps:  85, steps per second: 154, episode reward: -46.541, mean reward: -0.548 [-100.000, 116.755], mean action: 1.565 [0.000, 3.000],  loss: 13.641902, mse: 17551.944072, mean_q: 91.511029, mean_eps: 0.909889
  30182/300000: episode: 324, duration: 0.640s, episode steps: 102, steps per second: 159, episode reward: -306.925, mean reward: -3.009 [-100.000,  0.501], mean action: 1.422 [0.000, 3.000],  loss: 17.342621, mse: 18130.283423, mean_q: 92.372748, mean_eps: 0.909609
  30254/300000: episode: 325, duration: 0.461s, episode steps:  72, steps per second: 156, episode reward: -52.312, mean reward: -0.727 [-100.000,  7.234], mean action: 1.667 [0.000, 3.000],  loss: 21.731634, mse: 18364.518460, mean_q: 97.264897, mean_eps: 0.909348
  30333/300000: episode: 326, duration: 0.539s, episode steps:  79, steps per second: 146, episode reward: -110.786, mean reward: -1.402 [-100.000,  5.041], mean action: 1.557 [0.000, 3.000],  loss: 15.507800, mse: 17864.853330, mean_q: 92.523118, mean_eps: 0.909121
  30402/300000: episode: 327, duration: 0.461s, episode steps:  69, steps per second: 150, episode reward: -103.877, mean reward: -1.505 [-100.000,  9.610], mean action: 1.449 [0.000, 3.000],  loss: 22.470592, mse: 19114.703111, mean_q: 96.691407, mean_eps: 0.908899
  30469/300000: episode: 328, duration: 0.424s, episode steps:  67, steps per second: 158, episode reward: -71.138, mean reward: -1.062 [-100.000,  6.576], mean action: 1.493 [0.000, 3.000],  loss: 18.354276, mse: 19074.293552, mean_q: 96.685516, mean_eps: 0.908695
  30550/300000: episode: 329, duration: 0.522s, episode steps:  81, steps per second: 155, episode reward: -83.297, mean reward: -1.028 [-100.000,  8.867], mean action: 1.765 [0.000, 3.000],  loss: 23.963304, mse: 19071.201630, mean_q: 94.999973, mean_eps: 0.908473
  30651/300000: episode: 330, duration: 0.658s, episode steps: 101, steps per second: 153, episode reward: -97.627, mean reward: -0.967 [-100.000, 11.229], mean action: 1.396 [0.000, 3.000],  loss: 25.831258, mse: 19813.880743, mean_q: 98.443604, mean_eps: 0.908200
  30768/300000: episode: 331, duration: 0.769s, episode steps: 117, steps per second: 152, episode reward: -104.994, mean reward: -0.897 [-100.000, 23.446], mean action: 1.453 [0.000, 3.000],  loss: 29.223566, mse: 19769.316231, mean_q: 96.965580, mean_eps: 0.907873
  30856/300000: episode: 332, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -82.936, mean reward: -0.942 [-100.000, 15.564], mean action: 1.648 [0.000, 3.000],  loss: 34.994913, mse: 20277.680831, mean_q: 96.284852, mean_eps: 0.907566
  30980/300000: episode: 333, duration: 0.801s, episode steps: 124, steps per second: 155, episode reward: -188.950, mean reward: -1.524 [-100.000,  9.277], mean action: 1.476 [0.000, 3.000],  loss: 31.440223, mse: 18590.946911, mean_q: 92.312212, mean_eps: 0.907247
  31100/300000: episode: 334, duration: 0.790s, episode steps: 120, steps per second: 152, episode reward: -109.740, mean reward: -0.914 [-100.000,  9.708], mean action: 1.508 [0.000, 3.000],  loss: 18.858733, mse: 21063.774154, mean_q: 98.139317, mean_eps: 0.906882
  31194/300000: episode: 335, duration: 0.595s, episode steps:  94, steps per second: 158, episode reward: -246.391, mean reward: -2.621 [-100.000,  5.217], mean action: 1.447 [0.000, 3.000],  loss: 23.464486, mse: 19571.039291, mean_q: 91.878301, mean_eps: 0.906561
  31287/300000: episode: 336, duration: 0.623s, episode steps:  93, steps per second: 149, episode reward: -79.532, mean reward: -0.855 [-100.000, 10.065], mean action: 1.731 [0.000, 3.000],  loss: 38.620963, mse: 19202.315472, mean_q: 92.176284, mean_eps: 0.906280
  31400/300000: episode: 337, duration: 0.927s, episode steps: 113, steps per second: 122, episode reward: -321.942, mean reward: -2.849 [-100.000, 108.023], mean action: 1.442 [0.000, 3.000],  loss: 37.529611, mse: 19612.317495, mean_q: 92.905501, mean_eps: 0.905971
  31475/300000: episode: 338, duration: 0.542s, episode steps:  75, steps per second: 138, episode reward: -98.894, mean reward: -1.319 [-100.000, 14.145], mean action: 1.800 [0.000, 3.000],  loss: 20.791632, mse: 19759.430560, mean_q: 95.831735, mean_eps: 0.905689
  31591/300000: episode: 339, duration: 0.890s, episode steps: 116, steps per second: 130, episode reward: -255.886, mean reward: -2.206 [-100.000,  1.335], mean action: 1.629 [0.000, 3.000],  loss: 34.773279, mse: 22314.086089, mean_q: 101.486446, mean_eps: 0.905402
  31706/300000: episode: 340, duration: 0.829s, episode steps: 115, steps per second: 139, episode reward: -82.187, mean reward: -0.715 [-100.000, 11.811], mean action: 1.626 [0.000, 3.000],  loss: 18.199650, mse: 21950.796221, mean_q: 99.515178, mean_eps: 0.905056
  31784/300000: episode: 341, duration: 0.557s, episode steps:  78, steps per second: 140, episode reward: -23.712, mean reward: -0.304 [-100.000, 17.114], mean action: 1.692 [0.000, 3.000],  loss: 39.633686, mse: 24160.409781, mean_q: 103.564829, mean_eps: 0.904767
  31864/300000: episode: 342, duration: 0.581s, episode steps:  80, steps per second: 138, episode reward: -47.166, mean reward: -0.590 [-100.000, 109.717], mean action: 1.375 [0.000, 3.000],  loss: 24.512142, mse: 21649.966614, mean_q: 97.637357, mean_eps: 0.904529
  31992/300000: episode: 343, duration: 0.913s, episode steps: 128, steps per second: 140, episode reward: -92.980, mean reward: -0.726 [-100.000, 13.125], mean action: 1.555 [0.000, 3.000],  loss: 41.139372, mse: 23192.674286, mean_q: 100.439820, mean_eps: 0.904218
  32104/300000: episode: 344, duration: 0.777s, episode steps: 112, steps per second: 144, episode reward: -367.764, mean reward: -3.284 [-100.000,  1.516], mean action: 1.429 [0.000, 3.000],  loss: 34.658030, mse: 24136.085981, mean_q: 103.060903, mean_eps: 0.903857
  32199/300000: episode: 345, duration: 0.638s, episode steps:  95, steps per second: 149, episode reward: -146.644, mean reward: -1.544 [-100.000, 12.116], mean action: 1.684 [0.000, 3.000],  loss: 21.099079, mse: 24456.531116, mean_q: 103.537258, mean_eps: 0.903547
  32271/300000: episode: 346, duration: 0.474s, episode steps:  72, steps per second: 152, episode reward: -68.585, mean reward: -0.953 [-100.000, 10.246], mean action: 1.500 [0.000, 3.000],  loss: 38.450637, mse: 24620.773410, mean_q: 103.965000, mean_eps: 0.903296
  32395/300000: episode: 347, duration: 0.786s, episode steps: 124, steps per second: 158, episode reward: -230.810, mean reward: -1.861 [-100.000,  8.195], mean action: 1.435 [0.000, 3.000],  loss: 35.105846, mse: 26057.711221, mean_q: 104.491446, mean_eps: 0.903002
  32576/300000: episode: 348, duration: 1.187s, episode steps: 181, steps per second: 153, episode reward: -158.694, mean reward: -0.877 [-100.000, 10.049], mean action: 1.597 [0.000, 3.000],  loss: 23.435401, mse: 26523.049233, mean_q: 107.256594, mean_eps: 0.902545
  32664/300000: episode: 349, duration: 0.567s, episode steps:  88, steps per second: 155, episode reward: -121.632, mean reward: -1.382 [-100.000,  7.390], mean action: 1.580 [0.000, 3.000],  loss: 20.551315, mse: 28582.998746, mean_q: 112.990117, mean_eps: 0.902142
  32778/300000: episode: 350, duration: 0.731s, episode steps: 114, steps per second: 156, episode reward: -131.692, mean reward: -1.155 [-100.000, 12.783], mean action: 1.570 [0.000, 3.000],  loss: 45.040960, mse: 27788.061498, mean_q: 109.471830, mean_eps: 0.901838
  32861/300000: episode: 351, duration: 0.552s, episode steps:  83, steps per second: 150, episode reward: -115.404, mean reward: -1.390 [-100.000, 12.090], mean action: 1.663 [0.000, 3.000],  loss: 24.036651, mse: 29593.578996, mean_q: 114.616986, mean_eps: 0.901543
  33001/300000: episode: 352, duration: 1.024s, episode steps: 140, steps per second: 137, episode reward: -128.985, mean reward: -0.921 [-100.000, 14.485], mean action: 1.600 [0.000, 3.000],  loss: 35.535253, mse: 30251.358573, mean_q: 115.357012, mean_eps: 0.901208
  33095/300000: episode: 353, duration: 0.650s, episode steps:  94, steps per second: 145, episode reward: -88.391, mean reward: -0.940 [-100.000, 19.701], mean action: 1.468 [0.000, 3.000],  loss: 25.837792, mse: 30880.800293, mean_q: 118.482314, mean_eps: 0.900857
  33183/300000: episode: 354, duration: 0.670s, episode steps:  88, steps per second: 131, episode reward: -75.143, mean reward: -0.854 [-100.000,  6.310], mean action: 1.591 [0.000, 3.000],  loss: 45.358230, mse: 33917.739502, mean_q: 122.451862, mean_eps: 0.900585
  33317/300000: episode: 355, duration: 1.105s, episode steps: 134, steps per second: 121, episode reward: -116.173, mean reward: -0.867 [-100.000, 24.223], mean action: 1.433 [0.000, 3.000],  loss: 46.448870, mse: 32226.475498, mean_q: 121.609636, mean_eps: 0.900251
  33421/300000: episode: 356, duration: 0.837s, episode steps: 104, steps per second: 124, episode reward: -162.630, mean reward: -1.564 [-100.000,  8.765], mean action: 1.587 [0.000, 3.000],  loss: 56.698022, mse: 33994.446843, mean_q: 125.238649, mean_eps: 0.899895
  33509/300000: episode: 357, duration: 0.610s, episode steps:  88, steps per second: 144, episode reward: -144.107, mean reward: -1.638 [-100.000,  7.173], mean action: 1.727 [0.000, 3.000],  loss: 67.557968, mse: 35178.604226, mean_q: 126.972803, mean_eps: 0.899606
  33619/300000: episode: 358, duration: 0.708s, episode steps: 110, steps per second: 155, episode reward: -328.147, mean reward: -2.983 [-100.000, 95.530], mean action: 1.382 [0.000, 3.000],  loss: 33.309662, mse: 36478.925213, mean_q: 128.740517, mean_eps: 0.899309
  33699/300000: episode: 359, duration: 0.531s, episode steps:  80, steps per second: 151, episode reward: -409.399, mean reward: -5.117 [-100.000,  0.129], mean action: 1.575 [0.000, 3.000],  loss: 32.826769, mse: 37856.965906, mean_q: 129.079424, mean_eps: 0.899025
  33806/300000: episode: 360, duration: 0.744s, episode steps: 107, steps per second: 144, episode reward: -349.501, mean reward: -3.266 [-100.000,  5.771], mean action: 1.449 [0.000, 3.000],  loss: 38.228753, mse: 37973.952413, mean_q: 131.151185, mean_eps: 0.898744
  33931/300000: episode: 361, duration: 0.801s, episode steps: 125, steps per second: 156, episode reward: -223.870, mean reward: -1.791 [-100.000,  1.348], mean action: 1.528 [0.000, 3.000],  loss: 31.152471, mse: 40779.285461, mean_q: 138.572198, mean_eps: 0.898396
  34016/300000: episode: 362, duration: 0.544s, episode steps:  85, steps per second: 156, episode reward: 39.682, mean reward:  0.467 [-100.000, 84.938], mean action: 1.624 [0.000, 3.000],  loss: 36.576641, mse: 43173.822898, mean_q: 143.634707, mean_eps: 0.898081
  34124/300000: episode: 363, duration: 0.730s, episode steps: 108, steps per second: 148, episode reward: -87.458, mean reward: -0.810 [-100.000, 10.362], mean action: 1.528 [0.000, 3.000],  loss: 57.179710, mse: 40703.831046, mean_q: 136.347645, mean_eps: 0.897792
  34215/300000: episode: 364, duration: 0.596s, episode steps:  91, steps per second: 153, episode reward: -133.450, mean reward: -1.466 [-100.000,  6.373], mean action: 1.516 [0.000, 3.000],  loss: 61.701981, mse: 41157.466153, mean_q: 137.762434, mean_eps: 0.897493
  34283/300000: episode: 365, duration: 0.430s, episode steps:  68, steps per second: 158, episode reward: -102.313, mean reward: -1.505 [-100.000, 22.229], mean action: 1.456 [0.000, 3.000],  loss: 39.265518, mse: 40499.057158, mean_q: 138.410587, mean_eps: 0.897254
  34380/300000: episode: 366, duration: 0.615s, episode steps:  97, steps per second: 158, episode reward: -134.080, mean reward: -1.382 [-100.000,  4.855], mean action: 1.443 [0.000, 3.000],  loss: 38.280880, mse: 42591.953024, mean_q: 141.256586, mean_eps: 0.897007
  34449/300000: episode: 367, duration: 0.476s, episode steps:  69, steps per second: 145, episode reward: -113.249, mean reward: -1.641 [-100.000, 13.455], mean action: 1.609 [0.000, 3.000],  loss: 29.782294, mse: 41624.949332, mean_q: 137.851546, mean_eps: 0.896758
  34515/300000: episode: 368, duration: 0.460s, episode steps:  66, steps per second: 143, episode reward: -123.653, mean reward: -1.874 [-100.000, 10.113], mean action: 1.273 [0.000, 3.000],  loss: 23.949199, mse: 41934.353752, mean_q: 140.168453, mean_eps: 0.896556
  34599/300000: episode: 369, duration: 0.556s, episode steps:  84, steps per second: 151, episode reward: -235.395, mean reward: -2.802 [-100.000,  4.796], mean action: 1.488 [0.000, 3.000],  loss: 52.000187, mse: 43585.555594, mean_q: 145.700025, mean_eps: 0.896331
  34681/300000: episode: 370, duration: 0.527s, episode steps:  82, steps per second: 156, episode reward: -140.643, mean reward: -1.715 [-100.000,  5.358], mean action: 1.622 [0.000, 3.000],  loss: 20.526764, mse: 45431.081317, mean_q: 145.280282, mean_eps: 0.896081
  34777/300000: episode: 371, duration: 0.660s, episode steps:  96, steps per second: 145, episode reward: -93.074, mean reward: -0.970 [-100.000, 21.920], mean action: 1.531 [0.000, 3.000],  loss: 48.525215, mse: 45809.805766, mean_q: 145.809512, mean_eps: 0.895814
  34856/300000: episode: 372, duration: 0.560s, episode steps:  79, steps per second: 141, episode reward: -108.518, mean reward: -1.374 [-100.000, 32.077], mean action: 1.608 [0.000, 3.000],  loss: 25.517010, mse: 42823.797271, mean_q: 138.618653, mean_eps: 0.895552
  34985/300000: episode: 373, duration: 0.977s, episode steps: 129, steps per second: 132, episode reward: -149.189, mean reward: -1.157 [-100.000, 25.692], mean action: 1.612 [0.000, 3.000],  loss: 55.098195, mse: 47096.480893, mean_q: 147.709234, mean_eps: 0.895240
  35142/300000: episode: 374, duration: 1.132s, episode steps: 157, steps per second: 139, episode reward: -202.074, mean reward: -1.287 [-100.000,  5.972], mean action: 1.478 [0.000, 3.000],  loss: 46.592913, mse: 49541.476339, mean_q: 153.775104, mean_eps: 0.894811
  35211/300000: episode: 375, duration: 0.530s, episode steps:  69, steps per second: 130, episode reward: -91.533, mean reward: -1.327 [-100.000,  6.215], mean action: 1.594 [0.000, 3.000],  loss: 52.648590, mse: 50944.957682, mean_q: 157.950201, mean_eps: 0.894472
  35280/300000: episode: 376, duration: 0.534s, episode steps:  69, steps per second: 129, episode reward: -145.460, mean reward: -2.108 [-100.000, 70.077], mean action: 1.609 [0.000, 3.000],  loss: 60.587105, mse: 48914.685547, mean_q: 153.098996, mean_eps: 0.894265
  35359/300000: episode: 377, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -90.059, mean reward: -1.140 [-100.000, 12.394], mean action: 1.443 [0.000, 3.000],  loss: 52.976195, mse: 53817.654173, mean_q: 163.075565, mean_eps: 0.894043
  35430/300000: episode: 378, duration: 0.477s, episode steps:  71, steps per second: 149, episode reward: -162.914, mean reward: -2.295 [-100.000, 15.756], mean action: 1.577 [0.000, 3.000],  loss: 39.940080, mse: 52001.042364, mean_q: 158.690373, mean_eps: 0.893818
  35491/300000: episode: 379, duration: 0.402s, episode steps:  61, steps per second: 152, episode reward: -101.198, mean reward: -1.659 [-100.000,  9.505], mean action: 1.459 [0.000, 3.000],  loss: 46.212337, mse: 53820.954694, mean_q: 164.513059, mean_eps: 0.893620
  35614/300000: episode: 380, duration: 0.836s, episode steps: 123, steps per second: 147, episode reward: -149.573, mean reward: -1.216 [-100.000,  6.133], mean action: 1.415 [0.000, 3.000],  loss: 42.787220, mse: 56125.678481, mean_q: 168.592429, mean_eps: 0.893344
  35714/300000: episode: 381, duration: 0.669s, episode steps: 100, steps per second: 149, episode reward: -356.767, mean reward: -3.568 [-100.000,  0.853], mean action: 1.430 [0.000, 3.000],  loss: 41.517921, mse: 59598.491992, mean_q: 173.411323, mean_eps: 0.893010
  35803/300000: episode: 382, duration: 0.576s, episode steps:  89, steps per second: 155, episode reward: -90.988, mean reward: -1.022 [-100.000, 17.898], mean action: 1.461 [0.000, 3.000],  loss: 46.461307, mse: 57569.285639, mean_q: 173.030391, mean_eps: 0.892726
  35877/300000: episode: 383, duration: 0.491s, episode steps:  74, steps per second: 151, episode reward: -178.570, mean reward: -2.413 [-100.000, 67.558], mean action: 1.635 [0.000, 3.000],  loss: 31.199193, mse: 59882.438345, mean_q: 175.537009, mean_eps: 0.892481
  35960/300000: episode: 384, duration: 0.548s, episode steps:  83, steps per second: 151, episode reward: -128.133, mean reward: -1.544 [-100.000,  7.072], mean action: 1.566 [0.000, 3.000],  loss: 33.940333, mse: 60484.819254, mean_q: 177.867703, mean_eps: 0.892246
  36073/300000: episode: 385, duration: 0.767s, episode steps: 113, steps per second: 147, episode reward: -85.368, mean reward: -0.755 [-100.000, 14.105], mean action: 1.575 [0.000, 3.000],  loss: 40.903302, mse: 61160.490908, mean_q: 180.698694, mean_eps: 0.891952
  36176/300000: episode: 386, duration: 0.670s, episode steps: 103, steps per second: 154, episode reward: -170.459, mean reward: -1.655 [-100.000,  7.759], mean action: 1.485 [0.000, 3.000],  loss: 44.121883, mse: 65201.114267, mean_q: 188.470213, mean_eps: 0.891628
  36263/300000: episode: 387, duration: 0.562s, episode steps:  87, steps per second: 155, episode reward: -46.414, mean reward: -0.533 [-100.000, 23.132], mean action: 1.494 [0.000, 3.000],  loss: 46.107904, mse: 64827.336566, mean_q: 188.672445, mean_eps: 0.891343
  36356/300000: episode: 388, duration: 0.638s, episode steps:  93, steps per second: 146, episode reward: -235.485, mean reward: -2.532 [-100.000,  5.053], mean action: 1.742 [0.000, 3.000],  loss: 33.178152, mse: 64071.823904, mean_q: 188.697615, mean_eps: 0.891073
  36472/300000: episode: 389, duration: 0.772s, episode steps: 116, steps per second: 150, episode reward: -124.638, mean reward: -1.074 [-100.000, 12.676], mean action: 1.509 [0.000, 3.000],  loss: 33.005918, mse: 66626.167497, mean_q: 191.652568, mean_eps: 0.890760
  36531/300000: episode: 390, duration: 0.397s, episode steps:  59, steps per second: 149, episode reward: -163.046, mean reward: -2.763 [-100.000,  5.785], mean action: 1.136 [0.000, 3.000],  loss: 30.040419, mse: 65253.300649, mean_q: 188.448034, mean_eps: 0.890497
  36668/300000: episode: 391, duration: 0.939s, episode steps: 137, steps per second: 146, episode reward: -163.752, mean reward: -1.195 [-100.000,  6.383], mean action: 1.518 [0.000, 3.000],  loss: 28.981113, mse: 70536.368784, mean_q: 196.447832, mean_eps: 0.890203
  36776/300000: episode: 392, duration: 0.693s, episode steps: 108, steps per second: 156, episode reward: -140.879, mean reward: -1.304 [-100.000,  7.913], mean action: 1.343 [0.000, 3.000],  loss: 49.399039, mse: 71874.568685, mean_q: 197.985846, mean_eps: 0.889836
  36865/300000: episode: 393, duration: 0.577s, episode steps:  89, steps per second: 154, episode reward: -115.089, mean reward: -1.293 [-100.000, 11.511], mean action: 1.483 [0.000, 3.000],  loss: 31.677954, mse: 72685.750746, mean_q: 196.706372, mean_eps: 0.889540
  36971/300000: episode: 394, duration: 0.731s, episode steps: 106, steps per second: 145, episode reward: -113.334, mean reward: -1.069 [-100.000,  8.351], mean action: 1.443 [0.000, 3.000],  loss: 35.205925, mse: 76812.082142, mean_q: 204.436239, mean_eps: 0.889247
  37073/300000: episode: 395, duration: 0.666s, episode steps: 102, steps per second: 153, episode reward: -93.200, mean reward: -0.914 [-100.000, 19.716], mean action: 1.637 [0.000, 3.000],  loss: 36.311601, mse: 76252.974992, mean_q: 204.774952, mean_eps: 0.888936
  37140/300000: episode: 396, duration: 0.440s, episode steps:  67, steps per second: 152, episode reward: -57.954, mean reward: -0.865 [-100.000, 10.279], mean action: 1.522 [0.000, 3.000],  loss: 34.577460, mse: 78026.310518, mean_q: 208.550421, mean_eps: 0.888682
  37239/300000: episode: 397, duration: 0.649s, episode steps:  99, steps per second: 153, episode reward: -84.578, mean reward: -0.854 [-100.000, 10.499], mean action: 1.444 [0.000, 3.000],  loss: 42.088995, mse: 81975.853180, mean_q: 213.243848, mean_eps: 0.888433
  37374/300000: episode: 398, duration: 0.923s, episode steps: 135, steps per second: 146, episode reward: -126.951, mean reward: -0.940 [-100.000, 11.983], mean action: 1.481 [0.000, 3.000],  loss: 37.376949, mse: 79811.081742, mean_q: 209.297126, mean_eps: 0.888082
  37510/300000: episode: 399, duration: 0.871s, episode steps: 136, steps per second: 156, episode reward: -239.018, mean reward: -1.757 [-100.000,  2.210], mean action: 1.471 [0.000, 3.000],  loss: 38.946024, mse: 84094.336828, mean_q: 215.984764, mean_eps: 0.887676
  37640/300000: episode: 400, duration: 0.901s, episode steps: 130, steps per second: 144, episode reward: -120.671, mean reward: -0.928 [-100.000, 16.778], mean action: 1.646 [0.000, 3.000],  loss: 61.706367, mse: 87920.137139, mean_q: 221.679603, mean_eps: 0.887277
  37711/300000: episode: 401, duration: 0.496s, episode steps:  71, steps per second: 143, episode reward: -95.559, mean reward: -1.346 [-100.000,  7.314], mean action: 1.282 [0.000, 3.000],  loss: 84.095819, mse: 84200.883308, mean_q: 213.890404, mean_eps: 0.886975
  37787/300000: episode: 402, duration: 0.603s, episode steps:  76, steps per second: 126, episode reward: -116.821, mean reward: -1.537 [-100.000, 17.776], mean action: 1.579 [0.000, 3.000],  loss: 59.768617, mse: 88357.669254, mean_q: 219.294106, mean_eps: 0.886754
  37889/300000: episode: 403, duration: 0.735s, episode steps: 102, steps per second: 139, episode reward: -117.623, mean reward: -1.153 [-100.000, 13.886], mean action: 1.520 [0.000, 3.000],  loss: 39.363899, mse: 91063.095818, mean_q: 224.999516, mean_eps: 0.886488
  37997/300000: episode: 404, duration: 0.803s, episode steps: 108, steps per second: 135, episode reward: -164.259, mean reward: -1.521 [-100.000,  8.621], mean action: 1.704 [0.000, 3.000],  loss: 41.943986, mse: 91973.013383, mean_q: 226.792555, mean_eps: 0.886173
  38105/300000: episode: 405, duration: 0.721s, episode steps: 108, steps per second: 150, episode reward: -124.430, mean reward: -1.152 [-100.000,  6.938], mean action: 1.546 [0.000, 3.000],  loss: 45.159842, mse: 93618.274776, mean_q: 226.963295, mean_eps: 0.885848
  38196/300000: episode: 406, duration: 0.634s, episode steps:  91, steps per second: 144, episode reward: -181.138, mean reward: -1.991 [-100.000, 33.540], mean action: 1.571 [0.000, 3.000],  loss: 33.287975, mse: 93761.243432, mean_q: 225.824856, mean_eps: 0.885550
  38290/300000: episode: 407, duration: 0.800s, episode steps:  94, steps per second: 117, episode reward: -28.620, mean reward: -0.304 [-100.000, 12.667], mean action: 1.553 [0.000, 3.000],  loss: 56.706407, mse: 97212.224318, mean_q: 232.159495, mean_eps: 0.885272
  38372/300000: episode: 408, duration: 0.563s, episode steps:  82, steps per second: 146, episode reward: -99.447, mean reward: -1.213 [-100.000,  7.398], mean action: 1.500 [0.000, 3.000],  loss: 43.858446, mse: 99117.179973, mean_q: 233.684329, mean_eps: 0.885008
  38471/300000: episode: 409, duration: 0.816s, episode steps:  99, steps per second: 121, episode reward: -163.483, mean reward: -1.651 [-100.000,  7.304], mean action: 1.717 [0.000, 3.000],  loss: 31.658301, mse: 98389.071772, mean_q: 228.426405, mean_eps: 0.884737
  38541/300000: episode: 410, duration: 0.512s, episode steps:  70, steps per second: 137, episode reward: -73.654, mean reward: -1.052 [-100.000, 12.578], mean action: 1.557 [0.000, 3.000],  loss: 39.799611, mse: 101629.808929, mean_q: 235.593568, mean_eps: 0.884483
  38625/300000: episode: 411, duration: 0.572s, episode steps:  84, steps per second: 147, episode reward: -210.166, mean reward: -2.502 [-100.000, 16.124], mean action: 1.548 [0.000, 3.000],  loss: 36.428236, mse: 102928.404390, mean_q: 237.493978, mean_eps: 0.884252
  38732/300000: episode: 412, duration: 0.837s, episode steps: 107, steps per second: 128, episode reward: -311.841, mean reward: -2.914 [-100.000,  6.917], mean action: 1.486 [0.000, 3.000],  loss: 40.073435, mse: 103274.776468, mean_q: 237.835491, mean_eps: 0.883966
  38835/300000: episode: 413, duration: 0.756s, episode steps: 103, steps per second: 136, episode reward: -326.310, mean reward: -3.168 [-100.000,  0.893], mean action: 1.515 [0.000, 3.000],  loss: 34.174510, mse: 108890.270631, mean_q: 242.974302, mean_eps: 0.883651
  38954/300000: episode: 414, duration: 0.842s, episode steps: 119, steps per second: 141, episode reward: -156.192, mean reward: -1.313 [-100.000,  6.460], mean action: 1.613 [0.000, 3.000],  loss: 51.288678, mse: 106020.260898, mean_q: 240.474804, mean_eps: 0.883318
  39031/300000: episode: 415, duration: 0.517s, episode steps:  77, steps per second: 149, episode reward: -105.991, mean reward: -1.377 [-100.000,  5.664], mean action: 1.558 [0.000, 3.000],  loss: 90.155004, mse: 110369.077110, mean_q: 246.551535, mean_eps: 0.883024
  39094/300000: episode: 416, duration: 0.456s, episode steps:  63, steps per second: 138, episode reward: -102.900, mean reward: -1.633 [-100.000,  9.073], mean action: 1.397 [0.000, 3.000],  loss: 39.194298, mse: 116302.209077, mean_q: 252.777638, mean_eps: 0.882814
  39181/300000: episode: 417, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -398.046, mean reward: -4.575 [-100.000,  0.494], mean action: 1.448 [0.000, 3.000],  loss: 52.723184, mse: 113033.651670, mean_q: 250.475698, mean_eps: 0.882589
  39296/300000: episode: 418, duration: 0.771s, episode steps: 115, steps per second: 149, episode reward: -104.711, mean reward: -0.911 [-100.000, 12.891], mean action: 1.496 [0.000, 3.000],  loss: 43.639675, mse: 110082.570992, mean_q: 245.838367, mean_eps: 0.882286
  39399/300000: episode: 419, duration: 0.962s, episode steps: 103, steps per second: 107, episode reward: -96.127, mean reward: -0.933 [-100.000,  7.673], mean action: 1.476 [0.000, 3.000],  loss: 38.304227, mse: 115125.587227, mean_q: 252.310622, mean_eps: 0.881959
  39523/300000: episode: 420, duration: 1.499s, episode steps: 124, steps per second:  83, episode reward: -146.824, mean reward: -1.184 [-100.000,  6.880], mean action: 1.492 [0.000, 3.000],  loss: 44.976870, mse: 116023.677923, mean_q: 252.962706, mean_eps: 0.881618
  39651/300000: episode: 421, duration: 1.540s, episode steps: 128, steps per second:  83, episode reward: -187.323, mean reward: -1.463 [-100.000,  3.046], mean action: 1.539 [0.000, 3.000],  loss: 32.942311, mse: 116374.452881, mean_q: 253.300252, mean_eps: 0.881240
  39765/300000: episode: 422, duration: 0.896s, episode steps: 114, steps per second: 127, episode reward: -112.492, mean reward: -0.987 [-100.000,  6.532], mean action: 1.412 [0.000, 3.000],  loss: 89.191830, mse: 121027.324596, mean_q: 261.082632, mean_eps: 0.880877
  39864/300000: episode: 423, duration: 0.898s, episode steps:  99, steps per second: 110, episode reward: -263.461, mean reward: -2.661 [-100.000, 25.513], mean action: 1.798 [0.000, 3.000],  loss: 46.406936, mse: 124989.811948, mean_q: 262.608152, mean_eps: 0.880558
  39977/300000: episode: 424, duration: 0.861s, episode steps: 113, steps per second: 131, episode reward: -173.900, mean reward: -1.539 [-100.000,  2.387], mean action: 1.637 [0.000, 3.000],  loss: 33.010753, mse: 123397.556070, mean_q: 264.974571, mean_eps: 0.880240
  40058/300000: episode: 425, duration: 0.622s, episode steps:  81, steps per second: 130, episode reward: -237.726, mean reward: -2.935 [-100.000, 16.653], mean action: 1.543 [0.000, 3.000],  loss: 40.996987, mse: 118973.828029, mean_q: 254.957355, mean_eps: 0.879949
  40170/300000: episode: 426, duration: 0.845s, episode steps: 112, steps per second: 133, episode reward: -320.149, mean reward: -2.858 [-100.000,  6.472], mean action: 1.527 [0.000, 3.000],  loss: 36.220691, mse: 124095.767299, mean_q: 263.101415, mean_eps: 0.879660
  40251/300000: episode: 427, duration: 0.642s, episode steps:  81, steps per second: 126, episode reward: -225.696, mean reward: -2.786 [-100.000, 49.770], mean action: 1.457 [0.000, 3.000],  loss: 38.152696, mse: 130801.751350, mean_q: 272.221860, mean_eps: 0.879370
  40352/300000: episode: 428, duration: 0.880s, episode steps: 101, steps per second: 115, episode reward: -118.284, mean reward: -1.171 [-100.000, 12.066], mean action: 1.455 [0.000, 3.000],  loss: 37.910631, mse: 130906.803527, mean_q: 269.013254, mean_eps: 0.879097
  40421/300000: episode: 429, duration: 0.584s, episode steps:  69, steps per second: 118, episode reward: -116.317, mean reward: -1.686 [-100.000,  5.502], mean action: 1.435 [0.000, 3.000],  loss: 34.294065, mse: 134734.584126, mean_q: 276.594229, mean_eps: 0.878842
  40540/300000: episode: 430, duration: 0.928s, episode steps: 119, steps per second: 128, episode reward: -144.400, mean reward: -1.213 [-100.000, 11.187], mean action: 1.538 [0.000, 3.000],  loss: 82.448855, mse: 131529.585412, mean_q: 268.949264, mean_eps: 0.878560
  40629/300000: episode: 431, duration: 0.708s, episode steps:  89, steps per second: 126, episode reward: -348.570, mean reward: -3.917 [-100.000,  0.591], mean action: 1.404 [0.000, 3.000],  loss: 70.580924, mse: 130995.335586, mean_q: 269.237321, mean_eps: 0.878248
  40698/300000: episode: 432, duration: 0.539s, episode steps:  69, steps per second: 128, episode reward: -126.667, mean reward: -1.836 [-100.000,  6.619], mean action: 1.348 [0.000, 3.000],  loss: 42.721009, mse: 129863.483243, mean_q: 269.281213, mean_eps: 0.878011
  40764/300000: episode: 433, duration: 0.468s, episode steps:  66, steps per second: 141, episode reward: -99.417, mean reward: -1.506 [-100.000, 57.007], mean action: 1.545 [0.000, 3.000],  loss: 37.836961, mse: 130940.825758, mean_q: 267.101585, mean_eps: 0.877808
  40893/300000: episode: 434, duration: 0.971s, episode steps: 129, steps per second: 133, episode reward: -231.179, mean reward: -1.792 [-100.000,  1.586], mean action: 1.705 [0.000, 3.000],  loss: 59.668825, mse: 134654.228289, mean_q: 272.264470, mean_eps: 0.877516
  41036/300000: episode: 435, duration: 1.527s, episode steps: 143, steps per second:  94, episode reward: -114.966, mean reward: -0.804 [-100.000, 12.042], mean action: 1.566 [0.000, 3.000],  loss: 53.658787, mse: 135118.430125, mean_q: 272.403587, mean_eps: 0.877108
  41165/300000: episode: 436, duration: 1.240s, episode steps: 129, steps per second: 104, episode reward: -13.781, mean reward: -0.107 [-100.000, 68.770], mean action: 1.682 [0.000, 3.000],  loss: 68.880139, mse: 138262.379360, mean_q: 274.224074, mean_eps: 0.876700
  41305/300000: episode: 437, duration: 1.224s, episode steps: 140, steps per second: 114, episode reward: -172.302, mean reward: -1.231 [-100.000,  8.898], mean action: 1.543 [0.000, 3.000],  loss: 39.587077, mse: 143109.436998, mean_q: 282.688123, mean_eps: 0.876296
  41400/300000: episode: 438, duration: 0.822s, episode steps:  95, steps per second: 116, episode reward: -88.430, mean reward: -0.931 [-100.000, 13.382], mean action: 1.705 [0.000, 3.000],  loss: 35.476021, mse: 144028.472944, mean_q: 285.180671, mean_eps: 0.875944
  41527/300000: episode: 439, duration: 0.876s, episode steps: 127, steps per second: 145, episode reward: -124.487, mean reward: -0.980 [-100.000, 25.057], mean action: 1.480 [0.000, 3.000],  loss: 58.434677, mse: 143933.930979, mean_q: 282.396950, mean_eps: 0.875611
  41636/300000: episode: 440, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -244.134, mean reward: -2.240 [-100.000,  0.607], mean action: 1.413 [0.000, 3.000],  loss: 41.788916, mse: 147183.703412, mean_q: 286.517394, mean_eps: 0.875257
  41779/300000: episode: 441, duration: 0.977s, episode steps: 143, steps per second: 146, episode reward: -207.380, mean reward: -1.450 [-100.000,  4.711], mean action: 1.538 [0.000, 3.000],  loss: 44.100411, mse: 148731.586593, mean_q: 291.026604, mean_eps: 0.874879
  41852/300000: episode: 442, duration: 0.479s, episode steps:  73, steps per second: 152, episode reward: -184.239, mean reward: -2.524 [-100.000, 14.154], mean action: 1.603 [0.000, 3.000],  loss: 37.638901, mse: 156170.930651, mean_q: 300.863700, mean_eps: 0.874555
  41962/300000: episode: 443, duration: 0.833s, episode steps: 110, steps per second: 132, episode reward: -317.748, mean reward: -2.889 [-100.000, 136.234], mean action: 1.582 [0.000, 3.000],  loss: 60.402291, mse: 153825.870810, mean_q: 293.066072, mean_eps: 0.874281
  42109/300000: episode: 444, duration: 1.010s, episode steps: 147, steps per second: 146, episode reward: -82.531, mean reward: -0.561 [-100.000, 12.891], mean action: 1.449 [0.000, 3.000],  loss: 44.653833, mse: 155443.735278, mean_q: 295.183727, mean_eps: 0.873895
  42184/300000: episode: 445, duration: 0.769s, episode steps:  75, steps per second:  98, episode reward: -165.554, mean reward: -2.207 [-100.000,  4.858], mean action: 1.800 [0.000, 3.000],  loss: 42.584918, mse: 157281.708125, mean_q: 298.316635, mean_eps: 0.873562
  42260/300000: episode: 446, duration: 0.622s, episode steps:  76, steps per second: 122, episode reward: -124.878, mean reward: -1.643 [-100.000,  9.227], mean action: 1.474 [0.000, 3.000],  loss: 79.016139, mse: 153194.796772, mean_q: 291.878453, mean_eps: 0.873335
  42366/300000: episode: 447, duration: 0.857s, episode steps: 106, steps per second: 124, episode reward: -117.526, mean reward: -1.109 [-100.000, 10.573], mean action: 1.613 [0.000, 3.000],  loss: 34.533043, mse: 163181.892836, mean_q: 303.667218, mean_eps: 0.873063
  42489/300000: episode: 448, duration: 1.112s, episode steps: 123, steps per second: 111, episode reward: -113.095, mean reward: -0.919 [-100.000, 25.382], mean action: 1.480 [0.000, 3.000],  loss: 51.984516, mse: 160347.632685, mean_q: 297.683479, mean_eps: 0.872719
  42613/300000: episode: 449, duration: 1.102s, episode steps: 124, steps per second: 113, episode reward: -9.779, mean reward: -0.079 [-100.000, 113.562], mean action: 1.516 [0.000, 3.000],  loss: 42.567650, mse: 161051.553868, mean_q: 300.460013, mean_eps: 0.872348
  42708/300000: episode: 450, duration: 0.749s, episode steps:  95, steps per second: 127, episode reward: -64.453, mean reward: -0.678 [-100.000, 69.737], mean action: 1.421 [0.000, 3.000],  loss: 42.805024, mse: 166595.120066, mean_q: 306.352480, mean_eps: 0.872020
  42827/300000: episode: 451, duration: 0.937s, episode steps: 119, steps per second: 127, episode reward: -208.820, mean reward: -1.755 [-100.000, 20.984], mean action: 1.437 [0.000, 3.000],  loss: 59.322369, mse: 164872.471376, mean_q: 300.833587, mean_eps: 0.871699
  42926/300000: episode: 452, duration: 0.795s, episode steps:  99, steps per second: 124, episode reward: -239.584, mean reward: -2.420 [-100.000, 23.585], mean action: 1.374 [0.000, 3.000],  loss: 39.326690, mse: 170308.085464, mean_q: 309.919109, mean_eps: 0.871372
  43059/300000: episode: 453, duration: 1.033s, episode steps: 133, steps per second: 129, episode reward: -223.523, mean reward: -1.681 [-100.000, 15.976], mean action: 1.669 [0.000, 3.000],  loss: 45.029179, mse: 166208.244831, mean_q: 305.307730, mean_eps: 0.871024
  43133/300000: episode: 454, duration: 0.522s, episode steps:  74, steps per second: 142, episode reward: -101.657, mean reward: -1.374 [-100.000,  7.898], mean action: 1.527 [0.000, 3.000],  loss: 51.702501, mse: 174337.028505, mean_q: 316.812919, mean_eps: 0.870713
  43219/300000: episode: 455, duration: 0.638s, episode steps:  86, steps per second: 135, episode reward: -298.743, mean reward: -3.474 [-100.000, 82.965], mean action: 1.547 [0.000, 3.000],  loss: 39.696319, mse: 173714.064589, mean_q: 318.232883, mean_eps: 0.870474
  43350/300000: episode: 456, duration: 1.027s, episode steps: 131, steps per second: 128, episode reward: -136.025, mean reward: -1.038 [-100.000, 11.862], mean action: 1.618 [0.000, 3.000],  loss: 44.701046, mse: 173153.340172, mean_q: 315.295728, mean_eps: 0.870148
  43443/300000: episode: 457, duration: 0.700s, episode steps:  93, steps per second: 133, episode reward: -236.691, mean reward: -2.545 [-100.000, 25.096], mean action: 1.763 [0.000, 3.000],  loss: 84.269174, mse: 176423.318548, mean_q: 317.530087, mean_eps: 0.869812
  43578/300000: episode: 458, duration: 0.994s, episode steps: 135, steps per second: 136, episode reward: -79.043, mean reward: -0.586 [-100.000,  9.976], mean action: 1.696 [0.000, 3.000],  loss: 44.201619, mse: 178638.797569, mean_q: 317.485657, mean_eps: 0.869470
  43682/300000: episode: 459, duration: 0.793s, episode steps: 104, steps per second: 131, episode reward:  0.214, mean reward:  0.002 [-100.000, 81.259], mean action: 1.740 [0.000, 3.000],  loss: 75.450045, mse: 177574.472957, mean_q: 315.661936, mean_eps: 0.869112
  43745/300000: episode: 460, duration: 0.461s, episode steps:  63, steps per second: 137, episode reward: -82.239, mean reward: -1.305 [-100.000, 28.324], mean action: 1.540 [0.000, 3.000],  loss: 35.240990, mse: 173269.555556, mean_q: 308.248457, mean_eps: 0.868861
  43874/300000: episode: 461, duration: 0.952s, episode steps: 129, steps per second: 135, episode reward: -156.987, mean reward: -1.217 [-100.000,  5.806], mean action: 1.473 [0.000, 3.000],  loss: 53.293758, mse: 178803.511567, mean_q: 318.512527, mean_eps: 0.868573
  43997/300000: episode: 462, duration: 0.903s, episode steps: 123, steps per second: 136, episode reward: -74.502, mean reward: -0.606 [-100.000, 21.110], mean action: 1.545 [0.000, 3.000],  loss: 43.881061, mse: 184937.871507, mean_q: 326.323482, mean_eps: 0.868195
  44078/300000: episode: 463, duration: 0.571s, episode steps:  81, steps per second: 142, episode reward: -123.898, mean reward: -1.530 [-100.000,  6.841], mean action: 1.531 [0.000, 3.000],  loss: 51.810120, mse: 190923.366705, mean_q: 333.848150, mean_eps: 0.867889
  44142/300000: episode: 464, duration: 0.461s, episode steps:  64, steps per second: 139, episode reward: -108.549, mean reward: -1.696 [-100.000, 12.728], mean action: 1.438 [0.000, 3.000],  loss: 51.223286, mse: 186458.150024, mean_q: 325.205258, mean_eps: 0.867671
  44246/300000: episode: 465, duration: 0.751s, episode steps: 104, steps per second: 138, episode reward: -369.755, mean reward: -3.555 [-100.000, 85.239], mean action: 1.346 [0.000, 3.000],  loss: 59.282515, mse: 186279.752855, mean_q: 330.348928, mean_eps: 0.867420
  44313/300000: episode: 466, duration: 0.472s, episode steps:  67, steps per second: 142, episode reward: -110.542, mean reward: -1.650 [-100.000, 19.352], mean action: 1.761 [0.000, 3.000],  loss: 71.770639, mse: 190316.505247, mean_q: 333.076809, mean_eps: 0.867163
  44410/300000: episode: 467, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -247.936, mean reward: -2.556 [-100.000, 22.226], mean action: 1.722 [0.000, 3.000],  loss: 40.589258, mse: 191945.090126, mean_q: 336.955747, mean_eps: 0.866917
  44545/300000: episode: 468, duration: 1.059s, episode steps: 135, steps per second: 128, episode reward: -109.254, mean reward: -0.809 [-100.000, 13.536], mean action: 1.607 [0.000, 3.000],  loss: 55.070723, mse: 192593.146586, mean_q: 334.489426, mean_eps: 0.866569
  44664/300000: episode: 469, duration: 0.845s, episode steps: 119, steps per second: 141, episode reward: -309.700, mean reward: -2.603 [-100.000,  3.545], mean action: 1.555 [0.000, 3.000],  loss: 46.419440, mse: 191373.966452, mean_q: 332.297395, mean_eps: 0.866188
  44784/300000: episode: 470, duration: 0.892s, episode steps: 120, steps per second: 134, episode reward: -325.229, mean reward: -2.710 [-100.000, 30.145], mean action: 1.633 [0.000, 3.000],  loss: 43.554973, mse: 203324.520573, mean_q: 346.085787, mean_eps: 0.865830
  44910/300000: episode: 471, duration: 0.904s, episode steps: 126, steps per second: 139, episode reward: -119.616, mean reward: -0.949 [-100.000,  7.461], mean action: 1.619 [0.000, 3.000],  loss: 63.011136, mse: 202451.196305, mean_q: 342.004739, mean_eps: 0.865460
  45036/300000: episode: 472, duration: 0.960s, episode steps: 126, steps per second: 131, episode reward: 22.007, mean reward:  0.175 [-100.000, 73.809], mean action: 1.603 [0.000, 3.000],  loss: 55.506322, mse: 211584.636719, mean_q: 352.979620, mean_eps: 0.865082
  45149/300000: episode: 473, duration: 0.868s, episode steps: 113, steps per second: 130, episode reward: -219.893, mean reward: -1.946 [-100.000, 95.898], mean action: 1.504 [0.000, 3.000],  loss: 59.175788, mse: 208466.200221, mean_q: 349.493356, mean_eps: 0.864724
  45220/300000: episode: 474, duration: 0.494s, episode steps:  71, steps per second: 144, episode reward: -59.033, mean reward: -0.831 [-100.000, 13.540], mean action: 1.746 [0.000, 3.000],  loss: 56.305416, mse: 214471.954445, mean_q: 356.158921, mean_eps: 0.864448
  45357/300000: episode: 475, duration: 0.967s, episode steps: 137, steps per second: 142, episode reward: -288.924, mean reward: -2.109 [-100.000, 68.437], mean action: 1.504 [0.000, 3.000],  loss: 47.331909, mse: 217149.775034, mean_q: 358.883092, mean_eps: 0.864136
  45445/300000: episode: 476, duration: 0.668s, episode steps:  88, steps per second: 132, episode reward: -103.419, mean reward: -1.175 [-100.000, 12.413], mean action: 1.409 [0.000, 3.000],  loss: 47.199485, mse: 223541.084428, mean_q: 369.915922, mean_eps: 0.863799
  45534/300000: episode: 477, duration: 0.642s, episode steps:  89, steps per second: 139, episode reward: -238.645, mean reward: -2.681 [-100.000, 12.576], mean action: 1.528 [0.000, 3.000],  loss: 46.580568, mse: 217080.361131, mean_q: 358.488671, mean_eps: 0.863533
  45666/300000: episode: 478, duration: 0.973s, episode steps: 132, steps per second: 136, episode reward: -103.252, mean reward: -0.782 [-100.000,  6.375], mean action: 1.485 [0.000, 3.000],  loss: 57.372730, mse: 221890.314276, mean_q: 365.922628, mean_eps: 0.863202
  45764/300000: episode: 479, duration: 0.677s, episode steps:  98, steps per second: 145, episode reward: -99.480, mean reward: -1.015 [-100.000,  8.498], mean action: 1.796 [0.000, 3.000],  loss: 49.406458, mse: 220490.248246, mean_q: 361.108500, mean_eps: 0.862856
  45891/300000: episode: 480, duration: 0.891s, episode steps: 127, steps per second: 143, episode reward: -402.693, mean reward: -3.171 [-100.000,  4.510], mean action: 1.575 [0.000, 3.000],  loss: 45.157073, mse: 224401.093812, mean_q: 366.999920, mean_eps: 0.862519
  46009/300000: episode: 481, duration: 0.936s, episode steps: 118, steps per second: 126, episode reward: -235.343, mean reward: -1.994 [-100.000,  7.084], mean action: 1.568 [0.000, 3.000],  loss: 48.396882, mse: 219985.153668, mean_q: 358.240206, mean_eps: 0.862152
  46141/300000: episode: 482, duration: 1.009s, episode steps: 132, steps per second: 131, episode reward: -182.564, mean reward: -1.383 [-100.000,  2.871], mean action: 1.758 [0.000, 3.000],  loss: 51.855288, mse: 224638.278883, mean_q: 363.725428, mean_eps: 0.861777
  46252/300000: episode: 483, duration: 0.848s, episode steps: 111, steps per second: 131, episode reward: -163.689, mean reward: -1.475 [-100.000,  5.354], mean action: 1.595 [0.000, 3.000],  loss: 42.309539, mse: 223788.983671, mean_q: 360.105466, mean_eps: 0.861412
  46317/300000: episode: 484, duration: 0.446s, episode steps:  65, steps per second: 146, episode reward: -124.200, mean reward: -1.911 [-100.000, 14.217], mean action: 1.462 [0.000, 3.000],  loss: 54.693053, mse: 221690.915144, mean_q: 354.329905, mean_eps: 0.861148
  46415/300000: episode: 485, duration: 0.670s, episode steps:  98, steps per second: 146, episode reward: -369.996, mean reward: -3.775 [-100.000, 99.899], mean action: 1.602 [0.000, 3.000],  loss: 48.588400, mse: 224344.973214, mean_q: 359.315548, mean_eps: 0.860904
  46541/300000: episode: 486, duration: 0.866s, episode steps: 126, steps per second: 145, episode reward: -96.306, mean reward: -0.764 [-100.000,  8.215], mean action: 1.619 [0.000, 3.000],  loss: 51.996411, mse: 222714.334077, mean_q: 358.995149, mean_eps: 0.860568
  46654/300000: episode: 487, duration: 0.773s, episode steps: 113, steps per second: 146, episode reward: -122.371, mean reward: -1.083 [-100.000,  5.920], mean action: 1.513 [0.000, 3.000],  loss: 51.699044, mse: 222103.140763, mean_q: 358.721171, mean_eps: 0.860209
  46779/300000: episode: 488, duration: 0.857s, episode steps: 125, steps per second: 146, episode reward: -221.125, mean reward: -1.769 [-100.000,  5.741], mean action: 1.560 [0.000, 3.000],  loss: 51.181117, mse: 233429.307000, mean_q: 367.379885, mean_eps: 0.859852
  46853/300000: episode: 489, duration: 0.527s, episode steps:  74, steps per second: 141, episode reward: -110.986, mean reward: -1.500 [-100.000,  6.714], mean action: 1.432 [0.000, 3.000],  loss: 63.857944, mse: 222380.979941, mean_q: 352.419372, mean_eps: 0.859554
  47006/300000: episode: 490, duration: 1.101s, episode steps: 153, steps per second: 139, episode reward: -147.689, mean reward: -0.965 [-100.000,  9.040], mean action: 1.464 [0.000, 3.000],  loss: 55.427247, mse: 230172.575572, mean_q: 363.406959, mean_eps: 0.859213
  47152/300000: episode: 491, duration: 1.014s, episode steps: 146, steps per second: 144, episode reward: -150.110, mean reward: -1.028 [-100.000,  5.540], mean action: 1.534 [0.000, 3.000],  loss: 55.811010, mse: 238684.399294, mean_q: 369.269677, mean_eps: 0.858764
  47212/300000: episode: 492, duration: 0.442s, episode steps:  60, steps per second: 136, episode reward: -123.112, mean reward: -2.052 [-100.000,  7.824], mean action: 1.700 [0.000, 3.000],  loss: 50.418463, mse: 245104.211458, mean_q: 377.136627, mean_eps: 0.858456
  47315/300000: episode: 493, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -116.185, mean reward: -1.128 [-100.000, 12.119], mean action: 1.553 [0.000, 3.000],  loss: 49.882153, mse: 229106.575546, mean_q: 355.493093, mean_eps: 0.858211
  47407/300000: episode: 494, duration: 0.659s, episode steps:  92, steps per second: 140, episode reward: -263.054, mean reward: -2.859 [-100.000, 18.773], mean action: 1.326 [0.000, 3.000],  loss: 49.205340, mse: 234723.415931, mean_q: 367.489230, mean_eps: 0.857919
  47499/300000: episode: 495, duration: 0.711s, episode steps:  92, steps per second: 129, episode reward: -119.891, mean reward: -1.303 [-100.000,  7.670], mean action: 1.565 [0.000, 3.000],  loss: 49.008907, mse: 235345.185292, mean_q: 362.790302, mean_eps: 0.857643
  47607/300000: episode: 496, duration: 0.788s, episode steps: 108, steps per second: 137, episode reward: -200.060, mean reward: -1.852 [-100.000, 45.179], mean action: 1.620 [0.000, 3.000],  loss: 53.864444, mse: 240528.887008, mean_q: 370.497106, mean_eps: 0.857342
  47750/300000: episode: 497, duration: 1.108s, episode steps: 143, steps per second: 129, episode reward: -146.791, mean reward: -1.027 [-100.000,  8.470], mean action: 1.594 [0.000, 3.000],  loss: 53.841889, mse: 241175.690122, mean_q: 376.747691, mean_eps: 0.856966
  47855/300000: episode: 498, duration: 0.762s, episode steps: 105, steps per second: 138, episode reward: -92.764, mean reward: -0.883 [-100.000, 10.370], mean action: 1.552 [0.000, 3.000],  loss: 50.047211, mse: 240592.026935, mean_q: 370.490480, mean_eps: 0.856594
  47938/300000: episode: 499, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -230.302, mean reward: -2.775 [-100.000, 42.374], mean action: 1.542 [0.000, 3.000],  loss: 55.212363, mse: 231908.908886, mean_q: 360.161382, mean_eps: 0.856312
  48040/300000: episode: 500, duration: 0.719s, episode steps: 102, steps per second: 142, episode reward: -111.497, mean reward: -1.093 [-100.000,  7.626], mean action: 1.559 [0.000, 3.000],  loss: 50.070499, mse: 241890.532935, mean_q: 370.898833, mean_eps: 0.856034
  48130/300000: episode: 501, duration: 0.661s, episode steps:  90, steps per second: 136, episode reward: -103.497, mean reward: -1.150 [-100.000,  9.387], mean action: 1.533 [0.000, 3.000],  loss: 50.263232, mse: 243119.724653, mean_q: 373.917066, mean_eps: 0.855747
  48210/300000: episode: 502, duration: 0.546s, episode steps:  80, steps per second: 146, episode reward: -105.516, mean reward: -1.319 [-100.000,  6.616], mean action: 1.325 [0.000, 3.000],  loss: 52.752265, mse: 245358.938672, mean_q: 373.153737, mean_eps: 0.855492
  48279/300000: episode: 503, duration: 0.461s, episode steps:  69, steps per second: 150, episode reward: -126.517, mean reward: -1.834 [-100.000,  6.797], mean action: 1.725 [0.000, 3.000],  loss: 48.046714, mse: 239996.075181, mean_q: 369.881946, mean_eps: 0.855268
  48360/300000: episode: 504, duration: 0.605s, episode steps:  81, steps per second: 134, episode reward: -83.152, mean reward: -1.027 [-100.000, 11.715], mean action: 1.605 [0.000, 3.000],  loss: 51.399258, mse: 250260.274306, mean_q: 380.679816, mean_eps: 0.855043
  48494/300000: episode: 505, duration: 0.929s, episode steps: 134, steps per second: 144, episode reward: -332.655, mean reward: -2.482 [-100.000,  1.887], mean action: 1.507 [0.000, 3.000],  loss: 49.359439, mse: 245165.369170, mean_q: 373.991926, mean_eps: 0.854720
  48604/300000: episode: 506, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: -74.436, mean reward: -0.677 [-100.000,  7.942], mean action: 1.600 [0.000, 3.000],  loss: 47.907340, mse: 252478.080256, mean_q: 380.302472, mean_eps: 0.854355
  48718/300000: episode: 507, duration: 0.863s, episode steps: 114, steps per second: 132, episode reward: -153.805, mean reward: -1.349 [-100.000,  8.983], mean action: 1.596 [0.000, 3.000],  loss: 56.437475, mse: 245239.938459, mean_q: 369.712925, mean_eps: 0.854019
  48823/300000: episode: 508, duration: 0.772s, episode steps: 105, steps per second: 136, episode reward: -110.810, mean reward: -1.055 [-100.000,  7.629], mean action: 1.505 [0.000, 3.000],  loss: 43.194093, mse: 248136.264137, mean_q: 372.075933, mean_eps: 0.853690
  48928/300000: episode: 509, duration: 0.812s, episode steps: 105, steps per second: 129, episode reward: -141.613, mean reward: -1.349 [-100.000,  7.139], mean action: 1.581 [0.000, 3.000],  loss: 57.448331, mse: 260803.031250, mean_q: 383.457741, mean_eps: 0.853375
  48996/300000: episode: 510, duration: 0.505s, episode steps:  68, steps per second: 135, episode reward: -52.926, mean reward: -0.778 [-100.000, 13.268], mean action: 1.662 [0.000, 3.000],  loss: 57.790661, mse: 254396.297794, mean_q: 373.748688, mean_eps: 0.853115
  49106/300000: episode: 511, duration: 0.820s, episode steps: 110, steps per second: 134, episode reward: -163.189, mean reward: -1.484 [-100.000,  2.032], mean action: 1.600 [0.000, 3.000],  loss: 63.041174, mse: 251339.629261, mean_q: 374.155274, mean_eps: 0.852849
  49191/300000: episode: 512, duration: 0.666s, episode steps:  85, steps per second: 128, episode reward: -89.323, mean reward: -1.051 [-100.000,  8.089], mean action: 1.694 [0.000, 3.000],  loss: 49.344406, mse: 250984.432721, mean_q: 371.323433, mean_eps: 0.852556
  49284/300000: episode: 513, duration: 0.816s, episode steps:  93, steps per second: 114, episode reward: -355.200, mean reward: -3.819 [-100.000,  0.352], mean action: 1.409 [0.000, 3.000],  loss: 55.894097, mse: 260491.125000, mean_q: 380.973865, mean_eps: 0.852289
  49371/300000: episode: 514, duration: 0.633s, episode steps:  87, steps per second: 137, episode reward: -181.772, mean reward: -2.089 [-100.000, 13.999], mean action: 1.460 [0.000, 3.000],  loss: 46.939845, mse: 260581.171516, mean_q: 379.464766, mean_eps: 0.852019
  49501/300000: episode: 515, duration: 0.910s, episode steps: 130, steps per second: 143, episode reward: -116.975, mean reward: -0.900 [-100.000,  7.514], mean action: 1.515 [0.000, 3.000],  loss: 50.005170, mse: 268271.154567, mean_q: 389.173990, mean_eps: 0.851693
  49628/300000: episode: 516, duration: 0.882s, episode steps: 127, steps per second: 144, episode reward: -129.606, mean reward: -1.021 [-100.000, 15.928], mean action: 1.677 [0.000, 3.000],  loss: 57.883938, mse: 275047.891240, mean_q: 391.087017, mean_eps: 0.851308
  49717/300000: episode: 517, duration: 0.726s, episode steps:  89, steps per second: 123, episode reward: -77.861, mean reward: -0.875 [-100.000,  8.905], mean action: 1.506 [0.000, 3.000],  loss: 50.048329, mse: 270483.029846, mean_q: 386.593027, mean_eps: 0.850984
  49807/300000: episode: 518, duration: 0.776s, episode steps:  90, steps per second: 116, episode reward: -34.152, mean reward: -0.379 [-100.000, 67.684], mean action: 1.556 [0.000, 3.000],  loss: 61.815152, mse: 282872.159896, mean_q: 400.911065, mean_eps: 0.850715
  49928/300000: episode: 519, duration: 0.959s, episode steps: 121, steps per second: 126, episode reward: -118.763, mean reward: -0.982 [-100.000,  7.866], mean action: 1.603 [0.000, 3.000],  loss: 73.587586, mse: 266453.095041, mean_q: 385.128979, mean_eps: 0.850399
  50071/300000: episode: 520, duration: 1.212s, episode steps: 143, steps per second: 118, episode reward: -206.313, mean reward: -1.443 [-100.000, 70.593], mean action: 1.594 [0.000, 3.000],  loss: 59.346297, mse: 283319.340581, mean_q: 400.198186, mean_eps: 0.850003
  50149/300000: episode: 521, duration: 0.575s, episode steps:  78, steps per second: 136, episode reward: -191.370, mean reward: -2.453 [-100.000, 26.274], mean action: 1.628 [0.000, 3.000],  loss: 53.494389, mse: 278953.543470, mean_q: 396.532868, mean_eps: 0.849672
  50315/300000: episode: 522, duration: 1.368s, episode steps: 166, steps per second: 121, episode reward: -359.051, mean reward: -2.163 [-100.000,  6.732], mean action: 1.620 [0.000, 3.000],  loss: 57.902005, mse: 278862.061465, mean_q: 391.748691, mean_eps: 0.849306
  50388/300000: episode: 523, duration: 0.520s, episode steps:  73, steps per second: 141, episode reward: -39.182, mean reward: -0.537 [-100.000, 16.233], mean action: 1.521 [0.000, 3.000],  loss: 58.835568, mse: 272526.612158, mean_q: 384.200436, mean_eps: 0.848947
  50481/300000: episode: 524, duration: 0.626s, episode steps:  93, steps per second: 149, episode reward: -130.091, mean reward: -1.399 [-100.000, 12.190], mean action: 1.548 [0.000, 3.000],  loss: 55.995421, mse: 286991.775202, mean_q: 403.462532, mean_eps: 0.848698
  50592/300000: episode: 525, duration: 0.787s, episode steps: 111, steps per second: 141, episode reward: -30.589, mean reward: -0.276 [-100.000, 82.436], mean action: 1.423 [0.000, 3.000],  loss: 66.222609, mse: 281862.757742, mean_q: 396.515245, mean_eps: 0.848392
  50723/300000: episode: 526, duration: 0.973s, episode steps: 131, steps per second: 135, episode reward: -291.571, mean reward: -2.226 [-100.000, 75.891], mean action: 1.435 [0.000, 3.000],  loss: 62.116789, mse: 286484.841961, mean_q: 395.398768, mean_eps: 0.848029
  50803/300000: episode: 527, duration: 0.555s, episode steps:  80, steps per second: 144, episode reward: -110.621, mean reward: -1.383 [-100.000, 16.220], mean action: 1.625 [0.000, 3.000],  loss: 59.884813, mse: 270883.978711, mean_q: 379.766724, mean_eps: 0.847712
  50882/300000: episode: 528, duration: 0.526s, episode steps:  79, steps per second: 150, episode reward: -76.478, mean reward: -0.968 [-100.000,  8.284], mean action: 1.557 [0.000, 3.000],  loss: 56.897131, mse: 297189.728639, mean_q: 399.516848, mean_eps: 0.847474
  50979/300000: episode: 529, duration: 0.685s, episode steps:  97, steps per second: 142, episode reward: -105.502, mean reward: -1.088 [-100.000,  6.411], mean action: 1.505 [0.000, 3.000],  loss: 61.852955, mse: 289458.635309, mean_q: 392.150857, mean_eps: 0.847210
  51098/300000: episode: 530, duration: 0.836s, episode steps: 119, steps per second: 142, episode reward: -497.269, mean reward: -4.179 [-100.000, 84.331], mean action: 1.689 [0.000, 3.000],  loss: 60.684382, mse: 303222.639181, mean_q: 408.195369, mean_eps: 0.846886
  51164/300000: episode: 531, duration: 0.537s, episode steps:  66, steps per second: 123, episode reward: -79.067, mean reward: -1.198 [-100.000,  8.216], mean action: 1.515 [0.000, 3.000],  loss: 66.644773, mse: 286557.615767, mean_q: 391.705410, mean_eps: 0.846608
  51246/300000: episode: 532, duration: 0.667s, episode steps:  82, steps per second: 123, episode reward: -87.367, mean reward: -1.065 [-100.000, 44.702], mean action: 1.854 [0.000, 3.000],  loss: 62.785081, mse: 302807.654916, mean_q: 405.265425, mean_eps: 0.846387
  51353/300000: episode: 533, duration: 0.814s, episode steps: 107, steps per second: 131, episode reward: -409.729, mean reward: -3.829 [-100.000,  1.952], mean action: 1.533 [0.000, 3.000],  loss: 61.637586, mse: 304064.839515, mean_q: 406.075654, mean_eps: 0.846103
  51421/300000: episode: 534, duration: 0.518s, episode steps:  68, steps per second: 131, episode reward: -89.051, mean reward: -1.310 [-100.000,  5.913], mean action: 1.544 [0.000, 3.000],  loss: 60.391652, mse: 292578.493796, mean_q: 393.011991, mean_eps: 0.845840
  51498/300000: episode: 535, duration: 0.623s, episode steps:  77, steps per second: 124, episode reward: -80.757, mean reward: -1.049 [-100.000, 10.180], mean action: 1.545 [0.000, 3.000],  loss: 89.250197, mse: 303879.197646, mean_q: 399.898666, mean_eps: 0.845623
  51587/300000: episode: 536, duration: 0.751s, episode steps:  89, steps per second: 119, episode reward: -59.817, mean reward: -0.672 [-100.000,  7.328], mean action: 1.494 [0.000, 3.000],  loss: 62.132190, mse: 317650.317065, mean_q: 411.820830, mean_eps: 0.845374
  51665/300000: episode: 537, duration: 0.593s, episode steps:  78, steps per second: 131, episode reward: -169.631, mean reward: -2.175 [-100.000, 10.122], mean action: 1.397 [0.000, 3.000],  loss: 61.834506, mse: 278512.388221, mean_q: 377.253302, mean_eps: 0.845124
  51781/300000: episode: 538, duration: 0.842s, episode steps: 116, steps per second: 138, episode reward: -157.572, mean reward: -1.358 [-100.000,  3.544], mean action: 1.595 [0.000, 3.000],  loss: 74.255991, mse: 302242.116649, mean_q: 397.906153, mean_eps: 0.844833
  51945/300000: episode: 539, duration: 1.172s, episode steps: 164, steps per second: 140, episode reward: -327.714, mean reward: -1.998 [-100.000, 87.988], mean action: 1.457 [0.000, 3.000],  loss: 72.517881, mse: 311964.160442, mean_q: 405.904435, mean_eps: 0.844413
  52063/300000: episode: 540, duration: 0.813s, episode steps: 118, steps per second: 145, episode reward: -33.108, mean reward: -0.281 [-100.000, 73.254], mean action: 1.636 [0.000, 3.000],  loss: 65.195583, mse: 315773.051774, mean_q: 411.428552, mean_eps: 0.843989
  52175/300000: episode: 541, duration: 0.811s, episode steps: 112, steps per second: 138, episode reward: -150.090, mean reward: -1.340 [-100.000,  6.307], mean action: 1.446 [0.000, 3.000],  loss: 85.078555, mse: 328723.802176, mean_q: 418.101036, mean_eps: 0.843644
  52254/300000: episode: 542, duration: 0.533s, episode steps:  79, steps per second: 148, episode reward: -69.836, mean reward: -0.884 [-100.000, 23.156], mean action: 1.481 [0.000, 3.000],  loss: 67.423421, mse: 325046.838608, mean_q: 410.552177, mean_eps: 0.843358
  52333/300000: episode: 543, duration: 0.533s, episode steps:  79, steps per second: 148, episode reward: -110.368, mean reward: -1.397 [-100.000, 14.640], mean action: 1.646 [0.000, 3.000],  loss: 84.037922, mse: 336479.557951, mean_q: 424.742666, mean_eps: 0.843121
  52408/300000: episode: 544, duration: 0.540s, episode steps:  75, steps per second: 139, episode reward: -99.290, mean reward: -1.324 [-100.000,  5.882], mean action: 1.400 [0.000, 3.000],  loss: 68.292305, mse: 340067.135833, mean_q: 424.769320, mean_eps: 0.842890
  52532/300000: episode: 545, duration: 0.841s, episode steps: 124, steps per second: 147, episode reward: -125.192, mean reward: -1.010 [-100.000, 27.536], mean action: 1.774 [0.000, 3.000],  loss: 64.515634, mse: 343799.977823, mean_q: 424.321690, mean_eps: 0.842591
  52656/300000: episode: 546, duration: 0.847s, episode steps: 124, steps per second: 146, episode reward: -193.876, mean reward: -1.564 [-100.000,  1.679], mean action: 1.669 [0.000, 3.000],  loss: 82.226231, mse: 367750.560358, mean_q: 446.239308, mean_eps: 0.842219
  52747/300000: episode: 547, duration: 0.651s, episode steps:  91, steps per second: 140, episode reward: -102.283, mean reward: -1.124 [-100.000,  9.541], mean action: 1.363 [0.000, 3.000],  loss: 78.166485, mse: 384581.951923, mean_q: 460.976218, mean_eps: 0.841897
  52821/300000: episode: 548, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -71.239, mean reward: -0.963 [-100.000, 89.130], mean action: 1.568 [0.000, 3.000],  loss: 93.138071, mse: 363944.803209, mean_q: 440.152900, mean_eps: 0.841650
  52962/300000: episode: 549, duration: 0.982s, episode steps: 141, steps per second: 144, episode reward: -211.637, mean reward: -1.501 [-100.000,  4.148], mean action: 1.603 [0.000, 3.000],  loss: 78.096826, mse: 378635.862921, mean_q: 451.332346, mean_eps: 0.841327
  53067/300000: episode: 550, duration: 0.761s, episode steps: 105, steps per second: 138, episode reward: -161.095, mean reward: -1.534 [-100.000,  5.834], mean action: 1.390 [0.000, 3.000],  loss: 81.655031, mse: 364600.096726, mean_q: 441.449034, mean_eps: 0.840958
  53184/300000: episode: 551, duration: 0.812s, episode steps: 117, steps per second: 144, episode reward: -282.599, mean reward: -2.415 [-100.000, 86.342], mean action: 1.701 [0.000, 3.000],  loss: 85.271625, mse: 367158.403178, mean_q: 442.482027, mean_eps: 0.840625
  53249/300000: episode: 552, duration: 0.442s, episode steps:  65, steps per second: 147, episode reward: -239.770, mean reward: -3.689 [-100.000,  6.273], mean action: 1.400 [0.000, 3.000],  loss: 73.650980, mse: 349865.656731, mean_q: 428.016097, mean_eps: 0.840352
  53332/300000: episode: 553, duration: 0.612s, episode steps:  83, steps per second: 136, episode reward: -81.307, mean reward: -0.980 [-100.000,  5.979], mean action: 1.602 [0.000, 3.000],  loss: 71.045690, mse: 372275.663215, mean_q: 443.260678, mean_eps: 0.840130
  53414/300000: episode: 554, duration: 0.581s, episode steps:  82, steps per second: 141, episode reward: -67.818, mean reward: -0.827 [-100.000, 13.897], mean action: 1.646 [0.000, 3.000],  loss: 71.437580, mse: 370909.453316, mean_q: 441.796714, mean_eps: 0.839883
  53490/300000: episode: 555, duration: 0.537s, episode steps:  76, steps per second: 142, episode reward: -77.859, mean reward: -1.024 [-100.000,  9.686], mean action: 1.566 [0.000, 3.000],  loss: 83.662640, mse: 382167.968544, mean_q: 446.384059, mean_eps: 0.839646
  53582/300000: episode: 556, duration: 0.786s, episode steps:  92, steps per second: 117, episode reward: -74.795, mean reward: -0.813 [-100.000, 13.093], mean action: 1.630 [0.000, 3.000],  loss: 76.577200, mse: 381635.911515, mean_q: 451.943756, mean_eps: 0.839393
  53662/300000: episode: 557, duration: 0.596s, episode steps:  80, steps per second: 134, episode reward: -76.358, mean reward: -0.954 [-100.000,  7.159], mean action: 1.562 [0.000, 3.000],  loss: 66.565114, mse: 375389.022852, mean_q: 445.060986, mean_eps: 0.839136
  53755/300000: episode: 558, duration: 0.628s, episode steps:  93, steps per second: 148, episode reward: -133.246, mean reward: -1.433 [-100.000,  6.303], mean action: 1.839 [0.000, 3.000],  loss: 80.919947, mse: 373547.365591, mean_q: 437.921439, mean_eps: 0.838876
  53849/300000: episode: 559, duration: 0.639s, episode steps:  94, steps per second: 147, episode reward: -255.976, mean reward: -2.723 [-100.000, 43.239], mean action: 1.436 [0.000, 3.000],  loss: 75.451293, mse: 391306.858211, mean_q: 450.714173, mean_eps: 0.838596
  53940/300000: episode: 560, duration: 0.676s, episode steps:  91, steps per second: 135, episode reward: -109.279, mean reward: -1.201 [-100.000, 12.536], mean action: 1.538 [0.000, 3.000],  loss: 80.986448, mse: 360852.976305, mean_q: 424.779828, mean_eps: 0.838318
  54029/300000: episode: 561, duration: 0.637s, episode steps:  89, steps per second: 140, episode reward: -121.825, mean reward: -1.369 [-100.000, 51.619], mean action: 1.618 [0.000, 3.000],  loss: 66.532556, mse: 389018.786166, mean_q: 446.273675, mean_eps: 0.838048
  54137/300000: episode: 562, duration: 0.736s, episode steps: 108, steps per second: 147, episode reward: -358.227, mean reward: -3.317 [-100.000,  6.061], mean action: 1.602 [0.000, 3.000],  loss: 87.817123, mse: 390846.160590, mean_q: 449.551137, mean_eps: 0.837753
  54286/300000: episode: 563, duration: 1.038s, episode steps: 149, steps per second: 144, episode reward: -172.411, mean reward: -1.157 [-100.000, 10.535], mean action: 1.537 [0.000, 3.000],  loss: 76.330386, mse: 390085.645973, mean_q: 449.231425, mean_eps: 0.837367
  54366/300000: episode: 564, duration: 0.556s, episode steps:  80, steps per second: 144, episode reward: -83.149, mean reward: -1.039 [-100.000,  8.308], mean action: 1.575 [0.000, 3.000],  loss: 71.225955, mse: 365316.456250, mean_q: 428.992669, mean_eps: 0.837023
  54480/300000: episode: 565, duration: 0.799s, episode steps: 114, steps per second: 143, episode reward: -194.180, mean reward: -1.703 [-100.000, 29.307], mean action: 1.658 [0.000, 3.000],  loss: 78.490170, mse: 398282.736157, mean_q: 455.364168, mean_eps: 0.836732
  54579/300000: episode: 566, duration: 0.750s, episode steps:  99, steps per second: 132, episode reward: -195.300, mean reward: -1.973 [-100.000,  7.381], mean action: 1.394 [0.000, 3.000],  loss: 70.769302, mse: 387732.978220, mean_q: 448.896126, mean_eps: 0.836413
  54692/300000: episode: 567, duration: 0.826s, episode steps: 113, steps per second: 137, episode reward: -107.053, mean reward: -0.947 [-100.000,  7.051], mean action: 1.434 [0.000, 3.000],  loss: 85.167976, mse: 417348.306139, mean_q: 465.320925, mean_eps: 0.836095
  54777/300000: episode: 568, duration: 0.591s, episode steps:  85, steps per second: 144, episode reward: -109.852, mean reward: -1.292 [-100.000, 10.510], mean action: 1.694 [0.000, 3.000],  loss: 80.451509, mse: 419747.178493, mean_q: 471.288027, mean_eps: 0.835798
  54884/300000: episode: 569, duration: 0.843s, episode steps: 107, steps per second: 127, episode reward: -105.699, mean reward: -0.988 [-100.000, 16.569], mean action: 1.542 [0.000, 3.000],  loss: 95.899840, mse: 428416.998102, mean_q: 470.329545, mean_eps: 0.835510
  54979/300000: episode: 570, duration: 0.698s, episode steps:  95, steps per second: 136, episode reward: -144.272, mean reward: -1.519 [-100.000,  5.926], mean action: 1.695 [0.000, 3.000],  loss: 100.200096, mse: 421258.346546, mean_q: 470.222566, mean_eps: 0.835207
  55088/300000: episode: 571, duration: 0.759s, episode steps: 109, steps per second: 144, episode reward: -182.471, mean reward: -1.674 [-100.000, 54.230], mean action: 1.661 [0.000, 3.000],  loss: 87.510016, mse: 435450.224341, mean_q: 479.569847, mean_eps: 0.834901
  55199/300000: episode: 572, duration: 0.847s, episode steps: 111, steps per second: 131, episode reward: -131.209, mean reward: -1.182 [-100.000, 18.116], mean action: 1.423 [0.000, 3.000],  loss: 91.592563, mse: 428773.087416, mean_q: 473.592287, mean_eps: 0.834571
  55319/300000: episode: 573, duration: 0.834s, episode steps: 120, steps per second: 144, episode reward: -283.713, mean reward: -2.364 [-100.000, 101.028], mean action: 1.567 [0.000, 3.000],  loss: 95.429747, mse: 434131.977734, mean_q: 474.379834, mean_eps: 0.834225
  55407/300000: episode: 574, duration: 0.604s, episode steps:  88, steps per second: 146, episode reward: -86.612, mean reward: -0.984 [-100.000,  7.260], mean action: 1.830 [0.000, 3.000],  loss: 79.546667, mse: 438503.054510, mean_q: 476.173439, mean_eps: 0.833912
  55551/300000: episode: 575, duration: 0.985s, episode steps: 144, steps per second: 146, episode reward: -332.506, mean reward: -2.309 [-100.000,  4.127], mean action: 1.681 [0.000, 3.000],  loss: 98.357903, mse: 444777.904297, mean_q: 482.704914, mean_eps: 0.833565
  55649/300000: episode: 576, duration: 0.681s, episode steps:  98, steps per second: 144, episode reward: -183.203, mean reward: -1.869 [-100.000, 42.278], mean action: 1.673 [0.000, 3.000],  loss: 75.945681, mse: 453855.169802, mean_q: 482.736809, mean_eps: 0.833201
  55750/300000: episode: 577, duration: 0.761s, episode steps: 101, steps per second: 133, episode reward: -170.772, mean reward: -1.691 [-100.000,  2.525], mean action: 1.703 [0.000, 3.000],  loss: 86.918243, mse: 465658.271504, mean_q: 491.774996, mean_eps: 0.832903
  55871/300000: episode: 578, duration: 0.910s, episode steps: 121, steps per second: 133, episode reward: -121.796, mean reward: -1.007 [-100.000,  6.393], mean action: 1.603 [0.000, 3.000],  loss: 80.462050, mse: 467280.192020, mean_q: 491.588258, mean_eps: 0.832570
  55999/300000: episode: 579, duration: 0.971s, episode steps: 128, steps per second: 132, episode reward: -126.166, mean reward: -0.986 [-100.000,  6.105], mean action: 1.633 [0.000, 3.000],  loss: 92.918763, mse: 489065.974609, mean_q: 509.321911, mean_eps: 0.832197
  56146/300000: episode: 580, duration: 1.051s, episode steps: 147, steps per second: 140, episode reward: -280.727, mean reward: -1.910 [-100.000, 86.674], mean action: 1.653 [0.000, 3.000],  loss: 93.705668, mse: 478535.650723, mean_q: 499.371200, mean_eps: 0.831784
  56220/300000: episode: 581, duration: 0.509s, episode steps:  74, steps per second: 145, episode reward: -142.328, mean reward: -1.923 [-100.000,  6.504], mean action: 1.635 [0.000, 3.000],  loss: 95.412881, mse: 484494.628801, mean_q: 501.820063, mean_eps: 0.831452
  56335/300000: episode: 582, duration: 0.820s, episode steps: 115, steps per second: 140, episode reward: -159.776, mean reward: -1.389 [-100.000,  5.896], mean action: 1.713 [0.000, 3.000],  loss: 101.117447, mse: 503148.621332, mean_q: 517.537369, mean_eps: 0.831169
  56466/300000: episode: 583, duration: 0.907s, episode steps: 131, steps per second: 144, episode reward: -93.232, mean reward: -0.712 [-100.000,  8.432], mean action: 1.588 [0.000, 3.000],  loss: 85.626854, mse: 512168.563931, mean_q: 521.199573, mean_eps: 0.830800
  56574/300000: episode: 584, duration: 0.795s, episode steps: 108, steps per second: 136, episode reward: -163.724, mean reward: -1.516 [-100.000,  9.237], mean action: 1.472 [0.000, 3.000],  loss: 109.259007, mse: 506688.493056, mean_q: 517.457864, mean_eps: 0.830442
  56655/300000: episode: 585, duration: 0.621s, episode steps:  81, steps per second: 130, episode reward: -130.223, mean reward: -1.608 [-100.000, 11.167], mean action: 1.691 [0.000, 3.000],  loss: 111.136271, mse: 532554.313272, mean_q: 535.942310, mean_eps: 0.830158
  56770/300000: episode: 586, duration: 0.798s, episode steps: 115, steps per second: 144, episode reward: -39.125, mean reward: -0.340 [-100.000, 12.809], mean action: 1.600 [0.000, 3.000],  loss: 130.672550, mse: 546402.378533, mean_q: 541.928121, mean_eps: 0.829864
  57770/300000: episode: 587, duration: 8.086s, episode steps: 1000, steps per second: 124, episode reward: 65.055, mean reward:  0.065 [-23.770, 113.134], mean action: 1.626 [0.000, 3.000],  loss: 131.373435, mse: 574728.800297, mean_q: 553.614713, mean_eps: 0.828191
  57868/300000: episode: 588, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -111.577, mean reward: -1.139 [-100.000, 11.954], mean action: 1.459 [0.000, 3.000],  loss: 152.762001, mse: 606253.620855, mean_q: 563.969609, mean_eps: 0.826544
  57970/300000: episode: 589, duration: 0.688s, episode steps: 102, steps per second: 148, episode reward: -262.305, mean reward: -2.572 [-100.000,  1.985], mean action: 1.471 [0.000, 3.000],  loss: 146.749004, mse: 613457.057904, mean_q: 567.772005, mean_eps: 0.826245
  58038/300000: episode: 590, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: -157.360, mean reward: -2.314 [-100.000, 62.549], mean action: 1.765 [0.000, 3.000],  loss: 181.090902, mse: 642411.107077, mean_q: 585.638916, mean_eps: 0.825990
  58174/300000: episode: 591, duration: 0.937s, episode steps: 136, steps per second: 145, episode reward: -94.673, mean reward: -0.696 [-100.000,  8.185], mean action: 1.529 [0.000, 3.000],  loss: 171.436937, mse: 638473.543428, mean_q: 583.187110, mean_eps: 0.825684
  58309/300000: episode: 592, duration: 0.962s, episode steps: 135, steps per second: 140, episode reward: -168.035, mean reward: -1.245 [-100.000,  6.928], mean action: 1.519 [0.000, 3.000],  loss: 163.119183, mse: 671717.202778, mean_q: 606.017652, mean_eps: 0.825277
  58409/300000: episode: 593, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -129.333, mean reward: -1.293 [-100.000,  7.912], mean action: 1.390 [0.000, 3.000],  loss: 153.652820, mse: 703339.121250, mean_q: 618.431221, mean_eps: 0.824925
  58499/300000: episode: 594, duration: 0.618s, episode steps:  90, steps per second: 146, episode reward: -242.195, mean reward: -2.691 [-100.000,  7.232], mean action: 1.422 [0.000, 3.000],  loss: 162.023621, mse: 699696.494792, mean_q: 618.984652, mean_eps: 0.824640
  58635/300000: episode: 595, duration: 1.122s, episode steps: 136, steps per second: 121, episode reward: -89.284, mean reward: -0.656 [-100.000,  9.190], mean action: 1.500 [0.000, 3.000],  loss: 140.786036, mse: 705587.396369, mean_q: 618.821505, mean_eps: 0.824301
  58714/300000: episode: 596, duration: 0.644s, episode steps:  79, steps per second: 123, episode reward: -86.240, mean reward: -1.092 [-100.000, 10.408], mean action: 1.557 [0.000, 3.000],  loss: 151.971160, mse: 723791.207278, mean_q: 627.054516, mean_eps: 0.823978
  58830/300000: episode: 597, duration: 0.891s, episode steps: 116, steps per second: 130, episode reward: -294.232, mean reward: -2.536 [-100.000, 43.146], mean action: 1.603 [0.000, 3.000],  loss: 129.081271, mse: 738748.672144, mean_q: 633.710896, mean_eps: 0.823686
  58904/300000: episode: 598, duration: 0.603s, episode steps:  74, steps per second: 123, episode reward: -89.620, mean reward: -1.211 [-100.000,  6.808], mean action: 1.473 [0.000, 3.000],  loss: 156.475179, mse: 754975.850507, mean_q: 644.388891, mean_eps: 0.823400
  59023/300000: episode: 599, duration: 0.954s, episode steps: 119, steps per second: 125, episode reward: -122.434, mean reward: -1.029 [-100.000, 25.708], mean action: 1.613 [0.000, 3.000],  loss: 144.507236, mse: 776441.034139, mean_q: 659.966816, mean_eps: 0.823111
  59103/300000: episode: 600, duration: 0.648s, episode steps:  80, steps per second: 123, episode reward: -36.304, mean reward: -0.454 [-100.000, 27.082], mean action: 1.613 [0.000, 3.000],  loss: 176.955379, mse: 762488.858594, mean_q: 645.295793, mean_eps: 0.822812
  59177/300000: episode: 601, duration: 0.615s, episode steps:  74, steps per second: 120, episode reward: -94.249, mean reward: -1.274 [-100.000, 27.255], mean action: 1.689 [0.000, 3.000],  loss: 160.616712, mse: 744938.295186, mean_q: 630.333089, mean_eps: 0.822581
  59331/300000: episode: 602, duration: 1.087s, episode steps: 154, steps per second: 142, episode reward: -136.369, mean reward: -0.886 [-100.000, 16.728], mean action: 1.610 [0.000, 3.000],  loss: 191.048068, mse: 775158.544237, mean_q: 647.779595, mean_eps: 0.822240
  59435/300000: episode: 603, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: -189.604, mean reward: -1.823 [-100.000,  2.414], mean action: 1.548 [0.000, 3.000],  loss: 144.995516, mse: 793222.586238, mean_q: 665.303008, mean_eps: 0.821852
  59534/300000: episode: 604, duration: 0.713s, episode steps:  99, steps per second: 139, episode reward: -115.751, mean reward: -1.169 [-100.000, 13.894], mean action: 1.545 [0.000, 3.000],  loss: 166.715972, mse: 834994.789457, mean_q: 681.045508, mean_eps: 0.821548
  59658/300000: episode: 605, duration: 0.865s, episode steps: 124, steps per second: 143, episode reward: 22.772, mean reward:  0.184 [-100.000, 90.856], mean action: 1.782 [0.000, 3.000],  loss: 216.026445, mse: 823022.391633, mean_q: 668.343644, mean_eps: 0.821213
  59757/300000: episode: 606, duration: 0.731s, episode steps:  99, steps per second: 135, episode reward: -154.105, mean reward: -1.557 [-100.000,  5.931], mean action: 1.566 [0.000, 3.000],  loss: 194.828526, mse: 879867.357639, mean_q: 698.978198, mean_eps: 0.820879
  59892/300000: episode: 607, duration: 0.966s, episode steps: 135, steps per second: 140, episode reward: -225.599, mean reward: -1.671 [-100.000, 36.034], mean action: 1.681 [0.000, 3.000],  loss: 178.963644, mse: 860601.295370, mean_q: 691.422057, mean_eps: 0.820528
  59969/300000: episode: 608, duration: 0.529s, episode steps:  77, steps per second: 146, episode reward: -158.348, mean reward: -2.056 [-100.000, 25.195], mean action: 1.571 [0.000, 3.000],  loss: 153.229810, mse: 861937.531656, mean_q: 684.217754, mean_eps: 0.820210
  60070/300000: episode: 609, duration: 0.746s, episode steps: 101, steps per second: 135, episode reward: -154.635, mean reward: -1.531 [-100.000,  2.672], mean action: 1.653 [0.000, 3.000],  loss: 215.590267, mse: 873284.109839, mean_q: 699.205659, mean_eps: 0.819943
  60185/300000: episode: 610, duration: 0.859s, episode steps: 115, steps per second: 134, episode reward: -348.298, mean reward: -3.029 [-100.000, 10.196], mean action: 1.504 [0.000, 3.000],  loss: 191.305957, mse: 891977.758152, mean_q: 705.664345, mean_eps: 0.819619
  60342/300000: episode: 611, duration: 1.205s, episode steps: 157, steps per second: 130, episode reward: -193.863, mean reward: -1.235 [-100.000, 22.720], mean action: 1.599 [0.000, 3.000],  loss: 185.173033, mse: 923103.667994, mean_q: 712.630366, mean_eps: 0.819211
  60496/300000: episode: 612, duration: 1.136s, episode steps: 154, steps per second: 136, episode reward: -258.155, mean reward: -1.676 [-100.000,  3.633], mean action: 1.455 [0.000, 3.000],  loss: 182.426636, mse: 967656.239042, mean_q: 738.498726, mean_eps: 0.818745
  60611/300000: episode: 613, duration: 0.834s, episode steps: 115, steps per second: 138, episode reward: -116.960, mean reward: -1.017 [-100.000,  5.511], mean action: 1.600 [0.000, 3.000],  loss: 198.741693, mse: 997764.774185, mean_q: 752.911966, mean_eps: 0.818341
  60727/300000: episode: 614, duration: 0.816s, episode steps: 116, steps per second: 142, episode reward: -245.108, mean reward: -2.113 [-100.000,  2.210], mean action: 1.612 [0.000, 3.000],  loss: 191.464128, mse: 1009740.349677, mean_q: 752.413288, mean_eps: 0.817995
  60807/300000: episode: 615, duration: 0.538s, episode steps:  80, steps per second: 149, episode reward: -134.542, mean reward: -1.682 [-100.000, 15.551], mean action: 1.575 [0.000, 3.000],  loss: 201.937661, mse: 1056155.922656, mean_q: 775.837396, mean_eps: 0.817700
  60936/300000: episode: 616, duration: 0.901s, episode steps: 129, steps per second: 143, episode reward: -115.187, mean reward: -0.893 [-100.000,  7.414], mean action: 1.628 [0.000, 3.000],  loss: 191.929888, mse: 1030049.730136, mean_q: 761.318511, mean_eps: 0.817387
  61053/300000: episode: 617, duration: 0.811s, episode steps: 117, steps per second: 144, episode reward: -189.270, mean reward: -1.618 [-100.000,  2.590], mean action: 1.692 [0.000, 3.000],  loss: 175.365372, mse: 1039162.244658, mean_q: 759.328481, mean_eps: 0.817018
  61161/300000: episode: 618, duration: 0.731s, episode steps: 108, steps per second: 148, episode reward: -192.443, mean reward: -1.782 [-100.000,  4.808], mean action: 1.667 [0.000, 3.000],  loss: 170.656855, mse: 1028097.460069, mean_q: 750.295475, mean_eps: 0.816680
  61292/300000: episode: 619, duration: 0.903s, episode steps: 131, steps per second: 145, episode reward: -126.294, mean reward: -0.964 [-100.000, 15.175], mean action: 1.718 [0.000, 3.000],  loss: 165.573711, mse: 1115445.913168, mean_q: 794.054423, mean_eps: 0.816322
  61508/300000: episode: 620, duration: 1.461s, episode steps: 216, steps per second: 148, episode reward: -106.445, mean reward: -0.493 [-100.000,  7.515], mean action: 1.620 [0.000, 3.000],  loss: 182.658562, mse: 1154190.032407, mean_q: 812.455695, mean_eps: 0.815801
  61587/300000: episode: 621, duration: 0.575s, episode steps:  79, steps per second: 137, episode reward: -119.937, mean reward: -1.518 [-100.000,  9.549], mean action: 1.658 [0.000, 3.000],  loss: 187.373199, mse: 1202821.370253, mean_q: 822.696748, mean_eps: 0.815359
  61656/300000: episode: 622, duration: 0.478s, episode steps:  69, steps per second: 144, episode reward: -65.842, mean reward: -0.954 [-100.000, 17.071], mean action: 1.391 [0.000, 3.000],  loss: 206.125823, mse: 1234338.668478, mean_q: 828.592291, mean_eps: 0.815137
  61718/300000: episode: 623, duration: 0.430s, episode steps:  62, steps per second: 144, episode reward: -54.668, mean reward: -0.882 [-100.000,  8.208], mean action: 1.742 [0.000, 3.000],  loss: 210.095802, mse: 1233732.864919, mean_q: 844.643890, mean_eps: 0.814941
  61824/300000: episode: 624, duration: 0.717s, episode steps: 106, steps per second: 148, episode reward: -120.707, mean reward: -1.139 [-100.000, 12.701], mean action: 1.500 [0.000, 3.000],  loss: 189.869160, mse: 1219717.660967, mean_q: 836.528306, mean_eps: 0.814688
  61945/300000: episode: 625, duration: 0.852s, episode steps: 121, steps per second: 142, episode reward: -201.880, mean reward: -1.668 [-100.000,  1.222], mean action: 1.537 [0.000, 3.000],  loss: 198.269181, mse: 1288033.059917, mean_q: 865.839542, mean_eps: 0.814348
  62025/300000: episode: 626, duration: 0.556s, episode steps:  80, steps per second: 144, episode reward: -105.224, mean reward: -1.315 [-100.000, 10.777], mean action: 1.700 [0.000, 3.000],  loss: 199.400380, mse: 1318730.014844, mean_q: 890.709170, mean_eps: 0.814047
  62134/300000: episode: 627, duration: 0.737s, episode steps: 109, steps per second: 148, episode reward: -66.796, mean reward: -0.613 [-100.000,  8.713], mean action: 1.725 [0.000, 3.000],  loss: 210.474192, mse: 1311389.921445, mean_q: 873.647655, mean_eps: 0.813763
  62271/300000: episode: 628, duration: 0.981s, episode steps: 137, steps per second: 140, episode reward: -349.242, mean reward: -2.549 [-100.000, 68.575], mean action: 1.577 [0.000, 3.000],  loss: 223.913074, mse: 1311812.487682, mean_q: 867.599212, mean_eps: 0.813394
  62369/300000: episode: 629, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -72.561, mean reward: -0.740 [-100.000, 18.895], mean action: 1.582 [0.000, 3.000],  loss: 201.823952, mse: 1338488.189413, mean_q: 879.684907, mean_eps: 0.813041
  62488/300000: episode: 630, duration: 0.818s, episode steps: 119, steps per second: 146, episode reward: -193.436, mean reward: -1.626 [-100.000,  2.335], mean action: 1.462 [0.000, 3.000],  loss: 223.886665, mse: 1386094.826681, mean_q: 898.302500, mean_eps: 0.812716
  62591/300000: episode: 631, duration: 0.748s, episode steps: 103, steps per second: 138, episode reward: -212.562, mean reward: -2.064 [-100.000,  0.954], mean action: 1.485 [0.000, 3.000],  loss: 223.677616, mse: 1352996.239684, mean_q: 879.786410, mean_eps: 0.812383
  62750/300000: episode: 632, duration: 1.082s, episode steps: 159, steps per second: 147, episode reward: -73.091, mean reward: -0.460 [-100.000, 12.919], mean action: 1.654 [0.000, 3.000],  loss: 212.439064, mse: 1479668.895833, mean_q: 931.506963, mean_eps: 0.811990
  62849/300000: episode: 633, duration: 0.714s, episode steps:  99, steps per second: 139, episode reward: -115.575, mean reward: -1.167 [-100.000, 10.177], mean action: 1.525 [0.000, 3.000],  loss: 208.893317, mse: 1465730.183712, mean_q: 922.240665, mean_eps: 0.811603
  62939/300000: episode: 634, duration: 0.640s, episode steps:  90, steps per second: 141, episode reward: -44.216, mean reward: -0.491 [-100.000, 90.719], mean action: 1.689 [0.000, 3.000],  loss: 271.598521, mse: 1447673.776389, mean_q: 919.621827, mean_eps: 0.811319
  63022/300000: episode: 635, duration: 0.572s, episode steps:  83, steps per second: 145, episode reward: -114.489, mean reward: -1.379 [-100.000,  9.012], mean action: 1.554 [0.000, 3.000],  loss: 307.782069, mse: 1475852.582831, mean_q: 915.674341, mean_eps: 0.811060
  63099/300000: episode: 636, duration: 0.531s, episode steps:  77, steps per second: 145, episode reward: -184.760, mean reward: -2.399 [-100.000, 77.826], mean action: 1.455 [0.000, 3.000],  loss: 196.598708, mse: 1526098.113636, mean_q: 945.660520, mean_eps: 0.810820
  63228/300000: episode: 637, duration: 0.900s, episode steps: 129, steps per second: 143, episode reward: -313.545, mean reward: -2.431 [-100.000,  4.722], mean action: 1.434 [0.000, 3.000],  loss: 289.558642, mse: 1596800.327519, mean_q: 973.821222, mean_eps: 0.810511
  63327/300000: episode: 638, duration: 0.686s, episode steps:  99, steps per second: 144, episode reward: -244.546, mean reward: -2.470 [-100.000,  1.487], mean action: 1.586 [0.000, 3.000],  loss: 225.388832, mse: 1577422.993687, mean_q: 961.584104, mean_eps: 0.810169
  63434/300000: episode: 639, duration: 0.762s, episode steps: 107, steps per second: 140, episode reward: -61.506, mean reward: -0.575 [-100.000, 12.278], mean action: 1.589 [0.000, 3.000],  loss: 233.925967, mse: 1649964.386682, mean_q: 992.569586, mean_eps: 0.809860
  63515/300000: episode: 640, duration: 0.581s, episode steps:  81, steps per second: 140, episode reward: -128.190, mean reward: -1.583 [-100.000,  8.049], mean action: 1.407 [0.000, 3.000],  loss: 269.754404, mse: 1692257.686728, mean_q: 1000.394383, mean_eps: 0.809578
  63592/300000: episode: 641, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -78.309, mean reward: -1.017 [-100.000,  9.184], mean action: 1.623 [0.000, 3.000],  loss: 227.823051, mse: 1702010.004058, mean_q: 1006.383657, mean_eps: 0.809341
  63676/300000: episode: 642, duration: 0.576s, episode steps:  84, steps per second: 146, episode reward: -70.346, mean reward: -0.837 [-100.000,  7.911], mean action: 1.548 [0.000, 3.000],  loss: 317.170423, mse: 1721090.089286, mean_q: 1011.036107, mean_eps: 0.809100
  63759/300000: episode: 643, duration: 0.628s, episode steps:  83, steps per second: 132, episode reward: -173.130, mean reward: -2.086 [-100.000, 10.735], mean action: 1.542 [0.000, 3.000],  loss: 279.712358, mse: 1730764.358434, mean_q: 1021.551793, mean_eps: 0.808849
  63848/300000: episode: 644, duration: 0.616s, episode steps:  89, steps per second: 145, episode reward: -353.709, mean reward: -3.974 [-100.000,  0.412], mean action: 1.607 [0.000, 3.000],  loss: 266.471409, mse: 1799755.504916, mean_q: 1042.289473, mean_eps: 0.808591
  63924/300000: episode: 645, duration: 0.532s, episode steps:  76, steps per second: 143, episode reward: -84.830, mean reward: -1.116 [-100.000, 36.853], mean action: 1.632 [0.000, 3.000],  loss: 287.575145, mse: 1815224.192434, mean_q: 1048.733228, mean_eps: 0.808343
  64013/300000: episode: 646, duration: 0.633s, episode steps:  89, steps per second: 141, episode reward: -266.761, mean reward: -2.997 [-100.000, 44.957], mean action: 1.494 [0.000, 3.000],  loss: 254.755335, mse: 1835804.448034, mean_q: 1046.646754, mean_eps: 0.808096
  64124/300000: episode: 647, duration: 0.813s, episode steps: 111, steps per second: 137, episode reward: -67.737, mean reward: -0.610 [-100.000,  9.278], mean action: 1.658 [0.000, 3.000],  loss: 277.858967, mse: 1874752.470721, mean_q: 1068.593374, mean_eps: 0.807796
  64219/300000: episode: 648, duration: 0.683s, episode steps:  95, steps per second: 139, episode reward: -186.701, mean reward: -1.965 [-100.000,  1.635], mean action: 1.768 [0.000, 3.000],  loss: 285.111727, mse: 1896727.046711, mean_q: 1067.960418, mean_eps: 0.807487
  64313/300000: episode: 649, duration: 0.676s, episode steps:  94, steps per second: 139, episode reward: -310.059, mean reward: -3.299 [-100.000,  0.700], mean action: 1.809 [0.000, 3.000],  loss: 1112.736470, mse: 1919572.219415, mean_q: 1084.084464, mean_eps: 0.807203
  64403/300000: episode: 650, duration: 0.673s, episode steps:  90, steps per second: 134, episode reward: -103.189, mean reward: -1.147 [-100.000,  7.692], mean action: 1.478 [0.000, 3.000],  loss: 325.298121, mse: 1977038.388194, mean_q: 1096.091719, mean_eps: 0.806928
  64525/300000: episode: 651, duration: 0.852s, episode steps: 122, steps per second: 143, episode reward: -70.677, mean reward: -0.579 [-100.000, 14.166], mean action: 1.467 [0.000, 3.000],  loss: 840.101041, mse: 2031955.700820, mean_q: 1122.471786, mean_eps: 0.806609
  64645/300000: episode: 652, duration: 0.848s, episode steps: 120, steps per second: 141, episode reward: -107.707, mean reward: -0.898 [-100.000,  8.361], mean action: 1.583 [0.000, 3.000],  loss: 321.932693, mse: 2043349.089583, mean_q: 1119.563673, mean_eps: 0.806247
  64763/300000: episode: 653, duration: 0.828s, episode steps: 118, steps per second: 143, episode reward: -165.496, mean reward: -1.403 [-100.000, 39.750], mean action: 1.627 [0.000, 3.000],  loss: 307.904629, mse: 2087742.242585, mean_q: 1140.170659, mean_eps: 0.805889
  64851/300000: episode: 654, duration: 0.598s, episode steps:  88, steps per second: 147, episode reward: -36.504, mean reward: -0.415 [-100.000, 11.216], mean action: 1.705 [0.000, 3.000],  loss: 873.406385, mse: 2154687.928977, mean_q: 1153.786122, mean_eps: 0.805580
  64952/300000: episode: 655, duration: 0.715s, episode steps: 101, steps per second: 141, episode reward: -93.987, mean reward: -0.931 [-100.000,  7.980], mean action: 1.594 [0.000, 3.000],  loss: 873.042214, mse: 2123511.316832, mean_q: 1146.076044, mean_eps: 0.805297
  65075/300000: episode: 656, duration: 0.857s, episode steps: 123, steps per second: 144, episode reward: -232.806, mean reward: -1.893 [-100.000,  2.793], mean action: 1.569 [0.000, 3.000],  loss: 320.867739, mse: 2238639.815041, mean_q: 1184.288343, mean_eps: 0.804961
  65160/300000: episode: 657, duration: 0.642s, episode steps:  85, steps per second: 132, episode reward: -188.186, mean reward: -2.214 [-100.000,  8.251], mean action: 1.400 [0.000, 3.000],  loss: 319.103168, mse: 2161864.719118, mean_q: 1143.505828, mean_eps: 0.804649
  65343/300000: episode: 658, duration: 1.409s, episode steps: 183, steps per second: 130, episode reward: -131.078, mean reward: -0.716 [-100.000, 11.525], mean action: 1.656 [0.000, 3.000],  loss: 355.022464, mse: 2255550.913251, mean_q: 1176.848686, mean_eps: 0.804247
  65415/300000: episode: 659, duration: 0.512s, episode steps:  72, steps per second: 141, episode reward: -70.217, mean reward: -0.975 [-100.000,  7.361], mean action: 1.569 [0.000, 3.000],  loss: 345.266030, mse: 2354184.809028, mean_q: 1205.851974, mean_eps: 0.803864
  65518/300000: episode: 660, duration: 0.746s, episode steps: 103, steps per second: 138, episode reward: -58.187, mean reward: -0.565 [-100.000, 11.595], mean action: 1.534 [0.000, 3.000],  loss: 400.240504, mse: 2462286.061893, mean_q: 1253.798310, mean_eps: 0.803602
  65651/300000: episode: 661, duration: 1.057s, episode steps: 133, steps per second: 126, episode reward: -187.953, mean reward: -1.413 [-100.000,  6.813], mean action: 1.632 [0.000, 3.000],  loss: 398.645937, mse: 2447392.143797, mean_q: 1244.856693, mean_eps: 0.803248
  65710/300000: episode: 662, duration: 0.459s, episode steps:  59, steps per second: 129, episode reward: -100.106, mean reward: -1.697 [-100.000, 34.004], mean action: 1.492 [0.000, 3.000],  loss: 380.193606, mse: 2499240.487288, mean_q: 1271.158848, mean_eps: 0.802960
  65840/300000: episode: 663, duration: 0.943s, episode steps: 130, steps per second: 138, episode reward: -87.119, mean reward: -0.670 [-100.000,  8.730], mean action: 1.646 [0.000, 3.000],  loss: 388.961172, mse: 2521581.747115, mean_q: 1262.067946, mean_eps: 0.802677
  65937/300000: episode: 664, duration: 0.727s, episode steps:  97, steps per second: 134, episode reward: -239.844, mean reward: -2.473 [-100.000, 83.586], mean action: 1.515 [0.000, 3.000],  loss: 383.600435, mse: 2534489.304124, mean_q: 1263.100111, mean_eps: 0.802336
  66058/300000: episode: 665, duration: 0.874s, episode steps: 121, steps per second: 138, episode reward: -266.535, mean reward: -2.203 [-100.000, 24.438], mean action: 1.537 [0.000, 3.000],  loss: 379.349084, mse: 2589625.543388, mean_q: 1279.235204, mean_eps: 0.802009
  66187/300000: episode: 666, duration: 0.905s, episode steps: 129, steps per second: 143, episode reward: -169.925, mean reward: -1.317 [-100.000,  6.422], mean action: 1.628 [0.000, 3.000],  loss: 456.411496, mse: 2678877.055233, mean_q: 1313.454010, mean_eps: 0.801634
  66315/300000: episode: 667, duration: 0.911s, episode steps: 128, steps per second: 141, episode reward: -300.044, mean reward: -2.344 [-100.000, 11.130], mean action: 1.578 [0.000, 3.000],  loss: 476.706175, mse: 2840617.528320, mean_q: 1352.949539, mean_eps: 0.801249
  66410/300000: episode: 668, duration: 0.736s, episode steps:  95, steps per second: 129, episode reward: -235.016, mean reward: -2.474 [-100.000,  6.252], mean action: 1.632 [0.000, 3.000],  loss: 1234.661672, mse: 2814970.319737, mean_q: 1336.393560, mean_eps: 0.800914
  66484/300000: episode: 669, duration: 0.558s, episode steps:  74, steps per second: 133, episode reward: -265.706, mean reward: -3.591 [-100.000,  4.951], mean action: 1.446 [0.000, 3.000],  loss: 452.074366, mse: 2863075.716216, mean_q: 1355.793945, mean_eps: 0.800660
  66581/300000: episode: 670, duration: 0.694s, episode steps:  97, steps per second: 140, episode reward: -54.152, mean reward: -0.558 [-100.000, 10.360], mean action: 1.464 [0.000, 3.000],  loss: 403.700466, mse: 2932018.284794, mean_q: 1374.546213, mean_eps: 0.800404
  66714/300000: episode: 671, duration: 1.065s, episode steps: 133, steps per second: 125, episode reward: -184.061, mean reward: -1.384 [-100.000,  5.355], mean action: 1.699 [0.000, 3.000],  loss: 865.115183, mse: 2883536.185150, mean_q: 1351.132597, mean_eps: 0.800059
  66799/300000: episode: 672, duration: 0.648s, episode steps:  85, steps per second: 131, episode reward: -196.823, mean reward: -2.316 [-100.000,  5.754], mean action: 1.576 [0.000, 3.000],  loss: 608.890308, mse: 2983809.066176, mean_q: 1390.542308, mean_eps: 0.799732
  66952/300000: episode: 673, duration: 1.113s, episode steps: 153, steps per second: 137, episode reward: -359.225, mean reward: -2.348 [-100.000, 42.266], mean action: 1.562 [0.000, 3.000],  loss: 646.265055, mse: 3030821.965686, mean_q: 1386.277769, mean_eps: 0.799375
  67030/300000: episode: 674, duration: 0.571s, episode steps:  78, steps per second: 137, episode reward: -83.204, mean reward: -1.067 [-100.000, 16.582], mean action: 1.872 [0.000, 3.000],  loss: 460.986154, mse: 3007446.735577, mean_q: 1383.196511, mean_eps: 0.799028
  67106/300000: episode: 675, duration: 0.525s, episode steps:  76, steps per second: 145, episode reward: -250.303, mean reward: -3.293 [-100.000, 22.682], mean action: 1.566 [0.000, 3.000],  loss: 608.031394, mse: 2985570.623355, mean_q: 1378.347394, mean_eps: 0.798797
  67199/300000: episode: 676, duration: 0.637s, episode steps:  93, steps per second: 146, episode reward: -214.397, mean reward: -2.305 [-100.000, 39.023], mean action: 1.419 [0.000, 3.000],  loss: 1159.807698, mse: 3024732.258065, mean_q: 1389.363797, mean_eps: 0.798544
  67294/300000: episode: 677, duration: 0.707s, episode steps:  95, steps per second: 134, episode reward: -246.237, mean reward: -2.592 [-100.000, 50.842], mean action: 1.705 [0.000, 3.000],  loss: 834.749454, mse: 3221620.917105, mean_q: 1451.369864, mean_eps: 0.798262
  67394/300000: episode: 678, duration: 0.706s, episode steps: 100, steps per second: 142, episode reward: -45.420, mean reward: -0.454 [-100.000,  6.821], mean action: 1.600 [0.000, 3.000],  loss: 663.915305, mse: 3376926.036250, mean_q: 1479.159674, mean_eps: 0.797970
  67550/300000: episode: 679, duration: 1.080s, episode steps: 156, steps per second: 144, episode reward: -193.770, mean reward: -1.242 [-100.000, 12.653], mean action: 1.449 [0.000, 3.000],  loss: 649.788173, mse: 3322738.432692, mean_q: 1454.934593, mean_eps: 0.797586
  67641/300000: episode: 680, duration: 0.691s, episode steps:  91, steps per second: 132, episode reward: -300.692, mean reward: -3.304 [-100.000,  0.172], mean action: 1.626 [0.000, 3.000],  loss: 569.010046, mse: 3339143.453297, mean_q: 1459.887730, mean_eps: 0.797215
  67782/300000: episode: 681, duration: 1.196s, episode steps: 141, steps per second: 118, episode reward: -120.587, mean reward: -0.855 [-100.000,  5.750], mean action: 1.645 [0.000, 3.000],  loss: 503.963476, mse: 3446279.015071, mean_q: 1490.398377, mean_eps: 0.796867
  67855/300000: episode: 682, duration: 0.612s, episode steps:  73, steps per second: 119, episode reward: -152.616, mean reward: -2.091 [-100.000,  5.768], mean action: 1.562 [0.000, 3.000],  loss: 555.563638, mse: 3546032.683219, mean_q: 1519.102412, mean_eps: 0.796546
  67980/300000: episode: 683, duration: 1.095s, episode steps: 125, steps per second: 114, episode reward: -219.122, mean reward: -1.753 [-100.000,  3.754], mean action: 1.768 [0.000, 3.000],  loss: 517.844793, mse: 3498303.501000, mean_q: 1496.992818, mean_eps: 0.796249
  68101/300000: episode: 684, duration: 1.143s, episode steps: 121, steps per second: 106, episode reward: -195.983, mean reward: -1.620 [-100.000,  4.834], mean action: 1.678 [0.000, 3.000],  loss: 585.405212, mse: 3664742.227273, mean_q: 1547.726661, mean_eps: 0.795880
  68195/300000: episode: 685, duration: 0.779s, episode steps:  94, steps per second: 121, episode reward: -243.805, mean reward: -2.594 [-100.000,  1.597], mean action: 1.723 [0.000, 3.000],  loss: 519.338224, mse: 3815452.521277, mean_q: 1592.277600, mean_eps: 0.795558
  68284/300000: episode: 686, duration: 0.721s, episode steps:  89, steps per second: 123, episode reward: -78.839, mean reward: -0.886 [-100.000,  8.963], mean action: 1.640 [0.000, 3.000],  loss: 674.310353, mse: 3750479.525281, mean_q: 1552.987303, mean_eps: 0.795283
  68364/300000: episode: 687, duration: 0.597s, episode steps:  80, steps per second: 134, episode reward: -186.189, mean reward: -2.327 [-100.000,  6.476], mean action: 1.512 [0.000, 3.000],  loss: 527.708419, mse: 4011800.478125, mean_q: 1637.604099, mean_eps: 0.795029
  68487/300000: episode: 688, duration: 0.908s, episode steps: 123, steps per second: 135, episode reward: -383.353, mean reward: -3.117 [-100.000,  3.073], mean action: 1.699 [0.000, 3.000],  loss: 594.972498, mse: 3958082.991870, mean_q: 1614.364371, mean_eps: 0.794725
  68578/300000: episode: 689, duration: 0.650s, episode steps:  91, steps per second: 140, episode reward: -322.759, mean reward: -3.547 [-100.000,  0.211], mean action: 1.615 [0.000, 3.000],  loss: 784.084380, mse: 4162983.225275, mean_q: 1664.067377, mean_eps: 0.794404
  68666/300000: episode: 690, duration: 0.638s, episode steps:  88, steps per second: 138, episode reward: -120.259, mean reward: -1.367 [-100.000, 15.177], mean action: 1.614 [0.000, 3.000],  loss: 1243.681404, mse: 4062797.605114, mean_q: 1645.653677, mean_eps: 0.794136
  68756/300000: episode: 691, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -58.347, mean reward: -0.648 [-100.000, 12.704], mean action: 1.611 [0.000, 3.000],  loss: 763.958106, mse: 4034639.788889, mean_q: 1625.405035, mean_eps: 0.793869
  69017/300000: episode: 692, duration: 1.813s, episode steps: 261, steps per second: 144, episode reward: -105.176, mean reward: -0.403 [-100.000, 37.465], mean action: 1.609 [0.000, 3.000],  loss: 577.849592, mse: 4316111.159004, mean_q: 1689.808565, mean_eps: 0.793342
  69096/300000: episode: 693, duration: 0.543s, episode steps:  79, steps per second: 146, episode reward: -260.106, mean reward: -3.292 [-100.000,  6.950], mean action: 1.532 [0.000, 3.000],  loss: 805.806616, mse: 4507329.148734, mean_q: 1737.135983, mean_eps: 0.792832
  69193/300000: episode: 694, duration: 0.724s, episode steps:  97, steps per second: 134, episode reward: -418.204, mean reward: -4.311 [-100.000, -0.291], mean action: 1.629 [0.000, 3.000],  loss: 883.289297, mse: 4560937.721649, mean_q: 1744.287604, mean_eps: 0.792568
  69290/300000: episode: 695, duration: 0.759s, episode steps:  97, steps per second: 128, episode reward: -101.876, mean reward: -1.050 [-100.000, 15.400], mean action: 1.804 [0.000, 3.000],  loss: 814.944594, mse: 4621563.938144, mean_q: 1750.509253, mean_eps: 0.792277
  69456/300000: episode: 696, duration: 1.231s, episode steps: 166, steps per second: 135, episode reward: -36.242, mean reward: -0.218 [-100.000, 67.797], mean action: 1.500 [0.000, 3.000],  loss: 727.540243, mse: 4946955.739458, mean_q: 1834.239423, mean_eps: 0.791883
  69600/300000: episode: 697, duration: 1.087s, episode steps: 144, steps per second: 132, episode reward: -133.264, mean reward: -0.925 [-100.000,  9.148], mean action: 1.562 [0.000, 3.000],  loss: 639.529272, mse: 4903532.755208, mean_q: 1812.361632, mean_eps: 0.791417
  69662/300000: episode: 698, duration: 0.473s, episode steps:  62, steps per second: 131, episode reward: -78.095, mean reward: -1.260 [-100.000,  6.000], mean action: 1.532 [0.000, 3.000],  loss: 834.610807, mse: 5012663.447581, mean_q: 1848.904821, mean_eps: 0.791109
  69769/300000: episode: 699, duration: 0.821s, episode steps: 107, steps per second: 130, episode reward: -77.827, mean reward: -0.727 [-100.000,  6.779], mean action: 1.636 [0.000, 3.000],  loss: 973.195595, mse: 5109519.212617, mean_q: 1866.213101, mean_eps: 0.790855
  69874/300000: episode: 700, duration: 0.765s, episode steps: 105, steps per second: 137, episode reward: -272.099, mean reward: -2.591 [-100.000,  1.242], mean action: 1.714 [0.000, 3.000],  loss: 1044.483372, mse: 5201780.469048, mean_q: 1873.682608, mean_eps: 0.790537
  70006/300000: episode: 701, duration: 0.906s, episode steps: 132, steps per second: 146, episode reward: -394.810, mean reward: -2.991 [-100.000,  1.589], mean action: 1.576 [0.000, 3.000],  loss: 948.454605, mse: 5281583.774621, mean_q: 1888.308197, mean_eps: 0.790181
  70090/300000: episode: 702, duration: 0.568s, episode steps:  84, steps per second: 148, episode reward: -201.623, mean reward: -2.400 [-100.000,  4.894], mean action: 1.524 [0.000, 3.000],  loss: 1097.965494, mse: 5561828.815476, mean_q: 1948.212443, mean_eps: 0.789857
  70169/300000: episode: 703, duration: 0.551s, episode steps:  79, steps per second: 143, episode reward: -208.215, mean reward: -2.636 [-100.000, 16.285], mean action: 1.582 [0.000, 3.000],  loss: 1064.245570, mse: 5699018.946203, mean_q: 1946.152290, mean_eps: 0.789613
  70265/300000: episode: 704, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -86.558, mean reward: -0.902 [-100.000, 18.095], mean action: 1.646 [0.000, 3.000],  loss: 936.138730, mse: 6065807.333333, mean_q: 2048.735475, mean_eps: 0.789350
  70411/300000: episode: 705, duration: 0.987s, episode steps: 146, steps per second: 148, episode reward: -200.085, mean reward: -1.370 [-100.000,  7.813], mean action: 1.822 [0.000, 3.000],  loss: 1011.734995, mse: 5856100.558219, mean_q: 1987.944411, mean_eps: 0.788987
  70521/300000: episode: 706, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: -292.421, mean reward: -2.658 [-100.000,  0.626], mean action: 1.618 [0.000, 3.000],  loss: 1551.217957, mse: 6260357.940909, mean_q: 2059.873048, mean_eps: 0.788604
  70645/300000: episode: 707, duration: 0.841s, episode steps: 124, steps per second: 148, episode reward: -29.834, mean reward: -0.241 [-100.000, 94.079], mean action: 1.484 [0.000, 3.000],  loss: 1199.700434, mse: 6361947.512097, mean_q: 2075.452489, mean_eps: 0.788253
  70767/300000: episode: 708, duration: 0.862s, episode steps: 122, steps per second: 142, episode reward: -293.161, mean reward: -2.403 [-100.000,  8.003], mean action: 1.598 [0.000, 3.000],  loss: 1129.181314, mse: 6760205.881148, mean_q: 2140.053175, mean_eps: 0.787883
  70879/300000: episode: 709, duration: 0.816s, episode steps: 112, steps per second: 137, episode reward: -156.769, mean reward: -1.400 [-100.000,  7.977], mean action: 1.634 [0.000, 3.000],  loss: 869.543444, mse: 6644478.685268, mean_q: 2109.565376, mean_eps: 0.787533
  70967/300000: episode: 710, duration: 0.591s, episode steps:  88, steps per second: 149, episode reward: -270.081, mean reward: -3.069 [-100.000,  5.527], mean action: 1.489 [0.000, 3.000],  loss: 998.944945, mse: 6738043.812500, mean_q: 2127.022383, mean_eps: 0.787233
  71074/300000: episode: 711, duration: 0.755s, episode steps: 107, steps per second: 142, episode reward: -325.491, mean reward: -3.042 [-100.000,  1.366], mean action: 1.710 [0.000, 3.000],  loss: 1360.273364, mse: 6979321.831776, mean_q: 2172.561549, mean_eps: 0.786940
  71205/300000: episode: 712, duration: 0.912s, episode steps: 131, steps per second: 144, episode reward: -178.096, mean reward: -1.360 [-100.000,  4.089], mean action: 1.603 [0.000, 3.000],  loss: 1131.986326, mse: 7130779.358779, mean_q: 2181.318549, mean_eps: 0.786583
  71301/300000: episode: 713, duration: 0.669s, episode steps:  96, steps per second: 144, episode reward: -370.596, mean reward: -3.860 [-100.000, -0.410], mean action: 1.552 [0.000, 3.000],  loss: 1802.945946, mse: 7230270.200521, mean_q: 2198.575703, mean_eps: 0.786242
  71470/300000: episode: 714, duration: 1.167s, episode steps: 169, steps per second: 145, episode reward: -145.337, mean reward: -0.860 [-100.000, 41.049], mean action: 1.627 [0.000, 3.000],  loss: 1432.067722, mse: 7599871.269231, mean_q: 2264.485588, mean_eps: 0.785845
  71556/300000: episode: 715, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -13.173, mean reward: -0.153 [-100.000, 12.064], mean action: 1.430 [0.000, 3.000],  loss: 1479.665858, mse: 7514928.191860, mean_q: 2240.072639, mean_eps: 0.785462
  71644/300000: episode: 716, duration: 0.597s, episode steps:  88, steps per second: 147, episode reward: -96.236, mean reward: -1.094 [-100.000,  8.654], mean action: 1.591 [0.000, 3.000],  loss: 1488.169525, mse: 7765897.761364, mean_q: 2287.597692, mean_eps: 0.785202
  71769/300000: episode: 717, duration: 0.862s, episode steps: 125, steps per second: 145, episode reward: -25.889, mean reward: -0.207 [-100.000, 35.190], mean action: 1.648 [0.000, 3.000],  loss: 2754.836616, mse: 8023601.580000, mean_q: 2346.505393, mean_eps: 0.784882
  71879/300000: episode: 718, duration: 0.739s, episode steps: 110, steps per second: 149, episode reward: -26.565, mean reward: -0.241 [-100.000, 94.686], mean action: 1.482 [0.000, 3.000],  loss: 2009.611746, mse: 8172204.527273, mean_q: 2363.302165, mean_eps: 0.784530
  71958/300000: episode: 719, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -159.705, mean reward: -2.022 [-100.000,  9.155], mean action: 1.658 [0.000, 3.000],  loss: 3399.852652, mse: 8546519.050633, mean_q: 2412.219155, mean_eps: 0.784246
  72106/300000: episode: 720, duration: 1.003s, episode steps: 148, steps per second: 148, episode reward: -361.147, mean reward: -2.440 [-100.000,  2.617], mean action: 1.514 [0.000, 3.000],  loss: 1488.681965, mse: 8659568.837838, mean_q: 2411.230968, mean_eps: 0.783905
  72181/300000: episode: 721, duration: 0.495s, episode steps:  75, steps per second: 151, episode reward: -37.338, mean reward: -0.498 [-100.000, 17.695], mean action: 1.707 [0.000, 3.000],  loss: 1419.721658, mse: 9031901.220000, mean_q: 2470.731903, mean_eps: 0.783571
  72305/300000: episode: 722, duration: 0.837s, episode steps: 124, steps per second: 148, episode reward: -256.893, mean reward: -2.072 [-100.000,  3.505], mean action: 1.516 [0.000, 3.000],  loss: 1815.655909, mse: 9011215.068548, mean_q: 2448.334878, mean_eps: 0.783273
  72448/300000: episode: 723, duration: 0.983s, episode steps: 143, steps per second: 145, episode reward: -83.914, mean reward: -0.587 [-100.000, 19.736], mean action: 1.657 [0.000, 3.000],  loss: 1778.474455, mse: 9479153.314685, mean_q: 2529.901956, mean_eps: 0.782872
  72526/300000: episode: 724, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -84.078, mean reward: -1.078 [-100.000,  9.630], mean action: 1.474 [0.000, 3.000],  loss: 2063.186366, mse: 9399183.833333, mean_q: 2502.412109, mean_eps: 0.782541
  72643/300000: episode: 725, duration: 0.804s, episode steps: 117, steps per second: 146, episode reward: -332.565, mean reward: -2.842 [-100.000,  1.381], mean action: 1.547 [0.000, 3.000],  loss: 2675.365446, mse: 9955967.307692, mean_q: 2590.256093, mean_eps: 0.782248
  72741/300000: episode: 726, duration: 0.688s, episode steps:  98, steps per second: 142, episode reward: -123.369, mean reward: -1.259 [-100.000, 12.203], mean action: 1.561 [0.000, 3.000],  loss: 1371.591226, mse: 10452235.668367, mean_q: 2663.003782, mean_eps: 0.781925
  72813/300000: episode: 727, duration: 0.484s, episode steps:  72, steps per second: 149, episode reward: -87.536, mean reward: -1.216 [-100.000,  9.295], mean action: 1.486 [0.000, 3.000],  loss: 6408.303847, mse: 10200363.034722, mean_q: 2601.128257, mean_eps: 0.781671
  72937/300000: episode: 728, duration: 0.832s, episode steps: 124, steps per second: 149, episode reward: -71.697, mean reward: -0.578 [-100.000,  7.619], mean action: 1.685 [0.000, 3.000],  loss: 1678.082455, mse: 10588424.185484, mean_q: 2651.375791, mean_eps: 0.781377
  73111/300000: episode: 729, duration: 1.242s, episode steps: 174, steps per second: 140, episode reward: -125.251, mean reward: -0.720 [-100.000, 12.733], mean action: 1.563 [0.000, 3.000],  loss: 2174.380482, mse: 11223499.775862, mean_q: 2747.404560, mean_eps: 0.780929
  73198/300000: episode: 730, duration: 0.616s, episode steps:  87, steps per second: 141, episode reward: -272.360, mean reward: -3.131 [-100.000,  2.307], mean action: 1.655 [0.000, 3.000],  loss: 1696.514848, mse: 11129793.448276, mean_q: 2729.273066, mean_eps: 0.780538
  73272/300000: episode: 731, duration: 0.587s, episode steps:  74, steps per second: 126, episode reward: -59.696, mean reward: -0.807 [-100.000, 20.104], mean action: 1.730 [0.000, 3.000],  loss: 2302.100361, mse: 11367709.635135, mean_q: 2744.169232, mean_eps: 0.780296
  73461/300000: episode: 732, duration: 1.383s, episode steps: 189, steps per second: 137, episode reward: -186.870, mean reward: -0.989 [-100.000,  5.469], mean action: 1.619 [0.000, 3.000],  loss: 2306.827068, mse: 12187818.089947, mean_q: 2835.218815, mean_eps: 0.779902
  73566/300000: episode: 733, duration: 0.734s, episode steps: 105, steps per second: 143, episode reward: -317.915, mean reward: -3.028 [-100.000,  0.848], mean action: 1.676 [0.000, 3.000],  loss: 2058.627852, mse: 12635216.147619, mean_q: 2889.721673, mean_eps: 0.779461
  73646/300000: episode: 734, duration: 0.595s, episode steps:  80, steps per second: 134, episode reward: 38.035, mean reward:  0.475 [-100.000, 100.762], mean action: 1.562 [0.000, 3.000],  loss: 1875.384100, mse: 12952666.218750, mean_q: 2946.333350, mean_eps: 0.779184
  73725/300000: episode: 735, duration: 0.571s, episode steps:  79, steps per second: 138, episode reward: -14.621, mean reward: -0.185 [-100.000, 10.334], mean action: 1.646 [0.000, 3.000],  loss: 3107.359926, mse: 13472202.544304, mean_q: 2979.773722, mean_eps: 0.778945
  73930/300000: episode: 736, duration: 1.458s, episode steps: 205, steps per second: 141, episode reward: -335.867, mean reward: -1.638 [-100.000,  2.376], mean action: 1.610 [0.000, 3.000],  loss: 2964.020432, mse: 13585518.273171, mean_q: 3005.500370, mean_eps: 0.778519
  74025/300000: episode: 737, duration: 0.645s, episode steps:  95, steps per second: 147, episode reward: -212.985, mean reward: -2.242 [-100.000,  1.495], mean action: 1.600 [0.000, 3.000],  loss: 2264.093596, mse: 15058358.568421, mean_q: 3167.235205, mean_eps: 0.778069
  74147/300000: episode: 738, duration: 0.811s, episode steps: 122, steps per second: 150, episode reward: -198.745, mean reward: -1.629 [-100.000,  1.466], mean action: 1.672 [0.000, 3.000],  loss: 2537.071673, mse: 14966949.959016, mean_q: 3176.276435, mean_eps: 0.777743
  74246/300000: episode: 739, duration: 0.677s, episode steps:  99, steps per second: 146, episode reward: -157.113, mean reward: -1.587 [-100.000, 10.809], mean action: 1.657 [0.000, 3.000],  loss: 2569.285943, mse: 15275559.484848, mean_q: 3211.367565, mean_eps: 0.777412
  74344/300000: episode: 740, duration: 0.653s, episode steps:  98, steps per second: 150, episode reward: -116.234, mean reward: -1.186 [-100.000,  8.031], mean action: 1.592 [0.000, 3.000],  loss: 2652.327592, mse: 15687055.704082, mean_q: 3226.576030, mean_eps: 0.777116
  74522/300000: episode: 741, duration: 1.234s, episode steps: 178, steps per second: 144, episode reward: -213.062, mean reward: -1.197 [-100.000, 25.087], mean action: 1.573 [0.000, 3.000],  loss: 3004.141604, mse: 16416465.165730, mean_q: 3297.970013, mean_eps: 0.776702
  74593/300000: episode: 742, duration: 0.501s, episode steps:  71, steps per second: 142, episode reward: -31.411, mean reward: -0.442 [-100.000, 17.136], mean action: 1.507 [0.000, 3.000],  loss: 3363.878243, mse: 17304615.549296, mean_q: 3409.470033, mean_eps: 0.776329
  74691/300000: episode: 743, duration: 0.650s, episode steps:  98, steps per second: 151, episode reward: -217.652, mean reward: -2.221 [-100.000,  1.597], mean action: 1.500 [0.000, 3.000],  loss: 2339.394531, mse: 17288275.938776, mean_q: 3367.082191, mean_eps: 0.776075
  74785/300000: episode: 744, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -211.617, mean reward: -2.251 [-100.000, 26.676], mean action: 1.883 [0.000, 3.000],  loss: 4334.600289, mse: 17534148.393617, mean_q: 3438.763539, mean_eps: 0.775787
  74928/300000: episode: 745, duration: 1.003s, episode steps: 143, steps per second: 143, episode reward: -454.348, mean reward: -3.177 [-100.000,  1.940], mean action: 1.594 [0.000, 3.000],  loss: 3534.893469, mse: 18846084.468531, mean_q: 3544.350630, mean_eps: 0.775432
  75037/300000: episode: 746, duration: 0.716s, episode steps: 109, steps per second: 152, episode reward: -135.325, mean reward: -1.242 [-100.000,  9.512], mean action: 1.661 [0.000, 3.000],  loss: 4417.084279, mse: 18834444.981651, mean_q: 3505.954930, mean_eps: 0.775054
  75125/300000: episode: 747, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: -414.955, mean reward: -4.715 [-100.000,  0.408], mean action: 1.784 [0.000, 3.000],  loss: 6852.461365, mse: 19780450.181818, mean_q: 3609.863842, mean_eps: 0.774759
  75280/300000: episode: 748, duration: 1.097s, episode steps: 155, steps per second: 141, episode reward: -137.863, mean reward: -0.889 [-100.000,  6.260], mean action: 1.639 [0.000, 3.000],  loss: 4277.908250, mse: 21098915.858065, mean_q: 3736.009123, mean_eps: 0.774394
  75405/300000: episode: 749, duration: 0.861s, episode steps: 125, steps per second: 145, episode reward: -152.614, mean reward: -1.221 [-100.000, 12.352], mean action: 1.624 [0.000, 3.000],  loss: 4945.180789, mse: 21793833.552000, mean_q: 3779.841564, mean_eps: 0.773974
  75518/300000: episode: 750, duration: 0.803s, episode steps: 113, steps per second: 141, episode reward: -560.006, mean reward: -4.956 [-100.000, 63.909], mean action: 1.673 [0.000, 3.000],  loss: 3927.814991, mse: 22057686.407080, mean_q: 3789.076429, mean_eps: 0.773617
  75616/300000: episode: 751, duration: 0.646s, episode steps:  98, steps per second: 152, episode reward: -240.087, mean reward: -2.450 [-100.000,  0.793], mean action: 1.500 [0.000, 3.000],  loss: 7330.742860, mse: 23270949.183673, mean_q: 3945.723164, mean_eps: 0.773300
  75724/300000: episode: 752, duration: 0.726s, episode steps: 108, steps per second: 149, episode reward: -190.365, mean reward: -1.763 [-100.000,  1.804], mean action: 1.556 [0.000, 3.000],  loss: 5685.787222, mse: 23730910.425926, mean_q: 3930.590079, mean_eps: 0.772992
  75911/300000: episode: 753, duration: 1.270s, episode steps: 187, steps per second: 147, episode reward: -418.528, mean reward: -2.238 [-100.000,  2.427], mean action: 1.663 [0.000, 3.000],  loss: 3847.705008, mse: 25151005.572193, mean_q: 4059.358165, mean_eps: 0.772549
  76000/300000: episode: 754, duration: 0.621s, episode steps:  89, steps per second: 143, episode reward: -55.182, mean reward: -0.620 [-100.000, 25.358], mean action: 1.809 [0.000, 3.000],  loss: 4496.452096, mse: 26652181.831461, mean_q: 4207.977048, mean_eps: 0.772135
  76114/300000: episode: 755, duration: 0.804s, episode steps: 114, steps per second: 142, episode reward: -221.945, mean reward: -1.947 [-100.000,  2.117], mean action: 1.561 [0.000, 3.000],  loss: 5802.009723, mse: 27432315.780702, mean_q: 4245.497923, mean_eps: 0.771830
  76185/300000: episode: 756, duration: 0.473s, episode steps:  71, steps per second: 150, episode reward: -115.358, mean reward: -1.625 [-100.000,  5.304], mean action: 1.704 [0.000, 3.000],  loss: 3867.614633, mse: 28398422.464789, mean_q: 4303.132307, mean_eps: 0.771553
  76311/300000: episode: 757, duration: 0.858s, episode steps: 126, steps per second: 147, episode reward: -219.548, mean reward: -1.742 [-100.000, 36.835], mean action: 1.611 [0.000, 3.000],  loss: 4914.653259, mse: 27581845.380952, mean_q: 4213.179174, mean_eps: 0.771258
  76452/300000: episode: 758, duration: 0.975s, episode steps: 141, steps per second: 145, episode reward: -285.191, mean reward: -2.023 [-100.000,  7.161], mean action: 1.667 [0.000, 3.000],  loss: 5303.118710, mse: 30741100.425532, mean_q: 4454.162421, mean_eps: 0.770857
  76565/300000: episode: 759, duration: 0.764s, episode steps: 113, steps per second: 148, episode reward: -238.938, mean reward: -2.114 [-100.000,  6.090], mean action: 1.442 [0.000, 3.000],  loss: 7763.856837, mse: 30850268.017699, mean_q: 4472.331003, mean_eps: 0.770476
  76695/300000: episode: 760, duration: 0.937s, episode steps: 130, steps per second: 139, episode reward: -84.681, mean reward: -0.651 [-100.000,  6.965], mean action: 1.623 [0.000, 3.000],  loss: 5774.899207, mse: 32970471.923077, mean_q: 4623.976549, mean_eps: 0.770111
  76867/300000: episode: 761, duration: 1.246s, episode steps: 172, steps per second: 138, episode reward: -242.745, mean reward: -1.411 [-100.000,  2.878], mean action: 1.616 [0.000, 3.000],  loss: 6676.271093, mse: 34091569.569767, mean_q: 4676.044224, mean_eps: 0.769659
  76957/300000: episode: 762, duration: 0.717s, episode steps:  90, steps per second: 125, episode reward: -42.076, mean reward: -0.468 [-100.000, 11.327], mean action: 1.600 [0.000, 3.000],  loss: 14788.237318, mse: 37156388.444444, mean_q: 4899.007935, mean_eps: 0.769266
  77056/300000: episode: 763, duration: 0.826s, episode steps:  99, steps per second: 120, episode reward: -33.874, mean reward: -0.342 [-100.000, 59.284], mean action: 1.707 [0.000, 3.000],  loss: 9298.879854, mse: 36885281.818182, mean_q: 4879.177024, mean_eps: 0.768982
  77160/300000: episode: 764, duration: 0.811s, episode steps: 104, steps per second: 128, episode reward: -230.013, mean reward: -2.212 [-100.000,  0.797], mean action: 1.606 [0.000, 3.000],  loss: 9492.253366, mse: 40298813.423077, mean_q: 5116.807819, mean_eps: 0.768678
  77262/300000: episode: 765, duration: 0.815s, episode steps: 102, steps per second: 125, episode reward: -75.160, mean reward: -0.737 [-100.000, 96.103], mean action: 1.559 [0.000, 3.000],  loss: 7909.343542, mse: 42043169.490196, mean_q: 5264.935990, mean_eps: 0.768369
  77375/300000: episode: 766, duration: 0.870s, episode steps: 113, steps per second: 130, episode reward: -372.699, mean reward: -3.298 [-100.000,  0.773], mean action: 1.469 [0.000, 3.000],  loss: 9405.154932, mse: 41316045.398230, mean_q: 5146.447812, mean_eps: 0.768046
  77475/300000: episode: 767, duration: 0.770s, episode steps: 100, steps per second: 130, episode reward: -273.148, mean reward: -2.731 [-100.000,  1.377], mean action: 1.630 [0.000, 3.000],  loss: 9604.852314, mse: 42626946.180000, mean_q: 5217.607693, mean_eps: 0.767727
  77567/300000: episode: 768, duration: 0.773s, episode steps:  92, steps per second: 119, episode reward: -213.162, mean reward: -2.317 [-100.000, 68.455], mean action: 1.696 [0.000, 3.000],  loss: 15072.121014, mse: 45171449.500000, mean_q: 5424.401813, mean_eps: 0.767438
  77662/300000: episode: 769, duration: 0.712s, episode steps:  95, steps per second: 133, episode reward: -222.797, mean reward: -2.345 [-100.000,  5.812], mean action: 1.495 [0.000, 3.000],  loss: 11504.565748, mse: 46173889.284211, mean_q: 5386.448507, mean_eps: 0.767158
  77808/300000: episode: 770, duration: 1.024s, episode steps: 146, steps per second: 143, episode reward: -154.017, mean reward: -1.055 [-100.000,  3.389], mean action: 1.555 [0.000, 3.000],  loss: 11019.159053, mse: 48166699.890411, mean_q: 5532.797009, mean_eps: 0.766797
  77980/300000: episode: 771, duration: 1.210s, episode steps: 172, steps per second: 142, episode reward: -126.691, mean reward: -0.737 [-100.000,  6.622], mean action: 1.599 [0.000, 3.000],  loss: 10304.033561, mse: 50548511.093023, mean_q: 5697.723650, mean_eps: 0.766319
  78078/300000: episode: 772, duration: 0.655s, episode steps:  98, steps per second: 150, episode reward: -386.458, mean reward: -3.943 [-100.000,  0.453], mean action: 1.673 [0.000, 3.000],  loss: 11023.745917, mse: 53264948.285714, mean_q: 5838.208053, mean_eps: 0.765915
  78192/300000: episode: 773, duration: 0.788s, episode steps: 114, steps per second: 145, episode reward: -103.580, mean reward: -0.909 [-100.000,  6.281], mean action: 1.711 [0.000, 3.000],  loss: 12541.642257, mse: 55360303.508772, mean_q: 5937.996618, mean_eps: 0.765596
  78452/300000: episode: 774, duration: 1.842s, episode steps: 260, steps per second: 141, episode reward: -92.518, mean reward: -0.356 [-100.000, 125.407], mean action: 1.635 [0.000, 3.000],  loss: 11181.739129, mse: 59212681.753846, mean_q: 6087.278767, mean_eps: 0.765035
  78516/300000: episode: 775, duration: 0.510s, episode steps:  64, steps per second: 126, episode reward: -70.742, mean reward: -1.105 [-100.000, 45.257], mean action: 1.531 [0.000, 3.000],  loss: 12546.642372, mse: 64282022.406250, mean_q: 6306.391075, mean_eps: 0.764549
  78688/300000: episode: 776, duration: 1.240s, episode steps: 172, steps per second: 139, episode reward: -427.047, mean reward: -2.483 [-100.000,  3.710], mean action: 1.733 [0.000, 3.000],  loss: 13083.496043, mse: 63838111.372093, mean_q: 6274.575891, mean_eps: 0.764196
  78791/300000: episode: 777, duration: 0.773s, episode steps: 103, steps per second: 133, episode reward: -129.717, mean reward: -1.259 [-100.000,  9.274], mean action: 1.515 [0.000, 3.000],  loss: 12277.389454, mse: 69005542.252427, mean_q: 6564.869513, mean_eps: 0.763783
  78873/300000: episode: 778, duration: 0.553s, episode steps:  82, steps per second: 148, episode reward: -154.265, mean reward: -1.881 [-100.000, 10.425], mean action: 1.488 [0.000, 3.000],  loss: 13974.611453, mse: 68553388.000000, mean_q: 6367.630341, mean_eps: 0.763506
  79043/300000: episode: 779, duration: 1.181s, episode steps: 170, steps per second: 144, episode reward: -197.101, mean reward: -1.159 [-100.000,  7.726], mean action: 1.694 [0.000, 3.000],  loss: 18290.320673, mse: 74382961.152941, mean_q: 6723.355736, mean_eps: 0.763127
  79138/300000: episode: 780, duration: 0.638s, episode steps:  95, steps per second: 149, episode reward: -75.946, mean reward: -0.799 [-100.000, 52.720], mean action: 1.642 [0.000, 3.000],  loss: 19754.138656, mse: 76265922.105263, mean_q: 6896.212999, mean_eps: 0.762730
  79222/300000: episode: 781, duration: 0.560s, episode steps:  84, steps per second: 150, episode reward: -247.894, mean reward: -2.951 [-100.000, 12.382], mean action: 1.619 [0.000, 3.000],  loss: 17144.026170, mse: 83347157.476190, mean_q: 7276.468837, mean_eps: 0.762462
  79413/300000: episode: 782, duration: 1.393s, episode steps: 191, steps per second: 137, episode reward: -229.946, mean reward: -1.204 [-100.000, 10.606], mean action: 1.728 [0.000, 3.000],  loss: 18414.254738, mse: 84925873.612565, mean_q: 7153.813755, mean_eps: 0.762049
  79495/300000: episode: 783, duration: 0.610s, episode steps:  82, steps per second: 134, episode reward: -394.448, mean reward: -4.810 [-100.000, -0.207], mean action: 1.683 [0.000, 3.000],  loss: 18915.632908, mse: 88084838.292683, mean_q: 7430.845745, mean_eps: 0.761640
  79630/300000: episode: 784, duration: 0.998s, episode steps: 135, steps per second: 135, episode reward: -234.796, mean reward: -1.739 [-100.000, 37.628], mean action: 1.607 [0.000, 3.000],  loss: 19577.246369, mse: 93759844.237037, mean_q: 7524.673195, mean_eps: 0.761314
  79704/300000: episode: 785, duration: 0.534s, episode steps:  74, steps per second: 139, episode reward: -139.642, mean reward: -1.887 [-100.000, 12.042], mean action: 1.500 [0.000, 3.000],  loss: 18650.193425, mse: 99057179.945946, mean_q: 7807.239443, mean_eps: 0.761001
  79841/300000: episode: 786, duration: 0.932s, episode steps: 137, steps per second: 147, episode reward: -101.646, mean reward: -0.742 [-100.000,  7.151], mean action: 1.701 [0.000, 3.000],  loss: 25933.556391, mse: 106247297.664234, mean_q: 8027.322455, mean_eps: 0.760684
  79931/300000: episode: 787, duration: 0.610s, episode steps:  90, steps per second: 148, episode reward: -335.701, mean reward: -3.730 [-100.000, 15.394], mean action: 1.567 [0.000, 3.000],  loss: 22133.399902, mse: 109210754.311111, mean_q: 8156.396305, mean_eps: 0.760343
  80048/300000: episode: 788, duration: 0.873s, episode steps: 117, steps per second: 134, episode reward: -252.955, mean reward: -2.162 [-100.000,  7.644], mean action: 1.598 [0.000, 3.000],  loss: 29152.067529, mse: 112719163.863248, mean_q: 8192.982651, mean_eps: 0.760033
  80169/300000: episode: 789, duration: 0.836s, episode steps: 121, steps per second: 145, episode reward: -71.813, mean reward: -0.593 [-100.000,  6.050], mean action: 1.702 [0.000, 3.000],  loss: 26647.033550, mse: 119810190.181818, mean_q: 8547.303582, mean_eps: 0.759676
  80282/300000: episode: 790, duration: 0.822s, episode steps: 113, steps per second: 137, episode reward: -412.660, mean reward: -3.652 [-100.000,  1.433], mean action: 1.619 [0.000, 3.000],  loss: 25364.641485, mse: 127205617.769911, mean_q: 8817.499408, mean_eps: 0.759325
  80363/300000: episode: 791, duration: 0.587s, episode steps:  81, steps per second: 138, episode reward: -302.273, mean reward: -3.732 [-100.000,  1.126], mean action: 1.716 [0.000, 3.000],  loss: 29342.338879, mse: 128937930.074074, mean_q: 8956.974694, mean_eps: 0.759034
  80437/300000: episode: 792, duration: 0.548s, episode steps:  74, steps per second: 135, episode reward: -63.962, mean reward: -0.864 [-100.000,  7.580], mean action: 1.378 [0.000, 3.000],  loss: 28872.675715, mse: 135892239.621622, mean_q: 9069.743217, mean_eps: 0.758802
  80564/300000: episode: 793, duration: 0.905s, episode steps: 127, steps per second: 140, episode reward: -578.546, mean reward: -4.555 [-100.000,  2.332], mean action: 1.772 [0.000, 3.000],  loss: 34058.327102, mse: 140811543.874016, mean_q: 9248.194105, mean_eps: 0.758500
  80786/300000: episode: 794, duration: 1.566s, episode steps: 222, steps per second: 142, episode reward: -131.095, mean reward: -0.591 [-100.000,  8.581], mean action: 1.626 [0.000, 3.000],  loss: 36027.361220, mse: 146472476.828829, mean_q: 9399.022619, mean_eps: 0.757976
  80890/300000: episode: 795, duration: 0.893s, episode steps: 104, steps per second: 116, episode reward: -211.180, mean reward: -2.031 [-100.000, 25.728], mean action: 1.760 [0.000, 3.000],  loss: 39653.393273, mse: 154447359.807692, mean_q: 9568.003991, mean_eps: 0.757487
  80993/300000: episode: 796, duration: 0.723s, episode steps: 103, steps per second: 142, episode reward: -321.284, mean reward: -3.119 [-100.000,  0.962], mean action: 1.718 [0.000, 3.000],  loss: 37028.901396, mse: 167306407.533981, mean_q: 10060.855393, mean_eps: 0.757177
  81099/300000: episode: 797, duration: 0.734s, episode steps: 106, steps per second: 144, episode reward: -112.183, mean reward: -1.058 [-100.000, 17.903], mean action: 1.585 [0.000, 3.000],  loss: 40551.269844, mse: 173820254.264151, mean_q: 10224.999175, mean_eps: 0.756864
  81177/300000: episode: 798, duration: 0.581s, episode steps:  78, steps per second: 134, episode reward: -122.481, mean reward: -1.570 [-100.000, 19.891], mean action: 1.833 [0.000, 3.000],  loss: 40262.846542, mse: 178475818.974359, mean_q: 10236.714155, mean_eps: 0.756588
  81269/300000: episode: 799, duration: 0.701s, episode steps:  92, steps per second: 131, episode reward: -403.933, mean reward: -4.391 [-100.000,  0.881], mean action: 1.511 [0.000, 3.000],  loss: 56614.418882, mse: 193830022.869565, mean_q: 10984.589010, mean_eps: 0.756332
  81364/300000: episode: 800, duration: 0.684s, episode steps:  95, steps per second: 139, episode reward: -66.539, mean reward: -0.700 [-100.000, 11.254], mean action: 1.684 [0.000, 3.000],  loss: 48176.940502, mse: 201822361.094737, mean_q: 10997.273494, mean_eps: 0.756052
  81455/300000: episode: 801, duration: 0.705s, episode steps:  91, steps per second: 129, episode reward: -436.839, mean reward: -4.800 [-100.000,  0.689], mean action: 1.692 [0.000, 3.000],  loss: 51677.132029, mse: 209473699.428571, mean_q: 11207.544847, mean_eps: 0.755773
  81606/300000: episode: 802, duration: 1.152s, episode steps: 151, steps per second: 131, episode reward: -288.822, mean reward: -1.913 [-100.000,  1.664], mean action: 1.722 [0.000, 3.000],  loss: 49653.354182, mse: 218903206.410596, mean_q: 11485.154636, mean_eps: 0.755410
  81726/300000: episode: 803, duration: 0.838s, episode steps: 120, steps per second: 143, episode reward: -200.921, mean reward: -1.674 [-100.000,  4.126], mean action: 1.575 [0.000, 3.000],  loss: 51232.471908, mse: 227618520.933333, mean_q: 11620.401847, mean_eps: 0.755004
  81975/300000: episode: 804, duration: 1.887s, episode steps: 249, steps per second: 132, episode reward: -159.347, mean reward: -0.640 [-100.000, 12.565], mean action: 1.622 [0.000, 3.000],  loss: 53308.872847, mse: 254063809.060241, mean_q: 12253.887589, mean_eps: 0.754450
  82127/300000: episode: 805, duration: 1.100s, episode steps: 152, steps per second: 138, episode reward: -251.117, mean reward: -1.652 [-100.000, 11.876], mean action: 1.461 [0.000, 3.000],  loss: 60936.241674, mse: 268629100.684211, mean_q: 12710.113310, mean_eps: 0.753848
  82271/300000: episode: 806, duration: 0.973s, episode steps: 144, steps per second: 148, episode reward: -270.351, mean reward: -1.877 [-100.000,  3.801], mean action: 1.681 [0.000, 3.000],  loss: 67113.354519, mse: 295418391.111111, mean_q: 13281.358351, mean_eps: 0.753404
  82422/300000: episode: 807, duration: 1.035s, episode steps: 151, steps per second: 146, episode reward: -314.392, mean reward: -2.082 [-100.000,  1.986], mean action: 1.576 [0.000, 3.000],  loss: 81056.581106, mse: 317764303.682119, mean_q: 13691.438981, mean_eps: 0.752962
  82486/300000: episode: 808, duration: 0.446s, episode steps:  64, steps per second: 144, episode reward: -145.117, mean reward: -2.267 [-100.000,  8.730], mean action: 1.594 [0.000, 3.000],  loss: 63076.619476, mse: 312962367.500000, mean_q: 13589.665909, mean_eps: 0.752640
  82584/300000: episode: 809, duration: 0.728s, episode steps:  98, steps per second: 135, episode reward: -296.939, mean reward: -3.030 [-100.000,  0.864], mean action: 1.541 [0.000, 3.000],  loss: 78316.869848, mse: 332276687.020408, mean_q: 13935.752232, mean_eps: 0.752397
  82668/300000: episode: 810, duration: 0.675s, episode steps:  84, steps per second: 124, episode reward: -372.538, mean reward: -4.435 [-100.000,  0.147], mean action: 1.762 [0.000, 3.000],  loss: 69390.557780, mse: 334485276.761905, mean_q: 13865.679571, mean_eps: 0.752123
  82795/300000: episode: 811, duration: 0.969s, episode steps: 127, steps per second: 131, episode reward: -130.125, mean reward: -1.025 [-100.000, 12.177], mean action: 1.630 [0.000, 3.000],  loss: 81858.177042, mse: 363388092.094488, mean_q: 14670.869494, mean_eps: 0.751807
  82875/300000: episode: 812, duration: 0.574s, episode steps:  80, steps per second: 139, episode reward: -103.366, mean reward: -1.292 [-100.000, 10.890], mean action: 1.550 [0.000, 3.000],  loss: 103516.427319, mse: 378657757.200000, mean_q: 14829.684973, mean_eps: 0.751497
  82940/300000: episode: 813, duration: 0.483s, episode steps:  65, steps per second: 135, episode reward: -156.950, mean reward: -2.415 [-100.000, 24.114], mean action: 1.692 [0.000, 3.000],  loss: 78425.275841, mse: 381474086.892308, mean_q: 14940.893044, mean_eps: 0.751279
  83021/300000: episode: 814, duration: 0.569s, episode steps:  81, steps per second: 142, episode reward: -88.181, mean reward: -1.089 [-100.000, 11.523], mean action: 1.333 [0.000, 3.000],  loss: 118354.434534, mse: 396291359.604938, mean_q: 15161.673973, mean_eps: 0.751060
  83089/300000: episode: 815, duration: 0.481s, episode steps:  68, steps per second: 141, episode reward: -58.968, mean reward: -0.867 [-100.000, 11.077], mean action: 1.647 [0.000, 3.000],  loss: 104889.764304, mse: 433014549.882353, mean_q: 15709.058622, mean_eps: 0.750837
  83201/300000: episode: 816, duration: 0.804s, episode steps: 112, steps per second: 139, episode reward: -118.297, mean reward: -1.056 [-100.000,  6.952], mean action: 1.580 [0.000, 3.000],  loss: 108677.431606, mse: 436838865.571429, mean_q: 15926.758187, mean_eps: 0.750566
  83298/300000: episode: 817, duration: 0.795s, episode steps:  97, steps per second: 122, episode reward: -114.670, mean reward: -1.182 [-100.000, 10.224], mean action: 1.588 [0.000, 3.000],  loss: 107898.995329, mse: 489294797.855670, mean_q: 16913.505255, mean_eps: 0.750253
  83418/300000: episode: 818, duration: 0.865s, episode steps: 120, steps per second: 139, episode reward: -252.150, mean reward: -2.101 [-100.000,  1.880], mean action: 1.592 [0.000, 3.000],  loss: 142616.866862, mse: 481019428.666667, mean_q: 16574.116569, mean_eps: 0.749927
  83526/300000: episode: 819, duration: 0.787s, episode steps: 108, steps per second: 137, episode reward: -210.359, mean reward: -1.948 [-100.000,  0.964], mean action: 1.574 [0.000, 3.000],  loss: 123743.109990, mse: 488859005.185185, mean_q: 16776.520164, mean_eps: 0.749585
  83708/300000: episode: 820, duration: 1.285s, episode steps: 182, steps per second: 142, episode reward: -128.252, mean reward: -0.705 [-100.000, 72.615], mean action: 1.698 [0.000, 3.000],  loss: 117795.263071, mse: 555170906.637363, mean_q: 17777.885576, mean_eps: 0.749151
  83792/300000: episode: 821, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -108.802, mean reward: -1.295 [-100.000, 64.923], mean action: 1.726 [0.000, 3.000],  loss: 144399.129185, mse: 598399268.952381, mean_q: 18562.184047, mean_eps: 0.748751
  83892/300000: episode: 822, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: -203.388, mean reward: -2.034 [-100.000,  5.589], mean action: 1.660 [0.000, 3.000],  loss: 159321.572305, mse: 595167123.840000, mean_q: 18388.271572, mean_eps: 0.748475
  84073/300000: episode: 823, duration: 1.274s, episode steps: 181, steps per second: 142, episode reward: -220.130, mean reward: -1.216 [-100.000,  7.710], mean action: 1.575 [0.000, 3.000],  loss: 157498.586002, mse: 678371608.397790, mean_q: 19946.076566, mean_eps: 0.748054
  84192/300000: episode: 824, duration: 0.862s, episode steps: 119, steps per second: 138, episode reward: -126.683, mean reward: -1.065 [-100.000, 21.107], mean action: 1.672 [0.000, 3.000],  loss: 206230.216682, mse: 746205295.865546, mean_q: 20992.063378, mean_eps: 0.747604
  84360/300000: episode: 825, duration: 1.190s, episode steps: 168, steps per second: 141, episode reward: -366.528, mean reward: -2.182 [-100.000, 18.540], mean action: 1.661 [0.000, 3.000],  loss: 184113.342959, mse: 780984872.761905, mean_q: 21014.215809, mean_eps: 0.747174
  84550/300000: episode: 826, duration: 1.370s, episode steps: 190, steps per second: 139, episode reward: -75.003, mean reward: -0.395 [-100.000, 11.367], mean action: 1.642 [0.000, 3.000],  loss: 194195.178598, mse: 784834420.884210, mean_q: 21316.145482, mean_eps: 0.746636
  84641/300000: episode: 827, duration: 0.657s, episode steps:  91, steps per second: 138, episode reward: -285.290, mean reward: -3.135 [-100.000,  1.026], mean action: 1.758 [0.000, 3.000],  loss: 231061.675309, mse: 833718879.648352, mean_q: 22130.137674, mean_eps: 0.746215
  84769/300000: episode: 828, duration: 0.961s, episode steps: 128, steps per second: 133, episode reward: -120.574, mean reward: -0.942 [-100.000, 11.863], mean action: 1.594 [0.000, 3.000],  loss: 221283.094147, mse: 894786402.500000, mean_q: 22525.785950, mean_eps: 0.745886
  84872/300000: episode: 829, duration: 0.771s, episode steps: 103, steps per second: 134, episode reward: -253.244, mean reward: -2.459 [-100.000, 78.074], mean action: 1.757 [0.000, 3.000],  loss: 232558.092461, mse: 943625991.145631, mean_q: 23516.482763, mean_eps: 0.745540
  84960/300000: episode: 830, duration: 0.634s, episode steps:  88, steps per second: 139, episode reward: -84.928, mean reward: -0.965 [-100.000,  8.052], mean action: 1.557 [0.000, 3.000],  loss: 227480.262917, mse: 996653576.727273, mean_q: 24144.336914, mean_eps: 0.745254
  85052/300000: episode: 831, duration: 0.795s, episode steps:  92, steps per second: 116, episode reward: -139.202, mean reward: -1.513 [-100.000,  5.407], mean action: 1.685 [0.000, 3.000],  loss: 253303.299423, mse: 1065315706.782609, mean_q: 24631.953146, mean_eps: 0.744984
  85164/300000: episode: 832, duration: 0.786s, episode steps: 112, steps per second: 142, episode reward: -93.126, mean reward: -0.831 [-100.000,  7.697], mean action: 1.688 [0.000, 3.000],  loss: 247476.808733, mse: 1046377397.714286, mean_q: 24818.952811, mean_eps: 0.744677
  85307/300000: episode: 833, duration: 1.118s, episode steps: 143, steps per second: 128, episode reward: -30.381, mean reward: -0.212 [-100.000, 41.187], mean action: 1.601 [0.000, 3.000],  loss: 278406.546602, mse: 1117473496.391608, mean_q: 25311.774715, mean_eps: 0.744295
  85389/300000: episode: 834, duration: 0.618s, episode steps:  82, steps per second: 133, episode reward: -91.117, mean reward: -1.111 [-100.000,  5.813], mean action: 1.561 [0.000, 3.000],  loss: 262569.533060, mse: 1143745159.414634, mean_q: 25404.620867, mean_eps: 0.743958
  85452/300000: episode: 835, duration: 0.489s, episode steps:  63, steps per second: 129, episode reward: -230.412, mean reward: -3.657 [-100.000,  4.020], mean action: 1.698 [0.000, 3.000],  loss: 328956.847842, mse: 1215450702.730159, mean_q: 26654.279948, mean_eps: 0.743740
  85572/300000: episode: 836, duration: 0.941s, episode steps: 120, steps per second: 128, episode reward: -327.219, mean reward: -2.727 [-100.000,  1.494], mean action: 1.742 [0.000, 3.000],  loss: 295437.800456, mse: 1293985481.066667, mean_q: 27310.856836, mean_eps: 0.743465
  85686/300000: episode: 837, duration: 0.850s, episode steps: 114, steps per second: 134, episode reward: -308.322, mean reward: -2.705 [-100.000, 35.606], mean action: 1.421 [0.000, 3.000],  loss: 550178.163309, mse: 1317615512.701754, mean_q: 27513.478721, mean_eps: 0.743115
  85831/300000: episode: 838, duration: 0.989s, episode steps: 145, steps per second: 147, episode reward: -266.426, mean reward: -1.837 [-100.000, 20.994], mean action: 1.786 [0.000, 3.000],  loss: 437891.562931, mse: 1386530942.234483, mean_q: 28572.510453, mean_eps: 0.742726
  85912/300000: episode: 839, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -238.345, mean reward: -2.943 [-100.000, 17.620], mean action: 1.506 [0.000, 3.000],  loss: 329213.175058, mse: 1458842936.098765, mean_q: 29141.907962, mean_eps: 0.742387
  86004/300000: episode: 840, duration: 0.767s, episode steps:  92, steps per second: 120, episode reward: -278.015, mean reward: -3.022 [-100.000,  0.765], mean action: 1.630 [0.000, 3.000],  loss: 406538.772418, mse: 1524042462.608696, mean_q: 29374.013099, mean_eps: 0.742128
  86074/300000: episode: 841, duration: 0.592s, episode steps:  70, steps per second: 118, episode reward: -271.264, mean reward: -3.875 [-100.000,  2.361], mean action: 1.857 [0.000, 3.000],  loss: 461028.039844, mse: 1570286992.457143, mean_q: 29770.850893, mean_eps: 0.741885
  86171/300000: episode: 842, duration: 0.798s, episode steps:  97, steps per second: 122, episode reward: -70.596, mean reward: -0.728 [-100.000, 17.533], mean action: 1.691 [0.000, 3.000],  loss: 516424.584085, mse: 1681300540.041237, mean_q: 31179.669418, mean_eps: 0.741634
  86311/300000: episode: 843, duration: 1.071s, episode steps: 140, steps per second: 131, episode reward: -208.614, mean reward: -1.490 [-100.000,  3.077], mean action: 1.657 [0.000, 3.000],  loss: 452219.630692, mse: 1784099275.885714, mean_q: 32549.336007, mean_eps: 0.741278
  86404/300000: episode: 844, duration: 0.707s, episode steps:  93, steps per second: 132, episode reward: -136.249, mean reward: -1.465 [-100.000,  6.754], mean action: 1.656 [0.000, 3.000],  loss: 478478.553931, mse: 1892363948.043011, mean_q: 33412.241515, mean_eps: 0.740929
  86506/300000: episode: 845, duration: 0.777s, episode steps: 102, steps per second: 131, episode reward: -479.047, mean reward: -4.697 [-100.000,  1.521], mean action: 1.794 [0.000, 3.000],  loss: 740211.118260, mse: 1958262066.196079, mean_q: 33996.858169, mean_eps: 0.740637
  86650/300000: episode: 846, duration: 1.073s, episode steps: 144, steps per second: 134, episode reward: -351.686, mean reward: -2.442 [-100.000, 99.176], mean action: 1.681 [0.000, 3.000],  loss: 850893.577365, mse: 2071739954.666667, mean_q: 34468.477960, mean_eps: 0.740267
  86793/300000: episode: 847, duration: 1.014s, episode steps: 143, steps per second: 141, episode reward: -148.568, mean reward: -1.039 [-100.000, 12.300], mean action: 1.643 [0.000, 3.000],  loss: 527278.393684, mse: 2214197501.762238, mean_q: 35593.554565, mean_eps: 0.739837
  86858/300000: episode: 848, duration: 0.439s, episode steps:  65, steps per second: 148, episode reward: -354.533, mean reward: -5.454 [-100.000,  3.330], mean action: 1.769 [0.000, 3.000],  loss: 743336.832212, mse: 2396358543.753846, mean_q: 36718.273377, mean_eps: 0.739525
  87008/300000: episode: 849, duration: 1.029s, episode steps: 150, steps per second: 146, episode reward: -191.519, mean reward: -1.277 [-100.000,  4.253], mean action: 1.740 [0.000, 3.000],  loss: 967378.357083, mse: 2396263109.973333, mean_q: 37215.771315, mean_eps: 0.739202
  87080/300000: episode: 850, duration: 0.517s, episode steps:  72, steps per second: 139, episode reward: -71.308, mean reward: -0.990 [-100.000, 13.004], mean action: 1.486 [0.000, 3.000],  loss: 1449477.876953, mse: 2573140627.555555, mean_q: 38394.991943, mean_eps: 0.738870
  87247/300000: episode: 851, duration: 1.143s, episode steps: 167, steps per second: 146, episode reward: -204.847, mean reward: -1.227 [-100.000, 24.710], mean action: 1.629 [0.000, 3.000],  loss: 929507.277133, mse: 2734886601.580838, mean_q: 39557.000129, mean_eps: 0.738511
  87344/300000: episode: 852, duration: 0.674s, episode steps:  97, steps per second: 144, episode reward: -315.887, mean reward: -3.257 [-100.000, 38.481], mean action: 1.536 [0.000, 3.000],  loss: 929968.398679, mse: 2807279341.525773, mean_q: 40321.631685, mean_eps: 0.738115
  87496/300000: episode: 853, duration: 1.043s, episode steps: 152, steps per second: 146, episode reward: -498.403, mean reward: -3.279 [-100.000,  3.921], mean action: 1.783 [0.000, 3.000],  loss: 789461.673314, mse: 3124159013.052631, mean_q: 42310.778115, mean_eps: 0.737741
  87596/300000: episode: 854, duration: 0.892s, episode steps: 100, steps per second: 112, episode reward: -239.108, mean reward: -2.391 [-100.000,  0.916], mean action: 1.580 [0.000, 3.000],  loss: 877498.278438, mse: 3266049542.400000, mean_q: 43137.021328, mean_eps: 0.737364
  87728/300000: episode: 855, duration: 1.079s, episode steps: 132, steps per second: 122, episode reward: -156.445, mean reward: -1.185 [-100.000,  3.194], mean action: 1.667 [0.000, 3.000],  loss: 997790.973722, mse: 3353892535.272727, mean_q: 43904.705108, mean_eps: 0.737016
  87827/300000: episode: 856, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -369.756, mean reward: -3.735 [-100.000,  0.826], mean action: 1.657 [0.000, 3.000],  loss: 983057.880051, mse: 3455419442.424242, mean_q: 44652.454743, mean_eps: 0.736669
  88024/300000: episode: 857, duration: 1.483s, episode steps: 197, steps per second: 133, episode reward: -153.442, mean reward: -0.779 [-100.000,  6.722], mean action: 1.599 [0.000, 3.000],  loss: 1070088.095019, mse: 3766142158.619289, mean_q: 46176.288468, mean_eps: 0.736225
  88107/300000: episode: 858, duration: 0.561s, episode steps:  83, steps per second: 148, episode reward: -89.439, mean reward: -1.078 [-100.000, 10.765], mean action: 1.699 [0.000, 3.000],  loss: 1124782.915663, mse: 4049350591.228916, mean_q: 48034.643166, mean_eps: 0.735805
  88219/300000: episode: 859, duration: 0.810s, episode steps: 112, steps per second: 138, episode reward: -38.042, mean reward: -0.340 [-100.000, 20.941], mean action: 1.696 [0.000, 3.000],  loss: 1281487.873047, mse: 4426696180.571428, mean_q: 50203.819720, mean_eps: 0.735512
  88316/300000: episode: 860, duration: 0.672s, episode steps:  97, steps per second: 144, episode reward: -157.051, mean reward: -1.619 [-100.000,  8.507], mean action: 1.742 [0.000, 3.000],  loss: 1176364.960696, mse: 4415368036.288660, mean_q: 50535.550741, mean_eps: 0.735199
  88464/300000: episode: 861, duration: 1.002s, episode steps: 148, steps per second: 148, episode reward: -89.621, mean reward: -0.606 [-100.000,  6.810], mean action: 1.703 [0.000, 3.000],  loss: 1526546.205448, mse: 4563014048.000000, mean_q: 51062.487120, mean_eps: 0.734832
  88587/300000: episode: 862, duration: 0.862s, episode steps: 123, steps per second: 143, episode reward: -109.882, mean reward: -0.893 [-100.000, 16.151], mean action: 1.528 [0.000, 3.000],  loss: 2241075.147612, mse: 4697978634.406504, mean_q: 51174.317867, mean_eps: 0.734425
  88707/300000: episode: 863, duration: 0.810s, episode steps: 120, steps per second: 148, episode reward: -146.445, mean reward: -1.220 [-100.000,  7.341], mean action: 1.717 [0.000, 3.000],  loss: 1638223.757292, mse: 5110175176.533334, mean_q: 53859.004492, mean_eps: 0.734061
  88838/300000: episode: 864, duration: 0.912s, episode steps: 131, steps per second: 144, episode reward: -34.688, mean reward: -0.265 [-100.000, 12.672], mean action: 1.672 [0.000, 3.000],  loss: 1683669.703244, mse: 5540431115.725191, mean_q: 56046.370468, mean_eps: 0.733684
  88971/300000: episode: 865, duration: 0.923s, episode steps: 133, steps per second: 144, episode reward: -345.733, mean reward: -2.599 [-100.000,  2.315], mean action: 1.820 [0.000, 3.000],  loss: 1579932.441494, mse: 5959494650.225564, mean_q: 58213.249677, mean_eps: 0.733288
  89066/300000: episode: 866, duration: 0.675s, episode steps:  95, steps per second: 141, episode reward: -195.691, mean reward: -2.060 [-100.000,  1.413], mean action: 1.505 [0.000, 3.000],  loss: 1767741.882895, mse: 6127628205.810526, mean_q: 59228.578803, mean_eps: 0.732946
  89215/300000: episode: 867, duration: 1.060s, episode steps: 149, steps per second: 141, episode reward: -193.757, mean reward: -1.300 [-100.000,  2.552], mean action: 1.611 [0.000, 3.000],  loss: 2423432.869966, mse: 6466557936.536913, mean_q: 60479.788512, mean_eps: 0.732580
  89326/300000: episode: 868, duration: 0.738s, episode steps: 111, steps per second: 150, episode reward: -175.371, mean reward: -1.580 [-100.000,  2.745], mean action: 1.730 [0.000, 3.000],  loss: 1892687.410473, mse: 6997485231.279280, mean_q: 63027.067568, mean_eps: 0.732190
  89446/300000: episode: 869, duration: 0.824s, episode steps: 120, steps per second: 146, episode reward: -283.580, mean reward: -2.363 [-100.000, 21.048], mean action: 1.417 [0.000, 3.000],  loss: 2017259.316667, mse: 7282348292.266666, mean_q: 65027.541048, mean_eps: 0.731843
  89552/300000: episode: 870, duration: 0.719s, episode steps: 106, steps per second: 148, episode reward: -45.813, mean reward: -0.432 [-100.000,  7.853], mean action: 1.462 [0.000, 3.000],  loss: 2203674.666274, mse: 8059334812.981133, mean_q: 68628.773511, mean_eps: 0.731505
  89688/300000: episode: 871, duration: 0.914s, episode steps: 136, steps per second: 149, episode reward: -43.156, mean reward: -0.317 [-100.000, 13.233], mean action: 1.603 [0.000, 3.000],  loss: 3617864.207721, mse: 8233630366.117647, mean_q: 68587.618796, mean_eps: 0.731141
  89801/300000: episode: 872, duration: 0.810s, episode steps: 113, steps per second: 140, episode reward: -348.829, mean reward: -3.087 [-100.000,  2.304], mean action: 1.664 [0.000, 3.000],  loss: 3698844.596239, mse: 8213300658.973452, mean_q: 68217.965328, mean_eps: 0.730768
  89929/300000: episode: 873, duration: 0.936s, episode steps: 128, steps per second: 137, episode reward: -313.970, mean reward: -2.453 [-100.000,  3.433], mean action: 1.812 [0.000, 3.000],  loss: 3678401.818359, mse: 9096326968.000000, mean_q: 72065.118195, mean_eps: 0.730406
  90019/300000: episode: 874, duration: 0.634s, episode steps:  90, steps per second: 142, episode reward: -337.037, mean reward: -3.745 [-100.000,  0.728], mean action: 1.611 [0.000, 3.000],  loss: 3694004.444444, mse: 9330509838.222221, mean_q: 72281.067708, mean_eps: 0.730079
  90101/300000: episode: 875, duration: 0.640s, episode steps:  82, steps per second: 128, episode reward: -267.664, mean reward: -3.264 [-100.000,  2.411], mean action: 1.622 [0.000, 3.000],  loss: 3951610.346799, mse: 9689219215.609756, mean_q: 74918.314691, mean_eps: 0.729822
  90207/300000: episode: 876, duration: 0.780s, episode steps: 106, steps per second: 136, episode reward: -166.245, mean reward: -1.568 [-100.000,  1.746], mean action: 1.566 [0.000, 3.000],  loss: 5774556.757075, mse: 10620006066.716982, mean_q: 78602.690043, mean_eps: 0.729539
  90299/300000: episode: 877, duration: 0.708s, episode steps:  92, steps per second: 130, episode reward: -162.032, mean reward: -1.761 [-100.000,  5.247], mean action: 1.576 [0.000, 3.000],  loss: 3342030.576087, mse: 10769038063.304348, mean_q: 78608.235946, mean_eps: 0.729242
  90400/300000: episode: 878, duration: 0.803s, episode steps: 101, steps per second: 126, episode reward: -162.106, mean reward: -1.605 [-100.000,  8.581], mean action: 1.693 [0.000, 3.000],  loss: 4009619.424505, mse: 11295889331.960396, mean_q: 81559.829517, mean_eps: 0.728953
  90498/300000: episode: 879, duration: 0.701s, episode steps:  98, steps per second: 140, episode reward: -254.881, mean reward: -2.601 [-100.000, 22.456], mean action: 1.418 [0.000, 3.000],  loss: 3416395.817602, mse: 11633790761.795918, mean_q: 81590.598573, mean_eps: 0.728654
  90661/300000: episode: 880, duration: 1.240s, episode steps: 163, steps per second: 132, episode reward: -496.812, mean reward: -3.048 [-100.000,  1.694], mean action: 1.828 [0.000, 3.000],  loss: 4070234.878067, mse: 13137024047.116564, mean_q: 87667.048648, mean_eps: 0.728263
  90781/300000: episode: 881, duration: 0.890s, episode steps: 120, steps per second: 135, episode reward: -118.649, mean reward: -0.989 [-100.000,  7.321], mean action: 1.733 [0.000, 3.000],  loss: 5033187.113542, mse: 13391878353.066668, mean_q: 87658.821419, mean_eps: 0.727839
  90846/300000: episode: 882, duration: 0.481s, episode steps:  65, steps per second: 135, episode reward: -79.804, mean reward: -1.228 [-100.000,  6.133], mean action: 1.538 [0.000, 3.000],  loss: 9320645.176923, mse: 13290984117.169231, mean_q: 86776.146274, mean_eps: 0.727561
  90941/300000: episode: 883, duration: 0.699s, episode steps:  95, steps per second: 136, episode reward: -91.117, mean reward: -0.959 [-100.000, 11.298], mean action: 1.684 [0.000, 3.000],  loss: 3832902.845395, mse: 14756204015.831579, mean_q: 92692.628701, mean_eps: 0.727321
  91067/300000: episode: 884, duration: 0.975s, episode steps: 126, steps per second: 129, episode reward: -238.453, mean reward: -1.892 [-100.000,  1.503], mean action: 1.778 [0.000, 3.000],  loss: 4711313.132937, mse: 14994962021.587301, mean_q: 93364.771019, mean_eps: 0.726990
  91192/300000: episode: 885, duration: 0.969s, episode steps: 125, steps per second: 129, episode reward: -91.438, mean reward: -0.732 [-100.000,  7.663], mean action: 1.544 [0.000, 3.000],  loss: 7290443.356000, mse: 15488060862.464001, mean_q: 94901.203187, mean_eps: 0.726613
  91292/300000: episode: 886, duration: 0.720s, episode steps: 100, steps per second: 139, episode reward: -285.717, mean reward: -2.857 [-100.000,  3.005], mean action: 1.630 [0.000, 3.000],  loss: 6378205.077500, mse: 16828641827.840000, mean_q: 98525.556875, mean_eps: 0.726276
  91386/300000: episode: 887, duration: 0.699s, episode steps:  94, steps per second: 134, episode reward: -147.148, mean reward: -1.565 [-100.000,  4.614], mean action: 1.638 [0.000, 3.000],  loss: 6659432.781915, mse: 17679566455.829788, mean_q: 100863.842171, mean_eps: 0.725985
  91515/300000: episode: 888, duration: 0.963s, episode steps: 129, steps per second: 134, episode reward: -89.560, mean reward: -0.694 [-100.000,  5.840], mean action: 1.550 [0.000, 3.000],  loss: 5665616.770349, mse: 17926692475.038761, mean_q: 101685.799267, mean_eps: 0.725650
  91634/300000: episode: 889, duration: 0.837s, episode steps: 119, steps per second: 142, episode reward: -237.520, mean reward: -1.996 [-100.000,  1.501], mean action: 1.756 [0.000, 3.000],  loss: 7010342.794118, mse: 19482400406.588234, mean_q: 105928.956079, mean_eps: 0.725278
  91749/300000: episode: 890, duration: 0.781s, episode steps: 115, steps per second: 147, episode reward: -243.322, mean reward: -2.116 [-100.000,  1.475], mean action: 1.643 [0.000, 3.000],  loss: 7459856.775000, mse: 20368444496.139130, mean_q: 108240.303363, mean_eps: 0.724927
  91856/300000: episode: 891, duration: 0.734s, episode steps: 107, steps per second: 146, episode reward: -197.642, mean reward: -1.847 [-100.000,  1.251], mean action: 1.645 [0.000, 3.000],  loss: 6052160.671729, mse: 21139466106.018692, mean_q: 110681.537091, mean_eps: 0.724594
  92006/300000: episode: 892, duration: 1.003s, episode steps: 150, steps per second: 149, episode reward: -412.548, mean reward: -2.750 [-100.000, 93.229], mean action: 1.713 [0.000, 3.000],  loss: 8762938.009167, mse: 22222908880.213333, mean_q: 114053.490000, mean_eps: 0.724208
  92094/300000: episode: 893, duration: 0.597s, episode steps:  88, steps per second: 147, episode reward: -184.215, mean reward: -2.093 [-100.000,  7.254], mean action: 1.591 [0.000, 3.000],  loss: 13211120.392045, mse: 22961159028.363636, mean_q: 114925.735884, mean_eps: 0.723851
  92322/300000: episode: 894, duration: 1.553s, episode steps: 228, steps per second: 147, episode reward: -83.119, mean reward: -0.365 [-100.000, 117.110], mean action: 1.658 [0.000, 3.000],  loss: 7576233.228070, mse: 25059687715.929825, mean_q: 121055.443085, mean_eps: 0.723378
  92423/300000: episode: 895, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: -35.243, mean reward: -0.349 [-100.000, 15.311], mean action: 1.515 [0.000, 3.000],  loss: 10026487.282178, mse: 27063547224.712872, mean_q: 124251.092822, mean_eps: 0.722884
  92504/300000: episode: 896, duration: 0.557s, episode steps:  81, steps per second: 145, episode reward: -472.899, mean reward: -5.838 [-100.000,  0.600], mean action: 1.630 [0.000, 3.000],  loss: 8368911.830247, mse: 29868553860.740742, mean_q: 131875.792438, mean_eps: 0.722611
  92639/300000: episode: 897, duration: 0.926s, episode steps: 135, steps per second: 146, episode reward: -333.453, mean reward: -2.470 [-100.000,  1.937], mean action: 1.637 [0.000, 3.000],  loss: 15691592.662037, mse: 30426020583.348148, mean_q: 133759.853356, mean_eps: 0.722287
  92743/300000: episode: 898, duration: 0.724s, episode steps: 104, steps per second: 144, episode reward: -406.243, mean reward: -3.906 [-100.000,  1.008], mean action: 1.817 [0.000, 3.000],  loss: 17415765.983173, mse: 31785240871.384617, mean_q: 136743.086088, mean_eps: 0.721928
  92975/300000: episode: 899, duration: 1.773s, episode steps: 232, steps per second: 131, episode reward: -268.450, mean reward: -1.157 [-100.000,  2.634], mean action: 1.685 [0.000, 3.000],  loss: 11302167.118534, mse: 34603923579.586205, mean_q: 142345.623586, mean_eps: 0.721425
  93085/300000: episode: 900, duration: 0.835s, episode steps: 110, steps per second: 132, episode reward: -228.718, mean reward: -2.079 [-100.000,  1.780], mean action: 1.545 [0.000, 3.000],  loss: 13268952.306818, mse: 37252126850.327271, mean_q: 148423.783594, mean_eps: 0.720912
  93210/300000: episode: 901, duration: 0.879s, episode steps: 125, steps per second: 142, episode reward: -173.338, mean reward: -1.387 [-100.000,  3.742], mean action: 1.728 [0.000, 3.000],  loss: 13229005.014000, mse: 41818407485.440002, mean_q: 157410.357000, mean_eps: 0.720559
  93424/300000: episode: 902, duration: 2.093s, episode steps: 214, steps per second: 102, episode reward: -299.690, mean reward: -1.400 [-100.000,  2.672], mean action: 1.776 [0.000, 3.000],  loss: 18023185.619159, mse: 44244356584.074768, mean_q: 161197.468421, mean_eps: 0.720050
  93517/300000: episode: 903, duration: 0.731s, episode steps:  93, steps per second: 127, episode reward: -447.936, mean reward: -4.817 [-100.000,  0.219], mean action: 1.828 [0.000, 3.000],  loss: 16084095.924731, mse: 45656935941.505379, mean_q: 164257.039062, mean_eps: 0.719590
  93631/300000: episode: 904, duration: 1.196s, episode steps: 114, steps per second:  95, episode reward: -88.064, mean reward: -0.772 [-100.000, 20.469], mean action: 1.588 [0.000, 3.000],  loss: 17745645.868421, mse: 47332889007.157898, mean_q: 166153.626508, mean_eps: 0.719279
  93733/300000: episode: 905, duration: 0.767s, episode steps: 102, steps per second: 133, episode reward: -83.435, mean reward: -0.818 [-100.000, 10.403], mean action: 1.824 [0.000, 3.000],  loss: 18294093.960784, mse: 52941056341.333336, mean_q: 177881.653339, mean_eps: 0.718955
  93824/300000: episode: 906, duration: 0.699s, episode steps:  91, steps per second: 130, episode reward: -140.372, mean reward: -1.543 [-100.000, 10.058], mean action: 1.857 [0.000, 3.000],  loss: 21544653.934066, mse: 53822543826.989014, mean_q: 177779.185440, mean_eps: 0.718666
  93891/300000: episode: 907, duration: 0.492s, episode steps:  67, steps per second: 136, episode reward: -340.550, mean reward: -5.083 [-100.000,  1.591], mean action: 1.985 [0.000, 3.000],  loss: 39879491.641791, mse: 57637185291.462685, mean_q: 187755.517257, mean_eps: 0.718429
  94004/300000: episode: 908, duration: 0.783s, episode steps: 113, steps per second: 144, episode reward: -207.911, mean reward: -1.840 [-100.000, 17.680], mean action: 1.549 [0.000, 3.000],  loss: 36198893.783186, mse: 57736499943.079643, mean_q: 184011.362279, mean_eps: 0.718159
  94106/300000: episode: 909, duration: 0.716s, episode steps: 102, steps per second: 143, episode reward: -180.657, mean reward: -1.771 [-100.000,  2.592], mean action: 1.784 [0.000, 3.000],  loss: 45565728.745098, mse: 61894814579.450981, mean_q: 191002.211550, mean_eps: 0.717836
  94226/300000: episode: 910, duration: 0.824s, episode steps: 120, steps per second: 146, episode reward: -368.978, mean reward: -3.075 [-100.000,  1.313], mean action: 1.575 [0.000, 3.000],  loss: 31686741.012500, mse: 64215435144.533333, mean_q: 195610.374219, mean_eps: 0.717504
  94313/300000: episode: 911, duration: 0.616s, episode steps:  87, steps per second: 141, episode reward: -62.165, mean reward: -0.715 [-100.000,  7.678], mean action: 1.667 [0.000, 3.000],  loss: 33722468.879310, mse: 67368564830.160919, mean_q: 200974.655172, mean_eps: 0.717193
  94394/300000: episode: 912, duration: 0.611s, episode steps:  81, steps per second: 132, episode reward: -218.665, mean reward: -2.700 [-100.000,  9.312], mean action: 1.432 [0.000, 3.000],  loss: 47961551.975309, mse: 69895504655.802475, mean_q: 205592.431713, mean_eps: 0.716941
  94499/300000: episode: 913, duration: 0.843s, episode steps: 105, steps per second: 125, episode reward: -217.272, mean reward: -2.069 [-100.000,  1.287], mean action: 1.390 [0.000, 3.000],  loss: 27941736.576190, mse: 73851990796.190475, mean_q: 210198.155655, mean_eps: 0.716662
  94753/300000: episode: 914, duration: 2.168s, episode steps: 254, steps per second: 117, episode reward: -145.626, mean reward: -0.573 [-100.000,  8.248], mean action: 1.748 [0.000, 3.000],  loss: 30090961.639764, mse: 77105939286.677170, mean_q: 214471.007443, mean_eps: 0.716124
  94851/300000: episode: 915, duration: 0.764s, episode steps:  98, steps per second: 128, episode reward: -125.920, mean reward: -1.285 [-100.000,  8.326], mean action: 1.622 [0.000, 3.000],  loss: 31553417.882653, mse: 78964718612.897964, mean_q: 212084.896205, mean_eps: 0.715595
  94982/300000: episode: 916, duration: 1.325s, episode steps: 131, steps per second:  99, episode reward: -184.055, mean reward: -1.405 [-100.000,  2.203], mean action: 1.733 [0.000, 3.000],  loss: 32979760.076336, mse: 84928956353.465652, mean_q: 223045.782264, mean_eps: 0.715252
  95105/300000: episode: 917, duration: 1.090s, episode steps: 123, steps per second: 113, episode reward: -367.242, mean reward: -2.986 [-100.000,  1.542], mean action: 1.772 [0.000, 3.000],  loss: 30467707.012195, mse: 90781460163.642273, mean_q: 229613.858994, mean_eps: 0.714871
  95197/300000: episode: 918, duration: 0.877s, episode steps:  92, steps per second: 105, episode reward: -291.539, mean reward: -3.169 [-100.000,  5.072], mean action: 1.772 [0.000, 3.000],  loss: 37892116.923913, mse: 99154417975.652176, mean_q: 244691.233865, mean_eps: 0.714549
  95296/300000: episode: 919, duration: 0.762s, episode steps:  99, steps per second: 130, episode reward: -182.261, mean reward: -1.841 [-100.000,  1.946], mean action: 1.828 [0.000, 3.000],  loss: 127512166.989899, mse: 101210323337.050507, mean_q: 244156.956597, mean_eps: 0.714262
  95417/300000: episode: 920, duration: 0.941s, episode steps: 121, steps per second: 129, episode reward: -281.797, mean reward: -2.329 [-100.000, 28.739], mean action: 1.694 [0.000, 3.000],  loss: 79477142.396694, mse: 110907110797.752060, mean_q: 260177.252841, mean_eps: 0.713932
  95534/300000: episode: 921, duration: 0.808s, episode steps: 117, steps per second: 145, episode reward: -441.821, mean reward: -3.776 [-100.000,  0.129], mean action: 1.761 [0.000, 3.000],  loss: 116078887.076923, mse: 108506662264.341873, mean_q: 254073.566106, mean_eps: 0.713575
  95684/300000: episode: 922, duration: 1.048s, episode steps: 150, steps per second: 143, episode reward: -60.995, mean reward: -0.407 [-100.000, 17.923], mean action: 1.793 [0.000, 3.000],  loss: 33029881.213333, mse: 113661953310.720001, mean_q: 259254.680938, mean_eps: 0.713175
  95786/300000: episode: 923, duration: 0.745s, episode steps: 102, steps per second: 137, episode reward: -241.872, mean reward: -2.371 [-100.000,  0.662], mean action: 1.667 [0.000, 3.000],  loss: 50701319.843137, mse: 123279942917.019608, mean_q: 268808.443168, mean_eps: 0.712796
  95895/300000: episode: 924, duration: 0.740s, episode steps: 109, steps per second: 147, episode reward: -71.086, mean reward: -0.652 [-100.000, 41.663], mean action: 1.514 [0.000, 3.000],  loss: 79911639.293578, mse: 123730360188.477066, mean_q: 269547.728641, mean_eps: 0.712480
  96018/300000: episode: 925, duration: 0.838s, episode steps: 123, steps per second: 147, episode reward: -190.627, mean reward: -1.550 [-100.000,  5.931], mean action: 1.618 [0.000, 3.000],  loss: 109276195.081301, mse: 126892660186.536591, mean_q: 272924.383130, mean_eps: 0.712132
  96167/300000: episode: 926, duration: 1.047s, episode steps: 149, steps per second: 142, episode reward: -193.705, mean reward: -1.300 [-100.000, 15.010], mean action: 1.651 [0.000, 3.000],  loss: 48050608.476510, mse: 128334571368.805374, mean_q: 274596.967596, mean_eps: 0.711724
  96256/300000: episode: 927, duration: 0.600s, episode steps:  89, steps per second: 148, episode reward: -242.597, mean reward: -2.726 [-100.000, 84.271], mean action: 1.584 [0.000, 3.000],  loss: 51492773.752809, mse: 135826760416.359558, mean_q: 286095.134480, mean_eps: 0.711367
  96324/300000: episode: 928, duration: 0.484s, episode steps:  68, steps per second: 140, episode reward: -66.219, mean reward: -0.974 [-100.000,  8.104], mean action: 1.441 [0.000, 3.000],  loss: 224531811.794118, mse: 143837408557.176483, mean_q: 296672.021599, mean_eps: 0.711131
  96408/300000: episode: 929, duration: 0.674s, episode steps:  84, steps per second: 125, episode reward: -174.057, mean reward: -2.072 [-100.000, 12.933], mean action: 1.631 [0.000, 3.000],  loss: 55319978.392857, mse: 144588436333.714294, mean_q: 293471.607515, mean_eps: 0.710904
  96497/300000: episode: 930, duration: 0.680s, episode steps:  89, steps per second: 131, episode reward: -465.147, mean reward: -5.226 [-100.000,  0.232], mean action: 1.753 [0.000, 3.000],  loss: 73782824.494382, mse: 152417096784.539337, mean_q: 298276.256847, mean_eps: 0.710644
  96629/300000: episode: 931, duration: 1.003s, episode steps: 132, steps per second: 132, episode reward: -174.359, mean reward: -1.321 [-100.000, 10.395], mean action: 1.629 [0.000, 3.000],  loss: 78529695.840909, mse: 165052830316.606049, mean_q: 314187.921165, mean_eps: 0.710312
  96779/300000: episode: 932, duration: 1.191s, episode steps: 150, steps per second: 126, episode reward: -289.604, mean reward: -1.931 [-100.000,  1.805], mean action: 1.653 [0.000, 3.000],  loss: 186251107.013333, mse: 171951618102.613342, mean_q: 319375.198021, mean_eps: 0.709889
  96887/300000: episode: 933, duration: 0.819s, episode steps: 108, steps per second: 132, episode reward: -315.663, mean reward: -2.923 [-100.000,  1.893], mean action: 1.704 [0.000, 3.000],  loss: 188035472.500000, mse: 176342522766.222229, mean_q: 320272.435909, mean_eps: 0.709503
  97000/300000: episode: 934, duration: 0.809s, episode steps: 113, steps per second: 140, episode reward: -138.358, mean reward: -1.224 [-100.000,  2.714], mean action: 1.858 [0.000, 3.000],  loss: 72196310.442478, mse: 187042858482.407074, mean_q: 331374.408601, mean_eps: 0.709171
  97134/300000: episode: 935, duration: 0.916s, episode steps: 134, steps per second: 146, episode reward: -387.771, mean reward: -2.894 [-100.000,  1.972], mean action: 1.694 [0.000, 3.000],  loss: 185651792.597015, mse: 191771939121.671631, mean_q: 340845.974464, mean_eps: 0.708800
  97244/300000: episode: 936, duration: 0.780s, episode steps: 110, steps per second: 141, episode reward: -496.659, mean reward: -4.515 [-100.000,  7.516], mean action: 1.627 [0.000, 3.000],  loss: 70443770.818182, mse: 201896742018.327271, mean_q: 344495.800142, mean_eps: 0.708434
  97339/300000: episode: 937, duration: 0.655s, episode steps:  95, steps per second: 145, episode reward: -447.911, mean reward: -4.715 [-100.000,  0.154], mean action: 1.663 [0.000, 3.000],  loss: 218181308.547368, mse: 211173041249.010529, mean_q: 352491.550658, mean_eps: 0.708127
  97444/300000: episode: 938, duration: 0.711s, episode steps: 105, steps per second: 148, episode reward: -331.929, mean reward: -3.161 [-100.000,  0.884], mean action: 1.705 [0.000, 3.000],  loss: 109037267.238095, mse: 211755980741.485718, mean_q: 351132.980804, mean_eps: 0.707827
  97559/300000: episode: 939, duration: 0.824s, episode steps: 115, steps per second: 140, episode reward: -502.592, mean reward: -4.370 [-100.000,  1.818], mean action: 1.574 [0.000, 3.000],  loss: 213449475.095652, mse: 227079293301.982605, mean_q: 366955.427446, mean_eps: 0.707497
  97734/300000: episode: 940, duration: 1.216s, episode steps: 175, steps per second: 144, episode reward: -104.322, mean reward: -0.596 [-100.000,  3.334], mean action: 1.731 [0.000, 3.000],  loss: 151735762.971429, mse: 235957602502.948578, mean_q: 377744.768393, mean_eps: 0.707062
  97906/300000: episode: 941, duration: 1.209s, episode steps: 172, steps per second: 142, episode reward: -167.207, mean reward: -0.972 [-100.000, 25.759], mean action: 1.698 [0.000, 3.000],  loss: 228341397.825581, mse: 258723235601.860474, mean_q: 393946.761083, mean_eps: 0.706542
  98001/300000: episode: 942, duration: 0.662s, episode steps:  95, steps per second: 144, episode reward: -356.208, mean reward: -3.750 [-100.000,  1.760], mean action: 1.926 [0.000, 3.000],  loss: 174938666.021053, mse: 266350386768.842102, mean_q: 393763.306908, mean_eps: 0.706141
  98218/300000: episode: 943, duration: 1.546s, episode steps: 217, steps per second: 140, episode reward: -24.911, mean reward: -0.115 [-100.000, 90.139], mean action: 1.631 [0.000, 3.000],  loss: 320175007.695853, mse: 294710302918.193542, mean_q: 422480.045363, mean_eps: 0.705673
  98407/300000: episode: 944, duration: 1.289s, episode steps: 189, steps per second: 147, episode reward: -99.757, mean reward: -0.528 [-100.000,  7.152], mean action: 1.677 [0.000, 3.000],  loss: 133339546.296296, mse: 309119216211.978821, mean_q: 432427.470073, mean_eps: 0.705064
  98556/300000: episode: 945, duration: 1.037s, episode steps: 149, steps per second: 144, episode reward: -233.924, mean reward: -1.570 [-100.000, 14.512], mean action: 1.597 [0.000, 3.000],  loss: 123009148.644295, mse: 326730913475.865784, mean_q: 445308.972945, mean_eps: 0.704557
  98712/300000: episode: 946, duration: 1.066s, episode steps: 156, steps per second: 146, episode reward: -431.346, mean reward: -2.765 [-100.000,  1.960], mean action: 1.583 [0.000, 3.000],  loss: 163346807.923077, mse: 361592745984.000000, mean_q: 468902.071915, mean_eps: 0.704099
  98846/300000: episode: 947, duration: 0.930s, episode steps: 134, steps per second: 144, episode reward: -151.075, mean reward: -1.127 [-100.000, 21.313], mean action: 1.791 [0.000, 3.000],  loss: 147018546.373134, mse: 377555792422.208984, mean_q: 475521.945662, mean_eps: 0.703665
  99030/300000: episode: 948, duration: 1.256s, episode steps: 184, steps per second: 147, episode reward: -29.198, mean reward: -0.159 [-100.000, 69.458], mean action: 1.641 [0.000, 3.000],  loss: 152588549.521739, mse: 410382489689.043457, mean_q: 499269.227072, mean_eps: 0.703188
  99117/300000: episode: 949, duration: 0.635s, episode steps:  87, steps per second: 137, episode reward: -25.624, mean reward: -0.295 [-100.000, 12.767], mean action: 1.701 [0.000, 3.000],  loss: 169561323.080460, mse: 416942002411.402283, mean_q: 506776.073635, mean_eps: 0.702781
  99223/300000: episode: 950, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -264.307, mean reward: -2.493 [-100.000,  1.562], mean action: 1.660 [0.000, 3.000],  loss: 379154643.132075, mse: 440464247827.320740, mean_q: 514404.118514, mean_eps: 0.702491
  99336/300000: episode: 951, duration: 0.768s, episode steps: 113, steps per second: 147, episode reward: -113.686, mean reward: -1.006 [-100.000,  7.977], mean action: 1.805 [0.000, 3.000],  loss: 180581810.973451, mse: 446071188815.292053, mean_q: 518274.840985, mean_eps: 0.702163
  99459/300000: episode: 952, duration: 0.868s, episode steps: 123, steps per second: 142, episode reward: -309.508, mean reward: -2.516 [-100.000,  0.841], mean action: 1.805 [0.000, 3.000],  loss: 244670670.764228, mse: 460047753415.804871, mean_q: 524725.719512, mean_eps: 0.701809
  99566/300000: episode: 953, duration: 0.735s, episode steps: 107, steps per second: 146, episode reward: -217.351, mean reward: -2.031 [-100.000,  0.872], mean action: 1.804 [0.000, 3.000],  loss: 215848844.112150, mse: 475248660671.401855, mean_q: 537071.929322, mean_eps: 0.701464
  99702/300000: episode: 954, duration: 0.981s, episode steps: 136, steps per second: 139, episode reward: -68.907, mean reward: -0.507 [-100.000,  7.378], mean action: 1.721 [0.000, 3.000],  loss: 228653410.529412, mse: 525126842127.058838, mean_q: 569021.369715, mean_eps: 0.701099
  99810/300000: episode: 955, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -227.575, mean reward: -2.107 [-100.000,  4.760], mean action: 1.611 [0.000, 3.000],  loss: 253419744.296296, mse: 526439795067.259277, mean_q: 569275.756076, mean_eps: 0.700734
  99910/300000: episode: 956, duration: 0.691s, episode steps: 100, steps per second: 145, episode reward: -41.739, mean reward: -0.417 [-100.000, 20.302], mean action: 1.680 [0.000, 3.000],  loss: 225281184.440000, mse: 569915449016.319946, mean_q: 588844.910937, mean_eps: 0.700421
  99996/300000: episode: 957, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -181.400, mean reward: -2.109 [-100.000,  6.899], mean action: 1.616 [0.000, 3.000],  loss: 236902031.906977, mse: 566340865381.209351, mean_q: 584988.204215, mean_eps: 0.700142
 100108/300000: episode: 958, duration: 0.767s, episode steps: 112, steps per second: 146, episode reward: -395.968, mean reward: -3.535 [-100.000,  1.203], mean action: 1.679 [0.000, 3.000],  loss: 268353702.750000, mse: 574701059218.285767, mean_q: 590251.803850, mean_eps: 0.699846
 100232/300000: episode: 959, duration: 0.850s, episode steps: 124, steps per second: 146, episode reward: -50.727, mean reward: -0.409 [-100.000, 43.965], mean action: 1.653 [0.000, 3.000],  loss: 334497283.903226, mse: 629809986526.967773, mean_q: 621279.371220, mean_eps: 0.699492
 100342/300000: episode: 960, duration: 0.804s, episode steps: 110, steps per second: 137, episode reward: -402.940, mean reward: -3.663 [-100.000,  0.463], mean action: 1.718 [0.000, 3.000],  loss: 329536246.836364, mse: 653709427581.672729, mean_q: 624689.449432, mean_eps: 0.699141
 100448/300000: episode: 961, duration: 0.715s, episode steps: 106, steps per second: 148, episode reward: -56.552, mean reward: -0.534 [-100.000, 11.357], mean action: 1.670 [0.000, 3.000],  loss: 289989944.226415, mse: 659795591168.000000, mean_q: 628338.104363, mean_eps: 0.698817
 100556/300000: episode: 962, duration: 0.730s, episode steps: 108, steps per second: 148, episode reward: -234.201, mean reward: -2.169 [-100.000,  2.229], mean action: 1.731 [0.000, 3.000],  loss: 300980624.814815, mse: 723669694388.148193, mean_q: 669869.527778, mean_eps: 0.698496
 100732/300000: episode: 963, duration: 1.238s, episode steps: 176, steps per second: 142, episode reward: -322.706, mean reward: -1.834 [-100.000,  3.850], mean action: 1.506 [0.000, 3.000],  loss: 328062332.000000, mse: 782649636305.454590, mean_q: 693968.273970, mean_eps: 0.698070
 100833/300000: episode: 964, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -587.658, mean reward: -5.818 [-100.000,  1.126], mean action: 1.752 [0.000, 3.000],  loss: 1039948324.198020, mse: 835299105923.802002, mean_q: 717756.430074, mean_eps: 0.697654
 100935/300000: episode: 965, duration: 0.735s, episode steps: 102, steps per second: 139, episode reward: -317.559, mean reward: -3.113 [-100.000,  6.451], mean action: 1.598 [0.000, 3.000],  loss: 420756107.450980, mse: 859989744860.862793, mean_q: 728407.623162, mean_eps: 0.697350
 101010/300000: episode: 966, duration: 0.520s, episode steps:  75, steps per second: 144, episode reward: -26.340, mean reward: -0.351 [-100.000, 25.574], mean action: 1.600 [0.000, 3.000],  loss: 473290800.106667, mse: 868310994998.613281, mean_q: 732818.565000, mean_eps: 0.697084
 101151/300000: episode: 967, duration: 0.964s, episode steps: 141, steps per second: 146, episode reward: -415.759, mean reward: -2.949 [-100.000,  1.696], mean action: 1.752 [0.000, 3.000],  loss: 319596174.978723, mse: 881728038897.475220, mean_q: 733756.302305, mean_eps: 0.696760
 101247/300000: episode: 968, duration: 0.727s, episode steps:  96, steps per second: 132, episode reward: -250.514, mean reward: -2.610 [-100.000,  1.198], mean action: 1.656 [0.000, 3.000],  loss: 358598711.833333, mse: 944400777898.666626, mean_q: 759074.412109, mean_eps: 0.696404
 101349/300000: episode: 969, duration: 0.713s, episode steps: 102, steps per second: 143, episode reward: -362.727, mean reward: -3.556 [-100.000,  1.279], mean action: 1.716 [0.000, 3.000],  loss: 1128592750.117647, mse: 984999557240.470581, mean_q: 780396.569240, mean_eps: 0.696107
 101428/300000: episode: 970, duration: 0.532s, episode steps:  79, steps per second: 148, episode reward: -435.000, mean reward: -5.506 [-100.000, -0.415], mean action: 1.873 [0.000, 3.000],  loss: 365403774.379747, mse: 983258666256.202515, mean_q: 780362.705301, mean_eps: 0.695836
 101598/300000: episode: 971, duration: 1.219s, episode steps: 170, steps per second: 139, episode reward: -291.854, mean reward: -1.717 [-100.000, 19.010], mean action: 1.682 [0.000, 3.000],  loss: 457346964.988235, mse: 1032232051169.882324, mean_q: 793411.702574, mean_eps: 0.695462
 101681/300000: episode: 972, duration: 0.699s, episode steps:  83, steps per second: 119, episode reward: -212.957, mean reward: -2.566 [-100.000,  8.561], mean action: 1.301 [0.000, 3.000],  loss: 1039239758.361446, mse: 1098104224410.216919, mean_q: 821101.094127, mean_eps: 0.695083
 101772/300000: episode: 973, duration: 0.697s, episode steps:  91, steps per second: 131, episode reward: -75.740, mean reward: -0.832 [-100.000,  9.887], mean action: 1.802 [0.000, 3.000],  loss: 638105751.560440, mse: 1115732082688.000000, mean_q: 832133.452610, mean_eps: 0.694822
 101889/300000: episode: 974, duration: 0.863s, episode steps: 117, steps per second: 136, episode reward: 59.451, mean reward:  0.508 [-100.000, 78.833], mean action: 1.846 [0.000, 3.000],  loss: 954995285.333333, mse: 1182925998132.512939, mean_q: 853172.663996, mean_eps: 0.694510
 102008/300000: episode: 975, duration: 0.805s, episode steps: 119, steps per second: 148, episode reward: -461.124, mean reward: -3.875 [-100.000,  3.299], mean action: 1.824 [0.000, 3.000],  loss: 556439768.605042, mse: 1201397829941.781494, mean_q: 861385.528361, mean_eps: 0.694156
 102182/300000: episode: 976, duration: 1.237s, episode steps: 174, steps per second: 141, episode reward: -45.892, mean reward: -0.264 [-100.000,  7.995], mean action: 1.632 [0.000, 3.000],  loss: 1024175153.287356, mse: 1296294093647.448242, mean_q: 889689.497126, mean_eps: 0.693716
 102384/300000: episode: 977, duration: 1.463s, episode steps: 202, steps per second: 138, episode reward: -117.378, mean reward: -0.581 [-100.000, 18.366], mean action: 1.594 [0.000, 3.000],  loss: 685644353.663366, mse: 1397717194934.495117, mean_q: 931159.839109, mean_eps: 0.693153
 102488/300000: episode: 978, duration: 0.740s, episode steps: 104, steps per second: 141, episode reward: -44.293, mean reward: -0.426 [-100.000, 104.496], mean action: 1.712 [0.000, 3.000],  loss: 874632923.538462, mse: 1445614488024.615479, mean_q: 950636.780048, mean_eps: 0.692694
 102611/300000: episode: 979, duration: 0.832s, episode steps: 123, steps per second: 148, episode reward: -393.711, mean reward: -3.201 [-100.000,  3.300], mean action: 1.911 [0.000, 3.000],  loss: 632048051.382114, mse: 1495435991422.959229, mean_q: 962595.704268, mean_eps: 0.692353
 102717/300000: episode: 980, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -59.902, mean reward: -0.565 [-100.000,  8.063], mean action: 1.726 [0.000, 3.000],  loss: 1527181796.075472, mse: 1563558450620.377441, mean_q: 986238.745873, mean_eps: 0.692009
 102814/300000: episode: 981, duration: 0.686s, episode steps:  97, steps per second: 141, episode reward: -298.388, mean reward: -3.076 [-100.000,  1.190], mean action: 1.701 [0.000, 3.000],  loss: 888836620.865979, mse: 1628919445356.206299, mean_q: 998359.904639, mean_eps: 0.691705
 102998/300000: episode: 982, duration: 1.265s, episode steps: 184, steps per second: 145, episode reward: -253.132, mean reward: -1.376 [-100.000,  2.682], mean action: 1.500 [0.000, 3.000],  loss: 1048030456.565217, mse: 1685823775788.521729, mean_q: 1023007.279891, mean_eps: 0.691283
 103101/300000: episode: 983, duration: 0.724s, episode steps: 103, steps per second: 142, episode reward: -325.458, mean reward: -3.160 [-100.000,  1.650], mean action: 1.835 [0.000, 3.000],  loss: 2051770544.466019, mse: 1824385462550.368896, mean_q: 1060122.549757, mean_eps: 0.690853
 103197/300000: episode: 984, duration: 0.660s, episode steps:  96, steps per second: 145, episode reward: -224.047, mean reward: -2.334 [-100.000, 10.028], mean action: 1.583 [0.000, 3.000],  loss: 1198612207.500000, mse: 1882665411242.666748, mean_q: 1082257.411458, mean_eps: 0.690554
 103326/300000: episode: 985, duration: 0.894s, episode steps: 129, steps per second: 144, episode reward: -237.003, mean reward: -1.837 [-100.000,  2.085], mean action: 1.643 [0.000, 3.000],  loss: 1019934794.914729, mse: 1985006320266.914795, mean_q: 1105131.250000, mean_eps: 0.690217
 103524/300000: episode: 986, duration: 1.387s, episode steps: 198, steps per second: 143, episode reward: -293.974, mean reward: -1.485 [-100.000, 19.545], mean action: 1.556 [0.000, 3.000],  loss: 815428147.232323, mse: 2038343064193.292969, mean_q: 1117892.319129, mean_eps: 0.689727
 103711/300000: episode: 987, duration: 1.289s, episode steps: 187, steps per second: 145, episode reward: -234.621, mean reward: -1.255 [-100.000,  9.511], mean action: 1.690 [0.000, 3.000],  loss: 1379099743.486631, mse: 2228970353406.630859, mean_q: 1174961.350267, mean_eps: 0.689149
 103801/300000: episode: 988, duration: 0.613s, episode steps:  90, steps per second: 147, episode reward: -91.869, mean reward: -1.021 [-100.000, 19.209], mean action: 1.578 [0.000, 3.000],  loss: 1189708676.622222, mse: 2408878957454.222168, mean_q: 1236565.136806, mean_eps: 0.688733
 103874/300000: episode: 989, duration: 0.499s, episode steps:  73, steps per second: 146, episode reward: -162.610, mean reward: -2.228 [-100.000, 21.470], mean action: 1.740 [0.000, 3.000],  loss: 966915555.068493, mse: 2387285542210.630371, mean_q: 1207936.096747, mean_eps: 0.688489
 103996/300000: episode: 990, duration: 0.848s, episode steps: 122, steps per second: 144, episode reward: -67.328, mean reward: -0.552 [-100.000, 11.882], mean action: 1.721 [0.000, 3.000],  loss: 902172996.721311, mse: 2495568146230.557617, mean_q: 1240953.961578, mean_eps: 0.688197
 104078/300000: episode: 991, duration: 0.645s, episode steps:  82, steps per second: 127, episode reward: -56.087, mean reward: -0.684 [-100.000, 12.910], mean action: 1.415 [0.000, 3.000],  loss: 1044405011.707317, mse: 2489573398378.146484, mean_q: 1227384.836128, mean_eps: 0.687891
 104163/300000: episode: 992, duration: 0.698s, episode steps:  85, steps per second: 122, episode reward: -356.026, mean reward: -4.189 [-100.000,  0.005], mean action: 1.753 [0.000, 3.000],  loss: 937409645.552941, mse: 2545404770424.470703, mean_q: 1257302.085294, mean_eps: 0.687640
 104252/300000: episode: 993, duration: 0.730s, episode steps:  89, steps per second: 122, episode reward: -415.935, mean reward: -4.673 [-100.000,  0.586], mean action: 1.607 [0.000, 3.000],  loss: 2281780074.786517, mse: 2674647104063.280762, mean_q: 1283798.018258, mean_eps: 0.687379
 104369/300000: episode: 994, duration: 0.877s, episode steps: 117, steps per second: 133, episode reward: -437.820, mean reward: -3.742 [-100.000,  1.312], mean action: 1.581 [0.000, 3.000],  loss: 1023744397.401709, mse: 2807484933286.290527, mean_q: 1307979.036325, mean_eps: 0.687070
 104448/300000: episode: 995, duration: 0.605s, episode steps:  79, steps per second: 131, episode reward: -38.233, mean reward: -0.484 [-100.000,  7.418], mean action: 1.608 [0.000, 3.000],  loss: 1672159600.607595, mse: 2793417050967.493652, mean_q: 1297826.386867, mean_eps: 0.686776
 104543/300000: episode: 996, duration: 0.747s, episode steps:  95, steps per second: 127, episode reward: -35.324, mean reward: -0.372 [-100.000, 23.366], mean action: 1.663 [0.000, 3.000],  loss: 1007696131.368421, mse: 2905761459663.494629, mean_q: 1342129.638816, mean_eps: 0.686515
 104690/300000: episode: 997, duration: 1.187s, episode steps: 147, steps per second: 124, episode reward: -229.486, mean reward: -1.561 [-100.000,  2.717], mean action: 1.605 [0.000, 3.000],  loss: 1618831321.687075, mse: 3051589682155.102051, mean_q: 1374132.908163, mean_eps: 0.686152
 104770/300000: episode: 998, duration: 0.566s, episode steps:  80, steps per second: 141, episode reward: -117.182, mean reward: -1.465 [-100.000, 23.572], mean action: 1.475 [0.000, 3.000],  loss: 1493028474.000000, mse: 3316253270016.000000, mean_q: 1437053.988281, mean_eps: 0.685812
 104862/300000: episode: 999, duration: 0.656s, episode steps:  92, steps per second: 140, episode reward: -318.820, mean reward: -3.465 [-100.000,  8.089], mean action: 1.696 [0.000, 3.000],  loss: 1743582365.217391, mse: 3388586433491.478027, mean_q: 1453139.825408, mean_eps: 0.685554
 105024/300000: episode: 1000, duration: 1.100s, episode steps: 162, steps per second: 147, episode reward: -196.349, mean reward: -1.212 [-100.000, 19.963], mean action: 1.741 [0.000, 3.000],  loss: 1850416552.888889, mse: 3364892186143.604980, mean_q: 1437763.345293, mean_eps: 0.685172
 105127/300000: episode: 1001, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -59.253, mean reward: -0.575 [-100.000,  8.062], mean action: 1.573 [0.000, 3.000],  loss: 2128223288.854369, mse: 3505154023980.737793, mean_q: 1463843.183252, mean_eps: 0.684775
 105235/300000: episode: 1002, duration: 0.735s, episode steps: 108, steps per second: 147, episode reward: -333.989, mean reward: -3.092 [-100.000,  1.129], mean action: 1.574 [0.000, 3.000],  loss: 4149977181.925926, mse: 3675125380740.740723, mean_q: 1508677.075810, mean_eps: 0.684459
 105329/300000: episode: 1003, duration: 0.646s, episode steps:  94, steps per second: 146, episode reward: -285.345, mean reward: -3.036 [-100.000, 52.182], mean action: 1.617 [0.000, 3.000],  loss: 1756034983.829787, mse: 3797176153371.233887, mean_q: 1530066.390957, mean_eps: 0.684155
 105427/300000: episode: 1004, duration: 0.691s, episode steps:  98, steps per second: 142, episode reward: -207.522, mean reward: -2.118 [-100.000, 26.613], mean action: 1.490 [0.000, 3.000],  loss: 2145942216.489796, mse: 3908353926290.285645, mean_q: 1539225.022959, mean_eps: 0.683867
 105564/300000: episode: 1005, duration: 0.939s, episode steps: 137, steps per second: 146, episode reward: -349.777, mean reward: -2.553 [-100.000,  3.070], mean action: 1.599 [0.000, 3.000],  loss: 3896157853.664234, mse: 4166612860569.226074, mean_q: 1601505.522810, mean_eps: 0.683515
 105748/300000: episode: 1006, duration: 1.464s, episode steps: 184, steps per second: 126, episode reward: -342.177, mean reward: -1.860 [-100.000,  2.490], mean action: 1.685 [0.000, 3.000],  loss: 2017408657.391304, mse: 4378895242106.434570, mean_q: 1654407.733696, mean_eps: 0.683033
 106065/300000: episode: 1007, duration: 2.502s, episode steps: 317, steps per second: 127, episode reward: -275.794, mean reward: -0.870 [-100.000, 28.548], mean action: 1.760 [0.000, 3.000],  loss: 4835389831.369085, mse: 4638704132431.949219, mean_q: 1693075.061909, mean_eps: 0.682282
 106139/300000: episode: 1008, duration: 0.548s, episode steps:  74, steps per second: 135, episode reward: -123.854, mean reward: -1.674 [-100.000,  4.753], mean action: 1.676 [0.000, 3.000],  loss: 4636136140.972973, mse: 4903219013133.837891, mean_q: 1724161.543919, mean_eps: 0.681696
 106308/300000: episode: 1009, duration: 1.171s, episode steps: 169, steps per second: 144, episode reward: -469.311, mean reward: -2.777 [-100.000,  2.193], mean action: 1.746 [0.000, 3.000],  loss: 2543437151.053255, mse: 5170301077976.615234, mean_q: 1793454.935651, mean_eps: 0.681331
 106478/300000: episode: 1010, duration: 1.208s, episode steps: 170, steps per second: 141, episode reward: -241.381, mean reward: -1.420 [-100.000,  3.016], mean action: 1.694 [0.000, 3.000],  loss: 3475129382.400000, mse: 5482321353499.105469, mean_q: 1844835.648529, mean_eps: 0.680822
 106564/300000: episode: 1011, duration: 0.577s, episode steps:  86, steps per second: 149, episode reward: -37.609, mean reward: -0.437 [-100.000, 13.729], mean action: 1.547 [0.000, 3.000],  loss: 2971706565.953488, mse: 5709086893746.604492, mean_q: 1855697.956395, mean_eps: 0.680439
 106691/300000: episode: 1012, duration: 0.903s, episode steps: 127, steps per second: 141, episode reward: -368.830, mean reward: -2.904 [-100.000,  2.198], mean action: 1.732 [0.000, 3.000],  loss: 2691102877.228346, mse: 6106670182496.755859, mean_q: 1943550.137795, mean_eps: 0.680119
 106786/300000: episode: 1013, duration: 0.644s, episode steps:  95, steps per second: 147, episode reward: -153.874, mean reward: -1.620 [-100.000, 19.773], mean action: 1.621 [0.000, 3.000],  loss: 3411864241.852632, mse: 6177590469901.473633, mean_q: 1944934.334211, mean_eps: 0.679786
 106898/300000: episode: 1014, duration: 0.768s, episode steps: 112, steps per second: 146, episode reward: -471.699, mean reward: -4.212 [-100.000,  0.881], mean action: 1.750 [0.000, 3.000],  loss: 5380349411.428572, mse: 6419008351670.857422, mean_q: 1979059.271205, mean_eps: 0.679476
 107005/300000: episode: 1015, duration: 0.759s, episode steps: 107, steps per second: 141, episode reward: -216.384, mean reward: -2.022 [-100.000, 71.190], mean action: 1.645 [0.000, 3.000],  loss: 5410503721.869159, mse: 6256471702327.028320, mean_q: 1938528.789136, mean_eps: 0.679147
 107169/300000: episode: 1016, duration: 1.109s, episode steps: 164, steps per second: 148, episode reward: -67.931, mean reward: -0.414 [-100.000, 24.826], mean action: 1.628 [0.000, 3.000],  loss: 2690943511.414634, mse: 6741119222958.829102, mean_q: 2013402.828506, mean_eps: 0.678741
 107272/300000: episode: 1017, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -199.666, mean reward: -1.939 [-100.000,  4.809], mean action: 1.583 [0.000, 3.000],  loss: 2478596176.776699, mse: 6815807367585.553711, mean_q: 2015951.027913, mean_eps: 0.678340
 107384/300000: episode: 1018, duration: 0.775s, episode steps: 112, steps per second: 145, episode reward: -284.493, mean reward: -2.540 [-100.000,  1.063], mean action: 1.518 [0.000, 3.000],  loss: 3183295870.285714, mse: 7487612129865.142578, mean_q: 2130599.750000, mean_eps: 0.678018
 107552/300000: episode: 1019, duration: 1.157s, episode steps: 168, steps per second: 145, episode reward: -415.991, mean reward: -2.476 [-100.000,  2.004], mean action: 1.768 [0.000, 3.000],  loss: 5319526045.714286, mse: 7875001804702.476562, mean_q: 2187800.042411, mean_eps: 0.677598
 107640/300000: episode: 1020, duration: 0.607s, episode steps:  88, steps per second: 145, episode reward: -342.171, mean reward: -3.888 [-100.000,  0.537], mean action: 1.693 [0.000, 3.000],  loss: 4364414837.090909, mse: 7898609656552.727539, mean_q: 2169043.203125, mean_eps: 0.677214
 107834/300000: episode: 1021, duration: 1.350s, episode steps: 194, steps per second: 144, episode reward: -117.851, mean reward: -0.607 [-100.000, 23.114], mean action: 1.727 [0.000, 3.000],  loss: 4722344084.123712, mse: 8401874928122.721680, mean_q: 2257494.041237, mean_eps: 0.676790
 107943/300000: episode: 1022, duration: 0.759s, episode steps: 109, steps per second: 144, episode reward: -352.196, mean reward: -3.231 [-100.000,  0.864], mean action: 1.670 [0.000, 3.000],  loss: 2891039284.256881, mse: 8849218538430.238281, mean_q: 2318327.623853, mean_eps: 0.676336
 108087/300000: episode: 1023, duration: 0.986s, episode steps: 144, steps per second: 146, episode reward: -276.302, mean reward: -1.919 [-100.000, 30.466], mean action: 1.486 [0.000, 3.000],  loss: 5126003482.222222, mse: 9376998903352.888672, mean_q: 2378078.474826, mean_eps: 0.675956
 108194/300000: episode: 1024, duration: 0.748s, episode steps: 107, steps per second: 143, episode reward: -121.981, mean reward: -1.140 [-100.000,  6.905], mean action: 1.607 [0.000, 3.000],  loss: 7998190493.308412, mse: 9598377379476.335938, mean_q: 2392593.767523, mean_eps: 0.675580
 108287/300000: episode: 1025, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward: -216.238, mean reward: -2.325 [-100.000,  5.138], mean action: 1.430 [0.000, 3.000],  loss: 3211087774.279570, mse: 10131653787648.000000, mean_q: 2480548.370968, mean_eps: 0.675280
 108382/300000: episode: 1026, duration: 0.646s, episode steps:  95, steps per second: 147, episode reward: -186.957, mean reward: -1.968 [-100.000,  7.972], mean action: 1.621 [0.000, 3.000],  loss: 4296231622.063158, mse: 10459690099291.621094, mean_q: 2516973.792105, mean_eps: 0.674998
 108505/300000: episode: 1027, duration: 0.872s, episode steps: 123, steps per second: 141, episode reward: -301.815, mean reward: -2.454 [-100.000,  1.193], mean action: 1.496 [0.000, 3.000],  loss: 3749405373.398374, mse: 10822899195570.992188, mean_q: 2575011.235772, mean_eps: 0.674671
 108590/300000: episode: 1028, duration: 0.575s, episode steps:  85, steps per second: 148, episode reward: -365.043, mean reward: -4.295 [-100.000, -0.449], mean action: 1.565 [0.000, 3.000],  loss: 6030664119.717647, mse: 11352807380245.082031, mean_q: 2622086.335294, mean_eps: 0.674359
 108704/300000: episode: 1029, duration: 0.767s, episode steps: 114, steps per second: 149, episode reward: -397.746, mean reward: -3.489 [-100.000,  1.319], mean action: 1.605 [0.000, 3.000],  loss: 9314910976.000000, mse: 11788850951940.492188, mean_q: 2686424.768640, mean_eps: 0.674061
 108850/300000: episode: 1030, duration: 1.045s, episode steps: 146, steps per second: 140, episode reward: -113.040, mean reward: -0.774 [-100.000, 74.739], mean action: 1.575 [0.000, 3.000],  loss: 4321387985.534246, mse: 12454228006154.521484, mean_q: 2751373.704623, mean_eps: 0.673671
 108929/300000: episode: 1031, duration: 0.539s, episode steps:  79, steps per second: 147, episode reward: -417.056, mean reward: -5.279 [-100.000,  0.541], mean action: 1.570 [0.000, 3.000],  loss: 6228144005.670886, mse: 12901285410167.898438, mean_q: 2810519.761076, mean_eps: 0.673333
 109034/300000: episode: 1032, duration: 0.703s, episode steps: 105, steps per second: 149, episode reward: -90.304, mean reward: -0.860 [-100.000,  7.538], mean action: 1.533 [0.000, 3.000],  loss: 5350737445.790476, mse: 13427084226911.085938, mean_q: 2880331.865476, mean_eps: 0.673057
 109190/300000: episode: 1033, duration: 1.102s, episode steps: 156, steps per second: 142, episode reward: -89.665, mean reward: -0.575 [-100.000, 120.461], mean action: 1.558 [0.000, 3.000],  loss: 11386521104.410257, mse: 13757082773346.460938, mean_q: 2880217.447917, mean_eps: 0.672666
 109270/300000: episode: 1034, duration: 0.541s, episode steps:  80, steps per second: 148, episode reward: -415.027, mean reward: -5.188 [-100.000,  0.080], mean action: 1.775 [0.000, 3.000],  loss: 5142473299.200000, mse: 13989019169587.199219, mean_q: 2942956.475000, mean_eps: 0.672311
 109424/300000: episode: 1035, duration: 1.072s, episode steps: 154, steps per second: 144, episode reward: -225.223, mean reward: -1.462 [-100.000,  5.681], mean action: 1.584 [0.000, 3.000],  loss: 9999482313.974026, mse: 14897944338751.167969, mean_q: 3001038.137987, mean_eps: 0.671961
 109543/300000: episode: 1036, duration: 0.800s, episode steps: 119, steps per second: 149, episode reward: -140.721, mean reward: -1.183 [-100.000, 25.241], mean action: 1.546 [0.000, 3.000],  loss: 9602692467.092438, mse: 15366391713964.101562, mean_q: 3045024.819328, mean_eps: 0.671551
 109718/300000: episode: 1037, duration: 1.213s, episode steps: 175, steps per second: 144, episode reward: -274.999, mean reward: -1.571 [-100.000, 14.962], mean action: 1.549 [0.000, 3.000],  loss: 10265891474.285715, mse: 16603826573429.029297, mean_q: 3206369.135000, mean_eps: 0.671110
 109818/300000: episode: 1038, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -204.049, mean reward: -2.040 [-100.000,  1.088], mean action: 1.570 [0.000, 3.000],  loss: 5156280839.680000, mse: 17454172932669.439453, mean_q: 3261499.032500, mean_eps: 0.670697
 109909/300000: episode: 1039, duration: 0.616s, episode steps:  91, steps per second: 148, episode reward: -206.080, mean reward: -2.265 [-100.000,  6.873], mean action: 1.582 [0.000, 3.000],  loss: 11312141365.450550, mse: 17127596495073.054688, mean_q: 3209557.656593, mean_eps: 0.670411
 110165/300000: episode: 1040, duration: 1.801s, episode steps: 256, steps per second: 142, episode reward: -359.540, mean reward: -1.404 [-100.000,  4.672], mean action: 1.637 [0.000, 3.000],  loss: 8656277711.000000, mse: 18574007003136.000000, mean_q: 3349501.448242, mean_eps: 0.669890
 110239/300000: episode: 1041, duration: 0.507s, episode steps:  74, steps per second: 146, episode reward: -93.791, mean reward: -1.267 [-100.000, 10.423], mean action: 1.649 [0.000, 3.000],  loss: 12707730259.027027, mse: 19304121177281.730469, mean_q: 3410223.341216, mean_eps: 0.669395
 110387/300000: episode: 1042, duration: 1.032s, episode steps: 148, steps per second: 143, episode reward: -299.943, mean reward: -2.027 [-100.000,  2.431], mean action: 1.730 [0.000, 3.000],  loss: 19369500760.216217, mse: 20690323317400.214844, mean_q: 3532986.758446, mean_eps: 0.669062
 110672/300000: episode: 1043, duration: 2.029s, episode steps: 285, steps per second: 140, episode reward: -313.303, mean reward: -1.099 [-100.000, 43.062], mean action: 1.628 [0.000, 3.000],  loss: 22636851837.754387, mse: 21664628368078.597656, mean_q: 3606167.490789, mean_eps: 0.668413
 110780/300000: episode: 1044, duration: 0.741s, episode steps: 108, steps per second: 146, episode reward: -243.424, mean reward: -2.254 [-100.000,  0.753], mean action: 1.602 [0.000, 3.000],  loss: 6813028027.259259, mse: 22707185870923.851562, mean_q: 3694784.416667, mean_eps: 0.667824
 110914/300000: episode: 1045, duration: 0.932s, episode steps: 134, steps per second: 144, episode reward: -321.303, mean reward: -2.398 [-100.000,  6.423], mean action: 1.619 [0.000, 3.000],  loss: 23392714430.089554, mse: 23568316672030.566406, mean_q: 3774236.296642, mean_eps: 0.667461
 111054/300000: episode: 1046, duration: 0.994s, episode steps: 140, steps per second: 141, episode reward: -243.039, mean reward: -1.736 [-100.000, 22.577], mean action: 1.700 [0.000, 3.000],  loss: 11688797046.857143, mse: 24296045629557.027344, mean_q: 3828621.703571, mean_eps: 0.667049
 111179/300000: episode: 1047, duration: 0.842s, episode steps: 125, steps per second: 148, episode reward: -265.051, mean reward: -2.120 [-100.000,  2.561], mean action: 1.736 [0.000, 3.000],  loss: 12826918154.240000, mse: 26475085665665.023438, mean_q: 4008333.498000, mean_eps: 0.666652
 111447/300000: episode: 1048, duration: 1.911s, episode steps: 268, steps per second: 140, episode reward: -203.885, mean reward: -0.761 [-100.000, 10.093], mean action: 1.672 [0.000, 3.000],  loss: 24392286292.059700, mse: 27992221814600.597656, mean_q: 4125500.481343, mean_eps: 0.666063
 111576/300000: episode: 1049, duration: 0.917s, episode steps: 129, steps per second: 141, episode reward: -92.804, mean reward: -0.719 [-100.000, 13.537], mean action: 1.597 [0.000, 3.000],  loss: 26245443500.651161, mse: 29295048268744.433594, mean_q: 4209182.021318, mean_eps: 0.665467
 111686/300000: episode: 1050, duration: 0.774s, episode steps: 110, steps per second: 142, episode reward: -314.144, mean reward: -2.856 [-100.000,  1.526], mean action: 1.745 [0.000, 3.000],  loss: 16511450817.163637, mse: 29129638736393.308594, mean_q: 4159290.547727, mean_eps: 0.665108
 111793/300000: episode: 1051, duration: 0.721s, episode steps: 107, steps per second: 148, episode reward: -288.006, mean reward: -2.692 [-100.000,  1.122], mean action: 1.720 [0.000, 3.000],  loss: 11685505299.140186, mse: 31699171538542.054688, mean_q: 4380850.990654, mean_eps: 0.664783
 111895/300000: episode: 1052, duration: 0.710s, episode steps: 102, steps per second: 144, episode reward: -200.359, mean reward: -1.964 [-100.000, 19.192], mean action: 1.471 [0.000, 3.000],  loss: 13631970017.882353, mse: 33273383007412.707031, mean_q: 4526587.497549, mean_eps: 0.664469
 112063/300000: episode: 1053, duration: 1.163s, episode steps: 168, steps per second: 144, episode reward: -608.854, mean reward: -3.624 [-100.000,  2.516], mean action: 1.643 [0.000, 3.000],  loss: 13280499027.809525, mse: 34658358706956.191406, mean_q: 4604702.593750, mean_eps: 0.664065
 112155/300000: episode: 1054, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -405.641, mean reward: -4.409 [-100.000,  0.447], mean action: 1.500 [0.000, 3.000],  loss: 11485456589.913044, mse: 36325919669559.648438, mean_q: 4721567.771739, mean_eps: 0.663675
 112256/300000: episode: 1055, duration: 0.741s, episode steps: 101, steps per second: 136, episode reward: -314.415, mean reward: -3.113 [-100.000,  1.161], mean action: 1.673 [0.000, 3.000],  loss: 43431282170.930695, mse: 35948577516797.468750, mean_q: 4685907.970297, mean_eps: 0.663385
 112355/300000: episode: 1056, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -368.317, mean reward: -3.720 [-100.000,  0.342], mean action: 1.768 [0.000, 3.000],  loss: 20033165886.060608, mse: 38376521310952.726562, mean_q: 4851317.255051, mean_eps: 0.663085
 112457/300000: episode: 1057, duration: 0.696s, episode steps: 102, steps per second: 146, episode reward: -240.204, mean reward: -2.355 [-100.000,  0.855], mean action: 1.735 [0.000, 3.000],  loss: 11766607159.215687, mse: 40658931152373.960938, mean_q: 5042031.985294, mean_eps: 0.662783
 112548/300000: episode: 1058, duration: 0.676s, episode steps:  91, steps per second: 135, episode reward: -274.816, mean reward: -3.020 [-100.000, 13.986], mean action: 1.571 [0.000, 3.000],  loss: 62815087520.351646, mse: 40514728977014.156250, mean_q: 4956249.423077, mean_eps: 0.662494
 112674/300000: episode: 1059, duration: 0.888s, episode steps: 126, steps per second: 142, episode reward: -419.461, mean reward: -3.329 [-100.000,  1.953], mean action: 1.786 [0.000, 3.000],  loss: 23594125316.063492, mse: 42278445625018.921875, mean_q: 5109723.498016, mean_eps: 0.662168
 112790/300000: episode: 1060, duration: 0.783s, episode steps: 116, steps per second: 148, episode reward: -204.743, mean reward: -1.765 [-100.000,  1.452], mean action: 1.828 [0.000, 3.000],  loss: 16786197499.586206, mse: 44315641868994.203125, mean_q: 5234834.047414, mean_eps: 0.661805
 112966/300000: episode: 1061, duration: 1.245s, episode steps: 176, steps per second: 141, episode reward: -274.337, mean reward: -1.559 [-100.000, 22.678], mean action: 1.528 [0.000, 3.000],  loss: 52948193338.181816, mse: 45736297329757.093750, mean_q: 5293075.776989, mean_eps: 0.661367
 113067/300000: episode: 1062, duration: 0.680s, episode steps: 101, steps per second: 148, episode reward: -187.994, mean reward: -1.861 [-100.000, 90.563], mean action: 1.545 [0.000, 3.000],  loss: 12608163049.188118, mse: 48311065739588.437500, mean_q: 5457818.051980, mean_eps: 0.660952
 113154/300000: episode: 1063, duration: 0.617s, episode steps:  87, steps per second: 141, episode reward: -327.064, mean reward: -3.759 [-100.000,  0.571], mean action: 1.678 [0.000, 3.000],  loss: 16356631028.229885, mse: 49770658490109.054688, mean_q: 5546483.224138, mean_eps: 0.660670
 113250/300000: episode: 1064, duration: 0.663s, episode steps:  96, steps per second: 145, episode reward: -331.254, mean reward: -3.451 [-100.000,  1.870], mean action: 1.708 [0.000, 3.000],  loss: 16925235216.000000, mse: 52602403138218.664062, mean_q: 5724313.026042, mean_eps: 0.660396
 113337/300000: episode: 1065, duration: 0.589s, episode steps:  87, steps per second: 148, episode reward: -320.987, mean reward: -3.690 [-100.000,  1.151], mean action: 1.736 [0.000, 3.000],  loss: 17058918947.310345, mse: 53524871912812.875000, mean_q: 5789290.201149, mean_eps: 0.660121
 113433/300000: episode: 1066, duration: 0.770s, episode steps:  96, steps per second: 125, episode reward: -374.259, mean reward: -3.899 [-100.000,  0.611], mean action: 1.708 [0.000, 3.000],  loss: 21860603376.000000, mse: 51840564002816.000000, mean_q: 5645728.653646, mean_eps: 0.659847
 113562/300000: episode: 1067, duration: 1.100s, episode steps: 129, steps per second: 117, episode reward: -163.443, mean reward: -1.267 [-100.000,  8.484], mean action: 1.698 [0.000, 3.000],  loss: 69995704907.410858, mse: 54803162931644.523438, mean_q: 5794639.370155, mean_eps: 0.659509
 113765/300000: episode: 1068, duration: 1.697s, episode steps: 203, steps per second: 120, episode reward: -425.212, mean reward: -2.095 [-100.000,  6.720], mean action: 1.709 [0.000, 3.000],  loss: 18142824377.379311, mse: 58169550837427.070312, mean_q: 6016035.805419, mean_eps: 0.659011
 113988/300000: episode: 1069, duration: 1.792s, episode steps: 223, steps per second: 124, episode reward: -346.648, mean reward: -1.554 [-100.000, 14.594], mean action: 1.771 [0.000, 3.000],  loss: 29649035856.358746, mse: 62251553426445.773438, mean_q: 6247471.948430, mean_eps: 0.658372
 114083/300000: episode: 1070, duration: 0.673s, episode steps:  95, steps per second: 141, episode reward: -137.939, mean reward: -1.452 [-100.000,  8.979], mean action: 1.621 [0.000, 3.000],  loss: 28042578211.031578, mse: 67946170479238.734375, mean_q: 6547624.502632, mean_eps: 0.657895
 114211/300000: episode: 1071, duration: 0.870s, episode steps: 128, steps per second: 147, episode reward: -340.633, mean reward: -2.661 [-100.000,  1.865], mean action: 1.742 [0.000, 3.000],  loss: 42010220744.000000, mse: 69344060375040.000000, mean_q: 6622426.726562, mean_eps: 0.657560
 114323/300000: episode: 1072, duration: 0.802s, episode steps: 112, steps per second: 140, episode reward: -5.510, mean reward: -0.049 [-100.000, 17.483], mean action: 1.545 [0.000, 3.000],  loss: 61760563172.571426, mse: 72513879202669.718750, mean_q: 6779967.897321, mean_eps: 0.657201
 114444/300000: episode: 1073, duration: 0.823s, episode steps: 121, steps per second: 147, episode reward: -404.160, mean reward: -3.340 [-100.000,  0.294], mean action: 1.620 [0.000, 3.000],  loss: 81329806412.165283, mse: 71844746307634.781250, mean_q: 6650302.603306, mean_eps: 0.656851
 114559/300000: episode: 1074, duration: 0.787s, episode steps: 115, steps per second: 146, episode reward: -479.740, mean reward: -4.172 [-100.000,  1.542], mean action: 1.609 [0.000, 3.000],  loss: 53335137360.139130, mse: 78193483697926.671875, mean_q: 7061097.430435, mean_eps: 0.656497
 114773/300000: episode: 1075, duration: 1.486s, episode steps: 214, steps per second: 144, episode reward: -160.962, mean reward: -0.752 [-100.000,  9.371], mean action: 1.607 [0.000, 3.000],  loss: 42552753238.130844, mse: 81003625610345.265625, mean_q: 7102922.051402, mean_eps: 0.656003
 114888/300000: episode: 1076, duration: 0.810s, episode steps: 115, steps per second: 142, episode reward: -224.497, mean reward: -1.952 [-100.000,  1.251], mean action: 1.687 [0.000, 3.000],  loss: 57681596015.304344, mse: 81940315129660.109375, mean_q: 7195242.834783, mean_eps: 0.655510
 114959/300000: episode: 1077, duration: 0.527s, episode steps:  71, steps per second: 135, episode reward: -106.564, mean reward: -1.501 [-100.000,  7.641], mean action: 1.423 [0.000, 3.000],  loss: 25236006508.169014, mse: 86088802572605.296875, mean_q: 7359306.028169, mean_eps: 0.655231
 115053/300000: episode: 1078, duration: 0.720s, episode steps:  94, steps per second: 131, episode reward: -357.103, mean reward: -3.799 [-100.000,  0.774], mean action: 1.585 [0.000, 3.000],  loss: 57721805736.851067, mse: 87545966175776.687500, mean_q: 7345939.452128, mean_eps: 0.654984
 115225/300000: episode: 1079, duration: 1.424s, episode steps: 172, steps per second: 121, episode reward: -118.228, mean reward: -0.687 [-100.000, 15.887], mean action: 1.593 [0.000, 3.000],  loss: 72837936241.116272, mse: 92271229308642.234375, mean_q: 7586891.145349, mean_eps: 0.654584
 115420/300000: episode: 1080, duration: 1.449s, episode steps: 195, steps per second: 135, episode reward: -292.792, mean reward: -1.501 [-100.000,  4.362], mean action: 1.713 [0.000, 3.000],  loss: 82324604964.758972, mse: 98706241712022.968750, mean_q: 7892435.669231, mean_eps: 0.654034
 115516/300000: episode: 1081, duration: 0.702s, episode steps:  96, steps per second: 137, episode reward: -198.965, mean reward: -2.073 [-100.000,  1.608], mean action: 1.729 [0.000, 3.000],  loss: 107014118016.000000, mse: 105303994269696.000000, mean_q: 8201987.473958, mean_eps: 0.653597
 115621/300000: episode: 1082, duration: 0.711s, episode steps: 105, steps per second: 148, episode reward: -522.142, mean reward: -4.973 [-100.000,  1.196], mean action: 1.724 [0.000, 3.000],  loss: 29677641376.914288, mse: 108994897017319.625000, mean_q: 8318321.766667, mean_eps: 0.653296
 115853/300000: episode: 1083, duration: 1.599s, episode steps: 232, steps per second: 145, episode reward: -262.553, mean reward: -1.132 [-100.000,  2.750], mean action: 1.772 [0.000, 3.000],  loss: 36144911726.344826, mse: 113351353181007.453125, mean_q: 8431822.290948, mean_eps: 0.652790
 116022/300000: episode: 1084, duration: 1.151s, episode steps: 169, steps per second: 147, episode reward: -117.978, mean reward: -0.698 [-100.000,  7.358], mean action: 1.627 [0.000, 3.000],  loss: 76709202435.029587, mse: 122815609317994.031250, mean_q: 8892537.553254, mean_eps: 0.652189
 116106/300000: episode: 1085, duration: 0.604s, episode steps:  84, steps per second: 139, episode reward: -96.807, mean reward: -1.152 [-100.000, 28.068], mean action: 1.560 [0.000, 3.000],  loss: 60262524867.047623, mse: 124644983595398.093750, mean_q: 8904773.357143, mean_eps: 0.651810
 116198/300000: episode: 1086, duration: 0.622s, episode steps:  92, steps per second: 148, episode reward: -387.446, mean reward: -4.211 [-100.000,  0.156], mean action: 1.554 [0.000, 3.000],  loss: 135537086464.000000, mse: 132013441075289.046875, mean_q: 9151721.418478, mean_eps: 0.651546
 116311/300000: episode: 1087, duration: 0.777s, episode steps: 113, steps per second: 145, episode reward: -194.500, mean reward: -1.721 [-100.000,  0.987], mean action: 1.761 [0.000, 3.000],  loss: 37159323657.061951, mse: 130108516406371.687500, mean_q: 9049068.230088, mean_eps: 0.651238
 116417/300000: episode: 1088, duration: 0.757s, episode steps: 106, steps per second: 140, episode reward: -303.745, mean reward: -2.866 [-100.000,  1.607], mean action: 1.642 [0.000, 3.000],  loss: 54137375541.132072, mse: 135185684728851.328125, mean_q: 9276682.042453, mean_eps: 0.650910
 116525/300000: episode: 1089, duration: 0.738s, episode steps: 108, steps per second: 146, episode reward: -502.804, mean reward: -4.656 [-100.000,  1.127], mean action: 1.778 [0.000, 3.000],  loss: 177706088561.777771, mse: 143205951991049.468750, mean_q: 9543210.925926, mean_eps: 0.650589
 116714/300000: episode: 1090, duration: 1.304s, episode steps: 189, steps per second: 145, episode reward: -146.150, mean reward: -0.773 [-100.000, 20.721], mean action: 1.651 [0.000, 3.000],  loss: 71304921694.814819, mse: 152261646488310.531250, mean_q: 9919808.986772, mean_eps: 0.650143
 116806/300000: episode: 1091, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -392.473, mean reward: -4.266 [-100.000,  1.046], mean action: 1.761 [0.000, 3.000],  loss: 111141347261.217392, mse: 159615059800598.250000, mean_q: 10172130.423913, mean_eps: 0.649721
 116904/300000: episode: 1092, duration: 0.661s, episode steps:  98, steps per second: 148, episode reward: -443.951, mean reward: -4.530 [-100.000,  1.203], mean action: 1.806 [0.000, 3.000],  loss: 75159706519.510208, mse: 162980294793090.625000, mean_q: 10319823.637755, mean_eps: 0.649436
 117014/300000: episode: 1093, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -356.381, mean reward: -3.240 [-100.000,  0.611], mean action: 1.600 [0.000, 3.000],  loss: 80955309540.072723, mse: 171999149753511.562500, mean_q: 10597526.581818, mean_eps: 0.649125
 117168/300000: episode: 1094, duration: 1.072s, episode steps: 154, steps per second: 144, episode reward: -290.007, mean reward: -1.883 [-100.000, 51.747], mean action: 1.753 [0.000, 3.000],  loss: 151848603475.116882, mse: 177468818897454.531250, mean_q: 10782510.555195, mean_eps: 0.648729
 117328/300000: episode: 1095, duration: 1.115s, episode steps: 160, steps per second: 143, episode reward: -136.679, mean reward: -0.854 [-100.000, 41.924], mean action: 1.706 [0.000, 3.000],  loss: 78109220249.600006, mse: 185599703751065.593750, mean_q: 10973820.390625, mean_eps: 0.648257
 117421/300000: episode: 1096, duration: 0.637s, episode steps:  93, steps per second: 146, episode reward: -360.427, mean reward: -3.876 [-100.000,  1.208], mean action: 1.613 [0.000, 3.000],  loss: 70467201156.129028, mse: 195889272274767.812500, mean_q: 11454018.602151, mean_eps: 0.647878
 117526/300000: episode: 1097, duration: 0.720s, episode steps: 105, steps per second: 146, episode reward: -355.850, mean reward: -3.389 [-100.000,  1.365], mean action: 1.695 [0.000, 3.000],  loss: 55407920118.247620, mse: 201251044835484.031250, mean_q: 11392161.423810, mean_eps: 0.647581
 117606/300000: episode: 1098, duration: 0.568s, episode steps:  80, steps per second: 141, episode reward: -365.411, mean reward: -4.568 [-100.000,  0.382], mean action: 1.575 [0.000, 3.000],  loss: 96179796684.800003, mse: 209134133077606.406250, mean_q: 11686621.125000, mean_eps: 0.647303
 117703/300000: episode: 1099, duration: 0.670s, episode steps:  97, steps per second: 145, episode reward: -351.773, mean reward: -3.627 [-100.000,  0.762], mean action: 1.557 [0.000, 3.000],  loss: 74306440477.030930, mse: 208260925232413.031250, mean_q: 11673276.463918, mean_eps: 0.647038
 117959/300000: episode: 1100, duration: 1.887s, episode steps: 256, steps per second: 136, episode reward: -391.697, mean reward: -1.530 [-100.000,  2.800], mean action: 1.672 [0.000, 3.000],  loss: 177334416064.000000, mse: 218010162200576.000000, mean_q: 11968186.914062, mean_eps: 0.646508
 118068/300000: episode: 1101, duration: 0.899s, episode steps: 109, steps per second: 121, episode reward: -322.678, mean reward: -2.960 [-100.000,  1.467], mean action: 1.624 [0.000, 3.000],  loss: 81506107016.220184, mse: 245128615719099.875000, mean_q: 12804867.761468, mean_eps: 0.645961
 118173/300000: episode: 1102, duration: 0.802s, episode steps: 105, steps per second: 131, episode reward: -292.279, mean reward: -2.784 [-100.000,  0.809], mean action: 1.600 [0.000, 3.000],  loss: 87798922834.895233, mse: 246939143724821.937500, mean_q: 12574909.380952, mean_eps: 0.645640
 118429/300000: episode: 1103, duration: 1.853s, episode steps: 256, steps per second: 138, episode reward: -178.005, mean reward: -0.695 [-100.000,  2.962], mean action: 1.684 [0.000, 3.000],  loss: 148714456168.000000, mse: 264533349433344.000000, mean_q: 13252269.816406, mean_eps: 0.645099
 118611/300000: episode: 1104, duration: 1.359s, episode steps: 182, steps per second: 134, episode reward: -238.914, mean reward: -1.313 [-100.000, 32.247], mean action: 1.511 [0.000, 3.000],  loss: 126358836505.318680, mse: 279406900916539.062500, mean_q: 13638285.203297, mean_eps: 0.644442
 118725/300000: episode: 1105, duration: 0.776s, episode steps: 114, steps per second: 147, episode reward: -242.260, mean reward: -2.125 [-100.000,  3.102], mean action: 1.675 [0.000, 3.000],  loss: 120969722790.175446, mse: 301163575085882.375000, mean_q: 14233261.254386, mean_eps: 0.643998
 118842/300000: episode: 1106, duration: 0.822s, episode steps: 117, steps per second: 142, episode reward: -188.455, mean reward: -1.611 [-100.000, 10.268], mean action: 1.632 [0.000, 3.000],  loss: 126520768809.572647, mse: 315937127076785.250000, mean_q: 14530773.863248, mean_eps: 0.643651
 118942/300000: episode: 1107, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: -104.349, mean reward: -1.043 [-100.000, 18.452], mean action: 1.790 [0.000, 3.000],  loss: 134650399293.440002, mse: 314138365567959.062500, mean_q: 14516489.190000, mean_eps: 0.643325
 119142/300000: episode: 1108, duration: 1.422s, episode steps: 200, steps per second: 141, episode reward: -229.281, mean reward: -1.146 [-100.000,  2.673], mean action: 1.720 [0.000, 3.000],  loss: 236345999523.839996, mse: 330318236342026.250000, mean_q: 14740949.100000, mean_eps: 0.642875
 119230/300000: episode: 1109, duration: 0.605s, episode steps:  88, steps per second: 146, episode reward: -109.899, mean reward: -1.249 [-100.000, 19.484], mean action: 1.659 [0.000, 3.000],  loss: 215623140817.454559, mse: 346862829699072.000000, mean_q: 15172686.181818, mean_eps: 0.642443
 119323/300000: episode: 1110, duration: 0.643s, episode steps:  93, steps per second: 145, episode reward: -430.351, mean reward: -4.627 [-100.000,  0.783], mean action: 1.645 [0.000, 3.000],  loss: 173672565947.182800, mse: 362556184192121.125000, mean_q: 15532227.344086, mean_eps: 0.642172
 119407/300000: episode: 1111, duration: 0.585s, episode steps:  84, steps per second: 144, episode reward: -287.345, mean reward: -3.421 [-100.000,  1.629], mean action: 1.595 [0.000, 3.000],  loss: 108614497816.380951, mse: 361722190408362.687500, mean_q: 15440425.750000, mean_eps: 0.641906
 119511/300000: episode: 1112, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: -260.938, mean reward: -2.509 [-100.000,  1.362], mean action: 1.731 [0.000, 3.000],  loss: 140944398375.384613, mse: 387489094575970.437500, mean_q: 16010137.355769, mean_eps: 0.641625
 119599/300000: episode: 1113, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -226.141, mean reward: -2.570 [-100.000,  1.515], mean action: 1.830 [0.000, 3.000],  loss: 375461202944.000000, mse: 407591934635659.625000, mean_q: 16698052.102273, mean_eps: 0.641336
 119713/300000: episode: 1114, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -302.048, mean reward: -2.650 [-100.000,  2.066], mean action: 1.693 [0.000, 3.000],  loss: 197684577944.701752, mse: 405324752261048.125000, mean_q: 16438498.973684, mean_eps: 0.641033
 119816/300000: episode: 1115, duration: 0.719s, episode steps: 103, steps per second: 143, episode reward: -36.509, mean reward: -0.354 [-100.000, 14.164], mean action: 1.806 [0.000, 3.000],  loss: 173433570632.077667, mse: 434618923592574.750000, mean_q: 17195636.766990, mean_eps: 0.640708
 119945/300000: episode: 1116, duration: 0.911s, episode steps: 129, steps per second: 142, episode reward: -466.733, mean reward: -3.618 [-100.000,  1.686], mean action: 1.674 [0.000, 3.000],  loss: 708177316649.674438, mse: 454674747644745.437500, mean_q: 17541520.201550, mean_eps: 0.640360
 120055/300000: episode: 1117, duration: 0.792s, episode steps: 110, steps per second: 139, episode reward: -298.318, mean reward: -2.712 [-100.000,  2.228], mean action: 1.709 [0.000, 3.000],  loss: 467146579409.454529, mse: 452244715487529.875000, mean_q: 17323398.036364, mean_eps: 0.640002
 120176/300000: episode: 1118, duration: 0.826s, episode steps: 121, steps per second: 147, episode reward: -337.769, mean reward: -2.791 [-100.000,  0.983], mean action: 1.587 [0.000, 3.000],  loss: 280014935979.371887, mse: 483252481487152.687500, mean_q: 18038349.355372, mean_eps: 0.639655
 120334/300000: episode: 1119, duration: 1.094s, episode steps: 158, steps per second: 144, episode reward: -121.432, mean reward: -0.769 [-100.000, 18.543], mean action: 1.608 [0.000, 3.000],  loss: 401338083276.151917, mse: 506178136709612.562500, mean_q: 18369279.170886, mean_eps: 0.639236
 120432/300000: episode: 1120, duration: 0.695s, episode steps:  98, steps per second: 141, episode reward: -337.806, mean reward: -3.447 [-100.000,  0.534], mean action: 1.500 [0.000, 3.000],  loss: 280879373834.448975, mse: 532429799402600.500000, mean_q: 19061723.163265, mean_eps: 0.638852
 120574/300000: episode: 1121, duration: 0.969s, episode steps: 142, steps per second: 146, episode reward: -149.872, mean reward: -1.055 [-100.000, 13.115], mean action: 1.711 [0.000, 3.000],  loss: 494459740996.507019, mse: 541866605951408.687500, mean_q: 19060661.852113, mean_eps: 0.638492
 120724/300000: episode: 1122, duration: 1.083s, episode steps: 150, steps per second: 139, episode reward: -262.210, mean reward: -1.748 [-100.000,  3.258], mean action: 1.740 [0.000, 3.000],  loss: 350088172776.106689, mse: 558151200106960.187500, mean_q: 19241023.153333, mean_eps: 0.638054
 120863/300000: episode: 1123, duration: 0.940s, episode steps: 139, steps per second: 148, episode reward: -232.974, mean reward: -1.676 [-100.000,  1.390], mean action: 1.763 [0.000, 3.000],  loss: 497700673757.007202, mse: 592325584293092.375000, mean_q: 20102891.589928, mean_eps: 0.637621
 120948/300000: episode: 1124, duration: 0.585s, episode steps:  85, steps per second: 145, episode reward: -266.878, mean reward: -3.140 [-100.000,  1.304], mean action: 1.765 [0.000, 3.000],  loss: 1364099407920.188232, mse: 604439205651492.125000, mean_q: 20108768.788235, mean_eps: 0.637285
 121097/300000: episode: 1125, duration: 1.045s, episode steps: 149, steps per second: 143, episode reward: -226.388, mean reward: -1.519 [-100.000,  2.174], mean action: 1.705 [0.000, 3.000],  loss: 1351960970645.476562, mse: 632061148875233.125000, mean_q: 20778379.765101, mean_eps: 0.636934
 121185/300000: episode: 1126, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -390.055, mean reward: -4.432 [-100.000, -0.051], mean action: 1.625 [0.000, 3.000],  loss: 1296703825733.818115, mse: 664601366435653.875000, mean_q: 21152257.465909, mean_eps: 0.636578
 121352/300000: episode: 1127, duration: 1.170s, episode steps: 167, steps per second: 143, episode reward: -469.699, mean reward: -2.813 [-100.000,  2.073], mean action: 1.605 [0.000, 3.000],  loss: 886274562575.329346, mse: 662564449189299.375000, mean_q: 21126369.335329, mean_eps: 0.636196
 121435/300000: episode: 1128, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -377.390, mean reward: -4.547 [-100.000, -0.234], mean action: 1.699 [0.000, 3.000],  loss: 649273286063.807251, mse: 676941715027055.000000, mean_q: 21568520.433735, mean_eps: 0.635821
 121549/300000: episode: 1129, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -249.284, mean reward: -2.187 [-100.000,  3.874], mean action: 1.675 [0.000, 3.000],  loss: 469260016154.947388, mse: 733022466646447.125000, mean_q: 22594923.017544, mean_eps: 0.635525
 121718/300000: episode: 1130, duration: 1.183s, episode steps: 169, steps per second: 143, episode reward: -475.458, mean reward: -2.813 [-100.000,  2.545], mean action: 1.669 [0.000, 3.000],  loss: 1560558801744.283936, mse: 731842510002624.375000, mean_q: 22362735.130178, mean_eps: 0.635101
 121933/300000: episode: 1131, duration: 1.517s, episode steps: 215, steps per second: 142, episode reward: -367.218, mean reward: -1.708 [-100.000,  2.743], mean action: 1.693 [0.000, 3.000],  loss: 867855209305.302368, mse: 786455165199517.125000, mean_q: 23246338.837209, mean_eps: 0.634525
 122045/300000: episode: 1132, duration: 0.768s, episode steps: 112, steps per second: 146, episode reward: -231.870, mean reward: -2.070 [-100.000,  3.134], mean action: 1.830 [0.000, 3.000],  loss: 521287925174.857117, mse: 800285688145042.250000, mean_q: 23666081.196429, mean_eps: 0.634035
 122240/300000: episode: 1133, duration: 1.354s, episode steps: 195, steps per second: 144, episode reward: -157.995, mean reward: -0.810 [-100.000, 105.437], mean action: 1.656 [0.000, 3.000],  loss: 740410765038.933350, mse: 841461242840972.500000, mean_q: 24237146.697436, mean_eps: 0.633574
 122371/300000: episode: 1134, duration: 0.909s, episode steps: 131, steps per second: 144, episode reward: -137.350, mean reward: -1.048 [-100.000,  2.107], mean action: 1.824 [0.000, 3.000],  loss: 733111017034.259521, mse: 876720574019005.500000, mean_q: 24731880.656489, mean_eps: 0.633085
 122486/300000: episode: 1135, duration: 0.774s, episode steps: 115, steps per second: 149, episode reward: -366.086, mean reward: -3.183 [-100.000,  1.717], mean action: 1.557 [0.000, 3.000],  loss: 908792968058.434814, mse: 901162054426125.375000, mean_q: 24958690.095652, mean_eps: 0.632716
 122586/300000: episode: 1136, duration: 0.731s, episode steps: 100, steps per second: 137, episode reward: -261.436, mean reward: -2.614 [-100.000,  0.818], mean action: 1.810 [0.000, 3.000],  loss: 549150173839.359985, mse: 970073239390781.500000, mean_q: 26361843.660000, mean_eps: 0.632393
 122672/300000: episode: 1137, duration: 0.697s, episode steps:  86, steps per second: 123, episode reward: -393.599, mean reward: -4.577 [-100.000,  0.262], mean action: 1.837 [0.000, 3.000],  loss: 991278729263.627930, mse: 975722287939965.000000, mean_q: 26130346.744186, mean_eps: 0.632115
 122788/300000: episode: 1138, duration: 0.925s, episode steps: 116, steps per second: 125, episode reward: -281.225, mean reward: -2.424 [-100.000,  4.063], mean action: 1.672 [0.000, 3.000],  loss: 613200706242.206909, mse: 1015274378082657.125000, mean_q: 26572942.431034, mean_eps: 0.631812
 122900/300000: episode: 1139, duration: 0.908s, episode steps: 112, steps per second: 123, episode reward: -269.127, mean reward: -2.403 [-100.000,  1.054], mean action: 1.625 [0.000, 3.000],  loss: 1280335145691.428467, mse: 1017942483702345.125000, mean_q: 26509069.500000, mean_eps: 0.631469
 123047/300000: episode: 1140, duration: 1.102s, episode steps: 147, steps per second: 133, episode reward: -282.892, mean reward: -1.924 [-100.000, 97.766], mean action: 1.673 [0.000, 3.000],  loss: 2089256772364.190430, mse: 1045818167168717.500000, mean_q: 26938818.340136, mean_eps: 0.631081
 123148/300000: episode: 1141, duration: 0.795s, episode steps: 101, steps per second: 127, episode reward: -317.314, mean reward: -3.142 [-100.000,  0.686], mean action: 1.535 [0.000, 3.000],  loss: 476148380986.297058, mse: 1092047841715230.375000, mean_q: 27678448.316832, mean_eps: 0.630709
 123288/300000: episode: 1142, duration: 1.030s, episode steps: 140, steps per second: 136, episode reward: -219.563, mean reward: -1.568 [-100.000,  2.405], mean action: 1.671 [0.000, 3.000],  loss: 1705631853538.742920, mse: 1122722297818082.750000, mean_q: 28044410.871429, mean_eps: 0.630347
 123536/300000: episode: 1143, duration: 1.741s, episode steps: 248, steps per second: 142, episode reward: -521.696, mean reward: -2.104 [-100.000,  4.166], mean action: 1.698 [0.000, 3.000],  loss: 1620255585577.290283, mse: 1203833335083933.000000, mean_q: 29360008.040323, mean_eps: 0.629765
 123635/300000: episode: 1144, duration: 0.664s, episode steps:  99, steps per second: 149, episode reward: -350.061, mean reward: -3.536 [-100.000,  0.190], mean action: 1.646 [0.000, 3.000],  loss: 1071297789290.020142, mse: 1322814928878623.000000, mean_q: 31015297.535354, mean_eps: 0.629245
 123730/300000: episode: 1145, duration: 0.737s, episode steps:  95, steps per second: 129, episode reward: -48.453, mean reward: -0.510 [-100.000,  9.614], mean action: 1.842 [0.000, 3.000],  loss: 734693727997.305298, mse: 1316834233499130.500000, mean_q: 30683792.210526, mean_eps: 0.628954
 123872/300000: episode: 1146, duration: 0.993s, episode steps: 142, steps per second: 143, episode reward: -359.946, mean reward: -2.535 [-100.000,  4.984], mean action: 1.718 [0.000, 3.000],  loss: 2341763590865.126953, mse: 1332489097506296.750000, mean_q: 30560515.887324, mean_eps: 0.628598
 123969/300000: episode: 1147, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -84.131, mean reward: -0.867 [-100.000,  9.463], mean action: 1.660 [0.000, 3.000],  loss: 775085550306.969116, mse: 1308724960017249.750000, mean_q: 30403800.206186, mean_eps: 0.628240
 124072/300000: episode: 1148, duration: 0.711s, episode steps: 103, steps per second: 145, episode reward: -445.721, mean reward: -4.327 [-100.000,  0.652], mean action: 1.806 [0.000, 3.000],  loss: 860058066228.194214, mse: 1382486886992528.250000, mean_q: 31257351.165049, mean_eps: 0.627940
 124243/300000: episode: 1149, duration: 1.298s, episode steps: 171, steps per second: 132, episode reward: -322.165, mean reward: -1.884 [-100.000,  4.631], mean action: 1.497 [0.000, 3.000],  loss: 2196921235875.181396, mse: 1453067060707711.250000, mean_q: 32151936.467836, mean_eps: 0.627529
 124345/300000: episode: 1150, duration: 0.797s, episode steps: 102, steps per second: 128, episode reward:  2.079, mean reward:  0.020 [-100.000, 12.629], mean action: 1.686 [0.000, 3.000],  loss: 3501629192633.725586, mse: 1528233548039107.750000, mean_q: 33050517.333333, mean_eps: 0.627119
 124443/300000: episode: 1151, duration: 0.711s, episode steps:  98, steps per second: 138, episode reward: -379.547, mean reward: -3.873 [-100.000,  1.255], mean action: 1.622 [0.000, 3.000],  loss: 894408139253.551025, mse: 1470004705436818.250000, mean_q: 32240669.285714, mean_eps: 0.626820
 124597/300000: episode: 1152, duration: 1.151s, episode steps: 154, steps per second: 134, episode reward: -70.047, mean reward: -0.455 [-100.000, 22.208], mean action: 1.708 [0.000, 3.000],  loss: 2135727159535.376709, mse: 1564581057520999.000000, mean_q: 33358918.415584, mean_eps: 0.626441
 124703/300000: episode: 1153, duration: 0.728s, episode steps: 106, steps per second: 146, episode reward: -58.194, mean reward: -0.549 [-100.000, 14.614], mean action: 1.811 [0.000, 3.000],  loss: 2393076748365.283203, mse: 1592586943124615.250000, mean_q: 33693104.886792, mean_eps: 0.626051
 124785/300000: episode: 1154, duration: 0.552s, episode steps:  82, steps per second: 149, episode reward: -342.385, mean reward: -4.175 [-100.000, -0.120], mean action: 1.598 [0.000, 3.000],  loss: 737060100945.170776, mse: 1637937970202574.000000, mean_q: 34082883.902439, mean_eps: 0.625769
 124868/300000: episode: 1155, duration: 0.584s, episode steps:  83, steps per second: 142, episode reward: -279.481, mean reward: -3.367 [-100.000,  0.545], mean action: 1.639 [0.000, 3.000],  loss: 570411923332.626465, mse: 1694499121628320.500000, mean_q: 34827886.144578, mean_eps: 0.625522
 124976/300000: episode: 1156, duration: 0.738s, episode steps: 108, steps per second: 146, episode reward: -196.510, mean reward: -1.820 [-100.000,  2.394], mean action: 1.528 [0.000, 3.000],  loss: 652199777773.036987, mse: 1666562596419811.500000, mean_q: 34155773.888889, mean_eps: 0.625235
 125122/300000: episode: 1157, duration: 0.989s, episode steps: 146, steps per second: 148, episode reward: -13.014, mean reward: -0.089 [-100.000, 16.820], mean action: 1.705 [0.000, 3.000],  loss: 2094851051127.232910, mse: 1719120444109529.500000, mean_q: 34390227.027397, mean_eps: 0.624854
 125198/300000: episode: 1158, duration: 0.548s, episode steps:  76, steps per second: 139, episode reward: -144.695, mean reward: -1.904 [-100.000, 12.171], mean action: 1.671 [0.000, 3.000],  loss: 1327032332072.421143, mse: 1790132887196726.000000, mean_q: 35575143.789474, mean_eps: 0.624521
 125303/300000: episode: 1159, duration: 0.728s, episode steps: 105, steps per second: 144, episode reward: -441.919, mean reward: -4.209 [-100.000,  4.129], mean action: 1.524 [0.000, 3.000],  loss: 2301929598390.856934, mse: 1867129264048264.500000, mean_q: 36623091.276190, mean_eps: 0.624250
 125416/300000: episode: 1160, duration: 0.761s, episode steps: 113, steps per second: 148, episode reward: -351.098, mean reward: -3.107 [-100.000,  0.700], mean action: 1.566 [0.000, 3.000],  loss: 2712299957873.274414, mse: 1911510560862244.250000, mean_q: 36774224.920354, mean_eps: 0.623923
 125571/300000: episode: 1161, duration: 1.065s, episode steps: 155, steps per second: 146, episode reward: -629.254, mean reward: -4.060 [-100.000,  1.283], mean action: 1.697 [0.000, 3.000],  loss: 3310801062376.877441, mse: 2022330684008672.500000, mean_q: 38060458.361290, mean_eps: 0.623521
 125653/300000: episode: 1162, duration: 0.548s, episode steps:  82, steps per second: 150, episode reward: -515.904, mean reward: -6.292 [-100.000, -0.692], mean action: 1.805 [0.000, 3.000],  loss: 599615100778.146362, mse: 2058705577334634.250000, mean_q: 38539013.121951, mean_eps: 0.623166
 125747/300000: episode: 1163, duration: 0.636s, episode steps:  94, steps per second: 148, episode reward: -411.960, mean reward: -4.383 [-100.000,  0.742], mean action: 1.670 [0.000, 3.000],  loss: 666518056175.659546, mse: 2044419176856205.500000, mean_q: 38235491.468085, mean_eps: 0.622901
 125840/300000: episode: 1164, duration: 0.644s, episode steps:  93, steps per second: 144, episode reward: -159.334, mean reward: -1.713 [-100.000, 18.970], mean action: 1.312 [0.000, 3.000],  loss: 1375239000548.473145, mse: 2032304941401825.750000, mean_q: 38069711.333333, mean_eps: 0.622621
 125928/300000: episode: 1165, duration: 0.598s, episode steps:  88, steps per second: 147, episode reward: -526.700, mean reward: -5.985 [-100.000,  0.746], mean action: 1.705 [0.000, 3.000],  loss: 3130105211438.545410, mse: 2156153359779281.500000, mean_q: 39314131.818182, mean_eps: 0.622349
 126118/300000: episode: 1166, duration: 1.310s, episode steps: 190, steps per second: 145, episode reward: -205.843, mean reward: -1.083 [-100.000,  8.974], mean action: 1.663 [0.000, 3.000],  loss: 1876649603977.431641, mse: 2214767991789632.750000, mean_q: 40057143.957895, mean_eps: 0.621933
 126281/300000: episode: 1167, duration: 1.106s, episode steps: 163, steps per second: 147, episode reward: -484.243, mean reward: -2.971 [-100.000,  2.801], mean action: 1.724 [0.000, 3.000],  loss: 1207925774222.920166, mse: 2273250010919389.500000, mean_q: 40374885.006135, mean_eps: 0.621403
 126354/300000: episode: 1168, duration: 0.491s, episode steps:  73, steps per second: 149, episode reward: -69.385, mean reward: -0.950 [-100.000, 13.219], mean action: 1.438 [0.000, 3.000],  loss: 1401161903735.232910, mse: 2369449245691174.500000, mean_q: 41135217.972603, mean_eps: 0.621049
 126508/300000: episode: 1169, duration: 1.093s, episode steps: 154, steps per second: 141, episode reward: -471.300, mean reward: -3.060 [-100.000,  2.558], mean action: 1.662 [0.000, 3.000],  loss: 1826282576324.155762, mse: 2410407638638485.500000, mean_q: 41698589.000000, mean_eps: 0.620708
 126661/300000: episode: 1170, duration: 1.045s, episode steps: 153, steps per second: 146, episode reward: -329.177, mean reward: -2.151 [-100.000,  2.527], mean action: 1.686 [0.000, 3.000],  loss: 4582552467790.640625, mse: 2570045828321788.500000, mean_q: 43222206.758170, mean_eps: 0.620248
 126811/300000: episode: 1171, duration: 1.069s, episode steps: 150, steps per second: 140, episode reward: -68.868, mean reward: -0.459 [-100.000, 11.012], mean action: 1.680 [0.000, 3.000],  loss: 2287095241113.600098, mse: 2601518680929293.500000, mean_q: 43361413.146667, mean_eps: 0.619794
 126904/300000: episode: 1172, duration: 0.649s, episode steps:  93, steps per second: 143, episode reward: -482.629, mean reward: -5.190 [-100.000,  0.329], mean action: 1.699 [0.000, 3.000],  loss: 896406930993.548340, mse: 2763572045186081.000000, mean_q: 44947150.150538, mean_eps: 0.619429
 126997/300000: episode: 1173, duration: 0.623s, episode steps:  93, steps per second: 149, episode reward: -406.951, mean reward: -4.376 [-100.000,  0.432], mean action: 1.667 [0.000, 3.000],  loss: 4961734625346.064453, mse: 2745836456880106.000000, mean_q: 44697659.913978, mean_eps: 0.619150
 127123/300000: episode: 1174, duration: 0.898s, episode steps: 126, steps per second: 140, episode reward: -20.185, mean reward: -0.160 [-100.000,  9.637], mean action: 1.643 [0.000, 3.000],  loss: 3473070212632.380859, mse: 2861154248059302.500000, mean_q: 45491759.111111, mean_eps: 0.618822
 127210/300000: episode: 1175, duration: 0.591s, episode steps:  87, steps per second: 147, episode reward: -77.143, mean reward: -0.887 [-100.000, 12.275], mean action: 1.851 [0.000, 3.000],  loss: 3320292477893.149414, mse: 2959156823683778.000000, mean_q: 46599851.080460, mean_eps: 0.618502
 127297/300000: episode: 1176, duration: 0.583s, episode steps:  87, steps per second: 149, episode reward: -461.599, mean reward: -5.306 [-100.000,  0.839], mean action: 1.747 [0.000, 3.000],  loss: 1005370659804.689697, mse: 2960166702552970.500000, mean_q: 46165525.517241, mean_eps: 0.618241
 127384/300000: episode: 1177, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -336.411, mean reward: -3.867 [-100.000,  0.101], mean action: 1.621 [0.000, 3.000],  loss: 3951938397572.413574, mse: 3007747530448425.000000, mean_q: 47260655.126437, mean_eps: 0.617980
 127488/300000: episode: 1178, duration: 0.721s, episode steps: 104, steps per second: 144, episode reward: -321.284, mean reward: -3.089 [-100.000,  0.805], mean action: 1.683 [0.000, 3.000],  loss: 3576019970520.615234, mse: 3070269251105083.000000, mean_q: 47637941.423077, mean_eps: 0.617693
 127744/300000: episode: 1179, duration: 1.764s, episode steps: 256, steps per second: 145, episode reward: -435.454, mean reward: -1.701 [-100.000,  4.682], mean action: 1.691 [0.000, 3.000],  loss: 3465226977280.000000, mse: 3205811673235456.000000, mean_q: 48255344.234375, mean_eps: 0.617154
 127850/300000: episode: 1180, duration: 0.718s, episode steps: 106, steps per second: 148, episode reward: -391.883, mean reward: -3.697 [-100.000,  1.102], mean action: 1.509 [0.000, 3.000],  loss: 7366866171478.943359, mse: 3378150069121101.500000, mean_q: 49729749.207547, mean_eps: 0.616611
 128005/300000: episode: 1181, duration: 1.099s, episode steps: 155, steps per second: 141, episode reward: -125.486, mean reward: -0.810 [-100.000, 12.095], mean action: 1.548 [0.000, 3.000],  loss: 3768634633685.058105, mse: 3407415187797950.000000, mean_q: 49748845.935484, mean_eps: 0.616219
 128111/300000: episode: 1182, duration: 0.734s, episode steps: 106, steps per second: 144, episode reward: -285.583, mean reward: -2.694 [-100.000,  1.166], mean action: 1.670 [0.000, 3.000],  loss: 2440503008816.301758, mse: 3454753297671844.000000, mean_q: 50099550.867925, mean_eps: 0.615827
 128228/300000: episode: 1183, duration: 0.787s, episode steps: 117, steps per second: 149, episode reward: -421.619, mean reward: -3.604 [-100.000,  1.187], mean action: 1.573 [0.000, 3.000],  loss: 4413755583566.769531, mse: 3647910893129553.000000, mean_q: 52129725.538462, mean_eps: 0.615493
 128326/300000: episode: 1184, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -437.513, mean reward: -4.464 [-100.000,  0.223], mean action: 1.622 [0.000, 3.000],  loss: 1845381616201.142822, mse: 3727902303734052.500000, mean_q: 52559170.000000, mean_eps: 0.615171
 128411/300000: episode: 1185, duration: 0.594s, episode steps:  85, steps per second: 143, episode reward: -410.894, mean reward: -4.834 [-100.000,  0.659], mean action: 1.635 [0.000, 3.000],  loss: 5327796236914.447266, mse: 3822273152778625.500000, mean_q: 53046118.400000, mean_eps: 0.614896
 128555/300000: episode: 1186, duration: 0.977s, episode steps: 144, steps per second: 147, episode reward: -171.203, mean reward: -1.189 [-100.000, 12.851], mean action: 1.861 [0.000, 3.000],  loss: 6194448819541.333008, mse: 3980931950386290.000000, mean_q: 54664293.944444, mean_eps: 0.614552
 128987/300000: episode: 1187, duration: 3.146s, episode steps: 432, steps per second: 137, episode reward: -51.148, mean reward: -0.118 [-100.000, 21.957], mean action: 1.630 [0.000, 3.000],  loss: 4233794713979.259277, mse: 4154412700425936.500000, mean_q: 55522012.750000, mean_eps: 0.613688
 129142/300000: episode: 1188, duration: 1.078s, episode steps: 155, steps per second: 144, episode reward: -55.877, mean reward: -0.360 [-100.000,  9.357], mean action: 1.652 [0.000, 3.000],  loss: 7552583019249.135742, mse: 4342398852193617.000000, mean_q: 56761647.354839, mean_eps: 0.612808
 129256/300000: episode: 1189, duration: 0.825s, episode steps: 114, steps per second: 138, episode reward: -63.670, mean reward: -0.559 [-100.000, 19.268], mean action: 1.614 [0.000, 3.000],  loss: 8611699464371.649414, mse: 4543817004139933.000000, mean_q: 58901224.982456, mean_eps: 0.612405
 129431/300000: episode: 1190, duration: 1.200s, episode steps: 175, steps per second: 146, episode reward: -97.461, mean reward: -0.557 [-100.000,  9.122], mean action: 1.634 [0.000, 3.000],  loss: 3101319930090.057129, mse: 4553042796960809.000000, mean_q: 58059109.851429, mean_eps: 0.611971
 129517/300000: episode: 1191, duration: 0.629s, episode steps:  86, steps per second: 137, episode reward: -271.110, mean reward: -3.152 [-100.000,  4.728], mean action: 1.616 [0.000, 3.000],  loss: 3373729758088.930176, mse: 4661296652463271.000000, mean_q: 58245349.534884, mean_eps: 0.611579
 129635/300000: episode: 1192, duration: 0.839s, episode steps: 118, steps per second: 141, episode reward: -212.924, mean reward: -1.804 [-100.000,  2.408], mean action: 1.695 [0.000, 3.000],  loss: 4097025065793.084961, mse: 4814308822203028.000000, mean_q: 59561318.237288, mean_eps: 0.611274
 129727/300000: episode: 1193, duration: 0.649s, episode steps:  92, steps per second: 142, episode reward: -357.893, mean reward: -3.890 [-100.000,  0.572], mean action: 1.826 [0.000, 3.000],  loss: 4036422977625.043457, mse: 4940156191956636.000000, mean_q: 60563550.130435, mean_eps: 0.610958
 129815/300000: episode: 1194, duration: 0.602s, episode steps:  88, steps per second: 146, episode reward: -101.362, mean reward: -1.152 [-100.000,  9.775], mean action: 1.625 [0.000, 3.000],  loss: 3703770797893.818359, mse: 5129379928318697.000000, mean_q: 61905001.681818, mean_eps: 0.610688
 129900/300000: episode: 1195, duration: 0.624s, episode steps:  85, steps per second: 136, episode reward: -85.138, mean reward: -1.002 [-100.000, 15.559], mean action: 1.765 [0.000, 3.000],  loss: 5364432710186.165039, mse: 5214197311507083.000000, mean_q: 62768750.117647, mean_eps: 0.610429
 129991/300000: episode: 1196, duration: 0.628s, episode steps:  91, steps per second: 145, episode reward: -385.986, mean reward: -4.242 [-100.000,  0.748], mean action: 1.659 [0.000, 3.000],  loss: 3087328762373.626465, mse: 5474638795164301.000000, mean_q: 64646702.109890, mean_eps: 0.610165
 130075/300000: episode: 1197, duration: 0.576s, episode steps:  84, steps per second: 146, episode reward: -278.155, mean reward: -3.311 [-100.000,  0.341], mean action: 1.452 [0.000, 3.000],  loss: 6645657431478.857422, mse: 5429034076764648.000000, mean_q: 63853585.190476, mean_eps: 0.609902
 130182/300000: episode: 1198, duration: 0.761s, episode steps: 107, steps per second: 141, episode reward: -372.099, mean reward: -3.478 [-100.000,  1.174], mean action: 1.598 [0.000, 3.000],  loss: 8735384859045.083984, mse: 5517597788957610.000000, mean_q: 63890612.261682, mean_eps: 0.609616
 130298/300000: episode: 1199, duration: 0.803s, episode steps: 116, steps per second: 144, episode reward: -570.395, mean reward: -4.917 [-100.000,  1.955], mean action: 1.759 [0.000, 3.000],  loss: 8503912256476.689453, mse: 5735310348517376.000000, mean_q: 65073637.758621, mean_eps: 0.609282
 130384/300000: episode: 1200, duration: 0.596s, episode steps:  86, steps per second: 144, episode reward: -345.223, mean reward: -4.014 [-100.000,  0.726], mean action: 1.605 [0.000, 3.000],  loss: 3063505242731.162598, mse: 5859011574414979.000000, mean_q: 66563896.976744, mean_eps: 0.608978
 130528/300000: episode: 1201, duration: 1.000s, episode steps: 144, steps per second: 144, episode reward: -389.375, mean reward: -2.704 [-100.000,  1.991], mean action: 1.625 [0.000, 3.000],  loss: 6225321445603.555664, mse: 5932173112406471.000000, mean_q: 66644142.805556, mean_eps: 0.608633
 130660/300000: episode: 1202, duration: 0.890s, episode steps: 132, steps per second: 148, episode reward: -436.537, mean reward: -3.307 [-100.000,  2.280], mean action: 1.720 [0.000, 3.000],  loss: 10146540866963.394531, mse: 6200260611658100.000000, mean_q: 68282073.575758, mean_eps: 0.608220
 130752/300000: episode: 1203, duration: 0.649s, episode steps:  92, steps per second: 142, episode reward: -189.003, mean reward: -2.054 [-100.000,  7.572], mean action: 1.522 [0.000, 3.000],  loss: 17891503881527.652344, mse: 6031028344210120.000000, mean_q: 66314747.304348, mean_eps: 0.607884
 130874/300000: episode: 1204, duration: 0.956s, episode steps: 122, steps per second: 128, episode reward: -347.155, mean reward: -2.846 [-100.000,  1.627], mean action: 1.721 [0.000, 3.000],  loss: 15521030133793.574219, mse: 6529924426238053.000000, mean_q: 69889726.360656, mean_eps: 0.607563
 131055/300000: episode: 1205, duration: 1.555s, episode steps: 181, steps per second: 116, episode reward: -88.041, mean reward: -0.486 [-100.000,  8.601], mean action: 1.613 [0.000, 3.000],  loss: 6282459863141.833984, mse: 6500620330401430.000000, mean_q: 69450542.386740, mean_eps: 0.607108
 131289/300000: episode: 1206, duration: 1.602s, episode steps: 234, steps per second: 146, episode reward: -449.954, mean reward: -1.923 [-100.000,  5.134], mean action: 1.654 [0.000, 3.000],  loss: 9892025475282.050781, mse: 6983444282391596.000000, mean_q: 72373349.760684, mean_eps: 0.606485
 131451/300000: episode: 1207, duration: 1.142s, episode steps: 162, steps per second: 142, episode reward: -368.721, mean reward: -2.276 [-100.000,  2.101], mean action: 1.685 [0.000, 3.000],  loss: 15420871482886.320312, mse: 7360534434011667.000000, mean_q: 74302506.592593, mean_eps: 0.605891
 131563/300000: episode: 1208, duration: 0.754s, episode steps: 112, steps per second: 149, episode reward: -96.924, mean reward: -0.865 [-100.000,  9.092], mean action: 1.643 [0.000, 3.000],  loss: 8388662226944.000000, mse: 7477444165051538.000000, mean_q: 74981674.892857, mean_eps: 0.605480
 131720/300000: episode: 1209, duration: 1.121s, episode steps: 157, steps per second: 140, episode reward: -464.545, mean reward: -2.959 [-100.000, 58.477], mean action: 1.796 [0.000, 3.000],  loss: 13118504851469.044922, mse: 7629986774115817.000000, mean_q: 75369576.815287, mean_eps: 0.605077
 131805/300000: episode: 1210, duration: 0.571s, episode steps:  85, steps per second: 149, episode reward: -384.692, mean reward: -4.526 [-100.000,  0.419], mean action: 1.659 [0.000, 3.000],  loss: 4093006044894.870605, mse: 7910508889923680.000000, mean_q: 77527135.952941, mean_eps: 0.604714
 131953/300000: episode: 1211, duration: 1.241s, episode steps: 148, steps per second: 119, episode reward: -37.434, mean reward: -0.253 [-100.000, 12.756], mean action: 1.682 [0.000, 3.000],  loss: 5319515546707.027344, mse: 8048965348930532.000000, mean_q: 77863811.702703, mean_eps: 0.604364
 132200/300000: episode: 1212, duration: 2.101s, episode steps: 247, steps per second: 118, episode reward: -297.625, mean reward: -1.205 [-100.000,  2.445], mean action: 1.725 [0.000, 3.000],  loss: 8826359978738.525391, mse: 8252394855407649.000000, mean_q: 78502227.255061, mean_eps: 0.603772
 132301/300000: episode: 1213, duration: 0.825s, episode steps: 101, steps per second: 122, episode reward: -378.111, mean reward: -3.744 [-100.000,  0.885], mean action: 1.584 [0.000, 3.000],  loss: 9637982825401.029297, mse: 8882382434081113.000000, mean_q: 82158788.435644, mean_eps: 0.603250
 132479/300000: episode: 1214, duration: 1.398s, episode steps: 178, steps per second: 127, episode reward: -234.820, mean reward: -1.319 [-100.000,  3.496], mean action: 1.640 [0.000, 3.000],  loss: 7516540839268.673828, mse: 8939227162637070.000000, mean_q: 81770441.235955, mean_eps: 0.602832
 132580/300000: episode: 1215, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -411.331, mean reward: -4.073 [-100.000,  0.306], mean action: 1.663 [0.000, 3.000],  loss: 16818663810402.851562, mse: 9539873675390692.000000, mean_q: 85329160.435644, mean_eps: 0.602413
 132663/300000: episode: 1216, duration: 0.555s, episode steps:  83, steps per second: 149, episode reward: -393.824, mean reward: -4.745 [-100.000,  0.653], mean action: 1.639 [0.000, 3.000],  loss: 12849290407294.457031, mse: 9367157042075228.000000, mean_q: 83816298.987952, mean_eps: 0.602137
 132791/300000: episode: 1217, duration: 0.897s, episode steps: 128, steps per second: 143, episode reward: -303.263, mean reward: -2.369 [-100.000,  1.652], mean action: 1.750 [0.000, 3.000],  loss: 4326744731136.000000, mse: 9788592876421120.000000, mean_q: 86438303.687500, mean_eps: 0.601820
 132874/300000: episode: 1218, duration: 0.583s, episode steps:  83, steps per second: 142, episode reward: -373.151, mean reward: -4.496 [-100.000,  0.444], mean action: 1.687 [0.000, 3.000],  loss: 2812251486084.626465, mse: 9793309479866220.000000, mean_q: 85494195.036145, mean_eps: 0.601504
 132974/300000: episode: 1219, duration: 0.670s, episode steps: 100, steps per second: 149, episode reward: -110.754, mean reward: -1.108 [-100.000, 25.426], mean action: 1.670 [0.000, 3.000],  loss: 17362753485537.279297, mse: 10392068992847052.000000, mean_q: 89064396.400000, mean_eps: 0.601229
 133097/300000: episode: 1220, duration: 0.849s, episode steps: 123, steps per second: 145, episode reward: -55.414, mean reward: -0.451 [-100.000, 92.730], mean action: 1.602 [0.000, 3.000],  loss: 15976653063742.439453, mse: 9995631767047692.000000, mean_q: 86132583.349593, mean_eps: 0.600895
 133201/300000: episode: 1221, duration: 0.791s, episode steps: 104, steps per second: 132, episode reward: -99.512, mean reward: -0.957 [-100.000, 101.860], mean action: 1.529 [0.000, 3.000],  loss: 17329135399227.076172, mse: 10849931139118632.000000, mean_q: 91134728.384615, mean_eps: 0.600554
 133358/300000: episode: 1222, duration: 1.326s, episode steps: 157, steps per second: 118, episode reward: -396.127, mean reward: -2.523 [-100.000,  2.126], mean action: 1.726 [0.000, 3.000],  loss: 14082892869240.662109, mse: 10816568188435854.000000, mean_q: 90198771.617834, mean_eps: 0.600163
 133444/300000: episode: 1223, duration: 0.696s, episode steps:  86, steps per second: 124, episode reward: -420.150, mean reward: -4.885 [-100.000,  0.844], mean action: 1.686 [0.000, 3.000],  loss: 17481447364179.349609, mse: 10922549733893048.000000, mean_q: 90116528.976744, mean_eps: 0.599799
 133539/300000: episode: 1224, duration: 0.704s, episode steps:  95, steps per second: 135, episode reward: -172.645, mean reward: -1.817 [-100.000,  1.878], mean action: 1.853 [0.000, 3.000],  loss: 12429705810652.968750, mse: 11354310467842694.000000, mean_q: 93375895.747368, mean_eps: 0.599527
 133619/300000: episode: 1225, duration: 0.600s, episode steps:  80, steps per second: 133, episode reward: -165.257, mean reward: -2.066 [-100.000, 18.286], mean action: 1.600 [0.000, 3.000],  loss: 9105467419852.800781, mse: 11595724967483802.000000, mean_q: 94523788.200000, mean_eps: 0.599264
 133729/300000: episode: 1226, duration: 0.836s, episode steps: 110, steps per second: 132, episode reward: -197.704, mean reward: -1.797 [-100.000, 17.413], mean action: 1.545 [0.000, 3.000],  loss: 18715045925534.253906, mse: 11838867464181276.000000, mean_q: 94415767.381818, mean_eps: 0.598979
 133891/300000: episode: 1227, duration: 1.124s, episode steps: 162, steps per second: 144, episode reward: -486.004, mean reward: -3.000 [-100.000,  1.855], mean action: 1.728 [0.000, 3.000],  loss: 22875432560918.125000, mse: 12468422471876204.000000, mean_q: 97577555.753086, mean_eps: 0.598571
 134016/300000: episode: 1228, duration: 0.885s, episode steps: 125, steps per second: 141, episode reward: -488.127, mean reward: -3.905 [-100.000,  2.705], mean action: 1.704 [0.000, 3.000],  loss: 16325774720106.496094, mse: 12736652089527108.000000, mean_q: 99279606.176000, mean_eps: 0.598141
 134126/300000: episode: 1229, duration: 0.760s, episode steps: 110, steps per second: 145, episode reward: -444.616, mean reward: -4.042 [-100.000,  1.817], mean action: 1.855 [0.000, 3.000],  loss: 7929567160208.291016, mse: 13088227177195706.000000, mean_q: 100170582.218182, mean_eps: 0.597788
 134244/300000: episode: 1230, duration: 0.808s, episode steps: 118, steps per second: 146, episode reward: -247.512, mean reward: -2.098 [-100.000,  2.415], mean action: 1.551 [0.000, 3.000],  loss: 25373822022395.660156, mse: 12849884823996312.000000, mean_q: 99028308.542373, mean_eps: 0.597446
 134401/300000: episode: 1231, duration: 1.105s, episode steps: 157, steps per second: 142, episode reward: -461.938, mean reward: -2.942 [-100.000,  2.167], mean action: 1.643 [0.000, 3.000],  loss: 22237589223697.937500, mse: 13295103005123448.000000, mean_q: 100733736.407643, mean_eps: 0.597034
 134495/300000: episode: 1232, duration: 0.755s, episode steps:  94, steps per second: 124, episode reward: -353.211, mean reward: -3.758 [-100.000,  0.795], mean action: 1.702 [0.000, 3.000],  loss: 15810924306257.701172, mse: 13880690803770042.000000, mean_q: 102891445.872340, mean_eps: 0.596658
 134710/300000: episode: 1233, duration: 1.510s, episode steps: 215, steps per second: 142, episode reward: -591.848, mean reward: -2.753 [-100.000,  2.153], mean action: 1.786 [0.000, 3.000],  loss: 17347470193840.222656, mse: 14486225770878166.000000, mean_q: 105102928.372093, mean_eps: 0.596194
 134856/300000: episode: 1234, duration: 1.015s, episode steps: 146, steps per second: 144, episode reward: -73.905, mean reward: -0.506 [-100.000, 16.172], mean action: 1.740 [0.000, 3.000],  loss: 17131281042277.699219, mse: 15166164846387986.000000, mean_q: 107250724.602740, mean_eps: 0.595652
 134955/300000: episode: 1235, duration: 0.688s, episode steps:  99, steps per second: 144, episode reward: -288.040, mean reward: -2.909 [-100.000,  1.066], mean action: 1.636 [0.000, 3.000],  loss: 47480385468147.070312, mse: 15661901907728528.000000, mean_q: 108999560.888889, mean_eps: 0.595285
 135070/300000: episode: 1236, duration: 0.828s, episode steps: 115, steps per second: 139, episode reward: -299.776, mean reward: -2.607 [-100.000,  1.052], mean action: 1.748 [0.000, 3.000],  loss: 19311053961554.363281, mse: 16129529096209808.000000, mean_q: 111105844.382609, mean_eps: 0.594964
 135205/300000: episode: 1237, duration: 0.961s, episode steps: 135, steps per second: 141, episode reward: -381.439, mean reward: -2.825 [-100.000,  1.796], mean action: 1.711 [0.000, 3.000],  loss: 26986589539775.527344, mse: 16137299150938900.000000, mean_q: 110487013.985185, mean_eps: 0.594589
 135337/300000: episode: 1238, duration: 0.902s, episode steps: 132, steps per second: 146, episode reward: -507.582, mean reward: -3.845 [-100.000,  1.890], mean action: 1.667 [0.000, 3.000],  loss: 26639681189950.062500, mse: 16776511283718330.000000, mean_q: 113153722.606061, mean_eps: 0.594189
 135432/300000: episode: 1239, duration: 0.656s, episode steps:  95, steps per second: 145, episode reward: -322.606, mean reward: -3.396 [-100.000,  1.246], mean action: 1.758 [0.000, 3.000],  loss: 31706912091771.957031, mse: 16444355435525088.000000, mean_q: 111566742.063158, mean_eps: 0.593848
 135568/300000: episode: 1240, duration: 0.949s, episode steps: 136, steps per second: 143, episode reward: -309.928, mean reward: -2.279 [-100.000,  1.300], mean action: 1.794 [0.000, 3.000],  loss: 19326624582475.292969, mse: 16888821964726754.000000, mean_q: 113924970.588235, mean_eps: 0.593502
 135686/300000: episode: 1241, duration: 0.801s, episode steps: 118, steps per second: 147, episode reward: -44.060, mean reward: -0.373 [-100.000, 24.652], mean action: 1.712 [0.000, 3.000],  loss: 29479120604420.339844, mse: 18222867759488328.000000, mean_q: 118721487.864407, mean_eps: 0.593121
 135799/300000: episode: 1242, duration: 0.829s, episode steps: 113, steps per second: 136, episode reward: -93.483, mean reward: -0.827 [-100.000, 12.248], mean action: 1.611 [0.000, 3.000],  loss: 40735148555563.046875, mse: 17741491602939106.000000, mean_q: 116158961.415929, mean_eps: 0.592774
 135893/300000: episode: 1243, duration: 0.651s, episode steps:  94, steps per second: 144, episode reward: -263.535, mean reward: -2.804 [-100.000,  0.778], mean action: 1.809 [0.000, 3.000],  loss: 11114873129003.574219, mse: 18243747684141448.000000, mean_q: 117725984.765957, mean_eps: 0.592464
 135985/300000: episode: 1244, duration: 0.630s, episode steps:  92, steps per second: 146, episode reward: -335.017, mean reward: -3.641 [-100.000,  2.438], mean action: 1.750 [0.000, 3.000],  loss: 9799143750255.304688, mse: 18010990803065812.000000, mean_q: 116119068.782609, mean_eps: 0.592184
 136081/300000: episode: 1245, duration: 0.688s, episode steps:  96, steps per second: 139, episode reward: -345.912, mean reward: -3.603 [-100.000,  1.022], mean action: 1.656 [0.000, 3.000],  loss: 15138991785301.333984, mse: 18642993906275668.000000, mean_q: 119753421.583333, mean_eps: 0.591902
 136192/300000: episode: 1246, duration: 0.769s, episode steps: 111, steps per second: 144, episode reward: -368.005, mean reward: -3.315 [-100.000,  1.087], mean action: 1.739 [0.000, 3.000],  loss: 16321462318052.324219, mse: 18014854917470632.000000, mean_q: 116791322.594595, mean_eps: 0.591592
 136272/300000: episode: 1247, duration: 0.537s, episode steps:  80, steps per second: 149, episode reward: -117.672, mean reward: -1.471 [-100.000,  8.740], mean action: 1.525 [0.000, 3.000],  loss: 67193255025049.601562, mse: 18744439575307880.000000, mean_q: 119635338.200000, mean_eps: 0.591306
 136410/300000: episode: 1248, duration: 0.982s, episode steps: 138, steps per second: 140, episode reward: -278.688, mean reward: -2.019 [-100.000,  1.544], mean action: 1.674 [0.000, 3.000],  loss: 22603520219640.578125, mse: 19487129676956360.000000, mean_q: 122370240.173913, mean_eps: 0.590979
 136862/300000: episode: 1249, duration: 3.254s, episode steps: 452, steps per second: 139, episode reward: -461.352, mean reward: -1.021 [-100.000,  4.864], mean action: 1.686 [0.000, 3.000],  loss: 29343296636184.921875, mse: 20522166596106260.000000, mean_q: 125999836.991150, mean_eps: 0.590094
 136980/300000: episode: 1250, duration: 0.969s, episode steps: 118, steps per second: 122, episode reward: -283.677, mean reward: -2.404 [-100.000,  2.930], mean action: 1.678 [0.000, 3.000],  loss: 42278556032885.156250, mse: 21082997777964640.000000, mean_q: 127680199.050847, mean_eps: 0.589238
 137095/300000: episode: 1251, duration: 0.822s, episode steps: 115, steps per second: 140, episode reward: -146.453, mean reward: -1.274 [-100.000, 13.025], mean action: 1.722 [0.000, 3.000],  loss: 70804317227381.984375, mse: 21914637611104596.000000, mean_q: 130090355.200000, mean_eps: 0.588889
 137227/300000: episode: 1252, duration: 0.890s, episode steps: 132, steps per second: 148, episode reward: -36.689, mean reward: -0.278 [-100.000, 11.607], mean action: 1.583 [0.000, 3.000],  loss: 57577106547432.726562, mse: 22385053167096676.000000, mean_q: 131719312.060606, mean_eps: 0.588518
 137337/300000: episode: 1253, duration: 0.789s, episode steps: 110, steps per second: 139, episode reward: -446.502, mean reward: -4.059 [-100.000,  1.655], mean action: 1.727 [0.000, 3.000],  loss: 22986026788156.507812, mse: 22626387913076064.000000, mean_q: 132115243.490909, mean_eps: 0.588155
 137432/300000: episode: 1254, duration: 0.639s, episode steps:  95, steps per second: 149, episode reward: -406.887, mean reward: -4.283 [-100.000,  0.696], mean action: 1.632 [0.000, 3.000],  loss: 21673220458690.019531, mse: 23059062236726112.000000, mean_q: 133965387.031579, mean_eps: 0.587848
 137531/300000: episode: 1255, duration: 0.681s, episode steps:  99, steps per second: 145, episode reward: -345.553, mean reward: -3.490 [-100.000,  0.703], mean action: 1.566 [0.000, 3.000],  loss: 39927177458946.585938, mse: 23755986245541144.000000, mean_q: 136610536.484848, mean_eps: 0.587557
 137606/300000: episode: 1256, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -435.386, mean reward: -5.805 [-100.000,  0.114], mean action: 1.880 [0.000, 3.000],  loss: 25737097432268.800781, mse: 24203367169504488.000000, mean_q: 137922516.160000, mean_eps: 0.587296
 137710/300000: episode: 1257, duration: 0.720s, episode steps: 104, steps per second: 145, episode reward: -456.292, mean reward: -4.387 [-100.000,  0.868], mean action: 1.731 [0.000, 3.000],  loss: 11810034977240.615234, mse: 23900611325984768.000000, mean_q: 136272353.000000, mean_eps: 0.587028
 137812/300000: episode: 1258, duration: 0.692s, episode steps: 102, steps per second: 147, episode reward: -227.363, mean reward: -2.229 [-100.000,  2.589], mean action: 1.794 [0.000, 3.000],  loss: 46059832691531.296875, mse: 24531240202940980.000000, mean_q: 137917828.784314, mean_eps: 0.586719
 137941/300000: episode: 1259, duration: 0.912s, episode steps: 129, steps per second: 141, episode reward: -361.077, mean reward: -2.799 [-100.000,  1.727], mean action: 1.605 [0.000, 3.000],  loss: 32340122103077.707031, mse: 24677930348028096.000000, mean_q: 139162157.333333, mean_eps: 0.586372
 138047/300000: episode: 1260, duration: 0.758s, episode steps: 106, steps per second: 140, episode reward: -400.297, mean reward: -3.776 [-100.000,  1.120], mean action: 1.651 [0.000, 3.000],  loss: 48216899329275.171875, mse: 25371128987791436.000000, mean_q: 139859247.094340, mean_eps: 0.586019
 138232/300000: episode: 1261, duration: 1.323s, episode steps: 185, steps per second: 140, episode reward: -289.406, mean reward: -1.564 [-100.000, 12.309], mean action: 1.876 [0.000, 3.000],  loss: 32451650584930.250000, mse: 26148018636713828.000000, mean_q: 142308636.237838, mean_eps: 0.585583
 138528/300000: episode: 1262, duration: 2.090s, episode steps: 296, steps per second: 142, episode reward: -378.580, mean reward: -1.279 [-100.000,  4.194], mean action: 1.709 [0.000, 3.000],  loss: 34984203681792.000000, mse: 27348956322740056.000000, mean_q: 146540972.297297, mean_eps: 0.584862
 138621/300000: episode: 1263, duration: 0.664s, episode steps:  93, steps per second: 140, episode reward: -293.303, mean reward: -3.154 [-100.000,  1.057], mean action: 1.538 [0.000, 3.000],  loss: 20446100800721.203125, mse: 28243710942213724.000000, mean_q: 148816879.225806, mean_eps: 0.584278
 138689/300000: episode: 1264, duration: 0.474s, episode steps:  68, steps per second: 144, episode reward: -78.360, mean reward: -1.152 [-100.000,  6.046], mean action: 1.926 [0.000, 3.000],  loss: 39444097963670.585938, mse: 28984775617713816.000000, mean_q: 150517720.941176, mean_eps: 0.584037
 138793/300000: episode: 1265, duration: 0.694s, episode steps: 104, steps per second: 150, episode reward: -419.050, mean reward: -4.029 [-100.000,  0.879], mean action: 1.606 [0.000, 3.000],  loss: 27041810995357.539062, mse: 29866866825575188.000000, mean_q: 153863522.769231, mean_eps: 0.583778
 138985/300000: episode: 1266, duration: 1.351s, episode steps: 192, steps per second: 142, episode reward: -342.170, mean reward: -1.782 [-100.000,  3.046], mean action: 1.771 [0.000, 3.000],  loss: 46473207529472.000000, mse: 30300182401428140.000000, mean_q: 153928101.875000, mean_eps: 0.583334
 139103/300000: episode: 1267, duration: 0.798s, episode steps: 118, steps per second: 148, episode reward: -434.053, mean reward: -3.678 [-100.000,  1.574], mean action: 1.729 [0.000, 3.000],  loss: 46829839796675.257812, mse: 31267894800207940.000000, mean_q: 157122733.491525, mean_eps: 0.582869
 139208/300000: episode: 1268, duration: 1.150s, episode steps: 105, steps per second:  91, episode reward: -337.404, mean reward: -3.213 [-100.000,  0.679], mean action: 1.714 [0.000, 3.000],  loss: 54945749002142.476562, mse: 32073853407522544.000000, mean_q: 158835819.809524, mean_eps: 0.582535
 139305/300000: episode: 1269, duration: 0.761s, episode steps:  97, steps per second: 127, episode reward: -23.035, mean reward: -0.237 [-100.000, 12.894], mean action: 1.763 [0.000, 3.000],  loss: 54882324336428.867188, mse: 32512495360804168.000000, mean_q: 160618922.474227, mean_eps: 0.582232
 139469/300000: episode: 1270, duration: 1.167s, episode steps: 164, steps per second: 141, episode reward: -193.155, mean reward: -1.178 [-100.000,  4.421], mean action: 1.665 [0.000, 3.000],  loss: 95060981996968.578125, mse: 32366182851680504.000000, mean_q: 160293876.878049, mean_eps: 0.581840
 139578/300000: episode: 1271, duration: 0.745s, episode steps: 109, steps per second: 146, episode reward: -246.375, mean reward: -2.260 [-100.000,  1.933], mean action: 1.587 [0.000, 3.000],  loss: 73215242651554.062500, mse: 32818033240124576.000000, mean_q: 161193897.541284, mean_eps: 0.581431
 139693/300000: episode: 1272, duration: 0.816s, episode steps: 115, steps per second: 141, episode reward: -269.560, mean reward: -2.344 [-100.000,  1.576], mean action: 1.774 [0.000, 3.000],  loss: 71167830882927.296875, mse: 32879531819283820.000000, mean_q: 159665829.217391, mean_eps: 0.581095
 139767/300000: episode: 1273, duration: 0.512s, episode steps:  74, steps per second: 145, episode reward: -419.898, mean reward: -5.674 [-100.000,  0.308], mean action: 1.622 [0.000, 3.000],  loss: 29661928449992.648438, mse: 32026087331151428.000000, mean_q: 157376941.837838, mean_eps: 0.580811
 139875/300000: episode: 1274, duration: 0.742s, episode steps: 108, steps per second: 146, episode reward: -175.935, mean reward: -1.629 [-100.000, 16.339], mean action: 1.417 [0.000, 3.000],  loss: 36252271274059.851562, mse: 34057587756756232.000000, mean_q: 163532158.518519, mean_eps: 0.580539
 139986/300000: episode: 1275, duration: 0.766s, episode steps: 111, steps per second: 145, episode reward: -25.668, mean reward: -0.231 [-100.000, 14.076], mean action: 1.676 [0.000, 3.000],  loss: 37538057966342.921875, mse: 35175013471174212.000000, mean_q: 166737734.918919, mean_eps: 0.580210
 140155/300000: episode: 1276, duration: 1.174s, episode steps: 169, steps per second: 144, episode reward: -520.575, mean reward: -3.080 [-100.000,  1.917], mean action: 1.746 [0.000, 3.000],  loss: 63715190097520.093750, mse: 35680435944737532.000000, mean_q: 167951229.538462, mean_eps: 0.579790
 140258/300000: episode: 1277, duration: 0.702s, episode steps: 103, steps per second: 147, episode reward: -350.139, mean reward: -3.399 [-100.000,  1.017], mean action: 1.631 [0.000, 3.000],  loss: 55722792971293.828125, mse: 35983059140802700.000000, mean_q: 167874116.038835, mean_eps: 0.579382
 140358/300000: episode: 1278, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: -458.450, mean reward: -4.585 [-100.000,  0.321], mean action: 1.610 [0.000, 3.000],  loss: 35864621597327.359375, mse: 37395172681580544.000000, mean_q: 171690189.760000, mean_eps: 0.579078
 140452/300000: episode: 1279, duration: 0.643s, episode steps:  94, steps per second: 146, episode reward: -242.086, mean reward: -2.575 [-100.000,  1.165], mean action: 1.638 [0.000, 3.000],  loss: 27392114559716.765625, mse: 39441943333041432.000000, mean_q: 176756604.510638, mean_eps: 0.578787
 140547/300000: episode: 1280, duration: 0.651s, episode steps:  95, steps per second: 146, episode reward: -452.456, mean reward: -4.763 [-100.000,  0.600], mean action: 1.632 [0.000, 3.000],  loss: 70321810785118.312500, mse: 37929776647677712.000000, mean_q: 172176158.315789, mean_eps: 0.578503
 140661/300000: episode: 1281, duration: 0.829s, episode steps: 114, steps per second: 138, episode reward: -408.307, mean reward: -3.582 [-100.000,  1.394], mean action: 1.702 [0.000, 3.000],  loss: 92043506812029.750000, mse: 39312726169011616.000000, mean_q: 175281963.859649, mean_eps: 0.578189
 140888/300000: episode: 1282, duration: 1.569s, episode steps: 227, steps per second: 145, episode reward: -454.392, mean reward: -2.002 [-100.000,  3.927], mean action: 1.744 [0.000, 3.000],  loss: 81905904701891.093750, mse: 40796856835753888.000000, mean_q: 180049818.678414, mean_eps: 0.577678
 141042/300000: episode: 1283, duration: 1.224s, episode steps: 154, steps per second: 126, episode reward: -460.754, mean reward: -2.992 [-100.000,  2.186], mean action: 1.799 [0.000, 3.000],  loss: 62812603503389.921875, mse: 42702800401254264.000000, mean_q: 184583523.740260, mean_eps: 0.577107
 141157/300000: episode: 1284, duration: 0.925s, episode steps: 115, steps per second: 124, episode reward: -307.610, mean reward: -2.675 [-100.000,  2.002], mean action: 1.652 [0.000, 3.000],  loss: 50348491896315.546875, mse: 43951650642295408.000000, mean_q: 186688989.913043, mean_eps: 0.576703
 141286/300000: episode: 1285, duration: 0.996s, episode steps: 129, steps per second: 130, episode reward: -96.958, mean reward: -0.752 [-100.000, 11.668], mean action: 1.698 [0.000, 3.000],  loss: 34029707261459.843750, mse: 44579163259214288.000000, mean_q: 189097506.914729, mean_eps: 0.576337
 141410/300000: episode: 1286, duration: 0.959s, episode steps: 124, steps per second: 129, episode reward: -303.437, mean reward: -2.447 [-100.000,  2.718], mean action: 1.492 [0.000, 3.000],  loss: 69958400957142.710938, mse: 44777761520898712.000000, mean_q: 188509348.516129, mean_eps: 0.575958
 141512/300000: episode: 1287, duration: 0.790s, episode steps: 102, steps per second: 129, episode reward: -223.689, mean reward: -2.193 [-100.000, 16.447], mean action: 1.667 [0.000, 3.000],  loss: 58411711699867.609375, mse: 46129554003066880.000000, mean_q: 193099096.470588, mean_eps: 0.575619
 141630/300000: episode: 1288, duration: 0.832s, episode steps: 118, steps per second: 142, episode reward: -65.802, mean reward: -0.558 [-100.000, 12.773], mean action: 1.585 [0.000, 3.000],  loss: 59744529629461.695312, mse: 46708855133571248.000000, mean_q: 192332630.372881, mean_eps: 0.575289
 141748/300000: episode: 1289, duration: 0.807s, episode steps: 118, steps per second: 146, episode reward: -511.270, mean reward: -4.333 [-100.000,  1.265], mean action: 1.568 [0.000, 3.000],  loss: 50084986251298.710938, mse: 48308081146910960.000000, mean_q: 196588976.677966, mean_eps: 0.574935
 141894/300000: episode: 1290, duration: 1.019s, episode steps: 146, steps per second: 143, episode reward: -362.503, mean reward: -2.483 [-100.000,  2.499], mean action: 1.651 [0.000, 3.000],  loss: 106421162610183.015625, mse: 48336259072386032.000000, mean_q: 195338475.561644, mean_eps: 0.574538
 141989/300000: episode: 1291, duration: 0.637s, episode steps:  95, steps per second: 149, episode reward: -311.861, mean reward: -3.283 [-100.000,  1.567], mean action: 1.863 [0.000, 3.000],  loss: 116741967059698.531250, mse: 49775060611130984.000000, mean_q: 198892737.178947, mean_eps: 0.574177
 142088/300000: episode: 1292, duration: 0.703s, episode steps:  99, steps per second: 141, episode reward: -295.521, mean reward: -2.985 [-100.000,  0.595], mean action: 1.838 [0.000, 3.000],  loss: 54474156047391.031250, mse: 50907393268365208.000000, mean_q: 201811405.252525, mean_eps: 0.573886
 142201/300000: episode: 1293, duration: 0.780s, episode steps: 113, steps per second: 145, episode reward: -360.673, mean reward: -3.192 [-100.000,  4.029], mean action: 1.850 [0.000, 3.000],  loss: 80168081974190.437500, mse: 50764669738488928.000000, mean_q: 200382205.451327, mean_eps: 0.573568
 142290/300000: episode: 1294, duration: 0.608s, episode steps:  89, steps per second: 146, episode reward: -476.082, mean reward: -5.349 [-100.000,  0.127], mean action: 1.730 [0.000, 3.000],  loss: 122893999143487.281250, mse: 50595801928573576.000000, mean_q: 199251054.382022, mean_eps: 0.573265
 142388/300000: episode: 1295, duration: 0.691s, episode steps:  98, steps per second: 142, episode reward: -40.656, mean reward: -0.415 [-100.000,  9.332], mean action: 1.612 [0.000, 3.000],  loss: 70122474933519.671875, mse: 53187195354667888.000000, mean_q: 205259172.571429, mean_eps: 0.572985
 142531/300000: episode: 1296, duration: 1.057s, episode steps: 143, steps per second: 135, episode reward: -11.816, mean reward: -0.083 [-100.000, 47.127], mean action: 1.713 [0.000, 3.000],  loss: 123161671979151.218750, mse: 54112412785884608.000000, mean_q: 208072088.503496, mean_eps: 0.572623
 142725/300000: episode: 1297, duration: 1.518s, episode steps: 194, steps per second: 128, episode reward: -656.058, mean reward: -3.382 [-100.000,  2.893], mean action: 1.753 [0.000, 3.000],  loss: 68726533744365.523438, mse: 54548597370041208.000000, mean_q: 207580998.185567, mean_eps: 0.572118
 142832/300000: episode: 1298, duration: 0.776s, episode steps: 107, steps per second: 138, episode reward: -472.971, mean reward: -4.420 [-100.000,  1.617], mean action: 1.738 [0.000, 3.000],  loss: 141459713244112.156250, mse: 55261754979452720.000000, mean_q: 207195628.859813, mean_eps: 0.571666
 142920/300000: episode: 1299, duration: 0.601s, episode steps:  88, steps per second: 146, episode reward: -296.857, mean reward: -3.373 [-100.000,  0.487], mean action: 1.773 [0.000, 3.000],  loss: 123052978816465.453125, mse: 56770729834994968.000000, mean_q: 210326292.545455, mean_eps: 0.571373
 143158/300000: episode: 1300, duration: 1.750s, episode steps: 238, steps per second: 136, episode reward: -545.470, mean reward: -2.292 [-100.000,  3.473], mean action: 1.706 [0.000, 3.000],  loss: 93220194133964.375000, mse: 58953651867619640.000000, mean_q: 214637033.344538, mean_eps: 0.570885
 143275/300000: episode: 1301, duration: 0.813s, episode steps: 117, steps per second: 144, episode reward: -163.914, mean reward: -1.401 [-100.000,  9.033], mean action: 1.556 [0.000, 3.000],  loss: 65419595158449.234375, mse: 59048840876133664.000000, mean_q: 216070148.649573, mean_eps: 0.570352
 143360/300000: episode: 1302, duration: 0.624s, episode steps:  85, steps per second: 136, episode reward: -442.278, mean reward: -5.203 [-100.000,  1.033], mean action: 1.918 [0.000, 3.000],  loss: 70932980844616.281250, mse: 61867099712497064.000000, mean_q: 222461876.329412, mean_eps: 0.570049
 143536/300000: episode: 1303, duration: 1.240s, episode steps: 176, steps per second: 142, episode reward: -130.426, mean reward: -0.741 [-100.000, 10.825], mean action: 1.693 [0.000, 3.000],  loss: 92568489885696.000000, mse: 62291593296132656.000000, mean_q: 220433238.090909, mean_eps: 0.569658
 143620/300000: episode: 1304, duration: 0.613s, episode steps:  84, steps per second: 137, episode reward: -413.435, mean reward: -4.922 [-100.000,  0.774], mean action: 1.679 [0.000, 3.000],  loss: 24754066130456.382812, mse: 66301082248051176.000000, mean_q: 230060674.476190, mean_eps: 0.569268
 143719/300000: episode: 1305, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward: -138.879, mean reward: -1.403 [-100.000, 10.666], mean action: 1.889 [0.000, 3.000],  loss: 107179381880232.078125, mse: 64796603454470368.000000, mean_q: 226427460.202020, mean_eps: 0.568993
 143853/300000: episode: 1306, duration: 0.913s, episode steps: 134, steps per second: 147, episode reward: -194.231, mean reward: -1.449 [-100.000,  3.276], mean action: 1.575 [0.000, 3.000],  loss: 154739466638106.750000, mse: 65558468372193648.000000, mean_q: 227121040.000000, mean_eps: 0.568643
 144067/300000: episode: 1307, duration: 1.497s, episode steps: 214, steps per second: 143, episode reward: -470.911, mean reward: -2.201 [-100.000,  2.151], mean action: 1.743 [0.000, 3.000],  loss: 88104655839461.687500, mse: 69220786864174552.000000, mean_q: 234629843.289720, mean_eps: 0.568122
 144212/300000: episode: 1308, duration: 1.007s, episode steps: 145, steps per second: 144, episode reward: -354.639, mean reward: -2.446 [-100.000,  3.643], mean action: 1.634 [0.000, 3.000],  loss: 52233723861062.617188, mse: 71227256333942672.000000, mean_q: 237064369.655172, mean_eps: 0.567583
 144358/300000: episode: 1309, duration: 1.023s, episode steps: 146, steps per second: 143, episode reward: -325.215, mean reward: -2.227 [-100.000,  1.693], mean action: 1.842 [0.000, 3.000],  loss: 154050845385040.656250, mse: 74058750532977536.000000, mean_q: 242096427.397260, mean_eps: 0.567147
 144452/300000: episode: 1310, duration: 0.680s, episode steps:  94, steps per second: 138, episode reward: -123.609, mean reward: -1.315 [-100.000,  9.186], mean action: 1.617 [0.000, 3.000],  loss: 158505211242713.875000, mse: 73858313997527952.000000, mean_q: 240848169.702128, mean_eps: 0.566787
 144553/300000: episode: 1311, duration: 0.728s, episode steps: 101, steps per second: 139, episode reward: -308.229, mean reward: -3.052 [-100.000,  1.011], mean action: 1.653 [0.000, 3.000],  loss: 67212426535966.414062, mse: 77839401362196384.000000, mean_q: 247782502.970297, mean_eps: 0.566494
 144674/300000: episode: 1312, duration: 0.819s, episode steps: 121, steps per second: 148, episode reward: -268.955, mean reward: -2.223 [-100.000,  1.145], mean action: 1.744 [0.000, 3.000],  loss: 192754256577595.250000, mse: 77783626405191616.000000, mean_q: 247939627.504132, mean_eps: 0.566161
 144767/300000: episode: 1313, duration: 0.640s, episode steps:  93, steps per second: 145, episode reward: -300.630, mean reward: -3.233 [-100.000,  0.583], mean action: 1.677 [0.000, 3.000],  loss: 112073358605983.656250, mse: 79311170476424688.000000, mean_q: 251728985.806452, mean_eps: 0.565840
 144915/300000: episode: 1314, duration: 1.052s, episode steps: 148, steps per second: 141, episode reward: -519.146, mean reward: -3.508 [-100.000,  1.834], mean action: 1.743 [0.000, 3.000],  loss: 137406261564609.734375, mse: 79797704548644256.000000, mean_q: 252087285.405405, mean_eps: 0.565478
 145096/300000: episode: 1315, duration: 1.237s, episode steps: 181, steps per second: 146, episode reward: -417.573, mean reward: -2.307 [-100.000,  4.455], mean action: 1.674 [0.000, 3.000],  loss: 246073724710572.562500, mse: 82851234348996624.000000, mean_q: 256975905.767956, mean_eps: 0.564985
 145182/300000: episode: 1316, duration: 0.634s, episode steps:  86, steps per second: 136, episode reward: -338.172, mean reward: -3.932 [-100.000,  0.374], mean action: 1.756 [0.000, 3.000],  loss: 100324552035685.203125, mse: 88331114052620000.000000, mean_q: 267408210.232558, mean_eps: 0.564585
 145294/300000: episode: 1317, duration: 0.771s, episode steps: 112, steps per second: 145, episode reward: -479.788, mean reward: -4.284 [-100.000,  1.049], mean action: 1.768 [0.000, 3.000],  loss: 159665898904429.718750, mse: 86731046395950816.000000, mean_q: 266227995.000000, mean_eps: 0.564288
 145395/300000: episode: 1318, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -266.999, mean reward: -2.644 [-100.000,  2.840], mean action: 1.545 [0.000, 3.000],  loss: 117047627827585.265625, mse: 88298615179294976.000000, mean_q: 264651904.792079, mean_eps: 0.563968
 145507/300000: episode: 1319, duration: 0.806s, episode steps: 112, steps per second: 139, episode reward: -213.905, mean reward: -1.910 [-100.000,  2.154], mean action: 1.821 [0.000, 3.000],  loss: 105380953409828.578125, mse: 93749519448164640.000000, mean_q: 273925993.000000, mean_eps: 0.563648
 145583/300000: episode: 1320, duration: 0.529s, episode steps:  76, steps per second: 144, episode reward: -66.251, mean reward: -0.872 [-100.000,  6.664], mean action: 1.500 [0.000, 3.000],  loss: 86339827090270.312500, mse: 93041347913194224.000000, mean_q: 274542140.000000, mean_eps: 0.563366
 145682/300000: episode: 1321, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -166.855, mean reward: -1.685 [-100.000,  4.212], mean action: 1.515 [0.000, 3.000],  loss: 139924841764677.812500, mse: 93148250552253632.000000, mean_q: 271932722.101010, mean_eps: 0.563104
 145852/300000: episode: 1322, duration: 1.203s, episode steps: 170, steps per second: 141, episode reward: -459.046, mean reward: -2.700 [-100.000,  1.797], mean action: 1.635 [0.000, 3.000],  loss: 122158267797648.562500, mse: 96992235031298816.000000, mean_q: 276992742.588235, mean_eps: 0.562700
 145949/300000: episode: 1323, duration: 0.662s, episode steps:  97, steps per second: 147, episode reward: -448.083, mean reward: -4.619 [-100.000,  0.945], mean action: 1.856 [0.000, 3.000],  loss: 233713740883862.437500, mse: 99840762341063696.000000, mean_q: 282567001.731959, mean_eps: 0.562300
 146094/300000: episode: 1324, duration: 1.011s, episode steps: 145, steps per second: 143, episode reward: -316.740, mean reward: -2.184 [-100.000,  1.650], mean action: 1.690 [0.000, 3.000],  loss: 125440086950452.968750, mse: 103289307131841856.000000, mean_q: 287561693.241379, mean_eps: 0.561937
 146262/300000: episode: 1325, duration: 1.155s, episode steps: 168, steps per second: 145, episode reward: -75.295, mean reward: -0.448 [-100.000, 107.848], mean action: 1.655 [0.000, 3.000],  loss: 123696715349235.812500, mse: 105398087223332672.000000, mean_q: 290937149.523810, mean_eps: 0.561468
 146368/300000: episode: 1326, duration: 0.739s, episode steps: 106, steps per second: 143, episode reward: -241.064, mean reward: -2.274 [-100.000,  8.585], mean action: 1.689 [0.000, 3.000],  loss: 247334564453936.312500, mse: 105630154689698752.000000, mean_q: 288826539.924528, mean_eps: 0.561056
 146585/300000: episode: 1327, duration: 1.516s, episode steps: 217, steps per second: 143, episode reward: -182.263, mean reward: -0.840 [-100.000,  3.965], mean action: 1.737 [0.000, 3.000],  loss: 162719125747872.437500, mse: 112848108005834368.000000, mean_q: 301834275.539171, mean_eps: 0.560572
 146693/300000: episode: 1328, duration: 0.810s, episode steps: 108, steps per second: 133, episode reward: -439.661, mean reward: -4.071 [-100.000,  1.196], mean action: 1.759 [0.000, 3.000],  loss: 147029068330021.937500, mse: 113985512253443040.000000, mean_q: 301972138.518519, mean_eps: 0.560085
 146804/300000: episode: 1329, duration: 0.767s, episode steps: 111, steps per second: 145, episode reward: -290.200, mean reward: -2.614 [-100.000,  1.539], mean action: 1.685 [0.000, 3.000],  loss: 138612722229174.203125, mse: 112085819331650208.000000, mean_q: 297848594.594595, mean_eps: 0.559756
 147087/300000: episode: 1330, duration: 2.268s, episode steps: 283, steps per second: 125, episode reward: -783.065, mean reward: -2.767 [-100.000,  2.753], mean action: 1.731 [0.000, 3.000],  loss: 170629357611304.718750, mse: 118368953820721600.000000, mean_q: 307786909.964664, mean_eps: 0.559165
 147177/300000: episode: 1331, duration: 0.688s, episode steps:  90, steps per second: 131, episode reward: -93.993, mean reward: -1.044 [-100.000, 31.235], mean action: 1.611 [0.000, 3.000],  loss: 105144580606088.531250, mse: 124646352721840272.000000, mean_q: 311918774.577778, mean_eps: 0.558606
 147274/300000: episode: 1332, duration: 0.749s, episode steps:  97, steps per second: 130, episode reward: -341.892, mean reward: -3.525 [-100.000,  0.726], mean action: 1.825 [0.000, 3.000],  loss: 91249050626955.875000, mse: 122825524992967664.000000, mean_q: 312709422.680412, mean_eps: 0.558325
 147354/300000: episode: 1333, duration: 0.732s, episode steps:  80, steps per second: 109, episode reward: -394.788, mean reward: -4.935 [-100.000,  0.003], mean action: 1.688 [0.000, 3.000],  loss: 142111696984473.593750, mse: 131819113104487216.000000, mean_q: 326159551.200000, mean_eps: 0.558060
 147484/300000: episode: 1334, duration: 1.043s, episode steps: 130, steps per second: 125, episode reward: -40.065, mean reward: -0.308 [-100.000, 20.090], mean action: 1.738 [0.000, 3.000],  loss: 232591937995256.125000, mse: 133138996382110144.000000, mean_q: 323719745.476923, mean_eps: 0.557744
 147913/300000: episode: 1335, duration: 3.614s, episode steps: 429, steps per second: 119, episode reward: -384.386, mean reward: -0.896 [-100.000,  8.874], mean action: 1.683 [0.000, 3.000],  loss: 217896429345078.312500, mse: 136608063215241920.000000, mean_q: 329652293.072261, mean_eps: 0.556906
 148005/300000: episode: 1336, duration: 0.713s, episode steps:  92, steps per second: 129, episode reward: -352.199, mean reward: -3.828 [-100.000,  2.834], mean action: 1.728 [0.000, 3.000],  loss: 334963915312617.750000, mse: 141204270536344272.000000, mean_q: 336215345.913043, mean_eps: 0.556125
 148181/300000: episode: 1337, duration: 1.241s, episode steps: 176, steps per second: 142, episode reward: -346.710, mean reward: -1.970 [-100.000, 47.539], mean action: 1.648 [0.000, 3.000],  loss: 328412253698606.562500, mse: 144733470239370880.000000, mean_q: 338533689.090909, mean_eps: 0.555723
 148265/300000: episode: 1338, duration: 0.576s, episode steps:  84, steps per second: 146, episode reward: -428.131, mean reward: -5.097 [-100.000,  0.700], mean action: 1.524 [0.000, 3.000],  loss: 178737102055326.468750, mse: 143405068818292352.000000, mean_q: 335309703.428571, mean_eps: 0.555333
 148377/300000: episode: 1339, duration: 0.815s, episode steps: 112, steps per second: 137, episode reward: -134.124, mean reward: -1.198 [-100.000,  1.839], mean action: 1.643 [0.000, 3.000],  loss: 310931649181403.437500, mse: 148839098434153920.000000, mean_q: 343946863.142857, mean_eps: 0.555038
 148467/300000: episode: 1340, duration: 0.630s, episode steps:  90, steps per second: 143, episode reward: -302.143, mean reward: -3.357 [-100.000,  1.332], mean action: 1.656 [0.000, 3.000],  loss: 207525816979364.968750, mse: 151828736404537696.000000, mean_q: 347256149.688889, mean_eps: 0.554736
 148575/300000: episode: 1341, duration: 0.730s, episode steps: 108, steps per second: 148, episode reward: -263.695, mean reward: -2.442 [-100.000,  1.187], mean action: 1.491 [0.000, 3.000],  loss: 207345136619671.718750, mse: 156292337255997152.000000, mean_q: 352813667.851852, mean_eps: 0.554439
 148732/300000: episode: 1342, duration: 1.164s, episode steps: 157, steps per second: 135, episode reward: -150.278, mean reward: -0.957 [-100.000,  7.913], mean action: 1.682 [0.000, 3.000],  loss: 148647667445994.812500, mse: 158261678228328416.000000, mean_q: 352570704.101911, mean_eps: 0.554041
 148802/300000: episode: 1343, duration: 0.508s, episode steps:  70, steps per second: 138, episode reward: -485.980, mean reward: -6.943 [-100.000, -1.222], mean action: 1.614 [0.000, 3.000],  loss: 343969562399305.125000, mse: 161291089937207872.000000, mean_q: 357542115.200000, mean_eps: 0.553701
 148917/300000: episode: 1344, duration: 0.817s, episode steps: 115, steps per second: 141, episode reward: -278.137, mean reward: -2.419 [-100.000, 16.216], mean action: 1.783 [0.000, 3.000],  loss: 472202498585110.250000, mse: 160762346789306656.000000, mean_q: 357003271.234783, mean_eps: 0.553423
 148996/300000: episode: 1345, duration: 0.564s, episode steps:  79, steps per second: 140, episode reward: -371.061, mean reward: -4.697 [-100.000,  0.772], mean action: 1.696 [0.000, 3.000],  loss: 202001468176150.687500, mse: 174149265571825056.000000, mean_q: 370859904.000000, mean_eps: 0.553132
 149120/300000: episode: 1346, duration: 0.870s, episode steps: 124, steps per second: 142, episode reward: -9.451, mean reward: -0.076 [-100.000, 15.383], mean action: 1.782 [0.000, 3.000],  loss: 713277270096863.000000, mse: 171589831806089152.000000, mean_q: 370566469.677419, mean_eps: 0.552828
 149217/300000: episode: 1347, duration: 0.742s, episode steps:  97, steps per second: 131, episode reward: -215.261, mean reward: -2.219 [-100.000,  1.777], mean action: 1.835 [0.000, 3.000],  loss: 475188031443746.312500, mse: 168465704323031968.000000, mean_q: 363660479.010309, mean_eps: 0.552496
 149533/300000: episode: 1348, duration: 2.572s, episode steps: 316, steps per second: 123, episode reward: -1033.236, mean reward: -3.270 [-100.000,  1.705], mean action: 1.677 [0.000, 3.000],  loss: 381150068148664.687500, mse: 181579011902058240.000000, mean_q: 378217408.405063, mean_eps: 0.551876
 149668/300000: episode: 1349, duration: 1.064s, episode steps: 135, steps per second: 127, episode reward: -347.667, mean reward: -2.575 [-100.000, 26.604], mean action: 1.659 [0.000, 3.000],  loss: 308987259779178.187500, mse: 192188336813109472.000000, mean_q: 383640142.696296, mean_eps: 0.551200
 149882/300000: episode: 1350, duration: 1.629s, episode steps: 214, steps per second: 131, episode reward: -300.698, mean reward: -1.405 [-100.000,  6.795], mean action: 1.813 [0.000, 3.000],  loss: 364663063305857.187500, mse: 201032206191038048.000000, mean_q: 397182686.205607, mean_eps: 0.550677
 149991/300000: episode: 1351, duration: 0.940s, episode steps: 109, steps per second: 116, episode reward: -286.490, mean reward: -2.628 [-100.000,  1.150], mean action: 1.725 [0.000, 3.000],  loss: 571225207238843.875000, mse: 208214765508397120.000000, mean_q: 404012196.110092, mean_eps: 0.550192
 150078/300000: episode: 1352, duration: 0.848s, episode steps:  87, steps per second: 103, episode reward: -265.037, mean reward: -3.046 [-100.000,  2.402], mean action: 1.782 [0.000, 3.000],  loss: 437982779236634.500000, mse: 206469664075123264.000000, mean_q: 398418607.816092, mean_eps: 0.549898
 150177/300000: episode: 1353, duration: 0.915s, episode steps:  99, steps per second: 108, episode reward: -203.536, mean reward: -2.056 [-100.000,  1.343], mean action: 1.707 [0.000, 3.000],  loss: 209074099532065.625000, mse: 205665805615248320.000000, mean_q: 399847745.292929, mean_eps: 0.549619
 150319/300000: episode: 1354, duration: 1.175s, episode steps: 142, steps per second: 121, episode reward: -235.914, mean reward: -1.661 [-100.000, 117.396], mean action: 1.782 [0.000, 3.000],  loss: 214331344972843.281250, mse: 209520771488432352.000000, mean_q: 405414489.464789, mean_eps: 0.549257
 150436/300000: episode: 1355, duration: 0.907s, episode steps: 117, steps per second: 129, episode reward: -429.814, mean reward: -3.674 [-100.000,  1.750], mean action: 1.684 [0.000, 3.000],  loss: 327093681972110.250000, mse: 215823416332061440.000000, mean_q: 407809516.581197, mean_eps: 0.548869
 150533/300000: episode: 1356, duration: 0.795s, episode steps:  97, steps per second: 122, episode reward: -327.837, mean reward: -3.380 [-100.000,  1.632], mean action: 1.753 [0.000, 3.000],  loss: 256219856800060.687500, mse: 217187478055738816.000000, mean_q: 409423823.835052, mean_eps: 0.548548
 150624/300000: episode: 1357, duration: 0.730s, episode steps:  91, steps per second: 125, episode reward: -274.172, mean reward: -3.013 [-100.000, 17.912], mean action: 1.879 [0.000, 3.000],  loss: 231942032714110.593750, mse: 229884184759058624.000000, mean_q: 423999659.956044, mean_eps: 0.548266
 150818/300000: episode: 1358, duration: 1.353s, episode steps: 194, steps per second: 143, episode reward: -471.373, mean reward: -2.430 [-100.000,  2.396], mean action: 1.768 [0.000, 3.000],  loss: 404260103543227.375000, mse: 228036580588515360.000000, mean_q: 422305819.381443, mean_eps: 0.547838
 150921/300000: episode: 1359, duration: 0.736s, episode steps: 103, steps per second: 140, episode reward: -145.249, mean reward: -1.410 [-100.000,  3.749], mean action: 1.612 [0.000, 3.000],  loss: 736335532489161.375000, mse: 235337956480302304.000000, mean_q: 428128274.330097, mean_eps: 0.547393
 151118/300000: episode: 1360, duration: 1.472s, episode steps: 197, steps per second: 134, episode reward: -385.818, mean reward: -1.958 [-100.000,  7.687], mean action: 1.822 [0.000, 3.000],  loss: 465210386339767.250000, mse: 234430935932674848.000000, mean_q: 428187207.309645, mean_eps: 0.546943
 151211/300000: episode: 1361, duration: 0.711s, episode steps:  93, steps per second: 131, episode reward: -247.446, mean reward: -2.661 [-100.000,  3.569], mean action: 1.806 [0.000, 3.000],  loss: 410811830669091.812500, mse: 239974355703741216.000000, mean_q: 429935661.419355, mean_eps: 0.546508
 151337/300000: episode: 1362, duration: 1.156s, episode steps: 126, steps per second: 109, episode reward: -176.949, mean reward: -1.404 [-100.000,  3.703], mean action: 1.754 [0.000, 3.000],  loss: 365737493330927.750000, mse: 245594449257376960.000000, mean_q: 436854023.365079, mean_eps: 0.546180
 151477/300000: episode: 1363, duration: 1.248s, episode steps: 140, steps per second: 112, episode reward: -396.875, mean reward: -2.835 [-100.000,  4.289], mean action: 1.664 [0.000, 3.000],  loss: 323011894238061.687500, mse: 242085042960331104.000000, mean_q: 431800783.542857, mean_eps: 0.545780
 151581/300000: episode: 1364, duration: 0.808s, episode steps: 104, steps per second: 129, episode reward: -339.405, mean reward: -3.264 [-100.000,  1.181], mean action: 1.779 [0.000, 3.000],  loss: 342366634014247.375000, mse: 255457872682337504.000000, mean_q: 448072140.000000, mean_eps: 0.545414
 151674/300000: episode: 1365, duration: 0.711s, episode steps:  93, steps per second: 131, episode reward: -308.252, mean reward: -3.315 [-100.000,  0.949], mean action: 1.656 [0.000, 3.000],  loss: 356139828561457.562500, mse: 263185808361149088.000000, mean_q: 449652461.075269, mean_eps: 0.545119
 151824/300000: episode: 1366, duration: 1.174s, episode steps: 150, steps per second: 128, episode reward: -699.756, mean reward: -4.665 [-100.000,  2.655], mean action: 1.760 [0.000, 3.000],  loss: 327150565784726.187500, mse: 254775375507925664.000000, mean_q: 445830178.346667, mean_eps: 0.544755
 152119/300000: episode: 1367, duration: 2.244s, episode steps: 295, steps per second: 131, episode reward: -722.536, mean reward: -2.449 [-100.000,  2.202], mean action: 1.719 [0.000, 3.000],  loss: 552278631654070.250000, mse: 277762577905132768.000000, mean_q: 468232336.488136, mean_eps: 0.544087
 152251/300000: episode: 1368, duration: 1.037s, episode steps: 132, steps per second: 127, episode reward: -184.296, mean reward: -1.396 [-100.000,  9.403], mean action: 1.598 [0.000, 3.000],  loss: 397075249396767.000000, mse: 284211818764475168.000000, mean_q: 474465198.303030, mean_eps: 0.543446
 152355/300000: episode: 1369, duration: 0.763s, episode steps: 104, steps per second: 136, episode reward: -137.474, mean reward: -1.322 [-100.000,  2.257], mean action: 1.625 [0.000, 3.000],  loss: 489793496184359.375000, mse: 297956249806226688.000000, mean_q: 484051870.153846, mean_eps: 0.543092
 152445/300000: episode: 1370, duration: 0.625s, episode steps:  90, steps per second: 144, episode reward: -320.948, mean reward: -3.566 [-100.000,  0.649], mean action: 1.744 [0.000, 3.000],  loss: 200015913710023.125000, mse: 299990644671002880.000000, mean_q: 488456513.422222, mean_eps: 0.542801
 152592/300000: episode: 1371, duration: 1.062s, episode steps: 147, steps per second: 138, episode reward: -585.069, mean reward: -3.980 [-100.000,  1.802], mean action: 1.850 [0.000, 3.000],  loss: 888483527536835.000000, mse: 307322005401076352.000000, mean_q: 488481081.687075, mean_eps: 0.542446
 152680/300000: episode: 1372, duration: 0.615s, episode steps:  88, steps per second: 143, episode reward: -451.479, mean reward: -5.130 [-100.000,  0.278], mean action: 1.614 [0.000, 3.000],  loss: 387752545222656.000000, mse: 300529535670979520.000000, mean_q: 480346534.181818, mean_eps: 0.542094
 152805/300000: episode: 1373, duration: 0.882s, episode steps: 125, steps per second: 142, episode reward: -274.771, mean reward: -2.198 [-100.000,  2.290], mean action: 1.736 [0.000, 3.000],  loss: 496627695099052.062500, mse: 306742714890111168.000000, mean_q: 488470557.696000, mean_eps: 0.541774
 152896/300000: episode: 1374, duration: 0.657s, episode steps:  91, steps per second: 139, episode reward: -417.789, mean reward: -4.591 [-100.000,  0.635], mean action: 1.659 [0.000, 3.000],  loss: 513048726787533.375000, mse: 314725787938618304.000000, mean_q: 489435477.450549, mean_eps: 0.541450
 152993/300000: episode: 1375, duration: 0.657s, episode steps:  97, steps per second: 148, episode reward: -538.796, mean reward: -5.555 [-100.000,  1.372], mean action: 1.773 [0.000, 3.000],  loss: 490711338015839.000000, mse: 307198888451152128.000000, mean_q: 491333643.876289, mean_eps: 0.541168
 153101/300000: episode: 1376, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -330.269, mean reward: -3.058 [-100.000,  3.094], mean action: 1.750 [0.000, 3.000],  loss: 764508246658010.125000, mse: 326130843086079744.000000, mean_q: 503765584.592593, mean_eps: 0.540860
 153258/300000: episode: 1377, duration: 1.101s, episode steps: 157, steps per second: 143, episode reward: -462.243, mean reward: -2.944 [-100.000,  2.352], mean action: 1.611 [0.000, 3.000],  loss: 398441589876722.937500, mse: 317964326191408704.000000, mean_q: 495972013.248408, mean_eps: 0.540463
 153338/300000: episode: 1378, duration: 0.557s, episode steps:  80, steps per second: 144, episode reward: -470.265, mean reward: -5.878 [-100.000,  0.540], mean action: 1.650 [0.000, 3.000],  loss: 367974210076672.000000, mse: 334782327088689984.000000, mean_q: 515817792.800000, mean_eps: 0.540108
 153421/300000: episode: 1379, duration: 0.602s, episode steps:  83, steps per second: 138, episode reward: -388.139, mean reward: -4.676 [-100.000,  0.326], mean action: 1.735 [0.000, 3.000],  loss: 989498772042073.500000, mse: 329382494767611136.000000, mean_q: 508196613.397590, mean_eps: 0.539863
 153661/300000: episode: 1380, duration: 1.704s, episode steps: 240, steps per second: 141, episode reward: -758.977, mean reward: -3.162 [-100.000,  1.782], mean action: 1.688 [0.000, 3.000],  loss: 537360153863782.375000, mse: 339955636886306816.000000, mean_q: 517223944.400000, mean_eps: 0.539378
 153784/300000: episode: 1381, duration: 0.911s, episode steps: 123, steps per second: 135, episode reward: -246.825, mean reward: -2.007 [-100.000,  2.470], mean action: 1.813 [0.000, 3.000],  loss: 343068576675490.312500, mse: 360620345068204288.000000, mean_q: 531351125.333333, mean_eps: 0.538834
 153865/300000: episode: 1382, duration: 0.572s, episode steps:  81, steps per second: 142, episode reward: -351.990, mean reward: -4.346 [-100.000,  0.239], mean action: 1.728 [0.000, 3.000],  loss: 219950255627580.062500, mse: 358539878838000576.000000, mean_q: 528515096.888889, mean_eps: 0.538528
 154087/300000: episode: 1383, duration: 1.577s, episode steps: 222, steps per second: 141, episode reward: -789.204, mean reward: -3.555 [-100.000,  1.944], mean action: 1.802 [0.000, 3.000],  loss: 905330490480400.125000, mse: 375826893643925376.000000, mean_q: 539240192.576577, mean_eps: 0.538073
 154262/300000: episode: 1384, duration: 1.239s, episode steps: 175, steps per second: 141, episode reward: -544.636, mean reward: -3.112 [-100.000,  1.806], mean action: 1.811 [0.000, 3.000],  loss: 1048012860042210.750000, mse: 385181420494055936.000000, mean_q: 546669811.748571, mean_eps: 0.537478
 154399/300000: episode: 1385, duration: 0.987s, episode steps: 137, steps per second: 139, episode reward: -167.722, mean reward: -1.224 [-100.000,  2.082], mean action: 1.679 [0.000, 3.000],  loss: 605056722976409.250000, mse: 387283403640097920.000000, mean_q: 548738715.094890, mean_eps: 0.537010
 154498/300000: episode: 1386, duration: 0.673s, episode steps:  99, steps per second: 147, episode reward: -146.042, mean reward: -1.475 [-100.000, 12.374], mean action: 1.626 [0.000, 3.000],  loss: 797193794560413.750000, mse: 387551520728282560.000000, mean_q: 554068758.949495, mean_eps: 0.536656
 154603/300000: episode: 1387, duration: 0.720s, episode steps: 105, steps per second: 146, episode reward: -330.649, mean reward: -3.149 [-100.000, 37.932], mean action: 1.724 [0.000, 3.000],  loss: 589921126698813.000000, mse: 397335030980945664.000000, mean_q: 557802098.590476, mean_eps: 0.536350
 154689/300000: episode: 1388, duration: 0.636s, episode steps:  86, steps per second: 135, episode reward: -472.273, mean reward: -5.492 [-100.000,  0.080], mean action: 1.686 [0.000, 3.000],  loss: 631616569539083.875000, mse: 404407944428388352.000000, mean_q: 562307691.906977, mean_eps: 0.536063
 154773/300000: episode: 1389, duration: 0.599s, episode steps:  84, steps per second: 140, episode reward: -523.357, mean reward: -6.230 [-100.000, -0.119], mean action: 1.774 [0.000, 3.000],  loss: 549637343274910.500000, mse: 410802015633632704.000000, mean_q: 566225615.238095, mean_eps: 0.535809
 154857/300000: episode: 1390, duration: 0.570s, episode steps:  84, steps per second: 147, episode reward: -414.840, mean reward: -4.939 [-100.000,  0.786], mean action: 1.786 [0.000, 3.000],  loss: 1232777733160179.750000, mse: 421181466961036032.000000, mean_q: 574481669.714286, mean_eps: 0.535556
 155074/300000: episode: 1391, duration: 1.555s, episode steps: 217, steps per second: 140, episode reward: -171.587, mean reward: -0.791 [-100.000, 13.447], mean action: 1.756 [0.000, 3.000],  loss: 483963638923792.500000, mse: 415746132995439552.000000, mean_q: 571807288.774194, mean_eps: 0.535105
 155233/300000: episode: 1392, duration: 1.119s, episode steps: 159, steps per second: 142, episode reward: -277.513, mean reward: -1.745 [-100.000,  4.165], mean action: 1.748 [0.000, 3.000],  loss: 870866333672712.000000, mse: 424135314468988288.000000, mean_q: 573050205.383648, mean_eps: 0.534541
 155320/300000: episode: 1393, duration: 0.648s, episode steps:  87, steps per second: 134, episode reward: -277.619, mean reward: -3.191 [-100.000,  0.953], mean action: 1.816 [0.000, 3.000],  loss: 1153684979198258.000000, mse: 444717506619425344.000000, mean_q: 591363679.264368, mean_eps: 0.534172
 155409/300000: episode: 1394, duration: 0.616s, episode steps:  89, steps per second: 145, episode reward: -99.814, mean reward: -1.122 [-100.000, 31.523], mean action: 1.910 [0.000, 3.000],  loss: 728657514404323.250000, mse: 437508726443489664.000000, mean_q: 584433678.022472, mean_eps: 0.533908
 155619/300000: episode: 1395, duration: 1.601s, episode steps: 210, steps per second: 131, episode reward: -283.606, mean reward: -1.351 [-100.000,  4.531], mean action: 1.686 [0.000, 3.000],  loss: 620572591182253.125000, mse: 447750351734010496.000000, mean_q: 597329679.238095, mean_eps: 0.533460
 155774/300000: episode: 1396, duration: 1.161s, episode steps: 155, steps per second: 134, episode reward: -403.885, mean reward: -2.606 [-100.000,  3.264], mean action: 1.665 [0.000, 3.000],  loss: 832906762551507.375000, mse: 463991270531998400.000000, mean_q: 604415345.341936, mean_eps: 0.532912
 156059/300000: episode: 1397, duration: 2.859s, episode steps: 285, steps per second: 100, episode reward: -1039.783, mean reward: -3.648 [-100.000,  1.720], mean action: 1.684 [0.000, 3.000],  loss: 641892299011959.500000, mse: 488764797632250304.000000, mean_q: 619859527.073684, mean_eps: 0.532252
 156193/300000: episode: 1398, duration: 1.152s, episode steps: 134, steps per second: 116, episode reward: -509.383, mean reward: -3.801 [-100.000,  2.488], mean action: 1.687 [0.000, 3.000],  loss: 427151935750388.562500, mse: 524245571780942528.000000, mean_q: 638316877.134328, mean_eps: 0.531624
 156430/300000: episode: 1399, duration: 1.678s, episode steps: 237, steps per second: 141, episode reward: -571.579, mean reward: -2.412 [-100.000,  2.805], mean action: 1.684 [0.000, 3.000],  loss: 1016747463358230.625000, mse: 528267748763750528.000000, mean_q: 643781189.805907, mean_eps: 0.531067
 156535/300000: episode: 1400, duration: 0.700s, episode steps: 105, steps per second: 150, episode reward: -381.104, mean reward: -3.630 [-100.000,  0.930], mean action: 1.695 [0.000, 3.000],  loss: 1331397915124882.250000, mse: 540301506083478400.000000, mean_q: 645209348.571429, mean_eps: 0.530554
 156688/300000: episode: 1401, duration: 1.061s, episode steps: 153, steps per second: 144, episode reward: -71.563, mean reward: -0.468 [-100.000, 14.948], mean action: 1.797 [0.000, 3.000],  loss: 1113850082766553.500000, mse: 531425978383025280.000000, mean_q: 642774151.947712, mean_eps: 0.530167
 156802/300000: episode: 1402, duration: 0.775s, episode steps: 114, steps per second: 147, episode reward: -355.517, mean reward: -3.119 [-100.000,  0.971], mean action: 1.789 [0.000, 3.000],  loss: 990524219515023.750000, mse: 555134051807372864.000000, mean_q: 655888347.228070, mean_eps: 0.529767
 156912/300000: episode: 1403, duration: 0.761s, episode steps: 110, steps per second: 145, episode reward: -124.557, mean reward: -1.132 [-100.000,  3.449], mean action: 1.564 [0.000, 3.000],  loss: 825488686081638.375000, mse: 544214989341844480.000000, mean_q: 648605637.236364, mean_eps: 0.529431
 157064/300000: episode: 1404, duration: 1.089s, episode steps: 152, steps per second: 140, episode reward: -189.357, mean reward: -1.246 [-100.000, 10.501], mean action: 1.888 [0.000, 3.000],  loss: 1123606836640067.375000, mse: 555183212605845312.000000, mean_q: 653959703.578947, mean_eps: 0.529038
 157217/300000: episode: 1405, duration: 1.077s, episode steps: 153, steps per second: 142, episode reward: -169.551, mean reward: -1.108 [-100.000,  2.563], mean action: 1.686 [0.000, 3.000],  loss: 1024736745918437.250000, mse: 564477410425272000.000000, mean_q: 659558547.241830, mean_eps: 0.528580
 157406/300000: episode: 1406, duration: 1.302s, episode steps: 189, steps per second: 145, episode reward: -54.333, mean reward: -0.287 [-100.000, 12.802], mean action: 1.746 [0.000, 3.000],  loss: 1289234782329103.000000, mse: 582867644735961344.000000, mean_q: 671048362.497355, mean_eps: 0.528067
 157516/300000: episode: 1407, duration: 0.743s, episode steps: 110, steps per second: 148, episode reward: -268.440, mean reward: -2.440 [-100.000,  1.855], mean action: 1.764 [0.000, 3.000],  loss: 1378259106934411.750000, mse: 581287657702438912.000000, mean_q: 673504160.290909, mean_eps: 0.527619
 157599/300000: episode: 1408, duration: 0.584s, episode steps:  83, steps per second: 142, episode reward: -382.325, mean reward: -4.606 [-100.000,  0.426], mean action: 1.699 [0.000, 3.000],  loss: 986742764822429.250000, mse: 582412846727745920.000000, mean_q: 677430050.698795, mean_eps: 0.527329
 157694/300000: episode: 1409, duration: 0.633s, episode steps:  95, steps per second: 150, episode reward: -445.580, mean reward: -4.690 [-100.000,  0.560], mean action: 1.695 [0.000, 3.000],  loss: 1017087970365310.625000, mse: 607447133298462464.000000, mean_q: 682460254.989474, mean_eps: 0.527062
 157817/300000: episode: 1410, duration: 0.823s, episode steps: 123, steps per second: 149, episode reward: -489.880, mean reward: -3.983 [-100.000,  1.853], mean action: 1.675 [0.000, 3.000],  loss: 893290383398495.750000, mse: 586395874842742656.000000, mean_q: 674051484.617886, mean_eps: 0.526735
 158145/300000: episode: 1411, duration: 2.298s, episode steps: 328, steps per second: 143, episode reward: -524.023, mean reward: -1.598 [-100.000,  4.105], mean action: 1.686 [0.000, 3.000],  loss: 746714805394856.625000, mse: 612475119384581248.000000, mean_q: 690750712.585366, mean_eps: 0.526058
 158245/300000: episode: 1412, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward:  8.773, mean reward:  0.088 [-100.000, 15.786], mean action: 1.770 [0.000, 3.000],  loss: 632717321143582.750000, mse: 617116546108951040.000000, mean_q: 696270631.680000, mean_eps: 0.525417
 158333/300000: episode: 1413, duration: 0.589s, episode steps:  88, steps per second: 149, episode reward: -358.519, mean reward: -4.074 [-100.000,  2.306], mean action: 1.636 [0.000, 3.000],  loss: 706504565790533.875000, mse: 618804363209188096.000000, mean_q: 695893965.090909, mean_eps: 0.525135
 158427/300000: episode: 1414, duration: 0.633s, episode steps:  94, steps per second: 148, episode reward: -417.062, mean reward: -4.437 [-100.000,  0.758], mean action: 1.702 [0.000, 3.000],  loss: 958044860791786.250000, mse: 679676561988601600.000000, mean_q: 736981043.744681, mean_eps: 0.524861
 158546/300000: episode: 1415, duration: 0.831s, episode steps: 119, steps per second: 143, episode reward: -188.362, mean reward: -1.583 [-100.000,  3.609], mean action: 1.538 [0.000, 3.000],  loss: 627282628399000.750000, mse: 648783009877669376.000000, mean_q: 708943128.201681, mean_eps: 0.524542
 158659/300000: episode: 1416, duration: 0.757s, episode steps: 113, steps per second: 149, episode reward: -429.417, mean reward: -3.800 [-100.000,  0.894], mean action: 1.708 [0.000, 3.000],  loss: 1804002982223944.500000, mse: 656294946701346816.000000, mean_q: 718157261.592920, mean_eps: 0.524194
 158784/300000: episode: 1417, duration: 0.893s, episode steps: 125, steps per second: 140, episode reward: -412.941, mean reward: -3.304 [-100.000,  1.445], mean action: 1.800 [0.000, 3.000],  loss: 1128378156860833.750000, mse: 692474632641940864.000000, mean_q: 742762335.232000, mean_eps: 0.523837
 158917/300000: episode: 1418, duration: 1.098s, episode steps: 133, steps per second: 121, episode reward: -330.029, mean reward: -2.481 [-100.000,  1.707], mean action: 1.759 [0.000, 3.000],  loss: 965215633747998.750000, mse: 701163225298322304.000000, mean_q: 747153809.563910, mean_eps: 0.523450
 159061/300000: episode: 1419, duration: 1.109s, episode steps: 144, steps per second: 130, episode reward: -568.459, mean reward: -3.948 [-100.000,  2.333], mean action: 1.799 [0.000, 3.000],  loss: 660219362512440.875000, mse: 707380476087165824.000000, mean_q: 744527904.888889, mean_eps: 0.523034
 159151/300000: episode: 1420, duration: 0.687s, episode steps:  90, steps per second: 131, episode reward: -391.298, mean reward: -4.348 [-100.000,  1.296], mean action: 1.611 [0.000, 3.000],  loss: 870113657079580.500000, mse: 712022345028352256.000000, mean_q: 750109996.088889, mean_eps: 0.522684
 159246/300000: episode: 1421, duration: 0.702s, episode steps:  95, steps per second: 135, episode reward: -221.417, mean reward: -2.331 [-100.000,  1.251], mean action: 1.811 [0.000, 3.000],  loss: 935473011322492.000000, mse: 708018201069575808.000000, mean_q: 751540010.442105, mean_eps: 0.522406
 159356/300000: episode: 1422, duration: 0.851s, episode steps: 110, steps per second: 129, episode reward: -225.744, mean reward: -2.052 [-100.000,  1.971], mean action: 1.609 [0.000, 3.000],  loss: 745414527663830.125000, mse: 726335104195255552.000000, mean_q: 756308585.309091, mean_eps: 0.522098
 159476/300000: episode: 1423, duration: 0.914s, episode steps: 120, steps per second: 131, episode reward: -282.839, mean reward: -2.357 [-100.000,  0.795], mean action: 1.858 [0.000, 3.000],  loss: 739578639583368.500000, mse: 733296640513592576.000000, mean_q: 762272699.733333, mean_eps: 0.521754
 159587/300000: episode: 1424, duration: 0.750s, episode steps: 111, steps per second: 148, episode reward: -412.803, mean reward: -3.719 [-100.000,  0.995], mean action: 1.766 [0.000, 3.000],  loss: 1921259826763979.000000, mse: 764005803064859904.000000, mean_q: 779168902.918919, mean_eps: 0.521407
 160026/300000: episode: 1425, duration: 3.199s, episode steps: 439, steps per second: 137, episode reward: -297.514, mean reward: -0.678 [-100.000,  5.633], mean action: 1.708 [0.000, 3.000],  loss: 1473919039895957.750000, mse: 766864411731055360.000000, mean_q: 776308442.678815, mean_eps: 0.520582
 160111/300000: episode: 1426, duration: 0.576s, episode steps:  85, steps per second: 148, episode reward: -111.249, mean reward: -1.309 [-100.000, 14.109], mean action: 1.894 [0.000, 3.000],  loss: 1883801761732053.750000, mse: 760691831510974720.000000, mean_q: 774515113.411765, mean_eps: 0.519796
 160252/300000: episode: 1427, duration: 0.952s, episode steps: 141, steps per second: 148, episode reward: -77.315, mean reward: -0.548 [-100.000, 11.348], mean action: 1.631 [0.000, 3.000],  loss: 1540984813116336.000000, mse: 796114406440924672.000000, mean_q: 786114299.007092, mean_eps: 0.519457
 160370/300000: episode: 1428, duration: 0.860s, episode steps: 118, steps per second: 137, episode reward: -281.187, mean reward: -2.383 [-100.000,  1.946], mean action: 1.661 [0.000, 3.000],  loss: 1029402675162511.125000, mse: 803979245526721152.000000, mean_q: 796855187.525424, mean_eps: 0.519068
 160450/300000: episode: 1429, duration: 0.574s, episode steps:  80, steps per second: 139, episode reward: -55.740, mean reward: -0.697 [-100.000, 111.341], mean action: 1.587 [0.000, 3.000],  loss: 1444682257242521.500000, mse: 833083301476224256.000000, mean_q: 808858289.600000, mean_eps: 0.518771
 160544/300000: episode: 1430, duration: 0.728s, episode steps:  94, steps per second: 129, episode reward: -82.188, mean reward: -0.874 [-100.000,  5.110], mean action: 1.734 [0.000, 3.000],  loss: 985998106340940.250000, mse: 819668018179905664.000000, mean_q: 801959773.276596, mean_eps: 0.518511
 160732/300000: episode: 1431, duration: 1.589s, episode steps: 188, steps per second: 118, episode reward: -291.301, mean reward: -1.549 [-100.000,  2.631], mean action: 1.729 [0.000, 3.000],  loss: 1208281152267111.500000, mse: 862691280561261696.000000, mean_q: 824905245.276596, mean_eps: 0.518087
 160891/300000: episode: 1432, duration: 1.321s, episode steps: 159, steps per second: 120, episode reward: -337.902, mean reward: -2.125 [-100.000,  2.213], mean action: 1.780 [0.000, 3.000],  loss: 1383734789864235.500000, mse: 885880648582894848.000000, mean_q: 840393485.685535, mean_eps: 0.517567
 161011/300000: episode: 1433, duration: 0.876s, episode steps: 120, steps per second: 137, episode reward: -177.866, mean reward: -1.482 [-100.000,  2.718], mean action: 1.783 [0.000, 3.000],  loss: 1643799613031492.250000, mse: 916757538084137600.000000, mean_q: 855405903.466667, mean_eps: 0.517149
 161172/300000: episode: 1434, duration: 1.240s, episode steps: 161, steps per second: 130, episode reward: -307.893, mean reward: -1.912 [-100.000,  3.433], mean action: 1.602 [0.000, 3.000],  loss: 1064980627274453.125000, mse: 902185687583622400.000000, mean_q: 840253207.850932, mean_eps: 0.516727
 161257/300000: episode: 1435, duration: 0.588s, episode steps:  85, steps per second: 145, episode reward: -163.386, mean reward: -1.922 [-100.000,  3.424], mean action: 2.141 [0.000, 3.000],  loss: 2165044645989388.000000, mse: 955204923369267456.000000, mean_q: 867245115.482353, mean_eps: 0.516358
 161377/300000: episode: 1436, duration: 0.837s, episode steps: 120, steps per second: 143, episode reward: -363.797, mean reward: -3.032 [-100.000,  1.364], mean action: 1.817 [0.000, 3.000],  loss: 1252405258957619.250000, mse: 953907774256910592.000000, mean_q: 873941643.200000, mean_eps: 0.516050
 161479/300000: episode: 1437, duration: 0.769s, episode steps: 102, steps per second: 133, episode reward: -58.936, mean reward: -0.578 [-100.000,  8.089], mean action: 1.618 [0.000, 3.000],  loss: 925291036905873.625000, mse: 968436299292141312.000000, mean_q: 881454332.862745, mean_eps: 0.515717
 161567/300000: episode: 1438, duration: 0.686s, episode steps:  88, steps per second: 128, episode reward: -396.957, mean reward: -4.511 [-100.000,  0.726], mean action: 1.761 [0.000, 3.000],  loss: 1110582676780125.125000, mse: 986706000449610496.000000, mean_q: 875346693.090909, mean_eps: 0.515432
 161717/300000: episode: 1439, duration: 1.172s, episode steps: 150, steps per second: 128, episode reward: -134.873, mean reward: -0.899 [-100.000, 44.158], mean action: 1.827 [0.000, 3.000],  loss: 2303890631635763.000000, mse: 961137822101625856.000000, mean_q: 864915708.160000, mean_eps: 0.515076
 161839/300000: episode: 1440, duration: 0.915s, episode steps: 122, steps per second: 133, episode reward: -496.583, mean reward: -4.070 [-100.000,  1.658], mean action: 1.721 [0.000, 3.000],  loss: 1186836127510863.750000, mse: 982349594555361280.000000, mean_q: 872712765.901639, mean_eps: 0.514667
 161926/300000: episode: 1441, duration: 0.607s, episode steps:  87, steps per second: 143, episode reward: -317.507, mean reward: -3.650 [-100.000,  0.417], mean action: 1.621 [0.000, 3.000],  loss: 799106990375288.625000, mse: 1037821226192096384.000000, mean_q: 893667575.172414, mean_eps: 0.514354
 162016/300000: episode: 1442, duration: 0.645s, episode steps:  90, steps per second: 139, episode reward: -417.456, mean reward: -4.638 [-100.000,  1.213], mean action: 1.744 [0.000, 3.000],  loss: 1570292990447161.000000, mse: 982668787199479296.000000, mean_q: 880156105.955556, mean_eps: 0.514089
 162132/300000: episode: 1443, duration: 0.784s, episode steps: 116, steps per second: 148, episode reward: -548.934, mean reward: -4.732 [-100.000,  2.201], mean action: 1.776 [0.000, 3.000],  loss: 2013565724510984.750000, mse: 1034926701603699072.000000, mean_q: 897165413.517241, mean_eps: 0.513779
 162239/300000: episode: 1444, duration: 0.721s, episode steps: 107, steps per second: 148, episode reward: -104.353, mean reward: -0.975 [-100.000,  6.979], mean action: 1.598 [0.000, 3.000],  loss: 1584617434838025.500000, mse: 1015387270616995328.000000, mean_q: 891210493.009346, mean_eps: 0.513445
 162340/300000: episode: 1445, duration: 0.708s, episode steps: 101, steps per second: 143, episode reward: -102.624, mean reward: -1.016 [-100.000, 11.886], mean action: 1.842 [0.000, 3.000],  loss: 1749663427776471.500000, mse: 1045182856555137536.000000, mean_q: 905490284.356436, mean_eps: 0.513133
 162439/300000: episode: 1446, duration: 0.690s, episode steps:  99, steps per second: 143, episode reward: -351.361, mean reward: -3.549 [-100.000,  0.950], mean action: 1.737 [0.000, 3.000],  loss: 731261936106527.000000, mse: 1069913464727581952.000000, mean_q: 910500837.494949, mean_eps: 0.512833
 162520/300000: episode: 1447, duration: 0.561s, episode steps:  81, steps per second: 144, episode reward: -298.445, mean reward: -3.685 [-100.000,  1.688], mean action: 1.951 [0.000, 3.000],  loss: 1035068419610952.750000, mse: 1098968323957427712.000000, mean_q: 924841973.728395, mean_eps: 0.512563
 162774/300000: episode: 1448, duration: 1.841s, episode steps: 254, steps per second: 138, episode reward: -576.416, mean reward: -2.269 [-100.000,  2.908], mean action: 1.713 [0.000, 3.000],  loss: 1477596012008875.250000, mse: 1097349277183251456.000000, mean_q: 929770277.795276, mean_eps: 0.512061
 163008/300000: episode: 1449, duration: 1.678s, episode steps: 234, steps per second: 139, episode reward: -841.953, mean reward: -3.598 [-100.000,  1.832], mean action: 1.731 [0.000, 3.000],  loss: 1612318095123762.250000, mse: 1139693993251063296.000000, mean_q: 939107724.307692, mean_eps: 0.511328
 163088/300000: episode: 1450, duration: 0.563s, episode steps:  80, steps per second: 142, episode reward: -401.763, mean reward: -5.022 [-100.000,  0.968], mean action: 1.812 [0.000, 3.000],  loss: 2738608050012160.000000, mse: 1174823781386105600.000000, mean_q: 956809889.600000, mean_eps: 0.510857
 163375/300000: episode: 1451, duration: 2.226s, episode steps: 287, steps per second: 129, episode reward: -321.844, mean reward: -1.121 [-100.000,  3.726], mean action: 1.728 [0.000, 3.000],  loss: 2359704384620037.500000, mse: 1212618557012678400.000000, mean_q: 975873823.665505, mean_eps: 0.510307
 163503/300000: episode: 1452, duration: 1.032s, episode steps: 128, steps per second: 124, episode reward: -255.120, mean reward: -1.993 [-100.000,  1.500], mean action: 1.703 [0.000, 3.000],  loss: 1292437290483712.000000, mse: 1266436031024988160.000000, mean_q: 1000967459.500000, mean_eps: 0.509684
 163604/300000: episode: 1453, duration: 0.758s, episode steps: 101, steps per second: 133, episode reward: -310.210, mean reward: -3.071 [-100.000,  1.149], mean action: 1.762 [0.000, 3.000],  loss: 1621200079052881.000000, mse: 1264589635757731328.000000, mean_q: 999804367.841584, mean_eps: 0.509341
 163739/300000: episode: 1454, duration: 0.950s, episode steps: 135, steps per second: 142, episode reward: -499.063, mean reward: -3.697 [-100.000,  2.027], mean action: 1.719 [0.000, 3.000],  loss: 1314907380353646.000000, mse: 1273344344353709824.000000, mean_q: 1002812240.592593, mean_eps: 0.508987
 164000/300000: episode: 1455, duration: 1.806s, episode steps: 261, steps per second: 144, episode reward: -420.621, mean reward: -1.612 [-100.000,  4.573], mean action: 1.663 [0.000, 3.000],  loss: 2505726733370411.000000, mse: 1312965030118322176.000000, mean_q: 1016504282.973180, mean_eps: 0.508393
 164136/300000: episode: 1456, duration: 0.956s, episode steps: 136, steps per second: 142, episode reward: -234.698, mean reward: -1.726 [-100.000,  1.626], mean action: 1.699 [0.000, 3.000],  loss: 2383829828863879.500000, mse: 1411144806761342464.000000, mean_q: 1059189313.882353, mean_eps: 0.507798
 164243/300000: episode: 1457, duration: 0.777s, episode steps: 107, steps per second: 138, episode reward: -297.632, mean reward: -2.782 [-100.000,  1.023], mean action: 1.785 [0.000, 3.000],  loss: 2391976637429970.500000, mse: 1414688750125816832.000000, mean_q: 1060464709.981308, mean_eps: 0.507433
 164361/300000: episode: 1458, duration: 0.883s, episode steps: 118, steps per second: 134, episode reward: -207.470, mean reward: -1.758 [-100.000,  3.675], mean action: 1.661 [0.000, 3.000],  loss: 1806362155573109.250000, mse: 1392260626913907200.000000, mean_q: 1047586802.983051, mean_eps: 0.507096
 164616/300000: episode: 1459, duration: 2.280s, episode steps: 255, steps per second: 112, episode reward: -475.431, mean reward: -1.864 [-100.000,  5.595], mean action: 1.647 [0.000, 3.000],  loss: 1844553832724472.000000, mse: 1468201050932753408.000000, mean_q: 1078572013.929412, mean_eps: 0.506536
 164734/300000: episode: 1460, duration: 0.995s, episode steps: 118, steps per second: 119, episode reward: -199.911, mean reward: -1.694 [-100.000,  2.416], mean action: 1.610 [0.000, 3.000],  loss: 2151244703714286.750000, mse: 1485673470983103488.000000, mean_q: 1077088034.711864, mean_eps: 0.505977
 165007/300000: episode: 1461, duration: 2.197s, episode steps: 273, steps per second: 124, episode reward: -305.727, mean reward: -1.120 [-100.000,  3.739], mean action: 1.725 [0.000, 3.000],  loss: 2456605409701063.000000, mse: 1488479945704157184.000000, mean_q: 1086223055.238095, mean_eps: 0.505390
 165296/300000: episode: 1462, duration: 2.416s, episode steps: 289, steps per second: 120, episode reward: -303.955, mean reward: -1.052 [-100.000, 43.684], mean action: 1.782 [0.000, 3.000],  loss: 1720314559915146.250000, mse: 1562670673663789056.000000, mean_q: 1111645199.723183, mean_eps: 0.504547
 165418/300000: episode: 1463, duration: 0.900s, episode steps: 122, steps per second: 136, episode reward: -298.068, mean reward: -2.443 [-100.000,  1.364], mean action: 1.648 [0.000, 3.000],  loss: 4133624517217699.500000, mse: 1581550594485615104.000000, mean_q: 1125653310.426229, mean_eps: 0.503930
 165510/300000: episode: 1464, duration: 0.685s, episode steps:  92, steps per second: 134, episode reward: -216.585, mean reward: -2.354 [-100.000,  2.385], mean action: 1.989 [0.000, 3.000],  loss: 978348224920887.625000, mse: 1584920827244781824.000000, mean_q: 1124124434.086957, mean_eps: 0.503609
 165683/300000: episode: 1465, duration: 1.317s, episode steps: 173, steps per second: 131, episode reward: -288.434, mean reward: -1.667 [-100.000,  3.251], mean action: 1.751 [0.000, 3.000],  loss: 3234086370443832.000000, mse: 1646692471231788800.000000, mean_q: 1134120435.791908, mean_eps: 0.503212
 165804/300000: episode: 1466, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -37.971, mean reward: -0.314 [-100.000, 13.914], mean action: 1.702 [0.000, 3.000],  loss: 1979491249741891.750000, mse: 1636078261859533824.000000, mean_q: 1133017075.305785, mean_eps: 0.502771
 165894/300000: episode: 1467, duration: 0.608s, episode steps:  90, steps per second: 148, episode reward: -372.764, mean reward: -4.142 [-100.000,  0.785], mean action: 1.600 [0.000, 3.000],  loss: 2005178038004394.750000, mse: 1688405471260177664.000000, mean_q: 1161976300.800000, mean_eps: 0.502455
 166024/300000: episode: 1468, duration: 0.982s, episode steps: 130, steps per second: 132, episode reward: -234.333, mean reward: -1.803 [-100.000,  3.075], mean action: 1.962 [0.000, 3.000],  loss: 2656413427506065.500000, mse: 1731759800679709952.000000, mean_q: 1169092590.276923, mean_eps: 0.502124
 166123/300000: episode: 1469, duration: 0.717s, episode steps:  99, steps per second: 138, episode reward: -322.887, mean reward: -3.261 [-100.000,  1.012], mean action: 1.818 [0.000, 3.000],  loss: 3449263536983112.500000, mse: 1737541907622300672.000000, mean_q: 1174159001.858586, mean_eps: 0.501781
 166301/300000: episode: 1470, duration: 1.285s, episode steps: 178, steps per second: 139, episode reward: -367.269, mean reward: -2.063 [-100.000,  2.743], mean action: 1.725 [0.000, 3.000],  loss: 3722588939848945.500000, mse: 1796808179835802624.000000, mean_q: 1188438429.483146, mean_eps: 0.501365
 166391/300000: episode: 1471, duration: 0.656s, episode steps:  90, steps per second: 137, episode reward: -278.289, mean reward: -3.092 [-100.000,  1.466], mean action: 1.522 [0.000, 3.000],  loss: 1115406365942670.250000, mse: 1739448803549284608.000000, mean_q: 1165732260.977778, mean_eps: 0.500964
 166517/300000: episode: 1472, duration: 0.883s, episode steps: 126, steps per second: 143, episode reward: -177.213, mean reward: -1.406 [-100.000,  7.468], mean action: 1.690 [0.000, 3.000],  loss: 1245597471292659.750000, mse: 1835116247202676992.000000, mean_q: 1197869840.761905, mean_eps: 0.500640
 166662/300000: episode: 1473, duration: 1.062s, episode steps: 145, steps per second: 137, episode reward: -324.735, mean reward: -2.240 [-100.000,  3.847], mean action: 1.559 [0.000, 3.000],  loss: 2807758135402213.500000, mse: 1836812923876582912.000000, mean_q: 1203677978.924138, mean_eps: 0.500233
 166789/300000: episode: 1474, duration: 0.880s, episode steps: 127, steps per second: 144, episode reward: -459.087, mean reward: -3.615 [-100.000,  1.518], mean action: 1.740 [0.000, 3.000],  loss: 2541753147254219.500000, mse: 1934842206664876800.000000, mean_q: 1226459391.496063, mean_eps: 0.499825
 166939/300000: episode: 1475, duration: 1.051s, episode steps: 150, steps per second: 143, episode reward: -509.146, mean reward: -3.394 [-100.000,  3.790], mean action: 1.740 [0.000, 3.000],  loss: 2423538001515643.000000, mse: 1931305730250654720.000000, mean_q: 1227552632.746667, mean_eps: 0.499410
 167060/300000: episode: 1476, duration: 0.821s, episode steps: 121, steps per second: 147, episode reward: -355.958, mean reward: -2.942 [-100.000, 13.169], mean action: 1.901 [0.000, 3.000],  loss: 2659469219992872.000000, mse: 1951000784381264128.000000, mean_q: 1243220048.925620, mean_eps: 0.499003
 167147/300000: episode: 1477, duration: 0.592s, episode steps:  87, steps per second: 147, episode reward: -319.497, mean reward: -3.672 [-100.000,  1.195], mean action: 1.724 [0.000, 3.000],  loss: 1173854548466299.500000, mse: 1926607317857673216.000000, mean_q: 1228333789.425287, mean_eps: 0.498691
 167255/300000: episode: 1478, duration: 0.814s, episode steps: 108, steps per second: 133, episode reward: -261.889, mean reward: -2.425 [-100.000,  3.702], mean action: 1.787 [0.000, 3.000],  loss: 3196458949469070.000000, mse: 2026206524114226176.000000, mean_q: 1271797025.185185, mean_eps: 0.498399
 167459/300000: episode: 1479, duration: 1.423s, episode steps: 204, steps per second: 143, episode reward: -692.663, mean reward: -3.395 [-100.000,  2.507], mean action: 1.760 [0.000, 3.000],  loss: 1933842976681261.250000, mse: 2039139506692020224.000000, mean_q: 1272030429.803921, mean_eps: 0.497930
 167546/300000: episode: 1480, duration: 0.620s, episode steps:  87, steps per second: 140, episode reward: -430.826, mean reward: -4.952 [-100.000,  0.387], mean action: 1.747 [0.000, 3.000],  loss: 3768333877843097.000000, mse: 2104900601347176960.000000, mean_q: 1292670984.827586, mean_eps: 0.497494
 167646/300000: episode: 1481, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: -135.980, mean reward: -1.360 [-100.000,  9.321], mean action: 1.670 [0.000, 3.000],  loss: 6384173154188657.000000, mse: 2150128226352475648.000000, mean_q: 1297680216.960000, mean_eps: 0.497213
 167790/300000: episode: 1482, duration: 1.169s, episode steps: 144, steps per second: 123, episode reward: -300.731, mean reward: -2.088 [-100.000,  2.807], mean action: 1.632 [0.000, 3.000],  loss: 3488684617527751.000000, mse: 2118571294917365248.000000, mean_q: 1290668685.333333, mean_eps: 0.496848
 168209/300000: episode: 1483, duration: 3.335s, episode steps: 419, steps per second: 126, episode reward: -958.245, mean reward: -2.287 [-100.000,  2.298], mean action: 1.821 [0.000, 3.000],  loss: 4537799459730376.000000, mse: 2283651964570454272.000000, mean_q: 1337896152.286396, mean_eps: 0.496003
 168309/300000: episode: 1484, duration: 0.679s, episode steps: 100, steps per second: 147, episode reward: -105.243, mean reward: -1.052 [-100.000,  5.708], mean action: 1.580 [0.000, 3.000],  loss: 4627829632396165.000000, mse: 2510427527009439744.000000, mean_q: 1393298016.000000, mean_eps: 0.495225
 168416/300000: episode: 1485, duration: 0.797s, episode steps: 107, steps per second: 134, episode reward: -312.999, mean reward: -2.925 [-100.000,  2.469], mean action: 1.785 [0.000, 3.000],  loss: 3664978437687851.000000, mse: 2490922676958314496.000000, mean_q: 1371552801.495327, mean_eps: 0.494914
 168536/300000: episode: 1486, duration: 0.839s, episode steps: 120, steps per second: 143, episode reward: -579.618, mean reward: -4.830 [-100.000,  1.687], mean action: 1.800 [0.000, 3.000],  loss: 2766651368162372.500000, mse: 2438116415826716160.000000, mean_q: 1381622262.400000, mean_eps: 0.494574
 168651/300000: episode: 1487, duration: 0.855s, episode steps: 115, steps per second: 134, episode reward: -49.598, mean reward: -0.431 [-100.000, 19.410], mean action: 1.730 [0.000, 3.000],  loss: 8923814946841787.000000, mse: 2565976308668322816.000000, mean_q: 1400196540.104348, mean_eps: 0.494221
 168787/300000: episode: 1488, duration: 1.000s, episode steps: 136, steps per second: 136, episode reward: -182.067, mean reward: -1.339 [-100.000,  4.543], mean action: 1.537 [0.000, 3.000],  loss: 3323888965455631.000000, mse: 2537606027427492864.000000, mean_q: 1405838274.352941, mean_eps: 0.493845
 168871/300000: episode: 1489, duration: 0.614s, episode steps:  84, steps per second: 137, episode reward: -412.598, mean reward: -4.912 [-100.000,  0.643], mean action: 1.690 [0.000, 3.000],  loss: 4183481196116455.500000, mse: 2515836743773984768.000000, mean_q: 1371410257.523809, mean_eps: 0.493514
 169006/300000: episode: 1490, duration: 1.006s, episode steps: 135, steps per second: 134, episode reward: -209.143, mean reward: -1.549 [-100.000,  4.044], mean action: 1.704 [0.000, 3.000],  loss: 7203654187544303.000000, mse: 2542335320036857344.000000, mean_q: 1389142361.600000, mean_eps: 0.493186
 169256/300000: episode: 1491, duration: 2.018s, episode steps: 250, steps per second: 124, episode reward: -629.144, mean reward: -2.517 [-100.000,  2.705], mean action: 1.732 [0.000, 3.000],  loss: 3419318958514241.500000, mse: 2588761549034108416.000000, mean_q: 1398958164.736000, mean_eps: 0.492609
 169363/300000: episode: 1492, duration: 0.792s, episode steps: 107, steps per second: 135, episode reward: -574.788, mean reward: -5.372 [-100.000,  0.998], mean action: 1.785 [0.000, 3.000],  loss: 3073871359170387.500000, mse: 2689423887902813696.000000, mean_q: 1429516508.112149, mean_eps: 0.492073
 169476/300000: episode: 1493, duration: 0.835s, episode steps: 113, steps per second: 135, episode reward: -390.873, mean reward: -3.459 [-100.000,  1.187], mean action: 1.841 [0.000, 3.000],  loss: 3080186087293363.000000, mse: 2640035110832275968.000000, mean_q: 1421320117.238938, mean_eps: 0.491743
 169592/300000: episode: 1494, duration: 0.900s, episode steps: 116, steps per second: 129, episode reward: -460.287, mean reward: -3.968 [-100.000,  4.260], mean action: 1.716 [0.000, 3.000],  loss: 2255870336831911.500000, mse: 2809140350247013888.000000, mean_q: 1456618466.206897, mean_eps: 0.491400
 169809/300000: episode: 1495, duration: 1.558s, episode steps: 217, steps per second: 139, episode reward: -526.409, mean reward: -2.426 [-100.000,  3.922], mean action: 1.765 [0.000, 3.000],  loss: 4191065709393325.500000, mse: 2811234907743981568.000000, mean_q: 1460096389.308756, mean_eps: 0.490900
 169987/300000: episode: 1496, duration: 1.243s, episode steps: 178, steps per second: 143, episode reward: -449.730, mean reward: -2.527 [-100.000,  3.903], mean action: 1.865 [0.000, 3.000],  loss: 4858337930760698.000000, mse: 2835247302985062912.000000, mean_q: 1474705972.494382, mean_eps: 0.490307
 170095/300000: episode: 1497, duration: 0.784s, episode steps: 108, steps per second: 138, episode reward: -100.831, mean reward: -0.934 [-100.000,  9.068], mean action: 1.824 [0.000, 3.000],  loss: 4130082110240388.500000, mse: 2861261319437986304.000000, mean_q: 1469812407.703704, mean_eps: 0.489879
 170170/300000: episode: 1498, duration: 0.548s, episode steps:  75, steps per second: 137, episode reward: -435.703, mean reward: -5.809 [-100.000, -0.330], mean action: 1.813 [0.000, 3.000],  loss: 4682133807720038.000000, mse: 2782906589119694336.000000, mean_q: 1460575733.760000, mean_eps: 0.489604
 170255/300000: episode: 1499, duration: 0.612s, episode steps:  85, steps per second: 139, episode reward: -267.207, mean reward: -3.144 [-100.000,  1.814], mean action: 1.871 [0.000, 3.000],  loss: 4932568591547067.000000, mse: 2892151605322305536.000000, mean_q: 1484238566.400000, mean_eps: 0.489364
 170347/300000: episode: 1500, duration: 0.794s, episode steps:  92, steps per second: 116, episode reward: -114.834, mean reward: -1.248 [-100.000, 14.382], mean action: 1.587 [0.000, 3.000],  loss: 2767201500089833.500000, mse: 2770909174514809344.000000, mean_q: 1444952513.391304, mean_eps: 0.489098
 170650/300000: episode: 1501, duration: 2.313s, episode steps: 303, steps per second: 131, episode reward: -1139.707, mean reward: -3.761 [-100.000,  1.649], mean action: 1.670 [0.000, 3.000],  loss: 5858014628206666.000000, mse: 3047977512704084480.000000, mean_q: 1511161300.066007, mean_eps: 0.488506
 170768/300000: episode: 1502, duration: 0.866s, episode steps: 118, steps per second: 136, episode reward: -2.984, mean reward: -0.025 [-100.000, 11.767], mean action: 1.720 [0.000, 3.000],  loss: 2808048582487161.500000, mse: 3169344084053874176.000000, mean_q: 1537711909.966102, mean_eps: 0.487874
 170908/300000: episode: 1503, duration: 0.975s, episode steps: 140, steps per second: 144, episode reward: -643.146, mean reward: -4.594 [-100.000,  1.967], mean action: 1.821 [0.000, 3.000],  loss: 9336679340246864.000000, mse: 3242494787177511936.000000, mean_q: 1559285677.714286, mean_eps: 0.487488
 171040/300000: episode: 1504, duration: 0.985s, episode steps: 132, steps per second: 134, episode reward: -32.395, mean reward: -0.245 [-100.000, 18.021], mean action: 1.773 [0.000, 3.000],  loss: 2930385457385348.000000, mse: 3071988765009188864.000000, mean_q: 1519405276.606061, mean_eps: 0.487079
 171271/300000: episode: 1505, duration: 1.685s, episode steps: 231, steps per second: 137, episode reward: -529.158, mean reward: -2.291 [-100.000,  2.717], mean action: 1.719 [0.000, 3.000],  loss: 6036981263293294.000000, mse: 3201071768947684352.000000, mean_q: 1547740699.705628, mean_eps: 0.486535
 171346/300000: episode: 1506, duration: 0.556s, episode steps:  75, steps per second: 135, episode reward: -66.934, mean reward: -0.892 [-100.000, 10.349], mean action: 1.853 [0.000, 3.000],  loss: 4443212410453142.000000, mse: 3219569295960274944.000000, mean_q: 1542964933.973333, mean_eps: 0.486076
 171462/300000: episode: 1507, duration: 0.827s, episode steps: 116, steps per second: 140, episode reward: -524.642, mean reward: -4.523 [-100.000,  1.039], mean action: 1.664 [0.000, 3.000],  loss: 6822775006673249.000000, mse: 3305753918860770816.000000, mean_q: 1562461054.896552, mean_eps: 0.485789
 171561/300000: episode: 1508, duration: 0.717s, episode steps:  99, steps per second: 138, episode reward: -352.090, mean reward: -3.556 [-100.000,  1.136], mean action: 1.788 [0.000, 3.000],  loss: 3789000567655993.000000, mse: 3311532063123080704.000000, mean_q: 1567825395.070707, mean_eps: 0.485467
 171698/300000: episode: 1509, duration: 0.956s, episode steps: 137, steps per second: 143, episode reward: -654.870, mean reward: -4.780 [-100.000,  1.769], mean action: 1.730 [0.000, 3.000],  loss: 2600202288282332.500000, mse: 3378067526228239872.000000, mean_q: 1596421383.474452, mean_eps: 0.485113
 172054/300000: episode: 1510, duration: 2.490s, episode steps: 356, steps per second: 143, episode reward: -1211.166, mean reward: -3.402 [-100.000,  1.791], mean action: 1.713 [0.000, 3.000],  loss: 6982480830931588.000000, mse: 3486914270740920320.000000, mean_q: 1601949911.730337, mean_eps: 0.484373
 172160/300000: episode: 1511, duration: 0.772s, episode steps: 106, steps per second: 137, episode reward: -297.893, mean reward: -2.810 [-100.000,  1.204], mean action: 1.745 [0.000, 3.000],  loss: 6731628879979578.000000, mse: 3681497438706689536.000000, mean_q: 1639562887.245283, mean_eps: 0.483680
 172277/300000: episode: 1512, duration: 0.905s, episode steps: 117, steps per second: 129, episode reward: -280.131, mean reward: -2.394 [-100.000,  2.154], mean action: 1.872 [0.000, 3.000],  loss: 4713259578420863.000000, mse: 3966065572419993600.000000, mean_q: 1694231800.341880, mean_eps: 0.483346
 172371/300000: episode: 1513, duration: 0.651s, episode steps:  94, steps per second: 144, episode reward: -275.178, mean reward: -2.927 [-100.000,  1.992], mean action: 1.532 [0.000, 3.000],  loss: 1875855184377463.750000, mse: 3861029957676815360.000000, mean_q: 1666498720.000000, mean_eps: 0.483030
 172620/300000: episode: 1514, duration: 1.793s, episode steps: 249, steps per second: 139, episode reward: -221.795, mean reward: -0.891 [-100.000,  7.025], mean action: 1.566 [0.000, 3.000],  loss: 7187127101885263.000000, mse: 3885482239372574208.000000, mean_q: 1675773098.152611, mean_eps: 0.482515
 172705/300000: episode: 1515, duration: 0.580s, episode steps:  85, steps per second: 147, episode reward: -322.793, mean reward: -3.798 [-100.000,  0.472], mean action: 1.471 [0.000, 3.000],  loss: 2973864665744131.000000, mse: 4025916123190018560.000000, mean_q: 1700470116.894118, mean_eps: 0.482014
 172957/300000: episode: 1516, duration: 1.886s, episode steps: 252, steps per second: 134, episode reward: -604.170, mean reward: -2.398 [-100.000,  2.879], mean action: 1.774 [0.000, 3.000],  loss: 6086681592910669.000000, mse: 3992705741637808128.000000, mean_q: 1700090859.682540, mean_eps: 0.481509
 173059/300000: episode: 1517, duration: 0.744s, episode steps: 102, steps per second: 137, episode reward: -541.155, mean reward: -5.305 [-100.000,  0.818], mean action: 1.804 [0.000, 3.000],  loss: 6580692917533114.000000, mse: 4104779369201178112.000000, mean_q: 1738422825.411765, mean_eps: 0.480977
 173180/300000: episode: 1518, duration: 0.851s, episode steps: 121, steps per second: 142, episode reward: -458.792, mean reward: -3.792 [-100.000,  1.734], mean action: 1.826 [0.000, 3.000],  loss: 10081566925468900.000000, mse: 4158177049844951552.000000, mean_q: 1749750209.586777, mean_eps: 0.480643
 173281/300000: episode: 1519, duration: 0.702s, episode steps: 101, steps per second: 144, episode reward: -360.661, mean reward: -3.571 [-100.000,  0.822], mean action: 1.693 [0.000, 3.000],  loss: 4818475933520156.000000, mse: 4261925080098743808.000000, mean_q: 1771882535.287129, mean_eps: 0.480310
 173384/300000: episode: 1520, duration: 0.782s, episode steps: 103, steps per second: 132, episode reward: -485.258, mean reward: -4.711 [-100.000,  1.482], mean action: 1.786 [0.000, 3.000],  loss: 4395132418795460.500000, mse: 4240171039086511616.000000, mean_q: 1765528384.621359, mean_eps: 0.480004
 173484/300000: episode: 1521, duration: 0.754s, episode steps: 100, steps per second: 133, episode reward: -417.890, mean reward: -4.179 [-100.000,  0.862], mean action: 1.880 [0.000, 3.000],  loss: 2118997440483819.500000, mse: 4388125168261199360.000000, mean_q: 1803036616.960000, mean_eps: 0.479699
 173577/300000: episode: 1522, duration: 0.667s, episode steps:  93, steps per second: 139, episode reward: -95.585, mean reward: -1.028 [-100.000,  7.461], mean action: 1.548 [0.000, 3.000],  loss: 13516080862766378.000000, mse: 4527235793364679680.000000, mean_q: 1823190753.720430, mean_eps: 0.479410
 173663/300000: episode: 1523, duration: 0.629s, episode steps:  86, steps per second: 137, episode reward: -376.334, mean reward: -4.376 [-100.000,  0.330], mean action: 1.791 [0.000, 3.000],  loss: 9582014756839234.000000, mse: 4262835831412730880.000000, mean_q: 1774639499.906977, mean_eps: 0.479141
 173834/300000: episode: 1524, duration: 1.267s, episode steps: 171, steps per second: 135, episode reward: -490.291, mean reward: -2.867 [-100.000,  2.673], mean action: 1.719 [0.000, 3.000],  loss: 3683385564850954.500000, mse: 4387722834749354496.000000, mean_q: 1795522748.631579, mean_eps: 0.478756
 173914/300000: episode: 1525, duration: 0.562s, episode steps:  80, steps per second: 142, episode reward: -284.770, mean reward: -3.560 [-100.000,  1.422], mean action: 2.013 [0.000, 3.000],  loss: 5443026426422886.000000, mse: 4385627905225392128.000000, mean_q: 1791935512.000000, mean_eps: 0.478379
 174012/300000: episode: 1526, duration: 0.744s, episode steps:  98, steps per second: 132, episode reward: -478.209, mean reward: -4.880 [-100.000,  0.509], mean action: 1.776 [0.000, 3.000],  loss: 10176687111057826.000000, mse: 4293552953802968576.000000, mean_q: 1773452504.816327, mean_eps: 0.478112
 174125/300000: episode: 1527, duration: 0.794s, episode steps: 113, steps per second: 142, episode reward: -52.710, mean reward: -0.466 [-100.000, 18.607], mean action: 1.743 [0.000, 3.000],  loss: 8440468893910877.000000, mse: 4429790988088899072.000000, mean_q: 1796748523.610620, mean_eps: 0.477796
 174257/300000: episode: 1528, duration: 0.919s, episode steps: 132, steps per second: 144, episode reward: -502.582, mean reward: -3.807 [-100.000,  1.280], mean action: 1.750 [0.000, 3.000],  loss: 5479805394370064.000000, mse: 4512808289224534528.000000, mean_q: 1815853961.696970, mean_eps: 0.477428
 174358/300000: episode: 1529, duration: 0.721s, episode steps: 101, steps per second: 140, episode reward: -24.987, mean reward: -0.247 [-100.000, 18.016], mean action: 1.723 [0.000, 3.000],  loss: 3576740185953513.000000, mse: 4649627875867088896.000000, mean_q: 1827234875.564356, mean_eps: 0.477079
 174446/300000: episode: 1530, duration: 0.596s, episode steps:  88, steps per second: 148, episode reward: -492.162, mean reward: -5.593 [-100.000,  0.403], mean action: 1.693 [0.000, 3.000],  loss: 10732911411545554.000000, mse: 4421087186457293824.000000, mean_q: 1803127195.636364, mean_eps: 0.476796
 174531/300000: episode: 1531, duration: 0.614s, episode steps:  85, steps per second: 138, episode reward: -401.178, mean reward: -4.720 [-100.000,  0.502], mean action: 1.835 [0.000, 3.000],  loss: 4192351797619627.500000, mse: 4530656684384714752.000000, mean_q: 1827381756.988235, mean_eps: 0.476536
 174640/300000: episode: 1532, duration: 0.974s, episode steps: 109, steps per second: 112, episode reward: -289.471, mean reward: -2.656 [-100.000,  2.450], mean action: 1.872 [0.000, 3.000],  loss: 2735703982878457.000000, mse: 4763558482956981248.000000, mean_q: 1888342058.275229, mean_eps: 0.476245
 174730/300000: episode: 1533, duration: 0.689s, episode steps:  90, steps per second: 131, episode reward: -413.948, mean reward: -4.599 [-100.000,  0.814], mean action: 1.844 [0.000, 3.000],  loss: 3177124997598777.000000, mse: 4846305859930382336.000000, mean_q: 1870304119.466667, mean_eps: 0.475947
 174860/300000: episode: 1534, duration: 1.072s, episode steps: 130, steps per second: 121, episode reward: -216.356, mean reward: -1.664 [-100.000,  3.405], mean action: 1.938 [0.000, 3.000],  loss: 10505687009248492.000000, mse: 4794378100575124480.000000, mean_q: 1876921764.430769, mean_eps: 0.475616
 175087/300000: episode: 1535, duration: 1.915s, episode steps: 227, steps per second: 119, episode reward: -721.377, mean reward: -3.178 [-100.000,  2.217], mean action: 1.758 [0.000, 3.000],  loss: 11745652086089592.000000, mse: 4976593406294661120.000000, mean_q: 1922272504.669604, mean_eps: 0.475081
 175183/300000: episode: 1536, duration: 0.852s, episode steps:  96, steps per second: 113, episode reward: -223.894, mean reward: -2.332 [-100.000,  2.701], mean action: 1.625 [0.000, 3.000],  loss: 15511842426191872.000000, mse: 4959705829716175872.000000, mean_q: 1903354820.000000, mean_eps: 0.474596
 175312/300000: episode: 1537, duration: 0.995s, episode steps: 129, steps per second: 130, episode reward: -351.557, mean reward: -2.725 [-100.000,  3.540], mean action: 1.783 [0.000, 3.000],  loss: 17692970340298910.000000, mse: 4868539038884712448.000000, mean_q: 1892042634.914729, mean_eps: 0.474259
 175404/300000: episode: 1538, duration: 0.674s, episode steps:  92, steps per second: 137, episode reward: -297.405, mean reward: -3.233 [-100.000,  1.351], mean action: 1.674 [0.000, 3.000],  loss: 9849214526588304.000000, mse: 4942106137687319552.000000, mean_q: 1913852047.304348, mean_eps: 0.473927
 175508/300000: episode: 1539, duration: 0.697s, episode steps: 104, steps per second: 149, episode reward: -520.185, mean reward: -5.002 [-100.000,  1.707], mean action: 1.779 [0.000, 3.000],  loss: 14821570700478936.000000, mse: 4894116975566429184.000000, mean_q: 1904288692.923077, mean_eps: 0.473634
 175623/300000: episode: 1540, duration: 0.922s, episode steps: 115, steps per second: 125, episode reward: -126.197, mean reward: -1.097 [-100.000, 12.363], mean action: 1.722 [0.000, 3.000],  loss: 16924388983903240.000000, mse: 4886020948285709312.000000, mean_q: 1907359957.704348, mean_eps: 0.473305
 175710/300000: episode: 1541, duration: 0.786s, episode steps:  87, steps per second: 111, episode reward: -476.528, mean reward: -5.477 [-100.000,  0.425], mean action: 1.713 [0.000, 3.000],  loss: 6243294471390691.000000, mse: 5221066395308686336.000000, mean_q: 1945137696.367816, mean_eps: 0.473002
 175851/300000: episode: 1542, duration: 1.259s, episode steps: 141, steps per second: 112, episode reward: -516.647, mean reward: -3.664 [-100.000,  2.608], mean action: 1.780 [0.000, 3.000],  loss: 4318252584942105.500000, mse: 5278398648449196032.000000, mean_q: 1984741614.751773, mean_eps: 0.472660
 176146/300000: episode: 1543, duration: 2.612s, episode steps: 295, steps per second: 113, episode reward: -191.318, mean reward: -0.649 [-100.000, 14.666], mean action: 1.742 [0.000, 3.000],  loss: 14048108438347970.000000, mse: 5283792071245728768.000000, mean_q: 1974906751.132203, mean_eps: 0.472006
 176313/300000: episode: 1544, duration: 1.294s, episode steps: 167, steps per second: 129, episode reward: -412.945, mean reward: -2.473 [-100.000,  4.615], mean action: 1.713 [0.000, 3.000],  loss: 13767099528874094.000000, mse: 5184761751357276160.000000, mean_q: 1952858435.449102, mean_eps: 0.471313
 176397/300000: episode: 1545, duration: 0.740s, episode steps:  84, steps per second: 114, episode reward: -110.483, mean reward: -1.315 [-100.000,  5.952], mean action: 1.060 [0.000, 3.000],  loss: 4177066329312305.000000, mse: 5352413932270074880.000000, mean_q: 2003383689.142857, mean_eps: 0.470937
 176536/300000: episode: 1546, duration: 1.091s, episode steps: 139, steps per second: 127, episode reward: -368.199, mean reward: -2.649 [-100.000,  3.794], mean action: 1.669 [0.000, 3.000],  loss: 9096458174194312.000000, mse: 5231332957343117312.000000, mean_q: 1961797686.330935, mean_eps: 0.470602
 176801/300000: episode: 1547, duration: 2.392s, episode steps: 265, steps per second: 111, episode reward: -884.437, mean reward: -3.337 [-100.000,  1.829], mean action: 1.672 [0.000, 3.000],  loss: 7345900089981678.000000, mse: 5306618074006650880.000000, mean_q: 1979001895.124528, mean_eps: 0.469996
 176974/300000: episode: 1548, duration: 1.338s, episode steps: 173, steps per second: 129, episode reward: -354.006, mean reward: -2.046 [-100.000,  5.258], mean action: 1.757 [0.000, 3.000],  loss: 10131398550430216.000000, mse: 5410313411783355392.000000, mean_q: 1999668716.763006, mean_eps: 0.469339
 177085/300000: episode: 1549, duration: 0.813s, episode steps: 111, steps per second: 137, episode reward: -338.144, mean reward: -3.046 [-100.000,  2.163], mean action: 1.739 [0.000, 3.000],  loss: 10235831164494046.000000, mse: 5503964926272857088.000000, mean_q: 2004470767.855856, mean_eps: 0.468913
 177201/300000: episode: 1550, duration: 0.882s, episode steps: 116, steps per second: 132, episode reward: -610.532, mean reward: -5.263 [-100.000,  1.678], mean action: 1.828 [0.000, 3.000],  loss: 9154440719562540.000000, mse: 5742264724018502656.000000, mean_q: 2050185888.000000, mean_eps: 0.468573
 177311/300000: episode: 1551, duration: 0.861s, episode steps: 110, steps per second: 128, episode reward: -316.512, mean reward: -2.877 [-100.000,  1.783], mean action: 1.791 [0.000, 3.000],  loss: 13716201246775408.000000, mse: 5702332084037425152.000000, mean_q: 2050642866.036364, mean_eps: 0.468234
 177437/300000: episode: 1552, duration: 1.205s, episode steps: 126, steps per second: 105, episode reward: -436.173, mean reward: -3.462 [-100.000,  1.890], mean action: 1.802 [0.000, 3.000],  loss: 17145832923671730.000000, mse: 5749667212870777856.000000, mean_q: 2061556758.349206, mean_eps: 0.467879
 177523/300000: episode: 1553, duration: 0.666s, episode steps:  86, steps per second: 129, episode reward: -399.892, mean reward: -4.650 [-100.000,  0.129], mean action: 1.698 [0.000, 3.000],  loss: 24349922177879780.000000, mse: 6118215480646925312.000000, mean_q: 2106103066.790698, mean_eps: 0.467562
 177618/300000: episode: 1554, duration: 0.647s, episode steps:  95, steps per second: 147, episode reward: -55.087, mean reward: -0.580 [-100.000, 34.324], mean action: 1.200 [0.000, 3.000],  loss: 12083053747136102.000000, mse: 5498575369423147008.000000, mean_q: 2041017711.831579, mean_eps: 0.467290
 177726/300000: episode: 1555, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: -242.889, mean reward: -2.249 [-100.000,  1.798], mean action: 1.815 [0.000, 3.000],  loss: 5921432062209896.000000, mse: 5910378835228433408.000000, mean_q: 2100875605.333333, mean_eps: 0.466985
 177902/300000: episode: 1556, duration: 1.355s, episode steps: 176, steps per second: 130, episode reward: -438.937, mean reward: -2.494 [-100.000,  1.981], mean action: 1.761 [0.000, 3.000],  loss: 6224996103499404.000000, mse: 5769468023511259136.000000, mean_q: 2064879264.000000, mean_eps: 0.466560
 178021/300000: episode: 1557, duration: 0.885s, episode steps: 119, steps per second: 134, episode reward: -312.700, mean reward: -2.628 [-100.000,  1.698], mean action: 1.773 [0.000, 3.000],  loss: 6647042596530813.000000, mse: 6018380754713257984.000000, mean_q: 2108251991.126050, mean_eps: 0.466117
 178346/300000: episode: 1558, duration: 2.500s, episode steps: 325, steps per second: 130, episode reward: -1328.412, mean reward: -4.087 [-100.000,  2.828], mean action: 1.745 [0.000, 3.000],  loss: 6953051401721056.000000, mse: 6219284442454062080.000000, mean_q: 2163066347.913846, mean_eps: 0.465451
 178442/300000: episode: 1559, duration: 0.723s, episode steps:  96, steps per second: 133, episode reward: -93.271, mean reward: -0.972 [-100.000,  6.051], mean action: 1.490 [0.000, 3.000],  loss: 5381509612918101.000000, mse: 6334178935909821440.000000, mean_q: 2158464912.000000, mean_eps: 0.464819
 178559/300000: episode: 1560, duration: 0.834s, episode steps: 117, steps per second: 140, episode reward: -574.656, mean reward: -4.912 [-100.000,  1.498], mean action: 1.803 [0.000, 3.000],  loss: 15166716754839884.000000, mse: 6655052568731978752.000000, mean_q: 2213273377.914530, mean_eps: 0.464500
 178671/300000: episode: 1561, duration: 0.818s, episode steps: 112, steps per second: 137, episode reward: -510.675, mean reward: -4.560 [-100.000,  1.665], mean action: 1.893 [0.000, 3.000],  loss: 4873350274276206.000000, mse: 6656919946947198976.000000, mean_q: 2227188030.857143, mean_eps: 0.464157
 178795/300000: episode: 1562, duration: 0.854s, episode steps: 124, steps per second: 145, episode reward: -41.228, mean reward: -0.332 [-100.000,  7.270], mean action: 1.629 [0.000, 3.000],  loss: 7787597516302138.000000, mse: 6872227698817427456.000000, mean_q: 2259503349.677419, mean_eps: 0.463803
 178886/300000: episode: 1563, duration: 0.654s, episode steps:  91, steps per second: 139, episode reward: -515.712, mean reward: -5.667 [-100.000,  1.013], mean action: 1.791 [0.000, 3.000],  loss: 14420130849902276.000000, mse: 6800372487268122624.000000, mean_q: 2221370984.087912, mean_eps: 0.463480
 179094/300000: episode: 1564, duration: 1.456s, episode steps: 208, steps per second: 143, episode reward: -234.407, mean reward: -1.127 [-100.000,  7.666], mean action: 1.851 [0.000, 3.000],  loss: 15617505110100440.000000, mse: 6826931262377164800.000000, mean_q: 2254125260.307693, mean_eps: 0.463031
 179192/300000: episode: 1565, duration: 0.751s, episode steps:  98, steps per second: 130, episode reward: -342.975, mean reward: -3.500 [-100.000,  0.893], mean action: 1.724 [0.000, 3.000],  loss: 13713648276953296.000000, mse: 6826471516455856128.000000, mean_q: 2256385552.979592, mean_eps: 0.462572
 179267/300000: episode: 1566, duration: 0.589s, episode steps:  75, steps per second: 127, episode reward: -100.671, mean reward: -1.342 [-100.000,  8.398], mean action: 1.467 [0.000, 3.000],  loss: 11015519417894174.000000, mse: 7004211834236492800.000000, mean_q: 2272959054.506667, mean_eps: 0.462313
 179359/300000: episode: 1567, duration: 0.727s, episode steps:  92, steps per second: 127, episode reward: -420.404, mean reward: -4.570 [-100.000,  0.296], mean action: 1.837 [0.000, 3.000],  loss: 10356250597073252.000000, mse: 6641029896777532416.000000, mean_q: 2203918265.043478, mean_eps: 0.462062
 179513/300000: episode: 1568, duration: 1.281s, episode steps: 154, steps per second: 120, episode reward: -254.900, mean reward: -1.655 [-100.000,  4.066], mean action: 1.929 [0.000, 3.000],  loss: 10626049534945040.000000, mse: 6873210214600416256.000000, mean_q: 2252551893.610390, mean_eps: 0.461693
 179622/300000: episode: 1569, duration: 0.840s, episode steps: 109, steps per second: 130, episode reward: -403.947, mean reward: -3.706 [-100.000,  1.564], mean action: 1.716 [0.000, 3.000],  loss: 7569786508923143.000000, mse: 6841110924318149632.000000, mean_q: 2266893280.293578, mean_eps: 0.461299
 179795/300000: episode: 1570, duration: 1.365s, episode steps: 173, steps per second: 127, episode reward: -442.105, mean reward: -2.556 [-100.000,  2.442], mean action: 1.775 [0.000, 3.000],  loss: 7077262789772400.000000, mse: 7148346462525335552.000000, mean_q: 2313024369.942196, mean_eps: 0.460876
 179874/300000: episode: 1571, duration: 0.593s, episode steps:  79, steps per second: 133, episode reward: -142.376, mean reward: -1.802 [-100.000,  4.599], mean action: 1.557 [0.000, 3.000],  loss: 6750075953965808.000000, mse: 6885949310478300160.000000, mean_q: 2254095481.518987, mean_eps: 0.460498
 179970/300000: episode: 1572, duration: 0.685s, episode steps:  96, steps per second: 140, episode reward: -435.033, mean reward: -4.532 [-100.000,  1.249], mean action: 1.708 [0.000, 3.000],  loss: 32589799367202132.000000, mse: 7298978499266609152.000000, mean_q: 2320228506.666667, mean_eps: 0.460236
 180127/300000: episode: 1573, duration: 1.374s, episode steps: 157, steps per second: 114, episode reward: -509.928, mean reward: -3.248 [-100.000,  2.407], mean action: 1.777 [0.000, 3.000],  loss: 18599203255665580.000000, mse: 7018334799360150528.000000, mean_q: 2257827147.006370, mean_eps: 0.459856
 180263/300000: episode: 1574, duration: 1.708s, episode steps: 136, steps per second:  80, episode reward: -187.856, mean reward: -1.381 [-100.000,  3.748], mean action: 1.779 [0.000, 3.000],  loss: 9446734917734160.000000, mse: 7245898890767407104.000000, mean_q: 2313098481.882353, mean_eps: 0.459417
 180358/300000: episode: 1575, duration: 1.484s, episode steps:  95, steps per second:  64, episode reward: -157.600, mean reward: -1.659 [-100.000,  4.028], mean action: 1.768 [0.000, 3.000],  loss: 7789564910977886.000000, mse: 7156933142101302272.000000, mean_q: 2320238156.800000, mean_eps: 0.459070
 180448/300000: episode: 1576, duration: 1.033s, episode steps:  90, steps per second:  87, episode reward: -127.755, mean reward: -1.420 [-100.000,  6.735], mean action: 1.567 [0.000, 3.000],  loss: 5269472688192444.000000, mse: 6912110838970732544.000000, mean_q: 2254880109.511111, mean_eps: 0.458792
 180549/300000: episode: 1577, duration: 1.364s, episode steps: 101, steps per second:  74, episode reward: -138.110, mean reward: -1.367 [-100.000,  4.021], mean action: 1.881 [0.000, 3.000],  loss: 16733367066933512.000000, mse: 7280248069749439488.000000, mean_q: 2322153726.732673, mean_eps: 0.458506
 180653/300000: episode: 1578, duration: 1.003s, episode steps: 104, steps per second: 104, episode reward: -232.451, mean reward: -2.235 [-100.000,  2.286], mean action: 1.894 [0.000, 3.000],  loss: 10284872070251756.000000, mse: 7430709909684042752.000000, mean_q: 2333543347.692307, mean_eps: 0.458198
 180810/300000: episode: 1579, duration: 1.114s, episode steps: 157, steps per second: 141, episode reward: -505.064, mean reward: -3.217 [-100.000,  2.843], mean action: 1.669 [0.000, 3.000],  loss: 22334343606687964.000000, mse: 7487694746722325504.000000, mean_q: 2333533428.585987, mean_eps: 0.457807
 181015/300000: episode: 1580, duration: 1.750s, episode steps: 205, steps per second: 117, episode reward: -296.218, mean reward: -1.445 [-100.000, 16.112], mean action: 1.902 [0.000, 3.000],  loss: 10956225805889716.000000, mse: 7323879083831956480.000000, mean_q: 2323930991.141463, mean_eps: 0.457264
 181345/300000: episode: 1581, duration: 2.985s, episode steps: 330, steps per second: 111, episode reward: -57.695, mean reward: -0.175 [-100.000, 17.896], mean action: 1.752 [0.000, 3.000],  loss: 15051654846985606.000000, mse: 7566362950478872576.000000, mean_q: 2350134155.248485, mean_eps: 0.456462
 181558/300000: episode: 1582, duration: 2.190s, episode steps: 213, steps per second:  97, episode reward: -348.645, mean reward: -1.637 [-100.000,  4.466], mean action: 1.789 [0.000, 3.000],  loss: 16040696272421326.000000, mse: 7537107607205691392.000000, mean_q: 2342017231.924882, mean_eps: 0.455647
 181644/300000: episode: 1583, duration: 0.992s, episode steps:  86, steps per second:  87, episode reward: -275.599, mean reward: -3.205 [-100.000,  2.262], mean action: 1.663 [0.000, 3.000],  loss: 10091037527735558.000000, mse: 7676667052614739968.000000, mean_q: 2368165094.697674, mean_eps: 0.455199
 181780/300000: episode: 1584, duration: 1.368s, episode steps: 136, steps per second:  99, episode reward: -97.236, mean reward: -0.715 [-100.000,  9.956], mean action: 1.581 [0.000, 3.000],  loss: 8791232296594733.000000, mse: 7662511213663870976.000000, mean_q: 2378941806.117647, mean_eps: 0.454865
 182107/300000: episode: 1585, duration: 2.446s, episode steps: 327, steps per second: 134, episode reward: -522.156, mean reward: -1.597 [-100.000,  3.732], mean action: 1.786 [0.000, 3.000],  loss: 9151551584835754.000000, mse: 7885648352660331520.000000, mean_q: 2397507969.174312, mean_eps: 0.454171
 182204/300000: episode: 1586, duration: 0.754s, episode steps:  97, steps per second: 129, episode reward: -188.816, mean reward: -1.947 [-100.000,  3.673], mean action: 1.619 [0.000, 3.000],  loss: 10175668862771960.000000, mse: 7928298748886895616.000000, mean_q: 2405492121.072165, mean_eps: 0.453535
 182287/300000: episode: 1587, duration: 0.698s, episode steps:  83, steps per second: 119, episode reward: -314.790, mean reward: -3.793 [-100.000,  2.115], mean action: 2.012 [0.000, 3.000],  loss: 10003551361550460.000000, mse: 8131259032757426176.000000, mean_q: 2454682858.409638, mean_eps: 0.453265
 182405/300000: episode: 1588, duration: 0.943s, episode steps: 118, steps per second: 125, episode reward: -419.528, mean reward: -3.555 [-100.000,  1.082], mean action: 1.737 [0.000, 3.000],  loss: 15205278878056170.000000, mse: 8145740567009406976.000000, mean_q: 2425469397.694915, mean_eps: 0.452963
 182506/300000: episode: 1589, duration: 0.784s, episode steps: 101, steps per second: 129, episode reward: -380.134, mean reward: -3.764 [-100.000,  1.313], mean action: 1.851 [0.000, 3.000],  loss: 9698893967774304.000000, mse: 8016292103002078208.000000, mean_q: 2429714037.861386, mean_eps: 0.452635
 182614/300000: episode: 1590, duration: 0.874s, episode steps: 108, steps per second: 124, episode reward: -149.696, mean reward: -1.386 [-100.000, 25.282], mean action: 1.444 [0.000, 3.000],  loss: 9177101200290248.000000, mse: 7990787674819487744.000000, mean_q: 2421435453.629630, mean_eps: 0.452322
 182741/300000: episode: 1591, duration: 0.967s, episode steps: 127, steps per second: 131, episode reward: -271.695, mean reward: -2.139 [-100.000,  5.208], mean action: 1.638 [0.000, 3.000],  loss: 16350105071891302.000000, mse: 7849116558142039040.000000, mean_q: 2411064476.220472, mean_eps: 0.451969
 182847/300000: episode: 1592, duration: 0.790s, episode steps: 106, steps per second: 134, episode reward: -272.123, mean reward: -2.567 [-100.000,  1.001], mean action: 1.679 [0.000, 3.000],  loss: 9544141047629630.000000, mse: 8106217899631886336.000000, mean_q: 2432125398.943396, mean_eps: 0.451619
 183033/300000: episode: 1593, duration: 1.409s, episode steps: 186, steps per second: 132, episode reward: -520.237, mean reward: -2.797 [-100.000,  2.476], mean action: 1.763 [0.000, 3.000],  loss: 13682381163740304.000000, mse: 8363714697263006720.000000, mean_q: 2454560145.204301, mean_eps: 0.451182
 183173/300000: episode: 1594, duration: 1.031s, episode steps: 140, steps per second: 136, episode reward: -70.347, mean reward: -0.502 [-100.000,  7.743], mean action: 1.671 [0.000, 3.000],  loss: 10941291254835726.000000, mse: 8452587092371053568.000000, mean_q: 2496460430.628572, mean_eps: 0.450693
 183330/300000: episode: 1595, duration: 1.241s, episode steps: 157, steps per second: 126, episode reward: -51.208, mean reward: -0.326 [-100.000, 10.067], mean action: 1.841 [0.000, 3.000],  loss: 13025471786286896.000000, mse: 8399601544702720000.000000, mean_q: 2477472225.019108, mean_eps: 0.450247
 183509/300000: episode: 1596, duration: 1.343s, episode steps: 179, steps per second: 133, episode reward: -226.147, mean reward: -1.263 [-100.000,  4.526], mean action: 1.715 [0.000, 3.000],  loss: 12537516373907536.000000, mse: 8495471477100605440.000000, mean_q: 2503529665.072626, mean_eps: 0.449743
 183660/300000: episode: 1597, duration: 1.098s, episode steps: 151, steps per second: 138, episode reward: -141.986, mean reward: -0.940 [-100.000, 47.605], mean action: 1.629 [0.000, 3.000],  loss: 19519697527421564.000000, mse: 8776273389440224256.000000, mean_q: 2512481248.635762, mean_eps: 0.449248
 183798/300000: episode: 1598, duration: 0.991s, episode steps: 138, steps per second: 139, episode reward: -633.971, mean reward: -4.594 [-100.000,  1.851], mean action: 1.768 [0.000, 3.000],  loss: 13550769974827276.000000, mse: 8826204020573219840.000000, mean_q: 2556104069.565217, mean_eps: 0.448814
 183918/300000: episode: 1599, duration: 0.955s, episode steps: 120, steps per second: 126, episode reward: -378.706, mean reward: -3.156 [-100.000,  1.335], mean action: 1.792 [0.000, 3.000],  loss: 12844678769890646.000000, mse: 9121615103596936192.000000, mean_q: 2597544098.133333, mean_eps: 0.448428
 184063/300000: episode: 1600, duration: 1.202s, episode steps: 145, steps per second: 121, episode reward: -345.794, mean reward: -2.385 [-100.000,  2.649], mean action: 1.655 [0.000, 3.000],  loss: 11097421678603186.000000, mse: 9293423135914594304.000000, mean_q: 2614413369.379310, mean_eps: 0.448030
 184150/300000: episode: 1601, duration: 1.019s, episode steps:  87, steps per second:  85, episode reward: -408.454, mean reward: -4.695 [-100.000,  1.055], mean action: 1.690 [0.000, 3.000],  loss: 19007829952601828.000000, mse: 9346322011556917248.000000, mean_q: 2604170213.517241, mean_eps: 0.447682
 184239/300000: episode: 1602, duration: 0.909s, episode steps:  89, steps per second:  98, episode reward: -276.917, mean reward: -3.111 [-100.000,  1.558], mean action: 1.787 [0.000, 3.000],  loss: 14989791650107898.000000, mse: 9278082475338459136.000000, mean_q: 2617948344.089888, mean_eps: 0.447418
 184341/300000: episode: 1603, duration: 0.871s, episode steps: 102, steps per second: 117, episode reward: -333.182, mean reward: -3.266 [-100.000,  1.765], mean action: 1.725 [0.000, 3.000],  loss: 21596366648279684.000000, mse: 9237136197710409728.000000, mean_q: 2626906665.411765, mean_eps: 0.447132
 184472/300000: episode: 1604, duration: 1.113s, episode steps: 131, steps per second: 118, episode reward: -110.403, mean reward: -0.843 [-100.000, 10.754], mean action: 1.557 [0.000, 3.000],  loss: 27498983662977276.000000, mse: 9250526587397165056.000000, mean_q: 2619257422.167939, mean_eps: 0.446782
 184715/300000: episode: 1605, duration: 1.983s, episode steps: 243, steps per second: 123, episode reward: -769.758, mean reward: -3.168 [-100.000,  2.368], mean action: 1.786 [0.000, 3.000],  loss: 12902577213918036.000000, mse: 9560248411374077952.000000, mean_q: 2652109850.864198, mean_eps: 0.446221
 185099/300000: episode: 1606, duration: 2.690s, episode steps: 384, steps per second: 143, episode reward: -1171.829, mean reward: -3.052 [-100.000,  2.430], mean action: 1.732 [0.000, 3.000],  loss: 19152113407535788.000000, mse: 10147312682971518976.000000, mean_q: 2718056931.000000, mean_eps: 0.445280
 185189/300000: episode: 1607, duration: 0.601s, episode steps:  90, steps per second: 150, episode reward: -429.646, mean reward: -4.774 [-100.000,  0.797], mean action: 1.800 [0.000, 3.000],  loss: 25713905295617776.000000, mse: 10636467349553885184.000000, mean_q: 2788960345.600000, mean_eps: 0.444570
 185395/300000: episode: 1608, duration: 1.401s, episode steps: 206, steps per second: 147, episode reward: -309.549, mean reward: -1.503 [-100.000,  4.871], mean action: 1.874 [0.000, 3.000],  loss: 23815323026469300.000000, mse: 10480213295843012608.000000, mean_q: 2776705201.708738, mean_eps: 0.444125
 185497/300000: episode: 1609, duration: 0.816s, episode steps: 102, steps per second: 125, episode reward: -317.215, mean reward: -3.110 [-100.000,  1.488], mean action: 1.804 [0.000, 3.000],  loss: 21864159721127292.000000, mse: 10904451761630238720.000000, mean_q: 2837871205.647059, mean_eps: 0.443663
 185609/300000: episode: 1610, duration: 0.761s, episode steps: 112, steps per second: 147, episode reward: -195.270, mean reward: -1.743 [-100.000,  4.531], mean action: 1.545 [0.000, 3.000],  loss: 24912427256943176.000000, mse: 10689255502626273280.000000, mean_q: 2795133241.142857, mean_eps: 0.443342
 185772/300000: episode: 1611, duration: 1.278s, episode steps: 163, steps per second: 127, episode reward: -411.069, mean reward: -2.522 [-100.000,  2.704], mean action: 1.755 [0.000, 3.000],  loss: 14175241456286738.000000, mse: 10938106114185965568.000000, mean_q: 2837039337.226994, mean_eps: 0.442930
 185875/300000: episode: 1612, duration: 0.750s, episode steps: 103, steps per second: 137, episode reward: -512.217, mean reward: -4.973 [-100.000,  1.849], mean action: 1.738 [0.000, 3.000],  loss: 7864749830319293.000000, mse: 11310062363271139328.000000, mean_q: 2876156813.669903, mean_eps: 0.442531
 185966/300000: episode: 1613, duration: 0.741s, episode steps:  91, steps per second: 123, episode reward: -371.862, mean reward: -4.086 [-100.000,  2.970], mean action: 1.604 [0.000, 3.000],  loss: 22642971654234564.000000, mse: 11134262929863026688.000000, mean_q: 2836765056.000000, mean_eps: 0.442240
 186066/300000: episode: 1614, duration: 0.717s, episode steps: 100, steps per second: 139, episode reward: -112.703, mean reward: -1.127 [-100.000, 18.455], mean action: 1.500 [0.000, 3.000],  loss: 11276211912221656.000000, mse: 11162448610431461376.000000, mean_q: 2869311866.880000, mean_eps: 0.441954
 186180/300000: episode: 1615, duration: 0.900s, episode steps: 114, steps per second: 127, episode reward: -270.770, mean reward: -2.375 [-100.000,  2.567], mean action: 1.833 [0.000, 3.000],  loss: 22785226417414576.000000, mse: 11095212865032042496.000000, mean_q: 2823414488.701755, mean_eps: 0.441633
 186351/300000: episode: 1616, duration: 1.262s, episode steps: 171, steps per second: 136, episode reward: -219.970, mean reward: -1.286 [-100.000,  5.191], mean action: 1.942 [0.000, 3.000],  loss: 11070496601230426.000000, mse: 11466712719252373504.000000, mean_q: 2878645502.502924, mean_eps: 0.441205
 186497/300000: episode: 1617, duration: 1.016s, episode steps: 146, steps per second: 144, episode reward: -300.507, mean reward: -2.058 [-100.000,  4.199], mean action: 1.705 [0.000, 3.000],  loss: 31275735911533640.000000, mse: 11219783668313835520.000000, mean_q: 2850644935.890411, mean_eps: 0.440729
 186643/300000: episode: 1618, duration: 1.029s, episode steps: 146, steps per second: 142, episode reward: -659.575, mean reward: -4.518 [-100.000,  2.511], mean action: 1.856 [0.000, 3.000],  loss: 23721062703961356.000000, mse: 11250530321102145536.000000, mean_q: 2830286361.424657, mean_eps: 0.440292
 186750/300000: episode: 1619, duration: 0.771s, episode steps: 107, steps per second: 139, episode reward: -507.592, mean reward: -4.744 [-100.000,  1.041], mean action: 1.757 [0.000, 3.000],  loss: 51732823055612880.000000, mse: 11620491918395348992.000000, mean_q: 2892061473.495327, mean_eps: 0.439912
 186880/300000: episode: 1620, duration: 0.961s, episode steps: 130, steps per second: 135, episode reward: -491.217, mean reward: -3.779 [-100.000,  1.052], mean action: 1.892 [0.000, 3.000],  loss: 29786163519733004.000000, mse: 11488313484164218880.000000, mean_q: 2890745927.876923, mean_eps: 0.439557
 186994/300000: episode: 1621, duration: 0.792s, episode steps: 114, steps per second: 144, episode reward: -487.869, mean reward: -4.280 [-100.000,  1.387], mean action: 1.772 [0.000, 3.000],  loss: 37844360364547088.000000, mse: 11252074893950320640.000000, mean_q: 2860403996.070176, mean_eps: 0.439191
 187156/300000: episode: 1622, duration: 1.137s, episode steps: 162, steps per second: 142, episode reward: -170.742, mean reward: -1.054 [-100.000,  4.717], mean action: 1.907 [0.000, 3.000],  loss: 23866236928058256.000000, mse: 11871923203113060352.000000, mean_q: 2937763041.975308, mean_eps: 0.438777
 187235/300000: episode: 1623, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -63.876, mean reward: -0.809 [-100.000,  9.848], mean action: 1.759 [0.000, 3.000],  loss: 11557752387659284.000000, mse: 12269149099509323776.000000, mean_q: 2934866969.924051, mean_eps: 0.438415
 187472/300000: episode: 1624, duration: 1.661s, episode steps: 237, steps per second: 143, episode reward: -685.422, mean reward: -2.892 [-100.000,  5.052], mean action: 1.806 [0.000, 3.000],  loss: 36607478445398712.000000, mse: 11863057158968647680.000000, mean_q: 2906817911.898734, mean_eps: 0.437941
 187577/300000: episode: 1625, duration: 0.742s, episode steps: 105, steps per second: 142, episode reward: -237.289, mean reward: -2.260 [-100.000,  2.984], mean action: 1.600 [0.000, 3.000],  loss: 17336116696602566.000000, mse: 11839438199359754240.000000, mean_q: 2926763688.228571, mean_eps: 0.437428
 187682/300000: episode: 1626, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -298.689, mean reward: -2.845 [-100.000,  2.159], mean action: 1.590 [0.000, 3.000],  loss: 24465914279795720.000000, mse: 12064253828753682432.000000, mean_q: 2983507687.619048, mean_eps: 0.437113
 187769/300000: episode: 1627, duration: 0.635s, episode steps:  87, steps per second: 137, episode reward: -50.204, mean reward: -0.577 [-100.000, 22.742], mean action: 1.828 [0.000, 3.000],  loss: 53842817621287568.000000, mse: 11927199191065434112.000000, mean_q: 2916096768.000000, mean_eps: 0.436825
 187843/300000: episode: 1628, duration: 0.634s, episode steps:  74, steps per second: 117, episode reward: -371.493, mean reward: -5.020 [-100.000,  0.990], mean action: 1.932 [0.000, 3.000],  loss: 6486466686608467.000000, mse: 11874350081901649920.000000, mean_q: 2922697122.594594, mean_eps: 0.436583
 187939/300000: episode: 1629, duration: 0.713s, episode steps:  96, steps per second: 135, episode reward: -53.298, mean reward: -0.555 [-100.000, 11.073], mean action: 1.708 [0.000, 3.000],  loss: 32469159623262208.000000, mse: 12966063071812036608.000000, mean_q: 3057452898.666667, mean_eps: 0.436328
 188052/300000: episode: 1630, duration: 0.831s, episode steps: 113, steps per second: 136, episode reward: -218.961, mean reward: -1.938 [-100.000,  6.876], mean action: 1.973 [0.000, 3.000],  loss: 24599563329951836.000000, mse: 12320202932675213312.000000, mean_q: 2972068691.823009, mean_eps: 0.436015
 188168/300000: episode: 1631, duration: 0.795s, episode steps: 116, steps per second: 146, episode reward: -163.440, mean reward: -1.409 [-100.000, 12.180], mean action: 1.707 [0.000, 3.000],  loss: 18098737779111724.000000, mse: 11878666291265314816.000000, mean_q: 2912114996.965517, mean_eps: 0.435671
 188263/300000: episode: 1632, duration: 0.685s, episode steps:  95, steps per second: 139, episode reward: -515.781, mean reward: -5.429 [-100.000,  0.118], mean action: 1.674 [0.000, 3.000],  loss: 38862616387177680.000000, mse: 12142691054075684864.000000, mean_q: 2924912291.031579, mean_eps: 0.435355
 188366/300000: episode: 1633, duration: 0.743s, episode steps: 103, steps per second: 139, episode reward: -208.524, mean reward: -2.025 [-100.000,  1.270], mean action: 1.757 [0.000, 3.000],  loss: 23576690454086108.000000, mse: 11965574964611092480.000000, mean_q: 2915941235.572815, mean_eps: 0.435058
 188551/300000: episode: 1634, duration: 1.295s, episode steps: 185, steps per second: 143, episode reward: -347.181, mean reward: -1.877 [-100.000,  2.516], mean action: 1.762 [0.000, 3.000],  loss: 27122573936982468.000000, mse: 12203897469168928768.000000, mean_q: 2926894789.881081, mean_eps: 0.434626
 188647/300000: episode: 1635, duration: 0.720s, episode steps:  96, steps per second: 133, episode reward: -416.193, mean reward: -4.335 [-100.000,  0.967], mean action: 1.760 [0.000, 3.000],  loss: 23043662076881580.000000, mse: 11813172410796998656.000000, mean_q: 2883798246.666667, mean_eps: 0.434205
 188733/300000: episode: 1636, duration: 0.626s, episode steps:  86, steps per second: 137, episode reward: -493.426, mean reward: -5.738 [-100.000, -0.239], mean action: 1.756 [0.000, 3.000],  loss: 27581104224493376.000000, mse: 12683379650850668544.000000, mean_q: 2984610173.023256, mean_eps: 0.433932
 188896/300000: episode: 1637, duration: 1.179s, episode steps: 163, steps per second: 138, episode reward: -635.137, mean reward: -3.897 [-100.000,  1.534], mean action: 1.730 [0.000, 3.000],  loss: 11913383445937070.000000, mse: 12417573077993107456.000000, mean_q: 2980006537.423313, mean_eps: 0.433558
 189020/300000: episode: 1638, duration: 0.883s, episode steps: 124, steps per second: 140, episode reward: -304.907, mean reward: -2.459 [-100.000,  1.678], mean action: 1.758 [0.000, 3.000],  loss: 26686922239905924.000000, mse: 12783495149786742784.000000, mean_q: 3023345344.000000, mean_eps: 0.433127
 189108/300000: episode: 1639, duration: 0.609s, episode steps:  88, steps per second: 145, episode reward: -303.610, mean reward: -3.450 [-100.000,  0.875], mean action: 1.807 [0.000, 3.000],  loss: 9312708389526808.000000, mse: 13031572409818284032.000000, mean_q: 3041259418.181818, mean_eps: 0.432810
 189337/300000: episode: 1640, duration: 1.661s, episode steps: 229, steps per second: 138, episode reward: -313.453, mean reward: -1.369 [-100.000,  3.553], mean action: 1.795 [0.000, 3.000],  loss: 21430242162783656.000000, mse: 12987337992031008768.000000, mean_q: 3051399541.938865, mean_eps: 0.432334
 189415/300000: episode: 1641, duration: 0.526s, episode steps:  78, steps per second: 148, episode reward: -458.149, mean reward: -5.874 [-100.000,  0.443], mean action: 1.821 [0.000, 3.000],  loss: 13047274781977258.000000, mse: 13081376982735540224.000000, mean_q: 3061272818.871795, mean_eps: 0.431874
 189571/300000: episode: 1642, duration: 1.105s, episode steps: 156, steps per second: 141, episode reward: -505.487, mean reward: -3.240 [-100.000,  1.860], mean action: 1.744 [0.000, 3.000],  loss: 18557273128439072.000000, mse: 13122959219162234880.000000, mean_q: 3083257705.025641, mean_eps: 0.431522
 189729/300000: episode: 1643, duration: 1.097s, episode steps: 158, steps per second: 144, episode reward: -126.954, mean reward: -0.804 [-100.000, 13.981], mean action: 1.823 [0.000, 3.000],  loss: 28066633868487616.000000, mse: 13317918850213945344.000000, mean_q: 3117601435.544304, mean_eps: 0.431052
 189844/300000: episode: 1644, duration: 0.830s, episode steps: 115, steps per second: 138, episode reward: -462.841, mean reward: -4.025 [-100.000,  1.198], mean action: 1.774 [0.000, 3.000],  loss: 25927803057706276.000000, mse: 13752040170802163712.000000, mean_q: 3147670959.860869, mean_eps: 0.430642
 189960/300000: episode: 1645, duration: 0.810s, episode steps: 116, steps per second: 143, episode reward: -406.277, mean reward: -3.502 [-100.000,  1.512], mean action: 1.853 [0.000, 3.000],  loss: 27070635002569552.000000, mse: 13592157889550071808.000000, mean_q: 3136474765.241379, mean_eps: 0.430296
 190343/300000: episode: 1646, duration: 2.720s, episode steps: 383, steps per second: 141, episode reward: -533.000, mean reward: -1.392 [-100.000,  9.826], mean action: 1.864 [0.000, 3.000],  loss: 24097061791781344.000000, mse: 13556208455403151360.000000, mean_q: 3137318285.702350, mean_eps: 0.429547
 190447/300000: episode: 1647, duration: 0.784s, episode steps: 104, steps per second: 133, episode reward: -19.005, mean reward: -0.183 [-100.000, 15.148], mean action: 1.673 [0.000, 3.000],  loss: 19098926699894940.000000, mse: 13838388079544176640.000000, mean_q: 3182590555.076923, mean_eps: 0.428816
 190611/300000: episode: 1648, duration: 1.168s, episode steps: 164, steps per second: 140, episode reward: -699.376, mean reward: -4.264 [-100.000,  1.467], mean action: 1.774 [0.000, 3.000],  loss: 21685496065396636.000000, mse: 14122619502638649344.000000, mean_q: 3206258065.170732, mean_eps: 0.428415
 190702/300000: episode: 1649, duration: 0.675s, episode steps:  91, steps per second: 135, episode reward: -51.294, mean reward: -0.564 [-100.000, 19.381], mean action: 1.659 [0.000, 3.000],  loss: 19592063436314780.000000, mse: 14556353970374365184.000000, mean_q: 3204187996.835165, mean_eps: 0.428032
 190882/300000: episode: 1650, duration: 1.332s, episode steps: 180, steps per second: 135, episode reward: -648.590, mean reward: -3.603 [-100.000,  4.049], mean action: 1.728 [0.000, 3.000],  loss: 24335814262638364.000000, mse: 14421549859851317248.000000, mean_q: 3223371504.355556, mean_eps: 0.427625
 190993/300000: episode: 1651, duration: 0.792s, episode steps: 111, steps per second: 140, episode reward: -335.313, mean reward: -3.021 [-100.000,  3.215], mean action: 2.072 [0.000, 3.000],  loss: 14191843802847878.000000, mse: 14631480541437972480.000000, mean_q: 3247819600.720721, mean_eps: 0.427189
 191172/300000: episode: 1652, duration: 1.343s, episode steps: 179, steps per second: 133, episode reward: -291.729, mean reward: -1.630 [-100.000,  5.959], mean action: 1.765 [0.000, 3.000],  loss: 28185264717662904.000000, mse: 14476603968415172608.000000, mean_q: 3229139172.826816, mean_eps: 0.426754
 191273/300000: episode: 1653, duration: 0.779s, episode steps: 101, steps per second: 130, episode reward: -58.931, mean reward: -0.583 [-100.000, 10.296], mean action: 1.901 [0.000, 3.000],  loss: 19448486375350352.000000, mse: 14610393751626713088.000000, mean_q: 3251070142.099010, mean_eps: 0.426334
 191417/300000: episode: 1654, duration: 1.120s, episode steps: 144, steps per second: 129, episode reward: -503.020, mean reward: -3.493 [-100.000,  1.393], mean action: 1.833 [0.000, 3.000],  loss: 38396961487031864.000000, mse: 15246554397088182272.000000, mean_q: 3349530949.333333, mean_eps: 0.425967
 191547/300000: episode: 1655, duration: 0.906s, episode steps: 130, steps per second: 143, episode reward: -219.512, mean reward: -1.689 [-100.000,  1.244], mean action: 1.769 [0.000, 3.000],  loss: 24681145485153452.000000, mse: 15140072301097994240.000000, mean_q: 3331425774.276923, mean_eps: 0.425555
 191665/300000: episode: 1656, duration: 0.887s, episode steps: 118, steps per second: 133, episode reward: -232.236, mean reward: -1.968 [-100.000,  3.919], mean action: 1.585 [0.000, 3.000],  loss: 22511326191792612.000000, mse: 15345366491943501824.000000, mean_q: 3331011438.644068, mean_eps: 0.425183
 191871/300000: episode: 1657, duration: 1.573s, episode steps: 206, steps per second: 131, episode reward: -824.443, mean reward: -4.002 [-100.000,  2.202], mean action: 1.820 [0.000, 3.000],  loss: 18301438043736404.000000, mse: 15273545189549281280.000000, mean_q: 3335585370.097087, mean_eps: 0.424698
 191958/300000: episode: 1658, duration: 0.867s, episode steps:  87, steps per second: 100, episode reward: -361.893, mean reward: -4.160 [-100.000,  1.044], mean action: 1.678 [0.000, 3.000],  loss: 29480156459662984.000000, mse: 15952068176124473344.000000, mean_q: 3433259770.114943, mean_eps: 0.424258
 192075/300000: episode: 1659, duration: 1.047s, episode steps: 117, steps per second: 112, episode reward: -203.335, mean reward: -1.738 [-100.000,  3.501], mean action: 1.641 [0.000, 3.000],  loss: 30902235610711416.000000, mse: 16112931255421868032.000000, mean_q: 3430746781.538462, mean_eps: 0.423952
 192171/300000: episode: 1660, duration: 0.747s, episode steps:  96, steps per second: 128, episode reward: -282.356, mean reward: -2.941 [-100.000,  1.283], mean action: 1.781 [0.000, 3.000],  loss: 33170324392509440.000000, mse: 16240174795410024448.000000, mean_q: 3459972653.333333, mean_eps: 0.423633
 192269/300000: episode: 1661, duration: 0.740s, episode steps:  98, steps per second: 132, episode reward: -232.076, mean reward: -2.368 [-100.000,  2.251], mean action: 1.867 [0.000, 3.000],  loss: 8752622600898184.000000, mse: 16223540397707773952.000000, mean_q: 3473357714.285714, mean_eps: 0.423341
 192358/300000: episode: 1662, duration: 0.676s, episode steps:  89, steps per second: 132, episode reward: -60.943, mean reward: -0.685 [-100.000,  9.495], mean action: 1.775 [0.000, 3.000],  loss: 20716093743568644.000000, mse: 15566389437456900096.000000, mean_q: 3347239556.314607, mean_eps: 0.423061
 192472/300000: episode: 1663, duration: 0.841s, episode steps: 114, steps per second: 136, episode reward: -316.972, mean reward: -2.780 [-100.000,  1.584], mean action: 1.789 [0.000, 3.000],  loss: 53316200674296960.000000, mse: 16801651815265576960.000000, mean_q: 3520549039.157895, mean_eps: 0.422756
 192584/300000: episode: 1664, duration: 0.853s, episode steps: 112, steps per second: 131, episode reward: -209.528, mean reward: -1.871 [-100.000,  4.583], mean action: 1.625 [0.000, 3.000],  loss: 39661607998980096.000000, mse: 16707579000444758016.000000, mean_q: 3520467305.142857, mean_eps: 0.422418
 192675/300000: episode: 1665, duration: 0.631s, episode steps:  91, steps per second: 144, episode reward: -517.292, mean reward: -5.685 [-100.000,  0.651], mean action: 1.703 [0.000, 3.000],  loss: 16146752650346496.000000, mse: 15900010946843490304.000000, mean_q: 3425755375.120879, mean_eps: 0.422113
 192881/300000: episode: 1666, duration: 1.595s, episode steps: 206, steps per second: 129, episode reward: -232.001, mean reward: -1.126 [-100.000,  6.210], mean action: 1.859 [0.000, 3.000],  loss: 21928810398924004.000000, mse: 16537584241285699584.000000, mean_q: 3458979940.660194, mean_eps: 0.421667
 192962/300000: episode: 1667, duration: 0.642s, episode steps:  81, steps per second: 126, episode reward: -456.765, mean reward: -5.639 [-100.000,  0.069], mean action: 1.877 [0.000, 3.000],  loss: 29616584023359284.000000, mse: 16140723873446066176.000000, mean_q: 3405761978.469136, mean_eps: 0.421237
 193056/300000: episode: 1668, duration: 0.766s, episode steps:  94, steps per second: 123, episode reward: -522.319, mean reward: -5.557 [-100.000,  1.159], mean action: 1.745 [0.000, 3.000],  loss: 28171388817880000.000000, mse: 17251838817107705856.000000, mean_q: 3562377156.085106, mean_eps: 0.420975
 193245/300000: episode: 1669, duration: 1.760s, episode steps: 189, steps per second: 107, episode reward: -556.532, mean reward: -2.945 [-100.000,  2.753], mean action: 1.799 [0.000, 3.000],  loss: 25441796220297868.000000, mse: 16234382194651820032.000000, mean_q: 3454878122.666667, mean_eps: 0.420550
 193406/300000: episode: 1670, duration: 1.318s, episode steps: 161, steps per second: 122, episode reward: -432.015, mean reward: -2.683 [-100.000,  2.130], mean action: 1.764 [0.000, 3.000],  loss: 27691307524646748.000000, mse: 17414279924020105216.000000, mean_q: 3574948221.614907, mean_eps: 0.420025
 193613/300000: episode: 1671, duration: 1.655s, episode steps: 207, steps per second: 125, episode reward: -538.965, mean reward: -2.604 [-100.000,  4.075], mean action: 1.754 [0.000, 3.000],  loss: 22217152156073668.000000, mse: 17447290171214520320.000000, mean_q: 3561968460.676329, mean_eps: 0.419473
 193712/300000: episode: 1672, duration: 0.783s, episode steps:  99, steps per second: 126, episode reward: -290.112, mean reward: -2.930 [-100.000,  3.381], mean action: 1.495 [0.000, 3.000],  loss: 48893504865523032.000000, mse: 17518226950526525440.000000, mean_q: 3599640785.454545, mean_eps: 0.419014
 193821/300000: episode: 1673, duration: 0.832s, episode steps: 109, steps per second: 131, episode reward: -41.462, mean reward: -0.380 [-100.000, 16.170], mean action: 1.734 [0.000, 3.000],  loss: 56350512670599000.000000, mse: 17378338578086023168.000000, mean_q: 3562915612.183486, mean_eps: 0.418702
 193928/300000: episode: 1674, duration: 0.729s, episode steps: 107, steps per second: 147, episode reward: -455.587, mean reward: -4.258 [-100.000,  1.713], mean action: 1.701 [0.000, 3.000],  loss: 50510668102337432.000000, mse: 16886814572133801984.000000, mean_q: 3533461903.551402, mean_eps: 0.418378
 194038/300000: episode: 1675, duration: 0.788s, episode steps: 110, steps per second: 140, episode reward: -473.855, mean reward: -4.308 [-100.000,  1.232], mean action: 1.827 [0.000, 3.000],  loss: 24085349531686132.000000, mse: 16918089174337421312.000000, mean_q: 3514929945.600000, mean_eps: 0.418053
 194127/300000: episode: 1676, duration: 0.650s, episode steps:  89, steps per second: 137, episode reward: -359.606, mean reward: -4.041 [-100.000,  0.598], mean action: 1.820 [0.000, 3.000],  loss: 63007238452627432.000000, mse: 17413634905756057600.000000, mean_q: 3562850643.415730, mean_eps: 0.417754
 194355/300000: episode: 1677, duration: 1.707s, episode steps: 228, steps per second: 134, episode reward: -358.798, mean reward: -1.574 [-100.000,  7.440], mean action: 1.697 [0.000, 3.000],  loss: 31839961715581936.000000, mse: 17537831675040845824.000000, mean_q: 3576769571.929824, mean_eps: 0.417278
 194449/300000: episode: 1678, duration: 0.695s, episode steps:  94, steps per second: 135, episode reward: -259.752, mean reward: -2.763 [-100.000,  0.946], mean action: 1.574 [0.000, 3.000],  loss: 27001508391668104.000000, mse: 18170895590421530624.000000, mean_q: 3668248031.319149, mean_eps: 0.416796
 194543/300000: episode: 1679, duration: 0.754s, episode steps:  94, steps per second: 125, episode reward: -519.030, mean reward: -5.522 [-100.000,  0.837], mean action: 1.809 [0.000, 3.000],  loss: 21966792820048916.000000, mse: 18828498616496582656.000000, mean_q: 3695502564.765957, mean_eps: 0.416514
 194648/300000: episode: 1680, duration: 0.830s, episode steps: 105, steps per second: 126, episode reward: -1.104, mean reward: -0.011 [-100.000, 17.561], mean action: 1.724 [0.000, 3.000],  loss: 25933841183274560.000000, mse: 18211409107662280704.000000, mean_q: 3630050301.561905, mean_eps: 0.416215
 194971/300000: episode: 1681, duration: 2.646s, episode steps: 323, steps per second: 122, episode reward: -1367.719, mean reward: -4.234 [-100.000,  2.111], mean action: 1.728 [0.000, 3.000],  loss: 32027812182973628.000000, mse: 19062026501708615680.000000, mean_q: 3722834479.554180, mean_eps: 0.415573
 195091/300000: episode: 1682, duration: 0.880s, episode steps: 120, steps per second: 136, episode reward: -104.287, mean reward: -0.869 [-100.000, 20.402], mean action: 1.825 [0.000, 3.000],  loss: 21671164774728772.000000, mse: 19246322352632713216.000000, mean_q: 3719510596.266667, mean_eps: 0.414908
 195434/300000: episode: 1683, duration: 2.571s, episode steps: 343, steps per second: 133, episode reward: -128.382, mean reward: -0.374 [-100.000, 11.841], mean action: 1.822 [0.000, 3.000],  loss: 32099821760300968.000000, mse: 19796672545650716672.000000, mean_q: 3800684143.953353, mean_eps: 0.414214
 195528/300000: episode: 1684, duration: 0.701s, episode steps:  94, steps per second: 134, episode reward: -366.929, mean reward: -3.903 [-100.000,  0.682], mean action: 1.723 [0.000, 3.000],  loss: 44666909007315640.000000, mse: 19974526920104005632.000000, mean_q: 3772011803.234043, mean_eps: 0.413558
 195679/300000: episode: 1685, duration: 1.049s, episode steps: 151, steps per second: 144, episode reward: -740.528, mean reward: -4.904 [-100.000,  1.937], mean action: 1.881 [0.000, 3.000],  loss: 28064624504704184.000000, mse: 19705269141050159104.000000, mean_q: 3775153848.794702, mean_eps: 0.413191
 195817/300000: episode: 1686, duration: 0.967s, episode steps: 138, steps per second: 143, episode reward: -259.150, mean reward: -1.878 [-100.000,  4.599], mean action: 1.957 [0.000, 3.000],  loss: 27972029766100488.000000, mse: 20531743105333612544.000000, mean_q: 3833497499.826087, mean_eps: 0.412757
 195882/300000: episode: 1687, duration: 0.450s, episode steps:  65, steps per second: 144, episode reward: -48.236, mean reward: -0.742 [-100.000, 25.107], mean action: 1.723 [0.000, 3.000],  loss: 22856279189364608.000000, mse: 20874334753715183616.000000, mean_q: 3904541361.230769, mean_eps: 0.412453
 196007/300000: episode: 1688, duration: 0.893s, episode steps: 125, steps per second: 140, episode reward: -276.492, mean reward: -2.212 [-100.000,  7.312], mean action: 1.640 [0.000, 3.000],  loss: 47324428127598280.000000, mse: 20725296958031241216.000000, mean_q: 3868346122.240000, mean_eps: 0.412168
 196261/300000: episode: 1689, duration: 1.861s, episode steps: 254, steps per second: 137, episode reward: -420.614, mean reward: -1.656 [-100.000,  4.484], mean action: 1.776 [0.000, 3.000],  loss: 24001262185179860.000000, mse: 20252827703360090112.000000, mean_q: 3790562409.826772, mean_eps: 0.411600
 196379/300000: episode: 1690, duration: 0.935s, episode steps: 118, steps per second: 126, episode reward: -146.964, mean reward: -1.245 [-100.000, 71.329], mean action: 1.881 [0.000, 3.000],  loss: 48157043644516800.000000, mse: 21780748931930644480.000000, mean_q: 3996140841.220339, mean_eps: 0.411041
 196476/300000: episode: 1691, duration: 0.690s, episode steps:  97, steps per second: 141, episode reward: -438.523, mean reward: -4.521 [-100.000,  0.635], mean action: 1.753 [0.000, 3.000],  loss: 35975663319909704.000000, mse: 21039217431617941504.000000, mean_q: 3892821564.701031, mean_eps: 0.410719
 196624/300000: episode: 1692, duration: 1.082s, episode steps: 148, steps per second: 137, episode reward: -281.855, mean reward: -1.904 [-100.000, 16.407], mean action: 1.777 [0.000, 3.000],  loss: 30189603904687576.000000, mse: 21303045038993436672.000000, mean_q: 3907037393.297297, mean_eps: 0.410351
 196741/300000: episode: 1693, duration: 0.869s, episode steps: 117, steps per second: 135, episode reward: -221.697, mean reward: -1.895 [-100.000,  6.866], mean action: 1.889 [0.000, 3.000],  loss: 32455056411642188.000000, mse: 20450423798797246464.000000, mean_q: 3829851076.923077, mean_eps: 0.409954
 196847/300000: episode: 1694, duration: 0.745s, episode steps: 106, steps per second: 142, episode reward: -316.223, mean reward: -2.983 [-100.000,  1.174], mean action: 1.802 [0.000, 3.000],  loss: 39078977175893280.000000, mse: 21588451039184580608.000000, mean_q: 3945724005.433962, mean_eps: 0.409619
 197024/300000: episode: 1695, duration: 1.339s, episode steps: 177, steps per second: 132, episode reward: -716.764, mean reward: -4.050 [-100.000,  2.222], mean action: 1.876 [0.000, 3.000],  loss: 51975066513645888.000000, mse: 21628843905859727360.000000, mean_q: 3909861822.915254, mean_eps: 0.409195
 197131/300000: episode: 1696, duration: 0.745s, episode steps: 107, steps per second: 144, episode reward: -314.468, mean reward: -2.939 [-100.000,  1.473], mean action: 1.897 [0.000, 3.000],  loss: 48479219275221544.000000, mse: 21848723096968069120.000000, mean_q: 3976950494.504673, mean_eps: 0.408769
 197203/300000: episode: 1697, duration: 0.491s, episode steps:  72, steps per second: 147, episode reward: -44.118, mean reward: -0.613 [-100.000, 13.356], mean action: 2.000 [0.000, 3.000],  loss: 56226351310671416.000000, mse: 21618699681390161920.000000, mean_q: 3967401187.555555, mean_eps: 0.408500
 197295/300000: episode: 1698, duration: 0.677s, episode steps:  92, steps per second: 136, episode reward: -518.113, mean reward: -5.632 [-100.000,  0.994], mean action: 1.826 [0.000, 3.000],  loss: 51653513524748648.000000, mse: 22021487010124468224.000000, mean_q: 3968384584.347826, mean_eps: 0.408255
 197463/300000: episode: 1699, duration: 1.148s, episode steps: 168, steps per second: 146, episode reward: -354.621, mean reward: -2.111 [-100.000,  3.600], mean action: 1.738 [0.000, 3.000],  loss: 28080708069496928.000000, mse: 20951418007684583424.000000, mean_q: 3874500182.857143, mean_eps: 0.407864
 197536/300000: episode: 1700, duration: 0.497s, episode steps:  73, steps per second: 147, episode reward: -80.150, mean reward: -1.098 [-100.000, 11.227], mean action: 1.753 [0.000, 3.000],  loss: 26481373924226428.000000, mse: 21530204602101719040.000000, mean_q: 3936730269.808219, mean_eps: 0.407503
 197697/300000: episode: 1701, duration: 1.127s, episode steps: 161, steps per second: 143, episode reward: -432.981, mean reward: -2.689 [-100.000,  4.280], mean action: 1.764 [0.000, 3.000],  loss: 45828445228608032.000000, mse: 22407501998746329088.000000, mean_q: 4008671840.993789, mean_eps: 0.407152
 197789/300000: episode: 1702, duration: 0.645s, episode steps:  92, steps per second: 143, episode reward: -112.947, mean reward: -1.228 [-100.000, 10.899], mean action: 1.696 [0.000, 3.000],  loss: 34856288016149636.000000, mse: 22606454803404128256.000000, mean_q: 4051940396.521739, mean_eps: 0.406772
 197924/300000: episode: 1703, duration: 0.985s, episode steps: 135, steps per second: 137, episode reward:  0.040, mean reward:  0.000 [-100.000, 17.305], mean action: 1.807 [0.000, 3.000],  loss: 41148393426659768.000000, mse: 22472537207840497664.000000, mean_q: 3998659329.896297, mean_eps: 0.406432
 198026/300000: episode: 1704, duration: 0.714s, episode steps: 102, steps per second: 143, episode reward: -227.793, mean reward: -2.233 [-100.000,  4.005], mean action: 2.020 [0.000, 3.000],  loss: 69722769629940880.000000, mse: 22889089250745573376.000000, mean_q: 4034193729.254902, mean_eps: 0.406077
 198169/300000: episode: 1705, duration: 0.980s, episode steps: 143, steps per second: 146, episode reward: -286.602, mean reward: -2.004 [-100.000,  2.430], mean action: 1.755 [0.000, 3.000],  loss: 49903025957008936.000000, mse: 22658471019661246464.000000, mean_q: 4011806114.909091, mean_eps: 0.405709
 198371/300000: episode: 1706, duration: 1.397s, episode steps: 202, steps per second: 145, episode reward: -269.992, mean reward: -1.337 [-100.000,  3.253], mean action: 1.777 [0.000, 3.000],  loss: 46945772862379728.000000, mse: 23147493685557252096.000000, mean_q: 4080399245.940594, mean_eps: 0.405192
 198545/300000: episode: 1707, duration: 1.204s, episode steps: 174, steps per second: 144, episode reward: -545.709, mean reward: -3.136 [-100.000,  2.103], mean action: 1.839 [0.000, 3.000],  loss: 26235168465547592.000000, mse: 23819315645358247936.000000, mean_q: 4138198909.057471, mean_eps: 0.404627
 198637/300000: episode: 1708, duration: 0.634s, episode steps:  92, steps per second: 145, episode reward: -342.508, mean reward: -3.723 [-100.000,  2.250], mean action: 1.967 [0.000, 3.000],  loss: 88638084511341520.000000, mse: 23681581731201609728.000000, mean_q: 4078842470.956522, mean_eps: 0.404228
 198770/300000: episode: 1709, duration: 0.889s, episode steps: 133, steps per second: 150, episode reward: -584.144, mean reward: -4.392 [-100.000,  2.359], mean action: 1.684 [0.000, 3.000],  loss: 60015429505062192.000000, mse: 23659320050063437824.000000, mean_q: 4106125702.736842, mean_eps: 0.403891
 198875/300000: episode: 1710, duration: 0.738s, episode steps: 105, steps per second: 142, episode reward: -234.083, mean reward: -2.229 [-100.000,  2.648], mean action: 1.638 [0.000, 3.000],  loss: 51105686106187208.000000, mse: 23038436038648954880.000000, mean_q: 4068090575.238095, mean_eps: 0.403534
 198982/300000: episode: 1711, duration: 0.735s, episode steps: 107, steps per second: 146, episode reward: -419.374, mean reward: -3.919 [-100.000,  1.191], mean action: 1.879 [0.000, 3.000],  loss: 46278356707451264.000000, mse: 23559611301128097792.000000, mean_q: 4127528907.364486, mean_eps: 0.403216
 199178/300000: episode: 1712, duration: 1.399s, episode steps: 196, steps per second: 140, episode reward:  7.201, mean reward:  0.037 [-100.000, 24.077], mean action: 1.939 [0.000, 3.000],  loss: 40145096157108768.000000, mse: 23322922707854364672.000000, mean_q: 4122526238.040816, mean_eps: 0.402762
 199351/300000: episode: 1713, duration: 1.191s, episode steps: 173, steps per second: 145, episode reward: -97.763, mean reward: -0.565 [-100.000,  6.472], mean action: 1.884 [0.000, 3.000],  loss: 45131084962831960.000000, mse: 23487665532014157824.000000, mean_q: 4133405549.502890, mean_eps: 0.402208
 199455/300000: episode: 1714, duration: 0.739s, episode steps: 104, steps per second: 141, episode reward: -5.338, mean reward: -0.051 [-100.000, 11.549], mean action: 1.740 [0.000, 3.000],  loss: 73639088539828224.000000, mse: 22641958473301934080.000000, mean_q: 4005618754.461538, mean_eps: 0.401792
 199570/300000: episode: 1715, duration: 0.838s, episode steps: 115, steps per second: 137, episode reward: -405.960, mean reward: -3.530 [-100.000,  2.135], mean action: 1.678 [0.000, 3.000],  loss: 44636396890815944.000000, mse: 23215495586310950912.000000, mean_q: 4096944508.660870, mean_eps: 0.401464
 199696/300000: episode: 1716, duration: 0.871s, episode steps: 126, steps per second: 145, episode reward: -201.707, mean reward: -1.601 [-100.000, 10.564], mean action: 1.841 [0.000, 3.000],  loss: 80008081785216896.000000, mse: 23889588027000832000.000000, mean_q: 4160607888.253968, mean_eps: 0.401103
 199906/300000: episode: 1717, duration: 1.479s, episode steps: 210, steps per second: 142, episode reward: -204.746, mean reward: -0.975 [-100.000,  4.637], mean action: 1.738 [0.000, 3.000],  loss: 58839948105143952.000000, mse: 22972329357572886528.000000, mean_q: 4089516913.371428, mean_eps: 0.400599
 200031/300000: episode: 1718, duration: 0.863s, episode steps: 125, steps per second: 145, episode reward: -232.344, mean reward: -1.859 [-100.000,  3.570], mean action: 1.648 [0.000, 3.000],  loss: 50035099839859000.000000, mse: 23378283539807301632.000000, mean_q: 4147165605.888000, mean_eps: 0.400096
 200123/300000: episode: 1719, duration: 0.661s, episode steps:  92, steps per second: 139, episode reward: -416.472, mean reward: -4.527 [-100.000,  0.626], mean action: 1.837 [0.000, 3.000],  loss: 49037425897236928.000000, mse: 24816492058150973440.000000, mean_q: 4260407638.260870, mean_eps: 0.399770
 200213/300000: episode: 1720, duration: 0.609s, episode steps:  90, steps per second: 148, episode reward: -59.552, mean reward: -0.662 [-100.000, 13.352], mean action: 1.889 [0.000, 3.000],  loss: 38350915826789032.000000, mse: 24786236834402177024.000000, mean_q: 4271896277.333333, mean_eps: 0.399497
 200337/300000: episode: 1721, duration: 0.841s, episode steps: 124, steps per second: 147, episode reward: -185.175, mean reward: -1.493 [-100.000,  5.036], mean action: 1.863 [0.000, 3.000],  loss: 44490690699705840.000000, mse: 24715899651134357504.000000, mean_q: 4263082766.451613, mean_eps: 0.399176
 200447/300000: episode: 1722, duration: 0.763s, episode steps: 110, steps per second: 144, episode reward: -533.073, mean reward: -4.846 [-100.000,  1.399], mean action: 1.809 [0.000, 3.000],  loss: 26985779450080608.000000, mse: 24757506120223969280.000000, mean_q: 4270977449.890909, mean_eps: 0.398825
 200646/300000: episode: 1723, duration: 1.394s, episode steps: 199, steps per second: 143, episode reward: -660.177, mean reward: -3.317 [-100.000,  2.450], mean action: 1.709 [0.000, 3.000],  loss: 37131111970814928.000000, mse: 24599596644661075968.000000, mean_q: 4238090342.914573, mean_eps: 0.398362
 200871/300000: episode: 1724, duration: 1.550s, episode steps: 225, steps per second: 145, episode reward: -897.966, mean reward: -3.991 [-100.000,  2.462], mean action: 1.858 [0.000, 3.000],  loss: 52945485258868800.000000, mse: 24783240012839587840.000000, mean_q: 4253372163.413333, mean_eps: 0.397726
 201004/300000: episode: 1725, duration: 0.980s, episode steps: 133, steps per second: 136, episode reward: -271.157, mean reward: -2.039 [-100.000,  3.757], mean action: 1.880 [0.000, 3.000],  loss: 46924717983408808.000000, mse: 26171914523068727296.000000, mean_q: 4366928426.345864, mean_eps: 0.397189
 201150/300000: episode: 1726, duration: 0.987s, episode steps: 146, steps per second: 148, episode reward: -293.241, mean reward: -2.008 [-100.000,  2.147], mean action: 1.603 [0.000, 3.000],  loss: 59648373394624232.000000, mse: 25265137337415962624.000000, mean_q: 4281362410.958904, mean_eps: 0.396770
 201236/300000: episode: 1727, duration: 0.581s, episode steps:  86, steps per second: 148, episode reward: -431.394, mean reward: -5.016 [-100.000,  0.603], mean action: 1.802 [0.000, 3.000],  loss: 32648323042969076.000000, mse: 26236499873381941248.000000, mean_q: 4386023290.046512, mean_eps: 0.396423
 201430/300000: episode: 1728, duration: 1.346s, episode steps: 194, steps per second: 144, episode reward: -122.487, mean reward: -0.631 [-100.000, 38.542], mean action: 1.820 [0.000, 3.000],  loss: 46717397562029424.000000, mse: 26217874334492176384.000000, mean_q: 4386799795.463918, mean_eps: 0.396003
 201538/300000: episode: 1729, duration: 0.742s, episode steps: 108, steps per second: 146, episode reward: -27.381, mean reward: -0.254 [-100.000, 25.738], mean action: 1.769 [0.000, 3.000],  loss: 30669410930786304.000000, mse: 25549797943641841664.000000, mean_q: 4300683282.962963, mean_eps: 0.395549
 201608/300000: episode: 1730, duration: 0.493s, episode steps:  70, steps per second: 142, episode reward: -115.753, mean reward: -1.654 [-100.000,  6.048], mean action: 1.857 [0.000, 3.000],  loss: 78376755874908448.000000, mse: 25217416108250517504.000000, mean_q: 4233374306.742857, mean_eps: 0.395282
 201757/300000: episode: 1731, duration: 1.047s, episode steps: 149, steps per second: 142, episode reward: -568.838, mean reward: -3.818 [-100.000,  1.677], mean action: 1.758 [0.000, 3.000],  loss: 46849435262320200.000000, mse: 26555458326393532416.000000, mean_q: 4368333251.865772, mean_eps: 0.394954
 201962/300000: episode: 1732, duration: 1.450s, episode steps: 205, steps per second: 141, episode reward: -416.936, mean reward: -2.034 [-100.000,  2.830], mean action: 1.888 [0.000, 3.000],  loss: 64927857590146040.000000, mse: 25997648056400650240.000000, mean_q: 4345454236.097561, mean_eps: 0.394423
 202103/300000: episode: 1733, duration: 0.967s, episode steps: 141, steps per second: 146, episode reward: -240.146, mean reward: -1.703 [-100.000, 14.143], mean action: 1.723 [0.000, 3.000],  loss: 40938183918992128.000000, mse: 26604386657835597824.000000, mean_q: 4414112261.446809, mean_eps: 0.393904
 202275/300000: episode: 1734, duration: 1.369s, episode steps: 172, steps per second: 126, episode reward: -367.195, mean reward: -2.135 [-100.000, 66.758], mean action: 1.855 [0.000, 3.000],  loss: 43240134912312824.000000, mse: 25494963329161457664.000000, mean_q: 4310186765.395349, mean_eps: 0.393434
 202371/300000: episode: 1735, duration: 0.765s, episode steps:  96, steps per second: 125, episode reward: -449.933, mean reward: -4.687 [-100.000,  0.977], mean action: 1.740 [0.000, 3.000],  loss: 31225382988414976.000000, mse: 26026623281062916096.000000, mean_q: 4313721429.333333, mean_eps: 0.393033
 202473/300000: episode: 1736, duration: 0.801s, episode steps: 102, steps per second: 127, episode reward: -286.443, mean reward: -2.808 [-100.000,  2.183], mean action: 1.686 [0.000, 3.000],  loss: 27419592556163636.000000, mse: 27055147636820439040.000000, mean_q: 4446989409.882353, mean_eps: 0.392735
 202737/300000: episode: 1737, duration: 2.200s, episode steps: 264, steps per second: 120, episode reward: -184.250, mean reward: -0.698 [-100.000,  8.386], mean action: 1.826 [0.000, 3.000],  loss: 76653040737650624.000000, mse: 26259074855510425600.000000, mean_q: 4294518600.727273, mean_eps: 0.392186
 202817/300000: episode: 1738, duration: 0.638s, episode steps:  80, steps per second: 125, episode reward: -28.288, mean reward: -0.354 [-100.000, 15.972], mean action: 1.725 [0.000, 3.000],  loss: 29420191896253236.000000, mse: 26339997779317542912.000000, mean_q: 4325049830.400000, mean_eps: 0.391670
 203163/300000: episode: 1739, duration: 2.532s, episode steps: 346, steps per second: 137, episode reward: -137.235, mean reward: -0.397 [-100.000, 21.028], mean action: 1.824 [0.000, 3.000],  loss: 70501244378034008.000000, mse: 26904326005360955392.000000, mean_q: 4374268576.554914, mean_eps: 0.391031
 203261/300000: episode: 1740, duration: 0.677s, episode steps:  98, steps per second: 145, episode reward: -353.896, mean reward: -3.611 [-100.000,  1.179], mean action: 1.745 [0.000, 3.000],  loss: 62331230245976104.000000, mse: 26890018433581080576.000000, mean_q: 4344735582.040816, mean_eps: 0.390365
 203521/300000: episode: 1741, duration: 1.824s, episode steps: 260, steps per second: 143, episode reward: -1055.492, mean reward: -4.060 [-100.000,  1.402], mean action: 1.785 [0.000, 3.000],  loss: 69980025017714912.000000, mse: 25886085875983642624.000000, mean_q: 4295855872.984615, mean_eps: 0.389829
 203747/300000: episode: 1742, duration: 1.684s, episode steps: 226, steps per second: 134, episode reward: -547.734, mean reward: -2.424 [-100.000,  4.824], mean action: 1.730 [0.000, 3.000],  loss: 38941826904965552.000000, mse: 26895876875384381440.000000, mean_q: 4378895779.115045, mean_eps: 0.389099
 203850/300000: episode: 1743, duration: 0.759s, episode steps: 103, steps per second: 136, episode reward: -320.822, mean reward: -3.115 [-100.000,  1.304], mean action: 1.825 [0.000, 3.000],  loss: 80818792911079920.000000, mse: 28288360226199871488.000000, mean_q: 4493028429.048544, mean_eps: 0.388606
 204134/300000: episode: 1744, duration: 2.178s, episode steps: 284, steps per second: 130, episode reward: -736.152, mean reward: -2.592 [-100.000,  1.984], mean action: 1.824 [0.000, 3.000],  loss: 46728245349161736.000000, mse: 26831423704980398080.000000, mean_q: 4367515783.211267, mean_eps: 0.388025
 204486/300000: episode: 1745, duration: 2.596s, episode steps: 352, steps per second: 136, episode reward: -1943.860, mean reward: -5.522 [-100.000,  1.544], mean action: 1.815 [0.000, 3.000],  loss: 42098441693310600.000000, mse: 29375227198047256576.000000, mean_q: 4551336003.636364, mean_eps: 0.387072
 204835/300000: episode: 1746, duration: 2.530s, episode steps: 349, steps per second: 138, episode reward: -960.973, mean reward: -2.754 [-100.000,  3.938], mean action: 1.805 [0.000, 3.000],  loss: 66867252556113352.000000, mse: 33277509579144511488.000000, mean_q: 4663823156.813753, mean_eps: 0.386020
 205018/300000: episode: 1747, duration: 1.342s, episode steps: 183, steps per second: 136, episode reward: -663.799, mean reward: -3.627 [-100.000,  2.174], mean action: 1.831 [0.000, 3.000],  loss: 50432033009109448.000000, mse: 33359827875603292160.000000, mean_q: 4695189692.852459, mean_eps: 0.385222
 205280/300000: episode: 1748, duration: 1.921s, episode steps: 262, steps per second: 136, episode reward: -481.838, mean reward: -1.839 [-100.000,  4.150], mean action: 1.805 [0.000, 3.000],  loss: 149821754339351776.000000, mse: 34276451029456842752.000000, mean_q: 4779147161.404580, mean_eps: 0.384554
 205596/300000: episode: 1749, duration: 2.337s, episode steps: 316, steps per second: 135, episode reward: -648.249, mean reward: -2.051 [-100.000,  3.103], mean action: 1.772 [0.000, 3.000],  loss: 196301237336388160.000000, mse: 35049088592207036416.000000, mean_q: 4802558073.518988, mean_eps: 0.383688
 205700/300000: episode: 1750, duration: 0.771s, episode steps: 104, steps per second: 135, episode reward: -408.881, mean reward: -3.932 [-100.000,  1.178], mean action: 1.894 [0.000, 3.000],  loss: 60419798346538376.000000, mse: 33764498813444562944.000000, mean_q: 4708785582.769231, mean_eps: 0.383057
 205820/300000: episode: 1751, duration: 0.941s, episode steps: 120, steps per second: 127, episode reward: -544.674, mean reward: -4.539 [-100.000,  1.402], mean action: 1.733 [0.000, 3.000],  loss: 68594159714793336.000000, mse: 33783024773835173888.000000, mean_q: 4694045130.666667, mean_eps: 0.382721
 205917/300000: episode: 1752, duration: 0.672s, episode steps:  97, steps per second: 144, episode reward: -300.047, mean reward: -3.093 [-100.000,  2.263], mean action: 1.938 [0.000, 3.000],  loss: 481043067675698944.000000, mse: 34117274641936580608.000000, mean_q: 4638468747.876288, mean_eps: 0.382396
 206019/300000: episode: 1753, duration: 0.732s, episode steps: 102, steps per second: 139, episode reward: -401.197, mean reward: -3.933 [-100.000,  1.782], mean action: 1.755 [0.000, 3.000],  loss: 49058207739782568.000000, mse: 32454875273383092224.000000, mean_q: 4625138130.823529, mean_eps: 0.382097
 206124/300000: episode: 1754, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -242.272, mean reward: -2.307 [-100.000,  3.447], mean action: 1.914 [0.000, 3.000],  loss: 71105852109817680.000000, mse: 32494302060577918976.000000, mean_q: 4638138992.152381, mean_eps: 0.381787
 206264/300000: episode: 1755, duration: 0.958s, episode steps: 140, steps per second: 146, episode reward: -513.080, mean reward: -3.665 [-100.000,  2.337], mean action: 1.786 [0.000, 3.000],  loss: 57421366521929960.000000, mse: 30851488723372232704.000000, mean_q: 4514286167.771428, mean_eps: 0.381420
 206651/300000: episode: 1756, duration: 2.829s, episode steps: 387, steps per second: 137, episode reward: -217.335, mean reward: -0.562 [-100.000, 37.186], mean action: 1.778 [0.000, 3.000],  loss: 61227556017074376.000000, mse: 31942137721133191168.000000, mean_q: 4594811350.325582, mean_eps: 0.380629
 206921/300000: episode: 1757, duration: 1.952s, episode steps: 270, steps per second: 138, episode reward: -583.761, mean reward: -2.162 [-100.000,  2.492], mean action: 1.719 [0.000, 3.000],  loss: 71066466037124784.000000, mse: 33181509962156978176.000000, mean_q: 4639550347.377778, mean_eps: 0.379643
 207053/300000: episode: 1758, duration: 0.942s, episode steps: 132, steps per second: 140, episode reward: -448.166, mean reward: -3.395 [-100.000,  2.831], mean action: 1.720 [0.000, 3.000],  loss: 50456149763610936.000000, mse: 32747465694297767936.000000, mean_q: 4634164491.636364, mean_eps: 0.379040
 207177/300000: episode: 1759, duration: 0.889s, episode steps: 124, steps per second: 139, episode reward: -437.746, mean reward: -3.530 [-100.000,  1.973], mean action: 1.831 [0.000, 3.000],  loss: 79667353499755296.000000, mse: 33130689710809948160.000000, mean_q: 4676272008.258064, mean_eps: 0.378657
 207306/300000: episode: 1760, duration: 0.949s, episode steps: 129, steps per second: 136, episode reward: -323.234, mean reward: -2.506 [-100.000,  2.527], mean action: 1.798 [0.000, 3.000],  loss: 52325944610124824.000000, mse: 31229395889176391680.000000, mean_q: 4540398101.829457, mean_eps: 0.378277
 207458/300000: episode: 1761, duration: 1.090s, episode steps: 152, steps per second: 139, episode reward: -730.112, mean reward: -4.803 [-100.000,  1.701], mean action: 1.855 [0.000, 3.000],  loss: 54435327133972800.000000, mse: 32438988803185352704.000000, mean_q: 4599712262.736842, mean_eps: 0.377856
 207650/300000: episode: 1762, duration: 1.429s, episode steps: 192, steps per second: 134, episode reward: -280.070, mean reward: -1.459 [-100.000,  4.878], mean action: 1.797 [0.000, 3.000],  loss: 173838354016632832.000000, mse: 33664314477660602368.000000, mean_q: 4667350680.000000, mean_eps: 0.377339
 207791/300000: episode: 1763, duration: 1.004s, episode steps: 141, steps per second: 140, episode reward: 52.298, mean reward:  0.371 [-100.000, 20.300], mean action: 1.823 [0.000, 3.000],  loss: 179290134596232736.000000, mse: 32996127985199382528.000000, mean_q: 4626949996.936171, mean_eps: 0.376840
 207911/300000: episode: 1764, duration: 0.862s, episode steps: 120, steps per second: 139, episode reward: -303.466, mean reward: -2.529 [-100.000,  2.100], mean action: 1.883 [0.000, 3.000],  loss: 34318599175868688.000000, mse: 32057384475685113856.000000, mean_q: 4564759754.666667, mean_eps: 0.376449
 208057/300000: episode: 1765, duration: 1.046s, episode steps: 146, steps per second: 140, episode reward: -19.909, mean reward: -0.136 [-100.000, 18.812], mean action: 1.815 [0.000, 3.000],  loss: 166069214141646144.000000, mse: 32910928268259557376.000000, mean_q: 4577160547.945206, mean_eps: 0.376049
 208158/300000: episode: 1766, duration: 0.736s, episode steps: 101, steps per second: 137, episode reward: -231.047, mean reward: -2.288 [-100.000,  2.473], mean action: 1.921 [0.000, 3.000],  loss: 21890509246069160.000000, mse: 32576014087896109056.000000, mean_q: 4558738442.138614, mean_eps: 0.375679
 208241/300000: episode: 1767, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -0.699, mean reward: -0.008 [-100.000,  8.590], mean action: 1.916 [0.000, 3.000],  loss: 61604054106383616.000000, mse: 31762739314664361984.000000, mean_q: 4548702445.493976, mean_eps: 0.375403
 208360/300000: episode: 1768, duration: 0.868s, episode steps: 119, steps per second: 137, episode reward: -237.629, mean reward: -1.997 [-100.000,  1.991], mean action: 1.748 [0.000, 3.000],  loss: 358696285875291904.000000, mse: 32269249013976989696.000000, mean_q: 4526577698.420168, mean_eps: 0.375100
 208583/300000: episode: 1769, duration: 1.696s, episode steps: 223, steps per second: 131, episode reward: -170.766, mean reward: -0.766 [-100.000,  7.873], mean action: 1.740 [0.000, 3.000],  loss: 144808127711060736.000000, mse: 31597371538794487808.000000, mean_q: 4484594648.968610, mean_eps: 0.374587
 208838/300000: episode: 1770, duration: 2.021s, episode steps: 255, steps per second: 126, episode reward: -400.612, mean reward: -1.571 [-100.000, 11.826], mean action: 1.796 [0.000, 3.000],  loss: 75222589010678032.000000, mse: 31918527867973795840.000000, mean_q: 4502067511.215686, mean_eps: 0.373870
 209231/300000: episode: 1771, duration: 2.806s, episode steps: 393, steps per second: 140, episode reward: -300.960, mean reward: -0.766 [-100.000,  8.120], mean action: 1.751 [0.000, 3.000],  loss: 43881198534579624.000000, mse: 30354673080723025920.000000, mean_q: 4425355536.936387, mean_eps: 0.372898
 209541/300000: episode: 1772, duration: 2.208s, episode steps: 310, steps per second: 140, episode reward: -138.781, mean reward: -0.448 [-100.000, 14.332], mean action: 1.768 [0.000, 3.000],  loss: 49574314153179280.000000, mse: 30398764661416026112.000000, mean_q: 4446607890.993548, mean_eps: 0.371843
 209644/300000: episode: 1773, duration: 0.744s, episode steps: 103, steps per second: 138, episode reward: -49.967, mean reward: -0.485 [-100.000,  7.801], mean action: 1.553 [0.000, 3.000],  loss: 130596641781620496.000000, mse: 30872587709110816768.000000, mean_q: 4404855176.699029, mean_eps: 0.371224
 209736/300000: episode: 1774, duration: 0.669s, episode steps:  92, steps per second: 138, episode reward: -221.751, mean reward: -2.410 [-100.000, 54.863], mean action: 1.793 [0.000, 3.000],  loss: 94639505822369888.000000, mse: 28354896135267889152.000000, mean_q: 4300358149.565217, mean_eps: 0.370932
 209859/300000: episode: 1775, duration: 0.943s, episode steps: 123, steps per second: 130, episode reward: -494.363, mean reward: -4.019 [-100.000,  2.019], mean action: 1.772 [0.000, 3.000],  loss: 82750742958807024.000000, mse: 30369308081625284608.000000, mean_q: 4424015745.040650, mean_eps: 0.370609
 209995/300000: episode: 1776, duration: 0.963s, episode steps: 136, steps per second: 141, episode reward: -215.553, mean reward: -1.585 [-100.000,  2.991], mean action: 1.757 [0.000, 3.000],  loss: 86263782086294592.000000, mse: 30649601339152629760.000000, mean_q: 4385868540.235294, mean_eps: 0.370220
 210155/300000: episode: 1777, duration: 1.142s, episode steps: 160, steps per second: 140, episode reward: -728.708, mean reward: -4.554 [-100.000,  1.650], mean action: 1.744 [0.000, 3.000],  loss: 176049587342435936.000000, mse: 30243969965003218944.000000, mean_q: 4343186198.400000, mean_eps: 0.369777
 210509/300000: episode: 1778, duration: 2.499s, episode steps: 354, steps per second: 142, episode reward: -32.779, mean reward: -0.093 [-100.000, 13.000], mean action: 1.825 [0.000, 3.000],  loss: 82833656041434784.000000, mse: 30514887778660282368.000000, mean_q: 4411108352.000000, mean_eps: 0.369006
 210645/300000: episode: 1779, duration: 0.907s, episode steps: 136, steps per second: 150, episode reward: -415.325, mean reward: -3.054 [-100.000,  1.680], mean action: 1.875 [0.000, 3.000],  loss: 182600851959100608.000000, mse: 31597816205470937088.000000, mean_q: 4412105984.000000, mean_eps: 0.368270
 210840/300000: episode: 1780, duration: 1.333s, episode steps: 195, steps per second: 146, episode reward: -576.016, mean reward: -2.954 [-100.000,  4.455], mean action: 1.836 [0.000, 3.000],  loss: 53149225758037584.000000, mse: 30180519722772975616.000000, mean_q: 4344577104.082051, mean_eps: 0.367774
 211076/300000: episode: 1781, duration: 1.615s, episode steps: 236, steps per second: 146, episode reward: -648.399, mean reward: -2.747 [-100.000,  4.135], mean action: 1.856 [0.000, 3.000],  loss: 45401050551912064.000000, mse: 30150387987859038208.000000, mean_q: 4410650681.491526, mean_eps: 0.367127
 211213/300000: episode: 1782, duration: 1.036s, episode steps: 137, steps per second: 132, episode reward: -413.586, mean reward: -3.019 [-100.000,  3.152], mean action: 1.964 [0.000, 3.000],  loss: 193959609938427296.000000, mse: 29571339426627612672.000000, mean_q: 4367043537.284672, mean_eps: 0.366568
 211359/300000: episode: 1783, duration: 1.175s, episode steps: 146, steps per second: 124, episode reward: -625.746, mean reward: -4.286 [-100.000,  2.212], mean action: 1.849 [0.000, 3.000],  loss: 78757201272811344.000000, mse: 30764163346911748096.000000, mean_q: 4445127406.465754, mean_eps: 0.366144
 211624/300000: episode: 1784, duration: 2.065s, episode steps: 265, steps per second: 128, episode reward: -583.521, mean reward: -2.202 [-100.000,  3.997], mean action: 1.800 [0.000, 3.000],  loss: 58416309170929416.000000, mse: 30683403721408888832.000000, mean_q: 4454096336.664151, mean_eps: 0.365527
 211825/300000: episode: 1785, duration: 1.468s, episode steps: 201, steps per second: 137, episode reward: -480.992, mean reward: -2.393 [-100.000,  4.383], mean action: 1.816 [0.000, 3.000],  loss: 28020615830008156.000000, mse: 30019238282457583616.000000, mean_q: 4432868941.691543, mean_eps: 0.364828
 212082/300000: episode: 1786, duration: 1.793s, episode steps: 257, steps per second: 143, episode reward: -319.214, mean reward: -1.242 [-100.000,  5.831], mean action: 1.767 [0.000, 3.000],  loss: 121009430986462320.000000, mse: 29533428847794450432.000000, mean_q: 4412910510.319066, mean_eps: 0.364141
 212227/300000: episode: 1787, duration: 0.988s, episode steps: 145, steps per second: 147, episode reward:  8.172, mean reward:  0.056 [-100.000, 10.835], mean action: 1.731 [0.000, 3.000],  loss: 47635050991168320.000000, mse: 30850301245668737024.000000, mean_q: 4512096049.434483, mean_eps: 0.363538
 212313/300000: episode: 1788, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -237.116, mean reward: -2.757 [-100.000,  3.870], mean action: 1.593 [0.000, 3.000],  loss: 93778466127594608.000000, mse: 29990382986356887552.000000, mean_q: 4403884889.302325, mean_eps: 0.363191
 212387/300000: episode: 1789, duration: 0.498s, episode steps:  74, steps per second: 149, episode reward: -55.699, mean reward: -0.753 [-100.000,  8.059], mean action: 1.905 [0.000, 3.000],  loss: 82497756082329216.000000, mse: 30260430458831814656.000000, mean_q: 4460917632.000000, mean_eps: 0.362952
 212479/300000: episode: 1790, duration: 0.623s, episode steps:  92, steps per second: 148, episode reward: -321.583, mean reward: -3.495 [-100.000,  1.500], mean action: 1.837 [0.000, 3.000],  loss: 90448560265044320.000000, mse: 29695629356134973440.000000, mean_q: 4410711763.478261, mean_eps: 0.362703
 212607/300000: episode: 1791, duration: 0.894s, episode steps: 128, steps per second: 143, episode reward: -345.617, mean reward: -2.700 [-100.000,  1.547], mean action: 1.773 [0.000, 3.000],  loss: 215403509003059200.000000, mse: 30350039750373539840.000000, mean_q: 4409445034.000000, mean_eps: 0.362372
 212766/300000: episode: 1792, duration: 1.164s, episode steps: 159, steps per second: 137, episode reward: -230.234, mean reward: -1.448 [-100.000,  2.287], mean action: 1.975 [0.000, 3.000],  loss: 276877836852107520.000000, mse: 29682702011579809792.000000, mean_q: 4383019080.452830, mean_eps: 0.361942
 213112/300000: episode: 1793, duration: 2.626s, episode steps: 346, steps per second: 132, episode reward: -1668.851, mean reward: -4.823 [-100.000,  1.944], mean action: 1.815 [0.000, 3.000],  loss: 53581945022574896.000000, mse: 29763289562843750400.000000, mean_q: 4334509064.138728, mean_eps: 0.361185
 213483/300000: episode: 1794, duration: 2.772s, episode steps: 371, steps per second: 134, episode reward: -45.804, mean reward: -0.123 [-100.000, 15.721], mean action: 1.779 [0.000, 3.000],  loss: 140920939322383872.000000, mse: 32184780313118298112.000000, mean_q: 4447457370.393531, mean_eps: 0.360109
 213959/300000: episode: 1795, duration: 3.536s, episode steps: 476, steps per second: 135, episode reward: -74.357, mean reward: -0.156 [-100.000, 12.223], mean action: 1.815 [0.000, 3.000],  loss: 58967207056224912.000000, mse: 31127781930461118464.000000, mean_q: 4344298723.495798, mean_eps: 0.358838
 214309/300000: episode: 1796, duration: 2.712s, episode steps: 350, steps per second: 129, episode reward: -510.290, mean reward: -1.458 [-100.000,  5.990], mean action: 1.803 [0.000, 3.000],  loss: 151647391885241696.000000, mse: 31582114421502439424.000000, mean_q: 4342759387.428572, mean_eps: 0.357599
 214433/300000: episode: 1797, duration: 0.855s, episode steps: 124, steps per second: 145, episode reward: -363.498, mean reward: -2.931 [-100.000,  1.949], mean action: 1.968 [0.000, 3.000],  loss: 52391527377577256.000000, mse: 32268274021668495360.000000, mean_q: 4412963410.580646, mean_eps: 0.356888
 214528/300000: episode: 1798, duration: 0.661s, episode steps:  95, steps per second: 144, episode reward: 48.461, mean reward:  0.510 [-100.000, 16.591], mean action: 1.768 [0.000, 3.000],  loss: 44823729942248952.000000, mse: 30862818497508118528.000000, mean_q: 4257403639.915790, mean_eps: 0.356560
 214653/300000: episode: 1799, duration: 0.914s, episode steps: 125, steps per second: 137, episode reward: -623.977, mean reward: -4.992 [-100.000,  0.863], mean action: 1.896 [0.000, 3.000],  loss: 111968416909741792.000000, mse: 30567925475327410176.000000, mean_q: 4277975029.760000, mean_eps: 0.356230
 214780/300000: episode: 1800, duration: 0.927s, episode steps: 127, steps per second: 137, episode reward: -58.597, mean reward: -0.461 [-100.000, 32.008], mean action: 1.819 [0.000, 3.000],  loss: 98326229164418512.000000, mse: 30710622678422974464.000000, mean_q: 4298653127.559055, mean_eps: 0.355852
 214879/300000: episode: 1801, duration: 0.707s, episode steps:  99, steps per second: 140, episode reward: -615.101, mean reward: -6.213 [-100.000,  0.007], mean action: 1.949 [0.000, 3.000],  loss: 232565872545605664.000000, mse: 30564755270821105664.000000, mean_q: 4290749039.191919, mean_eps: 0.355513
 215106/300000: episode: 1802, duration: 1.748s, episode steps: 227, steps per second: 130, episode reward: -538.084, mean reward: -2.370 [-100.000,  3.016], mean action: 1.833 [0.000, 3.000],  loss: 44546322215719648.000000, mse: 30078524935606804480.000000, mean_q: 4228293947.770925, mean_eps: 0.355024
 215886/300000: episode: 1803, duration: 5.862s, episode steps: 780, steps per second: 133, episode reward: -669.463, mean reward: -0.858 [-100.000,  6.932], mean action: 1.840 [0.000, 3.000],  loss: 111926006642880832.000000, mse: 29916686349515882496.000000, mean_q: 4230679148.635898, mean_eps: 0.353514
 216019/300000: episode: 1804, duration: 0.909s, episode steps: 133, steps per second: 146, episode reward: -668.096, mean reward: -5.023 [-100.000,  4.966], mean action: 1.774 [0.000, 3.000],  loss: 44355899634100824.000000, mse: 28979361887832915968.000000, mean_q: 4170149223.939850, mean_eps: 0.352144
 216193/300000: episode: 1805, duration: 1.189s, episode steps: 174, steps per second: 146, episode reward: -358.843, mean reward: -2.062 [-100.000,  3.789], mean action: 1.805 [0.000, 3.000],  loss: 60758241246358184.000000, mse: 28298291400192118784.000000, mean_q: 4128717348.781609, mean_eps: 0.351684
 216276/300000: episode: 1806, duration: 0.564s, episode steps:  83, steps per second: 147, episode reward: -182.450, mean reward: -2.198 [-100.000, 51.869], mean action: 1.530 [0.000, 3.000],  loss: 76550037785289984.000000, mse: 28754880889681674240.000000, mean_q: 4139922832.963855, mean_eps: 0.351298
 216372/300000: episode: 1807, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: -335.126, mean reward: -3.491 [-100.000, 29.913], mean action: 1.646 [0.000, 3.000],  loss: 48367676543729664.000000, mse: 29482069709308997632.000000, mean_q: 4211271978.666667, mean_eps: 0.351029
 216498/300000: episode: 1808, duration: 0.863s, episode steps: 126, steps per second: 146, episode reward: -284.388, mean reward: -2.257 [-100.000,  2.002], mean action: 1.810 [0.000, 3.000],  loss: 72453007253505040.000000, mse: 29421094416385277952.000000, mean_q: 4206123731.301587, mean_eps: 0.350697
 216800/300000: episode: 1809, duration: 2.122s, episode steps: 302, steps per second: 142, episode reward: -885.076, mean reward: -2.931 [-100.000,  2.070], mean action: 1.858 [0.000, 3.000],  loss: 111160836464202944.000000, mse: 28777004955296096256.000000, mean_q: 4168885448.900662, mean_eps: 0.350054
 217001/300000: episode: 1810, duration: 1.433s, episode steps: 201, steps per second: 140, episode reward: -848.170, mean reward: -4.220 [-100.000,  2.224], mean action: 1.950 [0.000, 3.000],  loss: 111977835475408464.000000, mse: 29151594864083038208.000000, mean_q: 4170923900.815920, mean_eps: 0.349300
 217629/300000: episode: 1811, duration: 5.615s, episode steps: 628, steps per second: 112, episode reward: -348.920, mean reward: -0.556 [-100.000,  8.232], mean action: 1.846 [0.000, 3.000],  loss: 86652626754140128.000000, mse: 28228530296639897600.000000, mean_q: 4114249238.828025, mean_eps: 0.348056
 217723/300000: episode: 1812, duration: 0.721s, episode steps:  94, steps per second: 130, episode reward: -351.785, mean reward: -3.742 [-100.000,  1.100], mean action: 1.904 [0.000, 3.000],  loss: 133380899776138656.000000, mse: 28357863943970029568.000000, mean_q: 4106270001.021276, mean_eps: 0.346973
 217841/300000: episode: 1813, duration: 0.904s, episode steps: 118, steps per second: 130, episode reward: -269.104, mean reward: -2.281 [-100.000,  1.988], mean action: 1.636 [0.000, 3.000],  loss: 168327335616739712.000000, mse: 28788996865625759744.000000, mean_q: 4081738023.050848, mean_eps: 0.346655
 218030/300000: episode: 1814, duration: 1.352s, episode steps: 189, steps per second: 140, episode reward: -980.285, mean reward: -5.187 [-100.000,  1.867], mean action: 1.825 [0.000, 3.000],  loss: 57730192413101800.000000, mse: 28264695581178867712.000000, mean_q: 4087072050.116402, mean_eps: 0.346195
 218197/300000: episode: 1815, duration: 1.241s, episode steps: 167, steps per second: 135, episode reward: -62.716, mean reward: -0.376 [-100.000, 11.506], mean action: 1.760 [0.000, 3.000],  loss: 133603995574893216.000000, mse: 28345002207365984256.000000, mean_q: 4114993256.239521, mean_eps: 0.345661
 218336/300000: episode: 1816, duration: 1.076s, episode steps: 139, steps per second: 129, episode reward: 25.958, mean reward:  0.187 [-100.000, 11.064], mean action: 1.820 [0.000, 3.000],  loss: 137872028971512256.000000, mse: 27468589345402789888.000000, mean_q: 4044310772.949640, mean_eps: 0.345202
 218442/300000: episode: 1817, duration: 0.756s, episode steps: 106, steps per second: 140, episode reward: -437.263, mean reward: -4.125 [-100.000,  1.570], mean action: 1.858 [0.000, 3.000],  loss: 29678799900579880.000000, mse: 28958588603450769408.000000, mean_q: 4105180732.377358, mean_eps: 0.344834
 219025/300000: episode: 1818, duration: 4.619s, episode steps: 583, steps per second: 126, episode reward: -423.564, mean reward: -0.727 [-100.000,  7.694], mean action: 1.827 [0.000, 3.000],  loss: 54304029234898032.000000, mse: 28398235605025460224.000000, mean_q: 4110859168.274443, mean_eps: 0.343801
 219108/300000: episode: 1819, duration: 0.582s, episode steps:  83, steps per second: 143, episode reward: -172.416, mean reward: -2.077 [-100.000,  7.646], mean action: 1.759 [0.000, 3.000],  loss: 50045357110095624.000000, mse: 27307364413006532608.000000, mean_q: 4008524374.361446, mean_eps: 0.342802
 219201/300000: episode: 1820, duration: 0.695s, episode steps:  93, steps per second: 134, episode reward: -429.553, mean reward: -4.619 [-100.000,  1.208], mean action: 1.935 [0.000, 3.000],  loss: 69943873122068984.000000, mse: 29194684521760309248.000000, mean_q: 4171904781.763441, mean_eps: 0.342538
 219601/300000: episode: 1821, duration: 2.921s, episode steps: 400, steps per second: 137, episode reward: -54.422, mean reward: -0.136 [-100.000, 16.772], mean action: 1.820 [0.000, 3.000],  loss: 67133133487142336.000000, mse: 27713137479928795136.000000, mean_q: 4079775029.120000, mean_eps: 0.341799
 219717/300000: episode: 1822, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -320.717, mean reward: -2.765 [-100.000,  2.314], mean action: 1.690 [0.000, 3.000],  loss: 68502547309979016.000000, mse: 27219252495673368576.000000, mean_q: 4084394999.172414, mean_eps: 0.341024
 219811/300000: episode: 1823, duration: 0.708s, episode steps:  94, steps per second: 133, episode reward: -319.726, mean reward: -3.401 [-100.000,  1.783], mean action: 1.947 [0.000, 3.000],  loss: 26938344963443428.000000, mse: 29548975054973861888.000000, mean_q: 4185660165.446808, mean_eps: 0.340709
 219984/300000: episode: 1824, duration: 1.349s, episode steps: 173, steps per second: 128, episode reward: -416.567, mean reward: -2.408 [-100.000,  2.568], mean action: 1.821 [0.000, 3.000],  loss: 27773151288301108.000000, mse: 27605805942389166080.000000, mean_q: 4044225617.387283, mean_eps: 0.340309
 220079/300000: episode: 1825, duration: 0.790s, episode steps:  95, steps per second: 120, episode reward: -374.254, mean reward: -3.940 [-100.000,  1.754], mean action: 1.821 [0.000, 3.000],  loss: 196528047903771680.000000, mse: 27539917279127941120.000000, mean_q: 4104740492.126316, mean_eps: 0.339907
 220179/300000: episode: 1826, duration: 0.761s, episode steps: 100, steps per second: 131, episode reward: -331.607, mean reward: -3.316 [-100.000,  1.406], mean action: 1.910 [0.000, 3.000],  loss: 60140692185116832.000000, mse: 28051292773082759168.000000, mean_q: 4139773396.480000, mean_eps: 0.339615
 220371/300000: episode: 1827, duration: 1.547s, episode steps: 192, steps per second: 124, episode reward: -1021.294, mean reward: -5.319 [-100.000,  1.450], mean action: 1.854 [0.000, 3.000],  loss: 60367046322072232.000000, mse: 27265377565323296768.000000, mean_q: 4043127330.666667, mean_eps: 0.339176
 220479/300000: episode: 1828, duration: 0.900s, episode steps: 108, steps per second: 120, episode reward: -729.544, mean reward: -6.755 [-100.000,  0.796], mean action: 1.935 [0.000, 3.000],  loss: 59635465053322656.000000, mse: 25015782107015962624.000000, mean_q: 3983605176.888889, mean_eps: 0.338726
 220698/300000: episode: 1829, duration: 1.647s, episode steps: 219, steps per second: 133, episode reward: -852.079, mean reward: -3.891 [-100.000,  1.802], mean action: 1.900 [0.000, 3.000],  loss: 54192413314508360.000000, mse: 28395927940376879104.000000, mean_q: 4132106261.041096, mean_eps: 0.338236
 220826/300000: episode: 1830, duration: 0.953s, episode steps: 128, steps per second: 134, episode reward: -430.666, mean reward: -3.365 [-100.000,  1.637], mean action: 1.797 [0.000, 3.000],  loss: 45094062281195520.000000, mse: 28237995337821716480.000000, mean_q: 4129570334.000000, mean_eps: 0.337716
 221029/300000: episode: 1831, duration: 1.541s, episode steps: 203, steps per second: 132, episode reward: -869.272, mean reward: -4.282 [-100.000,  1.613], mean action: 1.882 [0.000, 3.000],  loss: 39957567503635016.000000, mse: 28467702912366784512.000000, mean_q: 4173121785.694582, mean_eps: 0.337219
 221574/300000: episode: 1832, duration: 4.436s, episode steps: 545, steps per second: 123, episode reward: -642.970, mean reward: -1.180 [-100.000,  4.679], mean action: 1.872 [0.000, 3.000],  loss: 65882574958908072.000000, mse: 28047021968361758720.000000, mean_q: 4134440033.233027, mean_eps: 0.336097
 221722/300000: episode: 1833, duration: 1.163s, episode steps: 148, steps per second: 127, episode reward: -250.871, mean reward: -1.695 [-100.000,  1.749], mean action: 1.899 [0.000, 3.000],  loss: 85566660397703840.000000, mse: 26590875042012344320.000000, mean_q: 4061532741.189189, mean_eps: 0.335058
 222215/300000: episode: 1834, duration: 3.589s, episode steps: 493, steps per second: 137, episode reward: -31.022, mean reward: -0.063 [-100.000, 23.237], mean action: 1.795 [0.000, 3.000],  loss: 80259600478856256.000000, mse: 26777476803816800256.000000, mean_q: 4017113277.014199, mean_eps: 0.334096
 222332/300000: episode: 1835, duration: 0.869s, episode steps: 117, steps per second: 135, episode reward: -277.206, mean reward: -2.369 [-100.000,  2.006], mean action: 1.744 [0.000, 3.000],  loss: 60098152687544720.000000, mse: 27027121137391271936.000000, mean_q: 4038737456.136752, mean_eps: 0.333181
 222439/300000: episode: 1836, duration: 0.768s, episode steps: 107, steps per second: 139, episode reward: -308.641, mean reward: -2.884 [-100.000,  2.020], mean action: 1.776 [0.000, 3.000],  loss: 44132312229736872.000000, mse: 27276430669921374208.000000, mean_q: 4046184046.056075, mean_eps: 0.332845
 222728/300000: episode: 1837, duration: 2.118s, episode steps: 289, steps per second: 136, episode reward: -1291.245, mean reward: -4.468 [-100.000,  4.173], mean action: 1.875 [0.000, 3.000],  loss: 74375203320814512.000000, mse: 26564608511082676224.000000, mean_q: 4026140052.816609, mean_eps: 0.332251
 222801/300000: episode: 1838, duration: 0.518s, episode steps:  73, steps per second: 141, episode reward: -55.740, mean reward: -0.764 [-100.000, 12.986], mean action: 1.877 [0.000, 3.000],  loss: 171027550995476992.000000, mse: 28181367448978100224.000000, mean_q: 4109553881.424657, mean_eps: 0.331708
 222966/300000: episode: 1839, duration: 1.205s, episode steps: 165, steps per second: 137, episode reward: -532.924, mean reward: -3.230 [-100.000,  4.355], mean action: 1.764 [0.000, 3.000],  loss: 47607636240162664.000000, mse: 25634687470872756224.000000, mean_q: 3976812421.430303, mean_eps: 0.331351
 223206/300000: episode: 1840, duration: 1.711s, episode steps: 240, steps per second: 140, episode reward: -1101.728, mean reward: -4.591 [-100.000,  1.313], mean action: 1.804 [0.000, 3.000],  loss: 70591271032463904.000000, mse: 26365967845392646144.000000, mean_q: 3993721914.666667, mean_eps: 0.330743
 223426/300000: episode: 1841, duration: 1.505s, episode steps: 220, steps per second: 146, episode reward: -863.603, mean reward: -3.925 [-100.000,  1.970], mean action: 1.850 [0.000, 3.000],  loss: 102166312994295584.000000, mse: 27523079130266619904.000000, mean_q: 4089787081.309091, mean_eps: 0.330054
 223598/300000: episode: 1842, duration: 1.279s, episode steps: 172, steps per second: 135, episode reward: -487.900, mean reward: -2.837 [-100.000,  4.281], mean action: 1.919 [0.000, 3.000],  loss: 164607522857123072.000000, mse: 28539940392156356608.000000, mean_q: 4154882324.837209, mean_eps: 0.329465
 223704/300000: episode: 1843, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: 37.602, mean reward:  0.355 [-100.000, 15.451], mean action: 1.792 [0.000, 3.000],  loss: 82540061798615776.000000, mse: 29026671120653500416.000000, mean_q: 4171689421.283019, mean_eps: 0.329048
 223892/300000: episode: 1844, duration: 1.432s, episode steps: 188, steps per second: 131, episode reward: -365.932, mean reward: -1.946 [-100.000,  2.974], mean action: 1.840 [0.000, 3.000],  loss: 60830473047780240.000000, mse: 28490379184948924416.000000, mean_q: 4153560815.659575, mean_eps: 0.328607
 224099/300000: episode: 1845, duration: 1.509s, episode steps: 207, steps per second: 137, episode reward: -908.157, mean reward: -4.387 [-100.000,  1.933], mean action: 1.850 [0.000, 3.000],  loss: 154472027577930784.000000, mse: 28929132737553661952.000000, mean_q: 4154604400.541063, mean_eps: 0.328015
 224286/300000: episode: 1846, duration: 1.323s, episode steps: 187, steps per second: 141, episode reward: -86.429, mean reward: -0.462 [-100.000, 17.571], mean action: 1.861 [0.000, 3.000],  loss: 125260001985477168.000000, mse: 28375080638036500480.000000, mean_q: 4123349547.807487, mean_eps: 0.327424
 224402/300000: episode: 1847, duration: 0.875s, episode steps: 116, steps per second: 133, episode reward: -401.139, mean reward: -3.458 [-100.000,  1.309], mean action: 1.802 [0.000, 3.000],  loss: 107498613378555616.000000, mse: 27085724798896369664.000000, mean_q: 4050812879.448276, mean_eps: 0.326969
 224501/300000: episode: 1848, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -407.357, mean reward: -4.115 [-100.000,  1.235], mean action: 1.869 [0.000, 3.000],  loss: 55739265586648408.000000, mse: 27036024622883696640.000000, mean_q: 4077574449.131313, mean_eps: 0.326647
 224626/300000: episode: 1849, duration: 0.904s, episode steps: 125, steps per second: 138, episode reward: 44.275, mean reward:  0.354 [-100.000, 13.272], mean action: 1.792 [0.000, 3.000],  loss: 48714428440362616.000000, mse: 27217043951708717056.000000, mean_q: 4081464936.448000, mean_eps: 0.326311
 224737/300000: episode: 1850, duration: 0.847s, episode steps: 111, steps per second: 131, episode reward: -460.130, mean reward: -4.145 [-100.000,  1.029], mean action: 1.874 [0.000, 3.000],  loss: 137159519557579888.000000, mse: 27597704345010622464.000000, mean_q: 4063914242.306306, mean_eps: 0.325957
 225091/300000: episode: 1851, duration: 2.700s, episode steps: 354, steps per second: 131, episode reward: -934.097, mean reward: -2.639 [-100.000,  4.019], mean action: 1.802 [0.000, 3.000],  loss: 76784501469209360.000000, mse: 26612550098191458304.000000, mean_q: 4015778624.361582, mean_eps: 0.325259
 225358/300000: episode: 1852, duration: 1.887s, episode steps: 267, steps per second: 142, episode reward: -250.002, mean reward: -0.936 [-100.000,  5.494], mean action: 1.835 [0.000, 3.000],  loss: 54994766145098416.000000, mse: 27188451050426253312.000000, mean_q: 4057071962.127341, mean_eps: 0.324328
 225662/300000: episode: 1853, duration: 2.337s, episode steps: 304, steps per second: 130, episode reward: -1606.413, mean reward: -5.284 [-100.000,  1.777], mean action: 1.862 [0.000, 3.000],  loss: 56189887410174272.000000, mse: 28581855616449904640.000000, mean_q: 4165647866.947369, mean_eps: 0.323471
 225851/300000: episode: 1854, duration: 1.409s, episode steps: 189, steps per second: 134, episode reward: -364.334, mean reward: -1.928 [-100.000,  2.007], mean action: 1.794 [0.000, 3.000],  loss: 45832082960373768.000000, mse: 29957815368670425088.000000, mean_q: 4216398852.063492, mean_eps: 0.322732
 226048/300000: episode: 1855, duration: 1.454s, episode steps: 197, steps per second: 135, episode reward: -779.085, mean reward: -3.955 [-100.000,  2.707], mean action: 1.832 [0.000, 3.000],  loss: 35278929827198364.000000, mse: 28904058405673623552.000000, mean_q: 4193907485.888325, mean_eps: 0.322153
 226181/300000: episode: 1856, duration: 0.951s, episode steps: 133, steps per second: 140, episode reward: -582.417, mean reward: -4.379 [-100.000,  1.366], mean action: 1.850 [0.000, 3.000],  loss: 87761881738861168.000000, mse: 28911344585676828672.000000, mean_q: 4249385302.616541, mean_eps: 0.321658
 226334/300000: episode: 1857, duration: 1.078s, episode steps: 153, steps per second: 142, episode reward: -353.909, mean reward: -2.313 [-100.000,  3.886], mean action: 1.987 [0.000, 3.000],  loss: 88398845063017440.000000, mse: 29603208530232365056.000000, mean_q: 4254990208.836601, mean_eps: 0.321229
 226459/300000: episode: 1858, duration: 0.882s, episode steps: 125, steps per second: 142, episode reward: -461.255, mean reward: -3.690 [-100.000,  1.374], mean action: 1.856 [0.000, 3.000],  loss: 129694808540285232.000000, mse: 27136562919925559296.000000, mean_q: 4086687617.024000, mean_eps: 0.320812
 226580/300000: episode: 1859, duration: 0.830s, episode steps: 121, steps per second: 146, episode reward: -328.388, mean reward: -2.714 [-100.000,  1.981], mean action: 1.843 [0.000, 3.000],  loss: 51175350745579384.000000, mse: 29455270976664186880.000000, mean_q: 4246525541.553719, mean_eps: 0.320443
 226742/300000: episode: 1860, duration: 1.178s, episode steps: 162, steps per second: 138, episode reward: -434.017, mean reward: -2.679 [-100.000,  4.253], mean action: 1.772 [0.000, 3.000],  loss: 64076298847191848.000000, mse: 28584926168954634240.000000, mean_q: 4215368691.358025, mean_eps: 0.320018
 226854/300000: episode: 1861, duration: 0.782s, episode steps: 112, steps per second: 143, episode reward: -385.268, mean reward: -3.440 [-100.000,  1.639], mean action: 1.812 [0.000, 3.000],  loss: 79556277072709920.000000, mse: 27578102174323834880.000000, mean_q: 4135784745.142857, mean_eps: 0.319608
 226948/300000: episode: 1862, duration: 0.635s, episode steps:  94, steps per second: 148, episode reward: -397.421, mean reward: -4.228 [-100.000,  0.959], mean action: 1.894 [0.000, 3.000],  loss: 63231090665410992.000000, mse: 29779078941834874880.000000, mean_q: 4243706346.212766, mean_eps: 0.319299
 227063/300000: episode: 1863, duration: 0.844s, episode steps: 115, steps per second: 136, episode reward: -379.255, mean reward: -3.298 [-100.000,  1.744], mean action: 1.913 [0.000, 3.000],  loss: 41246079396692112.000000, mse: 27683150476716666880.000000, mean_q: 4156991405.634783, mean_eps: 0.318985
 227178/300000: episode: 1864, duration: 0.774s, episode steps: 115, steps per second: 149, episode reward: -559.834, mean reward: -4.868 [-100.000,  1.109], mean action: 1.817 [0.000, 3.000],  loss: 48833745093608704.000000, mse: 30246629606844260352.000000, mean_q: 4303998617.600000, mean_eps: 0.318640
 227439/300000: episode: 1865, duration: 1.874s, episode steps: 261, steps per second: 139, episode reward: -1172.995, mean reward: -4.494 [-100.000,  1.511], mean action: 1.782 [0.000, 3.000],  loss: 109131176190812512.000000, mse: 29215473527602552832.000000, mean_q: 4224951930.605364, mean_eps: 0.318076
 227612/300000: episode: 1866, duration: 1.304s, episode steps: 173, steps per second: 133, episode reward: -619.371, mean reward: -3.580 [-100.000,  2.106], mean action: 1.751 [0.000, 3.000],  loss: 168059671655654080.000000, mse: 29848751903736176640.000000, mean_q: 4271985518.982659, mean_eps: 0.317425
 227725/300000: episode: 1867, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -440.380, mean reward: -3.897 [-100.000,  1.442], mean action: 1.903 [0.000, 3.000],  loss: 69668359955325728.000000, mse: 28823483063333294080.000000, mean_q: 4187136346.619469, mean_eps: 0.316996
 227863/300000: episode: 1868, duration: 0.942s, episode steps: 138, steps per second: 146, episode reward: -319.512, mean reward: -2.315 [-100.000,  1.770], mean action: 1.797 [0.000, 3.000],  loss: 153524823350312960.000000, mse: 29049806613931151360.000000, mean_q: 4262777395.942029, mean_eps: 0.316619
 228032/300000: episode: 1869, duration: 1.217s, episode steps: 169, steps per second: 139, episode reward: -798.562, mean reward: -4.725 [-100.000,  1.815], mean action: 1.775 [0.000, 3.000],  loss: 68860646650049312.000000, mse: 29668603636687208448.000000, mean_q: 4220017539.786982, mean_eps: 0.316159
 228278/300000: episode: 1870, duration: 1.750s, episode steps: 246, steps per second: 141, episode reward: -227.311, mean reward: -0.924 [-100.000,  7.160], mean action: 1.622 [0.000, 3.000],  loss: 128312531739041200.000000, mse: 28747493268631392256.000000, mean_q: 4189031954.731707, mean_eps: 0.315537
 228636/300000: episode: 1871, duration: 2.588s, episode steps: 358, steps per second: 138, episode reward: -555.249, mean reward: -1.551 [-100.000,  3.463], mean action: 1.894 [0.000, 3.000],  loss: 99269147708177056.000000, mse: 28116836637601992704.000000, mean_q: 4138864742.256983, mean_eps: 0.314631
 228767/300000: episode: 1872, duration: 0.899s, episode steps: 131, steps per second: 146, episode reward: -469.427, mean reward: -3.583 [-100.000,  2.748], mean action: 1.893 [0.000, 3.000],  loss: 69146569537979120.000000, mse: 27206567824794406912.000000, mean_q: 4061492237.679389, mean_eps: 0.313897
 228975/300000: episode: 1873, duration: 1.827s, episode steps: 208, steps per second: 114, episode reward: -364.007, mean reward: -1.750 [-100.000,  2.377], mean action: 1.875 [0.000, 3.000],  loss: 111646717236679600.000000, mse: 28287864460132847616.000000, mean_q: 4164750429.538462, mean_eps: 0.313388
 229118/300000: episode: 1874, duration: 1.156s, episode steps: 143, steps per second: 124, episode reward: -571.292, mean reward: -3.995 [-100.000,  2.008], mean action: 1.790 [0.000, 3.000],  loss: 52219361994335984.000000, mse: 28931563956968132608.000000, mean_q: 4212330689.342658, mean_eps: 0.312862
 229412/300000: episode: 1875, duration: 2.339s, episode steps: 294, steps per second: 126, episode reward: -549.269, mean reward: -1.868 [-100.000,  3.167], mean action: 1.854 [0.000, 3.000],  loss: 93230644100351408.000000, mse: 28708233497905819648.000000, mean_q: 4195710870.639456, mean_eps: 0.312206
 229783/300000: episode: 1876, duration: 2.666s, episode steps: 371, steps per second: 139, episode reward: -1613.943, mean reward: -4.350 [-100.000,  1.799], mean action: 1.849 [0.000, 3.000],  loss: 55671465338069056.000000, mse: 28252189940854984704.000000, mean_q: 4212191032.582210, mean_eps: 0.311209
 229894/300000: episode: 1877, duration: 0.800s, episode steps: 111, steps per second: 139, episode reward: -529.191, mean reward: -4.767 [-100.000,  1.127], mean action: 1.802 [0.000, 3.000],  loss: 90331950806872768.000000, mse: 31237418251051593728.000000, mean_q: 4379672061.693694, mean_eps: 0.310486
 230017/300000: episode: 1878, duration: 0.911s, episode steps: 123, steps per second: 135, episode reward: -503.460, mean reward: -4.093 [-100.000,  1.086], mean action: 1.780 [0.000, 3.000],  loss: 73071754905830304.000000, mse: 29732190437265559552.000000, mean_q: 4256830424.455285, mean_eps: 0.310135
 230136/300000: episode: 1879, duration: 0.811s, episode steps: 119, steps per second: 147, episode reward: -396.185, mean reward: -3.329 [-100.000,  1.438], mean action: 1.807 [0.000, 3.000],  loss: 96385251466155392.000000, mse: 28904115445530288128.000000, mean_q: 4231311138.420168, mean_eps: 0.309772
 230266/300000: episode: 1880, duration: 1.007s, episode steps: 130, steps per second: 129, episode reward: -374.568, mean reward: -2.881 [-100.000,  1.463], mean action: 1.885 [0.000, 3.000],  loss: 121811347217188832.000000, mse: 30318847394578141184.000000, mean_q: 4347302608.738461, mean_eps: 0.309398
 230399/300000: episode: 1881, duration: 1.013s, episode steps: 133, steps per second: 131, episode reward: -400.724, mean reward: -3.013 [-100.000,  3.249], mean action: 1.865 [0.000, 3.000],  loss: 93116802859366720.000000, mse: 29616914151617085440.000000, mean_q: 4270842019.609023, mean_eps: 0.309004
 231399/300000: episode: 1882, duration: 8.518s, episode steps: 1000, steps per second: 117, episode reward: -288.005, mean reward: -0.288 [-6.746,  9.278], mean action: 1.842 [0.000, 3.000],  loss: 88907305197865264.000000, mse: 29836376187901091840.000000, mean_q: 4304506699.776000, mean_eps: 0.307304
 231684/300000: episode: 1883, duration: 2.061s, episode steps: 285, steps per second: 138, episode reward: -766.542, mean reward: -2.690 [-100.000,  2.603], mean action: 1.846 [0.000, 3.000],  loss: 131371415350855984.000000, mse: 28429458138099982336.000000, mean_q: 4211717573.614035, mean_eps: 0.305377
 231942/300000: episode: 1884, duration: 1.890s, episode steps: 258, steps per second: 137, episode reward: -291.950, mean reward: -1.132 [-100.000,  9.388], mean action: 1.988 [0.000, 3.000],  loss: 115753587756277440.000000, mse: 29292603932255674368.000000, mean_q: 4249679330.232558, mean_eps: 0.304562
 232401/300000: episode: 1885, duration: 3.500s, episode steps: 459, steps per second: 131, episode reward: -645.854, mean reward: -1.407 [-100.000,  5.435], mean action: 1.826 [0.000, 3.000],  loss: 145835644896373376.000000, mse: 28328902875047333888.000000, mean_q: 4198730208.209150, mean_eps: 0.303487
 232483/300000: episode: 1886, duration: 0.614s, episode steps:  82, steps per second: 134, episode reward: -372.799, mean reward: -4.546 [-100.000,  1.119], mean action: 1.902 [0.000, 3.000],  loss: 72373489327758768.000000, mse: 27568656328592580608.000000, mean_q: 4081185998.048780, mean_eps: 0.302675
 233252/300000: episode: 1887, duration: 5.796s, episode steps: 769, steps per second: 133, episode reward: -615.002, mean reward: -0.800 [-100.000,  9.658], mean action: 1.809 [0.000, 3.000],  loss: 93937550058972000.000000, mse: 27596684993446875136.000000, mean_q: 4147457665.498049, mean_eps: 0.301399
 233371/300000: episode: 1888, duration: 0.829s, episode steps: 119, steps per second: 144, episode reward: -535.491, mean reward: -4.500 [-100.000,  0.873], mean action: 1.882 [0.000, 3.000],  loss: 249355571342573280.000000, mse: 27516034431677071360.000000, mean_q: 4117378824.605042, mean_eps: 0.300067
 233661/300000: episode: 1889, duration: 2.101s, episode steps: 290, steps per second: 138, episode reward: -588.301, mean reward: -2.029 [-100.000,  2.168], mean action: 1.793 [0.000, 3.000],  loss: 137008960038688608.000000, mse: 27462842809688539136.000000, mean_q: 4124956713.489655, mean_eps: 0.299453
 233905/300000: episode: 1890, duration: 1.764s, episode steps: 244, steps per second: 138, episode reward: -896.079, mean reward: -3.672 [-100.000,  2.370], mean action: 1.852 [0.000, 3.000],  loss: 126441690984560272.000000, mse: 27767389417604587520.000000, mean_q: 4159741321.442623, mean_eps: 0.298652
 233975/300000: episode: 1891, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -89.831, mean reward: -1.283 [-100.000, 13.121], mean action: 1.643 [0.000, 3.000],  loss: 40702691823515824.000000, mse: 25643045054382874624.000000, mean_q: 3993002112.000000, mean_eps: 0.298181
 234089/300000: episode: 1892, duration: 0.782s, episode steps: 114, steps per second: 146, episode reward: -350.107, mean reward: -3.071 [-100.000,  1.586], mean action: 1.868 [0.000, 3.000],  loss: 162054722734039456.000000, mse: 25548237445689389056.000000, mean_q: 4034615662.035088, mean_eps: 0.297905
 234607/300000: episode: 1893, duration: 4.194s, episode steps: 518, steps per second: 124, episode reward: -400.851, mean reward: -0.774 [-100.000,  7.547], mean action: 1.840 [0.000, 3.000],  loss: 80694852908454224.000000, mse: 26533469320236093440.000000, mean_q: 4067082226.162162, mean_eps: 0.296957
 234701/300000: episode: 1894, duration: 0.706s, episode steps:  94, steps per second: 133, episode reward: -491.952, mean reward: -5.234 [-100.000,  0.692], mean action: 1.915 [0.000, 3.000],  loss: 129122458886722672.000000, mse: 25163111191190282240.000000, mean_q: 3954665984.000000, mean_eps: 0.296040
 234891/300000: episode: 1895, duration: 1.482s, episode steps: 190, steps per second: 128, episode reward: -748.850, mean reward: -3.941 [-100.000,  4.333], mean action: 1.842 [0.000, 3.000],  loss: 73261131172303056.000000, mse: 25169052196158709760.000000, mean_q: 3977239320.252632, mean_eps: 0.295613
 235013/300000: episode: 1896, duration: 0.884s, episode steps: 122, steps per second: 138, episode reward: -419.230, mean reward: -3.436 [-100.000,  1.346], mean action: 1.943 [0.000, 3.000],  loss: 92860060859926096.000000, mse: 23620176278598426624.000000, mean_q: 3856010936.655738, mean_eps: 0.295145
 235256/300000: episode: 1897, duration: 1.719s, episode steps: 243, steps per second: 141, episode reward: -768.203, mean reward: -3.161 [-100.000,  1.746], mean action: 1.872 [0.000, 3.000],  loss: 65563459768463072.000000, mse: 24843829946202656768.000000, mean_q: 3978535432.427984, mean_eps: 0.294598
 235413/300000: episode: 1898, duration: 1.091s, episode steps: 157, steps per second: 144, episode reward: -634.339, mean reward: -4.040 [-100.000,  2.406], mean action: 1.815 [0.000, 3.000],  loss: 79598865988537504.000000, mse: 24635202215079755776.000000, mean_q: 3951152374.216560, mean_eps: 0.293998
 235494/300000: episode: 1899, duration: 0.600s, episode steps:  81, steps per second: 135, episode reward: -77.724, mean reward: -0.960 [-100.000, 12.652], mean action: 1.654 [0.000, 3.000],  loss: 83446273294198448.000000, mse: 25064809327104937984.000000, mean_q: 3955662275.950617, mean_eps: 0.293641
 235716/300000: episode: 1900, duration: 1.688s, episode steps: 222, steps per second: 132, episode reward: -935.472, mean reward: -4.214 [-100.000,  1.976], mean action: 1.874 [0.000, 3.000],  loss: 105917337493057056.000000, mse: 26057175433570418688.000000, mean_q: 4057268516.900901, mean_eps: 0.293187
 235892/300000: episode: 1901, duration: 1.256s, episode steps: 176, steps per second: 140, episode reward: -708.183, mean reward: -4.024 [-100.000,  2.252], mean action: 1.909 [0.000, 3.000],  loss: 69321481281553872.000000, mse: 25821813937045336064.000000, mean_q: 4036030269.090909, mean_eps: 0.292590
 236092/300000: episode: 1902, duration: 1.414s, episode steps: 200, steps per second: 141, episode reward: -701.324, mean reward: -3.507 [-100.000,  3.420], mean action: 1.835 [0.000, 3.000],  loss: 134488990733649840.000000, mse: 25450184250527956992.000000, mean_q: 4034815991.040000, mean_eps: 0.292026
 236295/300000: episode: 1903, duration: 1.501s, episode steps: 203, steps per second: 135, episode reward: -793.080, mean reward: -3.907 [-100.000,  2.228], mean action: 1.882 [0.000, 3.000],  loss: 56890337264504448.000000, mse: 25633110166836424704.000000, mean_q: 4080196864.000000, mean_eps: 0.291421
 236406/300000: episode: 1904, duration: 0.763s, episode steps: 111, steps per second: 145, episode reward: -435.634, mean reward: -3.925 [-100.000,  1.411], mean action: 1.865 [0.000, 3.000],  loss: 77892574621195040.000000, mse: 24838174597567086592.000000, mean_q: 4019833623.063063, mean_eps: 0.290950
 237111/300000: episode: 1905, duration: 5.391s, episode steps: 705, steps per second: 131, episode reward: -751.000, mean reward: -1.065 [-100.000,  8.949], mean action: 1.858 [0.000, 3.000],  loss: 92035910287722720.000000, mse: 25922668005018554368.000000, mean_q: 4106651898.190071, mean_eps: 0.289726
 237359/300000: episode: 1906, duration: 1.768s, episode steps: 248, steps per second: 140, episode reward: -994.165, mean reward: -4.009 [-100.000,  4.192], mean action: 1.871 [0.000, 3.000],  loss: 121995724097830384.000000, mse: 26033124778293649408.000000, mean_q: 4139864284.903226, mean_eps: 0.288296
 237591/300000: episode: 1907, duration: 1.918s, episode steps: 232, steps per second: 121, episode reward: -453.851, mean reward: -1.956 [-100.000,  4.457], mean action: 1.897 [0.000, 3.000],  loss: 100387565597586112.000000, mse: 25432920782820904960.000000, mean_q: 4119276758.068965, mean_eps: 0.287577
 237730/300000: episode: 1908, duration: 1.139s, episode steps: 139, steps per second: 122, episode reward: -522.459, mean reward: -3.759 [-100.000,  1.557], mean action: 1.899 [0.000, 3.000],  loss: 83733426337312864.000000, mse: 25719775538147168256.000000, mean_q: 4124026185.669065, mean_eps: 0.287020
 237814/300000: episode: 1909, duration: 0.682s, episode steps:  84, steps per second: 123, episode reward: -482.941, mean reward: -5.749 [-100.000,  0.352], mean action: 1.881 [0.000, 3.000],  loss: 51362881433364384.000000, mse: 25690143333933907968.000000, mean_q: 4142980028.952381, mean_eps: 0.286686
 237982/300000: episode: 1910, duration: 1.313s, episode steps: 168, steps per second: 128, episode reward: -592.291, mean reward: -3.526 [-100.000,  1.798], mean action: 1.893 [0.000, 3.000],  loss: 102136917079084864.000000, mse: 26243691167084744704.000000, mean_q: 4177702997.333333, mean_eps: 0.286307
 238147/300000: episode: 1911, duration: 1.186s, episode steps: 165, steps per second: 139, episode reward: -614.281, mean reward: -3.723 [-100.000,  1.743], mean action: 1.764 [0.000, 3.000],  loss: 132247572518141152.000000, mse: 25484881810345144320.000000, mean_q: 4120987590.593939, mean_eps: 0.285808
 238453/300000: episode: 1912, duration: 2.324s, episode steps: 306, steps per second: 132, episode reward: -294.862, mean reward: -0.964 [-100.000, 13.842], mean action: 1.846 [0.000, 3.000],  loss: 100272028386561600.000000, mse: 25321559104755556352.000000, mean_q: 4115331374.013072, mean_eps: 0.285102
 238720/300000: episode: 1913, duration: 2.012s, episode steps: 267, steps per second: 133, episode reward: -544.132, mean reward: -2.038 [-100.000,  3.483], mean action: 1.876 [0.000, 3.000],  loss: 100394568876309856.000000, mse: 24545631838312624128.000000, mean_q: 4043290303.760300, mean_eps: 0.284242
 238870/300000: episode: 1914, duration: 1.221s, episode steps: 150, steps per second: 123, episode reward: -671.631, mean reward: -4.478 [-100.000,  1.825], mean action: 1.780 [0.000, 3.000],  loss: 75976631210957872.000000, mse: 25768741295923847168.000000, mean_q: 4129695366.826667, mean_eps: 0.283616
 239227/300000: episode: 1915, duration: 3.590s, episode steps: 357, steps per second:  99, episode reward: -598.131, mean reward: -1.675 [-100.000,  3.281], mean action: 1.815 [0.000, 3.000],  loss: 92183064843199632.000000, mse: 24120578448403222528.000000, mean_q: 4015351011.316526, mean_eps: 0.282856
 239324/300000: episode: 1916, duration: 0.884s, episode steps:  97, steps per second: 110, episode reward: -556.766, mean reward: -5.740 [-100.000,  0.333], mean action: 1.784 [0.000, 3.000],  loss: 128285254941720064.000000, mse: 24929846853057142784.000000, mean_q: 4069899308.865979, mean_eps: 0.282175
 239617/300000: episode: 1917, duration: 2.751s, episode steps: 293, steps per second: 107, episode reward: -424.024, mean reward: -1.447 [-100.000,  4.185], mean action: 1.812 [0.000, 3.000],  loss: 92757565296826432.000000, mse: 24458350913537998848.000000, mean_q: 4069772244.313993, mean_eps: 0.281590
 239905/300000: episode: 1918, duration: 2.601s, episode steps: 288, steps per second: 111, episode reward: -781.045, mean reward: -2.712 [-100.000,  2.662], mean action: 1.878 [0.000, 3.000],  loss: 116178433384498976.000000, mse: 24017903301432328192.000000, mean_q: 4031333248.888889, mean_eps: 0.280718
 239959/300000: episode: 1919, duration: 0.437s, episode steps:  54, steps per second: 123, episode reward: -135.612, mean reward: -2.511 [-100.000,  6.936], mean action: 0.389 [0.000, 3.000],  loss: 57478155931921064.000000, mse: 25214462890992078848.000000, mean_q: 4117458782.814815, mean_eps: 0.280205
 240224/300000: episode: 1920, duration: 2.215s, episode steps: 265, steps per second: 120, episode reward: -433.208, mean reward: -1.635 [-100.000,  5.070], mean action: 1.842 [0.000, 3.000],  loss: 68244153217911456.000000, mse: 24003740450282164224.000000, mean_q: 4025651587.381132, mean_eps: 0.279727
 240516/300000: episode: 1921, duration: 2.543s, episode steps: 292, steps per second: 115, episode reward: -864.519, mean reward: -2.961 [-100.000,  1.890], mean action: 1.842 [0.000, 3.000],  loss: 98681889766811728.000000, mse: 23614042281499553792.000000, mean_q: 4038207060.164383, mean_eps: 0.278892
 240646/300000: episode: 1922, duration: 1.021s, episode steps: 130, steps per second: 127, episode reward: -432.714, mean reward: -3.329 [-100.000,  1.779], mean action: 1.900 [0.000, 3.000],  loss: 73644155539368784.000000, mse: 23379103876298383360.000000, mean_q: 3972338089.353846, mean_eps: 0.278259
 240793/300000: episode: 1923, duration: 1.134s, episode steps: 147, steps per second: 130, episode reward: -687.057, mean reward: -4.674 [-100.000,  1.116], mean action: 1.837 [0.000, 3.000],  loss: 54375140625677888.000000, mse: 24051224175954604032.000000, mean_q: 4095609187.265306, mean_eps: 0.277843
 241041/300000: episode: 1924, duration: 1.882s, episode steps: 248, steps per second: 132, episode reward: -784.993, mean reward: -3.165 [-100.000,  2.012], mean action: 1.863 [0.000, 3.000],  loss: 70904956714906528.000000, mse: 23962289557792772096.000000, mean_q: 4102057953.032258, mean_eps: 0.277250
 241189/300000: episode: 1925, duration: 1.150s, episode steps: 148, steps per second: 129, episode reward: -460.501, mean reward: -3.111 [-100.000,  2.085], mean action: 1.797 [0.000, 3.000],  loss: 76193476514315408.000000, mse: 24108466497446019072.000000, mean_q: 4107336159.135135, mean_eps: 0.276656
 241297/300000: episode: 1926, duration: 0.834s, episode steps: 108, steps per second: 130, episode reward: -414.824, mean reward: -3.841 [-100.000,  1.160], mean action: 1.861 [0.000, 3.000],  loss: 91167868112542832.000000, mse: 24750927205541482496.000000, mean_q: 4177561258.666667, mean_eps: 0.276273
 241524/300000: episode: 1927, duration: 1.690s, episode steps: 227, steps per second: 134, episode reward: -660.814, mean reward: -2.911 [-100.000,  2.209], mean action: 1.881 [0.000, 3.000],  loss: 96457656789007376.000000, mse: 24436234521069932544.000000, mean_q: 4131236119.682819, mean_eps: 0.275770
 241735/300000: episode: 1928, duration: 1.653s, episode steps: 211, steps per second: 128, episode reward: -496.546, mean reward: -2.353 [-100.000,  4.786], mean action: 1.834 [0.000, 3.000],  loss: 84152377343421040.000000, mse: 23747375162156380160.000000, mean_q: 4092830161.895735, mean_eps: 0.275113
 241873/300000: episode: 1929, duration: 1.087s, episode steps: 138, steps per second: 127, episode reward: -286.706, mean reward: -2.078 [-100.000,  1.556], mean action: 1.899 [0.000, 3.000],  loss: 155183640803484000.000000, mse: 24344566784265158656.000000, mean_q: 4118176988.753623, mean_eps: 0.274589
 241991/300000: episode: 1930, duration: 0.909s, episode steps: 118, steps per second: 130, episode reward: -496.240, mean reward: -4.205 [-100.000,  1.162], mean action: 1.898 [0.000, 3.000],  loss: 101417187460091696.000000, mse: 22609507461316354048.000000, mean_q: 3976946573.016949, mean_eps: 0.274206
 242141/300000: episode: 1931, duration: 1.218s, episode steps: 150, steps per second: 123, episode reward: -361.523, mean reward: -2.410 [-100.000,  1.632], mean action: 1.820 [0.000, 3.000],  loss: 89876627348541936.000000, mse: 23130622141786787840.000000, mean_q: 4067231546.026667, mean_eps: 0.273803
 242441/300000: episode: 1932, duration: 2.326s, episode steps: 300, steps per second: 129, episode reward: -622.827, mean reward: -2.076 [-100.000,  2.772], mean action: 1.870 [0.000, 3.000],  loss: 79060886797180816.000000, mse: 23711345697006063616.000000, mean_q: 4085300770.133333, mean_eps: 0.273128
 242734/300000: episode: 1933, duration: 2.293s, episode steps: 293, steps per second: 128, episode reward: -1349.077, mean reward: -4.604 [-100.000,  2.097], mean action: 1.870 [0.000, 3.000],  loss: 94479882482242976.000000, mse: 23332847645195448320.000000, mean_q: 4091906234.976109, mean_eps: 0.272239
 242870/300000: episode: 1934, duration: 1.051s, episode steps: 136, steps per second: 129, episode reward: -735.701, mean reward: -5.410 [-100.000,  1.812], mean action: 1.875 [0.000, 3.000],  loss: 118360609374678320.000000, mse: 25234470838227927040.000000, mean_q: 4191171845.647059, mean_eps: 0.271595
 243177/300000: episode: 1935, duration: 2.589s, episode steps: 307, steps per second: 119, episode reward: -1162.323, mean reward: -3.786 [-100.000,  1.756], mean action: 1.876 [0.000, 3.000],  loss: 92910082723203952.000000, mse: 24004807381213282304.000000, mean_q: 4146121350.254072, mean_eps: 0.270931
 243331/300000: episode: 1936, duration: 1.222s, episode steps: 154, steps per second: 126, episode reward: -530.408, mean reward: -3.444 [-100.000,  1.729], mean action: 1.779 [0.000, 3.000],  loss: 105429336773218672.000000, mse: 24860736529600286720.000000, mean_q: 4205111704.935065, mean_eps: 0.270239
 243520/300000: episode: 1937, duration: 1.394s, episode steps: 189, steps per second: 136, episode reward: -890.772, mean reward: -4.713 [-100.000,  1.764], mean action: 1.815 [0.000, 3.000],  loss: 135994011872990288.000000, mse: 24734855267393740800.000000, mean_q: 4229185737.820106, mean_eps: 0.269725
 243767/300000: episode: 1938, duration: 1.949s, episode steps: 247, steps per second: 127, episode reward: -686.949, mean reward: -2.781 [-100.000,  4.195], mean action: 1.854 [0.000, 3.000],  loss: 80820767072619232.000000, mse: 24030512671334109184.000000, mean_q: 4161980528.971660, mean_eps: 0.269071
 243993/300000: episode: 1939, duration: 1.761s, episode steps: 226, steps per second: 128, episode reward: -587.873, mean reward: -2.601 [-100.000,  4.181], mean action: 1.889 [0.000, 3.000],  loss: 110586258004802128.000000, mse: 24988045146210947072.000000, mean_q: 4230591218.407080, mean_eps: 0.268361
 244183/300000: episode: 1940, duration: 1.528s, episode steps: 190, steps per second: 124, episode reward: -846.179, mean reward: -4.454 [-100.000,  1.339], mean action: 1.926 [0.000, 3.000],  loss: 103267739311171168.000000, mse: 24487495377211748352.000000, mean_q: 4201646477.473684, mean_eps: 0.267738
 244380/300000: episode: 1941, duration: 1.459s, episode steps: 197, steps per second: 135, episode reward: -489.212, mean reward: -2.483 [-100.000,  2.349], mean action: 1.929 [0.000, 3.000],  loss: 99545246925468000.000000, mse: 23866432600087199744.000000, mean_q: 4191273830.659898, mean_eps: 0.267157
 244493/300000: episode: 1942, duration: 0.781s, episode steps: 113, steps per second: 145, episode reward: -415.493, mean reward: -3.677 [-100.000,  1.560], mean action: 1.841 [0.000, 3.000],  loss: 102983545139828976.000000, mse: 24168765458329636864.000000, mean_q: 4211542244.814159, mean_eps: 0.266692
 244642/300000: episode: 1943, duration: 1.066s, episode steps: 149, steps per second: 140, episode reward: -524.745, mean reward: -3.522 [-100.000,  1.813], mean action: 1.819 [0.000, 3.000],  loss: 121582196125300368.000000, mse: 25200667121544462336.000000, mean_q: 4260358116.510067, mean_eps: 0.266299
 244873/300000: episode: 1944, duration: 1.731s, episode steps: 231, steps per second: 133, episode reward: -546.783, mean reward: -2.367 [-100.000,  2.848], mean action: 1.853 [0.000, 3.000],  loss: 119356822803009760.000000, mse: 23516049206752817152.000000, mean_q: 4176153581.160173, mean_eps: 0.265729
 244981/300000: episode: 1945, duration: 0.811s, episode steps: 108, steps per second: 133, episode reward: -331.083, mean reward: -3.066 [-100.000,  1.246], mean action: 1.898 [0.000, 3.000],  loss: 120777612736856064.000000, mse: 23343096177372622848.000000, mean_q: 4128437620.148148, mean_eps: 0.265220
 245097/300000: episode: 1946, duration: 0.908s, episode steps: 116, steps per second: 128, episode reward: -426.076, mean reward: -3.673 [-100.000,  1.200], mean action: 1.948 [0.000, 3.000],  loss: 57459970537459008.000000, mse: 23157812796541100032.000000, mean_q: 4162410827.034483, mean_eps: 0.264884
 245243/300000: episode: 1947, duration: 1.142s, episode steps: 146, steps per second: 128, episode reward: -582.561, mean reward: -3.990 [-100.000,  1.623], mean action: 1.877 [0.000, 3.000],  loss: 129518949268810928.000000, mse: 23915430942562246656.000000, mean_q: 4219065401.863014, mean_eps: 0.264491
 245485/300000: episode: 1948, duration: 1.766s, episode steps: 242, steps per second: 137, episode reward: -1040.368, mean reward: -4.299 [-100.000,  1.377], mean action: 1.831 [0.000, 3.000],  loss: 113037595401963232.000000, mse: 24335658128723595264.000000, mean_q: 4246477736.198347, mean_eps: 0.263910
 245539/300000: episode: 1949, duration: 0.377s, episode steps:  54, steps per second: 143, episode reward: -99.554, mean reward: -1.844 [-100.000,  6.634], mean action: 0.704 [0.000, 3.000],  loss: 141217745851859552.000000, mse: 24158490437299179520.000000, mean_q: 4259943111.111111, mean_eps: 0.263465
 245679/300000: episode: 1950, duration: 1.380s, episode steps: 140, steps per second: 101, episode reward: -450.362, mean reward: -3.217 [-100.000,  1.430], mean action: 1.814 [0.000, 3.000],  loss: 133122179883088512.000000, mse: 24010725398559215616.000000, mean_q: 4242382720.000000, mean_eps: 0.263175
 246042/300000: episode: 1951, duration: 3.156s, episode steps: 363, steps per second: 115, episode reward: -1001.450, mean reward: -2.759 [-100.000,  3.239], mean action: 1.928 [0.000, 3.000],  loss: 124978175497343568.000000, mse: 23610254844636200960.000000, mean_q: 4196345480.815427, mean_eps: 0.262420
 246378/300000: episode: 1952, duration: 2.606s, episode steps: 336, steps per second: 129, episode reward: -1827.911, mean reward: -5.440 [-100.000,  3.229], mean action: 1.842 [0.000, 3.000],  loss: 95155512985230976.000000, mse: 23589184581297065984.000000, mean_q: 4199060962.285714, mean_eps: 0.261371
 246690/300000: episode: 1953, duration: 2.273s, episode steps: 312, steps per second: 137, episode reward: -659.371, mean reward: -2.113 [-100.000,  4.024], mean action: 1.856 [0.000, 3.000],  loss: 106689816377943488.000000, mse: 24696789962612465664.000000, mean_q: 4274405184.820513, mean_eps: 0.260400
 246850/300000: episode: 1954, duration: 1.166s, episode steps: 160, steps per second: 137, episode reward: -521.216, mean reward: -3.258 [-100.000,  3.907], mean action: 1.969 [0.000, 3.000],  loss: 96807580935074608.000000, mse: 23409049953504329728.000000, mean_q: 4200566531.200000, mean_eps: 0.259691
 247095/300000: episode: 1955, duration: 1.926s, episode steps: 245, steps per second: 127, episode reward: -865.832, mean reward: -3.534 [-100.000,  1.769], mean action: 1.914 [0.000, 3.000],  loss: 85370421856700512.000000, mse: 23852853604557348864.000000, mean_q: 4237881584.326530, mean_eps: 0.259084
 247579/300000: episode: 1956, duration: 3.697s, episode steps: 484, steps per second: 131, episode reward: -3365.791, mean reward: -6.954 [-100.000,  3.031], mean action: 1.878 [0.000, 3.000],  loss: 103662922003206784.000000, mse: 25634534833669918720.000000, mean_q: 4355434240.528926, mean_eps: 0.257991
 247669/300000: episode: 1957, duration: 0.671s, episode steps:  90, steps per second: 134, episode reward: -493.443, mean reward: -5.483 [-100.000, -0.048], mean action: 1.922 [0.000, 3.000],  loss: 99830572487552384.000000, mse: 27457842013430697984.000000, mean_q: 4461598387.200000, mean_eps: 0.257130
 247833/300000: episode: 1958, duration: 1.157s, episode steps: 164, steps per second: 142, episode reward: -790.961, mean reward: -4.823 [-100.000,  2.212], mean action: 1.878 [0.000, 3.000],  loss: 179656147820993152.000000, mse: 26610463810709983232.000000, mean_q: 4385101174.634147, mean_eps: 0.256748
 248060/300000: episode: 1959, duration: 1.867s, episode steps: 227, steps per second: 122, episode reward: -493.292, mean reward: -2.173 [-100.000,  5.007], mean action: 1.885 [0.000, 3.000],  loss: 83614106753895520.000000, mse: 26821841078204973056.000000, mean_q: 4453577545.303965, mean_eps: 0.256162
 248153/300000: episode: 1960, duration: 0.679s, episode steps:  93, steps per second: 137, episode reward: -349.334, mean reward: -3.756 [-100.000,  1.177], mean action: 1.968 [0.000, 3.000],  loss: 192654938433413920.000000, mse: 27001353426082603008.000000, mean_q: 4481044997.505377, mean_eps: 0.255682
 248312/300000: episode: 1961, duration: 1.147s, episode steps: 159, steps per second: 139, episode reward: -569.887, mean reward: -3.584 [-100.000,  3.930], mean action: 1.981 [0.000, 3.000],  loss: 140137682960014688.000000, mse: 27759275902625230848.000000, mean_q: 4521715979.270440, mean_eps: 0.255304
 248728/300000: episode: 1962, duration: 3.108s, episode steps: 416, steps per second: 134, episode reward: -3157.134, mean reward: -7.589 [-100.000,  2.513], mean action: 1.904 [0.000, 3.000],  loss: 130401789744049232.000000, mse: 27382465710937927680.000000, mean_q: 4475081011.076923, mean_eps: 0.254441
 249464/300000: episode: 1963, duration: 5.686s, episode steps: 736, steps per second: 129, episode reward: -2311.732, mean reward: -3.141 [-100.000,  7.313], mean action: 1.880 [0.000, 3.000],  loss: 162982013600484832.000000, mse: 29533996161702035456.000000, mean_q: 4585191975.304348, mean_eps: 0.252713
 249570/300000: episode: 1964, duration: 0.856s, episode steps: 106, steps per second: 124, episode reward: -472.013, mean reward: -4.453 [-100.000,  1.109], mean action: 1.849 [0.000, 3.000],  loss: 194130274548988416.000000, mse: 29519734517609680896.000000, mean_q: 4565588513.811320, mean_eps: 0.251450
 249688/300000: episode: 1965, duration: 0.876s, episode steps: 118, steps per second: 135, episode reward: -524.121, mean reward: -4.442 [-100.000,  1.055], mean action: 1.873 [0.000, 3.000],  loss: 154321768040303872.000000, mse: 29076059169077178368.000000, mean_q: 4577185883.118644, mean_eps: 0.251115
 250062/300000: episode: 1966, duration: 2.766s, episode steps: 374, steps per second: 135, episode reward: -841.698, mean reward: -2.251 [-100.000,  4.304], mean action: 1.810 [0.000, 3.000],  loss: 100261265993935296.000000, mse: 29022708165348794368.000000, mean_q: 4604113978.866310, mean_eps: 0.250376
 250237/300000: episode: 1967, duration: 1.234s, episode steps: 175, steps per second: 142, episode reward: -876.321, mean reward: -5.008 [-100.000,  1.774], mean action: 1.943 [0.000, 3.000],  loss: 91738233721302544.000000, mse: 30038259999474786304.000000, mean_q: 4692063672.320000, mean_eps: 0.249553
 250712/300000: episode: 1968, duration: 3.580s, episode steps: 475, steps per second: 133, episode reward: -1376.757, mean reward: -2.898 [-100.000,  4.654], mean action: 1.821 [0.000, 3.000],  loss: 100252295734682480.000000, mse: 29695439357103874048.000000, mean_q: 4674434076.564211, mean_eps: 0.248578
 250992/300000: episode: 1969, duration: 2.113s, episode steps: 280, steps per second: 132, episode reward: -787.275, mean reward: -2.812 [-100.000,  2.232], mean action: 1.904 [0.000, 3.000],  loss: 157127263738411648.000000, mse: 31157551768395489280.000000, mean_q: 4785569759.085714, mean_eps: 0.247445
 251046/300000: episode: 1970, duration: 0.394s, episode steps:  54, steps per second: 137, episode reward: -107.904, mean reward: -1.998 [-100.000, 10.231], mean action: 0.722 [0.000, 3.000],  loss: 109439108047447536.000000, mse: 29607450528735354880.000000, mean_q: 4693052690.962963, mean_eps: 0.246944
 251262/300000: episode: 1971, duration: 1.501s, episode steps: 216, steps per second: 144, episode reward: -772.196, mean reward: -3.575 [-100.000,  2.706], mean action: 1.847 [0.000, 3.000],  loss: 134147979960618560.000000, mse: 29019891815094321152.000000, mean_q: 4678975623.111111, mean_eps: 0.246539
 251519/300000: episode: 1972, duration: 1.864s, episode steps: 257, steps per second: 138, episode reward: -838.351, mean reward: -3.262 [-100.000,  3.003], mean action: 1.848 [0.000, 3.000],  loss: 198611402873332896.000000, mse: 30206840278571077632.000000, mean_q: 4765898705.182879, mean_eps: 0.245830
 251699/300000: episode: 1973, duration: 1.238s, episode steps: 180, steps per second: 145, episode reward: -916.919, mean reward: -5.094 [-100.000,  1.423], mean action: 1.900 [0.000, 3.000],  loss: 160964052386234720.000000, mse: 29811517199155490816.000000, mean_q: 4732911079.822222, mean_eps: 0.245174
 252560/300000: episode: 1974, duration: 6.856s, episode steps: 861, steps per second: 126, episode reward: -2960.866, mean reward: -3.439 [-100.000,  3.025], mean action: 1.884 [0.000, 3.000],  loss: 122447157135339728.000000, mse: 29661935163709493248.000000, mean_q: 4794774580.329849, mean_eps: 0.243613
 252612/300000: episode: 1975, duration: 0.360s, episode steps:  52, steps per second: 144, episode reward: -128.703, mean reward: -2.475 [-100.000,  7.080], mean action: 0.481 [0.000, 3.000],  loss: 209075356286905408.000000, mse: 31242461021312303104.000000, mean_q: 4939316544.000000, mean_eps: 0.242244
 253013/300000: episode: 1976, duration: 2.972s, episode steps: 401, steps per second: 135, episode reward: -1672.219, mean reward: -4.170 [-100.000,  3.865], mean action: 1.865 [0.000, 3.000],  loss: 155915810084143968.000000, mse: 29720595948427124736.000000, mean_q: 4837359797.306733, mean_eps: 0.241564
 253244/300000: episode: 1977, duration: 1.749s, episode steps: 231, steps per second: 132, episode reward: -601.422, mean reward: -2.604 [-100.000,  4.435], mean action: 1.896 [0.000, 3.000],  loss: 124726390363329232.000000, mse: 29459886487622643712.000000, mean_q: 4836058927.653680, mean_eps: 0.240616
 253590/300000: episode: 1978, duration: 2.620s, episode steps: 346, steps per second: 132, episode reward: -2137.777, mean reward: -6.179 [-100.000,  1.517], mean action: 1.818 [0.000, 3.000],  loss: 200227306503493440.000000, mse: 30943620862730567680.000000, mean_q: 4929906406.843930, mean_eps: 0.239750
 254144/300000: episode: 1979, duration: 4.251s, episode steps: 554, steps per second: 130, episode reward: -2886.075, mean reward: -5.210 [-100.000,  3.902], mean action: 1.875 [0.000, 3.000],  loss: 143690292350322416.000000, mse: 30148130730634895360.000000, mean_q: 4885151429.776174, mean_eps: 0.238400
 254326/300000: episode: 1980, duration: 1.505s, episode steps: 182, steps per second: 121, episode reward: -737.428, mean reward: -4.052 [-100.000,  1.625], mean action: 1.940 [0.000, 3.000],  loss: 114586174035556592.000000, mse: 31259649801373507584.000000, mean_q: 4949612404.747252, mean_eps: 0.237296
 254380/300000: episode: 1981, duration: 0.421s, episode steps:  54, steps per second: 128, episode reward: -125.481, mean reward: -2.324 [-100.000,  5.620], mean action: 0.278 [0.000, 3.000],  loss: 55823384877407344.000000, mse: 32072806918061400064.000000, mean_q: 4952483569.777778, mean_eps: 0.236943
 254855/300000: episode: 1982, duration: 3.732s, episode steps: 475, steps per second: 127, episode reward: -2093.980, mean reward: -4.408 [-100.000,  1.861], mean action: 1.863 [0.000, 3.000],  loss: 146379399420478336.000000, mse: 30568409852096626688.000000, mean_q: 4912145863.949473, mean_eps: 0.236149
 255072/300000: episode: 1983, duration: 1.575s, episode steps: 217, steps per second: 138, episode reward: -880.299, mean reward: -4.057 [-100.000,  1.996], mean action: 1.848 [0.000, 3.000],  loss: 109126539937681696.000000, mse: 30721462929371672576.000000, mean_q: 4932513998.451612, mean_eps: 0.235111
 255134/300000: episode: 1984, duration: 0.427s, episode steps:  62, steps per second: 145, episode reward: -146.076, mean reward: -2.356 [-100.000, 15.366], mean action: 0.435 [0.000, 3.000],  loss: 169025150500301664.000000, mse: 31362986334743326720.000000, mean_q: 4991186522.838710, mean_eps: 0.234693
 255713/300000: episode: 1985, duration: 4.406s, episode steps: 579, steps per second: 131, episode reward: -2347.000, mean reward: -4.054 [-100.000,  3.265], mean action: 1.895 [0.000, 3.000],  loss: 133473096571011152.000000, mse: 31136264783063388160.000000, mean_q: 4999052108.048359, mean_eps: 0.233731
 255926/300000: episode: 1986, duration: 1.744s, episode steps: 213, steps per second: 122, episode reward: -437.116, mean reward: -2.052 [-100.000,  5.089], mean action: 1.859 [0.000, 3.000],  loss: 128873787817767392.000000, mse: 31795757294408060928.000000, mean_q: 5086474050.103287, mean_eps: 0.232543
 256318/300000: episode: 1987, duration: 3.077s, episode steps: 392, steps per second: 127, episode reward: -2816.891, mean reward: -7.186 [-100.000,  1.725], mean action: 1.957 [0.000, 3.000],  loss: 105713803623201472.000000, mse: 31559257179262586880.000000, mean_q: 5076257710.367347, mean_eps: 0.231636
 257129/300000: episode: 1988, duration: 6.480s, episode steps: 811, steps per second: 125, episode reward: -695.594, mean reward: -0.858 [-100.000, 16.111], mean action: 1.266 [0.000, 3.000],  loss: 125654290953205952.000000, mse: 33268424514538971136.000000, mean_q: 5216310299.146732, mean_eps: 0.229831
 257182/300000: episode: 1989, duration: 0.427s, episode steps:  53, steps per second: 124, episode reward: -112.641, mean reward: -2.125 [-100.000,  5.325], mean action: 0.566 [0.000, 3.000],  loss: 99699057730251952.000000, mse: 33326691118611431424.000000, mean_q: 5241576882.716981, mean_eps: 0.228535
 257702/300000: episode: 1990, duration: 4.191s, episode steps: 520, steps per second: 124, episode reward: -2587.059, mean reward: -4.975 [-100.000,  3.555], mean action: 1.860 [0.000, 3.000],  loss: 153644722114720800.000000, mse: 33531876142717579264.000000, mean_q: 5268143738.092308, mean_eps: 0.227675
 258086/300000: episode: 1991, duration: 2.712s, episode steps: 384, steps per second: 142, episode reward: -2380.105, mean reward: -6.198 [-100.000,  1.979], mean action: 1.844 [0.000, 3.000],  loss: 127313599931613184.000000, mse: 34560322125362974720.000000, mean_q: 5370760861.333333, mean_eps: 0.226320
 258286/300000: episode: 1992, duration: 1.402s, episode steps: 200, steps per second: 143, episode reward: -683.217, mean reward: -3.416 [-100.000,  1.464], mean action: 1.920 [0.000, 3.000],  loss: 160078967648834016.000000, mse: 34645206633503784960.000000, mean_q: 5413470675.200000, mean_eps: 0.225444
 258392/300000: episode: 1993, duration: 0.803s, episode steps: 106, steps per second: 132, episode reward: -512.700, mean reward: -4.837 [-100.000,  0.581], mean action: 1.849 [0.000, 3.000],  loss: 69305077146578096.000000, mse: 34060012494052376576.000000, mean_q: 5390223244.075472, mean_eps: 0.224985
 258695/300000: episode: 1994, duration: 2.503s, episode steps: 303, steps per second: 121, episode reward: -1293.596, mean reward: -4.269 [-100.000,  1.573], mean action: 1.868 [0.000, 3.000],  loss: 144491666559732256.000000, mse: 34695055761432616960.000000, mean_q: 5416764819.009901, mean_eps: 0.224371
 259045/300000: episode: 1995, duration: 2.499s, episode steps: 350, steps per second: 140, episode reward: -2050.951, mean reward: -5.860 [-100.000,  1.940], mean action: 1.903 [0.000, 3.000],  loss: 165284624535916928.000000, mse: 33600391775314108416.000000, mean_q: 5367676941.897142, mean_eps: 0.223392
 259442/300000: episode: 1996, duration: 3.047s, episode steps: 397, steps per second: 130, episode reward: -1576.141, mean reward: -3.970 [-100.000,  2.312], mean action: 1.884 [0.000, 3.000],  loss: 148082510811167936.000000, mse: 34935202511331893248.000000, mean_q: 5487827981.541562, mean_eps: 0.222271
 259913/300000: episode: 1997, duration: 3.746s, episode steps: 471, steps per second: 126, episode reward: -2847.698, mean reward: -6.046 [-100.000,  5.022], mean action: 1.890 [0.000, 3.000],  loss: 84150560043027968.000000, mse: 36014802677745270784.000000, mean_q: 5600849154.174098, mean_eps: 0.220969
 260086/300000: episode: 1998, duration: 1.267s, episode steps: 173, steps per second: 137, episode reward: -920.821, mean reward: -5.323 [-100.000,  1.395], mean action: 1.809 [0.000, 3.000],  loss: 78965164461991168.000000, mse: 36557788740072615936.000000, mean_q: 5652978270.705202, mean_eps: 0.220003
 260285/300000: episode: 1999, duration: 1.462s, episode steps: 199, steps per second: 136, episode reward: -601.188, mean reward: -3.021 [-100.000,  1.491], mean action: 1.839 [0.000, 3.000],  loss: 110731573499863088.000000, mse: 37544795054155120640.000000, mean_q: 5774729987.859297, mean_eps: 0.219445
 260493/300000: episode: 2000, duration: 1.555s, episode steps: 208, steps per second: 134, episode reward: -433.424, mean reward: -2.084 [-100.000,  3.933], mean action: 1.971 [0.000, 3.000],  loss: 172126306834740768.000000, mse: 36951835634947932160.000000, mean_q: 5702439864.615385, mean_eps: 0.218834
 260688/300000: episode: 2001, duration: 1.394s, episode steps: 195, steps per second: 140, episode reward: -568.404, mean reward: -2.915 [-100.000,  4.708], mean action: 1.826 [0.000, 3.000],  loss: 154565433976925184.000000, mse: 37150234600154169344.000000, mean_q: 5750564018.543590, mean_eps: 0.218230
 261108/300000: episode: 2002, duration: 3.047s, episode steps: 420, steps per second: 138, episode reward: -1410.216, mean reward: -3.358 [-100.000,  2.669], mean action: 1.898 [0.000, 3.000],  loss: 141753009371780912.000000, mse: 36208100327939731456.000000, mean_q: 5709629269.333333, mean_eps: 0.217307
 261664/300000: episode: 2003, duration: 4.380s, episode steps: 556, steps per second: 127, episode reward: -2429.400, mean reward: -4.369 [-100.000,  2.928], mean action: 1.915 [0.000, 3.000],  loss: 125469412554159696.000000, mse: 36103724308687306752.000000, mean_q: 5726403044.834533, mean_eps: 0.215843
 261749/300000: episode: 2004, duration: 0.608s, episode steps:  85, steps per second: 140, episode reward: 24.056, mean reward:  0.283 [-100.000, 16.668], mean action: 1.718 [0.000, 3.000],  loss: 129396332038612576.000000, mse: 35913538009077288960.000000, mean_q: 5761662319.435294, mean_eps: 0.214882
 261944/300000: episode: 2005, duration: 1.482s, episode steps: 195, steps per second: 132, episode reward: -562.756, mean reward: -2.886 [-100.000,  3.636], mean action: 1.877 [0.000, 3.000],  loss: 79446660903148432.000000, mse: 36403106982003339264.000000, mean_q: 5781284309.989743, mean_eps: 0.214462
 262250/300000: episode: 2006, duration: 2.327s, episode steps: 306, steps per second: 132, episode reward: -1735.325, mean reward: -5.671 [-100.000,  1.819], mean action: 1.873 [0.000, 3.000],  loss: 152963034697799744.000000, mse: 36871319772420833280.000000, mean_q: 5831399362.928104, mean_eps: 0.213710
 262306/300000: episode: 2007, duration: 0.407s, episode steps:  56, steps per second: 138, episode reward: -118.531, mean reward: -2.117 [-100.000,  7.338], mean action: 0.714 [0.000, 3.000],  loss: 96016103766416240.000000, mse: 38430397202669576192.000000, mean_q: 5953167387.428572, mean_eps: 0.213167
 262447/300000: episode: 2008, duration: 1.062s, episode steps: 141, steps per second: 133, episode reward: -628.226, mean reward: -4.456 [-100.000,  1.378], mean action: 1.908 [0.000, 3.000],  loss: 126856696636960688.000000, mse: 36979227377933049856.000000, mean_q: 5856822711.375887, mean_eps: 0.212872
 263041/300000: episode: 2009, duration: 4.796s, episode steps: 594, steps per second: 124, episode reward: -2847.940, mean reward: -4.795 [-100.000,  4.376], mean action: 1.884 [0.000, 3.000],  loss: 142318498504651264.000000, mse: 37098395159397908480.000000, mean_q: 5890462982.033670, mean_eps: 0.211769
 263616/300000: episode: 2010, duration: 4.930s, episode steps: 575, steps per second: 117, episode reward: -2198.378, mean reward: -3.823 [-100.000,  3.572], mean action: 1.875 [0.000, 3.000],  loss: 122638170738444608.000000, mse: 37557785850925654016.000000, mean_q: 5949544361.627826, mean_eps: 0.210016
 263804/300000: episode: 2011, duration: 1.398s, episode steps: 188, steps per second: 134, episode reward: -614.935, mean reward: -3.271 [-100.000,  4.568], mean action: 1.862 [0.000, 3.000],  loss: 146750908745021408.000000, mse: 37338460245847539712.000000, mean_q: 5947051618.042553, mean_eps: 0.208872
 264172/300000: episode: 2012, duration: 2.697s, episode steps: 368, steps per second: 136, episode reward: -1415.924, mean reward: -3.848 [-100.000,  4.254], mean action: 1.812 [0.000, 3.000],  loss: 128875393055992960.000000, mse: 38225984866926452736.000000, mean_q: 6069287490.782609, mean_eps: 0.208038
 264561/300000: episode: 2013, duration: 3.086s, episode steps: 389, steps per second: 126, episode reward: -2450.423, mean reward: -6.299 [-100.000,  2.392], mean action: 1.897 [0.000, 3.000],  loss: 109215833830088048.000000, mse: 39029357037916151808.000000, mean_q: 6151777685.388175, mean_eps: 0.206902
 264702/300000: episode: 2014, duration: 1.044s, episode steps: 141, steps per second: 135, episode reward: -589.769, mean reward: -4.183 [-100.000,  1.527], mean action: 1.943 [0.000, 3.000],  loss: 129244570739181152.000000, mse: 39534987711377899520.000000, mean_q: 6213841723.914893, mean_eps: 0.206107
 265268/300000: episode: 2015, duration: 4.398s, episode steps: 566, steps per second: 129, episode reward: -2866.001, mean reward: -5.064 [-100.000,  4.691], mean action: 1.910 [0.000, 3.000],  loss: 100383984881500160.000000, mse: 40338887137759600640.000000, mean_q: 6263140221.738516, mean_eps: 0.205046
 265474/300000: episode: 2016, duration: 1.563s, episode steps: 206, steps per second: 132, episode reward: -648.685, mean reward: -3.149 [-100.000,  1.818], mean action: 1.845 [0.000, 3.000],  loss: 141357551465129760.000000, mse: 40223023547594842112.000000, mean_q: 6304676657.708738, mean_eps: 0.203889
 265611/300000: episode: 2017, duration: 1.006s, episode steps: 137, steps per second: 136, episode reward: -524.397, mean reward: -3.828 [-100.000,  1.658], mean action: 1.869 [0.000, 3.000],  loss: 125420013811819744.000000, mse: 40647739027622707200.000000, mean_q: 6361702556.963504, mean_eps: 0.203374
 265897/300000: episode: 2018, duration: 2.201s, episode steps: 286, steps per second: 130, episode reward: -1046.575, mean reward: -3.659 [-100.000,  2.546], mean action: 1.962 [0.000, 3.000],  loss: 170225850889782976.000000, mse: 40632202661455929344.000000, mean_q: 6335809958.489511, mean_eps: 0.202739
 266338/300000: episode: 2019, duration: 3.268s, episode steps: 441, steps per second: 135, episode reward: -3011.323, mean reward: -6.828 [-100.000,  1.228], mean action: 1.923 [0.000, 3.000],  loss: 119934164545956928.000000, mse: 41642576493863133184.000000, mean_q: 6456208414.185941, mean_eps: 0.201649
 266876/300000: episode: 2020, duration: 3.913s, episode steps: 538, steps per second: 137, episode reward: -2276.487, mean reward: -4.231 [-100.000,  2.182], mean action: 1.855 [0.000, 3.000],  loss: 129424808270925024.000000, mse: 42572601149182550016.000000, mean_q: 6545181220.163568, mean_eps: 0.200180
 267277/300000: episode: 2021, duration: 2.890s, episode steps: 401, steps per second: 139, episode reward: -2764.780, mean reward: -6.895 [-100.000,  1.923], mean action: 1.928 [0.000, 3.000],  loss: 123739556129453712.000000, mse: 42656938167127490560.000000, mean_q: 6583116386.314215, mean_eps: 0.198772
 267771/300000: episode: 2022, duration: 4.121s, episode steps: 494, steps per second: 120, episode reward: -2602.862, mean reward: -5.269 [-100.000,  2.283], mean action: 1.891 [0.000, 3.000],  loss: 133322577437953792.000000, mse: 44574966627874652160.000000, mean_q: 6744830092.955465, mean_eps: 0.197429
 268122/300000: episode: 2023, duration: 2.926s, episode steps: 351, steps per second: 120, episode reward: -2404.246, mean reward: -6.850 [-100.000,  1.859], mean action: 1.917 [0.000, 3.000],  loss: 152306472041580000.000000, mse: 45214240565837971456.000000, mean_q: 6828749303.247863, mean_eps: 0.196162
 268611/300000: episode: 2024, duration: 4.410s, episode steps: 489, steps per second: 111, episode reward: -3515.264, mean reward: -7.189 [-100.000,  0.594], mean action: 1.881 [0.000, 3.000],  loss: 143632600244348240.000000, mse: 46579037738459086848.000000, mean_q: 6925336479.672802, mean_eps: 0.194902
 269014/300000: episode: 2025, duration: 3.177s, episode steps: 403, steps per second: 127, episode reward: -1952.822, mean reward: -4.846 [-100.000,  1.813], mean action: 1.886 [0.000, 3.000],  loss: 129590706655069136.000000, mse: 46729673382892437504.000000, mean_q: 6975842635.593052, mean_eps: 0.193564
 269581/300000: episode: 2026, duration: 4.533s, episode steps: 567, steps per second: 125, episode reward: -3108.969, mean reward: -5.483 [-100.000,  4.513], mean action: 1.905 [0.000, 3.000],  loss: 157802483230519136.000000, mse: 46630693987573809152.000000, mean_q: 7008872167.167548, mean_eps: 0.192109
 270132/300000: episode: 2027, duration: 4.147s, episode steps: 551, steps per second: 133, episode reward: -3844.460, mean reward: -6.977 [-100.000,  1.922], mean action: 1.926 [0.000, 3.000],  loss: 142780257351785232.000000, mse: 47657154852906418176.000000, mean_q: 7110336196.994555, mean_eps: 0.190432
 270436/300000: episode: 2028, duration: 2.283s, episode steps: 304, steps per second: 133, episode reward: -1269.976, mean reward: -4.178 [-100.000,  1.979], mean action: 1.957 [0.000, 3.000],  loss: 169948579568508512.000000, mse: 48216306603274092544.000000, mean_q: 7178554054.736842, mean_eps: 0.189149
 270607/300000: episode: 2029, duration: 1.231s, episode steps: 171, steps per second: 139, episode reward: -852.628, mean reward: -4.986 [-100.000,  0.632], mean action: 1.860 [0.000, 3.000],  loss: 109968059822559632.000000, mse: 47758247136756441088.000000, mean_q: 7157360565.146199, mean_eps: 0.188437
 270663/300000: episode: 2030, duration: 0.443s, episode steps:  56, steps per second: 126, episode reward: -129.022, mean reward: -2.304 [-100.000,  6.691], mean action: 0.536 [0.000, 3.000],  loss: 194606857296281600.000000, mse: 48101010673138851840.000000, mean_q: 7188567899.428572, mean_eps: 0.188096
 270857/300000: episode: 2031, duration: 1.394s, episode steps: 194, steps per second: 139, episode reward: -690.700, mean reward: -3.560 [-100.000,  3.109], mean action: 1.887 [0.000, 3.000],  loss: 172194374920272896.000000, mse: 47884998242408251392.000000, mean_q: 7152803037.690722, mean_eps: 0.187722
 271077/300000: episode: 2032, duration: 1.631s, episode steps: 220, steps per second: 135, episode reward: -938.267, mean reward: -4.265 [-100.000,  2.996], mean action: 1.864 [0.000, 3.000],  loss: 115242138368012816.000000, mse: 48435909817475637248.000000, mean_q: 7208767136.581819, mean_eps: 0.187101
 271291/300000: episode: 2033, duration: 1.711s, episode steps: 214, steps per second: 125, episode reward: -1087.072, mean reward: -5.080 [-100.000,  1.637], mean action: 1.864 [0.000, 3.000],  loss: 151062019984419744.000000, mse: 48956621672716517376.000000, mean_q: 7261797311.401869, mean_eps: 0.186450
 271508/300000: episode: 2034, duration: 1.773s, episode steps: 217, steps per second: 122, episode reward: -794.299, mean reward: -3.660 [-100.000,  3.756], mean action: 1.912 [0.000, 3.000],  loss: 140818067550493856.000000, mse: 47174537867247443968.000000, mean_q: 7155695910.930876, mean_eps: 0.185803
 272012/300000: episode: 2035, duration: 4.076s, episode steps: 504, steps per second: 124, episode reward: -3840.570, mean reward: -7.620 [-100.000,  2.934], mean action: 1.835 [0.000, 3.000],  loss: 116374724418149488.000000, mse: 47952244406893838336.000000, mean_q: 7236172890.412699, mean_eps: 0.184721
 272129/300000: episode: 2036, duration: 1.027s, episode steps: 117, steps per second: 114, episode reward: -540.990, mean reward: -4.624 [-100.000,  1.359], mean action: 1.889 [0.000, 3.000],  loss: 191122639744903296.000000, mse: 48642370016808648704.000000, mean_q: 7295497592.341881, mean_eps: 0.183790
 272308/300000: episode: 2037, duration: 1.680s, episode steps: 179, steps per second: 107, episode reward: -837.310, mean reward: -4.678 [-100.000,  1.789], mean action: 1.916 [0.000, 3.000],  loss: 142493055895650208.000000, mse: 47747370498110046208.000000, mean_q: 7231185164.871509, mean_eps: 0.183346
 272463/300000: episode: 2038, duration: 1.203s, episode steps: 155, steps per second: 129, episode reward: -509.182, mean reward: -3.285 [-100.000,  2.362], mean action: 1.839 [0.000, 3.000],  loss: 155832349895334720.000000, mse: 47951632574359289856.000000, mean_q: 7284772788.025806, mean_eps: 0.182845
 272867/300000: episode: 2039, duration: 3.812s, episode steps: 404, steps per second: 106, episode reward: -2136.608, mean reward: -5.289 [-100.000,  1.926], mean action: 1.926 [0.000, 3.000],  loss: 185856818133317888.000000, mse: 47309855997412794368.000000, mean_q: 7219254421.544555, mean_eps: 0.182006
 273306/300000: episode: 2040, duration: 3.315s, episode steps: 439, steps per second: 132, episode reward: -1976.284, mean reward: -4.502 [-100.000,  2.946], mean action: 1.927 [0.000, 3.000],  loss: 140729663744284880.000000, mse: 47572184474373758976.000000, mean_q: 7269550701.630980, mean_eps: 0.180742
 273951/300000: episode: 2041, duration: 4.885s, episode steps: 645, steps per second: 132, episode reward: -2747.032, mean reward: -4.259 [-100.000,  4.176], mean action: 1.473 [0.000, 3.000],  loss: 125198156661633792.000000, mse: 47962789470231953408.000000, mean_q: 7314673706.865116, mean_eps: 0.179116
 274951/300000: episode: 2042, duration: 8.478s, episode steps: 1000, steps per second: 118, episode reward: -1662.784, mean reward: -1.663 [-14.287, 18.614], mean action: 1.090 [0.000, 3.000],  loss: 147951370886602304.000000, mse: 48243359246038474752.000000, mean_q: 7380995077.632000, mean_eps: 0.176648
 275312/300000: episode: 2043, duration: 2.794s, episode steps: 361, steps per second: 129, episode reward: -2249.031, mean reward: -6.230 [-100.000,  1.996], mean action: 1.914 [0.000, 3.000],  loss: 126307192861768960.000000, mse: 49269425036913090560.000000, mean_q: 7480503893.096952, mean_eps: 0.174607
 275858/300000: episode: 2044, duration: 4.603s, episode steps: 546, steps per second: 119, episode reward: -4686.489, mean reward: -8.583 [-100.000,  3.271], mean action: 1.879 [0.000, 3.000],  loss: 141206910764629840.000000, mse: 50289947409755389952.000000, mean_q: 7575337776.761905, mean_eps: 0.173246
 275970/300000: episode: 2045, duration: 0.839s, episode steps: 112, steps per second: 134, episode reward: -571.775, mean reward: -5.105 [-100.000,  0.822], mean action: 1.884 [0.000, 3.000],  loss: 133714148951776112.000000, mse: 49997911851414544384.000000, mean_q: 7560419826.285714, mean_eps: 0.172259
 276172/300000: episode: 2046, duration: 1.428s, episode steps: 202, steps per second: 141, episode reward: -686.683, mean reward: -3.399 [-100.000,  3.246], mean action: 1.851 [0.000, 3.000],  loss: 180989583922436448.000000, mse: 50806797225989668864.000000, mean_q: 7626175779.485148, mean_eps: 0.171788
 276777/300000: episode: 2047, duration: 5.030s, episode steps: 605, steps per second: 120, episode reward: -4861.477, mean reward: -8.035 [-100.000,  1.340], mean action: 1.694 [0.000, 3.000],  loss: 163274404290596928.000000, mse: 51033154046259519488.000000, mean_q: 7642426501.712397, mean_eps: 0.170578
 276980/300000: episode: 2048, duration: 1.835s, episode steps: 203, steps per second: 111, episode reward: -689.933, mean reward: -3.399 [-100.000,  3.874], mean action: 1.961 [0.000, 3.000],  loss: 117727120031236496.000000, mse: 50546021921172774912.000000, mean_q: 7635744107.192119, mean_eps: 0.169366
 277235/300000: episode: 2049, duration: 2.144s, episode steps: 255, steps per second: 119, episode reward: -895.459, mean reward: -3.512 [-100.000,  3.243], mean action: 1.929 [0.000, 3.000],  loss: 143192176710085488.000000, mse: 51543306990381867008.000000, mean_q: 7717864602.603922, mean_eps: 0.168679
 277524/300000: episode: 2050, duration: 2.218s, episode steps: 289, steps per second: 130, episode reward: -1385.987, mean reward: -4.796 [-100.000,  3.449], mean action: 1.872 [0.000, 3.000],  loss: 153508335215149184.000000, mse: 51052971360277815296.000000, mean_q: 7686266550.477509, mean_eps: 0.167863
 277772/300000: episode: 2051, duration: 1.774s, episode steps: 248, steps per second: 140, episode reward: -1075.583, mean reward: -4.337 [-100.000,  4.278], mean action: 1.903 [0.000, 3.000],  loss: 197806337117228128.000000, mse: 51143283194614349824.000000, mean_q: 7686797995.354838, mean_eps: 0.167057
 277998/300000: episode: 2052, duration: 1.592s, episode steps: 226, steps per second: 142, episode reward: -987.912, mean reward: -4.371 [-100.000,  4.093], mean action: 1.978 [0.000, 3.000],  loss: 129838255115351648.000000, mse: 51149809507056926720.000000, mean_q: 7702146883.964602, mean_eps: 0.166346
 278134/300000: episode: 2053, duration: 0.917s, episode steps: 136, steps per second: 148, episode reward: -548.712, mean reward: -4.035 [-100.000,  1.471], mean action: 1.904 [0.000, 3.000],  loss: 156483866230768576.000000, mse: 52426321170741501952.000000, mean_q: 7796064621.176471, mean_eps: 0.165803
 278597/300000: episode: 2054, duration: 3.355s, episode steps: 463, steps per second: 138, episode reward: -2160.287, mean reward: -4.666 [-100.000,  4.780], mean action: 1.929 [0.000, 3.000],  loss: 197944442790755552.000000, mse: 52099782395066925056.000000, mean_q: 7790318597.529158, mean_eps: 0.164905
 278945/300000: episode: 2055, duration: 2.561s, episode steps: 348, steps per second: 136, episode reward: -1978.356, mean reward: -5.685 [-100.000,  2.363], mean action: 1.928 [0.000, 3.000],  loss: 136895278845842400.000000, mse: 51868326508627779584.000000, mean_q: 7780986572.505747, mean_eps: 0.163688
 279155/300000: episode: 2056, duration: 1.562s, episode steps: 210, steps per second: 134, episode reward: -1357.704, mean reward: -6.465 [-100.000,  1.723], mean action: 1.948 [0.000, 3.000],  loss: 139100053070493712.000000, mse: 52732289994008977408.000000, mean_q: 7870368073.142858, mean_eps: 0.162851
 279310/300000: episode: 2057, duration: 1.058s, episode steps: 155, steps per second: 147, episode reward: -490.484, mean reward: -3.164 [-100.000,  1.735], mean action: 1.910 [0.000, 3.000],  loss: 170544148695166112.000000, mse: 52805900878088314880.000000, mean_q: 7864335099.045161, mean_eps: 0.162304
 279540/300000: episode: 2058, duration: 1.805s, episode steps: 230, steps per second: 127, episode reward: -1380.080, mean reward: -6.000 [-100.000,  1.909], mean action: 1.952 [0.000, 3.000],  loss: 108005679146795584.000000, mse: 52754176990845411328.000000, mean_q: 7855544273.252174, mean_eps: 0.161726
 279721/300000: episode: 2059, duration: 1.469s, episode steps: 181, steps per second: 123, episode reward: -742.217, mean reward: -4.101 [-100.000,  3.460], mean action: 1.972 [0.000, 3.000],  loss: 194423210804420928.000000, mse: 53621590844249481216.000000, mean_q: 7933235202.828730, mean_eps: 0.161110
 279951/300000: episode: 2060, duration: 1.858s, episode steps: 230, steps per second: 124, episode reward: -797.140, mean reward: -3.466 [-100.000,  4.161], mean action: 1.926 [0.000, 3.000],  loss: 182413864568669152.000000, mse: 53659410096039493632.000000, mean_q: 7936546050.226087, mean_eps: 0.160493
 280951/300000: episode: 2061, duration: 7.656s, episode steps: 1000, steps per second: 131, episode reward: -1854.384, mean reward: -1.854 [-14.234, 18.091], mean action: 1.187 [0.000, 3.000],  loss: 183565382700601696.000000, mse: 53687174468214595584.000000, mean_q: 7971456808.960000, mean_eps: 0.158648
 281174/300000: episode: 2062, duration: 1.867s, episode steps: 223, steps per second: 119, episode reward: -733.550, mean reward: -3.289 [-100.000,  2.877], mean action: 1.888 [0.000, 3.000],  loss: 159931969640268672.000000, mse: 54526670158361452544.000000, mean_q: 8062554320.932735, mean_eps: 0.156814
 281557/300000: episode: 2063, duration: 3.022s, episode steps: 383, steps per second: 127, episode reward: -1866.170, mean reward: -4.873 [-100.000,  4.329], mean action: 1.906 [0.000, 3.000],  loss: 187808991409190528.000000, mse: 54408017138183782400.000000, mean_q: 8061951771.404699, mean_eps: 0.155905
 281826/300000: episode: 2064, duration: 2.016s, episode steps: 269, steps per second: 133, episode reward: -1291.005, mean reward: -4.799 [-100.000,  2.030], mean action: 1.981 [0.000, 3.000],  loss: 195774583095995200.000000, mse: 54511477220073062400.000000, mean_q: 8095499195.479554, mean_eps: 0.154927
 282270/300000: episode: 2065, duration: 3.435s, episode steps: 444, steps per second: 129, episode reward: -3870.681, mean reward: -8.718 [-100.000,  3.160], mean action: 1.959 [0.000, 3.000],  loss: 171116249507156448.000000, mse: 54011482898991112192.000000, mean_q: 8046162436.612613, mean_eps: 0.153858
 282464/300000: episode: 2066, duration: 1.437s, episode steps: 194, steps per second: 135, episode reward: -1032.393, mean reward: -5.322 [-100.000,  2.170], mean action: 1.954 [0.000, 3.000],  loss: 153704244792727488.000000, mse: 54217249035255234560.000000, mean_q: 8093191331.628866, mean_eps: 0.152900
 282669/300000: episode: 2067, duration: 1.418s, episode steps: 205, steps per second: 145, episode reward: -967.774, mean reward: -4.721 [-100.000,  3.419], mean action: 1.937 [0.000, 3.000],  loss: 146196414991842944.000000, mse: 54819397102942257152.000000, mean_q: 8131392779.239024, mean_eps: 0.152302
 282837/300000: episode: 2068, duration: 1.250s, episode steps: 168, steps per second: 134, episode reward: -587.570, mean reward: -3.497 [-100.000,  2.524], mean action: 1.940 [0.000, 3.000],  loss: 156061511722554720.000000, mse: 54874258381732888576.000000, mean_q: 8131328460.190476, mean_eps: 0.151742
 283012/300000: episode: 2069, duration: 1.292s, episode steps: 175, steps per second: 135, episode reward: -943.333, mean reward: -5.390 [-100.000,  2.376], mean action: 1.960 [0.000, 3.000],  loss: 154583632050072928.000000, mse: 55609474848248741888.000000, mean_q: 8184870537.508572, mean_eps: 0.151228
 283140/300000: episode: 2070, duration: 0.933s, episode steps: 128, steps per second: 137, episode reward: -569.931, mean reward: -4.453 [-100.000,  1.152], mean action: 1.844 [0.000, 3.000],  loss: 129765276756475904.000000, mse: 55553395605557477376.000000, mean_q: 8206690760.000000, mean_eps: 0.150774
 283752/300000: episode: 2071, duration: 4.504s, episode steps: 612, steps per second: 136, episode reward: -5722.118, mean reward: -9.350 [-100.000,  1.648], mean action: 1.814 [0.000, 3.000],  loss: 156687540774190560.000000, mse: 55292092490374848512.000000, mean_q: 8190958716.653595, mean_eps: 0.149663
 284007/300000: episode: 2072, duration: 1.958s, episode steps: 255, steps per second: 130, episode reward: -1495.809, mean reward: -5.866 [-100.000,  2.278], mean action: 1.945 [0.000, 3.000],  loss: 190081630691109792.000000, mse: 55265478107162181632.000000, mean_q: 8201739344.313725, mean_eps: 0.148363
 284138/300000: episode: 2073, duration: 0.980s, episode steps: 131, steps per second: 134, episode reward: -675.797, mean reward: -5.159 [-100.000,  0.978], mean action: 1.840 [0.000, 3.000],  loss: 190192428037129312.000000, mse: 56250627781781438464.000000, mean_q: 8293669981.801527, mean_eps: 0.147784
 284548/300000: episode: 2074, duration: 3.114s, episode steps: 410, steps per second: 132, episode reward: -3394.456, mean reward: -8.279 [-100.000,  1.781], mean action: 1.971 [0.000, 3.000],  loss: 183412945397716768.000000, mse: 55453674789219508224.000000, mean_q: 8226891189.073171, mean_eps: 0.146972
 284686/300000: episode: 2075, duration: 0.951s, episode steps: 138, steps per second: 145, episode reward: -642.514, mean reward: -4.656 [-100.000,  1.120], mean action: 1.899 [0.000, 3.000],  loss: 120759152741776464.000000, mse: 56296009984209272832.000000, mean_q: 8287698506.202899, mean_eps: 0.146150
 285446/300000: episode: 2076, duration: 5.948s, episode steps: 760, steps per second: 128, episode reward: -4634.432, mean reward: -6.098 [-100.000,  1.292], mean action: 1.526 [0.000, 3.000],  loss: 164918158514059936.000000, mse: 55568974753269948416.000000, mean_q: 8247166999.578947, mean_eps: 0.144803
 285743/300000: episode: 2077, duration: 2.146s, episode steps: 297, steps per second: 138, episode reward: -1430.305, mean reward: -4.816 [-100.000,  4.426], mean action: 1.919 [0.000, 3.000],  loss: 166485710643724064.000000, mse: 55128184480861061120.000000, mean_q: 8210321704.511785, mean_eps: 0.143218
 285913/300000: episode: 2078, duration: 1.285s, episode steps: 170, steps per second: 132, episode reward: -521.126, mean reward: -3.065 [-100.000,  3.891], mean action: 1.947 [0.000, 3.000],  loss: 189440884052846560.000000, mse: 55152239637455290368.000000, mean_q: 8215408064.752941, mean_eps: 0.142517
 286298/300000: episode: 2079, duration: 2.880s, episode steps: 385, steps per second: 134, episode reward: -1855.135, mean reward: -4.819 [-100.000,  1.851], mean action: 1.932 [0.000, 3.000],  loss: 184849028816407616.000000, mse: 54741426719450423296.000000, mean_q: 8196846768.872727, mean_eps: 0.141685
 286541/300000: episode: 2080, duration: 1.772s, episode steps: 243, steps per second: 137, episode reward: -1290.642, mean reward: -5.311 [-100.000,  1.570], mean action: 1.947 [0.000, 3.000],  loss: 124103862011276224.000000, mse: 54223956796562612224.000000, mean_q: 8163870418.699589, mean_eps: 0.140743
 286690/300000: episode: 2081, duration: 1.045s, episode steps: 149, steps per second: 143, episode reward: -659.959, mean reward: -4.429 [-100.000,  1.458], mean action: 1.913 [0.000, 3.000],  loss: 216268531606881120.000000, mse: 54254708240834674688.000000, mean_q: 8167197101.530201, mean_eps: 0.140155
 286814/300000: episode: 2082, duration: 0.880s, episode steps: 124, steps per second: 141, episode reward: -576.230, mean reward: -4.647 [-100.000,  1.366], mean action: 1.984 [0.000, 3.000],  loss: 87199809698292512.000000, mse: 54297148345856811008.000000, mean_q: 8183968924.903226, mean_eps: 0.139745
 287515/300000: episode: 2083, duration: 5.184s, episode steps: 701, steps per second: 135, episode reward: -7019.095, mean reward: -10.013 [-100.000,  2.849], mean action: 1.702 [0.000, 3.000],  loss: 157053809565997696.000000, mse: 54438099060891631616.000000, mean_q: 8195874516.542083, mean_eps: 0.138508
 287706/300000: episode: 2084, duration: 1.350s, episode steps: 191, steps per second: 141, episode reward: -874.562, mean reward: -4.579 [-100.000,  3.673], mean action: 2.000 [0.000, 3.000],  loss: 144935479306478208.000000, mse: 54524880197522382848.000000, mean_q: 8166959712.502618, mean_eps: 0.137170
 287895/300000: episode: 2085, duration: 1.368s, episode steps: 189, steps per second: 138, episode reward: -706.986, mean reward: -3.741 [-100.000,  3.440], mean action: 1.968 [0.000, 3.000],  loss: 195259572762843232.000000, mse: 54657205591816642560.000000, mean_q: 8182836148.148149, mean_eps: 0.136600
 288034/300000: episode: 2086, duration: 1.040s, episode steps: 139, steps per second: 134, episode reward: -518.471, mean reward: -3.730 [-100.000,  1.378], mean action: 1.856 [0.000, 3.000],  loss: 107013447131213792.000000, mse: 55225987995425013760.000000, mean_q: 8232816544.230216, mean_eps: 0.136108
 288811/300000: episode: 2087, duration: 7.027s, episode steps: 777, steps per second: 111, episode reward: -4509.995, mean reward: -5.804 [-100.000,  1.683], mean action: 1.624 [0.000, 3.000],  loss: 193219802728176992.000000, mse: 54211802254541217792.000000, mean_q: 8167824648.895753, mean_eps: 0.134734
 289166/300000: episode: 2088, duration: 2.781s, episode steps: 355, steps per second: 128, episode reward: -2314.230, mean reward: -6.519 [-100.000,  2.602], mean action: 1.927 [0.000, 3.000],  loss: 146735591950535040.000000, mse: 53007923948451094528.000000, mean_q: 8080466269.025352, mean_eps: 0.133036
 289380/300000: episode: 2089, duration: 1.538s, episode steps: 214, steps per second: 139, episode reward: -1057.159, mean reward: -4.940 [-100.000,  3.201], mean action: 1.930 [0.000, 3.000],  loss: 123612863587062864.000000, mse: 52503407589898665984.000000, mean_q: 8056191949.757010, mean_eps: 0.132182
 289647/300000: episode: 2090, duration: 2.243s, episode steps: 267, steps per second: 119, episode reward: -1193.458, mean reward: -4.470 [-100.000,  2.798], mean action: 1.891 [0.000, 3.000],  loss: 192949517386673472.000000, mse: 52633855657011011584.000000, mean_q: 8062144373.932584, mean_eps: 0.131461
 289906/300000: episode: 2091, duration: 2.115s, episode steps: 259, steps per second: 122, episode reward: -1182.631, mean reward: -4.566 [-100.000,  3.237], mean action: 1.973 [0.000, 3.000],  loss: 167272304480099232.000000, mse: 53038449186194448384.000000, mean_q: 8091101816.586873, mean_eps: 0.130672
 290606/300000: episode: 2092, duration: 5.177s, episode steps: 700, steps per second: 135, episode reward: -5200.780, mean reward: -7.430 [-100.000,  1.319], mean action: 1.784 [0.000, 3.000],  loss: 151831842103101632.000000, mse: 52465121747412008960.000000, mean_q: 8049784774.948571, mean_eps: 0.129233
 290741/300000: episode: 2093, duration: 0.956s, episode steps: 135, steps per second: 141, episode reward: -781.387, mean reward: -5.788 [-100.000,  0.034], mean action: 2.037 [0.000, 3.000],  loss: 224508493849344480.000000, mse: 52441542706592301056.000000, mean_q: 8033103587.555555, mean_eps: 0.127981
 290878/300000: episode: 2094, duration: 0.933s, episode steps: 137, steps per second: 147, episode reward: -515.959, mean reward: -3.766 [-100.000,  1.240], mean action: 2.029 [0.000, 3.000],  loss: 113471394914448944.000000, mse: 52827562342842335232.000000, mean_q: 8083274068.087591, mean_eps: 0.127573
 291052/300000: episode: 2095, duration: 1.318s, episode steps: 174, steps per second: 132, episode reward: -699.080, mean reward: -4.018 [-100.000,  3.250], mean action: 1.931 [0.000, 3.000],  loss: 162712143408494752.000000, mse: 51747469669156208640.000000, mean_q: 8000583235.678161, mean_eps: 0.127106
 291341/300000: episode: 2096, duration: 2.073s, episode steps: 289, steps per second: 139, episode reward: -1725.071, mean reward: -5.969 [-100.000,  0.714], mean action: 1.941 [0.000, 3.000],  loss: 187060956828845664.000000, mse: 53136796400946618368.000000, mean_q: 8103210582.809689, mean_eps: 0.126412
 291529/300000: episode: 2097, duration: 1.316s, episode steps: 188, steps per second: 143, episode reward: -807.874, mean reward: -4.297 [-100.000,  3.806], mean action: 2.000 [0.000, 3.000],  loss: 98686759832153328.000000, mse: 51900074368283140096.000000, mean_q: 8021450626.723404, mean_eps: 0.125696
 291846/300000: episode: 2098, duration: 2.425s, episode steps: 317, steps per second: 131, episode reward: -1822.761, mean reward: -5.750 [-100.000,  2.427], mean action: 1.940 [0.000, 3.000],  loss: 164317071625489440.000000, mse: 52327519526819209216.000000, mean_q: 8048578800.656152, mean_eps: 0.124939
 292197/300000: episode: 2099, duration: 2.703s, episode steps: 351, steps per second: 130, episode reward: -1847.687, mean reward: -5.264 [-100.000,  3.485], mean action: 1.997 [0.000, 3.000],  loss: 128187236931674320.000000, mse: 52723091395063685120.000000, mean_q: 8076270344.022792, mean_eps: 0.123937
 292547/300000: episode: 2100, duration: 2.639s, episode steps: 350, steps per second: 133, episode reward: -1714.851, mean reward: -4.900 [-100.000,  4.674], mean action: 1.963 [0.000, 3.000],  loss: 178697664812245504.000000, mse: 52094138865316536320.000000, mean_q: 8040365013.577143, mean_eps: 0.122885
 292831/300000: episode: 2101, duration: 2.122s, episode steps: 284, steps per second: 134, episode reward: -1642.438, mean reward: -5.783 [-100.000,  2.328], mean action: 1.965 [0.000, 3.000],  loss: 113404292636307392.000000, mse: 51758365533727956992.000000, mean_q: 8038440240.676056, mean_eps: 0.121934
 293064/300000: episode: 2102, duration: 1.638s, episode steps: 233, steps per second: 142, episode reward: -1064.788, mean reward: -4.570 [-100.000,  1.499], mean action: 1.936 [0.000, 3.000],  loss: 125734390934044528.000000, mse: 51820873102418632704.000000, mean_q: 8016315047.004292, mean_eps: 0.121159
 293290/300000: episode: 2103, duration: 1.677s, episode steps: 226, steps per second: 135, episode reward: -1186.551, mean reward: -5.250 [-100.000,  2.112], mean action: 1.929 [0.000, 3.000],  loss: 185934388005572000.000000, mse: 50672597057282121728.000000, mean_q: 7951509975.221239, mean_eps: 0.120470
 293422/300000: episode: 2104, duration: 0.911s, episode steps: 132, steps per second: 145, episode reward: -708.127, mean reward: -5.365 [-100.000,  1.269], mean action: 1.977 [0.000, 3.000],  loss: 115001121021743472.000000, mse: 52473839992155095040.000000, mean_q: 8054974623.030303, mean_eps: 0.119933
 293612/300000: episode: 2105, duration: 1.340s, episode steps: 190, steps per second: 142, episode reward: -794.213, mean reward: -4.180 [-100.000,  4.286], mean action: 1.958 [0.000, 3.000],  loss: 166953902477415808.000000, mse: 51022084095087353856.000000, mean_q: 7970483685.052631, mean_eps: 0.119450
 293741/300000: episode: 2106, duration: 0.879s, episode steps: 129, steps per second: 147, episode reward: -603.336, mean reward: -4.677 [-100.000,  1.014], mean action: 1.977 [0.000, 3.000],  loss: 141675863447693360.000000, mse: 51555524218697220096.000000, mean_q: 7995577506.728683, mean_eps: 0.118972
 293876/300000: episode: 2107, duration: 0.960s, episode steps: 135, steps per second: 141, episode reward: -646.254, mean reward: -4.787 [-100.000,  1.117], mean action: 1.948 [0.000, 3.000],  loss: 169268878872781088.000000, mse: 51131059100725321728.000000, mean_q: 7970445433.362963, mean_eps: 0.118576
 294068/300000: episode: 2108, duration: 1.321s, episode steps: 192, steps per second: 145, episode reward: -602.654, mean reward: -3.139 [-100.000,  3.596], mean action: 1.974 [0.000, 3.000],  loss: 150569220306894848.000000, mse: 50004658143888760832.000000, mean_q: 7890715965.333333, mean_eps: 0.118085
 294385/300000: episode: 2109, duration: 2.247s, episode steps: 317, steps per second: 141, episode reward: -2199.062, mean reward: -6.937 [-100.000,  1.636], mean action: 1.981 [0.000, 3.000],  loss: 139494874549771584.000000, mse: 50841303194287276032.000000, mean_q: 7931544532.391168, mean_eps: 0.117322
 294746/300000: episode: 2110, duration: 2.597s, episode steps: 361, steps per second: 139, episode reward: -2927.063, mean reward: -8.108 [-100.000,  1.469], mean action: 1.936 [0.000, 3.000],  loss: 148556552408365728.000000, mse: 50076259991435673600.000000, mean_q: 7860997250.481995, mean_eps: 0.116305
 295014/300000: episode: 2111, duration: 1.904s, episode steps: 268, steps per second: 141, episode reward: -1874.056, mean reward: -6.993 [-100.000,  0.104], mean action: 1.959 [0.000, 3.000],  loss: 134638662565056576.000000, mse: 49813480768502743040.000000, mean_q: 7871995867.701492, mean_eps: 0.115361
 295189/300000: episode: 2112, duration: 1.216s, episode steps: 175, steps per second: 144, episode reward: -748.747, mean reward: -4.279 [-100.000,  1.563], mean action: 2.017 [0.000, 3.000],  loss: 86049331117188704.000000, mse: 49476768122840817664.000000, mean_q: 7853716845.714286, mean_eps: 0.114697
 295538/300000: episode: 2113, duration: 2.543s, episode steps: 349, steps per second: 137, episode reward: -2554.217, mean reward: -7.319 [-100.000,  2.092], mean action: 1.934 [0.000, 3.000],  loss: 174714758783418592.000000, mse: 49858632110472511488.000000, mean_q: 7855560363.644699, mean_eps: 0.113911
 296008/300000: episode: 2114, duration: 3.413s, episode steps: 470, steps per second: 138, episode reward: -2452.077, mean reward: -5.217 [-100.000,  2.987], mean action: 1.847 [0.000, 3.000],  loss: 174098645357466656.000000, mse: 49343134274457591808.000000, mean_q: 7816949715.336170, mean_eps: 0.112682
 296145/300000: episode: 2115, duration: 0.938s, episode steps: 137, steps per second: 146, episode reward: -621.089, mean reward: -4.533 [-100.000,  1.026], mean action: 2.000 [0.000, 3.000],  loss: 75039849535986576.000000, mse: 49408517774857150464.000000, mean_q: 7826442127.883212, mean_eps: 0.111772
 296429/300000: episode: 2116, duration: 2.028s, episode steps: 284, steps per second: 140, episode reward: -2356.867, mean reward: -8.299 [-100.000,  1.151], mean action: 1.965 [0.000, 3.000],  loss: 113403590170965296.000000, mse: 48412320365811769344.000000, mean_q: 7745770023.661972, mean_eps: 0.111140
 296731/300000: episode: 2117, duration: 2.114s, episode steps: 302, steps per second: 143, episode reward: -1911.365, mean reward: -6.329 [-100.000,  1.152], mean action: 1.960 [0.000, 3.000],  loss: 140461987925228912.000000, mse: 48180640342142156800.000000, mean_q: 7731287679.152318, mean_eps: 0.110261
 297201/300000: episode: 2118, duration: 3.845s, episode steps: 470, steps per second: 122, episode reward: -3457.405, mean reward: -7.356 [-100.000,  2.458], mean action: 1.926 [0.000, 3.000],  loss: 130144957361390544.000000, mse: 48192147643625578496.000000, mean_q: 7723709824.544681, mean_eps: 0.109103
 297434/300000: episode: 2119, duration: 1.932s, episode steps: 233, steps per second: 121, episode reward: -1459.230, mean reward: -6.263 [-100.000,  1.747], mean action: 1.974 [0.000, 3.000],  loss: 142190029822345280.000000, mse: 47884650694620037120.000000, mean_q: 7684970515.776824, mean_eps: 0.108049
 297725/300000: episode: 2120, duration: 2.100s, episode steps: 291, steps per second: 139, episode reward: -1514.595, mean reward: -5.205 [-100.000,  4.118], mean action: 1.973 [0.000, 3.000],  loss: 174070282470017856.000000, mse: 47250069523509460992.000000, mean_q: 7647297400.522337, mean_eps: 0.107263
 298071/300000: episode: 2121, duration: 2.460s, episode steps: 346, steps per second: 141, episode reward: -2298.540, mean reward: -6.643 [-100.000,  2.597], mean action: 1.919 [0.000, 3.000],  loss: 108577269400822560.000000, mse: 46958081773078364160.000000, mean_q: 7626066396.485549, mean_eps: 0.106307
 298529/300000: episode: 2122, duration: 3.365s, episode steps: 458, steps per second: 136, episode reward: -3450.302, mean reward: -7.533 [-100.000,  0.328], mean action: 1.670 [0.000, 3.000],  loss: 109481888357844512.000000, mse: 46843171796679180288.000000, mean_q: 7626121798.427948, mean_eps: 0.105101
 298787/300000: episode: 2123, duration: 1.942s, episode steps: 258, steps per second: 133, episode reward: -1622.360, mean reward: -6.288 [-100.000,  1.223], mean action: 1.965 [0.000, 3.000],  loss: 106160764384676080.000000, mse: 46432531106213380096.000000, mean_q: 7584077057.984496, mean_eps: 0.104027
 299186/300000: episode: 2124, duration: 2.957s, episode steps: 399, steps per second: 135, episode reward: -2165.688, mean reward: -5.428 [-100.000,  4.020], mean action: 1.907 [0.000, 3.000],  loss: 161108245446592256.000000, mse: 46678291907789676544.000000, mean_q: 7571070036.691730, mean_eps: 0.103042
 299603/300000: episode: 2125, duration: 2.937s, episode steps: 417, steps per second: 142, episode reward: -3752.900, mean reward: -9.000 [-100.000,  1.443], mean action: 1.892 [0.000, 3.000],  loss: 129737787605529488.000000, mse: 45209632721447591936.000000, mean_q: 7462263844.834533, mean_eps: 0.101818
 299930/300000: episode: 2126, duration: 2.286s, episode steps: 327, steps per second: 143, episode reward: -1659.009, mean reward: -5.073 [-100.000,  1.138], mean action: 1.908 [0.000, 3.000],  loss: 192255167840155392.000000, mse: 44753466976303153152.000000, mean_q: 7423497956.599388, mean_eps: 0.100702
done, took 2213.476 seconds
Testing for 200 episodes ...
Episode 1: reward: -858.543, steps: 138
Episode 2: reward: -3996.591, steps: 580
Episode 3: reward: -3237.701, steps: 486
Episode 4: reward: -3738.415, steps: 486
Episode 5: reward: -2089.469, steps: 288
Episode 6: reward: -1210.298, steps: 169
Episode 7: reward: -1679.687, steps: 296
Episode 8: reward: -1890.364, steps: 277
Episode 9: reward: -2082.186, steps: 293
Episode 10: reward: -4820.828, steps: 501
Episode 11: reward: -3229.107, steps: 316
Episode 12: reward: -3368.837, steps: 573
Episode 13: reward: -2120.429, steps: 287
Episode 14: reward: -2920.012, steps: 629
Episode 15: reward: -976.358, steps: 172
Episode 16: reward: -745.208, steps: 147
Episode 17: reward: -5474.450, steps: 681
Episode 18: reward: -4641.417, steps: 479
Episode 19: reward: -1587.872, steps: 229
Episode 20: reward: -2065.508, steps: 307
Episode 21: reward: -765.361, steps: 127
Episode 22: reward: -1804.451, steps: 285
Episode 23: reward: -1098.473, steps: 154
Episode 24: reward: -4695.626, steps: 537
Episode 25: reward: -1098.321, steps: 229
Episode 26: reward: -4860.351, steps: 655
Episode 27: reward: -2133.972, steps: 260
Episode 28: reward: -3268.462, steps: 552
Episode 29: reward: -3385.042, steps: 582
Episode 30: reward: -5903.687, steps: 634
Episode 31: reward: -2260.246, steps: 319
Episode 32: reward: -4153.617, steps: 436
Episode 33: reward: -1624.362, steps: 245
Episode 34: reward: -2247.868, steps: 303
Episode 35: reward: -702.633, steps: 151
Episode 36: reward: -1012.387, steps: 181
Episode 37: reward: -836.181, steps: 134
Episode 38: reward: -1015.496, steps: 173
Episode 39: reward: -813.280, steps: 167
Episode 40: reward: -1553.151, steps: 257
Episode 41: reward: -875.060, steps: 142
Episode 42: reward: -1606.597, steps: 243
Episode 43: reward: -3448.389, steps: 554
Episode 44: reward: -1479.730, steps: 210
Episode 45: reward: -1814.417, steps: 295
Episode 46: reward: -1903.725, steps: 258
Episode 47: reward: -874.313, steps: 167
Episode 48: reward: -2175.072, steps: 298
Episode 49: reward: -2136.569, steps: 281
Episode 50: reward: -3179.436, steps: 481
Episode 51: reward: -1616.589, steps: 257
Episode 52: reward: -3948.469, steps: 491
Episode 53: reward: -3537.232, steps: 518
Episode 54: reward: -1360.184, steps: 195
Episode 55: reward: -4393.913, steps: 459
Episode 56: reward: -3053.808, steps: 570
Episode 57: reward: -3823.906, steps: 493
Episode 58: reward: -3780.834, steps: 462
Episode 59: reward: -1405.505, steps: 249
Episode 60: reward: -1941.340, steps: 273
Episode 61: reward: -3799.483, steps: 482
Episode 62: reward: -1939.337, steps: 265
Episode 63: reward: -1162.913, steps: 160
Episode 64: reward: -3638.860, steps: 582
Episode 65: reward: -4214.017, steps: 423
Episode 66: reward: -3743.920, steps: 453
Episode 67: reward: -3800.233, steps: 587
Episode 68: reward: -5017.607, steps: 636
Episode 69: reward: -1115.268, steps: 176
Episode 70: reward: -761.795, steps: 167
Episode 71: reward: -2432.137, steps: 305
Episode 72: reward: -919.752, steps: 179
Episode 73: reward: -2794.775, steps: 292
Episode 74: reward: -4236.606, steps: 396
Episode 75: reward: -779.896, steps: 135
Episode 76: reward: -2880.841, steps: 630
Episode 77: reward: -3224.115, steps: 455
Episode 78: reward: -2514.816, steps: 292
Episode 79: reward: -707.787, steps: 133
Episode 80: reward: -1378.496, steps: 242
Episode 81: reward: -969.762, steps: 167
Episode 82: reward: -4372.735, steps: 474
Episode 83: reward: -2064.189, steps: 239
Episode 84: reward: -4767.286, steps: 494
Episode 85: reward: -673.188, steps: 167
Episode 86: reward: -2056.742, steps: 293
Episode 87: reward: -4346.825, steps: 431
Episode 88: reward: -3975.493, steps: 574
Episode 89: reward: -2728.976, steps: 331
Episode 90: reward: -1221.394, steps: 230
Episode 91: reward: -1698.570, steps: 239
Episode 92: reward: -692.604, steps: 166
Episode 93: reward: -4885.014, steps: 458
Episode 94: reward: -2430.234, steps: 312
Episode 95: reward: -2288.746, steps: 305
Episode 96: reward: -1837.919, steps: 230
Episode 97: reward: -816.859, steps: 127
Episode 98: reward: -1354.492, steps: 247
Episode 99: reward: -4017.123, steps: 569
Episode 100: reward: -3901.942, steps: 599
Episode 101: reward: -6517.373, steps: 664
Episode 102: reward: -783.376, steps: 171
Episode 103: reward: -659.769, steps: 133
Episode 104: reward: -702.421, steps: 114
Episode 105: reward: -4492.658, steps: 654
Episode 106: reward: -1287.307, steps: 224
Episode 107: reward: -4272.560, steps: 574
Episode 108: reward: -832.903, steps: 151
Episode 109: reward: -3509.486, steps: 446
Episode 110: reward: -3925.257, steps: 488
Episode 111: reward: -5263.902, steps: 642
Episode 112: reward: -2420.297, steps: 293
Episode 113: reward: -799.240, steps: 144
Episode 114: reward: -2502.616, steps: 298
Episode 115: reward: -916.280, steps: 153
Episode 116: reward: -3461.544, steps: 568
Episode 117: reward: -620.735, steps: 128
Episode 118: reward: -5582.403, steps: 674
Episode 119: reward: -1915.335, steps: 263
Episode 120: reward: -2565.592, steps: 651
Episode 121: reward: -2195.610, steps: 272
Episode 122: reward: -3378.152, steps: 549
Episode 123: reward: -3948.055, steps: 469
Episode 124: reward: -727.012, steps: 141
Episode 125: reward: -1591.140, steps: 221
Episode 126: reward: -2116.789, steps: 269
Episode 127: reward: -2711.645, steps: 285
Episode 128: reward: -5595.450, steps: 628
Episode 129: reward: -2384.477, steps: 279
Episode 130: reward: -745.689, steps: 130
Episode 131: reward: -2801.276, steps: 341
Episode 132: reward: -2064.166, steps: 295
Episode 133: reward: -853.673, steps: 157
Episode 134: reward: -2400.330, steps: 308
Episode 135: reward: -3844.879, steps: 470
Episode 136: reward: -719.825, steps: 124
Episode 137: reward: -3090.317, steps: 634
Episode 138: reward: -968.823, steps: 157
Episode 139: reward: -3534.474, steps: 434
Episode 140: reward: -2184.366, steps: 284
Episode 141: reward: -955.965, steps: 130
Episode 142: reward: -5852.269, steps: 641
Episode 143: reward: -2861.560, steps: 645
Episode 144: reward: -3547.040, steps: 507
Episode 145: reward: -5174.148, steps: 540
Episode 146: reward: -3198.381, steps: 628
Episode 147: reward: -1553.654, steps: 194
Episode 148: reward: -2000.987, steps: 716
Episode 149: reward: -4054.076, steps: 393
Episode 150: reward: -2957.371, steps: 327
Episode 151: reward: -715.623, steps: 169
Episode 152: reward: -1361.498, steps: 187
Episode 153: reward: -4214.593, steps: 484
Episode 154: reward: -2534.437, steps: 294
Episode 155: reward: -2216.890, steps: 260
Episode 156: reward: -3061.051, steps: 571
Episode 157: reward: -3686.341, steps: 417
Episode 158: reward: -4626.690, steps: 456
Episode 159: reward: -3623.215, steps: 478
Episode 160: reward: -763.263, steps: 134
Episode 161: reward: -3456.141, steps: 639
Episode 162: reward: -3645.242, steps: 488
Episode 163: reward: -3587.580, steps: 596
Episode 164: reward: -2619.200, steps: 312
Episode 165: reward: -3176.536, steps: 511
Episode 166: reward: -1338.528, steps: 191
Episode 167: reward: -2804.729, steps: 613
Episode 168: reward: -1695.898, steps: 210
Episode 169: reward: -1584.156, steps: 209
Episode 170: reward: -762.482, steps: 128
Episode 171: reward: -2227.146, steps: 280
Episode 172: reward: -3479.055, steps: 452
Episode 173: reward: -3649.528, steps: 472
Episode 174: reward: -1875.749, steps: 267
Episode 175: reward: -737.406, steps: 149
Episode 176: reward: -4276.055, steps: 453
Episode 177: reward: -775.924, steps: 170
Episode 178: reward: -2283.608, steps: 298
Episode 179: reward: -1563.211, steps: 249
Episode 180: reward: -863.386, steps: 176
Episode 181: reward: -2382.587, steps: 297
Episode 182: reward: -2235.768, steps: 283
Episode 183: reward: -1841.182, steps: 287
Episode 184: reward: -642.646, steps: 124
Episode 185: reward: -2040.363, steps: 313
Episode 186: reward: -1347.588, steps: 210
Episode 187: reward: -2947.882, steps: 576
Episode 188: reward: -2201.154, steps: 317
Episode 189: reward: -1897.066, steps: 334
Episode 190: reward: -1478.878, steps: 222
Episode 191: reward: -1593.550, steps: 243
Episode 192: reward: -5279.824, steps: 651
Episode 193: reward: -968.450, steps: 222
Episode 194: reward: -3599.766, steps: 577
Episode 195: reward: -845.315, steps: 152
Episode 196: reward: -3934.579, steps: 490
Episode 197: reward: -2117.835, steps: 321
Episode 198: reward: -3299.740, steps: 586
Episode 199: reward: -1249.765, steps: 164
Episode 200: reward: -3837.529, steps: 390