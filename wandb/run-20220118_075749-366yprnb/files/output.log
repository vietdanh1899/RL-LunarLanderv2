Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 300000 steps ...
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
     63/300000: episode: 1, duration: 1.313s, episode steps:  63, steps per second:  48, episode reward: -177.921, mean reward: -2.824 [-100.000,  4.420], mean action: 1.651 [0.000, 3.000],  loss: 1.182646, mse: 0.965497, mean_q: 0.832888, mean_eps: 0.999890
    154/300000: episode: 2, duration: 0.730s, episode steps:  91, steps per second: 125, episode reward: -291.863, mean reward: -3.207 [-100.000,  4.618], mean action: 1.407 [0.000, 3.000],  loss: 46.298809, mse: 28.101311, mean_q: 1.459959, mean_eps: 0.999676
    239/300000: episode: 3, duration: 0.782s, episode steps:  85, steps per second: 109, episode reward: -337.590, mean reward: -3.972 [-100.000,  0.642], mean action: 1.600 [0.000, 3.000],  loss: 34.034767, mse: 24.659185, mean_q: 0.745202, mean_eps: 0.999412
    300/300000: episode: 4, duration: 0.546s, episode steps:  61, steps per second: 112, episode reward: -61.901, mean reward: -1.015 [-100.000, 10.985], mean action: 1.213 [0.000, 3.000],  loss: 31.801658, mse: 28.511804, mean_q: 0.800207, mean_eps: 0.999193
    423/300000: episode: 5, duration: 1.207s, episode steps: 123, steps per second: 102, episode reward: -96.706, mean reward: -0.786 [-100.000, 12.033], mean action: 1.634 [0.000, 3.000],  loss: 24.440416, mse: 27.846882, mean_q: 1.440041, mean_eps: 0.998917
    506/300000: episode: 6, duration: 0.651s, episode steps:  83, steps per second: 128, episode reward: -98.084, mean reward: -1.182 [-100.000,  8.773], mean action: 1.482 [0.000, 3.000],  loss: 11.704794, mse: 34.865582, mean_q: 2.758127, mean_eps: 0.998608
    617/300000: episode: 7, duration: 0.868s, episode steps: 111, steps per second: 128, episode reward: -117.192, mean reward: -1.056 [-100.000,  8.054], mean action: 1.450 [0.000, 3.000],  loss: 16.342533, mse: 50.969511, mean_q: 4.120218, mean_eps: 0.998317
    689/300000: episode: 8, duration: 0.576s, episode steps:  72, steps per second: 125, episode reward: -99.971, mean reward: -1.388 [-100.000, 11.969], mean action: 1.486 [0.000, 3.000],  loss: 8.183943, mse: 75.431148, mean_q: 5.341652, mean_eps: 0.998042
    794/300000: episode: 9, duration: 0.826s, episode steps: 105, steps per second: 127, episode reward: -237.396, mean reward: -2.261 [-100.000,  7.758], mean action: 1.524 [0.000, 3.000],  loss: 12.540832, mse: 93.617052, mean_q: 5.576809, mean_eps: 0.997777
    903/300000: episode: 10, duration: 1.003s, episode steps: 109, steps per second: 109, episode reward: -254.512, mean reward: -2.335 [-100.000, 36.686], mean action: 1.431 [0.000, 3.000],  loss: 9.013509, mse: 129.796137, mean_q: 5.839495, mean_eps: 0.997456
    978/300000: episode: 11, duration: 0.706s, episode steps:  75, steps per second: 106, episode reward: -106.629, mean reward: -1.422 [-100.000,  9.373], mean action: 1.480 [0.000, 3.000],  loss: 10.522421, mse: 174.126039, mean_q: 5.987089, mean_eps: 0.997180
   1076/300000: episode: 12, duration: 0.792s, episode steps:  98, steps per second: 124, episode reward: -272.593, mean reward: -2.782 [-100.000,  0.516], mean action: 1.367 [0.000, 3.000],  loss: 12.876098, mse: 228.286395, mean_q: 6.353659, mean_eps: 0.996921
   1143/300000: episode: 13, duration: 0.521s, episode steps:  67, steps per second: 129, episode reward: -55.997, mean reward: -0.836 [-100.000,  7.403], mean action: 1.627 [0.000, 3.000],  loss: 16.693232, mse: 260.023491, mean_q: 6.556221, mean_eps: 0.996673
   1254/300000: episode: 14, duration: 0.867s, episode steps: 111, steps per second: 128, episode reward: -93.773, mean reward: -0.845 [-100.000, 21.994], mean action: 1.441 [0.000, 3.000],  loss: 18.442215, mse: 262.846634, mean_q: 7.233989, mean_eps: 0.996406
   1337/300000: episode: 15, duration: 0.652s, episode steps:  83, steps per second: 127, episode reward: -139.701, mean reward: -1.683 [-100.000,  8.383], mean action: 1.614 [0.000, 3.000],  loss: 14.109429, mse: 343.336041, mean_q: 7.761917, mean_eps: 0.996115
   1421/300000: episode: 16, duration: 0.635s, episode steps:  84, steps per second: 132, episode reward: -84.864, mean reward: -1.010 [-100.000, 71.167], mean action: 1.429 [0.000, 3.000],  loss: 16.882115, mse: 321.602359, mean_q: 9.086620, mean_eps: 0.995864
   1494/300000: episode: 17, duration: 0.622s, episode steps:  73, steps per second: 117, episode reward: -103.992, mean reward: -1.425 [-100.000, 12.041], mean action: 1.630 [0.000, 3.000],  loss: 12.034686, mse: 362.388162, mean_q: 9.711397, mean_eps: 0.995629
   1566/300000: episode: 18, duration: 0.682s, episode steps:  72, steps per second: 106, episode reward: -134.752, mean reward: -1.872 [-100.000, 15.203], mean action: 1.528 [0.000, 3.000],  loss: 16.262850, mse: 417.327540, mean_q: 10.009999, mean_eps: 0.995412
   1653/300000: episode: 19, duration: 0.848s, episode steps:  87, steps per second: 103, episode reward: -88.591, mean reward: -1.018 [-100.000,  9.931], mean action: 1.805 [0.000, 3.000],  loss: 9.076375, mse: 439.291372, mean_q: 10.382893, mean_eps: 0.995173
   1734/300000: episode: 20, duration: 0.741s, episode steps:  81, steps per second: 109, episode reward: -223.293, mean reward: -2.757 [-100.000, 49.824], mean action: 1.444 [0.000, 3.000],  loss: 11.637643, mse: 487.939219, mean_q: 10.363482, mean_eps: 0.994921
   1819/300000: episode: 21, duration: 0.692s, episode steps:  85, steps per second: 123, episode reward: -205.147, mean reward: -2.413 [-100.000, 37.794], mean action: 1.341 [0.000, 3.000],  loss: 16.706656, mse: 474.812574, mean_q: 11.326009, mean_eps: 0.994672
   1879/300000: episode: 22, duration: 0.480s, episode steps:  60, steps per second: 125, episode reward: -98.028, mean reward: -1.634 [-100.000,  8.023], mean action: 1.317 [0.000, 3.000],  loss: 17.505746, mse: 511.312988, mean_q: 11.322213, mean_eps: 0.994454
   2005/300000: episode: 23, duration: 1.124s, episode steps: 126, steps per second: 112, episode reward: -385.680, mean reward: -3.061 [-100.000,  1.359], mean action: 1.460 [0.000, 3.000],  loss: 13.383143, mse: 532.020242, mean_q: 11.015444, mean_eps: 0.994175
   2072/300000: episode: 24, duration: 0.725s, episode steps:  67, steps per second:  92, episode reward: -121.350, mean reward: -1.811 [-100.000,  8.782], mean action: 1.522 [0.000, 3.000],  loss: 10.487616, mse: 646.032708, mean_q: 11.541477, mean_eps: 0.993886
   2185/300000: episode: 25, duration: 1.244s, episode steps: 113, steps per second:  91, episode reward: -225.228, mean reward: -1.993 [-100.000,  1.458], mean action: 1.531 [0.000, 3.000],  loss: 9.467257, mse: 669.608166, mean_q: 11.744360, mean_eps: 0.993616
   2282/300000: episode: 26, duration: 1.278s, episode steps:  97, steps per second:  76, episode reward: -110.783, mean reward: -1.142 [-100.000, 25.316], mean action: 1.557 [0.000, 3.000],  loss: 10.581401, mse: 726.424092, mean_q: 11.929668, mean_eps: 0.993301
   2337/300000: episode: 27, duration: 0.593s, episode steps:  55, steps per second:  93, episode reward: -87.821, mean reward: -1.597 [-100.000, 13.853], mean action: 1.273 [0.000, 3.000],  loss: 8.752149, mse: 802.671342, mean_q: 12.140509, mean_eps: 0.993073
   2392/300000: episode: 28, duration: 0.750s, episode steps:  55, steps per second:  73, episode reward: -199.526, mean reward: -3.628 [-100.000, 13.198], mean action: 1.545 [0.000, 3.000],  loss: 4.955707, mse: 800.197902, mean_q: 13.353691, mean_eps: 0.992908
   2486/300000: episode: 29, duration: 1.222s, episode steps:  94, steps per second:  77, episode reward: -115.519, mean reward: -1.229 [-100.000, 17.137], mean action: 1.585 [0.000, 3.000],  loss: 6.110671, mse: 877.331170, mean_q: 13.005796, mean_eps: 0.992685
   2590/300000: episode: 30, duration: 0.895s, episode steps: 104, steps per second: 116, episode reward: -237.674, mean reward: -2.285 [-100.000,  7.092], mean action: 1.327 [0.000, 3.000],  loss: 4.768618, mse: 916.506794, mean_q: 13.413557, mean_eps: 0.992387
   2679/300000: episode: 31, duration: 0.612s, episode steps:  89, steps per second: 145, episode reward: -351.715, mean reward: -3.952 [-100.000,  4.096], mean action: 1.584 [0.000, 3.000],  loss: 15.593983, mse: 983.284077, mean_q: 13.799992, mean_eps: 0.992098
   2737/300000: episode: 32, duration: 0.422s, episode steps:  58, steps per second: 137, episode reward: -58.578, mean reward: -1.010 [-100.000, 16.964], mean action: 1.414 [0.000, 3.000],  loss: 20.100230, mse: 982.839565, mean_q: 14.722341, mean_eps: 0.991877
   2857/300000: episode: 33, duration: 0.816s, episode steps: 120, steps per second: 147, episode reward: -185.229, mean reward: -1.544 [-100.000,  8.486], mean action: 1.625 [0.000, 3.000],  loss: 12.577555, mse: 1005.121308, mean_q: 15.741347, mean_eps: 0.991610
   2958/300000: episode: 34, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: -294.638, mean reward: -2.917 [-100.000,  2.303], mean action: 1.297 [0.000, 3.000],  loss: 13.414532, mse: 1072.260217, mean_q: 14.761678, mean_eps: 0.991279
   3021/300000: episode: 35, duration: 0.395s, episode steps:  63, steps per second: 159, episode reward: -57.063, mean reward: -0.906 [-100.000, 17.675], mean action: 1.397 [0.000, 3.000],  loss: 5.214301, mse: 1087.673654, mean_q: 15.137366, mean_eps: 0.991033
   3124/300000: episode: 36, duration: 0.634s, episode steps: 103, steps per second: 162, episode reward: -130.659, mean reward: -1.269 [-100.000, 30.542], mean action: 1.485 [0.000, 3.000],  loss: 10.147612, mse: 1089.885032, mean_q: 15.527123, mean_eps: 0.990784
   3223/300000: episode: 37, duration: 0.632s, episode steps:  99, steps per second: 157, episode reward: -121.635, mean reward: -1.229 [-100.000,  7.908], mean action: 1.434 [0.000, 3.000],  loss: 5.106680, mse: 1133.242633, mean_q: 14.963049, mean_eps: 0.990481
   3308/300000: episode: 38, duration: 0.554s, episode steps:  85, steps per second: 153, episode reward: -112.379, mean reward: -1.322 [-100.000,  7.724], mean action: 1.494 [0.000, 3.000],  loss: 7.588767, mse: 1181.258943, mean_q: 15.326673, mean_eps: 0.990205
   3399/300000: episode: 39, duration: 0.575s, episode steps:  91, steps per second: 158, episode reward: -200.578, mean reward: -2.204 [-100.000, 72.083], mean action: 1.440 [0.000, 3.000],  loss: 8.365731, mse: 1201.031805, mean_q: 15.628348, mean_eps: 0.989941
   3489/300000: episode: 40, duration: 0.583s, episode steps:  90, steps per second: 154, episode reward:  9.632, mean reward:  0.107 [-100.000, 99.345], mean action: 1.333 [0.000, 3.000],  loss: 11.146204, mse: 1277.262373, mean_q: 16.298272, mean_eps: 0.989669
   3579/300000: episode: 41, duration: 0.615s, episode steps:  90, steps per second: 146, episode reward: -235.895, mean reward: -2.621 [-100.000, 103.826], mean action: 1.367 [0.000, 3.000],  loss: 12.609835, mse: 1314.442975, mean_q: 16.881286, mean_eps: 0.989399
   3691/300000: episode: 42, duration: 0.696s, episode steps: 112, steps per second: 161, episode reward: -102.616, mean reward: -0.916 [-100.000,  7.435], mean action: 1.348 [0.000, 3.000],  loss: 9.988346, mse: 1340.516781, mean_q: 16.916631, mean_eps: 0.989096
   3815/300000: episode: 43, duration: 0.821s, episode steps: 124, steps per second: 151, episode reward: -102.700, mean reward: -0.828 [-100.000, 14.004], mean action: 1.556 [0.000, 3.000],  loss: 13.153874, mse: 1360.391843, mean_q: 18.158096, mean_eps: 0.988743
   3913/300000: episode: 44, duration: 0.674s, episode steps:  98, steps per second: 145, episode reward: -343.911, mean reward: -3.509 [-100.000,  0.534], mean action: 1.531 [0.000, 3.000],  loss: 12.421975, mse: 1459.514480, mean_q: 18.718104, mean_eps: 0.988409
   4020/300000: episode: 45, duration: 0.685s, episode steps: 107, steps per second: 156, episode reward: -101.999, mean reward: -0.953 [-100.000,  9.771], mean action: 1.589 [0.000, 3.000],  loss: 13.336846, mse: 1471.793605, mean_q: 19.227308, mean_eps: 0.988102
   4097/300000: episode: 46, duration: 0.475s, episode steps:  77, steps per second: 162, episode reward: -155.550, mean reward: -2.020 [-100.000,  7.384], mean action: 1.455 [0.000, 3.000],  loss: 13.981320, mse: 1561.561083, mean_q: 19.572307, mean_eps: 0.987826
   4208/300000: episode: 47, duration: 0.694s, episode steps: 111, steps per second: 160, episode reward: -366.241, mean reward: -3.299 [-100.000, 41.134], mean action: 1.640 [0.000, 3.000],  loss: 10.311549, mse: 1602.694821, mean_q: 18.867486, mean_eps: 0.987544
   4316/300000: episode: 48, duration: 0.673s, episode steps: 108, steps per second: 160, episode reward: -119.764, mean reward: -1.109 [-100.000, 14.823], mean action: 1.556 [0.000, 3.000],  loss: 13.922335, mse: 1589.049980, mean_q: 18.886528, mean_eps: 0.987216
   4409/300000: episode: 49, duration: 0.577s, episode steps:  93, steps per second: 161, episode reward:  3.290, mean reward:  0.035 [-100.000, 97.566], mean action: 1.462 [0.000, 3.000],  loss: 14.639524, mse: 1607.484470, mean_q: 19.582072, mean_eps: 0.986914
   4490/300000: episode: 50, duration: 0.488s, episode steps:  81, steps per second: 166, episode reward: -228.949, mean reward: -2.827 [-100.000,  4.934], mean action: 1.531 [0.000, 3.000],  loss: 12.154152, mse: 1647.218429, mean_q: 19.507811, mean_eps: 0.986653
   4568/300000: episode: 51, duration: 0.489s, episode steps:  78, steps per second: 159, episode reward: -110.032, mean reward: -1.411 [-100.000, 16.719], mean action: 1.641 [0.000, 3.000],  loss: 18.168094, mse: 1661.378792, mean_q: 19.220261, mean_eps: 0.986414
   4659/300000: episode: 52, duration: 0.568s, episode steps:  91, steps per second: 160, episode reward: -95.878, mean reward: -1.054 [-100.000, 15.766], mean action: 1.495 [0.000, 3.000],  loss: 10.018338, mse: 1724.000223, mean_q: 19.194611, mean_eps: 0.986161
   4756/300000: episode: 53, duration: 0.598s, episode steps:  97, steps per second: 162, episode reward: -329.424, mean reward: -3.396 [-100.000, 38.903], mean action: 1.680 [0.000, 3.000],  loss: 9.498658, mse: 1772.011485, mean_q: 19.079587, mean_eps: 0.985879
   4837/300000: episode: 54, duration: 0.497s, episode steps:  81, steps per second: 163, episode reward: -95.692, mean reward: -1.181 [-100.000,  8.094], mean action: 1.654 [0.000, 3.000],  loss: 10.612745, mse: 1821.940387, mean_q: 20.979099, mean_eps: 0.985612
   4908/300000: episode: 55, duration: 0.475s, episode steps:  71, steps per second: 149, episode reward: -86.216, mean reward: -1.214 [-100.000, 16.334], mean action: 1.394 [0.000, 3.000],  loss: 10.671606, mse: 1838.101795, mean_q: 19.841170, mean_eps: 0.985384
   4981/300000: episode: 56, duration: 0.454s, episode steps:  73, steps per second: 161, episode reward: -93.106, mean reward: -1.275 [-100.000, 10.091], mean action: 1.479 [0.000, 3.000],  loss: 12.092190, mse: 1875.398593, mean_q: 19.882466, mean_eps: 0.985168
   5092/300000: episode: 57, duration: 0.678s, episode steps: 111, steps per second: 164, episode reward: -41.200, mean reward: -0.371 [-100.000, 47.981], mean action: 1.577 [0.000, 3.000],  loss: 9.572587, mse: 1879.477907, mean_q: 20.008510, mean_eps: 0.984892
   5156/300000: episode: 58, duration: 0.396s, episode steps:  64, steps per second: 162, episode reward: -106.622, mean reward: -1.666 [-100.000, 26.012], mean action: 1.281 [0.000, 3.000],  loss: 10.175310, mse: 1977.483030, mean_q: 20.228419, mean_eps: 0.984629
   5258/300000: episode: 59, duration: 0.647s, episode steps: 102, steps per second: 158, episode reward: -32.568, mean reward: -0.319 [-100.000, 111.244], mean action: 1.431 [0.000, 3.000],  loss: 12.449778, mse: 2007.736151, mean_q: 19.742857, mean_eps: 0.984380
   5373/300000: episode: 60, duration: 0.795s, episode steps: 115, steps per second: 145, episode reward: -164.537, mean reward: -1.431 [-100.000,  8.263], mean action: 1.617 [0.000, 3.000],  loss: 12.367371, mse: 2041.720466, mean_q: 20.677100, mean_eps: 0.984055
   5446/300000: episode: 61, duration: 0.602s, episode steps:  73, steps per second: 121, episode reward: -233.266, mean reward: -3.195 [-100.000,  5.556], mean action: 1.534 [0.000, 3.000],  loss: 14.668646, mse: 2016.919969, mean_q: 20.909847, mean_eps: 0.983773
   5526/300000: episode: 62, duration: 0.574s, episode steps:  80, steps per second: 139, episode reward: -54.318, mean reward: -0.679 [-100.000,  7.993], mean action: 1.538 [0.000, 3.000],  loss: 11.603083, mse: 2059.944147, mean_q: 21.702902, mean_eps: 0.983544
   5637/300000: episode: 63, duration: 0.750s, episode steps: 111, steps per second: 148, episode reward: -339.952, mean reward: -3.063 [-100.000,  1.897], mean action: 1.324 [0.000, 3.000],  loss: 18.587261, mse: 2112.122651, mean_q: 21.493885, mean_eps: 0.983257
   5701/300000: episode: 64, duration: 0.395s, episode steps:  64, steps per second: 162, episode reward: -197.548, mean reward: -3.087 [-100.000,  4.806], mean action: 1.656 [0.000, 3.000],  loss: 11.371584, mse: 2118.732162, mean_q: 22.829047, mean_eps: 0.982994
   5797/300000: episode: 65, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: -117.440, mean reward: -1.223 [-100.000,  7.943], mean action: 1.375 [0.000, 3.000],  loss: 13.310236, mse: 2203.130155, mean_q: 21.987514, mean_eps: 0.982754
   5864/300000: episode: 66, duration: 0.477s, episode steps:  67, steps per second: 141, episode reward: -24.633, mean reward: -0.368 [-100.000, 69.606], mean action: 1.537 [0.000, 3.000],  loss: 13.436408, mse: 2229.444227, mean_q: 22.708821, mean_eps: 0.982510
   5933/300000: episode: 67, duration: 0.526s, episode steps:  69, steps per second: 131, episode reward: -193.660, mean reward: -2.807 [-100.000, 45.125], mean action: 1.449 [0.000, 3.000],  loss: 12.071290, mse: 2233.791713, mean_q: 22.523882, mean_eps: 0.982306
   6031/300000: episode: 68, duration: 0.748s, episode steps:  98, steps per second: 131, episode reward: -312.024, mean reward: -3.184 [-100.000, 12.816], mean action: 1.531 [0.000, 3.000],  loss: 11.564082, mse: 2270.998262, mean_q: 22.676658, mean_eps: 0.982055
   6113/300000: episode: 69, duration: 0.562s, episode steps:  82, steps per second: 146, episode reward: -93.533, mean reward: -1.141 [-100.000, 17.055], mean action: 1.317 [0.000, 3.000],  loss: 12.208137, mse: 2323.491543, mean_q: 21.537035, mean_eps: 0.981785
   6238/300000: episode: 70, duration: 0.874s, episode steps: 125, steps per second: 143, episode reward: -407.497, mean reward: -3.260 [-100.000,  1.975], mean action: 1.448 [0.000, 3.000],  loss: 10.776129, mse: 2336.950782, mean_q: 22.063978, mean_eps: 0.981475
   6309/300000: episode: 71, duration: 0.478s, episode steps:  71, steps per second: 149, episode reward: -81.649, mean reward: -1.150 [-100.000,  6.125], mean action: 1.662 [0.000, 3.000],  loss: 12.122945, mse: 2336.457599, mean_q: 20.995878, mean_eps: 0.981181
   6421/300000: episode: 72, duration: 0.744s, episode steps: 112, steps per second: 151, episode reward: -129.779, mean reward: -1.159 [-100.000, 12.488], mean action: 1.420 [0.000, 3.000],  loss: 13.424954, mse: 2360.208600, mean_q: 21.644376, mean_eps: 0.980906
   6494/300000: episode: 73, duration: 0.512s, episode steps:  73, steps per second: 143, episode reward: -113.220, mean reward: -1.551 [-100.000,  7.328], mean action: 1.493 [0.000, 3.000],  loss: 14.570521, mse: 2527.970783, mean_q: 24.211331, mean_eps: 0.980629
   6593/300000: episode: 74, duration: 0.750s, episode steps:  99, steps per second: 132, episode reward: -125.377, mean reward: -1.266 [-100.000,  4.076], mean action: 1.515 [0.000, 3.000],  loss: 15.520537, mse: 2529.259577, mean_q: 24.377931, mean_eps: 0.980371
   6722/300000: episode: 75, duration: 0.972s, episode steps: 129, steps per second: 133, episode reward: -344.347, mean reward: -2.669 [-100.000, 68.402], mean action: 1.574 [0.000, 3.000],  loss: 12.733636, mse: 2524.878035, mean_q: 24.693113, mean_eps: 0.980029
   6822/300000: episode: 76, duration: 1.148s, episode steps: 100, steps per second:  87, episode reward: -99.339, mean reward: -0.993 [-100.000, 12.407], mean action: 1.540 [0.000, 3.000],  loss: 10.937081, mse: 2595.259375, mean_q: 25.269757, mean_eps: 0.979686
   6896/300000: episode: 77, duration: 0.755s, episode steps:  74, steps per second:  98, episode reward: -130.342, mean reward: -1.761 [-100.000, 18.197], mean action: 1.662 [0.000, 3.000],  loss: 13.398034, mse: 2635.134317, mean_q: 24.856442, mean_eps: 0.979425
   7043/300000: episode: 78, duration: 1.657s, episode steps: 147, steps per second:  89, episode reward: -175.213, mean reward: -1.192 [-100.000,  7.626], mean action: 1.517 [0.000, 3.000],  loss: 15.583044, mse: 2642.130865, mean_q: 25.706510, mean_eps: 0.979093
   7133/300000: episode: 79, duration: 1.011s, episode steps:  90, steps per second:  89, episode reward: -201.629, mean reward: -2.240 [-100.000,  7.922], mean action: 1.500 [0.000, 3.000],  loss: 11.179875, mse: 2702.827979, mean_q: 26.143058, mean_eps: 0.978738
   7195/300000: episode: 80, duration: 0.629s, episode steps:  62, steps per second:  99, episode reward: -84.955, mean reward: -1.370 [-100.000,  6.427], mean action: 1.484 [0.000, 3.000],  loss: 15.297439, mse: 2770.340529, mean_q: 27.631522, mean_eps: 0.978510
   7255/300000: episode: 81, duration: 0.689s, episode steps:  60, steps per second:  87, episode reward: -91.237, mean reward: -1.521 [-100.000, 13.755], mean action: 1.533 [0.000, 3.000],  loss: 13.831589, mse: 2788.936385, mean_q: 27.697458, mean_eps: 0.978326
   7324/300000: episode: 82, duration: 0.651s, episode steps:  69, steps per second: 106, episode reward: -95.077, mean reward: -1.378 [-100.000, 11.005], mean action: 1.319 [0.000, 3.000],  loss: 13.333622, mse: 2777.734021, mean_q: 29.349570, mean_eps: 0.978133
   7402/300000: episode: 83, duration: 0.610s, episode steps:  78, steps per second: 128, episode reward: -283.682, mean reward: -3.637 [-100.000,  1.280], mean action: 1.615 [0.000, 3.000],  loss: 14.606203, mse: 2770.299498, mean_q: 27.206008, mean_eps: 0.977912
   7500/300000: episode: 84, duration: 0.716s, episode steps:  98, steps per second: 137, episode reward: -386.100, mean reward: -3.940 [-100.000,  5.146], mean action: 1.714 [0.000, 3.000],  loss: 11.125780, mse: 2881.953002, mean_q: 27.319012, mean_eps: 0.977649
   7611/300000: episode: 85, duration: 0.791s, episode steps: 111, steps per second: 140, episode reward: -386.445, mean reward: -3.481 [-100.000,  1.193], mean action: 1.495 [0.000, 3.000],  loss: 14.061785, mse: 2995.712679, mean_q: 29.885872, mean_eps: 0.977335
   7710/300000: episode: 86, duration: 0.759s, episode steps:  99, steps per second: 130, episode reward: -280.613, mean reward: -2.834 [-100.000,  7.273], mean action: 1.434 [0.000, 3.000],  loss: 13.833340, mse: 3070.796123, mean_q: 28.373957, mean_eps: 0.977020
   7821/300000: episode: 87, duration: 1.022s, episode steps: 111, steps per second: 109, episode reward: -194.421, mean reward: -1.752 [-100.000, 18.351], mean action: 1.631 [0.000, 3.000],  loss: 13.130988, mse: 3093.244277, mean_q: 27.405282, mean_eps: 0.976705
   7925/300000: episode: 88, duration: 0.876s, episode steps: 104, steps per second: 119, episode reward: -130.067, mean reward: -1.251 [-100.000, 16.541], mean action: 1.413 [0.000, 3.000],  loss: 14.091983, mse: 3113.580656, mean_q: 27.684936, mean_eps: 0.976382
   8051/300000: episode: 89, duration: 1.141s, episode steps: 126, steps per second: 110, episode reward: -224.489, mean reward: -1.782 [-100.000,  5.656], mean action: 1.603 [0.000, 3.000],  loss: 11.046994, mse: 3274.781151, mean_q: 31.414600, mean_eps: 0.976037
   8131/300000: episode: 90, duration: 0.696s, episode steps:  80, steps per second: 115, episode reward: -58.721, mean reward: -0.734 [-100.000, 25.904], mean action: 1.562 [0.000, 3.000],  loss: 9.346876, mse: 3310.230457, mean_q: 30.720258, mean_eps: 0.975728
   8219/300000: episode: 91, duration: 0.794s, episode steps:  88, steps per second: 111, episode reward: -64.140, mean reward: -0.729 [-100.000,  7.228], mean action: 1.375 [0.000, 3.000],  loss: 14.715258, mse: 3386.633317, mean_q: 30.839751, mean_eps: 0.975477
   8320/300000: episode: 92, duration: 0.859s, episode steps: 101, steps per second: 118, episode reward: -261.062, mean reward: -2.585 [-100.000, 18.655], mean action: 1.337 [0.000, 3.000],  loss: 12.550160, mse: 3433.195989, mean_q: 32.140989, mean_eps: 0.975193
   8433/300000: episode: 93, duration: 1.109s, episode steps: 113, steps per second: 102, episode reward: -113.581, mean reward: -1.005 [-100.000,  9.042], mean action: 1.257 [0.000, 3.000],  loss: 11.118294, mse: 3544.933477, mean_q: 32.678560, mean_eps: 0.974872
   8569/300000: episode: 94, duration: 0.963s, episode steps: 136, steps per second: 141, episode reward: -207.108, mean reward: -1.523 [-100.000, 10.271], mean action: 1.493 [0.000, 3.000],  loss: 14.748673, mse: 3560.541019, mean_q: 33.524315, mean_eps: 0.974498
   8647/300000: episode: 95, duration: 0.495s, episode steps:  78, steps per second: 158, episode reward: -268.707, mean reward: -3.445 [-100.000, 12.370], mean action: 1.372 [0.000, 3.000],  loss: 11.977841, mse: 3574.551745, mean_q: 32.341503, mean_eps: 0.974177
   8752/300000: episode: 96, duration: 0.691s, episode steps: 105, steps per second: 152, episode reward: -151.399, mean reward: -1.442 [-100.000, 14.910], mean action: 1.495 [0.000, 3.000],  loss: 12.802598, mse: 3611.857173, mean_q: 31.699878, mean_eps: 0.973903
   8867/300000: episode: 97, duration: 0.720s, episode steps: 115, steps per second: 160, episode reward: -515.556, mean reward: -4.483 [-100.000, 33.220], mean action: 1.591 [0.000, 3.000],  loss: 11.249426, mse: 3712.094228, mean_q: 32.498666, mean_eps: 0.973573
   8955/300000: episode: 98, duration: 0.657s, episode steps:  88, steps per second: 134, episode reward: -151.413, mean reward: -1.721 [-100.000,  9.719], mean action: 1.523 [0.000, 3.000],  loss: 16.243126, mse: 3714.200328, mean_q: 32.466102, mean_eps: 0.973268
   9058/300000: episode: 99, duration: 0.681s, episode steps: 103, steps per second: 151, episode reward: -370.913, mean reward: -3.601 [-100.000,  4.841], mean action: 1.544 [0.000, 3.000],  loss: 14.802713, mse: 3773.996077, mean_q: 32.770096, mean_eps: 0.972982
   9130/300000: episode: 100, duration: 0.446s, episode steps:  72, steps per second: 161, episode reward: -198.277, mean reward: -2.754 [-100.000, 29.663], mean action: 1.417 [0.000, 3.000],  loss: 15.506762, mse: 3868.742645, mean_q: 33.920862, mean_eps: 0.972719
   9193/300000: episode: 101, duration: 0.384s, episode steps:  63, steps per second: 164, episode reward: -84.593, mean reward: -1.343 [-100.000, 16.833], mean action: 1.540 [0.000, 3.000],  loss: 11.589309, mse: 3943.648612, mean_q: 33.743325, mean_eps: 0.972517
   9265/300000: episode: 102, duration: 0.430s, episode steps:  72, steps per second: 167, episode reward: -73.231, mean reward: -1.017 [-100.000, 14.508], mean action: 1.417 [0.000, 3.000],  loss: 12.989452, mse: 3915.605669, mean_q: 33.539585, mean_eps: 0.972314
   9378/300000: episode: 103, duration: 0.720s, episode steps: 113, steps per second: 157, episode reward: -278.537, mean reward: -2.465 [-100.000,  0.929], mean action: 1.487 [0.000, 3.000],  loss: 15.138593, mse: 3900.000048, mean_q: 35.135944, mean_eps: 0.972037
   9450/300000: episode: 104, duration: 0.544s, episode steps:  72, steps per second: 132, episode reward: -68.708, mean reward: -0.954 [-100.000, 11.405], mean action: 1.444 [0.000, 3.000],  loss: 14.507453, mse: 3936.983212, mean_q: 33.075222, mean_eps: 0.971759
   9557/300000: episode: 105, duration: 0.778s, episode steps: 107, steps per second: 138, episode reward: -437.577, mean reward: -4.090 [-100.000, 35.123], mean action: 1.561 [0.000, 3.000],  loss: 13.140933, mse: 4028.450275, mean_q: 33.893717, mean_eps: 0.971491
   9673/300000: episode: 106, duration: 0.854s, episode steps: 116, steps per second: 136, episode reward: -173.966, mean reward: -1.500 [-100.000, 14.931], mean action: 1.422 [0.000, 3.000],  loss: 14.560373, mse: 4153.933952, mean_q: 34.164110, mean_eps: 0.971156
   9755/300000: episode: 107, duration: 0.630s, episode steps:  82, steps per second: 130, episode reward: -175.197, mean reward: -2.137 [-100.000,  8.684], mean action: 1.476 [0.000, 3.000],  loss: 10.750231, mse: 4096.885486, mean_q: 34.943785, mean_eps: 0.970859
   9854/300000: episode: 108, duration: 0.730s, episode steps:  99, steps per second: 136, episode reward: -270.045, mean reward: -2.728 [-100.000,  0.865], mean action: 1.586 [0.000, 3.000],  loss: 14.849221, mse: 4118.064426, mean_q: 34.974442, mean_eps: 0.970588
   9915/300000: episode: 109, duration: 0.465s, episode steps:  61, steps per second: 131, episode reward: -90.658, mean reward: -1.486 [-100.000,  7.253], mean action: 1.361 [0.000, 3.000],  loss: 18.058035, mse: 4120.509409, mean_q: 33.499733, mean_eps: 0.970348
  10013/300000: episode: 110, duration: 0.731s, episode steps:  98, steps per second: 134, episode reward: -273.690, mean reward: -2.793 [-100.000,  7.375], mean action: 1.316 [0.000, 3.000],  loss: 15.171065, mse: 4065.283883, mean_q: 32.694793, mean_eps: 0.970109
  10149/300000: episode: 111, duration: 0.894s, episode steps: 136, steps per second: 152, episode reward: -155.125, mean reward: -1.141 [-100.000,  5.577], mean action: 1.368 [0.000, 3.000],  loss: 12.974673, mse: 4174.377831, mean_q: 34.368768, mean_eps: 0.969759
  10250/300000: episode: 112, duration: 0.659s, episode steps: 101, steps per second: 153, episode reward: -422.551, mean reward: -4.184 [-100.000,  0.814], mean action: 1.485 [0.000, 3.000],  loss: 14.946825, mse: 4228.863467, mean_q: 36.429271, mean_eps: 0.969403
  10372/300000: episode: 113, duration: 0.811s, episode steps: 122, steps per second: 150, episode reward: -286.184, mean reward: -2.346 [-100.000,  4.646], mean action: 1.221 [0.000, 3.000],  loss: 14.629372, mse: 4368.382564, mean_q: 32.760038, mean_eps: 0.969068
  10471/300000: episode: 114, duration: 0.656s, episode steps:  99, steps per second: 151, episode reward: -103.434, mean reward: -1.045 [-100.000, 11.453], mean action: 1.566 [0.000, 3.000],  loss: 14.609170, mse: 4369.581605, mean_q: 35.069246, mean_eps: 0.968737
  10541/300000: episode: 115, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: -38.364, mean reward: -0.548 [-100.000, 14.973], mean action: 1.386 [0.000, 3.000],  loss: 17.144643, mse: 4290.281267, mean_q: 34.727424, mean_eps: 0.968483
  10655/300000: episode: 116, duration: 0.862s, episode steps: 114, steps per second: 132, episode reward: -179.520, mean reward: -1.575 [-100.000, 16.144], mean action: 1.570 [0.000, 3.000],  loss: 17.975260, mse: 4303.212394, mean_q: 34.826788, mean_eps: 0.968207
  10764/300000: episode: 117, duration: 0.846s, episode steps: 109, steps per second: 129, episode reward: -186.679, mean reward: -1.713 [-100.000, 62.019], mean action: 1.459 [0.000, 3.000],  loss: 14.089547, mse: 4486.538460, mean_q: 32.967938, mean_eps: 0.967873
  10840/300000: episode: 118, duration: 0.824s, episode steps:  76, steps per second:  92, episode reward: -113.830, mean reward: -1.498 [-100.000, 10.258], mean action: 1.461 [0.000, 3.000],  loss: 14.646762, mse: 4517.652723, mean_q: 34.250474, mean_eps: 0.967596
  10953/300000: episode: 119, duration: 1.036s, episode steps: 113, steps per second: 109, episode reward: -94.314, mean reward: -0.835 [-100.000, 11.672], mean action: 1.442 [0.000, 3.000],  loss: 19.870757, mse: 4578.577842, mean_q: 33.118583, mean_eps: 0.967312
  11062/300000: episode: 120, duration: 0.841s, episode steps: 109, steps per second: 130, episode reward: -88.867, mean reward: -0.815 [-100.000, 11.164], mean action: 1.596 [0.000, 3.000],  loss: 16.473977, mse: 4546.119613, mean_q: 34.317402, mean_eps: 0.966979
  11144/300000: episode: 121, duration: 0.795s, episode steps:  82, steps per second: 103, episode reward: -106.413, mean reward: -1.298 [-100.000,  8.282], mean action: 1.585 [0.000, 3.000],  loss: 15.285646, mse: 4543.289232, mean_q: 34.215486, mean_eps: 0.966692
  11257/300000: episode: 122, duration: 0.791s, episode steps: 113, steps per second: 143, episode reward: -142.819, mean reward: -1.264 [-100.000,  4.919], mean action: 1.602 [0.000, 3.000],  loss: 12.515302, mse: 4590.830888, mean_q: 33.599099, mean_eps: 0.966400
  11325/300000: episode: 123, duration: 0.557s, episode steps:  68, steps per second: 122, episode reward: -102.611, mean reward: -1.509 [-100.000, 17.427], mean action: 1.647 [0.000, 3.000],  loss: 16.366498, mse: 4640.140693, mean_q: 32.859667, mean_eps: 0.966128
  11429/300000: episode: 124, duration: 0.867s, episode steps: 104, steps per second: 120, episode reward: -98.599, mean reward: -0.948 [-100.000,  7.597], mean action: 1.433 [0.000, 3.000],  loss: 12.010398, mse: 4614.923288, mean_q: 34.982299, mean_eps: 0.965870
  11527/300000: episode: 125, duration: 0.726s, episode steps:  98, steps per second: 135, episode reward: -268.973, mean reward: -2.745 [-100.000, 89.876], mean action: 1.582 [0.000, 3.000],  loss: 11.648102, mse: 4453.020720, mean_q: 33.396634, mean_eps: 0.965567
  11617/300000: episode: 126, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -161.356, mean reward: -1.793 [-100.000,  5.650], mean action: 1.633 [0.000, 3.000],  loss: 8.901924, mse: 4358.386347, mean_q: 31.898904, mean_eps: 0.965286
  11731/300000: episode: 127, duration: 0.948s, episode steps: 114, steps per second: 120, episode reward: -129.080, mean reward: -1.132 [-100.000, 13.542], mean action: 1.474 [0.000, 3.000],  loss: 13.540329, mse: 4369.239416, mean_q: 33.271273, mean_eps: 0.964979
  11848/300000: episode: 128, duration: 0.840s, episode steps: 117, steps per second: 139, episode reward: -158.263, mean reward: -1.353 [-100.000,  8.182], mean action: 1.513 [0.000, 3.000],  loss: 16.697698, mse: 4416.411529, mean_q: 33.443432, mean_eps: 0.964633
  11936/300000: episode: 129, duration: 0.626s, episode steps:  88, steps per second: 141, episode reward: -165.200, mean reward: -1.877 [-100.000, 56.019], mean action: 1.557 [0.000, 3.000],  loss: 8.092687, mse: 4358.865048, mean_q: 32.735966, mean_eps: 0.964326
  12055/300000: episode: 130, duration: 0.761s, episode steps: 119, steps per second: 156, episode reward: -109.835, mean reward: -0.923 [-100.000, 18.037], mean action: 1.613 [0.000, 3.000],  loss: 11.281090, mse: 4318.077364, mean_q: 31.553725, mean_eps: 0.964015
  12145/300000: episode: 131, duration: 0.547s, episode steps:  90, steps per second: 165, episode reward: -209.427, mean reward: -2.327 [-100.000,  6.700], mean action: 1.511 [0.000, 3.000],  loss: 12.682133, mse: 4257.936222, mean_q: 32.858697, mean_eps: 0.963701
  12213/300000: episode: 132, duration: 0.430s, episode steps:  68, steps per second: 158, episode reward: -140.497, mean reward: -2.066 [-100.000,  6.809], mean action: 1.529 [0.000, 3.000],  loss: 13.063984, mse: 4254.087054, mean_q: 30.918490, mean_eps: 0.963464
  12318/300000: episode: 133, duration: 0.692s, episode steps: 105, steps per second: 152, episode reward: -55.311, mean reward: -0.527 [-100.000, 83.246], mean action: 1.743 [0.000, 3.000],  loss: 12.021935, mse: 4235.131603, mean_q: 32.101749, mean_eps: 0.963205
  12380/300000: episode: 134, duration: 0.427s, episode steps:  62, steps per second: 145, episode reward: -167.054, mean reward: -2.694 [-100.000,  6.646], mean action: 1.677 [0.000, 3.000],  loss: 11.821874, mse: 4253.769771, mean_q: 30.676484, mean_eps: 0.962954
  12493/300000: episode: 135, duration: 0.710s, episode steps: 113, steps per second: 159, episode reward: -127.934, mean reward: -1.132 [-100.000,  9.595], mean action: 1.416 [0.000, 3.000],  loss: 11.713201, mse: 4149.045518, mean_q: 32.090244, mean_eps: 0.962692
  12560/300000: episode: 136, duration: 0.417s, episode steps:  67, steps per second: 161, episode reward: -80.177, mean reward: -1.197 [-100.000,  7.303], mean action: 1.687 [0.000, 3.000],  loss: 15.397599, mse: 4218.977535, mean_q: 32.292880, mean_eps: 0.962422
  12664/300000: episode: 137, duration: 0.703s, episode steps: 104, steps per second: 148, episode reward: -130.688, mean reward: -1.257 [-100.000, 12.216], mean action: 1.462 [0.000, 3.000],  loss: 13.171495, mse: 4150.743770, mean_q: 32.725342, mean_eps: 0.962166
  12777/300000: episode: 138, duration: 0.686s, episode steps: 113, steps per second: 165, episode reward: -114.796, mean reward: -1.016 [-100.000, 21.658], mean action: 1.407 [0.000, 3.000],  loss: 14.249383, mse: 4045.646901, mean_q: 33.759628, mean_eps: 0.961840
  12850/300000: episode: 139, duration: 0.438s, episode steps:  73, steps per second: 167, episode reward: -134.633, mean reward: -1.844 [-100.000,  8.771], mean action: 1.671 [0.000, 3.000],  loss: 16.818183, mse: 4163.411875, mean_q: 33.916528, mean_eps: 0.961561
  12935/300000: episode: 140, duration: 0.523s, episode steps:  85, steps per second: 162, episode reward: -154.739, mean reward: -1.820 [-100.000, 15.007], mean action: 1.435 [0.000, 3.000],  loss: 15.149291, mse: 4012.242067, mean_q: 31.755535, mean_eps: 0.961324
  13001/300000: episode: 141, duration: 0.441s, episode steps:  66, steps per second: 150, episode reward: -77.048, mean reward: -1.167 [-100.000, 25.199], mean action: 1.515 [0.000, 3.000],  loss: 13.032108, mse: 4089.295654, mean_q: 32.246756, mean_eps: 0.961097
  13092/300000: episode: 142, duration: 0.582s, episode steps:  91, steps per second: 156, episode reward: -193.097, mean reward: -2.122 [-100.000,  5.845], mean action: 1.341 [0.000, 3.000],  loss: 13.961709, mse: 4049.477365, mean_q: 33.561231, mean_eps: 0.960862
  13171/300000: episode: 143, duration: 0.472s, episode steps:  79, steps per second: 167, episode reward: -109.121, mean reward: -1.381 [-100.000, 24.678], mean action: 1.506 [0.000, 3.000],  loss: 11.483587, mse: 4079.043803, mean_q: 34.263228, mean_eps: 0.960607
  13269/300000: episode: 144, duration: 0.687s, episode steps:  98, steps per second: 143, episode reward: -127.955, mean reward: -1.306 [-100.000, 14.594], mean action: 1.510 [0.000, 3.000],  loss: 10.349229, mse: 4022.570567, mean_q: 34.460293, mean_eps: 0.960341
  13334/300000: episode: 145, duration: 0.471s, episode steps:  65, steps per second: 138, episode reward: -219.117, mean reward: -3.371 [-100.000,  8.134], mean action: 1.477 [0.000, 3.000],  loss: 11.939764, mse: 4020.333830, mean_q: 31.747578, mean_eps: 0.960097
  13433/300000: episode: 146, duration: 0.622s, episode steps:  99, steps per second: 159, episode reward: -111.381, mean reward: -1.125 [-100.000, 13.663], mean action: 1.566 [0.000, 3.000],  loss: 16.472423, mse: 4032.153199, mean_q: 32.648043, mean_eps: 0.959851
  13522/300000: episode: 147, duration: 0.537s, episode steps:  89, steps per second: 166, episode reward: -76.734, mean reward: -0.862 [-100.000, 11.942], mean action: 1.685 [0.000, 3.000],  loss: 11.579323, mse: 4011.348564, mean_q: 30.714885, mean_eps: 0.959569
  13618/300000: episode: 148, duration: 0.585s, episode steps:  96, steps per second: 164, episode reward: -170.928, mean reward: -1.781 [-100.000,  5.776], mean action: 1.500 [0.000, 3.000],  loss: 9.665430, mse: 4071.847354, mean_q: 31.558509, mean_eps: 0.959291
  13715/300000: episode: 149, duration: 0.639s, episode steps:  97, steps per second: 152, episode reward: -241.058, mean reward: -2.485 [-100.000, 16.358], mean action: 1.474 [0.000, 3.000],  loss: 16.664938, mse: 4177.019395, mean_q: 33.000216, mean_eps: 0.959002
  13781/300000: episode: 150, duration: 0.400s, episode steps:  66, steps per second: 165, episode reward: -96.288, mean reward: -1.459 [-100.000, 19.669], mean action: 1.455 [0.000, 3.000],  loss: 9.675167, mse: 4216.719105, mean_q: 32.864618, mean_eps: 0.958758
  13878/300000: episode: 151, duration: 0.610s, episode steps:  97, steps per second: 159, episode reward: -153.641, mean reward: -1.584 [-100.000,  6.472], mean action: 1.515 [0.000, 3.000],  loss: 10.777228, mse: 4291.383900, mean_q: 34.973814, mean_eps: 0.958513
  13940/300000: episode: 152, duration: 0.466s, episode steps:  62, steps per second: 133, episode reward: -109.169, mean reward: -1.761 [-100.000, 10.036], mean action: 1.435 [0.000, 3.000],  loss: 11.502150, mse: 4240.852543, mean_q: 34.341853, mean_eps: 0.958275
  14940/300000: episode: 153, duration: 7.198s, episode steps: 1000, steps per second: 139, episode reward: 10.824, mean reward:  0.011 [-22.552, 88.095], mean action: 1.571 [0.000, 3.000],  loss: 16.062425, mse: 3972.302207, mean_q: 32.669946, mean_eps: 0.956682
  15033/300000: episode: 154, duration: 0.603s, episode steps:  93, steps per second: 154, episode reward: -359.821, mean reward: -3.869 [-100.000,  0.280], mean action: 1.548 [0.000, 3.000],  loss: 20.704234, mse: 3734.800632, mean_q: 30.092850, mean_eps: 0.955042
  15135/300000: episode: 155, duration: 0.621s, episode steps: 102, steps per second: 164, episode reward: -91.726, mean reward: -0.899 [-100.000, 10.034], mean action: 1.618 [0.000, 3.000],  loss: 15.237649, mse: 3665.941097, mean_q: 31.206151, mean_eps: 0.954749
  15237/300000: episode: 156, duration: 0.647s, episode steps: 102, steps per second: 158, episode reward: -274.483, mean reward: -2.691 [-100.000, 101.241], mean action: 1.598 [0.000, 3.000],  loss: 16.790545, mse: 3555.920719, mean_q: 30.879027, mean_eps: 0.954443
  15314/300000: episode: 157, duration: 0.480s, episode steps:  77, steps per second: 160, episode reward: -74.278, mean reward: -0.965 [-100.000,  7.671], mean action: 1.545 [0.000, 3.000],  loss: 10.532895, mse: 3493.878443, mean_q: 30.014789, mean_eps: 0.954175
  15428/300000: episode: 158, duration: 0.709s, episode steps: 114, steps per second: 161, episode reward: -119.101, mean reward: -1.045 [-100.000,  5.409], mean action: 1.254 [0.000, 3.000],  loss: 14.067610, mse: 3542.665564, mean_q: 31.058240, mean_eps: 0.953889
  15533/300000: episode: 159, duration: 0.647s, episode steps: 105, steps per second: 162, episode reward: -77.576, mean reward: -0.739 [-100.000, 16.481], mean action: 1.429 [0.000, 3.000],  loss: 15.238673, mse: 3428.216676, mean_q: 30.100722, mean_eps: 0.953560
  15619/300000: episode: 160, duration: 0.562s, episode steps:  86, steps per second: 153, episode reward: -76.771, mean reward: -0.893 [-100.000, 15.592], mean action: 1.337 [0.000, 3.000],  loss: 16.379324, mse: 3357.187892, mean_q: 29.820891, mean_eps: 0.953273
  15714/300000: episode: 161, duration: 0.734s, episode steps:  95, steps per second: 129, episode reward: -95.490, mean reward: -1.005 [-100.000, 17.985], mean action: 1.474 [0.000, 3.000],  loss: 14.042899, mse: 3335.983794, mean_q: 29.417007, mean_eps: 0.953002
  15839/300000: episode: 162, duration: 0.966s, episode steps: 125, steps per second: 129, episode reward: -140.714, mean reward: -1.126 [-100.000, 65.426], mean action: 1.520 [0.000, 3.000],  loss: 11.398756, mse: 3261.696078, mean_q: 29.521895, mean_eps: 0.952672
  15942/300000: episode: 163, duration: 0.728s, episode steps: 103, steps per second: 142, episode reward: -224.706, mean reward: -2.182 [-100.000,  0.980], mean action: 1.602 [0.000, 3.000],  loss: 13.920664, mse: 3183.128728, mean_q: 29.655182, mean_eps: 0.952330
  16092/300000: episode: 164, duration: 1.048s, episode steps: 150, steps per second: 143, episode reward: -99.405, mean reward: -0.663 [-100.000, 43.584], mean action: 1.627 [0.000, 3.000],  loss: 12.361202, mse: 3177.012869, mean_q: 29.984870, mean_eps: 0.951951
  16179/300000: episode: 165, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -150.212, mean reward: -1.727 [-100.000,  9.404], mean action: 1.448 [0.000, 3.000],  loss: 8.905366, mse: 3185.613264, mean_q: 31.320167, mean_eps: 0.951595
  16251/300000: episode: 166, duration: 0.502s, episode steps:  72, steps per second: 144, episode reward: -75.410, mean reward: -1.047 [-100.000,  6.942], mean action: 1.472 [0.000, 3.000],  loss: 11.459645, mse: 3224.240363, mean_q: 29.846689, mean_eps: 0.951357
  16313/300000: episode: 167, duration: 0.393s, episode steps:  62, steps per second: 158, episode reward: -164.723, mean reward: -2.657 [-100.000,  5.897], mean action: 1.435 [0.000, 3.000],  loss: 12.632817, mse: 3264.639700, mean_q: 28.962578, mean_eps: 0.951156
  16379/300000: episode: 168, duration: 0.409s, episode steps:  66, steps per second: 161, episode reward: -68.643, mean reward: -1.040 [-100.000, 16.778], mean action: 1.515 [0.000, 3.000],  loss: 15.966489, mse: 3107.669611, mean_q: 30.006199, mean_eps: 0.950963
  16444/300000: episode: 169, duration: 0.407s, episode steps:  65, steps per second: 160, episode reward: -96.062, mean reward: -1.478 [-100.000,  8.402], mean action: 1.292 [0.000, 3.000],  loss: 9.201906, mse: 3160.006130, mean_q: 32.373484, mean_eps: 0.950767
  16520/300000: episode: 170, duration: 0.513s, episode steps:  76, steps per second: 148, episode reward: -61.842, mean reward: -0.814 [-100.000,  7.562], mean action: 1.566 [0.000, 3.000],  loss: 18.431135, mse: 3163.840403, mean_q: 29.846616, mean_eps: 0.950556
  16621/300000: episode: 171, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: -215.910, mean reward: -2.138 [-100.000,  0.781], mean action: 1.535 [0.000, 3.000],  loss: 14.941978, mse: 3167.513051, mean_q: 30.875265, mean_eps: 0.950290
  16703/300000: episode: 172, duration: 0.515s, episode steps:  82, steps per second: 159, episode reward: -100.300, mean reward: -1.223 [-100.000,  7.627], mean action: 1.488 [0.000, 3.000],  loss: 11.371209, mse: 3220.827250, mean_q: 30.872943, mean_eps: 0.950015
  16788/300000: episode: 173, duration: 0.577s, episode steps:  85, steps per second: 147, episode reward: -129.988, mean reward: -1.529 [-100.000,  4.687], mean action: 1.588 [0.000, 3.000],  loss: 12.988400, mse: 3231.069531, mean_q: 29.007510, mean_eps: 0.949765
  16889/300000: episode: 174, duration: 0.641s, episode steps: 101, steps per second: 157, episode reward: -460.308, mean reward: -4.558 [-100.000,  3.759], mean action: 1.525 [0.000, 3.000],  loss: 18.826593, mse: 3212.257051, mean_q: 30.213992, mean_eps: 0.949486
  16977/300000: episode: 175, duration: 0.547s, episode steps:  88, steps per second: 161, episode reward: -97.225, mean reward: -1.105 [-100.000,  6.501], mean action: 1.568 [0.000, 3.000],  loss: 20.861460, mse: 3263.522483, mean_q: 29.277551, mean_eps: 0.949203
  17082/300000: episode: 176, duration: 0.649s, episode steps: 105, steps per second: 162, episode reward: -521.245, mean reward: -4.964 [-100.000,  2.185], mean action: 1.733 [0.000, 3.000],  loss: 17.912707, mse: 3190.103790, mean_q: 29.008531, mean_eps: 0.948913
  17182/300000: episode: 177, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: -113.993, mean reward: -1.140 [-100.000,  6.415], mean action: 1.590 [0.000, 3.000],  loss: 20.367452, mse: 3094.386588, mean_q: 26.550080, mean_eps: 0.948605
  17260/300000: episode: 178, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -52.760, mean reward: -0.676 [-100.000, 16.862], mean action: 1.551 [0.000, 3.000],  loss: 15.016442, mse: 2895.762126, mean_q: 26.878337, mean_eps: 0.948339
  17364/300000: episode: 179, duration: 0.720s, episode steps: 104, steps per second: 144, episode reward: -228.047, mean reward: -2.193 [-100.000,  6.246], mean action: 1.529 [0.000, 3.000],  loss: 14.726136, mse: 3010.686474, mean_q: 27.376227, mean_eps: 0.948066
  17474/300000: episode: 180, duration: 0.796s, episode steps: 110, steps per second: 138, episode reward: -320.059, mean reward: -2.910 [-100.000,  3.344], mean action: 1.536 [0.000, 3.000],  loss: 13.803782, mse: 3025.455525, mean_q: 26.417765, mean_eps: 0.947745
  17552/300000: episode: 181, duration: 0.534s, episode steps:  78, steps per second: 146, episode reward: -146.116, mean reward: -1.873 [-100.000, 14.042], mean action: 1.692 [0.000, 3.000],  loss: 15.866816, mse: 2941.057767, mean_q: 25.677165, mean_eps: 0.947462
  17633/300000: episode: 182, duration: 0.536s, episode steps:  81, steps per second: 151, episode reward: 27.385, mean reward:  0.338 [-100.000, 104.045], mean action: 1.469 [0.000, 3.000],  loss: 23.634335, mse: 2857.470679, mean_q: 24.529295, mean_eps: 0.947224
  17732/300000: episode: 183, duration: 0.647s, episode steps:  99, steps per second: 153, episode reward: -69.513, mean reward: -0.702 [-100.000, 13.371], mean action: 1.414 [0.000, 3.000],  loss: 16.952393, mse: 2900.222500, mean_q: 26.920735, mean_eps: 0.946954
  17822/300000: episode: 184, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: -129.519, mean reward: -1.439 [-100.000, 10.037], mean action: 1.489 [0.000, 3.000],  loss: 19.290065, mse: 2846.815484, mean_q: 26.583606, mean_eps: 0.946670
  17889/300000: episode: 185, duration: 0.462s, episode steps:  67, steps per second: 145, episode reward: -92.059, mean reward: -1.374 [-100.000,  7.159], mean action: 1.507 [0.000, 3.000],  loss: 16.682993, mse: 2956.291941, mean_q: 28.377802, mean_eps: 0.946435
  17961/300000: episode: 186, duration: 0.455s, episode steps:  72, steps per second: 158, episode reward: -55.900, mean reward: -0.776 [-100.000, 12.161], mean action: 1.458 [0.000, 3.000],  loss: 16.302368, mse: 2916.886498, mean_q: 27.595805, mean_eps: 0.946227
  18027/300000: episode: 187, duration: 0.416s, episode steps:  66, steps per second: 159, episode reward: -205.776, mean reward: -3.118 [-100.000,  6.771], mean action: 1.682 [0.000, 3.000],  loss: 14.139134, mse: 2974.683305, mean_q: 27.144489, mean_eps: 0.946020
  18122/300000: episode: 188, duration: 0.626s, episode steps:  95, steps per second: 152, episode reward: -109.964, mean reward: -1.158 [-100.000,  5.759], mean action: 1.305 [0.000, 3.000],  loss: 13.979946, mse: 2912.224193, mean_q: 27.716801, mean_eps: 0.945778
  18242/300000: episode: 189, duration: 0.743s, episode steps: 120, steps per second: 162, episode reward: -125.965, mean reward: -1.050 [-100.000, 10.936], mean action: 1.592 [0.000, 3.000],  loss: 15.024416, mse: 2846.654132, mean_q: 27.533138, mean_eps: 0.945455
  18341/300000: episode: 190, duration: 0.641s, episode steps:  99, steps per second: 155, episode reward: -111.783, mean reward: -1.129 [-100.000, 16.340], mean action: 1.586 [0.000, 3.000],  loss: 16.895516, mse: 2838.582368, mean_q: 26.332259, mean_eps: 0.945127
  18408/300000: episode: 191, duration: 0.447s, episode steps:  67, steps per second: 150, episode reward: -120.005, mean reward: -1.791 [-100.000,  5.434], mean action: 1.433 [0.000, 3.000],  loss: 19.296232, mse: 2846.860681, mean_q: 25.724502, mean_eps: 0.944878
  18483/300000: episode: 192, duration: 0.486s, episode steps:  75, steps per second: 154, episode reward: -170.311, mean reward: -2.271 [-100.000,  7.390], mean action: 1.347 [0.000, 3.000],  loss: 12.179251, mse: 2867.408561, mean_q: 27.020220, mean_eps: 0.944665
  18572/300000: episode: 193, duration: 0.548s, episode steps:  89, steps per second: 162, episode reward: -98.480, mean reward: -1.107 [-100.000, 22.681], mean action: 1.404 [0.000, 3.000],  loss: 17.961686, mse: 2840.415161, mean_q: 24.894398, mean_eps: 0.944419
  18691/300000: episode: 194, duration: 0.722s, episode steps: 119, steps per second: 165, episode reward: -226.900, mean reward: -1.907 [-100.000,  9.063], mean action: 1.546 [0.000, 3.000],  loss: 15.780449, mse: 2916.308255, mean_q: 26.511573, mean_eps: 0.944107
  18823/300000: episode: 195, duration: 0.866s, episode steps: 132, steps per second: 152, episode reward: -3.613, mean reward: -0.027 [-100.000, 99.315], mean action: 1.568 [0.000, 3.000],  loss: 13.923258, mse: 2905.112349, mean_q: 27.269486, mean_eps: 0.943731
  18892/300000: episode: 196, duration: 0.435s, episode steps:  69, steps per second: 159, episode reward: -144.753, mean reward: -2.098 [-100.000,  5.217], mean action: 1.464 [0.000, 3.000],  loss: 17.560145, mse: 2858.818487, mean_q: 25.817054, mean_eps: 0.943429
  18962/300000: episode: 197, duration: 0.431s, episode steps:  70, steps per second: 162, episode reward: -66.852, mean reward: -0.955 [-100.000, 12.369], mean action: 1.314 [0.000, 3.000],  loss: 24.170009, mse: 2758.386136, mean_q: 23.633846, mean_eps: 0.943220
  19040/300000: episode: 198, duration: 0.485s, episode steps:  78, steps per second: 161, episode reward: -99.217, mean reward: -1.272 [-100.000,  7.761], mean action: 1.705 [0.000, 3.000],  loss: 19.853280, mse: 2749.027106, mean_q: 25.827054, mean_eps: 0.942998
  19158/300000: episode: 199, duration: 0.751s, episode steps: 118, steps per second: 157, episode reward: -130.892, mean reward: -1.109 [-100.000, 10.639], mean action: 1.347 [0.000, 3.000],  loss: 20.876899, mse: 2717.242114, mean_q: 22.890997, mean_eps: 0.942705
  19241/300000: episode: 200, duration: 0.529s, episode steps:  83, steps per second: 157, episode reward: -130.903, mean reward: -1.577 [-100.000,  9.354], mean action: 1.470 [0.000, 3.000],  loss: 14.512805, mse: 2547.631029, mean_q: 23.254326, mean_eps: 0.942403
  19319/300000: episode: 201, duration: 0.492s, episode steps:  78, steps per second: 158, episode reward: -131.854, mean reward: -1.690 [-100.000,  6.975], mean action: 1.513 [0.000, 3.000],  loss: 15.718472, mse: 2578.009221, mean_q: 23.993574, mean_eps: 0.942161
  19396/300000: episode: 202, duration: 0.488s, episode steps:  77, steps per second: 158, episode reward: -93.349, mean reward: -1.212 [-100.000,  8.459], mean action: 1.481 [0.000, 3.000],  loss: 15.757270, mse: 2567.685147, mean_q: 22.655290, mean_eps: 0.941929
  19469/300000: episode: 203, duration: 0.502s, episode steps:  73, steps per second: 145, episode reward: -183.863, mean reward: -2.519 [-100.000,  9.120], mean action: 1.562 [0.000, 3.000],  loss: 12.919623, mse: 2429.783033, mean_q: 23.167717, mean_eps: 0.941704
  19568/300000: episode: 204, duration: 0.644s, episode steps:  99, steps per second: 154, episode reward: -171.795, mean reward: -1.735 [-100.000,  8.733], mean action: 1.657 [0.000, 3.000],  loss: 18.848631, mse: 2353.109443, mean_q: 22.113744, mean_eps: 0.941446
  19629/300000: episode: 205, duration: 0.392s, episode steps:  61, steps per second: 156, episode reward: -78.095, mean reward: -1.280 [-100.000,  9.726], mean action: 1.131 [0.000, 3.000],  loss: 18.104112, mse: 2339.612589, mean_q: 20.862647, mean_eps: 0.941206
  19693/300000: episode: 206, duration: 0.406s, episode steps:  64, steps per second: 158, episode reward: -138.054, mean reward: -2.157 [-100.000, 23.035], mean action: 1.500 [0.000, 3.000],  loss: 19.907135, mse: 2424.021517, mean_q: 20.242652, mean_eps: 0.941018
  19807/300000: episode: 207, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: -92.273, mean reward: -0.809 [-100.000,  9.707], mean action: 1.474 [0.000, 3.000],  loss: 16.818831, mse: 2381.850380, mean_q: 21.411380, mean_eps: 0.940751
  19879/300000: episode: 208, duration: 0.453s, episode steps:  72, steps per second: 159, episode reward: -100.025, mean reward: -1.389 [-100.000,  9.147], mean action: 1.431 [0.000, 3.000],  loss: 18.582526, mse: 2490.727051, mean_q: 23.111267, mean_eps: 0.940473
  19970/300000: episode: 209, duration: 0.568s, episode steps:  91, steps per second: 160, episode reward: -271.692, mean reward: -2.986 [-100.000,  9.189], mean action: 1.495 [0.000, 3.000],  loss: 14.588693, mse: 2540.508827, mean_q: 22.098845, mean_eps: 0.940228
  20084/300000: episode: 210, duration: 0.706s, episode steps: 114, steps per second: 162, episode reward: -201.346, mean reward: -1.766 [-100.000,  1.283], mean action: 1.553 [0.000, 3.000],  loss: 17.553131, mse: 2470.590662, mean_q: 23.423658, mean_eps: 0.939920
  20151/300000: episode: 211, duration: 0.451s, episode steps:  67, steps per second: 148, episode reward: -88.462, mean reward: -1.320 [-100.000,  8.377], mean action: 1.343 [0.000, 3.000],  loss: 10.722793, mse: 2394.231464, mean_q: 23.566944, mean_eps: 0.939649
  20223/300000: episode: 212, duration: 0.470s, episode steps:  72, steps per second: 153, episode reward: -179.121, mean reward: -2.488 [-100.000, 14.474], mean action: 1.486 [0.000, 3.000],  loss: 10.793332, mse: 2467.144906, mean_q: 24.214258, mean_eps: 0.939441
  20344/300000: episode: 213, duration: 0.763s, episode steps: 121, steps per second: 159, episode reward: -251.785, mean reward: -2.081 [-100.000, 91.924], mean action: 1.636 [0.000, 3.000],  loss: 15.212677, mse: 2383.646697, mean_q: 23.007003, mean_eps: 0.939151
  20461/300000: episode: 214, duration: 0.736s, episode steps: 117, steps per second: 159, episode reward: -160.592, mean reward: -1.373 [-100.000,  3.770], mean action: 1.556 [0.000, 3.000],  loss: 16.556251, mse: 2233.940621, mean_q: 21.225306, mean_eps: 0.938794
  20549/300000: episode: 215, duration: 0.563s, episode steps:  88, steps per second: 156, episode reward: -84.941, mean reward: -0.965 [-100.000, 52.390], mean action: 1.636 [0.000, 3.000],  loss: 18.673107, mse: 2242.170165, mean_q: 21.862716, mean_eps: 0.938487
  20617/300000: episode: 216, duration: 0.422s, episode steps:  68, steps per second: 161, episode reward: -79.379, mean reward: -1.167 [-100.000, 16.240], mean action: 1.471 [0.000, 3.000],  loss: 14.611493, mse: 2257.864057, mean_q: 22.794903, mean_eps: 0.938253
  20694/300000: episode: 217, duration: 0.495s, episode steps:  77, steps per second: 156, episode reward: -51.826, mean reward: -0.673 [-100.000, 13.260], mean action: 1.662 [0.000, 3.000],  loss: 17.541958, mse: 2279.937590, mean_q: 22.571942, mean_eps: 0.938035
  20805/300000: episode: 218, duration: 0.728s, episode steps: 111, steps per second: 153, episode reward: -122.111, mean reward: -1.100 [-100.000,  8.129], mean action: 1.495 [0.000, 3.000],  loss: 16.997409, mse: 2331.952485, mean_q: 23.108159, mean_eps: 0.937753
  20882/300000: episode: 219, duration: 0.487s, episode steps:  77, steps per second: 158, episode reward: -88.859, mean reward: -1.154 [-100.000, 11.357], mean action: 1.649 [0.000, 3.000],  loss: 17.797287, mse: 2289.439792, mean_q: 23.379685, mean_eps: 0.937471
  20984/300000: episode: 220, duration: 0.662s, episode steps: 102, steps per second: 154, episode reward: -193.842, mean reward: -1.900 [-100.000,  8.686], mean action: 1.373 [0.000, 3.000],  loss: 16.992452, mse: 2275.574109, mean_q: 22.528622, mean_eps: 0.937203
  21090/300000: episode: 221, duration: 0.690s, episode steps: 106, steps per second: 154, episode reward: -131.759, mean reward: -1.243 [-100.000, 10.000], mean action: 1.491 [0.000, 3.000],  loss: 14.438703, mse: 2251.652864, mean_q: 23.701692, mean_eps: 0.936891
  21152/300000: episode: 222, duration: 0.422s, episode steps:  62, steps per second: 147, episode reward: -45.799, mean reward: -0.739 [-100.000, 81.964], mean action: 1.581 [0.000, 3.000],  loss: 13.595602, mse: 2294.285389, mean_q: 23.703407, mean_eps: 0.936639
  21234/300000: episode: 223, duration: 0.520s, episode steps:  82, steps per second: 158, episode reward: -160.146, mean reward: -1.953 [-100.000,  6.125], mean action: 1.439 [0.000, 3.000],  loss: 6.758586, mse: 2200.533208, mean_q: 23.104649, mean_eps: 0.936423
  21305/300000: episode: 224, duration: 0.450s, episode steps:  71, steps per second: 158, episode reward: -172.094, mean reward: -2.424 [-100.000,  5.571], mean action: 1.662 [0.000, 3.000],  loss: 13.953504, mse: 2133.942419, mean_q: 22.350683, mean_eps: 0.936193
  21430/300000: episode: 225, duration: 0.774s, episode steps: 125, steps per second: 161, episode reward: -85.921, mean reward: -0.687 [-100.000, 18.107], mean action: 1.440 [0.000, 3.000],  loss: 18.463486, mse: 2297.962764, mean_q: 22.814367, mean_eps: 0.935899
  21550/300000: episode: 226, duration: 0.767s, episode steps: 120, steps per second: 157, episode reward: -132.487, mean reward: -1.104 [-100.000, 11.728], mean action: 1.408 [0.000, 3.000],  loss: 16.524437, mse: 2337.996866, mean_q: 23.763824, mean_eps: 0.935531
  21685/300000: episode: 227, duration: 0.934s, episode steps: 135, steps per second: 145, episode reward: -404.275, mean reward: -2.995 [-100.000, 108.685], mean action: 1.644 [0.000, 3.000],  loss: 18.998793, mse: 2305.362184, mean_q: 23.454697, mean_eps: 0.935149
  21807/300000: episode: 228, duration: 0.806s, episode steps: 122, steps per second: 151, episode reward: -112.919, mean reward: -0.926 [-100.000, 21.364], mean action: 1.533 [0.000, 3.000],  loss: 24.219287, mse: 2250.400711, mean_q: 23.252449, mean_eps: 0.934763
  21939/300000: episode: 229, duration: 0.826s, episode steps: 132, steps per second: 160, episode reward: -190.535, mean reward: -1.443 [-100.000, 14.015], mean action: 1.523 [0.000, 3.000],  loss: 16.424612, mse: 2205.377966, mean_q: 24.806063, mean_eps: 0.934382
  22051/300000: episode: 230, duration: 0.691s, episode steps: 112, steps per second: 162, episode reward: -139.573, mean reward: -1.246 [-100.000, 35.018], mean action: 1.464 [0.000, 3.000],  loss: 21.315216, mse: 2171.553249, mean_q: 25.999828, mean_eps: 0.934017
  22121/300000: episode: 231, duration: 0.460s, episode steps:  70, steps per second: 152, episode reward: -119.056, mean reward: -1.701 [-100.000, 13.023], mean action: 1.614 [0.000, 3.000],  loss: 17.548334, mse: 2190.242317, mean_q: 26.260102, mean_eps: 0.933744
  22223/300000: episode: 232, duration: 0.654s, episode steps: 102, steps per second: 156, episode reward: -170.623, mean reward: -1.673 [-100.000, 22.826], mean action: 1.510 [0.000, 3.000],  loss: 16.931961, mse: 2218.292434, mean_q: 26.015742, mean_eps: 0.933485
  22292/300000: episode: 233, duration: 0.426s, episode steps:  69, steps per second: 162, episode reward: -75.570, mean reward: -1.095 [-100.000, 13.018], mean action: 1.638 [0.000, 3.000],  loss: 16.547785, mse: 2330.959427, mean_q: 26.694257, mean_eps: 0.933229
  22396/300000: episode: 234, duration: 0.659s, episode steps: 104, steps per second: 158, episode reward: -175.532, mean reward: -1.688 [-100.000, 14.859], mean action: 1.221 [0.000, 3.000],  loss: 18.770023, mse: 2263.829815, mean_q: 25.406309, mean_eps: 0.932970
  22477/300000: episode: 235, duration: 0.533s, episode steps:  81, steps per second: 152, episode reward: -91.614, mean reward: -1.131 [-100.000, 20.522], mean action: 1.667 [0.000, 3.000],  loss: 11.295921, mse: 2206.892898, mean_q: 23.732361, mean_eps: 0.932692
  22567/300000: episode: 236, duration: 0.578s, episode steps:  90, steps per second: 156, episode reward: -141.936, mean reward: -1.577 [-100.000, 40.159], mean action: 1.478 [0.000, 3.000],  loss: 13.971348, mse: 2231.141911, mean_q: 23.884737, mean_eps: 0.932435
  22633/300000: episode: 237, duration: 0.412s, episode steps:  66, steps per second: 160, episode reward: -81.814, mean reward: -1.240 [-100.000, 19.739], mean action: 1.500 [0.000, 3.000],  loss: 14.236306, mse: 2268.250930, mean_q: 24.360369, mean_eps: 0.932202
  22705/300000: episode: 238, duration: 0.449s, episode steps:  72, steps per second: 160, episode reward: -79.012, mean reward: -1.097 [-100.000, 14.719], mean action: 1.472 [0.000, 3.000],  loss: 13.565884, mse: 2302.357966, mean_q: 26.686877, mean_eps: 0.931995
  22810/300000: episode: 239, duration: 0.676s, episode steps: 105, steps per second: 155, episode reward: -165.577, mean reward: -1.577 [-100.000,  6.218], mean action: 1.543 [0.000, 3.000],  loss: 13.268247, mse: 2279.227863, mean_q: 25.963559, mean_eps: 0.931729
  22887/300000: episode: 240, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -87.123, mean reward: -1.131 [-100.000,  9.560], mean action: 1.494 [0.000, 3.000],  loss: 15.126955, mse: 2288.676728, mean_q: 24.928335, mean_eps: 0.931456
  22969/300000: episode: 241, duration: 0.515s, episode steps:  82, steps per second: 159, episode reward: -198.526, mean reward: -2.421 [-100.000, 30.308], mean action: 1.500 [0.000, 3.000],  loss: 14.982936, mse: 2402.165550, mean_q: 26.059620, mean_eps: 0.931217
  23080/300000: episode: 242, duration: 0.678s, episode steps: 111, steps per second: 164, episode reward: -117.861, mean reward: -1.062 [-100.000, 17.819], mean action: 1.495 [0.000, 3.000],  loss: 16.728361, mse: 2385.299465, mean_q: 25.575759, mean_eps: 0.930928
  23161/300000: episode: 243, duration: 0.542s, episode steps:  81, steps per second: 150, episode reward: -131.210, mean reward: -1.620 [-100.000, 11.876], mean action: 1.494 [0.000, 3.000],  loss: 17.144685, mse: 2435.422427, mean_q: 25.086158, mean_eps: 0.930640
  23241/300000: episode: 244, duration: 0.520s, episode steps:  80, steps per second: 154, episode reward: -256.415, mean reward: -3.205 [-100.000, 16.531], mean action: 1.700 [0.000, 3.000],  loss: 14.723795, mse: 2305.846881, mean_q: 24.370352, mean_eps: 0.930399
  23318/300000: episode: 245, duration: 0.482s, episode steps:  77, steps per second: 160, episode reward: -158.540, mean reward: -2.059 [-100.000, 15.012], mean action: 1.442 [0.000, 3.000],  loss: 18.499811, mse: 2381.180599, mean_q: 25.548542, mean_eps: 0.930163
  23395/300000: episode: 246, duration: 0.471s, episode steps:  77, steps per second: 163, episode reward: -134.934, mean reward: -1.752 [-100.000, 14.125], mean action: 1.377 [0.000, 3.000],  loss: 14.699761, mse: 2336.687257, mean_q: 24.453554, mean_eps: 0.929932
  23503/300000: episode: 247, duration: 0.699s, episode steps: 108, steps per second: 154, episode reward: -83.279, mean reward: -0.771 [-100.000,  6.156], mean action: 1.426 [0.000, 3.000],  loss: 13.236462, mse: 2390.099085, mean_q: 25.937836, mean_eps: 0.929655
  23628/300000: episode: 248, duration: 0.807s, episode steps: 125, steps per second: 155, episode reward: -108.637, mean reward: -0.869 [-100.000,  9.184], mean action: 1.536 [0.000, 3.000],  loss: 15.491481, mse: 2525.273535, mean_q: 25.081461, mean_eps: 0.929305
  23723/300000: episode: 249, duration: 0.580s, episode steps:  95, steps per second: 164, episode reward: -140.005, mean reward: -1.474 [-100.000,  6.285], mean action: 1.526 [0.000, 3.000],  loss: 24.512316, mse: 2512.418244, mean_q: 26.952829, mean_eps: 0.928975
  23844/300000: episode: 250, duration: 0.768s, episode steps: 121, steps per second: 157, episode reward: -179.976, mean reward: -1.487 [-100.000, 38.217], mean action: 1.603 [0.000, 3.000],  loss: 15.687089, mse: 2418.000478, mean_q: 27.252753, mean_eps: 0.928651
  23931/300000: episode: 251, duration: 0.561s, episode steps:  87, steps per second: 155, episode reward: -86.161, mean reward: -0.990 [-100.000, 10.864], mean action: 1.460 [0.000, 3.000],  loss: 15.672504, mse: 2389.616605, mean_q: 25.628144, mean_eps: 0.928339
  24024/300000: episode: 252, duration: 0.589s, episode steps:  93, steps per second: 158, episode reward: -70.751, mean reward: -0.761 [-100.000, 37.339], mean action: 1.516 [0.000, 3.000],  loss: 16.351817, mse: 2337.654173, mean_q: 26.628109, mean_eps: 0.928069
  24106/300000: episode: 253, duration: 0.507s, episode steps:  82, steps per second: 162, episode reward: -121.418, mean reward: -1.481 [-100.000, 10.184], mean action: 1.232 [0.000, 3.000],  loss: 16.621462, mse: 2361.190497, mean_q: 25.713106, mean_eps: 0.927807
  24240/300000: episode: 254, duration: 0.905s, episode steps: 134, steps per second: 148, episode reward: -55.550, mean reward: -0.415 [-100.000, 13.561], mean action: 1.425 [0.000, 3.000],  loss: 17.377169, mse: 2281.281220, mean_q: 26.831719, mean_eps: 0.927482
  24355/300000: episode: 255, duration: 0.744s, episode steps: 115, steps per second: 154, episode reward: -223.316, mean reward: -1.942 [-100.000,  7.587], mean action: 1.704 [0.000, 3.000],  loss: 16.013094, mse: 2330.546342, mean_q: 25.688013, mean_eps: 0.927109
  24437/300000: episode: 256, duration: 0.504s, episode steps:  82, steps per second: 163, episode reward: -290.410, mean reward: -3.542 [-100.000, 38.653], mean action: 1.415 [0.000, 3.000],  loss: 13.047393, mse: 2333.425504, mean_q: 26.149381, mean_eps: 0.926813
  24519/300000: episode: 257, duration: 0.534s, episode steps:  82, steps per second: 154, episode reward: -65.651, mean reward: -0.801 [-100.000, 19.860], mean action: 1.585 [0.000, 3.000],  loss: 15.640789, mse: 2312.554833, mean_q: 26.468066, mean_eps: 0.926568
  24640/300000: episode: 258, duration: 0.777s, episode steps: 121, steps per second: 156, episode reward: -179.277, mean reward: -1.482 [-100.000,  2.386], mean action: 1.595 [0.000, 3.000],  loss: 15.009595, mse: 2332.867565, mean_q: 27.156809, mean_eps: 0.926263
  24715/300000: episode: 259, duration: 0.478s, episode steps:  75, steps per second: 157, episode reward: -178.255, mean reward: -2.377 [-100.000,  7.822], mean action: 1.507 [0.000, 3.000],  loss: 16.410550, mse: 2218.961829, mean_q: 26.396513, mean_eps: 0.925969
  24787/300000: episode: 260, duration: 0.443s, episode steps:  72, steps per second: 163, episode reward: -144.292, mean reward: -2.004 [-100.000, 11.524], mean action: 1.403 [0.000, 3.000],  loss: 15.133062, mse: 2292.628293, mean_q: 26.730612, mean_eps: 0.925748
  24904/300000: episode: 261, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -98.252, mean reward: -0.840 [-100.000,  9.748], mean action: 1.650 [0.000, 3.000],  loss: 15.205802, mse: 2279.610448, mean_q: 24.265324, mean_eps: 0.925465
  25013/300000: episode: 262, duration: 0.712s, episode steps: 109, steps per second: 153, episode reward: -122.939, mean reward: -1.128 [-100.000, 11.281], mean action: 1.569 [0.000, 3.000],  loss: 17.920690, mse: 2356.368746, mean_q: 25.087056, mean_eps: 0.925126
  25105/300000: episode: 263, duration: 0.577s, episode steps:  92, steps per second: 159, episode reward: -112.836, mean reward: -1.226 [-100.000,  6.282], mean action: 1.359 [0.000, 3.000],  loss: 11.726672, mse: 2395.245263, mean_q: 26.012185, mean_eps: 0.924825
  25174/300000: episode: 264, duration: 0.465s, episode steps:  69, steps per second: 149, episode reward: -107.544, mean reward: -1.559 [-100.000, 14.247], mean action: 1.377 [0.000, 3.000],  loss: 13.341725, mse: 2388.546565, mean_q: 25.317786, mean_eps: 0.924583
  25248/300000: episode: 265, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -82.806, mean reward: -1.119 [-100.000, 21.633], mean action: 1.149 [0.000, 3.000],  loss: 16.653273, mse: 2430.310318, mean_q: 26.204213, mean_eps: 0.924369
  25416/300000: episode: 266, duration: 1.043s, episode steps: 168, steps per second: 161, episode reward: 40.973, mean reward:  0.244 [-100.000, 104.138], mean action: 1.470 [0.000, 3.000],  loss: 12.844275, mse: 2338.624911, mean_q: 24.082335, mean_eps: 0.924006
  25494/300000: episode: 267, duration: 0.512s, episode steps:  78, steps per second: 152, episode reward: -142.136, mean reward: -1.822 [-100.000,  4.758], mean action: 1.667 [0.000, 3.000],  loss: 11.265214, mse: 2339.551163, mean_q: 24.243901, mean_eps: 0.923637
  25559/300000: episode: 268, duration: 0.427s, episode steps:  65, steps per second: 152, episode reward: -97.451, mean reward: -1.499 [-100.000, 11.041], mean action: 1.508 [0.000, 3.000],  loss: 10.517055, mse: 2429.332003, mean_q: 22.814227, mean_eps: 0.923422
  25653/300000: episode: 269, duration: 0.593s, episode steps:  94, steps per second: 158, episode reward: -310.664, mean reward: -3.305 [-100.000, 26.088], mean action: 1.383 [0.000, 3.000],  loss: 14.307040, mse: 2367.861574, mean_q: 22.906868, mean_eps: 0.923183
  25721/300000: episode: 270, duration: 0.495s, episode steps:  68, steps per second: 137, episode reward: -82.038, mean reward: -1.206 [-100.000, 20.098], mean action: 1.294 [0.000, 3.000],  loss: 14.808479, mse: 2340.521869, mean_q: 22.493271, mean_eps: 0.922941
  25837/300000: episode: 271, duration: 0.943s, episode steps: 116, steps per second: 123, episode reward: -176.112, mean reward: -1.518 [-100.000,  7.046], mean action: 1.440 [0.000, 3.000],  loss: 16.152521, mse: 2339.157411, mean_q: 24.681845, mean_eps: 0.922664
  25962/300000: episode: 272, duration: 0.911s, episode steps: 125, steps per second: 137, episode reward: -424.399, mean reward: -3.395 [-100.000,  1.298], mean action: 1.456 [0.000, 3.000],  loss: 18.565690, mse: 2329.440598, mean_q: 23.863024, mean_eps: 0.922303
  26119/300000: episode: 273, duration: 1.147s, episode steps: 157, steps per second: 137, episode reward: -101.270, mean reward: -0.645 [-100.000, 12.531], mean action: 1.656 [0.000, 3.000],  loss: 18.327505, mse: 2297.630059, mean_q: 24.943237, mean_eps: 0.921880
  26203/300000: episode: 274, duration: 0.580s, episode steps:  84, steps per second: 145, episode reward: -144.082, mean reward: -1.715 [-100.000, 21.916], mean action: 1.595 [0.000, 3.000],  loss: 19.451248, mse: 2306.720578, mean_q: 24.551617, mean_eps: 0.921519
  26316/300000: episode: 275, duration: 0.758s, episode steps: 113, steps per second: 149, episode reward: -70.408, mean reward: -0.623 [-100.000, 16.248], mean action: 1.699 [0.000, 3.000],  loss: 11.458844, mse: 2350.187286, mean_q: 24.010697, mean_eps: 0.921223
  26418/300000: episode: 276, duration: 0.725s, episode steps: 102, steps per second: 141, episode reward: -33.066, mean reward: -0.324 [-100.000, 116.333], mean action: 1.451 [0.000, 3.000],  loss: 16.482544, mse: 2357.104198, mean_q: 24.620168, mean_eps: 0.920900
  26514/300000: episode: 277, duration: 0.643s, episode steps:  96, steps per second: 149, episode reward: -361.709, mean reward: -3.768 [-100.000,  0.423], mean action: 1.625 [0.000, 3.000],  loss: 16.582566, mse: 2414.336290, mean_q: 26.557039, mean_eps: 0.920604
  26632/300000: episode: 278, duration: 0.744s, episode steps: 118, steps per second: 159, episode reward: -247.268, mean reward: -2.095 [-100.000, 25.706], mean action: 1.627 [0.000, 3.000],  loss: 18.767350, mse: 2385.761811, mean_q: 25.440075, mean_eps: 0.920283
  26731/300000: episode: 279, duration: 0.654s, episode steps:  99, steps per second: 151, episode reward: -136.458, mean reward: -1.378 [-100.000, 14.030], mean action: 1.434 [0.000, 3.000],  loss: 15.939531, mse: 2443.697767, mean_q: 26.706908, mean_eps: 0.919957
  26799/300000: episode: 280, duration: 0.454s, episode steps:  68, steps per second: 150, episode reward: -134.757, mean reward: -1.982 [-100.000,  9.814], mean action: 1.779 [0.000, 3.000],  loss: 28.169104, mse: 2509.840293, mean_q: 25.168776, mean_eps: 0.919706
  26920/300000: episode: 281, duration: 0.767s, episode steps: 121, steps per second: 158, episode reward: -249.948, mean reward: -2.066 [-100.000, 61.903], mean action: 1.612 [0.000, 3.000],  loss: 18.585046, mse: 2409.046352, mean_q: 27.203695, mean_eps: 0.919423
  26981/300000: episode: 282, duration: 0.493s, episode steps:  61, steps per second: 124, episode reward: -95.159, mean reward: -1.560 [-100.000, 11.449], mean action: 1.475 [0.000, 3.000],  loss: 16.101272, mse: 2461.135512, mean_q: 27.325153, mean_eps: 0.919150
  27116/300000: episode: 283, duration: 0.952s, episode steps: 135, steps per second: 142, episode reward: -339.633, mean reward: -2.516 [-100.000, 88.630], mean action: 1.733 [0.000, 3.000],  loss: 15.865405, mse: 2431.739164, mean_q: 27.976306, mean_eps: 0.918856
  27209/300000: episode: 284, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -148.833, mean reward: -1.600 [-100.000,  7.390], mean action: 1.656 [0.000, 3.000],  loss: 18.516392, mse: 2392.143272, mean_q: 25.746659, mean_eps: 0.918514
  27299/300000: episode: 285, duration: 0.586s, episode steps:  90, steps per second: 153, episode reward: -137.916, mean reward: -1.532 [-100.000,  6.899], mean action: 1.500 [0.000, 3.000],  loss: 21.670864, mse: 2386.265069, mean_q: 27.213850, mean_eps: 0.918239
  27391/300000: episode: 286, duration: 0.700s, episode steps:  92, steps per second: 131, episode reward: -19.133, mean reward: -0.208 [-100.000, 79.937], mean action: 1.554 [0.000, 3.000],  loss: 15.079363, mse: 2425.972652, mean_q: 26.858545, mean_eps: 0.917966
  27463/300000: episode: 287, duration: 0.524s, episode steps:  72, steps per second: 137, episode reward: -73.819, mean reward: -1.025 [-100.000,  8.278], mean action: 1.681 [0.000, 3.000],  loss: 18.406976, mse: 2432.463554, mean_q: 29.213436, mean_eps: 0.917721
  27550/300000: episode: 288, duration: 0.624s, episode steps:  87, steps per second: 139, episode reward: -221.708, mean reward: -2.548 [-100.000,  4.629], mean action: 1.552 [0.000, 3.000],  loss: 18.145346, mse: 2512.728918, mean_q: 29.422575, mean_eps: 0.917482
  27666/300000: episode: 289, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -130.771, mean reward: -1.127 [-100.000, 15.212], mean action: 1.595 [0.000, 3.000],  loss: 13.923203, mse: 2488.440063, mean_q: 30.318596, mean_eps: 0.917177
  27760/300000: episode: 290, duration: 0.668s, episode steps:  94, steps per second: 141, episode reward: -184.618, mean reward: -1.964 [-100.000,  9.067], mean action: 1.660 [0.000, 3.000],  loss: 16.987994, mse: 2540.801944, mean_q: 29.394959, mean_eps: 0.916863
  27885/300000: episode: 291, duration: 0.833s, episode steps: 125, steps per second: 150, episode reward: -368.944, mean reward: -2.952 [-100.000, 96.866], mean action: 1.504 [0.000, 3.000],  loss: 14.594750, mse: 2490.271202, mean_q: 29.896312, mean_eps: 0.916534
  27995/300000: episode: 292, duration: 0.766s, episode steps: 110, steps per second: 144, episode reward: -98.260, mean reward: -0.893 [-100.000, 12.571], mean action: 1.409 [0.000, 3.000],  loss: 18.785287, mse: 2566.647395, mean_q: 27.895858, mean_eps: 0.916181
  28111/300000: episode: 293, duration: 0.754s, episode steps: 116, steps per second: 154, episode reward: -243.755, mean reward: -2.101 [-100.000,  1.427], mean action: 1.603 [0.000, 3.000],  loss: 17.545974, mse: 2583.604515, mean_q: 29.585957, mean_eps: 0.915843
  28197/300000: episode: 294, duration: 0.531s, episode steps:  86, steps per second: 162, episode reward: -64.082, mean reward: -0.745 [-100.000,  7.663], mean action: 1.488 [0.000, 3.000],  loss: 12.944363, mse: 2549.196107, mean_q: 28.861425, mean_eps: 0.915539
  28271/300000: episode: 295, duration: 0.468s, episode steps:  74, steps per second: 158, episode reward: -63.434, mean reward: -0.857 [-100.000,  9.192], mean action: 1.527 [0.000, 3.000],  loss: 20.125874, mse: 2466.480723, mean_q: 27.567203, mean_eps: 0.915300
  28359/300000: episode: 296, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -233.733, mean reward: -2.656 [-100.000,  6.923], mean action: 1.591 [0.000, 3.000],  loss: 16.283967, mse: 2504.902630, mean_q: 29.550967, mean_eps: 0.915056
  28423/300000: episode: 297, duration: 0.415s, episode steps:  64, steps per second: 154, episode reward: -175.224, mean reward: -2.738 [-100.000, 23.832], mean action: 1.656 [0.000, 3.000],  loss: 25.165274, mse: 2569.137531, mean_q: 29.117769, mean_eps: 0.914829
  28518/300000: episode: 298, duration: 0.595s, episode steps:  95, steps per second: 160, episode reward: -133.567, mean reward: -1.406 [-100.000, 13.233], mean action: 1.463 [0.000, 3.000],  loss: 20.924116, mse: 2454.207543, mean_q: 27.962981, mean_eps: 0.914590
  28618/300000: episode: 299, duration: 0.634s, episode steps: 100, steps per second: 158, episode reward: -96.299, mean reward: -0.963 [-100.000, 13.878], mean action: 1.530 [0.000, 3.000],  loss: 21.463858, mse: 2437.178003, mean_q: 28.087704, mean_eps: 0.914297
  28709/300000: episode: 300, duration: 0.629s, episode steps:  91, steps per second: 145, episode reward: -134.724, mean reward: -1.480 [-100.000,  6.667], mean action: 1.429 [0.000, 3.000],  loss: 15.221675, mse: 2415.557561, mean_q: 27.032687, mean_eps: 0.914011
  28805/300000: episode: 301, duration: 0.622s, episode steps:  96, steps per second: 154, episode reward: -109.082, mean reward: -1.136 [-100.000, 10.143], mean action: 1.635 [0.000, 3.000],  loss: 13.252452, mse: 2358.827966, mean_q: 25.711080, mean_eps: 0.913730
  28907/300000: episode: 302, duration: 0.633s, episode steps: 102, steps per second: 161, episode reward: -312.653, mean reward: -3.065 [-100.000,  0.707], mean action: 1.490 [0.000, 3.000],  loss: 17.129977, mse: 2397.006769, mean_q: 27.100667, mean_eps: 0.913433
  28964/300000: episode: 303, duration: 0.378s, episode steps:  57, steps per second: 151, episode reward: -97.500, mean reward: -1.711 [-100.000,  7.730], mean action: 1.421 [0.000, 3.000],  loss: 13.034813, mse: 2322.766357, mean_q: 27.505745, mean_eps: 0.913195
  29044/300000: episode: 304, duration: 0.564s, episode steps:  80, steps per second: 142, episode reward: -158.233, mean reward: -1.978 [-100.000, 20.043], mean action: 1.725 [0.000, 3.000],  loss: 12.267969, mse: 2416.182397, mean_q: 25.268689, mean_eps: 0.912989
  29101/300000: episode: 305, duration: 0.376s, episode steps:  57, steps per second: 151, episode reward: -144.338, mean reward: -2.532 [-100.000,  8.429], mean action: 1.561 [0.000, 3.000],  loss: 12.893009, mse: 2414.223504, mean_q: 25.626400, mean_eps: 0.912784
  29180/300000: episode: 306, duration: 0.500s, episode steps:  79, steps per second: 158, episode reward: -94.883, mean reward: -1.201 [-100.000, 17.330], mean action: 1.456 [0.000, 3.000],  loss: 17.059650, mse: 2494.235817, mean_q: 26.804861, mean_eps: 0.912580
  29246/300000: episode: 307, duration: 0.412s, episode steps:  66, steps per second: 160, episode reward: -65.827, mean reward: -0.997 [-100.000,  7.868], mean action: 1.485 [0.000, 3.000],  loss: 26.988577, mse: 2410.520195, mean_q: 26.357942, mean_eps: 0.912362
  29322/300000: episode: 308, duration: 0.509s, episode steps:  76, steps per second: 149, episode reward: -67.881, mean reward: -0.893 [-100.000, 10.004], mean action: 1.461 [0.000, 3.000],  loss: 16.864896, mse: 2418.983906, mean_q: 29.052610, mean_eps: 0.912150
  29419/300000: episode: 309, duration: 0.744s, episode steps:  97, steps per second: 130, episode reward: -112.340, mean reward: -1.158 [-100.000, 11.008], mean action: 1.588 [0.000, 3.000],  loss: 22.977212, mse: 2443.240851, mean_q: 28.304536, mean_eps: 0.911890
  29506/300000: episode: 310, duration: 0.645s, episode steps:  87, steps per second: 135, episode reward: -112.322, mean reward: -1.291 [-100.000, 17.081], mean action: 1.701 [0.000, 3.000],  loss: 17.295028, mse: 2496.663785, mean_q: 28.060192, mean_eps: 0.911614
  29615/300000: episode: 311, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -55.712, mean reward: -0.511 [-100.000, 17.312], mean action: 1.560 [0.000, 3.000],  loss: 17.972560, mse: 2517.330088, mean_q: 28.222555, mean_eps: 0.911320
  29732/300000: episode: 312, duration: 0.798s, episode steps: 117, steps per second: 147, episode reward: -120.394, mean reward: -1.029 [-100.000, 11.351], mean action: 1.650 [0.000, 3.000],  loss: 16.935556, mse: 2557.012413, mean_q: 31.865623, mean_eps: 0.910981
  29835/300000: episode: 313, duration: 0.732s, episode steps: 103, steps per second: 141, episode reward: -119.225, mean reward: -1.158 [-100.000,  6.899], mean action: 1.718 [0.000, 3.000],  loss: 19.092421, mse: 2477.670157, mean_q: 29.716217, mean_eps: 0.910651
  29940/300000: episode: 314, duration: 0.721s, episode steps: 105, steps per second: 146, episode reward: -113.072, mean reward: -1.077 [-100.000, 42.594], mean action: 1.590 [0.000, 3.000],  loss: 12.962230, mse: 2468.924878, mean_q: 30.486208, mean_eps: 0.910339
  30038/300000: episode: 315, duration: 0.643s, episode steps:  98, steps per second: 152, episode reward: -249.282, mean reward: -2.544 [-100.000, 47.367], mean action: 1.673 [0.000, 3.000],  loss: 12.049668, mse: 2505.806200, mean_q: 30.240878, mean_eps: 0.910035
  30115/300000: episode: 316, duration: 0.531s, episode steps:  77, steps per second: 145, episode reward: -126.466, mean reward: -1.642 [-100.000,  5.842], mean action: 1.714 [0.000, 3.000],  loss: 12.218349, mse: 2506.786788, mean_q: 30.057909, mean_eps: 0.909772
  30199/300000: episode: 317, duration: 0.559s, episode steps:  84, steps per second: 150, episode reward: -108.231, mean reward: -1.288 [-100.000,  5.913], mean action: 1.369 [0.000, 3.000],  loss: 11.109103, mse: 2603.708107, mean_q: 29.546338, mean_eps: 0.909531
  30312/300000: episode: 318, duration: 0.825s, episode steps: 113, steps per second: 137, episode reward: -88.587, mean reward: -0.784 [-100.000, 38.301], mean action: 1.664 [0.000, 3.000],  loss: 14.130394, mse: 2512.713573, mean_q: 29.717367, mean_eps: 0.909235
  30386/300000: episode: 319, duration: 0.493s, episode steps:  74, steps per second: 150, episode reward: -177.423, mean reward: -2.398 [-100.000, 79.394], mean action: 1.541 [0.000, 3.000],  loss: 11.116221, mse: 2499.212313, mean_q: 29.669018, mean_eps: 0.908955
  30477/300000: episode: 320, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: -121.772, mean reward: -1.338 [-100.000,  9.991], mean action: 1.615 [0.000, 3.000],  loss: 14.357166, mse: 2468.676059, mean_q: 30.550609, mean_eps: 0.908707
  31477/300000: episode: 321, duration: 7.039s, episode steps: 1000, steps per second: 142, episode reward: 38.953, mean reward:  0.039 [-23.942, 81.956], mean action: 1.540 [0.000, 3.000],  loss: 16.947910, mse: 2494.063596, mean_q: 30.376823, mean_eps: 0.907070
  31581/300000: episode: 322, duration: 0.674s, episode steps: 104, steps per second: 154, episode reward: -95.216, mean reward: -0.916 [-100.000,  9.636], mean action: 1.654 [0.000, 3.000],  loss: 16.015384, mse: 2583.734890, mean_q: 32.376509, mean_eps: 0.905415
  31676/300000: episode: 323, duration: 0.601s, episode steps:  95, steps per second: 158, episode reward: -98.241, mean reward: -1.034 [-100.000,  6.231], mean action: 1.389 [0.000, 3.000],  loss: 12.247444, mse: 2600.220071, mean_q: 31.050227, mean_eps: 0.905116
  31795/300000: episode: 324, duration: 0.792s, episode steps: 119, steps per second: 150, episode reward: -120.472, mean reward: -1.012 [-100.000,  8.571], mean action: 1.605 [0.000, 3.000],  loss: 20.556226, mse: 2662.393141, mean_q: 32.588525, mean_eps: 0.904795
  31858/300000: episode: 325, duration: 0.441s, episode steps:  63, steps per second: 143, episode reward: -82.229, mean reward: -1.305 [-100.000, 17.523], mean action: 1.651 [0.000, 3.000],  loss: 18.926569, mse: 2687.823033, mean_q: 33.928917, mean_eps: 0.904522
  31935/300000: episode: 326, duration: 0.502s, episode steps:  77, steps per second: 154, episode reward: -173.965, mean reward: -2.259 [-100.000,  7.929], mean action: 1.714 [0.000, 3.000],  loss: 20.412062, mse: 2716.591340, mean_q: 32.441666, mean_eps: 0.904312
  32086/300000: episode: 327, duration: 0.955s, episode steps: 151, steps per second: 158, episode reward: -154.056, mean reward: -1.020 [-100.000,  6.583], mean action: 1.589 [0.000, 3.000],  loss: 17.711688, mse: 2751.603259, mean_q: 32.692261, mean_eps: 0.903970
  32171/300000: episode: 328, duration: 0.713s, episode steps:  85, steps per second: 119, episode reward: -136.141, mean reward: -1.602 [-100.000,  7.435], mean action: 1.459 [0.000, 3.000],  loss: 12.482191, mse: 2814.535693, mean_q: 33.436734, mean_eps: 0.903616
  32252/300000: episode: 329, duration: 0.610s, episode steps:  81, steps per second: 133, episode reward: -123.033, mean reward: -1.519 [-100.000, 19.789], mean action: 1.457 [0.000, 3.000],  loss: 14.317577, mse: 2762.057897, mean_q: 32.579166, mean_eps: 0.903367
  32343/300000: episode: 330, duration: 0.608s, episode steps:  91, steps per second: 150, episode reward: -278.848, mean reward: -3.064 [-100.000, 94.743], mean action: 1.582 [0.000, 3.000],  loss: 26.174528, mse: 2656.382458, mean_q: 32.074005, mean_eps: 0.903109
  32452/300000: episode: 331, duration: 0.791s, episode steps: 109, steps per second: 138, episode reward: -122.800, mean reward: -1.127 [-100.000,  7.046], mean action: 1.404 [0.000, 3.000],  loss: 19.994546, mse: 2637.857930, mean_q: 33.347622, mean_eps: 0.902809
  32535/300000: episode: 332, duration: 0.556s, episode steps:  83, steps per second: 149, episode reward: -45.904, mean reward: -0.553 [-100.000, 26.903], mean action: 1.590 [0.000, 3.000],  loss: 19.765412, mse: 2748.781603, mean_q: 32.110471, mean_eps: 0.902521
  32638/300000: episode: 333, duration: 0.650s, episode steps: 103, steps per second: 158, episode reward: -132.294, mean reward: -1.284 [-100.000,  5.541], mean action: 1.660 [0.000, 3.000],  loss: 23.280537, mse: 2803.920939, mean_q: 33.400721, mean_eps: 0.902242
  32771/300000: episode: 334, duration: 0.886s, episode steps: 133, steps per second: 150, episode reward: -84.626, mean reward: -0.636 [-100.000, 22.131], mean action: 1.579 [0.000, 3.000],  loss: 24.553132, mse: 2742.953542, mean_q: 32.781746, mean_eps: 0.901888
  32893/300000: episode: 335, duration: 0.789s, episode steps: 122, steps per second: 155, episode reward: -77.051, mean reward: -0.632 [-100.000,  9.623], mean action: 1.541 [0.000, 3.000],  loss: 15.457836, mse: 2684.494107, mean_q: 31.772591, mean_eps: 0.901505
  33034/300000: episode: 336, duration: 0.895s, episode steps: 141, steps per second: 157, episode reward: -66.883, mean reward: -0.474 [-100.000,  6.936], mean action: 1.709 [0.000, 3.000],  loss: 17.641319, mse: 2633.684764, mean_q: 31.550918, mean_eps: 0.901111
  33110/300000: episode: 337, duration: 0.524s, episode steps:  76, steps per second: 145, episode reward: -192.277, mean reward: -2.530 [-100.000,  6.820], mean action: 1.592 [0.000, 3.000],  loss: 15.496332, mse: 2768.317319, mean_q: 32.754286, mean_eps: 0.900786
  33240/300000: episode: 338, duration: 0.838s, episode steps: 130, steps per second: 155, episode reward: -67.400, mean reward: -0.518 [-100.000, 85.470], mean action: 1.654 [0.000, 3.000],  loss: 14.268129, mse: 2729.568681, mean_q: 33.201022, mean_eps: 0.900477
  33324/300000: episode: 339, duration: 0.527s, episode steps:  84, steps per second: 159, episode reward: -82.270, mean reward: -0.979 [-100.000, 15.503], mean action: 1.560 [0.000, 3.000],  loss: 23.568337, mse: 2766.874047, mean_q: 34.294973, mean_eps: 0.900156
  33434/300000: episode: 340, duration: 0.771s, episode steps: 110, steps per second: 143, episode reward: -168.477, mean reward: -1.532 [-100.000, 10.750], mean action: 1.582 [0.000, 3.000],  loss: 18.578501, mse: 2734.779844, mean_q: 34.013841, mean_eps: 0.899864
  33521/300000: episode: 341, duration: 0.655s, episode steps:  87, steps per second: 133, episode reward: -108.200, mean reward: -1.244 [-100.000, 10.907], mean action: 1.690 [0.000, 3.000],  loss: 20.689706, mse: 2797.920550, mean_q: 32.593102, mean_eps: 0.899569
  33597/300000: episode: 342, duration: 0.511s, episode steps:  76, steps per second: 149, episode reward: -95.583, mean reward: -1.258 [-100.000,  7.327], mean action: 1.579 [0.000, 3.000],  loss: 14.376982, mse: 2798.806814, mean_q: 32.869799, mean_eps: 0.899325
  33713/300000: episode: 343, duration: 0.768s, episode steps: 116, steps per second: 151, episode reward: -158.758, mean reward: -1.369 [-100.000,  6.039], mean action: 1.560 [0.000, 3.000],  loss: 20.938019, mse: 2775.373045, mean_q: 34.279640, mean_eps: 0.899036
  33792/300000: episode: 344, duration: 0.529s, episode steps:  79, steps per second: 149, episode reward: -197.511, mean reward: -2.500 [-100.000,  7.662], mean action: 1.380 [0.000, 3.000],  loss: 24.103859, mse: 2749.984205, mean_q: 34.074573, mean_eps: 0.898744
  33856/300000: episode: 345, duration: 0.418s, episode steps:  64, steps per second: 153, episode reward: -96.867, mean reward: -1.514 [-100.000, 15.363], mean action: 1.469 [0.000, 3.000],  loss: 16.535200, mse: 2769.467377, mean_q: 34.158955, mean_eps: 0.898529
  33935/300000: episode: 346, duration: 0.513s, episode steps:  79, steps per second: 154, episode reward: -141.389, mean reward: -1.790 [-100.000,  6.033], mean action: 1.570 [0.000, 3.000],  loss: 17.868837, mse: 2840.278172, mean_q: 34.131575, mean_eps: 0.898315
  34012/300000: episode: 347, duration: 0.520s, episode steps:  77, steps per second: 148, episode reward: -125.313, mean reward: -1.627 [-100.000, 10.409], mean action: 1.377 [0.000, 3.000],  loss: 24.203635, mse: 2815.945122, mean_q: 34.184278, mean_eps: 0.898081
  34113/300000: episode: 348, duration: 0.694s, episode steps: 101, steps per second: 146, episode reward: -157.973, mean reward: -1.564 [-100.000, 39.699], mean action: 1.812 [0.000, 3.000],  loss: 19.620575, mse: 2819.119353, mean_q: 33.832557, mean_eps: 0.897814
  34226/300000: episode: 349, duration: 0.773s, episode steps: 113, steps per second: 146, episode reward: -104.479, mean reward: -0.925 [-100.000, 14.837], mean action: 1.451 [0.000, 3.000],  loss: 17.306154, mse: 2889.309337, mean_q: 36.230378, mean_eps: 0.897493
  34364/300000: episode: 350, duration: 0.918s, episode steps: 138, steps per second: 150, episode reward: -163.002, mean reward: -1.181 [-100.000, 24.741], mean action: 1.601 [0.000, 3.000],  loss: 18.618827, mse: 2920.575892, mean_q: 35.215070, mean_eps: 0.897116
  34439/300000: episode: 351, duration: 0.534s, episode steps:  75, steps per second: 140, episode reward: -101.194, mean reward: -1.349 [-100.000, 12.439], mean action: 1.413 [0.000, 3.000],  loss: 20.330252, mse: 2966.826257, mean_q: 34.461380, mean_eps: 0.896797
  34551/300000: episode: 352, duration: 0.756s, episode steps: 112, steps per second: 148, episode reward: -102.146, mean reward: -0.912 [-100.000, 14.213], mean action: 1.545 [0.000, 3.000],  loss: 16.861948, mse: 2991.688038, mean_q: 34.722439, mean_eps: 0.896516
  34658/300000: episode: 353, duration: 0.701s, episode steps: 107, steps per second: 153, episode reward: -35.692, mean reward: -0.334 [-100.000, 116.817], mean action: 1.505 [0.000, 3.000],  loss: 24.451101, mse: 3103.140488, mean_q: 34.556816, mean_eps: 0.896188
  34743/300000: episode: 354, duration: 0.561s, episode steps:  85, steps per second: 152, episode reward: -116.283, mean reward: -1.368 [-100.000,  7.225], mean action: 1.776 [0.000, 3.000],  loss: 18.191488, mse: 3159.822223, mean_q: 32.830187, mean_eps: 0.895900
  34854/300000: episode: 355, duration: 0.729s, episode steps: 111, steps per second: 152, episode reward: -161.020, mean reward: -1.451 [-100.000,  8.928], mean action: 1.613 [0.000, 3.000],  loss: 12.375743, mse: 3068.777781, mean_q: 34.966813, mean_eps: 0.895606
  34908/300000: episode: 356, duration: 0.351s, episode steps:  54, steps per second: 154, episode reward: -86.883, mean reward: -1.609 [-100.000,  9.069], mean action: 1.037 [0.000, 3.000],  loss: 18.893511, mse: 3075.340047, mean_q: 36.280799, mean_eps: 0.895359
  35001/300000: episode: 357, duration: 0.610s, episode steps:  93, steps per second: 153, episode reward: -34.602, mean reward: -0.372 [-100.000, 12.096], mean action: 1.645 [0.000, 3.000],  loss: 19.107408, mse: 3188.532355, mean_q: 35.356388, mean_eps: 0.895138
  35078/300000: episode: 358, duration: 0.517s, episode steps:  77, steps per second: 149, episode reward: -70.067, mean reward: -0.910 [-100.000, 38.201], mean action: 1.714 [0.000, 3.000],  loss: 22.751838, mse: 3101.893406, mean_q: 35.445251, mean_eps: 0.894883
  35173/300000: episode: 359, duration: 0.618s, episode steps:  95, steps per second: 154, episode reward: -85.974, mean reward: -0.905 [-100.000,  7.310], mean action: 1.589 [0.000, 3.000],  loss: 13.842512, mse: 3132.318704, mean_q: 35.060594, mean_eps: 0.894625
  35307/300000: episode: 360, duration: 0.883s, episode steps: 134, steps per second: 152, episode reward: -79.328, mean reward: -0.592 [-100.000,  8.109], mean action: 1.381 [0.000, 3.000],  loss: 17.604391, mse: 3152.138617, mean_q: 36.237933, mean_eps: 0.894281
  35427/300000: episode: 361, duration: 0.890s, episode steps: 120, steps per second: 135, episode reward: -151.416, mean reward: -1.262 [-100.000,  7.753], mean action: 1.442 [0.000, 3.000],  loss: 18.083225, mse: 3167.547024, mean_q: 36.766486, mean_eps: 0.893900
  35521/300000: episode: 362, duration: 0.731s, episode steps:  94, steps per second: 129, episode reward: -288.522, mean reward: -3.069 [-100.000,  8.981], mean action: 1.468 [0.000, 3.000],  loss: 22.879070, mse: 3130.908390, mean_q: 35.834950, mean_eps: 0.893579
  35614/300000: episode: 363, duration: 0.690s, episode steps:  93, steps per second: 135, episode reward: -64.941, mean reward: -0.698 [-100.000, 16.765], mean action: 1.559 [0.000, 3.000],  loss: 23.802028, mse: 3173.788887, mean_q: 36.137243, mean_eps: 0.893299
  35699/300000: episode: 364, duration: 0.620s, episode steps:  85, steps per second: 137, episode reward: -83.249, mean reward: -0.979 [-100.000,  7.900], mean action: 1.612 [0.000, 3.000],  loss: 20.594678, mse: 3358.841082, mean_q: 37.958325, mean_eps: 0.893032
  35800/300000: episode: 365, duration: 0.720s, episode steps: 101, steps per second: 140, episode reward: -82.532, mean reward: -0.817 [-100.000,  5.671], mean action: 1.673 [0.000, 3.000],  loss: 21.358952, mse: 3361.443782, mean_q: 37.313148, mean_eps: 0.892753
  35887/300000: episode: 366, duration: 0.649s, episode steps:  87, steps per second: 134, episode reward: -62.220, mean reward: -0.715 [-100.000, 16.689], mean action: 1.563 [0.000, 3.000],  loss: 15.277418, mse: 3276.261222, mean_q: 38.596504, mean_eps: 0.892471
  35973/300000: episode: 367, duration: 0.646s, episode steps:  86, steps per second: 133, episode reward: -110.097, mean reward: -1.280 [-100.000, 72.331], mean action: 1.535 [0.000, 3.000],  loss: 15.483106, mse: 3254.162495, mean_q: 38.820827, mean_eps: 0.892211
  36066/300000: episode: 368, duration: 0.624s, episode steps:  93, steps per second: 149, episode reward: -74.598, mean reward: -0.802 [-100.000,  7.555], mean action: 1.613 [0.000, 3.000],  loss: 22.802320, mse: 3276.817750, mean_q: 38.908037, mean_eps: 0.891943
  36160/300000: episode: 369, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -185.197, mean reward: -1.970 [-100.000,  5.260], mean action: 1.617 [0.000, 3.000],  loss: 19.263449, mse: 3418.787276, mean_q: 38.322687, mean_eps: 0.891663
  36285/300000: episode: 370, duration: 0.861s, episode steps: 125, steps per second: 145, episode reward: -154.538, mean reward: -1.236 [-100.000,  8.056], mean action: 1.480 [0.000, 3.000],  loss: 18.067260, mse: 3321.587068, mean_q: 39.490604, mean_eps: 0.891334
  36365/300000: episode: 371, duration: 0.513s, episode steps:  80, steps per second: 156, episode reward: -43.164, mean reward: -0.540 [-100.000, 12.262], mean action: 1.550 [0.000, 3.000],  loss: 20.619151, mse: 3310.231924, mean_q: 39.277176, mean_eps: 0.891026
  36994/300000: episode: 372, duration: 4.454s, episode steps: 629, steps per second: 141, episode reward: -233.541, mean reward: -0.371 [-100.000, 141.025], mean action: 1.547 [0.000, 3.000],  loss: 18.204375, mse: 3443.650373, mean_q: 41.301964, mean_eps: 0.889963
  37072/300000: episode: 373, duration: 0.546s, episode steps:  78, steps per second: 143, episode reward: -192.897, mean reward: -2.473 [-100.000, 57.894], mean action: 1.679 [0.000, 3.000],  loss: 13.692209, mse: 3417.112924, mean_q: 40.093176, mean_eps: 0.888903
  37174/300000: episode: 374, duration: 0.752s, episode steps: 102, steps per second: 136, episode reward: -133.084, mean reward: -1.305 [-100.000, 16.265], mean action: 1.696 [0.000, 3.000],  loss: 16.733551, mse: 3418.845679, mean_q: 40.869724, mean_eps: 0.888633
  37287/300000: episode: 375, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -99.422, mean reward: -0.880 [-100.000, 11.900], mean action: 1.558 [0.000, 3.000],  loss: 22.012357, mse: 3396.032065, mean_q: 42.048595, mean_eps: 0.888310
  37406/300000: episode: 376, duration: 0.839s, episode steps: 119, steps per second: 142, episode reward: -114.155, mean reward: -0.959 [-100.000, 10.954], mean action: 1.471 [0.000, 3.000],  loss: 28.389475, mse: 3412.410193, mean_q: 42.294442, mean_eps: 0.887962
  37493/300000: episode: 377, duration: 0.649s, episode steps:  87, steps per second: 134, episode reward: -63.684, mean reward: -0.732 [-100.000,  8.140], mean action: 1.575 [0.000, 3.000],  loss: 20.129227, mse: 3397.670466, mean_q: 43.740760, mean_eps: 0.887653
  37590/300000: episode: 378, duration: 0.636s, episode steps:  97, steps per second: 152, episode reward: -208.292, mean reward: -2.147 [-100.000,  7.891], mean action: 1.639 [0.000, 3.000],  loss: 19.757320, mse: 3297.342026, mean_q: 40.043361, mean_eps: 0.887377
  37665/300000: episode: 379, duration: 0.478s, episode steps:  75, steps per second: 157, episode reward: -72.665, mean reward: -0.969 [-100.000, 15.143], mean action: 1.387 [0.000, 3.000],  loss: 19.988653, mse: 3317.553564, mean_q: 40.459624, mean_eps: 0.887119
  37761/300000: episode: 380, duration: 0.646s, episode steps:  96, steps per second: 149, episode reward: -101.260, mean reward: -1.055 [-100.000,  6.273], mean action: 1.344 [0.000, 3.000],  loss: 27.453425, mse: 3335.612022, mean_q: 39.424600, mean_eps: 0.886862
  37819/300000: episode: 381, duration: 0.407s, episode steps:  58, steps per second: 143, episode reward: -136.146, mean reward: -2.347 [-100.000,  6.342], mean action: 1.397 [0.000, 3.000],  loss: 22.044222, mse: 3320.324193, mean_q: 39.192797, mean_eps: 0.886632
  37916/300000: episode: 382, duration: 0.630s, episode steps:  97, steps per second: 154, episode reward: -125.965, mean reward: -1.299 [-100.000, 12.801], mean action: 1.505 [0.000, 3.000],  loss: 14.916680, mse: 3366.786563, mean_q: 39.178677, mean_eps: 0.886399
  38007/300000: episode: 383, duration: 0.580s, episode steps:  91, steps per second: 157, episode reward: -103.277, mean reward: -1.135 [-100.000,  8.370], mean action: 1.297 [0.000, 3.000],  loss: 21.042826, mse: 3367.066626, mean_q: 39.923062, mean_eps: 0.886117
  38105/300000: episode: 384, duration: 0.644s, episode steps:  98, steps per second: 152, episode reward: -253.745, mean reward: -2.589 [-100.000,  0.994], mean action: 1.500 [0.000, 3.000],  loss: 19.632524, mse: 3387.953130, mean_q: 39.568895, mean_eps: 0.885833
  38211/300000: episode: 385, duration: 0.705s, episode steps: 106, steps per second: 150, episode reward: -111.394, mean reward: -1.051 [-100.000, 12.265], mean action: 1.698 [0.000, 3.000],  loss: 27.595755, mse: 3366.816980, mean_q: 41.280178, mean_eps: 0.885527
  38323/300000: episode: 386, duration: 0.726s, episode steps: 112, steps per second: 154, episode reward: -108.629, mean reward: -0.970 [-100.000, 10.682], mean action: 1.652 [0.000, 3.000],  loss: 28.828226, mse: 3365.302196, mean_q: 39.271880, mean_eps: 0.885201
  38403/300000: episode: 387, duration: 0.528s, episode steps:  80, steps per second: 151, episode reward: -129.457, mean reward: -1.618 [-100.000, 12.152], mean action: 1.887 [0.000, 3.000],  loss: 30.588904, mse: 3424.482587, mean_q: 40.459435, mean_eps: 0.884913
  38464/300000: episode: 388, duration: 0.423s, episode steps:  61, steps per second: 144, episode reward: -70.665, mean reward: -1.158 [-100.000, 15.964], mean action: 1.410 [0.000, 3.000],  loss: 21.479271, mse: 3515.463071, mean_q: 39.698626, mean_eps: 0.884701
  38548/300000: episode: 389, duration: 0.564s, episode steps:  84, steps per second: 149, episode reward: -108.670, mean reward: -1.294 [-100.000, 18.518], mean action: 1.464 [0.000, 3.000],  loss: 29.631259, mse: 3511.889186, mean_q: 40.258493, mean_eps: 0.884483
  38691/300000: episode: 390, duration: 0.931s, episode steps: 143, steps per second: 154, episode reward: -47.470, mean reward: -0.332 [-100.000, 16.268], mean action: 1.483 [0.000, 3.000],  loss: 19.393517, mse: 3530.297356, mean_q: 41.193683, mean_eps: 0.884143
  38776/300000: episode: 391, duration: 0.587s, episode steps:  85, steps per second: 145, episode reward: -87.760, mean reward: -1.032 [-100.000, 10.216], mean action: 1.565 [0.000, 3.000],  loss: 15.822106, mse: 3507.204851, mean_q: 39.911838, mean_eps: 0.883801
  38877/300000: episode: 392, duration: 0.683s, episode steps: 101, steps per second: 148, episode reward: -136.411, mean reward: -1.351 [-100.000, 10.542], mean action: 1.584 [0.000, 3.000],  loss: 17.462182, mse: 3506.335831, mean_q: 39.244833, mean_eps: 0.883522
  38974/300000: episode: 393, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -176.992, mean reward: -1.825 [-100.000,  6.847], mean action: 1.732 [0.000, 3.000],  loss: 18.484905, mse: 3465.055619, mean_q: 38.053328, mean_eps: 0.883225
  39041/300000: episode: 394, duration: 0.453s, episode steps:  67, steps per second: 148, episode reward: -114.290, mean reward: -1.706 [-100.000, 14.064], mean action: 1.373 [0.000, 3.000],  loss: 24.628719, mse: 3490.940736, mean_q: 37.446824, mean_eps: 0.882979
  39130/300000: episode: 395, duration: 0.637s, episode steps:  89, steps per second: 140, episode reward: -112.132, mean reward: -1.260 [-100.000, 15.119], mean action: 1.438 [0.000, 3.000],  loss: 17.547842, mse: 3453.446103, mean_q: 40.215249, mean_eps: 0.882745
  39251/300000: episode: 396, duration: 0.797s, episode steps: 121, steps per second: 152, episode reward: -101.020, mean reward: -0.835 [-100.000,  5.911], mean action: 1.479 [0.000, 3.000],  loss: 19.415812, mse: 3518.631035, mean_q: 40.656986, mean_eps: 0.882430
  39345/300000: episode: 397, duration: 0.606s, episode steps:  94, steps per second: 155, episode reward: -181.940, mean reward: -1.936 [-100.000,  9.971], mean action: 1.745 [0.000, 3.000],  loss: 20.832225, mse: 3469.651204, mean_q: 39.516646, mean_eps: 0.882107
  39413/300000: episode: 398, duration: 0.490s, episode steps:  68, steps per second: 139, episode reward: -65.567, mean reward: -0.964 [-100.000, 13.348], mean action: 1.691 [0.000, 3.000],  loss: 18.046448, mse: 3556.265130, mean_q: 40.240776, mean_eps: 0.881865
  39521/300000: episode: 399, duration: 0.737s, episode steps: 108, steps per second: 147, episode reward: -149.024, mean reward: -1.380 [-100.000, 41.354], mean action: 1.667 [0.000, 3.000],  loss: 15.261760, mse: 3543.214294, mean_q: 41.232921, mean_eps: 0.881600
  39596/300000: episode: 400, duration: 0.505s, episode steps:  75, steps per second: 148, episode reward: -122.965, mean reward: -1.640 [-100.000, 10.976], mean action: 1.520 [0.000, 3.000],  loss: 16.667154, mse: 3578.445980, mean_q: 40.436104, mean_eps: 0.881326
  39677/300000: episode: 401, duration: 0.526s, episode steps:  81, steps per second: 154, episode reward: -49.059, mean reward: -0.606 [-100.000, 16.517], mean action: 1.667 [0.000, 3.000],  loss: 20.579449, mse: 3535.644706, mean_q: 39.560355, mean_eps: 0.881092
  39749/300000: episode: 402, duration: 0.501s, episode steps:  72, steps per second: 144, episode reward: -117.758, mean reward: -1.636 [-100.000,  6.204], mean action: 1.458 [0.000, 3.000],  loss: 16.739027, mse: 3570.441538, mean_q: 37.782151, mean_eps: 0.880862
  39825/300000: episode: 403, duration: 0.501s, episode steps:  76, steps per second: 152, episode reward: -86.150, mean reward: -1.134 [-100.000, 14.545], mean action: 1.605 [0.000, 3.000],  loss: 22.308138, mse: 3655.599523, mean_q: 40.873748, mean_eps: 0.880641
  39936/300000: episode: 404, duration: 0.842s, episode steps: 111, steps per second: 132, episode reward: -223.125, mean reward: -2.010 [-100.000,  9.217], mean action: 1.559 [0.000, 3.000],  loss: 26.975960, mse: 3728.386657, mean_q: 41.237610, mean_eps: 0.880360
  40050/300000: episode: 405, duration: 0.781s, episode steps: 114, steps per second: 146, episode reward: -89.457, mean reward: -0.785 [-100.000, 10.635], mean action: 1.605 [0.000, 3.000],  loss: 32.237905, mse: 3641.791294, mean_q: 40.069076, mean_eps: 0.880023
  40114/300000: episode: 406, duration: 0.437s, episode steps:  64, steps per second: 146, episode reward: -58.836, mean reward: -0.919 [-100.000, 17.348], mean action: 1.453 [0.000, 3.000],  loss: 18.006780, mse: 3622.444515, mean_q: 40.404729, mean_eps: 0.879756
  40242/300000: episode: 407, duration: 0.844s, episode steps: 128, steps per second: 152, episode reward: -102.020, mean reward: -0.797 [-100.000, 23.741], mean action: 1.445 [0.000, 3.000],  loss: 17.909280, mse: 3546.004379, mean_q: 39.746516, mean_eps: 0.879468
  40361/300000: episode: 408, duration: 0.800s, episode steps: 119, steps per second: 149, episode reward: -154.691, mean reward: -1.300 [-100.000, 22.424], mean action: 1.563 [0.000, 3.000],  loss: 21.363899, mse: 3699.918342, mean_q: 38.594536, mean_eps: 0.879097
  40461/300000: episode: 409, duration: 0.665s, episode steps: 100, steps per second: 150, episode reward: -79.126, mean reward: -0.791 [-100.000,  7.366], mean action: 1.730 [0.000, 3.000],  loss: 17.628484, mse: 3632.787817, mean_q: 39.778999, mean_eps: 0.878769
  40564/300000: episode: 410, duration: 0.672s, episode steps: 103, steps per second: 153, episode reward: -287.373, mean reward: -2.790 [-100.000,  6.647], mean action: 1.553 [0.000, 3.000],  loss: 17.803802, mse: 3680.413434, mean_q: 40.456437, mean_eps: 0.878464
  40661/300000: episode: 411, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -120.227, mean reward: -1.239 [-100.000, 21.115], mean action: 1.619 [0.000, 3.000],  loss: 16.794497, mse: 3660.542921, mean_q: 39.850784, mean_eps: 0.878164
  40741/300000: episode: 412, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -149.808, mean reward: -1.873 [-100.000, 30.491], mean action: 1.538 [0.000, 3.000],  loss: 19.155459, mse: 3616.119101, mean_q: 41.025388, mean_eps: 0.877899
  40819/300000: episode: 413, duration: 0.511s, episode steps:  78, steps per second: 153, episode reward: -80.053, mean reward: -1.026 [-100.000, 25.101], mean action: 1.590 [0.000, 3.000],  loss: 18.920712, mse: 3696.503246, mean_q: 39.809215, mean_eps: 0.877661
  40904/300000: episode: 414, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -26.316, mean reward: -0.310 [-100.000, 12.215], mean action: 1.718 [0.000, 3.000],  loss: 19.669693, mse: 3766.835943, mean_q: 39.741177, mean_eps: 0.877417
  40987/300000: episode: 415, duration: 0.593s, episode steps:  83, steps per second: 140, episode reward: -118.896, mean reward: -1.432 [-100.000, 14.447], mean action: 1.446 [0.000, 3.000],  loss: 13.657258, mse: 3739.507121, mean_q: 40.534064, mean_eps: 0.877165
  41087/300000: episode: 416, duration: 0.669s, episode steps: 100, steps per second: 150, episode reward: -83.192, mean reward: -0.832 [-100.000, 18.084], mean action: 1.790 [0.000, 3.000],  loss: 19.433723, mse: 3778.360901, mean_q: 40.699475, mean_eps: 0.876890
  41222/300000: episode: 417, duration: 0.880s, episode steps: 135, steps per second: 153, episode reward: -93.979, mean reward: -0.696 [-100.000, 16.171], mean action: 1.496 [0.000, 3.000],  loss: 20.550787, mse: 3760.007205, mean_q: 41.354261, mean_eps: 0.876538
  41309/300000: episode: 418, duration: 0.597s, episode steps:  87, steps per second: 146, episode reward: -109.527, mean reward: -1.259 [-100.000, 11.063], mean action: 1.736 [0.000, 3.000],  loss: 28.811740, mse: 3770.873414, mean_q: 39.916543, mean_eps: 0.876205
  41393/300000: episode: 419, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -88.738, mean reward: -1.056 [-100.000, 15.499], mean action: 1.488 [0.000, 3.000],  loss: 17.935086, mse: 3657.632359, mean_q: 39.069199, mean_eps: 0.875948
  41466/300000: episode: 420, duration: 0.476s, episode steps:  73, steps per second: 153, episode reward: -78.242, mean reward: -1.072 [-100.000, 16.593], mean action: 1.438 [0.000, 3.000],  loss: 18.710752, mse: 3765.732964, mean_q: 41.471567, mean_eps: 0.875713
  41562/300000: episode: 421, duration: 0.626s, episode steps:  96, steps per second: 153, episode reward: -124.065, mean reward: -1.292 [-100.000,  6.358], mean action: 1.448 [0.000, 3.000],  loss: 21.341118, mse: 3790.795390, mean_q: 40.674216, mean_eps: 0.875459
  41688/300000: episode: 422, duration: 0.862s, episode steps: 126, steps per second: 146, episode reward: -94.554, mean reward: -0.750 [-100.000,  6.407], mean action: 1.683 [0.000, 3.000],  loss: 23.530350, mse: 3763.821047, mean_q: 42.573561, mean_eps: 0.875127
  41765/300000: episode: 423, duration: 0.509s, episode steps:  77, steps per second: 151, episode reward: -39.402, mean reward: -0.512 [-100.000, 10.336], mean action: 1.623 [0.000, 3.000],  loss: 15.359429, mse: 3874.511605, mean_q: 43.044160, mean_eps: 0.874822
  41851/300000: episode: 424, duration: 0.558s, episode steps:  86, steps per second: 154, episode reward: -117.783, mean reward: -1.370 [-100.000,  5.782], mean action: 1.419 [0.000, 3.000],  loss: 20.578760, mse: 3889.296248, mean_q: 44.164299, mean_eps: 0.874577
  41957/300000: episode: 425, duration: 0.760s, episode steps: 106, steps per second: 140, episode reward: -24.362, mean reward: -0.230 [-100.000, 96.145], mean action: 1.717 [0.000, 3.000],  loss: 19.424923, mse: 3839.232224, mean_q: 43.373933, mean_eps: 0.874289
  42025/300000: episode: 426, duration: 0.461s, episode steps:  68, steps per second: 148, episode reward: -26.648, mean reward: -0.392 [-100.000, 17.894], mean action: 1.471 [0.000, 3.000],  loss: 11.430932, mse: 3888.683946, mean_q: 42.637594, mean_eps: 0.874028
  42158/300000: episode: 427, duration: 0.872s, episode steps: 133, steps per second: 153, episode reward: -48.064, mean reward: -0.361 [-100.000, 58.567], mean action: 1.571 [0.000, 3.000],  loss: 10.874241, mse: 4041.416620, mean_q: 43.786705, mean_eps: 0.873727
  42234/300000: episode: 428, duration: 0.501s, episode steps:  76, steps per second: 152, episode reward: -39.810, mean reward: -0.524 [-100.000, 11.693], mean action: 1.461 [0.000, 3.000],  loss: 23.404008, mse: 4077.639523, mean_q: 45.861692, mean_eps: 0.873414
  42304/300000: episode: 429, duration: 0.502s, episode steps:  70, steps per second: 139, episode reward: -69.656, mean reward: -0.995 [-100.000, 14.426], mean action: 1.629 [0.000, 3.000],  loss: 21.821947, mse: 4075.501733, mean_q: 46.750878, mean_eps: 0.873195
  42368/300000: episode: 430, duration: 0.423s, episode steps:  64, steps per second: 151, episode reward: -87.381, mean reward: -1.365 [-100.000,  8.451], mean action: 1.531 [0.000, 3.000],  loss: 23.187331, mse: 4097.914822, mean_q: 46.454465, mean_eps: 0.872993
  42467/300000: episode: 431, duration: 0.659s, episode steps:  99, steps per second: 150, episode reward: -178.626, mean reward: -1.804 [-100.000,  5.635], mean action: 1.788 [0.000, 3.000],  loss: 21.710383, mse: 4114.502227, mean_q: 46.493953, mean_eps: 0.872749
  42541/300000: episode: 432, duration: 0.493s, episode steps:  74, steps per second: 150, episode reward: -80.662, mean reward: -1.090 [-100.000, 11.694], mean action: 1.649 [0.000, 3.000],  loss: 19.379999, mse: 4067.340075, mean_q: 45.026511, mean_eps: 0.872490
  42609/300000: episode: 433, duration: 0.485s, episode steps:  68, steps per second: 140, episode reward: -81.712, mean reward: -1.202 [-100.000, 24.232], mean action: 1.618 [0.000, 3.000],  loss: 20.539364, mse: 4057.756229, mean_q: 45.087840, mean_eps: 0.872277
  42729/300000: episode: 434, duration: 0.859s, episode steps: 120, steps per second: 140, episode reward: -157.186, mean reward: -1.310 [-100.000, 18.503], mean action: 1.542 [0.000, 3.000],  loss: 15.445318, mse: 4248.773600, mean_q: 47.164922, mean_eps: 0.871995
  42800/300000: episode: 435, duration: 0.481s, episode steps:  71, steps per second: 148, episode reward: -62.732, mean reward: -0.884 [-100.000, 10.018], mean action: 1.690 [0.000, 3.000],  loss: 8.565061, mse: 4349.286116, mean_q: 48.358353, mean_eps: 0.871708
  42884/300000: episode: 436, duration: 0.570s, episode steps:  84, steps per second: 147, episode reward: -100.133, mean reward: -1.192 [-100.000, 10.572], mean action: 1.417 [0.000, 3.000],  loss: 23.080810, mse: 4396.773051, mean_q: 47.580256, mean_eps: 0.871475
  42988/300000: episode: 437, duration: 0.733s, episode steps: 104, steps per second: 142, episode reward: -90.937, mean reward: -0.874 [-100.000,  7.155], mean action: 1.538 [0.000, 3.000],  loss: 27.190647, mse: 4441.546319, mean_q: 47.279748, mean_eps: 0.871193
  43059/300000: episode: 438, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -115.765, mean reward: -1.630 [-100.000,  6.711], mean action: 1.634 [0.000, 3.000],  loss: 18.896253, mse: 4428.652609, mean_q: 48.605634, mean_eps: 0.870931
  43224/300000: episode: 439, duration: 1.096s, episode steps: 165, steps per second: 151, episode reward: -25.779, mean reward: -0.156 [-100.000, 37.602], mean action: 1.582 [0.000, 3.000],  loss: 18.787371, mse: 4377.505343, mean_q: 47.358802, mean_eps: 0.870577
  43316/300000: episode: 440, duration: 0.635s, episode steps:  92, steps per second: 145, episode reward: -73.117, mean reward: -0.795 [-100.000, 21.649], mean action: 1.435 [0.000, 3.000],  loss: 17.143889, mse: 4440.157373, mean_q: 47.464638, mean_eps: 0.870192
  43404/300000: episode: 441, duration: 0.585s, episode steps:  88, steps per second: 150, episode reward: -56.485, mean reward: -0.642 [-100.000, 16.886], mean action: 1.523 [0.000, 3.000],  loss: 28.427041, mse: 4468.339195, mean_q: 47.218432, mean_eps: 0.869921
  43498/300000: episode: 442, duration: 0.614s, episode steps:  94, steps per second: 153, episode reward: -386.108, mean reward: -4.108 [-100.000, 109.143], mean action: 1.426 [0.000, 3.000],  loss: 20.433425, mse: 4458.901274, mean_q: 48.526846, mean_eps: 0.869649
  43580/300000: episode: 443, duration: 0.568s, episode steps:  82, steps per second: 144, episode reward: -83.295, mean reward: -1.016 [-100.000, 10.425], mean action: 1.622 [0.000, 3.000],  loss: 24.611456, mse: 4433.611337, mean_q: 48.112144, mean_eps: 0.869385
  43674/300000: episode: 444, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -185.446, mean reward: -1.973 [-100.000,  6.148], mean action: 1.415 [0.000, 3.000],  loss: 14.910593, mse: 4658.363780, mean_q: 48.741539, mean_eps: 0.869120
  43790/300000: episode: 445, duration: 0.765s, episode steps: 116, steps per second: 152, episode reward: -83.046, mean reward: -0.716 [-100.000, 10.890], mean action: 1.552 [0.000, 3.000],  loss: 20.872160, mse: 4636.516477, mean_q: 49.580988, mean_eps: 0.868805
  43960/300000: episode: 446, duration: 1.158s, episode steps: 170, steps per second: 147, episode reward: -134.490, mean reward: -0.791 [-100.000,  7.928], mean action: 1.565 [0.000, 3.000],  loss: 21.886916, mse: 4687.447241, mean_q: 49.321902, mean_eps: 0.868377
  44047/300000: episode: 447, duration: 0.575s, episode steps:  87, steps per second: 151, episode reward: -28.670, mean reward: -0.330 [-100.000, 10.093], mean action: 1.655 [0.000, 3.000],  loss: 31.365861, mse: 4761.301320, mean_q: 49.597651, mean_eps: 0.867991
  44148/300000: episode: 448, duration: 0.668s, episode steps: 101, steps per second: 151, episode reward: -91.913, mean reward: -0.910 [-100.000, 20.873], mean action: 1.455 [0.000, 3.000],  loss: 29.596637, mse: 4599.968020, mean_q: 48.611932, mean_eps: 0.867709
  44229/300000: episode: 449, duration: 0.564s, episode steps:  81, steps per second: 144, episode reward: -200.681, mean reward: -2.478 [-100.000,  5.245], mean action: 1.395 [0.000, 3.000],  loss: 16.187403, mse: 4700.937696, mean_q: 49.679898, mean_eps: 0.867436
  44324/300000: episode: 450, duration: 0.632s, episode steps:  95, steps per second: 150, episode reward: -70.878, mean reward: -0.746 [-100.000, 10.390], mean action: 1.611 [0.000, 3.000],  loss: 27.823208, mse: 4584.048962, mean_q: 48.520499, mean_eps: 0.867172
  44477/300000: episode: 451, duration: 1.017s, episode steps: 153, steps per second: 150, episode reward: -47.063, mean reward: -0.308 [-100.000,  8.442], mean action: 1.569 [0.000, 3.000],  loss: 30.740220, mse: 4690.679498, mean_q: 49.334507, mean_eps: 0.866800
  44585/300000: episode: 452, duration: 0.737s, episode steps: 108, steps per second: 146, episode reward: -64.938, mean reward: -0.601 [-100.000, 20.436], mean action: 1.759 [0.000, 3.000],  loss: 27.586801, mse: 4910.555162, mean_q: 52.273520, mean_eps: 0.866409
  44674/300000: episode: 453, duration: 0.589s, episode steps:  89, steps per second: 151, episode reward: -129.927, mean reward: -1.460 [-100.000,  9.787], mean action: 1.213 [0.000, 3.000],  loss: 20.514854, mse: 4816.604797, mean_q: 53.299117, mean_eps: 0.866113
  44762/300000: episode: 454, duration: 0.582s, episode steps:  88, steps per second: 151, episode reward: -43.906, mean reward: -0.499 [-100.000, 12.644], mean action: 1.625 [0.000, 3.000],  loss: 26.308647, mse: 4696.894118, mean_q: 50.741463, mean_eps: 0.865847
  44875/300000: episode: 455, duration: 0.786s, episode steps: 113, steps per second: 144, episode reward: -102.941, mean reward: -0.911 [-100.000,  7.644], mean action: 1.637 [0.000, 3.000],  loss: 25.563588, mse: 4904.887764, mean_q: 52.615003, mean_eps: 0.865546
  44985/300000: episode: 456, duration: 0.770s, episode steps: 110, steps per second: 143, episode reward: -99.452, mean reward: -0.904 [-100.000,  8.205], mean action: 1.518 [0.000, 3.000],  loss: 17.813092, mse: 4891.211013, mean_q: 52.627022, mean_eps: 0.865212
  45125/300000: episode: 457, duration: 1.161s, episode steps: 140, steps per second: 121, episode reward: -89.032, mean reward: -0.636 [-100.000,  9.061], mean action: 1.643 [0.000, 3.000],  loss: 26.405864, mse: 4895.644904, mean_q: 53.128616, mean_eps: 0.864836
  45193/300000: episode: 458, duration: 0.556s, episode steps:  68, steps per second: 122, episode reward: -73.576, mean reward: -1.082 [-100.000, 11.205], mean action: 1.544 [0.000, 3.000],  loss: 18.288305, mse: 5027.897727, mean_q: 55.039171, mean_eps: 0.864524
  45296/300000: episode: 459, duration: 0.796s, episode steps: 103, steps per second: 129, episode reward: -140.912, mean reward: -1.368 [-100.000,  8.350], mean action: 1.379 [0.000, 3.000],  loss: 21.291076, mse: 4894.457439, mean_q: 52.887955, mean_eps: 0.864268
  45373/300000: episode: 460, duration: 0.609s, episode steps:  77, steps per second: 126, episode reward: -151.174, mean reward: -1.963 [-100.000, 16.480], mean action: 1.597 [0.000, 3.000],  loss: 32.340765, mse: 4934.353421, mean_q: 52.714978, mean_eps: 0.863998
  45478/300000: episode: 461, duration: 0.808s, episode steps: 105, steps per second: 130, episode reward: -104.679, mean reward: -0.997 [-100.000, 17.508], mean action: 1.705 [0.000, 3.000],  loss: 31.338715, mse: 5015.824860, mean_q: 53.275879, mean_eps: 0.863725
  45574/300000: episode: 462, duration: 0.817s, episode steps:  96, steps per second: 118, episode reward: -215.408, mean reward: -2.244 [-100.000,  7.100], mean action: 1.469 [0.000, 3.000],  loss: 23.103179, mse: 4910.091693, mean_q: 53.526921, mean_eps: 0.863424
  45688/300000: episode: 463, duration: 0.852s, episode steps: 114, steps per second: 134, episode reward: -105.700, mean reward: -0.927 [-100.000, 12.162], mean action: 1.421 [0.000, 3.000],  loss: 29.967024, mse: 4837.838739, mean_q: 52.027489, mean_eps: 0.863108
  45792/300000: episode: 464, duration: 0.713s, episode steps: 104, steps per second: 146, episode reward: -47.631, mean reward: -0.458 [-100.000,  7.392], mean action: 1.692 [0.000, 3.000],  loss: 28.490713, mse: 4904.632324, mean_q: 53.286831, mean_eps: 0.862781
  45922/300000: episode: 465, duration: 0.871s, episode steps: 130, steps per second: 149, episode reward: -123.210, mean reward: -0.948 [-100.000,  7.827], mean action: 1.500 [0.000, 3.000],  loss: 22.638997, mse: 4870.761696, mean_q: 52.196884, mean_eps: 0.862430
  46033/300000: episode: 466, duration: 0.818s, episode steps: 111, steps per second: 136, episode reward: -230.088, mean reward: -2.073 [-100.000, 75.551], mean action: 1.414 [0.000, 3.000],  loss: 22.245868, mse: 4888.986874, mean_q: 51.474393, mean_eps: 0.862069
  46134/300000: episode: 467, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -115.686, mean reward: -1.145 [-100.000,  9.517], mean action: 1.475 [0.000, 3.000],  loss: 25.049949, mse: 4909.516328, mean_q: 52.073001, mean_eps: 0.861751
  46241/300000: episode: 468, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -166.959, mean reward: -1.560 [-100.000,  7.493], mean action: 1.579 [0.000, 3.000],  loss: 34.135909, mse: 4979.253897, mean_q: 52.302144, mean_eps: 0.861439
  46356/300000: episode: 469, duration: 0.819s, episode steps: 115, steps per second: 140, episode reward: -127.854, mean reward: -1.112 [-100.000, 20.132], mean action: 1.574 [0.000, 3.000],  loss: 16.202792, mse: 5074.810118, mean_q: 54.161147, mean_eps: 0.861106
  46437/300000: episode: 470, duration: 0.539s, episode steps:  81, steps per second: 150, episode reward: -53.892, mean reward: -0.665 [-100.000, 13.517], mean action: 1.605 [0.000, 3.000],  loss: 20.723117, mse: 5143.141794, mean_q: 54.780860, mean_eps: 0.860812
  46515/300000: episode: 471, duration: 0.546s, episode steps:  78, steps per second: 143, episode reward: -19.558, mean reward: -0.251 [-100.000, 46.997], mean action: 1.769 [0.000, 3.000],  loss: 20.223248, mse: 5171.820995, mean_q: 55.535547, mean_eps: 0.860573
  46617/300000: episode: 472, duration: 0.797s, episode steps: 102, steps per second: 128, episode reward: -121.744, mean reward: -1.194 [-100.000,  8.931], mean action: 1.510 [0.000, 3.000],  loss: 17.719542, mse: 5138.978958, mean_q: 53.041456, mean_eps: 0.860304
  46721/300000: episode: 473, duration: 0.771s, episode steps: 104, steps per second: 135, episode reward: -124.229, mean reward: -1.195 [-100.000, 11.171], mean action: 1.404 [0.000, 3.000],  loss: 15.054568, mse: 5075.746143, mean_q: 54.117264, mean_eps: 0.859995
  46812/300000: episode: 474, duration: 0.695s, episode steps:  91, steps per second: 131, episode reward: -75.615, mean reward: -0.831 [-100.000, 11.070], mean action: 1.659 [0.000, 3.000],  loss: 25.855024, mse: 5001.830684, mean_q: 53.745215, mean_eps: 0.859702
  46914/300000: episode: 475, duration: 0.806s, episode steps: 102, steps per second: 127, episode reward: -122.450, mean reward: -1.200 [-100.000,  6.911], mean action: 1.529 [0.000, 3.000],  loss: 23.216642, mse: 5130.515003, mean_q: 54.646767, mean_eps: 0.859412
  47036/300000: episode: 476, duration: 0.893s, episode steps: 122, steps per second: 137, episode reward: -181.630, mean reward: -1.489 [-100.000, 74.651], mean action: 1.525 [0.000, 3.000],  loss: 24.047229, mse: 5130.479708, mean_q: 54.229823, mean_eps: 0.859077
  47136/300000: episode: 477, duration: 0.667s, episode steps: 100, steps per second: 150, episode reward: -153.803, mean reward: -1.538 [-100.000,  7.562], mean action: 1.600 [0.000, 3.000],  loss: 22.053807, mse: 5154.061472, mean_q: 54.421864, mean_eps: 0.858743
  47277/300000: episode: 478, duration: 0.979s, episode steps: 141, steps per second: 144, episode reward: -157.406, mean reward: -1.116 [-100.000,  8.006], mean action: 1.660 [0.000, 3.000],  loss: 21.116758, mse: 5013.484169, mean_q: 52.809981, mean_eps: 0.858382
  47366/300000: episode: 479, duration: 0.596s, episode steps:  89, steps per second: 149, episode reward: -82.788, mean reward: -0.930 [-100.000, 22.562], mean action: 1.719 [0.000, 3.000],  loss: 21.284296, mse: 5097.098265, mean_q: 54.562583, mean_eps: 0.858037
  47470/300000: episode: 480, duration: 0.707s, episode steps: 104, steps per second: 147, episode reward: -52.402, mean reward: -0.504 [-100.000, 10.709], mean action: 1.721 [0.000, 3.000],  loss: 27.073149, mse: 5165.184091, mean_q: 53.561625, mean_eps: 0.857747
  47567/300000: episode: 481, duration: 0.662s, episode steps:  97, steps per second: 147, episode reward: -56.822, mean reward: -0.586 [-100.000, 17.220], mean action: 1.423 [0.000, 3.000],  loss: 28.854988, mse: 4846.547995, mean_q: 50.203417, mean_eps: 0.857446
  47662/300000: episode: 482, duration: 0.636s, episode steps:  95, steps per second: 149, episode reward: -59.014, mean reward: -0.621 [-100.000, 24.255], mean action: 1.716 [0.000, 3.000],  loss: 23.002852, mse: 4867.594112, mean_q: 51.284999, mean_eps: 0.857158
  47743/300000: episode: 483, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -79.904, mean reward: -0.986 [-100.000, 21.832], mean action: 1.765 [0.000, 3.000],  loss: 25.595253, mse: 4788.019118, mean_q: 51.434228, mean_eps: 0.856894
  47811/300000: episode: 484, duration: 0.494s, episode steps:  68, steps per second: 138, episode reward: -58.196, mean reward: -0.856 [-100.000, 16.316], mean action: 1.632 [0.000, 3.000],  loss: 23.122664, mse: 4851.472757, mean_q: 51.552054, mean_eps: 0.856670
  47930/300000: episode: 485, duration: 0.803s, episode steps: 119, steps per second: 148, episode reward: -69.734, mean reward: -0.586 [-100.000,  9.634], mean action: 1.588 [0.000, 3.000],  loss: 26.170946, mse: 4854.690165, mean_q: 51.041314, mean_eps: 0.856390
  48054/300000: episode: 486, duration: 0.818s, episode steps: 124, steps per second: 152, episode reward: -189.207, mean reward: -1.526 [-100.000,  8.567], mean action: 1.556 [0.000, 3.000],  loss: 22.302338, mse: 4779.089627, mean_q: 52.322645, mean_eps: 0.856026
  48165/300000: episode: 487, duration: 0.774s, episode steps: 111, steps per second: 143, episode reward: -218.532, mean reward: -1.969 [-100.000,  5.466], mean action: 1.559 [0.000, 3.000],  loss: 19.713539, mse: 4882.989350, mean_q: 52.868583, mean_eps: 0.855673
  48271/300000: episode: 488, duration: 0.714s, episode steps: 106, steps per second: 148, episode reward: -61.405, mean reward: -0.579 [-100.000, 17.875], mean action: 1.613 [0.000, 3.000],  loss: 22.977132, mse: 4783.732443, mean_q: 51.931814, mean_eps: 0.855347
  48370/300000: episode: 489, duration: 0.659s, episode steps:  99, steps per second: 150, episode reward: -184.409, mean reward: -1.863 [-100.000, 11.323], mean action: 1.556 [0.000, 3.000],  loss: 23.833433, mse: 4733.766091, mean_q: 51.653538, mean_eps: 0.855040
  48469/300000: episode: 490, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -112.461, mean reward: -1.136 [-100.000, 11.240], mean action: 1.444 [0.000, 3.000],  loss: 15.215051, mse: 4936.615659, mean_q: 53.299365, mean_eps: 0.854743
  48539/300000: episode: 491, duration: 0.477s, episode steps:  70, steps per second: 147, episode reward: -29.054, mean reward: -0.415 [-100.000, 12.742], mean action: 1.743 [0.000, 3.000],  loss: 25.279566, mse: 4883.253435, mean_q: 53.478297, mean_eps: 0.854489
  48665/300000: episode: 492, duration: 0.843s, episode steps: 126, steps per second: 150, episode reward: -41.003, mean reward: -0.325 [-100.000, 43.496], mean action: 1.429 [0.000, 3.000],  loss: 25.406380, mse: 4918.851237, mean_q: 51.497722, mean_eps: 0.854195
  48726/300000: episode: 493, duration: 0.424s, episode steps:  61, steps per second: 144, episode reward: -56.478, mean reward: -0.926 [-100.000, 12.989], mean action: 1.738 [0.000, 3.000],  loss: 14.562294, mse: 4881.031234, mean_q: 51.391667, mean_eps: 0.853915
  48813/300000: episode: 494, duration: 0.707s, episode steps:  87, steps per second: 123, episode reward: -79.179, mean reward: -0.910 [-100.000,  5.232], mean action: 1.529 [0.000, 3.000],  loss: 15.146556, mse: 4977.522270, mean_q: 51.026356, mean_eps: 0.853693
  48905/300000: episode: 495, duration: 0.733s, episode steps:  92, steps per second: 125, episode reward: -193.067, mean reward: -2.099 [-100.000,  5.628], mean action: 1.609 [0.000, 3.000],  loss: 21.060286, mse: 4871.119632, mean_q: 49.448052, mean_eps: 0.853425
  49008/300000: episode: 496, duration: 0.771s, episode steps: 103, steps per second: 134, episode reward: -166.197, mean reward: -1.614 [-100.000,  6.561], mean action: 1.650 [0.000, 3.000],  loss: 18.388540, mse: 4918.629293, mean_q: 51.279103, mean_eps: 0.853132
  49078/300000: episode: 497, duration: 0.511s, episode steps:  70, steps per second: 137, episode reward: -73.992, mean reward: -1.057 [-100.000,  8.244], mean action: 1.657 [0.000, 3.000],  loss: 16.393998, mse: 4754.994214, mean_q: 49.879875, mean_eps: 0.852873
  49164/300000: episode: 498, duration: 0.593s, episode steps:  86, steps per second: 145, episode reward: -81.052, mean reward: -0.942 [-100.000,  7.724], mean action: 1.384 [0.000, 3.000],  loss: 23.813629, mse: 4761.485414, mean_q: 48.799737, mean_eps: 0.852639
  49234/300000: episode: 499, duration: 0.476s, episode steps:  70, steps per second: 147, episode reward: -58.721, mean reward: -0.839 [-100.000, 20.280], mean action: 1.571 [0.000, 3.000],  loss: 24.373963, mse: 4868.145295, mean_q: 51.946265, mean_eps: 0.852405
  49313/300000: episode: 500, duration: 0.552s, episode steps:  79, steps per second: 143, episode reward: -174.383, mean reward: -2.207 [-100.000, 15.085], mean action: 1.608 [0.000, 3.000],  loss: 14.094438, mse: 4889.079296, mean_q: 52.205458, mean_eps: 0.852181
  49379/300000: episode: 501, duration: 0.484s, episode steps:  66, steps per second: 136, episode reward: -76.187, mean reward: -1.154 [-100.000,  6.367], mean action: 1.591 [0.000, 3.000],  loss: 20.371403, mse: 4816.919992, mean_q: 49.686076, mean_eps: 0.851963
  49447/300000: episode: 502, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -80.121, mean reward: -1.178 [-100.000,  7.360], mean action: 1.294 [0.000, 3.000],  loss: 16.979857, mse: 4800.570912, mean_q: 49.514952, mean_eps: 0.851762
  49575/300000: episode: 503, duration: 0.922s, episode steps: 128, steps per second: 139, episode reward: -158.631, mean reward: -1.239 [-100.000,  6.165], mean action: 1.625 [0.000, 3.000],  loss: 22.749289, mse: 4946.890568, mean_q: 50.938223, mean_eps: 0.851468
  49693/300000: episode: 504, duration: 0.845s, episode steps: 118, steps per second: 140, episode reward: -165.942, mean reward: -1.406 [-100.000,  8.469], mean action: 1.432 [0.000, 3.000],  loss: 21.567289, mse: 4988.825903, mean_q: 51.070779, mean_eps: 0.851100
  49776/300000: episode: 505, duration: 0.599s, episode steps:  83, steps per second: 139, episode reward: -55.787, mean reward: -0.672 [-100.000, 17.441], mean action: 1.337 [0.000, 3.000],  loss: 25.510725, mse: 5184.617952, mean_q: 52.509374, mean_eps: 0.850798
  49867/300000: episode: 506, duration: 0.637s, episode steps:  91, steps per second: 143, episode reward: -125.190, mean reward: -1.376 [-100.000,  8.845], mean action: 1.527 [0.000, 3.000],  loss: 20.569179, mse: 5131.122046, mean_q: 53.710945, mean_eps: 0.850537
  49950/300000: episode: 507, duration: 0.596s, episode steps:  83, steps per second: 139, episode reward: -123.220, mean reward: -1.485 [-100.000, 12.664], mean action: 1.349 [0.000, 3.000],  loss: 12.134945, mse: 5027.189388, mean_q: 52.832706, mean_eps: 0.850276
  50044/300000: episode: 508, duration: 0.655s, episode steps:  94, steps per second: 144, episode reward: -76.466, mean reward: -0.813 [-100.000, 12.243], mean action: 1.479 [0.000, 3.000],  loss: 24.470432, mse: 5077.609876, mean_q: 51.886802, mean_eps: 0.850011
  50115/300000: episode: 509, duration: 0.505s, episode steps:  71, steps per second: 140, episode reward: -67.139, mean reward: -0.946 [-100.000,  7.979], mean action: 1.380 [0.000, 3.000],  loss: 15.904567, mse: 5235.980665, mean_q: 52.908103, mean_eps: 0.849763
  50169/300000: episode: 510, duration: 0.367s, episode steps:  54, steps per second: 147, episode reward: -111.492, mean reward: -2.065 [-100.000,  8.143], mean action: 1.519 [0.000, 3.000],  loss: 18.338365, mse: 5184.277502, mean_q: 51.839991, mean_eps: 0.849576
  50252/300000: episode: 511, duration: 0.619s, episode steps:  83, steps per second: 134, episode reward: -118.386, mean reward: -1.426 [-100.000, 12.790], mean action: 1.554 [0.000, 3.000],  loss: 22.289991, mse: 5183.437123, mean_q: 52.135995, mean_eps: 0.849370
  50366/300000: episode: 512, duration: 0.803s, episode steps: 114, steps per second: 142, episode reward: -79.029, mean reward: -0.693 [-100.000,  7.967], mean action: 1.579 [0.000, 3.000],  loss: 20.643631, mse: 5275.262175, mean_q: 53.782028, mean_eps: 0.849074
  50454/300000: episode: 513, duration: 0.606s, episode steps:  88, steps per second: 145, episode reward: -71.487, mean reward: -0.812 [-100.000, 15.727], mean action: 1.523 [0.000, 3.000],  loss: 29.181124, mse: 5351.304349, mean_q: 54.447799, mean_eps: 0.848771
  50545/300000: episode: 514, duration: 0.650s, episode steps:  91, steps per second: 140, episode reward: -103.776, mean reward: -1.140 [-100.000, 13.116], mean action: 1.780 [0.000, 3.000],  loss: 16.092413, mse: 5415.078356, mean_q: 53.156312, mean_eps: 0.848503
  50633/300000: episode: 515, duration: 0.608s, episode steps:  88, steps per second: 145, episode reward: -54.695, mean reward: -0.622 [-100.000, 20.505], mean action: 1.648 [0.000, 3.000],  loss: 25.641705, mse: 5555.814259, mean_q: 56.460879, mean_eps: 0.848234
  50718/300000: episode: 516, duration: 0.580s, episode steps:  85, steps per second: 146, episode reward: 26.574, mean reward:  0.313 [-100.000, 63.310], mean action: 1.812 [0.000, 3.000],  loss: 16.089998, mse: 5496.486414, mean_q: 55.077444, mean_eps: 0.847975
  50815/300000: episode: 517, duration: 0.667s, episode steps:  97, steps per second: 146, episode reward: -92.711, mean reward: -0.956 [-100.000, 16.690], mean action: 1.567 [0.000, 3.000],  loss: 21.973144, mse: 5613.496939, mean_q: 54.404775, mean_eps: 0.847702
  50926/300000: episode: 518, duration: 0.795s, episode steps: 111, steps per second: 140, episode reward: -131.924, mean reward: -1.189 [-100.000,  6.525], mean action: 1.532 [0.000, 3.000],  loss: 19.564041, mse: 5622.851351, mean_q: 56.276942, mean_eps: 0.847390
  51007/300000: episode: 519, duration: 0.552s, episode steps:  81, steps per second: 147, episode reward: -80.169, mean reward: -0.990 [-100.000, 19.101], mean action: 1.691 [0.000, 3.000],  loss: 19.084992, mse: 5656.012936, mean_q: 56.216184, mean_eps: 0.847102
  51128/300000: episode: 520, duration: 0.831s, episode steps: 121, steps per second: 146, episode reward: -78.387, mean reward: -0.648 [-100.000,  8.306], mean action: 1.521 [0.000, 3.000],  loss: 34.268789, mse: 5901.026028, mean_q: 57.866970, mean_eps: 0.846799
  51195/300000: episode: 521, duration: 0.487s, episode steps:  67, steps per second: 138, episode reward: -78.951, mean reward: -1.178 [-100.000,  6.859], mean action: 1.672 [0.000, 3.000],  loss: 36.020017, mse: 6187.362669, mean_q: 58.929593, mean_eps: 0.846517
  51293/300000: episode: 522, duration: 0.662s, episode steps:  98, steps per second: 148, episode reward: -57.034, mean reward: -0.582 [-100.000, 12.743], mean action: 1.541 [0.000, 3.000],  loss: 17.849677, mse: 5886.074612, mean_q: 58.776091, mean_eps: 0.846270
  51395/300000: episode: 523, duration: 0.687s, episode steps: 102, steps per second: 149, episode reward: -88.493, mean reward: -0.868 [-100.000,  6.233], mean action: 1.314 [0.000, 3.000],  loss: 18.081863, mse: 6092.126924, mean_q: 61.606854, mean_eps: 0.845969
  51506/300000: episode: 524, duration: 0.769s, episode steps: 111, steps per second: 144, episode reward: -61.388, mean reward: -0.553 [-100.000, 15.190], mean action: 1.649 [0.000, 3.000],  loss: 22.667002, mse: 6034.474407, mean_q: 61.624680, mean_eps: 0.845650
  51582/300000: episode: 525, duration: 0.531s, episode steps:  76, steps per second: 143, episode reward: -108.501, mean reward: -1.428 [-100.000, 19.699], mean action: 1.658 [0.000, 3.000],  loss: 22.198080, mse: 6067.523900, mean_q: 60.087203, mean_eps: 0.845370
  51685/300000: episode: 526, duration: 0.703s, episode steps: 103, steps per second: 147, episode reward: -205.975, mean reward: -2.000 [-100.000,  9.012], mean action: 1.689 [0.000, 3.000],  loss: 15.354277, mse: 6139.517839, mean_q: 61.825248, mean_eps: 0.845101
  51780/300000: episode: 527, duration: 0.663s, episode steps:  95, steps per second: 143, episode reward: -70.996, mean reward: -0.747 [-100.000, 45.125], mean action: 1.600 [0.000, 3.000],  loss: 20.591232, mse: 6077.708733, mean_q: 60.284950, mean_eps: 0.844804
  51873/300000: episode: 528, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -110.105, mean reward: -1.184 [-100.000, 10.049], mean action: 1.516 [0.000, 3.000],  loss: 32.263765, mse: 6044.608745, mean_q: 60.547008, mean_eps: 0.844522
  51963/300000: episode: 529, duration: 0.623s, episode steps:  90, steps per second: 144, episode reward: -158.388, mean reward: -1.760 [-100.000,  7.760], mean action: 1.744 [0.000, 3.000],  loss: 23.189714, mse: 5923.482530, mean_q: 58.378569, mean_eps: 0.844248
  52076/300000: episode: 530, duration: 0.761s, episode steps: 113, steps per second: 149, episode reward: -188.504, mean reward: -1.668 [-100.000, 32.924], mean action: 1.566 [0.000, 3.000],  loss: 28.497243, mse: 6043.405995, mean_q: 60.164466, mean_eps: 0.843943
  52151/300000: episode: 531, duration: 0.545s, episode steps:  75, steps per second: 138, episode reward: -136.973, mean reward: -1.826 [-100.000,  6.951], mean action: 1.587 [0.000, 3.000],  loss: 24.232364, mse: 6146.558737, mean_q: 60.212546, mean_eps: 0.843661
  52232/300000: episode: 532, duration: 0.549s, episode steps:  81, steps per second: 148, episode reward: -118.781, mean reward: -1.466 [-100.000, 17.453], mean action: 1.741 [0.000, 3.000],  loss: 14.655235, mse: 6111.836040, mean_q: 60.396941, mean_eps: 0.843427
  52299/300000: episode: 533, duration: 0.452s, episode steps:  67, steps per second: 148, episode reward: -53.228, mean reward: -0.794 [-100.000, 11.628], mean action: 1.687 [0.000, 3.000],  loss: 26.918870, mse: 6209.509088, mean_q: 60.065336, mean_eps: 0.843205
  52364/300000: episode: 534, duration: 0.438s, episode steps:  65, steps per second: 148, episode reward: -112.850, mean reward: -1.736 [-100.000, 11.462], mean action: 1.615 [0.000, 3.000],  loss: 12.136027, mse: 6140.351345, mean_q: 60.887931, mean_eps: 0.843007
  52458/300000: episode: 535, duration: 0.700s, episode steps:  94, steps per second: 134, episode reward: -136.280, mean reward: -1.450 [-100.000, 11.273], mean action: 1.691 [0.000, 3.000],  loss: 18.517751, mse: 6025.731191, mean_q: 60.367600, mean_eps: 0.842769
  52584/300000: episode: 536, duration: 0.845s, episode steps: 126, steps per second: 149, episode reward: -106.568, mean reward: -0.846 [-100.000,  6.630], mean action: 1.579 [0.000, 3.000],  loss: 19.063940, mse: 6032.854535, mean_q: 57.427113, mean_eps: 0.842438
  52659/300000: episode: 537, duration: 0.504s, episode steps:  75, steps per second: 149, episode reward: -95.865, mean reward: -1.278 [-100.000,  9.653], mean action: 1.587 [0.000, 3.000],  loss: 33.178137, mse: 6185.646263, mean_q: 60.296156, mean_eps: 0.842137
  52742/300000: episode: 538, duration: 0.586s, episode steps:  83, steps per second: 142, episode reward: -59.778, mean reward: -0.720 [-100.000,  6.772], mean action: 1.434 [0.000, 3.000],  loss: 22.304370, mse: 6204.717191, mean_q: 59.532106, mean_eps: 0.841900
  52826/300000: episode: 539, duration: 0.594s, episode steps:  84, steps per second: 141, episode reward: -106.711, mean reward: -1.270 [-100.000, 38.110], mean action: 1.595 [0.000, 3.000],  loss: 20.340639, mse: 6145.339536, mean_q: 58.774255, mean_eps: 0.841650
  52905/300000: episode: 540, duration: 0.581s, episode steps:  79, steps per second: 136, episode reward: 49.243, mean reward:  0.623 [-100.000, 131.933], mean action: 1.696 [0.000, 3.000],  loss: 16.548394, mse: 5937.750457, mean_q: 59.066042, mean_eps: 0.841405
  52986/300000: episode: 541, duration: 0.548s, episode steps:  81, steps per second: 148, episode reward: -124.203, mean reward: -1.533 [-100.000,  9.499], mean action: 1.494 [0.000, 3.000],  loss: 15.507635, mse: 5929.368170, mean_q: 58.213116, mean_eps: 0.841165
  53064/300000: episode: 542, duration: 0.554s, episode steps:  78, steps per second: 141, episode reward: -51.457, mean reward: -0.660 [-100.000,  9.333], mean action: 1.692 [0.000, 3.000],  loss: 20.196152, mse: 6009.339355, mean_q: 58.537098, mean_eps: 0.840927
  53181/300000: episode: 543, duration: 0.799s, episode steps: 117, steps per second: 146, episode reward: -109.398, mean reward: -0.935 [-100.000,  5.821], mean action: 1.385 [0.000, 3.000],  loss: 21.702764, mse: 5930.592619, mean_q: 56.926379, mean_eps: 0.840634
  53284/300000: episode: 544, duration: 0.692s, episode steps: 103, steps per second: 149, episode reward: -108.677, mean reward: -1.055 [-100.000,  9.975], mean action: 1.369 [0.000, 3.000],  loss: 27.707360, mse: 5962.366676, mean_q: 56.653569, mean_eps: 0.840304
  53389/300000: episode: 545, duration: 0.735s, episode steps: 105, steps per second: 143, episode reward: -126.436, mean reward: -1.204 [-100.000, 16.326], mean action: 1.552 [0.000, 3.000],  loss: 17.159265, mse: 6158.337872, mean_q: 58.736666, mean_eps: 0.839992
  53499/300000: episode: 546, duration: 0.738s, episode steps: 110, steps per second: 149, episode reward: -107.434, mean reward: -0.977 [-100.000, 11.029], mean action: 1.473 [0.000, 3.000],  loss: 21.240989, mse: 6110.824214, mean_q: 58.926132, mean_eps: 0.839670
  53587/300000: episode: 547, duration: 0.589s, episode steps:  88, steps per second: 150, episode reward: -67.145, mean reward: -0.763 [-100.000, 16.694], mean action: 1.682 [0.000, 3.000],  loss: 17.092527, mse: 6002.585261, mean_q: 58.240164, mean_eps: 0.839372
  53701/300000: episode: 548, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -178.895, mean reward: -1.569 [-100.000,  7.066], mean action: 1.500 [0.000, 3.000],  loss: 23.573042, mse: 6110.166787, mean_q: 58.565921, mean_eps: 0.839070
  53843/300000: episode: 549, duration: 0.967s, episode steps: 142, steps per second: 147, episode reward: -374.385, mean reward: -2.637 [-100.000, 35.601], mean action: 1.676 [0.000, 3.000],  loss: 17.337244, mse: 6204.827444, mean_q: 59.900304, mean_eps: 0.838686
  53931/300000: episode: 550, duration: 0.596s, episode steps:  88, steps per second: 148, episode reward: -69.798, mean reward: -0.793 [-100.000, 11.701], mean action: 1.580 [0.000, 3.000],  loss: 27.056732, mse: 6097.233276, mean_q: 59.045412, mean_eps: 0.838341
  54020/300000: episode: 551, duration: 0.629s, episode steps:  89, steps per second: 141, episode reward: -33.719, mean reward: -0.379 [-100.000, 17.568], mean action: 1.551 [0.000, 3.000],  loss: 22.289381, mse: 6065.223249, mean_q: 57.359982, mean_eps: 0.838075
  54092/300000: episode: 552, duration: 0.491s, episode steps:  72, steps per second: 147, episode reward: -57.141, mean reward: -0.794 [-100.000, 13.243], mean action: 1.542 [0.000, 3.000],  loss: 18.921026, mse: 5934.516764, mean_q: 56.495588, mean_eps: 0.837833
  54177/300000: episode: 553, duration: 0.605s, episode steps:  85, steps per second: 141, episode reward: -84.503, mean reward: -0.994 [-100.000,  6.619], mean action: 1.835 [0.000, 3.000],  loss: 21.569710, mse: 6042.115596, mean_q: 57.778171, mean_eps: 0.837598
  54279/300000: episode: 554, duration: 0.700s, episode steps: 102, steps per second: 146, episode reward: -98.637, mean reward: -0.967 [-100.000,  8.266], mean action: 1.520 [0.000, 3.000],  loss: 13.912227, mse: 6273.440794, mean_q: 58.563181, mean_eps: 0.837317
  54396/300000: episode: 555, duration: 0.972s, episode steps: 117, steps per second: 120, episode reward:  1.687, mean reward:  0.014 [-100.000, 60.750], mean action: 1.590 [0.000, 3.000],  loss: 20.154827, mse: 6164.169968, mean_q: 58.808777, mean_eps: 0.836989
  54536/300000: episode: 556, duration: 1.121s, episode steps: 140, steps per second: 125, episode reward: -109.780, mean reward: -0.784 [-100.000,  8.401], mean action: 1.571 [0.000, 3.000],  loss: 19.746610, mse: 6136.423124, mean_q: 59.766170, mean_eps: 0.836604
  54626/300000: episode: 557, duration: 0.734s, episode steps:  90, steps per second: 123, episode reward: -130.201, mean reward: -1.447 [-100.000, 15.327], mean action: 1.689 [0.000, 3.000],  loss: 25.856915, mse: 6041.100852, mean_q: 58.694818, mean_eps: 0.836259
  54694/300000: episode: 558, duration: 0.510s, episode steps:  68, steps per second: 133, episode reward: -50.097, mean reward: -0.737 [-100.000, 15.406], mean action: 1.515 [0.000, 3.000],  loss: 16.586848, mse: 6089.027064, mean_q: 57.930805, mean_eps: 0.836021
  54775/300000: episode: 559, duration: 0.628s, episode steps:  81, steps per second: 129, episode reward: -70.336, mean reward: -0.868 [-100.000,  6.134], mean action: 1.593 [0.000, 3.000],  loss: 22.243300, mse: 6154.804941, mean_q: 59.634660, mean_eps: 0.835798
  54867/300000: episode: 560, duration: 0.716s, episode steps:  92, steps per second: 129, episode reward: -184.381, mean reward: -2.004 [-100.000,  9.254], mean action: 1.630 [0.000, 3.000],  loss: 20.982995, mse: 6166.390211, mean_q: 58.028819, mean_eps: 0.835538
  54951/300000: episode: 561, duration: 0.586s, episode steps:  84, steps per second: 143, episode reward: -95.597, mean reward: -1.138 [-100.000,  7.154], mean action: 1.452 [0.000, 3.000],  loss: 16.075467, mse: 6304.333374, mean_q: 58.910660, mean_eps: 0.835275
  55063/300000: episode: 562, duration: 0.773s, episode steps: 112, steps per second: 145, episode reward: -72.400, mean reward: -0.646 [-100.000,  9.743], mean action: 1.536 [0.000, 3.000],  loss: 22.514669, mse: 6147.348873, mean_q: 59.636068, mean_eps: 0.834981
  55171/300000: episode: 563, duration: 0.797s, episode steps: 108, steps per second: 136, episode reward: -102.070, mean reward: -0.945 [-100.000,  4.855], mean action: 1.546 [0.000, 3.000],  loss: 36.933768, mse: 6293.389671, mean_q: 60.853531, mean_eps: 0.834650
  55242/300000: episode: 564, duration: 0.531s, episode steps:  71, steps per second: 134, episode reward: -74.574, mean reward: -1.050 [-100.000,  5.196], mean action: 1.282 [0.000, 3.000],  loss: 23.322888, mse: 6191.954390, mean_q: 58.777689, mean_eps: 0.834382
  55386/300000: episode: 565, duration: 1.034s, episode steps: 144, steps per second: 139, episode reward: -93.191, mean reward: -0.647 [-100.000,  9.290], mean action: 1.597 [0.000, 3.000],  loss: 22.590915, mse: 6444.218204, mean_q: 61.791338, mean_eps: 0.834059
  55466/300000: episode: 566, duration: 0.579s, episode steps:  80, steps per second: 138, episode reward: -105.296, mean reward: -1.316 [-100.000, 23.138], mean action: 1.462 [0.000, 3.000],  loss: 22.089792, mse: 6605.013574, mean_q: 62.126006, mean_eps: 0.833723
  55528/300000: episode: 567, duration: 0.426s, episode steps:  62, steps per second: 146, episode reward: -73.466, mean reward: -1.185 [-100.000,  6.773], mean action: 1.435 [0.000, 3.000],  loss: 11.061004, mse: 6582.425765, mean_q: 62.684562, mean_eps: 0.833510
  55641/300000: episode: 568, duration: 0.754s, episode steps: 113, steps per second: 150, episode reward: -94.527, mean reward: -0.837 [-100.000, 10.558], mean action: 1.522 [0.000, 3.000],  loss: 24.154651, mse: 6758.006624, mean_q: 62.899671, mean_eps: 0.833248
  55731/300000: episode: 569, duration: 0.650s, episode steps:  90, steps per second: 138, episode reward: -63.058, mean reward: -0.701 [-100.000, 11.865], mean action: 1.767 [0.000, 3.000],  loss: 12.658282, mse: 6663.949072, mean_q: 62.929602, mean_eps: 0.832944
  55819/300000: episode: 570, duration: 0.661s, episode steps:  88, steps per second: 133, episode reward: -104.737, mean reward: -1.190 [-100.000,  7.626], mean action: 1.739 [0.000, 3.000],  loss: 18.684527, mse: 6659.821189, mean_q: 63.476936, mean_eps: 0.832676
  55885/300000: episode: 571, duration: 0.509s, episode steps:  66, steps per second: 130, episode reward: -82.047, mean reward: -1.243 [-100.000,  8.868], mean action: 1.712 [0.000, 3.000],  loss: 8.540631, mse: 6740.873269, mean_q: 63.848256, mean_eps: 0.832446
  55961/300000: episode: 572, duration: 0.574s, episode steps:  76, steps per second: 132, episode reward: -107.862, mean reward: -1.419 [-100.000,  7.815], mean action: 1.658 [0.000, 3.000],  loss: 16.711887, mse: 6620.698692, mean_q: 63.271390, mean_eps: 0.832233
  56039/300000: episode: 573, duration: 0.608s, episode steps:  78, steps per second: 128, episode reward: -62.078, mean reward: -0.796 [-100.000,  6.056], mean action: 1.538 [0.000, 3.000],  loss: 18.941951, mse: 6837.293783, mean_q: 63.589575, mean_eps: 0.832002
  56117/300000: episode: 574, duration: 0.630s, episode steps:  78, steps per second: 124, episode reward: -121.255, mean reward: -1.555 [-100.000,  6.513], mean action: 1.500 [0.000, 3.000],  loss: 26.670512, mse: 6960.008789, mean_q: 64.442161, mean_eps: 0.831767
  56194/300000: episode: 575, duration: 0.562s, episode steps:  77, steps per second: 137, episode reward: -83.170, mean reward: -1.080 [-100.000,  9.277], mean action: 1.584 [0.000, 3.000],  loss: 17.489206, mse: 7105.489556, mean_q: 65.951663, mean_eps: 0.831535
  56274/300000: episode: 576, duration: 0.595s, episode steps:  80, steps per second: 134, episode reward: -106.130, mean reward: -1.327 [-100.000,  9.059], mean action: 1.312 [0.000, 3.000],  loss: 16.477943, mse: 6985.849829, mean_q: 64.089347, mean_eps: 0.831299
  56379/300000: episode: 577, duration: 0.802s, episode steps: 105, steps per second: 131, episode reward: -230.154, mean reward: -2.192 [-100.000,  9.156], mean action: 1.505 [0.000, 3.000],  loss: 19.957299, mse: 6893.525730, mean_q: 64.092689, mean_eps: 0.831022
  56464/300000: episode: 578, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -99.476, mean reward: -1.170 [-100.000,  9.683], mean action: 1.471 [0.000, 3.000],  loss: 19.054274, mse: 6892.057502, mean_q: 62.246245, mean_eps: 0.830737
  56597/300000: episode: 579, duration: 0.912s, episode steps: 133, steps per second: 146, episode reward: -116.214, mean reward: -0.874 [-100.000,  7.060], mean action: 1.429 [0.000, 3.000],  loss: 14.727329, mse: 7083.194182, mean_q: 63.468737, mean_eps: 0.830410
  56704/300000: episode: 580, duration: 0.759s, episode steps: 107, steps per second: 141, episode reward: -84.679, mean reward: -0.791 [-100.000, 15.681], mean action: 1.542 [0.000, 3.000],  loss: 18.928043, mse: 7208.958136, mean_q: 63.495295, mean_eps: 0.830050
  56771/300000: episode: 581, duration: 0.475s, episode steps:  67, steps per second: 141, episode reward: -133.412, mean reward: -1.991 [-100.000,  8.092], mean action: 1.687 [0.000, 3.000],  loss: 27.941942, mse: 7047.553470, mean_q: 64.206240, mean_eps: 0.829789
  56894/300000: episode: 582, duration: 0.824s, episode steps: 123, steps per second: 149, episode reward: -83.585, mean reward: -0.680 [-100.000,  6.535], mean action: 1.797 [0.000, 3.000],  loss: 14.975626, mse: 7032.055803, mean_q: 62.979576, mean_eps: 0.829504
  56993/300000: episode: 583, duration: 0.740s, episode steps:  99, steps per second: 134, episode reward: -84.061, mean reward: -0.849 [-100.000,  6.753], mean action: 1.636 [0.000, 3.000],  loss: 18.084028, mse: 6916.108122, mean_q: 61.930938, mean_eps: 0.829171
  57091/300000: episode: 584, duration: 0.659s, episode steps:  98, steps per second: 149, episode reward: -92.685, mean reward: -0.946 [-100.000,  8.123], mean action: 1.490 [0.000, 3.000],  loss: 25.038971, mse: 7144.006467, mean_q: 63.050540, mean_eps: 0.828875
  57185/300000: episode: 585, duration: 0.626s, episode steps:  94, steps per second: 150, episode reward: -66.127, mean reward: -0.703 [-100.000,  9.186], mean action: 1.511 [0.000, 3.000],  loss: 14.807564, mse: 6947.697806, mean_q: 61.996056, mean_eps: 0.828588
  57271/300000: episode: 586, duration: 0.617s, episode steps:  86, steps per second: 139, episode reward: -103.302, mean reward: -1.201 [-100.000, 18.703], mean action: 1.547 [0.000, 3.000],  loss: 18.965757, mse: 7026.813312, mean_q: 63.691098, mean_eps: 0.828318
  57399/300000: episode: 587, duration: 0.880s, episode steps: 128, steps per second: 145, episode reward: -39.361, mean reward: -0.308 [-100.000, 10.878], mean action: 1.727 [0.000, 3.000],  loss: 25.758319, mse: 6975.980309, mean_q: 65.573723, mean_eps: 0.827997
  57516/300000: episode: 588, duration: 0.790s, episode steps: 117, steps per second: 148, episode reward: -71.784, mean reward: -0.614 [-100.000,  6.886], mean action: 1.564 [0.000, 3.000],  loss: 26.267609, mse: 6907.694511, mean_q: 64.617271, mean_eps: 0.827629
  57647/300000: episode: 589, duration: 0.900s, episode steps: 131, steps per second: 146, episode reward: -144.964, mean reward: -1.107 [-100.000,  6.740], mean action: 1.634 [0.000, 3.000],  loss: 20.747982, mse: 7142.428588, mean_q: 65.406868, mean_eps: 0.827257
  57735/300000: episode: 590, duration: 0.590s, episode steps:  88, steps per second: 149, episode reward: -97.358, mean reward: -1.106 [-100.000,  9.261], mean action: 1.602 [0.000, 3.000],  loss: 16.268827, mse: 7107.943692, mean_q: 66.634429, mean_eps: 0.826928
  57834/300000: episode: 591, duration: 0.674s, episode steps:  99, steps per second: 147, episode reward: -92.520, mean reward: -0.935 [-100.000, 21.855], mean action: 1.687 [0.000, 3.000],  loss: 18.462260, mse: 7350.480612, mean_q: 68.972426, mean_eps: 0.826648
  57919/300000: episode: 592, duration: 0.593s, episode steps:  85, steps per second: 143, episode reward: -108.447, mean reward: -1.276 [-100.000,  6.103], mean action: 1.729 [0.000, 3.000],  loss: 17.527402, mse: 7200.978338, mean_q: 67.017495, mean_eps: 0.826372
  58032/300000: episode: 593, duration: 0.772s, episode steps: 113, steps per second: 146, episode reward: -168.444, mean reward: -1.491 [-100.000, 16.662], mean action: 1.637 [0.000, 3.000],  loss: 18.235300, mse: 7423.086633, mean_q: 69.386435, mean_eps: 0.826075
  58119/300000: episode: 594, duration: 0.582s, episode steps:  87, steps per second: 149, episode reward: -63.070, mean reward: -0.725 [-100.000,  6.948], mean action: 1.506 [0.000, 3.000],  loss: 25.603074, mse: 7462.068679, mean_q: 69.971354, mean_eps: 0.825775
  58229/300000: episode: 595, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -118.930, mean reward: -1.081 [-100.000, 10.649], mean action: 1.509 [0.000, 3.000],  loss: 18.690950, mse: 7535.793160, mean_q: 69.989677, mean_eps: 0.825480
  58312/300000: episode: 596, duration: 0.559s, episode steps:  83, steps per second: 149, episode reward: -102.875, mean reward: -1.239 [-100.000, 22.632], mean action: 1.410 [0.000, 3.000],  loss: 21.762646, mse: 7456.337149, mean_q: 70.041471, mean_eps: 0.825190
  58430/300000: episode: 597, duration: 0.794s, episode steps: 118, steps per second: 149, episode reward: -117.423, mean reward: -0.995 [-100.000, 10.897], mean action: 1.602 [0.000, 3.000],  loss: 23.672679, mse: 7470.455777, mean_q: 69.087084, mean_eps: 0.824888
  58551/300000: episode: 598, duration: 0.839s, episode steps: 121, steps per second: 144, episode reward: -185.178, mean reward: -1.530 [-100.000,  4.972], mean action: 1.653 [0.000, 3.000],  loss: 32.945047, mse: 7567.182884, mean_q: 70.511210, mean_eps: 0.824530
  58659/300000: episode: 599, duration: 0.724s, episode steps: 108, steps per second: 149, episode reward: -92.718, mean reward: -0.858 [-100.000, 30.063], mean action: 1.565 [0.000, 3.000],  loss: 17.998756, mse: 7475.884653, mean_q: 69.677362, mean_eps: 0.824186
  58751/300000: episode: 600, duration: 0.656s, episode steps:  92, steps per second: 140, episode reward: -141.395, mean reward: -1.537 [-100.000, 10.145], mean action: 1.413 [0.000, 3.000],  loss: 22.539102, mse: 7550.889473, mean_q: 69.240478, mean_eps: 0.823886
  58846/300000: episode: 601, duration: 0.672s, episode steps:  95, steps per second: 141, episode reward: -98.493, mean reward: -1.037 [-100.000, 16.554], mean action: 1.568 [0.000, 3.000],  loss: 21.151229, mse: 7544.976449, mean_q: 69.276452, mean_eps: 0.823606
  58965/300000: episode: 602, duration: 0.800s, episode steps: 119, steps per second: 149, episode reward: -76.882, mean reward: -0.646 [-100.000, 10.290], mean action: 1.639 [0.000, 3.000],  loss: 23.542585, mse: 7393.654838, mean_q: 66.725454, mean_eps: 0.823285
  59040/300000: episode: 603, duration: 0.533s, episode steps:  75, steps per second: 141, episode reward: -92.656, mean reward: -1.235 [-100.000, 11.288], mean action: 1.573 [0.000, 3.000],  loss: 19.339551, mse: 7310.784570, mean_q: 65.728339, mean_eps: 0.822994
  59159/300000: episode: 604, duration: 0.834s, episode steps: 119, steps per second: 143, episode reward: -41.235, mean reward: -0.347 [-100.000, 11.531], mean action: 1.487 [0.000, 3.000],  loss: 20.150093, mse: 7292.248305, mean_q: 65.524090, mean_eps: 0.822703
  59262/300000: episode: 605, duration: 0.694s, episode steps: 103, steps per second: 148, episode reward: -129.608, mean reward: -1.258 [-100.000,  8.067], mean action: 1.738 [0.000, 3.000],  loss: 23.142586, mse: 7315.203898, mean_q: 66.051717, mean_eps: 0.822370
  59342/300000: episode: 606, duration: 0.545s, episode steps:  80, steps per second: 147, episode reward: -73.689, mean reward: -0.921 [-100.000, 14.457], mean action: 1.613 [0.000, 3.000],  loss: 29.212713, mse: 7427.508661, mean_q: 66.902593, mean_eps: 0.822095
  59444/300000: episode: 607, duration: 0.722s, episode steps: 102, steps per second: 141, episode reward: -114.689, mean reward: -1.124 [-100.000,  6.196], mean action: 1.500 [0.000, 3.000],  loss: 24.338187, mse: 7410.767018, mean_q: 67.514997, mean_eps: 0.821822
  59555/300000: episode: 608, duration: 0.754s, episode steps: 111, steps per second: 147, episode reward: -138.587, mean reward: -1.249 [-100.000,  8.288], mean action: 1.333 [0.000, 3.000],  loss: 28.528139, mse: 7310.989328, mean_q: 67.359429, mean_eps: 0.821503
  59628/300000: episode: 609, duration: 0.489s, episode steps:  73, steps per second: 149, episode reward: -91.272, mean reward: -1.250 [-100.000, 17.745], mean action: 1.589 [0.000, 3.000],  loss: 16.938694, mse: 7461.621595, mean_q: 68.011055, mean_eps: 0.821227
  59720/300000: episode: 610, duration: 0.649s, episode steps:  92, steps per second: 142, episode reward: -58.482, mean reward: -0.636 [-100.000,  6.768], mean action: 1.674 [0.000, 3.000],  loss: 16.912906, mse: 7497.550590, mean_q: 69.101113, mean_eps: 0.820979
  59824/300000: episode: 611, duration: 0.725s, episode steps: 104, steps per second: 143, episode reward: -124.312, mean reward: -1.195 [-100.000,  7.606], mean action: 1.529 [0.000, 3.000],  loss: 22.652396, mse: 7534.563852, mean_q: 67.603198, mean_eps: 0.820685
  59896/300000: episode: 612, duration: 0.482s, episode steps:  72, steps per second: 149, episode reward: -68.511, mean reward: -0.952 [-100.000,  8.498], mean action: 1.556 [0.000, 3.000],  loss: 10.285208, mse: 7529.270569, mean_q: 68.388085, mean_eps: 0.820421
  59969/300000: episode: 613, duration: 0.488s, episode steps:  73, steps per second: 150, episode reward: -76.236, mean reward: -1.044 [-100.000, 15.862], mean action: 1.356 [0.000, 3.000],  loss: 9.661792, mse: 7759.700436, mean_q: 68.399184, mean_eps: 0.820204
  60056/300000: episode: 614, duration: 0.610s, episode steps:  87, steps per second: 143, episode reward: -187.367, mean reward: -2.154 [-100.000, 12.276], mean action: 1.391 [0.000, 3.000],  loss: 13.680420, mse: 7680.211392, mean_q: 68.528935, mean_eps: 0.819964
  60130/300000: episode: 615, duration: 0.601s, episode steps:  74, steps per second: 123, episode reward: -81.998, mean reward: -1.108 [-100.000,  9.677], mean action: 1.405 [0.000, 3.000],  loss: 18.140217, mse: 7664.423927, mean_q: 70.183625, mean_eps: 0.819722
  60234/300000: episode: 616, duration: 0.714s, episode steps: 104, steps per second: 146, episode reward: -118.969, mean reward: -1.144 [-100.000,  9.375], mean action: 1.702 [0.000, 3.000],  loss: 23.282507, mse: 7839.898146, mean_q: 69.091911, mean_eps: 0.819456
  60355/300000: episode: 617, duration: 0.865s, episode steps: 121, steps per second: 140, episode reward: -209.031, mean reward: -1.728 [-100.000,  9.649], mean action: 1.537 [0.000, 3.000],  loss: 21.193285, mse: 7997.432980, mean_q: 71.798885, mean_eps: 0.819118
  60418/300000: episode: 618, duration: 0.471s, episode steps:  63, steps per second: 134, episode reward: -74.158, mean reward: -1.177 [-100.000,  6.341], mean action: 1.302 [0.000, 3.000],  loss: 17.273809, mse: 8026.762912, mean_q: 72.274496, mean_eps: 0.818842
  60514/300000: episode: 619, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -90.358, mean reward: -0.941 [-100.000, 10.725], mean action: 1.615 [0.000, 3.000],  loss: 19.425024, mse: 7945.228378, mean_q: 71.997368, mean_eps: 0.818604
  60604/300000: episode: 620, duration: 0.600s, episode steps:  90, steps per second: 150, episode reward: -43.174, mean reward: -0.480 [-100.000, 19.450], mean action: 1.689 [0.000, 3.000],  loss: 15.534679, mse: 7733.029975, mean_q: 71.080721, mean_eps: 0.818324
  60732/300000: episode: 621, duration: 0.911s, episode steps: 128, steps per second: 141, episode reward: -284.519, mean reward: -2.223 [-100.000, 60.980], mean action: 1.500 [0.000, 3.000],  loss: 24.259380, mse: 7753.572468, mean_q: 70.613105, mean_eps: 0.817997
  60850/300000: episode: 622, duration: 0.788s, episode steps: 118, steps per second: 150, episode reward: -56.191, mean reward: -0.476 [-100.000,  6.325], mean action: 1.492 [0.000, 3.000],  loss: 11.790606, mse: 7600.867030, mean_q: 68.333397, mean_eps: 0.817629
  60943/300000: episode: 623, duration: 0.620s, episode steps:  93, steps per second: 150, episode reward: -56.745, mean reward: -0.610 [-100.000, 13.800], mean action: 1.344 [0.000, 3.000],  loss: 9.723605, mse: 7520.364300, mean_q: 67.482636, mean_eps: 0.817312
  61032/300000: episode: 624, duration: 0.651s, episode steps:  89, steps per second: 137, episode reward: -67.642, mean reward: -0.760 [-100.000, 17.481], mean action: 1.753 [0.000, 3.000],  loss: 21.037241, mse: 7607.866244, mean_q: 68.806686, mean_eps: 0.817039
  61139/300000: episode: 625, duration: 0.728s, episode steps: 107, steps per second: 147, episode reward: -46.599, mean reward: -0.436 [-100.000,  6.735], mean action: 1.495 [0.000, 3.000],  loss: 16.303218, mse: 7532.673860, mean_q: 67.915408, mean_eps: 0.816745
  61250/300000: episode: 626, duration: 0.739s, episode steps: 111, steps per second: 150, episode reward: -107.556, mean reward: -0.969 [-100.000, 18.138], mean action: 1.622 [0.000, 3.000],  loss: 14.346631, mse: 7556.948717, mean_q: 67.951442, mean_eps: 0.816418
  61317/300000: episode: 627, duration: 0.474s, episode steps:  67, steps per second: 141, episode reward: -81.091, mean reward: -1.210 [-100.000,  9.055], mean action: 1.896 [0.000, 3.000],  loss: 13.744564, mse: 7489.001713, mean_q: 67.838381, mean_eps: 0.816151
  61422/300000: episode: 628, duration: 0.715s, episode steps: 105, steps per second: 147, episode reward: -97.900, mean reward: -0.932 [-100.000,  6.520], mean action: 1.724 [0.000, 3.000],  loss: 11.663155, mse: 7338.646987, mean_q: 66.807552, mean_eps: 0.815893
  61504/300000: episode: 629, duration: 0.548s, episode steps:  82, steps per second: 150, episode reward: -61.679, mean reward: -0.752 [-100.000,  7.244], mean action: 1.561 [0.000, 3.000],  loss: 12.719924, mse: 7460.057552, mean_q: 67.586734, mean_eps: 0.815612
  61583/300000: episode: 630, duration: 0.553s, episode steps:  79, steps per second: 143, episode reward: -132.535, mean reward: -1.678 [-100.000, 25.676], mean action: 1.468 [0.000, 3.000],  loss: 31.134342, mse: 7387.664372, mean_q: 66.707655, mean_eps: 0.815371
  61653/300000: episode: 631, duration: 0.492s, episode steps:  70, steps per second: 142, episode reward: -119.116, mean reward: -1.702 [-100.000,  6.854], mean action: 1.643 [0.000, 3.000],  loss: 16.189031, mse: 7434.412026, mean_q: 66.636232, mean_eps: 0.815147
  61735/300000: episode: 632, duration: 0.562s, episode steps:  82, steps per second: 146, episode reward: -45.118, mean reward: -0.550 [-100.000, 59.488], mean action: 1.549 [0.000, 3.000],  loss: 18.736949, mse: 7155.942335, mean_q: 63.459424, mean_eps: 0.814919
  61847/300000: episode: 633, duration: 0.757s, episode steps: 112, steps per second: 148, episode reward: -68.382, mean reward: -0.611 [-100.000, 12.439], mean action: 1.679 [0.000, 3.000],  loss: 19.882984, mse: 7106.991080, mean_q: 65.341204, mean_eps: 0.814628
  61905/300000: episode: 634, duration: 0.409s, episode steps:  58, steps per second: 142, episode reward: -49.165, mean reward: -0.848 [-100.000, 12.339], mean action: 1.776 [0.000, 3.000],  loss: 15.232946, mse: 7103.885759, mean_q: 64.937101, mean_eps: 0.814373
  61976/300000: episode: 635, duration: 0.492s, episode steps:  71, steps per second: 144, episode reward: -89.694, mean reward: -1.263 [-100.000,  7.273], mean action: 1.408 [0.000, 3.000],  loss: 17.027340, mse: 7057.903066, mean_q: 65.392991, mean_eps: 0.814180
  62068/300000: episode: 636, duration: 0.625s, episode steps:  92, steps per second: 147, episode reward: -139.116, mean reward: -1.512 [-100.000, 11.207], mean action: 1.554 [0.000, 3.000],  loss: 15.464905, mse: 6922.974631, mean_q: 64.139817, mean_eps: 0.813935
  62214/300000: episode: 637, duration: 0.993s, episode steps: 146, steps per second: 147, episode reward: -98.585, mean reward: -0.675 [-100.000, 11.293], mean action: 1.610 [0.000, 3.000],  loss: 11.454213, mse: 7139.625719, mean_q: 66.983230, mean_eps: 0.813579
  62346/300000: episode: 638, duration: 0.952s, episode steps: 132, steps per second: 139, episode reward: -84.788, mean reward: -0.642 [-100.000, 10.415], mean action: 1.667 [0.000, 3.000],  loss: 9.211309, mse: 7169.695383, mean_q: 66.486559, mean_eps: 0.813161
  62426/300000: episode: 639, duration: 0.546s, episode steps:  80, steps per second: 147, episode reward: -101.679, mean reward: -1.271 [-100.000,  7.853], mean action: 1.375 [0.000, 3.000],  loss: 13.351352, mse: 7327.934113, mean_q: 67.601786, mean_eps: 0.812843
  62508/300000: episode: 640, duration: 0.546s, episode steps:  82, steps per second: 150, episode reward: -111.195, mean reward: -1.356 [-100.000, 11.320], mean action: 1.439 [0.000, 3.000],  loss: 12.624275, mse: 7441.908150, mean_q: 68.761362, mean_eps: 0.812600
  62653/300000: episode: 641, duration: 1.002s, episode steps: 145, steps per second: 145, episode reward: -88.793, mean reward: -0.612 [-100.000,  5.792], mean action: 1.524 [0.000, 3.000],  loss: 11.346386, mse: 7316.981583, mean_q: 66.948088, mean_eps: 0.812260
  62721/300000: episode: 642, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: -90.581, mean reward: -1.332 [-100.000,  7.440], mean action: 1.618 [0.000, 3.000],  loss: 8.567486, mse: 7399.469511, mean_q: 68.411688, mean_eps: 0.811941
  62825/300000: episode: 643, duration: 0.713s, episode steps: 104, steps per second: 146, episode reward: -78.040, mean reward: -0.750 [-100.000, 17.120], mean action: 1.731 [0.000, 3.000],  loss: 18.794935, mse: 7178.815524, mean_q: 65.447923, mean_eps: 0.811683
  62925/300000: episode: 644, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -107.077, mean reward: -1.071 [-100.000, 11.224], mean action: 1.560 [0.000, 3.000],  loss: 20.024782, mse: 7166.146899, mean_q: 65.107725, mean_eps: 0.811377
  63002/300000: episode: 645, duration: 0.564s, episode steps:  77, steps per second: 137, episode reward: -48.815, mean reward: -0.634 [-100.000, 11.943], mean action: 1.584 [0.000, 3.000],  loss: 19.013505, mse: 7095.557852, mean_q: 64.477838, mean_eps: 0.811111
  63074/300000: episode: 646, duration: 0.493s, episode steps:  72, steps per second: 146, episode reward: -69.783, mean reward: -0.969 [-100.000,  7.318], mean action: 1.667 [0.000, 3.000],  loss: 17.380094, mse: 7006.273126, mean_q: 64.438586, mean_eps: 0.810887
  63171/300000: episode: 647, duration: 0.689s, episode steps:  97, steps per second: 141, episode reward: -233.517, mean reward: -2.407 [-100.000,  5.756], mean action: 1.485 [0.000, 3.000],  loss: 13.131119, mse: 7058.859939, mean_q: 66.171786, mean_eps: 0.810634
  63250/300000: episode: 648, duration: 0.542s, episode steps:  79, steps per second: 146, episode reward: -85.692, mean reward: -1.085 [-100.000, 12.651], mean action: 1.570 [0.000, 3.000],  loss: 17.451969, mse: 6904.635087, mean_q: 64.094545, mean_eps: 0.810370
  63333/300000: episode: 649, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -107.508, mean reward: -1.295 [-100.000, 12.119], mean action: 1.542 [0.000, 3.000],  loss: 7.723484, mse: 7281.808129, mean_q: 67.757975, mean_eps: 0.810127
  63459/300000: episode: 650, duration: 0.876s, episode steps: 126, steps per second: 144, episode reward: -110.641, mean reward: -0.878 [-100.000, 17.743], mean action: 1.476 [0.000, 3.000],  loss: 17.643886, mse: 7216.879209, mean_q: 65.580928, mean_eps: 0.809813
  63568/300000: episode: 651, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -84.521, mean reward: -0.775 [-100.000, 66.914], mean action: 1.541 [0.000, 3.000],  loss: 16.300978, mse: 7203.405883, mean_q: 65.654602, mean_eps: 0.809461
  63706/300000: episode: 652, duration: 1.055s, episode steps: 138, steps per second: 131, episode reward: -137.663, mean reward: -0.998 [-100.000,  5.520], mean action: 1.616 [0.000, 3.000],  loss: 12.169676, mse: 7028.973633, mean_q: 64.637411, mean_eps: 0.809090
  63775/300000: episode: 653, duration: 0.609s, episode steps:  69, steps per second: 113, episode reward: -39.647, mean reward: -0.575 [-100.000, 11.626], mean action: 1.507 [0.000, 3.000],  loss: 18.058501, mse: 7042.718276, mean_q: 66.485767, mean_eps: 0.808780
  63884/300000: episode: 654, duration: 0.925s, episode steps: 109, steps per second: 118, episode reward: -258.880, mean reward: -2.375 [-100.000,  5.561], mean action: 1.633 [0.000, 3.000],  loss: 22.392834, mse: 6908.024965, mean_q: 63.698560, mean_eps: 0.808513
  63971/300000: episode: 655, duration: 0.716s, episode steps:  87, steps per second: 121, episode reward: -118.769, mean reward: -1.365 [-100.000, 17.164], mean action: 1.552 [0.000, 3.000],  loss: 22.251759, mse: 6978.491082, mean_q: 64.785849, mean_eps: 0.808219
  64045/300000: episode: 656, duration: 0.623s, episode steps:  74, steps per second: 119, episode reward: -115.407, mean reward: -1.560 [-100.000,  7.891], mean action: 1.459 [0.000, 3.000],  loss: 15.589403, mse: 6818.898081, mean_q: 62.084192, mean_eps: 0.807978
  64163/300000: episode: 657, duration: 0.877s, episode steps: 118, steps per second: 135, episode reward: -97.722, mean reward: -0.828 [-100.000,  8.166], mean action: 1.636 [0.000, 3.000],  loss: 18.085125, mse: 7008.790291, mean_q: 64.700736, mean_eps: 0.807689
  64270/300000: episode: 658, duration: 0.836s, episode steps: 107, steps per second: 128, episode reward: -106.884, mean reward: -0.999 [-100.000,  8.169], mean action: 1.533 [0.000, 3.000],  loss: 19.079067, mse: 6873.770490, mean_q: 64.817589, mean_eps: 0.807352
  64354/300000: episode: 659, duration: 0.658s, episode steps:  84, steps per second: 128, episode reward: -69.538, mean reward: -0.828 [-100.000, 19.719], mean action: 1.619 [0.000, 3.000],  loss: 21.595585, mse: 7039.105969, mean_q: 65.552591, mean_eps: 0.807065
  64468/300000: episode: 660, duration: 0.780s, episode steps: 114, steps per second: 146, episode reward: -157.074, mean reward: -1.378 [-100.000,  7.377], mean action: 1.395 [0.000, 3.000],  loss: 8.165836, mse: 6927.447433, mean_q: 64.257268, mean_eps: 0.806768
  64574/300000: episode: 661, duration: 0.714s, episode steps: 106, steps per second: 148, episode reward: -107.486, mean reward: -1.014 [-100.000,  7.294], mean action: 1.736 [0.000, 3.000],  loss: 13.789370, mse: 6898.744666, mean_q: 64.311299, mean_eps: 0.806438
  64679/300000: episode: 662, duration: 0.750s, episode steps: 105, steps per second: 140, episode reward: -107.034, mean reward: -1.019 [-100.000,  7.529], mean action: 1.448 [0.000, 3.000],  loss: 20.997022, mse: 6800.135007, mean_q: 61.954510, mean_eps: 0.806122
  64829/300000: episode: 663, duration: 1.020s, episode steps: 150, steps per second: 147, episode reward: -76.991, mean reward: -0.513 [-100.000,  9.116], mean action: 1.560 [0.000, 3.000],  loss: 18.504757, mse: 6985.509805, mean_q: 64.280997, mean_eps: 0.805740
  64964/300000: episode: 664, duration: 0.947s, episode steps: 135, steps per second: 143, episode reward: -80.194, mean reward: -0.594 [-100.000,  5.684], mean action: 1.652 [0.000, 3.000],  loss: 19.399704, mse: 6734.143859, mean_q: 63.578794, mean_eps: 0.805312
  65088/300000: episode: 665, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -79.619, mean reward: -0.642 [-100.000,  7.024], mean action: 1.500 [0.000, 3.000],  loss: 25.322454, mse: 6676.571927, mean_q: 64.233790, mean_eps: 0.804924
  65188/300000: episode: 666, duration: 0.842s, episode steps: 100, steps per second: 119, episode reward: -202.809, mean reward: -2.028 [-100.000, 81.962], mean action: 1.720 [0.000, 3.000],  loss: 20.083477, mse: 6725.186729, mean_q: 65.529692, mean_eps: 0.804587
  65271/300000: episode: 667, duration: 0.695s, episode steps:  83, steps per second: 119, episode reward: -51.760, mean reward: -0.624 [-100.000,  7.040], mean action: 1.711 [0.000, 3.000],  loss: 28.661396, mse: 6813.624929, mean_q: 65.649346, mean_eps: 0.804313
  65381/300000: episode: 668, duration: 0.795s, episode steps: 110, steps per second: 138, episode reward: -125.719, mean reward: -1.143 [-100.000, 11.328], mean action: 1.527 [0.000, 3.000],  loss: 21.179897, mse: 6823.554017, mean_q: 65.437041, mean_eps: 0.804024
  65467/300000: episode: 669, duration: 0.640s, episode steps:  86, steps per second: 134, episode reward: -51.907, mean reward: -0.604 [-100.000, 16.533], mean action: 1.605 [0.000, 3.000],  loss: 21.454608, mse: 6866.609608, mean_q: 64.910441, mean_eps: 0.803730
  65538/300000: episode: 670, duration: 0.620s, episode steps:  71, steps per second: 115, episode reward: -36.640, mean reward: -0.516 [-100.000, 17.205], mean action: 1.521 [0.000, 3.000],  loss: 23.357163, mse: 6921.062342, mean_q: 65.537665, mean_eps: 0.803494
  65698/300000: episode: 671, duration: 1.152s, episode steps: 160, steps per second: 139, episode reward: -84.546, mean reward: -0.528 [-100.000,  8.009], mean action: 1.688 [0.000, 3.000],  loss: 14.829081, mse: 6813.433127, mean_q: 64.810287, mean_eps: 0.803148
  65798/300000: episode: 672, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: -34.351, mean reward: -0.344 [-100.000, 79.771], mean action: 1.660 [0.000, 3.000],  loss: 21.834179, mse: 6887.655459, mean_q: 64.389330, mean_eps: 0.802757
  65916/300000: episode: 673, duration: 0.831s, episode steps: 118, steps per second: 142, episode reward: -194.895, mean reward: -1.652 [-100.000,  6.831], mean action: 1.458 [0.000, 3.000],  loss: 18.072113, mse: 7060.480957, mean_q: 66.204724, mean_eps: 0.802430
  66037/300000: episode: 674, duration: 0.813s, episode steps: 121, steps per second: 149, episode reward: -85.899, mean reward: -0.710 [-100.000,  7.468], mean action: 1.537 [0.000, 3.000],  loss: 21.303186, mse: 6973.284494, mean_q: 65.276571, mean_eps: 0.802072
  66140/300000: episode: 675, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -94.859, mean reward: -0.921 [-100.000, 21.189], mean action: 1.718 [0.000, 3.000],  loss: 21.132974, mse: 6861.994989, mean_q: 64.350434, mean_eps: 0.801736
  66222/300000: episode: 676, duration: 0.549s, episode steps:  82, steps per second: 149, episode reward: -83.164, mean reward: -1.014 [-100.000, 13.124], mean action: 1.537 [0.000, 3.000],  loss: 11.851807, mse: 6983.929902, mean_q: 64.389500, mean_eps: 0.801458
  66291/300000: episode: 677, duration: 0.476s, episode steps:  69, steps per second: 145, episode reward: -71.480, mean reward: -1.036 [-100.000,  7.987], mean action: 1.681 [0.000, 3.000],  loss: 14.394263, mse: 6958.919165, mean_q: 63.747122, mean_eps: 0.801232
  66365/300000: episode: 678, duration: 0.508s, episode steps:  74, steps per second: 146, episode reward: -65.284, mean reward: -0.882 [-100.000, 15.924], mean action: 1.554 [0.000, 3.000],  loss: 21.546219, mse: 6851.322411, mean_q: 63.418019, mean_eps: 0.801017
  66447/300000: episode: 679, duration: 0.579s, episode steps:  82, steps per second: 142, episode reward: -88.569, mean reward: -1.080 [-100.000, 17.750], mean action: 1.671 [0.000, 3.000],  loss: 18.099913, mse: 6849.178407, mean_q: 64.015303, mean_eps: 0.800783
  66534/300000: episode: 680, duration: 0.584s, episode steps:  87, steps per second: 149, episode reward: -76.455, mean reward: -0.879 [-100.000, 10.784], mean action: 1.517 [0.000, 3.000],  loss: 18.512008, mse: 6898.420803, mean_q: 65.429267, mean_eps: 0.800530
  66601/300000: episode: 681, duration: 0.460s, episode steps:  67, steps per second: 146, episode reward: -65.895, mean reward: -0.984 [-100.000, 12.414], mean action: 1.478 [0.000, 3.000],  loss: 8.505925, mse: 6828.298077, mean_q: 65.170781, mean_eps: 0.800299
  66683/300000: episode: 682, duration: 0.588s, episode steps:  82, steps per second: 140, episode reward: -117.967, mean reward: -1.439 [-100.000, 12.401], mean action: 1.512 [0.000, 3.000],  loss: 26.468697, mse: 6775.903029, mean_q: 64.259146, mean_eps: 0.800076
  66751/300000: episode: 683, duration: 0.480s, episode steps:  68, steps per second: 142, episode reward: -45.873, mean reward: -0.675 [-100.000, 16.469], mean action: 1.632 [0.000, 3.000],  loss: 20.938587, mse: 6925.482903, mean_q: 65.117074, mean_eps: 0.799851
  66850/300000: episode: 684, duration: 0.671s, episode steps:  99, steps per second: 147, episode reward: -99.055, mean reward: -1.001 [-100.000, 71.462], mean action: 1.505 [0.000, 3.000],  loss: 26.271799, mse: 6793.980903, mean_q: 63.496266, mean_eps: 0.799600
  66964/300000: episode: 685, duration: 0.789s, episode steps: 114, steps per second: 145, episode reward: -91.261, mean reward: -0.801 [-100.000,  6.729], mean action: 1.658 [0.000, 3.000],  loss: 15.521130, mse: 6755.967568, mean_q: 61.548900, mean_eps: 0.799280
  67072/300000: episode: 686, duration: 0.752s, episode steps: 108, steps per second: 144, episode reward: -65.201, mean reward: -0.604 [-100.000, 22.047], mean action: 1.620 [0.000, 3.000],  loss: 16.657547, mse: 6750.802332, mean_q: 62.113972, mean_eps: 0.798947
  67143/300000: episode: 687, duration: 0.475s, episode steps:  71, steps per second: 149, episode reward: -70.471, mean reward: -0.993 [-100.000, 11.563], mean action: 1.535 [0.000, 3.000],  loss: 13.657959, mse: 6741.428085, mean_q: 61.470004, mean_eps: 0.798679
  67264/300000: episode: 688, duration: 0.817s, episode steps: 121, steps per second: 148, episode reward: -103.738, mean reward: -0.857 [-100.000, 10.838], mean action: 1.488 [0.000, 3.000],  loss: 19.353758, mse: 6700.163013, mean_q: 60.693383, mean_eps: 0.798391
  67353/300000: episode: 689, duration: 0.626s, episode steps:  89, steps per second: 142, episode reward: -132.815, mean reward: -1.492 [-100.000,  7.612], mean action: 1.539 [0.000, 3.000],  loss: 21.009465, mse: 6852.977676, mean_q: 62.051065, mean_eps: 0.798076
  67508/300000: episode: 690, duration: 1.046s, episode steps: 155, steps per second: 148, episode reward: -44.153, mean reward: -0.285 [-100.000, 17.198], mean action: 1.697 [0.000, 3.000],  loss: 31.480041, mse: 6729.909507, mean_q: 61.778240, mean_eps: 0.797710
  67614/300000: episode: 691, duration: 0.724s, episode steps: 106, steps per second: 146, episode reward: -77.236, mean reward: -0.729 [-100.000,  8.882], mean action: 1.623 [0.000, 3.000],  loss: 27.186917, mse: 6779.515224, mean_q: 62.441671, mean_eps: 0.797318
  67757/300000: episode: 692, duration: 0.988s, episode steps: 143, steps per second: 145, episode reward: -287.276, mean reward: -2.009 [-100.000, 44.087], mean action: 1.427 [0.000, 3.000],  loss: 12.666981, mse: 6749.253527, mean_q: 61.748929, mean_eps: 0.796945
  67884/300000: episode: 693, duration: 0.843s, episode steps: 127, steps per second: 151, episode reward: -103.850, mean reward: -0.818 [-100.000,  6.477], mean action: 1.354 [0.000, 3.000],  loss: 21.794317, mse: 6769.672248, mean_q: 61.778827, mean_eps: 0.796540
  67999/300000: episode: 694, duration: 0.814s, episode steps: 115, steps per second: 141, episode reward: -43.798, mean reward: -0.381 [-100.000, 12.629], mean action: 1.487 [0.000, 3.000],  loss: 12.502411, mse: 6652.015009, mean_q: 59.525883, mean_eps: 0.796177
  68082/300000: episode: 695, duration: 0.563s, episode steps:  83, steps per second: 147, episode reward: -109.974, mean reward: -1.325 [-100.000, 29.552], mean action: 1.614 [0.000, 3.000],  loss: 16.559790, mse: 6655.083161, mean_q: 59.873585, mean_eps: 0.795880
  68172/300000: episode: 696, duration: 0.604s, episode steps:  90, steps per second: 149, episode reward: -89.488, mean reward: -0.994 [-100.000,  7.402], mean action: 1.367 [0.000, 3.000],  loss: 13.942681, mse: 6559.046647, mean_q: 59.812543, mean_eps: 0.795620
  68267/300000: episode: 697, duration: 0.651s, episode steps:  95, steps per second: 146, episode reward: -74.589, mean reward: -0.785 [-100.000, 12.631], mean action: 1.632 [0.000, 3.000],  loss: 6.615065, mse: 6495.357175, mean_q: 60.045684, mean_eps: 0.795343
  68368/300000: episode: 698, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: -133.586, mean reward: -1.323 [-100.000, 11.708], mean action: 1.455 [0.000, 3.000],  loss: 15.887300, mse: 6474.332969, mean_q: 58.835040, mean_eps: 0.795049
  68497/300000: episode: 699, duration: 0.866s, episode steps: 129, steps per second: 149, episode reward: -89.976, mean reward: -0.697 [-100.000,  8.527], mean action: 1.651 [0.000, 3.000],  loss: 18.370428, mse: 6497.921508, mean_q: 59.200551, mean_eps: 0.794704
  68584/300000: episode: 700, duration: 0.616s, episode steps:  87, steps per second: 141, episode reward: -18.493, mean reward: -0.213 [-100.000,  9.918], mean action: 1.632 [0.000, 3.000],  loss: 24.014616, mse: 6611.676303, mean_q: 61.248598, mean_eps: 0.794380
  68687/300000: episode: 701, duration: 0.716s, episode steps: 103, steps per second: 144, episode reward: -43.428, mean reward: -0.422 [-100.000,  8.463], mean action: 1.621 [0.000, 3.000],  loss: 23.849048, mse: 6647.890957, mean_q: 61.132502, mean_eps: 0.794095
  68781/300000: episode: 702, duration: 0.626s, episode steps:  94, steps per second: 150, episode reward: -127.890, mean reward: -1.361 [-100.000, 13.730], mean action: 1.404 [0.000, 3.000],  loss: 25.774853, mse: 6357.568437, mean_q: 58.139946, mean_eps: 0.793799
  68929/300000: episode: 703, duration: 1.026s, episode steps: 148, steps per second: 144, episode reward: -125.598, mean reward: -0.849 [-100.000, 14.265], mean action: 1.527 [0.000, 3.000],  loss: 14.544249, mse: 6407.168002, mean_q: 59.271209, mean_eps: 0.793437
  69006/300000: episode: 704, duration: 0.537s, episode steps:  77, steps per second: 143, episode reward: -41.608, mean reward: -0.540 [-100.000, 13.897], mean action: 1.532 [0.000, 3.000],  loss: 18.517504, mse: 6535.206695, mean_q: 59.889950, mean_eps: 0.793099
  69078/300000: episode: 705, duration: 0.483s, episode steps:  72, steps per second: 149, episode reward: -105.093, mean reward: -1.460 [-100.000, 14.256], mean action: 1.764 [0.000, 3.000],  loss: 9.906004, mse: 6721.189772, mean_q: 61.296345, mean_eps: 0.792875
  69180/300000: episode: 706, duration: 0.678s, episode steps: 102, steps per second: 151, episode reward: -65.104, mean reward: -0.638 [-100.000, 13.742], mean action: 1.559 [0.000, 3.000],  loss: 19.188591, mse: 6724.699185, mean_q: 61.157132, mean_eps: 0.792615
  69296/300000: episode: 707, duration: 0.812s, episode steps: 116, steps per second: 143, episode reward: -98.512, mean reward: -0.849 [-100.000, 11.379], mean action: 1.707 [0.000, 3.000],  loss: 17.015623, mse: 6512.368770, mean_q: 60.615100, mean_eps: 0.792288
  69420/300000: episode: 708, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -83.303, mean reward: -0.672 [-100.000, 20.295], mean action: 1.524 [0.000, 3.000],  loss: 11.237381, mse: 6483.005032, mean_q: 59.984813, mean_eps: 0.791928
  69519/300000: episode: 709, duration: 0.998s, episode steps:  99, steps per second:  99, episode reward: -102.915, mean reward: -1.040 [-100.000, 10.054], mean action: 1.667 [0.000, 3.000],  loss: 15.471801, mse: 6636.356736, mean_q: 62.035204, mean_eps: 0.791593
  69626/300000: episode: 710, duration: 1.130s, episode steps: 107, steps per second:  95, episode reward: -78.611, mean reward: -0.735 [-100.000, 11.926], mean action: 1.626 [0.000, 3.000],  loss: 18.147364, mse: 6828.033308, mean_q: 63.614769, mean_eps: 0.791284
  69775/300000: episode: 711, duration: 1.245s, episode steps: 149, steps per second: 120, episode reward: -0.076, mean reward: -0.001 [-100.000, 74.990], mean action: 1.624 [0.000, 3.000],  loss: 17.215768, mse: 6675.099023, mean_q: 62.418452, mean_eps: 0.790900
  70775/300000: episode: 712, duration: 7.859s, episode steps: 1000, steps per second: 127, episode reward: 43.401, mean reward:  0.043 [-23.639, 87.223], mean action: 1.554 [0.000, 3.000],  loss: 21.816377, mse: 6606.530714, mean_q: 63.268231, mean_eps: 0.789177
  70867/300000: episode: 713, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -89.253, mean reward: -0.970 [-100.000, 10.332], mean action: 1.522 [0.000, 3.000],  loss: 24.953048, mse: 6609.493191, mean_q: 63.134309, mean_eps: 0.787539
  71006/300000: episode: 714, duration: 0.927s, episode steps: 139, steps per second: 150, episode reward: -68.101, mean reward: -0.490 [-100.000, 21.709], mean action: 1.432 [0.000, 3.000],  loss: 16.920195, mse: 6760.740849, mean_q: 65.059844, mean_eps: 0.787192
  71145/300000: episode: 715, duration: 0.972s, episode steps: 139, steps per second: 143, episode reward:  2.567, mean reward:  0.018 [-100.000, 56.026], mean action: 1.612 [0.000, 3.000],  loss: 18.817724, mse: 6755.864957, mean_q: 64.507497, mean_eps: 0.786775
  71241/300000: episode: 716, duration: 0.638s, episode steps:  96, steps per second: 151, episode reward: -82.459, mean reward: -0.859 [-100.000, 16.904], mean action: 1.594 [0.000, 3.000],  loss: 10.734007, mse: 6800.872223, mean_q: 64.590522, mean_eps: 0.786423
  71308/300000: episode: 717, duration: 0.448s, episode steps:  67, steps per second: 149, episode reward: -62.021, mean reward: -0.926 [-100.000,  5.971], mean action: 1.627 [0.000, 3.000],  loss: 20.657463, mse: 6767.108238, mean_q: 64.399385, mean_eps: 0.786178
  71435/300000: episode: 718, duration: 0.910s, episode steps: 127, steps per second: 140, episode reward: -160.886, mean reward: -1.267 [-100.000,  8.482], mean action: 1.496 [0.000, 3.000],  loss: 15.620164, mse: 6765.986486, mean_q: 66.160114, mean_eps: 0.785887
  71518/300000: episode: 719, duration: 0.558s, episode steps:  83, steps per second: 149, episode reward: -96.087, mean reward: -1.158 [-100.000,  6.251], mean action: 1.506 [0.000, 3.000],  loss: 31.826182, mse: 6715.180417, mean_q: 65.577434, mean_eps: 0.785572
  71644/300000: episode: 720, duration: 0.842s, episode steps: 126, steps per second: 150, episode reward: -9.969, mean reward: -0.079 [-100.000, 48.313], mean action: 1.381 [0.000, 3.000],  loss: 16.897012, mse: 6638.691592, mean_q: 64.249815, mean_eps: 0.785258
  71720/300000: episode: 721, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -52.809, mean reward: -0.695 [-100.000,  7.466], mean action: 1.513 [0.000, 3.000],  loss: 13.566954, mse: 6368.214054, mean_q: 61.757360, mean_eps: 0.784956
  71793/300000: episode: 722, duration: 0.523s, episode steps:  73, steps per second: 140, episode reward: -65.397, mean reward: -0.896 [-100.000, 12.273], mean action: 1.616 [0.000, 3.000],  loss: 14.920280, mse: 6314.463359, mean_q: 62.811169, mean_eps: 0.784732
  71861/300000: episode: 723, duration: 0.462s, episode steps:  68, steps per second: 147, episode reward: -129.092, mean reward: -1.898 [-100.000,  7.665], mean action: 1.588 [0.000, 3.000],  loss: 19.701908, mse: 6221.696569, mean_q: 61.455156, mean_eps: 0.784521
  71982/300000: episode: 724, duration: 0.806s, episode steps: 121, steps per second: 150, episode reward: -94.085, mean reward: -0.778 [-100.000, 16.190], mean action: 1.537 [0.000, 3.000],  loss: 30.906165, mse: 6159.864883, mean_q: 61.379352, mean_eps: 0.784237
  72105/300000: episode: 725, duration: 0.860s, episode steps: 123, steps per second: 143, episode reward: -91.642, mean reward: -0.745 [-100.000,  7.665], mean action: 1.463 [0.000, 3.000],  loss: 23.150433, mse: 6282.822385, mean_q: 64.318431, mean_eps: 0.783871
  72192/300000: episode: 726, duration: 0.584s, episode steps:  87, steps per second: 149, episode reward: -69.368, mean reward: -0.797 [-100.000, 15.637], mean action: 1.759 [0.000, 3.000],  loss: 11.912173, mse: 6316.210803, mean_q: 64.108602, mean_eps: 0.783556
  72330/300000: episode: 727, duration: 0.927s, episode steps: 138, steps per second: 149, episode reward: -84.590, mean reward: -0.613 [-100.000, 13.533], mean action: 1.522 [0.000, 3.000],  loss: 16.851804, mse: 6258.573157, mean_q: 62.404108, mean_eps: 0.783219
  72425/300000: episode: 728, duration: 0.740s, episode steps:  95, steps per second: 128, episode reward: -49.864, mean reward: -0.525 [-100.000, 22.033], mean action: 1.674 [0.000, 3.000],  loss: 12.059341, mse: 6210.271567, mean_q: 61.548927, mean_eps: 0.782869
  72505/300000: episode: 729, duration: 0.578s, episode steps:  80, steps per second: 138, episode reward: -73.658, mean reward: -0.921 [-100.000, 11.807], mean action: 1.663 [0.000, 3.000],  loss: 36.080072, mse: 6258.406573, mean_q: 62.313844, mean_eps: 0.782606
  72614/300000: episode: 730, duration: 0.774s, episode steps: 109, steps per second: 141, episode reward: -51.504, mean reward: -0.473 [-100.000,  7.975], mean action: 1.587 [0.000, 3.000],  loss: 18.259363, mse: 6222.087770, mean_q: 62.739705, mean_eps: 0.782323
  72795/300000: episode: 731, duration: 1.413s, episode steps: 181, steps per second: 128, episode reward: -186.104, mean reward: -1.028 [-100.000, 84.615], mean action: 1.718 [0.000, 3.000],  loss: 14.609927, mse: 6236.179283, mean_q: 62.360863, mean_eps: 0.781888
  72904/300000: episode: 732, duration: 0.956s, episode steps: 109, steps per second: 114, episode reward: 22.474, mean reward:  0.206 [-100.000, 89.562], mean action: 1.394 [0.000, 3.000],  loss: 10.446620, mse: 6331.477230, mean_q: 62.923117, mean_eps: 0.781453
  73006/300000: episode: 733, duration: 0.930s, episode steps: 102, steps per second: 110, episode reward: -101.635, mean reward: -0.996 [-100.000,  6.226], mean action: 1.480 [0.000, 3.000],  loss: 18.599246, mse: 6282.110730, mean_q: 62.324342, mean_eps: 0.781137
  73141/300000: episode: 734, duration: 1.065s, episode steps: 135, steps per second: 127, episode reward: -10.443, mean reward: -0.077 [-100.000, 26.945], mean action: 1.659 [0.000, 3.000],  loss: 15.313908, mse: 6357.897490, mean_q: 63.245246, mean_eps: 0.780781
  73250/300000: episode: 735, duration: 0.833s, episode steps: 109, steps per second: 131, episode reward: -49.561, mean reward: -0.455 [-100.000,  9.737], mean action: 1.651 [0.000, 3.000],  loss: 22.930042, mse: 6518.424442, mean_q: 65.023901, mean_eps: 0.780415
  73376/300000: episode: 736, duration: 0.929s, episode steps: 126, steps per second: 136, episode reward: -40.914, mean reward: -0.325 [-100.000, 43.993], mean action: 1.587 [0.000, 3.000],  loss: 22.985836, mse: 6346.903867, mean_q: 64.077035, mean_eps: 0.780062
  73522/300000: episode: 737, duration: 1.046s, episode steps: 146, steps per second: 140, episode reward: -293.098, mean reward: -2.008 [-100.000, 86.954], mean action: 1.438 [0.000, 3.000],  loss: 31.503880, mse: 6284.034424, mean_q: 64.496083, mean_eps: 0.779654
  73639/300000: episode: 738, duration: 0.800s, episode steps: 117, steps per second: 146, episode reward: -173.424, mean reward: -1.482 [-100.000,  9.853], mean action: 1.667 [0.000, 3.000],  loss: 18.556206, mse: 6378.556115, mean_q: 65.747296, mean_eps: 0.779260
  73719/300000: episode: 739, duration: 0.551s, episode steps:  80, steps per second: 145, episode reward: -60.837, mean reward: -0.760 [-100.000, 17.090], mean action: 1.512 [0.000, 3.000],  loss: 19.721999, mse: 6380.294324, mean_q: 65.865307, mean_eps: 0.778965
  73850/300000: episode: 740, duration: 0.909s, episode steps: 131, steps per second: 144, episode reward: -62.603, mean reward: -0.478 [-100.000,  6.719], mean action: 1.656 [0.000, 3.000],  loss: 29.256904, mse: 6333.747965, mean_q: 67.034814, mean_eps: 0.778648
  73942/300000: episode: 741, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -89.473, mean reward: -0.973 [-100.000, 38.746], mean action: 1.304 [0.000, 3.000],  loss: 27.523439, mse: 6402.980612, mean_q: 66.205852, mean_eps: 0.778314
  74039/300000: episode: 742, duration: 0.664s, episode steps:  97, steps per second: 146, episode reward: -352.240, mean reward: -3.631 [-100.000, 49.946], mean action: 1.495 [0.000, 3.000],  loss: 35.226221, mse: 6320.805664, mean_q: 66.021916, mean_eps: 0.778030
  74130/300000: episode: 743, duration: 0.720s, episode steps:  91, steps per second: 126, episode reward: -64.582, mean reward: -0.710 [-100.000, 17.534], mean action: 1.593 [0.000, 3.000],  loss: 27.777785, mse: 6545.357749, mean_q: 68.189056, mean_eps: 0.777748
  74261/300000: episode: 744, duration: 1.025s, episode steps: 131, steps per second: 128, episode reward: -92.172, mean reward: -0.704 [-100.000, 10.954], mean action: 1.603 [0.000, 3.000],  loss: 24.432264, mse: 6318.098700, mean_q: 66.256700, mean_eps: 0.777415
  74367/300000: episode: 745, duration: 0.903s, episode steps: 106, steps per second: 117, episode reward: -71.291, mean reward: -0.673 [-100.000,  8.844], mean action: 1.566 [0.000, 3.000],  loss: 18.420443, mse: 6307.544774, mean_q: 67.122910, mean_eps: 0.777060
  74488/300000: episode: 746, duration: 0.866s, episode steps: 121, steps per second: 140, episode reward: -163.013, mean reward: -1.347 [-100.000, 56.782], mean action: 1.496 [0.000, 3.000],  loss: 29.243578, mse: 6097.378729, mean_q: 65.279294, mean_eps: 0.776719
  74565/300000: episode: 747, duration: 0.567s, episode steps:  77, steps per second: 136, episode reward: -50.906, mean reward: -0.661 [-100.000,  5.881], mean action: 1.805 [0.000, 3.000],  loss: 19.985743, mse: 6236.205338, mean_q: 64.784447, mean_eps: 0.776422
  74637/300000: episode: 748, duration: 0.564s, episode steps:  72, steps per second: 128, episode reward: -48.431, mean reward: -0.673 [-100.000,  6.589], mean action: 1.736 [0.000, 3.000],  loss: 45.386802, mse: 6232.395928, mean_q: 65.364766, mean_eps: 0.776199
  74782/300000: episode: 749, duration: 1.056s, episode steps: 145, steps per second: 137, episode reward: -85.920, mean reward: -0.593 [-100.000, 15.385], mean action: 1.455 [0.000, 3.000],  loss: 27.403647, mse: 6102.023848, mean_q: 65.333554, mean_eps: 0.775873
  74872/300000: episode: 750, duration: 0.609s, episode steps:  90, steps per second: 148, episode reward: -86.987, mean reward: -0.967 [-100.000,  9.973], mean action: 1.611 [0.000, 3.000],  loss: 32.982811, mse: 6253.449615, mean_q: 66.722521, mean_eps: 0.775521
  74976/300000: episode: 751, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -126.961, mean reward: -1.221 [-100.000,  9.669], mean action: 1.587 [0.000, 3.000],  loss: 22.667923, mse: 6329.906241, mean_q: 68.413227, mean_eps: 0.775229
  75071/300000: episode: 752, duration: 0.642s, episode steps:  95, steps per second: 148, episode reward: -74.098, mean reward: -0.780 [-100.000, 16.363], mean action: 1.684 [0.000, 3.000],  loss: 26.669477, mse: 6272.421304, mean_q: 67.500558, mean_eps: 0.774931
  75168/300000: episode: 753, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -62.481, mean reward: -0.644 [-100.000, 12.599], mean action: 1.608 [0.000, 3.000],  loss: 15.119148, mse: 6409.108947, mean_q: 67.613904, mean_eps: 0.774643
  75233/300000: episode: 754, duration: 0.466s, episode steps:  65, steps per second: 140, episode reward: -83.474, mean reward: -1.284 [-100.000, 10.207], mean action: 1.815 [0.000, 3.000],  loss: 22.361692, mse: 6323.727644, mean_q: 67.763564, mean_eps: 0.774400
  75360/300000: episode: 755, duration: 0.888s, episode steps: 127, steps per second: 143, episode reward: -30.854, mean reward: -0.243 [-100.000, 22.699], mean action: 1.669 [0.000, 3.000],  loss: 30.629324, mse: 6482.467600, mean_q: 68.213390, mean_eps: 0.774112
  75457/300000: episode: 756, duration: 0.652s, episode steps:  97, steps per second: 149, episode reward: -87.496, mean reward: -0.902 [-100.000, 10.025], mean action: 1.629 [0.000, 3.000],  loss: 27.030528, mse: 6541.931540, mean_q: 68.537069, mean_eps: 0.773776
  75530/300000: episode: 757, duration: 0.505s, episode steps:  73, steps per second: 145, episode reward: -78.047, mean reward: -1.069 [-100.000,  9.422], mean action: 1.589 [0.000, 3.000],  loss: 21.150878, mse: 6440.811878, mean_q: 66.061281, mean_eps: 0.773521
  75662/300000: episode: 758, duration: 0.943s, episode steps: 132, steps per second: 140, episode reward: -116.149, mean reward: -0.880 [-100.000,  7.934], mean action: 1.765 [0.000, 3.000],  loss: 24.602358, mse: 6478.612915, mean_q: 67.547858, mean_eps: 0.773214
  75757/300000: episode: 759, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -38.373, mean reward: -0.404 [-100.000, 48.523], mean action: 1.800 [0.000, 3.000],  loss: 19.064610, mse: 6423.026216, mean_q: 65.215722, mean_eps: 0.772873
  75848/300000: episode: 760, duration: 0.628s, episode steps:  91, steps per second: 145, episode reward: -41.559, mean reward: -0.457 [-100.000, 12.972], mean action: 1.538 [0.000, 3.000],  loss: 12.744647, mse: 6431.539653, mean_q: 66.206319, mean_eps: 0.772594
  75943/300000: episode: 761, duration: 0.687s, episode steps:  95, steps per second: 138, episode reward: -73.977, mean reward: -0.779 [-100.000, 14.193], mean action: 1.653 [0.000, 3.000],  loss: 19.860128, mse: 6479.575642, mean_q: 66.807764, mean_eps: 0.772315
  76008/300000: episode: 762, duration: 0.442s, episode steps:  65, steps per second: 147, episode reward: -51.688, mean reward: -0.795 [-100.000, 13.824], mean action: 1.385 [0.000, 3.000],  loss: 29.869518, mse: 6405.419599, mean_q: 65.634803, mean_eps: 0.772075
  76125/300000: episode: 763, duration: 0.780s, episode steps: 117, steps per second: 150, episode reward: -83.795, mean reward: -0.716 [-100.000,  6.297], mean action: 1.547 [0.000, 3.000],  loss: 31.976484, mse: 6403.751486, mean_q: 66.803254, mean_eps: 0.771802
  76267/300000: episode: 764, duration: 1.001s, episode steps: 142, steps per second: 142, episode reward: -48.865, mean reward: -0.344 [-100.000,  7.082], mean action: 1.697 [0.000, 3.000],  loss: 20.152031, mse: 6566.809667, mean_q: 67.424388, mean_eps: 0.771413
  76366/300000: episode: 765, duration: 0.665s, episode steps:  99, steps per second: 149, episode reward: -75.450, mean reward: -0.762 [-100.000, 15.552], mean action: 1.687 [0.000, 3.000],  loss: 27.213747, mse: 6687.495699, mean_q: 68.043534, mean_eps: 0.771052
  76446/300000: episode: 766, duration: 0.532s, episode steps:  80, steps per second: 150, episode reward: -111.722, mean reward: -1.397 [-100.000,  6.181], mean action: 1.500 [0.000, 3.000],  loss: 14.611095, mse: 6726.146918, mean_q: 67.822048, mean_eps: 0.770784
  76538/300000: episode: 767, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -54.785, mean reward: -0.595 [-100.000, 11.006], mean action: 1.554 [0.000, 3.000],  loss: 23.609381, mse: 6707.795219, mean_q: 68.931768, mean_eps: 0.770525
  76680/300000: episode: 768, duration: 0.968s, episode steps: 142, steps per second: 147, episode reward: -103.507, mean reward: -0.729 [-100.000,  6.240], mean action: 1.648 [0.000, 3.000],  loss: 14.794863, mse: 6593.797408, mean_q: 69.076264, mean_eps: 0.770174
  76763/300000: episode: 769, duration: 0.557s, episode steps:  83, steps per second: 149, episode reward: -74.851, mean reward: -0.902 [-100.000, 12.116], mean action: 1.578 [0.000, 3.000],  loss: 11.989968, mse: 6789.052440, mean_q: 68.681289, mean_eps: 0.769837
  76876/300000: episode: 770, duration: 0.796s, episode steps: 113, steps per second: 142, episode reward: -86.845, mean reward: -0.769 [-100.000, 10.235], mean action: 1.584 [0.000, 3.000],  loss: 12.827394, mse: 6762.651475, mean_q: 69.342201, mean_eps: 0.769543
  76963/300000: episode: 771, duration: 0.609s, episode steps:  87, steps per second: 143, episode reward: -82.659, mean reward: -0.950 [-100.000,  6.867], mean action: 1.644 [0.000, 3.000],  loss: 15.912297, mse: 6821.307090, mean_q: 68.944346, mean_eps: 0.769243
  77050/300000: episode: 772, duration: 0.590s, episode steps:  87, steps per second: 147, episode reward: -71.550, mean reward: -0.822 [-100.000, 50.569], mean action: 1.563 [0.000, 3.000],  loss: 18.963777, mse: 6926.414523, mean_q: 68.809987, mean_eps: 0.768982
  77142/300000: episode: 773, duration: 0.670s, episode steps:  92, steps per second: 137, episode reward: -50.738, mean reward: -0.552 [-100.000,  7.841], mean action: 1.772 [0.000, 3.000],  loss: 29.234931, mse: 6849.950604, mean_q: 67.527542, mean_eps: 0.768714
  77245/300000: episode: 774, duration: 0.703s, episode steps: 103, steps per second: 147, episode reward: -73.161, mean reward: -0.710 [-100.000, 14.951], mean action: 1.340 [0.000, 3.000],  loss: 36.793506, mse: 6916.791983, mean_q: 68.737294, mean_eps: 0.768421
  77319/300000: episode: 775, duration: 0.505s, episode steps:  74, steps per second: 147, episode reward: -70.997, mean reward: -0.959 [-100.000,  7.257], mean action: 1.541 [0.000, 3.000],  loss: 11.698945, mse: 6826.608854, mean_q: 69.144634, mean_eps: 0.768155
  77420/300000: episode: 776, duration: 0.688s, episode steps: 101, steps per second: 147, episode reward: -98.100, mean reward: -0.971 [-100.000, 42.941], mean action: 1.386 [0.000, 3.000],  loss: 25.748525, mse: 7058.735202, mean_q: 70.225298, mean_eps: 0.767893
  77530/300000: episode: 777, duration: 0.754s, episode steps: 110, steps per second: 146, episode reward: -106.961, mean reward: -0.972 [-100.000,  9.957], mean action: 1.673 [0.000, 3.000],  loss: 16.001179, mse: 6925.583762, mean_q: 68.389112, mean_eps: 0.767577
  77677/300000: episode: 778, duration: 1.034s, episode steps: 147, steps per second: 142, episode reward: -55.180, mean reward: -0.375 [-100.000,  6.871], mean action: 1.646 [0.000, 3.000],  loss: 17.234082, mse: 7044.282047, mean_q: 70.467397, mean_eps: 0.767191
  77806/300000: episode: 779, duration: 0.901s, episode steps: 129, steps per second: 143, episode reward: -127.046, mean reward: -0.985 [-100.000,  8.647], mean action: 1.651 [0.000, 3.000],  loss: 31.997574, mse: 7023.680289, mean_q: 69.686046, mean_eps: 0.766777
  77946/300000: episode: 780, duration: 0.935s, episode steps: 140, steps per second: 150, episode reward: -280.763, mean reward: -2.005 [-100.000, 25.190], mean action: 1.614 [0.000, 3.000],  loss: 22.120867, mse: 7080.140105, mean_q: 71.410561, mean_eps: 0.766374
  78024/300000: episode: 781, duration: 0.543s, episode steps:  78, steps per second: 144, episode reward: -111.935, mean reward: -1.435 [-100.000,  5.277], mean action: 1.705 [0.000, 3.000],  loss: 15.197014, mse: 7175.951429, mean_q: 70.629386, mean_eps: 0.766047
  78119/300000: episode: 782, duration: 0.686s, episode steps:  95, steps per second: 138, episode reward: -55.765, mean reward: -0.587 [-100.000, 20.414], mean action: 1.537 [0.000, 3.000],  loss: 11.236830, mse: 7088.694192, mean_q: 71.311170, mean_eps: 0.765787
  78272/300000: episode: 783, duration: 1.056s, episode steps: 153, steps per second: 145, episode reward: -11.747, mean reward: -0.077 [-100.000, 14.297], mean action: 1.791 [0.000, 3.000],  loss: 21.931089, mse: 7176.190248, mean_q: 70.514713, mean_eps: 0.765415
  78382/300000: episode: 784, duration: 0.793s, episode steps: 110, steps per second: 139, episode reward: -79.834, mean reward: -0.726 [-100.000, 13.063], mean action: 1.555 [0.000, 3.000],  loss: 15.975466, mse: 7150.293426, mean_q: 71.868570, mean_eps: 0.765021
  78500/300000: episode: 785, duration: 0.805s, episode steps: 118, steps per second: 147, episode reward: -93.748, mean reward: -0.794 [-100.000, 24.644], mean action: 1.458 [0.000, 3.000],  loss: 17.677510, mse: 7113.006542, mean_q: 71.091191, mean_eps: 0.764678
  78583/300000: episode: 786, duration: 0.564s, episode steps:  83, steps per second: 147, episode reward: -80.181, mean reward: -0.966 [-100.000,  7.761], mean action: 1.663 [0.000, 3.000],  loss: 20.232653, mse: 7082.639013, mean_q: 70.631690, mean_eps: 0.764377
  78661/300000: episode: 787, duration: 0.544s, episode steps:  78, steps per second: 143, episode reward: -107.281, mean reward: -1.375 [-100.000, 10.703], mean action: 1.641 [0.000, 3.000],  loss: 20.113002, mse: 6926.897749, mean_q: 70.833006, mean_eps: 0.764136
  78750/300000: episode: 788, duration: 0.621s, episode steps:  89, steps per second: 143, episode reward: -46.369, mean reward: -0.521 [-100.000, 10.727], mean action: 1.494 [0.000, 3.000],  loss: 12.714244, mse: 7016.975586, mean_q: 70.991856, mean_eps: 0.763885
  78849/300000: episode: 789, duration: 0.657s, episode steps:  99, steps per second: 151, episode reward: -130.178, mean reward: -1.315 [-100.000,  7.292], mean action: 1.798 [0.000, 3.000],  loss: 16.443099, mse: 6864.592645, mean_q: 70.072003, mean_eps: 0.763603
  78936/300000: episode: 790, duration: 0.586s, episode steps:  87, steps per second: 148, episode reward: -53.323, mean reward: -0.613 [-100.000, 11.471], mean action: 1.494 [0.000, 3.000],  loss: 13.829097, mse: 7145.045814, mean_q: 71.027007, mean_eps: 0.763324
  79039/300000: episode: 791, duration: 0.762s, episode steps: 103, steps per second: 135, episode reward: -78.916, mean reward: -0.766 [-100.000,  7.070], mean action: 1.680 [0.000, 3.000],  loss: 19.245304, mse: 7006.032497, mean_q: 72.163821, mean_eps: 0.763039
  79128/300000: episode: 792, duration: 0.630s, episode steps:  89, steps per second: 141, episode reward: -52.842, mean reward: -0.594 [-100.000, 11.478], mean action: 1.539 [0.000, 3.000],  loss: 22.535032, mse: 6925.161605, mean_q: 70.131588, mean_eps: 0.762751
  79225/300000: episode: 793, duration: 0.671s, episode steps:  97, steps per second: 145, episode reward: -60.853, mean reward: -0.627 [-100.000, 13.563], mean action: 1.680 [0.000, 3.000],  loss: 21.614269, mse: 6775.190898, mean_q: 67.717146, mean_eps: 0.762472
  79319/300000: episode: 794, duration: 0.703s, episode steps:  94, steps per second: 134, episode reward: -139.116, mean reward: -1.480 [-100.000,  7.896], mean action: 1.543 [0.000, 3.000],  loss: 18.412063, mse: 6779.502088, mean_q: 68.375690, mean_eps: 0.762185
  79457/300000: episode: 795, duration: 0.946s, episode steps: 138, steps per second: 146, episode reward: -46.646, mean reward: -0.338 [-100.000, 11.373], mean action: 1.543 [0.000, 3.000],  loss: 16.625402, mse: 6851.694255, mean_q: 68.117026, mean_eps: 0.761837
  79550/300000: episode: 796, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -110.227, mean reward: -1.185 [-100.000,  8.831], mean action: 1.505 [0.000, 3.000],  loss: 27.358375, mse: 6940.651509, mean_q: 70.754068, mean_eps: 0.761491
  79669/300000: episode: 797, duration: 0.886s, episode steps: 119, steps per second: 134, episode reward: 15.335, mean reward:  0.129 [-100.000, 53.642], mean action: 1.571 [0.000, 3.000],  loss: 15.279512, mse: 6856.957216, mean_q: 69.529713, mean_eps: 0.761173
  79800/300000: episode: 798, duration: 0.937s, episode steps: 131, steps per second: 140, episode reward: -130.203, mean reward: -0.994 [-100.000,  8.815], mean action: 1.710 [0.000, 3.000],  loss: 30.157831, mse: 6782.031213, mean_q: 70.517247, mean_eps: 0.760798
  79873/300000: episode: 799, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -62.564, mean reward: -0.857 [-100.000,  6.432], mean action: 1.370 [0.000, 3.000],  loss: 20.102188, mse: 6539.058085, mean_q: 70.084663, mean_eps: 0.760492
  79991/300000: episode: 800, duration: 0.836s, episode steps: 118, steps per second: 141, episode reward: -140.042, mean reward: -1.187 [-100.000, 18.054], mean action: 1.797 [0.000, 3.000],  loss: 25.699260, mse: 6454.318223, mean_q: 67.472281, mean_eps: 0.760206
  80059/300000: episode: 801, duration: 0.466s, episode steps:  68, steps per second: 146, episode reward: -70.322, mean reward: -1.034 [-100.000,  7.674], mean action: 1.662 [0.000, 3.000],  loss: 32.542516, mse: 6426.974983, mean_q: 67.365034, mean_eps: 0.759927
  80189/300000: episode: 802, duration: 0.921s, episode steps: 130, steps per second: 141, episode reward: -111.142, mean reward: -0.855 [-100.000, 11.327], mean action: 1.523 [0.000, 3.000],  loss: 14.069448, mse: 6510.189863, mean_q: 69.515255, mean_eps: 0.759629
  80283/300000: episode: 803, duration: 0.677s, episode steps:  94, steps per second: 139, episode reward: -71.756, mean reward: -0.763 [-100.000, 14.939], mean action: 1.755 [0.000, 3.000],  loss: 13.004867, mse: 6377.367894, mean_q: 68.294819, mean_eps: 0.759293
  80360/300000: episode: 804, duration: 0.526s, episode steps:  77, steps per second: 146, episode reward: -61.654, mean reward: -0.801 [-100.000, 11.850], mean action: 1.455 [0.000, 3.000],  loss: 13.904245, mse: 6528.172097, mean_q: 69.324934, mean_eps: 0.759037
  80448/300000: episode: 805, duration: 0.598s, episode steps:  88, steps per second: 147, episode reward: -63.753, mean reward: -0.724 [-100.000,  6.889], mean action: 1.580 [0.000, 3.000],  loss: 43.451042, mse: 6418.451649, mean_q: 68.169543, mean_eps: 0.758790
  80556/300000: episode: 806, duration: 0.775s, episode steps: 108, steps per second: 139, episode reward: -142.462, mean reward: -1.319 [-100.000,  3.604], mean action: 1.639 [0.000, 3.000],  loss: 39.753834, mse: 6389.633378, mean_q: 67.413963, mean_eps: 0.758495
  80655/300000: episode: 807, duration: 0.721s, episode steps:  99, steps per second: 137, episode reward: -11.649, mean reward: -0.118 [-100.000, 19.662], mean action: 1.707 [0.000, 3.000],  loss: 20.860797, mse: 6360.708585, mean_q: 68.485955, mean_eps: 0.758185
  80735/300000: episode: 808, duration: 0.596s, episode steps:  80, steps per second: 134, episode reward: -111.165, mean reward: -1.390 [-100.000,  9.027], mean action: 1.738 [0.000, 3.000],  loss: 34.937719, mse: 6323.988568, mean_q: 67.147704, mean_eps: 0.757916
  80827/300000: episode: 809, duration: 0.683s, episode steps:  92, steps per second: 135, episode reward: -44.928, mean reward: -0.488 [-100.000,  7.244], mean action: 1.870 [0.000, 3.000],  loss: 21.244031, mse: 6370.071507, mean_q: 68.544708, mean_eps: 0.757658
  80895/300000: episode: 810, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -76.983, mean reward: -1.132 [-100.000, 22.451], mean action: 1.559 [0.000, 3.000],  loss: 30.154298, mse: 6248.803302, mean_q: 67.001803, mean_eps: 0.757418
  80959/300000: episode: 811, duration: 0.464s, episode steps:  64, steps per second: 138, episode reward: -45.361, mean reward: -0.709 [-100.000, 17.768], mean action: 1.406 [0.000, 3.000],  loss: 15.802261, mse: 6389.179283, mean_q: 68.238451, mean_eps: 0.757220
  81075/300000: episode: 812, duration: 0.791s, episode steps: 116, steps per second: 147, episode reward: -84.396, mean reward: -0.728 [-100.000, 10.288], mean action: 1.664 [0.000, 3.000],  loss: 30.413204, mse: 6235.382442, mean_q: 67.233581, mean_eps: 0.756950
  81210/300000: episode: 813, duration: 0.984s, episode steps: 135, steps per second: 137, episode reward: -80.428, mean reward: -0.596 [-100.000, 12.358], mean action: 1.644 [0.000, 3.000],  loss: 16.105900, mse: 6175.753859, mean_q: 66.598021, mean_eps: 0.756574
  81316/300000: episode: 814, duration: 0.744s, episode steps: 106, steps per second: 143, episode reward: -100.335, mean reward: -0.947 [-100.000,  9.956], mean action: 1.528 [0.000, 3.000],  loss: 26.648749, mse: 6272.751225, mean_q: 68.169491, mean_eps: 0.756212
  81405/300000: episode: 815, duration: 0.668s, episode steps:  89, steps per second: 133, episode reward: -11.998, mean reward: -0.135 [-100.000, 13.367], mean action: 1.607 [0.000, 3.000],  loss: 17.562822, mse: 6322.960998, mean_q: 68.969199, mean_eps: 0.755920
  81580/300000: episode: 816, duration: 1.229s, episode steps: 175, steps per second: 142, episode reward: -57.699, mean reward: -0.330 [-100.000, 16.698], mean action: 1.703 [0.000, 3.000],  loss: 16.618424, mse: 6269.602294, mean_q: 67.610874, mean_eps: 0.755524
  81698/300000: episode: 817, duration: 0.806s, episode steps: 118, steps per second: 146, episode reward: -71.590, mean reward: -0.607 [-100.000, 11.219], mean action: 1.636 [0.000, 3.000],  loss: 36.000761, mse: 6342.341060, mean_q: 68.127542, mean_eps: 0.755084
  81841/300000: episode: 818, duration: 1.033s, episode steps: 143, steps per second: 138, episode reward: 43.663, mean reward:  0.305 [-100.000, 71.367], mean action: 1.517 [0.000, 3.000],  loss: 18.575401, mse: 6458.485345, mean_q: 69.847278, mean_eps: 0.754693
  81946/300000: episode: 819, duration: 0.734s, episode steps: 105, steps per second: 143, episode reward: -53.812, mean reward: -0.512 [-100.000, 25.516], mean action: 1.581 [0.000, 3.000],  loss: 15.233803, mse: 6415.699488, mean_q: 70.750886, mean_eps: 0.754321
  82088/300000: episode: 820, duration: 1.124s, episode steps: 142, steps per second: 126, episode reward: -41.716, mean reward: -0.294 [-100.000,  6.806], mean action: 1.613 [0.000, 3.000],  loss: 17.303648, mse: 6456.642805, mean_q: 70.301021, mean_eps: 0.753951
  82214/300000: episode: 821, duration: 1.031s, episode steps: 126, steps per second: 122, episode reward: -48.489, mean reward: -0.385 [-100.000, 18.150], mean action: 1.516 [0.000, 3.000],  loss: 16.640354, mse: 6387.925929, mean_q: 69.431203, mean_eps: 0.753549
  82327/300000: episode: 822, duration: 0.898s, episode steps: 113, steps per second: 126, episode reward: -50.917, mean reward: -0.451 [-100.000, 17.247], mean action: 1.407 [0.000, 3.000],  loss: 20.695704, mse: 6415.816830, mean_q: 69.765376, mean_eps: 0.753190
  82445/300000: episode: 823, duration: 0.864s, episode steps: 118, steps per second: 137, episode reward: -36.632, mean reward: -0.310 [-100.000, 17.079], mean action: 1.686 [0.000, 3.000],  loss: 21.517092, mse: 6242.063981, mean_q: 68.567872, mean_eps: 0.752844
  82561/300000: episode: 824, duration: 0.949s, episode steps: 116, steps per second: 122, episode reward: -46.439, mean reward: -0.400 [-100.000, 15.198], mean action: 1.534 [0.000, 3.000],  loss: 26.194725, mse: 6205.372083, mean_q: 68.407661, mean_eps: 0.752493
  82650/300000: episode: 825, duration: 0.721s, episode steps:  89, steps per second: 124, episode reward: -98.841, mean reward: -1.111 [-100.000,  9.723], mean action: 1.607 [0.000, 3.000],  loss: 35.722015, mse: 6293.748261, mean_q: 70.490211, mean_eps: 0.752185
  82756/300000: episode: 826, duration: 0.788s, episode steps: 106, steps per second: 134, episode reward: -62.358, mean reward: -0.588 [-100.000, 19.350], mean action: 1.726 [0.000, 3.000],  loss: 23.208956, mse: 6059.868758, mean_q: 67.701877, mean_eps: 0.751893
  82855/300000: episode: 827, duration: 0.728s, episode steps:  99, steps per second: 136, episode reward: -83.540, mean reward: -0.844 [-100.000, 12.528], mean action: 1.576 [0.000, 3.000],  loss: 18.032729, mse: 6201.954156, mean_q: 67.841443, mean_eps: 0.751585
  82960/300000: episode: 828, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward: -20.852, mean reward: -0.199 [-100.000,  9.304], mean action: 1.686 [0.000, 3.000],  loss: 39.367193, mse: 6055.618834, mean_q: 67.505141, mean_eps: 0.751279
  83100/300000: episode: 829, duration: 0.945s, episode steps: 140, steps per second: 148, episode reward: -91.584, mean reward: -0.654 [-100.000, 17.962], mean action: 1.457 [0.000, 3.000],  loss: 32.859002, mse: 6045.003523, mean_q: 68.021035, mean_eps: 0.750912
  83194/300000: episode: 830, duration: 0.682s, episode steps:  94, steps per second: 138, episode reward: -79.502, mean reward: -0.846 [-100.000, 11.155], mean action: 1.532 [0.000, 3.000],  loss: 41.402189, mse: 6346.672883, mean_q: 70.353505, mean_eps: 0.750560
  83331/300000: episode: 831, duration: 0.939s, episode steps: 137, steps per second: 146, episode reward: -45.334, mean reward: -0.331 [-100.000,  6.374], mean action: 1.496 [0.000, 3.000],  loss: 24.456174, mse: 6157.909686, mean_q: 68.019041, mean_eps: 0.750214
  83414/300000: episode: 832, duration: 0.565s, episode steps:  83, steps per second: 147, episode reward: -40.004, mean reward: -0.482 [-100.000, 17.793], mean action: 1.627 [0.000, 3.000],  loss: 22.031631, mse: 6068.428034, mean_q: 67.390353, mean_eps: 0.749884
  83535/300000: episode: 833, duration: 1.067s, episode steps: 121, steps per second: 113, episode reward: 34.597, mean reward:  0.286 [-100.000, 55.992], mean action: 1.777 [0.000, 3.000],  loss: 18.196108, mse: 5998.901149, mean_q: 68.081334, mean_eps: 0.749578
  83692/300000: episode: 834, duration: 1.304s, episode steps: 157, steps per second: 120, episode reward: -83.831, mean reward: -0.534 [-100.000, 15.574], mean action: 1.554 [0.000, 3.000],  loss: 37.689843, mse: 6095.431348, mean_q: 68.643454, mean_eps: 0.749161
  83851/300000: episode: 835, duration: 1.301s, episode steps: 159, steps per second: 122, episode reward: -84.092, mean reward: -0.529 [-100.000, 55.187], mean action: 1.553 [0.000, 3.000],  loss: 39.545207, mse: 6393.444803, mean_q: 70.774392, mean_eps: 0.748687
  83954/300000: episode: 836, duration: 0.719s, episode steps: 103, steps per second: 143, episode reward: -86.087, mean reward: -0.836 [-100.000,  8.246], mean action: 1.544 [0.000, 3.000],  loss: 23.139451, mse: 6534.646579, mean_q: 71.569127, mean_eps: 0.748294
  84031/300000: episode: 837, duration: 0.571s, episode steps:  77, steps per second: 135, episode reward: -106.299, mean reward: -1.381 [-100.000,  8.509], mean action: 1.597 [0.000, 3.000],  loss: 15.452295, mse: 6644.657265, mean_q: 72.038282, mean_eps: 0.748024
  84107/300000: episode: 838, duration: 0.532s, episode steps:  76, steps per second: 143, episode reward: -57.683, mean reward: -0.759 [-100.000,  7.961], mean action: 1.434 [0.000, 3.000],  loss: 18.939282, mse: 6549.824508, mean_q: 71.416230, mean_eps: 0.747794
  84198/300000: episode: 839, duration: 0.612s, episode steps:  91, steps per second: 149, episode reward: -83.479, mean reward: -0.917 [-100.000,  6.617], mean action: 1.659 [0.000, 3.000],  loss: 29.628996, mse: 6589.731182, mean_q: 72.387344, mean_eps: 0.747544
  84275/300000: episode: 840, duration: 0.527s, episode steps:  77, steps per second: 146, episode reward: -70.623, mean reward: -0.917 [-100.000,  6.605], mean action: 1.545 [0.000, 3.000],  loss: 40.033155, mse: 6725.341328, mean_q: 72.942299, mean_eps: 0.747292
  84383/300000: episode: 841, duration: 0.774s, episode steps: 108, steps per second: 140, episode reward: -195.781, mean reward: -1.813 [-100.000,  8.070], mean action: 1.500 [0.000, 3.000],  loss: 20.065887, mse: 6866.252776, mean_q: 73.745054, mean_eps: 0.747015
  84472/300000: episode: 842, duration: 0.601s, episode steps:  89, steps per second: 148, episode reward: -65.152, mean reward: -0.732 [-100.000, 11.535], mean action: 1.798 [0.000, 3.000],  loss: 11.926772, mse: 6893.424075, mean_q: 72.873667, mean_eps: 0.746719
  84572/300000: episode: 843, duration: 0.669s, episode steps: 100, steps per second: 150, episode reward: -104.007, mean reward: -1.040 [-100.000, 20.721], mean action: 1.720 [0.000, 3.000],  loss: 13.896501, mse: 6875.516069, mean_q: 72.158802, mean_eps: 0.746435
  84662/300000: episode: 844, duration: 0.652s, episode steps:  90, steps per second: 138, episode reward: -60.089, mean reward: -0.668 [-100.000, 11.606], mean action: 1.544 [0.000, 3.000],  loss: 14.063097, mse: 6825.381576, mean_q: 72.363687, mean_eps: 0.746150
  84795/300000: episode: 845, duration: 0.945s, episode steps: 133, steps per second: 141, episode reward: -7.387, mean reward: -0.056 [-100.000, 16.210], mean action: 1.579 [0.000, 3.000],  loss: 29.569934, mse: 6880.100946, mean_q: 72.662615, mean_eps: 0.745816
  84886/300000: episode: 846, duration: 0.605s, episode steps:  91, steps per second: 150, episode reward: -78.901, mean reward: -0.867 [-100.000, 17.847], mean action: 1.484 [0.000, 3.000],  loss: 14.291388, mse: 6769.909035, mean_q: 71.671532, mean_eps: 0.745480
  84962/300000: episode: 847, duration: 0.542s, episode steps:  76, steps per second: 140, episode reward: -158.994, mean reward: -2.092 [-100.000,  9.913], mean action: 1.592 [0.000, 3.000],  loss: 25.698450, mse: 6995.506656, mean_q: 73.437844, mean_eps: 0.745229
  85092/300000: episode: 848, duration: 0.884s, episode steps: 130, steps per second: 147, episode reward: -86.110, mean reward: -0.662 [-100.000,  9.124], mean action: 1.654 [0.000, 3.000],  loss: 11.513927, mse: 6862.804965, mean_q: 72.202939, mean_eps: 0.744920
  85196/300000: episode: 849, duration: 0.706s, episode steps: 104, steps per second: 147, episode reward: -57.522, mean reward: -0.553 [-100.000, 18.732], mean action: 1.615 [0.000, 3.000],  loss: 32.643563, mse: 7058.025837, mean_q: 73.644904, mean_eps: 0.744570
  85348/300000: episode: 850, duration: 1.056s, episode steps: 152, steps per second: 144, episode reward: -46.027, mean reward: -0.303 [-100.000, 15.860], mean action: 1.612 [0.000, 3.000],  loss: 15.892320, mse: 7189.569750, mean_q: 74.855916, mean_eps: 0.744186
  85420/300000: episode: 851, duration: 0.512s, episode steps:  72, steps per second: 141, episode reward: -76.659, mean reward: -1.065 [-100.000, 10.395], mean action: 1.694 [0.000, 3.000],  loss: 28.637062, mse: 7232.173279, mean_q: 74.110033, mean_eps: 0.743850
  85536/300000: episode: 852, duration: 0.817s, episode steps: 116, steps per second: 142, episode reward: -47.882, mean reward: -0.413 [-100.000, 18.828], mean action: 1.638 [0.000, 3.000],  loss: 13.588115, mse: 7359.791083, mean_q: 75.270140, mean_eps: 0.743568
  85648/300000: episode: 853, duration: 0.778s, episode steps: 112, steps per second: 144, episode reward: -119.296, mean reward: -1.065 [-100.000, 13.253], mean action: 1.661 [0.000, 3.000],  loss: 18.074206, mse: 7241.770569, mean_q: 74.713040, mean_eps: 0.743226
  85762/300000: episode: 854, duration: 0.772s, episode steps: 114, steps per second: 148, episode reward: -36.441, mean reward: -0.320 [-100.000, 11.674], mean action: 1.430 [0.000, 3.000],  loss: 29.023626, mse: 7458.209751, mean_q: 75.948203, mean_eps: 0.742887
  85867/300000: episode: 855, duration: 0.766s, episode steps: 105, steps per second: 137, episode reward: -76.383, mean reward: -0.727 [-100.000,  7.112], mean action: 1.524 [0.000, 3.000],  loss: 31.437845, mse: 7368.986044, mean_q: 75.144304, mean_eps: 0.742558
  85960/300000: episode: 856, duration: 0.641s, episode steps:  93, steps per second: 145, episode reward: -113.645, mean reward: -1.222 [-100.000,  6.632], mean action: 1.591 [0.000, 3.000],  loss: 21.710287, mse: 7353.509424, mean_q: 75.495796, mean_eps: 0.742261
  86096/300000: episode: 857, duration: 0.922s, episode steps: 136, steps per second: 147, episode reward: -81.436, mean reward: -0.599 [-100.000, 10.914], mean action: 1.625 [0.000, 3.000],  loss: 18.064961, mse: 7428.308985, mean_q: 76.017981, mean_eps: 0.741918
  86197/300000: episode: 858, duration: 0.692s, episode steps: 101, steps per second: 146, episode reward: -88.699, mean reward: -0.878 [-100.000, 13.540], mean action: 1.495 [0.000, 3.000],  loss: 18.880176, mse: 7345.257044, mean_q: 74.821185, mean_eps: 0.741562
  86282/300000: episode: 859, duration: 0.592s, episode steps:  85, steps per second: 144, episode reward: -34.306, mean reward: -0.404 [-100.000, 17.836], mean action: 1.765 [0.000, 3.000],  loss: 11.344274, mse: 7315.451166, mean_q: 75.896301, mean_eps: 0.741283
  86352/300000: episode: 860, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -76.406, mean reward: -1.092 [-100.000, 10.049], mean action: 1.371 [0.000, 3.000],  loss: 16.137412, mse: 7385.013421, mean_q: 76.712544, mean_eps: 0.741050
  86451/300000: episode: 861, duration: 0.672s, episode steps:  99, steps per second: 147, episode reward: -73.871, mean reward: -0.746 [-100.000, 16.531], mean action: 1.556 [0.000, 3.000],  loss: 11.493022, mse: 7403.628610, mean_q: 74.714739, mean_eps: 0.740797
  86539/300000: episode: 862, duration: 0.626s, episode steps:  88, steps per second: 141, episode reward: -54.275, mean reward: -0.617 [-100.000,  6.941], mean action: 1.670 [0.000, 3.000],  loss: 13.626208, mse: 7404.613309, mean_q: 74.354504, mean_eps: 0.740517
  86676/300000: episode: 863, duration: 0.933s, episode steps: 137, steps per second: 147, episode reward: -44.266, mean reward: -0.323 [-100.000, 13.899], mean action: 1.591 [0.000, 3.000],  loss: 26.205803, mse: 7356.559845, mean_q: 73.940436, mean_eps: 0.740179
  86834/300000: episode: 864, duration: 1.109s, episode steps: 158, steps per second: 143, episode reward: -195.302, mean reward: -1.236 [-100.000, 70.990], mean action: 1.658 [0.000, 3.000],  loss: 21.853327, mse: 7496.211701, mean_q: 75.242145, mean_eps: 0.739737
  86993/300000: episode: 865, duration: 1.390s, episode steps: 159, steps per second: 114, episode reward: -121.731, mean reward: -0.766 [-100.000,  6.564], mean action: 1.711 [0.000, 3.000],  loss: 19.780279, mse: 7402.846185, mean_q: 74.541449, mean_eps: 0.739261
  87105/300000: episode: 866, duration: 0.898s, episode steps: 112, steps per second: 125, episode reward: -118.993, mean reward: -1.062 [-100.000, 10.114], mean action: 1.482 [0.000, 3.000],  loss: 12.999825, mse: 7480.190792, mean_q: 74.840096, mean_eps: 0.738855
  87235/300000: episode: 867, duration: 0.882s, episode steps: 130, steps per second: 147, episode reward: -170.655, mean reward: -1.313 [-100.000,  4.731], mean action: 1.577 [0.000, 3.000],  loss: 18.167390, mse: 7633.319373, mean_q: 75.624959, mean_eps: 0.738491
  87369/300000: episode: 868, duration: 0.923s, episode steps: 134, steps per second: 145, episode reward: -80.507, mean reward: -0.601 [-100.000,  8.493], mean action: 1.537 [0.000, 3.000],  loss: 23.316315, mse: 7746.968458, mean_q: 77.322388, mean_eps: 0.738096
  87441/300000: episode: 869, duration: 0.510s, episode steps:  72, steps per second: 141, episode reward: -65.785, mean reward: -0.914 [-100.000, 12.175], mean action: 1.778 [0.000, 3.000],  loss: 13.825954, mse: 7892.126621, mean_q: 78.103849, mean_eps: 0.737786
  87520/300000: episode: 870, duration: 0.534s, episode steps:  79, steps per second: 148, episode reward: -24.581, mean reward: -0.311 [-100.000, 19.285], mean action: 1.785 [0.000, 3.000],  loss: 12.965899, mse: 7938.142658, mean_q: 79.192288, mean_eps: 0.737560
  87619/300000: episode: 871, duration: 0.713s, episode steps:  99, steps per second: 139, episode reward: -63.887, mean reward: -0.645 [-100.000, 11.292], mean action: 1.626 [0.000, 3.000],  loss: 20.564036, mse: 7842.579590, mean_q: 78.690050, mean_eps: 0.737293
  87723/300000: episode: 872, duration: 0.761s, episode steps: 104, steps per second: 137, episode reward: -85.252, mean reward: -0.820 [-100.000,  7.493], mean action: 1.673 [0.000, 3.000],  loss: 16.424516, mse: 7892.863680, mean_q: 78.401957, mean_eps: 0.736988
  87807/300000: episode: 873, duration: 0.573s, episode steps:  84, steps per second: 147, episode reward: -113.902, mean reward: -1.356 [-100.000,  8.169], mean action: 1.524 [0.000, 3.000],  loss: 21.403249, mse: 7844.340832, mean_q: 77.615927, mean_eps: 0.736706
  87923/300000: episode: 874, duration: 0.776s, episode steps: 116, steps per second: 149, episode reward: -50.925, mean reward: -0.439 [-100.000, 16.583], mean action: 1.759 [0.000, 3.000],  loss: 20.700792, mse: 7830.338846, mean_q: 77.147887, mean_eps: 0.736406
  88051/300000: episode: 875, duration: 0.903s, episode steps: 128, steps per second: 142, episode reward: -274.936, mean reward: -2.148 [-100.000, 81.503], mean action: 1.750 [0.000, 3.000],  loss: 26.017555, mse: 7998.831440, mean_q: 77.675846, mean_eps: 0.736040
  88144/300000: episode: 876, duration: 0.644s, episode steps:  93, steps per second: 144, episode reward: -10.227, mean reward: -0.110 [-100.000, 16.560], mean action: 1.763 [0.000, 3.000],  loss: 24.004107, mse: 8186.970955, mean_q: 78.258942, mean_eps: 0.735709
  88288/300000: episode: 877, duration: 1.012s, episode steps: 144, steps per second: 142, episode reward: -85.717, mean reward: -0.595 [-100.000,  7.352], mean action: 1.535 [0.000, 3.000],  loss: 15.520378, mse: 8119.565277, mean_q: 78.269359, mean_eps: 0.735353
  88397/300000: episode: 878, duration: 0.753s, episode steps: 109, steps per second: 145, episode reward: -98.528, mean reward: -0.904 [-100.000,  7.354], mean action: 1.450 [0.000, 3.000],  loss: 30.685919, mse: 7945.617716, mean_q: 77.881482, mean_eps: 0.734974
  88506/300000: episode: 879, duration: 0.732s, episode steps: 109, steps per second: 149, episode reward: -78.763, mean reward: -0.723 [-100.000, 19.561], mean action: 1.376 [0.000, 3.000],  loss: 33.923670, mse: 8043.029933, mean_q: 77.933927, mean_eps: 0.734647
  88609/300000: episode: 880, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -95.782, mean reward: -0.930 [-100.000,  6.672], mean action: 1.398 [0.000, 3.000],  loss: 20.602527, mse: 8209.882063, mean_q: 80.270755, mean_eps: 0.734329
  88694/300000: episode: 881, duration: 0.590s, episode steps:  85, steps per second: 144, episode reward: -91.741, mean reward: -1.079 [-100.000, 12.128], mean action: 1.471 [0.000, 3.000],  loss: 13.713622, mse: 8235.571197, mean_q: 79.938831, mean_eps: 0.734047
  88771/300000: episode: 882, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -91.074, mean reward: -1.183 [-100.000,  8.228], mean action: 1.494 [0.000, 3.000],  loss: 24.029240, mse: 8335.701628, mean_q: 81.867217, mean_eps: 0.733804
  88892/300000: episode: 883, duration: 0.808s, episode steps: 121, steps per second: 150, episode reward: -201.182, mean reward: -1.663 [-100.000, 59.294], mean action: 1.645 [0.000, 3.000],  loss: 16.842946, mse: 8205.986453, mean_q: 79.458539, mean_eps: 0.733507
  89011/300000: episode: 884, duration: 0.842s, episode steps: 119, steps per second: 141, episode reward: -92.892, mean reward: -0.781 [-100.000,  9.305], mean action: 1.588 [0.000, 3.000],  loss: 33.984545, mse: 8149.188518, mean_q: 79.944466, mean_eps: 0.733147
  89129/300000: episode: 885, duration: 0.786s, episode steps: 118, steps per second: 150, episode reward: -47.829, mean reward: -0.405 [-100.000,  7.640], mean action: 1.483 [0.000, 3.000],  loss: 24.389346, mse: 8140.957954, mean_q: 79.867598, mean_eps: 0.732792
  89260/300000: episode: 886, duration: 0.919s, episode steps: 131, steps per second: 143, episode reward: -149.373, mean reward: -1.140 [-100.000,  4.098], mean action: 1.359 [0.000, 3.000],  loss: 20.809397, mse: 8174.163399, mean_q: 81.290101, mean_eps: 0.732418
  89408/300000: episode: 887, duration: 1.019s, episode steps: 148, steps per second: 145, episode reward: -12.580, mean reward: -0.085 [-100.000, 13.834], mean action: 1.655 [0.000, 3.000],  loss: 18.333277, mse: 8247.310831, mean_q: 81.239052, mean_eps: 0.732000
  89498/300000: episode: 888, duration: 0.606s, episode steps:  90, steps per second: 149, episode reward: -104.513, mean reward: -1.161 [-100.000,  9.994], mean action: 1.744 [0.000, 3.000],  loss: 27.656813, mse: 8276.750890, mean_q: 80.770007, mean_eps: 0.731642
  89573/300000: episode: 889, duration: 0.551s, episode steps:  75, steps per second: 136, episode reward: -69.369, mean reward: -0.925 [-100.000,  6.899], mean action: 1.827 [0.000, 3.000],  loss: 13.465788, mse: 8287.843262, mean_q: 80.360417, mean_eps: 0.731395
  89653/300000: episode: 890, duration: 0.556s, episode steps:  80, steps per second: 144, episode reward: -44.724, mean reward: -0.559 [-100.000, 18.023], mean action: 1.550 [0.000, 3.000],  loss: 24.319008, mse: 8234.733917, mean_q: 80.442105, mean_eps: 0.731162
  89779/300000: episode: 891, duration: 0.844s, episode steps: 126, steps per second: 149, episode reward: -123.151, mean reward: -0.977 [-100.000,  8.011], mean action: 1.579 [0.000, 3.000],  loss: 17.090116, mse: 8147.684326, mean_q: 80.570052, mean_eps: 0.730854
  89913/300000: episode: 892, duration: 0.937s, episode steps: 134, steps per second: 143, episode reward: -123.623, mean reward: -0.923 [-100.000,  6.349], mean action: 1.530 [0.000, 3.000],  loss: 26.499619, mse: 8240.748750, mean_q: 80.753524, mean_eps: 0.730463
  90003/300000: episode: 893, duration: 0.682s, episode steps:  90, steps per second: 132, episode reward: -113.391, mean reward: -1.260 [-100.000,  9.976], mean action: 1.567 [0.000, 3.000],  loss: 15.789422, mse: 8225.218500, mean_q: 80.951035, mean_eps: 0.730128
  90132/300000: episode: 894, duration: 0.918s, episode steps: 129, steps per second: 140, episode reward: -63.748, mean reward: -0.494 [-100.000, 11.749], mean action: 1.744 [0.000, 3.000],  loss: 20.375386, mse: 8337.588564, mean_q: 82.830021, mean_eps: 0.729799
  90262/300000: episode: 895, duration: 0.919s, episode steps: 130, steps per second: 141, episode reward: -92.741, mean reward: -0.713 [-100.000,  7.852], mean action: 1.677 [0.000, 3.000],  loss: 17.919123, mse: 8390.733613, mean_q: 82.149265, mean_eps: 0.729410
  90341/300000: episode: 896, duration: 0.557s, episode steps:  79, steps per second: 142, episode reward: -35.020, mean reward: -0.443 [-100.000, 12.377], mean action: 1.797 [0.000, 3.000],  loss: 42.233929, mse: 8459.051745, mean_q: 83.324136, mean_eps: 0.729097
  90425/300000: episode: 897, duration: 0.571s, episode steps:  84, steps per second: 147, episode reward: -27.062, mean reward: -0.322 [-100.000, 13.455], mean action: 1.607 [0.000, 3.000],  loss: 12.052353, mse: 8851.709031, mean_q: 85.865521, mean_eps: 0.728853
  90530/300000: episode: 898, duration: 0.781s, episode steps: 105, steps per second: 134, episode reward: -101.781, mean reward: -0.969 [-100.000,  9.353], mean action: 1.619 [0.000, 3.000],  loss: 22.515032, mse: 8328.678181, mean_q: 82.187055, mean_eps: 0.728569
  90686/300000: episode: 899, duration: 1.108s, episode steps: 156, steps per second: 141, episode reward: -224.160, mean reward: -1.437 [-100.000, 50.855], mean action: 1.538 [0.000, 3.000],  loss: 24.195099, mse: 8506.213488, mean_q: 83.754372, mean_eps: 0.728177
  90752/300000: episode: 900, duration: 0.455s, episode steps:  66, steps per second: 145, episode reward: -89.626, mean reward: -1.358 [-100.000, 10.877], mean action: 1.318 [0.000, 3.000],  loss: 20.637196, mse: 8393.175123, mean_q: 81.678510, mean_eps: 0.727845
  90854/300000: episode: 901, duration: 0.791s, episode steps: 102, steps per second: 129, episode reward: -7.486, mean reward: -0.073 [-100.000, 17.785], mean action: 1.539 [0.000, 3.000],  loss: 18.210564, mse: 8646.956203, mean_q: 83.940999, mean_eps: 0.727593
  90975/300000: episode: 902, duration: 0.859s, episode steps: 121, steps per second: 141, episode reward: -109.032, mean reward: -0.901 [-100.000, 12.973], mean action: 1.587 [0.000, 3.000],  loss: 17.301230, mse: 8628.003858, mean_q: 84.214252, mean_eps: 0.727258
  91056/300000: episode: 903, duration: 0.570s, episode steps:  81, steps per second: 142, episode reward: -54.432, mean reward: -0.672 [-100.000,  8.223], mean action: 1.654 [0.000, 3.000],  loss: 25.147889, mse: 8651.165539, mean_q: 85.144018, mean_eps: 0.726955
  91141/300000: episode: 904, duration: 0.602s, episode steps:  85, steps per second: 141, episode reward: -77.463, mean reward: -0.911 [-100.000, 12.628], mean action: 1.529 [0.000, 3.000],  loss: 28.665592, mse: 8657.045962, mean_q: 83.141521, mean_eps: 0.726706
  91255/300000: episode: 905, duration: 0.903s, episode steps: 114, steps per second: 126, episode reward: -174.926, mean reward: -1.534 [-100.000, 19.304], mean action: 1.632 [0.000, 3.000],  loss: 15.086289, mse: 8676.656143, mean_q: 84.357619, mean_eps: 0.726407
  91350/300000: episode: 906, duration: 0.780s, episode steps:  95, steps per second: 122, episode reward: -93.109, mean reward: -0.980 [-100.000,  7.561], mean action: 1.579 [0.000, 3.000],  loss: 14.830061, mse: 8354.627395, mean_q: 80.193839, mean_eps: 0.726094
  91482/300000: episode: 907, duration: 1.043s, episode steps: 132, steps per second: 127, episode reward: -53.304, mean reward: -0.404 [-100.000, 18.991], mean action: 1.652 [0.000, 3.000],  loss: 22.266973, mse: 8338.253329, mean_q: 81.199175, mean_eps: 0.725754
  91614/300000: episode: 908, duration: 1.024s, episode steps: 132, steps per second: 129, episode reward: -90.424, mean reward: -0.685 [-100.000,  9.974], mean action: 1.530 [0.000, 3.000],  loss: 17.130796, mse: 8300.891106, mean_q: 81.507367, mean_eps: 0.725357
  91685/300000: episode: 909, duration: 0.598s, episode steps:  71, steps per second: 119, episode reward: -69.518, mean reward: -0.979 [-100.000,  7.260], mean action: 1.803 [0.000, 3.000],  loss: 12.695117, mse: 8279.270948, mean_q: 80.480824, mean_eps: 0.725053
  91810/300000: episode: 910, duration: 1.054s, episode steps: 125, steps per second: 119, episode reward: -35.117, mean reward: -0.281 [-100.000, 17.299], mean action: 1.600 [0.000, 3.000],  loss: 10.088058, mse: 8152.305090, mean_q: 80.868448, mean_eps: 0.724759
  91912/300000: episode: 911, duration: 0.749s, episode steps: 102, steps per second: 136, episode reward: -123.361, mean reward: -1.209 [-100.000,  6.022], mean action: 1.637 [0.000, 3.000],  loss: 22.224030, mse: 8451.173335, mean_q: 81.696914, mean_eps: 0.724418
  92003/300000: episode: 912, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -46.996, mean reward: -0.516 [-100.000,  7.327], mean action: 1.637 [0.000, 3.000],  loss: 19.456858, mse: 8391.866780, mean_q: 82.042510, mean_eps: 0.724129
  92130/300000: episode: 913, duration: 0.846s, episode steps: 127, steps per second: 150, episode reward: -57.044, mean reward: -0.449 [-100.000, 12.045], mean action: 1.543 [0.000, 3.000],  loss: 19.714893, mse: 8283.313992, mean_q: 80.201240, mean_eps: 0.723802
  92212/300000: episode: 914, duration: 0.619s, episode steps:  82, steps per second: 132, episode reward: -90.496, mean reward: -1.104 [-100.000, 12.096], mean action: 1.561 [0.000, 3.000],  loss: 26.121440, mse: 8376.818187, mean_q: 81.014025, mean_eps: 0.723489
  92270/300000: episode: 915, duration: 0.431s, episode steps:  58, steps per second: 134, episode reward: -63.395, mean reward: -1.093 [-100.000, 11.504], mean action: 1.707 [0.000, 3.000],  loss: 24.910387, mse: 8452.085542, mean_q: 82.305615, mean_eps: 0.723279
  92363/300000: episode: 916, duration: 0.638s, episode steps:  93, steps per second: 146, episode reward: -70.754, mean reward: -0.761 [-100.000,  7.803], mean action: 1.570 [0.000, 3.000],  loss: 27.495625, mse: 8194.923245, mean_q: 79.657368, mean_eps: 0.723052
  92464/300000: episode: 917, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -49.607, mean reward: -0.491 [-100.000, 21.291], mean action: 1.535 [0.000, 3.000],  loss: 26.150473, mse: 8384.371060, mean_q: 81.453344, mean_eps: 0.722761
  92555/300000: episode: 918, duration: 0.681s, episode steps:  91, steps per second: 134, episode reward: -105.188, mean reward: -1.156 [-100.000,  3.684], mean action: 1.538 [0.000, 3.000],  loss: 17.420246, mse: 8329.639182, mean_q: 80.470362, mean_eps: 0.722473
  92644/300000: episode: 919, duration: 0.615s, episode steps:  89, steps per second: 145, episode reward: -15.546, mean reward: -0.175 [-100.000, 19.824], mean action: 1.685 [0.000, 3.000],  loss: 19.355808, mse: 8310.193162, mean_q: 80.794909, mean_eps: 0.722203
  92712/300000: episode: 920, duration: 0.515s, episode steps:  68, steps per second: 132, episode reward: -53.547, mean reward: -0.787 [-100.000, 14.167], mean action: 1.809 [0.000, 3.000],  loss: 42.788529, mse: 8407.365794, mean_q: 83.725253, mean_eps: 0.721968
  92786/300000: episode: 921, duration: 0.603s, episode steps:  74, steps per second: 123, episode reward: -99.925, mean reward: -1.350 [-100.000, 15.964], mean action: 1.662 [0.000, 3.000],  loss: 15.581729, mse: 8297.582163, mean_q: 83.988444, mean_eps: 0.721754
  92879/300000: episode: 922, duration: 0.723s, episode steps:  93, steps per second: 129, episode reward: -73.318, mean reward: -0.788 [-100.000, 20.810], mean action: 1.570 [0.000, 3.000],  loss: 16.407606, mse: 8248.561523, mean_q: 81.619927, mean_eps: 0.721504
  92950/300000: episode: 923, duration: 0.529s, episode steps:  71, steps per second: 134, episode reward: -6.901, mean reward: -0.097 [-100.000, 21.422], mean action: 1.648 [0.000, 3.000],  loss: 16.589274, mse: 8474.366204, mean_q: 84.431065, mean_eps: 0.721258
  93025/300000: episode: 924, duration: 0.555s, episode steps:  75, steps per second: 135, episode reward: -75.624, mean reward: -1.008 [-100.000,  9.695], mean action: 1.600 [0.000, 3.000],  loss: 27.791070, mse: 8233.715098, mean_q: 81.323584, mean_eps: 0.721039
  93167/300000: episode: 925, duration: 1.126s, episode steps: 142, steps per second: 126, episode reward: -43.308, mean reward: -0.305 [-100.000, 16.422], mean action: 1.458 [0.000, 3.000],  loss: 20.752023, mse: 8164.176211, mean_q: 81.739833, mean_eps: 0.720714
  93242/300000: episode: 926, duration: 0.540s, episode steps:  75, steps per second: 139, episode reward: -28.176, mean reward: -0.376 [-100.000, 22.844], mean action: 1.440 [0.000, 3.000],  loss: 18.538918, mse: 8237.334284, mean_q: 82.569203, mean_eps: 0.720388
  93303/300000: episode: 927, duration: 0.413s, episode steps:  61, steps per second: 148, episode reward: -51.959, mean reward: -0.852 [-100.000, 13.178], mean action: 1.754 [0.000, 3.000],  loss: 26.372192, mse: 8223.492380, mean_q: 82.329986, mean_eps: 0.720184
  93412/300000: episode: 928, duration: 0.774s, episode steps: 109, steps per second: 141, episode reward: -110.812, mean reward: -1.017 [-100.000, 10.801], mean action: 1.560 [0.000, 3.000],  loss: 10.977555, mse: 8516.253714, mean_q: 84.136038, mean_eps: 0.719929
  93507/300000: episode: 929, duration: 0.661s, episode steps:  95, steps per second: 144, episode reward: -33.830, mean reward: -0.356 [-100.000, 16.268], mean action: 1.611 [0.000, 3.000],  loss: 18.973927, mse: 8566.758995, mean_q: 83.311720, mean_eps: 0.719623
  93616/300000: episode: 930, duration: 0.751s, episode steps: 109, steps per second: 145, episode reward: -87.311, mean reward: -0.801 [-100.000,  5.952], mean action: 1.495 [0.000, 3.000],  loss: 20.649686, mse: 8601.619015, mean_q: 85.418654, mean_eps: 0.719317
  93717/300000: episode: 931, duration: 0.725s, episode steps: 101, steps per second: 139, episode reward: -57.335, mean reward: -0.568 [-100.000,  7.497], mean action: 1.713 [0.000, 3.000],  loss: 15.139519, mse: 8559.456572, mean_q: 84.515562, mean_eps: 0.719002
  93856/300000: episode: 932, duration: 0.969s, episode steps: 139, steps per second: 143, episode reward: -121.612, mean reward: -0.875 [-100.000, 31.811], mean action: 1.619 [0.000, 3.000],  loss: 15.915980, mse: 8580.881625, mean_q: 84.695941, mean_eps: 0.718642
  93912/300000: episode: 933, duration: 0.423s, episode steps:  56, steps per second: 133, episode reward: -84.751, mean reward: -1.513 [-100.000, 12.194], mean action: 1.393 [0.000, 3.000],  loss: 13.302254, mse: 8535.005772, mean_q: 82.684488, mean_eps: 0.718349
  94005/300000: episode: 934, duration: 0.758s, episode steps:  93, steps per second: 123, episode reward: -48.029, mean reward: -0.516 [-100.000,  7.426], mean action: 1.742 [0.000, 3.000],  loss: 37.460782, mse: 8838.209047, mean_q: 86.196416, mean_eps: 0.718126
  94119/300000: episode: 935, duration: 0.820s, episode steps: 114, steps per second: 139, episode reward: -68.046, mean reward: -0.597 [-100.000,  9.764], mean action: 1.649 [0.000, 3.000],  loss: 16.586603, mse: 8798.277879, mean_q: 85.976176, mean_eps: 0.717815
  94229/300000: episode: 936, duration: 0.784s, episode steps: 110, steps per second: 140, episode reward: -68.357, mean reward: -0.621 [-100.000, 26.957], mean action: 1.445 [0.000, 3.000],  loss: 23.056506, mse: 8810.877339, mean_q: 86.033226, mean_eps: 0.717479
  94384/300000: episode: 937, duration: 1.262s, episode steps: 155, steps per second: 123, episode reward: -100.624, mean reward: -0.649 [-100.000,  9.644], mean action: 1.594 [0.000, 3.000],  loss: 21.102411, mse: 8651.556962, mean_q: 84.106153, mean_eps: 0.717082
  94523/300000: episode: 938, duration: 1.090s, episode steps: 139, steps per second: 128, episode reward: 21.777, mean reward:  0.157 [-100.000, 74.401], mean action: 1.669 [0.000, 3.000],  loss: 14.182651, mse: 8921.363973, mean_q: 86.092872, mean_eps: 0.716641
  94624/300000: episode: 939, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: -135.359, mean reward: -1.340 [-100.000,  7.823], mean action: 1.564 [0.000, 3.000],  loss: 20.108123, mse: 9075.086112, mean_q: 88.166445, mean_eps: 0.716281
  94698/300000: episode: 940, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -46.507, mean reward: -0.628 [-100.000,  7.974], mean action: 1.581 [0.000, 3.000],  loss: 36.569641, mse: 8953.281105, mean_q: 87.237241, mean_eps: 0.716019
  94787/300000: episode: 941, duration: 0.691s, episode steps:  89, steps per second: 129, episode reward: -100.585, mean reward: -1.130 [-100.000,  7.364], mean action: 1.528 [0.000, 3.000],  loss: 17.168388, mse: 8826.608031, mean_q: 88.182743, mean_eps: 0.715774
  94893/300000: episode: 942, duration: 0.781s, episode steps: 106, steps per second: 136, episode reward: -70.333, mean reward: -0.664 [-100.000,  8.253], mean action: 1.604 [0.000, 3.000],  loss: 14.034225, mse: 8868.906614, mean_q: 86.858493, mean_eps: 0.715481
  94993/300000: episode: 943, duration: 0.677s, episode steps: 100, steps per second: 148, episode reward: -81.529, mean reward: -0.815 [-100.000, 21.578], mean action: 1.470 [0.000, 3.000],  loss: 28.820352, mse: 8909.575620, mean_q: 88.557604, mean_eps: 0.715173
  95107/300000: episode: 944, duration: 0.810s, episode steps: 114, steps per second: 141, episode reward: -64.856, mean reward: -0.569 [-100.000, 11.376], mean action: 1.693 [0.000, 3.000],  loss: 22.979874, mse: 9156.254604, mean_q: 89.713367, mean_eps: 0.714851
  95211/300000: episode: 945, duration: 0.886s, episode steps: 104, steps per second: 117, episode reward: -60.430, mean reward: -0.581 [-100.000,  7.258], mean action: 1.606 [0.000, 3.000],  loss: 23.246202, mse: 9646.582111, mean_q: 92.282414, mean_eps: 0.714525
  95281/300000: episode: 946, duration: 0.557s, episode steps:  70, steps per second: 126, episode reward: -61.765, mean reward: -0.882 [-100.000,  6.784], mean action: 1.629 [0.000, 3.000],  loss: 18.990702, mse: 9818.315374, mean_q: 92.944454, mean_eps: 0.714263
  95394/300000: episode: 947, duration: 0.844s, episode steps: 113, steps per second: 134, episode reward: -78.389, mean reward: -0.694 [-100.000, 12.155], mean action: 1.681 [0.000, 3.000],  loss: 20.309133, mse: 9872.261701, mean_q: 92.907662, mean_eps: 0.713989
  95519/300000: episode: 948, duration: 0.950s, episode steps: 125, steps per second: 132, episode reward: -195.684, mean reward: -1.565 [-100.000,  6.599], mean action: 1.528 [0.000, 3.000],  loss: 26.062127, mse: 9543.050496, mean_q: 91.196982, mean_eps: 0.713632
  95664/300000: episode: 949, duration: 1.203s, episode steps: 145, steps per second: 121, episode reward: -64.877, mean reward: -0.447 [-100.000,  9.303], mean action: 1.724 [0.000, 3.000],  loss: 25.669950, mse: 9933.675347, mean_q: 93.897976, mean_eps: 0.713227
  95758/300000: episode: 950, duration: 0.676s, episode steps:  94, steps per second: 139, episode reward: -78.349, mean reward: -0.833 [-100.000,  8.260], mean action: 1.532 [0.000, 3.000],  loss: 16.780985, mse: 10051.955385, mean_q: 93.999969, mean_eps: 0.712868
  95858/300000: episode: 951, duration: 0.671s, episode steps: 100, steps per second: 149, episode reward: -26.039, mean reward: -0.260 [-100.000, 12.762], mean action: 1.680 [0.000, 3.000],  loss: 24.337971, mse: 9790.450562, mean_q: 91.390154, mean_eps: 0.712578
  95935/300000: episode: 952, duration: 0.540s, episode steps:  77, steps per second: 143, episode reward: -47.344, mean reward: -0.615 [-100.000, 17.777], mean action: 1.532 [0.000, 3.000],  loss: 29.477561, mse: 10069.618164, mean_q: 94.402759, mean_eps: 0.712312
  96042/300000: episode: 953, duration: 0.783s, episode steps: 107, steps per second: 137, episode reward: -92.975, mean reward: -0.869 [-100.000, 11.320], mean action: 1.710 [0.000, 3.000],  loss: 16.568253, mse: 10042.156583, mean_q: 92.936108, mean_eps: 0.712036
  96114/300000: episode: 954, duration: 0.484s, episode steps:  72, steps per second: 149, episode reward: -65.200, mean reward: -0.906 [-100.000,  7.561], mean action: 1.597 [0.000, 3.000],  loss: 15.638445, mse: 9894.092712, mean_q: 91.651962, mean_eps: 0.711767
  96250/300000: episode: 955, duration: 0.925s, episode steps: 136, steps per second: 147, episode reward: -29.138, mean reward: -0.214 [-100.000, 11.339], mean action: 1.529 [0.000, 3.000],  loss: 21.918565, mse: 9855.420249, mean_q: 92.201773, mean_eps: 0.711456
  96360/300000: episode: 956, duration: 0.837s, episode steps: 110, steps per second: 131, episode reward: -154.650, mean reward: -1.406 [-100.000, 44.482], mean action: 1.809 [0.000, 3.000],  loss: 25.953405, mse: 9910.258203, mean_q: 92.187762, mean_eps: 0.711086
  96474/300000: episode: 957, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -39.445, mean reward: -0.346 [-100.000, 18.008], mean action: 1.579 [0.000, 3.000],  loss: 25.282938, mse: 9985.807048, mean_q: 91.872737, mean_eps: 0.710751
  96556/300000: episode: 958, duration: 0.571s, episode steps:  82, steps per second: 144, episode reward: -27.331, mean reward: -0.333 [-100.000, 11.686], mean action: 1.744 [0.000, 3.000],  loss: 40.945520, mse: 10030.959229, mean_q: 92.530369, mean_eps: 0.710456
  96659/300000: episode: 959, duration: 0.756s, episode steps: 103, steps per second: 136, episode reward: -93.342, mean reward: -0.906 [-100.000,  8.331], mean action: 1.485 [0.000, 3.000],  loss: 21.550402, mse: 10181.488637, mean_q: 94.197995, mean_eps: 0.710179
  96753/300000: episode: 960, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -61.490, mean reward: -0.654 [-100.000, 10.644], mean action: 1.564 [0.000, 3.000],  loss: 33.702270, mse: 10369.075039, mean_q: 95.707249, mean_eps: 0.709883
  96850/300000: episode: 961, duration: 0.669s, episode steps:  97, steps per second: 145, episode reward: -79.223, mean reward: -0.817 [-100.000,  7.960], mean action: 1.299 [0.000, 3.000],  loss: 16.793272, mse: 10395.619855, mean_q: 95.484109, mean_eps: 0.709597
  96970/300000: episode: 962, duration: 0.848s, episode steps: 120, steps per second: 141, episode reward: -41.351, mean reward: -0.345 [-100.000, 27.742], mean action: 1.683 [0.000, 3.000],  loss: 27.774569, mse: 10403.083927, mean_q: 95.292035, mean_eps: 0.709272
  97078/300000: episode: 963, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: -9.363, mean reward: -0.087 [-100.000, 17.597], mean action: 1.667 [0.000, 3.000],  loss: 22.483384, mse: 10745.045600, mean_q: 97.754178, mean_eps: 0.708929
  97150/300000: episode: 964, duration: 0.517s, episode steps:  72, steps per second: 139, episode reward: -52.759, mean reward: -0.733 [-100.000, 10.330], mean action: 1.528 [0.000, 3.000],  loss: 51.077090, mse: 10981.295817, mean_q: 97.802682, mean_eps: 0.708659
  97220/300000: episode: 965, duration: 0.494s, episode steps:  70, steps per second: 142, episode reward: -15.116, mean reward: -0.216 [-100.000, 20.112], mean action: 1.557 [0.000, 3.000],  loss: 22.453925, mse: 11299.106662, mean_q: 99.488092, mean_eps: 0.708446
  97317/300000: episode: 966, duration: 0.672s, episode steps:  97, steps per second: 144, episode reward: -159.379, mean reward: -1.643 [-100.000,  5.744], mean action: 1.773 [0.000, 3.000],  loss: 24.016390, mse: 11165.233328, mean_q: 98.343798, mean_eps: 0.708196
  97431/300000: episode: 967, duration: 0.763s, episode steps: 114, steps per second: 149, episode reward: -51.024, mean reward: -0.448 [-100.000, 15.718], mean action: 1.614 [0.000, 3.000],  loss: 30.514829, mse: 11083.255315, mean_q: 98.309916, mean_eps: 0.707879
  97511/300000: episode: 968, duration: 0.566s, episode steps:  80, steps per second: 141, episode reward: -90.522, mean reward: -1.132 [-100.000, 12.340], mean action: 1.938 [0.000, 3.000],  loss: 40.301896, mse: 11006.080396, mean_q: 98.324882, mean_eps: 0.707588
  97650/300000: episode: 969, duration: 0.959s, episode steps: 139, steps per second: 145, episode reward: -206.482, mean reward: -1.485 [-100.000, 39.805], mean action: 1.568 [0.000, 3.000],  loss: 18.148012, mse: 11328.185800, mean_q: 99.239160, mean_eps: 0.707260
  97749/300000: episode: 970, duration: 0.670s, episode steps:  99, steps per second: 148, episode reward: -50.525, mean reward: -0.510 [-100.000, 15.962], mean action: 1.677 [0.000, 3.000],  loss: 39.743375, mse: 11445.723761, mean_q: 99.793402, mean_eps: 0.706903
  97873/300000: episode: 971, duration: 0.853s, episode steps: 124, steps per second: 145, episode reward:  4.952, mean reward:  0.040 [-100.000, 46.538], mean action: 1.613 [0.000, 3.000],  loss: 25.529852, mse: 11497.463859, mean_q: 101.274372, mean_eps: 0.706569
  97976/300000: episode: 972, duration: 0.725s, episode steps: 103, steps per second: 142, episode reward: -30.041, mean reward: -0.292 [-100.000, 13.871], mean action: 1.631 [0.000, 3.000],  loss: 41.982842, mse: 11461.994975, mean_q: 100.086978, mean_eps: 0.706228
  98052/300000: episode: 973, duration: 0.518s, episode steps:  76, steps per second: 147, episode reward: -73.137, mean reward: -0.962 [-100.000,  7.042], mean action: 1.671 [0.000, 3.000],  loss: 34.304956, mse: 11532.917840, mean_q: 102.132079, mean_eps: 0.705959
  98175/300000: episode: 974, duration: 0.842s, episode steps: 123, steps per second: 146, episode reward: -27.032, mean reward: -0.220 [-100.000, 24.120], mean action: 1.650 [0.000, 3.000],  loss: 28.225803, mse: 11692.450164, mean_q: 103.217313, mean_eps: 0.705661
  98284/300000: episode: 975, duration: 0.756s, episode steps: 109, steps per second: 144, episode reward: -86.210, mean reward: -0.791 [-100.000,  9.015], mean action: 1.697 [0.000, 3.000],  loss: 29.834464, mse: 11721.728068, mean_q: 102.694190, mean_eps: 0.705313
  98378/300000: episode: 976, duration: 0.649s, episode steps:  94, steps per second: 145, episode reward: -65.748, mean reward: -0.699 [-100.000,  7.390], mean action: 1.585 [0.000, 3.000],  loss: 29.375671, mse: 12238.846098, mean_q: 105.965950, mean_eps: 0.705008
  98517/300000: episode: 977, duration: 0.963s, episode steps: 139, steps per second: 144, episode reward: -72.659, mean reward: -0.523 [-100.000, 18.028], mean action: 1.647 [0.000, 3.000],  loss: 32.415291, mse: 12185.202345, mean_q: 106.242027, mean_eps: 0.704659
  98602/300000: episode: 978, duration: 0.577s, episode steps:  85, steps per second: 147, episode reward: -49.975, mean reward: -0.588 [-100.000, 10.779], mean action: 1.588 [0.000, 3.000],  loss: 40.071304, mse: 12166.556595, mean_q: 104.795610, mean_eps: 0.704323
  98706/300000: episode: 979, duration: 0.705s, episode steps: 104, steps per second: 148, episode reward: -92.514, mean reward: -0.890 [-100.000, 10.337], mean action: 1.500 [0.000, 3.000],  loss: 36.131884, mse: 12430.205857, mean_q: 106.677830, mean_eps: 0.704039
  98830/300000: episode: 980, duration: 0.862s, episode steps: 124, steps per second: 144, episode reward: -96.846, mean reward: -0.781 [-100.000,  8.043], mean action: 1.548 [0.000, 3.000],  loss: 32.884253, mse: 12122.821399, mean_q: 104.335358, mean_eps: 0.703698
  98895/300000: episode: 981, duration: 0.450s, episode steps:  65, steps per second: 144, episode reward: -107.446, mean reward: -1.653 [-100.000,  7.454], mean action: 1.369 [0.000, 3.000],  loss: 28.799623, mse: 12276.857797, mean_q: 104.827242, mean_eps: 0.703414
  98978/300000: episode: 982, duration: 0.565s, episode steps:  83, steps per second: 147, episode reward: -59.777, mean reward: -0.720 [-100.000,  6.965], mean action: 1.614 [0.000, 3.000],  loss: 27.401413, mse: 11966.538886, mean_q: 103.929378, mean_eps: 0.703192
  99099/300000: episode: 983, duration: 0.839s, episode steps: 121, steps per second: 144, episode reward: -67.138, mean reward: -0.555 [-100.000, 10.894], mean action: 1.570 [0.000, 3.000],  loss: 32.746817, mse: 12139.541831, mean_q: 104.456456, mean_eps: 0.702886
  99180/300000: episode: 984, duration: 0.553s, episode steps:  81, steps per second: 146, episode reward: -101.773, mean reward: -1.256 [-100.000, 22.162], mean action: 1.444 [0.000, 3.000],  loss: 22.772145, mse: 12274.185282, mean_q: 105.247953, mean_eps: 0.702583
  99277/300000: episode: 985, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -130.548, mean reward: -1.346 [-100.000,  5.992], mean action: 1.619 [0.000, 3.000],  loss: 22.305640, mse: 12072.602167, mean_q: 104.071860, mean_eps: 0.702316
  99349/300000: episode: 986, duration: 0.509s, episode steps:  72, steps per second: 141, episode reward: -86.875, mean reward: -1.207 [-100.000, 14.734], mean action: 1.694 [0.000, 3.000],  loss: 28.668175, mse: 12133.102390, mean_q: 105.375839, mean_eps: 0.702063
  99435/300000: episode: 987, duration: 0.609s, episode steps:  86, steps per second: 141, episode reward: -64.091, mean reward: -0.745 [-100.000,  6.954], mean action: 1.512 [0.000, 3.000],  loss: 16.337688, mse: 12049.979515, mean_q: 104.002654, mean_eps: 0.701825
  99570/300000: episode: 988, duration: 0.955s, episode steps: 135, steps per second: 141, episode reward:  4.321, mean reward:  0.032 [-100.000,  9.123], mean action: 1.630 [0.000, 3.000],  loss: 19.201491, mse: 12349.242101, mean_q: 104.931097, mean_eps: 0.701494
  99663/300000: episode: 989, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward: -78.378, mean reward: -0.843 [-100.000, 13.181], mean action: 1.516 [0.000, 3.000],  loss: 21.949063, mse: 12300.866757, mean_q: 105.188835, mean_eps: 0.701152
  99736/300000: episode: 990, duration: 0.548s, episode steps:  73, steps per second: 133, episode reward: -85.515, mean reward: -1.171 [-100.000,  7.342], mean action: 1.425 [0.000, 3.000],  loss: 27.939305, mse: 12466.609830, mean_q: 106.349004, mean_eps: 0.700903
  99860/300000: episode: 991, duration: 0.850s, episode steps: 124, steps per second: 146, episode reward: -52.053, mean reward: -0.420 [-100.000, 12.207], mean action: 1.573 [0.000, 3.000],  loss: 30.857661, mse: 12720.568856, mean_q: 106.424887, mean_eps: 0.700608
  99979/300000: episode: 992, duration: 0.820s, episode steps: 119, steps per second: 145, episode reward: -57.812, mean reward: -0.486 [-100.000, 13.803], mean action: 1.664 [0.000, 3.000],  loss: 20.085546, mse: 12512.588736, mean_q: 105.260129, mean_eps: 0.700243
 100079/300000: episode: 993, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: -82.635, mean reward: -0.826 [-100.000,  8.499], mean action: 1.590 [0.000, 3.000],  loss: 17.119910, mse: 12739.587168, mean_q: 106.792166, mean_eps: 0.699914
 100219/300000: episode: 994, duration: 0.941s, episode steps: 140, steps per second: 149, episode reward: -22.481, mean reward: -0.161 [-100.000, 15.713], mean action: 1.550 [0.000, 3.000],  loss: 29.204181, mse: 12447.699379, mean_q: 104.651418, mean_eps: 0.699554
 100311/300000: episode: 995, duration: 0.754s, episode steps:  92, steps per second: 122, episode reward: -42.759, mean reward: -0.465 [-100.000,  9.674], mean action: 1.641 [0.000, 3.000],  loss: 31.115463, mse: 12560.386199, mean_q: 105.594258, mean_eps: 0.699206
 100394/300000: episode: 996, duration: 0.701s, episode steps:  83, steps per second: 118, episode reward: -42.573, mean reward: -0.513 [-100.000,  6.338], mean action: 1.590 [0.000, 3.000],  loss: 13.697689, mse: 12327.360281, mean_q: 105.085415, mean_eps: 0.698944
 100497/300000: episode: 997, duration: 0.776s, episode steps: 103, steps per second: 133, episode reward: -69.318, mean reward: -0.673 [-100.000,  9.265], mean action: 1.427 [0.000, 3.000],  loss: 24.213292, mse: 12377.128480, mean_q: 104.484816, mean_eps: 0.698665
 100614/300000: episode: 998, duration: 0.981s, episode steps: 117, steps per second: 119, episode reward: -214.301, mean reward: -1.832 [-100.000, 42.626], mean action: 1.641 [0.000, 3.000],  loss: 16.905242, mse: 12916.314044, mean_q: 107.517819, mean_eps: 0.698335
 100687/300000: episode: 999, duration: 0.576s, episode steps:  73, steps per second: 127, episode reward: -56.788, mean reward: -0.778 [-100.000, 11.093], mean action: 1.384 [0.000, 3.000],  loss: 25.294027, mse: 12847.728074, mean_q: 107.936925, mean_eps: 0.698050
 100792/300000: episode: 1000, duration: 0.781s, episode steps: 105, steps per second: 134, episode reward: -58.989, mean reward: -0.562 [-100.000, 10.042], mean action: 1.667 [0.000, 3.000],  loss: 14.057847, mse: 13028.088560, mean_q: 107.685004, mean_eps: 0.697783
 100881/300000: episode: 1001, duration: 0.652s, episode steps:  89, steps per second: 137, episode reward: -73.504, mean reward: -0.826 [-100.000,  9.972], mean action: 1.472 [0.000, 3.000],  loss: 15.032259, mse: 12830.602627, mean_q: 105.674316, mean_eps: 0.697492
 100971/300000: episode: 1002, duration: 0.662s, episode steps:  90, steps per second: 136, episode reward: -100.222, mean reward: -1.114 [-100.000, 11.562], mean action: 1.678 [0.000, 3.000],  loss: 34.606921, mse: 12976.086556, mean_q: 108.438881, mean_eps: 0.697223
 101093/300000: episode: 1003, duration: 0.833s, episode steps: 122, steps per second: 146, episode reward: -68.398, mean reward: -0.561 [-100.000, 12.254], mean action: 1.656 [0.000, 3.000],  loss: 27.015184, mse: 13196.823530, mean_q: 107.421705, mean_eps: 0.696905
 101196/300000: episode: 1004, duration: 0.723s, episode steps: 103, steps per second: 142, episode reward: -51.405, mean reward: -0.499 [-100.000,  8.984], mean action: 1.689 [0.000, 3.000],  loss: 30.395137, mse: 13343.997962, mean_q: 107.976912, mean_eps: 0.696568
 101272/300000: episode: 1005, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -94.785, mean reward: -1.247 [-100.000,  5.266], mean action: 1.645 [0.000, 3.000],  loss: 36.542839, mse: 13607.182823, mean_q: 108.317841, mean_eps: 0.696299
 101352/300000: episode: 1006, duration: 0.538s, episode steps:  80, steps per second: 149, episode reward: -29.798, mean reward: -0.372 [-100.000, 17.223], mean action: 1.525 [0.000, 3.000],  loss: 18.300072, mse: 13606.533154, mean_q: 109.255123, mean_eps: 0.696065
 101432/300000: episode: 1007, duration: 0.534s, episode steps:  80, steps per second: 150, episode reward: -29.749, mean reward: -0.372 [-100.000, 22.522], mean action: 1.625 [0.000, 3.000],  loss: 30.757885, mse: 13303.840344, mean_q: 107.782394, mean_eps: 0.695826
 101549/300000: episode: 1008, duration: 0.825s, episode steps: 117, steps per second: 142, episode reward: -95.102, mean reward: -0.813 [-100.000, 12.527], mean action: 1.547 [0.000, 3.000],  loss: 16.105844, mse: 14113.906584, mean_q: 112.552842, mean_eps: 0.695530
 101629/300000: episode: 1009, duration: 0.546s, episode steps:  80, steps per second: 146, episode reward: -43.501, mean reward: -0.544 [-100.000,  6.815], mean action: 1.613 [0.000, 3.000],  loss: 24.822946, mse: 13809.585413, mean_q: 110.994949, mean_eps: 0.695235
 101740/300000: episode: 1010, duration: 0.738s, episode steps: 111, steps per second: 150, episode reward: -48.719, mean reward: -0.439 [-100.000,  8.787], mean action: 1.640 [0.000, 3.000],  loss: 28.063748, mse: 14037.658889, mean_q: 111.618205, mean_eps: 0.694948
 101844/300000: episode: 1011, duration: 0.794s, episode steps: 104, steps per second: 131, episode reward: -53.042, mean reward: -0.510 [-100.000,  8.750], mean action: 1.750 [0.000, 3.000],  loss: 28.617423, mse: 14406.999915, mean_q: 115.620017, mean_eps: 0.694626
 101946/300000: episode: 1012, duration: 0.795s, episode steps: 102, steps per second: 128, episode reward: -45.101, mean reward: -0.442 [-100.000, 13.530], mean action: 1.647 [0.000, 3.000],  loss: 33.886242, mse: 14713.980153, mean_q: 117.138466, mean_eps: 0.694317
 102038/300000: episode: 1013, duration: 0.686s, episode steps:  92, steps per second: 134, episode reward: -117.720, mean reward: -1.280 [-100.000,  6.093], mean action: 1.576 [0.000, 3.000],  loss: 21.386198, mse: 14867.280645, mean_q: 116.760782, mean_eps: 0.694025
 102146/300000: episode: 1014, duration: 0.860s, episode steps: 108, steps per second: 126, episode reward: -23.960, mean reward: -0.222 [-100.000, 18.855], mean action: 1.574 [0.000, 3.000],  loss: 24.347548, mse: 14962.508165, mean_q: 117.380545, mean_eps: 0.693726
 102229/300000: episode: 1015, duration: 0.626s, episode steps:  83, steps per second: 133, episode reward: -64.305, mean reward: -0.775 [-100.000,  9.023], mean action: 1.602 [0.000, 3.000],  loss: 22.106099, mse: 15367.384307, mean_q: 119.032919, mean_eps: 0.693439
 102344/300000: episode: 1016, duration: 0.826s, episode steps: 115, steps per second: 139, episode reward: -62.548, mean reward: -0.544 [-100.000,  7.157], mean action: 1.548 [0.000, 3.000],  loss: 39.789426, mse: 15842.549779, mean_q: 121.980731, mean_eps: 0.693142
 102469/300000: episode: 1017, duration: 0.898s, episode steps: 125, steps per second: 139, episode reward: -79.621, mean reward: -0.637 [-100.000, 14.254], mean action: 1.624 [0.000, 3.000],  loss: 42.780165, mse: 16087.051297, mean_q: 122.188145, mean_eps: 0.692782
 102587/300000: episode: 1018, duration: 0.866s, episode steps: 118, steps per second: 136, episode reward: -121.800, mean reward: -1.032 [-100.000,  9.727], mean action: 1.686 [0.000, 3.000],  loss: 22.729843, mse: 15910.754494, mean_q: 122.220914, mean_eps: 0.692418
 102708/300000: episode: 1019, duration: 0.884s, episode steps: 121, steps per second: 137, episode reward: -41.066, mean reward: -0.339 [-100.000, 17.921], mean action: 1.554 [0.000, 3.000],  loss: 21.835073, mse: 15987.053453, mean_q: 123.032938, mean_eps: 0.692059
 102821/300000: episode: 1020, duration: 0.802s, episode steps: 113, steps per second: 141, episode reward: -29.276, mean reward: -0.259 [-100.000, 12.502], mean action: 1.628 [0.000, 3.000],  loss: 20.188836, mse: 16315.649967, mean_q: 124.344830, mean_eps: 0.691708
 102917/300000: episode: 1021, duration: 0.663s, episode steps:  96, steps per second: 145, episode reward: -42.629, mean reward: -0.444 [-100.000,  6.997], mean action: 1.573 [0.000, 3.000],  loss: 17.001899, mse: 16142.873678, mean_q: 123.338092, mean_eps: 0.691395
 103033/300000: episode: 1022, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -62.157, mean reward: -0.536 [-100.000,  9.342], mean action: 1.707 [0.000, 3.000],  loss: 30.780360, mse: 16258.750034, mean_q: 124.185909, mean_eps: 0.691076
 103164/300000: episode: 1023, duration: 0.957s, episode steps: 131, steps per second: 137, episode reward: -65.885, mean reward: -0.503 [-100.000,  9.207], mean action: 1.603 [0.000, 3.000],  loss: 26.679588, mse: 16232.555955, mean_q: 123.865733, mean_eps: 0.690706
 103266/300000: episode: 1024, duration: 0.716s, episode steps: 102, steps per second: 143, episode reward: -34.637, mean reward: -0.340 [-100.000, 13.358], mean action: 1.618 [0.000, 3.000],  loss: 15.651669, mse: 16755.653167, mean_q: 127.589261, mean_eps: 0.690357
 103394/300000: episode: 1025, duration: 0.872s, episode steps: 128, steps per second: 147, episode reward: -192.423, mean reward: -1.503 [-100.000, 81.101], mean action: 1.492 [0.000, 3.000],  loss: 21.814567, mse: 16771.002571, mean_q: 126.118861, mean_eps: 0.690011
 103522/300000: episode: 1026, duration: 0.874s, episode steps: 128, steps per second: 146, episode reward: -101.605, mean reward: -0.794 [-100.000, 16.555], mean action: 1.680 [0.000, 3.000],  loss: 21.879789, mse: 16867.658813, mean_q: 126.993636, mean_eps: 0.689628
 103700/300000: episode: 1027, duration: 1.240s, episode steps: 178, steps per second: 144, episode reward: -38.376, mean reward: -0.216 [-100.000, 20.596], mean action: 1.674 [0.000, 3.000],  loss: 20.987283, mse: 17116.273854, mean_q: 126.845884, mean_eps: 0.689168
 103838/300000: episode: 1028, duration: 0.927s, episode steps: 138, steps per second: 149, episode reward: -83.333, mean reward: -0.604 [-100.000,  6.884], mean action: 1.587 [0.000, 3.000],  loss: 31.187500, mse: 16819.414239, mean_q: 126.115082, mean_eps: 0.688694
 103989/300000: episode: 1029, duration: 1.045s, episode steps: 151, steps per second: 144, episode reward: -160.706, mean reward: -1.064 [-100.000,  6.563], mean action: 1.636 [0.000, 3.000],  loss: 19.770200, mse: 17504.593957, mean_q: 128.858270, mean_eps: 0.688261
 104062/300000: episode: 1030, duration: 0.489s, episode steps:  73, steps per second: 149, episode reward: -30.071, mean reward: -0.412 [-100.000, 12.262], mean action: 1.452 [0.000, 3.000],  loss: 21.216823, mse: 17589.622993, mean_q: 129.137448, mean_eps: 0.687925
 104201/300000: episode: 1031, duration: 0.960s, episode steps: 139, steps per second: 145, episode reward:  9.962, mean reward:  0.072 [-100.000, 19.801], mean action: 1.633 [0.000, 3.000],  loss: 19.557761, mse: 17543.236103, mean_q: 127.984100, mean_eps: 0.687607
 104323/300000: episode: 1032, duration: 0.834s, episode steps: 122, steps per second: 146, episode reward: -204.933, mean reward: -1.680 [-100.000,  6.974], mean action: 1.852 [0.000, 3.000],  loss: 23.968924, mse: 17650.582431, mean_q: 126.858564, mean_eps: 0.687215
 104392/300000: episode: 1033, duration: 0.464s, episode steps:  69, steps per second: 149, episode reward: -40.044, mean reward: -0.580 [-100.000, 22.392], mean action: 1.623 [0.000, 3.000],  loss: 16.835928, mse: 18094.652117, mean_q: 130.159970, mean_eps: 0.686929
 104501/300000: episode: 1034, duration: 0.738s, episode steps: 109, steps per second: 148, episode reward: -88.845, mean reward: -0.815 [-100.000, 12.641], mean action: 1.606 [0.000, 3.000],  loss: 16.797929, mse: 18699.560771, mean_q: 132.672680, mean_eps: 0.686662
 104619/300000: episode: 1035, duration: 0.816s, episode steps: 118, steps per second: 145, episode reward: -53.224, mean reward: -0.451 [-100.000, 12.498], mean action: 1.669 [0.000, 3.000],  loss: 23.874123, mse: 18799.436573, mean_q: 132.900859, mean_eps: 0.686322
 104744/300000: episode: 1036, duration: 0.877s, episode steps: 125, steps per second: 142, episode reward: -197.180, mean reward: -1.577 [-100.000, 54.912], mean action: 1.808 [0.000, 3.000],  loss: 17.992224, mse: 18921.242578, mean_q: 132.926804, mean_eps: 0.685957
 104869/300000: episode: 1037, duration: 0.889s, episode steps: 125, steps per second: 141, episode reward: -197.317, mean reward: -1.579 [-100.000, 81.400], mean action: 1.592 [0.000, 3.000],  loss: 38.586922, mse: 18968.313664, mean_q: 133.892121, mean_eps: 0.685582
 104949/300000: episode: 1038, duration: 0.559s, episode steps:  80, steps per second: 143, episode reward: -28.591, mean reward: -0.357 [-100.000,  8.321], mean action: 1.700 [0.000, 3.000],  loss: 38.298526, mse: 19124.752368, mean_q: 133.805689, mean_eps: 0.685275
 105067/300000: episode: 1039, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -63.348, mean reward: -0.537 [-100.000, 10.517], mean action: 1.703 [0.000, 3.000],  loss: 28.130359, mse: 19373.895094, mean_q: 135.697288, mean_eps: 0.684978
 105201/300000: episode: 1040, duration: 0.954s, episode steps: 134, steps per second: 140, episode reward: -18.512, mean reward: -0.138 [-100.000, 15.291], mean action: 1.575 [0.000, 3.000],  loss: 19.968887, mse: 19766.387047, mean_q: 136.685254, mean_eps: 0.684600
 105280/300000: episode: 1041, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -76.649, mean reward: -0.970 [-100.000,  7.859], mean action: 1.709 [0.000, 3.000],  loss: 21.101256, mse: 19767.275415, mean_q: 137.362224, mean_eps: 0.684280
 105345/300000: episode: 1042, duration: 0.437s, episode steps:  65, steps per second: 149, episode reward: -61.275, mean reward: -0.943 [-100.000, 21.352], mean action: 1.477 [0.000, 3.000],  loss: 30.976995, mse: 20093.838552, mean_q: 136.269818, mean_eps: 0.684064
 105479/300000: episode: 1043, duration: 0.939s, episode steps: 134, steps per second: 143, episode reward: -92.001, mean reward: -0.687 [-100.000,  7.339], mean action: 1.672 [0.000, 3.000],  loss: 22.006836, mse: 19976.240278, mean_q: 138.245129, mean_eps: 0.683765
 105599/300000: episode: 1044, duration: 0.816s, episode steps: 120, steps per second: 147, episode reward: -77.815, mean reward: -0.648 [-100.000, 14.161], mean action: 1.417 [0.000, 3.000],  loss: 29.868497, mse: 20374.030623, mean_q: 139.248291, mean_eps: 0.683385
 105696/300000: episode: 1045, duration: 0.651s, episode steps:  97, steps per second: 149, episode reward: -43.111, mean reward: -0.444 [-100.000, 11.211], mean action: 1.598 [0.000, 3.000],  loss: 22.411670, mse: 20521.407458, mean_q: 140.896345, mean_eps: 0.683059
 105842/300000: episode: 1046, duration: 1.015s, episode steps: 146, steps per second: 144, episode reward: -24.001, mean reward: -0.164 [-100.000, 18.569], mean action: 1.541 [0.000, 3.000],  loss: 32.246769, mse: 20130.867716, mean_q: 139.743547, mean_eps: 0.682694
 105950/300000: episode: 1047, duration: 0.722s, episode steps: 108, steps per second: 149, episode reward: 27.257, mean reward:  0.252 [-100.000, 70.200], mean action: 1.806 [0.000, 3.000],  loss: 20.159953, mse: 20396.298249, mean_q: 140.239139, mean_eps: 0.682314
 106032/300000: episode: 1048, duration: 0.550s, episode steps:  82, steps per second: 149, episode reward: -73.225, mean reward: -0.893 [-100.000, 11.904], mean action: 1.463 [0.000, 3.000],  loss: 34.782687, mse: 20363.352813, mean_q: 139.873679, mean_eps: 0.682029
 106132/300000: episode: 1049, duration: 0.701s, episode steps: 100, steps per second: 143, episode reward: -200.090, mean reward: -2.001 [-100.000, 72.734], mean action: 1.690 [0.000, 3.000],  loss: 29.657736, mse: 19994.368770, mean_q: 138.683730, mean_eps: 0.681756
 106263/300000: episode: 1050, duration: 0.946s, episode steps: 131, steps per second: 138, episode reward: -95.738, mean reward: -0.731 [-100.000, 10.301], mean action: 1.595 [0.000, 3.000],  loss: 30.729232, mse: 20521.250157, mean_q: 140.738908, mean_eps: 0.681409
 106360/300000: episode: 1051, duration: 0.646s, episode steps:  97, steps per second: 150, episode reward: -117.841, mean reward: -1.215 [-100.000,  6.542], mean action: 1.660 [0.000, 3.000],  loss: 26.979918, mse: 20480.432607, mean_q: 141.906172, mean_eps: 0.681067
 106457/300000: episode: 1052, duration: 0.687s, episode steps:  97, steps per second: 141, episode reward: -118.720, mean reward: -1.224 [-100.000, 17.886], mean action: 1.732 [0.000, 3.000],  loss: 23.253113, mse: 20280.909784, mean_q: 140.598043, mean_eps: 0.680776
 106573/300000: episode: 1053, duration: 0.797s, episode steps: 116, steps per second: 146, episode reward: -60.809, mean reward: -0.524 [-100.000, 20.285], mean action: 1.586 [0.000, 3.000],  loss: 31.669444, mse: 20108.604341, mean_q: 139.251123, mean_eps: 0.680457
 106670/300000: episode: 1054, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -34.854, mean reward: -0.359 [-100.000,  6.976], mean action: 1.742 [0.000, 3.000],  loss: 26.022575, mse: 19890.957031, mean_q: 139.843918, mean_eps: 0.680137
 106756/300000: episode: 1055, duration: 0.599s, episode steps:  86, steps per second: 144, episode reward: -68.188, mean reward: -0.793 [-100.000, 12.362], mean action: 1.605 [0.000, 3.000],  loss: 26.415981, mse: 20239.376363, mean_q: 141.474301, mean_eps: 0.679862
 106859/300000: episode: 1056, duration: 0.704s, episode steps: 103, steps per second: 146, episode reward: -31.344, mean reward: -0.304 [-100.000, 16.695], mean action: 1.728 [0.000, 3.000],  loss: 28.388569, mse: 20270.641004, mean_q: 141.447122, mean_eps: 0.679579
 106984/300000: episode: 1057, duration: 0.874s, episode steps: 125, steps per second: 143, episode reward: -59.686, mean reward: -0.477 [-100.000, 12.950], mean action: 1.584 [0.000, 3.000],  loss: 28.319172, mse: 20321.010422, mean_q: 140.951188, mean_eps: 0.679237
 107100/300000: episode: 1058, duration: 0.819s, episode steps: 116, steps per second: 142, episode reward: -76.816, mean reward: -0.662 [-100.000, 15.596], mean action: 1.552 [0.000, 3.000],  loss: 30.050551, mse: 20410.439579, mean_q: 141.989755, mean_eps: 0.678876
 107208/300000: episode: 1059, duration: 0.728s, episode steps: 108, steps per second: 148, episode reward: -81.262, mean reward: -0.752 [-100.000, 19.444], mean action: 1.546 [0.000, 3.000],  loss: 18.304582, mse: 20466.152262, mean_q: 142.732298, mean_eps: 0.678540
 107343/300000: episode: 1060, duration: 0.913s, episode steps: 135, steps per second: 148, episode reward: -71.222, mean reward: -0.528 [-100.000, 20.484], mean action: 1.563 [0.000, 3.000],  loss: 34.641100, mse: 20363.581843, mean_q: 142.038950, mean_eps: 0.678175
 107443/300000: episode: 1061, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -79.279, mean reward: -0.793 [-100.000, 11.012], mean action: 1.650 [0.000, 3.000],  loss: 21.338451, mse: 20416.261719, mean_q: 141.820055, mean_eps: 0.677822
 107547/300000: episode: 1062, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -102.957, mean reward: -0.990 [-100.000,  9.165], mean action: 1.606 [0.000, 3.000],  loss: 25.899822, mse: 20551.123253, mean_q: 141.474120, mean_eps: 0.677516
 107644/300000: episode: 1063, duration: 0.670s, episode steps:  97, steps per second: 145, episode reward: -81.560, mean reward: -0.841 [-100.000, 16.345], mean action: 1.660 [0.000, 3.000],  loss: 19.983055, mse: 20909.838918, mean_q: 144.023742, mean_eps: 0.677215
 107740/300000: episode: 1064, duration: 0.661s, episode steps:  96, steps per second: 145, episode reward: -93.340, mean reward: -0.972 [-100.000, 12.232], mean action: 1.552 [0.000, 3.000],  loss: 41.361875, mse: 20189.826538, mean_q: 139.304888, mean_eps: 0.676925
 107864/300000: episode: 1065, duration: 0.837s, episode steps: 124, steps per second: 148, episode reward: -65.827, mean reward: -0.531 [-100.000, 10.701], mean action: 1.427 [0.000, 3.000],  loss: 18.025633, mse: 20358.439500, mean_q: 138.387910, mean_eps: 0.676596
 108022/300000: episode: 1066, duration: 1.106s, episode steps: 158, steps per second: 143, episode reward: -3.172, mean reward: -0.020 [-100.000, 17.034], mean action: 1.766 [0.000, 3.000],  loss: 20.113946, mse: 20946.857014, mean_q: 141.618642, mean_eps: 0.676173
 108118/300000: episode: 1067, duration: 0.648s, episode steps:  96, steps per second: 148, episode reward: -122.975, mean reward: -1.281 [-100.000,  5.070], mean action: 1.500 [0.000, 3.000],  loss: 28.169319, mse: 21208.237508, mean_q: 141.339584, mean_eps: 0.675791
 108183/300000: episode: 1068, duration: 0.439s, episode steps:  65, steps per second: 148, episode reward: -36.123, mean reward: -0.556 [-100.000, 13.116], mean action: 1.477 [0.000, 3.000],  loss: 22.944834, mse: 21341.903531, mean_q: 141.652993, mean_eps: 0.675550
 108259/300000: episode: 1069, duration: 0.511s, episode steps:  76, steps per second: 149, episode reward: -98.842, mean reward: -1.301 [-100.000,  7.399], mean action: 1.658 [0.000, 3.000],  loss: 26.360934, mse: 21016.759714, mean_q: 141.418458, mean_eps: 0.675339
 108356/300000: episode: 1070, duration: 0.678s, episode steps:  97, steps per second: 143, episode reward: -103.221, mean reward: -1.064 [-100.000,  7.333], mean action: 1.536 [0.000, 3.000],  loss: 18.628974, mse: 20981.843841, mean_q: 140.271032, mean_eps: 0.675079
 108433/300000: episode: 1071, duration: 0.528s, episode steps:  77, steps per second: 146, episode reward: -62.005, mean reward: -0.805 [-100.000,  7.158], mean action: 1.481 [0.000, 3.000],  loss: 39.154235, mse: 21037.565011, mean_q: 141.871531, mean_eps: 0.674818
 108549/300000: episode: 1072, duration: 0.783s, episode steps: 116, steps per second: 148, episode reward: -52.925, mean reward: -0.456 [-100.000,  6.978], mean action: 1.509 [0.000, 3.000],  loss: 26.791956, mse: 21165.016501, mean_q: 140.490435, mean_eps: 0.674528
 108667/300000: episode: 1073, duration: 0.824s, episode steps: 118, steps per second: 143, episode reward: -57.264, mean reward: -0.485 [-100.000, 22.576], mean action: 1.559 [0.000, 3.000],  loss: 33.123573, mse: 21029.097656, mean_q: 140.508085, mean_eps: 0.674177
 108769/300000: episode: 1074, duration: 0.688s, episode steps: 102, steps per second: 148, episode reward: -72.901, mean reward: -0.715 [-100.000, 14.183], mean action: 1.598 [0.000, 3.000],  loss: 38.282890, mse: 20854.505725, mean_q: 139.128177, mean_eps: 0.673848
 108844/300000: episode: 1075, duration: 0.514s, episode steps:  75, steps per second: 146, episode reward: -69.607, mean reward: -0.928 [-100.000,  7.367], mean action: 1.707 [0.000, 3.000],  loss: 29.792319, mse: 21009.006549, mean_q: 139.828951, mean_eps: 0.673582
 108923/300000: episode: 1076, duration: 0.568s, episode steps:  79, steps per second: 139, episode reward: -71.845, mean reward: -0.909 [-100.000, 24.366], mean action: 1.646 [0.000, 3.000],  loss: 22.039392, mse: 21598.410428, mean_q: 142.801369, mean_eps: 0.673351
 109009/300000: episode: 1077, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -39.188, mean reward: -0.456 [-100.000, 13.376], mean action: 1.884 [0.000, 3.000],  loss: 26.586643, mse: 21338.372956, mean_q: 140.707400, mean_eps: 0.673103
 109096/300000: episode: 1078, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -42.088, mean reward: -0.484 [-100.000,  8.966], mean action: 1.736 [0.000, 3.000],  loss: 30.425517, mse: 21800.315396, mean_q: 141.844727, mean_eps: 0.672844
 109222/300000: episode: 1079, duration: 0.868s, episode steps: 126, steps per second: 145, episode reward: -30.356, mean reward: -0.241 [-100.000,  8.425], mean action: 1.770 [0.000, 3.000],  loss: 30.409366, mse: 21569.759301, mean_q: 142.245471, mean_eps: 0.672524
 109373/300000: episode: 1080, duration: 1.031s, episode steps: 151, steps per second: 146, episode reward: -40.282, mean reward: -0.267 [-100.000, 16.655], mean action: 1.669 [0.000, 3.000],  loss: 25.005168, mse: 21714.184551, mean_q: 142.684898, mean_eps: 0.672109
 109469/300000: episode: 1081, duration: 0.650s, episode steps:  96, steps per second: 148, episode reward: -76.678, mean reward: -0.799 [-100.000,  8.028], mean action: 1.677 [0.000, 3.000],  loss: 34.285027, mse: 21756.606832, mean_q: 142.588499, mean_eps: 0.671739
 109593/300000: episode: 1082, duration: 0.930s, episode steps: 124, steps per second: 133, episode reward: -232.476, mean reward: -1.875 [-100.000, 36.047], mean action: 1.621 [0.000, 3.000],  loss: 45.886925, mse: 21914.021280, mean_q: 142.901934, mean_eps: 0.671408
 109728/300000: episode: 1083, duration: 1.092s, episode steps: 135, steps per second: 124, episode reward: -66.330, mean reward: -0.491 [-100.000, 18.839], mean action: 1.578 [0.000, 3.000],  loss: 24.827349, mse: 22281.480208, mean_q: 142.452685, mean_eps: 0.671020
 109868/300000: episode: 1084, duration: 1.150s, episode steps: 140, steps per second: 122, episode reward: -91.459, mean reward: -0.653 [-100.000,  6.673], mean action: 1.693 [0.000, 3.000],  loss: 31.751890, mse: 22658.174100, mean_q: 145.224759, mean_eps: 0.670608
 109972/300000: episode: 1085, duration: 0.907s, episode steps: 104, steps per second: 115, episode reward: -49.015, mean reward: -0.471 [-100.000,  8.188], mean action: 1.558 [0.000, 3.000],  loss: 35.679876, mse: 22326.302058, mean_q: 142.360701, mean_eps: 0.670242
 110082/300000: episode: 1086, duration: 0.897s, episode steps: 110, steps per second: 123, episode reward: 16.318, mean reward:  0.148 [-100.000, 22.937], mean action: 1.745 [0.000, 3.000],  loss: 29.429876, mse: 22520.394247, mean_q: 144.507777, mean_eps: 0.669921
 110158/300000: episode: 1087, duration: 0.592s, episode steps:  76, steps per second: 128, episode reward: -57.663, mean reward: -0.759 [-100.000,  9.676], mean action: 1.934 [0.000, 3.000],  loss: 35.655832, mse: 22438.216617, mean_q: 144.209508, mean_eps: 0.669642
 110246/300000: episode: 1088, duration: 0.655s, episode steps:  88, steps per second: 134, episode reward: -43.546, mean reward: -0.495 [-100.000, 20.633], mean action: 1.636 [0.000, 3.000],  loss: 27.255994, mse: 23117.107666, mean_q: 145.759779, mean_eps: 0.669396
 110346/300000: episode: 1089, duration: 0.729s, episode steps: 100, steps per second: 137, episode reward: -43.681, mean reward: -0.437 [-100.000, 12.622], mean action: 1.730 [0.000, 3.000],  loss: 39.924218, mse: 22633.828984, mean_q: 143.488449, mean_eps: 0.669114
 110435/300000: episode: 1090, duration: 0.672s, episode steps:  89, steps per second: 133, episode reward: -53.177, mean reward: -0.597 [-100.000, 10.973], mean action: 1.674 [0.000, 3.000],  loss: 29.470606, mse: 22616.985472, mean_q: 143.272301, mean_eps: 0.668830
 110577/300000: episode: 1091, duration: 0.979s, episode steps: 142, steps per second: 145, episode reward: 41.705, mean reward:  0.294 [-100.000, 41.590], mean action: 1.556 [0.000, 3.000],  loss: 27.707716, mse: 22964.227457, mean_q: 145.058725, mean_eps: 0.668484
 110672/300000: episode: 1092, duration: 0.682s, episode steps:  95, steps per second: 139, episode reward: -47.257, mean reward: -0.497 [-100.000, 11.832], mean action: 1.579 [0.000, 3.000],  loss: 33.911074, mse: 22122.539402, mean_q: 141.356114, mean_eps: 0.668128
 110766/300000: episode: 1093, duration: 0.656s, episode steps:  94, steps per second: 143, episode reward: -102.074, mean reward: -1.086 [-100.000,  6.489], mean action: 1.745 [0.000, 3.000],  loss: 21.216602, mse: 22128.711935, mean_q: 141.501365, mean_eps: 0.667844
 110909/300000: episode: 1094, duration: 0.964s, episode steps: 143, steps per second: 148, episode reward: -12.840, mean reward: -0.090 [-100.000, 12.867], mean action: 1.615 [0.000, 3.000],  loss: 27.502685, mse: 22356.935882, mean_q: 141.568150, mean_eps: 0.667489
 111014/300000: episode: 1095, duration: 0.747s, episode steps: 105, steps per second: 141, episode reward: -80.927, mean reward: -0.771 [-100.000,  6.323], mean action: 1.714 [0.000, 3.000],  loss: 24.887754, mse: 22690.377716, mean_q: 143.153189, mean_eps: 0.667117
 111140/300000: episode: 1096, duration: 0.868s, episode steps: 126, steps per second: 145, episode reward: -127.765, mean reward: -1.014 [-100.000,  7.582], mean action: 1.651 [0.000, 3.000],  loss: 25.095302, mse: 22637.506247, mean_q: 143.465804, mean_eps: 0.666770
 111251/300000: episode: 1097, duration: 0.827s, episode steps: 111, steps per second: 134, episode reward: -116.524, mean reward: -1.050 [-100.000,  5.447], mean action: 1.541 [0.000, 3.000],  loss: 32.746247, mse: 22841.106683, mean_q: 145.233818, mean_eps: 0.666415
 111328/300000: episode: 1098, duration: 0.606s, episode steps:  77, steps per second: 127, episode reward: -60.760, mean reward: -0.789 [-100.000, 10.310], mean action: 1.558 [0.000, 3.000],  loss: 32.389213, mse: 22254.267071, mean_q: 141.352616, mean_eps: 0.666133
 111443/300000: episode: 1099, duration: 0.854s, episode steps: 115, steps per second: 135, episode reward: -75.025, mean reward: -0.652 [-100.000,  7.048], mean action: 1.565 [0.000, 3.000],  loss: 33.824544, mse: 22913.962058, mean_q: 144.955931, mean_eps: 0.665845
 111549/300000: episode: 1100, duration: 0.780s, episode steps: 106, steps per second: 136, episode reward: -176.700, mean reward: -1.667 [-100.000, 33.061], mean action: 1.585 [0.000, 3.000],  loss: 24.206014, mse: 23553.046267, mean_q: 148.008856, mean_eps: 0.665513
 111655/300000: episode: 1101, duration: 0.784s, episode steps: 106, steps per second: 135, episode reward: -61.641, mean reward: -0.582 [-100.000, 26.400], mean action: 1.406 [0.000, 3.000],  loss: 28.671773, mse: 23202.906361, mean_q: 146.844850, mean_eps: 0.665195
 111754/300000: episode: 1102, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -72.476, mean reward: -0.732 [-100.000, 16.472], mean action: 1.535 [0.000, 3.000],  loss: 32.776088, mse: 23692.584458, mean_q: 146.959357, mean_eps: 0.664888
 112754/300000: episode: 1103, duration: 7.591s, episode steps: 1000, steps per second: 132, episode reward: 79.274, mean reward:  0.079 [-23.531, 58.589], mean action: 1.534 [0.000, 3.000],  loss: 32.612364, mse: 25371.021314, mean_q: 152.237037, mean_eps: 0.663239
 112836/300000: episode: 1104, duration: 0.548s, episode steps:  82, steps per second: 150, episode reward: -32.303, mean reward: -0.394 [-100.000, 11.436], mean action: 1.768 [0.000, 3.000],  loss: 37.373847, mse: 27209.723276, mean_q: 156.913819, mean_eps: 0.661616
 112942/300000: episode: 1105, duration: 0.722s, episode steps: 106, steps per second: 147, episode reward: -59.447, mean reward: -0.561 [-100.000,  7.884], mean action: 1.623 [0.000, 3.000],  loss: 24.940620, mse: 27256.511148, mean_q: 157.029717, mean_eps: 0.661334
 113055/300000: episode: 1106, duration: 0.774s, episode steps: 113, steps per second: 146, episode reward: -19.454, mean reward: -0.172 [-100.000, 16.956], mean action: 1.637 [0.000, 3.000],  loss: 26.749283, mse: 26212.401531, mean_q: 152.661243, mean_eps: 0.661006
 113146/300000: episode: 1107, duration: 0.613s, episode steps:  91, steps per second: 149, episode reward: -74.647, mean reward: -0.820 [-100.000,  6.761], mean action: 1.791 [0.000, 3.000],  loss: 22.499901, mse: 26554.886826, mean_q: 153.512066, mean_eps: 0.660700
 113243/300000: episode: 1108, duration: 0.685s, episode steps:  97, steps per second: 142, episode reward: -120.038, mean reward: -1.238 [-100.000,  6.261], mean action: 1.897 [0.000, 3.000],  loss: 22.105284, mse: 27218.853616, mean_q: 156.886129, mean_eps: 0.660418
 113308/300000: episode: 1109, duration: 0.481s, episode steps:  65, steps per second: 135, episode reward: -55.923, mean reward: -0.860 [-100.000,  5.741], mean action: 1.538 [0.000, 3.000],  loss: 14.880683, mse: 26917.814694, mean_q: 152.884645, mean_eps: 0.660175
 113427/300000: episode: 1110, duration: 0.828s, episode steps: 119, steps per second: 144, episode reward: -50.804, mean reward: -0.427 [-100.000,  8.703], mean action: 1.622 [0.000, 3.000],  loss: 24.200116, mse: 27508.909483, mean_q: 156.968761, mean_eps: 0.659899
 113521/300000: episode: 1111, duration: 0.631s, episode steps:  94, steps per second: 149, episode reward: -32.328, mean reward: -0.344 [-100.000, 29.208], mean action: 1.564 [0.000, 3.000],  loss: 33.716214, mse: 27577.852373, mean_q: 155.883112, mean_eps: 0.659580
 113637/300000: episode: 1112, duration: 0.800s, episode steps: 116, steps per second: 145, episode reward: -31.025, mean reward: -0.267 [-100.000, 10.868], mean action: 1.784 [0.000, 3.000],  loss: 31.182088, mse: 28067.496279, mean_q: 158.416665, mean_eps: 0.659265
 113733/300000: episode: 1113, duration: 0.665s, episode steps:  96, steps per second: 144, episode reward: -91.205, mean reward: -0.950 [-100.000,  5.323], mean action: 1.708 [0.000, 3.000],  loss: 28.062829, mse: 27807.377889, mean_q: 156.874512, mean_eps: 0.658946
 113876/300000: episode: 1114, duration: 0.956s, episode steps: 143, steps per second: 150, episode reward: -80.778, mean reward: -0.565 [-100.000,  6.174], mean action: 1.636 [0.000, 3.000],  loss: 33.175856, mse: 27620.402945, mean_q: 156.341778, mean_eps: 0.658588
 113970/300000: episode: 1115, duration: 0.735s, episode steps:  94, steps per second: 128, episode reward: -72.347, mean reward: -0.770 [-100.000,  6.844], mean action: 1.415 [0.000, 3.000],  loss: 18.449904, mse: 27782.142246, mean_q: 155.549418, mean_eps: 0.658232
 114084/300000: episode: 1116, duration: 0.813s, episode steps: 114, steps per second: 140, episode reward: -60.699, mean reward: -0.532 [-100.000,  7.627], mean action: 1.623 [0.000, 3.000],  loss: 38.148710, mse: 28462.292403, mean_q: 158.779815, mean_eps: 0.657921
 114208/300000: episode: 1117, duration: 0.854s, episode steps: 124, steps per second: 145, episode reward: -75.899, mean reward: -0.612 [-100.000,  7.486], mean action: 1.468 [0.000, 3.000],  loss: 25.675288, mse: 28506.622826, mean_q: 158.440519, mean_eps: 0.657564
 114282/300000: episode: 1118, duration: 0.539s, episode steps:  74, steps per second: 137, episode reward: -86.391, mean reward: -1.167 [-100.000, 11.453], mean action: 1.716 [0.000, 3.000],  loss: 19.111290, mse: 28279.902924, mean_q: 157.596781, mean_eps: 0.657266
 114369/300000: episode: 1119, duration: 0.603s, episode steps:  87, steps per second: 144, episode reward:  0.308, mean reward:  0.004 [-100.000, 18.895], mean action: 1.851 [0.000, 3.000],  loss: 20.327650, mse: 29433.155442, mean_q: 160.232577, mean_eps: 0.657025
 114492/300000: episode: 1120, duration: 0.852s, episode steps: 123, steps per second: 144, episode reward: -89.506, mean reward: -0.728 [-100.000, 15.650], mean action: 1.585 [0.000, 3.000],  loss: 36.529704, mse: 28962.576505, mean_q: 160.601878, mean_eps: 0.656710
 114581/300000: episode: 1121, duration: 0.699s, episode steps:  89, steps per second: 127, episode reward: -71.882, mean reward: -0.808 [-100.000,  9.576], mean action: 1.551 [0.000, 3.000],  loss: 27.804366, mse: 28412.479218, mean_q: 158.287331, mean_eps: 0.656392
 114658/300000: episode: 1122, duration: 0.538s, episode steps:  77, steps per second: 143, episode reward: -59.089, mean reward: -0.767 [-100.000, 10.852], mean action: 1.805 [0.000, 3.000],  loss: 31.431031, mse: 28899.552963, mean_q: 160.269608, mean_eps: 0.656143
 114730/300000: episode: 1123, duration: 0.554s, episode steps:  72, steps per second: 130, episode reward: -28.872, mean reward: -0.401 [-100.000, 13.845], mean action: 1.833 [0.000, 3.000],  loss: 29.788345, mse: 29260.590468, mean_q: 159.224708, mean_eps: 0.655919
 114825/300000: episode: 1124, duration: 0.709s, episode steps:  95, steps per second: 134, episode reward: -40.128, mean reward: -0.422 [-100.000, 13.644], mean action: 1.632 [0.000, 3.000],  loss: 39.201537, mse: 28169.690892, mean_q: 154.829134, mean_eps: 0.655669
 114942/300000: episode: 1125, duration: 0.856s, episode steps: 117, steps per second: 137, episode reward: -59.025, mean reward: -0.504 [-100.000, 10.319], mean action: 1.530 [0.000, 3.000],  loss: 21.704256, mse: 28756.280883, mean_q: 156.904872, mean_eps: 0.655351
 115041/300000: episode: 1126, duration: 0.683s, episode steps:  99, steps per second: 145, episode reward: -45.459, mean reward: -0.459 [-100.000,  6.917], mean action: 1.556 [0.000, 3.000],  loss: 24.666350, mse: 29952.728654, mean_q: 161.879890, mean_eps: 0.655027
 115170/300000: episode: 1127, duration: 0.906s, episode steps: 129, steps per second: 142, episode reward: -5.198, mean reward: -0.040 [-100.000, 20.085], mean action: 1.543 [0.000, 3.000],  loss: 27.067735, mse: 29398.599594, mean_q: 160.751505, mean_eps: 0.654685
 115271/300000: episode: 1128, duration: 0.699s, episode steps: 101, steps per second: 144, episode reward: -54.606, mean reward: -0.541 [-100.000, 14.137], mean action: 1.634 [0.000, 3.000],  loss: 56.575125, mse: 29788.484994, mean_q: 161.318185, mean_eps: 0.654340
 115377/300000: episode: 1129, duration: 0.774s, episode steps: 106, steps per second: 137, episode reward: -77.467, mean reward: -0.731 [-100.000, 12.333], mean action: 1.736 [0.000, 3.000],  loss: 36.769405, mse: 30017.651478, mean_q: 161.055295, mean_eps: 0.654030
 115455/300000: episode: 1130, duration: 0.716s, episode steps:  78, steps per second: 109, episode reward: -65.147, mean reward: -0.835 [-100.000,  8.414], mean action: 1.628 [0.000, 3.000],  loss: 23.608359, mse: 31145.205579, mean_q: 165.493878, mean_eps: 0.653753
 115569/300000: episode: 1131, duration: 0.833s, episode steps: 114, steps per second: 137, episode reward: -57.130, mean reward: -0.501 [-100.000, 18.663], mean action: 1.632 [0.000, 3.000],  loss: 21.709473, mse: 31380.759492, mean_q: 165.708330, mean_eps: 0.653466
 115695/300000: episode: 1132, duration: 0.864s, episode steps: 126, steps per second: 146, episode reward: -60.735, mean reward: -0.482 [-100.000, 12.992], mean action: 1.667 [0.000, 3.000],  loss: 21.990553, mse: 31580.402344, mean_q: 167.628151, mean_eps: 0.653106
 115870/300000: episode: 1133, duration: 1.243s, episode steps: 175, steps per second: 141, episode reward: -29.411, mean reward: -0.168 [-100.000, 15.314], mean action: 1.583 [0.000, 3.000],  loss: 29.748990, mse: 30859.157913, mean_q: 163.317072, mean_eps: 0.652654
 115985/300000: episode: 1134, duration: 0.791s, episode steps: 115, steps per second: 145, episode reward: -21.099, mean reward: -0.183 [-100.000, 14.018], mean action: 1.600 [0.000, 3.000],  loss: 29.625470, mse: 30941.696501, mean_q: 163.991209, mean_eps: 0.652219
 116093/300000: episode: 1135, duration: 0.764s, episode steps: 108, steps per second: 141, episode reward: -53.906, mean reward: -0.499 [-100.000, 18.212], mean action: 1.556 [0.000, 3.000],  loss: 21.676632, mse: 31107.598289, mean_q: 164.705141, mean_eps: 0.651884
 116188/300000: episode: 1136, duration: 0.678s, episode steps:  95, steps per second: 140, episode reward: -98.412, mean reward: -1.036 [-100.000,  8.184], mean action: 1.747 [0.000, 3.000],  loss: 16.956311, mse: 31387.011451, mean_q: 165.416076, mean_eps: 0.651580
 116294/300000: episode: 1137, duration: 0.727s, episode steps: 106, steps per second: 146, episode reward: -26.711, mean reward: -0.252 [-100.000, 12.473], mean action: 1.481 [0.000, 3.000],  loss: 38.547856, mse: 30624.164044, mean_q: 160.766825, mean_eps: 0.651278
 116412/300000: episode: 1138, duration: 0.832s, episode steps: 118, steps per second: 142, episode reward: -55.459, mean reward: -0.470 [-100.000, 22.637], mean action: 1.551 [0.000, 3.000],  loss: 25.275386, mse: 31274.173017, mean_q: 164.853924, mean_eps: 0.650942
 116490/300000: episode: 1139, duration: 0.543s, episode steps:  78, steps per second: 144, episode reward: -76.198, mean reward: -0.977 [-100.000,  6.547], mean action: 1.513 [0.000, 3.000],  loss: 23.631443, mse: 31190.562750, mean_q: 162.685944, mean_eps: 0.650649
 116561/300000: episode: 1140, duration: 0.492s, episode steps:  71, steps per second: 144, episode reward: -64.234, mean reward: -0.905 [-100.000, 13.130], mean action: 1.563 [0.000, 3.000],  loss: 25.482745, mse: 31769.180678, mean_q: 165.315865, mean_eps: 0.650425
 116661/300000: episode: 1141, duration: 0.718s, episode steps: 100, steps per second: 139, episode reward: -77.192, mean reward: -0.772 [-100.000, 17.389], mean action: 1.580 [0.000, 3.000],  loss: 41.094308, mse: 31477.354180, mean_q: 164.471936, mean_eps: 0.650169
 116787/300000: episode: 1142, duration: 0.894s, episode steps: 126, steps per second: 141, episode reward: -41.343, mean reward: -0.328 [-100.000, 14.760], mean action: 1.754 [0.000, 3.000],  loss: 22.034318, mse: 32352.972827, mean_q: 166.157705, mean_eps: 0.649830
 117787/300000: episode: 1143, duration: 7.935s, episode steps: 1000, steps per second: 126, episode reward: 10.147, mean reward:  0.010 [-24.308, 58.069], mean action: 1.487 [0.000, 3.000],  loss: 32.787552, mse: 32073.178377, mean_q: 165.263873, mean_eps: 0.648141
 117876/300000: episode: 1144, duration: 0.621s, episode steps:  89, steps per second: 143, episode reward: -77.765, mean reward: -0.874 [-100.000, 12.539], mean action: 1.528 [0.000, 3.000],  loss: 24.484555, mse: 32362.389681, mean_q: 164.669662, mean_eps: 0.646507
 117964/300000: episode: 1145, duration: 0.612s, episode steps:  88, steps per second: 144, episode reward: -153.156, mean reward: -1.740 [-100.000, 25.909], mean action: 1.761 [0.000, 3.000],  loss: 28.310870, mse: 32391.582941, mean_q: 165.323828, mean_eps: 0.646242
 118057/300000: episode: 1146, duration: 0.657s, episode steps:  93, steps per second: 141, episode reward: -92.128, mean reward: -0.991 [-100.000, 22.506], mean action: 1.731 [0.000, 3.000],  loss: 24.491412, mse: 32379.453860, mean_q: 164.370327, mean_eps: 0.645970
 118162/300000: episode: 1147, duration: 0.742s, episode steps: 105, steps per second: 142, episode reward: -73.728, mean reward: -0.702 [-100.000,  9.914], mean action: 1.676 [0.000, 3.000],  loss: 31.304666, mse: 32947.276302, mean_q: 167.216238, mean_eps: 0.645673
 118259/300000: episode: 1148, duration: 0.681s, episode steps:  97, steps per second: 143, episode reward: -64.490, mean reward: -0.665 [-100.000, 16.362], mean action: 1.701 [0.000, 3.000],  loss: 25.439666, mse: 32064.909774, mean_q: 163.392184, mean_eps: 0.645370
 118381/300000: episode: 1149, duration: 0.880s, episode steps: 122, steps per second: 139, episode reward: -56.862, mean reward: -0.466 [-100.000, 21.310], mean action: 1.721 [0.000, 3.000],  loss: 59.423015, mse: 33084.181513, mean_q: 166.328856, mean_eps: 0.645041
 118516/300000: episode: 1150, duration: 0.935s, episode steps: 135, steps per second: 144, episode reward: -75.002, mean reward: -0.556 [-100.000,  8.422], mean action: 1.593 [0.000, 3.000],  loss: 28.412726, mse: 32866.679268, mean_q: 165.076214, mean_eps: 0.644656
 118642/300000: episode: 1151, duration: 0.915s, episode steps: 126, steps per second: 138, episode reward: -67.007, mean reward: -0.532 [-100.000, 11.562], mean action: 1.667 [0.000, 3.000],  loss: 35.743131, mse: 34176.106910, mean_q: 169.304231, mean_eps: 0.644264
 118731/300000: episode: 1152, duration: 0.782s, episode steps:  89, steps per second: 114, episode reward: -234.719, mean reward: -2.637 [-100.000, 38.192], mean action: 1.494 [0.000, 3.000],  loss: 36.089832, mse: 33204.445466, mean_q: 167.892222, mean_eps: 0.643942
 118848/300000: episode: 1153, duration: 1.055s, episode steps: 117, steps per second: 111, episode reward: -112.755, mean reward: -0.964 [-100.000, 27.272], mean action: 1.538 [0.000, 3.000],  loss: 26.993329, mse: 33056.004624, mean_q: 167.328639, mean_eps: 0.643633
 118954/300000: episode: 1154, duration: 0.935s, episode steps: 106, steps per second: 113, episode reward: -41.143, mean reward: -0.388 [-100.000, 20.563], mean action: 1.632 [0.000, 3.000],  loss: 25.061575, mse: 33143.650243, mean_q: 167.631581, mean_eps: 0.643299
 119082/300000: episode: 1155, duration: 1.039s, episode steps: 128, steps per second: 123, episode reward: -83.791, mean reward: -0.655 [-100.000, 18.755], mean action: 1.789 [0.000, 3.000],  loss: 29.144000, mse: 33314.913895, mean_q: 168.670815, mean_eps: 0.642948
 119167/300000: episode: 1156, duration: 0.678s, episode steps:  85, steps per second: 125, episode reward: -57.098, mean reward: -0.672 [-100.000, 19.810], mean action: 1.682 [0.000, 3.000],  loss: 37.591515, mse: 33952.180285, mean_q: 170.533068, mean_eps: 0.642628
 119266/300000: episode: 1157, duration: 0.775s, episode steps:  99, steps per second: 128, episode reward: -59.142, mean reward: -0.597 [-100.000, 11.519], mean action: 1.646 [0.000, 3.000],  loss: 40.228272, mse: 34224.083393, mean_q: 171.424712, mean_eps: 0.642352
 119355/300000: episode: 1158, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: 11.966, mean reward:  0.134 [-100.000, 14.352], mean action: 1.652 [0.000, 3.000],  loss: 30.589543, mse: 34441.198473, mean_q: 170.904817, mean_eps: 0.642070
 119433/300000: episode: 1159, duration: 0.589s, episode steps:  78, steps per second: 132, episode reward: -17.893, mean reward: -0.229 [-100.000, 14.453], mean action: 1.756 [0.000, 3.000],  loss: 42.824947, mse: 35139.114608, mean_q: 174.358194, mean_eps: 0.641820
 119527/300000: episode: 1160, duration: 0.762s, episode steps:  94, steps per second: 123, episode reward: -102.990, mean reward: -1.096 [-100.000, 10.707], mean action: 1.734 [0.000, 3.000],  loss: 28.888334, mse: 34801.657247, mean_q: 172.783871, mean_eps: 0.641561
 119637/300000: episode: 1161, duration: 0.788s, episode steps: 110, steps per second: 140, episode reward: -41.497, mean reward: -0.377 [-100.000, 12.802], mean action: 1.764 [0.000, 3.000],  loss: 24.080440, mse: 34304.945668, mean_q: 171.914909, mean_eps: 0.641256
 119704/300000: episode: 1162, duration: 0.480s, episode steps:  67, steps per second: 139, episode reward: -44.880, mean reward: -0.670 [-100.000, 17.495], mean action: 1.612 [0.000, 3.000],  loss: 29.642124, mse: 35231.251574, mean_q: 173.807884, mean_eps: 0.640990
 119843/300000: episode: 1163, duration: 1.007s, episode steps: 139, steps per second: 138, episode reward: -143.038, mean reward: -1.029 [-100.000,  9.984], mean action: 1.532 [0.000, 3.000],  loss: 26.215557, mse: 35276.338059, mean_q: 172.969987, mean_eps: 0.640681
 119962/300000: episode: 1164, duration: 0.842s, episode steps: 119, steps per second: 141, episode reward: -163.543, mean reward: -1.374 [-100.000, 60.212], mean action: 1.655 [0.000, 3.000],  loss: 35.203866, mse: 35705.492647, mean_q: 176.481711, mean_eps: 0.640294
 120045/300000: episode: 1165, duration: 0.632s, episode steps:  83, steps per second: 131, episode reward: -74.211, mean reward: -0.894 [-100.000,  6.326], mean action: 1.843 [0.000, 3.000],  loss: 17.653652, mse: 36904.717150, mean_q: 177.707991, mean_eps: 0.639991
 120119/300000: episode: 1166, duration: 0.689s, episode steps:  74, steps per second: 107, episode reward: -52.354, mean reward: -0.707 [-100.000,  5.997], mean action: 1.595 [0.000, 3.000],  loss: 44.514469, mse: 36222.097445, mean_q: 177.520255, mean_eps: 0.639756
 120254/300000: episode: 1167, duration: 1.062s, episode steps: 135, steps per second: 127, episode reward: -61.325, mean reward: -0.454 [-100.000,  6.328], mean action: 1.859 [0.000, 3.000],  loss: 23.306610, mse: 36417.430729, mean_q: 176.349530, mean_eps: 0.639442
 120391/300000: episode: 1168, duration: 1.171s, episode steps: 137, steps per second: 117, episode reward: -91.797, mean reward: -0.670 [-100.000,  9.765], mean action: 1.635 [0.000, 3.000],  loss: 37.801120, mse: 36646.617772, mean_q: 177.918258, mean_eps: 0.639034
 120548/300000: episode: 1169, duration: 1.203s, episode steps: 157, steps per second: 131, episode reward: -8.424, mean reward: -0.054 [-100.000, 18.622], mean action: 1.637 [0.000, 3.000],  loss: 26.644886, mse: 37340.146758, mean_q: 180.460008, mean_eps: 0.638593
 120628/300000: episode: 1170, duration: 0.591s, episode steps:  80, steps per second: 135, episode reward: -54.205, mean reward: -0.678 [-100.000, 15.180], mean action: 1.575 [0.000, 3.000],  loss: 27.729574, mse: 36314.975366, mean_q: 177.044408, mean_eps: 0.638238
 120762/300000: episode: 1171, duration: 0.925s, episode steps: 134, steps per second: 145, episode reward: -16.778, mean reward: -0.125 [-100.000, 16.581], mean action: 1.664 [0.000, 3.000],  loss: 49.062340, mse: 36703.261529, mean_q: 179.253408, mean_eps: 0.637917
 120870/300000: episode: 1172, duration: 0.770s, episode steps: 108, steps per second: 140, episode reward: -104.434, mean reward: -0.967 [-100.000, 11.357], mean action: 1.787 [0.000, 3.000],  loss: 32.666283, mse: 36944.565629, mean_q: 178.300810, mean_eps: 0.637553
 120959/300000: episode: 1173, duration: 0.632s, episode steps:  89, steps per second: 141, episode reward: -318.361, mean reward: -3.577 [-100.000, 36.316], mean action: 1.640 [0.000, 3.000],  loss: 25.003291, mse: 37431.615893, mean_q: 178.427398, mean_eps: 0.637258
 121074/300000: episode: 1174, duration: 0.795s, episode steps: 115, steps per second: 145, episode reward: -52.334, mean reward: -0.455 [-100.000, 16.962], mean action: 1.687 [0.000, 3.000],  loss: 35.748304, mse: 36312.126885, mean_q: 176.245539, mean_eps: 0.636952
 121158/300000: episode: 1175, duration: 0.587s, episode steps:  84, steps per second: 143, episode reward: -95.094, mean reward: -1.132 [-100.000, 11.431], mean action: 1.655 [0.000, 3.000],  loss: 24.075905, mse: 36507.948847, mean_q: 176.674704, mean_eps: 0.636654
 121283/300000: episode: 1176, duration: 0.913s, episode steps: 125, steps per second: 137, episode reward: 39.396, mean reward:  0.315 [-100.000, 43.197], mean action: 1.696 [0.000, 3.000],  loss: 32.348847, mse: 36781.296078, mean_q: 176.079728, mean_eps: 0.636340
 121410/300000: episode: 1177, duration: 0.941s, episode steps: 127, steps per second: 135, episode reward: -89.039, mean reward: -0.701 [-100.000,  4.717], mean action: 1.528 [0.000, 3.000],  loss: 28.331288, mse: 36830.263211, mean_q: 177.115769, mean_eps: 0.635962
 121524/300000: episode: 1178, duration: 0.854s, episode steps: 114, steps per second: 133, episode reward: -76.491, mean reward: -0.671 [-100.000, 14.882], mean action: 1.702 [0.000, 3.000],  loss: 25.177565, mse: 37683.640505, mean_q: 179.795447, mean_eps: 0.635601
 121601/300000: episode: 1179, duration: 0.534s, episode steps:  77, steps per second: 144, episode reward: -52.190, mean reward: -0.678 [-100.000,  7.034], mean action: 1.649 [0.000, 3.000],  loss: 30.929498, mse: 36885.944754, mean_q: 177.785776, mean_eps: 0.635314
 121677/300000: episode: 1180, duration: 0.534s, episode steps:  76, steps per second: 142, episode reward: -61.813, mean reward: -0.813 [-100.000, 10.296], mean action: 1.513 [0.000, 3.000],  loss: 25.289771, mse: 38350.180227, mean_q: 180.683395, mean_eps: 0.635084
 121800/300000: episode: 1181, duration: 0.898s, episode steps: 123, steps per second: 137, episode reward: -80.657, mean reward: -0.656 [-100.000,  7.279], mean action: 1.667 [0.000, 3.000],  loss: 38.270356, mse: 38420.552766, mean_q: 180.836885, mean_eps: 0.634786
 121921/300000: episode: 1182, duration: 0.850s, episode steps: 121, steps per second: 142, episode reward: -26.532, mean reward: -0.219 [-100.000, 25.755], mean action: 1.595 [0.000, 3.000],  loss: 32.059768, mse: 37965.163175, mean_q: 179.627330, mean_eps: 0.634420
 122006/300000: episode: 1183, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -30.394, mean reward: -0.358 [-100.000, 21.118], mean action: 1.635 [0.000, 3.000],  loss: 48.883677, mse: 37921.402229, mean_q: 179.972404, mean_eps: 0.634111
 122075/300000: episode: 1184, duration: 0.513s, episode steps:  69, steps per second: 134, episode reward: -46.451, mean reward: -0.673 [-100.000, 17.073], mean action: 1.667 [0.000, 3.000],  loss: 30.653895, mse: 37864.360762, mean_q: 180.838497, mean_eps: 0.633880
 122163/300000: episode: 1185, duration: 0.619s, episode steps:  88, steps per second: 142, episode reward: -91.302, mean reward: -1.038 [-100.000,  9.473], mean action: 1.670 [0.000, 3.000],  loss: 37.989322, mse: 37704.414506, mean_q: 179.086559, mean_eps: 0.633645
 122328/300000: episode: 1186, duration: 1.212s, episode steps: 165, steps per second: 136, episode reward: -57.372, mean reward: -0.348 [-100.000, 10.564], mean action: 1.800 [0.000, 3.000],  loss: 34.319254, mse: 38208.351610, mean_q: 180.895113, mean_eps: 0.633265
 122411/300000: episode: 1187, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -13.837, mean reward: -0.167 [-100.000, 16.505], mean action: 1.699 [0.000, 3.000],  loss: 29.151361, mse: 37623.823113, mean_q: 179.961807, mean_eps: 0.632893
 122509/300000: episode: 1188, duration: 0.698s, episode steps:  98, steps per second: 140, episode reward: -116.808, mean reward: -1.192 [-100.000, 12.979], mean action: 1.582 [0.000, 3.000],  loss: 30.803808, mse: 37015.897780, mean_q: 175.752496, mean_eps: 0.632621
 122620/300000: episode: 1189, duration: 0.781s, episode steps: 111, steps per second: 142, episode reward: -67.877, mean reward: -0.612 [-100.000, 25.195], mean action: 1.847 [0.000, 3.000],  loss: 24.766314, mse: 37695.183295, mean_q: 178.939802, mean_eps: 0.632308
 122735/300000: episode: 1190, duration: 0.852s, episode steps: 115, steps per second: 135, episode reward: -100.494, mean reward: -0.874 [-100.000, 31.873], mean action: 1.609 [0.000, 3.000],  loss: 25.230110, mse: 37781.674983, mean_q: 180.195012, mean_eps: 0.631969
 122853/300000: episode: 1191, duration: 0.820s, episode steps: 118, steps per second: 144, episode reward: -64.828, mean reward: -0.549 [-100.000, 17.177], mean action: 1.788 [0.000, 3.000],  loss: 24.095011, mse: 38364.355982, mean_q: 180.189743, mean_eps: 0.631619
 122930/300000: episode: 1192, duration: 0.586s, episode steps:  77, steps per second: 131, episode reward: -69.050, mean reward: -0.897 [-100.000,  8.167], mean action: 1.675 [0.000, 3.000],  loss: 22.181476, mse: 38745.523843, mean_q: 182.952870, mean_eps: 0.631327
 123036/300000: episode: 1193, duration: 0.854s, episode steps: 106, steps per second: 124, episode reward: -93.326, mean reward: -0.880 [-100.000,  7.378], mean action: 1.594 [0.000, 3.000],  loss: 33.914608, mse: 40154.861162, mean_q: 184.143746, mean_eps: 0.631053
 123180/300000: episode: 1194, duration: 1.007s, episode steps: 144, steps per second: 143, episode reward: -13.760, mean reward: -0.096 [-100.000, 10.241], mean action: 1.674 [0.000, 3.000],  loss: 27.886028, mse: 39425.678073, mean_q: 183.104803, mean_eps: 0.630678
 123311/300000: episode: 1195, duration: 0.936s, episode steps: 131, steps per second: 140, episode reward: -198.397, mean reward: -1.514 [-100.000, 77.450], mean action: 1.763 [0.000, 3.000],  loss: 29.819767, mse: 40253.185159, mean_q: 185.340619, mean_eps: 0.630265
 123378/300000: episode: 1196, duration: 0.484s, episode steps:  67, steps per second: 139, episode reward: -55.999, mean reward: -0.836 [-100.000,  7.778], mean action: 1.403 [0.000, 3.000],  loss: 21.240128, mse: 38009.369432, mean_q: 176.824976, mean_eps: 0.629968
 123486/300000: episode: 1197, duration: 0.795s, episode steps: 108, steps per second: 136, episode reward: -68.432, mean reward: -0.634 [-100.000, 45.319], mean action: 1.565 [0.000, 3.000],  loss: 36.659274, mse: 40656.047490, mean_q: 185.673994, mean_eps: 0.629706
 123589/300000: episode: 1198, duration: 0.779s, episode steps: 103, steps per second: 132, episode reward: -151.217, mean reward: -1.468 [-100.000, 15.114], mean action: 1.709 [0.000, 3.000],  loss: 33.721363, mse: 40279.603364, mean_q: 183.325945, mean_eps: 0.629389
 123729/300000: episode: 1199, duration: 0.997s, episode steps: 140, steps per second: 140, episode reward: -55.436, mean reward: -0.396 [-100.000, 20.376], mean action: 1.750 [0.000, 3.000],  loss: 22.766531, mse: 40978.457757, mean_q: 187.453151, mean_eps: 0.629024
 123828/300000: episode: 1200, duration: 0.700s, episode steps:  99, steps per second: 142, episode reward: -37.906, mean reward: -0.383 [-100.000,  6.126], mean action: 1.798 [0.000, 3.000],  loss: 26.522844, mse: 41078.743391, mean_q: 186.551342, mean_eps: 0.628666
 123914/300000: episode: 1201, duration: 0.628s, episode steps:  86, steps per second: 137, episode reward: -38.981, mean reward: -0.453 [-100.000, 13.577], mean action: 1.733 [0.000, 3.000],  loss: 38.767063, mse: 41224.046557, mean_q: 187.791866, mean_eps: 0.628389
 124041/300000: episode: 1202, duration: 0.907s, episode steps: 127, steps per second: 140, episode reward: -66.729, mean reward: -0.525 [-100.000, 41.370], mean action: 1.764 [0.000, 3.000],  loss: 27.108850, mse: 40402.303565, mean_q: 185.284136, mean_eps: 0.628069
 124138/300000: episode: 1203, duration: 0.791s, episode steps:  97, steps per second: 123, episode reward: -70.441, mean reward: -0.726 [-100.000,  8.054], mean action: 1.732 [0.000, 3.000],  loss: 31.978707, mse: 41002.061815, mean_q: 188.656767, mean_eps: 0.627733
 124239/300000: episode: 1204, duration: 0.871s, episode steps: 101, steps per second: 116, episode reward: -81.127, mean reward: -0.803 [-100.000, 23.526], mean action: 1.624 [0.000, 3.000],  loss: 25.024552, mse: 41824.743793, mean_q: 191.183756, mean_eps: 0.627436
 124319/300000: episode: 1205, duration: 0.568s, episode steps:  80, steps per second: 141, episode reward: -164.656, mean reward: -2.058 [-100.000,  6.972], mean action: 1.512 [0.000, 3.000],  loss: 31.328621, mse: 41417.062793, mean_q: 187.141425, mean_eps: 0.627164
 124439/300000: episode: 1206, duration: 0.853s, episode steps: 120, steps per second: 141, episode reward: -150.812, mean reward: -1.257 [-100.000,  8.790], mean action: 1.583 [0.000, 3.000],  loss: 28.661242, mse: 42801.508350, mean_q: 193.669924, mean_eps: 0.626864
 124518/300000: episode: 1207, duration: 0.569s, episode steps:  79, steps per second: 139, episode reward: -92.678, mean reward: -1.173 [-100.000,  5.403], mean action: 1.759 [0.000, 3.000],  loss: 44.604667, mse: 41378.155187, mean_q: 190.324886, mean_eps: 0.626566
 124798/300000: episode: 1208, duration: 1.982s, episode steps: 280, steps per second: 141, episode reward: -307.912, mean reward: -1.100 [-100.000,  6.991], mean action: 1.736 [0.000, 3.000],  loss: 32.938588, mse: 41919.048717, mean_q: 192.822997, mean_eps: 0.626027
 124877/300000: episode: 1209, duration: 0.544s, episode steps:  79, steps per second: 145, episode reward: -76.464, mean reward: -0.968 [-100.000,  8.256], mean action: 1.595 [0.000, 3.000],  loss: 40.648690, mse: 42006.892998, mean_q: 192.441096, mean_eps: 0.625489
 124989/300000: episode: 1210, duration: 0.787s, episode steps: 112, steps per second: 142, episode reward: -352.615, mean reward: -3.148 [-100.000, 61.177], mean action: 1.571 [0.000, 3.000],  loss: 33.660552, mse: 41191.568656, mean_q: 190.452306, mean_eps: 0.625203
 125098/300000: episode: 1211, duration: 0.782s, episode steps: 109, steps per second: 139, episode reward: -356.797, mean reward: -3.273 [-100.000, 49.627], mean action: 1.569 [0.000, 3.000],  loss: 22.092062, mse: 41903.185601, mean_q: 193.662633, mean_eps: 0.624871
 125171/300000: episode: 1212, duration: 0.501s, episode steps:  73, steps per second: 146, episode reward: -64.410, mean reward: -0.882 [-100.000, 22.366], mean action: 1.616 [0.000, 3.000],  loss: 43.243365, mse: 40999.863228, mean_q: 189.595123, mean_eps: 0.624598
 125349/300000: episode: 1213, duration: 1.275s, episode steps: 178, steps per second: 140, episode reward: -272.740, mean reward: -1.532 [-100.000, 37.735], mean action: 1.640 [0.000, 3.000],  loss: 37.420290, mse: 41628.030548, mean_q: 192.125778, mean_eps: 0.624221
 125444/300000: episode: 1214, duration: 0.661s, episode steps:  95, steps per second: 144, episode reward: -58.518, mean reward: -0.616 [-100.000, 61.053], mean action: 1.589 [0.000, 3.000],  loss: 35.020831, mse: 40547.326254, mean_q: 188.082890, mean_eps: 0.623812
 125521/300000: episode: 1215, duration: 0.540s, episode steps:  77, steps per second: 143, episode reward: -73.819, mean reward: -0.959 [-100.000, 17.496], mean action: 1.519 [0.000, 3.000],  loss: 27.043439, mse: 41022.093547, mean_q: 189.288332, mean_eps: 0.623554
 125623/300000: episode: 1216, duration: 0.700s, episode steps: 102, steps per second: 146, episode reward: -55.165, mean reward: -0.541 [-100.000,  7.251], mean action: 1.569 [0.000, 3.000],  loss: 34.175409, mse: 41614.159486, mean_q: 189.275856, mean_eps: 0.623285
 125701/300000: episode: 1217, duration: 0.548s, episode steps:  78, steps per second: 142, episode reward: -129.382, mean reward: -1.659 [-100.000,  6.377], mean action: 1.410 [0.000, 3.000],  loss: 34.539058, mse: 42216.146184, mean_q: 193.624140, mean_eps: 0.623016
 125774/300000: episode: 1218, duration: 0.512s, episode steps:  73, steps per second: 143, episode reward: -25.998, mean reward: -0.356 [-100.000, 10.394], mean action: 1.616 [0.000, 3.000],  loss: 34.250038, mse: 39643.123876, mean_q: 184.298533, mean_eps: 0.622789
 125870/300000: episode: 1219, duration: 0.662s, episode steps:  96, steps per second: 145, episode reward:  1.745, mean reward:  0.018 [-100.000, 13.213], mean action: 1.583 [0.000, 3.000],  loss: 48.702638, mse: 40654.071309, mean_q: 187.067844, mean_eps: 0.622536
 126010/300000: episode: 1220, duration: 1.009s, episode steps: 140, steps per second: 139, episode reward: -75.353, mean reward: -0.538 [-100.000,  8.749], mean action: 1.707 [0.000, 3.000],  loss: 35.631200, mse: 42053.916755, mean_q: 193.844289, mean_eps: 0.622182
 126166/300000: episode: 1221, duration: 1.100s, episode steps: 156, steps per second: 142, episode reward: -30.636, mean reward: -0.196 [-100.000, 16.332], mean action: 1.545 [0.000, 3.000],  loss: 45.760163, mse: 42189.010091, mean_q: 192.116166, mean_eps: 0.621737
 126305/300000: episode: 1222, duration: 1.007s, episode steps: 139, steps per second: 138, episode reward: -66.669, mean reward: -0.480 [-100.000, 15.035], mean action: 1.662 [0.000, 3.000],  loss: 42.028365, mse: 42342.021934, mean_q: 193.953926, mean_eps: 0.621295
 126403/300000: episode: 1223, duration: 0.683s, episode steps:  98, steps per second: 144, episode reward: -146.201, mean reward: -1.492 [-100.000,  6.461], mean action: 1.602 [0.000, 3.000],  loss: 45.763291, mse: 41215.165697, mean_q: 189.335528, mean_eps: 0.620939
 126551/300000: episode: 1224, duration: 1.034s, episode steps: 148, steps per second: 143, episode reward: -55.545, mean reward: -0.375 [-100.000, 10.443], mean action: 1.743 [0.000, 3.000],  loss: 37.725177, mse: 41689.908995, mean_q: 191.965556, mean_eps: 0.620571
 126668/300000: episode: 1225, duration: 0.843s, episode steps: 117, steps per second: 139, episode reward: -104.240, mean reward: -0.891 [-100.000, 14.082], mean action: 1.598 [0.000, 3.000],  loss: 25.194093, mse: 42070.721655, mean_q: 192.628578, mean_eps: 0.620173
 126752/300000: episode: 1226, duration: 0.586s, episode steps:  84, steps per second: 143, episode reward: -98.042, mean reward: -1.167 [-100.000,  5.454], mean action: 1.631 [0.000, 3.000],  loss: 40.458027, mse: 41747.421177, mean_q: 191.674659, mean_eps: 0.619872
 126879/300000: episode: 1227, duration: 0.874s, episode steps: 127, steps per second: 145, episode reward: -161.293, mean reward: -1.270 [-100.000, 49.966], mean action: 1.575 [0.000, 3.000],  loss: 42.007306, mse: 42412.936147, mean_q: 194.378473, mean_eps: 0.619555
 126982/300000: episode: 1228, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -100.082, mean reward: -0.972 [-100.000,  9.284], mean action: 1.718 [0.000, 3.000],  loss: 63.077193, mse: 42134.801312, mean_q: 193.945673, mean_eps: 0.619210
 127068/300000: episode: 1229, duration: 0.618s, episode steps:  86, steps per second: 139, episode reward: -66.496, mean reward: -0.773 [-100.000, 12.592], mean action: 1.663 [0.000, 3.000],  loss: 45.155733, mse: 43035.103811, mean_q: 196.992285, mean_eps: 0.618927
 127152/300000: episode: 1230, duration: 0.580s, episode steps:  84, steps per second: 145, episode reward: -24.299, mean reward: -0.289 [-100.000, 14.919], mean action: 1.440 [0.000, 3.000],  loss: 44.071299, mse: 44163.575474, mean_q: 200.420890, mean_eps: 0.618672
 127257/300000: episode: 1231, duration: 0.774s, episode steps: 105, steps per second: 136, episode reward: -9.951, mean reward: -0.095 [-100.000, 13.531], mean action: 1.667 [0.000, 3.000],  loss: 25.425950, mse: 43087.813858, mean_q: 196.933605, mean_eps: 0.618388
 127335/300000: episode: 1232, duration: 0.543s, episode steps:  78, steps per second: 144, episode reward: -72.127, mean reward: -0.925 [-100.000,  6.739], mean action: 1.679 [0.000, 3.000],  loss: 46.328576, mse: 43422.477990, mean_q: 197.419740, mean_eps: 0.618113
 127416/300000: episode: 1233, duration: 0.577s, episode steps:  81, steps per second: 140, episode reward: -57.924, mean reward: -0.715 [-100.000, 15.382], mean action: 1.802 [0.000, 3.000],  loss: 31.005368, mse: 42730.590905, mean_q: 195.564073, mean_eps: 0.617875
 127497/300000: episode: 1234, duration: 0.592s, episode steps:  81, steps per second: 137, episode reward: -21.001, mean reward: -0.259 [-100.000, 14.531], mean action: 1.765 [0.000, 3.000],  loss: 43.616543, mse: 43271.259983, mean_q: 197.103431, mean_eps: 0.617632
 127640/300000: episode: 1235, duration: 1.116s, episode steps: 143, steps per second: 128, episode reward: -56.961, mean reward: -0.398 [-100.000,  7.720], mean action: 1.594 [0.000, 3.000],  loss: 41.655079, mse: 43695.960255, mean_q: 198.280784, mean_eps: 0.617296
 127759/300000: episode: 1236, duration: 0.996s, episode steps: 119, steps per second: 119, episode reward: -124.720, mean reward: -1.048 [-100.000, 16.738], mean action: 1.597 [0.000, 3.000],  loss: 53.515441, mse: 43770.047679, mean_q: 199.515686, mean_eps: 0.616903
 127924/300000: episode: 1237, duration: 1.322s, episode steps: 165, steps per second: 125, episode reward: -126.435, mean reward: -0.766 [-100.000,  9.569], mean action: 1.758 [0.000, 3.000],  loss: 36.546856, mse: 43866.581013, mean_q: 200.436868, mean_eps: 0.616477
 128061/300000: episode: 1238, duration: 1.057s, episode steps: 137, steps per second: 130, episode reward: -86.071, mean reward: -0.628 [-100.000, 13.347], mean action: 1.445 [0.000, 3.000],  loss: 42.232210, mse: 43991.080748, mean_q: 200.133569, mean_eps: 0.616024
 128175/300000: episode: 1239, duration: 0.869s, episode steps: 114, steps per second: 131, episode reward: -78.354, mean reward: -0.687 [-100.000,  6.198], mean action: 1.684 [0.000, 3.000],  loss: 30.053674, mse: 44197.428317, mean_q: 201.162232, mean_eps: 0.615648
 128306/300000: episode: 1240, duration: 0.926s, episode steps: 131, steps per second: 142, episode reward: -76.093, mean reward: -0.581 [-100.000, 18.033], mean action: 1.832 [0.000, 3.000],  loss: 47.104404, mse: 45097.726861, mean_q: 204.909434, mean_eps: 0.615280
 128385/300000: episode: 1241, duration: 0.578s, episode steps:  79, steps per second: 137, episode reward: -61.723, mean reward: -0.781 [-100.000, 16.058], mean action: 1.570 [0.000, 3.000],  loss: 54.445798, mse: 42512.361452, mean_q: 196.459026, mean_eps: 0.614965
 128481/300000: episode: 1242, duration: 0.666s, episode steps:  96, steps per second: 144, episode reward: -93.133, mean reward: -0.970 [-100.000,  9.670], mean action: 1.698 [0.000, 3.000],  loss: 51.894926, mse: 43804.785522, mean_q: 200.727796, mean_eps: 0.614703
 128596/300000: episode: 1243, duration: 0.816s, episode steps: 115, steps per second: 141, episode reward: -71.337, mean reward: -0.620 [-100.000, 10.908], mean action: 1.591 [0.000, 3.000],  loss: 49.776181, mse: 43307.334851, mean_q: 200.221759, mean_eps: 0.614386
 128708/300000: episode: 1244, duration: 0.813s, episode steps: 112, steps per second: 138, episode reward: -126.004, mean reward: -1.125 [-100.000, 22.623], mean action: 1.705 [0.000, 3.000],  loss: 24.529883, mse: 43354.043910, mean_q: 200.564250, mean_eps: 0.614046
 128902/300000: episode: 1245, duration: 1.378s, episode steps: 194, steps per second: 141, episode reward: -86.563, mean reward: -0.446 [-100.000,  9.993], mean action: 1.753 [0.000, 3.000],  loss: 44.603599, mse: 43375.595159, mean_q: 201.729312, mean_eps: 0.613587
 128998/300000: episode: 1246, duration: 0.678s, episode steps:  96, steps per second: 142, episode reward: -2.481, mean reward: -0.026 [-100.000,  6.987], mean action: 1.802 [0.000, 3.000],  loss: 28.801915, mse: 43635.848145, mean_q: 201.314948, mean_eps: 0.613151
 129099/300000: episode: 1247, duration: 0.715s, episode steps: 101, steps per second: 141, episode reward: -66.984, mean reward: -0.663 [-100.000, 19.738], mean action: 1.634 [0.000, 3.000],  loss: 46.358967, mse: 43283.897548, mean_q: 199.516436, mean_eps: 0.612856
 129227/300000: episode: 1248, duration: 0.965s, episode steps: 128, steps per second: 133, episode reward: -163.534, mean reward: -1.278 [-100.000,  6.386], mean action: 1.656 [0.000, 3.000],  loss: 40.405681, mse: 44654.236893, mean_q: 205.518825, mean_eps: 0.612513
 129336/300000: episode: 1249, duration: 0.833s, episode steps: 109, steps per second: 131, episode reward: -241.476, mean reward: -2.215 [-100.000, 37.512], mean action: 1.587 [0.000, 3.000],  loss: 38.569899, mse: 44817.019280, mean_q: 204.554528, mean_eps: 0.612157
 129431/300000: episode: 1250, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -57.273, mean reward: -0.603 [-100.000, 10.956], mean action: 1.705 [0.000, 3.000],  loss: 43.197951, mse: 43946.692311, mean_q: 202.560443, mean_eps: 0.611851
 130431/300000: episode: 1251, duration: 7.689s, episode steps: 1000, steps per second: 130, episode reward: -39.638, mean reward: -0.040 [-24.041, 52.255], mean action: 1.703 [0.000, 3.000],  loss: 51.050861, mse: 46932.251410, mean_q: 209.254052, mean_eps: 0.610208
 130545/300000: episode: 1252, duration: 0.782s, episode steps: 114, steps per second: 146, episode reward: -59.440, mean reward: -0.521 [-100.000, 17.401], mean action: 1.518 [0.000, 3.000],  loss: 47.733371, mse: 49433.775768, mean_q: 215.998906, mean_eps: 0.608538
 130635/300000: episode: 1253, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -29.535, mean reward: -0.328 [-100.000, 12.692], mean action: 1.822 [0.000, 3.000],  loss: 87.333045, mse: 49462.984462, mean_q: 215.387541, mean_eps: 0.608231
 130751/300000: episode: 1254, duration: 0.957s, episode steps: 116, steps per second: 121, episode reward: -57.313, mean reward: -0.494 [-100.000, 39.650], mean action: 1.586 [0.000, 3.000],  loss: 63.410291, mse: 49796.413793, mean_q: 216.197944, mean_eps: 0.607922
 130826/300000: episode: 1255, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -61.160, mean reward: -0.815 [-100.000,  7.505], mean action: 1.747 [0.000, 3.000],  loss: 61.177835, mse: 49298.371458, mean_q: 216.584123, mean_eps: 0.607636
 130928/300000: episode: 1256, duration: 0.725s, episode steps: 102, steps per second: 141, episode reward: -1.782, mean reward: -0.017 [-100.000, 20.634], mean action: 1.775 [0.000, 3.000],  loss: 47.710868, mse: 50335.197457, mean_q: 218.625758, mean_eps: 0.607371
 131042/300000: episode: 1257, duration: 0.814s, episode steps: 114, steps per second: 140, episode reward: -60.098, mean reward: -0.527 [-100.000, 10.888], mean action: 1.640 [0.000, 3.000],  loss: 76.279338, mse: 49891.251508, mean_q: 218.088200, mean_eps: 0.607046
 131125/300000: episode: 1258, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -64.763, mean reward: -0.780 [-100.000,  9.587], mean action: 1.554 [0.000, 3.000],  loss: 60.847059, mse: 49624.604433, mean_q: 216.277848, mean_eps: 0.606751
 131262/300000: episode: 1259, duration: 0.962s, episode steps: 137, steps per second: 142, episode reward: -82.223, mean reward: -0.600 [-100.000, 12.008], mean action: 1.672 [0.000, 3.000],  loss: 71.572270, mse: 50600.842838, mean_q: 220.822736, mean_eps: 0.606421
 131378/300000: episode: 1260, duration: 0.813s, episode steps: 116, steps per second: 143, episode reward: -49.687, mean reward: -0.428 [-100.000, 14.182], mean action: 1.690 [0.000, 3.000],  loss: 98.841319, mse: 50531.138672, mean_q: 221.205098, mean_eps: 0.606042
 131472/300000: episode: 1261, duration: 0.648s, episode steps:  94, steps per second: 145, episode reward: -25.662, mean reward: -0.273 [-100.000, 18.550], mean action: 1.787 [0.000, 3.000],  loss: 60.279079, mse: 50809.873379, mean_q: 219.420796, mean_eps: 0.605726
 131618/300000: episode: 1262, duration: 1.043s, episode steps: 146, steps per second: 140, episode reward: -27.771, mean reward: -0.190 [-100.000, 14.116], mean action: 1.705 [0.000, 3.000],  loss: 60.409001, mse: 50899.078633, mean_q: 221.936810, mean_eps: 0.605367
 131753/300000: episode: 1263, duration: 0.932s, episode steps: 135, steps per second: 145, episode reward: -31.948, mean reward: -0.237 [-100.000,  8.934], mean action: 1.519 [0.000, 3.000],  loss: 65.362772, mse: 49859.137384, mean_q: 219.919259, mean_eps: 0.604945
 131862/300000: episode: 1264, duration: 0.777s, episode steps: 109, steps per second: 140, episode reward: -36.529, mean reward: -0.335 [-100.000, 15.516], mean action: 1.596 [0.000, 3.000],  loss: 34.094249, mse: 49679.602960, mean_q: 218.069454, mean_eps: 0.604579
 131961/300000: episode: 1265, duration: 0.698s, episode steps:  99, steps per second: 142, episode reward: -83.734, mean reward: -0.846 [-100.000, 10.067], mean action: 1.566 [0.000, 3.000],  loss: 42.069321, mse: 50874.306108, mean_q: 222.211126, mean_eps: 0.604267
 132076/300000: episode: 1266, duration: 0.797s, episode steps: 115, steps per second: 144, episode reward: -45.734, mean reward: -0.398 [-100.000,  9.289], mean action: 1.791 [0.000, 3.000],  loss: 39.402014, mse: 50081.910088, mean_q: 217.992185, mean_eps: 0.603946
 132159/300000: episode: 1267, duration: 0.579s, episode steps:  83, steps per second: 143, episode reward: -88.141, mean reward: -1.062 [-100.000, 18.628], mean action: 1.602 [0.000, 3.000],  loss: 38.328105, mse: 51171.780732, mean_q: 221.480522, mean_eps: 0.603649
 132255/300000: episode: 1268, duration: 0.700s, episode steps:  96, steps per second: 137, episode reward: -93.642, mean reward: -0.975 [-100.000, 42.419], mean action: 1.677 [0.000, 3.000],  loss: 55.913061, mse: 52381.360840, mean_q: 225.940123, mean_eps: 0.603380
 132357/300000: episode: 1269, duration: 0.711s, episode steps: 102, steps per second: 143, episode reward: -77.081, mean reward: -0.756 [-100.000, 11.868], mean action: 1.637 [0.000, 3.000],  loss: 41.186920, mse: 50922.558402, mean_q: 219.923635, mean_eps: 0.603084
 132426/300000: episode: 1270, duration: 0.476s, episode steps:  69, steps per second: 145, episode reward: -47.857, mean reward: -0.694 [-100.000, 12.701], mean action: 1.783 [0.000, 3.000],  loss: 44.028976, mse: 51041.973449, mean_q: 219.350485, mean_eps: 0.602827
 132553/300000: episode: 1271, duration: 0.895s, episode steps: 127, steps per second: 142, episode reward: -91.711, mean reward: -0.722 [-100.000, 18.234], mean action: 1.669 [0.000, 3.000],  loss: 51.215691, mse: 51096.644377, mean_q: 218.337644, mean_eps: 0.602533
 132660/300000: episode: 1272, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -95.671, mean reward: -0.894 [-100.000,  6.011], mean action: 1.598 [0.000, 3.000],  loss: 49.655715, mse: 52260.335426, mean_q: 220.922566, mean_eps: 0.602182
 132769/300000: episode: 1273, duration: 0.754s, episode steps: 109, steps per second: 145, episode reward: -78.628, mean reward: -0.721 [-100.000, 11.261], mean action: 1.725 [0.000, 3.000],  loss: 51.268587, mse: 51776.882239, mean_q: 219.373049, mean_eps: 0.601858
 132834/300000: episode: 1274, duration: 0.462s, episode steps:  65, steps per second: 141, episode reward: -61.220, mean reward: -0.942 [-100.000, 20.032], mean action: 1.662 [0.000, 3.000],  loss: 59.160576, mse: 51992.851683, mean_q: 220.454506, mean_eps: 0.601597
 132929/300000: episode: 1275, duration: 0.680s, episode steps:  95, steps per second: 140, episode reward: -23.426, mean reward: -0.247 [-100.000,  8.228], mean action: 1.779 [0.000, 3.000],  loss: 40.965668, mse: 53083.217311, mean_q: 222.125097, mean_eps: 0.601357
 133062/300000: episode: 1276, duration: 0.938s, episode steps: 133, steps per second: 142, episode reward: -27.377, mean reward: -0.206 [-100.000, 13.736], mean action: 1.677 [0.000, 3.000],  loss: 51.001376, mse: 53994.554570, mean_q: 226.331896, mean_eps: 0.601015
 134062/300000: episode: 1277, duration: 8.042s, episode steps: 1000, steps per second: 124, episode reward: 30.202, mean reward:  0.030 [-24.097, 24.632], mean action: 1.749 [0.000, 3.000],  loss: 48.743418, mse: 55237.788863, mean_q: 228.517343, mean_eps: 0.599315
 134161/300000: episode: 1278, duration: 0.706s, episode steps:  99, steps per second: 140, episode reward: -67.032, mean reward: -0.677 [-100.000,  7.808], mean action: 1.818 [0.000, 3.000],  loss: 44.799667, mse: 57333.209557, mean_q: 235.330502, mean_eps: 0.597667
 134272/300000: episode: 1279, duration: 0.791s, episode steps: 111, steps per second: 140, episode reward:  0.677, mean reward:  0.006 [-100.000, 71.333], mean action: 1.694 [0.000, 3.000],  loss: 39.920626, mse: 58238.144848, mean_q: 238.252236, mean_eps: 0.597352
 134386/300000: episode: 1280, duration: 0.789s, episode steps: 114, steps per second: 144, episode reward: -93.721, mean reward: -0.822 [-100.000, 11.839], mean action: 1.684 [0.000, 3.000],  loss: 45.107250, mse: 56784.138192, mean_q: 231.663151, mean_eps: 0.597015
 134485/300000: episode: 1281, duration: 0.709s, episode steps:  99, steps per second: 140, episode reward: -42.092, mean reward: -0.425 [-100.000,  8.720], mean action: 1.535 [0.000, 3.000],  loss: 47.760116, mse: 57586.101247, mean_q: 235.016279, mean_eps: 0.596695
 134595/300000: episode: 1282, duration: 0.763s, episode steps: 110, steps per second: 144, episode reward: -47.330, mean reward: -0.430 [-100.000, 20.876], mean action: 1.573 [0.000, 3.000],  loss: 43.227573, mse: 57070.985192, mean_q: 233.781064, mean_eps: 0.596381
 134683/300000: episode: 1283, duration: 0.608s, episode steps:  88, steps per second: 145, episode reward: -17.042, mean reward: -0.194 [-100.000, 24.325], mean action: 1.795 [0.000, 3.000],  loss: 45.932421, mse: 58948.059792, mean_q: 237.573354, mean_eps: 0.596085
 134812/300000: episode: 1284, duration: 0.958s, episode steps: 129, steps per second: 135, episode reward: -59.793, mean reward: -0.464 [-100.000,  9.230], mean action: 1.628 [0.000, 3.000],  loss: 53.751308, mse: 58159.899376, mean_q: 235.469838, mean_eps: 0.595759
 134920/300000: episode: 1285, duration: 0.767s, episode steps: 108, steps per second: 141, episode reward: -54.572, mean reward: -0.505 [-100.000, 18.895], mean action: 1.694 [0.000, 3.000],  loss: 70.448368, mse: 56760.812247, mean_q: 231.570118, mean_eps: 0.595403
 135043/300000: episode: 1286, duration: 0.847s, episode steps: 123, steps per second: 145, episode reward: -73.609, mean reward: -0.598 [-100.000,  9.665], mean action: 1.797 [0.000, 3.000],  loss: 70.420596, mse: 57162.680958, mean_q: 233.382228, mean_eps: 0.595057
 135119/300000: episode: 1287, duration: 0.554s, episode steps:  76, steps per second: 137, episode reward: -129.651, mean reward: -1.706 [-100.000, 14.693], mean action: 1.500 [0.000, 3.000],  loss: 45.954411, mse: 57227.478207, mean_q: 232.883789, mean_eps: 0.594758
 135259/300000: episode: 1288, duration: 0.991s, episode steps: 140, steps per second: 141, episode reward: -4.427, mean reward: -0.032 [-100.000, 58.538], mean action: 1.643 [0.000, 3.000],  loss: 34.316343, mse: 55914.869001, mean_q: 231.694049, mean_eps: 0.594434
 135399/300000: episode: 1289, duration: 0.982s, episode steps: 140, steps per second: 143, episode reward: 11.714, mean reward:  0.084 [-100.000, 17.507], mean action: 1.764 [0.000, 3.000],  loss: 78.539385, mse: 56855.377846, mean_q: 232.906430, mean_eps: 0.594014
 135529/300000: episode: 1290, duration: 0.922s, episode steps: 130, steps per second: 141, episode reward: 26.647, mean reward:  0.205 [-100.000, 16.631], mean action: 1.700 [0.000, 3.000],  loss: 47.589798, mse: 55258.728456, mean_q: 229.263872, mean_eps: 0.593609
 135634/300000: episode: 1291, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward: -27.749, mean reward: -0.264 [-100.000, 11.929], mean action: 1.590 [0.000, 3.000],  loss: 62.189361, mse: 54605.616369, mean_q: 227.672732, mean_eps: 0.593257
 135702/300000: episode: 1292, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -54.920, mean reward: -0.808 [-100.000, 17.111], mean action: 1.706 [0.000, 3.000],  loss: 44.530426, mse: 55436.698357, mean_q: 228.909106, mean_eps: 0.592998
 135845/300000: episode: 1293, duration: 0.999s, episode steps: 143, steps per second: 143, episode reward: 11.941, mean reward:  0.084 [-100.000,  8.654], mean action: 1.741 [0.000, 3.000],  loss: 67.950297, mse: 55496.562227, mean_q: 229.468816, mean_eps: 0.592681
 136159/300000: episode: 1294, duration: 2.239s, episode steps: 314, steps per second: 140, episode reward: -243.611, mean reward: -0.776 [-100.000, 12.240], mean action: 1.643 [0.000, 3.000],  loss: 55.433280, mse: 55959.464371, mean_q: 230.363258, mean_eps: 0.591996
 136359/300000: episode: 1295, duration: 1.439s, episode steps: 200, steps per second: 139, episode reward: -54.508, mean reward: -0.273 [-100.000, 15.266], mean action: 1.730 [0.000, 3.000],  loss: 67.949813, mse: 57262.337285, mean_q: 235.164608, mean_eps: 0.591225
 136461/300000: episode: 1296, duration: 0.728s, episode steps: 102, steps per second: 140, episode reward: -80.681, mean reward: -0.791 [-100.000,  9.197], mean action: 1.775 [0.000, 3.000],  loss: 65.396094, mse: 58133.312155, mean_q: 238.415793, mean_eps: 0.590772
 136669/300000: episode: 1297, duration: 1.645s, episode steps: 208, steps per second: 126, episode reward: 50.173, mean reward:  0.241 [-100.000, 14.532], mean action: 1.697 [0.000, 3.000],  loss: 83.493680, mse: 58137.096567, mean_q: 237.773226, mean_eps: 0.590307
 136773/300000: episode: 1298, duration: 0.907s, episode steps: 104, steps per second: 115, episode reward: 10.145, mean reward:  0.098 [-100.000, 66.555], mean action: 1.606 [0.000, 3.000],  loss: 100.862687, mse: 58534.084473, mean_q: 241.272670, mean_eps: 0.589839
 136890/300000: episode: 1299, duration: 0.983s, episode steps: 117, steps per second: 119, episode reward: -57.020, mean reward: -0.487 [-100.000, 15.283], mean action: 1.701 [0.000, 3.000],  loss: 50.752678, mse: 58616.269298, mean_q: 239.499697, mean_eps: 0.589507
 136983/300000: episode: 1300, duration: 0.750s, episode steps:  93, steps per second: 124, episode reward: -53.965, mean reward: -0.580 [-100.000,  9.940], mean action: 1.624 [0.000, 3.000],  loss: 70.314382, mse: 57914.427167, mean_q: 238.056942, mean_eps: 0.589192
 137089/300000: episode: 1301, duration: 0.824s, episode steps: 106, steps per second: 129, episode reward: -79.240, mean reward: -0.748 [-100.000,  6.153], mean action: 1.358 [0.000, 3.000],  loss: 66.847427, mse: 58506.858491, mean_q: 239.879781, mean_eps: 0.588893
 137168/300000: episode: 1302, duration: 0.636s, episode steps:  79, steps per second: 124, episode reward: -61.384, mean reward: -0.777 [-100.000,  5.174], mean action: 1.835 [0.000, 3.000],  loss: 40.681334, mse: 59988.226661, mean_q: 243.111873, mean_eps: 0.588616
 137265/300000: episode: 1303, duration: 0.690s, episode steps:  97, steps per second: 141, episode reward: -22.745, mean reward: -0.234 [-100.000, 11.960], mean action: 1.804 [0.000, 3.000],  loss: 104.821783, mse: 57932.910237, mean_q: 237.875061, mean_eps: 0.588352
 137364/300000: episode: 1304, duration: 0.706s, episode steps:  99, steps per second: 140, episode reward: -43.077, mean reward: -0.435 [-100.000,  7.741], mean action: 1.545 [0.000, 3.000],  loss: 67.457258, mse: 57614.523674, mean_q: 235.946911, mean_eps: 0.588058
 137491/300000: episode: 1305, duration: 0.900s, episode steps: 127, steps per second: 141, episode reward: -2.643, mean reward: -0.021 [-100.000, 10.977], mean action: 1.756 [0.000, 3.000],  loss: 52.681700, mse: 57285.393701, mean_q: 237.295272, mean_eps: 0.587719
 137589/300000: episode: 1306, duration: 0.681s, episode steps:  98, steps per second: 144, episode reward: -36.879, mean reward: -0.376 [-100.000,  9.055], mean action: 1.714 [0.000, 3.000],  loss: 43.399211, mse: 57733.147282, mean_q: 238.648162, mean_eps: 0.587381
 137666/300000: episode: 1307, duration: 0.540s, episode steps:  77, steps per second: 143, episode reward: -73.993, mean reward: -0.961 [-100.000, 20.780], mean action: 1.701 [0.000, 3.000],  loss: 68.452104, mse: 58245.322545, mean_q: 238.585419, mean_eps: 0.587119
 137769/300000: episode: 1308, duration: 0.731s, episode steps: 103, steps per second: 141, episode reward: -101.856, mean reward: -0.989 [-100.000, 11.171], mean action: 1.641 [0.000, 3.000],  loss: 73.715344, mse: 57975.094433, mean_q: 238.906300, mean_eps: 0.586849
 137898/300000: episode: 1309, duration: 0.908s, episode steps: 129, steps per second: 142, episode reward: -29.492, mean reward: -0.229 [-100.000, 17.685], mean action: 1.690 [0.000, 3.000],  loss: 64.029821, mse: 58575.635810, mean_q: 240.154128, mean_eps: 0.586501
 138028/300000: episode: 1310, duration: 0.970s, episode steps: 130, steps per second: 134, episode reward: -53.776, mean reward: -0.414 [-100.000, 12.752], mean action: 1.792 [0.000, 3.000],  loss: 61.180270, mse: 58846.880409, mean_q: 240.175462, mean_eps: 0.586112
 138141/300000: episode: 1311, duration: 0.883s, episode steps: 113, steps per second: 128, episode reward: -5.416, mean reward: -0.048 [-100.000, 41.480], mean action: 1.699 [0.000, 3.000],  loss: 71.592578, mse: 59804.950083, mean_q: 243.917896, mean_eps: 0.585748
 138213/300000: episode: 1312, duration: 0.541s, episode steps:  72, steps per second: 133, episode reward: -55.139, mean reward: -0.766 [-100.000,  4.857], mean action: 1.778 [0.000, 3.000],  loss: 52.228143, mse: 59001.560493, mean_q: 240.542663, mean_eps: 0.585471
 138328/300000: episode: 1313, duration: 0.876s, episode steps: 115, steps per second: 131, episode reward: -16.421, mean reward: -0.143 [-100.000, 16.336], mean action: 1.922 [0.000, 3.000],  loss: 90.479881, mse: 59497.430707, mean_q: 241.055042, mean_eps: 0.585190
 138479/300000: episode: 1314, duration: 1.150s, episode steps: 151, steps per second: 131, episode reward: -26.837, mean reward: -0.178 [-100.000, 21.228], mean action: 1.550 [0.000, 3.000],  loss: 72.312355, mse: 58759.342094, mean_q: 241.292667, mean_eps: 0.584791
 138562/300000: episode: 1315, duration: 0.615s, episode steps:  83, steps per second: 135, episode reward: -181.706, mean reward: -2.189 [-100.000,  5.174], mean action: 1.675 [0.000, 3.000],  loss: 99.563108, mse: 57904.465597, mean_q: 238.830543, mean_eps: 0.584440
 138726/300000: episode: 1316, duration: 1.196s, episode steps: 164, steps per second: 137, episode reward: -9.442, mean reward: -0.058 [-100.000, 14.243], mean action: 1.677 [0.000, 3.000],  loss: 49.975306, mse: 59270.127310, mean_q: 240.770109, mean_eps: 0.584070
 138822/300000: episode: 1317, duration: 0.660s, episode steps:  96, steps per second: 145, episode reward: -80.475, mean reward: -0.838 [-100.000,  6.312], mean action: 1.688 [0.000, 3.000],  loss: 56.416294, mse: 58736.440511, mean_q: 238.331279, mean_eps: 0.583680
 138983/300000: episode: 1318, duration: 1.153s, episode steps: 161, steps per second: 140, episode reward: -53.513, mean reward: -0.332 [-100.000, 10.744], mean action: 1.615 [0.000, 3.000],  loss: 63.649698, mse: 60548.880483, mean_q: 245.562206, mean_eps: 0.583294
 139091/300000: episode: 1319, duration: 0.759s, episode steps: 108, steps per second: 142, episode reward: -40.863, mean reward: -0.378 [-100.000, 17.308], mean action: 1.583 [0.000, 3.000],  loss: 66.532239, mse: 61240.196108, mean_q: 245.273345, mean_eps: 0.582890
 140091/300000: episode: 1320, duration: 7.948s, episode steps: 1000, steps per second: 126, episode reward: -55.096, mean reward: -0.055 [-23.037, 19.373], mean action: 1.717 [0.000, 3.000],  loss: 105.957099, mse: 63188.391461, mean_q: 250.739611, mean_eps: 0.581228
 140185/300000: episode: 1321, duration: 0.654s, episode steps:  94, steps per second: 144, episode reward:  4.259, mean reward:  0.045 [-100.000, 14.486], mean action: 1.766 [0.000, 3.000],  loss: 97.043681, mse: 66675.140708, mean_q: 259.845249, mean_eps: 0.579587
 140304/300000: episode: 1322, duration: 0.858s, episode steps: 119, steps per second: 139, episode reward: -79.241, mean reward: -0.666 [-100.000, 10.882], mean action: 1.403 [0.000, 3.000],  loss: 86.364778, mse: 65928.640231, mean_q: 257.037955, mean_eps: 0.579268
 140407/300000: episode: 1323, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -60.494, mean reward: -0.587 [-100.000,  9.056], mean action: 1.650 [0.000, 3.000],  loss: 114.789805, mse: 64362.280492, mean_q: 254.706390, mean_eps: 0.578935
 140547/300000: episode: 1324, duration: 0.990s, episode steps: 140, steps per second: 141, episode reward: -44.327, mean reward: -0.317 [-100.000, 15.467], mean action: 1.686 [0.000, 3.000],  loss: 122.159716, mse: 67060.090095, mean_q: 260.455276, mean_eps: 0.578570
 140662/300000: episode: 1325, duration: 0.808s, episode steps: 115, steps per second: 142, episode reward: -47.212, mean reward: -0.411 [-100.000, 10.821], mean action: 1.678 [0.000, 3.000],  loss: 90.440071, mse: 67909.125476, mean_q: 261.706825, mean_eps: 0.578188
 140804/300000: episode: 1326, duration: 0.998s, episode steps: 142, steps per second: 142, episode reward: -7.418, mean reward: -0.052 [-100.000, 10.571], mean action: 1.690 [0.000, 3.000],  loss: 86.724800, mse: 68380.235310, mean_q: 261.609816, mean_eps: 0.577802
 140896/300000: episode: 1327, duration: 0.632s, episode steps:  92, steps per second: 146, episode reward: -29.158, mean reward: -0.317 [-100.000, 12.488], mean action: 1.674 [0.000, 3.000],  loss: 96.644420, mse: 68086.969684, mean_q: 260.675896, mean_eps: 0.577451
 141051/300000: episode: 1328, duration: 1.101s, episode steps: 155, steps per second: 141, episode reward: -6.336, mean reward: -0.041 [-100.000, 20.916], mean action: 1.781 [0.000, 3.000],  loss: 107.026165, mse: 66794.927293, mean_q: 259.997571, mean_eps: 0.577081
 141588/300000: episode: 1329, duration: 3.932s, episode steps: 537, steps per second: 137, episode reward: -43.246, mean reward: -0.081 [-100.000, 16.256], mean action: 1.760 [0.000, 3.000],  loss: 126.897574, mse: 67331.855723, mean_q: 260.056425, mean_eps: 0.576043
 141697/300000: episode: 1330, duration: 0.751s, episode steps: 109, steps per second: 145, episode reward: -80.178, mean reward: -0.736 [-100.000,  8.661], mean action: 1.587 [0.000, 3.000],  loss: 148.238405, mse: 69349.742833, mean_q: 262.917190, mean_eps: 0.575074
 141779/300000: episode: 1331, duration: 0.575s, episode steps:  82, steps per second: 142, episode reward: -30.703, mean reward: -0.374 [-100.000,  8.339], mean action: 1.634 [0.000, 3.000],  loss: 107.398199, mse: 69305.506002, mean_q: 262.492136, mean_eps: 0.574787
 141908/300000: episode: 1332, duration: 0.933s, episode steps: 129, steps per second: 138, episode reward:  1.174, mean reward:  0.009 [-100.000, 47.168], mean action: 1.674 [0.000, 3.000],  loss: 145.867269, mse: 67135.778888, mean_q: 260.921870, mean_eps: 0.574471
 142016/300000: episode: 1333, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: 19.751, mean reward:  0.183 [-100.000, 17.573], mean action: 1.796 [0.000, 3.000],  loss: 134.432208, mse: 67126.198749, mean_q: 257.943989, mean_eps: 0.574115
 142125/300000: episode: 1334, duration: 0.782s, episode steps: 109, steps per second: 139, episode reward: -32.405, mean reward: -0.297 [-100.000,  9.576], mean action: 1.624 [0.000, 3.000],  loss: 111.846014, mse: 67695.455096, mean_q: 260.218048, mean_eps: 0.573790
 142221/300000: episode: 1335, duration: 0.687s, episode steps:  96, steps per second: 140, episode reward: -45.245, mean reward: -0.471 [-100.000, 18.849], mean action: 1.844 [0.000, 3.000],  loss: 120.884127, mse: 69255.165120, mean_q: 263.101167, mean_eps: 0.573483
 142307/300000: episode: 1336, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -32.906, mean reward: -0.383 [-100.000, 21.264], mean action: 1.860 [0.000, 3.000],  loss: 110.303515, mse: 68785.214980, mean_q: 262.509795, mean_eps: 0.573209
 142415/300000: episode: 1337, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: -26.420, mean reward: -0.245 [-100.000, 35.957], mean action: 1.370 [0.000, 3.000],  loss: 65.004449, mse: 70171.065574, mean_q: 264.934928, mean_eps: 0.572918
 142516/300000: episode: 1338, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: -71.074, mean reward: -0.704 [-100.000,  8.491], mean action: 1.475 [0.000, 3.000],  loss: 108.055944, mse: 70856.391128, mean_q: 265.857732, mean_eps: 0.572605
 142625/300000: episode: 1339, duration: 0.758s, episode steps: 109, steps per second: 144, episode reward: -112.478, mean reward: -1.032 [-100.000, 15.403], mean action: 1.477 [0.000, 3.000],  loss: 137.605105, mse: 71603.403992, mean_q: 268.406588, mean_eps: 0.572290
 142718/300000: episode: 1340, duration: 0.671s, episode steps:  93, steps per second: 139, episode reward: -30.574, mean reward: -0.329 [-100.000, 12.817], mean action: 1.645 [0.000, 3.000],  loss: 136.732507, mse: 69511.163390, mean_q: 262.456924, mean_eps: 0.571987
 142863/300000: episode: 1341, duration: 1.014s, episode steps: 145, steps per second: 143, episode reward: -9.084, mean reward: -0.063 [-100.000, 25.411], mean action: 1.752 [0.000, 3.000],  loss: 149.071144, mse: 68857.866298, mean_q: 262.612810, mean_eps: 0.571630
 142949/300000: episode: 1342, duration: 0.593s, episode steps:  86, steps per second: 145, episode reward: -52.948, mean reward: -0.616 [-100.000,  8.557], mean action: 1.581 [0.000, 3.000],  loss: 225.088591, mse: 68811.425963, mean_q: 263.933504, mean_eps: 0.571284
 143051/300000: episode: 1343, duration: 0.730s, episode steps: 102, steps per second: 140, episode reward: -70.154, mean reward: -0.688 [-100.000,  8.104], mean action: 1.627 [0.000, 3.000],  loss: 112.511619, mse: 67704.766468, mean_q: 259.542882, mean_eps: 0.571001
 143134/300000: episode: 1344, duration: 0.597s, episode steps:  83, steps per second: 139, episode reward: -5.459, mean reward: -0.066 [-100.000, 12.211], mean action: 1.614 [0.000, 3.000],  loss: 93.749157, mse: 69789.088432, mean_q: 262.194822, mean_eps: 0.570724
 143225/300000: episode: 1345, duration: 0.625s, episode steps:  91, steps per second: 145, episode reward: -16.609, mean reward: -0.183 [-100.000, 24.826], mean action: 1.538 [0.000, 3.000],  loss: 210.083728, mse: 68396.199734, mean_q: 258.512349, mean_eps: 0.570463
 143362/300000: episode: 1346, duration: 0.977s, episode steps: 137, steps per second: 140, episode reward: -90.169, mean reward: -0.658 [-100.000,  5.647], mean action: 1.606 [0.000, 3.000],  loss: 156.587112, mse: 70040.779995, mean_q: 262.931455, mean_eps: 0.570121
 144096/300000: episode: 1347, duration: 5.915s, episode steps: 734, steps per second: 124, episode reward: -389.002, mean reward: -0.530 [-100.000, 20.431], mean action: 1.726 [0.000, 3.000],  loss: 160.324141, mse: 71264.531665, mean_q: 263.162609, mean_eps: 0.568814
 144176/300000: episode: 1348, duration: 0.578s, episode steps:  80, steps per second: 138, episode reward: -51.304, mean reward: -0.641 [-100.000, 19.324], mean action: 1.738 [0.000, 3.000],  loss: 84.970167, mse: 70079.510547, mean_q: 262.203432, mean_eps: 0.567593
 144281/300000: episode: 1349, duration: 0.739s, episode steps: 105, steps per second: 142, episode reward: 14.661, mean reward:  0.140 [-100.000, 11.795], mean action: 1.781 [0.000, 3.000],  loss: 118.407481, mse: 73685.086012, mean_q: 266.548985, mean_eps: 0.567316
 144415/300000: episode: 1350, duration: 0.927s, episode steps: 134, steps per second: 144, episode reward: -33.433, mean reward: -0.250 [-100.000, 18.357], mean action: 1.687 [0.000, 3.000],  loss: 104.321537, mse: 72908.377624, mean_q: 264.630869, mean_eps: 0.566958
 144507/300000: episode: 1351, duration: 0.678s, episode steps:  92, steps per second: 136, episode reward: -40.692, mean reward: -0.442 [-100.000, 20.940], mean action: 1.696 [0.000, 3.000],  loss: 247.678505, mse: 71502.629586, mean_q: 263.349622, mean_eps: 0.566618
 144640/300000: episode: 1352, duration: 0.928s, episode steps: 133, steps per second: 143, episode reward:  6.437, mean reward:  0.048 [-100.000, 14.527], mean action: 1.692 [0.000, 3.000],  loss: 207.295480, mse: 74646.661184, mean_q: 268.987457, mean_eps: 0.566281
 144780/300000: episode: 1353, duration: 1.005s, episode steps: 140, steps per second: 139, episode reward: 14.561, mean reward:  0.104 [-100.000, 21.172], mean action: 1.629 [0.000, 3.000],  loss: 120.649284, mse: 72823.365458, mean_q: 265.850063, mean_eps: 0.565871
 144894/300000: episode: 1354, duration: 0.804s, episode steps: 114, steps per second: 142, episode reward: -137.196, mean reward: -1.203 [-100.000, 58.197], mean action: 1.693 [0.000, 3.000],  loss: 132.031048, mse: 75086.160225, mean_q: 272.954829, mean_eps: 0.565491
 144992/300000: episode: 1355, duration: 0.712s, episode steps:  98, steps per second: 138, episode reward: -21.921, mean reward: -0.224 [-100.000, 12.301], mean action: 1.714 [0.000, 3.000],  loss: 166.219172, mse: 74584.234495, mean_q: 268.289113, mean_eps: 0.565172
 145101/300000: episode: 1356, duration: 0.809s, episode steps: 109, steps per second: 135, episode reward: -51.075, mean reward: -0.469 [-100.000, 15.442], mean action: 1.789 [0.000, 3.000],  loss: 225.761541, mse: 77250.004623, mean_q: 277.130110, mean_eps: 0.564862
 145229/300000: episode: 1357, duration: 0.881s, episode steps: 128, steps per second: 145, episode reward: -3.117, mean reward: -0.024 [-100.000, 18.758], mean action: 1.727 [0.000, 3.000],  loss: 169.412684, mse: 78668.312958, mean_q: 282.991470, mean_eps: 0.564507
 145351/300000: episode: 1358, duration: 0.842s, episode steps: 122, steps per second: 145, episode reward: -66.588, mean reward: -0.546 [-100.000, 16.146], mean action: 1.664 [0.000, 3.000],  loss: 153.908231, mse: 76232.533811, mean_q: 278.200209, mean_eps: 0.564132
 145483/300000: episode: 1359, duration: 1.046s, episode steps: 132, steps per second: 126, episode reward:  8.230, mean reward:  0.062 [-100.000, 16.612], mean action: 1.652 [0.000, 3.000],  loss: 166.629103, mse: 76075.507013, mean_q: 278.148149, mean_eps: 0.563751
 145573/300000: episode: 1360, duration: 0.757s, episode steps:  90, steps per second: 119, episode reward: 25.383, mean reward:  0.282 [-100.000, 17.763], mean action: 1.744 [0.000, 3.000],  loss: 183.272238, mse: 77875.837066, mean_q: 280.068362, mean_eps: 0.563418
 145747/300000: episode: 1361, duration: 1.391s, episode steps: 174, steps per second: 125, episode reward: -330.745, mean reward: -1.901 [-100.000, 18.816], mean action: 1.753 [0.000, 3.000],  loss: 196.616423, mse: 79518.739853, mean_q: 284.215861, mean_eps: 0.563021
 145825/300000: episode: 1362, duration: 0.571s, episode steps:  78, steps per second: 137, episode reward: -12.961, mean reward: -0.166 [-100.000, 11.427], mean action: 1.679 [0.000, 3.000],  loss: 89.725736, mse: 81181.642578, mean_q: 289.260032, mean_eps: 0.562643
 145918/300000: episode: 1363, duration: 0.710s, episode steps:  93, steps per second: 131, episode reward: -52.831, mean reward: -0.568 [-100.000,  7.860], mean action: 1.688 [0.000, 3.000],  loss: 155.761619, mse: 80950.433300, mean_q: 289.887496, mean_eps: 0.562387
 146041/300000: episode: 1364, duration: 0.924s, episode steps: 123, steps per second: 133, episode reward: -12.664, mean reward: -0.103 [-100.000, 17.269], mean action: 1.715 [0.000, 3.000],  loss: 152.595265, mse: 81720.159553, mean_q: 287.529053, mean_eps: 0.562063
 146160/300000: episode: 1365, duration: 0.828s, episode steps: 119, steps per second: 144, episode reward: -19.443, mean reward: -0.163 [-100.000, 17.712], mean action: 1.555 [0.000, 3.000],  loss: 160.290290, mse: 81532.830653, mean_q: 289.976146, mean_eps: 0.561700
 146264/300000: episode: 1366, duration: 0.740s, episode steps: 104, steps per second: 140, episode reward: -45.550, mean reward: -0.438 [-100.000,  8.748], mean action: 1.731 [0.000, 3.000],  loss: 236.545608, mse: 80355.642578, mean_q: 289.030721, mean_eps: 0.561366
 146866/300000: episode: 1367, duration: 4.551s, episode steps: 602, steps per second: 132, episode reward: -332.758, mean reward: -0.553 [-100.000, 22.260], mean action: 1.792 [0.000, 3.000],  loss: 142.756027, mse: 81490.283086, mean_q: 291.925828, mean_eps: 0.560306
 146967/300000: episode: 1368, duration: 0.758s, episode steps: 101, steps per second: 133, episode reward: -63.929, mean reward: -0.633 [-100.000,  8.956], mean action: 1.743 [0.000, 3.000],  loss: 176.579110, mse: 82633.453125, mean_q: 295.100696, mean_eps: 0.559252
 147052/300000: episode: 1369, duration: 0.638s, episode steps:  85, steps per second: 133, episode reward: -65.365, mean reward: -0.769 [-100.000,  7.568], mean action: 1.624 [0.000, 3.000],  loss: 133.653770, mse: 83453.022978, mean_q: 296.404137, mean_eps: 0.558973
 147155/300000: episode: 1370, duration: 0.855s, episode steps: 103, steps per second: 120, episode reward: -68.231, mean reward: -0.662 [-100.000,  7.499], mean action: 1.689 [0.000, 3.000],  loss: 145.949414, mse: 85245.616126, mean_q: 298.323088, mean_eps: 0.558691
 147258/300000: episode: 1371, duration: 0.774s, episode steps: 103, steps per second: 133, episode reward: -71.230, mean reward: -0.692 [-100.000,  6.900], mean action: 1.631 [0.000, 3.000],  loss: 115.458609, mse: 85165.586734, mean_q: 297.601290, mean_eps: 0.558382
 148258/300000: episode: 1372, duration: 7.690s, episode steps: 1000, steps per second: 130, episode reward: 57.388, mean reward:  0.057 [-21.454, 69.789], mean action: 1.990 [0.000, 3.000],  loss: 156.973694, mse: 91166.758453, mean_q: 308.172602, mean_eps: 0.556727
 148367/300000: episode: 1373, duration: 0.772s, episode steps: 109, steps per second: 141, episode reward: -49.434, mean reward: -0.454 [-100.000, 17.815], mean action: 1.532 [0.000, 3.000],  loss: 134.296573, mse: 95271.867044, mean_q: 314.209325, mean_eps: 0.555064
 148497/300000: episode: 1374, duration: 0.930s, episode steps: 130, steps per second: 140, episode reward: -34.618, mean reward: -0.266 [-100.000,  7.634], mean action: 1.715 [0.000, 3.000],  loss: 179.069433, mse: 96893.235096, mean_q: 316.258133, mean_eps: 0.554706
 149497/300000: episode: 1375, duration: 7.618s, episode steps: 1000, steps per second: 131, episode reward: 124.374, mean reward:  0.124 [-21.182, 23.608], mean action: 2.076 [0.000, 3.000],  loss: 138.315147, mse: 99998.299891, mean_q: 323.888320, mean_eps: 0.553010
 149706/300000: episode: 1376, duration: 1.468s, episode steps: 209, steps per second: 142, episode reward: -308.700, mean reward: -1.477 [-100.000, 19.369], mean action: 1.713 [0.000, 3.000],  loss: 152.148816, mse: 102141.571808, mean_q: 329.411916, mean_eps: 0.551197
 149823/300000: episode: 1377, duration: 0.816s, episode steps: 117, steps per second: 143, episode reward: -46.029, mean reward: -0.393 [-100.000, 11.499], mean action: 1.624 [0.000, 3.000],  loss: 194.745169, mse: 101166.693777, mean_q: 329.693325, mean_eps: 0.550708
 149966/300000: episode: 1378, duration: 1.033s, episode steps: 143, steps per second: 138, episode reward: -32.918, mean reward: -0.230 [-100.000, 15.547], mean action: 1.804 [0.000, 3.000],  loss: 165.297905, mse: 100821.006064, mean_q: 327.131562, mean_eps: 0.550318
 150066/300000: episode: 1379, duration: 0.692s, episode steps: 100, steps per second: 144, episode reward: -117.557, mean reward: -1.176 [-100.000,  7.504], mean action: 1.760 [0.000, 3.000],  loss: 170.719274, mse: 102356.756484, mean_q: 330.234014, mean_eps: 0.549953
 150146/300000: episode: 1380, duration: 0.551s, episode steps:  80, steps per second: 145, episode reward: -40.572, mean reward: -0.507 [-100.000, 22.620], mean action: 1.712 [0.000, 3.000],  loss: 239.654842, mse: 101104.467383, mean_q: 329.905618, mean_eps: 0.549684
 150378/300000: episode: 1381, duration: 1.641s, episode steps: 232, steps per second: 141, episode reward: -41.917, mean reward: -0.181 [-100.000, 21.068], mean action: 1.750 [0.000, 3.000],  loss: 151.812256, mse: 103139.946558, mean_q: 332.251243, mean_eps: 0.549216
 150468/300000: episode: 1382, duration: 0.623s, episode steps:  90, steps per second: 145, episode reward: -47.486, mean reward: -0.528 [-100.000, 11.172], mean action: 1.522 [0.000, 3.000],  loss: 119.232304, mse: 102847.975694, mean_q: 330.722282, mean_eps: 0.548732
 150589/300000: episode: 1383, duration: 0.858s, episode steps: 121, steps per second: 141, episode reward: -33.292, mean reward: -0.275 [-100.000,  8.591], mean action: 1.785 [0.000, 3.000],  loss: 209.323180, mse: 100656.028603, mean_q: 328.171817, mean_eps: 0.548416
 150723/300000: episode: 1384, duration: 0.934s, episode steps: 134, steps per second: 143, episode reward: -22.460, mean reward: -0.168 [-100.000, 19.700], mean action: 1.724 [0.000, 3.000],  loss: 156.994714, mse: 103717.413246, mean_q: 333.905563, mean_eps: 0.548034
 150826/300000: episode: 1385, duration: 0.718s, episode steps: 103, steps per second: 144, episode reward: 11.320, mean reward:  0.110 [-100.000, 23.476], mean action: 1.796 [0.000, 3.000],  loss: 192.837918, mse: 109282.107251, mean_q: 342.799800, mean_eps: 0.547678
 150930/300000: episode: 1386, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -53.582, mean reward: -0.515 [-100.000,  8.476], mean action: 1.712 [0.000, 3.000],  loss: 123.491131, mse: 105571.991061, mean_q: 339.685574, mean_eps: 0.547368
 151024/300000: episode: 1387, duration: 0.658s, episode steps:  94, steps per second: 143, episode reward: -24.115, mean reward: -0.257 [-100.000, 12.319], mean action: 1.723 [0.000, 3.000],  loss: 164.876722, mse: 108367.595994, mean_q: 344.202094, mean_eps: 0.547070
 151137/300000: episode: 1388, duration: 0.784s, episode steps: 113, steps per second: 144, episode reward: -40.663, mean reward: -0.360 [-100.000, 12.576], mean action: 1.814 [0.000, 3.000],  loss: 167.791729, mse: 105699.476562, mean_q: 338.451205, mean_eps: 0.546760
 151276/300000: episode: 1389, duration: 1.001s, episode steps: 139, steps per second: 139, episode reward:  6.033, mean reward:  0.043 [-100.000, 33.660], mean action: 1.532 [0.000, 3.000],  loss: 138.487112, mse: 105926.719143, mean_q: 338.046053, mean_eps: 0.546382
 151380/300000: episode: 1390, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: -77.267, mean reward: -0.743 [-100.000, 17.629], mean action: 1.837 [0.000, 3.000],  loss: 146.151529, mse: 108519.525015, mean_q: 342.200389, mean_eps: 0.546018
 151472/300000: episode: 1391, duration: 0.644s, episode steps:  92, steps per second: 143, episode reward: -47.771, mean reward: -0.519 [-100.000, 18.807], mean action: 1.924 [0.000, 3.000],  loss: 153.981961, mse: 109878.866848, mean_q: 345.858241, mean_eps: 0.545723
 151579/300000: episode: 1392, duration: 0.743s, episode steps: 107, steps per second: 144, episode reward: -9.511, mean reward: -0.089 [-100.000, 19.305], mean action: 1.766 [0.000, 3.000],  loss: 150.249610, mse: 110154.646174, mean_q: 346.723462, mean_eps: 0.545425
 151720/300000: episode: 1393, duration: 1.000s, episode steps: 141, steps per second: 141, episode reward: 26.962, mean reward:  0.191 [-100.000, 12.786], mean action: 1.730 [0.000, 3.000],  loss: 138.025558, mse: 109942.832835, mean_q: 342.778644, mean_eps: 0.545053
 151801/300000: episode: 1394, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward:  4.876, mean reward:  0.060 [-100.000, 15.727], mean action: 1.679 [0.000, 3.000],  loss: 144.958004, mse: 114882.708140, mean_q: 354.593518, mean_eps: 0.544720
 151921/300000: episode: 1395, duration: 0.835s, episode steps: 120, steps per second: 144, episode reward: 19.329, mean reward:  0.161 [-100.000, 16.457], mean action: 1.508 [0.000, 3.000],  loss: 206.377414, mse: 114610.983268, mean_q: 352.285279, mean_eps: 0.544419
 152013/300000: episode: 1396, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -13.064, mean reward: -0.142 [-100.000,  9.110], mean action: 1.750 [0.000, 3.000],  loss: 190.934286, mse: 114202.693784, mean_q: 353.581117, mean_eps: 0.544101
 152123/300000: episode: 1397, duration: 0.778s, episode steps: 110, steps per second: 141, episode reward: -116.186, mean reward: -1.056 [-100.000, 18.545], mean action: 1.864 [0.000, 3.000],  loss: 219.639867, mse: 114461.623651, mean_q: 355.415068, mean_eps: 0.543798
 152207/300000: episode: 1398, duration: 0.590s, episode steps:  84, steps per second: 142, episode reward: -39.301, mean reward: -0.468 [-100.000, 10.639], mean action: 1.536 [0.000, 3.000],  loss: 110.154058, mse: 115814.180339, mean_q: 358.584839, mean_eps: 0.543507
 152371/300000: episode: 1399, duration: 1.147s, episode steps: 164, steps per second: 143, episode reward: -12.764, mean reward: -0.078 [-100.000, 18.724], mean action: 1.732 [0.000, 3.000],  loss: 134.428855, mse: 117328.504097, mean_q: 359.108969, mean_eps: 0.543134
 152456/300000: episode: 1400, duration: 0.613s, episode steps:  85, steps per second: 139, episode reward: 17.374, mean reward:  0.204 [-100.000, 16.310], mean action: 1.824 [0.000, 3.000],  loss: 161.434791, mse: 118341.104320, mean_q: 361.930500, mean_eps: 0.542761
 152575/300000: episode: 1401, duration: 0.832s, episode steps: 119, steps per second: 143, episode reward: -184.736, mean reward: -1.552 [-100.000, 75.053], mean action: 1.672 [0.000, 3.000],  loss: 164.904308, mse: 121119.353729, mean_q: 365.415652, mean_eps: 0.542455
 152675/300000: episode: 1402, duration: 0.692s, episode steps: 100, steps per second: 145, episode reward: -18.156, mean reward: -0.182 [-100.000, 18.590], mean action: 1.520 [0.000, 3.000],  loss: 141.132421, mse: 119581.293516, mean_q: 362.851764, mean_eps: 0.542126
 152783/300000: episode: 1403, duration: 0.792s, episode steps: 108, steps per second: 136, episode reward: -56.844, mean reward: -0.526 [-100.000, 16.808], mean action: 1.630 [0.000, 3.000],  loss: 122.517878, mse: 119933.508174, mean_q: 364.252090, mean_eps: 0.541815
 152897/300000: episode: 1404, duration: 0.795s, episode steps: 114, steps per second: 143, episode reward: -26.733, mean reward: -0.234 [-100.000, 11.959], mean action: 1.711 [0.000, 3.000],  loss: 188.127968, mse: 122636.164474, mean_q: 367.856977, mean_eps: 0.541481
 153020/300000: episode: 1405, duration: 0.882s, episode steps: 123, steps per second: 140, episode reward: -54.986, mean reward: -0.447 [-100.000, 14.947], mean action: 1.683 [0.000, 3.000],  loss: 170.915381, mse: 122664.818089, mean_q: 369.488620, mean_eps: 0.541126
 153093/300000: episode: 1406, duration: 0.521s, episode steps:  73, steps per second: 140, episode reward: -21.748, mean reward: -0.298 [-100.000,  7.673], mean action: 1.849 [0.000, 3.000],  loss: 185.467081, mse: 124047.681079, mean_q: 370.229147, mean_eps: 0.540832
 153231/300000: episode: 1407, duration: 0.976s, episode steps: 138, steps per second: 141, episode reward: -1.650, mean reward: -0.012 [-100.000, 16.357], mean action: 1.732 [0.000, 3.000],  loss: 117.378169, mse: 125345.685405, mean_q: 373.085429, mean_eps: 0.540516
 153370/300000: episode: 1408, duration: 0.987s, episode steps: 139, steps per second: 141, episode reward: -55.313, mean reward: -0.398 [-100.000,  9.738], mean action: 1.691 [0.000, 3.000],  loss: 171.416702, mse: 125953.450259, mean_q: 373.885155, mean_eps: 0.540100
 153503/300000: episode: 1409, duration: 0.917s, episode steps: 133, steps per second: 145, episode reward: -41.058, mean reward: -0.309 [-100.000, 17.239], mean action: 1.586 [0.000, 3.000],  loss: 206.036684, mse: 125098.838640, mean_q: 372.079291, mean_eps: 0.539692
 153586/300000: episode: 1410, duration: 0.566s, episode steps:  83, steps per second: 147, episode reward: -38.547, mean reward: -0.464 [-100.000,  7.163], mean action: 1.518 [0.000, 3.000],  loss: 126.450216, mse: 126535.063065, mean_q: 377.562894, mean_eps: 0.539368
 153671/300000: episode: 1411, duration: 0.625s, episode steps:  85, steps per second: 136, episode reward: -72.533, mean reward: -0.853 [-100.000,  8.433], mean action: 1.847 [0.000, 3.000],  loss: 150.919230, mse: 129177.901103, mean_q: 379.228376, mean_eps: 0.539116
 153752/300000: episode: 1412, duration: 0.582s, episode steps:  81, steps per second: 139, episode reward: -95.150, mean reward: -1.175 [-100.000, 23.040], mean action: 1.778 [0.000, 3.000],  loss: 153.963995, mse: 125855.505112, mean_q: 372.250976, mean_eps: 0.538867
 153844/300000: episode: 1413, duration: 0.659s, episode steps:  92, steps per second: 140, episode reward: -50.839, mean reward: -0.553 [-100.000,  9.139], mean action: 1.696 [0.000, 3.000],  loss: 184.766618, mse: 128803.534901, mean_q: 379.131441, mean_eps: 0.538608
 154262/300000: episode: 1414, duration: 3.108s, episode steps: 418, steps per second: 135, episode reward: -158.091, mean reward: -0.378 [-100.000, 24.045], mean action: 1.648 [0.000, 3.000],  loss: 182.387991, mse: 126646.024222, mean_q: 373.886844, mean_eps: 0.537843
 154387/300000: episode: 1415, duration: 0.964s, episode steps: 125, steps per second: 130, episode reward:  3.985, mean reward:  0.032 [-100.000, 17.846], mean action: 1.592 [0.000, 3.000],  loss: 128.734608, mse: 125497.841250, mean_q: 373.223064, mean_eps: 0.537028
 154467/300000: episode: 1416, duration: 0.687s, episode steps:  80, steps per second: 116, episode reward: -25.745, mean reward: -0.322 [-100.000, 11.821], mean action: 1.712 [0.000, 3.000],  loss: 114.218737, mse: 124451.210449, mean_q: 371.615015, mean_eps: 0.536721
 154559/300000: episode: 1417, duration: 0.764s, episode steps:  92, steps per second: 120, episode reward: -111.082, mean reward: -1.207 [-100.000,  5.167], mean action: 1.543 [0.000, 3.000],  loss: 121.439638, mse: 125815.368122, mean_q: 373.301430, mean_eps: 0.536463
 154661/300000: episode: 1418, duration: 0.782s, episode steps: 102, steps per second: 130, episode reward: -82.832, mean reward: -0.812 [-100.000, 10.320], mean action: 1.745 [0.000, 3.000],  loss: 125.110476, mse: 125935.613128, mean_q: 373.518060, mean_eps: 0.536172
 154786/300000: episode: 1419, duration: 1.100s, episode steps: 125, steps per second: 114, episode reward: -67.668, mean reward: -0.541 [-100.000, 22.816], mean action: 1.576 [0.000, 3.000],  loss: 119.933182, mse: 127741.813938, mean_q: 376.447436, mean_eps: 0.535831
 154884/300000: episode: 1420, duration: 0.802s, episode steps:  98, steps per second: 122, episode reward: 13.579, mean reward:  0.139 [-100.000, 19.975], mean action: 1.602 [0.000, 3.000],  loss: 110.246007, mse: 125552.784518, mean_q: 371.402295, mean_eps: 0.535496
 155024/300000: episode: 1421, duration: 1.088s, episode steps: 140, steps per second: 129, episode reward:  5.374, mean reward:  0.038 [-100.000, 17.598], mean action: 1.829 [0.000, 3.000],  loss: 161.800439, mse: 124567.906083, mean_q: 372.307029, mean_eps: 0.535140
 155127/300000: episode: 1422, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -28.037, mean reward: -0.272 [-100.000,  5.384], mean action: 1.592 [0.000, 3.000],  loss: 128.219050, mse: 120908.048999, mean_q: 363.961660, mean_eps: 0.534775
 155207/300000: episode: 1423, duration: 0.559s, episode steps:  80, steps per second: 143, episode reward: -15.032, mean reward: -0.188 [-100.000,  9.135], mean action: 1.938 [0.000, 3.000],  loss: 150.251873, mse: 124990.638574, mean_q: 372.055774, mean_eps: 0.534501
 155336/300000: episode: 1424, duration: 0.920s, episode steps: 129, steps per second: 140, episode reward: 12.103, mean reward:  0.094 [-100.000, 19.092], mean action: 1.698 [0.000, 3.000],  loss: 158.921662, mse: 126206.802568, mean_q: 375.068613, mean_eps: 0.534187
 155857/300000: episode: 1425, duration: 3.938s, episode steps: 521, steps per second: 132, episode reward: -268.010, mean reward: -0.514 [-100.000, 15.714], mean action: 1.743 [0.000, 3.000],  loss: 152.746633, mse: 126191.518159, mean_q: 373.081590, mean_eps: 0.533212
 155997/300000: episode: 1426, duration: 1.119s, episode steps: 140, steps per second: 125, episode reward: -100.947, mean reward: -0.721 [-100.000,  8.280], mean action: 1.621 [0.000, 3.000],  loss: 155.746225, mse: 126333.241239, mean_q: 373.247139, mean_eps: 0.532220
 156109/300000: episode: 1427, duration: 0.829s, episode steps: 112, steps per second: 135, episode reward: -11.737, mean reward: -0.105 [-100.000, 33.728], mean action: 1.723 [0.000, 3.000],  loss: 147.175568, mse: 127151.865374, mean_q: 375.717356, mean_eps: 0.531842
 156215/300000: episode: 1428, duration: 0.835s, episode steps: 106, steps per second: 127, episode reward: -64.753, mean reward: -0.611 [-100.000, 12.093], mean action: 1.745 [0.000, 3.000],  loss: 110.947235, mse: 124628.512161, mean_q: 371.115348, mean_eps: 0.531515
 157215/300000: episode: 1429, duration: 8.389s, episode steps: 1000, steps per second: 119, episode reward:  2.607, mean reward:  0.003 [-23.897, 28.054], mean action: 1.827 [0.000, 3.000],  loss: 140.381439, mse: 125852.410227, mean_q: 375.015266, mean_eps: 0.529857
 157327/300000: episode: 1430, duration: 0.783s, episode steps: 112, steps per second: 143, episode reward:  1.494, mean reward:  0.013 [-100.000, 15.779], mean action: 1.670 [0.000, 3.000],  loss: 103.411212, mse: 128661.868304, mean_q: 380.674105, mean_eps: 0.528188
 157417/300000: episode: 1431, duration: 0.624s, episode steps:  90, steps per second: 144, episode reward: -54.177, mean reward: -0.602 [-100.000, 12.061], mean action: 1.656 [0.000, 3.000],  loss: 160.900083, mse: 127033.915538, mean_q: 377.512240, mean_eps: 0.527886
 157491/300000: episode: 1432, duration: 0.532s, episode steps:  74, steps per second: 139, episode reward: -62.284, mean reward: -0.842 [-100.000,  9.269], mean action: 1.581 [0.000, 3.000],  loss: 144.897024, mse: 128422.460938, mean_q: 380.594308, mean_eps: 0.527639
 157596/300000: episode: 1433, duration: 0.743s, episode steps: 105, steps per second: 141, episode reward: -85.616, mean reward: -0.815 [-100.000,  7.421], mean action: 1.638 [0.000, 3.000],  loss: 141.617198, mse: 128808.843676, mean_q: 381.574516, mean_eps: 0.527371
 157678/300000: episode: 1434, duration: 0.563s, episode steps:  82, steps per second: 146, episode reward: -116.778, mean reward: -1.424 [-100.000,  8.707], mean action: 1.585 [0.000, 3.000],  loss: 145.993237, mse: 125673.812881, mean_q: 376.448086, mean_eps: 0.527091
 157781/300000: episode: 1435, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -17.867, mean reward: -0.173 [-100.000, 17.327], mean action: 1.709 [0.000, 3.000],  loss: 163.550906, mse: 129436.337075, mean_q: 386.645259, mean_eps: 0.526813
 157885/300000: episode: 1436, duration: 0.741s, episode steps: 104, steps per second: 140, episode reward: -35.305, mean reward: -0.339 [-100.000,  8.912], mean action: 1.750 [0.000, 3.000],  loss: 122.794885, mse: 129829.898813, mean_q: 385.126212, mean_eps: 0.526502
 158001/300000: episode: 1437, duration: 0.821s, episode steps: 116, steps per second: 141, episode reward: 19.989, mean reward:  0.172 [-100.000,  9.318], mean action: 1.819 [0.000, 3.000],  loss: 130.523397, mse: 132509.440463, mean_q: 388.707578, mean_eps: 0.526173
 158085/300000: episode: 1438, duration: 0.608s, episode steps:  84, steps per second: 138, episode reward:  8.061, mean reward:  0.096 [-100.000, 37.831], mean action: 1.655 [0.000, 3.000],  loss: 107.456847, mse: 133738.712240, mean_q: 391.542507, mean_eps: 0.525872
 158199/300000: episode: 1439, duration: 0.805s, episode steps: 114, steps per second: 142, episode reward:  5.390, mean reward:  0.047 [-100.000, 18.023], mean action: 1.596 [0.000, 3.000],  loss: 197.675379, mse: 133556.975192, mean_q: 391.354526, mean_eps: 0.525575
 158296/300000: episode: 1440, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -23.827, mean reward: -0.246 [-100.000, 11.933], mean action: 1.588 [0.000, 3.000],  loss: 222.324562, mse: 135484.593750, mean_q: 393.423366, mean_eps: 0.525259
 158430/300000: episode: 1441, duration: 0.959s, episode steps: 134, steps per second: 140, episode reward: -27.980, mean reward: -0.209 [-100.000, 11.495], mean action: 1.545 [0.000, 3.000],  loss: 154.187412, mse: 133896.368237, mean_q: 391.097922, mean_eps: 0.524912
 158555/300000: episode: 1442, duration: 0.884s, episode steps: 125, steps per second: 141, episode reward:  5.141, mean reward:  0.041 [-100.000, 17.863], mean action: 1.608 [0.000, 3.000],  loss: 268.303864, mse: 133536.688562, mean_q: 390.817863, mean_eps: 0.524524
 158659/300000: episode: 1443, duration: 0.737s, episode steps: 104, steps per second: 141, episode reward:  0.089, mean reward:  0.001 [-100.000, 12.549], mean action: 1.894 [0.000, 3.000],  loss: 116.350997, mse: 133286.284480, mean_q: 390.531345, mean_eps: 0.524181
 158775/300000: episode: 1444, duration: 0.840s, episode steps: 116, steps per second: 138, episode reward: -10.319, mean reward: -0.089 [-100.000, 15.004], mean action: 1.647 [0.000, 3.000],  loss: 219.015862, mse: 129759.580078, mean_q: 383.105466, mean_eps: 0.523851
 158895/300000: episode: 1445, duration: 0.880s, episode steps: 120, steps per second: 136, episode reward: -45.388, mean reward: -0.378 [-100.000, 13.335], mean action: 1.775 [0.000, 3.000],  loss: 149.324994, mse: 130010.291602, mean_q: 384.313433, mean_eps: 0.523497
 158956/300000: episode: 1446, duration: 0.428s, episode steps:  61, steps per second: 143, episode reward: -85.947, mean reward: -1.409 [-100.000,  8.156], mean action: 1.639 [0.000, 3.000],  loss: 117.374806, mse: 130913.146132, mean_q: 387.266295, mean_eps: 0.523225
 159062/300000: episode: 1447, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: 20.350, mean reward:  0.192 [-100.000, 15.380], mean action: 1.745 [0.000, 3.000],  loss: 103.788599, mse: 129983.019089, mean_q: 385.985717, mean_eps: 0.522975
 160062/300000: episode: 1448, duration: 7.876s, episode steps: 1000, steps per second: 127, episode reward: 43.015, mean reward:  0.043 [-23.347, 24.458], mean action: 1.425 [0.000, 3.000],  loss: 185.686608, mse: 133902.894516, mean_q: 391.647549, mean_eps: 0.521315
 160162/300000: episode: 1449, duration: 0.702s, episode steps: 100, steps per second: 142, episode reward: -69.524, mean reward: -0.695 [-100.000,  6.931], mean action: 1.570 [0.000, 3.000],  loss: 215.137992, mse: 133590.218359, mean_q: 393.419242, mean_eps: 0.519666
 160248/300000: episode: 1450, duration: 0.609s, episode steps:  86, steps per second: 141, episode reward: -54.278, mean reward: -0.631 [-100.000,  5.663], mean action: 1.628 [0.000, 3.000],  loss: 107.371745, mse: 133356.563136, mean_q: 391.969049, mean_eps: 0.519386
 160338/300000: episode: 1451, duration: 0.625s, episode steps:  90, steps per second: 144, episode reward: -58.417, mean reward: -0.649 [-100.000, 11.025], mean action: 1.833 [0.000, 3.000],  loss: 177.226643, mse: 132873.349392, mean_q: 388.572264, mean_eps: 0.519122
 160445/300000: episode: 1452, duration: 0.755s, episode steps: 107, steps per second: 142, episode reward: -25.442, mean reward: -0.238 [-100.000,  7.396], mean action: 1.701 [0.000, 3.000],  loss: 147.628696, mse: 134726.080534, mean_q: 393.985021, mean_eps: 0.518827
 160563/300000: episode: 1453, duration: 0.831s, episode steps: 118, steps per second: 142, episode reward: -50.926, mean reward: -0.432 [-100.000, 13.590], mean action: 1.576 [0.000, 3.000],  loss: 153.361611, mse: 135699.617916, mean_q: 395.692291, mean_eps: 0.518490
 160701/300000: episode: 1454, duration: 0.959s, episode steps: 138, steps per second: 144, episode reward: -2.522, mean reward: -0.018 [-100.000, 13.028], mean action: 1.645 [0.000, 3.000],  loss: 170.317304, mse: 135679.703691, mean_q: 394.496088, mean_eps: 0.518105
 160823/300000: episode: 1455, duration: 0.878s, episode steps: 122, steps per second: 139, episode reward: -18.603, mean reward: -0.152 [-100.000, 14.719], mean action: 1.582 [0.000, 3.000],  loss: 128.797177, mse: 132938.324795, mean_q: 388.688312, mean_eps: 0.517715
 160932/300000: episode: 1456, duration: 0.794s, episode steps: 109, steps per second: 137, episode reward: 42.936, mean reward:  0.394 [-100.000, 17.322], mean action: 1.780 [0.000, 3.000],  loss: 120.012066, mse: 133059.249427, mean_q: 391.359506, mean_eps: 0.517369
 161053/300000: episode: 1457, duration: 0.873s, episode steps: 121, steps per second: 139, episode reward: -69.016, mean reward: -0.570 [-100.000, 16.189], mean action: 1.587 [0.000, 3.000],  loss: 137.457978, mse: 132904.967523, mean_q: 391.207107, mean_eps: 0.517024
 161180/300000: episode: 1458, duration: 0.924s, episode steps: 127, steps per second: 137, episode reward: 22.505, mean reward:  0.177 [-100.000, 17.896], mean action: 1.661 [0.000, 3.000],  loss: 156.273097, mse: 135460.064776, mean_q: 395.375570, mean_eps: 0.516652
 161280/300000: episode: 1459, duration: 0.700s, episode steps: 100, steps per second: 143, episode reward: -12.022, mean reward: -0.120 [-100.000, 13.176], mean action: 1.560 [0.000, 3.000],  loss: 136.475089, mse: 135900.108125, mean_q: 393.232134, mean_eps: 0.516311
 162280/300000: episode: 1460, duration: 8.303s, episode steps: 1000, steps per second: 120, episode reward: 116.650, mean reward:  0.117 [-23.551, 23.709], mean action: 1.361 [0.000, 3.000],  loss: 135.986056, mse: 130901.048477, mean_q: 386.705535, mean_eps: 0.514661
 163280/300000: episode: 1461, duration: 8.534s, episode steps: 1000, steps per second: 117, episode reward: 71.208, mean reward:  0.071 [-24.210, 25.167], mean action: 1.460 [0.000, 3.000],  loss: 115.162881, mse: 123355.077914, mean_q: 373.566240, mean_eps: 0.511661
 163422/300000: episode: 1462, duration: 1.209s, episode steps: 142, steps per second: 117, episode reward: 51.123, mean reward:  0.360 [-100.000, 14.786], mean action: 1.789 [0.000, 3.000],  loss: 100.834677, mse: 121602.012599, mean_q: 371.358415, mean_eps: 0.509949
 163516/300000: episode: 1463, duration: 0.695s, episode steps:  94, steps per second: 135, episode reward: -37.518, mean reward: -0.399 [-100.000,  7.450], mean action: 1.489 [0.000, 3.000],  loss: 178.984454, mse: 122906.290891, mean_q: 373.344704, mean_eps: 0.509595
 163651/300000: episode: 1464, duration: 1.074s, episode steps: 135, steps per second: 126, episode reward: -3.503, mean reward: -0.026 [-100.000, 15.586], mean action: 1.733 [0.000, 3.000],  loss: 125.750444, mse: 121546.197222, mean_q: 370.706507, mean_eps: 0.509251
 163741/300000: episode: 1465, duration: 0.857s, episode steps:  90, steps per second: 105, episode reward: -84.128, mean reward: -0.935 [-100.000,  7.785], mean action: 1.700 [0.000, 3.000],  loss: 143.106282, mse: 120400.779688, mean_q: 369.173882, mean_eps: 0.508914
 163827/300000: episode: 1466, duration: 0.744s, episode steps:  86, steps per second: 116, episode reward: -55.244, mean reward: -0.642 [-100.000,  7.139], mean action: 1.570 [0.000, 3.000],  loss: 113.750092, mse: 119590.764898, mean_q: 366.382586, mean_eps: 0.508649
 163919/300000: episode: 1467, duration: 0.789s, episode steps:  92, steps per second: 117, episode reward: -58.668, mean reward: -0.638 [-100.000, 21.455], mean action: 1.696 [0.000, 3.000],  loss: 150.418587, mse: 118207.740999, mean_q: 364.876648, mean_eps: 0.508382
 164025/300000: episode: 1468, duration: 0.773s, episode steps: 106, steps per second: 137, episode reward: -11.520, mean reward: -0.109 [-100.000, 13.169], mean action: 1.660 [0.000, 3.000],  loss: 146.197355, mse: 116783.825767, mean_q: 360.851903, mean_eps: 0.508085
 164152/300000: episode: 1469, duration: 0.992s, episode steps: 127, steps per second: 128, episode reward: -72.481, mean reward: -0.571 [-100.000,  6.243], mean action: 1.606 [0.000, 3.000],  loss: 90.506024, mse: 115565.208600, mean_q: 360.492124, mean_eps: 0.507736
 164277/300000: episode: 1470, duration: 1.184s, episode steps: 125, steps per second: 106, episode reward: 20.733, mean reward:  0.166 [-100.000, 14.886], mean action: 1.664 [0.000, 3.000],  loss: 83.276977, mse: 115628.854125, mean_q: 359.362389, mean_eps: 0.507358
 164378/300000: episode: 1471, duration: 0.867s, episode steps: 101, steps per second: 117, episode reward: -44.031, mean reward: -0.436 [-100.000, 13.405], mean action: 1.723 [0.000, 3.000],  loss: 89.735452, mse: 114363.745359, mean_q: 355.947585, mean_eps: 0.507019
 164561/300000: episode: 1472, duration: 1.466s, episode steps: 183, steps per second: 125, episode reward: 76.816, mean reward:  0.420 [-100.000, 16.326], mean action: 1.738 [0.000, 3.000],  loss: 111.621247, mse: 116899.730746, mean_q: 362.988890, mean_eps: 0.506593
 165561/300000: episode: 1473, duration: 7.855s, episode steps: 1000, steps per second: 127, episode reward: 115.248, mean reward:  0.115 [-24.322, 23.187], mean action: 1.343 [0.000, 3.000],  loss: 111.424038, mse: 113643.825328, mean_q: 357.290397, mean_eps: 0.504818
 166561/300000: episode: 1474, duration: 7.617s, episode steps: 1000, steps per second: 131, episode reward: 125.555, mean reward:  0.126 [-24.188, 23.256], mean action: 1.400 [0.000, 3.000],  loss: 101.589848, mse: 106831.185633, mean_q: 345.525419, mean_eps: 0.501818
 166629/300000: episode: 1475, duration: 0.486s, episode steps:  68, steps per second: 140, episode reward: -64.278, mean reward: -0.945 [-100.000,  6.340], mean action: 1.647 [0.000, 3.000],  loss: 81.511521, mse: 105472.980928, mean_q: 342.588880, mean_eps: 0.500216
 166740/300000: episode: 1476, duration: 0.804s, episode steps: 111, steps per second: 138, episode reward:  0.372, mean reward:  0.003 [-100.000, 17.905], mean action: 1.486 [0.000, 3.000],  loss: 125.353839, mse: 103578.115217, mean_q: 339.504335, mean_eps: 0.499948
 166847/300000: episode: 1477, duration: 0.737s, episode steps: 107, steps per second: 145, episode reward: -64.232, mean reward: -0.600 [-100.000, 20.627], mean action: 1.570 [0.000, 3.000],  loss: 104.218148, mse: 103764.946262, mean_q: 338.885695, mean_eps: 0.499621
 166928/300000: episode: 1478, duration: 0.562s, episode steps:  81, steps per second: 144, episode reward:  6.185, mean reward:  0.076 [-100.000, 23.618], mean action: 1.728 [0.000, 3.000],  loss: 108.875744, mse: 102172.692226, mean_q: 336.363788, mean_eps: 0.499339
 167040/300000: episode: 1479, duration: 0.836s, episode steps: 112, steps per second: 134, episode reward: -38.176, mean reward: -0.341 [-100.000, 13.227], mean action: 1.411 [0.000, 3.000],  loss: 86.124547, mse: 102310.819057, mean_q: 336.949986, mean_eps: 0.499049
 168040/300000: episode: 1480, duration: 7.908s, episode steps: 1000, steps per second: 126, episode reward: 48.166, mean reward:  0.048 [-25.036, 27.161], mean action: 1.525 [0.000, 3.000],  loss: 108.447890, mse: 100330.965609, mean_q: 333.948865, mean_eps: 0.497382
 168319/300000: episode: 1481, duration: 1.978s, episode steps: 279, steps per second: 141, episode reward: -225.900, mean reward: -0.810 [-100.000, 39.494], mean action: 1.391 [0.000, 3.000],  loss: 131.389903, mse: 96166.362763, mean_q: 325.005060, mean_eps: 0.495463
 168420/300000: episode: 1482, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: -42.234, mean reward: -0.418 [-100.000,  9.054], mean action: 1.604 [0.000, 3.000],  loss: 94.159392, mse: 96822.360922, mean_q: 328.165795, mean_eps: 0.494893
 168555/300000: episode: 1483, duration: 0.977s, episode steps: 135, steps per second: 138, episode reward: 11.223, mean reward:  0.083 [-100.000, 10.018], mean action: 1.733 [0.000, 3.000],  loss: 112.473368, mse: 93919.192766, mean_q: 321.732300, mean_eps: 0.494539
 168646/300000: episode: 1484, duration: 0.657s, episode steps:  91, steps per second: 139, episode reward: -48.430, mean reward: -0.532 [-100.000, 17.896], mean action: 1.868 [0.000, 3.000],  loss: 115.420079, mse: 96284.646635, mean_q: 325.705903, mean_eps: 0.494200
 168767/300000: episode: 1485, duration: 0.875s, episode steps: 121, steps per second: 138, episode reward: 30.926, mean reward:  0.256 [-100.000, 16.624], mean action: 1.661 [0.000, 3.000],  loss: 116.364399, mse: 94061.180269, mean_q: 323.471875, mean_eps: 0.493882
 168899/300000: episode: 1486, duration: 0.910s, episode steps: 132, steps per second: 145, episode reward: -126.340, mean reward: -0.957 [-100.000, 44.144], mean action: 1.826 [0.000, 3.000],  loss: 96.743397, mse: 92284.879084, mean_q: 319.798662, mean_eps: 0.493502
 169020/300000: episode: 1487, duration: 0.867s, episode steps: 121, steps per second: 140, episode reward: 10.786, mean reward:  0.089 [-100.000, 21.218], mean action: 1.612 [0.000, 3.000],  loss: 84.323429, mse: 90907.359440, mean_q: 316.327532, mean_eps: 0.493123
 169123/300000: episode: 1488, duration: 0.711s, episode steps: 103, steps per second: 145, episode reward: -33.856, mean reward: -0.329 [-100.000, 20.075], mean action: 1.757 [0.000, 3.000],  loss: 139.031656, mse: 93244.773513, mean_q: 321.525270, mean_eps: 0.492787
 169253/300000: episode: 1489, duration: 0.914s, episode steps: 130, steps per second: 142, episode reward: -102.122, mean reward: -0.786 [-100.000,  7.579], mean action: 1.492 [0.000, 3.000],  loss: 115.304968, mse: 91344.656130, mean_q: 318.404036, mean_eps: 0.492437
 169375/300000: episode: 1490, duration: 0.876s, episode steps: 122, steps per second: 139, episode reward: -11.435, mean reward: -0.094 [-100.000,  7.748], mean action: 1.459 [0.000, 3.000],  loss: 85.587965, mse: 91947.273373, mean_q: 318.787322, mean_eps: 0.492059
 169514/300000: episode: 1491, duration: 0.968s, episode steps: 139, steps per second: 144, episode reward: -4.504, mean reward: -0.032 [-100.000, 23.381], mean action: 1.597 [0.000, 3.000],  loss: 105.334478, mse: 92208.199078, mean_q: 320.972675, mean_eps: 0.491668
 169603/300000: episode: 1492, duration: 0.626s, episode steps:  89, steps per second: 142, episode reward: -43.951, mean reward: -0.494 [-100.000, 11.278], mean action: 1.573 [0.000, 3.000],  loss: 79.681800, mse: 91399.621138, mean_q: 318.937299, mean_eps: 0.491326
 169698/300000: episode: 1493, duration: 0.672s, episode steps:  95, steps per second: 141, episode reward: -43.100, mean reward: -0.454 [-100.000, 14.796], mean action: 1.474 [0.000, 3.000],  loss: 90.664122, mse: 91444.129030, mean_q: 319.739730, mean_eps: 0.491050
 169784/300000: episode: 1494, duration: 0.612s, episode steps:  86, steps per second: 141, episode reward: -63.029, mean reward: -0.733 [-100.000, 18.315], mean action: 1.779 [0.000, 3.000],  loss: 79.883128, mse: 93881.305414, mean_q: 325.655193, mean_eps: 0.490778
 169859/300000: episode: 1495, duration: 0.528s, episode steps:  75, steps per second: 142, episode reward: -59.703, mean reward: -0.796 [-100.000, 12.360], mean action: 1.760 [0.000, 3.000],  loss: 81.262961, mse: 93206.111458, mean_q: 322.224796, mean_eps: 0.490537
 169962/300000: episode: 1496, duration: 0.741s, episode steps: 103, steps per second: 139, episode reward: -44.176, mean reward: -0.429 [-100.000, 15.581], mean action: 1.660 [0.000, 3.000],  loss: 88.421350, mse: 92780.953580, mean_q: 319.923857, mean_eps: 0.490270
 170083/300000: episode: 1497, duration: 0.849s, episode steps: 121, steps per second: 143, episode reward: -67.006, mean reward: -0.554 [-100.000, 16.665], mean action: 1.612 [0.000, 3.000],  loss: 115.433355, mse: 92247.858213, mean_q: 320.933564, mean_eps: 0.489934
 170220/300000: episode: 1498, duration: 0.971s, episode steps: 137, steps per second: 141, episode reward: -72.292, mean reward: -0.528 [-100.000,  8.522], mean action: 1.584 [0.000, 3.000],  loss: 116.218053, mse: 91555.775205, mean_q: 318.809410, mean_eps: 0.489547
 170321/300000: episode: 1499, duration: 0.702s, episode steps: 101, steps per second: 144, episode reward: -5.244, mean reward: -0.052 [-100.000,  8.983], mean action: 1.752 [0.000, 3.000],  loss: 123.240512, mse: 90864.146658, mean_q: 319.012396, mean_eps: 0.489190
 170407/300000: episode: 1500, duration: 0.597s, episode steps:  86, steps per second: 144, episode reward: -50.383, mean reward: -0.586 [-100.000, 18.485], mean action: 1.767 [0.000, 3.000],  loss: 122.534612, mse: 90111.707031, mean_q: 315.889662, mean_eps: 0.488909
 170503/300000: episode: 1501, duration: 0.686s, episode steps:  96, steps per second: 140, episode reward: -149.727, mean reward: -1.560 [-100.000,  2.970], mean action: 1.604 [0.000, 3.000],  loss: 85.726386, mse: 90147.991943, mean_q: 316.418731, mean_eps: 0.488636
 170588/300000: episode: 1502, duration: 0.629s, episode steps:  85, steps per second: 135, episode reward: -67.908, mean reward: -0.799 [-100.000,  7.246], mean action: 1.553 [0.000, 3.000],  loss: 106.303500, mse: 89484.575919, mean_q: 313.483538, mean_eps: 0.488365
 171588/300000: episode: 1503, duration: 8.018s, episode steps: 1000, steps per second: 125, episode reward: 74.914, mean reward:  0.075 [-24.562, 24.321], mean action: 1.759 [0.000, 3.000],  loss: 98.905342, mse: 88198.127008, mean_q: 313.724442, mean_eps: 0.486737
 171699/300000: episode: 1504, duration: 0.894s, episode steps: 111, steps per second: 124, episode reward: 19.414, mean reward:  0.175 [-100.000, 13.570], mean action: 1.865 [0.000, 3.000],  loss: 99.011890, mse: 81703.547227, mean_q: 300.294344, mean_eps: 0.485071
 171812/300000: episode: 1505, duration: 0.889s, episode steps: 113, steps per second: 127, episode reward: -92.309, mean reward: -0.817 [-100.000, 23.083], mean action: 1.310 [0.000, 3.000],  loss: 65.913987, mse: 84320.771294, mean_q: 307.095493, mean_eps: 0.484735
 171912/300000: episode: 1506, duration: 0.782s, episode steps: 100, steps per second: 128, episode reward: -84.164, mean reward: -0.842 [-100.000, 19.273], mean action: 1.620 [0.000, 3.000],  loss: 85.244150, mse: 82228.052969, mean_q: 300.985467, mean_eps: 0.484416
 171980/300000: episode: 1507, duration: 0.480s, episode steps:  68, steps per second: 142, episode reward: -47.022, mean reward: -0.691 [-100.000,  8.130], mean action: 1.574 [0.000, 3.000],  loss: 120.443984, mse: 79948.671645, mean_q: 297.342648, mean_eps: 0.484164
 172096/300000: episode: 1508, duration: 0.897s, episode steps: 116, steps per second: 129, episode reward: -44.422, mean reward: -0.383 [-100.000, 11.331], mean action: 1.759 [0.000, 3.000],  loss: 116.242168, mse: 79966.409079, mean_q: 296.249371, mean_eps: 0.483888
 172196/300000: episode: 1509, duration: 0.728s, episode steps: 100, steps per second: 137, episode reward: -69.576, mean reward: -0.696 [-100.000, 13.992], mean action: 1.720 [0.000, 3.000],  loss: 91.667877, mse: 79864.292500, mean_q: 297.632968, mean_eps: 0.483563
 173196/300000: episode: 1510, duration: 7.784s, episode steps: 1000, steps per second: 128, episode reward: 49.793, mean reward:  0.050 [-24.295, 58.945], mean action: 1.812 [0.000, 3.000],  loss: 82.249390, mse: 75748.831391, mean_q: 288.058658, mean_eps: 0.481913
 173304/300000: episode: 1511, duration: 0.809s, episode steps: 108, steps per second: 133, episode reward: -35.524, mean reward: -0.329 [-100.000, 16.064], mean action: 1.713 [0.000, 3.000],  loss: 79.837857, mse: 72302.663303, mean_q: 280.975133, mean_eps: 0.480252
 173392/300000: episode: 1512, duration: 0.666s, episode steps:  88, steps per second: 132, episode reward: 10.350, mean reward:  0.118 [-100.000, 15.638], mean action: 1.807 [0.000, 3.000],  loss: 97.014442, mse: 69466.169522, mean_q: 271.841648, mean_eps: 0.479958
 173514/300000: episode: 1513, duration: 0.870s, episode steps: 122, steps per second: 140, episode reward: -6.390, mean reward: -0.052 [-100.000, 14.530], mean action: 1.590 [0.000, 3.000],  loss: 112.813008, mse: 71655.985496, mean_q: 278.306433, mean_eps: 0.479643
 173619/300000: episode: 1514, duration: 0.771s, episode steps: 105, steps per second: 136, episode reward: -82.258, mean reward: -0.783 [-100.000,  7.220], mean action: 1.524 [0.000, 3.000],  loss: 66.814707, mse: 70479.979613, mean_q: 275.996359, mean_eps: 0.479302
 173725/300000: episode: 1515, duration: 0.730s, episode steps: 106, steps per second: 145, episode reward: -35.300, mean reward: -0.333 [-100.000, 12.853], mean action: 1.613 [0.000, 3.000],  loss: 81.894563, mse: 72010.274138, mean_q: 281.022120, mean_eps: 0.478986
 173819/300000: episode: 1516, duration: 0.649s, episode steps:  94, steps per second: 145, episode reward: -25.533, mean reward: -0.272 [-100.000,  8.852], mean action: 1.702 [0.000, 3.000],  loss: 76.381541, mse: 70667.251205, mean_q: 277.923091, mean_eps: 0.478686
 173912/300000: episode: 1517, duration: 0.657s, episode steps:  93, steps per second: 141, episode reward: -22.402, mean reward: -0.241 [-100.000, 19.659], mean action: 1.731 [0.000, 3.000],  loss: 80.730389, mse: 71142.093204, mean_q: 278.754937, mean_eps: 0.478405
 174012/300000: episode: 1518, duration: 0.715s, episode steps: 100, steps per second: 140, episode reward: -109.217, mean reward: -1.092 [-100.000,  4.922], mean action: 1.730 [0.000, 3.000],  loss: 94.803219, mse: 71819.833047, mean_q: 280.903521, mean_eps: 0.478115
 174120/300000: episode: 1519, duration: 0.744s, episode steps: 108, steps per second: 145, episode reward: -177.799, mean reward: -1.646 [-100.000,  3.848], mean action: 1.546 [0.000, 3.000],  loss: 92.345774, mse: 71292.084165, mean_q: 281.267083, mean_eps: 0.477803
 174206/300000: episode: 1520, duration: 0.623s, episode steps:  86, steps per second: 138, episode reward: -32.728, mean reward: -0.381 [-100.000, 13.653], mean action: 1.698 [0.000, 3.000],  loss: 67.202130, mse: 71330.412109, mean_q: 280.956606, mean_eps: 0.477512
 174309/300000: episode: 1521, duration: 0.719s, episode steps: 103, steps per second: 143, episode reward:  5.578, mean reward:  0.054 [-100.000, 22.342], mean action: 1.602 [0.000, 3.000],  loss: 103.999441, mse: 69689.822702, mean_q: 276.224505, mean_eps: 0.477229
 174395/300000: episode: 1522, duration: 0.591s, episode steps:  86, steps per second: 145, episode reward: -33.451, mean reward: -0.389 [-100.000, 21.225], mean action: 1.628 [0.000, 3.000],  loss: 79.034498, mse: 68634.296239, mean_q: 275.411502, mean_eps: 0.476946
 174556/300000: episode: 1523, duration: 1.139s, episode steps: 161, steps per second: 141, episode reward: -121.142, mean reward: -0.752 [-100.000, 11.553], mean action: 1.615 [0.000, 3.000],  loss: 57.117272, mse: 70687.427868, mean_q: 280.868001, mean_eps: 0.476575
 174657/300000: episode: 1524, duration: 0.692s, episode steps: 101, steps per second: 146, episode reward: -54.527, mean reward: -0.540 [-100.000, 16.114], mean action: 1.554 [0.000, 3.000],  loss: 87.831239, mse: 67756.045869, mean_q: 274.426767, mean_eps: 0.476182
 174755/300000: episode: 1525, duration: 0.671s, episode steps:  98, steps per second: 146, episode reward: -117.411, mean reward: -1.198 [-100.000, 56.452], mean action: 1.867 [0.000, 3.000],  loss: 67.370257, mse: 67129.373964, mean_q: 273.584084, mean_eps: 0.475883
 174836/300000: episode: 1526, duration: 0.609s, episode steps:  81, steps per second: 133, episode reward: -7.150, mean reward: -0.088 [-100.000, 23.671], mean action: 1.704 [0.000, 3.000],  loss: 64.448024, mse: 66280.914014, mean_q: 272.290489, mean_eps: 0.475615
 174973/300000: episode: 1527, duration: 0.951s, episode steps: 137, steps per second: 144, episode reward: -54.203, mean reward: -0.396 [-100.000,  9.764], mean action: 1.664 [0.000, 3.000],  loss: 73.723800, mse: 66443.764285, mean_q: 272.645743, mean_eps: 0.475288
 175132/300000: episode: 1528, duration: 1.131s, episode steps: 159, steps per second: 141, episode reward: -165.212, mean reward: -1.039 [-100.000, 10.502], mean action: 1.623 [0.000, 3.000],  loss: 88.438724, mse: 66117.828543, mean_q: 271.923821, mean_eps: 0.474844
 175214/300000: episode: 1529, duration: 0.577s, episode steps:  82, steps per second: 142, episode reward: -37.368, mean reward: -0.456 [-100.000,  8.883], mean action: 1.634 [0.000, 3.000],  loss: 81.973283, mse: 67008.004335, mean_q: 274.365552, mean_eps: 0.474482
 175340/300000: episode: 1530, duration: 0.866s, episode steps: 126, steps per second: 145, episode reward: -6.147, mean reward: -0.049 [-100.000, 19.775], mean action: 1.690 [0.000, 3.000],  loss: 84.645108, mse: 66730.463139, mean_q: 273.356369, mean_eps: 0.474170
 175648/300000: episode: 1531, duration: 2.178s, episode steps: 308, steps per second: 141, episode reward: -228.063, mean reward: -0.740 [-100.000, 22.783], mean action: 1.487 [0.000, 3.000],  loss: 88.394425, mse: 64980.137074, mean_q: 269.173723, mean_eps: 0.473519
 175727/300000: episode: 1532, duration: 0.558s, episode steps:  79, steps per second: 142, episode reward: -140.425, mean reward: -1.778 [-100.000,  7.856], mean action: 1.494 [0.000, 3.000],  loss: 71.549862, mse: 65534.363430, mean_q: 270.408329, mean_eps: 0.472939
 175813/300000: episode: 1533, duration: 0.607s, episode steps:  86, steps per second: 142, episode reward: -18.316, mean reward: -0.213 [-100.000,  6.866], mean action: 1.837 [0.000, 3.000],  loss: 66.669503, mse: 62921.692633, mean_q: 264.474189, mean_eps: 0.472691
 176813/300000: episode: 1534, duration: 7.806s, episode steps: 1000, steps per second: 128, episode reward: 23.473, mean reward:  0.023 [-23.983, 23.066], mean action: 1.547 [0.000, 3.000],  loss: 69.220580, mse: 62881.183980, mean_q: 263.665592, mean_eps: 0.471062
 176897/300000: episode: 1535, duration: 0.604s, episode steps:  84, steps per second: 139, episode reward: -38.974, mean reward: -0.464 [-100.000, 13.599], mean action: 1.940 [0.000, 3.000],  loss: 59.504031, mse: 61512.865374, mean_q: 259.811818, mean_eps: 0.469437
 177024/300000: episode: 1536, duration: 0.879s, episode steps: 127, steps per second: 145, episode reward: -170.317, mean reward: -1.341 [-100.000,  3.519], mean action: 1.906 [0.000, 3.000],  loss: 77.820119, mse: 60617.528697, mean_q: 258.203867, mean_eps: 0.469120
 177173/300000: episode: 1537, duration: 1.057s, episode steps: 149, steps per second: 141, episode reward: -4.174, mean reward: -0.028 [-100.000, 29.182], mean action: 1.591 [0.000, 3.000],  loss: 68.734475, mse: 59888.642041, mean_q: 255.742209, mean_eps: 0.468706
 177263/300000: episode: 1538, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -34.036, mean reward: -0.378 [-100.000, 23.685], mean action: 1.567 [0.000, 3.000],  loss: 106.906869, mse: 59299.456901, mean_q: 253.916674, mean_eps: 0.468348
 177378/300000: episode: 1539, duration: 0.790s, episode steps: 115, steps per second: 146, episode reward: 17.125, mean reward:  0.149 [-100.000, 17.171], mean action: 1.704 [0.000, 3.000],  loss: 54.075683, mse: 59396.777276, mean_q: 254.261862, mean_eps: 0.468040
 177462/300000: episode: 1540, duration: 0.601s, episode steps:  84, steps per second: 140, episode reward: -30.503, mean reward: -0.363 [-100.000, 17.287], mean action: 1.595 [0.000, 3.000],  loss: 98.815651, mse: 59147.610538, mean_q: 253.411100, mean_eps: 0.467741
 177552/300000: episode: 1541, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: 47.430, mean reward:  0.527 [-100.000, 19.144], mean action: 1.944 [0.000, 3.000],  loss: 54.712080, mse: 58036.680642, mean_q: 250.637988, mean_eps: 0.467481
 177684/300000: episode: 1542, duration: 0.928s, episode steps: 132, steps per second: 142, episode reward: -118.940, mean reward: -0.901 [-100.000, 11.472], mean action: 1.455 [0.000, 3.000],  loss: 63.244215, mse: 56185.135683, mean_q: 245.983665, mean_eps: 0.467148
 177777/300000: episode: 1543, duration: 0.652s, episode steps:  93, steps per second: 143, episode reward: -1.443, mean reward: -0.016 [-100.000, 16.429], mean action: 1.753 [0.000, 3.000],  loss: 48.313293, mse: 55338.089718, mean_q: 244.368038, mean_eps: 0.466810
 177880/300000: episode: 1544, duration: 0.757s, episode steps: 103, steps per second: 136, episode reward: 37.668, mean reward:  0.366 [-100.000, 14.704], mean action: 1.932 [0.000, 3.000],  loss: 77.546172, mse: 54032.662432, mean_q: 239.942849, mean_eps: 0.466516
 178002/300000: episode: 1545, duration: 0.845s, episode steps: 122, steps per second: 144, episode reward: 17.383, mean reward:  0.142 [-100.000, 16.783], mean action: 1.648 [0.000, 3.000],  loss: 66.089898, mse: 55355.948674, mean_q: 243.212747, mean_eps: 0.466179
 178092/300000: episode: 1546, duration: 0.648s, episode steps:  90, steps per second: 139, episode reward: -48.467, mean reward: -0.539 [-100.000, 11.617], mean action: 1.489 [0.000, 3.000],  loss: 78.637171, mse: 53570.491059, mean_q: 238.448035, mean_eps: 0.465861
 178204/300000: episode: 1547, duration: 0.777s, episode steps: 112, steps per second: 144, episode reward: -54.167, mean reward: -0.484 [-100.000, 11.829], mean action: 1.562 [0.000, 3.000],  loss: 64.930377, mse: 54471.560721, mean_q: 241.260790, mean_eps: 0.465558
 178300/300000: episode: 1548, duration: 0.675s, episode steps:  96, steps per second: 142, episode reward: -10.618, mean reward: -0.111 [-100.000, 16.388], mean action: 1.688 [0.000, 3.000],  loss: 64.048590, mse: 52436.175212, mean_q: 236.430475, mean_eps: 0.465245
 178399/300000: episode: 1549, duration: 0.732s, episode steps:  99, steps per second: 135, episode reward: -9.756, mean reward: -0.099 [-100.000, 16.288], mean action: 1.687 [0.000, 3.000],  loss: 69.733162, mse: 53093.531842, mean_q: 238.393845, mean_eps: 0.464953
 179399/300000: episode: 1550, duration: 7.344s, episode steps: 1000, steps per second: 136, episode reward: 37.622, mean reward:  0.038 [-24.926, 21.822], mean action: 1.130 [0.000, 3.000],  loss: 61.832803, mse: 50864.459945, mean_q: 233.409154, mean_eps: 0.463305
 179504/300000: episode: 1551, duration: 0.742s, episode steps: 105, steps per second: 142, episode reward: -8.463, mean reward: -0.081 [-100.000, 20.364], mean action: 1.552 [0.000, 3.000],  loss: 55.054964, mse: 48997.881510, mean_q: 229.755711, mean_eps: 0.461647
 179626/300000: episode: 1552, duration: 0.878s, episode steps: 122, steps per second: 139, episode reward: -52.054, mean reward: -0.427 [-100.000, 10.271], mean action: 1.590 [0.000, 3.000],  loss: 61.592954, mse: 47465.261559, mean_q: 225.585617, mean_eps: 0.461307
 179703/300000: episode: 1553, duration: 0.538s, episode steps:  77, steps per second: 143, episode reward: -25.212, mean reward: -0.327 [-100.000,  7.899], mean action: 1.753 [0.000, 3.000],  loss: 53.116173, mse: 46369.848062, mean_q: 223.635715, mean_eps: 0.461008
 179790/300000: episode: 1554, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -49.585, mean reward: -0.570 [-100.000,  9.174], mean action: 1.701 [0.000, 3.000],  loss: 72.729622, mse: 45825.446480, mean_q: 222.034825, mean_eps: 0.460762
 179912/300000: episode: 1555, duration: 0.885s, episode steps: 122, steps per second: 138, episode reward: -51.706, mean reward: -0.424 [-100.000,  8.210], mean action: 1.713 [0.000, 3.000],  loss: 64.600655, mse: 45654.030321, mean_q: 222.478791, mean_eps: 0.460449
 180028/300000: episode: 1556, duration: 0.822s, episode steps: 116, steps per second: 141, episode reward: -64.156, mean reward: -0.553 [-100.000, 13.493], mean action: 1.500 [0.000, 3.000],  loss: 69.811605, mse: 45652.138234, mean_q: 221.824604, mean_eps: 0.460091
 180179/300000: episode: 1557, duration: 1.064s, episode steps: 151, steps per second: 142, episode reward: -0.319, mean reward: -0.002 [-100.000, 16.540], mean action: 1.662 [0.000, 3.000],  loss: 73.248613, mse: 44912.400028, mean_q: 219.431912, mean_eps: 0.459691
 180275/300000: episode: 1558, duration: 0.684s, episode steps:  96, steps per second: 140, episode reward: -12.938, mean reward: -0.135 [-100.000, 14.116], mean action: 1.792 [0.000, 3.000],  loss: 48.889628, mse: 44320.029582, mean_q: 218.927924, mean_eps: 0.459321
 180374/300000: episode: 1559, duration: 0.687s, episode steps:  99, steps per second: 144, episode reward: 27.634, mean reward:  0.279 [-100.000, 19.018], mean action: 1.707 [0.000, 3.000],  loss: 71.739378, mse: 43608.706716, mean_q: 216.047075, mean_eps: 0.459028
 180458/300000: episode: 1560, duration: 0.669s, episode steps:  84, steps per second: 126, episode reward: -35.260, mean reward: -0.420 [-100.000, 10.615], mean action: 1.738 [0.000, 3.000],  loss: 50.587480, mse: 43572.434431, mean_q: 216.134564, mean_eps: 0.458754
 180541/300000: episode: 1561, duration: 0.765s, episode steps:  83, steps per second: 109, episode reward: -90.300, mean reward: -1.088 [-100.000,  7.219], mean action: 1.627 [0.000, 3.000],  loss: 54.496387, mse: 42279.818901, mean_q: 213.018924, mean_eps: 0.458503
 180631/300000: episode: 1562, duration: 0.794s, episode steps:  90, steps per second: 113, episode reward:  8.175, mean reward:  0.091 [-100.000, 13.409], mean action: 1.767 [0.000, 3.000],  loss: 57.777089, mse: 42969.529102, mean_q: 215.434127, mean_eps: 0.458243
 180724/300000: episode: 1563, duration: 0.834s, episode steps:  93, steps per second: 112, episode reward: -142.339, mean reward: -1.531 [-100.000, 12.007], mean action: 1.570 [0.000, 3.000],  loss: 48.846954, mse: 42244.681872, mean_q: 212.914341, mean_eps: 0.457969
 180879/300000: episode: 1564, duration: 1.234s, episode steps: 155, steps per second: 126, episode reward: -240.651, mean reward: -1.553 [-100.000,  6.917], mean action: 1.626 [0.000, 3.000],  loss: 47.988850, mse: 42293.475202, mean_q: 214.358829, mean_eps: 0.457597
 180954/300000: episode: 1565, duration: 0.576s, episode steps:  75, steps per second: 130, episode reward: -53.907, mean reward: -0.719 [-100.000, 12.761], mean action: 1.440 [0.000, 3.000],  loss: 42.368144, mse: 42846.422448, mean_q: 215.081636, mean_eps: 0.457252
 181049/300000: episode: 1566, duration: 0.699s, episode steps:  95, steps per second: 136, episode reward: -82.781, mean reward: -0.871 [-100.000, 18.963], mean action: 1.632 [0.000, 3.000],  loss: 48.352814, mse: 41348.259498, mean_q: 210.638708, mean_eps: 0.456997
 181130/300000: episode: 1567, duration: 0.565s, episode steps:  81, steps per second: 143, episode reward: -90.156, mean reward: -1.113 [-100.000,  7.230], mean action: 1.556 [0.000, 3.000],  loss: 54.163909, mse: 41458.958237, mean_q: 212.585856, mean_eps: 0.456733
 181244/300000: episode: 1568, duration: 0.789s, episode steps: 114, steps per second: 145, episode reward: -50.954, mean reward: -0.447 [-100.000,  8.777], mean action: 1.640 [0.000, 3.000],  loss: 58.073754, mse: 41708.031044, mean_q: 212.878396, mean_eps: 0.456441
 181429/300000: episode: 1569, duration: 1.325s, episode steps: 185, steps per second: 140, episode reward: -251.808, mean reward: -1.361 [-100.000,  9.943], mean action: 1.659 [0.000, 3.000],  loss: 39.897513, mse: 41265.626478, mean_q: 211.727613, mean_eps: 0.455992
 181525/300000: episode: 1570, duration: 0.660s, episode steps:  96, steps per second: 145, episode reward: -65.628, mean reward: -0.684 [-100.000, 10.834], mean action: 1.521 [0.000, 3.000],  loss: 45.423413, mse: 40312.763794, mean_q: 208.235342, mean_eps: 0.455571
 181638/300000: episode: 1571, duration: 0.794s, episode steps: 113, steps per second: 142, episode reward: -15.061, mean reward: -0.133 [-100.000, 11.996], mean action: 1.398 [0.000, 3.000],  loss: 40.674675, mse: 40461.138897, mean_q: 209.614429, mean_eps: 0.455257
 181765/300000: episode: 1572, duration: 0.877s, episode steps: 127, steps per second: 145, episode reward: -87.406, mean reward: -0.688 [-100.000, 14.410], mean action: 1.567 [0.000, 3.000],  loss: 57.503342, mse: 40095.417661, mean_q: 207.552383, mean_eps: 0.454897
 181853/300000: episode: 1573, duration: 0.618s, episode steps:  88, steps per second: 143, episode reward: -103.049, mean reward: -1.171 [-100.000,  5.854], mean action: 1.443 [0.000, 3.000],  loss: 60.296108, mse: 39054.911244, mean_q: 203.363141, mean_eps: 0.454574
 181978/300000: episode: 1574, duration: 0.961s, episode steps: 125, steps per second: 130, episode reward: -90.012, mean reward: -0.720 [-100.000,  6.652], mean action: 1.824 [0.000, 3.000],  loss: 48.051967, mse: 38617.487828, mean_q: 202.402372, mean_eps: 0.454255
 182117/300000: episode: 1575, duration: 1.029s, episode steps: 139, steps per second: 135, episode reward: -5.003, mean reward: -0.036 [-100.000, 16.777], mean action: 1.612 [0.000, 3.000],  loss: 47.160132, mse: 37800.406292, mean_q: 200.800896, mean_eps: 0.453859
 182219/300000: episode: 1576, duration: 0.770s, episode steps: 102, steps per second: 132, episode reward:  8.438, mean reward:  0.083 [-100.000, 23.535], mean action: 1.804 [0.000, 3.000],  loss: 51.452637, mse: 37588.019244, mean_q: 199.506645, mean_eps: 0.453498
 183219/300000: episode: 1577, duration: 7.906s, episode steps: 1000, steps per second: 126, episode reward: 48.904, mean reward:  0.049 [-21.367, 48.804], mean action: 1.445 [0.000, 3.000],  loss: 45.814738, mse: 33936.324787, mean_q: 189.444131, mean_eps: 0.451844
 183301/300000: episode: 1578, duration: 0.585s, episode steps:  82, steps per second: 140, episode reward:  8.747, mean reward:  0.107 [-100.000, 17.357], mean action: 1.780 [0.000, 3.000],  loss: 38.896714, mse: 30612.443264, mean_q: 179.833911, mean_eps: 0.450221
 183389/300000: episode: 1579, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: 25.754, mean reward:  0.293 [-100.000, 15.119], mean action: 1.875 [0.000, 3.000],  loss: 45.067784, mse: 30267.546475, mean_q: 178.230558, mean_eps: 0.449966
 183544/300000: episode: 1580, duration: 1.066s, episode steps: 155, steps per second: 145, episode reward: -209.070, mean reward: -1.349 [-100.000, 36.361], mean action: 1.645 [0.000, 3.000],  loss: 40.198934, mse: 30980.177533, mean_q: 180.814696, mean_eps: 0.449602
 183649/300000: episode: 1581, duration: 0.747s, episode steps: 105, steps per second: 141, episode reward: -19.849, mean reward: -0.189 [-100.000, 10.530], mean action: 1.714 [0.000, 3.000],  loss: 36.105427, mse: 30137.003069, mean_q: 177.120619, mean_eps: 0.449212
 183751/300000: episode: 1582, duration: 0.702s, episode steps: 102, steps per second: 145, episode reward: -33.152, mean reward: -0.325 [-100.000, 11.419], mean action: 1.578 [0.000, 3.000],  loss: 41.839650, mse: 30191.031920, mean_q: 178.362315, mean_eps: 0.448902
 183830/300000: episode: 1583, duration: 0.568s, episode steps:  79, steps per second: 139, episode reward: -13.993, mean reward: -0.177 [-100.000,  8.835], mean action: 1.468 [0.000, 3.000],  loss: 50.113505, mse: 29547.471074, mean_q: 176.746897, mean_eps: 0.448630
 183949/300000: episode: 1584, duration: 0.838s, episode steps: 119, steps per second: 142, episode reward: -249.013, mean reward: -2.093 [-100.000,  9.380], mean action: 1.655 [0.000, 3.000],  loss: 37.915121, mse: 28992.382402, mean_q: 174.809889, mean_eps: 0.448333
 184049/300000: episode: 1585, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -51.114, mean reward: -0.511 [-100.000, 11.237], mean action: 1.280 [0.000, 3.000],  loss: 41.222836, mse: 28543.966680, mean_q: 172.730868, mean_eps: 0.448004
 184156/300000: episode: 1586, duration: 0.742s, episode steps: 107, steps per second: 144, episode reward: -19.641, mean reward: -0.184 [-100.000,  9.832], mean action: 1.692 [0.000, 3.000],  loss: 35.592392, mse: 28918.195294, mean_q: 174.121340, mean_eps: 0.447694
 184252/300000: episode: 1587, duration: 0.703s, episode steps:  96, steps per second: 137, episode reward: -10.372, mean reward: -0.108 [-100.000, 17.527], mean action: 1.469 [0.000, 3.000],  loss: 33.577473, mse: 28112.868591, mean_q: 171.597761, mean_eps: 0.447390
 184356/300000: episode: 1588, duration: 0.722s, episode steps: 104, steps per second: 144, episode reward:  3.394, mean reward:  0.033 [-100.000, 14.563], mean action: 1.740 [0.000, 3.000],  loss: 43.161878, mse: 28191.604887, mean_q: 172.055654, mean_eps: 0.447090
 184437/300000: episode: 1589, duration: 0.557s, episode steps:  81, steps per second: 146, episode reward:  9.961, mean reward:  0.123 [-100.000, 11.440], mean action: 1.988 [0.000, 3.000],  loss: 45.179701, mse: 27901.407866, mean_q: 170.288260, mean_eps: 0.446812
 184542/300000: episode: 1590, duration: 0.768s, episode steps: 105, steps per second: 137, episode reward: -35.891, mean reward: -0.342 [-100.000, 11.876], mean action: 1.676 [0.000, 3.000],  loss: 50.669817, mse: 26934.584784, mean_q: 166.929714, mean_eps: 0.446533
 184650/300000: episode: 1591, duration: 0.760s, episode steps: 108, steps per second: 142, episode reward: -16.267, mean reward: -0.151 [-100.000, 12.491], mean action: 1.759 [0.000, 3.000],  loss: 38.169294, mse: 27140.756746, mean_q: 169.145716, mean_eps: 0.446213
 185086/300000: episode: 1592, duration: 3.167s, episode steps: 436, steps per second: 138, episode reward: -144.975, mean reward: -0.333 [-100.000, 13.790], mean action: 1.892 [0.000, 3.000],  loss: 38.722610, mse: 26513.169796, mean_q: 167.214755, mean_eps: 0.445398
 185175/300000: episode: 1593, duration: 0.629s, episode steps:  89, steps per second: 141, episode reward: -34.384, mean reward: -0.386 [-100.000, 14.161], mean action: 1.663 [0.000, 3.000],  loss: 50.855451, mse: 25803.717565, mean_q: 164.881594, mean_eps: 0.444610
 185327/300000: episode: 1594, duration: 1.059s, episode steps: 152, steps per second: 144, episode reward: -101.038, mean reward: -0.665 [-100.000, 17.894], mean action: 1.842 [0.000, 3.000],  loss: 41.125398, mse: 24859.239630, mean_q: 161.968108, mean_eps: 0.444248
 185431/300000: episode: 1595, duration: 0.737s, episode steps: 104, steps per second: 141, episode reward: -57.355, mean reward: -0.551 [-100.000, 11.478], mean action: 1.558 [0.000, 3.000],  loss: 40.388130, mse: 25031.119235, mean_q: 161.960732, mean_eps: 0.443864
 185527/300000: episode: 1596, duration: 0.659s, episode steps:  96, steps per second: 146, episode reward:  4.292, mean reward:  0.045 [-100.000, 11.728], mean action: 1.635 [0.000, 3.000],  loss: 47.989736, mse: 25020.246175, mean_q: 162.703246, mean_eps: 0.443565
 185625/300000: episode: 1597, duration: 0.682s, episode steps:  98, steps per second: 144, episode reward:  2.026, mean reward:  0.021 [-100.000, 13.138], mean action: 1.796 [0.000, 3.000],  loss: 42.377400, mse: 24410.891721, mean_q: 160.888991, mean_eps: 0.443274
 185745/300000: episode: 1598, duration: 0.830s, episode steps: 120, steps per second: 145, episode reward: -0.272, mean reward: -0.002 [-100.000, 14.430], mean action: 1.542 [0.000, 3.000],  loss: 51.990128, mse: 24409.625993, mean_q: 161.288961, mean_eps: 0.442947
 185884/300000: episode: 1599, duration: 0.947s, episode steps: 139, steps per second: 147, episode reward: -42.436, mean reward: -0.305 [-100.000,  9.345], mean action: 1.604 [0.000, 3.000],  loss: 35.197325, mse: 24574.945762, mean_q: 161.760517, mean_eps: 0.442558
 186003/300000: episode: 1600, duration: 0.814s, episode steps: 119, steps per second: 146, episode reward: -35.953, mean reward: -0.302 [-100.000, 17.951], mean action: 1.697 [0.000, 3.000],  loss: 58.157902, mse: 23933.082474, mean_q: 159.314609, mean_eps: 0.442171
 186083/300000: episode: 1601, duration: 0.586s, episode steps:  80, steps per second: 137, episode reward: 42.813, mean reward:  0.535 [-100.000, 18.836], mean action: 1.750 [0.000, 3.000],  loss: 33.114666, mse: 23919.709155, mean_q: 158.493946, mean_eps: 0.441873
 186171/300000: episode: 1602, duration: 0.618s, episode steps:  88, steps per second: 142, episode reward: 11.524, mean reward:  0.131 [-100.000, 10.220], mean action: 1.784 [0.000, 3.000],  loss: 46.248553, mse: 24183.088157, mean_q: 160.458102, mean_eps: 0.441620
 186258/300000: episode: 1603, duration: 0.614s, episode steps:  87, steps per second: 142, episode reward:  2.951, mean reward:  0.034 [-100.000, 16.595], mean action: 1.586 [0.000, 3.000],  loss: 41.277345, mse: 23835.666061, mean_q: 159.598440, mean_eps: 0.441358
 186350/300000: episode: 1604, duration: 0.653s, episode steps:  92, steps per second: 141, episode reward: -22.215, mean reward: -0.241 [-100.000, 12.924], mean action: 1.620 [0.000, 3.000],  loss: 35.325332, mse: 23788.640837, mean_q: 159.433964, mean_eps: 0.441090
 186443/300000: episode: 1605, duration: 0.646s, episode steps:  93, steps per second: 144, episode reward: -25.999, mean reward: -0.280 [-100.000,  8.979], mean action: 1.613 [0.000, 3.000],  loss: 39.202215, mse: 23020.403919, mean_q: 154.898003, mean_eps: 0.440812
 186530/300000: episode: 1606, duration: 0.599s, episode steps:  87, steps per second: 145, episode reward: -31.725, mean reward: -0.365 [-100.000, 11.705], mean action: 1.460 [0.000, 3.000],  loss: 44.059174, mse: 22749.825207, mean_q: 154.032222, mean_eps: 0.440542
 186647/300000: episode: 1607, duration: 0.823s, episode steps: 117, steps per second: 142, episode reward: -173.094, mean reward: -1.479 [-100.000, 40.699], mean action: 2.017 [0.000, 3.000],  loss: 31.927903, mse: 22872.041383, mean_q: 153.941914, mean_eps: 0.440236
 186756/300000: episode: 1608, duration: 0.782s, episode steps: 109, steps per second: 139, episode reward: -15.898, mean reward: -0.146 [-100.000, 13.000], mean action: 1.743 [0.000, 3.000],  loss: 30.158832, mse: 22652.420997, mean_q: 153.979902, mean_eps: 0.439897
 186833/300000: episode: 1609, duration: 0.537s, episode steps:  77, steps per second: 143, episode reward: -14.414, mean reward: -0.187 [-100.000, 13.795], mean action: 1.779 [0.000, 3.000],  loss: 53.940098, mse: 22279.495384, mean_q: 151.669121, mean_eps: 0.439618
 186917/300000: episode: 1610, duration: 0.583s, episode steps:  84, steps per second: 144, episode reward: -14.312, mean reward: -0.170 [-100.000, 11.776], mean action: 1.798 [0.000, 3.000],  loss: 38.632884, mse: 21585.952346, mean_q: 150.093668, mean_eps: 0.439377
 187017/300000: episode: 1611, duration: 0.721s, episode steps: 100, steps per second: 139, episode reward: -63.620, mean reward: -0.636 [-100.000, 13.763], mean action: 1.700 [0.000, 3.000],  loss: 44.812808, mse: 21550.485742, mean_q: 150.184375, mean_eps: 0.439101
 187138/300000: episode: 1612, duration: 0.836s, episode steps: 121, steps per second: 145, episode reward: 11.955, mean reward:  0.099 [-100.000,  7.510], mean action: 1.669 [0.000, 3.000],  loss: 36.801248, mse: 21675.502195, mean_q: 149.920731, mean_eps: 0.438769
 187217/300000: episode: 1613, duration: 0.565s, episode steps:  79, steps per second: 140, episode reward: -7.312, mean reward: -0.093 [-100.000, 23.759], mean action: 1.747 [0.000, 3.000],  loss: 32.180910, mse: 20227.515279, mean_q: 144.527535, mean_eps: 0.438469
 187309/300000: episode: 1614, duration: 0.648s, episode steps:  92, steps per second: 142, episode reward:  2.648, mean reward:  0.029 [-100.000, 20.860], mean action: 1.630 [0.000, 3.000],  loss: 46.179386, mse: 21208.314082, mean_q: 148.064744, mean_eps: 0.438213
 187397/300000: episode: 1615, duration: 0.605s, episode steps:  88, steps per second: 145, episode reward: -51.485, mean reward: -0.585 [-100.000, 10.524], mean action: 1.773 [0.000, 3.000],  loss: 36.335331, mse: 21703.513716, mean_q: 150.201107, mean_eps: 0.437942
 187517/300000: episode: 1616, duration: 0.826s, episode steps: 120, steps per second: 145, episode reward: -34.344, mean reward: -0.286 [-100.000, 17.400], mean action: 1.542 [0.000, 3.000],  loss: 29.234945, mse: 21442.649634, mean_q: 149.036562, mean_eps: 0.437630
 187593/300000: episode: 1617, duration: 0.532s, episode steps:  76, steps per second: 143, episode reward: -38.990, mean reward: -0.513 [-100.000, 23.025], mean action: 1.842 [0.000, 3.000],  loss: 33.438837, mse: 21477.868215, mean_q: 149.421789, mean_eps: 0.437337
 187690/300000: episode: 1618, duration: 0.673s, episode steps:  97, steps per second: 144, episode reward: 23.742, mean reward:  0.245 [-100.000, 19.275], mean action: 1.577 [0.000, 3.000],  loss: 36.181834, mse: 21595.103193, mean_q: 149.180487, mean_eps: 0.437077
 187755/300000: episode: 1619, duration: 0.450s, episode steps:  65, steps per second: 144, episode reward: -62.583, mean reward: -0.963 [-100.000,  9.893], mean action: 1.692 [0.000, 3.000],  loss: 38.609291, mse: 21178.758203, mean_q: 146.863884, mean_eps: 0.436834
 187889/300000: episode: 1620, duration: 0.960s, episode steps: 134, steps per second: 140, episode reward:  3.862, mean reward:  0.029 [-100.000, 11.490], mean action: 1.731 [0.000, 3.000],  loss: 42.763794, mse: 21381.723006, mean_q: 147.927957, mean_eps: 0.436535
 187965/300000: episode: 1621, duration: 0.529s, episode steps:  76, steps per second: 144, episode reward: -5.857, mean reward: -0.077 [-100.000, 18.125], mean action: 1.829 [0.000, 3.000],  loss: 29.590695, mse: 20778.153012, mean_q: 145.614291, mean_eps: 0.436220
 188105/300000: episode: 1622, duration: 0.974s, episode steps: 140, steps per second: 144, episode reward: -37.086, mean reward: -0.265 [-100.000, 11.122], mean action: 1.700 [0.000, 3.000],  loss: 36.968494, mse: 21103.778871, mean_q: 147.325163, mean_eps: 0.435896
 188185/300000: episode: 1623, duration: 0.573s, episode steps:  80, steps per second: 140, episode reward: -15.681, mean reward: -0.196 [-100.000, 18.499], mean action: 1.725 [0.000, 3.000],  loss: 33.984495, mse: 20635.534375, mean_q: 144.635818, mean_eps: 0.435567
 188294/300000: episode: 1624, duration: 0.751s, episode steps: 109, steps per second: 145, episode reward: -8.273, mean reward: -0.076 [-100.000, 23.860], mean action: 1.486 [0.000, 3.000],  loss: 34.941860, mse: 20300.196379, mean_q: 143.240288, mean_eps: 0.435283
 188375/300000: episode: 1625, duration: 0.564s, episode steps:  81, steps per second: 144, episode reward: -17.232, mean reward: -0.213 [-100.000, 17.420], mean action: 1.728 [0.000, 3.000],  loss: 45.327718, mse: 20765.926444, mean_q: 145.296409, mean_eps: 0.434998
 188525/300000: episode: 1626, duration: 1.056s, episode steps: 150, steps per second: 142, episode reward: -250.029, mean reward: -1.667 [-100.000, 15.872], mean action: 1.733 [0.000, 3.000],  loss: 30.919513, mse: 19974.604115, mean_q: 142.174952, mean_eps: 0.434652
 188732/300000: episode: 1627, duration: 1.449s, episode steps: 207, steps per second: 143, episode reward: -194.781, mean reward: -0.941 [-100.000, 23.142], mean action: 1.860 [0.000, 3.000],  loss: 33.533407, mse: 19756.796672, mean_q: 141.763346, mean_eps: 0.434116
 188808/300000: episode: 1628, duration: 0.533s, episode steps:  76, steps per second: 143, episode reward: -9.192, mean reward: -0.121 [-100.000, 12.695], mean action: 1.842 [0.000, 3.000],  loss: 29.938935, mse: 19678.573075, mean_q: 141.538070, mean_eps: 0.433692
 188895/300000: episode: 1629, duration: 0.608s, episode steps:  87, steps per second: 143, episode reward: 24.834, mean reward:  0.285 [-100.000, 13.553], mean action: 1.655 [0.000, 3.000],  loss: 33.626560, mse: 19661.985150, mean_q: 141.997014, mean_eps: 0.433447
 189031/300000: episode: 1630, duration: 0.931s, episode steps: 136, steps per second: 146, episode reward: 53.827, mean reward:  0.396 [-100.000, 18.536], mean action: 1.493 [0.000, 3.000],  loss: 33.259747, mse: 19304.758408, mean_q: 140.180095, mean_eps: 0.433113
 189134/300000: episode: 1631, duration: 0.733s, episode steps: 103, steps per second: 140, episode reward: -39.154, mean reward: -0.380 [-100.000, 18.796], mean action: 1.777 [0.000, 3.000],  loss: 39.741014, mse: 19554.392114, mean_q: 140.349773, mean_eps: 0.432754
 189416/300000: episode: 1632, duration: 2.032s, episode steps: 282, steps per second: 139, episode reward: -254.271, mean reward: -0.902 [-100.000, 20.185], mean action: 1.777 [0.000, 3.000],  loss: 37.598316, mse: 19190.235649, mean_q: 139.319541, mean_eps: 0.432177
 189581/300000: episode: 1633, duration: 1.277s, episode steps: 165, steps per second: 129, episode reward: -453.860, mean reward: -2.751 [-100.000,  4.731], mean action: 1.570 [0.000, 3.000],  loss: 33.690249, mse: 18901.371869, mean_q: 138.869084, mean_eps: 0.431506
 189827/300000: episode: 1634, duration: 1.913s, episode steps: 246, steps per second: 129, episode reward: 43.327, mean reward:  0.176 [-100.000, 15.257], mean action: 1.740 [0.000, 3.000],  loss: 27.961063, mse: 18436.449147, mean_q: 137.042557, mean_eps: 0.430890
 189944/300000: episode: 1635, duration: 0.881s, episode steps: 117, steps per second: 133, episode reward: -5.001, mean reward: -0.043 [-100.000, 13.444], mean action: 1.530 [0.000, 3.000],  loss: 28.854481, mse: 18064.923094, mean_q: 135.666579, mean_eps: 0.430345
 190048/300000: episode: 1636, duration: 0.807s, episode steps: 104, steps per second: 129, episode reward: -28.497, mean reward: -0.274 [-100.000, 18.674], mean action: 1.635 [0.000, 3.000],  loss: 37.056234, mse: 18020.572191, mean_q: 135.480389, mean_eps: 0.430014
 190129/300000: episode: 1637, duration: 0.574s, episode steps:  81, steps per second: 141, episode reward:  0.895, mean reward:  0.011 [-100.000, 14.113], mean action: 1.642 [0.000, 3.000],  loss: 31.590551, mse: 18424.877098, mean_q: 136.815824, mean_eps: 0.429736
 190202/300000: episode: 1638, duration: 0.503s, episode steps:  73, steps per second: 145, episode reward: -36.638, mean reward: -0.502 [-100.000,  9.751], mean action: 1.644 [0.000, 3.000],  loss: 37.669508, mse: 17615.048266, mean_q: 133.699024, mean_eps: 0.429505
 190281/300000: episode: 1639, duration: 0.554s, episode steps:  79, steps per second: 142, episode reward: -80.524, mean reward: -1.019 [-100.000, 11.333], mean action: 1.646 [0.000, 3.000],  loss: 38.947478, mse: 18018.048173, mean_q: 135.531809, mean_eps: 0.429277
 190394/300000: episode: 1640, duration: 0.809s, episode steps: 113, steps per second: 140, episode reward: -159.184, mean reward: -1.409 [-100.000, 27.551], mean action: 1.549 [0.000, 3.000],  loss: 30.002116, mse: 17575.294637, mean_q: 132.820800, mean_eps: 0.428989
 190537/300000: episode: 1641, duration: 1.139s, episode steps: 143, steps per second: 126, episode reward: -2.101, mean reward: -0.015 [-100.000, 10.009], mean action: 1.783 [0.000, 3.000],  loss: 24.792231, mse: 17191.102908, mean_q: 131.229045, mean_eps: 0.428605
 190651/300000: episode: 1642, duration: 0.917s, episode steps: 114, steps per second: 124, episode reward:  7.222, mean reward:  0.063 [-100.000, 20.214], mean action: 1.781 [0.000, 3.000],  loss: 25.203314, mse: 16813.817323, mean_q: 129.581788, mean_eps: 0.428219
 190732/300000: episode: 1643, duration: 0.561s, episode steps:  81, steps per second: 144, episode reward: -39.078, mean reward: -0.482 [-100.000, 18.386], mean action: 1.975 [0.000, 3.000],  loss: 32.052590, mse: 16825.857181, mean_q: 129.699977, mean_eps: 0.427927
 190843/300000: episode: 1644, duration: 0.774s, episode steps: 111, steps per second: 143, episode reward: 20.996, mean reward:  0.189 [-100.000, 14.776], mean action: 1.640 [0.000, 3.000],  loss: 36.131475, mse: 16807.831591, mean_q: 129.373921, mean_eps: 0.427639
 190968/300000: episode: 1645, duration: 0.944s, episode steps: 125, steps per second: 132, episode reward: -258.156, mean reward: -2.065 [-100.000,  4.976], mean action: 1.648 [0.000, 3.000],  loss: 24.205338, mse: 16514.137133, mean_q: 127.747398, mean_eps: 0.427285
 191094/300000: episode: 1646, duration: 0.955s, episode steps: 126, steps per second: 132, episode reward: 15.915, mean reward:  0.126 [-100.000, 14.598], mean action: 1.516 [0.000, 3.000],  loss: 31.072687, mse: 16383.570398, mean_q: 127.172935, mean_eps: 0.426908
 191194/300000: episode: 1647, duration: 0.776s, episode steps: 100, steps per second: 129, episode reward: 54.509, mean reward:  0.545 [-100.000, 15.380], mean action: 1.610 [0.000, 3.000],  loss: 30.671174, mse: 16301.530898, mean_q: 126.192015, mean_eps: 0.426569
 191292/300000: episode: 1648, duration: 0.729s, episode steps:  98, steps per second: 135, episode reward:  8.389, mean reward:  0.086 [-100.000, 16.830], mean action: 1.806 [0.000, 3.000],  loss: 21.631184, mse: 16115.942981, mean_q: 125.035159, mean_eps: 0.426273
 191429/300000: episode: 1649, duration: 1.010s, episode steps: 137, steps per second: 136, episode reward: -255.058, mean reward: -1.862 [-100.000, 34.475], mean action: 1.737 [0.000, 3.000],  loss: 27.154803, mse: 16120.468757, mean_q: 125.220151, mean_eps: 0.425920
 191683/300000: episode: 1650, duration: 1.813s, episode steps: 254, steps per second: 140, episode reward: -148.422, mean reward: -0.584 [-100.000, 14.939], mean action: 1.780 [0.000, 3.000],  loss: 24.361291, mse: 16034.826156, mean_q: 124.583436, mean_eps: 0.425333
 191796/300000: episode: 1651, duration: 0.816s, episode steps: 113, steps per second: 138, episode reward: 15.283, mean reward:  0.135 [-100.000, 16.106], mean action: 1.646 [0.000, 3.000],  loss: 32.324416, mse: 15262.746543, mean_q: 120.583409, mean_eps: 0.424783
 192088/300000: episode: 1652, duration: 2.054s, episode steps: 292, steps per second: 142, episode reward: 40.704, mean reward:  0.139 [-100.000, 15.735], mean action: 1.640 [0.000, 3.000],  loss: 30.684038, mse: 14717.210285, mean_q: 118.235809, mean_eps: 0.424175
 192182/300000: episode: 1653, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -26.375, mean reward: -0.281 [-100.000, 11.307], mean action: 1.691 [0.000, 3.000],  loss: 27.438175, mse: 14129.391944, mean_q: 116.077195, mean_eps: 0.423596
 192553/300000: episode: 1654, duration: 2.797s, episode steps: 371, steps per second: 133, episode reward: -263.036, mean reward: -0.709 [-100.000, 14.888], mean action: 1.806 [0.000, 3.000],  loss: 26.280543, mse: 13666.504559, mean_q: 113.816628, mean_eps: 0.422899
 192990/300000: episode: 1655, duration: 3.214s, episode steps: 437, steps per second: 136, episode reward: -255.627, mean reward: -0.585 [-100.000, 13.005], mean action: 1.842 [0.000, 3.000],  loss: 27.852388, mse: 12818.349911, mean_q: 111.096766, mean_eps: 0.421687
 193990/300000: episode: 1656, duration: 7.489s, episode steps: 1000, steps per second: 134, episode reward: 103.190, mean reward:  0.103 [-23.133, 23.631], mean action: 1.412 [0.000, 3.000],  loss: 26.796715, mse: 11713.718823, mean_q: 106.238255, mean_eps: 0.419532
 194216/300000: episode: 1657, duration: 1.598s, episode steps: 226, steps per second: 141, episode reward: -203.043, mean reward: -0.898 [-100.000, 10.615], mean action: 1.819 [0.000, 3.000],  loss: 25.424209, mse: 11477.081128, mean_q: 105.063236, mean_eps: 0.417693
 194320/300000: episode: 1658, duration: 0.720s, episode steps: 104, steps per second: 144, episode reward: 27.425, mean reward:  0.264 [-100.000, 14.399], mean action: 1.654 [0.000, 3.000],  loss: 24.190478, mse: 11306.834116, mean_q: 103.421428, mean_eps: 0.417197
 194416/300000: episode: 1659, duration: 0.694s, episode steps:  96, steps per second: 138, episode reward: -58.442, mean reward: -0.609 [-100.000, 10.702], mean action: 1.656 [0.000, 3.000],  loss: 30.156026, mse: 11395.600708, mean_q: 104.333965, mean_eps: 0.416897
 194507/300000: episode: 1660, duration: 0.644s, episode steps:  91, steps per second: 141, episode reward: 23.408, mean reward:  0.257 [-100.000, 28.236], mean action: 1.747 [0.000, 3.000],  loss: 18.547260, mse: 11417.592119, mean_q: 104.200883, mean_eps: 0.416617
 195507/300000: episode: 1661, duration: 8.302s, episode steps: 1000, steps per second: 120, episode reward: 78.963, mean reward:  0.079 [-25.230, 23.820], mean action: 1.739 [0.000, 3.000],  loss: 23.867223, mse: 11224.069740, mean_q: 103.229038, mean_eps: 0.414981
 196507/300000: episode: 1662, duration: 7.775s, episode steps: 1000, steps per second: 129, episode reward: 148.180, mean reward:  0.148 [-23.921, 22.738], mean action: 1.503 [0.000, 3.000],  loss: 23.463302, mse: 10977.547600, mean_q: 101.020646, mean_eps: 0.411981
 197507/300000: episode: 1663, duration: 7.671s, episode steps: 1000, steps per second: 130, episode reward: 127.664, mean reward:  0.128 [-20.735, 23.476], mean action: 1.113 [0.000, 3.000],  loss: 24.829242, mse: 10903.656375, mean_q: 101.096800, mean_eps: 0.408981
 197676/300000: episode: 1664, duration: 1.158s, episode steps: 169, steps per second: 146, episode reward: 14.214, mean reward:  0.084 [-100.000, 19.603], mean action: 1.651 [0.000, 3.000],  loss: 27.200646, mse: 10514.748940, mean_q: 99.511020, mean_eps: 0.407227
 197826/300000: episode: 1665, duration: 1.058s, episode steps: 150, steps per second: 142, episode reward: -184.766, mean reward: -1.232 [-100.000, 30.479], mean action: 1.773 [0.000, 3.000],  loss: 21.903352, mse: 10314.431868, mean_q: 98.298973, mean_eps: 0.406749
 197956/300000: episode: 1666, duration: 0.894s, episode steps: 130, steps per second: 145, episode reward: -64.065, mean reward: -0.493 [-100.000, 14.529], mean action: 1.508 [0.000, 3.000],  loss: 19.879298, mse: 10084.782974, mean_q: 97.017682, mean_eps: 0.406328
 198096/300000: episode: 1667, duration: 0.987s, episode steps: 140, steps per second: 142, episode reward: -1.223, mean reward: -0.009 [-100.000, 14.321], mean action: 1.543 [0.000, 3.000],  loss: 19.712314, mse: 9833.753903, mean_q: 95.576568, mean_eps: 0.405923
 198200/300000: episode: 1668, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -32.138, mean reward: -0.309 [-100.000, 11.212], mean action: 1.731 [0.000, 3.000],  loss: 18.841205, mse: 9824.248620, mean_q: 96.201953, mean_eps: 0.405557
 198358/300000: episode: 1669, duration: 1.227s, episode steps: 158, steps per second: 129, episode reward: -21.664, mean reward: -0.137 [-100.000, 22.601], mean action: 1.690 [0.000, 3.000],  loss: 19.148858, mse: 9706.336685, mean_q: 95.216093, mean_eps: 0.405165
 198808/300000: episode: 1670, duration: 3.426s, episode steps: 450, steps per second: 131, episode reward: -228.194, mean reward: -0.507 [-100.000, 20.372], mean action: 1.587 [0.000, 3.000],  loss: 21.281445, mse: 9633.338684, mean_q: 94.229942, mean_eps: 0.404253
 198897/300000: episode: 1671, duration: 0.689s, episode steps:  89, steps per second: 129, episode reward: -24.785, mean reward: -0.278 [-100.000, 27.758], mean action: 1.685 [0.000, 3.000],  loss: 18.348680, mse: 9472.468755, mean_q: 94.011036, mean_eps: 0.403444
 199226/300000: episode: 1672, duration: 2.423s, episode steps: 329, steps per second: 136, episode reward: -179.065, mean reward: -0.544 [-100.000, 18.023], mean action: 1.751 [0.000, 3.000],  loss: 21.740935, mse: 9383.594246, mean_q: 92.526612, mean_eps: 0.402817
 199352/300000: episode: 1673, duration: 0.877s, episode steps: 126, steps per second: 144, episode reward: -58.332, mean reward: -0.463 [-100.000, 14.394], mean action: 1.722 [0.000, 3.000],  loss: 17.151229, mse: 9776.838088, mean_q: 95.182629, mean_eps: 0.402134
 199466/300000: episode: 1674, duration: 0.818s, episode steps: 114, steps per second: 139, episode reward: 18.373, mean reward:  0.161 [-100.000, 10.946], mean action: 1.754 [0.000, 3.000],  loss: 22.145796, mse: 9458.895551, mean_q: 93.436353, mean_eps: 0.401775
 199573/300000: episode: 1675, duration: 0.737s, episode steps: 107, steps per second: 145, episode reward: 13.119, mean reward:  0.123 [-100.000, 13.504], mean action: 1.832 [0.000, 3.000],  loss: 24.821684, mse: 9425.144832, mean_q: 92.930461, mean_eps: 0.401443
 199657/300000: episode: 1676, duration: 0.574s, episode steps:  84, steps per second: 146, episode reward: -77.224, mean reward: -0.919 [-100.000, 11.083], mean action: 1.643 [0.000, 3.000],  loss: 26.201776, mse: 9808.205619, mean_q: 95.866800, mean_eps: 0.401157
 199757/300000: episode: 1677, duration: 0.749s, episode steps: 100, steps per second: 134, episode reward: -28.234, mean reward: -0.282 [-100.000, 16.322], mean action: 1.520 [0.000, 3.000],  loss: 21.939465, mse: 9671.475591, mean_q: 95.363256, mean_eps: 0.400880
 199875/300000: episode: 1678, duration: 0.901s, episode steps: 118, steps per second: 131, episode reward: -11.986, mean reward: -0.102 [-100.000, 20.617], mean action: 1.653 [0.000, 3.000],  loss: 22.479525, mse: 9708.145371, mean_q: 94.877292, mean_eps: 0.400554
 199986/300000: episode: 1679, duration: 0.810s, episode steps: 111, steps per second: 137, episode reward: -2.071, mean reward: -0.019 [-100.000,  9.374], mean action: 1.883 [0.000, 3.000],  loss: 19.653142, mse: 9698.449170, mean_q: 94.025137, mean_eps: 0.400210
 200123/300000: episode: 1680, duration: 1.053s, episode steps: 137, steps per second: 130, episode reward:  7.498, mean reward:  0.055 [-100.000, 11.982], mean action: 1.453 [0.000, 3.000],  loss: 17.751545, mse: 9758.031382, mean_q: 94.457248, mean_eps: 0.399838
 200208/300000: episode: 1681, duration: 0.612s, episode steps:  85, steps per second: 139, episode reward: -33.878, mean reward: -0.399 [-100.000,  9.867], mean action: 1.706 [0.000, 3.000],  loss: 19.613428, mse: 9365.641337, mean_q: 91.351004, mean_eps: 0.399505
 200338/300000: episode: 1682, duration: 0.922s, episode steps: 130, steps per second: 141, episode reward:  9.237, mean reward:  0.071 [-100.000, 18.021], mean action: 1.562 [0.000, 3.000],  loss: 20.603424, mse: 9486.230548, mean_q: 91.826301, mean_eps: 0.399182
 200481/300000: episode: 1683, duration: 0.996s, episode steps: 143, steps per second: 144, episode reward:  2.244, mean reward:  0.016 [-100.000, 13.226], mean action: 1.545 [0.000, 3.000],  loss: 21.606476, mse: 9870.381993, mean_q: 93.775952, mean_eps: 0.398773
 200577/300000: episode: 1684, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: 48.710, mean reward:  0.507 [-100.000, 21.399], mean action: 1.740 [0.000, 3.000],  loss: 22.537653, mse: 9551.833252, mean_q: 92.801547, mean_eps: 0.398415
 200677/300000: episode: 1685, duration: 0.703s, episode steps: 100, steps per second: 142, episode reward: 45.878, mean reward:  0.459 [-100.000, 18.817], mean action: 1.690 [0.000, 3.000],  loss: 16.979928, mse: 9301.567808, mean_q: 90.821415, mean_eps: 0.398121
 200780/300000: episode: 1686, duration: 0.708s, episode steps: 103, steps per second: 145, episode reward: -20.405, mean reward: -0.198 [-100.000, 10.301], mean action: 1.563 [0.000, 3.000],  loss: 20.629107, mse: 9530.986143, mean_q: 91.535051, mean_eps: 0.397816
 200889/300000: episode: 1687, duration: 0.754s, episode steps: 109, steps per second: 145, episode reward: 37.593, mean reward:  0.345 [-100.000, 12.348], mean action: 1.798 [0.000, 3.000],  loss: 16.350998, mse: 9389.367479, mean_q: 90.925261, mean_eps: 0.397498
 200959/300000: episode: 1688, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -57.514, mean reward: -0.822 [-100.000, 12.953], mean action: 1.271 [0.000, 3.000],  loss: 21.316699, mse: 9583.452999, mean_q: 92.099613, mean_eps: 0.397229
 201083/300000: episode: 1689, duration: 0.863s, episode steps: 124, steps per second: 144, episode reward: -0.960, mean reward: -0.008 [-100.000,  9.368], mean action: 1.355 [0.000, 3.000],  loss: 21.152108, mse: 9414.337780, mean_q: 90.904803, mean_eps: 0.396939
 201168/300000: episode: 1690, duration: 0.588s, episode steps:  85, steps per second: 145, episode reward: 23.750, mean reward:  0.279 [-100.000, 17.828], mean action: 1.600 [0.000, 3.000],  loss: 17.875302, mse: 9343.841303, mean_q: 90.378711, mean_eps: 0.396625
 201250/300000: episode: 1691, duration: 0.578s, episode steps:  82, steps per second: 142, episode reward: -7.771, mean reward: -0.095 [-100.000, 21.736], mean action: 1.902 [0.000, 3.000],  loss: 18.500585, mse: 9381.244307, mean_q: 90.654492, mean_eps: 0.396375
 201319/300000: episode: 1692, duration: 0.512s, episode steps:  69, steps per second: 135, episode reward: -38.143, mean reward: -0.553 [-100.000, 15.075], mean action: 1.652 [0.000, 3.000],  loss: 14.222960, mse: 9403.483391, mean_q: 90.623357, mean_eps: 0.396148
 201445/300000: episode: 1693, duration: 0.868s, episode steps: 126, steps per second: 145, episode reward:  3.885, mean reward:  0.031 [-100.000, 12.678], mean action: 1.444 [0.000, 3.000],  loss: 21.137080, mse: 9334.811415, mean_q: 90.065431, mean_eps: 0.395855
 201531/300000: episode: 1694, duration: 0.592s, episode steps:  86, steps per second: 145, episode reward:  8.324, mean reward:  0.097 [-100.000, 18.234], mean action: 1.500 [0.000, 3.000],  loss: 17.050661, mse: 9116.501595, mean_q: 87.046083, mean_eps: 0.395537
 201640/300000: episode: 1695, duration: 0.766s, episode steps: 109, steps per second: 142, episode reward: -8.189, mean reward: -0.075 [-100.000, 11.305], mean action: 1.853 [0.000, 3.000],  loss: 22.918969, mse: 9219.855482, mean_q: 88.897857, mean_eps: 0.395245
 201748/300000: episode: 1696, duration: 0.749s, episode steps: 108, steps per second: 144, episode reward: 12.834, mean reward:  0.119 [-100.000, 13.522], mean action: 1.685 [0.000, 3.000],  loss: 16.905355, mse: 9537.317518, mean_q: 90.853568, mean_eps: 0.394920
 201855/300000: episode: 1697, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward:  1.393, mean reward:  0.013 [-100.000, 11.888], mean action: 1.477 [0.000, 3.000],  loss: 18.817117, mse: 9074.183334, mean_q: 88.047220, mean_eps: 0.394597
 201921/300000: episode: 1698, duration: 0.491s, episode steps:  66, steps per second: 134, episode reward: -46.555, mean reward: -0.705 [-100.000, 16.699], mean action: 1.606 [0.000, 3.000],  loss: 12.513679, mse: 9357.593173, mean_q: 90.073765, mean_eps: 0.394338
 202039/300000: episode: 1699, duration: 0.811s, episode steps: 118, steps per second: 145, episode reward: 15.832, mean reward:  0.134 [-100.000, 10.029], mean action: 1.686 [0.000, 3.000],  loss: 22.332495, mse: 9046.950675, mean_q: 88.140385, mean_eps: 0.394062
 202192/300000: episode: 1700, duration: 1.075s, episode steps: 153, steps per second: 142, episode reward: -19.390, mean reward: -0.127 [-100.000, 30.017], mean action: 1.582 [0.000, 3.000],  loss: 17.520564, mse: 9126.880119, mean_q: 88.502709, mean_eps: 0.393655
 202309/300000: episode: 1701, duration: 0.816s, episode steps: 117, steps per second: 143, episode reward: -1.248, mean reward: -0.011 [-100.000, 53.571], mean action: 1.778 [0.000, 3.000],  loss: 15.186985, mse: 9381.640400, mean_q: 89.878147, mean_eps: 0.393250
 202537/300000: episode: 1702, duration: 1.623s, episode steps: 228, steps per second: 141, episode reward: -527.235, mean reward: -2.312 [-100.000,  3.776], mean action: 1.544 [0.000, 3.000],  loss: 21.944313, mse: 9410.523043, mean_q: 89.756642, mean_eps: 0.392732
 202622/300000: episode: 1703, duration: 0.591s, episode steps:  85, steps per second: 144, episode reward: -7.789, mean reward: -0.092 [-100.000, 17.385], mean action: 1.624 [0.000, 3.000],  loss: 18.011251, mse: 9692.477953, mean_q: 91.550514, mean_eps: 0.392263
 202709/300000: episode: 1704, duration: 0.618s, episode steps:  87, steps per second: 141, episode reward: 15.139, mean reward:  0.174 [-100.000, 13.738], mean action: 1.782 [0.000, 3.000],  loss: 15.636851, mse: 9527.708861, mean_q: 91.178190, mean_eps: 0.392005
 202825/300000: episode: 1705, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -14.108, mean reward: -0.122 [-100.000, 18.108], mean action: 1.819 [0.000, 3.000],  loss: 17.960593, mse: 9715.351870, mean_q: 91.293932, mean_eps: 0.391701
 202949/300000: episode: 1706, duration: 0.857s, episode steps: 124, steps per second: 145, episode reward: -22.153, mean reward: -0.179 [-100.000, 15.633], mean action: 1.895 [0.000, 3.000],  loss: 18.179023, mse: 9386.827424, mean_q: 89.802202, mean_eps: 0.391341
 203071/300000: episode: 1707, duration: 0.889s, episode steps: 122, steps per second: 137, episode reward:  2.366, mean reward:  0.019 [-100.000, 10.701], mean action: 1.492 [0.000, 3.000],  loss: 21.396700, mse: 9124.692415, mean_q: 86.945368, mean_eps: 0.390972
 203197/300000: episode: 1708, duration: 0.886s, episode steps: 126, steps per second: 142, episode reward: 46.849, mean reward:  0.372 [-100.000, 16.363], mean action: 1.627 [0.000, 3.000],  loss: 17.163516, mse: 8989.909315, mean_q: 87.357946, mean_eps: 0.390599
 203277/300000: episode: 1709, duration: 0.563s, episode steps:  80, steps per second: 142, episode reward: 10.981, mean reward:  0.137 [-100.000, 21.079], mean action: 1.800 [0.000, 3.000],  loss: 22.489582, mse: 9224.367786, mean_q: 88.792971, mean_eps: 0.390290
 203377/300000: episode: 1710, duration: 0.722s, episode steps: 100, steps per second: 138, episode reward:  4.227, mean reward:  0.042 [-100.000, 12.836], mean action: 1.810 [0.000, 3.000],  loss: 23.526465, mse: 9358.658828, mean_q: 90.833171, mean_eps: 0.390020
 203496/300000: episode: 1711, duration: 0.850s, episode steps: 119, steps per second: 140, episode reward: 24.493, mean reward:  0.206 [-100.000, 18.037], mean action: 1.555 [0.000, 3.000],  loss: 25.899225, mse: 9494.318507, mean_q: 91.081639, mean_eps: 0.389692
 203615/300000: episode: 1712, duration: 0.819s, episode steps: 119, steps per second: 145, episode reward:  3.294, mean reward:  0.028 [-100.000, 20.856], mean action: 1.857 [0.000, 3.000],  loss: 21.003491, mse: 9474.506442, mean_q: 90.631185, mean_eps: 0.389335
 203778/300000: episode: 1713, duration: 1.160s, episode steps: 163, steps per second: 141, episode reward: -73.163, mean reward: -0.449 [-100.000, 12.263], mean action: 1.712 [0.000, 3.000],  loss: 19.180376, mse: 9298.009718, mean_q: 89.450128, mean_eps: 0.388912
 203906/300000: episode: 1714, duration: 0.886s, episode steps: 128, steps per second: 145, episode reward: -44.965, mean reward: -0.351 [-100.000, 13.778], mean action: 1.875 [0.000, 3.000],  loss: 19.253878, mse: 9178.477394, mean_q: 89.342819, mean_eps: 0.388475
 204047/300000: episode: 1715, duration: 1.013s, episode steps: 141, steps per second: 139, episode reward:  0.781, mean reward:  0.006 [-100.000, 10.874], mean action: 1.723 [0.000, 3.000],  loss: 20.946588, mse: 9032.770515, mean_q: 87.742904, mean_eps: 0.388072
 204130/300000: episode: 1716, duration: 0.585s, episode steps:  83, steps per second: 142, episode reward: 30.273, mean reward:  0.365 [-100.000, 14.891], mean action: 1.771 [0.000, 3.000],  loss: 20.014030, mse: 8967.871253, mean_q: 87.360370, mean_eps: 0.387736
 204278/300000: episode: 1717, duration: 1.030s, episode steps: 148, steps per second: 144, episode reward: 53.837, mean reward:  0.364 [-100.000, 20.373], mean action: 1.750 [0.000, 3.000],  loss: 23.276029, mse: 8727.144393, mean_q: 86.246071, mean_eps: 0.387389
 204367/300000: episode: 1718, duration: 0.637s, episode steps:  89, steps per second: 140, episode reward: -39.410, mean reward: -0.443 [-100.000, 15.662], mean action: 1.730 [0.000, 3.000],  loss: 17.342729, mse: 9229.721383, mean_q: 88.996958, mean_eps: 0.387034
 204505/300000: episode: 1719, duration: 0.954s, episode steps: 138, steps per second: 145, episode reward: -4.738, mean reward: -0.034 [-100.000, 15.333], mean action: 1.667 [0.000, 3.000],  loss: 17.378579, mse: 8680.091655, mean_q: 85.567971, mean_eps: 0.386694
 204616/300000: episode: 1720, duration: 0.781s, episode steps: 111, steps per second: 142, episode reward: -32.504, mean reward: -0.293 [-100.000, 13.411], mean action: 1.874 [0.000, 3.000],  loss: 20.809874, mse: 8404.242847, mean_q: 84.160589, mean_eps: 0.386320
 204704/300000: episode: 1721, duration: 0.622s, episode steps:  88, steps per second: 141, episode reward: -61.522, mean reward: -0.699 [-100.000, 12.441], mean action: 1.784 [0.000, 3.000],  loss: 23.031277, mse: 8067.931380, mean_q: 81.816837, mean_eps: 0.386022
 204800/300000: episode: 1722, duration: 0.664s, episode steps:  96, steps per second: 144, episode reward: 20.355, mean reward:  0.212 [-100.000, 20.847], mean action: 1.927 [0.000, 3.000],  loss: 20.282224, mse: 7859.123199, mean_q: 80.595610, mean_eps: 0.385746
 204937/300000: episode: 1723, duration: 0.957s, episode steps: 137, steps per second: 143, episode reward: -36.860, mean reward: -0.269 [-100.000, 10.750], mean action: 1.650 [0.000, 3.000],  loss: 24.331845, mse: 7812.912141, mean_q: 80.736625, mean_eps: 0.385396
 205118/300000: episode: 1724, duration: 1.344s, episode steps: 181, steps per second: 135, episode reward: -7.295, mean reward: -0.040 [-100.000, 12.657], mean action: 1.840 [0.000, 3.000],  loss: 15.964729, mse: 7711.519456, mean_q: 79.989565, mean_eps: 0.384919
 206118/300000: episode: 1725, duration: 7.727s, episode steps: 1000, steps per second: 129, episode reward: 38.893, mean reward:  0.039 [-22.189, 16.385], mean action: 1.612 [0.000, 3.000],  loss: 16.810849, mse: 7621.688679, mean_q: 79.523707, mean_eps: 0.383147
 206242/300000: episode: 1726, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: 21.185, mean reward:  0.171 [-100.000, 18.509], mean action: 1.847 [0.000, 3.000],  loss: 18.633820, mse: 7258.102070, mean_q: 77.431263, mean_eps: 0.381462
 206349/300000: episode: 1727, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: 10.422, mean reward:  0.097 [-100.000, 18.216], mean action: 1.860 [0.000, 3.000],  loss: 12.788080, mse: 7461.903316, mean_q: 79.375106, mean_eps: 0.381115
 206478/300000: episode: 1728, duration: 0.925s, episode steps: 129, steps per second: 139, episode reward: -20.959, mean reward: -0.162 [-100.000, 21.314], mean action: 1.752 [0.000, 3.000],  loss: 13.863778, mse: 7450.359920, mean_q: 79.319105, mean_eps: 0.380761
 206617/300000: episode: 1729, duration: 0.958s, episode steps: 139, steps per second: 145, episode reward: -0.256, mean reward: -0.002 [-100.000, 20.361], mean action: 1.626 [0.000, 3.000],  loss: 20.321069, mse: 7396.479886, mean_q: 78.491495, mean_eps: 0.380359
 206769/300000: episode: 1730, duration: 1.061s, episode steps: 152, steps per second: 143, episode reward: -117.797, mean reward: -0.775 [-100.000, 11.197], mean action: 1.730 [0.000, 3.000],  loss: 16.451011, mse: 7315.539011, mean_q: 77.056202, mean_eps: 0.379922
 207769/300000: episode: 1731, duration: 7.719s, episode steps: 1000, steps per second: 130, episode reward: 85.842, mean reward:  0.086 [-21.868, 22.431], mean action: 1.338 [0.000, 3.000],  loss: 16.622148, mse: 6869.186696, mean_q: 74.431299, mean_eps: 0.378194
 207897/300000: episode: 1732, duration: 1.076s, episode steps: 128, steps per second: 119, episode reward: -75.086, mean reward: -0.587 [-100.000,  8.088], mean action: 1.789 [0.000, 3.000],  loss: 17.177566, mse: 6464.067860, mean_q: 71.340463, mean_eps: 0.376502
 208061/300000: episode: 1733, duration: 1.165s, episode steps: 164, steps per second: 141, episode reward: -50.126, mean reward: -0.306 [-100.000, 15.753], mean action: 1.762 [0.000, 3.000],  loss: 16.427608, mse: 6323.505757, mean_q: 71.880407, mean_eps: 0.376064
 208174/300000: episode: 1734, duration: 0.797s, episode steps: 113, steps per second: 142, episode reward: -21.322, mean reward: -0.189 [-100.000, 11.811], mean action: 1.566 [0.000, 3.000],  loss: 16.017272, mse: 6048.997351, mean_q: 69.299501, mean_eps: 0.375649
 208310/300000: episode: 1735, duration: 0.933s, episode steps: 136, steps per second: 146, episode reward:  7.337, mean reward:  0.054 [-100.000, 16.916], mean action: 1.647 [0.000, 3.000],  loss: 13.800747, mse: 6198.810102, mean_q: 71.327054, mean_eps: 0.375276
 208397/300000: episode: 1736, duration: 0.607s, episode steps:  87, steps per second: 143, episode reward: -309.752, mean reward: -3.560 [-100.000,  0.595], mean action: 1.736 [0.000, 3.000],  loss: 17.043141, mse: 6039.895665, mean_q: 69.145073, mean_eps: 0.374941
 208606/300000: episode: 1737, duration: 1.493s, episode steps: 209, steps per second: 140, episode reward: -262.664, mean reward: -1.257 [-100.000,  3.176], mean action: 1.775 [0.000, 3.000],  loss: 18.389868, mse: 5967.068985, mean_q: 69.388570, mean_eps: 0.374497
 208972/300000: episode: 1738, duration: 2.732s, episode steps: 366, steps per second: 134, episode reward: -105.836, mean reward: -0.289 [-100.000, 22.181], mean action: 1.719 [0.000, 3.000],  loss: 15.086806, mse: 6071.204334, mean_q: 69.852779, mean_eps: 0.373634
 209742/300000: episode: 1739, duration: 6.237s, episode steps: 770, steps per second: 123, episode reward: -88.004, mean reward: -0.114 [-100.000, 24.404], mean action: 1.623 [0.000, 3.000],  loss: 17.883877, mse: 6026.185111, mean_q: 69.860907, mean_eps: 0.371930
 209866/300000: episode: 1740, duration: 0.877s, episode steps: 124, steps per second: 141, episode reward: 14.344, mean reward:  0.116 [-100.000, 15.738], mean action: 1.452 [0.000, 3.000],  loss: 16.211092, mse: 5994.170835, mean_q: 70.628635, mean_eps: 0.370589
 209973/300000: episode: 1741, duration: 0.742s, episode steps: 107, steps per second: 144, episode reward: 25.606, mean reward:  0.239 [-100.000,  9.620], mean action: 1.738 [0.000, 3.000],  loss: 14.383706, mse: 5816.787365, mean_q: 69.515786, mean_eps: 0.370243
 210053/300000: episode: 1742, duration: 0.566s, episode steps:  80, steps per second: 141, episode reward: -1.179, mean reward: -0.015 [-100.000, 18.343], mean action: 1.825 [0.000, 3.000],  loss: 16.206524, mse: 5723.082626, mean_q: 67.916703, mean_eps: 0.369962
 210199/300000: episode: 1743, duration: 1.032s, episode steps: 146, steps per second: 141, episode reward: -43.562, mean reward: -0.298 [-100.000, 11.141], mean action: 1.774 [0.000, 3.000],  loss: 17.201409, mse: 5710.421510, mean_q: 69.469362, mean_eps: 0.369624
 211199/300000: episode: 1744, duration: 7.813s, episode steps: 1000, steps per second: 128, episode reward: 18.329, mean reward:  0.018 [-22.183, 16.661], mean action: 1.406 [0.000, 3.000],  loss: 18.547349, mse: 5401.490347, mean_q: 67.184546, mean_eps: 0.367904
 211303/300000: episode: 1745, duration: 0.751s, episode steps: 104, steps per second: 138, episode reward:  4.995, mean reward:  0.048 [-100.000, 15.054], mean action: 1.827 [0.000, 3.000],  loss: 12.097438, mse: 5388.285877, mean_q: 67.211900, mean_eps: 0.366248
 211474/300000: episode: 1746, duration: 1.214s, episode steps: 171, steps per second: 141, episode reward: -3.960, mean reward: -0.023 [-100.000, 16.766], mean action: 1.737 [0.000, 3.000],  loss: 17.006406, mse: 5186.319626, mean_q: 65.645933, mean_eps: 0.365836
 211592/300000: episode: 1747, duration: 0.847s, episode steps: 118, steps per second: 139, episode reward: -46.104, mean reward: -0.391 [-100.000, 19.707], mean action: 1.695 [0.000, 3.000],  loss: 15.793235, mse: 4952.310919, mean_q: 63.797711, mean_eps: 0.365403
 211730/300000: episode: 1748, duration: 0.960s, episode steps: 138, steps per second: 144, episode reward: -276.685, mean reward: -2.005 [-100.000, 32.428], mean action: 1.696 [0.000, 3.000],  loss: 15.000983, mse: 4986.592000, mean_q: 64.047298, mean_eps: 0.365018
 212003/300000: episode: 1749, duration: 1.934s, episode steps: 273, steps per second: 141, episode reward: -86.739, mean reward: -0.318 [-100.000, 10.937], mean action: 1.828 [0.000, 3.000],  loss: 15.638120, mse: 4986.048304, mean_q: 63.081642, mean_eps: 0.364402
 212296/300000: episode: 1750, duration: 2.059s, episode steps: 293, steps per second: 142, episode reward: -113.005, mean reward: -0.386 [-100.000, 18.504], mean action: 1.519 [0.000, 3.000],  loss: 17.425405, mse: 4971.333077, mean_q: 63.207580, mean_eps: 0.363553
 212411/300000: episode: 1751, duration: 0.801s, episode steps: 115, steps per second: 144, episode reward: -6.803, mean reward: -0.059 [-100.000, 37.385], mean action: 1.722 [0.000, 3.000],  loss: 15.451330, mse: 4910.540897, mean_q: 63.594251, mean_eps: 0.362941
 212918/300000: episode: 1752, duration: 3.758s, episode steps: 507, steps per second: 135, episode reward: -154.494, mean reward: -0.305 [-100.000, 20.531], mean action: 1.469 [0.000, 3.000],  loss: 14.407891, mse: 4840.176147, mean_q: 63.200071, mean_eps: 0.362008
 213131/300000: episode: 1753, duration: 1.509s, episode steps: 213, steps per second: 141, episode reward: -104.210, mean reward: -0.489 [-100.000, 11.858], mean action: 1.685 [0.000, 3.000],  loss: 16.426850, mse: 4861.278738, mean_q: 62.974814, mean_eps: 0.360928
 213269/300000: episode: 1754, duration: 0.970s, episode steps: 138, steps per second: 142, episode reward: -228.716, mean reward: -1.657 [-100.000,  1.856], mean action: 1.746 [0.000, 3.000],  loss: 14.136310, mse: 5005.758773, mean_q: 63.696904, mean_eps: 0.360401
 213392/300000: episode: 1755, duration: 0.872s, episode steps: 123, steps per second: 141, episode reward: -172.142, mean reward: -1.400 [-100.000, 33.706], mean action: 1.634 [0.000, 3.000],  loss: 21.274667, mse: 5009.236922, mean_q: 63.148868, mean_eps: 0.360010
 213470/300000: episode: 1756, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -6.877, mean reward: -0.088 [-100.000, 74.265], mean action: 1.936 [0.000, 3.000],  loss: 14.163735, mse: 4941.025187, mean_q: 63.001773, mean_eps: 0.359708
 213556/300000: episode: 1757, duration: 0.614s, episode steps:  86, steps per second: 140, episode reward: -54.767, mean reward: -0.637 [-100.000, 16.711], mean action: 1.640 [0.000, 3.000],  loss: 16.349200, mse: 4961.181916, mean_q: 63.620191, mean_eps: 0.359463
 213724/300000: episode: 1758, duration: 1.174s, episode steps: 168, steps per second: 143, episode reward: -48.006, mean reward: -0.286 [-100.000, 10.960], mean action: 1.643 [0.000, 3.000],  loss: 21.926219, mse: 4967.772929, mean_q: 63.490960, mean_eps: 0.359081
 213843/300000: episode: 1759, duration: 0.823s, episode steps: 119, steps per second: 145, episode reward: -45.248, mean reward: -0.380 [-100.000, 10.220], mean action: 1.714 [0.000, 3.000],  loss: 25.991297, mse: 4815.827011, mean_q: 63.075175, mean_eps: 0.358651
 214843/300000: episode: 1760, duration: 7.654s, episode steps: 1000, steps per second: 131, episode reward: 145.534, mean reward:  0.146 [-22.308, 23.066], mean action: 0.975 [0.000, 3.000],  loss: 19.043128, mse: 4675.698350, mean_q: 61.810095, mean_eps: 0.356972
 215066/300000: episode: 1761, duration: 1.566s, episode steps: 223, steps per second: 142, episode reward: -261.233, mean reward: -1.171 [-100.000,  2.963], mean action: 1.776 [0.000, 3.000],  loss: 15.707876, mse: 4771.162175, mean_q: 62.085898, mean_eps: 0.355138
 215189/300000: episode: 1762, duration: 0.876s, episode steps: 123, steps per second: 140, episode reward: 28.028, mean reward:  0.228 [-100.000, 12.788], mean action: 1.683 [0.000, 3.000],  loss: 25.985041, mse: 4708.639464, mean_q: 62.292013, mean_eps: 0.354619
 215345/300000: episode: 1763, duration: 1.076s, episode steps: 156, steps per second: 145, episode reward: -30.144, mean reward: -0.193 [-100.000,  8.287], mean action: 1.763 [0.000, 3.000],  loss: 15.733164, mse: 4816.414674, mean_q: 62.043023, mean_eps: 0.354200
 215441/300000: episode: 1764, duration: 0.689s, episode steps:  96, steps per second: 139, episode reward:  7.469, mean reward:  0.078 [-100.000, 13.101], mean action: 1.792 [0.000, 3.000],  loss: 17.448119, mse: 4882.295456, mean_q: 62.769002, mean_eps: 0.353822
 216441/300000: episode: 1765, duration: 7.807s, episode steps: 1000, steps per second: 128, episode reward: 147.660, mean reward:  0.148 [-22.595, 27.400], mean action: 0.960 [0.000, 3.000],  loss: 16.302734, mse: 4700.161775, mean_q: 61.179743, mean_eps: 0.352179
 217441/300000: episode: 1766, duration: 7.899s, episode steps: 1000, steps per second: 127, episode reward: 119.130, mean reward:  0.119 [-22.604, 23.395], mean action: 0.984 [0.000, 3.000],  loss: 15.545065, mse: 4458.887541, mean_q: 59.418823, mean_eps: 0.349179
 217516/300000: episode: 1767, duration: 0.521s, episode steps:  75, steps per second: 144, episode reward: -38.972, mean reward: -0.520 [-100.000, 10.777], mean action: 1.493 [0.000, 3.000],  loss: 17.503121, mse: 4329.881771, mean_q: 58.287083, mean_eps: 0.347566
 217604/300000: episode: 1768, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: -26.889, mean reward: -0.306 [-100.000, 15.475], mean action: 1.818 [0.000, 3.000],  loss: 10.736314, mse: 4384.184853, mean_q: 57.983886, mean_eps: 0.347322
 217726/300000: episode: 1769, duration: 0.930s, episode steps: 122, steps per second: 131, episode reward: 31.035, mean reward:  0.254 [-100.000, 15.538], mean action: 1.574 [0.000, 3.000],  loss: 14.726611, mse: 4367.214718, mean_q: 57.940923, mean_eps: 0.347007
 217883/300000: episode: 1770, duration: 1.184s, episode steps: 157, steps per second: 133, episode reward: -236.859, mean reward: -1.509 [-100.000,  6.302], mean action: 1.503 [0.000, 3.000],  loss: 12.536412, mse: 4350.014935, mean_q: 57.575260, mean_eps: 0.346588
 218060/300000: episode: 1771, duration: 1.342s, episode steps: 177, steps per second: 132, episode reward: 29.256, mean reward:  0.165 [-100.000, 15.233], mean action: 1.706 [0.000, 3.000],  loss: 12.044578, mse: 4327.034315, mean_q: 57.546224, mean_eps: 0.346087
 218159/300000: episode: 1772, duration: 0.712s, episode steps:  99, steps per second: 139, episode reward: 49.757, mean reward:  0.503 [-100.000, 19.745], mean action: 1.828 [0.000, 3.000],  loss: 20.621631, mse: 4133.979184, mean_q: 57.039792, mean_eps: 0.345673
 218268/300000: episode: 1773, duration: 0.776s, episode steps: 109, steps per second: 140, episode reward: 28.641, mean reward:  0.263 [-100.000, 20.792], mean action: 1.615 [0.000, 3.000],  loss: 14.237292, mse: 4249.803169, mean_q: 56.963278, mean_eps: 0.345361
 218349/300000: episode: 1774, duration: 0.576s, episode steps:  81, steps per second: 141, episode reward: 43.481, mean reward:  0.537 [-100.000, 18.910], mean action: 1.654 [0.000, 3.000],  loss: 14.098219, mse: 4137.548147, mean_q: 55.408933, mean_eps: 0.345076
 218464/300000: episode: 1775, duration: 0.784s, episode steps: 115, steps per second: 147, episode reward: -4.962, mean reward: -0.043 [-100.000, 10.754], mean action: 1.435 [0.000, 3.000],  loss: 12.113423, mse: 4070.452819, mean_q: 56.044665, mean_eps: 0.344782
 218568/300000: episode: 1776, duration: 0.737s, episode steps: 104, steps per second: 141, episode reward: 35.683, mean reward:  0.343 [-100.000, 53.674], mean action: 1.510 [0.000, 3.000],  loss: 13.749201, mse: 4147.510287, mean_q: 56.088993, mean_eps: 0.344453
 218663/300000: episode: 1777, duration: 0.673s, episode steps:  95, steps per second: 141, episode reward: 30.967, mean reward:  0.326 [-100.000, 15.530], mean action: 1.853 [0.000, 3.000],  loss: 12.026823, mse: 4157.986369, mean_q: 56.596175, mean_eps: 0.344155
 218757/300000: episode: 1778, duration: 0.653s, episode steps:  94, steps per second: 144, episode reward: -60.024, mean reward: -0.639 [-100.000,  7.957], mean action: 1.606 [0.000, 3.000],  loss: 12.308682, mse: 4113.895744, mean_q: 55.464599, mean_eps: 0.343872
 218903/300000: episode: 1779, duration: 1.046s, episode steps: 146, steps per second: 140, episode reward: -55.374, mean reward: -0.379 [-100.000,  7.943], mean action: 1.685 [0.000, 3.000],  loss: 14.777189, mse: 3958.425019, mean_q: 54.661072, mean_eps: 0.343511
 218979/300000: episode: 1780, duration: 0.521s, episode steps:  76, steps per second: 146, episode reward: -15.668, mean reward: -0.206 [-100.000, 22.394], mean action: 1.553 [0.000, 3.000],  loss: 13.596885, mse: 4034.621213, mean_q: 54.556859, mean_eps: 0.343178
 219068/300000: episode: 1781, duration: 0.612s, episode steps:  89, steps per second: 145, episode reward: -70.760, mean reward: -0.795 [-100.000,  9.277], mean action: 1.764 [0.000, 3.000],  loss: 15.349218, mse: 4089.518593, mean_q: 55.547437, mean_eps: 0.342931
 220068/300000: episode: 1782, duration: 7.653s, episode steps: 1000, steps per second: 131, episode reward: 104.593, mean reward:  0.105 [-21.180, 23.190], mean action: 0.845 [0.000, 3.000],  loss: 14.657793, mse: 3862.062903, mean_q: 53.819649, mean_eps: 0.341298
 220188/300000: episode: 1783, duration: 0.842s, episode steps: 120, steps per second: 143, episode reward: -27.148, mean reward: -0.226 [-100.000,  9.125], mean action: 1.667 [0.000, 3.000],  loss: 13.232875, mse: 3818.496480, mean_q: 53.157530, mean_eps: 0.339617
 221188/300000: episode: 1784, duration: 7.334s, episode steps: 1000, steps per second: 136, episode reward: 164.927, mean reward:  0.165 [-24.228, 22.384], mean action: 0.855 [0.000, 3.000],  loss: 14.015957, mse: 3719.571661, mean_q: 51.361045, mean_eps: 0.337938
 221308/300000: episode: 1785, duration: 0.823s, episode steps: 120, steps per second: 146, episode reward: -31.091, mean reward: -0.259 [-100.000,  8.302], mean action: 1.433 [0.000, 3.000],  loss: 11.038948, mse: 3797.437329, mean_q: 52.684718, mean_eps: 0.336257
 221409/300000: episode: 1786, duration: 0.706s, episode steps: 101, steps per second: 143, episode reward: 25.275, mean reward:  0.250 [-100.000, 11.733], mean action: 1.792 [0.000, 3.000],  loss: 13.376612, mse: 3786.961348, mean_q: 51.803367, mean_eps: 0.335926
 221519/300000: episode: 1787, duration: 0.787s, episode steps: 110, steps per second: 140, episode reward: -2.190, mean reward: -0.020 [-100.000, 30.503], mean action: 1.582 [0.000, 3.000],  loss: 14.548273, mse: 3938.418259, mean_q: 54.095612, mean_eps: 0.335610
 222519/300000: episode: 1788, duration: 7.477s, episode steps: 1000, steps per second: 134, episode reward: 145.512, mean reward:  0.146 [-24.273, 23.855], mean action: 1.645 [0.000, 3.000],  loss: 13.297824, mse: 4025.746472, mean_q: 53.787545, mean_eps: 0.333944
 222614/300000: episode: 1789, duration: 0.690s, episode steps:  95, steps per second: 138, episode reward: -52.209, mean reward: -0.550 [-100.000, 13.213], mean action: 1.737 [0.000, 3.000],  loss: 15.073894, mse: 4039.385835, mean_q: 54.467511, mean_eps: 0.332302
 223614/300000: episode: 1790, duration: 7.736s, episode steps: 1000, steps per second: 129, episode reward: 141.243, mean reward:  0.141 [-23.896, 23.908], mean action: 0.775 [0.000, 3.000],  loss: 14.678739, mse: 4108.711759, mean_q: 54.562582, mean_eps: 0.330659
 223728/300000: episode: 1791, duration: 0.805s, episode steps: 114, steps per second: 142, episode reward: -30.931, mean reward: -0.271 [-100.000, 10.233], mean action: 1.667 [0.000, 3.000],  loss: 9.077173, mse: 4114.072852, mean_q: 54.798176, mean_eps: 0.328989
 223850/300000: episode: 1792, duration: 0.892s, episode steps: 122, steps per second: 137, episode reward: 23.676, mean reward:  0.194 [-100.000, 14.215], mean action: 1.598 [0.000, 3.000],  loss: 12.637473, mse: 4230.454510, mean_q: 56.186134, mean_eps: 0.328634
 223985/300000: episode: 1793, duration: 0.948s, episode steps: 135, steps per second: 142, episode reward: 32.620, mean reward:  0.242 [-100.000, 17.818], mean action: 1.770 [0.000, 3.000],  loss: 11.418038, mse: 4228.923928, mean_q: 56.145674, mean_eps: 0.328249
 224125/300000: episode: 1794, duration: 1.009s, episode steps: 140, steps per second: 139, episode reward: 37.467, mean reward:  0.268 [-100.000, 13.660], mean action: 1.843 [0.000, 3.000],  loss: 12.627754, mse: 4198.891248, mean_q: 55.302895, mean_eps: 0.327836
 224236/300000: episode: 1795, duration: 0.779s, episode steps: 111, steps per second: 143, episode reward: -29.172, mean reward: -0.263 [-100.000, 13.520], mean action: 1.658 [0.000, 3.000],  loss: 14.070145, mse: 4224.362063, mean_q: 55.385864, mean_eps: 0.327460
 224343/300000: episode: 1796, duration: 0.748s, episode steps: 107, steps per second: 143, episode reward: -14.887, mean reward: -0.139 [-100.000, 23.019], mean action: 1.748 [0.000, 3.000],  loss: 17.224231, mse: 4324.475041, mean_q: 56.463677, mean_eps: 0.327133
 225343/300000: episode: 1797, duration: 7.752s, episode steps: 1000, steps per second: 129, episode reward: 127.698, mean reward:  0.128 [-24.087, 23.514], mean action: 0.804 [0.000, 3.000],  loss: 13.086734, mse: 4330.948919, mean_q: 56.979334, mean_eps: 0.325472
 225468/300000: episode: 1798, duration: 0.932s, episode steps: 125, steps per second: 134, episode reward:  8.753, mean reward:  0.070 [-100.000, 18.652], mean action: 1.712 [0.000, 3.000],  loss: 12.705786, mse: 4480.761318, mean_q: 58.562297, mean_eps: 0.323785
 225571/300000: episode: 1799, duration: 0.807s, episode steps: 103, steps per second: 128, episode reward: -13.730, mean reward: -0.133 [-100.000, 10.893], mean action: 1.786 [0.000, 3.000],  loss: 13.537871, mse: 4568.443058, mean_q: 58.595205, mean_eps: 0.323443
 225702/300000: episode: 1800, duration: 0.983s, episode steps: 131, steps per second: 133, episode reward: -10.714, mean reward: -0.082 [-100.000, 10.189], mean action: 1.893 [0.000, 3.000],  loss: 12.381285, mse: 4491.309602, mean_q: 58.970492, mean_eps: 0.323092
 225841/300000: episode: 1801, duration: 0.998s, episode steps: 139, steps per second: 139, episode reward: -193.612, mean reward: -1.393 [-100.000, 65.684], mean action: 1.518 [0.000, 3.000],  loss: 12.681878, mse: 4588.772313, mean_q: 58.851675, mean_eps: 0.322687
 226678/300000: episode: 1802, duration: 6.310s, episode steps: 837, steps per second: 133, episode reward: 225.448, mean reward:  0.269 [-20.893, 100.000], mean action: 2.106 [0.000, 3.000],  loss: 13.610145, mse: 4744.664904, mean_q: 61.002631, mean_eps: 0.321223
 226875/300000: episode: 1803, duration: 1.472s, episode steps: 197, steps per second: 134, episode reward: -47.026, mean reward: -0.239 [-100.000, 10.532], mean action: 1.807 [0.000, 3.000],  loss: 15.087430, mse: 4832.943975, mean_q: 63.123812, mean_eps: 0.319672
 227299/300000: episode: 1804, duration: 3.162s, episode steps: 424, steps per second: 134, episode reward: -75.801, mean reward: -0.179 [-100.000, 18.043], mean action: 1.545 [0.000, 3.000],  loss: 14.385812, mse: 4792.713997, mean_q: 62.733555, mean_eps: 0.318740
 227403/300000: episode: 1805, duration: 0.719s, episode steps: 104, steps per second: 145, episode reward:  2.264, mean reward:  0.022 [-100.000, 17.938], mean action: 1.500 [0.000, 3.000],  loss: 12.768715, mse: 4752.409281, mean_q: 62.839813, mean_eps: 0.317948
 227851/300000: episode: 1806, duration: 3.226s, episode steps: 448, steps per second: 139, episode reward: -104.592, mean reward: -0.233 [-100.000, 21.136], mean action: 1.408 [0.000, 3.000],  loss: 14.062874, mse: 4912.371807, mean_q: 64.148103, mean_eps: 0.317120
 227947/300000: episode: 1807, duration: 0.685s, episode steps:  96, steps per second: 140, episode reward: -21.961, mean reward: -0.229 [-100.000, 13.444], mean action: 1.677 [0.000, 3.000],  loss: 14.373211, mse: 5122.572634, mean_q: 65.438509, mean_eps: 0.316304
 228183/300000: episode: 1808, duration: 1.679s, episode steps: 236, steps per second: 141, episode reward: -61.086, mean reward: -0.259 [-100.000, 11.077], mean action: 1.631 [0.000, 3.000],  loss: 14.586289, mse: 5078.920896, mean_q: 64.448064, mean_eps: 0.315807
 228288/300000: episode: 1809, duration: 0.730s, episode steps: 105, steps per second: 144, episode reward:  1.296, mean reward:  0.012 [-100.000, 28.777], mean action: 1.543 [0.000, 3.000],  loss: 13.781044, mse: 5018.331985, mean_q: 64.015099, mean_eps: 0.315295
 228371/300000: episode: 1810, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward:  2.516, mean reward:  0.030 [-100.000, 14.646], mean action: 1.831 [0.000, 3.000],  loss: 16.945767, mse: 4927.617685, mean_q: 63.133873, mean_eps: 0.315013
 228475/300000: episode: 1811, duration: 0.758s, episode steps: 104, steps per second: 137, episode reward: -31.052, mean reward: -0.299 [-100.000, 12.618], mean action: 1.548 [0.000, 3.000],  loss: 17.468535, mse: 4894.058181, mean_q: 62.662095, mean_eps: 0.314732
 228635/300000: episode: 1812, duration: 1.104s, episode steps: 160, steps per second: 145, episode reward: 23.147, mean reward:  0.145 [-100.000, 15.612], mean action: 1.300 [0.000, 3.000],  loss: 15.514103, mse: 4867.324243, mean_q: 63.408720, mean_eps: 0.314337
 228739/300000: episode: 1813, duration: 0.742s, episode steps: 104, steps per second: 140, episode reward: -15.226, mean reward: -0.146 [-100.000, 16.936], mean action: 1.683 [0.000, 3.000],  loss: 12.872330, mse: 4885.388343, mean_q: 63.002939, mean_eps: 0.313940
 229343/300000: episode: 1814, duration: 4.575s, episode steps: 604, steps per second: 132, episode reward: 182.676, mean reward:  0.302 [-23.291, 100.000], mean action: 1.379 [0.000, 3.000],  loss: 14.071808, mse: 4875.983116, mean_q: 62.904574, mean_eps: 0.312879
 229461/300000: episode: 1815, duration: 0.816s, episode steps: 118, steps per second: 145, episode reward: -10.985, mean reward: -0.093 [-100.000, 17.672], mean action: 1.805 [0.000, 3.000],  loss: 10.482309, mse: 4891.234077, mean_q: 63.209909, mean_eps: 0.311796
 230461/300000: episode: 1816, duration: 7.705s, episode steps: 1000, steps per second: 130, episode reward: 113.559, mean reward:  0.114 [-23.252, 24.802], mean action: 1.167 [0.000, 3.000],  loss: 14.846160, mse: 4772.942707, mean_q: 63.044174, mean_eps: 0.310118
 230551/300000: episode: 1817, duration: 0.618s, episode steps:  90, steps per second: 146, episode reward: -49.481, mean reward: -0.550 [-100.000,  9.666], mean action: 1.456 [0.000, 3.000],  loss: 9.898882, mse: 4579.090820, mean_q: 62.198232, mean_eps: 0.308484
 231551/300000: episode: 1818, duration: 7.846s, episode steps: 1000, steps per second: 127, episode reward: 137.463, mean reward:  0.137 [-21.207, 23.383], mean action: 1.928 [0.000, 3.000],  loss: 15.302498, mse: 4614.365922, mean_q: 62.989137, mean_eps: 0.306848
 232551/300000: episode: 1819, duration: 7.903s, episode steps: 1000, steps per second: 127, episode reward: 72.657, mean reward:  0.073 [-24.429, 24.637], mean action: 1.776 [0.000, 3.000],  loss: 16.162201, mse: 4615.977087, mean_q: 64.061014, mean_eps: 0.303848
 232657/300000: episode: 1820, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -32.351, mean reward: -0.305 [-100.000,  8.180], mean action: 1.651 [0.000, 3.000],  loss: 15.874002, mse: 4642.940418, mean_q: 64.522020, mean_eps: 0.302189
 232781/300000: episode: 1821, duration: 0.882s, episode steps: 124, steps per second: 141, episode reward: 16.870, mean reward:  0.136 [-100.000, 18.469], mean action: 1.887 [0.000, 3.000],  loss: 15.349517, mse: 4736.109300, mean_q: 65.290957, mean_eps: 0.301844
 233781/300000: episode: 1822, duration: 8.597s, episode steps: 1000, steps per second: 116, episode reward: 69.931, mean reward:  0.070 [-20.216, 22.799], mean action: 1.423 [0.000, 3.000],  loss: 14.642490, mse: 4587.082176, mean_q: 64.037324, mean_eps: 0.300158
 233907/300000: episode: 1823, duration: 0.932s, episode steps: 126, steps per second: 135, episode reward: -15.587, mean reward: -0.124 [-100.000, 45.611], mean action: 1.857 [0.000, 3.000],  loss: 13.052309, mse: 4472.374674, mean_q: 63.530738, mean_eps: 0.298469
 234039/300000: episode: 1824, duration: 1.020s, episode steps: 132, steps per second: 129, episode reward: -28.980, mean reward: -0.220 [-100.000,  9.108], mean action: 1.682 [0.000, 3.000],  loss: 15.069772, mse: 4428.157336, mean_q: 63.431793, mean_eps: 0.298083
 234205/300000: episode: 1825, duration: 1.200s, episode steps: 166, steps per second: 138, episode reward: -51.772, mean reward: -0.312 [-100.000, 18.101], mean action: 1.747 [0.000, 3.000],  loss: 15.652486, mse: 4335.425177, mean_q: 62.761438, mean_eps: 0.297635
 234294/300000: episode: 1826, duration: 0.660s, episode steps:  89, steps per second: 135, episode reward: -246.038, mean reward: -2.764 [-100.000,  4.239], mean action: 1.348 [0.000, 3.000],  loss: 13.969891, mse: 4374.375200, mean_q: 62.153714, mean_eps: 0.297253
 235294/300000: episode: 1827, duration: 7.741s, episode steps: 1000, steps per second: 129, episode reward: 118.890, mean reward:  0.119 [-24.492, 23.366], mean action: 1.237 [0.000, 3.000],  loss: 15.015954, mse: 4295.203868, mean_q: 62.614359, mean_eps: 0.295620
 236294/300000: episode: 1828, duration: 7.564s, episode steps: 1000, steps per second: 132, episode reward: 120.165, mean reward:  0.120 [-22.108, 23.988], mean action: 1.216 [0.000, 3.000],  loss: 16.005881, mse: 4466.548609, mean_q: 63.425415, mean_eps: 0.292620
 236418/300000: episode: 1829, duration: 0.853s, episode steps: 124, steps per second: 145, episode reward:  4.639, mean reward:  0.037 [-100.000, 18.875], mean action: 1.863 [0.000, 3.000],  loss: 13.206643, mse: 4473.134451, mean_q: 63.796945, mean_eps: 0.290933
 237418/300000: episode: 1830, duration: 7.462s, episode steps: 1000, steps per second: 134, episode reward: 108.082, mean reward:  0.108 [-21.639, 24.773], mean action: 1.040 [0.000, 3.000],  loss: 14.138109, mse: 4579.409297, mean_q: 63.719419, mean_eps: 0.289247
 237535/300000: episode: 1831, duration: 0.807s, episode steps: 117, steps per second: 145, episode reward: 27.567, mean reward:  0.236 [-100.000, 14.728], mean action: 1.692 [0.000, 3.000],  loss: 13.199243, mse: 4732.718139, mean_q: 64.804151, mean_eps: 0.287572
 238535/300000: episode: 1832, duration: 7.961s, episode steps: 1000, steps per second: 126, episode reward: 91.323, mean reward:  0.091 [-21.999, 23.487], mean action: 1.819 [0.000, 3.000],  loss: 15.297204, mse: 4426.151413, mean_q: 61.760788, mean_eps: 0.285896
 238627/300000: episode: 1833, duration: 0.650s, episode steps:  92, steps per second: 142, episode reward: -27.687, mean reward: -0.301 [-100.000, 21.063], mean action: 1.739 [0.000, 3.000],  loss: 15.982313, mse: 4522.334887, mean_q: 62.794125, mean_eps: 0.284258
 238776/300000: episode: 1834, duration: 1.038s, episode steps: 149, steps per second: 144, episode reward:  2.619, mean reward:  0.018 [-100.000, 19.820], mean action: 1.651 [0.000, 3.000],  loss: 13.301708, mse: 4630.224080, mean_q: 63.303038, mean_eps: 0.283897
 238878/300000: episode: 1835, duration: 0.729s, episode steps: 102, steps per second: 140, episode reward: -290.537, mean reward: -2.848 [-100.000, 67.653], mean action: 1.275 [0.000, 3.000],  loss: 15.537538, mse: 4652.770127, mean_q: 63.717227, mean_eps: 0.283521
 238994/300000: episode: 1836, duration: 0.800s, episode steps: 116, steps per second: 145, episode reward: 20.221, mean reward:  0.174 [-100.000, 16.265], mean action: 1.776 [0.000, 3.000],  loss: 16.549733, mse: 4647.111366, mean_q: 63.253058, mean_eps: 0.283193
 239994/300000: episode: 1837, duration: 7.820s, episode steps: 1000, steps per second: 128, episode reward: 115.683, mean reward:  0.116 [-23.413, 23.270], mean action: 1.127 [0.000, 3.000],  loss: 14.581193, mse: 4595.860280, mean_q: 63.259737, mean_eps: 0.281519
 240994/300000: episode: 1838, duration: 7.886s, episode steps: 1000, steps per second: 127, episode reward: 28.073, mean reward:  0.028 [-24.856, 22.055], mean action: 2.231 [0.000, 3.000],  loss: 15.071264, mse: 4545.375865, mean_q: 61.448229, mean_eps: 0.278519
 241120/300000: episode: 1839, duration: 0.906s, episode steps: 126, steps per second: 139, episode reward: -48.428, mean reward: -0.384 [-100.000,  8.713], mean action: 1.833 [0.000, 3.000],  loss: 13.225038, mse: 4500.691947, mean_q: 59.893346, mean_eps: 0.276830
 241244/300000: episode: 1840, duration: 0.856s, episode steps: 124, steps per second: 145, episode reward: -12.052, mean reward: -0.097 [-100.000, 19.418], mean action: 1.581 [0.000, 3.000],  loss: 11.164580, mse: 4417.007145, mean_q: 59.331658, mean_eps: 0.276456
 241432/300000: episode: 1841, duration: 1.341s, episode steps: 188, steps per second: 140, episode reward: 92.798, mean reward:  0.494 [-100.000, 17.479], mean action: 1.734 [0.000, 3.000],  loss: 10.592275, mse: 4461.322114, mean_q: 59.556773, mean_eps: 0.275987
 241553/300000: episode: 1842, duration: 0.844s, episode steps: 121, steps per second: 143, episode reward: -49.259, mean reward: -0.407 [-100.000, 11.536], mean action: 1.711 [0.000, 3.000],  loss: 12.445630, mse: 4528.232688, mean_q: 60.286010, mean_eps: 0.275524
 241680/300000: episode: 1843, duration: 0.919s, episode steps: 127, steps per second: 138, episode reward: -50.050, mean reward: -0.394 [-100.000, 12.666], mean action: 1.701 [0.000, 3.000],  loss: 12.776860, mse: 4467.204425, mean_q: 60.124233, mean_eps: 0.275152
 242680/300000: episode: 1844, duration: 8.071s, episode steps: 1000, steps per second: 124, episode reward: 66.727, mean reward:  0.067 [-19.870, 22.174], mean action: 1.280 [0.000, 3.000],  loss: 12.439008, mse: 4339.269052, mean_q: 59.041276, mean_eps: 0.273462
 242802/300000: episode: 1845, duration: 0.976s, episode steps: 122, steps per second: 125, episode reward: -58.548, mean reward: -0.480 [-100.000, 15.938], mean action: 1.557 [0.000, 3.000],  loss: 15.006093, mse: 4265.132114, mean_q: 58.911601, mean_eps: 0.271779
 243802/300000: episode: 1846, duration: 8.052s, episode steps: 1000, steps per second: 124, episode reward: 104.903, mean reward:  0.105 [-20.472, 16.517], mean action: 1.346 [0.000, 3.000],  loss: 12.355130, mse: 4244.755562, mean_q: 58.318594, mean_eps: 0.270095
 243953/300000: episode: 1847, duration: 1.137s, episode steps: 151, steps per second: 133, episode reward:  1.991, mean reward:  0.013 [-100.000, 19.026], mean action: 1.483 [0.000, 3.000],  loss: 10.537960, mse: 4229.286600, mean_q: 58.464735, mean_eps: 0.268369
 244068/300000: episode: 1848, duration: 1.043s, episode steps: 115, steps per second: 110, episode reward: 40.618, mean reward:  0.353 [-100.000, 31.646], mean action: 1.843 [0.000, 3.000],  loss: 15.475295, mse: 4187.379027, mean_q: 58.198344, mean_eps: 0.267970
 244924/300000: episode: 1849, duration: 6.930s, episode steps: 856, steps per second: 124, episode reward: 207.463, mean reward:  0.242 [-21.089, 100.000], mean action: 2.179 [0.000, 3.000],  loss: 13.192485, mse: 4152.688872, mean_q: 57.727013, mean_eps: 0.266514
 245054/300000: episode: 1850, duration: 0.950s, episode steps: 130, steps per second: 137, episode reward: 19.891, mean reward:  0.153 [-100.000, 11.254], mean action: 1.846 [0.000, 3.000],  loss: 12.494304, mse: 4123.239220, mean_q: 57.486785, mean_eps: 0.265035
 245519/300000: episode: 1851, duration: 3.539s, episode steps: 465, steps per second: 131, episode reward: -190.940, mean reward: -0.411 [-100.000, 21.294], mean action: 2.105 [0.000, 3.000],  loss: 15.683404, mse: 4162.875271, mean_q: 58.196205, mean_eps: 0.264142
 246519/300000: episode: 1852, duration: 7.506s, episode steps: 1000, steps per second: 133, episode reward: 90.107, mean reward:  0.090 [-24.241, 21.965], mean action: 1.060 [0.000, 3.000],  loss: 12.341871, mse: 4303.416930, mean_q: 58.256765, mean_eps: 0.261944
 246802/300000: episode: 1853, duration: 2.035s, episode steps: 283, steps per second: 139, episode reward: -153.922, mean reward: -0.544 [-100.000, 11.438], mean action: 1.534 [0.000, 3.000],  loss: 13.038704, mse: 4172.791722, mean_q: 56.381844, mean_eps: 0.260020
 247802/300000: episode: 1854, duration: 7.573s, episode steps: 1000, steps per second: 132, episode reward: 64.216, mean reward:  0.064 [-20.418, 24.014], mean action: 1.090 [0.000, 3.000],  loss: 12.625585, mse: 3849.602430, mean_q: 54.011047, mean_eps: 0.258095
 247908/300000: episode: 1855, duration: 0.745s, episode steps: 106, steps per second: 142, episode reward:  7.188, mean reward:  0.068 [-100.000, 15.217], mean action: 1.528 [0.000, 3.000],  loss: 11.957684, mse: 3830.374592, mean_q: 54.066580, mean_eps: 0.256436
 248128/300000: episode: 1856, duration: 1.588s, episode steps: 220, steps per second: 139, episode reward: -146.183, mean reward: -0.664 [-100.000, 25.617], mean action: 1.777 [0.000, 3.000],  loss: 14.700966, mse: 3705.118813, mean_q: 53.269357, mean_eps: 0.255947
 249128/300000: episode: 1857, duration: 7.222s, episode steps: 1000, steps per second: 138, episode reward: 176.811, mean reward:  0.177 [-20.256, 24.281], mean action: 0.916 [0.000, 3.000],  loss: 11.795284, mse: 3529.553679, mean_q: 51.300916, mean_eps: 0.254117
 249220/300000: episode: 1858, duration: 0.644s, episode steps:  92, steps per second: 143, episode reward: 29.600, mean reward:  0.322 [-100.000, 16.805], mean action: 1.500 [0.000, 3.000],  loss: 10.762853, mse: 3312.665591, mean_q: 50.255129, mean_eps: 0.252479
 249904/300000: episode: 1859, duration: 5.096s, episode steps: 684, steps per second: 134, episode reward: 293.268, mean reward:  0.429 [-20.715, 100.000], mean action: 1.076 [0.000, 3.000],  loss: 10.169187, mse: 3507.661415, mean_q: 51.664531, mean_eps: 0.251315
 250457/300000: episode: 1860, duration: 4.067s, episode steps: 553, steps per second: 136, episode reward: 234.684, mean reward:  0.424 [-17.924, 100.000], mean action: 1.324 [0.000, 3.000],  loss: 11.317462, mse: 3393.656064, mean_q: 51.085124, mean_eps: 0.249460
 251457/300000: episode: 1861, duration: 8.205s, episode steps: 1000, steps per second: 122, episode reward: 132.205, mean reward:  0.132 [-20.088, 22.882], mean action: 1.219 [0.000, 3.000],  loss: 11.228295, mse: 3334.754527, mean_q: 50.130020, mean_eps: 0.247130
 252457/300000: episode: 1862, duration: 8.105s, episode steps: 1000, steps per second: 123, episode reward: 76.800, mean reward:  0.077 [-22.440, 23.883], mean action: 1.090 [0.000, 3.000],  loss: 11.100416, mse: 3180.434331, mean_q: 49.416850, mean_eps: 0.244130
 253457/300000: episode: 1863, duration: 8.808s, episode steps: 1000, steps per second: 114, episode reward: 45.630, mean reward:  0.046 [-17.495, 24.897], mean action: 1.560 [0.000, 3.000],  loss: 9.542983, mse: 2989.785387, mean_q: 47.915068, mean_eps: 0.241130
 254457/300000: episode: 1864, duration: 7.958s, episode steps: 1000, steps per second: 126, episode reward: 22.437, mean reward:  0.022 [-17.912, 21.257], mean action: 2.039 [0.000, 3.000],  loss: 11.095291, mse: 2988.085413, mean_q: 47.822002, mean_eps: 0.238130
 255136/300000: episode: 1865, duration: 4.974s, episode steps: 679, steps per second: 137, episode reward: -264.322, mean reward: -0.389 [-100.000, 17.679], mean action: 1.685 [0.000, 3.000],  loss: 9.733978, mse: 3139.790389, mean_q: 50.221099, mean_eps: 0.235612
 256136/300000: episode: 1866, duration: 8.065s, episode steps: 1000, steps per second: 124, episode reward: 124.058, mean reward:  0.124 [-20.341, 23.621], mean action: 2.275 [0.000, 3.000],  loss: 9.991496, mse: 3130.692098, mean_q: 50.341997, mean_eps: 0.233094
 257136/300000: episode: 1867, duration: 8.186s, episode steps: 1000, steps per second: 122, episode reward: 84.685, mean reward:  0.085 [-20.897, 24.148], mean action: 1.289 [0.000, 3.000],  loss: 9.704883, mse: 3065.048721, mean_q: 49.646870, mean_eps: 0.230093
 257716/300000: episode: 1868, duration: 4.419s, episode steps: 580, steps per second: 131, episode reward: 201.529, mean reward:  0.347 [-20.327, 100.000], mean action: 1.231 [0.000, 3.000],  loss: 9.255918, mse: 3119.056307, mean_q: 50.851525, mean_eps: 0.227723
 258092/300000: episode: 1869, duration: 2.734s, episode steps: 376, steps per second: 138, episode reward: 238.890, mean reward:  0.635 [-18.201, 100.000], mean action: 1.364 [0.000, 3.000],  loss: 9.767470, mse: 3203.467178, mean_q: 51.652586, mean_eps: 0.226289
 258593/300000: episode: 1870, duration: 3.718s, episode steps: 501, steps per second: 135, episode reward: 265.323, mean reward:  0.530 [-22.286, 100.000], mean action: 0.912 [0.000, 3.000],  loss: 8.993618, mse: 3183.507331, mean_q: 51.617500, mean_eps: 0.224974
 259593/300000: episode: 1871, duration: 7.928s, episode steps: 1000, steps per second: 126, episode reward: 65.649, mean reward:  0.066 [-19.467, 22.548], mean action: 1.911 [0.000, 3.000],  loss: 8.803017, mse: 3114.959911, mean_q: 51.340637, mean_eps: 0.222722
 259723/300000: episode: 1872, duration: 1.012s, episode steps: 130, steps per second: 128, episode reward: -10.353, mean reward: -0.080 [-100.000, 15.942], mean action: 1.723 [0.000, 3.000],  loss: 8.442575, mse: 3069.271228, mean_q: 51.233979, mean_eps: 0.221027
 260723/300000: episode: 1873, duration: 7.839s, episode steps: 1000, steps per second: 128, episode reward: 71.072, mean reward:  0.071 [-20.312, 21.664], mean action: 1.536 [0.000, 3.000],  loss: 9.197903, mse: 3114.637929, mean_q: 51.568448, mean_eps: 0.219332
 261723/300000: episode: 1874, duration: 8.141s, episode steps: 1000, steps per second: 123, episode reward: 140.082, mean reward:  0.140 [-23.864, 23.029], mean action: 0.873 [0.000, 3.000],  loss: 10.085652, mse: 3295.766562, mean_q: 52.770141, mean_eps: 0.216332
 262667/300000: episode: 1875, duration: 7.360s, episode steps: 944, steps per second: 128, episode reward: 214.269, mean reward:  0.227 [-18.882, 100.000], mean action: 1.282 [0.000, 3.000],  loss: 8.474805, mse: 3153.537281, mean_q: 51.843830, mean_eps: 0.213416
 263667/300000: episode: 1876, duration: 7.805s, episode steps: 1000, steps per second: 128, episode reward: 112.154, mean reward:  0.112 [-19.610, 22.968], mean action: 1.071 [0.000, 3.000],  loss: 8.162132, mse: 3185.382067, mean_q: 52.351862, mean_eps: 0.210500
 263876/300000: episode: 1877, duration: 1.472s, episode steps: 209, steps per second: 142, episode reward: -113.902, mean reward: -0.545 [-100.000, 12.478], mean action: 1.670 [0.000, 3.000],  loss: 10.662814, mse: 3119.689924, mean_q: 52.823333, mean_eps: 0.208687
 263969/300000: episode: 1878, duration: 0.651s, episode steps:  93, steps per second: 143, episode reward: 45.059, mean reward:  0.485 [-100.000, 16.695], mean action: 1.484 [0.000, 3.000],  loss: 11.112035, mse: 3266.418041, mean_q: 53.560750, mean_eps: 0.208234
 264969/300000: episode: 1879, duration: 8.028s, episode steps: 1000, steps per second: 125, episode reward: 91.614, mean reward:  0.092 [-19.905, 22.565], mean action: 1.390 [0.000, 3.000],  loss: 9.127361, mse: 3408.383960, mean_q: 55.130444, mean_eps: 0.206594
 265969/300000: episode: 1880, duration: 7.597s, episode steps: 1000, steps per second: 132, episode reward: 104.426, mean reward:  0.104 [-21.312, 22.735], mean action: 1.420 [0.000, 3.000],  loss: 8.254782, mse: 3280.192248, mean_q: 53.824583, mean_eps: 0.203594
 266078/300000: episode: 1881, duration: 0.764s, episode steps: 109, steps per second: 143, episode reward: 14.661, mean reward:  0.135 [-100.000, 13.319], mean action: 1.505 [0.000, 3.000],  loss: 9.581980, mse: 3228.023553, mean_q: 53.749978, mean_eps: 0.201931
 267078/300000: episode: 1882, duration: 7.502s, episode steps: 1000, steps per second: 133, episode reward: 115.362, mean reward:  0.115 [-24.690, 24.168], mean action: 1.568 [0.000, 3.000],  loss: 8.328391, mse: 3272.904473, mean_q: 53.853220, mean_eps: 0.200267
 268078/300000: episode: 1883, duration: 7.873s, episode steps: 1000, steps per second: 127, episode reward: 156.021, mean reward:  0.156 [-23.904, 23.078], mean action: 0.960 [0.000, 3.000],  loss: 8.937766, mse: 3065.496207, mean_q: 51.545412, mean_eps: 0.197267
 268317/300000: episode: 1884, duration: 1.733s, episode steps: 239, steps per second: 138, episode reward: 228.637, mean reward:  0.957 [-10.772, 100.000], mean action: 1.126 [0.000, 3.000],  loss: 7.821379, mse: 2827.677476, mean_q: 48.570165, mean_eps: 0.195409
 268411/300000: episode: 1885, duration: 0.649s, episode steps:  94, steps per second: 145, episode reward: -89.230, mean reward: -0.949 [-100.000,  6.103], mean action: 1.468 [0.000, 3.000],  loss: 8.743937, mse: 2870.708723, mean_q: 49.501502, mean_eps: 0.194910
 268536/300000: episode: 1886, duration: 0.901s, episode steps: 125, steps per second: 139, episode reward: -42.946, mean reward: -0.344 [-100.000, 10.955], mean action: 1.632 [0.000, 3.000],  loss: 7.099687, mse: 2835.856784, mean_q: 48.878405, mean_eps: 0.194581
 269536/300000: episode: 1887, duration: 7.644s, episode steps: 1000, steps per second: 131, episode reward: 120.512, mean reward:  0.121 [-20.051, 22.952], mean action: 0.780 [0.000, 3.000],  loss: 6.896722, mse: 2669.119708, mean_q: 47.836691, mean_eps: 0.192893
 270536/300000: episode: 1888, duration: 7.917s, episode steps: 1000, steps per second: 126, episode reward: 64.989, mean reward:  0.065 [-25.052, 25.000], mean action: 2.389 [0.000, 3.000],  loss: 7.801376, mse: 2677.112000, mean_q: 48.533822, mean_eps: 0.189893
 270727/300000: episode: 1889, duration: 1.363s, episode steps: 191, steps per second: 140, episode reward: -271.298, mean reward: -1.420 [-100.000,  5.762], mean action: 2.026 [0.000, 3.000],  loss: 7.095426, mse: 2680.811126, mean_q: 48.915399, mean_eps: 0.188107
 271685/300000: episode: 1890, duration: 7.922s, episode steps: 958, steps per second: 121, episode reward: 203.770, mean reward:  0.213 [-20.257, 100.000], mean action: 1.120 [0.000, 3.000],  loss: 8.605289, mse: 2751.262187, mean_q: 49.201172, mean_eps: 0.186383
 272424/300000: episode: 1891, duration: 5.737s, episode steps: 739, steps per second: 129, episode reward: 105.558, mean reward:  0.143 [-11.233, 100.000], mean action: 1.796 [0.000, 3.000],  loss: 5.322735, mse: 2833.936425, mean_q: 50.590330, mean_eps: 0.183838
 273291/300000: episode: 1892, duration: 6.724s, episode steps: 867, steps per second: 129, episode reward: 133.462, mean reward:  0.154 [-20.034, 100.000], mean action: 1.221 [0.000, 3.000],  loss: 6.817332, mse: 2915.752317, mean_q: 51.268411, mean_eps: 0.181429
 273400/300000: episode: 1893, duration: 0.760s, episode steps: 109, steps per second: 143, episode reward: -0.345, mean reward: -0.003 [-100.000, 14.831], mean action: 1.826 [0.000, 3.000],  loss: 7.997000, mse: 2963.870639, mean_q: 52.003126, mean_eps: 0.179965
 273501/300000: episode: 1894, duration: 0.724s, episode steps: 101, steps per second: 139, episode reward: 58.737, mean reward:  0.582 [-100.000, 81.291], mean action: 1.574 [0.000, 3.000],  loss: 10.017112, mse: 3050.596506, mean_q: 52.626078, mean_eps: 0.179650
 273737/300000: episode: 1895, duration: 1.651s, episode steps: 236, steps per second: 143, episode reward: 290.039, mean reward:  1.229 [-20.760, 100.000], mean action: 1.203 [0.000, 3.000],  loss: 11.540165, mse: 2965.969023, mean_q: 52.188740, mean_eps: 0.179145
 273830/300000: episode: 1896, duration: 0.663s, episode steps:  93, steps per second: 140, episode reward: -249.364, mean reward: -2.681 [-100.000,  3.288], mean action: 1.989 [0.000, 3.000],  loss: 11.919236, mse: 3024.034550, mean_q: 52.853083, mean_eps: 0.178651
 273905/300000: episode: 1897, duration: 0.539s, episode steps:  75, steps per second: 139, episode reward: -307.914, mean reward: -4.106 [-100.000,  3.069], mean action: 2.067 [0.000, 3.000],  loss: 9.468451, mse: 2934.815882, mean_q: 51.801444, mean_eps: 0.178399
 274015/300000: episode: 1898, duration: 0.765s, episode steps: 110, steps per second: 144, episode reward: -2.713, mean reward: -0.025 [-100.000, 12.352], mean action: 1.955 [0.000, 3.000],  loss: 18.040036, mse: 2984.081027, mean_q: 52.254841, mean_eps: 0.178121
 274119/300000: episode: 1899, duration: 0.732s, episode steps: 104, steps per second: 142, episode reward: -41.410, mean reward: -0.398 [-100.000, 10.807], mean action: 1.202 [0.000, 3.000],  loss: 7.215593, mse: 2964.520738, mean_q: 52.540616, mean_eps: 0.177801
 274880/300000: episode: 1900, duration: 5.842s, episode steps: 761, steps per second: 130, episode reward: 189.818, mean reward:  0.249 [-18.759, 100.000], mean action: 1.342 [0.000, 3.000],  loss: 9.247552, mse: 2766.234483, mean_q: 50.009638, mean_eps: 0.176503
 275208/300000: episode: 1901, duration: 2.342s, episode steps: 328, steps per second: 140, episode reward: 298.980, mean reward:  0.912 [-18.223, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 7.521046, mse: 2598.675718, mean_q: 48.356648, mean_eps: 0.174869
 275294/300000: episode: 1902, duration: 0.612s, episode steps:  86, steps per second: 140, episode reward: -328.592, mean reward: -3.821 [-100.000,  1.775], mean action: 1.872 [0.000, 3.000],  loss: 5.280860, mse: 2491.082961, mean_q: 47.650295, mean_eps: 0.174249
 275481/300000: episode: 1903, duration: 1.304s, episode steps: 187, steps per second: 143, episode reward: -371.533, mean reward: -1.987 [-100.000,  5.997], mean action: 1.952 [0.000, 3.000],  loss: 9.972828, mse: 2567.777489, mean_q: 47.702714, mean_eps: 0.173839
 275974/300000: episode: 1904, duration: 3.751s, episode steps: 493, steps per second: 131, episode reward: 165.711, mean reward:  0.336 [-19.741, 100.000], mean action: 1.381 [0.000, 3.000],  loss: 9.334428, mse: 2550.795148, mean_q: 47.694972, mean_eps: 0.172819
 276603/300000: episode: 1905, duration: 5.063s, episode steps: 629, steps per second: 124, episode reward: 184.528, mean reward:  0.293 [-19.617, 100.000], mean action: 0.967 [0.000, 3.000],  loss: 9.571470, mse: 2511.481953, mean_q: 47.716499, mean_eps: 0.171136
 277603/300000: episode: 1906, duration: 8.011s, episode steps: 1000, steps per second: 125, episode reward: -66.692, mean reward: -0.067 [-5.226,  5.114], mean action: 1.604 [0.000, 3.000],  loss: 6.827445, mse: 2505.495679, mean_q: 48.209819, mean_eps: 0.168692
 278071/300000: episode: 1907, duration: 3.664s, episode steps: 468, steps per second: 128, episode reward: 234.681, mean reward:  0.501 [-17.714, 100.000], mean action: 1.113 [0.000, 3.000],  loss: 8.130484, mse: 2366.950446, mean_q: 46.349409, mean_eps: 0.166490
 278492/300000: episode: 1908, duration: 3.059s, episode steps: 421, steps per second: 138, episode reward: 253.122, mean reward:  0.601 [-17.734, 100.000], mean action: 1.014 [0.000, 3.000],  loss: 7.137966, mse: 2293.500901, mean_q: 45.409280, mean_eps: 0.165157
 279144/300000: episode: 1909, duration: 4.706s, episode steps: 652, steps per second: 139, episode reward: 244.745, mean reward:  0.375 [-19.197, 100.000], mean action: 1.089 [0.000, 3.000],  loss: 8.563891, mse: 2310.452471, mean_q: 45.734979, mean_eps: 0.163547
 279623/300000: episode: 1910, duration: 3.511s, episode steps: 479, steps per second: 136, episode reward: 264.268, mean reward:  0.552 [-18.719, 100.000], mean action: 0.908 [0.000, 3.000],  loss: 8.695810, mse: 2283.470221, mean_q: 45.834739, mean_eps: 0.161851
 280006/300000: episode: 1911, duration: 2.830s, episode steps: 383, steps per second: 135, episode reward: 224.779, mean reward:  0.587 [-17.442, 100.000], mean action: 1.133 [0.000, 3.000],  loss: 7.461997, mse: 2252.337846, mean_q: 45.484561, mean_eps: 0.160558
 280855/300000: episode: 1912, duration: 6.385s, episode steps: 849, steps per second: 133, episode reward: 234.427, mean reward:  0.276 [-21.633, 100.000], mean action: 0.730 [0.000, 3.000],  loss: 8.017085, mse: 2247.187755, mean_q: 45.184643, mean_eps: 0.158710
 281238/300000: episode: 1913, duration: 2.816s, episode steps: 383, steps per second: 136, episode reward: 212.944, mean reward:  0.556 [-6.646, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 5.952496, mse: 2435.268480, mean_q: 46.493236, mean_eps: 0.156862
 282238/300000: episode: 1914, duration: 7.661s, episode steps: 1000, steps per second: 131, episode reward: 131.118, mean reward:  0.131 [-19.669, 22.847], mean action: 0.802 [0.000, 3.000],  loss: 7.945887, mse: 2464.566115, mean_q: 47.061417, mean_eps: 0.154787
 283238/300000: episode: 1915, duration: 7.572s, episode steps: 1000, steps per second: 132, episode reward: 139.310, mean reward:  0.139 [-20.640, 22.913], mean action: 1.168 [0.000, 3.000],  loss: 7.906494, mse: 2371.693243, mean_q: 46.782946, mean_eps: 0.151787
 283918/300000: episode: 1916, duration: 5.226s, episode steps: 680, steps per second: 130, episode reward: 186.307, mean reward:  0.274 [-21.740, 100.000], mean action: 1.694 [0.000, 3.000],  loss: 6.828811, mse: 2366.829216, mean_q: 46.622031, mean_eps: 0.149267
 284657/300000: episode: 1917, duration: 5.885s, episode steps: 739, steps per second: 126, episode reward: 218.687, mean reward:  0.296 [-19.279, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 5.907285, mse: 2351.219397, mean_q: 46.700435, mean_eps: 0.147139
 284752/300000: episode: 1918, duration: 0.741s, episode steps:  95, steps per second: 128, episode reward: -41.398, mean reward: -0.436 [-100.000, 17.230], mean action: 1.905 [0.000, 3.000],  loss: 8.216005, mse: 2417.397095, mean_q: 47.277011, mean_eps: 0.145888
 285403/300000: episode: 1919, duration: 5.216s, episode steps: 651, steps per second: 125, episode reward: 226.636, mean reward:  0.348 [-20.640, 100.000], mean action: 1.026 [0.000, 3.000],  loss: 6.547131, mse: 2387.841344, mean_q: 46.758977, mean_eps: 0.144769
 285494/300000: episode: 1920, duration: 0.648s, episode steps:  91, steps per second: 141, episode reward: -48.666, mean reward: -0.535 [-100.000, 14.813], mean action: 1.813 [0.000, 3.000],  loss: 5.326719, mse: 2443.714782, mean_q: 47.182527, mean_eps: 0.143656
 285613/300000: episode: 1921, duration: 0.823s, episode steps: 119, steps per second: 145, episode reward: 30.020, mean reward:  0.252 [-100.000, 28.981], mean action: 1.891 [0.000, 3.000],  loss: 7.878270, mse: 2510.471049, mean_q: 48.176870, mean_eps: 0.143341
 285706/300000: episode: 1922, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -241.487, mean reward: -2.597 [-100.000, 30.161], mean action: 1.613 [0.000, 3.000],  loss: 14.806893, mse: 2592.436433, mean_q: 47.657781, mean_eps: 0.143023
 286189/300000: episode: 1923, duration: 3.513s, episode steps: 483, steps per second: 137, episode reward: 264.883, mean reward:  0.548 [-17.814, 100.000], mean action: 1.099 [0.000, 3.000],  loss: 7.347449, mse: 2622.968072, mean_q: 48.433813, mean_eps: 0.142159
 286305/300000: episode: 1924, duration: 0.880s, episode steps: 116, steps per second: 132, episode reward: 43.030, mean reward:  0.371 [-100.000, 14.891], mean action: 1.966 [0.000, 3.000],  loss: 10.386852, mse: 2688.193620, mean_q: 48.459947, mean_eps: 0.141260
 286745/300000: episode: 1925, duration: 3.313s, episode steps: 440, steps per second: 133, episode reward: 294.679, mean reward:  0.670 [-17.938, 100.000], mean action: 1.280 [0.000, 3.000],  loss: 7.455691, mse: 2722.631417, mean_q: 48.782131, mean_eps: 0.140426
 286819/300000: episode: 1926, duration: 0.520s, episode steps:  74, steps per second: 142, episode reward: -6.160, mean reward: -0.083 [-100.000, 18.060], mean action: 1.851 [0.000, 3.000],  loss: 5.696019, mse: 2658.703574, mean_q: 49.210957, mean_eps: 0.139655
 286916/300000: episode: 1927, duration: 0.707s, episode steps:  97, steps per second: 137, episode reward: -99.170, mean reward: -1.022 [-100.000, 25.614], mean action: 1.887 [0.000, 3.000],  loss: 9.340007, mse: 2616.022000, mean_q: 48.622730, mean_eps: 0.139399
 287916/300000: episode: 1928, duration: 7.759s, episode steps: 1000, steps per second: 129, episode reward: 81.819, mean reward:  0.082 [-20.227, 21.841], mean action: 1.170 [0.000, 3.000],  loss: 6.892475, mse: 2609.618977, mean_q: 48.473498, mean_eps: 0.137753
 288566/300000: episode: 1929, duration: 4.754s, episode steps: 650, steps per second: 137, episode reward: 224.595, mean reward:  0.346 [-21.114, 100.000], mean action: 0.728 [0.000, 3.000],  loss: 6.971510, mse: 2642.906222, mean_q: 49.039341, mean_eps: 0.135278
 288659/300000: episode: 1930, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: 23.075, mean reward:  0.248 [-100.000, 19.220], mean action: 1.903 [0.000, 3.000],  loss: 5.690017, mse: 2602.322331, mean_q: 48.530533, mean_eps: 0.134164
 289124/300000: episode: 1931, duration: 3.467s, episode steps: 465, steps per second: 134, episode reward: 265.749, mean reward:  0.572 [-18.089, 100.000], mean action: 1.086 [0.000, 3.000],  loss: 6.326848, mse: 2592.890561, mean_q: 48.749468, mean_eps: 0.133327
 289571/300000: episode: 1932, duration: 3.260s, episode steps: 447, steps per second: 137, episode reward: 248.930, mean reward:  0.557 [-18.809, 100.000], mean action: 1.188 [0.000, 3.000],  loss: 7.997964, mse: 2550.335576, mean_q: 48.194605, mean_eps: 0.131959
 290375/300000: episode: 1933, duration: 6.030s, episode steps: 804, steps per second: 133, episode reward: 241.679, mean reward:  0.301 [-21.355, 100.000], mean action: 2.129 [0.000, 3.000],  loss: 6.969382, mse: 2513.611704, mean_q: 48.645858, mean_eps: 0.130082
 290473/300000: episode: 1934, duration: 0.679s, episode steps:  98, steps per second: 144, episode reward: -45.820, mean reward: -0.468 [-100.000, 11.358], mean action: 1.816 [0.000, 3.000],  loss: 7.700895, mse: 2522.628959, mean_q: 48.941033, mean_eps: 0.128729
 290884/300000: episode: 1935, duration: 3.051s, episode steps: 411, steps per second: 135, episode reward: 208.200, mean reward:  0.507 [-9.795, 100.000], mean action: 1.287 [0.000, 3.000],  loss: 6.863866, mse: 2511.564825, mean_q: 49.045159, mean_eps: 0.127966
 291150/300000: episode: 1936, duration: 1.900s, episode steps: 266, steps per second: 140, episode reward: 219.407, mean reward:  0.825 [-8.356, 100.000], mean action: 1.620 [0.000, 3.000],  loss: 8.414268, mse: 2551.356468, mean_q: 49.408757, mean_eps: 0.126950
 291289/300000: episode: 1937, duration: 0.989s, episode steps: 139, steps per second: 141, episode reward: 35.489, mean reward:  0.255 [-100.000, 13.775], mean action: 1.878 [0.000, 3.000],  loss: 7.211027, mse: 2519.043540, mean_q: 49.027509, mean_eps: 0.126343
 291492/300000: episode: 1938, duration: 1.408s, episode steps: 203, steps per second: 144, episode reward: 258.401, mean reward:  1.273 [-3.328, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 7.105401, mse: 2521.670841, mean_q: 48.909350, mean_eps: 0.125830
 292461/300000: episode: 1939, duration: 6.868s, episode steps: 969, steps per second: 141, episode reward: 300.996, mean reward:  0.311 [-23.975, 100.000], mean action: 0.563 [0.000, 3.000],  loss: 7.635460, mse: 2506.883235, mean_q: 49.039457, mean_eps: 0.124072
 292756/300000: episode: 1940, duration: 2.099s, episode steps: 295, steps per second: 141, episode reward: 274.402, mean reward:  0.930 [-3.160, 100.000], mean action: 0.946 [0.000, 3.000],  loss: 6.837305, mse: 2539.598494, mean_q: 49.185604, mean_eps: 0.122176
 293189/300000: episode: 1941, duration: 3.166s, episode steps: 433, steps per second: 137, episode reward: 178.829, mean reward:  0.413 [-18.699, 100.000], mean action: 2.095 [0.000, 3.000],  loss: 7.741670, mse: 2500.532130, mean_q: 49.114425, mean_eps: 0.121084
 293342/300000: episode: 1942, duration: 1.096s, episode steps: 153, steps per second: 140, episode reward: -61.238, mean reward: -0.400 [-100.000, 13.990], mean action: 1.824 [0.000, 3.000],  loss: 5.907038, mse: 2436.825520, mean_q: 49.036493, mean_eps: 0.120205
 293887/300000: episode: 1943, duration: 4.339s, episode steps: 545, steps per second: 126, episode reward: 252.810, mean reward:  0.464 [-17.772, 100.000], mean action: 1.198 [0.000, 3.000],  loss: 7.880716, mse: 2529.340234, mean_q: 49.721452, mean_eps: 0.119158
 294443/300000: episode: 1944, duration: 4.221s, episode steps: 556, steps per second: 132, episode reward: 257.783, mean reward:  0.464 [-17.954, 100.000], mean action: 1.077 [0.000, 3.000],  loss: 7.391155, mse: 2505.779471, mean_q: 49.278397, mean_eps: 0.117506
 295399/300000: episode: 1945, duration: 7.272s, episode steps: 956, steps per second: 131, episode reward: 168.989, mean reward:  0.177 [-24.069, 100.000], mean action: 1.634 [0.000, 3.000],  loss: 7.246549, mse: 2435.951105, mean_q: 47.963503, mean_eps: 0.115238
 295760/300000: episode: 1946, duration: 2.675s, episode steps: 361, steps per second: 135, episode reward: -26.886, mean reward: -0.074 [-100.000, 11.033], mean action: 1.584 [0.000, 3.000],  loss: 7.893660, mse: 2379.781546, mean_q: 47.330944, mean_eps: 0.113263
 296760/300000: episode: 1947, duration: 7.624s, episode steps: 1000, steps per second: 131, episode reward: 86.106, mean reward:  0.086 [-21.251, 12.572], mean action: 0.890 [0.000, 3.000],  loss: 7.787690, mse: 2292.175724, mean_q: 46.216572, mean_eps: 0.111221
 297208/300000: episode: 1948, duration: 3.335s, episode steps: 448, steps per second: 134, episode reward: 237.564, mean reward:  0.530 [-18.858, 100.000], mean action: 1.138 [0.000, 3.000],  loss: 7.025414, mse: 2190.000718, mean_q: 45.262406, mean_eps: 0.109049
 297432/300000: episode: 1949, duration: 1.587s, episode steps: 224, steps per second: 141, episode reward: 29.179, mean reward:  0.130 [-100.000, 23.509], mean action: 1.714 [0.000, 3.000],  loss: 6.475936, mse: 2255.347948, mean_q: 45.489355, mean_eps: 0.108041
 298418/300000: episode: 1950, duration: 8.472s, episode steps: 986, steps per second: 116, episode reward: 182.035, mean reward:  0.185 [-24.088, 100.000], mean action: 0.923 [0.000, 3.000],  loss: 8.432814, mse: 2172.210771, mean_q: 44.536419, mean_eps: 0.106226
 298924/300000: episode: 1951, duration: 3.973s, episode steps: 506, steps per second: 127, episode reward: 248.864, mean reward:  0.492 [-10.223, 100.000], mean action: 1.312 [0.000, 3.000],  loss: 8.101913, mse: 2243.047542, mean_q: 45.394019, mean_eps: 0.103988
 299370/300000: episode: 1952, duration: 3.294s, episode steps: 446, steps per second: 135, episode reward: 253.951, mean reward:  0.569 [-20.137, 100.000], mean action: 1.061 [0.000, 3.000],  loss: 7.136550, mse: 2272.647884, mean_q: 45.070486, mean_eps: 0.102560
 299672/300000: episode: 1953, duration: 2.171s, episode steps: 302, steps per second: 139, episode reward: 250.777, mean reward:  0.830 [-17.813, 100.000], mean action: 1.219 [0.000, 3.000],  loss: 6.305560, mse: 2256.680167, mean_q: 45.066334, mean_eps: 0.101438
done, took 2209.009 seconds
Testing for 5 episodes ...
Episode 1: reward: 288.179, steps: 237
Episode 2: reward: 243.162, steps: 181
Episode 3: reward: -160.837, steps: 132
Episode 4: reward: 9.373, steps: 165
Episode 5: reward: 277.927, steps: 184
Testing for 5 episodes ...
Episode 1: reward: 156.023, steps: 1000
Episode 2: reward: -53.369, steps: 81
Episode 3: reward: 93.100, steps: 1000
Episode 4: reward: 258.697, steps: 158
Episode 5: reward: 276.043, steps: 202
Testing for 5 episodes ...
Episode 1: reward: 14.063, steps: 405
Episode 2: reward: -91.498, steps: 83
Episode 3: reward: -209.949, steps: 112
Episode 4: reward: 5.255, steps: 129
Episode 5: reward: -70.503, steps: 77
Testing for 100 episodes ...
Episode 1: reward: 51.405, steps: 110
Episode 2: reward: -131.089, steps: 126
Episode 3: reward: 248.330, steps: 439
Episode 4: reward: 289.822, steps: 177
Episode 5: reward: 242.570, steps: 174
Episode 6: reward: 249.040, steps: 352
Episode 7: reward: 264.651, steps: 208
Episode 8: reward: 25.309, steps: 149
Episode 9: reward: 20.407, steps: 103
Episode 10: reward: 22.760, steps: 121
Episode 11: reward: 115.981, steps: 1000
Episode 12: reward: 279.708, steps: 261
Episode 13: reward: 35.341, steps: 141
Episode 14: reward: 288.712, steps: 291
Episode 15: reward: 259.486, steps: 215
Episode 16: reward: 239.213, steps: 172
Episode 17: reward: 19.241, steps: 164
Episode 18: reward: 257.098, steps: 282
Episode 19: reward: 161.365, steps: 534
Episode 20: reward: 235.594, steps: 237
Episode 21: reward: 193.021, steps: 461
Episode 22: reward: 48.009, steps: 109
Episode 23: reward: 266.591, steps: 261
Episode 24: reward: -83.980, steps: 89
Episode 25: reward: 231.211, steps: 339
Episode 26: reward: 221.786, steps: 221
Episode 27: reward: 98.090, steps: 1000
Episode 28: reward: 14.984, steps: 99
Episode 29: reward: -20.024, steps: 77
Episode 30: reward: 39.685, steps: 116
Episode 31: reward: 109.516, steps: 1000
Episode 32: reward: -9.188, steps: 128
Episode 33: reward: 17.056, steps: 98
Episode 34: reward: 297.172, steps: 214
Episode 35: reward: 166.704, steps: 394
Episode 36: reward: -155.125, steps: 207
Episode 37: reward: 39.745, steps: 89
Episode 38: reward: 227.610, steps: 276
Episode 39: reward: 27.190, steps: 117
Episode 40: reward: 105.542, steps: 1000
Episode 41: reward: 193.598, steps: 757
Episode 42: reward: 261.420, steps: 194
Episode 43: reward: 279.237, steps: 219
Episode 44: reward: -26.001, steps: 412
Episode 45: reward: 22.311, steps: 181
Episode 46: reward: 46.206, steps: 93
Episode 47: reward: 150.579, steps: 1000
Episode 48: reward: 293.804, steps: 282
Episode 49: reward: 183.061, steps: 491
Episode 50: reward: 254.820, steps: 203
Episode 51: reward: 271.821, steps: 211
Episode 52: reward: 259.877, steps: 411
Episode 53: reward: 238.729, steps: 498
Episode 54: reward: 240.420, steps: 367
Episode 55: reward: 279.504, steps: 215
Episode 56: reward: 266.178, steps: 249
Episode 57: reward: -215.058, steps: 173
Episode 58: reward: 227.550, steps: 458
Episode 59: reward: 76.731, steps: 168
Episode 60: reward: 231.957, steps: 473
Episode 61: reward: 247.952, steps: 209
Episode 62: reward: 236.933, steps: 421
Episode 63: reward: 27.211, steps: 143
Episode 64: reward: 255.544, steps: 192
Episode 65: reward: 271.772, steps: 251
Episode 66: reward: 242.523, steps: 460
Episode 67: reward: -2.898, steps: 186
Episode 68: reward: 278.985, steps: 196
Episode 69: reward: 260.344, steps: 365
Episode 70: reward: 269.528, steps: 181
Episode 72: reward: 121.883, steps: 1000
Episode 73: reward: 231.132, steps: 268
Episode 74: reward: 287.831, steps: 167
Episode 75: reward: 264.893, steps: 399
Episode 76: reward: 281.513, steps: 208
Episode 77: reward: 249.516, steps: 189
Episode 78: reward: 39.565, steps: 144
Episode 79: reward: 23.554, steps: 142
Episode 80: reward: 244.178, steps: 464
Episode 81: reward: 260.746, steps: 419
Episode 82: reward: 270.180, steps: 417
Episode 83: reward: -187.267, steps: 109
Episode 84: reward: 237.492, steps: 502
Episode 85: reward: 239.124, steps: 313
Episode 86: reward: 272.841, steps: 364
Episode 87: reward: 245.716, steps: 268
Episode 88: reward: 273.627, steps: 201
Episode 89: reward: 243.879, steps: 356
Episode 90: reward: 212.846, steps: 417
Episode 91: reward: -210.118, steps: 266
Episode 92: reward: 244.687, steps: 484
Episode 93: reward: 211.838, steps: 373
Episode 94: reward: -148.864, steps: 120
Episode 95: reward: -8.691, steps: 120
Episode 96: reward: 288.956, steps: 212
Episode 97: reward: 123.841, steps: 1000
Episode 98: reward: -30.664, steps: 96
Episode 99: reward: 217.426, steps: 256
Episode 100: reward: 243.503, steps: 376
Testing for 100 episodes ...
Episode 1: reward: 231.720, steps: 471
Episode 2: reward: 252.109, steps: 214
Episode 3: reward: 270.545, steps: 585
Episode 4: reward: 26.354, steps: 135
Episode 5: reward: 228.146, steps: 559
Episode 6: reward: -205.003, steps: 245
Episode 7: reward: 262.903, steps: 209
Episode 8: reward: -16.206, steps: 143
Episode 9: reward: 296.144, steps: 256
Episode 10: reward: 147.789, steps: 1000
Episode 11: reward: 251.194, steps: 360
Episode 12: reward: 248.528, steps: 224
Episode 13: reward: 272.187, steps: 144
Episode 14: reward: 288.000, steps: 277
Episode 15: reward: -88.623, steps: 91
Episode 16: reward: 146.406, steps: 1000
Episode 17: reward: 251.240, steps: 265
Episode 18: reward: -10.126, steps: 436
Episode 19: reward: -146.914, steps: 252
Episode 20: reward: 300.933, steps: 256
Episode 21: reward: 237.472, steps: 929
Episode 22: reward: -195.451, steps: 223
Episode 23: reward: 281.566, steps: 175
Episode 24: reward: 250.435, steps: 275
Episode 25: reward: -199.291, steps: 246
Episode 26: reward: 187.389, steps: 422
Episode 27: reward: 234.473, steps: 253
Episode 28: reward: 267.823, steps: 293
Episode 29: reward: 267.770, steps: 256
Episode 30: reward: 33.064, steps: 118
Episode 31: reward: 270.651, steps: 158
Episode 32: reward: 60.829, steps: 88
Episode 33: reward: 183.498, steps: 336
Episode 34: reward: 222.477, steps: 508
Episode 35: reward: 267.443, steps: 211
Episode 36: reward: -54.370, steps: 142
Episode 37: reward: 257.704, steps: 519
Episode 38: reward: 228.445, steps: 196
Episode 39: reward: 52.020, steps: 147
Episode 40: reward: 116.670, steps: 1000
Episode 41: reward: 255.947, steps: 207
Episode 42: reward: 263.002, steps: 174
Episode 43: reward: 56.497, steps: 156
Episode 44: reward: 261.271, steps: 174
Episode 45: reward: 287.985, steps: 273
Episode 46: reward: -84.544, steps: 381
Episode 47: reward: 249.534, steps: 254
Episode 48: reward: 168.043, steps: 521
Episode 49: reward: 277.447, steps: 212
Episode 50: reward: 239.920, steps: 436
Episode 51: reward: 15.328, steps: 338
Episode 52: reward: 134.698, steps: 1000
Episode 53: reward: 106.391, steps: 1000
Episode 54: reward: -23.221, steps: 108
Episode 55: reward: 18.680, steps: 337
Episode 56: reward: 47.232, steps: 151
Episode 57: reward: 36.651, steps: 121
Episode 58: reward: 269.774, steps: 290
Episode 59: reward: 257.709, steps: 187
Episode 60: reward: 265.244, steps: 191
Episode 61: reward: 41.051, steps: 90
Episode 62: reward: 259.140, steps: 448
Episode 63: reward: 211.184, steps: 598
Episode 64: reward: 240.183, steps: 448
Episode 65: reward: 227.263, steps: 325
Episode 66: reward: 256.126, steps: 191
Episode 67: reward: 47.652, steps: 91
Episode 68: reward: 40.312, steps: 149
Episode 69: reward: 259.941, steps: 263
Episode 70: reward: -8.322, steps: 252
Episode 71: reward: -28.831, steps: 87
Episode 72: reward: 267.033, steps: 318
Episode 73: reward: 21.001, steps: 180
Episode 74: reward: -218.735, steps: 168
Episode 75: reward: -15.554, steps: 106
Episode 76: reward: 295.446, steps: 247
Episode 77: reward: 63.707, steps: 191
Episode 78: reward: 121.498, steps: 1000
Episode 79: reward: 236.454, steps: 311
Episode 80: reward: 194.219, steps: 508
Episode 81: reward: 263.083, steps: 264
Episode 82: reward: 251.195, steps: 312
Episode 83: reward: 232.530, steps: 322
Episode 84: reward: 287.970, steps: 218
Episode 85: reward: 21.416, steps: 101
Episode 86: reward: 31.097, steps: 151
Episode 87: reward: 279.297, steps: 148
Episode 88: reward: 231.236, steps: 239
Episode 89: reward: 39.824, steps: 155
Episode 90: reward: 231.845, steps: 253
Episode 91: reward: 151.475, steps: 1000
Episode 92: reward: 38.115, steps: 124
Episode 93: reward: 55.149, steps: 89
Episode 94: reward: 20.662, steps: 161
Episode 95: reward: 52.075, steps: 167
Episode 96: reward: 0.530, steps: 140
Episode 97: reward: 142.951, steps: 1000
Episode 98: reward: 222.974, steps: 471
Episode 99: reward: 270.594, steps: 235
Episode 100: reward: 147.830, steps: 1000