Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_2 (Activation)    (None, 64)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 260
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 9,156
Trainable params: 9,156
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 200000 steps ...
    109/200000: episode: 1, duration: 0.154s, episode steps: 109, steps per second: 707, episode reward: -517.303, mean reward: -4.746 [-100.000,  1.201], mean action: 1.404 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    232/200000: episode: 2, duration: 0.105s, episode steps: 123, steps per second: 1168, episode reward: -93.193, mean reward: -0.758 [-100.000,  8.244], mean action: 1.626 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    335/200000: episode: 3, duration: 0.082s, episode steps: 103, steps per second: 1251, episode reward: -295.986, mean reward: -2.874 [-100.000,  7.784], mean action: 1.495 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    448/200000: episode: 4, duration: 0.083s, episode steps: 113, steps per second: 1361, episode reward: -399.096, mean reward: -3.532 [-100.000,  1.049], mean action: 1.389 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    516/200000: episode: 5, duration: 0.057s, episode steps:  68, steps per second: 1196, episode reward: -48.245, mean reward: -0.709 [-100.000, 16.757], mean action: 1.529 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    597/200000: episode: 6, duration: 0.061s, episode steps:  81, steps per second: 1321, episode reward: -74.526, mean reward: -0.920 [-100.000, 11.949], mean action: 1.543 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    667/200000: episode: 7, duration: 0.051s, episode steps:  70, steps per second: 1386, episode reward: -83.459, mean reward: -1.192 [-100.000, 17.139], mean action: 1.671 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    761/200000: episode: 8, duration: 0.073s, episode steps:  94, steps per second: 1296, episode reward: -147.613, mean reward: -1.570 [-100.000, 16.294], mean action: 1.468 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    857/200000: episode: 9, duration: 0.074s, episode steps:  96, steps per second: 1292, episode reward: -88.447, mean reward: -0.921 [-100.000,  7.312], mean action: 1.667 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    965/200000: episode: 10, duration: 0.076s, episode steps: 108, steps per second: 1430, episode reward: -45.500, mean reward: -0.421 [-100.000, 102.757], mean action: 1.491 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1036/200000: episode: 11, duration: 0.806s, episode steps:  71, steps per second:  88, episode reward: -230.569, mean reward: -3.247 [-100.000, 16.092], mean action: 1.577 [0.000, 3.000],  loss: 78.681726, mae: 1.218878, mean_q: -0.230062, mean_eps: 0.995419
   1110/200000: episode: 12, duration: 0.495s, episode steps:  74, steps per second: 150, episode reward: -106.998, mean reward: -1.446 [-100.000, 39.614], mean action: 1.554 [0.000, 3.000],  loss: 61.410010, mae: 1.916879, mean_q: -0.806272, mean_eps: 0.995174
   1173/200000: episode: 13, duration: 0.402s, episode steps:  63, steps per second: 157, episode reward: -76.392, mean reward: -1.213 [-100.000,  7.364], mean action: 1.587 [0.000, 3.000],  loss: 49.882892, mae: 2.059743, mean_q: -0.652351, mean_eps: 0.994866
   1249/200000: episode: 14, duration: 0.484s, episode steps:  76, steps per second: 157, episode reward: -152.481, mean reward: -2.006 [-100.000, 28.502], mean action: 1.184 [0.000, 3.000],  loss: 45.577545, mae: 2.333494, mean_q: 0.361729, mean_eps: 0.994553
   1346/200000: episode: 15, duration: 0.653s, episode steps:  97, steps per second: 149, episode reward: -130.410, mean reward: -1.344 [-100.000, 20.295], mean action: 1.505 [0.000, 3.000],  loss: 49.049603, mae: 2.515301, mean_q: 0.458482, mean_eps: 0.994163
   1488/200000: episode: 16, duration: 0.918s, episode steps: 142, steps per second: 155, episode reward: -440.690, mean reward: -3.103 [-100.000, 122.395], mean action: 1.669 [0.000, 3.000],  loss: 41.271554, mae: 2.538954, mean_q: 0.535015, mean_eps: 0.993626
   1567/200000: episode: 17, duration: 0.503s, episode steps:  79, steps per second: 157, episode reward: -124.664, mean reward: -1.578 [-100.000,  6.467], mean action: 1.380 [0.000, 3.000],  loss: 52.199887, mae: 2.602942, mean_q: -0.036698, mean_eps: 0.993129
   1660/200000: episode: 18, duration: 0.699s, episode steps:  93, steps per second: 133, episode reward: -74.948, mean reward: -0.806 [-100.000, 49.691], mean action: 1.613 [0.000, 3.000],  loss: 42.524242, mae: 2.586353, mean_q: -0.226527, mean_eps: 0.992741
   1747/200000: episode: 19, duration: 0.795s, episode steps:  87, steps per second: 109, episode reward: -103.724, mean reward: -1.192 [-100.000, 14.351], mean action: 1.563 [0.000, 3.000],  loss: 44.382024, mae: 2.362642, mean_q: 0.218609, mean_eps: 0.992336
   1840/200000: episode: 20, duration: 0.669s, episode steps:  93, steps per second: 139, episode reward: -146.418, mean reward: -1.574 [-100.000, 15.640], mean action: 1.570 [0.000, 3.000],  loss: 27.323746, mae: 2.022648, mean_q: 0.379524, mean_eps: 0.991931
   1906/200000: episode: 21, duration: 0.446s, episode steps:  66, steps per second: 148, episode reward: -114.105, mean reward: -1.729 [-100.000, 11.144], mean action: 1.697 [0.000, 3.000],  loss: 43.281171, mae: 2.611377, mean_q: 0.203926, mean_eps: 0.991574
   2012/200000: episode: 22, duration: 0.652s, episode steps: 106, steps per second: 163, episode reward: -83.980, mean reward: -0.792 [-100.000, 39.429], mean action: 1.528 [0.000, 3.000],  loss: 30.752154, mae: 2.192583, mean_q: 0.986632, mean_eps: 0.991187
   2073/200000: episode: 23, duration: 0.363s, episode steps:  61, steps per second: 168, episode reward: -88.440, mean reward: -1.450 [-100.000, 11.444], mean action: 1.541 [0.000, 3.000],  loss: 35.393465, mae: 2.601002, mean_q: 1.201194, mean_eps: 0.990811
   2133/200000: episode: 24, duration: 0.364s, episode steps:  60, steps per second: 165, episode reward: -126.217, mean reward: -2.104 [-100.000,  8.582], mean action: 1.417 [0.000, 3.000],  loss: 37.350723, mae: 2.870628, mean_q: 2.248059, mean_eps: 0.990539
   2211/200000: episode: 25, duration: 0.472s, episode steps:  78, steps per second: 165, episode reward: -92.134, mean reward: -1.181 [-100.000, 11.368], mean action: 1.628 [0.000, 3.000],  loss: 20.947104, mae: 2.563326, mean_q: 2.243844, mean_eps: 0.990228
   2349/200000: episode: 26, duration: 0.787s, episode steps: 138, steps per second: 175, episode reward: -289.324, mean reward: -2.097 [-100.000, 61.838], mean action: 1.572 [0.000, 3.000],  loss: 26.712568, mae: 2.846446, mean_q: 2.085968, mean_eps: 0.989742
   2420/200000: episode: 27, duration: 0.390s, episode steps:  71, steps per second: 182, episode reward: -59.550, mean reward: -0.839 [-100.000,  7.583], mean action: 1.648 [0.000, 3.000],  loss: 27.275822, mae: 3.081347, mean_q: 1.896621, mean_eps: 0.989272
   2516/200000: episode: 28, duration: 0.541s, episode steps:  96, steps per second: 177, episode reward: -151.367, mean reward: -1.577 [-100.000, 15.570], mean action: 1.583 [0.000, 3.000],  loss: 35.064972, mae: 3.009453, mean_q: 2.102077, mean_eps: 0.988896
   2603/200000: episode: 29, duration: 0.519s, episode steps:  87, steps per second: 168, episode reward: -266.123, mean reward: -3.059 [-100.000, 28.892], mean action: 1.632 [0.000, 3.000],  loss: 31.577572, mae: 2.986752, mean_q: 2.138390, mean_eps: 0.988485
   2744/200000: episode: 30, duration: 0.908s, episode steps: 141, steps per second: 155, episode reward: -472.613, mean reward: -3.352 [-100.000,  2.989], mean action: 1.702 [0.000, 3.000],  loss: 23.662253, mae: 2.822208, mean_q: 2.152595, mean_eps: 0.987972
   2831/200000: episode: 31, duration: 0.488s, episode steps:  87, steps per second: 178, episode reward: -141.257, mean reward: -1.624 [-100.000, 41.158], mean action: 1.310 [0.000, 3.000],  loss: 22.991182, mae: 3.125930, mean_q: 1.808019, mean_eps: 0.987458
   2917/200000: episode: 32, duration: 0.503s, episode steps:  86, steps per second: 171, episode reward: -76.597, mean reward: -0.891 [-100.000, 49.002], mean action: 1.500 [0.000, 3.000],  loss: 26.149893, mae: 3.314600, mean_q: 1.237426, mean_eps: 0.987069
   3023/200000: episode: 33, duration: 0.589s, episode steps: 106, steps per second: 180, episode reward: -396.920, mean reward: -3.745 [-100.000,  0.226], mean action: 1.585 [0.000, 3.000],  loss: 26.473478, mae: 2.896611, mean_q: 1.687356, mean_eps: 0.986637
   3132/200000: episode: 34, duration: 0.594s, episode steps: 109, steps per second: 183, episode reward: -183.017, mean reward: -1.679 [-100.000,  9.544], mean action: 1.385 [0.000, 3.000],  loss: 26.847265, mae: 3.554412, mean_q: 1.287950, mean_eps: 0.986153
   3214/200000: episode: 35, duration: 0.474s, episode steps:  82, steps per second: 173, episode reward: -63.251, mean reward: -0.771 [-100.000, 11.810], mean action: 1.561 [0.000, 3.000],  loss: 22.990943, mae: 3.748355, mean_q: 1.415458, mean_eps: 0.985724
   3349/200000: episode: 36, duration: 0.745s, episode steps: 135, steps per second: 181, episode reward: 11.450, mean reward:  0.085 [-100.000, 114.889], mean action: 1.689 [0.000, 3.000],  loss: 24.353828, mae: 3.677236, mean_q: 1.826735, mean_eps: 0.985236
   3477/200000: episode: 37, duration: 0.726s, episode steps: 128, steps per second: 176, episode reward: -440.939, mean reward: -3.445 [-100.000,  1.107], mean action: 1.539 [0.000, 3.000],  loss: 28.443699, mae: 3.899348, mean_q: 1.412380, mean_eps: 0.984644
   3568/200000: episode: 38, duration: 0.508s, episode steps:  91, steps per second: 179, episode reward: -114.863, mean reward: -1.262 [-100.000,  9.190], mean action: 1.440 [0.000, 3.000],  loss: 30.874732, mae: 4.170577, mean_q: 1.021943, mean_eps: 0.984151
   3650/200000: episode: 39, duration: 0.473s, episode steps:  82, steps per second: 174, episode reward: -182.871, mean reward: -2.230 [-100.000,  6.344], mean action: 1.280 [0.000, 3.000],  loss: 20.439979, mae: 3.664855, mean_q: 1.559317, mean_eps: 0.983762
   3789/200000: episode: 40, duration: 0.759s, episode steps: 139, steps per second: 183, episode reward: -128.831, mean reward: -0.927 [-100.000, 26.542], mean action: 1.504 [0.000, 3.000],  loss: 18.815947, mae: 3.644281, mean_q: 1.416339, mean_eps: 0.983264
   3906/200000: episode: 41, duration: 0.651s, episode steps: 117, steps per second: 180, episode reward: -85.174, mean reward: -0.728 [-100.000, 72.505], mean action: 1.615 [0.000, 3.000],  loss: 33.109622, mae: 3.968574, mean_q: 1.226443, mean_eps: 0.982688
   4014/200000: episode: 42, duration: 0.610s, episode steps: 108, steps per second: 177, episode reward: -160.103, mean reward: -1.482 [-100.000,  9.498], mean action: 1.500 [0.000, 3.000],  loss: 29.594533, mae: 3.824403, mean_q: 1.567624, mean_eps: 0.982182
   4080/200000: episode: 43, duration: 0.358s, episode steps:  66, steps per second: 184, episode reward: -130.848, mean reward: -1.983 [-100.000, 10.138], mean action: 1.667 [0.000, 3.000],  loss: 19.861043, mae: 4.492447, mean_q: 1.581799, mean_eps: 0.981791
   4156/200000: episode: 44, duration: 0.415s, episode steps:  76, steps per second: 183, episode reward: -122.622, mean reward: -1.613 [-100.000, 16.300], mean action: 1.474 [0.000, 3.000],  loss: 28.211798, mae: 4.893292, mean_q: 1.739998, mean_eps: 0.981471
   4277/200000: episode: 45, duration: 0.697s, episode steps: 121, steps per second: 174, episode reward: -109.509, mean reward: -0.905 [-100.000, 12.004], mean action: 1.512 [0.000, 3.000],  loss: 24.288217, mae: 4.857238, mean_q: 1.643256, mean_eps: 0.981028
   4442/200000: episode: 46, duration: 0.949s, episode steps: 165, steps per second: 174, episode reward:  0.678, mean reward:  0.004 [-100.000, 96.569], mean action: 1.436 [0.000, 3.000],  loss: 26.278425, mae: 4.824758, mean_q: 2.132360, mean_eps: 0.980384
   4537/200000: episode: 47, duration: 0.512s, episode steps:  95, steps per second: 185, episode reward: -117.255, mean reward: -1.234 [-100.000,  8.253], mean action: 1.484 [0.000, 3.000],  loss: 22.969011, mae: 4.830358, mean_q: 1.799098, mean_eps: 0.979800
   4609/200000: episode: 48, duration: 0.393s, episode steps:  72, steps per second: 183, episode reward: -22.403, mean reward: -0.311 [-100.000, 16.745], mean action: 1.514 [0.000, 3.000],  loss: 31.102056, mae: 4.811419, mean_q: 2.236670, mean_eps: 0.979424
   4688/200000: episode: 49, duration: 0.434s, episode steps:  79, steps per second: 182, episode reward: -424.233, mean reward: -5.370 [-100.000,  3.691], mean action: 1.329 [0.000, 3.000],  loss: 20.343312, mae: 4.999658, mean_q: 1.733328, mean_eps: 0.979084
   4791/200000: episode: 50, duration: 0.611s, episode steps: 103, steps per second: 169, episode reward: -316.950, mean reward: -3.077 [-100.000,  4.059], mean action: 1.728 [0.000, 3.000],  loss: 26.678116, mae: 4.899738, mean_q: 1.882006, mean_eps: 0.978674
   4864/200000: episode: 51, duration: 0.406s, episode steps:  73, steps per second: 180, episode reward: -112.333, mean reward: -1.539 [-100.000,  8.300], mean action: 1.466 [0.000, 3.000],  loss: 28.367220, mae: 5.273961, mean_q: 1.598616, mean_eps: 0.978278
   4973/200000: episode: 52, duration: 0.608s, episode steps: 109, steps per second: 179, episode reward: -233.634, mean reward: -2.143 [-100.000,  5.947], mean action: 1.606 [0.000, 3.000],  loss: 23.486350, mae: 4.812357, mean_q: 1.846220, mean_eps: 0.977869
   5036/200000: episode: 53, duration: 0.356s, episode steps:  63, steps per second: 177, episode reward: -79.481, mean reward: -1.262 [-100.000,  8.045], mean action: 1.587 [0.000, 3.000],  loss: 20.028742, mae: 4.986375, mean_q: 1.959540, mean_eps: 0.977482
   5108/200000: episode: 54, duration: 0.396s, episode steps:  72, steps per second: 182, episode reward: -91.453, mean reward: -1.270 [-100.000,  8.156], mean action: 1.417 [0.000, 3.000],  loss: 21.399084, mae: 5.585089, mean_q: 2.765728, mean_eps: 0.977178
   5216/200000: episode: 55, duration: 0.617s, episode steps: 108, steps per second: 175, episode reward: -177.545, mean reward: -1.644 [-100.000, 14.670], mean action: 1.556 [0.000, 3.000],  loss: 26.387776, mae: 6.048387, mean_q: 2.382397, mean_eps: 0.976773
   5286/200000: episode: 56, duration: 0.382s, episode steps:  70, steps per second: 183, episode reward: -108.325, mean reward: -1.548 [-100.000,  9.594], mean action: 1.600 [0.000, 3.000],  loss: 21.618250, mae: 5.630878, mean_q: 2.808736, mean_eps: 0.976373
   5418/200000: episode: 57, duration: 0.727s, episode steps: 132, steps per second: 182, episode reward: -138.485, mean reward: -1.049 [-100.000,  6.438], mean action: 1.538 [0.000, 3.000],  loss: 22.330037, mae: 5.914112, mean_q: 2.381728, mean_eps: 0.975918
   5517/200000: episode: 58, duration: 0.558s, episode steps:  99, steps per second: 177, episode reward: -243.188, mean reward: -2.456 [-100.000,  0.757], mean action: 1.626 [0.000, 3.000],  loss: 21.777970, mae: 5.576452, mean_q: 2.670263, mean_eps: 0.975399
   5606/200000: episode: 59, duration: 0.514s, episode steps:  89, steps per second: 173, episode reward: -369.070, mean reward: -4.147 [-100.000,  0.343], mean action: 1.247 [0.000, 3.000],  loss: 16.520236, mae: 5.718424, mean_q: 2.729287, mean_eps: 0.974975
   5679/200000: episode: 60, duration: 0.410s, episode steps:  73, steps per second: 178, episode reward: -44.398, mean reward: -0.608 [-100.000,  7.999], mean action: 1.521 [0.000, 3.000],  loss: 24.472522, mae: 5.880787, mean_q: 2.212848, mean_eps: 0.974611
   5741/200000: episode: 61, duration: 0.354s, episode steps:  62, steps per second: 175, episode reward: -155.013, mean reward: -2.500 [-100.000, 22.619], mean action: 1.726 [0.000, 3.000],  loss: 19.883447, mae: 6.049133, mean_q: 2.434502, mean_eps: 0.974307
   5818/200000: episode: 62, duration: 0.422s, episode steps:  77, steps per second: 182, episode reward: -127.029, mean reward: -1.650 [-100.000,  5.515], mean action: 1.532 [0.000, 3.000],  loss: 24.082944, mae: 5.807066, mean_q: 2.483960, mean_eps: 0.973995
   5944/200000: episode: 63, duration: 0.765s, episode steps: 126, steps per second: 165, episode reward: -163.731, mean reward: -1.299 [-100.000, 24.545], mean action: 1.325 [0.000, 3.000],  loss: 19.402560, mae: 5.733906, mean_q: 2.898751, mean_eps: 0.973538
   6040/200000: episode: 64, duration: 0.581s, episode steps:  96, steps per second: 165, episode reward: -359.793, mean reward: -3.748 [-100.000,  3.769], mean action: 1.583 [0.000, 3.000],  loss: 22.728096, mae: 6.027363, mean_q: 2.684711, mean_eps: 0.973038
   6104/200000: episode: 65, duration: 0.367s, episode steps:  64, steps per second: 175, episode reward: -111.037, mean reward: -1.735 [-100.000,  9.921], mean action: 1.406 [0.000, 3.000],  loss: 16.692646, mae: 6.059864, mean_q: 3.040295, mean_eps: 0.972678
   6199/200000: episode: 66, duration: 0.523s, episode steps:  95, steps per second: 182, episode reward: 13.201, mean reward:  0.139 [-100.000, 74.419], mean action: 1.432 [0.000, 3.000],  loss: 23.513052, mae: 6.887256, mean_q: 2.680111, mean_eps: 0.972321
   6269/200000: episode: 67, duration: 0.410s, episode steps:  70, steps per second: 171, episode reward: -89.475, mean reward: -1.278 [-100.000,  6.821], mean action: 1.343 [0.000, 3.000],  loss: 21.330164, mae: 6.799836, mean_q: 3.110683, mean_eps: 0.971949
   6374/200000: episode: 68, duration: 0.628s, episode steps: 105, steps per second: 167, episode reward: -365.404, mean reward: -3.480 [-100.000, 87.284], mean action: 1.371 [0.000, 3.000],  loss: 22.486496, mae: 6.723766, mean_q: 3.108930, mean_eps: 0.971556
   6511/200000: episode: 69, duration: 0.843s, episode steps: 137, steps per second: 163, episode reward: -125.019, mean reward: -0.913 [-100.000,  8.981], mean action: 1.438 [0.000, 3.000],  loss: 14.569478, mae: 6.354681, mean_q: 3.118996, mean_eps: 0.971011
   6609/200000: episode: 70, duration: 0.576s, episode steps:  98, steps per second: 170, episode reward: -99.845, mean reward: -1.019 [-100.000, 13.599], mean action: 1.643 [0.000, 3.000],  loss: 24.233422, mae: 6.675590, mean_q: 3.177522, mean_eps: 0.970482
   6713/200000: episode: 71, duration: 0.658s, episode steps: 104, steps per second: 158, episode reward: -285.796, mean reward: -2.748 [-100.000, 17.709], mean action: 1.587 [0.000, 3.000],  loss: 16.192825, mae: 6.196523, mean_q: 3.411867, mean_eps: 0.970028
   6813/200000: episode: 72, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -146.772, mean reward: -1.468 [-100.000, 82.058], mean action: 1.530 [0.000, 3.000],  loss: 20.299006, mae: 6.544444, mean_q: 2.935784, mean_eps: 0.969569
   6887/200000: episode: 73, duration: 0.420s, episode steps:  74, steps per second: 176, episode reward: -221.657, mean reward: -2.995 [-100.000, 14.146], mean action: 1.514 [0.000, 3.000],  loss: 20.986598, mae: 6.841510, mean_q: 2.823401, mean_eps: 0.969177
   6961/200000: episode: 74, duration: 0.416s, episode steps:  74, steps per second: 178, episode reward: -235.209, mean reward: -3.179 [-100.000, 11.562], mean action: 1.568 [0.000, 3.000],  loss: 24.436701, mae: 6.185946, mean_q: 3.023102, mean_eps: 0.968844
   7065/200000: episode: 75, duration: 0.631s, episode steps: 104, steps per second: 165, episode reward: -301.802, mean reward: -2.902 [-100.000,  7.279], mean action: 1.625 [0.000, 3.000],  loss: 22.805084, mae: 7.720670, mean_q: 3.163217, mean_eps: 0.968444
   7165/200000: episode: 76, duration: 0.583s, episode steps: 100, steps per second: 172, episode reward: -246.039, mean reward: -2.460 [-100.000,  5.322], mean action: 1.730 [0.000, 3.000],  loss: 20.075010, mae: 8.426557, mean_q: 3.267000, mean_eps: 0.967985
   7263/200000: episode: 77, duration: 0.561s, episode steps:  98, steps per second: 175, episode reward: -132.759, mean reward: -1.355 [-100.000,  9.613], mean action: 1.449 [0.000, 3.000],  loss: 31.824910, mae: 8.668567, mean_q: 2.735406, mean_eps: 0.967539
   7344/200000: episode: 78, duration: 0.450s, episode steps:  81, steps per second: 180, episode reward: -130.415, mean reward: -1.610 [-100.000,  9.198], mean action: 1.309 [0.000, 3.000],  loss: 25.476699, mae: 8.048640, mean_q: 3.747502, mean_eps: 0.967136
   7476/200000: episode: 79, duration: 0.787s, episode steps: 132, steps per second: 168, episode reward: -149.325, mean reward: -1.131 [-100.000, 11.419], mean action: 1.455 [0.000, 3.000],  loss: 25.452284, mae: 7.786799, mean_q: 3.795766, mean_eps: 0.966657
   7561/200000: episode: 80, duration: 0.483s, episode steps:  85, steps per second: 176, episode reward: -132.986, mean reward: -1.565 [-100.000,  7.005], mean action: 1.459 [0.000, 3.000],  loss: 20.111381, mae: 8.309916, mean_q: 3.689998, mean_eps: 0.966169
   7682/200000: episode: 81, duration: 0.677s, episode steps: 121, steps per second: 179, episode reward: -290.847, mean reward: -2.404 [-100.000, 12.644], mean action: 1.562 [0.000, 3.000],  loss: 22.499443, mae: 8.149646, mean_q: 3.666610, mean_eps: 0.965705
   7776/200000: episode: 82, duration: 0.551s, episode steps:  94, steps per second: 171, episode reward: -395.892, mean reward: -4.212 [-100.000,  0.291], mean action: 1.532 [0.000, 3.000],  loss: 25.878181, mae: 7.963660, mean_q: 3.652453, mean_eps: 0.965222
   7878/200000: episode: 83, duration: 0.579s, episode steps: 102, steps per second: 176, episode reward: -193.826, mean reward: -1.900 [-100.000,  8.836], mean action: 1.324 [0.000, 3.000],  loss: 19.757854, mae: 7.850408, mean_q: 3.766459, mean_eps: 0.964781
   7971/200000: episode: 84, duration: 0.536s, episode steps:  93, steps per second: 174, episode reward: -120.378, mean reward: -1.294 [-100.000,  8.173], mean action: 1.505 [0.000, 3.000],  loss: 27.254221, mae: 7.879995, mean_q: 3.069596, mean_eps: 0.964342
   8083/200000: episode: 85, duration: 0.655s, episode steps: 112, steps per second: 171, episode reward: -265.237, mean reward: -2.368 [-100.000, 14.797], mean action: 1.643 [0.000, 3.000],  loss: 21.063102, mae: 8.489010, mean_q: 3.699176, mean_eps: 0.963881
   8190/200000: episode: 86, duration: 0.634s, episode steps: 107, steps per second: 169, episode reward: 10.951, mean reward:  0.102 [-100.000, 80.734], mean action: 1.393 [0.000, 3.000],  loss: 27.913070, mae: 9.134653, mean_q: 4.147606, mean_eps: 0.963388
   8291/200000: episode: 87, duration: 0.562s, episode steps: 101, steps per second: 180, episode reward: -253.815, mean reward: -2.513 [-100.000,  7.559], mean action: 1.426 [0.000, 3.000],  loss: 21.335435, mae: 9.293374, mean_q: 3.533515, mean_eps: 0.962920
   8356/200000: episode: 88, duration: 0.369s, episode steps:  65, steps per second: 176, episode reward: -96.900, mean reward: -1.491 [-100.000,  7.212], mean action: 1.554 [0.000, 3.000],  loss: 14.721091, mae: 8.744469, mean_q: 3.941486, mean_eps: 0.962547
   8433/200000: episode: 89, duration: 0.440s, episode steps:  77, steps per second: 175, episode reward: -62.718, mean reward: -0.815 [-100.000,  7.177], mean action: 1.532 [0.000, 3.000],  loss: 24.985089, mae: 8.759523, mean_q: 4.165425, mean_eps: 0.962227
   8560/200000: episode: 90, duration: 0.762s, episode steps: 127, steps per second: 167, episode reward: -86.572, mean reward: -0.682 [-100.000, 11.596], mean action: 1.559 [0.000, 3.000],  loss: 27.468903, mae: 9.304279, mean_q: 3.473649, mean_eps: 0.961768
   8642/200000: episode: 91, duration: 0.516s, episode steps:  82, steps per second: 159, episode reward: -150.977, mean reward: -1.841 [-100.000,  6.580], mean action: 1.463 [0.000, 3.000],  loss: 25.317307, mae: 9.090169, mean_q: 3.885504, mean_eps: 0.961298
   8717/200000: episode: 92, duration: 0.481s, episode steps:  75, steps per second: 156, episode reward: -109.427, mean reward: -1.459 [-100.000, 15.022], mean action: 1.440 [0.000, 3.000],  loss: 24.084864, mae: 8.953655, mean_q: 4.358825, mean_eps: 0.960944
   8829/200000: episode: 93, duration: 0.682s, episode steps: 112, steps per second: 164, episode reward: -82.037, mean reward: -0.732 [-100.000,  7.583], mean action: 1.446 [0.000, 3.000],  loss: 22.509823, mae: 9.234519, mean_q: 3.969748, mean_eps: 0.960524
   8911/200000: episode: 94, duration: 0.522s, episode steps:  82, steps per second: 157, episode reward: -212.188, mean reward: -2.588 [-100.000,  5.273], mean action: 1.378 [0.000, 3.000],  loss: 19.117854, mae: 9.361492, mean_q: 3.839079, mean_eps: 0.960087
   8976/200000: episode: 95, duration: 0.404s, episode steps:  65, steps per second: 161, episode reward: -127.438, mean reward: -1.961 [-100.000, 17.194], mean action: 1.492 [0.000, 3.000],  loss: 15.310808, mae: 9.152205, mean_q: 4.159175, mean_eps: 0.959757
   9077/200000: episode: 96, duration: 0.598s, episode steps: 101, steps per second: 169, episode reward: -88.104, mean reward: -0.872 [-100.000, 16.862], mean action: 1.545 [0.000, 3.000],  loss: 26.532097, mae: 9.367365, mean_q: 4.070500, mean_eps: 0.959383
   9163/200000: episode: 97, duration: 0.533s, episode steps:  86, steps per second: 161, episode reward: -112.530, mean reward: -1.308 [-100.000, 18.291], mean action: 1.314 [0.000, 3.000],  loss: 12.801602, mae: 9.455092, mean_q: 5.040714, mean_eps: 0.958962
   9227/200000: episode: 98, duration: 0.404s, episode steps:  64, steps per second: 158, episode reward: -59.770, mean reward: -0.934 [-100.000, 10.880], mean action: 1.359 [0.000, 3.000],  loss: 17.157072, mae: 9.645325, mean_q: 3.769868, mean_eps: 0.958625
   9295/200000: episode: 99, duration: 0.415s, episode steps:  68, steps per second: 164, episode reward: -131.776, mean reward: -1.938 [-100.000,  8.074], mean action: 1.603 [0.000, 3.000],  loss: 12.849466, mae: 9.702635, mean_q: 4.933817, mean_eps: 0.958328
   9373/200000: episode: 100, duration: 0.449s, episode steps:  78, steps per second: 174, episode reward: -29.753, mean reward: -0.381 [-100.000, 17.258], mean action: 1.385 [0.000, 3.000],  loss: 13.316356, mae: 9.864535, mean_q: 4.694929, mean_eps: 0.957999
   9472/200000: episode: 101, duration: 0.613s, episode steps:  99, steps per second: 161, episode reward: -168.836, mean reward: -1.705 [-100.000, 29.916], mean action: 1.404 [0.000, 3.000],  loss: 20.401694, mae: 9.756700, mean_q: 4.530009, mean_eps: 0.957601
   9552/200000: episode: 102, duration: 0.476s, episode steps:  80, steps per second: 168, episode reward: -75.343, mean reward: -0.942 [-100.000, 16.265], mean action: 1.550 [0.000, 3.000],  loss: 23.711120, mae: 9.638055, mean_q: 5.052750, mean_eps: 0.957198
   9666/200000: episode: 103, duration: 0.676s, episode steps: 114, steps per second: 169, episode reward: -106.909, mean reward: -0.938 [-100.000,  7.313], mean action: 1.658 [0.000, 3.000],  loss: 21.022635, mae: 9.957990, mean_q: 4.452698, mean_eps: 0.956762
   9785/200000: episode: 104, duration: 0.654s, episode steps: 119, steps per second: 182, episode reward: -108.774, mean reward: -0.914 [-100.000, 11.831], mean action: 1.513 [0.000, 3.000],  loss: 21.823459, mae: 9.886198, mean_q: 4.301619, mean_eps: 0.956238
   9856/200000: episode: 105, duration: 0.394s, episode steps:  71, steps per second: 180, episode reward: -86.827, mean reward: -1.223 [-100.000, 11.859], mean action: 1.606 [0.000, 3.000],  loss: 20.084037, mae: 9.675665, mean_q: 5.015907, mean_eps: 0.955810
   9920/200000: episode: 106, duration: 0.362s, episode steps:  64, steps per second: 177, episode reward: -94.478, mean reward: -1.476 [-100.000, 17.804], mean action: 1.406 [0.000, 3.000],  loss: 8.388861, mae: 9.559975, mean_q: 4.539291, mean_eps: 0.955506
  10031/200000: episode: 107, duration: 0.677s, episode steps: 111, steps per second: 164, episode reward: -114.551, mean reward: -1.032 [-100.000,  9.966], mean action: 1.378 [0.000, 3.000],  loss: 18.145678, mae: 9.697624, mean_q: 5.336003, mean_eps: 0.955113
  10128/200000: episode: 108, duration: 0.544s, episode steps:  97, steps per second: 178, episode reward: -355.410, mean reward: -3.664 [-100.000, -0.009], mean action: 1.536 [0.000, 3.000],  loss: 18.646581, mae: 10.927380, mean_q: 4.867237, mean_eps: 0.954645
  10228/200000: episode: 109, duration: 0.548s, episode steps: 100, steps per second: 183, episode reward: -130.014, mean reward: -1.300 [-100.000,  5.366], mean action: 1.290 [0.000, 3.000],  loss: 15.738722, mae: 10.966421, mean_q: 4.312140, mean_eps: 0.954201
  10308/200000: episode: 110, duration: 0.470s, episode steps:  80, steps per second: 170, episode reward: -92.527, mean reward: -1.157 [-100.000,  7.331], mean action: 1.525 [0.000, 3.000],  loss: 28.843451, mae: 10.873139, mean_q: 4.703699, mean_eps: 0.953796
  10376/200000: episode: 111, duration: 0.399s, episode steps:  68, steps per second: 170, episode reward: -21.706, mean reward: -0.319 [-100.000, 95.608], mean action: 1.441 [0.000, 3.000],  loss: 21.068841, mae: 10.518795, mean_q: 5.751058, mean_eps: 0.953463
  10446/200000: episode: 112, duration: 0.417s, episode steps:  70, steps per second: 168, episode reward: -239.735, mean reward: -3.425 [-100.000,  4.127], mean action: 1.586 [0.000, 3.000],  loss: 27.252012, mae: 10.758768, mean_q: 6.332397, mean_eps: 0.953153
  10550/200000: episode: 113, duration: 0.601s, episode steps: 104, steps per second: 173, episode reward: -120.403, mean reward: -1.158 [-100.000, 20.380], mean action: 1.442 [0.000, 3.000],  loss: 19.681769, mae: 10.931463, mean_q: 5.231357, mean_eps: 0.952761
  10615/200000: episode: 114, duration: 0.365s, episode steps:  65, steps per second: 178, episode reward: -113.405, mean reward: -1.745 [-100.000, 11.777], mean action: 1.785 [0.000, 3.000],  loss: 24.429142, mae: 11.232464, mean_q: 4.509427, mean_eps: 0.952381
  10700/200000: episode: 115, duration: 0.520s, episode steps:  85, steps per second: 163, episode reward: -270.411, mean reward: -3.181 [-100.000,  3.480], mean action: 1.224 [0.000, 3.000],  loss: 16.088314, mae: 11.299886, mean_q: 4.829073, mean_eps: 0.952043
  10804/200000: episode: 116, duration: 0.591s, episode steps: 104, steps per second: 176, episode reward: -100.110, mean reward: -0.963 [-100.000, 11.081], mean action: 1.462 [0.000, 3.000],  loss: 17.254681, mae: 11.124009, mean_q: 5.253895, mean_eps: 0.951618
  10896/200000: episode: 117, duration: 0.519s, episode steps:  92, steps per second: 177, episode reward: -109.542, mean reward: -1.191 [-100.000,  7.528], mean action: 1.500 [0.000, 3.000],  loss: 15.312092, mae: 11.056936, mean_q: 5.074380, mean_eps: 0.951177
  11009/200000: episode: 118, duration: 0.653s, episode steps: 113, steps per second: 173, episode reward: -43.555, mean reward: -0.385 [-100.000, 85.574], mean action: 1.646 [0.000, 3.000],  loss: 21.680451, mae: 11.244468, mean_q: 5.326788, mean_eps: 0.950716
  11082/200000: episode: 119, duration: 0.457s, episode steps:  73, steps per second: 160, episode reward: -101.246, mean reward: -1.387 [-100.000,  7.036], mean action: 1.616 [0.000, 3.000],  loss: 20.772996, mae: 11.959612, mean_q: 5.988379, mean_eps: 0.950298
  11172/200000: episode: 120, duration: 0.523s, episode steps:  90, steps per second: 172, episode reward: -382.978, mean reward: -4.255 [-100.000,  1.359], mean action: 1.767 [0.000, 3.000],  loss: 30.440839, mae: 11.696808, mean_q: 6.694799, mean_eps: 0.949931
  11260/200000: episode: 121, duration: 0.487s, episode steps:  88, steps per second: 181, episode reward: -231.903, mean reward: -2.635 [-100.000,  6.413], mean action: 1.534 [0.000, 3.000],  loss: 15.730011, mae: 11.978919, mean_q: 5.995221, mean_eps: 0.949530
  11378/200000: episode: 122, duration: 0.650s, episode steps: 118, steps per second: 182, episode reward: -249.091, mean reward: -2.111 [-100.000,  1.561], mean action: 1.576 [0.000, 3.000],  loss: 14.121660, mae: 12.002959, mean_q: 6.149460, mean_eps: 0.949067
  11451/200000: episode: 123, duration: 0.450s, episode steps:  73, steps per second: 162, episode reward: -100.350, mean reward: -1.375 [-100.000,  8.539], mean action: 1.329 [0.000, 3.000],  loss: 20.169555, mae: 12.041760, mean_q: 5.849728, mean_eps: 0.948637
  11533/200000: episode: 124, duration: 0.502s, episode steps:  82, steps per second: 163, episode reward: -245.212, mean reward: -2.990 [-100.000,  5.644], mean action: 1.805 [0.000, 3.000],  loss: 13.613365, mae: 11.610889, mean_q: 5.881467, mean_eps: 0.948288
  11609/200000: episode: 125, duration: 0.447s, episode steps:  76, steps per second: 170, episode reward: -121.112, mean reward: -1.594 [-100.000,  7.775], mean action: 1.632 [0.000, 3.000],  loss: 9.958481, mae: 11.970677, mean_q: 5.751800, mean_eps: 0.947933
  11709/200000: episode: 126, duration: 0.573s, episode steps: 100, steps per second: 174, episode reward: -95.200, mean reward: -0.952 [-100.000,  9.895], mean action: 1.550 [0.000, 3.000],  loss: 13.495295, mae: 11.911956, mean_q: 5.876990, mean_eps: 0.947537
  11829/200000: episode: 127, duration: 0.723s, episode steps: 120, steps per second: 166, episode reward: -140.416, mean reward: -1.170 [-100.000, 19.489], mean action: 1.550 [0.000, 3.000],  loss: 19.601789, mae: 11.802766, mean_q: 6.423664, mean_eps: 0.947042
  11983/200000: episode: 128, duration: 0.876s, episode steps: 154, steps per second: 176, episode reward: -209.046, mean reward: -1.357 [-100.000, 127.707], mean action: 1.643 [0.000, 3.000],  loss: 16.967729, mae: 11.975626, mean_q: 5.939869, mean_eps: 0.946425
  12115/200000: episode: 129, duration: 0.754s, episode steps: 132, steps per second: 175, episode reward: -73.618, mean reward: -0.558 [-100.000, 11.906], mean action: 1.576 [0.000, 3.000],  loss: 21.907438, mae: 12.755321, mean_q: 6.520034, mean_eps: 0.945782
  12243/200000: episode: 130, duration: 0.765s, episode steps: 128, steps per second: 167, episode reward: -317.177, mean reward: -2.478 [-100.000, 118.608], mean action: 1.430 [0.000, 3.000],  loss: 15.981674, mae: 12.914995, mean_q: 6.317271, mean_eps: 0.945197
  12298/200000: episode: 131, duration: 0.311s, episode steps:  55, steps per second: 177, episode reward: -102.866, mean reward: -1.870 [-100.000,  7.162], mean action: 1.291 [0.000, 3.000],  loss: 13.873627, mae: 13.377178, mean_q: 6.851299, mean_eps: 0.944785
  12406/200000: episode: 132, duration: 0.600s, episode steps: 108, steps per second: 180, episode reward: -95.342, mean reward: -0.883 [-100.000, 12.784], mean action: 1.611 [0.000, 3.000],  loss: 15.402335, mae: 12.670357, mean_q: 6.975586, mean_eps: 0.944418
  12542/200000: episode: 133, duration: 0.762s, episode steps: 136, steps per second: 179, episode reward:  6.729, mean reward:  0.049 [-100.000, 83.015], mean action: 1.544 [0.000, 3.000],  loss: 13.368220, mae: 12.936965, mean_q: 6.164064, mean_eps: 0.943869
  12639/200000: episode: 134, duration: 0.594s, episode steps:  97, steps per second: 163, episode reward: -98.344, mean reward: -1.014 [-100.000, 10.165], mean action: 1.454 [0.000, 3.000],  loss: 17.145793, mae: 13.456516, mean_q: 6.196126, mean_eps: 0.943345
  12726/200000: episode: 135, duration: 0.495s, episode steps:  87, steps per second: 176, episode reward: -107.094, mean reward: -1.231 [-100.000,  9.076], mean action: 1.690 [0.000, 3.000],  loss: 15.522213, mae: 13.138347, mean_q: 6.749053, mean_eps: 0.942931
  12832/200000: episode: 136, duration: 0.590s, episode steps: 106, steps per second: 180, episode reward: -100.843, mean reward: -0.951 [-100.000,  5.554], mean action: 1.557 [0.000, 3.000],  loss: 14.513121, mae: 12.889392, mean_q: 7.685592, mean_eps: 0.942497
  12926/200000: episode: 137, duration: 0.532s, episode steps:  94, steps per second: 177, episode reward: -142.039, mean reward: -1.511 [-100.000, 41.022], mean action: 1.543 [0.000, 3.000],  loss: 24.469031, mae: 13.226134, mean_q: 7.217341, mean_eps: 0.942047
  13033/200000: episode: 138, duration: 0.666s, episode steps: 107, steps per second: 161, episode reward: -114.723, mean reward: -1.072 [-100.000, 17.707], mean action: 1.393 [0.000, 3.000],  loss: 19.053578, mae: 13.512118, mean_q: 7.348063, mean_eps: 0.941595
  13147/200000: episode: 139, duration: 0.768s, episode steps: 114, steps per second: 148, episode reward: -230.455, mean reward: -2.022 [-100.000, 27.406], mean action: 1.404 [0.000, 3.000],  loss: 16.481080, mae: 14.086205, mean_q: 8.028843, mean_eps: 0.941097
  13269/200000: episode: 140, duration: 0.820s, episode steps: 122, steps per second: 149, episode reward: -120.234, mean reward: -0.986 [-100.000, 12.341], mean action: 1.484 [0.000, 3.000],  loss: 15.503778, mae: 14.369001, mean_q: 7.537845, mean_eps: 0.940566
  13340/200000: episode: 141, duration: 0.464s, episode steps:  71, steps per second: 153, episode reward: -95.094, mean reward: -1.339 [-100.000, 10.728], mean action: 1.620 [0.000, 3.000],  loss: 14.982375, mae: 14.141321, mean_q: 8.025580, mean_eps: 0.940132
  13449/200000: episode: 142, duration: 0.675s, episode steps: 109, steps per second: 161, episode reward: -99.845, mean reward: -0.916 [-100.000, 12.056], mean action: 1.532 [0.000, 3.000],  loss: 11.647311, mae: 14.086019, mean_q: 8.290106, mean_eps: 0.939727
  13538/200000: episode: 143, duration: 0.529s, episode steps:  89, steps per second: 168, episode reward: -274.959, mean reward: -3.089 [-100.000, 109.144], mean action: 1.562 [0.000, 3.000],  loss: 8.126072, mae: 13.892395, mean_q: 8.551762, mean_eps: 0.939281
  13598/200000: episode: 144, duration: 0.412s, episode steps:  60, steps per second: 146, episode reward: -121.371, mean reward: -2.023 [-100.000, 11.314], mean action: 1.633 [0.000, 3.000],  loss: 17.843822, mae: 14.055333, mean_q: 9.399453, mean_eps: 0.938946
  13720/200000: episode: 145, duration: 0.796s, episode steps: 122, steps per second: 153, episode reward: -134.259, mean reward: -1.100 [-100.000, 14.602], mean action: 1.623 [0.000, 3.000],  loss: 11.966462, mae: 14.197168, mean_q: 7.434985, mean_eps: 0.938537
  13849/200000: episode: 146, duration: 0.734s, episode steps: 129, steps per second: 176, episode reward: -145.091, mean reward: -1.125 [-100.000,  7.086], mean action: 1.442 [0.000, 3.000],  loss: 18.111789, mae: 14.571007, mean_q: 7.720020, mean_eps: 0.937972
  13919/200000: episode: 147, duration: 0.385s, episode steps:  70, steps per second: 182, episode reward: -67.770, mean reward: -0.968 [-100.000,  9.539], mean action: 1.443 [0.000, 3.000],  loss: 17.044973, mae: 14.291391, mean_q: 7.916073, mean_eps: 0.937524
  14037/200000: episode: 148, duration: 0.708s, episode steps: 118, steps per second: 167, episode reward: -236.322, mean reward: -2.003 [-100.000,  6.295], mean action: 1.678 [0.000, 3.000],  loss: 12.055100, mae: 14.274492, mean_q: 8.883983, mean_eps: 0.937101
  14137/200000: episode: 149, duration: 0.588s, episode steps: 100, steps per second: 170, episode reward: -319.021, mean reward: -3.190 [-100.000,  4.502], mean action: 1.440 [0.000, 3.000],  loss: 16.456364, mae: 14.949246, mean_q: 8.998807, mean_eps: 0.936611
  14226/200000: episode: 150, duration: 0.508s, episode steps:  89, steps per second: 175, episode reward: -133.850, mean reward: -1.504 [-100.000, 21.933], mean action: 1.303 [0.000, 3.000],  loss: 19.322081, mae: 15.354288, mean_q: 8.813748, mean_eps: 0.936186
  14336/200000: episode: 151, duration: 0.627s, episode steps: 110, steps per second: 175, episode reward: -158.564, mean reward: -1.441 [-100.000, 41.325], mean action: 1.300 [0.000, 3.000],  loss: 19.660439, mae: 15.667518, mean_q: 8.453526, mean_eps: 0.935738
  14413/200000: episode: 152, duration: 0.568s, episode steps:  77, steps per second: 136, episode reward: -105.478, mean reward: -1.370 [-100.000,  9.435], mean action: 1.610 [0.000, 3.000],  loss: 13.316734, mae: 15.281950, mean_q: 8.685328, mean_eps: 0.935317
  14482/200000: episode: 153, duration: 0.437s, episode steps:  69, steps per second: 158, episode reward: -75.559, mean reward: -1.095 [-100.000,  9.085], mean action: 1.435 [0.000, 3.000],  loss: 18.289645, mae: 15.316740, mean_q: 9.243192, mean_eps: 0.934989
  14545/200000: episode: 154, duration: 0.398s, episode steps:  63, steps per second: 158, episode reward: -110.806, mean reward: -1.759 [-100.000, 15.841], mean action: 1.476 [0.000, 3.000],  loss: 16.669595, mae: 15.627499, mean_q: 7.877747, mean_eps: 0.934692
  14621/200000: episode: 155, duration: 0.438s, episode steps:  76, steps per second: 174, episode reward: -129.300, mean reward: -1.701 [-100.000,  6.507], mean action: 1.763 [0.000, 3.000],  loss: 24.152195, mae: 15.334140, mean_q: 9.328585, mean_eps: 0.934379
  14695/200000: episode: 156, duration: 0.442s, episode steps:  74, steps per second: 167, episode reward: 31.887, mean reward:  0.431 [-100.000, 100.648], mean action: 1.459 [0.000, 3.000],  loss: 12.014610, mae: 14.828295, mean_q: 10.303044, mean_eps: 0.934041
  14795/200000: episode: 157, duration: 0.570s, episode steps: 100, steps per second: 175, episode reward: -364.955, mean reward: -3.650 [-100.000, -0.214], mean action: 1.680 [0.000, 3.000],  loss: 13.735493, mae: 15.367058, mean_q: 9.019486, mean_eps: 0.933650
  14865/200000: episode: 158, duration: 0.426s, episode steps:  70, steps per second: 164, episode reward: -70.911, mean reward: -1.013 [-100.000, 11.685], mean action: 1.600 [0.000, 3.000],  loss: 18.742630, mae: 15.319483, mean_q: 7.956593, mean_eps: 0.933267
  14974/200000: episode: 159, duration: 0.623s, episode steps: 109, steps per second: 175, episode reward: -236.972, mean reward: -2.174 [-100.000,  6.258], mean action: 1.468 [0.000, 3.000],  loss: 22.603037, mae: 15.587226, mean_q: 8.251345, mean_eps: 0.932864
  15062/200000: episode: 160, duration: 0.547s, episode steps:  88, steps per second: 161, episode reward: -59.520, mean reward: -0.676 [-100.000,  8.958], mean action: 1.443 [0.000, 3.000],  loss: 21.320579, mae: 16.906482, mean_q: 9.769698, mean_eps: 0.932421
  15176/200000: episode: 161, duration: 0.671s, episode steps: 114, steps per second: 170, episode reward: -135.346, mean reward: -1.187 [-100.000,  5.691], mean action: 1.561 [0.000, 3.000],  loss: 20.787021, mae: 17.335583, mean_q: 10.277894, mean_eps: 0.931967
  15305/200000: episode: 162, duration: 0.730s, episode steps: 129, steps per second: 177, episode reward: -189.793, mean reward: -1.471 [-100.000, 17.187], mean action: 1.612 [0.000, 3.000],  loss: 16.276465, mae: 17.308633, mean_q: 10.295851, mean_eps: 0.931420
  15367/200000: episode: 163, duration: 0.359s, episode steps:  62, steps per second: 173, episode reward: -58.990, mean reward: -0.951 [-100.000, 17.476], mean action: 1.339 [0.000, 3.000],  loss: 15.641006, mae: 17.281353, mean_q: 9.726485, mean_eps: 0.930990
  15438/200000: episode: 164, duration: 0.445s, episode steps:  71, steps per second: 160, episode reward: -80.758, mean reward: -1.137 [-100.000, 12.867], mean action: 1.437 [0.000, 3.000],  loss: 10.743405, mae: 16.257442, mean_q: 10.650827, mean_eps: 0.930691
  15531/200000: episode: 165, duration: 0.533s, episode steps:  93, steps per second: 175, episode reward: -188.712, mean reward: -2.029 [-100.000, 80.235], mean action: 1.624 [0.000, 3.000],  loss: 18.095403, mae: 16.819163, mean_q: 11.297793, mean_eps: 0.930322
  15596/200000: episode: 166, duration: 0.370s, episode steps:  65, steps per second: 176, episode reward: -90.726, mean reward: -1.396 [-100.000, 11.319], mean action: 1.415 [0.000, 3.000],  loss: 17.814610, mae: 17.266391, mean_q: 10.335353, mean_eps: 0.929967
  15682/200000: episode: 167, duration: 0.483s, episode steps:  86, steps per second: 178, episode reward: -86.038, mean reward: -1.000 [-100.000, 12.006], mean action: 1.593 [0.000, 3.000],  loss: 14.058398, mae: 17.447981, mean_q: 9.941254, mean_eps: 0.929627
  15762/200000: episode: 168, duration: 0.452s, episode steps:  80, steps per second: 177, episode reward: -136.947, mean reward: -1.712 [-100.000, 19.082], mean action: 1.475 [0.000, 3.000],  loss: 10.769820, mae: 16.841230, mean_q: 11.237220, mean_eps: 0.929253
  15857/200000: episode: 169, duration: 0.580s, episode steps:  95, steps per second: 164, episode reward: -83.421, mean reward: -0.878 [-100.000, 11.042], mean action: 1.579 [0.000, 3.000],  loss: 14.160805, mae: 17.477989, mean_q: 10.412139, mean_eps: 0.928860
  15981/200000: episode: 170, duration: 0.689s, episode steps: 124, steps per second: 180, episode reward: -142.054, mean reward: -1.146 [-100.000, 35.769], mean action: 1.548 [0.000, 3.000],  loss: 23.364284, mae: 17.257030, mean_q: 11.753340, mean_eps: 0.928367
  16063/200000: episode: 171, duration: 0.466s, episode steps:  82, steps per second: 176, episode reward: -99.881, mean reward: -1.218 [-100.000,  8.156], mean action: 1.707 [0.000, 3.000],  loss: 18.930191, mae: 18.293426, mean_q: 11.658689, mean_eps: 0.927903
  16154/200000: episode: 172, duration: 0.534s, episode steps:  91, steps per second: 170, episode reward: -70.379, mean reward: -0.773 [-100.000, 22.859], mean action: 1.385 [0.000, 3.000],  loss: 22.179637, mae: 18.351836, mean_q: 12.315575, mean_eps: 0.927514
  16265/200000: episode: 173, duration: 0.666s, episode steps: 111, steps per second: 167, episode reward: -119.602, mean reward: -1.077 [-100.000,  9.035], mean action: 1.405 [0.000, 3.000],  loss: 19.982405, mae: 18.479251, mean_q: 11.872784, mean_eps: 0.927060
  16373/200000: episode: 174, duration: 0.598s, episode steps: 108, steps per second: 181, episode reward: -223.099, mean reward: -2.066 [-100.000,  6.005], mean action: 1.444 [0.000, 3.000],  loss: 15.199922, mae: 18.957999, mean_q: 11.416783, mean_eps: 0.926567
  16437/200000: episode: 175, duration: 0.359s, episode steps:  64, steps per second: 178, episode reward: -45.776, mean reward: -0.715 [-100.000, 16.940], mean action: 1.625 [0.000, 3.000],  loss: 12.821742, mae: 18.062565, mean_q: 13.424855, mean_eps: 0.926180
  16516/200000: episode: 176, duration: 0.467s, episode steps:  79, steps per second: 169, episode reward: -70.898, mean reward: -0.897 [-100.000, 10.307], mean action: 1.658 [0.000, 3.000],  loss: 18.008882, mae: 18.366362, mean_q: 11.899408, mean_eps: 0.925858
  16600/200000: episode: 177, duration: 0.515s, episode steps:  84, steps per second: 163, episode reward: -124.578, mean reward: -1.483 [-100.000, 10.076], mean action: 1.655 [0.000, 3.000],  loss: 24.202651, mae: 18.646742, mean_q: 12.545242, mean_eps: 0.925491
  16690/200000: episode: 178, duration: 0.530s, episode steps:  90, steps per second: 170, episode reward: -99.251, mean reward: -1.103 [-100.000,  8.995], mean action: 1.700 [0.000, 3.000],  loss: 20.945032, mae: 18.802665, mean_q: 11.049895, mean_eps: 0.925100
  16804/200000: episode: 179, duration: 0.632s, episode steps: 114, steps per second: 180, episode reward: -172.743, mean reward: -1.515 [-100.000,  9.418], mean action: 1.579 [0.000, 3.000],  loss: 18.990015, mae: 18.832254, mean_q: 13.183883, mean_eps: 0.924641
  16877/200000: episode: 180, duration: 0.419s, episode steps:  73, steps per second: 174, episode reward: -193.663, mean reward: -2.653 [-100.000,  7.302], mean action: 1.479 [0.000, 3.000],  loss: 17.927554, mae: 19.032427, mean_q: 12.312005, mean_eps: 0.924220
  16985/200000: episode: 181, duration: 0.776s, episode steps: 108, steps per second: 139, episode reward: -320.200, mean reward: -2.965 [-100.000, 22.746], mean action: 1.315 [0.000, 3.000],  loss: 19.812206, mae: 18.836520, mean_q: 12.261069, mean_eps: 0.923813
  17052/200000: episode: 182, duration: 0.472s, episode steps:  67, steps per second: 142, episode reward: -71.839, mean reward: -1.072 [-100.000,  9.056], mean action: 1.672 [0.000, 3.000],  loss: 13.308836, mae: 19.784203, mean_q: 12.796903, mean_eps: 0.923419
  17124/200000: episode: 183, duration: 0.440s, episode steps:  72, steps per second: 164, episode reward: -85.889, mean reward: -1.193 [-100.000,  6.781], mean action: 1.431 [0.000, 3.000],  loss: 13.056447, mae: 20.091405, mean_q: 12.827157, mean_eps: 0.923106
  17192/200000: episode: 184, duration: 0.395s, episode steps:  68, steps per second: 172, episode reward: -78.820, mean reward: -1.159 [-100.000,  7.948], mean action: 1.574 [0.000, 3.000],  loss: 16.367247, mae: 20.108206, mean_q: 13.446555, mean_eps: 0.922791
  17308/200000: episode: 185, duration: 0.685s, episode steps: 116, steps per second: 169, episode reward: -35.077, mean reward: -0.302 [-100.000, 119.662], mean action: 1.397 [0.000, 3.000],  loss: 20.757742, mae: 20.308096, mean_q: 13.094197, mean_eps: 0.922377
  17394/200000: episode: 186, duration: 0.604s, episode steps:  86, steps per second: 142, episode reward: -133.155, mean reward: -1.548 [-100.000,  4.221], mean action: 1.360 [0.000, 3.000],  loss: 20.762795, mae: 20.461468, mean_q: 13.047654, mean_eps: 0.921923
  17507/200000: episode: 187, duration: 0.737s, episode steps: 113, steps per second: 153, episode reward: -69.180, mean reward: -0.612 [-100.000, 63.179], mean action: 1.761 [0.000, 3.000],  loss: 15.495065, mae: 20.153219, mean_q: 13.854176, mean_eps: 0.921475
  17566/200000: episode: 188, duration: 0.383s, episode steps:  59, steps per second: 154, episode reward: -75.125, mean reward: -1.273 [-100.000, 15.608], mean action: 1.610 [0.000, 3.000],  loss: 18.285497, mae: 20.150359, mean_q: 13.286128, mean_eps: 0.921088
  17676/200000: episode: 189, duration: 0.870s, episode steps: 110, steps per second: 126, episode reward: -88.110, mean reward: -0.801 [-100.000, 18.060], mean action: 1.445 [0.000, 3.000],  loss: 12.381654, mae: 19.331004, mean_q: 13.809843, mean_eps: 0.920708
  17746/200000: episode: 190, duration: 0.500s, episode steps:  70, steps per second: 140, episode reward: -69.751, mean reward: -0.996 [-100.000, 11.791], mean action: 1.186 [0.000, 3.000],  loss: 15.165909, mae: 20.351910, mean_q: 14.260767, mean_eps: 0.920303
  17829/200000: episode: 191, duration: 0.598s, episode steps:  83, steps per second: 139, episode reward: -72.492, mean reward: -0.873 [-100.000, 18.883], mean action: 1.590 [0.000, 3.000],  loss: 9.809353, mae: 20.209040, mean_q: 14.006571, mean_eps: 0.919959
  17907/200000: episode: 192, duration: 0.558s, episode steps:  78, steps per second: 140, episode reward: -134.676, mean reward: -1.727 [-100.000,  7.670], mean action: 1.500 [0.000, 3.000],  loss: 19.497283, mae: 20.207705, mean_q: 13.460390, mean_eps: 0.919596
  18013/200000: episode: 193, duration: 0.667s, episode steps: 106, steps per second: 159, episode reward: -78.143, mean reward: -0.737 [-100.000,  8.114], mean action: 1.594 [0.000, 3.000],  loss: 16.525567, mae: 20.597141, mean_q: 13.706686, mean_eps: 0.919182
  18082/200000: episode: 194, duration: 0.402s, episode steps:  69, steps per second: 171, episode reward: -69.416, mean reward: -1.006 [-100.000, 17.736], mean action: 1.623 [0.000, 3.000],  loss: 11.884858, mae: 20.670675, mean_q: 14.496072, mean_eps: 0.918789
  18138/200000: episode: 195, duration: 0.345s, episode steps:  56, steps per second: 162, episode reward: -62.623, mean reward: -1.118 [-100.000, 10.622], mean action: 1.714 [0.000, 3.000],  loss: 17.727755, mae: 21.589183, mean_q: 15.246975, mean_eps: 0.918507
  18218/200000: episode: 196, duration: 0.509s, episode steps:  80, steps per second: 157, episode reward: 15.628, mean reward:  0.195 [-100.000, 116.600], mean action: 1.525 [0.000, 3.000],  loss: 14.496604, mae: 21.045330, mean_q: 14.837731, mean_eps: 0.918201
  18304/200000: episode: 197, duration: 0.519s, episode steps:  86, steps per second: 166, episode reward: -111.338, mean reward: -1.295 [-100.000,  8.248], mean action: 1.419 [0.000, 3.000],  loss: 15.140521, mae: 21.578006, mean_q: 13.104140, mean_eps: 0.917828
  18393/200000: episode: 198, duration: 0.535s, episode steps:  89, steps per second: 166, episode reward: -233.697, mean reward: -2.626 [-100.000,  5.204], mean action: 1.551 [0.000, 3.000],  loss: 14.288535, mae: 21.700422, mean_q: 13.750473, mean_eps: 0.917434
  18458/200000: episode: 199, duration: 0.372s, episode steps:  65, steps per second: 175, episode reward: -85.275, mean reward: -1.312 [-100.000,  9.968], mean action: 1.569 [0.000, 3.000],  loss: 17.381379, mae: 22.166323, mean_q: 13.647763, mean_eps: 0.917088
  18547/200000: episode: 200, duration: 0.546s, episode steps:  89, steps per second: 163, episode reward: -159.499, mean reward: -1.792 [-100.000,  6.216], mean action: 1.584 [0.000, 3.000],  loss: 13.506405, mae: 21.073735, mean_q: 13.831645, mean_eps: 0.916741
  18661/200000: episode: 201, duration: 0.726s, episode steps: 114, steps per second: 157, episode reward: -93.581, mean reward: -0.821 [-100.000,  6.644], mean action: 1.544 [0.000, 3.000],  loss: 11.900632, mae: 21.471658, mean_q: 14.676857, mean_eps: 0.916284
  18752/200000: episode: 202, duration: 0.548s, episode steps:  91, steps per second: 166, episode reward: -115.464, mean reward: -1.269 [-100.000,  7.029], mean action: 1.374 [0.000, 3.000],  loss: 24.031788, mae: 21.592597, mean_q: 13.944366, mean_eps: 0.915823
  18830/200000: episode: 203, duration: 0.434s, episode steps:  78, steps per second: 180, episode reward: -166.919, mean reward: -2.140 [-100.000,  5.171], mean action: 1.513 [0.000, 3.000],  loss: 18.796407, mae: 21.954739, mean_q: 13.529661, mean_eps: 0.915443
  18952/200000: episode: 204, duration: 0.720s, episode steps: 122, steps per second: 169, episode reward: -49.136, mean reward: -0.403 [-100.000, 13.141], mean action: 1.557 [0.000, 3.000],  loss: 12.090245, mae: 21.449048, mean_q: 14.115790, mean_eps: 0.914993
  19035/200000: episode: 205, duration: 0.556s, episode steps:  83, steps per second: 149, episode reward: -64.606, mean reward: -0.778 [-100.000, 43.745], mean action: 1.578 [0.000, 3.000],  loss: 23.185954, mae: 21.800864, mean_q: 14.221926, mean_eps: 0.914532
  19108/200000: episode: 206, duration: 0.475s, episode steps:  73, steps per second: 154, episode reward: -85.656, mean reward: -1.173 [-100.000, 22.765], mean action: 1.671 [0.000, 3.000],  loss: 9.215788, mae: 20.860408, mean_q: 13.816621, mean_eps: 0.914180
  19221/200000: episode: 207, duration: 0.702s, episode steps: 113, steps per second: 161, episode reward: -133.896, mean reward: -1.185 [-100.000, 11.530], mean action: 1.531 [0.000, 3.000],  loss: 15.611671, mae: 21.175688, mean_q: 15.199655, mean_eps: 0.913762
  19342/200000: episode: 208, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward: -110.513, mean reward: -0.913 [-100.000, 11.907], mean action: 1.661 [0.000, 3.000],  loss: 12.775208, mae: 21.219577, mean_q: 15.021138, mean_eps: 0.913235
  19445/200000: episode: 209, duration: 0.699s, episode steps: 103, steps per second: 147, episode reward: -56.305, mean reward: -0.547 [-100.000, 12.667], mean action: 1.534 [0.000, 3.000],  loss: 15.298405, mae: 20.990769, mean_q: 15.053268, mean_eps: 0.912732
  19569/200000: episode: 210, duration: 0.831s, episode steps: 124, steps per second: 149, episode reward: -187.784, mean reward: -1.514 [-100.000,  5.156], mean action: 1.621 [0.000, 3.000],  loss: 14.140225, mae: 21.062673, mean_q: 14.903430, mean_eps: 0.912221
  19668/200000: episode: 211, duration: 0.655s, episode steps:  99, steps per second: 151, episode reward: -106.450, mean reward: -1.075 [-100.000, 10.623], mean action: 1.505 [0.000, 3.000],  loss: 15.408078, mae: 21.014584, mean_q: 14.105042, mean_eps: 0.911719
  19786/200000: episode: 212, duration: 0.734s, episode steps: 118, steps per second: 161, episode reward: -203.265, mean reward: -1.723 [-100.000, 35.886], mean action: 1.695 [0.000, 3.000],  loss: 13.275270, mae: 20.829513, mean_q: 15.612749, mean_eps: 0.911231
  19845/200000: episode: 213, duration: 0.375s, episode steps:  59, steps per second: 157, episode reward: -59.504, mean reward: -1.009 [-100.000, 18.207], mean action: 1.492 [0.000, 3.000],  loss: 19.822054, mae: 21.206207, mean_q: 15.916431, mean_eps: 0.910833
  19949/200000: episode: 214, duration: 0.678s, episode steps: 104, steps per second: 153, episode reward: -122.062, mean reward: -1.174 [-100.000,  9.000], mean action: 1.635 [0.000, 3.000],  loss: 16.290704, mae: 20.959108, mean_q: 14.683776, mean_eps: 0.910466
  20012/200000: episode: 215, duration: 0.405s, episode steps:  63, steps per second: 156, episode reward: -56.934, mean reward: -0.904 [-100.000,  7.871], mean action: 1.492 [0.000, 3.000],  loss: 10.942339, mae: 21.311830, mean_q: 14.104766, mean_eps: 0.910090
  20109/200000: episode: 216, duration: 0.576s, episode steps:  97, steps per second: 168, episode reward: -118.998, mean reward: -1.227 [-100.000, 12.831], mean action: 1.598 [0.000, 3.000],  loss: 15.087340, mae: 21.705850, mean_q: 15.319954, mean_eps: 0.909730
  20230/200000: episode: 217, duration: 0.857s, episode steps: 121, steps per second: 141, episode reward: -95.769, mean reward: -0.791 [-100.000, 22.356], mean action: 1.504 [0.000, 3.000],  loss: 16.383446, mae: 21.767843, mean_q: 14.602024, mean_eps: 0.909240
  20324/200000: episode: 218, duration: 0.621s, episode steps:  94, steps per second: 151, episode reward: -118.029, mean reward: -1.256 [-100.000, 12.794], mean action: 1.479 [0.000, 3.000],  loss: 17.634749, mae: 22.102684, mean_q: 15.119594, mean_eps: 0.908756
  20418/200000: episode: 219, duration: 0.553s, episode steps:  94, steps per second: 170, episode reward: -98.907, mean reward: -1.052 [-100.000,  7.144], mean action: 1.479 [0.000, 3.000],  loss: 17.090820, mae: 21.652796, mean_q: 15.879724, mean_eps: 0.908333
  20537/200000: episode: 220, duration: 0.683s, episode steps: 119, steps per second: 174, episode reward: -77.297, mean reward: -0.650 [-100.000,  8.252], mean action: 1.605 [0.000, 3.000],  loss: 15.484789, mae: 21.697363, mean_q: 15.599257, mean_eps: 0.907854
  20649/200000: episode: 221, duration: 0.675s, episode steps: 112, steps per second: 166, episode reward: -91.702, mean reward: -0.819 [-100.000,  8.124], mean action: 1.554 [0.000, 3.000],  loss: 13.689792, mae: 21.943000, mean_q: 16.218017, mean_eps: 0.907334
  20756/200000: episode: 222, duration: 0.599s, episode steps: 107, steps per second: 179, episode reward: -162.327, mean reward: -1.517 [-100.000,  6.593], mean action: 1.626 [0.000, 3.000],  loss: 12.131430, mae: 21.575966, mean_q: 15.827071, mean_eps: 0.906841
  20850/200000: episode: 223, duration: 0.532s, episode steps:  94, steps per second: 177, episode reward: -106.863, mean reward: -1.137 [-100.000, 14.979], mean action: 1.521 [0.000, 3.000],  loss: 15.009427, mae: 21.570411, mean_q: 15.841455, mean_eps: 0.906389
  20945/200000: episode: 224, duration: 0.554s, episode steps:  95, steps per second: 171, episode reward: -62.168, mean reward: -0.654 [-100.000, 16.693], mean action: 1.642 [0.000, 3.000],  loss: 13.928005, mae: 21.431491, mean_q: 15.251075, mean_eps: 0.905964
  21013/200000: episode: 225, duration: 0.435s, episode steps:  68, steps per second: 156, episode reward: -52.504, mean reward: -0.772 [-100.000, 12.781], mean action: 1.500 [0.000, 3.000],  loss: 19.135422, mae: 21.782695, mean_q: 14.492061, mean_eps: 0.905597
  21084/200000: episode: 226, duration: 0.409s, episode steps:  71, steps per second: 173, episode reward: -64.874, mean reward: -0.914 [-100.000, 13.857], mean action: 1.648 [0.000, 3.000],  loss: 22.691434, mae: 22.929677, mean_q: 16.523775, mean_eps: 0.905284
  21162/200000: episode: 227, duration: 0.445s, episode steps:  78, steps per second: 175, episode reward: -69.723, mean reward: -0.894 [-100.000, 21.969], mean action: 1.513 [0.000, 3.000],  loss: 11.925032, mae: 22.045918, mean_q: 17.998705, mean_eps: 0.904949
  21255/200000: episode: 228, duration: 0.513s, episode steps:  93, steps per second: 181, episode reward: -101.350, mean reward: -1.090 [-100.000,  8.549], mean action: 1.548 [0.000, 3.000],  loss: 15.436855, mae: 22.733475, mean_q: 17.472793, mean_eps: 0.904564
  21366/200000: episode: 229, duration: 0.673s, episode steps: 111, steps per second: 165, episode reward: -108.429, mean reward: -0.977 [-100.000, 12.773], mean action: 1.505 [0.000, 3.000],  loss: 7.451109, mae: 22.544826, mean_q: 17.411340, mean_eps: 0.904105
  21441/200000: episode: 230, duration: 0.469s, episode steps:  75, steps per second: 160, episode reward: -120.968, mean reward: -1.613 [-100.000, 10.871], mean action: 1.733 [0.000, 3.000],  loss: 10.893584, mae: 22.279674, mean_q: 17.154415, mean_eps: 0.903687
  21552/200000: episode: 231, duration: 0.647s, episode steps: 111, steps per second: 172, episode reward: -214.980, mean reward: -1.937 [-100.000,  0.936], mean action: 1.505 [0.000, 3.000],  loss: 17.131442, mae: 22.851000, mean_q: 17.030620, mean_eps: 0.903268
  21620/200000: episode: 232, duration: 0.381s, episode steps:  68, steps per second: 178, episode reward: -54.628, mean reward: -0.803 [-100.000, 16.001], mean action: 1.603 [0.000, 3.000],  loss: 12.115931, mae: 23.251915, mean_q: 16.843227, mean_eps: 0.902865
  21686/200000: episode: 233, duration: 0.388s, episode steps:  66, steps per second: 170, episode reward: -105.728, mean reward: -1.602 [-100.000,  8.646], mean action: 1.515 [0.000, 3.000],  loss: 12.885713, mae: 22.390481, mean_q: 17.234440, mean_eps: 0.902564
  21762/200000: episode: 234, duration: 0.467s, episode steps:  76, steps per second: 163, episode reward: -64.612, mean reward: -0.850 [-100.000, 12.724], mean action: 1.539 [0.000, 3.000],  loss: 19.565054, mae: 23.264973, mean_q: 16.804457, mean_eps: 0.902244
  21899/200000: episode: 235, duration: 0.790s, episode steps: 137, steps per second: 173, episode reward: -111.848, mean reward: -0.816 [-100.000,  6.990], mean action: 1.482 [0.000, 3.000],  loss: 11.503634, mae: 22.646022, mean_q: 16.858894, mean_eps: 0.901765
  21998/200000: episode: 236, duration: 0.574s, episode steps:  99, steps per second: 172, episode reward: -136.318, mean reward: -1.377 [-100.000,  9.424], mean action: 1.535 [0.000, 3.000],  loss: 16.144992, mae: 22.865233, mean_q: 16.881224, mean_eps: 0.901234
  22125/200000: episode: 237, duration: 0.764s, episode steps: 127, steps per second: 166, episode reward: -188.205, mean reward: -1.482 [-100.000,  6.033], mean action: 1.638 [0.000, 3.000],  loss: 12.977761, mae: 24.027803, mean_q: 18.084995, mean_eps: 0.900726
  22193/200000: episode: 238, duration: 0.402s, episode steps:  68, steps per second: 169, episode reward: -103.857, mean reward: -1.527 [-100.000, 10.699], mean action: 1.824 [0.000, 3.000],  loss: 7.656359, mae: 23.865388, mean_q: 17.797864, mean_eps: 0.900287
  22306/200000: episode: 239, duration: 0.642s, episode steps: 113, steps per second: 176, episode reward: -96.185, mean reward: -0.851 [-100.000, 21.503], mean action: 1.628 [0.000, 3.000],  loss: 16.018034, mae: 23.967797, mean_q: 18.822474, mean_eps: 0.899880
  22390/200000: episode: 240, duration: 0.477s, episode steps:  84, steps per second: 176, episode reward: -127.094, mean reward: -1.513 [-100.000,  5.461], mean action: 1.595 [0.000, 3.000],  loss: 12.586114, mae: 23.870364, mean_q: 18.755588, mean_eps: 0.899436
  22488/200000: episode: 241, duration: 0.594s, episode steps:  98, steps per second: 165, episode reward: -121.709, mean reward: -1.242 [-100.000,  6.000], mean action: 1.602 [0.000, 3.000],  loss: 15.907437, mae: 24.223761, mean_q: 17.752572, mean_eps: 0.899027
  22576/200000: episode: 242, duration: 0.519s, episode steps:  88, steps per second: 169, episode reward: -156.923, mean reward: -1.783 [-100.000,  5.327], mean action: 1.500 [0.000, 3.000],  loss: 16.584322, mae: 24.169755, mean_q: 18.019121, mean_eps: 0.898608
  22648/200000: episode: 243, duration: 0.414s, episode steps:  72, steps per second: 174, episode reward: -139.526, mean reward: -1.938 [-100.000, 11.779], mean action: 1.625 [0.000, 3.000],  loss: 18.687163, mae: 24.393201, mean_q: 18.187862, mean_eps: 0.898248
  22740/200000: episode: 244, duration: 0.517s, episode steps:  92, steps per second: 178, episode reward: -213.420, mean reward: -2.320 [-100.000,  8.288], mean action: 1.511 [0.000, 3.000],  loss: 16.714042, mae: 24.478070, mean_q: 17.991089, mean_eps: 0.897879
  22835/200000: episode: 245, duration: 0.558s, episode steps:  95, steps per second: 170, episode reward: -98.665, mean reward: -1.039 [-100.000,  8.350], mean action: 1.526 [0.000, 3.000],  loss: 14.085965, mae: 24.069333, mean_q: 17.419933, mean_eps: 0.897458
  22947/200000: episode: 246, duration: 0.670s, episode steps: 112, steps per second: 167, episode reward: -79.460, mean reward: -0.709 [-100.000, 15.923], mean action: 1.482 [0.000, 3.000],  loss: 14.535783, mae: 24.054919, mean_q: 18.062364, mean_eps: 0.896993
  23020/200000: episode: 247, duration: 0.454s, episode steps:  73, steps per second: 161, episode reward: -100.928, mean reward: -1.383 [-100.000,  7.139], mean action: 1.507 [0.000, 3.000],  loss: 9.759480, mae: 24.021697, mean_q: 19.010242, mean_eps: 0.896576
  23096/200000: episode: 248, duration: 0.433s, episode steps:  76, steps per second: 176, episode reward: -117.137, mean reward: -1.541 [-100.000, 32.096], mean action: 1.776 [0.000, 3.000],  loss: 7.679502, mae: 25.734716, mean_q: 18.584908, mean_eps: 0.896241
  23199/200000: episode: 249, duration: 0.596s, episode steps: 103, steps per second: 173, episode reward: -111.244, mean reward: -1.080 [-100.000, 10.356], mean action: 1.485 [0.000, 3.000],  loss: 11.546336, mae: 25.223890, mean_q: 19.107503, mean_eps: 0.895838
  23302/200000: episode: 250, duration: 0.617s, episode steps: 103, steps per second: 167, episode reward: -413.716, mean reward: -4.017 [-100.000,  9.977], mean action: 1.495 [0.000, 3.000],  loss: 17.125035, mae: 25.009309, mean_q: 18.308585, mean_eps: 0.895375
  23424/200000: episode: 251, duration: 0.769s, episode steps: 122, steps per second: 159, episode reward: 26.415, mean reward:  0.217 [-100.000, 100.250], mean action: 1.574 [0.000, 3.000],  loss: 13.084893, mae: 24.788869, mean_q: 19.820918, mean_eps: 0.894869
  23521/200000: episode: 252, duration: 0.569s, episode steps:  97, steps per second: 171, episode reward: -84.897, mean reward: -0.875 [-100.000, 13.246], mean action: 1.670 [0.000, 3.000],  loss: 14.145788, mae: 25.149220, mean_q: 18.719005, mean_eps: 0.894376
  23589/200000: episode: 253, duration: 0.440s, episode steps:  68, steps per second: 154, episode reward: -47.067, mean reward: -0.692 [-100.000, 13.564], mean action: 1.721 [0.000, 3.000],  loss: 11.799530, mae: 25.432225, mean_q: 18.050669, mean_eps: 0.894005
  23716/200000: episode: 254, duration: 0.750s, episode steps: 127, steps per second: 169, episode reward: -139.660, mean reward: -1.100 [-100.000, 10.070], mean action: 1.504 [0.000, 3.000],  loss: 12.942463, mae: 24.848442, mean_q: 19.076612, mean_eps: 0.893566
  23811/200000: episode: 255, duration: 0.627s, episode steps:  95, steps per second: 151, episode reward: -117.781, mean reward: -1.240 [-100.000, 10.330], mean action: 1.484 [0.000, 3.000],  loss: 14.057050, mae: 24.903547, mean_q: 17.368049, mean_eps: 0.893066
  23890/200000: episode: 256, duration: 0.538s, episode steps:  79, steps per second: 147, episode reward: -64.951, mean reward: -0.822 [-100.000,  6.735], mean action: 1.620 [0.000, 3.000],  loss: 10.889009, mae: 25.146753, mean_q: 19.582299, mean_eps: 0.892675
  23976/200000: episode: 257, duration: 0.619s, episode steps:  86, steps per second: 139, episode reward: -105.574, mean reward: -1.228 [-100.000,  8.344], mean action: 1.477 [0.000, 3.000],  loss: 9.185971, mae: 25.135064, mean_q: 19.283816, mean_eps: 0.892304
  24075/200000: episode: 258, duration: 0.651s, episode steps:  99, steps per second: 152, episode reward: -126.945, mean reward: -1.282 [-100.000,  7.021], mean action: 1.596 [0.000, 3.000],  loss: 10.777728, mae: 25.787313, mean_q: 19.535597, mean_eps: 0.891888
  24147/200000: episode: 259, duration: 0.437s, episode steps:  72, steps per second: 165, episode reward: -47.584, mean reward: -0.661 [-100.000, 16.909], mean action: 1.361 [0.000, 3.000],  loss: 18.113397, mae: 25.557941, mean_q: 19.665617, mean_eps: 0.891503
  24223/200000: episode: 260, duration: 0.500s, episode steps:  76, steps per second: 152, episode reward: -99.950, mean reward: -1.315 [-100.000, 12.027], mean action: 1.316 [0.000, 3.000],  loss: 9.106249, mae: 25.712040, mean_q: 19.247370, mean_eps: 0.891170
  24318/200000: episode: 261, duration: 0.624s, episode steps:  95, steps per second: 152, episode reward: -108.284, mean reward: -1.140 [-100.000, 12.013], mean action: 1.653 [0.000, 3.000],  loss: 18.810031, mae: 25.421358, mean_q: 19.357410, mean_eps: 0.890785
  24416/200000: episode: 262, duration: 0.622s, episode steps:  98, steps per second: 158, episode reward: -113.603, mean reward: -1.159 [-100.000,  9.606], mean action: 1.633 [0.000, 3.000],  loss: 12.967705, mae: 25.863337, mean_q: 19.622478, mean_eps: 0.890351
  24487/200000: episode: 263, duration: 0.432s, episode steps:  71, steps per second: 165, episode reward: -88.304, mean reward: -1.244 [-100.000, 10.102], mean action: 1.521 [0.000, 3.000],  loss: 11.850683, mae: 25.442718, mean_q: 19.438483, mean_eps: 0.889970
  24565/200000: episode: 264, duration: 0.474s, episode steps:  78, steps per second: 165, episode reward: -116.333, mean reward: -1.491 [-100.000,  6.159], mean action: 1.705 [0.000, 3.000],  loss: 9.245215, mae: 25.651952, mean_q: 19.473323, mean_eps: 0.889635
  24665/200000: episode: 265, duration: 0.598s, episode steps: 100, steps per second: 167, episode reward: -111.021, mean reward: -1.110 [-100.000, 19.633], mean action: 1.700 [0.000, 3.000],  loss: 12.877629, mae: 26.219307, mean_q: 19.614143, mean_eps: 0.889235
  24736/200000: episode: 266, duration: 0.429s, episode steps:  71, steps per second: 166, episode reward: -104.374, mean reward: -1.470 [-100.000,  9.444], mean action: 1.676 [0.000, 3.000],  loss: 13.046613, mae: 26.296277, mean_q: 20.327000, mean_eps: 0.888850
  24837/200000: episode: 267, duration: 0.582s, episode steps: 101, steps per second: 174, episode reward: -165.534, mean reward: -1.639 [-100.000,  3.382], mean action: 1.277 [0.000, 3.000],  loss: 9.334202, mae: 25.953775, mean_q: 19.655697, mean_eps: 0.888463
  24906/200000: episode: 268, duration: 0.414s, episode steps:  69, steps per second: 167, episode reward: -68.467, mean reward: -0.992 [-100.000,  7.563], mean action: 1.420 [0.000, 3.000],  loss: 7.086326, mae: 25.967278, mean_q: 20.168744, mean_eps: 0.888080
  24995/200000: episode: 269, duration: 0.562s, episode steps:  89, steps per second: 158, episode reward: -95.506, mean reward: -1.073 [-100.000,  9.549], mean action: 1.640 [0.000, 3.000],  loss: 11.727447, mae: 25.908402, mean_q: 19.953852, mean_eps: 0.887725
  25067/200000: episode: 270, duration: 0.417s, episode steps:  72, steps per second: 173, episode reward: -104.903, mean reward: -1.457 [-100.000, 11.025], mean action: 1.778 [0.000, 3.000],  loss: 11.066781, mae: 26.452678, mean_q: 19.237764, mean_eps: 0.887363
  25150/200000: episode: 271, duration: 0.475s, episode steps:  83, steps per second: 175, episode reward: -76.883, mean reward: -0.926 [-100.000, 28.288], mean action: 1.639 [0.000, 3.000],  loss: 7.476936, mae: 26.289176, mean_q: 20.952755, mean_eps: 0.887014
  25257/200000: episode: 272, duration: 0.615s, episode steps: 107, steps per second: 174, episode reward: -107.446, mean reward: -1.004 [-100.000, 10.363], mean action: 1.486 [0.000, 3.000],  loss: 12.225889, mae: 26.317955, mean_q: 20.327139, mean_eps: 0.886587
  25360/200000: episode: 273, duration: 0.642s, episode steps: 103, steps per second: 160, episode reward: -129.292, mean reward: -1.255 [-100.000,  5.947], mean action: 1.456 [0.000, 3.000],  loss: 16.377320, mae: 25.751006, mean_q: 19.958741, mean_eps: 0.886114
  25439/200000: episode: 274, duration: 0.461s, episode steps:  79, steps per second: 171, episode reward: -79.574, mean reward: -1.007 [-100.000, 10.727], mean action: 1.266 [0.000, 3.000],  loss: 10.541554, mae: 26.730863, mean_q: 20.392607, mean_eps: 0.885705
  25524/200000: episode: 275, duration: 0.493s, episode steps:  85, steps per second: 172, episode reward: -117.270, mean reward: -1.380 [-100.000,  6.351], mean action: 1.459 [0.000, 3.000],  loss: 8.110337, mae: 26.972204, mean_q: 17.789448, mean_eps: 0.885336
  25586/200000: episode: 276, duration: 0.350s, episode steps:  62, steps per second: 177, episode reward: -58.958, mean reward: -0.951 [-100.000, 12.717], mean action: 1.677 [0.000, 3.000],  loss: 12.373076, mae: 26.071527, mean_q: 18.778126, mean_eps: 0.885005
  25702/200000: episode: 277, duration: 0.713s, episode steps: 116, steps per second: 163, episode reward: -103.242, mean reward: -0.890 [-100.000, 10.104], mean action: 1.543 [0.000, 3.000],  loss: 8.814526, mae: 26.595389, mean_q: 20.792369, mean_eps: 0.884604
  25788/200000: episode: 278, duration: 0.503s, episode steps:  86, steps per second: 171, episode reward: -153.905, mean reward: -1.790 [-100.000, 11.467], mean action: 1.756 [0.000, 3.000],  loss: 14.137265, mae: 26.817289, mean_q: 19.369506, mean_eps: 0.884150
  25926/200000: episode: 279, duration: 0.785s, episode steps: 138, steps per second: 176, episode reward: -230.672, mean reward: -1.672 [-100.000,  5.202], mean action: 1.659 [0.000, 3.000],  loss: 13.286586, mae: 26.669941, mean_q: 19.685147, mean_eps: 0.883646
  26044/200000: episode: 280, duration: 0.705s, episode steps: 118, steps per second: 167, episode reward: -71.302, mean reward: -0.604 [-100.000,  9.948], mean action: 1.576 [0.000, 3.000],  loss: 17.626033, mae: 26.714650, mean_q: 19.264712, mean_eps: 0.883070
  26144/200000: episode: 281, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -278.425, mean reward: -2.784 [-100.000,  0.850], mean action: 1.450 [0.000, 3.000],  loss: 13.830749, mae: 27.116271, mean_q: 21.303312, mean_eps: 0.882579
  26265/200000: episode: 282, duration: 0.704s, episode steps: 121, steps per second: 172, episode reward: -92.067, mean reward: -0.761 [-100.000, 12.409], mean action: 1.504 [0.000, 3.000],  loss: 15.118436, mae: 27.272948, mean_q: 20.920962, mean_eps: 0.882082
  26340/200000: episode: 283, duration: 0.436s, episode steps:  75, steps per second: 172, episode reward: -130.636, mean reward: -1.742 [-100.000, 18.088], mean action: 1.640 [0.000, 3.000],  loss: 9.262924, mae: 26.824333, mean_q: 20.325177, mean_eps: 0.881641
  26429/200000: episode: 284, duration: 0.537s, episode steps:  89, steps per second: 166, episode reward: -153.398, mean reward: -1.724 [-100.000,  1.669], mean action: 1.483 [0.000, 3.000],  loss: 15.824939, mae: 26.971475, mean_q: 19.854943, mean_eps: 0.881272
  26498/200000: episode: 285, duration: 0.416s, episode steps:  69, steps per second: 166, episode reward: -131.581, mean reward: -1.907 [-100.000,  6.877], mean action: 1.638 [0.000, 3.000],  loss: 12.355613, mae: 26.610173, mean_q: 21.802983, mean_eps: 0.880916
  26563/200000: episode: 286, duration: 0.382s, episode steps:  65, steps per second: 170, episode reward: -67.193, mean reward: -1.034 [-100.000, 10.813], mean action: 1.754 [0.000, 3.000],  loss: 9.929619, mae: 27.270954, mean_q: 23.123393, mean_eps: 0.880615
  26645/200000: episode: 287, duration: 0.544s, episode steps:  82, steps per second: 151, episode reward: -136.379, mean reward: -1.663 [-100.000, 10.565], mean action: 1.659 [0.000, 3.000],  loss: 8.865619, mae: 26.995326, mean_q: 20.335214, mean_eps: 0.880284
  26751/200000: episode: 288, duration: 0.857s, episode steps: 106, steps per second: 124, episode reward: -103.852, mean reward: -0.980 [-100.000,  8.214], mean action: 1.538 [0.000, 3.000],  loss: 15.153157, mae: 27.464865, mean_q: 19.910315, mean_eps: 0.879861
  26843/200000: episode: 289, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -143.151, mean reward: -1.556 [-100.000,  5.495], mean action: 1.196 [0.000, 3.000],  loss: 13.759658, mae: 26.986113, mean_q: 20.010435, mean_eps: 0.879416
  26936/200000: episode: 290, duration: 0.654s, episode steps:  93, steps per second: 142, episode reward: -101.046, mean reward: -1.087 [-100.000,  6.618], mean action: 1.452 [0.000, 3.000],  loss: 14.857770, mae: 26.961280, mean_q: 21.440452, mean_eps: 0.879000
  27000/200000: episode: 291, duration: 0.435s, episode steps:  64, steps per second: 147, episode reward: -69.150, mean reward: -1.080 [-100.000, 18.103], mean action: 1.328 [0.000, 3.000],  loss: 22.536343, mae: 27.381616, mean_q: 19.336713, mean_eps: 0.878646
  27077/200000: episode: 292, duration: 0.496s, episode steps:  77, steps per second: 155, episode reward: -216.339, mean reward: -2.810 [-100.000,  7.748], mean action: 1.597 [0.000, 3.000],  loss: 19.178735, mae: 28.617356, mean_q: 21.379835, mean_eps: 0.878329
  27166/200000: episode: 293, duration: 0.530s, episode steps:  89, steps per second: 168, episode reward: -101.964, mean reward: -1.146 [-100.000,  7.743], mean action: 1.753 [0.000, 3.000],  loss: 8.920260, mae: 28.450401, mean_q: 21.595118, mean_eps: 0.877956
  27297/200000: episode: 294, duration: 0.800s, episode steps: 131, steps per second: 164, episode reward: -207.167, mean reward: -1.581 [-100.000, 68.920], mean action: 1.695 [0.000, 3.000],  loss: 12.978957, mae: 28.702920, mean_q: 23.541220, mean_eps: 0.877461
  27380/200000: episode: 295, duration: 0.516s, episode steps:  83, steps per second: 161, episode reward: -110.672, mean reward: -1.333 [-100.000,  6.507], mean action: 1.458 [0.000, 3.000],  loss: 6.695475, mae: 28.406431, mean_q: 22.965360, mean_eps: 0.876979
  27455/200000: episode: 296, duration: 0.460s, episode steps:  75, steps per second: 163, episode reward: -120.194, mean reward: -1.603 [-100.000, 22.863], mean action: 1.280 [0.000, 3.000],  loss: 16.623120, mae: 27.953334, mean_q: 22.495942, mean_eps: 0.876623
  27557/200000: episode: 297, duration: 0.671s, episode steps: 102, steps per second: 152, episode reward: -110.516, mean reward: -1.083 [-100.000, 18.475], mean action: 1.598 [0.000, 3.000],  loss: 12.757622, mae: 28.493162, mean_q: 21.707392, mean_eps: 0.876225
  27643/200000: episode: 298, duration: 0.540s, episode steps:  86, steps per second: 159, episode reward: -8.728, mean reward: -0.101 [-100.000, 59.281], mean action: 1.267 [0.000, 3.000],  loss: 15.368420, mae: 28.888530, mean_q: 22.006415, mean_eps: 0.875802
  27741/200000: episode: 299, duration: 0.635s, episode steps:  98, steps per second: 154, episode reward: -131.378, mean reward: -1.341 [-100.000,  6.915], mean action: 1.520 [0.000, 3.000],  loss: 8.823109, mae: 28.186646, mean_q: 21.974484, mean_eps: 0.875388
  27809/200000: episode: 300, duration: 0.463s, episode steps:  68, steps per second: 147, episode reward: -239.336, mean reward: -3.520 [-100.000,  3.018], mean action: 1.603 [0.000, 3.000],  loss: 12.848262, mae: 27.736213, mean_q: 21.701465, mean_eps: 0.875015
  27875/200000: episode: 301, duration: 0.451s, episode steps:  66, steps per second: 146, episode reward: -38.027, mean reward: -0.576 [-100.000, 15.127], mean action: 1.455 [0.000, 3.000],  loss: 13.910595, mae: 28.326278, mean_q: 20.343451, mean_eps: 0.874713
  27995/200000: episode: 302, duration: 0.771s, episode steps: 120, steps per second: 156, episode reward: -83.115, mean reward: -0.693 [-100.000, 18.340], mean action: 1.633 [0.000, 3.000],  loss: 17.172600, mae: 28.387666, mean_q: 22.401394, mean_eps: 0.874295
  28086/200000: episode: 303, duration: 0.584s, episode steps:  91, steps per second: 156, episode reward: -133.249, mean reward: -1.464 [-100.000,  7.930], mean action: 1.549 [0.000, 3.000],  loss: 8.267236, mae: 29.163918, mean_q: 21.636189, mean_eps: 0.873820
  28156/200000: episode: 304, duration: 0.464s, episode steps:  70, steps per second: 151, episode reward: -83.033, mean reward: -1.186 [-100.000,  7.310], mean action: 1.529 [0.000, 3.000],  loss: 19.372844, mae: 29.386585, mean_q: 23.156980, mean_eps: 0.873458
  28258/200000: episode: 305, duration: 0.652s, episode steps: 102, steps per second: 156, episode reward: -102.074, mean reward: -1.001 [-100.000, 26.590], mean action: 1.539 [0.000, 3.000],  loss: 12.644713, mae: 29.049939, mean_q: 23.134627, mean_eps: 0.873071
  28348/200000: episode: 306, duration: 0.530s, episode steps:  90, steps per second: 170, episode reward: -156.466, mean reward: -1.739 [-100.000,  7.805], mean action: 1.522 [0.000, 3.000],  loss: 13.851081, mae: 29.129861, mean_q: 23.714145, mean_eps: 0.872639
  28430/200000: episode: 307, duration: 0.521s, episode steps:  82, steps per second: 157, episode reward: -159.163, mean reward: -1.941 [-100.000, 16.917], mean action: 1.622 [0.000, 3.000],  loss: 11.969861, mae: 29.495618, mean_q: 22.838813, mean_eps: 0.872252
  28551/200000: episode: 308, duration: 0.723s, episode steps: 121, steps per second: 167, episode reward: -61.791, mean reward: -0.511 [-100.000, 14.554], mean action: 1.570 [0.000, 3.000],  loss: 13.203096, mae: 29.699470, mean_q: 24.488613, mean_eps: 0.871795
  28657/200000: episode: 309, duration: 0.713s, episode steps: 106, steps per second: 149, episode reward: -182.295, mean reward: -1.720 [-100.000,  7.955], mean action: 1.538 [0.000, 3.000],  loss: 14.820773, mae: 29.591314, mean_q: 22.517480, mean_eps: 0.871284
  28767/200000: episode: 310, duration: 0.731s, episode steps: 110, steps per second: 151, episode reward: -107.974, mean reward: -0.982 [-100.000, 22.229], mean action: 1.382 [0.000, 3.000],  loss: 11.275490, mae: 29.589989, mean_q: 23.185417, mean_eps: 0.870798
  28856/200000: episode: 311, duration: 0.572s, episode steps:  89, steps per second: 156, episode reward: -95.396, mean reward: -1.072 [-100.000,  6.128], mean action: 1.764 [0.000, 3.000],  loss: 15.555238, mae: 29.749351, mean_q: 23.481225, mean_eps: 0.870351
  28983/200000: episode: 312, duration: 0.881s, episode steps: 127, steps per second: 144, episode reward: -153.581, mean reward: -1.209 [-100.000,  8.908], mean action: 1.669 [0.000, 3.000],  loss: 6.364587, mae: 29.032838, mean_q: 22.284690, mean_eps: 0.869864
  29112/200000: episode: 313, duration: 0.798s, episode steps: 129, steps per second: 162, episode reward: -90.494, mean reward: -0.702 [-100.000, 10.210], mean action: 1.349 [0.000, 3.000],  loss: 13.785261, mae: 29.297772, mean_q: 23.645642, mean_eps: 0.869289
  29207/200000: episode: 314, duration: 0.556s, episode steps:  95, steps per second: 171, episode reward: -108.897, mean reward: -1.146 [-100.000,  7.846], mean action: 1.526 [0.000, 3.000],  loss: 10.547672, mae: 29.632755, mean_q: 23.593145, mean_eps: 0.868785
  29334/200000: episode: 315, duration: 0.785s, episode steps: 127, steps per second: 162, episode reward: -74.285, mean reward: -0.585 [-100.000, 19.597], mean action: 1.598 [0.000, 3.000],  loss: 8.835002, mae: 29.101758, mean_q: 23.531836, mean_eps: 0.868285
  29424/200000: episode: 316, duration: 0.561s, episode steps:  90, steps per second: 160, episode reward: -87.478, mean reward: -0.972 [-100.000, 22.139], mean action: 1.656 [0.000, 3.000],  loss: 14.811692, mae: 29.869236, mean_q: 23.513609, mean_eps: 0.867797
  29541/200000: episode: 317, duration: 0.693s, episode steps: 117, steps per second: 169, episode reward: -62.188, mean reward: -0.532 [-100.000, 12.713], mean action: 1.556 [0.000, 3.000],  loss: 13.500930, mae: 29.175951, mean_q: 23.106584, mean_eps: 0.867331
  29618/200000: episode: 318, duration: 0.463s, episode steps:  77, steps per second: 166, episode reward: -99.720, mean reward: -1.295 [-100.000, 18.220], mean action: 1.675 [0.000, 3.000],  loss: 21.177378, mae: 29.346269, mean_q: 24.246994, mean_eps: 0.866894
  29748/200000: episode: 319, duration: 0.759s, episode steps: 130, steps per second: 171, episode reward: -2.773, mean reward: -0.021 [-100.000, 43.058], mean action: 1.631 [0.000, 3.000],  loss: 12.375528, mae: 29.404090, mean_q: 24.323912, mean_eps: 0.866429
  29874/200000: episode: 320, duration: 0.765s, episode steps: 126, steps per second: 165, episode reward: -157.175, mean reward: -1.247 [-100.000,  6.282], mean action: 1.571 [0.000, 3.000],  loss: 17.238767, mae: 29.161960, mean_q: 23.099938, mean_eps: 0.865853
  30006/200000: episode: 321, duration: 0.764s, episode steps: 132, steps per second: 173, episode reward: -135.427, mean reward: -1.026 [-100.000,  5.835], mean action: 1.439 [0.000, 3.000],  loss: 13.035687, mae: 29.401326, mean_q: 22.637165, mean_eps: 0.865272
  30120/200000: episode: 322, duration: 0.659s, episode steps: 114, steps per second: 173, episode reward: -112.869, mean reward: -0.990 [-100.000,  7.282], mean action: 1.518 [0.000, 3.000],  loss: 7.746158, mae: 29.816498, mean_q: 24.496695, mean_eps: 0.864719
  30242/200000: episode: 323, duration: 0.790s, episode steps: 122, steps per second: 154, episode reward: -100.702, mean reward: -0.825 [-100.000, 11.706], mean action: 1.623 [0.000, 3.000],  loss: 15.324134, mae: 30.157935, mean_q: 24.203875, mean_eps: 0.864188
  30366/200000: episode: 324, duration: 0.832s, episode steps: 124, steps per second: 149, episode reward: -50.709, mean reward: -0.409 [-100.000, 10.777], mean action: 1.685 [0.000, 3.000],  loss: 8.786329, mae: 29.817861, mean_q: 23.411230, mean_eps: 0.863634
  30432/200000: episode: 325, duration: 0.451s, episode steps:  66, steps per second: 146, episode reward: -147.894, mean reward: -2.241 [-100.000,  5.039], mean action: 1.682 [0.000, 3.000],  loss: 8.919939, mae: 30.365649, mean_q: 24.519343, mean_eps: 0.863207
  30552/200000: episode: 326, duration: 0.859s, episode steps: 120, steps per second: 140, episode reward: -143.396, mean reward: -1.195 [-100.000, 10.604], mean action: 1.492 [0.000, 3.000],  loss: 20.758787, mae: 29.979945, mean_q: 24.810783, mean_eps: 0.862788
  30663/200000: episode: 327, duration: 0.775s, episode steps: 111, steps per second: 143, episode reward: -140.353, mean reward: -1.264 [-100.000, 36.049], mean action: 1.541 [0.000, 3.000],  loss: 10.713987, mae: 30.319058, mean_q: 25.129477, mean_eps: 0.862268
  30736/200000: episode: 328, duration: 0.538s, episode steps:  73, steps per second: 136, episode reward: -91.235, mean reward: -1.250 [-100.000, 26.350], mean action: 1.521 [0.000, 3.000],  loss: 12.794437, mae: 30.025651, mean_q: 23.576091, mean_eps: 0.861854
  30829/200000: episode: 329, duration: 0.912s, episode steps:  93, steps per second: 102, episode reward: -81.175, mean reward: -0.873 [-100.000, 22.961], mean action: 1.613 [0.000, 3.000],  loss: 18.361795, mae: 29.660896, mean_q: 24.590607, mean_eps: 0.861481
  30912/200000: episode: 330, duration: 0.563s, episode steps:  83, steps per second: 147, episode reward: -44.555, mean reward: -0.537 [-100.000, 14.974], mean action: 1.494 [0.000, 3.000],  loss: 16.564185, mae: 30.256656, mean_q: 24.837069, mean_eps: 0.861085
  30976/200000: episode: 331, duration: 0.494s, episode steps:  64, steps per second: 130, episode reward: -62.911, mean reward: -0.983 [-100.000, 81.912], mean action: 1.672 [0.000, 3.000],  loss: 6.528179, mae: 29.860271, mean_q: 24.629826, mean_eps: 0.860754
  31092/200000: episode: 332, duration: 0.915s, episode steps: 116, steps per second: 127, episode reward: -94.251, mean reward: -0.813 [-100.000,  7.705], mean action: 1.560 [0.000, 3.000],  loss: 10.255869, mae: 30.328158, mean_q: 24.719577, mean_eps: 0.860349
  31163/200000: episode: 333, duration: 0.483s, episode steps:  71, steps per second: 147, episode reward: -92.016, mean reward: -1.296 [-100.000,  7.209], mean action: 1.535 [0.000, 3.000],  loss: 8.374512, mae: 30.411677, mean_q: 25.828935, mean_eps: 0.859929
  31232/200000: episode: 334, duration: 0.457s, episode steps:  69, steps per second: 151, episode reward: -68.680, mean reward: -0.995 [-100.000, 17.762], mean action: 1.478 [0.000, 3.000],  loss: 12.822567, mae: 29.950920, mean_q: 23.572675, mean_eps: 0.859614
  31333/200000: episode: 335, duration: 0.698s, episode steps: 101, steps per second: 145, episode reward: -90.600, mean reward: -0.897 [-100.000,  7.814], mean action: 1.554 [0.000, 3.000],  loss: 12.258327, mae: 29.927325, mean_q: 24.148499, mean_eps: 0.859231
  31411/200000: episode: 336, duration: 0.574s, episode steps:  78, steps per second: 136, episode reward: -48.985, mean reward: -0.628 [-100.000, 13.889], mean action: 1.538 [0.000, 3.000],  loss: 7.042584, mae: 30.087417, mean_q: 24.594149, mean_eps: 0.858828
  31482/200000: episode: 337, duration: 0.781s, episode steps:  71, steps per second:  91, episode reward: -58.962, mean reward: -0.830 [-100.000,  9.096], mean action: 1.535 [0.000, 3.000],  loss: 15.010484, mae: 30.544548, mean_q: 25.058663, mean_eps: 0.858493
  31572/200000: episode: 338, duration: 0.696s, episode steps:  90, steps per second: 129, episode reward: -80.323, mean reward: -0.892 [-100.000, 11.623], mean action: 1.567 [0.000, 3.000],  loss: 8.813093, mae: 30.728461, mean_q: 24.250522, mean_eps: 0.858131
  31671/200000: episode: 339, duration: 0.651s, episode steps:  99, steps per second: 152, episode reward: -109.211, mean reward: -1.103 [-100.000, 10.143], mean action: 1.667 [0.000, 3.000],  loss: 9.533223, mae: 30.555332, mean_q: 23.788964, mean_eps: 0.857705
  31746/200000: episode: 340, duration: 0.492s, episode steps:  75, steps per second: 152, episode reward: -149.951, mean reward: -1.999 [-100.000,  5.198], mean action: 1.427 [0.000, 3.000],  loss: 14.902359, mae: 31.267588, mean_q: 25.297481, mean_eps: 0.857314
  31820/200000: episode: 341, duration: 0.506s, episode steps:  74, steps per second: 146, episode reward: -147.474, mean reward: -1.993 [-100.000, 11.808], mean action: 1.351 [0.000, 3.000],  loss: 12.933590, mae: 30.492430, mean_q: 24.486478, mean_eps: 0.856979
  31923/200000: episode: 342, duration: 0.856s, episode steps: 103, steps per second: 120, episode reward: -32.512, mean reward: -0.316 [-100.000, 15.436], mean action: 1.524 [0.000, 3.000],  loss: 11.907104, mae: 30.559106, mean_q: 24.513184, mean_eps: 0.856581
  32040/200000: episode: 343, duration: 1.190s, episode steps: 117, steps per second:  98, episode reward: -86.715, mean reward: -0.741 [-100.000,  9.435], mean action: 1.556 [0.000, 3.000],  loss: 15.449306, mae: 31.401283, mean_q: 24.773144, mean_eps: 0.856085
  32182/200000: episode: 344, duration: 1.279s, episode steps: 142, steps per second: 111, episode reward: -117.133, mean reward: -0.825 [-100.000,  6.467], mean action: 1.697 [0.000, 3.000],  loss: 16.754183, mae: 31.457893, mean_q: 25.543847, mean_eps: 0.855503
  32262/200000: episode: 345, duration: 0.528s, episode steps:  80, steps per second: 151, episode reward: -74.592, mean reward: -0.932 [-100.000, 64.612], mean action: 1.363 [0.000, 3.000],  loss: 11.397844, mae: 31.760065, mean_q: 23.980537, mean_eps: 0.855003
  32337/200000: episode: 346, duration: 0.455s, episode steps:  75, steps per second: 165, episode reward: -72.764, mean reward: -0.970 [-100.000, 11.884], mean action: 1.587 [0.000, 3.000],  loss: 9.636776, mae: 32.009881, mean_q: 24.459972, mean_eps: 0.854654
  32406/200000: episode: 347, duration: 0.405s, episode steps:  69, steps per second: 170, episode reward: -96.517, mean reward: -1.399 [-100.000, 13.313], mean action: 1.565 [0.000, 3.000],  loss: 16.072063, mae: 31.061372, mean_q: 23.806399, mean_eps: 0.854330
  32528/200000: episode: 348, duration: 0.771s, episode steps: 122, steps per second: 158, episode reward: -81.553, mean reward: -0.668 [-100.000,  6.602], mean action: 1.541 [0.000, 3.000],  loss: 10.459688, mae: 31.841966, mean_q: 24.491168, mean_eps: 0.853901
  32614/200000: episode: 349, duration: 0.530s, episode steps:  86, steps per second: 162, episode reward: -185.061, mean reward: -2.152 [-100.000, 38.966], mean action: 1.477 [0.000, 3.000],  loss: 8.020721, mae: 31.561593, mean_q: 24.852188, mean_eps: 0.853433
  32723/200000: episode: 350, duration: 0.649s, episode steps: 109, steps per second: 168, episode reward: -186.062, mean reward: -1.707 [-100.000, 16.371], mean action: 1.486 [0.000, 3.000],  loss: 10.613012, mae: 31.630781, mean_q: 23.833503, mean_eps: 0.852994
  32817/200000: episode: 351, duration: 0.576s, episode steps:  94, steps per second: 163, episode reward: -96.448, mean reward: -1.026 [-100.000, 12.689], mean action: 1.553 [0.000, 3.000],  loss: 11.069521, mae: 31.505849, mean_q: 23.579541, mean_eps: 0.852537
  32937/200000: episode: 352, duration: 0.784s, episode steps: 120, steps per second: 153, episode reward: -172.221, mean reward: -1.435 [-100.000,  8.330], mean action: 1.550 [0.000, 3.000],  loss: 10.159582, mae: 31.421924, mean_q: 24.720769, mean_eps: 0.852056
  33060/200000: episode: 353, duration: 0.988s, episode steps: 123, steps per second: 124, episode reward: -118.193, mean reward: -0.961 [-100.000,  8.003], mean action: 1.577 [0.000, 3.000],  loss: 7.877011, mae: 31.969913, mean_q: 25.305722, mean_eps: 0.851509
  33139/200000: episode: 354, duration: 0.566s, episode steps:  79, steps per second: 140, episode reward: -43.940, mean reward: -0.556 [-100.000, 14.418], mean action: 1.316 [0.000, 3.000],  loss: 8.555551, mae: 31.323605, mean_q: 24.706643, mean_eps: 0.851055
  33230/200000: episode: 355, duration: 0.568s, episode steps:  91, steps per second: 160, episode reward: -87.728, mean reward: -0.964 [-100.000, 17.153], mean action: 1.429 [0.000, 3.000],  loss: 5.705206, mae: 32.271220, mean_q: 24.700411, mean_eps: 0.850672
  33360/200000: episode: 356, duration: 0.751s, episode steps: 130, steps per second: 173, episode reward: -52.758, mean reward: -0.406 [-100.000, 12.506], mean action: 1.508 [0.000, 3.000],  loss: 16.565770, mae: 31.869237, mean_q: 25.403564, mean_eps: 0.850175
  33466/200000: episode: 357, duration: 0.621s, episode steps: 106, steps per second: 171, episode reward: -111.567, mean reward: -1.053 [-100.000,  5.567], mean action: 1.802 [0.000, 3.000],  loss: 16.836882, mae: 31.904811, mean_q: 25.516670, mean_eps: 0.849644
  33574/200000: episode: 358, duration: 0.676s, episode steps: 108, steps per second: 160, episode reward: -55.072, mean reward: -0.510 [-100.000, 13.915], mean action: 1.444 [0.000, 3.000],  loss: 7.381646, mae: 32.016721, mean_q: 25.462848, mean_eps: 0.849162
  33649/200000: episode: 359, duration: 0.443s, episode steps:  75, steps per second: 169, episode reward: -122.414, mean reward: -1.632 [-100.000,  5.834], mean action: 1.453 [0.000, 3.000],  loss: 12.732715, mae: 32.249738, mean_q: 25.611873, mean_eps: 0.848751
  33763/200000: episode: 360, duration: 0.689s, episode steps: 114, steps per second: 165, episode reward: -224.512, mean reward: -1.969 [-100.000, 31.110], mean action: 1.711 [0.000, 3.000],  loss: 8.551255, mae: 32.549257, mean_q: 26.151146, mean_eps: 0.848325
  33865/200000: episode: 361, duration: 0.783s, episode steps: 102, steps per second: 130, episode reward: -80.478, mean reward: -0.789 [-100.000, 22.191], mean action: 1.539 [0.000, 3.000],  loss: 7.965448, mae: 32.123474, mean_q: 25.216968, mean_eps: 0.847839
  33974/200000: episode: 362, duration: 0.792s, episode steps: 109, steps per second: 138, episode reward: -130.188, mean reward: -1.194 [-100.000, 18.299], mean action: 1.404 [0.000, 3.000],  loss: 10.857203, mae: 31.850727, mean_q: 24.980860, mean_eps: 0.847365
  34078/200000: episode: 363, duration: 1.033s, episode steps: 104, steps per second: 101, episode reward: -113.645, mean reward: -1.093 [-100.000,  6.647], mean action: 1.510 [0.000, 3.000],  loss: 9.206241, mae: 31.945705, mean_q: 24.502249, mean_eps: 0.846885
  34146/200000: episode: 364, duration: 0.592s, episode steps:  68, steps per second: 115, episode reward: -109.622, mean reward: -1.612 [-100.000,  7.991], mean action: 1.647 [0.000, 3.000],  loss: 17.796482, mae: 32.218453, mean_q: 26.008779, mean_eps: 0.846498
  34223/200000: episode: 365, duration: 0.643s, episode steps:  77, steps per second: 120, episode reward: -86.391, mean reward: -1.122 [-100.000,  9.270], mean action: 1.494 [0.000, 3.000],  loss: 15.785258, mae: 31.902305, mean_q: 25.083474, mean_eps: 0.846172
  34313/200000: episode: 366, duration: 0.632s, episode steps:  90, steps per second: 142, episode reward: -162.147, mean reward: -1.802 [-100.000,  4.269], mean action: 1.633 [0.000, 3.000],  loss: 14.026673, mae: 32.373495, mean_q: 24.017030, mean_eps: 0.845796
  34375/200000: episode: 367, duration: 0.442s, episode steps:  62, steps per second: 140, episode reward: -101.003, mean reward: -1.629 [-100.000,  8.505], mean action: 1.565 [0.000, 3.000],  loss: 4.059916, mae: 32.447572, mean_q: 26.131468, mean_eps: 0.845454
  34469/200000: episode: 368, duration: 0.602s, episode steps:  94, steps per second: 156, episode reward: -114.446, mean reward: -1.218 [-100.000, 18.363], mean action: 1.585 [0.000, 3.000],  loss: 9.817068, mae: 32.106976, mean_q: 25.597763, mean_eps: 0.845103
  34594/200000: episode: 369, duration: 0.782s, episode steps: 125, steps per second: 160, episode reward: -105.940, mean reward: -0.848 [-100.000, 14.313], mean action: 1.648 [0.000, 3.000],  loss: 5.932990, mae: 31.967436, mean_q: 24.480384, mean_eps: 0.844611
  34665/200000: episode: 370, duration: 0.427s, episode steps:  71, steps per second: 166, episode reward: -62.862, mean reward: -0.885 [-100.000,  6.703], mean action: 1.634 [0.000, 3.000],  loss: 10.046685, mae: 31.999098, mean_q: 25.785682, mean_eps: 0.844170
  34786/200000: episode: 371, duration: 1.040s, episode steps: 121, steps per second: 116, episode reward: -73.052, mean reward: -0.604 [-100.000, 11.420], mean action: 1.636 [0.000, 3.000],  loss: 9.223859, mae: 32.153432, mean_q: 25.491374, mean_eps: 0.843738
  34863/200000: episode: 372, duration: 0.511s, episode steps:  77, steps per second: 151, episode reward: -49.503, mean reward: -0.643 [-100.000, 12.837], mean action: 1.610 [0.000, 3.000],  loss: 6.420376, mae: 32.393438, mean_q: 26.290864, mean_eps: 0.843292
  34949/200000: episode: 373, duration: 0.544s, episode steps:  86, steps per second: 158, episode reward: -100.430, mean reward: -1.168 [-100.000,  7.773], mean action: 1.477 [0.000, 3.000],  loss: 11.317784, mae: 32.585047, mean_q: 25.086420, mean_eps: 0.842925
  35032/200000: episode: 374, duration: 0.606s, episode steps:  83, steps per second: 137, episode reward: -33.579, mean reward: -0.405 [-100.000, 14.019], mean action: 1.494 [0.000, 3.000],  loss: 12.285531, mae: 32.395658, mean_q: 26.574011, mean_eps: 0.842545
  35142/200000: episode: 375, duration: 0.708s, episode steps: 110, steps per second: 155, episode reward: -81.646, mean reward: -0.742 [-100.000, 11.508], mean action: 1.445 [0.000, 3.000],  loss: 23.807769, mae: 32.759303, mean_q: 25.695798, mean_eps: 0.842111
  35216/200000: episode: 376, duration: 0.460s, episode steps:  74, steps per second: 161, episode reward: -94.733, mean reward: -1.280 [-100.000,  4.170], mean action: 1.554 [0.000, 3.000],  loss: 7.025146, mae: 32.535405, mean_q: 24.623500, mean_eps: 0.841697
  35279/200000: episode: 377, duration: 0.394s, episode steps:  63, steps per second: 160, episode reward: -168.327, mean reward: -2.672 [-100.000, 29.091], mean action: 1.413 [0.000, 3.000],  loss: 11.885606, mae: 32.935443, mean_q: 25.637108, mean_eps: 0.841388
  35378/200000: episode: 378, duration: 0.831s, episode steps:  99, steps per second: 119, episode reward: -152.806, mean reward: -1.543 [-100.000,  7.335], mean action: 1.596 [0.000, 3.000],  loss: 13.933035, mae: 32.953505, mean_q: 26.553163, mean_eps: 0.841024
  35500/200000: episode: 379, duration: 0.816s, episode steps: 122, steps per second: 150, episode reward: -101.679, mean reward: -0.833 [-100.000, 11.820], mean action: 1.557 [0.000, 3.000],  loss: 12.931678, mae: 32.821428, mean_q: 26.806663, mean_eps: 0.840527
  35557/200000: episode: 380, duration: 0.362s, episode steps:  57, steps per second: 158, episode reward: -106.693, mean reward: -1.872 [-100.000, 15.630], mean action: 1.439 [0.000, 3.000],  loss: 7.474858, mae: 32.991086, mean_q: 25.526121, mean_eps: 0.840124
  35629/200000: episode: 381, duration: 0.472s, episode steps:  72, steps per second: 153, episode reward: -44.121, mean reward: -0.613 [-100.000,  9.523], mean action: 1.694 [0.000, 3.000],  loss: 9.786735, mae: 32.154098, mean_q: 26.640896, mean_eps: 0.839834
  35720/200000: episode: 382, duration: 0.590s, episode steps:  91, steps per second: 154, episode reward: -101.596, mean reward: -1.116 [-100.000,  8.936], mean action: 1.549 [0.000, 3.000],  loss: 16.290269, mae: 32.815653, mean_q: 25.334871, mean_eps: 0.839467
  35824/200000: episode: 383, duration: 0.647s, episode steps: 104, steps per second: 161, episode reward: -122.162, mean reward: -1.175 [-100.000, 16.101], mean action: 1.596 [0.000, 3.000],  loss: 28.834113, mae: 33.146648, mean_q: 24.907835, mean_eps: 0.839028
  35923/200000: episode: 384, duration: 0.656s, episode steps:  99, steps per second: 151, episode reward: -317.358, mean reward: -3.206 [-100.000,  0.610], mean action: 1.727 [0.000, 3.000],  loss: 9.573077, mae: 33.194853, mean_q: 26.056284, mean_eps: 0.838572
  35993/200000: episode: 385, duration: 0.476s, episode steps:  70, steps per second: 147, episode reward: -76.472, mean reward: -1.092 [-100.000, 13.000], mean action: 1.457 [0.000, 3.000],  loss: 6.219489, mae: 32.478331, mean_q: 26.671855, mean_eps: 0.838191
  36073/200000: episode: 386, duration: 0.553s, episode steps:  80, steps per second: 145, episode reward: -109.517, mean reward: -1.369 [-100.000, 18.012], mean action: 1.512 [0.000, 3.000],  loss: 14.019261, mae: 33.875046, mean_q: 27.836956, mean_eps: 0.837854
  36195/200000: episode: 387, duration: 0.739s, episode steps: 122, steps per second: 165, episode reward: -93.054, mean reward: -0.763 [-100.000,  7.922], mean action: 1.443 [0.000, 3.000],  loss: 9.763843, mae: 33.221304, mean_q: 26.034624, mean_eps: 0.837399
  36258/200000: episode: 388, duration: 0.363s, episode steps:  63, steps per second: 174, episode reward: -79.844, mean reward: -1.267 [-100.000,  9.016], mean action: 1.397 [0.000, 3.000],  loss: 5.993661, mae: 33.809829, mean_q: 28.155181, mean_eps: 0.836983
  37258/200000: episode: 389, duration: 6.622s, episode steps: 1000, steps per second: 151, episode reward: 31.941, mean reward:  0.032 [-23.704, 131.229], mean action: 1.526 [0.000, 3.000],  loss: 12.641015, mae: 33.279630, mean_q: 25.454180, mean_eps: 0.834591
  37399/200000: episode: 390, duration: 0.857s, episode steps: 141, steps per second: 165, episode reward: -112.041, mean reward: -0.795 [-100.000, 10.214], mean action: 1.574 [0.000, 3.000],  loss: 19.737106, mae: 33.189623, mean_q: 24.737564, mean_eps: 0.832024
  37470/200000: episode: 391, duration: 0.433s, episode steps:  71, steps per second: 164, episode reward: -67.693, mean reward: -0.953 [-100.000, 29.350], mean action: 1.408 [0.000, 3.000],  loss: 10.948905, mae: 32.933729, mean_q: 25.270549, mean_eps: 0.831547
  37568/200000: episode: 392, duration: 0.626s, episode steps:  98, steps per second: 157, episode reward: -157.039, mean reward: -1.602 [-100.000,  5.588], mean action: 1.480 [0.000, 3.000],  loss: 24.296916, mae: 33.505471, mean_q: 25.713669, mean_eps: 0.831167
  37656/200000: episode: 393, duration: 0.589s, episode steps:  88, steps per second: 149, episode reward: -66.687, mean reward: -0.758 [-100.000, 12.376], mean action: 1.580 [0.000, 3.000],  loss: 24.635960, mae: 33.518286, mean_q: 24.070050, mean_eps: 0.830748
  37731/200000: episode: 394, duration: 0.507s, episode steps:  75, steps per second: 148, episode reward: -27.517, mean reward: -0.367 [-100.000, 28.650], mean action: 1.480 [0.000, 3.000],  loss: 15.522574, mae: 33.054623, mean_q: 25.834581, mean_eps: 0.830381
  37832/200000: episode: 395, duration: 0.633s, episode steps: 101, steps per second: 160, episode reward: -113.067, mean reward: -1.119 [-100.000, 16.772], mean action: 1.535 [0.000, 3.000],  loss: 10.709720, mae: 33.204179, mean_q: 23.174850, mean_eps: 0.829986
  37971/200000: episode: 396, duration: 0.909s, episode steps: 139, steps per second: 153, episode reward: -123.339, mean reward: -0.887 [-100.000,  9.773], mean action: 1.597 [0.000, 3.000],  loss: 14.584261, mae: 32.993349, mean_q: 24.950670, mean_eps: 0.829445
  38105/200000: episode: 397, duration: 0.835s, episode steps: 134, steps per second: 160, episode reward: -60.978, mean reward: -0.455 [-100.000,  9.274], mean action: 1.672 [0.000, 3.000],  loss: 10.691068, mae: 33.633076, mean_q: 24.941380, mean_eps: 0.828831
  38210/200000: episode: 398, duration: 0.663s, episode steps: 105, steps per second: 158, episode reward: -87.704, mean reward: -0.835 [-100.000, 39.555], mean action: 1.505 [0.000, 3.000],  loss: 18.557224, mae: 33.946977, mean_q: 25.267238, mean_eps: 0.828294
  38317/200000: episode: 399, duration: 0.685s, episode steps: 107, steps per second: 156, episode reward: -105.212, mean reward: -0.983 [-100.000, 17.734], mean action: 1.692 [0.000, 3.000],  loss: 18.364460, mae: 33.899623, mean_q: 25.081252, mean_eps: 0.827816
  38404/200000: episode: 400, duration: 0.566s, episode steps:  87, steps per second: 154, episode reward: -177.908, mean reward: -2.045 [-100.000,  4.666], mean action: 1.494 [0.000, 3.000],  loss: 26.174510, mae: 33.512222, mean_q: 25.286497, mean_eps: 0.827380
  38539/200000: episode: 401, duration: 0.873s, episode steps: 135, steps per second: 155, episode reward: -198.861, mean reward: -1.473 [-100.000,  4.401], mean action: 1.393 [0.000, 3.000],  loss: 11.862382, mae: 33.930411, mean_q: 25.129647, mean_eps: 0.826881
  38612/200000: episode: 402, duration: 0.433s, episode steps:  73, steps per second: 169, episode reward: -81.871, mean reward: -1.122 [-100.000, 11.402], mean action: 1.342 [0.000, 3.000],  loss: 11.790606, mae: 33.796684, mean_q: 25.582085, mean_eps: 0.826412
  38749/200000: episode: 403, duration: 0.872s, episode steps: 137, steps per second: 157, episode reward: -80.455, mean reward: -0.587 [-100.000,  6.256], mean action: 1.460 [0.000, 3.000],  loss: 17.237471, mae: 33.484229, mean_q: 24.690055, mean_eps: 0.825940
  38842/200000: episode: 404, duration: 0.559s, episode steps:  93, steps per second: 166, episode reward: -91.530, mean reward: -0.984 [-100.000, 17.221], mean action: 1.484 [0.000, 3.000],  loss: 9.648772, mae: 33.549606, mean_q: 25.074726, mean_eps: 0.825422
  38935/200000: episode: 405, duration: 0.582s, episode steps:  93, steps per second: 160, episode reward: -104.008, mean reward: -1.118 [-100.000, 11.072], mean action: 1.495 [0.000, 3.000],  loss: 12.571336, mae: 33.765751, mean_q: 24.597889, mean_eps: 0.825004
  39040/200000: episode: 406, duration: 0.682s, episode steps: 105, steps per second: 154, episode reward: -92.101, mean reward: -0.877 [-100.000, 12.868], mean action: 1.533 [0.000, 3.000],  loss: 15.797158, mae: 34.017978, mean_q: 24.732876, mean_eps: 0.824558
  39132/200000: episode: 407, duration: 0.810s, episode steps:  92, steps per second: 114, episode reward: -88.886, mean reward: -0.966 [-100.000, 13.880], mean action: 1.554 [0.000, 3.000],  loss: 10.236755, mae: 33.737481, mean_q: 24.726553, mean_eps: 0.824115
  39208/200000: episode: 408, duration: 0.543s, episode steps:  76, steps per second: 140, episode reward: -67.441, mean reward: -0.887 [-100.000, 15.135], mean action: 1.618 [0.000, 3.000],  loss: 17.359829, mae: 34.177689, mean_q: 24.631907, mean_eps: 0.823737
  39344/200000: episode: 409, duration: 0.966s, episode steps: 136, steps per second: 141, episode reward: -23.067, mean reward: -0.170 [-100.000, 11.024], mean action: 1.478 [0.000, 3.000],  loss: 8.051464, mae: 33.732207, mean_q: 25.462933, mean_eps: 0.823260
  39417/200000: episode: 410, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -90.922, mean reward: -1.246 [-100.000, 12.539], mean action: 1.767 [0.000, 3.000],  loss: 8.174956, mae: 34.032876, mean_q: 23.924066, mean_eps: 0.822790
  39499/200000: episode: 411, duration: 0.532s, episode steps:  82, steps per second: 154, episode reward: -81.849, mean reward: -0.998 [-100.000, 18.081], mean action: 1.585 [0.000, 3.000],  loss: 9.828929, mae: 34.153528, mean_q: 25.506460, mean_eps: 0.822441
  39601/200000: episode: 412, duration: 0.671s, episode steps: 102, steps per second: 152, episode reward: -86.280, mean reward: -0.846 [-100.000,  9.211], mean action: 1.480 [0.000, 3.000],  loss: 13.102652, mae: 34.338982, mean_q: 24.527029, mean_eps: 0.822027
  39726/200000: episode: 413, duration: 0.831s, episode steps: 125, steps per second: 150, episode reward: -119.740, mean reward: -0.958 [-100.000, 36.911], mean action: 1.600 [0.000, 3.000],  loss: 10.965463, mae: 33.519179, mean_q: 25.420641, mean_eps: 0.821517
  39847/200000: episode: 414, duration: 0.883s, episode steps: 121, steps per second: 137, episode reward: -194.240, mean reward: -1.605 [-100.000, 48.707], mean action: 1.529 [0.000, 3.000],  loss: 14.287512, mae: 34.061078, mean_q: 25.236196, mean_eps: 0.820963
  39956/200000: episode: 415, duration: 0.806s, episode steps: 109, steps per second: 135, episode reward: -157.322, mean reward: -1.443 [-100.000,  4.655], mean action: 1.495 [0.000, 3.000],  loss: 19.720001, mae: 33.829784, mean_q: 25.177300, mean_eps: 0.820445
  40055/200000: episode: 416, duration: 0.649s, episode steps:  99, steps per second: 153, episode reward: -149.712, mean reward: -1.512 [-100.000,  4.854], mean action: 1.465 [0.000, 3.000],  loss: 10.314696, mae: 34.181585, mean_q: 26.530662, mean_eps: 0.819978
  40165/200000: episode: 417, duration: 0.680s, episode steps: 110, steps per second: 162, episode reward: -127.782, mean reward: -1.162 [-100.000,  7.287], mean action: 1.609 [0.000, 3.000],  loss: 10.832922, mae: 33.627254, mean_q: 25.668185, mean_eps: 0.819507
  40259/200000: episode: 418, duration: 0.674s, episode steps:  94, steps per second: 140, episode reward: -227.336, mean reward: -2.418 [-100.000, 14.061], mean action: 1.681 [0.000, 3.000],  loss: 17.072546, mae: 34.221052, mean_q: 26.661845, mean_eps: 0.819048
  40400/200000: episode: 419, duration: 0.965s, episode steps: 141, steps per second: 146, episode reward: -204.111, mean reward: -1.448 [-100.000, 76.268], mean action: 1.667 [0.000, 3.000],  loss: 15.970875, mae: 33.714296, mean_q: 25.776177, mean_eps: 0.818519
  40491/200000: episode: 420, duration: 0.615s, episode steps:  91, steps per second: 148, episode reward: -110.604, mean reward: -1.215 [-100.000, 24.330], mean action: 1.648 [0.000, 3.000],  loss: 19.493577, mae: 33.706452, mean_q: 25.062087, mean_eps: 0.817998
  40629/200000: episode: 421, duration: 0.918s, episode steps: 138, steps per second: 150, episode reward: -115.458, mean reward: -0.837 [-100.000, 11.102], mean action: 1.457 [0.000, 3.000],  loss: 9.137120, mae: 33.703095, mean_q: 25.598210, mean_eps: 0.817482
  40701/200000: episode: 422, duration: 0.493s, episode steps:  72, steps per second: 146, episode reward: -23.545, mean reward: -0.327 [-100.000, 21.954], mean action: 1.597 [0.000, 3.000],  loss: 21.287588, mae: 33.997662, mean_q: 25.128935, mean_eps: 0.817010
  40793/200000: episode: 423, duration: 0.582s, episode steps:  92, steps per second: 158, episode reward: -144.290, mean reward: -1.568 [-100.000,  5.895], mean action: 1.489 [0.000, 3.000],  loss: 8.618809, mae: 33.894769, mean_q: 26.606474, mean_eps: 0.816641
  40860/200000: episode: 424, duration: 0.462s, episode steps:  67, steps per second: 145, episode reward: -70.188, mean reward: -1.048 [-100.000, 11.169], mean action: 1.582 [0.000, 3.000],  loss: 11.583213, mae: 33.946945, mean_q: 26.155946, mean_eps: 0.816283
  40937/200000: episode: 425, duration: 0.520s, episode steps:  77, steps per second: 148, episode reward: -99.904, mean reward: -1.297 [-100.000, 10.940], mean action: 1.896 [0.000, 3.000],  loss: 16.712061, mae: 33.733846, mean_q: 25.306082, mean_eps: 0.815959
  41073/200000: episode: 426, duration: 0.805s, episode steps: 136, steps per second: 169, episode reward: -101.529, mean reward: -0.747 [-100.000,  6.746], mean action: 1.676 [0.000, 3.000],  loss: 16.458374, mae: 34.311970, mean_q: 26.506420, mean_eps: 0.815480
  41150/200000: episode: 427, duration: 0.446s, episode steps:  77, steps per second: 173, episode reward: -75.207, mean reward: -0.977 [-100.000, 16.802], mean action: 1.610 [0.000, 3.000],  loss: 12.643847, mae: 34.678891, mean_q: 27.778579, mean_eps: 0.815001
  41220/200000: episode: 428, duration: 0.447s, episode steps:  70, steps per second: 157, episode reward: -104.384, mean reward: -1.491 [-100.000,  7.856], mean action: 1.429 [0.000, 3.000],  loss: 12.764277, mae: 34.379796, mean_q: 25.631372, mean_eps: 0.814670
  41306/200000: episode: 429, duration: 0.568s, episode steps:  86, steps per second: 151, episode reward: -103.268, mean reward: -1.201 [-100.000,  6.061], mean action: 1.593 [0.000, 3.000],  loss: 12.451427, mae: 34.741550, mean_q: 25.299453, mean_eps: 0.814319
  41402/200000: episode: 430, duration: 0.600s, episode steps:  96, steps per second: 160, episode reward: -95.415, mean reward: -0.994 [-100.000, 11.329], mean action: 1.323 [0.000, 3.000],  loss: 17.659252, mae: 34.114455, mean_q: 26.069823, mean_eps: 0.813909
  41473/200000: episode: 431, duration: 0.433s, episode steps:  71, steps per second: 164, episode reward: -81.865, mean reward: -1.153 [-100.000,  6.597], mean action: 1.465 [0.000, 3.000],  loss: 8.520575, mae: 34.518024, mean_q: 25.990243, mean_eps: 0.813533
  41551/200000: episode: 432, duration: 0.512s, episode steps:  78, steps per second: 152, episode reward: -72.447, mean reward: -0.929 [-100.000,  6.055], mean action: 1.538 [0.000, 3.000],  loss: 12.210290, mae: 34.551112, mean_q: 26.592681, mean_eps: 0.813198
  41656/200000: episode: 433, duration: 0.756s, episode steps: 105, steps per second: 139, episode reward: -112.450, mean reward: -1.071 [-100.000,  4.734], mean action: 1.467 [0.000, 3.000],  loss: 8.329010, mae: 35.046696, mean_q: 27.513364, mean_eps: 0.812786
  41768/200000: episode: 434, duration: 0.898s, episode steps: 112, steps per second: 125, episode reward: -130.412, mean reward: -1.164 [-100.000, 10.372], mean action: 1.482 [0.000, 3.000],  loss: 13.262325, mae: 34.180680, mean_q: 24.865985, mean_eps: 0.812298
  41829/200000: episode: 435, duration: 0.435s, episode steps:  61, steps per second: 140, episode reward: -68.347, mean reward: -1.120 [-100.000,  7.889], mean action: 1.459 [0.000, 3.000],  loss: 19.494929, mae: 35.202714, mean_q: 26.138943, mean_eps: 0.811909
  41920/200000: episode: 436, duration: 0.617s, episode steps:  91, steps per second: 147, episode reward: -77.430, mean reward: -0.851 [-100.000, 12.120], mean action: 1.648 [0.000, 3.000],  loss: 10.664031, mae: 34.470552, mean_q: 27.126476, mean_eps: 0.811567
  41993/200000: episode: 437, duration: 0.439s, episode steps:  73, steps per second: 166, episode reward: -63.648, mean reward: -0.872 [-100.000, 12.033], mean action: 1.534 [0.000, 3.000],  loss: 20.289701, mae: 35.253609, mean_q: 26.946898, mean_eps: 0.811198
  42116/200000: episode: 438, duration: 0.724s, episode steps: 123, steps per second: 170, episode reward: -63.143, mean reward: -0.513 [-100.000, 12.620], mean action: 1.317 [0.000, 3.000],  loss: 14.875657, mae: 34.858706, mean_q: 26.310311, mean_eps: 0.810757
  42179/200000: episode: 439, duration: 0.368s, episode steps:  63, steps per second: 171, episode reward: -71.930, mean reward: -1.142 [-100.000,  9.329], mean action: 1.714 [0.000, 3.000],  loss: 11.976623, mae: 34.496066, mean_q: 25.989984, mean_eps: 0.810338
  42247/200000: episode: 440, duration: 0.444s, episode steps:  68, steps per second: 153, episode reward: -79.217, mean reward: -1.165 [-100.000,  6.849], mean action: 1.456 [0.000, 3.000],  loss: 8.192394, mae: 34.214037, mean_q: 25.927238, mean_eps: 0.810044
  42362/200000: episode: 441, duration: 0.708s, episode steps: 115, steps per second: 162, episode reward: -144.156, mean reward: -1.254 [-100.000, 22.415], mean action: 1.435 [0.000, 3.000],  loss: 14.272653, mae: 34.577620, mean_q: 26.710888, mean_eps: 0.809632
  42451/200000: episode: 442, duration: 0.562s, episode steps:  89, steps per second: 158, episode reward: -83.219, mean reward: -0.935 [-100.000, 11.355], mean action: 1.416 [0.000, 3.000],  loss: 14.281650, mae: 33.611837, mean_q: 26.205856, mean_eps: 0.809173
  42564/200000: episode: 443, duration: 0.743s, episode steps: 113, steps per second: 152, episode reward: -184.229, mean reward: -1.630 [-100.000, 11.319], mean action: 1.646 [0.000, 3.000],  loss: 7.750329, mae: 34.351360, mean_q: 27.242104, mean_eps: 0.808718
  42632/200000: episode: 444, duration: 0.423s, episode steps:  68, steps per second: 161, episode reward: -115.530, mean reward: -1.699 [-100.000,  9.215], mean action: 1.676 [0.000, 3.000],  loss: 15.432815, mae: 34.237294, mean_q: 25.922331, mean_eps: 0.808311
  42737/200000: episode: 445, duration: 0.622s, episode steps: 105, steps per second: 169, episode reward: -83.439, mean reward: -0.795 [-100.000,  9.611], mean action: 1.419 [0.000, 3.000],  loss: 7.537320, mae: 34.284076, mean_q: 27.188167, mean_eps: 0.807922
  42820/200000: episode: 446, duration: 0.495s, episode steps:  83, steps per second: 168, episode reward: -186.700, mean reward: -2.249 [-100.000,  8.292], mean action: 1.723 [0.000, 3.000],  loss: 11.217289, mae: 34.319991, mean_q: 26.085115, mean_eps: 0.807499
  42933/200000: episode: 447, duration: 0.702s, episode steps: 113, steps per second: 161, episode reward: -9.767, mean reward: -0.086 [-100.000, 10.857], mean action: 1.460 [0.000, 3.000],  loss: 11.554495, mae: 34.224233, mean_q: 26.915276, mean_eps: 0.807058
  43027/200000: episode: 448, duration: 0.575s, episode steps:  94, steps per second: 164, episode reward: -52.030, mean reward: -0.554 [-100.000, 21.652], mean action: 1.638 [0.000, 3.000],  loss: 8.858457, mae: 33.614841, mean_q: 26.745819, mean_eps: 0.806592
  43088/200000: episode: 449, duration: 0.365s, episode steps:  61, steps per second: 167, episode reward: -66.021, mean reward: -1.082 [-100.000, 11.223], mean action: 1.574 [0.000, 3.000],  loss: 14.689370, mae: 34.222897, mean_q: 26.160796, mean_eps: 0.806244
  43168/200000: episode: 450, duration: 0.464s, episode steps:  80, steps per second: 172, episode reward: -75.889, mean reward: -0.949 [-100.000,  7.116], mean action: 1.363 [0.000, 3.000],  loss: 12.277467, mae: 34.043553, mean_q: 26.293500, mean_eps: 0.805926
  43253/200000: episode: 451, duration: 0.502s, episode steps:  85, steps per second: 169, episode reward: -83.970, mean reward: -0.988 [-100.000, 15.055], mean action: 1.553 [0.000, 3.000],  loss: 14.154548, mae: 34.859531, mean_q: 25.308824, mean_eps: 0.805555
  43329/200000: episode: 452, duration: 0.491s, episode steps:  76, steps per second: 155, episode reward: -62.231, mean reward: -0.819 [-100.000, 11.299], mean action: 1.566 [0.000, 3.000],  loss: 11.607730, mae: 34.668829, mean_q: 27.069084, mean_eps: 0.805193
  43442/200000: episode: 453, duration: 0.667s, episode steps: 113, steps per second: 170, episode reward: -88.703, mean reward: -0.785 [-100.000, 21.498], mean action: 1.566 [0.000, 3.000],  loss: 9.725480, mae: 34.267186, mean_q: 25.553417, mean_eps: 0.804767
  43552/200000: episode: 454, duration: 0.641s, episode steps: 110, steps per second: 172, episode reward: -136.725, mean reward: -1.243 [-100.000, 26.817], mean action: 1.445 [0.000, 3.000],  loss: 7.075181, mae: 34.281524, mean_q: 26.100666, mean_eps: 0.804266
  43626/200000: episode: 455, duration: 0.459s, episode steps:  74, steps per second: 161, episode reward: -115.738, mean reward: -1.564 [-100.000, 24.364], mean action: 1.284 [0.000, 3.000],  loss: 23.487992, mae: 34.357987, mean_q: 26.316803, mean_eps: 0.803852
  43689/200000: episode: 456, duration: 0.435s, episode steps:  63, steps per second: 145, episode reward: -125.690, mean reward: -1.995 [-100.000,  7.494], mean action: 1.683 [0.000, 3.000],  loss: 10.326258, mae: 34.781800, mean_q: 25.792752, mean_eps: 0.803544
  43748/200000: episode: 457, duration: 0.405s, episode steps:  59, steps per second: 146, episode reward: -138.140, mean reward: -2.341 [-100.000,  8.083], mean action: 1.492 [0.000, 3.000],  loss: 13.041430, mae: 34.974911, mean_q: 27.021006, mean_eps: 0.803269
  43868/200000: episode: 458, duration: 0.832s, episode steps: 120, steps per second: 144, episode reward: -107.070, mean reward: -0.892 [-100.000, 15.261], mean action: 1.300 [0.000, 3.000],  loss: 16.837629, mae: 34.273187, mean_q: 25.930107, mean_eps: 0.802866
  43962/200000: episode: 459, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -118.819, mean reward: -1.264 [-100.000,  6.730], mean action: 1.543 [0.000, 3.000],  loss: 5.156058, mae: 34.750430, mean_q: 26.392806, mean_eps: 0.802385
  44079/200000: episode: 460, duration: 0.800s, episode steps: 117, steps per second: 146, episode reward: -99.579, mean reward: -0.851 [-100.000,  5.717], mean action: 1.726 [0.000, 3.000],  loss: 18.055086, mae: 34.698404, mean_q: 26.297586, mean_eps: 0.801910
  44201/200000: episode: 461, duration: 0.787s, episode steps: 122, steps per second: 155, episode reward: -94.569, mean reward: -0.775 [-100.000, 15.515], mean action: 1.648 [0.000, 3.000],  loss: 12.223421, mae: 35.086170, mean_q: 27.412898, mean_eps: 0.801372
  44300/200000: episode: 462, duration: 0.675s, episode steps:  99, steps per second: 147, episode reward: -106.236, mean reward: -1.073 [-100.000,  7.727], mean action: 1.566 [0.000, 3.000],  loss: 10.367690, mae: 35.542629, mean_q: 26.221324, mean_eps: 0.800875
  44409/200000: episode: 463, duration: 0.683s, episode steps: 109, steps per second: 160, episode reward: -104.482, mean reward: -0.959 [-100.000,  7.948], mean action: 1.541 [0.000, 3.000],  loss: 12.693072, mae: 34.909390, mean_q: 26.430575, mean_eps: 0.800407
  44532/200000: episode: 464, duration: 0.744s, episode steps: 123, steps per second: 165, episode reward: -43.945, mean reward: -0.357 [-100.000, 11.835], mean action: 1.659 [0.000, 3.000],  loss: 19.098954, mae: 34.954237, mean_q: 25.008198, mean_eps: 0.799885
  44598/200000: episode: 465, duration: 0.396s, episode steps:  66, steps per second: 167, episode reward: -58.734, mean reward: -0.890 [-100.000,  8.902], mean action: 1.530 [0.000, 3.000],  loss: 9.590812, mae: 35.066935, mean_q: 26.667306, mean_eps: 0.799460
  44690/200000: episode: 466, duration: 0.595s, episode steps:  92, steps per second: 155, episode reward: -95.610, mean reward: -1.039 [-100.000, 15.939], mean action: 1.522 [0.000, 3.000],  loss: 14.498878, mae: 34.894518, mean_q: 25.740006, mean_eps: 0.799104
  44781/200000: episode: 467, duration: 0.544s, episode steps:  91, steps per second: 167, episode reward: -423.454, mean reward: -4.653 [-100.000, 86.316], mean action: 1.571 [0.000, 3.000],  loss: 8.469992, mae: 35.113617, mean_q: 27.313646, mean_eps: 0.798693
  44879/200000: episode: 468, duration: 0.569s, episode steps:  98, steps per second: 172, episode reward: -103.687, mean reward: -1.058 [-100.000,  7.258], mean action: 1.296 [0.000, 3.000],  loss: 21.532604, mae: 34.995606, mean_q: 24.343030, mean_eps: 0.798267
  44987/200000: episode: 469, duration: 0.684s, episode steps: 108, steps per second: 158, episode reward: -69.737, mean reward: -0.646 [-100.000,  7.516], mean action: 1.630 [0.000, 3.000],  loss: 6.424712, mae: 35.278463, mean_q: 24.886666, mean_eps: 0.797804
  45118/200000: episode: 470, duration: 0.790s, episode steps: 131, steps per second: 166, episode reward: -34.329, mean reward: -0.262 [-100.000, 45.331], mean action: 1.573 [0.000, 3.000],  loss: 11.576972, mae: 35.724450, mean_q: 26.568649, mean_eps: 0.797266
  45180/200000: episode: 471, duration: 0.367s, episode steps:  62, steps per second: 169, episode reward: -65.847, mean reward: -1.062 [-100.000,  8.721], mean action: 1.629 [0.000, 3.000],  loss: 35.264521, mae: 35.717067, mean_q: 27.009205, mean_eps: 0.796832
  45288/200000: episode: 472, duration: 0.642s, episode steps: 108, steps per second: 168, episode reward: -55.901, mean reward: -0.518 [-100.000, 13.098], mean action: 1.454 [0.000, 3.000],  loss: 10.038284, mae: 34.960694, mean_q: 26.324769, mean_eps: 0.796449
  45408/200000: episode: 473, duration: 0.767s, episode steps: 120, steps per second: 157, episode reward: -95.642, mean reward: -0.797 [-100.000, 12.431], mean action: 1.517 [0.000, 3.000],  loss: 8.682348, mae: 35.603447, mean_q: 27.428787, mean_eps: 0.795936
  45481/200000: episode: 474, duration: 0.432s, episode steps:  73, steps per second: 169, episode reward: -61.574, mean reward: -0.843 [-100.000, 12.530], mean action: 1.616 [0.000, 3.000],  loss: 6.607926, mae: 35.257611, mean_q: 26.097006, mean_eps: 0.795502
  45581/200000: episode: 475, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -121.387, mean reward: -1.214 [-100.000, 14.710], mean action: 1.490 [0.000, 3.000],  loss: 14.609908, mae: 35.263149, mean_q: 26.216437, mean_eps: 0.795113
  45665/200000: episode: 476, duration: 0.511s, episode steps:  84, steps per second: 165, episode reward: -47.785, mean reward: -0.569 [-100.000,  6.902], mean action: 1.571 [0.000, 3.000],  loss: 5.821734, mae: 35.260579, mean_q: 24.970243, mean_eps: 0.794699
  45747/200000: episode: 477, duration: 0.521s, episode steps:  82, steps per second: 157, episode reward: -83.887, mean reward: -1.023 [-100.000,  6.363], mean action: 1.598 [0.000, 3.000],  loss: 10.243786, mae: 35.583967, mean_q: 26.750674, mean_eps: 0.794325
  45857/200000: episode: 478, duration: 0.653s, episode steps: 110, steps per second: 168, episode reward: -59.527, mean reward: -0.541 [-100.000,  6.723], mean action: 1.573 [0.000, 3.000],  loss: 10.467700, mae: 35.212418, mean_q: 24.928333, mean_eps: 0.793893
  45958/200000: episode: 479, duration: 0.591s, episode steps: 101, steps per second: 171, episode reward: -77.484, mean reward: -0.767 [-100.000, 10.798], mean action: 1.505 [0.000, 3.000],  loss: 9.571817, mae: 35.426426, mean_q: 25.695771, mean_eps: 0.793418
  46084/200000: episode: 480, duration: 0.772s, episode steps: 126, steps per second: 163, episode reward: -52.934, mean reward: -0.420 [-100.000, 11.141], mean action: 1.675 [0.000, 3.000],  loss: 6.768061, mae: 35.575754, mean_q: 25.991688, mean_eps: 0.792908
  46169/200000: episode: 481, duration: 0.509s, episode steps:  85, steps per second: 167, episode reward: -80.982, mean reward: -0.953 [-100.000, 17.586], mean action: 1.635 [0.000, 3.000],  loss: 18.267108, mae: 35.726332, mean_q: 25.275479, mean_eps: 0.792433
  46258/200000: episode: 482, duration: 0.538s, episode steps:  89, steps per second: 166, episode reward: -111.201, mean reward: -1.249 [-100.000, 12.100], mean action: 1.663 [0.000, 3.000],  loss: 9.153921, mae: 35.234611, mean_q: 24.073803, mean_eps: 0.792041
  46370/200000: episode: 483, duration: 0.664s, episode steps: 112, steps per second: 169, episode reward: 16.308, mean reward:  0.146 [-100.000, 78.021], mean action: 1.500 [0.000, 3.000],  loss: 8.577442, mae: 35.898664, mean_q: 27.514449, mean_eps: 0.791589
  46441/200000: episode: 484, duration: 0.474s, episode steps:  71, steps per second: 150, episode reward: -59.830, mean reward: -0.843 [-100.000,  8.568], mean action: 1.549 [0.000, 3.000],  loss: 13.275293, mae: 35.957698, mean_q: 26.514705, mean_eps: 0.791178
  46518/200000: episode: 485, duration: 0.454s, episode steps:  77, steps per second: 169, episode reward: -31.105, mean reward: -0.404 [-100.000, 17.826], mean action: 1.740 [0.000, 3.000],  loss: 7.801067, mae: 35.906362, mean_q: 26.479625, mean_eps: 0.790844
  46610/200000: episode: 486, duration: 0.543s, episode steps:  92, steps per second: 170, episode reward: -68.587, mean reward: -0.746 [-100.000, 11.624], mean action: 1.587 [0.000, 3.000],  loss: 8.622642, mae: 35.927085, mean_q: 26.617020, mean_eps: 0.790464
  46723/200000: episode: 487, duration: 0.672s, episode steps: 113, steps per second: 168, episode reward: -63.412, mean reward: -0.561 [-100.000, 22.296], mean action: 1.522 [0.000, 3.000],  loss: 11.311272, mae: 35.901169, mean_q: 24.255850, mean_eps: 0.790003
  46815/200000: episode: 488, duration: 0.593s, episode steps:  92, steps per second: 155, episode reward: -90.983, mean reward: -0.989 [-100.000,  7.956], mean action: 1.652 [0.000, 3.000],  loss: 7.805610, mae: 35.350826, mean_q: 25.480566, mean_eps: 0.789542
  46918/200000: episode: 489, duration: 0.618s, episode steps: 103, steps per second: 167, episode reward: -102.152, mean reward: -0.992 [-100.000,  6.703], mean action: 1.641 [0.000, 3.000],  loss: 16.164180, mae: 35.469438, mean_q: 26.381852, mean_eps: 0.789103
  46979/200000: episode: 490, duration: 0.353s, episode steps:  61, steps per second: 173, episode reward: -111.392, mean reward: -1.826 [-100.000,  6.527], mean action: 1.295 [0.000, 3.000],  loss: 5.241039, mae: 35.616488, mean_q: 26.140330, mean_eps: 0.788734
  47073/200000: episode: 491, duration: 0.556s, episode steps:  94, steps per second: 169, episode reward: -115.999, mean reward: -1.234 [-100.000,  5.675], mean action: 1.553 [0.000, 3.000],  loss: 13.432542, mae: 36.005049, mean_q: 25.444848, mean_eps: 0.788385
  47171/200000: episode: 492, duration: 0.636s, episode steps:  98, steps per second: 154, episode reward: -232.345, mean reward: -2.371 [-100.000,  1.320], mean action: 1.806 [0.000, 3.000],  loss: 14.042501, mae: 36.298355, mean_q: 26.211071, mean_eps: 0.787953
  47250/200000: episode: 493, duration: 0.491s, episode steps:  79, steps per second: 161, episode reward: -82.029, mean reward: -1.038 [-100.000,  8.199], mean action: 1.620 [0.000, 3.000],  loss: 11.121620, mae: 36.607954, mean_q: 26.226845, mean_eps: 0.787555
  47320/200000: episode: 494, duration: 0.420s, episode steps:  70, steps per second: 167, episode reward: -59.496, mean reward: -0.850 [-100.000, 10.043], mean action: 1.514 [0.000, 3.000],  loss: 8.905897, mae: 35.994855, mean_q: 26.270741, mean_eps: 0.787220
  47411/200000: episode: 495, duration: 0.538s, episode steps:  91, steps per second: 169, episode reward: -107.256, mean reward: -1.179 [-100.000,  7.526], mean action: 1.527 [0.000, 3.000],  loss: 7.119135, mae: 35.698028, mean_q: 25.749848, mean_eps: 0.786857
  47541/200000: episode: 496, duration: 0.812s, episode steps: 130, steps per second: 160, episode reward: -90.409, mean reward: -0.695 [-100.000, 16.612], mean action: 1.423 [0.000, 3.000],  loss: 12.532028, mae: 36.097353, mean_q: 26.213329, mean_eps: 0.786360
  47636/200000: episode: 497, duration: 0.569s, episode steps:  95, steps per second: 167, episode reward: -136.946, mean reward: -1.442 [-100.000,  9.163], mean action: 1.758 [0.000, 3.000],  loss: 9.067194, mae: 36.480620, mean_q: 25.059172, mean_eps: 0.785854
  47723/200000: episode: 498, duration: 0.523s, episode steps:  87, steps per second: 166, episode reward: -103.543, mean reward: -1.190 [-100.000, 13.208], mean action: 1.448 [0.000, 3.000],  loss: 8.951168, mae: 36.094747, mean_q: 24.658646, mean_eps: 0.785444
  47822/200000: episode: 499, duration: 0.610s, episode steps:  99, steps per second: 162, episode reward: -106.940, mean reward: -1.080 [-100.000,  7.303], mean action: 1.505 [0.000, 3.000],  loss: 9.834869, mae: 36.161328, mean_q: 25.005026, mean_eps: 0.785026
  47914/200000: episode: 500, duration: 0.583s, episode steps:  92, steps per second: 158, episode reward: -114.270, mean reward: -1.242 [-100.000,  5.518], mean action: 1.348 [0.000, 3.000],  loss: 8.508144, mae: 36.091019, mean_q: 25.162779, mean_eps: 0.784596
  47997/200000: episode: 501, duration: 0.506s, episode steps:  83, steps per second: 164, episode reward: -28.397, mean reward: -0.342 [-100.000, 14.115], mean action: 1.663 [0.000, 3.000],  loss: 10.166876, mae: 36.437641, mean_q: 26.013460, mean_eps: 0.784203
  48064/200000: episode: 502, duration: 0.397s, episode steps:  67, steps per second: 169, episode reward: -64.033, mean reward: -0.956 [-100.000,  9.953], mean action: 1.821 [0.000, 3.000],  loss: 8.365091, mae: 36.193547, mean_q: 26.046785, mean_eps: 0.783865
  48190/200000: episode: 503, duration: 0.767s, episode steps: 126, steps per second: 164, episode reward: -64.728, mean reward: -0.514 [-100.000,  8.480], mean action: 1.484 [0.000, 3.000],  loss: 8.633429, mae: 36.804851, mean_q: 25.750349, mean_eps: 0.783431
  48285/200000: episode: 504, duration: 0.581s, episode steps:  95, steps per second: 163, episode reward: -50.719, mean reward: -0.534 [-100.000, 115.474], mean action: 1.484 [0.000, 3.000],  loss: 15.882731, mae: 36.538025, mean_q: 25.506162, mean_eps: 0.782934
  48418/200000: episode: 505, duration: 0.786s, episode steps: 133, steps per second: 169, episode reward: -240.117, mean reward: -1.805 [-100.000, 64.108], mean action: 1.541 [0.000, 3.000],  loss: 16.715025, mae: 36.782554, mean_q: 26.005502, mean_eps: 0.782420
  48525/200000: episode: 506, duration: 0.655s, episode steps: 107, steps per second: 163, episode reward: -167.521, mean reward: -1.566 [-100.000,  2.690], mean action: 1.514 [0.000, 3.000],  loss: 5.850132, mae: 36.671577, mean_q: 25.308322, mean_eps: 0.781880
  48616/200000: episode: 507, duration: 0.573s, episode steps:  91, steps per second: 159, episode reward: -17.383, mean reward: -0.191 [-100.000, 56.673], mean action: 1.681 [0.000, 3.000],  loss: 11.473057, mae: 35.991407, mean_q: 26.045026, mean_eps: 0.781435
  48713/200000: episode: 508, duration: 0.587s, episode steps:  97, steps per second: 165, episode reward: -76.040, mean reward: -0.784 [-100.000,  7.067], mean action: 1.423 [0.000, 3.000],  loss: 12.866666, mae: 35.779663, mean_q: 24.579125, mean_eps: 0.781012
  48784/200000: episode: 509, duration: 0.422s, episode steps:  71, steps per second: 168, episode reward: -107.050, mean reward: -1.508 [-100.000,  6.819], mean action: 1.563 [0.000, 3.000],  loss: 6.821346, mae: 36.360564, mean_q: 26.662374, mean_eps: 0.780634
  48885/200000: episode: 510, duration: 0.646s, episode steps: 101, steps per second: 156, episode reward: -84.936, mean reward: -0.841 [-100.000,  6.704], mean action: 1.455 [0.000, 3.000],  loss: 9.747528, mae: 36.110737, mean_q: 26.004098, mean_eps: 0.780247
  48947/200000: episode: 511, duration: 0.420s, episode steps:  62, steps per second: 147, episode reward: -78.264, mean reward: -1.262 [-100.000, 12.837], mean action: 1.581 [0.000, 3.000],  loss: 8.748904, mae: 36.488105, mean_q: 26.746113, mean_eps: 0.779880
  49036/200000: episode: 512, duration: 0.601s, episode steps:  89, steps per second: 148, episode reward: -115.469, mean reward: -1.297 [-100.000,  5.455], mean action: 1.764 [0.000, 3.000],  loss: 10.142052, mae: 36.786327, mean_q: 25.868740, mean_eps: 0.779540
  49106/200000: episode: 513, duration: 0.516s, episode steps:  70, steps per second: 136, episode reward: -42.322, mean reward: -0.605 [-100.000,  8.527], mean action: 1.543 [0.000, 3.000],  loss: 6.875209, mae: 36.253929, mean_q: 26.614949, mean_eps: 0.779183
  49167/200000: episode: 514, duration: 0.453s, episode steps:  61, steps per second: 135, episode reward: -88.892, mean reward: -1.457 [-100.000, 15.349], mean action: 1.557 [0.000, 3.000],  loss: 14.352768, mae: 36.271899, mean_q: 26.337839, mean_eps: 0.778888
  49279/200000: episode: 515, duration: 0.767s, episode steps: 112, steps per second: 146, episode reward: -99.761, mean reward: -0.891 [-100.000, 13.025], mean action: 1.420 [0.000, 3.000],  loss: 10.778165, mae: 36.112807, mean_q: 26.600419, mean_eps: 0.778499
  49358/200000: episode: 516, duration: 0.502s, episode steps:  79, steps per second: 157, episode reward: -36.772, mean reward: -0.465 [-100.000, 18.261], mean action: 1.519 [0.000, 3.000],  loss: 22.680014, mae: 36.284802, mean_q: 26.632790, mean_eps: 0.778069
  49441/200000: episode: 517, duration: 0.493s, episode steps:  83, steps per second: 168, episode reward: -52.078, mean reward: -0.627 [-100.000, 10.067], mean action: 1.494 [0.000, 3.000],  loss: 9.323911, mae: 35.446673, mean_q: 26.570100, mean_eps: 0.777704
  49515/200000: episode: 518, duration: 0.489s, episode steps:  74, steps per second: 151, episode reward: -59.028, mean reward: -0.798 [-100.000, 15.227], mean action: 1.554 [0.000, 3.000],  loss: 12.586056, mae: 35.174599, mean_q: 25.159606, mean_eps: 0.777351
  49591/200000: episode: 519, duration: 0.500s, episode steps:  76, steps per second: 152, episode reward: -79.419, mean reward: -1.045 [-100.000,  6.501], mean action: 1.592 [0.000, 3.000],  loss: 17.910285, mae: 36.178718, mean_q: 26.861774, mean_eps: 0.777014
  49687/200000: episode: 520, duration: 0.595s, episode steps:  96, steps per second: 161, episode reward: -39.393, mean reward: -0.410 [-100.000, 19.092], mean action: 1.594 [0.000, 3.000],  loss: 8.439787, mae: 36.220437, mean_q: 25.870133, mean_eps: 0.776627
  49775/200000: episode: 521, duration: 0.557s, episode steps:  88, steps per second: 158, episode reward: -107.508, mean reward: -1.222 [-100.000,  8.848], mean action: 1.284 [0.000, 3.000],  loss: 6.009681, mae: 36.086078, mean_q: 27.002007, mean_eps: 0.776213
  49858/200000: episode: 522, duration: 0.553s, episode steps:  83, steps per second: 150, episode reward: -89.957, mean reward: -1.084 [-100.000,  7.623], mean action: 1.518 [0.000, 3.000],  loss: 18.350835, mae: 36.285118, mean_q: 25.680097, mean_eps: 0.775828
  49940/200000: episode: 523, duration: 0.539s, episode steps:  82, steps per second: 152, episode reward: -108.666, mean reward: -1.325 [-100.000, 32.362], mean action: 1.598 [0.000, 3.000],  loss: 7.635059, mae: 36.189501, mean_q: 25.814444, mean_eps: 0.775457
  50012/200000: episode: 524, duration: 0.476s, episode steps:  72, steps per second: 151, episode reward: -115.060, mean reward: -1.598 [-100.000,  5.489], mean action: 1.625 [0.000, 3.000],  loss: 18.394886, mae: 35.915979, mean_q: 25.579678, mean_eps: 0.775110
  50109/200000: episode: 525, duration: 0.635s, episode steps:  97, steps per second: 153, episode reward: -151.521, mean reward: -1.562 [-100.000, 21.612], mean action: 1.794 [0.000, 3.000],  loss: 8.403198, mae: 36.071526, mean_q: 27.415531, mean_eps: 0.774730
  50178/200000: episode: 526, duration: 0.496s, episode steps:  69, steps per second: 139, episode reward: -60.850, mean reward: -0.882 [-100.000,  6.519], mean action: 1.580 [0.000, 3.000],  loss: 5.411179, mae: 36.570827, mean_q: 29.741823, mean_eps: 0.774357
  50260/200000: episode: 527, duration: 0.571s, episode steps:  82, steps per second: 144, episode reward: -63.631, mean reward: -0.776 [-100.000, 15.075], mean action: 1.671 [0.000, 3.000],  loss: 12.685932, mae: 36.895796, mean_q: 27.708301, mean_eps: 0.774017
  50388/200000: episode: 528, duration: 0.884s, episode steps: 128, steps per second: 145, episode reward: -27.761, mean reward: -0.217 [-100.000, 18.098], mean action: 1.492 [0.000, 3.000],  loss: 15.450386, mae: 36.815077, mean_q: 27.728511, mean_eps: 0.773544
  50520/200000: episode: 529, duration: 0.952s, episode steps: 132, steps per second: 139, episode reward: -35.467, mean reward: -0.269 [-100.000, 10.637], mean action: 1.523 [0.000, 3.000],  loss: 19.644503, mae: 36.360085, mean_q: 28.632832, mean_eps: 0.772959
  50611/200000: episode: 530, duration: 0.598s, episode steps:  91, steps per second: 152, episode reward: -72.443, mean reward: -0.796 [-100.000, 12.150], mean action: 1.703 [0.000, 3.000],  loss: 11.832255, mae: 36.468090, mean_q: 26.251562, mean_eps: 0.772458
  50694/200000: episode: 531, duration: 0.544s, episode steps:  83, steps per second: 153, episode reward: -18.470, mean reward: -0.223 [-100.000, 11.277], mean action: 1.759 [0.000, 3.000],  loss: 16.344803, mae: 37.464158, mean_q: 28.804973, mean_eps: 0.772066
  50805/200000: episode: 532, duration: 0.717s, episode steps: 111, steps per second: 155, episode reward: -73.824, mean reward: -0.665 [-100.000, 12.318], mean action: 1.721 [0.000, 3.000],  loss: 8.636963, mae: 37.253034, mean_q: 29.139743, mean_eps: 0.771629
  50892/200000: episode: 533, duration: 0.549s, episode steps:  87, steps per second: 158, episode reward: -140.107, mean reward: -1.610 [-100.000, 12.637], mean action: 1.598 [0.000, 3.000],  loss: 20.606191, mae: 36.879454, mean_q: 26.985775, mean_eps: 0.771184
  51025/200000: episode: 534, duration: 0.818s, episode steps: 133, steps per second: 163, episode reward: -102.352, mean reward: -0.770 [-100.000,  6.228], mean action: 1.549 [0.000, 3.000],  loss: 7.726046, mae: 36.853052, mean_q: 28.479674, mean_eps: 0.770689
  51118/200000: episode: 535, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -98.145, mean reward: -1.055 [-100.000, 11.417], mean action: 1.527 [0.000, 3.000],  loss: 11.233696, mae: 36.740417, mean_q: 29.087940, mean_eps: 0.770181
  51227/200000: episode: 536, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -30.626, mean reward: -0.281 [-100.000, 11.296], mean action: 1.716 [0.000, 3.000],  loss: 11.069765, mae: 36.312084, mean_q: 29.369515, mean_eps: 0.769726
  51316/200000: episode: 537, duration: 0.609s, episode steps:  89, steps per second: 146, episode reward: -127.852, mean reward: -1.437 [-100.000,  5.521], mean action: 1.618 [0.000, 3.000],  loss: 12.966873, mae: 36.194864, mean_q: 28.769765, mean_eps: 0.769281
  51432/200000: episode: 538, duration: 0.713s, episode steps: 116, steps per second: 163, episode reward: -47.904, mean reward: -0.413 [-100.000, 11.860], mean action: 1.491 [0.000, 3.000],  loss: 15.385686, mae: 36.673296, mean_q: 28.751133, mean_eps: 0.768819
  51510/200000: episode: 539, duration: 0.612s, episode steps:  78, steps per second: 127, episode reward: -84.007, mean reward: -1.077 [-100.000,  4.999], mean action: 1.603 [0.000, 3.000],  loss: 8.948608, mae: 36.212553, mean_q: 28.814285, mean_eps: 0.768383
  51605/200000: episode: 540, duration: 0.595s, episode steps:  95, steps per second: 160, episode reward: -36.843, mean reward: -0.388 [-100.000, 17.282], mean action: 1.495 [0.000, 3.000],  loss: 8.738602, mae: 36.565968, mean_q: 27.653601, mean_eps: 0.767993
  51683/200000: episode: 541, duration: 0.463s, episode steps:  78, steps per second: 169, episode reward: -84.740, mean reward: -1.086 [-100.000,  6.635], mean action: 1.244 [0.000, 3.000],  loss: 7.791871, mae: 36.228892, mean_q: 29.715961, mean_eps: 0.767604
  51810/200000: episode: 542, duration: 0.763s, episode steps: 127, steps per second: 166, episode reward: -125.949, mean reward: -0.992 [-100.000, 17.403], mean action: 1.661 [0.000, 3.000],  loss: 9.831809, mae: 36.587448, mean_q: 27.920856, mean_eps: 0.767143
  51901/200000: episode: 543, duration: 0.556s, episode steps:  91, steps per second: 164, episode reward: -72.506, mean reward: -0.797 [-100.000,  9.221], mean action: 1.648 [0.000, 3.000],  loss: 10.537416, mae: 36.120314, mean_q: 28.434636, mean_eps: 0.766652
  51969/200000: episode: 544, duration: 0.413s, episode steps:  68, steps per second: 165, episode reward: -82.800, mean reward: -1.218 [-100.000,  6.960], mean action: 1.838 [0.000, 3.000],  loss: 9.156953, mae: 36.721545, mean_q: 29.998902, mean_eps: 0.766295
  52056/200000: episode: 545, duration: 0.514s, episode steps:  87, steps per second: 169, episode reward: -136.306, mean reward: -1.567 [-100.000, 54.335], mean action: 1.471 [0.000, 3.000],  loss: 15.744933, mae: 35.918876, mean_q: 28.578233, mean_eps: 0.765946
  52153/200000: episode: 546, duration: 0.569s, episode steps:  97, steps per second: 170, episode reward: -108.959, mean reward: -1.123 [-100.000, 18.420], mean action: 1.526 [0.000, 3.000],  loss: 8.269701, mae: 35.147761, mean_q: 27.726757, mean_eps: 0.765532
  52266/200000: episode: 547, duration: 0.712s, episode steps: 113, steps per second: 159, episode reward: -86.305, mean reward: -0.764 [-100.000, 10.188], mean action: 1.558 [0.000, 3.000],  loss: 5.423688, mae: 35.770839, mean_q: 29.091776, mean_eps: 0.765060
  52371/200000: episode: 548, duration: 0.627s, episode steps: 105, steps per second: 168, episode reward: 16.844, mean reward:  0.160 [-100.000, 110.926], mean action: 1.543 [0.000, 3.000],  loss: 9.600974, mae: 34.942375, mean_q: 29.310524, mean_eps: 0.764569
  52481/200000: episode: 549, duration: 0.658s, episode steps: 110, steps per second: 167, episode reward: -79.545, mean reward: -0.723 [-100.000,  8.439], mean action: 1.436 [0.000, 3.000],  loss: 31.495490, mae: 35.659611, mean_q: 28.645698, mean_eps: 0.764085
  52569/200000: episode: 550, duration: 0.568s, episode steps:  88, steps per second: 155, episode reward: -112.640, mean reward: -1.280 [-100.000, 11.970], mean action: 1.568 [0.000, 3.000],  loss: 5.231421, mae: 35.191259, mean_q: 31.244436, mean_eps: 0.763640
  52658/200000: episode: 551, duration: 0.527s, episode steps:  89, steps per second: 169, episode reward: -61.408, mean reward: -0.690 [-100.000, 15.939], mean action: 1.427 [0.000, 3.000],  loss: 8.226459, mae: 35.699179, mean_q: 28.974596, mean_eps: 0.763241
  52752/200000: episode: 552, duration: 0.554s, episode steps:  94, steps per second: 170, episode reward: -100.479, mean reward: -1.069 [-100.000,  8.252], mean action: 1.553 [0.000, 3.000],  loss: 19.515209, mae: 35.401902, mean_q: 28.951762, mean_eps: 0.762830
  52849/200000: episode: 553, duration: 0.568s, episode steps:  97, steps per second: 171, episode reward: -102.389, mean reward: -1.056 [-100.000,  6.220], mean action: 1.608 [0.000, 3.000],  loss: 7.800655, mae: 35.533296, mean_q: 28.371366, mean_eps: 0.762400
  52952/200000: episode: 554, duration: 0.655s, episode steps: 103, steps per second: 157, episode reward: -78.212, mean reward: -0.759 [-100.000,  7.255], mean action: 1.524 [0.000, 3.000],  loss: 7.626336, mae: 35.092802, mean_q: 29.051939, mean_eps: 0.761950
  53064/200000: episode: 555, duration: 0.670s, episode steps: 112, steps per second: 167, episode reward: -60.025, mean reward: -0.536 [-100.000, 13.820], mean action: 1.518 [0.000, 3.000],  loss: 17.183030, mae: 35.348393, mean_q: 29.073769, mean_eps: 0.761466
  53165/200000: episode: 556, duration: 0.590s, episode steps: 101, steps per second: 171, episode reward: -310.716, mean reward: -3.076 [-100.000, 60.319], mean action: 1.475 [0.000, 3.000],  loss: 9.088537, mae: 35.824686, mean_q: 28.602810, mean_eps: 0.760987
  53265/200000: episode: 557, duration: 0.631s, episode steps: 100, steps per second: 158, episode reward: -41.213, mean reward: -0.412 [-100.000, 13.501], mean action: 1.570 [0.000, 3.000],  loss: 8.002042, mae: 36.012950, mean_q: 27.528637, mean_eps: 0.760535
  53363/200000: episode: 558, duration: 0.603s, episode steps:  98, steps per second: 163, episode reward: -64.705, mean reward: -0.660 [-100.000, 13.609], mean action: 1.622 [0.000, 3.000],  loss: 13.440953, mae: 35.642536, mean_q: 29.399686, mean_eps: 0.760089
  53447/200000: episode: 559, duration: 0.503s, episode steps:  84, steps per second: 167, episode reward: -83.630, mean reward: -0.996 [-100.000, 11.529], mean action: 1.643 [0.000, 3.000],  loss: 17.536339, mae: 36.187487, mean_q: 28.513467, mean_eps: 0.759680
  53547/200000: episode: 560, duration: 0.592s, episode steps: 100, steps per second: 169, episode reward: -84.859, mean reward: -0.849 [-100.000, 11.200], mean action: 1.820 [0.000, 3.000],  loss: 10.022218, mae: 35.442407, mean_q: 28.460620, mean_eps: 0.759266
  53670/200000: episode: 561, duration: 0.771s, episode steps: 123, steps per second: 160, episode reward: -80.326, mean reward: -0.653 [-100.000,  8.027], mean action: 1.520 [0.000, 3.000],  loss: 12.498407, mae: 35.631158, mean_q: 29.384396, mean_eps: 0.758764
  53772/200000: episode: 562, duration: 0.607s, episode steps: 102, steps per second: 168, episode reward: -68.545, mean reward: -0.672 [-100.000, 11.255], mean action: 1.676 [0.000, 3.000],  loss: 12.961362, mae: 35.748456, mean_q: 27.385569, mean_eps: 0.758258
  53843/200000: episode: 563, duration: 0.419s, episode steps:  71, steps per second: 170, episode reward: -111.225, mean reward: -1.567 [-100.000,  6.475], mean action: 1.535 [0.000, 3.000],  loss: 11.473302, mae: 35.103143, mean_q: 28.907725, mean_eps: 0.757869
  53931/200000: episode: 564, duration: 0.531s, episode steps:  88, steps per second: 166, episode reward: -77.375, mean reward: -0.879 [-100.000, 12.203], mean action: 1.739 [0.000, 3.000],  loss: 13.757567, mae: 35.344206, mean_q: 27.726056, mean_eps: 0.757511
  54052/200000: episode: 565, duration: 0.767s, episode steps: 121, steps per second: 158, episode reward: -16.456, mean reward: -0.136 [-100.000, 42.095], mean action: 1.736 [0.000, 3.000],  loss: 8.584493, mae: 35.625367, mean_q: 29.086625, mean_eps: 0.757041
  54169/200000: episode: 566, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -78.088, mean reward: -0.667 [-100.000, 10.971], mean action: 1.829 [0.000, 3.000],  loss: 4.710001, mae: 35.910018, mean_q: 28.554537, mean_eps: 0.756505
  54233/200000: episode: 567, duration: 0.462s, episode steps:  64, steps per second: 138, episode reward: -86.235, mean reward: -1.347 [-100.000,  9.754], mean action: 1.547 [0.000, 3.000],  loss: 5.631851, mae: 35.854525, mean_q: 29.699362, mean_eps: 0.756098
  54302/200000: episode: 568, duration: 0.514s, episode steps:  69, steps per second: 134, episode reward: -29.254, mean reward: -0.424 [-100.000, 11.033], mean action: 1.391 [0.000, 3.000],  loss: 12.231201, mae: 36.081503, mean_q: 28.121738, mean_eps: 0.755799
  54367/200000: episode: 569, duration: 0.452s, episode steps:  65, steps per second: 144, episode reward: -83.317, mean reward: -1.282 [-100.000, 10.733], mean action: 1.400 [0.000, 3.000],  loss: 9.109046, mae: 36.542627, mean_q: 28.591922, mean_eps: 0.755497
  54463/200000: episode: 570, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -136.066, mean reward: -1.417 [-100.000, 11.275], mean action: 1.656 [0.000, 3.000],  loss: 6.162320, mae: 35.793099, mean_q: 28.858288, mean_eps: 0.755135
  54537/200000: episode: 571, duration: 0.489s, episode steps:  74, steps per second: 151, episode reward: -45.582, mean reward: -0.616 [-100.000, 13.506], mean action: 1.649 [0.000, 3.000],  loss: 7.758059, mae: 35.368899, mean_q: 28.983070, mean_eps: 0.754752
  54633/200000: episode: 572, duration: 0.653s, episode steps:  96, steps per second: 147, episode reward: -8.894, mean reward: -0.093 [-100.000, 12.099], mean action: 1.708 [0.000, 3.000],  loss: 5.664346, mae: 35.378212, mean_q: 28.454401, mean_eps: 0.754370
  54768/200000: episode: 573, duration: 0.872s, episode steps: 135, steps per second: 155, episode reward: -148.906, mean reward: -1.103 [-100.000,  7.453], mean action: 1.741 [0.000, 3.000],  loss: 18.669171, mae: 35.840327, mean_q: 28.683505, mean_eps: 0.753850
  54872/200000: episode: 574, duration: 0.648s, episode steps: 104, steps per second: 160, episode reward: -79.384, mean reward: -0.763 [-100.000,  9.237], mean action: 1.481 [0.000, 3.000],  loss: 8.811023, mae: 36.224542, mean_q: 29.659933, mean_eps: 0.753312
  54973/200000: episode: 575, duration: 0.680s, episode steps: 101, steps per second: 149, episode reward: -102.373, mean reward: -1.014 [-100.000,  7.245], mean action: 1.475 [0.000, 3.000],  loss: 7.455984, mae: 36.188397, mean_q: 28.200714, mean_eps: 0.752851
  55059/200000: episode: 576, duration: 0.513s, episode steps:  86, steps per second: 168, episode reward: -40.597, mean reward: -0.472 [-100.000, 17.645], mean action: 1.488 [0.000, 3.000],  loss: 5.978025, mae: 36.483294, mean_q: 29.531335, mean_eps: 0.752430
  55125/200000: episode: 577, duration: 0.393s, episode steps:  66, steps per second: 168, episode reward: -90.651, mean reward: -1.373 [-100.000,  9.685], mean action: 1.561 [0.000, 3.000],  loss: 9.996295, mae: 36.171083, mean_q: 27.470285, mean_eps: 0.752088
  55267/200000: episode: 578, duration: 0.880s, episode steps: 142, steps per second: 161, episode reward: -9.990, mean reward: -0.070 [-100.000, 26.935], mean action: 1.613 [0.000, 3.000],  loss: 13.028046, mae: 35.966605, mean_q: 29.171152, mean_eps: 0.751620
  55383/200000: episode: 579, duration: 0.881s, episode steps: 116, steps per second: 132, episode reward: -102.961, mean reward: -0.888 [-100.000, 12.383], mean action: 1.698 [0.000, 3.000],  loss: 12.308784, mae: 35.827182, mean_q: 29.452538, mean_eps: 0.751040
  55447/200000: episode: 580, duration: 0.498s, episode steps:  64, steps per second: 129, episode reward: -46.165, mean reward: -0.721 [-100.000, 13.283], mean action: 1.672 [0.000, 3.000],  loss: 7.405331, mae: 35.750477, mean_q: 28.480947, mean_eps: 0.750635
  55527/200000: episode: 581, duration: 0.540s, episode steps:  80, steps per second: 148, episode reward: -87.190, mean reward: -1.090 [-100.000, 16.929], mean action: 1.775 [0.000, 3.000],  loss: 7.200710, mae: 36.114526, mean_q: 29.221962, mean_eps: 0.750311
  55649/200000: episode: 582, duration: 0.863s, episode steps: 122, steps per second: 141, episode reward: -183.475, mean reward: -1.504 [-100.000, 46.913], mean action: 1.664 [0.000, 3.000],  loss: 13.830947, mae: 36.102006, mean_q: 28.748025, mean_eps: 0.749856
  55733/200000: episode: 583, duration: 0.557s, episode steps:  84, steps per second: 151, episode reward: -82.472, mean reward: -0.982 [-100.000,  5.688], mean action: 1.488 [0.000, 3.000],  loss: 7.007031, mae: 36.140808, mean_q: 29.657568, mean_eps: 0.749393
  55849/200000: episode: 584, duration: 0.747s, episode steps: 116, steps per second: 155, episode reward: -47.939, mean reward: -0.413 [-100.000, 18.339], mean action: 1.595 [0.000, 3.000],  loss: 7.631495, mae: 35.876772, mean_q: 29.622927, mean_eps: 0.748943
  55926/200000: episode: 585, duration: 0.686s, episode steps:  77, steps per second: 112, episode reward: -51.222, mean reward: -0.665 [-100.000,  8.254], mean action: 1.584 [0.000, 3.000],  loss: 6.804704, mae: 35.927185, mean_q: 28.757074, mean_eps: 0.748509
  56038/200000: episode: 586, duration: 0.811s, episode steps: 112, steps per second: 138, episode reward: -178.259, mean reward: -1.592 [-100.000,  2.501], mean action: 1.598 [0.000, 3.000],  loss: 6.007556, mae: 36.288727, mean_q: 28.876696, mean_eps: 0.748083
  56145/200000: episode: 587, duration: 0.664s, episode steps: 107, steps per second: 161, episode reward: -62.138, mean reward: -0.581 [-100.000, 16.185], mean action: 1.533 [0.000, 3.000],  loss: 7.074053, mae: 36.377519, mean_q: 30.313922, mean_eps: 0.747590
  56247/200000: episode: 588, duration: 0.697s, episode steps: 102, steps per second: 146, episode reward: -73.857, mean reward: -0.724 [-100.000, 11.552], mean action: 1.431 [0.000, 3.000],  loss: 9.217982, mae: 36.128881, mean_q: 29.857369, mean_eps: 0.747120
  56363/200000: episode: 589, duration: 0.805s, episode steps: 116, steps per second: 144, episode reward: -126.716, mean reward: -1.092 [-100.000,  7.222], mean action: 1.224 [0.000, 3.000],  loss: 7.746278, mae: 36.183322, mean_q: 28.924100, mean_eps: 0.746630
  56428/200000: episode: 590, duration: 0.498s, episode steps:  65, steps per second: 131, episode reward: -53.970, mean reward: -0.830 [-100.000,  7.428], mean action: 1.815 [0.000, 3.000],  loss: 3.463098, mae: 36.799223, mean_q: 31.478412, mean_eps: 0.746223
  56532/200000: episode: 591, duration: 0.924s, episode steps: 104, steps per second: 113, episode reward: -84.522, mean reward: -0.813 [-100.000, 13.158], mean action: 1.606 [0.000, 3.000],  loss: 20.535101, mae: 36.824637, mean_q: 30.929935, mean_eps: 0.745842
  56658/200000: episode: 592, duration: 0.987s, episode steps: 126, steps per second: 128, episode reward: -68.599, mean reward: -0.544 [-100.000, 10.059], mean action: 1.611 [0.000, 3.000],  loss: 9.648913, mae: 36.853516, mean_q: 30.019173, mean_eps: 0.745325
  56774/200000: episode: 593, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -84.476, mean reward: -0.728 [-100.000,  7.222], mean action: 1.422 [0.000, 3.000],  loss: 9.589900, mae: 36.414886, mean_q: 29.570387, mean_eps: 0.744780
  56858/200000: episode: 594, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -103.309, mean reward: -1.230 [-100.000, 14.175], mean action: 1.702 [0.000, 3.000],  loss: 12.253893, mae: 36.183362, mean_q: 30.037643, mean_eps: 0.744330
  56964/200000: episode: 595, duration: 0.731s, episode steps: 106, steps per second: 145, episode reward: -75.849, mean reward: -0.716 [-100.000, 22.565], mean action: 1.755 [0.000, 3.000],  loss: 16.192023, mae: 36.008572, mean_q: 28.915989, mean_eps: 0.743903
  57036/200000: episode: 596, duration: 0.478s, episode steps:  72, steps per second: 151, episode reward: -28.769, mean reward: -0.400 [-100.000, 11.418], mean action: 1.611 [0.000, 3.000],  loss: 17.615989, mae: 36.260163, mean_q: 29.125600, mean_eps: 0.743502
  57130/200000: episode: 597, duration: 0.676s, episode steps:  94, steps per second: 139, episode reward: -111.775, mean reward: -1.189 [-100.000,  6.620], mean action: 1.553 [0.000, 3.000],  loss: 9.769612, mae: 36.933579, mean_q: 30.870045, mean_eps: 0.743129
  57239/200000: episode: 598, duration: 0.739s, episode steps: 109, steps per second: 147, episode reward: -164.059, mean reward: -1.505 [-100.000,  7.471], mean action: 1.697 [0.000, 3.000],  loss: 7.356176, mae: 36.568854, mean_q: 29.541501, mean_eps: 0.742672
  57328/200000: episode: 599, duration: 0.577s, episode steps:  89, steps per second: 154, episode reward: -88.567, mean reward: -0.995 [-100.000, 12.850], mean action: 1.416 [0.000, 3.000],  loss: 5.174734, mae: 36.518396, mean_q: 30.015283, mean_eps: 0.742227
  57392/200000: episode: 600, duration: 0.420s, episode steps:  64, steps per second: 152, episode reward: -46.729, mean reward: -0.730 [-100.000, 22.811], mean action: 1.734 [0.000, 3.000],  loss: 7.009052, mae: 37.363199, mean_q: 32.520199, mean_eps: 0.741882
  57472/200000: episode: 601, duration: 0.575s, episode steps:  80, steps per second: 139, episode reward: -29.037, mean reward: -0.363 [-100.000, 11.582], mean action: 1.637 [0.000, 3.000],  loss: 8.418550, mae: 37.520761, mean_q: 29.794443, mean_eps: 0.741558
  57538/200000: episode: 602, duration: 0.407s, episode steps:  66, steps per second: 162, episode reward: -88.266, mean reward: -1.337 [-100.000,  5.900], mean action: 1.545 [0.000, 3.000],  loss: 6.684379, mae: 36.634207, mean_q: 31.393915, mean_eps: 0.741230
  57644/200000: episode: 603, duration: 0.673s, episode steps: 106, steps per second: 158, episode reward: -90.085, mean reward: -0.850 [-100.000, 17.941], mean action: 1.660 [0.000, 3.000],  loss: 4.456043, mae: 36.876290, mean_q: 30.814176, mean_eps: 0.740843
  57728/200000: episode: 604, duration: 0.606s, episode steps:  84, steps per second: 139, episode reward: -40.950, mean reward: -0.488 [-100.000, 11.597], mean action: 1.524 [0.000, 3.000],  loss: 6.967615, mae: 36.278585, mean_q: 28.673172, mean_eps: 0.740415
  57839/200000: episode: 605, duration: 0.761s, episode steps: 111, steps per second: 146, episode reward: -82.115, mean reward: -0.740 [-100.000,  7.890], mean action: 1.595 [0.000, 3.000],  loss: 11.284596, mae: 36.693680, mean_q: 30.310770, mean_eps: 0.739976
  57958/200000: episode: 606, duration: 0.732s, episode steps: 119, steps per second: 163, episode reward: -92.768, mean reward: -0.780 [-100.000, 12.049], mean action: 1.487 [0.000, 3.000],  loss: 5.580737, mae: 37.636800, mean_q: 30.937775, mean_eps: 0.739459
  58054/200000: episode: 607, duration: 0.584s, episode steps:  96, steps per second: 164, episode reward: -117.287, mean reward: -1.222 [-100.000,  8.649], mean action: 1.646 [0.000, 3.000],  loss: 13.690213, mae: 37.769958, mean_q: 32.515364, mean_eps: 0.738975
  58127/200000: episode: 608, duration: 0.488s, episode steps:  73, steps per second: 149, episode reward: -51.883, mean reward: -0.711 [-100.000, 20.293], mean action: 1.575 [0.000, 3.000],  loss: 7.861469, mae: 36.945654, mean_q: 30.593433, mean_eps: 0.738595
  58240/200000: episode: 609, duration: 0.708s, episode steps: 113, steps per second: 160, episode reward: -78.191, mean reward: -0.692 [-100.000,  7.691], mean action: 1.611 [0.000, 3.000],  loss: 6.941532, mae: 36.643704, mean_q: 30.125636, mean_eps: 0.738177
  58304/200000: episode: 610, duration: 0.410s, episode steps:  64, steps per second: 156, episode reward: -84.886, mean reward: -1.326 [-100.000,  6.445], mean action: 1.547 [0.000, 3.000],  loss: 17.592866, mae: 36.611249, mean_q: 29.480624, mean_eps: 0.737778
  58392/200000: episode: 611, duration: 0.644s, episode steps:  88, steps per second: 137, episode reward: -88.900, mean reward: -1.010 [-100.000,  8.871], mean action: 1.705 [0.000, 3.000],  loss: 5.333398, mae: 36.777416, mean_q: 29.528492, mean_eps: 0.737436
  58459/200000: episode: 612, duration: 0.447s, episode steps:  67, steps per second: 150, episode reward: -90.623, mean reward: -1.353 [-100.000, 16.637], mean action: 1.731 [0.000, 3.000],  loss: 6.451652, mae: 37.279364, mean_q: 30.620649, mean_eps: 0.737088
  58537/200000: episode: 613, duration: 0.523s, episode steps:  78, steps per second: 149, episode reward: -100.235, mean reward: -1.285 [-100.000, 10.580], mean action: 1.667 [0.000, 3.000],  loss: 14.442649, mae: 37.001556, mean_q: 30.740456, mean_eps: 0.736761
  58612/200000: episode: 614, duration: 0.509s, episode steps:  75, steps per second: 147, episode reward: -71.090, mean reward: -0.948 [-100.000, 13.334], mean action: 1.613 [0.000, 3.000],  loss: 4.360862, mae: 36.969403, mean_q: 31.243557, mean_eps: 0.736417
  58697/200000: episode: 615, duration: 0.547s, episode steps:  85, steps per second: 155, episode reward: -72.479, mean reward: -0.853 [-100.000, 16.020], mean action: 1.624 [0.000, 3.000],  loss: 5.494083, mae: 37.856502, mean_q: 31.646453, mean_eps: 0.736057
  58786/200000: episode: 616, duration: 0.582s, episode steps:  89, steps per second: 153, episode reward: -139.949, mean reward: -1.572 [-100.000, 17.356], mean action: 1.539 [0.000, 3.000],  loss: 9.089965, mae: 36.806713, mean_q: 30.174238, mean_eps: 0.735666
  58896/200000: episode: 617, duration: 0.714s, episode steps: 110, steps per second: 154, episode reward: -33.058, mean reward: -0.301 [-100.000, 29.579], mean action: 1.527 [0.000, 3.000],  loss: 7.480344, mae: 36.921552, mean_q: 29.543030, mean_eps: 0.735218
  59022/200000: episode: 618, duration: 0.779s, episode steps: 126, steps per second: 162, episode reward: -123.490, mean reward: -0.980 [-100.000,  9.043], mean action: 1.508 [0.000, 3.000],  loss: 11.276185, mae: 36.921315, mean_q: 30.099358, mean_eps: 0.734687
  59139/200000: episode: 619, duration: 0.786s, episode steps: 117, steps per second: 149, episode reward: -53.747, mean reward: -0.459 [-100.000,  7.173], mean action: 1.504 [0.000, 3.000],  loss: 9.363483, mae: 36.887794, mean_q: 30.380570, mean_eps: 0.734140
  59226/200000: episode: 620, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -48.223, mean reward: -0.554 [-100.000,  7.103], mean action: 1.609 [0.000, 3.000],  loss: 8.864712, mae: 36.620963, mean_q: 29.547248, mean_eps: 0.733681
  59306/200000: episode: 621, duration: 0.492s, episode steps:  80, steps per second: 163, episode reward: -28.721, mean reward: -0.359 [-100.000, 11.623], mean action: 1.725 [0.000, 3.000],  loss: 16.147591, mae: 37.230851, mean_q: 29.904560, mean_eps: 0.733305
  59401/200000: episode: 622, duration: 0.602s, episode steps:  95, steps per second: 158, episode reward: -78.841, mean reward: -0.830 [-100.000, 19.961], mean action: 1.600 [0.000, 3.000],  loss: 6.329253, mae: 36.476579, mean_q: 30.557096, mean_eps: 0.732911
  59463/200000: episode: 623, duration: 0.393s, episode steps:  62, steps per second: 158, episode reward: -83.551, mean reward: -1.348 [-100.000, 20.353], mean action: 1.790 [0.000, 3.000],  loss: 6.508822, mae: 36.957952, mean_q: 30.757528, mean_eps: 0.732558
  59596/200000: episode: 624, duration: 0.820s, episode steps: 133, steps per second: 162, episode reward: -317.540, mean reward: -2.388 [-100.000, 70.489], mean action: 1.519 [0.000, 3.000],  loss: 8.514435, mae: 37.295520, mean_q: 29.705923, mean_eps: 0.732120
  59694/200000: episode: 625, duration: 0.618s, episode steps:  98, steps per second: 159, episode reward: -57.958, mean reward: -0.591 [-100.000, 16.696], mean action: 1.612 [0.000, 3.000],  loss: 7.657873, mae: 37.015599, mean_q: 29.665250, mean_eps: 0.731600
  59831/200000: episode: 626, duration: 0.904s, episode steps: 137, steps per second: 151, episode reward: -133.927, mean reward: -0.978 [-100.000,  4.049], mean action: 1.526 [0.000, 3.000],  loss: 12.925213, mae: 36.949507, mean_q: 29.678554, mean_eps: 0.731071
  59923/200000: episode: 627, duration: 0.591s, episode steps:  92, steps per second: 156, episode reward: -56.476, mean reward: -0.614 [-100.000, 11.200], mean action: 1.500 [0.000, 3.000],  loss: 6.430477, mae: 37.578842, mean_q: 28.999862, mean_eps: 0.730556
  59993/200000: episode: 628, duration: 0.460s, episode steps:  70, steps per second: 152, episode reward: -50.128, mean reward: -0.716 [-100.000, 11.837], mean action: 1.686 [0.000, 3.000],  loss: 5.462488, mae: 37.557434, mean_q: 30.526387, mean_eps: 0.730191
  60121/200000: episode: 629, duration: 0.825s, episode steps: 128, steps per second: 155, episode reward: -55.049, mean reward: -0.430 [-100.000,  9.311], mean action: 1.680 [0.000, 3.000],  loss: 10.100384, mae: 38.215211, mean_q: 29.597192, mean_eps: 0.729746
  60205/200000: episode: 630, duration: 0.505s, episode steps:  84, steps per second: 166, episode reward: -42.885, mean reward: -0.511 [-100.000, 15.067], mean action: 1.917 [0.000, 3.000],  loss: 20.004387, mae: 37.351220, mean_q: 30.833867, mean_eps: 0.729269
  60309/200000: episode: 631, duration: 0.672s, episode steps: 104, steps per second: 155, episode reward: -110.235, mean reward: -1.060 [-100.000,  7.401], mean action: 1.721 [0.000, 3.000],  loss: 8.759805, mae: 37.327602, mean_q: 31.626841, mean_eps: 0.728846
  60381/200000: episode: 632, duration: 0.482s, episode steps:  72, steps per second: 150, episode reward: -70.369, mean reward: -0.977 [-100.000, 11.445], mean action: 1.694 [0.000, 3.000],  loss: 4.818863, mae: 37.798012, mean_q: 29.194379, mean_eps: 0.728450
  60462/200000: episode: 633, duration: 0.614s, episode steps:  81, steps per second: 132, episode reward: -49.050, mean reward: -0.606 [-100.000, 14.820], mean action: 1.679 [0.000, 3.000],  loss: 10.269089, mae: 37.252620, mean_q: 29.626299, mean_eps: 0.728105
  60557/200000: episode: 634, duration: 0.628s, episode steps:  95, steps per second: 151, episode reward: -71.241, mean reward: -0.750 [-100.000,  8.204], mean action: 1.558 [0.000, 3.000],  loss: 14.006555, mae: 37.258424, mean_q: 29.630928, mean_eps: 0.727710
  60659/200000: episode: 635, duration: 0.677s, episode steps: 102, steps per second: 151, episode reward: -49.471, mean reward: -0.485 [-100.000, 16.742], mean action: 1.608 [0.000, 3.000],  loss: 7.326358, mae: 37.519150, mean_q: 30.552718, mean_eps: 0.727266
  60774/200000: episode: 636, duration: 0.790s, episode steps: 115, steps per second: 146, episode reward: -84.468, mean reward: -0.735 [-100.000,  6.651], mean action: 1.696 [0.000, 3.000],  loss: 8.184684, mae: 37.665548, mean_q: 28.909725, mean_eps: 0.726778
  60847/200000: episode: 637, duration: 0.466s, episode steps:  73, steps per second: 157, episode reward: -108.518, mean reward: -1.487 [-100.000, 12.666], mean action: 1.521 [0.000, 3.000],  loss: 9.962771, mae: 37.203159, mean_q: 29.416750, mean_eps: 0.726355
  60936/200000: episode: 638, duration: 0.557s, episode steps:  89, steps per second: 160, episode reward: -58.365, mean reward: -0.656 [-100.000, 14.508], mean action: 1.652 [0.000, 3.000],  loss: 4.048235, mae: 37.352608, mean_q: 31.512015, mean_eps: 0.725991
  61062/200000: episode: 639, duration: 0.763s, episode steps: 126, steps per second: 165, episode reward: -100.295, mean reward: -0.796 [-100.000,  9.749], mean action: 1.603 [0.000, 3.000],  loss: 8.419550, mae: 37.169835, mean_q: 29.863339, mean_eps: 0.725507
  61176/200000: episode: 640, duration: 0.770s, episode steps: 114, steps per second: 148, episode reward: -84.146, mean reward: -0.738 [-100.000, 10.491], mean action: 1.447 [0.000, 3.000],  loss: 12.481812, mae: 37.398248, mean_q: 29.489278, mean_eps: 0.724967
  61247/200000: episode: 641, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -143.039, mean reward: -2.015 [-100.000,  9.588], mean action: 1.451 [0.000, 3.000],  loss: 7.708776, mae: 37.420768, mean_q: 30.206551, mean_eps: 0.724550
  61330/200000: episode: 642, duration: 0.539s, episode steps:  83, steps per second: 154, episode reward: -114.307, mean reward: -1.377 [-100.000,  6.432], mean action: 1.759 [0.000, 3.000],  loss: 4.728707, mae: 36.800107, mean_q: 29.826922, mean_eps: 0.724204
  61410/200000: episode: 643, duration: 0.561s, episode steps:  80, steps per second: 143, episode reward: -84.612, mean reward: -1.058 [-100.000,  6.278], mean action: 1.775 [0.000, 3.000],  loss: 2.783608, mae: 37.158912, mean_q: 28.947039, mean_eps: 0.723837
  61489/200000: episode: 644, duration: 0.516s, episode steps:  79, steps per second: 153, episode reward: -51.380, mean reward: -0.650 [-100.000, 28.572], mean action: 1.570 [0.000, 3.000],  loss: 10.296380, mae: 36.944392, mean_q: 30.381064, mean_eps: 0.723480
  61612/200000: episode: 645, duration: 0.925s, episode steps: 123, steps per second: 133, episode reward: -62.464, mean reward: -0.508 [-100.000, 11.189], mean action: 1.675 [0.000, 3.000],  loss: 12.970063, mae: 37.949093, mean_q: 29.344169, mean_eps: 0.723025
  61731/200000: episode: 646, duration: 1.021s, episode steps: 119, steps per second: 117, episode reward: -61.610, mean reward: -0.518 [-100.000, 42.503], mean action: 1.672 [0.000, 3.000],  loss: 14.279361, mae: 36.923314, mean_q: 30.132886, mean_eps: 0.722481
  61817/200000: episode: 647, duration: 0.573s, episode steps:  86, steps per second: 150, episode reward: -57.103, mean reward: -0.664 [-100.000, 11.455], mean action: 1.500 [0.000, 3.000],  loss: 10.409480, mae: 37.133549, mean_q: 29.078355, mean_eps: 0.722019
  61895/200000: episode: 648, duration: 0.479s, episode steps:  78, steps per second: 163, episode reward: -51.302, mean reward: -0.658 [-100.000,  9.261], mean action: 1.628 [0.000, 3.000],  loss: 16.161904, mae: 37.119108, mean_q: 30.085888, mean_eps: 0.721650
  61988/200000: episode: 649, duration: 1.051s, episode steps:  93, steps per second:  88, episode reward: -59.846, mean reward: -0.644 [-100.000, 10.152], mean action: 1.645 [0.000, 3.000],  loss: 11.056387, mae: 36.950392, mean_q: 27.238980, mean_eps: 0.721266
  62090/200000: episode: 650, duration: 0.958s, episode steps: 102, steps per second: 106, episode reward: -114.097, mean reward: -1.119 [-100.000,  8.642], mean action: 1.500 [0.000, 3.000],  loss: 6.096347, mae: 36.495717, mean_q: 29.570709, mean_eps: 0.720827
  62202/200000: episode: 651, duration: 1.658s, episode steps: 112, steps per second:  68, episode reward: -197.784, mean reward: -1.766 [-100.000,  5.672], mean action: 1.393 [0.000, 3.000],  loss: 8.521855, mae: 36.878419, mean_q: 28.510749, mean_eps: 0.720345
  62282/200000: episode: 652, duration: 1.254s, episode steps:  80, steps per second:  64, episode reward: -94.406, mean reward: -1.180 [-100.000,  9.842], mean action: 1.650 [0.000, 3.000],  loss: 3.279873, mae: 36.822917, mean_q: 27.815207, mean_eps: 0.719913
  62376/200000: episode: 653, duration: 1.536s, episode steps:  94, steps per second:  61, episode reward: -97.527, mean reward: -1.038 [-100.000, 11.233], mean action: 1.660 [0.000, 3.000],  loss: 6.022840, mae: 37.193906, mean_q: 28.490953, mean_eps: 0.719522
  62474/200000: episode: 654, duration: 1.190s, episode steps:  98, steps per second:  82, episode reward: -66.974, mean reward: -0.683 [-100.000, 13.698], mean action: 1.622 [0.000, 3.000],  loss: 6.476652, mae: 36.844128, mean_q: 30.095774, mean_eps: 0.719090
  62561/200000: episode: 655, duration: 1.770s, episode steps:  87, steps per second:  49, episode reward: -36.439, mean reward: -0.419 [-100.000, 13.777], mean action: 1.747 [0.000, 3.000],  loss: 4.965337, mae: 37.647713, mean_q: 29.185925, mean_eps: 0.718674
  62639/200000: episode: 656, duration: 1.551s, episode steps:  78, steps per second:  50, episode reward: -32.533, mean reward: -0.417 [-100.000, 56.095], mean action: 1.551 [0.000, 3.000],  loss: 5.248721, mae: 36.154591, mean_q: 27.980289, mean_eps: 0.718302
  62734/200000: episode: 657, duration: 1.401s, episode steps:  95, steps per second:  68, episode reward: -85.640, mean reward: -0.901 [-100.000,  9.755], mean action: 1.495 [0.000, 3.000],  loss: 8.059083, mae: 37.618482, mean_q: 29.708328, mean_eps: 0.717913
  62817/200000: episode: 658, duration: 1.092s, episode steps:  83, steps per second:  76, episode reward: -125.281, mean reward: -1.509 [-100.000, 10.471], mean action: 1.627 [0.000, 3.000],  loss: 9.347503, mae: 36.867637, mean_q: 28.930917, mean_eps: 0.717512
  62926/200000: episode: 659, duration: 1.288s, episode steps: 109, steps per second:  85, episode reward: -61.830, mean reward: -0.567 [-100.000, 10.813], mean action: 1.541 [0.000, 3.000],  loss: 17.178424, mae: 37.319828, mean_q: 29.353433, mean_eps: 0.717080
  63061/200000: episode: 660, duration: 1.180s, episode steps: 135, steps per second: 114, episode reward: -71.996, mean reward: -0.533 [-100.000,  7.736], mean action: 1.756 [0.000, 3.000],  loss: 8.435013, mae: 37.150425, mean_q: 29.347114, mean_eps: 0.716531
  63135/200000: episode: 661, duration: 0.678s, episode steps:  74, steps per second: 109, episode reward: -167.227, mean reward: -2.260 [-100.000,  4.676], mean action: 1.432 [0.000, 3.000],  loss: 5.527994, mae: 37.361738, mean_q: 29.376442, mean_eps: 0.716061
  63249/200000: episode: 662, duration: 0.955s, episode steps: 114, steps per second: 119, episode reward: -92.562, mean reward: -0.812 [-100.000,  9.598], mean action: 1.544 [0.000, 3.000],  loss: 8.400576, mae: 37.257241, mean_q: 27.977670, mean_eps: 0.715638
  63368/200000: episode: 663, duration: 0.899s, episode steps: 119, steps per second: 132, episode reward: -209.705, mean reward: -1.762 [-100.000,  3.842], mean action: 1.580 [0.000, 3.000],  loss: 6.589918, mae: 37.607967, mean_q: 30.083681, mean_eps: 0.715114
  63491/200000: episode: 664, duration: 1.019s, episode steps: 123, steps per second: 121, episode reward: -43.055, mean reward: -0.350 [-100.000, 13.367], mean action: 1.480 [0.000, 3.000],  loss: 11.148269, mae: 37.261211, mean_q: 28.727542, mean_eps: 0.714570
  63552/200000: episode: 665, duration: 0.500s, episode steps:  61, steps per second: 122, episode reward: -95.660, mean reward: -1.568 [-100.000, 13.299], mean action: 1.541 [0.000, 3.000],  loss: 5.718527, mae: 37.481263, mean_q: 29.035842, mean_eps: 0.714155
  63651/200000: episode: 666, duration: 0.746s, episode steps:  99, steps per second: 133, episode reward: -61.733, mean reward: -0.624 [-100.000, 19.406], mean action: 1.707 [0.000, 3.000],  loss: 6.963421, mae: 37.189278, mean_q: 30.005254, mean_eps: 0.713795
  63751/200000: episode: 667, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: -107.999, mean reward: -1.080 [-100.000, 12.631], mean action: 1.720 [0.000, 3.000],  loss: 9.605990, mae: 37.720480, mean_q: 30.008397, mean_eps: 0.713348
  63859/200000: episode: 668, duration: 0.754s, episode steps: 108, steps per second: 143, episode reward: -87.021, mean reward: -0.806 [-100.000, 10.164], mean action: 1.398 [0.000, 3.000],  loss: 8.683792, mae: 37.222827, mean_q: 29.728394, mean_eps: 0.712880
  63949/200000: episode: 669, duration: 0.645s, episode steps:  90, steps per second: 140, episode reward: -7.147, mean reward: -0.079 [-100.000, 11.631], mean action: 1.511 [0.000, 3.000],  loss: 9.693252, mae: 37.678321, mean_q: 29.764880, mean_eps: 0.712434
  64050/200000: episode: 670, duration: 0.712s, episode steps: 101, steps per second: 142, episode reward: -98.972, mean reward: -0.980 [-100.000, 13.974], mean action: 1.545 [0.000, 3.000],  loss: 13.117463, mae: 37.486155, mean_q: 29.037291, mean_eps: 0.712004
  64147/200000: episode: 671, duration: 0.695s, episode steps:  97, steps per second: 140, episode reward: -61.952, mean reward: -0.639 [-100.000, 32.832], mean action: 1.639 [0.000, 3.000],  loss: 11.645115, mae: 38.017841, mean_q: 30.774707, mean_eps: 0.711559
  64256/200000: episode: 672, duration: 0.749s, episode steps: 109, steps per second: 145, episode reward: -171.404, mean reward: -1.573 [-100.000,  9.822], mean action: 1.734 [0.000, 3.000],  loss: 11.873945, mae: 36.973254, mean_q: 30.523267, mean_eps: 0.711096
  64363/200000: episode: 673, duration: 0.757s, episode steps: 107, steps per second: 141, episode reward: -151.804, mean reward: -1.419 [-100.000, 13.732], mean action: 1.505 [0.000, 3.000],  loss: 9.730196, mae: 36.816389, mean_q: 29.268897, mean_eps: 0.710609
  64430/200000: episode: 674, duration: 0.434s, episode steps:  67, steps per second: 154, episode reward: -44.286, mean reward: -0.661 [-100.000,  7.005], mean action: 1.746 [0.000, 3.000],  loss: 14.738042, mae: 37.621788, mean_q: 29.620160, mean_eps: 0.710218
  64542/200000: episode: 675, duration: 0.969s, episode steps: 112, steps per second: 116, episode reward: -76.985, mean reward: -0.687 [-100.000, 10.805], mean action: 1.554 [0.000, 3.000],  loss: 7.870140, mae: 37.289451, mean_q: 28.249401, mean_eps: 0.709815
  64629/200000: episode: 676, duration: 0.716s, episode steps:  87, steps per second: 122, episode reward: -90.472, mean reward: -1.040 [-100.000, 12.676], mean action: 1.621 [0.000, 3.000],  loss: 11.019398, mae: 37.142357, mean_q: 30.193309, mean_eps: 0.709367
  64721/200000: episode: 677, duration: 0.639s, episode steps:  92, steps per second: 144, episode reward: -21.222, mean reward: -0.231 [-100.000,  6.946], mean action: 1.674 [0.000, 3.000],  loss: 6.435280, mae: 37.320102, mean_q: 31.628154, mean_eps: 0.708965
  64855/200000: episode: 678, duration: 0.949s, episode steps: 134, steps per second: 141, episode reward: -160.078, mean reward: -1.195 [-100.000, 12.949], mean action: 1.567 [0.000, 3.000],  loss: 6.995881, mae: 36.763307, mean_q: 29.129071, mean_eps: 0.708456
  64928/200000: episode: 679, duration: 0.503s, episode steps:  73, steps per second: 145, episode reward: -79.095, mean reward: -1.083 [-100.000, 18.383], mean action: 1.425 [0.000, 3.000],  loss: 17.639708, mae: 37.462460, mean_q: 29.307003, mean_eps: 0.707990
  65006/200000: episode: 680, duration: 0.494s, episode steps:  78, steps per second: 158, episode reward: -21.145, mean reward: -0.271 [-100.000,  6.756], mean action: 1.551 [0.000, 3.000],  loss: 11.161867, mae: 37.770511, mean_q: 29.503131, mean_eps: 0.707651
  65109/200000: episode: 681, duration: 0.695s, episode steps: 103, steps per second: 148, episode reward: -39.454, mean reward: -0.383 [-100.000,  6.919], mean action: 1.728 [0.000, 3.000],  loss: 13.677197, mae: 38.059699, mean_q: 29.965413, mean_eps: 0.707244
  65204/200000: episode: 682, duration: 0.660s, episode steps:  95, steps per second: 144, episode reward: -85.553, mean reward: -0.901 [-100.000, 11.815], mean action: 1.705 [0.000, 3.000],  loss: 9.218835, mae: 36.784620, mean_q: 28.571821, mean_eps: 0.706798
  65276/200000: episode: 683, duration: 0.510s, episode steps:  72, steps per second: 141, episode reward: -59.420, mean reward: -0.825 [-100.000, 21.445], mean action: 1.847 [0.000, 3.000],  loss: 9.836849, mae: 38.269509, mean_q: 30.531631, mean_eps: 0.706422
  65379/200000: episode: 684, duration: 0.754s, episode steps: 103, steps per second: 137, episode reward: -123.001, mean reward: -1.194 [-100.000, 10.447], mean action: 1.524 [0.000, 3.000],  loss: 7.409953, mae: 37.525380, mean_q: 30.291081, mean_eps: 0.706028
  65498/200000: episode: 685, duration: 0.854s, episode steps: 119, steps per second: 139, episode reward: -67.286, mean reward: -0.565 [-100.000,  9.352], mean action: 1.597 [0.000, 3.000],  loss: 5.093505, mae: 37.040021, mean_q: 29.856217, mean_eps: 0.705529
  65571/200000: episode: 686, duration: 0.490s, episode steps:  73, steps per second: 149, episode reward: -97.628, mean reward: -1.337 [-100.000, 15.427], mean action: 1.603 [0.000, 3.000],  loss: 14.079982, mae: 37.155989, mean_q: 27.937831, mean_eps: 0.705097
  65680/200000: episode: 687, duration: 0.699s, episode steps: 109, steps per second: 156, episode reward: -8.859, mean reward: -0.081 [-100.000, 18.440], mean action: 1.697 [0.000, 3.000],  loss: 9.705544, mae: 37.552911, mean_q: 30.277669, mean_eps: 0.704687
  65777/200000: episode: 688, duration: 0.667s, episode steps:  97, steps per second: 145, episode reward: -20.980, mean reward: -0.216 [-100.000, 19.419], mean action: 1.567 [0.000, 3.000],  loss: 11.426156, mae: 36.754780, mean_q: 29.223914, mean_eps: 0.704224
  65851/200000: episode: 689, duration: 0.547s, episode steps:  74, steps per second: 135, episode reward: -31.233, mean reward: -0.422 [-100.000, 17.435], mean action: 1.378 [0.000, 3.000],  loss: 8.008253, mae: 37.172633, mean_q: 30.361115, mean_eps: 0.703839
  65948/200000: episode: 690, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -98.684, mean reward: -1.017 [-100.000,  6.857], mean action: 1.701 [0.000, 3.000],  loss: 6.624278, mae: 36.851326, mean_q: 28.309662, mean_eps: 0.703454
  66028/200000: episode: 691, duration: 0.580s, episode steps:  80, steps per second: 138, episode reward: -63.575, mean reward: -0.795 [-100.000, 10.544], mean action: 1.775 [0.000, 3.000],  loss: 3.369512, mae: 36.829635, mean_q: 29.753004, mean_eps: 0.703056
  66111/200000: episode: 692, duration: 0.573s, episode steps:  83, steps per second: 145, episode reward: -22.192, mean reward: -0.267 [-100.000,  7.368], mean action: 1.687 [0.000, 3.000],  loss: 11.041039, mae: 36.888672, mean_q: 29.622750, mean_eps: 0.702689
  66236/200000: episode: 693, duration: 0.955s, episode steps: 125, steps per second: 131, episode reward: -38.038, mean reward: -0.304 [-100.000, 10.352], mean action: 1.568 [0.000, 3.000],  loss: 7.081883, mae: 37.114459, mean_q: 29.539569, mean_eps: 0.702222
  66360/200000: episode: 694, duration: 0.905s, episode steps: 124, steps per second: 137, episode reward: -34.477, mean reward: -0.278 [-100.000, 11.344], mean action: 1.702 [0.000, 3.000],  loss: 10.044464, mae: 37.367334, mean_q: 29.705277, mean_eps: 0.701661
  66483/200000: episode: 695, duration: 0.860s, episode steps: 123, steps per second: 143, episode reward: -112.407, mean reward: -0.914 [-100.000,  5.919], mean action: 1.683 [0.000, 3.000],  loss: 13.002878, mae: 37.447265, mean_q: 29.398148, mean_eps: 0.701106
  66597/200000: episode: 696, duration: 0.777s, episode steps: 114, steps per second: 147, episode reward: -83.845, mean reward: -0.735 [-100.000,  7.100], mean action: 1.561 [0.000, 3.000],  loss: 10.393561, mae: 37.876228, mean_q: 30.919237, mean_eps: 0.700572
  66704/200000: episode: 697, duration: 0.759s, episode steps: 107, steps per second: 141, episode reward: -36.186, mean reward: -0.338 [-100.000, 12.004], mean action: 1.664 [0.000, 3.000],  loss: 6.291492, mae: 37.594209, mean_q: 30.649819, mean_eps: 0.700075
  66786/200000: episode: 698, duration: 0.561s, episode steps:  82, steps per second: 146, episode reward: -64.106, mean reward: -0.782 [-100.000, 14.041], mean action: 1.780 [0.000, 3.000],  loss: 6.061000, mae: 37.509752, mean_q: 30.150371, mean_eps: 0.699650
  66885/200000: episode: 699, duration: 0.637s, episode steps:  99, steps per second: 156, episode reward: -82.211, mean reward: -0.830 [-100.000,  7.533], mean action: 1.646 [0.000, 3.000],  loss: 5.679518, mae: 37.516236, mean_q: 29.321802, mean_eps: 0.699242
  66985/200000: episode: 700, duration: 0.631s, episode steps: 100, steps per second: 159, episode reward: -71.420, mean reward: -0.714 [-100.000, 11.245], mean action: 1.750 [0.000, 3.000],  loss: 10.617378, mae: 37.563082, mean_q: 29.581979, mean_eps: 0.698795
  67103/200000: episode: 701, duration: 0.843s, episode steps: 118, steps per second: 140, episode reward: -55.636, mean reward: -0.471 [-100.000, 12.554], mean action: 1.475 [0.000, 3.000],  loss: 12.443478, mae: 38.374402, mean_q: 29.936940, mean_eps: 0.698304
  67226/200000: episode: 702, duration: 0.812s, episode steps: 123, steps per second: 151, episode reward: -92.891, mean reward: -0.755 [-100.000,  9.453], mean action: 1.504 [0.000, 3.000],  loss: 7.379013, mae: 37.812147, mean_q: 28.536851, mean_eps: 0.697762
  67305/200000: episode: 703, duration: 0.589s, episode steps:  79, steps per second: 134, episode reward: -6.214, mean reward: -0.079 [-100.000, 19.140], mean action: 1.709 [0.000, 3.000],  loss: 7.369319, mae: 38.040231, mean_q: 29.982570, mean_eps: 0.697308
  67444/200000: episode: 704, duration: 1.171s, episode steps: 139, steps per second: 119, episode reward: -101.776, mean reward: -0.732 [-100.000, 19.375], mean action: 1.518 [0.000, 3.000],  loss: 5.479965, mae: 37.387991, mean_q: 28.893650, mean_eps: 0.696817
  67537/200000: episode: 705, duration: 0.739s, episode steps:  93, steps per second: 126, episode reward: -59.411, mean reward: -0.639 [-100.000, 17.811], mean action: 1.699 [0.000, 3.000],  loss: 4.549802, mae: 38.292465, mean_q: 29.305536, mean_eps: 0.696295
  67652/200000: episode: 706, duration: 0.759s, episode steps: 115, steps per second: 151, episode reward: -97.125, mean reward: -0.845 [-100.000,  9.736], mean action: 1.357 [0.000, 3.000],  loss: 9.756602, mae: 38.188310, mean_q: 28.115251, mean_eps: 0.695827
  67727/200000: episode: 707, duration: 0.508s, episode steps:  75, steps per second: 148, episode reward: -117.046, mean reward: -1.561 [-100.000,  5.181], mean action: 2.013 [0.000, 3.000],  loss: 6.272477, mae: 38.026188, mean_q: 28.463325, mean_eps: 0.695399
  67804/200000: episode: 708, duration: 0.506s, episode steps:  77, steps per second: 152, episode reward: -53.366, mean reward: -0.693 [-100.000, 10.016], mean action: 1.753 [0.000, 3.000],  loss: 8.658687, mae: 38.140477, mean_q: 29.430691, mean_eps: 0.695057
  67868/200000: episode: 709, duration: 0.392s, episode steps:  64, steps per second: 163, episode reward: -72.211, mean reward: -1.128 [-100.000,  5.790], mean action: 1.516 [0.000, 3.000],  loss: 8.000138, mae: 36.708881, mean_q: 27.495501, mean_eps: 0.694740
  67978/200000: episode: 710, duration: 0.789s, episode steps: 110, steps per second: 139, episode reward: -186.925, mean reward: -1.699 [-100.000, 43.372], mean action: 1.564 [0.000, 3.000],  loss: 4.920154, mae: 38.512424, mean_q: 29.305239, mean_eps: 0.694349
  68074/200000: episode: 711, duration: 0.674s, episode steps:  96, steps per second: 143, episode reward: -88.741, mean reward: -0.924 [-100.000,  9.163], mean action: 1.604 [0.000, 3.000],  loss: 8.825987, mae: 38.773735, mean_q: 30.357177, mean_eps: 0.693885
  68200/200000: episode: 712, duration: 0.797s, episode steps: 126, steps per second: 158, episode reward: -77.125, mean reward: -0.612 [-100.000,  6.190], mean action: 1.706 [0.000, 3.000],  loss: 4.626548, mae: 38.174389, mean_q: 28.966266, mean_eps: 0.693386
  68327/200000: episode: 713, duration: 0.872s, episode steps: 127, steps per second: 146, episode reward: -41.700, mean reward: -0.328 [-100.000,  7.865], mean action: 1.449 [0.000, 3.000],  loss: 5.636164, mae: 37.917695, mean_q: 28.945402, mean_eps: 0.692817
  68399/200000: episode: 714, duration: 0.511s, episode steps:  72, steps per second: 141, episode reward: -28.525, mean reward: -0.396 [-100.000, 13.190], mean action: 1.569 [0.000, 3.000],  loss: 4.346837, mae: 38.440780, mean_q: 29.846386, mean_eps: 0.692369
  68533/200000: episode: 715, duration: 0.909s, episode steps: 134, steps per second: 147, episode reward: -153.906, mean reward: -1.149 [-100.000, 10.371], mean action: 1.701 [0.000, 3.000],  loss: 6.900834, mae: 38.590834, mean_q: 29.976581, mean_eps: 0.691905
  68597/200000: episode: 716, duration: 0.524s, episode steps:  64, steps per second: 122, episode reward: -83.966, mean reward: -1.312 [-100.000, 12.454], mean action: 1.641 [0.000, 3.000],  loss: 6.500167, mae: 38.216285, mean_q: 30.638421, mean_eps: 0.691460
  68685/200000: episode: 717, duration: 0.651s, episode steps:  88, steps per second: 135, episode reward: -88.088, mean reward: -1.001 [-100.000, 11.162], mean action: 1.602 [0.000, 3.000],  loss: 5.768624, mae: 38.310084, mean_q: 30.834260, mean_eps: 0.691118
  68781/200000: episode: 718, duration: 0.632s, episode steps:  96, steps per second: 152, episode reward: -21.825, mean reward: -0.227 [-100.000, 14.030], mean action: 1.542 [0.000, 3.000],  loss: 8.060792, mae: 38.018165, mean_q: 27.936523, mean_eps: 0.690704
  68930/200000: episode: 719, duration: 1.014s, episode steps: 149, steps per second: 147, episode reward: -10.525, mean reward: -0.071 [-100.000, 18.109], mean action: 1.651 [0.000, 3.000],  loss: 5.624630, mae: 38.690513, mean_q: 29.387166, mean_eps: 0.690153
  69062/200000: episode: 720, duration: 0.809s, episode steps: 132, steps per second: 163, episode reward: -74.637, mean reward: -0.565 [-100.000, 12.301], mean action: 1.606 [0.000, 3.000],  loss: 6.754585, mae: 38.303391, mean_q: 29.621778, mean_eps: 0.689520
  69138/200000: episode: 721, duration: 0.523s, episode steps:  76, steps per second: 145, episode reward: -84.910, mean reward: -1.117 [-100.000,  8.683], mean action: 1.513 [0.000, 3.000],  loss: 6.034578, mae: 38.929013, mean_q: 29.764700, mean_eps: 0.689052
  69260/200000: episode: 722, duration: 0.954s, episode steps: 122, steps per second: 128, episode reward: -61.515, mean reward: -0.504 [-100.000, 10.781], mean action: 1.459 [0.000, 3.000],  loss: 7.809671, mae: 38.609426, mean_q: 29.252226, mean_eps: 0.688607
  69348/200000: episode: 723, duration: 0.612s, episode steps:  88, steps per second: 144, episode reward: -44.290, mean reward: -0.503 [-100.000,  7.260], mean action: 1.750 [0.000, 3.000],  loss: 11.423456, mae: 38.700855, mean_q: 28.849817, mean_eps: 0.688134
  69476/200000: episode: 724, duration: 0.976s, episode steps: 128, steps per second: 131, episode reward: -53.933, mean reward: -0.421 [-100.000, 14.507], mean action: 1.484 [0.000, 3.000],  loss: 7.140090, mae: 39.090900, mean_q: 31.221825, mean_eps: 0.687648
  69550/200000: episode: 725, duration: 0.605s, episode steps:  74, steps per second: 122, episode reward: -32.204, mean reward: -0.435 [-100.000, 17.525], mean action: 1.676 [0.000, 3.000],  loss: 5.542056, mae: 39.057744, mean_q: 30.781311, mean_eps: 0.687194
  69653/200000: episode: 726, duration: 0.898s, episode steps: 103, steps per second: 115, episode reward: -48.815, mean reward: -0.474 [-100.000, 14.893], mean action: 1.515 [0.000, 3.000],  loss: 11.193157, mae: 39.126696, mean_q: 29.760778, mean_eps: 0.686796
  69767/200000: episode: 727, duration: 0.784s, episode steps: 114, steps per second: 145, episode reward: -148.761, mean reward: -1.305 [-100.000,  3.587], mean action: 1.614 [0.000, 3.000],  loss: 6.100507, mae: 38.861282, mean_q: 29.582945, mean_eps: 0.686307
  69875/200000: episode: 728, duration: 0.783s, episode steps: 108, steps per second: 138, episode reward: -119.827, mean reward: -1.110 [-100.000, 21.074], mean action: 1.620 [0.000, 3.000],  loss: 6.144323, mae: 38.909045, mean_q: 28.751421, mean_eps: 0.685808
  69974/200000: episode: 729, duration: 0.740s, episode steps:  99, steps per second: 134, episode reward: -92.724, mean reward: -0.937 [-100.000,  8.086], mean action: 1.667 [0.000, 3.000],  loss: 6.592521, mae: 38.821961, mean_q: 30.965995, mean_eps: 0.685342
  70050/200000: episode: 730, duration: 0.555s, episode steps:  76, steps per second: 137, episode reward: -102.055, mean reward: -1.343 [-100.000,  5.530], mean action: 1.855 [0.000, 3.000],  loss: 7.106995, mae: 39.568474, mean_q: 29.665675, mean_eps: 0.684948
  70141/200000: episode: 731, duration: 0.843s, episode steps:  91, steps per second: 108, episode reward: -118.749, mean reward: -1.305 [-100.000,  4.663], mean action: 1.978 [0.000, 3.000],  loss: 7.260355, mae: 38.946803, mean_q: 30.015205, mean_eps: 0.684572
  70247/200000: episode: 732, duration: 0.861s, episode steps: 106, steps per second: 123, episode reward: -62.660, mean reward: -0.591 [-100.000,  9.020], mean action: 1.642 [0.000, 3.000],  loss: 6.675594, mae: 38.482065, mean_q: 30.076601, mean_eps: 0.684129
  70317/200000: episode: 733, duration: 0.849s, episode steps:  70, steps per second:  82, episode reward: -90.479, mean reward: -1.293 [-100.000,  7.709], mean action: 1.529 [0.000, 3.000],  loss: 7.076731, mae: 38.676220, mean_q: 29.069085, mean_eps: 0.683733
  70409/200000: episode: 734, duration: 2.506s, episode steps:  92, steps per second:  37, episode reward: -24.288, mean reward: -0.264 [-100.000, 20.858], mean action: 1.663 [0.000, 3.000],  loss: 9.914768, mae: 39.073914, mean_q: 28.640885, mean_eps: 0.683369
  70491/200000: episode: 735, duration: 1.340s, episode steps:  82, steps per second:  61, episode reward: -70.285, mean reward: -0.857 [-100.000, 10.966], mean action: 1.463 [0.000, 3.000],  loss: 9.251947, mae: 39.069123, mean_q: 29.267733, mean_eps: 0.682977
  70579/200000: episode: 736, duration: 1.034s, episode steps:  88, steps per second:  85, episode reward: -30.175, mean reward: -0.343 [-100.000, 20.077], mean action: 1.500 [0.000, 3.000],  loss: 3.744847, mae: 39.516096, mean_q: 31.703524, mean_eps: 0.682595
  70685/200000: episode: 737, duration: 0.922s, episode steps: 106, steps per second: 115, episode reward: -78.863, mean reward: -0.744 [-100.000,  7.070], mean action: 1.566 [0.000, 3.000],  loss: 6.744651, mae: 39.163066, mean_q: 29.083664, mean_eps: 0.682158
  70783/200000: episode: 738, duration: 0.842s, episode steps:  98, steps per second: 116, episode reward: -56.740, mean reward: -0.579 [-100.000, 15.515], mean action: 1.704 [0.000, 3.000],  loss: 10.379961, mae: 39.312417, mean_q: 30.664566, mean_eps: 0.681699
  70879/200000: episode: 739, duration: 0.830s, episode steps:  96, steps per second: 116, episode reward: 11.185, mean reward:  0.117 [-100.000, 21.038], mean action: 1.812 [0.000, 3.000],  loss: 8.321050, mae: 38.211607, mean_q: 29.160836, mean_eps: 0.681263
  70961/200000: episode: 740, duration: 0.673s, episode steps:  82, steps per second: 122, episode reward: -83.788, mean reward: -1.022 [-100.000, 14.574], mean action: 1.537 [0.000, 3.000],  loss: 6.690654, mae: 38.989054, mean_q: 28.860233, mean_eps: 0.680862
  71042/200000: episode: 741, duration: 0.876s, episode steps:  81, steps per second:  92, episode reward: -64.955, mean reward: -0.802 [-100.000, 15.101], mean action: 1.481 [0.000, 3.000],  loss: 3.441078, mae: 38.958526, mean_q: 29.556799, mean_eps: 0.680495
  71111/200000: episode: 742, duration: 0.730s, episode steps:  69, steps per second:  95, episode reward: -48.984, mean reward: -0.710 [-100.000,  6.725], mean action: 1.652 [0.000, 3.000],  loss: 7.491342, mae: 39.417573, mean_q: 29.653497, mean_eps: 0.680158
  71199/200000: episode: 743, duration: 0.903s, episode steps:  88, steps per second:  97, episode reward: -94.292, mean reward: -1.071 [-100.000,  7.557], mean action: 1.580 [0.000, 3.000],  loss: 3.868417, mae: 39.478323, mean_q: 30.740924, mean_eps: 0.679805
  71332/200000: episode: 744, duration: 2.039s, episode steps: 133, steps per second:  65, episode reward: -37.179, mean reward: -0.280 [-100.000, 11.245], mean action: 1.677 [0.000, 3.000],  loss: 11.877941, mae: 39.844685, mean_q: 31.022686, mean_eps: 0.679307
  71414/200000: episode: 745, duration: 1.138s, episode steps:  82, steps per second:  72, episode reward: -98.018, mean reward: -1.195 [-100.000,  9.901], mean action: 1.756 [0.000, 3.000],  loss: 9.510285, mae: 39.205733, mean_q: 29.729691, mean_eps: 0.678824
  71494/200000: episode: 746, duration: 0.984s, episode steps:  80, steps per second:  81, episode reward: -50.832, mean reward: -0.635 [-100.000, 10.320], mean action: 1.663 [0.000, 3.000],  loss: 7.844823, mae: 39.025571, mean_q: 29.191407, mean_eps: 0.678459
  71629/200000: episode: 747, duration: 1.216s, episode steps: 135, steps per second: 111, episode reward: -174.914, mean reward: -1.296 [-100.000, 52.183], mean action: 1.496 [0.000, 3.000],  loss: 7.039533, mae: 39.489782, mean_q: 30.063966, mean_eps: 0.677975
  71717/200000: episode: 748, duration: 0.753s, episode steps:  88, steps per second: 117, episode reward: -99.420, mean reward: -1.130 [-100.000,  6.560], mean action: 1.773 [0.000, 3.000],  loss: 2.563771, mae: 39.351837, mean_q: 29.982029, mean_eps: 0.677474
  71792/200000: episode: 749, duration: 0.654s, episode steps:  75, steps per second: 115, episode reward: -12.619, mean reward: -0.168 [-100.000, 21.662], mean action: 1.800 [0.000, 3.000],  loss: 3.800640, mae: 39.134557, mean_q: 28.650094, mean_eps: 0.677107
  71923/200000: episode: 750, duration: 1.226s, episode steps: 131, steps per second: 107, episode reward: -68.593, mean reward: -0.524 [-100.000, 14.600], mean action: 1.626 [0.000, 3.000],  loss: 7.186751, mae: 39.574286, mean_q: 29.419675, mean_eps: 0.676643
  72045/200000: episode: 751, duration: 1.038s, episode steps: 122, steps per second: 118, episode reward: -65.363, mean reward: -0.536 [-100.000, 13.626], mean action: 1.598 [0.000, 3.000],  loss: 11.481646, mae: 38.838440, mean_q: 29.677884, mean_eps: 0.676074
  72159/200000: episode: 752, duration: 1.038s, episode steps: 114, steps per second: 110, episode reward: -48.042, mean reward: -0.421 [-100.000, 13.320], mean action: 1.632 [0.000, 3.000],  loss: 5.719194, mae: 38.521507, mean_q: 30.552901, mean_eps: 0.675543
  72258/200000: episode: 753, duration: 0.889s, episode steps:  99, steps per second: 111, episode reward: -70.652, mean reward: -0.714 [-100.000, 11.787], mean action: 1.556 [0.000, 3.000],  loss: 5.768586, mae: 39.042477, mean_q: 29.922635, mean_eps: 0.675064
  72389/200000: episode: 754, duration: 2.084s, episode steps: 131, steps per second:  63, episode reward: -26.189, mean reward: -0.200 [-100.000, 13.448], mean action: 1.466 [0.000, 3.000],  loss: 7.571105, mae: 39.238030, mean_q: 29.347523, mean_eps: 0.674547
  72475/200000: episode: 755, duration: 0.818s, episode steps:  86, steps per second: 105, episode reward: -49.104, mean reward: -0.571 [-100.000,  7.981], mean action: 1.547 [0.000, 3.000],  loss: 10.540350, mae: 38.984383, mean_q: 29.959867, mean_eps: 0.674058
  72576/200000: episode: 756, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -51.780, mean reward: -0.513 [-100.000, 19.279], mean action: 1.495 [0.000, 3.000],  loss: 7.108194, mae: 38.985127, mean_q: 28.843437, mean_eps: 0.673638
  72674/200000: episode: 757, duration: 0.597s, episode steps:  98, steps per second: 164, episode reward: -198.314, mean reward: -2.024 [-100.000, 56.808], mean action: 1.684 [0.000, 3.000],  loss: 5.016427, mae: 39.259895, mean_q: 30.064795, mean_eps: 0.673190
  72770/200000: episode: 758, duration: 0.615s, episode steps:  96, steps per second: 156, episode reward: -34.646, mean reward: -0.361 [-100.000, 25.113], mean action: 1.677 [0.000, 3.000],  loss: 14.925279, mae: 38.857269, mean_q: 29.452567, mean_eps: 0.672753
  72907/200000: episode: 759, duration: 0.995s, episode steps: 137, steps per second: 138, episode reward: -99.147, mean reward: -0.724 [-100.000, 16.162], mean action: 1.737 [0.000, 3.000],  loss: 6.367835, mae: 38.490992, mean_q: 29.931973, mean_eps: 0.672229
  73005/200000: episode: 760, duration: 0.656s, episode steps:  98, steps per second: 149, episode reward: -115.143, mean reward: -1.175 [-100.000,  8.821], mean action: 1.459 [0.000, 3.000],  loss: 13.347694, mae: 39.253135, mean_q: 29.698604, mean_eps: 0.671700
  73106/200000: episode: 761, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -88.296, mean reward: -0.874 [-100.000, 12.112], mean action: 1.713 [0.000, 3.000],  loss: 6.037767, mae: 39.240990, mean_q: 29.341108, mean_eps: 0.671252
  73213/200000: episode: 762, duration: 0.663s, episode steps: 107, steps per second: 161, episode reward: -51.676, mean reward: -0.483 [-100.000,  8.642], mean action: 1.486 [0.000, 3.000],  loss: 6.744219, mae: 39.264939, mean_q: 30.373891, mean_eps: 0.670785
  73312/200000: episode: 763, duration: 0.667s, episode steps:  99, steps per second: 148, episode reward: -21.423, mean reward: -0.216 [-100.000, 10.460], mean action: 1.636 [0.000, 3.000],  loss: 9.483745, mae: 38.319245, mean_q: 29.194971, mean_eps: 0.670321
  73466/200000: episode: 764, duration: 1.321s, episode steps: 154, steps per second: 117, episode reward: -34.345, mean reward: -0.223 [-100.000, 13.383], mean action: 1.539 [0.000, 3.000],  loss: 5.340141, mae: 39.447739, mean_q: 30.034273, mean_eps: 0.669752
  73561/200000: episode: 765, duration: 0.657s, episode steps:  95, steps per second: 145, episode reward: -30.475, mean reward: -0.321 [-100.000, 19.716], mean action: 1.579 [0.000, 3.000],  loss: 6.694685, mae: 39.369416, mean_q: 28.540956, mean_eps: 0.669191
  73654/200000: episode: 766, duration: 0.596s, episode steps:  93, steps per second: 156, episode reward: -73.024, mean reward: -0.785 [-100.000, 20.255], mean action: 1.699 [0.000, 3.000],  loss: 10.098030, mae: 39.402444, mean_q: 29.575256, mean_eps: 0.668768
  73737/200000: episode: 767, duration: 0.601s, episode steps:  83, steps per second: 138, episode reward: -78.966, mean reward: -0.951 [-100.000, 13.663], mean action: 1.795 [0.000, 3.000],  loss: 5.504766, mae: 39.150223, mean_q: 28.053100, mean_eps: 0.668373
  73882/200000: episode: 768, duration: 1.499s, episode steps: 145, steps per second:  97, episode reward: -50.840, mean reward: -0.351 [-100.000, 12.848], mean action: 1.738 [0.000, 3.000],  loss: 6.841773, mae: 39.480118, mean_q: 29.034413, mean_eps: 0.667860
  74005/200000: episode: 769, duration: 0.898s, episode steps: 123, steps per second: 137, episode reward: -45.343, mean reward: -0.369 [-100.000, 36.727], mean action: 1.480 [0.000, 3.000],  loss: 6.144893, mae: 38.876952, mean_q: 29.572896, mean_eps: 0.667257
  74137/200000: episode: 770, duration: 0.822s, episode steps: 132, steps per second: 161, episode reward: -98.565, mean reward: -0.747 [-100.000, 13.451], mean action: 1.629 [0.000, 3.000],  loss: 5.138662, mae: 38.873703, mean_q: 29.038649, mean_eps: 0.666683
  74236/200000: episode: 771, duration: 0.598s, episode steps:  99, steps per second: 165, episode reward: -102.341, mean reward: -1.034 [-100.000,  9.381], mean action: 1.545 [0.000, 3.000],  loss: 11.682914, mae: 38.321313, mean_q: 28.819403, mean_eps: 0.666163
  74322/200000: episode: 772, duration: 0.677s, episode steps:  86, steps per second: 127, episode reward: -65.004, mean reward: -0.756 [-100.000, 13.063], mean action: 1.488 [0.000, 3.000],  loss: 7.468077, mae: 39.829662, mean_q: 27.664142, mean_eps: 0.665747
  74445/200000: episode: 773, duration: 1.012s, episode steps: 123, steps per second: 122, episode reward: -29.978, mean reward: -0.244 [-100.000, 26.827], mean action: 1.813 [0.000, 3.000],  loss: 5.362087, mae: 39.035638, mean_q: 28.636334, mean_eps: 0.665277
  74564/200000: episode: 774, duration: 0.856s, episode steps: 119, steps per second: 139, episode reward: -78.099, mean reward: -0.656 [-100.000,  8.323], mean action: 1.622 [0.000, 3.000],  loss: 4.999606, mae: 39.671658, mean_q: 28.915253, mean_eps: 0.664732
  74661/200000: episode: 775, duration: 0.621s, episode steps:  97, steps per second: 156, episode reward: -212.984, mean reward: -2.196 [-100.000, 42.395], mean action: 1.474 [0.000, 3.000],  loss: 11.903614, mae: 39.137094, mean_q: 28.968352, mean_eps: 0.664246
  74722/200000: episode: 776, duration: 0.406s, episode steps:  61, steps per second: 150, episode reward: -56.030, mean reward: -0.919 [-100.000,  6.687], mean action: 1.574 [0.000, 3.000],  loss: 5.331137, mae: 39.447362, mean_q: 30.339421, mean_eps: 0.663891
  74840/200000: episode: 777, duration: 0.768s, episode steps: 118, steps per second: 154, episode reward: -60.369, mean reward: -0.512 [-100.000,  8.038], mean action: 1.669 [0.000, 3.000],  loss: 7.814147, mae: 39.405479, mean_q: 28.314738, mean_eps: 0.663488
  74919/200000: episode: 778, duration: 0.571s, episode steps:  79, steps per second: 138, episode reward: -80.854, mean reward: -1.023 [-100.000,  9.532], mean action: 1.734 [0.000, 3.000],  loss: 6.446644, mae: 39.409281, mean_q: 30.862611, mean_eps: 0.663045
  75018/200000: episode: 779, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -60.819, mean reward: -0.614 [-100.000,  9.332], mean action: 1.838 [0.000, 3.000],  loss: 5.097725, mae: 38.886615, mean_q: 27.821390, mean_eps: 0.662644
  75149/200000: episode: 780, duration: 0.864s, episode steps: 131, steps per second: 152, episode reward: -250.506, mean reward: -1.912 [-100.000,  9.236], mean action: 1.481 [0.000, 3.000],  loss: 10.577992, mae: 39.342164, mean_q: 26.917688, mean_eps: 0.662127
  75232/200000: episode: 781, duration: 0.546s, episode steps:  83, steps per second: 152, episode reward:  0.041, mean reward:  0.000 [-100.000, 16.249], mean action: 1.602 [0.000, 3.000],  loss: 4.882148, mae: 39.675464, mean_q: 28.297868, mean_eps: 0.661645
  75356/200000: episode: 782, duration: 0.838s, episode steps: 124, steps per second: 148, episode reward: -70.995, mean reward: -0.573 [-100.000,  7.515], mean action: 1.653 [0.000, 3.000],  loss: 7.534566, mae: 39.887542, mean_q: 29.051662, mean_eps: 0.661179
  75431/200000: episode: 783, duration: 0.491s, episode steps:  75, steps per second: 153, episode reward: -49.235, mean reward: -0.656 [-100.000, 17.020], mean action: 1.747 [0.000, 3.000],  loss: 6.541477, mae: 39.502042, mean_q: 28.738161, mean_eps: 0.660732
  75502/200000: episode: 784, duration: 0.490s, episode steps:  71, steps per second: 145, episode reward: -43.384, mean reward: -0.611 [-100.000,  7.100], mean action: 1.732 [0.000, 3.000],  loss: 6.489350, mae: 39.357420, mean_q: 29.022837, mean_eps: 0.660403
  75632/200000: episode: 785, duration: 0.896s, episode steps: 130, steps per second: 145, episode reward: -51.616, mean reward: -0.397 [-100.000, 15.597], mean action: 1.362 [0.000, 3.000],  loss: 5.057776, mae: 39.045940, mean_q: 28.975463, mean_eps: 0.659951
  75716/200000: episode: 786, duration: 0.529s, episode steps:  84, steps per second: 159, episode reward: -123.640, mean reward: -1.472 [-100.000,  9.454], mean action: 1.964 [0.000, 3.000],  loss: 5.402192, mae: 38.754023, mean_q: 27.550102, mean_eps: 0.659469
  75797/200000: episode: 787, duration: 0.541s, episode steps:  81, steps per second: 150, episode reward: -114.731, mean reward: -1.416 [-100.000, 12.273], mean action: 1.741 [0.000, 3.000],  loss: 6.032584, mae: 39.806518, mean_q: 28.094869, mean_eps: 0.659098
  75902/200000: episode: 788, duration: 0.705s, episode steps: 105, steps per second: 149, episode reward: -62.947, mean reward: -0.599 [-100.000, 10.023], mean action: 1.581 [0.000, 3.000],  loss: 11.293822, mae: 39.421581, mean_q: 28.507629, mean_eps: 0.658680
  76033/200000: episode: 789, duration: 0.851s, episode steps: 131, steps per second: 154, episode reward: -81.108, mean reward: -0.619 [-100.000,  8.247], mean action: 1.733 [0.000, 3.000],  loss: 4.560737, mae: 39.698928, mean_q: 27.924666, mean_eps: 0.658149
  76189/200000: episode: 790, duration: 1.037s, episode steps: 156, steps per second: 150, episode reward: -52.900, mean reward: -0.339 [-100.000, 31.507], mean action: 1.673 [0.000, 3.000],  loss: 5.367347, mae: 39.504439, mean_q: 26.036444, mean_eps: 0.657503
  76333/200000: episode: 791, duration: 1.049s, episode steps: 144, steps per second: 137, episode reward:  1.168, mean reward:  0.008 [-100.000, 57.631], mean action: 1.597 [0.000, 3.000],  loss: 5.384534, mae: 39.471862, mean_q: 28.338045, mean_eps: 0.656828
  76433/200000: episode: 792, duration: 0.672s, episode steps: 100, steps per second: 149, episode reward: -87.508, mean reward: -0.875 [-100.000, 10.891], mean action: 1.650 [0.000, 3.000],  loss: 5.843950, mae: 39.734690, mean_q: 28.008143, mean_eps: 0.656279
  76513/200000: episode: 793, duration: 0.577s, episode steps:  80, steps per second: 139, episode reward: -32.725, mean reward: -0.409 [-100.000,  7.411], mean action: 1.488 [0.000, 3.000],  loss: 4.387634, mae: 39.644025, mean_q: 28.495567, mean_eps: 0.655874
  76579/200000: episode: 794, duration: 0.488s, episode steps:  66, steps per second: 135, episode reward: -39.800, mean reward: -0.603 [-100.000, 10.838], mean action: 1.712 [0.000, 3.000],  loss: 5.070704, mae: 39.702987, mean_q: 27.859569, mean_eps: 0.655545
  76684/200000: episode: 795, duration: 0.874s, episode steps: 105, steps per second: 120, episode reward: -96.564, mean reward: -0.920 [-100.000, 11.401], mean action: 1.638 [0.000, 3.000],  loss: 6.575420, mae: 39.794224, mean_q: 28.594765, mean_eps: 0.655160
  76789/200000: episode: 796, duration: 0.775s, episode steps: 105, steps per second: 136, episode reward: -56.750, mean reward: -0.540 [-100.000,  9.817], mean action: 1.610 [0.000, 3.000],  loss: 6.944900, mae: 39.488643, mean_q: 28.043347, mean_eps: 0.654688
  76882/200000: episode: 797, duration: 0.608s, episode steps:  93, steps per second: 153, episode reward: -115.113, mean reward: -1.238 [-100.000, 17.863], mean action: 1.860 [0.000, 3.000],  loss: 6.542443, mae: 40.355658, mean_q: 28.663322, mean_eps: 0.654243
  76994/200000: episode: 798, duration: 0.706s, episode steps: 112, steps per second: 159, episode reward: -52.749, mean reward: -0.471 [-100.000,  7.155], mean action: 1.464 [0.000, 3.000],  loss: 4.802637, mae: 39.901453, mean_q: 27.533454, mean_eps: 0.653781
  77090/200000: episode: 799, duration: 0.701s, episode steps:  96, steps per second: 137, episode reward: -98.603, mean reward: -1.027 [-100.000, 10.085], mean action: 1.771 [0.000, 3.000],  loss: 8.904764, mae: 39.792510, mean_q: 29.260802, mean_eps: 0.653313
  77205/200000: episode: 800, duration: 0.794s, episode steps: 115, steps per second: 145, episode reward: -30.893, mean reward: -0.269 [-100.000, 13.346], mean action: 1.617 [0.000, 3.000],  loss: 8.032715, mae: 39.165826, mean_q: 27.012172, mean_eps: 0.652838
  77307/200000: episode: 801, duration: 0.695s, episode steps: 102, steps per second: 147, episode reward: -77.942, mean reward: -0.764 [-100.000, 15.173], mean action: 1.814 [0.000, 3.000],  loss: 5.116958, mae: 39.923427, mean_q: 28.007453, mean_eps: 0.652350
  77397/200000: episode: 802, duration: 0.560s, episode steps:  90, steps per second: 161, episode reward: -4.309, mean reward: -0.048 [-100.000,  9.825], mean action: 1.678 [0.000, 3.000],  loss: 9.595323, mae: 39.955251, mean_q: 27.259590, mean_eps: 0.651918
  77490/200000: episode: 803, duration: 0.615s, episode steps:  93, steps per second: 151, episode reward: -91.612, mean reward: -0.985 [-100.000,  6.001], mean action: 1.452 [0.000, 3.000],  loss: 7.549810, mae: 40.114815, mean_q: 29.016224, mean_eps: 0.651506
  77604/200000: episode: 804, duration: 0.708s, episode steps: 114, steps per second: 161, episode reward: -11.934, mean reward: -0.105 [-100.000, 18.689], mean action: 1.623 [0.000, 3.000],  loss: 3.377295, mae: 40.037307, mean_q: 28.900042, mean_eps: 0.651041
  77711/200000: episode: 805, duration: 0.644s, episode steps: 107, steps per second: 166, episode reward: -62.523, mean reward: -0.584 [-100.000,  7.109], mean action: 1.523 [0.000, 3.000],  loss: 9.198615, mae: 39.960162, mean_q: 29.899390, mean_eps: 0.650543
  77809/200000: episode: 806, duration: 0.631s, episode steps:  98, steps per second: 155, episode reward: -70.277, mean reward: -0.717 [-100.000,  7.242], mean action: 1.735 [0.000, 3.000],  loss: 8.296930, mae: 39.619772, mean_q: 28.592955, mean_eps: 0.650082
  77928/200000: episode: 807, duration: 0.789s, episode steps: 119, steps per second: 151, episode reward: -29.540, mean reward: -0.248 [-100.000, 10.317], mean action: 1.597 [0.000, 3.000],  loss: 11.669492, mae: 40.127231, mean_q: 29.247214, mean_eps: 0.649594
  78054/200000: episode: 808, duration: 0.856s, episode steps: 126, steps per second: 147, episode reward: -22.659, mean reward: -0.180 [-100.000,  9.938], mean action: 1.587 [0.000, 3.000],  loss: 8.743186, mae: 39.734358, mean_q: 28.871357, mean_eps: 0.649043
  78141/200000: episode: 809, duration: 0.583s, episode steps:  87, steps per second: 149, episode reward: -123.028, mean reward: -1.414 [-100.000,  9.930], mean action: 1.287 [0.000, 3.000],  loss: 9.682983, mae: 40.078718, mean_q: 28.629137, mean_eps: 0.648564
  78216/200000: episode: 810, duration: 0.493s, episode steps:  75, steps per second: 152, episode reward: -75.151, mean reward: -1.002 [-100.000, 17.334], mean action: 1.680 [0.000, 3.000],  loss: 5.769730, mae: 40.075674, mean_q: 29.381669, mean_eps: 0.648199
  78284/200000: episode: 811, duration: 0.456s, episode steps:  68, steps per second: 149, episode reward: -70.202, mean reward: -1.032 [-100.000,  6.680], mean action: 1.441 [0.000, 3.000],  loss: 12.127075, mae: 40.369723, mean_q: 29.760221, mean_eps: 0.647877
  78378/200000: episode: 812, duration: 0.656s, episode steps:  94, steps per second: 143, episode reward: -100.453, mean reward: -1.069 [-100.000, 11.773], mean action: 1.830 [0.000, 3.000],  loss: 5.737987, mae: 40.152665, mean_q: 29.240796, mean_eps: 0.647513
  78462/200000: episode: 813, duration: 0.672s, episode steps:  84, steps per second: 125, episode reward: -56.441, mean reward: -0.672 [-100.000, 11.667], mean action: 1.726 [0.000, 3.000],  loss: 11.662126, mae: 39.932966, mean_q: 29.709780, mean_eps: 0.647112
  78562/200000: episode: 814, duration: 1.050s, episode steps: 100, steps per second:  95, episode reward: -26.043, mean reward: -0.260 [-100.000, 13.873], mean action: 1.680 [0.000, 3.000],  loss: 6.177124, mae: 40.335956, mean_q: 29.952899, mean_eps: 0.646698
  78682/200000: episode: 815, duration: 1.074s, episode steps: 120, steps per second: 112, episode reward: -70.194, mean reward: -0.585 [-100.000,  8.897], mean action: 1.708 [0.000, 3.000],  loss: 6.598248, mae: 40.155651, mean_q: 28.998349, mean_eps: 0.646203
  78781/200000: episode: 816, duration: 0.747s, episode steps:  99, steps per second: 133, episode reward: -5.673, mean reward: -0.057 [-100.000, 12.965], mean action: 1.768 [0.000, 3.000],  loss: 6.774132, mae: 40.370455, mean_q: 29.041250, mean_eps: 0.645710
  78891/200000: episode: 817, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -45.372, mean reward: -0.412 [-100.000, 10.880], mean action: 1.555 [0.000, 3.000],  loss: 4.517294, mae: 39.650913, mean_q: 28.821463, mean_eps: 0.645240
  78974/200000: episode: 818, duration: 0.606s, episode steps:  83, steps per second: 137, episode reward: -41.203, mean reward: -0.496 [-100.000, 26.210], mean action: 1.711 [0.000, 3.000],  loss: 5.368632, mae: 40.706879, mean_q: 29.735675, mean_eps: 0.644806
  79152/200000: episode: 819, duration: 1.190s, episode steps: 178, steps per second: 150, episode reward: 14.199, mean reward:  0.080 [-100.000, 14.727], mean action: 1.652 [0.000, 3.000],  loss: 5.312553, mae: 40.229270, mean_q: 29.771110, mean_eps: 0.644219
  79238/200000: episode: 820, duration: 0.555s, episode steps:  86, steps per second: 155, episode reward: -14.440, mean reward: -0.168 [-100.000, 21.082], mean action: 1.593 [0.000, 3.000],  loss: 13.686757, mae: 40.159541, mean_q: 29.938649, mean_eps: 0.643625
  79306/200000: episode: 821, duration: 0.466s, episode steps:  68, steps per second: 146, episode reward: -59.657, mean reward: -0.877 [-100.000, 11.429], mean action: 1.603 [0.000, 3.000],  loss: 3.661432, mae: 40.066045, mean_q: 27.827033, mean_eps: 0.643278
  79385/200000: episode: 822, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -60.878, mean reward: -0.771 [-100.000, 14.871], mean action: 1.810 [0.000, 3.000],  loss: 4.039576, mae: 40.016174, mean_q: 27.756705, mean_eps: 0.642947
  79505/200000: episode: 823, duration: 0.813s, episode steps: 120, steps per second: 148, episode reward: -45.746, mean reward: -0.381 [-100.000, 22.235], mean action: 1.667 [0.000, 3.000],  loss: 8.037982, mae: 40.576762, mean_q: 28.516510, mean_eps: 0.642500
  79592/200000: episode: 824, duration: 0.665s, episode steps:  87, steps per second: 131, episode reward: -51.753, mean reward: -0.595 [-100.000,  7.217], mean action: 1.667 [0.000, 3.000],  loss: 8.385449, mae: 40.582647, mean_q: 28.975512, mean_eps: 0.642034
  79674/200000: episode: 825, duration: 0.597s, episode steps:  82, steps per second: 137, episode reward: -55.722, mean reward: -0.680 [-100.000, 13.561], mean action: 1.659 [0.000, 3.000],  loss: 4.131679, mae: 40.120567, mean_q: 30.272093, mean_eps: 0.641654
  79768/200000: episode: 826, duration: 0.810s, episode steps:  94, steps per second: 116, episode reward: -42.516, mean reward: -0.452 [-100.000,  9.238], mean action: 1.702 [0.000, 3.000],  loss: 3.393103, mae: 40.370056, mean_q: 30.042653, mean_eps: 0.641258
  79871/200000: episode: 827, duration: 0.836s, episode steps: 103, steps per second: 123, episode reward: -122.558, mean reward: -1.190 [-100.000,  5.860], mean action: 1.602 [0.000, 3.000],  loss: 5.003976, mae: 40.529293, mean_q: 29.193628, mean_eps: 0.640814
  79976/200000: episode: 828, duration: 0.757s, episode steps: 105, steps per second: 139, episode reward: -225.706, mean reward: -2.150 [-100.000, 34.250], mean action: 1.733 [0.000, 3.000],  loss: 15.065697, mae: 40.949370, mean_q: 30.847839, mean_eps: 0.640347
  80103/200000: episode: 829, duration: 0.990s, episode steps: 127, steps per second: 128, episode reward: -44.930, mean reward: -0.354 [-100.000, 16.565], mean action: 1.622 [0.000, 3.000],  loss: 5.713989, mae: 41.325231, mean_q: 30.776402, mean_eps: 0.639825
  80326/200000: episode: 830, duration: 1.898s, episode steps: 223, steps per second: 118, episode reward: -43.787, mean reward: -0.196 [-100.000, 19.365], mean action: 1.632 [0.000, 3.000],  loss: 6.617452, mae: 40.425377, mean_q: 29.121787, mean_eps: 0.639037
  80436/200000: episode: 831, duration: 0.852s, episode steps: 110, steps per second: 129, episode reward: -76.359, mean reward: -0.694 [-100.000,  9.440], mean action: 1.573 [0.000, 3.000],  loss: 16.095147, mae: 40.519695, mean_q: 28.546418, mean_eps: 0.638288
  80580/200000: episode: 832, duration: 1.218s, episode steps: 144, steps per second: 118, episode reward:  3.392, mean reward:  0.024 [-100.000, 62.295], mean action: 1.736 [0.000, 3.000],  loss: 11.346624, mae: 40.716966, mean_q: 29.273793, mean_eps: 0.637716
  80669/200000: episode: 833, duration: 1.608s, episode steps:  89, steps per second:  55, episode reward: -90.190, mean reward: -1.013 [-100.000,  6.698], mean action: 1.629 [0.000, 3.000],  loss: 9.317461, mae: 40.939440, mean_q: 28.484856, mean_eps: 0.637192
  80808/200000: episode: 834, duration: 1.814s, episode steps: 139, steps per second:  77, episode reward: -174.200, mean reward: -1.253 [-100.000, 19.507], mean action: 1.612 [0.000, 3.000],  loss: 6.939314, mae: 40.485931, mean_q: 29.935074, mean_eps: 0.636679
  80930/200000: episode: 835, duration: 1.760s, episode steps: 122, steps per second:  69, episode reward: -74.185, mean reward: -0.608 [-100.000, 11.262], mean action: 1.508 [0.000, 3.000],  loss: 7.717620, mae: 40.086066, mean_q: 29.086359, mean_eps: 0.636092
  81083/200000: episode: 836, duration: 2.439s, episode steps: 153, steps per second:  63, episode reward: 17.055, mean reward:  0.111 [-100.000, 16.636], mean action: 1.765 [0.000, 3.000],  loss: 8.218216, mae: 40.007593, mean_q: 29.256424, mean_eps: 0.635473
  81246/200000: episode: 837, duration: 2.894s, episode steps: 163, steps per second:  56, episode reward: -9.230, mean reward: -0.057 [-100.000, 17.135], mean action: 1.558 [0.000, 3.000],  loss: 6.637680, mae: 40.362588, mean_q: 29.616290, mean_eps: 0.634762
  81356/200000: episode: 838, duration: 1.617s, episode steps: 110, steps per second:  68, episode reward: -75.055, mean reward: -0.682 [-100.000, 20.149], mean action: 1.827 [0.000, 3.000],  loss: 4.965987, mae: 40.002285, mean_q: 29.084572, mean_eps: 0.634148
  81495/200000: episode: 839, duration: 1.079s, episode steps: 139, steps per second: 129, episode reward: -198.530, mean reward: -1.428 [-100.000,  5.886], mean action: 1.561 [0.000, 3.000],  loss: 5.555245, mae: 40.489282, mean_q: 28.819203, mean_eps: 0.633587
  81581/200000: episode: 840, duration: 0.682s, episode steps:  86, steps per second: 126, episode reward: -23.539, mean reward: -0.274 [-100.000,  6.977], mean action: 1.640 [0.000, 3.000],  loss: 10.932388, mae: 40.404003, mean_q: 28.348915, mean_eps: 0.633081
  81692/200000: episode: 841, duration: 0.919s, episode steps: 111, steps per second: 121, episode reward: -55.670, mean reward: -0.502 [-100.000, 16.766], mean action: 1.649 [0.000, 3.000],  loss: 5.360884, mae: 39.308015, mean_q: 28.566419, mean_eps: 0.632638
  81823/200000: episode: 842, duration: 0.979s, episode steps: 131, steps per second: 134, episode reward: -133.138, mean reward: -1.016 [-100.000, 15.763], mean action: 1.519 [0.000, 3.000],  loss: 6.100691, mae: 40.141613, mean_q: 29.340471, mean_eps: 0.632093
  81938/200000: episode: 843, duration: 1.253s, episode steps: 115, steps per second:  92, episode reward: -129.898, mean reward: -1.130 [-100.000,  4.848], mean action: 1.426 [0.000, 3.000],  loss: 8.585546, mae: 40.328593, mean_q: 29.776837, mean_eps: 0.631540
  82045/200000: episode: 844, duration: 0.969s, episode steps: 107, steps per second: 110, episode reward: -76.048, mean reward: -0.711 [-100.000,  6.626], mean action: 1.645 [0.000, 3.000],  loss: 4.997555, mae: 39.938322, mean_q: 28.759435, mean_eps: 0.631041
  82187/200000: episode: 845, duration: 1.285s, episode steps: 142, steps per second: 110, episode reward: -90.681, mean reward: -0.639 [-100.000,  8.157], mean action: 1.606 [0.000, 3.000],  loss: 6.162612, mae: 40.198093, mean_q: 28.892055, mean_eps: 0.630480
  82259/200000: episode: 846, duration: 0.519s, episode steps:  72, steps per second: 139, episode reward: -53.441, mean reward: -0.742 [-100.000, 12.872], mean action: 1.708 [0.000, 3.000],  loss: 8.319293, mae: 40.386043, mean_q: 29.618197, mean_eps: 0.629999
  82421/200000: episode: 847, duration: 1.319s, episode steps: 162, steps per second: 123, episode reward: -76.376, mean reward: -0.471 [-100.000,  6.443], mean action: 1.765 [0.000, 3.000],  loss: 5.654758, mae: 40.135107, mean_q: 27.996763, mean_eps: 0.629472
  82502/200000: episode: 848, duration: 0.951s, episode steps:  81, steps per second:  85, episode reward: -110.126, mean reward: -1.360 [-100.000, 35.505], mean action: 1.716 [0.000, 3.000],  loss: 7.576065, mae: 40.383631, mean_q: 29.846681, mean_eps: 0.628926
  82623/200000: episode: 849, duration: 0.920s, episode steps: 121, steps per second: 131, episode reward: -30.760, mean reward: -0.254 [-100.000, 33.588], mean action: 1.554 [0.000, 3.000],  loss: 5.562964, mae: 40.379059, mean_q: 29.947867, mean_eps: 0.628471
  82702/200000: episode: 850, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward: -57.404, mean reward: -0.727 [-100.000, 12.486], mean action: 1.532 [0.000, 3.000],  loss: 5.873705, mae: 40.095463, mean_q: 27.516582, mean_eps: 0.628021
  82804/200000: episode: 851, duration: 0.676s, episode steps: 102, steps per second: 151, episode reward: -73.543, mean reward: -0.721 [-100.000, 10.960], mean action: 1.490 [0.000, 3.000],  loss: 8.023370, mae: 40.732192, mean_q: 27.358520, mean_eps: 0.627614
  82918/200000: episode: 852, duration: 0.844s, episode steps: 114, steps per second: 135, episode reward: -89.966, mean reward: -0.789 [-100.000, 28.069], mean action: 1.430 [0.000, 3.000],  loss: 7.197615, mae: 40.383143, mean_q: 29.599080, mean_eps: 0.627128
  83093/200000: episode: 853, duration: 1.217s, episode steps: 175, steps per second: 144, episode reward: -33.395, mean reward: -0.191 [-100.000, 15.623], mean action: 1.554 [0.000, 3.000],  loss: 6.259074, mae: 39.953764, mean_q: 29.012717, mean_eps: 0.626478
  83242/200000: episode: 854, duration: 1.144s, episode steps: 149, steps per second: 130, episode reward: -203.244, mean reward: -1.364 [-100.000, 40.379], mean action: 1.644 [0.000, 3.000],  loss: 9.346789, mae: 40.485956, mean_q: 28.209442, mean_eps: 0.625748
  83374/200000: episode: 855, duration: 1.022s, episode steps: 132, steps per second: 129, episode reward: -77.921, mean reward: -0.590 [-100.000,  6.683], mean action: 1.712 [0.000, 3.000],  loss: 7.909099, mae: 40.152504, mean_q: 28.970287, mean_eps: 0.625116
  83467/200000: episode: 856, duration: 0.652s, episode steps:  93, steps per second: 143, episode reward: -46.904, mean reward: -0.504 [-100.000,  7.087], mean action: 1.731 [0.000, 3.000],  loss: 9.593534, mae: 40.284181, mean_q: 26.861130, mean_eps: 0.624610
  83573/200000: episode: 857, duration: 0.765s, episode steps: 106, steps per second: 139, episode reward: -70.476, mean reward: -0.665 [-100.000, 15.932], mean action: 1.708 [0.000, 3.000],  loss: 6.541159, mae: 40.025773, mean_q: 28.632349, mean_eps: 0.624162
  83800/200000: episode: 858, duration: 1.625s, episode steps: 227, steps per second: 140, episode reward:  6.706, mean reward:  0.030 [-100.000, 16.504], mean action: 1.577 [0.000, 3.000],  loss: 7.699132, mae: 40.353931, mean_q: 28.803537, mean_eps: 0.623413
  83912/200000: episode: 859, duration: 0.834s, episode steps: 112, steps per second: 134, episode reward: -87.621, mean reward: -0.782 [-100.000,  7.495], mean action: 1.812 [0.000, 3.000],  loss: 6.211227, mae: 40.928169, mean_q: 28.089345, mean_eps: 0.622650
  84026/200000: episode: 860, duration: 0.837s, episode steps: 114, steps per second: 136, episode reward: -91.498, mean reward: -0.803 [-100.000,  5.982], mean action: 1.588 [0.000, 3.000],  loss: 6.825308, mae: 39.673593, mean_q: 28.887229, mean_eps: 0.622142
  84173/200000: episode: 861, duration: 1.114s, episode steps: 147, steps per second: 132, episode reward: -8.564, mean reward: -0.058 [-100.000, 12.645], mean action: 1.327 [0.000, 3.000],  loss: 8.924555, mae: 39.076067, mean_q: 28.438117, mean_eps: 0.621555
  84295/200000: episode: 862, duration: 0.874s, episode steps: 122, steps per second: 140, episode reward: 30.367, mean reward:  0.249 [-100.000, 75.602], mean action: 1.623 [0.000, 3.000],  loss: 5.410554, mae: 39.141589, mean_q: 28.127914, mean_eps: 0.620949
  84496/200000: episode: 863, duration: 1.524s, episode steps: 201, steps per second: 132, episode reward: 37.586, mean reward:  0.187 [-100.000, 16.573], mean action: 1.687 [0.000, 3.000],  loss: 8.636120, mae: 39.191366, mean_q: 28.672553, mean_eps: 0.620222
  84630/200000: episode: 864, duration: 1.026s, episode steps: 134, steps per second: 131, episode reward: -60.096, mean reward: -0.448 [-100.000, 15.244], mean action: 1.799 [0.000, 3.000],  loss: 4.080434, mae: 39.099375, mean_q: 29.624769, mean_eps: 0.619469
  84771/200000: episode: 865, duration: 1.076s, episode steps: 141, steps per second: 131, episode reward: -14.112, mean reward: -0.100 [-100.000, 13.863], mean action: 1.652 [0.000, 3.000],  loss: 13.885802, mae: 39.088672, mean_q: 29.769439, mean_eps: 0.618850
  84866/200000: episode: 866, duration: 0.654s, episode steps:  95, steps per second: 145, episode reward: -116.853, mean reward: -1.230 [-100.000, 19.066], mean action: 1.632 [0.000, 3.000],  loss: 9.307777, mae: 38.950941, mean_q: 28.790708, mean_eps: 0.618319
  84973/200000: episode: 867, duration: 0.792s, episode steps: 107, steps per second: 135, episode reward: -67.673, mean reward: -0.632 [-100.000,  7.238], mean action: 1.822 [0.000, 3.000],  loss: 10.576758, mae: 39.343469, mean_q: 29.640539, mean_eps: 0.617865
  85071/200000: episode: 868, duration: 0.753s, episode steps:  98, steps per second: 130, episode reward: -63.084, mean reward: -0.644 [-100.000, 12.494], mean action: 1.755 [0.000, 3.000],  loss: 11.167052, mae: 39.759851, mean_q: 29.195572, mean_eps: 0.617403
  85180/200000: episode: 869, duration: 0.794s, episode steps: 109, steps per second: 137, episode reward: -88.602, mean reward: -0.813 [-100.000,  6.137], mean action: 1.596 [0.000, 3.000],  loss: 8.065007, mae: 39.397881, mean_q: 30.858308, mean_eps: 0.616937
  85326/200000: episode: 870, duration: 1.053s, episode steps: 146, steps per second: 139, episode reward: -77.517, mean reward: -0.531 [-100.000,  8.132], mean action: 1.575 [0.000, 3.000],  loss: 5.476476, mae: 39.190477, mean_q: 30.204204, mean_eps: 0.616364
  85453/200000: episode: 871, duration: 0.968s, episode steps: 127, steps per second: 131, episode reward: -20.263, mean reward: -0.160 [-100.000, 13.890], mean action: 1.646 [0.000, 3.000],  loss: 4.982958, mae: 39.696927, mean_q: 30.191899, mean_eps: 0.615750
  85567/200000: episode: 872, duration: 0.933s, episode steps: 114, steps per second: 122, episode reward: -95.171, mean reward: -0.835 [-100.000,  7.151], mean action: 1.658 [0.000, 3.000],  loss: 5.287971, mae: 39.691855, mean_q: 29.733502, mean_eps: 0.615207
  85674/200000: episode: 873, duration: 0.777s, episode steps: 107, steps per second: 138, episode reward: -70.775, mean reward: -0.661 [-100.000,  6.767], mean action: 1.570 [0.000, 3.000],  loss: 8.451555, mae: 38.822904, mean_q: 28.632045, mean_eps: 0.614710
  85794/200000: episode: 874, duration: 0.872s, episode steps: 120, steps per second: 138, episode reward: -62.073, mean reward: -0.517 [-100.000,  6.715], mean action: 1.550 [0.000, 3.000],  loss: 7.179556, mae: 39.366920, mean_q: 29.271764, mean_eps: 0.614199
  85910/200000: episode: 875, duration: 0.886s, episode steps: 116, steps per second: 131, episode reward: -191.539, mean reward: -1.651 [-100.000, 13.701], mean action: 1.457 [0.000, 3.000],  loss: 6.081814, mae: 39.036310, mean_q: 29.663613, mean_eps: 0.613668
  86015/200000: episode: 876, duration: 0.765s, episode steps: 105, steps per second: 137, episode reward: -45.478, mean reward: -0.433 [-100.000, 18.336], mean action: 1.714 [0.000, 3.000],  loss: 8.174545, mae: 39.342003, mean_q: 30.800495, mean_eps: 0.613171
  86110/200000: episode: 877, duration: 0.920s, episode steps:  95, steps per second: 103, episode reward: 10.338, mean reward:  0.109 [-100.000, 18.047], mean action: 1.726 [0.000, 3.000],  loss: 4.606177, mae: 39.481762, mean_q: 30.694155, mean_eps: 0.612721
  86196/200000: episode: 878, duration: 0.878s, episode steps:  86, steps per second:  98, episode reward: -32.354, mean reward: -0.376 [-100.000, 16.075], mean action: 1.802 [0.000, 3.000],  loss: 3.544512, mae: 39.038036, mean_q: 31.922088, mean_eps: 0.612314
  86321/200000: episode: 879, duration: 1.633s, episode steps: 125, steps per second:  77, episode reward: -51.995, mean reward: -0.416 [-100.000,  6.594], mean action: 1.640 [0.000, 3.000],  loss: 6.361392, mae: 39.285298, mean_q: 29.647367, mean_eps: 0.611839
  86420/200000: episode: 880, duration: 0.922s, episode steps:  99, steps per second: 107, episode reward: -9.388, mean reward: -0.095 [-100.000, 23.078], mean action: 1.444 [0.000, 3.000],  loss: 14.355123, mae: 39.980165, mean_q: 29.384553, mean_eps: 0.611335
  86730/200000: episode: 881, duration: 2.737s, episode steps: 310, steps per second: 113, episode reward: -20.917, mean reward: -0.067 [-100.000, 53.673], mean action: 1.681 [0.000, 3.000],  loss: 6.735225, mae: 39.603235, mean_q: 30.209491, mean_eps: 0.610415
  86854/200000: episode: 882, duration: 1.033s, episode steps: 124, steps per second: 120, episode reward: -63.775, mean reward: -0.514 [-100.000,  9.451], mean action: 1.492 [0.000, 3.000],  loss: 7.026709, mae: 39.714265, mean_q: 29.551873, mean_eps: 0.609438
  87007/200000: episode: 883, duration: 1.609s, episode steps: 153, steps per second:  95, episode reward: -27.093, mean reward: -0.177 [-100.000, 17.112], mean action: 1.582 [0.000, 3.000],  loss: 7.474183, mae: 39.867380, mean_q: 29.780434, mean_eps: 0.608815
  87123/200000: episode: 884, duration: 1.074s, episode steps: 116, steps per second: 108, episode reward: -66.508, mean reward: -0.573 [-100.000, 13.743], mean action: 1.612 [0.000, 3.000],  loss: 4.275208, mae: 39.384852, mean_q: 29.975469, mean_eps: 0.608210
  87196/200000: episode: 885, duration: 0.572s, episode steps:  73, steps per second: 128, episode reward: -100.013, mean reward: -1.370 [-100.000, 22.321], mean action: 1.644 [0.000, 3.000],  loss: 6.699406, mae: 38.988560, mean_q: 29.878130, mean_eps: 0.607784
  87300/200000: episode: 886, duration: 1.069s, episode steps: 104, steps per second:  97, episode reward: -93.600, mean reward: -0.900 [-100.000,  8.478], mean action: 1.788 [0.000, 3.000],  loss: 6.055382, mae: 39.917857, mean_q: 31.118981, mean_eps: 0.607386
  87415/200000: episode: 887, duration: 0.862s, episode steps: 115, steps per second: 133, episode reward: -57.643, mean reward: -0.501 [-100.000, 13.332], mean action: 1.609 [0.000, 3.000],  loss: 8.125565, mae: 39.107638, mean_q: 29.078442, mean_eps: 0.606894
  87499/200000: episode: 888, duration: 0.663s, episode steps:  84, steps per second: 127, episode reward: -73.361, mean reward: -0.873 [-100.000, 10.465], mean action: 1.738 [0.000, 3.000],  loss: 7.220594, mae: 40.156059, mean_q: 31.342823, mean_eps: 0.606446
  87643/200000: episode: 889, duration: 0.997s, episode steps: 144, steps per second: 145, episode reward: -163.813, mean reward: -1.138 [-100.000, 30.440], mean action: 1.507 [0.000, 3.000],  loss: 4.802664, mae: 39.060719, mean_q: 30.342912, mean_eps: 0.605933
  87726/200000: episode: 890, duration: 0.814s, episode steps:  83, steps per second: 102, episode reward: -84.776, mean reward: -1.021 [-100.000,  9.344], mean action: 1.651 [0.000, 3.000],  loss: 5.958059, mae: 40.036811, mean_q: 30.837024, mean_eps: 0.605422
  87856/200000: episode: 891, duration: 1.338s, episode steps: 130, steps per second:  97, episode reward: -49.585, mean reward: -0.381 [-100.000,  8.543], mean action: 1.577 [0.000, 3.000],  loss: 6.111777, mae: 39.419618, mean_q: 30.099381, mean_eps: 0.604943
  87925/200000: episode: 892, duration: 0.532s, episode steps:  69, steps per second: 130, episode reward: -58.989, mean reward: -0.855 [-100.000,  8.758], mean action: 1.913 [0.000, 3.000],  loss: 5.925998, mae: 39.225649, mean_q: 28.314195, mean_eps: 0.604495
  88067/200000: episode: 893, duration: 1.826s, episode steps: 142, steps per second:  78, episode reward: -50.759, mean reward: -0.357 [-100.000, 12.984], mean action: 1.641 [0.000, 3.000],  loss: 7.779461, mae: 39.998896, mean_q: 29.850519, mean_eps: 0.604020
  88163/200000: episode: 894, duration: 0.776s, episode steps:  96, steps per second: 124, episode reward: -33.047, mean reward: -0.344 [-100.000, 10.930], mean action: 1.729 [0.000, 3.000],  loss: 4.343945, mae: 40.047686, mean_q: 30.464911, mean_eps: 0.603485
  88272/200000: episode: 895, duration: 1.025s, episode steps: 109, steps per second: 106, episode reward: -58.194, mean reward: -0.534 [-100.000,  9.510], mean action: 1.541 [0.000, 3.000],  loss: 8.372833, mae: 40.504293, mean_q: 30.524973, mean_eps: 0.603023
  88407/200000: episode: 896, duration: 0.985s, episode steps: 135, steps per second: 137, episode reward: -175.535, mean reward: -1.300 [-100.000,  5.956], mean action: 1.748 [0.000, 3.000],  loss: 4.639678, mae: 40.267602, mean_q: 31.247581, mean_eps: 0.602475
  88510/200000: episode: 897, duration: 0.792s, episode steps: 103, steps per second: 130, episode reward: -64.374, mean reward: -0.625 [-100.000, 13.861], mean action: 1.738 [0.000, 3.000],  loss: 5.213192, mae: 39.725811, mean_q: 29.235817, mean_eps: 0.601939
  88675/200000: episode: 898, duration: 1.496s, episode steps: 165, steps per second: 110, episode reward: -72.566, mean reward: -0.440 [-100.000, 10.045], mean action: 1.636 [0.000, 3.000],  loss: 4.664235, mae: 39.139403, mean_q: 29.666617, mean_eps: 0.601336
  88814/200000: episode: 899, duration: 1.357s, episode steps: 139, steps per second: 102, episode reward: -32.683, mean reward: -0.235 [-100.000,  7.494], mean action: 1.511 [0.000, 3.000],  loss: 9.058713, mae: 40.467157, mean_q: 31.384000, mean_eps: 0.600652
  88943/200000: episode: 900, duration: 1.352s, episode steps: 129, steps per second:  95, episode reward: -36.492, mean reward: -0.283 [-100.000, 15.225], mean action: 1.605 [0.000, 3.000],  loss: 9.014519, mae: 39.696935, mean_q: 30.043346, mean_eps: 0.600049
  89056/200000: episode: 901, duration: 1.044s, episode steps: 113, steps per second: 108, episode reward: -24.051, mean reward: -0.213 [-100.000, 15.283], mean action: 1.735 [0.000, 3.000],  loss: 7.636682, mae: 39.333394, mean_q: 30.109073, mean_eps: 0.599505
  89147/200000: episode: 902, duration: 0.760s, episode steps:  91, steps per second: 120, episode reward: -36.291, mean reward: -0.399 [-100.000, 10.753], mean action: 1.703 [0.000, 3.000],  loss: 11.768409, mae: 39.206260, mean_q: 29.622007, mean_eps: 0.599046
  89269/200000: episode: 903, duration: 1.028s, episode steps: 122, steps per second: 119, episode reward: -34.408, mean reward: -0.282 [-100.000,  7.179], mean action: 1.615 [0.000, 3.000],  loss: 11.612567, mae: 40.056094, mean_q: 29.952682, mean_eps: 0.598566
  89379/200000: episode: 904, duration: 0.868s, episode steps: 110, steps per second: 127, episode reward: -71.175, mean reward: -0.647 [-100.000,  9.814], mean action: 1.627 [0.000, 3.000],  loss: 6.470739, mae: 39.814150, mean_q: 29.682111, mean_eps: 0.598044
  89773/200000: episode: 905, duration: 3.298s, episode steps: 394, steps per second: 119, episode reward: -118.074, mean reward: -0.300 [-100.000, 13.746], mean action: 1.744 [0.000, 3.000],  loss: 5.864922, mae: 39.639090, mean_q: 30.276856, mean_eps: 0.596910
  89874/200000: episode: 906, duration: 0.792s, episode steps: 101, steps per second: 128, episode reward: -41.035, mean reward: -0.406 [-100.000,  9.155], mean action: 1.564 [0.000, 3.000],  loss: 7.411373, mae: 38.735116, mean_q: 30.537945, mean_eps: 0.595796
  90007/200000: episode: 907, duration: 1.316s, episode steps: 133, steps per second: 101, episode reward: -20.800, mean reward: -0.156 [-100.000, 20.684], mean action: 1.699 [0.000, 3.000],  loss: 3.769547, mae: 38.870715, mean_q: 30.150490, mean_eps: 0.595270
  90176/200000: episode: 908, duration: 1.783s, episode steps: 169, steps per second:  95, episode reward: -12.971, mean reward: -0.077 [-100.000, 11.569], mean action: 1.716 [0.000, 3.000],  loss: 5.121282, mae: 39.923218, mean_q: 30.242113, mean_eps: 0.594591
  90295/200000: episode: 909, duration: 0.798s, episode steps: 119, steps per second: 149, episode reward: -64.937, mean reward: -0.546 [-100.000,  9.815], mean action: 1.613 [0.000, 3.000],  loss: 4.982963, mae: 39.505410, mean_q: 30.866214, mean_eps: 0.593943
  90411/200000: episode: 910, duration: 0.835s, episode steps: 116, steps per second: 139, episode reward: -54.436, mean reward: -0.469 [-100.000,  6.796], mean action: 1.629 [0.000, 3.000],  loss: 3.889582, mae: 39.148269, mean_q: 29.174133, mean_eps: 0.593414
  90611/200000: episode: 911, duration: 1.915s, episode steps: 200, steps per second: 104, episode reward: -117.306, mean reward: -0.587 [-100.000,  7.793], mean action: 1.600 [0.000, 3.000],  loss: 5.812857, mae: 39.375526, mean_q: 29.065842, mean_eps: 0.592703
  90721/200000: episode: 912, duration: 0.939s, episode steps: 110, steps per second: 117, episode reward: -70.594, mean reward: -0.642 [-100.000,  6.052], mean action: 1.855 [0.000, 3.000],  loss: 7.110766, mae: 39.212555, mean_q: 29.597147, mean_eps: 0.592005
  90851/200000: episode: 913, duration: 0.910s, episode steps: 130, steps per second: 143, episode reward: -88.628, mean reward: -0.682 [-100.000,  9.774], mean action: 1.723 [0.000, 3.000],  loss: 6.502915, mae: 39.571602, mean_q: 29.012292, mean_eps: 0.591465
  91008/200000: episode: 914, duration: 1.042s, episode steps: 157, steps per second: 151, episode reward: -75.790, mean reward: -0.483 [-100.000, 12.544], mean action: 1.643 [0.000, 3.000],  loss: 5.212380, mae: 40.040194, mean_q: 30.498891, mean_eps: 0.590820
  91129/200000: episode: 915, duration: 0.825s, episode steps: 121, steps per second: 147, episode reward: 26.405, mean reward:  0.218 [-100.000, 21.099], mean action: 1.678 [0.000, 3.000],  loss: 6.863353, mae: 39.372876, mean_q: 30.230705, mean_eps: 0.590194
  91270/200000: episode: 916, duration: 0.989s, episode steps: 141, steps per second: 143, episode reward: -69.641, mean reward: -0.494 [-100.000,  8.854], mean action: 1.702 [0.000, 3.000],  loss: 5.466135, mae: 39.776611, mean_q: 30.309783, mean_eps: 0.589604
  91384/200000: episode: 917, duration: 1.184s, episode steps: 114, steps per second:  96, episode reward: -74.439, mean reward: -0.653 [-100.000, 21.813], mean action: 1.711 [0.000, 3.000],  loss: 4.968681, mae: 39.481121, mean_q: 28.436069, mean_eps: 0.589031
  91523/200000: episode: 918, duration: 0.903s, episode steps: 139, steps per second: 154, episode reward: -73.620, mean reward: -0.530 [-100.000, 12.073], mean action: 1.540 [0.000, 3.000],  loss: 6.485802, mae: 39.115202, mean_q: 28.914734, mean_eps: 0.588462
  91688/200000: episode: 919, duration: 1.507s, episode steps: 165, steps per second: 110, episode reward: -88.850, mean reward: -0.538 [-100.000,  6.462], mean action: 1.618 [0.000, 3.000],  loss: 6.709393, mae: 39.412538, mean_q: 28.930164, mean_eps: 0.587777
  91767/200000: episode: 920, duration: 0.628s, episode steps:  79, steps per second: 126, episode reward: -33.260, mean reward: -0.421 [-100.000,  7.832], mean action: 1.544 [0.000, 3.000],  loss: 7.735825, mae: 39.218575, mean_q: 27.100804, mean_eps: 0.587228
  91844/200000: episode: 921, duration: 0.533s, episode steps:  77, steps per second: 145, episode reward: -74.038, mean reward: -0.962 [-100.000, 18.768], mean action: 1.844 [0.000, 3.000],  loss: 5.296853, mae: 39.467753, mean_q: 30.707239, mean_eps: 0.586877
  91948/200000: episode: 922, duration: 0.667s, episode steps: 104, steps per second: 156, episode reward: -78.293, mean reward: -0.753 [-100.000, 11.840], mean action: 1.663 [0.000, 3.000],  loss: 6.129131, mae: 39.560588, mean_q: 28.095503, mean_eps: 0.586470
  92046/200000: episode: 923, duration: 0.947s, episode steps:  98, steps per second: 103, episode reward: -48.245, mean reward: -0.492 [-100.000, 19.199], mean action: 1.673 [0.000, 3.000],  loss: 9.329314, mae: 39.515849, mean_q: 30.819657, mean_eps: 0.586016
  92197/200000: episode: 924, duration: 1.256s, episode steps: 151, steps per second: 120, episode reward: -66.261, mean reward: -0.439 [-100.000, 20.440], mean action: 1.642 [0.000, 3.000],  loss: 9.251280, mae: 39.524872, mean_q: 28.972313, mean_eps: 0.585456
  92263/200000: episode: 925, duration: 0.456s, episode steps:  66, steps per second: 145, episode reward: -19.275, mean reward: -0.292 [-100.000, 17.934], mean action: 1.530 [0.000, 3.000],  loss: 4.296659, mae: 38.686767, mean_q: 27.402711, mean_eps: 0.584967
  92368/200000: episode: 926, duration: 0.651s, episode steps: 105, steps per second: 161, episode reward: -79.052, mean reward: -0.753 [-100.000, 12.827], mean action: 1.524 [0.000, 3.000],  loss: 5.248520, mae: 38.583582, mean_q: 27.759101, mean_eps: 0.584583
  92597/200000: episode: 927, duration: 1.957s, episode steps: 229, steps per second: 117, episode reward: -159.348, mean reward: -0.696 [-100.000, 18.290], mean action: 1.677 [0.000, 3.000],  loss: 4.627115, mae: 38.943619, mean_q: 29.528016, mean_eps: 0.583831
  92741/200000: episode: 928, duration: 1.335s, episode steps: 144, steps per second: 108, episode reward: -13.590, mean reward: -0.094 [-100.000,  8.311], mean action: 1.667 [0.000, 3.000],  loss: 10.673568, mae: 38.516696, mean_q: 29.920438, mean_eps: 0.582992
  93028/200000: episode: 929, duration: 3.057s, episode steps: 287, steps per second:  94, episode reward: -264.971, mean reward: -0.923 [-100.000, 47.073], mean action: 1.641 [0.000, 3.000],  loss: 7.089892, mae: 38.434992, mean_q: 29.071284, mean_eps: 0.582022
  93139/200000: episode: 930, duration: 0.812s, episode steps: 111, steps per second: 137, episode reward: -83.135, mean reward: -0.749 [-100.000, 12.164], mean action: 1.532 [0.000, 3.000],  loss: 4.840311, mae: 38.590235, mean_q: 30.991130, mean_eps: 0.581126
  93292/200000: episode: 931, duration: 1.020s, episode steps: 153, steps per second: 150, episode reward: -34.283, mean reward: -0.224 [-100.000, 13.090], mean action: 1.693 [0.000, 3.000],  loss: 6.719504, mae: 38.741423, mean_q: 28.331985, mean_eps: 0.580533
  93404/200000: episode: 932, duration: 0.847s, episode steps: 112, steps per second: 132, episode reward: -12.846, mean reward: -0.115 [-100.000, 19.900], mean action: 1.741 [0.000, 3.000],  loss: 5.866187, mae: 38.381565, mean_q: 28.089014, mean_eps: 0.579936
  93499/200000: episode: 933, duration: 0.714s, episode steps:  95, steps per second: 133, episode reward: -53.320, mean reward: -0.561 [-100.000,  9.085], mean action: 1.716 [0.000, 3.000],  loss: 3.864599, mae: 38.523120, mean_q: 28.948528, mean_eps: 0.579471
  93667/200000: episode: 934, duration: 1.182s, episode steps: 168, steps per second: 142, episode reward: -283.474, mean reward: -1.687 [-100.000, 28.325], mean action: 1.548 [0.000, 3.000],  loss: 7.307306, mae: 38.689326, mean_q: 27.725253, mean_eps: 0.578879
  93817/200000: episode: 935, duration: 1.032s, episode steps: 150, steps per second: 145, episode reward: -82.867, mean reward: -0.552 [-100.000,  7.288], mean action: 1.613 [0.000, 3.000],  loss: 11.884409, mae: 38.389159, mean_q: 28.518953, mean_eps: 0.578163
  93945/200000: episode: 936, duration: 0.905s, episode steps: 128, steps per second: 141, episode reward: -52.846, mean reward: -0.413 [-100.000,  7.511], mean action: 1.492 [0.000, 3.000],  loss: 8.705772, mae: 38.837656, mean_q: 29.784577, mean_eps: 0.577538
  94038/200000: episode: 937, duration: 0.905s, episode steps:  93, steps per second: 103, episode reward: -88.256, mean reward: -0.949 [-100.000,  5.259], mean action: 1.710 [0.000, 3.000],  loss: 6.968573, mae: 38.636964, mean_q: 29.615639, mean_eps: 0.577040
  94181/200000: episode: 938, duration: 1.819s, episode steps: 143, steps per second:  79, episode reward: -20.332, mean reward: -0.142 [-100.000,  8.819], mean action: 1.685 [0.000, 3.000],  loss: 6.783905, mae: 38.548262, mean_q: 28.722799, mean_eps: 0.576509
  94306/200000: episode: 939, duration: 1.238s, episode steps: 125, steps per second: 101, episode reward: -17.812, mean reward: -0.142 [-100.000, 15.994], mean action: 1.608 [0.000, 3.000],  loss: 7.921073, mae: 38.172334, mean_q: 29.368596, mean_eps: 0.575906
  94428/200000: episode: 940, duration: 1.038s, episode steps: 122, steps per second: 117, episode reward: -39.814, mean reward: -0.326 [-100.000, 10.346], mean action: 1.672 [0.000, 3.000],  loss: 6.025368, mae: 37.884036, mean_q: 28.224081, mean_eps: 0.575351
  94554/200000: episode: 941, duration: 1.053s, episode steps: 126, steps per second: 120, episode reward: 23.945, mean reward:  0.190 [-100.000, 14.921], mean action: 1.706 [0.000, 3.000],  loss: 6.274572, mae: 37.436242, mean_q: 28.059114, mean_eps: 0.574793
  94650/200000: episode: 942, duration: 0.728s, episode steps:  96, steps per second: 132, episode reward: -34.951, mean reward: -0.364 [-100.000, 10.943], mean action: 1.802 [0.000, 3.000],  loss: 4.349907, mae: 38.251034, mean_q: 30.512997, mean_eps: 0.574293
  94766/200000: episode: 943, duration: 0.901s, episode steps: 116, steps per second: 129, episode reward: -30.892, mean reward: -0.266 [-100.000, 10.251], mean action: 1.681 [0.000, 3.000],  loss: 6.381238, mae: 38.134281, mean_q: 30.132333, mean_eps: 0.573816
  94974/200000: episode: 944, duration: 1.616s, episode steps: 208, steps per second: 129, episode reward: -69.651, mean reward: -0.335 [-100.000, 21.265], mean action: 1.755 [0.000, 3.000],  loss: 5.759410, mae: 37.923474, mean_q: 29.891634, mean_eps: 0.573087
  95103/200000: episode: 945, duration: 1.516s, episode steps: 129, steps per second:  85, episode reward: -44.755, mean reward: -0.347 [-100.000, 26.069], mean action: 1.721 [0.000, 3.000],  loss: 5.361989, mae: 37.732454, mean_q: 29.948193, mean_eps: 0.572329
  95223/200000: episode: 946, duration: 1.488s, episode steps: 120, steps per second:  81, episode reward: -58.448, mean reward: -0.487 [-100.000, 11.540], mean action: 1.558 [0.000, 3.000],  loss: 6.042557, mae: 38.513426, mean_q: 29.340327, mean_eps: 0.571769
  95339/200000: episode: 947, duration: 0.962s, episode steps: 116, steps per second: 121, episode reward: -20.883, mean reward: -0.180 [-100.000, 10.418], mean action: 1.345 [0.000, 3.000],  loss: 7.112840, mae: 37.940305, mean_q: 30.647377, mean_eps: 0.571238
  95408/200000: episode: 948, duration: 0.574s, episode steps:  69, steps per second: 120, episode reward: -73.473, mean reward: -1.065 [-100.000, 17.862], mean action: 1.565 [0.000, 3.000],  loss: 6.800207, mae: 37.635223, mean_q: 28.813566, mean_eps: 0.570822
  95505/200000: episode: 949, duration: 0.789s, episode steps:  97, steps per second: 123, episode reward: -17.056, mean reward: -0.176 [-100.000, 13.645], mean action: 1.680 [0.000, 3.000],  loss: 10.882956, mae: 37.997798, mean_q: 29.240451, mean_eps: 0.570448
  95621/200000: episode: 950, duration: 1.306s, episode steps: 116, steps per second:  89, episode reward: -48.584, mean reward: -0.419 [-100.000, 12.494], mean action: 1.724 [0.000, 3.000],  loss: 5.133818, mae: 38.570655, mean_q: 30.508527, mean_eps: 0.569969
  95752/200000: episode: 951, duration: 1.093s, episode steps: 131, steps per second: 120, episode reward: -36.786, mean reward: -0.281 [-100.000, 15.263], mean action: 1.641 [0.000, 3.000],  loss: 7.286763, mae: 38.034244, mean_q: 30.029543, mean_eps: 0.569413
  95850/200000: episode: 952, duration: 0.822s, episode steps:  98, steps per second: 119, episode reward:  2.465, mean reward:  0.025 [-100.000, 14.567], mean action: 1.724 [0.000, 3.000],  loss: 10.614661, mae: 38.047909, mean_q: 29.076915, mean_eps: 0.568898
  95975/200000: episode: 953, duration: 1.073s, episode steps: 125, steps per second: 117, episode reward: -19.950, mean reward: -0.160 [-100.000, 21.856], mean action: 1.776 [0.000, 3.000],  loss: 8.264789, mae: 38.192101, mean_q: 30.700316, mean_eps: 0.568396
  96089/200000: episode: 954, duration: 0.962s, episode steps: 114, steps per second: 119, episode reward:  1.362, mean reward:  0.012 [-100.000, 17.052], mean action: 1.588 [0.000, 3.000],  loss: 5.349184, mae: 37.942489, mean_q: 29.497752, mean_eps: 0.567858
  96261/200000: episode: 955, duration: 1.376s, episode steps: 172, steps per second: 125, episode reward: -28.830, mean reward: -0.168 [-100.000, 15.449], mean action: 1.703 [0.000, 3.000],  loss: 6.216913, mae: 37.435275, mean_q: 29.602180, mean_eps: 0.567215
  96405/200000: episode: 956, duration: 1.025s, episode steps: 144, steps per second: 141, episode reward: -32.530, mean reward: -0.226 [-100.000, 21.065], mean action: 1.632 [0.000, 3.000],  loss: 14.056285, mae: 37.235208, mean_q: 29.625791, mean_eps: 0.566504
  96481/200000: episode: 957, duration: 0.613s, episode steps:  76, steps per second: 124, episode reward: -44.352, mean reward: -0.584 [-100.000, 14.468], mean action: 1.539 [0.000, 3.000],  loss: 6.299455, mae: 37.685265, mean_q: 29.770462, mean_eps: 0.566009
  96604/200000: episode: 958, duration: 0.939s, episode steps: 123, steps per second: 131, episode reward: -61.325, mean reward: -0.499 [-100.000,  8.013], mean action: 1.659 [0.000, 3.000],  loss: 7.693126, mae: 37.613773, mean_q: 30.553289, mean_eps: 0.565561
  96801/200000: episode: 959, duration: 1.558s, episode steps: 197, steps per second: 126, episode reward: -7.351, mean reward: -0.037 [-100.000, 36.919], mean action: 1.574 [0.000, 3.000],  loss: 7.923994, mae: 37.213262, mean_q: 28.288273, mean_eps: 0.564841
  96946/200000: episode: 960, duration: 1.147s, episode steps: 145, steps per second: 126, episode reward: -60.355, mean reward: -0.416 [-100.000, 15.972], mean action: 1.566 [0.000, 3.000],  loss: 7.921260, mae: 37.289821, mean_q: 29.138601, mean_eps: 0.564072
  97063/200000: episode: 961, duration: 0.977s, episode steps: 117, steps per second: 120, episode reward: -100.654, mean reward: -0.860 [-100.000,  6.483], mean action: 1.650 [0.000, 3.000],  loss: 8.439288, mae: 37.941357, mean_q: 29.642918, mean_eps: 0.563482
  97222/200000: episode: 962, duration: 1.287s, episode steps: 159, steps per second: 124, episode reward: -50.266, mean reward: -0.316 [-100.000,  5.686], mean action: 1.642 [0.000, 3.000],  loss: 8.429656, mae: 37.540114, mean_q: 31.809321, mean_eps: 0.562861
  97411/200000: episode: 963, duration: 1.896s, episode steps: 189, steps per second: 100, episode reward: -1.658, mean reward: -0.009 [-100.000, 36.903], mean action: 1.508 [0.000, 3.000],  loss: 6.275165, mae: 38.001236, mean_q: 30.130639, mean_eps: 0.562078
  97593/200000: episode: 964, duration: 2.245s, episode steps: 182, steps per second:  81, episode reward: -51.039, mean reward: -0.280 [-100.000, 15.326], mean action: 1.824 [0.000, 3.000],  loss: 6.961965, mae: 37.407779, mean_q: 30.248888, mean_eps: 0.561243
  97732/200000: episode: 965, duration: 1.301s, episode steps: 139, steps per second: 107, episode reward: -7.929, mean reward: -0.057 [-100.000, 15.673], mean action: 1.683 [0.000, 3.000],  loss: 6.581396, mae: 37.863853, mean_q: 31.706844, mean_eps: 0.560521
  97830/200000: episode: 966, duration: 0.710s, episode steps:  98, steps per second: 138, episode reward: -45.768, mean reward: -0.467 [-100.000, 10.708], mean action: 1.827 [0.000, 3.000],  loss: 7.283986, mae: 38.639712, mean_q: 29.576068, mean_eps: 0.559988
  97943/200000: episode: 967, duration: 0.920s, episode steps: 113, steps per second: 123, episode reward: -19.084, mean reward: -0.169 [-100.000, 16.789], mean action: 1.699 [0.000, 3.000],  loss: 10.034411, mae: 37.479842, mean_q: 30.370105, mean_eps: 0.559513
  98064/200000: episode: 968, duration: 1.220s, episode steps: 121, steps per second:  99, episode reward: -67.083, mean reward: -0.554 [-100.000,  6.683], mean action: 1.554 [0.000, 3.000],  loss: 5.845700, mae: 37.060893, mean_q: 29.323292, mean_eps: 0.558987
  98170/200000: episode: 969, duration: 0.865s, episode steps: 106, steps per second: 123, episode reward: 14.370, mean reward:  0.136 [-100.000, 11.644], mean action: 1.915 [0.000, 3.000],  loss: 6.114900, mae: 37.736716, mean_q: 30.518149, mean_eps: 0.558476
  98299/200000: episode: 970, duration: 1.034s, episode steps: 129, steps per second: 125, episode reward: 16.811, mean reward:  0.130 [-100.000, 19.318], mean action: 1.736 [0.000, 3.000],  loss: 9.283283, mae: 37.611990, mean_q: 30.413970, mean_eps: 0.557947
  98386/200000: episode: 971, duration: 0.749s, episode steps:  87, steps per second: 116, episode reward: -35.006, mean reward: -0.402 [-100.000, 12.163], mean action: 1.736 [0.000, 3.000],  loss: 6.040634, mae: 37.449809, mean_q: 30.124676, mean_eps: 0.557461
  98541/200000: episode: 972, duration: 1.247s, episode steps: 155, steps per second: 124, episode reward: -74.705, mean reward: -0.482 [-100.000,  8.888], mean action: 1.645 [0.000, 3.000],  loss: 4.655313, mae: 37.404610, mean_q: 30.891548, mean_eps: 0.556917
  98735/200000: episode: 973, duration: 1.318s, episode steps: 194, steps per second: 147, episode reward: -7.986, mean reward: -0.041 [-100.000, 12.931], mean action: 1.660 [0.000, 3.000],  loss: 6.402280, mae: 37.172382, mean_q: 29.747774, mean_eps: 0.556131
  98819/200000: episode: 974, duration: 0.650s, episode steps:  84, steps per second: 129, episode reward: -32.323, mean reward: -0.385 [-100.000, 21.834], mean action: 1.702 [0.000, 3.000],  loss: 6.737577, mae: 37.499765, mean_q: 30.121877, mean_eps: 0.555506
  99136/200000: episode: 975, duration: 2.339s, episode steps: 317, steps per second: 136, episode reward: -118.968, mean reward: -0.375 [-100.000, 20.157], mean action: 1.603 [0.000, 3.000],  loss: 7.635391, mae: 37.442627, mean_q: 31.510594, mean_eps: 0.554603
  99349/200000: episode: 976, duration: 1.618s, episode steps: 213, steps per second: 132, episode reward: -33.604, mean reward: -0.158 [-100.000, 14.920], mean action: 1.751 [0.000, 3.000],  loss: 6.581063, mae: 37.053533, mean_q: 31.225263, mean_eps: 0.553411
  99490/200000: episode: 977, duration: 1.374s, episode steps: 141, steps per second: 103, episode reward: -112.596, mean reward: -0.799 [-100.000, 12.203], mean action: 1.660 [0.000, 3.000],  loss: 6.402896, mae: 37.110011, mean_q: 31.356870, mean_eps: 0.552615
  99594/200000: episode: 978, duration: 0.793s, episode steps: 104, steps per second: 131, episode reward: -68.941, mean reward: -0.663 [-100.000,  6.763], mean action: 1.625 [0.000, 3.000],  loss: 4.497093, mae: 36.869881, mean_q: 31.129995, mean_eps: 0.552063
  99670/200000: episode: 979, duration: 0.574s, episode steps:  76, steps per second: 132, episode reward: -25.319, mean reward: -0.333 [-100.000,  9.176], mean action: 1.789 [0.000, 3.000],  loss: 6.249763, mae: 37.746827, mean_q: 30.928802, mean_eps: 0.551658
  99770/200000: episode: 980, duration: 0.833s, episode steps: 100, steps per second: 120, episode reward: -62.162, mean reward: -0.622 [-100.000,  7.363], mean action: 1.610 [0.000, 3.000],  loss: 4.282093, mae: 36.588861, mean_q: 31.213327, mean_eps: 0.551262
  99853/200000: episode: 981, duration: 0.637s, episode steps:  83, steps per second: 130, episode reward: -66.807, mean reward: -0.805 [-100.000, 15.867], mean action: 1.831 [0.000, 3.000],  loss: 7.962588, mae: 37.460689, mean_q: 30.044295, mean_eps: 0.550851
  99948/200000: episode: 982, duration: 0.708s, episode steps:  95, steps per second: 134, episode reward: -173.131, mean reward: -1.822 [-100.000, 31.173], mean action: 1.800 [0.000, 3.000],  loss: 6.489229, mae: 37.752906, mean_q: 32.319783, mean_eps: 0.550450
 100177/200000: episode: 983, duration: 1.656s, episode steps: 229, steps per second: 138, episode reward: -127.731, mean reward: -0.558 [-100.000,  5.614], mean action: 1.725 [0.000, 3.000],  loss: 6.363105, mae: 37.229103, mean_q: 30.033018, mean_eps: 0.549721
 100306/200000: episode: 984, duration: 0.886s, episode steps: 129, steps per second: 146, episode reward: -57.669, mean reward: -0.447 [-100.000,  8.228], mean action: 1.760 [0.000, 3.000],  loss: 7.649000, mae: 37.243851, mean_q: 29.164419, mean_eps: 0.548916
 100426/200000: episode: 985, duration: 0.852s, episode steps: 120, steps per second: 141, episode reward: -62.102, mean reward: -0.518 [-100.000,  4.371], mean action: 1.658 [0.000, 3.000],  loss: 6.341301, mae: 37.354298, mean_q: 30.694649, mean_eps: 0.548355
 100562/200000: episode: 986, duration: 0.972s, episode steps: 136, steps per second: 140, episode reward: -45.569, mean reward: -0.335 [-100.000, 12.239], mean action: 1.676 [0.000, 3.000],  loss: 5.577063, mae: 37.038991, mean_q: 29.846056, mean_eps: 0.547779
 100991/200000: episode: 987, duration: 2.891s, episode steps: 429, steps per second: 148, episode reward: -110.929, mean reward: -0.259 [-100.000, 26.911], mean action: 1.716 [0.000, 3.000],  loss: 7.088153, mae: 37.019375, mean_q: 30.530688, mean_eps: 0.546508
 101080/200000: episode: 988, duration: 0.616s, episode steps:  89, steps per second: 144, episode reward: -24.335, mean reward: -0.273 [-100.000, 17.652], mean action: 1.775 [0.000, 3.000],  loss: 8.133931, mae: 37.024246, mean_q: 30.321200, mean_eps: 0.545342
 101213/200000: episode: 989, duration: 0.881s, episode steps: 133, steps per second: 151, episode reward:  5.279, mean reward:  0.040 [-100.000, 16.909], mean action: 1.647 [0.000, 3.000],  loss: 6.729441, mae: 36.930649, mean_q: 31.702927, mean_eps: 0.544843
 101308/200000: episode: 990, duration: 0.624s, episode steps:  95, steps per second: 152, episode reward: -45.192, mean reward: -0.476 [-100.000,  9.899], mean action: 1.453 [0.000, 3.000],  loss: 6.713626, mae: 36.226455, mean_q: 29.309942, mean_eps: 0.544330
 101491/200000: episode: 991, duration: 1.519s, episode steps: 183, steps per second: 120, episode reward: 20.405, mean reward:  0.112 [-100.000, 16.546], mean action: 1.776 [0.000, 3.000],  loss: 5.527732, mae: 36.540009, mean_q: 29.647229, mean_eps: 0.543705
 101607/200000: episode: 992, duration: 0.874s, episode steps: 116, steps per second: 133, episode reward: -21.699, mean reward: -0.187 [-100.000, 12.990], mean action: 1.388 [0.000, 3.000],  loss: 7.256198, mae: 36.998032, mean_q: 30.148758, mean_eps: 0.543032
 101727/200000: episode: 993, duration: 0.901s, episode steps: 120, steps per second: 133, episode reward: -80.600, mean reward: -0.672 [-100.000, 12.209], mean action: 1.783 [0.000, 3.000],  loss: 6.203147, mae: 36.068514, mean_q: 28.665540, mean_eps: 0.542501
 101827/200000: episode: 994, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -56.078, mean reward: -0.561 [-100.000,  9.840], mean action: 1.660 [0.000, 3.000],  loss: 5.540544, mae: 36.563172, mean_q: 29.504110, mean_eps: 0.542006
 102013/200000: episode: 995, duration: 1.326s, episode steps: 186, steps per second: 140, episode reward: -52.928, mean reward: -0.285 [-100.000, 14.915], mean action: 1.624 [0.000, 3.000],  loss: 6.574927, mae: 36.711620, mean_q: 29.827383, mean_eps: 0.541362
 102291/200000: episode: 996, duration: 2.022s, episode steps: 278, steps per second: 138, episode reward: -147.618, mean reward: -0.531 [-100.000, 13.305], mean action: 1.629 [0.000, 3.000],  loss: 7.005602, mae: 37.364583, mean_q: 30.651749, mean_eps: 0.540318
 103291/200000: episode: 997, duration: 7.957s, episode steps: 1000, steps per second: 126, episode reward: -161.406, mean reward: -0.161 [-22.773, 25.545], mean action: 1.717 [0.000, 3.000],  loss: 6.185532, mae: 37.104944, mean_q: 30.217200, mean_eps: 0.537443
 104291/200000: episode: 998, duration: 8.123s, episode steps: 1000, steps per second: 123, episode reward: -86.440, mean reward: -0.086 [-22.875, 19.199], mean action: 1.667 [0.000, 3.000],  loss: 5.922603, mae: 36.831607, mean_q: 29.943483, mean_eps: 0.532943
 104641/200000: episode: 999, duration: 2.467s, episode steps: 350, steps per second: 142, episode reward: -103.354, mean reward: -0.295 [-100.000, 52.336], mean action: 1.457 [0.000, 3.000],  loss: 6.607340, mae: 36.295730, mean_q: 29.359197, mean_eps: 0.529905
 104812/200000: episode: 1000, duration: 1.207s, episode steps: 171, steps per second: 142, episode reward: -208.602, mean reward: -1.220 [-100.000, 59.710], mean action: 1.602 [0.000, 3.000],  loss: 5.705687, mae: 36.526567, mean_q: 30.426476, mean_eps: 0.528733
 104946/200000: episode: 1001, duration: 0.937s, episode steps: 134, steps per second: 143, episode reward: -2.831, mean reward: -0.021 [-100.000, 14.394], mean action: 1.739 [0.000, 3.000],  loss: 8.127267, mae: 36.655168, mean_q: 29.253345, mean_eps: 0.528047
 105946/200000: episode: 1002, duration: 8.733s, episode steps: 1000, steps per second: 115, episode reward: 22.451, mean reward:  0.022 [-21.099, 16.092], mean action: 1.743 [0.000, 3.000],  loss: 7.219336, mae: 35.997756, mean_q: 28.960905, mean_eps: 0.525495
 106025/200000: episode: 1003, duration: 0.489s, episode steps:  79, steps per second: 162, episode reward: -21.746, mean reward: -0.275 [-100.000,  8.932], mean action: 1.595 [0.000, 3.000],  loss: 4.858093, mae: 36.131885, mean_q: 29.934148, mean_eps: 0.523068
 106180/200000: episode: 1004, duration: 1.062s, episode steps: 155, steps per second: 146, episode reward: -38.017, mean reward: -0.245 [-100.000, 16.917], mean action: 1.665 [0.000, 3.000],  loss: 7.368826, mae: 36.166332, mean_q: 29.726395, mean_eps: 0.522541
 106314/200000: episode: 1005, duration: 0.859s, episode steps: 134, steps per second: 156, episode reward:  3.992, mean reward:  0.030 [-100.000, 18.828], mean action: 1.664 [0.000, 3.000],  loss: 5.377423, mae: 36.109691, mean_q: 29.279250, mean_eps: 0.521891
 107314/200000: episode: 1006, duration: 7.336s, episode steps: 1000, steps per second: 136, episode reward: 52.378, mean reward:  0.052 [-20.417, 30.249], mean action: 1.611 [0.000, 3.000],  loss: 7.096781, mae: 36.039465, mean_q: 29.511951, mean_eps: 0.519339
 107521/200000: episode: 1007, duration: 1.361s, episode steps: 207, steps per second: 152, episode reward:  7.302, mean reward:  0.035 [-100.000, 14.449], mean action: 1.768 [0.000, 3.000],  loss: 8.941646, mae: 35.779585, mean_q: 28.482372, mean_eps: 0.516624
 107636/200000: episode: 1008, duration: 0.777s, episode steps: 115, steps per second: 148, episode reward: -5.207, mean reward: -0.045 [-100.000, 16.138], mean action: 1.678 [0.000, 3.000],  loss: 6.930497, mae: 35.977299, mean_q: 27.641285, mean_eps: 0.515899
 107746/200000: episode: 1009, duration: 0.719s, episode steps: 110, steps per second: 153, episode reward: -25.689, mean reward: -0.234 [-100.000, 12.583], mean action: 1.691 [0.000, 3.000],  loss: 9.683741, mae: 35.402505, mean_q: 28.059446, mean_eps: 0.515393
 107889/200000: episode: 1010, duration: 0.912s, episode steps: 143, steps per second: 157, episode reward: 28.371, mean reward:  0.198 [-100.000, 10.350], mean action: 1.685 [0.000, 3.000],  loss: 8.682471, mae: 35.453057, mean_q: 29.135408, mean_eps: 0.514823
 108057/200000: episode: 1011, duration: 1.114s, episode steps: 168, steps per second: 151, episode reward: -55.674, mean reward: -0.331 [-100.000, 12.960], mean action: 1.732 [0.000, 3.000],  loss: 7.667934, mae: 36.275629, mean_q: 31.437475, mean_eps: 0.514124
 108254/200000: episode: 1012, duration: 1.261s, episode steps: 197, steps per second: 156, episode reward: -62.576, mean reward: -0.318 [-100.000, 19.957], mean action: 1.543 [0.000, 3.000],  loss: 8.414672, mae: 36.122003, mean_q: 30.369665, mean_eps: 0.513302
 108428/200000: episode: 1013, duration: 1.445s, episode steps: 174, steps per second: 120, episode reward: -25.199, mean reward: -0.145 [-100.000, 15.597], mean action: 1.730 [0.000, 3.000],  loss: 8.473227, mae: 35.868477, mean_q: 29.538485, mean_eps: 0.512468
 108495/200000: episode: 1014, duration: 0.540s, episode steps:  67, steps per second: 124, episode reward: -45.566, mean reward: -0.680 [-100.000, 10.097], mean action: 1.701 [0.000, 3.000],  loss: 10.903027, mae: 35.968986, mean_q: 31.169752, mean_eps: 0.511926
 108654/200000: episode: 1015, duration: 1.238s, episode steps: 159, steps per second: 128, episode reward: -3.617, mean reward: -0.023 [-100.000, 20.031], mean action: 1.660 [0.000, 3.000],  loss: 6.654437, mae: 35.970094, mean_q: 30.813359, mean_eps: 0.511417
 109178/200000: episode: 1016, duration: 3.553s, episode steps: 524, steps per second: 147, episode reward: -26.985, mean reward: -0.051 [-100.000, 17.818], mean action: 1.584 [0.000, 3.000],  loss: 8.153160, mae: 36.212760, mean_q: 31.437503, mean_eps: 0.509880
 109307/200000: episode: 1017, duration: 0.834s, episode steps: 129, steps per second: 155, episode reward: 29.510, mean reward:  0.229 [-100.000, 19.899], mean action: 1.690 [0.000, 3.000],  loss: 6.226805, mae: 35.851799, mean_q: 31.415466, mean_eps: 0.508411
 110050/200000: episode: 1018, duration: 5.958s, episode steps: 743, steps per second: 125, episode reward: -120.684, mean reward: -0.162 [-100.000, 20.030], mean action: 1.725 [0.000, 3.000],  loss: 6.921794, mae: 35.215790, mean_q: 30.979737, mean_eps: 0.506449
 110161/200000: episode: 1019, duration: 0.851s, episode steps: 111, steps per second: 130, episode reward: -60.666, mean reward: -0.547 [-100.000, 10.504], mean action: 1.676 [0.000, 3.000],  loss: 6.278237, mae: 35.428019, mean_q: 31.598538, mean_eps: 0.504528
 110782/200000: episode: 1020, duration: 5.058s, episode steps: 621, steps per second: 123, episode reward: -27.802, mean reward: -0.045 [-100.000, 17.937], mean action: 1.715 [0.000, 3.000],  loss: 7.109275, mae: 34.926292, mean_q: 31.272614, mean_eps: 0.502880
 111782/200000: episode: 1021, duration: 9.427s, episode steps: 1000, steps per second: 106, episode reward: -2.805, mean reward: -0.003 [-23.299, 21.613], mean action: 1.703 [0.000, 3.000],  loss: 7.553280, mae: 34.887073, mean_q: 30.733889, mean_eps: 0.499233
 111966/200000: episode: 1022, duration: 1.252s, episode steps: 184, steps per second: 147, episode reward: -21.551, mean reward: -0.117 [-100.000,  6.825], mean action: 1.707 [0.000, 3.000],  loss: 6.562164, mae: 34.884167, mean_q: 30.395682, mean_eps: 0.496569
 112101/200000: episode: 1023, duration: 0.954s, episode steps: 135, steps per second: 142, episode reward: -141.139, mean reward: -1.045 [-100.000, 22.893], mean action: 1.430 [0.000, 3.000],  loss: 9.049951, mae: 34.417448, mean_q: 30.944102, mean_eps: 0.495852
 112231/200000: episode: 1024, duration: 0.875s, episode steps: 130, steps per second: 149, episode reward: 20.070, mean reward:  0.154 [-100.000, 13.992], mean action: 1.685 [0.000, 3.000],  loss: 5.708708, mae: 34.133736, mean_q: 31.559045, mean_eps: 0.495255
 112324/200000: episode: 1025, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -15.211, mean reward: -0.164 [-100.000, 28.062], mean action: 1.645 [0.000, 3.000],  loss: 6.820948, mae: 34.924631, mean_q: 32.639882, mean_eps: 0.494754
 113324/200000: episode: 1026, duration: 7.783s, episode steps: 1000, steps per second: 128, episode reward: -0.553, mean reward: -0.001 [-13.455, 17.568], mean action: 1.787 [0.000, 3.000],  loss: 8.294270, mae: 34.086046, mean_q: 30.555723, mean_eps: 0.492294
 113435/200000: episode: 1027, duration: 0.724s, episode steps: 111, steps per second: 153, episode reward: -43.733, mean reward: -0.394 [-100.000, 10.522], mean action: 1.631 [0.000, 3.000],  loss: 8.784218, mae: 34.736437, mean_q: 31.035229, mean_eps: 0.489795
 113577/200000: episode: 1028, duration: 1.001s, episode steps: 142, steps per second: 142, episode reward: -42.821, mean reward: -0.302 [-100.000, 13.254], mean action: 1.563 [0.000, 3.000],  loss: 6.235952, mae: 33.769665, mean_q: 31.010793, mean_eps: 0.489225
 113681/200000: episode: 1029, duration: 0.680s, episode steps: 104, steps per second: 153, episode reward: -121.882, mean reward: -1.172 [-100.000, 14.707], mean action: 1.981 [0.000, 3.000],  loss: 8.927423, mae: 33.727979, mean_q: 29.640688, mean_eps: 0.488672
 113829/200000: episode: 1030, duration: 1.005s, episode steps: 148, steps per second: 147, episode reward: 18.778, mean reward:  0.127 [-100.000, 11.397], mean action: 1.534 [0.000, 3.000],  loss: 6.427773, mae: 34.209528, mean_q: 31.500364, mean_eps: 0.488105
 114572/200000: episode: 1031, duration: 5.370s, episode steps: 743, steps per second: 138, episode reward: -62.585, mean reward: -0.084 [-100.000, 17.882], mean action: 1.789 [0.000, 3.000],  loss: 7.261780, mae: 34.222713, mean_q: 32.095865, mean_eps: 0.486100
 114686/200000: episode: 1032, duration: 0.763s, episode steps: 114, steps per second: 150, episode reward: 12.933, mean reward:  0.113 [-100.000, 20.635], mean action: 1.798 [0.000, 3.000],  loss: 6.106260, mae: 33.493030, mean_q: 32.082400, mean_eps: 0.484172
 114875/200000: episode: 1033, duration: 1.289s, episode steps: 189, steps per second: 147, episode reward: 27.164, mean reward:  0.144 [-100.000, 13.922], mean action: 1.693 [0.000, 3.000],  loss: 7.307499, mae: 33.881406, mean_q: 31.776005, mean_eps: 0.483490
 115875/200000: episode: 1034, duration: 7.482s, episode steps: 1000, steps per second: 134, episode reward: 29.780, mean reward:  0.030 [-24.928, 30.201], mean action: 1.757 [0.000, 3.000],  loss: 8.952872, mae: 33.762005, mean_q: 31.680859, mean_eps: 0.480815
 116875/200000: episode: 1035, duration: 8.430s, episode steps: 1000, steps per second: 119, episode reward: -3.834, mean reward: -0.004 [-22.137, 21.663], mean action: 1.713 [0.000, 3.000],  loss: 8.266911, mae: 33.364325, mean_q: 31.442548, mean_eps: 0.476315
 117016/200000: episode: 1036, duration: 0.956s, episode steps: 141, steps per second: 148, episode reward:  8.423, mean reward:  0.060 [-100.000, 10.155], mean action: 1.667 [0.000, 3.000],  loss: 8.349785, mae: 32.536875, mean_q: 31.319833, mean_eps: 0.473747
 118016/200000: episode: 1037, duration: 9.022s, episode steps: 1000, steps per second: 111, episode reward: 49.010, mean reward:  0.049 [-23.441, 28.124], mean action: 1.710 [0.000, 3.000],  loss: 9.149076, mae: 32.931708, mean_q: 31.610581, mean_eps: 0.471180
 118142/200000: episode: 1038, duration: 0.835s, episode steps: 126, steps per second: 151, episode reward: -32.328, mean reward: -0.257 [-100.000, 17.480], mean action: 1.619 [0.000, 3.000],  loss: 6.157545, mae: 33.668110, mean_q: 32.355033, mean_eps: 0.468647
 118290/200000: episode: 1039, duration: 1.030s, episode steps: 148, steps per second: 144, episode reward: 17.231, mean reward:  0.116 [-100.000, 18.884], mean action: 1.703 [0.000, 3.000],  loss: 7.450284, mae: 33.486943, mean_q: 32.190704, mean_eps: 0.468030
 118363/200000: episode: 1040, duration: 0.523s, episode steps:  73, steps per second: 140, episode reward: -42.089, mean reward: -0.577 [-100.000,  9.250], mean action: 1.616 [0.000, 3.000],  loss: 6.855257, mae: 33.450889, mean_q: 31.666550, mean_eps: 0.467533
 119363/200000: episode: 1041, duration: 8.117s, episode steps: 1000, steps per second: 123, episode reward: 36.609, mean reward:  0.037 [-22.659, 22.864], mean action: 1.199 [0.000, 3.000],  loss: 7.801214, mae: 33.053723, mean_q: 31.855183, mean_eps: 0.465119
 120363/200000: episode: 1042, duration: 8.923s, episode steps: 1000, steps per second: 112, episode reward: -17.194, mean reward: -0.017 [-19.365, 14.679], mean action: 1.694 [0.000, 3.000],  loss: 9.550877, mae: 32.643327, mean_q: 32.097827, mean_eps: 0.460619
 120469/200000: episode: 1043, duration: 0.780s, episode steps: 106, steps per second: 136, episode reward: 39.816, mean reward:  0.376 [-100.000, 18.554], mean action: 1.802 [0.000, 3.000],  loss: 8.711845, mae: 32.376477, mean_q: 31.322762, mean_eps: 0.458130
 120602/200000: episode: 1044, duration: 1.234s, episode steps: 133, steps per second: 108, episode reward:  2.280, mean reward:  0.017 [-100.000,  9.642], mean action: 1.617 [0.000, 3.000],  loss: 11.792313, mae: 33.049060, mean_q: 32.111335, mean_eps: 0.457593
 121602/200000: episode: 1045, duration: 8.735s, episode steps: 1000, steps per second: 114, episode reward: 31.681, mean reward:  0.032 [-22.157, 24.169], mean action: 1.571 [0.000, 3.000],  loss: 9.773174, mae: 32.448668, mean_q: 31.642657, mean_eps: 0.455043
 122602/200000: episode: 1046, duration: 9.475s, episode steps: 1000, steps per second: 106, episode reward: -27.615, mean reward: -0.028 [-23.095, 14.359], mean action: 1.720 [0.000, 3.000],  loss: 8.346328, mae: 32.095007, mean_q: 31.907623, mean_eps: 0.450543
 122723/200000: episode: 1047, duration: 1.091s, episode steps: 121, steps per second: 111, episode reward: -26.505, mean reward: -0.219 [-100.000, 14.177], mean action: 1.760 [0.000, 3.000],  loss: 6.840661, mae: 32.184284, mean_q: 31.156361, mean_eps: 0.448021
 122876/200000: episode: 1048, duration: 1.425s, episode steps: 153, steps per second: 107, episode reward: 15.713, mean reward:  0.103 [-100.000, 17.982], mean action: 1.627 [0.000, 3.000],  loss: 7.851129, mae: 31.825496, mean_q: 31.576315, mean_eps: 0.447404
 123200/200000: episode: 1049, duration: 2.609s, episode steps: 324, steps per second: 124, episode reward: -207.251, mean reward: -0.640 [-100.000, 18.042], mean action: 1.769 [0.000, 3.000],  loss: 7.935965, mae: 31.689625, mean_q: 31.793558, mean_eps: 0.446331
 124200/200000: episode: 1050, duration: 9.385s, episode steps: 1000, steps per second: 107, episode reward: 77.799, mean reward:  0.078 [-24.088, 31.256], mean action: 1.442 [0.000, 3.000],  loss: 8.102881, mae: 31.830012, mean_q: 31.558942, mean_eps: 0.443352
 124310/200000: episode: 1051, duration: 0.757s, episode steps: 110, steps per second: 145, episode reward: 18.126, mean reward:  0.165 [-100.000, 16.418], mean action: 1.818 [0.000, 3.000],  loss: 10.486769, mae: 31.947237, mean_q: 32.409291, mean_eps: 0.440855
 124416/200000: episode: 1052, duration: 0.784s, episode steps: 106, steps per second: 135, episode reward: 22.831, mean reward:  0.215 [-100.000, 11.937], mean action: 1.726 [0.000, 3.000],  loss: 7.795468, mae: 31.823472, mean_q: 32.147005, mean_eps: 0.440369
 124920/200000: episode: 1053, duration: 3.833s, episode steps: 504, steps per second: 131, episode reward: -137.424, mean reward: -0.273 [-100.000, 11.011], mean action: 1.758 [0.000, 3.000],  loss: 8.164788, mae: 31.615109, mean_q: 32.152571, mean_eps: 0.438996
 125529/200000: episode: 1054, duration: 4.373s, episode steps: 609, steps per second: 139, episode reward: -288.994, mean reward: -0.475 [-100.000, 25.254], mean action: 1.708 [0.000, 3.000],  loss: 8.592418, mae: 31.345259, mean_q: 31.405480, mean_eps: 0.436492
 126529/200000: episode: 1055, duration: 8.688s, episode steps: 1000, steps per second: 115, episode reward: 16.220, mean reward:  0.016 [-24.445, 27.564], mean action: 1.674 [0.000, 3.000],  loss: 8.480447, mae: 31.305328, mean_q: 31.950868, mean_eps: 0.432872
 126656/200000: episode: 1056, duration: 0.933s, episode steps: 127, steps per second: 136, episode reward: -79.200, mean reward: -0.624 [-100.000, 10.044], mean action: 1.677 [0.000, 3.000],  loss: 9.208547, mae: 30.708537, mean_q: 32.017046, mean_eps: 0.430336
 126883/200000: episode: 1057, duration: 1.533s, episode steps: 227, steps per second: 148, episode reward: -76.738, mean reward: -0.338 [-100.000, 13.890], mean action: 1.626 [0.000, 3.000],  loss: 8.021899, mae: 31.331479, mean_q: 32.379872, mean_eps: 0.429539
 127883/200000: episode: 1058, duration: 10.660s, episode steps: 1000, steps per second:  94, episode reward: -4.963, mean reward: -0.005 [-20.381, 24.799], mean action: 1.451 [0.000, 3.000],  loss: 8.054139, mae: 31.280076, mean_q: 32.160943, mean_eps: 0.426779
 128040/200000: episode: 1059, duration: 1.199s, episode steps: 157, steps per second: 131, episode reward: -11.993, mean reward: -0.076 [-100.000, 23.187], mean action: 1.758 [0.000, 3.000],  loss: 8.950840, mae: 30.946166, mean_q: 32.106712, mean_eps: 0.424175
 128220/200000: episode: 1060, duration: 1.406s, episode steps: 180, steps per second: 128, episode reward: -108.976, mean reward: -0.605 [-100.000, 11.290], mean action: 1.972 [0.000, 3.000],  loss: 7.620969, mae: 30.656197, mean_q: 32.525546, mean_eps: 0.423417
 128813/200000: episode: 1061, duration: 5.748s, episode steps: 593, steps per second: 103, episode reward: -275.276, mean reward: -0.464 [-100.000, 16.280], mean action: 1.723 [0.000, 3.000],  loss: 9.350821, mae: 31.194539, mean_q: 32.884400, mean_eps: 0.421678
 128981/200000: episode: 1062, duration: 2.321s, episode steps: 168, steps per second:  72, episode reward: 10.588, mean reward:  0.063 [-100.000, 19.168], mean action: 1.738 [0.000, 3.000],  loss: 8.921565, mae: 30.546300, mean_q: 31.296935, mean_eps: 0.419966
 129981/200000: episode: 1063, duration: 11.274s, episode steps: 1000, steps per second:  89, episode reward: 31.396, mean reward:  0.031 [-21.502, 23.148], mean action: 1.785 [0.000, 3.000],  loss: 8.313830, mae: 31.133275, mean_q: 33.019868, mean_eps: 0.417338
 130075/200000: episode: 1064, duration: 0.625s, episode steps:  94, steps per second: 150, episode reward: -21.314, mean reward: -0.227 [-100.000, 10.759], mean action: 1.606 [0.000, 3.000],  loss: 7.178071, mae: 30.189523, mean_q: 33.139203, mean_eps: 0.414876
 131075/200000: episode: 1065, duration: 10.634s, episode steps: 1000, steps per second:  94, episode reward:  3.220, mean reward:  0.003 [-22.025, 24.318], mean action: 1.338 [0.000, 3.000],  loss: 8.727350, mae: 30.740527, mean_q: 32.855471, mean_eps: 0.412415
 132075/200000: episode: 1066, duration: 9.296s, episode steps: 1000, steps per second: 108, episode reward:  8.491, mean reward:  0.008 [-17.507, 15.391], mean action: 1.632 [0.000, 3.000],  loss: 10.546939, mae: 30.441084, mean_q: 33.148110, mean_eps: 0.407915
 132349/200000: episode: 1067, duration: 2.283s, episode steps: 274, steps per second: 120, episode reward: -79.072, mean reward: -0.289 [-100.000, 15.593], mean action: 1.708 [0.000, 3.000],  loss: 12.226161, mae: 30.273733, mean_q: 32.716763, mean_eps: 0.405048
 133349/200000: episode: 1068, duration: 8.987s, episode steps: 1000, steps per second: 111, episode reward: 22.415, mean reward:  0.022 [-24.825, 22.989], mean action: 1.213 [0.000, 3.000],  loss: 9.116919, mae: 30.054886, mean_q: 33.356275, mean_eps: 0.402182
 133482/200000: episode: 1069, duration: 1.033s, episode steps: 133, steps per second: 129, episode reward: 11.515, mean reward:  0.087 [-100.000, 24.602], mean action: 1.669 [0.000, 3.000],  loss: 9.255640, mae: 29.769442, mean_q: 33.223346, mean_eps: 0.399633
 134482/200000: episode: 1070, duration: 8.775s, episode steps: 1000, steps per second: 114, episode reward: 82.888, mean reward:  0.083 [-23.692, 24.411], mean action: 1.473 [0.000, 3.000],  loss: 7.914669, mae: 29.389195, mean_q: 32.854303, mean_eps: 0.397083
 134665/200000: episode: 1071, duration: 1.392s, episode steps: 183, steps per second: 131, episode reward: -6.022, mean reward: -0.033 [-100.000, 15.741], mean action: 1.623 [0.000, 3.000],  loss: 9.188152, mae: 28.707625, mean_q: 32.147183, mean_eps: 0.394422
 135665/200000: episode: 1072, duration: 8.393s, episode steps: 1000, steps per second: 119, episode reward: 50.815, mean reward:  0.051 [-20.703, 23.860], mean action: 1.640 [0.000, 3.000],  loss: 9.940512, mae: 29.148606, mean_q: 33.115088, mean_eps: 0.391760
 136665/200000: episode: 1073, duration: 9.045s, episode steps: 1000, steps per second: 111, episode reward: 70.818, mean reward:  0.071 [-23.957, 20.907], mean action: 1.690 [0.000, 3.000],  loss: 9.478420, mae: 28.823612, mean_q: 33.277034, mean_eps: 0.387260
 137665/200000: episode: 1074, duration: 9.533s, episode steps: 1000, steps per second: 105, episode reward: 142.642, mean reward:  0.143 [-20.968, 21.579], mean action: 1.643 [0.000, 3.000],  loss: 9.243443, mae: 28.648421, mean_q: 33.491700, mean_eps: 0.382760
 138665/200000: episode: 1075, duration: 8.210s, episode steps: 1000, steps per second: 122, episode reward: 85.384, mean reward:  0.085 [-23.762, 23.811], mean action: 1.637 [0.000, 3.000],  loss: 8.528130, mae: 27.911253, mean_q: 32.806431, mean_eps: 0.378260
 138897/200000: episode: 1076, duration: 1.989s, episode steps: 232, steps per second: 117, episode reward: -7.187, mean reward: -0.031 [-100.000, 11.123], mean action: 1.690 [0.000, 3.000],  loss: 8.912414, mae: 28.054971, mean_q: 32.698146, mean_eps: 0.375488
 139897/200000: episode: 1077, duration: 8.567s, episode steps: 1000, steps per second: 117, episode reward: 90.255, mean reward:  0.090 [-21.568, 22.196], mean action: 1.071 [0.000, 3.000],  loss: 10.072711, mae: 27.742469, mean_q: 32.746181, mean_eps: 0.372716
 140897/200000: episode: 1078, duration: 9.867s, episode steps: 1000, steps per second: 101, episode reward: -16.664, mean reward: -0.017 [-22.738, 22.293], mean action: 1.455 [0.000, 3.000],  loss: 9.320524, mae: 27.419086, mean_q: 33.059463, mean_eps: 0.368216
 141147/200000: episode: 1079, duration: 2.709s, episode steps: 250, steps per second:  92, episode reward: -81.222, mean reward: -0.325 [-100.000, 10.689], mean action: 1.784 [0.000, 3.000],  loss: 10.247892, mae: 27.048873, mean_q: 32.464202, mean_eps: 0.365403
 142147/200000: episode: 1080, duration: 7.268s, episode steps: 1000, steps per second: 138, episode reward: 74.305, mean reward:  0.074 [-20.957, 22.875], mean action: 1.426 [0.000, 3.000],  loss: 9.352693, mae: 27.279949, mean_q: 33.382818, mean_eps: 0.362591
 142277/200000: episode: 1081, duration: 0.934s, episode steps: 130, steps per second: 139, episode reward: -95.598, mean reward: -0.735 [-100.000, 13.620], mean action: 1.815 [0.000, 3.000],  loss: 8.359825, mae: 27.922027, mean_q: 33.858110, mean_eps: 0.360048
 142435/200000: episode: 1082, duration: 1.087s, episode steps: 158, steps per second: 145, episode reward: -31.248, mean reward: -0.198 [-100.000, 16.739], mean action: 1.816 [0.000, 3.000],  loss: 8.377718, mae: 27.002028, mean_q: 33.404713, mean_eps: 0.359400
 142513/200000: episode: 1083, duration: 0.515s, episode steps:  78, steps per second: 151, episode reward: -35.348, mean reward: -0.453 [-100.000,  9.504], mean action: 1.628 [0.000, 3.000],  loss: 6.362798, mae: 27.591392, mean_q: 33.406521, mean_eps: 0.358869
 143023/200000: episode: 1084, duration: 3.528s, episode steps: 510, steps per second: 145, episode reward: -199.717, mean reward: -0.392 [-100.000, 16.292], mean action: 1.390 [0.000, 3.000],  loss: 8.461466, mae: 27.219376, mean_q: 33.845619, mean_eps: 0.357546
 144023/200000: episode: 1085, duration: 8.524s, episode steps: 1000, steps per second: 117, episode reward: 118.045, mean reward:  0.118 [-24.154, 23.423], mean action: 1.249 [0.000, 3.000],  loss: 10.140171, mae: 27.264923, mean_q: 34.019889, mean_eps: 0.354149
 145023/200000: episode: 1086, duration: 10.388s, episode steps: 1000, steps per second:  96, episode reward: 42.153, mean reward:  0.042 [-20.828, 23.969], mean action: 1.272 [0.000, 3.000],  loss: 8.574892, mae: 27.201287, mean_q: 34.171624, mean_eps: 0.349649
 145123/200000: episode: 1087, duration: 0.742s, episode steps: 100, steps per second: 135, episode reward: -4.493, mean reward: -0.045 [-100.000, 20.404], mean action: 1.720 [0.000, 3.000],  loss: 7.440892, mae: 27.179286, mean_q: 34.315122, mean_eps: 0.347174
 146123/200000: episode: 1088, duration: 9.807s, episode steps: 1000, steps per second: 102, episode reward: 131.701, mean reward:  0.132 [-22.776, 23.462], mean action: 0.970 [0.000, 3.000],  loss: 7.973388, mae: 26.745014, mean_q: 33.813454, mean_eps: 0.344699
 147112/200000: episode: 1089, duration: 9.060s, episode steps: 989, steps per second: 109, episode reward: 171.315, mean reward:  0.173 [-22.340, 100.000], mean action: 1.445 [0.000, 3.000],  loss: 8.768618, mae: 26.898189, mean_q: 34.296463, mean_eps: 0.340223
 147219/200000: episode: 1090, duration: 1.386s, episode steps: 107, steps per second:  77, episode reward: 24.513, mean reward:  0.229 [-100.000, 20.188], mean action: 1.785 [0.000, 3.000],  loss: 7.091242, mae: 26.708076, mean_q: 33.845354, mean_eps: 0.337758
 148219/200000: episode: 1091, duration: 10.641s, episode steps: 1000, steps per second:  94, episode reward: 21.453, mean reward:  0.021 [-23.184, 22.544], mean action: 1.511 [0.000, 3.000],  loss: 8.803045, mae: 26.326249, mean_q: 33.678171, mean_eps: 0.335267
 149014/200000: episode: 1092, duration: 7.316s, episode steps: 795, steps per second: 109, episode reward: 183.421, mean reward:  0.231 [-22.921, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 8.740316, mae: 26.131951, mean_q: 33.557067, mean_eps: 0.331228
 150014/200000: episode: 1093, duration: 8.064s, episode steps: 1000, steps per second: 124, episode reward: 66.797, mean reward:  0.067 [-23.430, 21.860], mean action: 1.126 [0.000, 3.000],  loss: 8.961577, mae: 25.570789, mean_q: 32.813187, mean_eps: 0.327189
 151014/200000: episode: 1094, duration: 7.693s, episode steps: 1000, steps per second: 130, episode reward: 32.200, mean reward:  0.032 [-19.559, 21.900], mean action: 2.103 [0.000, 3.000],  loss: 8.605406, mae: 25.325406, mean_q: 32.610365, mean_eps: 0.322689
 152014/200000: episode: 1095, duration: 7.248s, episode steps: 1000, steps per second: 138, episode reward: -11.553, mean reward: -0.012 [-25.169, 25.406], mean action: 1.960 [0.000, 3.000],  loss: 6.731348, mae: 24.674002, mean_q: 32.144256, mean_eps: 0.318189
 153014/200000: episode: 1096, duration: 6.816s, episode steps: 1000, steps per second: 147, episode reward: 120.177, mean reward:  0.120 [-24.287, 24.074], mean action: 0.924 [0.000, 3.000],  loss: 8.324220, mae: 24.679095, mean_q: 32.345079, mean_eps: 0.313689
 154014/200000: episode: 1097, duration: 7.235s, episode steps: 1000, steps per second: 138, episode reward: 46.497, mean reward:  0.046 [-20.748, 22.139], mean action: 1.733 [0.000, 3.000],  loss: 6.532541, mae: 24.748135, mean_q: 32.356269, mean_eps: 0.309189
 155014/200000: episode: 1098, duration: 7.231s, episode steps: 1000, steps per second: 138, episode reward: 102.489, mean reward:  0.102 [-21.512, 23.172], mean action: 1.200 [0.000, 3.000],  loss: 5.910741, mae: 24.612509, mean_q: 32.330571, mean_eps: 0.304689
 155165/200000: episode: 1099, duration: 0.993s, episode steps: 151, steps per second: 152, episode reward: -15.825, mean reward: -0.105 [-100.000, 11.772], mean action: 1.735 [0.000, 3.000],  loss: 8.326479, mae: 24.656328, mean_q: 32.354485, mean_eps: 0.302100
 155859/200000: episode: 1100, duration: 5.120s, episode steps: 694, steps per second: 136, episode reward: 179.118, mean reward:  0.258 [-21.180, 100.000], mean action: 1.147 [0.000, 3.000],  loss: 5.887106, mae: 24.548223, mean_q: 32.571862, mean_eps: 0.300198
 156859/200000: episode: 1101, duration: 7.409s, episode steps: 1000, steps per second: 135, episode reward: 131.723, mean reward:  0.132 [-20.555, 22.659], mean action: 1.068 [0.000, 3.000],  loss: 6.599629, mae: 24.160314, mean_q: 31.917968, mean_eps: 0.296387
 157859/200000: episode: 1102, duration: 7.434s, episode steps: 1000, steps per second: 135, episode reward: 108.544, mean reward:  0.109 [-19.245, 23.608], mean action: 1.083 [0.000, 3.000],  loss: 6.593032, mae: 23.824718, mean_q: 31.422787, mean_eps: 0.291887
 158859/200000: episode: 1103, duration: 7.866s, episode steps: 1000, steps per second: 127, episode reward: 99.705, mean reward:  0.100 [-23.657, 22.369], mean action: 1.197 [0.000, 3.000],  loss: 5.595901, mae: 23.233968, mean_q: 30.561670, mean_eps: 0.287387
 159340/200000: episode: 1104, duration: 3.260s, episode steps: 481, steps per second: 148, episode reward:  0.033, mean reward:  0.000 [-100.000, 15.648], mean action: 1.780 [0.000, 3.000],  loss: 5.776496, mae: 22.915233, mean_q: 30.169111, mean_eps: 0.284054
 160340/200000: episode: 1105, duration: 7.110s, episode steps: 1000, steps per second: 141, episode reward: 113.147, mean reward:  0.113 [-24.463, 22.466], mean action: 1.020 [0.000, 3.000],  loss: 5.742236, mae: 22.632289, mean_q: 29.817086, mean_eps: 0.280722
 161340/200000: episode: 1106, duration: 7.053s, episode steps: 1000, steps per second: 142, episode reward: 159.168, mean reward:  0.159 [-20.879, 23.663], mean action: 1.405 [0.000, 3.000],  loss: 5.474226, mae: 22.456211, mean_q: 29.406970, mean_eps: 0.276222
 162217/200000: episode: 1107, duration: 6.839s, episode steps: 877, steps per second: 128, episode reward: 196.852, mean reward:  0.224 [-20.177, 100.000], mean action: 1.223 [0.000, 3.000],  loss: 6.207288, mae: 22.532254, mean_q: 29.640347, mean_eps: 0.271999
 163217/200000: episode: 1108, duration: 7.354s, episode steps: 1000, steps per second: 136, episode reward: 171.515, mean reward:  0.172 [-19.946, 22.135], mean action: 0.780 [0.000, 3.000],  loss: 5.032062, mae: 22.204565, mean_q: 29.260059, mean_eps: 0.267776
 164217/200000: episode: 1109, duration: 6.903s, episode steps: 1000, steps per second: 145, episode reward: 135.292, mean reward:  0.135 [-18.750, 23.082], mean action: 0.961 [0.000, 3.000],  loss: 4.934179, mae: 21.558684, mean_q: 28.388035, mean_eps: 0.263276
 164424/200000: episode: 1110, duration: 1.354s, episode steps: 207, steps per second: 153, episode reward: -178.316, mean reward: -0.861 [-100.000, 20.716], mean action: 1.541 [0.000, 3.000],  loss: 6.102989, mae: 22.070025, mean_q: 29.128555, mean_eps: 0.260560
 165424/200000: episode: 1111, duration: 8.477s, episode steps: 1000, steps per second: 118, episode reward: 171.114, mean reward:  0.171 [-21.432, 23.200], mean action: 1.138 [0.000, 3.000],  loss: 4.787992, mae: 21.834139, mean_q: 28.794102, mean_eps: 0.257844
 166424/200000: episode: 1112, duration: 7.484s, episode steps: 1000, steps per second: 134, episode reward: 56.448, mean reward:  0.056 [-22.600, 23.314], mean action: 1.182 [0.000, 3.000],  loss: 3.932595, mae: 21.700126, mean_q: 28.699543, mean_eps: 0.253344
 166558/200000: episode: 1113, duration: 1.035s, episode steps: 134, steps per second: 130, episode reward: -53.172, mean reward: -0.397 [-100.000,  9.281], mean action: 2.015 [0.000, 3.000],  loss: 2.718798, mae: 21.819021, mean_q: 28.955111, mean_eps: 0.250793
 167558/200000: episode: 1114, duration: 9.108s, episode steps: 1000, steps per second: 110, episode reward:  7.779, mean reward:  0.008 [-22.177, 22.756], mean action: 2.128 [0.000, 3.000],  loss: 4.173199, mae: 21.623461, mean_q: 28.559066, mean_eps: 0.248241
 168558/200000: episode: 1115, duration: 8.785s, episode steps: 1000, steps per second: 114, episode reward: 105.248, mean reward:  0.105 [-24.561, 22.629], mean action: 1.475 [0.000, 3.000],  loss: 6.150156, mae: 22.026565, mean_q: 29.205151, mean_eps: 0.243741
 168673/200000: episode: 1116, duration: 1.188s, episode steps: 115, steps per second:  97, episode reward: -11.742, mean reward: -0.102 [-100.000, 26.251], mean action: 1.530 [0.000, 3.000],  loss: 4.403676, mae: 21.283407, mean_q: 28.297099, mean_eps: 0.241232
 169673/200000: episode: 1117, duration: 8.168s, episode steps: 1000, steps per second: 122, episode reward: 110.317, mean reward:  0.110 [-19.624, 22.186], mean action: 1.275 [0.000, 3.000],  loss: 4.832010, mae: 22.035279, mean_q: 29.229492, mean_eps: 0.238724
 169971/200000: episode: 1118, duration: 2.134s, episode steps: 298, steps per second: 140, episode reward: -26.951, mean reward: -0.090 [-100.000, 14.318], mean action: 1.752 [0.000, 3.000],  loss: 3.841178, mae: 22.266138, mean_q: 29.525961, mean_eps: 0.235803
 170971/200000: episode: 1119, duration: 8.549s, episode steps: 1000, steps per second: 117, episode reward: 163.298, mean reward:  0.163 [-17.718, 22.585], mean action: 1.102 [0.000, 3.000],  loss: 3.750467, mae: 21.762524, mean_q: 28.967925, mean_eps: 0.232883
 171971/200000: episode: 1120, duration: 8.172s, episode steps: 1000, steps per second: 122, episode reward: 104.859, mean reward:  0.105 [-24.811, 22.655], mean action: 1.299 [0.000, 3.000],  loss: 4.795805, mae: 21.986020, mean_q: 29.271133, mean_eps: 0.228383
 172971/200000: episode: 1121, duration: 7.208s, episode steps: 1000, steps per second: 139, episode reward: 106.141, mean reward:  0.106 [-21.291, 21.241], mean action: 0.921 [0.000, 3.000],  loss: 4.681010, mae: 22.243846, mean_q: 29.715943, mean_eps: 0.223883
 173443/200000: episode: 1122, duration: 3.379s, episode steps: 472, steps per second: 140, episode reward: -66.551, mean reward: -0.141 [-100.000, 11.495], mean action: 1.786 [0.000, 3.000],  loss: 5.127594, mae: 21.656301, mean_q: 29.044904, mean_eps: 0.220571
 174443/200000: episode: 1123, duration: 8.178s, episode steps: 1000, steps per second: 122, episode reward: 132.478, mean reward:  0.132 [-19.043, 25.466], mean action: 0.914 [0.000, 3.000],  loss: 4.675010, mae: 21.859236, mean_q: 29.248951, mean_eps: 0.217259
 175443/200000: episode: 1124, duration: 8.201s, episode steps: 1000, steps per second: 122, episode reward: 128.092, mean reward:  0.128 [-20.476, 22.748], mean action: 1.226 [0.000, 3.000],  loss: 3.486417, mae: 20.990993, mean_q: 28.138409, mean_eps: 0.212759
 175920/200000: episode: 1125, duration: 3.565s, episode steps: 477, steps per second: 134, episode reward: 181.840, mean reward:  0.381 [-18.942, 100.000], mean action: 2.031 [0.000, 3.000],  loss: 4.411798, mae: 20.892958, mean_q: 28.040139, mean_eps: 0.209435
 176920/200000: episode: 1126, duration: 8.175s, episode steps: 1000, steps per second: 122, episode reward: 71.964, mean reward:  0.072 [-21.194, 21.116], mean action: 1.030 [0.000, 3.000],  loss: 3.383903, mae: 20.946679, mean_q: 28.029198, mean_eps: 0.206112
 177687/200000: episode: 1127, duration: 6.167s, episode steps: 767, steps per second: 124, episode reward: 180.772, mean reward:  0.236 [-20.260, 100.000], mean action: 1.675 [0.000, 3.000],  loss: 4.941091, mae: 20.828483, mean_q: 27.947953, mean_eps: 0.202136
 177809/200000: episode: 1128, duration: 0.889s, episode steps: 122, steps per second: 137, episode reward: 19.058, mean reward:  0.156 [-100.000, 10.416], mean action: 1.705 [0.000, 3.000],  loss: 3.771743, mae: 21.719754, mean_q: 29.317269, mean_eps: 0.200136
 178085/200000: episode: 1129, duration: 2.167s, episode steps: 276, steps per second: 127, episode reward: 264.162, mean reward:  0.957 [-8.016, 100.000], mean action: 1.507 [0.000, 3.000],  loss: 3.170806, mae: 21.160419, mean_q: 28.521712, mean_eps: 0.199241
 179085/200000: episode: 1130, duration: 7.951s, episode steps: 1000, steps per second: 126, episode reward: -26.779, mean reward: -0.027 [-19.129, 21.275], mean action: 1.460 [0.000, 3.000],  loss: 3.799578, mae: 21.152796, mean_q: 28.520092, mean_eps: 0.196370
 179671/200000: episode: 1131, duration: 4.248s, episode steps: 586, steps per second: 138, episode reward: 248.874, mean reward:  0.425 [-18.155, 100.000], mean action: 1.218 [0.000, 3.000],  loss: 3.424294, mae: 20.929696, mean_q: 28.337235, mean_eps: 0.192801
 180671/200000: episode: 1132, duration: 7.357s, episode steps: 1000, steps per second: 136, episode reward: 71.120, mean reward:  0.071 [-20.155, 20.993], mean action: 0.974 [0.000, 3.000],  loss: 4.603047, mae: 20.873798, mean_q: 28.148264, mean_eps: 0.189233
 180955/200000: episode: 1133, duration: 1.862s, episode steps: 284, steps per second: 153, episode reward: 273.072, mean reward:  0.962 [-10.525, 100.000], mean action: 1.331 [0.000, 3.000],  loss: 4.334228, mae: 20.636589, mean_q: 27.913768, mean_eps: 0.186344
 181488/200000: episode: 1134, duration: 3.681s, episode steps: 533, steps per second: 145, episode reward: 208.036, mean reward:  0.390 [-19.221, 100.000], mean action: 1.418 [0.000, 3.000],  loss: 3.050488, mae: 21.185012, mean_q: 28.675216, mean_eps: 0.184505
 182360/200000: episode: 1135, duration: 7.508s, episode steps: 872, steps per second: 116, episode reward: 204.509, mean reward:  0.235 [-18.966, 100.000], mean action: 1.202 [0.000, 3.000],  loss: 3.654896, mae: 20.749549, mean_q: 28.047952, mean_eps: 0.181344
 182592/200000: episode: 1136, duration: 1.758s, episode steps: 232, steps per second: 132, episode reward: 68.823, mean reward:  0.297 [-100.000, 16.252], mean action: 1.759 [0.000, 3.000],  loss: 3.215024, mae: 20.787845, mean_q: 28.120080, mean_eps: 0.178860
 183407/200000: episode: 1137, duration: 6.313s, episode steps: 815, steps per second: 129, episode reward: 286.286, mean reward:  0.351 [-20.170, 100.000], mean action: 1.133 [0.000, 3.000],  loss: 4.876295, mae: 20.976215, mean_q: 28.353627, mean_eps: 0.176504
 184301/200000: episode: 1138, duration: 7.311s, episode steps: 894, steps per second: 122, episode reward: 158.747, mean reward:  0.178 [-17.532, 100.000], mean action: 1.544 [0.000, 3.000],  loss: 3.444106, mae: 20.674106, mean_q: 27.992732, mean_eps: 0.172659
 184711/200000: episode: 1139, duration: 2.564s, episode steps: 410, steps per second: 160, episode reward: 48.468, mean reward:  0.118 [-100.000, 15.452], mean action: 1.812 [0.000, 3.000],  loss: 3.808016, mae: 20.176279, mean_q: 27.369725, mean_eps: 0.169725
 185711/200000: episode: 1140, duration: 7.178s, episode steps: 1000, steps per second: 139, episode reward: 76.739, mean reward:  0.077 [-19.227, 12.177], mean action: 1.293 [0.000, 3.000],  loss: 4.070187, mae: 20.343067, mean_q: 27.509198, mean_eps: 0.166553
 185998/200000: episode: 1141, duration: 2.166s, episode steps: 287, steps per second: 132, episode reward: 304.311, mean reward:  1.060 [-17.723, 100.000], mean action: 1.334 [0.000, 3.000],  loss: 3.060494, mae: 20.083709, mean_q: 27.175236, mean_eps: 0.163657
 186115/200000: episode: 1142, duration: 0.825s, episode steps: 117, steps per second: 142, episode reward: -1.486, mean reward: -0.013 [-100.000, 16.599], mean action: 1.325 [0.000, 3.000],  loss: 2.435801, mae: 21.144373, mean_q: 28.682770, mean_eps: 0.162748
 187115/200000: episode: 1143, duration: 7.077s, episode steps: 1000, steps per second: 141, episode reward: -37.098, mean reward: -0.037 [-19.010, 22.912], mean action: 1.442 [0.000, 3.000],  loss: 3.722176, mae: 20.624624, mean_q: 27.836556, mean_eps: 0.160235
 187729/200000: episode: 1144, duration: 4.374s, episode steps: 614, steps per second: 140, episode reward: 226.243, mean reward:  0.368 [-20.478, 100.000], mean action: 1.127 [0.000, 3.000],  loss: 3.901595, mae: 20.225754, mean_q: 27.441353, mean_eps: 0.156603
 188519/200000: episode: 1145, duration: 6.609s, episode steps: 790, steps per second: 120, episode reward: 218.518, mean reward:  0.277 [-20.050, 100.000], mean action: 1.111 [0.000, 3.000],  loss: 4.122391, mae: 20.310972, mean_q: 27.420084, mean_eps: 0.153444
 189216/200000: episode: 1146, duration: 4.902s, episode steps: 697, steps per second: 142, episode reward: 212.180, mean reward:  0.304 [-19.885, 100.000], mean action: 1.260 [0.000, 3.000],  loss: 4.869002, mae: 20.157238, mean_q: 27.233151, mean_eps: 0.150098
 189613/200000: episode: 1147, duration: 2.687s, episode steps: 397, steps per second: 148, episode reward: 264.194, mean reward:  0.665 [-9.225, 100.000], mean action: 1.267 [0.000, 3.000],  loss: 3.966109, mae: 20.538743, mean_q: 27.772711, mean_eps: 0.147637
 190016/200000: episode: 1148, duration: 2.802s, episode steps: 403, steps per second: 144, episode reward: 264.473, mean reward:  0.656 [-11.325, 100.000], mean action: 0.846 [0.000, 3.000],  loss: 4.292170, mae: 20.904814, mean_q: 28.254514, mean_eps: 0.145837
 190624/200000: episode: 1149, duration: 4.556s, episode steps: 608, steps per second: 133, episode reward: -92.689, mean reward: -0.152 [-100.000,  8.593], mean action: 1.508 [0.000, 3.000],  loss: 4.079022, mae: 21.253826, mean_q: 28.766519, mean_eps: 0.143562
 191624/200000: episode: 1150, duration: 8.034s, episode steps: 1000, steps per second: 124, episode reward: -8.871, mean reward: -0.009 [-4.930,  6.647], mean action: 1.713 [0.000, 3.000],  loss: 5.423184, mae: 20.935008, mean_q: 28.239972, mean_eps: 0.139944
 191769/200000: episode: 1151, duration: 0.943s, episode steps: 145, steps per second: 154, episode reward: -78.738, mean reward: -0.543 [-100.000, 13.640], mean action: 1.607 [0.000, 3.000],  loss: 6.502309, mae: 21.604729, mean_q: 29.147837, mean_eps: 0.137368
 192279/200000: episode: 1152, duration: 4.505s, episode steps: 510, steps per second: 113, episode reward: 237.777, mean reward:  0.466 [-4.974, 100.000], mean action: 1.716 [0.000, 3.000],  loss: 6.020388, mae: 21.446423, mean_q: 28.864446, mean_eps: 0.135894
 192573/200000: episode: 1153, duration: 2.039s, episode steps: 294, steps per second: 144, episode reward: -0.955, mean reward: -0.003 [-100.000,  5.415], mean action: 1.633 [0.000, 3.000],  loss: 3.139671, mae: 21.611336, mean_q: 29.237907, mean_eps: 0.134085
 193207/200000: episode: 1154, duration: 4.599s, episode steps: 634, steps per second: 138, episode reward: 203.782, mean reward:  0.321 [-4.040, 100.000], mean action: 1.549 [0.000, 3.000],  loss: 5.245088, mae: 21.265799, mean_q: 28.734504, mean_eps: 0.131997
 193455/200000: episode: 1155, duration: 1.945s, episode steps: 248, steps per second: 128, episode reward: 266.906, mean reward:  1.076 [-9.422, 100.000], mean action: 1.278 [0.000, 3.000],  loss: 6.431435, mae: 21.390880, mean_q: 28.937930, mean_eps: 0.130013
 193875/200000: episode: 1156, duration: 3.010s, episode steps: 420, steps per second: 140, episode reward: 247.006, mean reward:  0.588 [-18.046, 100.000], mean action: 1.167 [0.000, 3.000],  loss: 3.775927, mae: 21.656373, mean_q: 29.241171, mean_eps: 0.128510
 194875/200000: episode: 1157, duration: 7.030s, episode steps: 1000, steps per second: 142, episode reward: 61.337, mean reward:  0.061 [-20.520, 22.430], mean action: 1.838 [0.000, 3.000],  loss: 5.090810, mae: 21.518493, mean_q: 29.145688, mean_eps: 0.125315
 195395/200000: episode: 1158, duration: 3.622s, episode steps: 520, steps per second: 144, episode reward: 231.769, mean reward:  0.446 [-17.671, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 5.578631, mae: 21.506898, mean_q: 29.121463, mean_eps: 0.121895
 195735/200000: episode: 1159, duration: 2.537s, episode steps: 340, steps per second: 134, episode reward: 260.015, mean reward:  0.765 [-9.896, 100.000], mean action: 1.476 [0.000, 3.000],  loss: 2.253645, mae: 21.348228, mean_q: 29.042944, mean_eps: 0.119960
 196284/200000: episode: 1160, duration: 4.479s, episode steps: 549, steps per second: 123, episode reward: 146.247, mean reward:  0.266 [-22.129, 100.000], mean action: 1.419 [0.000, 3.000],  loss: 5.735850, mae: 21.660911, mean_q: 29.422359, mean_eps: 0.117959
 196659/200000: episode: 1161, duration: 2.810s, episode steps: 375, steps per second: 133, episode reward: 191.749, mean reward:  0.511 [-13.101, 100.000], mean action: 1.360 [0.000, 3.000],  loss: 3.517327, mae: 21.588661, mean_q: 29.296420, mean_eps: 0.115880
 196762/200000: episode: 1162, duration: 0.804s, episode steps: 103, steps per second: 128, episode reward: 25.520, mean reward:  0.248 [-100.000,  8.589], mean action: 1.913 [0.000, 3.000],  loss: 3.317225, mae: 21.657785, mean_q: 29.251364, mean_eps: 0.114805
 197251/200000: episode: 1163, duration: 3.877s, episode steps: 489, steps per second: 126, episode reward: 177.880, mean reward:  0.364 [-17.594, 100.000], mean action: 1.313 [0.000, 3.000],  loss: 4.759019, mae: 21.702399, mean_q: 29.374079, mean_eps: 0.113473
 197376/200000: episode: 1164, duration: 0.863s, episode steps: 125, steps per second: 145, episode reward: 63.757, mean reward:  0.510 [-100.000, 16.992], mean action: 1.984 [0.000, 3.000],  loss: 3.452779, mae: 22.084916, mean_q: 29.896613, mean_eps: 0.112091
 197651/200000: episode: 1165, duration: 1.898s, episode steps: 275, steps per second: 145, episode reward: -3.918, mean reward: -0.014 [-100.000, 20.694], mean action: 1.804 [0.000, 3.000],  loss: 4.497406, mae: 22.575215, mean_q: 30.537048, mean_eps: 0.111191
 198099/200000: episode: 1166, duration: 2.929s, episode steps: 448, steps per second: 153, episode reward: 185.939, mean reward:  0.415 [-19.978, 100.000], mean action: 1.355 [0.000, 3.000],  loss: 4.537236, mae: 22.161833, mean_q: 30.029246, mean_eps: 0.109565
 198271/200000: episode: 1167, duration: 1.029s, episode steps: 172, steps per second: 167, episode reward: -80.100, mean reward: -0.466 [-100.000, 10.469], mean action: 1.802 [0.000, 3.000],  loss: 7.438487, mae: 22.331080, mean_q: 30.218357, mean_eps: 0.108170
 198457/200000: episode: 1168, duration: 1.169s, episode steps: 186, steps per second: 159, episode reward: -203.583, mean reward: -1.095 [-100.000, 37.752], mean action: 1.570 [0.000, 3.000],  loss: 4.660069, mae: 22.764264, mean_q: 30.818886, mean_eps: 0.107364
 198580/200000: episode: 1169, duration: 0.738s, episode steps: 123, steps per second: 167, episode reward: 26.117, mean reward:  0.212 [-100.000, 13.683], mean action: 1.854 [0.000, 3.000],  loss: 4.787029, mae: 22.453598, mean_q: 30.378311, mean_eps: 0.106669
 199516/200000: episode: 1170, duration: 7.550s, episode steps: 936, steps per second: 124, episode reward: 147.589, mean reward:  0.158 [-18.631, 100.000], mean action: 2.173 [0.000, 3.000],  loss: 6.664904, mae: 23.092853, mean_q: 31.209851, mean_eps: 0.104286
done, took 1514.676 seconds
Testing for 5 episodes ...
Episode 1: reward: -41.351, steps: 227
Episode 2: reward: 222.859, steps: 394
Episode 3: reward: 126.620, steps: 1000
Episode 4: reward: 244.933, steps: 316
Episode 5: reward: 234.707, steps: 344