Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 150000 steps ...
     93/150000: episode: 1, duration: 0.133s, episode steps:  93, steps per second: 701, episode reward: -115.098, mean reward: -1.238 [-100.000, 12.663], mean action: 1.559 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    210/150000: episode: 2, duration: 1.218s, episode steps: 117, steps per second:  96, episode reward: -171.760, mean reward: -1.468 [-100.000, 19.164], mean action: 1.624 [0.000, 3.000],  loss: 24.544618, mae: 1.193189, mean_q: 1.460490, mean_eps: 0.999070
    294/150000: episode: 3, duration: 0.487s, episode steps:  84, steps per second: 172, episode reward: -80.888, mean reward: -0.963 [-100.000,  7.298], mean action: 1.583 [0.000, 3.000],  loss: 31.907632, mae: 1.597973, mean_q: 1.511593, mean_eps: 0.998491
    414/150000: episode: 4, duration: 0.715s, episode steps: 120, steps per second: 168, episode reward: -379.890, mean reward: -3.166 [-100.000, 110.786], mean action: 1.450 [0.000, 3.000],  loss: 23.614650, mae: 2.098975, mean_q: 2.325129, mean_eps: 0.997879
    506/150000: episode: 5, duration: 0.543s, episode steps:  92, steps per second: 169, episode reward: -331.912, mean reward: -3.608 [-100.000,  5.127], mean action: 1.598 [0.000, 3.000],  loss: 25.103136, mae: 2.500624, mean_q: 1.984108, mean_eps: 0.997243
    594/150000: episode: 6, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -357.003, mean reward: -4.057 [-100.000,  2.393], mean action: 1.375 [0.000, 3.000],  loss: 21.315886, mae: 2.567499, mean_q: 0.764884, mean_eps: 0.996703
    686/150000: episode: 7, duration: 0.553s, episode steps:  92, steps per second: 166, episode reward: -96.949, mean reward: -1.054 [-100.000, 11.946], mean action: 1.348 [0.000, 3.000],  loss: 20.300616, mae: 3.281930, mean_q: 0.970192, mean_eps: 0.996163
    784/150000: episode: 8, duration: 0.590s, episode steps:  98, steps per second: 166, episode reward: -450.167, mean reward: -4.594 [-100.000,  2.428], mean action: 1.449 [0.000, 3.000],  loss: 22.135595, mae: 3.134641, mean_q: 0.807429, mean_eps: 0.995593
    889/150000: episode: 9, duration: 0.620s, episode steps: 105, steps per second: 169, episode reward: -76.807, mean reward: -0.731 [-100.000, 11.145], mean action: 1.552 [0.000, 3.000],  loss: 17.355506, mae: 3.063171, mean_q: 0.240727, mean_eps: 0.994984
    965/150000: episode: 10, duration: 0.450s, episode steps:  76, steps per second: 169, episode reward: -94.236, mean reward: -1.240 [-100.000, 15.678], mean action: 1.368 [0.000, 3.000],  loss: 13.721080, mae: 2.715511, mean_q: 0.482537, mean_eps: 0.994441
   1056/150000: episode: 11, duration: 0.561s, episode steps:  91, steps per second: 162, episode reward: -84.069, mean reward: -0.924 [-100.000, 22.789], mean action: 1.549 [0.000, 3.000],  loss: 24.372398, mae: 2.923046, mean_q: 1.189298, mean_eps: 0.993940
   1119/150000: episode: 12, duration: 0.435s, episode steps:  63, steps per second: 145, episode reward: -115.662, mean reward: -1.836 [-100.000,  6.761], mean action: 1.444 [0.000, 3.000],  loss: 22.383188, mae: 3.594937, mean_q: 1.897417, mean_eps: 0.993478
   1216/150000: episode: 13, duration: 0.584s, episode steps:  97, steps per second: 166, episode reward: -95.299, mean reward: -0.982 [-100.000, 11.131], mean action: 1.557 [0.000, 3.000],  loss: 25.642489, mae: 3.769144, mean_q: 1.724758, mean_eps: 0.992998
   1303/150000: episode: 14, duration: 0.529s, episode steps:  87, steps per second: 165, episode reward: -132.976, mean reward: -1.528 [-100.000,  7.319], mean action: 1.230 [0.000, 3.000],  loss: 25.364900, mae: 3.857790, mean_q: 2.121058, mean_eps: 0.992446
   1399/150000: episode: 15, duration: 0.552s, episode steps:  96, steps per second: 174, episode reward: -316.739, mean reward: -3.299 [-100.000,  5.028], mean action: 1.552 [0.000, 3.000],  loss: 25.301258, mae: 3.805932, mean_q: 1.934462, mean_eps: 0.991897
   1472/150000: episode: 16, duration: 0.451s, episode steps:  73, steps per second: 162, episode reward: -125.011, mean reward: -1.712 [-100.000,  7.256], mean action: 1.521 [0.000, 3.000],  loss: 22.070422, mae: 3.838246, mean_q: 1.789814, mean_eps: 0.991390
   1545/150000: episode: 17, duration: 0.420s, episode steps:  73, steps per second: 174, episode reward: -102.498, mean reward: -1.404 [-100.000,  6.832], mean action: 1.726 [0.000, 3.000],  loss: 29.302615, mae: 3.839415, mean_q: 1.695730, mean_eps: 0.990952
   1655/150000: episode: 18, duration: 0.662s, episode steps: 110, steps per second: 166, episode reward: -183.445, mean reward: -1.668 [-100.000, 16.506], mean action: 1.536 [0.000, 3.000],  loss: 25.980364, mae: 3.732849, mean_q: 2.085446, mean_eps: 0.990403
   1781/150000: episode: 19, duration: 0.777s, episode steps: 126, steps per second: 162, episode reward: -60.760, mean reward: -0.482 [-100.000, 15.988], mean action: 1.635 [0.000, 3.000],  loss: 21.664564, mae: 3.725184, mean_q: 2.071692, mean_eps: 0.989695
   1854/150000: episode: 20, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -93.250, mean reward: -1.277 [-100.000, 12.710], mean action: 1.370 [0.000, 3.000],  loss: 23.525035, mae: 3.957651, mean_q: 2.059763, mean_eps: 0.989098
   1934/150000: episode: 21, duration: 0.512s, episode steps:  80, steps per second: 156, episode reward: -135.145, mean reward: -1.689 [-100.000, 31.207], mean action: 1.575 [0.000, 3.000],  loss: 18.134310, mae: 3.532435, mean_q: 2.298551, mean_eps: 0.988639
   2043/150000: episode: 22, duration: 0.692s, episode steps: 109, steps per second: 157, episode reward: -278.415, mean reward: -2.554 [-100.000,  0.971], mean action: 1.569 [0.000, 3.000],  loss: 21.745122, mae: 3.923042, mean_q: 2.435586, mean_eps: 0.988072
   2144/150000: episode: 23, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: -138.341, mean reward: -1.370 [-100.000, 27.223], mean action: 1.614 [0.000, 3.000],  loss: 15.828365, mae: 4.186327, mean_q: 2.786721, mean_eps: 0.987442
   2250/150000: episode: 24, duration: 0.694s, episode steps: 106, steps per second: 153, episode reward: -169.733, mean reward: -1.601 [-100.000, 12.931], mean action: 1.425 [0.000, 3.000],  loss: 13.743928, mae: 4.363479, mean_q: 2.769529, mean_eps: 0.986821
   2336/150000: episode: 25, duration: 0.539s, episode steps:  86, steps per second: 160, episode reward: -376.697, mean reward: -4.380 [-100.000,  0.112], mean action: 1.593 [0.000, 3.000],  loss: 13.825151, mae: 4.463016, mean_q: 2.750993, mean_eps: 0.986245
   2414/150000: episode: 26, duration: 0.477s, episode steps:  78, steps per second: 164, episode reward:  1.029, mean reward:  0.013 [-100.000, 64.269], mean action: 1.551 [0.000, 3.000],  loss: 16.753841, mae: 4.663751, mean_q: 2.687259, mean_eps: 0.985753
   2525/150000: episode: 27, duration: 0.701s, episode steps: 111, steps per second: 158, episode reward: -128.106, mean reward: -1.154 [-100.000, 14.263], mean action: 1.550 [0.000, 3.000],  loss: 18.297826, mae: 4.579930, mean_q: 2.567764, mean_eps: 0.985186
   2618/150000: episode: 28, duration: 0.622s, episode steps:  93, steps per second: 150, episode reward: -239.842, mean reward: -2.579 [-100.000,  7.022], mean action: 1.505 [0.000, 3.000],  loss: 16.213098, mae: 4.375281, mean_q: 2.645303, mean_eps: 0.984574
   2705/150000: episode: 29, duration: 0.670s, episode steps:  87, steps per second: 130, episode reward: -97.515, mean reward: -1.121 [-100.000, 17.995], mean action: 1.356 [0.000, 3.000],  loss: 15.457628, mae: 4.470663, mean_q: 2.523006, mean_eps: 0.984034
   2829/150000: episode: 30, duration: 0.986s, episode steps: 124, steps per second: 126, episode reward: -120.050, mean reward: -0.968 [-100.000,  8.142], mean action: 1.573 [0.000, 3.000],  loss: 15.676964, mae: 4.335182, mean_q: 2.983806, mean_eps: 0.983401
   2888/150000: episode: 31, duration: 0.358s, episode steps:  59, steps per second: 165, episode reward: -48.233, mean reward: -0.818 [-100.000, 12.629], mean action: 1.356 [0.000, 3.000],  loss: 21.702825, mae: 4.757501, mean_q: 2.936187, mean_eps: 0.982852
   2980/150000: episode: 32, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -348.631, mean reward: -3.789 [-100.000,  0.277], mean action: 1.685 [0.000, 3.000],  loss: 15.528913, mae: 4.261560, mean_q: 3.086770, mean_eps: 0.982399
   3039/150000: episode: 33, duration: 0.382s, episode steps:  59, steps per second: 154, episode reward: -86.501, mean reward: -1.466 [-100.000, 19.130], mean action: 1.441 [0.000, 3.000],  loss: 12.371638, mae: 4.738926, mean_q: 3.448950, mean_eps: 0.981946
   3140/150000: episode: 34, duration: 0.662s, episode steps: 101, steps per second: 153, episode reward: -121.345, mean reward: -1.201 [-100.000, 13.871], mean action: 1.475 [0.000, 3.000],  loss: 18.427108, mae: 5.363812, mean_q: 3.543080, mean_eps: 0.981466
   3278/150000: episode: 35, duration: 0.886s, episode steps: 138, steps per second: 156, episode reward: -154.994, mean reward: -1.123 [-100.000,  7.176], mean action: 1.645 [0.000, 3.000],  loss: 16.396252, mae: 5.380090, mean_q: 3.677482, mean_eps: 0.980749
   3345/150000: episode: 36, duration: 0.487s, episode steps:  67, steps per second: 138, episode reward: -95.707, mean reward: -1.428 [-100.000, 16.914], mean action: 1.478 [0.000, 3.000],  loss: 13.288225, mae: 5.221876, mean_q: 3.907543, mean_eps: 0.980134
   3470/150000: episode: 37, duration: 0.866s, episode steps: 125, steps per second: 144, episode reward: -156.282, mean reward: -1.250 [-100.000,  5.550], mean action: 1.552 [0.000, 3.000],  loss: 18.133352, mae: 5.372973, mean_q: 3.759711, mean_eps: 0.979558
   3576/150000: episode: 38, duration: 0.661s, episode steps: 106, steps per second: 160, episode reward: -478.025, mean reward: -4.510 [-100.000,  0.311], mean action: 1.358 [0.000, 3.000],  loss: 19.983842, mae: 5.382182, mean_q: 3.459592, mean_eps: 0.978865
   3645/150000: episode: 39, duration: 0.475s, episode steps:  69, steps per second: 145, episode reward: -123.353, mean reward: -1.788 [-100.000, 30.942], mean action: 1.710 [0.000, 3.000],  loss: 27.674396, mae: 5.681601, mean_q: 3.236923, mean_eps: 0.978340
   3728/150000: episode: 40, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -119.891, mean reward: -1.444 [-100.000,  6.585], mean action: 1.530 [0.000, 3.000],  loss: 19.615228, mae: 5.408394, mean_q: 3.784668, mean_eps: 0.977884
   3813/150000: episode: 41, duration: 0.635s, episode steps:  85, steps per second: 134, episode reward: -141.807, mean reward: -1.668 [-100.000, 26.790], mean action: 1.447 [0.000, 3.000],  loss: 15.949291, mae: 5.381942, mean_q: 3.469315, mean_eps: 0.977380
   3904/150000: episode: 42, duration: 0.745s, episode steps:  91, steps per second: 122, episode reward: -278.685, mean reward: -3.062 [-100.000, 15.930], mean action: 1.484 [0.000, 3.000],  loss: 18.016397, mae: 5.180421, mean_q: 3.489792, mean_eps: 0.976852
   4009/150000: episode: 43, duration: 0.774s, episode steps: 105, steps per second: 136, episode reward: -327.133, mean reward: -3.116 [-100.000,  3.727], mean action: 1.629 [0.000, 3.000],  loss: 19.529468, mae: 5.072009, mean_q: 3.394358, mean_eps: 0.976264
   4085/150000: episode: 44, duration: 0.595s, episode steps:  76, steps per second: 128, episode reward: -200.879, mean reward: -2.643 [-100.000,  7.260], mean action: 1.566 [0.000, 3.000],  loss: 16.439981, mae: 6.312844, mean_q: 3.710289, mean_eps: 0.975721
   4172/150000: episode: 45, duration: 0.744s, episode steps:  87, steps per second: 117, episode reward: -170.397, mean reward: -1.959 [-100.000, 25.179], mean action: 1.540 [0.000, 3.000],  loss: 13.867018, mae: 6.069509, mean_q: 3.953249, mean_eps: 0.975232
   4271/150000: episode: 46, duration: 1.053s, episode steps:  99, steps per second:  94, episode reward: -150.924, mean reward: -1.524 [-100.000,  7.881], mean action: 1.303 [0.000, 3.000],  loss: 12.670297, mae: 6.141579, mean_q: 3.726781, mean_eps: 0.974674
   4390/150000: episode: 47, duration: 1.289s, episode steps: 119, steps per second:  92, episode reward: -131.561, mean reward: -1.106 [-100.000,  7.589], mean action: 1.479 [0.000, 3.000],  loss: 14.436843, mae: 6.127183, mean_q: 3.947853, mean_eps: 0.974020
   4475/150000: episode: 48, duration: 0.818s, episode steps:  85, steps per second: 104, episode reward: -123.710, mean reward: -1.455 [-100.000, 10.458], mean action: 1.518 [0.000, 3.000],  loss: 14.492799, mae: 5.947930, mean_q: 4.131017, mean_eps: 0.973408
   4568/150000: episode: 49, duration: 0.945s, episode steps:  93, steps per second:  98, episode reward: -28.945, mean reward: -0.311 [-100.000, 96.969], mean action: 1.409 [0.000, 3.000],  loss: 13.367179, mae: 6.190709, mean_q: 3.958899, mean_eps: 0.972874
   4691/150000: episode: 50, duration: 1.063s, episode steps: 123, steps per second: 116, episode reward: -156.595, mean reward: -1.273 [-100.000,  4.874], mean action: 1.480 [0.000, 3.000],  loss: 19.648580, mae: 6.048591, mean_q: 3.874250, mean_eps: 0.972226
   4813/150000: episode: 51, duration: 1.081s, episode steps: 122, steps per second: 113, episode reward: -64.932, mean reward: -0.532 [-100.000, 17.829], mean action: 1.557 [0.000, 3.000],  loss: 18.357118, mae: 6.086884, mean_q: 3.743352, mean_eps: 0.971491
   4903/150000: episode: 52, duration: 0.706s, episode steps:  90, steps per second: 127, episode reward: -101.949, mean reward: -1.133 [-100.000, 14.233], mean action: 1.589 [0.000, 3.000],  loss: 16.949552, mae: 5.992758, mean_q: 4.159093, mean_eps: 0.970855
   4981/150000: episode: 53, duration: 0.623s, episode steps:  78, steps per second: 125, episode reward: -115.312, mean reward: -1.478 [-100.000,  6.921], mean action: 1.423 [0.000, 3.000],  loss: 11.798558, mae: 5.958168, mean_q: 4.306607, mean_eps: 0.970351
   5076/150000: episode: 54, duration: 0.719s, episode steps:  95, steps per second: 132, episode reward: -135.663, mean reward: -1.428 [-100.000, 11.008], mean action: 1.400 [0.000, 3.000],  loss: 17.837174, mae: 6.670062, mean_q: 3.969616, mean_eps: 0.969832
   5185/150000: episode: 55, duration: 0.798s, episode steps: 109, steps per second: 137, episode reward: -282.845, mean reward: -2.595 [-100.000, 78.101], mean action: 1.440 [0.000, 3.000],  loss: 15.213002, mae: 7.172789, mean_q: 3.970953, mean_eps: 0.969220
   5275/150000: episode: 56, duration: 0.659s, episode steps:  90, steps per second: 137, episode reward: -261.971, mean reward: -2.911 [-100.000,  6.310], mean action: 1.567 [0.000, 3.000],  loss: 14.504699, mae: 7.181021, mean_q: 3.667340, mean_eps: 0.968623
   5400/150000: episode: 57, duration: 0.887s, episode steps: 125, steps per second: 141, episode reward: -114.557, mean reward: -0.916 [-100.000,  7.701], mean action: 1.472 [0.000, 3.000],  loss: 13.155445, mae: 6.893158, mean_q: 3.637199, mean_eps: 0.967978
   5459/150000: episode: 58, duration: 0.379s, episode steps:  59, steps per second: 156, episode reward: -241.314, mean reward: -4.090 [-100.000, 44.872], mean action: 1.559 [0.000, 3.000],  loss: 12.104109, mae: 6.543019, mean_q: 4.285657, mean_eps: 0.967426
   5556/150000: episode: 59, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -156.023, mean reward: -1.608 [-100.000, 18.031], mean action: 1.546 [0.000, 3.000],  loss: 12.573703, mae: 7.219485, mean_q: 3.531512, mean_eps: 0.966958
   5620/150000: episode: 60, duration: 0.470s, episode steps:  64, steps per second: 136, episode reward: -200.643, mean reward: -3.135 [-100.000, 42.057], mean action: 1.375 [0.000, 3.000],  loss: 11.220124, mae: 6.844668, mean_q: 4.094527, mean_eps: 0.966475
   5748/150000: episode: 61, duration: 0.883s, episode steps: 128, steps per second: 145, episode reward: -113.771, mean reward: -0.889 [-100.000, 12.246], mean action: 1.469 [0.000, 3.000],  loss: 13.972066, mae: 6.829935, mean_q: 3.585158, mean_eps: 0.965899
   5813/150000: episode: 62, duration: 0.439s, episode steps:  65, steps per second: 148, episode reward: -128.274, mean reward: -1.973 [-100.000, 34.066], mean action: 1.462 [0.000, 3.000],  loss: 14.487140, mae: 6.972938, mean_q: 3.433629, mean_eps: 0.965320
   5899/150000: episode: 63, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -76.410, mean reward: -0.888 [-100.000, 10.819], mean action: 1.407 [0.000, 3.000],  loss: 11.692378, mae: 6.833830, mean_q: 3.593269, mean_eps: 0.964867
   6021/150000: episode: 64, duration: 0.865s, episode steps: 122, steps per second: 141, episode reward: -232.824, mean reward: -1.908 [-100.000,  4.818], mean action: 1.541 [0.000, 3.000],  loss: 12.544515, mae: 7.079640, mean_q: 4.117552, mean_eps: 0.964243
   6091/150000: episode: 65, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -92.088, mean reward: -1.316 [-100.000, 36.836], mean action: 1.571 [0.000, 3.000],  loss: 13.810270, mae: 7.940336, mean_q: 4.422783, mean_eps: 0.963667
   6163/150000: episode: 66, duration: 0.461s, episode steps:  72, steps per second: 156, episode reward: -50.242, mean reward: -0.698 [-100.000, 11.999], mean action: 1.528 [0.000, 3.000],  loss: 13.400370, mae: 7.743819, mean_q: 5.035287, mean_eps: 0.963241
   6269/150000: episode: 67, duration: 0.717s, episode steps: 106, steps per second: 148, episode reward: -285.082, mean reward: -2.689 [-100.000, 108.083], mean action: 1.500 [0.000, 3.000],  loss: 11.429059, mae: 7.966436, mean_q: 4.650674, mean_eps: 0.962707
   6351/150000: episode: 68, duration: 0.640s, episode steps:  82, steps per second: 128, episode reward: -123.706, mean reward: -1.509 [-100.000,  8.191], mean action: 1.476 [0.000, 3.000],  loss: 12.435406, mae: 8.026945, mean_q: 4.363116, mean_eps: 0.962143
   6488/150000: episode: 69, duration: 1.036s, episode steps: 137, steps per second: 132, episode reward: -211.840, mean reward: -1.546 [-100.000, 14.325], mean action: 1.723 [0.000, 3.000],  loss: 15.239830, mae: 8.051095, mean_q: 4.386737, mean_eps: 0.961486
   6549/150000: episode: 70, duration: 0.459s, episode steps:  61, steps per second: 133, episode reward: -85.250, mean reward: -1.398 [-100.000, 30.569], mean action: 1.639 [0.000, 3.000],  loss: 9.300645, mae: 8.341609, mean_q: 4.252768, mean_eps: 0.960892
   6653/150000: episode: 71, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -104.640, mean reward: -1.006 [-100.000, 12.297], mean action: 1.577 [0.000, 3.000],  loss: 12.321892, mae: 8.085947, mean_q: 4.605631, mean_eps: 0.960397
   6723/150000: episode: 72, duration: 0.467s, episode steps:  70, steps per second: 150, episode reward: -96.039, mean reward: -1.372 [-100.000, 10.918], mean action: 1.586 [0.000, 3.000],  loss: 9.484796, mae: 8.196789, mean_q: 4.473190, mean_eps: 0.959875
   6787/150000: episode: 73, duration: 0.424s, episode steps:  64, steps per second: 151, episode reward: -85.285, mean reward: -1.333 [-100.000,  7.534], mean action: 1.422 [0.000, 3.000],  loss: 12.936113, mae: 8.042929, mean_q: 4.599909, mean_eps: 0.959473
   6917/150000: episode: 74, duration: 0.866s, episode steps: 130, steps per second: 150, episode reward: -78.401, mean reward: -0.603 [-100.000, 16.031], mean action: 1.331 [0.000, 3.000],  loss: 13.242859, mae: 8.070098, mean_q: 4.789736, mean_eps: 0.958891
   7010/150000: episode: 75, duration: 0.580s, episode steps:  93, steps per second: 160, episode reward: -366.374, mean reward: -3.940 [-100.000,  0.115], mean action: 1.495 [0.000, 3.000],  loss: 14.408352, mae: 8.352772, mean_q: 4.799095, mean_eps: 0.958222
   7071/150000: episode: 76, duration: 0.377s, episode steps:  61, steps per second: 162, episode reward: -92.532, mean reward: -1.517 [-100.000,  8.755], mean action: 1.246 [0.000, 3.000],  loss: 12.277572, mae: 9.420037, mean_q: 5.079244, mean_eps: 0.957760
   7137/150000: episode: 77, duration: 0.411s, episode steps:  66, steps per second: 161, episode reward: -197.381, mean reward: -2.991 [-100.000, 57.200], mean action: 1.682 [0.000, 3.000],  loss: 13.439673, mae: 8.951017, mean_q: 5.209466, mean_eps: 0.957379
   7232/150000: episode: 78, duration: 0.631s, episode steps:  95, steps per second: 151, episode reward: -36.845, mean reward: -0.388 [-100.000, 20.381], mean action: 1.432 [0.000, 3.000],  loss: 11.048859, mae: 8.879045, mean_q: 5.409947, mean_eps: 0.956896
   7333/150000: episode: 79, duration: 0.662s, episode steps: 101, steps per second: 152, episode reward: -105.292, mean reward: -1.042 [-100.000, 12.621], mean action: 1.366 [0.000, 3.000],  loss: 10.852253, mae: 9.056146, mean_q: 5.392955, mean_eps: 0.956308
   7432/150000: episode: 80, duration: 0.721s, episode steps:  99, steps per second: 137, episode reward: -103.479, mean reward: -1.045 [-100.000,  7.287], mean action: 1.535 [0.000, 3.000],  loss: 13.248922, mae: 9.173028, mean_q: 5.336189, mean_eps: 0.955708
   7558/150000: episode: 81, duration: 0.846s, episode steps: 126, steps per second: 149, episode reward: -46.802, mean reward: -0.371 [-100.000,  8.500], mean action: 1.675 [0.000, 3.000],  loss: 12.487651, mae: 9.131971, mean_q: 5.332175, mean_eps: 0.955033
   7661/150000: episode: 82, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -86.808, mean reward: -0.843 [-100.000, 14.472], mean action: 1.534 [0.000, 3.000],  loss: 13.142907, mae: 9.189865, mean_q: 5.114441, mean_eps: 0.954346
   7747/150000: episode: 83, duration: 0.626s, episode steps:  86, steps per second: 137, episode reward: -114.220, mean reward: -1.328 [-100.000, 12.335], mean action: 1.395 [0.000, 3.000],  loss: 15.072293, mae: 9.345139, mean_q: 5.132587, mean_eps: 0.953779
   7864/150000: episode: 84, duration: 0.809s, episode steps: 117, steps per second: 145, episode reward: -278.998, mean reward: -2.385 [-100.000, 90.948], mean action: 1.453 [0.000, 3.000],  loss: 11.507803, mae: 9.147995, mean_q: 5.350573, mean_eps: 0.953170
   7951/150000: episode: 85, duration: 0.528s, episode steps:  87, steps per second: 165, episode reward: -205.312, mean reward: -2.360 [-100.000, 15.243], mean action: 1.529 [0.000, 3.000],  loss: 11.988898, mae: 9.700462, mean_q: 5.181514, mean_eps: 0.952558
   8027/150000: episode: 86, duration: 0.491s, episode steps:  76, steps per second: 155, episode reward: -146.336, mean reward: -1.925 [-100.000,  9.333], mean action: 1.671 [0.000, 3.000],  loss: 14.072720, mae: 9.371829, mean_q: 5.572890, mean_eps: 0.952069
   8107/150000: episode: 87, duration: 0.508s, episode steps:  80, steps per second: 158, episode reward: -91.867, mean reward: -1.148 [-100.000,  5.938], mean action: 1.637 [0.000, 3.000],  loss: 14.132175, mae: 10.563053, mean_q: 5.918343, mean_eps: 0.951601
   8212/150000: episode: 88, duration: 0.728s, episode steps: 105, steps per second: 144, episode reward: -248.056, mean reward: -2.362 [-100.000,  0.485], mean action: 1.448 [0.000, 3.000],  loss: 10.850248, mae: 10.711540, mean_q: 5.223080, mean_eps: 0.951046
   8302/150000: episode: 89, duration: 0.572s, episode steps:  90, steps per second: 157, episode reward: -81.499, mean reward: -0.906 [-100.000, 20.974], mean action: 1.578 [0.000, 3.000],  loss: 16.790941, mae: 10.507625, mean_q: 5.616654, mean_eps: 0.950461
   8411/150000: episode: 90, duration: 0.736s, episode steps: 109, steps per second: 148, episode reward: -165.063, mean reward: -1.514 [-100.000, 11.231], mean action: 1.486 [0.000, 3.000],  loss: 14.050628, mae: 10.513776, mean_q: 5.526505, mean_eps: 0.949864
   8503/150000: episode: 91, duration: 0.647s, episode steps:  92, steps per second: 142, episode reward: -143.237, mean reward: -1.557 [-100.000,  7.819], mean action: 1.511 [0.000, 3.000],  loss: 10.039402, mae: 10.445404, mean_q: 5.835890, mean_eps: 0.949261
   8608/150000: episode: 92, duration: 0.690s, episode steps: 105, steps per second: 152, episode reward: -155.673, mean reward: -1.483 [-100.000,  8.567], mean action: 1.448 [0.000, 3.000],  loss: 12.924803, mae: 10.404852, mean_q: 5.812021, mean_eps: 0.948670
   8707/150000: episode: 93, duration: 0.607s, episode steps:  99, steps per second: 163, episode reward: -124.075, mean reward: -1.253 [-100.000, 11.613], mean action: 1.384 [0.000, 3.000],  loss: 10.835006, mae: 10.550228, mean_q: 6.002290, mean_eps: 0.948058
   8773/150000: episode: 94, duration: 0.405s, episode steps:  66, steps per second: 163, episode reward: -52.301, mean reward: -0.792 [-100.000, 13.296], mean action: 1.485 [0.000, 3.000],  loss: 11.222327, mae: 9.976468, mean_q: 6.612273, mean_eps: 0.947563
   8898/150000: episode: 95, duration: 0.796s, episode steps: 125, steps per second: 157, episode reward: -207.181, mean reward: -1.657 [-100.000,  1.935], mean action: 1.536 [0.000, 3.000],  loss: 11.461208, mae: 10.379064, mean_q: 6.118290, mean_eps: 0.946990
   8978/150000: episode: 96, duration: 0.491s, episode steps:  80, steps per second: 163, episode reward: -148.117, mean reward: -1.851 [-100.000, 16.557], mean action: 1.538 [0.000, 3.000],  loss: 10.333081, mae: 10.306512, mean_q: 5.640471, mean_eps: 0.946375
   9075/150000: episode: 97, duration: 0.607s, episode steps:  97, steps per second: 160, episode reward: -129.451, mean reward: -1.335 [-100.000, 16.573], mean action: 1.629 [0.000, 3.000],  loss: 11.375873, mae: 11.142555, mean_q: 5.957091, mean_eps: 0.945844
   9148/150000: episode: 98, duration: 0.491s, episode steps:  73, steps per second: 149, episode reward: -124.938, mean reward: -1.711 [-100.000,  9.914], mean action: 1.493 [0.000, 3.000],  loss: 12.929275, mae: 11.127684, mean_q: 5.931338, mean_eps: 0.945334
   9236/150000: episode: 99, duration: 0.569s, episode steps:  88, steps per second: 155, episode reward: -311.146, mean reward: -3.536 [-100.000,  6.630], mean action: 1.602 [0.000, 3.000],  loss: 12.039179, mae: 11.220533, mean_q: 6.191321, mean_eps: 0.944851
   9304/150000: episode: 100, duration: 0.424s, episode steps:  68, steps per second: 160, episode reward: -137.964, mean reward: -2.029 [-100.000, 15.702], mean action: 1.603 [0.000, 3.000],  loss: 8.473913, mae: 11.102570, mean_q: 5.624313, mean_eps: 0.944383
   9369/150000: episode: 101, duration: 0.408s, episode steps:  65, steps per second: 159, episode reward: -102.909, mean reward: -1.583 [-100.000,  6.942], mean action: 1.800 [0.000, 3.000],  loss: 10.137023, mae: 11.353505, mean_q: 5.571027, mean_eps: 0.943984
   9430/150000: episode: 102, duration: 0.445s, episode steps:  61, steps per second: 137, episode reward: -124.798, mean reward: -2.046 [-100.000,  5.235], mean action: 1.361 [0.000, 3.000],  loss: 12.355260, mae: 11.122309, mean_q: 6.054047, mean_eps: 0.943606
   9524/150000: episode: 103, duration: 0.761s, episode steps:  94, steps per second: 124, episode reward: -112.373, mean reward: -1.195 [-100.000,  8.829], mean action: 1.617 [0.000, 3.000],  loss: 12.055286, mae: 11.191515, mean_q: 6.087221, mean_eps: 0.943141
   9605/150000: episode: 104, duration: 0.546s, episode steps:  81, steps per second: 148, episode reward: -297.695, mean reward: -3.675 [-100.000, 110.900], mean action: 1.346 [0.000, 3.000],  loss: 10.943264, mae: 11.197520, mean_q: 6.320689, mean_eps: 0.942616
   9704/150000: episode: 105, duration: 0.629s, episode steps:  99, steps per second: 157, episode reward: -403.225, mean reward: -4.073 [-100.000,  1.004], mean action: 1.556 [0.000, 3.000],  loss: 10.027367, mae: 11.437673, mean_q: 5.935436, mean_eps: 0.942076
   9806/150000: episode: 106, duration: 0.665s, episode steps: 102, steps per second: 153, episode reward: -222.297, mean reward: -2.179 [-100.000,  6.119], mean action: 1.520 [0.000, 3.000],  loss: 11.551233, mae: 11.394166, mean_q: 5.647771, mean_eps: 0.941473
   9890/150000: episode: 107, duration: 0.538s, episode steps:  84, steps per second: 156, episode reward: -121.088, mean reward: -1.442 [-100.000,  8.677], mean action: 1.655 [0.000, 3.000],  loss: 10.567910, mae: 11.496427, mean_q: 5.518203, mean_eps: 0.940915
   9968/150000: episode: 108, duration: 0.503s, episode steps:  78, steps per second: 155, episode reward: -129.450, mean reward: -1.660 [-100.000, 41.462], mean action: 1.474 [0.000, 3.000],  loss: 13.956005, mae: 11.221833, mean_q: 6.061694, mean_eps: 0.940429
  10080/150000: episode: 109, duration: 0.730s, episode steps: 112, steps per second: 153, episode reward: -188.992, mean reward: -1.687 [-100.000, 127.959], mean action: 1.464 [0.000, 3.000],  loss: 14.394460, mae: 12.317203, mean_q: 6.601867, mean_eps: 0.939859
  10164/150000: episode: 110, duration: 0.550s, episode steps:  84, steps per second: 153, episode reward: -91.171, mean reward: -1.085 [-100.000, 18.431], mean action: 1.548 [0.000, 3.000],  loss: 8.768426, mae: 12.574885, mean_q: 6.389343, mean_eps: 0.939271
  10249/150000: episode: 111, duration: 0.531s, episode steps:  85, steps per second: 160, episode reward: -23.889, mean reward: -0.281 [-100.000, 91.854], mean action: 1.529 [0.000, 3.000],  loss: 11.921529, mae: 12.956030, mean_q: 6.333822, mean_eps: 0.938764
  10331/150000: episode: 112, duration: 0.495s, episode steps:  82, steps per second: 166, episode reward: -84.826, mean reward: -1.034 [-100.000,  6.610], mean action: 1.537 [0.000, 3.000],  loss: 9.357200, mae: 12.364864, mean_q: 7.132626, mean_eps: 0.938263
  10417/150000: episode: 113, duration: 0.519s, episode steps:  86, steps per second: 166, episode reward: -105.541, mean reward: -1.227 [-100.000,  7.556], mean action: 1.558 [0.000, 3.000],  loss: 17.273128, mae: 12.157358, mean_q: 7.013533, mean_eps: 0.937759
  10514/150000: episode: 114, duration: 0.649s, episode steps:  97, steps per second: 149, episode reward: -156.585, mean reward: -1.614 [-100.000,  8.594], mean action: 1.629 [0.000, 3.000],  loss: 12.123404, mae: 12.465786, mean_q: 6.745987, mean_eps: 0.937210
  10608/150000: episode: 115, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -235.055, mean reward: -2.501 [-100.000,  0.524], mean action: 1.702 [0.000, 3.000],  loss: 15.648363, mae: 12.243364, mean_q: 7.178287, mean_eps: 0.936637
  10713/150000: episode: 116, duration: 0.684s, episode steps: 105, steps per second: 154, episode reward: -260.500, mean reward: -2.481 [-100.000, 42.698], mean action: 1.695 [0.000, 3.000],  loss: 14.493563, mae: 12.632924, mean_q: 6.438381, mean_eps: 0.936040
  10773/150000: episode: 117, duration: 0.404s, episode steps:  60, steps per second: 148, episode reward: -68.356, mean reward: -1.139 [-100.000,  7.579], mean action: 1.433 [0.000, 3.000],  loss: 11.178477, mae: 12.150961, mean_q: 6.074088, mean_eps: 0.935545
  10867/150000: episode: 118, duration: 0.624s, episode steps:  94, steps per second: 151, episode reward: -153.002, mean reward: -1.628 [-100.000,  8.478], mean action: 1.564 [0.000, 3.000],  loss: 13.486776, mae: 12.568342, mean_q: 6.386041, mean_eps: 0.935083
  10968/150000: episode: 119, duration: 0.673s, episode steps: 101, steps per second: 150, episode reward: -154.561, mean reward: -1.530 [-100.000,  5.155], mean action: 1.485 [0.000, 3.000],  loss: 11.610609, mae: 12.062466, mean_q: 7.001323, mean_eps: 0.934498
  11048/150000: episode: 120, duration: 0.495s, episode steps:  80, steps per second: 162, episode reward: -114.111, mean reward: -1.426 [-100.000, 11.960], mean action: 1.500 [0.000, 3.000],  loss: 10.393907, mae: 12.756925, mean_q: 6.434813, mean_eps: 0.933955
  11116/150000: episode: 121, duration: 0.449s, episode steps:  68, steps per second: 152, episode reward: -101.466, mean reward: -1.492 [-100.000,  8.578], mean action: 1.353 [0.000, 3.000],  loss: 12.976033, mae: 13.527197, mean_q: 6.834679, mean_eps: 0.933511
  11194/150000: episode: 122, duration: 0.538s, episode steps:  78, steps per second: 145, episode reward: -87.937, mean reward: -1.127 [-100.000, 11.451], mean action: 1.590 [0.000, 3.000],  loss: 11.435376, mae: 13.201228, mean_q: 6.927893, mean_eps: 0.933073
  11338/150000: episode: 123, duration: 0.979s, episode steps: 144, steps per second: 147, episode reward: -171.030, mean reward: -1.188 [-100.000,  6.352], mean action: 1.674 [0.000, 3.000],  loss: 12.991920, mae: 13.697908, mean_q: 6.807731, mean_eps: 0.932407
  11421/150000: episode: 124, duration: 0.542s, episode steps:  83, steps per second: 153, episode reward: -95.762, mean reward: -1.154 [-100.000, 16.835], mean action: 1.506 [0.000, 3.000],  loss: 12.107483, mae: 13.450967, mean_q: 6.965660, mean_eps: 0.931726
  11533/150000: episode: 125, duration: 0.748s, episode steps: 112, steps per second: 150, episode reward: -50.468, mean reward: -0.451 [-100.000, 21.265], mean action: 1.571 [0.000, 3.000],  loss: 10.566610, mae: 13.508488, mean_q: 6.928934, mean_eps: 0.931141
  11618/150000: episode: 126, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -75.451, mean reward: -0.888 [-100.000, 17.541], mean action: 1.424 [0.000, 3.000],  loss: 8.174546, mae: 13.445860, mean_q: 7.034694, mean_eps: 0.930550
  11690/150000: episode: 127, duration: 0.461s, episode steps:  72, steps per second: 156, episode reward: -79.898, mean reward: -1.110 [-100.000,  7.513], mean action: 1.597 [0.000, 3.000],  loss: 8.459563, mae: 13.323304, mean_q: 7.372065, mean_eps: 0.930079
  11771/150000: episode: 128, duration: 0.527s, episode steps:  81, steps per second: 154, episode reward: -361.027, mean reward: -4.457 [-100.000, 81.221], mean action: 1.593 [0.000, 3.000],  loss: 9.411826, mae: 13.880548, mean_q: 6.901197, mean_eps: 0.929620
  11874/150000: episode: 129, duration: 0.653s, episode steps: 103, steps per second: 158, episode reward: -97.950, mean reward: -0.951 [-100.000,  7.584], mean action: 1.592 [0.000, 3.000],  loss: 8.990219, mae: 13.630288, mean_q: 7.445079, mean_eps: 0.929068
  11946/150000: episode: 130, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -138.816, mean reward: -1.928 [-100.000,  7.761], mean action: 1.597 [0.000, 3.000],  loss: 10.594880, mae: 13.542006, mean_q: 6.519893, mean_eps: 0.928543
  12033/150000: episode: 131, duration: 0.570s, episode steps:  87, steps per second: 153, episode reward: -32.795, mean reward: -0.377 [-100.000, 11.218], mean action: 1.552 [0.000, 3.000],  loss: 12.190500, mae: 14.187948, mean_q: 7.037169, mean_eps: 0.928066
  12137/150000: episode: 132, duration: 0.817s, episode steps: 104, steps per second: 127, episode reward: -187.941, mean reward: -1.807 [-100.000,  1.813], mean action: 1.644 [0.000, 3.000],  loss: 13.383901, mae: 15.008656, mean_q: 6.699350, mean_eps: 0.927493
  12252/150000: episode: 133, duration: 0.843s, episode steps: 115, steps per second: 136, episode reward: -107.796, mean reward: -0.937 [-100.000,  5.788], mean action: 1.391 [0.000, 3.000],  loss: 13.526540, mae: 14.983747, mean_q: 7.089528, mean_eps: 0.926836
  12345/150000: episode: 134, duration: 0.626s, episode steps:  93, steps per second: 149, episode reward: -124.682, mean reward: -1.341 [-100.000,  5.664], mean action: 1.570 [0.000, 3.000],  loss: 11.321056, mae: 14.755470, mean_q: 7.462072, mean_eps: 0.926212
  12453/150000: episode: 135, duration: 0.731s, episode steps: 108, steps per second: 148, episode reward: -224.150, mean reward: -2.075 [-100.000, 50.143], mean action: 1.370 [0.000, 3.000],  loss: 8.265681, mae: 14.562903, mean_q: 7.295548, mean_eps: 0.925609
  12573/150000: episode: 136, duration: 0.768s, episode steps: 120, steps per second: 156, episode reward: -439.905, mean reward: -3.666 [-100.000, 57.632], mean action: 1.667 [0.000, 3.000],  loss: 13.062385, mae: 14.635695, mean_q: 7.409147, mean_eps: 0.924925
  12676/150000: episode: 137, duration: 0.690s, episode steps: 103, steps per second: 149, episode reward: -315.210, mean reward: -3.060 [-100.000,  1.435], mean action: 1.524 [0.000, 3.000],  loss: 12.360355, mae: 14.971633, mean_q: 6.256960, mean_eps: 0.924256
  12764/150000: episode: 138, duration: 0.603s, episode steps:  88, steps per second: 146, episode reward: -90.104, mean reward: -1.024 [-100.000,  7.221], mean action: 1.477 [0.000, 3.000],  loss: 9.636107, mae: 14.711194, mean_q: 7.448241, mean_eps: 0.923683
  12874/150000: episode: 139, duration: 0.739s, episode steps: 110, steps per second: 149, episode reward: -85.821, mean reward: -0.780 [-100.000, 87.564], mean action: 1.691 [0.000, 3.000],  loss: 9.317491, mae: 14.788428, mean_q: 7.041134, mean_eps: 0.923089
  12959/150000: episode: 140, duration: 0.544s, episode steps:  85, steps per second: 156, episode reward: -91.702, mean reward: -1.079 [-100.000,  6.827], mean action: 1.576 [0.000, 3.000],  loss: 12.659550, mae: 14.575220, mean_q: 6.817129, mean_eps: 0.922504
  13045/150000: episode: 141, duration: 0.656s, episode steps:  86, steps per second: 131, episode reward: -201.996, mean reward: -2.349 [-100.000,  3.889], mean action: 1.488 [0.000, 3.000],  loss: 11.320487, mae: 14.634859, mean_q: 7.538830, mean_eps: 0.921991
  13133/150000: episode: 142, duration: 0.666s, episode steps:  88, steps per second: 132, episode reward: -151.724, mean reward: -1.724 [-100.000, 19.854], mean action: 1.341 [0.000, 3.000],  loss: 11.290389, mae: 15.260948, mean_q: 6.473481, mean_eps: 0.921469
  13227/150000: episode: 143, duration: 0.655s, episode steps:  94, steps per second: 143, episode reward: -65.080, mean reward: -0.692 [-100.000, 28.841], mean action: 1.426 [0.000, 3.000],  loss: 7.761809, mae: 14.825413, mean_q: 7.774977, mean_eps: 0.920923
  13291/150000: episode: 144, duration: 0.432s, episode steps:  64, steps per second: 148, episode reward: -53.342, mean reward: -0.833 [-100.000, 41.234], mean action: 1.656 [0.000, 3.000],  loss: 13.977358, mae: 15.475560, mean_q: 7.010533, mean_eps: 0.920449
  13369/150000: episode: 145, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -93.573, mean reward: -1.200 [-100.000,  6.512], mean action: 1.603 [0.000, 3.000],  loss: 8.014302, mae: 16.012865, mean_q: 6.200238, mean_eps: 0.920023
  13468/150000: episode: 146, duration: 0.626s, episode steps:  99, steps per second: 158, episode reward: -132.326, mean reward: -1.337 [-100.000,  4.507], mean action: 1.778 [0.000, 3.000],  loss: 7.529725, mae: 15.220029, mean_q: 7.335620, mean_eps: 0.919492
  13570/150000: episode: 147, duration: 0.619s, episode steps: 102, steps per second: 165, episode reward: -104.650, mean reward: -1.026 [-100.000, 35.513], mean action: 1.598 [0.000, 3.000],  loss: 10.948159, mae: 15.556617, mean_q: 7.341832, mean_eps: 0.918889
  13627/150000: episode: 148, duration: 0.357s, episode steps:  57, steps per second: 159, episode reward: -177.266, mean reward: -3.110 [-100.000,  8.109], mean action: 1.842 [0.000, 3.000],  loss: 8.766634, mae: 15.657765, mean_q: 6.920639, mean_eps: 0.918412
  13687/150000: episode: 149, duration: 0.402s, episode steps:  60, steps per second: 149, episode reward: -76.509, mean reward: -1.275 [-100.000,  6.959], mean action: 1.550 [0.000, 3.000],  loss: 10.986724, mae: 16.173386, mean_q: 6.279199, mean_eps: 0.918061
  13780/150000: episode: 150, duration: 0.590s, episode steps:  93, steps per second: 158, episode reward: -242.281, mean reward: -2.605 [-100.000, 35.619], mean action: 1.667 [0.000, 3.000],  loss: 8.204503, mae: 15.617425, mean_q: 7.117529, mean_eps: 0.917602
  13856/150000: episode: 151, duration: 0.489s, episode steps:  76, steps per second: 155, episode reward: -80.422, mean reward: -1.058 [-100.000,  6.714], mean action: 1.724 [0.000, 3.000],  loss: 7.597196, mae: 15.068427, mean_q: 6.782148, mean_eps: 0.917095
  13950/150000: episode: 152, duration: 0.598s, episode steps:  94, steps per second: 157, episode reward: -121.510, mean reward: -1.293 [-100.000, 13.293], mean action: 1.606 [0.000, 3.000],  loss: 12.540627, mae: 15.643845, mean_q: 6.554240, mean_eps: 0.916585
  14031/150000: episode: 153, duration: 0.539s, episode steps:  81, steps per second: 150, episode reward: -65.144, mean reward: -0.804 [-100.000, 12.592], mean action: 1.617 [0.000, 3.000],  loss: 9.527960, mae: 16.017937, mean_q: 6.689892, mean_eps: 0.916060
  14101/150000: episode: 154, duration: 0.487s, episode steps:  70, steps per second: 144, episode reward: -68.122, mean reward: -0.973 [-100.000,  8.011], mean action: 1.600 [0.000, 3.000],  loss: 8.779377, mae: 16.713633, mean_q: 7.030440, mean_eps: 0.915607
  14169/150000: episode: 155, duration: 0.477s, episode steps:  68, steps per second: 143, episode reward: -137.230, mean reward: -2.018 [-100.000,  7.379], mean action: 1.632 [0.000, 3.000],  loss: 19.664206, mae: 16.352230, mean_q: 8.255395, mean_eps: 0.915193
  14250/150000: episode: 156, duration: 0.623s, episode steps:  81, steps per second: 130, episode reward: -218.287, mean reward: -2.695 [-100.000,  6.372], mean action: 1.457 [0.000, 3.000],  loss: 11.755174, mae: 16.643790, mean_q: 6.823065, mean_eps: 0.914746
  14397/150000: episode: 157, duration: 1.147s, episode steps: 147, steps per second: 128, episode reward: -76.657, mean reward: -0.521 [-100.000,  7.462], mean action: 1.565 [0.000, 3.000],  loss: 14.919827, mae: 16.453890, mean_q: 7.572069, mean_eps: 0.914062
  14521/150000: episode: 158, duration: 0.902s, episode steps: 124, steps per second: 138, episode reward: -288.251, mean reward: -2.325 [-100.000, 78.224], mean action: 1.702 [0.000, 3.000],  loss: 12.238087, mae: 16.524942, mean_q: 7.080277, mean_eps: 0.913249
  14602/150000: episode: 159, duration: 0.590s, episode steps:  81, steps per second: 137, episode reward: -95.046, mean reward: -1.173 [-100.000,  8.566], mean action: 1.667 [0.000, 3.000],  loss: 11.527403, mae: 16.510637, mean_q: 7.064383, mean_eps: 0.912634
  14703/150000: episode: 160, duration: 0.705s, episode steps: 101, steps per second: 143, episode reward: -104.972, mean reward: -1.039 [-100.000,  8.669], mean action: 1.624 [0.000, 3.000],  loss: 13.664420, mae: 16.580495, mean_q: 7.205198, mean_eps: 0.912088
  14826/150000: episode: 161, duration: 0.879s, episode steps: 123, steps per second: 140, episode reward: -99.343, mean reward: -0.808 [-100.000,  7.823], mean action: 1.528 [0.000, 3.000],  loss: 14.991182, mae: 16.551938, mean_q: 7.250371, mean_eps: 0.911416
  14895/150000: episode: 162, duration: 0.443s, episode steps:  69, steps per second: 156, episode reward: -85.697, mean reward: -1.242 [-100.000,  9.940], mean action: 1.507 [0.000, 3.000],  loss: 8.922827, mae: 16.574114, mean_q: 7.380854, mean_eps: 0.910840
  14991/150000: episode: 163, duration: 0.637s, episode steps:  96, steps per second: 151, episode reward: -268.709, mean reward: -2.799 [-100.000,  4.555], mean action: 1.635 [0.000, 3.000],  loss: 10.439651, mae: 16.272304, mean_q: 7.679170, mean_eps: 0.910345
  15080/150000: episode: 164, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -191.669, mean reward: -2.154 [-100.000, 22.924], mean action: 1.449 [0.000, 3.000],  loss: 11.294942, mae: 17.579012, mean_q: 7.130084, mean_eps: 0.909790
  15178/150000: episode: 165, duration: 0.605s, episode steps:  98, steps per second: 162, episode reward: -112.564, mean reward: -1.149 [-100.000, 13.232], mean action: 1.541 [0.000, 3.000],  loss: 10.623696, mae: 17.486859, mean_q: 7.138445, mean_eps: 0.909229
  15284/150000: episode: 166, duration: 0.675s, episode steps: 106, steps per second: 157, episode reward: -108.310, mean reward: -1.022 [-100.000, 13.135], mean action: 1.443 [0.000, 3.000],  loss: 11.511229, mae: 17.666696, mean_q: 7.225993, mean_eps: 0.908617
  15370/150000: episode: 167, duration: 0.578s, episode steps:  86, steps per second: 149, episode reward: -68.675, mean reward: -0.799 [-100.000, 11.270], mean action: 1.488 [0.000, 3.000],  loss: 8.964550, mae: 17.396663, mean_q: 7.651501, mean_eps: 0.908041
  15453/150000: episode: 168, duration: 0.624s, episode steps:  83, steps per second: 133, episode reward: -136.006, mean reward: -1.639 [-100.000, 50.240], mean action: 1.578 [0.000, 3.000],  loss: 6.201578, mae: 17.537744, mean_q: 7.824611, mean_eps: 0.907534
  15564/150000: episode: 169, duration: 0.766s, episode steps: 111, steps per second: 145, episode reward: -41.540, mean reward: -0.374 [-100.000, 94.862], mean action: 1.514 [0.000, 3.000],  loss: 9.719827, mae: 17.287352, mean_q: 6.938051, mean_eps: 0.906952
  15681/150000: episode: 170, duration: 0.772s, episode steps: 117, steps per second: 152, episode reward: -50.148, mean reward: -0.429 [-100.000, 18.685], mean action: 1.504 [0.000, 3.000],  loss: 11.551053, mae: 17.516600, mean_q: 7.246475, mean_eps: 0.906268
  15799/150000: episode: 171, duration: 0.839s, episode steps: 118, steps per second: 141, episode reward: -71.945, mean reward: -0.610 [-100.000, 17.695], mean action: 1.441 [0.000, 3.000],  loss: 6.682312, mae: 17.146108, mean_q: 8.246255, mean_eps: 0.905563
  15866/150000: episode: 172, duration: 0.537s, episode steps:  67, steps per second: 125, episode reward: -99.095, mean reward: -1.479 [-100.000,  4.408], mean action: 1.493 [0.000, 3.000],  loss: 15.145099, mae: 17.376396, mean_q: 7.434748, mean_eps: 0.905008
  15950/150000: episode: 173, duration: 0.625s, episode steps:  84, steps per second: 134, episode reward: -144.478, mean reward: -1.720 [-100.000,  7.702], mean action: 1.464 [0.000, 3.000],  loss: 7.481525, mae: 17.283752, mean_q: 8.627468, mean_eps: 0.904555
  16061/150000: episode: 174, duration: 0.827s, episode steps: 111, steps per second: 134, episode reward: -152.617, mean reward: -1.375 [-100.000,  3.110], mean action: 1.459 [0.000, 3.000],  loss: 10.366693, mae: 17.798408, mean_q: 7.580970, mean_eps: 0.903970
  16138/150000: episode: 175, duration: 0.642s, episode steps:  77, steps per second: 120, episode reward: -154.392, mean reward: -2.005 [-100.000, 17.387], mean action: 1.429 [0.000, 3.000],  loss: 11.741501, mae: 18.466651, mean_q: 7.124630, mean_eps: 0.903406
  16228/150000: episode: 176, duration: 0.699s, episode steps:  90, steps per second: 129, episode reward: -55.492, mean reward: -0.617 [-100.000,  6.514], mean action: 1.589 [0.000, 3.000],  loss: 16.462879, mae: 18.172802, mean_q: 8.864143, mean_eps: 0.902905
  16301/150000: episode: 177, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -250.353, mean reward: -3.429 [-100.000, 31.555], mean action: 1.466 [0.000, 3.000],  loss: 11.258347, mae: 18.215853, mean_q: 7.504936, mean_eps: 0.902416
  16431/150000: episode: 178, duration: 0.908s, episode steps: 130, steps per second: 143, episode reward: -105.325, mean reward: -0.810 [-100.000, 30.276], mean action: 1.592 [0.000, 3.000],  loss: 9.615874, mae: 18.166809, mean_q: 8.020037, mean_eps: 0.901807
  16497/150000: episode: 179, duration: 0.497s, episode steps:  66, steps per second: 133, episode reward: -83.296, mean reward: -1.262 [-100.000, 21.265], mean action: 1.576 [0.000, 3.000],  loss: 9.702877, mae: 18.594639, mean_q: 7.146628, mean_eps: 0.901219
  16589/150000: episode: 180, duration: 0.605s, episode steps:  92, steps per second: 152, episode reward: -134.120, mean reward: -1.458 [-100.000,  7.496], mean action: 1.457 [0.000, 3.000],  loss: 10.662786, mae: 18.606013, mean_q: 7.165043, mean_eps: 0.900745
  16718/150000: episode: 181, duration: 1.125s, episode steps: 129, steps per second: 115, episode reward: -123.731, mean reward: -0.959 [-100.000, 26.780], mean action: 1.558 [0.000, 3.000],  loss: 10.841178, mae: 17.994066, mean_q: 7.738415, mean_eps: 0.900082
  16802/150000: episode: 182, duration: 0.595s, episode steps:  84, steps per second: 141, episode reward: -351.946, mean reward: -4.190 [-100.000,  0.404], mean action: 1.714 [0.000, 3.000],  loss: 10.853743, mae: 18.706087, mean_q: 7.423218, mean_eps: 0.899443
  16921/150000: episode: 183, duration: 0.752s, episode steps: 119, steps per second: 158, episode reward: -80.498, mean reward: -0.676 [-100.000, 17.892], mean action: 1.580 [0.000, 3.000],  loss: 8.817826, mae: 17.967051, mean_q: 8.158943, mean_eps: 0.898834
  16989/150000: episode: 184, duration: 0.413s, episode steps:  68, steps per second: 165, episode reward: -135.250, mean reward: -1.989 [-100.000, 31.043], mean action: 1.603 [0.000, 3.000],  loss: 10.417955, mae: 18.694812, mean_q: 7.562504, mean_eps: 0.898273
  17107/150000: episode: 185, duration: 0.773s, episode steps: 118, steps per second: 153, episode reward: -84.034, mean reward: -0.712 [-100.000, 46.734], mean action: 1.602 [0.000, 3.000],  loss: 8.828962, mae: 18.839963, mean_q: 8.113135, mean_eps: 0.897715
  17192/150000: episode: 186, duration: 0.542s, episode steps:  85, steps per second: 157, episode reward: -103.210, mean reward: -1.214 [-100.000,  7.573], mean action: 1.600 [0.000, 3.000],  loss: 12.758458, mae: 18.691289, mean_q: 8.120745, mean_eps: 0.897106
  17313/150000: episode: 187, duration: 0.750s, episode steps: 121, steps per second: 161, episode reward: -161.854, mean reward: -1.338 [-100.000, 31.681], mean action: 1.636 [0.000, 3.000],  loss: 9.283376, mae: 19.235897, mean_q: 7.565408, mean_eps: 0.896488
  17400/150000: episode: 188, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -184.117, mean reward: -2.116 [-100.000, 39.019], mean action: 1.425 [0.000, 3.000],  loss: 10.778050, mae: 18.791201, mean_q: 6.931871, mean_eps: 0.895864
  17514/150000: episode: 189, duration: 0.747s, episode steps: 114, steps per second: 153, episode reward: -148.272, mean reward: -1.301 [-100.000, 33.866], mean action: 1.807 [0.000, 3.000],  loss: 8.144416, mae: 19.168386, mean_q: 7.485106, mean_eps: 0.895261
  17605/150000: episode: 190, duration: 0.570s, episode steps:  91, steps per second: 160, episode reward: -99.852, mean reward: -1.097 [-100.000, 18.576], mean action: 1.703 [0.000, 3.000],  loss: 10.513468, mae: 18.930559, mean_q: 7.641686, mean_eps: 0.894646
  17722/150000: episode: 191, duration: 0.725s, episode steps: 117, steps per second: 161, episode reward: -97.885, mean reward: -0.837 [-100.000, 16.629], mean action: 1.487 [0.000, 3.000],  loss: 11.702518, mae: 18.693685, mean_q: 7.444970, mean_eps: 0.894022
  17846/150000: episode: 192, duration: 0.797s, episode steps: 124, steps per second: 156, episode reward: -70.552, mean reward: -0.569 [-100.000, 12.669], mean action: 1.605 [0.000, 3.000],  loss: 10.096229, mae: 18.584124, mean_q: 7.743111, mean_eps: 0.893299
  17939/150000: episode: 193, duration: 0.579s, episode steps:  93, steps per second: 161, episode reward: -107.261, mean reward: -1.153 [-100.000,  6.679], mean action: 1.333 [0.000, 3.000],  loss: 7.031063, mae: 19.109713, mean_q: 7.372798, mean_eps: 0.892648
  18025/150000: episode: 194, duration: 0.536s, episode steps:  86, steps per second: 160, episode reward: -107.255, mean reward: -1.247 [-100.000, 27.514], mean action: 1.488 [0.000, 3.000],  loss: 6.349089, mae: 18.902823, mean_q: 7.534113, mean_eps: 0.892111
  18141/150000: episode: 195, duration: 0.780s, episode steps: 116, steps per second: 149, episode reward: -71.174, mean reward: -0.614 [-100.000, 12.258], mean action: 1.500 [0.000, 3.000],  loss: 11.313719, mae: 20.082786, mean_q: 7.672297, mean_eps: 0.891505
  18229/150000: episode: 196, duration: 0.557s, episode steps:  88, steps per second: 158, episode reward: -53.625, mean reward: -0.609 [-100.000, 15.754], mean action: 1.500 [0.000, 3.000],  loss: 7.659841, mae: 19.732425, mean_q: 7.785956, mean_eps: 0.890893
  18298/150000: episode: 197, duration: 0.473s, episode steps:  69, steps per second: 146, episode reward: -120.373, mean reward: -1.745 [-100.000,  8.433], mean action: 1.681 [0.000, 3.000],  loss: 10.017442, mae: 19.735716, mean_q: 7.419286, mean_eps: 0.890422
  18382/150000: episode: 198, duration: 0.521s, episode steps:  84, steps per second: 161, episode reward: -192.632, mean reward: -2.293 [-100.000,  5.345], mean action: 1.476 [0.000, 3.000],  loss: 7.976857, mae: 19.659109, mean_q: 8.290318, mean_eps: 0.889963
  18472/150000: episode: 199, duration: 0.596s, episode steps:  90, steps per second: 151, episode reward: -99.473, mean reward: -1.105 [-100.000,  7.078], mean action: 1.578 [0.000, 3.000],  loss: 7.030227, mae: 19.532912, mean_q: 8.666698, mean_eps: 0.889441
  18601/150000: episode: 200, duration: 0.827s, episode steps: 129, steps per second: 156, episode reward: -74.931, mean reward: -0.581 [-100.000, 13.464], mean action: 1.496 [0.000, 3.000],  loss: 7.472643, mae: 19.476080, mean_q: 7.996724, mean_eps: 0.888784
  18680/150000: episode: 201, duration: 0.498s, episode steps:  79, steps per second: 159, episode reward: -101.253, mean reward: -1.282 [-100.000,  7.603], mean action: 1.646 [0.000, 3.000],  loss: 6.094900, mae: 19.781719, mean_q: 8.405573, mean_eps: 0.888160
  18813/150000: episode: 202, duration: 0.863s, episode steps: 133, steps per second: 154, episode reward: -38.435, mean reward: -0.289 [-100.000, 96.634], mean action: 1.504 [0.000, 3.000],  loss: 10.192111, mae: 19.511451, mean_q: 7.982678, mean_eps: 0.887524
  18896/150000: episode: 203, duration: 0.525s, episode steps:  83, steps per second: 158, episode reward: -81.142, mean reward: -0.978 [-100.000, 14.280], mean action: 1.506 [0.000, 3.000],  loss: 9.404693, mae: 19.997385, mean_q: 7.716247, mean_eps: 0.886876
  18996/150000: episode: 204, duration: 0.626s, episode steps: 100, steps per second: 160, episode reward: -76.538, mean reward: -0.765 [-100.000,  9.433], mean action: 1.500 [0.000, 3.000],  loss: 8.271892, mae: 19.201341, mean_q: 8.835458, mean_eps: 0.886327
  19052/150000: episode: 205, duration: 0.346s, episode steps:  56, steps per second: 162, episode reward: -85.928, mean reward: -1.534 [-100.000,  5.591], mean action: 1.464 [0.000, 3.000],  loss: 5.331395, mae: 21.044201, mean_q: 8.824346, mean_eps: 0.885859
  19152/150000: episode: 206, duration: 0.654s, episode steps: 100, steps per second: 153, episode reward: -76.886, mean reward: -0.769 [-100.000,  6.656], mean action: 1.620 [0.000, 3.000],  loss: 8.822658, mae: 20.778007, mean_q: 7.927048, mean_eps: 0.885391
  19266/150000: episode: 207, duration: 0.741s, episode steps: 114, steps per second: 154, episode reward: -139.308, mean reward: -1.222 [-100.000, 14.415], mean action: 1.561 [0.000, 3.000],  loss: 14.558546, mae: 21.212594, mean_q: 7.813957, mean_eps: 0.884749
  19330/150000: episode: 208, duration: 0.406s, episode steps:  64, steps per second: 158, episode reward: -91.806, mean reward: -1.434 [-100.000, 11.553], mean action: 1.578 [0.000, 3.000],  loss: 8.398684, mae: 21.080510, mean_q: 8.859583, mean_eps: 0.884215
  19447/150000: episode: 209, duration: 0.752s, episode steps: 117, steps per second: 156, episode reward: -3.446, mean reward: -0.029 [-100.000, 133.110], mean action: 1.513 [0.000, 3.000],  loss: 8.542378, mae: 21.183170, mean_q: 7.397651, mean_eps: 0.883672
  19525/150000: episode: 210, duration: 0.503s, episode steps:  78, steps per second: 155, episode reward: -140.787, mean reward: -1.805 [-100.000,  7.841], mean action: 1.205 [0.000, 3.000],  loss: 6.343709, mae: 20.773852, mean_q: 8.892349, mean_eps: 0.883087
  19593/150000: episode: 211, duration: 0.433s, episode steps:  68, steps per second: 157, episode reward: -91.065, mean reward: -1.339 [-100.000, 62.717], mean action: 1.632 [0.000, 3.000],  loss: 6.914142, mae: 21.027030, mean_q: 7.514275, mean_eps: 0.882649
  19665/150000: episode: 212, duration: 0.456s, episode steps:  72, steps per second: 158, episode reward: -84.216, mean reward: -1.170 [-100.000, 20.807], mean action: 1.389 [0.000, 3.000],  loss: 17.706742, mae: 20.622881, mean_q: 7.769143, mean_eps: 0.882229
  19780/150000: episode: 213, duration: 0.747s, episode steps: 115, steps per second: 154, episode reward: -84.219, mean reward: -0.732 [-100.000,  8.118], mean action: 1.530 [0.000, 3.000],  loss: 9.529319, mae: 21.064092, mean_q: 7.907050, mean_eps: 0.881668
  19884/150000: episode: 214, duration: 0.664s, episode steps: 104, steps per second: 157, episode reward: -86.296, mean reward: -0.830 [-100.000,  7.897], mean action: 1.692 [0.000, 3.000],  loss: 5.465820, mae: 20.805060, mean_q: 7.863950, mean_eps: 0.881011
  19970/150000: episode: 215, duration: 0.539s, episode steps:  86, steps per second: 159, episode reward: -156.270, mean reward: -1.817 [-100.000, 10.443], mean action: 1.616 [0.000, 3.000],  loss: 6.640847, mae: 21.372467, mean_q: 7.491424, mean_eps: 0.880441
  20045/150000: episode: 216, duration: 0.459s, episode steps:  75, steps per second: 163, episode reward: -98.052, mean reward: -1.307 [-100.000, 10.152], mean action: 1.480 [0.000, 3.000],  loss: 6.724036, mae: 21.834071, mean_q: 8.636888, mean_eps: 0.879958
  20119/150000: episode: 217, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -135.520, mean reward: -1.831 [-100.000, 16.912], mean action: 1.473 [0.000, 3.000],  loss: 11.329150, mae: 22.275876, mean_q: 8.326961, mean_eps: 0.879511
  20240/150000: episode: 218, duration: 0.766s, episode steps: 121, steps per second: 158, episode reward: -84.876, mean reward: -0.701 [-100.000,  9.739], mean action: 1.678 [0.000, 3.000],  loss: 9.526022, mae: 21.496211, mean_q: 9.528322, mean_eps: 0.878926
  20299/150000: episode: 219, duration: 0.371s, episode steps:  59, steps per second: 159, episode reward: -66.820, mean reward: -1.133 [-100.000, 15.848], mean action: 1.525 [0.000, 3.000],  loss: 6.313708, mae: 21.787911, mean_q: 8.991735, mean_eps: 0.878386
  20371/150000: episode: 220, duration: 0.441s, episode steps:  72, steps per second: 163, episode reward: -85.039, mean reward: -1.181 [-100.000,  5.733], mean action: 1.528 [0.000, 3.000],  loss: 9.126885, mae: 22.132461, mean_q: 9.274862, mean_eps: 0.877993
  20488/150000: episode: 221, duration: 0.768s, episode steps: 117, steps per second: 152, episode reward: -204.073, mean reward: -1.744 [-100.000, 24.541], mean action: 1.470 [0.000, 3.000],  loss: 7.161215, mae: 21.872134, mean_q: 9.275556, mean_eps: 0.877426
  20557/150000: episode: 222, duration: 0.434s, episode steps:  69, steps per second: 159, episode reward: -51.222, mean reward: -0.742 [-100.000, 12.618], mean action: 1.580 [0.000, 3.000],  loss: 6.946953, mae: 21.859357, mean_q: 8.822259, mean_eps: 0.876868
  20629/150000: episode: 223, duration: 0.448s, episode steps:  72, steps per second: 161, episode reward: -81.732, mean reward: -1.135 [-100.000,  5.818], mean action: 1.639 [0.000, 3.000],  loss: 9.312547, mae: 22.507777, mean_q: 8.882951, mean_eps: 0.876445
  20708/150000: episode: 224, duration: 0.495s, episode steps:  79, steps per second: 160, episode reward: -113.749, mean reward: -1.440 [-100.000, 17.822], mean action: 1.380 [0.000, 3.000],  loss: 21.726519, mae: 22.189366, mean_q: 8.199723, mean_eps: 0.875992
  20780/150000: episode: 225, duration: 0.480s, episode steps:  72, steps per second: 150, episode reward: -77.318, mean reward: -1.074 [-100.000,  5.286], mean action: 1.500 [0.000, 3.000],  loss: 8.069377, mae: 21.965464, mean_q: 8.388654, mean_eps: 0.875539
  20877/150000: episode: 226, duration: 0.624s, episode steps:  97, steps per second: 155, episode reward: -123.656, mean reward: -1.275 [-100.000,  5.304], mean action: 1.794 [0.000, 3.000],  loss: 6.823948, mae: 22.218126, mean_q: 9.224854, mean_eps: 0.875032
  20938/150000: episode: 227, duration: 0.384s, episode steps:  61, steps per second: 159, episode reward: -42.857, mean reward: -0.703 [-100.000, 16.644], mean action: 1.525 [0.000, 3.000],  loss: 12.613511, mae: 22.113790, mean_q: 9.168183, mean_eps: 0.874558
  21003/150000: episode: 228, duration: 0.402s, episode steps:  65, steps per second: 162, episode reward: -35.461, mean reward: -0.546 [-100.000, 23.445], mean action: 1.538 [0.000, 3.000],  loss: 7.793853, mae: 22.083455, mean_q: 9.742774, mean_eps: 0.874180
  21084/150000: episode: 229, duration: 0.501s, episode steps:  81, steps per second: 162, episode reward: -101.666, mean reward: -1.255 [-100.000,  8.232], mean action: 1.642 [0.000, 3.000],  loss: 9.019715, mae: 22.879476, mean_q: 9.805700, mean_eps: 0.873742
  21162/150000: episode: 230, duration: 0.528s, episode steps:  78, steps per second: 148, episode reward: -126.833, mean reward: -1.626 [-100.000, 38.193], mean action: 1.385 [0.000, 3.000],  loss: 10.056599, mae: 23.461335, mean_q: 8.579105, mean_eps: 0.873265
  21226/150000: episode: 231, duration: 0.410s, episode steps:  64, steps per second: 156, episode reward: -84.517, mean reward: -1.321 [-100.000,  6.729], mean action: 1.328 [0.000, 3.000],  loss: 8.668160, mae: 22.679077, mean_q: 9.581096, mean_eps: 0.872839
  21314/150000: episode: 232, duration: 0.550s, episode steps:  88, steps per second: 160, episode reward: -250.656, mean reward: -2.848 [-100.000,  6.211], mean action: 1.455 [0.000, 3.000],  loss: 13.244479, mae: 22.968193, mean_q: 9.522601, mean_eps: 0.872383
  21395/150000: episode: 233, duration: 0.502s, episode steps:  81, steps per second: 162, episode reward: -150.500, mean reward: -1.858 [-100.000,  6.336], mean action: 1.457 [0.000, 3.000],  loss: 6.644822, mae: 22.990564, mean_q: 9.701810, mean_eps: 0.871876
  21473/150000: episode: 234, duration: 0.526s, episode steps:  78, steps per second: 148, episode reward: -115.066, mean reward: -1.475 [-100.000,  8.204], mean action: 1.551 [0.000, 3.000],  loss: 11.171973, mae: 23.391291, mean_q: 8.951751, mean_eps: 0.871399
  21588/150000: episode: 235, duration: 0.731s, episode steps: 115, steps per second: 157, episode reward: -137.114, mean reward: -1.192 [-100.000, 22.169], mean action: 1.383 [0.000, 3.000],  loss: 8.201370, mae: 22.630695, mean_q: 9.356466, mean_eps: 0.870820
  21679/150000: episode: 236, duration: 0.557s, episode steps:  91, steps per second: 163, episode reward: -85.421, mean reward: -0.939 [-100.000,  6.923], mean action: 1.725 [0.000, 3.000],  loss: 6.094671, mae: 22.775588, mean_q: 9.072569, mean_eps: 0.870202
  21774/150000: episode: 237, duration: 0.590s, episode steps:  95, steps per second: 161, episode reward: -114.181, mean reward: -1.202 [-100.000,  9.493], mean action: 1.526 [0.000, 3.000],  loss: 6.876441, mae: 22.641122, mean_q: 9.201949, mean_eps: 0.869644
  21847/150000: episode: 238, duration: 0.488s, episode steps:  73, steps per second: 149, episode reward: -86.487, mean reward: -1.185 [-100.000,  9.054], mean action: 1.521 [0.000, 3.000],  loss: 5.175130, mae: 22.713048, mean_q: 9.308610, mean_eps: 0.869140
  21938/150000: episode: 239, duration: 0.576s, episode steps:  91, steps per second: 158, episode reward: -131.308, mean reward: -1.443 [-100.000, 33.493], mean action: 1.429 [0.000, 3.000],  loss: 7.196986, mae: 22.939020, mean_q: 9.446867, mean_eps: 0.868648
  22040/150000: episode: 240, duration: 0.646s, episode steps: 102, steps per second: 158, episode reward: -173.055, mean reward: -1.697 [-100.000,  8.145], mean action: 1.471 [0.000, 3.000],  loss: 9.927540, mae: 23.006929, mean_q: 9.283925, mean_eps: 0.868069
  22118/150000: episode: 241, duration: 0.484s, episode steps:  78, steps per second: 161, episode reward: -65.361, mean reward: -0.838 [-100.000, 19.704], mean action: 1.526 [0.000, 3.000],  loss: 12.431535, mae: 23.311359, mean_q: 9.966584, mean_eps: 0.867529
  22222/150000: episode: 242, duration: 0.676s, episode steps: 104, steps per second: 154, episode reward: -86.454, mean reward: -0.831 [-100.000, 11.015], mean action: 1.558 [0.000, 3.000],  loss: 12.774572, mae: 23.237092, mean_q: 9.575196, mean_eps: 0.866983
  22334/150000: episode: 243, duration: 0.692s, episode steps: 112, steps per second: 162, episode reward: -200.628, mean reward: -1.791 [-100.000,  1.684], mean action: 1.598 [0.000, 3.000],  loss: 7.801349, mae: 23.097096, mean_q: 9.828577, mean_eps: 0.866335
  22405/150000: episode: 244, duration: 0.462s, episode steps:  71, steps per second: 154, episode reward: -110.261, mean reward: -1.553 [-100.000,  8.708], mean action: 1.676 [0.000, 3.000],  loss: 11.076716, mae: 22.779967, mean_q: 10.305810, mean_eps: 0.865786
  22506/150000: episode: 245, duration: 0.663s, episode steps: 101, steps per second: 152, episode reward: -131.983, mean reward: -1.307 [-100.000, 11.120], mean action: 1.416 [0.000, 3.000],  loss: 7.823640, mae: 23.139576, mean_q: 10.090708, mean_eps: 0.865270
  22589/150000: episode: 246, duration: 0.523s, episode steps:  83, steps per second: 159, episode reward: -86.982, mean reward: -1.048 [-100.000, 12.116], mean action: 1.410 [0.000, 3.000],  loss: 7.174603, mae: 23.406656, mean_q: 9.949354, mean_eps: 0.864718
  22716/150000: episode: 247, duration: 0.795s, episode steps: 127, steps per second: 160, episode reward: -159.510, mean reward: -1.256 [-100.000, 12.834], mean action: 1.677 [0.000, 3.000],  loss: 7.114228, mae: 23.367514, mean_q: 9.942548, mean_eps: 0.864088
  22840/150000: episode: 248, duration: 0.796s, episode steps: 124, steps per second: 156, episode reward: -174.652, mean reward: -1.408 [-100.000,  4.550], mean action: 1.613 [0.000, 3.000],  loss: 9.981604, mae: 23.564270, mean_q: 9.721359, mean_eps: 0.863335
  22905/150000: episode: 249, duration: 0.416s, episode steps:  65, steps per second: 156, episode reward: -104.824, mean reward: -1.613 [-100.000,  7.513], mean action: 1.615 [0.000, 3.000],  loss: 12.505203, mae: 23.173022, mean_q: 9.817061, mean_eps: 0.862768
  22979/150000: episode: 250, duration: 0.467s, episode steps:  74, steps per second: 159, episode reward: -5.385, mean reward: -0.073 [-100.000, 17.185], mean action: 1.595 [0.000, 3.000],  loss: 7.117789, mae: 23.066391, mean_q: 9.187196, mean_eps: 0.862351
  23054/150000: episode: 251, duration: 0.465s, episode steps:  75, steps per second: 161, episode reward: -88.113, mean reward: -1.175 [-100.000, 15.245], mean action: 1.360 [0.000, 3.000],  loss: 7.134691, mae: 23.771190, mean_q: 10.271324, mean_eps: 0.861904
  23153/150000: episode: 252, duration: 0.632s, episode steps:  99, steps per second: 157, episode reward: -97.470, mean reward: -0.985 [-100.000,  8.712], mean action: 1.616 [0.000, 3.000],  loss: 6.455176, mae: 23.921343, mean_q: 10.455506, mean_eps: 0.861382
  23237/150000: episode: 253, duration: 0.544s, episode steps:  84, steps per second: 154, episode reward: -85.137, mean reward: -1.014 [-100.000, 13.593], mean action: 1.595 [0.000, 3.000],  loss: 6.217886, mae: 23.856830, mean_q: 10.293542, mean_eps: 0.860833
  23360/150000: episode: 254, duration: 0.778s, episode steps: 123, steps per second: 158, episode reward: -55.932, mean reward: -0.455 [-100.000, 12.845], mean action: 1.626 [0.000, 3.000],  loss: 7.848247, mae: 23.647751, mean_q: 10.398873, mean_eps: 0.860212
  23440/150000: episode: 255, duration: 0.497s, episode steps:  80, steps per second: 161, episode reward: -65.548, mean reward: -0.819 [-100.000,  9.526], mean action: 1.525 [0.000, 3.000],  loss: 8.172085, mae: 23.620971, mean_q: 11.152214, mean_eps: 0.859603
  23558/150000: episode: 256, duration: 0.769s, episode steps: 118, steps per second: 153, episode reward: -74.425, mean reward: -0.631 [-100.000, 13.399], mean action: 1.449 [0.000, 3.000],  loss: 15.435926, mae: 24.021821, mean_q: 10.441952, mean_eps: 0.859009
  23684/150000: episode: 257, duration: 0.801s, episode steps: 126, steps per second: 157, episode reward: -115.290, mean reward: -0.915 [-100.000,  7.512], mean action: 1.706 [0.000, 3.000],  loss: 8.741194, mae: 24.040603, mean_q: 10.355318, mean_eps: 0.858277
  23750/150000: episode: 258, duration: 0.412s, episode steps:  66, steps per second: 160, episode reward: -87.172, mean reward: -1.321 [-100.000, 10.715], mean action: 1.667 [0.000, 3.000],  loss: 8.767964, mae: 23.961454, mean_q: 9.965940, mean_eps: 0.857701
  23823/150000: episode: 259, duration: 0.470s, episode steps:  73, steps per second: 155, episode reward: 45.813, mean reward:  0.628 [-100.000, 131.430], mean action: 1.589 [0.000, 3.000],  loss: 7.190174, mae: 23.789901, mean_q: 10.638375, mean_eps: 0.857284
  23958/150000: episode: 260, duration: 0.876s, episode steps: 135, steps per second: 154, episode reward: -81.745, mean reward: -0.606 [-100.000,  9.963], mean action: 1.496 [0.000, 3.000],  loss: 11.059071, mae: 23.926115, mean_q: 10.773417, mean_eps: 0.856660
  24021/150000: episode: 261, duration: 0.393s, episode steps:  63, steps per second: 160, episode reward: -91.382, mean reward: -1.451 [-100.000,  6.259], mean action: 1.857 [0.000, 3.000],  loss: 8.974356, mae: 24.207237, mean_q: 11.113940, mean_eps: 0.856066
  24094/150000: episode: 262, duration: 0.454s, episode steps:  73, steps per second: 161, episode reward: -52.730, mean reward: -0.722 [-100.000, 11.849], mean action: 1.562 [0.000, 3.000],  loss: 7.059006, mae: 24.318617, mean_q: 11.092889, mean_eps: 0.855658
  24172/150000: episode: 263, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -101.721, mean reward: -1.304 [-100.000,  7.751], mean action: 1.385 [0.000, 3.000],  loss: 10.426828, mae: 24.756396, mean_q: 10.620101, mean_eps: 0.855205
  24262/150000: episode: 264, duration: 0.703s, episode steps:  90, steps per second: 128, episode reward: -100.765, mean reward: -1.120 [-100.000,  9.955], mean action: 1.800 [0.000, 3.000],  loss: 7.187333, mae: 24.204444, mean_q: 10.781003, mean_eps: 0.854701
  24347/150000: episode: 265, duration: 0.602s, episode steps:  85, steps per second: 141, episode reward: -127.792, mean reward: -1.503 [-100.000, 16.808], mean action: 1.659 [0.000, 3.000],  loss: 8.506203, mae: 24.357103, mean_q: 11.850988, mean_eps: 0.854176
  24436/150000: episode: 266, duration: 0.640s, episode steps:  89, steps per second: 139, episode reward: -92.193, mean reward: -1.036 [-100.000,  8.281], mean action: 1.449 [0.000, 3.000],  loss: 5.393649, mae: 24.325101, mean_q: 11.855529, mean_eps: 0.853654
  24556/150000: episode: 267, duration: 0.887s, episode steps: 120, steps per second: 135, episode reward: -100.864, mean reward: -0.841 [-100.000,  6.861], mean action: 1.533 [0.000, 3.000],  loss: 11.514071, mae: 24.167256, mean_q: 11.276886, mean_eps: 0.853027
  24644/150000: episode: 268, duration: 0.609s, episode steps:  88, steps per second: 145, episode reward: -132.741, mean reward: -1.508 [-100.000, 14.478], mean action: 1.670 [0.000, 3.000],  loss: 13.146259, mae: 24.588589, mean_q: 11.487566, mean_eps: 0.852403
  24762/150000: episode: 269, duration: 0.806s, episode steps: 118, steps per second: 146, episode reward: -78.711, mean reward: -0.667 [-100.000, 11.432], mean action: 1.517 [0.000, 3.000],  loss: 8.416912, mae: 24.822749, mean_q: 11.270550, mean_eps: 0.851785
  24872/150000: episode: 270, duration: 0.753s, episode steps: 110, steps per second: 146, episode reward: -68.135, mean reward: -0.619 [-100.000, 18.004], mean action: 1.609 [0.000, 3.000],  loss: 7.156376, mae: 24.666934, mean_q: 11.038063, mean_eps: 0.851101
  25002/150000: episode: 271, duration: 0.816s, episode steps: 130, steps per second: 159, episode reward: -44.354, mean reward: -0.341 [-100.000, 23.340], mean action: 1.562 [0.000, 3.000],  loss: 10.362331, mae: 24.232456, mean_q: 11.628600, mean_eps: 0.850381
  25088/150000: episode: 272, duration: 0.543s, episode steps:  86, steps per second: 159, episode reward: -190.244, mean reward: -2.212 [-100.000,  4.866], mean action: 1.616 [0.000, 3.000],  loss: 7.263087, mae: 24.836510, mean_q: 12.061356, mean_eps: 0.849733
  25183/150000: episode: 273, duration: 0.623s, episode steps:  95, steps per second: 152, episode reward: -122.863, mean reward: -1.293 [-100.000, 16.286], mean action: 1.411 [0.000, 3.000],  loss: 13.784942, mae: 24.907736, mean_q: 11.916348, mean_eps: 0.849190
  25302/150000: episode: 274, duration: 0.761s, episode steps: 119, steps per second: 156, episode reward: -265.517, mean reward: -2.231 [-100.000,  0.926], mean action: 1.513 [0.000, 3.000],  loss: 8.172749, mae: 24.988684, mean_q: 11.428060, mean_eps: 0.848548
  25387/150000: episode: 275, duration: 0.641s, episode steps:  85, steps per second: 133, episode reward: -48.313, mean reward: -0.568 [-100.000, 14.611], mean action: 1.612 [0.000, 3.000],  loss: 11.618548, mae: 25.610704, mean_q: 10.741539, mean_eps: 0.847936
  25480/150000: episode: 276, duration: 0.799s, episode steps:  93, steps per second: 116, episode reward: -50.927, mean reward: -0.548 [-100.000, 11.352], mean action: 1.699 [0.000, 3.000],  loss: 10.324776, mae: 25.310371, mean_q: 11.351872, mean_eps: 0.847402
  25576/150000: episode: 277, duration: 0.716s, episode steps:  96, steps per second: 134, episode reward: -66.427, mean reward: -0.692 [-100.000,  9.890], mean action: 1.562 [0.000, 3.000],  loss: 9.533588, mae: 25.165713, mean_q: 11.073875, mean_eps: 0.846835
  25650/150000: episode: 278, duration: 0.477s, episode steps:  74, steps per second: 155, episode reward: -93.802, mean reward: -1.268 [-100.000, 20.982], mean action: 1.689 [0.000, 3.000],  loss: 7.968168, mae: 25.104205, mean_q: 12.048421, mean_eps: 0.846325
  25729/150000: episode: 279, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -152.966, mean reward: -1.936 [-100.000, 11.429], mean action: 1.418 [0.000, 3.000],  loss: 11.659296, mae: 25.805282, mean_q: 10.809963, mean_eps: 0.845866
  25800/150000: episode: 280, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -98.570, mean reward: -1.388 [-100.000, 10.587], mean action: 1.648 [0.000, 3.000],  loss: 6.565073, mae: 25.053648, mean_q: 11.609679, mean_eps: 0.845416
  25903/150000: episode: 281, duration: 0.647s, episode steps: 103, steps per second: 159, episode reward: -232.783, mean reward: -2.260 [-100.000, 46.167], mean action: 1.495 [0.000, 3.000],  loss: 9.581506, mae: 24.864179, mean_q: 10.995523, mean_eps: 0.844894
  26010/150000: episode: 282, duration: 0.690s, episode steps: 107, steps per second: 155, episode reward: -321.639, mean reward: -3.006 [-100.000,  4.459], mean action: 1.495 [0.000, 3.000],  loss: 8.762706, mae: 25.054324, mean_q: 12.036370, mean_eps: 0.844264
  26066/150000: episode: 283, duration: 0.387s, episode steps:  56, steps per second: 145, episode reward: -63.125, mean reward: -1.127 [-100.000, 20.825], mean action: 1.804 [0.000, 3.000],  loss: 8.133678, mae: 26.155935, mean_q: 12.106827, mean_eps: 0.843775
  26183/150000: episode: 284, duration: 0.749s, episode steps: 117, steps per second: 156, episode reward: -26.752, mean reward: -0.229 [-100.000, 15.245], mean action: 1.564 [0.000, 3.000],  loss: 12.806310, mae: 25.663606, mean_q: 12.248301, mean_eps: 0.843256
  26292/150000: episode: 285, duration: 0.688s, episode steps: 109, steps per second: 158, episode reward: -75.669, mean reward: -0.694 [-100.000, 16.452], mean action: 1.385 [0.000, 3.000],  loss: 8.862425, mae: 26.144959, mean_q: 10.961057, mean_eps: 0.842578
  26356/150000: episode: 286, duration: 0.397s, episode steps:  64, steps per second: 161, episode reward: -63.198, mean reward: -0.987 [-100.000, 10.858], mean action: 1.609 [0.000, 3.000],  loss: 11.417369, mae: 25.550125, mean_q: 12.783933, mean_eps: 0.842059
  26450/150000: episode: 287, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -79.812, mean reward: -0.849 [-100.000, 14.330], mean action: 1.404 [0.000, 3.000],  loss: 9.118011, mae: 25.900843, mean_q: 12.650569, mean_eps: 0.841585
  26519/150000: episode: 288, duration: 0.440s, episode steps:  69, steps per second: 157, episode reward: -165.587, mean reward: -2.400 [-100.000,  4.474], mean action: 1.536 [0.000, 3.000],  loss: 8.280825, mae: 26.029982, mean_q: 12.487156, mean_eps: 0.841096
  26651/150000: episode: 289, duration: 0.861s, episode steps: 132, steps per second: 153, episode reward: -160.294, mean reward: -1.214 [-100.000,  4.971], mean action: 1.500 [0.000, 3.000],  loss: 7.638061, mae: 25.755821, mean_q: 12.278675, mean_eps: 0.840493
  26722/150000: episode: 290, duration: 0.475s, episode steps:  71, steps per second: 150, episode reward: -79.581, mean reward: -1.121 [-100.000,  9.054], mean action: 1.592 [0.000, 3.000],  loss: 7.008504, mae: 25.848175, mean_q: 12.372808, mean_eps: 0.839884
  26789/150000: episode: 291, duration: 0.437s, episode steps:  67, steps per second: 153, episode reward: -76.360, mean reward: -1.140 [-100.000,  7.244], mean action: 1.716 [0.000, 3.000],  loss: 7.169850, mae: 26.287747, mean_q: 11.961996, mean_eps: 0.839470
  26867/150000: episode: 292, duration: 0.495s, episode steps:  78, steps per second: 158, episode reward: -107.178, mean reward: -1.374 [-100.000, 12.242], mean action: 1.769 [0.000, 3.000],  loss: 5.651642, mae: 25.713452, mean_q: 12.921466, mean_eps: 0.839035
  26960/150000: episode: 293, duration: 0.609s, episode steps:  93, steps per second: 153, episode reward: -171.800, mean reward: -1.847 [-100.000, 16.518], mean action: 1.355 [0.000, 3.000],  loss: 9.364415, mae: 25.853615, mean_q: 11.677040, mean_eps: 0.838522
  27046/150000: episode: 294, duration: 0.573s, episode steps:  86, steps per second: 150, episode reward: -55.123, mean reward: -0.641 [-100.000, 17.248], mean action: 1.512 [0.000, 3.000],  loss: 10.840374, mae: 26.658566, mean_q: 11.924837, mean_eps: 0.837985
  27152/150000: episode: 295, duration: 0.694s, episode steps: 106, steps per second: 153, episode reward: -90.325, mean reward: -0.852 [-100.000,  9.129], mean action: 1.736 [0.000, 3.000],  loss: 7.347052, mae: 26.800338, mean_q: 12.264972, mean_eps: 0.837409
  27238/150000: episode: 296, duration: 0.547s, episode steps:  86, steps per second: 157, episode reward: -195.257, mean reward: -2.270 [-100.000,  5.986], mean action: 1.616 [0.000, 3.000],  loss: 8.565084, mae: 26.924950, mean_q: 12.922411, mean_eps: 0.836833
  27339/150000: episode: 297, duration: 0.644s, episode steps: 101, steps per second: 157, episode reward: -123.443, mean reward: -1.222 [-100.000, 15.946], mean action: 1.545 [0.000, 3.000],  loss: 9.028960, mae: 26.967320, mean_q: 12.253362, mean_eps: 0.836272
  27449/150000: episode: 298, duration: 0.728s, episode steps: 110, steps per second: 151, episode reward: -91.886, mean reward: -0.835 [-100.000,  5.802], mean action: 1.564 [0.000, 3.000],  loss: 8.355589, mae: 26.589205, mean_q: 12.963347, mean_eps: 0.835639
  27520/150000: episode: 299, duration: 0.456s, episode steps:  71, steps per second: 156, episode reward: -66.968, mean reward: -0.943 [-100.000, 21.114], mean action: 1.817 [0.000, 3.000],  loss: 9.790359, mae: 27.131203, mean_q: 12.593679, mean_eps: 0.835096
  27631/150000: episode: 300, duration: 0.693s, episode steps: 111, steps per second: 160, episode reward: -159.037, mean reward: -1.433 [-100.000,  3.480], mean action: 1.640 [0.000, 3.000],  loss: 5.279614, mae: 26.942927, mean_q: 12.293150, mean_eps: 0.834550
  27703/150000: episode: 301, duration: 0.460s, episode steps:  72, steps per second: 157, episode reward: -45.343, mean reward: -0.630 [-100.000, 46.864], mean action: 1.639 [0.000, 3.000],  loss: 9.489001, mae: 26.928053, mean_q: 11.816812, mean_eps: 0.834001
  27796/150000: episode: 302, duration: 0.615s, episode steps:  93, steps per second: 151, episode reward: -91.291, mean reward: -0.982 [-100.000, 11.127], mean action: 1.656 [0.000, 3.000],  loss: 7.116927, mae: 27.294129, mean_q: 12.145231, mean_eps: 0.833506
  27876/150000: episode: 303, duration: 0.506s, episode steps:  80, steps per second: 158, episode reward: -123.342, mean reward: -1.542 [-100.000, 13.502], mean action: 1.475 [0.000, 3.000],  loss: 6.123927, mae: 27.266624, mean_q: 11.270436, mean_eps: 0.832987
  27948/150000: episode: 304, duration: 0.456s, episode steps:  72, steps per second: 158, episode reward: -48.981, mean reward: -0.680 [-100.000, 50.644], mean action: 1.375 [0.000, 3.000],  loss: 5.053753, mae: 26.909141, mean_q: 11.186096, mean_eps: 0.832531
  28027/150000: episode: 305, duration: 0.504s, episode steps:  79, steps per second: 157, episode reward: -63.157, mean reward: -0.799 [-100.000, 12.394], mean action: 1.658 [0.000, 3.000],  loss: 17.248409, mae: 26.934537, mean_q: 13.331948, mean_eps: 0.832078
  28151/150000: episode: 306, duration: 0.834s, episode steps: 124, steps per second: 149, episode reward: -75.740, mean reward: -0.611 [-100.000, 18.299], mean action: 1.589 [0.000, 3.000],  loss: 10.543964, mae: 27.817158, mean_q: 13.521070, mean_eps: 0.831469
  28283/150000: episode: 307, duration: 0.830s, episode steps: 132, steps per second: 159, episode reward: -233.694, mean reward: -1.770 [-100.000, 133.065], mean action: 1.576 [0.000, 3.000],  loss: 10.455588, mae: 27.255021, mean_q: 13.558041, mean_eps: 0.830701
  28366/150000: episode: 308, duration: 0.519s, episode steps:  83, steps per second: 160, episode reward: -80.665, mean reward: -0.972 [-100.000, 54.337], mean action: 1.590 [0.000, 3.000],  loss: 10.808168, mae: 27.771212, mean_q: 13.386609, mean_eps: 0.830056
  28467/150000: episode: 309, duration: 0.677s, episode steps: 101, steps per second: 149, episode reward: -141.068, mean reward: -1.397 [-100.000,  5.165], mean action: 1.376 [0.000, 3.000],  loss: 10.555371, mae: 27.547221, mean_q: 13.393620, mean_eps: 0.829504
  28538/150000: episode: 310, duration: 0.444s, episode steps:  71, steps per second: 160, episode reward: -78.470, mean reward: -1.105 [-100.000,  9.130], mean action: 1.577 [0.000, 3.000],  loss: 9.075955, mae: 27.626343, mean_q: 12.790338, mean_eps: 0.828988
  28642/150000: episode: 311, duration: 0.653s, episode steps: 104, steps per second: 159, episode reward: -119.730, mean reward: -1.151 [-100.000,  7.541], mean action: 1.423 [0.000, 3.000],  loss: 9.930584, mae: 27.566207, mean_q: 12.781009, mean_eps: 0.828463
  28737/150000: episode: 312, duration: 0.626s, episode steps:  95, steps per second: 152, episode reward: -97.195, mean reward: -1.023 [-100.000, 12.521], mean action: 1.611 [0.000, 3.000],  loss: 11.222173, mae: 27.385597, mean_q: 13.342487, mean_eps: 0.827866
  28806/150000: episode: 313, duration: 0.470s, episode steps:  69, steps per second: 147, episode reward: -68.962, mean reward: -0.999 [-100.000, 10.061], mean action: 1.536 [0.000, 3.000],  loss: 12.662858, mae: 27.580688, mean_q: 12.380700, mean_eps: 0.827374
  28872/150000: episode: 314, duration: 0.426s, episode steps:  66, steps per second: 155, episode reward: -89.659, mean reward: -1.358 [-100.000,  6.881], mean action: 1.652 [0.000, 3.000],  loss: 14.333322, mae: 27.419022, mean_q: 13.228741, mean_eps: 0.826969
  28941/150000: episode: 315, duration: 0.448s, episode steps:  69, steps per second: 154, episode reward: -157.241, mean reward: -2.279 [-100.000, 11.156], mean action: 1.478 [0.000, 3.000],  loss: 15.384858, mae: 27.357744, mean_q: 13.012692, mean_eps: 0.826564
  29013/150000: episode: 316, duration: 0.471s, episode steps:  72, steps per second: 153, episode reward: -101.689, mean reward: -1.412 [-100.000,  6.808], mean action: 1.542 [0.000, 3.000],  loss: 7.634714, mae: 27.384709, mean_q: 13.518932, mean_eps: 0.826141
  29083/150000: episode: 317, duration: 0.471s, episode steps:  70, steps per second: 149, episode reward: -43.140, mean reward: -0.616 [-100.000,  7.638], mean action: 1.586 [0.000, 3.000],  loss: 9.676105, mae: 27.883937, mean_q: 12.658164, mean_eps: 0.825715
  29145/150000: episode: 318, duration: 0.404s, episode steps:  62, steps per second: 154, episode reward: -80.936, mean reward: -1.305 [-100.000, 17.517], mean action: 1.710 [0.000, 3.000],  loss: 10.012495, mae: 28.010193, mean_q: 14.183500, mean_eps: 0.825319
  29226/150000: episode: 319, duration: 0.512s, episode steps:  81, steps per second: 158, episode reward: -90.539, mean reward: -1.118 [-100.000,  7.082], mean action: 1.556 [0.000, 3.000],  loss: 9.371987, mae: 27.721300, mean_q: 14.192125, mean_eps: 0.824890
  29346/150000: episode: 320, duration: 0.757s, episode steps: 120, steps per second: 159, episode reward: -82.922, mean reward: -0.691 [-100.000,  9.955], mean action: 1.492 [0.000, 3.000],  loss: 8.831904, mae: 28.105896, mean_q: 13.845156, mean_eps: 0.824287
  29463/150000: episode: 321, duration: 0.786s, episode steps: 117, steps per second: 149, episode reward: -53.578, mean reward: -0.458 [-100.000, 20.822], mean action: 1.632 [0.000, 3.000],  loss: 13.331882, mae: 28.235000, mean_q: 14.265326, mean_eps: 0.823576
  29546/150000: episode: 322, duration: 0.524s, episode steps:  83, steps per second: 158, episode reward: -75.365, mean reward: -0.908 [-100.000, 12.023], mean action: 1.518 [0.000, 3.000],  loss: 7.352708, mae: 27.843152, mean_q: 13.862409, mean_eps: 0.822976
  29630/150000: episode: 323, duration: 0.529s, episode steps:  84, steps per second: 159, episode reward: -64.072, mean reward: -0.763 [-100.000,  6.640], mean action: 1.548 [0.000, 3.000],  loss: 12.115000, mae: 27.494587, mean_q: 13.881583, mean_eps: 0.822475
  29722/150000: episode: 324, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -56.758, mean reward: -0.617 [-100.000, 13.596], mean action: 1.413 [0.000, 3.000],  loss: 9.325412, mae: 28.197578, mean_q: 14.035832, mean_eps: 0.821947
  29813/150000: episode: 325, duration: 0.603s, episode steps:  91, steps per second: 151, episode reward: -47.530, mean reward: -0.522 [-100.000, 27.732], mean action: 1.593 [0.000, 3.000],  loss: 13.961379, mae: 28.272974, mean_q: 14.891711, mean_eps: 0.821398
  29919/150000: episode: 326, duration: 0.671s, episode steps: 106, steps per second: 158, episode reward: -85.145, mean reward: -0.803 [-100.000,  5.694], mean action: 1.500 [0.000, 3.000],  loss: 7.908998, mae: 27.971016, mean_q: 13.512732, mean_eps: 0.820807
  29992/150000: episode: 327, duration: 0.463s, episode steps:  73, steps per second: 158, episode reward: -27.210, mean reward: -0.373 [-100.000, 13.067], mean action: 1.507 [0.000, 3.000],  loss: 11.949754, mae: 27.910984, mean_q: 15.024967, mean_eps: 0.820270
  30079/150000: episode: 328, duration: 0.595s, episode steps:  87, steps per second: 146, episode reward: -108.251, mean reward: -1.244 [-100.000, 12.419], mean action: 1.598 [0.000, 3.000],  loss: 10.424096, mae: 28.634374, mean_q: 15.144071, mean_eps: 0.819790
  30184/150000: episode: 329, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward: -113.034, mean reward: -1.077 [-100.000, 10.970], mean action: 1.524 [0.000, 3.000],  loss: 8.105559, mae: 28.666679, mean_q: 14.542409, mean_eps: 0.819214
  30276/150000: episode: 330, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -122.754, mean reward: -1.334 [-100.000,  7.441], mean action: 1.446 [0.000, 3.000],  loss: 10.419827, mae: 28.791707, mean_q: 14.370485, mean_eps: 0.818623
  30355/150000: episode: 331, duration: 0.515s, episode steps:  79, steps per second: 153, episode reward: -60.999, mean reward: -0.772 [-100.000, 12.825], mean action: 1.506 [0.000, 3.000],  loss: 7.460090, mae: 28.681606, mean_q: 14.452848, mean_eps: 0.818110
  30435/150000: episode: 332, duration: 0.554s, episode steps:  80, steps per second: 144, episode reward: -82.559, mean reward: -1.032 [-100.000,  6.387], mean action: 1.400 [0.000, 3.000],  loss: 8.808724, mae: 29.151039, mean_q: 14.734888, mean_eps: 0.817633
  30506/150000: episode: 333, duration: 0.469s, episode steps:  71, steps per second: 151, episode reward: -78.705, mean reward: -1.109 [-100.000, 18.750], mean action: 1.465 [0.000, 3.000],  loss: 13.167106, mae: 29.143324, mean_q: 15.859782, mean_eps: 0.817180
  30610/150000: episode: 334, duration: 0.674s, episode steps: 104, steps per second: 154, episode reward: -97.343, mean reward: -0.936 [-100.000, 12.069], mean action: 1.548 [0.000, 3.000],  loss: 11.541815, mae: 28.764599, mean_q: 14.796213, mean_eps: 0.816655
  30698/150000: episode: 335, duration: 0.587s, episode steps:  88, steps per second: 150, episode reward: -63.484, mean reward: -0.721 [-100.000, 24.553], mean action: 1.352 [0.000, 3.000],  loss: 9.326261, mae: 28.863434, mean_q: 13.699096, mean_eps: 0.816079
  30821/150000: episode: 336, duration: 0.834s, episode steps: 123, steps per second: 148, episode reward: -165.289, mean reward: -1.344 [-100.000,  3.004], mean action: 1.528 [0.000, 3.000],  loss: 6.896164, mae: 28.470297, mean_q: 15.225984, mean_eps: 0.815446
  30922/150000: episode: 337, duration: 0.642s, episode steps: 101, steps per second: 157, episode reward: -70.156, mean reward: -0.695 [-100.000, 20.284], mean action: 1.495 [0.000, 3.000],  loss: 10.809849, mae: 29.000053, mean_q: 14.255219, mean_eps: 0.814774
  31024/150000: episode: 338, duration: 0.663s, episode steps: 102, steps per second: 154, episode reward: -138.708, mean reward: -1.360 [-100.000, 13.539], mean action: 1.480 [0.000, 3.000],  loss: 10.281146, mae: 28.752783, mean_q: 15.246112, mean_eps: 0.814165
  31096/150000: episode: 339, duration: 0.479s, episode steps:  72, steps per second: 150, episode reward: -15.285, mean reward: -0.212 [-100.000, 16.784], mean action: 1.653 [0.000, 3.000],  loss: 11.271337, mae: 28.706611, mean_q: 15.052944, mean_eps: 0.813643
  31202/150000: episode: 340, duration: 0.678s, episode steps: 106, steps per second: 156, episode reward: -227.568, mean reward: -2.147 [-100.000,  1.043], mean action: 1.557 [0.000, 3.000],  loss: 9.057155, mae: 28.622036, mean_q: 14.864075, mean_eps: 0.813109
  31279/150000: episode: 341, duration: 0.498s, episode steps:  77, steps per second: 155, episode reward: -123.675, mean reward: -1.606 [-100.000, 10.723], mean action: 1.610 [0.000, 3.000],  loss: 9.911514, mae: 28.762571, mean_q: 13.717631, mean_eps: 0.812560
  31390/150000: episode: 342, duration: 0.751s, episode steps: 111, steps per second: 148, episode reward: -170.410, mean reward: -1.535 [-100.000,  4.106], mean action: 1.541 [0.000, 3.000],  loss: 11.465237, mae: 29.023916, mean_q: 13.625587, mean_eps: 0.811996
  31508/150000: episode: 343, duration: 0.766s, episode steps: 118, steps per second: 154, episode reward: -61.913, mean reward: -0.525 [-100.000, 17.434], mean action: 1.712 [0.000, 3.000],  loss: 10.366988, mae: 29.125084, mean_q: 14.363934, mean_eps: 0.811309
  31631/150000: episode: 344, duration: 0.812s, episode steps: 123, steps per second: 152, episode reward: -130.981, mean reward: -1.065 [-100.000,  3.880], mean action: 1.504 [0.000, 3.000],  loss: 10.745216, mae: 29.029111, mean_q: 14.191909, mean_eps: 0.810586
  31750/150000: episode: 345, duration: 0.791s, episode steps: 119, steps per second: 150, episode reward: -94.972, mean reward: -0.798 [-100.000,  6.047], mean action: 1.345 [0.000, 3.000],  loss: 12.368853, mae: 28.654581, mean_q: 15.054213, mean_eps: 0.809860
  31835/150000: episode: 346, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -74.187, mean reward: -0.873 [-100.000,  7.342], mean action: 1.706 [0.000, 3.000],  loss: 11.272364, mae: 28.595484, mean_q: 15.131096, mean_eps: 0.809248
  31908/150000: episode: 347, duration: 0.472s, episode steps:  73, steps per second: 155, episode reward: -50.894, mean reward: -0.697 [-100.000,  7.536], mean action: 1.699 [0.000, 3.000],  loss: 7.105833, mae: 29.057659, mean_q: 15.594260, mean_eps: 0.808774
  31989/150000: episode: 348, duration: 0.534s, episode steps:  81, steps per second: 152, episode reward: -97.388, mean reward: -1.202 [-100.000,  6.455], mean action: 1.654 [0.000, 3.000],  loss: 8.809448, mae: 28.736169, mean_q: 14.673190, mean_eps: 0.808312
  32074/150000: episode: 349, duration: 0.581s, episode steps:  85, steps per second: 146, episode reward: -81.365, mean reward: -0.957 [-100.000, 14.760], mean action: 1.624 [0.000, 3.000],  loss: 14.182684, mae: 29.073108, mean_q: 15.552888, mean_eps: 0.807814
  32184/150000: episode: 350, duration: 0.735s, episode steps: 110, steps per second: 150, episode reward: -121.228, mean reward: -1.102 [-100.000,  8.801], mean action: 1.527 [0.000, 3.000],  loss: 11.669397, mae: 29.195710, mean_q: 15.646055, mean_eps: 0.807229
  32278/150000: episode: 351, duration: 0.594s, episode steps:  94, steps per second: 158, episode reward: -90.960, mean reward: -0.968 [-100.000,  9.983], mean action: 1.511 [0.000, 3.000],  loss: 11.085165, mae: 30.117556, mean_q: 14.627182, mean_eps: 0.806617
  32353/150000: episode: 352, duration: 0.517s, episode steps:  75, steps per second: 145, episode reward: -148.360, mean reward: -1.978 [-100.000,  7.935], mean action: 1.760 [0.000, 3.000],  loss: 11.023617, mae: 29.586235, mean_q: 15.615245, mean_eps: 0.806110
  32450/150000: episode: 353, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -116.106, mean reward: -1.197 [-100.000, 10.340], mean action: 1.443 [0.000, 3.000],  loss: 7.620746, mae: 29.800356, mean_q: 15.966608, mean_eps: 0.805594
  32548/150000: episode: 354, duration: 0.629s, episode steps:  98, steps per second: 156, episode reward: -44.067, mean reward: -0.450 [-100.000, 12.967], mean action: 1.745 [0.000, 3.000],  loss: 7.460371, mae: 29.792957, mean_q: 15.851097, mean_eps: 0.805009
  32637/150000: episode: 355, duration: 0.568s, episode steps:  89, steps per second: 157, episode reward: -130.689, mean reward: -1.468 [-100.000,  5.566], mean action: 1.393 [0.000, 3.000],  loss: 8.809092, mae: 29.558242, mean_q: 16.008864, mean_eps: 0.804448
  32738/150000: episode: 356, duration: 0.696s, episode steps: 101, steps per second: 145, episode reward: -75.536, mean reward: -0.748 [-100.000, 14.352], mean action: 1.653 [0.000, 3.000],  loss: 10.884951, mae: 29.421460, mean_q: 16.351891, mean_eps: 0.803878
  32810/150000: episode: 357, duration: 0.467s, episode steps:  72, steps per second: 154, episode reward:  3.460, mean reward:  0.048 [-100.000, 21.374], mean action: 1.625 [0.000, 3.000],  loss: 6.653661, mae: 29.533008, mean_q: 15.114765, mean_eps: 0.803359
  32911/150000: episode: 358, duration: 0.636s, episode steps: 101, steps per second: 159, episode reward: -98.008, mean reward: -0.970 [-100.000,  8.565], mean action: 1.515 [0.000, 3.000],  loss: 8.425837, mae: 29.641410, mean_q: 16.074552, mean_eps: 0.802840
  33042/150000: episode: 359, duration: 0.897s, episode steps: 131, steps per second: 146, episode reward: -125.746, mean reward: -0.960 [-100.000, 20.251], mean action: 1.626 [0.000, 3.000],  loss: 9.111683, mae: 29.787874, mean_q: 16.797418, mean_eps: 0.802144
  33164/150000: episode: 360, duration: 0.778s, episode steps: 122, steps per second: 157, episode reward: -162.090, mean reward: -1.329 [-100.000,  4.390], mean action: 1.697 [0.000, 3.000],  loss: 8.740901, mae: 29.760982, mean_q: 17.920476, mean_eps: 0.801385
  33244/150000: episode: 361, duration: 0.498s, episode steps:  80, steps per second: 161, episode reward: -92.315, mean reward: -1.154 [-100.000, 10.376], mean action: 1.475 [0.000, 3.000],  loss: 8.203890, mae: 29.709443, mean_q: 16.756325, mean_eps: 0.800779
  33372/150000: episode: 362, duration: 0.878s, episode steps: 128, steps per second: 146, episode reward: -71.533, mean reward: -0.559 [-100.000,  6.489], mean action: 1.625 [0.000, 3.000],  loss: 7.598985, mae: 30.279523, mean_q: 17.518711, mean_eps: 0.800155
  33488/150000: episode: 363, duration: 0.760s, episode steps: 116, steps per second: 153, episode reward: -83.245, mean reward: -0.718 [-100.000, 12.168], mean action: 1.664 [0.000, 3.000],  loss: 5.867672, mae: 29.979723, mean_q: 17.283605, mean_eps: 0.799423
  33587/150000: episode: 364, duration: 0.630s, episode steps:  99, steps per second: 157, episode reward: -135.552, mean reward: -1.369 [-100.000, 16.168], mean action: 1.515 [0.000, 3.000],  loss: 7.869435, mae: 30.165568, mean_q: 16.982833, mean_eps: 0.798778
  33686/150000: episode: 365, duration: 0.676s, episode steps:  99, steps per second: 146, episode reward: -112.069, mean reward: -1.132 [-100.000,  6.280], mean action: 1.323 [0.000, 3.000],  loss: 7.389785, mae: 29.752965, mean_q: 17.780193, mean_eps: 0.798184
  33804/150000: episode: 366, duration: 0.790s, episode steps: 118, steps per second: 149, episode reward: -92.290, mean reward: -0.782 [-100.000,  8.311], mean action: 1.542 [0.000, 3.000],  loss: 7.636603, mae: 29.880243, mean_q: 16.460371, mean_eps: 0.797533
  33859/150000: episode: 367, duration: 0.357s, episode steps:  55, steps per second: 154, episode reward: -83.779, mean reward: -1.523 [-100.000,  6.957], mean action: 1.618 [0.000, 3.000],  loss: 8.685911, mae: 29.301985, mean_q: 17.131013, mean_eps: 0.797014
  33953/150000: episode: 368, duration: 0.607s, episode steps:  94, steps per second: 155, episode reward: -73.101, mean reward: -0.778 [-100.000, 15.621], mean action: 1.521 [0.000, 3.000],  loss: 8.729969, mae: 29.963798, mean_q: 17.553120, mean_eps: 0.796567
  34085/150000: episode: 369, duration: 0.965s, episode steps: 132, steps per second: 137, episode reward: -63.198, mean reward: -0.479 [-100.000,  7.447], mean action: 1.553 [0.000, 3.000],  loss: 7.585697, mae: 29.947959, mean_q: 17.271172, mean_eps: 0.795889
  34154/150000: episode: 370, duration: 0.542s, episode steps:  69, steps per second: 127, episode reward: -99.775, mean reward: -1.446 [-100.000,  6.240], mean action: 1.855 [0.000, 3.000],  loss: 8.773002, mae: 30.426358, mean_q: 17.747586, mean_eps: 0.795286
  34273/150000: episode: 371, duration: 0.911s, episode steps: 119, steps per second: 131, episode reward: -72.550, mean reward: -0.610 [-100.000, 17.170], mean action: 1.613 [0.000, 3.000],  loss: 7.840276, mae: 30.239765, mean_q: 16.266555, mean_eps: 0.794722
  34388/150000: episode: 372, duration: 0.824s, episode steps: 115, steps per second: 139, episode reward: -88.576, mean reward: -0.770 [-100.000,  6.216], mean action: 1.409 [0.000, 3.000],  loss: 6.650422, mae: 29.977321, mean_q: 17.075735, mean_eps: 0.794020
  34505/150000: episode: 373, duration: 0.805s, episode steps: 117, steps per second: 145, episode reward: -149.342, mean reward: -1.276 [-100.000,  3.784], mean action: 1.419 [0.000, 3.000],  loss: 7.290934, mae: 29.993946, mean_q: 17.421017, mean_eps: 0.793324
  34626/150000: episode: 374, duration: 0.876s, episode steps: 121, steps per second: 138, episode reward: -53.149, mean reward: -0.439 [-100.000,  7.513], mean action: 1.496 [0.000, 3.000],  loss: 10.476968, mae: 30.077077, mean_q: 16.750405, mean_eps: 0.792610
  34704/150000: episode: 375, duration: 0.562s, episode steps:  78, steps per second: 139, episode reward: -68.950, mean reward: -0.884 [-100.000, 14.808], mean action: 1.551 [0.000, 3.000],  loss: 9.371532, mae: 30.107026, mean_q: 16.608814, mean_eps: 0.792013
  34794/150000: episode: 376, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -107.387, mean reward: -1.193 [-100.000,  5.282], mean action: 1.756 [0.000, 3.000],  loss: 7.715158, mae: 30.344710, mean_q: 16.874498, mean_eps: 0.791509
  34872/150000: episode: 377, duration: 0.511s, episode steps:  78, steps per second: 153, episode reward: -97.335, mean reward: -1.248 [-100.000,  6.778], mean action: 1.590 [0.000, 3.000],  loss: 8.459799, mae: 30.133276, mean_q: 18.010357, mean_eps: 0.791005
  34972/150000: episode: 378, duration: 0.682s, episode steps: 100, steps per second: 147, episode reward: -77.862, mean reward: -0.779 [-100.000, 11.630], mean action: 1.680 [0.000, 3.000],  loss: 7.663939, mae: 29.938790, mean_q: 17.384289, mean_eps: 0.790471
  35103/150000: episode: 379, duration: 0.890s, episode steps: 131, steps per second: 147, episode reward: -86.293, mean reward: -0.659 [-100.000,  4.580], mean action: 1.496 [0.000, 3.000],  loss: 6.928013, mae: 30.840786, mean_q: 17.561084, mean_eps: 0.789778
  35173/150000: episode: 380, duration: 0.448s, episode steps:  70, steps per second: 156, episode reward: -75.198, mean reward: -1.074 [-100.000, 22.002], mean action: 1.457 [0.000, 3.000],  loss: 6.529119, mae: 30.727700, mean_q: 17.801434, mean_eps: 0.789175
  35243/150000: episode: 381, duration: 0.482s, episode steps:  70, steps per second: 145, episode reward: -65.855, mean reward: -0.941 [-100.000,  6.498], mean action: 1.586 [0.000, 3.000],  loss: 6.253640, mae: 30.442142, mean_q: 18.932989, mean_eps: 0.788755
  35370/150000: episode: 382, duration: 0.824s, episode steps: 127, steps per second: 154, episode reward: -91.052, mean reward: -0.717 [-100.000, 17.044], mean action: 1.386 [0.000, 3.000],  loss: 6.751417, mae: 30.657831, mean_q: 17.631533, mean_eps: 0.788164
  35443/150000: episode: 383, duration: 0.470s, episode steps:  73, steps per second: 155, episode reward: -86.824, mean reward: -1.189 [-100.000, 10.578], mean action: 1.493 [0.000, 3.000],  loss: 6.365860, mae: 30.991905, mean_q: 18.239846, mean_eps: 0.787564
  35512/150000: episode: 384, duration: 0.443s, episode steps:  69, steps per second: 156, episode reward: -55.490, mean reward: -0.804 [-100.000, 12.704], mean action: 1.638 [0.000, 3.000],  loss: 6.500273, mae: 30.854160, mean_q: 16.738288, mean_eps: 0.787138
  35610/150000: episode: 385, duration: 0.663s, episode steps:  98, steps per second: 148, episode reward: -70.255, mean reward: -0.717 [-100.000, 11.406], mean action: 1.592 [0.000, 3.000],  loss: 6.785493, mae: 31.152460, mean_q: 18.249561, mean_eps: 0.786637
  35687/150000: episode: 386, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -77.275, mean reward: -1.004 [-100.000, 17.048], mean action: 1.766 [0.000, 3.000],  loss: 6.901076, mae: 30.767627, mean_q: 18.802491, mean_eps: 0.786112
  35795/150000: episode: 387, duration: 0.737s, episode steps: 108, steps per second: 146, episode reward: -112.353, mean reward: -1.040 [-100.000,  7.872], mean action: 1.509 [0.000, 3.000],  loss: 8.161109, mae: 31.306365, mean_q: 17.303164, mean_eps: 0.785557
  35879/150000: episode: 388, duration: 0.575s, episode steps:  84, steps per second: 146, episode reward: -113.543, mean reward: -1.352 [-100.000, 12.096], mean action: 1.667 [0.000, 3.000],  loss: 7.983832, mae: 30.819564, mean_q: 18.267524, mean_eps: 0.784981
  35941/150000: episode: 389, duration: 0.418s, episode steps:  62, steps per second: 148, episode reward: -48.951, mean reward: -0.790 [-100.000, 16.065], mean action: 1.597 [0.000, 3.000],  loss: 5.921978, mae: 31.285604, mean_q: 17.841787, mean_eps: 0.784543
  36008/150000: episode: 390, duration: 0.439s, episode steps:  67, steps per second: 153, episode reward: -78.701, mean reward: -1.175 [-100.000, 10.628], mean action: 1.866 [0.000, 3.000],  loss: 8.252138, mae: 31.004162, mean_q: 18.350079, mean_eps: 0.784156
  36091/150000: episode: 391, duration: 0.550s, episode steps:  83, steps per second: 151, episode reward: -105.578, mean reward: -1.272 [-100.000,  7.910], mean action: 1.518 [0.000, 3.000],  loss: 7.909808, mae: 31.858931, mean_q: 19.179546, mean_eps: 0.783706
  36195/150000: episode: 392, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -113.134, mean reward: -1.088 [-100.000, 20.669], mean action: 1.356 [0.000, 3.000],  loss: 9.656413, mae: 32.014560, mean_q: 20.144271, mean_eps: 0.783145
  36280/150000: episode: 393, duration: 0.565s, episode steps:  85, steps per second: 151, episode reward: -132.523, mean reward: -1.559 [-100.000,  9.537], mean action: 1.400 [0.000, 3.000],  loss: 6.694509, mae: 31.970315, mean_q: 19.139614, mean_eps: 0.782578
  36340/150000: episode: 394, duration: 0.387s, episode steps:  60, steps per second: 155, episode reward: -79.873, mean reward: -1.331 [-100.000, 10.312], mean action: 1.617 [0.000, 3.000],  loss: 5.291765, mae: 31.925253, mean_q: 19.791206, mean_eps: 0.782143
  36428/150000: episode: 395, duration: 0.587s, episode steps:  88, steps per second: 150, episode reward: -38.206, mean reward: -0.434 [-100.000, 23.744], mean action: 1.614 [0.000, 3.000],  loss: 5.086478, mae: 31.609363, mean_q: 18.050821, mean_eps: 0.781699
  36530/150000: episode: 396, duration: 0.705s, episode steps: 102, steps per second: 145, episode reward: -93.192, mean reward: -0.914 [-100.000, 10.682], mean action: 1.480 [0.000, 3.000],  loss: 9.501787, mae: 32.085555, mean_q: 18.018271, mean_eps: 0.781129
  36615/150000: episode: 397, duration: 0.561s, episode steps:  85, steps per second: 152, episode reward: -110.620, mean reward: -1.301 [-100.000,  7.693], mean action: 1.471 [0.000, 3.000],  loss: 4.445803, mae: 31.932347, mean_q: 17.559721, mean_eps: 0.780568
  36734/150000: episode: 398, duration: 0.766s, episode steps: 119, steps per second: 155, episode reward: -88.501, mean reward: -0.744 [-100.000, 11.278], mean action: 1.655 [0.000, 3.000],  loss: 7.062330, mae: 32.153701, mean_q: 18.218742, mean_eps: 0.779956
  36830/150000: episode: 399, duration: 0.640s, episode steps:  96, steps per second: 150, episode reward: -118.821, mean reward: -1.238 [-100.000, 11.440], mean action: 1.354 [0.000, 3.000],  loss: 6.048376, mae: 31.794065, mean_q: 18.904256, mean_eps: 0.779311
  36942/150000: episode: 400, duration: 0.739s, episode steps: 112, steps per second: 152, episode reward: -133.525, mean reward: -1.192 [-100.000,  5.493], mean action: 1.473 [0.000, 3.000],  loss: 5.968680, mae: 31.740067, mean_q: 18.443730, mean_eps: 0.778687
  37034/150000: episode: 401, duration: 0.586s, episode steps:  92, steps per second: 157, episode reward: -113.386, mean reward: -1.232 [-100.000,  5.040], mean action: 1.783 [0.000, 3.000],  loss: 5.766474, mae: 32.081151, mean_q: 18.968005, mean_eps: 0.778075
  37112/150000: episode: 402, duration: 0.506s, episode steps:  78, steps per second: 154, episode reward: -31.444, mean reward: -0.403 [-100.000, 16.472], mean action: 1.449 [0.000, 3.000],  loss: 7.118307, mae: 31.834173, mean_q: 18.540733, mean_eps: 0.777565
  37187/150000: episode: 403, duration: 0.532s, episode steps:  75, steps per second: 141, episode reward: -35.683, mean reward: -0.476 [-100.000, 10.443], mean action: 1.520 [0.000, 3.000],  loss: 7.277517, mae: 32.081957, mean_q: 19.492869, mean_eps: 0.777106
  37287/150000: episode: 404, duration: 0.666s, episode steps: 100, steps per second: 150, episode reward: -94.598, mean reward: -0.946 [-100.000,  6.539], mean action: 1.590 [0.000, 3.000],  loss: 7.502652, mae: 32.001813, mean_q: 19.008834, mean_eps: 0.776581
  37380/150000: episode: 405, duration: 0.597s, episode steps:  93, steps per second: 156, episode reward: -305.491, mean reward: -3.285 [-100.000,  0.643], mean action: 1.602 [0.000, 3.000],  loss: 4.701860, mae: 31.772373, mean_q: 19.444060, mean_eps: 0.776002
  37494/150000: episode: 406, duration: 0.760s, episode steps: 114, steps per second: 150, episode reward: -94.555, mean reward: -0.829 [-100.000, 12.167], mean action: 1.351 [0.000, 3.000],  loss: 6.733562, mae: 31.611419, mean_q: 19.007639, mean_eps: 0.775381
  37567/150000: episode: 407, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -40.763, mean reward: -0.558 [-100.000, 20.998], mean action: 1.397 [0.000, 3.000],  loss: 9.790390, mae: 32.805953, mean_q: 18.396304, mean_eps: 0.774820
  37665/150000: episode: 408, duration: 0.743s, episode steps:  98, steps per second: 132, episode reward: -58.873, mean reward: -0.601 [-100.000, 13.595], mean action: 1.612 [0.000, 3.000],  loss: 10.816384, mae: 32.243324, mean_q: 19.080495, mean_eps: 0.774307
  37769/150000: episode: 409, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -180.958, mean reward: -1.740 [-100.000,  6.736], mean action: 1.433 [0.000, 3.000],  loss: 10.806942, mae: 32.194654, mean_q: 19.016313, mean_eps: 0.773701
  37841/150000: episode: 410, duration: 0.494s, episode steps:  72, steps per second: 146, episode reward: -56.320, mean reward: -0.782 [-100.000, 31.958], mean action: 1.347 [0.000, 3.000],  loss: 7.107541, mae: 32.131677, mean_q: 19.215151, mean_eps: 0.773173
  37949/150000: episode: 411, duration: 0.708s, episode steps: 108, steps per second: 153, episode reward: -113.915, mean reward: -1.055 [-100.000,  8.948], mean action: 1.657 [0.000, 3.000],  loss: 7.710605, mae: 32.020217, mean_q: 18.804976, mean_eps: 0.772633
  38009/150000: episode: 412, duration: 0.387s, episode steps:  60, steps per second: 155, episode reward: -112.399, mean reward: -1.873 [-100.000, 28.292], mean action: 1.317 [0.000, 3.000],  loss: 6.882494, mae: 32.151157, mean_q: 19.433284, mean_eps: 0.772129
  38133/150000: episode: 413, duration: 0.853s, episode steps: 124, steps per second: 145, episode reward: -36.090, mean reward: -0.291 [-100.000, 83.478], mean action: 1.629 [0.000, 3.000],  loss: 6.087614, mae: 32.075532, mean_q: 20.934148, mean_eps: 0.771577
  38217/150000: episode: 414, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -76.272, mean reward: -0.908 [-100.000, 16.159], mean action: 1.738 [0.000, 3.000],  loss: 5.278961, mae: 32.522115, mean_q: 19.353142, mean_eps: 0.770953
  38301/150000: episode: 415, duration: 0.544s, episode steps:  84, steps per second: 154, episode reward: -176.752, mean reward: -2.104 [-100.000, 15.146], mean action: 1.524 [0.000, 3.000],  loss: 9.681424, mae: 32.059745, mean_q: 20.209554, mean_eps: 0.770449
  38407/150000: episode: 416, duration: 0.681s, episode steps: 106, steps per second: 156, episode reward: -63.575, mean reward: -0.600 [-100.000, 12.084], mean action: 1.557 [0.000, 3.000],  loss: 6.339162, mae: 32.327040, mean_q: 20.587382, mean_eps: 0.769879
  38505/150000: episode: 417, duration: 0.678s, episode steps:  98, steps per second: 145, episode reward: -131.475, mean reward: -1.342 [-100.000,  5.511], mean action: 1.296 [0.000, 3.000],  loss: 11.684878, mae: 32.408845, mean_q: 19.986259, mean_eps: 0.769267
  38634/150000: episode: 418, duration: 0.842s, episode steps: 129, steps per second: 153, episode reward: -393.334, mean reward: -3.049 [-100.000, 45.490], mean action: 1.612 [0.000, 3.000],  loss: 7.206176, mae: 32.583365, mean_q: 19.111198, mean_eps: 0.768586
  38716/150000: episode: 419, duration: 0.538s, episode steps:  82, steps per second: 152, episode reward: -100.596, mean reward: -1.227 [-100.000, 35.003], mean action: 1.366 [0.000, 3.000],  loss: 5.882298, mae: 32.444584, mean_q: 19.951000, mean_eps: 0.767953
  38836/150000: episode: 420, duration: 0.832s, episode steps: 120, steps per second: 144, episode reward: -125.941, mean reward: -1.050 [-100.000, 26.796], mean action: 1.667 [0.000, 3.000],  loss: 8.523036, mae: 32.584182, mean_q: 19.579815, mean_eps: 0.767347
  38932/150000: episode: 421, duration: 0.624s, episode steps:  96, steps per second: 154, episode reward: -112.816, mean reward: -1.175 [-100.000,  5.067], mean action: 1.740 [0.000, 3.000],  loss: 7.256391, mae: 32.723338, mean_q: 20.619795, mean_eps: 0.766699
  39031/150000: episode: 422, duration: 0.663s, episode steps:  99, steps per second: 149, episode reward: -64.068, mean reward: -0.647 [-100.000, 13.849], mean action: 1.646 [0.000, 3.000],  loss: 5.589372, mae: 32.216735, mean_q: 19.146680, mean_eps: 0.766114
  39119/150000: episode: 423, duration: 0.615s, episode steps:  88, steps per second: 143, episode reward: -79.073, mean reward: -0.899 [-100.000,  9.265], mean action: 1.568 [0.000, 3.000],  loss: 6.569020, mae: 32.073645, mean_q: 20.580177, mean_eps: 0.765553
  39217/150000: episode: 424, duration: 0.646s, episode steps:  98, steps per second: 152, episode reward: -112.807, mean reward: -1.151 [-100.000,  6.126], mean action: 1.469 [0.000, 3.000],  loss: 11.821123, mae: 32.745690, mean_q: 21.010797, mean_eps: 0.764995
  39304/150000: episode: 425, duration: 0.563s, episode steps:  87, steps per second: 154, episode reward: -78.482, mean reward: -0.902 [-100.000,  8.862], mean action: 1.310 [0.000, 3.000],  loss: 8.422361, mae: 32.648089, mean_q: 20.796829, mean_eps: 0.764440
  39423/150000: episode: 426, duration: 0.817s, episode steps: 119, steps per second: 146, episode reward: -168.413, mean reward: -1.415 [-100.000,  8.314], mean action: 1.328 [0.000, 3.000],  loss: 9.565725, mae: 32.661990, mean_q: 20.984986, mean_eps: 0.763822
  39534/150000: episode: 427, duration: 0.757s, episode steps: 111, steps per second: 147, episode reward: -65.417, mean reward: -0.589 [-100.000, 15.150], mean action: 1.775 [0.000, 3.000],  loss: 6.685282, mae: 32.611452, mean_q: 20.875988, mean_eps: 0.763132
  39596/150000: episode: 428, duration: 0.408s, episode steps:  62, steps per second: 152, episode reward: -67.663, mean reward: -1.091 [-100.000,  4.977], mean action: 1.452 [0.000, 3.000],  loss: 7.434659, mae: 33.014838, mean_q: 20.363267, mean_eps: 0.762613
  39704/150000: episode: 429, duration: 0.701s, episode steps: 108, steps per second: 154, episode reward: -71.674, mean reward: -0.664 [-100.000,  6.453], mean action: 1.435 [0.000, 3.000],  loss: 11.570972, mae: 32.741863, mean_q: 19.501608, mean_eps: 0.762103
  39772/150000: episode: 430, duration: 0.512s, episode steps:  68, steps per second: 133, episode reward: -72.832, mean reward: -1.071 [-100.000, 11.205], mean action: 1.574 [0.000, 3.000],  loss: 10.316469, mae: 32.740824, mean_q: 20.360522, mean_eps: 0.761575
  39866/150000: episode: 431, duration: 0.632s, episode steps:  94, steps per second: 149, episode reward: -82.757, mean reward: -0.880 [-100.000, 17.993], mean action: 1.660 [0.000, 3.000],  loss: 8.921808, mae: 32.717304, mean_q: 19.898667, mean_eps: 0.761089
  39935/150000: episode: 432, duration: 0.450s, episode steps:  69, steps per second: 153, episode reward: -88.634, mean reward: -1.285 [-100.000,  6.808], mean action: 1.652 [0.000, 3.000],  loss: 8.892406, mae: 32.041185, mean_q: 19.946677, mean_eps: 0.760600
  40051/150000: episode: 433, duration: 0.770s, episode steps: 116, steps per second: 151, episode reward: -51.555, mean reward: -0.444 [-100.000, 17.179], mean action: 1.543 [0.000, 3.000],  loss: 8.657865, mae: 32.528799, mean_q: 20.441977, mean_eps: 0.760045
  40162/150000: episode: 434, duration: 0.752s, episode steps: 111, steps per second: 148, episode reward: -43.574, mean reward: -0.393 [-100.000, 12.917], mean action: 1.532 [0.000, 3.000],  loss: 10.094839, mae: 32.565030, mean_q: 19.720029, mean_eps: 0.759364
  40271/150000: episode: 435, duration: 0.724s, episode steps: 109, steps per second: 151, episode reward: -91.758, mean reward: -0.842 [-100.000,  6.087], mean action: 1.523 [0.000, 3.000],  loss: 8.283279, mae: 32.162467, mean_q: 20.644542, mean_eps: 0.758704
  40348/150000: episode: 436, duration: 0.500s, episode steps:  77, steps per second: 154, episode reward: -89.169, mean reward: -1.158 [-100.000,  9.921], mean action: 1.740 [0.000, 3.000],  loss: 6.106678, mae: 32.646252, mean_q: 21.279463, mean_eps: 0.758146
  40444/150000: episode: 437, duration: 0.672s, episode steps:  96, steps per second: 143, episode reward: -82.975, mean reward: -0.864 [-100.000, 16.345], mean action: 1.667 [0.000, 3.000],  loss: 6.664262, mae: 32.078551, mean_q: 20.758964, mean_eps: 0.757627
  40525/150000: episode: 438, duration: 0.535s, episode steps:  81, steps per second: 151, episode reward: -109.648, mean reward: -1.354 [-100.000,  8.199], mean action: 1.840 [0.000, 3.000],  loss: 15.030863, mae: 32.325520, mean_q: 19.987512, mean_eps: 0.757096
  40609/150000: episode: 439, duration: 0.553s, episode steps:  84, steps per second: 152, episode reward: -46.042, mean reward: -0.548 [-100.000,  7.285], mean action: 1.440 [0.000, 3.000],  loss: 13.603793, mae: 32.618410, mean_q: 19.860856, mean_eps: 0.756601
  40725/150000: episode: 440, duration: 0.784s, episode steps: 116, steps per second: 148, episode reward: -106.504, mean reward: -0.918 [-100.000, 11.502], mean action: 1.724 [0.000, 3.000],  loss: 11.875912, mae: 32.319305, mean_q: 20.725486, mean_eps: 0.756001
  40843/150000: episode: 441, duration: 0.775s, episode steps: 118, steps per second: 152, episode reward: -83.849, mean reward: -0.711 [-100.000,  8.084], mean action: 1.475 [0.000, 3.000],  loss: 7.233231, mae: 32.548781, mean_q: 20.391330, mean_eps: 0.755299
  40909/150000: episode: 442, duration: 0.438s, episode steps:  66, steps per second: 151, episode reward: -68.156, mean reward: -1.033 [-100.000, 10.167], mean action: 1.273 [0.000, 3.000],  loss: 7.210431, mae: 32.516223, mean_q: 20.566239, mean_eps: 0.754747
  41002/150000: episode: 443, duration: 0.629s, episode steps:  93, steps per second: 148, episode reward: -191.327, mean reward: -2.057 [-100.000, 23.043], mean action: 1.516 [0.000, 3.000],  loss: 6.617531, mae: 32.320684, mean_q: 20.597581, mean_eps: 0.754270
  41110/150000: episode: 444, duration: 0.729s, episode steps: 108, steps per second: 148, episode reward: -38.836, mean reward: -0.360 [-100.000, 59.461], mean action: 1.528 [0.000, 3.000],  loss: 6.756866, mae: 32.331626, mean_q: 19.154707, mean_eps: 0.753667
  41187/150000: episode: 445, duration: 0.506s, episode steps:  77, steps per second: 152, episode reward: -71.223, mean reward: -0.925 [-100.000, 11.786], mean action: 1.792 [0.000, 3.000],  loss: 8.145605, mae: 32.243996, mean_q: 19.695443, mean_eps: 0.753112
  41259/150000: episode: 446, duration: 0.504s, episode steps:  72, steps per second: 143, episode reward: -104.918, mean reward: -1.457 [-100.000,  4.972], mean action: 1.764 [0.000, 3.000],  loss: 5.744923, mae: 32.348609, mean_q: 20.626204, mean_eps: 0.752665
  41346/150000: episode: 447, duration: 0.708s, episode steps:  87, steps per second: 123, episode reward: -121.115, mean reward: -1.392 [-100.000,  9.433], mean action: 1.690 [0.000, 3.000],  loss: 7.771732, mae: 32.704156, mean_q: 19.701834, mean_eps: 0.752188
  41455/150000: episode: 448, duration: 0.850s, episode steps: 109, steps per second: 128, episode reward: -78.981, mean reward: -0.725 [-100.000, 11.974], mean action: 1.606 [0.000, 3.000],  loss: 9.112602, mae: 32.624766, mean_q: 20.186016, mean_eps: 0.751600
  41539/150000: episode: 449, duration: 0.560s, episode steps:  84, steps per second: 150, episode reward: -68.705, mean reward: -0.818 [-100.000, 17.948], mean action: 1.643 [0.000, 3.000],  loss: 6.418258, mae: 32.895306, mean_q: 20.646968, mean_eps: 0.751021
  41621/150000: episode: 450, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -122.527, mean reward: -1.494 [-100.000, 21.356], mean action: 1.134 [0.000, 3.000],  loss: 4.903860, mae: 32.459277, mean_q: 20.114383, mean_eps: 0.750523
  41714/150000: episode: 451, duration: 0.639s, episode steps:  93, steps per second: 145, episode reward: -115.683, mean reward: -1.244 [-100.000, 17.172], mean action: 1.495 [0.000, 3.000],  loss: 8.299766, mae: 32.113703, mean_q: 19.624049, mean_eps: 0.749998
  41843/150000: episode: 452, duration: 0.845s, episode steps: 129, steps per second: 153, episode reward: -42.656, mean reward: -0.331 [-100.000,  7.154], mean action: 1.504 [0.000, 3.000],  loss: 9.779988, mae: 32.354453, mean_q: 20.160950, mean_eps: 0.749332
  41911/150000: episode: 453, duration: 0.457s, episode steps:  68, steps per second: 149, episode reward: -148.433, mean reward: -2.183 [-100.000,  7.564], mean action: 1.412 [0.000, 3.000],  loss: 6.597249, mae: 32.432332, mean_q: 19.920374, mean_eps: 0.748741
  41993/150000: episode: 454, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -41.799, mean reward: -0.510 [-100.000,  8.143], mean action: 1.561 [0.000, 3.000],  loss: 7.919655, mae: 32.318135, mean_q: 19.462687, mean_eps: 0.748291
  42087/150000: episode: 455, duration: 0.617s, episode steps:  94, steps per second: 152, episode reward: -94.662, mean reward: -1.007 [-100.000, 12.925], mean action: 1.415 [0.000, 3.000],  loss: 9.563846, mae: 32.612709, mean_q: 20.432389, mean_eps: 0.747763
  42152/150000: episode: 456, duration: 0.429s, episode steps:  65, steps per second: 152, episode reward: -54.413, mean reward: -0.837 [-100.000, 16.026], mean action: 1.446 [0.000, 3.000],  loss: 10.849479, mae: 32.860985, mean_q: 18.980067, mean_eps: 0.747286
  42241/150000: episode: 457, duration: 0.590s, episode steps:  89, steps per second: 151, episode reward: -55.567, mean reward: -0.624 [-100.000, 12.219], mean action: 1.506 [0.000, 3.000],  loss: 7.110647, mae: 33.073816, mean_q: 21.667205, mean_eps: 0.746824
  42327/150000: episode: 458, duration: 0.610s, episode steps:  86, steps per second: 141, episode reward: -53.931, mean reward: -0.627 [-100.000,  7.406], mean action: 1.581 [0.000, 3.000],  loss: 7.703996, mae: 32.644628, mean_q: 20.363266, mean_eps: 0.746299
  42406/150000: episode: 459, duration: 0.525s, episode steps:  79, steps per second: 151, episode reward: -106.758, mean reward: -1.351 [-100.000, 11.095], mean action: 1.316 [0.000, 3.000],  loss: 6.395485, mae: 32.597037, mean_q: 20.464549, mean_eps: 0.745804
  42508/150000: episode: 460, duration: 0.667s, episode steps: 102, steps per second: 153, episode reward: -148.874, mean reward: -1.460 [-100.000,  4.206], mean action: 1.569 [0.000, 3.000],  loss: 9.170164, mae: 32.553393, mean_q: 21.271517, mean_eps: 0.745261
  42591/150000: episode: 461, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -98.605, mean reward: -1.188 [-100.000,  8.993], mean action: 1.687 [0.000, 3.000],  loss: 6.761790, mae: 32.560058, mean_q: 19.549642, mean_eps: 0.744706
  42680/150000: episode: 462, duration: 0.611s, episode steps:  89, steps per second: 146, episode reward: -52.738, mean reward: -0.593 [-100.000, 17.699], mean action: 1.528 [0.000, 3.000],  loss: 6.292523, mae: 32.677424, mean_q: 20.875411, mean_eps: 0.744190
  42767/150000: episode: 463, duration: 0.577s, episode steps:  87, steps per second: 151, episode reward: -42.937, mean reward: -0.494 [-100.000, 16.088], mean action: 1.483 [0.000, 3.000],  loss: 5.990875, mae: 32.334145, mean_q: 20.097716, mean_eps: 0.743662
  42839/150000: episode: 464, duration: 0.474s, episode steps:  72, steps per second: 152, episode reward: -87.722, mean reward: -1.218 [-100.000,  4.465], mean action: 1.611 [0.000, 3.000],  loss: 8.198121, mae: 32.826904, mean_q: 19.931370, mean_eps: 0.743185
  42906/150000: episode: 465, duration: 0.475s, episode steps:  67, steps per second: 141, episode reward: -63.454, mean reward: -0.947 [-100.000,  6.111], mean action: 1.552 [0.000, 3.000],  loss: 5.912547, mae: 32.396054, mean_q: 19.288541, mean_eps: 0.742768
  43020/150000: episode: 466, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -132.299, mean reward: -1.161 [-100.000, 11.881], mean action: 1.746 [0.000, 3.000],  loss: 5.883731, mae: 32.786618, mean_q: 20.295375, mean_eps: 0.742225
  43103/150000: episode: 467, duration: 0.548s, episode steps:  83, steps per second: 152, episode reward: -43.739, mean reward: -0.527 [-100.000, 12.850], mean action: 1.446 [0.000, 3.000],  loss: 5.589702, mae: 32.810340, mean_q: 20.796973, mean_eps: 0.741634
  43221/150000: episode: 468, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -150.322, mean reward: -1.274 [-100.000,  4.559], mean action: 1.636 [0.000, 3.000],  loss: 10.420708, mae: 32.903049, mean_q: 20.640541, mean_eps: 0.741031
  43313/150000: episode: 469, duration: 0.629s, episode steps:  92, steps per second: 146, episode reward: -106.986, mean reward: -1.163 [-100.000, 33.600], mean action: 1.620 [0.000, 3.000],  loss: 6.023368, mae: 32.526867, mean_q: 21.296508, mean_eps: 0.740401
  43400/150000: episode: 470, duration: 0.596s, episode steps:  87, steps per second: 146, episode reward: -87.891, mean reward: -1.010 [-100.000,  8.946], mean action: 1.471 [0.000, 3.000],  loss: 7.925484, mae: 32.837077, mean_q: 19.906266, mean_eps: 0.739864
  43505/150000: episode: 471, duration: 0.707s, episode steps: 105, steps per second: 148, episode reward: -35.807, mean reward: -0.341 [-100.000, 18.283], mean action: 1.657 [0.000, 3.000],  loss: 9.982200, mae: 32.705021, mean_q: 20.376066, mean_eps: 0.739288
  43570/150000: episode: 472, duration: 0.469s, episode steps:  65, steps per second: 139, episode reward: -129.081, mean reward: -1.986 [-100.000,  7.204], mean action: 1.585 [0.000, 3.000],  loss: 6.813015, mae: 33.051339, mean_q: 19.530237, mean_eps: 0.738778
  43629/150000: episode: 473, duration: 0.404s, episode steps:  59, steps per second: 146, episode reward: -50.088, mean reward: -0.849 [-100.000, 12.240], mean action: 1.525 [0.000, 3.000],  loss: 9.197358, mae: 32.778443, mean_q: 20.502710, mean_eps: 0.738406
  43744/150000: episode: 474, duration: 0.852s, episode steps: 115, steps per second: 135, episode reward: -61.723, mean reward: -0.537 [-100.000, 12.065], mean action: 1.461 [0.000, 3.000],  loss: 5.733152, mae: 32.949661, mean_q: 20.545690, mean_eps: 0.737884
  43904/150000: episode: 475, duration: 1.313s, episode steps: 160, steps per second: 122, episode reward: -111.214, mean reward: -0.695 [-100.000, 10.367], mean action: 1.444 [0.000, 3.000],  loss: 9.480790, mae: 32.847805, mean_q: 20.444382, mean_eps: 0.737059
  43990/150000: episode: 476, duration: 0.657s, episode steps:  86, steps per second: 131, episode reward: -90.810, mean reward: -1.056 [-100.000,  7.777], mean action: 1.535 [0.000, 3.000],  loss: 6.921609, mae: 32.455351, mean_q: 19.136260, mean_eps: 0.736321
  44082/150000: episode: 477, duration: 0.668s, episode steps:  92, steps per second: 138, episode reward: -64.031, mean reward: -0.696 [-100.000, 20.723], mean action: 1.543 [0.000, 3.000],  loss: 8.568198, mae: 33.374560, mean_q: 20.536567, mean_eps: 0.735787
  44196/150000: episode: 478, duration: 0.868s, episode steps: 114, steps per second: 131, episode reward: -110.253, mean reward: -0.967 [-100.000, 11.880], mean action: 1.395 [0.000, 3.000],  loss: 7.258040, mae: 33.127193, mean_q: 19.745953, mean_eps: 0.735169
  44284/150000: episode: 479, duration: 0.644s, episode steps:  88, steps per second: 137, episode reward: -60.533, mean reward: -0.688 [-100.000,  7.856], mean action: 1.568 [0.000, 3.000],  loss: 7.972269, mae: 32.650247, mean_q: 19.871130, mean_eps: 0.734563
  44366/150000: episode: 480, duration: 0.585s, episode steps:  82, steps per second: 140, episode reward: -78.301, mean reward: -0.955 [-100.000, 13.028], mean action: 1.585 [0.000, 3.000],  loss: 4.788377, mae: 33.223570, mean_q: 20.137521, mean_eps: 0.734053
  44434/150000: episode: 481, duration: 0.506s, episode steps:  68, steps per second: 134, episode reward: -49.972, mean reward: -0.735 [-100.000,  7.068], mean action: 1.412 [0.000, 3.000],  loss: 9.352428, mae: 33.139482, mean_q: 19.292596, mean_eps: 0.733603
  44514/150000: episode: 482, duration: 0.554s, episode steps:  80, steps per second: 144, episode reward: -27.953, mean reward: -0.349 [-100.000, 15.580], mean action: 1.325 [0.000, 3.000],  loss: 6.861582, mae: 33.649924, mean_q: 19.742610, mean_eps: 0.733159
  44603/150000: episode: 483, duration: 0.585s, episode steps:  89, steps per second: 152, episode reward: -69.632, mean reward: -0.782 [-100.000, 10.537], mean action: 1.753 [0.000, 3.000],  loss: 5.937384, mae: 32.831088, mean_q: 19.363054, mean_eps: 0.732652
  44709/150000: episode: 484, duration: 0.717s, episode steps: 106, steps per second: 148, episode reward: -66.825, mean reward: -0.630 [-100.000, 10.648], mean action: 1.519 [0.000, 3.000],  loss: 8.214511, mae: 33.492190, mean_q: 19.998093, mean_eps: 0.732067
  44819/150000: episode: 485, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -31.774, mean reward: -0.289 [-100.000, 12.404], mean action: 1.618 [0.000, 3.000],  loss: 5.992909, mae: 33.210313, mean_q: 19.274331, mean_eps: 0.731419
  44909/150000: episode: 486, duration: 0.591s, episode steps:  90, steps per second: 152, episode reward: -77.514, mean reward: -0.861 [-100.000, 10.434], mean action: 1.511 [0.000, 3.000],  loss: 6.060509, mae: 33.005041, mean_q: 18.886165, mean_eps: 0.730819
  45050/150000: episode: 487, duration: 0.974s, episode steps: 141, steps per second: 145, episode reward: -107.127, mean reward: -0.760 [-100.000,  7.325], mean action: 1.624 [0.000, 3.000],  loss: 7.080080, mae: 33.138426, mean_q: 20.649681, mean_eps: 0.730126
  45135/150000: episode: 488, duration: 0.580s, episode steps:  85, steps per second: 146, episode reward: -63.535, mean reward: -0.747 [-100.000, 13.447], mean action: 1.671 [0.000, 3.000],  loss: 5.974269, mae: 32.893354, mean_q: 19.975242, mean_eps: 0.729448
  45241/150000: episode: 489, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: -87.994, mean reward: -0.830 [-100.000, 13.781], mean action: 1.623 [0.000, 3.000],  loss: 4.578677, mae: 32.865906, mean_q: 19.107833, mean_eps: 0.728875
  45326/150000: episode: 490, duration: 0.601s, episode steps:  85, steps per second: 141, episode reward: -13.106, mean reward: -0.154 [-100.000, 12.669], mean action: 1.635 [0.000, 3.000],  loss: 9.729048, mae: 33.013880, mean_q: 19.541811, mean_eps: 0.728302
  45430/150000: episode: 491, duration: 0.734s, episode steps: 104, steps per second: 142, episode reward: -75.568, mean reward: -0.727 [-100.000,  9.280], mean action: 1.308 [0.000, 3.000],  loss: 6.918378, mae: 32.973044, mean_q: 19.092008, mean_eps: 0.727735
  45496/150000: episode: 492, duration: 0.445s, episode steps:  66, steps per second: 148, episode reward: -90.267, mean reward: -1.368 [-100.000, 10.447], mean action: 1.394 [0.000, 3.000],  loss: 6.139608, mae: 32.761367, mean_q: 20.541921, mean_eps: 0.727225
  45600/150000: episode: 493, duration: 0.688s, episode steps: 104, steps per second: 151, episode reward: -49.845, mean reward: -0.479 [-100.000, 13.993], mean action: 1.615 [0.000, 3.000],  loss: 9.092040, mae: 33.243156, mean_q: 19.194118, mean_eps: 0.726715
  45719/150000: episode: 494, duration: 0.831s, episode steps: 119, steps per second: 143, episode reward: -101.099, mean reward: -0.850 [-100.000, 19.481], mean action: 1.723 [0.000, 3.000],  loss: 9.624940, mae: 32.808121, mean_q: 18.892948, mean_eps: 0.726046
  45796/150000: episode: 495, duration: 0.519s, episode steps:  77, steps per second: 148, episode reward: -52.595, mean reward: -0.683 [-100.000, 10.074], mean action: 1.545 [0.000, 3.000],  loss: 9.026571, mae: 32.983285, mean_q: 18.789885, mean_eps: 0.725458
  45865/150000: episode: 496, duration: 0.457s, episode steps:  69, steps per second: 151, episode reward: -76.414, mean reward: -1.107 [-100.000, 10.017], mean action: 1.652 [0.000, 3.000],  loss: 8.611504, mae: 32.893142, mean_q: 20.403673, mean_eps: 0.725020
  45941/150000: episode: 497, duration: 0.529s, episode steps:  76, steps per second: 144, episode reward: -100.829, mean reward: -1.327 [-100.000, 11.381], mean action: 1.539 [0.000, 3.000],  loss: 7.936373, mae: 33.439732, mean_q: 18.784657, mean_eps: 0.724585
  46024/150000: episode: 498, duration: 0.603s, episode steps:  83, steps per second: 138, episode reward:  8.782, mean reward:  0.106 [-100.000, 120.814], mean action: 1.566 [0.000, 3.000],  loss: 5.845604, mae: 33.370996, mean_q: 18.092209, mean_eps: 0.724108
  46092/150000: episode: 499, duration: 0.468s, episode steps:  68, steps per second: 145, episode reward: -52.986, mean reward: -0.779 [-100.000,  7.400], mean action: 1.456 [0.000, 3.000],  loss: 6.206284, mae: 32.796259, mean_q: 17.886573, mean_eps: 0.723655
  46208/150000: episode: 500, duration: 0.772s, episode steps: 116, steps per second: 150, episode reward: -62.594, mean reward: -0.540 [-100.000, 11.226], mean action: 1.612 [0.000, 3.000],  loss: 9.960474, mae: 33.122800, mean_q: 18.967463, mean_eps: 0.723103
  46305/150000: episode: 501, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -103.802, mean reward: -1.070 [-100.000,  8.855], mean action: 1.680 [0.000, 3.000],  loss: 5.495162, mae: 33.009394, mean_q: 19.410094, mean_eps: 0.722464
  46442/150000: episode: 502, duration: 0.919s, episode steps: 137, steps per second: 149, episode reward: -79.032, mean reward: -0.577 [-100.000, 16.120], mean action: 1.620 [0.000, 3.000],  loss: 6.391685, mae: 33.210581, mean_q: 19.179801, mean_eps: 0.721762
  46536/150000: episode: 503, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -139.690, mean reward: -1.486 [-100.000, 10.529], mean action: 1.649 [0.000, 3.000],  loss: 7.155630, mae: 33.154487, mean_q: 18.373870, mean_eps: 0.721069
  46632/150000: episode: 504, duration: 0.670s, episode steps:  96, steps per second: 143, episode reward: -32.918, mean reward: -0.343 [-100.000,  8.374], mean action: 1.604 [0.000, 3.000],  loss: 3.955996, mae: 33.388277, mean_q: 19.017047, mean_eps: 0.720499
  46740/150000: episode: 505, duration: 0.732s, episode steps: 108, steps per second: 147, episode reward: -130.074, mean reward: -1.204 [-100.000, 10.469], mean action: 1.556 [0.000, 3.000],  loss: 9.183806, mae: 33.420114, mean_q: 18.932653, mean_eps: 0.719887
  46813/150000: episode: 506, duration: 0.488s, episode steps:  73, steps per second: 150, episode reward: -80.413, mean reward: -1.102 [-100.000,  7.901], mean action: 1.671 [0.000, 3.000],  loss: 6.174142, mae: 33.313051, mean_q: 19.750757, mean_eps: 0.719344
  46923/150000: episode: 507, duration: 0.754s, episode steps: 110, steps per second: 146, episode reward: -93.332, mean reward: -0.848 [-100.000, 10.344], mean action: 1.564 [0.000, 3.000],  loss: 10.650814, mae: 33.017713, mean_q: 19.288571, mean_eps: 0.718795
  47006/150000: episode: 508, duration: 0.583s, episode steps:  83, steps per second: 142, episode reward: -84.395, mean reward: -1.017 [-100.000,  8.906], mean action: 1.542 [0.000, 3.000],  loss: 11.599483, mae: 32.869937, mean_q: 17.317914, mean_eps: 0.718216
  47087/150000: episode: 509, duration: 0.540s, episode steps:  81, steps per second: 150, episode reward: -76.482, mean reward: -0.944 [-100.000, 12.094], mean action: 1.568 [0.000, 3.000],  loss: 7.639192, mae: 32.936887, mean_q: 19.808550, mean_eps: 0.717724
  47168/150000: episode: 510, duration: 0.548s, episode steps:  81, steps per second: 148, episode reward: -95.200, mean reward: -1.175 [-100.000, 10.778], mean action: 1.654 [0.000, 3.000],  loss: 8.824720, mae: 33.137394, mean_q: 20.103400, mean_eps: 0.717238
  47258/150000: episode: 511, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -130.387, mean reward: -1.449 [-100.000,  7.106], mean action: 1.489 [0.000, 3.000],  loss: 4.567222, mae: 33.302499, mean_q: 20.283869, mean_eps: 0.716725
  47340/150000: episode: 512, duration: 0.569s, episode steps:  82, steps per second: 144, episode reward: -66.525, mean reward: -0.811 [-100.000, 20.049], mean action: 1.756 [0.000, 3.000],  loss: 4.937275, mae: 33.569147, mean_q: 20.506982, mean_eps: 0.716209
  47444/150000: episode: 513, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -56.057, mean reward: -0.539 [-100.000,  8.274], mean action: 1.596 [0.000, 3.000],  loss: 5.681581, mae: 33.373263, mean_q: 20.209115, mean_eps: 0.715651
  47546/150000: episode: 514, duration: 0.696s, episode steps: 102, steps per second: 147, episode reward: -98.972, mean reward: -0.970 [-100.000, 10.900], mean action: 1.461 [0.000, 3.000],  loss: 7.559752, mae: 33.156245, mean_q: 20.851578, mean_eps: 0.715033
  47634/150000: episode: 515, duration: 0.613s, episode steps:  88, steps per second: 144, episode reward: -115.189, mean reward: -1.309 [-100.000,  6.511], mean action: 1.455 [0.000, 3.000],  loss: 6.945227, mae: 33.188907, mean_q: 20.111664, mean_eps: 0.714463
  47703/150000: episode: 516, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -50.480, mean reward: -0.732 [-100.000, 14.107], mean action: 1.652 [0.000, 3.000],  loss: 4.655087, mae: 33.363126, mean_q: 18.590262, mean_eps: 0.713992
  47811/150000: episode: 517, duration: 0.723s, episode steps: 108, steps per second: 149, episode reward: -89.929, mean reward: -0.833 [-100.000, 13.225], mean action: 1.676 [0.000, 3.000],  loss: 10.156859, mae: 33.279927, mean_q: 20.154879, mean_eps: 0.713461
  47900/150000: episode: 518, duration: 0.641s, episode steps:  89, steps per second: 139, episode reward: -85.850, mean reward: -0.965 [-100.000, 16.076], mean action: 1.528 [0.000, 3.000],  loss: 8.129365, mae: 33.491549, mean_q: 19.930695, mean_eps: 0.712870
  47980/150000: episode: 519, duration: 0.562s, episode steps:  80, steps per second: 142, episode reward: -36.065, mean reward: -0.451 [-100.000, 22.715], mean action: 1.812 [0.000, 3.000],  loss: 5.223819, mae: 32.987230, mean_q: 20.528307, mean_eps: 0.712363
  48102/150000: episode: 520, duration: 0.831s, episode steps: 122, steps per second: 147, episode reward: -84.719, mean reward: -0.694 [-100.000, 10.909], mean action: 1.426 [0.000, 3.000],  loss: 9.105728, mae: 33.162427, mean_q: 21.167879, mean_eps: 0.711757
  48183/150000: episode: 521, duration: 0.587s, episode steps:  81, steps per second: 138, episode reward:  0.154, mean reward:  0.002 [-100.000, 41.017], mean action: 1.877 [0.000, 3.000],  loss: 6.937239, mae: 33.310631, mean_q: 20.432835, mean_eps: 0.711148
  48289/150000: episode: 522, duration: 0.736s, episode steps: 106, steps per second: 144, episode reward: -83.908, mean reward: -0.792 [-100.000, 17.946], mean action: 1.519 [0.000, 3.000],  loss: 6.805354, mae: 33.514233, mean_q: 20.288960, mean_eps: 0.710587
  48405/150000: episode: 523, duration: 0.823s, episode steps: 116, steps per second: 141, episode reward: -68.586, mean reward: -0.591 [-100.000, 17.283], mean action: 1.672 [0.000, 3.000],  loss: 5.876519, mae: 33.552652, mean_q: 20.265074, mean_eps: 0.709921
  48509/150000: episode: 524, duration: 0.727s, episode steps: 104, steps per second: 143, episode reward: -52.523, mean reward: -0.505 [-100.000, 26.875], mean action: 1.740 [0.000, 3.000],  loss: 8.908086, mae: 33.489133, mean_q: 20.339846, mean_eps: 0.709261
  48611/150000: episode: 525, duration: 0.703s, episode steps: 102, steps per second: 145, episode reward: -142.431, mean reward: -1.396 [-100.000, 10.980], mean action: 1.716 [0.000, 3.000],  loss: 7.328302, mae: 33.909450, mean_q: 20.753625, mean_eps: 0.708643
  48675/150000: episode: 526, duration: 0.425s, episode steps:  64, steps per second: 151, episode reward: -66.741, mean reward: -1.043 [-100.000, 10.581], mean action: 1.719 [0.000, 3.000],  loss: 2.931695, mae: 33.026689, mean_q: 20.004853, mean_eps: 0.708145
  48774/150000: episode: 527, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -58.331, mean reward: -0.589 [-100.000, 16.949], mean action: 1.828 [0.000, 3.000],  loss: 7.046735, mae: 33.209940, mean_q: 19.915617, mean_eps: 0.707656
  48871/150000: episode: 528, duration: 0.717s, episode steps:  97, steps per second: 135, episode reward: -133.853, mean reward: -1.380 [-100.000, 10.046], mean action: 1.557 [0.000, 3.000],  loss: 7.240093, mae: 33.496199, mean_q: 20.102287, mean_eps: 0.707068
  48942/150000: episode: 529, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -64.921, mean reward: -0.914 [-100.000, 10.899], mean action: 1.676 [0.000, 3.000],  loss: 5.636192, mae: 33.751275, mean_q: 20.165463, mean_eps: 0.706564
  49040/150000: episode: 530, duration: 0.657s, episode steps:  98, steps per second: 149, episode reward: -60.532, mean reward: -0.618 [-100.000, 39.513], mean action: 1.867 [0.000, 3.000],  loss: 5.426255, mae: 33.473598, mean_q: 20.104794, mean_eps: 0.706057
  49123/150000: episode: 531, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -113.671, mean reward: -1.370 [-100.000,  9.214], mean action: 1.494 [0.000, 3.000],  loss: 4.789377, mae: 33.243107, mean_q: 21.012272, mean_eps: 0.705514
  49237/150000: episode: 532, duration: 0.822s, episode steps: 114, steps per second: 139, episode reward: -55.034, mean reward: -0.483 [-100.000,  8.865], mean action: 1.596 [0.000, 3.000],  loss: 5.396751, mae: 33.355447, mean_q: 20.373609, mean_eps: 0.704923
  49328/150000: episode: 533, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: -62.291, mean reward: -0.685 [-100.000,  7.788], mean action: 1.495 [0.000, 3.000],  loss: 6.889116, mae: 33.818113, mean_q: 20.190117, mean_eps: 0.704308
  49415/150000: episode: 534, duration: 0.588s, episode steps:  87, steps per second: 148, episode reward: -102.901, mean reward: -1.183 [-100.000, 11.870], mean action: 1.540 [0.000, 3.000],  loss: 4.922357, mae: 33.887600, mean_q: 19.911374, mean_eps: 0.703774
  49544/150000: episode: 535, duration: 0.923s, episode steps: 129, steps per second: 140, episode reward: -40.697, mean reward: -0.315 [-100.000, 20.668], mean action: 1.729 [0.000, 3.000],  loss: 6.254702, mae: 33.635881, mean_q: 19.549952, mean_eps: 0.703126
  49661/150000: episode: 536, duration: 0.778s, episode steps: 117, steps per second: 150, episode reward: -111.982, mean reward: -0.957 [-100.000,  6.219], mean action: 1.479 [0.000, 3.000],  loss: 5.965131, mae: 33.369841, mean_q: 21.112045, mean_eps: 0.702388
  49762/150000: episode: 537, duration: 0.695s, episode steps: 101, steps per second: 145, episode reward: -54.678, mean reward: -0.541 [-100.000, 17.432], mean action: 1.465 [0.000, 3.000],  loss: 8.849202, mae: 33.705648, mean_q: 20.152445, mean_eps: 0.701734
  49859/150000: episode: 538, duration: 0.682s, episode steps:  97, steps per second: 142, episode reward: -42.233, mean reward: -0.435 [-100.000, 28.427], mean action: 1.670 [0.000, 3.000],  loss: 4.937115, mae: 33.188009, mean_q: 20.950075, mean_eps: 0.701140
  49935/150000: episode: 539, duration: 0.512s, episode steps:  76, steps per second: 149, episode reward: -23.673, mean reward: -0.311 [-100.000, 11.473], mean action: 1.816 [0.000, 3.000],  loss: 6.334548, mae: 33.070709, mean_q: 20.027848, mean_eps: 0.700621
  50004/150000: episode: 540, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -76.432, mean reward: -1.108 [-100.000,  7.437], mean action: 1.478 [0.000, 3.000],  loss: 6.813707, mae: 33.478699, mean_q: 19.608808, mean_eps: 0.700186
  50098/150000: episode: 541, duration: 0.652s, episode steps:  94, steps per second: 144, episode reward: -107.282, mean reward: -1.141 [-100.000,  6.430], mean action: 1.457 [0.000, 3.000],  loss: 8.137921, mae: 33.565540, mean_q: 20.661531, mean_eps: 0.699697
  50195/150000: episode: 542, duration: 0.673s, episode steps:  97, steps per second: 144, episode reward: -94.121, mean reward: -0.970 [-100.000, 22.178], mean action: 1.433 [0.000, 3.000],  loss: 6.989102, mae: 33.803553, mean_q: 20.354395, mean_eps: 0.699124
  50324/150000: episode: 543, duration: 0.871s, episode steps: 129, steps per second: 148, episode reward: -29.174, mean reward: -0.226 [-100.000, 22.271], mean action: 1.868 [0.000, 3.000],  loss: 7.513691, mae: 33.749313, mean_q: 20.900645, mean_eps: 0.698446
  50442/150000: episode: 544, duration: 0.830s, episode steps: 118, steps per second: 142, episode reward: -83.615, mean reward: -0.709 [-100.000, 15.829], mean action: 1.542 [0.000, 3.000],  loss: 7.240811, mae: 33.845057, mean_q: 21.272572, mean_eps: 0.697705
  50560/150000: episode: 545, duration: 0.810s, episode steps: 118, steps per second: 146, episode reward: -153.250, mean reward: -1.299 [-100.000, 10.571], mean action: 1.551 [0.000, 3.000],  loss: 5.263118, mae: 33.565252, mean_q: 21.866387, mean_eps: 0.696997
  50655/150000: episode: 546, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -86.695, mean reward: -0.913 [-100.000,  6.764], mean action: 1.684 [0.000, 3.000],  loss: 5.015145, mae: 33.279986, mean_q: 20.521668, mean_eps: 0.696358
  50751/150000: episode: 547, duration: 0.681s, episode steps:  96, steps per second: 141, episode reward: -106.419, mean reward: -1.109 [-100.000, 15.261], mean action: 1.375 [0.000, 3.000],  loss: 5.898320, mae: 33.909001, mean_q: 20.465735, mean_eps: 0.695785
  50858/150000: episode: 548, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -67.228, mean reward: -0.628 [-100.000, 11.775], mean action: 1.505 [0.000, 3.000],  loss: 9.287596, mae: 33.220127, mean_q: 20.833539, mean_eps: 0.695176
  50959/150000: episode: 549, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: -89.596, mean reward: -0.887 [-100.000, 12.506], mean action: 1.713 [0.000, 3.000],  loss: 6.221144, mae: 33.826143, mean_q: 19.860324, mean_eps: 0.694552
  51034/150000: episode: 550, duration: 0.610s, episode steps:  75, steps per second: 123, episode reward: -47.462, mean reward: -0.633 [-100.000, 10.876], mean action: 1.893 [0.000, 3.000],  loss: 7.134578, mae: 33.369643, mean_q: 20.571568, mean_eps: 0.694024
  51165/150000: episode: 551, duration: 0.913s, episode steps: 131, steps per second: 143, episode reward: -6.705, mean reward: -0.051 [-100.000, 18.353], mean action: 1.626 [0.000, 3.000],  loss: 10.128694, mae: 33.800230, mean_q: 19.291645, mean_eps: 0.693406
  51259/150000: episode: 552, duration: 0.636s, episode steps:  94, steps per second: 148, episode reward: -84.259, mean reward: -0.896 [-100.000, 10.756], mean action: 1.436 [0.000, 3.000],  loss: 4.409045, mae: 33.406114, mean_q: 18.824204, mean_eps: 0.692731
  51356/150000: episode: 553, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -60.988, mean reward: -0.629 [-100.000, 10.031], mean action: 1.722 [0.000, 3.000],  loss: 7.028455, mae: 33.544851, mean_q: 19.001192, mean_eps: 0.692158
  51453/150000: episode: 554, duration: 0.662s, episode steps:  97, steps per second: 147, episode reward: -102.640, mean reward: -1.058 [-100.000,  7.283], mean action: 1.711 [0.000, 3.000],  loss: 8.797604, mae: 33.674250, mean_q: 19.618052, mean_eps: 0.691576
  51571/150000: episode: 555, duration: 0.803s, episode steps: 118, steps per second: 147, episode reward: -38.603, mean reward: -0.327 [-100.000, 11.648], mean action: 1.576 [0.000, 3.000],  loss: 7.649442, mae: 33.558434, mean_q: 19.865572, mean_eps: 0.690931
  51635/150000: episode: 556, duration: 0.459s, episode steps:  64, steps per second: 140, episode reward: -143.989, mean reward: -2.250 [-100.000,  6.177], mean action: 1.250 [0.000, 3.000],  loss: 8.879874, mae: 33.929300, mean_q: 19.121085, mean_eps: 0.690385
  51750/150000: episode: 557, duration: 0.785s, episode steps: 115, steps per second: 147, episode reward: -86.859, mean reward: -0.755 [-100.000, 11.685], mean action: 1.478 [0.000, 3.000],  loss: 5.028843, mae: 33.639879, mean_q: 20.227063, mean_eps: 0.689848
  51867/150000: episode: 558, duration: 0.801s, episode steps: 117, steps per second: 146, episode reward: -102.113, mean reward: -0.873 [-100.000,  7.083], mean action: 1.658 [0.000, 3.000],  loss: 10.708431, mae: 33.597943, mean_q: 18.527634, mean_eps: 0.689152
  51957/150000: episode: 559, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -39.607, mean reward: -0.440 [-100.000, 47.027], mean action: 1.567 [0.000, 3.000],  loss: 9.319432, mae: 33.923810, mean_q: 20.009073, mean_eps: 0.688531
  52039/150000: episode: 560, duration: 0.565s, episode steps:  82, steps per second: 145, episode reward: -57.449, mean reward: -0.701 [-100.000,  7.148], mean action: 1.549 [0.000, 3.000],  loss: 5.137671, mae: 33.363244, mean_q: 19.398048, mean_eps: 0.688015
  52131/150000: episode: 561, duration: 0.619s, episode steps:  92, steps per second: 149, episode reward: -85.203, mean reward: -0.926 [-100.000,  5.892], mean action: 1.761 [0.000, 3.000],  loss: 4.235358, mae: 33.676686, mean_q: 19.581904, mean_eps: 0.687493
  52271/150000: episode: 562, duration: 0.986s, episode steps: 140, steps per second: 142, episode reward: -15.663, mean reward: -0.112 [-100.000, 13.084], mean action: 1.414 [0.000, 3.000],  loss: 7.196080, mae: 33.470519, mean_q: 18.901939, mean_eps: 0.686797
  52390/150000: episode: 563, duration: 0.808s, episode steps: 119, steps per second: 147, episode reward: -151.946, mean reward: -1.277 [-100.000,  5.852], mean action: 1.563 [0.000, 3.000],  loss: 4.812533, mae: 33.955571, mean_q: 19.837192, mean_eps: 0.686020
  52514/150000: episode: 564, duration: 0.832s, episode steps: 124, steps per second: 149, episode reward: -102.550, mean reward: -0.827 [-100.000,  6.948], mean action: 1.589 [0.000, 3.000],  loss: 6.052687, mae: 33.567792, mean_q: 19.782713, mean_eps: 0.685291
  52597/150000: episode: 565, duration: 0.603s, episode steps:  83, steps per second: 138, episode reward: -91.980, mean reward: -1.108 [-100.000, 12.858], mean action: 1.494 [0.000, 3.000],  loss: 7.386704, mae: 33.476703, mean_q: 19.794533, mean_eps: 0.684670
  52689/150000: episode: 566, duration: 0.628s, episode steps:  92, steps per second: 146, episode reward: -25.008, mean reward: -0.272 [-100.000, 17.832], mean action: 1.652 [0.000, 3.000],  loss: 8.656379, mae: 33.444216, mean_q: 20.143057, mean_eps: 0.684145
  52768/150000: episode: 567, duration: 0.561s, episode steps:  79, steps per second: 141, episode reward: -41.372, mean reward: -0.524 [-100.000,  9.094], mean action: 1.734 [0.000, 3.000],  loss: 4.728429, mae: 33.921241, mean_q: 20.650402, mean_eps: 0.683632
  52866/150000: episode: 568, duration: 0.697s, episode steps:  98, steps per second: 141, episode reward: -74.096, mean reward: -0.756 [-100.000, 12.528], mean action: 1.786 [0.000, 3.000],  loss: 5.944958, mae: 33.749324, mean_q: 20.471800, mean_eps: 0.683101
  52964/150000: episode: 569, duration: 0.675s, episode steps:  98, steps per second: 145, episode reward: -53.840, mean reward: -0.549 [-100.000, 18.759], mean action: 1.755 [0.000, 3.000],  loss: 5.888999, mae: 33.494484, mean_q: 19.416169, mean_eps: 0.682513
  53083/150000: episode: 570, duration: 0.794s, episode steps: 119, steps per second: 150, episode reward: -114.489, mean reward: -0.962 [-100.000, 11.242], mean action: 1.647 [0.000, 3.000],  loss: 5.576333, mae: 33.795159, mean_q: 19.980019, mean_eps: 0.681862
  53155/150000: episode: 571, duration: 0.535s, episode steps:  72, steps per second: 135, episode reward: -56.826, mean reward: -0.789 [-100.000,  8.211], mean action: 1.514 [0.000, 3.000],  loss: 3.811608, mae: 33.427821, mean_q: 20.040351, mean_eps: 0.681289
  53267/150000: episode: 572, duration: 0.945s, episode steps: 112, steps per second: 118, episode reward: -59.400, mean reward: -0.530 [-100.000, 15.510], mean action: 1.482 [0.000, 3.000],  loss: 5.594029, mae: 33.395960, mean_q: 19.314433, mean_eps: 0.680737
  53348/150000: episode: 573, duration: 0.634s, episode steps:  81, steps per second: 128, episode reward: -119.538, mean reward: -1.476 [-100.000, 10.449], mean action: 1.469 [0.000, 3.000],  loss: 10.408171, mae: 33.682089, mean_q: 19.499982, mean_eps: 0.680158
  53460/150000: episode: 574, duration: 0.877s, episode steps: 112, steps per second: 128, episode reward: -76.447, mean reward: -0.683 [-100.000,  9.638], mean action: 1.491 [0.000, 3.000],  loss: 6.992164, mae: 33.704069, mean_q: 19.691640, mean_eps: 0.679579
  53561/150000: episode: 575, duration: 0.847s, episode steps: 101, steps per second: 119, episode reward: -99.703, mean reward: -0.987 [-100.000,  5.732], mean action: 1.822 [0.000, 3.000],  loss: 5.941418, mae: 33.225642, mean_q: 20.424473, mean_eps: 0.678940
  53657/150000: episode: 576, duration: 0.779s, episode steps:  96, steps per second: 123, episode reward: -98.455, mean reward: -1.026 [-100.000, 10.564], mean action: 1.594 [0.000, 3.000],  loss: 7.412554, mae: 34.047202, mean_q: 20.011757, mean_eps: 0.678349
  53732/150000: episode: 577, duration: 0.669s, episode steps:  75, steps per second: 112, episode reward: -45.636, mean reward: -0.608 [-100.000, 12.769], mean action: 1.507 [0.000, 3.000],  loss: 4.262622, mae: 33.871964, mean_q: 20.780944, mean_eps: 0.677836
  53816/150000: episode: 578, duration: 0.590s, episode steps:  84, steps per second: 142, episode reward: -21.752, mean reward: -0.259 [-100.000, 17.104], mean action: 1.643 [0.000, 3.000],  loss: 8.515072, mae: 33.722496, mean_q: 19.382689, mean_eps: 0.677359
  53920/150000: episode: 579, duration: 0.701s, episode steps: 104, steps per second: 148, episode reward: -45.806, mean reward: -0.440 [-100.000, 13.315], mean action: 1.663 [0.000, 3.000],  loss: 5.334314, mae: 33.871264, mean_q: 21.374811, mean_eps: 0.676795
  54008/150000: episode: 580, duration: 0.633s, episode steps:  88, steps per second: 139, episode reward: -76.860, mean reward: -0.873 [-100.000,  8.202], mean action: 1.500 [0.000, 3.000],  loss: 7.578843, mae: 33.789845, mean_q: 19.966911, mean_eps: 0.676219
  54133/150000: episode: 581, duration: 0.846s, episode steps: 125, steps per second: 148, episode reward: -57.337, mean reward: -0.459 [-100.000,  7.230], mean action: 1.416 [0.000, 3.000],  loss: 7.051161, mae: 33.719210, mean_q: 20.790442, mean_eps: 0.675580
  54204/150000: episode: 582, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -125.941, mean reward: -1.774 [-100.000,  8.821], mean action: 1.563 [0.000, 3.000],  loss: 6.045807, mae: 34.239129, mean_q: 20.820702, mean_eps: 0.674992
  54291/150000: episode: 583, duration: 0.641s, episode steps:  87, steps per second: 136, episode reward: -106.943, mean reward: -1.229 [-100.000, 15.776], mean action: 1.448 [0.000, 3.000],  loss: 7.314054, mae: 33.962470, mean_q: 20.976435, mean_eps: 0.674518
  54424/150000: episode: 584, duration: 1.170s, episode steps: 133, steps per second: 114, episode reward: -60.592, mean reward: -0.456 [-100.000, 19.111], mean action: 1.684 [0.000, 3.000],  loss: 5.922913, mae: 33.515897, mean_q: 20.900988, mean_eps: 0.673858
  54530/150000: episode: 585, duration: 0.852s, episode steps: 106, steps per second: 124, episode reward: -80.513, mean reward: -0.760 [-100.000, 18.772], mean action: 1.623 [0.000, 3.000],  loss: 7.913945, mae: 33.525383, mean_q: 19.631906, mean_eps: 0.673141
  54622/150000: episode: 586, duration: 0.684s, episode steps:  92, steps per second: 135, episode reward: -141.908, mean reward: -1.542 [-100.000,  7.984], mean action: 1.457 [0.000, 3.000],  loss: 5.524576, mae: 34.098807, mean_q: 20.620034, mean_eps: 0.672547
  54732/150000: episode: 587, duration: 0.755s, episode steps: 110, steps per second: 146, episode reward: -113.407, mean reward: -1.031 [-100.000, 10.037], mean action: 1.418 [0.000, 3.000],  loss: 5.564389, mae: 33.873680, mean_q: 20.425891, mean_eps: 0.671941
  54801/150000: episode: 588, duration: 0.490s, episode steps:  69, steps per second: 141, episode reward: -56.532, mean reward: -0.819 [-100.000, 19.607], mean action: 1.464 [0.000, 3.000],  loss: 6.026823, mae: 34.021450, mean_q: 20.420641, mean_eps: 0.671404
  54920/150000: episode: 589, duration: 0.855s, episode steps: 119, steps per second: 139, episode reward: -66.968, mean reward: -0.563 [-100.000, 16.679], mean action: 1.471 [0.000, 3.000],  loss: 8.056402, mae: 33.988142, mean_q: 19.683870, mean_eps: 0.670840
  55008/150000: episode: 590, duration: 0.617s, episode steps:  88, steps per second: 143, episode reward: -78.507, mean reward: -0.892 [-100.000, 15.807], mean action: 1.727 [0.000, 3.000],  loss: 3.238982, mae: 33.807907, mean_q: 19.483316, mean_eps: 0.670219
  55109/150000: episode: 591, duration: 0.704s, episode steps: 101, steps per second: 143, episode reward: -79.182, mean reward: -0.784 [-100.000, 16.196], mean action: 1.525 [0.000, 3.000],  loss: 4.633976, mae: 34.041289, mean_q: 18.911936, mean_eps: 0.669652
  55197/150000: episode: 592, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -111.530, mean reward: -1.267 [-100.000,  6.097], mean action: 1.568 [0.000, 3.000],  loss: 10.450600, mae: 34.383733, mean_q: 19.973418, mean_eps: 0.669085
  55322/150000: episode: 593, duration: 0.866s, episode steps: 125, steps per second: 144, episode reward: -67.106, mean reward: -0.537 [-100.000, 11.149], mean action: 1.672 [0.000, 3.000],  loss: 6.247831, mae: 34.008780, mean_q: 20.309381, mean_eps: 0.668446
  55444/150000: episode: 594, duration: 0.813s, episode steps: 122, steps per second: 150, episode reward:  4.226, mean reward:  0.035 [-100.000, 13.029], mean action: 1.566 [0.000, 3.000],  loss: 6.732795, mae: 34.059885, mean_q: 19.136424, mean_eps: 0.667705
  55545/150000: episode: 595, duration: 0.718s, episode steps: 101, steps per second: 141, episode reward: -108.719, mean reward: -1.076 [-100.000, 10.721], mean action: 1.535 [0.000, 3.000],  loss: 5.775786, mae: 34.003286, mean_q: 21.074181, mean_eps: 0.667036
  55618/150000: episode: 596, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -74.322, mean reward: -1.018 [-100.000,  6.137], mean action: 1.466 [0.000, 3.000],  loss: 4.992118, mae: 34.210089, mean_q: 19.891237, mean_eps: 0.666514
  55703/150000: episode: 597, duration: 0.576s, episode steps:  85, steps per second: 148, episode reward: -58.384, mean reward: -0.687 [-100.000,  6.992], mean action: 1.424 [0.000, 3.000],  loss: 4.847952, mae: 34.122183, mean_q: 18.371074, mean_eps: 0.666040
  55808/150000: episode: 598, duration: 0.725s, episode steps: 105, steps per second: 145, episode reward: -114.953, mean reward: -1.095 [-100.000,  8.537], mean action: 1.438 [0.000, 3.000],  loss: 4.878800, mae: 34.133659, mean_q: 18.954863, mean_eps: 0.665470
  55950/150000: episode: 599, duration: 0.995s, episode steps: 142, steps per second: 143, episode reward: -50.498, mean reward: -0.356 [-100.000, 13.305], mean action: 1.711 [0.000, 3.000],  loss: 6.507764, mae: 33.933593, mean_q: 20.394965, mean_eps: 0.664729
  56061/150000: episode: 600, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -80.389, mean reward: -0.724 [-100.000,  9.853], mean action: 1.703 [0.000, 3.000],  loss: 6.003672, mae: 34.287419, mean_q: 20.736802, mean_eps: 0.663970
  56162/150000: episode: 601, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -97.929, mean reward: -0.970 [-100.000,  8.623], mean action: 1.535 [0.000, 3.000],  loss: 5.892475, mae: 34.088233, mean_q: 20.282081, mean_eps: 0.663334
  56315/150000: episode: 602, duration: 1.033s, episode steps: 153, steps per second: 148, episode reward: -93.543, mean reward: -0.611 [-100.000,  6.332], mean action: 1.621 [0.000, 3.000],  loss: 3.740415, mae: 34.368440, mean_q: 20.626225, mean_eps: 0.662572
  56385/150000: episode: 603, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: -52.840, mean reward: -0.755 [-100.000,  7.536], mean action: 1.529 [0.000, 3.000],  loss: 6.656526, mae: 34.377232, mean_q: 19.818590, mean_eps: 0.661903
  56537/150000: episode: 604, duration: 1.057s, episode steps: 152, steps per second: 144, episode reward: -86.210, mean reward: -0.567 [-100.000,  6.081], mean action: 1.664 [0.000, 3.000],  loss: 8.395933, mae: 34.420101, mean_q: 19.376641, mean_eps: 0.661237
  56610/150000: episode: 605, duration: 0.507s, episode steps:  73, steps per second: 144, episode reward: -40.980, mean reward: -0.561 [-100.000,  8.202], mean action: 1.562 [0.000, 3.000],  loss: 7.713591, mae: 34.500465, mean_q: 19.999662, mean_eps: 0.660562
  56691/150000: episode: 606, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -63.248, mean reward: -0.781 [-100.000,  6.910], mean action: 1.543 [0.000, 3.000],  loss: 4.286816, mae: 34.489568, mean_q: 19.924422, mean_eps: 0.660100
  56792/150000: episode: 607, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: -54.695, mean reward: -0.542 [-100.000, 14.834], mean action: 1.614 [0.000, 3.000],  loss: 6.599880, mae: 34.263492, mean_q: 19.333870, mean_eps: 0.659554
  56948/150000: episode: 608, duration: 1.104s, episode steps: 156, steps per second: 141, episode reward: -47.830, mean reward: -0.307 [-100.000,  8.410], mean action: 1.833 [0.000, 3.000],  loss: 5.417301, mae: 34.299618, mean_q: 20.695863, mean_eps: 0.658783
  57041/150000: episode: 609, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: 16.083, mean reward:  0.173 [-100.000, 12.858], mean action: 1.602 [0.000, 3.000],  loss: 4.967083, mae: 34.201913, mean_q: 20.321308, mean_eps: 0.658036
  57141/150000: episode: 610, duration: 0.694s, episode steps: 100, steps per second: 144, episode reward: -67.699, mean reward: -0.677 [-100.000, 22.509], mean action: 1.650 [0.000, 3.000],  loss: 6.706931, mae: 34.437203, mean_q: 18.912294, mean_eps: 0.657457
  57291/150000: episode: 611, duration: 1.002s, episode steps: 150, steps per second: 150, episode reward: -240.447, mean reward: -1.603 [-100.000, 25.560], mean action: 1.447 [0.000, 3.000],  loss: 5.803976, mae: 33.859607, mean_q: 19.865143, mean_eps: 0.656707
  57428/150000: episode: 612, duration: 0.982s, episode steps: 137, steps per second: 140, episode reward: -91.274, mean reward: -0.666 [-100.000, 10.310], mean action: 1.708 [0.000, 3.000],  loss: 7.221682, mae: 34.414675, mean_q: 19.608450, mean_eps: 0.655846
  57527/150000: episode: 613, duration: 0.670s, episode steps:  99, steps per second: 148, episode reward: -101.215, mean reward: -1.022 [-100.000,  9.722], mean action: 1.576 [0.000, 3.000],  loss: 7.453058, mae: 34.700111, mean_q: 19.973006, mean_eps: 0.655138
  57658/150000: episode: 614, duration: 0.884s, episode steps: 131, steps per second: 148, episode reward: -28.550, mean reward: -0.218 [-100.000, 12.534], mean action: 1.702 [0.000, 3.000],  loss: 6.521541, mae: 34.314649, mean_q: 18.228117, mean_eps: 0.654448
  57773/150000: episode: 615, duration: 0.810s, episode steps: 115, steps per second: 142, episode reward: -55.714, mean reward: -0.484 [-100.000,  6.808], mean action: 1.383 [0.000, 3.000],  loss: 4.006840, mae: 34.164292, mean_q: 19.731641, mean_eps: 0.653710
  57874/150000: episode: 616, duration: 0.676s, episode steps: 101, steps per second: 149, episode reward: -37.194, mean reward: -0.368 [-100.000,  9.831], mean action: 1.891 [0.000, 3.000],  loss: 6.356189, mae: 34.619933, mean_q: 19.930195, mean_eps: 0.653062
  57991/150000: episode: 617, duration: 0.810s, episode steps: 117, steps per second: 144, episode reward: -33.348, mean reward: -0.285 [-100.000, 11.520], mean action: 1.547 [0.000, 3.000],  loss: 4.194417, mae: 34.145467, mean_q: 19.129543, mean_eps: 0.652408
  58076/150000: episode: 618, duration: 0.603s, episode steps:  85, steps per second: 141, episode reward: -52.804, mean reward: -0.621 [-100.000, 10.487], mean action: 1.776 [0.000, 3.000],  loss: 7.748186, mae: 34.435793, mean_q: 18.742906, mean_eps: 0.651802
  58155/150000: episode: 619, duration: 0.540s, episode steps:  79, steps per second: 146, episode reward: -32.135, mean reward: -0.407 [-100.000,  6.477], mean action: 1.835 [0.000, 3.000],  loss: 6.307708, mae: 34.571487, mean_q: 19.375634, mean_eps: 0.651310
  58229/150000: episode: 620, duration: 0.497s, episode steps:  74, steps per second: 149, episode reward: -56.940, mean reward: -0.769 [-100.000,  7.810], mean action: 1.676 [0.000, 3.000],  loss: 4.413882, mae: 33.967141, mean_q: 18.377313, mean_eps: 0.650851
  58320/150000: episode: 621, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -42.825, mean reward: -0.471 [-100.000,  6.705], mean action: 1.692 [0.000, 3.000],  loss: 7.273078, mae: 34.297149, mean_q: 19.613203, mean_eps: 0.650356
  58406/150000: episode: 622, duration: 0.617s, episode steps:  86, steps per second: 139, episode reward: -90.552, mean reward: -1.053 [-100.000,  7.533], mean action: 1.302 [0.000, 3.000],  loss: 6.621874, mae: 34.317586, mean_q: 20.609198, mean_eps: 0.649825
  58507/150000: episode: 623, duration: 0.806s, episode steps: 101, steps per second: 125, episode reward: -88.877, mean reward: -0.880 [-100.000, 11.601], mean action: 1.673 [0.000, 3.000],  loss: 5.448025, mae: 34.499647, mean_q: 19.959995, mean_eps: 0.649264
  58586/150000: episode: 624, duration: 0.609s, episode steps:  79, steps per second: 130, episode reward: -50.260, mean reward: -0.636 [-100.000,  6.768], mean action: 1.468 [0.000, 3.000],  loss: 6.495329, mae: 34.185804, mean_q: 20.121817, mean_eps: 0.648724
  58677/150000: episode: 625, duration: 0.658s, episode steps:  91, steps per second: 138, episode reward: -1.037, mean reward: -0.011 [-100.000, 18.506], mean action: 1.527 [0.000, 3.000],  loss: 6.818342, mae: 34.354140, mean_q: 18.896554, mean_eps: 0.648214
  58811/150000: episode: 626, duration: 0.917s, episode steps: 134, steps per second: 146, episode reward: -85.482, mean reward: -0.638 [-100.000,  7.133], mean action: 1.604 [0.000, 3.000],  loss: 7.444105, mae: 33.920972, mean_q: 19.418286, mean_eps: 0.647539
  58879/150000: episode: 627, duration: 0.456s, episode steps:  68, steps per second: 149, episode reward: -173.726, mean reward: -2.555 [-100.000,  8.649], mean action: 1.603 [0.000, 3.000],  loss: 6.850133, mae: 34.558313, mean_q: 18.691424, mean_eps: 0.646933
  58975/150000: episode: 628, duration: 0.683s, episode steps:  96, steps per second: 140, episode reward: -13.334, mean reward: -0.139 [-100.000, 19.502], mean action: 1.656 [0.000, 3.000],  loss: 7.979210, mae: 34.178937, mean_q: 19.434015, mean_eps: 0.646441
  59085/150000: episode: 629, duration: 0.753s, episode steps: 110, steps per second: 146, episode reward: -79.931, mean reward: -0.727 [-100.000,  9.136], mean action: 1.655 [0.000, 3.000],  loss: 11.908951, mae: 34.739056, mean_q: 19.121359, mean_eps: 0.645823
  59151/150000: episode: 630, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -67.188, mean reward: -1.018 [-100.000, 10.507], mean action: 1.530 [0.000, 3.000],  loss: 8.424400, mae: 34.529596, mean_q: 19.826889, mean_eps: 0.645295
  59227/150000: episode: 631, duration: 0.539s, episode steps:  76, steps per second: 141, episode reward: -64.434, mean reward: -0.848 [-100.000,  8.975], mean action: 1.368 [0.000, 3.000],  loss: 7.921813, mae: 34.579163, mean_q: 20.017507, mean_eps: 0.644869
  59321/150000: episode: 632, duration: 0.647s, episode steps:  94, steps per second: 145, episode reward: -112.295, mean reward: -1.195 [-100.000,  8.361], mean action: 1.819 [0.000, 3.000],  loss: 6.686746, mae: 34.203472, mean_q: 20.085188, mean_eps: 0.644359
  59434/150000: episode: 633, duration: 0.767s, episode steps: 113, steps per second: 147, episode reward: -49.406, mean reward: -0.437 [-100.000, 11.000], mean action: 1.558 [0.000, 3.000],  loss: 11.859434, mae: 34.354254, mean_q: 20.318120, mean_eps: 0.643738
  59521/150000: episode: 634, duration: 0.601s, episode steps:  87, steps per second: 145, episode reward: -30.780, mean reward: -0.354 [-100.000, 20.373], mean action: 1.609 [0.000, 3.000],  loss: 9.891578, mae: 33.991891, mean_q: 19.772446, mean_eps: 0.643138
  59596/150000: episode: 635, duration: 0.532s, episode steps:  75, steps per second: 141, episode reward: -48.874, mean reward: -0.652 [-100.000,  9.737], mean action: 1.600 [0.000, 3.000],  loss: 6.478784, mae: 34.387929, mean_q: 20.027574, mean_eps: 0.642652
  59693/150000: episode: 636, duration: 0.660s, episode steps:  97, steps per second: 147, episode reward: -42.993, mean reward: -0.443 [-100.000, 10.902], mean action: 1.742 [0.000, 3.000],  loss: 8.058281, mae: 34.152354, mean_q: 21.396451, mean_eps: 0.642136
  59788/150000: episode: 637, duration: 0.639s, episode steps:  95, steps per second: 149, episode reward: -154.011, mean reward: -1.621 [-100.000, 20.822], mean action: 1.611 [0.000, 3.000],  loss: 6.250321, mae: 34.283777, mean_q: 18.873258, mean_eps: 0.641560
  59892/150000: episode: 638, duration: 0.738s, episode steps: 104, steps per second: 141, episode reward: -82.380, mean reward: -0.792 [-100.000, 10.619], mean action: 1.452 [0.000, 3.000],  loss: 8.571310, mae: 34.810650, mean_q: 18.951843, mean_eps: 0.640963
  60024/150000: episode: 639, duration: 0.894s, episode steps: 132, steps per second: 148, episode reward: -107.509, mean reward: -0.814 [-100.000,  7.262], mean action: 1.788 [0.000, 3.000],  loss: 5.441905, mae: 34.136262, mean_q: 18.570224, mean_eps: 0.640255
  60135/150000: episode: 640, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward: -12.524, mean reward: -0.113 [-100.000, 21.809], mean action: 1.577 [0.000, 3.000],  loss: 9.387061, mae: 34.617944, mean_q: 20.222678, mean_eps: 0.639526
  60221/150000: episode: 641, duration: 0.641s, episode steps:  86, steps per second: 134, episode reward: -49.843, mean reward: -0.580 [-100.000,  9.449], mean action: 1.488 [0.000, 3.000],  loss: 6.346303, mae: 34.099555, mean_q: 19.907237, mean_eps: 0.638935
  60340/150000: episode: 642, duration: 0.803s, episode steps: 119, steps per second: 148, episode reward: -150.849, mean reward: -1.268 [-100.000, 11.264], mean action: 1.840 [0.000, 3.000],  loss: 10.729680, mae: 34.247815, mean_q: 19.123897, mean_eps: 0.638320
  60469/150000: episode: 643, duration: 0.873s, episode steps: 129, steps per second: 148, episode reward: -79.156, mean reward: -0.614 [-100.000,  6.668], mean action: 1.620 [0.000, 3.000],  loss: 8.032526, mae: 34.591223, mean_q: 18.909427, mean_eps: 0.637576
  60579/150000: episode: 644, duration: 0.790s, episode steps: 110, steps per second: 139, episode reward: -130.743, mean reward: -1.189 [-100.000,  3.967], mean action: 1.527 [0.000, 3.000],  loss: 4.421055, mae: 34.144685, mean_q: 18.508574, mean_eps: 0.636859
  60700/150000: episode: 645, duration: 0.814s, episode steps: 121, steps per second: 149, episode reward: 30.014, mean reward:  0.248 [-100.000, 59.733], mean action: 1.612 [0.000, 3.000],  loss: 9.360707, mae: 34.422966, mean_q: 20.083960, mean_eps: 0.636166
  60809/150000: episode: 646, duration: 0.771s, episode steps: 109, steps per second: 141, episode reward: -79.332, mean reward: -0.728 [-100.000,  7.260], mean action: 1.606 [0.000, 3.000],  loss: 6.643009, mae: 34.160375, mean_q: 19.197716, mean_eps: 0.635476
  60921/150000: episode: 647, duration: 0.778s, episode steps: 112, steps per second: 144, episode reward: -34.226, mean reward: -0.306 [-100.000, 13.322], mean action: 1.545 [0.000, 3.000],  loss: 8.205706, mae: 34.579063, mean_q: 18.646245, mean_eps: 0.634813
  61006/150000: episode: 648, duration: 0.573s, episode steps:  85, steps per second: 148, episode reward: -47.422, mean reward: -0.558 [-100.000,  7.385], mean action: 1.671 [0.000, 3.000],  loss: 5.772511, mae: 34.566939, mean_q: 18.032526, mean_eps: 0.634222
  61093/150000: episode: 649, duration: 0.628s, episode steps:  87, steps per second: 139, episode reward: -64.654, mean reward: -0.743 [-100.000, 13.786], mean action: 1.770 [0.000, 3.000],  loss: 7.072076, mae: 33.953010, mean_q: 18.574381, mean_eps: 0.633706
  61199/150000: episode: 650, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: -87.074, mean reward: -0.821 [-100.000, 21.084], mean action: 1.462 [0.000, 3.000],  loss: 6.187842, mae: 33.848505, mean_q: 18.106783, mean_eps: 0.633127
  61294/150000: episode: 651, duration: 0.633s, episode steps:  95, steps per second: 150, episode reward: 43.167, mean reward:  0.454 [-100.000, 15.974], mean action: 1.600 [0.000, 3.000],  loss: 6.339229, mae: 33.857428, mean_q: 18.686450, mean_eps: 0.632524
  61371/150000: episode: 652, duration: 0.518s, episode steps:  77, steps per second: 149, episode reward: -60.521, mean reward: -0.786 [-100.000,  9.755], mean action: 1.571 [0.000, 3.000],  loss: 4.897202, mae: 33.893928, mean_q: 18.523928, mean_eps: 0.632008
  61470/150000: episode: 653, duration: 0.752s, episode steps:  99, steps per second: 132, episode reward: -65.730, mean reward: -0.664 [-100.000, 10.697], mean action: 1.626 [0.000, 3.000],  loss: 9.479945, mae: 34.163269, mean_q: 17.642646, mean_eps: 0.631480
  61560/150000: episode: 654, duration: 0.621s, episode steps:  90, steps per second: 145, episode reward: -89.360, mean reward: -0.993 [-100.000, 17.571], mean action: 1.767 [0.000, 3.000],  loss: 7.094330, mae: 34.040008, mean_q: 18.442110, mean_eps: 0.630913
  61667/150000: episode: 655, duration: 0.733s, episode steps: 107, steps per second: 146, episode reward: -54.889, mean reward: -0.513 [-100.000, 23.534], mean action: 1.636 [0.000, 3.000],  loss: 7.122644, mae: 33.670268, mean_q: 18.785266, mean_eps: 0.630322
  61756/150000: episode: 656, duration: 0.663s, episode steps:  89, steps per second: 134, episode reward: -100.561, mean reward: -1.130 [-100.000,  7.002], mean action: 1.674 [0.000, 3.000],  loss: 12.802180, mae: 34.129701, mean_q: 19.404098, mean_eps: 0.629734
  61868/150000: episode: 657, duration: 0.777s, episode steps: 112, steps per second: 144, episode reward: 10.873, mean reward:  0.097 [-100.000, 17.308], mean action: 1.741 [0.000, 3.000],  loss: 8.778694, mae: 34.027816, mean_q: 19.840213, mean_eps: 0.629131
  61955/150000: episode: 658, duration: 0.582s, episode steps:  87, steps per second: 149, episode reward: -68.927, mean reward: -0.792 [-100.000, 22.911], mean action: 1.517 [0.000, 3.000],  loss: 4.882060, mae: 33.911149, mean_q: 18.471509, mean_eps: 0.628534
  62045/150000: episode: 659, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -94.006, mean reward: -1.045 [-100.000, 11.681], mean action: 1.622 [0.000, 3.000],  loss: 6.172145, mae: 34.317827, mean_q: 17.787802, mean_eps: 0.628003
  62105/150000: episode: 660, duration: 0.415s, episode steps:  60, steps per second: 145, episode reward: -81.733, mean reward: -1.362 [-100.000,  3.955], mean action: 1.683 [0.000, 3.000],  loss: 5.695239, mae: 34.338385, mean_q: 18.104178, mean_eps: 0.627553
  62262/150000: episode: 661, duration: 1.071s, episode steps: 157, steps per second: 147, episode reward: -95.314, mean reward: -0.607 [-100.000,  8.262], mean action: 1.720 [0.000, 3.000],  loss: 6.383680, mae: 34.574796, mean_q: 18.127456, mean_eps: 0.626902
  62325/150000: episode: 662, duration: 0.451s, episode steps:  63, steps per second: 140, episode reward: -60.855, mean reward: -0.966 [-100.000, 10.666], mean action: 1.683 [0.000, 3.000],  loss: 4.342902, mae: 34.264159, mean_q: 20.070298, mean_eps: 0.626242
  62434/150000: episode: 663, duration: 0.796s, episode steps: 109, steps per second: 137, episode reward: -26.388, mean reward: -0.242 [-100.000, 18.037], mean action: 1.642 [0.000, 3.000],  loss: 10.240203, mae: 34.549676, mean_q: 19.080561, mean_eps: 0.625726
  62541/150000: episode: 664, duration: 0.920s, episode steps: 107, steps per second: 116, episode reward: 49.871, mean reward:  0.466 [-100.000, 14.605], mean action: 1.654 [0.000, 3.000],  loss: 6.164603, mae: 34.748319, mean_q: 19.258470, mean_eps: 0.625078
  62657/150000: episode: 665, duration: 0.906s, episode steps: 116, steps per second: 128, episode reward: -84.663, mean reward: -0.730 [-100.000, 10.819], mean action: 1.647 [0.000, 3.000],  loss: 3.439838, mae: 34.592868, mean_q: 19.195699, mean_eps: 0.624409
  62726/150000: episode: 666, duration: 0.533s, episode steps:  69, steps per second: 129, episode reward: -64.136, mean reward: -0.930 [-100.000,  8.416], mean action: 1.580 [0.000, 3.000],  loss: 4.681256, mae: 34.509479, mean_q: 19.531085, mean_eps: 0.623854
  62847/150000: episode: 667, duration: 0.892s, episode steps: 121, steps per second: 136, episode reward: -83.052, mean reward: -0.686 [-100.000,  6.670], mean action: 1.504 [0.000, 3.000],  loss: 7.575097, mae: 34.695170, mean_q: 19.102174, mean_eps: 0.623284
  62924/150000: episode: 668, duration: 0.698s, episode steps:  77, steps per second: 110, episode reward: -23.271, mean reward: -0.302 [-100.000, 22.360], mean action: 1.727 [0.000, 3.000],  loss: 8.924383, mae: 34.915149, mean_q: 18.249594, mean_eps: 0.622690
  63030/150000: episode: 669, duration: 0.837s, episode steps: 106, steps per second: 127, episode reward: -79.674, mean reward: -0.752 [-100.000, 13.484], mean action: 1.698 [0.000, 3.000],  loss: 7.917097, mae: 34.739991, mean_q: 19.662564, mean_eps: 0.622141
  63145/150000: episode: 670, duration: 0.798s, episode steps: 115, steps per second: 144, episode reward: -74.307, mean reward: -0.646 [-100.000,  6.395], mean action: 1.661 [0.000, 3.000],  loss: 5.325526, mae: 35.113496, mean_q: 19.121845, mean_eps: 0.621478
  63227/150000: episode: 671, duration: 0.595s, episode steps:  82, steps per second: 138, episode reward: -23.556, mean reward: -0.287 [-100.000, 13.574], mean action: 1.671 [0.000, 3.000],  loss: 7.800311, mae: 34.524424, mean_q: 19.983227, mean_eps: 0.620887
  63318/150000: episode: 672, duration: 0.621s, episode steps:  91, steps per second: 147, episode reward: -12.475, mean reward: -0.137 [-100.000, 12.354], mean action: 1.758 [0.000, 3.000],  loss: 4.127451, mae: 35.087238, mean_q: 19.667471, mean_eps: 0.620368
  63427/150000: episode: 673, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -68.215, mean reward: -0.626 [-100.000, 13.209], mean action: 1.560 [0.000, 3.000],  loss: 7.997273, mae: 34.812765, mean_q: 18.215681, mean_eps: 0.619768
  63550/150000: episode: 674, duration: 0.868s, episode steps: 123, steps per second: 142, episode reward: -11.828, mean reward: -0.096 [-100.000, 16.708], mean action: 1.659 [0.000, 3.000],  loss: 4.127840, mae: 34.523863, mean_q: 18.345703, mean_eps: 0.619072
  63639/150000: episode: 675, duration: 0.609s, episode steps:  89, steps per second: 146, episode reward: -41.734, mean reward: -0.469 [-100.000, 13.773], mean action: 1.438 [0.000, 3.000],  loss: 5.959537, mae: 34.586038, mean_q: 20.143349, mean_eps: 0.618436
  63730/150000: episode: 676, duration: 0.660s, episode steps:  91, steps per second: 138, episode reward: -67.095, mean reward: -0.737 [-100.000, 10.808], mean action: 1.505 [0.000, 3.000],  loss: 7.065000, mae: 34.901721, mean_q: 19.475058, mean_eps: 0.617896
  63809/150000: episode: 677, duration: 0.583s, episode steps:  79, steps per second: 136, episode reward: -94.313, mean reward: -1.194 [-100.000, 13.706], mean action: 1.481 [0.000, 3.000],  loss: 6.720678, mae: 34.670010, mean_q: 19.540962, mean_eps: 0.617386
  63920/150000: episode: 678, duration: 0.810s, episode steps: 111, steps per second: 137, episode reward: -66.154, mean reward: -0.596 [-100.000,  7.840], mean action: 1.730 [0.000, 3.000],  loss: 7.122122, mae: 34.839386, mean_q: 19.742557, mean_eps: 0.616816
  64031/150000: episode: 679, duration: 0.774s, episode steps: 111, steps per second: 143, episode reward: -43.631, mean reward: -0.393 [-100.000,  7.149], mean action: 1.676 [0.000, 3.000],  loss: 5.005772, mae: 34.886401, mean_q: 18.685553, mean_eps: 0.616150
  64104/150000: episode: 680, duration: 0.531s, episode steps:  73, steps per second: 137, episode reward: -45.526, mean reward: -0.624 [-100.000, 20.705], mean action: 1.877 [0.000, 3.000],  loss: 3.267840, mae: 34.808286, mean_q: 18.918084, mean_eps: 0.615598
  64189/150000: episode: 681, duration: 0.586s, episode steps:  85, steps per second: 145, episode reward: -51.224, mean reward: -0.603 [-100.000, 13.419], mean action: 1.529 [0.000, 3.000],  loss: 5.147128, mae: 34.833677, mean_q: 20.052915, mean_eps: 0.615124
  64265/150000: episode: 682, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: -135.487, mean reward: -1.783 [-100.000,  8.448], mean action: 1.579 [0.000, 3.000],  loss: 5.655348, mae: 34.905649, mean_q: 19.236667, mean_eps: 0.614641
  64395/150000: episode: 683, duration: 0.932s, episode steps: 130, steps per second: 140, episode reward: -73.858, mean reward: -0.568 [-100.000,  9.116], mean action: 1.631 [0.000, 3.000],  loss: 6.832927, mae: 34.640267, mean_q: 18.825338, mean_eps: 0.614023
  64487/150000: episode: 684, duration: 0.648s, episode steps:  92, steps per second: 142, episode reward: -17.983, mean reward: -0.195 [-100.000, 22.608], mean action: 1.717 [0.000, 3.000],  loss: 5.138800, mae: 34.701540, mean_q: 18.651879, mean_eps: 0.613357
  64611/150000: episode: 685, duration: 0.825s, episode steps: 124, steps per second: 150, episode reward: -42.199, mean reward: -0.340 [-100.000, 21.428], mean action: 1.750 [0.000, 3.000],  loss: 4.953001, mae: 35.068424, mean_q: 20.060553, mean_eps: 0.612709
  64735/150000: episode: 686, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: -63.354, mean reward: -0.511 [-100.000, 12.476], mean action: 1.524 [0.000, 3.000],  loss: 10.491777, mae: 34.902869, mean_q: 18.779679, mean_eps: 0.611965
  64837/150000: episode: 687, duration: 0.698s, episode steps: 102, steps per second: 146, episode reward: -59.062, mean reward: -0.579 [-100.000,  9.415], mean action: 1.784 [0.000, 3.000],  loss: 9.290318, mae: 35.003243, mean_q: 20.294789, mean_eps: 0.611287
  64944/150000: episode: 688, duration: 0.708s, episode steps: 107, steps per second: 151, episode reward: -31.195, mean reward: -0.292 [-100.000, 10.336], mean action: 1.477 [0.000, 3.000],  loss: 4.100169, mae: 35.031604, mean_q: 17.999041, mean_eps: 0.610660
  65102/150000: episode: 689, duration: 1.118s, episode steps: 158, steps per second: 141, episode reward: -180.545, mean reward: -1.143 [-100.000, 12.424], mean action: 1.551 [0.000, 3.000],  loss: 4.690164, mae: 35.118007, mean_q: 19.337630, mean_eps: 0.609865
  65216/150000: episode: 690, duration: 0.770s, episode steps: 114, steps per second: 148, episode reward: -15.736, mean reward: -0.138 [-100.000, 12.065], mean action: 1.693 [0.000, 3.000],  loss: 4.601220, mae: 35.361693, mean_q: 20.039314, mean_eps: 0.609049
  65302/150000: episode: 691, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -92.925, mean reward: -1.081 [-100.000, 11.038], mean action: 1.651 [0.000, 3.000],  loss: 5.724815, mae: 35.136077, mean_q: 19.038065, mean_eps: 0.608449
  65380/150000: episode: 692, duration: 0.576s, episode steps:  78, steps per second: 135, episode reward: -85.553, mean reward: -1.097 [-100.000,  9.008], mean action: 1.718 [0.000, 3.000],  loss: 4.855089, mae: 35.064757, mean_q: 18.272292, mean_eps: 0.607957
  65474/150000: episode: 693, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -98.244, mean reward: -1.045 [-100.000,  5.999], mean action: 1.553 [0.000, 3.000],  loss: 4.031968, mae: 34.919962, mean_q: 18.799691, mean_eps: 0.607441
  65558/150000: episode: 694, duration: 0.559s, episode steps:  84, steps per second: 150, episode reward: 14.187, mean reward:  0.169 [-100.000, 14.521], mean action: 1.607 [0.000, 3.000],  loss: 7.592201, mae: 35.171119, mean_q: 19.304616, mean_eps: 0.606907
  65636/150000: episode: 695, duration: 0.545s, episode steps:  78, steps per second: 143, episode reward: -73.109, mean reward: -0.937 [-100.000, 12.845], mean action: 1.667 [0.000, 3.000],  loss: 4.147592, mae: 34.785584, mean_q: 19.045912, mean_eps: 0.606421
  65737/150000: episode: 696, duration: 0.755s, episode steps: 101, steps per second: 134, episode reward: -81.169, mean reward: -0.804 [-100.000, 12.633], mean action: 1.683 [0.000, 3.000],  loss: 7.361699, mae: 35.522094, mean_q: 19.588776, mean_eps: 0.605884
  65821/150000: episode: 697, duration: 0.573s, episode steps:  84, steps per second: 147, episode reward: -75.387, mean reward: -0.897 [-100.000, 12.325], mean action: 1.738 [0.000, 3.000],  loss: 7.806322, mae: 35.140761, mean_q: 19.573931, mean_eps: 0.605329
  65952/150000: episode: 698, duration: 0.889s, episode steps: 131, steps per second: 147, episode reward: -38.246, mean reward: -0.292 [-100.000,  7.038], mean action: 1.687 [0.000, 3.000],  loss: 4.180930, mae: 35.298612, mean_q: 19.370725, mean_eps: 0.604684
  66052/150000: episode: 699, duration: 0.741s, episode steps: 100, steps per second: 135, episode reward: -47.295, mean reward: -0.473 [-100.000,  7.144], mean action: 1.610 [0.000, 3.000],  loss: 4.890831, mae: 35.090493, mean_q: 19.362786, mean_eps: 0.603991
  66160/150000: episode: 700, duration: 0.757s, episode steps: 108, steps per second: 143, episode reward: -126.944, mean reward: -1.175 [-100.000,  6.156], mean action: 1.694 [0.000, 3.000],  loss: 4.452780, mae: 35.492556, mean_q: 19.183860, mean_eps: 0.603367
  66263/150000: episode: 701, duration: 0.732s, episode steps: 103, steps per second: 141, episode reward: -47.740, mean reward: -0.463 [-100.000, 13.416], mean action: 1.806 [0.000, 3.000],  loss: 5.543384, mae: 35.484907, mean_q: 20.449976, mean_eps: 0.602734
  66351/150000: episode: 702, duration: 0.640s, episode steps:  88, steps per second: 138, episode reward: -62.396, mean reward: -0.709 [-100.000, 10.704], mean action: 1.443 [0.000, 3.000],  loss: 7.719386, mae: 35.825962, mean_q: 20.574966, mean_eps: 0.602161
  66445/150000: episode: 703, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -12.260, mean reward: -0.130 [-100.000, 14.538], mean action: 1.617 [0.000, 3.000],  loss: 3.192311, mae: 35.594746, mean_q: 19.843708, mean_eps: 0.601615
  66528/150000: episode: 704, duration: 0.568s, episode steps:  83, steps per second: 146, episode reward: -13.364, mean reward: -0.161 [-100.000, 21.055], mean action: 1.663 [0.000, 3.000],  loss: 5.641013, mae: 35.323754, mean_q: 18.724342, mean_eps: 0.601084
  66769/150000: episode: 705, duration: 1.682s, episode steps: 241, steps per second: 143, episode reward: -198.973, mean reward: -0.826 [-100.000,  9.376], mean action: 1.568 [0.000, 3.000],  loss: 4.603144, mae: 35.656179, mean_q: 18.993503, mean_eps: 0.600112
  66915/150000: episode: 706, duration: 0.997s, episode steps: 146, steps per second: 147, episode reward: -25.405, mean reward: -0.174 [-100.000, 22.052], mean action: 1.644 [0.000, 3.000],  loss: 3.563983, mae: 35.237519, mean_q: 19.668299, mean_eps: 0.598951
  67050/150000: episode: 707, duration: 0.926s, episode steps: 135, steps per second: 146, episode reward: -37.293, mean reward: -0.276 [-100.000, 21.379], mean action: 1.681 [0.000, 3.000],  loss: 8.543249, mae: 35.557248, mean_q: 20.605136, mean_eps: 0.598108
  67153/150000: episode: 708, duration: 0.709s, episode steps: 103, steps per second: 145, episode reward: -3.719, mean reward: -0.036 [-100.000, 18.073], mean action: 1.757 [0.000, 3.000],  loss: 8.119633, mae: 35.761494, mean_q: 21.876140, mean_eps: 0.597394
  67267/150000: episode: 709, duration: 0.821s, episode steps: 114, steps per second: 139, episode reward: -40.720, mean reward: -0.357 [-100.000, 13.240], mean action: 1.632 [0.000, 3.000],  loss: 6.414598, mae: 35.777223, mean_q: 21.215323, mean_eps: 0.596743
  67379/150000: episode: 710, duration: 0.756s, episode steps: 112, steps per second: 148, episode reward: -44.461, mean reward: -0.397 [-100.000, 13.264], mean action: 1.634 [0.000, 3.000],  loss: 6.327446, mae: 35.263782, mean_q: 21.864816, mean_eps: 0.596065
  67499/150000: episode: 711, duration: 0.821s, episode steps: 120, steps per second: 146, episode reward: -53.517, mean reward: -0.446 [-100.000,  7.270], mean action: 1.525 [0.000, 3.000],  loss: 6.624555, mae: 35.608440, mean_q: 21.890209, mean_eps: 0.595369
  67625/150000: episode: 712, duration: 0.884s, episode steps: 126, steps per second: 142, episode reward: -21.592, mean reward: -0.171 [-100.000, 13.385], mean action: 1.667 [0.000, 3.000],  loss: 4.528449, mae: 35.724324, mean_q: 21.785397, mean_eps: 0.594631
  67722/150000: episode: 713, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -22.227, mean reward: -0.229 [-100.000, 11.382], mean action: 1.773 [0.000, 3.000],  loss: 7.088929, mae: 35.753658, mean_q: 21.220845, mean_eps: 0.593962
  67799/150000: episode: 714, duration: 0.516s, episode steps:  77, steps per second: 149, episode reward: -62.380, mean reward: -0.810 [-100.000, 14.993], mean action: 1.506 [0.000, 3.000],  loss: 6.880829, mae: 35.547575, mean_q: 21.529032, mean_eps: 0.593440
  67912/150000: episode: 715, duration: 0.809s, episode steps: 113, steps per second: 140, episode reward: -3.325, mean reward: -0.029 [-100.000, 16.881], mean action: 1.841 [0.000, 3.000],  loss: 7.879380, mae: 35.559071, mean_q: 21.635119, mean_eps: 0.592870
  68042/150000: episode: 716, duration: 0.895s, episode steps: 130, steps per second: 145, episode reward: 29.430, mean reward:  0.226 [-100.000, 18.684], mean action: 1.838 [0.000, 3.000],  loss: 5.348825, mae: 35.793380, mean_q: 21.389748, mean_eps: 0.592141
  68123/150000: episode: 717, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -27.495, mean reward: -0.339 [-100.000,  8.162], mean action: 1.679 [0.000, 3.000],  loss: 3.039068, mae: 35.543290, mean_q: 21.070532, mean_eps: 0.591508
  68231/150000: episode: 718, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: -54.475, mean reward: -0.504 [-100.000, 12.741], mean action: 1.593 [0.000, 3.000],  loss: 7.765725, mae: 36.154503, mean_q: 20.931989, mean_eps: 0.590941
  68356/150000: episode: 719, duration: 0.840s, episode steps: 125, steps per second: 149, episode reward: -52.756, mean reward: -0.422 [-100.000,  7.342], mean action: 1.736 [0.000, 3.000],  loss: 3.474166, mae: 35.907056, mean_q: 20.992865, mean_eps: 0.590242
  68481/150000: episode: 720, duration: 0.860s, episode steps: 125, steps per second: 145, episode reward: -23.294, mean reward: -0.186 [-100.000,  9.021], mean action: 1.688 [0.000, 3.000],  loss: 7.209289, mae: 36.390743, mean_q: 21.652409, mean_eps: 0.589492
  68565/150000: episode: 721, duration: 0.582s, episode steps:  84, steps per second: 144, episode reward: -63.986, mean reward: -0.762 [-100.000,  6.638], mean action: 1.940 [0.000, 3.000],  loss: 7.269158, mae: 36.300079, mean_q: 21.710704, mean_eps: 0.588865
  68688/150000: episode: 722, duration: 0.829s, episode steps: 123, steps per second: 148, episode reward: -115.391, mean reward: -0.938 [-100.000, 26.355], mean action: 1.431 [0.000, 3.000],  loss: 5.360419, mae: 36.221335, mean_q: 20.722402, mean_eps: 0.588244
  68812/150000: episode: 723, duration: 0.873s, episode steps: 124, steps per second: 142, episode reward: -91.554, mean reward: -0.738 [-100.000, 14.514], mean action: 1.661 [0.000, 3.000],  loss: 4.481031, mae: 36.113929, mean_q: 21.851270, mean_eps: 0.587503
  68901/150000: episode: 724, duration: 0.619s, episode steps:  89, steps per second: 144, episode reward: -98.460, mean reward: -1.106 [-100.000, 10.311], mean action: 1.596 [0.000, 3.000],  loss: 4.636523, mae: 36.484809, mean_q: 21.410015, mean_eps: 0.586864
  69030/150000: episode: 725, duration: 0.865s, episode steps: 129, steps per second: 149, episode reward: -241.389, mean reward: -1.871 [-100.000, 62.455], mean action: 1.682 [0.000, 3.000],  loss: 3.660970, mae: 36.410665, mean_q: 22.362284, mean_eps: 0.586210
  69126/150000: episode: 726, duration: 0.684s, episode steps:  96, steps per second: 140, episode reward: -43.025, mean reward: -0.448 [-100.000, 11.077], mean action: 1.562 [0.000, 3.000],  loss: 3.514967, mae: 36.318483, mean_q: 21.351330, mean_eps: 0.585535
  69230/150000: episode: 727, duration: 0.710s, episode steps: 104, steps per second: 146, episode reward: -43.769, mean reward: -0.421 [-100.000, 13.266], mean action: 1.683 [0.000, 3.000],  loss: 4.286851, mae: 36.195783, mean_q: 20.732494, mean_eps: 0.584935
  69309/150000: episode: 728, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -17.824, mean reward: -0.226 [-100.000, 14.378], mean action: 1.772 [0.000, 3.000],  loss: 4.328065, mae: 36.482120, mean_q: 22.088511, mean_eps: 0.584386
  69384/150000: episode: 729, duration: 0.536s, episode steps:  75, steps per second: 140, episode reward: -51.995, mean reward: -0.693 [-100.000, 10.833], mean action: 1.853 [0.000, 3.000],  loss: 5.349059, mae: 36.504904, mean_q: 20.961572, mean_eps: 0.583924
  69526/150000: episode: 730, duration: 1.003s, episode steps: 142, steps per second: 142, episode reward: -28.501, mean reward: -0.201 [-100.000, 12.291], mean action: 1.479 [0.000, 3.000],  loss: 2.773069, mae: 36.195601, mean_q: 21.869101, mean_eps: 0.583273
  69613/150000: episode: 731, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -72.872, mean reward: -0.838 [-100.000,  9.567], mean action: 1.713 [0.000, 3.000],  loss: 5.023647, mae: 36.802401, mean_q: 22.586802, mean_eps: 0.582586
  69747/150000: episode: 732, duration: 0.946s, episode steps: 134, steps per second: 142, episode reward: -53.980, mean reward: -0.403 [-100.000, 14.334], mean action: 1.731 [0.000, 3.000],  loss: 5.308601, mae: 36.473396, mean_q: 22.440795, mean_eps: 0.581923
  69858/150000: episode: 733, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -70.189, mean reward: -0.632 [-100.000,  6.519], mean action: 1.739 [0.000, 3.000],  loss: 4.205894, mae: 36.382450, mean_q: 22.363845, mean_eps: 0.581188
  69944/150000: episode: 734, duration: 0.585s, episode steps:  86, steps per second: 147, episode reward: -77.620, mean reward: -0.903 [-100.000,  9.589], mean action: 1.640 [0.000, 3.000],  loss: 3.249365, mae: 36.490816, mean_q: 21.559542, mean_eps: 0.580597
  70021/150000: episode: 735, duration: 0.542s, episode steps:  77, steps per second: 142, episode reward: -60.293, mean reward: -0.783 [-100.000,  8.044], mean action: 1.247 [0.000, 3.000],  loss: 5.305318, mae: 36.617161, mean_q: 22.022612, mean_eps: 0.580108
  70116/150000: episode: 736, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -48.717, mean reward: -0.513 [-100.000, 14.350], mean action: 1.526 [0.000, 3.000],  loss: 5.813395, mae: 36.661232, mean_q: 22.499155, mean_eps: 0.579592
  70199/150000: episode: 737, duration: 0.577s, episode steps:  83, steps per second: 144, episode reward: -14.132, mean reward: -0.170 [-100.000,  8.113], mean action: 1.723 [0.000, 3.000],  loss: 8.442727, mae: 37.149132, mean_q: 20.778706, mean_eps: 0.579058
  70352/150000: episode: 738, duration: 1.108s, episode steps: 153, steps per second: 138, episode reward: -49.272, mean reward: -0.322 [-100.000,  9.479], mean action: 1.569 [0.000, 3.000],  loss: 6.111862, mae: 36.745453, mean_q: 22.146757, mean_eps: 0.578350
  70424/150000: episode: 739, duration: 0.490s, episode steps:  72, steps per second: 147, episode reward: -45.653, mean reward: -0.634 [-100.000, 16.357], mean action: 1.792 [0.000, 3.000],  loss: 3.565128, mae: 36.613608, mean_q: 22.023906, mean_eps: 0.577675
  70573/150000: episode: 740, duration: 0.994s, episode steps: 149, steps per second: 150, episode reward: -13.732, mean reward: -0.092 [-100.000, 14.822], mean action: 1.799 [0.000, 3.000],  loss: 4.018649, mae: 36.838660, mean_q: 21.461408, mean_eps: 0.577012
  70703/150000: episode: 741, duration: 0.912s, episode steps: 130, steps per second: 143, episode reward: -19.721, mean reward: -0.152 [-100.000, 17.865], mean action: 1.577 [0.000, 3.000],  loss: 5.425392, mae: 36.523182, mean_q: 22.308418, mean_eps: 0.576175
  70812/150000: episode: 742, duration: 0.747s, episode steps: 109, steps per second: 146, episode reward: -36.287, mean reward: -0.333 [-100.000, 18.094], mean action: 1.853 [0.000, 3.000],  loss: 4.352991, mae: 36.905960, mean_q: 21.843466, mean_eps: 0.575458
  70910/150000: episode: 743, duration: 0.656s, episode steps:  98, steps per second: 149, episode reward: -4.719, mean reward: -0.048 [-100.000, 15.403], mean action: 1.684 [0.000, 3.000],  loss: 4.824387, mae: 36.396665, mean_q: 21.940711, mean_eps: 0.574837
  71013/150000: episode: 744, duration: 0.747s, episode steps: 103, steps per second: 138, episode reward:  5.074, mean reward:  0.049 [-100.000, 17.726], mean action: 1.796 [0.000, 3.000],  loss: 8.454351, mae: 36.888464, mean_q: 22.778310, mean_eps: 0.574234
  71162/150000: episode: 745, duration: 1.008s, episode steps: 149, steps per second: 148, episode reward: -16.755, mean reward: -0.112 [-100.000, 20.087], mean action: 1.785 [0.000, 3.000],  loss: 4.682479, mae: 36.139233, mean_q: 21.967696, mean_eps: 0.573478
  71268/150000: episode: 746, duration: 0.743s, episode steps: 106, steps per second: 143, episode reward: -52.491, mean reward: -0.495 [-100.000,  6.740], mean action: 1.443 [0.000, 3.000],  loss: 3.917749, mae: 36.303776, mean_q: 21.759268, mean_eps: 0.572713
  71359/150000: episode: 747, duration: 0.633s, episode steps:  91, steps per second: 144, episode reward: -41.060, mean reward: -0.451 [-100.000, 11.965], mean action: 1.659 [0.000, 3.000],  loss: 4.324324, mae: 35.676967, mean_q: 21.524586, mean_eps: 0.572122
  71440/150000: episode: 748, duration: 0.694s, episode steps:  81, steps per second: 117, episode reward: -51.682, mean reward: -0.638 [-100.000, 10.022], mean action: 1.630 [0.000, 3.000],  loss: 8.389355, mae: 36.274605, mean_q: 21.593559, mean_eps: 0.571606
  71553/150000: episode: 749, duration: 1.085s, episode steps: 113, steps per second: 104, episode reward: -43.887, mean reward: -0.388 [-100.000, 20.107], mean action: 1.646 [0.000, 3.000],  loss: 5.890268, mae: 35.958462, mean_q: 21.161815, mean_eps: 0.571024
  71647/150000: episode: 750, duration: 0.800s, episode steps:  94, steps per second: 118, episode reward: 17.106, mean reward:  0.182 [-100.000, 23.677], mean action: 1.734 [0.000, 3.000],  loss: 3.934232, mae: 35.806577, mean_q: 20.039732, mean_eps: 0.570403
  71740/150000: episode: 751, duration: 0.785s, episode steps:  93, steps per second: 118, episode reward: -58.757, mean reward: -0.632 [-100.000,  9.860], mean action: 1.634 [0.000, 3.000],  loss: 4.066548, mae: 35.871122, mean_q: 20.594357, mean_eps: 0.569842
  71821/150000: episode: 752, duration: 0.708s, episode steps:  81, steps per second: 114, episode reward: -59.327, mean reward: -0.732 [-100.000, 16.359], mean action: 1.790 [0.000, 3.000],  loss: 5.045706, mae: 35.865047, mean_q: 20.771824, mean_eps: 0.569320
  71910/150000: episode: 753, duration: 0.686s, episode steps:  89, steps per second: 130, episode reward: -43.078, mean reward: -0.484 [-100.000, 17.229], mean action: 1.708 [0.000, 3.000],  loss: 8.150986, mae: 36.285751, mean_q: 20.596742, mean_eps: 0.568810
  72027/150000: episode: 754, duration: 0.861s, episode steps: 117, steps per second: 136, episode reward:  2.072, mean reward:  0.018 [-100.000, 15.233], mean action: 1.726 [0.000, 3.000],  loss: 3.316351, mae: 35.939502, mean_q: 21.553800, mean_eps: 0.568192
  72292/150000: episode: 755, duration: 2.041s, episode steps: 265, steps per second: 130, episode reward: -261.541, mean reward: -0.987 [-100.000, 16.609], mean action: 1.766 [0.000, 3.000],  loss: 4.814669, mae: 36.105500, mean_q: 21.802585, mean_eps: 0.567046
  72389/150000: episode: 756, duration: 0.707s, episode steps:  97, steps per second: 137, episode reward: -71.044, mean reward: -0.732 [-100.000,  9.979], mean action: 1.711 [0.000, 3.000],  loss: 4.128348, mae: 35.934794, mean_q: 20.348513, mean_eps: 0.565960
  72525/150000: episode: 757, duration: 0.915s, episode steps: 136, steps per second: 149, episode reward: -28.042, mean reward: -0.206 [-100.000, 13.613], mean action: 1.691 [0.000, 3.000],  loss: 4.751277, mae: 36.330026, mean_q: 21.443783, mean_eps: 0.565261
  72752/150000: episode: 758, duration: 1.565s, episode steps: 227, steps per second: 145, episode reward: -3.663, mean reward: -0.016 [-100.000, 44.721], mean action: 1.692 [0.000, 3.000],  loss: 5.710840, mae: 36.251321, mean_q: 20.914908, mean_eps: 0.564172
  72845/150000: episode: 759, duration: 0.668s, episode steps:  93, steps per second: 139, episode reward: -117.690, mean reward: -1.265 [-100.000,  9.033], mean action: 1.914 [0.000, 3.000],  loss: 6.749282, mae: 36.293944, mean_q: 21.257512, mean_eps: 0.563212
  72957/150000: episode: 760, duration: 0.841s, episode steps: 112, steps per second: 133, episode reward: -79.070, mean reward: -0.706 [-100.000,  6.281], mean action: 1.545 [0.000, 3.000],  loss: 5.141545, mae: 36.299545, mean_q: 20.552661, mean_eps: 0.562597
  73078/150000: episode: 761, duration: 0.986s, episode steps: 121, steps per second: 123, episode reward: -12.946, mean reward: -0.107 [-100.000, 13.618], mean action: 1.488 [0.000, 3.000],  loss: 3.556545, mae: 35.772853, mean_q: 21.276152, mean_eps: 0.561898
  73178/150000: episode: 762, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -68.307, mean reward: -0.683 [-100.000,  8.331], mean action: 1.750 [0.000, 3.000],  loss: 6.108676, mae: 35.890546, mean_q: 21.002347, mean_eps: 0.561235
  73281/150000: episode: 763, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -64.277, mean reward: -0.624 [-100.000, 20.903], mean action: 1.592 [0.000, 3.000],  loss: 3.170224, mae: 35.969801, mean_q: 20.284536, mean_eps: 0.560626
  73441/150000: episode: 764, duration: 1.104s, episode steps: 160, steps per second: 145, episode reward: -148.886, mean reward: -0.931 [-100.000,  3.158], mean action: 1.525 [0.000, 3.000],  loss: 3.157916, mae: 35.654537, mean_q: 20.709480, mean_eps: 0.559837
  73550/150000: episode: 765, duration: 0.769s, episode steps: 109, steps per second: 142, episode reward: -86.901, mean reward: -0.797 [-100.000, 15.362], mean action: 1.853 [0.000, 3.000],  loss: 4.961266, mae: 35.870443, mean_q: 21.200992, mean_eps: 0.559030
  73662/150000: episode: 766, duration: 0.780s, episode steps: 112, steps per second: 144, episode reward: -262.273, mean reward: -2.342 [-100.000, 71.286], mean action: 1.714 [0.000, 3.000],  loss: 3.275687, mae: 35.889108, mean_q: 20.726728, mean_eps: 0.558367
  73783/150000: episode: 767, duration: 0.814s, episode steps: 121, steps per second: 149, episode reward: -3.793, mean reward: -0.031 [-100.000, 19.553], mean action: 1.785 [0.000, 3.000],  loss: 3.403918, mae: 35.986214, mean_q: 21.147194, mean_eps: 0.557668
  73865/150000: episode: 768, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -35.007, mean reward: -0.427 [-100.000, 13.540], mean action: 1.659 [0.000, 3.000],  loss: 4.242954, mae: 35.551923, mean_q: 21.322710, mean_eps: 0.557059
  74015/150000: episode: 769, duration: 1.039s, episode steps: 150, steps per second: 144, episode reward: -197.843, mean reward: -1.319 [-100.000, 72.997], mean action: 1.573 [0.000, 3.000],  loss: 4.273875, mae: 35.908107, mean_q: 21.589448, mean_eps: 0.556363
  74100/150000: episode: 770, duration: 0.618s, episode steps:  85, steps per second: 137, episode reward: -63.968, mean reward: -0.753 [-100.000, 12.973], mean action: 1.553 [0.000, 3.000],  loss: 6.437460, mae: 36.742774, mean_q: 21.593810, mean_eps: 0.555658
  74222/150000: episode: 771, duration: 0.869s, episode steps: 122, steps per second: 140, episode reward: -71.738, mean reward: -0.588 [-100.000,  7.041], mean action: 1.697 [0.000, 3.000],  loss: 5.166299, mae: 36.440906, mean_q: 21.334916, mean_eps: 0.555037
  74322/150000: episode: 772, duration: 0.674s, episode steps: 100, steps per second: 148, episode reward: -57.358, mean reward: -0.574 [-100.000, 15.490], mean action: 1.680 [0.000, 3.000],  loss: 4.642952, mae: 36.725521, mean_q: 21.432260, mean_eps: 0.554371
  74437/150000: episode: 773, duration: 0.776s, episode steps: 115, steps per second: 148, episode reward: -56.778, mean reward: -0.494 [-100.000, 18.160], mean action: 1.539 [0.000, 3.000],  loss: 5.619163, mae: 36.519520, mean_q: 21.920360, mean_eps: 0.553726
  74506/150000: episode: 774, duration: 0.508s, episode steps:  69, steps per second: 136, episode reward: -41.556, mean reward: -0.602 [-100.000,  7.830], mean action: 1.623 [0.000, 3.000],  loss: 4.595090, mae: 36.163545, mean_q: 21.136884, mean_eps: 0.553174
  74755/150000: episode: 775, duration: 1.687s, episode steps: 249, steps per second: 148, episode reward: -126.142, mean reward: -0.507 [-100.000, 15.325], mean action: 1.687 [0.000, 3.000],  loss: 4.380234, mae: 36.655147, mean_q: 21.677743, mean_eps: 0.552220
  74891/150000: episode: 776, duration: 0.952s, episode steps: 136, steps per second: 143, episode reward: -66.526, mean reward: -0.489 [-100.000,  8.329], mean action: 1.559 [0.000, 3.000],  loss: 3.873906, mae: 36.662254, mean_q: 22.372681, mean_eps: 0.551065
  75011/150000: episode: 777, duration: 0.802s, episode steps: 120, steps per second: 150, episode reward: -103.075, mean reward: -0.859 [-100.000, 12.853], mean action: 1.625 [0.000, 3.000],  loss: 4.349100, mae: 36.358925, mean_q: 20.940856, mean_eps: 0.550297
  75129/150000: episode: 778, duration: 0.823s, episode steps: 118, steps per second: 143, episode reward: -71.501, mean reward: -0.606 [-100.000, 11.312], mean action: 1.449 [0.000, 3.000],  loss: 4.196645, mae: 36.808459, mean_q: 21.020142, mean_eps: 0.549583
  75198/150000: episode: 779, duration: 0.491s, episode steps:  69, steps per second: 141, episode reward: -40.706, mean reward: -0.590 [-100.000, 11.580], mean action: 1.754 [0.000, 3.000],  loss: 3.908158, mae: 36.595754, mean_q: 22.217193, mean_eps: 0.549022
  75298/150000: episode: 780, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -44.979, mean reward: -0.450 [-100.000, 11.957], mean action: 1.720 [0.000, 3.000],  loss: 4.242731, mae: 36.560308, mean_q: 20.284332, mean_eps: 0.548515
  75433/150000: episode: 781, duration: 0.935s, episode steps: 135, steps per second: 144, episode reward: 20.395, mean reward:  0.151 [-100.000, 15.833], mean action: 1.793 [0.000, 3.000],  loss: 4.370534, mae: 36.828076, mean_q: 22.095911, mean_eps: 0.547810
  75519/150000: episode: 782, duration: 0.600s, episode steps:  86, steps per second: 143, episode reward: -15.616, mean reward: -0.182 [-100.000,  9.509], mean action: 1.698 [0.000, 3.000],  loss: 3.824433, mae: 36.432795, mean_q: 22.282680, mean_eps: 0.547147
  76519/150000: episode: 783, duration: 7.996s, episode steps: 1000, steps per second: 125, episode reward: -52.589, mean reward: -0.053 [-22.579, 21.348], mean action: 1.655 [0.000, 3.000],  loss: 4.577694, mae: 36.356741, mean_q: 20.829761, mean_eps: 0.543889
  76611/150000: episode: 784, duration: 0.617s, episode steps:  92, steps per second: 149, episode reward: -88.421, mean reward: -0.961 [-100.000,  7.403], mean action: 1.413 [0.000, 3.000],  loss: 5.747871, mae: 35.950014, mean_q: 20.072426, mean_eps: 0.540613
  76747/150000: episode: 785, duration: 0.909s, episode steps: 136, steps per second: 150, episode reward: -51.533, mean reward: -0.379 [-100.000, 17.293], mean action: 1.596 [0.000, 3.000],  loss: 6.506839, mae: 36.312094, mean_q: 20.135310, mean_eps: 0.539929
  76833/150000: episode: 786, duration: 0.596s, episode steps:  86, steps per second: 144, episode reward: -34.633, mean reward: -0.403 [-100.000, 13.415], mean action: 1.640 [0.000, 3.000],  loss: 4.321814, mae: 35.456975, mean_q: 19.868764, mean_eps: 0.539263
  76909/150000: episode: 787, duration: 0.516s, episode steps:  76, steps per second: 147, episode reward: -4.949, mean reward: -0.065 [-100.000, 12.075], mean action: 1.908 [0.000, 3.000],  loss: 6.797551, mae: 35.902047, mean_q: 19.141976, mean_eps: 0.538777
  76985/150000: episode: 788, duration: 0.510s, episode steps:  76, steps per second: 149, episode reward: -24.347, mean reward: -0.320 [-100.000, 13.471], mean action: 1.513 [0.000, 3.000],  loss: 5.936214, mae: 35.878996, mean_q: 19.499187, mean_eps: 0.538321
  77129/150000: episode: 789, duration: 0.969s, episode steps: 144, steps per second: 149, episode reward: -47.422, mean reward: -0.329 [-100.000, 17.031], mean action: 1.403 [0.000, 3.000],  loss: 5.005792, mae: 35.692809, mean_q: 20.640704, mean_eps: 0.537661
  77231/150000: episode: 790, duration: 0.740s, episode steps: 102, steps per second: 138, episode reward: -51.113, mean reward: -0.501 [-100.000,  6.271], mean action: 1.667 [0.000, 3.000],  loss: 7.515250, mae: 35.967484, mean_q: 21.132468, mean_eps: 0.536923
  77403/150000: episode: 791, duration: 1.204s, episode steps: 172, steps per second: 143, episode reward: -52.844, mean reward: -0.307 [-100.000, 14.588], mean action: 1.715 [0.000, 3.000],  loss: 5.258401, mae: 35.504668, mean_q: 20.835137, mean_eps: 0.536101
  77520/150000: episode: 792, duration: 0.842s, episode steps: 117, steps per second: 139, episode reward: -12.763, mean reward: -0.109 [-100.000, 11.503], mean action: 1.718 [0.000, 3.000],  loss: 4.872863, mae: 35.466528, mean_q: 20.016095, mean_eps: 0.535234
  77654/150000: episode: 793, duration: 0.897s, episode steps: 134, steps per second: 149, episode reward: -6.078, mean reward: -0.045 [-100.000, 11.167], mean action: 1.649 [0.000, 3.000],  loss: 4.967400, mae: 35.467295, mean_q: 20.536129, mean_eps: 0.534481
  77806/150000: episode: 794, duration: 1.083s, episode steps: 152, steps per second: 140, episode reward: -113.227, mean reward: -0.745 [-100.000, 12.230], mean action: 1.737 [0.000, 3.000],  loss: 4.804559, mae: 35.663728, mean_q: 20.679481, mean_eps: 0.533623
  77927/150000: episode: 795, duration: 0.834s, episode steps: 121, steps per second: 145, episode reward: -10.819, mean reward: -0.089 [-100.000, 30.080], mean action: 1.645 [0.000, 3.000],  loss: 5.005462, mae: 35.453552, mean_q: 19.417763, mean_eps: 0.532804
  77996/150000: episode: 796, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -26.972, mean reward: -0.391 [-100.000, 10.701], mean action: 1.652 [0.000, 3.000],  loss: 3.679572, mae: 35.659533, mean_q: 19.767613, mean_eps: 0.532234
  78166/150000: episode: 797, duration: 1.244s, episode steps: 170, steps per second: 137, episode reward: -106.823, mean reward: -0.628 [-100.000,  4.308], mean action: 1.771 [0.000, 3.000],  loss: 4.509367, mae: 35.453814, mean_q: 20.184122, mean_eps: 0.531517
  78257/150000: episode: 798, duration: 0.622s, episode steps:  91, steps per second: 146, episode reward: -44.043, mean reward: -0.484 [-100.000, 12.331], mean action: 1.813 [0.000, 3.000],  loss: 5.996061, mae: 35.904939, mean_q: 21.139708, mean_eps: 0.530734
  78376/150000: episode: 799, duration: 0.809s, episode steps: 119, steps per second: 147, episode reward: -7.062, mean reward: -0.059 [-100.000, 18.242], mean action: 1.790 [0.000, 3.000],  loss: 5.011944, mae: 35.754384, mean_q: 20.604812, mean_eps: 0.530104
  78488/150000: episode: 800, duration: 0.789s, episode steps: 112, steps per second: 142, episode reward: -42.047, mean reward: -0.375 [-100.000,  8.610], mean action: 1.652 [0.000, 3.000],  loss: 4.602283, mae: 35.807922, mean_q: 20.558461, mean_eps: 0.529411
  79488/150000: episode: 801, duration: 7.582s, episode steps: 1000, steps per second: 132, episode reward: 15.565, mean reward:  0.016 [-23.974, 48.603], mean action: 1.765 [0.000, 3.000],  loss: 5.081888, mae: 35.640990, mean_q: 20.747656, mean_eps: 0.526075
  79641/150000: episode: 802, duration: 1.065s, episode steps: 153, steps per second: 144, episode reward: -186.349, mean reward: -1.218 [-100.000,  8.672], mean action: 1.673 [0.000, 3.000],  loss: 4.303985, mae: 35.575775, mean_q: 20.611393, mean_eps: 0.522616
  79775/150000: episode: 803, duration: 0.912s, episode steps: 134, steps per second: 147, episode reward: -10.044, mean reward: -0.075 [-100.000, 15.200], mean action: 1.552 [0.000, 3.000],  loss: 6.506110, mae: 35.953140, mean_q: 20.787103, mean_eps: 0.521755
  79933/150000: episode: 804, duration: 1.131s, episode steps: 158, steps per second: 140, episode reward: -151.013, mean reward: -0.956 [-100.000, 41.239], mean action: 1.797 [0.000, 3.000],  loss: 4.691068, mae: 35.455677, mean_q: 21.312353, mean_eps: 0.520879
  80036/150000: episode: 805, duration: 0.691s, episode steps: 103, steps per second: 149, episode reward: -39.704, mean reward: -0.385 [-100.000, 15.872], mean action: 1.689 [0.000, 3.000],  loss: 5.614282, mae: 35.406254, mean_q: 19.930113, mean_eps: 0.520096
  80154/150000: episode: 806, duration: 0.826s, episode steps: 118, steps per second: 143, episode reward: -41.870, mean reward: -0.355 [-100.000, 10.730], mean action: 1.746 [0.000, 3.000],  loss: 6.125961, mae: 35.844496, mean_q: 19.668017, mean_eps: 0.519433
  80257/150000: episode: 807, duration: 0.705s, episode steps: 103, steps per second: 146, episode reward: -162.598, mean reward: -1.579 [-100.000,  2.252], mean action: 1.495 [0.000, 3.000],  loss: 6.821621, mae: 35.738516, mean_q: 20.645926, mean_eps: 0.518770
  80393/150000: episode: 808, duration: 0.915s, episode steps: 136, steps per second: 149, episode reward: -81.556, mean reward: -0.600 [-100.000,  7.500], mean action: 1.559 [0.000, 3.000],  loss: 4.981175, mae: 35.504718, mean_q: 21.004957, mean_eps: 0.518053
  80569/150000: episode: 809, duration: 1.245s, episode steps: 176, steps per second: 141, episode reward: -106.933, mean reward: -0.608 [-100.000, 10.834], mean action: 1.790 [0.000, 3.000],  loss: 4.906412, mae: 35.715554, mean_q: 21.065445, mean_eps: 0.517117
  80657/150000: episode: 810, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -51.828, mean reward: -0.589 [-100.000, 10.330], mean action: 1.920 [0.000, 3.000],  loss: 6.217384, mae: 35.786314, mean_q: 20.121386, mean_eps: 0.516325
  80764/150000: episode: 811, duration: 0.731s, episode steps: 107, steps per second: 146, episode reward: -64.617, mean reward: -0.604 [-100.000,  9.261], mean action: 1.523 [0.000, 3.000],  loss: 4.493007, mae: 35.851904, mean_q: 19.826113, mean_eps: 0.515740
  80874/150000: episode: 812, duration: 0.908s, episode steps: 110, steps per second: 121, episode reward: -44.844, mean reward: -0.408 [-100.000, 11.934], mean action: 1.536 [0.000, 3.000],  loss: 6.060891, mae: 35.470042, mean_q: 20.957128, mean_eps: 0.515089
  81295/150000: episode: 813, duration: 3.517s, episode steps: 421, steps per second: 120, episode reward: -133.024, mean reward: -0.316 [-100.000, 15.575], mean action: 1.734 [0.000, 3.000],  loss: 5.397520, mae: 35.567431, mean_q: 20.534688, mean_eps: 0.513496
  81422/150000: episode: 814, duration: 0.969s, episode steps: 127, steps per second: 131, episode reward: -46.059, mean reward: -0.363 [-100.000, 15.462], mean action: 1.614 [0.000, 3.000],  loss: 6.828501, mae: 35.482960, mean_q: 20.016577, mean_eps: 0.511852
  81580/150000: episode: 815, duration: 1.083s, episode steps: 158, steps per second: 146, episode reward: -35.568, mean reward: -0.225 [-100.000, 36.367], mean action: 1.500 [0.000, 3.000],  loss: 5.591426, mae: 35.580034, mean_q: 21.035507, mean_eps: 0.510997
  81676/150000: episode: 816, duration: 0.690s, episode steps:  96, steps per second: 139, episode reward: -22.171, mean reward: -0.231 [-100.000, 18.828], mean action: 1.719 [0.000, 3.000],  loss: 5.250139, mae: 35.403021, mean_q: 20.943447, mean_eps: 0.510235
  81799/150000: episode: 817, duration: 0.863s, episode steps: 123, steps per second: 143, episode reward: -61.055, mean reward: -0.496 [-100.000, 16.184], mean action: 1.715 [0.000, 3.000],  loss: 6.104762, mae: 35.473036, mean_q: 20.853394, mean_eps: 0.509578
  81951/150000: episode: 818, duration: 1.077s, episode steps: 152, steps per second: 141, episode reward: -1.385, mean reward: -0.009 [-100.000, 19.242], mean action: 1.691 [0.000, 3.000],  loss: 6.263600, mae: 35.893554, mean_q: 20.055369, mean_eps: 0.508753
  82114/150000: episode: 819, duration: 1.114s, episode steps: 163, steps per second: 146, episode reward: 13.026, mean reward:  0.080 [-100.000,  9.708], mean action: 1.718 [0.000, 3.000],  loss: 5.237599, mae: 35.468968, mean_q: 21.171157, mean_eps: 0.507808
  82199/150000: episode: 820, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -19.540, mean reward: -0.230 [-100.000, 14.117], mean action: 1.565 [0.000, 3.000],  loss: 6.125871, mae: 35.731931, mean_q: 20.541586, mean_eps: 0.507064
  82329/150000: episode: 821, duration: 0.940s, episode steps: 130, steps per second: 138, episode reward:  8.619, mean reward:  0.066 [-100.000, 18.036], mean action: 1.669 [0.000, 3.000],  loss: 4.914950, mae: 35.922277, mean_q: 20.118262, mean_eps: 0.506419
  82450/150000: episode: 822, duration: 0.859s, episode steps: 121, steps per second: 141, episode reward: -30.627, mean reward: -0.253 [-100.000, 11.588], mean action: 1.521 [0.000, 3.000],  loss: 6.754905, mae: 35.644247, mean_q: 20.964024, mean_eps: 0.505666
  82575/150000: episode: 823, duration: 0.906s, episode steps: 125, steps per second: 138, episode reward:  7.753, mean reward:  0.062 [-100.000, 14.680], mean action: 1.664 [0.000, 3.000],  loss: 4.959880, mae: 35.735304, mean_q: 20.788937, mean_eps: 0.504928
  82683/150000: episode: 824, duration: 0.735s, episode steps: 108, steps per second: 147, episode reward: -90.269, mean reward: -0.836 [-100.000, 29.163], mean action: 1.648 [0.000, 3.000],  loss: 4.763344, mae: 35.656641, mean_q: 20.778176, mean_eps: 0.504229
  83085/150000: episode: 825, duration: 2.839s, episode steps: 402, steps per second: 142, episode reward: -93.587, mean reward: -0.233 [-100.000, 15.450], mean action: 1.649 [0.000, 3.000],  loss: 6.422804, mae: 35.645006, mean_q: 21.079220, mean_eps: 0.502699
  83746/150000: episode: 826, duration: 5.238s, episode steps: 661, steps per second: 126, episode reward: -143.164, mean reward: -0.217 [-100.000, 15.191], mean action: 1.744 [0.000, 3.000],  loss: 5.960707, mae: 35.502576, mean_q: 21.424155, mean_eps: 0.499510
  83986/150000: episode: 827, duration: 1.696s, episode steps: 240, steps per second: 142, episode reward: -53.123, mean reward: -0.221 [-100.000, 45.734], mean action: 1.733 [0.000, 3.000],  loss: 5.621496, mae: 35.481159, mean_q: 21.987164, mean_eps: 0.496807
  84125/150000: episode: 828, duration: 0.968s, episode steps: 139, steps per second: 144, episode reward: -68.546, mean reward: -0.493 [-100.000, 13.539], mean action: 1.683 [0.000, 3.000],  loss: 5.295702, mae: 35.673493, mean_q: 21.757947, mean_eps: 0.495670
  85125/150000: episode: 829, duration: 7.570s, episode steps: 1000, steps per second: 132, episode reward: 22.406, mean reward:  0.022 [-24.348, 24.484], mean action: 1.575 [0.000, 3.000],  loss: 6.497860, mae: 35.199233, mean_q: 21.255008, mean_eps: 0.492253
  85302/150000: episode: 830, duration: 1.198s, episode steps: 177, steps per second: 148, episode reward: -23.315, mean reward: -0.132 [-100.000, 11.201], mean action: 1.734 [0.000, 3.000],  loss: 6.471549, mae: 35.009728, mean_q: 21.705213, mean_eps: 0.488722
  85426/150000: episode: 831, duration: 0.835s, episode steps: 124, steps per second: 149, episode reward: -29.155, mean reward: -0.235 [-100.000, 21.396], mean action: 1.645 [0.000, 3.000],  loss: 7.763185, mae: 34.689285, mean_q: 21.246245, mean_eps: 0.487819
  85580/150000: episode: 832, duration: 1.071s, episode steps: 154, steps per second: 144, episode reward: -10.657, mean reward: -0.069 [-100.000, 10.838], mean action: 1.701 [0.000, 3.000],  loss: 6.757041, mae: 34.920906, mean_q: 20.857450, mean_eps: 0.486985
  85732/150000: episode: 833, duration: 1.024s, episode steps: 152, steps per second: 148, episode reward: 32.891, mean reward:  0.216 [-100.000, 20.552], mean action: 1.684 [0.000, 3.000],  loss: 6.505469, mae: 35.214989, mean_q: 22.253549, mean_eps: 0.486067
  85845/150000: episode: 834, duration: 0.799s, episode steps: 113, steps per second: 141, episode reward: -23.486, mean reward: -0.208 [-100.000, 21.832], mean action: 1.717 [0.000, 3.000],  loss: 8.117824, mae: 35.277742, mean_q: 21.396441, mean_eps: 0.485272
  85926/150000: episode: 835, duration: 0.552s, episode steps:  81, steps per second: 147, episode reward: -36.282, mean reward: -0.448 [-100.000, 10.965], mean action: 1.840 [0.000, 3.000],  loss: 9.446624, mae: 35.244554, mean_q: 21.342148, mean_eps: 0.484690
  86163/150000: episode: 836, duration: 1.778s, episode steps: 237, steps per second: 133, episode reward: -73.835, mean reward: -0.312 [-100.000, 15.453], mean action: 1.759 [0.000, 3.000],  loss: 7.407952, mae: 34.959829, mean_q: 21.621873, mean_eps: 0.483736
  86262/150000: episode: 837, duration: 0.657s, episode steps:  99, steps per second: 151, episode reward: -163.675, mean reward: -1.653 [-100.000,  1.490], mean action: 1.586 [0.000, 3.000],  loss: 8.212032, mae: 34.555781, mean_q: 22.227849, mean_eps: 0.482728
  86426/150000: episode: 838, duration: 1.148s, episode steps: 164, steps per second: 143, episode reward: -180.754, mean reward: -1.102 [-100.000, 62.293], mean action: 1.726 [0.000, 3.000],  loss: 6.107570, mae: 34.504635, mean_q: 22.527803, mean_eps: 0.481939
  86533/150000: episode: 839, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -15.367, mean reward: -0.144 [-100.000, 17.245], mean action: 1.598 [0.000, 3.000],  loss: 7.098134, mae: 35.200714, mean_q: 22.206715, mean_eps: 0.481126
  86968/150000: episode: 840, duration: 3.184s, episode steps: 435, steps per second: 137, episode reward: -103.924, mean reward: -0.239 [-100.000, 16.910], mean action: 1.766 [0.000, 3.000],  loss: 6.816389, mae: 34.723196, mean_q: 22.034133, mean_eps: 0.479500
  87115/150000: episode: 841, duration: 1.013s, episode steps: 147, steps per second: 145, episode reward: -15.989, mean reward: -0.109 [-100.000, 19.496], mean action: 1.347 [0.000, 3.000],  loss: 6.823284, mae: 35.079414, mean_q: 22.381924, mean_eps: 0.477754
  87247/150000: episode: 842, duration: 0.880s, episode steps: 132, steps per second: 150, episode reward: -59.131, mean reward: -0.448 [-100.000,  9.511], mean action: 1.561 [0.000, 3.000],  loss: 6.422491, mae: 35.074170, mean_q: 21.895566, mean_eps: 0.476917
  87391/150000: episode: 843, duration: 1.016s, episode steps: 144, steps per second: 142, episode reward: -15.955, mean reward: -0.111 [-100.000, 16.296], mean action: 1.576 [0.000, 3.000],  loss: 8.274765, mae: 34.965051, mean_q: 21.011541, mean_eps: 0.476089
  87506/150000: episode: 844, duration: 0.786s, episode steps: 115, steps per second: 146, episode reward: -14.120, mean reward: -0.123 [-100.000,  9.814], mean action: 1.696 [0.000, 3.000],  loss: 6.785170, mae: 35.073935, mean_q: 22.776617, mean_eps: 0.475312
  88265/150000: episode: 845, duration: 5.846s, episode steps: 759, steps per second: 130, episode reward: -116.295, mean reward: -0.153 [-100.000, 19.360], mean action: 1.777 [0.000, 3.000],  loss: 7.372237, mae: 34.918484, mean_q: 22.151521, mean_eps: 0.472690
  88397/150000: episode: 846, duration: 0.897s, episode steps: 132, steps per second: 147, episode reward: -25.019, mean reward: -0.190 [-100.000, 13.539], mean action: 1.644 [0.000, 3.000],  loss: 6.909422, mae: 35.021709, mean_q: 22.357846, mean_eps: 0.470017
  88654/150000: episode: 847, duration: 1.777s, episode steps: 257, steps per second: 145, episode reward: -160.905, mean reward: -0.626 [-100.000, 13.242], mean action: 1.661 [0.000, 3.000],  loss: 7.824690, mae: 34.905597, mean_q: 22.501442, mean_eps: 0.468850
  88752/150000: episode: 848, duration: 0.685s, episode steps:  98, steps per second: 143, episode reward: -266.744, mean reward: -2.722 [-100.000,  0.620], mean action: 1.735 [0.000, 3.000],  loss: 8.571570, mae: 34.837198, mean_q: 22.986872, mean_eps: 0.467785
  88841/150000: episode: 849, duration: 0.636s, episode steps:  89, steps per second: 140, episode reward: -337.787, mean reward: -3.795 [-100.000,  0.570], mean action: 1.730 [0.000, 3.000],  loss: 8.438253, mae: 34.886386, mean_q: 23.182469, mean_eps: 0.467224
  89324/150000: episode: 850, duration: 3.570s, episode steps: 483, steps per second: 135, episode reward: -208.763, mean reward: -0.432 [-100.000, 19.868], mean action: 1.770 [0.000, 3.000],  loss: 9.270411, mae: 34.945797, mean_q: 22.881412, mean_eps: 0.465508
  90324/150000: episode: 851, duration: 8.721s, episode steps: 1000, steps per second: 115, episode reward: 38.247, mean reward:  0.038 [-20.033, 22.405], mean action: 1.635 [0.000, 3.000],  loss: 7.847772, mae: 34.632139, mean_q: 23.462788, mean_eps: 0.461059
  90471/150000: episode: 852, duration: 1.148s, episode steps: 147, steps per second: 128, episode reward: -87.894, mean reward: -0.598 [-100.000,  3.466], mean action: 1.653 [0.000, 3.000],  loss: 8.178186, mae: 34.366293, mean_q: 23.026568, mean_eps: 0.457618
  91471/150000: episode: 853, duration: 8.130s, episode steps: 1000, steps per second: 123, episode reward: -74.430, mean reward: -0.074 [-22.376, 16.140], mean action: 1.751 [0.000, 3.000],  loss: 7.885833, mae: 34.275232, mean_q: 23.300263, mean_eps: 0.454177
  91974/150000: episode: 854, duration: 3.636s, episode steps: 503, steps per second: 138, episode reward: -68.803, mean reward: -0.137 [-100.000, 64.622], mean action: 1.720 [0.000, 3.000],  loss: 8.036730, mae: 33.865098, mean_q: 22.905469, mean_eps: 0.449668
  92089/150000: episode: 855, duration: 0.802s, episode steps: 115, steps per second: 143, episode reward: -52.325, mean reward: -0.455 [-100.000, 11.579], mean action: 1.626 [0.000, 3.000],  loss: 8.752930, mae: 34.014210, mean_q: 22.854601, mean_eps: 0.447814
  92191/150000: episode: 856, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -34.205, mean reward: -0.335 [-100.000,  4.844], mean action: 1.676 [0.000, 3.000],  loss: 8.468569, mae: 33.626166, mean_q: 22.659419, mean_eps: 0.447163
  93191/150000: episode: 857, duration: 8.044s, episode steps: 1000, steps per second: 124, episode reward: -16.665, mean reward: -0.017 [-22.370, 37.773], mean action: 1.818 [0.000, 3.000],  loss: 8.738808, mae: 33.365216, mean_q: 23.033527, mean_eps: 0.443857
  93477/150000: episode: 858, duration: 2.009s, episode steps: 286, steps per second: 142, episode reward: -219.898, mean reward: -0.769 [-100.000, 34.175], mean action: 1.556 [0.000, 3.000],  loss: 7.647557, mae: 32.774604, mean_q: 22.960178, mean_eps: 0.439999
  93681/150000: episode: 859, duration: 1.373s, episode steps: 204, steps per second: 149, episode reward: -80.095, mean reward: -0.393 [-100.000,  5.063], mean action: 1.789 [0.000, 3.000],  loss: 8.171220, mae: 33.024388, mean_q: 23.170986, mean_eps: 0.438529
  93800/150000: episode: 860, duration: 0.853s, episode steps: 119, steps per second: 139, episode reward: 22.319, mean reward:  0.188 [-100.000, 12.970], mean action: 1.824 [0.000, 3.000],  loss: 8.266218, mae: 32.886396, mean_q: 22.572926, mean_eps: 0.437560
  94800/150000: episode: 861, duration: 7.438s, episode steps: 1000, steps per second: 134, episode reward: 10.294, mean reward:  0.010 [-24.051, 25.001], mean action: 1.525 [0.000, 3.000],  loss: 8.343103, mae: 32.937948, mean_q: 24.025984, mean_eps: 0.434203
  95800/150000: episode: 862, duration: 8.027s, episode steps: 1000, steps per second: 125, episode reward: -11.130, mean reward: -0.011 [-21.004, 20.852], mean action: 1.510 [0.000, 3.000],  loss: 8.499094, mae: 32.243147, mean_q: 23.016518, mean_eps: 0.428203
  96800/150000: episode: 863, duration: 7.490s, episode steps: 1000, steps per second: 134, episode reward: -60.616, mean reward: -0.061 [-22.172, 26.005], mean action: 1.679 [0.000, 3.000],  loss: 8.386689, mae: 31.758982, mean_q: 22.958407, mean_eps: 0.422203
  97800/150000: episode: 864, duration: 7.497s, episode steps: 1000, steps per second: 133, episode reward: -21.918, mean reward: -0.022 [-12.616, 26.665], mean action: 1.631 [0.000, 3.000],  loss: 7.940961, mae: 31.395204, mean_q: 23.320692, mean_eps: 0.416203
  98800/150000: episode: 865, duration: 8.768s, episode steps: 1000, steps per second: 114, episode reward: -13.176, mean reward: -0.013 [-9.101, 12.974], mean action: 1.729 [0.000, 3.000],  loss: 8.123252, mae: 30.534930, mean_q: 23.244690, mean_eps: 0.410203
  99009/150000: episode: 866, duration: 1.464s, episode steps: 209, steps per second: 143, episode reward: -44.859, mean reward: -0.215 [-100.000, 52.220], mean action: 1.579 [0.000, 3.000],  loss: 7.151001, mae: 30.051914, mean_q: 23.205500, mean_eps: 0.406576
  99307/150000: episode: 867, duration: 2.098s, episode steps: 298, steps per second: 142, episode reward: -166.036, mean reward: -0.557 [-100.000, 18.946], mean action: 1.762 [0.000, 3.000],  loss: 7.398812, mae: 30.582721, mean_q: 24.413634, mean_eps: 0.405055
  99463/150000: episode: 868, duration: 1.127s, episode steps: 156, steps per second: 138, episode reward: -55.357, mean reward: -0.355 [-100.000,  9.588], mean action: 1.641 [0.000, 3.000],  loss: 7.815472, mae: 30.394103, mean_q: 23.955935, mean_eps: 0.403693
  99688/150000: episode: 869, duration: 1.579s, episode steps: 225, steps per second: 142, episode reward: -81.323, mean reward: -0.361 [-100.000, 19.505], mean action: 1.711 [0.000, 3.000],  loss: 8.012784, mae: 30.193883, mean_q: 23.806162, mean_eps: 0.402550
 100688/150000: episode: 870, duration: 7.606s, episode steps: 1000, steps per second: 131, episode reward: 21.667, mean reward:  0.022 [-19.946, 22.503], mean action: 1.704 [0.000, 3.000],  loss: 7.913500, mae: 30.137689, mean_q: 23.934411, mean_eps: 0.398875
 101688/150000: episode: 871, duration: 7.625s, episode steps: 1000, steps per second: 131, episode reward: -43.650, mean reward: -0.044 [-10.840, 21.634], mean action: 1.762 [0.000, 3.000],  loss: 8.510247, mae: 29.295404, mean_q: 23.718748, mean_eps: 0.392875
 101922/150000: episode: 872, duration: 1.632s, episode steps: 234, steps per second: 143, episode reward: -54.821, mean reward: -0.234 [-100.000, 19.985], mean action: 1.803 [0.000, 3.000],  loss: 7.573802, mae: 29.122369, mean_q: 23.542422, mean_eps: 0.389173
 102024/150000: episode: 873, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -68.267, mean reward: -0.669 [-100.000, 10.314], mean action: 1.618 [0.000, 3.000],  loss: 8.997639, mae: 29.078090, mean_q: 23.351493, mean_eps: 0.388165
 102160/150000: episode: 874, duration: 0.913s, episode steps: 136, steps per second: 149, episode reward: -59.234, mean reward: -0.436 [-100.000, 26.595], mean action: 1.419 [0.000, 3.000],  loss: 7.752003, mae: 28.978607, mean_q: 24.697773, mean_eps: 0.387451
 103160/150000: episode: 875, duration: 8.665s, episode steps: 1000, steps per second: 115, episode reward: 11.002, mean reward:  0.011 [-22.218, 22.509], mean action: 1.600 [0.000, 3.000],  loss: 7.974023, mae: 28.906004, mean_q: 24.049033, mean_eps: 0.384043
 103498/150000: episode: 876, duration: 2.571s, episode steps: 338, steps per second: 131, episode reward: -71.249, mean reward: -0.211 [-100.000, 11.763], mean action: 1.660 [0.000, 3.000],  loss: 7.098737, mae: 28.641694, mean_q: 23.908333, mean_eps: 0.380029
 103649/150000: episode: 877, duration: 1.093s, episode steps: 151, steps per second: 138, episode reward: 11.534, mean reward:  0.076 [-100.000, 15.443], mean action: 1.768 [0.000, 3.000],  loss: 8.318904, mae: 28.823324, mean_q: 23.972096, mean_eps: 0.378562
 104649/150000: episode: 878, duration: 7.851s, episode steps: 1000, steps per second: 127, episode reward: 54.639, mean reward:  0.055 [-22.467, 22.702], mean action: 1.743 [0.000, 3.000],  loss: 8.558579, mae: 28.622840, mean_q: 24.280774, mean_eps: 0.375109
 105649/150000: episode: 879, duration: 7.611s, episode steps: 1000, steps per second: 131, episode reward: 13.819, mean reward:  0.014 [-23.005, 23.032], mean action: 1.794 [0.000, 3.000],  loss: 8.627192, mae: 28.107960, mean_q: 24.443740, mean_eps: 0.369109
 106211/150000: episode: 880, duration: 4.220s, episode steps: 562, steps per second: 133, episode reward: -261.033, mean reward: -0.464 [-100.000, 27.729], mean action: 1.904 [0.000, 3.000],  loss: 7.992407, mae: 27.687328, mean_q: 24.649899, mean_eps: 0.364423
 107211/150000: episode: 881, duration: 7.884s, episode steps: 1000, steps per second: 127, episode reward: 111.282, mean reward:  0.111 [-20.133, 23.339], mean action: 1.333 [0.000, 3.000],  loss: 9.250638, mae: 27.293259, mean_q: 24.556780, mean_eps: 0.359737
 107674/150000: episode: 882, duration: 3.481s, episode steps: 463, steps per second: 133, episode reward: -182.889, mean reward: -0.395 [-100.000, 21.526], mean action: 1.933 [0.000, 3.000],  loss: 7.594004, mae: 26.714251, mean_q: 24.552011, mean_eps: 0.355348
 107805/150000: episode: 883, duration: 0.927s, episode steps: 131, steps per second: 141, episode reward: 26.818, mean reward:  0.205 [-100.000, 20.611], mean action: 1.656 [0.000, 3.000],  loss: 9.789029, mae: 26.755128, mean_q: 23.929877, mean_eps: 0.353566
 107912/150000: episode: 884, duration: 0.776s, episode steps: 107, steps per second: 138, episode reward: 44.363, mean reward:  0.415 [-100.000, 14.683], mean action: 1.841 [0.000, 3.000],  loss: 8.083629, mae: 26.474517, mean_q: 23.887819, mean_eps: 0.352852
 108396/150000: episode: 885, duration: 3.505s, episode steps: 484, steps per second: 138, episode reward: -117.111, mean reward: -0.242 [-100.000, 24.335], mean action: 1.919 [0.000, 3.000],  loss: 8.610033, mae: 26.292803, mean_q: 24.392386, mean_eps: 0.351079
 108701/150000: episode: 886, duration: 2.126s, episode steps: 305, steps per second: 143, episode reward: -57.654, mean reward: -0.189 [-100.000, 25.131], mean action: 1.646 [0.000, 3.000],  loss: 9.905664, mae: 26.272158, mean_q: 23.609871, mean_eps: 0.348712
 109701/150000: episode: 887, duration: 7.856s, episode steps: 1000, steps per second: 127, episode reward: 78.623, mean reward:  0.079 [-19.391, 22.791], mean action: 1.354 [0.000, 3.000],  loss: 9.043689, mae: 26.087902, mean_q: 24.523891, mean_eps: 0.344797
 110195/150000: episode: 888, duration: 3.524s, episode steps: 494, steps per second: 140, episode reward: -113.646, mean reward: -0.230 [-100.000, 25.524], mean action: 1.725 [0.000, 3.000],  loss: 9.574002, mae: 25.928770, mean_q: 24.634965, mean_eps: 0.340315
 110360/150000: episode: 889, duration: 1.162s, episode steps: 165, steps per second: 142, episode reward: -148.686, mean reward: -0.901 [-100.000, 10.148], mean action: 1.933 [0.000, 3.000],  loss: 8.809262, mae: 25.553641, mean_q: 24.872564, mean_eps: 0.338338
 111360/150000: episode: 890, duration: 7.503s, episode steps: 1000, steps per second: 133, episode reward: 95.900, mean reward:  0.096 [-23.020, 23.109], mean action: 1.202 [0.000, 3.000],  loss: 8.013693, mae: 25.682395, mean_q: 25.048202, mean_eps: 0.334843
 112360/150000: episode: 891, duration: 7.680s, episode steps: 1000, steps per second: 130, episode reward: 51.508, mean reward:  0.052 [-21.660, 26.954], mean action: 1.533 [0.000, 3.000],  loss: 8.232634, mae: 25.267229, mean_q: 25.072512, mean_eps: 0.328843
 113360/150000: episode: 892, duration: 7.492s, episode steps: 1000, steps per second: 133, episode reward: 51.635, mean reward:  0.052 [-20.285, 22.832], mean action: 1.300 [0.000, 3.000],  loss: 9.063558, mae: 24.859703, mean_q: 25.131332, mean_eps: 0.322843
 114360/150000: episode: 893, duration: 7.709s, episode steps: 1000, steps per second: 130, episode reward:  1.943, mean reward:  0.002 [-22.687, 23.312], mean action: 1.735 [0.000, 3.000],  loss: 8.272357, mae: 24.691043, mean_q: 25.324500, mean_eps: 0.316843
 115360/150000: episode: 894, duration: 8.088s, episode steps: 1000, steps per second: 124, episode reward: 110.981, mean reward:  0.111 [-23.365, 22.934], mean action: 1.251 [0.000, 3.000],  loss: 8.651311, mae: 24.630885, mean_q: 25.604355, mean_eps: 0.310843
 116360/150000: episode: 895, duration: 7.888s, episode steps: 1000, steps per second: 127, episode reward: 63.824, mean reward:  0.064 [-20.611, 22.846], mean action: 1.402 [0.000, 3.000],  loss: 8.428711, mae: 24.012449, mean_q: 25.172490, mean_eps: 0.304843
 117360/150000: episode: 896, duration: 7.481s, episode steps: 1000, steps per second: 134, episode reward: 60.606, mean reward:  0.061 [-19.814, 22.024], mean action: 2.267 [0.000, 3.000],  loss: 8.632892, mae: 23.657083, mean_q: 25.204981, mean_eps: 0.298843
 118360/150000: episode: 897, duration: 8.515s, episode steps: 1000, steps per second: 117, episode reward: 76.613, mean reward:  0.077 [-22.025, 23.124], mean action: 1.603 [0.000, 3.000],  loss: 8.977007, mae: 23.682635, mean_q: 25.482052, mean_eps: 0.292843
 119360/150000: episode: 898, duration: 7.605s, episode steps: 1000, steps per second: 131, episode reward: 153.488, mean reward:  0.153 [-24.100, 22.827], mean action: 1.089 [0.000, 3.000],  loss: 9.017034, mae: 23.155876, mean_q: 25.286232, mean_eps: 0.286843
 119901/150000: episode: 899, duration: 4.095s, episode steps: 541, steps per second: 132, episode reward: -204.692, mean reward: -0.378 [-100.000, 16.156], mean action: 1.750 [0.000, 3.000],  loss: 9.133801, mae: 22.828125, mean_q: 25.539530, mean_eps: 0.282220
 120901/150000: episode: 900, duration: 7.460s, episode steps: 1000, steps per second: 134, episode reward: 58.004, mean reward:  0.058 [-21.500, 23.964], mean action: 1.314 [0.000, 3.000],  loss: 8.508068, mae: 22.530595, mean_q: 25.537361, mean_eps: 0.277597
 121901/150000: episode: 901, duration: 7.886s, episode steps: 1000, steps per second: 127, episode reward: -11.128, mean reward: -0.011 [-21.814, 14.587], mean action: 1.711 [0.000, 3.000],  loss: 8.809993, mae: 22.079764, mean_q: 25.254574, mean_eps: 0.271597
 122901/150000: episode: 902, duration: 7.841s, episode steps: 1000, steps per second: 128, episode reward: 24.680, mean reward:  0.025 [-20.011, 19.386], mean action: 1.818 [0.000, 3.000],  loss: 8.716094, mae: 21.638868, mean_q: 24.768744, mean_eps: 0.265597
 123881/150000: episode: 903, duration: 7.217s, episode steps: 980, steps per second: 136, episode reward: -197.825, mean reward: -0.202 [-100.000, 22.613], mean action: 1.850 [0.000, 3.000],  loss: 7.595725, mae: 21.604142, mean_q: 25.030321, mean_eps: 0.259657
 124881/150000: episode: 904, duration: 8.429s, episode steps: 1000, steps per second: 119, episode reward: -28.336, mean reward: -0.028 [-20.179, 23.486], mean action: 1.794 [0.000, 3.000],  loss: 8.058826, mae: 21.355172, mean_q: 25.320868, mean_eps: 0.253717
 125881/150000: episode: 905, duration: 7.853s, episode steps: 1000, steps per second: 127, episode reward: 46.321, mean reward:  0.046 [-24.312, 22.691], mean action: 2.051 [0.000, 3.000],  loss: 8.031562, mae: 21.038520, mean_q: 25.058695, mean_eps: 0.247717
 126881/150000: episode: 906, duration: 7.679s, episode steps: 1000, steps per second: 130, episode reward: 103.817, mean reward:  0.104 [-23.198, 22.561], mean action: 1.330 [0.000, 3.000],  loss: 8.632450, mae: 21.150104, mean_q: 25.028578, mean_eps: 0.241717
 127856/150000: episode: 907, duration: 7.031s, episode steps: 975, steps per second: 139, episode reward: 206.585, mean reward:  0.212 [-22.595, 100.000], mean action: 1.887 [0.000, 3.000],  loss: 7.476966, mae: 20.532820, mean_q: 24.106286, mean_eps: 0.235792
 128856/150000: episode: 908, duration: 8.304s, episode steps: 1000, steps per second: 120, episode reward: 22.475, mean reward:  0.022 [-20.224, 22.733], mean action: 1.934 [0.000, 3.000],  loss: 7.536078, mae: 20.238007, mean_q: 24.037914, mean_eps: 0.229867
 129856/150000: episode: 909, duration: 8.035s, episode steps: 1000, steps per second: 124, episode reward: 90.706, mean reward:  0.091 [-22.682, 23.758], mean action: 1.189 [0.000, 3.000],  loss: 6.086863, mae: 20.401951, mean_q: 24.755484, mean_eps: 0.223867
 130856/150000: episode: 910, duration: 7.342s, episode steps: 1000, steps per second: 136, episode reward: 89.660, mean reward:  0.090 [-20.456, 23.384], mean action: 1.210 [0.000, 3.000],  loss: 6.127046, mae: 20.374796, mean_q: 25.083467, mean_eps: 0.217867
 131856/150000: episode: 911, duration: 7.511s, episode steps: 1000, steps per second: 133, episode reward: 97.790, mean reward:  0.098 [-22.839, 23.708], mean action: 1.152 [0.000, 3.000],  loss: 5.329500, mae: 20.272331, mean_q: 25.086568, mean_eps: 0.211867
 132501/150000: episode: 912, duration: 5.502s, episode steps: 645, steps per second: 117, episode reward: 222.210, mean reward:  0.345 [-19.947, 100.000], mean action: 1.290 [0.000, 3.000],  loss: 5.822323, mae: 20.024011, mean_q: 25.062293, mean_eps: 0.206932
 133501/150000: episode: 913, duration: 8.031s, episode steps: 1000, steps per second: 125, episode reward: 82.066, mean reward:  0.082 [-23.825, 21.473], mean action: 1.133 [0.000, 3.000],  loss: 5.479495, mae: 19.898757, mean_q: 24.928216, mean_eps: 0.201997
 134164/150000: episode: 914, duration: 4.959s, episode steps: 663, steps per second: 134, episode reward: 233.230, mean reward:  0.352 [-17.762, 100.000], mean action: 1.065 [0.000, 3.000],  loss: 4.992855, mae: 19.976328, mean_q: 25.013328, mean_eps: 0.197008
 134545/150000: episode: 915, duration: 2.682s, episode steps: 381, steps per second: 142, episode reward: 294.945, mean reward:  0.774 [-17.372, 100.000], mean action: 1.622 [0.000, 3.000],  loss: 4.593479, mae: 20.085063, mean_q: 25.242307, mean_eps: 0.193876
 135151/150000: episode: 916, duration: 4.389s, episode steps: 606, steps per second: 138, episode reward: 270.722, mean reward:  0.447 [-18.599, 100.000], mean action: 1.117 [0.000, 3.000],  loss: 5.393462, mae: 20.249019, mean_q: 25.449015, mean_eps: 0.190915
 136151/150000: episode: 917, duration: 7.263s, episode steps: 1000, steps per second: 138, episode reward: 107.641, mean reward:  0.108 [-19.281, 22.906], mean action: 1.149 [0.000, 3.000],  loss: 5.681535, mae: 20.313771, mean_q: 25.860807, mean_eps: 0.186097
 136935/150000: episode: 918, duration: 5.762s, episode steps: 784, steps per second: 136, episode reward: 191.095, mean reward:  0.244 [-18.168, 100.000], mean action: 2.111 [0.000, 3.000],  loss: 5.333574, mae: 20.450680, mean_q: 26.265476, mean_eps: 0.180745
 137558/150000: episode: 919, duration: 4.516s, episode steps: 623, steps per second: 138, episode reward: 306.144, mean reward:  0.491 [-24.075, 100.000], mean action: 0.957 [0.000, 3.000],  loss: 5.148395, mae: 20.255063, mean_q: 25.925680, mean_eps: 0.176524
 137968/150000: episode: 920, duration: 2.919s, episode steps: 410, steps per second: 140, episode reward: -164.841, mean reward: -0.402 [-100.000, 12.961], mean action: 1.756 [0.000, 3.000],  loss: 4.728936, mae: 19.863712, mean_q: 25.616634, mean_eps: 0.173425
 138968/150000: episode: 921, duration: 7.224s, episode steps: 1000, steps per second: 138, episode reward: -90.688, mean reward: -0.091 [-4.858,  4.691], mean action: 1.779 [0.000, 3.000],  loss: 4.436804, mae: 20.272071, mean_q: 26.149805, mean_eps: 0.169195
 139289/150000: episode: 922, duration: 2.243s, episode steps: 321, steps per second: 143, episode reward: 214.937, mean reward:  0.670 [-8.467, 100.000], mean action: 1.393 [0.000, 3.000],  loss: 5.613106, mae: 20.503319, mean_q: 26.722146, mean_eps: 0.165232
 140289/150000: episode: 923, duration: 7.516s, episode steps: 1000, steps per second: 133, episode reward: 31.412, mean reward:  0.031 [-19.471, 12.691], mean action: 1.564 [0.000, 3.000],  loss: 4.684790, mae: 20.248683, mean_q: 26.455072, mean_eps: 0.161269
 141289/150000: episode: 924, duration: 8.092s, episode steps: 1000, steps per second: 124, episode reward: 146.954, mean reward:  0.147 [-18.352, 23.763], mean action: 0.863 [0.000, 3.000],  loss: 4.643184, mae: 20.488609, mean_q: 27.059469, mean_eps: 0.155269
 141805/150000: episode: 925, duration: 3.979s, episode steps: 516, steps per second: 130, episode reward: 263.949, mean reward:  0.512 [-21.178, 100.000], mean action: 1.917 [0.000, 3.000],  loss: 5.247336, mae: 20.574942, mean_q: 27.116220, mean_eps: 0.150721
 142386/150000: episode: 926, duration: 4.916s, episode steps: 581, steps per second: 118, episode reward: 230.064, mean reward:  0.396 [-19.481, 100.000], mean action: 0.954 [0.000, 3.000],  loss: 4.819347, mae: 20.480150, mean_q: 26.944067, mean_eps: 0.147430
 142633/150000: episode: 927, duration: 1.923s, episode steps: 247, steps per second: 128, episode reward: 264.106, mean reward:  1.069 [-9.412, 100.000], mean action: 1.591 [0.000, 3.000],  loss: 3.832227, mae: 20.397477, mean_q: 26.960477, mean_eps: 0.144946
 143290/150000: episode: 928, duration: 5.243s, episode steps: 657, steps per second: 125, episode reward: 264.894, mean reward:  0.403 [-20.802, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 4.712239, mae: 20.676616, mean_q: 27.316226, mean_eps: 0.142234
 143426/150000: episode: 929, duration: 0.964s, episode steps: 136, steps per second: 141, episode reward: -5.396, mean reward: -0.040 [-100.000, 13.816], mean action: 1.971 [0.000, 3.000],  loss: 3.684033, mae: 20.610588, mean_q: 27.416432, mean_eps: 0.139855
 143771/150000: episode: 930, duration: 2.478s, episode steps: 345, steps per second: 139, episode reward: 236.485, mean reward:  0.685 [-14.019, 100.000], mean action: 1.522 [0.000, 3.000],  loss: 6.165199, mae: 20.828962, mean_q: 27.483780, mean_eps: 0.138412
 144713/150000: episode: 931, duration: 7.255s, episode steps: 942, steps per second: 130, episode reward: 140.280, mean reward:  0.149 [-20.089, 100.000], mean action: 1.304 [0.000, 3.000],  loss: 4.109369, mae: 20.768226, mean_q: 27.566711, mean_eps: 0.134551
 145456/150000: episode: 932, duration: 5.700s, episode steps: 743, steps per second: 130, episode reward: 174.815, mean reward:  0.235 [-20.721, 100.000], mean action: 1.857 [0.000, 3.000],  loss: 4.049153, mae: 20.871992, mean_q: 27.780278, mean_eps: 0.129496
 145878/150000: episode: 933, duration: 3.054s, episode steps: 422, steps per second: 138, episode reward: 254.777, mean reward:  0.604 [-10.768, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 4.593402, mae: 20.964077, mean_q: 27.897643, mean_eps: 0.126001
 146179/150000: episode: 934, duration: 2.141s, episode steps: 301, steps per second: 141, episode reward: 233.057, mean reward:  0.774 [-16.102, 100.000], mean action: 1.910 [0.000, 3.000],  loss: 4.409941, mae: 21.271796, mean_q: 28.286290, mean_eps: 0.123832
 146383/150000: episode: 935, duration: 1.413s, episode steps: 204, steps per second: 144, episode reward: 255.075, mean reward:  1.250 [-17.614, 100.000], mean action: 1.328 [0.000, 3.000],  loss: 3.273195, mae: 21.267406, mean_q: 28.337403, mean_eps: 0.122317
 146735/150000: episode: 936, duration: 2.506s, episode steps: 352, steps per second: 140, episode reward: 293.936, mean reward:  0.835 [-2.823, 100.000], mean action: 1.349 [0.000, 3.000],  loss: 4.273325, mae: 21.510608, mean_q: 28.628458, mean_eps: 0.120649
 146997/150000: episode: 937, duration: 1.847s, episode steps: 262, steps per second: 142, episode reward: 268.972, mean reward:  1.027 [-9.323, 100.000], mean action: 1.996 [0.000, 3.000],  loss: 2.696681, mae: 21.468177, mean_q: 28.636129, mean_eps: 0.118807
 147551/150000: episode: 938, duration: 3.961s, episode steps: 554, steps per second: 140, episode reward: 265.810, mean reward:  0.480 [-18.879, 100.000], mean action: 1.025 [0.000, 3.000],  loss: 5.001952, mae: 21.751230, mean_q: 28.929986, mean_eps: 0.116359
 147749/150000: episode: 939, duration: 1.354s, episode steps: 198, steps per second: 146, episode reward: 248.109, mean reward:  1.253 [-3.333, 100.000], mean action: 1.677 [0.000, 3.000],  loss: 4.176093, mae: 21.551781, mean_q: 28.628291, mean_eps: 0.114103
 148749/150000: episode: 940, duration: 8.264s, episode steps: 1000, steps per second: 121, episode reward: -16.379, mean reward: -0.016 [-17.039, 12.359], mean action: 1.586 [0.000, 3.000],  loss: 4.684753, mae: 21.869294, mean_q: 29.063121, mean_eps: 0.110509
 149240/150000: episode: 941, duration: 3.663s, episode steps: 491, steps per second: 134, episode reward: 242.262, mean reward:  0.493 [-19.211, 100.000], mean action: 1.132 [0.000, 3.000],  loss: 4.711354, mae: 21.938617, mean_q: 29.236764, mean_eps: 0.106036
 149626/150000: episode: 942, duration: 2.919s, episode steps: 386, steps per second: 132, episode reward: 263.739, mean reward:  0.683 [-15.139, 100.000], mean action: 1.137 [0.000, 3.000],  loss: 3.910424, mae: 22.089773, mean_q: 29.402481, mean_eps: 0.103405
 149862/150000: episode: 943, duration: 2.032s, episode steps: 236, steps per second: 116, episode reward: 231.513, mean reward:  0.981 [-14.715, 100.000], mean action: 1.585 [0.000, 3.000],  loss: 3.942204, mae: 21.693685, mean_q: 28.877046, mean_eps: 0.101539
done, took 1086.260 seconds
Testing for 5 episodes ...
Episode 1: reward: 254.280, steps: 309
Episode 2: reward: -24.383, steps: 1000
Episode 3: reward: 251.280, steps: 166
Episode 4: reward: 237.124, steps: 210
Episode 5: reward: -17.097, steps: 1000
Training for 150000 steps ...
     70/150000: episode: 1, duration: 0.098s, episode steps:  70, steps per second: 713, episode reward: -214.131, mean reward: -3.059 [-100.000,  5.700], mean action: 1.500 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    140/150000: episode: 2, duration: 0.325s, episode steps:  70, steps per second: 216, episode reward: -137.160, mean reward: -1.959 [-100.000,  7.950], mean action: 1.543 [0.000, 3.000],  loss: 3.966835, mae: 21.525545, mean_q: 28.860424, mean_eps: 0.999280
    209/150000: episode: 3, duration: 0.484s, episode steps:  69, steps per second: 143, episode reward: -109.945, mean reward: -1.593 [-100.000,  7.324], mean action: 1.609 [0.000, 3.000],  loss: 3.445681, mae: 22.255357, mean_q: 29.570408, mean_eps: 0.998956
    322/150000: episode: 4, duration: 0.766s, episode steps: 113, steps per second: 147, episode reward: -90.664, mean reward: -0.802 [-100.000, 14.063], mean action: 1.460 [0.000, 3.000],  loss: 6.035657, mae: 22.067842, mean_q: 29.299775, mean_eps: 0.998410
    438/150000: episode: 5, duration: 0.815s, episode steps: 116, steps per second: 142, episode reward: -354.031, mean reward: -3.052 [-100.000, 87.532], mean action: 1.681 [0.000, 3.000],  loss: 2.644319, mae: 22.248963, mean_q: 29.401115, mean_eps: 0.997723
    501/150000: episode: 6, duration: 0.434s, episode steps:  63, steps per second: 145, episode reward: -47.738, mean reward: -0.758 [-100.000, 15.887], mean action: 1.349 [0.000, 3.000],  loss: 2.365190, mae: 22.061697, mean_q: 29.059042, mean_eps: 0.997186
    562/150000: episode: 7, duration: 0.422s, episode steps:  61, steps per second: 145, episode reward: -120.326, mean reward: -1.973 [-100.000,  7.372], mean action: 1.377 [0.000, 3.000],  loss: 4.616094, mae: 23.262254, mean_q: 30.440248, mean_eps: 0.996814
    653/150000: episode: 8, duration: 0.620s, episode steps:  91, steps per second: 147, episode reward: -109.915, mean reward: -1.208 [-100.000,  9.065], mean action: 1.385 [0.000, 3.000],  loss: 4.664022, mae: 22.515237, mean_q: 28.913223, mean_eps: 0.996358
    731/150000: episode: 9, duration: 0.542s, episode steps:  78, steps per second: 144, episode reward: -113.987, mean reward: -1.461 [-100.000, 36.350], mean action: 1.526 [0.000, 3.000],  loss: 4.722318, mae: 22.196285, mean_q: 29.448945, mean_eps: 0.995851
    846/150000: episode: 10, duration: 0.793s, episode steps: 115, steps per second: 145, episode reward: -82.227, mean reward: -0.715 [-100.000,  7.470], mean action: 1.470 [0.000, 3.000],  loss: 11.479939, mae: 22.439232, mean_q: 29.299798, mean_eps: 0.995272
    964/150000: episode: 11, duration: 0.790s, episode steps: 118, steps per second: 149, episode reward: -33.598, mean reward: -0.285 [-100.000, 122.533], mean action: 1.678 [0.000, 3.000],  loss: 6.831785, mae: 22.598633, mean_q: 29.448862, mean_eps: 0.994573
   1046/150000: episode: 12, duration: 0.581s, episode steps:  82, steps per second: 141, episode reward: -20.923, mean reward: -0.255 [-100.000, 109.478], mean action: 1.439 [0.000, 3.000],  loss: 5.354471, mae: 22.213676, mean_q: 29.255189, mean_eps: 0.993973
   1108/150000: episode: 13, duration: 0.423s, episode steps:  62, steps per second: 147, episode reward: -257.252, mean reward: -4.149 [-100.000,  6.267], mean action: 1.452 [0.000, 3.000],  loss: 9.711126, mae: 22.910627, mean_q: 29.744029, mean_eps: 0.993541
   1196/150000: episode: 14, duration: 0.604s, episode steps:  88, steps per second: 146, episode reward: -138.516, mean reward: -1.574 [-100.000,  6.821], mean action: 1.489 [0.000, 3.000],  loss: 16.090516, mae: 22.992465, mean_q: 29.992342, mean_eps: 0.993091
   1260/150000: episode: 15, duration: 0.430s, episode steps:  64, steps per second: 149, episode reward: -76.615, mean reward: -1.197 [-100.000,  6.196], mean action: 1.312 [0.000, 3.000],  loss: 14.099774, mae: 23.482585, mean_q: 30.177258, mean_eps: 0.992635
   1340/150000: episode: 16, duration: 0.564s, episode steps:  80, steps per second: 142, episode reward: -89.578, mean reward: -1.120 [-100.000,  9.447], mean action: 1.438 [0.000, 3.000],  loss: 5.848149, mae: 23.113494, mean_q: 30.490964, mean_eps: 0.992203
   1405/150000: episode: 17, duration: 0.448s, episode steps:  65, steps per second: 145, episode reward: -144.241, mean reward: -2.219 [-100.000, 50.313], mean action: 1.508 [0.000, 3.000],  loss: 5.736251, mae: 23.018704, mean_q: 30.465202, mean_eps: 0.991768
   1512/150000: episode: 18, duration: 0.739s, episode steps: 107, steps per second: 145, episode reward: -483.499, mean reward: -4.519 [-100.000,  5.911], mean action: 1.607 [0.000, 3.000],  loss: 7.842734, mae: 23.101111, mean_q: 30.117475, mean_eps: 0.991252
   1616/150000: episode: 19, duration: 0.710s, episode steps: 104, steps per second: 146, episode reward: -17.815, mean reward: -0.171 [-100.000, 45.413], mean action: 1.644 [0.000, 3.000],  loss: 9.455496, mae: 23.319972, mean_q: 30.435493, mean_eps: 0.990619
   1703/150000: episode: 20, duration: 0.600s, episode steps:  87, steps per second: 145, episode reward: -99.285, mean reward: -1.141 [-100.000, 13.818], mean action: 1.471 [0.000, 3.000],  loss: 14.237606, mae: 23.301476, mean_q: 30.137550, mean_eps: 0.990046
   1797/150000: episode: 21, duration: 0.639s, episode steps:  94, steps per second: 147, episode reward: -164.081, mean reward: -1.746 [-100.000, 10.115], mean action: 1.553 [0.000, 3.000],  loss: 11.740828, mae: 23.270653, mean_q: 30.054501, mean_eps: 0.989503
   1877/150000: episode: 22, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -82.535, mean reward: -1.032 [-100.000,  7.921], mean action: 1.387 [0.000, 3.000],  loss: 6.939896, mae: 23.426186, mean_q: 30.550876, mean_eps: 0.988981
   1994/150000: episode: 23, duration: 0.815s, episode steps: 117, steps per second: 143, episode reward: -144.701, mean reward: -1.237 [-100.000, 21.729], mean action: 1.325 [0.000, 3.000],  loss: 9.312768, mae: 23.344827, mean_q: 29.962923, mean_eps: 0.988390
   2100/150000: episode: 24, duration: 0.720s, episode steps: 106, steps per second: 147, episode reward: -90.454, mean reward: -0.853 [-100.000,  6.575], mean action: 1.406 [0.000, 3.000],  loss: 5.970687, mae: 23.107186, mean_q: 29.991162, mean_eps: 0.987721
   2228/150000: episode: 25, duration: 0.874s, episode steps: 128, steps per second: 146, episode reward: -206.188, mean reward: -1.611 [-100.000, 67.300], mean action: 1.422 [0.000, 3.000],  loss: 6.822995, mae: 23.210591, mean_q: 30.135873, mean_eps: 0.987019
   2346/150000: episode: 26, duration: 0.820s, episode steps: 118, steps per second: 144, episode reward: -210.838, mean reward: -1.787 [-100.000,  5.937], mean action: 1.407 [0.000, 3.000],  loss: 6.258398, mae: 23.743886, mean_q: 31.006685, mean_eps: 0.986281
   2446/150000: episode: 27, duration: 0.681s, episode steps: 100, steps per second: 147, episode reward: -368.147, mean reward: -3.681 [-100.000,  8.095], mean action: 1.480 [0.000, 3.000],  loss: 10.563498, mae: 23.413763, mean_q: 30.795859, mean_eps: 0.985627
   2548/150000: episode: 28, duration: 0.703s, episode steps: 102, steps per second: 145, episode reward: -104.087, mean reward: -1.020 [-100.000,  8.119], mean action: 1.539 [0.000, 3.000],  loss: 10.334740, mae: 23.914572, mean_q: 31.198934, mean_eps: 0.985021
   2629/150000: episode: 29, duration: 0.588s, episode steps:  81, steps per second: 138, episode reward: -130.675, mean reward: -1.613 [-100.000,  9.159], mean action: 1.815 [0.000, 3.000],  loss: 11.206601, mae: 23.986180, mean_q: 31.374326, mean_eps: 0.984472
   2689/150000: episode: 30, duration: 0.405s, episode steps:  60, steps per second: 148, episode reward: -130.960, mean reward: -2.183 [-100.000, 15.497], mean action: 1.433 [0.000, 3.000],  loss: 8.005285, mae: 23.307518, mean_q: 30.582744, mean_eps: 0.984049
   2820/150000: episode: 31, duration: 0.891s, episode steps: 131, steps per second: 147, episode reward: -233.390, mean reward: -1.782 [-100.000, 12.192], mean action: 1.328 [0.000, 3.000],  loss: 6.476997, mae: 23.490670, mean_q: 30.509601, mean_eps: 0.983476
   2884/150000: episode: 32, duration: 0.436s, episode steps:  64, steps per second: 147, episode reward: -95.294, mean reward: -1.489 [-100.000,  8.788], mean action: 1.750 [0.000, 3.000],  loss: 6.790817, mae: 23.666380, mean_q: 30.644854, mean_eps: 0.982891
   2958/150000: episode: 33, duration: 0.515s, episode steps:  74, steps per second: 144, episode reward: -137.824, mean reward: -1.862 [-100.000,  8.854], mean action: 1.459 [0.000, 3.000],  loss: 13.590689, mae: 24.217871, mean_q: 30.916127, mean_eps: 0.982477
   3041/150000: episode: 34, duration: 0.564s, episode steps:  83, steps per second: 147, episode reward: -347.257, mean reward: -4.184 [-100.000, 27.167], mean action: 1.627 [0.000, 3.000],  loss: 10.028717, mae: 24.007705, mean_q: 31.466178, mean_eps: 0.982006
   3127/150000: episode: 35, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -341.155, mean reward: -3.967 [-100.000, 15.423], mean action: 1.419 [0.000, 3.000],  loss: 10.925684, mae: 24.268061, mean_q: 30.989759, mean_eps: 0.981499
   3194/150000: episode: 36, duration: 0.453s, episode steps:  67, steps per second: 148, episode reward: -151.633, mean reward: -2.263 [-100.000,  7.462], mean action: 1.433 [0.000, 3.000],  loss: 7.422144, mae: 24.095904, mean_q: 30.903521, mean_eps: 0.981040
   3296/150000: episode: 37, duration: 0.706s, episode steps: 102, steps per second: 144, episode reward: -106.700, mean reward: -1.046 [-100.000,  6.639], mean action: 1.569 [0.000, 3.000],  loss: 10.819191, mae: 24.019809, mean_q: 31.217621, mean_eps: 0.980533
   3403/150000: episode: 38, duration: 0.728s, episode steps: 107, steps per second: 147, episode reward: -266.847, mean reward: -2.494 [-100.000, 70.825], mean action: 1.561 [0.000, 3.000],  loss: 7.172635, mae: 23.738739, mean_q: 30.568939, mean_eps: 0.979906
   3521/150000: episode: 39, duration: 0.794s, episode steps: 118, steps per second: 149, episode reward: -378.710, mean reward: -3.209 [-100.000,  1.225], mean action: 1.492 [0.000, 3.000],  loss: 8.173313, mae: 24.101447, mean_q: 31.202206, mean_eps: 0.979231
   3614/150000: episode: 40, duration: 0.676s, episode steps:  93, steps per second: 138, episode reward: -161.121, mean reward: -1.732 [-100.000,  4.878], mean action: 1.538 [0.000, 3.000],  loss: 9.489903, mae: 23.875273, mean_q: 31.319747, mean_eps: 0.978598
   3713/150000: episode: 41, duration: 0.672s, episode steps:  99, steps per second: 147, episode reward: -181.115, mean reward: -1.829 [-100.000,  5.769], mean action: 1.333 [0.000, 3.000],  loss: 7.792505, mae: 23.688266, mean_q: 30.528827, mean_eps: 0.978022
   3784/150000: episode: 42, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -82.633, mean reward: -1.164 [-100.000, 17.868], mean action: 1.577 [0.000, 3.000],  loss: 6.911911, mae: 24.203166, mean_q: 31.538226, mean_eps: 0.977512
   3853/150000: episode: 43, duration: 0.560s, episode steps:  69, steps per second: 123, episode reward: -175.824, mean reward: -2.548 [-100.000,  8.402], mean action: 1.536 [0.000, 3.000],  loss: 8.584153, mae: 24.082217, mean_q: 31.555337, mean_eps: 0.977092
   3938/150000: episode: 44, duration: 0.647s, episode steps:  85, steps per second: 131, episode reward: -432.144, mean reward: -5.084 [-100.000, 101.474], mean action: 1.412 [0.000, 3.000],  loss: 5.626490, mae: 24.227551, mean_q: 31.048519, mean_eps: 0.976630
   4024/150000: episode: 45, duration: 0.619s, episode steps:  86, steps per second: 139, episode reward: -117.864, mean reward: -1.371 [-100.000, 30.312], mean action: 1.349 [0.000, 3.000],  loss: 11.901790, mae: 23.906878, mean_q: 30.752713, mean_eps: 0.976117
   4093/150000: episode: 46, duration: 0.503s, episode steps:  69, steps per second: 137, episode reward: -136.265, mean reward: -1.975 [-100.000,  6.030], mean action: 1.522 [0.000, 3.000],  loss: 8.428414, mae: 24.192628, mean_q: 31.198633, mean_eps: 0.975652
   4175/150000: episode: 47, duration: 0.610s, episode steps:  82, steps per second: 134, episode reward: -190.799, mean reward: -2.327 [-100.000,  6.661], mean action: 1.402 [0.000, 3.000],  loss: 14.227924, mae: 23.721753, mean_q: 30.258988, mean_eps: 0.975199
   4238/150000: episode: 48, duration: 0.474s, episode steps:  63, steps per second: 133, episode reward: -90.861, mean reward: -1.442 [-100.000, 12.130], mean action: 1.619 [0.000, 3.000],  loss: 10.900760, mae: 23.765830, mean_q: 30.237881, mean_eps: 0.974764
   4316/150000: episode: 49, duration: 0.559s, episode steps:  78, steps per second: 140, episode reward: -74.341, mean reward: -0.953 [-100.000, 11.978], mean action: 1.692 [0.000, 3.000],  loss: 22.033944, mae: 24.341250, mean_q: 30.956799, mean_eps: 0.974341
   4397/150000: episode: 50, duration: 0.565s, episode steps:  81, steps per second: 143, episode reward: -308.900, mean reward: -3.814 [-100.000,  3.341], mean action: 1.284 [0.000, 3.000],  loss: 25.713616, mae: 24.398172, mean_q: 31.102203, mean_eps: 0.973864
   4531/150000: episode: 51, duration: 0.963s, episode steps: 134, steps per second: 139, episode reward: -49.596, mean reward: -0.370 [-100.000, 14.729], mean action: 1.552 [0.000, 3.000],  loss: 11.106859, mae: 23.783079, mean_q: 30.866801, mean_eps: 0.973219
   4593/150000: episode: 52, duration: 0.417s, episode steps:  62, steps per second: 149, episode reward: -97.279, mean reward: -1.569 [-100.000,  6.054], mean action: 1.726 [0.000, 3.000],  loss: 8.394786, mae: 24.401028, mean_q: 31.511581, mean_eps: 0.972631
   4734/150000: episode: 53, duration: 0.959s, episode steps: 141, steps per second: 147, episode reward: -61.213, mean reward: -0.434 [-100.000,  7.342], mean action: 1.518 [0.000, 3.000],  loss: 9.496592, mae: 24.201983, mean_q: 30.836621, mean_eps: 0.972022
   4813/150000: episode: 54, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward: -144.283, mean reward: -1.826 [-100.000,  6.828], mean action: 1.329 [0.000, 3.000],  loss: 8.082388, mae: 24.411643, mean_q: 31.093838, mean_eps: 0.971362
   4894/150000: episode: 55, duration: 0.545s, episode steps:  81, steps per second: 149, episode reward: -125.478, mean reward: -1.549 [-100.000,  7.773], mean action: 1.160 [0.000, 3.000],  loss: 5.768421, mae: 24.287408, mean_q: 31.929866, mean_eps: 0.970882
   5019/150000: episode: 56, duration: 0.850s, episode steps: 125, steps per second: 147, episode reward: -100.352, mean reward: -0.803 [-100.000, 91.480], mean action: 1.680 [0.000, 3.000],  loss: 13.251556, mae: 24.459466, mean_q: 31.342091, mean_eps: 0.970264
   5080/150000: episode: 57, duration: 0.434s, episode steps:  61, steps per second: 141, episode reward: -76.034, mean reward: -1.246 [-100.000, 22.965], mean action: 1.852 [0.000, 3.000],  loss: 24.258510, mae: 24.617763, mean_q: 31.505391, mean_eps: 0.969706
   5157/150000: episode: 58, duration: 0.525s, episode steps:  77, steps per second: 147, episode reward: -100.190, mean reward: -1.301 [-100.000, 30.132], mean action: 1.623 [0.000, 3.000],  loss: 8.237280, mae: 24.430608, mean_q: 31.536707, mean_eps: 0.969292
   5250/150000: episode: 59, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -174.249, mean reward: -1.874 [-100.000,  9.147], mean action: 1.484 [0.000, 3.000],  loss: 14.850460, mae: 24.642460, mean_q: 31.311190, mean_eps: 0.968782
   5324/150000: episode: 60, duration: 0.497s, episode steps:  74, steps per second: 149, episode reward: -80.010, mean reward: -1.081 [-100.000,  8.937], mean action: 1.500 [0.000, 3.000],  loss: 9.886791, mae: 24.736484, mean_q: 31.316022, mean_eps: 0.968281
   5400/150000: episode: 61, duration: 0.533s, episode steps:  76, steps per second: 143, episode reward: -201.692, mean reward: -2.654 [-100.000,  5.263], mean action: 1.474 [0.000, 3.000],  loss: 10.259147, mae: 24.627619, mean_q: 31.723081, mean_eps: 0.967831
   5494/150000: episode: 62, duration: 0.645s, episode steps:  94, steps per second: 146, episode reward: -8.540, mean reward: -0.091 [-100.000, 117.569], mean action: 1.479 [0.000, 3.000],  loss: 10.604564, mae: 24.798866, mean_q: 31.077337, mean_eps: 0.967321
   5588/150000: episode: 63, duration: 0.636s, episode steps:  94, steps per second: 148, episode reward: -134.760, mean reward: -1.434 [-100.000, 10.914], mean action: 1.606 [0.000, 3.000],  loss: 10.083960, mae: 25.121959, mean_q: 31.886709, mean_eps: 0.966757
   5662/150000: episode: 64, duration: 0.500s, episode steps:  74, steps per second: 148, episode reward: -85.425, mean reward: -1.154 [-100.000, 14.796], mean action: 1.378 [0.000, 3.000],  loss: 7.636148, mae: 24.641449, mean_q: 31.104801, mean_eps: 0.966253
   5765/150000: episode: 65, duration: 0.716s, episode steps: 103, steps per second: 144, episode reward: -113.916, mean reward: -1.106 [-100.000, 10.094], mean action: 1.505 [0.000, 3.000],  loss: 10.087862, mae: 25.032555, mean_q: 31.483959, mean_eps: 0.965722
   5838/150000: episode: 66, duration: 0.496s, episode steps:  73, steps per second: 147, episode reward: -62.054, mean reward: -0.850 [-100.000, 16.089], mean action: 1.603 [0.000, 3.000],  loss: 9.190693, mae: 24.536937, mean_q: 31.271810, mean_eps: 0.965194
   5899/150000: episode: 67, duration: 0.417s, episode steps:  61, steps per second: 146, episode reward: -60.451, mean reward: -0.991 [-100.000, 13.700], mean action: 1.639 [0.000, 3.000],  loss: 4.015522, mae: 25.269704, mean_q: 32.114213, mean_eps: 0.964792
   6050/150000: episode: 68, duration: 1.027s, episode steps: 151, steps per second: 147, episode reward: -82.913, mean reward: -0.549 [-100.000, 64.269], mean action: 1.483 [0.000, 3.000],  loss: 12.763364, mae: 25.396303, mean_q: 32.342182, mean_eps: 0.964156
   6118/150000: episode: 69, duration: 0.460s, episode steps:  68, steps per second: 148, episode reward: -77.881, mean reward: -1.145 [-100.000,  7.080], mean action: 1.691 [0.000, 3.000],  loss: 7.333560, mae: 25.761267, mean_q: 33.242162, mean_eps: 0.963499
   6229/150000: episode: 70, duration: 0.758s, episode steps: 111, steps per second: 146, episode reward: -46.694, mean reward: -0.421 [-100.000, 104.641], mean action: 1.396 [0.000, 3.000],  loss: 16.574857, mae: 25.948400, mean_q: 33.329046, mean_eps: 0.962962
   6360/150000: episode: 71, duration: 0.894s, episode steps: 131, steps per second: 147, episode reward: -172.268, mean reward: -1.315 [-100.000, 12.155], mean action: 1.412 [0.000, 3.000],  loss: 14.401880, mae: 25.732068, mean_q: 33.147547, mean_eps: 0.962236
   6429/150000: episode: 72, duration: 0.474s, episode steps:  69, steps per second: 146, episode reward: -114.422, mean reward: -1.658 [-100.000, 11.836], mean action: 1.594 [0.000, 3.000],  loss: 19.553273, mae: 25.350546, mean_q: 31.964047, mean_eps: 0.961636
   6545/150000: episode: 73, duration: 0.790s, episode steps: 116, steps per second: 147, episode reward: -195.844, mean reward: -1.688 [-100.000, 24.028], mean action: 1.621 [0.000, 3.000],  loss: 12.541262, mae: 26.145281, mean_q: 33.640220, mean_eps: 0.961081
   6657/150000: episode: 74, duration: 0.793s, episode steps: 112, steps per second: 141, episode reward: -160.680, mean reward: -1.435 [-100.000,  4.615], mean action: 1.536 [0.000, 3.000],  loss: 6.948746, mae: 25.861335, mean_q: 33.317517, mean_eps: 0.960397
   6759/150000: episode: 75, duration: 0.696s, episode steps: 102, steps per second: 147, episode reward: -85.100, mean reward: -0.834 [-100.000, 17.090], mean action: 1.588 [0.000, 3.000],  loss: 13.662995, mae: 26.690870, mean_q: 33.322990, mean_eps: 0.959755
   6871/150000: episode: 76, duration: 0.760s, episode steps: 112, steps per second: 147, episode reward: -148.289, mean reward: -1.324 [-100.000, 12.826], mean action: 1.670 [0.000, 3.000],  loss: 14.127606, mae: 26.832665, mean_q: 34.068117, mean_eps: 0.959113
   6935/150000: episode: 77, duration: 0.431s, episode steps:  64, steps per second: 149, episode reward: -102.495, mean reward: -1.601 [-100.000, 11.797], mean action: 1.672 [0.000, 3.000],  loss: 14.119585, mae: 26.620224, mean_q: 33.740432, mean_eps: 0.958585
   7024/150000: episode: 78, duration: 0.610s, episode steps:  89, steps per second: 146, episode reward: -79.912, mean reward: -0.898 [-100.000, 18.446], mean action: 1.404 [0.000, 3.000],  loss: 7.758983, mae: 26.510221, mean_q: 33.701421, mean_eps: 0.958126
   7134/150000: episode: 79, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -87.106, mean reward: -0.792 [-100.000,  8.006], mean action: 1.491 [0.000, 3.000],  loss: 13.241679, mae: 26.164615, mean_q: 33.000128, mean_eps: 0.957529
   7240/150000: episode: 80, duration: 0.719s, episode steps: 106, steps per second: 147, episode reward: -192.132, mean reward: -1.813 [-100.000, 17.581], mean action: 1.547 [0.000, 3.000],  loss: 9.298275, mae: 26.593414, mean_q: 33.787529, mean_eps: 0.956881
   7307/150000: episode: 81, duration: 0.470s, episode steps:  67, steps per second: 143, episode reward: -84.075, mean reward: -1.255 [-100.000,  5.681], mean action: 1.582 [0.000, 3.000],  loss: 11.150749, mae: 25.975639, mean_q: 33.412098, mean_eps: 0.956362
   7379/150000: episode: 82, duration: 0.489s, episode steps:  72, steps per second: 147, episode reward: -107.972, mean reward: -1.500 [-100.000, 12.226], mean action: 1.472 [0.000, 3.000],  loss: 9.146341, mae: 25.979185, mean_q: 33.197208, mean_eps: 0.955945
   7445/150000: episode: 83, duration: 0.461s, episode steps:  66, steps per second: 143, episode reward: -78.513, mean reward: -1.190 [-100.000,  7.239], mean action: 1.697 [0.000, 3.000],  loss: 10.311180, mae: 26.119520, mean_q: 33.466929, mean_eps: 0.955531
   7524/150000: episode: 84, duration: 0.535s, episode steps:  79, steps per second: 148, episode reward: -90.486, mean reward: -1.145 [-100.000, 21.248], mean action: 1.506 [0.000, 3.000],  loss: 11.943419, mae: 27.174928, mean_q: 34.707063, mean_eps: 0.955096
   7600/150000: episode: 85, duration: 0.522s, episode steps:  76, steps per second: 146, episode reward: -113.200, mean reward: -1.489 [-100.000, 18.018], mean action: 1.513 [0.000, 3.000],  loss: 11.413086, mae: 26.566774, mean_q: 33.761859, mean_eps: 0.954631
   7728/150000: episode: 86, duration: 0.872s, episode steps: 128, steps per second: 147, episode reward: -126.992, mean reward: -0.992 [-100.000, 17.374], mean action: 1.391 [0.000, 3.000],  loss: 13.277048, mae: 26.607703, mean_q: 33.649766, mean_eps: 0.954019
   7795/150000: episode: 87, duration: 0.462s, episode steps:  67, steps per second: 145, episode reward: -164.333, mean reward: -2.453 [-100.000, 26.076], mean action: 1.731 [0.000, 3.000],  loss: 12.288463, mae: 27.074834, mean_q: 34.517609, mean_eps: 0.953434
   7891/150000: episode: 88, duration: 0.669s, episode steps:  96, steps per second: 144, episode reward: -270.335, mean reward: -2.816 [-100.000,  0.231], mean action: 1.531 [0.000, 3.000],  loss: 15.061063, mae: 26.986652, mean_q: 34.557762, mean_eps: 0.952945
   8013/150000: episode: 89, duration: 0.902s, episode steps: 122, steps per second: 135, episode reward: -294.207, mean reward: -2.412 [-100.000,  3.424], mean action: 1.680 [0.000, 3.000],  loss: 8.644246, mae: 27.130582, mean_q: 34.178240, mean_eps: 0.952291
   8092/150000: episode: 90, duration: 0.614s, episode steps:  79, steps per second: 129, episode reward: -28.494, mean reward: -0.361 [-100.000, 13.908], mean action: 1.557 [0.000, 3.000],  loss: 11.675526, mae: 26.851062, mean_q: 33.966258, mean_eps: 0.951688
   8233/150000: episode: 91, duration: 1.089s, episode steps: 141, steps per second: 130, episode reward: -157.032, mean reward: -1.114 [-100.000, 13.972], mean action: 1.376 [0.000, 3.000],  loss: 13.822443, mae: 27.042659, mean_q: 34.550324, mean_eps: 0.951028
   8302/150000: episode: 92, duration: 0.533s, episode steps:  69, steps per second: 129, episode reward: -93.329, mean reward: -1.353 [-100.000,  9.708], mean action: 1.435 [0.000, 3.000],  loss: 12.293773, mae: 27.710546, mean_q: 34.691378, mean_eps: 0.950398
   8376/150000: episode: 93, duration: 0.520s, episode steps:  74, steps per second: 142, episode reward: -113.862, mean reward: -1.539 [-100.000,  7.056], mean action: 1.757 [0.000, 3.000],  loss: 9.060133, mae: 26.449012, mean_q: 33.870458, mean_eps: 0.949969
   8451/150000: episode: 94, duration: 0.505s, episode steps:  75, steps per second: 148, episode reward: -59.772, mean reward: -0.797 [-100.000,  7.270], mean action: 1.440 [0.000, 3.000],  loss: 13.954339, mae: 27.087738, mean_q: 34.252646, mean_eps: 0.949522
   8528/150000: episode: 95, duration: 0.563s, episode steps:  77, steps per second: 137, episode reward: -147.299, mean reward: -1.913 [-100.000,  5.152], mean action: 1.403 [0.000, 3.000],  loss: 13.127685, mae: 27.540400, mean_q: 34.672655, mean_eps: 0.949066
   8605/150000: episode: 96, duration: 0.537s, episode steps:  77, steps per second: 143, episode reward: -143.096, mean reward: -1.858 [-100.000,  5.936], mean action: 1.390 [0.000, 3.000],  loss: 6.771933, mae: 27.171387, mean_q: 34.434333, mean_eps: 0.948604
   8724/150000: episode: 97, duration: 0.807s, episode steps: 119, steps per second: 147, episode reward: -128.790, mean reward: -1.082 [-100.000,  5.510], mean action: 1.546 [0.000, 3.000],  loss: 7.591510, mae: 27.318931, mean_q: 34.756508, mean_eps: 0.948016
   8794/150000: episode: 98, duration: 0.487s, episode steps:  70, steps per second: 144, episode reward: -70.264, mean reward: -1.004 [-100.000,  8.076], mean action: 1.786 [0.000, 3.000],  loss: 13.759727, mae: 27.401782, mean_q: 34.834237, mean_eps: 0.947449
   8873/150000: episode: 99, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -105.898, mean reward: -1.340 [-100.000,  9.728], mean action: 1.380 [0.000, 3.000],  loss: 9.371805, mae: 26.869653, mean_q: 33.670387, mean_eps: 0.947002
   8941/150000: episode: 100, duration: 0.464s, episode steps:  68, steps per second: 147, episode reward: -59.117, mean reward: -0.869 [-100.000,  8.023], mean action: 1.706 [0.000, 3.000],  loss: 8.898139, mae: 27.375742, mean_q: 34.489654, mean_eps: 0.946561
   9050/150000: episode: 101, duration: 0.740s, episode steps: 109, steps per second: 147, episode reward: -107.452, mean reward: -0.986 [-100.000,  9.771], mean action: 1.725 [0.000, 3.000],  loss: 15.969349, mae: 28.109758, mean_q: 34.960275, mean_eps: 0.946030
   9174/150000: episode: 102, duration: 0.890s, episode steps: 124, steps per second: 139, episode reward: -117.464, mean reward: -0.947 [-100.000,  6.532], mean action: 1.476 [0.000, 3.000],  loss: 11.915902, mae: 28.507920, mean_q: 35.280598, mean_eps: 0.945331
   9255/150000: episode: 103, duration: 0.569s, episode steps:  81, steps per second: 142, episode reward: -96.936, mean reward: -1.197 [-100.000, 22.732], mean action: 1.543 [0.000, 3.000],  loss: 12.575627, mae: 27.940782, mean_q: 35.285360, mean_eps: 0.944716
   9325/150000: episode: 104, duration: 0.475s, episode steps:  70, steps per second: 147, episode reward: -116.960, mean reward: -1.671 [-100.000,  5.254], mean action: 1.471 [0.000, 3.000],  loss: 16.150001, mae: 28.248133, mean_q: 35.682676, mean_eps: 0.944263
   9430/150000: episode: 105, duration: 0.746s, episode steps: 105, steps per second: 141, episode reward: -122.463, mean reward: -1.166 [-100.000,  8.047], mean action: 1.571 [0.000, 3.000],  loss: 13.857941, mae: 28.678934, mean_q: 36.476642, mean_eps: 0.943738
   9507/150000: episode: 106, duration: 0.546s, episode steps:  77, steps per second: 141, episode reward: -81.733, mean reward: -1.061 [-100.000,  7.165], mean action: 1.545 [0.000, 3.000],  loss: 13.196680, mae: 28.367124, mean_q: 35.795257, mean_eps: 0.943192
   9608/150000: episode: 107, duration: 0.687s, episode steps: 101, steps per second: 147, episode reward: -75.820, mean reward: -0.751 [-100.000, 11.797], mean action: 1.446 [0.000, 3.000],  loss: 7.986235, mae: 28.765347, mean_q: 36.085107, mean_eps: 0.942658
   9714/150000: episode: 108, duration: 0.735s, episode steps: 106, steps per second: 144, episode reward: -176.622, mean reward: -1.666 [-100.000,  9.541], mean action: 1.632 [0.000, 3.000],  loss: 10.846868, mae: 29.117469, mean_q: 35.893661, mean_eps: 0.942037
   9807/150000: episode: 109, duration: 0.663s, episode steps:  93, steps per second: 140, episode reward: -92.749, mean reward: -0.997 [-100.000,  7.363], mean action: 1.452 [0.000, 3.000],  loss: 13.111487, mae: 28.795348, mean_q: 36.339052, mean_eps: 0.941440
   9893/150000: episode: 110, duration: 0.605s, episode steps:  86, steps per second: 142, episode reward: -122.098, mean reward: -1.420 [-100.000, 10.947], mean action: 1.500 [0.000, 3.000],  loss: 10.134576, mae: 28.384299, mean_q: 35.539460, mean_eps: 0.940903
  10010/150000: episode: 111, duration: 0.821s, episode steps: 117, steps per second: 143, episode reward: -165.545, mean reward: -1.415 [-100.000,  7.537], mean action: 1.667 [0.000, 3.000],  loss: 10.146151, mae: 28.609309, mean_q: 35.664919, mean_eps: 0.940294
  10109/150000: episode: 112, duration: 0.716s, episode steps:  99, steps per second: 138, episode reward: -114.456, mean reward: -1.156 [-100.000, 15.081], mean action: 1.616 [0.000, 3.000],  loss: 9.420235, mae: 28.099164, mean_q: 35.287173, mean_eps: 0.939646
  10225/150000: episode: 113, duration: 0.784s, episode steps: 116, steps per second: 148, episode reward: -116.796, mean reward: -1.007 [-100.000,  7.255], mean action: 1.655 [0.000, 3.000],  loss: 9.722517, mae: 28.423488, mean_q: 36.189317, mean_eps: 0.939001
  10301/150000: episode: 114, duration: 0.514s, episode steps:  76, steps per second: 148, episode reward: -70.412, mean reward: -0.926 [-100.000, 17.992], mean action: 1.618 [0.000, 3.000],  loss: 11.000025, mae: 28.603954, mean_q: 36.349112, mean_eps: 0.938425
  10388/150000: episode: 115, duration: 0.623s, episode steps:  87, steps per second: 140, episode reward: -87.846, mean reward: -1.010 [-100.000,  9.354], mean action: 1.391 [0.000, 3.000],  loss: 11.526476, mae: 28.730573, mean_q: 36.500893, mean_eps: 0.937936
  10463/150000: episode: 116, duration: 0.511s, episode steps:  75, steps per second: 147, episode reward: -254.474, mean reward: -3.393 [-100.000, 11.780], mean action: 1.373 [0.000, 3.000],  loss: 5.631788, mae: 27.960386, mean_q: 35.772785, mean_eps: 0.937450
  10526/150000: episode: 117, duration: 0.433s, episode steps:  63, steps per second: 146, episode reward: -58.211, mean reward: -0.924 [-100.000, 12.252], mean action: 1.556 [0.000, 3.000],  loss: 10.138429, mae: 28.802567, mean_q: 36.381196, mean_eps: 0.937036
  10616/150000: episode: 118, duration: 0.605s, episode steps:  90, steps per second: 149, episode reward: -258.151, mean reward: -2.868 [-100.000,  8.194], mean action: 1.700 [0.000, 3.000],  loss: 8.756598, mae: 28.497863, mean_q: 36.192660, mean_eps: 0.936577
  10691/150000: episode: 119, duration: 0.545s, episode steps:  75, steps per second: 138, episode reward: -149.455, mean reward: -1.993 [-100.000,  6.432], mean action: 1.427 [0.000, 3.000],  loss: 12.769955, mae: 28.806608, mean_q: 36.214865, mean_eps: 0.936082
  10830/150000: episode: 120, duration: 0.947s, episode steps: 139, steps per second: 147, episode reward: -316.498, mean reward: -2.277 [-100.000, 11.006], mean action: 1.511 [0.000, 3.000],  loss: 9.822748, mae: 28.689704, mean_q: 35.776088, mean_eps: 0.935440
  10931/150000: episode: 121, duration: 0.698s, episode steps: 101, steps per second: 145, episode reward: -323.980, mean reward: -3.208 [-100.000,  0.677], mean action: 1.475 [0.000, 3.000],  loss: 10.965168, mae: 28.836260, mean_q: 35.655522, mean_eps: 0.934720
  11026/150000: episode: 122, duration: 0.699s, episode steps:  95, steps per second: 136, episode reward: -124.593, mean reward: -1.312 [-100.000, 13.330], mean action: 1.653 [0.000, 3.000],  loss: 10.455646, mae: 29.520359, mean_q: 37.222607, mean_eps: 0.934132
  11094/150000: episode: 123, duration: 0.470s, episode steps:  68, steps per second: 145, episode reward: -85.864, mean reward: -1.263 [-100.000,  6.304], mean action: 1.647 [0.000, 3.000],  loss: 8.677136, mae: 28.875381, mean_q: 36.116141, mean_eps: 0.933643
  11210/150000: episode: 124, duration: 0.785s, episode steps: 116, steps per second: 148, episode reward: -87.444, mean reward: -0.754 [-100.000, 23.668], mean action: 1.543 [0.000, 3.000],  loss: 9.877851, mae: 29.544833, mean_q: 36.610509, mean_eps: 0.933091
  11298/150000: episode: 125, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: -129.395, mean reward: -1.470 [-100.000, 58.505], mean action: 1.466 [0.000, 3.000],  loss: 10.482437, mae: 29.544335, mean_q: 37.586389, mean_eps: 0.932479
  11361/150000: episode: 126, duration: 0.435s, episode steps:  63, steps per second: 145, episode reward: -92.751, mean reward: -1.472 [-100.000,  7.488], mean action: 1.302 [0.000, 3.000],  loss: 9.872915, mae: 29.403070, mean_q: 36.976183, mean_eps: 0.932026
  11434/150000: episode: 127, duration: 0.513s, episode steps:  73, steps per second: 142, episode reward: -71.007, mean reward: -0.973 [-100.000, 13.701], mean action: 1.589 [0.000, 3.000],  loss: 8.466483, mae: 29.126685, mean_q: 36.096920, mean_eps: 0.931618
  11521/150000: episode: 128, duration: 0.582s, episode steps:  87, steps per second: 150, episode reward: -271.581, mean reward: -3.122 [-100.000,  5.056], mean action: 1.471 [0.000, 3.000],  loss: 7.930285, mae: 29.519181, mean_q: 36.752180, mean_eps: 0.931138
  11594/150000: episode: 129, duration: 0.522s, episode steps:  73, steps per second: 140, episode reward: 23.352, mean reward:  0.320 [-100.000, 96.624], mean action: 1.370 [0.000, 3.000],  loss: 9.312943, mae: 29.902642, mean_q: 37.342640, mean_eps: 0.930658
  11711/150000: episode: 130, duration: 0.806s, episode steps: 117, steps per second: 145, episode reward: -101.285, mean reward: -0.866 [-100.000,  7.311], mean action: 1.410 [0.000, 3.000],  loss: 9.644427, mae: 30.206058, mean_q: 37.859993, mean_eps: 0.930088
  11798/150000: episode: 131, duration: 0.594s, episode steps:  87, steps per second: 147, episode reward: -274.070, mean reward: -3.150 [-100.000, 19.936], mean action: 1.644 [0.000, 3.000],  loss: 8.020662, mae: 29.973469, mean_q: 37.154432, mean_eps: 0.929476
  11888/150000: episode: 132, duration: 0.609s, episode steps:  90, steps per second: 148, episode reward: -133.639, mean reward: -1.485 [-100.000,  4.847], mean action: 1.300 [0.000, 3.000],  loss: 10.239148, mae: 30.124533, mean_q: 37.912384, mean_eps: 0.928945
  11964/150000: episode: 133, duration: 0.568s, episode steps:  76, steps per second: 134, episode reward: -97.537, mean reward: -1.283 [-100.000,  8.971], mean action: 1.461 [0.000, 3.000],  loss: 10.759986, mae: 30.445207, mean_q: 37.702752, mean_eps: 0.928447
  12076/150000: episode: 134, duration: 0.766s, episode steps: 112, steps per second: 146, episode reward: -210.576, mean reward: -1.880 [-100.000,  8.949], mean action: 1.786 [0.000, 3.000],  loss: 11.856333, mae: 30.296348, mean_q: 38.216424, mean_eps: 0.927883
  12144/150000: episode: 135, duration: 0.463s, episode steps:  68, steps per second: 147, episode reward: -63.980, mean reward: -0.941 [-100.000,  6.908], mean action: 1.559 [0.000, 3.000],  loss: 10.893160, mae: 30.112420, mean_q: 37.245741, mean_eps: 0.927343
  12221/150000: episode: 136, duration: 0.533s, episode steps:  77, steps per second: 145, episode reward: -99.049, mean reward: -1.286 [-100.000,  7.419], mean action: 1.558 [0.000, 3.000],  loss: 13.228962, mae: 30.364403, mean_q: 36.999281, mean_eps: 0.926908
  12338/150000: episode: 137, duration: 0.825s, episode steps: 117, steps per second: 142, episode reward: -166.629, mean reward: -1.424 [-100.000, 11.091], mean action: 1.607 [0.000, 3.000],  loss: 17.259517, mae: 30.243309, mean_q: 37.714452, mean_eps: 0.926326
  12443/150000: episode: 138, duration: 0.706s, episode steps: 105, steps per second: 149, episode reward: -103.584, mean reward: -0.987 [-100.000,  8.817], mean action: 1.495 [0.000, 3.000],  loss: 15.654969, mae: 30.589302, mean_q: 38.590873, mean_eps: 0.925660
  12511/150000: episode: 139, duration: 0.460s, episode steps:  68, steps per second: 148, episode reward: -76.644, mean reward: -1.127 [-100.000,  8.544], mean action: 1.618 [0.000, 3.000],  loss: 7.125882, mae: 30.044937, mean_q: 37.776814, mean_eps: 0.925141
  12590/150000: episode: 140, duration: 0.574s, episode steps:  79, steps per second: 138, episode reward: -117.429, mean reward: -1.486 [-100.000,  7.658], mean action: 1.658 [0.000, 3.000],  loss: 13.039204, mae: 30.505253, mean_q: 38.090418, mean_eps: 0.924700
  12659/150000: episode: 141, duration: 0.484s, episode steps:  69, steps per second: 142, episode reward: -38.236, mean reward: -0.554 [-100.000, 11.095], mean action: 1.696 [0.000, 3.000],  loss: 8.097507, mae: 31.070012, mean_q: 38.872839, mean_eps: 0.924256
  12719/150000: episode: 142, duration: 0.407s, episode steps:  60, steps per second: 147, episode reward: -77.258, mean reward: -1.288 [-100.000,  9.129], mean action: 0.983 [0.000, 3.000],  loss: 7.799939, mae: 30.278050, mean_q: 37.088981, mean_eps: 0.923869
  12808/150000: episode: 143, duration: 0.599s, episode steps:  89, steps per second: 149, episode reward: -138.844, mean reward: -1.560 [-100.000,  8.187], mean action: 1.742 [0.000, 3.000],  loss: 14.463283, mae: 30.931402, mean_q: 38.064318, mean_eps: 0.923422
  12907/150000: episode: 144, duration: 0.743s, episode steps:  99, steps per second: 133, episode reward: -106.926, mean reward: -1.080 [-100.000, 13.091], mean action: 1.778 [0.000, 3.000],  loss: 10.993557, mae: 30.610937, mean_q: 38.260964, mean_eps: 0.922858
  13013/150000: episode: 145, duration: 0.716s, episode steps: 106, steps per second: 148, episode reward: -303.942, mean reward: -2.867 [-100.000, 96.218], mean action: 1.434 [0.000, 3.000],  loss: 8.490107, mae: 31.307928, mean_q: 39.212696, mean_eps: 0.922243
  13119/150000: episode: 146, duration: 0.722s, episode steps: 106, steps per second: 147, episode reward: -282.992, mean reward: -2.670 [-100.000, 104.708], mean action: 1.377 [0.000, 3.000],  loss: 11.885764, mae: 30.901770, mean_q: 37.753949, mean_eps: 0.921607
  13220/150000: episode: 147, duration: 0.730s, episode steps: 101, steps per second: 138, episode reward: -177.293, mean reward: -1.755 [-100.000, 18.814], mean action: 1.455 [0.000, 3.000],  loss: 8.536048, mae: 31.135635, mean_q: 38.282848, mean_eps: 0.920986
  13348/150000: episode: 148, duration: 1.035s, episode steps: 128, steps per second: 124, episode reward: -154.746, mean reward: -1.209 [-100.000,  4.331], mean action: 1.578 [0.000, 3.000],  loss: 11.465963, mae: 31.051878, mean_q: 38.524784, mean_eps: 0.920299
  13450/150000: episode: 149, duration: 0.856s, episode steps: 102, steps per second: 119, episode reward: -95.917, mean reward: -0.940 [-100.000,  5.536], mean action: 1.510 [0.000, 3.000],  loss: 11.046308, mae: 31.058332, mean_q: 38.698777, mean_eps: 0.919609
  13551/150000: episode: 150, duration: 0.804s, episode steps: 101, steps per second: 126, episode reward: -124.241, mean reward: -1.230 [-100.000, 35.579], mean action: 1.475 [0.000, 3.000],  loss: 12.610977, mae: 31.074218, mean_q: 38.263864, mean_eps: 0.919000
  13635/150000: episode: 151, duration: 0.652s, episode steps:  84, steps per second: 129, episode reward: -118.006, mean reward: -1.405 [-100.000, 10.593], mean action: 1.476 [0.000, 3.000],  loss: 10.670971, mae: 30.909084, mean_q: 38.421644, mean_eps: 0.918445
  13720/150000: episode: 152, duration: 0.660s, episode steps:  85, steps per second: 129, episode reward: -112.532, mean reward: -1.324 [-100.000,  8.810], mean action: 1.588 [0.000, 3.000],  loss: 10.069352, mae: 31.403324, mean_q: 39.398859, mean_eps: 0.917938
  13849/150000: episode: 153, duration: 1.032s, episode steps: 129, steps per second: 125, episode reward: -108.203, mean reward: -0.839 [-100.000, 33.866], mean action: 1.388 [0.000, 3.000],  loss: 12.612462, mae: 31.100902, mean_q: 38.461737, mean_eps: 0.917296
  13924/150000: episode: 154, duration: 0.558s, episode steps:  75, steps per second: 135, episode reward: -74.854, mean reward: -0.998 [-100.000,  7.249], mean action: 1.640 [0.000, 3.000],  loss: 12.708045, mae: 32.183901, mean_q: 39.174968, mean_eps: 0.916684
  14003/150000: episode: 155, duration: 0.582s, episode steps:  79, steps per second: 136, episode reward: -69.536, mean reward: -0.880 [-100.000, 16.799], mean action: 1.544 [0.000, 3.000],  loss: 8.122704, mae: 31.821012, mean_q: 38.884032, mean_eps: 0.916222
  14085/150000: episode: 156, duration: 0.576s, episode steps:  82, steps per second: 142, episode reward: -369.473, mean reward: -4.506 [-100.000,  0.632], mean action: 1.659 [0.000, 3.000],  loss: 17.907289, mae: 31.830164, mean_q: 38.871913, mean_eps: 0.915739
  14216/150000: episode: 157, duration: 0.889s, episode steps: 131, steps per second: 147, episode reward: -115.740, mean reward: -0.884 [-100.000,  5.928], mean action: 1.725 [0.000, 3.000],  loss: 11.588670, mae: 31.496723, mean_q: 38.251343, mean_eps: 0.915100
  14274/150000: episode: 158, duration: 0.404s, episode steps:  58, steps per second: 143, episode reward: -80.251, mean reward: -1.384 [-100.000,  9.589], mean action: 1.741 [0.000, 3.000],  loss: 10.032141, mae: 31.320802, mean_q: 38.547510, mean_eps: 0.914533
  14367/150000: episode: 159, duration: 0.667s, episode steps:  93, steps per second: 139, episode reward: -114.171, mean reward: -1.228 [-100.000,  6.521], mean action: 1.516 [0.000, 3.000],  loss: 11.541000, mae: 31.581597, mean_q: 39.099560, mean_eps: 0.914080
  14456/150000: episode: 160, duration: 0.623s, episode steps:  89, steps per second: 143, episode reward: -294.450, mean reward: -3.308 [-100.000, 134.280], mean action: 1.393 [0.000, 3.000],  loss: 14.404167, mae: 31.425588, mean_q: 38.518135, mean_eps: 0.913534
  14564/150000: episode: 161, duration: 0.753s, episode steps: 108, steps per second: 143, episode reward: -149.292, mean reward: -1.382 [-100.000,  5.946], mean action: 1.417 [0.000, 3.000],  loss: 9.459395, mae: 31.594371, mean_q: 38.172723, mean_eps: 0.912943
  14668/150000: episode: 162, duration: 0.741s, episode steps: 104, steps per second: 140, episode reward: -172.004, mean reward: -1.654 [-100.000, 16.145], mean action: 1.337 [0.000, 3.000],  loss: 13.383854, mae: 31.717837, mean_q: 38.631057, mean_eps: 0.912307
  14762/150000: episode: 163, duration: 0.641s, episode steps:  94, steps per second: 147, episode reward: -202.815, mean reward: -2.158 [-100.000, 20.802], mean action: 1.606 [0.000, 3.000],  loss: 9.732383, mae: 32.292124, mean_q: 39.015593, mean_eps: 0.911713
  14830/150000: episode: 164, duration: 0.475s, episode steps:  68, steps per second: 143, episode reward: -69.985, mean reward: -1.029 [-100.000, 11.058], mean action: 1.426 [0.000, 3.000],  loss: 11.195827, mae: 31.880424, mean_q: 39.136745, mean_eps: 0.911227
  14923/150000: episode: 165, duration: 0.669s, episode steps:  93, steps per second: 139, episode reward: -103.302, mean reward: -1.111 [-100.000,  6.103], mean action: 1.559 [0.000, 3.000],  loss: 11.257170, mae: 31.819365, mean_q: 38.686687, mean_eps: 0.910744
  15029/150000: episode: 166, duration: 0.727s, episode steps: 106, steps per second: 146, episode reward: -171.490, mean reward: -1.618 [-100.000,  9.107], mean action: 1.538 [0.000, 3.000],  loss: 13.094458, mae: 31.686943, mean_q: 38.848221, mean_eps: 0.910147
  15109/150000: episode: 167, duration: 0.542s, episode steps:  80, steps per second: 148, episode reward: -114.843, mean reward: -1.436 [-100.000,  8.429], mean action: 1.663 [0.000, 3.000],  loss: 9.936949, mae: 31.870999, mean_q: 38.291843, mean_eps: 0.909589
  15180/150000: episode: 168, duration: 0.517s, episode steps:  71, steps per second: 137, episode reward: -76.898, mean reward: -1.083 [-100.000, 22.706], mean action: 1.493 [0.000, 3.000],  loss: 9.057202, mae: 32.158204, mean_q: 38.713748, mean_eps: 0.909136
  15275/150000: episode: 169, duration: 0.706s, episode steps:  95, steps per second: 134, episode reward: -88.646, mean reward: -0.933 [-100.000,  7.433], mean action: 1.474 [0.000, 3.000],  loss: 27.480614, mae: 32.148921, mean_q: 38.860198, mean_eps: 0.908638
  15389/150000: episode: 170, duration: 0.782s, episode steps: 114, steps per second: 146, episode reward: -106.960, mean reward: -0.938 [-100.000, 12.312], mean action: 1.482 [0.000, 3.000],  loss: 14.918746, mae: 32.550888, mean_q: 39.173158, mean_eps: 0.908011
  15488/150000: episode: 171, duration: 0.678s, episode steps:  99, steps per second: 146, episode reward: -177.067, mean reward: -1.789 [-100.000,  7.079], mean action: 1.636 [0.000, 3.000],  loss: 7.460000, mae: 32.312196, mean_q: 39.191060, mean_eps: 0.907372
  15576/150000: episode: 172, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: -136.366, mean reward: -1.550 [-100.000, 17.221], mean action: 1.591 [0.000, 3.000],  loss: 11.019321, mae: 32.298868, mean_q: 38.190397, mean_eps: 0.906811
  15690/150000: episode: 173, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -116.294, mean reward: -1.020 [-100.000,  5.530], mean action: 1.623 [0.000, 3.000],  loss: 10.177276, mae: 32.702299, mean_q: 39.912381, mean_eps: 0.906205
  15765/150000: episode: 174, duration: 0.505s, episode steps:  75, steps per second: 148, episode reward: -90.569, mean reward: -1.208 [-100.000, 61.423], mean action: 1.520 [0.000, 3.000],  loss: 9.997169, mae: 32.016491, mean_q: 38.543272, mean_eps: 0.905638
  15832/150000: episode: 175, duration: 0.459s, episode steps:  67, steps per second: 146, episode reward: -99.504, mean reward: -1.485 [-100.000, 17.424], mean action: 1.433 [0.000, 3.000],  loss: 12.320526, mae: 33.341338, mean_q: 40.504454, mean_eps: 0.905212
  15934/150000: episode: 176, duration: 0.706s, episode steps: 102, steps per second: 144, episode reward: -81.936, mean reward: -0.803 [-100.000, 64.729], mean action: 1.402 [0.000, 3.000],  loss: 10.792360, mae: 32.734226, mean_q: 39.274973, mean_eps: 0.904705
  15999/150000: episode: 177, duration: 0.433s, episode steps:  65, steps per second: 150, episode reward: -54.130, mean reward: -0.833 [-100.000, 22.920], mean action: 1.585 [0.000, 3.000],  loss: 12.221342, mae: 33.729199, mean_q: 40.430296, mean_eps: 0.904204
  16068/150000: episode: 178, duration: 0.466s, episode steps:  69, steps per second: 148, episode reward: -83.515, mean reward: -1.210 [-100.000, 16.607], mean action: 1.652 [0.000, 3.000],  loss: 17.412804, mae: 33.212682, mean_q: 39.316716, mean_eps: 0.903802
  16156/150000: episode: 179, duration: 0.616s, episode steps:  88, steps per second: 143, episode reward: -110.954, mean reward: -1.261 [-100.000, 11.025], mean action: 1.591 [0.000, 3.000],  loss: 16.453488, mae: 33.356765, mean_q: 39.425973, mean_eps: 0.903331
  16230/150000: episode: 180, duration: 0.514s, episode steps:  74, steps per second: 144, episode reward: -75.552, mean reward: -1.021 [-100.000, 16.737], mean action: 1.419 [0.000, 3.000],  loss: 8.984767, mae: 33.522907, mean_q: 39.878848, mean_eps: 0.902845
  16326/150000: episode: 181, duration: 0.646s, episode steps:  96, steps per second: 148, episode reward: -151.613, mean reward: -1.579 [-100.000,  8.997], mean action: 1.500 [0.000, 3.000],  loss: 12.579067, mae: 33.603082, mean_q: 39.766070, mean_eps: 0.902335
  16440/150000: episode: 182, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -118.009, mean reward: -1.035 [-100.000, 19.180], mean action: 1.544 [0.000, 3.000],  loss: 9.827073, mae: 34.094454, mean_q: 40.763627, mean_eps: 0.901705
  16539/150000: episode: 183, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -142.628, mean reward: -1.441 [-100.000,  5.849], mean action: 1.475 [0.000, 3.000],  loss: 14.031676, mae: 33.329441, mean_q: 39.027863, mean_eps: 0.901066
  16653/150000: episode: 184, duration: 0.769s, episode steps: 114, steps per second: 148, episode reward: -107.705, mean reward: -0.945 [-100.000, 11.748], mean action: 1.430 [0.000, 3.000],  loss: 10.426993, mae: 33.688641, mean_q: 40.523492, mean_eps: 0.900427
  16746/150000: episode: 185, duration: 0.622s, episode steps:  93, steps per second: 150, episode reward: -397.040, mean reward: -4.269 [-100.000,  1.228], mean action: 1.710 [0.000, 3.000],  loss: 10.110736, mae: 34.052743, mean_q: 39.584557, mean_eps: 0.899806
  16834/150000: episode: 186, duration: 0.623s, episode steps:  88, steps per second: 141, episode reward: -115.973, mean reward: -1.318 [-100.000, 11.434], mean action: 1.455 [0.000, 3.000],  loss: 11.050890, mae: 33.620421, mean_q: 39.596674, mean_eps: 0.899263
  16953/150000: episode: 187, duration: 0.799s, episode steps: 119, steps per second: 149, episode reward: -131.510, mean reward: -1.105 [-100.000, 10.669], mean action: 1.664 [0.000, 3.000],  loss: 17.725203, mae: 34.255820, mean_q: 40.365117, mean_eps: 0.898642
  17077/150000: episode: 188, duration: 0.837s, episode steps: 124, steps per second: 148, episode reward: -138.629, mean reward: -1.118 [-100.000, 14.899], mean action: 1.460 [0.000, 3.000],  loss: 9.846051, mae: 33.979052, mean_q: 39.955446, mean_eps: 0.897913
  17151/150000: episode: 189, duration: 0.523s, episode steps:  74, steps per second: 142, episode reward: -53.568, mean reward: -0.724 [-100.000, 13.096], mean action: 1.662 [0.000, 3.000],  loss: 11.542399, mae: 33.504400, mean_q: 38.740776, mean_eps: 0.897319
  17261/150000: episode: 190, duration: 0.738s, episode steps: 110, steps per second: 149, episode reward: -182.156, mean reward: -1.656 [-100.000, 21.409], mean action: 1.536 [0.000, 3.000],  loss: 15.133781, mae: 33.626631, mean_q: 39.649821, mean_eps: 0.896767
  17344/150000: episode: 191, duration: 0.555s, episode steps:  83, steps per second: 150, episode reward: -141.882, mean reward: -1.709 [-100.000, 34.919], mean action: 1.361 [0.000, 3.000],  loss: 19.616684, mae: 34.201682, mean_q: 39.459386, mean_eps: 0.896188
  17420/150000: episode: 192, duration: 0.522s, episode steps:  76, steps per second: 146, episode reward: -90.261, mean reward: -1.188 [-100.000,  6.737], mean action: 1.566 [0.000, 3.000],  loss: 8.465098, mae: 34.215850, mean_q: 39.592625, mean_eps: 0.895711
  17487/150000: episode: 193, duration: 0.466s, episode steps:  67, steps per second: 144, episode reward: -102.716, mean reward: -1.533 [-100.000, 18.723], mean action: 1.836 [0.000, 3.000],  loss: 12.164657, mae: 34.499833, mean_q: 40.012333, mean_eps: 0.895282
  17603/150000: episode: 194, duration: 0.775s, episode steps: 116, steps per second: 150, episode reward: -264.866, mean reward: -2.283 [-100.000,  1.124], mean action: 1.586 [0.000, 3.000],  loss: 7.728078, mae: 34.400438, mean_q: 39.767755, mean_eps: 0.894733
  17710/150000: episode: 195, duration: 0.735s, episode steps: 107, steps per second: 145, episode reward: -125.132, mean reward: -1.169 [-100.000, 12.197], mean action: 1.738 [0.000, 3.000],  loss: 8.307235, mae: 33.864218, mean_q: 39.691489, mean_eps: 0.894064
  17805/150000: episode: 196, duration: 0.699s, episode steps:  95, steps per second: 136, episode reward: -89.838, mean reward: -0.946 [-100.000, 13.706], mean action: 1.558 [0.000, 3.000],  loss: 16.895929, mae: 34.901697, mean_q: 40.731142, mean_eps: 0.893458
  17878/150000: episode: 197, duration: 0.494s, episode steps:  73, steps per second: 148, episode reward: -81.028, mean reward: -1.110 [-100.000, 10.091], mean action: 1.342 [0.000, 3.000],  loss: 20.087833, mae: 34.369534, mean_q: 39.730128, mean_eps: 0.892954
  17988/150000: episode: 198, duration: 0.737s, episode steps: 110, steps per second: 149, episode reward: -463.658, mean reward: -4.215 [-100.000,  1.098], mean action: 1.536 [0.000, 3.000],  loss: 11.316640, mae: 33.846980, mean_q: 39.227691, mean_eps: 0.892405
  18073/150000: episode: 199, duration: 0.607s, episode steps:  85, steps per second: 140, episode reward: -116.855, mean reward: -1.375 [-100.000, 19.627], mean action: 1.471 [0.000, 3.000],  loss: 13.095815, mae: 34.962086, mean_q: 40.014676, mean_eps: 0.891820
  18181/150000: episode: 200, duration: 0.732s, episode steps: 108, steps per second: 148, episode reward: -159.594, mean reward: -1.478 [-100.000, 13.304], mean action: 1.481 [0.000, 3.000],  loss: 7.581624, mae: 34.324985, mean_q: 39.896534, mean_eps: 0.891241
  18257/150000: episode: 201, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -63.512, mean reward: -0.836 [-100.000, 14.313], mean action: 1.632 [0.000, 3.000],  loss: 14.890135, mae: 33.881505, mean_q: 39.058994, mean_eps: 0.890689
  18381/150000: episode: 202, duration: 1.013s, episode steps: 124, steps per second: 122, episode reward: -270.369, mean reward: -2.180 [-100.000,  4.799], mean action: 1.621 [0.000, 3.000],  loss: 13.288830, mae: 34.121616, mean_q: 39.073330, mean_eps: 0.890089
  18499/150000: episode: 203, duration: 0.829s, episode steps: 118, steps per second: 142, episode reward: -283.516, mean reward: -2.403 [-100.000, 46.348], mean action: 1.398 [0.000, 3.000],  loss: 13.424384, mae: 35.163436, mean_q: 39.906322, mean_eps: 0.889363
  18615/150000: episode: 204, duration: 0.776s, episode steps: 116, steps per second: 149, episode reward: -92.753, mean reward: -0.800 [-100.000, 13.793], mean action: 1.491 [0.000, 3.000],  loss: 11.894467, mae: 34.790992, mean_q: 39.655428, mean_eps: 0.888661
  18680/150000: episode: 205, duration: 0.459s, episode steps:  65, steps per second: 142, episode reward: -42.362, mean reward: -0.652 [-100.000, 11.123], mean action: 1.446 [0.000, 3.000],  loss: 17.099942, mae: 35.090884, mean_q: 40.374823, mean_eps: 0.888118
  18793/150000: episode: 206, duration: 0.765s, episode steps: 113, steps per second: 148, episode reward: -163.076, mean reward: -1.443 [-100.000, 10.747], mean action: 1.487 [0.000, 3.000],  loss: 9.119943, mae: 35.379619, mean_q: 40.352203, mean_eps: 0.887584
  18860/150000: episode: 207, duration: 0.454s, episode steps:  67, steps per second: 148, episode reward: -132.064, mean reward: -1.971 [-100.000,  8.037], mean action: 1.358 [0.000, 3.000],  loss: 15.269255, mae: 34.885509, mean_q: 40.534781, mean_eps: 0.887044
  18933/150000: episode: 208, duration: 0.492s, episode steps:  73, steps per second: 148, episode reward: -106.599, mean reward: -1.460 [-100.000, 13.883], mean action: 1.466 [0.000, 3.000],  loss: 14.449563, mae: 36.031337, mean_q: 40.708977, mean_eps: 0.886624
  19010/150000: episode: 209, duration: 0.548s, episode steps:  77, steps per second: 140, episode reward: -110.811, mean reward: -1.439 [-100.000,  5.244], mean action: 1.753 [0.000, 3.000],  loss: 9.555238, mae: 35.057316, mean_q: 39.822072, mean_eps: 0.886174
  19124/150000: episode: 210, duration: 0.771s, episode steps: 114, steps per second: 148, episode reward: -141.459, mean reward: -1.241 [-100.000,  7.270], mean action: 1.430 [0.000, 3.000],  loss: 16.676195, mae: 35.456459, mean_q: 40.806261, mean_eps: 0.885601
  19194/150000: episode: 211, duration: 0.474s, episode steps:  70, steps per second: 148, episode reward: -73.342, mean reward: -1.048 [-100.000, 10.313], mean action: 1.914 [0.000, 3.000],  loss: 13.283152, mae: 35.525760, mean_q: 40.859725, mean_eps: 0.885049
  19285/150000: episode: 212, duration: 0.616s, episode steps:  91, steps per second: 148, episode reward: -339.742, mean reward: -3.733 [-100.000,  0.456], mean action: 1.637 [0.000, 3.000],  loss: 11.687940, mae: 34.935099, mean_q: 39.879856, mean_eps: 0.884566
  19350/150000: episode: 213, duration: 0.459s, episode steps:  65, steps per second: 142, episode reward: -68.957, mean reward: -1.061 [-100.000,  8.515], mean action: 1.631 [0.000, 3.000],  loss: 20.002015, mae: 35.427192, mean_q: 39.417952, mean_eps: 0.884098
  19472/150000: episode: 214, duration: 0.814s, episode steps: 122, steps per second: 150, episode reward: -151.762, mean reward: -1.244 [-100.000,  5.478], mean action: 1.475 [0.000, 3.000],  loss: 11.025386, mae: 36.111004, mean_q: 40.544007, mean_eps: 0.883537
  19544/150000: episode: 215, duration: 0.485s, episode steps:  72, steps per second: 148, episode reward: -220.518, mean reward: -3.063 [-100.000,  4.716], mean action: 1.639 [0.000, 3.000],  loss: 10.285375, mae: 36.108471, mean_q: 40.624224, mean_eps: 0.882955
  19621/150000: episode: 216, duration: 0.545s, episode steps:  77, steps per second: 141, episode reward: -94.089, mean reward: -1.222 [-100.000, 34.717], mean action: 1.675 [0.000, 3.000],  loss: 17.970907, mae: 35.696804, mean_q: 39.785034, mean_eps: 0.882508
  19693/150000: episode: 217, duration: 0.536s, episode steps:  72, steps per second: 134, episode reward: -123.087, mean reward: -1.710 [-100.000, 33.567], mean action: 1.528 [0.000, 3.000],  loss: 15.854258, mae: 35.825240, mean_q: 40.665512, mean_eps: 0.882061
  19761/150000: episode: 218, duration: 0.638s, episode steps:  68, steps per second: 107, episode reward: -63.010, mean reward: -0.927 [-100.000, 18.081], mean action: 1.544 [0.000, 3.000],  loss: 10.226269, mae: 36.063780, mean_q: 41.534581, mean_eps: 0.881641
  19880/150000: episode: 219, duration: 0.931s, episode steps: 119, steps per second: 128, episode reward: -136.964, mean reward: -1.151 [-100.000,  6.503], mean action: 1.328 [0.000, 3.000],  loss: 10.651782, mae: 36.012676, mean_q: 40.602753, mean_eps: 0.881080
  19957/150000: episode: 220, duration: 0.672s, episode steps:  77, steps per second: 115, episode reward: -111.523, mean reward: -1.448 [-100.000, 14.242], mean action: 1.519 [0.000, 3.000],  loss: 13.777427, mae: 35.556179, mean_q: 40.057192, mean_eps: 0.880492
  20040/150000: episode: 221, duration: 0.759s, episode steps:  83, steps per second: 109, episode reward: -154.435, mean reward: -1.861 [-100.000, 12.715], mean action: 1.723 [0.000, 3.000],  loss: 8.799312, mae: 36.134662, mean_q: 40.776422, mean_eps: 0.880012
  20138/150000: episode: 222, duration: 0.838s, episode steps:  98, steps per second: 117, episode reward: -112.075, mean reward: -1.144 [-100.000,  5.160], mean action: 1.531 [0.000, 3.000],  loss: 10.067869, mae: 36.685805, mean_q: 41.303948, mean_eps: 0.879469
  20246/150000: episode: 223, duration: 0.808s, episode steps: 108, steps per second: 134, episode reward: -86.731, mean reward: -0.803 [-100.000, 12.235], mean action: 1.481 [0.000, 3.000],  loss: 11.898731, mae: 36.186717, mean_q: 41.678005, mean_eps: 0.878851
  20335/150000: episode: 224, duration: 0.616s, episode steps:  89, steps per second: 144, episode reward: -112.529, mean reward: -1.264 [-100.000, 12.471], mean action: 1.674 [0.000, 3.000],  loss: 17.352554, mae: 36.610013, mean_q: 41.070843, mean_eps: 0.878260
  20402/150000: episode: 225, duration: 0.497s, episode steps:  67, steps per second: 135, episode reward: -78.609, mean reward: -1.173 [-100.000, 10.071], mean action: 1.328 [0.000, 3.000],  loss: 18.021220, mae: 36.043412, mean_q: 40.452233, mean_eps: 0.877792
  20484/150000: episode: 226, duration: 0.598s, episode steps:  82, steps per second: 137, episode reward: -159.117, mean reward: -1.940 [-100.000,  7.468], mean action: 1.537 [0.000, 3.000],  loss: 13.821671, mae: 36.472453, mean_q: 40.053759, mean_eps: 0.877345
  20581/150000: episode: 227, duration: 0.686s, episode steps:  97, steps per second: 141, episode reward: -98.912, mean reward: -1.020 [-100.000,  7.467], mean action: 1.464 [0.000, 3.000],  loss: 7.608835, mae: 36.833130, mean_q: 41.126291, mean_eps: 0.876808
  20655/150000: episode: 228, duration: 0.510s, episode steps:  74, steps per second: 145, episode reward: -129.578, mean reward: -1.751 [-100.000,  6.760], mean action: 1.676 [0.000, 3.000],  loss: 12.602063, mae: 36.481693, mean_q: 41.325838, mean_eps: 0.876295
  20798/150000: episode: 229, duration: 1.023s, episode steps: 143, steps per second: 140, episode reward: -73.082, mean reward: -0.511 [-100.000,  7.538], mean action: 1.594 [0.000, 3.000],  loss: 11.192948, mae: 36.641223, mean_q: 41.748919, mean_eps: 0.875644
  20892/150000: episode: 230, duration: 0.659s, episode steps:  94, steps per second: 143, episode reward: -160.102, mean reward: -1.703 [-100.000,  5.646], mean action: 1.479 [0.000, 3.000],  loss: 16.481210, mae: 36.970371, mean_q: 41.154138, mean_eps: 0.874933
  21000/150000: episode: 231, duration: 0.787s, episode steps: 108, steps per second: 137, episode reward: -94.731, mean reward: -0.877 [-100.000,  6.195], mean action: 1.694 [0.000, 3.000],  loss: 11.909002, mae: 36.394360, mean_q: 40.419020, mean_eps: 0.874327
  21070/150000: episode: 232, duration: 0.636s, episode steps:  70, steps per second: 110, episode reward: -59.485, mean reward: -0.850 [-100.000, 16.376], mean action: 1.671 [0.000, 3.000],  loss: 8.605163, mae: 37.511285, mean_q: 41.620125, mean_eps: 0.873793
  21141/150000: episode: 233, duration: 0.501s, episode steps:  71, steps per second: 142, episode reward: -177.289, mean reward: -2.497 [-100.000,  6.255], mean action: 1.366 [0.000, 3.000],  loss: 11.990518, mae: 37.580421, mean_q: 41.491916, mean_eps: 0.873370
  21226/150000: episode: 234, duration: 0.580s, episode steps:  85, steps per second: 147, episode reward: -115.409, mean reward: -1.358 [-100.000,  9.256], mean action: 1.435 [0.000, 3.000],  loss: 10.695725, mae: 37.090106, mean_q: 41.971617, mean_eps: 0.872902
  21316/150000: episode: 235, duration: 0.627s, episode steps:  90, steps per second: 143, episode reward: -94.973, mean reward: -1.055 [-100.000, 11.615], mean action: 1.567 [0.000, 3.000],  loss: 10.277408, mae: 37.095933, mean_q: 41.253745, mean_eps: 0.872377
  21415/150000: episode: 236, duration: 0.679s, episode steps:  99, steps per second: 146, episode reward: -105.472, mean reward: -1.065 [-100.000,  7.464], mean action: 1.354 [0.000, 3.000],  loss: 13.031773, mae: 37.814630, mean_q: 42.271076, mean_eps: 0.871810
  21499/150000: episode: 237, duration: 0.567s, episode steps:  84, steps per second: 148, episode reward: -117.066, mean reward: -1.394 [-100.000,  6.501], mean action: 1.607 [0.000, 3.000],  loss: 9.482263, mae: 38.157887, mean_q: 42.480132, mean_eps: 0.871261
  21570/150000: episode: 238, duration: 0.478s, episode steps:  71, steps per second: 149, episode reward: -64.180, mean reward: -0.904 [-100.000,  7.453], mean action: 1.549 [0.000, 3.000],  loss: 8.283327, mae: 37.894009, mean_q: 41.858364, mean_eps: 0.870796
  21640/150000: episode: 239, duration: 0.493s, episode steps:  70, steps per second: 142, episode reward: -100.857, mean reward: -1.441 [-100.000, 10.963], mean action: 1.629 [0.000, 3.000],  loss: 10.998426, mae: 38.333612, mean_q: 42.332709, mean_eps: 0.870373
  21712/150000: episode: 240, duration: 0.490s, episode steps:  72, steps per second: 147, episode reward: -93.240, mean reward: -1.295 [-100.000, 11.513], mean action: 1.597 [0.000, 3.000],  loss: 15.450044, mae: 37.588846, mean_q: 41.597144, mean_eps: 0.869947
  21794/150000: episode: 241, duration: 0.560s, episode steps:  82, steps per second: 146, episode reward: -25.903, mean reward: -0.316 [-100.000, 27.803], mean action: 1.598 [0.000, 3.000],  loss: 10.080769, mae: 38.711162, mean_q: 43.588801, mean_eps: 0.869485
  21894/150000: episode: 242, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: -104.757, mean reward: -1.048 [-100.000, 13.534], mean action: 1.490 [0.000, 3.000],  loss: 8.934447, mae: 37.417825, mean_q: 41.760963, mean_eps: 0.868939
  22022/150000: episode: 243, duration: 0.920s, episode steps: 128, steps per second: 139, episode reward: -93.119, mean reward: -0.727 [-100.000, 12.366], mean action: 1.578 [0.000, 3.000],  loss: 14.798606, mae: 38.199588, mean_q: 42.917101, mean_eps: 0.868255
  22101/150000: episode: 244, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -66.204, mean reward: -0.838 [-100.000, 10.537], mean action: 1.608 [0.000, 3.000],  loss: 11.997534, mae: 38.192364, mean_q: 42.484940, mean_eps: 0.867634
  22204/150000: episode: 245, duration: 0.690s, episode steps: 103, steps per second: 149, episode reward: -92.640, mean reward: -0.899 [-100.000, 15.619], mean action: 1.515 [0.000, 3.000],  loss: 19.388124, mae: 39.002959, mean_q: 42.850588, mean_eps: 0.867088
  22304/150000: episode: 246, duration: 0.693s, episode steps: 100, steps per second: 144, episode reward: -147.841, mean reward: -1.478 [-100.000,  8.757], mean action: 1.160 [0.000, 3.000],  loss: 15.608256, mae: 38.456273, mean_q: 42.066766, mean_eps: 0.866479
  22427/150000: episode: 247, duration: 0.830s, episode steps: 123, steps per second: 148, episode reward: -101.746, mean reward: -0.827 [-100.000, 20.791], mean action: 1.512 [0.000, 3.000],  loss: 12.875338, mae: 38.777438, mean_q: 42.593834, mean_eps: 0.865810
  22497/150000: episode: 248, duration: 0.569s, episode steps:  70, steps per second: 123, episode reward: -71.695, mean reward: -1.024 [-100.000,  8.076], mean action: 1.500 [0.000, 3.000],  loss: 12.192749, mae: 38.536456, mean_q: 43.583207, mean_eps: 0.865231
  22572/150000: episode: 249, duration: 0.614s, episode steps:  75, steps per second: 122, episode reward: -43.469, mean reward: -0.580 [-100.000, 13.645], mean action: 1.573 [0.000, 3.000],  loss: 11.266484, mae: 38.716467, mean_q: 43.272218, mean_eps: 0.864796
  22638/150000: episode: 250, duration: 0.538s, episode steps:  66, steps per second: 123, episode reward: -111.657, mean reward: -1.692 [-100.000, 19.717], mean action: 1.439 [0.000, 3.000],  loss: 7.136859, mae: 39.376257, mean_q: 43.790714, mean_eps: 0.864373
  22710/150000: episode: 251, duration: 0.537s, episode steps:  72, steps per second: 134, episode reward: -122.275, mean reward: -1.698 [-100.000, 17.013], mean action: 1.597 [0.000, 3.000],  loss: 11.295244, mae: 38.984612, mean_q: 43.489881, mean_eps: 0.863959
  22797/150000: episode: 252, duration: 0.822s, episode steps:  87, steps per second: 106, episode reward: -116.272, mean reward: -1.336 [-100.000, 14.563], mean action: 1.655 [0.000, 3.000],  loss: 10.715721, mae: 38.446248, mean_q: 43.177809, mean_eps: 0.863482
  22906/150000: episode: 253, duration: 0.804s, episode steps: 109, steps per second: 136, episode reward: -122.484, mean reward: -1.124 [-100.000,  6.525], mean action: 1.661 [0.000, 3.000],  loss: 18.236365, mae: 38.695091, mean_q: 43.907626, mean_eps: 0.862894
  22987/150000: episode: 254, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -133.723, mean reward: -1.651 [-100.000,  6.493], mean action: 1.741 [0.000, 3.000],  loss: 13.389796, mae: 38.462916, mean_q: 43.051390, mean_eps: 0.862324
  23068/150000: episode: 255, duration: 0.608s, episode steps:  81, steps per second: 133, episode reward: -78.854, mean reward: -0.974 [-100.000,  7.077], mean action: 1.617 [0.000, 3.000],  loss: 12.241465, mae: 39.588072, mean_q: 44.257579, mean_eps: 0.861838
  23131/150000: episode: 256, duration: 0.475s, episode steps:  63, steps per second: 133, episode reward: -95.564, mean reward: -1.517 [-100.000, 22.448], mean action: 1.429 [0.000, 3.000],  loss: 8.717613, mae: 39.784365, mean_q: 43.731814, mean_eps: 0.861406
  23217/150000: episode: 257, duration: 0.596s, episode steps:  86, steps per second: 144, episode reward: -70.693, mean reward: -0.822 [-100.000,  9.411], mean action: 1.488 [0.000, 3.000],  loss: 9.627357, mae: 39.005748, mean_q: 43.154309, mean_eps: 0.860959
  23279/150000: episode: 258, duration: 0.421s, episode steps:  62, steps per second: 147, episode reward: -62.008, mean reward: -1.000 [-100.000, 24.831], mean action: 1.484 [0.000, 3.000],  loss: 11.077669, mae: 40.612907, mean_q: 45.482446, mean_eps: 0.860515
  23348/150000: episode: 259, duration: 0.468s, episode steps:  69, steps per second: 147, episode reward: -75.104, mean reward: -1.088 [-100.000, 14.731], mean action: 2.029 [0.000, 3.000],  loss: 10.829249, mae: 40.221275, mean_q: 44.210470, mean_eps: 0.860122
  23440/150000: episode: 260, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -119.803, mean reward: -1.302 [-100.000,  5.171], mean action: 1.283 [0.000, 3.000],  loss: 8.865177, mae: 40.395629, mean_q: 45.475030, mean_eps: 0.859639
  23535/150000: episode: 261, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -102.048, mean reward: -1.074 [-100.000,  6.521], mean action: 1.579 [0.000, 3.000],  loss: 11.969506, mae: 40.386925, mean_q: 44.741621, mean_eps: 0.859078
  23627/150000: episode: 262, duration: 0.614s, episode steps:  92, steps per second: 150, episode reward: -96.301, mean reward: -1.047 [-100.000, 13.971], mean action: 1.587 [0.000, 3.000],  loss: 11.990426, mae: 39.827678, mean_q: 43.721557, mean_eps: 0.858517
  23696/150000: episode: 263, duration: 0.461s, episode steps:  69, steps per second: 150, episode reward: -85.938, mean reward: -1.245 [-100.000, 17.246], mean action: 1.435 [0.000, 3.000],  loss: 10.241299, mae: 39.962786, mean_q: 44.896254, mean_eps: 0.858034
  23795/150000: episode: 264, duration: 0.691s, episode steps:  99, steps per second: 143, episode reward: -129.382, mean reward: -1.307 [-100.000,  9.061], mean action: 1.475 [0.000, 3.000],  loss: 6.920817, mae: 39.885898, mean_q: 44.990770, mean_eps: 0.857530
  23913/150000: episode: 265, duration: 0.853s, episode steps: 118, steps per second: 138, episode reward: -112.666, mean reward: -0.955 [-100.000,  6.170], mean action: 1.661 [0.000, 3.000],  loss: 15.248460, mae: 40.225465, mean_q: 44.053081, mean_eps: 0.856879
  23997/150000: episode: 266, duration: 0.596s, episode steps:  84, steps per second: 141, episode reward: -120.211, mean reward: -1.431 [-100.000, 11.860], mean action: 1.488 [0.000, 3.000],  loss: 16.529662, mae: 40.641085, mean_q: 44.488296, mean_eps: 0.856273
  24092/150000: episode: 267, duration: 0.691s, episode steps:  95, steps per second: 137, episode reward: -61.797, mean reward: -0.650 [-100.000, 22.019], mean action: 1.453 [0.000, 3.000],  loss: 11.455509, mae: 40.311471, mean_q: 43.354352, mean_eps: 0.855736
  24160/150000: episode: 268, duration: 0.460s, episode steps:  68, steps per second: 148, episode reward: -88.848, mean reward: -1.307 [-100.000,  6.274], mean action: 1.574 [0.000, 3.000],  loss: 15.567128, mae: 40.422365, mean_q: 43.618208, mean_eps: 0.855247
  24277/150000: episode: 269, duration: 0.781s, episode steps: 117, steps per second: 150, episode reward: -63.101, mean reward: -0.539 [-100.000, 28.065], mean action: 1.410 [0.000, 3.000],  loss: 15.668605, mae: 40.103227, mean_q: 43.778072, mean_eps: 0.854692
  24408/150000: episode: 270, duration: 0.907s, episode steps: 131, steps per second: 144, episode reward: -144.880, mean reward: -1.106 [-100.000,  4.600], mean action: 1.382 [0.000, 3.000],  loss: 14.857272, mae: 40.877148, mean_q: 44.669241, mean_eps: 0.853948
  24493/150000: episode: 271, duration: 0.570s, episode steps:  85, steps per second: 149, episode reward: -169.169, mean reward: -1.990 [-100.000,  7.083], mean action: 1.494 [0.000, 3.000],  loss: 9.246060, mae: 40.836858, mean_q: 44.838219, mean_eps: 0.853300
  24554/150000: episode: 272, duration: 0.413s, episode steps:  61, steps per second: 148, episode reward: -81.459, mean reward: -1.335 [-100.000,  7.422], mean action: 1.410 [0.000, 3.000],  loss: 10.957068, mae: 41.481661, mean_q: 46.268654, mean_eps: 0.852862
  24664/150000: episode: 273, duration: 0.757s, episode steps: 110, steps per second: 145, episode reward: -213.927, mean reward: -1.945 [-100.000,  7.314], mean action: 1.545 [0.000, 3.000],  loss: 10.545223, mae: 40.716196, mean_q: 45.042325, mean_eps: 0.852349
  24759/150000: episode: 274, duration: 0.652s, episode steps:  95, steps per second: 146, episode reward: -87.760, mean reward: -0.924 [-100.000, 11.858], mean action: 1.505 [0.000, 3.000],  loss: 9.376987, mae: 41.189085, mean_q: 44.857347, mean_eps: 0.851734
  24865/150000: episode: 275, duration: 0.707s, episode steps: 106, steps per second: 150, episode reward: -127.495, mean reward: -1.203 [-100.000, 14.463], mean action: 1.613 [0.000, 3.000],  loss: 11.128701, mae: 41.070372, mean_q: 44.035188, mean_eps: 0.851131
  24937/150000: episode: 276, duration: 0.486s, episode steps:  72, steps per second: 148, episode reward: -98.755, mean reward: -1.372 [-100.000, 10.487], mean action: 1.486 [0.000, 3.000],  loss: 6.873351, mae: 40.873019, mean_q: 45.076848, mean_eps: 0.850597
  25010/150000: episode: 277, duration: 0.521s, episode steps:  73, steps per second: 140, episode reward: -71.024, mean reward: -0.973 [-100.000,  8.661], mean action: 1.288 [0.000, 3.000],  loss: 13.716161, mae: 40.891367, mean_q: 43.655320, mean_eps: 0.850162
  25107/150000: episode: 278, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -62.401, mean reward: -0.643 [-100.000, 10.881], mean action: 1.680 [0.000, 3.000],  loss: 13.415274, mae: 41.516609, mean_q: 45.181180, mean_eps: 0.849652
  25187/150000: episode: 279, duration: 0.535s, episode steps:  80, steps per second: 150, episode reward: -80.633, mean reward: -1.008 [-100.000,  9.573], mean action: 1.575 [0.000, 3.000],  loss: 10.201066, mae: 40.646962, mean_q: 44.204527, mean_eps: 0.849121
  25271/150000: episode: 280, duration: 0.570s, episode steps:  84, steps per second: 147, episode reward: -218.308, mean reward: -2.599 [-100.000,  5.461], mean action: 1.464 [0.000, 3.000],  loss: 15.845451, mae: 41.426305, mean_q: 44.177908, mean_eps: 0.848629
  25371/150000: episode: 281, duration: 0.697s, episode steps: 100, steps per second: 143, episode reward: -89.491, mean reward: -0.895 [-100.000, 17.225], mean action: 1.600 [0.000, 3.000],  loss: 12.551396, mae: 41.048025, mean_q: 45.026160, mean_eps: 0.848077
  25451/150000: episode: 282, duration: 0.535s, episode steps:  80, steps per second: 150, episode reward: -108.845, mean reward: -1.361 [-100.000, 21.756], mean action: 1.438 [0.000, 3.000],  loss: 11.675443, mae: 40.717317, mean_q: 44.698493, mean_eps: 0.847537
  25545/150000: episode: 283, duration: 0.640s, episode steps:  94, steps per second: 147, episode reward: -161.754, mean reward: -1.721 [-100.000,  6.810], mean action: 1.660 [0.000, 3.000],  loss: 17.665138, mae: 41.116449, mean_q: 44.060322, mean_eps: 0.847015
  25644/150000: episode: 284, duration: 0.715s, episode steps:  99, steps per second: 138, episode reward: -96.058, mean reward: -0.970 [-100.000,  7.931], mean action: 1.596 [0.000, 3.000],  loss: 13.187446, mae: 41.118776, mean_q: 43.184264, mean_eps: 0.846436
  25748/150000: episode: 285, duration: 0.703s, episode steps: 104, steps per second: 148, episode reward: -121.819, mean reward: -1.171 [-100.000, 10.430], mean action: 1.538 [0.000, 3.000],  loss: 11.310104, mae: 41.091087, mean_q: 43.952490, mean_eps: 0.845827
  25869/150000: episode: 286, duration: 0.811s, episode steps: 121, steps per second: 149, episode reward: -82.239, mean reward: -0.680 [-100.000, 28.787], mean action: 1.545 [0.000, 3.000],  loss: 10.784456, mae: 41.040050, mean_q: 43.213466, mean_eps: 0.845152
  25934/150000: episode: 287, duration: 0.457s, episode steps:  65, steps per second: 142, episode reward: -100.751, mean reward: -1.550 [-100.000, 15.008], mean action: 1.523 [0.000, 3.000],  loss: 14.895576, mae: 41.502117, mean_q: 44.422160, mean_eps: 0.844594
  26050/150000: episode: 288, duration: 0.788s, episode steps: 116, steps per second: 147, episode reward: -67.426, mean reward: -0.581 [-100.000, 14.543], mean action: 1.483 [0.000, 3.000],  loss: 8.740522, mae: 41.693643, mean_q: 45.871680, mean_eps: 0.844051
  26159/150000: episode: 289, duration: 0.731s, episode steps: 109, steps per second: 149, episode reward: -120.054, mean reward: -1.101 [-100.000, 10.689], mean action: 1.596 [0.000, 3.000],  loss: 10.417816, mae: 42.073300, mean_q: 46.354389, mean_eps: 0.843376
  26255/150000: episode: 290, duration: 0.665s, episode steps:  96, steps per second: 144, episode reward: -103.794, mean reward: -1.081 [-100.000, 11.304], mean action: 1.438 [0.000, 3.000],  loss: 15.479313, mae: 42.519706, mean_q: 46.499316, mean_eps: 0.842761
  26329/150000: episode: 291, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -117.867, mean reward: -1.593 [-100.000, 24.739], mean action: 1.419 [0.000, 3.000],  loss: 11.637790, mae: 43.337010, mean_q: 46.736993, mean_eps: 0.842251
  26410/150000: episode: 292, duration: 0.550s, episode steps:  81, steps per second: 147, episode reward: -19.877, mean reward: -0.245 [-100.000, 35.634], mean action: 1.568 [0.000, 3.000],  loss: 12.736118, mae: 42.116047, mean_q: 46.522079, mean_eps: 0.841786
  26475/150000: episode: 293, duration: 0.438s, episode steps:  65, steps per second: 148, episode reward: -80.782, mean reward: -1.243 [-100.000,  6.777], mean action: 1.492 [0.000, 3.000],  loss: 9.015092, mae: 41.911757, mean_q: 44.491724, mean_eps: 0.841348
  26583/150000: episode: 294, duration: 0.754s, episode steps: 108, steps per second: 143, episode reward: -98.627, mean reward: -0.913 [-100.000,  6.963], mean action: 1.667 [0.000, 3.000],  loss: 8.868568, mae: 42.268398, mean_q: 46.241884, mean_eps: 0.840829
  26664/150000: episode: 295, duration: 0.557s, episode steps:  81, steps per second: 145, episode reward: -77.455, mean reward: -0.956 [-100.000, 11.162], mean action: 1.494 [0.000, 3.000],  loss: 11.783678, mae: 42.243234, mean_q: 47.524533, mean_eps: 0.840262
  26769/150000: episode: 296, duration: 0.712s, episode steps: 105, steps per second: 147, episode reward: -81.123, mean reward: -0.773 [-100.000, 12.747], mean action: 1.524 [0.000, 3.000],  loss: 12.227871, mae: 42.672973, mean_q: 45.723900, mean_eps: 0.839704
  26879/150000: episode: 297, duration: 0.752s, episode steps: 110, steps per second: 146, episode reward: -80.451, mean reward: -0.731 [-100.000, 10.105], mean action: 1.382 [0.000, 3.000],  loss: 11.020621, mae: 42.324404, mean_q: 46.106818, mean_eps: 0.839059
  26955/150000: episode: 298, duration: 0.531s, episode steps:  76, steps per second: 143, episode reward: -78.385, mean reward: -1.031 [-100.000,  7.025], mean action: 1.724 [0.000, 3.000],  loss: 14.054236, mae: 42.542458, mean_q: 45.817818, mean_eps: 0.838501
  27046/150000: episode: 299, duration: 0.620s, episode steps:  91, steps per second: 147, episode reward: -110.314, mean reward: -1.212 [-100.000,  6.796], mean action: 1.571 [0.000, 3.000],  loss: 11.268994, mae: 43.041170, mean_q: 46.943832, mean_eps: 0.838000
  27150/150000: episode: 300, duration: 0.699s, episode steps: 104, steps per second: 149, episode reward: -299.157, mean reward: -2.877 [-100.000, 119.412], mean action: 1.663 [0.000, 3.000],  loss: 10.775953, mae: 43.229946, mean_q: 47.002923, mean_eps: 0.837415
  27229/150000: episode: 301, duration: 0.572s, episode steps:  79, steps per second: 138, episode reward: -97.398, mean reward: -1.233 [-100.000, 19.036], mean action: 1.544 [0.000, 3.000],  loss: 14.483205, mae: 42.858239, mean_q: 46.149861, mean_eps: 0.836866
  27306/150000: episode: 302, duration: 0.521s, episode steps:  77, steps per second: 148, episode reward: -121.805, mean reward: -1.582 [-100.000,  6.210], mean action: 1.468 [0.000, 3.000],  loss: 14.606920, mae: 42.970731, mean_q: 47.327964, mean_eps: 0.836398
  27418/150000: episode: 303, duration: 0.751s, episode steps: 112, steps per second: 149, episode reward: -144.215, mean reward: -1.288 [-100.000, 10.480], mean action: 1.518 [0.000, 3.000],  loss: 10.749373, mae: 43.688929, mean_q: 47.153464, mean_eps: 0.835831
  27507/150000: episode: 304, duration: 0.644s, episode steps:  89, steps per second: 138, episode reward: -169.938, mean reward: -1.909 [-100.000,  8.452], mean action: 1.483 [0.000, 3.000],  loss: 16.641768, mae: 43.567302, mean_q: 47.115716, mean_eps: 0.835228
  27597/150000: episode: 305, duration: 0.624s, episode steps:  90, steps per second: 144, episode reward: -103.566, mean reward: -1.151 [-100.000, 12.887], mean action: 1.500 [0.000, 3.000],  loss: 12.850732, mae: 43.581037, mean_q: 46.552183, mean_eps: 0.834691
  27696/150000: episode: 306, duration: 0.666s, episode steps:  99, steps per second: 149, episode reward: -149.880, mean reward: -1.514 [-100.000,  7.719], mean action: 1.556 [0.000, 3.000],  loss: 8.284305, mae: 43.640288, mean_q: 47.164293, mean_eps: 0.834124
  27805/150000: episode: 307, duration: 0.743s, episode steps: 109, steps per second: 147, episode reward: -79.254, mean reward: -0.727 [-100.000, 16.941], mean action: 1.523 [0.000, 3.000],  loss: 8.955673, mae: 43.148499, mean_q: 45.933422, mean_eps: 0.833500
  27922/150000: episode: 308, duration: 0.807s, episode steps: 117, steps per second: 145, episode reward: -90.740, mean reward: -0.776 [-100.000, 22.750], mean action: 1.624 [0.000, 3.000],  loss: 11.304250, mae: 43.028745, mean_q: 46.711227, mean_eps: 0.832822
  27994/150000: episode: 309, duration: 0.484s, episode steps:  72, steps per second: 149, episode reward: -70.422, mean reward: -0.978 [-100.000,  7.658], mean action: 1.583 [0.000, 3.000],  loss: 16.093547, mae: 43.065258, mean_q: 45.927629, mean_eps: 0.832255
  28060/150000: episode: 310, duration: 0.444s, episode steps:  66, steps per second: 149, episode reward: -75.096, mean reward: -1.138 [-100.000,  8.838], mean action: 1.667 [0.000, 3.000],  loss: 8.425316, mae: 44.193139, mean_q: 47.518463, mean_eps: 0.831841
  28180/150000: episode: 311, duration: 0.863s, episode steps: 120, steps per second: 139, episode reward: -97.443, mean reward: -0.812 [-100.000,  7.941], mean action: 1.450 [0.000, 3.000],  loss: 11.721830, mae: 44.135785, mean_q: 46.606130, mean_eps: 0.831283
  28253/150000: episode: 312, duration: 0.504s, episode steps:  73, steps per second: 145, episode reward: -82.792, mean reward: -1.134 [-100.000, 13.111], mean action: 1.534 [0.000, 3.000],  loss: 18.093665, mae: 44.317836, mean_q: 47.799330, mean_eps: 0.830704
  28379/150000: episode: 313, duration: 0.907s, episode steps: 126, steps per second: 139, episode reward: -180.450, mean reward: -1.432 [-100.000,  4.497], mean action: 1.706 [0.000, 3.000],  loss: 9.109107, mae: 43.867123, mean_q: 46.890507, mean_eps: 0.830107
  28487/150000: episode: 314, duration: 0.769s, episode steps: 108, steps per second: 140, episode reward: -66.628, mean reward: -0.617 [-100.000, 20.851], mean action: 1.713 [0.000, 3.000],  loss: 23.699806, mae: 43.782463, mean_q: 46.636513, mean_eps: 0.829405
  28546/150000: episode: 315, duration: 0.405s, episode steps:  59, steps per second: 146, episode reward: -60.517, mean reward: -1.026 [-100.000,  9.886], mean action: 1.441 [0.000, 3.000],  loss: 9.486776, mae: 45.200969, mean_q: 47.792642, mean_eps: 0.828904
  28670/150000: episode: 316, duration: 0.829s, episode steps: 124, steps per second: 150, episode reward: -106.194, mean reward: -0.856 [-100.000, 17.120], mean action: 1.645 [0.000, 3.000],  loss: 17.601275, mae: 44.357367, mean_q: 46.793890, mean_eps: 0.828355
  28756/150000: episode: 317, duration: 0.599s, episode steps:  86, steps per second: 143, episode reward: -288.696, mean reward: -3.357 [-100.000, 61.743], mean action: 1.698 [0.000, 3.000],  loss: 17.676651, mae: 43.826786, mean_q: 45.819633, mean_eps: 0.827725
  28888/150000: episode: 318, duration: 0.892s, episode steps: 132, steps per second: 148, episode reward: -39.309, mean reward: -0.298 [-100.000, 24.743], mean action: 1.500 [0.000, 3.000],  loss: 10.410567, mae: 43.850222, mean_q: 46.893565, mean_eps: 0.827071
  28988/150000: episode: 319, duration: 0.685s, episode steps: 100, steps per second: 146, episode reward: -67.215, mean reward: -0.672 [-100.000, 10.611], mean action: 1.600 [0.000, 3.000],  loss: 13.110561, mae: 44.596202, mean_q: 47.815343, mean_eps: 0.826375
  29061/150000: episode: 320, duration: 0.509s, episode steps:  73, steps per second: 143, episode reward: -58.776, mean reward: -0.805 [-100.000, 17.815], mean action: 1.671 [0.000, 3.000],  loss: 9.213581, mae: 45.350041, mean_q: 49.071283, mean_eps: 0.825856
  29158/150000: episode: 321, duration: 0.670s, episode steps:  97, steps per second: 145, episode reward: -100.928, mean reward: -1.040 [-100.000,  8.105], mean action: 1.423 [0.000, 3.000],  loss: 10.750427, mae: 44.908756, mean_q: 49.991877, mean_eps: 0.825346
  29244/150000: episode: 322, duration: 0.578s, episode steps:  86, steps per second: 149, episode reward: -97.946, mean reward: -1.139 [-100.000,  7.353], mean action: 1.628 [0.000, 3.000],  loss: 10.753606, mae: 45.086975, mean_q: 48.730635, mean_eps: 0.824797
  29358/150000: episode: 323, duration: 0.771s, episode steps: 114, steps per second: 148, episode reward: -110.646, mean reward: -0.971 [-100.000, 11.297], mean action: 1.368 [0.000, 3.000],  loss: 19.291140, mae: 44.589458, mean_q: 48.152682, mean_eps: 0.824197
  29425/150000: episode: 324, duration: 0.472s, episode steps:  67, steps per second: 142, episode reward: -105.110, mean reward: -1.569 [-100.000,  8.136], mean action: 1.746 [0.000, 3.000],  loss: 9.042808, mae: 44.839256, mean_q: 48.915935, mean_eps: 0.823654
  29500/150000: episode: 325, duration: 0.502s, episode steps:  75, steps per second: 149, episode reward: -69.610, mean reward: -0.928 [-100.000,  6.765], mean action: 1.613 [0.000, 3.000],  loss: 12.239252, mae: 45.409010, mean_q: 48.616369, mean_eps: 0.823228
  29613/150000: episode: 326, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -102.403, mean reward: -0.906 [-100.000,  9.786], mean action: 1.628 [0.000, 3.000],  loss: 12.046544, mae: 45.346973, mean_q: 49.173979, mean_eps: 0.822664
  29700/150000: episode: 327, duration: 0.610s, episode steps:  87, steps per second: 143, episode reward: -93.231, mean reward: -1.072 [-100.000, 17.324], mean action: 1.839 [0.000, 3.000],  loss: 9.944684, mae: 45.208086, mean_q: 49.659093, mean_eps: 0.822064
  29810/150000: episode: 328, duration: 0.740s, episode steps: 110, steps per second: 149, episode reward: -97.443, mean reward: -0.886 [-100.000, 18.809], mean action: 1.573 [0.000, 3.000],  loss: 11.288537, mae: 45.692568, mean_q: 49.696956, mean_eps: 0.821473
  29874/150000: episode: 329, duration: 0.443s, episode steps:  64, steps per second: 145, episode reward: -39.443, mean reward: -0.616 [-100.000, 16.787], mean action: 1.484 [0.000, 3.000],  loss: 9.942809, mae: 45.796388, mean_q: 49.913167, mean_eps: 0.820951
  29940/150000: episode: 330, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -65.191, mean reward: -0.988 [-100.000, 13.488], mean action: 1.348 [0.000, 3.000],  loss: 12.234928, mae: 45.656698, mean_q: 48.438354, mean_eps: 0.820561
  30025/150000: episode: 331, duration: 0.589s, episode steps:  85, steps per second: 144, episode reward: -93.234, mean reward: -1.097 [-100.000, 20.435], mean action: 1.529 [0.000, 3.000],  loss: 8.619565, mae: 45.552958, mean_q: 48.654771, mean_eps: 0.820108
  30106/150000: episode: 332, duration: 0.553s, episode steps:  81, steps per second: 146, episode reward: -76.985, mean reward: -0.950 [-100.000, 12.659], mean action: 1.444 [0.000, 3.000],  loss: 10.888938, mae: 46.635283, mean_q: 49.745827, mean_eps: 0.819610
  30175/150000: episode: 333, duration: 0.470s, episode steps:  69, steps per second: 147, episode reward: -77.096, mean reward: -1.117 [-100.000, 10.365], mean action: 1.594 [0.000, 3.000],  loss: 13.354738, mae: 46.191666, mean_q: 49.324799, mean_eps: 0.819160
  30272/150000: episode: 334, duration: 0.649s, episode steps:  97, steps per second: 150, episode reward: -102.957, mean reward: -1.061 [-100.000,  8.257], mean action: 1.464 [0.000, 3.000],  loss: 8.408596, mae: 45.778452, mean_q: 48.970825, mean_eps: 0.818662
  30360/150000: episode: 335, duration: 0.619s, episode steps:  88, steps per second: 142, episode reward: -81.956, mean reward: -0.931 [-100.000, 23.132], mean action: 1.523 [0.000, 3.000],  loss: 15.554544, mae: 45.575948, mean_q: 47.551163, mean_eps: 0.818107
  30449/150000: episode: 336, duration: 0.602s, episode steps:  89, steps per second: 148, episode reward: -96.409, mean reward: -1.083 [-100.000, 16.953], mean action: 1.607 [0.000, 3.000],  loss: 15.106793, mae: 45.793618, mean_q: 48.615374, mean_eps: 0.817576
  30550/150000: episode: 337, duration: 0.682s, episode steps: 101, steps per second: 148, episode reward: -145.121, mean reward: -1.437 [-100.000,  8.579], mean action: 1.663 [0.000, 3.000],  loss: 16.677719, mae: 46.012128, mean_q: 49.516474, mean_eps: 0.817006
  30631/150000: episode: 338, duration: 0.564s, episode steps:  81, steps per second: 144, episode reward: -67.994, mean reward: -0.839 [-100.000,  6.779], mean action: 1.506 [0.000, 3.000],  loss: 17.136124, mae: 46.706353, mean_q: 50.213237, mean_eps: 0.816460
  30743/150000: episode: 339, duration: 0.768s, episode steps: 112, steps per second: 146, episode reward: -93.259, mean reward: -0.833 [-100.000,  7.947], mean action: 1.509 [0.000, 3.000],  loss: 11.535275, mae: 46.602850, mean_q: 49.494094, mean_eps: 0.815881
  30823/150000: episode: 340, duration: 0.539s, episode steps:  80, steps per second: 148, episode reward: -89.655, mean reward: -1.121 [-100.000, 17.828], mean action: 1.550 [0.000, 3.000],  loss: 12.924835, mae: 46.566834, mean_q: 49.456807, mean_eps: 0.815305
  30946/150000: episode: 341, duration: 0.837s, episode steps: 123, steps per second: 147, episode reward: -53.685, mean reward: -0.436 [-100.000,  7.268], mean action: 1.293 [0.000, 3.000],  loss: 8.555950, mae: 46.280221, mean_q: 49.500533, mean_eps: 0.814696
  31017/150000: episode: 342, duration: 0.491s, episode steps:  71, steps per second: 145, episode reward: -61.700, mean reward: -0.869 [-100.000,  6.959], mean action: 1.690 [0.000, 3.000],  loss: 12.010598, mae: 46.457763, mean_q: 48.239646, mean_eps: 0.814114
  31116/150000: episode: 343, duration: 0.670s, episode steps:  99, steps per second: 148, episode reward: -84.150, mean reward: -0.850 [-100.000, 11.438], mean action: 1.545 [0.000, 3.000],  loss: 13.611862, mae: 47.119796, mean_q: 50.379290, mean_eps: 0.813604
  31180/150000: episode: 344, duration: 0.449s, episode steps:  64, steps per second: 143, episode reward: -86.102, mean reward: -1.345 [-100.000,  6.935], mean action: 1.703 [0.000, 3.000],  loss: 12.411165, mae: 46.904663, mean_q: 49.076199, mean_eps: 0.813115
  31297/150000: episode: 345, duration: 0.825s, episode steps: 117, steps per second: 142, episode reward: -130.842, mean reward: -1.118 [-100.000, 10.819], mean action: 1.513 [0.000, 3.000],  loss: 17.130526, mae: 47.401761, mean_q: 50.339544, mean_eps: 0.812572
  31393/150000: episode: 346, duration: 0.652s, episode steps:  96, steps per second: 147, episode reward: -117.583, mean reward: -1.225 [-100.000,  6.079], mean action: 1.385 [0.000, 3.000],  loss: 15.325726, mae: 47.292148, mean_q: 50.027989, mean_eps: 0.811933
  31492/150000: episode: 347, duration: 0.669s, episode steps:  99, steps per second: 148, episode reward: -113.745, mean reward: -1.149 [-100.000,  6.790], mean action: 1.455 [0.000, 3.000],  loss: 11.133479, mae: 47.036103, mean_q: 50.050154, mean_eps: 0.811348
  31587/150000: episode: 348, duration: 0.659s, episode steps:  95, steps per second: 144, episode reward: -184.626, mean reward: -1.943 [-100.000,  8.366], mean action: 1.505 [0.000, 3.000],  loss: 15.002000, mae: 47.219124, mean_q: 49.323628, mean_eps: 0.810766
  31669/150000: episode: 349, duration: 0.560s, episode steps:  82, steps per second: 146, episode reward: -128.912, mean reward: -1.572 [-100.000,  8.937], mean action: 1.524 [0.000, 3.000],  loss: 10.718577, mae: 47.040121, mean_q: 49.498407, mean_eps: 0.810235
  31797/150000: episode: 350, duration: 0.873s, episode steps: 128, steps per second: 147, episode reward: -37.946, mean reward: -0.296 [-100.000, 52.311], mean action: 1.516 [0.000, 3.000],  loss: 21.094518, mae: 47.357645, mean_q: 49.014663, mean_eps: 0.809605
  31860/150000: episode: 351, duration: 0.427s, episode steps:  63, steps per second: 147, episode reward: -58.801, mean reward: -0.933 [-100.000, 12.315], mean action: 1.698 [0.000, 3.000],  loss: 12.592739, mae: 47.627797, mean_q: 48.964848, mean_eps: 0.809032
  31985/150000: episode: 352, duration: 0.990s, episode steps: 125, steps per second: 126, episode reward: -87.707, mean reward: -0.702 [-100.000,  7.435], mean action: 1.432 [0.000, 3.000],  loss: 18.506663, mae: 47.160710, mean_q: 49.506663, mean_eps: 0.808468
  32076/150000: episode: 353, duration: 0.721s, episode steps:  91, steps per second: 126, episode reward: -95.754, mean reward: -1.052 [-100.000,  8.609], mean action: 1.637 [0.000, 3.000],  loss: 10.943820, mae: 47.395172, mean_q: 48.858942, mean_eps: 0.807820
  32195/150000: episode: 354, duration: 0.945s, episode steps: 119, steps per second: 126, episode reward: -137.470, mean reward: -1.155 [-100.000,  8.330], mean action: 1.546 [0.000, 3.000],  loss: 13.778656, mae: 47.853065, mean_q: 50.000641, mean_eps: 0.807190
  32267/150000: episode: 355, duration: 0.543s, episode steps:  72, steps per second: 133, episode reward: -181.398, mean reward: -2.519 [-100.000,  6.639], mean action: 1.611 [0.000, 3.000],  loss: 8.936892, mae: 47.485929, mean_q: 49.775465, mean_eps: 0.806617
  32343/150000: episode: 356, duration: 0.538s, episode steps:  76, steps per second: 141, episode reward: -58.007, mean reward: -0.763 [-100.000, 10.387], mean action: 1.447 [0.000, 3.000],  loss: 13.992863, mae: 48.079879, mean_q: 50.517313, mean_eps: 0.806173
  32450/150000: episode: 357, duration: 0.791s, episode steps: 107, steps per second: 135, episode reward: -114.880, mean reward: -1.074 [-100.000, 31.470], mean action: 1.645 [0.000, 3.000],  loss: 12.261840, mae: 48.666351, mean_q: 49.952705, mean_eps: 0.805624
  32578/150000: episode: 358, duration: 0.937s, episode steps: 128, steps per second: 137, episode reward: -114.229, mean reward: -0.892 [-100.000, 18.284], mean action: 1.406 [0.000, 3.000],  loss: 10.764207, mae: 48.045164, mean_q: 50.651093, mean_eps: 0.804919
  32655/150000: episode: 359, duration: 0.522s, episode steps:  77, steps per second: 148, episode reward: -65.716, mean reward: -0.853 [-100.000, 14.923], mean action: 1.429 [0.000, 3.000],  loss: 11.525289, mae: 47.575058, mean_q: 50.360726, mean_eps: 0.804304
  32765/150000: episode: 360, duration: 0.834s, episode steps: 110, steps per second: 132, episode reward: -201.846, mean reward: -1.835 [-100.000, 13.604], mean action: 1.500 [0.000, 3.000],  loss: 12.135473, mae: 47.612010, mean_q: 49.782373, mean_eps: 0.803743
  32857/150000: episode: 361, duration: 0.661s, episode steps:  92, steps per second: 139, episode reward: -71.700, mean reward: -0.779 [-100.000,  8.235], mean action: 1.565 [0.000, 3.000],  loss: 17.652969, mae: 48.574328, mean_q: 51.156783, mean_eps: 0.803137
  32959/150000: episode: 362, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: -176.206, mean reward: -1.728 [-100.000,  5.558], mean action: 1.275 [0.000, 3.000],  loss: 10.759660, mae: 48.191758, mean_q: 50.330420, mean_eps: 0.802555
  33031/150000: episode: 363, duration: 0.489s, episode steps:  72, steps per second: 147, episode reward: -85.660, mean reward: -1.190 [-100.000, 11.013], mean action: 1.333 [0.000, 3.000],  loss: 16.997140, mae: 49.564417, mean_q: 51.016805, mean_eps: 0.802033
  33132/150000: episode: 364, duration: 0.714s, episode steps: 101, steps per second: 141, episode reward: -153.457, mean reward: -1.519 [-100.000,  4.594], mean action: 1.752 [0.000, 3.000],  loss: 13.338212, mae: 49.636372, mean_q: 52.519203, mean_eps: 0.801514
  33230/150000: episode: 365, duration: 0.678s, episode steps:  98, steps per second: 145, episode reward: -97.062, mean reward: -0.990 [-100.000,  7.361], mean action: 1.520 [0.000, 3.000],  loss: 18.518239, mae: 49.424174, mean_q: 52.264461, mean_eps: 0.800917
  33334/150000: episode: 366, duration: 0.705s, episode steps: 104, steps per second: 147, episode reward: -56.939, mean reward: -0.547 [-100.000, 18.563], mean action: 1.606 [0.000, 3.000],  loss: 15.227703, mae: 49.161278, mean_q: 51.334769, mean_eps: 0.800311
  33427/150000: episode: 367, duration: 0.664s, episode steps:  93, steps per second: 140, episode reward: -65.768, mean reward: -0.707 [-100.000,  8.135], mean action: 1.484 [0.000, 3.000],  loss: 17.011635, mae: 48.696870, mean_q: 50.119878, mean_eps: 0.799720
  33520/150000: episode: 368, duration: 0.629s, episode steps:  93, steps per second: 148, episode reward: -127.260, mean reward: -1.368 [-100.000, 21.014], mean action: 1.290 [0.000, 3.000],  loss: 15.120110, mae: 49.371302, mean_q: 50.021086, mean_eps: 0.799162
  33612/150000: episode: 369, duration: 0.615s, episode steps:  92, steps per second: 150, episode reward: -116.320, mean reward: -1.264 [-100.000,  6.793], mean action: 1.609 [0.000, 3.000],  loss: 12.381817, mae: 49.458093, mean_q: 50.467898, mean_eps: 0.798607
  33732/150000: episode: 370, duration: 0.828s, episode steps: 120, steps per second: 145, episode reward: -91.818, mean reward: -0.765 [-100.000, 10.609], mean action: 1.675 [0.000, 3.000],  loss: 11.778554, mae: 49.916983, mean_q: 51.485608, mean_eps: 0.797971
  33804/150000: episode: 371, duration: 0.488s, episode steps:  72, steps per second: 148, episode reward: -75.468, mean reward: -1.048 [-100.000,  9.581], mean action: 1.542 [0.000, 3.000],  loss: 8.222047, mae: 49.843470, mean_q: 51.859644, mean_eps: 0.797395
  33929/150000: episode: 372, duration: 0.835s, episode steps: 125, steps per second: 150, episode reward: -138.989, mean reward: -1.112 [-100.000, 10.749], mean action: 1.496 [0.000, 3.000],  loss: 17.976463, mae: 49.713254, mean_q: 50.172857, mean_eps: 0.796804
  34018/150000: episode: 373, duration: 0.617s, episode steps:  89, steps per second: 144, episode reward: -63.411, mean reward: -0.712 [-100.000, 13.676], mean action: 1.404 [0.000, 3.000],  loss: 15.585889, mae: 49.097102, mean_q: 50.093922, mean_eps: 0.796162
  34090/150000: episode: 374, duration: 0.489s, episode steps:  72, steps per second: 147, episode reward: -148.953, mean reward: -2.069 [-100.000, 34.352], mean action: 1.653 [0.000, 3.000],  loss: 12.524662, mae: 49.508289, mean_q: 49.817560, mean_eps: 0.795679
  34162/150000: episode: 375, duration: 0.486s, episode steps:  72, steps per second: 148, episode reward: -64.781, mean reward: -0.900 [-100.000,  9.661], mean action: 1.819 [0.000, 3.000],  loss: 12.630143, mae: 49.279212, mean_q: 50.192391, mean_eps: 0.795247
  34242/150000: episode: 376, duration: 0.544s, episode steps:  80, steps per second: 147, episode reward: -76.072, mean reward: -0.951 [-100.000,  9.430], mean action: 1.738 [0.000, 3.000],  loss: 13.486482, mae: 49.932332, mean_q: 52.101195, mean_eps: 0.794791
  34306/150000: episode: 377, duration: 0.440s, episode steps:  64, steps per second: 146, episode reward: -88.208, mean reward: -1.378 [-100.000, 12.141], mean action: 1.594 [0.000, 3.000],  loss: 12.088425, mae: 49.905456, mean_q: 52.289465, mean_eps: 0.794359
  34392/150000: episode: 378, duration: 0.608s, episode steps:  86, steps per second: 142, episode reward: -76.263, mean reward: -0.887 [-100.000, 12.941], mean action: 1.395 [0.000, 3.000],  loss: 13.651320, mae: 49.212831, mean_q: 49.594172, mean_eps: 0.793909
  34480/150000: episode: 379, duration: 0.591s, episode steps:  88, steps per second: 149, episode reward: -117.988, mean reward: -1.341 [-100.000,  5.692], mean action: 1.352 [0.000, 3.000],  loss: 10.426621, mae: 49.388417, mean_q: 49.991934, mean_eps: 0.793387
  34547/150000: episode: 380, duration: 0.455s, episode steps:  67, steps per second: 147, episode reward: -80.607, mean reward: -1.203 [-100.000, 17.507], mean action: 1.642 [0.000, 3.000],  loss: 11.975285, mae: 50.609616, mean_q: 52.133035, mean_eps: 0.792922
  34660/150000: episode: 381, duration: 0.783s, episode steps: 113, steps per second: 144, episode reward: -58.704, mean reward: -0.520 [-100.000, 11.892], mean action: 1.522 [0.000, 3.000],  loss: 10.836076, mae: 50.335030, mean_q: 51.676849, mean_eps: 0.792382
  34761/150000: episode: 382, duration: 0.689s, episode steps: 101, steps per second: 147, episode reward: -98.752, mean reward: -0.978 [-100.000, 15.086], mean action: 1.396 [0.000, 3.000],  loss: 14.788338, mae: 50.139495, mean_q: 51.626521, mean_eps: 0.791740
  34893/150000: episode: 383, duration: 0.884s, episode steps: 132, steps per second: 149, episode reward: -341.907, mean reward: -2.590 [-100.000, 104.969], mean action: 1.712 [0.000, 3.000],  loss: 12.643453, mae: 50.014942, mean_q: 51.845915, mean_eps: 0.791041
  34946/150000: episode: 384, duration: 0.367s, episode steps:  53, steps per second: 144, episode reward: -109.523, mean reward: -2.066 [-100.000,  4.331], mean action: 1.396 [0.000, 3.000],  loss: 15.828080, mae: 50.546932, mean_q: 51.452105, mean_eps: 0.790486
  35046/150000: episode: 385, duration: 0.684s, episode steps: 100, steps per second: 146, episode reward: -101.547, mean reward: -1.015 [-100.000, 10.538], mean action: 1.400 [0.000, 3.000],  loss: 11.194842, mae: 50.384158, mean_q: 52.261922, mean_eps: 0.790027
  35155/150000: episode: 386, duration: 0.729s, episode steps: 109, steps per second: 149, episode reward: -114.530, mean reward: -1.051 [-100.000,  7.812], mean action: 1.560 [0.000, 3.000],  loss: 15.608005, mae: 50.830469, mean_q: 53.287891, mean_eps: 0.789400
  35232/150000: episode: 387, duration: 0.517s, episode steps:  77, steps per second: 149, episode reward: -76.216, mean reward: -0.990 [-100.000,  7.295], mean action: 1.558 [0.000, 3.000],  loss: 12.650515, mae: 50.732384, mean_q: 51.967376, mean_eps: 0.788842
  35314/150000: episode: 388, duration: 0.574s, episode steps:  82, steps per second: 143, episode reward: -98.538, mean reward: -1.202 [-100.000,  5.629], mean action: 1.537 [0.000, 3.000],  loss: 14.083637, mae: 50.763079, mean_q: 52.820646, mean_eps: 0.788365
  35395/150000: episode: 389, duration: 0.551s, episode steps:  81, steps per second: 147, episode reward: -106.824, mean reward: -1.319 [-100.000,  7.042], mean action: 1.716 [0.000, 3.000],  loss: 8.783578, mae: 51.196252, mean_q: 52.680399, mean_eps: 0.787876
  35491/150000: episode: 390, duration: 0.641s, episode steps:  96, steps per second: 150, episode reward: -90.154, mean reward: -0.939 [-100.000,  8.974], mean action: 1.562 [0.000, 3.000],  loss: 11.259859, mae: 50.336083, mean_q: 51.629450, mean_eps: 0.787345
  35566/150000: episode: 391, duration: 0.507s, episode steps:  75, steps per second: 148, episode reward: -81.113, mean reward: -1.082 [-100.000,  7.906], mean action: 1.533 [0.000, 3.000],  loss: 11.989164, mae: 51.185458, mean_q: 52.968066, mean_eps: 0.786832
  35651/150000: episode: 392, duration: 0.599s, episode steps:  85, steps per second: 142, episode reward: -113.817, mean reward: -1.339 [-100.000, 16.322], mean action: 1.306 [0.000, 3.000],  loss: 9.135234, mae: 51.338484, mean_q: 54.281830, mean_eps: 0.786352
  35737/150000: episode: 393, duration: 0.587s, episode steps:  86, steps per second: 147, episode reward: -79.241, mean reward: -0.921 [-100.000, 21.361], mean action: 1.558 [0.000, 3.000],  loss: 8.649871, mae: 50.950104, mean_q: 52.958229, mean_eps: 0.785839
  35832/150000: episode: 394, duration: 0.636s, episode steps:  95, steps per second: 149, episode reward: -113.417, mean reward: -1.194 [-100.000, 11.367], mean action: 1.474 [0.000, 3.000],  loss: 13.422885, mae: 50.783998, mean_q: 51.391601, mean_eps: 0.785296
  35938/150000: episode: 395, duration: 0.755s, episode steps: 106, steps per second: 140, episode reward: -237.525, mean reward: -2.241 [-100.000, 35.451], mean action: 1.481 [0.000, 3.000],  loss: 16.630288, mae: 51.490185, mean_q: 52.234688, mean_eps: 0.784693
  36048/150000: episode: 396, duration: 0.750s, episode steps: 110, steps per second: 147, episode reward: -121.717, mean reward: -1.107 [-100.000,  7.525], mean action: 1.500 [0.000, 3.000],  loss: 14.327083, mae: 51.373455, mean_q: 51.717224, mean_eps: 0.784045
  36132/150000: episode: 397, duration: 0.570s, episode steps:  84, steps per second: 147, episode reward: -50.408, mean reward: -0.600 [-100.000, 15.302], mean action: 1.655 [0.000, 3.000],  loss: 9.407898, mae: 51.713741, mean_q: 52.249394, mean_eps: 0.783463
  36215/150000: episode: 398, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -89.841, mean reward: -1.082 [-100.000, 14.191], mean action: 1.723 [0.000, 3.000],  loss: 7.586619, mae: 51.692270, mean_q: 51.667786, mean_eps: 0.782962
  36337/150000: episode: 399, duration: 0.837s, episode steps: 122, steps per second: 146, episode reward: -93.808, mean reward: -0.769 [-100.000,  5.459], mean action: 1.746 [0.000, 3.000],  loss: 14.929057, mae: 51.505320, mean_q: 52.319007, mean_eps: 0.782347
  36454/150000: episode: 400, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -190.231, mean reward: -1.626 [-100.000,  4.172], mean action: 1.786 [0.000, 3.000],  loss: 11.584212, mae: 51.214023, mean_q: 52.123319, mean_eps: 0.781630
  36521/150000: episode: 401, duration: 0.466s, episode steps:  67, steps per second: 144, episode reward: -41.295, mean reward: -0.616 [-100.000, 25.453], mean action: 1.657 [0.000, 3.000],  loss: 12.856129, mae: 52.431667, mean_q: 53.381830, mean_eps: 0.781078
  36599/150000: episode: 402, duration: 0.541s, episode steps:  78, steps per second: 144, episode reward: -90.377, mean reward: -1.159 [-100.000, 22.858], mean action: 1.577 [0.000, 3.000],  loss: 11.641084, mae: 51.804377, mean_q: 52.893968, mean_eps: 0.780643
  36673/150000: episode: 403, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward: -42.331, mean reward: -0.572 [-100.000, 20.801], mean action: 1.676 [0.000, 3.000],  loss: 9.310952, mae: 51.953149, mean_q: 52.248972, mean_eps: 0.780187
  36764/150000: episode: 404, duration: 0.608s, episode steps:  91, steps per second: 150, episode reward: -123.415, mean reward: -1.356 [-100.000,  8.785], mean action: 1.451 [0.000, 3.000],  loss: 14.378416, mae: 52.376779, mean_q: 53.136205, mean_eps: 0.779692
  36835/150000: episode: 405, duration: 0.504s, episode steps:  71, steps per second: 141, episode reward: -40.199, mean reward: -0.566 [-100.000, 10.632], mean action: 1.577 [0.000, 3.000],  loss: 11.112828, mae: 52.206079, mean_q: 52.674958, mean_eps: 0.779206
  36915/150000: episode: 406, duration: 0.556s, episode steps:  80, steps per second: 144, episode reward: -38.683, mean reward: -0.484 [-100.000, 17.268], mean action: 1.413 [0.000, 3.000],  loss: 11.165891, mae: 51.794908, mean_q: 52.758058, mean_eps: 0.778753
  37010/150000: episode: 407, duration: 0.637s, episode steps:  95, steps per second: 149, episode reward: -121.051, mean reward: -1.274 [-100.000,  6.848], mean action: 1.474 [0.000, 3.000],  loss: 9.281371, mae: 52.232617, mean_q: 53.547235, mean_eps: 0.778228
  37138/150000: episode: 408, duration: 0.878s, episode steps: 128, steps per second: 146, episode reward: -133.027, mean reward: -1.039 [-100.000,  4.976], mean action: 1.609 [0.000, 3.000],  loss: 11.319606, mae: 51.737443, mean_q: 52.054635, mean_eps: 0.777559
  37251/150000: episode: 409, duration: 0.864s, episode steps: 113, steps per second: 131, episode reward: -78.936, mean reward: -0.699 [-100.000,  7.439], mean action: 1.496 [0.000, 3.000],  loss: 7.965022, mae: 52.322494, mean_q: 53.160350, mean_eps: 0.776836
  37326/150000: episode: 410, duration: 0.516s, episode steps:  75, steps per second: 145, episode reward: -29.104, mean reward: -0.388 [-100.000, 17.179], mean action: 1.613 [0.000, 3.000],  loss: 10.663976, mae: 51.695926, mean_q: 52.582212, mean_eps: 0.776272
  37433/150000: episode: 411, duration: 0.721s, episode steps: 107, steps per second: 148, episode reward: -61.393, mean reward: -0.574 [-100.000, 24.414], mean action: 1.626 [0.000, 3.000],  loss: 12.251472, mae: 52.223588, mean_q: 53.772571, mean_eps: 0.775726
  37526/150000: episode: 412, duration: 0.646s, episode steps:  93, steps per second: 144, episode reward: -101.086, mean reward: -1.087 [-100.000, 11.983], mean action: 1.419 [0.000, 3.000],  loss: 11.534707, mae: 52.214436, mean_q: 53.574625, mean_eps: 0.775126
  37655/150000: episode: 413, duration: 0.871s, episode steps: 129, steps per second: 148, episode reward: -229.863, mean reward: -1.782 [-100.000, 82.012], mean action: 1.550 [0.000, 3.000],  loss: 9.258339, mae: 52.152843, mean_q: 53.102068, mean_eps: 0.774460
  37781/150000: episode: 414, duration: 0.870s, episode steps: 126, steps per second: 145, episode reward: -71.415, mean reward: -0.567 [-100.000,  9.587], mean action: 1.532 [0.000, 3.000],  loss: 7.516718, mae: 52.476752, mean_q: 53.328682, mean_eps: 0.773695
  37861/150000: episode: 415, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -72.756, mean reward: -0.909 [-100.000,  9.152], mean action: 1.562 [0.000, 3.000],  loss: 10.520874, mae: 52.332916, mean_q: 53.655710, mean_eps: 0.773077
  37927/150000: episode: 416, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -82.641, mean reward: -1.252 [-100.000,  6.446], mean action: 1.364 [0.000, 3.000],  loss: 12.837218, mae: 51.911871, mean_q: 52.683486, mean_eps: 0.772639
  38042/150000: episode: 417, duration: 0.774s, episode steps: 115, steps per second: 149, episode reward: -109.318, mean reward: -0.951 [-100.000, 10.364], mean action: 1.513 [0.000, 3.000],  loss: 7.759357, mae: 52.513127, mean_q: 54.574052, mean_eps: 0.772096
  38153/150000: episode: 418, duration: 0.773s, episode steps: 111, steps per second: 144, episode reward: -173.534, mean reward: -1.563 [-100.000,  2.542], mean action: 1.766 [0.000, 3.000],  loss: 12.864788, mae: 52.951570, mean_q: 52.673193, mean_eps: 0.771418
  38230/150000: episode: 419, duration: 0.517s, episode steps:  77, steps per second: 149, episode reward: -73.549, mean reward: -0.955 [-100.000,  6.764], mean action: 1.429 [0.000, 3.000],  loss: 17.097523, mae: 52.618708, mean_q: 55.196085, mean_eps: 0.770854
  38317/150000: episode: 420, duration: 0.589s, episode steps:  87, steps per second: 148, episode reward: -62.848, mean reward: -0.722 [-100.000, 13.365], mean action: 1.724 [0.000, 3.000],  loss: 16.587411, mae: 53.260298, mean_q: 54.268791, mean_eps: 0.770362
  38407/150000: episode: 421, duration: 0.608s, episode steps:  90, steps per second: 148, episode reward: -94.838, mean reward: -1.054 [-100.000, 11.757], mean action: 1.656 [0.000, 3.000],  loss: 12.005080, mae: 52.615896, mean_q: 52.976873, mean_eps: 0.769831
  38476/150000: episode: 422, duration: 0.485s, episode steps:  69, steps per second: 142, episode reward: -29.420, mean reward: -0.426 [-100.000, 17.311], mean action: 1.768 [0.000, 3.000],  loss: 16.483364, mae: 52.563151, mean_q: 54.668033, mean_eps: 0.769354
  38578/150000: episode: 423, duration: 0.689s, episode steps: 102, steps per second: 148, episode reward: -90.109, mean reward: -0.883 [-100.000,  5.871], mean action: 1.480 [0.000, 3.000],  loss: 9.984141, mae: 53.108577, mean_q: 53.941428, mean_eps: 0.768841
  38667/150000: episode: 424, duration: 0.594s, episode steps:  89, steps per second: 150, episode reward: -102.314, mean reward: -1.150 [-100.000, 10.600], mean action: 1.719 [0.000, 3.000],  loss: 10.083432, mae: 52.613511, mean_q: 53.390918, mean_eps: 0.768268
  38746/150000: episode: 425, duration: 0.558s, episode steps:  79, steps per second: 142, episode reward: -29.522, mean reward: -0.374 [-100.000, 64.477], mean action: 1.519 [0.000, 3.000],  loss: 15.129194, mae: 52.235558, mean_q: 52.701067, mean_eps: 0.767764
  38823/150000: episode: 426, duration: 0.526s, episode steps:  77, steps per second: 146, episode reward: -58.909, mean reward: -0.765 [-100.000, 12.106], mean action: 1.519 [0.000, 3.000],  loss: 13.305346, mae: 53.297192, mean_q: 53.519567, mean_eps: 0.767296
  38894/150000: episode: 427, duration: 0.478s, episode steps:  71, steps per second: 149, episode reward: -149.444, mean reward: -2.105 [-100.000, 19.360], mean action: 1.563 [0.000, 3.000],  loss: 12.855100, mae: 52.909786, mean_q: 54.678949, mean_eps: 0.766852
  38989/150000: episode: 428, duration: 0.637s, episode steps:  95, steps per second: 149, episode reward: -109.695, mean reward: -1.155 [-100.000, 17.009], mean action: 1.516 [0.000, 3.000],  loss: 11.949717, mae: 52.645907, mean_q: 53.882931, mean_eps: 0.766354
  39068/150000: episode: 429, duration: 0.549s, episode steps:  79, steps per second: 144, episode reward: -53.249, mean reward: -0.674 [-100.000,  8.261], mean action: 1.544 [0.000, 3.000],  loss: 14.316315, mae: 52.601804, mean_q: 53.448917, mean_eps: 0.765832
  39147/150000: episode: 430, duration: 0.536s, episode steps:  79, steps per second: 147, episode reward: -73.862, mean reward: -0.935 [-100.000, 18.718], mean action: 1.544 [0.000, 3.000],  loss: 12.856747, mae: 54.051352, mean_q: 56.254817, mean_eps: 0.765358
  39260/150000: episode: 431, duration: 0.750s, episode steps: 113, steps per second: 151, episode reward: -66.961, mean reward: -0.593 [-100.000,  8.438], mean action: 1.372 [0.000, 3.000],  loss: 12.558543, mae: 53.302140, mean_q: 54.276005, mean_eps: 0.764782
  39356/150000: episode: 432, duration: 0.659s, episode steps:  96, steps per second: 146, episode reward: -114.775, mean reward: -1.196 [-100.000,  9.148], mean action: 1.646 [0.000, 3.000],  loss: 13.302656, mae: 53.437097, mean_q: 55.243251, mean_eps: 0.764155
  39481/150000: episode: 433, duration: 0.843s, episode steps: 125, steps per second: 148, episode reward: -52.031, mean reward: -0.416 [-100.000, 12.212], mean action: 1.576 [0.000, 3.000],  loss: 14.655003, mae: 54.072567, mean_q: 56.164814, mean_eps: 0.763492
  39553/150000: episode: 434, duration: 0.485s, episode steps:  72, steps per second: 148, episode reward: -39.668, mean reward: -0.551 [-100.000, 13.391], mean action: 1.431 [0.000, 3.000],  loss: 14.949493, mae: 53.845071, mean_q: 56.010972, mean_eps: 0.762901
  39664/150000: episode: 435, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward: -153.456, mean reward: -1.382 [-100.000,  7.545], mean action: 1.667 [0.000, 3.000],  loss: 9.468754, mae: 53.708592, mean_q: 55.941689, mean_eps: 0.762352
  39744/150000: episode: 436, duration: 0.570s, episode steps:  80, steps per second: 140, episode reward: -78.386, mean reward: -0.980 [-100.000,  9.453], mean action: 1.688 [0.000, 3.000],  loss: 11.905535, mae: 53.536981, mean_q: 55.717548, mean_eps: 0.761779
  39831/150000: episode: 437, duration: 0.587s, episode steps:  87, steps per second: 148, episode reward: -116.689, mean reward: -1.341 [-100.000, 10.404], mean action: 1.529 [0.000, 3.000],  loss: 15.050014, mae: 53.489830, mean_q: 54.485044, mean_eps: 0.761278
  39919/150000: episode: 438, duration: 0.586s, episode steps:  88, steps per second: 150, episode reward: -58.052, mean reward: -0.660 [-100.000, 18.002], mean action: 1.511 [0.000, 3.000],  loss: 17.482345, mae: 53.736987, mean_q: 56.129185, mean_eps: 0.760753
  40027/150000: episode: 439, duration: 0.749s, episode steps: 108, steps per second: 144, episode reward: -63.814, mean reward: -0.591 [-100.000, 12.530], mean action: 1.519 [0.000, 3.000],  loss: 15.568670, mae: 54.127015, mean_q: 55.609972, mean_eps: 0.760165
  40144/150000: episode: 440, duration: 0.789s, episode steps: 117, steps per second: 148, episode reward: -120.614, mean reward: -1.031 [-100.000, 20.203], mean action: 1.598 [0.000, 3.000],  loss: 15.688024, mae: 53.792902, mean_q: 56.177832, mean_eps: 0.759490
  40235/150000: episode: 441, duration: 0.612s, episode steps:  91, steps per second: 149, episode reward: -108.799, mean reward: -1.196 [-100.000, 10.270], mean action: 1.725 [0.000, 3.000],  loss: 10.431069, mae: 53.660013, mean_q: 56.577458, mean_eps: 0.758866
  40321/150000: episode: 442, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -85.491, mean reward: -0.994 [-100.000, 11.030], mean action: 1.326 [0.000, 3.000],  loss: 10.844569, mae: 53.863062, mean_q: 56.000399, mean_eps: 0.758335
  40423/150000: episode: 443, duration: 0.701s, episode steps: 102, steps per second: 146, episode reward: -104.935, mean reward: -1.029 [-100.000,  9.253], mean action: 1.480 [0.000, 3.000],  loss: 17.369902, mae: 54.604490, mean_q: 56.521728, mean_eps: 0.757771
  40500/150000: episode: 444, duration: 0.554s, episode steps:  77, steps per second: 139, episode reward: -79.409, mean reward: -1.031 [-100.000,  9.128], mean action: 1.805 [0.000, 3.000],  loss: 15.365146, mae: 54.234321, mean_q: 57.141046, mean_eps: 0.757234
  40577/150000: episode: 445, duration: 0.525s, episode steps:  77, steps per second: 147, episode reward: -5.682, mean reward: -0.074 [-100.000, 16.292], mean action: 1.636 [0.000, 3.000],  loss: 9.428811, mae: 53.943796, mean_q: 56.689887, mean_eps: 0.756772
  40657/150000: episode: 446, duration: 0.563s, episode steps:  80, steps per second: 142, episode reward: -66.896, mean reward: -0.836 [-100.000, 10.573], mean action: 1.538 [0.000, 3.000],  loss: 16.883680, mae: 53.956858, mean_q: 56.807854, mean_eps: 0.756301
  40761/150000: episode: 447, duration: 0.711s, episode steps: 104, steps per second: 146, episode reward: -78.512, mean reward: -0.755 [-100.000,  6.388], mean action: 1.635 [0.000, 3.000],  loss: 15.374113, mae: 54.206740, mean_q: 56.675194, mean_eps: 0.755749
  40863/150000: episode: 448, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: -67.902, mean reward: -0.666 [-100.000, 11.036], mean action: 1.480 [0.000, 3.000],  loss: 12.916333, mae: 54.911703, mean_q: 57.845366, mean_eps: 0.755131
  40972/150000: episode: 449, duration: 0.773s, episode steps: 109, steps per second: 141, episode reward: -109.509, mean reward: -1.005 [-100.000,  7.877], mean action: 1.569 [0.000, 3.000],  loss: 9.797617, mae: 54.610837, mean_q: 57.672353, mean_eps: 0.754498
  41067/150000: episode: 450, duration: 0.658s, episode steps:  95, steps per second: 144, episode reward: -59.203, mean reward: -0.623 [-100.000, 12.371], mean action: 1.600 [0.000, 3.000],  loss: 12.045380, mae: 55.329425, mean_q: 57.815401, mean_eps: 0.753886
  41145/150000: episode: 451, duration: 0.525s, episode steps:  78, steps per second: 149, episode reward: -23.566, mean reward: -0.302 [-100.000, 13.486], mean action: 1.577 [0.000, 3.000],  loss: 13.521073, mae: 55.039163, mean_q: 57.530649, mean_eps: 0.753367
  41242/150000: episode: 452, duration: 0.653s, episode steps:  97, steps per second: 148, episode reward: -76.126, mean reward: -0.785 [-100.000, 18.216], mean action: 1.423 [0.000, 3.000],  loss: 14.818993, mae: 55.523670, mean_q: 57.850729, mean_eps: 0.752842
  41338/150000: episode: 453, duration: 0.669s, episode steps:  96, steps per second: 144, episode reward: -91.502, mean reward: -0.953 [-100.000,  8.514], mean action: 1.542 [0.000, 3.000],  loss: 9.765638, mae: 55.108957, mean_q: 57.427628, mean_eps: 0.752263
  41455/150000: episode: 454, duration: 0.869s, episode steps: 117, steps per second: 135, episode reward: -52.087, mean reward: -0.445 [-100.000, 64.605], mean action: 1.650 [0.000, 3.000],  loss: 18.079589, mae: 55.475837, mean_q: 56.741119, mean_eps: 0.751624
  41551/150000: episode: 455, duration: 0.808s, episode steps:  96, steps per second: 119, episode reward: -55.509, mean reward: -0.578 [-100.000, 12.089], mean action: 1.729 [0.000, 3.000],  loss: 24.550293, mae: 55.602847, mean_q: 57.840390, mean_eps: 0.750985
  41632/150000: episode: 456, duration: 0.691s, episode steps:  81, steps per second: 117, episode reward: -103.726, mean reward: -1.281 [-100.000,  8.652], mean action: 1.605 [0.000, 3.000],  loss: 11.391035, mae: 55.578400, mean_q: 57.987151, mean_eps: 0.750454
  41707/150000: episode: 457, duration: 0.608s, episode steps:  75, steps per second: 123, episode reward: -118.387, mean reward: -1.578 [-100.000,  5.359], mean action: 1.347 [0.000, 3.000],  loss: 11.812989, mae: 56.018372, mean_q: 59.220732, mean_eps: 0.749986
  41791/150000: episode: 458, duration: 0.679s, episode steps:  84, steps per second: 124, episode reward: -65.435, mean reward: -0.779 [-100.000, 13.562], mean action: 1.595 [0.000, 3.000],  loss: 14.581969, mae: 55.910644, mean_q: 58.384019, mean_eps: 0.749509
  41906/150000: episode: 459, duration: 0.934s, episode steps: 115, steps per second: 123, episode reward: -101.663, mean reward: -0.884 [-100.000, 11.774], mean action: 1.600 [0.000, 3.000],  loss: 19.536157, mae: 56.025832, mean_q: 58.268071, mean_eps: 0.748912
  42014/150000: episode: 460, duration: 0.846s, episode steps: 108, steps per second: 128, episode reward: -63.389, mean reward: -0.587 [-100.000,  7.237], mean action: 1.667 [0.000, 3.000],  loss: 20.820527, mae: 55.415486, mean_q: 57.558383, mean_eps: 0.748243
  42088/150000: episode: 461, duration: 0.534s, episode steps:  74, steps per second: 139, episode reward: -84.034, mean reward: -1.136 [-100.000, 10.580], mean action: 1.622 [0.000, 3.000],  loss: 19.440108, mae: 56.997238, mean_q: 59.050849, mean_eps: 0.747697
  42159/150000: episode: 462, duration: 0.509s, episode steps:  71, steps per second: 140, episode reward: -96.381, mean reward: -1.357 [-100.000, 11.646], mean action: 1.437 [0.000, 3.000],  loss: 12.030400, mae: 56.279546, mean_q: 58.450641, mean_eps: 0.747262
  42256/150000: episode: 463, duration: 0.654s, episode steps:  97, steps per second: 148, episode reward: -111.319, mean reward: -1.148 [-100.000, 11.557], mean action: 1.423 [0.000, 3.000],  loss: 9.418174, mae: 56.444914, mean_q: 59.835552, mean_eps: 0.746758
  42362/150000: episode: 464, duration: 0.714s, episode steps: 106, steps per second: 148, episode reward: -191.908, mean reward: -1.810 [-100.000, 43.489], mean action: 1.708 [0.000, 3.000],  loss: 15.515658, mae: 56.654936, mean_q: 59.858100, mean_eps: 0.746149
  42430/150000: episode: 465, duration: 0.473s, episode steps:  68, steps per second: 144, episode reward: -57.762, mean reward: -0.849 [-100.000,  6.437], mean action: 1.735 [0.000, 3.000],  loss: 13.063247, mae: 56.389278, mean_q: 59.212031, mean_eps: 0.745627
  42511/150000: episode: 466, duration: 0.551s, episode steps:  81, steps per second: 147, episode reward: -80.186, mean reward: -0.990 [-100.000,  9.765], mean action: 1.593 [0.000, 3.000],  loss: 18.647563, mae: 56.777761, mean_q: 59.376113, mean_eps: 0.745180
  42578/150000: episode: 467, duration: 0.454s, episode steps:  67, steps per second: 147, episode reward: -83.931, mean reward: -1.253 [-100.000, 10.612], mean action: 1.582 [0.000, 3.000],  loss: 17.548342, mae: 56.857102, mean_q: 59.959478, mean_eps: 0.744736
  42650/150000: episode: 468, duration: 0.483s, episode steps:  72, steps per second: 149, episode reward: -99.267, mean reward: -1.379 [-100.000,  6.607], mean action: 1.542 [0.000, 3.000],  loss: 11.830326, mae: 57.527875, mean_q: 60.347984, mean_eps: 0.744319
  42730/150000: episode: 469, duration: 0.543s, episode steps:  80, steps per second: 147, episode reward: -28.755, mean reward: -0.359 [-100.000, 12.293], mean action: 1.738 [0.000, 3.000],  loss: 12.926890, mae: 56.630831, mean_q: 59.087824, mean_eps: 0.743863
  42861/150000: episode: 470, duration: 0.895s, episode steps: 131, steps per second: 146, episode reward: -85.395, mean reward: -0.652 [-100.000, 14.161], mean action: 1.519 [0.000, 3.000],  loss: 15.847001, mae: 56.348839, mean_q: 58.778365, mean_eps: 0.743230
  42925/150000: episode: 471, duration: 0.430s, episode steps:  64, steps per second: 149, episode reward: -92.100, mean reward: -1.439 [-100.000,  6.438], mean action: 1.688 [0.000, 3.000],  loss: 15.177919, mae: 57.127201, mean_q: 60.037328, mean_eps: 0.742645
  43000/150000: episode: 472, duration: 0.505s, episode steps:  75, steps per second: 148, episode reward: -69.647, mean reward: -0.929 [-100.000, 16.473], mean action: 1.613 [0.000, 3.000],  loss: 10.563650, mae: 57.819336, mean_q: 60.910541, mean_eps: 0.742228
  43093/150000: episode: 473, duration: 0.669s, episode steps:  93, steps per second: 139, episode reward: -74.499, mean reward: -0.801 [-100.000, 11.263], mean action: 1.602 [0.000, 3.000],  loss: 21.161632, mae: 57.450452, mean_q: 59.398819, mean_eps: 0.741724
  43185/150000: episode: 474, duration: 0.621s, episode steps:  92, steps per second: 148, episode reward: -45.617, mean reward: -0.496 [-100.000,  7.271], mean action: 1.609 [0.000, 3.000],  loss: 19.104218, mae: 57.083489, mean_q: 60.225814, mean_eps: 0.741169
  43266/150000: episode: 475, duration: 0.543s, episode steps:  81, steps per second: 149, episode reward: -38.465, mean reward: -0.475 [-100.000, 11.247], mean action: 1.642 [0.000, 3.000],  loss: 12.554480, mae: 57.217844, mean_q: 60.745793, mean_eps: 0.740650
  43332/150000: episode: 476, duration: 0.456s, episode steps:  66, steps per second: 145, episode reward: -53.884, mean reward: -0.816 [-100.000, 12.493], mean action: 1.773 [0.000, 3.000],  loss: 12.312084, mae: 57.452977, mean_q: 59.645757, mean_eps: 0.740209
  43439/150000: episode: 477, duration: 0.738s, episode steps: 107, steps per second: 145, episode reward: -33.268, mean reward: -0.311 [-100.000, 13.186], mean action: 1.495 [0.000, 3.000],  loss: 19.091064, mae: 58.186012, mean_q: 62.301656, mean_eps: 0.739690
  43521/150000: episode: 478, duration: 0.548s, episode steps:  82, steps per second: 150, episode reward: -119.321, mean reward: -1.455 [-100.000, 13.605], mean action: 1.707 [0.000, 3.000],  loss: 15.646829, mae: 56.961187, mean_q: 58.923215, mean_eps: 0.739123
  43630/150000: episode: 479, duration: 0.730s, episode steps: 109, steps per second: 149, episode reward: -82.958, mean reward: -0.761 [-100.000,  7.920], mean action: 1.624 [0.000, 3.000],  loss: 17.492553, mae: 57.716667, mean_q: 60.613062, mean_eps: 0.738550
  43732/150000: episode: 480, duration: 0.705s, episode steps: 102, steps per second: 145, episode reward: -66.935, mean reward: -0.656 [-100.000, 10.374], mean action: 1.539 [0.000, 3.000],  loss: 13.963707, mae: 57.374175, mean_q: 60.208898, mean_eps: 0.737917
  43851/150000: episode: 481, duration: 0.865s, episode steps: 119, steps per second: 138, episode reward: -228.024, mean reward: -1.916 [-100.000, 13.159], mean action: 1.571 [0.000, 3.000],  loss: 17.927873, mae: 57.932758, mean_q: 60.787429, mean_eps: 0.737254
  43911/150000: episode: 482, duration: 0.416s, episode steps:  60, steps per second: 144, episode reward: -72.859, mean reward: -1.214 [-100.000,  5.574], mean action: 1.700 [0.000, 3.000],  loss: 14.693132, mae: 58.874803, mean_q: 61.969833, mean_eps: 0.736717
  44056/150000: episode: 483, duration: 1.017s, episode steps: 145, steps per second: 143, episode reward: -15.186, mean reward: -0.105 [-100.000, 59.721], mean action: 1.683 [0.000, 3.000],  loss: 13.920863, mae: 58.220112, mean_q: 61.209422, mean_eps: 0.736102
  44159/150000: episode: 484, duration: 0.689s, episode steps: 103, steps per second: 150, episode reward: -106.873, mean reward: -1.038 [-100.000,  7.301], mean action: 1.641 [0.000, 3.000],  loss: 11.069194, mae: 58.643748, mean_q: 61.843562, mean_eps: 0.735358
  44262/150000: episode: 485, duration: 0.692s, episode steps: 103, steps per second: 149, episode reward: -113.018, mean reward: -1.097 [-100.000,  7.981], mean action: 1.573 [0.000, 3.000],  loss: 10.687053, mae: 57.916079, mean_q: 60.334386, mean_eps: 0.734740
  44320/150000: episode: 486, duration: 0.417s, episode steps:  58, steps per second: 139, episode reward: -55.402, mean reward: -0.955 [-100.000, 20.285], mean action: 1.655 [0.000, 3.000],  loss: 12.646664, mae: 58.558713, mean_q: 60.582661, mean_eps: 0.734257
  44399/150000: episode: 487, duration: 0.559s, episode steps:  79, steps per second: 141, episode reward: -61.225, mean reward: -0.775 [-100.000,  9.431], mean action: 1.532 [0.000, 3.000],  loss: 15.955090, mae: 58.773568, mean_q: 61.240992, mean_eps: 0.733846
  44486/150000: episode: 488, duration: 0.587s, episode steps:  87, steps per second: 148, episode reward: -84.836, mean reward: -0.975 [-100.000, 12.503], mean action: 1.437 [0.000, 3.000],  loss: 15.383795, mae: 58.759867, mean_q: 62.035163, mean_eps: 0.733348
  44586/150000: episode: 489, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -79.852, mean reward: -0.799 [-100.000, 11.257], mean action: 1.640 [0.000, 3.000],  loss: 12.602037, mae: 58.626771, mean_q: 61.069859, mean_eps: 0.732787
  44727/150000: episode: 490, duration: 0.976s, episode steps: 141, steps per second: 144, episode reward: -39.677, mean reward: -0.281 [-100.000, 14.374], mean action: 1.596 [0.000, 3.000],  loss: 13.492573, mae: 58.605880, mean_q: 61.346691, mean_eps: 0.732064
  44798/150000: episode: 491, duration: 0.501s, episode steps:  71, steps per second: 142, episode reward: -72.358, mean reward: -1.019 [-100.000, 12.739], mean action: 1.634 [0.000, 3.000],  loss: 9.174827, mae: 59.501436, mean_q: 61.480536, mean_eps: 0.731428
  44876/150000: episode: 492, duration: 0.531s, episode steps:  78, steps per second: 147, episode reward: -48.969, mean reward: -0.628 [-100.000, 14.435], mean action: 1.538 [0.000, 3.000],  loss: 20.201923, mae: 59.409314, mean_q: 61.202796, mean_eps: 0.730981
  44985/150000: episode: 493, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -121.423, mean reward: -1.114 [-100.000, 14.225], mean action: 1.606 [0.000, 3.000],  loss: 15.967237, mae: 58.682612, mean_q: 59.854143, mean_eps: 0.730420
  45089/150000: episode: 494, duration: 0.700s, episode steps: 104, steps per second: 149, episode reward: -62.519, mean reward: -0.601 [-100.000, 12.347], mean action: 1.548 [0.000, 3.000],  loss: 10.641093, mae: 58.920434, mean_q: 61.472921, mean_eps: 0.729781
  45200/150000: episode: 495, duration: 0.744s, episode steps: 111, steps per second: 149, episode reward: -67.353, mean reward: -0.607 [-100.000, 10.206], mean action: 1.387 [0.000, 3.000],  loss: 17.267748, mae: 59.515232, mean_q: 61.508459, mean_eps: 0.729136
  45306/150000: episode: 496, duration: 0.742s, episode steps: 106, steps per second: 143, episode reward: -95.516, mean reward: -0.901 [-100.000,  8.878], mean action: 1.651 [0.000, 3.000],  loss: 15.575951, mae: 59.189426, mean_q: 60.496074, mean_eps: 0.728485
  45398/150000: episode: 497, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: 19.555, mean reward:  0.213 [-100.000, 11.768], mean action: 1.609 [0.000, 3.000],  loss: 18.169605, mae: 60.028006, mean_q: 63.470607, mean_eps: 0.727891
  45489/150000: episode: 498, duration: 0.613s, episode steps:  91, steps per second: 148, episode reward: -64.339, mean reward: -0.707 [-100.000, 19.475], mean action: 1.495 [0.000, 3.000],  loss: 17.570779, mae: 59.432918, mean_q: 61.278486, mean_eps: 0.727342
  45594/150000: episode: 499, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -79.033, mean reward: -0.753 [-100.000,  6.847], mean action: 1.714 [0.000, 3.000],  loss: 13.328576, mae: 59.076277, mean_q: 59.596245, mean_eps: 0.726754
  45741/150000: episode: 500, duration: 0.995s, episode steps: 147, steps per second: 148, episode reward: 10.749, mean reward:  0.073 [-100.000, 62.514], mean action: 1.592 [0.000, 3.000],  loss: 12.507982, mae: 59.431424, mean_q: 61.568460, mean_eps: 0.725998
  45836/150000: episode: 501, duration: 0.641s, episode steps:  95, steps per second: 148, episode reward: -92.018, mean reward: -0.969 [-100.000,  8.601], mean action: 1.621 [0.000, 3.000],  loss: 16.121410, mae: 59.930013, mean_q: 61.074519, mean_eps: 0.725272
  45933/150000: episode: 502, duration: 0.697s, episode steps:  97, steps per second: 139, episode reward: -57.886, mean reward: -0.597 [-100.000, 11.729], mean action: 1.588 [0.000, 3.000],  loss: 12.633683, mae: 59.234201, mean_q: 61.248278, mean_eps: 0.724696
  46033/150000: episode: 503, duration: 0.712s, episode steps: 100, steps per second: 140, episode reward: -126.268, mean reward: -1.263 [-100.000,  9.921], mean action: 1.790 [0.000, 3.000],  loss: 17.297411, mae: 59.423863, mean_q: 61.280698, mean_eps: 0.724105
  46183/150000: episode: 504, duration: 1.051s, episode steps: 150, steps per second: 143, episode reward: -70.206, mean reward: -0.468 [-100.000, 54.981], mean action: 1.540 [0.000, 3.000],  loss: 11.269560, mae: 59.233888, mean_q: 60.711579, mean_eps: 0.723355
  46296/150000: episode: 505, duration: 0.784s, episode steps: 113, steps per second: 144, episode reward: -120.377, mean reward: -1.065 [-100.000, 22.319], mean action: 1.451 [0.000, 3.000],  loss: 13.045144, mae: 59.414490, mean_q: 61.167113, mean_eps: 0.722566
  46377/150000: episode: 506, duration: 0.557s, episode steps:  81, steps per second: 145, episode reward: -45.926, mean reward: -0.567 [-100.000, 16.794], mean action: 1.741 [0.000, 3.000],  loss: 17.499616, mae: 58.960525, mean_q: 60.854251, mean_eps: 0.721984
  46468/150000: episode: 507, duration: 0.640s, episode steps:  91, steps per second: 142, episode reward: -54.678, mean reward: -0.601 [-100.000, 11.926], mean action: 1.330 [0.000, 3.000],  loss: 19.171978, mae: 59.458762, mean_q: 60.425505, mean_eps: 0.721468
  46598/150000: episode: 508, duration: 0.897s, episode steps: 130, steps per second: 145, episode reward: -252.608, mean reward: -1.943 [-100.000, 20.733], mean action: 1.638 [0.000, 3.000],  loss: 10.791691, mae: 59.096868, mean_q: 60.522460, mean_eps: 0.720805
  46708/150000: episode: 509, duration: 0.736s, episode steps: 110, steps per second: 149, episode reward: -83.210, mean reward: -0.756 [-100.000, 13.999], mean action: 1.491 [0.000, 3.000],  loss: 11.817568, mae: 58.818302, mean_q: 59.810366, mean_eps: 0.720085
  46784/150000: episode: 510, duration: 0.560s, episode steps:  76, steps per second: 136, episode reward: -45.443, mean reward: -0.598 [-100.000, 11.076], mean action: 1.724 [0.000, 3.000],  loss: 15.617314, mae: 58.834793, mean_q: 60.000254, mean_eps: 0.719527
  46878/150000: episode: 511, duration: 0.664s, episode steps:  94, steps per second: 141, episode reward: -94.760, mean reward: -1.008 [-100.000, 13.899], mean action: 1.532 [0.000, 3.000],  loss: 13.553341, mae: 59.812571, mean_q: 61.675646, mean_eps: 0.719017
  46981/150000: episode: 512, duration: 0.688s, episode steps: 103, steps per second: 150, episode reward: -70.054, mean reward: -0.680 [-100.000, 13.172], mean action: 1.437 [0.000, 3.000],  loss: 10.177203, mae: 59.013359, mean_q: 60.646241, mean_eps: 0.718426
  47064/150000: episode: 513, duration: 0.557s, episode steps:  83, steps per second: 149, episode reward: -45.207, mean reward: -0.545 [-100.000, 14.877], mean action: 1.711 [0.000, 3.000],  loss: 12.129146, mae: 59.743755, mean_q: 62.753127, mean_eps: 0.717868
  47163/150000: episode: 514, duration: 0.696s, episode steps:  99, steps per second: 142, episode reward: -96.746, mean reward: -0.977 [-100.000,  9.426], mean action: 1.667 [0.000, 3.000],  loss: 13.045880, mae: 59.218531, mean_q: 60.756480, mean_eps: 0.717322
  47264/150000: episode: 515, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -81.037, mean reward: -0.802 [-100.000, 12.265], mean action: 1.505 [0.000, 3.000],  loss: 12.225973, mae: 58.397761, mean_q: 59.604005, mean_eps: 0.716722
  47361/150000: episode: 516, duration: 0.655s, episode steps:  97, steps per second: 148, episode reward: -126.606, mean reward: -1.305 [-100.000,  8.178], mean action: 1.753 [0.000, 3.000],  loss: 8.570879, mae: 59.277919, mean_q: 61.365351, mean_eps: 0.716128
  47449/150000: episode: 517, duration: 0.625s, episode steps:  88, steps per second: 141, episode reward: -86.506, mean reward: -0.983 [-100.000,  9.879], mean action: 1.625 [0.000, 3.000],  loss: 7.440815, mae: 58.450254, mean_q: 59.289776, mean_eps: 0.715573
  47554/150000: episode: 518, duration: 0.710s, episode steps: 105, steps per second: 148, episode reward: -43.636, mean reward: -0.416 [-100.000, 14.212], mean action: 1.638 [0.000, 3.000],  loss: 11.011894, mae: 59.099802, mean_q: 60.722143, mean_eps: 0.714994
  47622/150000: episode: 519, duration: 0.457s, episode steps:  68, steps per second: 149, episode reward: -65.742, mean reward: -0.967 [-100.000,  7.287], mean action: 1.574 [0.000, 3.000],  loss: 10.409002, mae: 59.440198, mean_q: 61.314556, mean_eps: 0.714475
  47698/150000: episode: 520, duration: 0.513s, episode steps:  76, steps per second: 148, episode reward: -88.644, mean reward: -1.166 [-100.000,  7.026], mean action: 1.605 [0.000, 3.000],  loss: 25.439830, mae: 59.521348, mean_q: 59.665876, mean_eps: 0.714043
  47811/150000: episode: 521, duration: 0.790s, episode steps: 113, steps per second: 143, episode reward: -162.659, mean reward: -1.439 [-100.000,  8.037], mean action: 1.434 [0.000, 3.000],  loss: 16.137339, mae: 59.572212, mean_q: 60.028562, mean_eps: 0.713476
  47893/150000: episode: 522, duration: 0.550s, episode steps:  82, steps per second: 149, episode reward: -9.448, mean reward: -0.115 [-100.000,  6.616], mean action: 1.622 [0.000, 3.000],  loss: 8.858875, mae: 59.633621, mean_q: 60.609434, mean_eps: 0.712891
  47963/150000: episode: 523, duration: 0.472s, episode steps:  70, steps per second: 148, episode reward: -67.021, mean reward: -0.957 [-100.000,  6.512], mean action: 1.514 [0.000, 3.000],  loss: 9.754597, mae: 59.735806, mean_q: 61.856093, mean_eps: 0.712435
  48068/150000: episode: 524, duration: 0.716s, episode steps: 105, steps per second: 147, episode reward: -100.340, mean reward: -0.956 [-100.000, 14.368], mean action: 1.610 [0.000, 3.000],  loss: 10.470457, mae: 59.575318, mean_q: 62.382135, mean_eps: 0.711910
  48203/150000: episode: 525, duration: 0.912s, episode steps: 135, steps per second: 148, episode reward: -100.250, mean reward: -0.743 [-100.000, 10.518], mean action: 1.511 [0.000, 3.000],  loss: 9.564719, mae: 59.180192, mean_q: 60.402010, mean_eps: 0.711190
  48281/150000: episode: 526, duration: 0.525s, episode steps:  78, steps per second: 149, episode reward: -97.627, mean reward: -1.252 [-100.000, 17.261], mean action: 1.654 [0.000, 3.000],  loss: 12.128658, mae: 59.682495, mean_q: 61.219192, mean_eps: 0.710551
  48384/150000: episode: 527, duration: 0.703s, episode steps: 103, steps per second: 146, episode reward: -59.593, mean reward: -0.579 [-100.000, 13.992], mean action: 1.466 [0.000, 3.000],  loss: 8.385842, mae: 60.082070, mean_q: 61.562552, mean_eps: 0.710008
  48498/150000: episode: 528, duration: 0.773s, episode steps: 114, steps per second: 147, episode reward: -166.644, mean reward: -1.462 [-100.000,  4.047], mean action: 1.526 [0.000, 3.000],  loss: 13.279560, mae: 59.704712, mean_q: 60.000519, mean_eps: 0.709357
  48602/150000: episode: 529, duration: 0.698s, episode steps: 104, steps per second: 149, episode reward: -77.131, mean reward: -0.742 [-100.000,  8.226], mean action: 1.558 [0.000, 3.000],  loss: 11.728623, mae: 59.750189, mean_q: 60.319969, mean_eps: 0.708703
  48685/150000: episode: 530, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -86.719, mean reward: -1.045 [-100.000, 13.119], mean action: 1.410 [0.000, 3.000],  loss: 14.957905, mae: 59.937155, mean_q: 61.151804, mean_eps: 0.708142
  48762/150000: episode: 531, duration: 0.528s, episode steps:  77, steps per second: 146, episode reward: -82.885, mean reward: -1.076 [-100.000, 11.512], mean action: 1.636 [0.000, 3.000],  loss: 11.587861, mae: 60.346906, mean_q: 62.294321, mean_eps: 0.707662
  48850/150000: episode: 532, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -63.345, mean reward: -0.720 [-100.000, 10.299], mean action: 1.534 [0.000, 3.000],  loss: 14.004025, mae: 60.574309, mean_q: 61.895585, mean_eps: 0.707167
  48947/150000: episode: 533, duration: 0.657s, episode steps:  97, steps per second: 148, episode reward: -108.184, mean reward: -1.115 [-100.000,  8.252], mean action: 1.619 [0.000, 3.000],  loss: 9.891235, mae: 60.756376, mean_q: 61.189973, mean_eps: 0.706612
  49037/150000: episode: 534, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -53.400, mean reward: -0.593 [-100.000, 12.986], mean action: 1.767 [0.000, 3.000],  loss: 11.863480, mae: 60.715863, mean_q: 60.419532, mean_eps: 0.706051
  49110/150000: episode: 535, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -13.315, mean reward: -0.182 [-100.000, 11.400], mean action: 1.699 [0.000, 3.000],  loss: 12.280586, mae: 60.923039, mean_q: 61.677341, mean_eps: 0.705562
  49193/150000: episode: 536, duration: 0.558s, episode steps:  83, steps per second: 149, episode reward: -103.033, mean reward: -1.241 [-100.000,  6.212], mean action: 1.325 [0.000, 3.000],  loss: 13.156679, mae: 61.148082, mean_q: 63.176209, mean_eps: 0.705094
  49293/150000: episode: 537, duration: 0.678s, episode steps: 100, steps per second: 147, episode reward: -117.581, mean reward: -1.176 [-100.000,  5.890], mean action: 1.310 [0.000, 3.000],  loss: 9.605232, mae: 60.962417, mean_q: 62.530937, mean_eps: 0.704545
  49359/150000: episode: 538, duration: 0.472s, episode steps:  66, steps per second: 140, episode reward: -77.042, mean reward: -1.167 [-100.000,  8.058], mean action: 1.409 [0.000, 3.000],  loss: 10.104074, mae: 60.940531, mean_q: 61.348643, mean_eps: 0.704047
  49439/150000: episode: 539, duration: 0.543s, episode steps:  80, steps per second: 147, episode reward: -30.026, mean reward: -0.375 [-100.000, 18.622], mean action: 1.650 [0.000, 3.000],  loss: 10.891746, mae: 60.625171, mean_q: 61.512230, mean_eps: 0.703609
  49553/150000: episode: 540, duration: 0.767s, episode steps: 114, steps per second: 149, episode reward: -72.039, mean reward: -0.632 [-100.000,  8.296], mean action: 1.763 [0.000, 3.000],  loss: 11.732757, mae: 60.497699, mean_q: 60.100201, mean_eps: 0.703027
  49662/150000: episode: 541, duration: 0.749s, episode steps: 109, steps per second: 145, episode reward: -68.012, mean reward: -0.624 [-100.000,  6.624], mean action: 1.477 [0.000, 3.000],  loss: 17.455979, mae: 60.498671, mean_q: 60.521018, mean_eps: 0.702358
  49727/150000: episode: 542, duration: 0.462s, episode steps:  65, steps per second: 141, episode reward: -30.813, mean reward: -0.474 [-100.000, 10.713], mean action: 1.569 [0.000, 3.000],  loss: 12.238337, mae: 60.727824, mean_q: 60.834086, mean_eps: 0.701836
  49843/150000: episode: 543, duration: 0.786s, episode steps: 116, steps per second: 148, episode reward: -109.574, mean reward: -0.945 [-100.000, 10.747], mean action: 1.802 [0.000, 3.000],  loss: 8.240965, mae: 60.370268, mean_q: 61.553017, mean_eps: 0.701293
  49931/150000: episode: 544, duration: 0.598s, episode steps:  88, steps per second: 147, episode reward: -75.330, mean reward: -0.856 [-100.000,  6.458], mean action: 1.648 [0.000, 3.000],  loss: 9.836025, mae: 60.275764, mean_q: 61.362806, mean_eps: 0.700681
  50003/150000: episode: 545, duration: 0.511s, episode steps:  72, steps per second: 141, episode reward: -31.056, mean reward: -0.431 [-100.000, 18.341], mean action: 1.597 [0.000, 3.000],  loss: 12.342258, mae: 61.092356, mean_q: 62.855889, mean_eps: 0.700201
  50102/150000: episode: 546, duration: 0.662s, episode steps:  99, steps per second: 150, episode reward: -86.338, mean reward: -0.872 [-100.000,  8.696], mean action: 1.626 [0.000, 3.000],  loss: 11.775113, mae: 60.593695, mean_q: 62.005596, mean_eps: 0.699688
  50181/150000: episode: 547, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -50.080, mean reward: -0.634 [-100.000, 13.012], mean action: 1.709 [0.000, 3.000],  loss: 9.344804, mae: 60.706522, mean_q: 61.771767, mean_eps: 0.699154
  50252/150000: episode: 548, duration: 0.481s, episode steps:  71, steps per second: 148, episode reward: -37.269, mean reward: -0.525 [-100.000,  9.391], mean action: 1.592 [0.000, 3.000],  loss: 11.311740, mae: 60.722798, mean_q: 63.475256, mean_eps: 0.698704
  50354/150000: episode: 549, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -135.207, mean reward: -1.326 [-100.000,  6.595], mean action: 1.647 [0.000, 3.000],  loss: 13.876604, mae: 60.932683, mean_q: 61.753016, mean_eps: 0.698185
  50449/150000: episode: 550, duration: 0.665s, episode steps:  95, steps per second: 143, episode reward: -47.089, mean reward: -0.496 [-100.000,  7.214], mean action: 1.558 [0.000, 3.000],  loss: 10.892606, mae: 61.238193, mean_q: 63.677419, mean_eps: 0.697594
  50541/150000: episode: 551, duration: 0.632s, episode steps:  92, steps per second: 146, episode reward: -118.498, mean reward: -1.288 [-100.000,  4.805], mean action: 1.815 [0.000, 3.000],  loss: 10.578087, mae: 60.710477, mean_q: 62.275111, mean_eps: 0.697033
  50643/150000: episode: 552, duration: 0.736s, episode steps: 102, steps per second: 139, episode reward: -101.815, mean reward: -0.998 [-100.000, 17.472], mean action: 1.520 [0.000, 3.000],  loss: 12.088842, mae: 60.483216, mean_q: 62.241703, mean_eps: 0.696451
  50763/150000: episode: 553, duration: 0.810s, episode steps: 120, steps per second: 148, episode reward: -47.084, mean reward: -0.392 [-100.000, 17.165], mean action: 1.475 [0.000, 3.000],  loss: 10.560584, mae: 60.760541, mean_q: 61.636051, mean_eps: 0.695785
  50906/150000: episode: 554, duration: 1.072s, episode steps: 143, steps per second: 133, episode reward: 19.968, mean reward:  0.140 [-100.000, 79.514], mean action: 1.741 [0.000, 3.000],  loss: 8.088588, mae: 60.708134, mean_q: 62.923141, mean_eps: 0.694996
  50999/150000: episode: 555, duration: 0.766s, episode steps:  93, steps per second: 121, episode reward: -129.324, mean reward: -1.391 [-100.000,  8.745], mean action: 1.817 [0.000, 3.000],  loss: 9.952579, mae: 60.778313, mean_q: 62.077896, mean_eps: 0.694288
  51079/150000: episode: 556, duration: 0.661s, episode steps:  80, steps per second: 121, episode reward: -6.369, mean reward: -0.080 [-100.000, 16.101], mean action: 1.562 [0.000, 3.000],  loss: 14.735247, mae: 61.384585, mean_q: 63.036802, mean_eps: 0.693769
  51189/150000: episode: 557, duration: 1.086s, episode steps: 110, steps per second: 101, episode reward: -36.339, mean reward: -0.330 [-100.000, 10.944], mean action: 1.727 [0.000, 3.000],  loss: 13.184114, mae: 60.753564, mean_q: 61.978482, mean_eps: 0.693199
  51278/150000: episode: 558, duration: 0.698s, episode steps:  89, steps per second: 128, episode reward: -101.097, mean reward: -1.136 [-100.000,  8.736], mean action: 1.584 [0.000, 3.000],  loss: 15.061222, mae: 62.167141, mean_q: 63.549154, mean_eps: 0.692602
  51373/150000: episode: 559, duration: 0.720s, episode steps:  95, steps per second: 132, episode reward: -66.033, mean reward: -0.695 [-100.000, 11.796], mean action: 1.579 [0.000, 3.000],  loss: 7.998930, mae: 60.071690, mean_q: 60.185184, mean_eps: 0.692050
  51438/150000: episode: 560, duration: 0.503s, episode steps:  65, steps per second: 129, episode reward: -79.403, mean reward: -1.222 [-100.000,  9.152], mean action: 1.677 [0.000, 3.000],  loss: 12.352936, mae: 60.356806, mean_q: 60.124397, mean_eps: 0.691570
  51596/150000: episode: 561, duration: 1.100s, episode steps: 158, steps per second: 144, episode reward: -201.885, mean reward: -1.278 [-100.000,  5.566], mean action: 1.614 [0.000, 3.000],  loss: 11.622540, mae: 61.027699, mean_q: 62.989795, mean_eps: 0.690901
  51698/150000: episode: 562, duration: 0.691s, episode steps: 102, steps per second: 148, episode reward: -90.280, mean reward: -0.885 [-100.000, 12.137], mean action: 1.706 [0.000, 3.000],  loss: 8.160652, mae: 61.386732, mean_q: 62.735690, mean_eps: 0.690121
  51839/150000: episode: 563, duration: 0.961s, episode steps: 141, steps per second: 147, episode reward: -267.010, mean reward: -1.894 [-100.000,  6.234], mean action: 1.560 [0.000, 3.000],  loss: 9.795349, mae: 60.882587, mean_q: 62.496491, mean_eps: 0.689392
  51931/150000: episode: 564, duration: 0.618s, episode steps:  92, steps per second: 149, episode reward: -62.627, mean reward: -0.681 [-100.000, 10.600], mean action: 1.793 [0.000, 3.000],  loss: 9.870751, mae: 61.711292, mean_q: 62.402301, mean_eps: 0.688693
  52020/150000: episode: 565, duration: 0.613s, episode steps:  89, steps per second: 145, episode reward: -105.699, mean reward: -1.188 [-100.000, 25.917], mean action: 1.573 [0.000, 3.000],  loss: 11.594274, mae: 61.518208, mean_q: 63.546693, mean_eps: 0.688150
  52153/150000: episode: 566, duration: 0.905s, episode steps: 133, steps per second: 147, episode reward: -128.351, mean reward: -0.965 [-100.000,  9.834], mean action: 1.496 [0.000, 3.000],  loss: 11.315049, mae: 61.467906, mean_q: 62.439178, mean_eps: 0.687484
  52264/150000: episode: 567, duration: 0.740s, episode steps: 111, steps per second: 150, episode reward: -72.296, mean reward: -0.651 [-100.000, 10.550], mean action: 1.775 [0.000, 3.000],  loss: 8.474077, mae: 61.651960, mean_q: 61.969645, mean_eps: 0.686752
  52380/150000: episode: 568, duration: 0.800s, episode steps: 116, steps per second: 145, episode reward: -95.017, mean reward: -0.819 [-100.000, 16.590], mean action: 1.560 [0.000, 3.000],  loss: 11.009934, mae: 61.276120, mean_q: 61.874015, mean_eps: 0.686071
  52449/150000: episode: 569, duration: 0.462s, episode steps:  69, steps per second: 149, episode reward: -88.171, mean reward: -1.278 [-100.000,  5.808], mean action: 1.333 [0.000, 3.000],  loss: 12.432888, mae: 61.328946, mean_q: 61.296295, mean_eps: 0.685516
  52558/150000: episode: 570, duration: 0.727s, episode steps: 109, steps per second: 150, episode reward: -137.888, mean reward: -1.265 [-100.000,  7.209], mean action: 1.569 [0.000, 3.000],  loss: 8.818662, mae: 61.540257, mean_q: 61.611932, mean_eps: 0.684982
  52661/150000: episode: 571, duration: 0.696s, episode steps: 103, steps per second: 148, episode reward: -169.385, mean reward: -1.645 [-100.000, 57.495], mean action: 1.689 [0.000, 3.000],  loss: 11.967499, mae: 60.793536, mean_q: 62.836458, mean_eps: 0.684346
  52761/150000: episode: 572, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -166.619, mean reward: -1.666 [-100.000, 23.608], mean action: 1.780 [0.000, 3.000],  loss: 10.795051, mae: 61.469778, mean_q: 62.924270, mean_eps: 0.683737
  52841/150000: episode: 573, duration: 0.537s, episode steps:  80, steps per second: 149, episode reward: -59.270, mean reward: -0.741 [-100.000, 13.033], mean action: 1.675 [0.000, 3.000],  loss: 11.379486, mae: 62.003661, mean_q: 62.087379, mean_eps: 0.683197
  52934/150000: episode: 574, duration: 0.622s, episode steps:  93, steps per second: 150, episode reward: -48.978, mean reward: -0.527 [-100.000, 11.034], mean action: 1.699 [0.000, 3.000],  loss: 11.697089, mae: 62.157977, mean_q: 63.760873, mean_eps: 0.682678
  53016/150000: episode: 575, duration: 0.584s, episode steps:  82, steps per second: 140, episode reward: -70.288, mean reward: -0.857 [-100.000,  8.326], mean action: 1.512 [0.000, 3.000],  loss: 12.259371, mae: 61.461062, mean_q: 61.832175, mean_eps: 0.682153
  53122/150000: episode: 576, duration: 0.711s, episode steps: 106, steps per second: 149, episode reward: -61.984, mean reward: -0.585 [-100.000,  7.594], mean action: 1.708 [0.000, 3.000],  loss: 11.630526, mae: 62.213424, mean_q: 63.571665, mean_eps: 0.681589
  53248/150000: episode: 577, duration: 0.848s, episode steps: 126, steps per second: 149, episode reward: -87.751, mean reward: -0.696 [-100.000, 11.132], mean action: 1.690 [0.000, 3.000],  loss: 17.272623, mae: 61.850000, mean_q: 63.316619, mean_eps: 0.680893
  53363/150000: episode: 578, duration: 0.805s, episode steps: 115, steps per second: 143, episode reward: -126.459, mean reward: -1.100 [-100.000, 17.181], mean action: 1.661 [0.000, 3.000],  loss: 9.973300, mae: 62.275234, mean_q: 65.640446, mean_eps: 0.680170
  53473/150000: episode: 579, duration: 0.742s, episode steps: 110, steps per second: 148, episode reward: -47.678, mean reward: -0.433 [-100.000,  9.092], mean action: 1.809 [0.000, 3.000],  loss: 10.379824, mae: 61.757248, mean_q: 62.416206, mean_eps: 0.679495
  53552/150000: episode: 580, duration: 0.530s, episode steps:  79, steps per second: 149, episode reward: -98.815, mean reward: -1.251 [-100.000,  8.551], mean action: 1.646 [0.000, 3.000],  loss: 10.372956, mae: 62.073705, mean_q: 65.030708, mean_eps: 0.678928
  53644/150000: episode: 581, duration: 0.645s, episode steps:  92, steps per second: 143, episode reward: -151.422, mean reward: -1.646 [-100.000, 18.452], mean action: 1.511 [0.000, 3.000],  loss: 6.893386, mae: 60.822305, mean_q: 62.821995, mean_eps: 0.678415
  53784/150000: episode: 582, duration: 0.941s, episode steps: 140, steps per second: 149, episode reward: -40.653, mean reward: -0.290 [-100.000, 12.798], mean action: 1.679 [0.000, 3.000],  loss: 6.234195, mae: 61.762922, mean_q: 63.615404, mean_eps: 0.677719
  53890/150000: episode: 583, duration: 0.710s, episode steps: 106, steps per second: 149, episode reward: -81.433, mean reward: -0.768 [-100.000, 10.001], mean action: 1.689 [0.000, 3.000],  loss: 13.501598, mae: 62.144008, mean_q: 63.865415, mean_eps: 0.676981
  53970/150000: episode: 584, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -124.166, mean reward: -1.552 [-100.000,  6.391], mean action: 1.550 [0.000, 3.000],  loss: 8.355146, mae: 61.451277, mean_q: 62.257831, mean_eps: 0.676423
  54077/150000: episode: 585, duration: 0.724s, episode steps: 107, steps per second: 148, episode reward: -105.514, mean reward: -0.986 [-100.000, 11.155], mean action: 1.682 [0.000, 3.000],  loss: 9.964784, mae: 62.159990, mean_q: 64.531774, mean_eps: 0.675862
  54181/150000: episode: 586, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -31.369, mean reward: -0.302 [-100.000,  7.744], mean action: 1.663 [0.000, 3.000],  loss: 12.782733, mae: 62.023394, mean_q: 63.377119, mean_eps: 0.675229
  54280/150000: episode: 587, duration: 0.690s, episode steps:  99, steps per second: 144, episode reward: -83.028, mean reward: -0.839 [-100.000, 12.157], mean action: 1.556 [0.000, 3.000],  loss: 9.981587, mae: 62.300557, mean_q: 63.607647, mean_eps: 0.674620
  54403/150000: episode: 588, duration: 0.834s, episode steps: 123, steps per second: 148, episode reward: -79.993, mean reward: -0.650 [-100.000,  6.710], mean action: 1.626 [0.000, 3.000],  loss: 9.935460, mae: 61.639612, mean_q: 62.988566, mean_eps: 0.673954
  54479/150000: episode: 589, duration: 0.514s, episode steps:  76, steps per second: 148, episode reward: -97.348, mean reward: -1.281 [-100.000, 11.853], mean action: 1.605 [0.000, 3.000],  loss: 16.807174, mae: 62.365531, mean_q: 63.902126, mean_eps: 0.673357
  54556/150000: episode: 590, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -60.817, mean reward: -0.790 [-100.000,  8.258], mean action: 1.649 [0.000, 3.000],  loss: 11.666117, mae: 62.406048, mean_q: 65.013705, mean_eps: 0.672898
  54641/150000: episode: 591, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -48.682, mean reward: -0.573 [-100.000, 13.354], mean action: 1.600 [0.000, 3.000],  loss: 11.134784, mae: 61.829613, mean_q: 64.730011, mean_eps: 0.672412
  54718/150000: episode: 592, duration: 0.526s, episode steps:  77, steps per second: 146, episode reward: -83.968, mean reward: -1.090 [-100.000,  9.956], mean action: 1.429 [0.000, 3.000],  loss: 14.181153, mae: 62.314000, mean_q: 64.878570, mean_eps: 0.671926
  54830/150000: episode: 593, duration: 0.771s, episode steps: 112, steps per second: 145, episode reward: -108.881, mean reward: -0.972 [-100.000, 11.216], mean action: 1.634 [0.000, 3.000],  loss: 8.791942, mae: 62.647414, mean_q: 64.104877, mean_eps: 0.671359
  54919/150000: episode: 594, duration: 0.647s, episode steps:  89, steps per second: 138, episode reward: 29.033, mean reward:  0.326 [-100.000, 83.625], mean action: 1.742 [0.000, 3.000],  loss: 10.306484, mae: 61.790240, mean_q: 62.290965, mean_eps: 0.670756
  55034/150000: episode: 595, duration: 0.779s, episode steps: 115, steps per second: 148, episode reward: -82.290, mean reward: -0.716 [-100.000, 16.774], mean action: 1.478 [0.000, 3.000],  loss: 8.174795, mae: 62.155681, mean_q: 63.836965, mean_eps: 0.670144
  55128/150000: episode: 596, duration: 0.627s, episode steps:  94, steps per second: 150, episode reward: -42.371, mean reward: -0.451 [-100.000, 13.108], mean action: 1.532 [0.000, 3.000],  loss: 13.057911, mae: 62.708237, mean_q: 65.493251, mean_eps: 0.669517
  55229/150000: episode: 597, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -203.048, mean reward: -2.010 [-100.000, 71.895], mean action: 1.663 [0.000, 3.000],  loss: 6.333234, mae: 62.706026, mean_q: 65.349149, mean_eps: 0.668932
  55298/150000: episode: 598, duration: 0.471s, episode steps:  69, steps per second: 146, episode reward: -9.931, mean reward: -0.144 [-100.000, 16.285], mean action: 1.638 [0.000, 3.000],  loss: 6.909350, mae: 62.357064, mean_q: 64.433469, mean_eps: 0.668422
  55404/150000: episode: 599, duration: 0.707s, episode steps: 106, steps per second: 150, episode reward: 10.742, mean reward:  0.101 [-100.000, 11.771], mean action: 1.802 [0.000, 3.000],  loss: 9.254015, mae: 63.181455, mean_q: 66.017241, mean_eps: 0.667897
  55508/150000: episode: 600, duration: 0.734s, episode steps: 104, steps per second: 142, episode reward: -104.379, mean reward: -1.004 [-100.000, 18.248], mean action: 1.683 [0.000, 3.000],  loss: 8.705435, mae: 62.378518, mean_q: 65.682887, mean_eps: 0.667267
  55987/150000: episode: 601, duration: 3.458s, episode steps: 479, steps per second: 139, episode reward: -249.516, mean reward: -0.521 [-100.000, 29.392], mean action: 1.591 [0.000, 3.000],  loss: 12.037480, mae: 62.422013, mean_q: 64.318936, mean_eps: 0.665518
  56091/150000: episode: 602, duration: 1.224s, episode steps: 104, steps per second:  85, episode reward: -62.310, mean reward: -0.599 [-100.000, 10.661], mean action: 1.731 [0.000, 3.000],  loss: 9.501879, mae: 62.980582, mean_q: 66.054847, mean_eps: 0.663769
  56159/150000: episode: 603, duration: 0.513s, episode steps:  68, steps per second: 133, episode reward: -24.136, mean reward: -0.355 [-100.000, 21.259], mean action: 1.471 [0.000, 3.000],  loss: 8.873091, mae: 62.515326, mean_q: 63.378431, mean_eps: 0.663253
  56275/150000: episode: 604, duration: 0.811s, episode steps: 116, steps per second: 143, episode reward: -89.651, mean reward: -0.773 [-100.000, 23.423], mean action: 1.603 [0.000, 3.000],  loss: 9.514289, mae: 62.075744, mean_q: 63.867718, mean_eps: 0.662701
  56375/150000: episode: 605, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -39.891, mean reward: -0.399 [-100.000, 13.837], mean action: 1.640 [0.000, 3.000],  loss: 7.950531, mae: 61.824305, mean_q: 63.541753, mean_eps: 0.662053
  56497/150000: episode: 606, duration: 0.823s, episode steps: 122, steps per second: 148, episode reward: -14.478, mean reward: -0.119 [-100.000, 16.323], mean action: 1.730 [0.000, 3.000],  loss: 10.540824, mae: 62.199011, mean_q: 64.787974, mean_eps: 0.661387
  56610/150000: episode: 607, duration: 0.751s, episode steps: 113, steps per second: 150, episode reward: -71.125, mean reward: -0.629 [-100.000,  9.205], mean action: 1.690 [0.000, 3.000],  loss: 10.294775, mae: 62.240983, mean_q: 63.923449, mean_eps: 0.660682
  56869/150000: episode: 608, duration: 1.766s, episode steps: 259, steps per second: 147, episode reward: -147.449, mean reward: -0.569 [-100.000, 48.185], mean action: 1.764 [0.000, 3.000],  loss: 11.652014, mae: 61.837463, mean_q: 63.365754, mean_eps: 0.659566
  56967/150000: episode: 609, duration: 0.675s, episode steps:  98, steps per second: 145, episode reward: -67.126, mean reward: -0.685 [-100.000, 11.892], mean action: 1.357 [0.000, 3.000],  loss: 8.486345, mae: 62.030952, mean_q: 63.787210, mean_eps: 0.658495
  57051/150000: episode: 610, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -52.415, mean reward: -0.624 [-100.000, 15.394], mean action: 1.607 [0.000, 3.000],  loss: 8.693035, mae: 62.400738, mean_q: 65.040141, mean_eps: 0.657949
  57171/150000: episode: 611, duration: 0.803s, episode steps: 120, steps per second: 149, episode reward: -58.879, mean reward: -0.491 [-100.000,  9.057], mean action: 1.558 [0.000, 3.000],  loss: 10.065366, mae: 63.151879, mean_q: 65.768080, mean_eps: 0.657337
  57251/150000: episode: 612, duration: 0.542s, episode steps:  80, steps per second: 148, episode reward: -63.140, mean reward: -0.789 [-100.000, 12.203], mean action: 1.637 [0.000, 3.000],  loss: 10.507252, mae: 63.101763, mean_q: 64.703660, mean_eps: 0.656737
  57371/150000: episode: 613, duration: 0.822s, episode steps: 120, steps per second: 146, episode reward: -12.050, mean reward: -0.100 [-100.000, 16.615], mean action: 1.750 [0.000, 3.000],  loss: 6.477774, mae: 63.450891, mean_q: 66.055349, mean_eps: 0.656137
  57439/150000: episode: 614, duration: 0.455s, episode steps:  68, steps per second: 149, episode reward: -85.594, mean reward: -1.259 [-100.000,  7.247], mean action: 1.662 [0.000, 3.000],  loss: 13.821625, mae: 62.392489, mean_q: 65.251613, mean_eps: 0.655573
  57564/150000: episode: 615, duration: 0.868s, episode steps: 125, steps per second: 144, episode reward: -140.075, mean reward: -1.121 [-100.000,  4.535], mean action: 1.368 [0.000, 3.000],  loss: 11.009445, mae: 62.801761, mean_q: 64.840661, mean_eps: 0.654994
  57664/150000: episode: 616, duration: 0.738s, episode steps: 100, steps per second: 136, episode reward: -105.926, mean reward: -1.059 [-100.000,  9.558], mean action: 1.610 [0.000, 3.000],  loss: 11.321934, mae: 63.234254, mean_q: 65.392144, mean_eps: 0.654319
  57751/150000: episode: 617, duration: 0.588s, episode steps:  87, steps per second: 148, episode reward: -100.182, mean reward: -1.152 [-100.000,  8.748], mean action: 1.540 [0.000, 3.000],  loss: 12.394118, mae: 62.126854, mean_q: 65.493639, mean_eps: 0.653758
  57870/150000: episode: 618, duration: 0.793s, episode steps: 119, steps per second: 150, episode reward: -66.387, mean reward: -0.558 [-100.000, 18.525], mean action: 1.504 [0.000, 3.000],  loss: 13.023872, mae: 62.668582, mean_q: 65.045268, mean_eps: 0.653140
  57979/150000: episode: 619, duration: 0.761s, episode steps: 109, steps per second: 143, episode reward: -37.578, mean reward: -0.345 [-100.000, 20.083], mean action: 1.532 [0.000, 3.000],  loss: 11.210349, mae: 62.395642, mean_q: 64.709913, mean_eps: 0.652456
  58061/150000: episode: 620, duration: 0.555s, episode steps:  82, steps per second: 148, episode reward: -25.144, mean reward: -0.307 [-100.000, 13.428], mean action: 1.537 [0.000, 3.000],  loss: 9.458304, mae: 62.491687, mean_q: 65.315438, mean_eps: 0.651883
  58178/150000: episode: 621, duration: 0.779s, episode steps: 117, steps per second: 150, episode reward: -44.768, mean reward: -0.383 [-100.000, 13.204], mean action: 1.581 [0.000, 3.000],  loss: 8.314578, mae: 62.797316, mean_q: 65.223351, mean_eps: 0.651286
  58270/150000: episode: 622, duration: 0.644s, episode steps:  92, steps per second: 143, episode reward: -77.889, mean reward: -0.847 [-100.000, 14.540], mean action: 1.467 [0.000, 3.000],  loss: 12.268230, mae: 63.116176, mean_q: 64.622823, mean_eps: 0.650659
  58403/150000: episode: 623, duration: 0.896s, episode steps: 133, steps per second: 148, episode reward: -28.896, mean reward: -0.217 [-100.000, 11.431], mean action: 1.571 [0.000, 3.000],  loss: 9.969335, mae: 63.434195, mean_q: 66.546622, mean_eps: 0.649984
  58523/150000: episode: 624, duration: 0.799s, episode steps: 120, steps per second: 150, episode reward: -68.820, mean reward: -0.573 [-100.000, 13.547], mean action: 1.600 [0.000, 3.000],  loss: 6.715330, mae: 63.104613, mean_q: 65.957413, mean_eps: 0.649225
  58606/150000: episode: 625, duration: 0.593s, episode steps:  83, steps per second: 140, episode reward: -53.026, mean reward: -0.639 [-100.000,  6.808], mean action: 1.506 [0.000, 3.000],  loss: 7.824894, mae: 62.718278, mean_q: 64.238659, mean_eps: 0.648616
  58694/150000: episode: 626, duration: 0.593s, episode steps:  88, steps per second: 148, episode reward: -43.708, mean reward: -0.497 [-100.000, 23.649], mean action: 1.739 [0.000, 3.000],  loss: 13.604575, mae: 63.192308, mean_q: 65.733830, mean_eps: 0.648103
  58830/150000: episode: 627, duration: 0.899s, episode steps: 136, steps per second: 151, episode reward: -67.157, mean reward: -0.494 [-100.000, 17.584], mean action: 1.507 [0.000, 3.000],  loss: 9.642200, mae: 62.472540, mean_q: 64.471977, mean_eps: 0.647431
  58909/150000: episode: 628, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -57.712, mean reward: -0.731 [-100.000, 11.583], mean action: 1.734 [0.000, 3.000],  loss: 9.423449, mae: 62.299444, mean_q: 64.353851, mean_eps: 0.646786
  59033/150000: episode: 629, duration: 0.842s, episode steps: 124, steps per second: 147, episode reward: -2.066, mean reward: -0.017 [-100.000,  8.768], mean action: 1.742 [0.000, 3.000],  loss: 9.493475, mae: 62.647921, mean_q: 64.869628, mean_eps: 0.646177
  59132/150000: episode: 630, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -63.298, mean reward: -0.639 [-100.000,  6.503], mean action: 1.758 [0.000, 3.000],  loss: 9.039691, mae: 62.576731, mean_q: 64.680751, mean_eps: 0.645508
  59244/150000: episode: 631, duration: 0.800s, episode steps: 112, steps per second: 140, episode reward: -47.413, mean reward: -0.423 [-100.000, 18.235], mean action: 1.580 [0.000, 3.000],  loss: 9.545089, mae: 63.086767, mean_q: 65.772693, mean_eps: 0.644875
  59345/150000: episode: 632, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -87.440, mean reward: -0.866 [-100.000, 15.953], mean action: 1.495 [0.000, 3.000],  loss: 7.279147, mae: 62.958933, mean_q: 66.079593, mean_eps: 0.644236
  59424/150000: episode: 633, duration: 0.534s, episode steps:  79, steps per second: 148, episode reward: -8.391, mean reward: -0.106 [-100.000, 11.973], mean action: 1.823 [0.000, 3.000],  loss: 8.304052, mae: 63.260626, mean_q: 66.575623, mean_eps: 0.643696
  59525/150000: episode: 634, duration: 0.701s, episode steps: 101, steps per second: 144, episode reward: -90.802, mean reward: -0.899 [-100.000, 11.897], mean action: 1.663 [0.000, 3.000],  loss: 8.166873, mae: 62.065902, mean_q: 62.622874, mean_eps: 0.643156
  59632/150000: episode: 635, duration: 0.735s, episode steps: 107, steps per second: 146, episode reward: -32.109, mean reward: -0.300 [-100.000, 18.732], mean action: 1.551 [0.000, 3.000],  loss: 11.131933, mae: 63.098139, mean_q: 66.729604, mean_eps: 0.642532
  59719/150000: episode: 636, duration: 0.685s, episode steps:  87, steps per second: 127, episode reward: -28.408, mean reward: -0.327 [-100.000, 14.831], mean action: 1.690 [0.000, 3.000],  loss: 9.434478, mae: 63.325895, mean_q: 67.560823, mean_eps: 0.641950
  59831/150000: episode: 637, duration: 0.906s, episode steps: 112, steps per second: 124, episode reward: -69.500, mean reward: -0.621 [-100.000,  9.905], mean action: 1.688 [0.000, 3.000],  loss: 10.308658, mae: 63.022585, mean_q: 65.390186, mean_eps: 0.641353
  59918/150000: episode: 638, duration: 0.663s, episode steps:  87, steps per second: 131, episode reward: -9.584, mean reward: -0.110 [-100.000, 46.848], mean action: 1.563 [0.000, 3.000],  loss: 12.308696, mae: 62.960640, mean_q: 65.292140, mean_eps: 0.640756
  60036/150000: episode: 639, duration: 0.806s, episode steps: 118, steps per second: 146, episode reward: -71.917, mean reward: -0.609 [-100.000, 11.489], mean action: 1.763 [0.000, 3.000],  loss: 10.581370, mae: 62.718491, mean_q: 64.139967, mean_eps: 0.640141
  60155/150000: episode: 640, duration: 0.847s, episode steps: 119, steps per second: 141, episode reward: -12.193, mean reward: -0.102 [-100.000, 38.990], mean action: 1.681 [0.000, 3.000],  loss: 12.477680, mae: 62.678637, mean_q: 67.383712, mean_eps: 0.639430
  60257/150000: episode: 641, duration: 0.842s, episode steps: 102, steps per second: 121, episode reward: -18.977, mean reward: -0.186 [-100.000, 16.171], mean action: 1.706 [0.000, 3.000],  loss: 13.093858, mae: 63.019857, mean_q: 65.699990, mean_eps: 0.638767
  60390/150000: episode: 642, duration: 1.043s, episode steps: 133, steps per second: 127, episode reward:  3.171, mean reward:  0.024 [-100.000, 48.032], mean action: 1.624 [0.000, 3.000],  loss: 11.413312, mae: 62.625302, mean_q: 67.047005, mean_eps: 0.638062
  60492/150000: episode: 643, duration: 0.769s, episode steps: 102, steps per second: 133, episode reward: -67.377, mean reward: -0.661 [-100.000, 17.008], mean action: 1.706 [0.000, 3.000],  loss: 11.494748, mae: 62.814751, mean_q: 66.177883, mean_eps: 0.637357
  60600/150000: episode: 644, duration: 0.791s, episode steps: 108, steps per second: 136, episode reward: -72.412, mean reward: -0.670 [-100.000,  7.803], mean action: 1.722 [0.000, 3.000],  loss: 9.246202, mae: 62.541014, mean_q: 66.389166, mean_eps: 0.636727
  60708/150000: episode: 645, duration: 0.816s, episode steps: 108, steps per second: 132, episode reward: -72.593, mean reward: -0.672 [-100.000,  9.113], mean action: 1.556 [0.000, 3.000],  loss: 12.948742, mae: 61.827908, mean_q: 64.583117, mean_eps: 0.636079
  60841/150000: episode: 646, duration: 0.961s, episode steps: 133, steps per second: 138, episode reward: -79.499, mean reward: -0.598 [-100.000, 28.188], mean action: 1.662 [0.000, 3.000],  loss: 10.992132, mae: 62.823809, mean_q: 66.782582, mean_eps: 0.635356
  60911/150000: episode: 647, duration: 0.472s, episode steps:  70, steps per second: 148, episode reward: -64.726, mean reward: -0.925 [-100.000,  7.113], mean action: 1.671 [0.000, 3.000],  loss: 10.408920, mae: 62.264281, mean_q: 66.252018, mean_eps: 0.634747
  61030/150000: episode: 648, duration: 0.833s, episode steps: 119, steps per second: 143, episode reward: -75.589, mean reward: -0.635 [-100.000, 23.380], mean action: 1.613 [0.000, 3.000],  loss: 14.227562, mae: 62.790183, mean_q: 66.654958, mean_eps: 0.634180
  61094/150000: episode: 649, duration: 0.428s, episode steps:  64, steps per second: 150, episode reward: -97.557, mean reward: -1.524 [-100.000,  9.735], mean action: 1.812 [0.000, 3.000],  loss: 10.157825, mae: 63.177923, mean_q: 66.131986, mean_eps: 0.633631
  61188/150000: episode: 650, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -73.555, mean reward: -0.783 [-100.000, 11.194], mean action: 1.596 [0.000, 3.000],  loss: 8.346478, mae: 61.639349, mean_q: 65.058985, mean_eps: 0.633157
  61275/150000: episode: 651, duration: 0.593s, episode steps:  87, steps per second: 147, episode reward: -54.316, mean reward: -0.624 [-100.000,  8.565], mean action: 1.770 [0.000, 3.000],  loss: 14.334293, mae: 62.518231, mean_q: 65.070193, mean_eps: 0.632614
  61397/150000: episode: 652, duration: 0.826s, episode steps: 122, steps per second: 148, episode reward: -49.006, mean reward: -0.402 [-100.000, 14.158], mean action: 1.680 [0.000, 3.000],  loss: 9.225482, mae: 62.157010, mean_q: 64.777354, mean_eps: 0.631987
  61499/150000: episode: 653, duration: 0.687s, episode steps: 102, steps per second: 148, episode reward: -13.766, mean reward: -0.135 [-100.000, 18.401], mean action: 1.735 [0.000, 3.000],  loss: 8.369714, mae: 62.896583, mean_q: 66.570101, mean_eps: 0.631315
  61576/150000: episode: 654, duration: 0.522s, episode steps:  77, steps per second: 147, episode reward: -20.602, mean reward: -0.268 [-100.000, 18.855], mean action: 1.766 [0.000, 3.000],  loss: 10.603910, mae: 61.967562, mean_q: 65.571749, mean_eps: 0.630778
  61713/150000: episode: 655, duration: 0.935s, episode steps: 137, steps per second: 147, episode reward: -18.587, mean reward: -0.136 [-100.000, 23.680], mean action: 1.701 [0.000, 3.000],  loss: 12.758178, mae: 62.150577, mean_q: 65.308793, mean_eps: 0.630136
  61859/150000: episode: 656, duration: 0.983s, episode steps: 146, steps per second: 148, episode reward:  1.988, mean reward:  0.014 [-100.000, 12.473], mean action: 1.705 [0.000, 3.000],  loss: 7.881794, mae: 61.793315, mean_q: 65.212351, mean_eps: 0.629287
  62008/150000: episode: 657, duration: 1.042s, episode steps: 149, steps per second: 143, episode reward: -166.669, mean reward: -1.119 [-100.000, 10.182], mean action: 1.691 [0.000, 3.000],  loss: 9.778887, mae: 61.926571, mean_q: 65.566032, mean_eps: 0.628402
  62116/150000: episode: 658, duration: 0.726s, episode steps: 108, steps per second: 149, episode reward: -61.572, mean reward: -0.570 [-100.000, 20.684], mean action: 1.704 [0.000, 3.000],  loss: 12.558522, mae: 62.804163, mean_q: 66.575888, mean_eps: 0.627631
  62224/150000: episode: 659, duration: 0.737s, episode steps: 108, steps per second: 147, episode reward: -41.399, mean reward: -0.383 [-100.000, 15.919], mean action: 1.750 [0.000, 3.000],  loss: 8.181722, mae: 62.428889, mean_q: 65.742121, mean_eps: 0.626983
  62320/150000: episode: 660, duration: 0.656s, episode steps:  96, steps per second: 146, episode reward: -100.154, mean reward: -1.043 [-100.000,  6.103], mean action: 1.427 [0.000, 3.000],  loss: 18.011442, mae: 62.634246, mean_q: 65.586874, mean_eps: 0.626371
  62418/150000: episode: 661, duration: 0.655s, episode steps:  98, steps per second: 150, episode reward: -55.487, mean reward: -0.566 [-100.000,  7.315], mean action: 1.592 [0.000, 3.000],  loss: 8.645491, mae: 63.047435, mean_q: 67.468736, mean_eps: 0.625789
  62529/150000: episode: 662, duration: 0.754s, episode steps: 111, steps per second: 147, episode reward: -62.981, mean reward: -0.567 [-100.000, 20.961], mean action: 1.676 [0.000, 3.000],  loss: 9.335537, mae: 63.166998, mean_q: 67.177425, mean_eps: 0.625162
  62658/150000: episode: 663, duration: 0.879s, episode steps: 129, steps per second: 147, episode reward: -125.452, mean reward: -0.972 [-100.000,  8.267], mean action: 1.643 [0.000, 3.000],  loss: 8.029303, mae: 62.614041, mean_q: 66.357016, mean_eps: 0.624442
  62740/150000: episode: 664, duration: 0.548s, episode steps:  82, steps per second: 150, episode reward: -40.531, mean reward: -0.494 [-100.000, 29.104], mean action: 1.927 [0.000, 3.000],  loss: 9.219119, mae: 62.577400, mean_q: 65.639189, mean_eps: 0.623809
  62826/150000: episode: 665, duration: 0.575s, episode steps:  86, steps per second: 150, episode reward: -83.408, mean reward: -0.970 [-100.000,  8.999], mean action: 1.698 [0.000, 3.000],  loss: 7.803437, mae: 62.730695, mean_q: 65.827652, mean_eps: 0.623305
  62918/150000: episode: 666, duration: 0.656s, episode steps:  92, steps per second: 140, episode reward: -56.129, mean reward: -0.610 [-100.000, 17.097], mean action: 1.793 [0.000, 3.000],  loss: 10.972164, mae: 62.792975, mean_q: 66.541399, mean_eps: 0.622771
  63013/150000: episode: 667, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -69.762, mean reward: -0.734 [-100.000, 10.573], mean action: 1.684 [0.000, 3.000],  loss: 10.052822, mae: 63.008411, mean_q: 66.375308, mean_eps: 0.622210
  63127/150000: episode: 668, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -3.317, mean reward: -0.029 [-100.000,  8.747], mean action: 1.702 [0.000, 3.000],  loss: 18.075255, mae: 63.406276, mean_q: 66.942700, mean_eps: 0.621583
  63241/150000: episode: 669, duration: 0.794s, episode steps: 114, steps per second: 144, episode reward: -32.219, mean reward: -0.283 [-100.000,  7.562], mean action: 1.526 [0.000, 3.000],  loss: 15.982412, mae: 63.538280, mean_q: 67.694197, mean_eps: 0.620899
  63375/150000: episode: 670, duration: 0.901s, episode steps: 134, steps per second: 149, episode reward: -78.310, mean reward: -0.584 [-100.000, 16.909], mean action: 1.642 [0.000, 3.000],  loss: 11.030347, mae: 63.323843, mean_q: 66.316755, mean_eps: 0.620155
  63499/150000: episode: 671, duration: 0.883s, episode steps: 124, steps per second: 140, episode reward: -41.066, mean reward: -0.331 [-100.000, 13.282], mean action: 1.726 [0.000, 3.000],  loss: 12.091326, mae: 63.551165, mean_q: 68.005480, mean_eps: 0.619381
  63586/150000: episode: 672, duration: 0.607s, episode steps:  87, steps per second: 143, episode reward: -40.596, mean reward: -0.467 [-100.000, 20.275], mean action: 1.701 [0.000, 3.000],  loss: 10.310171, mae: 63.603507, mean_q: 67.612769, mean_eps: 0.618748
  64586/150000: episode: 673, duration: 7.879s, episode steps: 1000, steps per second: 127, episode reward: 67.770, mean reward:  0.068 [-22.989, 92.991], mean action: 1.764 [0.000, 3.000],  loss: 12.123798, mae: 62.952750, mean_q: 66.660212, mean_eps: 0.615487
  64665/150000: episode: 674, duration: 0.537s, episode steps:  79, steps per second: 147, episode reward: -50.300, mean reward: -0.637 [-100.000, 10.306], mean action: 1.494 [0.000, 3.000],  loss: 7.464032, mae: 63.207118, mean_q: 67.357463, mean_eps: 0.612250
  64783/150000: episode: 675, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -52.464, mean reward: -0.445 [-100.000, 16.034], mean action: 1.720 [0.000, 3.000],  loss: 11.104997, mae: 62.102522, mean_q: 65.106970, mean_eps: 0.611659
  64895/150000: episode: 676, duration: 0.771s, episode steps: 112, steps per second: 145, episode reward: -33.797, mean reward: -0.302 [-100.000, 22.081], mean action: 1.821 [0.000, 3.000],  loss: 11.075664, mae: 62.472798, mean_q: 66.699437, mean_eps: 0.610969
  64982/150000: episode: 677, duration: 0.624s, episode steps:  87, steps per second: 140, episode reward: 19.094, mean reward:  0.219 [-100.000, 18.731], mean action: 1.759 [0.000, 3.000],  loss: 14.779118, mae: 62.440899, mean_q: 65.631357, mean_eps: 0.610372
  65052/150000: episode: 678, duration: 0.469s, episode steps:  70, steps per second: 149, episode reward: -55.953, mean reward: -0.799 [-100.000, 13.265], mean action: 1.443 [0.000, 3.000],  loss: 11.760064, mae: 62.991815, mean_q: 67.864658, mean_eps: 0.609901
  65153/150000: episode: 679, duration: 0.677s, episode steps: 101, steps per second: 149, episode reward: -38.234, mean reward: -0.379 [-100.000, 14.920], mean action: 1.762 [0.000, 3.000],  loss: 14.948466, mae: 62.560733, mean_q: 66.058254, mean_eps: 0.609388
  65288/150000: episode: 680, duration: 0.931s, episode steps: 135, steps per second: 145, episode reward: -20.332, mean reward: -0.151 [-100.000, 11.914], mean action: 1.696 [0.000, 3.000],  loss: 9.662145, mae: 62.818966, mean_q: 66.428484, mean_eps: 0.608680
  65414/150000: episode: 681, duration: 0.851s, episode steps: 126, steps per second: 148, episode reward: -74.040, mean reward: -0.588 [-100.000,  9.724], mean action: 1.643 [0.000, 3.000],  loss: 9.606603, mae: 62.221442, mean_q: 66.718788, mean_eps: 0.607897
  65511/150000: episode: 682, duration: 0.642s, episode steps:  97, steps per second: 151, episode reward: -119.884, mean reward: -1.236 [-100.000,  8.502], mean action: 1.701 [0.000, 3.000],  loss: 10.042989, mae: 62.675549, mean_q: 66.914601, mean_eps: 0.607228
  65599/150000: episode: 683, duration: 0.621s, episode steps:  88, steps per second: 142, episode reward: -13.977, mean reward: -0.159 [-100.000, 10.762], mean action: 1.773 [0.000, 3.000],  loss: 11.927332, mae: 62.363498, mean_q: 66.140756, mean_eps: 0.606673
  65709/150000: episode: 684, duration: 0.742s, episode steps: 110, steps per second: 148, episode reward: -19.803, mean reward: -0.180 [-100.000, 14.096], mean action: 1.618 [0.000, 3.000],  loss: 8.985414, mae: 62.719629, mean_q: 68.415957, mean_eps: 0.606079
  65834/150000: episode: 685, duration: 0.831s, episode steps: 125, steps per second: 150, episode reward: -55.842, mean reward: -0.447 [-100.000, 11.476], mean action: 1.664 [0.000, 3.000],  loss: 11.969818, mae: 62.533793, mean_q: 66.787808, mean_eps: 0.605374
  65974/150000: episode: 686, duration: 0.964s, episode steps: 140, steps per second: 145, episode reward: -38.074, mean reward: -0.272 [-100.000, 14.219], mean action: 1.593 [0.000, 3.000],  loss: 11.522001, mae: 62.437086, mean_q: 66.852639, mean_eps: 0.604579
  66087/150000: episode: 687, duration: 0.759s, episode steps: 113, steps per second: 149, episode reward: -73.385, mean reward: -0.649 [-100.000, 13.782], mean action: 1.858 [0.000, 3.000],  loss: 10.857956, mae: 61.971374, mean_q: 66.042685, mean_eps: 0.603820
  66211/150000: episode: 688, duration: 0.851s, episode steps: 124, steps per second: 146, episode reward: -89.262, mean reward: -0.720 [-100.000, 19.962], mean action: 1.597 [0.000, 3.000],  loss: 11.431345, mae: 62.717699, mean_q: 66.815999, mean_eps: 0.603109
  66336/150000: episode: 689, duration: 0.848s, episode steps: 125, steps per second: 147, episode reward:  7.965, mean reward:  0.064 [-100.000, 19.129], mean action: 1.672 [0.000, 3.000],  loss: 17.717812, mae: 62.417860, mean_q: 65.889049, mean_eps: 0.602362
  66447/150000: episode: 690, duration: 0.740s, episode steps: 111, steps per second: 150, episode reward: -59.738, mean reward: -0.538 [-100.000,  8.636], mean action: 1.748 [0.000, 3.000],  loss: 12.904370, mae: 61.681343, mean_q: 65.083711, mean_eps: 0.601654
  66525/150000: episode: 691, duration: 0.544s, episode steps:  78, steps per second: 143, episode reward: -37.377, mean reward: -0.479 [-100.000, 11.547], mean action: 1.526 [0.000, 3.000],  loss: 8.149025, mae: 62.376782, mean_q: 68.032373, mean_eps: 0.601087
  66677/150000: episode: 692, duration: 1.022s, episode steps: 152, steps per second: 149, episode reward: -74.476, mean reward: -0.490 [-100.000, 13.221], mean action: 1.770 [0.000, 3.000],  loss: 14.611573, mae: 63.196082, mean_q: 67.166479, mean_eps: 0.600397
  66746/150000: episode: 693, duration: 0.460s, episode steps:  69, steps per second: 150, episode reward: -13.526, mean reward: -0.196 [-100.000, 13.657], mean action: 1.768 [0.000, 3.000],  loss: 10.500986, mae: 62.287216, mean_q: 66.243825, mean_eps: 0.599734
  66809/150000: episode: 694, duration: 0.425s, episode steps:  63, steps per second: 148, episode reward: -69.672, mean reward: -1.106 [-100.000, 17.947], mean action: 1.524 [0.000, 3.000],  loss: 7.419829, mae: 62.723779, mean_q: 65.750548, mean_eps: 0.599338
  66901/150000: episode: 695, duration: 0.629s, episode steps:  92, steps per second: 146, episode reward: -31.928, mean reward: -0.347 [-100.000,  9.388], mean action: 1.674 [0.000, 3.000],  loss: 10.227090, mae: 62.537801, mean_q: 67.116693, mean_eps: 0.598873
  66987/150000: episode: 696, duration: 0.574s, episode steps:  86, steps per second: 150, episode reward:  8.651, mean reward:  0.101 [-100.000, 12.790], mean action: 1.686 [0.000, 3.000],  loss: 7.071652, mae: 62.337462, mean_q: 65.856623, mean_eps: 0.598339
  67135/150000: episode: 697, duration: 1.004s, episode steps: 148, steps per second: 147, episode reward: -66.878, mean reward: -0.452 [-100.000, 16.798], mean action: 1.662 [0.000, 3.000],  loss: 12.962848, mae: 62.632690, mean_q: 67.970263, mean_eps: 0.597637
  67221/150000: episode: 698, duration: 0.604s, episode steps:  86, steps per second: 143, episode reward: -21.808, mean reward: -0.254 [-100.000,  8.346], mean action: 1.779 [0.000, 3.000],  loss: 7.234202, mae: 62.594810, mean_q: 67.195355, mean_eps: 0.596935
  67328/150000: episode: 699, duration: 0.838s, episode steps: 107, steps per second: 128, episode reward: -45.509, mean reward: -0.425 [-100.000, 12.259], mean action: 1.636 [0.000, 3.000],  loss: 12.593449, mae: 62.585160, mean_q: 66.330472, mean_eps: 0.596356
  67415/150000: episode: 700, duration: 0.581s, episode steps:  87, steps per second: 150, episode reward: -30.285, mean reward: -0.348 [-100.000, 11.628], mean action: 1.655 [0.000, 3.000],  loss: 15.469480, mae: 63.369151, mean_q: 68.111240, mean_eps: 0.595774
  67512/150000: episode: 701, duration: 0.671s, episode steps:  97, steps per second: 145, episode reward: 30.002, mean reward:  0.309 [-100.000, 10.646], mean action: 1.639 [0.000, 3.000],  loss: 10.186505, mae: 62.152957, mean_q: 67.772404, mean_eps: 0.595222
  67586/150000: episode: 702, duration: 0.492s, episode steps:  74, steps per second: 150, episode reward: -64.170, mean reward: -0.867 [-100.000, 13.068], mean action: 1.635 [0.000, 3.000],  loss: 15.378160, mae: 62.538974, mean_q: 66.762996, mean_eps: 0.594709
  67684/150000: episode: 703, duration: 0.656s, episode steps:  98, steps per second: 149, episode reward: -15.494, mean reward: -0.158 [-100.000, 11.289], mean action: 1.622 [0.000, 3.000],  loss: 15.240863, mae: 62.168492, mean_q: 66.880444, mean_eps: 0.594193
  67769/150000: episode: 704, duration: 0.611s, episode steps:  85, steps per second: 139, episode reward: -24.974, mean reward: -0.294 [-100.000, 11.467], mean action: 1.776 [0.000, 3.000],  loss: 11.567101, mae: 62.611380, mean_q: 68.025217, mean_eps: 0.593644
  67850/150000: episode: 705, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -26.621, mean reward: -0.329 [-100.000,  6.813], mean action: 1.716 [0.000, 3.000],  loss: 15.012198, mae: 62.797349, mean_q: 67.838094, mean_eps: 0.593146
  67944/150000: episode: 706, duration: 0.630s, episode steps:  94, steps per second: 149, episode reward: -63.734, mean reward: -0.678 [-100.000,  6.155], mean action: 1.660 [0.000, 3.000],  loss: 10.184229, mae: 62.657034, mean_q: 66.459775, mean_eps: 0.592621
  68032/150000: episode: 707, duration: 0.601s, episode steps:  88, steps per second: 146, episode reward: -57.119, mean reward: -0.649 [-100.000, 11.472], mean action: 1.693 [0.000, 3.000],  loss: 11.129043, mae: 62.230636, mean_q: 67.469921, mean_eps: 0.592075
  68107/150000: episode: 708, duration: 0.557s, episode steps:  75, steps per second: 135, episode reward: -54.982, mean reward: -0.733 [-100.000, 15.909], mean action: 1.920 [0.000, 3.000],  loss: 9.085459, mae: 63.335141, mean_q: 68.665076, mean_eps: 0.591586
  68225/150000: episode: 709, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -74.018, mean reward: -0.627 [-100.000, 11.110], mean action: 1.602 [0.000, 3.000],  loss: 19.504354, mae: 62.588304, mean_q: 66.968091, mean_eps: 0.591007
  68307/150000: episode: 710, duration: 0.553s, episode steps:  82, steps per second: 148, episode reward: -44.589, mean reward: -0.544 [-100.000, 60.828], mean action: 1.646 [0.000, 3.000],  loss: 7.958267, mae: 62.286175, mean_q: 66.048950, mean_eps: 0.590407
  68396/150000: episode: 711, duration: 0.628s, episode steps:  89, steps per second: 142, episode reward: -28.075, mean reward: -0.315 [-100.000,  9.917], mean action: 1.730 [0.000, 3.000],  loss: 7.868220, mae: 63.710489, mean_q: 70.110837, mean_eps: 0.589894
  68519/150000: episode: 712, duration: 0.844s, episode steps: 123, steps per second: 146, episode reward: -202.612, mean reward: -1.647 [-100.000,  4.548], mean action: 1.870 [0.000, 3.000],  loss: 19.113985, mae: 62.235818, mean_q: 67.854154, mean_eps: 0.589258
  68657/150000: episode: 713, duration: 0.955s, episode steps: 138, steps per second: 145, episode reward: -28.300, mean reward: -0.205 [-100.000, 17.145], mean action: 1.681 [0.000, 3.000],  loss: 10.187698, mae: 62.796009, mean_q: 67.267597, mean_eps: 0.588475
  68795/150000: episode: 714, duration: 1.021s, episode steps: 138, steps per second: 135, episode reward: -105.183, mean reward: -0.762 [-100.000,  6.873], mean action: 1.667 [0.000, 3.000],  loss: 13.123046, mae: 62.898740, mean_q: 66.601049, mean_eps: 0.587647
  68917/150000: episode: 715, duration: 0.825s, episode steps: 122, steps per second: 148, episode reward:  8.662, mean reward:  0.071 [-100.000, 16.813], mean action: 1.828 [0.000, 3.000],  loss: 8.073500, mae: 62.960126, mean_q: 67.958286, mean_eps: 0.586867
  69005/150000: episode: 716, duration: 0.620s, episode steps:  88, steps per second: 142, episode reward: -5.305, mean reward: -0.060 [-100.000, 10.300], mean action: 1.750 [0.000, 3.000],  loss: 15.304006, mae: 62.541713, mean_q: 67.881520, mean_eps: 0.586237
  69127/150000: episode: 717, duration: 0.829s, episode steps: 122, steps per second: 147, episode reward: -59.317, mean reward: -0.486 [-100.000, 20.457], mean action: 1.680 [0.000, 3.000],  loss: 8.440840, mae: 61.861544, mean_q: 66.166623, mean_eps: 0.585607
  69257/150000: episode: 718, duration: 0.873s, episode steps: 130, steps per second: 149, episode reward: -6.678, mean reward: -0.051 [-100.000,  8.050], mean action: 1.592 [0.000, 3.000],  loss: 11.348627, mae: 62.248536, mean_q: 67.808164, mean_eps: 0.584851
  69376/150000: episode: 719, duration: 0.836s, episode steps: 119, steps per second: 142, episode reward: -124.519, mean reward: -1.046 [-100.000,  2.777], mean action: 1.689 [0.000, 3.000],  loss: 15.436251, mae: 62.206804, mean_q: 67.021774, mean_eps: 0.584104
  69505/150000: episode: 720, duration: 0.875s, episode steps: 129, steps per second: 147, episode reward: -18.860, mean reward: -0.146 [-100.000, 14.499], mean action: 1.705 [0.000, 3.000],  loss: 17.737142, mae: 62.533825, mean_q: 67.351808, mean_eps: 0.583360
  69637/150000: episode: 721, duration: 1.110s, episode steps: 132, steps per second: 119, episode reward: -62.560, mean reward: -0.474 [-100.000, 11.699], mean action: 1.477 [0.000, 3.000],  loss: 9.748587, mae: 61.939917, mean_q: 66.759516, mean_eps: 0.582577
  69724/150000: episode: 722, duration: 0.686s, episode steps:  87, steps per second: 127, episode reward: -47.049, mean reward: -0.541 [-100.000, 14.597], mean action: 1.506 [0.000, 3.000],  loss: 12.808531, mae: 62.497676, mean_q: 66.530473, mean_eps: 0.581920
  69844/150000: episode: 723, duration: 0.917s, episode steps: 120, steps per second: 131, episode reward: -138.785, mean reward: -1.157 [-100.000, 15.956], mean action: 1.817 [0.000, 3.000],  loss: 8.044913, mae: 63.340166, mean_q: 68.561510, mean_eps: 0.581299
  69944/150000: episode: 724, duration: 0.794s, episode steps: 100, steps per second: 126, episode reward: -101.509, mean reward: -1.015 [-100.000,  8.225], mean action: 1.780 [0.000, 3.000],  loss: 13.894762, mae: 62.640367, mean_q: 68.429052, mean_eps: 0.580639
  70043/150000: episode: 725, duration: 0.753s, episode steps:  99, steps per second: 131, episode reward: 21.102, mean reward:  0.213 [-100.000, 13.139], mean action: 1.636 [0.000, 3.000],  loss: 9.722761, mae: 61.787248, mean_q: 65.795634, mean_eps: 0.580042
  70118/150000: episode: 726, duration: 0.564s, episode steps:  75, steps per second: 133, episode reward: -64.546, mean reward: -0.861 [-100.000,  8.621], mean action: 1.693 [0.000, 3.000],  loss: 27.237101, mae: 62.100357, mean_q: 67.271986, mean_eps: 0.579520
  70223/150000: episode: 727, duration: 0.758s, episode steps: 105, steps per second: 139, episode reward: -78.422, mean reward: -0.747 [-100.000, 17.917], mean action: 1.629 [0.000, 3.000],  loss: 16.563617, mae: 62.832136, mean_q: 67.637768, mean_eps: 0.578980
  70342/150000: episode: 728, duration: 0.805s, episode steps: 119, steps per second: 148, episode reward: -26.759, mean reward: -0.225 [-100.000, 13.166], mean action: 1.689 [0.000, 3.000],  loss: 19.780442, mae: 62.722324, mean_q: 67.348648, mean_eps: 0.578308
  70463/150000: episode: 729, duration: 0.821s, episode steps: 121, steps per second: 147, episode reward: -37.326, mean reward: -0.308 [-100.000, 16.200], mean action: 1.636 [0.000, 3.000],  loss: 16.975814, mae: 61.909138, mean_q: 66.946690, mean_eps: 0.577588
  70561/150000: episode: 730, duration: 0.678s, episode steps:  98, steps per second: 144, episode reward: -37.895, mean reward: -0.387 [-100.000,  7.890], mean action: 1.520 [0.000, 3.000],  loss: 15.201792, mae: 62.239428, mean_q: 66.468485, mean_eps: 0.576931
  70638/150000: episode: 731, duration: 0.520s, episode steps:  77, steps per second: 148, episode reward: -40.502, mean reward: -0.526 [-100.000, 11.265], mean action: 1.584 [0.000, 3.000],  loss: 14.576806, mae: 62.116486, mean_q: 66.431114, mean_eps: 0.576406
  70809/150000: episode: 732, duration: 1.176s, episode steps: 171, steps per second: 145, episode reward: -199.421, mean reward: -1.166 [-100.000,  8.869], mean action: 1.596 [0.000, 3.000],  loss: 19.314069, mae: 62.482774, mean_q: 67.317827, mean_eps: 0.575662
  70877/150000: episode: 733, duration: 0.470s, episode steps:  68, steps per second: 145, episode reward: -22.388, mean reward: -0.329 [-100.000, 16.273], mean action: 1.706 [0.000, 3.000],  loss: 22.462782, mae: 62.462090, mean_q: 66.417883, mean_eps: 0.574945
  70975/150000: episode: 734, duration: 0.655s, episode steps:  98, steps per second: 150, episode reward: -52.244, mean reward: -0.533 [-100.000, 18.740], mean action: 1.694 [0.000, 3.000],  loss: 13.902660, mae: 62.476150, mean_q: 67.000669, mean_eps: 0.574447
  71087/150000: episode: 735, duration: 0.757s, episode steps: 112, steps per second: 148, episode reward: -54.926, mean reward: -0.490 [-100.000,  6.906], mean action: 1.679 [0.000, 3.000],  loss: 17.615552, mae: 62.923764, mean_q: 68.022538, mean_eps: 0.573817
  71164/150000: episode: 736, duration: 0.552s, episode steps:  77, steps per second: 139, episode reward:  7.030, mean reward:  0.091 [-100.000, 14.748], mean action: 1.792 [0.000, 3.000],  loss: 16.673412, mae: 63.165287, mean_q: 68.196383, mean_eps: 0.573250
  71266/150000: episode: 737, duration: 0.686s, episode steps: 102, steps per second: 149, episode reward: -180.001, mean reward: -1.765 [-100.000, 33.245], mean action: 1.657 [0.000, 3.000],  loss: 10.405435, mae: 64.005354, mean_q: 68.493594, mean_eps: 0.572713
  71391/150000: episode: 738, duration: 0.836s, episode steps: 125, steps per second: 150, episode reward: -45.779, mean reward: -0.366 [-100.000, 10.215], mean action: 1.592 [0.000, 3.000],  loss: 13.571447, mae: 63.265750, mean_q: 68.410933, mean_eps: 0.572032
  71499/150000: episode: 739, duration: 0.742s, episode steps: 108, steps per second: 145, episode reward: -65.188, mean reward: -0.604 [-100.000, 10.885], mean action: 1.657 [0.000, 3.000],  loss: 16.119835, mae: 62.920135, mean_q: 69.102608, mean_eps: 0.571333
  71573/150000: episode: 740, duration: 0.500s, episode steps:  74, steps per second: 148, episode reward: -7.027, mean reward: -0.095 [-100.000, 11.791], mean action: 1.581 [0.000, 3.000],  loss: 9.475420, mae: 62.748277, mean_q: 67.666113, mean_eps: 0.570787
  71656/150000: episode: 741, duration: 0.561s, episode steps:  83, steps per second: 148, episode reward: -18.928, mean reward: -0.228 [-100.000, 13.750], mean action: 1.566 [0.000, 3.000],  loss: 11.437766, mae: 62.697263, mean_q: 68.178491, mean_eps: 0.570316
  71730/150000: episode: 742, duration: 0.510s, episode steps:  74, steps per second: 145, episode reward: -25.405, mean reward: -0.343 [-100.000, 11.248], mean action: 1.514 [0.000, 3.000],  loss: 8.406014, mae: 63.292573, mean_q: 68.272783, mean_eps: 0.569845
  71842/150000: episode: 743, duration: 0.762s, episode steps: 112, steps per second: 147, episode reward: -109.005, mean reward: -0.973 [-100.000, 32.184], mean action: 1.688 [0.000, 3.000],  loss: 10.134591, mae: 63.134017, mean_q: 68.756469, mean_eps: 0.569287
  71967/150000: episode: 744, duration: 0.838s, episode steps: 125, steps per second: 149, episode reward: -79.964, mean reward: -0.640 [-100.000,  7.532], mean action: 1.592 [0.000, 3.000],  loss: 9.014257, mae: 63.380704, mean_q: 68.229346, mean_eps: 0.568576
  72059/150000: episode: 745, duration: 0.666s, episode steps:  92, steps per second: 138, episode reward: -57.469, mean reward: -0.625 [-100.000,  9.488], mean action: 1.489 [0.000, 3.000],  loss: 11.403168, mae: 62.909026, mean_q: 68.863724, mean_eps: 0.567925
  72172/150000: episode: 746, duration: 0.819s, episode steps: 113, steps per second: 138, episode reward: 14.210, mean reward:  0.126 [-100.000, 16.541], mean action: 1.664 [0.000, 3.000],  loss: 17.742318, mae: 63.677042, mean_q: 70.987722, mean_eps: 0.567310
  72260/150000: episode: 747, duration: 0.588s, episode steps:  88, steps per second: 150, episode reward: -74.373, mean reward: -0.845 [-100.000, 12.825], mean action: 1.534 [0.000, 3.000],  loss: 20.312507, mae: 63.048565, mean_q: 69.926043, mean_eps: 0.566707
  72359/150000: episode: 748, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -22.584, mean reward: -0.228 [-100.000,  9.596], mean action: 1.465 [0.000, 3.000],  loss: 16.533881, mae: 62.967624, mean_q: 70.044958, mean_eps: 0.566146
  72457/150000: episode: 749, duration: 0.680s, episode steps:  98, steps per second: 144, episode reward: -91.872, mean reward: -0.937 [-100.000,  9.507], mean action: 1.765 [0.000, 3.000],  loss: 13.138812, mae: 63.163143, mean_q: 68.274043, mean_eps: 0.565555
  72812/150000: episode: 750, duration: 2.462s, episode steps: 355, steps per second: 144, episode reward: -158.114, mean reward: -0.445 [-100.000,  5.303], mean action: 1.749 [0.000, 3.000],  loss: 15.912539, mae: 62.926158, mean_q: 69.162110, mean_eps: 0.564196
  72902/150000: episode: 751, duration: 0.607s, episode steps:  90, steps per second: 148, episode reward: -35.997, mean reward: -0.400 [-100.000, 11.481], mean action: 1.689 [0.000, 3.000],  loss: 11.452682, mae: 62.986408, mean_q: 69.123286, mean_eps: 0.562861
  73007/150000: episode: 752, duration: 0.733s, episode steps: 105, steps per second: 143, episode reward: -244.837, mean reward: -2.332 [-100.000,  6.842], mean action: 1.590 [0.000, 3.000],  loss: 18.554676, mae: 62.672108, mean_q: 69.837716, mean_eps: 0.562276
  73124/150000: episode: 753, duration: 0.836s, episode steps: 117, steps per second: 140, episode reward: -146.224, mean reward: -1.250 [-100.000, 17.445], mean action: 1.607 [0.000, 3.000],  loss: 10.417946, mae: 62.183767, mean_q: 68.572010, mean_eps: 0.561610
  73208/150000: episode: 754, duration: 0.566s, episode steps:  84, steps per second: 148, episode reward: -45.681, mean reward: -0.544 [-100.000,  8.826], mean action: 1.810 [0.000, 3.000],  loss: 22.651885, mae: 62.623944, mean_q: 69.190250, mean_eps: 0.561007
  73314/150000: episode: 755, duration: 0.735s, episode steps: 106, steps per second: 144, episode reward: -231.160, mean reward: -2.181 [-100.000, 24.653], mean action: 1.679 [0.000, 3.000],  loss: 14.610623, mae: 62.041740, mean_q: 68.495037, mean_eps: 0.560437
  73420/150000: episode: 756, duration: 0.719s, episode steps: 106, steps per second: 147, episode reward: -107.375, mean reward: -1.013 [-100.000,  7.782], mean action: 1.557 [0.000, 3.000],  loss: 18.837346, mae: 62.321655, mean_q: 67.646291, mean_eps: 0.559801
  73509/150000: episode: 757, duration: 0.600s, episode steps:  89, steps per second: 148, episode reward: -45.438, mean reward: -0.511 [-100.000,  8.844], mean action: 1.742 [0.000, 3.000],  loss: 16.765243, mae: 61.702564, mean_q: 66.926276, mean_eps: 0.559216
  73626/150000: episode: 758, duration: 0.801s, episode steps: 117, steps per second: 146, episode reward: -34.106, mean reward: -0.292 [-100.000, 15.537], mean action: 1.752 [0.000, 3.000],  loss: 15.469957, mae: 61.971019, mean_q: 68.342121, mean_eps: 0.558598
  73739/150000: episode: 759, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -106.706, mean reward: -0.944 [-100.000,  9.499], mean action: 1.735 [0.000, 3.000],  loss: 14.958730, mae: 62.119723, mean_q: 67.522404, mean_eps: 0.557908
  73840/150000: episode: 760, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -51.523, mean reward: -0.510 [-100.000, 23.237], mean action: 1.614 [0.000, 3.000],  loss: 13.882075, mae: 62.463909, mean_q: 67.874967, mean_eps: 0.557266
  73962/150000: episode: 761, duration: 0.842s, episode steps: 122, steps per second: 145, episode reward: -34.055, mean reward: -0.279 [-100.000, 16.173], mean action: 1.721 [0.000, 3.000],  loss: 13.280506, mae: 62.515590, mean_q: 68.748287, mean_eps: 0.556597
  74043/150000: episode: 762, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -14.753, mean reward: -0.182 [-100.000, 12.434], mean action: 1.654 [0.000, 3.000],  loss: 8.908732, mae: 63.106708, mean_q: 69.532956, mean_eps: 0.555988
  74129/150000: episode: 763, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -29.781, mean reward: -0.346 [-100.000,  8.952], mean action: 1.640 [0.000, 3.000],  loss: 10.792103, mae: 63.428617, mean_q: 69.874578, mean_eps: 0.555487
  74259/150000: episode: 764, duration: 0.878s, episode steps: 130, steps per second: 148, episode reward: -28.949, mean reward: -0.223 [-100.000, 20.154], mean action: 1.754 [0.000, 3.000],  loss: 27.083781, mae: 63.131484, mean_q: 69.815049, mean_eps: 0.554839
  74368/150000: episode: 765, duration: 0.750s, episode steps: 109, steps per second: 145, episode reward: -12.156, mean reward: -0.112 [-100.000, 17.203], mean action: 1.761 [0.000, 3.000],  loss: 16.285180, mae: 62.697996, mean_q: 69.433669, mean_eps: 0.554122
  74513/150000: episode: 766, duration: 0.963s, episode steps: 145, steps per second: 151, episode reward: -42.137, mean reward: -0.291 [-100.000, 11.938], mean action: 1.552 [0.000, 3.000],  loss: 10.276757, mae: 62.855749, mean_q: 69.472172, mean_eps: 0.553360
  74612/150000: episode: 767, duration: 0.690s, episode steps:  99, steps per second: 144, episode reward: -31.218, mean reward: -0.315 [-100.000,  7.128], mean action: 1.788 [0.000, 3.000],  loss: 15.875802, mae: 62.824389, mean_q: 68.391924, mean_eps: 0.552628
  74728/150000: episode: 768, duration: 0.941s, episode steps: 116, steps per second: 123, episode reward: -287.868, mean reward: -2.482 [-100.000, 30.004], mean action: 1.750 [0.000, 3.000],  loss: 13.452486, mae: 62.912739, mean_q: 67.992104, mean_eps: 0.551983
  74840/150000: episode: 769, duration: 0.785s, episode steps: 112, steps per second: 143, episode reward: -65.754, mean reward: -0.587 [-100.000, 21.277], mean action: 1.732 [0.000, 3.000],  loss: 10.115426, mae: 62.378331, mean_q: 68.555297, mean_eps: 0.551299
  74934/150000: episode: 770, duration: 0.683s, episode steps:  94, steps per second: 138, episode reward: -17.922, mean reward: -0.191 [-100.000, 13.464], mean action: 1.691 [0.000, 3.000],  loss: 11.195645, mae: 63.157736, mean_q: 70.471707, mean_eps: 0.550681
  75034/150000: episode: 771, duration: 0.732s, episode steps: 100, steps per second: 137, episode reward: -22.479, mean reward: -0.225 [-100.000, 19.189], mean action: 1.650 [0.000, 3.000],  loss: 13.072597, mae: 62.793558, mean_q: 68.582814, mean_eps: 0.550099
  75162/150000: episode: 772, duration: 0.868s, episode steps: 128, steps per second: 148, episode reward: 15.471, mean reward:  0.121 [-100.000, 10.317], mean action: 1.688 [0.000, 3.000],  loss: 7.963764, mae: 62.811839, mean_q: 68.906041, mean_eps: 0.549415
  75269/150000: episode: 773, duration: 0.743s, episode steps: 107, steps per second: 144, episode reward: 40.846, mean reward:  0.382 [-100.000, 19.706], mean action: 1.748 [0.000, 3.000],  loss: 15.006980, mae: 63.144496, mean_q: 70.438695, mean_eps: 0.548710
  75414/150000: episode: 774, duration: 0.977s, episode steps: 145, steps per second: 148, episode reward: -78.496, mean reward: -0.541 [-100.000, 29.363], mean action: 1.862 [0.000, 3.000],  loss: 13.296122, mae: 63.413396, mean_q: 70.049777, mean_eps: 0.547954
  75532/150000: episode: 775, duration: 0.816s, episode steps: 118, steps per second: 145, episode reward: -51.773, mean reward: -0.439 [-100.000, 17.922], mean action: 1.593 [0.000, 3.000],  loss: 12.142176, mae: 63.186636, mean_q: 70.373165, mean_eps: 0.547165
  75659/150000: episode: 776, duration: 0.856s, episode steps: 127, steps per second: 148, episode reward: -64.731, mean reward: -0.510 [-100.000, 12.936], mean action: 1.732 [0.000, 3.000],  loss: 10.906310, mae: 63.196686, mean_q: 69.962571, mean_eps: 0.546430
  75759/150000: episode: 777, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -50.502, mean reward: -0.505 [-100.000, 18.064], mean action: 1.810 [0.000, 3.000],  loss: 9.943464, mae: 63.102143, mean_q: 69.290019, mean_eps: 0.545749
  75888/150000: episode: 778, duration: 0.932s, episode steps: 129, steps per second: 138, episode reward: -75.095, mean reward: -0.582 [-100.000, 11.198], mean action: 1.690 [0.000, 3.000],  loss: 10.301004, mae: 63.287789, mean_q: 69.598149, mean_eps: 0.545062
  76021/150000: episode: 779, duration: 0.899s, episode steps: 133, steps per second: 148, episode reward: -80.487, mean reward: -0.605 [-100.000, 22.257], mean action: 1.767 [0.000, 3.000],  loss: 15.200653, mae: 63.220087, mean_q: 70.044167, mean_eps: 0.544276
  76154/150000: episode: 780, duration: 0.932s, episode steps: 133, steps per second: 143, episode reward: -90.364, mean reward: -0.679 [-100.000, 18.244], mean action: 1.632 [0.000, 3.000],  loss: 8.464499, mae: 63.698296, mean_q: 71.180592, mean_eps: 0.543478
  76329/150000: episode: 781, duration: 1.172s, episode steps: 175, steps per second: 149, episode reward: -91.936, mean reward: -0.525 [-100.000, 10.000], mean action: 1.749 [0.000, 3.000],  loss: 10.669973, mae: 63.543397, mean_q: 70.033644, mean_eps: 0.542554
  76433/150000: episode: 782, duration: 0.704s, episode steps: 104, steps per second: 148, episode reward: -99.088, mean reward: -0.953 [-100.000, 20.857], mean action: 1.644 [0.000, 3.000],  loss: 11.069539, mae: 63.626300, mean_q: 71.156056, mean_eps: 0.541717
  76525/150000: episode: 783, duration: 0.663s, episode steps:  92, steps per second: 139, episode reward: -41.454, mean reward: -0.451 [-100.000,  7.355], mean action: 1.946 [0.000, 3.000],  loss: 8.490149, mae: 63.367468, mean_q: 71.252104, mean_eps: 0.541129
  76755/150000: episode: 784, duration: 1.582s, episode steps: 230, steps per second: 145, episode reward: -105.006, mean reward: -0.457 [-100.000, 15.434], mean action: 1.613 [0.000, 3.000],  loss: 11.094035, mae: 63.126021, mean_q: 70.629001, mean_eps: 0.540163
  76856/150000: episode: 785, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: -293.295, mean reward: -2.904 [-100.000, 48.319], mean action: 1.802 [0.000, 3.000],  loss: 14.812623, mae: 64.012112, mean_q: 72.508170, mean_eps: 0.539170
  76959/150000: episode: 786, duration: 0.696s, episode steps: 103, steps per second: 148, episode reward: -130.366, mean reward: -1.266 [-100.000,  7.728], mean action: 1.961 [0.000, 3.000],  loss: 10.005534, mae: 63.673383, mean_q: 72.408011, mean_eps: 0.538558
  77063/150000: episode: 787, duration: 0.714s, episode steps: 104, steps per second: 146, episode reward: 23.639, mean reward:  0.227 [-100.000, 15.078], mean action: 1.827 [0.000, 3.000],  loss: 10.906090, mae: 63.887240, mean_q: 72.079227, mean_eps: 0.537937
  77154/150000: episode: 788, duration: 0.627s, episode steps:  91, steps per second: 145, episode reward: -45.425, mean reward: -0.499 [-100.000,  9.349], mean action: 1.549 [0.000, 3.000],  loss: 13.517393, mae: 64.677033, mean_q: 72.727671, mean_eps: 0.537352
  77240/150000: episode: 789, duration: 0.581s, episode steps:  86, steps per second: 148, episode reward: -19.285, mean reward: -0.224 [-100.000, 13.022], mean action: 1.453 [0.000, 3.000],  loss: 13.461606, mae: 64.512189, mean_q: 73.459829, mean_eps: 0.536821
  77379/150000: episode: 790, duration: 0.978s, episode steps: 139, steps per second: 142, episode reward: -59.008, mean reward: -0.425 [-100.000, 15.888], mean action: 1.755 [0.000, 3.000],  loss: 11.198596, mae: 64.437571, mean_q: 73.076372, mean_eps: 0.536146
  77497/150000: episode: 791, duration: 0.857s, episode steps: 118, steps per second: 138, episode reward: -59.815, mean reward: -0.507 [-100.000, 10.677], mean action: 1.695 [0.000, 3.000],  loss: 12.926122, mae: 64.168410, mean_q: 71.888380, mean_eps: 0.535375
  77594/150000: episode: 792, duration: 0.675s, episode steps:  97, steps per second: 144, episode reward: -15.449, mean reward: -0.159 [-100.000, 15.566], mean action: 1.711 [0.000, 3.000],  loss: 10.801811, mae: 63.641797, mean_q: 72.127953, mean_eps: 0.534730
  77709/150000: episode: 793, duration: 0.803s, episode steps: 115, steps per second: 143, episode reward: -140.304, mean reward: -1.220 [-100.000,  7.994], mean action: 1.643 [0.000, 3.000],  loss: 14.443608, mae: 63.955241, mean_q: 71.825310, mean_eps: 0.534094
  77844/150000: episode: 794, duration: 0.911s, episode steps: 135, steps per second: 148, episode reward: -30.574, mean reward: -0.226 [-100.000, 17.249], mean action: 1.644 [0.000, 3.000],  loss: 9.576170, mae: 64.240737, mean_q: 72.301068, mean_eps: 0.533344
  77934/150000: episode: 795, duration: 0.615s, episode steps:  90, steps per second: 146, episode reward: 13.477, mean reward:  0.150 [-100.000, 17.212], mean action: 1.556 [0.000, 3.000],  loss: 13.535797, mae: 63.658176, mean_q: 71.151391, mean_eps: 0.532669
  78024/150000: episode: 796, duration: 0.630s, episode steps:  90, steps per second: 143, episode reward: -29.650, mean reward: -0.329 [-100.000, 10.281], mean action: 1.811 [0.000, 3.000],  loss: 8.751964, mae: 64.273700, mean_q: 72.663885, mean_eps: 0.532129
  78161/150000: episode: 797, duration: 0.936s, episode steps: 137, steps per second: 146, episode reward: 11.610, mean reward:  0.085 [-100.000, 14.966], mean action: 1.774 [0.000, 3.000],  loss: 10.819812, mae: 65.211845, mean_q: 74.622653, mean_eps: 0.531448
  78308/150000: episode: 798, duration: 1.006s, episode steps: 147, steps per second: 146, episode reward: -7.321, mean reward: -0.050 [-100.000, 15.263], mean action: 1.694 [0.000, 3.000],  loss: 12.552766, mae: 64.955453, mean_q: 73.646694, mean_eps: 0.530596
  78457/150000: episode: 799, duration: 1.015s, episode steps: 149, steps per second: 147, episode reward: -106.701, mean reward: -0.716 [-100.000,  6.627], mean action: 1.705 [0.000, 3.000],  loss: 16.117538, mae: 64.652452, mean_q: 73.165018, mean_eps: 0.529708
  78578/150000: episode: 800, duration: 0.810s, episode steps: 121, steps per second: 149, episode reward: -20.909, mean reward: -0.173 [-100.000, 22.356], mean action: 1.711 [0.000, 3.000],  loss: 11.701366, mae: 64.769880, mean_q: 73.540373, mean_eps: 0.528898
  78719/150000: episode: 801, duration: 0.984s, episode steps: 141, steps per second: 143, episode reward: -40.356, mean reward: -0.286 [-100.000,  9.120], mean action: 1.738 [0.000, 3.000],  loss: 11.892547, mae: 64.513102, mean_q: 72.090355, mean_eps: 0.528112
  78813/150000: episode: 802, duration: 0.633s, episode steps:  94, steps per second: 149, episode reward: -22.783, mean reward: -0.242 [-100.000, 14.095], mean action: 1.628 [0.000, 3.000],  loss: 15.896750, mae: 64.655318, mean_q: 72.354205, mean_eps: 0.527407
  78937/150000: episode: 803, duration: 0.849s, episode steps: 124, steps per second: 146, episode reward: -174.886, mean reward: -1.410 [-100.000,  9.640], mean action: 1.766 [0.000, 3.000],  loss: 11.649571, mae: 64.583377, mean_q: 72.859590, mean_eps: 0.526753
  79024/150000: episode: 804, duration: 0.697s, episode steps:  87, steps per second: 125, episode reward: -32.780, mean reward: -0.377 [-100.000, 16.396], mean action: 1.713 [0.000, 3.000],  loss: 15.224509, mae: 65.792976, mean_q: 74.588444, mean_eps: 0.526120
  79118/150000: episode: 805, duration: 0.757s, episode steps:  94, steps per second: 124, episode reward: -101.137, mean reward: -1.076 [-100.000, 10.504], mean action: 1.489 [0.000, 3.000],  loss: 18.156488, mae: 66.127112, mean_q: 75.403567, mean_eps: 0.525577
  79378/150000: episode: 806, duration: 1.999s, episode steps: 260, steps per second: 130, episode reward: -240.429, mean reward: -0.925 [-100.000, 12.098], mean action: 1.592 [0.000, 3.000],  loss: 14.935435, mae: 65.432030, mean_q: 74.775023, mean_eps: 0.524515
  79507/150000: episode: 807, duration: 0.950s, episode steps: 129, steps per second: 136, episode reward: 35.905, mean reward:  0.278 [-100.000, 65.793], mean action: 1.682 [0.000, 3.000],  loss: 12.806345, mae: 65.237171, mean_q: 74.147428, mean_eps: 0.523348
  79641/150000: episode: 808, duration: 1.000s, episode steps: 134, steps per second: 134, episode reward: -296.445, mean reward: -2.212 [-100.000, 31.843], mean action: 1.396 [0.000, 3.000],  loss: 13.873545, mae: 65.666616, mean_q: 75.658653, mean_eps: 0.522559
  79731/150000: episode: 809, duration: 0.608s, episode steps:  90, steps per second: 148, episode reward: -161.714, mean reward: -1.797 [-100.000, 23.838], mean action: 1.833 [0.000, 3.000],  loss: 12.198064, mae: 65.021363, mean_q: 73.624032, mean_eps: 0.521887
  79816/150000: episode: 810, duration: 0.591s, episode steps:  85, steps per second: 144, episode reward: -53.166, mean reward: -0.625 [-100.000,  9.326], mean action: 1.459 [0.000, 3.000],  loss: 15.820533, mae: 64.742559, mean_q: 73.993265, mean_eps: 0.521362
  79993/150000: episode: 811, duration: 1.197s, episode steps: 177, steps per second: 148, episode reward: -71.477, mean reward: -0.404 [-100.000, 18.502], mean action: 1.802 [0.000, 3.000],  loss: 13.104783, mae: 65.183345, mean_q: 74.715964, mean_eps: 0.520576
  80075/150000: episode: 812, duration: 0.558s, episode steps:  82, steps per second: 147, episode reward: -92.593, mean reward: -1.129 [-100.000,  9.991], mean action: 1.768 [0.000, 3.000],  loss: 17.856235, mae: 64.854110, mean_q: 74.126538, mean_eps: 0.519799
  80198/150000: episode: 813, duration: 0.852s, episode steps: 123, steps per second: 144, episode reward: -102.835, mean reward: -0.836 [-100.000, 14.337], mean action: 1.837 [0.000, 3.000],  loss: 19.860576, mae: 66.191855, mean_q: 76.247449, mean_eps: 0.519184
  80348/150000: episode: 814, duration: 1.004s, episode steps: 150, steps per second: 149, episode reward: -77.345, mean reward: -0.516 [-100.000, 11.997], mean action: 1.660 [0.000, 3.000],  loss: 13.438763, mae: 66.007847, mean_q: 75.406960, mean_eps: 0.518365
  80510/150000: episode: 815, duration: 1.123s, episode steps: 162, steps per second: 144, episode reward: -268.786, mean reward: -1.659 [-100.000, 57.233], mean action: 1.673 [0.000, 3.000],  loss: 12.749928, mae: 65.724913, mean_q: 74.451826, mean_eps: 0.517429
  80628/150000: episode: 816, duration: 0.789s, episode steps: 118, steps per second: 150, episode reward: -26.037, mean reward: -0.221 [-100.000, 20.496], mean action: 1.568 [0.000, 3.000],  loss: 15.113892, mae: 65.020594, mean_q: 74.058567, mean_eps: 0.516589
  80744/150000: episode: 817, duration: 0.802s, episode steps: 116, steps per second: 145, episode reward: -46.876, mean reward: -0.404 [-100.000, 20.954], mean action: 1.664 [0.000, 3.000],  loss: 11.443732, mae: 66.270337, mean_q: 75.930175, mean_eps: 0.515887
  80866/150000: episode: 818, duration: 0.849s, episode steps: 122, steps per second: 144, episode reward: -94.672, mean reward: -0.776 [-100.000, 13.584], mean action: 1.787 [0.000, 3.000],  loss: 16.046688, mae: 65.536075, mean_q: 74.046057, mean_eps: 0.515173
  81007/150000: episode: 819, duration: 0.967s, episode steps: 141, steps per second: 146, episode reward: -73.438, mean reward: -0.521 [-100.000, 16.828], mean action: 1.702 [0.000, 3.000],  loss: 15.902123, mae: 65.524889, mean_q: 73.041750, mean_eps: 0.514384
  81128/150000: episode: 820, duration: 0.853s, episode steps: 121, steps per second: 142, episode reward: -105.303, mean reward: -0.870 [-100.000, 10.066], mean action: 1.785 [0.000, 3.000],  loss: 14.277580, mae: 65.832318, mean_q: 73.666069, mean_eps: 0.513598
  81207/150000: episode: 821, duration: 0.543s, episode steps:  79, steps per second: 146, episode reward: -46.040, mean reward: -0.583 [-100.000,  8.536], mean action: 1.873 [0.000, 3.000],  loss: 10.296723, mae: 65.084220, mean_q: 73.871076, mean_eps: 0.512998
  81309/150000: episode: 822, duration: 0.692s, episode steps: 102, steps per second: 147, episode reward: -17.735, mean reward: -0.174 [-100.000, 17.459], mean action: 1.735 [0.000, 3.000],  loss: 14.970636, mae: 65.861510, mean_q: 74.601740, mean_eps: 0.512455
  81400/150000: episode: 823, duration: 0.646s, episode steps:  91, steps per second: 141, episode reward:  5.486, mean reward:  0.060 [-100.000, 14.210], mean action: 1.670 [0.000, 3.000],  loss: 18.667264, mae: 64.517925, mean_q: 73.594694, mean_eps: 0.511876
  81507/150000: episode: 824, duration: 0.727s, episode steps: 107, steps per second: 147, episode reward: -148.719, mean reward: -1.390 [-100.000,  9.581], mean action: 1.785 [0.000, 3.000],  loss: 13.917128, mae: 65.939521, mean_q: 74.265397, mean_eps: 0.511282
  81625/150000: episode: 825, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -182.772, mean reward: -1.549 [-100.000, 54.969], mean action: 1.669 [0.000, 3.000],  loss: 19.106633, mae: 64.864178, mean_q: 71.862606, mean_eps: 0.510607
  81716/150000: episode: 826, duration: 0.634s, episode steps:  91, steps per second: 144, episode reward: -26.440, mean reward: -0.291 [-100.000, 22.564], mean action: 1.505 [0.000, 3.000],  loss: 16.362031, mae: 65.085614, mean_q: 72.618953, mean_eps: 0.509980
  81791/150000: episode: 827, duration: 0.513s, episode steps:  75, steps per second: 146, episode reward: -97.847, mean reward: -1.305 [-100.000,  7.306], mean action: 1.787 [0.000, 3.000],  loss: 11.621256, mae: 64.799690, mean_q: 72.791117, mean_eps: 0.509482
  81902/150000: episode: 828, duration: 0.786s, episode steps: 111, steps per second: 141, episode reward: -131.028, mean reward: -1.180 [-100.000, 16.669], mean action: 1.847 [0.000, 3.000],  loss: 19.448715, mae: 65.751996, mean_q: 73.729047, mean_eps: 0.508924
  82002/150000: episode: 829, duration: 0.709s, episode steps: 100, steps per second: 141, episode reward: -10.127, mean reward: -0.101 [-100.000, 18.480], mean action: 1.760 [0.000, 3.000],  loss: 13.806837, mae: 65.921305, mean_q: 74.511947, mean_eps: 0.508291
  82819/150000: episode: 830, duration: 6.148s, episode steps: 817, steps per second: 133, episode reward: -361.387, mean reward: -0.442 [-100.000, 19.798], mean action: 1.726 [0.000, 3.000],  loss: 17.728506, mae: 65.354646, mean_q: 72.714377, mean_eps: 0.505540
  82967/150000: episode: 831, duration: 1.013s, episode steps: 148, steps per second: 146, episode reward: -105.935, mean reward: -0.716 [-100.000, 11.060], mean action: 1.689 [0.000, 3.000],  loss: 18.001283, mae: 65.501383, mean_q: 73.792095, mean_eps: 0.502645
  83050/150000: episode: 832, duration: 0.556s, episode steps:  83, steps per second: 149, episode reward: -16.773, mean reward: -0.202 [-100.000, 24.252], mean action: 1.530 [0.000, 3.000],  loss: 16.769276, mae: 65.271337, mean_q: 72.540422, mean_eps: 0.501952
  83136/150000: episode: 833, duration: 0.575s, episode steps:  86, steps per second: 150, episode reward: -8.446, mean reward: -0.098 [-100.000, 15.574], mean action: 1.721 [0.000, 3.000],  loss: 17.979323, mae: 66.514722, mean_q: 75.555216, mean_eps: 0.501445
  83233/150000: episode: 834, duration: 0.673s, episode steps:  97, steps per second: 144, episode reward: -4.487, mean reward: -0.046 [-100.000, 15.422], mean action: 1.701 [0.000, 3.000],  loss: 23.100405, mae: 65.342255, mean_q: 73.399340, mean_eps: 0.500896
  83331/150000: episode: 835, duration: 0.659s, episode steps:  98, steps per second: 149, episode reward: -250.991, mean reward: -2.561 [-100.000,  5.468], mean action: 1.480 [0.000, 3.000],  loss: 20.263577, mae: 65.864355, mean_q: 73.309292, mean_eps: 0.500311
  83416/150000: episode: 836, duration: 0.566s, episode steps:  85, steps per second: 150, episode reward: -48.521, mean reward: -0.571 [-100.000,  6.159], mean action: 1.635 [0.000, 3.000],  loss: 16.825284, mae: 65.669784, mean_q: 74.074861, mean_eps: 0.499762
  83539/150000: episode: 837, duration: 0.856s, episode steps: 123, steps per second: 144, episode reward: 24.600, mean reward:  0.200 [-100.000, 19.655], mean action: 1.707 [0.000, 3.000],  loss: 21.416451, mae: 65.563291, mean_q: 72.857898, mean_eps: 0.499138
  84284/150000: episode: 838, duration: 5.523s, episode steps: 745, steps per second: 135, episode reward: -165.204, mean reward: -0.222 [-100.000, 23.974], mean action: 1.811 [0.000, 3.000],  loss: 17.259292, mae: 65.500938, mean_q: 73.701098, mean_eps: 0.496534
  84516/150000: episode: 839, duration: 1.576s, episode steps: 232, steps per second: 147, episode reward: -178.774, mean reward: -0.771 [-100.000,  5.051], mean action: 1.724 [0.000, 3.000],  loss: 17.276780, mae: 65.692702, mean_q: 74.134356, mean_eps: 0.493603
  84712/150000: episode: 840, duration: 1.332s, episode steps: 196, steps per second: 147, episode reward: -86.848, mean reward: -0.443 [-100.000, 66.877], mean action: 1.750 [0.000, 3.000],  loss: 17.511291, mae: 65.336253, mean_q: 73.245527, mean_eps: 0.492319
  84842/150000: episode: 841, duration: 0.864s, episode steps: 130, steps per second: 150, episode reward: 18.187, mean reward:  0.140 [-100.000, 13.581], mean action: 1.754 [0.000, 3.000],  loss: 17.107573, mae: 66.071507, mean_q: 73.511605, mean_eps: 0.491341
  84958/150000: episode: 842, duration: 0.774s, episode steps: 116, steps per second: 150, episode reward: -32.314, mean reward: -0.279 [-100.000, 10.272], mean action: 1.569 [0.000, 3.000],  loss: 21.257426, mae: 66.109195, mean_q: 74.753575, mean_eps: 0.490603
  85068/150000: episode: 843, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -19.935, mean reward: -0.181 [-100.000, 18.221], mean action: 1.664 [0.000, 3.000],  loss: 18.577291, mae: 66.092630, mean_q: 74.320482, mean_eps: 0.489925
  85170/150000: episode: 844, duration: 0.692s, episode steps: 102, steps per second: 147, episode reward: -47.911, mean reward: -0.470 [-100.000, 15.279], mean action: 1.725 [0.000, 3.000],  loss: 26.795104, mae: 65.419837, mean_q: 73.443905, mean_eps: 0.489289
  85290/150000: episode: 845, duration: 0.845s, episode steps: 120, steps per second: 142, episode reward: -17.724, mean reward: -0.148 [-100.000, 23.751], mean action: 1.775 [0.000, 3.000],  loss: 17.239227, mae: 65.812858, mean_q: 73.405506, mean_eps: 0.488623
  85383/150000: episode: 846, duration: 0.671s, episode steps:  93, steps per second: 139, episode reward: -25.343, mean reward: -0.273 [-100.000, 17.917], mean action: 1.871 [0.000, 3.000],  loss: 23.477371, mae: 65.486829, mean_q: 72.499618, mean_eps: 0.487984
  85492/150000: episode: 847, duration: 0.742s, episode steps: 109, steps per second: 147, episode reward: -32.761, mean reward: -0.301 [-100.000, 13.655], mean action: 1.697 [0.000, 3.000],  loss: 19.699628, mae: 65.787789, mean_q: 73.932376, mean_eps: 0.487378
  85682/150000: episode: 848, duration: 1.307s, episode steps: 190, steps per second: 145, episode reward:  4.961, mean reward:  0.026 [-100.000, 10.413], mean action: 1.663 [0.000, 3.000],  loss: 19.095633, mae: 65.500965, mean_q: 73.709436, mean_eps: 0.486481
  85823/150000: episode: 849, duration: 0.952s, episode steps: 141, steps per second: 148, episode reward: 15.846, mean reward:  0.112 [-100.000, 24.045], mean action: 1.667 [0.000, 3.000],  loss: 14.386839, mae: 66.202132, mean_q: 74.764813, mean_eps: 0.485488
  85951/150000: episode: 850, duration: 0.872s, episode steps: 128, steps per second: 147, episode reward:  0.657, mean reward:  0.005 [-100.000, 19.383], mean action: 1.727 [0.000, 3.000],  loss: 17.063961, mae: 65.465912, mean_q: 74.255507, mean_eps: 0.484681
  86059/150000: episode: 851, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -17.529, mean reward: -0.162 [-100.000,  9.549], mean action: 1.713 [0.000, 3.000],  loss: 14.676940, mae: 66.924440, mean_q: 75.486959, mean_eps: 0.483973
  86198/150000: episode: 852, duration: 0.935s, episode steps: 139, steps per second: 149, episode reward: -45.803, mean reward: -0.330 [-100.000, 11.411], mean action: 1.741 [0.000, 3.000],  loss: 16.972078, mae: 64.929213, mean_q: 73.539447, mean_eps: 0.483232
  86348/150000: episode: 853, duration: 1.082s, episode steps: 150, steps per second: 139, episode reward: -55.074, mean reward: -0.367 [-100.000, 10.607], mean action: 1.780 [0.000, 3.000],  loss: 12.606181, mae: 65.994756, mean_q: 74.893687, mean_eps: 0.482365
  86825/150000: episode: 854, duration: 3.409s, episode steps: 477, steps per second: 140, episode reward: -199.369, mean reward: -0.418 [-100.000, 18.407], mean action: 1.767 [0.000, 3.000],  loss: 13.312681, mae: 65.897469, mean_q: 74.631311, mean_eps: 0.480484
  86955/150000: episode: 855, duration: 0.895s, episode steps: 130, steps per second: 145, episode reward: -48.250, mean reward: -0.371 [-100.000, 12.456], mean action: 1.792 [0.000, 3.000],  loss: 12.327791, mae: 65.910374, mean_q: 75.687473, mean_eps: 0.478663
  87035/150000: episode: 856, duration: 0.550s, episode steps:  80, steps per second: 145, episode reward: -66.159, mean reward: -0.827 [-100.000, 21.495], mean action: 1.525 [0.000, 3.000],  loss: 22.815898, mae: 66.185702, mean_q: 76.333296, mean_eps: 0.478033
  88035/150000: episode: 857, duration: 7.776s, episode steps: 1000, steps per second: 129, episode reward: -50.253, mean reward: -0.050 [-22.051, 22.854], mean action: 1.716 [0.000, 3.000],  loss: 17.961302, mae: 65.495137, mean_q: 74.497607, mean_eps: 0.474793
  88117/150000: episode: 858, duration: 0.556s, episode steps:  82, steps per second: 148, episode reward: -60.286, mean reward: -0.735 [-100.000,  8.435], mean action: 1.744 [0.000, 3.000],  loss: 21.212589, mae: 65.283610, mean_q: 75.293062, mean_eps: 0.471547
  88279/150000: episode: 859, duration: 1.757s, episode steps: 162, steps per second:  92, episode reward: -19.770, mean reward: -0.122 [-100.000, 16.696], mean action: 1.765 [0.000, 3.000],  loss: 16.454177, mae: 65.340984, mean_q: 75.657592, mean_eps: 0.470815
  88653/150000: episode: 860, duration: 3.082s, episode steps: 374, steps per second: 121, episode reward: -137.370, mean reward: -0.367 [-100.000, 15.518], mean action: 1.695 [0.000, 3.000],  loss: 16.766917, mae: 64.991260, mean_q: 74.670539, mean_eps: 0.469207
  88798/150000: episode: 861, duration: 1.109s, episode steps: 145, steps per second: 131, episode reward: -52.947, mean reward: -0.365 [-100.000, 17.995], mean action: 1.586 [0.000, 3.000],  loss: 17.760985, mae: 65.103737, mean_q: 75.338856, mean_eps: 0.467650
  88907/150000: episode: 862, duration: 0.750s, episode steps: 109, steps per second: 145, episode reward: -12.396, mean reward: -0.114 [-100.000,  9.899], mean action: 1.615 [0.000, 3.000],  loss: 19.396078, mae: 65.416803, mean_q: 75.437323, mean_eps: 0.466888
  88994/150000: episode: 863, duration: 0.579s, episode steps:  87, steps per second: 150, episode reward: -38.076, mean reward: -0.438 [-100.000,  6.659], mean action: 1.586 [0.000, 3.000],  loss: 15.370602, mae: 64.888042, mean_q: 75.084707, mean_eps: 0.466300
  89994/150000: episode: 864, duration: 7.847s, episode steps: 1000, steps per second: 127, episode reward: -16.381, mean reward: -0.016 [-23.696, 21.939], mean action: 1.733 [0.000, 3.000],  loss: 18.889344, mae: 64.854094, mean_q: 74.925629, mean_eps: 0.463039
  90110/150000: episode: 865, duration: 0.782s, episode steps: 116, steps per second: 148, episode reward: -18.608, mean reward: -0.160 [-100.000, 20.851], mean action: 1.586 [0.000, 3.000],  loss: 17.483386, mae: 64.103073, mean_q: 73.986395, mean_eps: 0.459691
  90246/150000: episode: 866, duration: 0.942s, episode steps: 136, steps per second: 144, episode reward: -48.243, mean reward: -0.355 [-100.000,  8.581], mean action: 1.684 [0.000, 3.000],  loss: 18.656985, mae: 64.698692, mean_q: 75.054518, mean_eps: 0.458935
  90392/150000: episode: 867, duration: 1.045s, episode steps: 146, steps per second: 140, episode reward: -59.264, mean reward: -0.406 [-100.000, 10.812], mean action: 1.815 [0.000, 3.000],  loss: 16.727524, mae: 64.752560, mean_q: 75.030676, mean_eps: 0.458089
  91392/150000: episode: 868, duration: 8.237s, episode steps: 1000, steps per second: 121, episode reward: 30.139, mean reward:  0.030 [-22.241, 25.292], mean action: 1.769 [0.000, 3.000],  loss: 17.319792, mae: 64.763191, mean_q: 75.375390, mean_eps: 0.454651
  91472/150000: episode: 869, duration: 0.542s, episode steps:  80, steps per second: 148, episode reward: -5.124, mean reward: -0.064 [-100.000,  9.141], mean action: 1.738 [0.000, 3.000],  loss: 19.339048, mae: 64.706081, mean_q: 76.077476, mean_eps: 0.451411
  92472/150000: episode: 870, duration: 8.016s, episode steps: 1000, steps per second: 125, episode reward: 21.570, mean reward:  0.022 [-22.861, 24.536], mean action: 1.880 [0.000, 3.000],  loss: 18.262653, mae: 64.841730, mean_q: 76.109393, mean_eps: 0.448171
  92571/150000: episode: 871, duration: 0.661s, episode steps:  99, steps per second: 150, episode reward:  0.642, mean reward:  0.006 [-100.000, 11.954], mean action: 1.737 [0.000, 3.000],  loss: 19.543738, mae: 64.606280, mean_q: 76.445385, mean_eps: 0.444874
  92694/150000: episode: 872, duration: 0.834s, episode steps: 123, steps per second: 147, episode reward: -14.813, mean reward: -0.120 [-100.000, 17.059], mean action: 1.789 [0.000, 3.000],  loss: 18.796065, mae: 64.766428, mean_q: 75.578160, mean_eps: 0.444208
  93694/150000: episode: 873, duration: 8.972s, episode steps: 1000, steps per second: 111, episode reward: 12.982, mean reward:  0.013 [-24.320, 24.339], mean action: 1.748 [0.000, 3.000],  loss: 18.101855, mae: 64.088940, mean_q: 75.636163, mean_eps: 0.440839
  94291/150000: episode: 874, duration: 4.670s, episode steps: 597, steps per second: 128, episode reward: -175.411, mean reward: -0.294 [-100.000, 17.869], mean action: 1.486 [0.000, 3.000],  loss: 17.511053, mae: 63.592210, mean_q: 75.173476, mean_eps: 0.436048
  94384/150000: episode: 875, duration: 0.624s, episode steps:  93, steps per second: 149, episode reward: -2.957, mean reward: -0.032 [-100.000, 34.256], mean action: 1.688 [0.000, 3.000],  loss: 20.532849, mae: 63.236570, mean_q: 74.478593, mean_eps: 0.433978
  94514/150000: episode: 876, duration: 0.895s, episode steps: 130, steps per second: 145, episode reward: 32.013, mean reward:  0.246 [-100.000, 15.928], mean action: 1.777 [0.000, 3.000],  loss: 16.267380, mae: 63.520215, mean_q: 74.508851, mean_eps: 0.433309
  94635/150000: episode: 877, duration: 0.813s, episode steps: 121, steps per second: 149, episode reward: -69.648, mean reward: -0.576 [-100.000,  8.995], mean action: 1.702 [0.000, 3.000],  loss: 14.547172, mae: 63.512276, mean_q: 75.447087, mean_eps: 0.432556
  94746/150000: episode: 878, duration: 0.768s, episode steps: 111, steps per second: 145, episode reward: 13.828, mean reward:  0.125 [-100.000, 13.540], mean action: 1.495 [0.000, 3.000],  loss: 18.244703, mae: 63.496603, mean_q: 75.007277, mean_eps: 0.431860
  94896/150000: episode: 879, duration: 1.052s, episode steps: 150, steps per second: 143, episode reward:  6.458, mean reward:  0.043 [-100.000,  9.891], mean action: 1.873 [0.000, 3.000],  loss: 14.502997, mae: 63.497691, mean_q: 75.434525, mean_eps: 0.431077
  95029/150000: episode: 880, duration: 0.898s, episode steps: 133, steps per second: 148, episode reward: 19.122, mean reward:  0.144 [-100.000, 18.027], mean action: 1.579 [0.000, 3.000],  loss: 13.374007, mae: 62.962826, mean_q: 74.591548, mean_eps: 0.430228
  95122/150000: episode: 881, duration: 0.645s, episode steps:  93, steps per second: 144, episode reward: -13.646, mean reward: -0.147 [-100.000, 18.354], mean action: 1.527 [0.000, 3.000],  loss: 17.956698, mae: 63.194954, mean_q: 75.501463, mean_eps: 0.429550
  96122/150000: episode: 882, duration: 7.756s, episode steps: 1000, steps per second: 129, episode reward: 92.305, mean reward:  0.092 [-24.426, 24.567], mean action: 1.497 [0.000, 3.000],  loss: 17.874188, mae: 62.917143, mean_q: 74.574444, mean_eps: 0.426271
  96255/150000: episode: 883, duration: 0.910s, episode steps: 133, steps per second: 146, episode reward:  7.007, mean reward:  0.053 [-100.000, 18.252], mean action: 1.910 [0.000, 3.000],  loss: 17.277362, mae: 63.422130, mean_q: 75.708291, mean_eps: 0.422872
  96414/150000: episode: 884, duration: 1.067s, episode steps: 159, steps per second: 149, episode reward:  0.048, mean reward:  0.000 [-100.000, 21.272], mean action: 1.560 [0.000, 3.000],  loss: 15.179031, mae: 62.714769, mean_q: 74.451614, mean_eps: 0.421996
  96515/150000: episode: 885, duration: 0.684s, episode steps: 101, steps per second: 148, episode reward: 12.094, mean reward:  0.120 [-100.000,  9.799], mean action: 1.950 [0.000, 3.000],  loss: 16.903888, mae: 62.430541, mean_q: 74.822785, mean_eps: 0.421216
  96639/150000: episode: 886, duration: 1.020s, episode steps: 124, steps per second: 122, episode reward: -67.746, mean reward: -0.546 [-100.000, 12.321], mean action: 1.661 [0.000, 3.000],  loss: 19.563170, mae: 63.077801, mean_q: 74.775157, mean_eps: 0.420541
  96817/150000: episode: 887, duration: 1.412s, episode steps: 178, steps per second: 126, episode reward: -50.480, mean reward: -0.284 [-100.000, 20.319], mean action: 1.747 [0.000, 3.000],  loss: 14.212351, mae: 62.764253, mean_q: 74.196074, mean_eps: 0.419635
  97817/150000: episode: 888, duration: 8.285s, episode steps: 1000, steps per second: 121, episode reward: 102.538, mean reward:  0.103 [-24.178, 28.950], mean action: 1.517 [0.000, 3.000],  loss: 17.445152, mae: 63.346383, mean_q: 76.246136, mean_eps: 0.416101
  98817/150000: episode: 889, duration: 7.868s, episode steps: 1000, steps per second: 127, episode reward: 60.874, mean reward:  0.061 [-23.129, 23.895], mean action: 1.385 [0.000, 3.000],  loss: 18.215275, mae: 62.905023, mean_q: 75.561826, mean_eps: 0.410101
  99817/150000: episode: 890, duration: 7.812s, episode steps: 1000, steps per second: 128, episode reward: 97.763, mean reward:  0.098 [-20.744, 22.756], mean action: 1.442 [0.000, 3.000],  loss: 15.983599, mae: 62.732651, mean_q: 75.498340, mean_eps: 0.404101
  99891/150000: episode: 891, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward:  0.646, mean reward:  0.009 [-100.000, 22.481], mean action: 1.676 [0.000, 3.000],  loss: 13.774930, mae: 62.509709, mean_q: 75.320304, mean_eps: 0.400879
 100039/150000: episode: 892, duration: 0.999s, episode steps: 148, steps per second: 148, episode reward: -82.114, mean reward: -0.555 [-100.000, 28.655], mean action: 1.885 [0.000, 3.000],  loss: 15.123819, mae: 62.309995, mean_q: 75.407530, mean_eps: 0.400213
 100162/150000: episode: 893, duration: 0.858s, episode steps: 123, steps per second: 143, episode reward: 16.740, mean reward:  0.136 [-100.000, 17.747], mean action: 1.813 [0.000, 3.000],  loss: 19.136880, mae: 62.685413, mean_q: 75.747185, mean_eps: 0.399400
 101162/150000: episode: 894, duration: 7.514s, episode steps: 1000, steps per second: 133, episode reward: 105.957, mean reward:  0.106 [-24.205, 26.572], mean action: 1.509 [0.000, 3.000],  loss: 16.102717, mae: 62.158648, mean_q: 74.864177, mean_eps: 0.396031
 101288/150000: episode: 895, duration: 0.878s, episode steps: 126, steps per second: 143, episode reward: 34.990, mean reward:  0.278 [-100.000, 24.210], mean action: 1.802 [0.000, 3.000],  loss: 15.373033, mae: 61.536667, mean_q: 74.225113, mean_eps: 0.392653
 101639/150000: episode: 896, duration: 2.430s, episode steps: 351, steps per second: 144, episode reward: -144.998, mean reward: -0.413 [-100.000, 10.698], mean action: 1.764 [0.000, 3.000],  loss: 14.070807, mae: 61.753069, mean_q: 74.628170, mean_eps: 0.391222
 101734/150000: episode: 897, duration: 0.634s, episode steps:  95, steps per second: 150, episode reward: -18.252, mean reward: -0.192 [-100.000, 19.037], mean action: 1.716 [0.000, 3.000],  loss: 19.207085, mae: 61.666968, mean_q: 74.060146, mean_eps: 0.389884
 102147/150000: episode: 898, duration: 2.870s, episode steps: 413, steps per second: 144, episode reward: -64.035, mean reward: -0.155 [-100.000, 19.449], mean action: 1.697 [0.000, 3.000],  loss: 15.788829, mae: 61.823121, mean_q: 75.074708, mean_eps: 0.388360
 103147/150000: episode: 899, duration: 7.318s, episode steps: 1000, steps per second: 137, episode reward: 152.454, mean reward:  0.152 [-21.975, 23.399], mean action: 1.409 [0.000, 3.000],  loss: 17.388677, mae: 61.781318, mean_q: 75.221862, mean_eps: 0.384121
 104147/150000: episode: 900, duration: 7.513s, episode steps: 1000, steps per second: 133, episode reward: 77.955, mean reward:  0.078 [-20.965, 19.715], mean action: 1.564 [0.000, 3.000],  loss: 17.810887, mae: 61.522442, mean_q: 75.224339, mean_eps: 0.378121
 104249/150000: episode: 901, duration: 0.720s, episode steps: 102, steps per second: 142, episode reward: -0.671, mean reward: -0.007 [-100.000,  9.497], mean action: 1.784 [0.000, 3.000],  loss: 18.473387, mae: 61.652173, mean_q: 76.526494, mean_eps: 0.374815
 105249/150000: episode: 902, duration: 8.304s, episode steps: 1000, steps per second: 120, episode reward: 70.255, mean reward:  0.070 [-23.251, 22.935], mean action: 1.524 [0.000, 3.000],  loss: 19.200349, mae: 61.321271, mean_q: 75.630994, mean_eps: 0.371509
 106249/150000: episode: 903, duration: 8.484s, episode steps: 1000, steps per second: 118, episode reward: 130.706, mean reward:  0.131 [-24.195, 23.609], mean action: 1.582 [0.000, 3.000],  loss: 18.769483, mae: 60.603776, mean_q: 74.999154, mean_eps: 0.365509
 107083/150000: episode: 904, duration: 6.345s, episode steps: 834, steps per second: 131, episode reward: -250.848, mean reward: -0.301 [-100.000, 21.550], mean action: 1.641 [0.000, 3.000],  loss: 14.969976, mae: 60.471134, mean_q: 75.114558, mean_eps: 0.360007
 107194/150000: episode: 905, duration: 0.794s, episode steps: 111, steps per second: 140, episode reward: 33.787, mean reward:  0.304 [-100.000, 19.422], mean action: 1.577 [0.000, 3.000],  loss: 14.602602, mae: 60.412143, mean_q: 75.510892, mean_eps: 0.357172
 108194/150000: episode: 906, duration: 7.880s, episode steps: 1000, steps per second: 127, episode reward: 89.363, mean reward:  0.089 [-20.494, 23.023], mean action: 1.574 [0.000, 3.000],  loss: 15.658144, mae: 60.526254, mean_q: 75.637320, mean_eps: 0.353839
 108301/150000: episode: 907, duration: 0.775s, episode steps: 107, steps per second: 138, episode reward: -8.751, mean reward: -0.082 [-100.000, 14.321], mean action: 1.533 [0.000, 3.000],  loss: 14.760223, mae: 60.448399, mean_q: 75.671843, mean_eps: 0.350518
 108396/150000: episode: 908, duration: 0.653s, episode steps:  95, steps per second: 145, episode reward: -38.552, mean reward: -0.406 [-100.000,  8.936], mean action: 1.589 [0.000, 3.000],  loss: 14.781409, mae: 59.758728, mean_q: 74.741852, mean_eps: 0.349912
 109396/150000: episode: 909, duration: 7.544s, episode steps: 1000, steps per second: 133, episode reward: 97.263, mean reward:  0.097 [-21.096, 24.145], mean action: 1.576 [0.000, 3.000],  loss: 15.622664, mae: 60.724744, mean_q: 76.288294, mean_eps: 0.346627
 110396/150000: episode: 910, duration: 7.787s, episode steps: 1000, steps per second: 128, episode reward: 127.185, mean reward:  0.127 [-23.152, 26.816], mean action: 1.481 [0.000, 3.000],  loss: 15.415714, mae: 61.003241, mean_q: 76.991678, mean_eps: 0.340627
 111396/150000: episode: 911, duration: 7.753s, episode steps: 1000, steps per second: 129, episode reward: 26.905, mean reward:  0.027 [-21.491, 23.586], mean action: 1.628 [0.000, 3.000],  loss: 14.412016, mae: 61.212057, mean_q: 77.394870, mean_eps: 0.334627
 112396/150000: episode: 912, duration: 7.637s, episode steps: 1000, steps per second: 131, episode reward: 144.701, mean reward:  0.145 [-24.464, 23.449], mean action: 1.431 [0.000, 3.000],  loss: 15.749651, mae: 61.621014, mean_q: 77.781701, mean_eps: 0.328627
 112522/150000: episode: 913, duration: 0.901s, episode steps: 126, steps per second: 140, episode reward: -9.785, mean reward: -0.078 [-100.000, 18.420], mean action: 1.952 [0.000, 3.000],  loss: 13.163659, mae: 61.604442, mean_q: 78.126353, mean_eps: 0.325249
 112869/150000: episode: 914, duration: 2.473s, episode steps: 347, steps per second: 140, episode reward: -26.403, mean reward: -0.076 [-100.000,  4.610], mean action: 1.804 [0.000, 3.000],  loss: 13.991721, mae: 61.034595, mean_q: 77.499842, mean_eps: 0.323830
 113019/150000: episode: 915, duration: 1.070s, episode steps: 150, steps per second: 140, episode reward: 62.622, mean reward:  0.417 [-100.000, 19.112], mean action: 1.840 [0.000, 3.000],  loss: 19.447328, mae: 61.644433, mean_q: 78.103598, mean_eps: 0.322339
 114019/150000: episode: 916, duration: 7.904s, episode steps: 1000, steps per second: 127, episode reward: 134.027, mean reward:  0.134 [-20.697, 23.118], mean action: 1.287 [0.000, 3.000],  loss: 14.735920, mae: 61.190073, mean_q: 77.971759, mean_eps: 0.318889
 114167/150000: episode: 917, duration: 1.182s, episode steps: 148, steps per second: 125, episode reward: -175.163, mean reward: -1.184 [-100.000,  1.481], mean action: 1.473 [0.000, 3.000],  loss: 18.157244, mae: 61.152298, mean_q: 77.422985, mean_eps: 0.315445
 114759/150000: episode: 918, duration: 4.485s, episode steps: 592, steps per second: 132, episode reward: 233.597, mean reward:  0.395 [-20.363, 100.000], mean action: 1.370 [0.000, 3.000],  loss: 12.661250, mae: 61.387662, mean_q: 78.032657, mean_eps: 0.313225
 115759/150000: episode: 919, duration: 7.724s, episode steps: 1000, steps per second: 129, episode reward: 116.521, mean reward:  0.117 [-21.295, 21.678], mean action: 1.328 [0.000, 3.000],  loss: 16.350889, mae: 61.092513, mean_q: 77.602606, mean_eps: 0.308449
 116759/150000: episode: 920, duration: 7.829s, episode steps: 1000, steps per second: 128, episode reward: 14.949, mean reward:  0.015 [-21.324, 23.947], mean action: 1.479 [0.000, 3.000],  loss: 16.112582, mae: 61.144556, mean_q: 78.145781, mean_eps: 0.302449
 117332/150000: episode: 921, duration: 4.290s, episode steps: 573, steps per second: 134, episode reward: -251.689, mean reward: -0.439 [-100.000, 14.582], mean action: 1.742 [0.000, 3.000],  loss: 19.458195, mae: 61.511541, mean_q: 78.908357, mean_eps: 0.297730
 118332/150000: episode: 922, duration: 7.806s, episode steps: 1000, steps per second: 128, episode reward: 137.014, mean reward:  0.137 [-24.108, 24.505], mean action: 1.532 [0.000, 3.000],  loss: 15.497703, mae: 61.516337, mean_q: 79.237378, mean_eps: 0.293011
 118501/150000: episode: 923, duration: 1.198s, episode steps: 169, steps per second: 141, episode reward: 54.183, mean reward:  0.321 [-100.000, 14.362], mean action: 1.751 [0.000, 3.000],  loss: 11.539056, mae: 61.362450, mean_q: 79.489664, mean_eps: 0.289504
 119068/150000: episode: 924, duration: 4.208s, episode steps: 567, steps per second: 135, episode reward: 190.189, mean reward:  0.335 [-20.190, 100.000], mean action: 1.228 [0.000, 3.000],  loss: 14.483510, mae: 61.436018, mean_q: 79.130788, mean_eps: 0.287296
 119702/150000: episode: 925, duration: 4.552s, episode steps: 634, steps per second: 139, episode reward: -141.993, mean reward: -0.224 [-100.000, 17.031], mean action: 1.558 [0.000, 3.000],  loss: 13.617125, mae: 61.469778, mean_q: 79.284646, mean_eps: 0.283693
 120702/150000: episode: 926, duration: 7.768s, episode steps: 1000, steps per second: 129, episode reward: 11.973, mean reward:  0.012 [-23.809, 22.232], mean action: 1.609 [0.000, 3.000],  loss: 20.715766, mae: 61.804560, mean_q: 79.737647, mean_eps: 0.278791
 121702/150000: episode: 927, duration: 8.179s, episode steps: 1000, steps per second: 122, episode reward: -50.617, mean reward: -0.051 [-21.542, 20.567], mean action: 1.613 [0.000, 3.000],  loss: 14.200267, mae: 61.477405, mean_q: 79.448781, mean_eps: 0.272791
 122529/150000: episode: 928, duration: 6.410s, episode steps: 827, steps per second: 129, episode reward: 218.393, mean reward:  0.264 [-20.724, 100.000], mean action: 1.354 [0.000, 3.000],  loss: 14.783511, mae: 60.650630, mean_q: 78.434850, mean_eps: 0.267310
 122705/150000: episode: 929, duration: 1.306s, episode steps: 176, steps per second: 135, episode reward: -186.944, mean reward: -1.062 [-100.000, 15.536], mean action: 1.676 [0.000, 3.000],  loss: 14.231323, mae: 60.038417, mean_q: 77.618257, mean_eps: 0.264301
 123705/150000: episode: 930, duration: 7.653s, episode steps: 1000, steps per second: 131, episode reward: 126.106, mean reward:  0.126 [-22.988, 23.647], mean action: 1.792 [0.000, 3.000],  loss: 16.203809, mae: 60.272387, mean_q: 78.196584, mean_eps: 0.260773
 124705/150000: episode: 931, duration: 7.685s, episode steps: 1000, steps per second: 130, episode reward: 43.160, mean reward:  0.043 [-22.774, 21.454], mean action: 1.090 [0.000, 3.000],  loss: 13.773677, mae: 60.042378, mean_q: 78.359566, mean_eps: 0.254773
 125705/150000: episode: 932, duration: 7.730s, episode steps: 1000, steps per second: 129, episode reward: 141.210, mean reward:  0.141 [-21.485, 23.167], mean action: 1.295 [0.000, 3.000],  loss: 14.852823, mae: 58.885457, mean_q: 76.774384, mean_eps: 0.248773
 125867/150000: episode: 933, duration: 1.128s, episode steps: 162, steps per second: 144, episode reward: -119.035, mean reward: -0.735 [-100.000,  3.622], mean action: 1.852 [0.000, 3.000],  loss: 16.537400, mae: 58.585320, mean_q: 76.241061, mean_eps: 0.245287
 126867/150000: episode: 934, duration: 7.499s, episode steps: 1000, steps per second: 133, episode reward: 65.542, mean reward:  0.066 [-19.246, 22.684], mean action: 1.316 [0.000, 3.000],  loss: 13.888192, mae: 57.992544, mean_q: 75.787163, mean_eps: 0.241801
 127867/150000: episode: 935, duration: 8.069s, episode steps: 1000, steps per second: 124, episode reward: -40.858, mean reward: -0.041 [-22.804, 20.383], mean action: 1.645 [0.000, 3.000],  loss: 13.139604, mae: 57.549319, mean_q: 75.432515, mean_eps: 0.235801
 128582/150000: episode: 936, duration: 5.420s, episode steps: 715, steps per second: 132, episode reward: -167.778, mean reward: -0.235 [-100.000, 16.077], mean action: 1.550 [0.000, 3.000],  loss: 12.059786, mae: 57.548967, mean_q: 75.609685, mean_eps: 0.230656
 129043/150000: episode: 937, duration: 3.436s, episode steps: 461, steps per second: 134, episode reward: -346.981, mean reward: -0.753 [-100.000, 12.412], mean action: 1.727 [0.000, 3.000],  loss: 12.494719, mae: 57.419894, mean_q: 75.545771, mean_eps: 0.227128
 129426/150000: episode: 938, duration: 2.753s, episode steps: 383, steps per second: 139, episode reward: -260.002, mean reward: -0.679 [-100.000, 16.232], mean action: 1.773 [0.000, 3.000],  loss: 12.711694, mae: 57.441501, mean_q: 75.445605, mean_eps: 0.224596
 130426/150000: episode: 939, duration: 7.785s, episode steps: 1000, steps per second: 128, episode reward: 14.107, mean reward:  0.014 [-24.519, 23.532], mean action: 1.550 [0.000, 3.000],  loss: 14.062284, mae: 57.065250, mean_q: 75.434677, mean_eps: 0.220447
 130907/150000: episode: 940, duration: 3.639s, episode steps: 481, steps per second: 132, episode reward: -209.632, mean reward: -0.436 [-100.000, 21.684], mean action: 1.345 [0.000, 3.000],  loss: 11.602550, mae: 56.184313, mean_q: 74.204014, mean_eps: 0.216004
 131907/150000: episode: 941, duration: 8.038s, episode steps: 1000, steps per second: 124, episode reward: 147.717, mean reward:  0.148 [-21.025, 22.254], mean action: 2.175 [0.000, 3.000],  loss: 13.808229, mae: 55.692840, mean_q: 73.938190, mean_eps: 0.211561
 132907/150000: episode: 942, duration: 7.462s, episode steps: 1000, steps per second: 134, episode reward: 131.164, mean reward:  0.131 [-20.227, 21.921], mean action: 1.287 [0.000, 3.000],  loss: 14.796936, mae: 54.916385, mean_q: 72.885289, mean_eps: 0.205561
 133907/150000: episode: 943, duration: 7.296s, episode steps: 1000, steps per second: 137, episode reward: 151.037, mean reward:  0.151 [-21.112, 23.128], mean action: 0.988 [0.000, 3.000],  loss: 13.299177, mae: 54.162461, mean_q: 71.918109, mean_eps: 0.199561
 134259/150000: episode: 944, duration: 2.581s, episode steps: 352, steps per second: 136, episode reward: 257.012, mean reward:  0.730 [-11.349, 100.000], mean action: 1.469 [0.000, 3.000],  loss: 12.870124, mae: 53.947361, mean_q: 71.814957, mean_eps: 0.195505
 134555/150000: episode: 945, duration: 2.094s, episode steps: 296, steps per second: 141, episode reward: 244.630, mean reward:  0.826 [-20.399, 100.000], mean action: 1.274 [0.000, 3.000],  loss: 13.851721, mae: 54.182726, mean_q: 72.130620, mean_eps: 0.193561
 135484/150000: episode: 946, duration: 7.117s, episode steps: 929, steps per second: 131, episode reward: 251.023, mean reward:  0.270 [-21.691, 100.000], mean action: 1.022 [0.000, 3.000],  loss: 11.344747, mae: 53.870323, mean_q: 71.743917, mean_eps: 0.189886
 136484/150000: episode: 947, duration: 7.497s, episode steps: 1000, steps per second: 133, episode reward: 117.675, mean reward:  0.118 [-22.634, 22.563], mean action: 1.115 [0.000, 3.000],  loss: 11.100771, mae: 53.120538, mean_q: 70.708389, mean_eps: 0.184099
 137197/150000: episode: 948, duration: 5.316s, episode steps: 713, steps per second: 134, episode reward: 208.946, mean reward:  0.293 [-20.618, 100.000], mean action: 1.696 [0.000, 3.000],  loss: 10.547973, mae: 53.192158, mean_q: 70.966400, mean_eps: 0.178960
 137524/150000: episode: 949, duration: 2.389s, episode steps: 327, steps per second: 137, episode reward: -16.499, mean reward: -0.050 [-100.000, 11.403], mean action: 1.670 [0.000, 3.000],  loss: 11.763891, mae: 54.069673, mean_q: 72.163686, mean_eps: 0.175840
 138156/150000: episode: 950, duration: 4.810s, episode steps: 632, steps per second: 131, episode reward: 220.256, mean reward:  0.349 [-19.771, 100.000], mean action: 1.231 [0.000, 3.000],  loss: 9.265086, mae: 53.688847, mean_q: 71.749275, mean_eps: 0.172963
 138477/150000: episode: 951, duration: 2.350s, episode steps: 321, steps per second: 137, episode reward: 183.870, mean reward:  0.573 [-22.496, 100.000], mean action: 1.237 [0.000, 3.000],  loss: 12.588822, mae: 53.671656, mean_q: 71.618186, mean_eps: 0.170104
 138708/150000: episode: 952, duration: 1.631s, episode steps: 231, steps per second: 142, episode reward: 231.696, mean reward:  1.003 [-19.029, 100.000], mean action: 1.407 [0.000, 3.000],  loss: 10.203281, mae: 53.726674, mean_q: 71.579755, mean_eps: 0.168448
 139708/150000: episode: 953, duration: 7.806s, episode steps: 1000, steps per second: 128, episode reward: 149.424, mean reward:  0.149 [-24.531, 33.917], mean action: 1.479 [0.000, 3.000],  loss: 10.036777, mae: 53.536607, mean_q: 71.399287, mean_eps: 0.164755
 140620/150000: episode: 954, duration: 7.040s, episode steps: 912, steps per second: 130, episode reward: 253.514, mean reward:  0.278 [-20.403, 100.000], mean action: 0.899 [0.000, 3.000],  loss: 8.152721, mae: 53.284719, mean_q: 71.284092, mean_eps: 0.159019
 141368/150000: episode: 955, duration: 5.650s, episode steps: 748, steps per second: 132, episode reward: 222.845, mean reward:  0.298 [-21.851, 100.000], mean action: 1.120 [0.000, 3.000],  loss: 9.177407, mae: 52.930637, mean_q: 70.780205, mean_eps: 0.154039
 141651/150000: episode: 956, duration: 2.040s, episode steps: 283, steps per second: 139, episode reward: 234.904, mean reward:  0.830 [-17.363, 100.000], mean action: 1.332 [0.000, 3.000],  loss: 7.535387, mae: 52.867330, mean_q: 70.768937, mean_eps: 0.150946
 141906/150000: episode: 957, duration: 1.797s, episode steps: 255, steps per second: 142, episode reward: -218.191, mean reward: -0.856 [-100.000,  5.246], mean action: 1.827 [0.000, 3.000],  loss: 5.909323, mae: 53.092572, mean_q: 71.097441, mean_eps: 0.149332
 142202/150000: episode: 958, duration: 2.103s, episode steps: 296, steps per second: 141, episode reward: 242.727, mean reward:  0.820 [-17.921, 100.000], mean action: 1.257 [0.000, 3.000],  loss: 7.234545, mae: 52.941370, mean_q: 70.735550, mean_eps: 0.147679
 142469/150000: episode: 959, duration: 1.925s, episode steps: 267, steps per second: 139, episode reward: 252.727, mean reward:  0.947 [-19.342, 100.000], mean action: 1.412 [0.000, 3.000],  loss: 8.768081, mae: 53.021519, mean_q: 70.847974, mean_eps: 0.145990
 142703/150000: episode: 960, duration: 1.651s, episode steps: 234, steps per second: 142, episode reward: 246.859, mean reward:  1.055 [-3.290, 100.000], mean action: 1.466 [0.000, 3.000],  loss: 11.119076, mae: 52.988446, mean_q: 70.869041, mean_eps: 0.144487
 143107/150000: episode: 961, duration: 2.882s, episode steps: 404, steps per second: 140, episode reward: 265.284, mean reward:  0.657 [-20.760, 100.000], mean action: 1.032 [0.000, 3.000],  loss: 8.616274, mae: 53.245063, mean_q: 71.019791, mean_eps: 0.142573
 143898/150000: episode: 962, duration: 5.709s, episode steps: 791, steps per second: 139, episode reward: 250.707, mean reward:  0.317 [-19.411, 100.000], mean action: 0.855 [0.000, 3.000],  loss: 8.716616, mae: 53.358010, mean_q: 71.175420, mean_eps: 0.138988
 144132/150000: episode: 963, duration: 1.684s, episode steps: 234, steps per second: 139, episode reward: 258.518, mean reward:  1.105 [-12.793, 100.000], mean action: 1.415 [0.000, 3.000],  loss: 8.583609, mae: 53.427976, mean_q: 71.150333, mean_eps: 0.135913
 144405/150000: episode: 964, duration: 1.928s, episode steps: 273, steps per second: 142, episode reward: 267.665, mean reward:  0.980 [-13.454, 100.000], mean action: 1.495 [0.000, 3.000],  loss: 10.452089, mae: 53.275926, mean_q: 70.959862, mean_eps: 0.134392
 145017/150000: episode: 965, duration: 4.301s, episode steps: 612, steps per second: 142, episode reward: 236.497, mean reward:  0.386 [-20.831, 100.000], mean action: 0.846 [0.000, 3.000],  loss: 8.098829, mae: 53.418384, mean_q: 71.295876, mean_eps: 0.131737
 145314/150000: episode: 966, duration: 2.119s, episode steps: 297, steps per second: 140, episode reward: 256.380, mean reward:  0.863 [-8.158, 100.000], mean action: 1.636 [0.000, 3.000],  loss: 8.423043, mae: 52.620706, mean_q: 70.220780, mean_eps: 0.129010
 146229/150000: episode: 967, duration: 6.753s, episode steps: 915, steps per second: 136, episode reward: 247.002, mean reward:  0.270 [-20.910, 100.000], mean action: 1.308 [0.000, 3.000],  loss: 8.279545, mae: 52.910597, mean_q: 70.630613, mean_eps: 0.125374
 147127/150000: episode: 968, duration: 6.533s, episode steps: 898, steps per second: 137, episode reward: 266.456, mean reward:  0.297 [-20.257, 100.000], mean action: 1.173 [0.000, 3.000],  loss: 7.428621, mae: 52.553046, mean_q: 70.189205, mean_eps: 0.119935
 147702/150000: episode: 969, duration: 4.238s, episode steps: 575, steps per second: 136, episode reward: 197.184, mean reward:  0.343 [-19.324, 100.000], mean action: 1.374 [0.000, 3.000],  loss: 6.997431, mae: 52.060869, mean_q: 69.640574, mean_eps: 0.115516
 148110/150000: episode: 970, duration: 2.915s, episode steps: 408, steps per second: 140, episode reward: 249.924, mean reward:  0.613 [-6.464, 100.000], mean action: 1.578 [0.000, 3.000],  loss: 8.002566, mae: 52.005325, mean_q: 69.444183, mean_eps: 0.112567
 148753/150000: episode: 971, duration: 4.940s, episode steps: 643, steps per second: 130, episode reward: 221.855, mean reward:  0.345 [-20.240, 100.000], mean action: 1.333 [0.000, 3.000],  loss: 6.772570, mae: 52.151884, mean_q: 69.778430, mean_eps: 0.109414
 149140/150000: episode: 972, duration: 2.905s, episode steps: 387, steps per second: 133, episode reward: 273.985, mean reward:  0.708 [-19.075, 100.000], mean action: 0.984 [0.000, 3.000],  loss: 7.134005, mae: 52.167714, mean_q: 69.762538, mean_eps: 0.106324
 149475/150000: episode: 973, duration: 2.398s, episode steps: 335, steps per second: 140, episode reward: 238.537, mean reward:  0.712 [-2.881, 100.000], mean action: 1.340 [0.000, 3.000],  loss: 8.241784, mae: 51.877896, mean_q: 69.234756, mean_eps: 0.104158
 149734/150000: episode: 974, duration: 1.837s, episode steps: 259, steps per second: 141, episode reward: 265.408, mean reward:  1.025 [-3.085, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 7.192661, mae: 52.178632, mean_q: 69.734216, mean_eps: 0.102376
done, took 1089.577 seconds
Testing for 5 episodes ...
Episode 1: reward: 277.616, steps: 183
Episode 2: reward: 234.725, steps: 195
Episode 3: reward: 275.047, steps: 222
Episode 4: reward: 176.668, steps: 1000
Episode 5: reward: 199.726, steps: 547
Testing for 5 episodes ...
Episode 1: reward: 17.063, steps: 214
Episode 2: reward: 246.001, steps: 229
Episode 3: reward: 214.868, steps: 381
Episode 4: reward: 196.448, steps: 403
Episode 5: reward: 262.259, steps: 366
Testing for 5 episodes ...
Episode 1: reward: 173.987, steps: 424
Episode 2: reward: 243.327, steps: 271
Episode 3: reward: 273.437, steps: 191
Episode 4: reward: 224.802, steps: 243
Episode 5: reward: 274.639, steps: 278
Testing for 5 episodes ...
Episode 1: reward: 258.270, steps: 226
Episode 2: reward: 226.336, steps: 281
Episode 3: reward: 226.119, steps: 905
Episode 4: reward: -16.397, steps: 1000
Episode 5: reward: 294.049, steps: 193
Testing for 5 episodes ...
Episode 1: reward: 280.730, steps: 193
Episode 2: reward: 286.327, steps: 201
Episode 3: reward: 253.837, steps: 204
Episode 4: reward: -189.218, steps: 454
Episode 5: reward: 234.989, steps: 210
Testing for 5 episodes ...
Episode 1: reward: 282.024, steps: 298
Episode 2: reward: 242.367, steps: 303
Episode 3: reward: 240.919, steps: 208
Episode 4: reward: 184.337, steps: 436
Episode 5: reward: 64.736, steps: 135
Testing for 5 episodes ...
Episode 1: reward: 254.114, steps: 264
Episode 2: reward: 285.385, steps: 212
Episode 3: reward: 269.755, steps: 223
Episode 4: reward: 267.127, steps: 202
Episode 5: reward: 270.343, steps: 189
Testing for 5 episodes ...
Episode 1: reward: 285.477, steps: 184
Episode 2: reward: 242.380, steps: 258
Episode 3: reward: 115.188, steps: 1000
Episode 4: reward: 311.632, steps: 228
