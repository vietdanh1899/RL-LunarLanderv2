Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten_1 (Flatten)          (None, 8)                 0
_________________________________________________________________
dense_4 (Dense)              (None, 16)                144
_________________________________________________________________
activation_4 (Activation)    (None, 16)                0
_________________________________________________________________
dense_5 (Dense)              (None, 32)                544
_________________________________________________________________
activation_5 (Activation)    (None, 32)                0
_________________________________________________________________
dense_6 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_6 (Activation)    (None, 4)                 0
=================================================================
Total params: 820
Trainable params: 820
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
Training for 200000 steps ...
     58/200000: episode: 1, duration: 0.830s, episode steps:  58, steps per second:  70, episode reward: -137.810, mean reward: -2.376 [-100.000, 13.098], mean action: 1.345 [0.000, 3.000],  loss: 0.828940, mae: 0.588459, mean_q: 0.434828, mean_eps: 0.999847
    174/200000: episode: 2, duration: 0.735s, episode steps: 116, steps per second: 158, episode reward: -174.664, mean reward: -1.506 [-100.000, 21.227], mean action: 1.397 [0.000, 3.000],  loss: 39.768968, mae: 0.944148, mean_q: 0.068919, mean_eps: 0.999480
    243/200000: episode: 3, duration: 0.450s, episode steps:  69, steps per second: 154, episode reward: -250.331, mean reward: -3.628 [-100.000, 119.312], mean action: 1.580 [0.000, 3.000],  loss: 40.619907, mae: 1.318164, mean_q: -0.633112, mean_eps: 0.999064
    323/200000: episode: 4, duration: 0.539s, episode steps:  80, steps per second: 149, episode reward: -130.735, mean reward: -1.634 [-100.000, 14.418], mean action: 1.562 [0.000, 3.000],  loss: 66.042969, mae: 2.300614, mean_q: -1.365693, mean_eps: 0.998729
    380/200000: episode: 5, duration: 0.421s, episode steps:  57, steps per second: 135, episode reward: -88.244, mean reward: -1.548 [-100.000, 45.006], mean action: 1.404 [0.000, 3.000],  loss: 72.217925, mae: 2.890753, mean_q: -1.611851, mean_eps: 0.998420
    460/200000: episode: 6, duration: 0.558s, episode steps:  80, steps per second: 143, episode reward: -156.139, mean reward: -1.952 [-100.000,  7.475], mean action: 1.587 [0.000, 3.000],  loss: 81.472211, mae: 3.031369, mean_q: -1.648979, mean_eps: 0.998112
    524/200000: episode: 7, duration: 0.517s, episode steps:  64, steps per second: 124, episode reward: -102.527, mean reward: -1.602 [-100.000, 13.269], mean action: 1.781 [0.000, 3.000],  loss: 64.480543, mae: 2.963772, mean_q: -1.137237, mean_eps: 0.997788
    615/200000: episode: 8, duration: 0.672s, episode steps:  91, steps per second: 135, episode reward: -67.182, mean reward: -0.738 [-100.000, 10.525], mean action: 1.538 [0.000, 3.000],  loss: 59.385321, mae: 2.845627, mean_q: -1.243333, mean_eps: 0.997439
    698/200000: episode: 9, duration: 0.632s, episode steps:  83, steps per second: 131, episode reward: -122.763, mean reward: -1.479 [-100.000,  8.820], mean action: 1.590 [0.000, 3.000],  loss: 64.132779, mae: 2.836181, mean_q: -1.004089, mean_eps: 0.997048
    801/200000: episode: 10, duration: 0.855s, episode steps: 103, steps per second: 121, episode reward: -5.204, mean reward: -0.051 [-100.000, 102.956], mean action: 1.379 [0.000, 3.000],  loss: 56.090501, mae: 3.012650, mean_q: -0.692892, mean_eps: 0.996629
    881/200000: episode: 11, duration: 0.768s, episode steps:  80, steps per second: 104, episode reward: -267.635, mean reward: -3.345 [-100.000,  3.596], mean action: 1.550 [0.000, 3.000],  loss: 57.760742, mae: 3.565291, mean_q: -0.330827, mean_eps: 0.996218
    957/200000: episode: 12, duration: 0.631s, episode steps:  76, steps per second: 120, episode reward: -103.011, mean reward: -1.355 [-100.000,  7.199], mean action: 1.250 [0.000, 3.000],  loss: 34.584949, mae: 4.062293, mean_q: -0.265727, mean_eps: 0.995867
   1042/200000: episode: 13, duration: 0.558s, episode steps:  85, steps per second: 152, episode reward: -104.862, mean reward: -1.234 [-100.000, 11.137], mean action: 1.424 [0.000, 3.000],  loss: 40.225865, mae: 4.528921, mean_q: 0.011615, mean_eps: 0.995505
   1110/200000: episode: 14, duration: 0.501s, episode steps:  68, steps per second: 136, episode reward: -250.500, mean reward: -3.684 [-100.000, 69.213], mean action: 1.500 [0.000, 3.000],  loss: 29.403668, mae: 4.780577, mean_q: 0.420042, mean_eps: 0.995160
   1233/200000: episode: 15, duration: 0.816s, episode steps: 123, steps per second: 151, episode reward: -181.081, mean reward: -1.472 [-100.000,  2.852], mean action: 1.423 [0.000, 3.000],  loss: 39.119410, mae: 6.142218, mean_q: 0.124396, mean_eps: 0.994731
   1323/200000: episode: 16, duration: 0.690s, episode steps:  90, steps per second: 130, episode reward: -129.429, mean reward: -1.438 [-100.000,  9.170], mean action: 1.544 [0.000, 3.000],  loss: 34.390587, mae: 6.546011, mean_q: 0.856903, mean_eps: 0.994251
   1407/200000: episode: 17, duration: 0.584s, episode steps:  84, steps per second: 144, episode reward: -383.732, mean reward: -4.568 [-100.000, 40.329], mean action: 1.369 [0.000, 3.000],  loss: 32.637505, mae: 7.538412, mean_q: 1.070775, mean_eps: 0.993860
   1495/200000: episode: 18, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -355.524, mean reward: -4.040 [-100.000, 57.813], mean action: 1.409 [0.000, 3.000],  loss: 25.855584, mae: 8.031558, mean_q: 1.177026, mean_eps: 0.993473
   1601/200000: episode: 19, duration: 0.703s, episode steps: 106, steps per second: 151, episode reward: -153.488, mean reward: -1.448 [-100.000, 16.611], mean action: 1.566 [0.000, 3.000],  loss: 31.955817, mae: 9.597129, mean_q: -0.471087, mean_eps: 0.993036
   1660/200000: episode: 20, duration: 0.399s, episode steps:  59, steps per second: 148, episode reward: -73.990, mean reward: -1.254 [-100.000,  6.399], mean action: 1.305 [0.000, 3.000],  loss: 21.859181, mae: 9.346150, mean_q: 0.359406, mean_eps: 0.992665
   1797/200000: episode: 21, duration: 0.931s, episode steps: 137, steps per second: 147, episode reward: -51.958, mean reward: -0.379 [-100.000, 75.778], mean action: 1.453 [0.000, 3.000],  loss: 22.458357, mae: 10.501151, mean_q: 0.118933, mean_eps: 0.992224
   1921/200000: episode: 22, duration: 0.933s, episode steps: 124, steps per second: 133, episode reward: -150.017, mean reward: -1.210 [-100.000, 13.547], mean action: 1.629 [0.000, 3.000],  loss: 19.895379, mae: 11.647663, mean_q: -0.498294, mean_eps: 0.991637
   2002/200000: episode: 23, duration: 0.477s, episode steps:  81, steps per second: 170, episode reward: -134.190, mean reward: -1.657 [-100.000, 12.024], mean action: 1.420 [0.000, 3.000],  loss: 21.453183, mae: 12.138441, mean_q: -0.104097, mean_eps: 0.991175
   2104/200000: episode: 24, duration: 0.621s, episode steps: 102, steps per second: 164, episode reward: -435.272, mean reward: -4.267 [-100.000,  0.709], mean action: 1.725 [0.000, 3.000],  loss: 17.989716, mae: 12.325705, mean_q: 0.520029, mean_eps: 0.990764
   2193/200000: episode: 25, duration: 0.506s, episode steps:  89, steps per second: 176, episode reward: -498.108, mean reward: -5.597 [-100.000,  0.816], mean action: 1.539 [0.000, 3.000],  loss: 21.203917, mae: 13.262747, mean_q: -0.989499, mean_eps: 0.990334
   2265/200000: episode: 26, duration: 0.481s, episode steps:  72, steps per second: 150, episode reward: -214.273, mean reward: -2.976 [-100.000, 29.799], mean action: 1.667 [0.000, 3.000],  loss: 24.554882, mae: 13.388252, mean_q: -0.856701, mean_eps: 0.989972
   2372/200000: episode: 27, duration: 0.629s, episode steps: 107, steps per second: 170, episode reward: -285.943, mean reward: -2.672 [-100.000, 67.319], mean action: 1.364 [0.000, 3.000],  loss: 25.154264, mae: 13.642792, mean_q: -2.389425, mean_eps: 0.989569
   2465/200000: episode: 28, duration: 0.547s, episode steps:  93, steps per second: 170, episode reward: -402.360, mean reward: -4.326 [-100.000,  1.781], mean action: 1.742 [0.000, 3.000],  loss: 18.988320, mae: 14.633225, mean_q: -4.094072, mean_eps: 0.989119
   2554/200000: episode: 29, duration: 0.495s, episode steps:  89, steps per second: 180, episode reward: -300.010, mean reward: -3.371 [-100.000,  3.000], mean action: 1.483 [0.000, 3.000],  loss: 22.585227, mae: 15.148316, mean_q: -5.405381, mean_eps: 0.988710
   2677/200000: episode: 30, duration: 0.751s, episode steps: 123, steps per second: 164, episode reward: -365.608, mean reward: -2.972 [-100.000, 47.562], mean action: 1.415 [0.000, 3.000],  loss: 21.697925, mae: 15.905570, mean_q: -6.685987, mean_eps: 0.988232
   2813/200000: episode: 31, duration: 0.789s, episode steps: 136, steps per second: 172, episode reward: -263.734, mean reward: -1.939 [-100.000,  1.831], mean action: 1.574 [0.000, 3.000],  loss: 22.131134, mae: 17.060352, mean_q: -7.860849, mean_eps: 0.987650
   2903/200000: episode: 32, duration: 0.508s, episode steps:  90, steps per second: 177, episode reward: -126.010, mean reward: -1.400 [-100.000,  8.092], mean action: 1.456 [0.000, 3.000],  loss: 21.478497, mae: 17.377900, mean_q: -7.879612, mean_eps: 0.987141
   2983/200000: episode: 33, duration: 0.512s, episode steps:  80, steps per second: 156, episode reward: -218.580, mean reward: -2.732 [-100.000, 15.960], mean action: 1.425 [0.000, 3.000],  loss: 23.094904, mae: 17.810372, mean_q: -8.806626, mean_eps: 0.986759
   3052/200000: episode: 34, duration: 0.425s, episode steps:  69, steps per second: 163, episode reward: -128.639, mean reward: -1.864 [-100.000, 22.163], mean action: 1.478 [0.000, 3.000],  loss: 19.868926, mae: 18.537930, mean_q: -9.324358, mean_eps: 0.986424
   3116/200000: episode: 35, duration: 0.373s, episode steps:  64, steps per second: 171, episode reward: -230.953, mean reward: -3.609 [-100.000,  5.598], mean action: 1.438 [0.000, 3.000],  loss: 24.401591, mae: 17.953987, mean_q: -8.448084, mean_eps: 0.986124
   3195/200000: episode: 36, duration: 0.457s, episode steps:  79, steps per second: 173, episode reward: -79.573, mean reward: -1.007 [-100.000, 17.074], mean action: 1.519 [0.000, 3.000],  loss: 27.948220, mae: 18.545681, mean_q: -9.286922, mean_eps: 0.985802
   3298/200000: episode: 37, duration: 0.581s, episode steps: 103, steps per second: 177, episode reward: -219.680, mean reward: -2.133 [-100.000,  1.191], mean action: 1.505 [0.000, 3.000],  loss: 18.552747, mae: 19.439822, mean_q: -11.045545, mean_eps: 0.985393
   3412/200000: episode: 38, duration: 0.671s, episode steps: 114, steps per second: 170, episode reward: -334.577, mean reward: -2.935 [-100.000, 115.311], mean action: 1.596 [0.000, 3.000],  loss: 25.509987, mae: 19.742183, mean_q: -11.401337, mean_eps: 0.984905
   3532/200000: episode: 39, duration: 0.682s, episode steps: 120, steps per second: 176, episode reward: -120.705, mean reward: -1.006 [-100.000,  9.563], mean action: 1.717 [0.000, 3.000],  loss: 19.142114, mae: 20.901820, mean_q: -12.474538, mean_eps: 0.984378
   3656/200000: episode: 40, duration: 0.716s, episode steps: 124, steps per second: 173, episode reward: -191.881, mean reward: -1.547 [-100.000,  8.007], mean action: 1.484 [0.000, 3.000],  loss: 20.803412, mae: 21.146835, mean_q: -11.805121, mean_eps: 0.983829
   3785/200000: episode: 41, duration: 0.811s, episode steps: 129, steps per second: 159, episode reward: -156.164, mean reward: -1.211 [-100.000,  7.366], mean action: 1.535 [0.000, 3.000],  loss: 22.208350, mae: 21.948543, mean_q: -12.420024, mean_eps: 0.983260
   3861/200000: episode: 42, duration: 0.427s, episode steps:  76, steps per second: 178, episode reward: -146.501, mean reward: -1.928 [-100.000,  7.350], mean action: 1.408 [0.000, 3.000],  loss: 19.379226, mae: 22.785324, mean_q: -13.572103, mean_eps: 0.982799
   3949/200000: episode: 43, duration: 0.491s, episode steps:  88, steps per second: 179, episode reward: -212.513, mean reward: -2.415 [-100.000, 21.854], mean action: 1.716 [0.000, 3.000],  loss: 26.465981, mae: 23.476637, mean_q: -14.140722, mean_eps: 0.982430
   4038/200000: episode: 44, duration: 0.490s, episode steps:  89, steps per second: 181, episode reward: -321.337, mean reward: -3.611 [-100.000, 78.652], mean action: 1.663 [0.000, 3.000],  loss: 25.816885, mae: 22.699529, mean_q: -12.559478, mean_eps: 0.982031
   4156/200000: episode: 45, duration: 0.708s, episode steps: 118, steps per second: 167, episode reward: -70.757, mean reward: -0.600 [-100.000, 12.809], mean action: 1.373 [0.000, 3.000],  loss: 21.550927, mae: 23.576362, mean_q: -13.177053, mean_eps: 0.981566
   4238/200000: episode: 46, duration: 0.470s, episode steps:  82, steps per second: 174, episode reward: -231.832, mean reward: -2.827 [-100.000,  9.719], mean action: 1.598 [0.000, 3.000],  loss: 23.890791, mae: 24.242233, mean_q: -13.131457, mean_eps: 0.981116
   4312/200000: episode: 47, duration: 0.428s, episode steps:  74, steps per second: 173, episode reward: -138.645, mean reward: -1.874 [-100.000,  6.927], mean action: 1.500 [0.000, 3.000],  loss: 27.265819, mae: 24.830518, mean_q: -14.294104, mean_eps: 0.980765
   4397/200000: episode: 48, duration: 0.484s, episode steps:  85, steps per second: 175, episode reward: -327.100, mean reward: -3.848 [-100.000,  0.111], mean action: 1.541 [0.000, 3.000],  loss: 36.643445, mae: 25.084495, mean_q: -13.798595, mean_eps: 0.980407
   4497/200000: episode: 49, duration: 0.931s, episode steps: 100, steps per second: 107, episode reward: -121.708, mean reward: -1.217 [-100.000,  7.604], mean action: 1.390 [0.000, 3.000],  loss: 26.801021, mae: 25.343386, mean_q: -13.379345, mean_eps: 0.979991
   4576/200000: episode: 50, duration: 0.448s, episode steps:  79, steps per second: 176, episode reward: -284.158, mean reward: -3.597 [-100.000, 108.692], mean action: 1.443 [0.000, 3.000],  loss: 21.856376, mae: 25.620516, mean_q: -14.172169, mean_eps: 0.979588
   4708/200000: episode: 51, duration: 0.742s, episode steps: 132, steps per second: 178, episode reward: -203.862, mean reward: -1.544 [-100.000, 29.053], mean action: 1.667 [0.000, 3.000],  loss: 24.532517, mae: 26.375106, mean_q: -15.051613, mean_eps: 0.979113
   4779/200000: episode: 52, duration: 0.433s, episode steps:  71, steps per second: 164, episode reward: -80.683, mean reward: -1.136 [-100.000, 60.060], mean action: 1.437 [0.000, 3.000],  loss: 24.292447, mae: 26.025372, mean_q: -13.959489, mean_eps: 0.978657
   4879/200000: episode: 53, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -139.749, mean reward: -1.397 [-100.000, 16.628], mean action: 1.440 [0.000, 3.000],  loss: 29.727680, mae: 26.613721, mean_q: -15.131066, mean_eps: 0.978272
   5006/200000: episode: 54, duration: 0.719s, episode steps: 127, steps per second: 177, episode reward: -132.428, mean reward: -1.043 [-100.000,  9.632], mean action: 1.457 [0.000, 3.000],  loss: 27.357540, mae: 25.974145, mean_q: -14.468773, mean_eps: 0.977761
   5104/200000: episode: 55, duration: 0.535s, episode steps:  98, steps per second: 183, episode reward: -85.651, mean reward: -0.874 [-100.000, 14.111], mean action: 1.592 [0.000, 3.000],  loss: 24.612061, mae: 26.962575, mean_q: -16.289107, mean_eps: 0.977255
   5204/200000: episode: 56, duration: 0.596s, episode steps: 100, steps per second: 168, episode reward: -29.223, mean reward: -0.292 [-100.000, 87.876], mean action: 1.440 [0.000, 3.000],  loss: 36.248729, mae: 27.150473, mean_q: -16.340986, mean_eps: 0.976809
   5330/200000: episode: 57, duration: 0.740s, episode steps: 126, steps per second: 170, episode reward: -340.316, mean reward: -2.701 [-100.000, 82.363], mean action: 1.524 [0.000, 3.000],  loss: 23.521183, mae: 26.461051, mean_q: -14.883252, mean_eps: 0.976301
   5398/200000: episode: 58, duration: 0.385s, episode steps:  68, steps per second: 176, episode reward: -112.583, mean reward: -1.656 [-100.000,  6.021], mean action: 1.471 [0.000, 3.000],  loss: 39.962934, mae: 26.953583, mean_q: -15.657863, mean_eps: 0.975864
   5467/200000: episode: 59, duration: 0.388s, episode steps:  69, steps per second: 178, episode reward: -161.381, mean reward: -2.339 [-100.000, 26.924], mean action: 1.261 [0.000, 3.000],  loss: 25.440329, mae: 27.122936, mean_q: -14.989178, mean_eps: 0.975556
   5546/200000: episode: 60, duration: 0.481s, episode steps:  79, steps per second: 164, episode reward: -108.091, mean reward: -1.368 [-100.000,  6.566], mean action: 1.557 [0.000, 3.000],  loss: 25.241222, mae: 28.155124, mean_q: -16.472246, mean_eps: 0.975223
   5627/200000: episode: 61, duration: 0.473s, episode steps:  81, steps per second: 171, episode reward: -69.566, mean reward: -0.859 [-100.000, 23.529], mean action: 1.457 [0.000, 3.000],  loss: 30.643958, mae: 26.811035, mean_q: -14.132214, mean_eps: 0.974863
   5740/200000: episode: 62, duration: 0.629s, episode steps: 113, steps per second: 180, episode reward: -171.711, mean reward: -1.520 [-100.000,  4.109], mean action: 1.637 [0.000, 3.000],  loss: 20.113386, mae: 26.667958, mean_q: -13.613828, mean_eps: 0.974427
   5821/200000: episode: 63, duration: 0.495s, episode steps:  81, steps per second: 164, episode reward: -100.513, mean reward: -1.241 [-100.000,  6.915], mean action: 1.370 [0.000, 3.000],  loss: 21.710301, mae: 27.172359, mean_q: -14.050198, mean_eps: 0.973990
   5900/200000: episode: 64, duration: 0.476s, episode steps:  79, steps per second: 166, episode reward: -96.513, mean reward: -1.222 [-100.000,  5.405], mean action: 1.570 [0.000, 3.000],  loss: 26.143558, mae: 26.748445, mean_q: -12.439496, mean_eps: 0.973630
   6003/200000: episode: 65, duration: 0.596s, episode steps: 103, steps per second: 173, episode reward: -331.734, mean reward: -3.221 [-100.000,  3.574], mean action: 1.408 [0.000, 3.000],  loss: 29.448967, mae: 27.696027, mean_q: -13.846434, mean_eps: 0.973220
   6096/200000: episode: 66, duration: 0.519s, episode steps:  93, steps per second: 179, episode reward: -158.960, mean reward: -1.709 [-100.000, 15.344], mean action: 1.613 [0.000, 3.000],  loss: 21.457642, mae: 27.132245, mean_q: -11.688704, mean_eps: 0.972780
   6184/200000: episode: 67, duration: 0.495s, episode steps:  88, steps per second: 178, episode reward: -105.032, mean reward: -1.194 [-100.000, 10.589], mean action: 1.511 [0.000, 3.000],  loss: 35.760882, mae: 27.784054, mean_q: -12.409730, mean_eps: 0.972372
   6269/200000: episode: 68, duration: 0.511s, episode steps:  85, steps per second: 166, episode reward: -210.258, mean reward: -2.474 [-100.000, 25.075], mean action: 1.353 [0.000, 3.000],  loss: 34.398643, mae: 28.320514, mean_q: -12.230365, mean_eps: 0.971983
   6334/200000: episode: 69, duration: 0.408s, episode steps:  65, steps per second: 159, episode reward: -42.544, mean reward: -0.655 [-100.000, 12.490], mean action: 1.415 [0.000, 3.000],  loss: 32.810716, mae: 27.578691, mean_q: -11.660861, mean_eps: 0.971646
   6442/200000: episode: 70, duration: 0.617s, episode steps: 108, steps per second: 175, episode reward: -119.459, mean reward: -1.106 [-100.000, 21.422], mean action: 1.556 [0.000, 3.000],  loss: 24.375492, mae: 28.351385, mean_q: -11.242761, mean_eps: 0.971256
   6525/200000: episode: 71, duration: 0.468s, episode steps:  83, steps per second: 177, episode reward: -311.690, mean reward: -3.755 [-100.000,  2.180], mean action: 1.398 [0.000, 3.000],  loss: 19.634506, mae: 28.134488, mean_q: -11.709156, mean_eps: 0.970827
   6601/200000: episode: 72, duration: 0.417s, episode steps:  76, steps per second: 182, episode reward: -130.193, mean reward: -1.713 [-100.000,  8.173], mean action: 1.618 [0.000, 3.000],  loss: 22.709276, mae: 29.074073, mean_q: -12.191625, mean_eps: 0.970469
   6667/200000: episode: 73, duration: 0.385s, episode steps:  66, steps per second: 171, episode reward: -124.290, mean reward: -1.883 [-100.000,  8.400], mean action: 1.106 [0.000, 3.000],  loss: 29.413137, mae: 27.458788, mean_q: -8.020921, mean_eps: 0.970149
   6790/200000: episode: 74, duration: 0.694s, episode steps: 123, steps per second: 177, episode reward: -133.154, mean reward: -1.083 [-100.000,  9.222], mean action: 1.358 [0.000, 3.000],  loss: 20.576302, mae: 28.372428, mean_q: -9.770553, mean_eps: 0.969724
   6853/200000: episode: 75, duration: 0.355s, episode steps:  63, steps per second: 177, episode reward: -114.659, mean reward: -1.820 [-100.000,  8.865], mean action: 1.492 [0.000, 3.000],  loss: 23.717149, mae: 28.441397, mean_q: -8.593366, mean_eps: 0.969305
   6938/200000: episode: 76, duration: 0.474s, episode steps:  85, steps per second: 179, episode reward: -236.243, mean reward: -2.779 [-100.000, 12.060], mean action: 1.624 [0.000, 3.000],  loss: 24.055075, mae: 28.156705, mean_q: -8.929125, mean_eps: 0.968973
   7036/200000: episode: 77, duration: 0.559s, episode steps:  98, steps per second: 175, episode reward: -317.950, mean reward: -3.244 [-100.000, 109.099], mean action: 1.582 [0.000, 3.000],  loss: 17.995355, mae: 28.884727, mean_q: -10.568676, mean_eps: 0.968561
   7136/200000: episode: 78, duration: 0.584s, episode steps: 100, steps per second: 171, episode reward: -337.490, mean reward: -3.375 [-100.000,  0.540], mean action: 1.500 [0.000, 3.000],  loss: 34.128444, mae: 29.804363, mean_q: -11.339438, mean_eps: 0.968115
   7236/200000: episode: 79, duration: 0.560s, episode steps: 100, steps per second: 179, episode reward: -134.112, mean reward: -1.341 [-100.000,  5.924], mean action: 1.650 [0.000, 3.000],  loss: 26.802196, mae: 28.913226, mean_q: -10.470895, mean_eps: 0.967665
   7336/200000: episode: 80, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -123.873, mean reward: -1.239 [-100.000, 11.458], mean action: 1.730 [0.000, 3.000],  loss: 19.287634, mae: 28.896666, mean_q: -9.652053, mean_eps: 0.967215
   7399/200000: episode: 81, duration: 0.379s, episode steps:  63, steps per second: 166, episode reward: -240.021, mean reward: -3.810 [-100.000, 11.122], mean action: 1.492 [0.000, 3.000],  loss: 25.442734, mae: 28.942702, mean_q: -10.281193, mean_eps: 0.966848
   7496/200000: episode: 82, duration: 0.567s, episode steps:  97, steps per second: 171, episode reward: -486.123, mean reward: -5.012 [-100.000, 78.886], mean action: 1.505 [0.000, 3.000],  loss: 27.686342, mae: 29.307085, mean_q: -10.719827, mean_eps: 0.966489
   7586/200000: episode: 83, duration: 0.554s, episode steps:  90, steps per second: 163, episode reward: -147.963, mean reward: -1.644 [-100.000,  4.254], mean action: 1.667 [0.000, 3.000],  loss: 18.289436, mae: 28.636633, mean_q: -8.324694, mean_eps: 0.966068
   7692/200000: episode: 84, duration: 0.603s, episode steps: 106, steps per second: 176, episode reward: -101.251, mean reward: -0.955 [-100.000,  6.476], mean action: 1.509 [0.000, 3.000],  loss: 22.187149, mae: 28.483897, mean_q: -8.077317, mean_eps: 0.965627
   7796/200000: episode: 85, duration: 0.617s, episode steps: 104, steps per second: 169, episode reward: -57.992, mean reward: -0.558 [-100.000, 29.659], mean action: 1.548 [0.000, 3.000],  loss: 20.792917, mae: 29.862063, mean_q: -8.800441, mean_eps: 0.965154
   7866/200000: episode: 86, duration: 0.406s, episode steps:  70, steps per second: 173, episode reward: -160.619, mean reward: -2.295 [-100.000, 14.425], mean action: 1.771 [0.000, 3.000],  loss: 32.646632, mae: 29.807390, mean_q: -9.133134, mean_eps: 0.964763
   7963/200000: episode: 87, duration: 0.669s, episode steps:  97, steps per second: 145, episode reward: -418.932, mean reward: -4.319 [-100.000,  2.022], mean action: 1.670 [0.000, 3.000],  loss: 19.182321, mae: 30.085779, mean_q: -9.200133, mean_eps: 0.964387
   8080/200000: episode: 88, duration: 0.727s, episode steps: 117, steps per second: 161, episode reward: -238.444, mean reward: -2.038 [-100.000, 110.458], mean action: 1.538 [0.000, 3.000],  loss: 35.297689, mae: 29.692324, mean_q: -8.510738, mean_eps: 0.963905
   8175/200000: episode: 89, duration: 0.589s, episode steps:  95, steps per second: 161, episode reward: -458.759, mean reward: -4.829 [-100.000,  0.263], mean action: 1.516 [0.000, 3.000],  loss: 22.228168, mae: 29.861909, mean_q: -8.516574, mean_eps: 0.963429
   8275/200000: episode: 90, duration: 0.627s, episode steps: 100, steps per second: 159, episode reward: -80.058, mean reward: -0.801 [-100.000, 20.459], mean action: 1.390 [0.000, 3.000],  loss: 25.522190, mae: 29.596473, mean_q: -8.472156, mean_eps: 0.962990
   8344/200000: episode: 91, duration: 0.493s, episode steps:  69, steps per second: 140, episode reward: -365.642, mean reward: -5.299 [-100.000,  3.994], mean action: 1.290 [0.000, 3.000],  loss: 24.474346, mae: 29.920655, mean_q: -7.643318, mean_eps: 0.962610
   8427/200000: episode: 92, duration: 1.025s, episode steps:  83, steps per second:  81, episode reward: -66.459, mean reward: -0.801 [-100.000, 47.555], mean action: 1.422 [0.000, 3.000],  loss: 37.381706, mae: 30.056333, mean_q: -8.397308, mean_eps: 0.962268
   8524/200000: episode: 93, duration: 0.815s, episode steps:  97, steps per second: 119, episode reward: -70.192, mean reward: -0.724 [-100.000, 15.772], mean action: 1.464 [0.000, 3.000],  loss: 22.403416, mae: 29.827453, mean_q: -7.672622, mean_eps: 0.961863
   8616/200000: episode: 94, duration: 0.754s, episode steps:  92, steps per second: 122, episode reward: -233.567, mean reward: -2.539 [-100.000, 31.759], mean action: 1.707 [0.000, 3.000],  loss: 32.992633, mae: 30.032306, mean_q: -7.627459, mean_eps: 0.961437
   8730/200000: episode: 95, duration: 0.846s, episode steps: 114, steps per second: 135, episode reward: -127.423, mean reward: -1.118 [-100.000, 88.139], mean action: 1.570 [0.000, 3.000],  loss: 28.835360, mae: 30.480569, mean_q: -8.267300, mean_eps: 0.960974
   8793/200000: episode: 96, duration: 0.402s, episode steps:  63, steps per second: 157, episode reward: -81.797, mean reward: -1.298 [-100.000,  8.984], mean action: 1.619 [0.000, 3.000],  loss: 19.474592, mae: 30.696737, mean_q: -7.137783, mean_eps: 0.960575
   8871/200000: episode: 97, duration: 0.461s, episode steps:  78, steps per second: 169, episode reward: -124.305, mean reward: -1.594 [-100.000, 41.647], mean action: 1.538 [0.000, 3.000],  loss: 38.115157, mae: 30.483473, mean_q: -8.030695, mean_eps: 0.960258
   8968/200000: episode: 98, duration: 0.601s, episode steps:  97, steps per second: 161, episode reward: -337.722, mean reward: -3.482 [-100.000, 17.382], mean action: 1.474 [0.000, 3.000],  loss: 29.811301, mae: 30.499832, mean_q: -7.590402, mean_eps: 0.959865
   9033/200000: episode: 99, duration: 0.417s, episode steps:  65, steps per second: 156, episode reward: -94.730, mean reward: -1.457 [-100.000,  7.426], mean action: 1.662 [0.000, 3.000],  loss: 23.186933, mae: 31.610287, mean_q: -9.842601, mean_eps: 0.959500
   9150/200000: episode: 100, duration: 0.714s, episode steps: 117, steps per second: 164, episode reward: -153.381, mean reward: -1.311 [-100.000, 10.867], mean action: 1.632 [0.000, 3.000],  loss: 30.199021, mae: 31.359544, mean_q: -7.397978, mean_eps: 0.959090
   9235/200000: episode: 101, duration: 0.492s, episode steps:  85, steps per second: 173, episode reward: -67.145, mean reward: -0.790 [-100.000, 10.533], mean action: 1.718 [0.000, 3.000],  loss: 27.140439, mae: 31.847188, mean_q: -7.002495, mean_eps: 0.958636
   9325/200000: episode: 102, duration: 0.514s, episode steps:  90, steps per second: 175, episode reward: -138.806, mean reward: -1.542 [-100.000, 21.360], mean action: 1.522 [0.000, 3.000],  loss: 22.428668, mae: 31.267605, mean_q: -3.304182, mean_eps: 0.958242
   9387/200000: episode: 103, duration: 0.399s, episode steps:  62, steps per second: 155, episode reward: -62.641, mean reward: -1.010 [-100.000, 11.774], mean action: 1.371 [0.000, 3.000],  loss: 30.320083, mae: 31.731832, mean_q: -4.876630, mean_eps: 0.957900
   9499/200000: episode: 104, duration: 0.694s, episode steps: 112, steps per second: 161, episode reward: -207.016, mean reward: -1.848 [-100.000,  7.899], mean action: 1.438 [0.000, 3.000],  loss: 25.456897, mae: 31.510574, mean_q: -6.358294, mean_eps: 0.957509
   9591/200000: episode: 105, duration: 0.567s, episode steps:  92, steps per second: 162, episode reward: -270.803, mean reward: -2.944 [-100.000,  3.196], mean action: 1.402 [0.000, 3.000],  loss: 32.092564, mae: 32.416427, mean_q: -5.690982, mean_eps: 0.957050
   9695/200000: episode: 106, duration: 0.660s, episode steps: 104, steps per second: 158, episode reward: -129.119, mean reward: -1.242 [-100.000, 26.670], mean action: 1.663 [0.000, 3.000],  loss: 27.038799, mae: 32.337686, mean_q: -5.498278, mean_eps: 0.956609
   9769/200000: episode: 107, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward: -96.756, mean reward: -1.308 [-100.000, 17.337], mean action: 1.541 [0.000, 3.000],  loss: 40.338590, mae: 31.332260, mean_q: -5.263430, mean_eps: 0.956208
   9857/200000: episode: 108, duration: 0.512s, episode steps:  88, steps per second: 172, episode reward: -274.250, mean reward: -3.116 [-100.000,  4.135], mean action: 1.443 [0.000, 3.000],  loss: 27.104816, mae: 32.428169, mean_q: -4.022719, mean_eps: 0.955844
   9942/200000: episode: 109, duration: 0.540s, episode steps:  85, steps per second: 157, episode reward: -129.436, mean reward: -1.523 [-100.000, 10.844], mean action: 1.459 [0.000, 3.000],  loss: 31.479865, mae: 33.172364, mean_q: -5.651107, mean_eps: 0.955454
  10041/200000: episode: 110, duration: 0.590s, episode steps:  99, steps per second: 168, episode reward: -126.885, mean reward: -1.282 [-100.000, 31.522], mean action: 1.455 [0.000, 3.000],  loss: 22.834051, mae: 33.831693, mean_q: -5.069046, mean_eps: 0.955041
  10128/200000: episode: 111, duration: 0.568s, episode steps:  87, steps per second: 153, episode reward: -153.209, mean reward: -1.761 [-100.000, 22.439], mean action: 1.540 [0.000, 3.000],  loss: 24.280014, mae: 32.604390, mean_q: -2.747601, mean_eps: 0.954622
  10235/200000: episode: 112, duration: 0.688s, episode steps: 107, steps per second: 156, episode reward: -121.707, mean reward: -1.137 [-100.000,  8.279], mean action: 1.439 [0.000, 3.000],  loss: 29.425696, mae: 33.175363, mean_q: -4.026240, mean_eps: 0.954186
  10317/200000: episode: 113, duration: 0.501s, episode steps:  82, steps per second: 164, episode reward: -114.248, mean reward: -1.393 [-100.000, 10.066], mean action: 1.610 [0.000, 3.000],  loss: 35.264585, mae: 32.424048, mean_q: -2.329022, mean_eps: 0.953760
  10383/200000: episode: 114, duration: 0.383s, episode steps:  66, steps per second: 172, episode reward: -118.581, mean reward: -1.797 [-100.000, 15.808], mean action: 1.652 [0.000, 3.000],  loss: 30.136583, mae: 32.903256, mean_q: -0.752091, mean_eps: 0.953427
  10466/200000: episode: 115, duration: 0.524s, episode steps:  83, steps per second: 159, episode reward: -236.216, mean reward: -2.846 [-100.000, 39.338], mean action: 1.349 [0.000, 3.000],  loss: 28.031835, mae: 32.745504, mean_q: -1.631997, mean_eps: 0.953092
  10537/200000: episode: 116, duration: 0.437s, episode steps:  71, steps per second: 162, episode reward: -121.719, mean reward: -1.714 [-100.000,  6.275], mean action: 1.549 [0.000, 3.000],  loss: 24.108360, mae: 32.187142, mean_q: -1.896947, mean_eps: 0.952746
  10647/200000: episode: 117, duration: 0.675s, episode steps: 110, steps per second: 163, episode reward: -223.808, mean reward: -2.035 [-100.000,  1.296], mean action: 1.627 [0.000, 3.000],  loss: 18.460562, mae: 32.913288, mean_q: -3.184589, mean_eps: 0.952338
  10765/200000: episode: 118, duration: 0.792s, episode steps: 118, steps per second: 149, episode reward: -181.252, mean reward: -1.536 [-100.000, 23.753], mean action: 1.636 [0.000, 3.000],  loss: 26.681815, mae: 33.366679, mean_q: -3.058763, mean_eps: 0.951825
  10839/200000: episode: 119, duration: 0.483s, episode steps:  74, steps per second: 153, episode reward: -26.600, mean reward: -0.359 [-100.000, 14.837], mean action: 1.351 [0.000, 3.000],  loss: 41.536232, mae: 33.508539, mean_q: -3.604858, mean_eps: 0.951393
  10912/200000: episode: 120, duration: 0.447s, episode steps:  73, steps per second: 163, episode reward: -119.511, mean reward: -1.637 [-100.000,  6.705], mean action: 1.397 [0.000, 3.000],  loss: 26.829071, mae: 32.782365, mean_q: -2.674342, mean_eps: 0.951062
  11001/200000: episode: 121, duration: 0.541s, episode steps:  89, steps per second: 165, episode reward: -116.290, mean reward: -1.307 [-100.000, 27.991], mean action: 1.640 [0.000, 3.000],  loss: 33.479804, mae: 33.923343, mean_q: -3.565711, mean_eps: 0.950698
  11111/200000: episode: 122, duration: 0.686s, episode steps: 110, steps per second: 160, episode reward: -245.285, mean reward: -2.230 [-100.000, 70.353], mean action: 1.545 [0.000, 3.000],  loss: 33.852490, mae: 34.173355, mean_q: -3.451019, mean_eps: 0.950250
  11172/200000: episode: 123, duration: 0.430s, episode steps:  61, steps per second: 142, episode reward: -75.486, mean reward: -1.237 [-100.000,  8.588], mean action: 1.590 [0.000, 3.000],  loss: 36.905573, mae: 34.219726, mean_q: -2.648400, mean_eps: 0.949865
  11277/200000: episode: 124, duration: 0.708s, episode steps: 105, steps per second: 148, episode reward: -72.237, mean reward: -0.688 [-100.000, 16.046], mean action: 1.590 [0.000, 3.000],  loss: 31.595234, mae: 34.699556, mean_q: -2.702022, mean_eps: 0.949492
  11389/200000: episode: 125, duration: 0.747s, episode steps: 112, steps per second: 150, episode reward: -149.833, mean reward: -1.338 [-100.000,  5.228], mean action: 1.598 [0.000, 3.000],  loss: 24.025612, mae: 33.754312, mean_q: -1.965986, mean_eps: 0.949004
  11463/200000: episode: 126, duration: 0.522s, episode steps:  74, steps per second: 142, episode reward: -116.456, mean reward: -1.574 [-100.000,  7.726], mean action: 1.514 [0.000, 3.000],  loss: 19.680132, mae: 32.637627, mean_q: -0.267441, mean_eps: 0.948585
  11535/200000: episode: 127, duration: 0.436s, episode steps:  72, steps per second: 165, episode reward: -33.002, mean reward: -0.458 [-100.000, 66.065], mean action: 1.569 [0.000, 3.000],  loss: 23.532098, mae: 33.369754, mean_q: -0.703040, mean_eps: 0.948257
  11632/200000: episode: 128, duration: 0.615s, episode steps:  97, steps per second: 158, episode reward: -107.050, mean reward: -1.104 [-100.000,  7.889], mean action: 1.546 [0.000, 3.000],  loss: 31.955963, mae: 35.076572, mean_q: -3.735126, mean_eps: 0.947877
  11712/200000: episode: 129, duration: 0.572s, episode steps:  80, steps per second: 140, episode reward: -144.066, mean reward: -1.801 [-100.000, 13.535], mean action: 1.300 [0.000, 3.000],  loss: 31.691891, mae: 34.521373, mean_q: -0.694110, mean_eps: 0.947478
  11811/200000: episode: 130, duration: 0.773s, episode steps:  99, steps per second: 128, episode reward: -32.095, mean reward: -0.324 [-100.000, 87.390], mean action: 1.485 [0.000, 3.000],  loss: 20.047705, mae: 33.862352, mean_q: 0.884648, mean_eps: 0.947075
  11901/200000: episode: 131, duration: 0.699s, episode steps:  90, steps per second: 129, episode reward: -268.457, mean reward: -2.983 [-100.000,  2.842], mean action: 1.544 [0.000, 3.000],  loss: 26.438255, mae: 33.708136, mean_q: -0.746102, mean_eps: 0.946650
  11968/200000: episode: 132, duration: 0.490s, episode steps:  67, steps per second: 137, episode reward: -92.757, mean reward: -1.384 [-100.000, 26.077], mean action: 1.746 [0.000, 3.000],  loss: 25.909390, mae: 34.290110, mean_q: 0.717005, mean_eps: 0.946297
  12087/200000: episode: 133, duration: 0.900s, episode steps: 119, steps per second: 132, episode reward: -87.272, mean reward: -0.733 [-100.000,  8.379], mean action: 1.403 [0.000, 3.000],  loss: 26.284802, mae: 34.790563, mean_q: -2.149354, mean_eps: 0.945878
  12223/200000: episode: 134, duration: 0.886s, episode steps: 136, steps per second: 154, episode reward: -45.329, mean reward: -0.333 [-100.000, 90.280], mean action: 1.449 [0.000, 3.000],  loss: 22.992036, mae: 34.454012, mean_q: 1.160359, mean_eps: 0.945305
  12352/200000: episode: 135, duration: 0.917s, episode steps: 129, steps per second: 141, episode reward: -29.503, mean reward: -0.229 [-100.000, 65.138], mean action: 1.426 [0.000, 3.000],  loss: 33.838096, mae: 34.112268, mean_q: 1.613600, mean_eps: 0.944709
  12466/200000: episode: 136, duration: 0.823s, episode steps: 114, steps per second: 138, episode reward: -170.493, mean reward: -1.496 [-100.000, 12.277], mean action: 1.614 [0.000, 3.000],  loss: 25.451067, mae: 34.284103, mean_q: 0.817890, mean_eps: 0.944162
  12562/200000: episode: 137, duration: 0.604s, episode steps:  96, steps per second: 159, episode reward: -129.291, mean reward: -1.347 [-100.000, 14.122], mean action: 1.708 [0.000, 3.000],  loss: 26.751986, mae: 34.837615, mean_q: 1.591185, mean_eps: 0.943689
  12631/200000: episode: 138, duration: 0.487s, episode steps:  69, steps per second: 142, episode reward: -80.540, mean reward: -1.167 [-100.000, 19.784], mean action: 1.522 [0.000, 3.000],  loss: 26.797823, mae: 34.389653, mean_q: 1.043086, mean_eps: 0.943318
  12706/200000: episode: 139, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -127.461, mean reward: -1.699 [-100.000, 12.388], mean action: 1.547 [0.000, 3.000],  loss: 20.361833, mae: 34.492446, mean_q: 2.558162, mean_eps: 0.942994
  12811/200000: episode: 140, duration: 0.692s, episode steps: 105, steps per second: 152, episode reward: -386.445, mean reward: -3.680 [-100.000,  0.795], mean action: 1.371 [0.000, 3.000],  loss: 23.167672, mae: 34.737676, mean_q: 1.656496, mean_eps: 0.942589
  12895/200000: episode: 141, duration: 0.559s, episode steps:  84, steps per second: 150, episode reward: -53.753, mean reward: -0.640 [-100.000, 56.328], mean action: 1.560 [0.000, 3.000],  loss: 17.492036, mae: 34.856725, mean_q: 0.310404, mean_eps: 0.942164
  13006/200000: episode: 142, duration: 0.726s, episode steps: 111, steps per second: 153, episode reward: -69.006, mean reward: -0.622 [-100.000, 117.549], mean action: 1.423 [0.000, 3.000],  loss: 17.411810, mae: 34.886646, mean_q: 1.583560, mean_eps: 0.941725
  13090/200000: episode: 143, duration: 0.590s, episode steps:  84, steps per second: 142, episode reward: -103.299, mean reward: -1.230 [-100.000,  6.785], mean action: 1.643 [0.000, 3.000],  loss: 21.747155, mae: 34.577394, mean_q: 4.185649, mean_eps: 0.941286
  13173/200000: episode: 144, duration: 0.588s, episode steps:  83, steps per second: 141, episode reward: -141.688, mean reward: -1.707 [-100.000, 30.511], mean action: 1.687 [0.000, 3.000],  loss: 25.773886, mae: 34.995417, mean_q: 1.353131, mean_eps: 0.940911
  13289/200000: episode: 145, duration: 0.735s, episode steps: 116, steps per second: 158, episode reward: -176.957, mean reward: -1.525 [-100.000, 14.989], mean action: 1.586 [0.000, 3.000],  loss: 20.720016, mae: 34.944101, mean_q: 2.560230, mean_eps: 0.940463
  13390/200000: episode: 146, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: -180.409, mean reward: -1.786 [-100.000,  7.994], mean action: 1.376 [0.000, 3.000],  loss: 17.161552, mae: 35.515435, mean_q: 1.464034, mean_eps: 0.939975
  13460/200000: episode: 147, duration: 0.454s, episode steps:  70, steps per second: 154, episode reward: -189.181, mean reward: -2.703 [-100.000, 29.851], mean action: 1.700 [0.000, 3.000],  loss: 40.278623, mae: 35.531162, mean_q: 1.326873, mean_eps: 0.939590
  13524/200000: episode: 148, duration: 0.420s, episode steps:  64, steps per second: 152, episode reward: -190.538, mean reward: -2.977 [-100.000,  6.056], mean action: 1.391 [0.000, 3.000],  loss: 20.360948, mae: 35.918887, mean_q: 2.592765, mean_eps: 0.939288
  13610/200000: episode: 149, duration: 0.544s, episode steps:  86, steps per second: 158, episode reward: -197.113, mean reward: -2.292 [-100.000, 34.842], mean action: 1.419 [0.000, 3.000],  loss: 21.914807, mae: 36.765188, mean_q: 2.390655, mean_eps: 0.938951
  13751/200000: episode: 150, duration: 0.862s, episode steps: 141, steps per second: 164, episode reward: -229.720, mean reward: -1.629 [-100.000, 13.683], mean action: 1.369 [0.000, 3.000],  loss: 23.058226, mae: 36.267165, mean_q: 3.125993, mean_eps: 0.938440
  13825/200000: episode: 151, duration: 0.455s, episode steps:  74, steps per second: 163, episode reward: -228.155, mean reward: -3.083 [-100.000, 13.633], mean action: 1.419 [0.000, 3.000],  loss: 30.905723, mae: 36.755234, mean_q: 3.526579, mean_eps: 0.937956
  13909/200000: episode: 152, duration: 0.481s, episode steps:  84, steps per second: 175, episode reward: -82.776, mean reward: -0.985 [-100.000, 16.870], mean action: 1.369 [0.000, 3.000],  loss: 27.550386, mae: 36.643249, mean_q: 1.959625, mean_eps: 0.937601
  14017/200000: episode: 153, duration: 0.657s, episode steps: 108, steps per second: 164, episode reward: -137.951, mean reward: -1.277 [-100.000, 18.647], mean action: 1.500 [0.000, 3.000],  loss: 21.517622, mae: 36.402974, mean_q: 4.493594, mean_eps: 0.937169
  14112/200000: episode: 154, duration: 0.548s, episode steps:  95, steps per second: 173, episode reward: -174.399, mean reward: -1.836 [-100.000,  7.692], mean action: 1.758 [0.000, 3.000],  loss: 30.050741, mae: 36.965104, mean_q: 4.604840, mean_eps: 0.936712
  14227/200000: episode: 155, duration: 0.635s, episode steps: 115, steps per second: 181, episode reward: -122.434, mean reward: -1.065 [-100.000,  5.874], mean action: 1.748 [0.000, 3.000],  loss: 30.248935, mae: 35.669948, mean_q: 4.684447, mean_eps: 0.936240
  14333/200000: episode: 156, duration: 0.617s, episode steps: 106, steps per second: 172, episode reward: -140.309, mean reward: -1.324 [-100.000, 12.967], mean action: 1.575 [0.000, 3.000],  loss: 21.703755, mae: 36.940114, mean_q: 3.779932, mean_eps: 0.935742
  14405/200000: episode: 157, duration: 0.452s, episode steps:  72, steps per second: 159, episode reward: -129.568, mean reward: -1.800 [-100.000,  5.444], mean action: 1.611 [0.000, 3.000],  loss: 26.791428, mae: 36.173612, mean_q: 7.065167, mean_eps: 0.935342
  14500/200000: episode: 158, duration: 0.570s, episode steps:  95, steps per second: 167, episode reward: -289.409, mean reward: -3.046 [-100.000,  0.146], mean action: 1.579 [0.000, 3.000],  loss: 29.903828, mae: 35.984937, mean_q: 5.478508, mean_eps: 0.934966
  14587/200000: episode: 159, duration: 0.495s, episode steps:  87, steps per second: 176, episode reward: -79.719, mean reward: -0.916 [-100.000, 13.339], mean action: 1.310 [0.000, 3.000],  loss: 29.160548, mae: 36.620452, mean_q: 5.216691, mean_eps: 0.934557
  14681/200000: episode: 160, duration: 0.521s, episode steps:  94, steps per second: 180, episode reward: -74.106, mean reward: -0.788 [-100.000, 15.100], mean action: 1.691 [0.000, 3.000],  loss: 31.096137, mae: 36.397030, mean_q: 7.820181, mean_eps: 0.934149
  14746/200000: episode: 161, duration: 0.403s, episode steps:  65, steps per second: 161, episode reward: -152.879, mean reward: -2.352 [-100.000,  8.468], mean action: 1.846 [0.000, 3.000],  loss: 20.043127, mae: 35.634802, mean_q: 7.620638, mean_eps: 0.933791
  14842/200000: episode: 162, duration: 0.581s, episode steps:  96, steps per second: 165, episode reward: -28.771, mean reward: -0.300 [-100.000, 98.195], mean action: 1.552 [0.000, 3.000],  loss: 20.727359, mae: 37.018085, mean_q: 5.109721, mean_eps: 0.933429
  14947/200000: episode: 163, duration: 0.624s, episode steps: 105, steps per second: 168, episode reward: -101.670, mean reward: -0.968 [-100.000, 11.998], mean action: 1.410 [0.000, 3.000],  loss: 20.213128, mae: 36.224122, mean_q: 7.765495, mean_eps: 0.932977
  15063/200000: episode: 164, duration: 0.671s, episode steps: 116, steps per second: 173, episode reward: -92.016, mean reward: -0.793 [-100.000, 11.934], mean action: 1.414 [0.000, 3.000],  loss: 32.537351, mae: 36.895378, mean_q: 9.241011, mean_eps: 0.932480
  15160/200000: episode: 165, duration: 0.596s, episode steps:  97, steps per second: 163, episode reward: -79.402, mean reward: -0.819 [-100.000,  9.768], mean action: 1.361 [0.000, 3.000],  loss: 26.275777, mae: 36.129418, mean_q: 8.900607, mean_eps: 0.932001
  15219/200000: episode: 166, duration: 0.346s, episode steps:  59, steps per second: 170, episode reward: -106.949, mean reward: -1.813 [-100.000, 48.192], mean action: 1.356 [0.000, 3.000],  loss: 20.972509, mae: 36.727233, mean_q: 7.918090, mean_eps: 0.931650
  15297/200000: episode: 167, duration: 0.445s, episode steps:  78, steps per second: 175, episode reward: -141.821, mean reward: -1.818 [-100.000,  7.275], mean action: 1.372 [0.000, 3.000],  loss: 23.335483, mae: 36.723353, mean_q: 8.066596, mean_eps: 0.931341
  15415/200000: episode: 168, duration: 0.675s, episode steps: 118, steps per second: 175, episode reward: -7.650, mean reward: -0.065 [-100.000, 82.596], mean action: 1.636 [0.000, 3.000],  loss: 21.598694, mae: 37.290137, mean_q: 8.224371, mean_eps: 0.930900
  15513/200000: episode: 169, duration: 0.595s, episode steps:  98, steps per second: 165, episode reward: -210.974, mean reward: -2.153 [-100.000,  7.312], mean action: 1.694 [0.000, 3.000],  loss: 23.675164, mae: 37.212189, mean_q: 7.053329, mean_eps: 0.930414
  15624/200000: episode: 170, duration: 0.628s, episode steps: 111, steps per second: 177, episode reward: -370.593, mean reward: -3.339 [-100.000, 71.199], mean action: 1.577 [0.000, 3.000],  loss: 18.945126, mae: 36.794904, mean_q: 9.247483, mean_eps: 0.929944
  15691/200000: episode: 171, duration: 0.375s, episode steps:  67, steps per second: 179, episode reward: -105.507, mean reward: -1.575 [-100.000, 12.897], mean action: 1.507 [0.000, 3.000],  loss: 31.396788, mae: 37.943173, mean_q: 8.346903, mean_eps: 0.929543
  15788/200000: episode: 172, duration: 0.543s, episode steps:  97, steps per second: 178, episode reward: -249.410, mean reward: -2.571 [-100.000,  0.508], mean action: 1.598 [0.000, 3.000],  loss: 24.050665, mae: 37.258303, mean_q: 9.530356, mean_eps: 0.929175
  15854/200000: episode: 173, duration: 0.403s, episode steps:  66, steps per second: 164, episode reward: -76.830, mean reward: -1.164 [-100.000,  7.001], mean action: 1.545 [0.000, 3.000],  loss: 25.816247, mae: 37.840384, mean_q: 7.317642, mean_eps: 0.928808
  15952/200000: episode: 174, duration: 0.576s, episode steps:  98, steps per second: 170, episode reward: -91.747, mean reward: -0.936 [-100.000,  6.061], mean action: 1.510 [0.000, 3.000],  loss: 23.865952, mae: 37.358409, mean_q: 9.172420, mean_eps: 0.928439
  16060/200000: episode: 175, duration: 0.610s, episode steps: 108, steps per second: 177, episode reward: -109.144, mean reward: -1.011 [-100.000, 16.222], mean action: 1.481 [0.000, 3.000],  loss: 22.195142, mae: 37.316234, mean_q: 9.474651, mean_eps: 0.927975
  16166/200000: episode: 176, duration: 0.600s, episode steps: 106, steps per second: 177, episode reward: -171.068, mean reward: -1.614 [-100.000,  5.344], mean action: 1.481 [0.000, 3.000],  loss: 20.937224, mae: 37.182664, mean_q: 8.154285, mean_eps: 0.927494
  16281/200000: episode: 177, duration: 0.696s, episode steps: 115, steps per second: 165, episode reward: -96.744, mean reward: -0.841 [-100.000,  9.750], mean action: 1.339 [0.000, 3.000],  loss: 23.297665, mae: 38.223621, mean_q: 8.919311, mean_eps: 0.926997
  16374/200000: episode: 178, duration: 0.541s, episode steps:  93, steps per second: 172, episode reward: -64.203, mean reward: -0.690 [-100.000, 46.671], mean action: 1.548 [0.000, 3.000],  loss: 23.484331, mae: 37.266343, mean_q: 9.548522, mean_eps: 0.926529
  16465/200000: episode: 179, duration: 0.510s, episode steps:  91, steps per second: 179, episode reward: -118.024, mean reward: -1.297 [-100.000,  7.086], mean action: 1.451 [0.000, 3.000],  loss: 15.666110, mae: 37.899459, mean_q: 10.116299, mean_eps: 0.926115
  16560/200000: episode: 180, duration: 0.540s, episode steps:  95, steps per second: 176, episode reward: -95.410, mean reward: -1.004 [-100.000, 11.464], mean action: 1.516 [0.000, 3.000],  loss: 16.382146, mae: 37.740022, mean_q: 10.208992, mean_eps: 0.925696
  16637/200000: episode: 181, duration: 0.483s, episode steps:  77, steps per second: 159, episode reward: -100.831, mean reward: -1.309 [-100.000,  7.789], mean action: 1.494 [0.000, 3.000],  loss: 31.947784, mae: 37.808094, mean_q: 11.296263, mean_eps: 0.925309
  16705/200000: episode: 182, duration: 0.394s, episode steps:  68, steps per second: 173, episode reward: -87.477, mean reward: -1.286 [-100.000, 27.329], mean action: 1.529 [0.000, 3.000],  loss: 14.561940, mae: 37.452933, mean_q: 9.600087, mean_eps: 0.924983
  16839/200000: episode: 183, duration: 0.763s, episode steps: 134, steps per second: 176, episode reward: -193.455, mean reward: -1.444 [-100.000,  3.272], mean action: 1.567 [0.000, 3.000],  loss: 22.805501, mae: 37.102563, mean_q: 11.131860, mean_eps: 0.924528
  16961/200000: episode: 184, duration: 0.710s, episode steps: 122, steps per second: 172, episode reward: -120.319, mean reward: -0.986 [-100.000, 18.074], mean action: 1.525 [0.000, 3.000],  loss: 18.894287, mae: 37.724047, mean_q: 10.896500, mean_eps: 0.923952
  17027/200000: episode: 185, duration: 0.401s, episode steps:  66, steps per second: 165, episode reward: -91.507, mean reward: -1.386 [-100.000, 10.831], mean action: 1.545 [0.000, 3.000],  loss: 27.084549, mae: 37.943076, mean_q: 11.097883, mean_eps: 0.923529
  17126/200000: episode: 186, duration: 0.565s, episode steps:  99, steps per second: 175, episode reward: -146.981, mean reward: -1.485 [-100.000, 23.732], mean action: 1.455 [0.000, 3.000],  loss: 10.145442, mae: 37.802342, mean_q: 12.718777, mean_eps: 0.923158
  17217/200000: episode: 187, duration: 0.515s, episode steps:  91, steps per second: 177, episode reward: -130.207, mean reward: -1.431 [-100.000, 38.674], mean action: 1.473 [0.000, 3.000],  loss: 17.743354, mae: 37.464506, mean_q: 11.852394, mean_eps: 0.922731
  17297/200000: episode: 188, duration: 0.470s, episode steps:  80, steps per second: 170, episode reward: -192.056, mean reward: -2.401 [-100.000,  4.767], mean action: 1.375 [0.000, 3.000],  loss: 22.493689, mae: 38.059348, mean_q: 12.459258, mean_eps: 0.922346
  17378/200000: episode: 189, duration: 0.497s, episode steps:  81, steps per second: 163, episode reward: -101.697, mean reward: -1.256 [-100.000,  5.875], mean action: 1.457 [0.000, 3.000],  loss: 27.532684, mae: 37.821295, mean_q: 15.385966, mean_eps: 0.921983
  17455/200000: episode: 190, duration: 0.448s, episode steps:  77, steps per second: 172, episode reward: -71.007, mean reward: -0.922 [-100.000, 10.445], mean action: 1.364 [0.000, 3.000],  loss: 25.550736, mae: 38.100368, mean_q: 16.298336, mean_eps: 0.921628
  17579/200000: episode: 191, duration: 0.745s, episode steps: 124, steps per second: 166, episode reward: -280.315, mean reward: -2.261 [-100.000, 14.962], mean action: 1.589 [0.000, 3.000],  loss: 25.102273, mae: 37.572879, mean_q: 13.832275, mean_eps: 0.921176
  17645/200000: episode: 192, duration: 0.424s, episode steps:  66, steps per second: 156, episode reward: -105.364, mean reward: -1.596 [-100.000, 50.101], mean action: 1.333 [0.000, 3.000],  loss: 26.678624, mae: 37.549653, mean_q: 11.752723, mean_eps: 0.920748
  17726/200000: episode: 193, duration: 0.560s, episode steps:  81, steps per second: 145, episode reward: -146.068, mean reward: -1.803 [-100.000,  4.706], mean action: 1.519 [0.000, 3.000],  loss: 16.683998, mae: 37.231986, mean_q: 13.710040, mean_eps: 0.920417
  17837/200000: episode: 194, duration: 0.702s, episode steps: 111, steps per second: 158, episode reward: -85.251, mean reward: -0.768 [-100.000, 38.861], mean action: 1.550 [0.000, 3.000],  loss: 24.604468, mae: 37.579585, mean_q: 12.963594, mean_eps: 0.919985
  17941/200000: episode: 195, duration: 0.695s, episode steps: 104, steps per second: 150, episode reward: -113.514, mean reward: -1.091 [-100.000,  7.644], mean action: 1.615 [0.000, 3.000],  loss: 21.810334, mae: 37.187279, mean_q: 15.194175, mean_eps: 0.919502
  18006/200000: episode: 196, duration: 0.449s, episode steps:  65, steps per second: 145, episode reward: -18.353, mean reward: -0.282 [-100.000, 48.930], mean action: 1.415 [0.000, 3.000],  loss: 16.129849, mae: 36.518343, mean_q: 12.366421, mean_eps: 0.919121
  18079/200000: episode: 197, duration: 0.502s, episode steps:  73, steps per second: 145, episode reward: -39.309, mean reward: -0.538 [-100.000, 12.463], mean action: 1.548 [0.000, 3.000],  loss: 11.683355, mae: 37.528788, mean_q: 14.986752, mean_eps: 0.918811
  18152/200000: episode: 198, duration: 0.474s, episode steps:  73, steps per second: 154, episode reward: -108.534, mean reward: -1.487 [-100.000,  7.840], mean action: 1.589 [0.000, 3.000],  loss: 17.227939, mae: 37.291390, mean_q: 15.468201, mean_eps: 0.918482
  18230/200000: episode: 199, duration: 0.492s, episode steps:  78, steps per second: 159, episode reward: -352.259, mean reward: -4.516 [-100.000, 19.422], mean action: 1.603 [0.000, 3.000],  loss: 21.736660, mae: 37.464794, mean_q: 13.646250, mean_eps: 0.918143
  18348/200000: episode: 200, duration: 0.769s, episode steps: 118, steps per second: 153, episode reward: -190.301, mean reward: -1.613 [-100.000, 110.534], mean action: 1.254 [0.000, 3.000],  loss: 23.938788, mae: 38.253444, mean_q: 13.811038, mean_eps: 0.917702
  18462/200000: episode: 201, duration: 0.732s, episode steps: 114, steps per second: 156, episode reward: -324.007, mean reward: -2.842 [-100.000, 122.823], mean action: 1.544 [0.000, 3.000],  loss: 34.739755, mae: 38.202127, mean_q: 13.567987, mean_eps: 0.917180
  18553/200000: episode: 202, duration: 0.638s, episode steps:  91, steps per second: 143, episode reward: -142.360, mean reward: -1.564 [-100.000,  8.131], mean action: 1.385 [0.000, 3.000],  loss: 13.354993, mae: 37.879732, mean_q: 13.852474, mean_eps: 0.916719
  18696/200000: episode: 203, duration: 1.292s, episode steps: 143, steps per second: 111, episode reward: 38.215, mean reward:  0.267 [-100.000, 92.495], mean action: 1.531 [0.000, 3.000],  loss: 19.016716, mae: 37.745256, mean_q: 13.953717, mean_eps: 0.916192
  18827/200000: episode: 204, duration: 1.042s, episode steps: 131, steps per second: 126, episode reward: 12.993, mean reward:  0.099 [-100.000, 91.299], mean action: 1.435 [0.000, 3.000],  loss: 28.003334, mae: 39.196942, mean_q: 13.084953, mean_eps: 0.915576
  18922/200000: episode: 205, duration: 1.040s, episode steps:  95, steps per second:  91, episode reward: -119.144, mean reward: -1.254 [-100.000, 74.811], mean action: 1.611 [0.000, 3.000],  loss: 25.927083, mae: 39.224464, mean_q: 11.663263, mean_eps: 0.915067
  18994/200000: episode: 206, duration: 0.536s, episode steps:  72, steps per second: 134, episode reward: -144.701, mean reward: -2.010 [-100.000, 27.625], mean action: 1.444 [0.000, 3.000],  loss: 12.038759, mae: 39.845264, mean_q: 11.024710, mean_eps: 0.914691
  19060/200000: episode: 207, duration: 0.504s, episode steps:  66, steps per second: 131, episode reward: -52.032, mean reward: -0.788 [-100.000, 17.965], mean action: 1.470 [0.000, 3.000],  loss: 19.313088, mae: 39.101238, mean_q: 16.157373, mean_eps: 0.914381
  19138/200000: episode: 208, duration: 0.616s, episode steps:  78, steps per second: 127, episode reward: -109.825, mean reward: -1.408 [-100.000,  7.247], mean action: 1.462 [0.000, 3.000],  loss: 16.673726, mae: 38.935371, mean_q: 15.186695, mean_eps: 0.914057
  19254/200000: episode: 209, duration: 0.726s, episode steps: 116, steps per second: 160, episode reward: -165.282, mean reward: -1.425 [-100.000, 10.922], mean action: 1.431 [0.000, 3.000],  loss: 23.636469, mae: 39.914174, mean_q: 16.142189, mean_eps: 0.913620
  19352/200000: episode: 210, duration: 0.724s, episode steps:  98, steps per second: 135, episode reward: -73.274, mean reward: -0.748 [-100.000, 16.714], mean action: 1.551 [0.000, 3.000],  loss: 21.159865, mae: 40.338474, mean_q: 15.141356, mean_eps: 0.913139
  19413/200000: episode: 211, duration: 0.418s, episode steps:  61, steps per second: 146, episode reward: -76.879, mean reward: -1.260 [-100.000,  6.762], mean action: 1.689 [0.000, 3.000],  loss: 28.661191, mae: 40.793339, mean_q: 15.406305, mean_eps: 0.912781
  19543/200000: episode: 212, duration: 0.850s, episode steps: 130, steps per second: 153, episode reward: -228.690, mean reward: -1.759 [-100.000,  7.720], mean action: 1.546 [0.000, 3.000],  loss: 24.389989, mae: 39.287060, mean_q: 16.217166, mean_eps: 0.912351
  19629/200000: episode: 213, duration: 0.543s, episode steps:  86, steps per second: 158, episode reward: -98.119, mean reward: -1.141 [-100.000,  7.982], mean action: 1.570 [0.000, 3.000],  loss: 23.576824, mae: 38.959201, mean_q: 14.224337, mean_eps: 0.911865
  19747/200000: episode: 214, duration: 0.843s, episode steps: 118, steps per second: 140, episode reward: -120.792, mean reward: -1.024 [-100.000,  7.878], mean action: 1.390 [0.000, 3.000],  loss: 15.486393, mae: 40.101338, mean_q: 15.538243, mean_eps: 0.911406
  19813/200000: episode: 215, duration: 0.422s, episode steps:  66, steps per second: 157, episode reward: -67.319, mean reward: -1.020 [-100.000, 17.112], mean action: 1.470 [0.000, 3.000],  loss: 22.474314, mae: 39.912599, mean_q: 16.689753, mean_eps: 0.910992
  19946/200000: episode: 216, duration: 0.787s, episode steps: 133, steps per second: 169, episode reward: -130.653, mean reward: -0.982 [-100.000, 11.415], mean action: 1.586 [0.000, 3.000],  loss: 15.409202, mae: 40.529002, mean_q: 16.953107, mean_eps: 0.910544
  20020/200000: episode: 217, duration: 0.451s, episode steps:  74, steps per second: 164, episode reward: -1.752, mean reward: -0.024 [-100.000, 81.061], mean action: 1.608 [0.000, 3.000],  loss: 28.693493, mae: 41.463102, mean_q: 18.203896, mean_eps: 0.910079
  20146/200000: episode: 218, duration: 0.754s, episode steps: 126, steps per second: 167, episode reward: -74.679, mean reward: -0.593 [-100.000, 10.416], mean action: 1.508 [0.000, 3.000],  loss: 21.861505, mae: 41.035984, mean_q: 17.375855, mean_eps: 0.909629
  20270/200000: episode: 219, duration: 0.771s, episode steps: 124, steps per second: 161, episode reward: -154.253, mean reward: -1.244 [-100.000,  8.750], mean action: 1.774 [0.000, 3.000],  loss: 22.069191, mae: 41.054404, mean_q: 18.939204, mean_eps: 0.909066
  20342/200000: episode: 220, duration: 0.423s, episode steps:  72, steps per second: 170, episode reward: -128.202, mean reward: -1.781 [-100.000, 10.965], mean action: 1.542 [0.000, 3.000],  loss: 18.880861, mae: 41.093507, mean_q: 17.618992, mean_eps: 0.908625
  20444/200000: episode: 221, duration: 0.613s, episode steps: 102, steps per second: 166, episode reward: -139.304, mean reward: -1.366 [-100.000,  7.613], mean action: 1.510 [0.000, 3.000],  loss: 23.574472, mae: 39.600475, mean_q: 16.557953, mean_eps: 0.908234
  20553/200000: episode: 222, duration: 0.668s, episode steps: 109, steps per second: 163, episode reward: -59.768, mean reward: -0.548 [-100.000,  7.904], mean action: 1.670 [0.000, 3.000],  loss: 19.503860, mae: 40.829431, mean_q: 16.358891, mean_eps: 0.907759
  20666/200000: episode: 223, duration: 0.746s, episode steps: 113, steps per second: 152, episode reward: -98.806, mean reward: -0.874 [-100.000, 11.839], mean action: 1.602 [0.000, 3.000],  loss: 23.338530, mae: 40.606751, mean_q: 15.935779, mean_eps: 0.907259
  20761/200000: episode: 224, duration: 0.706s, episode steps:  95, steps per second: 135, episode reward: -161.638, mean reward: -1.701 [-100.000,  7.082], mean action: 1.547 [0.000, 3.000],  loss: 17.783765, mae: 40.467895, mean_q: 18.397673, mean_eps: 0.906791
  20890/200000: episode: 225, duration: 0.885s, episode steps: 129, steps per second: 146, episode reward: -141.305, mean reward: -1.095 [-100.000,  7.989], mean action: 1.512 [0.000, 3.000],  loss: 15.832887, mae: 40.719713, mean_q: 18.829766, mean_eps: 0.906288
  20949/200000: episode: 226, duration: 0.385s, episode steps:  59, steps per second: 153, episode reward: -93.088, mean reward: -1.578 [-100.000, 17.105], mean action: 1.542 [0.000, 3.000],  loss: 19.064742, mae: 41.079097, mean_q: 19.542812, mean_eps: 0.905864
  21044/200000: episode: 227, duration: 0.646s, episode steps:  95, steps per second: 147, episode reward: -366.225, mean reward: -3.855 [-100.000, 82.019], mean action: 1.537 [0.000, 3.000],  loss: 22.894205, mae: 41.427130, mean_q: 21.355987, mean_eps: 0.905518
  21173/200000: episode: 228, duration: 0.795s, episode steps: 129, steps per second: 162, episode reward: -377.256, mean reward: -2.924 [-100.000, 98.529], mean action: 1.550 [0.000, 3.000],  loss: 25.507499, mae: 41.745791, mean_q: 19.057599, mean_eps: 0.905014
  21258/200000: episode: 229, duration: 0.516s, episode steps:  85, steps per second: 165, episode reward: 34.550, mean reward:  0.406 [-100.000, 112.411], mean action: 1.447 [0.000, 3.000],  loss: 19.873148, mae: 42.305839, mean_q: 18.316112, mean_eps: 0.904532
  21339/200000: episode: 230, duration: 0.593s, episode steps:  81, steps per second: 137, episode reward: -138.174, mean reward: -1.706 [-100.000, 10.938], mean action: 1.444 [0.000, 3.000],  loss: 20.647402, mae: 41.709228, mean_q: 17.055605, mean_eps: 0.904159
  21427/200000: episode: 231, duration: 0.575s, episode steps:  88, steps per second: 153, episode reward: -205.780, mean reward: -2.338 [-100.000,  4.999], mean action: 1.568 [0.000, 3.000],  loss: 15.490589, mae: 41.537011, mean_q: 19.244654, mean_eps: 0.903779
  21499/200000: episode: 232, duration: 0.516s, episode steps:  72, steps per second: 140, episode reward: -145.102, mean reward: -2.015 [-100.000,  6.928], mean action: 1.583 [0.000, 3.000],  loss: 23.871560, mae: 41.766546, mean_q: 17.467140, mean_eps: 0.903419
  21591/200000: episode: 233, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -172.889, mean reward: -1.879 [-100.000, 12.444], mean action: 1.228 [0.000, 3.000],  loss: 8.738252, mae: 40.963243, mean_q: 18.799233, mean_eps: 0.903050
  21688/200000: episode: 234, duration: 0.608s, episode steps:  97, steps per second: 160, episode reward: -131.706, mean reward: -1.358 [-100.000, 34.657], mean action: 1.598 [0.000, 3.000],  loss: 20.353356, mae: 41.131012, mean_q: 18.152779, mean_eps: 0.902624
  21773/200000: episode: 235, duration: 0.556s, episode steps:  85, steps per second: 153, episode reward: -114.639, mean reward: -1.349 [-100.000, 38.432], mean action: 1.882 [0.000, 3.000],  loss: 22.056542, mae: 41.825058, mean_q: 18.003591, mean_eps: 0.902215
  21851/200000: episode: 236, duration: 0.588s, episode steps:  78, steps per second: 133, episode reward: -89.485, mean reward: -1.147 [-100.000, 14.694], mean action: 1.449 [0.000, 3.000],  loss: 14.608058, mae: 40.918923, mean_q: 20.012621, mean_eps: 0.901848
  21930/200000: episode: 237, duration: 0.525s, episode steps:  79, steps per second: 150, episode reward: -300.163, mean reward: -3.800 [-100.000,  6.321], mean action: 1.494 [0.000, 3.000],  loss: 15.210651, mae: 41.052660, mean_q: 18.375444, mean_eps: 0.901495
  22060/200000: episode: 238, duration: 0.947s, episode steps: 130, steps per second: 137, episode reward: -322.478, mean reward: -2.481 [-100.000, 98.849], mean action: 1.469 [0.000, 3.000],  loss: 16.824726, mae: 40.501234, mean_q: 18.626627, mean_eps: 0.901025
  22143/200000: episode: 239, duration: 0.621s, episode steps:  83, steps per second: 134, episode reward: -116.970, mean reward: -1.409 [-100.000,  7.211], mean action: 1.386 [0.000, 3.000],  loss: 18.340657, mae: 41.236225, mean_q: 19.324111, mean_eps: 0.900546
  22217/200000: episode: 240, duration: 0.475s, episode steps:  74, steps per second: 156, episode reward: -83.772, mean reward: -1.132 [-100.000,  9.430], mean action: 1.554 [0.000, 3.000],  loss: 17.889157, mae: 41.161716, mean_q: 19.448212, mean_eps: 0.900192
  22301/200000: episode: 241, duration: 0.502s, episode steps:  84, steps per second: 167, episode reward: -161.895, mean reward: -1.927 [-100.000, 20.704], mean action: 1.714 [0.000, 3.000],  loss: 16.377310, mae: 40.930850, mean_q: 19.317218, mean_eps: 0.899837
  22406/200000: episode: 242, duration: 0.608s, episode steps: 105, steps per second: 173, episode reward: -96.414, mean reward: -0.918 [-100.000, 12.564], mean action: 1.390 [0.000, 3.000],  loss: 19.582613, mae: 41.533369, mean_q: 19.341575, mean_eps: 0.899411
  22520/200000: episode: 243, duration: 0.730s, episode steps: 114, steps per second: 156, episode reward: -146.051, mean reward: -1.281 [-100.000, 18.006], mean action: 1.447 [0.000, 3.000],  loss: 18.306316, mae: 41.393395, mean_q: 20.365554, mean_eps: 0.898919
  22625/200000: episode: 244, duration: 0.616s, episode steps: 105, steps per second: 170, episode reward: -79.989, mean reward: -0.762 [-100.000, 21.688], mean action: 1.600 [0.000, 3.000],  loss: 18.504235, mae: 41.118467, mean_q: 20.504734, mean_eps: 0.898426
  22689/200000: episode: 245, duration: 0.367s, episode steps:  64, steps per second: 175, episode reward: -93.037, mean reward: -1.454 [-100.000,  6.173], mean action: 1.359 [0.000, 3.000],  loss: 15.275558, mae: 40.692600, mean_q: 21.050939, mean_eps: 0.898046
  22807/200000: episode: 246, duration: 0.682s, episode steps: 118, steps per second: 173, episode reward: -191.987, mean reward: -1.627 [-100.000, 12.333], mean action: 1.492 [0.000, 3.000],  loss: 17.726165, mae: 40.938286, mean_q: 19.511289, mean_eps: 0.897636
  22881/200000: episode: 247, duration: 0.471s, episode steps:  74, steps per second: 157, episode reward: -69.059, mean reward: -0.933 [-100.000, 16.203], mean action: 1.649 [0.000, 3.000],  loss: 15.722615, mae: 41.395782, mean_q: 19.840352, mean_eps: 0.897204
  22964/200000: episode: 248, duration: 0.534s, episode steps:  83, steps per second: 156, episode reward: -354.771, mean reward: -4.274 [-100.000, 36.380], mean action: 1.747 [0.000, 3.000],  loss: 18.131511, mae: 41.787101, mean_q: 18.489973, mean_eps: 0.896851
  23034/200000: episode: 249, duration: 0.414s, episode steps:  70, steps per second: 169, episode reward: -99.651, mean reward: -1.424 [-100.000,  8.364], mean action: 1.586 [0.000, 3.000],  loss: 22.262827, mae: 41.412001, mean_q: 20.904651, mean_eps: 0.896507
  23144/200000: episode: 250, duration: 0.664s, episode steps: 110, steps per second: 166, episode reward: -91.907, mean reward: -0.836 [-100.000, 54.803], mean action: 1.582 [0.000, 3.000],  loss: 12.114080, mae: 40.233434, mean_q: 20.539324, mean_eps: 0.896102
  23242/200000: episode: 251, duration: 0.635s, episode steps:  98, steps per second: 154, episode reward: -107.651, mean reward: -1.098 [-100.000, 18.810], mean action: 1.673 [0.000, 3.000],  loss: 18.806892, mae: 40.361213, mean_q: 21.463920, mean_eps: 0.895634
  23347/200000: episode: 252, duration: 0.698s, episode steps: 105, steps per second: 151, episode reward: -96.867, mean reward: -0.923 [-100.000, 13.450], mean action: 1.352 [0.000, 3.000],  loss: 20.282721, mae: 39.407240, mean_q: 18.375929, mean_eps: 0.895177
  23466/200000: episode: 253, duration: 0.750s, episode steps: 119, steps per second: 159, episode reward: -74.893, mean reward: -0.629 [-100.000, 96.074], mean action: 1.437 [0.000, 3.000],  loss: 18.688711, mae: 40.463265, mean_q: 20.375949, mean_eps: 0.894673
  23566/200000: episode: 254, duration: 0.739s, episode steps: 100, steps per second: 135, episode reward: -255.298, mean reward: -2.553 [-100.000,  0.930], mean action: 1.580 [0.000, 3.000],  loss: 20.218478, mae: 40.983715, mean_q: 18.086751, mean_eps: 0.894180
  23633/200000: episode: 255, duration: 0.401s, episode steps:  67, steps per second: 167, episode reward: -130.414, mean reward: -1.946 [-100.000,  5.388], mean action: 1.493 [0.000, 3.000],  loss: 23.766801, mae: 39.609677, mean_q: 18.785262, mean_eps: 0.893805
  23716/200000: episode: 256, duration: 0.482s, episode steps:  83, steps per second: 172, episode reward: -109.982, mean reward: -1.325 [-100.000, 17.559], mean action: 1.771 [0.000, 3.000],  loss: 18.339822, mae: 40.754268, mean_q: 19.025205, mean_eps: 0.893467
  23809/200000: episode: 257, duration: 0.594s, episode steps:  93, steps per second: 157, episode reward: -110.399, mean reward: -1.187 [-100.000,  7.559], mean action: 1.452 [0.000, 3.000],  loss: 13.365561, mae: 39.982414, mean_q: 21.876097, mean_eps: 0.893071
  23909/200000: episode: 258, duration: 0.708s, episode steps: 100, steps per second: 141, episode reward: -216.846, mean reward: -2.168 [-100.000,  5.293], mean action: 1.450 [0.000, 3.000],  loss: 19.027851, mae: 40.041081, mean_q: 20.880348, mean_eps: 0.892637
  24007/200000: episode: 259, duration: 0.650s, episode steps:  98, steps per second: 151, episode reward: -76.352, mean reward: -0.779 [-100.000, 17.006], mean action: 1.296 [0.000, 3.000],  loss: 15.758527, mae: 40.519369, mean_q: 20.030625, mean_eps: 0.892191
  24117/200000: episode: 260, duration: 0.636s, episode steps: 110, steps per second: 173, episode reward:  1.045, mean reward:  0.009 [-100.000, 95.313], mean action: 1.600 [0.000, 3.000],  loss: 13.536992, mae: 39.801218, mean_q: 20.985723, mean_eps: 0.891723
  24212/200000: episode: 261, duration: 0.689s, episode steps:  95, steps per second: 138, episode reward: -15.451, mean reward: -0.163 [-100.000, 108.195], mean action: 1.516 [0.000, 3.000],  loss: 16.157492, mae: 40.482910, mean_q: 20.089379, mean_eps: 0.891262
  24287/200000: episode: 262, duration: 0.476s, episode steps:  75, steps per second: 158, episode reward: -83.215, mean reward: -1.110 [-100.000, 53.923], mean action: 1.413 [0.000, 3.000],  loss: 15.603712, mae: 40.989623, mean_q: 22.032882, mean_eps: 0.890879
  24398/200000: episode: 263, duration: 0.679s, episode steps: 111, steps per second: 164, episode reward: -100.587, mean reward: -0.906 [-100.000, 10.000], mean action: 1.604 [0.000, 3.000],  loss: 18.898858, mae: 40.801286, mean_q: 20.734814, mean_eps: 0.890461
  24478/200000: episode: 264, duration: 0.476s, episode steps:  80, steps per second: 168, episode reward: -115.352, mean reward: -1.442 [-100.000,  9.968], mean action: 1.438 [0.000, 3.000],  loss: 17.028705, mae: 41.207478, mean_q: 20.797337, mean_eps: 0.890031
  24616/200000: episode: 265, duration: 0.862s, episode steps: 138, steps per second: 160, episode reward: -275.079, mean reward: -1.993 [-100.000, 49.857], mean action: 1.601 [0.000, 3.000],  loss: 11.716113, mae: 41.409848, mean_q: 21.960129, mean_eps: 0.889541
  24753/200000: episode: 266, duration: 0.920s, episode steps: 137, steps per second: 149, episode reward: -36.155, mean reward: -0.264 [-100.000, 74.132], mean action: 1.555 [0.000, 3.000],  loss: 15.214002, mae: 42.050374, mean_q: 22.907435, mean_eps: 0.888922
  24837/200000: episode: 267, duration: 0.523s, episode steps:  84, steps per second: 160, episode reward: -60.592, mean reward: -0.721 [-100.000,  7.464], mean action: 1.655 [0.000, 3.000],  loss: 12.203769, mae: 42.682970, mean_q: 24.459779, mean_eps: 0.888425
  24933/200000: episode: 268, duration: 0.644s, episode steps:  96, steps per second: 149, episode reward: -129.611, mean reward: -1.350 [-100.000, 15.990], mean action: 1.458 [0.000, 3.000],  loss: 22.374601, mae: 42.061928, mean_q: 23.537894, mean_eps: 0.888020
  25041/200000: episode: 269, duration: 0.667s, episode steps: 108, steps per second: 162, episode reward: -172.328, mean reward: -1.596 [-100.000,  3.819], mean action: 1.639 [0.000, 3.000],  loss: 18.168728, mae: 42.396705, mean_q: 25.244395, mean_eps: 0.887561
  25169/200000: episode: 270, duration: 0.769s, episode steps: 128, steps per second: 166, episode reward: -42.411, mean reward: -0.331 [-100.000,  9.297], mean action: 1.477 [0.000, 3.000],  loss: 17.387207, mae: 41.762702, mean_q: 23.297644, mean_eps: 0.887030
  25245/200000: episode: 271, duration: 0.489s, episode steps:  76, steps per second: 156, episode reward: -76.934, mean reward: -1.012 [-100.000, 11.605], mean action: 1.750 [0.000, 3.000],  loss: 9.296431, mae: 41.914830, mean_q: 21.551127, mean_eps: 0.886571
  25424/200000: episode: 272, duration: 1.165s, episode steps: 179, steps per second: 154, episode reward: -312.218, mean reward: -1.744 [-100.000, 99.573], mean action: 1.581 [0.000, 3.000],  loss: 17.008000, mae: 41.966013, mean_q: 24.154727, mean_eps: 0.885997
  25549/200000: episode: 273, duration: 0.903s, episode steps: 125, steps per second: 138, episode reward: -172.441, mean reward: -1.380 [-100.000,  9.412], mean action: 1.616 [0.000, 3.000],  loss: 14.324154, mae: 42.149144, mean_q: 24.059556, mean_eps: 0.885313
  25668/200000: episode: 274, duration: 0.778s, episode steps: 119, steps per second: 153, episode reward: -106.758, mean reward: -0.897 [-100.000,  9.618], mean action: 1.563 [0.000, 3.000],  loss: 17.554160, mae: 42.653080, mean_q: 23.856028, mean_eps: 0.884764
  25781/200000: episode: 275, duration: 0.827s, episode steps: 113, steps per second: 137, episode reward: -243.222, mean reward: -2.152 [-100.000, 13.227], mean action: 1.575 [0.000, 3.000],  loss: 16.064374, mae: 43.243635, mean_q: 26.039791, mean_eps: 0.884242
  25872/200000: episode: 276, duration: 0.706s, episode steps:  91, steps per second: 129, episode reward: -271.129, mean reward: -2.979 [-100.000, 108.898], mean action: 1.549 [0.000, 3.000],  loss: 15.061574, mae: 43.733922, mean_q: 24.998094, mean_eps: 0.883783
  25953/200000: episode: 277, duration: 0.563s, episode steps:  81, steps per second: 144, episode reward: -87.160, mean reward: -1.076 [-100.000, 52.113], mean action: 1.630 [0.000, 3.000],  loss: 17.295969, mae: 41.978667, mean_q: 24.632015, mean_eps: 0.883396
  26030/200000: episode: 278, duration: 0.524s, episode steps:  77, steps per second: 147, episode reward: -247.290, mean reward: -3.212 [-100.000,  4.760], mean action: 1.455 [0.000, 3.000],  loss: 13.374722, mae: 43.146455, mean_q: 27.739073, mean_eps: 0.883041
  26139/200000: episode: 279, duration: 0.781s, episode steps: 109, steps per second: 139, episode reward: -92.262, mean reward: -0.846 [-100.000,  7.094], mean action: 1.661 [0.000, 3.000],  loss: 24.181610, mae: 43.101135, mean_q: 26.138563, mean_eps: 0.882622
  26241/200000: episode: 280, duration: 0.797s, episode steps: 102, steps per second: 128, episode reward: -155.118, mean reward: -1.521 [-100.000, 13.568], mean action: 1.618 [0.000, 3.000],  loss: 17.106520, mae: 42.704873, mean_q: 26.961962, mean_eps: 0.882147
  26346/200000: episode: 281, duration: 0.771s, episode steps: 105, steps per second: 136, episode reward: -106.659, mean reward: -1.016 [-100.000,  8.519], mean action: 1.486 [0.000, 3.000],  loss: 18.332016, mae: 42.311612, mean_q: 22.918175, mean_eps: 0.881681
  26450/200000: episode: 282, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -161.839, mean reward: -1.556 [-100.000,  7.850], mean action: 1.567 [0.000, 3.000],  loss: 23.744916, mae: 43.357920, mean_q: 27.756207, mean_eps: 0.881211
  26558/200000: episode: 283, duration: 0.765s, episode steps: 108, steps per second: 141, episode reward: -211.828, mean reward: -1.961 [-100.000,  8.981], mean action: 1.657 [0.000, 3.000],  loss: 15.876767, mae: 43.069914, mean_q: 26.177562, mean_eps: 0.880734
  26641/200000: episode: 284, duration: 0.692s, episode steps:  83, steps per second: 120, episode reward: -102.655, mean reward: -1.237 [-100.000,  7.361], mean action: 1.554 [0.000, 3.000],  loss: 20.407684, mae: 42.535073, mean_q: 26.271936, mean_eps: 0.880305
  26722/200000: episode: 285, duration: 0.998s, episode steps:  81, steps per second:  81, episode reward: 16.755, mean reward:  0.207 [-100.000, 130.687], mean action: 1.506 [0.000, 3.000],  loss: 16.068602, mae: 42.574589, mean_q: 28.767652, mean_eps: 0.879935
  26810/200000: episode: 286, duration: 0.718s, episode steps:  88, steps per second: 123, episode reward: -95.971, mean reward: -1.091 [-100.000, 11.238], mean action: 1.523 [0.000, 3.000],  loss: 21.611717, mae: 42.745453, mean_q: 27.014689, mean_eps: 0.879555
  26873/200000: episode: 287, duration: 0.454s, episode steps:  63, steps per second: 139, episode reward: -116.761, mean reward: -1.853 [-100.000,  7.607], mean action: 1.429 [0.000, 3.000],  loss: 14.091040, mae: 42.371070, mean_q: 28.449284, mean_eps: 0.879216
  26956/200000: episode: 288, duration: 0.663s, episode steps:  83, steps per second: 125, episode reward: -106.594, mean reward: -1.284 [-100.000,  6.012], mean action: 1.506 [0.000, 3.000],  loss: 17.527946, mae: 42.146996, mean_q: 27.307948, mean_eps: 0.878887
  27071/200000: episode: 289, duration: 1.076s, episode steps: 115, steps per second: 107, episode reward: -56.893, mean reward: -0.495 [-100.000, 13.905], mean action: 1.565 [0.000, 3.000],  loss: 13.774064, mae: 42.084998, mean_q: 29.457428, mean_eps: 0.878442
  27146/200000: episode: 290, duration: 0.717s, episode steps:  75, steps per second: 105, episode reward: -129.433, mean reward: -1.726 [-100.000, 11.063], mean action: 1.707 [0.000, 3.000],  loss: 14.406495, mae: 42.523111, mean_q: 28.604683, mean_eps: 0.878014
  27212/200000: episode: 291, duration: 0.963s, episode steps:  66, steps per second:  69, episode reward: -98.555, mean reward: -1.493 [-100.000,  9.002], mean action: 1.485 [0.000, 3.000],  loss: 8.815577, mae: 42.816986, mean_q: 27.987945, mean_eps: 0.877697
  27321/200000: episode: 292, duration: 2.598s, episode steps: 109, steps per second:  42, episode reward: -111.639, mean reward: -1.024 [-100.000, 12.207], mean action: 1.853 [0.000, 3.000],  loss: 15.809766, mae: 43.269670, mean_q: 28.733081, mean_eps: 0.877303
  27426/200000: episode: 293, duration: 1.579s, episode steps: 105, steps per second:  67, episode reward: -211.381, mean reward: -2.013 [-100.000, 33.381], mean action: 1.667 [0.000, 3.000],  loss: 27.499483, mae: 42.503635, mean_q: 26.077312, mean_eps: 0.876822
  27493/200000: episode: 294, duration: 0.783s, episode steps:  67, steps per second:  86, episode reward: -41.908, mean reward: -0.625 [-100.000, 12.387], mean action: 1.433 [0.000, 3.000],  loss: 16.719029, mae: 42.867995, mean_q: 28.376157, mean_eps: 0.876435
  27601/200000: episode: 295, duration: 1.416s, episode steps: 108, steps per second:  76, episode reward: -105.729, mean reward: -0.979 [-100.000, 10.384], mean action: 1.519 [0.000, 3.000],  loss: 9.511259, mae: 42.255545, mean_q: 28.468214, mean_eps: 0.876041
  27693/200000: episode: 296, duration: 0.771s, episode steps:  92, steps per second: 119, episode reward: -74.306, mean reward: -0.808 [-100.000, 17.140], mean action: 1.533 [0.000, 3.000],  loss: 23.521564, mae: 42.030670, mean_q: 26.423587, mean_eps: 0.875591
  27782/200000: episode: 297, duration: 0.629s, episode steps:  89, steps per second: 142, episode reward: -140.286, mean reward: -1.576 [-100.000, 61.113], mean action: 1.584 [0.000, 3.000],  loss: 23.620561, mae: 43.040644, mean_q: 28.138050, mean_eps: 0.875183
  27871/200000: episode: 298, duration: 0.659s, episode steps:  89, steps per second: 135, episode reward: -143.838, mean reward: -1.616 [-100.000, 30.607], mean action: 1.506 [0.000, 3.000],  loss: 14.308256, mae: 42.658339, mean_q: 29.245889, mean_eps: 0.874783
  27950/200000: episode: 299, duration: 0.647s, episode steps:  79, steps per second: 122, episode reward: -134.016, mean reward: -1.696 [-100.000, 12.307], mean action: 1.747 [0.000, 3.000],  loss: 16.858241, mae: 42.245984, mean_q: 27.899731, mean_eps: 0.874405
  28043/200000: episode: 300, duration: 0.693s, episode steps:  93, steps per second: 134, episode reward: -153.595, mean reward: -1.652 [-100.000, 12.285], mean action: 1.505 [0.000, 3.000],  loss: 14.846803, mae: 41.631658, mean_q: 28.216998, mean_eps: 0.874018
  28126/200000: episode: 301, duration: 0.607s, episode steps:  83, steps per second: 137, episode reward: -290.376, mean reward: -3.499 [-100.000, 130.812], mean action: 1.506 [0.000, 3.000],  loss: 18.330007, mae: 42.038905, mean_q: 29.182598, mean_eps: 0.873622
  28203/200000: episode: 302, duration: 0.695s, episode steps:  77, steps per second: 111, episode reward: -59.209, mean reward: -0.769 [-100.000, 22.583], mean action: 1.558 [0.000, 3.000],  loss: 18.519754, mae: 41.684932, mean_q: 27.235005, mean_eps: 0.873262
  28323/200000: episode: 303, duration: 0.927s, episode steps: 120, steps per second: 129, episode reward: -126.640, mean reward: -1.055 [-100.000,  6.477], mean action: 1.492 [0.000, 3.000],  loss: 12.902646, mae: 41.737811, mean_q: 27.341900, mean_eps: 0.872819
  28408/200000: episode: 304, duration: 0.671s, episode steps:  85, steps per second: 127, episode reward: -52.961, mean reward: -0.623 [-100.000, 13.801], mean action: 1.494 [0.000, 3.000],  loss: 11.074134, mae: 42.298086, mean_q: 28.125420, mean_eps: 0.872358
  28505/200000: episode: 305, duration: 0.752s, episode steps:  97, steps per second: 129, episode reward: -71.976, mean reward: -0.742 [-100.000, 18.244], mean action: 1.639 [0.000, 3.000],  loss: 17.024567, mae: 41.669979, mean_q: 27.458256, mean_eps: 0.871948
  28594/200000: episode: 306, duration: 0.737s, episode steps:  89, steps per second: 121, episode reward: -52.562, mean reward: -0.591 [-100.000, 10.704], mean action: 1.438 [0.000, 3.000],  loss: 21.700871, mae: 42.517375, mean_q: 28.623979, mean_eps: 0.871529
  28681/200000: episode: 307, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -180.466, mean reward: -2.074 [-100.000, 23.555], mean action: 1.586 [0.000, 3.000],  loss: 13.005397, mae: 42.287208, mean_q: 29.554587, mean_eps: 0.871133
  28791/200000: episode: 308, duration: 0.795s, episode steps: 110, steps per second: 138, episode reward: -87.489, mean reward: -0.795 [-100.000,  7.253], mean action: 1.473 [0.000, 3.000],  loss: 17.138840, mae: 42.449020, mean_q: 29.499638, mean_eps: 0.870690
  28911/200000: episode: 309, duration: 0.834s, episode steps: 120, steps per second: 144, episode reward: -95.314, mean reward: -0.794 [-100.000,  8.022], mean action: 1.508 [0.000, 3.000],  loss: 13.029082, mae: 43.845392, mean_q: 27.705440, mean_eps: 0.870173
  28983/200000: episode: 310, duration: 0.481s, episode steps:  72, steps per second: 150, episode reward: -94.678, mean reward: -1.315 [-100.000,  9.146], mean action: 1.694 [0.000, 3.000],  loss: 24.014984, mae: 43.603916, mean_q: 25.703666, mean_eps: 0.869741
  29081/200000: episode: 311, duration: 0.715s, episode steps:  98, steps per second: 137, episode reward: -134.729, mean reward: -1.375 [-100.000,  8.307], mean action: 1.500 [0.000, 3.000],  loss: 16.534682, mae: 43.304620, mean_q: 29.341809, mean_eps: 0.869358
  29215/200000: episode: 312, duration: 0.951s, episode steps: 134, steps per second: 141, episode reward: -154.654, mean reward: -1.154 [-100.000,  8.101], mean action: 1.500 [0.000, 3.000],  loss: 18.961010, mae: 42.898735, mean_q: 27.703488, mean_eps: 0.868836
  29289/200000: episode: 313, duration: 0.513s, episode steps:  74, steps per second: 144, episode reward: -136.198, mean reward: -1.841 [-100.000,  6.853], mean action: 1.581 [0.000, 3.000],  loss: 17.437058, mae: 42.889573, mean_q: 27.669287, mean_eps: 0.868368
  29374/200000: episode: 314, duration: 0.658s, episode steps:  85, steps per second: 129, episode reward: -84.638, mean reward: -0.996 [-100.000, 16.871], mean action: 1.859 [0.000, 3.000],  loss: 9.869220, mae: 42.973737, mean_q: 29.311649, mean_eps: 0.868011
  29461/200000: episode: 315, duration: 0.651s, episode steps:  87, steps per second: 134, episode reward: -123.650, mean reward: -1.421 [-100.000,  7.535], mean action: 1.552 [0.000, 3.000],  loss: 12.035021, mae: 42.703089, mean_q: 29.147709, mean_eps: 0.867623
  29582/200000: episode: 316, duration: 0.885s, episode steps: 121, steps per second: 137, episode reward: -209.625, mean reward: -1.732 [-100.000,  1.498], mean action: 1.595 [0.000, 3.000],  loss: 19.030998, mae: 43.074535, mean_q: 26.952701, mean_eps: 0.867155
  29688/200000: episode: 317, duration: 0.744s, episode steps: 106, steps per second: 142, episode reward: -137.329, mean reward: -1.296 [-100.000, 18.031], mean action: 1.660 [0.000, 3.000],  loss: 16.013167, mae: 43.435831, mean_q: 30.083928, mean_eps: 0.866645
  29765/200000: episode: 318, duration: 0.494s, episode steps:  77, steps per second: 156, episode reward: -89.277, mean reward: -1.159 [-100.000, 35.757], mean action: 1.519 [0.000, 3.000],  loss: 12.611854, mae: 42.905405, mean_q: 27.203616, mean_eps: 0.866233
  29855/200000: episode: 319, duration: 0.556s, episode steps:  90, steps per second: 162, episode reward: -103.060, mean reward: -1.145 [-100.000, 12.785], mean action: 1.411 [0.000, 3.000],  loss: 16.805206, mae: 43.298099, mean_q: 28.601795, mean_eps: 0.865857
  29962/200000: episode: 320, duration: 0.709s, episode steps: 107, steps per second: 151, episode reward: -70.257, mean reward: -0.657 [-100.000, 22.815], mean action: 1.748 [0.000, 3.000],  loss: 14.271711, mae: 43.125585, mean_q: 25.430999, mean_eps: 0.865414
  30102/200000: episode: 321, duration: 0.935s, episode steps: 140, steps per second: 150, episode reward: -116.101, mean reward: -0.829 [-100.000, 10.273], mean action: 1.536 [0.000, 3.000],  loss: 13.489372, mae: 43.847304, mean_q: 28.484777, mean_eps: 0.864858
  30203/200000: episode: 322, duration: 0.783s, episode steps: 101, steps per second: 129, episode reward: -140.046, mean reward: -1.387 [-100.000, 51.242], mean action: 1.356 [0.000, 3.000],  loss: 15.537625, mae: 43.249386, mean_q: 29.727279, mean_eps: 0.864316
  30304/200000: episode: 323, duration: 0.792s, episode steps: 101, steps per second: 127, episode reward: -116.738, mean reward: -1.156 [-100.000,  9.037], mean action: 1.465 [0.000, 3.000],  loss: 15.938479, mae: 43.655454, mean_q: 28.506685, mean_eps: 0.863861
  30397/200000: episode: 324, duration: 0.705s, episode steps:  93, steps per second: 132, episode reward: -72.841, mean reward: -0.783 [-100.000, 14.466], mean action: 1.581 [0.000, 3.000],  loss: 9.084493, mae: 43.624434, mean_q: 29.067900, mean_eps: 0.863425
  30504/200000: episode: 325, duration: 0.752s, episode steps: 107, steps per second: 142, episode reward: -80.015, mean reward: -0.748 [-100.000, 10.146], mean action: 1.467 [0.000, 3.000],  loss: 15.035661, mae: 43.837798, mean_q: 27.134294, mean_eps: 0.862975
  30625/200000: episode: 326, duration: 0.850s, episode steps: 121, steps per second: 142, episode reward: -182.571, mean reward: -1.509 [-100.000,  4.933], mean action: 1.612 [0.000, 3.000],  loss: 17.384223, mae: 44.207778, mean_q: 29.273917, mean_eps: 0.862462
  30694/200000: episode: 327, duration: 0.486s, episode steps:  69, steps per second: 142, episode reward: -34.391, mean reward: -0.498 [-100.000,  6.248], mean action: 1.449 [0.000, 3.000],  loss: 9.636359, mae: 44.112234, mean_q: 28.894778, mean_eps: 0.862035
  30789/200000: episode: 328, duration: 0.665s, episode steps:  95, steps per second: 143, episode reward: 10.874, mean reward:  0.114 [-100.000, 114.510], mean action: 1.547 [0.000, 3.000],  loss: 16.258839, mae: 44.171541, mean_q: 28.340495, mean_eps: 0.861665
  30898/200000: episode: 329, duration: 0.797s, episode steps: 109, steps per second: 137, episode reward: -145.223, mean reward: -1.332 [-100.000,  8.280], mean action: 1.697 [0.000, 3.000],  loss: 19.821657, mae: 44.006433, mean_q: 27.824434, mean_eps: 0.861206
  31009/200000: episode: 330, duration: 0.759s, episode steps: 111, steps per second: 146, episode reward: -120.197, mean reward: -1.083 [-100.000, 36.228], mean action: 1.658 [0.000, 3.000],  loss: 14.649627, mae: 43.354566, mean_q: 27.378177, mean_eps: 0.860711
  31105/200000: episode: 331, duration: 0.628s, episode steps:  96, steps per second: 153, episode reward: -190.750, mean reward: -1.987 [-100.000,  8.771], mean action: 1.594 [0.000, 3.000],  loss: 11.857306, mae: 43.078183, mean_q: 28.408681, mean_eps: 0.860246
  31194/200000: episode: 332, duration: 0.650s, episode steps:  89, steps per second: 137, episode reward: -48.215, mean reward: -0.542 [-100.000, 49.209], mean action: 1.438 [0.000, 3.000],  loss: 16.551045, mae: 43.326875, mean_q: 29.851531, mean_eps: 0.859830
  31305/200000: episode: 333, duration: 0.802s, episode steps: 111, steps per second: 138, episode reward: 31.374, mean reward:  0.283 [-100.000, 105.565], mean action: 1.468 [0.000, 3.000],  loss: 18.239085, mae: 42.353276, mean_q: 27.809748, mean_eps: 0.859380
  31372/200000: episode: 334, duration: 0.462s, episode steps:  67, steps per second: 145, episode reward: -140.601, mean reward: -2.099 [-100.000, 19.863], mean action: 1.672 [0.000, 3.000],  loss: 7.184817, mae: 42.740055, mean_q: 30.925982, mean_eps: 0.858979
  31508/200000: episode: 335, duration: 0.986s, episode steps: 136, steps per second: 138, episode reward: -82.754, mean reward: -0.608 [-100.000, 13.997], mean action: 1.588 [0.000, 3.000],  loss: 14.514382, mae: 43.042109, mean_q: 28.215795, mean_eps: 0.858522
  31637/200000: episode: 336, duration: 0.884s, episode steps: 129, steps per second: 146, episode reward: -84.543, mean reward: -0.655 [-100.000,  6.887], mean action: 1.651 [0.000, 3.000],  loss: 20.719806, mae: 43.196367, mean_q: 27.662428, mean_eps: 0.857926
  31786/200000: episode: 337, duration: 1.007s, episode steps: 149, steps per second: 148, episode reward: -104.070, mean reward: -0.698 [-100.000, 23.742], mean action: 1.691 [0.000, 3.000],  loss: 17.240529, mae: 43.411177, mean_q: 29.525931, mean_eps: 0.857300
  31849/200000: episode: 338, duration: 0.426s, episode steps:  63, steps per second: 148, episode reward: -210.366, mean reward: -3.339 [-100.000,  4.839], mean action: 1.810 [0.000, 3.000],  loss: 14.333007, mae: 43.590833, mean_q: 29.418797, mean_eps: 0.856823
  31935/200000: episode: 339, duration: 0.608s, episode steps:  86, steps per second: 142, episode reward: -168.560, mean reward: -1.960 [-100.000,  8.600], mean action: 1.651 [0.000, 3.000],  loss: 23.514805, mae: 43.211482, mean_q: 29.774892, mean_eps: 0.856488
  32064/200000: episode: 340, duration: 0.968s, episode steps: 129, steps per second: 133, episode reward: -153.414, mean reward: -1.189 [-100.000,  6.757], mean action: 1.504 [0.000, 3.000],  loss: 15.398155, mae: 42.827472, mean_q: 30.263285, mean_eps: 0.856004
  32167/200000: episode: 341, duration: 0.833s, episode steps: 103, steps per second: 124, episode reward: -146.681, mean reward: -1.424 [-100.000, 14.661], mean action: 1.583 [0.000, 3.000],  loss: 13.443644, mae: 42.763833, mean_q: 27.274028, mean_eps: 0.855483
  32275/200000: episode: 342, duration: 0.725s, episode steps: 108, steps per second: 149, episode reward: -226.687, mean reward: -2.099 [-100.000,  8.850], mean action: 1.537 [0.000, 3.000],  loss: 14.761301, mae: 43.371941, mean_q: 28.597421, mean_eps: 0.855008
  32351/200000: episode: 343, duration: 0.547s, episode steps:  76, steps per second: 139, episode reward: -96.444, mean reward: -1.269 [-100.000,  6.369], mean action: 1.513 [0.000, 3.000],  loss: 19.021590, mae: 43.417583, mean_q: 27.740480, mean_eps: 0.854594
  32417/200000: episode: 344, duration: 0.491s, episode steps:  66, steps per second: 134, episode reward: -183.579, mean reward: -2.781 [-100.000,  7.711], mean action: 1.470 [0.000, 3.000],  loss: 10.562577, mae: 43.048861, mean_q: 29.820532, mean_eps: 0.854274
  32540/200000: episode: 345, duration: 0.813s, episode steps: 123, steps per second: 151, episode reward: -121.221, mean reward: -0.986 [-100.000,  5.482], mean action: 1.488 [0.000, 3.000],  loss: 15.419078, mae: 42.836670, mean_q: 29.720671, mean_eps: 0.853849
  32602/200000: episode: 346, duration: 0.450s, episode steps:  62, steps per second: 138, episode reward: -101.321, mean reward: -1.634 [-100.000, 56.860], mean action: 1.565 [0.000, 3.000],  loss: 16.218228, mae: 42.222064, mean_q: 29.419507, mean_eps: 0.853433
  32742/200000: episode: 347, duration: 0.982s, episode steps: 140, steps per second: 143, episode reward: -140.951, mean reward: -1.007 [-100.000,  6.753], mean action: 1.593 [0.000, 3.000],  loss: 13.996734, mae: 42.671792, mean_q: 28.419247, mean_eps: 0.852978
  32851/200000: episode: 348, duration: 0.763s, episode steps: 109, steps per second: 143, episode reward: -273.717, mean reward: -2.511 [-100.000,  5.039], mean action: 1.578 [0.000, 3.000],  loss: 17.223015, mae: 42.865559, mean_q: 28.924311, mean_eps: 0.852418
  32922/200000: episode: 349, duration: 0.481s, episode steps:  71, steps per second: 148, episode reward: -93.723, mean reward: -1.320 [-100.000,  5.801], mean action: 1.606 [0.000, 3.000],  loss: 20.621733, mae: 42.743811, mean_q: 30.551734, mean_eps: 0.852013
  33034/200000: episode: 350, duration: 0.763s, episode steps: 112, steps per second: 147, episode reward: -162.816, mean reward: -1.454 [-100.000,  6.051], mean action: 1.518 [0.000, 3.000],  loss: 15.985385, mae: 42.638131, mean_q: 30.816088, mean_eps: 0.851601
  33160/200000: episode: 351, duration: 0.866s, episode steps: 126, steps per second: 146, episode reward: -72.597, mean reward: -0.576 [-100.000, 25.712], mean action: 1.698 [0.000, 3.000],  loss: 10.328670, mae: 42.825945, mean_q: 32.156347, mean_eps: 0.851066
  33255/200000: episode: 352, duration: 0.618s, episode steps:  95, steps per second: 154, episode reward: -236.467, mean reward: -2.489 [-100.000,  8.064], mean action: 1.621 [0.000, 3.000],  loss: 15.639359, mae: 42.973492, mean_q: 30.759525, mean_eps: 0.850569
  33327/200000: episode: 353, duration: 0.483s, episode steps:  72, steps per second: 149, episode reward: -74.475, mean reward: -1.034 [-100.000,  8.432], mean action: 1.764 [0.000, 3.000],  loss: 16.088087, mae: 42.344065, mean_q: 30.596803, mean_eps: 0.850193
  33455/200000: episode: 354, duration: 0.863s, episode steps: 128, steps per second: 148, episode reward: -96.882, mean reward: -0.757 [-100.000, 13.096], mean action: 1.641 [0.000, 3.000],  loss: 15.124945, mae: 43.021381, mean_q: 32.109014, mean_eps: 0.849743
  33539/200000: episode: 355, duration: 0.618s, episode steps:  84, steps per second: 136, episode reward: -145.115, mean reward: -1.728 [-100.000,  7.827], mean action: 1.595 [0.000, 3.000],  loss: 13.201700, mae: 43.059548, mean_q: 31.474927, mean_eps: 0.849266
  33650/200000: episode: 356, duration: 0.748s, episode steps: 111, steps per second: 148, episode reward: -52.735, mean reward: -0.475 [-100.000, 12.589], mean action: 1.775 [0.000, 3.000],  loss: 12.250660, mae: 42.614396, mean_q: 31.677214, mean_eps: 0.848827
  33751/200000: episode: 357, duration: 0.760s, episode steps: 101, steps per second: 133, episode reward: -125.835, mean reward: -1.246 [-100.000, 10.418], mean action: 1.653 [0.000, 3.000],  loss: 15.769447, mae: 43.072714, mean_q: 33.160387, mean_eps: 0.848350
  33856/200000: episode: 358, duration: 0.780s, episode steps: 105, steps per second: 135, episode reward: -298.651, mean reward: -2.844 [-100.000, 73.419], mean action: 1.562 [0.000, 3.000],  loss: 16.595504, mae: 42.552881, mean_q: 32.290305, mean_eps: 0.847886
  33948/200000: episode: 359, duration: 0.705s, episode steps:  92, steps per second: 130, episode reward: -22.071, mean reward: -0.240 [-100.000, 88.471], mean action: 1.359 [0.000, 3.000],  loss: 15.750690, mae: 42.190510, mean_q: 32.530931, mean_eps: 0.847443
  34060/200000: episode: 360, duration: 0.792s, episode steps: 112, steps per second: 141, episode reward: -179.638, mean reward: -1.604 [-100.000,  9.152], mean action: 1.643 [0.000, 3.000],  loss: 17.265218, mae: 42.585334, mean_q: 29.944258, mean_eps: 0.846984
  34225/200000: episode: 361, duration: 1.127s, episode steps: 165, steps per second: 146, episode reward: -146.481, mean reward: -0.888 [-100.000, 14.856], mean action: 1.606 [0.000, 3.000],  loss: 10.722613, mae: 42.455834, mean_q: 32.383285, mean_eps: 0.846361
  34329/200000: episode: 362, duration: 0.858s, episode steps: 104, steps per second: 121, episode reward: -67.349, mean reward: -0.648 [-100.000, 12.753], mean action: 1.548 [0.000, 3.000],  loss: 10.410912, mae: 42.780276, mean_q: 31.327116, mean_eps: 0.845756
  34459/200000: episode: 363, duration: 0.889s, episode steps: 130, steps per second: 146, episode reward: -114.891, mean reward: -0.884 [-100.000, 12.977], mean action: 1.485 [0.000, 3.000],  loss: 13.713914, mae: 42.306079, mean_q: 31.332995, mean_eps: 0.845229
  34545/200000: episode: 364, duration: 0.587s, episode steps:  86, steps per second: 146, episode reward: -155.628, mean reward: -1.810 [-100.000,  7.822], mean action: 1.500 [0.000, 3.000],  loss: 16.380008, mae: 42.455607, mean_q: 30.676928, mean_eps: 0.844743
  34629/200000: episode: 365, duration: 0.560s, episode steps:  84, steps per second: 150, episode reward: -94.781, mean reward: -1.128 [-100.000,  9.202], mean action: 1.571 [0.000, 3.000],  loss: 12.939809, mae: 41.976690, mean_q: 29.240279, mean_eps: 0.844361
  34723/200000: episode: 366, duration: 0.687s, episode steps:  94, steps per second: 137, episode reward: -102.560, mean reward: -1.091 [-100.000, 10.455], mean action: 1.660 [0.000, 3.000],  loss: 11.560172, mae: 41.337855, mean_q: 30.016179, mean_eps: 0.843960
  34792/200000: episode: 367, duration: 0.499s, episode steps:  69, steps per second: 138, episode reward: -105.807, mean reward: -1.533 [-100.000, 25.037], mean action: 1.391 [0.000, 3.000],  loss: 16.951410, mae: 41.511409, mean_q: 29.659172, mean_eps: 0.843593
  34912/200000: episode: 368, duration: 0.891s, episode steps: 120, steps per second: 135, episode reward: -54.288, mean reward: -0.452 [-100.000, 12.399], mean action: 1.600 [0.000, 3.000],  loss: 16.153682, mae: 41.760705, mean_q: 30.089779, mean_eps: 0.843168
  34991/200000: episode: 369, duration: 0.662s, episode steps:  79, steps per second: 119, episode reward: -131.772, mean reward: -1.668 [-100.000,  7.842], mean action: 1.443 [0.000, 3.000],  loss: 21.486325, mae: 42.412073, mean_q: 29.600077, mean_eps: 0.842721
  35104/200000: episode: 370, duration: 0.845s, episode steps: 113, steps per second: 134, episode reward: -112.391, mean reward: -0.995 [-100.000, 17.970], mean action: 1.708 [0.000, 3.000],  loss: 21.383897, mae: 41.395352, mean_q: 30.231682, mean_eps: 0.842288
  35220/200000: episode: 371, duration: 0.934s, episode steps: 116, steps per second: 124, episode reward: -126.497, mean reward: -1.090 [-100.000, 11.755], mean action: 1.440 [0.000, 3.000],  loss: 20.818368, mae: 42.754044, mean_q: 32.289151, mean_eps: 0.841773
  35311/200000: episode: 372, duration: 0.579s, episode steps:  91, steps per second: 157, episode reward: -117.503, mean reward: -1.291 [-100.000,  3.392], mean action: 1.571 [0.000, 3.000],  loss: 13.831916, mae: 41.931690, mean_q: 30.390655, mean_eps: 0.841308
  35434/200000: episode: 373, duration: 0.886s, episode steps: 123, steps per second: 139, episode reward: -120.317, mean reward: -0.978 [-100.000,  7.372], mean action: 1.642 [0.000, 3.000],  loss: 10.296746, mae: 41.379402, mean_q: 30.916661, mean_eps: 0.840826
  35553/200000: episode: 374, duration: 0.981s, episode steps: 119, steps per second: 121, episode reward: -92.654, mean reward: -0.779 [-100.000, 29.849], mean action: 1.487 [0.000, 3.000],  loss: 13.783320, mae: 41.288821, mean_q: 29.617947, mean_eps: 0.840282
  35659/200000: episode: 375, duration: 0.758s, episode steps: 106, steps per second: 140, episode reward: -333.632, mean reward: -3.147 [-100.000, 93.068], mean action: 1.509 [0.000, 3.000],  loss: 15.164847, mae: 41.570266, mean_q: 30.343636, mean_eps: 0.839775
  35729/200000: episode: 376, duration: 0.508s, episode steps:  70, steps per second: 138, episode reward: -93.414, mean reward: -1.334 [-100.000, 18.091], mean action: 1.386 [0.000, 3.000],  loss: 7.177418, mae: 41.955849, mean_q: 30.992148, mean_eps: 0.839379
  35838/200000: episode: 377, duration: 0.777s, episode steps: 109, steps per second: 140, episode reward: -126.537, mean reward: -1.161 [-100.000,  8.470], mean action: 1.688 [0.000, 3.000],  loss: 14.892324, mae: 42.596716, mean_q: 31.335158, mean_eps: 0.838977
  35961/200000: episode: 378, duration: 0.886s, episode steps: 123, steps per second: 139, episode reward: -34.529, mean reward: -0.281 [-100.000, 19.462], mean action: 1.675 [0.000, 3.000],  loss: 12.974549, mae: 42.276702, mean_q: 30.378070, mean_eps: 0.838454
  36038/200000: episode: 379, duration: 0.764s, episode steps:  77, steps per second: 101, episode reward: -77.439, mean reward: -1.006 [-100.000, 17.788], mean action: 1.558 [0.000, 3.000],  loss: 7.731258, mae: 42.157643, mean_q: 29.706298, mean_eps: 0.838005
  36159/200000: episode: 380, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -119.505, mean reward: -0.988 [-100.000,  9.957], mean action: 1.595 [0.000, 3.000],  loss: 14.365071, mae: 42.497828, mean_q: 29.693666, mean_eps: 0.837559
  36274/200000: episode: 381, duration: 0.742s, episode steps: 115, steps per second: 155, episode reward: -175.048, mean reward: -1.522 [-100.000, 70.148], mean action: 1.548 [0.000, 3.000],  loss: 14.764590, mae: 42.137415, mean_q: 29.817384, mean_eps: 0.837028
  36367/200000: episode: 382, duration: 0.649s, episode steps:  93, steps per second: 143, episode reward: -73.321, mean reward: -0.788 [-100.000,  7.596], mean action: 1.473 [0.000, 3.000],  loss: 15.294000, mae: 42.118491, mean_q: 28.909909, mean_eps: 0.836560
  36489/200000: episode: 383, duration: 0.835s, episode steps: 122, steps per second: 146, episode reward: -28.528, mean reward: -0.234 [-100.000, 107.182], mean action: 1.672 [0.000, 3.000],  loss: 10.514717, mae: 41.832797, mean_q: 29.309001, mean_eps: 0.836076
  36608/200000: episode: 384, duration: 0.864s, episode steps: 119, steps per second: 138, episode reward: -163.340, mean reward: -1.373 [-100.000,  7.370], mean action: 1.588 [0.000, 3.000],  loss: 10.674768, mae: 41.238113, mean_q: 28.869366, mean_eps: 0.835534
  36687/200000: episode: 385, duration: 0.589s, episode steps:  79, steps per second: 134, episode reward: -79.270, mean reward: -1.003 [-100.000, 22.578], mean action: 1.747 [0.000, 3.000],  loss: 10.354905, mae: 41.106415, mean_q: 27.280457, mean_eps: 0.835089
  36790/200000: episode: 386, duration: 0.648s, episode steps: 103, steps per second: 159, episode reward: -130.411, mean reward: -1.266 [-100.000, 25.315], mean action: 1.602 [0.000, 3.000],  loss: 11.490134, mae: 41.463429, mean_q: 26.376103, mean_eps: 0.834679
  36914/200000: episode: 387, duration: 0.804s, episode steps: 124, steps per second: 154, episode reward: -372.408, mean reward: -3.003 [-100.000,  5.175], mean action: 1.508 [0.000, 3.000],  loss: 11.394802, mae: 41.864750, mean_q: 26.284568, mean_eps: 0.834168
  37037/200000: episode: 388, duration: 0.887s, episode steps: 123, steps per second: 139, episode reward: -108.349, mean reward: -0.881 [-100.000, 13.193], mean action: 1.569 [0.000, 3.000],  loss: 14.273273, mae: 41.719867, mean_q: 25.983785, mean_eps: 0.833613
  37137/200000: episode: 389, duration: 0.653s, episode steps: 100, steps per second: 153, episode reward: -126.774, mean reward: -1.268 [-100.000, 11.388], mean action: 1.450 [0.000, 3.000],  loss: 14.125854, mae: 41.554681, mean_q: 25.708582, mean_eps: 0.833111
  37217/200000: episode: 390, duration: 0.537s, episode steps:  80, steps per second: 149, episode reward: -84.046, mean reward: -1.051 [-100.000,  6.894], mean action: 1.750 [0.000, 3.000],  loss: 14.200024, mae: 41.070228, mean_q: 26.944329, mean_eps: 0.832706
  37290/200000: episode: 391, duration: 0.512s, episode steps:  73, steps per second: 143, episode reward: -104.697, mean reward: -1.434 [-100.000, 11.402], mean action: 1.521 [0.000, 3.000],  loss: 24.154410, mae: 42.086305, mean_q: 28.451474, mean_eps: 0.832361
  37403/200000: episode: 392, duration: 0.699s, episode steps: 113, steps per second: 162, episode reward: -101.012, mean reward: -0.894 [-100.000,  6.110], mean action: 1.389 [0.000, 3.000],  loss: 15.029975, mae: 41.887804, mean_q: 25.978861, mean_eps: 0.831943
  37518/200000: episode: 393, duration: 0.740s, episode steps: 115, steps per second: 155, episode reward: -121.150, mean reward: -1.053 [-100.000, 19.169], mean action: 1.661 [0.000, 3.000],  loss: 11.602867, mae: 41.336259, mean_q: 26.280967, mean_eps: 0.831430
  37662/200000: episode: 394, duration: 1.046s, episode steps: 144, steps per second: 138, episode reward: -59.262, mean reward: -0.412 [-100.000, 12.461], mean action: 1.528 [0.000, 3.000],  loss: 11.532074, mae: 40.652925, mean_q: 27.977247, mean_eps: 0.830847
  37761/200000: episode: 395, duration: 0.689s, episode steps:  99, steps per second: 144, episode reward: -239.569, mean reward: -2.420 [-100.000,  1.055], mean action: 1.667 [0.000, 3.000],  loss: 13.663177, mae: 40.702320, mean_q: 27.873188, mean_eps: 0.830300
  37908/200000: episode: 396, duration: 0.994s, episode steps: 147, steps per second: 148, episode reward: -114.431, mean reward: -0.778 [-100.000,  6.547], mean action: 1.483 [0.000, 3.000],  loss: 11.702875, mae: 41.472484, mean_q: 26.158066, mean_eps: 0.829747
  38004/200000: episode: 397, duration: 0.807s, episode steps:  96, steps per second: 119, episode reward: -153.328, mean reward: -1.597 [-100.000,  6.688], mean action: 1.500 [0.000, 3.000],  loss: 20.049500, mae: 41.726979, mean_q: 24.978246, mean_eps: 0.829200
  38131/200000: episode: 398, duration: 1.104s, episode steps: 127, steps per second: 115, episode reward: -96.277, mean reward: -0.758 [-100.000,  9.853], mean action: 1.614 [0.000, 3.000],  loss: 9.592752, mae: 41.419607, mean_q: 27.503084, mean_eps: 0.828699
  38239/200000: episode: 399, duration: 0.788s, episode steps: 108, steps per second: 137, episode reward: -132.004, mean reward: -1.222 [-100.000,  7.289], mean action: 1.593 [0.000, 3.000],  loss: 18.798388, mae: 41.110989, mean_q: 27.117702, mean_eps: 0.828170
  38400/200000: episode: 400, duration: 1.058s, episode steps: 161, steps per second: 152, episode reward: -58.177, mean reward: -0.361 [-100.000, 12.749], mean action: 1.758 [0.000, 3.000],  loss: 11.724967, mae: 41.647316, mean_q: 30.289898, mean_eps: 0.827565
  38517/200000: episode: 401, duration: 0.752s, episode steps: 117, steps per second: 156, episode reward: -114.797, mean reward: -0.981 [-100.000, 15.615], mean action: 1.581 [0.000, 3.000],  loss: 10.846471, mae: 41.296835, mean_q: 29.920568, mean_eps: 0.826939
  38611/200000: episode: 402, duration: 0.623s, episode steps:  94, steps per second: 151, episode reward: -114.525, mean reward: -1.218 [-100.000,  8.022], mean action: 1.617 [0.000, 3.000],  loss: 13.154624, mae: 42.267300, mean_q: 30.336116, mean_eps: 0.826464
  38677/200000: episode: 403, duration: 0.389s, episode steps:  66, steps per second: 169, episode reward: -35.789, mean reward: -0.542 [-100.000, 15.598], mean action: 1.591 [0.000, 3.000],  loss: 11.240573, mae: 42.494118, mean_q: 29.643018, mean_eps: 0.826104
  38793/200000: episode: 404, duration: 0.854s, episode steps: 116, steps per second: 136, episode reward: -143.673, mean reward: -1.239 [-100.000,  5.571], mean action: 1.543 [0.000, 3.000],  loss: 12.310807, mae: 42.106271, mean_q: 30.050505, mean_eps: 0.825695
  38889/200000: episode: 405, duration: 0.692s, episode steps:  96, steps per second: 139, episode reward: -134.624, mean reward: -1.402 [-100.000,  7.991], mean action: 1.750 [0.000, 3.000],  loss: 12.322660, mae: 42.485398, mean_q: 29.930049, mean_eps: 0.825218
  38998/200000: episode: 406, duration: 0.705s, episode steps: 109, steps per second: 155, episode reward: -89.130, mean reward: -0.818 [-100.000, 10.801], mean action: 1.606 [0.000, 3.000],  loss: 9.078534, mae: 42.269991, mean_q: 30.106361, mean_eps: 0.824757
  39093/200000: episode: 407, duration: 0.571s, episode steps:  95, steps per second: 166, episode reward: -68.965, mean reward: -0.726 [-100.000,  6.569], mean action: 1.411 [0.000, 3.000],  loss: 8.875184, mae: 42.586174, mean_q: 30.543310, mean_eps: 0.824297
  39210/200000: episode: 408, duration: 0.748s, episode steps: 117, steps per second: 157, episode reward: -78.533, mean reward: -0.671 [-100.000, 11.381], mean action: 1.726 [0.000, 3.000],  loss: 15.557416, mae: 43.051673, mean_q: 29.089984, mean_eps: 0.823820
  39351/200000: episode: 409, duration: 0.836s, episode steps: 141, steps per second: 169, episode reward: -84.019, mean reward: -0.596 [-100.000,  6.805], mean action: 1.574 [0.000, 3.000],  loss: 10.843857, mae: 42.761799, mean_q: 29.040343, mean_eps: 0.823240
  39476/200000: episode: 410, duration: 0.736s, episode steps: 125, steps per second: 170, episode reward: -112.544, mean reward: -0.900 [-100.000,  6.592], mean action: 1.576 [0.000, 3.000],  loss: 15.372583, mae: 43.141119, mean_q: 29.826436, mean_eps: 0.822642
  39567/200000: episode: 411, duration: 0.575s, episode steps:  91, steps per second: 158, episode reward: -84.600, mean reward: -0.930 [-100.000, 12.059], mean action: 1.791 [0.000, 3.000],  loss: 8.325331, mae: 42.572625, mean_q: 28.888696, mean_eps: 0.822155
  39683/200000: episode: 412, duration: 0.737s, episode steps: 116, steps per second: 157, episode reward: -70.433, mean reward: -0.607 [-100.000, 17.932], mean action: 1.638 [0.000, 3.000],  loss: 16.917532, mae: 42.303053, mean_q: 28.033636, mean_eps: 0.821690
  39807/200000: episode: 413, duration: 0.762s, episode steps: 124, steps per second: 163, episode reward: -135.095, mean reward: -1.089 [-100.000,  8.995], mean action: 1.468 [0.000, 3.000],  loss: 11.497601, mae: 42.024877, mean_q: 29.795809, mean_eps: 0.821150
  39895/200000: episode: 414, duration: 0.725s, episode steps:  88, steps per second: 121, episode reward: -116.416, mean reward: -1.323 [-100.000,  6.298], mean action: 1.523 [0.000, 3.000],  loss: 11.798937, mae: 41.930073, mean_q: 27.901991, mean_eps: 0.820673
  39972/200000: episode: 415, duration: 0.588s, episode steps:  77, steps per second: 131, episode reward: -70.656, mean reward: -0.918 [-100.000,  9.639], mean action: 1.455 [0.000, 3.000],  loss: 13.862822, mae: 41.889311, mean_q: 28.433832, mean_eps: 0.820302
  40093/200000: episode: 416, duration: 0.875s, episode steps: 121, steps per second: 138, episode reward: -106.238, mean reward: -0.878 [-100.000,  6.771], mean action: 1.653 [0.000, 3.000],  loss: 11.351362, mae: 42.215806, mean_q: 27.457559, mean_eps: 0.819856
  40215/200000: episode: 417, duration: 0.925s, episode steps: 122, steps per second: 132, episode reward: -290.612, mean reward: -2.382 [-100.000, 96.228], mean action: 1.516 [0.000, 3.000],  loss: 10.169005, mae: 42.120834, mean_q: 27.934051, mean_eps: 0.819309
  40321/200000: episode: 418, duration: 0.725s, episode steps: 106, steps per second: 146, episode reward: -40.940, mean reward: -0.386 [-100.000, 37.727], mean action: 1.368 [0.000, 3.000],  loss: 15.187683, mae: 42.804580, mean_q: 28.686651, mean_eps: 0.818796
  40396/200000: episode: 419, duration: 0.512s, episode steps:  75, steps per second: 147, episode reward: -161.452, mean reward: -2.153 [-100.000,  4.818], mean action: 1.547 [0.000, 3.000],  loss: 16.081551, mae: 42.318419, mean_q: 25.591898, mean_eps: 0.818389
  40527/200000: episode: 420, duration: 0.914s, episode steps: 131, steps per second: 143, episode reward: -103.660, mean reward: -0.791 [-100.000, 18.173], mean action: 1.473 [0.000, 3.000],  loss: 14.314951, mae: 42.381067, mean_q: 27.475801, mean_eps: 0.817925
  40619/200000: episode: 421, duration: 0.596s, episode steps:  92, steps per second: 154, episode reward: -105.540, mean reward: -1.147 [-100.000,  8.622], mean action: 1.663 [0.000, 3.000],  loss: 15.617995, mae: 41.929945, mean_q: 27.878082, mean_eps: 0.817424
  40767/200000: episode: 422, duration: 0.954s, episode steps: 148, steps per second: 155, episode reward: -56.010, mean reward: -0.378 [-100.000, 13.651], mean action: 1.642 [0.000, 3.000],  loss: 12.243683, mae: 41.698875, mean_q: 25.383687, mean_eps: 0.816884
  40890/200000: episode: 423, duration: 0.769s, episode steps: 123, steps per second: 160, episode reward: -116.802, mean reward: -0.950 [-100.000, 16.346], mean action: 1.496 [0.000, 3.000],  loss: 7.672308, mae: 42.401363, mean_q: 26.829011, mean_eps: 0.816274
  41015/200000: episode: 424, duration: 0.767s, episode steps: 125, steps per second: 163, episode reward: -185.482, mean reward: -1.484 [-100.000,  7.096], mean action: 1.544 [0.000, 3.000],  loss: 10.380435, mae: 42.295134, mean_q: 29.328802, mean_eps: 0.815716
  41137/200000: episode: 425, duration: 0.754s, episode steps: 122, steps per second: 162, episode reward: -142.388, mean reward: -1.167 [-100.000,  3.972], mean action: 1.582 [0.000, 3.000],  loss: 23.368064, mae: 42.646501, mean_q: 29.047946, mean_eps: 0.815160
  41238/200000: episode: 426, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: -137.416, mean reward: -1.361 [-100.000, 17.555], mean action: 1.406 [0.000, 3.000],  loss: 12.385181, mae: 42.417948, mean_q: 27.205727, mean_eps: 0.814659
  41321/200000: episode: 427, duration: 0.492s, episode steps:  83, steps per second: 169, episode reward: -102.147, mean reward: -1.231 [-100.000,  9.228], mean action: 1.554 [0.000, 3.000],  loss: 9.601291, mae: 41.146311, mean_q: 27.375177, mean_eps: 0.814244
  41440/200000: episode: 428, duration: 0.682s, episode steps: 119, steps per second: 174, episode reward: -98.551, mean reward: -0.828 [-100.000,  8.150], mean action: 1.529 [0.000, 3.000],  loss: 14.150266, mae: 41.765248, mean_q: 25.515137, mean_eps: 0.813790
  41559/200000: episode: 429, duration: 0.743s, episode steps: 119, steps per second: 160, episode reward: -96.691, mean reward: -0.813 [-100.000, 14.881], mean action: 1.571 [0.000, 3.000],  loss: 9.610150, mae: 41.982380, mean_q: 29.079147, mean_eps: 0.813254
  41685/200000: episode: 430, duration: 0.745s, episode steps: 126, steps per second: 169, episode reward: -124.192, mean reward: -0.986 [-100.000,  7.949], mean action: 1.635 [0.000, 3.000],  loss: 10.648433, mae: 42.217632, mean_q: 27.303046, mean_eps: 0.812703
  41801/200000: episode: 431, duration: 0.670s, episode steps: 116, steps per second: 173, episode reward: -110.284, mean reward: -0.951 [-100.000, 20.705], mean action: 1.603 [0.000, 3.000],  loss: 14.856138, mae: 42.805246, mean_q: 27.124641, mean_eps: 0.812159
  41895/200000: episode: 432, duration: 0.597s, episode steps:  94, steps per second: 158, episode reward: -93.801, mean reward: -0.998 [-100.000,  6.827], mean action: 1.670 [0.000, 3.000],  loss: 11.236901, mae: 43.443334, mean_q: 28.141967, mean_eps: 0.811686
  41962/200000: episode: 433, duration: 0.405s, episode steps:  67, steps per second: 166, episode reward: -56.802, mean reward: -0.848 [-100.000, 16.251], mean action: 1.493 [0.000, 3.000],  loss: 8.453591, mae: 42.601157, mean_q: 28.305622, mean_eps: 0.811324
  42077/200000: episode: 434, duration: 0.672s, episode steps: 115, steps per second: 171, episode reward: -113.002, mean reward: -0.983 [-100.000,  9.925], mean action: 1.617 [0.000, 3.000],  loss: 19.542315, mae: 43.169440, mean_q: 27.042585, mean_eps: 0.810915
  42181/200000: episode: 435, duration: 0.638s, episode steps: 104, steps per second: 163, episode reward: -78.776, mean reward: -0.757 [-100.000, 26.337], mean action: 1.596 [0.000, 3.000],  loss: 17.237514, mae: 42.413430, mean_q: 27.014014, mean_eps: 0.810422
  42307/200000: episode: 436, duration: 0.922s, episode steps: 126, steps per second: 137, episode reward: -233.894, mean reward: -1.856 [-100.000,  8.234], mean action: 1.659 [0.000, 3.000],  loss: 9.571347, mae: 43.352482, mean_q: 28.848016, mean_eps: 0.809904
  42380/200000: episode: 437, duration: 0.455s, episode steps:  73, steps per second: 160, episode reward: -144.275, mean reward: -1.976 [-100.000, 26.741], mean action: 1.589 [0.000, 3.000],  loss: 14.554982, mae: 42.741827, mean_q: 28.716766, mean_eps: 0.809457
  42474/200000: episode: 438, duration: 0.553s, episode steps:  94, steps per second: 170, episode reward: -148.329, mean reward: -1.578 [-100.000, 11.787], mean action: 1.596 [0.000, 3.000],  loss: 7.546274, mae: 42.683260, mean_q: 26.241826, mean_eps: 0.809081
  42549/200000: episode: 439, duration: 0.473s, episode steps:  75, steps per second: 159, episode reward: -59.116, mean reward: -0.788 [-100.000,  9.011], mean action: 1.640 [0.000, 3.000],  loss: 16.789836, mae: 43.166752, mean_q: 28.946916, mean_eps: 0.808701
  42636/200000: episode: 440, duration: 0.565s, episode steps:  87, steps per second: 154, episode reward: -60.403, mean reward: -0.694 [-100.000, 11.433], mean action: 1.575 [0.000, 3.000],  loss: 14.462499, mae: 43.174162, mean_q: 28.046711, mean_eps: 0.808336
  42730/200000: episode: 441, duration: 0.589s, episode steps:  94, steps per second: 160, episode reward: -234.734, mean reward: -2.497 [-100.000,  0.811], mean action: 1.415 [0.000, 3.000],  loss: 14.433855, mae: 43.058675, mean_q: 27.446096, mean_eps: 0.807929
  42840/200000: episode: 442, duration: 0.720s, episode steps: 110, steps per second: 153, episode reward: -85.903, mean reward: -0.781 [-100.000, 17.494], mean action: 1.473 [0.000, 3.000],  loss: 10.430940, mae: 42.468102, mean_q: 27.270305, mean_eps: 0.807470
  42986/200000: episode: 443, duration: 0.946s, episode steps: 146, steps per second: 154, episode reward: -1.525, mean reward: -0.010 [-100.000, 94.599], mean action: 1.630 [0.000, 3.000],  loss: 12.996523, mae: 42.219096, mean_q: 27.102608, mean_eps: 0.806894
  43073/200000: episode: 444, duration: 0.521s, episode steps:  87, steps per second: 167, episode reward: -100.002, mean reward: -1.149 [-100.000, 11.883], mean action: 1.690 [0.000, 3.000],  loss: 13.228574, mae: 42.640681, mean_q: 30.710527, mean_eps: 0.806369
  43187/200000: episode: 445, duration: 0.727s, episode steps: 114, steps per second: 157, episode reward: -134.611, mean reward: -1.181 [-100.000, 12.065], mean action: 1.316 [0.000, 3.000],  loss: 6.080674, mae: 42.201591, mean_q: 25.909206, mean_eps: 0.805917
  43301/200000: episode: 446, duration: 0.754s, episode steps: 114, steps per second: 151, episode reward: -69.802, mean reward: -0.612 [-100.000, 12.952], mean action: 1.588 [0.000, 3.000],  loss: 19.683126, mae: 43.029040, mean_q: 29.207257, mean_eps: 0.805404
  43374/200000: episode: 447, duration: 0.469s, episode steps:  73, steps per second: 156, episode reward: 24.419, mean reward:  0.335 [-100.000, 81.407], mean action: 1.521 [0.000, 3.000],  loss: 12.922407, mae: 43.102922, mean_q: 27.939957, mean_eps: 0.804983
  43464/200000: episode: 448, duration: 0.660s, episode steps:  90, steps per second: 136, episode reward: -94.370, mean reward: -1.049 [-100.000, 29.535], mean action: 1.667 [0.000, 3.000],  loss: 14.514340, mae: 43.229297, mean_q: 29.345661, mean_eps: 0.804617
  43553/200000: episode: 449, duration: 0.663s, episode steps:  89, steps per second: 134, episode reward: -108.805, mean reward: -1.223 [-100.000,  7.167], mean action: 1.506 [0.000, 3.000],  loss: 19.554787, mae: 42.423355, mean_q: 29.027502, mean_eps: 0.804214
  43678/200000: episode: 450, duration: 0.750s, episode steps: 125, steps per second: 167, episode reward: -75.461, mean reward: -0.604 [-100.000, 12.698], mean action: 1.728 [0.000, 3.000],  loss: 11.877718, mae: 42.226291, mean_q: 29.049357, mean_eps: 0.803733
  43776/200000: episode: 451, duration: 0.572s, episode steps:  98, steps per second: 171, episode reward: -119.509, mean reward: -1.219 [-100.000,  7.831], mean action: 1.714 [0.000, 3.000],  loss: 8.321315, mae: 41.740516, mean_q: 28.597353, mean_eps: 0.803231
  43878/200000: episode: 452, duration: 0.618s, episode steps: 102, steps per second: 165, episode reward: -62.266, mean reward: -0.610 [-100.000, 14.585], mean action: 1.510 [0.000, 3.000],  loss: 7.260401, mae: 41.874087, mean_q: 28.470092, mean_eps: 0.802781
  43984/200000: episode: 453, duration: 0.638s, episode steps: 106, steps per second: 166, episode reward: -98.595, mean reward: -0.930 [-100.000, 15.247], mean action: 1.509 [0.000, 3.000],  loss: 14.873944, mae: 41.428559, mean_q: 28.906323, mean_eps: 0.802313
  44103/200000: episode: 454, duration: 0.710s, episode steps: 119, steps per second: 168, episode reward: -59.289, mean reward: -0.498 [-100.000,  8.375], mean action: 1.529 [0.000, 3.000],  loss: 11.820161, mae: 41.661282, mean_q: 28.215150, mean_eps: 0.801807
  44610/200000: episode: 455, duration: 3.160s, episode steps: 507, steps per second: 160, episode reward: -51.310, mean reward: -0.101 [-100.000, 44.981], mean action: 1.604 [0.000, 3.000],  loss: 14.473613, mae: 42.328559, mean_q: 27.147739, mean_eps: 0.800398
  44745/200000: episode: 456, duration: 0.788s, episode steps: 135, steps per second: 171, episode reward: -111.491, mean reward: -0.826 [-100.000, 19.102], mean action: 1.689 [0.000, 3.000],  loss: 16.512566, mae: 42.810500, mean_q: 25.299932, mean_eps: 0.798953
  44862/200000: episode: 457, duration: 0.703s, episode steps: 117, steps per second: 167, episode reward: -58.562, mean reward: -0.501 [-100.000, 23.734], mean action: 1.641 [0.000, 3.000],  loss: 12.965876, mae: 42.312075, mean_q: 25.106000, mean_eps: 0.798387
  44993/200000: episode: 458, duration: 0.886s, episode steps: 131, steps per second: 148, episode reward: -198.851, mean reward: -1.518 [-100.000, 71.229], mean action: 1.573 [0.000, 3.000],  loss: 16.002107, mae: 42.142029, mean_q: 26.747565, mean_eps: 0.797829
  45126/200000: episode: 459, duration: 0.894s, episode steps: 133, steps per second: 149, episode reward: -181.799, mean reward: -1.367 [-100.000,  9.860], mean action: 1.602 [0.000, 3.000],  loss: 11.579522, mae: 42.338010, mean_q: 25.743374, mean_eps: 0.797235
  45294/200000: episode: 460, duration: 1.111s, episode steps: 168, steps per second: 151, episode reward: -50.142, mean reward: -0.298 [-100.000, 13.333], mean action: 1.589 [0.000, 3.000],  loss: 14.290505, mae: 42.109188, mean_q: 25.326515, mean_eps: 0.796557
  45418/200000: episode: 461, duration: 0.860s, episode steps: 124, steps per second: 144, episode reward: -100.305, mean reward: -0.809 [-100.000, 20.075], mean action: 1.524 [0.000, 3.000],  loss: 10.183608, mae: 42.469865, mean_q: 25.293276, mean_eps: 0.795900
  45496/200000: episode: 462, duration: 0.580s, episode steps:  78, steps per second: 135, episode reward: -94.021, mean reward: -1.205 [-100.000,  5.481], mean action: 1.359 [0.000, 3.000],  loss: 16.150135, mae: 41.565102, mean_q: 26.064324, mean_eps: 0.795446
  45613/200000: episode: 463, duration: 0.931s, episode steps: 117, steps per second: 126, episode reward: -187.976, mean reward: -1.607 [-100.000, 25.215], mean action: 1.632 [0.000, 3.000],  loss: 12.463562, mae: 42.229252, mean_q: 25.272473, mean_eps: 0.795007
  45690/200000: episode: 464, duration: 0.556s, episode steps:  77, steps per second: 138, episode reward: -108.572, mean reward: -1.410 [-100.000, 28.739], mean action: 1.545 [0.000, 3.000],  loss: 8.592798, mae: 41.943429, mean_q: 24.632257, mean_eps: 0.794570
  45771/200000: episode: 465, duration: 0.588s, episode steps:  81, steps per second: 138, episode reward: -91.159, mean reward: -1.125 [-100.000, 18.157], mean action: 1.420 [0.000, 3.000],  loss: 12.160617, mae: 41.150613, mean_q: 26.749402, mean_eps: 0.794215
  45938/200000: episode: 466, duration: 1.222s, episode steps: 167, steps per second: 137, episode reward: -39.879, mean reward: -0.239 [-100.000, 17.106], mean action: 1.551 [0.000, 3.000],  loss: 8.350821, mae: 40.935833, mean_q: 25.935468, mean_eps: 0.793657
  46106/200000: episode: 467, duration: 1.132s, episode steps: 168, steps per second: 148, episode reward: -171.253, mean reward: -1.019 [-100.000,  9.743], mean action: 1.613 [0.000, 3.000],  loss: 14.370299, mae: 41.548958, mean_q: 26.980802, mean_eps: 0.792903
  46252/200000: episode: 468, duration: 0.929s, episode steps: 146, steps per second: 157, episode reward: -46.125, mean reward: -0.316 [-100.000, 10.803], mean action: 1.740 [0.000, 3.000],  loss: 20.155936, mae: 41.250098, mean_q: 27.292942, mean_eps: 0.792197
  46323/200000: episode: 469, duration: 0.414s, episode steps:  71, steps per second: 172, episode reward: -51.471, mean reward: -0.725 [-100.000,  7.614], mean action: 1.662 [0.000, 3.000],  loss: 10.580351, mae: 41.530051, mean_q: 28.732598, mean_eps: 0.791709
  46405/200000: episode: 470, duration: 0.500s, episode steps:  82, steps per second: 164, episode reward: -71.054, mean reward: -0.867 [-100.000,  6.908], mean action: 1.585 [0.000, 3.000],  loss: 9.876517, mae: 41.411854, mean_q: 28.052537, mean_eps: 0.791364
  46478/200000: episode: 471, duration: 0.450s, episode steps:  73, steps per second: 162, episode reward: -76.369, mean reward: -1.046 [-100.000,  8.099], mean action: 1.616 [0.000, 3.000],  loss: 17.696994, mae: 41.219070, mean_q: 26.776189, mean_eps: 0.791015
  46562/200000: episode: 472, duration: 0.542s, episode steps:  84, steps per second: 155, episode reward: -64.923, mean reward: -0.773 [-100.000,  6.930], mean action: 1.690 [0.000, 3.000],  loss: 6.693408, mae: 40.604942, mean_q: 26.410973, mean_eps: 0.790662
  46659/200000: episode: 473, duration: 0.582s, episode steps:  97, steps per second: 167, episode reward: -98.726, mean reward: -1.018 [-100.000, 24.052], mean action: 1.711 [0.000, 3.000],  loss: 10.288041, mae: 41.167328, mean_q: 29.060520, mean_eps: 0.790255
  46740/200000: episode: 474, duration: 0.468s, episode steps:  81, steps per second: 173, episode reward: -54.091, mean reward: -0.668 [-100.000, 15.645], mean action: 1.506 [0.000, 3.000],  loss: 11.348485, mae: 40.957662, mean_q: 27.862987, mean_eps: 0.789855
  46831/200000: episode: 475, duration: 0.534s, episode steps:  91, steps per second: 170, episode reward: -95.767, mean reward: -1.052 [-100.000, 11.953], mean action: 1.505 [0.000, 3.000],  loss: 8.926765, mae: 41.717617, mean_q: 26.119126, mean_eps: 0.789468
  46902/200000: episode: 476, duration: 0.444s, episode steps:  71, steps per second: 160, episode reward: -71.556, mean reward: -1.008 [-100.000,  7.327], mean action: 1.676 [0.000, 3.000],  loss: 17.488414, mae: 41.762624, mean_q: 29.898404, mean_eps: 0.789103
  47022/200000: episode: 477, duration: 0.705s, episode steps: 120, steps per second: 170, episode reward: -123.744, mean reward: -1.031 [-100.000,  6.963], mean action: 1.667 [0.000, 3.000],  loss: 9.878663, mae: 41.601475, mean_q: 27.825814, mean_eps: 0.788673
  47125/200000: episode: 478, duration: 0.672s, episode steps: 103, steps per second: 153, episode reward: -71.019, mean reward: -0.690 [-100.000,  7.991], mean action: 1.670 [0.000, 3.000],  loss: 6.840959, mae: 40.781540, mean_q: 27.956352, mean_eps: 0.788172
  47230/200000: episode: 479, duration: 0.729s, episode steps: 105, steps per second: 144, episode reward: -64.468, mean reward: -0.614 [-100.000,  7.614], mean action: 1.724 [0.000, 3.000],  loss: 10.414674, mae: 41.366313, mean_q: 28.086422, mean_eps: 0.787704
  47359/200000: episode: 480, duration: 0.818s, episode steps: 129, steps per second: 158, episode reward: -104.775, mean reward: -0.812 [-100.000, 26.857], mean action: 1.519 [0.000, 3.000],  loss: 12.482007, mae: 41.627664, mean_q: 27.771207, mean_eps: 0.787177
  47464/200000: episode: 481, duration: 0.653s, episode steps: 105, steps per second: 161, episode reward: -118.986, mean reward: -1.133 [-100.000,  6.369], mean action: 1.476 [0.000, 3.000],  loss: 5.014723, mae: 41.381551, mean_q: 27.901953, mean_eps: 0.786651
  47548/200000: episode: 482, duration: 0.547s, episode steps:  84, steps per second: 154, episode reward: -81.461, mean reward: -0.970 [-100.000, 14.906], mean action: 1.560 [0.000, 3.000],  loss: 8.627412, mae: 41.204693, mean_q: 26.976436, mean_eps: 0.786225
  47654/200000: episode: 483, duration: 0.683s, episode steps: 106, steps per second: 155, episode reward: -36.210, mean reward: -0.342 [-100.000,  8.232], mean action: 1.623 [0.000, 3.000],  loss: 14.815222, mae: 41.692030, mean_q: 24.768989, mean_eps: 0.785798
  47777/200000: episode: 484, duration: 0.726s, episode steps: 123, steps per second: 169, episode reward: -124.719, mean reward: -1.014 [-100.000,  9.187], mean action: 1.732 [0.000, 3.000],  loss: 13.802892, mae: 40.823039, mean_q: 26.808618, mean_eps: 0.785282
  47850/200000: episode: 485, duration: 0.422s, episode steps:  73, steps per second: 173, episode reward: -66.609, mean reward: -0.912 [-100.000,  6.679], mean action: 1.452 [0.000, 3.000],  loss: 8.418503, mae: 41.461009, mean_q: 27.733990, mean_eps: 0.784842
  47966/200000: episode: 486, duration: 0.742s, episode steps: 116, steps per second: 156, episode reward: -70.396, mean reward: -0.607 [-100.000,  9.950], mean action: 1.552 [0.000, 3.000],  loss: 13.592324, mae: 41.889649, mean_q: 27.194529, mean_eps: 0.784416
  48090/200000: episode: 487, duration: 0.726s, episode steps: 124, steps per second: 171, episode reward: -168.034, mean reward: -1.355 [-100.000,  5.274], mean action: 1.508 [0.000, 3.000],  loss: 7.681352, mae: 42.382210, mean_q: 29.563132, mean_eps: 0.783876
  48249/200000: episode: 488, duration: 0.923s, episode steps: 159, steps per second: 172, episode reward: -72.447, mean reward: -0.456 [-100.000, 16.765], mean action: 1.673 [0.000, 3.000],  loss: 13.484826, mae: 42.497256, mean_q: 27.939041, mean_eps: 0.783239
  48365/200000: episode: 489, duration: 0.717s, episode steps: 116, steps per second: 162, episode reward: -119.674, mean reward: -1.032 [-100.000, 10.120], mean action: 1.664 [0.000, 3.000],  loss: 7.979714, mae: 42.503589, mean_q: 29.678096, mean_eps: 0.782621
  48476/200000: episode: 490, duration: 0.632s, episode steps: 111, steps per second: 175, episode reward: -62.621, mean reward: -0.564 [-100.000, 43.809], mean action: 1.658 [0.000, 3.000],  loss: 16.145711, mae: 41.927148, mean_q: 29.441481, mean_eps: 0.782110
  48582/200000: episode: 491, duration: 0.688s, episode steps: 106, steps per second: 154, episode reward: -90.088, mean reward: -0.850 [-100.000, 16.710], mean action: 1.594 [0.000, 3.000],  loss: 11.940708, mae: 42.655327, mean_q: 30.109100, mean_eps: 0.781622
  48664/200000: episode: 492, duration: 0.513s, episode steps:  82, steps per second: 160, episode reward:  5.319, mean reward:  0.065 [-100.000, 55.899], mean action: 1.561 [0.000, 3.000],  loss: 11.168236, mae: 43.662772, mean_q: 28.605364, mean_eps: 0.781199
  48783/200000: episode: 493, duration: 0.705s, episode steps: 119, steps per second: 169, episode reward: -141.953, mean reward: -1.193 [-100.000, 11.558], mean action: 1.412 [0.000, 3.000],  loss: 8.279954, mae: 42.184004, mean_q: 28.800962, mean_eps: 0.780747
  48841/200000: episode: 494, duration: 0.331s, episode steps:  58, steps per second: 175, episode reward: -80.097, mean reward: -1.381 [-100.000, 17.358], mean action: 1.638 [0.000, 3.000],  loss: 17.281556, mae: 42.063310, mean_q: 29.342340, mean_eps: 0.780348
  48948/200000: episode: 495, duration: 0.615s, episode steps: 107, steps per second: 174, episode reward: -89.261, mean reward: -0.834 [-100.000, 17.446], mean action: 1.626 [0.000, 3.000],  loss: 9.690843, mae: 42.760445, mean_q: 29.361839, mean_eps: 0.779977
  49022/200000: episode: 496, duration: 0.450s, episode steps:  74, steps per second: 165, episode reward: -63.424, mean reward: -0.857 [-100.000, 17.925], mean action: 1.662 [0.000, 3.000],  loss: 15.043701, mae: 43.399930, mean_q: 31.107663, mean_eps: 0.779570
  49098/200000: episode: 497, duration: 0.453s, episode steps:  76, steps per second: 168, episode reward: -54.771, mean reward: -0.721 [-100.000, 16.758], mean action: 1.658 [0.000, 3.000],  loss: 9.369547, mae: 43.503994, mean_q: 33.392543, mean_eps: 0.779232
  49223/200000: episode: 498, duration: 0.736s, episode steps: 125, steps per second: 170, episode reward: -18.263, mean reward: -0.146 [-100.000, 38.037], mean action: 1.800 [0.000, 3.000],  loss: 7.526811, mae: 42.784359, mean_q: 32.304794, mean_eps: 0.778780
  49304/200000: episode: 499, duration: 0.467s, episode steps:  81, steps per second: 174, episode reward: -62.323, mean reward: -0.769 [-100.000,  9.371], mean action: 1.765 [0.000, 3.000],  loss: 8.876596, mae: 42.947644, mean_q: 30.118718, mean_eps: 0.778317
  49467/200000: episode: 500, duration: 1.056s, episode steps: 163, steps per second: 154, episode reward: -31.523, mean reward: -0.193 [-100.000, 35.405], mean action: 1.620 [0.000, 3.000],  loss: 9.100302, mae: 42.970908, mean_q: 32.041597, mean_eps: 0.777768
  50408/200000: episode: 501, duration: 7.176s, episode steps: 941, steps per second: 131, episode reward: -297.007, mean reward: -0.316 [-100.000, 84.019], mean action: 1.532 [0.000, 3.000],  loss: 10.644313, mae: 43.441792, mean_q: 31.540526, mean_eps: 0.775284
  50538/200000: episode: 502, duration: 0.900s, episode steps: 130, steps per second: 144, episode reward: -74.430, mean reward: -0.573 [-100.000,  9.392], mean action: 1.600 [0.000, 3.000],  loss: 12.410974, mae: 42.937292, mean_q: 29.728770, mean_eps: 0.772874
  50663/200000: episode: 503, duration: 0.967s, episode steps: 125, steps per second: 129, episode reward: -188.985, mean reward: -1.512 [-100.000, 20.833], mean action: 1.464 [0.000, 3.000],  loss: 10.327234, mae: 42.908472, mean_q: 32.407067, mean_eps: 0.772300
  50758/200000: episode: 504, duration: 0.621s, episode steps:  95, steps per second: 153, episode reward: -91.025, mean reward: -0.958 [-100.000,  6.861], mean action: 1.516 [0.000, 3.000],  loss: 8.070188, mae: 43.106820, mean_q: 30.886306, mean_eps: 0.771805
  50861/200000: episode: 505, duration: 0.645s, episode steps: 103, steps per second: 160, episode reward: -87.291, mean reward: -0.847 [-100.000, 11.528], mean action: 1.573 [0.000, 3.000],  loss: 10.973262, mae: 43.372855, mean_q: 31.569753, mean_eps: 0.771360
  51000/200000: episode: 506, duration: 0.817s, episode steps: 139, steps per second: 170, episode reward: -136.080, mean reward: -0.979 [-100.000, 14.302], mean action: 1.597 [0.000, 3.000],  loss: 14.025642, mae: 43.510604, mean_q: 30.880596, mean_eps: 0.770815
  51134/200000: episode: 507, duration: 0.758s, episode steps: 134, steps per second: 177, episode reward: -66.797, mean reward: -0.498 [-100.000, 10.963], mean action: 1.597 [0.000, 3.000],  loss: 13.418465, mae: 43.401491, mean_q: 32.618022, mean_eps: 0.770201
  51259/200000: episode: 508, duration: 0.774s, episode steps: 125, steps per second: 162, episode reward: -161.205, mean reward: -1.290 [-100.000,  7.264], mean action: 1.592 [0.000, 3.000],  loss: 12.186267, mae: 42.899658, mean_q: 32.493663, mean_eps: 0.769618
  51389/200000: episode: 509, duration: 0.873s, episode steps: 130, steps per second: 149, episode reward: -178.362, mean reward: -1.372 [-100.000, 15.697], mean action: 1.415 [0.000, 3.000],  loss: 14.158231, mae: 43.486440, mean_q: 34.775518, mean_eps: 0.769044
  51520/200000: episode: 510, duration: 0.970s, episode steps: 131, steps per second: 135, episode reward: -436.844, mean reward: -3.335 [-100.000, 51.718], mean action: 1.679 [0.000, 3.000],  loss: 12.823829, mae: 43.570184, mean_q: 34.692542, mean_eps: 0.768457
  51631/200000: episode: 511, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: -159.697, mean reward: -1.439 [-100.000,  4.891], mean action: 1.613 [0.000, 3.000],  loss: 9.965522, mae: 43.820157, mean_q: 35.635876, mean_eps: 0.767912
  51734/200000: episode: 512, duration: 0.644s, episode steps: 103, steps per second: 160, episode reward: -118.276, mean reward: -1.148 [-100.000,  7.794], mean action: 1.709 [0.000, 3.000],  loss: 10.979921, mae: 43.738060, mean_q: 32.843582, mean_eps: 0.767431
  51879/200000: episode: 513, duration: 0.960s, episode steps: 145, steps per second: 151, episode reward: -84.535, mean reward: -0.583 [-100.000,  7.348], mean action: 1.524 [0.000, 3.000],  loss: 10.683272, mae: 43.835412, mean_q: 34.550315, mean_eps: 0.766873
  51945/200000: episode: 514, duration: 0.431s, episode steps:  66, steps per second: 153, episode reward: -64.502, mean reward: -0.977 [-100.000,  8.465], mean action: 1.530 [0.000, 3.000],  loss: 7.781655, mae: 44.112765, mean_q: 35.428881, mean_eps: 0.766398
  52070/200000: episode: 515, duration: 0.775s, episode steps: 125, steps per second: 161, episode reward: -166.488, mean reward: -1.332 [-100.000, 67.514], mean action: 1.584 [0.000, 3.000],  loss: 10.570331, mae: 45.023036, mean_q: 34.919239, mean_eps: 0.765969
  52189/200000: episode: 516, duration: 0.811s, episode steps: 119, steps per second: 147, episode reward: -107.798, mean reward: -0.906 [-100.000, 30.630], mean action: 1.790 [0.000, 3.000],  loss: 11.170639, mae: 45.170648, mean_q: 36.067833, mean_eps: 0.765420
  52278/200000: episode: 517, duration: 0.554s, episode steps:  89, steps per second: 161, episode reward: -107.506, mean reward: -1.208 [-100.000,  8.701], mean action: 1.596 [0.000, 3.000],  loss: 10.340208, mae: 44.867707, mean_q: 36.586140, mean_eps: 0.764951
  52368/200000: episode: 518, duration: 0.572s, episode steps:  90, steps per second: 157, episode reward: -41.218, mean reward: -0.458 [-100.000,  8.232], mean action: 1.533 [0.000, 3.000],  loss: 23.645646, mae: 45.431485, mean_q: 35.946277, mean_eps: 0.764549
  52469/200000: episode: 519, duration: 0.616s, episode steps: 101, steps per second: 164, episode reward: -118.565, mean reward: -1.174 [-100.000,  8.122], mean action: 1.624 [0.000, 3.000],  loss: 13.154745, mae: 45.475339, mean_q: 38.158716, mean_eps: 0.764119
  52545/200000: episode: 520, duration: 0.493s, episode steps:  76, steps per second: 154, episode reward: -71.679, mean reward: -0.943 [-100.000, 14.578], mean action: 1.789 [0.000, 3.000],  loss: 22.589848, mae: 45.649100, mean_q: 38.273074, mean_eps: 0.763721
  52612/200000: episode: 521, duration: 0.472s, episode steps:  67, steps per second: 142, episode reward: -83.266, mean reward: -1.243 [-100.000,  8.124], mean action: 1.448 [0.000, 3.000],  loss: 10.928378, mae: 44.156363, mean_q: 36.896298, mean_eps: 0.763399
  52721/200000: episode: 522, duration: 0.682s, episode steps: 109, steps per second: 160, episode reward: -99.417, mean reward: -0.912 [-100.000,  7.140], mean action: 1.514 [0.000, 3.000],  loss: 9.115621, mae: 44.554587, mean_q: 36.231491, mean_eps: 0.763003
  52827/200000: episode: 523, duration: 0.676s, episode steps: 106, steps per second: 157, episode reward: -127.583, mean reward: -1.204 [-100.000,  7.435], mean action: 1.613 [0.000, 3.000],  loss: 16.044292, mae: 45.195627, mean_q: 38.465203, mean_eps: 0.762519
  52959/200000: episode: 524, duration: 0.855s, episode steps: 132, steps per second: 154, episode reward: 12.962, mean reward:  0.098 [-100.000, 80.571], mean action: 1.455 [0.000, 3.000],  loss: 14.742592, mae: 45.222674, mean_q: 39.069485, mean_eps: 0.761984
  53066/200000: episode: 525, duration: 0.803s, episode steps: 107, steps per second: 133, episode reward: -31.586, mean reward: -0.295 [-100.000, 15.231], mean action: 1.579 [0.000, 3.000],  loss: 17.741437, mae: 44.625551, mean_q: 39.655681, mean_eps: 0.761446
  53167/200000: episode: 526, duration: 0.707s, episode steps: 101, steps per second: 143, episode reward: -74.653, mean reward: -0.739 [-100.000,  9.410], mean action: 1.663 [0.000, 3.000],  loss: 9.619205, mae: 45.317348, mean_q: 39.501612, mean_eps: 0.760978
  53269/200000: episode: 527, duration: 0.623s, episode steps: 102, steps per second: 164, episode reward: -107.741, mean reward: -1.056 [-100.000,  7.763], mean action: 1.608 [0.000, 3.000],  loss: 11.626688, mae: 45.520574, mean_q: 40.883340, mean_eps: 0.760521
  53389/200000: episode: 528, duration: 0.700s, episode steps: 120, steps per second: 171, episode reward: -37.161, mean reward: -0.310 [-100.000, 26.454], mean action: 1.592 [0.000, 3.000],  loss: 13.827090, mae: 45.127190, mean_q: 39.479164, mean_eps: 0.760022
  53477/200000: episode: 529, duration: 0.498s, episode steps:  88, steps per second: 177, episode reward: -86.145, mean reward: -0.979 [-100.000,  5.443], mean action: 1.761 [0.000, 3.000],  loss: 15.383143, mae: 45.199698, mean_q: 39.525080, mean_eps: 0.759554
  53626/200000: episode: 530, duration: 0.975s, episode steps: 149, steps per second: 153, episode reward: -99.396, mean reward: -0.667 [-100.000, 23.544], mean action: 1.564 [0.000, 3.000],  loss: 15.741806, mae: 45.224638, mean_q: 40.789412, mean_eps: 0.759020
  53704/200000: episode: 531, duration: 0.498s, episode steps:  78, steps per second: 157, episode reward: -80.860, mean reward: -1.037 [-100.000,  8.853], mean action: 1.692 [0.000, 3.000],  loss: 15.693408, mae: 43.818310, mean_q: 38.375518, mean_eps: 0.758510
  53891/200000: episode: 532, duration: 1.145s, episode steps: 187, steps per second: 163, episode reward: -48.536, mean reward: -0.260 [-100.000,  6.616], mean action: 1.668 [0.000, 3.000],  loss: 12.030663, mae: 44.723072, mean_q: 40.795359, mean_eps: 0.757914
  53969/200000: episode: 533, duration: 0.494s, episode steps:  78, steps per second: 158, episode reward: -75.518, mean reward: -0.968 [-100.000,  7.893], mean action: 1.641 [0.000, 3.000],  loss: 18.936183, mae: 45.346936, mean_q: 39.965411, mean_eps: 0.757317
  54044/200000: episode: 534, duration: 0.485s, episode steps:  75, steps per second: 155, episode reward: -89.894, mean reward: -1.199 [-100.000, 13.705], mean action: 1.507 [0.000, 3.000],  loss: 9.669270, mae: 44.860623, mean_q: 39.453375, mean_eps: 0.756973
  54183/200000: episode: 535, duration: 0.920s, episode steps: 139, steps per second: 151, episode reward: -279.003, mean reward: -2.007 [-100.000,  6.619], mean action: 1.676 [0.000, 3.000],  loss: 7.446882, mae: 44.402153, mean_q: 40.377260, mean_eps: 0.756492
  54278/200000: episode: 536, duration: 0.646s, episode steps:  95, steps per second: 147, episode reward: -99.571, mean reward: -1.048 [-100.000, 14.991], mean action: 1.653 [0.000, 3.000],  loss: 12.072743, mae: 45.123857, mean_q: 40.717171, mean_eps: 0.755965
  54396/200000: episode: 537, duration: 0.781s, episode steps: 118, steps per second: 151, episode reward: -83.029, mean reward: -0.704 [-100.000, 13.647], mean action: 1.669 [0.000, 3.000],  loss: 10.324836, mae: 45.920866, mean_q: 40.739532, mean_eps: 0.755486
  54515/200000: episode: 538, duration: 0.825s, episode steps: 119, steps per second: 144, episode reward: -229.374, mean reward: -1.928 [-100.000, 63.124], mean action: 1.496 [0.000, 3.000],  loss: 18.157269, mae: 45.668142, mean_q: 39.726561, mean_eps: 0.754953
  54595/200000: episode: 539, duration: 0.677s, episode steps:  80, steps per second: 118, episode reward: -75.755, mean reward: -0.947 [-100.000, 18.107], mean action: 1.663 [0.000, 3.000],  loss: 9.877744, mae: 45.519837, mean_q: 41.539934, mean_eps: 0.754505
  54675/200000: episode: 540, duration: 0.496s, episode steps:  80, steps per second: 161, episode reward: -46.211, mean reward: -0.578 [-100.000, 11.717], mean action: 1.550 [0.000, 3.000],  loss: 13.519226, mae: 45.760202, mean_q: 40.673806, mean_eps: 0.754145
  54792/200000: episode: 541, duration: 0.733s, episode steps: 117, steps per second: 160, episode reward: -180.889, mean reward: -1.546 [-100.000,  7.221], mean action: 1.846 [0.000, 3.000],  loss: 11.845118, mae: 46.248580, mean_q: 41.839097, mean_eps: 0.753701
  54881/200000: episode: 542, duration: 0.606s, episode steps:  89, steps per second: 147, episode reward: -28.398, mean reward: -0.319 [-100.000, 17.974], mean action: 1.629 [0.000, 3.000],  loss: 11.806889, mae: 46.273384, mean_q: 43.143471, mean_eps: 0.753238
  54986/200000: episode: 543, duration: 0.712s, episode steps: 105, steps per second: 148, episode reward: -73.729, mean reward: -0.702 [-100.000, 15.352], mean action: 1.629 [0.000, 3.000],  loss: 14.875752, mae: 45.939362, mean_q: 42.600138, mean_eps: 0.752802
  55061/200000: episode: 544, duration: 0.475s, episode steps:  75, steps per second: 158, episode reward: -23.379, mean reward: -0.312 [-100.000, 12.880], mean action: 1.920 [0.000, 3.000],  loss: 18.572045, mae: 46.015247, mean_q: 41.363553, mean_eps: 0.752397
  55188/200000: episode: 545, duration: 0.871s, episode steps: 127, steps per second: 146, episode reward: -85.204, mean reward: -0.671 [-100.000,  8.509], mean action: 1.394 [0.000, 3.000],  loss: 14.222538, mae: 46.245469, mean_q: 43.903409, mean_eps: 0.751942
  55271/200000: episode: 546, duration: 0.609s, episode steps:  83, steps per second: 136, episode reward: -81.237, mean reward: -0.979 [-100.000,  9.212], mean action: 1.530 [0.000, 3.000],  loss: 14.805697, mae: 45.986604, mean_q: 42.695017, mean_eps: 0.751470
  55340/200000: episode: 547, duration: 0.505s, episode steps:  69, steps per second: 137, episode reward: -52.045, mean reward: -0.754 [-100.000, 17.347], mean action: 1.623 [0.000, 3.000],  loss: 16.948253, mae: 45.470571, mean_q: 42.265942, mean_eps: 0.751127
  55434/200000: episode: 548, duration: 0.718s, episode steps:  94, steps per second: 131, episode reward: -51.657, mean reward: -0.550 [-100.000,  7.414], mean action: 1.596 [0.000, 3.000],  loss: 13.224206, mae: 45.817037, mean_q: 43.875501, mean_eps: 0.750761
  55520/200000: episode: 549, duration: 0.732s, episode steps:  86, steps per second: 117, episode reward: -66.435, mean reward: -0.772 [-100.000, 12.931], mean action: 1.814 [0.000, 3.000],  loss: 15.008612, mae: 45.814376, mean_q: 42.412749, mean_eps: 0.750356
  55614/200000: episode: 550, duration: 0.660s, episode steps:  94, steps per second: 142, episode reward: -72.623, mean reward: -0.773 [-100.000, 17.728], mean action: 1.702 [0.000, 3.000],  loss: 15.740098, mae: 45.966810, mean_q: 45.385275, mean_eps: 0.749951
  55692/200000: episode: 551, duration: 0.517s, episode steps:  78, steps per second: 151, episode reward: -96.395, mean reward: -1.236 [-100.000,  8.535], mean action: 1.462 [0.000, 3.000],  loss: 13.426304, mae: 45.478622, mean_q: 43.443383, mean_eps: 0.749564
  55765/200000: episode: 552, duration: 0.581s, episode steps:  73, steps per second: 126, episode reward: -136.132, mean reward: -1.865 [-100.000, 21.753], mean action: 1.521 [0.000, 3.000],  loss: 23.581866, mae: 44.970550, mean_q: 43.395461, mean_eps: 0.749224
  55923/200000: episode: 553, duration: 1.150s, episode steps: 158, steps per second: 137, episode reward: -10.011, mean reward: -0.063 [-100.000, 84.793], mean action: 1.551 [0.000, 3.000],  loss: 13.965678, mae: 45.690448, mean_q: 44.520349, mean_eps: 0.748704
  56081/200000: episode: 554, duration: 1.171s, episode steps: 158, steps per second: 135, episode reward:  2.021, mean reward:  0.013 [-100.000, 19.868], mean action: 1.709 [0.000, 3.000],  loss: 12.491543, mae: 46.247641, mean_q: 44.461084, mean_eps: 0.747993
  56167/200000: episode: 555, duration: 0.632s, episode steps:  86, steps per second: 136, episode reward: -193.658, mean reward: -2.252 [-100.000,  5.325], mean action: 1.593 [0.000, 3.000],  loss: 16.332919, mae: 46.285140, mean_q: 45.209664, mean_eps: 0.747444
  56286/200000: episode: 556, duration: 0.846s, episode steps: 119, steps per second: 141, episode reward: -94.639, mean reward: -0.795 [-100.000, 19.979], mean action: 1.630 [0.000, 3.000],  loss: 13.935298, mae: 45.968573, mean_q: 44.839372, mean_eps: 0.746983
  56412/200000: episode: 557, duration: 0.911s, episode steps: 126, steps per second: 138, episode reward: -96.315, mean reward: -0.764 [-100.000, 12.769], mean action: 1.516 [0.000, 3.000],  loss: 9.801846, mae: 46.275503, mean_q: 45.235914, mean_eps: 0.746432
  56496/200000: episode: 558, duration: 0.531s, episode steps:  84, steps per second: 158, episode reward: -105.333, mean reward: -1.254 [-100.000, 12.782], mean action: 1.524 [0.000, 3.000],  loss: 9.186926, mae: 45.530600, mean_q: 42.758412, mean_eps: 0.745959
  56607/200000: episode: 559, duration: 0.948s, episode steps: 111, steps per second: 117, episode reward: -95.222, mean reward: -0.858 [-100.000,  8.621], mean action: 1.568 [0.000, 3.000],  loss: 14.080236, mae: 48.045446, mean_q: 47.173098, mean_eps: 0.745521
  56749/200000: episode: 560, duration: 1.128s, episode steps: 142, steps per second: 126, episode reward: -97.873, mean reward: -0.689 [-100.000,  7.147], mean action: 1.556 [0.000, 3.000],  loss: 14.215845, mae: 47.889766, mean_q: 46.449069, mean_eps: 0.744951
  56895/200000: episode: 561, duration: 0.978s, episode steps: 146, steps per second: 149, episode reward: -75.393, mean reward: -0.516 [-100.000,  7.428], mean action: 1.589 [0.000, 3.000],  loss: 16.000014, mae: 46.323387, mean_q: 45.380632, mean_eps: 0.744303
  56969/200000: episode: 562, duration: 0.601s, episode steps:  74, steps per second: 123, episode reward: -28.002, mean reward: -0.378 [-100.000, 11.188], mean action: 1.608 [0.000, 3.000],  loss: 16.317580, mae: 46.896337, mean_q: 46.566460, mean_eps: 0.743808
  57054/200000: episode: 563, duration: 0.584s, episode steps:  85, steps per second: 146, episode reward: -56.209, mean reward: -0.661 [-100.000, 13.900], mean action: 1.659 [0.000, 3.000],  loss: 11.055327, mae: 46.473133, mean_q: 45.211298, mean_eps: 0.743451
  57170/200000: episode: 564, duration: 1.146s, episode steps: 116, steps per second: 101, episode reward: -45.218, mean reward: -0.390 [-100.000,  9.316], mean action: 1.741 [0.000, 3.000],  loss: 11.001315, mae: 46.285430, mean_q: 45.857414, mean_eps: 0.742998
  57263/200000: episode: 565, duration: 0.926s, episode steps:  93, steps per second: 100, episode reward: -41.762, mean reward: -0.449 [-100.000,  8.879], mean action: 1.796 [0.000, 3.000],  loss: 14.270160, mae: 47.144024, mean_q: 46.292242, mean_eps: 0.742528
  57445/200000: episode: 566, duration: 1.671s, episode steps: 182, steps per second: 109, episode reward: -213.321, mean reward: -1.172 [-100.000, 88.829], mean action: 1.577 [0.000, 3.000],  loss: 12.288622, mae: 46.424185, mean_q: 45.720187, mean_eps: 0.741909
  57610/200000: episode: 567, duration: 1.099s, episode steps: 165, steps per second: 150, episode reward: -191.870, mean reward: -1.163 [-100.000, 13.534], mean action: 1.545 [0.000, 3.000],  loss: 11.300710, mae: 46.723963, mean_q: 45.907088, mean_eps: 0.741128
  57746/200000: episode: 568, duration: 0.885s, episode steps: 136, steps per second: 154, episode reward: -223.230, mean reward: -1.641 [-100.000,  4.705], mean action: 1.390 [0.000, 3.000],  loss: 11.168719, mae: 46.707368, mean_q: 46.309944, mean_eps: 0.740451
  57846/200000: episode: 569, duration: 1.075s, episode steps: 100, steps per second:  93, episode reward: -36.142, mean reward: -0.361 [-100.000,  8.163], mean action: 1.470 [0.000, 3.000],  loss: 11.798589, mae: 46.551432, mean_q: 45.908986, mean_eps: 0.739920
  57951/200000: episode: 570, duration: 0.769s, episode steps: 105, steps per second: 137, episode reward: -140.254, mean reward: -1.336 [-100.000,  8.714], mean action: 1.562 [0.000, 3.000],  loss: 15.106622, mae: 46.831167, mean_q: 45.500254, mean_eps: 0.739459
  58029/200000: episode: 571, duration: 0.574s, episode steps:  78, steps per second: 136, episode reward: -67.175, mean reward: -0.861 [-100.000, 10.048], mean action: 1.590 [0.000, 3.000],  loss: 12.186857, mae: 47.176223, mean_q: 47.898615, mean_eps: 0.739047
  58104/200000: episode: 572, duration: 0.542s, episode steps:  75, steps per second: 138, episode reward: -126.569, mean reward: -1.688 [-100.000, 16.269], mean action: 1.560 [0.000, 3.000],  loss: 15.449311, mae: 46.667371, mean_q: 45.814593, mean_eps: 0.738703
  58164/200000: episode: 573, duration: 0.532s, episode steps:  60, steps per second: 113, episode reward: -71.303, mean reward: -1.188 [-100.000, 15.244], mean action: 1.717 [0.000, 3.000],  loss: 13.867268, mae: 47.249079, mean_q: 45.129388, mean_eps: 0.738399
  58320/200000: episode: 574, duration: 1.206s, episode steps: 156, steps per second: 129, episode reward: -45.929, mean reward: -0.294 [-100.000, 16.848], mean action: 1.705 [0.000, 3.000],  loss: 12.621584, mae: 47.454002, mean_q: 45.481862, mean_eps: 0.737913
  58441/200000: episode: 575, duration: 1.067s, episode steps: 121, steps per second: 113, episode reward: -4.127, mean reward: -0.034 [-100.000, 76.325], mean action: 1.446 [0.000, 3.000],  loss: 13.125173, mae: 48.004033, mean_q: 45.225523, mean_eps: 0.737290
  58571/200000: episode: 576, duration: 0.941s, episode steps: 130, steps per second: 138, episode reward: -168.566, mean reward: -1.297 [-100.000,  7.591], mean action: 1.746 [0.000, 3.000],  loss: 24.815451, mae: 47.830190, mean_q: 45.281199, mean_eps: 0.736725
  58655/200000: episode: 577, duration: 0.533s, episode steps:  84, steps per second: 157, episode reward: -122.277, mean reward: -1.456 [-100.000, 10.808], mean action: 1.583 [0.000, 3.000],  loss: 11.640320, mae: 47.032899, mean_q: 46.217294, mean_eps: 0.736244
  58829/200000: episode: 578, duration: 1.131s, episode steps: 174, steps per second: 154, episode reward: -23.338, mean reward: -0.134 [-100.000, 12.493], mean action: 1.609 [0.000, 3.000],  loss: 15.236206, mae: 47.623902, mean_q: 46.026873, mean_eps: 0.735663
  58983/200000: episode: 579, duration: 1.069s, episode steps: 154, steps per second: 144, episode reward: -173.017, mean reward: -1.123 [-100.000,  4.817], mean action: 1.455 [0.000, 3.000],  loss: 14.359636, mae: 47.223895, mean_q: 46.090937, mean_eps: 0.734925
  59097/200000: episode: 580, duration: 0.736s, episode steps: 114, steps per second: 155, episode reward: -66.819, mean reward: -0.586 [-100.000, 11.927], mean action: 1.561 [0.000, 3.000],  loss: 11.450951, mae: 46.550299, mean_q: 44.551722, mean_eps: 0.734322
  59236/200000: episode: 581, duration: 0.902s, episode steps: 139, steps per second: 154, episode reward: -116.045, mean reward: -0.835 [-100.000, 13.759], mean action: 1.676 [0.000, 3.000],  loss: 8.982055, mae: 46.237338, mean_q: 45.675546, mean_eps: 0.733753
  59315/200000: episode: 582, duration: 0.512s, episode steps:  79, steps per second: 154, episode reward: -110.792, mean reward: -1.402 [-100.000, 18.246], mean action: 1.418 [0.000, 3.000],  loss: 10.265760, mae: 46.564531, mean_q: 44.874350, mean_eps: 0.733263
  59380/200000: episode: 583, duration: 0.388s, episode steps:  65, steps per second: 167, episode reward: -62.368, mean reward: -0.960 [-100.000, 21.137], mean action: 1.369 [0.000, 3.000],  loss: 14.515309, mae: 45.916953, mean_q: 45.062140, mean_eps: 0.732938
  59453/200000: episode: 584, duration: 0.519s, episode steps:  73, steps per second: 141, episode reward: -65.490, mean reward: -0.897 [-100.000, 30.484], mean action: 1.712 [0.000, 3.000],  loss: 19.301671, mae: 47.474746, mean_q: 45.366381, mean_eps: 0.732628
  59524/200000: episode: 585, duration: 0.523s, episode steps:  71, steps per second: 136, episode reward: -112.069, mean reward: -1.578 [-100.000, 17.398], mean action: 1.507 [0.000, 3.000],  loss: 15.341984, mae: 46.501755, mean_q: 47.139495, mean_eps: 0.732304
  59616/200000: episode: 586, duration: 0.663s, episode steps:  92, steps per second: 139, episode reward: -76.322, mean reward: -0.830 [-100.000, 10.769], mean action: 1.663 [0.000, 3.000],  loss: 18.138449, mae: 47.696012, mean_q: 47.189776, mean_eps: 0.731937
  59747/200000: episode: 587, duration: 0.903s, episode steps: 131, steps per second: 145, episode reward: -66.576, mean reward: -0.508 [-100.000,  6.889], mean action: 1.450 [0.000, 3.000],  loss: 13.497269, mae: 48.244268, mean_q: 50.091444, mean_eps: 0.731436
  59868/200000: episode: 588, duration: 0.836s, episode steps: 121, steps per second: 145, episode reward: -44.492, mean reward: -0.368 [-100.000,  9.699], mean action: 1.694 [0.000, 3.000],  loss: 15.985741, mae: 47.378724, mean_q: 47.505223, mean_eps: 0.730868
  59973/200000: episode: 589, duration: 0.709s, episode steps: 105, steps per second: 148, episode reward: -83.449, mean reward: -0.795 [-100.000,  9.970], mean action: 1.638 [0.000, 3.000],  loss: 13.167053, mae: 47.573673, mean_q: 49.348564, mean_eps: 0.730360
  60103/200000: episode: 590, duration: 0.856s, episode steps: 130, steps per second: 152, episode reward: -270.883, mean reward: -2.084 [-100.000, 35.102], mean action: 1.569 [0.000, 3.000],  loss: 12.712444, mae: 47.813276, mean_q: 48.333319, mean_eps: 0.729831
  60194/200000: episode: 591, duration: 0.644s, episode steps:  91, steps per second: 141, episode reward: -18.035, mean reward: -0.198 [-100.000, 16.728], mean action: 1.835 [0.000, 3.000],  loss: 13.878744, mae: 48.005681, mean_q: 47.892048, mean_eps: 0.729334
  60361/200000: episode: 592, duration: 1.046s, episode steps: 167, steps per second: 160, episode reward: -31.051, mean reward: -0.186 [-100.000, 15.979], mean action: 1.641 [0.000, 3.000],  loss: 11.685584, mae: 48.200407, mean_q: 49.250979, mean_eps: 0.728754
  60513/200000: episode: 593, duration: 0.915s, episode steps: 152, steps per second: 166, episode reward: -143.822, mean reward: -0.946 [-100.000, 15.901], mean action: 1.434 [0.000, 3.000],  loss: 10.932516, mae: 48.707365, mean_q: 50.497180, mean_eps: 0.728036
  60675/200000: episode: 594, duration: 0.974s, episode steps: 162, steps per second: 166, episode reward: -36.182, mean reward: -0.223 [-100.000,  9.471], mean action: 1.623 [0.000, 3.000],  loss: 12.608018, mae: 49.127606, mean_q: 50.858650, mean_eps: 0.727329
  60780/200000: episode: 595, duration: 0.658s, episode steps: 105, steps per second: 160, episode reward: -101.569, mean reward: -0.967 [-100.000, 10.878], mean action: 1.619 [0.000, 3.000],  loss: 9.264423, mae: 48.538402, mean_q: 49.689942, mean_eps: 0.726728
  60909/200000: episode: 596, duration: 0.832s, episode steps: 129, steps per second: 155, episode reward: -131.242, mean reward: -1.017 [-100.000,  8.391], mean action: 1.558 [0.000, 3.000],  loss: 15.458540, mae: 49.175859, mean_q: 49.718626, mean_eps: 0.726202
  61012/200000: episode: 597, duration: 0.651s, episode steps: 103, steps per second: 158, episode reward: -83.406, mean reward: -0.810 [-100.000, 14.574], mean action: 1.670 [0.000, 3.000],  loss: 15.014039, mae: 48.039179, mean_q: 49.099283, mean_eps: 0.725680
  61198/200000: episode: 598, duration: 1.181s, episode steps: 186, steps per second: 158, episode reward: -55.712, mean reward: -0.300 [-100.000, 10.393], mean action: 1.667 [0.000, 3.000],  loss: 13.094978, mae: 48.649530, mean_q: 49.041654, mean_eps: 0.725030
  61289/200000: episode: 599, duration: 0.616s, episode steps:  91, steps per second: 148, episode reward: -119.083, mean reward: -1.309 [-100.000,  5.813], mean action: 1.736 [0.000, 3.000],  loss: 9.807873, mae: 47.877996, mean_q: 47.504794, mean_eps: 0.724407
  61419/200000: episode: 600, duration: 0.837s, episode steps: 130, steps per second: 155, episode reward: -23.010, mean reward: -0.177 [-100.000, 17.388], mean action: 1.692 [0.000, 3.000],  loss: 17.402340, mae: 48.626501, mean_q: 49.013167, mean_eps: 0.723909
  61523/200000: episode: 601, duration: 0.650s, episode steps: 104, steps per second: 160, episode reward: -89.806, mean reward: -0.864 [-100.000, 10.760], mean action: 1.663 [0.000, 3.000],  loss: 13.473186, mae: 47.856093, mean_q: 48.053669, mean_eps: 0.723383
  61619/200000: episode: 602, duration: 0.612s, episode steps:  96, steps per second: 157, episode reward: -82.859, mean reward: -0.863 [-100.000, 20.036], mean action: 1.521 [0.000, 3.000],  loss: 17.448697, mae: 48.355058, mean_q: 48.204602, mean_eps: 0.722933
  61707/200000: episode: 603, duration: 0.531s, episode steps:  88, steps per second: 166, episode reward: -101.489, mean reward: -1.153 [-100.000,  5.181], mean action: 1.739 [0.000, 3.000],  loss: 15.321268, mae: 47.377820, mean_q: 47.701151, mean_eps: 0.722519
  61804/200000: episode: 604, duration: 0.634s, episode steps:  97, steps per second: 153, episode reward: -70.106, mean reward: -0.723 [-100.000, 12.057], mean action: 1.505 [0.000, 3.000],  loss: 14.630380, mae: 47.490331, mean_q: 47.443390, mean_eps: 0.722102
  61925/200000: episode: 605, duration: 0.798s, episode steps: 121, steps per second: 152, episode reward: -30.786, mean reward: -0.254 [-100.000, 24.080], mean action: 1.818 [0.000, 3.000],  loss: 13.413952, mae: 47.516356, mean_q: 48.106117, mean_eps: 0.721612
  62029/200000: episode: 606, duration: 0.648s, episode steps: 104, steps per second: 161, episode reward: -227.125, mean reward: -2.184 [-100.000, 71.468], mean action: 1.712 [0.000, 3.000],  loss: 23.793161, mae: 47.267957, mean_q: 47.098639, mean_eps: 0.721106
  62110/200000: episode: 607, duration: 0.479s, episode steps:  81, steps per second: 169, episode reward: -47.188, mean reward: -0.583 [-100.000, 19.472], mean action: 1.654 [0.000, 3.000],  loss: 16.879481, mae: 47.451266, mean_q: 46.532425, mean_eps: 0.720689
  62256/200000: episode: 608, duration: 0.925s, episode steps: 146, steps per second: 158, episode reward: -74.305, mean reward: -0.509 [-100.000, 22.824], mean action: 1.582 [0.000, 3.000],  loss: 15.442017, mae: 48.322566, mean_q: 47.740540, mean_eps: 0.720179
  62443/200000: episode: 609, duration: 1.139s, episode steps: 187, steps per second: 164, episode reward: -149.727, mean reward: -0.801 [-100.000,  5.450], mean action: 1.647 [0.000, 3.000],  loss: 15.002621, mae: 49.259730, mean_q: 50.865137, mean_eps: 0.719430
  62561/200000: episode: 610, duration: 0.733s, episode steps: 118, steps per second: 161, episode reward: -19.552, mean reward: -0.166 [-100.000, 83.398], mean action: 1.542 [0.000, 3.000],  loss: 11.662506, mae: 49.321845, mean_q: 51.903632, mean_eps: 0.718743
  63561/200000: episode: 611, duration: 6.930s, episode steps: 1000, steps per second: 144, episode reward: 60.526, mean reward:  0.061 [-24.550, 35.390], mean action: 1.784 [0.000, 3.000],  loss: 15.252722, mae: 48.665706, mean_q: 50.193398, mean_eps: 0.716228
  63735/200000: episode: 612, duration: 1.149s, episode steps: 174, steps per second: 151, episode reward: -236.957, mean reward: -1.362 [-100.000, 34.091], mean action: 1.586 [0.000, 3.000],  loss: 16.828629, mae: 48.131447, mean_q: 49.444957, mean_eps: 0.713586
  63830/200000: episode: 613, duration: 0.641s, episode steps:  95, steps per second: 148, episode reward: -107.161, mean reward: -1.128 [-100.000, 11.818], mean action: 1.621 [0.000, 3.000],  loss: 16.094274, mae: 46.977921, mean_q: 48.494434, mean_eps: 0.712981
  63977/200000: episode: 614, duration: 0.968s, episode steps: 147, steps per second: 152, episode reward: -376.645, mean reward: -2.562 [-100.000, 68.988], mean action: 1.592 [0.000, 3.000],  loss: 13.720049, mae: 47.833426, mean_q: 48.921879, mean_eps: 0.712437
  64165/200000: episode: 615, duration: 1.212s, episode steps: 188, steps per second: 155, episode reward: -76.829, mean reward: -0.409 [-100.000, 10.229], mean action: 1.702 [0.000, 3.000],  loss: 22.806363, mae: 48.344871, mean_q: 48.413915, mean_eps: 0.711683
  64327/200000: episode: 616, duration: 1.018s, episode steps: 162, steps per second: 159, episode reward: -253.814, mean reward: -1.567 [-100.000, 76.900], mean action: 1.556 [0.000, 3.000],  loss: 11.222622, mae: 48.713724, mean_q: 50.431046, mean_eps: 0.710895
  64479/200000: episode: 617, duration: 0.975s, episode steps: 152, steps per second: 156, episode reward: -45.998, mean reward: -0.303 [-100.000, 12.707], mean action: 1.645 [0.000, 3.000],  loss: 13.523170, mae: 48.941877, mean_q: 50.926611, mean_eps: 0.710189
  64576/200000: episode: 618, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -26.020, mean reward: -0.268 [-100.000, 22.643], mean action: 1.608 [0.000, 3.000],  loss: 16.068411, mae: 49.529696, mean_q: 51.711853, mean_eps: 0.709628
  64655/200000: episode: 619, duration: 0.492s, episode steps:  79, steps per second: 160, episode reward: -97.659, mean reward: -1.236 [-100.000, 12.815], mean action: 1.620 [0.000, 3.000],  loss: 18.295321, mae: 49.397939, mean_q: 52.071039, mean_eps: 0.709232
  64780/200000: episode: 620, duration: 0.781s, episode steps: 125, steps per second: 160, episode reward: -16.449, mean reward: -0.132 [-100.000, 59.661], mean action: 1.616 [0.000, 3.000],  loss: 15.421949, mae: 49.334558, mean_q: 51.408510, mean_eps: 0.708774
  64906/200000: episode: 621, duration: 0.810s, episode steps: 126, steps per second: 156, episode reward: -68.643, mean reward: -0.545 [-100.000, 10.856], mean action: 1.603 [0.000, 3.000],  loss: 14.756668, mae: 49.845959, mean_q: 51.759996, mean_eps: 0.708209
  65057/200000: episode: 622, duration: 0.933s, episode steps: 151, steps per second: 162, episode reward: -76.060, mean reward: -0.504 [-100.000, 11.770], mean action: 1.828 [0.000, 3.000],  loss: 16.899954, mae: 49.909237, mean_q: 51.640821, mean_eps: 0.707586
  65229/200000: episode: 623, duration: 1.102s, episode steps: 172, steps per second: 156, episode reward: -24.507, mean reward: -0.142 [-100.000, 24.104], mean action: 1.715 [0.000, 3.000],  loss: 14.212302, mae: 50.115496, mean_q: 53.772177, mean_eps: 0.706859
  65345/200000: episode: 624, duration: 0.711s, episode steps: 116, steps per second: 163, episode reward: -73.976, mean reward: -0.638 [-100.000, 14.364], mean action: 1.655 [0.000, 3.000],  loss: 19.841595, mae: 48.861447, mean_q: 51.940169, mean_eps: 0.706211
  65489/200000: episode: 625, duration: 0.869s, episode steps: 144, steps per second: 166, episode reward: -78.354, mean reward: -0.544 [-100.000,  9.583], mean action: 1.674 [0.000, 3.000],  loss: 19.116675, mae: 50.228354, mean_q: 54.061994, mean_eps: 0.705626
  65577/200000: episode: 626, duration: 0.559s, episode steps:  88, steps per second: 157, episode reward: -88.926, mean reward: -1.011 [-100.000, 18.251], mean action: 1.807 [0.000, 3.000],  loss: 12.446310, mae: 49.241475, mean_q: 53.745279, mean_eps: 0.705104
  65668/200000: episode: 627, duration: 0.554s, episode steps:  91, steps per second: 164, episode reward: -64.103, mean reward: -0.704 [-100.000, 41.573], mean action: 1.615 [0.000, 3.000],  loss: 19.624832, mae: 49.815791, mean_q: 52.544484, mean_eps: 0.704701
  65803/200000: episode: 628, duration: 0.871s, episode steps: 135, steps per second: 155, episode reward: -79.744, mean reward: -0.591 [-100.000, 15.735], mean action: 1.511 [0.000, 3.000],  loss: 16.727625, mae: 51.139753, mean_q: 56.322916, mean_eps: 0.704192
  65908/200000: episode: 629, duration: 0.775s, episode steps: 105, steps per second: 136, episode reward: -26.809, mean reward: -0.255 [-100.000, 51.396], mean action: 1.552 [0.000, 3.000],  loss: 15.526483, mae: 50.528318, mean_q: 54.546769, mean_eps: 0.703652
  66034/200000: episode: 630, duration: 0.823s, episode steps: 126, steps per second: 153, episode reward: -62.617, mean reward: -0.497 [-100.000,  8.622], mean action: 1.413 [0.000, 3.000],  loss: 19.333716, mae: 51.023367, mean_q: 54.296850, mean_eps: 0.703133
  66150/200000: episode: 631, duration: 0.698s, episode steps: 116, steps per second: 166, episode reward: -60.676, mean reward: -0.523 [-100.000,  8.175], mean action: 1.595 [0.000, 3.000],  loss: 17.659070, mae: 51.130325, mean_q: 54.157929, mean_eps: 0.702588
  66256/200000: episode: 632, duration: 0.694s, episode steps: 106, steps per second: 153, episode reward: -123.486, mean reward: -1.165 [-100.000, 12.158], mean action: 1.613 [0.000, 3.000],  loss: 14.360730, mae: 51.004124, mean_q: 55.691238, mean_eps: 0.702089
  66375/200000: episode: 633, duration: 0.749s, episode steps: 119, steps per second: 159, episode reward: -115.644, mean reward: -0.972 [-100.000,  9.739], mean action: 1.479 [0.000, 3.000],  loss: 18.069412, mae: 51.445163, mean_q: 57.185357, mean_eps: 0.701583
  66467/200000: episode: 634, duration: 0.575s, episode steps:  92, steps per second: 160, episode reward: -122.405, mean reward: -1.330 [-100.000,  7.595], mean action: 1.707 [0.000, 3.000],  loss: 19.118391, mae: 51.215879, mean_q: 54.464398, mean_eps: 0.701108
  66580/200000: episode: 635, duration: 0.730s, episode steps: 113, steps per second: 155, episode reward: -69.896, mean reward: -0.619 [-100.000, 18.351], mean action: 1.619 [0.000, 3.000],  loss: 17.027517, mae: 50.941576, mean_q: 55.383267, mean_eps: 0.700647
  66696/200000: episode: 636, duration: 0.736s, episode steps: 116, steps per second: 158, episode reward:  3.847, mean reward:  0.033 [-100.000, 11.621], mean action: 1.716 [0.000, 3.000],  loss: 20.653460, mae: 51.433210, mean_q: 56.514409, mean_eps: 0.700131
  66794/200000: episode: 637, duration: 0.612s, episode steps:  98, steps per second: 160, episode reward: -47.732, mean reward: -0.487 [-100.000, 13.261], mean action: 1.786 [0.000, 3.000],  loss: 16.941726, mae: 51.097896, mean_q: 55.507347, mean_eps: 0.699650
  66909/200000: episode: 638, duration: 0.755s, episode steps: 115, steps per second: 152, episode reward: -262.794, mean reward: -2.285 [-100.000, 13.312], mean action: 1.583 [0.000, 3.000],  loss: 16.056504, mae: 51.386190, mean_q: 55.443854, mean_eps: 0.699171
  67069/200000: episode: 639, duration: 1.018s, episode steps: 160, steps per second: 157, episode reward: 14.161, mean reward:  0.089 [-100.000, 25.156], mean action: 1.606 [0.000, 3.000],  loss: 19.998385, mae: 51.892361, mean_q: 56.418606, mean_eps: 0.698552
  67154/200000: episode: 640, duration: 0.534s, episode steps:  85, steps per second: 159, episode reward: -39.080, mean reward: -0.460 [-100.000, 19.958], mean action: 1.776 [0.000, 3.000],  loss: 14.915942, mae: 51.734259, mean_q: 56.228773, mean_eps: 0.698001
  67271/200000: episode: 641, duration: 0.772s, episode steps: 117, steps per second: 152, episode reward: -95.250, mean reward: -0.814 [-100.000, 16.791], mean action: 1.658 [0.000, 3.000],  loss: 11.569700, mae: 52.398572, mean_q: 57.499794, mean_eps: 0.697546
  67372/200000: episode: 642, duration: 0.652s, episode steps: 101, steps per second: 155, episode reward: -49.777, mean reward: -0.493 [-100.000, 21.999], mean action: 1.515 [0.000, 3.000],  loss: 19.134881, mae: 52.291192, mean_q: 58.228386, mean_eps: 0.697055
  67522/200000: episode: 643, duration: 0.952s, episode steps: 150, steps per second: 158, episode reward: -26.415, mean reward: -0.176 [-100.000, 11.771], mean action: 1.620 [0.000, 3.000],  loss: 17.480237, mae: 51.871014, mean_q: 56.377153, mean_eps: 0.696491
  67630/200000: episode: 644, duration: 0.685s, episode steps: 108, steps per second: 158, episode reward: -203.212, mean reward: -1.882 [-100.000,  6.708], mean action: 1.444 [0.000, 3.000],  loss: 18.868344, mae: 53.300040, mean_q: 57.240691, mean_eps: 0.695910
  67742/200000: episode: 645, duration: 0.682s, episode steps: 112, steps per second: 164, episode reward: -114.297, mean reward: -1.021 [-100.000,  8.615], mean action: 1.580 [0.000, 3.000],  loss: 15.079748, mae: 52.733095, mean_q: 57.599728, mean_eps: 0.695415
  67898/200000: episode: 646, duration: 0.996s, episode steps: 156, steps per second: 157, episode reward: -83.744, mean reward: -0.537 [-100.000,  8.971], mean action: 1.667 [0.000, 3.000],  loss: 20.194442, mae: 52.737991, mean_q: 56.824476, mean_eps: 0.694812
  68051/200000: episode: 647, duration: 0.984s, episode steps: 153, steps per second: 155, episode reward: -65.143, mean reward: -0.426 [-100.000,  5.259], mean action: 1.810 [0.000, 3.000],  loss: 19.595313, mae: 52.279162, mean_q: 57.418765, mean_eps: 0.694117
  68177/200000: episode: 648, duration: 0.776s, episode steps: 126, steps per second: 162, episode reward: -94.727, mean reward: -0.752 [-100.000,  9.481], mean action: 1.651 [0.000, 3.000],  loss: 18.372465, mae: 52.440913, mean_q: 57.855467, mean_eps: 0.693489
  68321/200000: episode: 649, duration: 1.024s, episode steps: 144, steps per second: 141, episode reward: -47.013, mean reward: -0.326 [-100.000, 11.515], mean action: 1.639 [0.000, 3.000],  loss: 18.076339, mae: 52.689575, mean_q: 57.690714, mean_eps: 0.692882
  68394/200000: episode: 650, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -58.941, mean reward: -0.807 [-100.000, 10.896], mean action: 1.603 [0.000, 3.000],  loss: 24.356277, mae: 52.119199, mean_q: 57.638924, mean_eps: 0.692394
  68546/200000: episode: 651, duration: 1.194s, episode steps: 152, steps per second: 127, episode reward: -27.290, mean reward: -0.180 [-100.000, 18.569], mean action: 1.743 [0.000, 3.000],  loss: 15.609091, mae: 53.056560, mean_q: 58.780618, mean_eps: 0.691887
  68631/200000: episode: 652, duration: 0.640s, episode steps:  85, steps per second: 133, episode reward: -117.416, mean reward: -1.381 [-100.000, 10.321], mean action: 1.729 [0.000, 3.000],  loss: 20.727241, mae: 51.691862, mean_q: 55.070392, mean_eps: 0.691354
  68748/200000: episode: 653, duration: 0.771s, episode steps: 117, steps per second: 152, episode reward: -22.798, mean reward: -0.195 [-100.000,  8.402], mean action: 1.650 [0.000, 3.000],  loss: 16.426892, mae: 52.134485, mean_q: 57.185123, mean_eps: 0.690899
  68880/200000: episode: 654, duration: 0.853s, episode steps: 132, steps per second: 155, episode reward: -88.100, mean reward: -0.667 [-100.000, 19.915], mean action: 1.583 [0.000, 3.000],  loss: 18.902370, mae: 52.355442, mean_q: 58.101544, mean_eps: 0.690339
  68977/200000: episode: 655, duration: 0.605s, episode steps:  97, steps per second: 160, episode reward: -94.406, mean reward: -0.973 [-100.000, 19.632], mean action: 1.526 [0.000, 3.000],  loss: 12.791227, mae: 51.802569, mean_q: 56.768631, mean_eps: 0.689824
  69133/200000: episode: 656, duration: 1.195s, episode steps: 156, steps per second: 131, episode reward: -238.245, mean reward: -1.527 [-100.000, 39.081], mean action: 1.506 [0.000, 3.000],  loss: 16.339690, mae: 51.899007, mean_q: 57.712981, mean_eps: 0.689255
  69236/200000: episode: 657, duration: 0.705s, episode steps: 103, steps per second: 146, episode reward: -69.473, mean reward: -0.674 [-100.000, 14.646], mean action: 1.592 [0.000, 3.000],  loss: 17.107424, mae: 52.342415, mean_q: 57.382572, mean_eps: 0.688672
  69359/200000: episode: 658, duration: 0.770s, episode steps: 123, steps per second: 160, episode reward: -90.776, mean reward: -0.738 [-100.000, 14.300], mean action: 1.699 [0.000, 3.000],  loss: 20.816481, mae: 52.766682, mean_q: 57.990849, mean_eps: 0.688164
  69507/200000: episode: 659, duration: 0.978s, episode steps: 148, steps per second: 151, episode reward: -164.606, mean reward: -1.112 [-100.000, 66.162], mean action: 1.534 [0.000, 3.000],  loss: 15.394529, mae: 52.695594, mean_q: 58.967631, mean_eps: 0.687554
  69661/200000: episode: 660, duration: 1.069s, episode steps: 154, steps per second: 144, episode reward: -25.107, mean reward: -0.163 [-100.000, 19.848], mean action: 1.682 [0.000, 3.000],  loss: 19.741399, mae: 53.418579, mean_q: 58.088516, mean_eps: 0.686874
  69764/200000: episode: 661, duration: 0.789s, episode steps: 103, steps per second: 131, episode reward: -20.949, mean reward: -0.203 [-100.000, 14.883], mean action: 1.583 [0.000, 3.000],  loss: 16.517539, mae: 53.657502, mean_q: 58.876092, mean_eps: 0.686296
  69849/200000: episode: 662, duration: 0.640s, episode steps:  85, steps per second: 133, episode reward: -63.707, mean reward: -0.749 [-100.000,  6.234], mean action: 1.812 [0.000, 3.000],  loss: 17.059436, mae: 54.029059, mean_q: 59.097658, mean_eps: 0.685873
  69968/200000: episode: 663, duration: 0.793s, episode steps: 119, steps per second: 150, episode reward: -118.675, mean reward: -0.997 [-100.000,  3.677], mean action: 1.529 [0.000, 3.000],  loss: 19.042575, mae: 53.819848, mean_q: 58.569791, mean_eps: 0.685414
  70119/200000: episode: 664, duration: 1.080s, episode steps: 151, steps per second: 140, episode reward: -21.896, mean reward: -0.145 [-100.000, 10.649], mean action: 1.682 [0.000, 3.000],  loss: 14.814127, mae: 54.111904, mean_q: 59.066465, mean_eps: 0.684807
  70213/200000: episode: 665, duration: 0.577s, episode steps:  94, steps per second: 163, episode reward: -115.922, mean reward: -1.233 [-100.000, 32.056], mean action: 1.734 [0.000, 3.000],  loss: 12.198626, mae: 54.934867, mean_q: 60.433383, mean_eps: 0.684255
  70332/200000: episode: 666, duration: 0.836s, episode steps: 119, steps per second: 142, episode reward: -69.567, mean reward: -0.585 [-100.000, 12.962], mean action: 1.571 [0.000, 3.000],  loss: 13.789595, mae: 54.360937, mean_q: 59.881703, mean_eps: 0.683776
  70407/200000: episode: 667, duration: 0.591s, episode steps:  75, steps per second: 127, episode reward: -78.187, mean reward: -1.042 [-100.000, 10.452], mean action: 1.533 [0.000, 3.000],  loss: 13.836318, mae: 55.365499, mean_q: 60.235005, mean_eps: 0.683339
  70524/200000: episode: 668, duration: 0.853s, episode steps: 117, steps per second: 137, episode reward: -195.310, mean reward: -1.669 [-100.000, 23.855], mean action: 1.590 [0.000, 3.000],  loss: 12.230385, mae: 55.406616, mean_q: 60.776201, mean_eps: 0.682908
  70672/200000: episode: 669, duration: 1.078s, episode steps: 148, steps per second: 137, episode reward: -330.659, mean reward: -2.234 [-100.000, 62.527], mean action: 1.392 [0.000, 3.000],  loss: 15.111797, mae: 55.906700, mean_q: 61.758689, mean_eps: 0.682311
  71089/200000: episode: 670, duration: 2.900s, episode steps: 417, steps per second: 144, episode reward: -83.564, mean reward: -0.200 [-100.000, 89.749], mean action: 1.542 [0.000, 3.000],  loss: 14.203270, mae: 55.391045, mean_q: 60.416709, mean_eps: 0.681040
  71231/200000: episode: 671, duration: 0.922s, episode steps: 142, steps per second: 154, episode reward: -98.960, mean reward: -0.697 [-100.000, 13.693], mean action: 1.676 [0.000, 3.000],  loss: 18.576039, mae: 55.935177, mean_q: 60.299919, mean_eps: 0.679782
  71373/200000: episode: 672, duration: 0.932s, episode steps: 142, steps per second: 152, episode reward: -84.606, mean reward: -0.596 [-100.000, 12.232], mean action: 1.648 [0.000, 3.000],  loss: 21.086394, mae: 56.297194, mean_q: 60.716668, mean_eps: 0.679143
  71478/200000: episode: 673, duration: 0.688s, episode steps: 105, steps per second: 153, episode reward: -174.562, mean reward: -1.662 [-100.000, 12.630], mean action: 1.305 [0.000, 3.000],  loss: 14.213931, mae: 55.620823, mean_q: 61.451661, mean_eps: 0.678588
  71619/200000: episode: 674, duration: 0.928s, episode steps: 141, steps per second: 152, episode reward: -102.197, mean reward: -0.725 [-100.000,  9.328], mean action: 1.681 [0.000, 3.000],  loss: 13.347747, mae: 55.555536, mean_q: 60.604806, mean_eps: 0.678034
  71719/200000: episode: 675, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -16.001, mean reward: -0.160 [-100.000, 17.626], mean action: 1.670 [0.000, 3.000],  loss: 15.566726, mae: 55.888550, mean_q: 60.752469, mean_eps: 0.677492
  71823/200000: episode: 676, duration: 0.637s, episode steps: 104, steps per second: 163, episode reward: -79.296, mean reward: -0.762 [-100.000,  9.033], mean action: 1.606 [0.000, 3.000],  loss: 14.833944, mae: 56.568106, mean_q: 61.136554, mean_eps: 0.677033
  71930/200000: episode: 677, duration: 0.691s, episode steps: 107, steps per second: 155, episode reward: 21.074, mean reward:  0.197 [-100.000, 79.634], mean action: 1.710 [0.000, 3.000],  loss: 15.193497, mae: 56.971048, mean_q: 61.492850, mean_eps: 0.676558
  72107/200000: episode: 678, duration: 1.155s, episode steps: 177, steps per second: 153, episode reward: -14.153, mean reward: -0.080 [-100.000, 12.986], mean action: 1.661 [0.000, 3.000],  loss: 14.054386, mae: 55.898586, mean_q: 61.637598, mean_eps: 0.675919
  72216/200000: episode: 679, duration: 0.671s, episode steps: 109, steps per second: 163, episode reward: -63.182, mean reward: -0.580 [-100.000,  6.757], mean action: 1.688 [0.000, 3.000],  loss: 25.788277, mae: 57.403585, mean_q: 61.494976, mean_eps: 0.675276
  72314/200000: episode: 680, duration: 0.609s, episode steps:  98, steps per second: 161, episode reward: -44.357, mean reward: -0.453 [-100.000, 11.656], mean action: 1.582 [0.000, 3.000],  loss: 16.917931, mae: 57.760968, mean_q: 63.411038, mean_eps: 0.674810
  72525/200000: episode: 681, duration: 1.331s, episode steps: 211, steps per second: 159, episode reward: -57.829, mean reward: -0.274 [-100.000, 12.111], mean action: 1.697 [0.000, 3.000],  loss: 16.949110, mae: 57.183146, mean_q: 61.474839, mean_eps: 0.674115
  72601/200000: episode: 682, duration: 0.486s, episode steps:  76, steps per second: 156, episode reward: -69.055, mean reward: -0.909 [-100.000, 10.451], mean action: 1.539 [0.000, 3.000],  loss: 18.876088, mae: 56.762722, mean_q: 60.210498, mean_eps: 0.673469
  72733/200000: episode: 683, duration: 0.966s, episode steps: 132, steps per second: 137, episode reward: -39.520, mean reward: -0.299 [-100.000, 17.285], mean action: 1.689 [0.000, 3.000],  loss: 14.716611, mae: 56.713396, mean_q: 59.844565, mean_eps: 0.673001
  72870/200000: episode: 684, duration: 0.950s, episode steps: 137, steps per second: 144, episode reward: -21.965, mean reward: -0.160 [-100.000, 10.322], mean action: 1.562 [0.000, 3.000],  loss: 16.786511, mae: 56.851712, mean_q: 62.037256, mean_eps: 0.672395
  72984/200000: episode: 685, duration: 0.752s, episode steps: 114, steps per second: 152, episode reward: -134.363, mean reward: -1.179 [-100.000,  5.703], mean action: 1.640 [0.000, 3.000],  loss: 13.111445, mae: 56.275084, mean_q: 60.672474, mean_eps: 0.671831
  73070/200000: episode: 686, duration: 0.580s, episode steps:  86, steps per second: 148, episode reward: -67.655, mean reward: -0.787 [-100.000,  6.948], mean action: 1.686 [0.000, 3.000],  loss: 9.788312, mae: 56.544000, mean_q: 59.784301, mean_eps: 0.671381
  73148/200000: episode: 687, duration: 0.504s, episode steps:  78, steps per second: 155, episode reward: -29.776, mean reward: -0.382 [-100.000,  9.781], mean action: 1.808 [0.000, 3.000],  loss: 13.950455, mae: 55.302110, mean_q: 57.960505, mean_eps: 0.671012
  73242/200000: episode: 688, duration: 0.593s, episode steps:  94, steps per second: 158, episode reward: -97.051, mean reward: -1.032 [-100.000,  9.462], mean action: 1.564 [0.000, 3.000],  loss: 17.323417, mae: 55.958663, mean_q: 60.170914, mean_eps: 0.670625
  73393/200000: episode: 689, duration: 0.994s, episode steps: 151, steps per second: 152, episode reward: -282.179, mean reward: -1.869 [-100.000,  4.541], mean action: 1.424 [0.000, 3.000],  loss: 18.937163, mae: 55.983530, mean_q: 59.478577, mean_eps: 0.670073
  73509/200000: episode: 690, duration: 0.755s, episode steps: 116, steps per second: 154, episode reward: -28.291, mean reward: -0.244 [-100.000, 19.976], mean action: 1.681 [0.000, 3.000],  loss: 17.492239, mae: 56.810342, mean_q: 62.676801, mean_eps: 0.669473
  73666/200000: episode: 691, duration: 0.987s, episode steps: 157, steps per second: 159, episode reward: -69.720, mean reward: -0.444 [-100.000, 11.648], mean action: 1.662 [0.000, 3.000],  loss: 20.697638, mae: 56.562919, mean_q: 61.003123, mean_eps: 0.668859
  73797/200000: episode: 692, duration: 0.798s, episode steps: 131, steps per second: 164, episode reward: -11.760, mean reward: -0.090 [-100.000, 19.865], mean action: 1.695 [0.000, 3.000],  loss: 15.998927, mae: 56.803800, mean_q: 61.070888, mean_eps: 0.668210
  73880/200000: episode: 693, duration: 0.504s, episode steps:  83, steps per second: 165, episode reward: -91.830, mean reward: -1.106 [-100.000,  6.967], mean action: 1.699 [0.000, 3.000],  loss: 20.627793, mae: 57.331056, mean_q: 61.179076, mean_eps: 0.667729
  74030/200000: episode: 694, duration: 0.897s, episode steps: 150, steps per second: 167, episode reward: -23.621, mean reward: -0.157 [-100.000,  9.909], mean action: 1.580 [0.000, 3.000],  loss: 16.018327, mae: 57.311634, mean_q: 62.212652, mean_eps: 0.667205
  74113/200000: episode: 695, duration: 0.543s, episode steps:  83, steps per second: 153, episode reward: -76.290, mean reward: -0.919 [-100.000, 17.814], mean action: 1.819 [0.000, 3.000],  loss: 17.626925, mae: 56.989789, mean_q: 60.866982, mean_eps: 0.666681
  74238/200000: episode: 696, duration: 0.764s, episode steps: 125, steps per second: 164, episode reward: -120.091, mean reward: -0.961 [-100.000, 19.979], mean action: 1.568 [0.000, 3.000],  loss: 16.384205, mae: 56.299803, mean_q: 60.837751, mean_eps: 0.666212
  74342/200000: episode: 697, duration: 0.652s, episode steps: 104, steps per second: 159, episode reward: -24.412, mean reward: -0.235 [-100.000, 15.320], mean action: 1.692 [0.000, 3.000],  loss: 13.871078, mae: 57.256521, mean_q: 62.377486, mean_eps: 0.665697
  74521/200000: episode: 698, duration: 1.138s, episode steps: 179, steps per second: 157, episode reward: -58.057, mean reward: -0.324 [-100.000, 11.538], mean action: 1.570 [0.000, 3.000],  loss: 17.967407, mae: 57.767869, mean_q: 63.380251, mean_eps: 0.665060
  74652/200000: episode: 699, duration: 0.838s, episode steps: 131, steps per second: 156, episode reward: -103.396, mean reward: -0.789 [-100.000,  8.348], mean action: 1.832 [0.000, 3.000],  loss: 15.531566, mae: 58.281510, mean_q: 63.673750, mean_eps: 0.664363
  74757/200000: episode: 700, duration: 0.696s, episode steps: 105, steps per second: 151, episode reward: -55.724, mean reward: -0.531 [-100.000, 34.728], mean action: 1.524 [0.000, 3.000],  loss: 15.253769, mae: 58.195461, mean_q: 64.018446, mean_eps: 0.663832
  74918/200000: episode: 701, duration: 1.047s, episode steps: 161, steps per second: 154, episode reward: -264.075, mean reward: -1.640 [-100.000,  5.238], mean action: 1.571 [0.000, 3.000],  loss: 15.139229, mae: 58.198848, mean_q: 63.277443, mean_eps: 0.663234
  75019/200000: episode: 702, duration: 0.678s, episode steps: 101, steps per second: 149, episode reward: -67.968, mean reward: -0.673 [-100.000, 12.280], mean action: 1.802 [0.000, 3.000],  loss: 12.858296, mae: 57.980443, mean_q: 63.285561, mean_eps: 0.662644
  75119/200000: episode: 703, duration: 0.713s, episode steps: 100, steps per second: 140, episode reward: -32.828, mean reward: -0.328 [-100.000, 12.175], mean action: 1.680 [0.000, 3.000],  loss: 14.592643, mae: 58.327502, mean_q: 64.970826, mean_eps: 0.662192
  75227/200000: episode: 704, duration: 0.685s, episode steps: 108, steps per second: 158, episode reward: -157.390, mean reward: -1.457 [-100.000,  4.255], mean action: 1.611 [0.000, 3.000],  loss: 28.725592, mae: 59.075882, mean_q: 62.546865, mean_eps: 0.661724
  75344/200000: episode: 705, duration: 0.751s, episode steps: 117, steps per second: 156, episode reward: -48.776, mean reward: -0.417 [-100.000, 20.636], mean action: 1.658 [0.000, 3.000],  loss: 19.406559, mae: 59.051756, mean_q: 64.681213, mean_eps: 0.661218
  75459/200000: episode: 706, duration: 0.794s, episode steps: 115, steps per second: 145, episode reward: -60.954, mean reward: -0.530 [-100.000, 14.008], mean action: 1.652 [0.000, 3.000],  loss: 16.154846, mae: 58.534606, mean_q: 63.822045, mean_eps: 0.660696
  75618/200000: episode: 707, duration: 1.002s, episode steps: 159, steps per second: 159, episode reward: -54.815, mean reward: -0.345 [-100.000,  8.548], mean action: 1.711 [0.000, 3.000],  loss: 13.599096, mae: 59.136662, mean_q: 65.959341, mean_eps: 0.660079
  75807/200000: episode: 708, duration: 1.231s, episode steps: 189, steps per second: 154, episode reward: -40.503, mean reward: -0.214 [-100.000, 17.946], mean action: 1.614 [0.000, 3.000],  loss: 18.754455, mae: 58.761056, mean_q: 65.734321, mean_eps: 0.659296
  75901/200000: episode: 709, duration: 0.598s, episode steps:  94, steps per second: 157, episode reward: -17.748, mean reward: -0.189 [-100.000, 13.291], mean action: 1.574 [0.000, 3.000],  loss: 14.842642, mae: 59.044758, mean_q: 66.459244, mean_eps: 0.658659
  76046/200000: episode: 710, duration: 0.890s, episode steps: 145, steps per second: 163, episode reward: -22.210, mean reward: -0.153 [-100.000, 24.433], mean action: 1.662 [0.000, 3.000],  loss: 13.681711, mae: 58.798798, mean_q: 66.943358, mean_eps: 0.658122
  76205/200000: episode: 711, duration: 0.983s, episode steps: 159, steps per second: 162, episode reward: -98.972, mean reward: -0.622 [-100.000, 18.016], mean action: 1.717 [0.000, 3.000],  loss: 20.564971, mae: 60.196423, mean_q: 68.389399, mean_eps: 0.657438
  76329/200000: episode: 712, duration: 0.758s, episode steps: 124, steps per second: 164, episode reward: -134.926, mean reward: -1.088 [-100.000, 14.548], mean action: 1.798 [0.000, 3.000],  loss: 19.477582, mae: 60.733249, mean_q: 70.030739, mean_eps: 0.656801
  76454/200000: episode: 713, duration: 0.806s, episode steps: 125, steps per second: 155, episode reward: -40.105, mean reward: -0.321 [-100.000, 15.713], mean action: 1.600 [0.000, 3.000],  loss: 16.336889, mae: 61.065359, mean_q: 70.992052, mean_eps: 0.656241
  76566/200000: episode: 714, duration: 0.690s, episode steps: 112, steps per second: 162, episode reward: -33.814, mean reward: -0.302 [-100.000, 10.879], mean action: 1.536 [0.000, 3.000],  loss: 14.660881, mae: 60.549123, mean_q: 69.025005, mean_eps: 0.655707
  76664/200000: episode: 715, duration: 0.606s, episode steps:  98, steps per second: 162, episode reward: -18.067, mean reward: -0.184 [-100.000, 73.504], mean action: 1.684 [0.000, 3.000],  loss: 23.220951, mae: 60.535817, mean_q: 68.470517, mean_eps: 0.655235
  76763/200000: episode: 716, duration: 0.599s, episode steps:  99, steps per second: 165, episode reward: -46.251, mean reward: -0.467 [-100.000, 16.696], mean action: 1.606 [0.000, 3.000],  loss: 15.700444, mae: 60.572984, mean_q: 69.321911, mean_eps: 0.654792
  76860/200000: episode: 717, duration: 0.641s, episode steps:  97, steps per second: 151, episode reward: -49.302, mean reward: -0.508 [-100.000, 11.311], mean action: 1.691 [0.000, 3.000],  loss: 16.423964, mae: 61.149724, mean_q: 69.176328, mean_eps: 0.654351
  77008/200000: episode: 718, duration: 0.949s, episode steps: 148, steps per second: 156, episode reward: -45.374, mean reward: -0.307 [-100.000,  7.663], mean action: 1.601 [0.000, 3.000],  loss: 15.736475, mae: 60.549712, mean_q: 70.886419, mean_eps: 0.653799
  77079/200000: episode: 719, duration: 0.474s, episode steps:  71, steps per second: 150, episode reward: -97.834, mean reward: -1.378 [-100.000, 12.096], mean action: 1.620 [0.000, 3.000],  loss: 16.264629, mae: 61.718671, mean_q: 70.861229, mean_eps: 0.653307
  77203/200000: episode: 720, duration: 0.809s, episode steps: 124, steps per second: 153, episode reward: -47.808, mean reward: -0.386 [-100.000,  7.601], mean action: 1.581 [0.000, 3.000],  loss: 17.692019, mae: 61.088868, mean_q: 70.482736, mean_eps: 0.652868
  77305/200000: episode: 721, duration: 0.690s, episode steps: 102, steps per second: 148, episode reward: -96.432, mean reward: -0.945 [-100.000, 16.772], mean action: 1.735 [0.000, 3.000],  loss: 14.230478, mae: 61.056924, mean_q: 71.864818, mean_eps: 0.652359
  77466/200000: episode: 722, duration: 1.123s, episode steps: 161, steps per second: 143, episode reward: -17.683, mean reward: -0.110 [-100.000, 20.007], mean action: 1.720 [0.000, 3.000],  loss: 15.817137, mae: 61.244700, mean_q: 71.310956, mean_eps: 0.651767
  77602/200000: episode: 723, duration: 0.877s, episode steps: 136, steps per second: 155, episode reward: -7.456, mean reward: -0.055 [-100.000, 14.851], mean action: 1.765 [0.000, 3.000],  loss: 23.113723, mae: 61.848470, mean_q: 71.803655, mean_eps: 0.651099
  77808/200000: episode: 724, duration: 1.327s, episode steps: 206, steps per second: 155, episode reward: -39.248, mean reward: -0.191 [-100.000,  9.452], mean action: 1.806 [0.000, 3.000],  loss: 19.728585, mae: 61.884165, mean_q: 72.542165, mean_eps: 0.650330
  77904/200000: episode: 725, duration: 0.599s, episode steps:  96, steps per second: 160, episode reward: -100.322, mean reward: -1.045 [-100.000,  6.968], mean action: 1.490 [0.000, 3.000],  loss: 21.106716, mae: 63.871098, mean_q: 75.987793, mean_eps: 0.649650
  78025/200000: episode: 726, duration: 0.738s, episode steps: 121, steps per second: 164, episode reward: -54.454, mean reward: -0.450 [-100.000,  8.022], mean action: 1.653 [0.000, 3.000],  loss: 20.151226, mae: 64.185179, mean_q: 77.754740, mean_eps: 0.649162
  78119/200000: episode: 727, duration: 0.591s, episode steps:  94, steps per second: 159, episode reward: -31.934, mean reward: -0.340 [-100.000, 21.724], mean action: 1.766 [0.000, 3.000],  loss: 16.729426, mae: 63.845577, mean_q: 75.996711, mean_eps: 0.648678
  78292/200000: episode: 728, duration: 1.099s, episode steps: 173, steps per second: 157, episode reward: -325.263, mean reward: -1.880 [-100.000,  4.029], mean action: 1.434 [0.000, 3.000],  loss: 18.081431, mae: 64.854350, mean_q: 77.270874, mean_eps: 0.648077
  78467/200000: episode: 729, duration: 1.158s, episode steps: 175, steps per second: 151, episode reward: -85.144, mean reward: -0.487 [-100.000,  7.405], mean action: 1.651 [0.000, 3.000],  loss: 18.908639, mae: 64.652532, mean_q: 76.133068, mean_eps: 0.647294
  78560/200000: episode: 730, duration: 0.606s, episode steps:  93, steps per second: 153, episode reward: -110.426, mean reward: -1.187 [-100.000,  5.395], mean action: 1.753 [0.000, 3.000],  loss: 20.491465, mae: 65.519218, mean_q: 77.906409, mean_eps: 0.646691
  78664/200000: episode: 731, duration: 0.629s, episode steps: 104, steps per second: 165, episode reward: -101.455, mean reward: -0.976 [-100.000, 15.530], mean action: 1.606 [0.000, 3.000],  loss: 16.454128, mae: 64.758770, mean_q: 76.505332, mean_eps: 0.646248
  78827/200000: episode: 732, duration: 1.074s, episode steps: 163, steps per second: 152, episode reward: -23.240, mean reward: -0.143 [-100.000, 13.080], mean action: 1.589 [0.000, 3.000],  loss: 21.422195, mae: 65.490383, mean_q: 77.437067, mean_eps: 0.645648
  79011/200000: episode: 733, duration: 1.126s, episode steps: 184, steps per second: 163, episode reward: -183.655, mean reward: -0.998 [-100.000, 41.976], mean action: 1.701 [0.000, 3.000],  loss: 16.378340, mae: 66.034927, mean_q: 80.010868, mean_eps: 0.644867
  79125/200000: episode: 734, duration: 0.728s, episode steps: 114, steps per second: 157, episode reward: -69.032, mean reward: -0.606 [-100.000,  7.014], mean action: 1.500 [0.000, 3.000],  loss: 17.725436, mae: 65.165637, mean_q: 78.386546, mean_eps: 0.644196
  79229/200000: episode: 735, duration: 0.616s, episode steps: 104, steps per second: 169, episode reward: -86.730, mean reward: -0.834 [-100.000,  9.481], mean action: 1.625 [0.000, 3.000],  loss: 13.537095, mae: 65.560561, mean_q: 77.914309, mean_eps: 0.643706
  79335/200000: episode: 736, duration: 0.675s, episode steps: 106, steps per second: 157, episode reward: -28.613, mean reward: -0.270 [-100.000, 16.177], mean action: 1.736 [0.000, 3.000],  loss: 19.166593, mae: 66.225056, mean_q: 78.264246, mean_eps: 0.643233
  79475/200000: episode: 737, duration: 0.958s, episode steps: 140, steps per second: 146, episode reward: -172.862, mean reward: -1.235 [-100.000, 50.890], mean action: 1.500 [0.000, 3.000],  loss: 19.096176, mae: 67.301180, mean_q: 81.573352, mean_eps: 0.642680
  79588/200000: episode: 738, duration: 0.822s, episode steps: 113, steps per second: 138, episode reward: -24.282, mean reward: -0.215 [-100.000, 12.109], mean action: 1.496 [0.000, 3.000],  loss: 26.566450, mae: 67.092510, mean_q: 80.150954, mean_eps: 0.642111
  79664/200000: episode: 739, duration: 0.518s, episode steps:  76, steps per second: 147, episode reward: -79.865, mean reward: -1.051 [-100.000,  3.888], mean action: 1.763 [0.000, 3.000],  loss: 20.101790, mae: 67.326255, mean_q: 80.507425, mean_eps: 0.641685
  79768/200000: episode: 740, duration: 0.724s, episode steps: 104, steps per second: 144, episode reward: -70.258, mean reward: -0.676 [-100.000, 11.889], mean action: 1.673 [0.000, 3.000],  loss: 18.500125, mae: 65.576822, mean_q: 77.229417, mean_eps: 0.641280
  79927/200000: episode: 741, duration: 1.181s, episode steps: 159, steps per second: 135, episode reward: -140.234, mean reward: -0.882 [-100.000,  9.190], mean action: 1.635 [0.000, 3.000],  loss: 18.135452, mae: 66.185578, mean_q: 79.325212, mean_eps: 0.640688
  80058/200000: episode: 742, duration: 0.909s, episode steps: 131, steps per second: 144, episode reward: -68.287, mean reward: -0.521 [-100.000,  9.424], mean action: 1.611 [0.000, 3.000],  loss: 22.400304, mae: 66.729394, mean_q: 78.237467, mean_eps: 0.640036
  80177/200000: episode: 743, duration: 0.831s, episode steps: 119, steps per second: 143, episode reward: -136.292, mean reward: -1.145 [-100.000,  3.104], mean action: 1.613 [0.000, 3.000],  loss: 21.739323, mae: 65.733633, mean_q: 76.509460, mean_eps: 0.639474
  80332/200000: episode: 744, duration: 0.998s, episode steps: 155, steps per second: 155, episode reward: -9.566, mean reward: -0.062 [-100.000, 24.796], mean action: 1.529 [0.000, 3.000],  loss: 22.868596, mae: 66.025186, mean_q: 78.282519, mean_eps: 0.638857
  80478/200000: episode: 745, duration: 0.994s, episode steps: 146, steps per second: 147, episode reward: -142.181, mean reward: -0.974 [-100.000, 10.795], mean action: 1.651 [0.000, 3.000],  loss: 23.000900, mae: 66.987469, mean_q: 80.263637, mean_eps: 0.638180
  80591/200000: episode: 746, duration: 0.711s, episode steps: 113, steps per second: 159, episode reward: -30.925, mean reward: -0.274 [-100.000, 12.090], mean action: 1.673 [0.000, 3.000],  loss: 21.385617, mae: 67.968991, mean_q: 81.669034, mean_eps: 0.637597
  80682/200000: episode: 747, duration: 0.680s, episode steps:  91, steps per second: 134, episode reward: -121.626, mean reward: -1.337 [-100.000,  7.177], mean action: 1.615 [0.000, 3.000],  loss: 24.497159, mae: 67.786161, mean_q: 81.010519, mean_eps: 0.637138
  80793/200000: episode: 748, duration: 0.883s, episode steps: 111, steps per second: 126, episode reward: -63.935, mean reward: -0.576 [-100.000,  5.814], mean action: 1.685 [0.000, 3.000],  loss: 15.413513, mae: 68.821956, mean_q: 81.918866, mean_eps: 0.636684
  80961/200000: episode: 749, duration: 1.433s, episode steps: 168, steps per second: 117, episode reward: -62.241, mean reward: -0.370 [-100.000, 31.741], mean action: 1.607 [0.000, 3.000],  loss: 22.205628, mae: 69.022544, mean_q: 82.445156, mean_eps: 0.636056
  81035/200000: episode: 750, duration: 0.593s, episode steps:  74, steps per second: 125, episode reward: -117.572, mean reward: -1.589 [-100.000,  5.908], mean action: 1.473 [0.000, 3.000],  loss: 23.626274, mae: 67.075785, mean_q: 80.012622, mean_eps: 0.635511
  81111/200000: episode: 751, duration: 0.595s, episode steps:  76, steps per second: 128, episode reward: -42.072, mean reward: -0.554 [-100.000,  7.095], mean action: 1.592 [0.000, 3.000],  loss: 18.213354, mae: 69.611495, mean_q: 83.163063, mean_eps: 0.635174
  81193/200000: episode: 752, duration: 0.641s, episode steps:  82, steps per second: 128, episode reward: -101.011, mean reward: -1.232 [-100.000, 11.553], mean action: 1.756 [0.000, 3.000],  loss: 14.565918, mae: 67.729282, mean_q: 81.447358, mean_eps: 0.634818
  81337/200000: episode: 753, duration: 1.310s, episode steps: 144, steps per second: 110, episode reward: -16.305, mean reward: -0.113 [-100.000, 15.725], mean action: 1.715 [0.000, 3.000],  loss: 14.597517, mae: 68.346781, mean_q: 82.078981, mean_eps: 0.634310
  81464/200000: episode: 754, duration: 1.310s, episode steps: 127, steps per second:  97, episode reward: -100.664, mean reward: -0.793 [-100.000,  6.346], mean action: 1.504 [0.000, 3.000],  loss: 14.856038, mae: 68.701889, mean_q: 82.915314, mean_eps: 0.633700
  81587/200000: episode: 755, duration: 1.481s, episode steps: 123, steps per second:  83, episode reward: -96.272, mean reward: -0.783 [-100.000, 11.979], mean action: 1.496 [0.000, 3.000],  loss: 18.399741, mae: 69.506147, mean_q: 83.322143, mean_eps: 0.633138
  81721/200000: episode: 756, duration: 1.255s, episode steps: 134, steps per second: 107, episode reward: -78.379, mean reward: -0.585 [-100.000,  7.562], mean action: 1.687 [0.000, 3.000],  loss: 27.474546, mae: 68.813637, mean_q: 81.948359, mean_eps: 0.632559
  81824/200000: episode: 757, duration: 0.907s, episode steps: 103, steps per second: 114, episode reward: -81.011, mean reward: -0.787 [-100.000, 11.410], mean action: 1.641 [0.000, 3.000],  loss: 28.567980, mae: 69.084158, mean_q: 82.719843, mean_eps: 0.632026
  81916/200000: episode: 758, duration: 0.693s, episode steps:  92, steps per second: 133, episode reward:  5.713, mean reward:  0.062 [-100.000, 29.825], mean action: 1.750 [0.000, 3.000],  loss: 24.492883, mae: 67.830509, mean_q: 81.482296, mean_eps: 0.631587
  82048/200000: episode: 759, duration: 1.235s, episode steps: 132, steps per second: 107, episode reward: -88.265, mean reward: -0.669 [-100.000,  7.406], mean action: 1.773 [0.000, 3.000],  loss: 19.914751, mae: 69.190851, mean_q: 83.579888, mean_eps: 0.631083
  82178/200000: episode: 760, duration: 1.386s, episode steps: 130, steps per second:  94, episode reward: -74.607, mean reward: -0.574 [-100.000, 15.330], mean action: 1.492 [0.000, 3.000],  loss: 20.322781, mae: 69.149528, mean_q: 84.131133, mean_eps: 0.630494
  82294/200000: episode: 761, duration: 0.923s, episode steps: 116, steps per second: 126, episode reward: -21.540, mean reward: -0.186 [-100.000, 13.286], mean action: 1.810 [0.000, 3.000],  loss: 23.756644, mae: 70.227330, mean_q: 84.711322, mean_eps: 0.629940
  82419/200000: episode: 762, duration: 0.908s, episode steps: 125, steps per second: 138, episode reward: -28.490, mean reward: -0.228 [-100.000, 11.051], mean action: 1.728 [0.000, 3.000],  loss: 20.762051, mae: 69.912159, mean_q: 84.730638, mean_eps: 0.629398
  82552/200000: episode: 763, duration: 0.819s, episode steps: 133, steps per second: 162, episode reward: -23.855, mean reward: -0.179 [-100.000, 19.879], mean action: 1.647 [0.000, 3.000],  loss: 26.637240, mae: 70.787129, mean_q: 85.944018, mean_eps: 0.628818
  82640/200000: episode: 764, duration: 0.646s, episode steps:  88, steps per second: 136, episode reward: -77.381, mean reward: -0.879 [-100.000, 10.152], mean action: 1.716 [0.000, 3.000],  loss: 20.019010, mae: 71.077766, mean_q: 87.348252, mean_eps: 0.628320
  82744/200000: episode: 765, duration: 0.712s, episode steps: 104, steps per second: 146, episode reward: -103.623, mean reward: -0.996 [-100.000,  5.398], mean action: 1.673 [0.000, 3.000],  loss: 14.343800, mae: 71.161762, mean_q: 87.381047, mean_eps: 0.627888
  82933/200000: episode: 766, duration: 1.250s, episode steps: 189, steps per second: 151, episode reward: -83.407, mean reward: -0.441 [-100.000, 63.806], mean action: 1.571 [0.000, 3.000],  loss: 18.672748, mae: 71.350778, mean_q: 86.072454, mean_eps: 0.627229
  83117/200000: episode: 767, duration: 1.381s, episode steps: 184, steps per second: 133, episode reward: -70.282, mean reward: -0.382 [-100.000,  4.094], mean action: 1.652 [0.000, 3.000],  loss: 21.697359, mae: 72.659794, mean_q: 87.859131, mean_eps: 0.626390
  83240/200000: episode: 768, duration: 0.870s, episode steps: 123, steps per second: 141, episode reward: -1.772, mean reward: -0.014 [-100.000, 12.452], mean action: 1.577 [0.000, 3.000],  loss: 28.693589, mae: 71.616957, mean_q: 85.523156, mean_eps: 0.625699
  83353/200000: episode: 769, duration: 0.839s, episode steps: 113, steps per second: 135, episode reward: -100.457, mean reward: -0.889 [-100.000,  9.011], mean action: 1.743 [0.000, 3.000],  loss: 23.055099, mae: 72.386639, mean_q: 88.631322, mean_eps: 0.625168
  83488/200000: episode: 770, duration: 1.012s, episode steps: 135, steps per second: 133, episode reward: -14.091, mean reward: -0.104 [-100.000, 16.930], mean action: 1.711 [0.000, 3.000],  loss: 16.469715, mae: 72.684425, mean_q: 88.601426, mean_eps: 0.624610
  83586/200000: episode: 771, duration: 0.727s, episode steps:  98, steps per second: 135, episode reward: -34.046, mean reward: -0.347 [-100.000, 15.042], mean action: 1.704 [0.000, 3.000],  loss: 21.437798, mae: 71.957902, mean_q: 87.588899, mean_eps: 0.624086
  83698/200000: episode: 772, duration: 0.788s, episode steps: 112, steps per second: 142, episode reward: -144.062, mean reward: -1.286 [-100.000, 10.280], mean action: 1.688 [0.000, 3.000],  loss: 18.182828, mae: 73.087731, mean_q: 88.731026, mean_eps: 0.623613
  83811/200000: episode: 773, duration: 0.776s, episode steps: 113, steps per second: 146, episode reward:  3.527, mean reward:  0.031 [-100.000, 19.230], mean action: 1.708 [0.000, 3.000],  loss: 16.813502, mae: 72.015732, mean_q: 87.336337, mean_eps: 0.623107
  83953/200000: episode: 774, duration: 0.898s, episode steps: 142, steps per second: 158, episode reward: -34.943, mean reward: -0.246 [-100.000, 10.553], mean action: 1.549 [0.000, 3.000],  loss: 18.137824, mae: 73.582130, mean_q: 88.746458, mean_eps: 0.622533
  84054/200000: episode: 775, duration: 0.666s, episode steps: 101, steps per second: 152, episode reward: -98.776, mean reward: -0.978 [-100.000, 12.394], mean action: 1.515 [0.000, 3.000],  loss: 18.516748, mae: 73.923846, mean_q: 89.857183, mean_eps: 0.621986
  84166/200000: episode: 776, duration: 0.680s, episode steps: 112, steps per second: 165, episode reward: -49.524, mean reward: -0.442 [-100.000,  9.036], mean action: 1.643 [0.000, 3.000],  loss: 24.543740, mae: 74.066024, mean_q: 90.393023, mean_eps: 0.621507
  84534/200000: episode: 777, duration: 2.553s, episode steps: 368, steps per second: 144, episode reward: -78.362, mean reward: -0.213 [-100.000, 56.475], mean action: 1.622 [0.000, 3.000],  loss: 21.213250, mae: 72.557809, mean_q: 87.304113, mean_eps: 0.620427
  84671/200000: episode: 778, duration: 0.931s, episode steps: 137, steps per second: 147, episode reward: -104.355, mean reward: -0.762 [-100.000,  5.784], mean action: 1.723 [0.000, 3.000],  loss: 17.333694, mae: 72.863416, mean_q: 88.372617, mean_eps: 0.619291
  84794/200000: episode: 779, duration: 0.809s, episode steps: 123, steps per second: 152, episode reward: 10.073, mean reward:  0.082 [-100.000, 15.248], mean action: 1.691 [0.000, 3.000],  loss: 15.052685, mae: 74.152360, mean_q: 88.727935, mean_eps: 0.618706
  84900/200000: episode: 780, duration: 0.677s, episode steps: 106, steps per second: 156, episode reward:  2.513, mean reward:  0.024 [-100.000, 14.350], mean action: 1.764 [0.000, 3.000],  loss: 14.697090, mae: 74.593209, mean_q: 89.921236, mean_eps: 0.618191
  85037/200000: episode: 781, duration: 0.961s, episode steps: 137, steps per second: 143, episode reward: -65.422, mean reward: -0.478 [-100.000, 72.093], mean action: 1.613 [0.000, 3.000],  loss: 16.559824, mae: 74.398307, mean_q: 90.511335, mean_eps: 0.617644
  85136/200000: episode: 782, duration: 0.627s, episode steps:  99, steps per second: 158, episode reward: -60.764, mean reward: -0.614 [-100.000,  6.912], mean action: 1.545 [0.000, 3.000],  loss: 17.679779, mae: 75.332270, mean_q: 90.532438, mean_eps: 0.617113
  85246/200000: episode: 783, duration: 0.741s, episode steps: 110, steps per second: 148, episode reward: -31.787, mean reward: -0.289 [-100.000, 36.798], mean action: 1.727 [0.000, 3.000],  loss: 17.855239, mae: 75.332756, mean_q: 89.192616, mean_eps: 0.616643
  85391/200000: episode: 784, duration: 0.876s, episode steps: 145, steps per second: 165, episode reward: -63.925, mean reward: -0.441 [-100.000, 11.969], mean action: 1.738 [0.000, 3.000],  loss: 21.090737, mae: 75.576364, mean_q: 90.659004, mean_eps: 0.616069
  85513/200000: episode: 785, duration: 0.755s, episode steps: 122, steps per second: 162, episode reward: -68.974, mean reward: -0.565 [-100.000, 10.325], mean action: 1.549 [0.000, 3.000],  loss: 19.445437, mae: 75.043941, mean_q: 90.213479, mean_eps: 0.615468
  85641/200000: episode: 786, duration: 0.906s, episode steps: 128, steps per second: 141, episode reward: -119.442, mean reward: -0.933 [-100.000, 11.431], mean action: 1.633 [0.000, 3.000],  loss: 27.446381, mae: 75.734560, mean_q: 90.060141, mean_eps: 0.614906
  85745/200000: episode: 787, duration: 0.757s, episode steps: 104, steps per second: 137, episode reward: 28.948, mean reward:  0.278 [-100.000, 15.660], mean action: 1.702 [0.000, 3.000],  loss: 28.961053, mae: 76.755985, mean_q: 91.699234, mean_eps: 0.614384
  85825/200000: episode: 788, duration: 0.532s, episode steps:  80, steps per second: 150, episode reward: -79.176, mean reward: -0.990 [-100.000, 14.787], mean action: 1.663 [0.000, 3.000],  loss: 21.411676, mae: 75.937020, mean_q: 92.979370, mean_eps: 0.613970
  85906/200000: episode: 789, duration: 0.617s, episode steps:  81, steps per second: 131, episode reward: -57.450, mean reward: -0.709 [-100.000, 17.948], mean action: 1.654 [0.000, 3.000],  loss: 13.216708, mae: 75.047330, mean_q: 89.895956, mean_eps: 0.613607
  86038/200000: episode: 790, duration: 0.872s, episode steps: 132, steps per second: 151, episode reward: -25.879, mean reward: -0.196 [-100.000, 31.908], mean action: 1.720 [0.000, 3.000],  loss: 21.192452, mae: 76.510682, mean_q: 91.464611, mean_eps: 0.613128
  86214/200000: episode: 791, duration: 1.256s, episode steps: 176, steps per second: 140, episode reward: -10.334, mean reward: -0.059 [-100.000, 11.304], mean action: 1.756 [0.000, 3.000],  loss: 18.663796, mae: 76.561688, mean_q: 91.625936, mean_eps: 0.612435
  86292/200000: episode: 792, duration: 0.523s, episode steps:  78, steps per second: 149, episode reward: -67.143, mean reward: -0.861 [-100.000, 10.588], mean action: 1.821 [0.000, 3.000],  loss: 21.172436, mae: 75.721728, mean_q: 91.492450, mean_eps: 0.611864
  86398/200000: episode: 793, duration: 0.743s, episode steps: 106, steps per second: 143, episode reward: -74.489, mean reward: -0.703 [-100.000,  9.835], mean action: 1.689 [0.000, 3.000],  loss: 28.936060, mae: 76.450553, mean_q: 90.683354, mean_eps: 0.611450
  86509/200000: episode: 794, duration: 0.771s, episode steps: 111, steps per second: 144, episode reward: -68.678, mean reward: -0.619 [-100.000,  9.965], mean action: 1.802 [0.000, 3.000],  loss: 20.055889, mae: 76.844946, mean_q: 91.573777, mean_eps: 0.610961
  86579/200000: episode: 795, duration: 0.514s, episode steps:  70, steps per second: 136, episode reward: -40.699, mean reward: -0.581 [-100.000, 12.944], mean action: 1.600 [0.000, 3.000],  loss: 15.449614, mae: 76.116796, mean_q: 90.081988, mean_eps: 0.610554
  86701/200000: episode: 796, duration: 0.832s, episode steps: 122, steps per second: 147, episode reward: -82.615, mean reward: -0.677 [-100.000,  8.995], mean action: 1.549 [0.000, 3.000],  loss: 18.350528, mae: 77.372492, mean_q: 93.740864, mean_eps: 0.610122
  86815/200000: episode: 797, duration: 1.251s, episode steps: 114, steps per second:  91, episode reward: -77.695, mean reward: -0.682 [-100.000, 18.850], mean action: 1.675 [0.000, 3.000],  loss: 24.739670, mae: 77.527449, mean_q: 93.560808, mean_eps: 0.609591
  86949/200000: episode: 798, duration: 0.893s, episode steps: 134, steps per second: 150, episode reward: -41.663, mean reward: -0.311 [-100.000, 10.090], mean action: 1.709 [0.000, 3.000],  loss: 19.369001, mae: 76.213457, mean_q: 91.750913, mean_eps: 0.609033
  87051/200000: episode: 799, duration: 0.603s, episode steps: 102, steps per second: 169, episode reward: -91.722, mean reward: -0.899 [-100.000,  6.942], mean action: 1.559 [0.000, 3.000],  loss: 19.348665, mae: 77.225703, mean_q: 94.712756, mean_eps: 0.608502
  87190/200000: episode: 800, duration: 0.989s, episode steps: 139, steps per second: 141, episode reward: -92.983, mean reward: -0.669 [-100.000, 15.865], mean action: 1.561 [0.000, 3.000],  loss: 17.991079, mae: 76.636165, mean_q: 92.602694, mean_eps: 0.607960
  87552/200000: episode: 801, duration: 2.763s, episode steps: 362, steps per second: 131, episode reward: -80.697, mean reward: -0.223 [-100.000, 49.556], mean action: 1.848 [0.000, 3.000],  loss: 21.358048, mae: 77.032695, mean_q: 93.536490, mean_eps: 0.606833
  87674/200000: episode: 802, duration: 0.776s, episode steps: 122, steps per second: 157, episode reward: -58.452, mean reward: -0.479 [-100.000,  8.555], mean action: 1.689 [0.000, 3.000],  loss: 14.704356, mae: 76.935665, mean_q: 94.622938, mean_eps: 0.605744
  87820/200000: episode: 803, duration: 1.102s, episode steps: 146, steps per second: 132, episode reward: -184.126, mean reward: -1.261 [-100.000, 74.885], mean action: 1.397 [0.000, 3.000],  loss: 16.288313, mae: 77.108276, mean_q: 94.878972, mean_eps: 0.605141
  87952/200000: episode: 804, duration: 0.872s, episode steps: 132, steps per second: 151, episode reward: -6.479, mean reward: -0.049 [-100.000, 12.270], mean action: 1.530 [0.000, 3.000],  loss: 19.177553, mae: 77.101878, mean_q: 93.767583, mean_eps: 0.604515
  88066/200000: episode: 805, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -5.291, mean reward: -0.046 [-100.000, 20.002], mean action: 1.781 [0.000, 3.000],  loss: 19.457578, mae: 76.049897, mean_q: 91.851658, mean_eps: 0.603962
  88161/200000: episode: 806, duration: 0.631s, episode steps:  95, steps per second: 150, episode reward: -99.100, mean reward: -1.043 [-100.000, 18.108], mean action: 1.621 [0.000, 3.000],  loss: 17.208280, mae: 76.615500, mean_q: 93.096033, mean_eps: 0.603491
  88237/200000: episode: 807, duration: 0.485s, episode steps:  76, steps per second: 157, episode reward: -76.416, mean reward: -1.005 [-100.000, 11.409], mean action: 1.632 [0.000, 3.000],  loss: 20.651967, mae: 76.871036, mean_q: 93.765860, mean_eps: 0.603107
  88353/200000: episode: 808, duration: 0.769s, episode steps: 116, steps per second: 151, episode reward: -17.475, mean reward: -0.151 [-100.000, 14.603], mean action: 1.586 [0.000, 3.000],  loss: 21.070580, mae: 77.146080, mean_q: 93.873464, mean_eps: 0.602675
  88610/200000: episode: 809, duration: 1.660s, episode steps: 257, steps per second: 155, episode reward: -154.343, mean reward: -0.601 [-100.000, 47.413], mean action: 1.681 [0.000, 3.000],  loss: 18.896539, mae: 76.512149, mean_q: 92.461173, mean_eps: 0.601835
  88768/200000: episode: 810, duration: 1.072s, episode steps: 158, steps per second: 147, episode reward: -60.476, mean reward: -0.383 [-100.000, 10.866], mean action: 1.785 [0.000, 3.000],  loss: 23.832377, mae: 75.731488, mean_q: 91.519762, mean_eps: 0.600902
  88867/200000: episode: 811, duration: 0.601s, episode steps:  99, steps per second: 165, episode reward: -77.250, mean reward: -0.780 [-100.000, 12.298], mean action: 1.727 [0.000, 3.000],  loss: 13.778276, mae: 75.480041, mean_q: 90.904961, mean_eps: 0.600324
  88977/200000: episode: 812, duration: 0.682s, episode steps: 110, steps per second: 161, episode reward: -46.804, mean reward: -0.425 [-100.000, 10.421], mean action: 1.809 [0.000, 3.000],  loss: 20.101193, mae: 76.336031, mean_q: 91.686558, mean_eps: 0.599853
  89083/200000: episode: 813, duration: 0.714s, episode steps: 106, steps per second: 148, episode reward: -130.630, mean reward: -1.232 [-100.000,  7.690], mean action: 1.585 [0.000, 3.000],  loss: 16.864233, mae: 75.898533, mean_q: 91.316098, mean_eps: 0.599367
  89194/200000: episode: 814, duration: 0.695s, episode steps: 111, steps per second: 160, episode reward: -146.087, mean reward: -1.316 [-100.000, 16.438], mean action: 1.874 [0.000, 3.000],  loss: 15.866008, mae: 75.835420, mean_q: 92.333747, mean_eps: 0.598879
  89320/200000: episode: 815, duration: 0.743s, episode steps: 126, steps per second: 170, episode reward: -90.073, mean reward: -0.715 [-100.000, 10.905], mean action: 1.690 [0.000, 3.000],  loss: 19.579316, mae: 76.275888, mean_q: 92.413859, mean_eps: 0.598346
  89455/200000: episode: 816, duration: 0.874s, episode steps: 135, steps per second: 154, episode reward: -28.468, mean reward: -0.211 [-100.000,  9.284], mean action: 1.667 [0.000, 3.000],  loss: 22.870909, mae: 76.960981, mean_q: 95.052009, mean_eps: 0.597758
  90455/200000: episode: 817, duration: 7.530s, episode steps: 1000, steps per second: 133, episode reward: -93.600, mean reward: -0.094 [-21.632, 24.793], mean action: 1.680 [0.000, 3.000],  loss: 19.889663, mae: 76.937497, mean_q: 94.739074, mean_eps: 0.595205
  90567/200000: episode: 818, duration: 0.742s, episode steps: 112, steps per second: 151, episode reward: 25.388, mean reward:  0.227 [-100.000, 17.311], mean action: 1.750 [0.000, 3.000],  loss: 17.028082, mae: 74.987554, mean_q: 92.289414, mean_eps: 0.592703
  90664/200000: episode: 819, duration: 0.656s, episode steps:  97, steps per second: 148, episode reward: -56.247, mean reward: -0.580 [-100.000,  9.604], mean action: 1.742 [0.000, 3.000],  loss: 22.814938, mae: 77.411987, mean_q: 97.143699, mean_eps: 0.592233
  90795/200000: episode: 820, duration: 0.833s, episode steps: 131, steps per second: 157, episode reward: -42.641, mean reward: -0.326 [-100.000,  7.142], mean action: 1.687 [0.000, 3.000],  loss: 26.266023, mae: 77.519001, mean_q: 96.968197, mean_eps: 0.591719
  90911/200000: episode: 821, duration: 0.793s, episode steps: 116, steps per second: 146, episode reward: -74.483, mean reward: -0.642 [-100.000, 10.108], mean action: 1.672 [0.000, 3.000],  loss: 22.859574, mae: 77.252879, mean_q: 96.017986, mean_eps: 0.591164
  91030/200000: episode: 822, duration: 0.834s, episode steps: 119, steps per second: 143, episode reward: -86.706, mean reward: -0.729 [-100.000,  5.755], mean action: 1.639 [0.000, 3.000],  loss: 21.918127, mae: 77.775151, mean_q: 96.608804, mean_eps: 0.590635
  91170/200000: episode: 823, duration: 0.862s, episode steps: 140, steps per second: 162, episode reward: -29.300, mean reward: -0.209 [-100.000, 10.202], mean action: 1.529 [0.000, 3.000],  loss: 22.270792, mae: 77.329151, mean_q: 97.564197, mean_eps: 0.590052
  91277/200000: episode: 824, duration: 0.740s, episode steps: 107, steps per second: 145, episode reward: -90.077, mean reward: -0.842 [-100.000,  8.870], mean action: 1.636 [0.000, 3.000],  loss: 22.116625, mae: 77.284508, mean_q: 96.718961, mean_eps: 0.589496
  91378/200000: episode: 825, duration: 0.647s, episode steps: 101, steps per second: 156, episode reward: -136.265, mean reward: -1.349 [-100.000, 10.765], mean action: 1.792 [0.000, 3.000],  loss: 23.980398, mae: 79.610406, mean_q: 98.691803, mean_eps: 0.589028
  91477/200000: episode: 826, duration: 0.641s, episode steps:  99, steps per second: 154, episode reward: -113.932, mean reward: -1.151 [-100.000,  8.700], mean action: 1.889 [0.000, 3.000],  loss: 17.158039, mae: 78.383490, mean_q: 99.195140, mean_eps: 0.588579
  91596/200000: episode: 827, duration: 0.795s, episode steps: 119, steps per second: 150, episode reward: -94.609, mean reward: -0.795 [-100.000,  6.839], mean action: 1.782 [0.000, 3.000],  loss: 19.287347, mae: 78.030110, mean_q: 98.294464, mean_eps: 0.588088
  91743/200000: episode: 828, duration: 0.982s, episode steps: 147, steps per second: 150, episode reward: -14.438, mean reward: -0.098 [-100.000, 15.257], mean action: 1.701 [0.000, 3.000],  loss: 19.938136, mae: 78.511511, mean_q: 97.685942, mean_eps: 0.587489
  91863/200000: episode: 829, duration: 0.778s, episode steps: 120, steps per second: 154, episode reward: -76.331, mean reward: -0.636 [-100.000,  8.550], mean action: 1.608 [0.000, 3.000],  loss: 16.985257, mae: 79.489771, mean_q: 99.405083, mean_eps: 0.586889
  92010/200000: episode: 830, duration: 1.067s, episode steps: 147, steps per second: 138, episode reward: -53.267, mean reward: -0.362 [-100.000, 21.298], mean action: 1.680 [0.000, 3.000],  loss: 19.357900, mae: 79.395074, mean_q: 99.374529, mean_eps: 0.586288
  92133/200000: episode: 831, duration: 0.838s, episode steps: 123, steps per second: 147, episode reward: -56.788, mean reward: -0.462 [-100.000,  6.352], mean action: 1.537 [0.000, 3.000],  loss: 18.677115, mae: 79.356368, mean_q: 99.485429, mean_eps: 0.585680
  92777/200000: episode: 832, duration: 4.600s, episode steps: 644, steps per second: 140, episode reward: -331.500, mean reward: -0.515 [-100.000, 38.615], mean action: 1.683 [0.000, 3.000],  loss: 20.409837, mae: 78.147015, mean_q: 97.791241, mean_eps: 0.583955
  92909/200000: episode: 833, duration: 0.829s, episode steps: 132, steps per second: 159, episode reward: -30.598, mean reward: -0.232 [-100.000, 12.468], mean action: 1.485 [0.000, 3.000],  loss: 23.498527, mae: 78.107217, mean_q: 98.138424, mean_eps: 0.582209
  92992/200000: episode: 834, duration: 0.489s, episode steps:  83, steps per second: 170, episode reward: -107.633, mean reward: -1.297 [-100.000,  7.531], mean action: 1.627 [0.000, 3.000],  loss: 23.922548, mae: 79.017701, mean_q: 97.995648, mean_eps: 0.581725
  93102/200000: episode: 835, duration: 0.680s, episode steps: 110, steps per second: 162, episode reward: -89.726, mean reward: -0.816 [-100.000, 10.348], mean action: 1.564 [0.000, 3.000],  loss: 27.505192, mae: 77.830973, mean_q: 95.371559, mean_eps: 0.581291
  93270/200000: episode: 836, duration: 1.111s, episode steps: 168, steps per second: 151, episode reward: -15.646, mean reward: -0.093 [-100.000,  7.759], mean action: 1.679 [0.000, 3.000],  loss: 18.524580, mae: 78.951400, mean_q: 98.340905, mean_eps: 0.580665
  93345/200000: episode: 837, duration: 0.499s, episode steps:  75, steps per second: 150, episode reward: -67.149, mean reward: -0.895 [-100.000, 11.833], mean action: 1.640 [0.000, 3.000],  loss: 27.198348, mae: 79.466555, mean_q: 99.276416, mean_eps: 0.580118
  93465/200000: episode: 838, duration: 0.847s, episode steps: 120, steps per second: 142, episode reward: -65.450, mean reward: -0.545 [-100.000,  9.707], mean action: 1.542 [0.000, 3.000],  loss: 19.526955, mae: 79.193257, mean_q: 98.701026, mean_eps: 0.579680
  93562/200000: episode: 839, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -112.712, mean reward: -1.162 [-100.000, 12.239], mean action: 1.691 [0.000, 3.000],  loss: 20.350833, mae: 78.767735, mean_q: 98.705841, mean_eps: 0.579191
  93737/200000: episode: 840, duration: 1.259s, episode steps: 175, steps per second: 139, episode reward: -72.374, mean reward: -0.414 [-100.000, 11.985], mean action: 1.703 [0.000, 3.000],  loss: 20.599672, mae: 78.475898, mean_q: 96.833679, mean_eps: 0.578580
  93864/200000: episode: 841, duration: 0.884s, episode steps: 127, steps per second: 144, episode reward: 23.332, mean reward:  0.184 [-100.000, 18.886], mean action: 1.630 [0.000, 3.000],  loss: 24.462712, mae: 77.698839, mean_q: 96.146666, mean_eps: 0.577900
  93968/200000: episode: 842, duration: 0.649s, episode steps: 104, steps per second: 160, episode reward: -13.604, mean reward: -0.131 [-100.000, 19.202], mean action: 1.663 [0.000, 3.000],  loss: 23.231665, mae: 77.205654, mean_q: 96.217145, mean_eps: 0.577380
  94138/200000: episode: 843, duration: 1.188s, episode steps: 170, steps per second: 143, episode reward: -63.648, mean reward: -0.374 [-100.000,  6.607], mean action: 1.706 [0.000, 3.000],  loss: 24.957010, mae: 78.915995, mean_q: 98.866969, mean_eps: 0.576764
  94237/200000: episode: 844, duration: 0.651s, episode steps:  99, steps per second: 152, episode reward: -171.273, mean reward: -1.730 [-100.000,  2.699], mean action: 1.848 [0.000, 3.000],  loss: 19.024815, mae: 78.210089, mean_q: 97.506930, mean_eps: 0.576159
  94371/200000: episode: 845, duration: 0.982s, episode steps: 134, steps per second: 136, episode reward: -43.878, mean reward: -0.327 [-100.000, 11.234], mean action: 1.701 [0.000, 3.000],  loss: 17.193180, mae: 77.429158, mean_q: 96.676008, mean_eps: 0.575634
  94478/200000: episode: 846, duration: 0.709s, episode steps: 107, steps per second: 151, episode reward: -79.700, mean reward: -0.745 [-100.000, 11.681], mean action: 1.645 [0.000, 3.000],  loss: 20.273874, mae: 77.757159, mean_q: 95.959148, mean_eps: 0.575092
  94623/200000: episode: 847, duration: 0.955s, episode steps: 145, steps per second: 152, episode reward: -80.073, mean reward: -0.552 [-100.000, 12.807], mean action: 1.621 [0.000, 3.000],  loss: 20.554485, mae: 78.131653, mean_q: 96.129295, mean_eps: 0.574525
  94720/200000: episode: 848, duration: 0.684s, episode steps:  97, steps per second: 142, episode reward: -118.684, mean reward: -1.224 [-100.000, 10.486], mean action: 1.773 [0.000, 3.000],  loss: 20.666365, mae: 78.739517, mean_q: 98.437186, mean_eps: 0.573981
  94843/200000: episode: 849, duration: 0.797s, episode steps: 123, steps per second: 154, episode reward: -97.341, mean reward: -0.791 [-100.000,  7.512], mean action: 1.504 [0.000, 3.000],  loss: 18.658646, mae: 78.772222, mean_q: 96.571408, mean_eps: 0.573486
  95042/200000: episode: 850, duration: 1.232s, episode steps: 199, steps per second: 162, episode reward: -186.977, mean reward: -0.940 [-100.000, 45.835], mean action: 1.558 [0.000, 3.000],  loss: 24.147560, mae: 79.424315, mean_q: 98.120174, mean_eps: 0.572761
  95145/200000: episode: 851, duration: 0.669s, episode steps: 103, steps per second: 154, episode reward: -28.626, mean reward: -0.278 [-100.000, 18.670], mean action: 1.602 [0.000, 3.000],  loss: 20.456136, mae: 80.528831, mean_q: 98.716640, mean_eps: 0.572082
  95258/200000: episode: 852, duration: 0.738s, episode steps: 113, steps per second: 153, episode reward: -55.587, mean reward: -0.492 [-100.000, 17.671], mean action: 1.805 [0.000, 3.000],  loss: 17.227835, mae: 79.149274, mean_q: 97.247990, mean_eps: 0.571596
  95357/200000: episode: 853, duration: 0.607s, episode steps:  99, steps per second: 163, episode reward: -83.214, mean reward: -0.841 [-100.000, 20.587], mean action: 1.525 [0.000, 3.000],  loss: 21.957947, mae: 80.229860, mean_q: 98.492238, mean_eps: 0.571118
  95472/200000: episode: 854, duration: 0.765s, episode steps: 115, steps per second: 150, episode reward: -45.773, mean reward: -0.398 [-100.000, 10.533], mean action: 1.730 [0.000, 3.000],  loss: 22.747310, mae: 80.235754, mean_q: 98.985766, mean_eps: 0.570637
  95576/200000: episode: 855, duration: 0.657s, episode steps: 104, steps per second: 158, episode reward: -34.633, mean reward: -0.333 [-100.000,  9.925], mean action: 1.750 [0.000, 3.000],  loss: 27.089106, mae: 80.621066, mean_q: 99.743595, mean_eps: 0.570144
  95725/200000: episode: 856, duration: 0.943s, episode steps: 149, steps per second: 158, episode reward: -21.153, mean reward: -0.142 [-100.000, 18.416], mean action: 1.591 [0.000, 3.000],  loss: 20.894469, mae: 81.197944, mean_q: 101.072159, mean_eps: 0.569575
  95841/200000: episode: 857, duration: 0.772s, episode steps: 116, steps per second: 150, episode reward: -36.737, mean reward: -0.317 [-100.000, 10.344], mean action: 1.784 [0.000, 3.000],  loss: 21.954435, mae: 80.046746, mean_q: 98.697065, mean_eps: 0.568979
  96025/200000: episode: 858, duration: 1.236s, episode steps: 184, steps per second: 149, episode reward: -97.603, mean reward: -0.530 [-100.000, 22.197], mean action: 1.668 [0.000, 3.000],  loss: 26.890539, mae: 80.571918, mean_q: 98.957554, mean_eps: 0.568304
  96428/200000: episode: 859, duration: 2.785s, episode steps: 403, steps per second: 145, episode reward: -49.691, mean reward: -0.123 [-100.000, 44.528], mean action: 1.754 [0.000, 3.000],  loss: 22.529376, mae: 79.445279, mean_q: 98.349938, mean_eps: 0.566983
  96518/200000: episode: 860, duration: 0.628s, episode steps:  90, steps per second: 143, episode reward: -102.134, mean reward: -1.135 [-100.000,  5.948], mean action: 1.778 [0.000, 3.000],  loss: 23.403386, mae: 77.445551, mean_q: 95.666996, mean_eps: 0.565874
  96622/200000: episode: 861, duration: 0.696s, episode steps: 104, steps per second: 149, episode reward: -161.041, mean reward: -1.548 [-100.000,  3.617], mean action: 1.942 [0.000, 3.000],  loss: 24.461667, mae: 79.121105, mean_q: 98.107977, mean_eps: 0.565437
  97622/200000: episode: 862, duration: 7.821s, episode steps: 1000, steps per second: 128, episode reward: -40.768, mean reward: -0.041 [-22.543, 19.242], mean action: 1.692 [0.000, 3.000],  loss: 22.937641, mae: 77.574317, mean_q: 96.675179, mean_eps: 0.562953
  97749/200000: episode: 863, duration: 0.831s, episode steps: 127, steps per second: 153, episode reward: -9.997, mean reward: -0.079 [-100.000, 10.957], mean action: 1.732 [0.000, 3.000],  loss: 22.144218, mae: 76.204867, mean_q: 94.267763, mean_eps: 0.560418
  97862/200000: episode: 864, duration: 0.725s, episode steps: 113, steps per second: 156, episode reward: -57.459, mean reward: -0.508 [-100.000, 20.389], mean action: 1.717 [0.000, 3.000],  loss: 24.215110, mae: 75.208990, mean_q: 93.181808, mean_eps: 0.559878
  97972/200000: episode: 865, duration: 0.660s, episode steps: 110, steps per second: 167, episode reward: -39.018, mean reward: -0.355 [-100.000,  9.064], mean action: 1.800 [0.000, 3.000],  loss: 30.386366, mae: 76.345267, mean_q: 95.068994, mean_eps: 0.559376
  98112/200000: episode: 866, duration: 0.855s, episode steps: 140, steps per second: 164, episode reward: -15.542, mean reward: -0.111 [-100.000, 18.720], mean action: 1.636 [0.000, 3.000],  loss: 23.731219, mae: 76.440673, mean_q: 95.420019, mean_eps: 0.558813
  98257/200000: episode: 867, duration: 0.935s, episode steps: 145, steps per second: 155, episode reward: -59.817, mean reward: -0.413 [-100.000, 10.009], mean action: 1.752 [0.000, 3.000],  loss: 24.402365, mae: 76.475678, mean_q: 95.129211, mean_eps: 0.558172
  98405/200000: episode: 868, duration: 0.858s, episode steps: 148, steps per second: 173, episode reward: -44.531, mean reward: -0.301 [-100.000, 15.954], mean action: 1.622 [0.000, 3.000],  loss: 21.373523, mae: 76.202013, mean_q: 94.779864, mean_eps: 0.557513
  98516/200000: episode: 869, duration: 0.670s, episode steps: 111, steps per second: 166, episode reward: -6.384, mean reward: -0.058 [-100.000, 18.718], mean action: 1.622 [0.000, 3.000],  loss: 26.662228, mae: 76.759848, mean_q: 96.265129, mean_eps: 0.556930
  98667/200000: episode: 870, duration: 0.919s, episode steps: 151, steps per second: 164, episode reward: -9.819, mean reward: -0.065 [-100.000, 18.731], mean action: 1.583 [0.000, 3.000],  loss: 25.622654, mae: 76.490663, mean_q: 96.028983, mean_eps: 0.556341
  98819/200000: episode: 871, duration: 1.124s, episode steps: 152, steps per second: 135, episode reward: -11.232, mean reward: -0.074 [-100.000, 16.836], mean action: 1.737 [0.000, 3.000],  loss: 19.831893, mae: 77.818555, mean_q: 97.730575, mean_eps: 0.555659
  98987/200000: episode: 872, duration: 1.196s, episode steps: 168, steps per second: 141, episode reward: -49.643, mean reward: -0.295 [-100.000, 12.898], mean action: 1.649 [0.000, 3.000],  loss: 25.723631, mae: 76.468528, mean_q: 95.590700, mean_eps: 0.554939
  99069/200000: episode: 873, duration: 0.549s, episode steps:  82, steps per second: 149, episode reward: -50.510, mean reward: -0.616 [-100.000, 12.105], mean action: 1.598 [0.000, 3.000],  loss: 22.881601, mae: 76.630927, mean_q: 96.269232, mean_eps: 0.554376
  99215/200000: episode: 874, duration: 0.958s, episode steps: 146, steps per second: 152, episode reward: -13.374, mean reward: -0.092 [-100.000, 11.581], mean action: 1.678 [0.000, 3.000],  loss: 24.174826, mae: 76.763382, mean_q: 96.679001, mean_eps: 0.553863
  99348/200000: episode: 875, duration: 0.839s, episode steps: 133, steps per second: 158, episode reward: -96.694, mean reward: -0.727 [-100.000,  8.348], mean action: 1.624 [0.000, 3.000],  loss: 27.860640, mae: 76.589854, mean_q: 96.229279, mean_eps: 0.553235
  99446/200000: episode: 876, duration: 0.623s, episode steps:  98, steps per second: 157, episode reward: 35.445, mean reward:  0.362 [-100.000, 17.258], mean action: 1.755 [0.000, 3.000],  loss: 25.395451, mae: 75.446088, mean_q: 95.548034, mean_eps: 0.552716
  99569/200000: episode: 877, duration: 0.761s, episode steps: 123, steps per second: 162, episode reward: -86.876, mean reward: -0.706 [-100.000, 15.668], mean action: 1.740 [0.000, 3.000],  loss: 27.527457, mae: 75.237031, mean_q: 94.825025, mean_eps: 0.552219
 100431/200000: episode: 878, duration: 5.911s, episode steps: 862, steps per second: 146, episode reward: -85.760, mean reward: -0.099 [-100.000, 18.797], mean action: 1.691 [0.000, 3.000],  loss: 26.509009, mae: 76.386194, mean_q: 96.658684, mean_eps: 0.550002
 100551/200000: episode: 879, duration: 0.736s, episode steps: 120, steps per second: 163, episode reward: -31.565, mean reward: -0.263 [-100.000, 12.608], mean action: 1.792 [0.000, 3.000],  loss: 29.012143, mae: 75.630479, mean_q: 95.392214, mean_eps: 0.547793
 100673/200000: episode: 880, duration: 0.725s, episode steps: 122, steps per second: 168, episode reward: 20.095, mean reward:  0.165 [-100.000, 15.828], mean action: 1.697 [0.000, 3.000],  loss: 25.908666, mae: 75.306665, mean_q: 94.715535, mean_eps: 0.547248
 100764/200000: episode: 881, duration: 0.526s, episode steps:  91, steps per second: 173, episode reward: -70.079, mean reward: -0.770 [-100.000, 14.047], mean action: 1.692 [0.000, 3.000],  loss: 15.169863, mae: 74.914064, mean_q: 93.456240, mean_eps: 0.546769
 100879/200000: episode: 882, duration: 0.711s, episode steps: 115, steps per second: 162, episode reward:  4.665, mean reward:  0.041 [-100.000, 17.210], mean action: 1.730 [0.000, 3.000],  loss: 21.010847, mae: 75.395709, mean_q: 94.674854, mean_eps: 0.546305
 101060/200000: episode: 883, duration: 1.144s, episode steps: 181, steps per second: 158, episode reward: 39.866, mean reward:  0.220 [-100.000, 17.173], mean action: 1.713 [0.000, 3.000],  loss: 31.828961, mae: 74.672284, mean_q: 94.421361, mean_eps: 0.545640
 101237/200000: episode: 884, duration: 1.125s, episode steps: 177, steps per second: 157, episode reward: -66.523, mean reward: -0.376 [-100.000, 12.897], mean action: 1.701 [0.000, 3.000],  loss: 35.091937, mae: 73.902340, mean_q: 94.071121, mean_eps: 0.544834
 101383/200000: episode: 885, duration: 0.838s, episode steps: 146, steps per second: 174, episode reward: -14.063, mean reward: -0.096 [-100.000, 13.261], mean action: 1.651 [0.000, 3.000],  loss: 31.967165, mae: 73.704145, mean_q: 93.630280, mean_eps: 0.544107
 101507/200000: episode: 886, duration: 0.723s, episode steps: 124, steps per second: 171, episode reward: -67.044, mean reward: -0.541 [-100.000, 14.859], mean action: 1.815 [0.000, 3.000],  loss: 27.888302, mae: 74.449695, mean_q: 94.604144, mean_eps: 0.543500
 101597/200000: episode: 887, duration: 0.553s, episode steps:  90, steps per second: 163, episode reward: -62.814, mean reward: -0.698 [-100.000, 15.700], mean action: 1.633 [0.000, 3.000],  loss: 22.999523, mae: 74.593818, mean_q: 94.425928, mean_eps: 0.543018
 101696/200000: episode: 888, duration: 0.569s, episode steps:  99, steps per second: 174, episode reward: -44.150, mean reward: -0.446 [-100.000, 24.725], mean action: 1.717 [0.000, 3.000],  loss: 28.494934, mae: 75.438194, mean_q: 95.205962, mean_eps: 0.542593
 102696/200000: episode: 889, duration: 7.263s, episode steps: 1000, steps per second: 138, episode reward: -3.799, mean reward: -0.004 [-13.107, 21.712], mean action: 1.718 [0.000, 3.000],  loss: 30.214567, mae: 74.593548, mean_q: 95.587429, mean_eps: 0.540120
 102844/200000: episode: 890, duration: 0.925s, episode steps: 148, steps per second: 160, episode reward: -23.556, mean reward: -0.159 [-100.000,  9.928], mean action: 1.628 [0.000, 3.000],  loss: 30.334030, mae: 73.534658, mean_q: 94.642508, mean_eps: 0.537537
 103028/200000: episode: 891, duration: 1.098s, episode steps: 184, steps per second: 168, episode reward: -225.499, mean reward: -1.226 [-100.000, 40.191], mean action: 1.734 [0.000, 3.000],  loss: 29.240573, mae: 74.091042, mean_q: 96.502804, mean_eps: 0.536790
 103171/200000: episode: 892, duration: 0.878s, episode steps: 143, steps per second: 163, episode reward: -65.524, mean reward: -0.458 [-100.000, 19.040], mean action: 1.629 [0.000, 3.000],  loss: 31.598748, mae: 74.014426, mean_q: 96.075590, mean_eps: 0.536054
 103280/200000: episode: 893, duration: 0.639s, episode steps: 109, steps per second: 171, episode reward: -62.080, mean reward: -0.570 [-100.000,  9.020], mean action: 1.624 [0.000, 3.000],  loss: 25.599270, mae: 74.649549, mean_q: 96.318108, mean_eps: 0.535488
 103413/200000: episode: 894, duration: 0.793s, episode steps: 133, steps per second: 168, episode reward:  3.672, mean reward:  0.028 [-100.000, 14.002], mean action: 1.684 [0.000, 3.000],  loss: 28.054369, mae: 73.737491, mean_q: 95.928679, mean_eps: 0.534943
 103520/200000: episode: 895, duration: 0.650s, episode steps: 107, steps per second: 165, episode reward: 23.013, mean reward:  0.215 [-100.000, 17.716], mean action: 1.589 [0.000, 3.000],  loss: 23.378729, mae: 75.227001, mean_q: 97.935189, mean_eps: 0.534403
 103666/200000: episode: 896, duration: 0.837s, episode steps: 146, steps per second: 174, episode reward: -32.828, mean reward: -0.225 [-100.000,  7.504], mean action: 1.685 [0.000, 3.000],  loss: 35.404892, mae: 75.188251, mean_q: 97.409817, mean_eps: 0.533834
 103741/200000: episode: 897, duration: 0.417s, episode steps:  75, steps per second: 180, episode reward: -65.183, mean reward: -0.869 [-100.000, 11.719], mean action: 1.653 [0.000, 3.000],  loss: 28.518699, mae: 76.620051, mean_q: 99.974807, mean_eps: 0.533336
 103882/200000: episode: 898, duration: 0.860s, episode steps: 141, steps per second: 164, episode reward: -102.053, mean reward: -0.724 [-100.000, 14.275], mean action: 1.794 [0.000, 3.000],  loss: 35.395504, mae: 75.104428, mean_q: 97.421291, mean_eps: 0.532851
 104053/200000: episode: 899, duration: 0.997s, episode steps: 171, steps per second: 172, episode reward: -87.499, mean reward: -0.512 [-100.000,  7.929], mean action: 1.637 [0.000, 3.000],  loss: 31.592131, mae: 74.461630, mean_q: 96.327034, mean_eps: 0.532149
 104191/200000: episode: 900, duration: 0.794s, episode steps: 138, steps per second: 174, episode reward: -87.500, mean reward: -0.634 [-100.000, 27.526], mean action: 1.710 [0.000, 3.000],  loss: 23.837179, mae: 75.127749, mean_q: 97.838492, mean_eps: 0.531453
 104324/200000: episode: 901, duration: 0.808s, episode steps: 133, steps per second: 165, episode reward:  3.356, mean reward:  0.025 [-100.000, 13.296], mean action: 1.714 [0.000, 3.000],  loss: 30.219283, mae: 74.513695, mean_q: 96.321734, mean_eps: 0.530843
 105305/200000: episode: 902, duration: 7.542s, episode steps: 981, steps per second: 130, episode reward: -240.769, mean reward: -0.245 [-100.000, 22.319], mean action: 1.735 [0.000, 3.000],  loss: 31.410946, mae: 76.090714, mean_q: 98.881119, mean_eps: 0.528337
 105419/200000: episode: 903, duration: 0.786s, episode steps: 114, steps per second: 145, episode reward: -41.321, mean reward: -0.362 [-100.000, 20.135], mean action: 1.904 [0.000, 3.000],  loss: 24.663079, mae: 75.894434, mean_q: 98.898896, mean_eps: 0.525873
 105529/200000: episode: 904, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: -18.299, mean reward: -0.166 [-100.000, 15.587], mean action: 1.664 [0.000, 3.000],  loss: 25.166926, mae: 76.910047, mean_q: 99.868550, mean_eps: 0.525369
 105693/200000: episode: 905, duration: 1.045s, episode steps: 164, steps per second: 157, episode reward: -141.728, mean reward: -0.864 [-100.000, 13.160], mean action: 1.805 [0.000, 3.000],  loss: 31.976721, mae: 76.500465, mean_q: 100.020125, mean_eps: 0.524753
 106032/200000: episode: 906, duration: 2.171s, episode steps: 339, steps per second: 156, episode reward: -196.665, mean reward: -0.580 [-100.000,  4.595], mean action: 1.737 [0.000, 3.000],  loss: 30.656263, mae: 77.759129, mean_q: 102.418024, mean_eps: 0.523621
 106184/200000: episode: 907, duration: 0.988s, episode steps: 152, steps per second: 154, episode reward:  8.915, mean reward:  0.059 [-100.000, 17.864], mean action: 1.691 [0.000, 3.000],  loss: 25.694310, mae: 77.407906, mean_q: 101.718275, mean_eps: 0.522516
 106333/200000: episode: 908, duration: 0.976s, episode steps: 149, steps per second: 153, episode reward: -88.880, mean reward: -0.597 [-100.000, 32.824], mean action: 1.698 [0.000, 3.000],  loss: 29.959275, mae: 77.785505, mean_q: 102.266086, mean_eps: 0.521839
 106471/200000: episode: 909, duration: 0.860s, episode steps: 138, steps per second: 160, episode reward: -88.173, mean reward: -0.639 [-100.000, 15.413], mean action: 1.667 [0.000, 3.000],  loss: 34.326677, mae: 78.627834, mean_q: 102.880662, mean_eps: 0.521193
 106552/200000: episode: 910, duration: 0.483s, episode steps:  81, steps per second: 168, episode reward: -46.069, mean reward: -0.569 [-100.000, 10.309], mean action: 1.642 [0.000, 3.000],  loss: 36.122022, mae: 77.677926, mean_q: 101.107068, mean_eps: 0.520701
 106693/200000: episode: 911, duration: 0.798s, episode steps: 141, steps per second: 177, episode reward: -162.981, mean reward: -1.156 [-100.000, 14.204], mean action: 1.589 [0.000, 3.000],  loss: 29.915999, mae: 78.460358, mean_q: 103.104194, mean_eps: 0.520201
 106814/200000: episode: 912, duration: 0.729s, episode steps: 121, steps per second: 166, episode reward: 11.704, mean reward:  0.097 [-100.000, 11.525], mean action: 1.669 [0.000, 3.000],  loss: 30.940099, mae: 77.627932, mean_q: 101.313376, mean_eps: 0.519612
 106969/200000: episode: 913, duration: 0.888s, episode steps: 155, steps per second: 175, episode reward: -54.169, mean reward: -0.349 [-100.000, 11.312], mean action: 1.677 [0.000, 3.000],  loss: 28.673235, mae: 77.293874, mean_q: 101.157099, mean_eps: 0.518991
 107141/200000: episode: 914, duration: 1.043s, episode steps: 172, steps per second: 165, episode reward: -142.422, mean reward: -0.828 [-100.000, 13.163], mean action: 1.791 [0.000, 3.000],  loss: 33.212392, mae: 77.251679, mean_q: 100.992453, mean_eps: 0.518255
 108141/200000: episode: 915, duration: 6.576s, episode steps: 1000, steps per second: 152, episode reward: -159.044, mean reward: -0.159 [-22.289, 22.368], mean action: 1.776 [0.000, 3.000],  loss: 31.202383, mae: 77.829149, mean_q: 101.655936, mean_eps: 0.515618
 108344/200000: episode: 916, duration: 1.156s, episode steps: 203, steps per second: 176, episode reward: -275.332, mean reward: -1.356 [-100.000, 54.372], mean action: 1.576 [0.000, 3.000],  loss: 27.746360, mae: 77.638836, mean_q: 102.111312, mean_eps: 0.512911
 108514/200000: episode: 917, duration: 1.031s, episode steps: 170, steps per second: 165, episode reward: -138.734, mean reward: -0.816 [-100.000, 16.710], mean action: 1.553 [0.000, 3.000],  loss: 27.064814, mae: 78.463573, mean_q: 102.710692, mean_eps: 0.512072
 108668/200000: episode: 918, duration: 0.922s, episode steps: 154, steps per second: 167, episode reward: -98.595, mean reward: -0.640 [-100.000, 14.181], mean action: 1.688 [0.000, 3.000],  loss: 30.613960, mae: 77.243286, mean_q: 101.138539, mean_eps: 0.511343
 109233/200000: episode: 919, duration: 4.168s, episode steps: 565, steps per second: 136, episode reward: -269.483, mean reward: -0.477 [-100.000, 16.003], mean action: 1.809 [0.000, 3.000],  loss: 31.981405, mae: 76.942007, mean_q: 100.394692, mean_eps: 0.509725
 109383/200000: episode: 920, duration: 1.195s, episode steps: 150, steps per second: 125, episode reward: -86.847, mean reward: -0.579 [-100.000,  9.106], mean action: 1.760 [0.000, 3.000],  loss: 30.166335, mae: 75.869630, mean_q: 98.876194, mean_eps: 0.508116
 109493/200000: episode: 921, duration: 0.752s, episode steps: 110, steps per second: 146, episode reward: -56.819, mean reward: -0.517 [-100.000, 16.030], mean action: 1.709 [0.000, 3.000],  loss: 24.341760, mae: 74.446428, mean_q: 97.342159, mean_eps: 0.507531
 109690/200000: episode: 922, duration: 1.245s, episode steps: 197, steps per second: 158, episode reward: -161.174, mean reward: -0.818 [-100.000,  4.624], mean action: 1.619 [0.000, 3.000],  loss: 29.279159, mae: 74.990921, mean_q: 96.848810, mean_eps: 0.506841
 109823/200000: episode: 923, duration: 0.825s, episode steps: 133, steps per second: 161, episode reward: -175.801, mean reward: -1.322 [-100.000,  7.549], mean action: 1.684 [0.000, 3.000],  loss: 37.713548, mae: 76.095642, mean_q: 99.157580, mean_eps: 0.506098
 110250/200000: episode: 924, duration: 2.817s, episode steps: 427, steps per second: 152, episode reward: -263.799, mean reward: -0.618 [-100.000, 16.621], mean action: 1.642 [0.000, 3.000],  loss: 29.419424, mae: 75.837453, mean_q: 98.087362, mean_eps: 0.504838
 110505/200000: episode: 925, duration: 1.618s, episode steps: 255, steps per second: 158, episode reward: -280.583, mean reward: -1.100 [-100.000, 21.770], mean action: 1.788 [0.000, 3.000],  loss: 27.208281, mae: 76.697938, mean_q: 99.485007, mean_eps: 0.503304
 110743/200000: episode: 926, duration: 1.516s, episode steps: 238, steps per second: 157, episode reward: -160.036, mean reward: -0.672 [-100.000, 13.841], mean action: 1.676 [0.000, 3.000],  loss: 31.051992, mae: 75.255365, mean_q: 97.303985, mean_eps: 0.502194
 110890/200000: episode: 927, duration: 0.891s, episode steps: 147, steps per second: 165, episode reward: -168.724, mean reward: -1.148 [-100.000, 10.586], mean action: 1.585 [0.000, 3.000],  loss: 27.240573, mae: 74.751148, mean_q: 96.768803, mean_eps: 0.501328
 111098/200000: episode: 928, duration: 1.303s, episode steps: 208, steps per second: 160, episode reward: -72.273, mean reward: -0.347 [-100.000, 16.522], mean action: 1.788 [0.000, 3.000],  loss: 28.015069, mae: 75.278964, mean_q: 96.389800, mean_eps: 0.500529
 111220/200000: episode: 929, duration: 0.794s, episode steps: 122, steps per second: 154, episode reward:  6.162, mean reward:  0.051 [-100.000, 15.556], mean action: 1.730 [0.000, 3.000],  loss: 33.678116, mae: 73.950777, mean_q: 94.481076, mean_eps: 0.499787
 111354/200000: episode: 930, duration: 0.803s, episode steps: 134, steps per second: 167, episode reward: -33.232, mean reward: -0.248 [-100.000, 10.186], mean action: 1.657 [0.000, 3.000],  loss: 30.033465, mae: 75.130886, mean_q: 96.816572, mean_eps: 0.499211
 111494/200000: episode: 931, duration: 0.892s, episode steps: 140, steps per second: 157, episode reward: -69.854, mean reward: -0.499 [-100.000,  8.574], mean action: 1.621 [0.000, 3.000],  loss: 35.350747, mae: 74.904314, mean_q: 95.214213, mean_eps: 0.498594
 111613/200000: episode: 932, duration: 0.742s, episode steps: 119, steps per second: 160, episode reward: -147.334, mean reward: -1.238 [-100.000, 38.219], mean action: 1.538 [0.000, 3.000],  loss: 25.838764, mae: 75.100721, mean_q: 97.826418, mean_eps: 0.498011
 111759/200000: episode: 933, duration: 0.922s, episode steps: 146, steps per second: 158, episode reward: -219.273, mean reward: -1.502 [-100.000,  6.783], mean action: 1.760 [0.000, 3.000],  loss: 27.672649, mae: 74.366692, mean_q: 96.135061, mean_eps: 0.497415
 111874/200000: episode: 934, duration: 0.695s, episode steps: 115, steps per second: 165, episode reward: -27.292, mean reward: -0.237 [-100.000, 23.656], mean action: 1.765 [0.000, 3.000],  loss: 38.928706, mae: 73.813302, mean_q: 94.813441, mean_eps: 0.496828
 111957/200000: episode: 935, duration: 0.489s, episode steps:  83, steps per second: 170, episode reward: -32.955, mean reward: -0.397 [-100.000,  8.393], mean action: 1.699 [0.000, 3.000],  loss: 29.866389, mae: 74.668104, mean_q: 96.528436, mean_eps: 0.496383
 112133/200000: episode: 936, duration: 1.082s, episode steps: 176, steps per second: 163, episode reward: -80.790, mean reward: -0.459 [-100.000,  5.477], mean action: 1.761 [0.000, 3.000],  loss: 28.260437, mae: 73.792505, mean_q: 95.619657, mean_eps: 0.495800
 112481/200000: episode: 937, duration: 2.222s, episode steps: 348, steps per second: 157, episode reward: -134.671, mean reward: -0.387 [-100.000, 11.644], mean action: 1.914 [0.000, 3.000],  loss: 33.288452, mae: 73.373456, mean_q: 94.455749, mean_eps: 0.494621
 112628/200000: episode: 938, duration: 0.887s, episode steps: 147, steps per second: 166, episode reward:  2.329, mean reward:  0.016 [-100.000, 20.416], mean action: 1.796 [0.000, 3.000],  loss: 28.615844, mae: 73.888105, mean_q: 95.456523, mean_eps: 0.493507
 113352/200000: episode: 939, duration: 4.804s, episode steps: 724, steps per second: 151, episode reward: -287.199, mean reward: -0.397 [-100.000, 23.537], mean action: 1.742 [0.000, 3.000],  loss: 29.272037, mae: 73.401666, mean_q: 94.269033, mean_eps: 0.491547
 113605/200000: episode: 940, duration: 1.642s, episode steps: 253, steps per second: 154, episode reward: -257.711, mean reward: -1.019 [-100.000, 18.454], mean action: 1.767 [0.000, 3.000],  loss: 30.993034, mae: 72.255195, mean_q: 92.431460, mean_eps: 0.489349
 113773/200000: episode: 941, duration: 1.029s, episode steps: 168, steps per second: 163, episode reward: -184.086, mean reward: -1.096 [-100.000, 11.886], mean action: 1.631 [0.000, 3.000],  loss: 34.738372, mae: 71.716734, mean_q: 91.330135, mean_eps: 0.488402
 113899/200000: episode: 942, duration: 0.771s, episode steps: 126, steps per second: 163, episode reward: -131.234, mean reward: -1.042 [-100.000,  7.043], mean action: 1.722 [0.000, 3.000],  loss: 28.511370, mae: 71.300268, mean_q: 91.513566, mean_eps: 0.487740
 113999/200000: episode: 943, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -32.540, mean reward: -0.325 [-100.000,  7.070], mean action: 1.690 [0.000, 3.000],  loss: 26.483181, mae: 71.597691, mean_q: 90.220109, mean_eps: 0.487232
 114131/200000: episode: 944, duration: 0.814s, episode steps: 132, steps per second: 162, episode reward:  3.082, mean reward:  0.023 [-100.000, 17.975], mean action: 1.758 [0.000, 3.000],  loss: 31.164420, mae: 71.758065, mean_q: 91.385569, mean_eps: 0.486710
 114248/200000: episode: 945, duration: 0.696s, episode steps: 117, steps per second: 168, episode reward: -21.961, mean reward: -0.188 [-100.000, 18.998], mean action: 1.462 [0.000, 3.000],  loss: 31.397762, mae: 70.367149, mean_q: 88.457672, mean_eps: 0.486149
 114366/200000: episode: 946, duration: 0.665s, episode steps: 118, steps per second: 178, episode reward: -9.937, mean reward: -0.084 [-100.000, 13.081], mean action: 1.551 [0.000, 3.000],  loss: 31.540513, mae: 71.825787, mean_q: 90.447194, mean_eps: 0.485621
 114525/200000: episode: 947, duration: 0.969s, episode steps: 159, steps per second: 164, episode reward: -12.431, mean reward: -0.078 [-100.000, 11.339], mean action: 1.509 [0.000, 3.000],  loss: 30.633093, mae: 71.709602, mean_q: 91.396049, mean_eps: 0.484997
 114661/200000: episode: 948, duration: 0.841s, episode steps: 136, steps per second: 162, episode reward: -13.238, mean reward: -0.097 [-100.000, 21.558], mean action: 1.640 [0.000, 3.000],  loss: 23.188613, mae: 71.542205, mean_q: 91.044513, mean_eps: 0.484334
 114765/200000: episode: 949, duration: 0.671s, episode steps: 104, steps per second: 155, episode reward: -56.126, mean reward: -0.540 [-100.000, 10.170], mean action: 1.846 [0.000, 3.000],  loss: 28.388061, mae: 70.117267, mean_q: 89.393076, mean_eps: 0.483794
 114898/200000: episode: 950, duration: 0.826s, episode steps: 133, steps per second: 161, episode reward: -30.175, mean reward: -0.227 [-100.000, 18.147], mean action: 1.729 [0.000, 3.000],  loss: 27.952530, mae: 70.589932, mean_q: 89.038883, mean_eps: 0.483260
 114996/200000: episode: 951, duration: 0.601s, episode steps:  98, steps per second: 163, episode reward: -77.609, mean reward: -0.792 [-100.000, 13.644], mean action: 1.745 [0.000, 3.000],  loss: 24.241658, mae: 70.673449, mean_q: 89.857428, mean_eps: 0.482741
 115996/200000: episode: 952, duration: 7.660s, episode steps: 1000, steps per second: 131, episode reward: 33.306, mean reward:  0.033 [-24.308, 24.474], mean action: 1.660 [0.000, 3.000],  loss: 24.465340, mae: 69.297409, mean_q: 87.600224, mean_eps: 0.480270
 116124/200000: episode: 953, duration: 0.822s, episode steps: 128, steps per second: 156, episode reward: -8.347, mean reward: -0.065 [-100.000, 16.417], mean action: 1.664 [0.000, 3.000],  loss: 25.565499, mae: 67.721211, mean_q: 84.343162, mean_eps: 0.477732
 116268/200000: episode: 954, duration: 0.872s, episode steps: 144, steps per second: 165, episode reward: -16.259, mean reward: -0.113 [-100.000, 18.968], mean action: 1.521 [0.000, 3.000],  loss: 26.787872, mae: 67.943769, mean_q: 84.784233, mean_eps: 0.477120
 117268/200000: episode: 955, duration: 7.177s, episode steps: 1000, steps per second: 139, episode reward: 19.065, mean reward:  0.019 [-24.669, 26.340], mean action: 1.695 [0.000, 3.000],  loss: 28.232257, mae: 67.680081, mean_q: 84.776120, mean_eps: 0.474546
 117383/200000: episode: 956, duration: 0.670s, episode steps: 115, steps per second: 172, episode reward: 12.240, mean reward:  0.106 [-100.000, 16.918], mean action: 1.635 [0.000, 3.000],  loss: 37.643445, mae: 66.763699, mean_q: 84.552189, mean_eps: 0.472037
 118356/200000: episode: 957, duration: 7.084s, episode steps: 973, steps per second: 137, episode reward: -227.894, mean reward: -0.234 [-100.000, 22.030], mean action: 1.676 [0.000, 3.000],  loss: 26.268089, mae: 66.369917, mean_q: 84.316878, mean_eps: 0.469590
 118460/200000: episode: 958, duration: 0.616s, episode steps: 104, steps per second: 169, episode reward: -66.753, mean reward: -0.642 [-100.000, 11.356], mean action: 1.442 [0.000, 3.000],  loss: 28.757803, mae: 64.434997, mean_q: 80.847042, mean_eps: 0.467166
 119460/200000: episode: 959, duration: 7.961s, episode steps: 1000, steps per second: 126, episode reward: 31.860, mean reward:  0.032 [-23.700, 25.192], mean action: 1.717 [0.000, 3.000],  loss: 25.451072, mae: 64.194320, mean_q: 81.221268, mean_eps: 0.464682
 120460/200000: episode: 960, duration: 7.511s, episode steps: 1000, steps per second: 133, episode reward: 53.794, mean reward:  0.054 [-21.085, 20.618], mean action: 1.772 [0.000, 3.000],  loss: 23.095618, mae: 63.248895, mean_q: 79.773217, mean_eps: 0.460182
 120552/200000: episode: 961, duration: 0.610s, episode steps:  92, steps per second: 151, episode reward: -49.555, mean reward: -0.539 [-100.000,  9.802], mean action: 1.641 [0.000, 3.000],  loss: 21.908580, mae: 61.479154, mean_q: 78.135679, mean_eps: 0.457725
 120699/200000: episode: 962, duration: 0.904s, episode steps: 147, steps per second: 163, episode reward: -38.362, mean reward: -0.261 [-100.000,  7.549], mean action: 1.823 [0.000, 3.000],  loss: 21.588441, mae: 61.999108, mean_q: 78.239978, mean_eps: 0.457187
 120811/200000: episode: 963, duration: 0.692s, episode steps: 112, steps per second: 162, episode reward: -4.848, mean reward: -0.043 [-100.000, 20.109], mean action: 1.429 [0.000, 3.000],  loss: 17.223693, mae: 61.260017, mean_q: 77.238355, mean_eps: 0.456605
 120945/200000: episode: 964, duration: 0.788s, episode steps: 134, steps per second: 170, episode reward: -17.685, mean reward: -0.132 [-100.000, 15.007], mean action: 1.746 [0.000, 3.000],  loss: 21.882819, mae: 61.671445, mean_q: 77.278454, mean_eps: 0.456051
 121060/200000: episode: 965, duration: 0.671s, episode steps: 115, steps per second: 172, episode reward: -80.270, mean reward: -0.698 [-100.000, 10.030], mean action: 1.765 [0.000, 3.000],  loss: 20.389085, mae: 61.836427, mean_q: 78.121927, mean_eps: 0.455491
 122060/200000: episode: 966, duration: 6.796s, episode steps: 1000, steps per second: 147, episode reward:  1.454, mean reward:  0.001 [-22.061, 29.045], mean action: 1.868 [0.000, 3.000],  loss: 22.676039, mae: 61.407045, mean_q: 77.290071, mean_eps: 0.452982
 122190/200000: episode: 967, duration: 0.945s, episode steps: 130, steps per second: 138, episode reward: 21.620, mean reward:  0.166 [-100.000, 20.899], mean action: 1.423 [0.000, 3.000],  loss: 18.270882, mae: 61.286383, mean_q: 76.867005, mean_eps: 0.450440
 122276/200000: episode: 968, duration: 0.580s, episode steps:  86, steps per second: 148, episode reward: -63.584, mean reward: -0.739 [-100.000, 11.462], mean action: 1.674 [0.000, 3.000],  loss: 19.175519, mae: 61.117520, mean_q: 76.968922, mean_eps: 0.449954
 122395/200000: episode: 969, duration: 0.920s, episode steps: 119, steps per second: 129, episode reward: -20.146, mean reward: -0.169 [-100.000,  8.412], mean action: 1.832 [0.000, 3.000],  loss: 22.440263, mae: 60.552845, mean_q: 75.947230, mean_eps: 0.449492
 123395/200000: episode: 970, duration: 7.329s, episode steps: 1000, steps per second: 136, episode reward: 20.427, mean reward:  0.020 [-23.693, 25.665], mean action: 1.486 [0.000, 3.000],  loss: 20.370772, mae: 61.440920, mean_q: 78.088219, mean_eps: 0.446975
 123497/200000: episode: 971, duration: 0.596s, episode steps: 102, steps per second: 171, episode reward: -22.092, mean reward: -0.217 [-100.000, 17.912], mean action: 1.716 [0.000, 3.000],  loss: 20.909520, mae: 62.049316, mean_q: 79.265699, mean_eps: 0.444495
 123658/200000: episode: 972, duration: 0.964s, episode steps: 161, steps per second: 167, episode reward: -11.730, mean reward: -0.073 [-100.000,  9.697], mean action: 1.789 [0.000, 3.000],  loss: 15.479090, mae: 61.822277, mean_q: 79.509001, mean_eps: 0.443903
 123872/200000: episode: 973, duration: 1.229s, episode steps: 214, steps per second: 174, episode reward: -167.961, mean reward: -0.785 [-100.000, 14.640], mean action: 1.827 [0.000, 3.000],  loss: 21.105518, mae: 61.885893, mean_q: 78.889089, mean_eps: 0.443060
 124035/200000: episode: 974, duration: 1.085s, episode steps: 163, steps per second: 150, episode reward: -18.685, mean reward: -0.115 [-100.000, 11.492], mean action: 1.865 [0.000, 3.000],  loss: 15.371249, mae: 61.944376, mean_q: 79.474318, mean_eps: 0.442211
 124459/200000: episode: 975, duration: 2.826s, episode steps: 424, steps per second: 150, episode reward: -86.753, mean reward: -0.205 [-100.000,  9.933], mean action: 1.800 [0.000, 3.000],  loss: 23.561943, mae: 61.234850, mean_q: 78.425859, mean_eps: 0.440891
 124642/200000: episode: 976, duration: 1.187s, episode steps: 183, steps per second: 154, episode reward: -49.549, mean reward: -0.271 [-100.000, 10.749], mean action: 1.710 [0.000, 3.000],  loss: 19.262541, mae: 60.858358, mean_q: 77.796391, mean_eps: 0.439525
 124730/200000: episode: 977, duration: 0.560s, episode steps:  88, steps per second: 157, episode reward: -63.373, mean reward: -0.720 [-100.000, 11.309], mean action: 1.750 [0.000, 3.000],  loss: 17.013240, mae: 59.857316, mean_q: 75.975281, mean_eps: 0.438915
 124873/200000: episode: 978, duration: 0.881s, episode steps: 143, steps per second: 162, episode reward: -34.208, mean reward: -0.239 [-100.000, 14.417], mean action: 1.706 [0.000, 3.000],  loss: 17.959201, mae: 60.127566, mean_q: 77.131828, mean_eps: 0.438395
 125040/200000: episode: 979, duration: 1.151s, episode steps: 167, steps per second: 145, episode reward: -30.148, mean reward: -0.181 [-100.000, 13.982], mean action: 1.713 [0.000, 3.000],  loss: 18.518454, mae: 60.543321, mean_q: 77.605986, mean_eps: 0.437698
 125134/200000: episode: 980, duration: 0.559s, episode steps:  94, steps per second: 168, episode reward: -47.657, mean reward: -0.507 [-100.000, 10.697], mean action: 1.660 [0.000, 3.000],  loss: 19.132489, mae: 60.992169, mean_q: 77.361316, mean_eps: 0.437111
 125261/200000: episode: 981, duration: 0.728s, episode steps: 127, steps per second: 175, episode reward: -85.834, mean reward: -0.676 [-100.000,  9.453], mean action: 1.630 [0.000, 3.000],  loss: 20.843120, mae: 61.204768, mean_q: 77.706349, mean_eps: 0.436613
 126261/200000: episode: 982, duration: 7.447s, episode steps: 1000, steps per second: 134, episode reward:  9.032, mean reward:  0.009 [-21.909, 21.975], mean action: 1.545 [0.000, 3.000],  loss: 18.978275, mae: 60.448735, mean_q: 77.144644, mean_eps: 0.434078
 126405/200000: episode: 983, duration: 0.937s, episode steps: 144, steps per second: 154, episode reward: -2.203, mean reward: -0.015 [-100.000, 10.876], mean action: 1.597 [0.000, 3.000],  loss: 18.997675, mae: 59.727001, mean_q: 76.295752, mean_eps: 0.431504
 126563/200000: episode: 984, duration: 1.255s, episode steps: 158, steps per second: 126, episode reward: -49.813, mean reward: -0.315 [-100.000, 23.601], mean action: 1.797 [0.000, 3.000],  loss: 22.863114, mae: 59.358489, mean_q: 75.990375, mean_eps: 0.430824
 127563/200000: episode: 985, duration: 8.579s, episode steps: 1000, steps per second: 117, episode reward: -13.231, mean reward: -0.013 [-22.787, 22.626], mean action: 1.523 [0.000, 3.000],  loss: 19.357474, mae: 59.030361, mean_q: 75.645112, mean_eps: 0.428219
 127690/200000: episode: 986, duration: 1.024s, episode steps: 127, steps per second: 124, episode reward: -30.420, mean reward: -0.240 [-100.000,  9.991], mean action: 1.677 [0.000, 3.000],  loss: 23.352976, mae: 58.293469, mean_q: 74.157225, mean_eps: 0.425683
 127819/200000: episode: 987, duration: 0.983s, episode steps: 129, steps per second: 131, episode reward: -29.875, mean reward: -0.232 [-100.000, 15.527], mean action: 1.450 [0.000, 3.000],  loss: 15.480455, mae: 58.299830, mean_q: 74.819316, mean_eps: 0.425107
 127991/200000: episode: 988, duration: 1.122s, episode steps: 172, steps per second: 153, episode reward:  4.777, mean reward:  0.028 [-100.000,  9.366], mean action: 1.715 [0.000, 3.000],  loss: 20.550957, mae: 59.541666, mean_q: 76.337957, mean_eps: 0.424430
 128105/200000: episode: 989, duration: 0.867s, episode steps: 114, steps per second: 131, episode reward: 23.624, mean reward:  0.207 [-100.000, 18.554], mean action: 1.754 [0.000, 3.000],  loss: 13.882861, mae: 59.943834, mean_q: 76.698412, mean_eps: 0.423786
 128242/200000: episode: 990, duration: 0.892s, episode steps: 137, steps per second: 154, episode reward: -36.288, mean reward: -0.265 [-100.000,  9.898], mean action: 1.664 [0.000, 3.000],  loss: 18.469851, mae: 59.750446, mean_q: 76.080041, mean_eps: 0.423222
 128371/200000: episode: 991, duration: 0.797s, episode steps: 129, steps per second: 162, episode reward: -72.429, mean reward: -0.561 [-100.000, 10.058], mean action: 1.481 [0.000, 3.000],  loss: 18.949547, mae: 60.656875, mean_q: 77.843941, mean_eps: 0.422623
 128512/200000: episode: 992, duration: 0.921s, episode steps: 141, steps per second: 153, episode reward: -51.967, mean reward: -0.369 [-100.000,  9.020], mean action: 1.603 [0.000, 3.000],  loss: 18.430692, mae: 60.010342, mean_q: 77.260914, mean_eps: 0.422015
 128663/200000: episode: 993, duration: 0.936s, episode steps: 151, steps per second: 161, episode reward: -6.961, mean reward: -0.046 [-100.000, 11.996], mean action: 1.788 [0.000, 3.000],  loss: 21.739984, mae: 59.517961, mean_q: 76.636200, mean_eps: 0.421359
 129663/200000: episode: 994, duration: 8.187s, episode steps: 1000, steps per second: 122, episode reward: 33.566, mean reward:  0.034 [-24.348, 23.833], mean action: 1.577 [0.000, 3.000],  loss: 17.497417, mae: 59.650788, mean_q: 76.694612, mean_eps: 0.418769
 129922/200000: episode: 995, duration: 1.688s, episode steps: 259, steps per second: 153, episode reward: -109.036, mean reward: -0.421 [-100.000, 21.600], mean action: 1.923 [0.000, 3.000],  loss: 17.128819, mae: 59.985039, mean_q: 77.006307, mean_eps: 0.415936
 130922/200000: episode: 996, duration: 8.047s, episode steps: 1000, steps per second: 124, episode reward: 15.681, mean reward:  0.016 [-20.862, 23.464], mean action: 1.710 [0.000, 3.000],  loss: 14.210839, mae: 57.818110, mean_q: 74.042048, mean_eps: 0.413103
 131041/200000: episode: 997, duration: 0.763s, episode steps: 119, steps per second: 156, episode reward: -19.920, mean reward: -0.167 [-100.000, 14.611], mean action: 1.655 [0.000, 3.000],  loss: 18.931180, mae: 57.565815, mean_q: 74.196775, mean_eps: 0.410585
 131149/200000: episode: 998, duration: 0.688s, episode steps: 108, steps per second: 157, episode reward: -2.014, mean reward: -0.019 [-100.000, 15.828], mean action: 1.759 [0.000, 3.000],  loss: 12.087357, mae: 57.368997, mean_q: 74.849125, mean_eps: 0.410075
 131274/200000: episode: 999, duration: 0.827s, episode steps: 125, steps per second: 151, episode reward: -11.163, mean reward: -0.089 [-100.000, 17.949], mean action: 1.736 [0.000, 3.000],  loss: 14.444543, mae: 56.806462, mean_q: 73.028037, mean_eps: 0.409550
 131385/200000: episode: 1000, duration: 0.749s, episode steps: 111, steps per second: 148, episode reward:  4.100, mean reward:  0.037 [-100.000, 10.303], mean action: 1.667 [0.000, 3.000],  loss: 17.886350, mae: 56.692200, mean_q: 72.740917, mean_eps: 0.409019
 131485/200000: episode: 1001, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: -32.148, mean reward: -0.321 [-100.000, 10.821], mean action: 1.540 [0.000, 3.000],  loss: 14.256102, mae: 55.758800, mean_q: 70.972859, mean_eps: 0.408545
 131607/200000: episode: 1002, duration: 0.842s, episode steps: 122, steps per second: 145, episode reward: -10.861, mean reward: -0.089 [-100.000,  9.952], mean action: 1.615 [0.000, 3.000],  loss: 15.676609, mae: 56.629171, mean_q: 72.600741, mean_eps: 0.408045
 131705/200000: episode: 1003, duration: 0.639s, episode steps:  98, steps per second: 153, episode reward:  4.200, mean reward:  0.043 [-100.000, 13.154], mean action: 1.541 [0.000, 3.000],  loss: 13.633650, mae: 56.437362, mean_q: 72.754841, mean_eps: 0.407550
 132705/200000: episode: 1004, duration: 6.951s, episode steps: 1000, steps per second: 144, episode reward: -31.517, mean reward: -0.032 [-19.599, 15.383], mean action: 1.740 [0.000, 3.000],  loss: 13.612421, mae: 55.807688, mean_q: 71.986930, mean_eps: 0.405080
 133104/200000: episode: 1005, duration: 2.751s, episode steps: 399, steps per second: 145, episode reward: -218.347, mean reward: -0.547 [-100.000, 17.276], mean action: 1.619 [0.000, 3.000],  loss: 13.903406, mae: 54.353681, mean_q: 70.361504, mean_eps: 0.401932
 133204/200000: episode: 1006, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: 13.747, mean reward:  0.137 [-100.000, 17.275], mean action: 1.700 [0.000, 3.000],  loss: 15.994289, mae: 54.515299, mean_q: 68.985535, mean_eps: 0.400809
 133322/200000: episode: 1007, duration: 0.781s, episode steps: 118, steps per second: 151, episode reward: -41.499, mean reward: -0.352 [-100.000,  9.952], mean action: 1.602 [0.000, 3.000],  loss: 12.759920, mae: 54.262697, mean_q: 69.623646, mean_eps: 0.400319
 133538/200000: episode: 1008, duration: 1.529s, episode steps: 216, steps per second: 141, episode reward: -245.311, mean reward: -1.136 [-100.000, 30.780], mean action: 1.565 [0.000, 3.000],  loss: 16.299527, mae: 53.977933, mean_q: 69.799236, mean_eps: 0.399567
 134273/200000: episode: 1009, duration: 5.554s, episode steps: 735, steps per second: 132, episode reward: -143.153, mean reward: -0.195 [-100.000, 22.789], mean action: 1.546 [0.000, 3.000],  loss: 15.738380, mae: 53.581099, mean_q: 69.099139, mean_eps: 0.397427
 134352/200000: episode: 1010, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -80.654, mean reward: -1.021 [-100.000,  9.135], mean action: 1.772 [0.000, 3.000],  loss: 10.516140, mae: 52.189742, mean_q: 67.073041, mean_eps: 0.395596
 135352/200000: episode: 1011, duration: 8.445s, episode steps: 1000, steps per second: 118, episode reward: -79.810, mean reward: -0.080 [-23.263, 25.422], mean action: 1.723 [0.000, 3.000],  loss: 15.586820, mae: 52.251367, mean_q: 68.038263, mean_eps: 0.393168
 136352/200000: episode: 1012, duration: 7.509s, episode steps: 1000, steps per second: 133, episode reward: 43.398, mean reward:  0.043 [-22.664, 25.782], mean action: 1.528 [0.000, 3.000],  loss: 13.975147, mae: 52.524719, mean_q: 68.671209, mean_eps: 0.388668
 136518/200000: episode: 1013, duration: 1.110s, episode steps: 166, steps per second: 150, episode reward: -12.202, mean reward: -0.074 [-100.000, 11.155], mean action: 1.795 [0.000, 3.000],  loss: 18.050180, mae: 51.936941, mean_q: 67.947069, mean_eps: 0.386045
 137518/200000: episode: 1014, duration: 6.955s, episode steps: 1000, steps per second: 144, episode reward: -87.651, mean reward: -0.088 [-20.035, 21.834], mean action: 1.672 [0.000, 3.000],  loss: 14.056406, mae: 51.005141, mean_q: 66.451219, mean_eps: 0.383421
 138518/200000: episode: 1015, duration: 8.099s, episode steps: 1000, steps per second: 123, episode reward: -74.228, mean reward: -0.074 [-25.426, 18.032], mean action: 1.838 [0.000, 3.000],  loss: 14.422518, mae: 49.517495, mean_q: 64.289176, mean_eps: 0.378921
 139518/200000: episode: 1016, duration: 7.705s, episode steps: 1000, steps per second: 130, episode reward: -14.282, mean reward: -0.014 [-25.621, 23.849], mean action: 1.669 [0.000, 3.000],  loss: 12.490053, mae: 47.764539, mean_q: 61.875075, mean_eps: 0.374421
 140518/200000: episode: 1017, duration: 8.114s, episode steps: 1000, steps per second: 123, episode reward: 73.074, mean reward:  0.073 [-21.255, 23.402], mean action: 1.619 [0.000, 3.000],  loss: 13.902504, mae: 48.019140, mean_q: 62.357776, mean_eps: 0.369921
 141518/200000: episode: 1018, duration: 7.268s, episode steps: 1000, steps per second: 138, episode reward:  6.046, mean reward:  0.006 [-22.130, 21.197], mean action: 1.728 [0.000, 3.000],  loss: 11.868737, mae: 46.244450, mean_q: 60.160391, mean_eps: 0.365421
 142518/200000: episode: 1019, duration: 8.248s, episode steps: 1000, steps per second: 121, episode reward: 21.620, mean reward:  0.022 [-23.808, 23.324], mean action: 1.620 [0.000, 3.000],  loss: 11.723205, mae: 44.677012, mean_q: 58.120742, mean_eps: 0.360921
 143518/200000: episode: 1020, duration: 7.969s, episode steps: 1000, steps per second: 125, episode reward: 28.547, mean reward:  0.029 [-23.216, 24.894], mean action: 1.631 [0.000, 3.000],  loss: 11.659762, mae: 44.187343, mean_q: 57.396198, mean_eps: 0.356421
 144518/200000: episode: 1021, duration: 7.384s, episode steps: 1000, steps per second: 135, episode reward: -36.490, mean reward: -0.036 [-24.104, 25.007], mean action: 1.590 [0.000, 3.000],  loss: 11.374174, mae: 43.104109, mean_q: 56.274540, mean_eps: 0.351921
 145518/200000: episode: 1022, duration: 7.923s, episode steps: 1000, steps per second: 126, episode reward: 48.291, mean reward:  0.048 [-22.788, 29.666], mean action: 1.437 [0.000, 3.000],  loss: 8.446274, mae: 41.916282, mean_q: 54.910946, mean_eps: 0.347421
 146518/200000: episode: 1023, duration: 7.193s, episode steps: 1000, steps per second: 139, episode reward: 42.352, mean reward:  0.042 [-24.197, 24.841], mean action: 1.420 [0.000, 3.000],  loss: 10.606963, mae: 41.266969, mean_q: 54.187352, mean_eps: 0.342921
 146716/200000: episode: 1024, duration: 1.343s, episode steps: 198, steps per second: 147, episode reward: -9.889, mean reward: -0.050 [-100.000, 32.218], mean action: 1.859 [0.000, 3.000],  loss: 6.863221, mae: 40.452469, mean_q: 52.861240, mean_eps: 0.340226
 146811/200000: episode: 1025, duration: 0.617s, episode steps:  95, steps per second: 154, episode reward: -149.608, mean reward: -1.575 [-100.000, 10.119], mean action: 1.516 [0.000, 3.000],  loss: 10.074801, mae: 40.068041, mean_q: 52.361169, mean_eps: 0.339566
 146948/200000: episode: 1026, duration: 0.897s, episode steps: 137, steps per second: 153, episode reward: -506.528, mean reward: -3.697 [-100.000, 20.140], mean action: 1.387 [0.000, 3.000],  loss: 10.102833, mae: 40.287956, mean_q: 52.545724, mean_eps: 0.339044
 147070/200000: episode: 1027, duration: 0.843s, episode steps: 122, steps per second: 145, episode reward:  3.825, mean reward:  0.031 [-100.000, 24.740], mean action: 1.770 [0.000, 3.000],  loss: 11.448903, mae: 40.501682, mean_q: 51.416356, mean_eps: 0.338462
 147164/200000: episode: 1028, duration: 0.622s, episode steps:  94, steps per second: 151, episode reward: -126.585, mean reward: -1.347 [-100.000,  9.907], mean action: 1.638 [0.000, 3.000],  loss: 8.540237, mae: 40.719772, mean_q: 52.031912, mean_eps: 0.337976
 148164/200000: episode: 1029, duration: 7.980s, episode steps: 1000, steps per second: 125, episode reward: 37.023, mean reward:  0.037 [-20.097, 20.771], mean action: 1.808 [0.000, 3.000],  loss: 9.691216, mae: 41.278510, mean_q: 52.586732, mean_eps: 0.335514
 149164/200000: episode: 1030, duration: 8.593s, episode steps: 1000, steps per second: 116, episode reward:  9.978, mean reward:  0.010 [-22.120, 20.707], mean action: 1.557 [0.000, 3.000],  loss: 14.804348, mae: 40.915311, mean_q: 51.904004, mean_eps: 0.331014
 150164/200000: episode: 1031, duration: 9.371s, episode steps: 1000, steps per second: 107, episode reward: -69.708, mean reward: -0.070 [-23.026, 30.267], mean action: 1.740 [0.000, 3.000],  loss: 10.078243, mae: 40.203486, mean_q: 51.690211, mean_eps: 0.326514
 151063/200000: episode: 1032, duration: 6.778s, episode steps: 899, steps per second: 133, episode reward: -290.346, mean reward: -0.323 [-100.000, 22.479], mean action: 1.745 [0.000, 3.000],  loss: 9.724553, mae: 39.023493, mean_q: 50.828914, mean_eps: 0.322242
 151220/200000: episode: 1033, duration: 0.986s, episode steps: 157, steps per second: 159, episode reward: -41.410, mean reward: -0.264 [-100.000, 12.166], mean action: 1.732 [0.000, 3.000],  loss: 5.352874, mae: 38.923376, mean_q: 50.526752, mean_eps: 0.319865
 152220/200000: episode: 1034, duration: 7.823s, episode steps: 1000, steps per second: 128, episode reward: 106.471, mean reward:  0.106 [-22.724, 22.793], mean action: 1.496 [0.000, 3.000],  loss: 7.515115, mae: 37.639801, mean_q: 49.111477, mean_eps: 0.317262
 152764/200000: episode: 1035, duration: 3.976s, episode steps: 544, steps per second: 137, episode reward: -147.377, mean reward: -0.271 [-100.000, 19.898], mean action: 1.612 [0.000, 3.000],  loss: 6.678061, mae: 37.823942, mean_q: 49.346698, mean_eps: 0.313788
 153764/200000: episode: 1036, duration: 8.201s, episode steps: 1000, steps per second: 122, episode reward: 71.492, mean reward:  0.071 [-19.395, 22.363], mean action: 1.624 [0.000, 3.000],  loss: 7.207562, mae: 37.781165, mean_q: 49.498732, mean_eps: 0.310314
 153949/200000: episode: 1037, duration: 1.189s, episode steps: 185, steps per second: 156, episode reward: -53.027, mean reward: -0.287 [-100.000, 10.060], mean action: 1.778 [0.000, 3.000],  loss: 5.594763, mae: 37.133036, mean_q: 48.816981, mean_eps: 0.307648
 154653/200000: episode: 1038, duration: 5.025s, episode steps: 704, steps per second: 140, episode reward: -99.422, mean reward: -0.141 [-100.000, 19.650], mean action: 1.474 [0.000, 3.000],  loss: 7.385630, mae: 37.093446, mean_q: 49.060424, mean_eps: 0.305648
 155653/200000: episode: 1039, duration: 7.723s, episode steps: 1000, steps per second: 129, episode reward: 20.659, mean reward:  0.021 [-20.114, 16.446], mean action: 1.713 [0.000, 3.000],  loss: 5.633420, mae: 36.384430, mean_q: 47.978655, mean_eps: 0.301814
 156653/200000: episode: 1040, duration: 7.332s, episode steps: 1000, steps per second: 136, episode reward: 55.749, mean reward:  0.056 [-24.246, 23.329], mean action: 1.321 [0.000, 3.000],  loss: 5.695204, mae: 35.786277, mean_q: 47.507877, mean_eps: 0.297314
 157653/200000: episode: 1041, duration: 8.287s, episode steps: 1000, steps per second: 121, episode reward: 125.463, mean reward:  0.125 [-20.357, 24.331], mean action: 1.383 [0.000, 3.000],  loss: 5.059499, mae: 35.118068, mean_q: 46.700871, mean_eps: 0.292814
 158290/200000: episode: 1042, duration: 4.455s, episode steps: 637, steps per second: 143, episode reward: -31.643, mean reward: -0.050 [-100.000, 21.300], mean action: 1.584 [0.000, 3.000],  loss: 5.270995, mae: 34.492076, mean_q: 45.946926, mean_eps: 0.289131
 158391/200000: episode: 1043, duration: 0.727s, episode steps: 101, steps per second: 139, episode reward: -41.294, mean reward: -0.409 [-100.000, 18.304], mean action: 1.733 [0.000, 3.000],  loss: 5.898321, mae: 33.910334, mean_q: 45.050809, mean_eps: 0.287470
 159391/200000: episode: 1044, duration: 8.555s, episode steps: 1000, steps per second: 117, episode reward: -24.373, mean reward: -0.024 [-4.546,  4.786], mean action: 1.651 [0.000, 3.000],  loss: 5.750849, mae: 34.747449, mean_q: 46.258435, mean_eps: 0.284993
 160391/200000: episode: 1045, duration: 7.886s, episode steps: 1000, steps per second: 127, episode reward: 15.643, mean reward:  0.016 [-19.694, 22.152], mean action: 1.758 [0.000, 3.000],  loss: 4.643130, mae: 33.545169, mean_q: 44.788020, mean_eps: 0.280493
 161178/200000: episode: 1046, duration: 6.745s, episode steps: 787, steps per second: 117, episode reward: -150.991, mean reward: -0.192 [-100.000, 12.583], mean action: 1.676 [0.000, 3.000],  loss: 5.514141, mae: 33.177687, mean_q: 44.149691, mean_eps: 0.276472
 162178/200000: episode: 1047, duration: 7.898s, episode steps: 1000, steps per second: 127, episode reward: 21.893, mean reward:  0.022 [-9.641, 10.003], mean action: 1.704 [0.000, 3.000],  loss: 4.881856, mae: 32.513930, mean_q: 43.283647, mean_eps: 0.272451
 162315/200000: episode: 1048, duration: 0.917s, episode steps: 137, steps per second: 149, episode reward: -138.540, mean reward: -1.011 [-100.000,  4.002], mean action: 1.796 [0.000, 3.000],  loss: 4.541529, mae: 32.429461, mean_q: 43.276027, mean_eps: 0.269893
 163315/200000: episode: 1049, duration: 8.095s, episode steps: 1000, steps per second: 124, episode reward: -63.900, mean reward: -0.064 [-3.722,  4.805], mean action: 1.687 [0.000, 3.000],  loss: 5.654369, mae: 32.039848, mean_q: 42.255846, mean_eps: 0.267335
 163457/200000: episode: 1050, duration: 0.892s, episode steps: 142, steps per second: 159, episode reward: -134.995, mean reward: -0.951 [-100.000,  3.536], mean action: 1.803 [0.000, 3.000],  loss: 4.382311, mae: 31.850170, mean_q: 42.005192, mean_eps: 0.264765
 164457/200000: episode: 1051, duration: 7.399s, episode steps: 1000, steps per second: 135, episode reward: -59.020, mean reward: -0.059 [-4.316,  5.000], mean action: 1.667 [0.000, 3.000],  loss: 5.421709, mae: 31.298753, mean_q: 41.346843, mean_eps: 0.262196
 165457/200000: episode: 1052, duration: 7.238s, episode steps: 1000, steps per second: 138, episode reward: 77.502, mean reward:  0.078 [-22.872, 22.196], mean action: 1.520 [0.000, 3.000],  loss: 5.751232, mae: 30.709679, mean_q: 40.472298, mean_eps: 0.257696
 166457/200000: episode: 1053, duration: 8.045s, episode steps: 1000, steps per second: 124, episode reward: 34.850, mean reward:  0.035 [-21.516, 23.433], mean action: 1.453 [0.000, 3.000],  loss: 4.291982, mae: 29.955195, mean_q: 39.377890, mean_eps: 0.253196
 167457/200000: episode: 1054, duration: 7.574s, episode steps: 1000, steps per second: 132, episode reward: 36.292, mean reward:  0.036 [-24.880, 21.168], mean action: 1.766 [0.000, 3.000],  loss: 4.715702, mae: 28.423257, mean_q: 37.726227, mean_eps: 0.248696
 168457/200000: episode: 1055, duration: 7.525s, episode steps: 1000, steps per second: 133, episode reward: 25.969, mean reward:  0.026 [-12.537,  9.170], mean action: 1.671 [0.000, 3.000],  loss: 3.934074, mae: 27.214756, mean_q: 36.226836, mean_eps: 0.244196
 169457/200000: episode: 1056, duration: 7.905s, episode steps: 1000, steps per second: 127, episode reward: 128.962, mean reward:  0.129 [-21.086, 23.577], mean action: 1.429 [0.000, 3.000],  loss: 4.216148, mae: 26.620975, mean_q: 35.282909, mean_eps: 0.239696
 170457/200000: episode: 1057, duration: 8.413s, episode steps: 1000, steps per second: 119, episode reward: 54.272, mean reward:  0.054 [-11.420, 13.674], mean action: 1.673 [0.000, 3.000],  loss: 3.730545, mae: 26.030347, mean_q: 34.315846, mean_eps: 0.235196
 171457/200000: episode: 1058, duration: 7.429s, episode steps: 1000, steps per second: 135, episode reward: 50.696, mean reward:  0.051 [-21.816, 22.368], mean action: 1.459 [0.000, 3.000],  loss: 3.273570, mae: 26.388238, mean_q: 34.681222, mean_eps: 0.230696
 172425/200000: episode: 1059, duration: 7.372s, episode steps: 968, steps per second: 131, episode reward: 183.231, mean reward:  0.189 [-17.604, 100.000], mean action: 1.579 [0.000, 3.000],  loss: 2.470967, mae: 26.047299, mean_q: 34.175752, mean_eps: 0.226268
 173425/200000: episode: 1060, duration: 8.011s, episode steps: 1000, steps per second: 125, episode reward: 60.893, mean reward:  0.061 [-20.237, 21.476], mean action: 1.830 [0.000, 3.000],  loss: 2.757623, mae: 25.363558, mean_q: 33.292839, mean_eps: 0.221840
 174425/200000: episode: 1061, duration: 7.425s, episode steps: 1000, steps per second: 135, episode reward: -11.903, mean reward: -0.012 [-18.554, 10.752], mean action: 1.927 [0.000, 3.000],  loss: 2.613661, mae: 24.646588, mean_q: 32.429635, mean_eps: 0.217340
 175425/200000: episode: 1062, duration: 8.223s, episode steps: 1000, steps per second: 122, episode reward: 46.303, mean reward:  0.046 [-21.739, 22.164], mean action: 1.506 [0.000, 3.000],  loss: 2.395600, mae: 23.434529, mean_q: 31.070208, mean_eps: 0.212840
 176290/200000: episode: 1063, duration: 6.693s, episode steps: 865, steps per second: 129, episode reward: 156.633, mean reward:  0.181 [-17.585, 100.000], mean action: 1.532 [0.000, 3.000],  loss: 2.294734, mae: 22.648516, mean_q: 30.166708, mean_eps: 0.208644
 177290/200000: episode: 1064, duration: 7.774s, episode steps: 1000, steps per second: 129, episode reward: 116.347, mean reward:  0.116 [-22.958, 22.174], mean action: 1.393 [0.000, 3.000],  loss: 2.620378, mae: 22.756655, mean_q: 30.411745, mean_eps: 0.204447
 177393/200000: episode: 1065, duration: 0.595s, episode steps: 103, steps per second: 173, episode reward: -10.831, mean reward: -0.105 [-100.000, 11.113], mean action: 1.854 [0.000, 3.000],  loss: 3.148966, mae: 22.990779, mean_q: 30.852868, mean_eps: 0.201966
 178065/200000: episode: 1066, duration: 4.933s, episode steps: 672, steps per second: 136, episode reward: 211.603, mean reward:  0.315 [-18.672, 100.000], mean action: 1.438 [0.000, 3.000],  loss: 2.422713, mae: 23.411010, mean_q: 31.276901, mean_eps: 0.200222
 178169/200000: episode: 1067, duration: 0.697s, episode steps: 104, steps per second: 149, episode reward: -71.355, mean reward: -0.686 [-100.000,  9.424], mean action: 1.433 [0.000, 3.000],  loss: 6.659636, mae: 23.780685, mean_q: 31.845784, mean_eps: 0.198476
 178865/200000: episode: 1068, duration: 5.334s, episode steps: 696, steps per second: 130, episode reward: 215.275, mean reward:  0.309 [-19.728, 100.000], mean action: 1.526 [0.000, 3.000],  loss: 3.686683, mae: 23.844825, mean_q: 32.023797, mean_eps: 0.196676
 179865/200000: episode: 1069, duration: 7.401s, episode steps: 1000, steps per second: 135, episode reward:  9.145, mean reward:  0.009 [-10.592, 13.924], mean action: 1.631 [0.000, 3.000],  loss: 3.241621, mae: 23.744392, mean_q: 31.831809, mean_eps: 0.192860
 180865/200000: episode: 1070, duration: 7.784s, episode steps: 1000, steps per second: 128, episode reward: -26.914, mean reward: -0.027 [-4.599,  5.021], mean action: 1.713 [0.000, 3.000],  loss: 2.546717, mae: 23.328587, mean_q: 31.292428, mean_eps: 0.188360
 181688/200000: episode: 1071, duration: 6.399s, episode steps: 823, steps per second: 129, episode reward: 185.916, mean reward:  0.226 [-18.405, 100.000], mean action: 1.441 [0.000, 3.000],  loss: 2.693278, mae: 22.989786, mean_q: 30.902159, mean_eps: 0.184258
 181766/200000: episode: 1072, duration: 0.518s, episode steps:  78, steps per second: 151, episode reward: -148.758, mean reward: -1.907 [-100.000, 13.587], mean action: 0.641 [0.000, 3.000],  loss: 3.273078, mae: 22.667256, mean_q: 30.383259, mean_eps: 0.182231
 182766/200000: episode: 1073, duration: 7.694s, episode steps: 1000, steps per second: 130, episode reward: 24.731, mean reward:  0.025 [-4.780,  5.287], mean action: 1.777 [0.000, 3.000],  loss: 3.042568, mae: 22.706925, mean_q: 30.513982, mean_eps: 0.179805
 183766/200000: episode: 1074, duration: 7.495s, episode steps: 1000, steps per second: 133, episode reward: -40.002, mean reward: -0.040 [-3.793,  5.046], mean action: 1.819 [0.000, 3.000],  loss: 2.638538, mae: 22.706103, mean_q: 30.518504, mean_eps: 0.175305
 184766/200000: episode: 1075, duration: 7.207s, episode steps: 1000, steps per second: 139, episode reward: 39.661, mean reward:  0.040 [-19.855, 24.860], mean action: 1.546 [0.000, 3.000],  loss: 2.596933, mae: 23.642982, mean_q: 31.736564, mean_eps: 0.170805
 185766/200000: episode: 1076, duration: 7.217s, episode steps: 1000, steps per second: 139, episode reward: -12.745, mean reward: -0.013 [-13.006, 15.154], mean action: 1.749 [0.000, 3.000],  loss: 2.639658, mae: 23.281198, mean_q: 31.211080, mean_eps: 0.166305
 186649/200000: episode: 1077, duration: 6.181s, episode steps: 883, steps per second: 143, episode reward: 205.796, mean reward:  0.233 [-20.476, 100.000], mean action: 1.187 [0.000, 3.000],  loss: 3.127037, mae: 23.402999, mean_q: 31.400184, mean_eps: 0.162068
 187143/200000: episode: 1078, duration: 3.130s, episode steps: 494, steps per second: 158, episode reward: -124.853, mean reward: -0.253 [-100.000, 16.519], mean action: 1.822 [0.000, 3.000],  loss: 2.850283, mae: 23.822741, mean_q: 31.990893, mean_eps: 0.158970
 187745/200000: episode: 1079, duration: 4.003s, episode steps: 602, steps per second: 150, episode reward: 164.583, mean reward:  0.273 [-17.826, 100.000], mean action: 1.686 [0.000, 3.000],  loss: 4.435858, mae: 24.899000, mean_q: 33.395632, mean_eps: 0.156504
 188745/200000: episode: 1080, duration: 7.157s, episode steps: 1000, steps per second: 140, episode reward: 53.841, mean reward:  0.054 [-18.207, 16.307], mean action: 1.281 [0.000, 3.000],  loss: 3.067660, mae: 24.792466, mean_q: 33.172217, mean_eps: 0.152900
 189692/200000: episode: 1081, duration: 6.717s, episode steps: 947, steps per second: 141, episode reward: 211.617, mean reward:  0.223 [-21.157, 100.000], mean action: 1.108 [0.000, 3.000],  loss: 4.473227, mae: 25.023119, mean_q: 33.503819, mean_eps: 0.148519
 190692/200000: episode: 1082, duration: 7.398s, episode steps: 1000, steps per second: 135, episode reward: 87.441, mean reward:  0.087 [-21.850, 12.909], mean action: 1.613 [0.000, 3.000],  loss: 3.216971, mae: 24.427962, mean_q: 32.779275, mean_eps: 0.144138
 191287/200000: episode: 1083, duration: 3.895s, episode steps: 595, steps per second: 153, episode reward: 197.399, mean reward:  0.332 [-12.549, 100.000], mean action: 1.487 [0.000, 3.000],  loss: 3.328087, mae: 24.552865, mean_q: 32.805398, mean_eps: 0.140549
 191950/200000: episode: 1084, duration: 4.390s, episode steps: 663, steps per second: 151, episode reward: 203.369, mean reward:  0.307 [-19.803, 100.000], mean action: 1.406 [0.000, 3.000],  loss: 4.201622, mae: 24.821983, mean_q: 33.151661, mean_eps: 0.137719
 192499/200000: episode: 1085, duration: 3.585s, episode steps: 549, steps per second: 153, episode reward: 219.819, mean reward:  0.400 [-18.495, 100.000], mean action: 1.732 [0.000, 3.000],  loss: 3.190531, mae: 24.997905, mean_q: 33.300011, mean_eps: 0.134992
 192796/200000: episode: 1086, duration: 1.872s, episode steps: 297, steps per second: 159, episode reward: -102.030, mean reward: -0.344 [-100.000, 17.943], mean action: 1.801 [0.000, 3.000],  loss: 5.818490, mae: 25.017283, mean_q: 33.382480, mean_eps: 0.133088
 193183/200000: episode: 1087, duration: 2.572s, episode steps: 387, steps per second: 150, episode reward: -200.059, mean reward: -0.517 [-100.000, 10.282], mean action: 1.915 [0.000, 3.000],  loss: 4.700856, mae: 25.859927, mean_q: 34.505651, mean_eps: 0.131549
 193856/200000: episode: 1088, duration: 4.386s, episode steps: 673, steps per second: 153, episode reward: 181.329, mean reward:  0.269 [-19.431, 100.000], mean action: 0.917 [0.000, 3.000],  loss: 4.520888, mae: 25.797616, mean_q: 34.048176, mean_eps: 0.129164
 194238/200000: episode: 1089, duration: 2.509s, episode steps: 382, steps per second: 152, episode reward: -125.570, mean reward: -0.329 [-100.000, 11.678], mean action: 1.812 [0.000, 3.000],  loss: 3.994983, mae: 26.734188, mean_q: 35.353592, mean_eps: 0.126791
 194694/200000: episode: 1090, duration: 3.083s, episode steps: 456, steps per second: 148, episode reward: 164.656, mean reward:  0.361 [-17.227, 100.000], mean action: 1.645 [0.000, 3.000],  loss: 4.959605, mae: 27.209003, mean_q: 35.849327, mean_eps: 0.124905
 195653/200000: episode: 1091, duration: 6.733s, episode steps: 959, steps per second: 142, episode reward: 232.451, mean reward:  0.242 [-19.904, 100.000], mean action: 0.993 [0.000, 3.000],  loss: 6.517666, mae: 27.754232, mean_q: 36.773194, mean_eps: 0.121721
 196393/200000: episode: 1092, duration: 5.074s, episode steps: 740, steps per second: 146, episode reward: 208.395, mean reward:  0.282 [-19.849, 100.000], mean action: 1.372 [0.000, 3.000],  loss: 5.071834, mae: 28.106655, mean_q: 37.400301, mean_eps: 0.117899
 196993/200000: episode: 1093, duration: 4.289s, episode steps: 600, steps per second: 140, episode reward: 251.537, mean reward:  0.419 [-18.159, 100.000], mean action: 1.335 [0.000, 3.000],  loss: 4.278400, mae: 28.580576, mean_q: 38.073771, mean_eps: 0.114884
 197490/200000: episode: 1094, duration: 3.412s, episode steps: 497, steps per second: 146, episode reward: 212.317, mean reward:  0.427 [-10.660, 100.000], mean action: 1.761 [0.000, 3.000],  loss: 4.708675, mae: 28.752098, mean_q: 38.268849, mean_eps: 0.112415
 197787/200000: episode: 1095, duration: 1.938s, episode steps: 297, steps per second: 153, episode reward: 10.014, mean reward:  0.034 [-100.000, 11.674], mean action: 1.613 [0.000, 3.000],  loss: 4.670845, mae: 29.278379, mean_q: 38.954395, mean_eps: 0.110629
 198236/200000: episode: 1096, duration: 2.825s, episode steps: 449, steps per second: 159, episode reward: 192.589, mean reward:  0.429 [-8.136, 100.000], mean action: 1.490 [0.000, 3.000],  loss: 5.840603, mae: 30.043086, mean_q: 40.071428, mean_eps: 0.108950
 198660/200000: episode: 1097, duration: 2.727s, episode steps: 424, steps per second: 155, episode reward: 217.730, mean reward:  0.514 [-12.181, 100.000], mean action: 1.767 [0.000, 3.000],  loss: 5.158831, mae: 30.399316, mean_q: 40.473446, mean_eps: 0.106986
 199259/200000: episode: 1098, duration: 4.011s, episode steps: 599, steps per second: 149, episode reward: 233.027, mean reward:  0.389 [-18.318, 100.000], mean action: 1.462 [0.000, 3.000],  loss: 5.163187, mae: 30.367143, mean_q: 40.471909, mean_eps: 0.104684
 199780/200000: episode: 1099, duration: 3.351s, episode steps: 521, steps per second: 155, episode reward: -168.634, mean reward: -0.324 [-100.000, 12.343], mean action: 1.720 [0.000, 3.000],  loss: 5.804381, mae: 30.943977, mean_q: 41.344420, mean_eps: 0.102164
done, took 1411.136 seconds
Testing for 5 episodes ...
Episode 1: reward: 193.667, steps: 566
Episode 2: reward: -24.658, steps: 1000
Episode 3: reward: -138.550, steps: 271
Episode 4: reward: -116.925, steps: 307
Episode 5: reward: 139.153, steps: 779
Testing for 5 episodes ...
Episode 1: reward: 269.678, steps: 408
Episode 2: reward: 111.101, steps: 693
Episode 3: reward: -220.537, steps: 319
Episode 4: reward: 234.543, steps: 394
Episode 5: reward: -39.911, steps: 1000