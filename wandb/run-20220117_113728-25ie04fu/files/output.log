Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
Training for 150000 steps ...
    152/150000: episode: 1, duration: 0.192s, episode steps: 152, steps per second: 791, episode reward: -171.063, mean reward: -1.125 [-100.000,  7.699], mean action: 1.605 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    213/150000: episode: 2, duration: 0.052s, episode steps:  61, steps per second: 1183, episode reward: -68.347, mean reward: -1.120 [-100.000, 17.423], mean action: 1.574 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    298/150000: episode: 3, duration: 0.073s, episode steps:  85, steps per second: 1171, episode reward: -158.043, mean reward: -1.859 [-100.000, 14.285], mean action: 1.494 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    366/150000: episode: 4, duration: 0.054s, episode steps:  68, steps per second: 1270, episode reward: -108.628, mean reward: -1.597 [-100.000, 20.113], mean action: 1.574 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    452/150000: episode: 5, duration: 0.074s, episode steps:  86, steps per second: 1167, episode reward: -330.696, mean reward: -3.845 [-100.000,  2.616], mean action: 1.395 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    545/150000: episode: 6, duration: 0.073s, episode steps:  93, steps per second: 1269, episode reward: -113.676, mean reward: -1.222 [-100.000,  6.551], mean action: 1.624 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    639/150000: episode: 7, duration: 0.065s, episode steps:  94, steps per second: 1448, episode reward: -121.446, mean reward: -1.292 [-100.000,  9.081], mean action: 1.511 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    707/150000: episode: 8, duration: 0.046s, episode steps:  68, steps per second: 1466, episode reward: -199.946, mean reward: -2.940 [-100.000,  5.433], mean action: 1.471 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    772/150000: episode: 9, duration: 0.046s, episode steps:  65, steps per second: 1418, episode reward: -70.299, mean reward: -1.082 [-100.000, 10.992], mean action: 1.385 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
    880/150000: episode: 10, duration: 0.081s, episode steps: 108, steps per second: 1329, episode reward: -368.118, mean reward: -3.409 [-100.000,  5.485], mean action: 1.352 [0.000, 3.000],  loss: --, mae: --, mean_q: --, mean_eps: --
   1014/150000: episode: 11, duration: 0.719s, episode steps: 134, steps per second: 186, episode reward: 42.057, mean reward:  0.314 [-100.000, 109.217], mean action: 1.201 [0.000, 3.000],  loss: 63.850709, mae: 0.872025, mean_q: 0.194975, mean_eps: 0.993958
   1084/150000: episode: 12, duration: 0.394s, episode steps:  70, steps per second: 178, episode reward: -268.304, mean reward: -3.833 [-100.000,  3.319], mean action: 1.757 [0.000, 3.000],  loss: 53.401904, mae: 0.770665, mean_q: 0.162534, mean_eps: 0.993709
   1187/150000: episode: 13, duration: 0.590s, episode steps: 103, steps per second: 174, episode reward: -220.538, mean reward: -2.141 [-100.000,  1.277], mean action: 1.505 [0.000, 3.000],  loss: 64.753162, mae: 1.198984, mean_q: 0.157954, mean_eps: 0.993190
   1309/150000: episode: 14, duration: 0.705s, episode steps: 122, steps per second: 173, episode reward: -163.805, mean reward: -1.343 [-100.000,  5.357], mean action: 1.590 [0.000, 3.000],  loss: 41.162268, mae: 1.637105, mean_q: 0.444589, mean_eps: 0.992515
   1401/150000: episode: 15, duration: 0.512s, episode steps:  92, steps per second: 180, episode reward: -218.075, mean reward: -2.370 [-100.000,  7.379], mean action: 1.370 [0.000, 3.000],  loss: 36.494812, mae: 1.660799, mean_q: 0.646535, mean_eps: 0.991873
   1534/150000: episode: 16, duration: 0.726s, episode steps: 133, steps per second: 183, episode reward: -232.048, mean reward: -1.745 [-100.000, 16.766], mean action: 1.496 [0.000, 3.000],  loss: 41.311694, mae: 2.048868, mean_q: 0.941839, mean_eps: 0.991198
   1652/150000: episode: 17, duration: 0.667s, episode steps: 118, steps per second: 177, episode reward: -212.192, mean reward: -1.798 [-100.000,  9.782], mean action: 1.525 [0.000, 3.000],  loss: 38.014882, mae: 1.987743, mean_q: 0.713847, mean_eps: 0.990445
   1733/150000: episode: 18, duration: 0.447s, episode steps:  81, steps per second: 181, episode reward: -115.284, mean reward: -1.423 [-100.000,  9.081], mean action: 1.383 [0.000, 3.000],  loss: 35.215135, mae: 1.846702, mean_q: 0.912337, mean_eps: 0.989848
   1827/150000: episode: 19, duration: 0.509s, episode steps:  94, steps per second: 185, episode reward: -219.127, mean reward: -2.331 [-100.000, 21.391], mean action: 1.468 [0.000, 3.000],  loss: 34.519054, mae: 2.065117, mean_q: 0.633052, mean_eps: 0.989323
   1931/150000: episode: 20, duration: 0.588s, episode steps: 104, steps per second: 177, episode reward: -31.470, mean reward: -0.303 [-100.000, 97.713], mean action: 1.606 [0.000, 3.000],  loss: 39.889373, mae: 2.484550, mean_q: 0.073878, mean_eps: 0.988729
   1993/150000: episode: 21, duration: 0.397s, episode steps:  62, steps per second: 156, episode reward: -78.043, mean reward: -1.259 [-100.000, 12.567], mean action: 1.500 [0.000, 3.000],  loss: 31.518894, mae: 1.995812, mean_q: 0.299428, mean_eps: 0.988231
   2066/150000: episode: 22, duration: 0.459s, episode steps:  73, steps per second: 159, episode reward: -76.240, mean reward: -1.044 [-100.000, 20.501], mean action: 1.356 [0.000, 3.000],  loss: 20.674508, mae: 1.882353, mean_q: 0.669237, mean_eps: 0.987826
   2171/150000: episode: 23, duration: 0.612s, episode steps: 105, steps per second: 172, episode reward: -138.239, mean reward: -1.317 [-100.000, 15.193], mean action: 1.610 [0.000, 3.000],  loss: 13.288441, mae: 2.290068, mean_q: 0.753821, mean_eps: 0.987292
   2248/150000: episode: 24, duration: 0.456s, episode steps:  77, steps per second: 169, episode reward: -62.743, mean reward: -0.815 [-100.000, 77.345], mean action: 1.584 [0.000, 3.000],  loss: 20.760430, mae: 2.213723, mean_q: 1.175860, mean_eps: 0.986746
   2357/150000: episode: 25, duration: 0.658s, episode steps: 109, steps per second: 166, episode reward: -231.395, mean reward: -2.123 [-100.000, 24.876], mean action: 1.404 [0.000, 3.000],  loss: 31.279122, mae: 2.564418, mean_q: 1.090622, mean_eps: 0.986188
   2464/150000: episode: 26, duration: 0.594s, episode steps: 107, steps per second: 180, episode reward: -298.510, mean reward: -2.790 [-100.000,  1.022], mean action: 1.523 [0.000, 3.000],  loss: 34.356828, mae: 2.675098, mean_q: 1.024377, mean_eps: 0.985540
   2572/150000: episode: 27, duration: 0.597s, episode steps: 108, steps per second: 181, episode reward: -269.465, mean reward: -2.495 [-100.000,  1.042], mean action: 1.593 [0.000, 3.000],  loss: 30.995656, mae: 2.558498, mean_q: 0.833510, mean_eps: 0.984895
   2653/150000: episode: 28, duration: 0.471s, episode steps:  81, steps per second: 172, episode reward: -76.718, mean reward: -0.947 [-100.000, 12.051], mean action: 1.519 [0.000, 3.000],  loss: 24.992895, mae: 3.078804, mean_q: 0.503107, mean_eps: 0.984328
   2762/150000: episode: 29, duration: 0.816s, episode steps: 109, steps per second: 134, episode reward: -166.812, mean reward: -1.530 [-100.000,  8.326], mean action: 1.587 [0.000, 3.000],  loss: 26.050331, mae: 2.353522, mean_q: 0.920903, mean_eps: 0.983758
   2848/150000: episode: 30, duration: 0.577s, episode steps:  86, steps per second: 149, episode reward: -123.137, mean reward: -1.432 [-100.000,  7.652], mean action: 1.430 [0.000, 3.000],  loss: 28.055292, mae: 2.411455, mean_q: 1.045894, mean_eps: 0.983173
   2960/150000: episode: 31, duration: 0.729s, episode steps: 112, steps per second: 154, episode reward: -213.472, mean reward: -1.906 [-100.000,  2.070], mean action: 1.616 [0.000, 3.000],  loss: 17.043120, mae: 2.449613, mean_q: 0.742692, mean_eps: 0.982579
   3073/150000: episode: 32, duration: 0.849s, episode steps: 113, steps per second: 133, episode reward: -170.196, mean reward: -1.506 [-100.000,  3.066], mean action: 1.566 [0.000, 3.000],  loss: 26.601440, mae: 2.532596, mean_q: 1.633865, mean_eps: 0.981904
   3147/150000: episode: 33, duration: 0.563s, episode steps:  74, steps per second: 131, episode reward: -155.928, mean reward: -2.107 [-100.000, 10.592], mean action: 1.500 [0.000, 3.000],  loss: 20.548417, mae: 3.270346, mean_q: 1.396261, mean_eps: 0.981343
   3215/150000: episode: 34, duration: 0.472s, episode steps:  68, steps per second: 144, episode reward: -130.883, mean reward: -1.925 [-100.000, 31.641], mean action: 1.632 [0.000, 3.000],  loss: 22.126645, mae: 3.320540, mean_q: 1.482717, mean_eps: 0.980917
   3328/150000: episode: 35, duration: 0.710s, episode steps: 113, steps per second: 159, episode reward: -285.343, mean reward: -2.525 [-100.000, 88.625], mean action: 1.531 [0.000, 3.000],  loss: 19.996814, mae: 3.104754, mean_q: 1.358538, mean_eps: 0.980374
   3464/150000: episode: 36, duration: 0.776s, episode steps: 136, steps per second: 175, episode reward: -228.207, mean reward: -1.678 [-100.000, 11.227], mean action: 1.507 [0.000, 3.000],  loss: 25.454398, mae: 3.413998, mean_q: 1.066182, mean_eps: 0.979627
   3543/150000: episode: 37, duration: 0.446s, episode steps:  79, steps per second: 177, episode reward: -87.738, mean reward: -1.111 [-100.000, 14.244], mean action: 1.456 [0.000, 3.000],  loss: 21.260649, mae: 3.247808, mean_q: 1.092048, mean_eps: 0.978982
   3633/150000: episode: 38, duration: 0.502s, episode steps:  90, steps per second: 179, episode reward: -234.825, mean reward: -2.609 [-100.000,  4.388], mean action: 1.422 [0.000, 3.000],  loss: 23.673397, mae: 3.431328, mean_q: 1.199110, mean_eps: 0.978475
   3712/150000: episode: 39, duration: 0.478s, episode steps:  79, steps per second: 165, episode reward: -217.096, mean reward: -2.748 [-100.000, 28.611], mean action: 1.392 [0.000, 3.000],  loss: 17.255362, mae: 3.364742, mean_q: 1.135078, mean_eps: 0.977968
   3806/150000: episode: 40, duration: 0.512s, episode steps:  94, steps per second: 184, episode reward: -335.477, mean reward: -3.569 [-100.000, 18.665], mean action: 1.511 [0.000, 3.000],  loss: 24.818629, mae: 3.424743, mean_q: 0.942843, mean_eps: 0.977449
   3868/150000: episode: 41, duration: 0.331s, episode steps:  62, steps per second: 187, episode reward: -153.393, mean reward: -2.474 [-100.000, 30.688], mean action: 1.613 [0.000, 3.000],  loss: 21.081429, mae: 3.678885, mean_q: 0.751679, mean_eps: 0.976981
   3953/150000: episode: 42, duration: 0.464s, episode steps:  85, steps per second: 183, episode reward: -139.777, mean reward: -1.644 [-100.000,  7.064], mean action: 1.318 [0.000, 3.000],  loss: 33.394409, mae: 3.548631, mean_q: 0.806002, mean_eps: 0.976540
   4063/150000: episode: 43, duration: 0.633s, episode steps: 110, steps per second: 174, episode reward: -393.312, mean reward: -3.576 [-100.000,  4.418], mean action: 1.327 [0.000, 3.000],  loss: 32.548371, mae: 3.863406, mean_q: 1.397971, mean_eps: 0.975955
   4189/150000: episode: 44, duration: 0.712s, episode steps: 126, steps per second: 177, episode reward: -160.525, mean reward: -1.274 [-100.000, 14.384], mean action: 1.524 [0.000, 3.000],  loss: 18.619612, mae: 3.938240, mean_q: 1.768475, mean_eps: 0.975247
   4315/150000: episode: 45, duration: 0.741s, episode steps: 126, steps per second: 170, episode reward: -48.381, mean reward: -0.384 [-100.000,  6.943], mean action: 1.437 [0.000, 3.000],  loss: 24.257978, mae: 4.083682, mean_q: 1.808534, mean_eps: 0.974491
   4436/150000: episode: 46, duration: 0.748s, episode steps: 121, steps per second: 162, episode reward: -262.837, mean reward: -2.172 [-100.000, 83.136], mean action: 1.405 [0.000, 3.000],  loss: 17.360185, mae: 4.273231, mean_q: 1.538276, mean_eps: 0.973750
   4535/150000: episode: 47, duration: 0.569s, episode steps:  99, steps per second: 174, episode reward: -150.244, mean reward: -1.518 [-100.000,  6.681], mean action: 1.414 [0.000, 3.000],  loss: 25.504900, mae: 4.322726, mean_q: 1.478724, mean_eps: 0.973090
   4616/150000: episode: 48, duration: 0.479s, episode steps:  81, steps per second: 169, episode reward: -124.677, mean reward: -1.539 [-100.000, 11.613], mean action: 1.309 [0.000, 3.000],  loss: 18.534769, mae: 4.116748, mean_q: 1.480273, mean_eps: 0.972550
   4700/150000: episode: 49, duration: 0.497s, episode steps:  84, steps per second: 169, episode reward: -245.766, mean reward: -2.926 [-100.000,  7.988], mean action: 1.571 [0.000, 3.000],  loss: 27.415949, mae: 4.097530, mean_q: 1.442157, mean_eps: 0.972055
   4787/150000: episode: 50, duration: 0.537s, episode steps:  87, steps per second: 162, episode reward: -125.170, mean reward: -1.439 [-100.000, 11.972], mean action: 1.575 [0.000, 3.000],  loss: 19.070055, mae: 4.353469, mean_q: 1.757897, mean_eps: 0.971542
   4870/150000: episode: 51, duration: 0.455s, episode steps:  83, steps per second: 183, episode reward: -365.065, mean reward: -4.398 [-100.000, -0.087], mean action: 1.627 [0.000, 3.000],  loss: 26.488219, mae: 4.565522, mean_q: 1.454866, mean_eps: 0.971032
   4980/150000: episode: 52, duration: 0.646s, episode steps: 110, steps per second: 170, episode reward: -177.593, mean reward: -1.614 [-100.000,  1.681], mean action: 1.509 [0.000, 3.000],  loss: 19.843803, mae: 3.992285, mean_q: 1.919338, mean_eps: 0.970453
   5044/150000: episode: 53, duration: 0.416s, episode steps:  64, steps per second: 154, episode reward: -120.744, mean reward: -1.887 [-100.000,  8.140], mean action: 1.531 [0.000, 3.000],  loss: 25.133403, mae: 4.856771, mean_q: 1.385356, mean_eps: 0.969931
   5122/150000: episode: 54, duration: 0.431s, episode steps:  78, steps per second: 181, episode reward: -120.310, mean reward: -1.542 [-100.000, 10.049], mean action: 1.462 [0.000, 3.000],  loss: 18.868573, mae: 5.390783, mean_q: 1.831147, mean_eps: 0.969505
   5238/150000: episode: 55, duration: 0.686s, episode steps: 116, steps per second: 169, episode reward: -227.074, mean reward: -1.958 [-100.000,  3.166], mean action: 1.586 [0.000, 3.000],  loss: 25.240678, mae: 5.492196, mean_q: 2.286109, mean_eps: 0.968923
   5305/150000: episode: 56, duration: 0.392s, episode steps:  67, steps per second: 171, episode reward: -117.280, mean reward: -1.750 [-100.000,  7.321], mean action: 1.448 [0.000, 3.000],  loss: 27.123210, mae: 5.625643, mean_q: 1.711375, mean_eps: 0.968374
   5395/150000: episode: 57, duration: 0.497s, episode steps:  90, steps per second: 181, episode reward: -279.280, mean reward: -3.103 [-100.000,  5.736], mean action: 1.678 [0.000, 3.000],  loss: 31.387446, mae: 5.746787, mean_q: 1.972384, mean_eps: 0.967903
   5504/150000: episode: 58, duration: 0.636s, episode steps: 109, steps per second: 172, episode reward: -75.921, mean reward: -0.697 [-100.000, 19.115], mean action: 1.569 [0.000, 3.000],  loss: 18.397378, mae: 5.480513, mean_q: 2.071052, mean_eps: 0.967306
   5592/150000: episode: 59, duration: 0.527s, episode steps:  88, steps per second: 167, episode reward: -238.388, mean reward: -2.709 [-100.000, 28.883], mean action: 1.318 [0.000, 3.000],  loss: 29.408491, mae: 5.444376, mean_q: 2.384514, mean_eps: 0.966715
   5677/150000: episode: 60, duration: 0.482s, episode steps:  85, steps per second: 176, episode reward: -78.523, mean reward: -0.924 [-100.000,  8.118], mean action: 1.376 [0.000, 3.000],  loss: 20.617702, mae: 5.903515, mean_q: 1.805561, mean_eps: 0.966196
   5750/150000: episode: 61, duration: 0.410s, episode steps:  73, steps per second: 178, episode reward: -78.867, mean reward: -1.080 [-100.000,  7.868], mean action: 1.575 [0.000, 3.000],  loss: 21.250911, mae: 5.398071, mean_q: 2.207962, mean_eps: 0.965722
   5857/150000: episode: 62, duration: 0.615s, episode steps: 107, steps per second: 174, episode reward: -131.430, mean reward: -1.228 [-100.000, 14.289], mean action: 1.645 [0.000, 3.000],  loss: 25.745156, mae: 5.357143, mean_q: 2.489862, mean_eps: 0.965182
   5916/150000: episode: 63, duration: 0.419s, episode steps:  59, steps per second: 141, episode reward: -133.361, mean reward: -2.260 [-100.000,  7.175], mean action: 1.542 [0.000, 3.000],  loss: 19.048164, mae: 5.643355, mean_q: 2.193421, mean_eps: 0.964684
   6017/150000: episode: 64, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: -92.191, mean reward: -0.913 [-100.000, 51.001], mean action: 1.861 [0.000, 3.000],  loss: 13.031588, mae: 5.613644, mean_q: 2.163033, mean_eps: 0.964204
   6122/150000: episode: 65, duration: 0.599s, episode steps: 105, steps per second: 175, episode reward: -376.693, mean reward: -3.588 [-100.000,  5.705], mean action: 1.505 [0.000, 3.000],  loss: 25.120673, mae: 6.839076, mean_q: 1.482669, mean_eps: 0.963586
   6211/150000: episode: 66, duration: 0.484s, episode steps:  89, steps per second: 184, episode reward: -161.649, mean reward: -1.816 [-100.000, 27.764], mean action: 1.596 [0.000, 3.000],  loss: 10.620004, mae: 6.394332, mean_q: 2.107296, mean_eps: 0.963004
   6312/150000: episode: 67, duration: 0.585s, episode steps: 101, steps per second: 173, episode reward: -122.243, mean reward: -1.210 [-100.000, 15.172], mean action: 1.386 [0.000, 3.000],  loss: 12.377425, mae: 6.731112, mean_q: 1.770014, mean_eps: 0.962434
   6441/150000: episode: 68, duration: 0.701s, episode steps: 129, steps per second: 184, episode reward: -159.838, mean reward: -1.239 [-100.000,  6.419], mean action: 1.481 [0.000, 3.000],  loss: 14.427906, mae: 6.578403, mean_q: 2.104166, mean_eps: 0.961744
   6513/150000: episode: 69, duration: 0.399s, episode steps:  72, steps per second: 181, episode reward: -82.663, mean reward: -1.148 [-100.000, 51.001], mean action: 1.528 [0.000, 3.000],  loss: 20.323779, mae: 6.936555, mean_q: 1.786736, mean_eps: 0.961141
   6608/150000: episode: 70, duration: 0.521s, episode steps:  95, steps per second: 182, episode reward: -191.443, mean reward: -2.015 [-100.000, 13.973], mean action: 1.547 [0.000, 3.000],  loss: 16.175606, mae: 6.381080, mean_q: 2.389815, mean_eps: 0.960640
   6709/150000: episode: 71, duration: 0.592s, episode steps: 101, steps per second: 171, episode reward: -280.011, mean reward: -2.772 [-100.000, 18.243], mean action: 1.634 [0.000, 3.000],  loss: 18.721135, mae: 6.950134, mean_q: 1.555920, mean_eps: 0.960052
   6840/150000: episode: 72, duration: 0.710s, episode steps: 131, steps per second: 184, episode reward: -397.184, mean reward: -3.032 [-100.000, 98.991], mean action: 1.290 [0.000, 3.000],  loss: 18.344690, mae: 6.483868, mean_q: 2.015066, mean_eps: 0.959356
   6920/150000: episode: 73, duration: 0.442s, episode steps:  80, steps per second: 181, episode reward: -63.352, mean reward: -0.792 [-100.000, 12.758], mean action: 1.587 [0.000, 3.000],  loss: 17.579677, mae: 6.871062, mean_q: 2.301468, mean_eps: 0.958723
   7020/150000: episode: 74, duration: 0.546s, episode steps: 100, steps per second: 183, episode reward: -245.039, mean reward: -2.450 [-100.000,  6.603], mean action: 1.430 [0.000, 3.000],  loss: 17.244043, mae: 7.079177, mean_q: 1.764618, mean_eps: 0.958183
   8020/150000: episode: 75, duration: 6.421s, episode steps: 1000, steps per second: 156, episode reward: 58.063, mean reward:  0.058 [-24.198, 118.046], mean action: 1.573 [0.000, 3.000],  loss: 27.539974, mae: 9.769578, mean_q: -0.721185, mean_eps: 0.954883
   8145/150000: episode: 76, duration: 0.798s, episode steps: 125, steps per second: 157, episode reward: -53.170, mean reward: -0.425 [-100.000, 88.840], mean action: 1.512 [0.000, 3.000],  loss: 22.569133, mae: 11.933569, mean_q: -3.218516, mean_eps: 0.951508
   8257/150000: episode: 77, duration: 0.622s, episode steps: 112, steps per second: 180, episode reward: -131.530, mean reward: -1.174 [-100.000, 11.089], mean action: 1.429 [0.000, 3.000],  loss: 23.479315, mae: 11.469019, mean_q: -2.944693, mean_eps: 0.950797
   8322/150000: episode: 78, duration: 0.365s, episode steps:  65, steps per second: 178, episode reward: -113.231, mean reward: -1.742 [-100.000, 19.505], mean action: 1.492 [0.000, 3.000],  loss: 29.010776, mae: 10.935581, mean_q: -1.541330, mean_eps: 0.950266
   8404/150000: episode: 79, duration: 0.460s, episode steps:  82, steps per second: 178, episode reward: -165.070, mean reward: -2.013 [-100.000,  5.812], mean action: 1.451 [0.000, 3.000],  loss: 42.237233, mae: 11.875846, mean_q: -2.935142, mean_eps: 0.949825
   8524/150000: episode: 80, duration: 0.726s, episode steps: 120, steps per second: 165, episode reward: -92.246, mean reward: -0.769 [-100.000, 56.986], mean action: 1.517 [0.000, 3.000],  loss: 19.724139, mae: 11.213964, mean_q: -2.293433, mean_eps: 0.949219
   8640/150000: episode: 81, duration: 0.639s, episode steps: 116, steps per second: 182, episode reward: -144.686, mean reward: -1.247 [-100.000,  5.900], mean action: 1.457 [0.000, 3.000],  loss: 20.000526, mae: 11.341490, mean_q: -2.507988, mean_eps: 0.948511
   8718/150000: episode: 82, duration: 0.452s, episode steps:  78, steps per second: 173, episode reward: -96.286, mean reward: -1.234 [-100.000,  6.866], mean action: 1.641 [0.000, 3.000],  loss: 25.798700, mae: 11.694240, mean_q: -3.302175, mean_eps: 0.947929
   8828/150000: episode: 83, duration: 0.651s, episode steps: 110, steps per second: 169, episode reward: -101.607, mean reward: -0.924 [-100.000,  5.371], mean action: 1.436 [0.000, 3.000],  loss: 21.478798, mae: 11.615551, mean_q: -2.221453, mean_eps: 0.947365
   8893/150000: episode: 84, duration: 0.410s, episode steps:  65, steps per second: 158, episode reward: -73.503, mean reward: -1.131 [-100.000, 10.786], mean action: 1.138 [0.000, 3.000],  loss: 32.128828, mae: 11.553109, mean_q: -2.154704, mean_eps: 0.946840
   8962/150000: episode: 85, duration: 0.416s, episode steps:  69, steps per second: 166, episode reward: -117.350, mean reward: -1.701 [-100.000,  4.145], mean action: 1.536 [0.000, 3.000],  loss: 14.349663, mae: 11.479415, mean_q: -2.211858, mean_eps: 0.946438
   9032/150000: episode: 86, duration: 0.429s, episode steps:  70, steps per second: 163, episode reward: -118.655, mean reward: -1.695 [-100.000,  9.704], mean action: 1.429 [0.000, 3.000],  loss: 38.387491, mae: 11.250680, mean_q: -1.148183, mean_eps: 0.946021
   9121/150000: episode: 87, duration: 0.553s, episode steps:  89, steps per second: 161, episode reward: -110.440, mean reward: -1.241 [-100.000,  4.719], mean action: 1.685 [0.000, 3.000],  loss: 24.599987, mae: 11.154390, mean_q: 0.033051, mean_eps: 0.945544
   9220/150000: episode: 88, duration: 0.645s, episode steps:  99, steps per second: 153, episode reward: -189.762, mean reward: -1.917 [-100.000,  1.226], mean action: 1.424 [0.000, 3.000],  loss: 19.844163, mae: 11.822851, mean_q: -0.994436, mean_eps: 0.944980
   9318/150000: episode: 89, duration: 0.573s, episode steps:  98, steps per second: 171, episode reward: -207.055, mean reward: -2.113 [-100.000,  8.814], mean action: 1.469 [0.000, 3.000],  loss: 15.423910, mae: 11.660392, mean_q: -0.735169, mean_eps: 0.944389
   9429/150000: episode: 90, duration: 0.664s, episode steps: 111, steps per second: 167, episode reward: -11.561, mean reward: -0.104 [-100.000, 74.998], mean action: 1.360 [0.000, 3.000],  loss: 10.105850, mae: 11.498554, mean_q: -1.009744, mean_eps: 0.943762
   9526/150000: episode: 91, duration: 0.630s, episode steps:  97, steps per second: 154, episode reward: -419.887, mean reward: -4.329 [-100.000, -0.254], mean action: 1.495 [0.000, 3.000],  loss: 22.858315, mae: 11.032826, mean_q: 0.054217, mean_eps: 0.943138
   9599/150000: episode: 92, duration: 0.453s, episode steps:  73, steps per second: 161, episode reward: -36.815, mean reward: -0.504 [-100.000, 13.015], mean action: 1.589 [0.000, 3.000],  loss: 20.928711, mae: 10.855790, mean_q: 0.604819, mean_eps: 0.942628
   9670/150000: episode: 93, duration: 0.423s, episode steps:  71, steps per second: 168, episode reward: -100.162, mean reward: -1.411 [-100.000,  9.989], mean action: 1.338 [0.000, 3.000],  loss: 18.356570, mae: 11.637900, mean_q: 0.104330, mean_eps: 0.942196
   9766/150000: episode: 94, duration: 0.552s, episode steps:  96, steps per second: 174, episode reward: -128.564, mean reward: -1.339 [-100.000, 12.172], mean action: 1.562 [0.000, 3.000],  loss: 13.895535, mae: 11.276819, mean_q: -0.276342, mean_eps: 0.941695
   9883/150000: episode: 95, duration: 0.710s, episode steps: 117, steps per second: 165, episode reward: -140.429, mean reward: -1.200 [-100.000, 16.024], mean action: 1.658 [0.000, 3.000],  loss: 22.334592, mae: 11.528873, mean_q: 0.093475, mean_eps: 0.941056
   9981/150000: episode: 96, duration: 0.625s, episode steps:  98, steps per second: 157, episode reward: -90.904, mean reward: -0.928 [-100.000, 11.288], mean action: 1.469 [0.000, 3.000],  loss: 15.573123, mae: 10.994494, mean_q: 0.643076, mean_eps: 0.940411
  10077/150000: episode: 97, duration: 0.549s, episode steps:  96, steps per second: 175, episode reward: -151.026, mean reward: -1.573 [-100.000,  7.729], mean action: 1.542 [0.000, 3.000],  loss: 13.827311, mae: 12.015244, mean_q: 0.200144, mean_eps: 0.939829
  10159/150000: episode: 98, duration: 0.461s, episode steps:  82, steps per second: 178, episode reward: -105.182, mean reward: -1.283 [-100.000,  7.432], mean action: 1.549 [0.000, 3.000],  loss: 21.173633, mae: 12.269807, mean_q: -0.077839, mean_eps: 0.939295
  10273/150000: episode: 99, duration: 0.673s, episode steps: 114, steps per second: 169, episode reward: -63.592, mean reward: -0.558 [-100.000, 17.754], mean action: 1.702 [0.000, 3.000],  loss: 23.599300, mae: 12.094868, mean_q: 0.564405, mean_eps: 0.938707
  10380/150000: episode: 100, duration: 0.621s, episode steps: 107, steps per second: 172, episode reward: -96.662, mean reward: -0.903 [-100.000, 11.442], mean action: 1.598 [0.000, 3.000],  loss: 16.333176, mae: 12.594980, mean_q: 0.003224, mean_eps: 0.938044
  10446/150000: episode: 101, duration: 0.375s, episode steps:  66, steps per second: 176, episode reward: -85.351, mean reward: -1.293 [-100.000,  4.598], mean action: 1.409 [0.000, 3.000],  loss: 21.792487, mae: 12.453916, mean_q: 0.252435, mean_eps: 0.937525
  10573/150000: episode: 102, duration: 0.735s, episode steps: 127, steps per second: 173, episode reward: -96.955, mean reward: -0.763 [-100.000,  7.437], mean action: 1.433 [0.000, 3.000],  loss: 24.591236, mae: 12.490336, mean_q: -0.010466, mean_eps: 0.936946
  10674/150000: episode: 103, duration: 0.606s, episode steps: 101, steps per second: 167, episode reward: -102.274, mean reward: -1.013 [-100.000,  9.532], mean action: 1.317 [0.000, 3.000],  loss: 16.519264, mae: 12.047601, mean_q: 0.910822, mean_eps: 0.936262
  10784/150000: episode: 104, duration: 0.604s, episode steps: 110, steps per second: 182, episode reward: -244.278, mean reward: -2.221 [-100.000,  6.476], mean action: 1.682 [0.000, 3.000],  loss: 17.535024, mae: 12.136998, mean_q: 1.270107, mean_eps: 0.935629
  10862/150000: episode: 105, duration: 0.436s, episode steps:  78, steps per second: 179, episode reward: -87.763, mean reward: -1.125 [-100.000,  7.972], mean action: 1.487 [0.000, 3.000],  loss: 12.711276, mae: 12.449620, mean_q: 1.016270, mean_eps: 0.935065
  10956/150000: episode: 106, duration: 0.527s, episode steps:  94, steps per second: 178, episode reward: -121.124, mean reward: -1.289 [-100.000, 10.232], mean action: 1.521 [0.000, 3.000],  loss: 17.011364, mae: 12.248974, mean_q: 0.622870, mean_eps: 0.934549
  11044/150000: episode: 107, duration: 0.527s, episode steps:  88, steps per second: 167, episode reward: -129.690, mean reward: -1.474 [-100.000,  7.045], mean action: 1.545 [0.000, 3.000],  loss: 19.940226, mae: 12.490777, mean_q: 1.044567, mean_eps: 0.934003
  11163/150000: episode: 108, duration: 0.656s, episode steps: 119, steps per second: 181, episode reward: -201.708, mean reward: -1.695 [-100.000, 15.015], mean action: 1.571 [0.000, 3.000],  loss: 16.354267, mae: 13.275126, mean_q: 2.032607, mean_eps: 0.933382
  11231/150000: episode: 109, duration: 0.368s, episode steps:  68, steps per second: 185, episode reward: -89.686, mean reward: -1.319 [-100.000, 12.321], mean action: 1.632 [0.000, 3.000],  loss: 19.903494, mae: 13.100663, mean_q: 2.112668, mean_eps: 0.932821
  11312/150000: episode: 110, duration: 0.469s, episode steps:  81, steps per second: 173, episode reward: -77.806, mean reward: -0.961 [-100.000,  6.001], mean action: 1.593 [0.000, 3.000],  loss: 11.686367, mae: 13.338132, mean_q: 1.433388, mean_eps: 0.932374
  11379/150000: episode: 111, duration: 0.511s, episode steps:  67, steps per second: 131, episode reward: -66.937, mean reward: -0.999 [-100.000,  7.932], mean action: 1.657 [0.000, 3.000],  loss: 24.158671, mae: 13.718152, mean_q: 1.408809, mean_eps: 0.931930
  11496/150000: episode: 112, duration: 0.746s, episode steps: 117, steps per second: 157, episode reward: -209.762, mean reward: -1.793 [-100.000,  4.634], mean action: 1.308 [0.000, 3.000],  loss: 13.810525, mae: 13.488534, mean_q: 1.000270, mean_eps: 0.931378
  11584/150000: episode: 113, duration: 0.533s, episode steps:  88, steps per second: 165, episode reward: -112.072, mean reward: -1.274 [-100.000, 14.668], mean action: 1.545 [0.000, 3.000],  loss: 16.210210, mae: 12.680106, mean_q: 1.833093, mean_eps: 0.930763
  11688/150000: episode: 114, duration: 0.641s, episode steps: 104, steps per second: 162, episode reward: -147.340, mean reward: -1.417 [-100.000,  6.277], mean action: 1.317 [0.000, 3.000],  loss: 13.549188, mae: 13.625067, mean_q: 1.277606, mean_eps: 0.930187
  11760/150000: episode: 115, duration: 0.468s, episode steps:  72, steps per second: 154, episode reward: -66.975, mean reward: -0.930 [-100.000, 12.792], mean action: 1.403 [0.000, 3.000],  loss: 14.427259, mae: 13.581306, mean_q: 1.506810, mean_eps: 0.929659
  11891/150000: episode: 116, duration: 0.780s, episode steps: 131, steps per second: 168, episode reward: -276.911, mean reward: -2.114 [-100.000, 15.096], mean action: 1.458 [0.000, 3.000],  loss: 20.593386, mae: 13.611448, mean_q: 1.733813, mean_eps: 0.929050
  11993/150000: episode: 117, duration: 0.559s, episode steps: 102, steps per second: 182, episode reward: -132.181, mean reward: -1.296 [-100.000, 36.996], mean action: 1.255 [0.000, 3.000],  loss: 11.488367, mae: 13.246175, mean_q: 2.113038, mean_eps: 0.928351
  12069/150000: episode: 118, duration: 0.455s, episode steps:  76, steps per second: 167, episode reward: -135.625, mean reward: -1.785 [-100.000,  3.775], mean action: 1.592 [0.000, 3.000],  loss: 25.758865, mae: 14.318652, mean_q: 2.328110, mean_eps: 0.927817
  12159/150000: episode: 119, duration: 0.514s, episode steps:  90, steps per second: 175, episode reward: -357.642, mean reward: -3.974 [-100.000,  6.477], mean action: 1.478 [0.000, 3.000],  loss: 8.827431, mae: 14.098382, mean_q: 2.371935, mean_eps: 0.927319
  12301/150000: episode: 120, duration: 0.780s, episode steps: 142, steps per second: 182, episode reward: -180.840, mean reward: -1.274 [-100.000,  9.537], mean action: 1.521 [0.000, 3.000],  loss: 15.380480, mae: 13.806793, mean_q: 2.341697, mean_eps: 0.926623
  12369/150000: episode: 121, duration: 0.374s, episode steps:  68, steps per second: 182, episode reward: -228.810, mean reward: -3.365 [-100.000, 36.390], mean action: 1.412 [0.000, 3.000],  loss: 10.857664, mae: 14.025651, mean_q: 2.928923, mean_eps: 0.925993
  12473/150000: episode: 122, duration: 0.600s, episode steps: 104, steps per second: 173, episode reward: -117.281, mean reward: -1.128 [-100.000, 11.468], mean action: 1.683 [0.000, 3.000],  loss: 13.100045, mae: 14.166839, mean_q: 1.481168, mean_eps: 0.925477
  12607/150000: episode: 123, duration: 0.780s, episode steps: 134, steps per second: 172, episode reward: -145.502, mean reward: -1.086 [-100.000,  5.210], mean action: 1.448 [0.000, 3.000],  loss: 11.496131, mae: 13.837504, mean_q: 2.141275, mean_eps: 0.924763
  12702/150000: episode: 124, duration: 0.522s, episode steps:  95, steps per second: 182, episode reward: -174.851, mean reward: -1.841 [-100.000, 14.549], mean action: 1.600 [0.000, 3.000],  loss: 13.532145, mae: 14.089490, mean_q: 2.039267, mean_eps: 0.924076
  12794/150000: episode: 125, duration: 0.506s, episode steps:  92, steps per second: 182, episode reward: -225.335, mean reward: -2.449 [-100.000, 29.312], mean action: 1.402 [0.000, 3.000],  loss: 30.093274, mae: 14.248578, mean_q: 2.288988, mean_eps: 0.923515
  12858/150000: episode: 126, duration: 0.386s, episode steps:  64, steps per second: 166, episode reward: -71.800, mean reward: -1.122 [-100.000,  7.730], mean action: 1.547 [0.000, 3.000],  loss: 12.676504, mae: 14.477235, mean_q: 2.439576, mean_eps: 0.923047
  12967/150000: episode: 127, duration: 0.621s, episode steps: 109, steps per second: 176, episode reward: -223.526, mean reward: -2.051 [-100.000,  1.052], mean action: 1.642 [0.000, 3.000],  loss: 9.937737, mae: 14.287287, mean_q: 3.245019, mean_eps: 0.922528
  13024/150000: episode: 128, duration: 0.320s, episode steps:  57, steps per second: 178, episode reward: -78.364, mean reward: -1.375 [-100.000, 11.166], mean action: 1.860 [0.000, 3.000],  loss: 20.653201, mae: 14.278109, mean_q: 1.932235, mean_eps: 0.922030
  13110/150000: episode: 129, duration: 0.487s, episode steps:  86, steps per second: 177, episode reward: -83.055, mean reward: -0.966 [-100.000, 15.674], mean action: 1.698 [0.000, 3.000],  loss: 8.868133, mae: 13.290357, mean_q: 2.248762, mean_eps: 0.921601
  13171/150000: episode: 130, duration: 0.334s, episode steps:  61, steps per second: 182, episode reward: -133.170, mean reward: -2.183 [-100.000,  7.293], mean action: 1.443 [0.000, 3.000],  loss: 14.512384, mae: 13.264032, mean_q: 3.075685, mean_eps: 0.921160
  13275/150000: episode: 131, duration: 0.626s, episode steps: 104, steps per second: 166, episode reward: -223.950, mean reward: -2.153 [-100.000, 19.292], mean action: 1.577 [0.000, 3.000],  loss: 9.941749, mae: 14.081649, mean_q: 2.303853, mean_eps: 0.920665
  13384/150000: episode: 132, duration: 0.609s, episode steps: 109, steps per second: 179, episode reward: -200.502, mean reward: -1.839 [-100.000,  7.645], mean action: 1.560 [0.000, 3.000],  loss: 11.521224, mae: 13.571988, mean_q: 2.177952, mean_eps: 0.920026
  13453/150000: episode: 133, duration: 0.395s, episode steps:  69, steps per second: 175, episode reward: -170.804, mean reward: -2.475 [-100.000,  7.079], mean action: 1.522 [0.000, 3.000],  loss: 14.378199, mae: 13.565583, mean_q: 1.391882, mean_eps: 0.919492
  13535/150000: episode: 134, duration: 0.450s, episode steps:  82, steps per second: 182, episode reward: -101.136, mean reward: -1.233 [-100.000,  6.374], mean action: 1.707 [0.000, 3.000],  loss: 9.102644, mae: 13.652302, mean_q: 1.871450, mean_eps: 0.919039
  13617/150000: episode: 135, duration: 0.482s, episode steps:  82, steps per second: 170, episode reward: -40.499, mean reward: -0.494 [-100.000,  7.439], mean action: 1.488 [0.000, 3.000],  loss: 8.776470, mae: 13.695708, mean_q: 1.543409, mean_eps: 0.918547
  13740/150000: episode: 136, duration: 0.691s, episode steps: 123, steps per second: 178, episode reward: -212.435, mean reward: -1.727 [-100.000, 12.019], mean action: 1.358 [0.000, 3.000],  loss: 8.100471, mae: 13.487742, mean_q: 2.113248, mean_eps: 0.917932
  13814/150000: episode: 137, duration: 0.464s, episode steps:  74, steps per second: 160, episode reward: -90.543, mean reward: -1.224 [-100.000, 10.868], mean action: 1.568 [0.000, 3.000],  loss: 12.853852, mae: 13.311188, mean_q: 2.793235, mean_eps: 0.917341
  13910/150000: episode: 138, duration: 0.551s, episode steps:  96, steps per second: 174, episode reward: -144.134, mean reward: -1.501 [-100.000, 19.308], mean action: 1.510 [0.000, 3.000],  loss: 17.453617, mae: 14.160127, mean_q: 2.147706, mean_eps: 0.916831
  14000/150000: episode: 139, duration: 0.577s, episode steps:  90, steps per second: 156, episode reward: -269.497, mean reward: -2.994 [-100.000,  1.115], mean action: 1.578 [0.000, 3.000],  loss: 15.905653, mae: 14.155130, mean_q: 1.770404, mean_eps: 0.916273
  14082/150000: episode: 140, duration: 0.495s, episode steps:  82, steps per second: 166, episode reward: -73.375, mean reward: -0.895 [-100.000,  7.027], mean action: 1.476 [0.000, 3.000],  loss: 15.931393, mae: 14.554098, mean_q: 2.835129, mean_eps: 0.915757
  14204/150000: episode: 141, duration: 0.691s, episode steps: 122, steps per second: 176, episode reward: -173.149, mean reward: -1.419 [-100.000,  9.271], mean action: 1.443 [0.000, 3.000],  loss: 15.584088, mae: 14.934400, mean_q: 2.836902, mean_eps: 0.915145
  14310/150000: episode: 142, duration: 0.597s, episode steps: 106, steps per second: 178, episode reward: -293.468, mean reward: -2.769 [-100.000, 122.417], mean action: 1.453 [0.000, 3.000],  loss: 9.027769, mae: 13.907244, mean_q: 3.323542, mean_eps: 0.914461
  14382/150000: episode: 143, duration: 0.439s, episode steps:  72, steps per second: 164, episode reward: -139.447, mean reward: -1.937 [-100.000, 39.973], mean action: 1.625 [0.000, 3.000],  loss: 20.786692, mae: 14.478686, mean_q: 2.837438, mean_eps: 0.913927
  14453/150000: episode: 144, duration: 0.412s, episode steps:  71, steps per second: 172, episode reward: -100.428, mean reward: -1.414 [-100.000,  6.962], mean action: 1.408 [0.000, 3.000],  loss: 9.861771, mae: 14.889314, mean_q: 2.515516, mean_eps: 0.913498
  14549/150000: episode: 145, duration: 0.541s, episode steps:  96, steps per second: 177, episode reward: -189.111, mean reward: -1.970 [-100.000, 11.833], mean action: 1.521 [0.000, 3.000],  loss: 14.959615, mae: 13.983701, mean_q: 3.145197, mean_eps: 0.912997
  14626/150000: episode: 146, duration: 0.443s, episode steps:  77, steps per second: 174, episode reward: -89.263, mean reward: -1.159 [-100.000,  8.463], mean action: 1.649 [0.000, 3.000],  loss: 11.403198, mae: 14.441706, mean_q: 3.278561, mean_eps: 0.912478
  14690/150000: episode: 147, duration: 0.405s, episode steps:  64, steps per second: 158, episode reward: -120.981, mean reward: -1.890 [-100.000, 12.597], mean action: 1.750 [0.000, 3.000],  loss: 20.534771, mae: 14.000878, mean_q: 3.531822, mean_eps: 0.912055
  14804/150000: episode: 148, duration: 0.677s, episode steps: 114, steps per second: 168, episode reward: -191.355, mean reward: -1.679 [-100.000, 14.778], mean action: 1.640 [0.000, 3.000],  loss: 7.811378, mae: 14.908682, mean_q: 2.559633, mean_eps: 0.911521
  14944/150000: episode: 149, duration: 0.874s, episode steps: 140, steps per second: 160, episode reward: -188.693, mean reward: -1.348 [-100.000, 27.663], mean action: 1.657 [0.000, 3.000],  loss: 7.281233, mae: 14.579619, mean_q: 2.779651, mean_eps: 0.910759
  15036/150000: episode: 150, duration: 0.567s, episode steps:  92, steps per second: 162, episode reward: -121.598, mean reward: -1.322 [-100.000, 10.530], mean action: 1.511 [0.000, 3.000],  loss: 17.543560, mae: 14.625858, mean_q: 3.346167, mean_eps: 0.910063
  15149/150000: episode: 151, duration: 0.674s, episode steps: 113, steps per second: 168, episode reward: -118.759, mean reward: -1.051 [-100.000, 17.316], mean action: 1.549 [0.000, 3.000],  loss: 14.504961, mae: 14.986332, mean_q: 4.466871, mean_eps: 0.909448
  15264/150000: episode: 152, duration: 0.717s, episode steps: 115, steps per second: 160, episode reward: -95.205, mean reward: -0.828 [-100.000,  9.551], mean action: 1.522 [0.000, 3.000],  loss: 22.473980, mae: 14.917165, mean_q: 3.744044, mean_eps: 0.908764
  15373/150000: episode: 153, duration: 0.736s, episode steps: 109, steps per second: 148, episode reward: -332.435, mean reward: -3.050 [-100.000,  2.289], mean action: 1.743 [0.000, 3.000],  loss: 11.830856, mae: 15.052987, mean_q: 4.438604, mean_eps: 0.908092
  15452/150000: episode: 154, duration: 0.590s, episode steps:  79, steps per second: 134, episode reward: -106.083, mean reward: -1.343 [-100.000,  5.855], mean action: 1.405 [0.000, 3.000],  loss: 29.191936, mae: 15.476371, mean_q: 3.758110, mean_eps: 0.907528
  15551/150000: episode: 155, duration: 0.755s, episode steps:  99, steps per second: 131, episode reward: -198.694, mean reward: -2.007 [-100.000,  6.782], mean action: 1.535 [0.000, 3.000],  loss: 8.815604, mae: 14.807569, mean_q: 3.379827, mean_eps: 0.906994
  15630/150000: episode: 156, duration: 0.567s, episode steps:  79, steps per second: 139, episode reward: -182.151, mean reward: -2.306 [-100.000,  6.970], mean action: 1.608 [0.000, 3.000],  loss: 25.983158, mae: 14.542838, mean_q: 5.207985, mean_eps: 0.906460
  15700/150000: episode: 157, duration: 0.614s, episode steps:  70, steps per second: 114, episode reward: -95.270, mean reward: -1.361 [-100.000,  6.042], mean action: 1.557 [0.000, 3.000],  loss: 13.905497, mae: 14.897275, mean_q: 4.070708, mean_eps: 0.906013
  15779/150000: episode: 158, duration: 0.553s, episode steps:  79, steps per second: 143, episode reward: -199.540, mean reward: -2.526 [-100.000, 21.420], mean action: 1.671 [0.000, 3.000],  loss: 17.924689, mae: 14.924471, mean_q: 4.213607, mean_eps: 0.905566
  15873/150000: episode: 159, duration: 0.626s, episode steps:  94, steps per second: 150, episode reward: -151.209, mean reward: -1.609 [-100.000,  6.316], mean action: 1.457 [0.000, 3.000],  loss: 18.187940, mae: 14.591491, mean_q: 4.667349, mean_eps: 0.905047
  15978/150000: episode: 160, duration: 0.743s, episode steps: 105, steps per second: 141, episode reward: -95.440, mean reward: -0.909 [-100.000,  6.716], mean action: 1.562 [0.000, 3.000],  loss: 13.468474, mae: 14.694595, mean_q: 4.674726, mean_eps: 0.904450
  16089/150000: episode: 161, duration: 0.792s, episode steps: 111, steps per second: 140, episode reward: -124.009, mean reward: -1.117 [-100.000, 11.068], mean action: 1.459 [0.000, 3.000],  loss: 16.275510, mae: 15.492467, mean_q: 5.877660, mean_eps: 0.903802
  16225/150000: episode: 162, duration: 0.934s, episode steps: 136, steps per second: 146, episode reward: -112.847, mean reward: -0.830 [-100.000,  6.055], mean action: 1.434 [0.000, 3.000],  loss: 18.269935, mae: 15.773637, mean_q: 5.978497, mean_eps: 0.903061
  16313/150000: episode: 163, duration: 0.767s, episode steps:  88, steps per second: 115, episode reward: -134.580, mean reward: -1.529 [-100.000, 12.137], mean action: 1.750 [0.000, 3.000],  loss: 15.734853, mae: 15.618670, mean_q: 6.317569, mean_eps: 0.902389
  16375/150000: episode: 164, duration: 0.539s, episode steps:  62, steps per second: 115, episode reward: -45.348, mean reward: -0.731 [-100.000, 14.006], mean action: 1.597 [0.000, 3.000],  loss: 22.457372, mae: 15.270098, mean_q: 6.308367, mean_eps: 0.901939
  16484/150000: episode: 165, duration: 0.830s, episode steps: 109, steps per second: 131, episode reward: -334.871, mean reward: -3.072 [-100.000,  5.139], mean action: 1.807 [0.000, 3.000],  loss: 11.565075, mae: 16.153342, mean_q: 5.959374, mean_eps: 0.901426
  16582/150000: episode: 166, duration: 0.660s, episode steps:  98, steps per second: 148, episode reward: -181.380, mean reward: -1.851 [-100.000,  6.166], mean action: 1.735 [0.000, 3.000],  loss: 23.193050, mae: 16.520565, mean_q: 5.059022, mean_eps: 0.900805
  16726/150000: episode: 167, duration: 0.867s, episode steps: 144, steps per second: 166, episode reward: -239.763, mean reward: -1.665 [-100.000,  7.410], mean action: 1.507 [0.000, 3.000],  loss: 11.276854, mae: 15.755728, mean_q: 5.700796, mean_eps: 0.900079
  16813/150000: episode: 168, duration: 0.562s, episode steps:  87, steps per second: 155, episode reward: -204.217, mean reward: -2.347 [-100.000,  3.624], mean action: 1.540 [0.000, 3.000],  loss: 13.593149, mae: 15.717728, mean_q: 4.949871, mean_eps: 0.899386
  16898/150000: episode: 169, duration: 0.717s, episode steps:  85, steps per second: 119, episode reward: -121.077, mean reward: -1.424 [-100.000, 11.342], mean action: 1.659 [0.000, 3.000],  loss: 9.968122, mae: 15.457623, mean_q: 6.185106, mean_eps: 0.898870
  16965/150000: episode: 170, duration: 0.502s, episode steps:  67, steps per second: 134, episode reward: -95.827, mean reward: -1.430 [-100.000,  6.683], mean action: 1.328 [0.000, 3.000],  loss: 16.474029, mae: 15.958795, mean_q: 6.416844, mean_eps: 0.898414
  17090/150000: episode: 171, duration: 0.900s, episode steps: 125, steps per second: 139, episode reward: -117.150, mean reward: -0.937 [-100.000,  5.760], mean action: 1.448 [0.000, 3.000],  loss: 16.954032, mae: 16.003714, mean_q: 6.798934, mean_eps: 0.897838
  17198/150000: episode: 172, duration: 0.794s, episode steps: 108, steps per second: 136, episode reward: -342.770, mean reward: -3.174 [-100.000,  5.252], mean action: 1.509 [0.000, 3.000],  loss: 10.379842, mae: 16.364518, mean_q: 6.608906, mean_eps: 0.897139
  17322/150000: episode: 173, duration: 0.738s, episode steps: 124, steps per second: 168, episode reward: -47.156, mean reward: -0.380 [-100.000, 44.897], mean action: 1.435 [0.000, 3.000],  loss: 13.430072, mae: 16.502321, mean_q: 7.441529, mean_eps: 0.896443
  17402/150000: episode: 174, duration: 0.509s, episode steps:  80, steps per second: 157, episode reward: -83.113, mean reward: -1.039 [-100.000,  7.432], mean action: 1.525 [0.000, 3.000],  loss: 16.653251, mae: 16.860829, mean_q: 6.939612, mean_eps: 0.895831
  17517/150000: episode: 175, duration: 0.714s, episode steps: 115, steps per second: 161, episode reward: -182.899, mean reward: -1.590 [-100.000, 39.395], mean action: 1.635 [0.000, 3.000],  loss: 13.339949, mae: 16.414629, mean_q: 7.216389, mean_eps: 0.895246
  17598/150000: episode: 176, duration: 0.573s, episode steps:  81, steps per second: 141, episode reward: -21.434, mean reward: -0.265 [-100.000, 21.324], mean action: 1.580 [0.000, 3.000],  loss: 10.772155, mae: 16.517545, mean_q: 5.454209, mean_eps: 0.894658
  17692/150000: episode: 177, duration: 0.565s, episode steps:  94, steps per second: 166, episode reward: -97.132, mean reward: -1.033 [-100.000, 16.252], mean action: 1.691 [0.000, 3.000],  loss: 24.461898, mae: 16.717211, mean_q: 7.115707, mean_eps: 0.894133
  17794/150000: episode: 178, duration: 0.577s, episode steps: 102, steps per second: 177, episode reward: -114.438, mean reward: -1.122 [-100.000,  6.776], mean action: 1.490 [0.000, 3.000],  loss: 8.349840, mae: 16.124434, mean_q: 7.630688, mean_eps: 0.893545
  17871/150000: episode: 179, duration: 0.458s, episode steps:  77, steps per second: 168, episode reward: -226.195, mean reward: -2.938 [-100.000,  5.809], mean action: 1.753 [0.000, 3.000],  loss: 13.371409, mae: 16.846621, mean_q: 7.179874, mean_eps: 0.893008
  17955/150000: episode: 180, duration: 0.527s, episode steps:  84, steps per second: 159, episode reward: -59.594, mean reward: -0.709 [-100.000, 15.567], mean action: 1.524 [0.000, 3.000],  loss: 9.809338, mae: 16.500328, mean_q: 6.461303, mean_eps: 0.892525
  18056/150000: episode: 181, duration: 0.601s, episode steps: 101, steps per second: 168, episode reward: -28.502, mean reward: -0.282 [-100.000, 78.902], mean action: 1.574 [0.000, 3.000],  loss: 15.014473, mae: 17.150306, mean_q: 7.981950, mean_eps: 0.891970
  18126/150000: episode: 182, duration: 0.397s, episode steps:  70, steps per second: 176, episode reward: -78.991, mean reward: -1.128 [-100.000, 13.112], mean action: 1.257 [0.000, 3.000],  loss: 11.759621, mae: 18.285333, mean_q: 8.432064, mean_eps: 0.891457
  18198/150000: episode: 183, duration: 0.426s, episode steps:  72, steps per second: 169, episode reward: -122.293, mean reward: -1.699 [-100.000, 10.483], mean action: 1.694 [0.000, 3.000],  loss: 31.753459, mae: 17.364813, mean_q: 8.695050, mean_eps: 0.891031
  18297/150000: episode: 184, duration: 0.605s, episode steps:  99, steps per second: 164, episode reward: -160.141, mean reward: -1.618 [-100.000, 11.425], mean action: 1.596 [0.000, 3.000],  loss: 10.931783, mae: 17.454246, mean_q: 8.517490, mean_eps: 0.890518
  18394/150000: episode: 185, duration: 0.583s, episode steps:  97, steps per second: 166, episode reward: -250.395, mean reward: -2.581 [-100.000, 38.491], mean action: 1.701 [0.000, 3.000],  loss: 11.655375, mae: 17.329429, mean_q: 8.891285, mean_eps: 0.889930
  18492/150000: episode: 186, duration: 0.610s, episode steps:  98, steps per second: 161, episode reward: -86.113, mean reward: -0.879 [-100.000,  9.476], mean action: 1.469 [0.000, 3.000],  loss: 9.407045, mae: 17.302427, mean_q: 8.240487, mean_eps: 0.889345
  18598/150000: episode: 187, duration: 0.709s, episode steps: 106, steps per second: 150, episode reward: -281.528, mean reward: -2.656 [-100.000,  5.901], mean action: 1.500 [0.000, 3.000],  loss: 19.054877, mae: 17.417710, mean_q: 9.135897, mean_eps: 0.888733
  18681/150000: episode: 188, duration: 0.504s, episode steps:  83, steps per second: 165, episode reward: -42.420, mean reward: -0.511 [-100.000, 15.811], mean action: 1.482 [0.000, 3.000],  loss: 7.364142, mae: 17.104430, mean_q: 8.280155, mean_eps: 0.888166
  18749/150000: episode: 189, duration: 0.459s, episode steps:  68, steps per second: 148, episode reward: -75.085, mean reward: -1.104 [-100.000, 11.746], mean action: 1.515 [0.000, 3.000],  loss: 15.955487, mae: 17.912854, mean_q: 8.013719, mean_eps: 0.887713
  18830/150000: episode: 190, duration: 0.536s, episode steps:  81, steps per second: 151, episode reward: -128.471, mean reward: -1.586 [-100.000, 14.929], mean action: 1.605 [0.000, 3.000],  loss: 12.554868, mae: 17.958376, mean_q: 8.105144, mean_eps: 0.887266
  18953/150000: episode: 191, duration: 0.733s, episode steps: 123, steps per second: 168, episode reward:  6.430, mean reward:  0.052 [-100.000, 84.282], mean action: 1.512 [0.000, 3.000],  loss: 19.952112, mae: 17.721478, mean_q: 9.014301, mean_eps: 0.886654
  19057/150000: episode: 192, duration: 0.645s, episode steps: 104, steps per second: 161, episode reward: -115.247, mean reward: -1.108 [-100.000, 13.402], mean action: 1.654 [0.000, 3.000],  loss: 30.122325, mae: 17.609610, mean_q: 9.548711, mean_eps: 0.885973
  19148/150000: episode: 193, duration: 0.556s, episode steps:  91, steps per second: 164, episode reward: -166.705, mean reward: -1.832 [-100.000, 20.962], mean action: 1.582 [0.000, 3.000],  loss: 10.730996, mae: 17.973748, mean_q: 10.967348, mean_eps: 0.885388
  19288/150000: episode: 194, duration: 0.828s, episode steps: 140, steps per second: 169, episode reward: -100.903, mean reward: -0.721 [-100.000, 40.699], mean action: 1.457 [0.000, 3.000],  loss: 16.260729, mae: 17.699602, mean_q: 10.386704, mean_eps: 0.884695
  19389/150000: episode: 195, duration: 0.595s, episode steps: 101, steps per second: 170, episode reward: -107.076, mean reward: -1.060 [-100.000, 10.175], mean action: 1.703 [0.000, 3.000],  loss: 18.022288, mae: 17.962489, mean_q: 10.062096, mean_eps: 0.883972
  19501/150000: episode: 196, duration: 0.626s, episode steps: 112, steps per second: 179, episode reward: -128.171, mean reward: -1.144 [-100.000,  5.283], mean action: 1.446 [0.000, 3.000],  loss: 22.409382, mae: 17.779036, mean_q: 9.434773, mean_eps: 0.883333
  19666/150000: episode: 197, duration: 0.971s, episode steps: 165, steps per second: 170, episode reward: -182.634, mean reward: -1.107 [-100.000, 107.456], mean action: 1.497 [0.000, 3.000],  loss: 9.665694, mae: 18.081242, mean_q: 9.845662, mean_eps: 0.882502
  19729/150000: episode: 198, duration: 0.374s, episode steps:  63, steps per second: 169, episode reward: -120.696, mean reward: -1.916 [-100.000, 14.222], mean action: 1.333 [0.000, 3.000],  loss: 12.592039, mae: 18.611282, mean_q: 8.477880, mean_eps: 0.881818
  19832/150000: episode: 199, duration: 0.569s, episode steps: 103, steps per second: 181, episode reward: -185.351, mean reward: -1.800 [-100.000, 19.550], mean action: 1.262 [0.000, 3.000],  loss: 12.377099, mae: 18.351092, mean_q: 9.340846, mean_eps: 0.881320
  19896/150000: episode: 200, duration: 0.357s, episode steps:  64, steps per second: 179, episode reward: -79.358, mean reward: -1.240 [-100.000, 10.810], mean action: 1.422 [0.000, 3.000],  loss: 13.239877, mae: 18.178953, mean_q: 10.367787, mean_eps: 0.880819
  19966/150000: episode: 201, duration: 0.392s, episode steps:  70, steps per second: 179, episode reward: -108.602, mean reward: -1.551 [-100.000,  8.551], mean action: 1.543 [0.000, 3.000],  loss: 7.885633, mae: 17.465403, mean_q: 10.827394, mean_eps: 0.880417
  20045/150000: episode: 202, duration: 0.484s, episode steps:  79, steps per second: 163, episode reward: -40.369, mean reward: -0.511 [-100.000, 22.932], mean action: 1.405 [0.000, 3.000],  loss: 19.307778, mae: 18.689319, mean_q: 10.686946, mean_eps: 0.879970
  20177/150000: episode: 203, duration: 0.762s, episode steps: 132, steps per second: 173, episode reward: -61.192, mean reward: -0.464 [-100.000,  8.042], mean action: 1.409 [0.000, 3.000],  loss: 19.964010, mae: 19.567980, mean_q: 12.199449, mean_eps: 0.879337
  20301/150000: episode: 204, duration: 0.786s, episode steps: 124, steps per second: 158, episode reward: -121.204, mean reward: -0.977 [-100.000, 12.472], mean action: 1.492 [0.000, 3.000],  loss: 22.710771, mae: 19.362878, mean_q: 10.975700, mean_eps: 0.878569
  20377/150000: episode: 205, duration: 0.455s, episode steps:  76, steps per second: 167, episode reward: -92.857, mean reward: -1.222 [-100.000, 11.951], mean action: 1.776 [0.000, 3.000],  loss: 13.746335, mae: 19.725539, mean_q: 11.980989, mean_eps: 0.877969
  20506/150000: episode: 206, duration: 0.758s, episode steps: 129, steps per second: 170, episode reward: -58.742, mean reward: -0.455 [-100.000,  7.415], mean action: 1.612 [0.000, 3.000],  loss: 12.203756, mae: 19.304952, mean_q: 11.557201, mean_eps: 0.877354
  20589/150000: episode: 207, duration: 0.468s, episode steps:  83, steps per second: 177, episode reward: -57.785, mean reward: -0.696 [-100.000, 49.709], mean action: 1.663 [0.000, 3.000],  loss: 26.392411, mae: 19.238766, mean_q: 11.668169, mean_eps: 0.876718
  20717/150000: episode: 208, duration: 0.703s, episode steps: 128, steps per second: 182, episode reward: -49.980, mean reward: -0.390 [-100.000, 14.444], mean action: 1.547 [0.000, 3.000],  loss: 25.681416, mae: 19.578033, mean_q: 11.264392, mean_eps: 0.876085
  20802/150000: episode: 209, duration: 0.534s, episode steps:  85, steps per second: 159, episode reward: -79.856, mean reward: -0.939 [-100.000,  8.080], mean action: 1.635 [0.000, 3.000],  loss: 26.581574, mae: 19.530396, mean_q: 12.288507, mean_eps: 0.875446
  20925/150000: episode: 210, duration: 0.784s, episode steps: 123, steps per second: 157, episode reward: -133.085, mean reward: -1.082 [-100.000,  9.309], mean action: 1.447 [0.000, 3.000],  loss: 14.381391, mae: 19.572786, mean_q: 11.398911, mean_eps: 0.874822
  21059/150000: episode: 211, duration: 0.810s, episode steps: 134, steps per second: 165, episode reward: -76.518, mean reward: -0.571 [-100.000,  7.078], mean action: 1.627 [0.000, 3.000],  loss: 11.022967, mae: 19.985921, mean_q: 11.027284, mean_eps: 0.874051
  21153/150000: episode: 212, duration: 0.588s, episode steps:  94, steps per second: 160, episode reward: -39.697, mean reward: -0.422 [-100.000,  7.478], mean action: 1.585 [0.000, 3.000],  loss: 13.036894, mae: 20.945132, mean_q: 13.027423, mean_eps: 0.873367
  21221/150000: episode: 213, duration: 0.417s, episode steps:  68, steps per second: 163, episode reward: -60.670, mean reward: -0.892 [-100.000, 18.549], mean action: 1.676 [0.000, 3.000],  loss: 18.371044, mae: 20.436439, mean_q: 13.954184, mean_eps: 0.872881
  21367/150000: episode: 214, duration: 0.816s, episode steps: 146, steps per second: 179, episode reward: -31.943, mean reward: -0.219 [-100.000, 17.815], mean action: 1.575 [0.000, 3.000],  loss: 12.100834, mae: 21.192716, mean_q: 11.805050, mean_eps: 0.872239
  21467/150000: episode: 215, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -275.648, mean reward: -2.756 [-100.000,  0.624], mean action: 1.500 [0.000, 3.000],  loss: 9.411836, mae: 20.340543, mean_q: 11.917899, mean_eps: 0.871501
  21552/150000: episode: 216, duration: 0.500s, episode steps:  85, steps per second: 170, episode reward: -104.147, mean reward: -1.225 [-100.000,  6.330], mean action: 1.765 [0.000, 3.000],  loss: 18.257544, mae: 20.297529, mean_q: 11.042786, mean_eps: 0.870946
  21662/150000: episode: 217, duration: 0.617s, episode steps: 110, steps per second: 178, episode reward: -104.503, mean reward: -0.950 [-100.000,  9.005], mean action: 1.609 [0.000, 3.000],  loss: 16.339808, mae: 21.100541, mean_q: 12.032339, mean_eps: 0.870361
  21763/150000: episode: 218, duration: 0.569s, episode steps: 101, steps per second: 178, episode reward: -115.981, mean reward: -1.148 [-100.000,  9.422], mean action: 1.653 [0.000, 3.000],  loss: 22.445207, mae: 20.782493, mean_q: 11.844472, mean_eps: 0.869728
  21844/150000: episode: 219, duration: 0.485s, episode steps:  81, steps per second: 167, episode reward: -100.177, mean reward: -1.237 [-100.000,  7.556], mean action: 1.395 [0.000, 3.000],  loss: 24.560924, mae: 20.863623, mean_q: 11.768455, mean_eps: 0.869182
  21920/150000: episode: 220, duration: 0.497s, episode steps:  76, steps per second: 153, episode reward: -68.788, mean reward: -0.905 [-100.000, 10.140], mean action: 1.500 [0.000, 3.000],  loss: 23.065558, mae: 20.367705, mean_q: 12.638673, mean_eps: 0.868711
  22005/150000: episode: 221, duration: 0.562s, episode steps:  85, steps per second: 151, episode reward: -119.654, mean reward: -1.408 [-100.000,  6.569], mean action: 1.600 [0.000, 3.000],  loss: 18.061591, mae: 20.408007, mean_q: 13.348036, mean_eps: 0.868228
  22113/150000: episode: 222, duration: 0.661s, episode steps: 108, steps per second: 163, episode reward: -85.821, mean reward: -0.795 [-100.000, 13.235], mean action: 1.713 [0.000, 3.000],  loss: 17.134725, mae: 20.625609, mean_q: 14.067664, mean_eps: 0.867649
  22240/150000: episode: 223, duration: 0.818s, episode steps: 127, steps per second: 155, episode reward: -146.072, mean reward: -1.150 [-100.000, 27.595], mean action: 1.480 [0.000, 3.000],  loss: 16.239360, mae: 21.067515, mean_q: 13.987348, mean_eps: 0.866944
  22311/150000: episode: 224, duration: 0.431s, episode steps:  71, steps per second: 165, episode reward: -96.115, mean reward: -1.354 [-100.000,  7.309], mean action: 1.493 [0.000, 3.000],  loss: 17.115225, mae: 20.093706, mean_q: 14.250264, mean_eps: 0.866350
  22373/150000: episode: 225, duration: 0.385s, episode steps:  62, steps per second: 161, episode reward: -200.381, mean reward: -3.232 [-100.000, 39.925], mean action: 1.565 [0.000, 3.000],  loss: 13.702118, mae: 20.793161, mean_q: 13.978858, mean_eps: 0.865951
  22491/150000: episode: 226, duration: 0.716s, episode steps: 118, steps per second: 165, episode reward: -132.934, mean reward: -1.127 [-100.000,  6.380], mean action: 1.551 [0.000, 3.000],  loss: 15.580503, mae: 21.081122, mean_q: 13.201580, mean_eps: 0.865411
  22571/150000: episode: 227, duration: 0.482s, episode steps:  80, steps per second: 166, episode reward: -76.632, mean reward: -0.958 [-100.000,  7.828], mean action: 1.425 [0.000, 3.000],  loss: 13.551881, mae: 20.990272, mean_q: 14.248697, mean_eps: 0.864817
  22641/150000: episode: 228, duration: 0.420s, episode steps:  70, steps per second: 166, episode reward: -110.245, mean reward: -1.575 [-100.000,  6.532], mean action: 1.414 [0.000, 3.000],  loss: 19.559689, mae: 20.831195, mean_q: 13.476460, mean_eps: 0.864367
  22779/150000: episode: 229, duration: 0.793s, episode steps: 138, steps per second: 174, episode reward: -52.810, mean reward: -0.383 [-100.000, 12.040], mean action: 1.464 [0.000, 3.000],  loss: 20.527219, mae: 20.920754, mean_q: 13.387577, mean_eps: 0.863743
  22865/150000: episode: 230, duration: 0.489s, episode steps:  86, steps per second: 176, episode reward: -157.251, mean reward: -1.829 [-100.000, 48.858], mean action: 1.558 [0.000, 3.000],  loss: 20.382503, mae: 21.130455, mean_q: 14.195453, mean_eps: 0.863071
  22942/150000: episode: 231, duration: 0.505s, episode steps:  77, steps per second: 153, episode reward: -85.599, mean reward: -1.112 [-100.000,  8.578], mean action: 1.662 [0.000, 3.000],  loss: 13.652611, mae: 20.927745, mean_q: 13.641822, mean_eps: 0.862582
  23048/150000: episode: 232, duration: 0.636s, episode steps: 106, steps per second: 167, episode reward: -113.678, mean reward: -1.072 [-100.000,  8.600], mean action: 1.425 [0.000, 3.000],  loss: 11.468396, mae: 21.127143, mean_q: 14.229570, mean_eps: 0.862033
  23127/150000: episode: 233, duration: 0.439s, episode steps:  79, steps per second: 180, episode reward: -87.241, mean reward: -1.104 [-100.000,  7.129], mean action: 1.658 [0.000, 3.000],  loss: 20.485966, mae: 21.849958, mean_q: 13.938197, mean_eps: 0.861478
  23234/150000: episode: 234, duration: 0.591s, episode steps: 107, steps per second: 181, episode reward: -141.630, mean reward: -1.324 [-100.000, 12.244], mean action: 1.514 [0.000, 3.000],  loss: 15.897477, mae: 21.525420, mean_q: 13.986196, mean_eps: 0.860920
  23327/150000: episode: 235, duration: 0.565s, episode steps:  93, steps per second: 165, episode reward: -0.318, mean reward: -0.003 [-100.000, 46.643], mean action: 1.710 [0.000, 3.000],  loss: 11.164956, mae: 21.431738, mean_q: 13.754685, mean_eps: 0.860320
  23448/150000: episode: 236, duration: 0.696s, episode steps: 121, steps per second: 174, episode reward: -64.590, mean reward: -0.534 [-100.000, 14.816], mean action: 1.678 [0.000, 3.000],  loss: 16.632125, mae: 21.558631, mean_q: 13.746764, mean_eps: 0.859678
  23519/150000: episode: 237, duration: 0.396s, episode steps:  71, steps per second: 179, episode reward: -71.017, mean reward: -1.000 [-100.000,  8.862], mean action: 1.563 [0.000, 3.000],  loss: 10.015038, mae: 21.514988, mean_q: 13.616780, mean_eps: 0.859102
  23633/150000: episode: 238, duration: 0.626s, episode steps: 114, steps per second: 182, episode reward: -127.576, mean reward: -1.119 [-100.000, 11.920], mean action: 1.596 [0.000, 3.000],  loss: 18.703650, mae: 21.331126, mean_q: 15.443496, mean_eps: 0.858547
  23746/150000: episode: 239, duration: 0.698s, episode steps: 113, steps per second: 162, episode reward: -106.306, mean reward: -0.941 [-100.000, 13.095], mean action: 1.469 [0.000, 3.000],  loss: 12.499801, mae: 21.601935, mean_q: 14.286502, mean_eps: 0.857866
  23811/150000: episode: 240, duration: 0.402s, episode steps:  65, steps per second: 162, episode reward: -92.648, mean reward: -1.425 [-100.000,  8.323], mean action: 1.677 [0.000, 3.000],  loss: 18.096641, mae: 21.216838, mean_q: 13.986808, mean_eps: 0.857332
  23914/150000: episode: 241, duration: 0.582s, episode steps: 103, steps per second: 177, episode reward: -91.562, mean reward: -0.889 [-100.000, 13.942], mean action: 1.641 [0.000, 3.000],  loss: 12.217848, mae: 21.653647, mean_q: 14.435962, mean_eps: 0.856828
  24008/150000: episode: 242, duration: 0.523s, episode steps:  94, steps per second: 180, episode reward: -219.682, mean reward: -2.337 [-100.000, 25.440], mean action: 1.617 [0.000, 3.000],  loss: 13.915819, mae: 21.904742, mean_q: 14.000665, mean_eps: 0.856237
  24115/150000: episode: 243, duration: 0.632s, episode steps: 107, steps per second: 169, episode reward: -118.214, mean reward: -1.105 [-100.000, 12.302], mean action: 1.411 [0.000, 3.000],  loss: 18.734122, mae: 22.240275, mean_q: 14.153186, mean_eps: 0.855634
  24184/150000: episode: 244, duration: 0.395s, episode steps:  69, steps per second: 175, episode reward: -76.082, mean reward: -1.103 [-100.000, 15.773], mean action: 1.565 [0.000, 3.000],  loss: 6.823596, mae: 22.200158, mean_q: 15.042619, mean_eps: 0.855106
  24315/150000: episode: 245, duration: 0.733s, episode steps: 131, steps per second: 179, episode reward: -90.586, mean reward: -0.691 [-100.000, 10.284], mean action: 1.588 [0.000, 3.000],  loss: 22.071016, mae: 22.827543, mean_q: 14.481625, mean_eps: 0.854506
  24398/150000: episode: 246, duration: 0.493s, episode steps:  83, steps per second: 168, episode reward: -53.708, mean reward: -0.647 [-100.000, 16.809], mean action: 1.494 [0.000, 3.000],  loss: 18.865442, mae: 22.602856, mean_q: 13.705905, mean_eps: 0.853864
  24498/150000: episode: 247, duration: 0.581s, episode steps: 100, steps per second: 172, episode reward: -246.131, mean reward: -2.461 [-100.000,  3.319], mean action: 1.670 [0.000, 3.000],  loss: 13.214154, mae: 22.907305, mean_q: 14.970927, mean_eps: 0.853315
  24591/150000: episode: 248, duration: 0.538s, episode steps:  93, steps per second: 173, episode reward: -161.874, mean reward: -1.741 [-100.000,  7.547], mean action: 1.613 [0.000, 3.000],  loss: 10.073354, mae: 22.359055, mean_q: 14.477153, mean_eps: 0.852736
  24676/150000: episode: 249, duration: 0.471s, episode steps:  85, steps per second: 180, episode reward: -125.086, mean reward: -1.472 [-100.000,  5.405], mean action: 1.365 [0.000, 3.000],  loss: 14.076575, mae: 22.471416, mean_q: 15.387628, mean_eps: 0.852202
  24772/150000: episode: 250, duration: 0.561s, episode steps:  96, steps per second: 171, episode reward: -102.577, mean reward: -1.069 [-100.000,  8.569], mean action: 1.573 [0.000, 3.000],  loss: 17.537675, mae: 22.866442, mean_q: 16.402070, mean_eps: 0.851659
  24901/150000: episode: 251, duration: 0.753s, episode steps: 129, steps per second: 171, episode reward: -142.132, mean reward: -1.102 [-100.000, 34.840], mean action: 1.659 [0.000, 3.000],  loss: 16.044792, mae: 22.842954, mean_q: 14.589081, mean_eps: 0.850984
  25028/150000: episode: 252, duration: 0.710s, episode steps: 127, steps per second: 179, episode reward: -88.194, mean reward: -0.694 [-100.000,  7.675], mean action: 1.480 [0.000, 3.000],  loss: 13.691285, mae: 23.241306, mean_q: 14.183000, mean_eps: 0.850216
  25120/150000: episode: 253, duration: 0.535s, episode steps:  92, steps per second: 172, episode reward: -63.059, mean reward: -0.685 [-100.000, 12.166], mean action: 1.652 [0.000, 3.000],  loss: 17.952072, mae: 23.456682, mean_q: 16.598677, mean_eps: 0.849559
  25213/150000: episode: 254, duration: 0.553s, episode steps:  93, steps per second: 168, episode reward: -176.852, mean reward: -1.902 [-100.000, 31.339], mean action: 1.624 [0.000, 3.000],  loss: 19.344291, mae: 24.014870, mean_q: 15.642010, mean_eps: 0.849004
  25335/150000: episode: 255, duration: 0.704s, episode steps: 122, steps per second: 173, episode reward: -108.025, mean reward: -0.885 [-100.000, 12.114], mean action: 1.598 [0.000, 3.000],  loss: 13.390487, mae: 23.647919, mean_q: 15.913204, mean_eps: 0.848359
  25415/150000: episode: 256, duration: 0.474s, episode steps:  80, steps per second: 169, episode reward: -112.385, mean reward: -1.405 [-100.000, 10.486], mean action: 1.488 [0.000, 3.000],  loss: 12.412358, mae: 23.677787, mean_q: 16.973985, mean_eps: 0.847753
  25484/150000: episode: 257, duration: 0.394s, episode steps:  69, steps per second: 175, episode reward: -46.684, mean reward: -0.677 [-100.000, 16.316], mean action: 1.754 [0.000, 3.000],  loss: 8.666198, mae: 24.128642, mean_q: 15.928896, mean_eps: 0.847306
  25558/150000: episode: 258, duration: 0.442s, episode steps:  74, steps per second: 167, episode reward: -115.251, mean reward: -1.557 [-100.000, 23.353], mean action: 1.595 [0.000, 3.000],  loss: 8.137656, mae: 24.392204, mean_q: 18.164093, mean_eps: 0.846877
  25683/150000: episode: 259, duration: 0.723s, episode steps: 125, steps per second: 173, episode reward: -184.294, mean reward: -1.474 [-100.000,  6.106], mean action: 1.464 [0.000, 3.000],  loss: 15.626701, mae: 24.590903, mean_q: 16.399334, mean_eps: 0.846280
  25809/150000: episode: 260, duration: 0.744s, episode steps: 126, steps per second: 169, episode reward: -70.323, mean reward: -0.558 [-100.000,  8.428], mean action: 1.579 [0.000, 3.000],  loss: 7.236072, mae: 23.721924, mean_q: 16.982013, mean_eps: 0.845527
  25891/150000: episode: 261, duration: 0.484s, episode steps:  82, steps per second: 169, episode reward: -41.303, mean reward: -0.504 [-100.000, 12.825], mean action: 1.488 [0.000, 3.000],  loss: 12.955102, mae: 23.734096, mean_q: 16.224776, mean_eps: 0.844903
  26049/150000: episode: 262, duration: 0.946s, episode steps: 158, steps per second: 167, episode reward: 12.587, mean reward:  0.080 [-100.000, 104.163], mean action: 1.608 [0.000, 3.000],  loss: 11.739340, mae: 24.004831, mean_q: 17.187075, mean_eps: 0.844183
  26121/150000: episode: 263, duration: 0.502s, episode steps:  72, steps per second: 143, episode reward: -57.665, mean reward: -0.801 [-100.000, 10.435], mean action: 1.708 [0.000, 3.000],  loss: 15.201154, mae: 25.400672, mean_q: 17.080123, mean_eps: 0.843493
  26205/150000: episode: 264, duration: 0.563s, episode steps:  84, steps per second: 149, episode reward: -156.348, mean reward: -1.861 [-100.000,  7.587], mean action: 1.476 [0.000, 3.000],  loss: 8.552228, mae: 25.251878, mean_q: 16.916592, mean_eps: 0.843025
  26302/150000: episode: 265, duration: 0.652s, episode steps:  97, steps per second: 149, episode reward: -133.643, mean reward: -1.378 [-100.000, 28.520], mean action: 1.567 [0.000, 3.000],  loss: 10.679267, mae: 24.717010, mean_q: 17.295500, mean_eps: 0.842482
  26414/150000: episode: 266, duration: 0.701s, episode steps: 112, steps per second: 160, episode reward: -66.933, mean reward: -0.598 [-100.000,  7.523], mean action: 1.607 [0.000, 3.000],  loss: 15.230266, mae: 24.828749, mean_q: 17.643328, mean_eps: 0.841855
  26502/150000: episode: 267, duration: 0.558s, episode steps:  88, steps per second: 158, episode reward: -127.835, mean reward: -1.453 [-100.000,  6.046], mean action: 1.466 [0.000, 3.000],  loss: 13.770023, mae: 24.935059, mean_q: 17.561203, mean_eps: 0.841255
  26601/150000: episode: 268, duration: 0.643s, episode steps:  99, steps per second: 154, episode reward: -150.074, mean reward: -1.516 [-100.000,  6.798], mean action: 1.495 [0.000, 3.000],  loss: 12.040958, mae: 24.578600, mean_q: 16.902026, mean_eps: 0.840694
  26677/150000: episode: 269, duration: 0.513s, episode steps:  76, steps per second: 148, episode reward: -105.775, mean reward: -1.392 [-100.000,  6.735], mean action: 1.908 [0.000, 3.000],  loss: 7.689579, mae: 25.001500, mean_q: 16.647265, mean_eps: 0.840169
  26740/150000: episode: 270, duration: 0.421s, episode steps:  63, steps per second: 150, episode reward: -100.471, mean reward: -1.595 [-100.000,  3.193], mean action: 1.524 [0.000, 3.000],  loss: 17.073385, mae: 24.821838, mean_q: 17.077920, mean_eps: 0.839752
  26813/150000: episode: 271, duration: 0.435s, episode steps:  73, steps per second: 168, episode reward: -73.708, mean reward: -1.010 [-100.000,  6.211], mean action: 1.479 [0.000, 3.000],  loss: 24.609440, mae: 25.182881, mean_q: 17.225458, mean_eps: 0.839344
  26932/150000: episode: 272, duration: 0.716s, episode steps: 119, steps per second: 166, episode reward: -138.488, mean reward: -1.164 [-100.000, 24.148], mean action: 1.563 [0.000, 3.000],  loss: 14.306471, mae: 25.096721, mean_q: 16.719241, mean_eps: 0.838768
  27052/150000: episode: 273, duration: 0.700s, episode steps: 120, steps per second: 171, episode reward: -147.740, mean reward: -1.231 [-100.000, 12.445], mean action: 1.667 [0.000, 3.000],  loss: 7.076868, mae: 24.876881, mean_q: 16.493278, mean_eps: 0.838051
  27141/150000: episode: 274, duration: 0.494s, episode steps:  89, steps per second: 180, episode reward: -104.144, mean reward: -1.170 [-100.000, 22.650], mean action: 1.472 [0.000, 3.000],  loss: 7.954375, mae: 25.350197, mean_q: 15.711560, mean_eps: 0.837424
  27235/150000: episode: 275, duration: 0.558s, episode steps:  94, steps per second: 169, episode reward: -78.117, mean reward: -0.831 [-100.000,  7.376], mean action: 1.660 [0.000, 3.000],  loss: 17.156853, mae: 26.313665, mean_q: 16.082521, mean_eps: 0.836875
  27297/150000: episode: 276, duration: 0.420s, episode steps:  62, steps per second: 148, episode reward: -71.165, mean reward: -1.148 [-100.000,  9.202], mean action: 1.484 [0.000, 3.000],  loss: 12.681082, mae: 25.919988, mean_q: 17.261556, mean_eps: 0.836407
  27366/150000: episode: 277, duration: 0.419s, episode steps:  69, steps per second: 165, episode reward: -109.766, mean reward: -1.591 [-100.000, 15.828], mean action: 1.319 [0.000, 3.000],  loss: 5.981005, mae: 25.629308, mean_q: 17.778541, mean_eps: 0.836014
  27504/150000: episode: 278, duration: 0.781s, episode steps: 138, steps per second: 177, episode reward: -320.564, mean reward: -2.323 [-100.000, 53.087], mean action: 1.493 [0.000, 3.000],  loss: 19.824422, mae: 25.849579, mean_q: 18.137912, mean_eps: 0.835393
  27618/150000: episode: 279, duration: 0.783s, episode steps: 114, steps per second: 146, episode reward: -159.120, mean reward: -1.396 [-100.000,  9.299], mean action: 1.518 [0.000, 3.000],  loss: 10.256465, mae: 25.374082, mean_q: 17.572424, mean_eps: 0.834637
  27751/150000: episode: 280, duration: 1.118s, episode steps: 133, steps per second: 119, episode reward: -76.825, mean reward: -0.578 [-100.000,  6.681], mean action: 1.722 [0.000, 3.000],  loss: 10.011545, mae: 25.950895, mean_q: 16.545550, mean_eps: 0.833896
  27851/150000: episode: 281, duration: 0.580s, episode steps: 100, steps per second: 173, episode reward: -143.987, mean reward: -1.440 [-100.000,  7.209], mean action: 1.640 [0.000, 3.000],  loss: 11.938397, mae: 25.766761, mean_q: 16.562205, mean_eps: 0.833197
  27919/150000: episode: 282, duration: 0.405s, episode steps:  68, steps per second: 168, episode reward: -91.408, mean reward: -1.344 [-100.000,  8.325], mean action: 1.574 [0.000, 3.000],  loss: 7.512924, mae: 25.260120, mean_q: 17.693487, mean_eps: 0.832693
  28002/150000: episode: 283, duration: 0.518s, episode steps:  83, steps per second: 160, episode reward: 14.264, mean reward:  0.172 [-100.000, 83.681], mean action: 1.566 [0.000, 3.000],  loss: 9.211075, mae: 25.544303, mean_q: 19.062685, mean_eps: 0.832240
  28132/150000: episode: 284, duration: 0.729s, episode steps: 130, steps per second: 178, episode reward: -129.716, mean reward: -0.998 [-100.000, 21.415], mean action: 1.500 [0.000, 3.000],  loss: 10.027656, mae: 26.838989, mean_q: 18.794720, mean_eps: 0.831601
  28265/150000: episode: 285, duration: 0.741s, episode steps: 133, steps per second: 179, episode reward: -97.644, mean reward: -0.734 [-100.000, 10.151], mean action: 1.594 [0.000, 3.000],  loss: 6.391258, mae: 26.265800, mean_q: 17.845157, mean_eps: 0.830812
  28355/150000: episode: 286, duration: 0.569s, episode steps:  90, steps per second: 158, episode reward: -534.168, mean reward: -5.935 [-100.000,  7.788], mean action: 1.367 [0.000, 3.000],  loss: 17.402122, mae: 26.231255, mean_q: 18.030297, mean_eps: 0.830143
  28422/150000: episode: 287, duration: 0.389s, episode steps:  67, steps per second: 172, episode reward: -73.395, mean reward: -1.095 [-100.000,  9.840], mean action: 1.299 [0.000, 3.000],  loss: 9.032301, mae: 26.474214, mean_q: 17.618848, mean_eps: 0.829672
  28493/150000: episode: 288, duration: 0.401s, episode steps:  71, steps per second: 177, episode reward: -58.845, mean reward: -0.829 [-100.000, 17.520], mean action: 1.479 [0.000, 3.000],  loss: 19.615100, mae: 26.584682, mean_q: 18.026800, mean_eps: 0.829258
  28572/150000: episode: 289, duration: 0.440s, episode steps:  79, steps per second: 180, episode reward: -57.015, mean reward: -0.722 [-100.000, 16.981], mean action: 1.620 [0.000, 3.000],  loss: 16.564769, mae: 26.817403, mean_q: 18.332074, mean_eps: 0.828808
  28636/150000: episode: 290, duration: 0.355s, episode steps:  64, steps per second: 180, episode reward: -57.794, mean reward: -0.903 [-100.000,  6.843], mean action: 1.656 [0.000, 3.000],  loss: 12.052490, mae: 26.485259, mean_q: 17.290259, mean_eps: 0.828379
  28724/150000: episode: 291, duration: 0.552s, episode steps:  88, steps per second: 159, episode reward: -113.361, mean reward: -1.288 [-100.000, 21.604], mean action: 1.420 [0.000, 3.000],  loss: 11.787015, mae: 26.408869, mean_q: 18.608165, mean_eps: 0.827923
  28818/150000: episode: 292, duration: 0.646s, episode steps:  94, steps per second: 146, episode reward: -130.623, mean reward: -1.390 [-100.000,  9.327], mean action: 1.426 [0.000, 3.000],  loss: 11.713694, mae: 26.961808, mean_q: 18.203156, mean_eps: 0.827377
  28895/150000: episode: 293, duration: 0.468s, episode steps:  77, steps per second: 165, episode reward: -60.795, mean reward: -0.790 [-100.000,  6.771], mean action: 1.597 [0.000, 3.000],  loss: 7.715027, mae: 26.515775, mean_q: 18.927101, mean_eps: 0.826864
  29027/150000: episode: 294, duration: 0.875s, episode steps: 132, steps per second: 151, episode reward: -51.660, mean reward: -0.391 [-100.000, 15.324], mean action: 1.561 [0.000, 3.000],  loss: 15.182075, mae: 27.228768, mean_q: 18.175821, mean_eps: 0.826237
  29141/150000: episode: 295, duration: 0.709s, episode steps: 114, steps per second: 161, episode reward: -42.461, mean reward: -0.372 [-100.000, 26.326], mean action: 1.482 [0.000, 3.000],  loss: 17.595503, mae: 27.802051, mean_q: 19.953937, mean_eps: 0.825499
  29253/150000: episode: 296, duration: 0.690s, episode steps: 112, steps per second: 162, episode reward: -168.297, mean reward: -1.503 [-100.000,  6.998], mean action: 1.625 [0.000, 3.000],  loss: 14.707297, mae: 27.672674, mean_q: 19.505877, mean_eps: 0.824821
  29366/150000: episode: 297, duration: 0.764s, episode steps: 113, steps per second: 148, episode reward: -50.091, mean reward: -0.443 [-100.000, 11.353], mean action: 1.717 [0.000, 3.000],  loss: 19.706928, mae: 28.075525, mean_q: 19.495988, mean_eps: 0.824146
  29443/150000: episode: 298, duration: 0.508s, episode steps:  77, steps per second: 151, episode reward: -59.678, mean reward: -0.775 [-100.000, 21.258], mean action: 1.675 [0.000, 3.000],  loss: 9.253772, mae: 27.322718, mean_q: 20.592444, mean_eps: 0.823576
  29540/150000: episode: 299, duration: 0.626s, episode steps:  97, steps per second: 155, episode reward: -79.675, mean reward: -0.821 [-100.000, 24.553], mean action: 1.619 [0.000, 3.000],  loss: 10.803221, mae: 27.534537, mean_q: 20.017709, mean_eps: 0.823054
  29619/150000: episode: 300, duration: 0.467s, episode steps:  79, steps per second: 169, episode reward: -100.994, mean reward: -1.278 [-100.000,  5.658], mean action: 1.481 [0.000, 3.000],  loss: 8.482070, mae: 27.672405, mean_q: 19.923867, mean_eps: 0.822526
  29709/150000: episode: 301, duration: 0.568s, episode steps:  90, steps per second: 158, episode reward: -141.595, mean reward: -1.573 [-100.000,  5.397], mean action: 1.444 [0.000, 3.000],  loss: 9.151062, mae: 27.427516, mean_q: 20.776568, mean_eps: 0.822019
  29841/150000: episode: 302, duration: 0.798s, episode steps: 132, steps per second: 165, episode reward: -52.404, mean reward: -0.397 [-100.000, 15.422], mean action: 1.553 [0.000, 3.000],  loss: 11.618225, mae: 27.514743, mean_q: 19.978269, mean_eps: 0.821353
  29910/150000: episode: 303, duration: 0.462s, episode steps:  69, steps per second: 149, episode reward: -109.457, mean reward: -1.586 [-100.000, 11.620], mean action: 1.594 [0.000, 3.000],  loss: 18.362580, mae: 27.517592, mean_q: 21.519268, mean_eps: 0.820750
  29970/150000: episode: 304, duration: 0.382s, episode steps:  60, steps per second: 157, episode reward: -77.242, mean reward: -1.287 [-100.000, 11.423], mean action: 1.400 [0.000, 3.000],  loss: 7.646927, mae: 27.685380, mean_q: 21.259358, mean_eps: 0.820363
  30068/150000: episode: 305, duration: 0.643s, episode steps:  98, steps per second: 152, episode reward: -127.752, mean reward: -1.304 [-100.000, 16.343], mean action: 1.663 [0.000, 3.000],  loss: 17.672363, mae: 27.441718, mean_q: 19.775370, mean_eps: 0.819889
  30147/150000: episode: 306, duration: 0.488s, episode steps:  79, steps per second: 162, episode reward: -141.735, mean reward: -1.794 [-100.000, 10.409], mean action: 1.759 [0.000, 3.000],  loss: 12.138145, mae: 27.589558, mean_q: 19.795602, mean_eps: 0.819358
  30214/150000: episode: 307, duration: 0.402s, episode steps:  67, steps per second: 167, episode reward: -100.894, mean reward: -1.506 [-100.000,  6.689], mean action: 1.448 [0.000, 3.000],  loss: 6.920134, mae: 28.131564, mean_q: 20.403475, mean_eps: 0.818920
  30339/150000: episode: 308, duration: 0.727s, episode steps: 125, steps per second: 172, episode reward: -118.497, mean reward: -0.948 [-100.000,  7.399], mean action: 1.512 [0.000, 3.000],  loss: 9.762706, mae: 27.831519, mean_q: 20.291297, mean_eps: 0.818344
  30435/150000: episode: 309, duration: 0.593s, episode steps:  96, steps per second: 162, episode reward: -108.151, mean reward: -1.127 [-100.000,  7.184], mean action: 1.625 [0.000, 3.000],  loss: 9.731521, mae: 27.967201, mean_q: 21.581275, mean_eps: 0.817681
  30507/150000: episode: 310, duration: 0.430s, episode steps:  72, steps per second: 167, episode reward: -87.848, mean reward: -1.220 [-100.000, 13.049], mean action: 1.292 [0.000, 3.000],  loss: 9.108558, mae: 27.888333, mean_q: 21.849070, mean_eps: 0.817177
  30605/150000: episode: 311, duration: 0.600s, episode steps:  98, steps per second: 163, episode reward: -90.709, mean reward: -0.926 [-100.000, 21.326], mean action: 1.786 [0.000, 3.000],  loss: 13.391977, mae: 27.836312, mean_q: 20.331061, mean_eps: 0.816667
  30693/150000: episode: 312, duration: 0.503s, episode steps:  88, steps per second: 175, episode reward: -84.901, mean reward: -0.965 [-100.000,  9.468], mean action: 1.443 [0.000, 3.000],  loss: 10.598713, mae: 27.933584, mean_q: 21.013192, mean_eps: 0.816109
  30761/150000: episode: 313, duration: 0.415s, episode steps:  68, steps per second: 164, episode reward: -92.200, mean reward: -1.356 [-100.000,  7.681], mean action: 1.882 [0.000, 3.000],  loss: 12.225711, mae: 28.194479, mean_q: 20.696471, mean_eps: 0.815641
  30836/150000: episode: 314, duration: 0.434s, episode steps:  75, steps per second: 173, episode reward: -76.886, mean reward: -1.025 [-100.000,  8.781], mean action: 1.667 [0.000, 3.000],  loss: 9.744541, mae: 28.273549, mean_q: 21.625431, mean_eps: 0.815212
  30925/150000: episode: 315, duration: 0.546s, episode steps:  89, steps per second: 163, episode reward: -243.061, mean reward: -2.731 [-100.000,  6.456], mean action: 1.685 [0.000, 3.000],  loss: 7.482648, mae: 28.197236, mean_q: 20.640384, mean_eps: 0.814720
  31007/150000: episode: 316, duration: 0.490s, episode steps:  82, steps per second: 167, episode reward: -56.968, mean reward: -0.695 [-100.000, 13.251], mean action: 1.256 [0.000, 3.000],  loss: 12.316033, mae: 28.127731, mean_q: 21.028756, mean_eps: 0.814207
  31076/150000: episode: 317, duration: 0.404s, episode steps:  69, steps per second: 171, episode reward: -52.342, mean reward: -0.759 [-100.000, 12.177], mean action: 1.478 [0.000, 3.000],  loss: 12.124894, mae: 29.043478, mean_q: 21.040507, mean_eps: 0.813754
  31204/150000: episode: 318, duration: 0.770s, episode steps: 128, steps per second: 166, episode reward: -85.220, mean reward: -0.666 [-100.000, 13.054], mean action: 1.695 [0.000, 3.000],  loss: 7.520037, mae: 28.992994, mean_q: 21.217303, mean_eps: 0.813163
  31280/150000: episode: 319, duration: 0.433s, episode steps:  76, steps per second: 175, episode reward: -71.107, mean reward: -0.936 [-100.000,  9.485], mean action: 1.447 [0.000, 3.000],  loss: 7.845408, mae: 29.296952, mean_q: 20.850873, mean_eps: 0.812551
  31411/150000: episode: 320, duration: 0.796s, episode steps: 131, steps per second: 165, episode reward: -103.104, mean reward: -0.787 [-100.000,  6.521], mean action: 1.733 [0.000, 3.000],  loss: 10.522110, mae: 29.507226, mean_q: 21.128034, mean_eps: 0.811930
  31482/150000: episode: 321, duration: 0.445s, episode steps:  71, steps per second: 160, episode reward: -64.678, mean reward: -0.911 [-100.000, 16.237], mean action: 1.577 [0.000, 3.000],  loss: 11.149505, mae: 29.824388, mean_q: 22.414177, mean_eps: 0.811324
  31562/150000: episode: 322, duration: 0.492s, episode steps:  80, steps per second: 163, episode reward: -32.139, mean reward: -0.402 [-100.000, 22.021], mean action: 1.637 [0.000, 3.000],  loss: 16.377708, mae: 29.199105, mean_q: 22.300444, mean_eps: 0.810871
  31684/150000: episode: 323, duration: 0.723s, episode steps: 122, steps per second: 169, episode reward: -95.369, mean reward: -0.782 [-100.000,  7.013], mean action: 1.648 [0.000, 3.000],  loss: 9.909343, mae: 29.161838, mean_q: 21.572158, mean_eps: 0.810265
  31765/150000: episode: 324, duration: 0.484s, episode steps:  81, steps per second: 167, episode reward: -137.901, mean reward: -1.702 [-100.000,  6.699], mean action: 1.519 [0.000, 3.000],  loss: 8.269853, mae: 29.263364, mean_q: 22.708417, mean_eps: 0.809656
  31895/150000: episode: 325, duration: 0.826s, episode steps: 130, steps per second: 157, episode reward: -58.861, mean reward: -0.453 [-100.000, 17.032], mean action: 1.554 [0.000, 3.000],  loss: 10.444164, mae: 29.270232, mean_q: 21.448879, mean_eps: 0.809023
  31976/150000: episode: 326, duration: 0.497s, episode steps:  81, steps per second: 163, episode reward: -10.420, mean reward: -0.129 [-100.000, 20.974], mean action: 1.667 [0.000, 3.000],  loss: 12.723341, mae: 29.544543, mean_q: 22.335782, mean_eps: 0.808390
  32064/150000: episode: 327, duration: 0.551s, episode steps:  88, steps per second: 160, episode reward: -92.436, mean reward: -1.050 [-100.000,  6.872], mean action: 1.307 [0.000, 3.000],  loss: 8.712400, mae: 29.424389, mean_q: 23.052904, mean_eps: 0.807883
  32153/150000: episode: 328, duration: 0.592s, episode steps:  89, steps per second: 150, episode reward: -55.354, mean reward: -0.622 [-100.000,  7.140], mean action: 1.584 [0.000, 3.000],  loss: 18.100500, mae: 30.716479, mean_q: 21.401925, mean_eps: 0.807352
  32241/150000: episode: 329, duration: 0.607s, episode steps:  88, steps per second: 145, episode reward: -44.373, mean reward: -0.504 [-100.000, 12.443], mean action: 1.432 [0.000, 3.000],  loss: 9.098153, mae: 29.965280, mean_q: 21.409879, mean_eps: 0.806821
  32384/150000: episode: 330, duration: 0.902s, episode steps: 143, steps per second: 158, episode reward: -77.334, mean reward: -0.541 [-100.000, 15.627], mean action: 1.692 [0.000, 3.000],  loss: 16.766376, mae: 29.815249, mean_q: 21.957822, mean_eps: 0.806128
  32471/150000: episode: 331, duration: 0.569s, episode steps:  87, steps per second: 153, episode reward: -93.436, mean reward: -1.074 [-100.000, 15.847], mean action: 1.655 [0.000, 3.000],  loss: 11.267513, mae: 30.252328, mean_q: 23.233587, mean_eps: 0.805438
  32577/150000: episode: 332, duration: 0.710s, episode steps: 106, steps per second: 149, episode reward: -93.529, mean reward: -0.882 [-100.000, 13.184], mean action: 1.491 [0.000, 3.000],  loss: 7.323578, mae: 30.243128, mean_q: 22.366856, mean_eps: 0.804859
  32684/150000: episode: 333, duration: 0.687s, episode steps: 107, steps per second: 156, episode reward: -80.063, mean reward: -0.748 [-100.000, 11.848], mean action: 1.617 [0.000, 3.000],  loss: 12.254184, mae: 29.519525, mean_q: 22.546437, mean_eps: 0.804220
  32770/150000: episode: 334, duration: 0.565s, episode steps:  86, steps per second: 152, episode reward: -67.198, mean reward: -0.781 [-100.000, 11.105], mean action: 1.535 [0.000, 3.000],  loss: 16.939565, mae: 30.224144, mean_q: 22.875294, mean_eps: 0.803641
  32866/150000: episode: 335, duration: 0.673s, episode steps:  96, steps per second: 143, episode reward: -71.768, mean reward: -0.748 [-100.000, 30.564], mean action: 1.677 [0.000, 3.000],  loss: 16.388218, mae: 30.223211, mean_q: 22.494860, mean_eps: 0.803095
  32968/150000: episode: 336, duration: 0.658s, episode steps: 102, steps per second: 155, episode reward: -106.275, mean reward: -1.042 [-100.000,  6.635], mean action: 1.539 [0.000, 3.000],  loss: 21.765358, mae: 29.872623, mean_q: 23.443158, mean_eps: 0.802501
  33082/150000: episode: 337, duration: 0.715s, episode steps: 114, steps per second: 160, episode reward: -74.040, mean reward: -0.649 [-100.000, 17.710], mean action: 1.623 [0.000, 3.000],  loss: 9.788467, mae: 30.567219, mean_q: 22.707546, mean_eps: 0.801853
  33208/150000: episode: 338, duration: 0.818s, episode steps: 126, steps per second: 154, episode reward: -39.703, mean reward: -0.315 [-100.000, 10.205], mean action: 1.611 [0.000, 3.000],  loss: 12.564271, mae: 31.092334, mean_q: 22.734942, mean_eps: 0.801133
  33316/150000: episode: 339, duration: 0.699s, episode steps: 108, steps per second: 154, episode reward: -97.089, mean reward: -0.899 [-100.000,  9.637], mean action: 1.583 [0.000, 3.000],  loss: 16.270626, mae: 30.661919, mean_q: 22.085738, mean_eps: 0.800431
  33469/150000: episode: 340, duration: 0.915s, episode steps: 153, steps per second: 167, episode reward: -16.697, mean reward: -0.109 [-100.000, 21.095], mean action: 1.634 [0.000, 3.000],  loss: 10.627756, mae: 30.949098, mean_q: 22.328073, mean_eps: 0.799648
  33543/150000: episode: 341, duration: 0.464s, episode steps:  74, steps per second: 160, episode reward: -73.029, mean reward: -0.987 [-100.000,  8.498], mean action: 1.392 [0.000, 3.000],  loss: 14.970107, mae: 31.586369, mean_q: 21.734268, mean_eps: 0.798967
  33650/150000: episode: 342, duration: 0.638s, episode steps: 107, steps per second: 168, episode reward: -85.449, mean reward: -0.799 [-100.000, 22.327], mean action: 1.589 [0.000, 3.000],  loss: 15.040943, mae: 30.790307, mean_q: 22.165420, mean_eps: 0.798424
  33750/150000: episode: 343, duration: 0.586s, episode steps: 100, steps per second: 171, episode reward: -245.809, mean reward: -2.458 [-100.000, 67.581], mean action: 1.450 [0.000, 3.000],  loss: 14.475878, mae: 31.062756, mean_q: 22.264712, mean_eps: 0.797803
  33880/150000: episode: 344, duration: 0.771s, episode steps: 130, steps per second: 169, episode reward: -37.539, mean reward: -0.289 [-100.000, 16.211], mean action: 1.569 [0.000, 3.000],  loss: 13.427378, mae: 31.324171, mean_q: 22.016125, mean_eps: 0.797113
  33994/150000: episode: 345, duration: 0.656s, episode steps: 114, steps per second: 174, episode reward: -71.313, mean reward: -0.626 [-100.000, 13.132], mean action: 1.632 [0.000, 3.000],  loss: 12.901737, mae: 30.919358, mean_q: 22.884879, mean_eps: 0.796381
  34068/150000: episode: 346, duration: 0.459s, episode steps:  74, steps per second: 161, episode reward: -43.826, mean reward: -0.592 [-100.000,  7.097], mean action: 1.527 [0.000, 3.000],  loss: 12.560319, mae: 31.308513, mean_q: 23.946899, mean_eps: 0.795817
  34198/150000: episode: 347, duration: 0.815s, episode steps: 130, steps per second: 160, episode reward: -144.705, mean reward: -1.113 [-100.000,  9.155], mean action: 1.631 [0.000, 3.000],  loss: 12.933705, mae: 31.429276, mean_q: 23.853199, mean_eps: 0.795205
  34278/150000: episode: 348, duration: 0.493s, episode steps:  80, steps per second: 162, episode reward: -59.075, mean reward: -0.738 [-100.000, 19.882], mean action: 1.575 [0.000, 3.000],  loss: 20.326940, mae: 32.020426, mean_q: 23.351423, mean_eps: 0.794575
  34361/150000: episode: 349, duration: 0.476s, episode steps:  83, steps per second: 174, episode reward: -120.114, mean reward: -1.447 [-100.000, 11.569], mean action: 1.422 [0.000, 3.000],  loss: 8.073219, mae: 32.089861, mean_q: 23.796886, mean_eps: 0.794086
  34477/150000: episode: 350, duration: 0.708s, episode steps: 116, steps per second: 164, episode reward: -119.391, mean reward: -1.029 [-100.000, 11.209], mean action: 1.500 [0.000, 3.000],  loss: 6.986516, mae: 31.895584, mean_q: 24.150035, mean_eps: 0.793489
  34587/150000: episode: 351, duration: 0.662s, episode steps: 110, steps per second: 166, episode reward: -100.086, mean reward: -0.910 [-100.000, 16.690], mean action: 1.618 [0.000, 3.000],  loss: 12.651340, mae: 32.100906, mean_q: 23.165562, mean_eps: 0.792811
  34706/150000: episode: 352, duration: 0.740s, episode steps: 119, steps per second: 161, episode reward: -62.625, mean reward: -0.526 [-100.000,  7.157], mean action: 1.487 [0.000, 3.000],  loss: 19.990871, mae: 31.665138, mean_q: 24.649829, mean_eps: 0.792124
  34809/150000: episode: 353, duration: 0.634s, episode steps: 103, steps per second: 162, episode reward: -183.906, mean reward: -1.785 [-100.000,  5.154], mean action: 1.456 [0.000, 3.000],  loss: 15.966617, mae: 31.575774, mean_q: 24.322965, mean_eps: 0.791458
  34909/150000: episode: 354, duration: 0.589s, episode steps: 100, steps per second: 170, episode reward: -113.043, mean reward: -1.130 [-100.000,  7.346], mean action: 1.630 [0.000, 3.000],  loss: 16.163790, mae: 31.560892, mean_q: 23.910440, mean_eps: 0.790849
  35015/150000: episode: 355, duration: 0.698s, episode steps: 106, steps per second: 152, episode reward: -108.522, mean reward: -1.024 [-100.000, 10.602], mean action: 1.349 [0.000, 3.000],  loss: 12.435976, mae: 31.843591, mean_q: 23.807785, mean_eps: 0.790231
  35076/150000: episode: 356, duration: 0.386s, episode steps:  61, steps per second: 158, episode reward: -57.253, mean reward: -0.939 [-100.000, 14.568], mean action: 1.590 [0.000, 3.000],  loss: 7.629700, mae: 31.916448, mean_q: 23.360576, mean_eps: 0.789730
  35193/150000: episode: 357, duration: 0.731s, episode steps: 117, steps per second: 160, episode reward: -127.602, mean reward: -1.091 [-100.000, 11.114], mean action: 1.658 [0.000, 3.000],  loss: 10.958352, mae: 32.000658, mean_q: 23.452938, mean_eps: 0.789196
  35280/150000: episode: 358, duration: 0.556s, episode steps:  87, steps per second: 156, episode reward: -87.441, mean reward: -1.005 [-100.000,  5.101], mean action: 1.575 [0.000, 3.000],  loss: 12.296889, mae: 32.328196, mean_q: 24.261490, mean_eps: 0.788584
  35380/150000: episode: 359, duration: 0.652s, episode steps: 100, steps per second: 153, episode reward: 41.182, mean reward:  0.412 [-100.000, 48.153], mean action: 1.940 [0.000, 3.000],  loss: 15.081804, mae: 32.564512, mean_q: 22.967692, mean_eps: 0.788023
  35479/150000: episode: 360, duration: 0.803s, episode steps:  99, steps per second: 123, episode reward: -76.180, mean reward: -0.769 [-100.000, 22.848], mean action: 1.606 [0.000, 3.000],  loss: 11.174872, mae: 32.049208, mean_q: 23.221542, mean_eps: 0.787426
  35592/150000: episode: 361, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -79.933, mean reward: -0.707 [-100.000,  8.175], mean action: 1.681 [0.000, 3.000],  loss: 10.835026, mae: 31.510388, mean_q: 24.238944, mean_eps: 0.786790
  35687/150000: episode: 362, duration: 0.632s, episode steps:  95, steps per second: 150, episode reward: -103.433, mean reward: -1.089 [-100.000,  8.645], mean action: 1.589 [0.000, 3.000],  loss: 8.256045, mae: 31.800262, mean_q: 22.993195, mean_eps: 0.786166
  35804/150000: episode: 363, duration: 0.783s, episode steps: 117, steps per second: 149, episode reward: -124.489, mean reward: -1.064 [-100.000, 16.411], mean action: 1.496 [0.000, 3.000],  loss: 10.215824, mae: 32.653039, mean_q: 22.957326, mean_eps: 0.785530
  35941/150000: episode: 364, duration: 0.863s, episode steps: 137, steps per second: 159, episode reward:  0.726, mean reward:  0.005 [-100.000, 60.346], mean action: 1.650 [0.000, 3.000],  loss: 10.262499, mae: 32.246286, mean_q: 23.683623, mean_eps: 0.784768
  36023/150000: episode: 365, duration: 0.503s, episode steps:  82, steps per second: 163, episode reward: -55.067, mean reward: -0.672 [-100.000, 14.178], mean action: 1.488 [0.000, 3.000],  loss: 12.100896, mae: 32.269964, mean_q: 24.587161, mean_eps: 0.784111
  36103/150000: episode: 366, duration: 0.460s, episode steps:  80, steps per second: 174, episode reward: -55.211, mean reward: -0.690 [-100.000,  8.893], mean action: 1.525 [0.000, 3.000],  loss: 9.021057, mae: 32.965368, mean_q: 24.745602, mean_eps: 0.783625
  36199/150000: episode: 367, duration: 0.579s, episode steps:  96, steps per second: 166, episode reward: -104.624, mean reward: -1.090 [-100.000,  7.006], mean action: 1.385 [0.000, 3.000],  loss: 10.787983, mae: 33.223958, mean_q: 25.590571, mean_eps: 0.783097
  36306/150000: episode: 368, duration: 0.689s, episode steps: 107, steps per second: 155, episode reward: -76.588, mean reward: -0.716 [-100.000, 14.609], mean action: 1.411 [0.000, 3.000],  loss: 10.696738, mae: 33.026632, mean_q: 26.081054, mean_eps: 0.782488
  36405/150000: episode: 369, duration: 0.587s, episode steps:  99, steps per second: 169, episode reward: -82.191, mean reward: -0.830 [-100.000, 11.124], mean action: 1.616 [0.000, 3.000],  loss: 12.337943, mae: 33.057913, mean_q: 25.479056, mean_eps: 0.781870
  36478/150000: episode: 370, duration: 0.440s, episode steps:  73, steps per second: 166, episode reward: -95.867, mean reward: -1.313 [-100.000, 11.877], mean action: 1.616 [0.000, 3.000],  loss: 22.276297, mae: 33.183572, mean_q: 25.492114, mean_eps: 0.781354
  36570/150000: episode: 371, duration: 0.662s, episode steps:  92, steps per second: 139, episode reward: -61.692, mean reward: -0.671 [-100.000, 11.720], mean action: 1.554 [0.000, 3.000],  loss: 9.023364, mae: 33.155646, mean_q: 25.171407, mean_eps: 0.780859
  36713/150000: episode: 372, duration: 1.061s, episode steps: 143, steps per second: 135, episode reward: -54.103, mean reward: -0.378 [-100.000, 10.754], mean action: 1.510 [0.000, 3.000],  loss: 9.531656, mae: 33.346292, mean_q: 25.341220, mean_eps: 0.780154
  36804/150000: episode: 373, duration: 0.651s, episode steps:  91, steps per second: 140, episode reward: -231.483, mean reward: -2.544 [-100.000,  4.359], mean action: 1.659 [0.000, 3.000],  loss: 8.776479, mae: 33.128590, mean_q: 25.953356, mean_eps: 0.779452
  36893/150000: episode: 374, duration: 0.646s, episode steps:  89, steps per second: 138, episode reward: -52.439, mean reward: -0.589 [-100.000, 17.679], mean action: 1.427 [0.000, 3.000],  loss: 12.644256, mae: 32.878248, mean_q: 25.551321, mean_eps: 0.778912
  37013/150000: episode: 375, duration: 0.868s, episode steps: 120, steps per second: 138, episode reward: -104.343, mean reward: -0.870 [-100.000, 17.357], mean action: 1.492 [0.000, 3.000],  loss: 10.010803, mae: 33.393452, mean_q: 25.807356, mean_eps: 0.778285
  37147/150000: episode: 376, duration: 0.877s, episode steps: 134, steps per second: 153, episode reward: -82.892, mean reward: -0.619 [-100.000,  6.280], mean action: 1.619 [0.000, 3.000],  loss: 7.211152, mae: 33.487821, mean_q: 26.332903, mean_eps: 0.777523
  37222/150000: episode: 377, duration: 0.517s, episode steps:  75, steps per second: 145, episode reward: -91.506, mean reward: -1.220 [-100.000,  6.238], mean action: 1.387 [0.000, 3.000],  loss: 10.680290, mae: 33.451036, mean_q: 26.633790, mean_eps: 0.776896
  37350/150000: episode: 378, duration: 0.752s, episode steps: 128, steps per second: 170, episode reward: -130.116, mean reward: -1.017 [-100.000,  5.049], mean action: 1.414 [0.000, 3.000],  loss: 12.673476, mae: 33.461618, mean_q: 26.944872, mean_eps: 0.776287
  37433/150000: episode: 379, duration: 0.486s, episode steps:  83, steps per second: 171, episode reward: -108.129, mean reward: -1.303 [-100.000,  7.085], mean action: 1.590 [0.000, 3.000],  loss: 14.779403, mae: 33.454490, mean_q: 27.125226, mean_eps: 0.775654
  37516/150000: episode: 380, duration: 0.506s, episode steps:  83, steps per second: 164, episode reward: -97.748, mean reward: -1.178 [-100.000,  7.457], mean action: 1.398 [0.000, 3.000],  loss: 14.893754, mae: 33.602702, mean_q: 27.240565, mean_eps: 0.775156
  37631/150000: episode: 381, duration: 0.759s, episode steps: 115, steps per second: 152, episode reward: -62.548, mean reward: -0.544 [-100.000,  9.400], mean action: 1.600 [0.000, 3.000],  loss: 11.698124, mae: 33.147872, mean_q: 27.241954, mean_eps: 0.774562
  37704/150000: episode: 382, duration: 0.438s, episode steps:  73, steps per second: 167, episode reward: -63.381, mean reward: -0.868 [-100.000,  8.665], mean action: 1.507 [0.000, 3.000],  loss: 7.791496, mae: 33.421537, mean_q: 26.861116, mean_eps: 0.773998
  37784/150000: episode: 383, duration: 0.468s, episode steps:  80, steps per second: 171, episode reward: -46.344, mean reward: -0.579 [-100.000, 13.350], mean action: 1.462 [0.000, 3.000],  loss: 11.033855, mae: 33.263773, mean_q: 25.250554, mean_eps: 0.773539
  37875/150000: episode: 384, duration: 0.524s, episode steps:  91, steps per second: 174, episode reward: -86.149, mean reward: -0.947 [-100.000, 12.369], mean action: 1.791 [0.000, 3.000],  loss: 9.199532, mae: 33.747607, mean_q: 25.445858, mean_eps: 0.773026
  38013/150000: episode: 385, duration: 0.837s, episode steps: 138, steps per second: 165, episode reward: -149.593, mean reward: -1.084 [-100.000,  5.211], mean action: 1.413 [0.000, 3.000],  loss: 11.602064, mae: 33.380939, mean_q: 25.667839, mean_eps: 0.772339
  38107/150000: episode: 386, duration: 0.579s, episode steps:  94, steps per second: 162, episode reward: -21.029, mean reward: -0.224 [-100.000, 12.924], mean action: 1.606 [0.000, 3.000],  loss: 9.484315, mae: 33.564096, mean_q: 25.405309, mean_eps: 0.771643
  38189/150000: episode: 387, duration: 0.512s, episode steps:  82, steps per second: 160, episode reward: -127.020, mean reward: -1.549 [-100.000, 11.117], mean action: 1.610 [0.000, 3.000],  loss: 5.809330, mae: 33.450296, mean_q: 26.820674, mean_eps: 0.771115
  38271/150000: episode: 388, duration: 0.552s, episode steps:  82, steps per second: 149, episode reward: -94.272, mean reward: -1.150 [-100.000, 16.031], mean action: 1.793 [0.000, 3.000],  loss: 15.211974, mae: 33.411506, mean_q: 27.223642, mean_eps: 0.770623
  38395/150000: episode: 389, duration: 0.865s, episode steps: 124, steps per second: 143, episode reward: -144.207, mean reward: -1.163 [-100.000, 13.928], mean action: 1.605 [0.000, 3.000],  loss: 18.892853, mae: 33.752003, mean_q: 26.803944, mean_eps: 0.770005
  38466/150000: episode: 390, duration: 0.497s, episode steps:  71, steps per second: 143, episode reward: -44.348, mean reward: -0.625 [-100.000, 13.898], mean action: 1.521 [0.000, 3.000],  loss: 12.277295, mae: 34.019760, mean_q: 25.059499, mean_eps: 0.769420
  38570/150000: episode: 391, duration: 0.781s, episode steps: 104, steps per second: 133, episode reward: -83.416, mean reward: -0.802 [-100.000,  7.012], mean action: 1.779 [0.000, 3.000],  loss: 9.693136, mae: 33.358797, mean_q: 26.185983, mean_eps: 0.768895
  38652/150000: episode: 392, duration: 0.665s, episode steps:  82, steps per second: 123, episode reward: -64.120, mean reward: -0.782 [-100.000,  9.890], mean action: 1.622 [0.000, 3.000],  loss: 8.583104, mae: 33.718742, mean_q: 25.324057, mean_eps: 0.768337
  38732/150000: episode: 393, duration: 0.784s, episode steps:  80, steps per second: 102, episode reward: -71.027, mean reward: -0.888 [-100.000,  6.496], mean action: 1.637 [0.000, 3.000],  loss: 13.609773, mae: 34.039797, mean_q: 27.967046, mean_eps: 0.767851
  38840/150000: episode: 394, duration: 0.967s, episode steps: 108, steps per second: 112, episode reward: -72.737, mean reward: -0.673 [-100.000, 10.721], mean action: 1.694 [0.000, 3.000],  loss: 20.778677, mae: 33.884615, mean_q: 26.206372, mean_eps: 0.767287
  38952/150000: episode: 395, duration: 1.323s, episode steps: 112, steps per second:  85, episode reward: -79.421, mean reward: -0.709 [-100.000, 10.201], mean action: 1.732 [0.000, 3.000],  loss: 11.031381, mae: 33.864687, mean_q: 26.515569, mean_eps: 0.766627
  39050/150000: episode: 396, duration: 0.987s, episode steps:  98, steps per second:  99, episode reward: -93.372, mean reward: -0.953 [-100.000,  6.909], mean action: 1.520 [0.000, 3.000],  loss: 15.349073, mae: 33.859573, mean_q: 26.844767, mean_eps: 0.765997
  39184/150000: episode: 397, duration: 1.516s, episode steps: 134, steps per second:  88, episode reward: -185.189, mean reward: -1.382 [-100.000, 20.270], mean action: 1.612 [0.000, 3.000],  loss: 13.854603, mae: 33.972942, mean_q: 28.005503, mean_eps: 0.765301
  39297/150000: episode: 398, duration: 1.067s, episode steps: 113, steps per second: 106, episode reward: -105.089, mean reward: -0.930 [-100.000, 11.208], mean action: 1.690 [0.000, 3.000],  loss: 17.252209, mae: 33.500200, mean_q: 27.169742, mean_eps: 0.764560
  39368/150000: episode: 399, duration: 0.480s, episode steps:  71, steps per second: 148, episode reward: -53.015, mean reward: -0.747 [-100.000, 11.265], mean action: 1.732 [0.000, 3.000],  loss: 12.728969, mae: 33.245793, mean_q: 25.620193, mean_eps: 0.764008
  39467/150000: episode: 400, duration: 0.618s, episode steps:  99, steps per second: 160, episode reward: -99.615, mean reward: -1.006 [-100.000,  8.870], mean action: 1.626 [0.000, 3.000],  loss: 12.043440, mae: 34.075248, mean_q: 26.404758, mean_eps: 0.763498
  39602/150000: episode: 401, duration: 0.846s, episode steps: 135, steps per second: 160, episode reward: -22.675, mean reward: -0.168 [-100.000, 17.158], mean action: 1.533 [0.000, 3.000],  loss: 12.832813, mae: 34.191126, mean_q: 26.803610, mean_eps: 0.762796
  39697/150000: episode: 402, duration: 0.607s, episode steps:  95, steps per second: 156, episode reward: -116.856, mean reward: -1.230 [-100.000, 19.486], mean action: 1.642 [0.000, 3.000],  loss: 15.231385, mae: 34.272694, mean_q: 25.909254, mean_eps: 0.762106
  39804/150000: episode: 403, duration: 0.672s, episode steps: 107, steps per second: 159, episode reward: -86.798, mean reward: -0.811 [-100.000,  7.974], mean action: 1.757 [0.000, 3.000],  loss: 11.024087, mae: 33.747644, mean_q: 27.353625, mean_eps: 0.761500
  39898/150000: episode: 404, duration: 0.616s, episode steps:  94, steps per second: 153, episode reward: 14.679, mean reward:  0.156 [-100.000, 21.155], mean action: 1.564 [0.000, 3.000],  loss: 6.670669, mae: 33.695929, mean_q: 25.672884, mean_eps: 0.760897
  39998/150000: episode: 405, duration: 0.602s, episode steps: 100, steps per second: 166, episode reward: -31.951, mean reward: -0.320 [-100.000, 19.668], mean action: 1.650 [0.000, 3.000],  loss: 15.046624, mae: 33.651398, mean_q: 27.594410, mean_eps: 0.760315
  40110/150000: episode: 406, duration: 0.673s, episode steps: 112, steps per second: 166, episode reward: -98.717, mean reward: -0.881 [-100.000, 13.530], mean action: 1.616 [0.000, 3.000],  loss: 11.392441, mae: 33.791225, mean_q: 27.083330, mean_eps: 0.759679
  40220/150000: episode: 407, duration: 0.697s, episode steps: 110, steps per second: 158, episode reward: -47.950, mean reward: -0.436 [-100.000, 12.369], mean action: 1.655 [0.000, 3.000],  loss: 16.085268, mae: 33.505307, mean_q: 27.144320, mean_eps: 0.759013
  40324/150000: episode: 408, duration: 0.651s, episode steps: 104, steps per second: 160, episode reward: -209.092, mean reward: -2.010 [-100.000, 20.985], mean action: 1.490 [0.000, 3.000],  loss: 8.119001, mae: 33.138790, mean_q: 25.768417, mean_eps: 0.758371
  40464/150000: episode: 409, duration: 0.849s, episode steps: 140, steps per second: 165, episode reward: -40.718, mean reward: -0.291 [-100.000, 11.668], mean action: 1.586 [0.000, 3.000],  loss: 14.126690, mae: 33.168654, mean_q: 25.839560, mean_eps: 0.757639
  40539/150000: episode: 410, duration: 0.447s, episode steps:  75, steps per second: 168, episode reward: -75.489, mean reward: -1.007 [-100.000, 22.926], mean action: 1.547 [0.000, 3.000],  loss: 13.826430, mae: 34.123278, mean_q: 25.686423, mean_eps: 0.756994
  40612/150000: episode: 411, duration: 0.458s, episode steps:  73, steps per second: 159, episode reward: -76.703, mean reward: -1.051 [-100.000, 12.719], mean action: 1.438 [0.000, 3.000],  loss: 17.927507, mae: 33.177713, mean_q: 26.715217, mean_eps: 0.756550
  40681/150000: episode: 412, duration: 0.407s, episode steps:  69, steps per second: 170, episode reward: -55.451, mean reward: -0.804 [-100.000,  8.192], mean action: 1.551 [0.000, 3.000],  loss: 12.265558, mae: 33.730366, mean_q: 26.323785, mean_eps: 0.756124
  40789/150000: episode: 413, duration: 0.657s, episode steps: 108, steps per second: 164, episode reward: -94.657, mean reward: -0.876 [-100.000,  9.599], mean action: 1.593 [0.000, 3.000],  loss: 11.403034, mae: 33.665607, mean_q: 26.286838, mean_eps: 0.755593
  40905/150000: episode: 414, duration: 0.718s, episode steps: 116, steps per second: 162, episode reward: -94.833, mean reward: -0.818 [-100.000, 14.593], mean action: 1.526 [0.000, 3.000],  loss: 8.533530, mae: 33.550742, mean_q: 26.506928, mean_eps: 0.754921
  40999/150000: episode: 415, duration: 0.594s, episode steps:  94, steps per second: 158, episode reward: -63.920, mean reward: -0.680 [-100.000,  6.897], mean action: 1.415 [0.000, 3.000],  loss: 6.686142, mae: 33.835215, mean_q: 26.888616, mean_eps: 0.754291
  41094/150000: episode: 416, duration: 0.574s, episode steps:  95, steps per second: 166, episode reward: -30.734, mean reward: -0.324 [-100.000, 16.849], mean action: 1.589 [0.000, 3.000],  loss: 10.142957, mae: 34.153333, mean_q: 24.990016, mean_eps: 0.753724
  41171/150000: episode: 417, duration: 0.468s, episode steps:  77, steps per second: 165, episode reward: -99.212, mean reward: -1.288 [-100.000,  9.077], mean action: 1.455 [0.000, 3.000],  loss: 7.934626, mae: 34.000092, mean_q: 26.685014, mean_eps: 0.753208
  41262/150000: episode: 418, duration: 0.576s, episode steps:  91, steps per second: 158, episode reward: -56.471, mean reward: -0.621 [-100.000, 19.268], mean action: 1.440 [0.000, 3.000],  loss: 13.386138, mae: 34.028271, mean_q: 27.147984, mean_eps: 0.752704
  41381/150000: episode: 419, duration: 0.756s, episode steps: 119, steps per second: 157, episode reward: -106.876, mean reward: -0.898 [-100.000, 10.893], mean action: 1.782 [0.000, 3.000],  loss: 9.314424, mae: 34.132869, mean_q: 26.121026, mean_eps: 0.752074
  41451/150000: episode: 420, duration: 0.407s, episode steps:  70, steps per second: 172, episode reward: -79.149, mean reward: -1.131 [-100.000,  6.499], mean action: 1.471 [0.000, 3.000],  loss: 7.875761, mae: 34.665281, mean_q: 26.112034, mean_eps: 0.751507
  41577/150000: episode: 421, duration: 0.728s, episode steps: 126, steps per second: 173, episode reward: -33.035, mean reward: -0.262 [-100.000, 48.112], mean action: 1.516 [0.000, 3.000],  loss: 9.029872, mae: 33.767152, mean_q: 24.861052, mean_eps: 0.750919
  41646/150000: episode: 422, duration: 0.436s, episode steps:  69, steps per second: 158, episode reward: -45.747, mean reward: -0.663 [-100.000, 10.973], mean action: 1.565 [0.000, 3.000],  loss: 10.563748, mae: 34.088043, mean_q: 28.187805, mean_eps: 0.750334
  41764/150000: episode: 423, duration: 0.694s, episode steps: 118, steps per second: 170, episode reward: -42.055, mean reward: -0.356 [-100.000, 16.390], mean action: 1.551 [0.000, 3.000],  loss: 7.828227, mae: 33.697393, mean_q: 24.861029, mean_eps: 0.749773
  41831/150000: episode: 424, duration: 0.396s, episode steps:  67, steps per second: 169, episode reward: -47.592, mean reward: -0.710 [-100.000, 14.933], mean action: 1.642 [0.000, 3.000],  loss: 11.685313, mae: 33.757282, mean_q: 25.656869, mean_eps: 0.749218
  41942/150000: episode: 425, duration: 0.645s, episode steps: 111, steps per second: 172, episode reward: -92.288, mean reward: -0.831 [-100.000, 18.595], mean action: 1.306 [0.000, 3.000],  loss: 8.699208, mae: 34.429487, mean_q: 24.786889, mean_eps: 0.748684
  42020/150000: episode: 426, duration: 0.494s, episode steps:  78, steps per second: 158, episode reward: -36.673, mean reward: -0.470 [-100.000, 14.719], mean action: 1.321 [0.000, 3.000],  loss: 9.555201, mae: 33.915045, mean_q: 25.690403, mean_eps: 0.748117
  42140/150000: episode: 427, duration: 0.721s, episode steps: 120, steps per second: 166, episode reward: -132.860, mean reward: -1.107 [-100.000,  9.447], mean action: 1.483 [0.000, 3.000],  loss: 7.986408, mae: 34.479659, mean_q: 26.517254, mean_eps: 0.747523
  42248/150000: episode: 428, duration: 0.636s, episode steps: 108, steps per second: 170, episode reward: -64.128, mean reward: -0.594 [-100.000, 11.941], mean action: 1.583 [0.000, 3.000],  loss: 12.414388, mae: 34.432384, mean_q: 25.096684, mean_eps: 0.746839
  42337/150000: episode: 429, duration: 0.546s, episode steps:  89, steps per second: 163, episode reward: -86.095, mean reward: -0.967 [-100.000,  9.151], mean action: 1.461 [0.000, 3.000],  loss: 7.947418, mae: 34.614742, mean_q: 27.234784, mean_eps: 0.746248
  42441/150000: episode: 430, duration: 0.637s, episode steps: 104, steps per second: 163, episode reward: -81.338, mean reward: -0.782 [-100.000,  7.803], mean action: 1.471 [0.000, 3.000],  loss: 11.940005, mae: 33.543467, mean_q: 24.006781, mean_eps: 0.745669
  42559/150000: episode: 431, duration: 0.688s, episode steps: 118, steps per second: 172, episode reward: -152.736, mean reward: -1.294 [-100.000,  5.769], mean action: 1.754 [0.000, 3.000],  loss: 17.085631, mae: 34.485120, mean_q: 26.834772, mean_eps: 0.745003
  42648/150000: episode: 432, duration: 0.554s, episode steps:  89, steps per second: 161, episode reward: -123.007, mean reward: -1.382 [-100.000,  7.067], mean action: 1.539 [0.000, 3.000],  loss: 11.712409, mae: 33.824575, mean_q: 27.245841, mean_eps: 0.744382
  42754/150000: episode: 433, duration: 0.714s, episode steps: 106, steps per second: 148, episode reward: -58.790, mean reward: -0.555 [-100.000, 13.159], mean action: 1.566 [0.000, 3.000],  loss: 5.728052, mae: 33.971544, mean_q: 25.924166, mean_eps: 0.743797
  42822/150000: episode: 434, duration: 0.431s, episode steps:  68, steps per second: 158, episode reward: -82.405, mean reward: -1.212 [-100.000,  9.908], mean action: 1.559 [0.000, 3.000],  loss: 7.128621, mae: 34.320262, mean_q: 26.308893, mean_eps: 0.743275
  42915/150000: episode: 435, duration: 0.589s, episode steps:  93, steps per second: 158, episode reward: -77.516, mean reward: -0.834 [-100.000,  6.769], mean action: 1.505 [0.000, 3.000],  loss: 8.071091, mae: 33.491576, mean_q: 25.658727, mean_eps: 0.742792
  42996/150000: episode: 436, duration: 0.519s, episode steps:  81, steps per second: 156, episode reward: -72.178, mean reward: -0.891 [-100.000, 16.297], mean action: 1.630 [0.000, 3.000],  loss: 11.330737, mae: 34.728135, mean_q: 27.055343, mean_eps: 0.742270
  43114/150000: episode: 437, duration: 0.816s, episode steps: 118, steps per second: 145, episode reward: -36.974, mean reward: -0.313 [-100.000, 10.163], mean action: 1.568 [0.000, 3.000],  loss: 11.323348, mae: 34.700143, mean_q: 27.323657, mean_eps: 0.741673
  43191/150000: episode: 438, duration: 0.538s, episode steps:  77, steps per second: 143, episode reward: -55.629, mean reward: -0.722 [-100.000, 12.752], mean action: 1.649 [0.000, 3.000],  loss: 16.706102, mae: 34.490720, mean_q: 25.319453, mean_eps: 0.741088
  43274/150000: episode: 439, duration: 0.515s, episode steps:  83, steps per second: 161, episode reward: -66.189, mean reward: -0.797 [-100.000,  7.985], mean action: 1.518 [0.000, 3.000],  loss: 23.795131, mae: 34.732822, mean_q: 29.555784, mean_eps: 0.740608
  43402/150000: episode: 440, duration: 0.812s, episode steps: 128, steps per second: 158, episode reward: -129.700, mean reward: -1.013 [-100.000,  6.757], mean action: 1.445 [0.000, 3.000],  loss: 19.988531, mae: 34.553745, mean_q: 27.353757, mean_eps: 0.739975
  43511/150000: episode: 441, duration: 0.677s, episode steps: 109, steps per second: 161, episode reward: -35.047, mean reward: -0.322 [-100.000, 18.959], mean action: 1.440 [0.000, 3.000],  loss: 9.855914, mae: 34.571853, mean_q: 25.098167, mean_eps: 0.739264
  43599/150000: episode: 442, duration: 0.522s, episode steps:  88, steps per second: 169, episode reward: -81.207, mean reward: -0.923 [-100.000, 11.118], mean action: 1.580 [0.000, 3.000],  loss: 16.523098, mae: 34.272839, mean_q: 27.375936, mean_eps: 0.738673
  43664/150000: episode: 443, duration: 0.430s, episode steps:  65, steps per second: 151, episode reward: -151.766, mean reward: -2.335 [-100.000,  8.668], mean action: 1.477 [0.000, 3.000],  loss: 14.419773, mae: 34.893414, mean_q: 26.691377, mean_eps: 0.738214
  43758/150000: episode: 444, duration: 0.649s, episode steps:  94, steps per second: 145, episode reward: -95.090, mean reward: -1.012 [-100.000,  6.335], mean action: 1.691 [0.000, 3.000],  loss: 9.791995, mae: 34.820815, mean_q: 27.328717, mean_eps: 0.737737
  43856/150000: episode: 445, duration: 0.622s, episode steps:  98, steps per second: 158, episode reward: -96.159, mean reward: -0.981 [-100.000,  7.695], mean action: 1.673 [0.000, 3.000],  loss: 15.797846, mae: 34.701780, mean_q: 25.979994, mean_eps: 0.737161
  44004/150000: episode: 446, duration: 0.938s, episode steps: 148, steps per second: 158, episode reward: -89.167, mean reward: -0.602 [-100.000, 11.764], mean action: 1.662 [0.000, 3.000],  loss: 17.072075, mae: 34.639605, mean_q: 27.390234, mean_eps: 0.736423
  44080/150000: episode: 447, duration: 0.514s, episode steps:  76, steps per second: 148, episode reward: -48.514, mean reward: -0.638 [-100.000, 10.914], mean action: 1.487 [0.000, 3.000],  loss: 6.736696, mae: 35.141404, mean_q: 28.263511, mean_eps: 0.735751
  44162/150000: episode: 448, duration: 0.511s, episode steps:  82, steps per second: 160, episode reward: -90.079, mean reward: -1.099 [-100.000, 11.641], mean action: 1.366 [0.000, 3.000],  loss: 20.641293, mae: 35.579095, mean_q: 25.752684, mean_eps: 0.735277
  44278/150000: episode: 449, duration: 0.746s, episode steps: 116, steps per second: 155, episode reward: -90.212, mean reward: -0.778 [-100.000,  8.707], mean action: 1.612 [0.000, 3.000],  loss: 8.952385, mae: 35.721982, mean_q: 27.914175, mean_eps: 0.734683
  44353/150000: episode: 450, duration: 0.506s, episode steps:  75, steps per second: 148, episode reward: -44.857, mean reward: -0.598 [-100.000,  7.913], mean action: 1.707 [0.000, 3.000],  loss: 12.731983, mae: 35.453216, mean_q: 26.725543, mean_eps: 0.734110
  44446/150000: episode: 451, duration: 0.641s, episode steps:  93, steps per second: 145, episode reward: -125.750, mean reward: -1.352 [-100.000, 20.986], mean action: 1.548 [0.000, 3.000],  loss: 15.137822, mae: 35.934998, mean_q: 27.984742, mean_eps: 0.733606
  44535/150000: episode: 452, duration: 0.541s, episode steps:  89, steps per second: 165, episode reward: -77.641, mean reward: -0.872 [-100.000, 14.229], mean action: 1.618 [0.000, 3.000],  loss: 14.073509, mae: 36.224928, mean_q: 27.793745, mean_eps: 0.733060
  44619/150000: episode: 453, duration: 0.514s, episode steps:  84, steps per second: 163, episode reward: -63.778, mean reward: -0.759 [-100.000,  6.614], mean action: 1.643 [0.000, 3.000],  loss: 11.286316, mae: 35.170213, mean_q: 27.216302, mean_eps: 0.732541
  44687/150000: episode: 454, duration: 0.558s, episode steps:  68, steps per second: 122, episode reward: -122.795, mean reward: -1.806 [-100.000,  3.837], mean action: 1.309 [0.000, 3.000],  loss: 23.176999, mae: 35.627281, mean_q: 25.122257, mean_eps: 0.732085
  44812/150000: episode: 455, duration: 0.803s, episode steps: 125, steps per second: 156, episode reward: -104.217, mean reward: -0.834 [-100.000, 11.071], mean action: 1.520 [0.000, 3.000],  loss: 12.891851, mae: 35.344040, mean_q: 26.627910, mean_eps: 0.731506
  44891/150000: episode: 456, duration: 0.488s, episode steps:  79, steps per second: 162, episode reward: -29.369, mean reward: -0.372 [-100.000, 27.002], mean action: 1.633 [0.000, 3.000],  loss: 4.835804, mae: 35.333669, mean_q: 27.501752, mean_eps: 0.730894
  44963/150000: episode: 457, duration: 0.439s, episode steps:  72, steps per second: 164, episode reward: -47.814, mean reward: -0.664 [-100.000,  7.146], mean action: 1.542 [0.000, 3.000],  loss: 12.486940, mae: 34.926624, mean_q: 27.143562, mean_eps: 0.730441
  45051/150000: episode: 458, duration: 0.584s, episode steps:  88, steps per second: 151, episode reward: -52.337, mean reward: -0.595 [-100.000, 12.228], mean action: 1.568 [0.000, 3.000],  loss: 14.375734, mae: 35.891169, mean_q: 28.258079, mean_eps: 0.729961
  45150/150000: episode: 459, duration: 0.628s, episode steps:  99, steps per second: 158, episode reward: -148.524, mean reward: -1.500 [-100.000,  7.511], mean action: 1.626 [0.000, 3.000],  loss: 13.163644, mae: 36.568462, mean_q: 28.665849, mean_eps: 0.729400
  45261/150000: episode: 460, duration: 0.679s, episode steps: 111, steps per second: 164, episode reward: -101.759, mean reward: -0.917 [-100.000, 10.456], mean action: 1.622 [0.000, 3.000],  loss: 21.111003, mae: 36.159030, mean_q: 27.724965, mean_eps: 0.728770
  45380/150000: episode: 461, duration: 0.745s, episode steps: 119, steps per second: 160, episode reward: -64.905, mean reward: -0.545 [-100.000, 13.836], mean action: 1.462 [0.000, 3.000],  loss: 10.302591, mae: 36.943009, mean_q: 28.735438, mean_eps: 0.728080
  45457/150000: episode: 462, duration: 0.472s, episode steps:  77, steps per second: 163, episode reward: -107.890, mean reward: -1.401 [-100.000,  9.291], mean action: 1.558 [0.000, 3.000],  loss: 13.993980, mae: 36.323628, mean_q: 27.861271, mean_eps: 0.727492
  45572/150000: episode: 463, duration: 0.665s, episode steps: 115, steps per second: 173, episode reward: -62.269, mean reward: -0.541 [-100.000,  6.430], mean action: 1.339 [0.000, 3.000],  loss: 15.593735, mae: 36.248118, mean_q: 28.174429, mean_eps: 0.726916
  45690/150000: episode: 464, duration: 0.744s, episode steps: 118, steps per second: 159, episode reward: -153.316, mean reward: -1.299 [-100.000,  9.112], mean action: 1.517 [0.000, 3.000],  loss: 16.718820, mae: 35.666359, mean_q: 28.677744, mean_eps: 0.726217
  45777/150000: episode: 465, duration: 0.602s, episode steps:  87, steps per second: 145, episode reward: -77.585, mean reward: -0.892 [-100.000,  6.540], mean action: 1.828 [0.000, 3.000],  loss: 7.301484, mae: 36.101665, mean_q: 30.374274, mean_eps: 0.725602
  45908/150000: episode: 466, duration: 0.856s, episode steps: 131, steps per second: 153, episode reward: -50.865, mean reward: -0.388 [-100.000,  7.622], mean action: 1.603 [0.000, 3.000],  loss: 10.062950, mae: 36.030372, mean_q: 27.839733, mean_eps: 0.724948
  46018/150000: episode: 467, duration: 0.751s, episode steps: 110, steps per second: 146, episode reward: -129.010, mean reward: -1.173 [-100.000,  7.871], mean action: 1.500 [0.000, 3.000],  loss: 13.152255, mae: 36.184236, mean_q: 28.555950, mean_eps: 0.724225
  46146/150000: episode: 468, duration: 0.879s, episode steps: 128, steps per second: 146, episode reward: -29.524, mean reward: -0.231 [-100.000, 11.688], mean action: 1.609 [0.000, 3.000],  loss: 11.312597, mae: 36.069896, mean_q: 30.069180, mean_eps: 0.723511
  46250/150000: episode: 469, duration: 0.670s, episode steps: 104, steps per second: 155, episode reward: -53.512, mean reward: -0.515 [-100.000, 11.716], mean action: 1.692 [0.000, 3.000],  loss: 11.054591, mae: 36.575373, mean_q: 29.390349, mean_eps: 0.722815
  46337/150000: episode: 470, duration: 0.575s, episode steps:  87, steps per second: 151, episode reward: -47.560, mean reward: -0.547 [-100.000, 12.257], mean action: 1.655 [0.000, 3.000],  loss: 10.620273, mae: 36.476706, mean_q: 30.282304, mean_eps: 0.722242
  46424/150000: episode: 471, duration: 0.676s, episode steps:  87, steps per second: 129, episode reward: -101.675, mean reward: -1.169 [-100.000, 19.176], mean action: 1.414 [0.000, 3.000],  loss: 8.730771, mae: 36.359319, mean_q: 28.736227, mean_eps: 0.721720
  46521/150000: episode: 472, duration: 0.703s, episode steps:  97, steps per second: 138, episode reward: -63.482, mean reward: -0.654 [-100.000, 13.073], mean action: 1.629 [0.000, 3.000],  loss: 7.762217, mae: 36.548918, mean_q: 30.686249, mean_eps: 0.721168
  46604/150000: episode: 473, duration: 0.547s, episode steps:  83, steps per second: 152, episode reward: -119.995, mean reward: -1.446 [-100.000,  7.570], mean action: 1.506 [0.000, 3.000],  loss: 13.137322, mae: 36.792973, mean_q: 30.653470, mean_eps: 0.720628
  46699/150000: episode: 474, duration: 0.655s, episode steps:  95, steps per second: 145, episode reward: -75.743, mean reward: -0.797 [-100.000, 14.393], mean action: 1.747 [0.000, 3.000],  loss: 11.574058, mae: 35.699911, mean_q: 29.940999, mean_eps: 0.720094
  46772/150000: episode: 475, duration: 0.511s, episode steps:  73, steps per second: 143, episode reward: -64.922, mean reward: -0.889 [-100.000, 10.613], mean action: 1.575 [0.000, 3.000],  loss: 9.094960, mae: 36.406031, mean_q: 29.228051, mean_eps: 0.719590
  46837/150000: episode: 476, duration: 0.435s, episode steps:  65, steps per second: 149, episode reward: -80.477, mean reward: -1.238 [-100.000,  7.618], mean action: 1.615 [0.000, 3.000],  loss: 9.576217, mae: 36.467742, mean_q: 31.101149, mean_eps: 0.719176
  46946/150000: episode: 477, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward: -43.945, mean reward: -0.403 [-100.000, 23.712], mean action: 1.468 [0.000, 3.000],  loss: 8.834737, mae: 37.011502, mean_q: 29.510766, mean_eps: 0.718654
  47085/150000: episode: 478, duration: 1.016s, episode steps: 139, steps per second: 137, episode reward: -64.274, mean reward: -0.462 [-100.000,  9.088], mean action: 1.662 [0.000, 3.000],  loss: 9.325032, mae: 36.304580, mean_q: 30.506316, mean_eps: 0.717910
  47179/150000: episode: 479, duration: 0.625s, episode steps:  94, steps per second: 150, episode reward: -61.484, mean reward: -0.654 [-100.000, 12.971], mean action: 1.755 [0.000, 3.000],  loss: 12.096716, mae: 36.587749, mean_q: 31.371407, mean_eps: 0.717211
  47289/150000: episode: 480, duration: 0.757s, episode steps: 110, steps per second: 145, episode reward: -65.395, mean reward: -0.594 [-100.000, 11.245], mean action: 1.473 [0.000, 3.000],  loss: 18.033349, mae: 35.512476, mean_q: 30.914426, mean_eps: 0.716599
  47367/150000: episode: 481, duration: 0.549s, episode steps:  78, steps per second: 142, episode reward: -76.973, mean reward: -0.987 [-100.000,  6.911], mean action: 1.564 [0.000, 3.000],  loss: 14.777631, mae: 36.138645, mean_q: 31.466138, mean_eps: 0.716035
  47481/150000: episode: 482, duration: 0.690s, episode steps: 114, steps per second: 165, episode reward: -54.208, mean reward: -0.476 [-100.000, 18.119], mean action: 1.447 [0.000, 3.000],  loss: 23.108083, mae: 36.233398, mean_q: 30.332853, mean_eps: 0.715459
  47600/150000: episode: 483, duration: 0.688s, episode steps: 119, steps per second: 173, episode reward: -93.050, mean reward: -0.782 [-100.000,  9.652], mean action: 1.395 [0.000, 3.000],  loss: 14.845726, mae: 35.925979, mean_q: 29.531035, mean_eps: 0.714760
  47699/150000: episode: 484, duration: 0.633s, episode steps:  99, steps per second: 156, episode reward: -39.376, mean reward: -0.398 [-100.000, 15.923], mean action: 1.788 [0.000, 3.000],  loss: 11.911330, mae: 35.887798, mean_q: 30.377048, mean_eps: 0.714106
  47795/150000: episode: 485, duration: 0.581s, episode steps:  96, steps per second: 165, episode reward: -71.638, mean reward: -0.746 [-100.000, 19.773], mean action: 1.448 [0.000, 3.000],  loss: 12.056671, mae: 36.717574, mean_q: 31.278666, mean_eps: 0.713521
  47938/150000: episode: 486, duration: 0.904s, episode steps: 143, steps per second: 158, episode reward: -179.701, mean reward: -1.257 [-100.000, 19.225], mean action: 1.643 [0.000, 3.000],  loss: 11.233621, mae: 36.056934, mean_q: 30.285696, mean_eps: 0.712804
  48017/150000: episode: 487, duration: 0.528s, episode steps:  79, steps per second: 149, episode reward: -62.124, mean reward: -0.786 [-100.000, 12.077], mean action: 1.494 [0.000, 3.000],  loss: 11.085972, mae: 36.095542, mean_q: 31.572255, mean_eps: 0.712138
  48113/150000: episode: 488, duration: 0.590s, episode steps:  96, steps per second: 163, episode reward: -50.045, mean reward: -0.521 [-100.000, 19.758], mean action: 1.729 [0.000, 3.000],  loss: 14.379344, mae: 36.119331, mean_q: 30.742810, mean_eps: 0.711613
  48212/150000: episode: 489, duration: 0.581s, episode steps:  99, steps per second: 170, episode reward: -115.846, mean reward: -1.170 [-100.000,  9.036], mean action: 1.535 [0.000, 3.000],  loss: 14.114247, mae: 36.774645, mean_q: 30.596102, mean_eps: 0.711028
  48287/150000: episode: 490, duration: 0.462s, episode steps:  75, steps per second: 162, episode reward: -37.607, mean reward: -0.501 [-100.000, 13.482], mean action: 1.520 [0.000, 3.000],  loss: 10.274138, mae: 36.621973, mean_q: 30.018044, mean_eps: 0.710506
  48360/150000: episode: 491, duration: 0.489s, episode steps:  73, steps per second: 149, episode reward: -70.986, mean reward: -0.972 [-100.000,  7.605], mean action: 1.397 [0.000, 3.000],  loss: 13.407033, mae: 35.994008, mean_q: 29.149992, mean_eps: 0.710062
  48474/150000: episode: 492, duration: 0.738s, episode steps: 114, steps per second: 155, episode reward: -76.883, mean reward: -0.674 [-100.000,  9.533], mean action: 1.684 [0.000, 3.000],  loss: 16.835046, mae: 36.575209, mean_q: 30.216944, mean_eps: 0.709501
  48566/150000: episode: 493, duration: 0.567s, episode steps:  92, steps per second: 162, episode reward: -99.298, mean reward: -1.079 [-100.000,  6.641], mean action: 1.478 [0.000, 3.000],  loss: 16.827629, mae: 36.398534, mean_q: 30.909981, mean_eps: 0.708883
  48715/150000: episode: 494, duration: 0.971s, episode steps: 149, steps per second: 153, episode reward: -53.710, mean reward: -0.360 [-100.000, 17.202], mean action: 1.718 [0.000, 3.000],  loss: 13.990565, mae: 36.980435, mean_q: 30.685425, mean_eps: 0.708160
  48825/150000: episode: 495, duration: 0.690s, episode steps: 110, steps per second: 159, episode reward: -87.569, mean reward: -0.796 [-100.000, 16.649], mean action: 1.518 [0.000, 3.000],  loss: 11.998250, mae: 36.557949, mean_q: 30.025086, mean_eps: 0.707383
  48910/150000: episode: 496, duration: 0.522s, episode steps:  85, steps per second: 163, episode reward: 14.616, mean reward:  0.172 [-100.000, 15.924], mean action: 1.635 [0.000, 3.000],  loss: 6.449893, mae: 36.702209, mean_q: 31.369706, mean_eps: 0.706798
  49018/150000: episode: 497, duration: 0.688s, episode steps: 108, steps per second: 157, episode reward: -80.201, mean reward: -0.743 [-100.000, 11.646], mean action: 1.713 [0.000, 3.000],  loss: 11.377120, mae: 36.334497, mean_q: 29.780233, mean_eps: 0.706219
  49115/150000: episode: 498, duration: 0.605s, episode steps:  97, steps per second: 160, episode reward: -84.687, mean reward: -0.873 [-100.000,  8.269], mean action: 1.443 [0.000, 3.000],  loss: 7.247518, mae: 37.022200, mean_q: 30.793676, mean_eps: 0.705604
  49198/150000: episode: 499, duration: 0.568s, episode steps:  83, steps per second: 146, episode reward: -83.521, mean reward: -1.006 [-100.000, 10.867], mean action: 1.578 [0.000, 3.000],  loss: 18.811103, mae: 36.158926, mean_q: 31.418695, mean_eps: 0.705064
  49273/150000: episode: 500, duration: 0.682s, episode steps:  75, steps per second: 110, episode reward: -81.615, mean reward: -1.088 [-100.000, 11.693], mean action: 1.640 [0.000, 3.000],  loss: 5.665597, mae: 36.432180, mean_q: 29.943829, mean_eps: 0.704590
  49366/150000: episode: 501, duration: 0.737s, episode steps:  93, steps per second: 126, episode reward: -70.458, mean reward: -0.758 [-100.000,  6.991], mean action: 1.581 [0.000, 3.000],  loss: 19.642662, mae: 36.569859, mean_q: 31.198730, mean_eps: 0.704086
  49483/150000: episode: 502, duration: 0.830s, episode steps: 117, steps per second: 141, episode reward: -91.289, mean reward: -0.780 [-100.000,  7.722], mean action: 1.513 [0.000, 3.000],  loss: 10.119222, mae: 36.881588, mean_q: 32.127353, mean_eps: 0.703456
  49608/150000: episode: 503, duration: 1.300s, episode steps: 125, steps per second:  96, episode reward: -89.492, mean reward: -0.716 [-100.000, 12.086], mean action: 1.656 [0.000, 3.000],  loss: 14.091876, mae: 36.943497, mean_q: 31.891158, mean_eps: 0.702730
  49694/150000: episode: 504, duration: 0.842s, episode steps:  86, steps per second: 102, episode reward: -38.293, mean reward: -0.445 [-100.000, 12.914], mean action: 1.663 [0.000, 3.000],  loss: 7.928885, mae: 36.943081, mean_q: 31.343247, mean_eps: 0.702097
  50607/150000: episode: 505, duration: 8.660s, episode steps: 913, steps per second: 105, episode reward: -83.360, mean reward: -0.091 [-100.000, 50.857], mean action: 1.590 [0.000, 3.000],  loss: 12.334648, mae: 36.731454, mean_q: 31.133108, mean_eps: 0.699100
  50686/150000: episode: 506, duration: 0.511s, episode steps:  79, steps per second: 155, episode reward: -53.072, mean reward: -0.672 [-100.000, 13.845], mean action: 1.519 [0.000, 3.000],  loss: 11.750992, mae: 37.069064, mean_q: 30.787839, mean_eps: 0.696124
  50772/150000: episode: 507, duration: 0.632s, episode steps:  86, steps per second: 136, episode reward: -69.870, mean reward: -0.812 [-100.000, 10.832], mean action: 1.744 [0.000, 3.000],  loss: 12.354666, mae: 36.142377, mean_q: 30.299520, mean_eps: 0.695629
  50877/150000: episode: 508, duration: 0.671s, episode steps: 105, steps per second: 156, episode reward: -39.607, mean reward: -0.377 [-100.000, 22.116], mean action: 1.638 [0.000, 3.000],  loss: 12.709570, mae: 36.875853, mean_q: 29.474557, mean_eps: 0.695056
  50961/150000: episode: 509, duration: 0.528s, episode steps:  84, steps per second: 159, episode reward: -52.248, mean reward: -0.622 [-100.000, 22.042], mean action: 1.631 [0.000, 3.000],  loss: 11.299636, mae: 36.294170, mean_q: 28.965464, mean_eps: 0.694489
  51068/150000: episode: 510, duration: 0.777s, episode steps: 107, steps per second: 138, episode reward: -117.309, mean reward: -1.096 [-100.000,  6.518], mean action: 1.514 [0.000, 3.000],  loss: 12.527176, mae: 36.630690, mean_q: 30.538025, mean_eps: 0.693916
  51157/150000: episode: 511, duration: 0.605s, episode steps:  89, steps per second: 147, episode reward: -63.194, mean reward: -0.710 [-100.000, 13.781], mean action: 1.258 [0.000, 3.000],  loss: 11.272373, mae: 36.689632, mean_q: 30.866870, mean_eps: 0.693328
  51279/150000: episode: 512, duration: 0.749s, episode steps: 122, steps per second: 163, episode reward: -111.059, mean reward: -0.910 [-100.000, 12.433], mean action: 1.582 [0.000, 3.000],  loss: 5.463552, mae: 36.557443, mean_q: 28.984192, mean_eps: 0.692695
  51359/150000: episode: 513, duration: 0.483s, episode steps:  80, steps per second: 166, episode reward: -44.048, mean reward: -0.551 [-100.000,  7.364], mean action: 1.750 [0.000, 3.000],  loss: 9.627184, mae: 36.479120, mean_q: 30.342799, mean_eps: 0.692089
  51455/150000: episode: 514, duration: 0.651s, episode steps:  96, steps per second: 147, episode reward: -79.619, mean reward: -0.829 [-100.000, 11.300], mean action: 1.240 [0.000, 3.000],  loss: 6.716426, mae: 37.318061, mean_q: 31.458320, mean_eps: 0.691561
  51586/150000: episode: 515, duration: 0.865s, episode steps: 131, steps per second: 151, episode reward: -52.428, mean reward: -0.400 [-100.000,  7.550], mean action: 1.557 [0.000, 3.000],  loss: 8.121382, mae: 36.815127, mean_q: 30.159983, mean_eps: 0.690880
  51707/150000: episode: 516, duration: 0.714s, episode steps: 121, steps per second: 169, episode reward: -94.353, mean reward: -0.780 [-100.000,  7.153], mean action: 1.455 [0.000, 3.000],  loss: 10.782161, mae: 36.776292, mean_q: 30.737789, mean_eps: 0.690124
  51784/150000: episode: 517, duration: 0.499s, episode steps:  77, steps per second: 154, episode reward: -64.711, mean reward: -0.840 [-100.000, 14.127], mean action: 1.351 [0.000, 3.000],  loss: 4.746526, mae: 37.027071, mean_q: 30.538521, mean_eps: 0.689530
  51898/150000: episode: 518, duration: 0.715s, episode steps: 114, steps per second: 159, episode reward: -56.270, mean reward: -0.494 [-100.000, 21.675], mean action: 1.509 [0.000, 3.000],  loss: 5.769096, mae: 37.141563, mean_q: 30.703736, mean_eps: 0.688957
  51999/150000: episode: 519, duration: 0.657s, episode steps: 101, steps per second: 154, episode reward: -90.924, mean reward: -0.900 [-100.000, 11.866], mean action: 1.535 [0.000, 3.000],  loss: 6.032418, mae: 36.778574, mean_q: 30.373786, mean_eps: 0.688312
  52079/150000: episode: 520, duration: 0.520s, episode steps:  80, steps per second: 154, episode reward: -39.965, mean reward: -0.500 [-100.000, 12.083], mean action: 1.775 [0.000, 3.000],  loss: 6.495387, mae: 36.512373, mean_q: 29.065149, mean_eps: 0.687769
  52176/150000: episode: 521, duration: 0.684s, episode steps:  97, steps per second: 142, episode reward: -66.136, mean reward: -0.682 [-100.000, 13.853], mean action: 1.598 [0.000, 3.000],  loss: 20.875172, mae: 37.361392, mean_q: 32.502467, mean_eps: 0.687238
  52272/150000: episode: 522, duration: 0.615s, episode steps:  96, steps per second: 156, episode reward: -67.324, mean reward: -0.701 [-100.000, 12.572], mean action: 1.615 [0.000, 3.000],  loss: 9.474658, mae: 37.310528, mean_q: 30.378281, mean_eps: 0.686659
  52384/150000: episode: 523, duration: 0.704s, episode steps: 112, steps per second: 159, episode reward: -61.589, mean reward: -0.550 [-100.000,  9.696], mean action: 1.607 [0.000, 3.000],  loss: 19.033185, mae: 36.808689, mean_q: 31.522396, mean_eps: 0.686035
  52525/150000: episode: 524, duration: 0.930s, episode steps: 141, steps per second: 152, episode reward: -62.031, mean reward: -0.440 [-100.000, 22.199], mean action: 1.638 [0.000, 3.000],  loss: 13.340027, mae: 37.264828, mean_q: 30.058107, mean_eps: 0.685276
  52625/150000: episode: 525, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: -62.003, mean reward: -0.620 [-100.000, 12.408], mean action: 1.600 [0.000, 3.000],  loss: 8.359067, mae: 37.249845, mean_q: 29.978266, mean_eps: 0.684553
  52739/150000: episode: 526, duration: 0.732s, episode steps: 114, steps per second: 156, episode reward: -177.652, mean reward: -1.558 [-100.000,  5.202], mean action: 1.281 [0.000, 3.000],  loss: 10.085240, mae: 37.261612, mean_q: 31.574421, mean_eps: 0.683911
  52812/150000: episode: 527, duration: 0.500s, episode steps:  73, steps per second: 146, episode reward: -94.116, mean reward: -1.289 [-100.000,  5.450], mean action: 1.521 [0.000, 3.000],  loss: 7.540045, mae: 37.320596, mean_q: 30.514303, mean_eps: 0.683350
  52933/150000: episode: 528, duration: 0.718s, episode steps: 121, steps per second: 168, episode reward: -63.578, mean reward: -0.525 [-100.000, 16.877], mean action: 1.603 [0.000, 3.000],  loss: 19.914505, mae: 37.483647, mean_q: 30.187828, mean_eps: 0.682768
  53026/150000: episode: 529, duration: 0.541s, episode steps:  93, steps per second: 172, episode reward: -101.253, mean reward: -1.089 [-100.000,  8.456], mean action: 1.462 [0.000, 3.000],  loss: 15.251395, mae: 37.672032, mean_q: 30.807040, mean_eps: 0.682126
  53115/150000: episode: 530, duration: 0.540s, episode steps:  89, steps per second: 165, episode reward: -82.460, mean reward: -0.927 [-100.000, 16.961], mean action: 1.472 [0.000, 3.000],  loss: 12.242465, mae: 37.109425, mean_q: 30.712239, mean_eps: 0.681580
  53210/150000: episode: 531, duration: 0.589s, episode steps:  95, steps per second: 161, episode reward: -137.066, mean reward: -1.443 [-100.000,  5.292], mean action: 1.484 [0.000, 3.000],  loss: 9.431960, mae: 37.012441, mean_q: 30.557653, mean_eps: 0.681028
  53336/150000: episode: 532, duration: 0.774s, episode steps: 126, steps per second: 163, episode reward: -42.265, mean reward: -0.335 [-100.000, 11.194], mean action: 1.738 [0.000, 3.000],  loss: 14.548352, mae: 37.109060, mean_q: 30.162940, mean_eps: 0.680365
  53453/150000: episode: 533, duration: 0.736s, episode steps: 117, steps per second: 159, episode reward: -60.433, mean reward: -0.517 [-100.000,  7.824], mean action: 1.513 [0.000, 3.000],  loss: 15.104314, mae: 37.327278, mean_q: 30.939573, mean_eps: 0.679636
  53550/150000: episode: 534, duration: 0.627s, episode steps:  97, steps per second: 155, episode reward: -20.176, mean reward: -0.208 [-100.000, 60.424], mean action: 1.742 [0.000, 3.000],  loss: 8.334340, mae: 36.728227, mean_q: 31.931268, mean_eps: 0.678994
  53685/150000: episode: 535, duration: 0.802s, episode steps: 135, steps per second: 168, episode reward: -85.846, mean reward: -0.636 [-100.000,  7.244], mean action: 1.533 [0.000, 3.000],  loss: 8.922914, mae: 36.956811, mean_q: 30.889224, mean_eps: 0.678298
  53803/150000: episode: 536, duration: 0.711s, episode steps: 118, steps per second: 166, episode reward: -47.633, mean reward: -0.404 [-100.000, 17.835], mean action: 1.754 [0.000, 3.000],  loss: 10.997242, mae: 36.413344, mean_q: 30.688996, mean_eps: 0.677539
  53900/150000: episode: 537, duration: 0.600s, episode steps:  97, steps per second: 162, episode reward: -114.438, mean reward: -1.180 [-100.000, 15.460], mean action: 1.495 [0.000, 3.000],  loss: 12.417144, mae: 36.707930, mean_q: 30.096335, mean_eps: 0.676894
  53983/150000: episode: 538, duration: 0.497s, episode steps:  83, steps per second: 167, episode reward: -116.477, mean reward: -1.403 [-100.000,  7.880], mean action: 1.590 [0.000, 3.000],  loss: 10.845033, mae: 36.985670, mean_q: 30.407093, mean_eps: 0.676354
  54120/150000: episode: 539, duration: 0.839s, episode steps: 137, steps per second: 163, episode reward: -42.222, mean reward: -0.308 [-100.000, 19.154], mean action: 1.730 [0.000, 3.000],  loss: 8.056462, mae: 36.745469, mean_q: 30.703964, mean_eps: 0.675694
  54245/150000: episode: 540, duration: 0.810s, episode steps: 125, steps per second: 154, episode reward: -11.320, mean reward: -0.091 [-100.000, 19.299], mean action: 1.696 [0.000, 3.000],  loss: 10.745543, mae: 36.950618, mean_q: 30.951814, mean_eps: 0.674908
  54361/150000: episode: 541, duration: 0.714s, episode steps: 116, steps per second: 162, episode reward: -111.811, mean reward: -0.964 [-100.000, 13.605], mean action: 1.440 [0.000, 3.000],  loss: 7.080767, mae: 36.356494, mean_q: 30.338544, mean_eps: 0.674185
  54459/150000: episode: 542, duration: 0.582s, episode steps:  98, steps per second: 168, episode reward: -47.355, mean reward: -0.483 [-100.000, 10.122], mean action: 1.439 [0.000, 3.000],  loss: 10.593402, mae: 37.655468, mean_q: 31.560428, mean_eps: 0.673543
  54544/150000: episode: 543, duration: 0.533s, episode steps:  85, steps per second: 160, episode reward: -57.208, mean reward: -0.673 [-100.000,  9.090], mean action: 1.729 [0.000, 3.000],  loss: 30.379507, mae: 37.169179, mean_q: 31.137916, mean_eps: 0.672994
  54632/150000: episode: 544, duration: 0.530s, episode steps:  88, steps per second: 166, episode reward: -73.182, mean reward: -0.832 [-100.000, 22.325], mean action: 1.670 [0.000, 3.000],  loss: 8.888881, mae: 37.235253, mean_q: 32.340968, mean_eps: 0.672475
  54746/150000: episode: 545, duration: 0.669s, episode steps: 114, steps per second: 170, episode reward: -98.015, mean reward: -0.860 [-100.000,  8.639], mean action: 1.851 [0.000, 3.000],  loss: 7.431113, mae: 37.344009, mean_q: 29.886484, mean_eps: 0.671869
  54829/150000: episode: 546, duration: 0.494s, episode steps:  83, steps per second: 168, episode reward: -61.836, mean reward: -0.745 [-100.000, 13.223], mean action: 1.795 [0.000, 3.000],  loss: 7.415791, mae: 37.244738, mean_q: 31.215355, mean_eps: 0.671278
  54920/150000: episode: 547, duration: 0.568s, episode steps:  91, steps per second: 160, episode reward: -55.961, mean reward: -0.615 [-100.000, 12.741], mean action: 1.560 [0.000, 3.000],  loss: 16.006383, mae: 36.810461, mean_q: 31.275997, mean_eps: 0.670756
  54987/150000: episode: 548, duration: 0.399s, episode steps:  67, steps per second: 168, episode reward: -102.986, mean reward: -1.537 [-100.000,  9.864], mean action: 1.582 [0.000, 3.000],  loss: 10.745222, mae: 37.212941, mean_q: 28.948715, mean_eps: 0.670282
  55072/150000: episode: 549, duration: 0.502s, episode steps:  85, steps per second: 169, episode reward: -2.881, mean reward: -0.034 [-100.000, 10.454], mean action: 1.576 [0.000, 3.000],  loss: 8.880728, mae: 36.811668, mean_q: 31.552630, mean_eps: 0.669826
  55193/150000: episode: 550, duration: 0.710s, episode steps: 121, steps per second: 170, episode reward: -104.542, mean reward: -0.864 [-100.000,  9.093], mean action: 1.479 [0.000, 3.000],  loss: 6.657583, mae: 37.062799, mean_q: 31.795197, mean_eps: 0.669208
  55285/150000: episode: 551, duration: 0.578s, episode steps:  92, steps per second: 159, episode reward: -53.022, mean reward: -0.576 [-100.000, 10.749], mean action: 1.674 [0.000, 3.000],  loss: 11.523379, mae: 37.166289, mean_q: 31.134847, mean_eps: 0.668569
  55412/150000: episode: 552, duration: 0.775s, episode steps: 127, steps per second: 164, episode reward: -62.534, mean reward: -0.492 [-100.000,  6.041], mean action: 1.693 [0.000, 3.000],  loss: 7.091950, mae: 36.816133, mean_q: 30.648421, mean_eps: 0.667912
  55516/150000: episode: 553, duration: 0.645s, episode steps: 104, steps per second: 161, episode reward: -12.797, mean reward: -0.123 [-100.000, 12.407], mean action: 1.885 [0.000, 3.000],  loss: 8.807788, mae: 36.670156, mean_q: 32.359529, mean_eps: 0.667219
  55600/150000: episode: 554, duration: 0.522s, episode steps:  84, steps per second: 161, episode reward: -62.678, mean reward: -0.746 [-100.000,  7.121], mean action: 1.429 [0.000, 3.000],  loss: 5.831555, mae: 36.873690, mean_q: 30.523116, mean_eps: 0.666655
  55731/150000: episode: 555, duration: 0.829s, episode steps: 131, steps per second: 158, episode reward: -30.512, mean reward: -0.233 [-100.000, 23.340], mean action: 1.641 [0.000, 3.000],  loss: 9.650942, mae: 37.350843, mean_q: 31.935446, mean_eps: 0.666010
  55832/150000: episode: 556, duration: 0.637s, episode steps: 101, steps per second: 159, episode reward: -139.378, mean reward: -1.380 [-100.000,  9.712], mean action: 1.475 [0.000, 3.000],  loss: 18.008638, mae: 37.558576, mean_q: 31.354984, mean_eps: 0.665314
  55936/150000: episode: 557, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -5.663, mean reward: -0.054 [-100.000, 17.418], mean action: 1.558 [0.000, 3.000],  loss: 14.698408, mae: 37.389888, mean_q: 32.488729, mean_eps: 0.664699
  56015/150000: episode: 558, duration: 0.520s, episode steps:  79, steps per second: 152, episode reward: -55.408, mean reward: -0.701 [-100.000, 14.466], mean action: 1.544 [0.000, 3.000],  loss: 5.926682, mae: 36.628796, mean_q: 31.976458, mean_eps: 0.664150
  56125/150000: episode: 559, duration: 0.733s, episode steps: 110, steps per second: 150, episode reward: -135.833, mean reward: -1.235 [-100.000, 12.855], mean action: 1.591 [0.000, 3.000],  loss: 9.689523, mae: 37.706271, mean_q: 33.159817, mean_eps: 0.663583
  56234/150000: episode: 560, duration: 0.799s, episode steps: 109, steps per second: 136, episode reward: -99.725, mean reward: -0.915 [-100.000, 15.532], mean action: 1.358 [0.000, 3.000],  loss: 11.748063, mae: 37.760078, mean_q: 31.195363, mean_eps: 0.662926
  56348/150000: episode: 561, duration: 0.805s, episode steps: 114, steps per second: 142, episode reward: -191.083, mean reward: -1.676 [-100.000, 14.661], mean action: 1.675 [0.000, 3.000],  loss: 15.894762, mae: 37.475656, mean_q: 31.891236, mean_eps: 0.662257
  56435/150000: episode: 562, duration: 0.651s, episode steps:  87, steps per second: 134, episode reward: -105.133, mean reward: -1.208 [-100.000,  6.072], mean action: 1.379 [0.000, 3.000],  loss: 8.764814, mae: 37.229963, mean_q: 32.138567, mean_eps: 0.661654
  56523/150000: episode: 563, duration: 0.661s, episode steps:  88, steps per second: 133, episode reward: -89.060, mean reward: -1.012 [-100.000, 11.064], mean action: 1.511 [0.000, 3.000],  loss: 12.494187, mae: 37.206295, mean_q: 32.728202, mean_eps: 0.661129
  56633/150000: episode: 564, duration: 0.823s, episode steps: 110, steps per second: 134, episode reward: -96.238, mean reward: -0.875 [-100.000,  6.517], mean action: 1.500 [0.000, 3.000],  loss: 17.436751, mae: 37.288558, mean_q: 32.309422, mean_eps: 0.660535
  56745/150000: episode: 565, duration: 0.719s, episode steps: 112, steps per second: 156, episode reward: -33.839, mean reward: -0.302 [-100.000, 12.696], mean action: 1.518 [0.000, 3.000],  loss: 11.209518, mae: 37.652497, mean_q: 31.063853, mean_eps: 0.659869
  56895/150000: episode: 566, duration: 0.926s, episode steps: 150, steps per second: 162, episode reward:  0.825, mean reward:  0.005 [-100.000, 14.860], mean action: 1.600 [0.000, 3.000],  loss: 13.643659, mae: 37.145612, mean_q: 32.211991, mean_eps: 0.659083
  57010/150000: episode: 567, duration: 0.681s, episode steps: 115, steps per second: 169, episode reward: -73.934, mean reward: -0.643 [-100.000,  8.976], mean action: 1.748 [0.000, 3.000],  loss: 9.551642, mae: 37.791884, mean_q: 32.742286, mean_eps: 0.658288
  57096/150000: episode: 568, duration: 0.505s, episode steps:  86, steps per second: 170, episode reward: -56.240, mean reward: -0.654 [-100.000,  9.848], mean action: 1.570 [0.000, 3.000],  loss: 7.738861, mae: 37.157344, mean_q: 31.155676, mean_eps: 0.657685
  57184/150000: episode: 569, duration: 0.518s, episode steps:  88, steps per second: 170, episode reward: -88.506, mean reward: -1.006 [-100.000, 13.175], mean action: 1.682 [0.000, 3.000],  loss: 9.987371, mae: 36.177388, mean_q: 29.527889, mean_eps: 0.657163
  57292/150000: episode: 570, duration: 0.667s, episode steps: 108, steps per second: 162, episode reward:  2.704, mean reward:  0.025 [-100.000, 15.785], mean action: 1.630 [0.000, 3.000],  loss: 5.677585, mae: 37.173245, mean_q: 30.619708, mean_eps: 0.656575
  57406/150000: episode: 571, duration: 0.672s, episode steps: 114, steps per second: 170, episode reward: -39.274, mean reward: -0.345 [-100.000, 17.544], mean action: 1.789 [0.000, 3.000],  loss: 7.498372, mae: 37.103657, mean_q: 31.193351, mean_eps: 0.655909
  57484/150000: episode: 572, duration: 0.456s, episode steps:  78, steps per second: 171, episode reward: -44.619, mean reward: -0.572 [-100.000,  5.942], mean action: 1.654 [0.000, 3.000],  loss: 12.344783, mae: 37.931984, mean_q: 31.504849, mean_eps: 0.655333
  57616/150000: episode: 573, duration: 0.821s, episode steps: 132, steps per second: 161, episode reward: -55.573, mean reward: -0.421 [-100.000, 12.100], mean action: 1.576 [0.000, 3.000],  loss: 9.281397, mae: 37.116315, mean_q: 30.225371, mean_eps: 0.654703
  57689/150000: episode: 574, duration: 0.450s, episode steps:  73, steps per second: 162, episode reward: -12.530, mean reward: -0.172 [-100.000, 11.827], mean action: 1.603 [0.000, 3.000],  loss: 10.532969, mae: 38.053244, mean_q: 31.877122, mean_eps: 0.654088
  57769/150000: episode: 575, duration: 0.470s, episode steps:  80, steps per second: 170, episode reward: -37.646, mean reward: -0.471 [-100.000, 20.741], mean action: 1.613 [0.000, 3.000],  loss: 6.416655, mae: 36.761032, mean_q: 31.105117, mean_eps: 0.653629
  57857/150000: episode: 576, duration: 0.543s, episode steps:  88, steps per second: 162, episode reward: -1.606, mean reward: -0.018 [-100.000, 17.733], mean action: 1.750 [0.000, 3.000],  loss: 11.277386, mae: 36.920224, mean_q: 31.716996, mean_eps: 0.653125
  57941/150000: episode: 577, duration: 0.566s, episode steps:  84, steps per second: 148, episode reward: -62.821, mean reward: -0.748 [-100.000, 12.409], mean action: 1.643 [0.000, 3.000],  loss: 14.118514, mae: 37.539009, mean_q: 30.920882, mean_eps: 0.652609
  58036/150000: episode: 578, duration: 0.604s, episode steps:  95, steps per second: 157, episode reward: -60.161, mean reward: -0.633 [-100.000, 12.179], mean action: 1.705 [0.000, 3.000],  loss: 6.306352, mae: 37.026720, mean_q: 31.919302, mean_eps: 0.652072
  58128/150000: episode: 579, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -125.085, mean reward: -1.360 [-100.000, 15.928], mean action: 1.630 [0.000, 3.000],  loss: 5.765216, mae: 37.376609, mean_q: 33.599435, mean_eps: 0.651511
  58242/150000: episode: 580, duration: 0.757s, episode steps: 114, steps per second: 151, episode reward: -83.389, mean reward: -0.731 [-100.000, 11.385], mean action: 1.544 [0.000, 3.000],  loss: 12.849897, mae: 36.743316, mean_q: 30.227954, mean_eps: 0.650893
  58326/150000: episode: 581, duration: 0.554s, episode steps:  84, steps per second: 152, episode reward: -61.054, mean reward: -0.727 [-100.000,  9.079], mean action: 1.476 [0.000, 3.000],  loss: 13.426785, mae: 37.075927, mean_q: 29.472427, mean_eps: 0.650299
  58430/150000: episode: 582, duration: 0.621s, episode steps: 104, steps per second: 168, episode reward: -18.573, mean reward: -0.179 [-100.000, 12.887], mean action: 1.538 [0.000, 3.000],  loss: 10.021172, mae: 37.477491, mean_q: 31.937136, mean_eps: 0.649735
  58541/150000: episode: 583, duration: 0.651s, episode steps: 111, steps per second: 170, episode reward: -113.447, mean reward: -1.022 [-100.000,  9.111], mean action: 1.523 [0.000, 3.000],  loss: 8.842830, mae: 36.647939, mean_q: 30.294913, mean_eps: 0.649090
  58653/150000: episode: 584, duration: 0.695s, episode steps: 112, steps per second: 161, episode reward: -59.829, mean reward: -0.534 [-100.000,  8.414], mean action: 1.661 [0.000, 3.000],  loss: 8.918803, mae: 36.766414, mean_q: 31.103255, mean_eps: 0.648421
  58774/150000: episode: 585, duration: 0.724s, episode steps: 121, steps per second: 167, episode reward: -96.241, mean reward: -0.795 [-100.000, 11.046], mean action: 1.529 [0.000, 3.000],  loss: 10.708540, mae: 36.994087, mean_q: 31.920527, mean_eps: 0.647722
  58892/150000: episode: 586, duration: 0.694s, episode steps: 118, steps per second: 170, episode reward: -70.819, mean reward: -0.600 [-100.000,  7.242], mean action: 1.458 [0.000, 3.000],  loss: 8.301938, mae: 37.204896, mean_q: 32.109655, mean_eps: 0.647005
  58997/150000: episode: 587, duration: 0.650s, episode steps: 105, steps per second: 162, episode reward: -83.806, mean reward: -0.798 [-100.000, 13.089], mean action: 1.381 [0.000, 3.000],  loss: 5.638984, mae: 36.417250, mean_q: 30.933637, mean_eps: 0.646336
  59137/150000: episode: 588, duration: 0.831s, episode steps: 140, steps per second: 169, episode reward: -131.571, mean reward: -0.940 [-100.000,  3.224], mean action: 1.650 [0.000, 3.000],  loss: 6.396184, mae: 37.353534, mean_q: 32.039224, mean_eps: 0.645601
  59257/150000: episode: 589, duration: 0.697s, episode steps: 120, steps per second: 172, episode reward: -69.917, mean reward: -0.583 [-100.000, 36.640], mean action: 1.642 [0.000, 3.000],  loss: 6.762066, mae: 37.174066, mean_q: 30.873205, mean_eps: 0.644821
  59359/150000: episode: 590, duration: 0.632s, episode steps: 102, steps per second: 161, episode reward: -56.254, mean reward: -0.552 [-100.000, 13.740], mean action: 1.588 [0.000, 3.000],  loss: 6.157506, mae: 37.378699, mean_q: 31.765129, mean_eps: 0.644155
  59481/150000: episode: 591, duration: 0.714s, episode steps: 122, steps per second: 171, episode reward: -81.355, mean reward: -0.667 [-100.000,  8.229], mean action: 1.475 [0.000, 3.000],  loss: 12.386526, mae: 37.121151, mean_q: 30.384529, mean_eps: 0.643483
  59598/150000: episode: 592, duration: 0.680s, episode steps: 117, steps per second: 172, episode reward: -31.216, mean reward: -0.267 [-100.000, 13.231], mean action: 1.650 [0.000, 3.000],  loss: 5.187661, mae: 36.653679, mean_q: 30.874621, mean_eps: 0.642766
  59697/150000: episode: 593, duration: 0.608s, episode steps:  99, steps per second: 163, episode reward: -91.885, mean reward: -0.928 [-100.000,  6.471], mean action: 1.566 [0.000, 3.000],  loss: 19.222755, mae: 37.663849, mean_q: 31.007637, mean_eps: 0.642118
  59788/150000: episode: 594, duration: 0.547s, episode steps:  91, steps per second: 166, episode reward: -126.601, mean reward: -1.391 [-100.000, 32.948], mean action: 1.626 [0.000, 3.000],  loss: 8.210692, mae: 38.044441, mean_q: 32.076899, mean_eps: 0.641548
  59904/150000: episode: 595, duration: 0.712s, episode steps: 116, steps per second: 163, episode reward: -28.749, mean reward: -0.248 [-100.000, 16.340], mean action: 1.690 [0.000, 3.000],  loss: 6.793458, mae: 37.287640, mean_q: 30.650060, mean_eps: 0.640927
  59995/150000: episode: 596, duration: 0.555s, episode steps:  91, steps per second: 164, episode reward: -121.483, mean reward: -1.335 [-100.000,  7.454], mean action: 1.560 [0.000, 3.000],  loss: 15.311674, mae: 37.560747, mean_q: 30.381361, mean_eps: 0.640306
  60072/150000: episode: 597, duration: 0.561s, episode steps:  77, steps per second: 137, episode reward: -23.345, mean reward: -0.303 [-100.000, 11.108], mean action: 1.610 [0.000, 3.000],  loss: 8.259497, mae: 37.574518, mean_q: 31.796585, mean_eps: 0.639802
  60198/150000: episode: 598, duration: 0.791s, episode steps: 126, steps per second: 159, episode reward:  0.997, mean reward:  0.008 [-100.000, 19.272], mean action: 1.500 [0.000, 3.000],  loss: 8.295199, mae: 36.975005, mean_q: 30.274038, mean_eps: 0.639193
  60318/150000: episode: 599, duration: 0.724s, episode steps: 120, steps per second: 166, episode reward: -3.108, mean reward: -0.026 [-100.000, 17.704], mean action: 1.633 [0.000, 3.000],  loss: 9.251122, mae: 37.099882, mean_q: 30.580952, mean_eps: 0.638455
  61122/150000: episode: 600, duration: 6.172s, episode steps: 804, steps per second: 130, episode reward: -254.654, mean reward: -0.317 [-100.000, 43.631], mean action: 1.619 [0.000, 3.000],  loss: 10.138740, mae: 37.011481, mean_q: 32.117739, mean_eps: 0.635683
  61229/150000: episode: 601, duration: 0.662s, episode steps: 107, steps per second: 162, episode reward: -52.495, mean reward: -0.491 [-100.000, 15.465], mean action: 1.729 [0.000, 3.000],  loss: 13.973600, mae: 36.496714, mean_q: 31.942021, mean_eps: 0.632950
  61348/150000: episode: 602, duration: 0.873s, episode steps: 119, steps per second: 136, episode reward: -75.941, mean reward: -0.638 [-100.000, 10.894], mean action: 1.529 [0.000, 3.000],  loss: 7.741971, mae: 35.746239, mean_q: 30.327184, mean_eps: 0.632272
  61446/150000: episode: 603, duration: 0.675s, episode steps:  98, steps per second: 145, episode reward: -64.939, mean reward: -0.663 [-100.000, 11.269], mean action: 1.531 [0.000, 3.000],  loss: 5.626952, mae: 36.784505, mean_q: 30.348047, mean_eps: 0.631621
  61546/150000: episode: 604, duration: 0.690s, episode steps: 100, steps per second: 145, episode reward: -67.516, mean reward: -0.675 [-100.000, 13.026], mean action: 1.640 [0.000, 3.000],  loss: 12.127209, mae: 36.695458, mean_q: 32.532013, mean_eps: 0.631027
  61643/150000: episode: 605, duration: 0.714s, episode steps:  97, steps per second: 136, episode reward: -89.945, mean reward: -0.927 [-100.000, 16.370], mean action: 1.804 [0.000, 3.000],  loss: 10.445410, mae: 37.169690, mean_q: 32.677094, mean_eps: 0.630436
  61747/150000: episode: 606, duration: 0.743s, episode steps: 104, steps per second: 140, episode reward: -34.238, mean reward: -0.329 [-100.000, 14.646], mean action: 1.529 [0.000, 3.000],  loss: 12.401283, mae: 36.835230, mean_q: 32.113543, mean_eps: 0.629833
  61883/150000: episode: 607, duration: 0.964s, episode steps: 136, steps per second: 141, episode reward: -32.057, mean reward: -0.236 [-100.000, 29.973], mean action: 1.757 [0.000, 3.000],  loss: 17.365832, mae: 36.815997, mean_q: 31.217047, mean_eps: 0.629113
  61976/150000: episode: 608, duration: 0.628s, episode steps:  93, steps per second: 148, episode reward: -16.706, mean reward: -0.180 [-100.000, 13.734], mean action: 1.559 [0.000, 3.000],  loss: 9.047100, mae: 36.852361, mean_q: 31.846764, mean_eps: 0.628426
  62087/150000: episode: 609, duration: 0.700s, episode steps: 111, steps per second: 159, episode reward: -57.176, mean reward: -0.515 [-100.000, 16.299], mean action: 1.532 [0.000, 3.000],  loss: 9.764949, mae: 36.062024, mean_q: 32.363870, mean_eps: 0.627814
  62207/150000: episode: 610, duration: 0.836s, episode steps: 120, steps per second: 144, episode reward: -24.545, mean reward: -0.205 [-100.000,  8.600], mean action: 1.717 [0.000, 3.000],  loss: 12.513025, mae: 36.845879, mean_q: 31.409869, mean_eps: 0.627121
  62354/150000: episode: 611, duration: 1.060s, episode steps: 147, steps per second: 139, episode reward: -130.835, mean reward: -0.890 [-100.000,  3.807], mean action: 1.653 [0.000, 3.000],  loss: 8.862299, mae: 36.796997, mean_q: 32.694428, mean_eps: 0.626320
  62467/150000: episode: 612, duration: 0.810s, episode steps: 113, steps per second: 140, episode reward: -47.752, mean reward: -0.423 [-100.000, 15.732], mean action: 1.558 [0.000, 3.000],  loss: 10.002110, mae: 36.283402, mean_q: 31.949380, mean_eps: 0.625540
  62566/150000: episode: 613, duration: 0.694s, episode steps:  99, steps per second: 143, episode reward: -80.156, mean reward: -0.810 [-100.000,  6.997], mean action: 1.414 [0.000, 3.000],  loss: 10.746515, mae: 36.192582, mean_q: 31.276648, mean_eps: 0.624904
  62694/150000: episode: 614, duration: 0.846s, episode steps: 128, steps per second: 151, episode reward: -28.455, mean reward: -0.222 [-100.000, 17.526], mean action: 1.609 [0.000, 3.000],  loss: 12.046289, mae: 36.127426, mean_q: 32.027902, mean_eps: 0.624223
  62774/150000: episode: 615, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -52.501, mean reward: -0.656 [-100.000,  8.301], mean action: 1.562 [0.000, 3.000],  loss: 8.033662, mae: 36.648788, mean_q: 32.075576, mean_eps: 0.623599
  62899/150000: episode: 616, duration: 0.862s, episode steps: 125, steps per second: 145, episode reward: -50.954, mean reward: -0.408 [-100.000,  9.436], mean action: 1.624 [0.000, 3.000],  loss: 10.193516, mae: 36.685132, mean_q: 32.234609, mean_eps: 0.622984
  62983/150000: episode: 617, duration: 0.524s, episode steps:  84, steps per second: 160, episode reward: -71.641, mean reward: -0.853 [-100.000,  9.846], mean action: 1.548 [0.000, 3.000],  loss: 11.673400, mae: 36.371776, mean_q: 31.621680, mean_eps: 0.622357
  63112/150000: episode: 618, duration: 0.816s, episode steps: 129, steps per second: 158, episode reward: 10.606, mean reward:  0.082 [-100.000, 17.791], mean action: 1.667 [0.000, 3.000],  loss: 8.552759, mae: 35.946824, mean_q: 32.126259, mean_eps: 0.621718
  63223/150000: episode: 619, duration: 0.712s, episode steps: 111, steps per second: 156, episode reward: -120.824, mean reward: -1.089 [-100.000,  9.562], mean action: 1.550 [0.000, 3.000],  loss: 6.205409, mae: 36.351354, mean_q: 33.430557, mean_eps: 0.620998
  63388/150000: episode: 620, duration: 1.160s, episode steps: 165, steps per second: 142, episode reward: -31.703, mean reward: -0.192 [-100.000, 19.737], mean action: 1.552 [0.000, 3.000],  loss: 8.132185, mae: 36.636512, mean_q: 32.424961, mean_eps: 0.620170
  63515/150000: episode: 621, duration: 0.867s, episode steps: 127, steps per second: 146, episode reward: -112.128, mean reward: -0.883 [-100.000, 13.448], mean action: 1.512 [0.000, 3.000],  loss: 12.469698, mae: 36.318086, mean_q: 32.334608, mean_eps: 0.619294
  63584/150000: episode: 622, duration: 0.444s, episode steps:  69, steps per second: 156, episode reward: -15.310, mean reward: -0.222 [-100.000, 17.297], mean action: 1.754 [0.000, 3.000],  loss: 11.118805, mae: 36.383483, mean_q: 32.062408, mean_eps: 0.618706
  63658/150000: episode: 623, duration: 0.487s, episode steps:  74, steps per second: 152, episode reward: -82.922, mean reward: -1.121 [-100.000,  8.974], mean action: 1.770 [0.000, 3.000],  loss: 9.381908, mae: 36.505902, mean_q: 32.350917, mean_eps: 0.618277
  63764/150000: episode: 624, duration: 0.726s, episode steps: 106, steps per second: 146, episode reward: -103.383, mean reward: -0.975 [-100.000, 12.185], mean action: 1.349 [0.000, 3.000],  loss: 8.187799, mae: 35.731620, mean_q: 31.860172, mean_eps: 0.617737
  63872/150000: episode: 625, duration: 0.781s, episode steps: 108, steps per second: 138, episode reward: -37.150, mean reward: -0.344 [-100.000, 12.953], mean action: 1.463 [0.000, 3.000],  loss: 9.091719, mae: 36.273804, mean_q: 31.866423, mean_eps: 0.617095
  63942/150000: episode: 626, duration: 0.462s, episode steps:  70, steps per second: 152, episode reward: -77.048, mean reward: -1.101 [-100.000, 16.035], mean action: 1.586 [0.000, 3.000],  loss: 6.020522, mae: 36.340036, mean_q: 31.986498, mean_eps: 0.616561
  64054/150000: episode: 627, duration: 0.706s, episode steps: 112, steps per second: 159, episode reward: -82.185, mean reward: -0.734 [-100.000,  8.685], mean action: 1.527 [0.000, 3.000],  loss: 9.437363, mae: 36.544302, mean_q: 33.080641, mean_eps: 0.616015
  64170/150000: episode: 628, duration: 0.901s, episode steps: 116, steps per second: 129, episode reward: -49.701, mean reward: -0.428 [-100.000, 11.430], mean action: 1.664 [0.000, 3.000],  loss: 6.101989, mae: 36.623005, mean_q: 33.806065, mean_eps: 0.615331
  64253/150000: episode: 629, duration: 0.641s, episode steps:  83, steps per second: 129, episode reward: -50.130, mean reward: -0.604 [-100.000, 24.638], mean action: 1.687 [0.000, 3.000],  loss: 6.179828, mae: 36.763160, mean_q: 32.840833, mean_eps: 0.614734
  64337/150000: episode: 630, duration: 0.606s, episode steps:  84, steps per second: 139, episode reward: -38.190, mean reward: -0.455 [-100.000,  7.985], mean action: 1.476 [0.000, 3.000],  loss: 8.671219, mae: 36.014231, mean_q: 32.019442, mean_eps: 0.614233
  64459/150000: episode: 631, duration: 0.835s, episode steps: 122, steps per second: 146, episode reward: -80.648, mean reward: -0.661 [-100.000, 18.745], mean action: 1.664 [0.000, 3.000],  loss: 12.708392, mae: 36.289663, mean_q: 33.032401, mean_eps: 0.613615
  64554/150000: episode: 632, duration: 0.677s, episode steps:  95, steps per second: 140, episode reward: -40.821, mean reward: -0.430 [-100.000, 20.437], mean action: 1.632 [0.000, 3.000],  loss: 5.590254, mae: 36.515598, mean_q: 32.437167, mean_eps: 0.612964
  64640/150000: episode: 633, duration: 0.542s, episode steps:  86, steps per second: 159, episode reward: -6.980, mean reward: -0.081 [-100.000, 27.319], mean action: 1.651 [0.000, 3.000],  loss: 13.477446, mae: 36.530971, mean_q: 33.029412, mean_eps: 0.612421
  64750/150000: episode: 634, duration: 0.806s, episode steps: 110, steps per second: 137, episode reward: -35.872, mean reward: -0.326 [-100.000, 11.404], mean action: 1.545 [0.000, 3.000],  loss: 5.318326, mae: 36.484777, mean_q: 34.400749, mean_eps: 0.611833
  64924/150000: episode: 635, duration: 1.174s, episode steps: 174, steps per second: 148, episode reward: -11.706, mean reward: -0.067 [-100.000, 16.867], mean action: 1.713 [0.000, 3.000],  loss: 16.555889, mae: 36.503101, mean_q: 32.165806, mean_eps: 0.610981
  65042/150000: episode: 636, duration: 0.814s, episode steps: 118, steps per second: 145, episode reward: -123.966, mean reward: -1.051 [-100.000, 16.884], mean action: 1.686 [0.000, 3.000],  loss: 10.212868, mae: 36.655072, mean_q: 33.155814, mean_eps: 0.610105
  65126/150000: episode: 637, duration: 0.570s, episode steps:  84, steps per second: 147, episode reward: -91.122, mean reward: -1.085 [-100.000,  9.460], mean action: 1.369 [0.000, 3.000],  loss: 6.401228, mae: 37.014871, mean_q: 32.145575, mean_eps: 0.609499
  65248/150000: episode: 638, duration: 0.802s, episode steps: 122, steps per second: 152, episode reward: -76.381, mean reward: -0.626 [-100.000,  9.683], mean action: 1.377 [0.000, 3.000],  loss: 7.091774, mae: 36.202140, mean_q: 32.789044, mean_eps: 0.608881
  65329/150000: episode: 639, duration: 0.578s, episode steps:  81, steps per second: 140, episode reward: -63.012, mean reward: -0.778 [-100.000,  6.891], mean action: 1.617 [0.000, 3.000],  loss: 13.612518, mae: 36.298477, mean_q: 34.160384, mean_eps: 0.608272
  65421/150000: episode: 640, duration: 0.567s, episode steps:  92, steps per second: 162, episode reward: -92.727, mean reward: -1.008 [-100.000,  7.019], mean action: 1.587 [0.000, 3.000],  loss: 7.789081, mae: 36.367970, mean_q: 32.598399, mean_eps: 0.607753
  65508/150000: episode: 641, duration: 0.568s, episode steps:  87, steps per second: 153, episode reward: -39.620, mean reward: -0.455 [-100.000, 12.743], mean action: 1.655 [0.000, 3.000],  loss: 8.306753, mae: 36.261370, mean_q: 33.731461, mean_eps: 0.607216
  65609/150000: episode: 642, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: -50.808, mean reward: -0.503 [-100.000, 16.307], mean action: 1.564 [0.000, 3.000],  loss: 5.325551, mae: 36.339567, mean_q: 33.021239, mean_eps: 0.606652
  65766/150000: episode: 643, duration: 0.969s, episode steps: 157, steps per second: 162, episode reward:  0.891, mean reward:  0.006 [-100.000, 23.080], mean action: 1.541 [0.000, 3.000],  loss: 8.888986, mae: 36.586880, mean_q: 33.579307, mean_eps: 0.605878
  65848/150000: episode: 644, duration: 0.490s, episode steps:  82, steps per second: 167, episode reward: -63.115, mean reward: -0.770 [-100.000, 20.556], mean action: 1.488 [0.000, 3.000],  loss: 7.475310, mae: 36.562256, mean_q: 33.166438, mean_eps: 0.605161
  65962/150000: episode: 645, duration: 0.710s, episode steps: 114, steps per second: 161, episode reward: -86.983, mean reward: -0.763 [-100.000, 20.184], mean action: 1.482 [0.000, 3.000],  loss: 7.987037, mae: 36.261987, mean_q: 31.785556, mean_eps: 0.604573
  66105/150000: episode: 646, duration: 1.051s, episode steps: 143, steps per second: 136, episode reward:  1.582, mean reward:  0.011 [-100.000, 13.568], mean action: 1.538 [0.000, 3.000],  loss: 10.348276, mae: 36.420920, mean_q: 33.279029, mean_eps: 0.603802
  66201/150000: episode: 647, duration: 0.643s, episode steps:  96, steps per second: 149, episode reward: -98.083, mean reward: -1.022 [-100.000,  5.893], mean action: 1.458 [0.000, 3.000],  loss: 10.131660, mae: 36.190576, mean_q: 33.444989, mean_eps: 0.603085
  66300/150000: episode: 648, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward: -14.675, mean reward: -0.148 [-100.000, 17.438], mean action: 1.545 [0.000, 3.000],  loss: 5.937504, mae: 36.550457, mean_q: 32.802100, mean_eps: 0.602500
  66411/150000: episode: 649, duration: 0.831s, episode steps: 111, steps per second: 134, episode reward: -71.371, mean reward: -0.643 [-100.000, 11.124], mean action: 1.901 [0.000, 3.000],  loss: 13.406698, mae: 36.987794, mean_q: 35.178202, mean_eps: 0.601870
  66506/150000: episode: 650, duration: 0.672s, episode steps:  95, steps per second: 141, episode reward: -90.105, mean reward: -0.948 [-100.000, 10.374], mean action: 1.537 [0.000, 3.000],  loss: 8.543529, mae: 36.735572, mean_q: 34.275010, mean_eps: 0.601252
  66600/150000: episode: 651, duration: 0.650s, episode steps:  94, steps per second: 145, episode reward: -67.408, mean reward: -0.717 [-100.000, 11.052], mean action: 1.489 [0.000, 3.000],  loss: 10.802746, mae: 36.729541, mean_q: 33.790950, mean_eps: 0.600685
  66696/150000: episode: 652, duration: 0.654s, episode steps:  96, steps per second: 147, episode reward: -79.054, mean reward: -0.823 [-100.000,  6.971], mean action: 1.719 [0.000, 3.000],  loss: 10.118481, mae: 36.871937, mean_q: 33.409349, mean_eps: 0.600115
  66791/150000: episode: 653, duration: 0.571s, episode steps:  95, steps per second: 166, episode reward: -52.975, mean reward: -0.558 [-100.000, 13.093], mean action: 1.663 [0.000, 3.000],  loss: 7.988089, mae: 36.156872, mean_q: 31.840386, mean_eps: 0.599542
  66898/150000: episode: 654, duration: 0.635s, episode steps: 107, steps per second: 168, episode reward: -94.264, mean reward: -0.881 [-100.000, 11.537], mean action: 1.383 [0.000, 3.000],  loss: 5.157532, mae: 36.593515, mean_q: 32.985273, mean_eps: 0.598936
  67025/150000: episode: 655, duration: 0.803s, episode steps: 127, steps per second: 158, episode reward: -84.386, mean reward: -0.664 [-100.000, 11.555], mean action: 1.638 [0.000, 3.000],  loss: 10.677036, mae: 36.646324, mean_q: 34.107232, mean_eps: 0.598234
  67167/150000: episode: 656, duration: 0.834s, episode steps: 142, steps per second: 170, episode reward: -7.078, mean reward: -0.050 [-100.000, 17.640], mean action: 1.472 [0.000, 3.000],  loss: 7.362206, mae: 36.575797, mean_q: 34.630235, mean_eps: 0.597427
  67288/150000: episode: 657, duration: 0.715s, episode steps: 121, steps per second: 169, episode reward: -146.829, mean reward: -1.213 [-100.000, 17.045], mean action: 1.537 [0.000, 3.000],  loss: 11.040832, mae: 36.709035, mean_q: 34.397313, mean_eps: 0.596638
  67380/150000: episode: 658, duration: 0.580s, episode steps:  92, steps per second: 159, episode reward:  6.308, mean reward:  0.069 [-100.000, 29.479], mean action: 1.674 [0.000, 3.000],  loss: 14.520778, mae: 36.882134, mean_q: 34.411219, mean_eps: 0.595999
  67465/150000: episode: 659, duration: 0.511s, episode steps:  85, steps per second: 166, episode reward: -2.137, mean reward: -0.025 [-100.000, 13.864], mean action: 1.635 [0.000, 3.000],  loss: 6.826038, mae: 37.505252, mean_q: 34.966282, mean_eps: 0.595468
  67550/150000: episode: 660, duration: 0.498s, episode steps:  85, steps per second: 171, episode reward: -74.088, mean reward: -0.872 [-100.000, 12.947], mean action: 1.529 [0.000, 3.000],  loss: 11.589391, mae: 36.776743, mean_q: 33.823914, mean_eps: 0.594958
  67649/150000: episode: 661, duration: 0.589s, episode steps:  99, steps per second: 168, episode reward: -62.326, mean reward: -0.630 [-100.000,  8.536], mean action: 1.697 [0.000, 3.000],  loss: 22.220288, mae: 36.484659, mean_q: 33.253615, mean_eps: 0.594406
  67749/150000: episode: 662, duration: 0.650s, episode steps: 100, steps per second: 154, episode reward: -196.385, mean reward: -1.964 [-100.000, 23.538], mean action: 1.950 [0.000, 3.000],  loss: 13.471986, mae: 36.559913, mean_q: 33.441958, mean_eps: 0.593809
  67827/150000: episode: 663, duration: 0.496s, episode steps:  78, steps per second: 157, episode reward: -74.461, mean reward: -0.955 [-100.000, 11.175], mean action: 1.346 [0.000, 3.000],  loss: 11.644343, mae: 36.994900, mean_q: 34.420289, mean_eps: 0.593275
  67979/150000: episode: 664, duration: 0.921s, episode steps: 152, steps per second: 165, episode reward: -195.051, mean reward: -1.283 [-100.000, 78.364], mean action: 1.592 [0.000, 3.000],  loss: 12.499783, mae: 36.539656, mean_q: 34.007516, mean_eps: 0.592585
  68078/150000: episode: 665, duration: 0.665s, episode steps:  99, steps per second: 149, episode reward: -78.305, mean reward: -0.791 [-100.000, 10.340], mean action: 1.576 [0.000, 3.000],  loss: 6.832740, mae: 36.683656, mean_q: 33.106435, mean_eps: 0.591832
  68932/150000: episode: 666, duration: 5.999s, episode steps: 854, steps per second: 142, episode reward: -172.016, mean reward: -0.201 [-100.000, 43.276], mean action: 1.632 [0.000, 3.000],  loss: 10.904255, mae: 37.087920, mean_q: 35.100152, mean_eps: 0.588973
  69036/150000: episode: 667, duration: 0.632s, episode steps: 104, steps per second: 165, episode reward: -52.661, mean reward: -0.506 [-100.000, 17.274], mean action: 1.712 [0.000, 3.000],  loss: 8.003716, mae: 36.470110, mean_q: 34.469548, mean_eps: 0.586099
  69150/150000: episode: 668, duration: 0.677s, episode steps: 114, steps per second: 168, episode reward: -56.876, mean reward: -0.499 [-100.000, 11.558], mean action: 1.614 [0.000, 3.000],  loss: 7.889652, mae: 37.214550, mean_q: 35.172249, mean_eps: 0.585445
  69306/150000: episode: 669, duration: 0.935s, episode steps: 156, steps per second: 167, episode reward: -27.559, mean reward: -0.177 [-100.000, 52.744], mean action: 1.647 [0.000, 3.000],  loss: 9.857288, mae: 36.884766, mean_q: 34.901722, mean_eps: 0.584635
  69377/150000: episode: 670, duration: 0.446s, episode steps:  71, steps per second: 159, episode reward: -47.431, mean reward: -0.668 [-100.000, 19.962], mean action: 1.620 [0.000, 3.000],  loss: 7.454180, mae: 35.789007, mean_q: 34.224571, mean_eps: 0.583954
  69573/150000: episode: 671, duration: 1.164s, episode steps: 196, steps per second: 168, episode reward: -205.956, mean reward: -1.051 [-100.000, 38.982], mean action: 1.566 [0.000, 3.000],  loss: 9.356493, mae: 36.277627, mean_q: 34.465744, mean_eps: 0.583153
  69678/150000: episode: 672, duration: 0.671s, episode steps: 105, steps per second: 156, episode reward: -25.886, mean reward: -0.247 [-100.000, 16.325], mean action: 1.752 [0.000, 3.000],  loss: 6.260111, mae: 36.606420, mean_q: 35.040654, mean_eps: 0.582250
  69795/150000: episode: 673, duration: 0.700s, episode steps: 117, steps per second: 167, episode reward: -152.799, mean reward: -1.306 [-100.000,  2.811], mean action: 1.282 [0.000, 3.000],  loss: 7.523711, mae: 35.987432, mean_q: 34.557813, mean_eps: 0.581584
  69870/150000: episode: 674, duration: 0.445s, episode steps:  75, steps per second: 168, episode reward: -56.309, mean reward: -0.751 [-100.000,  4.856], mean action: 1.640 [0.000, 3.000],  loss: 10.313350, mae: 36.613759, mean_q: 34.685170, mean_eps: 0.581008
  69961/150000: episode: 675, duration: 0.535s, episode steps:  91, steps per second: 170, episode reward: -29.036, mean reward: -0.319 [-100.000, 19.538], mean action: 1.659 [0.000, 3.000],  loss: 17.181917, mae: 36.510195, mean_q: 35.280410, mean_eps: 0.580510
  70082/150000: episode: 676, duration: 0.819s, episode steps: 121, steps per second: 148, episode reward: -69.239, mean reward: -0.572 [-100.000,  7.119], mean action: 1.826 [0.000, 3.000],  loss: 10.252351, mae: 36.465498, mean_q: 35.197873, mean_eps: 0.579874
  70191/150000: episode: 677, duration: 0.736s, episode steps: 109, steps per second: 148, episode reward: -20.156, mean reward: -0.185 [-100.000, 17.306], mean action: 1.679 [0.000, 3.000],  loss: 8.634075, mae: 36.949740, mean_q: 34.833867, mean_eps: 0.579184
  70296/150000: episode: 678, duration: 0.660s, episode steps: 105, steps per second: 159, episode reward: -66.259, mean reward: -0.631 [-100.000, 11.381], mean action: 1.486 [0.000, 3.000],  loss: 13.936002, mae: 36.743848, mean_q: 36.396341, mean_eps: 0.578542
  70422/150000: episode: 679, duration: 0.845s, episode steps: 126, steps per second: 149, episode reward: -3.484, mean reward: -0.028 [-100.000,  8.042], mean action: 1.643 [0.000, 3.000],  loss: 6.781528, mae: 36.043537, mean_q: 34.638552, mean_eps: 0.577849
  70534/150000: episode: 680, duration: 0.682s, episode steps: 112, steps per second: 164, episode reward: -7.169, mean reward: -0.064 [-100.000, 21.188], mean action: 1.473 [0.000, 3.000],  loss: 8.670894, mae: 35.673443, mean_q: 33.547078, mean_eps: 0.577135
  70667/150000: episode: 681, duration: 0.795s, episode steps: 133, steps per second: 167, episode reward: -204.029, mean reward: -1.534 [-100.000, 36.190], mean action: 1.526 [0.000, 3.000],  loss: 6.879035, mae: 36.618894, mean_q: 34.993234, mean_eps: 0.576400
  70785/150000: episode: 682, duration: 0.726s, episode steps: 118, steps per second: 162, episode reward:  6.260, mean reward:  0.053 [-100.000, 17.702], mean action: 1.449 [0.000, 3.000],  loss: 12.422294, mae: 36.591832, mean_q: 35.346019, mean_eps: 0.575647
  70861/150000: episode: 683, duration: 0.459s, episode steps:  76, steps per second: 166, episode reward: -100.027, mean reward: -1.316 [-100.000,  9.990], mean action: 1.539 [0.000, 3.000],  loss: 13.276506, mae: 35.682008, mean_q: 33.447335, mean_eps: 0.575065
  70980/150000: episode: 684, duration: 0.720s, episode steps: 119, steps per second: 165, episode reward: -64.549, mean reward: -0.542 [-100.000,  6.081], mean action: 1.597 [0.000, 3.000],  loss: 10.228670, mae: 36.348263, mean_q: 35.182828, mean_eps: 0.574480
  71218/150000: episode: 685, duration: 1.526s, episode steps: 238, steps per second: 156, episode reward: -265.944, mean reward: -1.117 [-100.000, 43.481], mean action: 1.613 [0.000, 3.000],  loss: 7.543252, mae: 36.790333, mean_q: 35.198950, mean_eps: 0.573409
  71351/150000: episode: 686, duration: 0.826s, episode steps: 133, steps per second: 161, episode reward: -110.037, mean reward: -0.827 [-100.000,  3.939], mean action: 1.707 [0.000, 3.000],  loss: 7.569968, mae: 36.865139, mean_q: 35.074709, mean_eps: 0.572296
  71423/150000: episode: 687, duration: 0.486s, episode steps:  72, steps per second: 148, episode reward: -55.501, mean reward: -0.771 [-100.000,  6.562], mean action: 1.486 [0.000, 3.000],  loss: 10.208395, mae: 36.316172, mean_q: 33.824325, mean_eps: 0.571681
  71539/150000: episode: 688, duration: 0.790s, episode steps: 116, steps per second: 147, episode reward: -66.984, mean reward: -0.577 [-100.000, 10.168], mean action: 1.655 [0.000, 3.000],  loss: 6.716196, mae: 36.823066, mean_q: 35.251022, mean_eps: 0.571117
  71631/150000: episode: 689, duration: 0.616s, episode steps:  92, steps per second: 149, episode reward: -78.571, mean reward: -0.854 [-100.000,  6.706], mean action: 1.457 [0.000, 3.000],  loss: 11.210765, mae: 37.035214, mean_q: 35.757925, mean_eps: 0.570493
  71718/150000: episode: 690, duration: 0.616s, episode steps:  87, steps per second: 141, episode reward: -82.666, mean reward: -0.950 [-100.000, 10.549], mean action: 1.644 [0.000, 3.000],  loss: 9.292128, mae: 36.074262, mean_q: 33.050706, mean_eps: 0.569956
  71840/150000: episode: 691, duration: 0.815s, episode steps: 122, steps per second: 150, episode reward: -54.657, mean reward: -0.448 [-100.000,  6.953], mean action: 1.607 [0.000, 3.000],  loss: 12.853765, mae: 36.523194, mean_q: 35.245891, mean_eps: 0.569329
  71961/150000: episode: 692, duration: 0.731s, episode steps: 121, steps per second: 166, episode reward: -36.929, mean reward: -0.305 [-100.000, 11.054], mean action: 1.570 [0.000, 3.000],  loss: 9.581751, mae: 36.312684, mean_q: 34.692553, mean_eps: 0.568600
  72050/150000: episode: 693, duration: 0.571s, episode steps:  89, steps per second: 156, episode reward: -15.570, mean reward: -0.175 [-100.000, 14.589], mean action: 1.674 [0.000, 3.000],  loss: 9.762449, mae: 37.179571, mean_q: 35.827471, mean_eps: 0.567970
  72194/150000: episode: 694, duration: 0.865s, episode steps: 144, steps per second: 166, episode reward: -23.073, mean reward: -0.160 [-100.000, 14.952], mean action: 1.833 [0.000, 3.000],  loss: 7.610754, mae: 37.327129, mean_q: 35.972003, mean_eps: 0.567271
  72301/150000: episode: 695, duration: 0.657s, episode steps: 107, steps per second: 163, episode reward: -60.378, mean reward: -0.564 [-100.000, 10.550], mean action: 1.664 [0.000, 3.000],  loss: 13.445471, mae: 36.913796, mean_q: 35.782426, mean_eps: 0.566518
  72426/150000: episode: 696, duration: 0.870s, episode steps: 125, steps per second: 144, episode reward: -44.622, mean reward: -0.357 [-100.000, 13.771], mean action: 1.608 [0.000, 3.000],  loss: 9.367026, mae: 38.094198, mean_q: 36.470049, mean_eps: 0.565822
  72506/150000: episode: 697, duration: 0.526s, episode steps:  80, steps per second: 152, episode reward: -54.175, mean reward: -0.677 [-100.000, 10.302], mean action: 1.613 [0.000, 3.000],  loss: 10.296280, mae: 37.388580, mean_q: 36.505368, mean_eps: 0.565207
  72601/150000: episode: 698, duration: 0.603s, episode steps:  95, steps per second: 158, episode reward: -71.191, mean reward: -0.749 [-100.000, 10.763], mean action: 1.611 [0.000, 3.000],  loss: 12.327955, mae: 37.441034, mean_q: 36.057527, mean_eps: 0.564682
  72693/150000: episode: 699, duration: 0.588s, episode steps:  92, steps per second: 156, episode reward: -69.843, mean reward: -0.759 [-100.000, 11.020], mean action: 1.772 [0.000, 3.000],  loss: 11.822445, mae: 37.119775, mean_q: 34.879322, mean_eps: 0.564121
  72819/150000: episode: 700, duration: 0.816s, episode steps: 126, steps per second: 154, episode reward:  3.174, mean reward:  0.025 [-100.000, 18.750], mean action: 1.571 [0.000, 3.000],  loss: 10.257791, mae: 36.884113, mean_q: 35.894587, mean_eps: 0.563467
  72926/150000: episode: 701, duration: 0.662s, episode steps: 107, steps per second: 162, episode reward: 11.651, mean reward:  0.109 [-100.000, 14.891], mean action: 1.682 [0.000, 3.000],  loss: 7.331187, mae: 36.741415, mean_q: 34.872925, mean_eps: 0.562768
  73031/150000: episode: 702, duration: 0.666s, episode steps: 105, steps per second: 158, episode reward: -14.187, mean reward: -0.135 [-100.000, 14.367], mean action: 1.638 [0.000, 3.000],  loss: 7.843798, mae: 37.602688, mean_q: 35.572903, mean_eps: 0.562132
  73130/150000: episode: 703, duration: 0.640s, episode steps:  99, steps per second: 155, episode reward: -86.250, mean reward: -0.871 [-100.000, 12.138], mean action: 1.737 [0.000, 3.000],  loss: 9.153818, mae: 37.965843, mean_q: 35.156726, mean_eps: 0.561520
  73231/150000: episode: 704, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: -40.844, mean reward: -0.404 [-100.000, 13.993], mean action: 1.713 [0.000, 3.000],  loss: 6.667089, mae: 37.664401, mean_q: 35.744855, mean_eps: 0.560920
  73309/150000: episode: 705, duration: 0.534s, episode steps:  78, steps per second: 146, episode reward: -48.422, mean reward: -0.621 [-100.000,  8.255], mean action: 1.654 [0.000, 3.000],  loss: 8.896444, mae: 36.909713, mean_q: 35.990023, mean_eps: 0.560383
  73421/150000: episode: 706, duration: 0.801s, episode steps: 112, steps per second: 140, episode reward: -29.180, mean reward: -0.261 [-100.000, 11.593], mean action: 1.375 [0.000, 3.000],  loss: 9.463331, mae: 37.858881, mean_q: 35.972097, mean_eps: 0.559813
  73518/150000: episode: 707, duration: 0.636s, episode steps:  97, steps per second: 152, episode reward: -41.658, mean reward: -0.429 [-100.000, 10.089], mean action: 1.670 [0.000, 3.000],  loss: 9.575060, mae: 37.900701, mean_q: 37.681710, mean_eps: 0.559186
  73635/150000: episode: 708, duration: 0.804s, episode steps: 117, steps per second: 145, episode reward: -80.181, mean reward: -0.685 [-100.000, 10.252], mean action: 1.556 [0.000, 3.000],  loss: 9.079304, mae: 38.230059, mean_q: 36.571408, mean_eps: 0.558544
  73743/150000: episode: 709, duration: 0.827s, episode steps: 108, steps per second: 131, episode reward: -24.515, mean reward: -0.227 [-100.000, 13.437], mean action: 1.694 [0.000, 3.000],  loss: 6.803958, mae: 37.332007, mean_q: 35.598350, mean_eps: 0.557869
  73867/150000: episode: 710, duration: 0.786s, episode steps: 124, steps per second: 158, episode reward: -234.482, mean reward: -1.891 [-100.000, 45.546], mean action: 1.573 [0.000, 3.000],  loss: 5.723073, mae: 38.298832, mean_q: 37.184362, mean_eps: 0.557173
  73950/150000: episode: 711, duration: 0.575s, episode steps:  83, steps per second: 144, episode reward: -18.092, mean reward: -0.218 [-100.000,  8.916], mean action: 1.747 [0.000, 3.000],  loss: 7.039642, mae: 38.352551, mean_q: 37.148412, mean_eps: 0.556552
  74087/150000: episode: 712, duration: 0.961s, episode steps: 137, steps per second: 143, episode reward: -96.270, mean reward: -0.703 [-100.000,  4.378], mean action: 1.401 [0.000, 3.000],  loss: 11.072014, mae: 38.063633, mean_q: 36.475412, mean_eps: 0.555892
  74209/150000: episode: 713, duration: 0.830s, episode steps: 122, steps per second: 147, episode reward: -69.131, mean reward: -0.567 [-100.000, 12.508], mean action: 1.631 [0.000, 3.000],  loss: 18.642910, mae: 38.407461, mean_q: 35.803329, mean_eps: 0.555115
  74275/150000: episode: 714, duration: 0.436s, episode steps:  66, steps per second: 151, episode reward: -68.391, mean reward: -1.036 [-100.000, 10.744], mean action: 1.667 [0.000, 3.000],  loss: 13.763584, mae: 38.181439, mean_q: 35.559023, mean_eps: 0.554551
  74351/150000: episode: 715, duration: 0.586s, episode steps:  76, steps per second: 130, episode reward: -42.570, mean reward: -0.560 [-100.000,  9.105], mean action: 1.750 [0.000, 3.000],  loss: 7.168202, mae: 37.435510, mean_q: 34.289777, mean_eps: 0.554125
  74446/150000: episode: 716, duration: 0.759s, episode steps:  95, steps per second: 125, episode reward: -30.521, mean reward: -0.321 [-100.000, 12.501], mean action: 1.484 [0.000, 3.000],  loss: 11.732184, mae: 38.049630, mean_q: 36.201008, mean_eps: 0.553612
  74534/150000: episode: 717, duration: 0.919s, episode steps:  88, steps per second:  96, episode reward: 30.108, mean reward:  0.342 [-100.000, 51.595], mean action: 1.375 [0.000, 3.000],  loss: 7.596655, mae: 37.980282, mean_q: 36.869563, mean_eps: 0.553063
  74657/150000: episode: 718, duration: 0.979s, episode steps: 123, steps per second: 126, episode reward: -27.433, mean reward: -0.223 [-100.000, 16.619], mean action: 1.504 [0.000, 3.000],  loss: 7.512913, mae: 38.424508, mean_q: 37.815014, mean_eps: 0.552430
  74760/150000: episode: 719, duration: 0.646s, episode steps: 103, steps per second: 159, episode reward: -70.631, mean reward: -0.686 [-100.000,  8.627], mean action: 1.553 [0.000, 3.000],  loss: 9.491691, mae: 38.610747, mean_q: 35.903739, mean_eps: 0.551752
  74871/150000: episode: 720, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: 12.951, mean reward:  0.117 [-100.000, 19.166], mean action: 1.604 [0.000, 3.000],  loss: 9.617907, mae: 38.484868, mean_q: 36.518373, mean_eps: 0.551110
  75027/150000: episode: 721, duration: 1.040s, episode steps: 156, steps per second: 150, episode reward: -9.147, mean reward: -0.059 [-100.000, 13.757], mean action: 1.532 [0.000, 3.000],  loss: 8.607947, mae: 38.734129, mean_q: 35.990476, mean_eps: 0.550309
  75114/150000: episode: 722, duration: 0.561s, episode steps:  87, steps per second: 155, episode reward: -10.117, mean reward: -0.116 [-100.000, 26.782], mean action: 1.540 [0.000, 3.000],  loss: 7.547915, mae: 39.434100, mean_q: 35.953401, mean_eps: 0.549580
  75273/150000: episode: 723, duration: 1.430s, episode steps: 159, steps per second: 111, episode reward: -187.519, mean reward: -1.179 [-100.000, 15.406], mean action: 1.805 [0.000, 3.000],  loss: 10.235301, mae: 39.013017, mean_q: 36.933771, mean_eps: 0.548842
  75406/150000: episode: 724, duration: 1.239s, episode steps: 133, steps per second: 107, episode reward: -11.530, mean reward: -0.087 [-100.000, 11.289], mean action: 1.549 [0.000, 3.000],  loss: 11.788626, mae: 38.620603, mean_q: 37.407128, mean_eps: 0.547966
  75473/150000: episode: 725, duration: 0.543s, episode steps:  67, steps per second: 123, episode reward: -29.871, mean reward: -0.446 [-100.000,  9.973], mean action: 1.701 [0.000, 3.000],  loss: 13.656883, mae: 38.828170, mean_q: 36.349083, mean_eps: 0.547366
  75569/150000: episode: 726, duration: 0.771s, episode steps:  96, steps per second: 125, episode reward: -58.902, mean reward: -0.614 [-100.000,  9.787], mean action: 1.760 [0.000, 3.000],  loss: 9.287452, mae: 39.102688, mean_q: 35.995361, mean_eps: 0.546877
  75665/150000: episode: 727, duration: 0.973s, episode steps:  96, steps per second:  99, episode reward: -26.479, mean reward: -0.276 [-100.000,  8.437], mean action: 1.698 [0.000, 3.000],  loss: 9.143179, mae: 38.505404, mean_q: 34.689440, mean_eps: 0.546301
  75781/150000: episode: 728, duration: 1.641s, episode steps: 116, steps per second:  71, episode reward: -52.432, mean reward: -0.452 [-100.000, 16.403], mean action: 1.526 [0.000, 3.000],  loss: 7.781633, mae: 38.967373, mean_q: 35.735685, mean_eps: 0.545665
  75860/150000: episode: 729, duration: 1.132s, episode steps:  79, steps per second:  70, episode reward: -40.540, mean reward: -0.513 [-100.000, 19.514], mean action: 1.696 [0.000, 3.000],  loss: 5.877134, mae: 39.238386, mean_q: 36.186473, mean_eps: 0.545080
  75987/150000: episode: 730, duration: 1.184s, episode steps: 127, steps per second: 107, episode reward: -14.210, mean reward: -0.112 [-100.000, 17.797], mean action: 1.646 [0.000, 3.000],  loss: 7.700392, mae: 38.359911, mean_q: 35.379117, mean_eps: 0.544462
  76082/150000: episode: 731, duration: 0.905s, episode steps:  95, steps per second: 105, episode reward: -56.847, mean reward: -0.598 [-100.000,  8.677], mean action: 1.589 [0.000, 3.000],  loss: 10.067276, mae: 39.506857, mean_q: 37.775243, mean_eps: 0.543796
  76211/150000: episode: 732, duration: 1.106s, episode steps: 129, steps per second: 117, episode reward: -167.754, mean reward: -1.300 [-100.000,  3.862], mean action: 1.419 [0.000, 3.000],  loss: 9.039129, mae: 39.474887, mean_q: 38.197843, mean_eps: 0.543124
  76317/150000: episode: 733, duration: 0.798s, episode steps: 106, steps per second: 133, episode reward: -22.795, mean reward: -0.215 [-100.000, 11.701], mean action: 1.783 [0.000, 3.000],  loss: 8.809164, mae: 38.891992, mean_q: 38.118307, mean_eps: 0.542419
  76397/150000: episode: 734, duration: 0.546s, episode steps:  80, steps per second: 147, episode reward: -55.639, mean reward: -0.695 [-100.000,  8.507], mean action: 1.712 [0.000, 3.000],  loss: 6.757092, mae: 38.991876, mean_q: 37.266435, mean_eps: 0.541861
  76477/150000: episode: 735, duration: 0.542s, episode steps:  80, steps per second: 148, episode reward: -69.252, mean reward: -0.866 [-100.000,  9.440], mean action: 1.800 [0.000, 3.000],  loss: 17.913080, mae: 39.908138, mean_q: 38.267659, mean_eps: 0.541381
  76566/150000: episode: 736, duration: 0.574s, episode steps:  89, steps per second: 155, episode reward: -25.459, mean reward: -0.286 [-100.000, 10.961], mean action: 1.730 [0.000, 3.000],  loss: 13.340684, mae: 39.982444, mean_q: 37.487410, mean_eps: 0.540874
  76679/150000: episode: 737, duration: 0.871s, episode steps: 113, steps per second: 130, episode reward: -11.789, mean reward: -0.104 [-100.000, 17.801], mean action: 1.407 [0.000, 3.000],  loss: 13.854251, mae: 39.539337, mean_q: 38.215146, mean_eps: 0.540268
  76772/150000: episode: 738, duration: 0.678s, episode steps:  93, steps per second: 137, episode reward: 40.679, mean reward:  0.437 [-100.000, 17.989], mean action: 1.753 [0.000, 3.000],  loss: 14.100569, mae: 39.357339, mean_q: 37.213180, mean_eps: 0.539650
  76887/150000: episode: 739, duration: 0.765s, episode steps: 115, steps per second: 150, episode reward: -82.931, mean reward: -0.721 [-100.000,  8.669], mean action: 1.539 [0.000, 3.000],  loss: 11.765027, mae: 39.202429, mean_q: 37.460291, mean_eps: 0.539026
  76976/150000: episode: 740, duration: 0.614s, episode steps:  89, steps per second: 145, episode reward: -20.080, mean reward: -0.226 [-100.000,  7.823], mean action: 1.876 [0.000, 3.000],  loss: 10.951633, mae: 38.849458, mean_q: 38.282338, mean_eps: 0.538414
  77102/150000: episode: 741, duration: 0.863s, episode steps: 126, steps per second: 146, episode reward: -17.888, mean reward: -0.142 [-100.000, 11.985], mean action: 1.786 [0.000, 3.000],  loss: 10.204633, mae: 39.723314, mean_q: 38.897682, mean_eps: 0.537769
  77241/150000: episode: 742, duration: 0.924s, episode steps: 139, steps per second: 150, episode reward: -14.906, mean reward: -0.107 [-100.000, 16.370], mean action: 1.755 [0.000, 3.000],  loss: 7.810309, mae: 40.680537, mean_q: 38.975240, mean_eps: 0.536974
  77357/150000: episode: 743, duration: 0.752s, episode steps: 116, steps per second: 154, episode reward: -62.009, mean reward: -0.535 [-100.000,  9.719], mean action: 1.483 [0.000, 3.000],  loss: 8.071637, mae: 39.818079, mean_q: 37.805196, mean_eps: 0.536209
  77463/150000: episode: 744, duration: 0.668s, episode steps: 106, steps per second: 159, episode reward: -6.740, mean reward: -0.064 [-100.000, 13.058], mean action: 1.538 [0.000, 3.000],  loss: 9.000198, mae: 40.291658, mean_q: 38.416825, mean_eps: 0.535543
  78114/150000: episode: 745, duration: 5.479s, episode steps: 651, steps per second: 119, episode reward: -332.116, mean reward: -0.510 [-100.000, 19.167], mean action: 1.653 [0.000, 3.000],  loss: 9.930004, mae: 40.229806, mean_q: 38.567690, mean_eps: 0.533272
  78208/150000: episode: 746, duration: 0.848s, episode steps:  94, steps per second: 111, episode reward: -46.537, mean reward: -0.495 [-100.000, 14.392], mean action: 1.649 [0.000, 3.000],  loss: 10.978531, mae: 40.571672, mean_q: 39.059355, mean_eps: 0.531037
  78280/150000: episode: 747, duration: 0.640s, episode steps:  72, steps per second: 112, episode reward: -40.581, mean reward: -0.564 [-100.000,  8.485], mean action: 1.569 [0.000, 3.000],  loss: 7.636047, mae: 40.109450, mean_q: 38.764961, mean_eps: 0.530539
  78389/150000: episode: 748, duration: 0.774s, episode steps: 109, steps per second: 141, episode reward: -57.923, mean reward: -0.531 [-100.000,  9.712], mean action: 1.385 [0.000, 3.000],  loss: 8.326890, mae: 40.391852, mean_q: 38.820334, mean_eps: 0.529996
  78471/150000: episode: 749, duration: 0.550s, episode steps:  82, steps per second: 149, episode reward: -54.547, mean reward: -0.665 [-100.000, 12.603], mean action: 1.463 [0.000, 3.000],  loss: 7.904474, mae: 40.560240, mean_q: 40.090606, mean_eps: 0.529423
  78601/150000: episode: 750, duration: 0.853s, episode steps: 130, steps per second: 152, episode reward: -21.597, mean reward: -0.166 [-100.000, 11.477], mean action: 1.708 [0.000, 3.000],  loss: 9.937088, mae: 40.693100, mean_q: 38.798810, mean_eps: 0.528787
  78720/150000: episode: 751, duration: 0.864s, episode steps: 119, steps per second: 138, episode reward: -58.645, mean reward: -0.493 [-100.000,  9.292], mean action: 1.664 [0.000, 3.000],  loss: 6.858816, mae: 40.476868, mean_q: 39.934161, mean_eps: 0.528040
  78812/150000: episode: 752, duration: 0.632s, episode steps:  92, steps per second: 145, episode reward: -25.620, mean reward: -0.278 [-100.000,  6.906], mean action: 1.717 [0.000, 3.000],  loss: 9.063099, mae: 40.835762, mean_q: 40.404144, mean_eps: 0.527407
  78881/150000: episode: 753, duration: 0.456s, episode steps:  69, steps per second: 151, episode reward: -53.222, mean reward: -0.771 [-100.000,  8.990], mean action: 1.681 [0.000, 3.000],  loss: 7.764887, mae: 40.914381, mean_q: 37.676319, mean_eps: 0.526924
  78996/150000: episode: 754, duration: 0.822s, episode steps: 115, steps per second: 140, episode reward: -65.107, mean reward: -0.566 [-100.000, 10.785], mean action: 1.600 [0.000, 3.000],  loss: 9.221731, mae: 40.572400, mean_q: 38.531051, mean_eps: 0.526372
  79102/150000: episode: 755, duration: 0.735s, episode steps: 106, steps per second: 144, episode reward: -77.656, mean reward: -0.733 [-100.000,  9.456], mean action: 1.434 [0.000, 3.000],  loss: 8.235364, mae: 40.534537, mean_q: 38.889165, mean_eps: 0.525709
  79593/150000: episode: 756, duration: 3.663s, episode steps: 491, steps per second: 134, episode reward: -129.492, mean reward: -0.264 [-100.000, 15.562], mean action: 1.684 [0.000, 3.000],  loss: 9.564169, mae: 40.719354, mean_q: 40.308354, mean_eps: 0.523918
  79754/150000: episode: 757, duration: 1.073s, episode steps: 161, steps per second: 150, episode reward: -6.849, mean reward: -0.043 [-100.000, 10.564], mean action: 1.621 [0.000, 3.000],  loss: 7.018942, mae: 40.215783, mean_q: 39.464335, mean_eps: 0.521962
  79864/150000: episode: 758, duration: 0.771s, episode steps: 110, steps per second: 143, episode reward: -41.459, mean reward: -0.377 [-100.000, 12.196], mean action: 1.500 [0.000, 3.000],  loss: 10.983402, mae: 40.471840, mean_q: 39.765571, mean_eps: 0.521149
  79996/150000: episode: 759, duration: 0.910s, episode steps: 132, steps per second: 145, episode reward:  3.606, mean reward:  0.027 [-100.000, 12.384], mean action: 1.750 [0.000, 3.000],  loss: 9.564708, mae: 41.277614, mean_q: 39.711773, mean_eps: 0.520423
  80074/150000: episode: 760, duration: 0.516s, episode steps:  78, steps per second: 151, episode reward: -27.878, mean reward: -0.357 [-100.000,  8.666], mean action: 1.667 [0.000, 3.000],  loss: 5.557456, mae: 40.650711, mean_q: 39.893247, mean_eps: 0.519793
  80161/150000: episode: 761, duration: 0.561s, episode steps:  87, steps per second: 155, episode reward: -29.574, mean reward: -0.340 [-100.000, 12.985], mean action: 1.437 [0.000, 3.000],  loss: 9.122102, mae: 40.554248, mean_q: 39.869619, mean_eps: 0.519298
  80288/150000: episode: 762, duration: 0.829s, episode steps: 127, steps per second: 153, episode reward: -74.183, mean reward: -0.584 [-100.000,  6.262], mean action: 1.520 [0.000, 3.000],  loss: 11.543654, mae: 41.042796, mean_q: 38.810142, mean_eps: 0.518656
  80405/150000: episode: 763, duration: 0.727s, episode steps: 117, steps per second: 161, episode reward: -80.235, mean reward: -0.686 [-100.000, 11.798], mean action: 1.231 [0.000, 3.000],  loss: 7.557381, mae: 40.938967, mean_q: 40.181873, mean_eps: 0.517924
  80534/150000: episode: 764, duration: 0.878s, episode steps: 129, steps per second: 147, episode reward: -100.496, mean reward: -0.779 [-100.000,  3.576], mean action: 1.667 [0.000, 3.000],  loss: 7.888405, mae: 40.741617, mean_q: 39.102782, mean_eps: 0.517186
  80623/150000: episode: 765, duration: 0.620s, episode steps:  89, steps per second: 144, episode reward: 13.908, mean reward:  0.156 [-100.000, 16.769], mean action: 1.607 [0.000, 3.000],  loss: 6.733685, mae: 40.599186, mean_q: 39.276116, mean_eps: 0.516532
  80735/150000: episode: 766, duration: 0.701s, episode steps: 112, steps per second: 160, episode reward: -11.357, mean reward: -0.101 [-100.000, 16.411], mean action: 1.491 [0.000, 3.000],  loss: 5.948343, mae: 41.098470, mean_q: 41.146793, mean_eps: 0.515929
  80832/150000: episode: 767, duration: 0.615s, episode steps:  97, steps per second: 158, episode reward: -34.307, mean reward: -0.354 [-100.000, 11.840], mean action: 1.722 [0.000, 3.000],  loss: 8.712351, mae: 40.342800, mean_q: 39.785514, mean_eps: 0.515302
  80927/150000: episode: 768, duration: 0.626s, episode steps:  95, steps per second: 152, episode reward: -34.204, mean reward: -0.360 [-100.000, 18.368], mean action: 1.379 [0.000, 3.000],  loss: 11.139955, mae: 41.000564, mean_q: 39.562919, mean_eps: 0.514726
  81032/150000: episode: 769, duration: 0.673s, episode steps: 105, steps per second: 156, episode reward: -65.353, mean reward: -0.622 [-100.000,  9.458], mean action: 1.790 [0.000, 3.000],  loss: 11.892963, mae: 41.004385, mean_q: 39.899135, mean_eps: 0.514126
  81164/150000: episode: 770, duration: 0.815s, episode steps: 132, steps per second: 162, episode reward: 27.485, mean reward:  0.208 [-100.000, 17.292], mean action: 1.712 [0.000, 3.000],  loss: 11.331251, mae: 40.972908, mean_q: 39.536780, mean_eps: 0.513415
  81269/150000: episode: 771, duration: 0.744s, episode steps: 105, steps per second: 141, episode reward: -42.124, mean reward: -0.401 [-100.000,  8.504], mean action: 1.571 [0.000, 3.000],  loss: 7.093817, mae: 41.735488, mean_q: 42.020686, mean_eps: 0.512704
  81373/150000: episode: 772, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -86.035, mean reward: -0.827 [-100.000, 11.250], mean action: 1.827 [0.000, 3.000],  loss: 7.230668, mae: 41.090091, mean_q: 40.263493, mean_eps: 0.512077
  81486/150000: episode: 773, duration: 0.801s, episode steps: 113, steps per second: 141, episode reward: -104.263, mean reward: -0.923 [-100.000, 13.152], mean action: 1.690 [0.000, 3.000],  loss: 8.353188, mae: 41.122758, mean_q: 41.034463, mean_eps: 0.511426
  81573/150000: episode: 774, duration: 0.614s, episode steps:  87, steps per second: 142, episode reward: -26.540, mean reward: -0.305 [-100.000, 11.008], mean action: 1.437 [0.000, 3.000],  loss: 16.692768, mae: 41.189621, mean_q: 40.021463, mean_eps: 0.510826
  81712/150000: episode: 775, duration: 0.981s, episode steps: 139, steps per second: 142, episode reward: -45.557, mean reward: -0.328 [-100.000, 10.897], mean action: 1.590 [0.000, 3.000],  loss: 11.374244, mae: 41.022018, mean_q: 38.970667, mean_eps: 0.510148
  81791/150000: episode: 776, duration: 0.589s, episode steps:  79, steps per second: 134, episode reward: -7.177, mean reward: -0.091 [-100.000,  7.441], mean action: 1.734 [0.000, 3.000],  loss: 13.341571, mae: 41.421271, mean_q: 39.724815, mean_eps: 0.509494
  81882/150000: episode: 777, duration: 0.899s, episode steps:  91, steps per second: 101, episode reward: -80.909, mean reward: -0.889 [-100.000, 22.621], mean action: 1.582 [0.000, 3.000],  loss: 4.743730, mae: 40.450570, mean_q: 40.315539, mean_eps: 0.508984
  81980/150000: episode: 778, duration: 1.115s, episode steps:  98, steps per second:  88, episode reward: -86.786, mean reward: -0.886 [-100.000, 15.165], mean action: 1.643 [0.000, 3.000],  loss: 8.991819, mae: 41.027293, mean_q: 40.587790, mean_eps: 0.508417
  82088/150000: episode: 779, duration: 0.804s, episode steps: 108, steps per second: 134, episode reward: -52.114, mean reward: -0.483 [-100.000,  9.538], mean action: 1.444 [0.000, 3.000],  loss: 7.989747, mae: 41.719061, mean_q: 41.255190, mean_eps: 0.507799
  82197/150000: episode: 780, duration: 0.671s, episode steps: 109, steps per second: 163, episode reward: -24.697, mean reward: -0.227 [-100.000, 25.596], mean action: 1.495 [0.000, 3.000],  loss: 10.655818, mae: 41.265947, mean_q: 40.515535, mean_eps: 0.507148
  82301/150000: episode: 781, duration: 0.617s, episode steps: 104, steps per second: 168, episode reward: -69.068, mean reward: -0.664 [-100.000,  5.551], mean action: 1.529 [0.000, 3.000],  loss: 12.213133, mae: 40.678227, mean_q: 39.289798, mean_eps: 0.506509
  82406/150000: episode: 782, duration: 0.678s, episode steps: 105, steps per second: 155, episode reward: -79.607, mean reward: -0.758 [-100.000, 16.718], mean action: 1.571 [0.000, 3.000],  loss: 8.641531, mae: 40.990277, mean_q: 41.305808, mean_eps: 0.505882
  82529/150000: episode: 783, duration: 0.768s, episode steps: 123, steps per second: 160, episode reward:  6.483, mean reward:  0.053 [-100.000, 15.634], mean action: 1.667 [0.000, 3.000],  loss: 9.884085, mae: 41.324595, mean_q: 41.675925, mean_eps: 0.505198
  82646/150000: episode: 784, duration: 0.708s, episode steps: 117, steps per second: 165, episode reward: -207.575, mean reward: -1.774 [-100.000, 59.282], mean action: 1.709 [0.000, 3.000],  loss: 9.514997, mae: 40.763397, mean_q: 41.143635, mean_eps: 0.504478
  82735/150000: episode: 785, duration: 0.572s, episode steps:  89, steps per second: 156, episode reward: -61.661, mean reward: -0.693 [-100.000,  8.787], mean action: 1.596 [0.000, 3.000],  loss: 8.136115, mae: 40.807213, mean_q: 41.242273, mean_eps: 0.503860
  82853/150000: episode: 786, duration: 0.721s, episode steps: 118, steps per second: 164, episode reward: -39.223, mean reward: -0.332 [-100.000, 20.884], mean action: 1.534 [0.000, 3.000],  loss: 7.348092, mae: 41.324532, mean_q: 41.127450, mean_eps: 0.503239
  82945/150000: episode: 787, duration: 0.542s, episode steps:  92, steps per second: 170, episode reward: -60.203, mean reward: -0.654 [-100.000, 21.994], mean action: 1.359 [0.000, 3.000],  loss: 6.861991, mae: 41.058755, mean_q: 39.962555, mean_eps: 0.502609
  83093/150000: episode: 788, duration: 0.939s, episode steps: 148, steps per second: 158, episode reward: 29.507, mean reward:  0.199 [-100.000, 11.090], mean action: 1.669 [0.000, 3.000],  loss: 8.186181, mae: 41.052990, mean_q: 41.338255, mean_eps: 0.501889
  83187/150000: episode: 789, duration: 0.583s, episode steps:  94, steps per second: 161, episode reward: 35.452, mean reward:  0.377 [-100.000, 18.183], mean action: 1.681 [0.000, 3.000],  loss: 8.780860, mae: 42.183437, mean_q: 40.634028, mean_eps: 0.501163
  83315/150000: episode: 790, duration: 0.778s, episode steps: 128, steps per second: 165, episode reward: 22.045, mean reward:  0.172 [-100.000, 16.193], mean action: 1.781 [0.000, 3.000],  loss: 7.296863, mae: 41.074929, mean_q: 41.863455, mean_eps: 0.500497
  83413/150000: episode: 791, duration: 0.635s, episode steps:  98, steps per second: 154, episode reward: -19.930, mean reward: -0.203 [-100.000, 23.623], mean action: 1.837 [0.000, 3.000],  loss: 10.800264, mae: 41.888708, mean_q: 41.828174, mean_eps: 0.499819
  83615/150000: episode: 792, duration: 1.324s, episode steps: 202, steps per second: 153, episode reward: -139.842, mean reward: -0.692 [-100.000, 14.018], mean action: 1.767 [0.000, 3.000],  loss: 11.167231, mae: 41.813599, mean_q: 42.054331, mean_eps: 0.498919
  83752/150000: episode: 793, duration: 1.022s, episode steps: 137, steps per second: 134, episode reward: 14.669, mean reward:  0.107 [-100.000, 14.865], mean action: 1.540 [0.000, 3.000],  loss: 6.366407, mae: 41.642828, mean_q: 40.699302, mean_eps: 0.497902
  83872/150000: episode: 794, duration: 0.737s, episode steps: 120, steps per second: 163, episode reward: -54.736, mean reward: -0.456 [-100.000,  7.632], mean action: 1.650 [0.000, 3.000],  loss: 9.255774, mae: 40.966205, mean_q: 41.597686, mean_eps: 0.497131
  83970/150000: episode: 795, duration: 0.576s, episode steps:  98, steps per second: 170, episode reward: -11.579, mean reward: -0.118 [-100.000, 13.607], mean action: 1.694 [0.000, 3.000],  loss: 9.917009, mae: 41.545384, mean_q: 40.605725, mean_eps: 0.496477
  84069/150000: episode: 796, duration: 0.627s, episode steps:  99, steps per second: 158, episode reward: 41.507, mean reward:  0.419 [-100.000, 19.933], mean action: 1.768 [0.000, 3.000],  loss: 11.452025, mae: 41.716072, mean_q: 41.468011, mean_eps: 0.495886
  84174/150000: episode: 797, duration: 0.684s, episode steps: 105, steps per second: 154, episode reward: 45.095, mean reward:  0.429 [-100.000, 16.256], mean action: 1.752 [0.000, 3.000],  loss: 8.187139, mae: 41.671050, mean_q: 41.686493, mean_eps: 0.495274
  84257/150000: episode: 798, duration: 0.497s, episode steps:  83, steps per second: 167, episode reward: -23.674, mean reward: -0.285 [-100.000, 12.640], mean action: 1.614 [0.000, 3.000],  loss: 10.861084, mae: 41.669584, mean_q: 41.534253, mean_eps: 0.494710
  84340/150000: episode: 799, duration: 0.524s, episode steps:  83, steps per second: 158, episode reward: -59.717, mean reward: -0.719 [-100.000,  9.624], mean action: 1.639 [0.000, 3.000],  loss: 11.127498, mae: 41.775110, mean_q: 40.192424, mean_eps: 0.494212
  84441/150000: episode: 800, duration: 0.667s, episode steps: 101, steps per second: 152, episode reward: -33.304, mean reward: -0.330 [-100.000, 11.878], mean action: 1.673 [0.000, 3.000],  loss: 8.010002, mae: 41.946870, mean_q: 40.821868, mean_eps: 0.493660
  84536/150000: episode: 801, duration: 0.676s, episode steps:  95, steps per second: 140, episode reward: -82.845, mean reward: -0.872 [-100.000, 13.144], mean action: 1.579 [0.000, 3.000],  loss: 8.525712, mae: 42.046391, mean_q: 42.010951, mean_eps: 0.493072
  84602/150000: episode: 802, duration: 0.408s, episode steps:  66, steps per second: 162, episode reward: -40.629, mean reward: -0.616 [-100.000, 16.498], mean action: 1.606 [0.000, 3.000],  loss: 11.647882, mae: 42.454807, mean_q: 41.766761, mean_eps: 0.492589
  84703/150000: episode: 803, duration: 0.596s, episode steps: 101, steps per second: 169, episode reward: -11.426, mean reward: -0.113 [-100.000, 16.201], mean action: 1.673 [0.000, 3.000],  loss: 9.780987, mae: 41.584762, mean_q: 39.504247, mean_eps: 0.492088
  84819/150000: episode: 804, duration: 0.722s, episode steps: 116, steps per second: 161, episode reward: -39.905, mean reward: -0.344 [-100.000, 16.966], mean action: 1.741 [0.000, 3.000],  loss: 14.241297, mae: 41.947009, mean_q: 43.027287, mean_eps: 0.491437
  85017/150000: episode: 805, duration: 1.213s, episode steps: 198, steps per second: 163, episode reward: -47.678, mean reward: -0.241 [-100.000, 11.906], mean action: 1.581 [0.000, 3.000],  loss: 8.804133, mae: 41.646757, mean_q: 41.345293, mean_eps: 0.490495
  85486/150000: episode: 806, duration: 3.729s, episode steps: 469, steps per second: 126, episode reward: -194.968, mean reward: -0.416 [-100.000, 18.659], mean action: 1.499 [0.000, 3.000],  loss: 10.099293, mae: 41.702051, mean_q: 41.521270, mean_eps: 0.488494
  85575/150000: episode: 807, duration: 0.561s, episode steps:  89, steps per second: 159, episode reward: -16.253, mean reward: -0.183 [-100.000, 10.857], mean action: 1.809 [0.000, 3.000],  loss: 8.686027, mae: 41.739846, mean_q: 41.616563, mean_eps: 0.486820
  85675/150000: episode: 808, duration: 0.680s, episode steps: 100, steps per second: 147, episode reward: -66.393, mean reward: -0.664 [-100.000, 17.412], mean action: 1.610 [0.000, 3.000],  loss: 10.817304, mae: 42.049294, mean_q: 42.259780, mean_eps: 0.486253
  85780/150000: episode: 809, duration: 0.775s, episode steps: 105, steps per second: 135, episode reward: -85.521, mean reward: -0.814 [-100.000,  6.523], mean action: 1.581 [0.000, 3.000],  loss: 7.598658, mae: 41.219667, mean_q: 41.441157, mean_eps: 0.485638
  85901/150000: episode: 810, duration: 0.776s, episode steps: 121, steps per second: 156, episode reward: -205.299, mean reward: -1.697 [-100.000, 44.184], mean action: 1.769 [0.000, 3.000],  loss: 10.808716, mae: 41.784239, mean_q: 40.536819, mean_eps: 0.484960
  86002/150000: episode: 811, duration: 0.657s, episode steps: 101, steps per second: 154, episode reward: 14.042, mean reward:  0.139 [-100.000, 17.191], mean action: 1.752 [0.000, 3.000],  loss: 10.557838, mae: 40.966095, mean_q: 40.747500, mean_eps: 0.484294
  87002/150000: episode: 812, duration: 7.169s, episode steps: 1000, steps per second: 139, episode reward: 12.921, mean reward:  0.013 [-22.870, 27.311], mean action: 1.514 [0.000, 3.000],  loss: 9.630563, mae: 41.558601, mean_q: 41.330440, mean_eps: 0.480991
  87084/150000: episode: 813, duration: 0.541s, episode steps:  82, steps per second: 152, episode reward:  2.797, mean reward:  0.034 [-100.000,  9.732], mean action: 1.939 [0.000, 3.000],  loss: 6.264189, mae: 39.860763, mean_q: 39.527523, mean_eps: 0.477745
  87206/150000: episode: 814, duration: 0.950s, episode steps: 122, steps per second: 128, episode reward:  8.188, mean reward:  0.067 [-100.000, 16.867], mean action: 1.689 [0.000, 3.000],  loss: 13.364408, mae: 40.130800, mean_q: 38.195282, mean_eps: 0.477133
  87285/150000: episode: 815, duration: 0.541s, episode steps:  79, steps per second: 146, episode reward:  2.932, mean reward:  0.037 [-100.000, 14.356], mean action: 1.747 [0.000, 3.000],  loss: 9.996588, mae: 40.580025, mean_q: 40.537370, mean_eps: 0.476530
  87385/150000: episode: 816, duration: 0.615s, episode steps: 100, steps per second: 163, episode reward:  3.932, mean reward:  0.039 [-100.000, 14.783], mean action: 1.840 [0.000, 3.000],  loss: 9.562599, mae: 41.192183, mean_q: 40.901649, mean_eps: 0.475993
  87511/150000: episode: 817, duration: 0.856s, episode steps: 126, steps per second: 147, episode reward: -8.172, mean reward: -0.065 [-100.000, 17.080], mean action: 1.460 [0.000, 3.000],  loss: 7.372642, mae: 40.730299, mean_q: 38.974031, mean_eps: 0.475315
  87607/150000: episode: 818, duration: 0.584s, episode steps:  96, steps per second: 165, episode reward: -1.403, mean reward: -0.015 [-100.000, 19.834], mean action: 1.615 [0.000, 3.000],  loss: 8.164882, mae: 41.216261, mean_q: 40.888441, mean_eps: 0.474649
  87765/150000: episode: 819, duration: 0.922s, episode steps: 158, steps per second: 171, episode reward: 50.339, mean reward:  0.319 [-100.000, 18.406], mean action: 1.620 [0.000, 3.000],  loss: 9.120467, mae: 41.011368, mean_q: 40.268518, mean_eps: 0.473887
  87895/150000: episode: 820, duration: 0.794s, episode steps: 130, steps per second: 164, episode reward: -63.009, mean reward: -0.485 [-100.000,  8.524], mean action: 1.662 [0.000, 3.000],  loss: 13.466878, mae: 41.427075, mean_q: 41.612745, mean_eps: 0.473023
  87980/150000: episode: 821, duration: 0.505s, episode steps:  85, steps per second: 168, episode reward: -14.032, mean reward: -0.165 [-100.000, 14.340], mean action: 1.565 [0.000, 3.000],  loss: 10.905673, mae: 40.766840, mean_q: 40.045660, mean_eps: 0.472378
  88100/150000: episode: 822, duration: 0.732s, episode steps: 120, steps per second: 164, episode reward: -59.130, mean reward: -0.493 [-100.000, 10.786], mean action: 1.642 [0.000, 3.000],  loss: 9.648149, mae: 41.157300, mean_q: 41.225312, mean_eps: 0.471763
  88201/150000: episode: 823, duration: 0.611s, episode steps: 101, steps per second: 165, episode reward: -27.350, mean reward: -0.271 [-100.000, 12.685], mean action: 1.733 [0.000, 3.000],  loss: 8.912916, mae: 41.739234, mean_q: 40.960228, mean_eps: 0.471100
  88317/150000: episode: 824, duration: 0.744s, episode steps: 116, steps per second: 156, episode reward: -211.248, mean reward: -1.821 [-100.000, 42.325], mean action: 1.414 [0.000, 3.000],  loss: 9.296523, mae: 41.804153, mean_q: 42.901658, mean_eps: 0.470449
  88441/150000: episode: 825, duration: 0.752s, episode steps: 124, steps per second: 165, episode reward: 12.258, mean reward:  0.099 [-100.000, 13.587], mean action: 1.774 [0.000, 3.000],  loss: 10.005047, mae: 41.906554, mean_q: 41.964649, mean_eps: 0.469729
  88547/150000: episode: 826, duration: 0.645s, episode steps: 106, steps per second: 164, episode reward:  8.642, mean reward:  0.082 [-100.000, 18.223], mean action: 1.783 [0.000, 3.000],  loss: 8.916838, mae: 40.960498, mean_q: 40.402203, mean_eps: 0.469039
  89547/150000: episode: 827, duration: 7.456s, episode steps: 1000, steps per second: 134, episode reward: 13.625, mean reward:  0.014 [-20.460, 22.563], mean action: 1.606 [0.000, 3.000],  loss: 8.985007, mae: 41.500934, mean_q: 41.568422, mean_eps: 0.465721
  89692/150000: episode: 828, duration: 0.843s, episode steps: 145, steps per second: 172, episode reward: -76.744, mean reward: -0.529 [-100.000,  7.073], mean action: 1.400 [0.000, 3.000],  loss: 9.837959, mae: 41.832759, mean_q: 40.532241, mean_eps: 0.462286
  90692/150000: episode: 829, duration: 7.174s, episode steps: 1000, steps per second: 139, episode reward: -5.384, mean reward: -0.005 [-21.876, 22.936], mean action: 1.755 [0.000, 3.000],  loss: 9.634330, mae: 41.568171, mean_q: 41.878746, mean_eps: 0.458851
  90803/150000: episode: 830, duration: 0.646s, episode steps: 111, steps per second: 172, episode reward: 33.537, mean reward:  0.302 [-100.000, 14.471], mean action: 1.730 [0.000, 3.000],  loss: 7.838912, mae: 41.478142, mean_q: 42.899973, mean_eps: 0.455518
  90889/150000: episode: 831, duration: 0.518s, episode steps:  86, steps per second: 166, episode reward: -54.982, mean reward: -0.639 [-100.000, 12.750], mean action: 1.360 [0.000, 3.000],  loss: 7.779404, mae: 41.566633, mean_q: 43.736768, mean_eps: 0.454927
  91103/150000: episode: 832, duration: 1.414s, episode steps: 214, steps per second: 151, episode reward: 52.269, mean reward:  0.244 [-100.000, 16.253], mean action: 1.734 [0.000, 3.000],  loss: 10.284171, mae: 41.591585, mean_q: 43.214357, mean_eps: 0.454027
  92103/150000: episode: 833, duration: 6.645s, episode steps: 1000, steps per second: 150, episode reward: 12.542, mean reward:  0.013 [-24.552, 28.158], mean action: 2.099 [0.000, 3.000],  loss: 10.568759, mae: 41.378086, mean_q: 42.107797, mean_eps: 0.450385
  92243/150000: episode: 834, duration: 0.818s, episode steps: 140, steps per second: 171, episode reward: -60.468, mean reward: -0.432 [-100.000, 10.093], mean action: 1.729 [0.000, 3.000],  loss: 10.267938, mae: 41.016602, mean_q: 43.336598, mean_eps: 0.446965
  92336/150000: episode: 835, duration: 0.566s, episode steps:  93, steps per second: 164, episode reward: -32.508, mean reward: -0.350 [-100.000, 23.934], mean action: 1.602 [0.000, 3.000],  loss: 14.136314, mae: 41.011997, mean_q: 42.155594, mean_eps: 0.446266
  92466/150000: episode: 836, duration: 0.768s, episode steps: 130, steps per second: 169, episode reward: -57.006, mean reward: -0.439 [-100.000,  7.978], mean action: 1.815 [0.000, 3.000],  loss: 10.233894, mae: 41.491017, mean_q: 43.243581, mean_eps: 0.445597
  92602/150000: episode: 837, duration: 0.804s, episode steps: 136, steps per second: 169, episode reward: -83.911, mean reward: -0.617 [-100.000, 13.717], mean action: 1.618 [0.000, 3.000],  loss: 13.173568, mae: 41.575233, mean_q: 42.242333, mean_eps: 0.444799
  92693/150000: episode: 838, duration: 0.547s, episode steps:  91, steps per second: 166, episode reward: 12.796, mean reward:  0.141 [-100.000, 11.629], mean action: 1.549 [0.000, 3.000],  loss: 9.154884, mae: 42.002649, mean_q: 43.185266, mean_eps: 0.444118
  92815/150000: episode: 839, duration: 0.729s, episode steps: 122, steps per second: 167, episode reward:  5.590, mean reward:  0.046 [-100.000, 17.170], mean action: 1.943 [0.000, 3.000],  loss: 11.423968, mae: 41.466719, mean_q: 43.938396, mean_eps: 0.443479
  93815/150000: episode: 840, duration: 7.183s, episode steps: 1000, steps per second: 139, episode reward: 35.702, mean reward:  0.036 [-21.828, 25.723], mean action: 1.232 [0.000, 3.000],  loss: 11.150654, mae: 41.212100, mean_q: 43.167948, mean_eps: 0.440113
  93919/150000: episode: 841, duration: 0.620s, episode steps: 104, steps per second: 168, episode reward: -16.778, mean reward: -0.161 [-100.000, 10.198], mean action: 1.779 [0.000, 3.000],  loss: 17.488493, mae: 41.051917, mean_q: 42.244613, mean_eps: 0.436801
  94024/150000: episode: 842, duration: 0.627s, episode steps: 105, steps per second: 168, episode reward: -43.102, mean reward: -0.410 [-100.000, 15.278], mean action: 1.657 [0.000, 3.000],  loss: 11.055911, mae: 41.596294, mean_q: 44.462252, mean_eps: 0.436174
  94122/150000: episode: 843, duration: 0.579s, episode steps:  98, steps per second: 169, episode reward: -63.800, mean reward: -0.651 [-100.000,  9.385], mean action: 1.633 [0.000, 3.000],  loss: 8.254915, mae: 42.078502, mean_q: 44.927880, mean_eps: 0.435565
  94212/150000: episode: 844, duration: 0.537s, episode steps:  90, steps per second: 168, episode reward: -27.412, mean reward: -0.305 [-100.000, 10.131], mean action: 1.811 [0.000, 3.000],  loss: 11.303795, mae: 41.965649, mean_q: 45.115586, mean_eps: 0.435001
  94345/150000: episode: 845, duration: 0.915s, episode steps: 133, steps per second: 145, episode reward: -54.030, mean reward: -0.406 [-100.000,  9.037], mean action: 1.436 [0.000, 3.000],  loss: 12.455304, mae: 40.877933, mean_q: 42.152856, mean_eps: 0.434332
  94435/150000: episode: 846, duration: 0.665s, episode steps:  90, steps per second: 135, episode reward: -35.572, mean reward: -0.395 [-100.000, 11.277], mean action: 1.544 [0.000, 3.000],  loss: 11.123939, mae: 41.297230, mean_q: 43.468171, mean_eps: 0.433663
  94536/150000: episode: 847, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: -50.561, mean reward: -0.501 [-100.000, 13.215], mean action: 1.465 [0.000, 3.000],  loss: 13.485056, mae: 40.554133, mean_q: 42.296472, mean_eps: 0.433090
  94650/150000: episode: 848, duration: 0.731s, episode steps: 114, steps per second: 156, episode reward: -26.380, mean reward: -0.231 [-100.000, 15.295], mean action: 1.544 [0.000, 3.000],  loss: 13.242503, mae: 41.367777, mean_q: 43.487209, mean_eps: 0.432445
  94758/150000: episode: 849, duration: 0.644s, episode steps: 108, steps per second: 168, episode reward:  6.823, mean reward:  0.063 [-100.000, 15.308], mean action: 1.685 [0.000, 3.000],  loss: 11.920796, mae: 41.239844, mean_q: 43.503379, mean_eps: 0.431779
  94848/150000: episode: 850, duration: 0.786s, episode steps:  90, steps per second: 114, episode reward: -63.887, mean reward: -0.710 [-100.000, 16.367], mean action: 1.478 [0.000, 3.000],  loss: 15.202514, mae: 41.178251, mean_q: 41.808211, mean_eps: 0.431185
  94997/150000: episode: 851, duration: 1.286s, episode steps: 149, steps per second: 116, episode reward: -59.789, mean reward: -0.401 [-100.000, 24.393], mean action: 1.597 [0.000, 3.000],  loss: 10.965787, mae: 41.070750, mean_q: 43.941098, mean_eps: 0.430468
  95997/150000: episode: 852, duration: 7.324s, episode steps: 1000, steps per second: 137, episode reward: 81.684, mean reward:  0.082 [-19.271, 22.834], mean action: 1.117 [0.000, 3.000],  loss: 9.902221, mae: 41.646667, mean_q: 44.250507, mean_eps: 0.427021
  96110/150000: episode: 853, duration: 0.694s, episode steps: 113, steps per second: 163, episode reward: -62.654, mean reward: -0.554 [-100.000,  6.224], mean action: 1.469 [0.000, 3.000],  loss: 11.550129, mae: 42.407630, mean_q: 46.551172, mean_eps: 0.423682
  96218/150000: episode: 854, duration: 0.691s, episode steps: 108, steps per second: 156, episode reward: -64.219, mean reward: -0.595 [-100.000, 10.567], mean action: 1.259 [0.000, 3.000],  loss: 9.711487, mae: 42.167200, mean_q: 44.968154, mean_eps: 0.423019
  96328/150000: episode: 855, duration: 0.809s, episode steps: 110, steps per second: 136, episode reward: -67.261, mean reward: -0.611 [-100.000,  8.945], mean action: 1.473 [0.000, 3.000],  loss: 15.612772, mae: 41.750551, mean_q: 43.990333, mean_eps: 0.422365
  96441/150000: episode: 856, duration: 0.752s, episode steps: 113, steps per second: 150, episode reward: -2.267, mean reward: -0.020 [-100.000, 19.773], mean action: 1.699 [0.000, 3.000],  loss: 11.893372, mae: 41.723498, mean_q: 45.101310, mean_eps: 0.421696
  96554/150000: episode: 857, duration: 0.739s, episode steps: 113, steps per second: 153, episode reward: -38.408, mean reward: -0.340 [-100.000,  9.493], mean action: 1.487 [0.000, 3.000],  loss: 11.272449, mae: 42.857736, mean_q: 46.412700, mean_eps: 0.421018
  96638/150000: episode: 858, duration: 0.744s, episode steps:  84, steps per second: 113, episode reward: -90.370, mean reward: -1.076 [-100.000,  9.249], mean action: 1.679 [0.000, 3.000],  loss: 13.056236, mae: 42.803608, mean_q: 44.873607, mean_eps: 0.420427
  96751/150000: episode: 859, duration: 1.088s, episode steps: 113, steps per second: 104, episode reward: -100.030, mean reward: -0.885 [-100.000,  6.919], mean action: 1.690 [0.000, 3.000],  loss: 11.463371, mae: 42.427308, mean_q: 45.349692, mean_eps: 0.419836
  96869/150000: episode: 860, duration: 0.880s, episode steps: 118, steps per second: 134, episode reward:  8.269, mean reward:  0.070 [-100.000, 13.344], mean action: 1.644 [0.000, 3.000],  loss: 12.721793, mae: 41.552849, mean_q: 43.803374, mean_eps: 0.419143
  97036/150000: episode: 861, duration: 1.309s, episode steps: 167, steps per second: 128, episode reward: -85.581, mean reward: -0.512 [-100.000,  5.515], mean action: 1.491 [0.000, 3.000],  loss: 12.456782, mae: 42.207994, mean_q: 46.080036, mean_eps: 0.418288
  97136/150000: episode: 862, duration: 0.871s, episode steps: 100, steps per second: 115, episode reward: 27.373, mean reward:  0.274 [-100.000, 17.736], mean action: 1.670 [0.000, 3.000],  loss: 8.379541, mae: 41.633794, mean_q: 43.878858, mean_eps: 0.417487
  97231/150000: episode: 863, duration: 0.773s, episode steps:  95, steps per second: 123, episode reward:  5.897, mean reward:  0.062 [-100.000, 17.312], mean action: 1.537 [0.000, 3.000],  loss: 10.538635, mae: 41.924483, mean_q: 45.301346, mean_eps: 0.416902
  97345/150000: episode: 864, duration: 0.969s, episode steps: 114, steps per second: 118, episode reward: -59.628, mean reward: -0.523 [-100.000,  7.542], mean action: 1.623 [0.000, 3.000],  loss: 14.695575, mae: 41.469935, mean_q: 45.136276, mean_eps: 0.416275
  97450/150000: episode: 865, duration: 0.739s, episode steps: 105, steps per second: 142, episode reward:  9.535, mean reward:  0.091 [-100.000, 10.176], mean action: 1.771 [0.000, 3.000],  loss: 15.164427, mae: 41.286548, mean_q: 45.610815, mean_eps: 0.415618
  97540/150000: episode: 866, duration: 0.571s, episode steps:  90, steps per second: 158, episode reward: -29.773, mean reward: -0.331 [-100.000,  7.163], mean action: 1.778 [0.000, 3.000],  loss: 12.339138, mae: 41.931163, mean_q: 45.407145, mean_eps: 0.415033
  98540/150000: episode: 867, duration: 8.002s, episode steps: 1000, steps per second: 125, episode reward: 113.872, mean reward:  0.114 [-22.028, 24.981], mean action: 1.539 [0.000, 3.000],  loss: 10.675414, mae: 41.298477, mean_q: 44.661495, mean_eps: 0.411763
  98618/150000: episode: 868, duration: 0.496s, episode steps:  78, steps per second: 157, episode reward: -92.992, mean reward: -1.192 [-100.000,  6.622], mean action: 1.846 [0.000, 3.000],  loss: 14.333677, mae: 40.981173, mean_q: 44.338101, mean_eps: 0.408529
  98735/150000: episode: 869, duration: 0.730s, episode steps: 117, steps per second: 160, episode reward: -5.358, mean reward: -0.046 [-100.000, 10.325], mean action: 1.684 [0.000, 3.000],  loss: 9.803908, mae: 40.822751, mean_q: 44.014669, mean_eps: 0.407944
  98851/150000: episode: 870, duration: 0.691s, episode steps: 116, steps per second: 168, episode reward: 38.023, mean reward:  0.328 [-100.000, 13.102], mean action: 1.672 [0.000, 3.000],  loss: 12.107463, mae: 41.285129, mean_q: 44.501002, mean_eps: 0.407245
  99114/150000: episode: 871, duration: 1.853s, episode steps: 263, steps per second: 142, episode reward: -77.949, mean reward: -0.296 [-100.000, 21.197], mean action: 1.612 [0.000, 3.000],  loss: 13.731721, mae: 41.434163, mean_q: 44.664459, mean_eps: 0.406108
 100071/150000: episode: 872, duration: 6.889s, episode steps: 957, steps per second: 139, episode reward: -314.317, mean reward: -0.328 [-100.000, 21.527], mean action: 1.471 [0.000, 3.000],  loss: 11.573283, mae: 40.913360, mean_q: 45.254177, mean_eps: 0.402448
 101071/150000: episode: 873, duration: 7.314s, episode steps: 1000, steps per second: 137, episode reward: 77.484, mean reward:  0.077 [-23.775, 22.862], mean action: 1.607 [0.000, 3.000],  loss: 10.598255, mae: 41.505749, mean_q: 45.654114, mean_eps: 0.396577
 101221/150000: episode: 874, duration: 0.916s, episode steps: 150, steps per second: 164, episode reward: -158.385, mean reward: -1.056 [-100.000, 10.266], mean action: 1.507 [0.000, 3.000],  loss: 11.437050, mae: 41.703740, mean_q: 46.057899, mean_eps: 0.393127
 101311/150000: episode: 875, duration: 0.589s, episode steps:  90, steps per second: 153, episode reward: -33.301, mean reward: -0.370 [-100.000,  8.715], mean action: 1.578 [0.000, 3.000],  loss: 11.458242, mae: 41.297669, mean_q: 45.535529, mean_eps: 0.392407
 101450/150000: episode: 876, duration: 0.960s, episode steps: 139, steps per second: 145, episode reward:  2.840, mean reward:  0.020 [-100.000, 21.130], mean action: 1.784 [0.000, 3.000],  loss: 13.052468, mae: 41.880728, mean_q: 46.457261, mean_eps: 0.391720
 101545/150000: episode: 877, duration: 0.613s, episode steps:  95, steps per second: 155, episode reward: -25.881, mean reward: -0.272 [-100.000, 22.796], mean action: 1.537 [0.000, 3.000],  loss: 13.224576, mae: 41.428370, mean_q: 45.217735, mean_eps: 0.391018
 101659/150000: episode: 878, duration: 0.713s, episode steps: 114, steps per second: 160, episode reward: -11.904, mean reward: -0.104 [-100.000, 18.666], mean action: 1.465 [0.000, 3.000],  loss: 13.108527, mae: 41.171135, mean_q: 45.436952, mean_eps: 0.390391
 101785/150000: episode: 879, duration: 0.811s, episode steps: 126, steps per second: 155, episode reward: -51.258, mean reward: -0.407 [-100.000, 36.832], mean action: 1.540 [0.000, 3.000],  loss: 11.629045, mae: 41.052288, mean_q: 45.527515, mean_eps: 0.389671
 101902/150000: episode: 880, duration: 0.710s, episode steps: 117, steps per second: 165, episode reward: 18.419, mean reward:  0.157 [-100.000, 14.734], mean action: 1.769 [0.000, 3.000],  loss: 12.516817, mae: 41.126739, mean_q: 45.398929, mean_eps: 0.388942
 102011/150000: episode: 881, duration: 0.766s, episode steps: 109, steps per second: 142, episode reward: -16.071, mean reward: -0.147 [-100.000, 13.692], mean action: 1.615 [0.000, 3.000],  loss: 14.346901, mae: 41.059557, mean_q: 46.493949, mean_eps: 0.388264
 102130/150000: episode: 882, duration: 0.939s, episode steps: 119, steps per second: 127, episode reward:  0.665, mean reward:  0.006 [-100.000, 17.948], mean action: 1.605 [0.000, 3.000],  loss: 10.934648, mae: 41.280112, mean_q: 46.334165, mean_eps: 0.387580
 102397/150000: episode: 883, duration: 1.836s, episode steps: 267, steps per second: 145, episode reward: -239.127, mean reward: -0.896 [-100.000, 15.969], mean action: 1.745 [0.000, 3.000],  loss: 11.251009, mae: 41.062399, mean_q: 46.090062, mean_eps: 0.386422
 103295/150000: episode: 884, duration: 6.531s, episode steps: 898, steps per second: 138, episode reward: -291.512, mean reward: -0.325 [-100.000, 15.911], mean action: 1.650 [0.000, 3.000],  loss: 12.300266, mae: 41.331281, mean_q: 45.875819, mean_eps: 0.382927
 103413/150000: episode: 885, duration: 0.706s, episode steps: 118, steps per second: 167, episode reward: -6.471, mean reward: -0.055 [-100.000, 12.016], mean action: 1.669 [0.000, 3.000],  loss: 9.812392, mae: 41.648029, mean_q: 47.420177, mean_eps: 0.379879
 103533/150000: episode: 886, duration: 0.771s, episode steps: 120, steps per second: 156, episode reward: -158.210, mean reward: -1.318 [-100.000,  8.394], mean action: 1.658 [0.000, 3.000],  loss: 13.495995, mae: 41.623294, mean_q: 46.736107, mean_eps: 0.379165
 103615/150000: episode: 887, duration: 0.527s, episode steps:  82, steps per second: 156, episode reward: -1.523, mean reward: -0.019 [-100.000,  8.952], mean action: 1.805 [0.000, 3.000],  loss: 9.309374, mae: 42.350807, mean_q: 45.821736, mean_eps: 0.378559
 103749/150000: episode: 888, duration: 0.786s, episode steps: 134, steps per second: 170, episode reward: -53.595, mean reward: -0.400 [-100.000, 11.989], mean action: 1.619 [0.000, 3.000],  loss: 11.478547, mae: 42.379780, mean_q: 47.433597, mean_eps: 0.377911
 103897/150000: episode: 889, duration: 0.886s, episode steps: 148, steps per second: 167, episode reward:  4.846, mean reward:  0.033 [-100.000, 21.341], mean action: 1.601 [0.000, 3.000],  loss: 8.848344, mae: 41.841378, mean_q: 45.819778, mean_eps: 0.377065
 103993/150000: episode: 890, duration: 0.677s, episode steps:  96, steps per second: 142, episode reward: 16.448, mean reward:  0.171 [-100.000, 14.359], mean action: 1.625 [0.000, 3.000],  loss: 9.081769, mae: 42.126560, mean_q: 47.687247, mean_eps: 0.376333
 104993/150000: episode: 891, duration: 7.556s, episode steps: 1000, steps per second: 132, episode reward: 66.246, mean reward:  0.066 [-20.485, 22.273], mean action: 1.449 [0.000, 3.000],  loss: 12.213077, mae: 41.212422, mean_q: 45.919440, mean_eps: 0.373045
 105993/150000: episode: 892, duration: 6.544s, episode steps: 1000, steps per second: 153, episode reward: -162.333, mean reward: -0.162 [-18.510, 19.432], mean action: 1.593 [0.000, 3.000],  loss: 11.323743, mae: 41.254999, mean_q: 47.162935, mean_eps: 0.367045
 106993/150000: episode: 893, duration: 6.940s, episode steps: 1000, steps per second: 144, episode reward: -62.849, mean reward: -0.063 [-12.742, 14.596], mean action: 1.732 [0.000, 3.000],  loss: 11.908331, mae: 40.524027, mean_q: 45.986089, mean_eps: 0.361045
 107993/150000: episode: 894, duration: 6.913s, episode steps: 1000, steps per second: 145, episode reward: 77.058, mean reward:  0.077 [-20.482, 51.390], mean action: 1.861 [0.000, 3.000],  loss: 13.545731, mae: 40.909099, mean_q: 46.348513, mean_eps: 0.355045
 108122/150000: episode: 895, duration: 0.796s, episode steps: 129, steps per second: 162, episode reward: -4.839, mean reward: -0.038 [-100.000, 13.441], mean action: 1.395 [0.000, 3.000],  loss: 11.882559, mae: 40.164373, mean_q: 45.428969, mean_eps: 0.351658
 108229/150000: episode: 896, duration: 0.675s, episode steps: 107, steps per second: 158, episode reward: -30.315, mean reward: -0.283 [-100.000, 12.510], mean action: 1.636 [0.000, 3.000],  loss: 14.397379, mae: 41.093957, mean_q: 46.664565, mean_eps: 0.350950
 108977/150000: episode: 897, duration: 5.039s, episode steps: 748, steps per second: 148, episode reward: -231.985, mean reward: -0.310 [-100.000, 11.175], mean action: 1.710 [0.000, 3.000],  loss: 12.788342, mae: 40.950620, mean_q: 46.420317, mean_eps: 0.348385
 109326/150000: episode: 898, duration: 2.204s, episode steps: 349, steps per second: 158, episode reward: -235.903, mean reward: -0.676 [-100.000, 16.008], mean action: 1.788 [0.000, 3.000],  loss: 14.127184, mae: 41.063569, mean_q: 46.834907, mean_eps: 0.345094
 110326/150000: episode: 899, duration: 7.272s, episode steps: 1000, steps per second: 138, episode reward: 83.538, mean reward:  0.084 [-24.866, 22.027], mean action: 1.996 [0.000, 3.000],  loss: 12.642933, mae: 40.572585, mean_q: 46.715736, mean_eps: 0.341047
 110432/150000: episode: 900, duration: 0.661s, episode steps: 106, steps per second: 160, episode reward: 35.471, mean reward:  0.335 [-100.000, 19.869], mean action: 1.538 [0.000, 3.000],  loss: 15.293431, mae: 40.859438, mean_q: 45.693579, mean_eps: 0.337729
 111432/150000: episode: 901, duration: 6.956s, episode steps: 1000, steps per second: 144, episode reward: 96.569, mean reward:  0.097 [-23.899, 24.247], mean action: 1.567 [0.000, 3.000],  loss: 10.722071, mae: 40.352146, mean_q: 46.426374, mean_eps: 0.334411
 111533/150000: episode: 902, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward:  5.423, mean reward:  0.054 [-100.000, 16.206], mean action: 1.663 [0.000, 3.000],  loss: 7.925297, mae: 40.142907, mean_q: 45.548991, mean_eps: 0.331108
 111644/150000: episode: 903, duration: 0.657s, episode steps: 111, steps per second: 169, episode reward:  5.540, mean reward:  0.050 [-100.000, 22.817], mean action: 1.613 [0.000, 3.000],  loss: 12.031745, mae: 40.265833, mean_q: 46.172524, mean_eps: 0.330472
 111767/150000: episode: 904, duration: 0.808s, episode steps: 123, steps per second: 152, episode reward: 18.729, mean reward:  0.152 [-100.000, 16.425], mean action: 1.740 [0.000, 3.000],  loss: 12.974236, mae: 40.033997, mean_q: 46.892763, mean_eps: 0.329770
 112767/150000: episode: 905, duration: 7.424s, episode steps: 1000, steps per second: 135, episode reward: 43.712, mean reward:  0.044 [-21.667, 22.991], mean action: 1.379 [0.000, 3.000],  loss: 12.728876, mae: 39.725609, mean_q: 46.034865, mean_eps: 0.326401
 112876/150000: episode: 906, duration: 0.687s, episode steps: 109, steps per second: 159, episode reward:  3.882, mean reward:  0.036 [-100.000, 19.660], mean action: 1.716 [0.000, 3.000],  loss: 9.001156, mae: 39.123410, mean_q: 46.002831, mean_eps: 0.323074
 113876/150000: episode: 907, duration: 7.600s, episode steps: 1000, steps per second: 132, episode reward: 139.403, mean reward:  0.139 [-20.869, 22.685], mean action: 1.267 [0.000, 3.000],  loss: 11.050951, mae: 39.517297, mean_q: 46.070556, mean_eps: 0.319747
 114287/150000: episode: 908, duration: 2.811s, episode steps: 411, steps per second: 146, episode reward: -193.359, mean reward: -0.470 [-100.000, 18.975], mean action: 1.635 [0.000, 3.000],  loss: 10.441887, mae: 39.139157, mean_q: 45.857954, mean_eps: 0.315514
 115267/150000: episode: 909, duration: 6.691s, episode steps: 980, steps per second: 146, episode reward: -143.973, mean reward: -0.147 [-100.000, 21.894], mean action: 1.284 [0.000, 3.000],  loss: 12.052471, mae: 38.717264, mean_q: 46.163518, mean_eps: 0.311341
 115423/150000: episode: 910, duration: 0.970s, episode steps: 156, steps per second: 161, episode reward: -153.631, mean reward: -0.985 [-100.000, 20.440], mean action: 1.955 [0.000, 3.000],  loss: 8.900039, mae: 38.535079, mean_q: 46.018334, mean_eps: 0.307933
 115560/150000: episode: 911, duration: 0.823s, episode steps: 137, steps per second: 166, episode reward: -28.149, mean reward: -0.205 [-100.000, 16.347], mean action: 1.752 [0.000, 3.000],  loss: 12.354608, mae: 38.811387, mean_q: 45.924603, mean_eps: 0.307054
 115713/150000: episode: 912, duration: 0.903s, episode steps: 153, steps per second: 169, episode reward: -62.808, mean reward: -0.411 [-100.000,  8.223], mean action: 1.562 [0.000, 3.000],  loss: 15.113753, mae: 38.203258, mean_q: 44.650826, mean_eps: 0.306184
 116713/150000: episode: 913, duration: 6.659s, episode steps: 1000, steps per second: 150, episode reward: 141.449, mean reward:  0.141 [-24.182, 25.158], mean action: 1.248 [0.000, 3.000],  loss: 13.088675, mae: 38.905355, mean_q: 46.432550, mean_eps: 0.302725
 117713/150000: episode: 914, duration: 8.790s, episode steps: 1000, steps per second: 114, episode reward: 91.778, mean reward:  0.092 [-20.604, 23.763], mean action: 1.310 [0.000, 3.000],  loss: 13.087500, mae: 38.529905, mean_q: 46.458067, mean_eps: 0.296725
 117817/150000: episode: 915, duration: 0.696s, episode steps: 104, steps per second: 149, episode reward: 13.887, mean reward:  0.134 [-100.000, 18.830], mean action: 1.750 [0.000, 3.000],  loss: 9.339986, mae: 38.642490, mean_q: 46.496211, mean_eps: 0.293413
 118139/150000: episode: 916, duration: 2.045s, episode steps: 322, steps per second: 157, episode reward: 18.123, mean reward:  0.056 [-100.000, 12.080], mean action: 1.326 [0.000, 3.000],  loss: 9.888908, mae: 38.497624, mean_q: 47.079213, mean_eps: 0.292135
 118308/150000: episode: 917, duration: 0.986s, episode steps: 169, steps per second: 171, episode reward: -133.547, mean reward: -0.790 [-100.000, 32.370], mean action: 1.355 [0.000, 3.000],  loss: 12.709675, mae: 38.959456, mean_q: 47.068610, mean_eps: 0.290662
 118428/150000: episode: 918, duration: 0.729s, episode steps: 120, steps per second: 165, episode reward: 23.113, mean reward:  0.193 [-100.000, 18.557], mean action: 1.792 [0.000, 3.000],  loss: 11.088813, mae: 38.080457, mean_q: 46.304959, mean_eps: 0.289795
 118778/150000: episode: 919, duration: 2.165s, episode steps: 350, steps per second: 162, episode reward: -244.865, mean reward: -0.700 [-100.000, 13.778], mean action: 1.783 [0.000, 3.000],  loss: 14.453286, mae: 38.489325, mean_q: 46.382808, mean_eps: 0.288385
 118901/150000: episode: 920, duration: 0.716s, episode steps: 123, steps per second: 172, episode reward: 10.660, mean reward:  0.087 [-100.000, 18.939], mean action: 1.341 [0.000, 3.000],  loss: 13.177615, mae: 38.765240, mean_q: 47.621137, mean_eps: 0.286966
 119168/150000: episode: 921, duration: 1.626s, episode steps: 267, steps per second: 164, episode reward: -87.693, mean reward: -0.328 [-100.000, 16.754], mean action: 1.629 [0.000, 3.000],  loss: 13.513832, mae: 38.602753, mean_q: 47.255221, mean_eps: 0.285796
 119852/150000: episode: 922, duration: 6.595s, episode steps: 684, steps per second: 104, episode reward: 245.668, mean reward:  0.359 [-19.556, 100.000], mean action: 1.583 [0.000, 3.000],  loss: 11.373325, mae: 39.007075, mean_q: 47.731421, mean_eps: 0.282943
 119959/150000: episode: 923, duration: 0.924s, episode steps: 107, steps per second: 116, episode reward: 36.539, mean reward:  0.341 [-100.000, 16.874], mean action: 1.738 [0.000, 3.000],  loss: 13.299199, mae: 38.526638, mean_q: 47.347796, mean_eps: 0.280570
 120056/150000: episode: 924, duration: 0.690s, episode steps:  97, steps per second: 141, episode reward: 37.803, mean reward:  0.390 [-100.000, 16.460], mean action: 1.866 [0.000, 3.000],  loss: 15.052434, mae: 39.400570, mean_q: 48.545843, mean_eps: 0.279958
 121056/150000: episode: 925, duration: 10.738s, episode steps: 1000, steps per second:  93, episode reward: 130.933, mean reward:  0.131 [-20.622, 27.421], mean action: 1.039 [0.000, 3.000],  loss: 13.416016, mae: 39.024344, mean_q: 47.709486, mean_eps: 0.276667
 121903/150000: episode: 926, duration: 7.253s, episode steps: 847, steps per second: 117, episode reward: 125.916, mean reward:  0.149 [-24.736, 100.000], mean action: 1.908 [0.000, 3.000],  loss: 11.430145, mae: 38.754494, mean_q: 47.750582, mean_eps: 0.271126
 122011/150000: episode: 927, duration: 1.039s, episode steps: 108, steps per second: 104, episode reward: 10.335, mean reward:  0.096 [-100.000, 10.779], mean action: 1.750 [0.000, 3.000],  loss: 12.621674, mae: 38.271230, mean_q: 46.641946, mean_eps: 0.268261
 122111/150000: episode: 928, duration: 1.029s, episode steps: 100, steps per second:  97, episode reward: 24.445, mean reward:  0.244 [-100.000, 18.198], mean action: 1.920 [0.000, 3.000],  loss: 16.104920, mae: 37.962308, mean_q: 46.579462, mean_eps: 0.267637
 122273/150000: episode: 929, duration: 1.509s, episode steps: 162, steps per second: 107, episode reward: 33.046, mean reward:  0.204 [-100.000, 17.479], mean action: 1.543 [0.000, 3.000],  loss: 11.976309, mae: 38.302785, mean_q: 47.248965, mean_eps: 0.266851
 123273/150000: episode: 930, duration: 8.686s, episode steps: 1000, steps per second: 115, episode reward: 56.929, mean reward:  0.057 [-20.614, 24.441], mean action: 1.276 [0.000, 3.000],  loss: 11.580188, mae: 37.756464, mean_q: 46.424323, mean_eps: 0.263365
 123532/150000: episode: 931, duration: 1.912s, episode steps: 259, steps per second: 135, episode reward: -55.808, mean reward: -0.215 [-100.000, 11.127], mean action: 1.645 [0.000, 3.000],  loss: 11.777440, mae: 37.729820, mean_q: 46.053411, mean_eps: 0.259588
 124532/150000: episode: 932, duration: 11.070s, episode steps: 1000, steps per second:  90, episode reward: 167.841, mean reward:  0.168 [-24.617, 23.235], mean action: 1.483 [0.000, 3.000],  loss: 12.459726, mae: 37.539314, mean_q: 46.490710, mean_eps: 0.255811
 125532/150000: episode: 933, duration: 8.309s, episode steps: 1000, steps per second: 120, episode reward: 94.281, mean reward:  0.094 [-22.588, 24.923], mean action: 1.901 [0.000, 3.000],  loss: 11.662132, mae: 36.823688, mean_q: 45.967235, mean_eps: 0.249811
 126532/150000: episode: 934, duration: 7.120s, episode steps: 1000, steps per second: 140, episode reward: 99.603, mean reward:  0.100 [-20.814, 23.155], mean action: 1.147 [0.000, 3.000],  loss: 12.307655, mae: 36.708457, mean_q: 46.163175, mean_eps: 0.243811
 127532/150000: episode: 935, duration: 8.712s, episode steps: 1000, steps per second: 115, episode reward: 51.367, mean reward:  0.051 [-19.140, 22.559], mean action: 1.458 [0.000, 3.000],  loss: 10.954567, mae: 36.161488, mean_q: 45.608113, mean_eps: 0.237811
 128532/150000: episode: 936, duration: 7.739s, episode steps: 1000, steps per second: 129, episode reward: -6.453, mean reward: -0.006 [-22.557, 13.464], mean action: 1.770 [0.000, 3.000],  loss: 12.671933, mae: 35.995961, mean_q: 45.494490, mean_eps: 0.231811
 128680/150000: episode: 937, duration: 0.951s, episode steps: 148, steps per second: 156, episode reward: -116.730, mean reward: -0.789 [-100.000, 31.472], mean action: 1.635 [0.000, 3.000],  loss: 12.297188, mae: 36.342986, mean_q: 46.044369, mean_eps: 0.228367
 129680/150000: episode: 938, duration: 8.420s, episode steps: 1000, steps per second: 119, episode reward: 114.564, mean reward:  0.115 [-20.981, 22.606], mean action: 0.970 [0.000, 3.000],  loss: 12.394362, mae: 35.348126, mean_q: 45.018014, mean_eps: 0.224923
 130680/150000: episode: 939, duration: 9.269s, episode steps: 1000, steps per second: 108, episode reward: 110.599, mean reward:  0.111 [-20.322, 22.856], mean action: 1.023 [0.000, 3.000],  loss: 10.405117, mae: 35.199004, mean_q: 45.158562, mean_eps: 0.218923
 131680/150000: episode: 940, duration: 7.673s, episode steps: 1000, steps per second: 130, episode reward: 42.941, mean reward:  0.043 [-21.761, 22.462], mean action: 1.100 [0.000, 3.000],  loss: 11.811820, mae: 34.592486, mean_q: 44.368536, mean_eps: 0.212923
 132680/150000: episode: 941, duration: 7.490s, episode steps: 1000, steps per second: 134, episode reward: 30.590, mean reward:  0.031 [-20.922, 22.576], mean action: 1.478 [0.000, 3.000],  loss: 9.982488, mae: 34.525466, mean_q: 44.484575, mean_eps: 0.206923
 133127/150000: episode: 942, duration: 2.923s, episode steps: 447, steps per second: 153, episode reward: 265.773, mean reward:  0.595 [-20.284, 100.000], mean action: 1.609 [0.000, 3.000],  loss: 8.966075, mae: 34.553801, mean_q: 44.517313, mean_eps: 0.202582
 133642/150000: episode: 943, duration: 3.357s, episode steps: 515, steps per second: 153, episode reward: 205.698, mean reward:  0.399 [-19.574, 100.000], mean action: 1.635 [0.000, 3.000],  loss: 9.330109, mae: 34.701321, mean_q: 44.741387, mean_eps: 0.199696
 134642/150000: episode: 944, duration: 7.098s, episode steps: 1000, steps per second: 141, episode reward: 103.762, mean reward:  0.104 [-20.395, 20.479], mean action: 1.397 [0.000, 3.000],  loss: 11.078531, mae: 34.845261, mean_q: 45.115638, mean_eps: 0.195151
 135585/150000: episode: 945, duration: 7.472s, episode steps: 943, steps per second: 126, episode reward: 154.387, mean reward:  0.164 [-21.412, 100.000], mean action: 1.563 [0.000, 3.000],  loss: 10.204913, mae: 34.071631, mean_q: 43.970570, mean_eps: 0.189322
 136488/150000: episode: 946, duration: 6.621s, episode steps: 903, steps per second: 136, episode reward: 178.639, mean reward:  0.198 [-19.654, 100.000], mean action: 1.256 [0.000, 3.000],  loss: 8.454621, mae: 33.861518, mean_q: 43.894023, mean_eps: 0.183784
 136728/150000: episode: 947, duration: 1.527s, episode steps: 240, steps per second: 157, episode reward: -159.926, mean reward: -0.666 [-100.000, 14.139], mean action: 1.908 [0.000, 3.000],  loss: 7.073778, mae: 34.123122, mean_q: 44.482296, mean_eps: 0.180355
 137728/150000: episode: 948, duration: 6.733s, episode steps: 1000, steps per second: 149, episode reward: 71.372, mean reward:  0.071 [-19.851, 13.752], mean action: 1.841 [0.000, 3.000],  loss: 9.510050, mae: 33.404533, mean_q: 43.241113, mean_eps: 0.176635
 138352/150000: episode: 949, duration: 4.319s, episode steps: 624, steps per second: 144, episode reward: 240.415, mean reward:  0.385 [-19.687, 100.000], mean action: 2.417 [0.000, 3.000],  loss: 8.195238, mae: 32.856508, mean_q: 42.707795, mean_eps: 0.171763
 139352/150000: episode: 950, duration: 7.847s, episode steps: 1000, steps per second: 127, episode reward: 64.693, mean reward:  0.065 [-21.363, 12.958], mean action: 1.586 [0.000, 3.000],  loss: 7.809394, mae: 32.293273, mean_q: 41.975523, mean_eps: 0.166891
 139810/150000: episode: 951, duration: 3.257s, episode steps: 458, steps per second: 141, episode reward: 226.354, mean reward:  0.494 [-20.839, 100.000], mean action: 1.434 [0.000, 3.000],  loss: 12.071322, mae: 32.004376, mean_q: 41.578887, mean_eps: 0.162517
 140810/150000: episode: 952, duration: 8.262s, episode steps: 1000, steps per second: 121, episode reward: 112.417, mean reward:  0.112 [-24.062, 19.730], mean action: 1.122 [0.000, 3.000],  loss: 7.907488, mae: 31.986456, mean_q: 41.729949, mean_eps: 0.158143
 141229/150000: episode: 953, duration: 3.080s, episode steps: 419, steps per second: 136, episode reward: 224.340, mean reward:  0.535 [-18.338, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 10.566367, mae: 32.465405, mean_q: 42.220909, mean_eps: 0.153886
 141895/150000: episode: 954, duration: 4.479s, episode steps: 666, steps per second: 149, episode reward: 217.661, mean reward:  0.327 [-24.394, 100.000], mean action: 0.823 [0.000, 3.000],  loss: 10.243996, mae: 32.618304, mean_q: 42.376661, mean_eps: 0.150631
 142107/150000: episode: 955, duration: 1.413s, episode steps: 212, steps per second: 150, episode reward: -47.943, mean reward: -0.226 [-100.000, 10.942], mean action: 1.726 [0.000, 3.000],  loss: 8.937098, mae: 33.038221, mean_q: 42.734225, mean_eps: 0.147997
 142215/150000: episode: 956, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward:  9.811, mean reward:  0.091 [-100.000, 15.721], mean action: 1.639 [0.000, 3.000],  loss: 12.261205, mae: 33.247151, mean_q: 43.040038, mean_eps: 0.147037
 143023/150000: episode: 957, duration: 6.115s, episode steps: 808, steps per second: 132, episode reward: 171.489, mean reward:  0.212 [-19.760, 100.000], mean action: 1.625 [0.000, 3.000],  loss: 7.143921, mae: 32.620727, mean_q: 42.427260, mean_eps: 0.144289
 143671/150000: episode: 958, duration: 4.707s, episode steps: 648, steps per second: 138, episode reward: -138.873, mean reward: -0.214 [-100.000, 18.554], mean action: 1.505 [0.000, 3.000],  loss: 7.624615, mae: 32.936333, mean_q: 42.922904, mean_eps: 0.139921
 144671/150000: episode: 959, duration: 8.948s, episode steps: 1000, steps per second: 112, episode reward: 10.095, mean reward:  0.010 [-19.038, 15.143], mean action: 1.387 [0.000, 3.000],  loss: 8.641550, mae: 32.598766, mean_q: 42.668217, mean_eps: 0.134977
 145519/150000: episode: 960, duration: 5.635s, episode steps: 848, steps per second: 150, episode reward: 208.155, mean reward:  0.245 [-24.352, 100.000], mean action: 1.050 [0.000, 3.000],  loss: 7.477693, mae: 32.207382, mean_q: 41.784183, mean_eps: 0.129433
 145720/150000: episode: 961, duration: 1.212s, episode steps: 201, steps per second: 166, episode reward: -126.896, mean reward: -0.631 [-100.000, 14.212], mean action: 1.995 [0.000, 3.000],  loss: 10.684846, mae: 32.385541, mean_q: 42.069358, mean_eps: 0.126286
 146249/150000: episode: 962, duration: 3.438s, episode steps: 529, steps per second: 154, episode reward: 230.765, mean reward:  0.436 [-19.899, 100.000], mean action: 0.847 [0.000, 3.000],  loss: 10.140782, mae: 32.201839, mean_q: 41.552775, mean_eps: 0.124096
 146982/150000: episode: 963, duration: 5.063s, episode steps: 733, steps per second: 145, episode reward: 278.143, mean reward:  0.379 [-24.723, 100.000], mean action: 0.963 [0.000, 3.000],  loss: 8.651824, mae: 32.382210, mean_q: 42.164318, mean_eps: 0.120310
 147421/150000: episode: 964, duration: 3.763s, episode steps: 439, steps per second: 117, episode reward: 257.222, mean reward:  0.586 [-8.873, 100.000], mean action: 1.558 [0.000, 3.000],  loss: 6.949458, mae: 32.207227, mean_q: 42.023639, mean_eps: 0.116794
 148302/150000: episode: 965, duration: 7.369s, episode steps: 881, steps per second: 120, episode reward: 260.480, mean reward:  0.296 [-20.259, 100.000], mean action: 1.144 [0.000, 3.000],  loss: 8.376194, mae: 31.893580, mean_q: 41.412318, mean_eps: 0.112834
 148676/150000: episode: 966, duration: 2.845s, episode steps: 374, steps per second: 131, episode reward: 245.382, mean reward:  0.656 [-9.854, 100.000], mean action: 1.366 [0.000, 3.000],  loss: 8.654697, mae: 32.026046, mean_q: 41.539050, mean_eps: 0.109069
 149257/150000: episode: 967, duration: 3.947s, episode steps: 581, steps per second: 147, episode reward: 270.988, mean reward:  0.466 [-19.078, 100.000], mean action: 1.379 [0.000, 3.000],  loss: 8.012315, mae: 31.842217, mean_q: 41.544907, mean_eps: 0.106204
 149660/150000: episode: 968, duration: 3.025s, episode steps: 403, steps per second: 133, episode reward: 222.082, mean reward:  0.551 [-19.764, 100.000], mean action: 1.084 [0.000, 3.000],  loss: 7.412347, mae: 32.198767, mean_q: 42.128601, mean_eps: 0.103252
 149962/150000: episode: 969, duration: 2.129s, episode steps: 302, steps per second: 142, episode reward: 277.759, mean reward:  0.920 [-18.725, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 7.835776, mae: 32.402417, mean_q: 42.502316, mean_eps: 0.101137
done, took 1032.362 seconds
Testing for 5 episodes ...
Episode 1: reward: 231.146, steps: 384
Episode 2: reward: 132.436, steps: 1000
Episode 3: reward: 214.355, steps: 337
Episode 4: reward: -18.741, steps: 337
Episode 5: reward: 272.681, steps: 305
Testing for 5 episodes ...
Episode 1: reward: -47.949, steps: 122
Episode 2: reward: 275.181, steps: 266
Episode 3: reward: 247.934, steps: 430
Episode 4: reward: 269.411, steps: 493
Episode 5: reward: 161.151, steps: 576
Testing for 5 episodes ...
Episode 1: reward: 130.072, steps: 699
Episode 2: reward: -183.233, steps: 367
Episode 3: reward: 277.430, steps: 332
Episode 4: reward: 112.603, steps: 1000
Episode 5: reward: -59.552, steps: 1000