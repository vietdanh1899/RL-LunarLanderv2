Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
Training for 150000 steps ...
     74/150000: episode: 1, duration: 0.979s, episode steps:  74, steps per second:  76, episode reward: -76.702, mean reward: -1.037 [-100.000, 16.957], mean action: 1.486 [0.000, 3.000],  loss: 1.329424, mse: 1.058805, mean_q: 0.770597, mean_eps: 0.999748
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\rl\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!
  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')
    163/150000: episode: 2, duration: 0.550s, episode steps:  89, steps per second: 162, episode reward: -109.929, mean reward: -1.235 [-100.000, 11.289], mean action: 1.517 [0.000, 3.000],  loss: 41.192726, mse: 25.970265, mean_q: 4.118402, mean_eps: 0.999292
    264/150000: episode: 3, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: -289.571, mean reward: -2.867 [-100.000,  5.919], mean action: 1.248 [0.000, 3.000],  loss: 54.334659, mse: 42.141297, mean_q: 6.386077, mean_eps: 0.998722
    339/150000: episode: 4, duration: 0.496s, episode steps:  75, steps per second: 151, episode reward: -204.297, mean reward: -2.724 [-100.000,  5.909], mean action: 1.333 [0.000, 3.000],  loss: 46.113826, mse: 43.222133, mean_q: 6.053957, mean_eps: 0.998194
    423/150000: episode: 5, duration: 0.547s, episode steps:  84, steps per second: 153, episode reward: -172.587, mean reward: -2.055 [-100.000,  7.697], mean action: 1.310 [0.000, 3.000],  loss: 44.582084, mse: 41.944857, mean_q: 5.665630, mean_eps: 0.997717
    522/150000: episode: 6, duration: 0.605s, episode steps:  99, steps per second: 164, episode reward: -363.317, mean reward: -3.670 [-100.000,  0.445], mean action: 1.586 [0.000, 3.000],  loss: 33.144567, mse: 43.766569, mean_q: 6.527561, mean_eps: 0.997168
    629/150000: episode: 7, duration: 0.666s, episode steps: 107, steps per second: 161, episode reward: -134.714, mean reward: -1.259 [-100.000,  6.685], mean action: 1.486 [0.000, 3.000],  loss: 32.976333, mse: 67.102105, mean_q: 7.685649, mean_eps: 0.996550
    710/150000: episode: 8, duration: 0.549s, episode steps:  81, steps per second: 147, episode reward: -203.930, mean reward: -2.518 [-100.000, 21.824], mean action: 1.506 [0.000, 3.000],  loss: 56.821924, mse: 108.612142, mean_q: 9.356658, mean_eps: 0.995986
    801/150000: episode: 9, duration: 0.728s, episode steps:  91, steps per second: 125, episode reward: -137.123, mean reward: -1.507 [-100.000,  7.374], mean action: 1.374 [0.000, 3.000],  loss: 31.738264, mse: 128.203369, mean_q: 10.751232, mean_eps: 0.995470
    940/150000: episode: 10, duration: 0.969s, episode steps: 139, steps per second: 144, episode reward: -275.058, mean reward: -1.979 [-100.000, 43.410], mean action: 1.388 [0.000, 3.000],  loss: 21.550515, mse: 142.006633, mean_q: 10.933588, mean_eps: 0.994780
   1026/150000: episode: 11, duration: 0.591s, episode steps:  86, steps per second: 146, episode reward: -163.275, mean reward: -1.899 [-100.000,  7.176], mean action: 1.512 [0.000, 3.000],  loss: 25.872061, mse: 186.056482, mean_q: 11.318360, mean_eps: 0.994105
   1087/150000: episode: 12, duration: 0.573s, episode steps:  61, steps per second: 107, episode reward: -104.883, mean reward: -1.719 [-100.000,  8.212], mean action: 1.475 [0.000, 3.000],  loss: 26.110606, mse: 220.536118, mean_q: 12.690298, mean_eps: 0.993664
   1168/150000: episode: 13, duration: 0.726s, episode steps:  81, steps per second: 112, episode reward: -110.933, mean reward: -1.370 [-100.000, 20.132], mean action: 1.370 [0.000, 3.000],  loss: 18.463129, mse: 231.921607, mean_q: 13.370155, mean_eps: 0.993238
   1276/150000: episode: 14, duration: 0.847s, episode steps: 108, steps per second: 127, episode reward: -145.895, mean reward: -1.351 [-100.000,  8.604], mean action: 1.713 [0.000, 3.000],  loss: 19.638518, mse: 270.417376, mean_q: 14.806545, mean_eps: 0.992671
   1375/150000: episode: 15, duration: 0.781s, episode steps:  99, steps per second: 127, episode reward: -65.751, mean reward: -0.664 [-100.000, 15.849], mean action: 1.374 [0.000, 3.000],  loss: 21.374081, mse: 318.121954, mean_q: 15.744956, mean_eps: 0.992050
   1436/150000: episode: 16, duration: 0.469s, episode steps:  61, steps per second: 130, episode reward: -55.204, mean reward: -0.905 [-100.000, 22.300], mean action: 1.475 [0.000, 3.000],  loss: 18.134559, mse: 345.545616, mean_q: 15.638340, mean_eps: 0.991570
   1533/150000: episode: 17, duration: 0.601s, episode steps:  97, steps per second: 161, episode reward: -224.442, mean reward: -2.314 [-100.000,  7.128], mean action: 1.443 [0.000, 3.000],  loss: 18.226266, mse: 372.818091, mean_q: 15.078207, mean_eps: 0.991096
   1631/150000: episode: 18, duration: 0.584s, episode steps:  98, steps per second: 168, episode reward: -300.746, mean reward: -3.069 [-100.000,  0.266], mean action: 1.500 [0.000, 3.000],  loss: 22.357351, mse: 395.046787, mean_q: 15.357260, mean_eps: 0.990511
   1724/150000: episode: 19, duration: 0.556s, episode steps:  93, steps per second: 167, episode reward: -114.254, mean reward: -1.229 [-100.000, 71.730], mean action: 1.473 [0.000, 3.000],  loss: 22.030157, mse: 415.629193, mean_q: 17.152685, mean_eps: 0.989938
   1844/150000: episode: 20, duration: 0.739s, episode steps: 120, steps per second: 162, episode reward: -340.626, mean reward: -2.839 [-100.000,  1.731], mean action: 1.342 [0.000, 3.000],  loss: 23.818651, mse: 454.147239, mean_q: 17.323839, mean_eps: 0.989299
   1946/150000: episode: 21, duration: 0.710s, episode steps: 102, steps per second: 144, episode reward: -361.456, mean reward: -3.544 [-100.000,  1.323], mean action: 1.627 [0.000, 3.000],  loss: 20.706692, mse: 496.968902, mean_q: 18.097043, mean_eps: 0.988633
   2035/150000: episode: 22, duration: 0.616s, episode steps:  89, steps per second: 145, episode reward: -375.601, mean reward: -4.220 [-100.000,  0.565], mean action: 1.652 [0.000, 3.000],  loss: 19.198110, mse: 507.365356, mean_q: 16.955152, mean_eps: 0.988060
   2163/150000: episode: 23, duration: 0.811s, episode steps: 128, steps per second: 158, episode reward: -151.233, mean reward: -1.182 [-100.000, 10.356], mean action: 1.477 [0.000, 3.000],  loss: 14.282409, mse: 506.538502, mean_q: 16.242122, mean_eps: 0.987409
   2261/150000: episode: 24, duration: 0.698s, episode steps:  98, steps per second: 140, episode reward: -243.692, mean reward: -2.487 [-100.000,  0.371], mean action: 1.520 [0.000, 3.000],  loss: 14.737263, mse: 530.681968, mean_q: 17.071565, mean_eps: 0.986731
   2376/150000: episode: 25, duration: 0.890s, episode steps: 115, steps per second: 129, episode reward: -143.976, mean reward: -1.252 [-100.000,  6.206], mean action: 1.722 [0.000, 3.000],  loss: 18.139103, mse: 577.150258, mean_q: 17.546553, mean_eps: 0.986092
   2457/150000: episode: 26, duration: 0.607s, episode steps:  81, steps per second: 133, episode reward: -91.653, mean reward: -1.132 [-100.000,  6.307], mean action: 1.778 [0.000, 3.000],  loss: 12.634967, mse: 609.192287, mean_q: 18.202669, mean_eps: 0.985504
   2557/150000: episode: 27, duration: 0.725s, episode steps: 100, steps per second: 138, episode reward: -183.288, mean reward: -1.833 [-100.000,  7.158], mean action: 1.450 [0.000, 3.000],  loss: 17.917861, mse: 638.101345, mean_q: 18.450198, mean_eps: 0.984961
   2650/150000: episode: 28, duration: 0.597s, episode steps:  93, steps per second: 156, episode reward: -101.385, mean reward: -1.090 [-100.000,  8.591], mean action: 1.441 [0.000, 3.000],  loss: 12.322625, mse: 639.891459, mean_q: 18.483945, mean_eps: 0.984382
   2742/150000: episode: 29, duration: 0.577s, episode steps:  92, steps per second: 159, episode reward: -114.582, mean reward: -1.245 [-100.000,  9.614], mean action: 1.630 [0.000, 3.000],  loss: 13.015881, mse: 699.151493, mean_q: 18.605992, mean_eps: 0.983827
   2860/150000: episode: 30, duration: 0.906s, episode steps: 118, steps per second: 130, episode reward: -413.870, mean reward: -3.507 [-100.000, 11.579], mean action: 1.517 [0.000, 3.000],  loss: 11.945822, mse: 738.759024, mean_q: 18.780114, mean_eps: 0.983197
   2962/150000: episode: 31, duration: 0.708s, episode steps: 102, steps per second: 144, episode reward: -283.605, mean reward: -2.780 [-100.000,  8.251], mean action: 1.480 [0.000, 3.000],  loss: 11.087063, mse: 796.115785, mean_q: 18.805598, mean_eps: 0.982537
   3028/150000: episode: 32, duration: 0.454s, episode steps:  66, steps per second: 145, episode reward: -143.024, mean reward: -2.167 [-100.000,  8.201], mean action: 1.652 [0.000, 3.000],  loss: 8.713620, mse: 862.145667, mean_q: 18.761788, mean_eps: 0.982033
   3107/150000: episode: 33, duration: 0.627s, episode steps:  79, steps per second: 126, episode reward: -131.371, mean reward: -1.663 [-100.000,  6.935], mean action: 1.380 [0.000, 3.000],  loss: 6.495655, mse: 894.611409, mean_q: 19.625962, mean_eps: 0.981598
   3176/150000: episode: 34, duration: 0.524s, episode steps:  69, steps per second: 132, episode reward: -96.614, mean reward: -1.400 [-100.000, 23.779], mean action: 1.609 [0.000, 3.000],  loss: 9.604774, mse: 959.790296, mean_q: 19.161736, mean_eps: 0.981154
   3271/150000: episode: 35, duration: 0.689s, episode steps:  95, steps per second: 138, episode reward: -127.110, mean reward: -1.338 [-100.000,  4.753], mean action: 1.526 [0.000, 3.000],  loss: 7.722551, mse: 995.708121, mean_q: 20.240659, mean_eps: 0.980662
   3365/150000: episode: 36, duration: 0.650s, episode steps:  94, steps per second: 145, episode reward: -119.059, mean reward: -1.267 [-100.000, 11.824], mean action: 1.521 [0.000, 3.000],  loss: 8.565319, mse: 1070.245224, mean_q: 21.482559, mean_eps: 0.980095
   3482/150000: episode: 37, duration: 0.826s, episode steps: 117, steps per second: 142, episode reward: -186.505, mean reward: -1.594 [-100.000, 21.809], mean action: 1.675 [0.000, 3.000],  loss: 8.010320, mse: 1125.446790, mean_q: 21.343058, mean_eps: 0.979462
   3614/150000: episode: 38, duration: 0.846s, episode steps: 132, steps per second: 156, episode reward: -284.112, mean reward: -2.152 [-100.000,  4.148], mean action: 1.765 [0.000, 3.000],  loss: 8.537549, mse: 1258.924469, mean_q: 22.167421, mean_eps: 0.978715
   3687/150000: episode: 39, duration: 0.532s, episode steps:  73, steps per second: 137, episode reward: -94.278, mean reward: -1.291 [-100.000, 14.304], mean action: 1.644 [0.000, 3.000],  loss: 7.059480, mse: 1286.421646, mean_q: 23.302253, mean_eps: 0.978100
   3810/150000: episode: 40, duration: 1.102s, episode steps: 123, steps per second: 112, episode reward: -289.652, mean reward: -2.355 [-100.000,  4.039], mean action: 1.821 [0.000, 3.000],  loss: 5.694332, mse: 1375.037173, mean_q: 23.575769, mean_eps: 0.977512
   3924/150000: episode: 41, duration: 1.085s, episode steps: 114, steps per second: 105, episode reward: -102.727, mean reward: -0.901 [-100.000,  6.026], mean action: 1.474 [0.000, 3.000],  loss: 5.541212, mse: 1439.025161, mean_q: 24.619619, mean_eps: 0.976801
   4025/150000: episode: 42, duration: 0.797s, episode steps: 101, steps per second: 127, episode reward: -336.766, mean reward: -3.334 [-100.000,  1.075], mean action: 1.426 [0.000, 3.000],  loss: 6.584589, mse: 1487.388714, mean_q: 24.320740, mean_eps: 0.976156
   4104/150000: episode: 43, duration: 0.616s, episode steps:  79, steps per second: 128, episode reward: -54.460, mean reward: -0.689 [-100.000, 12.202], mean action: 1.595 [0.000, 3.000],  loss: 4.928244, mse: 1595.826958, mean_q: 24.191268, mean_eps: 0.975616
   4194/150000: episode: 44, duration: 0.653s, episode steps:  90, steps per second: 138, episode reward: -350.294, mean reward: -3.892 [-100.000,  0.064], mean action: 1.733 [0.000, 3.000],  loss: 4.878934, mse: 1663.563778, mean_q: 23.786645, mean_eps: 0.975109
   4296/150000: episode: 45, duration: 0.787s, episode steps: 102, steps per second: 130, episode reward: -75.052, mean reward: -0.736 [-100.000,  7.807], mean action: 1.500 [0.000, 3.000],  loss: 4.188613, mse: 1733.460296, mean_q: 24.552652, mean_eps: 0.974533
   4363/150000: episode: 46, duration: 0.477s, episode steps:  67, steps per second: 141, episode reward: -107.094, mean reward: -1.598 [-100.000,  8.167], mean action: 1.433 [0.000, 3.000],  loss: 3.769319, mse: 1815.186855, mean_q: 25.312931, mean_eps: 0.974026
   4475/150000: episode: 47, duration: 0.689s, episode steps: 112, steps per second: 163, episode reward: -169.922, mean reward: -1.517 [-100.000,  2.754], mean action: 1.393 [0.000, 3.000],  loss: 5.068239, mse: 1856.396672, mean_q: 24.289293, mean_eps: 0.973489
   4587/150000: episode: 48, duration: 0.720s, episode steps: 112, steps per second: 156, episode reward: -160.547, mean reward: -1.433 [-100.000, 18.063], mean action: 1.545 [0.000, 3.000],  loss: 6.834125, mse: 1975.164837, mean_q: 24.390688, mean_eps: 0.972817
   4662/150000: episode: 49, duration: 0.568s, episode steps:  75, steps per second: 132, episode reward: -95.319, mean reward: -1.271 [-100.000, 12.170], mean action: 1.480 [0.000, 3.000],  loss: 5.711944, mse: 2082.640594, mean_q: 25.437943, mean_eps: 0.972256
   4781/150000: episode: 50, duration: 0.814s, episode steps: 119, steps per second: 146, episode reward: -202.045, mean reward: -1.698 [-100.000, 13.829], mean action: 1.639 [0.000, 3.000],  loss: 6.176980, mse: 2146.282762, mean_q: 26.212051, mean_eps: 0.971674
   4838/150000: episode: 51, duration: 0.467s, episode steps:  57, steps per second: 122, episode reward: -124.851, mean reward: -2.190 [-100.000,  7.967], mean action: 1.526 [0.000, 3.000],  loss: 5.067071, mse: 2252.569497, mean_q: 27.171671, mean_eps: 0.971146
   4941/150000: episode: 52, duration: 1.265s, episode steps: 103, steps per second:  81, episode reward: -170.780, mean reward: -1.658 [-100.000, 14.336], mean action: 1.485 [0.000, 3.000],  loss: 4.092445, mse: 2293.483576, mean_q: 26.793573, mean_eps: 0.970666
   5046/150000: episode: 53, duration: 1.732s, episode steps: 105, steps per second:  61, episode reward: -298.577, mean reward: -2.844 [-100.000, 19.600], mean action: 1.390 [0.000, 3.000],  loss: 5.634080, mse: 2299.192094, mean_q: 26.799790, mean_eps: 0.970042
   5137/150000: episode: 54, duration: 0.966s, episode steps:  91, steps per second:  94, episode reward: -312.790, mean reward: -3.437 [-100.000,  7.191], mean action: 1.484 [0.000, 3.000],  loss: 5.538244, mse: 2404.067610, mean_q: 28.435970, mean_eps: 0.969454
   5227/150000: episode: 55, duration: 3.186s, episode steps:  90, steps per second:  28, episode reward: -195.825, mean reward: -2.176 [-100.000, 10.044], mean action: 1.400 [0.000, 3.000],  loss: 5.894609, mse: 2493.054938, mean_q: 27.418657, mean_eps: 0.968911
   5293/150000: episode: 56, duration: 1.321s, episode steps:  66, steps per second:  50, episode reward: -134.369, mean reward: -2.036 [-100.000,  6.851], mean action: 1.712 [0.000, 3.000],  loss: 6.616678, mse: 2572.704386, mean_q: 27.123092, mean_eps: 0.968443
   5407/150000: episode: 57, duration: 1.786s, episode steps: 114, steps per second:  64, episode reward: -202.491, mean reward: -1.776 [-100.000, 11.937], mean action: 1.658 [0.000, 3.000],  loss: 6.559692, mse: 2579.760287, mean_q: 29.329224, mean_eps: 0.967903
   5493/150000: episode: 58, duration: 1.535s, episode steps:  86, steps per second:  56, episode reward: -471.961, mean reward: -5.488 [-100.000,  0.339], mean action: 1.826 [0.000, 3.000],  loss: 6.284925, mse: 2608.355060, mean_q: 29.335552, mean_eps: 0.967303
   5596/150000: episode: 59, duration: 1.630s, episode steps: 103, steps per second:  63, episode reward: -236.723, mean reward: -2.298 [-100.000, 43.385], mean action: 1.456 [0.000, 3.000],  loss: 5.753642, mse: 2738.107045, mean_q: 29.006463, mean_eps: 0.966736
   5753/150000: episode: 60, duration: 2.328s, episode steps: 157, steps per second:  67, episode reward: -258.498, mean reward: -1.646 [-100.000, 99.752], mean action: 1.490 [0.000, 3.000],  loss: 5.737269, mse: 2806.878326, mean_q: 29.463542, mean_eps: 0.965956
   5853/150000: episode: 61, duration: 1.982s, episode steps: 100, steps per second:  50, episode reward: -133.996, mean reward: -1.340 [-100.000, 15.953], mean action: 1.420 [0.000, 3.000],  loss: 8.815297, mse: 2876.349241, mean_q: 32.320464, mean_eps: 0.965185
   5934/150000: episode: 62, duration: 30.698s, episode steps:  81, steps per second:   3, episode reward: -80.357, mean reward: -0.992 [-100.000, 34.104], mean action: 1.815 [0.000, 3.000],  loss: 5.633556, mse: 2878.435089, mean_q: 31.411297, mean_eps: 0.964642
   6028/150000: episode: 63, duration: 33.969s, episode steps:  94, steps per second:   3, episode reward: -229.414, mean reward: -2.441 [-100.000,  5.851], mean action: 1.426 [0.000, 3.000],  loss: 11.146462, mse: 2969.902406, mean_q: 30.216692, mean_eps: 0.964117
   6139/150000: episode: 64, duration: 3.040s, episode steps: 111, steps per second:  37, episode reward: -109.099, mean reward: -0.983 [-100.000, 14.394], mean action: 1.432 [0.000, 3.000],  loss: 8.269780, mse: 3136.357703, mean_q: 33.212884, mean_eps: 0.963502
   6205/150000: episode: 65, duration: 1.259s, episode steps:  66, steps per second:  52, episode reward: -98.624, mean reward: -1.494 [-100.000, 12.041], mean action: 1.455 [0.000, 3.000],  loss: 5.308010, mse: 3173.526193, mean_q: 32.685682, mean_eps: 0.962971
   6334/150000: episode: 66, duration: 2.993s, episode steps: 129, steps per second:  43, episode reward: -88.773, mean reward: -0.688 [-100.000, 108.647], mean action: 1.744 [0.000, 3.000],  loss: 8.027729, mse: 3273.814798, mean_q: 32.654137, mean_eps: 0.962386
   6415/150000: episode: 67, duration: 1.786s, episode steps:  81, steps per second:  45, episode reward: -129.733, mean reward: -1.602 [-100.000,  9.442], mean action: 1.556 [0.000, 3.000],  loss: 7.141760, mse: 3269.849118, mean_q: 34.100802, mean_eps: 0.961756
   6475/150000: episode: 68, duration: 0.957s, episode steps:  60, steps per second:  63, episode reward: -86.136, mean reward: -1.436 [-100.000, 18.343], mean action: 1.500 [0.000, 3.000],  loss: 7.417392, mse: 3266.380037, mean_q: 33.132718, mean_eps: 0.961333
   6552/150000: episode: 69, duration: 1.150s, episode steps:  77, steps per second:  67, episode reward: -68.555, mean reward: -0.890 [-100.000, 11.679], mean action: 1.494 [0.000, 3.000],  loss: 7.358565, mse: 3290.135695, mean_q: 32.474098, mean_eps: 0.960922
   6665/150000: episode: 70, duration: 1.349s, episode steps: 113, steps per second:  84, episode reward: -192.389, mean reward: -1.703 [-100.000,  1.137], mean action: 1.593 [0.000, 3.000],  loss: 12.200255, mse: 3425.338634, mean_q: 34.348824, mean_eps: 0.960352
   6779/150000: episode: 71, duration: 1.054s, episode steps: 114, steps per second: 108, episode reward: -151.891, mean reward: -1.332 [-100.000,  6.844], mean action: 1.500 [0.000, 3.000],  loss: 7.278890, mse: 3509.044404, mean_q: 35.878850, mean_eps: 0.959671
   6886/150000: episode: 72, duration: 0.870s, episode steps: 107, steps per second: 123, episode reward: -334.985, mean reward: -3.131 [-100.000,  4.583], mean action: 1.477 [0.000, 3.000],  loss: 9.112023, mse: 3617.535425, mean_q: 36.855673, mean_eps: 0.959008
   6958/150000: episode: 73, duration: 0.572s, episode steps:  72, steps per second: 126, episode reward: -158.321, mean reward: -2.199 [-100.000,  4.519], mean action: 1.403 [0.000, 3.000],  loss: 5.979367, mse: 3473.639136, mean_q: 34.770893, mean_eps: 0.958471
   7026/150000: episode: 74, duration: 0.536s, episode steps:  68, steps per second: 127, episode reward: -67.444, mean reward: -0.992 [-100.000, 34.282], mean action: 1.544 [0.000, 3.000],  loss: 8.635553, mse: 3661.428815, mean_q: 36.996371, mean_eps: 0.958051
   7183/150000: episode: 75, duration: 1.557s, episode steps: 157, steps per second: 101, episode reward: -118.257, mean reward: -0.753 [-100.000, 16.175], mean action: 1.643 [0.000, 3.000],  loss: 6.863176, mse: 3665.820922, mean_q: 37.765394, mean_eps: 0.957376
   7245/150000: episode: 76, duration: 0.997s, episode steps:  62, steps per second:  62, episode reward: -73.515, mean reward: -1.186 [-100.000, 13.503], mean action: 1.742 [0.000, 3.000],  loss: 7.513381, mse: 3699.407813, mean_q: 39.789164, mean_eps: 0.956719
   7389/150000: episode: 77, duration: 1.310s, episode steps: 144, steps per second: 110, episode reward: -362.438, mean reward: -2.517 [-100.000, 82.259], mean action: 1.646 [0.000, 3.000],  loss: 5.315310, mse: 3743.294668, mean_q: 39.693318, mean_eps: 0.956101
   7460/150000: episode: 78, duration: 1.082s, episode steps:  71, steps per second:  66, episode reward: -50.420, mean reward: -0.710 [-100.000, 18.502], mean action: 1.535 [0.000, 3.000],  loss: 4.971281, mse: 3730.730881, mean_q: 37.559960, mean_eps: 0.955456
   7543/150000: episode: 79, duration: 0.977s, episode steps:  83, steps per second:  85, episode reward: -132.054, mean reward: -1.591 [-100.000,  8.863], mean action: 1.434 [0.000, 3.000],  loss: 12.633490, mse: 3700.779247, mean_q: 36.108191, mean_eps: 0.954994
   7634/150000: episode: 80, duration: 0.767s, episode steps:  91, steps per second: 119, episode reward: -326.977, mean reward: -3.593 [-100.000,  0.927], mean action: 1.615 [0.000, 3.000],  loss: 6.329402, mse: 3682.050953, mean_q: 36.316333, mean_eps: 0.954472
   7701/150000: episode: 81, duration: 0.526s, episode steps:  67, steps per second: 127, episode reward: -78.213, mean reward: -1.167 [-100.000,  6.754], mean action: 1.433 [0.000, 3.000],  loss: 9.563447, mse: 3664.000579, mean_q: 36.205659, mean_eps: 0.953998
   7766/150000: episode: 82, duration: 0.487s, episode steps:  65, steps per second: 133, episode reward: -116.333, mean reward: -1.790 [-100.000,  8.935], mean action: 1.400 [0.000, 3.000],  loss: 8.503824, mse: 3640.576968, mean_q: 37.113578, mean_eps: 0.953602
   7844/150000: episode: 83, duration: 0.578s, episode steps:  78, steps per second: 135, episode reward: -82.608, mean reward: -1.059 [-100.000, 16.297], mean action: 1.269 [0.000, 3.000],  loss: 5.790203, mse: 3694.886838, mean_q: 38.361340, mean_eps: 0.953173
   7926/150000: episode: 84, duration: 0.722s, episode steps:  82, steps per second: 114, episode reward: -221.537, mean reward: -2.702 [-100.000, 14.160], mean action: 1.402 [0.000, 3.000],  loss: 8.616465, mse: 3634.859235, mean_q: 37.284182, mean_eps: 0.952693
   8061/150000: episode: 85, duration: 2.456s, episode steps: 135, steps per second:  55, episode reward: -220.912, mean reward: -1.636 [-100.000, 36.527], mean action: 1.341 [0.000, 3.000],  loss: 10.306225, mse: 3705.623132, mean_q: 37.495088, mean_eps: 0.952042
   8171/150000: episode: 86, duration: 1.176s, episode steps: 110, steps per second:  94, episode reward: -233.827, mean reward: -2.126 [-100.000, 27.555], mean action: 1.582 [0.000, 3.000],  loss: 7.719575, mse: 3766.184086, mean_q: 37.925365, mean_eps: 0.951307
   8272/150000: episode: 87, duration: 0.727s, episode steps: 101, steps per second: 139, episode reward: -191.242, mean reward: -1.893 [-100.000,  7.730], mean action: 1.703 [0.000, 3.000],  loss: 6.772396, mse: 3777.479565, mean_q: 37.314525, mean_eps: 0.950674
   8348/150000: episode: 88, duration: 0.625s, episode steps:  76, steps per second: 122, episode reward: -73.205, mean reward: -0.963 [-100.000,  6.750], mean action: 1.566 [0.000, 3.000],  loss: 6.598491, mse: 3801.615305, mean_q: 36.715091, mean_eps: 0.950143
   8464/150000: episode: 89, duration: 1.105s, episode steps: 116, steps per second: 105, episode reward: -120.413, mean reward: -1.038 [-100.000,  6.042], mean action: 1.647 [0.000, 3.000],  loss: 6.593861, mse: 3704.128683, mean_q: 37.547499, mean_eps: 0.949567
   8581/150000: episode: 90, duration: 1.355s, episode steps: 117, steps per second:  86, episode reward: -249.663, mean reward: -2.134 [-100.000,  7.626], mean action: 1.453 [0.000, 3.000],  loss: 7.029506, mse: 3679.993540, mean_q: 37.417035, mean_eps: 0.948868
   8650/150000: episode: 91, duration: 0.683s, episode steps:  69, steps per second: 101, episode reward: -68.960, mean reward: -0.999 [-100.000, 17.804], mean action: 1.493 [0.000, 3.000],  loss: 6.582250, mse: 3610.862782, mean_q: 36.000596, mean_eps: 0.948310
   8753/150000: episode: 92, duration: 0.809s, episode steps: 103, steps per second: 127, episode reward: -262.406, mean reward: -2.548 [-100.000, 15.142], mean action: 1.641 [0.000, 3.000],  loss: 8.960549, mse: 3707.253290, mean_q: 37.912675, mean_eps: 0.947794
   8817/150000: episode: 93, duration: 0.484s, episode steps:  64, steps per second: 132, episode reward: -76.980, mean reward: -1.203 [-100.000, 47.040], mean action: 1.438 [0.000, 3.000],  loss: 5.572938, mse: 3675.427551, mean_q: 37.213910, mean_eps: 0.947293
   8891/150000: episode: 94, duration: 0.579s, episode steps:  74, steps per second: 128, episode reward: -97.570, mean reward: -1.319 [-100.000, 11.762], mean action: 1.554 [0.000, 3.000],  loss: 9.503469, mse: 3701.517106, mean_q: 38.162619, mean_eps: 0.946879
   8966/150000: episode: 95, duration: 0.647s, episode steps:  75, steps per second: 116, episode reward: -104.597, mean reward: -1.395 [-100.000,  6.298], mean action: 1.680 [0.000, 3.000],  loss: 9.335930, mse: 3676.140553, mean_q: 39.829764, mean_eps: 0.946432
   9085/150000: episode: 96, duration: 1.232s, episode steps: 119, steps per second:  97, episode reward: -60.932, mean reward: -0.512 [-100.000, 11.715], mean action: 1.622 [0.000, 3.000],  loss: 6.766971, mse: 3607.952936, mean_q: 37.974242, mean_eps: 0.945850
   9156/150000: episode: 97, duration: 0.671s, episode steps:  71, steps per second: 106, episode reward: -113.153, mean reward: -1.594 [-100.000,  9.470], mean action: 1.704 [0.000, 3.000],  loss: 5.113315, mse: 3623.455597, mean_q: 38.864387, mean_eps: 0.945280
   9295/150000: episode: 98, duration: 1.607s, episode steps: 139, steps per second:  87, episode reward: -122.002, mean reward: -0.878 [-100.000,  6.570], mean action: 1.504 [0.000, 3.000],  loss: 8.091013, mse: 3678.057348, mean_q: 38.365249, mean_eps: 0.944650
   9381/150000: episode: 99, duration: 0.917s, episode steps:  86, steps per second:  94, episode reward: -77.926, mean reward: -0.906 [-100.000,  8.159], mean action: 1.500 [0.000, 3.000],  loss: 8.738853, mse: 3605.264717, mean_q: 36.346225, mean_eps: 0.943975
   9448/150000: episode: 100, duration: 0.738s, episode steps:  67, steps per second:  91, episode reward: -79.787, mean reward: -1.191 [-100.000, 11.381], mean action: 1.478 [0.000, 3.000],  loss: 8.799690, mse: 3522.381442, mean_q: 35.293863, mean_eps: 0.943516
   9530/150000: episode: 101, duration: 0.796s, episode steps:  82, steps per second: 103, episode reward: -146.016, mean reward: -1.781 [-100.000, 12.967], mean action: 1.390 [0.000, 3.000],  loss: 7.349114, mse: 3656.094342, mean_q: 35.417189, mean_eps: 0.943069
   9594/150000: episode: 102, duration: 0.606s, episode steps:  64, steps per second: 106, episode reward: -51.824, mean reward: -0.810 [-100.000, 11.513], mean action: 1.422 [0.000, 3.000],  loss: 7.009604, mse: 3586.846340, mean_q: 35.825045, mean_eps: 0.942631
   9709/150000: episode: 103, duration: 0.932s, episode steps: 115, steps per second: 123, episode reward: -136.059, mean reward: -1.183 [-100.000,  5.807], mean action: 1.539 [0.000, 3.000],  loss: 4.765148, mse: 3576.331594, mean_q: 34.912002, mean_eps: 0.942094
   9810/150000: episode: 104, duration: 0.766s, episode steps: 101, steps per second: 132, episode reward: 12.223, mean reward:  0.121 [-100.000, 57.186], mean action: 1.663 [0.000, 3.000],  loss: 6.811156, mse: 3564.179497, mean_q: 35.327026, mean_eps: 0.941446
   9950/150000: episode: 105, duration: 1.057s, episode steps: 140, steps per second: 132, episode reward: -122.164, mean reward: -0.873 [-100.000, 11.199], mean action: 1.571 [0.000, 3.000],  loss: 5.535937, mse: 3492.859916, mean_q: 34.766451, mean_eps: 0.940723
  10093/150000: episode: 106, duration: 1.465s, episode steps: 143, steps per second:  98, episode reward: -155.737, mean reward: -1.089 [-100.000,  6.512], mean action: 1.552 [0.000, 3.000],  loss: 6.399692, mse: 3521.278414, mean_q: 34.767560, mean_eps: 0.939874
  10174/150000: episode: 107, duration: 0.943s, episode steps:  81, steps per second:  86, episode reward: -78.098, mean reward: -0.964 [-100.000,  9.949], mean action: 1.481 [0.000, 3.000],  loss: 8.800305, mse: 3398.349007, mean_q: 33.338231, mean_eps: 0.939202
  10253/150000: episode: 108, duration: 0.942s, episode steps:  79, steps per second:  84, episode reward: -102.140, mean reward: -1.293 [-100.000,  6.900], mean action: 1.418 [0.000, 3.000],  loss: 4.885778, mse: 3430.579287, mean_q: 34.580920, mean_eps: 0.938722
  10335/150000: episode: 109, duration: 0.922s, episode steps:  82, steps per second:  89, episode reward: -98.270, mean reward: -1.198 [-100.000, 15.769], mean action: 1.402 [0.000, 3.000],  loss: 5.947865, mse: 3391.993572, mean_q: 31.079961, mean_eps: 0.938239
  10405/150000: episode: 110, duration: 0.648s, episode steps:  70, steps per second: 108, episode reward: -34.196, mean reward: -0.489 [-100.000, 16.635], mean action: 1.471 [0.000, 3.000],  loss: 6.002316, mse: 3370.682247, mean_q: 33.170055, mean_eps: 0.937783
  10517/150000: episode: 111, duration: 0.899s, episode steps: 112, steps per second: 125, episode reward: -111.037, mean reward: -0.991 [-100.000, 11.867], mean action: 1.554 [0.000, 3.000],  loss: 7.619022, mse: 3357.409042, mean_q: 33.289653, mean_eps: 0.937237
  10639/150000: episode: 112, duration: 0.873s, episode steps: 122, steps per second: 140, episode reward: -117.021, mean reward: -0.959 [-100.000, 17.373], mean action: 1.492 [0.000, 3.000],  loss: 6.974647, mse: 3329.031990, mean_q: 31.810218, mean_eps: 0.936535
  10702/150000: episode: 113, duration: 0.495s, episode steps:  63, steps per second: 127, episode reward: -79.995, mean reward: -1.270 [-100.000,  8.610], mean action: 1.413 [0.000, 3.000],  loss: 4.327260, mse: 3401.683532, mean_q: 32.520455, mean_eps: 0.935980
  10791/150000: episode: 114, duration: 0.614s, episode steps:  89, steps per second: 145, episode reward: -362.407, mean reward: -4.072 [-100.000,  6.445], mean action: 1.303 [0.000, 3.000],  loss: 4.883030, mse: 3305.483662, mean_q: 31.729635, mean_eps: 0.935524
  10892/150000: episode: 115, duration: 0.700s, episode steps: 101, steps per second: 144, episode reward: -200.669, mean reward: -1.987 [-100.000, 99.018], mean action: 1.554 [0.000, 3.000],  loss: 5.139497, mse: 3281.921819, mean_q: 30.244549, mean_eps: 0.934954
  10972/150000: episode: 116, duration: 0.590s, episode steps:  80, steps per second: 136, episode reward: -90.816, mean reward: -1.135 [-100.000, 13.838], mean action: 1.613 [0.000, 3.000],  loss: 10.068021, mse: 3382.106479, mean_q: 32.331859, mean_eps: 0.934411
  11038/150000: episode: 117, duration: 0.575s, episode steps:  66, steps per second: 115, episode reward: -80.709, mean reward: -1.223 [-100.000,  9.778], mean action: 1.621 [0.000, 3.000],  loss: 6.081403, mse: 3333.918021, mean_q: 31.694112, mean_eps: 0.933973
  11100/150000: episode: 118, duration: 0.614s, episode steps:  62, steps per second: 101, episode reward: -83.167, mean reward: -1.341 [-100.000,  4.697], mean action: 1.532 [0.000, 3.000],  loss: 4.587986, mse: 3304.594731, mean_q: 29.349152, mean_eps: 0.933589
  11187/150000: episode: 119, duration: 0.750s, episode steps:  87, steps per second: 116, episode reward: -268.201, mean reward: -3.083 [-100.000,  8.269], mean action: 1.494 [0.000, 3.000],  loss: 6.696694, mse: 3254.104868, mean_q: 31.622586, mean_eps: 0.933142
  11287/150000: episode: 120, duration: 0.851s, episode steps: 100, steps per second: 117, episode reward: -67.579, mean reward: -0.676 [-100.000, 74.312], mean action: 1.580 [0.000, 3.000],  loss: 6.854185, mse: 3202.423794, mean_q: 29.620729, mean_eps: 0.932581
  11365/150000: episode: 121, duration: 0.625s, episode steps:  78, steps per second: 125, episode reward: -170.958, mean reward: -2.192 [-100.000, 27.062], mean action: 1.526 [0.000, 3.000],  loss: 9.033287, mse: 3268.107328, mean_q: 27.248561, mean_eps: 0.932047
  11491/150000: episode: 122, duration: 0.965s, episode steps: 126, steps per second: 131, episode reward: -113.724, mean reward: -0.903 [-100.000, 12.193], mean action: 1.611 [0.000, 3.000],  loss: 6.685398, mse: 3312.892532, mean_q: 31.042661, mean_eps: 0.931435
  11557/150000: episode: 123, duration: 0.440s, episode steps:  66, steps per second: 150, episode reward: -86.948, mean reward: -1.317 [-100.000,  7.626], mean action: 1.545 [0.000, 3.000],  loss: 10.197258, mse: 3215.101063, mean_q: 28.119489, mean_eps: 0.930859
  11665/150000: episode: 124, duration: 0.843s, episode steps: 108, steps per second: 128, episode reward: -181.040, mean reward: -1.676 [-100.000,  2.406], mean action: 1.546 [0.000, 3.000],  loss: 8.904599, mse: 3272.493291, mean_q: 27.761461, mean_eps: 0.930337
  11779/150000: episode: 125, duration: 0.788s, episode steps: 114, steps per second: 145, episode reward: -24.869, mean reward: -0.218 [-100.000, 65.545], mean action: 1.623 [0.000, 3.000],  loss: 8.180759, mse: 3260.692312, mean_q: 27.037420, mean_eps: 0.929671
  11867/150000: episode: 126, duration: 0.650s, episode steps:  88, steps per second: 135, episode reward: -127.918, mean reward: -1.454 [-100.000,  4.031], mean action: 1.523 [0.000, 3.000],  loss: 8.527364, mse: 3206.203197, mean_q: 27.969450, mean_eps: 0.929065
  11969/150000: episode: 127, duration: 0.775s, episode steps: 102, steps per second: 132, episode reward: -248.306, mean reward: -2.434 [-100.000,  0.633], mean action: 1.539 [0.000, 3.000],  loss: 9.630480, mse: 3176.026032, mean_q: 27.527596, mean_eps: 0.928495
  12046/150000: episode: 128, duration: 0.590s, episode steps:  77, steps per second: 131, episode reward: -75.664, mean reward: -0.983 [-100.000,  7.981], mean action: 1.597 [0.000, 3.000],  loss: 9.047922, mse: 3134.831178, mean_q: 25.162247, mean_eps: 0.927958
  12150/150000: episode: 129, duration: 0.769s, episode steps: 104, steps per second: 135, episode reward: -122.441, mean reward: -1.177 [-100.000, 11.842], mean action: 1.423 [0.000, 3.000],  loss: 6.374412, mse: 3086.685277, mean_q: 25.943034, mean_eps: 0.927415
  12255/150000: episode: 130, duration: 0.849s, episode steps: 105, steps per second: 124, episode reward: -94.185, mean reward: -0.897 [-100.000, 11.932], mean action: 1.562 [0.000, 3.000],  loss: 7.294536, mse: 3117.705934, mean_q: 26.339226, mean_eps: 0.926788
  12334/150000: episode: 131, duration: 0.758s, episode steps:  79, steps per second: 104, episode reward: -121.209, mean reward: -1.534 [-100.000,  9.906], mean action: 1.494 [0.000, 3.000],  loss: 6.145976, mse: 3071.567525, mean_q: 24.986343, mean_eps: 0.926236
  12443/150000: episode: 132, duration: 1.093s, episode steps: 109, steps per second: 100, episode reward: -254.897, mean reward: -2.339 [-100.000,  0.691], mean action: 1.505 [0.000, 3.000],  loss: 7.107462, mse: 3053.357619, mean_q: 25.501536, mean_eps: 0.925672
  12576/150000: episode: 133, duration: 1.301s, episode steps: 133, steps per second: 102, episode reward: -299.582, mean reward: -2.252 [-100.000,  6.398], mean action: 1.436 [0.000, 3.000],  loss: 8.356130, mse: 3114.336775, mean_q: 25.660991, mean_eps: 0.924946
  12665/150000: episode: 134, duration: 0.912s, episode steps:  89, steps per second:  98, episode reward: -140.007, mean reward: -1.573 [-100.000, 15.350], mean action: 1.472 [0.000, 3.000],  loss: 7.058775, mse: 3088.611281, mean_q: 26.101291, mean_eps: 0.924280
  12752/150000: episode: 135, duration: 1.289s, episode steps:  87, steps per second:  68, episode reward: -136.057, mean reward: -1.564 [-100.000,  7.584], mean action: 1.598 [0.000, 3.000],  loss: 6.272083, mse: 3100.809671, mean_q: 24.967410, mean_eps: 0.923752
  12860/150000: episode: 136, duration: 1.419s, episode steps: 108, steps per second:  76, episode reward: -257.589, mean reward: -2.385 [-100.000,  3.372], mean action: 1.287 [0.000, 3.000],  loss: 7.246170, mse: 3043.308212, mean_q: 24.763575, mean_eps: 0.923167
  12960/150000: episode: 137, duration: 1.036s, episode steps: 100, steps per second:  97, episode reward: -203.768, mean reward: -2.038 [-100.000, 23.832], mean action: 1.430 [0.000, 3.000],  loss: 5.369035, mse: 3080.638718, mean_q: 25.048574, mean_eps: 0.922543
  13057/150000: episode: 138, duration: 0.859s, episode steps:  97, steps per second: 113, episode reward: -106.825, mean reward: -1.101 [-100.000,  7.950], mean action: 1.454 [0.000, 3.000],  loss: 4.384227, mse: 2944.115984, mean_q: 26.168193, mean_eps: 0.921952
  13118/150000: episode: 139, duration: 0.563s, episode steps:  61, steps per second: 108, episode reward: -201.678, mean reward: -3.306 [-100.000,  4.948], mean action: 1.639 [0.000, 3.000],  loss: 5.768467, mse: 2897.401996, mean_q: 24.497921, mean_eps: 0.921478
  13220/150000: episode: 140, duration: 0.831s, episode steps: 102, steps per second: 123, episode reward: -336.036, mean reward: -3.294 [-100.000, 70.138], mean action: 1.569 [0.000, 3.000],  loss: 5.972807, mse: 2902.864114, mean_q: 23.696745, mean_eps: 0.920989
  13324/150000: episode: 141, duration: 0.835s, episode steps: 104, steps per second: 125, episode reward: -134.641, mean reward: -1.295 [-100.000,  5.422], mean action: 1.567 [0.000, 3.000],  loss: 5.355108, mse: 2934.137977, mean_q: 24.851909, mean_eps: 0.920371
  13408/150000: episode: 142, duration: 0.713s, episode steps:  84, steps per second: 118, episode reward: -126.911, mean reward: -1.511 [-100.000, 25.348], mean action: 1.536 [0.000, 3.000],  loss: 6.930906, mse: 2948.435338, mean_q: 23.258162, mean_eps: 0.919807
  13478/150000: episode: 143, duration: 0.607s, episode steps:  70, steps per second: 115, episode reward: -77.908, mean reward: -1.113 [-100.000,  6.960], mean action: 1.671 [0.000, 3.000],  loss: 6.807554, mse: 2922.079953, mean_q: 23.790328, mean_eps: 0.919345
  13550/150000: episode: 144, duration: 0.589s, episode steps:  72, steps per second: 122, episode reward: -45.741, mean reward: -0.635 [-100.000, 13.350], mean action: 1.403 [0.000, 3.000],  loss: 7.187488, mse: 2863.708727, mean_q: 24.652164, mean_eps: 0.918919
  13624/150000: episode: 145, duration: 0.647s, episode steps:  74, steps per second: 114, episode reward: -91.462, mean reward: -1.236 [-100.000,  5.034], mean action: 1.568 [0.000, 3.000],  loss: 9.084688, mse: 2870.909229, mean_q: 24.188540, mean_eps: 0.918481
  13685/150000: episode: 146, duration: 0.526s, episode steps:  61, steps per second: 116, episode reward: -74.994, mean reward: -1.229 [-100.000, 13.795], mean action: 1.525 [0.000, 3.000],  loss: 7.124960, mse: 2943.826112, mean_q: 26.840176, mean_eps: 0.918076
  13798/150000: episode: 147, duration: 1.307s, episode steps: 113, steps per second:  86, episode reward: -199.696, mean reward: -1.767 [-100.000,  6.302], mean action: 1.611 [0.000, 3.000],  loss: 6.083924, mse: 2896.196965, mean_q: 24.657620, mean_eps: 0.917554
  13892/150000: episode: 148, duration: 0.958s, episode steps:  94, steps per second:  98, episode reward: -159.973, mean reward: -1.702 [-100.000, 20.552], mean action: 1.489 [0.000, 3.000],  loss: 7.416993, mse: 2776.784554, mean_q: 23.525335, mean_eps: 0.916933
  13981/150000: episode: 149, duration: 0.825s, episode steps:  89, steps per second: 108, episode reward: -160.042, mean reward: -1.798 [-100.000, 32.058], mean action: 1.573 [0.000, 3.000],  loss: 5.320806, mse: 2746.555343, mean_q: 24.700034, mean_eps: 0.916384
  14062/150000: episode: 150, duration: 1.219s, episode steps:  81, steps per second:  66, episode reward: -129.002, mean reward: -1.593 [-100.000, 11.297], mean action: 1.444 [0.000, 3.000],  loss: 5.565952, mse: 2719.937099, mean_q: 23.471586, mean_eps: 0.915874
  14183/150000: episode: 151, duration: 1.318s, episode steps: 121, steps per second:  92, episode reward: -66.720, mean reward: -0.551 [-100.000,  7.218], mean action: 1.612 [0.000, 3.000],  loss: 7.065749, mse: 2760.289954, mean_q: 22.187294, mean_eps: 0.915268
  14280/150000: episode: 152, duration: 0.947s, episode steps:  97, steps per second: 102, episode reward: -68.052, mean reward: -0.702 [-100.000, 48.003], mean action: 1.557 [0.000, 3.000],  loss: 5.780488, mse: 2704.290059, mean_q: 23.376151, mean_eps: 0.914614
  14393/150000: episode: 153, duration: 0.886s, episode steps: 113, steps per second: 128, episode reward: -285.268, mean reward: -2.524 [-100.000,  7.200], mean action: 1.504 [0.000, 3.000],  loss: 6.489720, mse: 2744.255833, mean_q: 23.246437, mean_eps: 0.913984
  14472/150000: episode: 154, duration: 0.521s, episode steps:  79, steps per second: 152, episode reward: -71.886, mean reward: -0.910 [-100.000, 17.488], mean action: 1.734 [0.000, 3.000],  loss: 5.993546, mse: 2748.636546, mean_q: 22.661813, mean_eps: 0.913408
  14550/150000: episode: 155, duration: 0.564s, episode steps:  78, steps per second: 138, episode reward: -200.858, mean reward: -2.575 [-100.000,  7.307], mean action: 1.590 [0.000, 3.000],  loss: 5.745709, mse: 2692.097083, mean_q: 20.762033, mean_eps: 0.912937
  14635/150000: episode: 156, duration: 0.597s, episode steps:  85, steps per second: 142, episode reward: -86.821, mean reward: -1.021 [-100.000, 13.915], mean action: 1.600 [0.000, 3.000],  loss: 6.748780, mse: 2661.143739, mean_q: 21.173474, mean_eps: 0.912448
  14714/150000: episode: 157, duration: 0.562s, episode steps:  79, steps per second: 140, episode reward: -102.793, mean reward: -1.301 [-100.000,  7.644], mean action: 1.494 [0.000, 3.000],  loss: 7.168899, mse: 2575.112552, mean_q: 22.079892, mean_eps: 0.911956
  14826/150000: episode: 158, duration: 0.868s, episode steps: 112, steps per second: 129, episode reward: -117.140, mean reward: -1.046 [-100.000,  7.564], mean action: 1.491 [0.000, 3.000],  loss: 5.685361, mse: 2619.536208, mean_q: 21.815115, mean_eps: 0.911383
  14926/150000: episode: 159, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -69.071, mean reward: -0.691 [-100.000, 115.943], mean action: 1.590 [0.000, 3.000],  loss: 4.409917, mse: 2650.662607, mean_q: 21.748915, mean_eps: 0.910747
  14988/150000: episode: 160, duration: 0.425s, episode steps:  62, steps per second: 146, episode reward: -178.392, mean reward: -2.877 [-100.000, 28.376], mean action: 1.339 [0.000, 3.000],  loss: 5.781415, mse: 2622.431908, mean_q: 19.541709, mean_eps: 0.910261
  15112/150000: episode: 161, duration: 0.955s, episode steps: 124, steps per second: 130, episode reward: -2.906, mean reward: -0.023 [-100.000, 63.398], mean action: 1.718 [0.000, 3.000],  loss: 7.500561, mse: 2655.000432, mean_q: 20.378694, mean_eps: 0.909703
  15253/150000: episode: 162, duration: 1.060s, episode steps: 141, steps per second: 133, episode reward: -149.730, mean reward: -1.062 [-100.000, 12.692], mean action: 1.482 [0.000, 3.000],  loss: 9.085600, mse: 2639.357322, mean_q: 20.271153, mean_eps: 0.908908
  15379/150000: episode: 163, duration: 0.820s, episode steps: 126, steps per second: 154, episode reward: -114.201, mean reward: -0.906 [-100.000,  7.663], mean action: 1.548 [0.000, 3.000],  loss: 8.593776, mse: 2623.849789, mean_q: 21.029951, mean_eps: 0.908107
  15519/150000: episode: 164, duration: 0.919s, episode steps: 140, steps per second: 152, episode reward: -43.579, mean reward: -0.311 [-100.000, 15.555], mean action: 1.593 [0.000, 3.000],  loss: 8.897104, mse: 2642.375859, mean_q: 21.582599, mean_eps: 0.907309
  15611/150000: episode: 165, duration: 0.566s, episode steps:  92, steps per second: 162, episode reward: -105.674, mean reward: -1.149 [-100.000,  9.899], mean action: 1.587 [0.000, 3.000],  loss: 7.676199, mse: 2604.171030, mean_q: 21.009363, mean_eps: 0.906613
  15696/150000: episode: 166, duration: 0.518s, episode steps:  85, steps per second: 164, episode reward: -246.071, mean reward: -2.895 [-100.000,  6.953], mean action: 1.412 [0.000, 3.000],  loss: 12.003535, mse: 2506.455693, mean_q: 20.764131, mean_eps: 0.906082
  15780/150000: episode: 167, duration: 0.603s, episode steps:  84, steps per second: 139, episode reward: -52.655, mean reward: -0.627 [-100.000, 74.567], mean action: 1.429 [0.000, 3.000],  loss: 4.076003, mse: 2560.231868, mean_q: 19.845527, mean_eps: 0.905575
  15856/150000: episode: 168, duration: 0.490s, episode steps:  76, steps per second: 155, episode reward: -87.492, mean reward: -1.151 [-100.000,  8.918], mean action: 1.671 [0.000, 3.000],  loss: 9.323944, mse: 2512.937351, mean_q: 19.655181, mean_eps: 0.905095
  15961/150000: episode: 169, duration: 0.637s, episode steps: 105, steps per second: 165, episode reward: -198.206, mean reward: -1.888 [-100.000,  1.422], mean action: 1.352 [0.000, 3.000],  loss: 7.067439, mse: 2515.781115, mean_q: 18.586777, mean_eps: 0.904552
  16079/150000: episode: 170, duration: 0.741s, episode steps: 118, steps per second: 159, episode reward: -65.051, mean reward: -0.551 [-100.000, 15.338], mean action: 1.466 [0.000, 3.000],  loss: 9.606075, mse: 2552.913736, mean_q: 17.600561, mean_eps: 0.903883
  16176/150000: episode: 171, duration: 0.643s, episode steps:  97, steps per second: 151, episode reward: -120.824, mean reward: -1.246 [-100.000, 14.175], mean action: 1.577 [0.000, 3.000],  loss: 8.854199, mse: 2500.347831, mean_q: 17.234770, mean_eps: 0.903238
  16270/150000: episode: 172, duration: 0.598s, episode steps:  94, steps per second: 157, episode reward: -159.106, mean reward: -1.693 [-100.000, 11.657], mean action: 1.532 [0.000, 3.000],  loss: 7.778737, mse: 2504.762124, mean_q: 17.703053, mean_eps: 0.902665
  16378/150000: episode: 173, duration: 0.658s, episode steps: 108, steps per second: 164, episode reward: -168.914, mean reward: -1.564 [-100.000, 30.288], mean action: 1.509 [0.000, 3.000],  loss: 7.270575, mse: 2414.309989, mean_q: 18.292055, mean_eps: 0.902059
  16441/150000: episode: 174, duration: 0.458s, episode steps:  63, steps per second: 138, episode reward: -92.893, mean reward: -1.474 [-100.000,  9.312], mean action: 1.683 [0.000, 3.000],  loss: 9.874817, mse: 2350.774133, mean_q: 18.723209, mean_eps: 0.901546
  16533/150000: episode: 175, duration: 0.661s, episode steps:  92, steps per second: 139, episode reward: -95.268, mean reward: -1.036 [-100.000, 13.599], mean action: 1.620 [0.000, 3.000],  loss: 7.645157, mse: 2299.693978, mean_q: 17.929873, mean_eps: 0.901081
  16619/150000: episode: 176, duration: 0.536s, episode steps:  86, steps per second: 161, episode reward: -290.031, mean reward: -3.372 [-100.000, 117.176], mean action: 1.547 [0.000, 3.000],  loss: 6.852223, mse: 2327.302215, mean_q: 16.086646, mean_eps: 0.900547
  16711/150000: episode: 177, duration: 0.554s, episode steps:  92, steps per second: 166, episode reward: -99.214, mean reward: -1.078 [-100.000,  7.002], mean action: 1.489 [0.000, 3.000],  loss: 11.806012, mse: 2288.040978, mean_q: 15.703493, mean_eps: 0.900013
  16795/150000: episode: 178, duration: 0.624s, episode steps:  84, steps per second: 135, episode reward: -81.183, mean reward: -0.966 [-100.000, 11.901], mean action: 1.560 [0.000, 3.000],  loss: 7.328874, mse: 2223.502414, mean_q: 17.281804, mean_eps: 0.899485
  16892/150000: episode: 179, duration: 0.622s, episode steps:  97, steps per second: 156, episode reward: -197.366, mean reward: -2.035 [-100.000,  0.809], mean action: 1.629 [0.000, 3.000],  loss: 9.175517, mse: 2264.197209, mean_q: 16.758653, mean_eps: 0.898942
  16979/150000: episode: 180, duration: 0.544s, episode steps:  87, steps per second: 160, episode reward: -88.361, mean reward: -1.016 [-100.000, 13.941], mean action: 1.448 [0.000, 3.000],  loss: 7.432830, mse: 2234.904792, mean_q: 16.442495, mean_eps: 0.898390
  17074/150000: episode: 181, duration: 0.598s, episode steps:  95, steps per second: 159, episode reward: -212.019, mean reward: -2.232 [-100.000,  8.093], mean action: 1.337 [0.000, 3.000],  loss: 7.936868, mse: 2213.890813, mean_q: 15.994959, mean_eps: 0.897844
  17148/150000: episode: 182, duration: 0.478s, episode steps:  74, steps per second: 155, episode reward: -89.575, mean reward: -1.210 [-100.000,  6.021], mean action: 1.716 [0.000, 3.000],  loss: 7.540407, mse: 2206.018213, mean_q: 13.973793, mean_eps: 0.897337
  17241/150000: episode: 183, duration: 0.577s, episode steps:  93, steps per second: 161, episode reward: -353.214, mean reward: -3.798 [-100.000,  0.626], mean action: 1.409 [0.000, 3.000],  loss: 11.008638, mse: 2158.504595, mean_q: 14.099939, mean_eps: 0.896836
  17348/150000: episode: 184, duration: 0.670s, episode steps: 107, steps per second: 160, episode reward: -120.597, mean reward: -1.127 [-100.000,  6.489], mean action: 1.533 [0.000, 3.000],  loss: 7.412357, mse: 2171.697245, mean_q: 14.062709, mean_eps: 0.896236
  17419/150000: episode: 185, duration: 0.454s, episode steps:  71, steps per second: 156, episode reward: -80.804, mean reward: -1.138 [-100.000, 19.140], mean action: 1.437 [0.000, 3.000],  loss: 5.970938, mse: 2118.469570, mean_q: 13.600256, mean_eps: 0.895702
  17509/150000: episode: 186, duration: 0.580s, episode steps:  90, steps per second: 155, episode reward: -144.114, mean reward: -1.601 [-100.000,  4.242], mean action: 1.389 [0.000, 3.000],  loss: 10.772568, mse: 2128.493495, mean_q: 13.362290, mean_eps: 0.895219
  17585/150000: episode: 187, duration: 0.500s, episode steps:  76, steps per second: 152, episode reward: -110.969, mean reward: -1.460 [-100.000, 11.280], mean action: 1.566 [0.000, 3.000],  loss: 8.074700, mse: 2117.974402, mean_q: 14.142253, mean_eps: 0.894721
  17679/150000: episode: 188, duration: 0.583s, episode steps:  94, steps per second: 161, episode reward: -84.584, mean reward: -0.900 [-100.000,  7.124], mean action: 1.638 [0.000, 3.000],  loss: 6.753819, mse: 2084.357670, mean_q: 14.145628, mean_eps: 0.894211
  17794/150000: episode: 189, duration: 0.747s, episode steps: 115, steps per second: 154, episode reward: -188.147, mean reward: -1.636 [-100.000, 35.105], mean action: 1.643 [0.000, 3.000],  loss: 8.770001, mse: 2066.464014, mean_q: 14.477868, mean_eps: 0.893584
  17917/150000: episode: 190, duration: 0.803s, episode steps: 123, steps per second: 153, episode reward: -19.493, mean reward: -0.158 [-100.000, 101.091], mean action: 1.675 [0.000, 3.000],  loss: 6.662374, mse: 2075.235122, mean_q: 15.477446, mean_eps: 0.892870
  17980/150000: episode: 191, duration: 0.396s, episode steps:  63, steps per second: 159, episode reward: -97.617, mean reward: -1.549 [-100.000, 81.750], mean action: 1.317 [0.000, 3.000],  loss: 8.493338, mse: 2018.525739, mean_q: 14.135711, mean_eps: 0.892312
  18087/150000: episode: 192, duration: 0.667s, episode steps: 107, steps per second: 160, episode reward: -167.040, mean reward: -1.561 [-100.000,  3.295], mean action: 1.776 [0.000, 3.000],  loss: 10.202956, mse: 2111.238890, mean_q: 15.242040, mean_eps: 0.891802
  18166/150000: episode: 193, duration: 0.497s, episode steps:  79, steps per second: 159, episode reward: -274.730, mean reward: -3.478 [-100.000,  4.713], mean action: 1.532 [0.000, 3.000],  loss: 7.627162, mse: 2079.657516, mean_q: 15.112461, mean_eps: 0.891244
  18270/150000: episode: 194, duration: 0.656s, episode steps: 104, steps per second: 159, episode reward: -112.568, mean reward: -1.082 [-100.000, 15.476], mean action: 1.692 [0.000, 3.000],  loss: 8.323755, mse: 2124.369898, mean_q: 15.289897, mean_eps: 0.890695
  18369/150000: episode: 195, duration: 0.639s, episode steps:  99, steps per second: 155, episode reward: -151.318, mean reward: -1.528 [-100.000,  6.935], mean action: 1.657 [0.000, 3.000],  loss: 7.193453, mse: 2159.965910, mean_q: 17.260091, mean_eps: 0.890086
  18441/150000: episode: 196, duration: 0.515s, episode steps:  72, steps per second: 140, episode reward: -135.120, mean reward: -1.877 [-100.000,  6.604], mean action: 1.569 [0.000, 3.000],  loss: 6.982654, mse: 2150.867179, mean_q: 15.361730, mean_eps: 0.889573
  18530/150000: episode: 197, duration: 0.563s, episode steps:  89, steps per second: 158, episode reward: -110.765, mean reward: -1.245 [-100.000, 10.646], mean action: 1.483 [0.000, 3.000],  loss: 6.975859, mse: 2127.918015, mean_q: 16.519975, mean_eps: 0.889090
  18624/150000: episode: 198, duration: 0.584s, episode steps:  94, steps per second: 161, episode reward: -118.566, mean reward: -1.261 [-100.000,  9.499], mean action: 1.638 [0.000, 3.000],  loss: 6.616398, mse: 2133.987966, mean_q: 16.131255, mean_eps: 0.888541
  18727/150000: episode: 199, duration: 0.637s, episode steps: 103, steps per second: 162, episode reward: -80.312, mean reward: -0.780 [-100.000, 21.801], mean action: 1.398 [0.000, 3.000],  loss: 14.441313, mse: 2166.642573, mean_q: 16.332428, mean_eps: 0.887950
  18834/150000: episode: 200, duration: 0.706s, episode steps: 107, steps per second: 152, episode reward: -214.522, mean reward: -2.005 [-100.000, 11.741], mean action: 1.692 [0.000, 3.000],  loss: 5.075647, mse: 2149.933929, mean_q: 17.780750, mean_eps: 0.887320
  18936/150000: episode: 201, duration: 0.625s, episode steps: 102, steps per second: 163, episode reward: -235.035, mean reward: -2.304 [-100.000,  8.648], mean action: 1.725 [0.000, 3.000],  loss: 9.567122, mse: 2129.169681, mean_q: 16.452086, mean_eps: 0.886693
  19023/150000: episode: 202, duration: 0.537s, episode steps:  87, steps per second: 162, episode reward: -73.007, mean reward: -0.839 [-100.000, 11.248], mean action: 1.575 [0.000, 3.000],  loss: 3.654762, mse: 2135.508468, mean_q: 17.012890, mean_eps: 0.886126
  19136/150000: episode: 203, duration: 0.721s, episode steps: 113, steps per second: 157, episode reward: -163.186, mean reward: -1.444 [-100.000,  5.266], mean action: 1.558 [0.000, 3.000],  loss: 11.254135, mse: 2129.884663, mean_q: 16.074546, mean_eps: 0.885526
  19283/150000: episode: 204, duration: 1.010s, episode steps: 147, steps per second: 146, episode reward: -45.835, mean reward: -0.312 [-100.000,  9.921], mean action: 1.442 [0.000, 3.000],  loss: 11.271972, mse: 2029.056321, mean_q: 16.268843, mean_eps: 0.884746
  19377/150000: episode: 205, duration: 0.756s, episode steps:  94, steps per second: 124, episode reward: -100.381, mean reward: -1.068 [-100.000, 16.056], mean action: 1.660 [0.000, 3.000],  loss: 7.281634, mse: 2051.829357, mean_q: 16.935703, mean_eps: 0.884023
  19478/150000: episode: 206, duration: 0.869s, episode steps: 101, steps per second: 116, episode reward: -110.242, mean reward: -1.092 [-100.000, 18.399], mean action: 1.426 [0.000, 3.000],  loss: 10.338545, mse: 2051.450864, mean_q: 16.255711, mean_eps: 0.883438
  19539/150000: episode: 207, duration: 0.475s, episode steps:  61, steps per second: 128, episode reward: -58.982, mean reward: -0.967 [-100.000, 16.423], mean action: 1.426 [0.000, 3.000],  loss: 8.582745, mse: 2083.266892, mean_q: 16.390832, mean_eps: 0.882952
  19646/150000: episode: 208, duration: 0.695s, episode steps: 107, steps per second: 154, episode reward: -128.266, mean reward: -1.199 [-100.000, 22.986], mean action: 1.542 [0.000, 3.000],  loss: 4.978582, mse: 2041.573303, mean_q: 16.304123, mean_eps: 0.882448
  19771/150000: episode: 209, duration: 0.814s, episode steps: 125, steps per second: 154, episode reward: -55.309, mean reward: -0.442 [-100.000,  6.038], mean action: 1.616 [0.000, 3.000],  loss: 11.513273, mse: 2048.968395, mean_q: 16.382636, mean_eps: 0.881752
  19872/150000: episode: 210, duration: 0.643s, episode steps: 101, steps per second: 157, episode reward: -54.857, mean reward: -0.543 [-100.000, 10.318], mean action: 1.693 [0.000, 3.000],  loss: 9.135894, mse: 2031.152199, mean_q: 17.471925, mean_eps: 0.881074
  20012/150000: episode: 211, duration: 0.873s, episode steps: 140, steps per second: 160, episode reward: -119.787, mean reward: -0.856 [-100.000,  6.450], mean action: 1.321 [0.000, 3.000],  loss: 6.839958, mse: 2026.050667, mean_q: 18.855403, mean_eps: 0.880351
  20080/150000: episode: 212, duration: 0.458s, episode steps:  68, steps per second: 148, episode reward: -50.994, mean reward: -0.750 [-100.000, 17.764], mean action: 1.603 [0.000, 3.000],  loss: 8.577533, mse: 2037.122693, mean_q: 18.967550, mean_eps: 0.879727
  20157/150000: episode: 213, duration: 0.486s, episode steps:  77, steps per second: 159, episode reward: -122.601, mean reward: -1.592 [-100.000,  8.451], mean action: 1.571 [0.000, 3.000],  loss: 5.640749, mse: 2004.984126, mean_q: 17.050207, mean_eps: 0.879292
  20248/150000: episode: 214, duration: 0.558s, episode steps:  91, steps per second: 163, episode reward: -83.621, mean reward: -0.919 [-100.000, 17.166], mean action: 1.560 [0.000, 3.000],  loss: 9.273830, mse: 2036.366164, mean_q: 17.823490, mean_eps: 0.878788
  20353/150000: episode: 215, duration: 0.646s, episode steps: 105, steps per second: 163, episode reward: -84.297, mean reward: -0.803 [-100.000, 19.102], mean action: 1.324 [0.000, 3.000],  loss: 5.995778, mse: 2019.889718, mean_q: 18.086431, mean_eps: 0.878200
  20432/150000: episode: 216, duration: 0.528s, episode steps:  79, steps per second: 150, episode reward: -68.978, mean reward: -0.873 [-100.000, 17.948], mean action: 1.380 [0.000, 3.000],  loss: 6.017587, mse: 2029.761122, mean_q: 17.672499, mean_eps: 0.877648
  20507/150000: episode: 217, duration: 0.465s, episode steps:  75, steps per second: 161, episode reward: -108.221, mean reward: -1.443 [-100.000, 10.595], mean action: 1.360 [0.000, 3.000],  loss: 11.214689, mse: 2029.168577, mean_q: 17.364795, mean_eps: 0.877186
  20622/150000: episode: 218, duration: 0.700s, episode steps: 115, steps per second: 164, episode reward: -228.033, mean reward: -1.983 [-100.000,  7.413], mean action: 1.452 [0.000, 3.000],  loss: 8.572947, mse: 2133.682967, mean_q: 17.343463, mean_eps: 0.876616
  20700/150000: episode: 219, duration: 0.486s, episode steps:  78, steps per second: 161, episode reward: -53.452, mean reward: -0.685 [-100.000, 25.979], mean action: 1.577 [0.000, 3.000],  loss: 6.443497, mse: 2162.029796, mean_q: 16.479238, mean_eps: 0.876037
  20815/150000: episode: 220, duration: 0.800s, episode steps: 115, steps per second: 144, episode reward: -135.938, mean reward: -1.182 [-100.000, 23.567], mean action: 1.600 [0.000, 3.000],  loss: 9.324108, mse: 2156.696835, mean_q: 18.459921, mean_eps: 0.875458
  20903/150000: episode: 221, duration: 0.556s, episode steps:  88, steps per second: 158, episode reward: -108.887, mean reward: -1.237 [-100.000, 12.278], mean action: 1.341 [0.000, 3.000],  loss: 11.971050, mse: 2073.337124, mean_q: 19.004972, mean_eps: 0.874849
  21009/150000: episode: 222, duration: 0.654s, episode steps: 106, steps per second: 162, episode reward: -132.038, mean reward: -1.246 [-100.000, 11.327], mean action: 1.387 [0.000, 3.000],  loss: 7.727819, mse: 2133.959277, mean_q: 18.191969, mean_eps: 0.874267
  21078/150000: episode: 223, duration: 0.469s, episode steps:  69, steps per second: 147, episode reward: -67.224, mean reward: -0.974 [-100.000,  9.893], mean action: 1.406 [0.000, 3.000],  loss: 10.150897, mse: 2157.552027, mean_q: 18.765316, mean_eps: 0.873742
  21192/150000: episode: 224, duration: 0.713s, episode steps: 114, steps per second: 160, episode reward: -219.950, mean reward: -1.929 [-100.000,  5.240], mean action: 1.596 [0.000, 3.000],  loss: 9.096386, mse: 2170.185497, mean_q: 18.004696, mean_eps: 0.873193
  21257/150000: episode: 225, duration: 0.398s, episode steps:  65, steps per second: 163, episode reward: -78.178, mean reward: -1.203 [-100.000,  8.309], mean action: 1.615 [0.000, 3.000],  loss: 4.283867, mse: 2155.550652, mean_q: 17.542135, mean_eps: 0.872656
  21332/150000: episode: 226, duration: 0.503s, episode steps:  75, steps per second: 149, episode reward: -126.863, mean reward: -1.692 [-100.000,  7.294], mean action: 1.493 [0.000, 3.000],  loss: 8.302063, mse: 2059.852217, mean_q: 17.025630, mean_eps: 0.872236
  21460/150000: episode: 227, duration: 1.027s, episode steps: 128, steps per second: 125, episode reward: -80.545, mean reward: -0.629 [-100.000, 20.723], mean action: 1.445 [0.000, 3.000],  loss: 10.005597, mse: 2098.787712, mean_q: 18.608255, mean_eps: 0.871627
  21567/150000: episode: 228, duration: 0.744s, episode steps: 107, steps per second: 144, episode reward: -111.000, mean reward: -1.037 [-100.000,  9.152], mean action: 1.579 [0.000, 3.000],  loss: 9.318069, mse: 2113.043605, mean_q: 18.888670, mean_eps: 0.870922
  21665/150000: episode: 229, duration: 0.787s, episode steps:  98, steps per second: 125, episode reward: -148.178, mean reward: -1.512 [-100.000, 13.700], mean action: 1.327 [0.000, 3.000],  loss: 6.562007, mse: 2135.327501, mean_q: 19.048392, mean_eps: 0.870307
  21724/150000: episode: 230, duration: 0.450s, episode steps:  59, steps per second: 131, episode reward: -89.597, mean reward: -1.519 [-100.000,  9.341], mean action: 1.780 [0.000, 3.000],  loss: 9.659466, mse: 2112.890172, mean_q: 19.042524, mean_eps: 0.869836
  21853/150000: episode: 231, duration: 0.912s, episode steps: 129, steps per second: 141, episode reward: -140.403, mean reward: -1.088 [-100.000,  6.564], mean action: 1.589 [0.000, 3.000],  loss: 7.128609, mse: 2169.616146, mean_q: 20.960667, mean_eps: 0.869272
  21939/150000: episode: 232, duration: 0.571s, episode steps:  86, steps per second: 151, episode reward: -113.268, mean reward: -1.317 [-100.000,  9.770], mean action: 1.488 [0.000, 3.000],  loss: 6.147390, mse: 2082.569738, mean_q: 22.414056, mean_eps: 0.868627
  22045/150000: episode: 233, duration: 0.753s, episode steps: 106, steps per second: 141, episode reward: -233.329, mean reward: -2.201 [-100.000,  6.626], mean action: 1.509 [0.000, 3.000],  loss: 9.147433, mse: 2090.285618, mean_q: 21.464604, mean_eps: 0.868051
  22127/150000: episode: 234, duration: 0.526s, episode steps:  82, steps per second: 156, episode reward: -276.687, mean reward: -3.374 [-100.000, 115.633], mean action: 1.451 [0.000, 3.000],  loss: 10.164704, mse: 2141.349344, mean_q: 21.284016, mean_eps: 0.867487
  22229/150000: episode: 235, duration: 0.626s, episode steps: 102, steps per second: 163, episode reward: -34.897, mean reward: -0.342 [-100.000, 13.284], mean action: 1.490 [0.000, 3.000],  loss: 7.477009, mse: 2139.068081, mean_q: 21.277332, mean_eps: 0.866935
  22294/150000: episode: 236, duration: 0.396s, episode steps:  65, steps per second: 164, episode reward: -91.924, mean reward: -1.414 [-100.000,  8.098], mean action: 1.538 [0.000, 3.000],  loss: 10.866772, mse: 2184.050830, mean_q: 21.162105, mean_eps: 0.866434
  22417/150000: episode: 237, duration: 0.815s, episode steps: 123, steps per second: 151, episode reward: -71.741, mean reward: -0.583 [-100.000, 17.811], mean action: 1.561 [0.000, 3.000],  loss: 9.287108, mse: 2179.931848, mean_q: 22.533014, mean_eps: 0.865870
  22494/150000: episode: 238, duration: 0.485s, episode steps:  77, steps per second: 159, episode reward: -110.581, mean reward: -1.436 [-100.000,  6.678], mean action: 1.623 [0.000, 3.000],  loss: 22.278926, mse: 2091.037168, mean_q: 22.395482, mean_eps: 0.865270
  22590/150000: episode: 239, duration: 0.609s, episode steps:  96, steps per second: 158, episode reward: -96.365, mean reward: -1.004 [-100.000,  9.249], mean action: 1.365 [0.000, 3.000],  loss: 7.647849, mse: 2234.334394, mean_q: 23.165295, mean_eps: 0.864751
  22662/150000: episode: 240, duration: 0.516s, episode steps:  72, steps per second: 139, episode reward: -90.678, mean reward: -1.259 [-100.000, 11.379], mean action: 1.472 [0.000, 3.000],  loss: 6.696164, mse: 2235.658798, mean_q: 22.308548, mean_eps: 0.864247
  22778/150000: episode: 241, duration: 0.754s, episode steps: 116, steps per second: 154, episode reward: -129.272, mean reward: -1.114 [-100.000,  6.860], mean action: 1.681 [0.000, 3.000],  loss: 8.110917, mse: 2252.708556, mean_q: 24.095323, mean_eps: 0.863683
  22887/150000: episode: 242, duration: 0.703s, episode steps: 109, steps per second: 155, episode reward: -239.158, mean reward: -2.194 [-100.000,  4.086], mean action: 1.422 [0.000, 3.000],  loss: 7.750520, mse: 2278.022879, mean_q: 25.029985, mean_eps: 0.863008
  23031/150000: episode: 243, duration: 1.010s, episode steps: 144, steps per second: 143, episode reward: -93.922, mean reward: -0.652 [-100.000, 19.410], mean action: 1.632 [0.000, 3.000],  loss: 7.301923, mse: 2371.924303, mean_q: 26.975475, mean_eps: 0.862249
  23125/150000: episode: 244, duration: 0.705s, episode steps:  94, steps per second: 133, episode reward: -75.881, mean reward: -0.807 [-100.000,  7.366], mean action: 1.543 [0.000, 3.000],  loss: 5.398854, mse: 2464.780360, mean_q: 25.591483, mean_eps: 0.861535
  23222/150000: episode: 245, duration: 0.683s, episode steps:  97, steps per second: 142, episode reward: -110.774, mean reward: -1.142 [-100.000, 26.580], mean action: 1.691 [0.000, 3.000],  loss: 5.270065, mse: 2481.659934, mean_q: 26.890911, mean_eps: 0.860962
  23308/150000: episode: 246, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -117.921, mean reward: -1.371 [-100.000,  6.811], mean action: 1.488 [0.000, 3.000],  loss: 4.376030, mse: 2496.649008, mean_q: 26.708100, mean_eps: 0.860413
  23436/150000: episode: 247, duration: 0.814s, episode steps: 128, steps per second: 157, episode reward: -117.441, mean reward: -0.918 [-100.000, 23.940], mean action: 1.703 [0.000, 3.000],  loss: 6.507661, mse: 2483.597651, mean_q: 26.057074, mean_eps: 0.859771
  23564/150000: episode: 248, duration: 0.796s, episode steps: 128, steps per second: 161, episode reward: -104.905, mean reward: -0.820 [-100.000,  5.413], mean action: 1.531 [0.000, 3.000],  loss: 6.662698, mse: 2436.489796, mean_q: 24.772495, mean_eps: 0.859003
  23639/150000: episode: 249, duration: 0.495s, episode steps:  75, steps per second: 151, episode reward: -45.334, mean reward: -0.604 [-100.000, 13.014], mean action: 1.693 [0.000, 3.000],  loss: 7.930805, mse: 2485.709437, mean_q: 24.612630, mean_eps: 0.858394
  23741/150000: episode: 250, duration: 0.632s, episode steps: 102, steps per second: 161, episode reward: -117.916, mean reward: -1.156 [-100.000,  8.317], mean action: 1.676 [0.000, 3.000],  loss: 5.793595, mse: 2483.230517, mean_q: 24.661644, mean_eps: 0.857863
  23814/150000: episode: 251, duration: 0.461s, episode steps:  73, steps per second: 158, episode reward: -108.140, mean reward: -1.481 [-100.000,  8.901], mean action: 1.521 [0.000, 3.000],  loss: 7.556128, mse: 2401.304890, mean_q: 24.871603, mean_eps: 0.857338
  23928/150000: episode: 252, duration: 0.759s, episode steps: 114, steps per second: 150, episode reward: -88.282, mean reward: -0.774 [-100.000,  8.447], mean action: 1.518 [0.000, 3.000],  loss: 7.642856, mse: 2437.929384, mean_q: 23.834829, mean_eps: 0.856777
  24041/150000: episode: 253, duration: 0.732s, episode steps: 113, steps per second: 154, episode reward: -90.846, mean reward: -0.804 [-100.000,  7.320], mean action: 1.522 [0.000, 3.000],  loss: 8.331507, mse: 2351.830924, mean_q: 23.082986, mean_eps: 0.856096
  24163/150000: episode: 254, duration: 0.750s, episode steps: 122, steps per second: 163, episode reward: -63.007, mean reward: -0.516 [-100.000, 45.205], mean action: 1.500 [0.000, 3.000],  loss: 6.937915, mse: 2318.253116, mean_q: 22.677176, mean_eps: 0.855391
  24261/150000: episode: 255, duration: 0.637s, episode steps:  98, steps per second: 154, episode reward: -64.991, mean reward: -0.663 [-100.000, 52.005], mean action: 1.439 [0.000, 3.000],  loss: 7.986110, mse: 2361.352331, mean_q: 24.327713, mean_eps: 0.854731
  24349/150000: episode: 256, duration: 0.559s, episode steps:  88, steps per second: 157, episode reward: -312.813, mean reward: -3.555 [-100.000,  0.432], mean action: 1.591 [0.000, 3.000],  loss: 7.267863, mse: 2380.515070, mean_q: 25.405482, mean_eps: 0.854173
  24473/150000: episode: 257, duration: 0.761s, episode steps: 124, steps per second: 163, episode reward: -114.408, mean reward: -0.923 [-100.000, 11.489], mean action: 1.516 [0.000, 3.000],  loss: 8.803345, mse: 2377.796464, mean_q: 25.308278, mean_eps: 0.853537
  24551/150000: episode: 258, duration: 0.493s, episode steps:  78, steps per second: 158, episode reward: -153.200, mean reward: -1.964 [-100.000,  8.053], mean action: 1.436 [0.000, 3.000],  loss: 6.070402, mse: 2426.003554, mean_q: 25.004221, mean_eps: 0.852931
  24661/150000: episode: 259, duration: 0.724s, episode steps: 110, steps per second: 152, episode reward: -95.111, mean reward: -0.865 [-100.000,  7.736], mean action: 1.664 [0.000, 3.000],  loss: 6.788078, mse: 2477.712501, mean_q: 24.982303, mean_eps: 0.852367
  24779/150000: episode: 260, duration: 0.725s, episode steps: 118, steps per second: 163, episode reward: -91.249, mean reward: -0.773 [-100.000, 11.528], mean action: 1.568 [0.000, 3.000],  loss: 8.646840, mse: 2489.473229, mean_q: 25.583874, mean_eps: 0.851683
  24857/150000: episode: 261, duration: 0.512s, episode steps:  78, steps per second: 152, episode reward: -76.279, mean reward: -0.978 [-100.000,  7.276], mean action: 1.577 [0.000, 3.000],  loss: 7.926444, mse: 2503.075948, mean_q: 24.816821, mean_eps: 0.851095
  24968/150000: episode: 262, duration: 0.836s, episode steps: 111, steps per second: 133, episode reward: -95.899, mean reward: -0.864 [-100.000, 11.023], mean action: 1.559 [0.000, 3.000],  loss: 7.192986, mse: 2468.103769, mean_q: 25.918212, mean_eps: 0.850528
  25073/150000: episode: 263, duration: 0.673s, episode steps: 105, steps per second: 156, episode reward: -148.172, mean reward: -1.411 [-100.000,  4.984], mean action: 1.438 [0.000, 3.000],  loss: 8.963705, mse: 2434.531005, mean_q: 25.677473, mean_eps: 0.849880
  25210/150000: episode: 264, duration: 0.844s, episode steps: 137, steps per second: 162, episode reward: -182.137, mean reward: -1.329 [-100.000,  6.797], mean action: 1.460 [0.000, 3.000],  loss: 4.613127, mse: 2458.034933, mean_q: 24.982900, mean_eps: 0.849154
  25284/150000: episode: 265, duration: 0.499s, episode steps:  74, steps per second: 148, episode reward: -203.747, mean reward: -2.753 [-100.000, 22.765], mean action: 1.554 [0.000, 3.000],  loss: 17.811665, mse: 2481.040663, mean_q: 25.889003, mean_eps: 0.848521
  25351/150000: episode: 266, duration: 0.431s, episode steps:  67, steps per second: 155, episode reward: -88.838, mean reward: -1.326 [-100.000, 14.036], mean action: 1.478 [0.000, 3.000],  loss: 10.104886, mse: 2592.921511, mean_q: 25.549638, mean_eps: 0.848098
  25459/150000: episode: 267, duration: 0.680s, episode steps: 108, steps per second: 159, episode reward: -131.936, mean reward: -1.222 [-100.000,  6.818], mean action: 1.454 [0.000, 3.000],  loss: 9.182646, mse: 2574.862032, mean_q: 26.770709, mean_eps: 0.847573
  25590/150000: episode: 268, duration: 0.878s, episode steps: 131, steps per second: 149, episode reward: -77.078, mean reward: -0.588 [-100.000,  9.614], mean action: 1.634 [0.000, 3.000],  loss: 12.176181, mse: 2523.066229, mean_q: 26.376620, mean_eps: 0.846856
  25654/150000: episode: 269, duration: 0.433s, episode steps:  64, steps per second: 148, episode reward: -114.545, mean reward: -1.790 [-100.000,  6.125], mean action: 1.406 [0.000, 3.000],  loss: 9.192329, mse: 2460.488947, mean_q: 26.366383, mean_eps: 0.846271
  25744/150000: episode: 270, duration: 0.564s, episode steps:  90, steps per second: 160, episode reward: -112.962, mean reward: -1.255 [-100.000, 10.947], mean action: 1.600 [0.000, 3.000],  loss: 10.115988, mse: 2470.152851, mean_q: 25.854534, mean_eps: 0.845809
  25848/150000: episode: 271, duration: 0.648s, episode steps: 104, steps per second: 160, episode reward: -119.734, mean reward: -1.151 [-100.000, 11.652], mean action: 1.558 [0.000, 3.000],  loss: 10.520052, mse: 2490.134325, mean_q: 27.317915, mean_eps: 0.845227
  25938/150000: episode: 272, duration: 0.597s, episode steps:  90, steps per second: 151, episode reward: -65.154, mean reward: -0.724 [-100.000, 16.561], mean action: 1.822 [0.000, 3.000],  loss: 7.013475, mse: 2461.803044, mean_q: 25.036342, mean_eps: 0.844645
  26048/150000: episode: 273, duration: 0.701s, episode steps: 110, steps per second: 157, episode reward: -131.600, mean reward: -1.196 [-100.000,  6.669], mean action: 1.682 [0.000, 3.000],  loss: 11.050948, mse: 2488.806072, mean_q: 26.462534, mean_eps: 0.844045
  26149/150000: episode: 274, duration: 0.640s, episode steps: 101, steps per second: 158, episode reward: -181.717, mean reward: -1.799 [-100.000, 16.770], mean action: 1.604 [0.000, 3.000],  loss: 7.267807, mse: 2515.594474, mean_q: 24.992284, mean_eps: 0.843412
  26246/150000: episode: 275, duration: 0.612s, episode steps:  97, steps per second: 158, episode reward: -174.514, mean reward: -1.799 [-100.000, 50.063], mean action: 1.237 [0.000, 3.000],  loss: 10.839185, mse: 2520.636136, mean_q: 25.458357, mean_eps: 0.842818
  26326/150000: episode: 276, duration: 0.537s, episode steps:  80, steps per second: 149, episode reward: -103.837, mean reward: -1.298 [-100.000,  5.135], mean action: 1.425 [0.000, 3.000],  loss: 6.972690, mse: 2497.673500, mean_q: 26.805908, mean_eps: 0.842287
  26455/150000: episode: 277, duration: 0.826s, episode steps: 129, steps per second: 156, episode reward: -262.307, mean reward: -2.033 [-100.000, 18.596], mean action: 1.326 [0.000, 3.000],  loss: 7.985709, mse: 2563.061485, mean_q: 26.354372, mean_eps: 0.841660
  26568/150000: episode: 278, duration: 0.697s, episode steps: 113, steps per second: 162, episode reward: -194.235, mean reward: -1.719 [-100.000, 68.455], mean action: 1.327 [0.000, 3.000],  loss: 6.333139, mse: 2566.317339, mean_q: 25.489582, mean_eps: 0.840934
  26671/150000: episode: 279, duration: 0.692s, episode steps: 103, steps per second: 149, episode reward: -120.116, mean reward: -1.166 [-100.000, 15.036], mean action: 1.515 [0.000, 3.000],  loss: 5.961554, mse: 2461.584202, mean_q: 26.083652, mean_eps: 0.840286
  26756/150000: episode: 280, duration: 0.550s, episode steps:  85, steps per second: 154, episode reward: -128.299, mean reward: -1.509 [-100.000,  9.122], mean action: 1.624 [0.000, 3.000],  loss: 7.766837, mse: 2462.147309, mean_q: 25.814882, mean_eps: 0.839722
  26849/150000: episode: 281, duration: 0.578s, episode steps:  93, steps per second: 161, episode reward: -97.217, mean reward: -1.045 [-100.000, 17.718], mean action: 1.581 [0.000, 3.000],  loss: 5.239529, mse: 2449.302461, mean_q: 26.718339, mean_eps: 0.839188
  26963/150000: episode: 282, duration: 0.775s, episode steps: 114, steps per second: 147, episode reward: -68.838, mean reward: -0.604 [-100.000, 44.611], mean action: 1.526 [0.000, 3.000],  loss: 11.991738, mse: 2525.324605, mean_q: 25.041533, mean_eps: 0.838567
  27087/150000: episode: 283, duration: 0.813s, episode steps: 124, steps per second: 153, episode reward: -175.154, mean reward: -1.413 [-100.000, 11.804], mean action: 1.621 [0.000, 3.000],  loss: 7.346591, mse: 2488.404787, mean_q: 27.229108, mean_eps: 0.837853
  27195/150000: episode: 284, duration: 0.673s, episode steps: 108, steps per second: 160, episode reward: -129.004, mean reward: -1.194 [-100.000, 16.419], mean action: 1.565 [0.000, 3.000],  loss: 9.713584, mse: 2571.888073, mean_q: 26.556853, mean_eps: 0.837157
  27315/150000: episode: 285, duration: 0.785s, episode steps: 120, steps per second: 153, episode reward: -84.164, mean reward: -0.701 [-100.000, 16.274], mean action: 1.467 [0.000, 3.000],  loss: 8.632859, mse: 2431.887623, mean_q: 26.511581, mean_eps: 0.836473
  27429/150000: episode: 286, duration: 0.736s, episode steps: 114, steps per second: 155, episode reward: -112.041, mean reward: -0.983 [-100.000, 21.134], mean action: 1.518 [0.000, 3.000],  loss: 8.132878, mse: 2435.310381, mean_q: 25.151050, mean_eps: 0.835771
  27551/150000: episode: 287, duration: 0.788s, episode steps: 122, steps per second: 155, episode reward: -103.883, mean reward: -0.851 [-100.000,  9.888], mean action: 1.598 [0.000, 3.000],  loss: 7.606570, mse: 2431.721307, mean_q: 25.022943, mean_eps: 0.835063
  27635/150000: episode: 288, duration: 0.568s, episode steps:  84, steps per second: 148, episode reward: -73.473, mean reward: -0.875 [-100.000,  6.470], mean action: 1.726 [0.000, 3.000],  loss: 9.814440, mse: 2507.785427, mean_q: 24.500817, mean_eps: 0.834445
  27776/150000: episode: 289, duration: 0.903s, episode steps: 141, steps per second: 156, episode reward: -65.194, mean reward: -0.462 [-100.000,  8.069], mean action: 1.652 [0.000, 3.000],  loss: 8.251681, mse: 2513.453727, mean_q: 24.968083, mean_eps: 0.833770
  27884/150000: episode: 290, duration: 0.675s, episode steps: 108, steps per second: 160, episode reward: -120.560, mean reward: -1.116 [-100.000, 15.544], mean action: 1.537 [0.000, 3.000],  loss: 6.810213, mse: 2523.875787, mean_q: 25.206056, mean_eps: 0.833023
  27998/150000: episode: 291, duration: 0.750s, episode steps: 114, steps per second: 152, episode reward: -97.329, mean reward: -0.854 [-100.000, 16.216], mean action: 1.561 [0.000, 3.000],  loss: 8.848157, mse: 2589.934886, mean_q: 24.684160, mean_eps: 0.832357
  28098/150000: episode: 292, duration: 0.624s, episode steps: 100, steps per second: 160, episode reward: -282.846, mean reward: -2.828 [-100.000,  6.893], mean action: 1.670 [0.000, 3.000],  loss: 5.823679, mse: 2501.177393, mean_q: 24.369317, mean_eps: 0.831715
  28204/150000: episode: 293, duration: 0.671s, episode steps: 106, steps per second: 158, episode reward: -125.642, mean reward: -1.185 [-100.000, 16.228], mean action: 1.462 [0.000, 3.000],  loss: 4.331264, mse: 2468.205026, mean_q: 25.749738, mean_eps: 0.831097
  28349/150000: episode: 294, duration: 0.961s, episode steps: 145, steps per second: 151, episode reward: -99.910, mean reward: -0.689 [-100.000, 11.804], mean action: 1.641 [0.000, 3.000],  loss: 5.882547, mse: 2466.513681, mean_q: 23.211855, mean_eps: 0.830344
  28454/150000: episode: 295, duration: 0.656s, episode steps: 105, steps per second: 160, episode reward: -119.270, mean reward: -1.136 [-100.000, 12.975], mean action: 1.695 [0.000, 3.000],  loss: 10.941260, mse: 2501.309225, mean_q: 24.718332, mean_eps: 0.829594
  28539/150000: episode: 296, duration: 0.543s, episode steps:  85, steps per second: 157, episode reward: -132.959, mean reward: -1.564 [-100.000,  8.629], mean action: 1.600 [0.000, 3.000],  loss: 4.691254, mse: 2434.274061, mean_q: 25.808958, mean_eps: 0.829024
  28612/150000: episode: 297, duration: 0.480s, episode steps:  73, steps per second: 152, episode reward: -114.924, mean reward: -1.574 [-100.000,  8.895], mean action: 1.507 [0.000, 3.000],  loss: 5.353075, mse: 2422.282778, mean_q: 26.412754, mean_eps: 0.828550
  28692/150000: episode: 298, duration: 0.528s, episode steps:  80, steps per second: 152, episode reward: -112.605, mean reward: -1.408 [-100.000,  9.205], mean action: 1.613 [0.000, 3.000],  loss: 5.789636, mse: 2418.300848, mean_q: 25.019497, mean_eps: 0.828091
  28782/150000: episode: 299, duration: 0.678s, episode steps:  90, steps per second: 133, episode reward: -112.534, mean reward: -1.250 [-100.000,  7.600], mean action: 1.467 [0.000, 3.000],  loss: 4.789431, mse: 2417.103004, mean_q: 25.316545, mean_eps: 0.827581
  28842/150000: episode: 300, duration: 0.436s, episode steps:  60, steps per second: 137, episode reward: -121.723, mean reward: -2.029 [-100.000, 18.119], mean action: 1.333 [0.000, 3.000],  loss: 5.050841, mse: 2448.147760, mean_q: 24.693963, mean_eps: 0.827131
  28954/150000: episode: 301, duration: 0.757s, episode steps: 112, steps per second: 148, episode reward: -187.578, mean reward: -1.675 [-100.000,  5.513], mean action: 1.723 [0.000, 3.000],  loss: 5.936631, mse: 2449.893542, mean_q: 24.291886, mean_eps: 0.826615
  29053/150000: episode: 302, duration: 0.629s, episode steps:  99, steps per second: 157, episode reward: -54.562, mean reward: -0.551 [-100.000, 18.590], mean action: 1.485 [0.000, 3.000],  loss: 7.238881, mse: 2477.924689, mean_q: 25.893669, mean_eps: 0.825982
  29174/150000: episode: 303, duration: 0.763s, episode steps: 121, steps per second: 159, episode reward: -217.762, mean reward: -1.800 [-100.000, 36.695], mean action: 1.562 [0.000, 3.000],  loss: 5.620058, mse: 2499.048422, mean_q: 25.297686, mean_eps: 0.825322
  29290/150000: episode: 304, duration: 0.766s, episode steps: 116, steps per second: 151, episode reward: -49.162, mean reward: -0.424 [-100.000, 17.977], mean action: 1.560 [0.000, 3.000],  loss: 8.575189, mse: 2560.811373, mean_q: 25.989302, mean_eps: 0.824611
  29405/150000: episode: 305, duration: 0.720s, episode steps: 115, steps per second: 160, episode reward: -71.518, mean reward: -0.622 [-100.000,  7.516], mean action: 1.591 [0.000, 3.000],  loss: 9.960540, mse: 2569.945569, mean_q: 25.156833, mean_eps: 0.823918
  29482/150000: episode: 306, duration: 0.483s, episode steps:  77, steps per second: 159, episode reward: -51.017, mean reward: -0.663 [-100.000, 10.482], mean action: 1.727 [0.000, 3.000],  loss: 6.354770, mse: 2633.777046, mean_q: 26.737661, mean_eps: 0.823342
  29586/150000: episode: 307, duration: 0.687s, episode steps: 104, steps per second: 151, episode reward: -9.098, mean reward: -0.087 [-100.000, 103.306], mean action: 1.529 [0.000, 3.000],  loss: 7.957241, mse: 2688.851781, mean_q: 25.547574, mean_eps: 0.822799
  29702/150000: episode: 308, duration: 0.748s, episode steps: 116, steps per second: 155, episode reward: -142.711, mean reward: -1.230 [-100.000, 26.621], mean action: 1.431 [0.000, 3.000],  loss: 5.876526, mse: 2615.963265, mean_q: 25.208694, mean_eps: 0.822139
  29787/150000: episode: 309, duration: 0.531s, episode steps:  85, steps per second: 160, episode reward: -153.330, mean reward: -1.804 [-100.000, 12.871], mean action: 1.824 [0.000, 3.000],  loss: 10.399375, mse: 2587.908295, mean_q: 25.998002, mean_eps: 0.821536
  29862/150000: episode: 310, duration: 0.472s, episode steps:  75, steps per second: 159, episode reward: -79.804, mean reward: -1.064 [-100.000,  7.426], mean action: 1.640 [0.000, 3.000],  loss: 4.781218, mse: 2660.609779, mean_q: 26.481913, mean_eps: 0.821056
  29975/150000: episode: 311, duration: 0.751s, episode steps: 113, steps per second: 151, episode reward: -116.524, mean reward: -1.031 [-100.000,  7.439], mean action: 1.496 [0.000, 3.000],  loss: 6.401661, mse: 2603.520125, mean_q: 28.224640, mean_eps: 0.820492
  30046/150000: episode: 312, duration: 0.455s, episode steps:  71, steps per second: 156, episode reward: -64.169, mean reward: -0.904 [-100.000, 10.237], mean action: 1.662 [0.000, 3.000],  loss: 7.567174, mse: 2528.199104, mean_q: 25.971374, mean_eps: 0.819940
  30132/150000: episode: 313, duration: 0.576s, episode steps:  86, steps per second: 149, episode reward: -63.264, mean reward: -0.736 [-100.000,  7.504], mean action: 1.477 [0.000, 3.000],  loss: 9.979740, mse: 2516.024963, mean_q: 25.488701, mean_eps: 0.819469
  30238/150000: episode: 314, duration: 0.707s, episode steps: 106, steps per second: 150, episode reward: -170.294, mean reward: -1.607 [-100.000,  7.025], mean action: 1.585 [0.000, 3.000],  loss: 5.354052, mse: 2558.090219, mean_q: 26.150057, mean_eps: 0.818893
  30335/150000: episode: 315, duration: 0.634s, episode steps:  97, steps per second: 153, episode reward: -190.145, mean reward: -1.960 [-100.000,  9.702], mean action: 1.515 [0.000, 3.000],  loss: 4.478609, mse: 2606.946757, mean_q: 25.658238, mean_eps: 0.818284
  30424/150000: episode: 316, duration: 0.556s, episode steps:  89, steps per second: 160, episode reward: -90.589, mean reward: -1.018 [-100.000,  7.871], mean action: 1.416 [0.000, 3.000],  loss: 7.722589, mse: 2621.732172, mean_q: 26.376581, mean_eps: 0.817726
  30507/150000: episode: 317, duration: 0.527s, episode steps:  83, steps per second: 157, episode reward: -136.976, mean reward: -1.650 [-100.000, 13.738], mean action: 1.566 [0.000, 3.000],  loss: 9.611418, mse: 2599.284221, mean_q: 26.000342, mean_eps: 0.817210
  30626/150000: episode: 318, duration: 0.827s, episode steps: 119, steps per second: 144, episode reward: -124.719, mean reward: -1.048 [-100.000,  6.678], mean action: 1.597 [0.000, 3.000],  loss: 10.026646, mse: 2620.752089, mean_q: 26.349839, mean_eps: 0.816604
  30750/150000: episode: 319, duration: 0.786s, episode steps: 124, steps per second: 158, episode reward: -40.581, mean reward: -0.327 [-100.000, 55.777], mean action: 1.581 [0.000, 3.000],  loss: 7.046394, mse: 2484.079433, mean_q: 25.720705, mean_eps: 0.815875
  30809/150000: episode: 320, duration: 0.373s, episode steps:  59, steps per second: 158, episode reward: -60.208, mean reward: -1.020 [-100.000, 55.427], mean action: 1.508 [0.000, 3.000],  loss: 13.187129, mse: 2543.359938, mean_q: 25.918045, mean_eps: 0.815326
  30910/150000: episode: 321, duration: 0.658s, episode steps: 101, steps per second: 154, episode reward: -93.328, mean reward: -0.924 [-100.000,  7.978], mean action: 1.693 [0.000, 3.000],  loss: 14.500871, mse: 2582.150021, mean_q: 26.729886, mean_eps: 0.814846
  31037/150000: episode: 322, duration: 0.832s, episode steps: 127, steps per second: 153, episode reward: -78.901, mean reward: -0.621 [-100.000, 16.961], mean action: 1.449 [0.000, 3.000],  loss: 13.144652, mse: 2553.657700, mean_q: 25.880576, mean_eps: 0.814162
  31119/150000: episode: 323, duration: 0.534s, episode steps:  82, steps per second: 153, episode reward: -91.052, mean reward: -1.110 [-100.000, 13.205], mean action: 1.537 [0.000, 3.000],  loss: 9.177640, mse: 2572.398031, mean_q: 26.538897, mean_eps: 0.813535
  31208/150000: episode: 324, duration: 0.558s, episode steps:  89, steps per second: 160, episode reward: -77.343, mean reward: -0.869 [-100.000, 18.464], mean action: 1.708 [0.000, 3.000],  loss: 12.566007, mse: 2500.013356, mean_q: 25.640145, mean_eps: 0.813022
  31281/150000: episode: 325, duration: 0.508s, episode steps:  73, steps per second: 144, episode reward: -67.435, mean reward: -0.924 [-100.000,  8.936], mean action: 1.726 [0.000, 3.000],  loss: 5.111431, mse: 2445.391839, mean_q: 22.237578, mean_eps: 0.812536
  31411/150000: episode: 326, duration: 0.951s, episode steps: 130, steps per second: 137, episode reward: -71.738, mean reward: -0.552 [-100.000,  6.972], mean action: 1.469 [0.000, 3.000],  loss: 9.639783, mse: 2394.085501, mean_q: 25.156060, mean_eps: 0.811927
  31492/150000: episode: 327, duration: 0.659s, episode steps:  81, steps per second: 123, episode reward: -85.169, mean reward: -1.051 [-100.000, 10.961], mean action: 1.420 [0.000, 3.000],  loss: 10.817870, mse: 2420.240656, mean_q: 26.010932, mean_eps: 0.811294
  31565/150000: episode: 328, duration: 0.619s, episode steps:  73, steps per second: 118, episode reward: -55.322, mean reward: -0.758 [-100.000, 14.626], mean action: 1.425 [0.000, 3.000],  loss: 23.122608, mse: 2459.306343, mean_q: 26.043290, mean_eps: 0.810832
  31710/150000: episode: 329, duration: 1.116s, episode steps: 145, steps per second: 130, episode reward: -76.790, mean reward: -0.530 [-100.000, 11.701], mean action: 1.490 [0.000, 3.000],  loss: 16.364452, mse: 2428.052489, mean_q: 26.081346, mean_eps: 0.810178
  31823/150000: episode: 330, duration: 0.844s, episode steps: 113, steps per second: 134, episode reward: -113.544, mean reward: -1.005 [-100.000, 24.439], mean action: 1.513 [0.000, 3.000],  loss: 11.429897, mse: 2360.031898, mean_q: 24.199706, mean_eps: 0.809404
  31950/150000: episode: 331, duration: 0.936s, episode steps: 127, steps per second: 136, episode reward: -88.921, mean reward: -0.700 [-100.000, 12.040], mean action: 1.543 [0.000, 3.000],  loss: 7.594291, mse: 2377.209249, mean_q: 24.751112, mean_eps: 0.808684
  32033/150000: episode: 332, duration: 0.539s, episode steps:  83, steps per second: 154, episode reward: -70.698, mean reward: -0.852 [-100.000,  7.695], mean action: 1.602 [0.000, 3.000],  loss: 12.845632, mse: 2350.975083, mean_q: 24.994151, mean_eps: 0.808054
  32143/150000: episode: 333, duration: 0.730s, episode steps: 110, steps per second: 151, episode reward: -94.728, mean reward: -0.861 [-100.000,  5.937], mean action: 1.473 [0.000, 3.000],  loss: 13.338333, mse: 2374.205428, mean_q: 25.623784, mean_eps: 0.807475
  32312/150000: episode: 334, duration: 1.096s, episode steps: 169, steps per second: 154, episode reward: 19.345, mean reward:  0.114 [-100.000, 78.864], mean action: 1.533 [0.000, 3.000],  loss: 8.732711, mse: 2371.736501, mean_q: 25.638095, mean_eps: 0.806638
  32453/150000: episode: 335, duration: 0.910s, episode steps: 141, steps per second: 155, episode reward: -68.452, mean reward: -0.485 [-100.000, 44.038], mean action: 1.532 [0.000, 3.000],  loss: 7.907813, mse: 2410.590823, mean_q: 24.741164, mean_eps: 0.805708
  32525/150000: episode: 336, duration: 0.473s, episode steps:  72, steps per second: 152, episode reward: -66.640, mean reward: -0.926 [-100.000, 13.119], mean action: 1.750 [0.000, 3.000],  loss: 11.359950, mse: 2434.392278, mean_q: 24.465934, mean_eps: 0.805069
  32581/150000: episode: 337, duration: 0.359s, episode steps:  56, steps per second: 156, episode reward: -154.841, mean reward: -2.765 [-100.000,  7.966], mean action: 1.679 [0.000, 3.000],  loss: 10.962067, mse: 2501.611261, mean_q: 23.363012, mean_eps: 0.804685
  32711/150000: episode: 338, duration: 0.836s, episode steps: 130, steps per second: 155, episode reward: -145.304, mean reward: -1.118 [-100.000,  5.878], mean action: 1.515 [0.000, 3.000],  loss: 6.438338, mse: 2449.287202, mean_q: 25.634932, mean_eps: 0.804127
  32810/150000: episode: 339, duration: 0.650s, episode steps:  99, steps per second: 152, episode reward: -105.984, mean reward: -1.071 [-100.000,  8.268], mean action: 1.556 [0.000, 3.000],  loss: 10.719502, mse: 2430.592632, mean_q: 26.280513, mean_eps: 0.803440
  32904/150000: episode: 340, duration: 0.668s, episode steps:  94, steps per second: 141, episode reward: -81.099, mean reward: -0.863 [-100.000, 12.684], mean action: 1.617 [0.000, 3.000],  loss: 10.450727, mse: 2392.042886, mean_q: 24.284926, mean_eps: 0.802861
  33014/150000: episode: 341, duration: 0.729s, episode steps: 110, steps per second: 151, episode reward: -37.375, mean reward: -0.340 [-100.000, 16.569], mean action: 1.636 [0.000, 3.000],  loss: 12.434787, mse: 2365.937113, mean_q: 25.027722, mean_eps: 0.802249
  33135/150000: episode: 342, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -75.887, mean reward: -0.627 [-100.000,  9.927], mean action: 1.570 [0.000, 3.000],  loss: 16.650589, mse: 2386.748835, mean_q: 24.579507, mean_eps: 0.801556
  33216/150000: episode: 343, duration: 0.532s, episode steps:  81, steps per second: 152, episode reward: -122.765, mean reward: -1.516 [-100.000,  9.943], mean action: 1.593 [0.000, 3.000],  loss: 7.476145, mse: 2388.667788, mean_q: 26.629203, mean_eps: 0.800950
  33363/150000: episode: 344, duration: 0.956s, episode steps: 147, steps per second: 154, episode reward: -230.261, mean reward: -1.566 [-100.000, 55.510], mean action: 1.612 [0.000, 3.000],  loss: 5.607559, mse: 2437.824614, mean_q: 24.208902, mean_eps: 0.800266
  33502/150000: episode: 345, duration: 0.929s, episode steps: 139, steps per second: 150, episode reward: -124.249, mean reward: -0.894 [-100.000, 11.094], mean action: 1.525 [0.000, 3.000],  loss: 9.479546, mse: 2414.773261, mean_q: 23.904115, mean_eps: 0.799408
  33647/150000: episode: 346, duration: 0.915s, episode steps: 145, steps per second: 158, episode reward: -48.987, mean reward: -0.338 [-100.000, 16.652], mean action: 1.628 [0.000, 3.000],  loss: 11.449729, mse: 2479.817347, mean_q: 24.488749, mean_eps: 0.798556
  33715/150000: episode: 347, duration: 0.439s, episode steps:  68, steps per second: 155, episode reward: -58.973, mean reward: -0.867 [-100.000, 17.674], mean action: 1.632 [0.000, 3.000],  loss: 4.819327, mse: 2384.516138, mean_q: 23.737178, mean_eps: 0.797917
  33805/150000: episode: 348, duration: 0.608s, episode steps:  90, steps per second: 148, episode reward: -64.000, mean reward: -0.711 [-100.000,  7.852], mean action: 1.733 [0.000, 3.000],  loss: 9.866488, mse: 2335.853818, mean_q: 23.492594, mean_eps: 0.797443
  33895/150000: episode: 349, duration: 0.579s, episode steps:  90, steps per second: 155, episode reward: -52.470, mean reward: -0.583 [-100.000, 14.582], mean action: 1.578 [0.000, 3.000],  loss: 13.065754, mse: 2341.834195, mean_q: 22.922803, mean_eps: 0.796903
  34003/150000: episode: 350, duration: 0.687s, episode steps: 108, steps per second: 157, episode reward: -215.280, mean reward: -1.993 [-100.000,  1.088], mean action: 1.704 [0.000, 3.000],  loss: 8.415479, mse: 2367.621266, mean_q: 25.063209, mean_eps: 0.796309
  34078/150000: episode: 351, duration: 0.506s, episode steps:  75, steps per second: 148, episode reward: -111.423, mean reward: -1.486 [-100.000,  7.767], mean action: 1.600 [0.000, 3.000],  loss: 11.984063, mse: 2336.578903, mean_q: 22.043011, mean_eps: 0.795760
  34174/150000: episode: 352, duration: 0.629s, episode steps:  96, steps per second: 153, episode reward: -113.920, mean reward: -1.187 [-100.000, 12.389], mean action: 1.615 [0.000, 3.000],  loss: 10.011330, mse: 2315.802424, mean_q: 23.238218, mean_eps: 0.795247
  34251/150000: episode: 353, duration: 0.494s, episode steps:  77, steps per second: 156, episode reward: -63.662, mean reward: -0.827 [-100.000, 11.869], mean action: 1.481 [0.000, 3.000],  loss: 6.711767, mse: 2300.844040, mean_q: 23.823382, mean_eps: 0.794728
  34347/150000: episode: 354, duration: 0.612s, episode steps:  96, steps per second: 157, episode reward: -107.343, mean reward: -1.118 [-100.000,  8.312], mean action: 1.333 [0.000, 3.000],  loss: 10.265642, mse: 2272.048281, mean_q: 23.717156, mean_eps: 0.794209
  34447/150000: episode: 355, duration: 0.696s, episode steps: 100, steps per second: 144, episode reward: -104.018, mean reward: -1.040 [-100.000, 39.938], mean action: 1.500 [0.000, 3.000],  loss: 6.790629, mse: 2264.578640, mean_q: 23.315459, mean_eps: 0.793621
  34566/150000: episode: 356, duration: 0.757s, episode steps: 119, steps per second: 157, episode reward: -100.961, mean reward: -0.848 [-100.000, 35.502], mean action: 1.496 [0.000, 3.000],  loss: 9.825828, mse: 2250.442844, mean_q: 23.534599, mean_eps: 0.792964
  34698/150000: episode: 357, duration: 0.867s, episode steps: 132, steps per second: 152, episode reward: -38.389, mean reward: -0.291 [-100.000, 14.421], mean action: 1.530 [0.000, 3.000],  loss: 11.675684, mse: 2220.192353, mean_q: 24.129107, mean_eps: 0.792211
  34783/150000: episode: 358, duration: 0.591s, episode steps:  85, steps per second: 144, episode reward: -101.592, mean reward: -1.195 [-100.000, 12.998], mean action: 1.624 [0.000, 3.000],  loss: 9.890349, mse: 2210.118598, mean_q: 24.229648, mean_eps: 0.791560
  34885/150000: episode: 359, duration: 0.660s, episode steps: 102, steps per second: 155, episode reward: -118.040, mean reward: -1.157 [-100.000, 19.177], mean action: 1.471 [0.000, 3.000],  loss: 11.079426, mse: 2200.374836, mean_q: 24.023321, mean_eps: 0.790999
  34975/150000: episode: 360, duration: 0.589s, episode steps:  90, steps per second: 153, episode reward: -97.548, mean reward: -1.084 [-100.000,  7.152], mean action: 1.333 [0.000, 3.000],  loss: 6.903456, mse: 2194.675302, mean_q: 25.956563, mean_eps: 0.790423
  35112/150000: episode: 361, duration: 0.917s, episode steps: 137, steps per second: 149, episode reward: -181.113, mean reward: -1.322 [-100.000,  8.012], mean action: 1.591 [0.000, 3.000],  loss: 8.071252, mse: 2217.023748, mean_q: 24.760511, mean_eps: 0.789742
  35207/150000: episode: 362, duration: 0.614s, episode steps:  95, steps per second: 155, episode reward: -67.409, mean reward: -0.710 [-100.000, 14.272], mean action: 1.453 [0.000, 3.000],  loss: 8.288741, mse: 2288.887748, mean_q: 26.356296, mean_eps: 0.789046
  35337/150000: episode: 363, duration: 0.844s, episode steps: 130, steps per second: 154, episode reward: -111.997, mean reward: -0.862 [-100.000, 23.022], mean action: 1.646 [0.000, 3.000],  loss: 5.706866, mse: 2284.237535, mean_q: 27.788993, mean_eps: 0.788371
  35448/150000: episode: 364, duration: 0.770s, episode steps: 111, steps per second: 144, episode reward: -80.192, mean reward: -0.722 [-100.000, 12.519], mean action: 1.550 [0.000, 3.000],  loss: 7.259212, mse: 2369.299172, mean_q: 26.883617, mean_eps: 0.787648
  35590/150000: episode: 365, duration: 0.911s, episode steps: 142, steps per second: 156, episode reward: -89.530, mean reward: -0.630 [-100.000,  8.355], mean action: 1.585 [0.000, 3.000],  loss: 11.770128, mse: 2380.449091, mean_q: 28.224472, mean_eps: 0.786889
  35712/150000: episode: 366, duration: 0.798s, episode steps: 122, steps per second: 153, episode reward: -129.988, mean reward: -1.065 [-100.000,  7.476], mean action: 1.607 [0.000, 3.000],  loss: 11.267549, mse: 2313.442640, mean_q: 28.382045, mean_eps: 0.786097
  35834/150000: episode: 367, duration: 0.830s, episode steps: 122, steps per second: 147, episode reward: -90.675, mean reward: -0.743 [-100.000,  4.958], mean action: 1.689 [0.000, 3.000],  loss: 5.068518, mse: 2338.436265, mean_q: 29.123546, mean_eps: 0.785365
  35975/150000: episode: 368, duration: 0.900s, episode steps: 141, steps per second: 157, episode reward: -67.913, mean reward: -0.482 [-100.000,  9.247], mean action: 1.404 [0.000, 3.000],  loss: 7.557953, mse: 2356.095230, mean_q: 28.687765, mean_eps: 0.784576
  36047/150000: episode: 369, duration: 0.476s, episode steps:  72, steps per second: 151, episode reward: -78.204, mean reward: -1.086 [-100.000, 10.352], mean action: 1.569 [0.000, 3.000],  loss: 11.319867, mse: 2268.925123, mean_q: 28.457307, mean_eps: 0.783937
  36159/150000: episode: 370, duration: 0.762s, episode steps: 112, steps per second: 147, episode reward: -87.162, mean reward: -0.778 [-100.000, 10.878], mean action: 1.580 [0.000, 3.000],  loss: 12.163343, mse: 2241.686576, mean_q: 28.151775, mean_eps: 0.783385
  36258/150000: episode: 371, duration: 0.638s, episode steps:  99, steps per second: 155, episode reward: -77.074, mean reward: -0.779 [-100.000, 18.088], mean action: 1.677 [0.000, 3.000],  loss: 6.653720, mse: 2249.724225, mean_q: 27.139286, mean_eps: 0.782752
  36341/150000: episode: 372, duration: 0.528s, episode steps:  83, steps per second: 157, episode reward: -72.821, mean reward: -0.877 [-100.000, 13.725], mean action: 1.530 [0.000, 3.000],  loss: 13.955450, mse: 2246.535236, mean_q: 26.088300, mean_eps: 0.782206
  36485/150000: episode: 373, duration: 0.975s, episode steps: 144, steps per second: 148, episode reward: -251.198, mean reward: -1.744 [-100.000, 22.453], mean action: 1.701 [0.000, 3.000],  loss: 14.270201, mse: 2215.761804, mean_q: 26.191305, mean_eps: 0.781525
  36592/150000: episode: 374, duration: 0.716s, episode steps: 107, steps per second: 149, episode reward: -91.075, mean reward: -0.851 [-100.000,  8.835], mean action: 1.654 [0.000, 3.000],  loss: 14.451635, mse: 2240.241776, mean_q: 24.726879, mean_eps: 0.780772
  36725/150000: episode: 375, duration: 0.877s, episode steps: 133, steps per second: 152, episode reward: -75.517, mean reward: -0.568 [-100.000, 11.345], mean action: 1.556 [0.000, 3.000],  loss: 8.403582, mse: 2222.119338, mean_q: 27.089584, mean_eps: 0.780052
  36823/150000: episode: 376, duration: 0.649s, episode steps:  98, steps per second: 151, episode reward: -84.170, mean reward: -0.859 [-100.000, 10.168], mean action: 1.663 [0.000, 3.000],  loss: 9.393064, mse: 2240.768732, mean_q: 27.424764, mean_eps: 0.779359
  36934/150000: episode: 377, duration: 0.720s, episode steps: 111, steps per second: 154, episode reward: -39.639, mean reward: -0.357 [-100.000, 13.385], mean action: 1.622 [0.000, 3.000],  loss: 12.210259, mse: 2257.351727, mean_q: 26.461460, mean_eps: 0.778732
  37026/150000: episode: 378, duration: 0.599s, episode steps:  92, steps per second: 154, episode reward: -90.801, mean reward: -0.987 [-100.000, 21.955], mean action: 1.717 [0.000, 3.000],  loss: 7.131171, mse: 2197.098793, mean_q: 26.370895, mean_eps: 0.778123
  37131/150000: episode: 379, duration: 0.697s, episode steps: 105, steps per second: 151, episode reward: -54.181, mean reward: -0.516 [-100.000, 12.039], mean action: 1.533 [0.000, 3.000],  loss: 8.096716, mse: 2262.780532, mean_q: 27.202064, mean_eps: 0.777532
  37211/150000: episode: 380, duration: 0.527s, episode steps:  80, steps per second: 152, episode reward: -92.474, mean reward: -1.156 [-100.000,  8.706], mean action: 1.550 [0.000, 3.000],  loss: 5.808965, mse: 2275.640256, mean_q: 27.260210, mean_eps: 0.776977
  37299/150000: episode: 381, duration: 0.570s, episode steps:  88, steps per second: 154, episode reward: -58.069, mean reward: -0.660 [-100.000,  6.624], mean action: 1.795 [0.000, 3.000],  loss: 5.910147, mse: 2228.804771, mean_q: 26.679225, mean_eps: 0.776473
  37381/150000: episode: 382, duration: 0.559s, episode steps:  82, steps per second: 147, episode reward: -60.323, mean reward: -0.736 [-100.000, 12.779], mean action: 1.622 [0.000, 3.000],  loss: 6.526400, mse: 2249.378961, mean_q: 25.697032, mean_eps: 0.775963
  37500/150000: episode: 383, duration: 0.778s, episode steps: 119, steps per second: 153, episode reward: -68.080, mean reward: -0.572 [-100.000, 13.014], mean action: 1.664 [0.000, 3.000],  loss: 9.358366, mse: 2309.500541, mean_q: 25.232606, mean_eps: 0.775360
  37623/150000: episode: 384, duration: 0.794s, episode steps: 123, steps per second: 155, episode reward: -66.173, mean reward: -0.538 [-100.000, 16.896], mean action: 1.447 [0.000, 3.000],  loss: 8.860260, mse: 2278.632433, mean_q: 26.931346, mean_eps: 0.774634
  37771/150000: episode: 385, duration: 0.987s, episode steps: 148, steps per second: 150, episode reward: -75.011, mean reward: -0.507 [-100.000, 15.279], mean action: 1.527 [0.000, 3.000],  loss: 7.082987, mse: 2294.762769, mean_q: 25.678107, mean_eps: 0.773821
  37901/150000: episode: 386, duration: 0.840s, episode steps: 130, steps per second: 155, episode reward: -57.041, mean reward: -0.439 [-100.000,  7.113], mean action: 1.592 [0.000, 3.000],  loss: 8.289405, mse: 2290.744134, mean_q: 24.900004, mean_eps: 0.772987
  38027/150000: episode: 387, duration: 0.830s, episode steps: 126, steps per second: 152, episode reward: -178.093, mean reward: -1.413 [-100.000,  6.341], mean action: 1.476 [0.000, 3.000],  loss: 6.681055, mse: 2293.500509, mean_q: 24.778692, mean_eps: 0.772219
  38093/150000: episode: 388, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -44.121, mean reward: -0.668 [-100.000,  8.543], mean action: 1.667 [0.000, 3.000],  loss: 6.630764, mse: 2290.470056, mean_q: 25.154564, mean_eps: 0.771643
  38173/150000: episode: 389, duration: 0.517s, episode steps:  80, steps per second: 155, episode reward: -43.010, mean reward: -0.538 [-100.000, 12.383], mean action: 1.650 [0.000, 3.000],  loss: 7.896262, mse: 2278.034494, mean_q: 24.950606, mean_eps: 0.771205
  38276/150000: episode: 390, duration: 0.677s, episode steps: 103, steps per second: 152, episode reward: -84.738, mean reward: -0.823 [-100.000,  7.804], mean action: 1.553 [0.000, 3.000],  loss: 11.082145, mse: 2288.640570, mean_q: 25.924878, mean_eps: 0.770656
  38357/150000: episode: 391, duration: 0.542s, episode steps:  81, steps per second: 149, episode reward: -64.702, mean reward: -0.799 [-100.000,  7.602], mean action: 1.765 [0.000, 3.000],  loss: 19.577290, mse: 2337.207520, mean_q: 24.491488, mean_eps: 0.770104
  38463/150000: episode: 392, duration: 0.695s, episode steps: 106, steps per second: 153, episode reward: -72.122, mean reward: -0.680 [-100.000,  8.328], mean action: 1.679 [0.000, 3.000],  loss: 6.521356, mse: 2317.120596, mean_q: 23.684629, mean_eps: 0.769543
  38609/150000: episode: 393, duration: 0.948s, episode steps: 146, steps per second: 154, episode reward: -48.739, mean reward: -0.334 [-100.000, 27.388], mean action: 1.651 [0.000, 3.000],  loss: 8.447354, mse: 2333.123642, mean_q: 24.278628, mean_eps: 0.768787
  38701/150000: episode: 394, duration: 0.665s, episode steps:  92, steps per second: 138, episode reward: -342.980, mean reward: -3.728 [-100.000, 73.453], mean action: 1.609 [0.000, 3.000],  loss: 10.184268, mse: 2496.049446, mean_q: 25.400509, mean_eps: 0.768073
  38783/150000: episode: 395, duration: 0.660s, episode steps:  82, steps per second: 124, episode reward: -81.012, mean reward: -0.988 [-100.000,  7.834], mean action: 1.537 [0.000, 3.000],  loss: 6.829911, mse: 2469.477244, mean_q: 25.264940, mean_eps: 0.767551
  38872/150000: episode: 396, duration: 0.620s, episode steps:  89, steps per second: 144, episode reward: -16.443, mean reward: -0.185 [-100.000, 11.266], mean action: 1.708 [0.000, 3.000],  loss: 13.786773, mse: 2437.026711, mean_q: 25.412860, mean_eps: 0.767038
  38995/150000: episode: 397, duration: 0.834s, episode steps: 123, steps per second: 147, episode reward: -63.594, mean reward: -0.517 [-100.000,  7.373], mean action: 1.569 [0.000, 3.000],  loss: 6.083393, mse: 2404.263877, mean_q: 25.697724, mean_eps: 0.766402
  39094/150000: episode: 398, duration: 0.649s, episode steps:  99, steps per second: 153, episode reward: -78.251, mean reward: -0.790 [-100.000, 10.703], mean action: 1.657 [0.000, 3.000],  loss: 7.917298, mse: 2430.921139, mean_q: 26.440540, mean_eps: 0.765736
  39179/150000: episode: 399, duration: 0.560s, episode steps:  85, steps per second: 152, episode reward: -80.429, mean reward: -0.946 [-100.000, 18.498], mean action: 1.541 [0.000, 3.000],  loss: 6.818650, mse: 2405.766918, mean_q: 27.284190, mean_eps: 0.765184
  39288/150000: episode: 400, duration: 0.716s, episode steps: 109, steps per second: 152, episode reward: -77.260, mean reward: -0.709 [-100.000, 10.333], mean action: 1.569 [0.000, 3.000],  loss: 10.571019, mse: 2471.408556, mean_q: 27.282395, mean_eps: 0.764602
  39351/150000: episode: 401, duration: 0.431s, episode steps:  63, steps per second: 146, episode reward: -56.155, mean reward: -0.891 [-100.000, 11.664], mean action: 1.254 [0.000, 3.000],  loss: 7.476285, mse: 2504.970608, mean_q: 26.162401, mean_eps: 0.764086
  39457/150000: episode: 402, duration: 0.716s, episode steps: 106, steps per second: 148, episode reward: -58.888, mean reward: -0.556 [-100.000, 14.368], mean action: 1.698 [0.000, 3.000],  loss: 6.948322, mse: 2548.515172, mean_q: 25.851821, mean_eps: 0.763579
  39551/150000: episode: 403, duration: 0.617s, episode steps:  94, steps per second: 152, episode reward: -74.620, mean reward: -0.794 [-100.000, 10.595], mean action: 1.606 [0.000, 3.000],  loss: 8.678753, mse: 2461.829476, mean_q: 25.590173, mean_eps: 0.762979
  39634/150000: episode: 404, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -65.225, mean reward: -0.786 [-100.000,  7.578], mean action: 1.651 [0.000, 3.000],  loss: 10.319637, mse: 2518.892797, mean_q: 26.620067, mean_eps: 0.762448
  39740/150000: episode: 405, duration: 0.706s, episode steps: 106, steps per second: 150, episode reward: -72.486, mean reward: -0.684 [-100.000, 12.125], mean action: 1.491 [0.000, 3.000],  loss: 7.628645, mse: 2467.731845, mean_q: 26.367199, mean_eps: 0.761881
  39832/150000: episode: 406, duration: 0.588s, episode steps:  92, steps per second: 156, episode reward: -117.368, mean reward: -1.276 [-100.000,  7.678], mean action: 1.641 [0.000, 3.000],  loss: 8.739200, mse: 2408.822957, mean_q: 25.721095, mean_eps: 0.761287
  39943/150000: episode: 407, duration: 0.751s, episode steps: 111, steps per second: 148, episode reward: -121.668, mean reward: -1.096 [-100.000,  7.677], mean action: 1.432 [0.000, 3.000],  loss: 8.232905, mse: 2400.968905, mean_q: 26.725649, mean_eps: 0.760678
  40006/150000: episode: 408, duration: 0.437s, episode steps:  63, steps per second: 144, episode reward: -54.399, mean reward: -0.863 [-100.000,  7.323], mean action: 1.683 [0.000, 3.000],  loss: 7.720915, mse: 2392.420226, mean_q: 26.186746, mean_eps: 0.760156
  40132/150000: episode: 409, duration: 0.815s, episode steps: 126, steps per second: 155, episode reward: -101.882, mean reward: -0.809 [-100.000, 13.012], mean action: 1.603 [0.000, 3.000],  loss: 6.234797, mse: 2425.999224, mean_q: 24.122943, mean_eps: 0.759589
  40193/150000: episode: 410, duration: 0.397s, episode steps:  61, steps per second: 154, episode reward: -73.044, mean reward: -1.197 [-100.000, 30.502], mean action: 1.869 [0.000, 3.000],  loss: 7.096768, mse: 2467.039927, mean_q: 24.433725, mean_eps: 0.759028
  40307/150000: episode: 411, duration: 1.629s, episode steps: 114, steps per second:  70, episode reward: -59.913, mean reward: -0.526 [-100.000, 16.868], mean action: 1.737 [0.000, 3.000],  loss: 7.622970, mse: 2488.266746, mean_q: 25.040948, mean_eps: 0.758503
  40399/150000: episode: 412, duration: 0.598s, episode steps:  92, steps per second: 154, episode reward: -190.569, mean reward: -2.071 [-100.000, 54.985], mean action: 1.500 [0.000, 3.000],  loss: 9.400118, mse: 2613.196698, mean_q: 24.899625, mean_eps: 0.757885
  40501/150000: episode: 413, duration: 0.681s, episode steps: 102, steps per second: 150, episode reward: -38.039, mean reward: -0.373 [-100.000, 12.417], mean action: 1.725 [0.000, 3.000],  loss: 6.991298, mse: 2562.275492, mean_q: 24.368061, mean_eps: 0.757303
  40643/150000: episode: 414, duration: 0.937s, episode steps: 142, steps per second: 152, episode reward:  4.326, mean reward:  0.030 [-100.000, 62.449], mean action: 1.507 [0.000, 3.000],  loss: 5.945153, mse: 2550.460184, mean_q: 24.877949, mean_eps: 0.756571
  40705/150000: episode: 415, duration: 0.445s, episode steps:  62, steps per second: 139, episode reward: -90.362, mean reward: -1.457 [-100.000,  9.272], mean action: 1.548 [0.000, 3.000],  loss: 6.104500, mse: 2614.800864, mean_q: 23.917764, mean_eps: 0.755959
  40854/150000: episode: 416, duration: 1.041s, episode steps: 149, steps per second: 143, episode reward: -72.875, mean reward: -0.489 [-100.000, 12.506], mean action: 1.664 [0.000, 3.000],  loss: 8.934135, mse: 2502.706528, mean_q: 23.331277, mean_eps: 0.755326
  40989/150000: episode: 417, duration: 0.922s, episode steps: 135, steps per second: 146, episode reward: -152.506, mean reward: -1.130 [-100.000,  7.651], mean action: 1.607 [0.000, 3.000],  loss: 5.573079, mse: 2434.431422, mean_q: 22.897712, mean_eps: 0.754474
  41159/150000: episode: 418, duration: 1.370s, episode steps: 170, steps per second: 124, episode reward: -86.688, mean reward: -0.510 [-100.000,  6.259], mean action: 1.671 [0.000, 3.000],  loss: 8.317316, mse: 2470.328921, mean_q: 23.290307, mean_eps: 0.753559
  41299/150000: episode: 419, duration: 1.037s, episode steps: 140, steps per second: 135, episode reward: -97.446, mean reward: -0.696 [-100.000,  6.813], mean action: 1.671 [0.000, 3.000],  loss: 6.485235, mse: 2426.689948, mean_q: 23.904738, mean_eps: 0.752629
  41419/150000: episode: 420, duration: 0.942s, episode steps: 120, steps per second: 127, episode reward: -85.756, mean reward: -0.715 [-100.000,  6.032], mean action: 1.683 [0.000, 3.000],  loss: 6.289466, mse: 2450.554370, mean_q: 21.803453, mean_eps: 0.751849
  41542/150000: episode: 421, duration: 0.903s, episode steps: 123, steps per second: 136, episode reward: -200.498, mean reward: -1.630 [-100.000, 14.558], mean action: 1.585 [0.000, 3.000],  loss: 6.426742, mse: 2433.131675, mean_q: 23.144797, mean_eps: 0.751120
  41703/150000: episode: 422, duration: 1.148s, episode steps: 161, steps per second: 140, episode reward: -25.542, mean reward: -0.159 [-100.000, 71.272], mean action: 1.646 [0.000, 3.000],  loss: 5.427650, mse: 2406.608734, mean_q: 23.193798, mean_eps: 0.750268
  41791/150000: episode: 423, duration: 0.585s, episode steps:  88, steps per second: 151, episode reward: -72.775, mean reward: -0.827 [-100.000,  8.830], mean action: 1.568 [0.000, 3.000],  loss: 6.351129, mse: 2481.067347, mean_q: 22.720449, mean_eps: 0.749521
  41902/150000: episode: 424, duration: 0.735s, episode steps: 111, steps per second: 151, episode reward: -112.879, mean reward: -1.017 [-100.000, 10.300], mean action: 1.622 [0.000, 3.000],  loss: 7.028499, mse: 2431.092978, mean_q: 23.183857, mean_eps: 0.748924
  42037/150000: episode: 425, duration: 0.920s, episode steps: 135, steps per second: 147, episode reward: -137.610, mean reward: -1.019 [-100.000, 11.175], mean action: 1.696 [0.000, 3.000],  loss: 8.700655, mse: 2471.089572, mean_q: 22.202004, mean_eps: 0.748186
  42112/150000: episode: 426, duration: 0.495s, episode steps:  75, steps per second: 151, episode reward: -74.224, mean reward: -0.990 [-100.000, 14.870], mean action: 1.560 [0.000, 3.000],  loss: 9.600514, mse: 2481.280272, mean_q: 22.294652, mean_eps: 0.747556
  42231/150000: episode: 427, duration: 0.780s, episode steps: 119, steps per second: 152, episode reward: -113.189, mean reward: -0.951 [-100.000, 18.121], mean action: 1.353 [0.000, 3.000],  loss: 8.841476, mse: 2450.100138, mean_q: 22.677062, mean_eps: 0.746974
  42352/150000: episode: 428, duration: 0.821s, episode steps: 121, steps per second: 147, episode reward: -113.426, mean reward: -0.937 [-100.000, 23.100], mean action: 1.521 [0.000, 3.000],  loss: 7.680017, mse: 2467.489880, mean_q: 23.113397, mean_eps: 0.746254
  42440/150000: episode: 429, duration: 0.586s, episode steps:  88, steps per second: 150, episode reward: -128.900, mean reward: -1.465 [-100.000,  9.846], mean action: 1.466 [0.000, 3.000],  loss: 5.709506, mse: 2493.446680, mean_q: 22.852394, mean_eps: 0.745627
  42549/150000: episode: 430, duration: 0.735s, episode steps: 109, steps per second: 148, episode reward: -99.378, mean reward: -0.912 [-100.000, 10.494], mean action: 1.606 [0.000, 3.000],  loss: 14.569722, mse: 2498.865186, mean_q: 23.110426, mean_eps: 0.745036
  42647/150000: episode: 431, duration: 0.665s, episode steps:  98, steps per second: 147, episode reward: -73.375, mean reward: -0.749 [-100.000,  6.381], mean action: 1.531 [0.000, 3.000],  loss: 12.922246, mse: 2503.357193, mean_q: 22.700921, mean_eps: 0.744415
  42756/150000: episode: 432, duration: 0.726s, episode steps: 109, steps per second: 150, episode reward: -149.313, mean reward: -1.370 [-100.000,  7.214], mean action: 1.505 [0.000, 3.000],  loss: 9.682632, mse: 2505.206191, mean_q: 22.794691, mean_eps: 0.743794
  42839/150000: episode: 433, duration: 0.543s, episode steps:  83, steps per second: 153, episode reward: -113.507, mean reward: -1.368 [-100.000,  6.951], mean action: 1.699 [0.000, 3.000],  loss: 9.454501, mse: 2457.526075, mean_q: 24.493384, mean_eps: 0.743218
  42951/150000: episode: 434, duration: 0.742s, episode steps: 112, steps per second: 151, episode reward: -83.550, mean reward: -0.746 [-100.000, 26.290], mean action: 1.714 [0.000, 3.000],  loss: 9.033824, mse: 2484.929985, mean_q: 23.168350, mean_eps: 0.742633
  43087/150000: episode: 435, duration: 0.932s, episode steps: 136, steps per second: 146, episode reward: -184.821, mean reward: -1.359 [-100.000, 52.129], mean action: 1.522 [0.000, 3.000],  loss: 8.828622, mse: 2431.254747, mean_q: 22.585233, mean_eps: 0.741889
  43240/150000: episode: 436, duration: 1.048s, episode steps: 153, steps per second: 146, episode reward: -82.460, mean reward: -0.539 [-100.000,  8.969], mean action: 1.641 [0.000, 3.000],  loss: 6.384543, mse: 2428.749217, mean_q: 23.217249, mean_eps: 0.741022
  43406/150000: episode: 437, duration: 1.148s, episode steps: 166, steps per second: 145, episode reward: -84.168, mean reward: -0.507 [-100.000,  7.385], mean action: 1.693 [0.000, 3.000],  loss: 10.019791, mse: 2441.444788, mean_q: 22.418904, mean_eps: 0.740065
  43486/150000: episode: 438, duration: 0.529s, episode steps:  80, steps per second: 151, episode reward: -100.931, mean reward: -1.262 [-100.000,  5.938], mean action: 1.650 [0.000, 3.000],  loss: 11.621984, mse: 2383.082439, mean_q: 22.972183, mean_eps: 0.739327
  43649/150000: episode: 439, duration: 1.110s, episode steps: 163, steps per second: 147, episode reward: 12.660, mean reward:  0.078 [-100.000, 51.981], mean action: 1.736 [0.000, 3.000],  loss: 9.816141, mse: 2335.882617, mean_q: 22.017191, mean_eps: 0.738598
  43716/150000: episode: 440, duration: 0.445s, episode steps:  67, steps per second: 151, episode reward: -61.289, mean reward: -0.915 [-100.000, 11.052], mean action: 1.507 [0.000, 3.000],  loss: 12.380435, mse: 2323.105272, mean_q: 23.917144, mean_eps: 0.737908
  43800/150000: episode: 441, duration: 0.551s, episode steps:  84, steps per second: 152, episode reward: -79.732, mean reward: -0.949 [-100.000,  9.833], mean action: 1.929 [0.000, 3.000],  loss: 5.864648, mse: 2273.669511, mean_q: 23.813286, mean_eps: 0.737455
  43896/150000: episode: 442, duration: 0.657s, episode steps:  96, steps per second: 146, episode reward: -54.351, mean reward: -0.566 [-100.000, 11.774], mean action: 1.604 [0.000, 3.000],  loss: 10.587259, mse: 2276.565849, mean_q: 24.271481, mean_eps: 0.736915
  44006/150000: episode: 443, duration: 0.741s, episode steps: 110, steps per second: 149, episode reward: -37.370, mean reward: -0.340 [-100.000, 27.508], mean action: 1.500 [0.000, 3.000],  loss: 6.862809, mse: 2381.256521, mean_q: 24.153796, mean_eps: 0.736297
  44140/150000: episode: 444, duration: 0.888s, episode steps: 134, steps per second: 151, episode reward: -161.160, mean reward: -1.203 [-100.000,  6.676], mean action: 1.388 [0.000, 3.000],  loss: 6.773829, mse: 2454.252512, mean_q: 24.109389, mean_eps: 0.735565
  44289/150000: episode: 445, duration: 1.044s, episode steps: 149, steps per second: 143, episode reward: -103.072, mean reward: -0.692 [-100.000, 34.657], mean action: 1.611 [0.000, 3.000],  loss: 7.463598, mse: 2430.867561, mean_q: 24.095799, mean_eps: 0.734716
  44442/150000: episode: 446, duration: 1.002s, episode steps: 153, steps per second: 153, episode reward: -54.424, mean reward: -0.356 [-100.000, 12.642], mean action: 1.510 [0.000, 3.000],  loss: 8.613159, mse: 2414.751750, mean_q: 24.232833, mean_eps: 0.733810
  44542/150000: episode: 447, duration: 0.755s, episode steps: 100, steps per second: 132, episode reward: -59.605, mean reward: -0.596 [-100.000, 14.341], mean action: 1.740 [0.000, 3.000],  loss: 6.331778, mse: 2310.781589, mean_q: 23.969900, mean_eps: 0.733051
  44609/150000: episode: 448, duration: 0.517s, episode steps:  67, steps per second: 130, episode reward: -93.265, mean reward: -1.392 [-100.000,  6.395], mean action: 1.836 [0.000, 3.000],  loss: 7.774627, mse: 2363.110452, mean_q: 22.506769, mean_eps: 0.732550
  44691/150000: episode: 449, duration: 0.571s, episode steps:  82, steps per second: 144, episode reward: -81.597, mean reward: -0.995 [-100.000, 11.475], mean action: 1.598 [0.000, 3.000],  loss: 10.601160, mse: 2354.945810, mean_q: 23.629899, mean_eps: 0.732103
  44769/150000: episode: 450, duration: 0.518s, episode steps:  78, steps per second: 151, episode reward: -106.979, mean reward: -1.372 [-100.000,  7.020], mean action: 1.667 [0.000, 3.000],  loss: 6.219826, mse: 2314.067294, mean_q: 25.302019, mean_eps: 0.731623
  44853/150000: episode: 451, duration: 0.573s, episode steps:  84, steps per second: 147, episode reward: -113.094, mean reward: -1.346 [-100.000, 16.348], mean action: 1.631 [0.000, 3.000],  loss: 5.496320, mse: 2314.022663, mean_q: 23.901933, mean_eps: 0.731137
  44978/150000: episode: 452, duration: 0.855s, episode steps: 125, steps per second: 146, episode reward: -96.715, mean reward: -0.774 [-100.000,  9.815], mean action: 1.608 [0.000, 3.000],  loss: 9.527938, mse: 2263.497423, mean_q: 23.619842, mean_eps: 0.730510
  45073/150000: episode: 453, duration: 0.651s, episode steps:  95, steps per second: 146, episode reward: -63.249, mean reward: -0.666 [-100.000, 15.567], mean action: 1.632 [0.000, 3.000],  loss: 12.309114, mse: 2221.825148, mean_q: 23.213191, mean_eps: 0.729850
  45160/150000: episode: 454, duration: 0.600s, episode steps:  87, steps per second: 145, episode reward: -47.773, mean reward: -0.549 [-100.000, 12.901], mean action: 1.494 [0.000, 3.000],  loss: 6.322487, mse: 2213.089468, mean_q: 22.586362, mean_eps: 0.729304
  45249/150000: episode: 455, duration: 0.619s, episode steps:  89, steps per second: 144, episode reward: -119.566, mean reward: -1.343 [-100.000,  6.078], mean action: 1.427 [0.000, 3.000],  loss: 7.870364, mse: 2224.209367, mean_q: 23.197755, mean_eps: 0.728776
  45359/150000: episode: 456, duration: 0.725s, episode steps: 110, steps per second: 152, episode reward: -107.801, mean reward: -0.980 [-100.000,  5.953], mean action: 1.555 [0.000, 3.000],  loss: 6.268348, mse: 2227.334278, mean_q: 23.318252, mean_eps: 0.728179
  45441/150000: episode: 457, duration: 0.547s, episode steps:  82, steps per second: 150, episode reward: -78.525, mean reward: -0.958 [-100.000,  7.771], mean action: 1.756 [0.000, 3.000],  loss: 8.284677, mse: 2258.894295, mean_q: 22.332568, mean_eps: 0.727603
  45521/150000: episode: 458, duration: 0.569s, episode steps:  80, steps per second: 141, episode reward: -95.201, mean reward: -1.190 [-100.000, 12.698], mean action: 1.512 [0.000, 3.000],  loss: 6.224111, mse: 2260.295300, mean_q: 22.765263, mean_eps: 0.727117
  45689/150000: episode: 459, duration: 1.112s, episode steps: 168, steps per second: 151, episode reward:  3.579, mean reward:  0.021 [-100.000, 34.648], mean action: 1.577 [0.000, 3.000],  loss: 6.414313, mse: 2242.538188, mean_q: 23.333466, mean_eps: 0.726373
  45762/150000: episode: 460, duration: 0.476s, episode steps:  73, steps per second: 153, episode reward: -93.486, mean reward: -1.281 [-100.000,  8.941], mean action: 1.493 [0.000, 3.000],  loss: 8.089451, mse: 2222.004921, mean_q: 22.709379, mean_eps: 0.725650
  45837/150000: episode: 461, duration: 0.523s, episode steps:  75, steps per second: 143, episode reward: -65.880, mean reward: -0.878 [-100.000, 15.623], mean action: 1.813 [0.000, 3.000],  loss: 4.730847, mse: 2257.908490, mean_q: 22.548337, mean_eps: 0.725206
  45911/150000: episode: 462, duration: 0.502s, episode steps:  74, steps per second: 147, episode reward: -90.550, mean reward: -1.224 [-100.000, 25.827], mean action: 1.324 [0.000, 3.000],  loss: 15.354735, mse: 2240.597130, mean_q: 23.632512, mean_eps: 0.724759
  45976/150000: episode: 463, duration: 0.440s, episode steps:  65, steps per second: 148, episode reward: -55.499, mean reward: -0.854 [-100.000, 12.321], mean action: 1.754 [0.000, 3.000],  loss: 4.682980, mse: 2276.657418, mean_q: 22.287974, mean_eps: 0.724342
  46119/150000: episode: 464, duration: 0.954s, episode steps: 143, steps per second: 150, episode reward: -69.768, mean reward: -0.488 [-100.000,  7.060], mean action: 1.692 [0.000, 3.000],  loss: 7.835656, mse: 2194.959431, mean_q: 24.060488, mean_eps: 0.723718
  46190/150000: episode: 465, duration: 0.483s, episode steps:  71, steps per second: 147, episode reward: -80.351, mean reward: -1.132 [-100.000, 10.610], mean action: 1.563 [0.000, 3.000],  loss: 5.609216, mse: 2176.276673, mean_q: 21.042975, mean_eps: 0.723076
  46271/150000: episode: 466, duration: 0.543s, episode steps:  81, steps per second: 149, episode reward: -73.009, mean reward: -0.901 [-100.000,  8.426], mean action: 1.506 [0.000, 3.000],  loss: 8.742708, mse: 2262.638523, mean_q: 23.573371, mean_eps: 0.722620
  46359/150000: episode: 467, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -47.021, mean reward: -0.534 [-100.000, 13.563], mean action: 1.727 [0.000, 3.000],  loss: 6.741482, mse: 2231.820991, mean_q: 22.770885, mean_eps: 0.722113
  46477/150000: episode: 468, duration: 0.807s, episode steps: 118, steps per second: 146, episode reward: -114.407, mean reward: -0.970 [-100.000,  8.855], mean action: 1.720 [0.000, 3.000],  loss: 6.643798, mse: 2271.486797, mean_q: 22.263386, mean_eps: 0.721495
  46676/150000: episode: 469, duration: 1.332s, episode steps: 199, steps per second: 149, episode reward:  1.533, mean reward:  0.008 [-100.000, 93.230], mean action: 1.638 [0.000, 3.000],  loss: 7.071642, mse: 2256.578454, mean_q: 21.771780, mean_eps: 0.720544
  46861/150000: episode: 470, duration: 1.269s, episode steps: 185, steps per second: 146, episode reward: -46.055, mean reward: -0.249 [-100.000, 11.532], mean action: 1.686 [0.000, 3.000],  loss: 5.935455, mse: 2230.450493, mean_q: 21.927213, mean_eps: 0.719392
  46942/150000: episode: 471, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -53.656, mean reward: -0.662 [-100.000, 13.420], mean action: 1.543 [0.000, 3.000],  loss: 3.976200, mse: 2230.210302, mean_q: 22.443326, mean_eps: 0.718594
  47026/150000: episode: 472, duration: 0.568s, episode steps:  84, steps per second: 148, episode reward: -109.840, mean reward: -1.308 [-100.000,  8.705], mean action: 1.690 [0.000, 3.000],  loss: 10.914989, mse: 2230.883211, mean_q: 22.953859, mean_eps: 0.718099
  47149/150000: episode: 473, duration: 0.879s, episode steps: 123, steps per second: 140, episode reward: -92.413, mean reward: -0.751 [-100.000, 15.895], mean action: 1.724 [0.000, 3.000],  loss: 9.288270, mse: 2206.598342, mean_q: 20.464517, mean_eps: 0.717478
  47265/150000: episode: 474, duration: 0.787s, episode steps: 116, steps per second: 147, episode reward: -62.039, mean reward: -0.535 [-100.000, 12.043], mean action: 1.793 [0.000, 3.000],  loss: 12.378777, mse: 2098.695804, mean_q: 20.918124, mean_eps: 0.716761
  47409/150000: episode: 475, duration: 1.023s, episode steps: 144, steps per second: 141, episode reward: -75.933, mean reward: -0.527 [-100.000, 12.314], mean action: 1.604 [0.000, 3.000],  loss: 10.535760, mse: 2110.759898, mean_q: 21.402514, mean_eps: 0.715981
  47536/150000: episode: 476, duration: 0.846s, episode steps: 127, steps per second: 150, episode reward: -92.852, mean reward: -0.731 [-100.000, 13.302], mean action: 1.638 [0.000, 3.000],  loss: 8.771689, mse: 2124.563525, mean_q: 20.677517, mean_eps: 0.715168
  47630/150000: episode: 477, duration: 0.635s, episode steps:  94, steps per second: 148, episode reward: -32.834, mean reward: -0.349 [-100.000, 16.105], mean action: 1.564 [0.000, 3.000],  loss: 6.938898, mse: 2136.175219, mean_q: 20.498058, mean_eps: 0.714505
  47742/150000: episode: 478, duration: 0.781s, episode steps: 112, steps per second: 143, episode reward: -97.427, mean reward: -0.870 [-100.000, 17.792], mean action: 1.562 [0.000, 3.000],  loss: 6.298014, mse: 2151.816271, mean_q: 19.991723, mean_eps: 0.713887
  47826/150000: episode: 479, duration: 0.562s, episode steps:  84, steps per second: 150, episode reward: -171.862, mean reward: -2.046 [-100.000,  7.082], mean action: 1.821 [0.000, 3.000],  loss: 14.471159, mse: 2076.260686, mean_q: 19.636420, mean_eps: 0.713299
  47912/150000: episode: 480, duration: 0.566s, episode steps:  86, steps per second: 152, episode reward: -50.716, mean reward: -0.590 [-100.000,  6.183], mean action: 1.558 [0.000, 3.000],  loss: 5.326633, mse: 2090.673174, mean_q: 20.296875, mean_eps: 0.712789
  47994/150000: episode: 481, duration: 0.573s, episode steps:  82, steps per second: 143, episode reward: -63.577, mean reward: -0.775 [-100.000, 17.328], mean action: 1.524 [0.000, 3.000],  loss: 9.962721, mse: 2053.911256, mean_q: 23.015249, mean_eps: 0.712285
  48103/150000: episode: 482, duration: 0.747s, episode steps: 109, steps per second: 146, episode reward: -100.626, mean reward: -0.923 [-100.000, 11.861], mean action: 1.560 [0.000, 3.000],  loss: 9.224469, mse: 2058.382360, mean_q: 21.268827, mean_eps: 0.711712
  48182/150000: episode: 483, duration: 0.523s, episode steps:  79, steps per second: 151, episode reward: -92.366, mean reward: -1.169 [-100.000, 13.471], mean action: 1.734 [0.000, 3.000],  loss: 12.045977, mse: 2043.118489, mean_q: 20.692706, mean_eps: 0.711148
  48294/150000: episode: 484, duration: 0.749s, episode steps: 112, steps per second: 150, episode reward: -25.572, mean reward: -0.228 [-100.000,  9.361], mean action: 1.759 [0.000, 3.000],  loss: 7.741650, mse: 2061.518340, mean_q: 19.339286, mean_eps: 0.710575
  48410/150000: episode: 485, duration: 0.801s, episode steps: 116, steps per second: 145, episode reward: -50.966, mean reward: -0.439 [-100.000,  6.851], mean action: 1.724 [0.000, 3.000],  loss: 6.651646, mse: 2081.784284, mean_q: 20.965375, mean_eps: 0.709891
  48511/150000: episode: 486, duration: 0.690s, episode steps: 101, steps per second: 146, episode reward: -127.705, mean reward: -1.264 [-100.000,  6.728], mean action: 1.644 [0.000, 3.000],  loss: 7.464890, mse: 2034.217815, mean_q: 21.178780, mean_eps: 0.709240
  48635/150000: episode: 487, duration: 0.836s, episode steps: 124, steps per second: 148, episode reward:  8.507, mean reward:  0.069 [-100.000, 64.423], mean action: 1.718 [0.000, 3.000],  loss: 6.403103, mse: 2059.428406, mean_q: 22.489156, mean_eps: 0.708565
  48708/150000: episode: 488, duration: 0.514s, episode steps:  73, steps per second: 142, episode reward: -53.579, mean reward: -0.734 [-100.000, 16.962], mean action: 1.767 [0.000, 3.000],  loss: 13.906203, mse: 2126.551027, mean_q: 21.795679, mean_eps: 0.707974
  48800/150000: episode: 489, duration: 0.611s, episode steps:  92, steps per second: 151, episode reward: -110.876, mean reward: -1.205 [-100.000,  7.576], mean action: 1.543 [0.000, 3.000],  loss: 8.048554, mse: 2118.952674, mean_q: 21.148277, mean_eps: 0.707479
  48869/150000: episode: 490, duration: 0.464s, episode steps:  69, steps per second: 149, episode reward: -79.433, mean reward: -1.151 [-100.000,  7.867], mean action: 1.478 [0.000, 3.000],  loss: 10.402441, mse: 2083.829623, mean_q: 21.857939, mean_eps: 0.706996
  48992/150000: episode: 491, duration: 0.842s, episode steps: 123, steps per second: 146, episode reward: -74.919, mean reward: -0.609 [-100.000, 17.678], mean action: 1.577 [0.000, 3.000],  loss: 8.767577, mse: 2051.531563, mean_q: 21.468742, mean_eps: 0.706420
  49123/150000: episode: 492, duration: 0.902s, episode steps: 131, steps per second: 145, episode reward: -75.852, mean reward: -0.579 [-100.000, 10.299], mean action: 1.481 [0.000, 3.000],  loss: 6.785904, mse: 2024.421240, mean_q: 21.129940, mean_eps: 0.705658
  49252/150000: episode: 493, duration: 0.853s, episode steps: 129, steps per second: 151, episode reward: -71.641, mean reward: -0.555 [-100.000, 12.764], mean action: 1.597 [0.000, 3.000],  loss: 4.807209, mse: 2032.505945, mean_q: 21.509704, mean_eps: 0.704878
  49327/150000: episode: 494, duration: 0.670s, episode steps:  75, steps per second: 112, episode reward: -75.041, mean reward: -1.001 [-100.000, 10.323], mean action: 1.453 [0.000, 3.000],  loss: 4.114985, mse: 2018.340278, mean_q: 20.153598, mean_eps: 0.704266
  49425/150000: episode: 495, duration: 0.695s, episode steps:  98, steps per second: 141, episode reward: -60.610, mean reward: -0.618 [-100.000, 13.125], mean action: 1.480 [0.000, 3.000],  loss: 7.292863, mse: 2079.776317, mean_q: 21.347315, mean_eps: 0.703747
  49575/150000: episode: 496, duration: 1.017s, episode steps: 150, steps per second: 148, episode reward: -102.341, mean reward: -0.682 [-100.000,  7.111], mean action: 1.600 [0.000, 3.000],  loss: 5.806951, mse: 2097.831713, mean_q: 20.196655, mean_eps: 0.703003
  49699/150000: episode: 497, duration: 0.896s, episode steps: 124, steps per second: 138, episode reward: -208.245, mean reward: -1.679 [-100.000,  6.098], mean action: 1.677 [0.000, 3.000],  loss: 9.002132, mse: 2089.503291, mean_q: 21.840874, mean_eps: 0.702181
  49800/150000: episode: 498, duration: 0.679s, episode steps: 101, steps per second: 149, episode reward: -97.145, mean reward: -0.962 [-100.000,  8.866], mean action: 1.505 [0.000, 3.000],  loss: 8.511911, mse: 2076.686168, mean_q: 22.040150, mean_eps: 0.701506
  49893/150000: episode: 499, duration: 0.638s, episode steps:  93, steps per second: 146, episode reward: -19.977, mean reward: -0.215 [-100.000, 22.738], mean action: 1.763 [0.000, 3.000],  loss: 5.507201, mse: 2103.953683, mean_q: 22.350118, mean_eps: 0.700924
  49965/150000: episode: 500, duration: 0.513s, episode steps:  72, steps per second: 140, episode reward: -75.405, mean reward: -1.047 [-100.000, 35.841], mean action: 1.597 [0.000, 3.000],  loss: 5.195809, mse: 2087.704324, mean_q: 22.759123, mean_eps: 0.700429
  50110/150000: episode: 501, duration: 0.973s, episode steps: 145, steps per second: 149, episode reward: -63.849, mean reward: -0.440 [-100.000,  9.460], mean action: 1.628 [0.000, 3.000],  loss: 10.443249, mse: 2074.375434, mean_q: 23.478895, mean_eps: 0.699778
  50242/150000: episode: 502, duration: 0.887s, episode steps: 132, steps per second: 149, episode reward: -56.887, mean reward: -0.431 [-100.000, 13.233], mean action: 1.508 [0.000, 3.000],  loss: 7.307014, mse: 2086.299187, mean_q: 23.207949, mean_eps: 0.698947
  50394/150000: episode: 503, duration: 1.026s, episode steps: 152, steps per second: 148, episode reward: -44.385, mean reward: -0.292 [-100.000, 11.886], mean action: 1.553 [0.000, 3.000],  loss: 8.994576, mse: 2091.430048, mean_q: 23.402732, mean_eps: 0.698095
  50518/150000: episode: 504, duration: 0.853s, episode steps: 124, steps per second: 145, episode reward: -59.911, mean reward: -0.483 [-100.000,  9.592], mean action: 1.815 [0.000, 3.000],  loss: 9.877485, mse: 2111.822333, mean_q: 23.436215, mean_eps: 0.697267
  50619/150000: episode: 505, duration: 0.752s, episode steps: 101, steps per second: 134, episode reward: -78.989, mean reward: -0.782 [-100.000,  5.316], mean action: 1.713 [0.000, 3.000],  loss: 8.468015, mse: 2186.035484, mean_q: 23.301149, mean_eps: 0.696592
  50745/150000: episode: 506, duration: 1.022s, episode steps: 126, steps per second: 123, episode reward: -156.270, mean reward: -1.240 [-100.000, 19.007], mean action: 1.794 [0.000, 3.000],  loss: 10.548898, mse: 2155.482154, mean_q: 23.078979, mean_eps: 0.695911
  50901/150000: episode: 507, duration: 1.204s, episode steps: 156, steps per second: 130, episode reward: -74.403, mean reward: -0.477 [-100.000, 11.710], mean action: 1.474 [0.000, 3.000],  loss: 10.802211, mse: 2163.229818, mean_q: 23.877852, mean_eps: 0.695065
  51068/150000: episode: 508, duration: 1.256s, episode steps: 167, steps per second: 133, episode reward: -130.290, mean reward: -0.780 [-100.000,  6.954], mean action: 1.617 [0.000, 3.000],  loss: 7.466810, mse: 2213.358662, mean_q: 24.380449, mean_eps: 0.694096
  51186/150000: episode: 509, duration: 1.035s, episode steps: 118, steps per second: 114, episode reward: -5.960, mean reward: -0.051 [-100.000, 60.571], mean action: 1.542 [0.000, 3.000],  loss: 5.447154, mse: 2254.286835, mean_q: 23.379196, mean_eps: 0.693241
  51299/150000: episode: 510, duration: 0.841s, episode steps: 113, steps per second: 134, episode reward: -123.479, mean reward: -1.093 [-100.000, 27.711], mean action: 1.584 [0.000, 3.000],  loss: 5.248690, mse: 2284.821590, mean_q: 24.010689, mean_eps: 0.692548
  51409/150000: episode: 511, duration: 0.767s, episode steps: 110, steps per second: 143, episode reward: -87.410, mean reward: -0.795 [-100.000, 18.289], mean action: 1.700 [0.000, 3.000],  loss: 6.723086, mse: 2299.732164, mean_q: 24.602866, mean_eps: 0.691879
  51544/150000: episode: 512, duration: 0.905s, episode steps: 135, steps per second: 149, episode reward: -74.264, mean reward: -0.550 [-100.000, 10.015], mean action: 1.674 [0.000, 3.000],  loss: 10.260680, mse: 2189.146257, mean_q: 22.920510, mean_eps: 0.691144
  51667/150000: episode: 513, duration: 0.841s, episode steps: 123, steps per second: 146, episode reward: -87.946, mean reward: -0.715 [-100.000, 21.634], mean action: 1.691 [0.000, 3.000],  loss: 7.854809, mse: 2167.714190, mean_q: 24.081089, mean_eps: 0.690370
  51744/150000: episode: 514, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -29.941, mean reward: -0.389 [-100.000, 13.442], mean action: 1.675 [0.000, 3.000],  loss: 6.696089, mse: 2117.262760, mean_q: 22.508441, mean_eps: 0.689770
  51840/150000: episode: 515, duration: 0.645s, episode steps:  96, steps per second: 149, episode reward: -57.122, mean reward: -0.595 [-100.000, 16.001], mean action: 1.479 [0.000, 3.000],  loss: 8.352996, mse: 2126.185804, mean_q: 24.931787, mean_eps: 0.689251
  51924/150000: episode: 516, duration: 0.579s, episode steps:  84, steps per second: 145, episode reward: -82.752, mean reward: -0.985 [-100.000,  6.282], mean action: 1.690 [0.000, 3.000],  loss: 10.545840, mse: 2215.197109, mean_q: 24.641057, mean_eps: 0.688711
  52074/150000: episode: 517, duration: 1.055s, episode steps: 150, steps per second: 142, episode reward:  7.245, mean reward:  0.048 [-100.000, 16.588], mean action: 1.653 [0.000, 3.000],  loss: 5.815411, mse: 2206.597779, mean_q: 25.604082, mean_eps: 0.688009
  52157/150000: episode: 518, duration: 0.552s, episode steps:  83, steps per second: 150, episode reward: -88.091, mean reward: -1.061 [-100.000,  3.952], mean action: 1.663 [0.000, 3.000],  loss: 6.739323, mse: 2183.377831, mean_q: 24.436810, mean_eps: 0.687310
  52227/150000: episode: 519, duration: 0.471s, episode steps:  70, steps per second: 149, episode reward: -47.764, mean reward: -0.682 [-100.000, 11.193], mean action: 1.643 [0.000, 3.000],  loss: 4.840533, mse: 2214.778646, mean_q: 25.167609, mean_eps: 0.686851
  52348/150000: episode: 520, duration: 0.841s, episode steps: 121, steps per second: 144, episode reward: -48.112, mean reward: -0.398 [-100.000,  6.034], mean action: 1.810 [0.000, 3.000],  loss: 4.724455, mse: 2249.550612, mean_q: 25.699620, mean_eps: 0.686278
  52438/150000: episode: 521, duration: 0.601s, episode steps:  90, steps per second: 150, episode reward: -38.672, mean reward: -0.430 [-100.000, 12.150], mean action: 1.811 [0.000, 3.000],  loss: 4.848860, mse: 2287.069720, mean_q: 26.291086, mean_eps: 0.685645
  52561/150000: episode: 522, duration: 0.820s, episode steps: 123, steps per second: 150, episode reward: -46.174, mean reward: -0.375 [-100.000,  6.882], mean action: 1.455 [0.000, 3.000],  loss: 5.242044, mse: 2259.843446, mean_q: 26.278450, mean_eps: 0.685006
  52673/150000: episode: 523, duration: 0.784s, episode steps: 112, steps per second: 143, episode reward: -74.567, mean reward: -0.666 [-100.000, 15.697], mean action: 1.732 [0.000, 3.000],  loss: 6.721589, mse: 2305.508039, mean_q: 26.184046, mean_eps: 0.684301
  52788/150000: episode: 524, duration: 0.774s, episode steps: 115, steps per second: 149, episode reward: -20.289, mean reward: -0.176 [-100.000, 25.519], mean action: 1.687 [0.000, 3.000],  loss: 7.713109, mse: 2249.430770, mean_q: 26.959862, mean_eps: 0.683620
  52876/150000: episode: 525, duration: 0.593s, episode steps:  88, steps per second: 148, episode reward: -42.448, mean reward: -0.482 [-100.000, 16.181], mean action: 1.511 [0.000, 3.000],  loss: 12.822774, mse: 2235.103586, mean_q: 26.049276, mean_eps: 0.683011
  52962/150000: episode: 526, duration: 0.618s, episode steps:  86, steps per second: 139, episode reward: -55.431, mean reward: -0.645 [-100.000, 24.005], mean action: 1.570 [0.000, 3.000],  loss: 4.228149, mse: 2236.471064, mean_q: 25.918029, mean_eps: 0.682489
  53056/150000: episode: 527, duration: 0.634s, episode steps:  94, steps per second: 148, episode reward: -39.717, mean reward: -0.423 [-100.000, 10.609], mean action: 1.809 [0.000, 3.000],  loss: 10.920613, mse: 2242.592564, mean_q: 25.805198, mean_eps: 0.681949
  53190/150000: episode: 528, duration: 0.894s, episode steps: 134, steps per second: 150, episode reward: 14.920, mean reward:  0.111 [-100.000, 71.272], mean action: 1.634 [0.000, 3.000],  loss: 6.466610, mse: 2227.492805, mean_q: 25.846188, mean_eps: 0.681265
  53330/150000: episode: 529, duration: 0.981s, episode steps: 140, steps per second: 143, episode reward: -63.114, mean reward: -0.451 [-100.000, 10.770], mean action: 1.564 [0.000, 3.000],  loss: 16.574014, mse: 2249.147028, mean_q: 26.267947, mean_eps: 0.680443
  53396/150000: episode: 530, duration: 0.469s, episode steps:  66, steps per second: 141, episode reward: -81.632, mean reward: -1.237 [-100.000, 17.598], mean action: 1.712 [0.000, 3.000],  loss: 10.024013, mse: 2303.756997, mean_q: 26.032521, mean_eps: 0.679825
  53541/150000: episode: 531, duration: 0.978s, episode steps: 145, steps per second: 148, episode reward: -84.275, mean reward: -0.581 [-100.000,  7.942], mean action: 1.593 [0.000, 3.000],  loss: 14.138064, mse: 2303.609251, mean_q: 26.648283, mean_eps: 0.679192
  53664/150000: episode: 532, duration: 0.890s, episode steps: 123, steps per second: 138, episode reward: -53.512, mean reward: -0.435 [-100.000,  6.553], mean action: 1.610 [0.000, 3.000],  loss: 6.667473, mse: 2325.938235, mean_q: 26.372604, mean_eps: 0.678388
  53738/150000: episode: 533, duration: 0.503s, episode steps:  74, steps per second: 147, episode reward:  0.057, mean reward:  0.001 [-100.000, 22.653], mean action: 1.622 [0.000, 3.000],  loss: 9.701097, mse: 2375.645986, mean_q: 28.501948, mean_eps: 0.677797
  53888/150000: episode: 534, duration: 1.019s, episode steps: 150, steps per second: 147, episode reward: -12.713, mean reward: -0.085 [-100.000, 12.952], mean action: 1.533 [0.000, 3.000],  loss: 10.824965, mse: 2355.749500, mean_q: 27.720371, mean_eps: 0.677125
  53977/150000: episode: 535, duration: 0.669s, episode steps:  89, steps per second: 133, episode reward: -51.144, mean reward: -0.575 [-100.000,  8.503], mean action: 1.596 [0.000, 3.000],  loss: 10.452439, mse: 2347.877985, mean_q: 26.640093, mean_eps: 0.676408
  54082/150000: episode: 536, duration: 0.730s, episode steps: 105, steps per second: 144, episode reward: -50.078, mean reward: -0.477 [-100.000, 12.608], mean action: 1.752 [0.000, 3.000],  loss: 9.347306, mse: 2315.837091, mean_q: 26.545068, mean_eps: 0.675826
  54158/150000: episode: 537, duration: 0.509s, episode steps:  76, steps per second: 149, episode reward: -58.446, mean reward: -0.769 [-100.000,  7.353], mean action: 1.724 [0.000, 3.000],  loss: 7.752335, mse: 2309.562612, mean_q: 26.466525, mean_eps: 0.675283
  54245/150000: episode: 538, duration: 0.633s, episode steps:  87, steps per second: 137, episode reward: -40.665, mean reward: -0.467 [-100.000,  6.618], mean action: 1.885 [0.000, 3.000],  loss: 8.011574, mse: 2337.996701, mean_q: 24.532835, mean_eps: 0.674794
  54384/150000: episode: 539, duration: 0.930s, episode steps: 139, steps per second: 150, episode reward: -208.828, mean reward: -1.502 [-100.000,  7.639], mean action: 1.705 [0.000, 3.000],  loss: 5.019818, mse: 2330.205977, mean_q: 25.885080, mean_eps: 0.674116
  54505/150000: episode: 540, duration: 0.838s, episode steps: 121, steps per second: 144, episode reward: 29.394, mean reward:  0.243 [-100.000, 73.854], mean action: 1.587 [0.000, 3.000],  loss: 8.666922, mse: 2286.360951, mean_q: 25.905518, mean_eps: 0.673336
  54630/150000: episode: 541, duration: 0.862s, episode steps: 125, steps per second: 145, episode reward: -81.959, mean reward: -0.656 [-100.000,  9.753], mean action: 1.728 [0.000, 3.000],  loss: 9.557812, mse: 2290.441860, mean_q: 26.182030, mean_eps: 0.672598
  54734/150000: episode: 542, duration: 0.698s, episode steps: 104, steps per second: 149, episode reward: -79.560, mean reward: -0.765 [-100.000, 13.378], mean action: 1.683 [0.000, 3.000],  loss: 8.143234, mse: 2350.305800, mean_q: 26.591828, mean_eps: 0.671911
  54800/150000: episode: 543, duration: 0.459s, episode steps:  66, steps per second: 144, episode reward: -77.745, mean reward: -1.178 [-100.000, 12.151], mean action: 1.682 [0.000, 3.000],  loss: 8.973454, mse: 2399.958232, mean_q: 26.076866, mean_eps: 0.671401
  54886/150000: episode: 544, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -73.619, mean reward: -0.856 [-100.000, 11.530], mean action: 1.628 [0.000, 3.000],  loss: 5.732115, mse: 2399.353022, mean_q: 24.736528, mean_eps: 0.670945
  54991/150000: episode: 545, duration: 0.714s, episode steps: 105, steps per second: 147, episode reward: -73.845, mean reward: -0.703 [-100.000, 10.614], mean action: 1.581 [0.000, 3.000],  loss: 5.561280, mse: 2324.813759, mean_q: 25.373047, mean_eps: 0.670372
  55104/150000: episode: 546, duration: 0.764s, episode steps: 113, steps per second: 148, episode reward: -73.683, mean reward: -0.652 [-100.000, 11.660], mean action: 1.761 [0.000, 3.000],  loss: 4.291743, mse: 2238.277348, mean_q: 24.528413, mean_eps: 0.669718
  55252/150000: episode: 547, duration: 1.034s, episode steps: 148, steps per second: 143, episode reward: -34.085, mean reward: -0.230 [-100.000, 20.876], mean action: 1.615 [0.000, 3.000],  loss: 5.527490, mse: 2230.525250, mean_q: 23.249585, mean_eps: 0.668935
  55333/150000: episode: 548, duration: 0.554s, episode steps:  81, steps per second: 146, episode reward: -94.633, mean reward: -1.168 [-100.000, 11.444], mean action: 1.506 [0.000, 3.000],  loss: 4.550911, mse: 2226.132772, mean_q: 24.051430, mean_eps: 0.668248
  55454/150000: episode: 549, duration: 0.848s, episode steps: 121, steps per second: 143, episode reward: -72.183, mean reward: -0.597 [-100.000, 14.938], mean action: 1.661 [0.000, 3.000],  loss: 8.382941, mse: 2198.338324, mean_q: 23.365261, mean_eps: 0.667642
  55566/150000: episode: 550, duration: 0.755s, episode steps: 112, steps per second: 148, episode reward: -42.492, mean reward: -0.379 [-100.000, 28.534], mean action: 1.688 [0.000, 3.000],  loss: 4.665092, mse: 2202.069759, mean_q: 24.091598, mean_eps: 0.666943
  55687/150000: episode: 551, duration: 0.818s, episode steps: 121, steps per second: 148, episode reward: -98.909, mean reward: -0.817 [-100.000,  6.751], mean action: 1.554 [0.000, 3.000],  loss: 6.846732, mse: 2185.499796, mean_q: 23.820514, mean_eps: 0.666244
  55849/150000: episode: 552, duration: 1.114s, episode steps: 162, steps per second: 145, episode reward: -93.116, mean reward: -0.575 [-100.000, 11.040], mean action: 1.722 [0.000, 3.000],  loss: 8.581554, mse: 2234.326375, mean_q: 23.810976, mean_eps: 0.665395
  55948/150000: episode: 553, duration: 0.667s, episode steps:  99, steps per second: 148, episode reward: -108.022, mean reward: -1.091 [-100.000,  9.886], mean action: 1.667 [0.000, 3.000],  loss: 9.077501, mse: 2253.792734, mean_q: 23.783745, mean_eps: 0.664612
  56040/150000: episode: 554, duration: 0.627s, episode steps:  92, steps per second: 147, episode reward: -84.077, mean reward: -0.914 [-100.000, 21.471], mean action: 1.804 [0.000, 3.000],  loss: 8.811450, mse: 2248.499601, mean_q: 23.036558, mean_eps: 0.664039
  56150/150000: episode: 555, duration: 0.768s, episode steps: 110, steps per second: 143, episode reward: -20.006, mean reward: -0.182 [-100.000, 19.350], mean action: 1.636 [0.000, 3.000],  loss: 6.887262, mse: 2238.869917, mean_q: 21.879429, mean_eps: 0.663433
  56237/150000: episode: 556, duration: 0.581s, episode steps:  87, steps per second: 150, episode reward: -60.800, mean reward: -0.699 [-100.000, 22.216], mean action: 1.586 [0.000, 3.000],  loss: 9.693487, mse: 2284.636142, mean_q: 22.451726, mean_eps: 0.662842
  56321/150000: episode: 557, duration: 0.589s, episode steps:  84, steps per second: 143, episode reward: -54.532, mean reward: -0.649 [-100.000, 16.095], mean action: 1.690 [0.000, 3.000],  loss: 8.868318, mse: 2354.842067, mean_q: 21.155720, mean_eps: 0.662329
  56394/150000: episode: 558, duration: 0.517s, episode steps:  73, steps per second: 141, episode reward: -76.178, mean reward: -1.044 [-100.000,  6.936], mean action: 1.521 [0.000, 3.000],  loss: 7.383420, mse: 2380.890486, mean_q: 22.715371, mean_eps: 0.661858
  56515/150000: episode: 559, duration: 0.838s, episode steps: 121, steps per second: 144, episode reward: -42.201, mean reward: -0.349 [-100.000,  9.212], mean action: 1.876 [0.000, 3.000],  loss: 7.866782, mse: 2351.331029, mean_q: 22.507646, mean_eps: 0.661276
  56640/150000: episode: 560, duration: 0.835s, episode steps: 125, steps per second: 150, episode reward: -167.352, mean reward: -1.339 [-100.000, 57.061], mean action: 1.440 [0.000, 3.000],  loss: 5.934648, mse: 2298.316503, mean_q: 22.946849, mean_eps: 0.660538
  56723/150000: episode: 561, duration: 0.578s, episode steps:  83, steps per second: 144, episode reward: -94.602, mean reward: -1.140 [-100.000, 19.786], mean action: 1.783 [0.000, 3.000],  loss: 4.528629, mse: 2191.385554, mean_q: 22.113437, mean_eps: 0.659914
  56833/150000: episode: 562, duration: 0.745s, episode steps: 110, steps per second: 148, episode reward: -88.621, mean reward: -0.806 [-100.000, 19.280], mean action: 1.673 [0.000, 3.000],  loss: 13.076936, mse: 2170.258843, mean_q: 23.001886, mean_eps: 0.659335
  57018/150000: episode: 563, duration: 1.276s, episode steps: 185, steps per second: 145, episode reward: -69.444, mean reward: -0.375 [-100.000, 10.255], mean action: 1.741 [0.000, 3.000],  loss: 5.225898, mse: 2175.568206, mean_q: 23.282638, mean_eps: 0.658450
  57131/150000: episode: 564, duration: 0.819s, episode steps: 113, steps per second: 138, episode reward: -40.537, mean reward: -0.359 [-100.000,  8.862], mean action: 1.593 [0.000, 3.000],  loss: 6.047837, mse: 2191.197505, mean_q: 23.041191, mean_eps: 0.657556
  57220/150000: episode: 565, duration: 0.715s, episode steps:  89, steps per second: 125, episode reward: -70.276, mean reward: -0.790 [-100.000,  9.866], mean action: 1.899 [0.000, 3.000],  loss: 7.272494, mse: 2198.602405, mean_q: 24.616959, mean_eps: 0.656950
  57392/150000: episode: 566, duration: 1.233s, episode steps: 172, steps per second: 139, episode reward: -5.731, mean reward: -0.033 [-100.000, 21.625], mean action: 1.698 [0.000, 3.000],  loss: 6.105289, mse: 2256.129121, mean_q: 22.554141, mean_eps: 0.656167
  57555/150000: episode: 567, duration: 1.081s, episode steps: 163, steps per second: 151, episode reward: -83.730, mean reward: -0.514 [-100.000, 14.342], mean action: 1.699 [0.000, 3.000],  loss: 6.076044, mse: 2262.190060, mean_q: 23.650753, mean_eps: 0.655162
  57681/150000: episode: 568, duration: 0.880s, episode steps: 126, steps per second: 143, episode reward: -52.001, mean reward: -0.413 [-100.000, 18.135], mean action: 1.627 [0.000, 3.000],  loss: 8.605869, mse: 2328.571500, mean_q: 22.764067, mean_eps: 0.654295
  57819/150000: episode: 569, duration: 0.922s, episode steps: 138, steps per second: 150, episode reward: -45.938, mean reward: -0.333 [-100.000, 17.252], mean action: 1.580 [0.000, 3.000],  loss: 6.405173, mse: 2397.523730, mean_q: 24.532249, mean_eps: 0.653503
  57962/150000: episode: 570, duration: 0.997s, episode steps: 143, steps per second: 143, episode reward: -100.677, mean reward: -0.704 [-100.000,  9.487], mean action: 1.615 [0.000, 3.000],  loss: 13.124679, mse: 2393.438185, mean_q: 25.453663, mean_eps: 0.652660
  58115/150000: episode: 571, duration: 1.030s, episode steps: 153, steps per second: 149, episode reward: -193.298, mean reward: -1.263 [-100.000, 12.866], mean action: 1.588 [0.000, 3.000],  loss: 9.680155, mse: 2351.366958, mean_q: 25.338652, mean_eps: 0.651772
  58225/150000: episode: 572, duration: 0.778s, episode steps: 110, steps per second: 141, episode reward: -135.319, mean reward: -1.230 [-100.000,  5.793], mean action: 1.527 [0.000, 3.000],  loss: 12.594507, mse: 2348.002691, mean_q: 25.710305, mean_eps: 0.650983
  58349/150000: episode: 573, duration: 0.872s, episode steps: 124, steps per second: 142, episode reward: -72.047, mean reward: -0.581 [-100.000, 29.073], mean action: 1.613 [0.000, 3.000],  loss: 5.750160, mse: 2287.116198, mean_q: 24.754080, mean_eps: 0.650281
  58487/150000: episode: 574, duration: 0.929s, episode steps: 138, steps per second: 149, episode reward: -0.391, mean reward: -0.003 [-100.000, 66.359], mean action: 1.543 [0.000, 3.000],  loss: 6.731163, mse: 2300.211871, mean_q: 26.109006, mean_eps: 0.649495
  58584/150000: episode: 575, duration: 0.676s, episode steps:  97, steps per second: 143, episode reward: -62.434, mean reward: -0.644 [-100.000, 19.598], mean action: 1.660 [0.000, 3.000],  loss: 14.349469, mse: 2334.524510, mean_q: 25.278466, mean_eps: 0.648790
  58675/150000: episode: 576, duration: 0.620s, episode steps:  91, steps per second: 147, episode reward:  1.883, mean reward:  0.021 [-100.000, 12.881], mean action: 1.659 [0.000, 3.000],  loss: 10.615163, mse: 2276.619114, mean_q: 25.609444, mean_eps: 0.648226
  58834/150000: episode: 577, duration: 1.061s, episode steps: 159, steps per second: 150, episode reward: -41.813, mean reward: -0.263 [-100.000, 23.870], mean action: 1.686 [0.000, 3.000],  loss: 8.794959, mse: 2224.126294, mean_q: 25.134717, mean_eps: 0.647476
  59008/150000: episode: 578, duration: 1.214s, episode steps: 174, steps per second: 143, episode reward: -63.470, mean reward: -0.365 [-100.000, 10.760], mean action: 1.713 [0.000, 3.000],  loss: 10.033317, mse: 2247.930908, mean_q: 25.701069, mean_eps: 0.646477
  59145/150000: episode: 579, duration: 0.915s, episode steps: 137, steps per second: 150, episode reward: -56.648, mean reward: -0.413 [-100.000, 10.401], mean action: 1.693 [0.000, 3.000],  loss: 8.091688, mse: 2206.507338, mean_q: 24.760216, mean_eps: 0.645544
  59238/150000: episode: 580, duration: 0.644s, episode steps:  93, steps per second: 144, episode reward: -61.121, mean reward: -0.657 [-100.000, 22.334], mean action: 1.806 [0.000, 3.000],  loss: 7.426998, mse: 2235.916675, mean_q: 26.659411, mean_eps: 0.644854
  59321/150000: episode: 581, duration: 0.563s, episode steps:  83, steps per second: 147, episode reward: -20.526, mean reward: -0.247 [-100.000,  7.445], mean action: 1.578 [0.000, 3.000],  loss: 9.434477, mse: 2221.712642, mean_q: 24.481259, mean_eps: 0.644326
  59463/150000: episode: 582, duration: 0.945s, episode steps: 142, steps per second: 150, episode reward: -54.086, mean reward: -0.381 [-100.000, 36.399], mean action: 1.585 [0.000, 3.000],  loss: 5.395218, mse: 2208.545785, mean_q: 25.782706, mean_eps: 0.643651
  59607/150000: episode: 583, duration: 1.000s, episode steps: 144, steps per second: 144, episode reward: 12.604, mean reward:  0.088 [-100.000, 20.925], mean action: 1.715 [0.000, 3.000],  loss: 7.913657, mse: 2190.975080, mean_q: 25.314168, mean_eps: 0.642793
  59729/150000: episode: 584, duration: 0.825s, episode steps: 122, steps per second: 148, episode reward: -177.639, mean reward: -1.456 [-100.000, 43.465], mean action: 1.656 [0.000, 3.000],  loss: 8.723005, mse: 2186.999228, mean_q: 24.929964, mean_eps: 0.641995
  59913/150000: episode: 585, duration: 1.281s, episode steps: 184, steps per second: 144, episode reward: 25.783, mean reward:  0.140 [-100.000, 11.681], mean action: 1.663 [0.000, 3.000],  loss: 6.618179, mse: 2173.458235, mean_q: 24.651247, mean_eps: 0.641077
  60036/150000: episode: 586, duration: 0.883s, episode steps: 123, steps per second: 139, episode reward: -55.608, mean reward: -0.452 [-100.000,  7.087], mean action: 1.455 [0.000, 3.000],  loss: 15.434885, mse: 2176.971709, mean_q: 25.550640, mean_eps: 0.640156
  60103/150000: episode: 587, duration: 0.569s, episode steps:  67, steps per second: 118, episode reward: -61.628, mean reward: -0.920 [-100.000, 35.869], mean action: 1.612 [0.000, 3.000],  loss: 7.264680, mse: 2138.093245, mean_q: 25.975646, mean_eps: 0.639586
  60239/150000: episode: 588, duration: 1.066s, episode steps: 136, steps per second: 128, episode reward: -129.246, mean reward: -0.950 [-100.000,  8.365], mean action: 1.574 [0.000, 3.000],  loss: 7.261110, mse: 2168.348046, mean_q: 25.384667, mean_eps: 0.638977
  60369/150000: episode: 589, duration: 1.011s, episode steps: 130, steps per second: 129, episode reward: -80.139, mean reward: -0.616 [-100.000,  7.175], mean action: 1.708 [0.000, 3.000],  loss: 6.670472, mse: 2166.360660, mean_q: 25.962089, mean_eps: 0.638179
  60472/150000: episode: 590, duration: 0.878s, episode steps: 103, steps per second: 117, episode reward: -68.070, mean reward: -0.661 [-100.000,  9.841], mean action: 1.718 [0.000, 3.000],  loss: 4.689009, mse: 2144.760032, mean_q: 26.675505, mean_eps: 0.637480
  60598/150000: episode: 591, duration: 0.978s, episode steps: 126, steps per second: 129, episode reward: -18.137, mean reward: -0.144 [-100.000, 14.160], mean action: 1.714 [0.000, 3.000],  loss: 8.938851, mse: 2178.250684, mean_q: 26.473706, mean_eps: 0.636793
  60699/150000: episode: 592, duration: 0.798s, episode steps: 101, steps per second: 127, episode reward: -98.834, mean reward: -0.979 [-100.000,  9.416], mean action: 1.515 [0.000, 3.000],  loss: 6.016038, mse: 2215.180388, mean_q: 27.362592, mean_eps: 0.636112
  60811/150000: episode: 593, duration: 0.781s, episode steps: 112, steps per second: 143, episode reward: -130.415, mean reward: -1.164 [-100.000, 11.063], mean action: 1.670 [0.000, 3.000],  loss: 5.851413, mse: 2196.433088, mean_q: 26.472576, mean_eps: 0.635473
  60912/150000: episode: 594, duration: 0.760s, episode steps: 101, steps per second: 133, episode reward: -94.011, mean reward: -0.931 [-100.000,  6.803], mean action: 1.683 [0.000, 3.000],  loss: 5.914012, mse: 2183.616268, mean_q: 26.737257, mean_eps: 0.634834
  61107/150000: episode: 595, duration: 1.372s, episode steps: 195, steps per second: 142, episode reward: -18.807, mean reward: -0.096 [-100.000, 12.695], mean action: 1.713 [0.000, 3.000],  loss: 5.058487, mse: 2161.861963, mean_q: 26.552324, mean_eps: 0.633946
  61177/150000: episode: 596, duration: 0.466s, episode steps:  70, steps per second: 150, episode reward: -43.456, mean reward: -0.621 [-100.000,  8.444], mean action: 1.771 [0.000, 3.000],  loss: 8.973773, mse: 2181.929768, mean_q: 26.727058, mean_eps: 0.633151
  61311/150000: episode: 597, duration: 0.939s, episode steps: 134, steps per second: 143, episode reward: -77.193, mean reward: -0.576 [-100.000,  5.736], mean action: 1.739 [0.000, 3.000],  loss: 8.217624, mse: 2222.908029, mean_q: 27.462035, mean_eps: 0.632539
  61458/150000: episode: 598, duration: 0.991s, episode steps: 147, steps per second: 148, episode reward: -51.865, mean reward: -0.353 [-100.000, 10.917], mean action: 1.633 [0.000, 3.000],  loss: 6.800374, mse: 2235.065807, mean_q: 26.673383, mean_eps: 0.631696
  61598/150000: episode: 599, duration: 0.990s, episode steps: 140, steps per second: 141, episode reward: -26.468, mean reward: -0.189 [-100.000, 14.716], mean action: 1.693 [0.000, 3.000],  loss: 8.358052, mse: 2297.889142, mean_q: 27.905722, mean_eps: 0.630835
  61731/150000: episode: 600, duration: 0.898s, episode steps: 133, steps per second: 148, episode reward: -56.628, mean reward: -0.426 [-100.000, 14.253], mean action: 1.722 [0.000, 3.000],  loss: 9.614255, mse: 2238.624775, mean_q: 28.462229, mean_eps: 0.630016
  61829/150000: episode: 601, duration: 0.652s, episode steps:  98, steps per second: 150, episode reward: -64.589, mean reward: -0.659 [-100.000,  6.304], mean action: 1.561 [0.000, 3.000],  loss: 8.340744, mse: 2252.828942, mean_q: 26.900594, mean_eps: 0.629323
  61951/150000: episode: 602, duration: 0.853s, episode steps: 122, steps per second: 143, episode reward: -113.865, mean reward: -0.933 [-100.000,  8.057], mean action: 1.656 [0.000, 3.000],  loss: 9.064610, mse: 2195.931846, mean_q: 27.459405, mean_eps: 0.628663
  62145/150000: episode: 603, duration: 1.300s, episode steps: 194, steps per second: 149, episode reward: -45.977, mean reward: -0.237 [-100.000,  9.616], mean action: 1.613 [0.000, 3.000],  loss: 8.110877, mse: 2150.206544, mean_q: 26.761252, mean_eps: 0.627715
  62285/150000: episode: 604, duration: 0.974s, episode steps: 140, steps per second: 144, episode reward: -37.449, mean reward: -0.267 [-100.000, 10.770], mean action: 1.679 [0.000, 3.000],  loss: 10.061404, mse: 2092.408347, mean_q: 26.790302, mean_eps: 0.626713
  62387/150000: episode: 605, duration: 0.685s, episode steps: 102, steps per second: 149, episode reward: -84.088, mean reward: -0.824 [-100.000,  9.174], mean action: 1.676 [0.000, 3.000],  loss: 6.715812, mse: 2064.503846, mean_q: 25.677508, mean_eps: 0.625987
  62537/150000: episode: 606, duration: 1.093s, episode steps: 150, steps per second: 137, episode reward: -196.897, mean reward: -1.313 [-100.000, 34.945], mean action: 1.460 [0.000, 3.000],  loss: 9.788970, mse: 2080.786554, mean_q: 27.043677, mean_eps: 0.625231
  62656/150000: episode: 607, duration: 0.832s, episode steps: 119, steps per second: 143, episode reward: -107.357, mean reward: -0.902 [-100.000, 11.731], mean action: 1.689 [0.000, 3.000],  loss: 4.470687, mse: 2084.836164, mean_q: 26.439983, mean_eps: 0.624424
  62788/150000: episode: 608, duration: 0.878s, episode steps: 132, steps per second: 150, episode reward: -51.645, mean reward: -0.391 [-100.000, 11.374], mean action: 1.636 [0.000, 3.000],  loss: 8.023212, mse: 2176.284328, mean_q: 27.529456, mean_eps: 0.623671
  62947/150000: episode: 609, duration: 1.086s, episode steps: 159, steps per second: 146, episode reward: -33.105, mean reward: -0.208 [-100.000,  7.995], mean action: 1.642 [0.000, 3.000],  loss: 19.395589, mse: 2164.333460, mean_q: 28.288796, mean_eps: 0.622798
  63137/150000: episode: 610, duration: 1.284s, episode steps: 190, steps per second: 148, episode reward: -71.204, mean reward: -0.375 [-100.000, 11.761], mean action: 1.500 [0.000, 3.000],  loss: 9.440868, mse: 2179.864726, mean_q: 28.707013, mean_eps: 0.621751
  63226/150000: episode: 611, duration: 0.614s, episode steps:  89, steps per second: 145, episode reward: -31.712, mean reward: -0.356 [-100.000, 13.728], mean action: 1.809 [0.000, 3.000],  loss: 12.336362, mse: 2171.611690, mean_q: 28.626561, mean_eps: 0.620914
  63340/150000: episode: 612, duration: 0.766s, episode steps: 114, steps per second: 149, episode reward: -57.075, mean reward: -0.501 [-100.000,  8.945], mean action: 1.833 [0.000, 3.000],  loss: 7.897621, mse: 2226.609045, mean_q: 28.872109, mean_eps: 0.620305
  63447/150000: episode: 613, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -47.470, mean reward: -0.444 [-100.000, 13.268], mean action: 1.832 [0.000, 3.000],  loss: 6.044664, mse: 2176.604020, mean_q: 27.069746, mean_eps: 0.619642
  63520/150000: episode: 614, duration: 0.509s, episode steps:  73, steps per second: 143, episode reward: -57.952, mean reward: -0.794 [-100.000,  7.302], mean action: 1.329 [0.000, 3.000],  loss: 4.354027, mse: 2210.531330, mean_q: 28.177311, mean_eps: 0.619102
  63645/150000: episode: 615, duration: 0.856s, episode steps: 125, steps per second: 146, episode reward: -74.997, mean reward: -0.600 [-100.000, 20.031], mean action: 1.496 [0.000, 3.000],  loss: 4.981821, mse: 2160.893736, mean_q: 28.920373, mean_eps: 0.618508
  63773/150000: episode: 616, duration: 0.913s, episode steps: 128, steps per second: 140, episode reward: -92.184, mean reward: -0.720 [-100.000, 10.196], mean action: 1.625 [0.000, 3.000],  loss: 7.305081, mse: 2125.561403, mean_q: 28.274342, mean_eps: 0.617749
  63969/150000: episode: 617, duration: 1.359s, episode steps: 196, steps per second: 144, episode reward: -92.730, mean reward: -0.473 [-100.000, 12.828], mean action: 1.638 [0.000, 3.000],  loss: 4.526467, mse: 2143.458359, mean_q: 28.281562, mean_eps: 0.616777
  64121/150000: episode: 618, duration: 1.059s, episode steps: 152, steps per second: 144, episode reward: -16.613, mean reward: -0.109 [-100.000, 15.192], mean action: 1.717 [0.000, 3.000],  loss: 5.586858, mse: 2184.678285, mean_q: 28.801304, mean_eps: 0.615733
  64204/150000: episode: 619, duration: 0.560s, episode steps:  83, steps per second: 148, episode reward: -87.789, mean reward: -1.058 [-100.000, 14.838], mean action: 1.687 [0.000, 3.000],  loss: 5.745311, mse: 2223.858393, mean_q: 28.351818, mean_eps: 0.615028
  64330/150000: episode: 620, duration: 0.849s, episode steps: 126, steps per second: 148, episode reward: -85.778, mean reward: -0.681 [-100.000,  7.931], mean action: 1.603 [0.000, 3.000],  loss: 4.017414, mse: 2255.889472, mean_q: 29.451097, mean_eps: 0.614401
  64505/150000: episode: 621, duration: 1.216s, episode steps: 175, steps per second: 144, episode reward: -7.229, mean reward: -0.041 [-100.000, 11.709], mean action: 1.657 [0.000, 3.000],  loss: 8.700810, mse: 2249.105310, mean_q: 29.210847, mean_eps: 0.613498
  64592/150000: episode: 622, duration: 0.583s, episode steps:  87, steps per second: 149, episode reward: -36.182, mean reward: -0.416 [-100.000, 13.714], mean action: 1.770 [0.000, 3.000],  loss: 4.748192, mse: 2281.266207, mean_q: 30.203050, mean_eps: 0.612712
  64707/150000: episode: 623, duration: 0.786s, episode steps: 115, steps per second: 146, episode reward: -8.821, mean reward: -0.077 [-100.000, 16.642], mean action: 1.817 [0.000, 3.000],  loss: 4.714301, mse: 2273.804034, mean_q: 28.733610, mean_eps: 0.612106
  64810/150000: episode: 624, duration: 0.706s, episode steps: 103, steps per second: 146, episode reward: -56.014, mean reward: -0.544 [-100.000, 11.977], mean action: 1.631 [0.000, 3.000],  loss: 8.691389, mse: 2330.521887, mean_q: 29.229903, mean_eps: 0.611452
  64971/150000: episode: 625, duration: 1.082s, episode steps: 161, steps per second: 149, episode reward: -53.576, mean reward: -0.333 [-100.000, 13.311], mean action: 1.702 [0.000, 3.000],  loss: 5.888600, mse: 2317.519925, mean_q: 29.756655, mean_eps: 0.610660
  65126/150000: episode: 626, duration: 1.064s, episode steps: 155, steps per second: 146, episode reward: -37.937, mean reward: -0.245 [-100.000, 18.674], mean action: 1.555 [0.000, 3.000],  loss: 5.328410, mse: 2396.670510, mean_q: 30.610442, mean_eps: 0.609712
  65229/150000: episode: 627, duration: 0.689s, episode steps: 103, steps per second: 149, episode reward: 64.241, mean reward:  0.624 [-100.000, 48.245], mean action: 1.893 [0.000, 3.000],  loss: 3.939400, mse: 2465.801200, mean_q: 31.023255, mean_eps: 0.608938
  65344/150000: episode: 628, duration: 0.791s, episode steps: 115, steps per second: 145, episode reward: -77.441, mean reward: -0.673 [-100.000, 11.051], mean action: 1.678 [0.000, 3.000],  loss: 9.021850, mse: 2456.682813, mean_q: 30.345561, mean_eps: 0.608284
  65430/150000: episode: 629, duration: 0.593s, episode steps:  86, steps per second: 145, episode reward: -87.151, mean reward: -1.013 [-100.000,  7.385], mean action: 1.907 [0.000, 3.000],  loss: 5.495392, mse: 2462.640624, mean_q: 29.345517, mean_eps: 0.607681
  66430/150000: episode: 630, duration: 7.640s, episode steps: 1000, steps per second: 131, episode reward: 48.691, mean reward:  0.049 [-24.187, 22.598], mean action: 1.865 [0.000, 3.000],  loss: 7.257424, mse: 2460.585529, mean_q: 30.125922, mean_eps: 0.604423
  66557/150000: episode: 631, duration: 0.867s, episode steps: 127, steps per second: 146, episode reward: -15.379, mean reward: -0.121 [-100.000, 10.203], mean action: 1.520 [0.000, 3.000],  loss: 9.480944, mse: 2493.992216, mean_q: 30.162940, mean_eps: 0.601042
  66651/150000: episode: 632, duration: 0.628s, episode steps:  94, steps per second: 150, episode reward: -130.406, mean reward: -1.387 [-100.000, 13.092], mean action: 1.489 [0.000, 3.000],  loss: 5.700050, mse: 2565.441518, mean_q: 30.548042, mean_eps: 0.600379
  66772/150000: episode: 633, duration: 0.828s, episode steps: 121, steps per second: 146, episode reward: 22.913, mean reward:  0.189 [-100.000, 14.608], mean action: 1.793 [0.000, 3.000],  loss: 7.839313, mse: 2568.260082, mean_q: 31.391136, mean_eps: 0.599734
  66858/150000: episode: 634, duration: 0.621s, episode steps:  86, steps per second: 138, episode reward: -46.060, mean reward: -0.536 [-100.000, 20.805], mean action: 1.430 [0.000, 3.000],  loss: 5.944433, mse: 2551.776827, mean_q: 30.497906, mean_eps: 0.599113
  66984/150000: episode: 635, duration: 0.878s, episode steps: 126, steps per second: 143, episode reward: -38.679, mean reward: -0.307 [-100.000, 26.472], mean action: 1.389 [0.000, 3.000],  loss: 5.790272, mse: 2574.667535, mean_q: 30.441184, mean_eps: 0.598477
  67073/150000: episode: 636, duration: 0.594s, episode steps:  89, steps per second: 150, episode reward: -53.554, mean reward: -0.602 [-100.000, 18.280], mean action: 1.539 [0.000, 3.000],  loss: 5.522604, mse: 2553.381965, mean_q: 29.464874, mean_eps: 0.597832
  67225/150000: episode: 637, duration: 1.042s, episode steps: 152, steps per second: 146, episode reward: -30.911, mean reward: -0.203 [-100.000, 22.859], mean action: 1.546 [0.000, 3.000],  loss: 5.086188, mse: 2569.173123, mean_q: 31.107976, mean_eps: 0.597109
  67309/150000: episode: 638, duration: 0.561s, episode steps:  84, steps per second: 150, episode reward: -46.867, mean reward: -0.558 [-100.000, 16.806], mean action: 1.512 [0.000, 3.000],  loss: 11.642839, mse: 2544.904423, mean_q: 31.239326, mean_eps: 0.596401
  67482/150000: episode: 639, duration: 1.185s, episode steps: 173, steps per second: 146, episode reward:  4.224, mean reward:  0.024 [-100.000, 10.286], mean action: 1.636 [0.000, 3.000],  loss: 10.802103, mse: 2595.858519, mean_q: 30.958654, mean_eps: 0.595630
  67604/150000: episode: 640, duration: 0.830s, episode steps: 122, steps per second: 147, episode reward: -186.149, mean reward: -1.526 [-100.000, 44.585], mean action: 1.770 [0.000, 3.000],  loss: 5.602981, mse: 2622.561381, mean_q: 30.400170, mean_eps: 0.594745
  67719/150000: episode: 641, duration: 0.769s, episode steps: 115, steps per second: 149, episode reward: -31.232, mean reward: -0.272 [-100.000,  7.564], mean action: 1.635 [0.000, 3.000],  loss: 4.924420, mse: 2645.969336, mean_q: 32.235486, mean_eps: 0.594034
  67852/150000: episode: 642, duration: 0.915s, episode steps: 133, steps per second: 145, episode reward: 29.338, mean reward:  0.221 [-100.000, 49.005], mean action: 1.654 [0.000, 3.000],  loss: 4.172578, mse: 2700.031575, mean_q: 32.401925, mean_eps: 0.593290
  67979/150000: episode: 643, duration: 0.852s, episode steps: 127, steps per second: 149, episode reward: 21.028, mean reward:  0.166 [-100.000, 18.880], mean action: 1.906 [0.000, 3.000],  loss: 5.901165, mse: 2676.596819, mean_q: 31.447972, mean_eps: 0.592510
  68133/150000: episode: 644, duration: 1.053s, episode steps: 154, steps per second: 146, episode reward: -22.905, mean reward: -0.149 [-100.000, 11.989], mean action: 1.532 [0.000, 3.000],  loss: 7.388081, mse: 2649.496246, mean_q: 32.359769, mean_eps: 0.591667
  68242/150000: episode: 645, duration: 0.736s, episode steps: 109, steps per second: 148, episode reward: -81.635, mean reward: -0.749 [-100.000, 11.978], mean action: 1.606 [0.000, 3.000],  loss: 4.801370, mse: 2670.027515, mean_q: 33.592456, mean_eps: 0.590878
  68357/150000: episode: 646, duration: 0.771s, episode steps: 115, steps per second: 149, episode reward: -30.814, mean reward: -0.268 [-100.000, 10.729], mean action: 1.635 [0.000, 3.000],  loss: 5.538118, mse: 2658.793270, mean_q: 31.806157, mean_eps: 0.590206
  68472/150000: episode: 647, duration: 0.803s, episode steps: 115, steps per second: 143, episode reward: -7.941, mean reward: -0.069 [-100.000, 12.613], mean action: 1.713 [0.000, 3.000],  loss: 7.776920, mse: 2685.246758, mean_q: 33.891203, mean_eps: 0.589516
  68552/150000: episode: 648, duration: 0.543s, episode steps:  80, steps per second: 147, episode reward: -46.954, mean reward: -0.587 [-100.000, 13.114], mean action: 1.825 [0.000, 3.000],  loss: 8.359208, mse: 2655.261511, mean_q: 35.786346, mean_eps: 0.588931
  68775/150000: episode: 649, duration: 1.537s, episode steps: 223, steps per second: 145, episode reward: -74.808, mean reward: -0.335 [-100.000, 35.012], mean action: 1.637 [0.000, 3.000],  loss: 7.465544, mse: 2646.808320, mean_q: 34.517930, mean_eps: 0.588022
  68862/150000: episode: 650, duration: 0.585s, episode steps:  87, steps per second: 149, episode reward: -34.744, mean reward: -0.399 [-100.000, 13.999], mean action: 1.816 [0.000, 3.000],  loss: 8.182342, mse: 2590.788106, mean_q: 34.552311, mean_eps: 0.587092
  68974/150000: episode: 651, duration: 0.779s, episode steps: 112, steps per second: 144, episode reward: -8.026, mean reward: -0.072 [-100.000, 21.575], mean action: 1.848 [0.000, 3.000],  loss: 7.781303, mse: 2615.243428, mean_q: 35.354261, mean_eps: 0.586495
  69166/150000: episode: 652, duration: 1.334s, episode steps: 192, steps per second: 144, episode reward: -64.786, mean reward: -0.337 [-100.000, 43.427], mean action: 1.776 [0.000, 3.000],  loss: 8.149847, mse: 2677.161078, mean_q: 35.974277, mean_eps: 0.585583
  69328/150000: episode: 653, duration: 1.090s, episode steps: 162, steps per second: 149, episode reward: 29.859, mean reward:  0.184 [-100.000, 22.381], mean action: 1.593 [0.000, 3.000],  loss: 9.169337, mse: 2775.809314, mean_q: 36.220185, mean_eps: 0.584521
  69460/150000: episode: 654, duration: 1.038s, episode steps: 132, steps per second: 127, episode reward: -30.886, mean reward: -0.234 [-100.000, 15.489], mean action: 1.659 [0.000, 3.000],  loss: 7.733782, mse: 2847.949790, mean_q: 37.639477, mean_eps: 0.583639
  69586/150000: episode: 655, duration: 0.984s, episode steps: 126, steps per second: 128, episode reward: -67.288, mean reward: -0.534 [-100.000, 24.723], mean action: 1.746 [0.000, 3.000],  loss: 4.825588, mse: 2875.730792, mean_q: 36.759200, mean_eps: 0.582865
  69670/150000: episode: 656, duration: 0.677s, episode steps:  84, steps per second: 124, episode reward: -22.414, mean reward: -0.267 [-100.000,  6.871], mean action: 1.548 [0.000, 3.000],  loss: 5.312535, mse: 2881.365380, mean_q: 36.553843, mean_eps: 0.582235
  69809/150000: episode: 657, duration: 1.104s, episode steps: 139, steps per second: 126, episode reward: -52.829, mean reward: -0.380 [-100.000, 13.374], mean action: 1.676 [0.000, 3.000],  loss: 9.416601, mse: 2917.707058, mean_q: 38.100859, mean_eps: 0.581566
  69975/150000: episode: 658, duration: 1.452s, episode steps: 166, steps per second: 114, episode reward: -58.148, mean reward: -0.350 [-100.000, 71.847], mean action: 1.645 [0.000, 3.000],  loss: 7.785046, mse: 2923.563693, mean_q: 37.053668, mean_eps: 0.580651
  70094/150000: episode: 659, duration: 0.998s, episode steps: 119, steps per second: 119, episode reward: -102.064, mean reward: -0.858 [-100.000,  9.278], mean action: 1.605 [0.000, 3.000],  loss: 8.026315, mse: 2965.569198, mean_q: 37.324762, mean_eps: 0.579796
  70238/150000: episode: 660, duration: 1.032s, episode steps: 144, steps per second: 140, episode reward: -40.000, mean reward: -0.278 [-100.000,  9.946], mean action: 1.632 [0.000, 3.000],  loss: 11.268659, mse: 3013.429152, mean_q: 37.537717, mean_eps: 0.579007
  71238/150000: episode: 661, duration: 8.014s, episode steps: 1000, steps per second: 125, episode reward: -24.276, mean reward: -0.024 [-26.572, 41.110], mean action: 1.813 [0.000, 3.000],  loss: 8.964546, mse: 3059.959021, mean_q: 39.346282, mean_eps: 0.575575
  71811/150000: episode: 662, duration: 4.105s, episode steps: 573, steps per second: 140, episode reward: -252.518, mean reward: -0.441 [-100.000, 24.077], mean action: 1.761 [0.000, 3.000],  loss: 8.049493, mse: 3281.810811, mean_q: 41.812036, mean_eps: 0.570856
  71923/150000: episode: 663, duration: 0.759s, episode steps: 112, steps per second: 148, episode reward: -3.004, mean reward: -0.027 [-100.000, 15.362], mean action: 1.759 [0.000, 3.000],  loss: 10.760772, mse: 3353.158345, mean_q: 42.695730, mean_eps: 0.568801
  72022/150000: episode: 664, duration: 0.668s, episode steps:  99, steps per second: 148, episode reward: -42.851, mean reward: -0.433 [-100.000, 11.195], mean action: 1.788 [0.000, 3.000],  loss: 5.475710, mse: 3371.026234, mean_q: 43.050950, mean_eps: 0.568168
  72120/150000: episode: 665, duration: 0.668s, episode steps:  98, steps per second: 147, episode reward: -47.172, mean reward: -0.481 [-100.000, 12.672], mean action: 1.776 [0.000, 3.000],  loss: 12.782077, mse: 3389.733436, mean_q: 42.248804, mean_eps: 0.567577
  72316/150000: episode: 666, duration: 1.328s, episode steps: 196, steps per second: 148, episode reward: -64.704, mean reward: -0.330 [-100.000, 12.786], mean action: 1.622 [0.000, 3.000],  loss: 11.249405, mse: 3436.140547, mean_q: 43.325635, mean_eps: 0.566695
  72425/150000: episode: 667, duration: 0.729s, episode steps: 109, steps per second: 150, episode reward: -2.976, mean reward: -0.027 [-100.000, 28.639], mean action: 1.771 [0.000, 3.000],  loss: 13.577504, mse: 3422.002406, mean_q: 43.960136, mean_eps: 0.565780
  72578/150000: episode: 668, duration: 1.051s, episode steps: 153, steps per second: 146, episode reward: -49.284, mean reward: -0.322 [-100.000, 11.730], mean action: 1.732 [0.000, 3.000],  loss: 11.599177, mse: 3429.206747, mean_q: 43.075010, mean_eps: 0.564994
  72702/150000: episode: 669, duration: 0.821s, episode steps: 124, steps per second: 151, episode reward: -92.177, mean reward: -0.743 [-100.000,  9.773], mean action: 1.613 [0.000, 3.000],  loss: 11.885057, mse: 3386.534221, mean_q: 41.978963, mean_eps: 0.564163
  72843/150000: episode: 670, duration: 0.968s, episode steps: 141, steps per second: 146, episode reward: -43.183, mean reward: -0.306 [-100.000,  6.958], mean action: 1.582 [0.000, 3.000],  loss: 10.031229, mse: 3462.400077, mean_q: 43.019621, mean_eps: 0.563368
  72952/150000: episode: 671, duration: 0.744s, episode steps: 109, steps per second: 147, episode reward: -36.988, mean reward: -0.339 [-100.000, 11.822], mean action: 1.596 [0.000, 3.000],  loss: 8.821703, mse: 3478.162499, mean_q: 43.352864, mean_eps: 0.562618
  73080/150000: episode: 672, duration: 0.860s, episode steps: 128, steps per second: 149, episode reward: -79.749, mean reward: -0.623 [-100.000, 11.949], mean action: 1.672 [0.000, 3.000],  loss: 9.130262, mse: 3486.453041, mean_q: 43.396798, mean_eps: 0.561907
  73197/150000: episode: 673, duration: 0.808s, episode steps: 117, steps per second: 145, episode reward: -62.634, mean reward: -0.535 [-100.000,  9.825], mean action: 1.607 [0.000, 3.000],  loss: 11.059245, mse: 3584.152991, mean_q: 44.535835, mean_eps: 0.561172
  73279/150000: episode: 674, duration: 0.545s, episode steps:  82, steps per second: 151, episode reward: -60.821, mean reward: -0.742 [-100.000, 12.031], mean action: 1.720 [0.000, 3.000],  loss: 10.101814, mse: 3521.279782, mean_q: 44.978304, mean_eps: 0.560575
  73447/150000: episode: 675, duration: 1.155s, episode steps: 168, steps per second: 145, episode reward: -1.611, mean reward: -0.010 [-100.000, 23.019], mean action: 1.643 [0.000, 3.000],  loss: 8.888762, mse: 3471.945590, mean_q: 44.072646, mean_eps: 0.559825
  73597/150000: episode: 676, duration: 0.998s, episode steps: 150, steps per second: 150, episode reward: -31.291, mean reward: -0.209 [-100.000, 22.717], mean action: 1.527 [0.000, 3.000],  loss: 10.946165, mse: 3461.159582, mean_q: 44.376870, mean_eps: 0.558871
  73708/150000: episode: 677, duration: 0.739s, episode steps: 111, steps per second: 150, episode reward: -31.810, mean reward: -0.287 [-100.000, 11.831], mean action: 1.721 [0.000, 3.000],  loss: 13.004612, mse: 3383.832498, mean_q: 42.693111, mean_eps: 0.558088
  73835/150000: episode: 678, duration: 0.898s, episode steps: 127, steps per second: 141, episode reward: -77.366, mean reward: -0.609 [-100.000,  8.914], mean action: 1.661 [0.000, 3.000],  loss: 10.890858, mse: 3353.388678, mean_q: 43.739074, mean_eps: 0.557374
  73921/150000: episode: 679, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -30.690, mean reward: -0.357 [-100.000, 22.742], mean action: 1.802 [0.000, 3.000],  loss: 9.696740, mse: 3365.193359, mean_q: 43.380563, mean_eps: 0.556735
  74055/150000: episode: 680, duration: 0.909s, episode steps: 134, steps per second: 147, episode reward: 11.997, mean reward:  0.090 [-100.000, 17.335], mean action: 1.739 [0.000, 3.000],  loss: 12.040848, mse: 3390.435033, mean_q: 42.985191, mean_eps: 0.556075
  74169/150000: episode: 681, duration: 0.778s, episode steps: 114, steps per second: 146, episode reward: -247.930, mean reward: -2.175 [-100.000, 80.888], mean action: 1.614 [0.000, 3.000],  loss: 10.243373, mse: 3432.740562, mean_q: 43.732212, mean_eps: 0.555331
  74301/150000: episode: 682, duration: 0.892s, episode steps: 132, steps per second: 148, episode reward: -22.787, mean reward: -0.173 [-100.000, 18.781], mean action: 1.462 [0.000, 3.000],  loss: 22.710594, mse: 3452.316362, mean_q: 45.230204, mean_eps: 0.554593
  74468/150000: episode: 683, duration: 1.139s, episode steps: 167, steps per second: 147, episode reward:  7.816, mean reward:  0.047 [-100.000, 17.730], mean action: 1.515 [0.000, 3.000],  loss: 10.920214, mse: 3424.251232, mean_q: 45.950516, mean_eps: 0.553696
  74581/150000: episode: 684, duration: 0.759s, episode steps: 113, steps per second: 149, episode reward: -54.528, mean reward: -0.483 [-100.000, 16.452], mean action: 1.655 [0.000, 3.000],  loss: 13.141448, mse: 3390.173150, mean_q: 44.809054, mean_eps: 0.552856
  74674/150000: episode: 685, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -60.799, mean reward: -0.654 [-100.000,  7.792], mean action: 1.699 [0.000, 3.000],  loss: 8.937293, mse: 3455.326508, mean_q: 46.267637, mean_eps: 0.552238
  74782/150000: episode: 686, duration: 0.745s, episode steps: 108, steps per second: 145, episode reward: -36.147, mean reward: -0.335 [-100.000, 12.361], mean action: 1.796 [0.000, 3.000],  loss: 10.408575, mse: 3476.868915, mean_q: 46.817885, mean_eps: 0.551635
  74894/150000: episode: 687, duration: 0.744s, episode steps: 112, steps per second: 151, episode reward: -65.867, mean reward: -0.588 [-100.000, 12.049], mean action: 1.679 [0.000, 3.000],  loss: 12.199015, mse: 3443.039060, mean_q: 46.430251, mean_eps: 0.550975
  74992/150000: episode: 688, duration: 0.669s, episode steps:  98, steps per second: 146, episode reward: -25.604, mean reward: -0.261 [-100.000, 10.969], mean action: 1.694 [0.000, 3.000],  loss: 10.111817, mse: 3358.530565, mean_q: 46.772890, mean_eps: 0.550345
  75115/150000: episode: 689, duration: 0.846s, episode steps: 123, steps per second: 145, episode reward: -89.342, mean reward: -0.726 [-100.000, 14.506], mean action: 1.634 [0.000, 3.000],  loss: 10.867874, mse: 3599.775895, mean_q: 48.997790, mean_eps: 0.549682
  75264/150000: episode: 690, duration: 1.003s, episode steps: 149, steps per second: 149, episode reward: -8.195, mean reward: -0.055 [-100.000, 10.935], mean action: 1.671 [0.000, 3.000],  loss: 9.197518, mse: 3715.652662, mean_q: 50.205445, mean_eps: 0.548866
  76162/150000: episode: 691, duration: 6.602s, episode steps: 898, steps per second: 136, episode reward: -286.479, mean reward: -0.319 [-100.000, 28.242], mean action: 1.528 [0.000, 3.000],  loss: 13.864265, mse: 3769.373427, mean_q: 51.547465, mean_eps: 0.545725
  76276/150000: episode: 692, duration: 0.768s, episode steps: 114, steps per second: 148, episode reward: -101.776, mean reward: -0.893 [-100.000, 10.137], mean action: 1.491 [0.000, 3.000],  loss: 16.732649, mse: 3729.384127, mean_q: 52.028950, mean_eps: 0.542689
  76381/150000: episode: 693, duration: 0.704s, episode steps: 105, steps per second: 149, episode reward: -114.655, mean reward: -1.092 [-100.000,  5.456], mean action: 1.838 [0.000, 3.000],  loss: 11.872209, mse: 3817.238049, mean_q: 52.911819, mean_eps: 0.542032
  76502/150000: episode: 694, duration: 0.831s, episode steps: 121, steps per second: 146, episode reward: -21.360, mean reward: -0.177 [-100.000, 15.627], mean action: 1.727 [0.000, 3.000],  loss: 16.999237, mse: 3808.564465, mean_q: 53.134160, mean_eps: 0.541354
  76700/150000: episode: 695, duration: 1.342s, episode steps: 198, steps per second: 148, episode reward: -54.902, mean reward: -0.277 [-100.000, 17.278], mean action: 1.737 [0.000, 3.000],  loss: 12.786622, mse: 3812.282949, mean_q: 53.532527, mean_eps: 0.540397
  76802/150000: episode: 696, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -24.433, mean reward: -0.240 [-100.000, 11.304], mean action: 1.637 [0.000, 3.000],  loss: 12.287072, mse: 3769.666478, mean_q: 53.425822, mean_eps: 0.539497
  76939/150000: episode: 697, duration: 0.976s, episode steps: 137, steps per second: 140, episode reward: -7.249, mean reward: -0.053 [-100.000, 20.027], mean action: 1.759 [0.000, 3.000],  loss: 12.107179, mse: 3832.817766, mean_q: 53.055996, mean_eps: 0.538780
  77058/150000: episode: 698, duration: 0.797s, episode steps: 119, steps per second: 149, episode reward: -15.158, mean reward: -0.127 [-100.000, 34.918], mean action: 1.739 [0.000, 3.000],  loss: 13.122573, mse: 3872.918533, mean_q: 52.306858, mean_eps: 0.538012
  77239/150000: episode: 699, duration: 1.258s, episode steps: 181, steps per second: 144, episode reward: -52.466, mean reward: -0.290 [-100.000, 16.200], mean action: 1.856 [0.000, 3.000],  loss: 15.182299, mse: 3931.153806, mean_q: 53.873242, mean_eps: 0.537112
  77492/150000: episode: 700, duration: 1.753s, episode steps: 253, steps per second: 144, episode reward: -45.247, mean reward: -0.179 [-100.000, 13.754], mean action: 1.723 [0.000, 3.000],  loss: 16.188495, mse: 4086.711002, mean_q: 56.633860, mean_eps: 0.535810
  77586/150000: episode: 701, duration: 0.632s, episode steps:  94, steps per second: 149, episode reward: -26.890, mean reward: -0.286 [-100.000,  9.589], mean action: 1.819 [0.000, 3.000],  loss: 24.311499, mse: 4150.453613, mean_q: 57.040386, mean_eps: 0.534769
  77735/150000: episode: 702, duration: 0.993s, episode steps: 149, steps per second: 150, episode reward: 19.379, mean reward:  0.130 [-100.000, 50.994], mean action: 1.624 [0.000, 3.000],  loss: 16.196845, mse: 4178.477628, mean_q: 57.252000, mean_eps: 0.534040
  77845/150000: episode: 703, duration: 0.769s, episode steps: 110, steps per second: 143, episode reward: 11.178, mean reward:  0.102 [-100.000, 19.076], mean action: 1.764 [0.000, 3.000],  loss: 14.832396, mse: 4143.569866, mean_q: 56.834794, mean_eps: 0.533263
  77948/150000: episode: 704, duration: 0.692s, episode steps: 103, steps per second: 149, episode reward: -11.307, mean reward: -0.110 [-100.000, 18.234], mean action: 1.699 [0.000, 3.000],  loss: 13.079639, mse: 4220.251486, mean_q: 57.361621, mean_eps: 0.532624
  78103/150000: episode: 705, duration: 1.056s, episode steps: 155, steps per second: 147, episode reward: -18.077, mean reward: -0.117 [-100.000, 15.169], mean action: 1.568 [0.000, 3.000],  loss: 12.809286, mse: 4280.033397, mean_q: 57.052393, mean_eps: 0.531850
  78481/150000: episode: 706, duration: 2.680s, episode steps: 378, steps per second: 141, episode reward: -47.084, mean reward: -0.125 [-100.000, 17.735], mean action: 1.741 [0.000, 3.000],  loss: 13.455749, mse: 4385.412508, mean_q: 58.369218, mean_eps: 0.530251
  78667/150000: episode: 707, duration: 1.361s, episode steps: 186, steps per second: 137, episode reward: -5.665, mean reward: -0.030 [-100.000, 19.014], mean action: 1.640 [0.000, 3.000],  loss: 18.814071, mse: 4383.950791, mean_q: 58.071403, mean_eps: 0.528559
  78835/150000: episode: 708, duration: 1.414s, episode steps: 168, steps per second: 119, episode reward: 14.475, mean reward:  0.086 [-100.000, 14.108], mean action: 1.726 [0.000, 3.000],  loss: 15.187106, mse: 4495.311858, mean_q: 60.276821, mean_eps: 0.527497
  78940/150000: episode: 709, duration: 0.840s, episode steps: 105, steps per second: 125, episode reward: -8.034, mean reward: -0.077 [-100.000,  7.096], mean action: 1.638 [0.000, 3.000],  loss: 16.334483, mse: 4433.770382, mean_q: 58.593993, mean_eps: 0.526678
  79590/150000: episode: 710, duration: 5.091s, episode steps: 650, steps per second: 128, episode reward: -256.640, mean reward: -0.395 [-100.000, 21.320], mean action: 1.745 [0.000, 3.000],  loss: 16.261020, mse: 4572.666093, mean_q: 60.967479, mean_eps: 0.524413
  79742/150000: episode: 711, duration: 1.014s, episode steps: 152, steps per second: 150, episode reward: -49.805, mean reward: -0.328 [-100.000, 10.526], mean action: 1.645 [0.000, 3.000],  loss: 18.092824, mse: 4859.392498, mean_q: 63.566126, mean_eps: 0.522007
  79937/150000: episode: 712, duration: 1.325s, episode steps: 195, steps per second: 147, episode reward: -64.342, mean reward: -0.330 [-100.000, 13.014], mean action: 1.749 [0.000, 3.000],  loss: 15.367432, mse: 5057.523893, mean_q: 64.381763, mean_eps: 0.520966
  80219/150000: episode: 713, duration: 1.945s, episode steps: 282, steps per second: 145, episode reward: 10.764, mean reward:  0.038 [-100.000, 56.599], mean action: 1.557 [0.000, 3.000],  loss: 14.415764, mse: 5178.468726, mean_q: 64.914794, mean_eps: 0.519535
  80334/150000: episode: 714, duration: 0.766s, episode steps: 115, steps per second: 150, episode reward: -12.143, mean reward: -0.106 [-100.000,  9.018], mean action: 1.687 [0.000, 3.000],  loss: 20.405875, mse: 5265.213022, mean_q: 66.286595, mean_eps: 0.518344
  80509/150000: episode: 715, duration: 1.186s, episode steps: 175, steps per second: 148, episode reward: -8.880, mean reward: -0.051 [-100.000, 15.474], mean action: 1.663 [0.000, 3.000],  loss: 18.388888, mse: 5374.462977, mean_q: 66.858547, mean_eps: 0.517474
  80598/150000: episode: 716, duration: 0.600s, episode steps:  89, steps per second: 148, episode reward: -2.924, mean reward: -0.033 [-100.000, 16.521], mean action: 1.685 [0.000, 3.000],  loss: 15.211324, mse: 5688.995227, mean_q: 69.340988, mean_eps: 0.516682
  80706/150000: episode: 717, duration: 0.755s, episode steps: 108, steps per second: 143, episode reward: -49.781, mean reward: -0.461 [-100.000, 10.856], mean action: 1.750 [0.000, 3.000],  loss: 18.833876, mse: 5517.626985, mean_q: 67.762365, mean_eps: 0.516091
  80808/150000: episode: 718, duration: 0.729s, episode steps: 102, steps per second: 140, episode reward: -16.413, mean reward: -0.161 [-100.000, 11.597], mean action: 1.627 [0.000, 3.000],  loss: 13.455843, mse: 5771.260268, mean_q: 70.259968, mean_eps: 0.515461
  81106/150000: episode: 719, duration: 2.118s, episode steps: 298, steps per second: 141, episode reward: -18.931, mean reward: -0.064 [-100.000, 13.254], mean action: 1.748 [0.000, 3.000],  loss: 14.027152, mse: 5753.136715, mean_q: 70.579616, mean_eps: 0.514261
  81258/150000: episode: 720, duration: 1.013s, episode steps: 152, steps per second: 150, episode reward: -42.534, mean reward: -0.280 [-100.000, 10.237], mean action: 1.678 [0.000, 3.000],  loss: 13.009113, mse: 5720.303724, mean_q: 69.588991, mean_eps: 0.512911
  81412/150000: episode: 721, duration: 1.037s, episode steps: 154, steps per second: 149, episode reward: -79.924, mean reward: -0.519 [-100.000, 13.944], mean action: 1.636 [0.000, 3.000],  loss: 19.487416, mse: 5711.796748, mean_q: 69.920169, mean_eps: 0.511993
  81906/150000: episode: 722, duration: 3.561s, episode steps: 494, steps per second: 139, episode reward: -189.018, mean reward: -0.383 [-100.000, 18.714], mean action: 1.476 [0.000, 3.000],  loss: 15.513693, mse: 5782.619023, mean_q: 69.648963, mean_eps: 0.510049
  81998/150000: episode: 723, duration: 0.619s, episode steps:  92, steps per second: 149, episode reward: -61.751, mean reward: -0.671 [-100.000, 10.198], mean action: 1.652 [0.000, 3.000],  loss: 13.866368, mse: 5813.020179, mean_q: 69.198766, mean_eps: 0.508291
  82168/150000: episode: 724, duration: 1.137s, episode steps: 170, steps per second: 149, episode reward:  1.720, mean reward:  0.010 [-100.000, 17.670], mean action: 1.624 [0.000, 3.000],  loss: 15.192289, mse: 5769.949667, mean_q: 68.655628, mean_eps: 0.507505
  82302/150000: episode: 725, duration: 0.926s, episode steps: 134, steps per second: 145, episode reward:  0.320, mean reward:  0.002 [-100.000, 11.963], mean action: 1.672 [0.000, 3.000],  loss: 15.260960, mse: 5730.586783, mean_q: 68.289349, mean_eps: 0.506593
  82434/150000: episode: 726, duration: 0.902s, episode steps: 132, steps per second: 146, episode reward: -64.916, mean reward: -0.492 [-100.000, 13.765], mean action: 1.583 [0.000, 3.000],  loss: 14.507567, mse: 5878.449256, mean_q: 70.210284, mean_eps: 0.505795
  82539/150000: episode: 727, duration: 0.702s, episode steps: 105, steps per second: 149, episode reward: -47.651, mean reward: -0.454 [-100.000,  8.226], mean action: 1.724 [0.000, 3.000],  loss: 12.661109, mse: 5814.687616, mean_q: 69.340703, mean_eps: 0.505084
  83133/150000: episode: 728, duration: 4.360s, episode steps: 594, steps per second: 136, episode reward: -111.369, mean reward: -0.187 [-100.000, 12.972], mean action: 1.820 [0.000, 3.000],  loss: 16.355144, mse: 6073.240770, mean_q: 71.904934, mean_eps: 0.502987
  83234/150000: episode: 729, duration: 0.693s, episode steps: 101, steps per second: 146, episode reward: -50.648, mean reward: -0.501 [-100.000, 11.842], mean action: 1.624 [0.000, 3.000],  loss: 17.881495, mse: 6053.184643, mean_q: 71.644851, mean_eps: 0.500902
  83311/150000: episode: 730, duration: 0.508s, episode steps:  77, steps per second: 152, episode reward: -23.574, mean reward: -0.306 [-100.000, 13.472], mean action: 1.688 [0.000, 3.000],  loss: 12.967749, mse: 6209.872077, mean_q: 73.369253, mean_eps: 0.500368
  83426/150000: episode: 731, duration: 0.768s, episode steps: 115, steps per second: 150, episode reward: -22.973, mean reward: -0.200 [-100.000, 17.978], mean action: 1.487 [0.000, 3.000],  loss: 12.410580, mse: 6151.474830, mean_q: 72.185793, mean_eps: 0.499792
  84214/150000: episode: 732, duration: 5.807s, episode steps: 788, steps per second: 136, episode reward: -202.484, mean reward: -0.257 [-100.000, 20.711], mean action: 1.717 [0.000, 3.000],  loss: 14.991825, mse: 6232.077954, mean_q: 72.878300, mean_eps: 0.497083
  84594/150000: episode: 733, duration: 2.648s, episode steps: 380, steps per second: 144, episode reward: -146.126, mean reward: -0.385 [-100.000, 12.193], mean action: 1.716 [0.000, 3.000],  loss: 15.382912, mse: 6303.851804, mean_q: 73.615955, mean_eps: 0.493579
  84795/150000: episode: 734, duration: 1.393s, episode steps: 201, steps per second: 144, episode reward: -131.509, mean reward: -0.654 [-100.000,  4.476], mean action: 1.811 [0.000, 3.000],  loss: 17.899185, mse: 6266.945521, mean_q: 72.553670, mean_eps: 0.491836
  85795/150000: episode: 735, duration: 8.125s, episode steps: 1000, steps per second: 123, episode reward: -35.318, mean reward: -0.035 [-23.338, 22.970], mean action: 1.677 [0.000, 3.000],  loss: 16.899623, mse: 6411.614592, mean_q: 73.750895, mean_eps: 0.488233
  85901/150000: episode: 736, duration: 0.720s, episode steps: 106, steps per second: 147, episode reward: -9.035, mean reward: -0.085 [-100.000,  9.443], mean action: 1.689 [0.000, 3.000],  loss: 13.804767, mse: 6489.211002, mean_q: 75.835708, mean_eps: 0.484915
  86022/150000: episode: 737, duration: 0.804s, episode steps: 121, steps per second: 151, episode reward: -8.737, mean reward: -0.072 [-100.000, 21.616], mean action: 1.661 [0.000, 3.000],  loss: 14.801529, mse: 6383.864435, mean_q: 74.409093, mean_eps: 0.484234
  86121/150000: episode: 738, duration: 0.670s, episode steps:  99, steps per second: 148, episode reward: -67.837, mean reward: -0.685 [-100.000,  8.483], mean action: 1.687 [0.000, 3.000],  loss: 20.449094, mse: 6410.618499, mean_q: 74.776473, mean_eps: 0.483574
  86296/150000: episode: 739, duration: 1.171s, episode steps: 175, steps per second: 150, episode reward: -47.106, mean reward: -0.269 [-100.000, 16.744], mean action: 1.640 [0.000, 3.000],  loss: 20.308715, mse: 6560.213284, mean_q: 74.395387, mean_eps: 0.482752
  87296/150000: episode: 740, duration: 8.139s, episode steps: 1000, steps per second: 123, episode reward: 36.338, mean reward:  0.036 [-21.789, 19.505], mean action: 1.651 [0.000, 3.000],  loss: 14.770046, mse: 6764.504470, mean_q: 76.762393, mean_eps: 0.479227
  87457/150000: episode: 741, duration: 1.080s, episode steps: 161, steps per second: 149, episode reward: -63.966, mean reward: -0.397 [-100.000, 15.524], mean action: 1.727 [0.000, 3.000],  loss: 20.044894, mse: 6581.186369, mean_q: 74.841574, mean_eps: 0.475744
  87679/150000: episode: 742, duration: 1.732s, episode steps: 222, steps per second: 128, episode reward: -232.904, mean reward: -1.049 [-100.000, 19.385], mean action: 1.788 [0.000, 3.000],  loss: 20.847371, mse: 6655.860264, mean_q: 75.599267, mean_eps: 0.474595
  87803/150000: episode: 743, duration: 1.035s, episode steps: 124, steps per second: 120, episode reward: -27.448, mean reward: -0.221 [-100.000, 20.697], mean action: 1.742 [0.000, 3.000],  loss: 17.608309, mse: 6628.379288, mean_q: 75.160116, mean_eps: 0.473557
  87954/150000: episode: 744, duration: 1.176s, episode steps: 151, steps per second: 128, episode reward: -138.898, mean reward: -0.920 [-100.000, 54.303], mean action: 1.887 [0.000, 3.000],  loss: 15.959082, mse: 6584.182181, mean_q: 76.227724, mean_eps: 0.472732
  88108/150000: episode: 745, duration: 1.176s, episode steps: 154, steps per second: 131, episode reward: -40.486, mean reward: -0.263 [-100.000,  9.329], mean action: 1.662 [0.000, 3.000],  loss: 16.427514, mse: 6559.426980, mean_q: 75.498277, mean_eps: 0.471817
  88302/150000: episode: 746, duration: 1.343s, episode steps: 194, steps per second: 144, episode reward: -61.132, mean reward: -0.315 [-100.000, 11.235], mean action: 1.773 [0.000, 3.000],  loss: 19.270378, mse: 6551.873199, mean_q: 76.494455, mean_eps: 0.470773
  88454/150000: episode: 747, duration: 1.050s, episode steps: 152, steps per second: 145, episode reward: -49.214, mean reward: -0.324 [-100.000, 11.341], mean action: 1.632 [0.000, 3.000],  loss: 20.165426, mse: 6666.426931, mean_q: 78.143219, mean_eps: 0.469735
  88562/150000: episode: 748, duration: 0.737s, episode steps: 108, steps per second: 146, episode reward: -62.192, mean reward: -0.576 [-100.000,  8.268], mean action: 1.750 [0.000, 3.000],  loss: 11.798684, mse: 6630.657516, mean_q: 77.449336, mean_eps: 0.468955
  88698/150000: episode: 749, duration: 0.951s, episode steps: 136, steps per second: 143, episode reward: -49.851, mean reward: -0.367 [-100.000, 10.989], mean action: 1.625 [0.000, 3.000],  loss: 20.439935, mse: 6547.861425, mean_q: 77.027959, mean_eps: 0.468223
  89698/150000: episode: 750, duration: 7.413s, episode steps: 1000, steps per second: 135, episode reward:  7.441, mean reward:  0.007 [-20.172, 27.404], mean action: 2.070 [0.000, 3.000],  loss: 19.456182, mse: 6622.310043, mean_q: 78.542509, mean_eps: 0.464815
  90698/150000: episode: 751, duration: 7.844s, episode steps: 1000, steps per second: 127, episode reward: -9.400, mean reward: -0.009 [-23.466, 19.349], mean action: 1.573 [0.000, 3.000],  loss: 18.216270, mse: 6808.012895, mean_q: 80.631581, mean_eps: 0.458815
  90846/150000: episode: 752, duration: 0.995s, episode steps: 148, steps per second: 149, episode reward: -20.539, mean reward: -0.139 [-100.000, 10.988], mean action: 1.730 [0.000, 3.000],  loss: 22.333321, mse: 6837.863816, mean_q: 80.416519, mean_eps: 0.455371
  91846/150000: episode: 753, duration: 7.619s, episode steps: 1000, steps per second: 131, episode reward: -54.147, mean reward: -0.054 [-24.414, 29.216], mean action: 2.019 [0.000, 3.000],  loss: 18.324250, mse: 7068.232733, mean_q: 81.207141, mean_eps: 0.451927
  92080/150000: episode: 754, duration: 1.590s, episode steps: 234, steps per second: 147, episode reward: 28.785, mean reward:  0.123 [-100.000, 20.378], mean action: 1.632 [0.000, 3.000],  loss: 21.383392, mse: 7412.204999, mean_q: 84.091115, mean_eps: 0.448225
  93080/150000: episode: 755, duration: 7.980s, episode steps: 1000, steps per second: 125, episode reward: 105.114, mean reward:  0.105 [-24.684, 24.203], mean action: 1.043 [0.000, 3.000],  loss: 17.749103, mse: 7275.947623, mean_q: 82.747541, mean_eps: 0.444523
  93174/150000: episode: 756, duration: 0.899s, episode steps:  94, steps per second: 105, episode reward:  5.074, mean reward:  0.054 [-100.000, 17.977], mean action: 1.734 [0.000, 3.000],  loss: 17.491210, mse: 7247.325865, mean_q: 85.030654, mean_eps: 0.441241
  93429/150000: episode: 757, duration: 1.843s, episode steps: 255, steps per second: 138, episode reward: -240.791, mean reward: -0.944 [-100.000, 17.958], mean action: 1.808 [0.000, 3.000],  loss: 18.943048, mse: 7256.636592, mean_q: 84.335841, mean_eps: 0.440194
  93531/150000: episode: 758, duration: 0.712s, episode steps: 102, steps per second: 143, episode reward: -36.688, mean reward: -0.360 [-100.000,  7.576], mean action: 1.588 [0.000, 3.000],  loss: 13.437410, mse: 7266.831725, mean_q: 84.059543, mean_eps: 0.439123
  93638/150000: episode: 759, duration: 0.723s, episode steps: 107, steps per second: 148, episode reward: 49.725, mean reward:  0.465 [-100.000, 15.873], mean action: 1.748 [0.000, 3.000],  loss: 15.212270, mse: 7316.119127, mean_q: 84.362887, mean_eps: 0.438496
  94638/150000: episode: 760, duration: 7.662s, episode steps: 1000, steps per second: 131, episode reward: 58.935, mean reward:  0.059 [-23.233, 23.995], mean action: 1.534 [0.000, 3.000],  loss: 16.379082, mse: 7389.868064, mean_q: 85.364653, mean_eps: 0.435175
  94747/150000: episode: 761, duration: 0.731s, episode steps: 109, steps per second: 149, episode reward: -67.659, mean reward: -0.621 [-100.000,  8.081], mean action: 1.661 [0.000, 3.000],  loss: 19.724754, mse: 7294.131903, mean_q: 85.478759, mean_eps: 0.431848
  95747/150000: episode: 762, duration: 7.537s, episode steps: 1000, steps per second: 133, episode reward: 42.993, mean reward:  0.043 [-22.759, 25.809], mean action: 1.254 [0.000, 3.000],  loss: 19.428414, mse: 7028.133951, mean_q: 83.837314, mean_eps: 0.428521
  96747/150000: episode: 763, duration: 8.205s, episode steps: 1000, steps per second: 122, episode reward: 78.599, mean reward:  0.079 [-24.271, 23.642], mean action: 1.448 [0.000, 3.000],  loss: 18.808465, mse: 7006.297086, mean_q: 84.635934, mean_eps: 0.422521
  96971/150000: episode: 764, duration: 1.556s, episode steps: 224, steps per second: 144, episode reward: -194.575, mean reward: -0.869 [-100.000, 10.318], mean action: 1.714 [0.000, 3.000],  loss: 18.575571, mse: 6760.809699, mean_q: 83.495476, mean_eps: 0.418849
  97136/150000: episode: 765, duration: 1.115s, episode steps: 165, steps per second: 148, episode reward: 36.473, mean reward:  0.221 [-100.000, 15.226], mean action: 1.794 [0.000, 3.000],  loss: 19.492475, mse: 6757.275139, mean_q: 83.208400, mean_eps: 0.417682
  97278/150000: episode: 766, duration: 0.988s, episode steps: 142, steps per second: 144, episode reward: -19.828, mean reward: -0.140 [-100.000, 20.709], mean action: 1.697 [0.000, 3.000],  loss: 17.338793, mse: 6978.748250, mean_q: 84.786496, mean_eps: 0.416761
  98278/150000: episode: 767, duration: 7.529s, episode steps: 1000, steps per second: 133, episode reward: -9.540, mean reward: -0.010 [-23.985, 22.828], mean action: 1.661 [0.000, 3.000],  loss: 18.210733, mse: 7005.612396, mean_q: 85.881968, mean_eps: 0.413335
  99278/150000: episode: 768, duration: 7.624s, episode steps: 1000, steps per second: 131, episode reward: 38.718, mean reward:  0.039 [-22.790, 23.659], mean action: 1.359 [0.000, 3.000],  loss: 20.364167, mse: 7121.552625, mean_q: 87.323555, mean_eps: 0.407335
  99441/150000: episode: 769, duration: 1.092s, episode steps: 163, steps per second: 149, episode reward:  9.914, mean reward:  0.061 [-100.000, 11.517], mean action: 1.748 [0.000, 3.000],  loss: 17.781078, mse: 7341.622379, mean_q: 87.738204, mean_eps: 0.403846
 100441/150000: episode: 770, duration: 8.195s, episode steps: 1000, steps per second: 122, episode reward: -9.534, mean reward: -0.010 [-21.767, 22.340], mean action: 1.787 [0.000, 3.000],  loss: 19.012771, mse: 7136.049979, mean_q: 86.343408, mean_eps: 0.400357
 101441/150000: episode: 771, duration: 7.872s, episode steps: 1000, steps per second: 127, episode reward: 36.117, mean reward:  0.036 [-22.614, 23.415], mean action: 1.642 [0.000, 3.000],  loss: 20.677224, mse: 7502.304853, mean_q: 89.844909, mean_eps: 0.394357
 101540/150000: episode: 772, duration: 0.687s, episode steps:  99, steps per second: 144, episode reward: -12.211, mean reward: -0.123 [-100.000, 12.286], mean action: 1.848 [0.000, 3.000],  loss: 21.112365, mse: 7717.216328, mean_q: 90.833645, mean_eps: 0.391060
 101656/150000: episode: 773, duration: 0.813s, episode steps: 116, steps per second: 143, episode reward: -62.871, mean reward: -0.542 [-100.000, 26.605], mean action: 1.716 [0.000, 3.000],  loss: 18.618849, mse: 7709.704817, mean_q: 90.484075, mean_eps: 0.390415
 101779/150000: episode: 774, duration: 0.867s, episode steps: 123, steps per second: 142, episode reward:  0.685, mean reward:  0.006 [-100.000, 23.345], mean action: 1.740 [0.000, 3.000],  loss: 15.770192, mse: 7699.644595, mean_q: 90.852320, mean_eps: 0.389698
 102779/150000: episode: 775, duration: 7.573s, episode steps: 1000, steps per second: 132, episode reward: 171.906, mean reward:  0.172 [-20.557, 22.415], mean action: 1.227 [0.000, 3.000],  loss: 21.178439, mse: 7908.024229, mean_q: 92.692302, mean_eps: 0.386329
 102931/150000: episode: 776, duration: 1.067s, episode steps: 152, steps per second: 143, episode reward: -69.342, mean reward: -0.456 [-100.000,  8.243], mean action: 1.546 [0.000, 3.000],  loss: 15.963793, mse: 8026.295433, mean_q: 93.601514, mean_eps: 0.382873
 103084/150000: episode: 777, duration: 1.053s, episode steps: 153, steps per second: 145, episode reward: -39.379, mean reward: -0.257 [-100.000,  9.549], mean action: 1.765 [0.000, 3.000],  loss: 19.346252, mse: 7881.462674, mean_q: 92.962620, mean_eps: 0.381958
 104084/150000: episode: 778, duration: 8.084s, episode steps: 1000, steps per second: 124, episode reward: 97.606, mean reward:  0.098 [-25.346, 23.428], mean action: 1.395 [0.000, 3.000],  loss: 20.984835, mse: 7974.092142, mean_q: 93.364448, mean_eps: 0.378499
 105084/150000: episode: 779, duration: 7.937s, episode steps: 1000, steps per second: 126, episode reward: -59.046, mean reward: -0.059 [-23.047, 28.474], mean action: 1.489 [0.000, 3.000],  loss: 20.917318, mse: 7793.638607, mean_q: 93.500573, mean_eps: 0.372499
 105253/150000: episode: 780, duration: 1.228s, episode steps: 169, steps per second: 138, episode reward: -286.025, mean reward: -1.692 [-100.000, 35.884], mean action: 1.846 [0.000, 3.000],  loss: 15.468347, mse: 8016.013132, mean_q: 95.277801, mean_eps: 0.368992
 105369/150000: episode: 781, duration: 0.899s, episode steps: 116, steps per second: 129, episode reward: 26.906, mean reward:  0.232 [-100.000, 16.179], mean action: 1.578 [0.000, 3.000],  loss: 17.340831, mse: 7911.560151, mean_q: 94.546762, mean_eps: 0.368137
 106369/150000: episode: 782, duration: 7.593s, episode steps: 1000, steps per second: 132, episode reward: 77.677, mean reward:  0.078 [-23.858, 23.602], mean action: 1.476 [0.000, 3.000],  loss: 21.143986, mse: 7950.766856, mean_q: 94.738857, mean_eps: 0.364789
 107369/150000: episode: 783, duration: 7.489s, episode steps: 1000, steps per second: 134, episode reward: 68.188, mean reward:  0.068 [-21.911, 22.632], mean action: 1.433 [0.000, 3.000],  loss: 20.757012, mse: 8015.031237, mean_q: 96.004531, mean_eps: 0.358789
 108369/150000: episode: 784, duration: 7.767s, episode steps: 1000, steps per second: 129, episode reward: 136.557, mean reward:  0.137 [-21.366, 22.977], mean action: 1.553 [0.000, 3.000],  loss: 21.289063, mse: 7978.070582, mean_q: 95.392455, mean_eps: 0.352789
 109369/150000: episode: 785, duration: 7.511s, episode steps: 1000, steps per second: 133, episode reward: 100.958, mean reward:  0.101 [-23.217, 24.218], mean action: 1.225 [0.000, 3.000],  loss: 20.794580, mse: 8134.324613, mean_q: 96.519027, mean_eps: 0.346789
 110369/150000: episode: 786, duration: 7.673s, episode steps: 1000, steps per second: 130, episode reward: 33.793, mean reward:  0.034 [-23.529, 24.942], mean action: 1.721 [0.000, 3.000],  loss: 21.903495, mse: 8226.818055, mean_q: 97.728964, mean_eps: 0.340789
 111369/150000: episode: 787, duration: 7.526s, episode steps: 1000, steps per second: 133, episode reward: 105.929, mean reward:  0.106 [-22.961, 22.945], mean action: 1.541 [0.000, 3.000],  loss: 20.592852, mse: 8097.798229, mean_q: 96.935788, mean_eps: 0.334789
 112369/150000: episode: 788, duration: 7.569s, episode steps: 1000, steps per second: 132, episode reward: 50.750, mean reward:  0.051 [-22.706, 23.898], mean action: 1.559 [0.000, 3.000],  loss: 19.766299, mse: 7939.569106, mean_q: 95.853018, mean_eps: 0.328789
 112729/150000: episode: 789, duration: 2.631s, episode steps: 360, steps per second: 137, episode reward: -14.986, mean reward: -0.042 [-100.000, 18.894], mean action: 1.647 [0.000, 3.000],  loss: 22.938844, mse: 7970.810502, mean_q: 95.707579, mean_eps: 0.324709
 113729/150000: episode: 790, duration: 8.399s, episode steps: 1000, steps per second: 119, episode reward: 76.782, mean reward:  0.077 [-24.403, 22.794], mean action: 1.694 [0.000, 3.000],  loss: 19.708748, mse: 7900.414509, mean_q: 95.160228, mean_eps: 0.320629
 114189/150000: episode: 791, duration: 3.509s, episode steps: 460, steps per second: 131, episode reward: -265.594, mean reward: -0.577 [-100.000, 23.668], mean action: 1.720 [0.000, 3.000],  loss: 17.196776, mse: 7597.736841, mean_q: 94.202872, mean_eps: 0.316249
 115189/150000: episode: 792, duration: 7.943s, episode steps: 1000, steps per second: 126, episode reward: 54.740, mean reward:  0.055 [-23.056, 24.774], mean action: 1.458 [0.000, 3.000],  loss: 19.374173, mse: 7291.430764, mean_q: 92.166216, mean_eps: 0.311869
 115731/150000: episode: 793, duration: 4.004s, episode steps: 542, steps per second: 135, episode reward:  2.474, mean reward:  0.005 [-100.000, 22.323], mean action: 1.692 [0.000, 3.000],  loss: 17.298742, mse: 7066.960419, mean_q: 90.652595, mean_eps: 0.307243
 115845/150000: episode: 794, duration: 0.785s, episode steps: 114, steps per second: 145, episode reward: -100.513, mean reward: -0.882 [-100.000,  3.066], mean action: 1.491 [0.000, 3.000],  loss: 18.781528, mse: 7111.159989, mean_q: 91.224317, mean_eps: 0.305275
 115942/150000: episode: 795, duration: 0.669s, episode steps:  97, steps per second: 145, episode reward:  0.763, mean reward:  0.008 [-100.000, 18.106], mean action: 1.732 [0.000, 3.000],  loss: 26.515593, mse: 7209.576952, mean_q: 91.552665, mean_eps: 0.304642
 116019/150000: episode: 796, duration: 0.571s, episode steps:  77, steps per second: 135, episode reward: -53.603, mean reward: -0.696 [-100.000,  9.131], mean action: 1.377 [0.000, 3.000],  loss: 23.565070, mse: 7275.634423, mean_q: 91.896018, mean_eps: 0.304120
 117019/150000: episode: 797, duration: 7.619s, episode steps: 1000, steps per second: 131, episode reward: 162.728, mean reward:  0.163 [-21.599, 25.252], mean action: 1.222 [0.000, 3.000],  loss: 20.602524, mse: 6980.086375, mean_q: 90.347976, mean_eps: 0.300889
 118019/150000: episode: 798, duration: 7.767s, episode steps: 1000, steps per second: 129, episode reward: 113.035, mean reward:  0.113 [-22.352, 23.841], mean action: 1.236 [0.000, 3.000],  loss: 19.271572, mse: 6700.047320, mean_q: 88.161832, mean_eps: 0.294889
 119019/150000: episode: 799, duration: 7.668s, episode steps: 1000, steps per second: 130, episode reward: 25.761, mean reward:  0.026 [-25.771, 22.139], mean action: 2.161 [0.000, 3.000],  loss: 18.992836, mse: 6444.107633, mean_q: 86.370740, mean_eps: 0.288889
 120019/150000: episode: 800, duration: 8.340s, episode steps: 1000, steps per second: 120, episode reward: -31.579, mean reward: -0.032 [-23.593, 23.896], mean action: 1.449 [0.000, 3.000],  loss: 17.243503, mse: 5946.339543, mean_q: 83.017876, mean_eps: 0.282889
 120756/150000: episode: 801, duration: 5.806s, episode steps: 737, steps per second: 127, episode reward: -354.714, mean reward: -0.481 [-100.000, 15.365], mean action: 1.739 [0.000, 3.000],  loss: 16.786568, mse: 5606.829558, mean_q: 80.778018, mean_eps: 0.277678
 121401/150000: episode: 802, duration: 4.938s, episode steps: 645, steps per second: 131, episode reward: 191.067, mean reward:  0.296 [-21.387, 100.000], mean action: 1.712 [0.000, 3.000],  loss: 16.703084, mse: 5644.683717, mean_q: 81.050803, mean_eps: 0.273532
 122401/150000: episode: 803, duration: 8.304s, episode steps: 1000, steps per second: 120, episode reward: -75.530, mean reward: -0.076 [-22.336, 22.420], mean action: 1.725 [0.000, 3.000],  loss: 15.443433, mse: 5703.043961, mean_q: 80.749556, mean_eps: 0.268597
 122590/150000: episode: 804, duration: 1.343s, episode steps: 189, steps per second: 141, episode reward: 11.905, mean reward:  0.063 [-100.000, 10.053], mean action: 1.820 [0.000, 3.000],  loss: 17.037798, mse: 5422.712772, mean_q: 77.877708, mean_eps: 0.265030
 123435/150000: episode: 805, duration: 6.366s, episode steps: 845, steps per second: 133, episode reward: 195.860, mean reward:  0.232 [-22.172, 100.000], mean action: 1.444 [0.000, 3.000],  loss: 15.512589, mse: 5320.352291, mean_q: 77.480894, mean_eps: 0.261928
 123618/150000: episode: 806, duration: 1.277s, episode steps: 183, steps per second: 143, episode reward: -189.403, mean reward: -1.035 [-100.000, 25.697], mean action: 1.672 [0.000, 3.000],  loss: 11.920817, mse: 5034.476384, mean_q: 75.431223, mean_eps: 0.258844
 124618/150000: episode: 807, duration: 7.532s, episode steps: 1000, steps per second: 133, episode reward: 34.447, mean reward:  0.034 [-26.415, 24.191], mean action: 1.662 [0.000, 3.000],  loss: 14.888775, mse: 5162.877896, mean_q: 76.067771, mean_eps: 0.255295
 125618/150000: episode: 808, duration: 7.554s, episode steps: 1000, steps per second: 132, episode reward: 76.760, mean reward:  0.077 [-20.059, 28.384], mean action: 1.593 [0.000, 3.000],  loss: 15.596949, mse: 5236.570788, mean_q: 76.509913, mean_eps: 0.249295
 125714/150000: episode: 809, duration: 0.680s, episode steps:  96, steps per second: 141, episode reward: 22.908, mean reward:  0.239 [-100.000,  9.654], mean action: 1.688 [0.000, 3.000],  loss: 10.899431, mse: 5253.502022, mean_q: 77.147066, mean_eps: 0.246007
 126714/150000: episode: 810, duration: 7.687s, episode steps: 1000, steps per second: 130, episode reward: 97.710, mean reward:  0.098 [-24.556, 22.959], mean action: 1.328 [0.000, 3.000],  loss: 18.303409, mse: 5398.836882, mean_q: 77.493751, mean_eps: 0.242719
 127714/150000: episode: 811, duration: 7.513s, episode steps: 1000, steps per second: 133, episode reward: 115.183, mean reward:  0.115 [-21.262, 23.186], mean action: 1.058 [0.000, 3.000],  loss: 14.819882, mse: 5378.396569, mean_q: 77.674822, mean_eps: 0.236719
 128714/150000: episode: 812, duration: 7.590s, episode steps: 1000, steps per second: 132, episode reward: 85.237, mean reward:  0.085 [-21.163, 23.337], mean action: 1.018 [0.000, 3.000],  loss: 12.727197, mse: 5158.814516, mean_q: 75.374049, mean_eps: 0.230719
 128821/150000: episode: 813, duration: 0.741s, episode steps: 107, steps per second: 144, episode reward: -24.523, mean reward: -0.229 [-100.000, 16.834], mean action: 1.645 [0.000, 3.000],  loss: 10.423792, mse: 5085.907227, mean_q: 74.236891, mean_eps: 0.227398
 128928/150000: episode: 814, duration: 0.770s, episode steps: 107, steps per second: 139, episode reward: -256.042, mean reward: -2.393 [-100.000,  8.074], mean action: 1.748 [0.000, 3.000],  loss: 12.751640, mse: 5033.440984, mean_q: 74.405116, mean_eps: 0.226756
 129928/150000: episode: 815, duration: 7.883s, episode steps: 1000, steps per second: 127, episode reward: 78.306, mean reward:  0.078 [-20.618, 22.992], mean action: 1.472 [0.000, 3.000],  loss: 13.089192, mse: 4995.217023, mean_q: 73.498507, mean_eps: 0.223435
 130023/150000: episode: 816, duration: 0.679s, episode steps:  95, steps per second: 140, episode reward: -319.610, mean reward: -3.364 [-100.000, 18.127], mean action: 1.916 [0.000, 3.000],  loss: 11.150323, mse: 5162.841100, mean_q: 74.453369, mean_eps: 0.220150
 130133/150000: episode: 817, duration: 0.765s, episode steps: 110, steps per second: 144, episode reward: -56.248, mean reward: -0.511 [-100.000, 10.971], mean action: 1.373 [0.000, 3.000],  loss: 12.103331, mse: 5259.090368, mean_q: 75.094763, mean_eps: 0.219535
 130237/150000: episode: 818, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: 11.963, mean reward:  0.115 [-100.000, 18.612], mean action: 1.712 [0.000, 3.000],  loss: 7.570375, mse: 5242.617190, mean_q: 74.784864, mean_eps: 0.218893
 131237/150000: episode: 819, duration: 8.122s, episode steps: 1000, steps per second: 123, episode reward: 23.643, mean reward:  0.024 [-19.600, 13.929], mean action: 2.385 [0.000, 3.000],  loss: 14.965962, mse: 5092.243056, mean_q: 73.903223, mean_eps: 0.215581
 131323/150000: episode: 820, duration: 0.624s, episode steps:  86, steps per second: 138, episode reward:  3.011, mean reward:  0.035 [-100.000, 15.750], mean action: 1.663 [0.000, 3.000],  loss: 10.742825, mse: 5109.469559, mean_q: 74.303705, mean_eps: 0.212323
 132323/150000: episode: 821, duration: 7.407s, episode steps: 1000, steps per second: 135, episode reward: 145.078, mean reward:  0.145 [-21.441, 21.498], mean action: 1.339 [0.000, 3.000],  loss: 11.358897, mse: 4973.422696, mean_q: 72.890429, mean_eps: 0.209065
 132760/150000: episode: 822, duration: 3.266s, episode steps: 437, steps per second: 134, episode reward: 174.744, mean reward:  0.400 [-20.682, 100.000], mean action: 1.398 [0.000, 3.000],  loss: 12.582922, mse: 4917.622811, mean_q: 72.649117, mean_eps: 0.204754
 133582/150000: episode: 823, duration: 6.211s, episode steps: 822, steps per second: 132, episode reward: 242.657, mean reward:  0.295 [-19.506, 100.000], mean action: 0.830 [0.000, 3.000],  loss: 9.962827, mse: 4906.186556, mean_q: 72.276977, mean_eps: 0.200977
 133872/150000: episode: 824, duration: 2.048s, episode steps: 290, steps per second: 142, episode reward: 258.249, mean reward:  0.891 [-10.801, 100.000], mean action: 1.028 [0.000, 3.000],  loss: 10.235440, mse: 4720.990297, mean_q: 71.171680, mean_eps: 0.197641
 134632/150000: episode: 825, duration: 5.645s, episode steps: 760, steps per second: 135, episode reward: 220.505, mean reward:  0.290 [-21.161, 100.000], mean action: 1.239 [0.000, 3.000],  loss: 10.119068, mse: 4862.972923, mean_q: 72.217735, mean_eps: 0.194491
 135375/150000: episode: 826, duration: 5.390s, episode steps: 743, steps per second: 138, episode reward: 270.604, mean reward:  0.364 [-24.659, 100.000], mean action: 1.129 [0.000, 3.000],  loss: 12.460587, mse: 4891.386474, mean_q: 71.849325, mean_eps: 0.189982
 135467/150000: episode: 827, duration: 0.654s, episode steps:  92, steps per second: 141, episode reward: -46.902, mean reward: -0.510 [-100.000, 14.652], mean action: 1.598 [0.000, 3.000],  loss: 5.404392, mse: 5035.951681, mean_q: 72.526804, mean_eps: 0.187477
 135550/150000: episode: 828, duration: 0.573s, episode steps:  83, steps per second: 145, episode reward: 11.847, mean reward:  0.143 [-100.000, 13.383], mean action: 1.446 [0.000, 3.000],  loss: 7.686657, mse: 4981.696018, mean_q: 71.702645, mean_eps: 0.186952
 135829/150000: episode: 829, duration: 1.984s, episode steps: 279, steps per second: 141, episode reward: 311.394, mean reward:  1.116 [-18.251, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 6.660886, mse: 4974.784626, mean_q: 71.948553, mean_eps: 0.185866
 135926/150000: episode: 830, duration: 0.667s, episode steps:  97, steps per second: 145, episode reward: 24.907, mean reward:  0.257 [-100.000, 18.567], mean action: 1.515 [0.000, 3.000],  loss: 10.897477, mse: 4975.286619, mean_q: 72.409588, mean_eps: 0.184738
 136926/150000: episode: 831, duration: 7.411s, episode steps: 1000, steps per second: 135, episode reward: 109.618, mean reward:  0.110 [-20.528, 22.485], mean action: 0.935 [0.000, 3.000],  loss: 10.738569, mse: 4876.886607, mean_q: 71.249639, mean_eps: 0.181447
 137926/150000: episode: 832, duration: 7.343s, episode steps: 1000, steps per second: 136, episode reward: 117.703, mean reward:  0.118 [-19.728, 23.839], mean action: 0.888 [0.000, 3.000],  loss: 9.480560, mse: 4823.364972, mean_q: 70.536882, mean_eps: 0.175447
 138327/150000: episode: 833, duration: 2.945s, episode steps: 401, steps per second: 136, episode reward: 245.852, mean reward:  0.613 [-18.936, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 8.588871, mse: 4560.462287, mean_q: 68.530759, mean_eps: 0.171244
 138511/150000: episode: 834, duration: 1.295s, episode steps: 184, steps per second: 142, episode reward:  9.177, mean reward:  0.050 [-100.000, 11.646], mean action: 1.761 [0.000, 3.000],  loss: 7.798757, mse: 4565.805653, mean_q: 68.558668, mean_eps: 0.169489
 138739/150000: episode: 835, duration: 1.597s, episode steps: 228, steps per second: 143, episode reward: 276.396, mean reward:  1.212 [-9.456, 100.000], mean action: 1.289 [0.000, 3.000],  loss: 10.764638, mse: 4631.804265, mean_q: 69.108502, mean_eps: 0.168253
 139158/150000: episode: 836, duration: 3.093s, episode steps: 419, steps per second: 135, episode reward: 187.757, mean reward:  0.448 [-21.009, 100.000], mean action: 1.768 [0.000, 3.000],  loss: 9.962906, mse: 4577.176716, mean_q: 68.886326, mean_eps: 0.166312
 139757/150000: episode: 837, duration: 4.915s, episode steps: 599, steps per second: 122, episode reward: 237.235, mean reward:  0.396 [-18.458, 100.000], mean action: 1.100 [0.000, 3.000],  loss: 8.756424, mse: 4585.910402, mean_q: 68.455045, mean_eps: 0.163258
 140757/150000: episode: 838, duration: 7.897s, episode steps: 1000, steps per second: 127, episode reward: 127.410, mean reward:  0.127 [-20.137, 26.863], mean action: 1.146 [0.000, 3.000],  loss: 10.220505, mse: 4929.923667, mean_q: 70.831924, mean_eps: 0.158461
 141446/150000: episode: 839, duration: 5.511s, episode steps: 689, steps per second: 125, episode reward: 217.463, mean reward:  0.316 [-20.129, 100.000], mean action: 1.241 [0.000, 3.000],  loss: 10.643905, mse: 4801.615279, mean_q: 69.956481, mean_eps: 0.153394
 141783/150000: episode: 840, duration: 2.435s, episode steps: 337, steps per second: 138, episode reward: 258.609, mean reward:  0.767 [-9.697, 100.000], mean action: 1.282 [0.000, 3.000],  loss: 8.918252, mse: 4747.931226, mean_q: 69.738605, mean_eps: 0.150316
 142783/150000: episode: 841, duration: 7.624s, episode steps: 1000, steps per second: 131, episode reward: 98.166, mean reward:  0.098 [-20.341, 23.444], mean action: 1.362 [0.000, 3.000],  loss: 9.827772, mse: 4888.664407, mean_q: 71.082320, mean_eps: 0.146305
 143566/150000: episode: 842, duration: 5.772s, episode steps: 783, steps per second: 136, episode reward: 270.941, mean reward:  0.346 [-18.874, 100.000], mean action: 0.881 [0.000, 3.000],  loss: 11.377615, mse: 4768.748095, mean_q: 70.084119, mean_eps: 0.140956
 143853/150000: episode: 843, duration: 2.073s, episode steps: 287, steps per second: 138, episode reward: 268.097, mean reward:  0.934 [-10.307, 100.000], mean action: 1.519 [0.000, 3.000],  loss: 7.741470, mse: 4646.172684, mean_q: 69.197679, mean_eps: 0.137746
 144517/150000: episode: 844, duration: 5.062s, episode steps: 664, steps per second: 131, episode reward: 250.292, mean reward:  0.377 [-17.567, 100.000], mean action: 1.047 [0.000, 3.000],  loss: 10.349314, mse: 4576.567877, mean_q: 68.362057, mean_eps: 0.134893
 145007/150000: episode: 845, duration: 3.523s, episode steps: 490, steps per second: 139, episode reward: 267.177, mean reward:  0.545 [-19.303, 100.000], mean action: 1.078 [0.000, 3.000],  loss: 11.161173, mse: 4553.873291, mean_q: 68.013422, mean_eps: 0.131431
 145730/150000: episode: 846, duration: 5.886s, episode steps: 723, steps per second: 123, episode reward: 114.973, mean reward:  0.159 [-19.591, 100.000], mean action: 1.555 [0.000, 3.000],  loss: 11.266182, mse: 4772.568791, mean_q: 69.944983, mean_eps: 0.127792
 146064/150000: episode: 847, duration: 2.413s, episode steps: 334, steps per second: 138, episode reward: -68.128, mean reward: -0.204 [-100.000, 17.801], mean action: 1.886 [0.000, 3.000],  loss: 11.872735, mse: 4975.467986, mean_q: 71.394657, mean_eps: 0.124621
 146273/150000: episode: 848, duration: 1.450s, episode steps: 209, steps per second: 144, episode reward: -293.957, mean reward: -1.406 [-100.000,  6.142], mean action: 1.823 [0.000, 3.000],  loss: 13.140419, mse: 5108.685582, mean_q: 72.330399, mean_eps: 0.122992
 146501/150000: episode: 849, duration: 1.630s, episode steps: 228, steps per second: 140, episode reward: -374.254, mean reward: -1.641 [-100.000,  4.992], mean action: 1.807 [0.000, 3.000],  loss: 9.088809, mse: 5203.561550, mean_q: 72.719488, mean_eps: 0.121681
 146953/150000: episode: 850, duration: 3.355s, episode steps: 452, steps per second: 135, episode reward: 242.955, mean reward:  0.538 [-19.743, 100.000], mean action: 2.250 [0.000, 3.000],  loss: 10.904898, mse: 5316.372561, mean_q: 72.596930, mean_eps: 0.119641
 147250/150000: episode: 851, duration: 2.172s, episode steps: 297, steps per second: 137, episode reward: -401.094, mean reward: -1.350 [-100.000, 23.549], mean action: 1.818 [0.000, 3.000],  loss: 13.995305, mse: 5231.791986, mean_q: 71.800527, mean_eps: 0.117394
 147650/150000: episode: 852, duration: 2.881s, episode steps: 400, steps per second: 139, episode reward: 263.160, mean reward:  0.658 [-17.463, 100.000], mean action: 1.110 [0.000, 3.000],  loss: 19.252703, mse: 5383.463488, mean_q: 72.229812, mean_eps: 0.115303
 147898/150000: episode: 853, duration: 1.821s, episode steps: 248, steps per second: 136, episode reward: -241.015, mean reward: -0.972 [-100.000,  6.054], mean action: 1.847 [0.000, 3.000],  loss: 10.152664, mse: 5226.023989, mean_q: 71.688975, mean_eps: 0.113359
 148162/150000: episode: 854, duration: 2.019s, episode steps: 264, steps per second: 131, episode reward: 291.256, mean reward:  1.103 [-8.197, 100.000], mean action: 1.273 [0.000, 3.000],  loss: 41.340388, mse: 5417.695145, mean_q: 73.197208, mean_eps: 0.111823
 148466/150000: episode: 855, duration: 2.368s, episode steps: 304, steps per second: 128, episode reward: 249.542, mean reward:  0.821 [-21.205, 100.000], mean action: 1.345 [0.000, 3.000],  loss: 12.739382, mse: 5348.448572, mean_q: 73.239258, mean_eps: 0.110119
 148645/150000: episode: 856, duration: 1.328s, episode steps: 179, steps per second: 135, episode reward: -329.970, mean reward: -1.843 [-100.000,  6.064], mean action: 1.916 [0.000, 3.000],  loss: 12.097205, mse: 5442.185449, mean_q: 74.104712, mean_eps: 0.108670
 149003/150000: episode: 857, duration: 2.625s, episode steps: 358, steps per second: 136, episode reward: -143.019, mean reward: -0.399 [-100.000, 16.487], mean action: 1.746 [0.000, 3.000],  loss: 18.999074, mse: 5606.118375, mean_q: 75.125345, mean_eps: 0.107059
 149576/150000: episode: 858, duration: 4.225s, episode steps: 573, steps per second: 136, episode reward: 291.806, mean reward:  0.509 [-20.622, 100.000], mean action: 1.164 [0.000, 3.000],  loss: 25.595883, mse: 5422.385604, mean_q: 74.286778, mean_eps: 0.104266
 149926/150000: episode: 859, duration: 2.558s, episode steps: 350, steps per second: 137, episode reward: -165.944, mean reward: -0.474 [-100.000, 16.444], mean action: 1.731 [0.000, 3.000],  loss: 10.458113, mse: 5420.373454, mean_q: 74.490243, mean_eps: 0.101497
done, took 1186.044 seconds
Testing for 5 episodes ...
Episode 1: reward: -102.673, steps: 205
Episode 2: reward: -115.829, steps: 235
Episode 3: reward: -126.221, steps: 251
Episode 4: reward: 157.578, steps: 345
Episode 5: reward: -423.666, steps: 164
Testing for 5 episodes ...
Episode 1: reward: 203.174, steps: 315
Episode 2: reward: -200.973, steps: 183
Episode 3: reward: -296.878, steps: 196
Episode 4: reward: -98.386, steps: 392
Episode 5: reward: 168.997, steps: 649
Training for 150000 steps ...
    101/150000: episode: 1, duration: 0.680s, episode steps: 101, steps per second: 148, episode reward: -80.514, mean reward: -0.797 [-100.000,  6.883], mean action: 1.554 [0.000, 3.000],  loss: 12.809403, mse: 5478.669884, mean_q: 74.996231, mean_eps: 0.999667
    192/150000: episode: 2, duration: 0.730s, episode steps:  91, steps per second: 125, episode reward: -69.796, mean reward: -0.767 [-100.000, 16.831], mean action: 1.593 [0.000, 3.000],  loss: 8.018393, mse: 5453.873549, mean_q: 74.532044, mean_eps: 0.999124
    262/150000: episode: 3, duration: 0.496s, episode steps:  70, steps per second: 141, episode reward: -78.401, mean reward: -1.120 [-100.000,  7.790], mean action: 1.486 [0.000, 3.000],  loss: 18.678165, mse: 5459.173535, mean_q: 74.923272, mean_eps: 0.998641
    340/150000: episode: 4, duration: 0.649s, episode steps:  78, steps per second: 120, episode reward: -99.475, mean reward: -1.275 [-100.000, 15.132], mean action: 1.590 [0.000, 3.000],  loss: 22.616721, mse: 5623.405856, mean_q: 76.132420, mean_eps: 0.998197
    480/150000: episode: 5, duration: 1.165s, episode steps: 140, steps per second: 120, episode reward: -184.291, mean reward: -1.316 [-100.000, 11.665], mean action: 1.650 [0.000, 3.000],  loss: 10.738470, mse: 5590.873537, mean_q: 76.232939, mean_eps: 0.997543
    577/150000: episode: 6, duration: 0.785s, episode steps:  97, steps per second: 124, episode reward: -138.244, mean reward: -1.425 [-100.000,  5.136], mean action: 1.361 [0.000, 3.000],  loss: 6.165714, mse: 5666.604938, mean_q: 76.505157, mean_eps: 0.996832
    653/150000: episode: 7, duration: 0.632s, episode steps:  76, steps per second: 120, episode reward: -111.549, mean reward: -1.468 [-100.000,  6.006], mean action: 1.513 [0.000, 3.000],  loss: 11.756526, mse: 5702.297630, mean_q: 77.200671, mean_eps: 0.996313
    734/150000: episode: 8, duration: 0.643s, episode steps:  81, steps per second: 126, episode reward: -54.869, mean reward: -0.677 [-100.000, 22.089], mean action: 1.420 [0.000, 3.000],  loss: 25.944529, mse: 5675.442112, mean_q: 76.489943, mean_eps: 0.995842
    842/150000: episode: 9, duration: 0.828s, episode steps: 108, steps per second: 130, episode reward: -368.597, mean reward: -3.413 [-100.000,  6.207], mean action: 1.481 [0.000, 3.000],  loss: 16.436737, mse: 5546.855114, mean_q: 75.623541, mean_eps: 0.995275
    911/150000: episode: 10, duration: 0.519s, episode steps:  69, steps per second: 133, episode reward: -294.582, mean reward: -4.269 [-100.000,  8.463], mean action: 1.710 [0.000, 3.000],  loss: 6.015974, mse: 5527.481948, mean_q: 75.254832, mean_eps: 0.994744
    980/150000: episode: 11, duration: 0.494s, episode steps:  69, steps per second: 140, episode reward: -66.385, mean reward: -0.962 [-100.000, 18.922], mean action: 1.507 [0.000, 3.000],  loss: 34.144835, mse: 5459.700330, mean_q: 74.100278, mean_eps: 0.994330
   1046/150000: episode: 12, duration: 0.447s, episode steps:  66, steps per second: 148, episode reward: -139.316, mean reward: -2.111 [-100.000, 10.217], mean action: 1.379 [0.000, 3.000],  loss: 12.658798, mse: 5400.770497, mean_q: 73.936879, mean_eps: 0.993925
   1160/150000: episode: 13, duration: 0.794s, episode steps: 114, steps per second: 144, episode reward: -98.947, mean reward: -0.868 [-100.000, 37.925], mean action: 1.605 [0.000, 3.000],  loss: 17.198333, mse: 5324.839707, mean_q: 73.683490, mean_eps: 0.993385
   1226/150000: episode: 14, duration: 0.489s, episode steps:  66, steps per second: 135, episode reward: -110.925, mean reward: -1.681 [-100.000,  9.234], mean action: 1.515 [0.000, 3.000],  loss: 10.230380, mse: 5451.044900, mean_q: 74.673391, mean_eps: 0.992845
   1344/150000: episode: 15, duration: 0.845s, episode steps: 118, steps per second: 140, episode reward: -142.672, mean reward: -1.209 [-100.000,  6.201], mean action: 1.458 [0.000, 3.000],  loss: 14.828818, mse: 5194.866587, mean_q: 72.773193, mean_eps: 0.992293
   1436/150000: episode: 16, duration: 0.656s, episode steps:  92, steps per second: 140, episode reward: -162.370, mean reward: -1.765 [-100.000, 49.781], mean action: 1.652 [0.000, 3.000],  loss: 15.955437, mse: 5424.414583, mean_q: 74.119858, mean_eps: 0.991663
   1563/150000: episode: 17, duration: 0.930s, episode steps: 127, steps per second: 137, episode reward: -113.507, mean reward: -0.894 [-100.000, 11.602], mean action: 1.504 [0.000, 3.000],  loss: 17.224860, mse: 5332.120950, mean_q: 73.749268, mean_eps: 0.991006
   1653/150000: episode: 18, duration: 0.619s, episode steps:  90, steps per second: 145, episode reward: -134.979, mean reward: -1.500 [-100.000,  9.385], mean action: 1.500 [0.000, 3.000],  loss: 9.532999, mse: 5320.666073, mean_q: 73.372111, mean_eps: 0.990355
   1770/150000: episode: 19, duration: 0.806s, episode steps: 117, steps per second: 145, episode reward: -415.121, mean reward: -3.548 [-100.000,  1.809], mean action: 1.564 [0.000, 3.000],  loss: 12.339483, mse: 5514.839902, mean_q: 74.669974, mean_eps: 0.989734
   1840/150000: episode: 20, duration: 0.553s, episode steps:  70, steps per second: 127, episode reward: -94.840, mean reward: -1.355 [-100.000, 19.882], mean action: 1.614 [0.000, 3.000],  loss: 12.298649, mse: 5274.749574, mean_q: 73.010222, mean_eps: 0.989173
   1964/150000: episode: 21, duration: 0.882s, episode steps: 124, steps per second: 141, episode reward: -115.278, mean reward: -0.930 [-100.000, 21.552], mean action: 1.476 [0.000, 3.000],  loss: 10.792969, mse: 5263.300559, mean_q: 73.084889, mean_eps: 0.988591
   2038/150000: episode: 22, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -204.967, mean reward: -2.770 [-100.000,  4.600], mean action: 1.703 [0.000, 3.000],  loss: 14.015784, mse: 5338.948895, mean_q: 73.513397, mean_eps: 0.987997
   2152/150000: episode: 23, duration: 0.844s, episode steps: 114, steps per second: 135, episode reward: -132.378, mean reward: -1.161 [-100.000, 53.207], mean action: 1.351 [0.000, 3.000],  loss: 10.692188, mse: 5384.800589, mean_q: 74.089906, mean_eps: 0.987433
   2221/150000: episode: 24, duration: 0.541s, episode steps:  69, steps per second: 128, episode reward: -226.261, mean reward: -3.279 [-100.000, 25.257], mean action: 1.420 [0.000, 3.000],  loss: 9.939006, mse: 5295.145734, mean_q: 73.632008, mean_eps: 0.986884
   2291/150000: episode: 25, duration: 0.472s, episode steps:  70, steps per second: 148, episode reward: -187.340, mean reward: -2.676 [-100.000,  6.742], mean action: 1.557 [0.000, 3.000],  loss: 11.510855, mse: 5353.643018, mean_q: 74.090687, mean_eps: 0.986467
   2381/150000: episode: 26, duration: 0.647s, episode steps:  90, steps per second: 139, episode reward: -362.395, mean reward: -4.027 [-100.000,  4.831], mean action: 1.689 [0.000, 3.000],  loss: 9.757333, mse: 5300.207359, mean_q: 73.175673, mean_eps: 0.985987
   2472/150000: episode: 27, duration: 0.697s, episode steps:  91, steps per second: 131, episode reward: -202.701, mean reward: -2.227 [-100.000,  6.051], mean action: 1.352 [0.000, 3.000],  loss: 16.401807, mse: 5103.876840, mean_q: 71.925436, mean_eps: 0.985444
   2559/150000: episode: 28, duration: 0.618s, episode steps:  87, steps per second: 141, episode reward: -109.380, mean reward: -1.257 [-100.000, 12.638], mean action: 1.632 [0.000, 3.000],  loss: 9.525635, mse: 5258.764727, mean_q: 73.345199, mean_eps: 0.984910
   2620/150000: episode: 29, duration: 0.424s, episode steps:  61, steps per second: 144, episode reward: -105.020, mean reward: -1.722 [-100.000, 23.925], mean action: 1.377 [0.000, 3.000],  loss: 9.616043, mse: 5365.184354, mean_q: 74.243357, mean_eps: 0.984466
   2726/150000: episode: 30, duration: 0.795s, episode steps: 106, steps per second: 133, episode reward: -280.403, mean reward: -2.645 [-100.000, 37.811], mean action: 1.604 [0.000, 3.000],  loss: 15.951517, mse: 5259.786423, mean_q: 73.365999, mean_eps: 0.983965
   2831/150000: episode: 31, duration: 0.920s, episode steps: 105, steps per second: 114, episode reward: -89.074, mean reward: -0.848 [-100.000, 27.894], mean action: 1.295 [0.000, 3.000],  loss: 9.534594, mse: 5421.387916, mean_q: 74.408333, mean_eps: 0.983332
   2933/150000: episode: 32, duration: 0.969s, episode steps: 102, steps per second: 105, episode reward: -321.955, mean reward: -3.156 [-100.000,  7.311], mean action: 1.382 [0.000, 3.000],  loss: 14.968071, mse: 5121.941550, mean_q: 71.737554, mean_eps: 0.982711
   3037/150000: episode: 33, duration: 1.008s, episode steps: 104, steps per second: 103, episode reward: -271.764, mean reward: -2.613 [-100.000,  5.886], mean action: 1.365 [0.000, 3.000],  loss: 14.510038, mse: 5327.710977, mean_q: 73.188263, mean_eps: 0.982093
   3132/150000: episode: 34, duration: 0.866s, episode steps:  95, steps per second: 110, episode reward: -222.916, mean reward: -2.346 [-100.000,  5.440], mean action: 1.568 [0.000, 3.000],  loss: 11.553653, mse: 5323.510537, mean_q: 73.596199, mean_eps: 0.981496
   3223/150000: episode: 35, duration: 0.740s, episode steps:  91, steps per second: 123, episode reward: -455.181, mean reward: -5.002 [-100.000,  0.088], mean action: 1.736 [0.000, 3.000],  loss: 5.783678, mse: 5186.365165, mean_q: 71.861888, mean_eps: 0.980938
   3328/150000: episode: 36, duration: 0.849s, episode steps: 105, steps per second: 124, episode reward: -240.997, mean reward: -2.295 [-100.000,  0.695], mean action: 1.533 [0.000, 3.000],  loss: 18.782270, mse: 5144.811735, mean_q: 71.249341, mean_eps: 0.980350
   3441/150000: episode: 37, duration: 0.801s, episode steps: 113, steps per second: 141, episode reward: -447.779, mean reward: -3.963 [-100.000,  2.128], mean action: 1.363 [0.000, 3.000],  loss: 12.787366, mse: 5296.559447, mean_q: 72.185945, mean_eps: 0.979696
   3565/150000: episode: 38, duration: 0.877s, episode steps: 124, steps per second: 141, episode reward: -84.578, mean reward: -0.682 [-100.000, 11.602], mean action: 1.411 [0.000, 3.000],  loss: 17.626336, mse: 5182.160306, mean_q: 71.308072, mean_eps: 0.978985
   3638/150000: episode: 39, duration: 0.524s, episode steps:  73, steps per second: 139, episode reward: -107.577, mean reward: -1.474 [-100.000,  6.308], mean action: 1.466 [0.000, 3.000],  loss: 8.093046, mse: 5119.885170, mean_q: 70.648524, mean_eps: 0.978394
   3709/150000: episode: 40, duration: 0.482s, episode steps:  71, steps per second: 147, episode reward: -239.906, mean reward: -3.379 [-100.000,  5.615], mean action: 1.493 [0.000, 3.000],  loss: 14.370819, mse: 5158.196774, mean_q: 71.695968, mean_eps: 0.977962
   3800/150000: episode: 41, duration: 0.674s, episode steps:  91, steps per second: 135, episode reward: -170.621, mean reward: -1.875 [-100.000,  6.910], mean action: 1.319 [0.000, 3.000],  loss: 6.179714, mse: 5142.264753, mean_q: 71.314285, mean_eps: 0.977476
   3877/150000: episode: 42, duration: 0.537s, episode steps:  77, steps per second: 143, episode reward: -87.387, mean reward: -1.135 [-100.000, 13.086], mean action: 1.481 [0.000, 3.000],  loss: 9.977552, mse: 5237.846635, mean_q: 72.201064, mean_eps: 0.976972
   3949/150000: episode: 43, duration: 0.495s, episode steps:  72, steps per second: 145, episode reward: -89.438, mean reward: -1.242 [-100.000, 17.623], mean action: 1.472 [0.000, 3.000],  loss: 7.516412, mse: 5134.055077, mean_q: 71.804583, mean_eps: 0.976525
   4057/150000: episode: 44, duration: 0.828s, episode steps: 108, steps per second: 130, episode reward: -287.076, mean reward: -2.658 [-100.000, 21.682], mean action: 1.426 [0.000, 3.000],  loss: 6.721725, mse: 5138.083736, mean_q: 71.701683, mean_eps: 0.975985
   4165/150000: episode: 45, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: -115.861, mean reward: -1.073 [-100.000, 12.258], mean action: 1.556 [0.000, 3.000],  loss: 12.709046, mse: 5164.854653, mean_q: 71.150753, mean_eps: 0.975337
   4284/150000: episode: 46, duration: 0.816s, episode steps: 119, steps per second: 146, episode reward: -428.929, mean reward: -3.604 [-100.000,  1.970], mean action: 1.471 [0.000, 3.000],  loss: 11.018742, mse: 5198.795226, mean_q: 71.875054, mean_eps: 0.974656
   4349/150000: episode: 47, duration: 0.458s, episode steps:  65, steps per second: 142, episode reward: -121.059, mean reward: -1.862 [-100.000, 12.698], mean action: 1.400 [0.000, 3.000],  loss: 11.598468, mse: 5180.770763, mean_q: 70.531688, mean_eps: 0.974104
   4475/150000: episode: 48, duration: 0.876s, episode steps: 126, steps per second: 144, episode reward: -182.504, mean reward: -1.448 [-100.000,  8.904], mean action: 1.579 [0.000, 3.000],  loss: 14.933929, mse: 5248.446746, mean_q: 71.944243, mean_eps: 0.973531
   4555/150000: episode: 49, duration: 0.548s, episode steps:  80, steps per second: 146, episode reward: -114.423, mean reward: -1.430 [-100.000, 17.962], mean action: 1.462 [0.000, 3.000],  loss: 9.062741, mse: 5087.567709, mean_q: 70.919085, mean_eps: 0.972913
   4662/150000: episode: 50, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -223.976, mean reward: -2.093 [-100.000,  1.078], mean action: 1.523 [0.000, 3.000],  loss: 9.124872, mse: 5221.540794, mean_q: 72.126050, mean_eps: 0.972352
   4728/150000: episode: 51, duration: 0.472s, episode steps:  66, steps per second: 140, episode reward: -156.817, mean reward: -2.376 [-100.000,  5.481], mean action: 1.758 [0.000, 3.000],  loss: 12.606746, mse: 5141.482921, mean_q: 72.056252, mean_eps: 0.971833
   4795/150000: episode: 52, duration: 0.452s, episode steps:  67, steps per second: 148, episode reward: -136.893, mean reward: -2.043 [-100.000, 14.379], mean action: 1.448 [0.000, 3.000],  loss: 23.830024, mse: 4986.888256, mean_q: 70.060745, mean_eps: 0.971434
   4879/150000: episode: 53, duration: 0.586s, episode steps:  84, steps per second: 143, episode reward: -135.137, mean reward: -1.609 [-100.000,  6.567], mean action: 1.655 [0.000, 3.000],  loss: 14.011166, mse: 4973.386271, mean_q: 70.115921, mean_eps: 0.970981
   4946/150000: episode: 54, duration: 0.476s, episode steps:  67, steps per second: 141, episode reward: -119.911, mean reward: -1.790 [-100.000,  6.189], mean action: 1.597 [0.000, 3.000],  loss: 12.367367, mse: 5019.726799, mean_q: 70.710675, mean_eps: 0.970528
   5050/150000: episode: 55, duration: 0.729s, episode steps: 104, steps per second: 143, episode reward: -166.741, mean reward: -1.603 [-100.000,  6.906], mean action: 1.471 [0.000, 3.000],  loss: 12.737050, mse: 5068.819207, mean_q: 70.388308, mean_eps: 0.970015
   5110/150000: episode: 56, duration: 0.427s, episode steps:  60, steps per second: 141, episode reward: -72.338, mean reward: -1.206 [-100.000,  8.837], mean action: 1.767 [0.000, 3.000],  loss: 7.883936, mse: 4930.585335, mean_q: 69.005227, mean_eps: 0.969523
   5253/150000: episode: 57, duration: 1.006s, episode steps: 143, steps per second: 142, episode reward: -165.735, mean reward: -1.159 [-100.000, 21.732], mean action: 1.483 [0.000, 3.000],  loss: 14.356530, mse: 4860.585113, mean_q: 68.923282, mean_eps: 0.968914
   5335/150000: episode: 58, duration: 0.586s, episode steps:  82, steps per second: 140, episode reward: -148.963, mean reward: -1.817 [-100.000, 23.110], mean action: 1.561 [0.000, 3.000],  loss: 7.120625, mse: 4779.939274, mean_q: 67.949253, mean_eps: 0.968239
   5488/150000: episode: 59, duration: 1.093s, episode steps: 153, steps per second: 140, episode reward: -118.627, mean reward: -0.775 [-100.000, 12.110], mean action: 1.516 [0.000, 3.000],  loss: 11.234842, mse: 4796.418438, mean_q: 67.845274, mean_eps: 0.967534
   5574/150000: episode: 60, duration: 0.602s, episode steps:  86, steps per second: 143, episode reward: -340.090, mean reward: -3.955 [-100.000,  0.524], mean action: 1.488 [0.000, 3.000],  loss: 10.642987, mse: 4762.094922, mean_q: 68.065376, mean_eps: 0.966817
   5678/150000: episode: 61, duration: 0.772s, episode steps: 104, steps per second: 135, episode reward: -123.751, mean reward: -1.190 [-100.000,  9.650], mean action: 1.462 [0.000, 3.000],  loss: 8.050089, mse: 4738.882343, mean_q: 68.190058, mean_eps: 0.966247
   5753/150000: episode: 62, duration: 0.531s, episode steps:  75, steps per second: 141, episode reward: -165.137, mean reward: -2.202 [-100.000, 64.766], mean action: 1.573 [0.000, 3.000],  loss: 11.413669, mse: 4787.060381, mean_q: 68.534227, mean_eps: 0.965710
   5866/150000: episode: 63, duration: 0.811s, episode steps: 113, steps per second: 139, episode reward: -337.795, mean reward: -2.989 [-100.000,  1.207], mean action: 1.407 [0.000, 3.000],  loss: 11.163018, mse: 4855.290795, mean_q: 68.276288, mean_eps: 0.965146
   5928/150000: episode: 64, duration: 0.440s, episode steps:  62, steps per second: 141, episode reward: -85.872, mean reward: -1.385 [-100.000, 11.943], mean action: 1.661 [0.000, 3.000],  loss: 6.871792, mse: 4865.794564, mean_q: 69.683473, mean_eps: 0.964621
   6036/150000: episode: 65, duration: 0.774s, episode steps: 108, steps per second: 139, episode reward: -118.802, mean reward: -1.100 [-100.000,  6.268], mean action: 1.694 [0.000, 3.000],  loss: 13.300338, mse: 4664.253863, mean_q: 66.745900, mean_eps: 0.964111
   6139/150000: episode: 66, duration: 0.714s, episode steps: 103, steps per second: 144, episode reward: -433.573, mean reward: -4.209 [-100.000,  0.642], mean action: 1.437 [0.000, 3.000],  loss: 9.567291, mse: 4795.132362, mean_q: 68.259188, mean_eps: 0.963478
   6201/150000: episode: 67, duration: 0.436s, episode steps:  62, steps per second: 142, episode reward: -90.507, mean reward: -1.460 [-100.000,  7.321], mean action: 1.677 [0.000, 3.000],  loss: 9.110778, mse: 4786.896366, mean_q: 67.883913, mean_eps: 0.962983
   6293/150000: episode: 68, duration: 0.658s, episode steps:  92, steps per second: 140, episode reward: -13.935, mean reward: -0.151 [-100.000, 64.947], mean action: 1.348 [0.000, 3.000],  loss: 13.933681, mse: 4904.719955, mean_q: 69.094069, mean_eps: 0.962521
   6362/150000: episode: 69, duration: 0.511s, episode steps:  69, steps per second: 135, episode reward: -53.502, mean reward: -0.775 [-100.000, 17.731], mean action: 1.507 [0.000, 3.000],  loss: 10.580875, mse: 4904.841478, mean_q: 69.170537, mean_eps: 0.962038
   6469/150000: episode: 70, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -89.744, mean reward: -0.839 [-100.000, 10.868], mean action: 1.467 [0.000, 3.000],  loss: 16.222128, mse: 4769.058484, mean_q: 67.272768, mean_eps: 0.961510
   6533/150000: episode: 71, duration: 0.487s, episode steps:  64, steps per second: 131, episode reward: -68.417, mean reward: -1.069 [-100.000,  6.356], mean action: 1.766 [0.000, 3.000],  loss: 12.136097, mse: 4574.506866, mean_q: 66.235498, mean_eps: 0.960997
   6625/150000: episode: 72, duration: 0.655s, episode steps:  92, steps per second: 140, episode reward: -347.858, mean reward: -3.781 [-100.000, 106.359], mean action: 1.554 [0.000, 3.000],  loss: 13.170301, mse: 4688.100650, mean_q: 67.016860, mean_eps: 0.960529
   6704/150000: episode: 73, duration: 0.544s, episode steps:  79, steps per second: 145, episode reward: -98.882, mean reward: -1.252 [-100.000,  6.196], mean action: 1.646 [0.000, 3.000],  loss: 15.022624, mse: 4777.778682, mean_q: 67.122378, mean_eps: 0.960016
   6798/150000: episode: 74, duration: 0.640s, episode steps:  94, steps per second: 147, episode reward: -323.383, mean reward: -3.440 [-100.000,  6.018], mean action: 1.436 [0.000, 3.000],  loss: 13.551897, mse: 4737.982006, mean_q: 66.037937, mean_eps: 0.959497
   6863/150000: episode: 75, duration: 0.479s, episode steps:  65, steps per second: 136, episode reward: -90.133, mean reward: -1.387 [-100.000, 22.794], mean action: 1.385 [0.000, 3.000],  loss: 15.740779, mse: 4850.236140, mean_q: 68.105857, mean_eps: 0.959020
   6945/150000: episode: 76, duration: 0.569s, episode steps:  82, steps per second: 144, episode reward: -56.189, mean reward: -0.685 [-100.000, 17.045], mean action: 1.585 [0.000, 3.000],  loss: 7.950244, mse: 4757.020922, mean_q: 67.110968, mean_eps: 0.958579
   7064/150000: episode: 77, duration: 0.809s, episode steps: 119, steps per second: 147, episode reward: -190.304, mean reward: -1.599 [-100.000, 10.334], mean action: 1.521 [0.000, 3.000],  loss: 8.473933, mse: 4749.319910, mean_q: 67.162404, mean_eps: 0.957976
   7167/150000: episode: 78, duration: 0.761s, episode steps: 103, steps per second: 135, episode reward: -186.957, mean reward: -1.815 [-100.000,  7.864], mean action: 1.476 [0.000, 3.000],  loss: 14.205866, mse: 4586.433011, mean_q: 65.082238, mean_eps: 0.957310
   7272/150000: episode: 79, duration: 0.719s, episode steps: 105, steps per second: 146, episode reward: -111.943, mean reward: -1.066 [-100.000, 11.629], mean action: 1.543 [0.000, 3.000],  loss: 9.079998, mse: 4812.730908, mean_q: 67.068954, mean_eps: 0.956686
   7389/150000: episode: 80, duration: 0.794s, episode steps: 117, steps per second: 147, episode reward: -345.845, mean reward: -2.956 [-100.000, 121.833], mean action: 1.470 [0.000, 3.000],  loss: 11.534857, mse: 4841.794567, mean_q: 67.102903, mean_eps: 0.956020
   7490/150000: episode: 81, duration: 0.719s, episode steps: 101, steps per second: 140, episode reward: -93.364, mean reward: -0.924 [-100.000, 13.319], mean action: 1.614 [0.000, 3.000],  loss: 9.230359, mse: 4859.030955, mean_q: 66.632800, mean_eps: 0.955366
   7556/150000: episode: 82, duration: 0.446s, episode steps:  66, steps per second: 148, episode reward: -59.832, mean reward: -0.907 [-100.000, 13.976], mean action: 1.273 [0.000, 3.000],  loss: 8.700990, mse: 5015.670270, mean_q: 68.150506, mean_eps: 0.954865
   7631/150000: episode: 83, duration: 0.506s, episode steps:  75, steps per second: 148, episode reward: -86.348, mean reward: -1.151 [-100.000, 13.710], mean action: 1.467 [0.000, 3.000],  loss: 12.989431, mse: 4988.352565, mean_q: 67.247279, mean_eps: 0.954442
   7705/150000: episode: 84, duration: 0.500s, episode steps:  74, steps per second: 148, episode reward: -109.505, mean reward: -1.480 [-100.000, 17.034], mean action: 1.419 [0.000, 3.000],  loss: 18.782691, mse: 4904.612549, mean_q: 66.909409, mean_eps: 0.953995
   7819/150000: episode: 85, duration: 0.779s, episode steps: 114, steps per second: 146, episode reward: -252.048, mean reward: -2.211 [-100.000,  4.648], mean action: 1.412 [0.000, 3.000],  loss: 7.383942, mse: 4971.376206, mean_q: 66.982765, mean_eps: 0.953431
   7953/150000: episode: 86, duration: 0.893s, episode steps: 134, steps per second: 150, episode reward: -201.384, mean reward: -1.503 [-100.000, 11.909], mean action: 1.634 [0.000, 3.000],  loss: 12.419915, mse: 4896.988635, mean_q: 66.451697, mean_eps: 0.952687
   8036/150000: episode: 87, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -123.611, mean reward: -1.489 [-100.000,  7.728], mean action: 1.735 [0.000, 3.000],  loss: 10.101954, mse: 4861.580234, mean_q: 66.374668, mean_eps: 0.952036
   8159/150000: episode: 88, duration: 0.911s, episode steps: 123, steps per second: 135, episode reward: -386.928, mean reward: -3.146 [-100.000,  1.777], mean action: 1.496 [0.000, 3.000],  loss: 10.981693, mse: 4829.394891, mean_q: 65.688818, mean_eps: 0.951418
   8272/150000: episode: 89, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -116.342, mean reward: -1.030 [-100.000,  8.226], mean action: 1.487 [0.000, 3.000],  loss: 14.202105, mse: 4980.851366, mean_q: 67.117560, mean_eps: 0.950710
   8399/150000: episode: 90, duration: 0.886s, episode steps: 127, steps per second: 143, episode reward: -27.615, mean reward: -0.217 [-100.000, 105.940], mean action: 1.677 [0.000, 3.000],  loss: 10.360367, mse: 4888.131480, mean_q: 66.481423, mean_eps: 0.949990
   8467/150000: episode: 91, duration: 0.470s, episode steps:  68, steps per second: 145, episode reward: -108.014, mean reward: -1.588 [-100.000, 53.027], mean action: 1.324 [0.000, 3.000],  loss: 11.283273, mse: 5033.362438, mean_q: 66.430194, mean_eps: 0.949405
   8533/150000: episode: 92, duration: 0.457s, episode steps:  66, steps per second: 144, episode reward: -105.348, mean reward: -1.596 [-100.000, 12.714], mean action: 1.470 [0.000, 3.000],  loss: 12.326403, mse: 5142.186993, mean_q: 69.198229, mean_eps: 0.949003
   8625/150000: episode: 93, duration: 0.637s, episode steps:  92, steps per second: 144, episode reward: -322.392, mean reward: -3.504 [-100.000,  0.585], mean action: 1.587 [0.000, 3.000],  loss: 17.713893, mse: 4912.906115, mean_q: 66.244950, mean_eps: 0.948529
   8705/150000: episode: 94, duration: 0.627s, episode steps:  80, steps per second: 128, episode reward: -54.952, mean reward: -0.687 [-100.000, 105.737], mean action: 1.387 [0.000, 3.000],  loss: 16.828836, mse: 5084.430768, mean_q: 68.814553, mean_eps: 0.948013
   8827/150000: episode: 95, duration: 0.897s, episode steps: 122, steps per second: 136, episode reward: -161.783, mean reward: -1.326 [-100.000,  4.932], mean action: 1.566 [0.000, 3.000],  loss: 11.224929, mse: 4800.568750, mean_q: 66.303100, mean_eps: 0.947407
   8937/150000: episode: 96, duration: 0.733s, episode steps: 110, steps per second: 150, episode reward: -518.726, mean reward: -4.716 [-100.000, 50.564], mean action: 1.636 [0.000, 3.000],  loss: 19.265038, mse: 4708.990039, mean_q: 65.103234, mean_eps: 0.946711
   9049/150000: episode: 97, duration: 0.772s, episode steps: 112, steps per second: 145, episode reward: -53.647, mean reward: -0.479 [-100.000, 10.867], mean action: 1.500 [0.000, 3.000],  loss: 14.425545, mse: 4758.448436, mean_q: 65.246984, mean_eps: 0.946045
   9156/150000: episode: 98, duration: 0.718s, episode steps: 107, steps per second: 149, episode reward: -130.368, mean reward: -1.218 [-100.000, 43.524], mean action: 1.458 [0.000, 3.000],  loss: 13.767890, mse: 4845.365816, mean_q: 66.198600, mean_eps: 0.945388
   9223/150000: episode: 99, duration: 0.450s, episode steps:  67, steps per second: 149, episode reward: -34.984, mean reward: -0.522 [-100.000, 58.013], mean action: 1.612 [0.000, 3.000],  loss: 17.694346, mse: 4712.456984, mean_q: 65.264478, mean_eps: 0.944866
   9291/150000: episode: 100, duration: 0.488s, episode steps:  68, steps per second: 139, episode reward: -120.911, mean reward: -1.778 [-100.000, 11.838], mean action: 1.691 [0.000, 3.000],  loss: 11.425388, mse: 4750.650215, mean_q: 65.078974, mean_eps: 0.944461
   9365/150000: episode: 101, duration: 0.611s, episode steps:  74, steps per second: 121, episode reward: -135.670, mean reward: -1.833 [-100.000,  7.625], mean action: 1.432 [0.000, 3.000],  loss: 13.784311, mse: 4796.389137, mean_q: 65.717646, mean_eps: 0.944035
   9439/150000: episode: 102, duration: 0.590s, episode steps:  74, steps per second: 125, episode reward: 43.595, mean reward:  0.589 [-100.000, 114.075], mean action: 1.541 [0.000, 3.000],  loss: 11.203502, mse: 4872.555100, mean_q: 66.912097, mean_eps: 0.943591
   9519/150000: episode: 103, duration: 0.599s, episode steps:  80, steps per second: 133, episode reward: -181.125, mean reward: -2.264 [-100.000,  5.531], mean action: 1.450 [0.000, 3.000],  loss: 16.103977, mse: 4745.688907, mean_q: 65.812378, mean_eps: 0.943129
   9589/150000: episode: 104, duration: 0.585s, episode steps:  70, steps per second: 120, episode reward: -92.951, mean reward: -1.328 [-100.000, 22.411], mean action: 1.357 [0.000, 3.000],  loss: 11.395433, mse: 4629.006843, mean_q: 65.318601, mean_eps: 0.942679
   9661/150000: episode: 105, duration: 0.587s, episode steps:  72, steps per second: 123, episode reward: -140.170, mean reward: -1.947 [-100.000, 11.768], mean action: 1.403 [0.000, 3.000],  loss: 9.777669, mse: 4759.720361, mean_q: 65.710772, mean_eps: 0.942253
   9735/150000: episode: 106, duration: 0.561s, episode steps:  74, steps per second: 132, episode reward: -180.866, mean reward: -2.444 [-100.000, 73.312], mean action: 1.446 [0.000, 3.000],  loss: 13.386491, mse: 4658.151034, mean_q: 64.835871, mean_eps: 0.941815
   9849/150000: episode: 107, duration: 0.891s, episode steps: 114, steps per second: 128, episode reward: -29.901, mean reward: -0.262 [-100.000, 80.205], mean action: 1.544 [0.000, 3.000],  loss: 13.837614, mse: 4690.815376, mean_q: 64.626986, mean_eps: 0.941251
   9905/150000: episode: 108, duration: 0.466s, episode steps:  56, steps per second: 120, episode reward: -124.280, mean reward: -2.219 [-100.000, 10.632], mean action: 1.589 [0.000, 3.000],  loss: 13.219353, mse: 4762.550607, mean_q: 64.699624, mean_eps: 0.940741
   9976/150000: episode: 109, duration: 0.504s, episode steps:  71, steps per second: 141, episode reward: -220.706, mean reward: -3.109 [-100.000,  7.136], mean action: 1.577 [0.000, 3.000],  loss: 19.650603, mse: 4617.895955, mean_q: 64.057802, mean_eps: 0.940360
  10068/150000: episode: 110, duration: 0.616s, episode steps:  92, steps per second: 149, episode reward: -141.950, mean reward: -1.543 [-100.000, 10.980], mean action: 1.609 [0.000, 3.000],  loss: 10.106070, mse: 4675.066565, mean_q: 64.301843, mean_eps: 0.939871
  10128/150000: episode: 111, duration: 0.410s, episode steps:  60, steps per second: 146, episode reward: -75.105, mean reward: -1.252 [-100.000, 17.064], mean action: 1.500 [0.000, 3.000],  loss: 13.252155, mse: 4479.711686, mean_q: 62.097445, mean_eps: 0.939415
  10235/150000: episode: 112, duration: 0.747s, episode steps: 107, steps per second: 143, episode reward: -393.071, mean reward: -3.674 [-100.000,  0.246], mean action: 1.673 [0.000, 3.000],  loss: 15.302398, mse: 4558.412141, mean_q: 62.236331, mean_eps: 0.938914
  10326/150000: episode: 113, duration: 0.610s, episode steps:  91, steps per second: 149, episode reward: -72.283, mean reward: -0.794 [-100.000, 13.005], mean action: 1.560 [0.000, 3.000],  loss: 14.196735, mse: 4652.954134, mean_q: 63.215112, mean_eps: 0.938320
  10427/150000: episode: 114, duration: 0.671s, episode steps: 101, steps per second: 151, episode reward: -284.699, mean reward: -2.819 [-100.000,  0.599], mean action: 1.574 [0.000, 3.000],  loss: 13.358028, mse: 4653.310958, mean_q: 63.516422, mean_eps: 0.937744
  10531/150000: episode: 115, duration: 0.722s, episode steps: 104, steps per second: 144, episode reward: -287.952, mean reward: -2.769 [-100.000, 12.005], mean action: 1.250 [0.000, 3.000],  loss: 20.348318, mse: 4685.301216, mean_q: 64.290449, mean_eps: 0.937129
  10646/150000: episode: 116, duration: 0.766s, episode steps: 115, steps per second: 150, episode reward: -77.479, mean reward: -0.674 [-100.000,  7.264], mean action: 1.591 [0.000, 3.000],  loss: 12.991452, mse: 4580.224546, mean_q: 63.541101, mean_eps: 0.936472
  10760/150000: episode: 117, duration: 0.765s, episode steps: 114, steps per second: 149, episode reward: -300.314, mean reward: -2.634 [-100.000, 71.553], mean action: 1.605 [0.000, 3.000],  loss: 11.472114, mse: 4722.215807, mean_q: 64.072899, mean_eps: 0.935785
  10850/150000: episode: 118, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -107.394, mean reward: -1.193 [-100.000, 20.686], mean action: 1.356 [0.000, 3.000],  loss: 14.986328, mse: 4685.550817, mean_q: 62.779585, mean_eps: 0.935173
  10973/150000: episode: 119, duration: 0.824s, episode steps: 123, steps per second: 149, episode reward: -124.086, mean reward: -1.009 [-100.000,  6.015], mean action: 1.537 [0.000, 3.000],  loss: 12.833902, mse: 4785.124617, mean_q: 64.869228, mean_eps: 0.934534
  11051/150000: episode: 120, duration: 0.526s, episode steps:  78, steps per second: 148, episode reward: -112.689, mean reward: -1.445 [-100.000, 10.028], mean action: 1.500 [0.000, 3.000],  loss: 13.983814, mse: 4666.878002, mean_q: 63.383065, mean_eps: 0.933931
  11120/150000: episode: 121, duration: 0.485s, episode steps:  69, steps per second: 142, episode reward: -88.651, mean reward: -1.285 [-100.000, 19.245], mean action: 1.768 [0.000, 3.000],  loss: 13.119655, mse: 4551.987160, mean_q: 62.091856, mean_eps: 0.933490
  11191/150000: episode: 122, duration: 0.478s, episode steps:  71, steps per second: 149, episode reward: -64.570, mean reward: -0.909 [-100.000,  7.764], mean action: 1.606 [0.000, 3.000],  loss: 14.144630, mse: 4468.647571, mean_q: 61.721532, mean_eps: 0.933070
  11274/150000: episode: 123, duration: 0.554s, episode steps:  83, steps per second: 150, episode reward: -128.690, mean reward: -1.550 [-100.000, 12.910], mean action: 1.639 [0.000, 3.000],  loss: 12.351005, mse: 4677.813429, mean_q: 63.148423, mean_eps: 0.932608
  11407/150000: episode: 124, duration: 0.949s, episode steps: 133, steps per second: 140, episode reward: -140.566, mean reward: -1.057 [-100.000,  6.436], mean action: 1.609 [0.000, 3.000],  loss: 9.864820, mse: 4664.925168, mean_q: 63.747656, mean_eps: 0.931960
  11522/150000: episode: 125, duration: 0.853s, episode steps: 115, steps per second: 135, episode reward: -32.501, mean reward: -0.283 [-100.000, 87.846], mean action: 1.348 [0.000, 3.000],  loss: 17.560802, mse: 4696.684258, mean_q: 64.767067, mean_eps: 0.931216
  11601/150000: episode: 126, duration: 0.609s, episode steps:  79, steps per second: 130, episode reward: -101.281, mean reward: -1.282 [-100.000, 11.960], mean action: 1.544 [0.000, 3.000],  loss: 16.239678, mse: 4667.732558, mean_q: 63.910011, mean_eps: 0.930634
  11693/150000: episode: 127, duration: 0.722s, episode steps:  92, steps per second: 127, episode reward: -212.637, mean reward: -2.311 [-100.000, 21.576], mean action: 1.533 [0.000, 3.000],  loss: 14.052078, mse: 4767.283187, mean_q: 64.869002, mean_eps: 0.930121
  11804/150000: episode: 128, duration: 1.220s, episode steps: 111, steps per second:  91, episode reward: -41.814, mean reward: -0.377 [-100.000, 89.212], mean action: 1.532 [0.000, 3.000],  loss: 19.845049, mse: 4687.634343, mean_q: 63.937647, mean_eps: 0.929512
  11869/150000: episode: 129, duration: 0.582s, episode steps:  65, steps per second: 112, episode reward: -55.288, mean reward: -0.851 [-100.000, 11.899], mean action: 1.292 [0.000, 3.000],  loss: 14.707903, mse: 4695.239183, mean_q: 63.842425, mean_eps: 0.928984
  11949/150000: episode: 130, duration: 0.731s, episode steps:  80, steps per second: 109, episode reward: -110.634, mean reward: -1.383 [-100.000, 12.850], mean action: 1.525 [0.000, 3.000],  loss: 11.993857, mse: 4678.590021, mean_q: 64.021933, mean_eps: 0.928549
  12022/150000: episode: 131, duration: 0.522s, episode steps:  73, steps per second: 140, episode reward: -105.479, mean reward: -1.445 [-100.000,  7.065], mean action: 1.521 [0.000, 3.000],  loss: 19.888224, mse: 4840.161250, mean_q: 66.208347, mean_eps: 0.928090
  12097/150000: episode: 132, duration: 0.549s, episode steps:  75, steps per second: 137, episode reward: -106.587, mean reward: -1.421 [-100.000,  9.980], mean action: 1.507 [0.000, 3.000],  loss: 9.787916, mse: 4709.614880, mean_q: 64.063632, mean_eps: 0.927646
  12158/150000: episode: 133, duration: 0.430s, episode steps:  61, steps per second: 142, episode reward: -262.398, mean reward: -4.302 [-100.000,  3.346], mean action: 1.951 [0.000, 3.000],  loss: 11.650588, mse: 4668.165787, mean_q: 63.836323, mean_eps: 0.927238
  12250/150000: episode: 134, duration: 0.902s, episode steps:  92, steps per second: 102, episode reward: -85.204, mean reward: -0.926 [-100.000,  7.380], mean action: 1.554 [0.000, 3.000],  loss: 15.418970, mse: 4694.530348, mean_q: 64.352182, mean_eps: 0.926779
  12357/150000: episode: 135, duration: 1.058s, episode steps: 107, steps per second: 101, episode reward: -118.181, mean reward: -1.104 [-100.000, 14.640], mean action: 1.364 [0.000, 3.000],  loss: 18.668209, mse: 4701.963717, mean_q: 64.241609, mean_eps: 0.926182
  12449/150000: episode: 136, duration: 0.781s, episode steps:  92, steps per second: 118, episode reward: -182.970, mean reward: -1.989 [-100.000, 79.812], mean action: 1.609 [0.000, 3.000],  loss: 11.496703, mse: 4685.114741, mean_q: 64.791774, mean_eps: 0.925585
  12511/150000: episode: 137, duration: 0.454s, episode steps:  62, steps per second: 137, episode reward: -121.501, mean reward: -1.960 [-100.000,  7.832], mean action: 1.452 [0.000, 3.000],  loss: 10.228834, mse: 4614.329617, mean_q: 63.693752, mean_eps: 0.925123
  12606/150000: episode: 138, duration: 0.712s, episode steps:  95, steps per second: 133, episode reward: -120.277, mean reward: -1.266 [-100.000, 17.675], mean action: 1.347 [0.000, 3.000],  loss: 16.487644, mse: 4637.247232, mean_q: 63.873223, mean_eps: 0.924652
  12705/150000: episode: 139, duration: 0.778s, episode steps:  99, steps per second: 127, episode reward: -88.956, mean reward: -0.899 [-100.000,  8.121], mean action: 1.515 [0.000, 3.000],  loss: 16.071384, mse: 4736.298254, mean_q: 64.727396, mean_eps: 0.924070
  12827/150000: episode: 140, duration: 1.075s, episode steps: 122, steps per second: 113, episode reward: -63.737, mean reward: -0.522 [-100.000, 21.893], mean action: 1.361 [0.000, 3.000],  loss: 18.238393, mse: 4629.819432, mean_q: 63.410266, mean_eps: 0.923407
  12895/150000: episode: 141, duration: 0.566s, episode steps:  68, steps per second: 120, episode reward: -110.217, mean reward: -1.621 [-100.000, 25.810], mean action: 1.456 [0.000, 3.000],  loss: 16.578607, mse: 4574.094773, mean_q: 63.499127, mean_eps: 0.922837
  12971/150000: episode: 142, duration: 0.723s, episode steps:  76, steps per second: 105, episode reward: -61.111, mean reward: -0.804 [-100.000, 10.345], mean action: 1.605 [0.000, 3.000],  loss: 14.802961, mse: 4573.906128, mean_q: 63.019864, mean_eps: 0.922405
  13066/150000: episode: 143, duration: 0.788s, episode steps:  95, steps per second: 121, episode reward: -288.750, mean reward: -3.039 [-100.000,  0.553], mean action: 1.516 [0.000, 3.000],  loss: 11.607577, mse: 4655.378459, mean_q: 62.556899, mean_eps: 0.921892
  13147/150000: episode: 144, duration: 0.568s, episode steps:  81, steps per second: 143, episode reward: -110.234, mean reward: -1.361 [-100.000,  5.853], mean action: 1.691 [0.000, 3.000],  loss: 13.104224, mse: 4698.007701, mean_q: 62.900779, mean_eps: 0.921364
  13212/150000: episode: 145, duration: 0.583s, episode steps:  65, steps per second: 112, episode reward: -81.381, mean reward: -1.252 [-100.000,  7.137], mean action: 1.538 [0.000, 3.000],  loss: 20.656132, mse: 4885.217180, mean_q: 64.517934, mean_eps: 0.920926
  13337/150000: episode: 146, duration: 1.179s, episode steps: 125, steps per second: 106, episode reward: -55.842, mean reward: -0.447 [-100.000, 10.386], mean action: 1.552 [0.000, 3.000],  loss: 18.744043, mse: 4751.985451, mean_q: 62.220740, mean_eps: 0.920356
  13423/150000: episode: 147, duration: 0.935s, episode steps:  86, steps per second:  92, episode reward: -82.720, mean reward: -0.962 [-100.000, 17.646], mean action: 1.465 [0.000, 3.000],  loss: 13.632774, mse: 4814.371960, mean_q: 63.774058, mean_eps: 0.919723
  13523/150000: episode: 148, duration: 0.913s, episode steps: 100, steps per second: 109, episode reward: -324.762, mean reward: -3.248 [-100.000,  4.564], mean action: 1.570 [0.000, 3.000],  loss: 17.097256, mse: 4796.038386, mean_q: 62.781189, mean_eps: 0.919165
  13599/150000: episode: 149, duration: 0.608s, episode steps:  76, steps per second: 125, episode reward: -119.902, mean reward: -1.578 [-100.000,  8.615], mean action: 1.618 [0.000, 3.000],  loss: 22.221079, mse: 4838.830974, mean_q: 62.713487, mean_eps: 0.918637
  13740/150000: episode: 150, duration: 1.000s, episode steps: 141, steps per second: 141, episode reward: -129.812, mean reward: -0.921 [-100.000,  8.407], mean action: 1.582 [0.000, 3.000],  loss: 17.864385, mse: 4544.566893, mean_q: 62.692276, mean_eps: 0.917986
  13829/150000: episode: 151, duration: 0.631s, episode steps:  89, steps per second: 141, episode reward: -91.756, mean reward: -1.031 [-100.000, 17.043], mean action: 1.573 [0.000, 3.000],  loss: 14.068165, mse: 4489.460117, mean_q: 61.889378, mean_eps: 0.917296
  13910/150000: episode: 152, duration: 0.551s, episode steps:  81, steps per second: 147, episode reward: -103.131, mean reward: -1.273 [-100.000,  5.805], mean action: 1.617 [0.000, 3.000],  loss: 13.019099, mse: 4547.347114, mean_q: 62.360559, mean_eps: 0.916786
  14014/150000: episode: 153, duration: 0.715s, episode steps: 104, steps per second: 145, episode reward: -127.202, mean reward: -1.223 [-100.000,  6.964], mean action: 1.500 [0.000, 3.000],  loss: 18.984063, mse: 4702.222567, mean_q: 63.765563, mean_eps: 0.916231
  14099/150000: episode: 154, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -97.403, mean reward: -1.146 [-100.000,  7.484], mean action: 1.400 [0.000, 3.000],  loss: 11.030600, mse: 4637.513675, mean_q: 63.026125, mean_eps: 0.915664
  14190/150000: episode: 155, duration: 0.612s, episode steps:  91, steps per second: 149, episode reward: -96.507, mean reward: -1.061 [-100.000, 20.432], mean action: 1.604 [0.000, 3.000],  loss: 14.225863, mse: 4864.864966, mean_q: 63.915834, mean_eps: 0.915136
  14308/150000: episode: 156, duration: 0.802s, episode steps: 118, steps per second: 147, episode reward: -132.616, mean reward: -1.124 [-100.000, 11.839], mean action: 1.585 [0.000, 3.000],  loss: 17.023880, mse: 4854.405809, mean_q: 64.635580, mean_eps: 0.914509
  14407/150000: episode: 157, duration: 0.693s, episode steps:  99, steps per second: 143, episode reward: -174.440, mean reward: -1.762 [-100.000,  6.804], mean action: 1.495 [0.000, 3.000],  loss: 14.008723, mse: 4906.623932, mean_q: 64.153304, mean_eps: 0.913858
  14522/150000: episode: 158, duration: 0.783s, episode steps: 115, steps per second: 147, episode reward: -158.356, mean reward: -1.377 [-100.000,  6.682], mean action: 1.409 [0.000, 3.000],  loss: 16.786418, mse: 4920.526832, mean_q: 65.068036, mean_eps: 0.913216
  14583/150000: episode: 159, duration: 0.520s, episode steps:  61, steps per second: 117, episode reward: -134.282, mean reward: -2.201 [-100.000,  6.591], mean action: 1.377 [0.000, 3.000],  loss: 24.900142, mse: 4808.011351, mean_q: 63.625819, mean_eps: 0.912688
  14661/150000: episode: 160, duration: 0.623s, episode steps:  78, steps per second: 125, episode reward: -82.199, mean reward: -1.054 [-100.000,  7.514], mean action: 1.590 [0.000, 3.000],  loss: 15.650915, mse: 4750.132687, mean_q: 61.920841, mean_eps: 0.912271
  14737/150000: episode: 161, duration: 0.598s, episode steps:  76, steps per second: 127, episode reward: -90.625, mean reward: -1.192 [-100.000,  7.281], mean action: 1.579 [0.000, 3.000],  loss: 15.215325, mse: 5184.195162, mean_q: 66.047563, mean_eps: 0.911809
  14864/150000: episode: 162, duration: 0.925s, episode steps: 127, steps per second: 137, episode reward: -250.907, mean reward: -1.976 [-100.000,  2.266], mean action: 1.488 [0.000, 3.000],  loss: 11.376108, mse: 5165.557192, mean_q: 66.625232, mean_eps: 0.911200
  14928/150000: episode: 163, duration: 0.488s, episode steps:  64, steps per second: 131, episode reward: -81.606, mean reward: -1.275 [-100.000,  7.662], mean action: 1.438 [0.000, 3.000],  loss: 17.992735, mse: 5212.011745, mean_q: 65.947422, mean_eps: 0.910627
  15031/150000: episode: 164, duration: 0.721s, episode steps: 103, steps per second: 143, episode reward: -209.543, mean reward: -2.034 [-100.000,  4.783], mean action: 1.699 [0.000, 3.000],  loss: 15.393453, mse: 5148.800931, mean_q: 65.914997, mean_eps: 0.910126
  15093/150000: episode: 165, duration: 0.429s, episode steps:  62, steps per second: 144, episode reward: -113.517, mean reward: -1.831 [-100.000, 24.472], mean action: 1.613 [0.000, 3.000],  loss: 15.968829, mse: 5110.329936, mean_q: 65.487216, mean_eps: 0.909631
  15186/150000: episode: 166, duration: 0.620s, episode steps:  93, steps per second: 150, episode reward: -113.402, mean reward: -1.219 [-100.000, 19.950], mean action: 1.462 [0.000, 3.000],  loss: 13.520036, mse: 5260.700282, mean_q: 66.257483, mean_eps: 0.909166
  15261/150000: episode: 167, duration: 0.531s, episode steps:  75, steps per second: 141, episode reward: -136.455, mean reward: -1.819 [-100.000,  8.127], mean action: 1.480 [0.000, 3.000],  loss: 13.045380, mse: 5302.812910, mean_q: 65.769026, mean_eps: 0.908662
  15389/150000: episode: 168, duration: 0.885s, episode steps: 128, steps per second: 145, episode reward: -91.360, mean reward: -0.714 [-100.000,  9.181], mean action: 1.422 [0.000, 3.000],  loss: 13.070063, mse: 5407.314661, mean_q: 66.149917, mean_eps: 0.908053
  15555/150000: episode: 169, duration: 1.151s, episode steps: 166, steps per second: 144, episode reward:  5.172, mean reward:  0.031 [-100.000, 75.369], mean action: 1.488 [0.000, 3.000],  loss: 10.744638, mse: 5508.002824, mean_q: 66.827899, mean_eps: 0.907171
  15644/150000: episode: 170, duration: 0.640s, episode steps:  89, steps per second: 139, episode reward: -69.521, mean reward: -0.781 [-100.000,  7.109], mean action: 1.596 [0.000, 3.000],  loss: 15.300280, mse: 5503.064692, mean_q: 67.256819, mean_eps: 0.906406
  15735/150000: episode: 171, duration: 0.666s, episode steps:  91, steps per second: 137, episode reward: -127.954, mean reward: -1.406 [-100.000,  7.117], mean action: 1.505 [0.000, 3.000],  loss: 12.022386, mse: 5499.472409, mean_q: 66.205341, mean_eps: 0.905866
  15806/150000: episode: 172, duration: 0.485s, episode steps:  71, steps per second: 147, episode reward: -142.993, mean reward: -2.014 [-100.000, 17.653], mean action: 1.380 [0.000, 3.000],  loss: 14.110308, mse: 5523.178625, mean_q: 65.745365, mean_eps: 0.905380
  15878/150000: episode: 173, duration: 0.519s, episode steps:  72, steps per second: 139, episode reward: -70.380, mean reward: -0.977 [-100.000,  6.342], mean action: 1.639 [0.000, 3.000],  loss: 14.316026, mse: 5368.975562, mean_q: 63.937960, mean_eps: 0.904951
  15984/150000: episode: 174, duration: 0.719s, episode steps: 106, steps per second: 147, episode reward: -141.197, mean reward: -1.332 [-100.000, 16.655], mean action: 1.425 [0.000, 3.000],  loss: 17.259433, mse: 5520.544507, mean_q: 65.525570, mean_eps: 0.904417
  16047/150000: episode: 175, duration: 0.467s, episode steps:  63, steps per second: 135, episode reward: -69.129, mean reward: -1.097 [-100.000, 17.621], mean action: 1.333 [0.000, 3.000],  loss: 11.530186, mse: 5373.092277, mean_q: 64.273175, mean_eps: 0.903910
  16122/150000: episode: 176, duration: 0.564s, episode steps:  75, steps per second: 133, episode reward: -63.422, mean reward: -0.846 [-100.000, 16.897], mean action: 1.720 [0.000, 3.000],  loss: 15.345586, mse: 5562.141113, mean_q: 66.713089, mean_eps: 0.903496
  16197/150000: episode: 177, duration: 0.553s, episode steps:  75, steps per second: 136, episode reward: -70.014, mean reward: -0.934 [-100.000, 10.156], mean action: 1.493 [0.000, 3.000],  loss: 10.564187, mse: 5622.987917, mean_q: 66.167535, mean_eps: 0.903046
  16323/150000: episode: 178, duration: 0.960s, episode steps: 126, steps per second: 131, episode reward: -99.576, mean reward: -0.790 [-100.000, 21.967], mean action: 1.532 [0.000, 3.000],  loss: 15.796783, mse: 5504.607806, mean_q: 65.246262, mean_eps: 0.902443
  16447/150000: episode: 179, duration: 1.117s, episode steps: 124, steps per second: 111, episode reward: -110.198, mean reward: -0.889 [-100.000, 15.623], mean action: 1.605 [0.000, 3.000],  loss: 13.087953, mse: 5688.281476, mean_q: 65.694221, mean_eps: 0.901693
  16520/150000: episode: 180, duration: 0.544s, episode steps:  73, steps per second: 134, episode reward: -65.312, mean reward: -0.895 [-100.000, 10.984], mean action: 1.630 [0.000, 3.000],  loss: 13.215432, mse: 5731.327148, mean_q: 66.254047, mean_eps: 0.901102
  16581/150000: episode: 181, duration: 0.465s, episode steps:  61, steps per second: 131, episode reward: -30.837, mean reward: -0.506 [-100.000, 83.289], mean action: 1.689 [0.000, 3.000],  loss: 5.320504, mse: 5665.092429, mean_q: 67.354812, mean_eps: 0.900700
  16705/150000: episode: 182, duration: 0.897s, episode steps: 124, steps per second: 138, episode reward: -69.770, mean reward: -0.563 [-100.000, 12.108], mean action: 1.387 [0.000, 3.000],  loss: 16.383077, mse: 5821.345321, mean_q: 67.188060, mean_eps: 0.900145
  16774/150000: episode: 183, duration: 0.488s, episode steps:  69, steps per second: 141, episode reward: -107.154, mean reward: -1.553 [-100.000,  3.052], mean action: 1.406 [0.000, 3.000],  loss: 18.584981, mse: 6084.690274, mean_q: 70.142083, mean_eps: 0.899566
  16892/150000: episode: 184, duration: 0.830s, episode steps: 118, steps per second: 142, episode reward: -107.224, mean reward: -0.909 [-100.000,  7.164], mean action: 1.449 [0.000, 3.000],  loss: 13.012448, mse: 5930.085478, mean_q: 67.795193, mean_eps: 0.899005
  16962/150000: episode: 185, duration: 0.524s, episode steps:  70, steps per second: 134, episode reward: -116.342, mean reward: -1.662 [-100.000,  7.391], mean action: 1.143 [0.000, 3.000],  loss: 25.112580, mse: 6184.558189, mean_q: 69.601950, mean_eps: 0.898441
  17056/150000: episode: 186, duration: 0.719s, episode steps:  94, steps per second: 131, episode reward: -90.916, mean reward: -0.967 [-100.000, 13.087], mean action: 1.532 [0.000, 3.000],  loss: 8.572363, mse: 6279.608876, mean_q: 71.392640, mean_eps: 0.897949
  17130/150000: episode: 187, duration: 0.519s, episode steps:  74, steps per second: 143, episode reward: -60.493, mean reward: -0.817 [-100.000,  6.441], mean action: 1.297 [0.000, 3.000],  loss: 15.236877, mse: 6264.007153, mean_q: 70.913472, mean_eps: 0.897445
  17192/150000: episode: 188, duration: 0.425s, episode steps:  62, steps per second: 146, episode reward: -59.676, mean reward: -0.963 [-100.000, 20.113], mean action: 1.597 [0.000, 3.000],  loss: 14.622349, mse: 6357.106115, mean_q: 71.481323, mean_eps: 0.897037
  17307/150000: episode: 189, duration: 0.805s, episode steps: 115, steps per second: 143, episode reward: -112.454, mean reward: -0.978 [-100.000, 10.416], mean action: 1.626 [0.000, 3.000],  loss: 21.937715, mse: 6306.069090, mean_q: 69.874488, mean_eps: 0.896506
  17402/150000: episode: 190, duration: 0.695s, episode steps:  95, steps per second: 137, episode reward: -82.564, mean reward: -0.869 [-100.000,  6.068], mean action: 1.495 [0.000, 3.000],  loss: 13.052866, mse: 6480.201938, mean_q: 71.509025, mean_eps: 0.895876
  17488/150000: episode: 191, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -107.494, mean reward: -1.250 [-100.000, 17.157], mean action: 1.512 [0.000, 3.000],  loss: 13.996176, mse: 6403.803118, mean_q: 71.207159, mean_eps: 0.895333
  17589/150000: episode: 192, duration: 0.709s, episode steps: 101, steps per second: 142, episode reward: -16.471, mean reward: -0.163 [-100.000, 112.649], mean action: 1.584 [0.000, 3.000],  loss: 22.066409, mse: 6533.497462, mean_q: 70.962305, mean_eps: 0.894772
  17688/150000: episode: 193, duration: 0.767s, episode steps:  99, steps per second: 129, episode reward: -315.826, mean reward: -3.190 [-100.000, 98.200], mean action: 1.586 [0.000, 3.000],  loss: 15.870460, mse: 6533.426200, mean_q: 71.378789, mean_eps: 0.894172
  17776/150000: episode: 194, duration: 0.992s, episode steps:  88, steps per second:  89, episode reward: -135.361, mean reward: -1.538 [-100.000,  9.557], mean action: 1.580 [0.000, 3.000],  loss: 13.821885, mse: 6590.935896, mean_q: 71.231984, mean_eps: 0.893611
  17841/150000: episode: 195, duration: 0.640s, episode steps:  65, steps per second: 102, episode reward: -97.712, mean reward: -1.503 [-100.000, 10.226], mean action: 1.477 [0.000, 3.000],  loss: 12.698019, mse: 6353.854703, mean_q: 68.661262, mean_eps: 0.893152
  17914/150000: episode: 196, duration: 0.589s, episode steps:  73, steps per second: 124, episode reward: -80.014, mean reward: -1.096 [-100.000,  7.089], mean action: 1.603 [0.000, 3.000],  loss: 14.426756, mse: 6635.921186, mean_q: 71.661594, mean_eps: 0.892738
  17996/150000: episode: 197, duration: 0.777s, episode steps:  82, steps per second: 105, episode reward: -106.518, mean reward: -1.299 [-100.000,  8.394], mean action: 1.463 [0.000, 3.000],  loss: 10.349752, mse: 6707.053532, mean_q: 71.171940, mean_eps: 0.892273
  18080/150000: episode: 198, duration: 1.048s, episode steps:  84, steps per second:  80, episode reward: -74.992, mean reward: -0.893 [-100.000,  9.543], mean action: 1.702 [0.000, 3.000],  loss: 12.664257, mse: 6362.588385, mean_q: 69.857822, mean_eps: 0.891775
  18171/150000: episode: 199, duration: 1.106s, episode steps:  91, steps per second:  82, episode reward: -133.766, mean reward: -1.470 [-100.000, 22.987], mean action: 1.440 [0.000, 3.000],  loss: 17.615693, mse: 6752.402483, mean_q: 72.520723, mean_eps: 0.891250
  18280/150000: episode: 200, duration: 0.979s, episode steps: 109, steps per second: 111, episode reward: -121.972, mean reward: -1.119 [-100.000, 14.497], mean action: 1.670 [0.000, 3.000],  loss: 20.171069, mse: 6745.719709, mean_q: 73.628461, mean_eps: 0.890650
  18370/150000: episode: 201, duration: 0.723s, episode steps:  90, steps per second: 124, episode reward: -80.928, mean reward: -0.899 [-100.000,  7.152], mean action: 1.467 [0.000, 3.000],  loss: 22.510517, mse: 6832.310986, mean_q: 73.766830, mean_eps: 0.890053
  18440/150000: episode: 202, duration: 0.534s, episode steps:  70, steps per second: 131, episode reward: -151.615, mean reward: -2.166 [-100.000,  6.514], mean action: 1.457 [0.000, 3.000],  loss: 20.124423, mse: 6796.754548, mean_q: 72.558062, mean_eps: 0.889573
  18546/150000: episode: 203, duration: 0.823s, episode steps: 106, steps per second: 129, episode reward: -83.920, mean reward: -0.792 [-100.000, 47.109], mean action: 1.660 [0.000, 3.000],  loss: 14.939389, mse: 6861.749926, mean_q: 73.363800, mean_eps: 0.889045
  18645/150000: episode: 204, duration: 0.811s, episode steps:  99, steps per second: 122, episode reward: -178.005, mean reward: -1.798 [-100.000,  7.537], mean action: 1.475 [0.000, 3.000],  loss: 15.237254, mse: 6995.867464, mean_q: 74.440280, mean_eps: 0.888430
  18736/150000: episode: 205, duration: 0.653s, episode steps:  91, steps per second: 139, episode reward: -102.273, mean reward: -1.124 [-100.000,  6.959], mean action: 1.637 [0.000, 3.000],  loss: 17.429215, mse: 7043.843610, mean_q: 74.994522, mean_eps: 0.887860
  18819/150000: episode: 206, duration: 0.596s, episode steps:  83, steps per second: 139, episode reward: -99.252, mean reward: -1.196 [-100.000, 14.805], mean action: 1.578 [0.000, 3.000],  loss: 18.518641, mse: 7088.566083, mean_q: 74.162316, mean_eps: 0.887338
  18959/150000: episode: 207, duration: 1.041s, episode steps: 140, steps per second: 134, episode reward: -99.602, mean reward: -0.711 [-100.000,  8.827], mean action: 1.450 [0.000, 3.000],  loss: 28.075967, mse: 7103.869702, mean_q: 74.672860, mean_eps: 0.886669
  19028/150000: episode: 208, duration: 0.534s, episode steps:  69, steps per second: 129, episode reward: -132.862, mean reward: -1.926 [-100.000, 21.596], mean action: 1.449 [0.000, 3.000],  loss: 20.113361, mse: 7013.688929, mean_q: 73.452086, mean_eps: 0.886042
  19133/150000: episode: 209, duration: 1.099s, episode steps: 105, steps per second:  96, episode reward: -74.750, mean reward: -0.712 [-100.000, 11.822], mean action: 1.533 [0.000, 3.000],  loss: 16.611429, mse: 6982.997800, mean_q: 73.289957, mean_eps: 0.885520
  19243/150000: episode: 210, duration: 0.940s, episode steps: 110, steps per second: 117, episode reward: -120.170, mean reward: -1.092 [-100.000, 13.515], mean action: 1.355 [0.000, 3.000],  loss: 15.838544, mse: 7056.992032, mean_q: 73.127231, mean_eps: 0.884875
  19329/150000: episode: 211, duration: 0.646s, episode steps:  86, steps per second: 133, episode reward:  0.403, mean reward:  0.005 [-100.000, 73.637], mean action: 1.674 [0.000, 3.000],  loss: 19.690545, mse: 7072.189209, mean_q: 74.866659, mean_eps: 0.884287
  19435/150000: episode: 212, duration: 0.752s, episode steps: 106, steps per second: 141, episode reward: -79.082, mean reward: -0.746 [-100.000, 11.991], mean action: 1.528 [0.000, 3.000],  loss: 26.684910, mse: 7200.193806, mean_q: 74.895566, mean_eps: 0.883711
  19543/150000: episode: 213, duration: 0.793s, episode steps: 108, steps per second: 136, episode reward: -120.100, mean reward: -1.112 [-100.000, 11.735], mean action: 1.426 [0.000, 3.000],  loss: 12.860561, mse: 7022.914691, mean_q: 73.890483, mean_eps: 0.883069
  19652/150000: episode: 214, duration: 0.991s, episode steps: 109, steps per second: 110, episode reward: -152.760, mean reward: -1.401 [-100.000, 64.547], mean action: 1.569 [0.000, 3.000],  loss: 15.491349, mse: 6912.138175, mean_q: 73.000485, mean_eps: 0.882418
  19737/150000: episode: 215, duration: 0.867s, episode steps:  85, steps per second:  98, episode reward: -55.338, mean reward: -0.651 [-100.000, 17.616], mean action: 1.459 [0.000, 3.000],  loss: 17.746878, mse: 7225.938367, mean_q: 75.724679, mean_eps: 0.881836
  19817/150000: episode: 216, duration: 0.814s, episode steps:  80, steps per second:  98, episode reward: -115.011, mean reward: -1.438 [-100.000,  7.975], mean action: 1.675 [0.000, 3.000],  loss: 14.229956, mse: 6941.817078, mean_q: 73.274767, mean_eps: 0.881341
  19936/150000: episode: 217, duration: 0.943s, episode steps: 119, steps per second: 126, episode reward: -114.821, mean reward: -0.965 [-100.000,  8.786], mean action: 1.445 [0.000, 3.000],  loss: 14.705827, mse: 7086.937668, mean_q: 73.719816, mean_eps: 0.880744
  20004/150000: episode: 218, duration: 0.520s, episode steps:  68, steps per second: 131, episode reward: -79.888, mean reward: -1.175 [-100.000,  8.369], mean action: 1.618 [0.000, 3.000],  loss: 13.361195, mse: 6930.403378, mean_q: 73.452529, mean_eps: 0.880183
  20118/150000: episode: 219, duration: 0.965s, episode steps: 114, steps per second: 118, episode reward: -144.600, mean reward: -1.268 [-100.000,  5.132], mean action: 1.333 [0.000, 3.000],  loss: 13.117667, mse: 6760.313753, mean_q: 71.649211, mean_eps: 0.879637
  21118/150000: episode: 220, duration: 8.119s, episode steps: 1000, steps per second: 123, episode reward: 104.432, mean reward:  0.104 [-22.460, 93.317], mean action: 1.582 [0.000, 3.000],  loss: 16.183108, mse: 6794.290860, mean_q: 71.739078, mean_eps: 0.876295
  21209/150000: episode: 221, duration: 0.714s, episode steps:  91, steps per second: 128, episode reward: -284.812, mean reward: -3.130 [-100.000,  0.612], mean action: 1.451 [0.000, 3.000],  loss: 16.172236, mse: 6609.789964, mean_q: 70.727196, mean_eps: 0.873022
  21323/150000: episode: 222, duration: 0.792s, episode steps: 114, steps per second: 144, episode reward: -159.798, mean reward: -1.402 [-100.000,  5.276], mean action: 1.333 [0.000, 3.000],  loss: 15.330968, mse: 6847.421352, mean_q: 71.804387, mean_eps: 0.872407
  21443/150000: episode: 223, duration: 0.867s, episode steps: 120, steps per second: 138, episode reward: -99.146, mean reward: -0.826 [-100.000, 11.273], mean action: 1.458 [0.000, 3.000],  loss: 17.768476, mse: 6766.270284, mean_q: 71.640839, mean_eps: 0.871705
  21575/150000: episode: 224, duration: 1.023s, episode steps: 132, steps per second: 129, episode reward: -358.261, mean reward: -2.714 [-100.000, 110.953], mean action: 1.409 [0.000, 3.000],  loss: 25.304753, mse: 7127.889016, mean_q: 72.995579, mean_eps: 0.870949
  21647/150000: episode: 225, duration: 0.491s, episode steps:  72, steps per second: 146, episode reward: -113.235, mean reward: -1.573 [-100.000, 14.933], mean action: 1.444 [0.000, 3.000],  loss: 19.605966, mse: 7345.422906, mean_q: 74.633201, mean_eps: 0.870337
  21759/150000: episode: 226, duration: 0.875s, episode steps: 112, steps per second: 128, episode reward: -55.762, mean reward: -0.498 [-100.000, 11.489], mean action: 1.598 [0.000, 3.000],  loss: 21.425809, mse: 7478.539437, mean_q: 75.369602, mean_eps: 0.869785
  21838/150000: episode: 227, duration: 0.614s, episode steps:  79, steps per second: 129, episode reward: -100.600, mean reward: -1.273 [-100.000,  9.648], mean action: 1.633 [0.000, 3.000],  loss: 16.501289, mse: 7258.469288, mean_q: 74.614273, mean_eps: 0.869212
  21903/150000: episode: 228, duration: 0.450s, episode steps:  65, steps per second: 145, episode reward: -66.075, mean reward: -1.017 [-100.000,  6.542], mean action: 1.431 [0.000, 3.000],  loss: 14.653117, mse: 7063.753012, mean_q: 72.376182, mean_eps: 0.868780
  21992/150000: episode: 229, duration: 0.595s, episode steps:  89, steps per second: 150, episode reward: -90.468, mean reward: -1.016 [-100.000,  7.047], mean action: 1.697 [0.000, 3.000],  loss: 17.083326, mse: 7166.742369, mean_q: 74.218924, mean_eps: 0.868318
  22055/150000: episode: 230, duration: 0.463s, episode steps:  63, steps per second: 136, episode reward: -90.149, mean reward: -1.431 [-100.000, 15.525], mean action: 1.540 [0.000, 3.000],  loss: 12.455735, mse: 7266.210232, mean_q: 73.426532, mean_eps: 0.867862
  22152/150000: episode: 231, duration: 0.672s, episode steps:  97, steps per second: 144, episode reward: -102.004, mean reward: -1.052 [-100.000, 11.356], mean action: 1.546 [0.000, 3.000],  loss: 13.598584, mse: 7371.643006, mean_q: 73.801985, mean_eps: 0.867382
  22253/150000: episode: 232, duration: 0.680s, episode steps: 101, steps per second: 148, episode reward: -77.176, mean reward: -0.764 [-100.000, 16.698], mean action: 1.564 [0.000, 3.000],  loss: 18.358885, mse: 7324.896480, mean_q: 74.788251, mean_eps: 0.866788
  22341/150000: episode: 233, duration: 0.600s, episode steps:  88, steps per second: 147, episode reward: -88.307, mean reward: -1.003 [-100.000, 11.647], mean action: 1.636 [0.000, 3.000],  loss: 12.849201, mse: 7404.499018, mean_q: 75.460956, mean_eps: 0.866221
  22422/150000: episode: 234, duration: 0.600s, episode steps:  81, steps per second: 135, episode reward: -102.688, mean reward: -1.268 [-100.000, 15.902], mean action: 1.383 [0.000, 3.000],  loss: 16.672602, mse: 7311.708689, mean_q: 75.231019, mean_eps: 0.865714
  22543/150000: episode: 235, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -197.445, mean reward: -1.632 [-100.000,  8.186], mean action: 1.364 [0.000, 3.000],  loss: 11.982465, mse: 7488.419050, mean_q: 76.316263, mean_eps: 0.865108
  22610/150000: episode: 236, duration: 0.525s, episode steps:  67, steps per second: 128, episode reward: -86.512, mean reward: -1.291 [-100.000, 18.762], mean action: 1.493 [0.000, 3.000],  loss: 14.302677, mse: 7210.633957, mean_q: 74.312158, mean_eps: 0.864544
  22720/150000: episode: 237, duration: 0.820s, episode steps: 110, steps per second: 134, episode reward: -66.279, mean reward: -0.603 [-100.000, 17.668], mean action: 1.427 [0.000, 3.000],  loss: 12.624960, mse: 7319.938077, mean_q: 75.387767, mean_eps: 0.864013
  22823/150000: episode: 238, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -150.073, mean reward: -1.457 [-100.000,  7.155], mean action: 1.544 [0.000, 3.000],  loss: 12.183692, mse: 7416.096684, mean_q: 76.356131, mean_eps: 0.863374
  22927/150000: episode: 239, duration: 0.691s, episode steps: 104, steps per second: 150, episode reward: -60.409, mean reward: -0.581 [-100.000, 16.204], mean action: 1.423 [0.000, 3.000],  loss: 30.298418, mse: 7541.668922, mean_q: 76.228201, mean_eps: 0.862753
  23044/150000: episode: 240, duration: 0.870s, episode steps: 117, steps per second: 135, episode reward: -130.181, mean reward: -1.113 [-100.000,  6.537], mean action: 1.436 [0.000, 3.000],  loss: 18.089223, mse: 7488.991082, mean_q: 76.058531, mean_eps: 0.862090
  23151/150000: episode: 241, duration: 0.816s, episode steps: 107, steps per second: 131, episode reward: -119.596, mean reward: -1.118 [-100.000, 10.356], mean action: 1.533 [0.000, 3.000],  loss: 12.179901, mse: 7492.640369, mean_q: 76.159288, mean_eps: 0.861418
  23233/150000: episode: 242, duration: 0.593s, episode steps:  82, steps per second: 138, episode reward: -84.500, mean reward: -1.030 [-100.000,  6.505], mean action: 1.378 [0.000, 3.000],  loss: 13.135556, mse: 7847.566162, mean_q: 78.658442, mean_eps: 0.860851
  23341/150000: episode: 243, duration: 0.786s, episode steps: 108, steps per second: 137, episode reward: -140.245, mean reward: -1.299 [-100.000,  5.581], mean action: 1.713 [0.000, 3.000],  loss: 13.372044, mse: 7839.084210, mean_q: 78.927196, mean_eps: 0.860281
  23433/150000: episode: 244, duration: 0.622s, episode steps:  92, steps per second: 148, episode reward: -281.034, mean reward: -3.055 [-100.000,  4.497], mean action: 1.663 [0.000, 3.000],  loss: 17.251223, mse: 7471.605331, mean_q: 77.256636, mean_eps: 0.859681
  23518/150000: episode: 245, duration: 0.580s, episode steps:  85, steps per second: 147, episode reward: -163.045, mean reward: -1.918 [-100.000,  7.864], mean action: 1.647 [0.000, 3.000],  loss: 13.355576, mse: 7721.569204, mean_q: 77.545865, mean_eps: 0.859150
  23602/150000: episode: 246, duration: 0.604s, episode steps:  84, steps per second: 139, episode reward: -92.140, mean reward: -1.097 [-100.000,  8.078], mean action: 1.750 [0.000, 3.000],  loss: 24.843478, mse: 7456.983259, mean_q: 76.652088, mean_eps: 0.858643
  23671/150000: episode: 247, duration: 0.470s, episode steps:  69, steps per second: 147, episode reward: -220.507, mean reward: -3.196 [-100.000,  3.029], mean action: 1.580 [0.000, 3.000],  loss: 13.032390, mse: 7769.090332, mean_q: 78.741209, mean_eps: 0.858184
  23788/150000: episode: 248, duration: 0.779s, episode steps: 117, steps per second: 150, episode reward: -85.486, mean reward: -0.731 [-100.000, 10.110], mean action: 1.368 [0.000, 3.000],  loss: 23.762382, mse: 7658.688301, mean_q: 76.620486, mean_eps: 0.857626
  23891/150000: episode: 249, duration: 0.718s, episode steps: 103, steps per second: 143, episode reward: -226.403, mean reward: -2.198 [-100.000, 32.505], mean action: 1.757 [0.000, 3.000],  loss: 19.658629, mse: 7504.145385, mean_q: 74.937418, mean_eps: 0.856966
  23980/150000: episode: 250, duration: 0.645s, episode steps:  89, steps per second: 138, episode reward: -43.744, mean reward: -0.492 [-100.000, 17.208], mean action: 1.663 [0.000, 3.000],  loss: 16.756922, mse: 7678.414874, mean_q: 77.070551, mean_eps: 0.856390
  24061/150000: episode: 251, duration: 0.563s, episode steps:  81, steps per second: 144, episode reward: -103.054, mean reward: -1.272 [-100.000,  7.270], mean action: 1.580 [0.000, 3.000],  loss: 17.454685, mse: 7540.448996, mean_q: 75.200239, mean_eps: 0.855880
  24146/150000: episode: 252, duration: 0.571s, episode steps:  85, steps per second: 149, episode reward: -103.244, mean reward: -1.215 [-100.000,  8.363], mean action: 1.741 [0.000, 3.000],  loss: 20.205012, mse: 7627.163948, mean_q: 76.144064, mean_eps: 0.855382
  24213/150000: episode: 253, duration: 0.480s, episode steps:  67, steps per second: 140, episode reward: -197.693, mean reward: -2.951 [-100.000,  7.329], mean action: 1.687 [0.000, 3.000],  loss: 38.744324, mse: 7557.228231, mean_q: 75.878887, mean_eps: 0.854926
  24288/150000: episode: 254, duration: 0.513s, episode steps:  75, steps per second: 146, episode reward: -93.002, mean reward: -1.240 [-100.000, 19.336], mean action: 1.333 [0.000, 3.000],  loss: 12.311604, mse: 7281.125365, mean_q: 74.287142, mean_eps: 0.854500
  24419/150000: episode: 255, duration: 0.899s, episode steps: 131, steps per second: 146, episode reward: -80.330, mean reward: -0.613 [-100.000, 50.207], mean action: 1.511 [0.000, 3.000],  loss: 20.317700, mse: 7548.978117, mean_q: 75.382632, mean_eps: 0.853882
  24553/150000: episode: 256, duration: 0.937s, episode steps: 134, steps per second: 143, episode reward: -51.310, mean reward: -0.383 [-100.000, 12.727], mean action: 1.493 [0.000, 3.000],  loss: 21.615009, mse: 7487.290833, mean_q: 73.822119, mean_eps: 0.853087
  24662/150000: episode: 257, duration: 0.754s, episode steps: 109, steps per second: 145, episode reward: -130.688, mean reward: -1.199 [-100.000,  5.490], mean action: 1.697 [0.000, 3.000],  loss: 15.712968, mse: 7758.968723, mean_q: 76.080177, mean_eps: 0.852358
  24760/150000: episode: 258, duration: 0.691s, episode steps:  98, steps per second: 142, episode reward: -133.408, mean reward: -1.361 [-100.000, 13.582], mean action: 1.429 [0.000, 3.000],  loss: 10.818931, mse: 7695.502192, mean_q: 75.669340, mean_eps: 0.851737
  24872/150000: episode: 259, duration: 0.812s, episode steps: 112, steps per second: 138, episode reward: -75.929, mean reward: -0.678 [-100.000, 17.461], mean action: 1.598 [0.000, 3.000],  loss: 13.433051, mse: 7952.175934, mean_q: 76.144693, mean_eps: 0.851107
  24940/150000: episode: 260, duration: 0.461s, episode steps:  68, steps per second: 147, episode reward: -68.016, mean reward: -1.000 [-100.000, 17.781], mean action: 1.721 [0.000, 3.000],  loss: 14.736403, mse: 7726.538553, mean_q: 75.715398, mean_eps: 0.850567
  25042/150000: episode: 261, duration: 0.682s, episode steps: 102, steps per second: 149, episode reward: -211.243, mean reward: -2.071 [-100.000, 39.421], mean action: 1.480 [0.000, 3.000],  loss: 10.838918, mse: 7627.196701, mean_q: 75.445817, mean_eps: 0.850057
  25164/150000: episode: 262, duration: 0.846s, episode steps: 122, steps per second: 144, episode reward: -11.130, mean reward: -0.091 [-100.000, 51.982], mean action: 1.549 [0.000, 3.000],  loss: 24.693111, mse: 7723.381424, mean_q: 76.172663, mean_eps: 0.849385
  25291/150000: episode: 263, duration: 0.843s, episode steps: 127, steps per second: 151, episode reward: -166.124, mean reward: -1.308 [-100.000,  6.063], mean action: 1.409 [0.000, 3.000],  loss: 16.361617, mse: 7829.713252, mean_q: 75.349507, mean_eps: 0.848638
  25396/150000: episode: 264, duration: 0.746s, episode steps: 105, steps per second: 141, episode reward: -53.581, mean reward: -0.510 [-100.000, 15.193], mean action: 1.610 [0.000, 3.000],  loss: 20.732036, mse: 7822.393666, mean_q: 74.690512, mean_eps: 0.847942
  25506/150000: episode: 265, duration: 0.814s, episode steps: 110, steps per second: 135, episode reward: -99.148, mean reward: -0.901 [-100.000,  8.066], mean action: 1.573 [0.000, 3.000],  loss: 11.070963, mse: 7883.162132, mean_q: 75.653093, mean_eps: 0.847297
  25584/150000: episode: 266, duration: 0.525s, episode steps:  78, steps per second: 149, episode reward: -57.546, mean reward: -0.738 [-100.000, 21.851], mean action: 1.692 [0.000, 3.000],  loss: 17.069737, mse: 8015.592936, mean_q: 77.016833, mean_eps: 0.846733
  25660/150000: episode: 267, duration: 0.515s, episode steps:  76, steps per second: 148, episode reward: -79.756, mean reward: -1.049 [-100.000,  6.121], mean action: 1.684 [0.000, 3.000],  loss: 16.246839, mse: 8140.271157, mean_q: 77.354852, mean_eps: 0.846271
  25788/150000: episode: 268, duration: 0.980s, episode steps: 128, steps per second: 131, episode reward: -74.775, mean reward: -0.584 [-100.000, 10.839], mean action: 1.539 [0.000, 3.000],  loss: 18.403961, mse: 8058.843685, mean_q: 76.274266, mean_eps: 0.845659
  25886/150000: episode: 269, duration: 0.729s, episode steps:  98, steps per second: 134, episode reward: -153.200, mean reward: -1.563 [-100.000, 11.457], mean action: 1.541 [0.000, 3.000],  loss: 28.712554, mse: 8193.382354, mean_q: 78.498952, mean_eps: 0.844981
  25987/150000: episode: 270, duration: 0.740s, episode steps: 101, steps per second: 136, episode reward: -73.313, mean reward: -0.726 [-100.000, 14.103], mean action: 1.535 [0.000, 3.000],  loss: 24.426811, mse: 8576.650835, mean_q: 80.415594, mean_eps: 0.844384
  26082/150000: episode: 271, duration: 0.796s, episode steps:  95, steps per second: 119, episode reward: -128.336, mean reward: -1.351 [-100.000, 15.075], mean action: 1.642 [0.000, 3.000],  loss: 25.009958, mse: 8742.133990, mean_q: 81.600525, mean_eps: 0.843796
  26182/150000: episode: 272, duration: 0.787s, episode steps: 100, steps per second: 127, episode reward: -115.594, mean reward: -1.156 [-100.000,  4.797], mean action: 1.630 [0.000, 3.000],  loss: 18.639552, mse: 8537.332520, mean_q: 79.931187, mean_eps: 0.843211
  26247/150000: episode: 273, duration: 0.470s, episode steps:  65, steps per second: 138, episode reward: -119.175, mean reward: -1.833 [-100.000,  9.959], mean action: 1.523 [0.000, 3.000],  loss: 15.399955, mse: 8605.105634, mean_q: 80.259826, mean_eps: 0.842716
  26320/150000: episode: 274, duration: 0.546s, episode steps:  73, steps per second: 134, episode reward: -102.176, mean reward: -1.400 [-100.000, 13.124], mean action: 1.521 [0.000, 3.000],  loss: 16.551956, mse: 8832.835697, mean_q: 82.942987, mean_eps: 0.842302
  26407/150000: episode: 275, duration: 0.617s, episode steps:  87, steps per second: 141, episode reward: -51.029, mean reward: -0.587 [-100.000, 61.160], mean action: 1.713 [0.000, 3.000],  loss: 19.459864, mse: 8748.630298, mean_q: 82.178661, mean_eps: 0.841822
  26489/150000: episode: 276, duration: 0.565s, episode steps:  82, steps per second: 145, episode reward: -131.410, mean reward: -1.603 [-100.000,  6.738], mean action: 1.476 [0.000, 3.000],  loss: 17.566980, mse: 8609.606981, mean_q: 81.631722, mean_eps: 0.841315
  26582/150000: episode: 277, duration: 0.795s, episode steps:  93, steps per second: 117, episode reward: -62.544, mean reward: -0.673 [-100.000, 13.373], mean action: 1.441 [0.000, 3.000],  loss: 16.617755, mse: 8343.628539, mean_q: 80.660701, mean_eps: 0.840790
  26669/150000: episode: 278, duration: 0.744s, episode steps:  87, steps per second: 117, episode reward: -114.967, mean reward: -1.321 [-100.000,  8.330], mean action: 1.471 [0.000, 3.000],  loss: 18.413409, mse: 8522.087655, mean_q: 80.105158, mean_eps: 0.840250
  26744/150000: episode: 279, duration: 0.653s, episode steps:  75, steps per second: 115, episode reward: -89.491, mean reward: -1.193 [-100.000,  8.158], mean action: 1.507 [0.000, 3.000],  loss: 9.685466, mse: 8389.915039, mean_q: 80.431511, mean_eps: 0.839764
  26890/150000: episode: 280, duration: 1.266s, episode steps: 146, steps per second: 115, episode reward: -18.361, mean reward: -0.126 [-100.000, 44.662], mean action: 1.596 [0.000, 3.000],  loss: 17.466916, mse: 8516.848736, mean_q: 80.245891, mean_eps: 0.839101
  27020/150000: episode: 281, duration: 1.009s, episode steps: 130, steps per second: 129, episode reward: -68.066, mean reward: -0.524 [-100.000, 10.773], mean action: 1.523 [0.000, 3.000],  loss: 18.288526, mse: 8461.724504, mean_q: 79.449923, mean_eps: 0.838273
  27124/150000: episode: 282, duration: 0.870s, episode steps: 104, steps per second: 120, episode reward: -142.234, mean reward: -1.368 [-100.000,  4.722], mean action: 1.644 [0.000, 3.000],  loss: 26.058714, mse: 8515.719750, mean_q: 80.350485, mean_eps: 0.837571
  27230/150000: episode: 283, duration: 0.831s, episode steps: 106, steps per second: 128, episode reward: -362.594, mean reward: -3.421 [-100.000, 77.254], mean action: 1.425 [0.000, 3.000],  loss: 22.520450, mse: 8664.881933, mean_q: 81.081502, mean_eps: 0.836941
  27312/150000: episode: 284, duration: 0.601s, episode steps:  82, steps per second: 137, episode reward: -33.179, mean reward: -0.405 [-100.000, 10.214], mean action: 1.488 [0.000, 3.000],  loss: 18.146088, mse: 8444.824368, mean_q: 80.901005, mean_eps: 0.836377
  27444/150000: episode: 285, duration: 0.994s, episode steps: 132, steps per second: 133, episode reward: -120.511, mean reward: -0.913 [-100.000, 12.831], mean action: 1.485 [0.000, 3.000],  loss: 20.116352, mse: 8354.028679, mean_q: 80.587012, mean_eps: 0.835735
  27548/150000: episode: 286, duration: 0.751s, episode steps: 104, steps per second: 138, episode reward: -131.346, mean reward: -1.263 [-100.000,  9.432], mean action: 1.394 [0.000, 3.000],  loss: 32.780649, mse: 8539.264799, mean_q: 82.013372, mean_eps: 0.835027
  27673/150000: episode: 287, duration: 0.941s, episode steps: 125, steps per second: 133, episode reward: -90.110, mean reward: -0.721 [-100.000, 10.894], mean action: 1.592 [0.000, 3.000],  loss: 15.550878, mse: 8663.107035, mean_q: 82.653544, mean_eps: 0.834340
  27771/150000: episode: 288, duration: 0.731s, episode steps:  98, steps per second: 134, episode reward: -78.136, mean reward: -0.797 [-100.000,  8.942], mean action: 1.684 [0.000, 3.000],  loss: 20.994404, mse: 8546.143166, mean_q: 80.854992, mean_eps: 0.833671
  27852/150000: episode: 289, duration: 0.577s, episode steps:  81, steps per second: 140, episode reward: -106.235, mean reward: -1.312 [-100.000,  6.970], mean action: 1.605 [0.000, 3.000],  loss: 24.593371, mse: 8563.828252, mean_q: 80.660417, mean_eps: 0.833134
  27933/150000: episode: 290, duration: 0.568s, episode steps:  81, steps per second: 142, episode reward: -86.174, mean reward: -1.064 [-100.000,  8.651], mean action: 1.605 [0.000, 3.000],  loss: 14.939901, mse: 8664.498752, mean_q: 80.257795, mean_eps: 0.832648
  28042/150000: episode: 291, duration: 0.853s, episode steps: 109, steps per second: 128, episode reward: -128.724, mean reward: -1.181 [-100.000, 10.543], mean action: 1.523 [0.000, 3.000],  loss: 29.555307, mse: 8908.787840, mean_q: 83.237196, mean_eps: 0.832078
  28118/150000: episode: 292, duration: 0.537s, episode steps:  76, steps per second: 142, episode reward: -84.684, mean reward: -1.114 [-100.000, 18.040], mean action: 1.579 [0.000, 3.000],  loss: 15.420753, mse: 8855.453645, mean_q: 81.884177, mean_eps: 0.831523
  28213/150000: episode: 293, duration: 0.653s, episode steps:  95, steps per second: 145, episode reward: -17.492, mean reward: -0.184 [-100.000, 17.945], mean action: 1.716 [0.000, 3.000],  loss: 22.516382, mse: 8756.849481, mean_q: 81.681659, mean_eps: 0.831010
  28330/150000: episode: 294, duration: 0.829s, episode steps: 117, steps per second: 141, episode reward: -75.562, mean reward: -0.646 [-100.000,  5.313], mean action: 1.513 [0.000, 3.000],  loss: 17.037525, mse: 8800.839627, mean_q: 81.889278, mean_eps: 0.830374
  28399/150000: episode: 295, duration: 0.507s, episode steps:  69, steps per second: 136, episode reward: -41.350, mean reward: -0.599 [-100.000, 87.483], mean action: 1.768 [0.000, 3.000],  loss: 18.144762, mse: 8827.094068, mean_q: 82.694281, mean_eps: 0.829816
  28482/150000: episode: 296, duration: 0.606s, episode steps:  83, steps per second: 137, episode reward: -109.338, mean reward: -1.317 [-100.000,  5.059], mean action: 1.530 [0.000, 3.000],  loss: 17.781810, mse: 8892.094132, mean_q: 83.324403, mean_eps: 0.829360
  28576/150000: episode: 297, duration: 0.698s, episode steps:  94, steps per second: 135, episode reward: -83.428, mean reward: -0.888 [-100.000, 12.097], mean action: 1.468 [0.000, 3.000],  loss: 29.221814, mse: 9056.633306, mean_q: 85.566155, mean_eps: 0.828829
  28698/150000: episode: 298, duration: 0.900s, episode steps: 122, steps per second: 136, episode reward: -128.652, mean reward: -1.055 [-100.000,  9.493], mean action: 1.680 [0.000, 3.000],  loss: 27.383015, mse: 8904.058826, mean_q: 84.759559, mean_eps: 0.828181
  28801/150000: episode: 299, duration: 0.751s, episode steps: 103, steps per second: 137, episode reward: -125.399, mean reward: -1.217 [-100.000, 11.626], mean action: 1.592 [0.000, 3.000],  loss: 28.196496, mse: 9128.197019, mean_q: 87.603205, mean_eps: 0.827506
  28930/150000: episode: 300, duration: 0.933s, episode steps: 129, steps per second: 138, episode reward: -112.581, mean reward: -0.873 [-100.000, 19.835], mean action: 1.504 [0.000, 3.000],  loss: 28.895907, mse: 9128.826009, mean_q: 87.192915, mean_eps: 0.826810
  29019/150000: episode: 301, duration: 0.637s, episode steps:  89, steps per second: 140, episode reward: -79.997, mean reward: -0.899 [-100.000,  7.138], mean action: 1.663 [0.000, 3.000],  loss: 23.148247, mse: 9323.058939, mean_q: 89.260327, mean_eps: 0.826156
  29111/150000: episode: 302, duration: 0.635s, episode steps:  92, steps per second: 145, episode reward: -28.431, mean reward: -0.309 [-100.000, 60.441], mean action: 1.598 [0.000, 3.000],  loss: 19.853145, mse: 9046.973113, mean_q: 87.360054, mean_eps: 0.825613
  29174/150000: episode: 303, duration: 0.462s, episode steps:  63, steps per second: 136, episode reward: -130.236, mean reward: -2.067 [-100.000,  5.224], mean action: 1.714 [0.000, 3.000],  loss: 15.350435, mse: 9568.794643, mean_q: 91.625786, mean_eps: 0.825148
  29286/150000: episode: 304, duration: 0.800s, episode steps: 112, steps per second: 140, episode reward: -91.897, mean reward: -0.821 [-100.000, 12.048], mean action: 1.562 [0.000, 3.000],  loss: 34.824373, mse: 9403.979959, mean_q: 89.117700, mean_eps: 0.824623
  29379/150000: episode: 305, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward: -103.068, mean reward: -1.108 [-100.000,  6.622], mean action: 1.645 [0.000, 3.000],  loss: 18.815758, mse: 9463.264795, mean_q: 90.278249, mean_eps: 0.824008
  29513/150000: episode: 306, duration: 0.944s, episode steps: 134, steps per second: 142, episode reward: -83.637, mean reward: -0.624 [-100.000,  6.900], mean action: 1.485 [0.000, 3.000],  loss: 22.758326, mse: 9264.806593, mean_q: 87.969331, mean_eps: 0.823327
  29625/150000: episode: 307, duration: 0.939s, episode steps: 112, steps per second: 119, episode reward: -100.797, mean reward: -0.900 [-100.000,  6.958], mean action: 1.616 [0.000, 3.000],  loss: 18.347747, mse: 9243.941328, mean_q: 88.841893, mean_eps: 0.822589
  29704/150000: episode: 308, duration: 0.569s, episode steps:  79, steps per second: 139, episode reward: -144.188, mean reward: -1.825 [-100.000, 21.437], mean action: 1.658 [0.000, 3.000],  loss: 25.980584, mse: 9560.338478, mean_q: 89.896004, mean_eps: 0.822016
  29806/150000: episode: 309, duration: 0.733s, episode steps: 102, steps per second: 139, episode reward: -44.912, mean reward: -0.440 [-100.000, 12.540], mean action: 1.559 [0.000, 3.000],  loss: 14.594802, mse: 9563.992365, mean_q: 92.095229, mean_eps: 0.821473
  29901/150000: episode: 310, duration: 0.663s, episode steps:  95, steps per second: 143, episode reward: -83.808, mean reward: -0.882 [-100.000, 16.951], mean action: 1.547 [0.000, 3.000],  loss: 31.480267, mse: 9502.931749, mean_q: 89.797725, mean_eps: 0.820882
  30037/150000: episode: 311, duration: 1.032s, episode steps: 136, steps per second: 132, episode reward: -31.041, mean reward: -0.228 [-100.000, 20.980], mean action: 1.618 [0.000, 3.000],  loss: 18.386232, mse: 9553.999400, mean_q: 91.664874, mean_eps: 0.820189
  30137/150000: episode: 312, duration: 0.796s, episode steps: 100, steps per second: 126, episode reward: -173.896, mean reward: -1.739 [-100.000,  9.510], mean action: 1.630 [0.000, 3.000],  loss: 17.265993, mse: 9594.725742, mean_q: 91.261644, mean_eps: 0.819481
  30226/150000: episode: 313, duration: 0.632s, episode steps:  89, steps per second: 141, episode reward: -120.598, mean reward: -1.355 [-100.000,  8.241], mean action: 1.697 [0.000, 3.000],  loss: 13.623955, mse: 9651.579047, mean_q: 91.944236, mean_eps: 0.818914
  30321/150000: episode: 314, duration: 0.674s, episode steps:  95, steps per second: 141, episode reward: -116.319, mean reward: -1.224 [-100.000,  9.346], mean action: 1.589 [0.000, 3.000],  loss: 17.832729, mse: 9911.082309, mean_q: 93.455761, mean_eps: 0.818362
  30403/150000: episode: 315, duration: 0.651s, episode steps:  82, steps per second: 126, episode reward: -46.994, mean reward: -0.573 [-100.000, 13.522], mean action: 1.537 [0.000, 3.000],  loss: 17.140205, mse: 9979.394186, mean_q: 93.298206, mean_eps: 0.817831
  30517/150000: episode: 316, duration: 0.903s, episode steps: 114, steps per second: 126, episode reward: -108.562, mean reward: -0.952 [-100.000,  7.195], mean action: 1.395 [0.000, 3.000],  loss: 27.886160, mse: 9719.930347, mean_q: 92.321121, mean_eps: 0.817243
  30669/150000: episode: 317, duration: 1.195s, episode steps: 152, steps per second: 127, episode reward: -112.426, mean reward: -0.740 [-100.000,  5.641], mean action: 1.605 [0.000, 3.000],  loss: 27.145535, mse: 10158.966228, mean_q: 94.529805, mean_eps: 0.816445
  30737/150000: episode: 318, duration: 0.487s, episode steps:  68, steps per second: 140, episode reward: -80.716, mean reward: -1.187 [-100.000, 16.370], mean action: 1.647 [0.000, 3.000],  loss: 20.948026, mse: 10455.877082, mean_q: 95.517038, mean_eps: 0.815785
  30812/150000: episode: 319, duration: 0.537s, episode steps:  75, steps per second: 140, episode reward: -60.991, mean reward: -0.813 [-100.000, 28.125], mean action: 1.613 [0.000, 3.000],  loss: 16.279397, mse: 10695.858477, mean_q: 95.875411, mean_eps: 0.815356
  30911/150000: episode: 320, duration: 0.797s, episode steps:  99, steps per second: 124, episode reward: -76.766, mean reward: -0.775 [-100.000, 26.265], mean action: 1.798 [0.000, 3.000],  loss: 19.969988, mse: 10487.768624, mean_q: 96.509589, mean_eps: 0.814834
  30992/150000: episode: 321, duration: 0.679s, episode steps:  81, steps per second: 119, episode reward: -85.475, mean reward: -1.055 [-100.000,  6.718], mean action: 1.481 [0.000, 3.000],  loss: 18.720527, mse: 10521.389606, mean_q: 96.421613, mean_eps: 0.814294
  31106/150000: episode: 322, duration: 0.791s, episode steps: 114, steps per second: 144, episode reward: -116.107, mean reward: -1.018 [-100.000,  7.758], mean action: 1.412 [0.000, 3.000],  loss: 24.247493, mse: 10931.471663, mean_q: 97.401107, mean_eps: 0.813709
  31221/150000: episode: 323, duration: 0.806s, episode steps: 115, steps per second: 143, episode reward: -65.276, mean reward: -0.568 [-100.000, 12.750], mean action: 1.609 [0.000, 3.000],  loss: 17.945287, mse: 10982.783059, mean_q: 97.947226, mean_eps: 0.813022
  31339/150000: episode: 324, duration: 0.849s, episode steps: 118, steps per second: 139, episode reward: -52.408, mean reward: -0.444 [-100.000,  8.046], mean action: 1.602 [0.000, 3.000],  loss: 21.923886, mse: 10996.996851, mean_q: 98.663121, mean_eps: 0.812323
  31419/150000: episode: 325, duration: 0.579s, episode steps:  80, steps per second: 138, episode reward: -55.092, mean reward: -0.689 [-100.000,  9.027], mean action: 1.562 [0.000, 3.000],  loss: 30.307920, mse: 10770.417462, mean_q: 98.047125, mean_eps: 0.811729
  31540/150000: episode: 326, duration: 0.872s, episode steps: 121, steps per second: 139, episode reward: -107.786, mean reward: -0.891 [-100.000, 10.236], mean action: 1.620 [0.000, 3.000],  loss: 21.198429, mse: 10955.700736, mean_q: 97.012874, mean_eps: 0.811126
  31674/150000: episode: 327, duration: 0.952s, episode steps: 134, steps per second: 141, episode reward: -166.687, mean reward: -1.244 [-100.000,  6.318], mean action: 1.612 [0.000, 3.000],  loss: 20.517266, mse: 10968.416336, mean_q: 96.812391, mean_eps: 0.810361
  31785/150000: episode: 328, duration: 0.746s, episode steps: 111, steps per second: 149, episode reward: -107.169, mean reward: -0.965 [-100.000, 14.872], mean action: 1.450 [0.000, 3.000],  loss: 26.967284, mse: 11439.304212, mean_q: 99.063930, mean_eps: 0.809626
  31874/150000: episode: 329, duration: 0.665s, episode steps:  89, steps per second: 134, episode reward: -98.916, mean reward: -1.111 [-100.000, 22.134], mean action: 1.753 [0.000, 3.000],  loss: 15.722458, mse: 11423.976343, mean_q: 98.523132, mean_eps: 0.809026
  31989/150000: episode: 330, duration: 0.788s, episode steps: 115, steps per second: 146, episode reward: -52.047, mean reward: -0.453 [-100.000, 12.920], mean action: 1.704 [0.000, 3.000],  loss: 19.773076, mse: 11657.520313, mean_q: 101.026896, mean_eps: 0.808414
  32073/150000: episode: 331, duration: 0.567s, episode steps:  84, steps per second: 148, episode reward: -72.419, mean reward: -0.862 [-100.000, 12.350], mean action: 1.571 [0.000, 3.000],  loss: 18.030594, mse: 11941.947091, mean_q: 101.788566, mean_eps: 0.807817
  32165/150000: episode: 332, duration: 0.662s, episode steps:  92, steps per second: 139, episode reward: -91.940, mean reward: -0.999 [-100.000, 11.113], mean action: 1.576 [0.000, 3.000],  loss: 29.404150, mse: 11861.049083, mean_q: 101.532610, mean_eps: 0.807289
  32272/150000: episode: 333, duration: 0.729s, episode steps: 107, steps per second: 147, episode reward: -65.634, mean reward: -0.613 [-100.000,  8.530], mean action: 1.636 [0.000, 3.000],  loss: 18.490086, mse: 11975.817629, mean_q: 102.167543, mean_eps: 0.806692
  32348/150000: episode: 334, duration: 0.516s, episode steps:  76, steps per second: 147, episode reward: -153.264, mean reward: -2.017 [-100.000,  4.478], mean action: 1.724 [0.000, 3.000],  loss: 21.706521, mse: 12109.511359, mean_q: 101.342805, mean_eps: 0.806143
  32449/150000: episode: 335, duration: 0.728s, episode steps: 101, steps per second: 139, episode reward: -120.583, mean reward: -1.194 [-100.000,  7.521], mean action: 1.713 [0.000, 3.000],  loss: 20.571294, mse: 12040.697005, mean_q: 100.860569, mean_eps: 0.805612
  32545/150000: episode: 336, duration: 0.654s, episode steps:  96, steps per second: 147, episode reward: -81.959, mean reward: -0.854 [-100.000, 27.940], mean action: 1.469 [0.000, 3.000],  loss: 17.672226, mse: 12130.145081, mean_q: 101.361274, mean_eps: 0.805021
  32616/150000: episode: 337, duration: 0.495s, episode steps:  71, steps per second: 144, episode reward: -144.351, mean reward: -2.033 [-100.000,  6.216], mean action: 1.901 [0.000, 3.000],  loss: 26.908718, mse: 12367.573132, mean_q: 102.938075, mean_eps: 0.804520
  32697/150000: episode: 338, duration: 0.612s, episode steps:  81, steps per second: 132, episode reward: -70.493, mean reward: -0.870 [-100.000, 66.621], mean action: 1.543 [0.000, 3.000],  loss: 16.806806, mse: 12659.296308, mean_q: 105.762043, mean_eps: 0.804064
  32833/150000: episode: 339, duration: 0.958s, episode steps: 136, steps per second: 142, episode reward: -93.454, mean reward: -0.687 [-100.000,  7.190], mean action: 1.581 [0.000, 3.000],  loss: 28.340764, mse: 12609.961153, mean_q: 104.281284, mean_eps: 0.803413
  32931/150000: episode: 340, duration: 0.663s, episode steps:  98, steps per second: 148, episode reward: -90.910, mean reward: -0.928 [-100.000, 12.189], mean action: 1.327 [0.000, 3.000],  loss: 24.779753, mse: 12810.349679, mean_q: 105.263790, mean_eps: 0.802711
  33032/150000: episode: 341, duration: 0.831s, episode steps: 101, steps per second: 122, episode reward: -97.059, mean reward: -0.961 [-100.000, 16.137], mean action: 1.535 [0.000, 3.000],  loss: 35.743362, mse: 12516.872805, mean_q: 104.205588, mean_eps: 0.802114
  33105/150000: episode: 342, duration: 0.594s, episode steps:  73, steps per second: 123, episode reward: -33.579, mean reward: -0.460 [-100.000, 12.174], mean action: 1.671 [0.000, 3.000],  loss: 19.029919, mse: 12954.237505, mean_q: 105.857851, mean_eps: 0.801592
  33166/150000: episode: 343, duration: 0.427s, episode steps:  61, steps per second: 143, episode reward: -79.862, mean reward: -1.309 [-100.000, 12.533], mean action: 1.672 [0.000, 3.000],  loss: 20.122824, mse: 12848.514360, mean_q: 106.128196, mean_eps: 0.801190
  33251/150000: episode: 344, duration: 0.580s, episode steps:  85, steps per second: 146, episode reward: -104.955, mean reward: -1.235 [-100.000,  8.987], mean action: 1.529 [0.000, 3.000],  loss: 20.012200, mse: 12631.475414, mean_q: 105.050029, mean_eps: 0.800752
  33318/150000: episode: 345, duration: 0.469s, episode steps:  67, steps per second: 143, episode reward: -120.126, mean reward: -1.793 [-100.000, 13.115], mean action: 1.507 [0.000, 3.000],  loss: 19.702614, mse: 12460.985541, mean_q: 103.113850, mean_eps: 0.800296
  33433/150000: episode: 346, duration: 0.793s, episode steps: 115, steps per second: 145, episode reward: -40.629, mean reward: -0.353 [-100.000, 66.346], mean action: 1.530 [0.000, 3.000],  loss: 33.273539, mse: 12258.517646, mean_q: 101.477483, mean_eps: 0.799750
  33528/150000: episode: 347, duration: 0.640s, episode steps:  95, steps per second: 148, episode reward: -153.492, mean reward: -1.616 [-100.000,  4.638], mean action: 1.632 [0.000, 3.000],  loss: 24.835692, mse: 12333.235023, mean_q: 102.385661, mean_eps: 0.799120
  33615/150000: episode: 348, duration: 0.599s, episode steps:  87, steps per second: 145, episode reward: -49.780, mean reward: -0.572 [-100.000,  8.980], mean action: 1.701 [0.000, 3.000],  loss: 36.314009, mse: 12398.447602, mean_q: 102.805151, mean_eps: 0.798574
  33709/150000: episode: 349, duration: 0.752s, episode steps:  94, steps per second: 125, episode reward: -51.363, mean reward: -0.546 [-100.000,  7.615], mean action: 1.638 [0.000, 3.000],  loss: 28.862761, mse: 11995.170888, mean_q: 100.707362, mean_eps: 0.798031
  33811/150000: episode: 350, duration: 0.732s, episode steps: 102, steps per second: 139, episode reward: -261.413, mean reward: -2.563 [-100.000,  0.856], mean action: 1.441 [0.000, 3.000],  loss: 31.497111, mse: 12061.402842, mean_q: 99.817573, mean_eps: 0.797443
  33928/150000: episode: 351, duration: 0.834s, episode steps: 117, steps per second: 140, episode reward: -98.323, mean reward: -0.840 [-100.000, 28.268], mean action: 1.504 [0.000, 3.000],  loss: 29.845528, mse: 11855.203626, mean_q: 100.016108, mean_eps: 0.796786
  34064/150000: episode: 352, duration: 0.942s, episode steps: 136, steps per second: 144, episode reward: -24.478, mean reward: -0.180 [-100.000, 60.683], mean action: 1.471 [0.000, 3.000],  loss: 30.678870, mse: 11914.339521, mean_q: 100.106010, mean_eps: 0.796027
  34182/150000: episode: 353, duration: 0.803s, episode steps: 118, steps per second: 147, episode reward: -41.636, mean reward: -0.353 [-100.000, 28.106], mean action: 1.576 [0.000, 3.000],  loss: 24.866166, mse: 11974.050012, mean_q: 99.747273, mean_eps: 0.795265
  34290/150000: episode: 354, duration: 0.771s, episode steps: 108, steps per second: 140, episode reward: -75.172, mean reward: -0.696 [-100.000,  4.390], mean action: 1.676 [0.000, 3.000],  loss: 31.341847, mse: 12172.246483, mean_q: 101.839070, mean_eps: 0.794587
  34381/150000: episode: 355, duration: 0.641s, episode steps:  91, steps per second: 142, episode reward: -77.765, mean reward: -0.855 [-100.000,  7.928], mean action: 1.407 [0.000, 3.000],  loss: 32.532639, mse: 12459.817394, mean_q: 103.325689, mean_eps: 0.793990
  34456/150000: episode: 356, duration: 0.525s, episode steps:  75, steps per second: 143, episode reward: -47.210, mean reward: -0.629 [-100.000, 23.646], mean action: 1.560 [0.000, 3.000],  loss: 16.448944, mse: 12479.327656, mean_q: 103.991838, mean_eps: 0.793492
  34558/150000: episode: 357, duration: 0.713s, episode steps: 102, steps per second: 143, episode reward: -85.914, mean reward: -0.842 [-100.000,  7.470], mean action: 1.559 [0.000, 3.000],  loss: 25.444753, mse: 12148.452359, mean_q: 100.597472, mean_eps: 0.792961
  34665/150000: episode: 358, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -103.580, mean reward: -0.968 [-100.000, 10.549], mean action: 1.486 [0.000, 3.000],  loss: 21.870737, mse: 12301.028790, mean_q: 101.749436, mean_eps: 0.792334
  34761/150000: episode: 359, duration: 0.649s, episode steps:  96, steps per second: 148, episode reward: -81.593, mean reward: -0.850 [-100.000,  9.613], mean action: 1.844 [0.000, 3.000],  loss: 27.268021, mse: 12207.140269, mean_q: 100.852868, mean_eps: 0.791725
  34850/150000: episode: 360, duration: 0.602s, episode steps:  89, steps per second: 148, episode reward: -55.378, mean reward: -0.622 [-100.000, 19.198], mean action: 1.539 [0.000, 3.000],  loss: 43.405984, mse: 12212.625757, mean_q: 100.320939, mean_eps: 0.791170
  34952/150000: episode: 361, duration: 0.784s, episode steps: 102, steps per second: 130, episode reward: -20.696, mean reward: -0.203 [-100.000, 26.724], mean action: 1.618 [0.000, 3.000],  loss: 34.544148, mse: 12139.687423, mean_q: 99.228737, mean_eps: 0.790597
  35041/150000: episode: 362, duration: 0.618s, episode steps:  89, steps per second: 144, episode reward: -101.251, mean reward: -1.138 [-100.000,  8.754], mean action: 1.674 [0.000, 3.000],  loss: 26.726814, mse: 12356.940715, mean_q: 100.806837, mean_eps: 0.790024
  35127/150000: episode: 363, duration: 0.584s, episode steps:  86, steps per second: 147, episode reward: -98.064, mean reward: -1.140 [-100.000, 12.596], mean action: 1.756 [0.000, 3.000],  loss: 25.718610, mse: 12229.929563, mean_q: 101.843075, mean_eps: 0.789499
  35204/150000: episode: 364, duration: 0.555s, episode steps:  77, steps per second: 139, episode reward: -29.900, mean reward: -0.388 [-100.000, 13.140], mean action: 1.701 [0.000, 3.000],  loss: 40.771186, mse: 12160.886731, mean_q: 102.610881, mean_eps: 0.789010
  35339/150000: episode: 365, duration: 0.925s, episode steps: 135, steps per second: 146, episode reward: -101.683, mean reward: -0.753 [-100.000, 13.121], mean action: 1.659 [0.000, 3.000],  loss: 19.845563, mse: 12431.439634, mean_q: 104.771683, mean_eps: 0.788374
  35414/150000: episode: 366, duration: 0.506s, episode steps:  75, steps per second: 148, episode reward: -79.578, mean reward: -1.061 [-100.000,  9.211], mean action: 1.573 [0.000, 3.000],  loss: 16.102789, mse: 12301.282708, mean_q: 104.328914, mean_eps: 0.787744
  35478/150000: episode: 367, duration: 0.464s, episode steps:  64, steps per second: 138, episode reward: -79.957, mean reward: -1.249 [-100.000,  8.254], mean action: 1.453 [0.000, 3.000],  loss: 36.657535, mse: 12065.727646, mean_q: 103.173108, mean_eps: 0.787327
  35580/150000: episode: 368, duration: 0.850s, episode steps: 102, steps per second: 120, episode reward: -72.564, mean reward: -0.711 [-100.000,  7.449], mean action: 1.578 [0.000, 3.000],  loss: 27.718568, mse: 12141.298158, mean_q: 102.464112, mean_eps: 0.786829
  35663/150000: episode: 369, duration: 0.652s, episode steps:  83, steps per second: 127, episode reward: -105.234, mean reward: -1.268 [-100.000, 11.437], mean action: 1.602 [0.000, 3.000],  loss: 23.130213, mse: 12296.195560, mean_q: 102.866051, mean_eps: 0.786274
  35748/150000: episode: 370, duration: 0.687s, episode steps:  85, steps per second: 124, episode reward: -86.482, mean reward: -1.017 [-100.000,  5.515], mean action: 1.459 [0.000, 3.000],  loss: 31.603649, mse: 12086.725804, mean_q: 102.305103, mean_eps: 0.785770
  35827/150000: episode: 371, duration: 0.622s, episode steps:  79, steps per second: 127, episode reward: -29.500, mean reward: -0.373 [-100.000, 11.777], mean action: 1.671 [0.000, 3.000],  loss: 38.018791, mse: 12126.526750, mean_q: 102.791430, mean_eps: 0.785278
  35886/150000: episode: 372, duration: 0.457s, episode steps:  59, steps per second: 129, episode reward: -58.479, mean reward: -0.991 [-100.000, 17.993], mean action: 1.407 [0.000, 3.000],  loss: 20.273158, mse: 12102.457379, mean_q: 102.207589, mean_eps: 0.784864
  36027/150000: episode: 373, duration: 1.065s, episode steps: 141, steps per second: 132, episode reward: -76.432, mean reward: -0.542 [-100.000,  6.589], mean action: 1.582 [0.000, 3.000],  loss: 27.488929, mse: 12385.818941, mean_q: 103.245436, mean_eps: 0.784264
  36154/150000: episode: 374, duration: 0.899s, episode steps: 127, steps per second: 141, episode reward: -375.556, mean reward: -2.957 [-100.000, 52.960], mean action: 1.740 [0.000, 3.000],  loss: 18.277474, mse: 12001.219542, mean_q: 101.856844, mean_eps: 0.783460
  36249/150000: episode: 375, duration: 0.643s, episode steps:  95, steps per second: 148, episode reward: -59.992, mean reward: -0.631 [-100.000, 26.357], mean action: 1.589 [0.000, 3.000],  loss: 19.445421, mse: 12121.378187, mean_q: 103.461799, mean_eps: 0.782794
  36338/150000: episode: 376, duration: 0.623s, episode steps:  89, steps per second: 143, episode reward: -100.327, mean reward: -1.127 [-100.000,  9.820], mean action: 1.685 [0.000, 3.000],  loss: 21.377705, mse: 11935.209050, mean_q: 99.527205, mean_eps: 0.782242
  36464/150000: episode: 377, duration: 0.868s, episode steps: 126, steps per second: 145, episode reward: -172.178, mean reward: -1.366 [-100.000, 28.178], mean action: 1.500 [0.000, 3.000],  loss: 22.224944, mse: 12015.341425, mean_q: 99.212226, mean_eps: 0.781597
  36574/150000: episode: 378, duration: 0.746s, episode steps: 110, steps per second: 147, episode reward: -87.057, mean reward: -0.791 [-100.000,  5.737], mean action: 1.473 [0.000, 3.000],  loss: 20.276832, mse: 11992.181632, mean_q: 99.899751, mean_eps: 0.780889
  36683/150000: episode: 379, duration: 0.753s, episode steps: 109, steps per second: 145, episode reward: -178.166, mean reward: -1.635 [-100.000, 10.165], mean action: 1.495 [0.000, 3.000],  loss: 47.647213, mse: 12363.606203, mean_q: 104.418336, mean_eps: 0.780232
  36762/150000: episode: 380, duration: 0.542s, episode steps:  79, steps per second: 146, episode reward: -91.848, mean reward: -1.163 [-100.000,  9.969], mean action: 1.696 [0.000, 3.000],  loss: 17.997801, mse: 12406.904210, mean_q: 101.562266, mean_eps: 0.779668
  36855/150000: episode: 381, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -134.884, mean reward: -1.450 [-100.000, 18.336], mean action: 1.613 [0.000, 3.000],  loss: 27.146437, mse: 12316.587166, mean_q: 102.552119, mean_eps: 0.779152
  36986/150000: episode: 382, duration: 0.905s, episode steps: 131, steps per second: 145, episode reward: -56.572, mean reward: -0.432 [-100.000, 12.048], mean action: 1.626 [0.000, 3.000],  loss: 33.413277, mse: 12576.947758, mean_q: 103.856924, mean_eps: 0.778480
  37093/150000: episode: 383, duration: 0.725s, episode steps: 107, steps per second: 148, episode reward: -105.172, mean reward: -0.983 [-100.000, 11.166], mean action: 1.514 [0.000, 3.000],  loss: 21.988310, mse: 12895.671346, mean_q: 106.911722, mean_eps: 0.777766
  37228/150000: episode: 384, duration: 0.910s, episode steps: 135, steps per second: 148, episode reward: -160.309, mean reward: -1.187 [-100.000, 30.594], mean action: 1.659 [0.000, 3.000],  loss: 22.726209, mse: 12849.339685, mean_q: 106.591590, mean_eps: 0.777040
  37324/150000: episode: 385, duration: 0.674s, episode steps:  96, steps per second: 142, episode reward: -149.640, mean reward: -1.559 [-100.000,  8.571], mean action: 1.531 [0.000, 3.000],  loss: 40.420800, mse: 13044.787923, mean_q: 108.117019, mean_eps: 0.776347
  37417/150000: episode: 386, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -40.909, mean reward: -0.440 [-100.000,  6.994], mean action: 1.656 [0.000, 3.000],  loss: 40.563986, mse: 12695.464707, mean_q: 105.738572, mean_eps: 0.775780
  37530/150000: episode: 387, duration: 0.762s, episode steps: 113, steps per second: 148, episode reward: -100.618, mean reward: -0.890 [-100.000,  8.087], mean action: 1.513 [0.000, 3.000],  loss: 33.275863, mse: 12605.018537, mean_q: 104.702320, mean_eps: 0.775162
  37608/150000: episode: 388, duration: 0.535s, episode steps:  78, steps per second: 146, episode reward: -149.707, mean reward: -1.919 [-100.000,  7.093], mean action: 1.513 [0.000, 3.000],  loss: 31.639012, mse: 12309.273613, mean_q: 103.116187, mean_eps: 0.774589
  37729/150000: episode: 389, duration: 0.831s, episode steps: 121, steps per second: 146, episode reward: -123.949, mean reward: -1.024 [-100.000,  6.659], mean action: 1.537 [0.000, 3.000],  loss: 47.093673, mse: 13051.330675, mean_q: 107.014373, mean_eps: 0.773992
  37839/150000: episode: 390, duration: 0.736s, episode steps: 110, steps per second: 150, episode reward: -78.537, mean reward: -0.714 [-100.000, 24.969], mean action: 1.327 [0.000, 3.000],  loss: 22.671665, mse: 12874.035938, mean_q: 105.617079, mean_eps: 0.773299
  37927/150000: episode: 391, duration: 0.609s, episode steps:  88, steps per second: 145, episode reward: -61.245, mean reward: -0.696 [-100.000, 21.035], mean action: 1.727 [0.000, 3.000],  loss: 28.597779, mse: 12922.637318, mean_q: 106.900759, mean_eps: 0.772705
  38002/150000: episode: 392, duration: 0.520s, episode steps:  75, steps per second: 144, episode reward: -138.297, mean reward: -1.844 [-100.000, 10.119], mean action: 1.440 [0.000, 3.000],  loss: 27.750467, mse: 13234.289688, mean_q: 109.022533, mean_eps: 0.772216
  38119/150000: episode: 393, duration: 0.789s, episode steps: 117, steps per second: 148, episode reward: -87.885, mean reward: -0.751 [-100.000,  9.857], mean action: 1.692 [0.000, 3.000],  loss: 26.209151, mse: 13172.602264, mean_q: 107.944083, mean_eps: 0.771640
  38194/150000: episode: 394, duration: 0.502s, episode steps:  75, steps per second: 149, episode reward: -61.255, mean reward: -0.817 [-100.000,  5.444], mean action: 1.427 [0.000, 3.000],  loss: 25.882823, mse: 12877.653125, mean_q: 107.180046, mean_eps: 0.771064
  38269/150000: episode: 395, duration: 0.536s, episode steps:  75, steps per second: 140, episode reward: -88.247, mean reward: -1.177 [-100.000, 16.978], mean action: 1.893 [0.000, 3.000],  loss: 14.868618, mse: 12964.541771, mean_q: 107.948536, mean_eps: 0.770614
  38392/150000: episode: 396, duration: 0.874s, episode steps: 123, steps per second: 141, episode reward: -68.787, mean reward: -0.559 [-100.000, 13.072], mean action: 1.512 [0.000, 3.000],  loss: 18.559946, mse: 12978.968885, mean_q: 106.373495, mean_eps: 0.770020
  38486/150000: episode: 397, duration: 0.638s, episode steps:  94, steps per second: 147, episode reward: -48.101, mean reward: -0.512 [-100.000, 20.625], mean action: 1.447 [0.000, 3.000],  loss: 22.186118, mse: 12788.314141, mean_q: 105.279676, mean_eps: 0.769369
  38595/150000: episode: 398, duration: 0.770s, episode steps: 109, steps per second: 142, episode reward: -87.559, mean reward: -0.803 [-100.000, 16.655], mean action: 1.450 [0.000, 3.000],  loss: 27.846918, mse: 12742.546248, mean_q: 103.699896, mean_eps: 0.768760
  38687/150000: episode: 399, duration: 0.632s, episode steps:  92, steps per second: 146, episode reward: -63.859, mean reward: -0.694 [-100.000, 21.817], mean action: 1.685 [0.000, 3.000],  loss: 29.599107, mse: 12892.327095, mean_q: 105.179791, mean_eps: 0.768157
  38758/150000: episode: 400, duration: 0.512s, episode steps:  71, steps per second: 139, episode reward: -74.779, mean reward: -1.053 [-100.000, 18.871], mean action: 1.620 [0.000, 3.000],  loss: 23.485052, mse: 12964.302102, mean_q: 105.583071, mean_eps: 0.767668
  38890/150000: episode: 401, duration: 0.928s, episode steps: 132, steps per second: 142, episode reward: -132.938, mean reward: -1.007 [-100.000,  4.111], mean action: 1.621 [0.000, 3.000],  loss: 24.731691, mse: 13021.776249, mean_q: 105.068162, mean_eps: 0.767059
  38997/150000: episode: 402, duration: 0.744s, episode steps: 107, steps per second: 144, episode reward: -68.694, mean reward: -0.642 [-100.000,  8.911], mean action: 1.579 [0.000, 3.000],  loss: 16.261575, mse: 13234.286680, mean_q: 105.178628, mean_eps: 0.766342
  39060/150000: episode: 403, duration: 0.428s, episode steps:  63, steps per second: 147, episode reward: -92.125, mean reward: -1.462 [-100.000,  8.771], mean action: 1.794 [0.000, 3.000],  loss: 55.352310, mse: 13525.154576, mean_q: 109.088139, mean_eps: 0.765832
  39193/150000: episode: 404, duration: 0.961s, episode steps: 133, steps per second: 138, episode reward: 12.643, mean reward:  0.095 [-100.000, 66.260], mean action: 1.564 [0.000, 3.000],  loss: 30.419445, mse: 13335.589484, mean_q: 107.425470, mean_eps: 0.765244
  39275/150000: episode: 405, duration: 0.603s, episode steps:  82, steps per second: 136, episode reward: -59.739, mean reward: -0.729 [-100.000,  7.852], mean action: 1.549 [0.000, 3.000],  loss: 20.492110, mse: 13586.615675, mean_q: 108.965543, mean_eps: 0.764599
  39405/150000: episode: 406, duration: 0.935s, episode steps: 130, steps per second: 139, episode reward: -80.755, mean reward: -0.621 [-100.000, 15.578], mean action: 1.685 [0.000, 3.000],  loss: 21.558152, mse: 13940.305416, mean_q: 111.106866, mean_eps: 0.763963
  39508/150000: episode: 407, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -60.495, mean reward: -0.587 [-100.000, 27.524], mean action: 1.544 [0.000, 3.000],  loss: 30.279946, mse: 13614.862456, mean_q: 108.883554, mean_eps: 0.763264
  39603/150000: episode: 408, duration: 0.651s, episode steps:  95, steps per second: 146, episode reward: -7.142, mean reward: -0.075 [-100.000, 10.302], mean action: 1.779 [0.000, 3.000],  loss: 29.357989, mse: 13385.523335, mean_q: 108.559587, mean_eps: 0.762670
  39684/150000: episode: 409, duration: 0.546s, episode steps:  81, steps per second: 148, episode reward: -30.436, mean reward: -0.376 [-100.000, 12.377], mean action: 1.593 [0.000, 3.000],  loss: 32.048329, mse: 13379.400475, mean_q: 108.217294, mean_eps: 0.762142
  39777/150000: episode: 410, duration: 0.641s, episode steps:  93, steps per second: 145, episode reward: -31.044, mean reward: -0.334 [-100.000, 21.561], mean action: 1.645 [0.000, 3.000],  loss: 25.489254, mse: 13858.166845, mean_q: 110.592709, mean_eps: 0.761620
  39893/150000: episode: 411, duration: 0.804s, episode steps: 116, steps per second: 144, episode reward: -188.142, mean reward: -1.622 [-100.000, 46.536], mean action: 1.664 [0.000, 3.000],  loss: 26.108531, mse: 13947.666420, mean_q: 110.925758, mean_eps: 0.760993
  40012/150000: episode: 412, duration: 0.812s, episode steps: 119, steps per second: 147, episode reward: -78.268, mean reward: -0.658 [-100.000,  6.451], mean action: 1.580 [0.000, 3.000],  loss: 32.072438, mse: 13819.944402, mean_q: 110.755360, mean_eps: 0.760288
  40104/150000: episode: 413, duration: 0.652s, episode steps:  92, steps per second: 141, episode reward: -20.025, mean reward: -0.218 [-100.000, 13.272], mean action: 1.685 [0.000, 3.000],  loss: 21.864400, mse: 13806.696958, mean_q: 112.013610, mean_eps: 0.759655
  40246/150000: episode: 414, duration: 0.968s, episode steps: 142, steps per second: 147, episode reward: -44.126, mean reward: -0.311 [-100.000,  8.667], mean action: 1.606 [0.000, 3.000],  loss: 37.733652, mse: 14034.064474, mean_q: 112.124055, mean_eps: 0.758953
  40353/150000: episode: 415, duration: 0.755s, episode steps: 107, steps per second: 142, episode reward: -91.154, mean reward: -0.852 [-100.000, 15.415], mean action: 1.645 [0.000, 3.000],  loss: 25.814732, mse: 14027.058420, mean_q: 110.960968, mean_eps: 0.758206
  40493/150000: episode: 416, duration: 0.989s, episode steps: 140, steps per second: 142, episode reward: -233.447, mean reward: -1.667 [-100.000, 79.236], mean action: 1.643 [0.000, 3.000],  loss: 29.378304, mse: 14229.444685, mean_q: 111.812650, mean_eps: 0.757465
  40627/150000: episode: 417, duration: 0.917s, episode steps: 134, steps per second: 146, episode reward: -90.036, mean reward: -0.672 [-100.000,  9.303], mean action: 1.590 [0.000, 3.000],  loss: 33.694426, mse: 14457.920264, mean_q: 114.155876, mean_eps: 0.756643
  40729/150000: episode: 418, duration: 0.733s, episode steps: 102, steps per second: 139, episode reward: -69.472, mean reward: -0.681 [-100.000, 11.721], mean action: 1.480 [0.000, 3.000],  loss: 16.536293, mse: 14344.661784, mean_q: 113.651943, mean_eps: 0.755935
  40835/150000: episode: 419, duration: 0.740s, episode steps: 106, steps per second: 143, episode reward: -210.826, mean reward: -1.989 [-100.000, 37.667], mean action: 1.585 [0.000, 3.000],  loss: 28.616611, mse: 14447.046239, mean_q: 113.132330, mean_eps: 0.755311
  40925/150000: episode: 420, duration: 0.643s, episode steps:  90, steps per second: 140, episode reward: -56.583, mean reward: -0.629 [-100.000,  9.156], mean action: 1.600 [0.000, 3.000],  loss: 24.582084, mse: 14566.804742, mean_q: 113.683431, mean_eps: 0.754723
  41019/150000: episode: 421, duration: 0.666s, episode steps:  94, steps per second: 141, episode reward: -99.756, mean reward: -1.061 [-100.000,  6.895], mean action: 1.606 [0.000, 3.000],  loss: 36.660000, mse: 14984.317549, mean_q: 115.353950, mean_eps: 0.754171
  41136/150000: episode: 422, duration: 0.810s, episode steps: 117, steps per second: 144, episode reward: -98.507, mean reward: -0.842 [-100.000,  5.722], mean action: 1.598 [0.000, 3.000],  loss: 29.951111, mse: 14292.932676, mean_q: 111.388241, mean_eps: 0.753538
  41233/150000: episode: 423, duration: 0.661s, episode steps:  97, steps per second: 147, episode reward: -193.856, mean reward: -1.999 [-100.000,  7.471], mean action: 1.629 [0.000, 3.000],  loss: 24.155357, mse: 14558.622856, mean_q: 110.929461, mean_eps: 0.752896
  41305/150000: episode: 424, duration: 0.496s, episode steps:  72, steps per second: 145, episode reward: -53.990, mean reward: -0.750 [-100.000, 20.251], mean action: 1.514 [0.000, 3.000],  loss: 26.868449, mse: 14351.814358, mean_q: 110.689036, mean_eps: 0.752389
  41372/150000: episode: 425, duration: 0.490s, episode steps:  67, steps per second: 137, episode reward: -78.824, mean reward: -1.176 [-100.000,  5.223], mean action: 1.403 [0.000, 3.000],  loss: 35.220185, mse: 14529.982626, mean_q: 110.375386, mean_eps: 0.751972
  41448/150000: episode: 426, duration: 0.539s, episode steps:  76, steps per second: 141, episode reward: -69.916, mean reward: -0.920 [-100.000, 13.915], mean action: 1.671 [0.000, 3.000],  loss: 28.195103, mse: 14446.061999, mean_q: 110.719923, mean_eps: 0.751543
  41558/150000: episode: 427, duration: 0.795s, episode steps: 110, steps per second: 138, episode reward: -93.316, mean reward: -0.848 [-100.000,  7.671], mean action: 1.564 [0.000, 3.000],  loss: 27.171446, mse: 14268.126944, mean_q: 110.513331, mean_eps: 0.750985
  41669/150000: episode: 428, duration: 0.814s, episode steps: 111, steps per second: 136, episode reward: 40.307, mean reward:  0.363 [-100.000, 58.295], mean action: 1.757 [0.000, 3.000],  loss: 30.735913, mse: 14276.912584, mean_q: 110.134790, mean_eps: 0.750322
  41808/150000: episode: 429, duration: 0.952s, episode steps: 139, steps per second: 146, episode reward: -30.767, mean reward: -0.221 [-100.000, 17.039], mean action: 1.712 [0.000, 3.000],  loss: 35.794832, mse: 13948.265829, mean_q: 108.502806, mean_eps: 0.749572
  41869/150000: episode: 430, duration: 0.421s, episode steps:  61, steps per second: 145, episode reward: -59.722, mean reward: -0.979 [-100.000, 14.548], mean action: 1.721 [0.000, 3.000],  loss: 26.271699, mse: 14234.440862, mean_q: 108.439278, mean_eps: 0.748972
  41944/150000: episode: 431, duration: 0.552s, episode steps:  75, steps per second: 136, episode reward: -116.644, mean reward: -1.555 [-100.000,  6.351], mean action: 1.467 [0.000, 3.000],  loss: 20.107979, mse: 14529.219232, mean_q: 111.251223, mean_eps: 0.748564
  42062/150000: episode: 432, duration: 0.828s, episode steps: 118, steps per second: 142, episode reward: -57.794, mean reward: -0.490 [-100.000, 17.671], mean action: 1.644 [0.000, 3.000],  loss: 23.623683, mse: 14628.684438, mean_q: 111.050024, mean_eps: 0.747985
  42174/150000: episode: 433, duration: 0.783s, episode steps: 112, steps per second: 143, episode reward: -75.734, mean reward: -0.676 [-100.000, 10.224], mean action: 1.750 [0.000, 3.000],  loss: 27.265627, mse: 14471.489458, mean_q: 110.135425, mean_eps: 0.747295
  42285/150000: episode: 434, duration: 0.807s, episode steps: 111, steps per second: 137, episode reward: -101.305, mean reward: -0.913 [-100.000,  7.804], mean action: 1.486 [0.000, 3.000],  loss: 30.591075, mse: 14784.659382, mean_q: 112.094293, mean_eps: 0.746626
  42374/150000: episode: 435, duration: 0.609s, episode steps:  89, steps per second: 146, episode reward: -41.919, mean reward: -0.471 [-100.000, 18.457], mean action: 1.618 [0.000, 3.000],  loss: 23.436101, mse: 14768.033313, mean_q: 111.122228, mean_eps: 0.746026
  42489/150000: episode: 436, duration: 0.790s, episode steps: 115, steps per second: 146, episode reward: -88.983, mean reward: -0.774 [-100.000, 19.957], mean action: 1.791 [0.000, 3.000],  loss: 39.112976, mse: 14883.506055, mean_q: 111.258277, mean_eps: 0.745414
  42561/150000: episode: 437, duration: 0.551s, episode steps:  72, steps per second: 131, episode reward: -53.673, mean reward: -0.745 [-100.000,  8.764], mean action: 1.708 [0.000, 3.000],  loss: 19.888703, mse: 15326.424805, mean_q: 114.610153, mean_eps: 0.744853
  42690/150000: episode: 438, duration: 0.911s, episode steps: 129, steps per second: 142, episode reward: -36.700, mean reward: -0.284 [-100.000,  8.719], mean action: 1.581 [0.000, 3.000],  loss: 21.566348, mse: 14902.598603, mean_q: 112.620142, mean_eps: 0.744250
  42806/150000: episode: 439, duration: 0.787s, episode steps: 116, steps per second: 147, episode reward: -109.925, mean reward: -0.948 [-100.000,  9.355], mean action: 1.517 [0.000, 3.000],  loss: 21.840072, mse: 15097.155871, mean_q: 112.146221, mean_eps: 0.743515
  42898/150000: episode: 440, duration: 0.694s, episode steps:  92, steps per second: 133, episode reward: -54.333, mean reward: -0.591 [-100.000, 24.916], mean action: 1.587 [0.000, 3.000],  loss: 24.656165, mse: 15163.736137, mean_q: 113.694471, mean_eps: 0.742891
  43038/150000: episode: 441, duration: 0.964s, episode steps: 140, steps per second: 145, episode reward: -82.055, mean reward: -0.586 [-100.000, 19.777], mean action: 1.729 [0.000, 3.000],  loss: 26.750759, mse: 15321.486844, mean_q: 113.709636, mean_eps: 0.742195
  43151/150000: episode: 442, duration: 0.822s, episode steps: 113, steps per second: 138, episode reward: -55.830, mean reward: -0.494 [-100.000, 18.977], mean action: 1.549 [0.000, 3.000],  loss: 32.836540, mse: 15217.604803, mean_q: 112.314105, mean_eps: 0.741436
  43249/150000: episode: 443, duration: 0.680s, episode steps:  98, steps per second: 144, episode reward: -111.667, mean reward: -1.139 [-100.000,  6.935], mean action: 1.582 [0.000, 3.000],  loss: 27.030937, mse: 15098.555385, mean_q: 113.448345, mean_eps: 0.740803
  43355/150000: episode: 444, duration: 0.718s, episode steps: 106, steps per second: 148, episode reward: -46.072, mean reward: -0.435 [-100.000, 51.494], mean action: 1.642 [0.000, 3.000],  loss: 34.555445, mse: 14696.793052, mean_q: 110.095692, mean_eps: 0.740191
  43438/150000: episode: 445, duration: 0.571s, episode steps:  83, steps per second: 145, episode reward: -142.306, mean reward: -1.715 [-100.000,  5.347], mean action: 1.482 [0.000, 3.000],  loss: 30.576047, mse: 14083.783297, mean_q: 107.011470, mean_eps: 0.739624
  43523/150000: episode: 446, duration: 0.617s, episode steps:  85, steps per second: 138, episode reward: 14.520, mean reward:  0.171 [-100.000, 61.602], mean action: 1.471 [0.000, 3.000],  loss: 25.823887, mse: 14198.230825, mean_q: 107.970876, mean_eps: 0.739120
  43624/150000: episode: 447, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -155.344, mean reward: -1.538 [-100.000, 34.322], mean action: 1.812 [0.000, 3.000],  loss: 29.995595, mse: 14143.258016, mean_q: 106.778466, mean_eps: 0.738562
  43685/150000: episode: 448, duration: 0.425s, episode steps:  61, steps per second: 143, episode reward: -76.247, mean reward: -1.250 [-100.000, 10.205], mean action: 1.607 [0.000, 3.000],  loss: 41.402295, mse: 14064.863121, mean_q: 106.902445, mean_eps: 0.738076
  43774/150000: episode: 449, duration: 0.655s, episode steps:  89, steps per second: 136, episode reward: -28.276, mean reward: -0.318 [-100.000, 17.871], mean action: 1.607 [0.000, 3.000],  loss: 37.251992, mse: 14067.929073, mean_q: 106.322558, mean_eps: 0.737626
  43875/150000: episode: 450, duration: 0.701s, episode steps: 101, steps per second: 144, episode reward: -41.356, mean reward: -0.409 [-100.000, 11.370], mean action: 1.644 [0.000, 3.000],  loss: 33.421014, mse: 14040.521774, mean_q: 105.447170, mean_eps: 0.737056
  43955/150000: episode: 451, duration: 0.545s, episode steps:  80, steps per second: 147, episode reward: -93.748, mean reward: -1.172 [-100.000,  7.209], mean action: 1.525 [0.000, 3.000],  loss: 23.535199, mse: 14257.612463, mean_q: 107.910324, mean_eps: 0.736513
  44041/150000: episode: 452, duration: 0.603s, episode steps:  86, steps per second: 143, episode reward: -127.583, mean reward: -1.484 [-100.000,  7.284], mean action: 1.686 [0.000, 3.000],  loss: 26.015138, mse: 13745.425452, mean_q: 105.200162, mean_eps: 0.736015
  44189/150000: episode: 453, duration: 1.095s, episode steps: 148, steps per second: 135, episode reward: -190.368, mean reward: -1.286 [-100.000, 66.399], mean action: 1.682 [0.000, 3.000],  loss: 26.229976, mse: 13791.545047, mean_q: 105.606633, mean_eps: 0.735313
  44291/150000: episode: 454, duration: 0.690s, episode steps: 102, steps per second: 148, episode reward: -75.663, mean reward: -0.742 [-100.000,  7.150], mean action: 1.676 [0.000, 3.000],  loss: 21.846366, mse: 14176.629318, mean_q: 107.639882, mean_eps: 0.734563
  44393/150000: episode: 455, duration: 0.723s, episode steps: 102, steps per second: 141, episode reward: -58.868, mean reward: -0.577 [-100.000, 17.703], mean action: 1.608 [0.000, 3.000],  loss: 30.565260, mse: 13678.555147, mean_q: 103.555766, mean_eps: 0.733951
  44515/150000: episode: 456, duration: 0.844s, episode steps: 122, steps per second: 145, episode reward: -91.655, mean reward: -0.751 [-100.000, 18.474], mean action: 1.697 [0.000, 3.000],  loss: 28.690354, mse: 13838.176222, mean_q: 106.336795, mean_eps: 0.733279
  44604/150000: episode: 457, duration: 0.606s, episode steps:  89, steps per second: 147, episode reward: -104.352, mean reward: -1.172 [-100.000,  5.052], mean action: 1.719 [0.000, 3.000],  loss: 22.905026, mse: 13842.189563, mean_q: 106.925322, mean_eps: 0.732646
  44709/150000: episode: 458, duration: 0.777s, episode steps: 105, steps per second: 135, episode reward: -166.634, mean reward: -1.587 [-100.000,  2.476], mean action: 1.629 [0.000, 3.000],  loss: 34.690837, mse: 13914.347517, mean_q: 107.825048, mean_eps: 0.732064
  44856/150000: episode: 459, duration: 1.102s, episode steps: 147, steps per second: 133, episode reward: -63.210, mean reward: -0.430 [-100.000,  6.603], mean action: 1.701 [0.000, 3.000],  loss: 38.434623, mse: 13748.550828, mean_q: 106.623171, mean_eps: 0.731308
  44935/150000: episode: 460, duration: 0.642s, episode steps:  79, steps per second: 123, episode reward: -56.729, mean reward: -0.718 [-100.000,  8.595], mean action: 1.810 [0.000, 3.000],  loss: 34.597116, mse: 14300.849943, mean_q: 110.191766, mean_eps: 0.730630
  45015/150000: episode: 461, duration: 0.704s, episode steps:  80, steps per second: 114, episode reward: -98.003, mean reward: -1.225 [-100.000, 10.867], mean action: 1.488 [0.000, 3.000],  loss: 30.553936, mse: 14053.261719, mean_q: 109.487430, mean_eps: 0.730153
  45093/150000: episode: 462, duration: 0.670s, episode steps:  78, steps per second: 116, episode reward: -61.678, mean reward: -0.791 [-100.000,  8.827], mean action: 1.808 [0.000, 3.000],  loss: 35.866021, mse: 13669.875701, mean_q: 103.688037, mean_eps: 0.729679
  45189/150000: episode: 463, duration: 0.770s, episode steps:  96, steps per second: 125, episode reward: -76.616, mean reward: -0.798 [-100.000, 13.091], mean action: 1.521 [0.000, 3.000],  loss: 26.014388, mse: 13844.571676, mean_q: 107.101847, mean_eps: 0.729157
  45254/150000: episode: 464, duration: 0.538s, episode steps:  65, steps per second: 121, episode reward: -73.141, mean reward: -1.125 [-100.000,  9.381], mean action: 1.446 [0.000, 3.000],  loss: 19.216433, mse: 14090.549624, mean_q: 108.673682, mean_eps: 0.728674
  45330/150000: episode: 465, duration: 0.645s, episode steps:  76, steps per second: 118, episode reward: -87.887, mean reward: -1.156 [-100.000,  9.009], mean action: 1.697 [0.000, 3.000],  loss: 36.035619, mse: 14276.021189, mean_q: 110.843259, mean_eps: 0.728251
  45459/150000: episode: 466, duration: 0.953s, episode steps: 129, steps per second: 135, episode reward: -103.487, mean reward: -0.802 [-100.000,  7.198], mean action: 1.636 [0.000, 3.000],  loss: 25.745714, mse: 14431.602925, mean_q: 110.070761, mean_eps: 0.727636
  45578/150000: episode: 467, duration: 0.882s, episode steps: 119, steps per second: 135, episode reward: -255.448, mean reward: -2.147 [-100.000, 58.154], mean action: 1.639 [0.000, 3.000],  loss: 17.813390, mse: 14223.667115, mean_q: 109.662108, mean_eps: 0.726892
  45692/150000: episode: 468, duration: 0.804s, episode steps: 114, steps per second: 142, episode reward: -159.318, mean reward: -1.398 [-100.000,  9.639], mean action: 1.640 [0.000, 3.000],  loss: 38.351150, mse: 13868.559579, mean_q: 107.865330, mean_eps: 0.726193
  45858/150000: episode: 469, duration: 1.162s, episode steps: 166, steps per second: 143, episode reward: -93.660, mean reward: -0.564 [-100.000, 28.025], mean action: 1.717 [0.000, 3.000],  loss: 43.420554, mse: 14107.909927, mean_q: 107.677610, mean_eps: 0.725353
  45951/150000: episode: 470, duration: 0.638s, episode steps:  93, steps per second: 146, episode reward: -50.770, mean reward: -0.546 [-100.000,  9.651], mean action: 1.731 [0.000, 3.000],  loss: 35.073896, mse: 14301.504179, mean_q: 108.866737, mean_eps: 0.724576
  46044/150000: episode: 471, duration: 0.628s, episode steps:  93, steps per second: 148, episode reward: -82.519, mean reward: -0.887 [-100.000, 12.048], mean action: 1.452 [0.000, 3.000],  loss: 19.086844, mse: 14138.421507, mean_q: 107.828137, mean_eps: 0.724018
  46135/150000: episode: 472, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -67.332, mean reward: -0.740 [-100.000, 14.618], mean action: 1.549 [0.000, 3.000],  loss: 31.207762, mse: 14190.777934, mean_q: 107.532463, mean_eps: 0.723466
  46258/150000: episode: 473, duration: 0.850s, episode steps: 123, steps per second: 145, episode reward: -126.955, mean reward: -1.032 [-100.000, 13.641], mean action: 1.496 [0.000, 3.000],  loss: 31.149035, mse: 14138.849308, mean_q: 107.142058, mean_eps: 0.722824
  46365/150000: episode: 474, duration: 0.717s, episode steps: 107, steps per second: 149, episode reward: -54.518, mean reward: -0.510 [-100.000, 13.913], mean action: 1.598 [0.000, 3.000],  loss: 33.928260, mse: 14418.194053, mean_q: 107.117432, mean_eps: 0.722134
  46459/150000: episode: 475, duration: 0.661s, episode steps:  94, steps per second: 142, episode reward: -340.662, mean reward: -3.624 [-100.000, 132.975], mean action: 1.436 [0.000, 3.000],  loss: 30.545145, mse: 14174.077076, mean_q: 107.814305, mean_eps: 0.721531
  46598/150000: episode: 476, duration: 0.962s, episode steps: 139, steps per second: 144, episode reward:  9.949, mean reward:  0.072 [-100.000, 61.148], mean action: 1.590 [0.000, 3.000],  loss: 37.858503, mse: 13999.310821, mean_q: 108.371014, mean_eps: 0.720832
  46685/150000: episode: 477, duration: 0.619s, episode steps:  87, steps per second: 141, episode reward: -87.004, mean reward: -1.000 [-100.000,  9.709], mean action: 1.529 [0.000, 3.000],  loss: 31.960520, mse: 13821.666397, mean_q: 106.086919, mean_eps: 0.720154
  46775/150000: episode: 478, duration: 0.650s, episode steps:  90, steps per second: 138, episode reward: -105.692, mean reward: -1.174 [-100.000,  7.325], mean action: 1.511 [0.000, 3.000],  loss: 17.570008, mse: 13758.790690, mean_q: 103.969050, mean_eps: 0.719623
  46918/150000: episode: 479, duration: 0.965s, episode steps: 143, steps per second: 148, episode reward: -146.682, mean reward: -1.026 [-100.000,  5.912], mean action: 1.692 [0.000, 3.000],  loss: 39.778459, mse: 13890.911263, mean_q: 105.564226, mean_eps: 0.718924
  46981/150000: episode: 480, duration: 0.424s, episode steps:  63, steps per second: 149, episode reward: -52.001, mean reward: -0.825 [-100.000,  7.587], mean action: 1.635 [0.000, 3.000],  loss: 19.713302, mse: 13434.796503, mean_q: 105.259941, mean_eps: 0.718306
  47109/150000: episode: 481, duration: 0.915s, episode steps: 128, steps per second: 140, episode reward: -98.853, mean reward: -0.772 [-100.000, 18.319], mean action: 1.609 [0.000, 3.000],  loss: 24.089270, mse: 13409.070366, mean_q: 103.634681, mean_eps: 0.717733
  47253/150000: episode: 482, duration: 0.975s, episode steps: 144, steps per second: 148, episode reward: -81.123, mean reward: -0.563 [-100.000, 12.011], mean action: 1.542 [0.000, 3.000],  loss: 15.658911, mse: 14157.233873, mean_q: 109.228626, mean_eps: 0.716917
  47352/150000: episode: 483, duration: 0.692s, episode steps:  99, steps per second: 143, episode reward: -65.514, mean reward: -0.662 [-100.000,  9.996], mean action: 1.727 [0.000, 3.000],  loss: 35.604210, mse: 13675.677369, mean_q: 105.382423, mean_eps: 0.716188
  47442/150000: episode: 484, duration: 0.631s, episode steps:  90, steps per second: 143, episode reward: -59.190, mean reward: -0.658 [-100.000, 17.744], mean action: 1.389 [0.000, 3.000],  loss: 26.695833, mse: 14208.376660, mean_q: 108.717518, mean_eps: 0.715621
  47542/150000: episode: 485, duration: 0.698s, episode steps: 100, steps per second: 143, episode reward: -66.032, mean reward: -0.660 [-100.000, 13.572], mean action: 1.710 [0.000, 3.000],  loss: 30.453191, mse: 14285.335645, mean_q: 110.145775, mean_eps: 0.715051
  47735/150000: episode: 486, duration: 1.365s, episode steps: 193, steps per second: 141, episode reward: -5.634, mean reward: -0.029 [-100.000, 23.931], mean action: 1.777 [0.000, 3.000],  loss: 25.131130, mse: 14144.207805, mean_q: 108.529094, mean_eps: 0.714172
  47835/150000: episode: 487, duration: 0.683s, episode steps: 100, steps per second: 146, episode reward: -78.249, mean reward: -0.782 [-100.000, 15.889], mean action: 1.490 [0.000, 3.000],  loss: 23.457242, mse: 14153.191680, mean_q: 108.846615, mean_eps: 0.713293
  47953/150000: episode: 488, duration: 0.796s, episode steps: 118, steps per second: 148, episode reward: -22.440, mean reward: -0.190 [-100.000, 18.782], mean action: 1.737 [0.000, 3.000],  loss: 26.887463, mse: 14248.454060, mean_q: 109.804926, mean_eps: 0.712639
  48074/150000: episode: 489, duration: 0.838s, episode steps: 121, steps per second: 144, episode reward: -85.664, mean reward: -0.708 [-100.000,  6.153], mean action: 1.455 [0.000, 3.000],  loss: 26.378930, mse: 14256.280580, mean_q: 109.008540, mean_eps: 0.711922
  48150/150000: episode: 490, duration: 0.523s, episode steps:  76, steps per second: 145, episode reward: -124.028, mean reward: -1.632 [-100.000,  6.775], mean action: 1.513 [0.000, 3.000],  loss: 30.457543, mse: 14693.784604, mean_q: 111.442862, mean_eps: 0.711331
  48257/150000: episode: 491, duration: 0.736s, episode steps: 107, steps per second: 145, episode reward: -145.374, mean reward: -1.359 [-100.000,  2.944], mean action: 1.495 [0.000, 3.000],  loss: 26.576596, mse: 14529.881005, mean_q: 110.687210, mean_eps: 0.710782
  48367/150000: episode: 492, duration: 0.842s, episode steps: 110, steps per second: 131, episode reward: -143.311, mean reward: -1.303 [-100.000, 23.438], mean action: 1.536 [0.000, 3.000],  loss: 22.144028, mse: 14374.812393, mean_q: 109.439291, mean_eps: 0.710131
  48441/150000: episode: 493, duration: 0.518s, episode steps:  74, steps per second: 143, episode reward: -75.995, mean reward: -1.027 [-100.000, 25.231], mean action: 1.635 [0.000, 3.000],  loss: 55.159425, mse: 13965.532979, mean_q: 106.723260, mean_eps: 0.709579
  48569/150000: episode: 494, duration: 0.876s, episode steps: 128, steps per second: 146, episode reward: -90.437, mean reward: -0.707 [-100.000, 10.168], mean action: 1.500 [0.000, 3.000],  loss: 40.136224, mse: 13660.326462, mean_q: 104.965134, mean_eps: 0.708973
  48684/150000: episode: 495, duration: 0.912s, episode steps: 115, steps per second: 126, episode reward: -132.529, mean reward: -1.152 [-100.000, 10.594], mean action: 1.470 [0.000, 3.000],  loss: 25.486523, mse: 13623.453023, mean_q: 106.082393, mean_eps: 0.708244
  48765/150000: episode: 496, duration: 0.602s, episode steps:  81, steps per second: 134, episode reward: -153.079, mean reward: -1.890 [-100.000,  9.801], mean action: 1.481 [0.000, 3.000],  loss: 36.170123, mse: 13543.344666, mean_q: 103.363269, mean_eps: 0.707656
  48861/150000: episode: 497, duration: 0.733s, episode steps:  96, steps per second: 131, episode reward: -95.107, mean reward: -0.991 [-100.000, 14.714], mean action: 1.625 [0.000, 3.000],  loss: 34.930545, mse: 13457.693929, mean_q: 103.181865, mean_eps: 0.707125
  48957/150000: episode: 498, duration: 0.861s, episode steps:  96, steps per second: 112, episode reward: -82.428, mean reward: -0.859 [-100.000,  7.446], mean action: 1.646 [0.000, 3.000],  loss: 14.481301, mse: 13312.937022, mean_q: 102.946099, mean_eps: 0.706549
  49058/150000: episode: 499, duration: 0.818s, episode steps: 101, steps per second: 123, episode reward: -202.902, mean reward: -2.009 [-100.000, 33.665], mean action: 1.644 [0.000, 3.000],  loss: 39.259142, mse: 13415.614645, mean_q: 103.253193, mean_eps: 0.705958
  49145/150000: episode: 500, duration: 0.657s, episode steps:  87, steps per second: 132, episode reward: -88.551, mean reward: -1.018 [-100.000, 11.333], mean action: 1.655 [0.000, 3.000],  loss: 21.555965, mse: 13434.556326, mean_q: 103.392593, mean_eps: 0.705394
  49255/150000: episode: 501, duration: 0.768s, episode steps: 110, steps per second: 143, episode reward: -59.033, mean reward: -0.537 [-100.000, 22.314], mean action: 1.627 [0.000, 3.000],  loss: 21.338702, mse: 13378.913281, mean_q: 102.026844, mean_eps: 0.704803
  49366/150000: episode: 502, duration: 0.748s, episode steps: 111, steps per second: 148, episode reward: -112.442, mean reward: -1.013 [-100.000, 16.894], mean action: 1.649 [0.000, 3.000],  loss: 35.356248, mse: 13261.227504, mean_q: 101.648944, mean_eps: 0.704140
  49451/150000: episode: 503, duration: 0.583s, episode steps:  85, steps per second: 146, episode reward: -89.354, mean reward: -1.051 [-100.000, 12.489], mean action: 1.494 [0.000, 3.000],  loss: 20.329857, mse: 13159.996553, mean_q: 101.991762, mean_eps: 0.703552
  49526/150000: episode: 504, duration: 0.526s, episode steps:  75, steps per second: 143, episode reward: -79.153, mean reward: -1.055 [-100.000,  8.097], mean action: 1.747 [0.000, 3.000],  loss: 18.601255, mse: 13153.250091, mean_q: 101.739107, mean_eps: 0.703072
  49666/150000: episode: 505, duration: 0.942s, episode steps: 140, steps per second: 149, episode reward: -28.768, mean reward: -0.205 [-100.000, 16.927], mean action: 1.486 [0.000, 3.000],  loss: 36.182093, mse: 12943.269448, mean_q: 100.449280, mean_eps: 0.702427
  49775/150000: episode: 506, duration: 0.765s, episode steps: 109, steps per second: 143, episode reward: -216.332, mean reward: -1.985 [-100.000, 62.180], mean action: 1.404 [0.000, 3.000],  loss: 24.742642, mse: 12961.379713, mean_q: 100.957906, mean_eps: 0.701680
  49890/150000: episode: 507, duration: 0.802s, episode steps: 115, steps per second: 143, episode reward: -104.894, mean reward: -0.912 [-100.000, 22.021], mean action: 1.704 [0.000, 3.000],  loss: 27.908759, mse: 13187.926299, mean_q: 102.615158, mean_eps: 0.701008
  49959/150000: episode: 508, duration: 0.508s, episode steps:  69, steps per second: 136, episode reward: -66.658, mean reward: -0.966 [-100.000,  6.946], mean action: 1.406 [0.000, 3.000],  loss: 21.121318, mse: 13350.773607, mean_q: 103.930199, mean_eps: 0.700456
  50043/150000: episode: 509, duration: 0.597s, episode steps:  84, steps per second: 141, episode reward: -66.688, mean reward: -0.794 [-100.000,  9.314], mean action: 1.571 [0.000, 3.000],  loss: 24.338528, mse: 13252.957508, mean_q: 103.181389, mean_eps: 0.699997
  50111/150000: episode: 510, duration: 0.495s, episode steps:  68, steps per second: 137, episode reward: -84.002, mean reward: -1.235 [-100.000, 10.247], mean action: 1.809 [0.000, 3.000],  loss: 24.161678, mse: 13317.691550, mean_q: 103.286314, mean_eps: 0.699541
  50213/150000: episode: 511, duration: 0.696s, episode steps: 102, steps per second: 146, episode reward: -84.354, mean reward: -0.827 [-100.000, 13.706], mean action: 1.725 [0.000, 3.000],  loss: 22.051394, mse: 13332.379691, mean_q: 102.457556, mean_eps: 0.699031
  50313/150000: episode: 512, duration: 0.673s, episode steps: 100, steps per second: 149, episode reward: -77.829, mean reward: -0.778 [-100.000,  6.201], mean action: 1.490 [0.000, 3.000],  loss: 25.456589, mse: 13024.297305, mean_q: 100.536889, mean_eps: 0.698425
  50464/150000: episode: 513, duration: 1.115s, episode steps: 151, steps per second: 135, episode reward: -18.788, mean reward: -0.124 [-100.000,  7.536], mean action: 1.702 [0.000, 3.000],  loss: 24.334046, mse: 12951.628525, mean_q: 99.478287, mean_eps: 0.697672
  50571/150000: episode: 514, duration: 0.942s, episode steps: 107, steps per second: 114, episode reward: -45.372, mean reward: -0.424 [-100.000, 16.202], mean action: 1.692 [0.000, 3.000],  loss: 20.888534, mse: 13223.217600, mean_q: 101.718309, mean_eps: 0.696898
  50676/150000: episode: 515, duration: 0.956s, episode steps: 105, steps per second: 110, episode reward: -122.843, mean reward: -1.170 [-100.000,  7.740], mean action: 1.771 [0.000, 3.000],  loss: 39.756730, mse: 13017.101432, mean_q: 99.697569, mean_eps: 0.696262
  50753/150000: episode: 516, duration: 0.640s, episode steps:  77, steps per second: 120, episode reward: -51.070, mean reward: -0.663 [-100.000, 12.810], mean action: 1.636 [0.000, 3.000],  loss: 26.322780, mse: 12948.030882, mean_q: 99.788110, mean_eps: 0.695716
  50833/150000: episode: 517, duration: 0.709s, episode steps:  80, steps per second: 113, episode reward: -85.991, mean reward: -1.075 [-100.000, 33.845], mean action: 1.650 [0.000, 3.000],  loss: 19.281663, mse: 12911.825195, mean_q: 98.378174, mean_eps: 0.695245
  50954/150000: episode: 518, duration: 1.376s, episode steps: 121, steps per second:  88, episode reward: -60.942, mean reward: -0.504 [-100.000, 89.520], mean action: 1.612 [0.000, 3.000],  loss: 15.170432, mse: 12973.856437, mean_q: 98.629882, mean_eps: 0.694642
  51035/150000: episode: 519, duration: 0.732s, episode steps:  81, steps per second: 111, episode reward: -131.933, mean reward: -1.629 [-100.000, 13.821], mean action: 1.519 [0.000, 3.000],  loss: 21.954352, mse: 13429.729528, mean_q: 101.707703, mean_eps: 0.694036
  51150/150000: episode: 520, duration: 0.927s, episode steps: 115, steps per second: 124, episode reward: -96.187, mean reward: -0.836 [-100.000, 22.230], mean action: 1.522 [0.000, 3.000],  loss: 24.174596, mse: 13150.106165, mean_q: 100.364143, mean_eps: 0.693448
  51265/150000: episode: 521, duration: 0.877s, episode steps: 115, steps per second: 131, episode reward: -82.494, mean reward: -0.717 [-100.000,  7.170], mean action: 1.617 [0.000, 3.000],  loss: 27.882301, mse: 13150.200051, mean_q: 100.256751, mean_eps: 0.692758
  51395/150000: episode: 522, duration: 1.190s, episode steps: 130, steps per second: 109, episode reward: -150.134, mean reward: -1.155 [-100.000, 28.476], mean action: 1.723 [0.000, 3.000],  loss: 23.369122, mse: 13206.668367, mean_q: 100.670058, mean_eps: 0.692023
  51474/150000: episode: 523, duration: 0.827s, episode steps:  79, steps per second:  96, episode reward: -32.763, mean reward: -0.415 [-100.000, 12.940], mean action: 1.481 [0.000, 3.000],  loss: 21.034441, mse: 13345.517640, mean_q: 102.020839, mean_eps: 0.691396
  51581/150000: episode: 524, duration: 1.006s, episode steps: 107, steps per second: 106, episode reward: -153.792, mean reward: -1.437 [-100.000,  7.852], mean action: 1.533 [0.000, 3.000],  loss: 15.126458, mse: 13407.890963, mean_q: 102.307899, mean_eps: 0.690838
  51707/150000: episode: 525, duration: 0.990s, episode steps: 126, steps per second: 127, episode reward: -108.915, mean reward: -0.864 [-100.000,  6.956], mean action: 1.508 [0.000, 3.000],  loss: 28.505752, mse: 13132.857089, mean_q: 100.143636, mean_eps: 0.690139
  51791/150000: episode: 526, duration: 1.390s, episode steps:  84, steps per second:  60, episode reward: -48.161, mean reward: -0.573 [-100.000,  7.096], mean action: 1.643 [0.000, 3.000],  loss: 19.566224, mse: 12742.517101, mean_q: 98.355711, mean_eps: 0.689509
  51918/150000: episode: 527, duration: 4.730s, episode steps: 127, steps per second:  27, episode reward: -5.990, mean reward: -0.047 [-100.000, 15.448], mean action: 1.638 [0.000, 3.000],  loss: 21.777817, mse: 12930.697973, mean_q: 99.541212, mean_eps: 0.688876
  51996/150000: episode: 528, duration: 1.187s, episode steps:  78, steps per second:  66, episode reward: -86.716, mean reward: -1.112 [-100.000, 10.488], mean action: 1.718 [0.000, 3.000],  loss: 32.308990, mse: 12790.183857, mean_q: 97.868854, mean_eps: 0.688261
  52109/150000: episode: 529, duration: 1.282s, episode steps: 113, steps per second:  88, episode reward: -19.532, mean reward: -0.173 [-100.000, 14.972], mean action: 1.602 [0.000, 3.000],  loss: 28.623715, mse: 12951.058948, mean_q: 100.428285, mean_eps: 0.687688
  52193/150000: episode: 530, duration: 0.909s, episode steps:  84, steps per second:  92, episode reward: -84.032, mean reward: -1.000 [-100.000, 18.773], mean action: 1.726 [0.000, 3.000],  loss: 19.408998, mse: 12963.188174, mean_q: 100.064731, mean_eps: 0.687097
  52281/150000: episode: 531, duration: 1.791s, episode steps:  88, steps per second:  49, episode reward: -97.285, mean reward: -1.106 [-100.000, 11.424], mean action: 1.659 [0.000, 3.000],  loss: 15.272442, mse: 12884.743208, mean_q: 99.217460, mean_eps: 0.686581
  52378/150000: episode: 532, duration: 2.357s, episode steps:  97, steps per second:  41, episode reward: -23.299, mean reward: -0.240 [-100.000, 16.233], mean action: 1.701 [0.000, 3.000],  loss: 19.648029, mse: 12896.352811, mean_q: 98.890531, mean_eps: 0.686026
  52463/150000: episode: 533, duration: 1.012s, episode steps:  85, steps per second:  84, episode reward: 34.876, mean reward:  0.410 [-100.000, 56.222], mean action: 1.529 [0.000, 3.000],  loss: 21.803063, mse: 12979.150919, mean_q: 99.824992, mean_eps: 0.685480
  52576/150000: episode: 534, duration: 1.148s, episode steps: 113, steps per second:  98, episode reward: -183.021, mean reward: -1.620 [-100.000,  8.766], mean action: 1.637 [0.000, 3.000],  loss: 27.569456, mse: 13004.546123, mean_q: 100.263036, mean_eps: 0.684886
  52656/150000: episode: 535, duration: 0.723s, episode steps:  80, steps per second: 111, episode reward: -40.618, mean reward: -0.508 [-100.000, 16.025], mean action: 1.575 [0.000, 3.000],  loss: 26.637196, mse: 12460.743103, mean_q: 96.518074, mean_eps: 0.684307
  52755/150000: episode: 536, duration: 1.613s, episode steps:  99, steps per second:  61, episode reward: -66.886, mean reward: -0.676 [-100.000, 34.709], mean action: 1.697 [0.000, 3.000],  loss: 24.770010, mse: 12670.277433, mean_q: 99.133095, mean_eps: 0.683770
  52847/150000: episode: 537, duration: 0.965s, episode steps:  92, steps per second:  95, episode reward: -32.147, mean reward: -0.349 [-100.000, 18.985], mean action: 1.652 [0.000, 3.000],  loss: 30.568087, mse: 12263.034243, mean_q: 95.471900, mean_eps: 0.683197
  52989/150000: episode: 538, duration: 1.162s, episode steps: 142, steps per second: 122, episode reward: -210.086, mean reward: -1.479 [-100.000, 63.122], mean action: 1.620 [0.000, 3.000],  loss: 24.109458, mse: 11984.080505, mean_q: 94.229388, mean_eps: 0.682495
  53069/150000: episode: 539, duration: 0.552s, episode steps:  80, steps per second: 145, episode reward: -11.150, mean reward: -0.139 [-100.000, 22.151], mean action: 1.587 [0.000, 3.000],  loss: 18.858859, mse: 12107.175830, mean_q: 94.379486, mean_eps: 0.681829
  53150/150000: episode: 540, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -76.432, mean reward: -0.944 [-100.000, 12.643], mean action: 1.630 [0.000, 3.000],  loss: 20.891415, mse: 12035.835311, mean_q: 94.498234, mean_eps: 0.681346
  53255/150000: episode: 541, duration: 0.741s, episode steps: 105, steps per second: 142, episode reward: -54.886, mean reward: -0.523 [-100.000, 18.926], mean action: 1.505 [0.000, 3.000],  loss: 21.950906, mse: 11996.776107, mean_q: 94.104922, mean_eps: 0.680788
  53398/150000: episode: 542, duration: 1.012s, episode steps: 143, steps per second: 141, episode reward: -32.330, mean reward: -0.226 [-100.000, 12.904], mean action: 1.643 [0.000, 3.000],  loss: 38.163529, mse: 12194.657868, mean_q: 96.626891, mean_eps: 0.680044
  53474/150000: episode: 543, duration: 0.589s, episode steps:  76, steps per second: 129, episode reward: -178.112, mean reward: -2.344 [-100.000,  9.572], mean action: 1.697 [0.000, 3.000],  loss: 23.764983, mse: 12422.805870, mean_q: 97.632061, mean_eps: 0.679387
  53594/150000: episode: 544, duration: 1.100s, episode steps: 120, steps per second: 109, episode reward: -28.258, mean reward: -0.235 [-100.000,  6.173], mean action: 1.675 [0.000, 3.000],  loss: 21.774176, mse: 12060.888501, mean_q: 95.267036, mean_eps: 0.678799
  53675/150000: episode: 545, duration: 0.717s, episode steps:  81, steps per second: 113, episode reward: -110.912, mean reward: -1.369 [-100.000, 25.065], mean action: 1.741 [0.000, 3.000],  loss: 22.072576, mse: 12167.182111, mean_q: 95.971081, mean_eps: 0.678196
  53766/150000: episode: 546, duration: 0.799s, episode steps:  91, steps per second: 114, episode reward: -56.032, mean reward: -0.616 [-100.000,  6.549], mean action: 1.571 [0.000, 3.000],  loss: 19.988345, mse: 12009.445602, mean_q: 95.869915, mean_eps: 0.677680
  53874/150000: episode: 547, duration: 0.764s, episode steps: 108, steps per second: 141, episode reward: -27.950, mean reward: -0.259 [-100.000, 11.202], mean action: 1.750 [0.000, 3.000],  loss: 24.228910, mse: 12022.738399, mean_q: 95.853029, mean_eps: 0.677083
  53984/150000: episode: 548, duration: 0.797s, episode steps: 110, steps per second: 138, episode reward: -64.860, mean reward: -0.590 [-100.000, 19.072], mean action: 1.455 [0.000, 3.000],  loss: 31.669076, mse: 11928.880238, mean_q: 95.589441, mean_eps: 0.676429
  54068/150000: episode: 549, duration: 0.615s, episode steps:  84, steps per second: 137, episode reward: -39.139, mean reward: -0.466 [-100.000,  8.505], mean action: 1.536 [0.000, 3.000],  loss: 26.079887, mse: 12058.310884, mean_q: 97.561003, mean_eps: 0.675847
  54196/150000: episode: 550, duration: 0.880s, episode steps: 128, steps per second: 146, episode reward: -62.212, mean reward: -0.486 [-100.000, 104.957], mean action: 1.789 [0.000, 3.000],  loss: 29.483015, mse: 12047.758453, mean_q: 96.414300, mean_eps: 0.675211
  54307/150000: episode: 551, duration: 0.754s, episode steps: 111, steps per second: 147, episode reward: -60.784, mean reward: -0.548 [-100.000, 17.015], mean action: 1.658 [0.000, 3.000],  loss: 23.745879, mse: 11972.081618, mean_q: 94.970278, mean_eps: 0.674494
  54435/150000: episode: 552, duration: 0.932s, episode steps: 128, steps per second: 137, episode reward: -58.762, mean reward: -0.459 [-100.000, 12.495], mean action: 1.695 [0.000, 3.000],  loss: 18.552590, mse: 12277.992424, mean_q: 98.314934, mean_eps: 0.673777
  54523/150000: episode: 553, duration: 0.627s, episode steps:  88, steps per second: 140, episode reward: -119.998, mean reward: -1.364 [-100.000, 28.519], mean action: 1.591 [0.000, 3.000],  loss: 26.362670, mse: 12192.850375, mean_q: 97.549675, mean_eps: 0.673129
  54650/150000: episode: 554, duration: 1.020s, episode steps: 127, steps per second: 125, episode reward: -119.254, mean reward: -0.939 [-100.000, 17.706], mean action: 1.583 [0.000, 3.000],  loss: 17.066617, mse: 12085.100194, mean_q: 96.293082, mean_eps: 0.672484
  54715/150000: episode: 555, duration: 0.508s, episode steps:  65, steps per second: 128, episode reward: -84.914, mean reward: -1.306 [-100.000, 15.275], mean action: 1.677 [0.000, 3.000],  loss: 20.386234, mse: 12278.992353, mean_q: 98.330271, mean_eps: 0.671908
  54820/150000: episode: 556, duration: 0.827s, episode steps: 105, steps per second: 127, episode reward: -87.132, mean reward: -0.830 [-100.000, 12.004], mean action: 1.505 [0.000, 3.000],  loss: 16.861594, mse: 12129.711561, mean_q: 97.077560, mean_eps: 0.671398
  54928/150000: episode: 557, duration: 0.913s, episode steps: 108, steps per second: 118, episode reward: -2.987, mean reward: -0.028 [-100.000, 84.676], mean action: 1.676 [0.000, 3.000],  loss: 34.855692, mse: 12333.775210, mean_q: 99.651853, mean_eps: 0.670759
  55010/150000: episode: 558, duration: 0.669s, episode steps:  82, steps per second: 123, episode reward: -73.390, mean reward: -0.895 [-100.000, 11.583], mean action: 1.646 [0.000, 3.000],  loss: 24.909365, mse: 12476.154345, mean_q: 100.752771, mean_eps: 0.670189
  55097/150000: episode: 559, duration: 0.809s, episode steps:  87, steps per second: 107, episode reward: -47.342, mean reward: -0.544 [-100.000,  6.370], mean action: 1.586 [0.000, 3.000],  loss: 23.437929, mse: 12333.594771, mean_q: 99.037330, mean_eps: 0.669682
  55233/150000: episode: 560, duration: 1.125s, episode steps: 136, steps per second: 121, episode reward: -124.262, mean reward: -0.914 [-100.000, 16.490], mean action: 1.684 [0.000, 3.000],  loss: 25.580324, mse: 12816.094856, mean_q: 101.283865, mean_eps: 0.669013
  55363/150000: episode: 561, duration: 0.990s, episode steps: 130, steps per second: 131, episode reward: -31.636, mean reward: -0.243 [-100.000, 10.222], mean action: 1.777 [0.000, 3.000],  loss: 33.208362, mse: 12732.650233, mean_q: 100.414480, mean_eps: 0.668215
  55457/150000: episode: 562, duration: 0.685s, episode steps:  94, steps per second: 137, episode reward: -189.132, mean reward: -2.012 [-100.000, 10.203], mean action: 1.840 [0.000, 3.000],  loss: 25.238672, mse: 12580.095017, mean_q: 99.312653, mean_eps: 0.667543
  55540/150000: episode: 563, duration: 0.581s, episode steps:  83, steps per second: 143, episode reward: -77.560, mean reward: -0.934 [-100.000,  9.546], mean action: 1.675 [0.000, 3.000],  loss: 44.901921, mse: 12425.122764, mean_q: 98.016660, mean_eps: 0.667012
  55645/150000: episode: 564, duration: 0.734s, episode steps: 105, steps per second: 143, episode reward: -55.659, mean reward: -0.530 [-100.000, 14.672], mean action: 1.600 [0.000, 3.000],  loss: 26.730630, mse: 12426.799563, mean_q: 98.233635, mean_eps: 0.666448
  55756/150000: episode: 565, duration: 0.881s, episode steps: 111, steps per second: 126, episode reward: -135.297, mean reward: -1.219 [-100.000, 18.753], mean action: 1.676 [0.000, 3.000],  loss: 38.005503, mse: 12615.012625, mean_q: 100.465306, mean_eps: 0.665800
  55842/150000: episode: 566, duration: 0.659s, episode steps:  86, steps per second: 131, episode reward: -73.969, mean reward: -0.860 [-100.000, 17.326], mean action: 1.616 [0.000, 3.000],  loss: 26.483954, mse: 12581.870100, mean_q: 100.397524, mean_eps: 0.665209
  55926/150000: episode: 567, duration: 0.616s, episode steps:  84, steps per second: 136, episode reward: -69.610, mean reward: -0.829 [-100.000,  8.988], mean action: 1.833 [0.000, 3.000],  loss: 43.585816, mse: 12367.847354, mean_q: 99.359261, mean_eps: 0.664699
  56041/150000: episode: 568, duration: 0.893s, episode steps: 115, steps per second: 129, episode reward: -176.083, mean reward: -1.531 [-100.000, 35.557], mean action: 1.696 [0.000, 3.000],  loss: 26.079433, mse: 12400.807150, mean_q: 100.615139, mean_eps: 0.664102
  56195/150000: episode: 569, duration: 1.120s, episode steps: 154, steps per second: 138, episode reward: -36.419, mean reward: -0.236 [-100.000, 13.709], mean action: 1.701 [0.000, 3.000],  loss: 26.777964, mse: 12161.562728, mean_q: 99.114973, mean_eps: 0.663295
  56281/150000: episode: 570, duration: 0.587s, episode steps:  86, steps per second: 147, episode reward: 10.540, mean reward:  0.123 [-100.000, 19.874], mean action: 1.802 [0.000, 3.000],  loss: 21.600077, mse: 12029.955771, mean_q: 98.373277, mean_eps: 0.662575
  56431/150000: episode: 571, duration: 1.060s, episode steps: 150, steps per second: 142, episode reward: -307.199, mean reward: -2.048 [-100.000, 75.854], mean action: 1.673 [0.000, 3.000],  loss: 23.530861, mse: 11835.588822, mean_q: 95.795199, mean_eps: 0.661867
  56508/150000: episode: 572, duration: 0.536s, episode steps:  77, steps per second: 144, episode reward: -79.912, mean reward: -1.038 [-100.000,  5.851], mean action: 1.442 [0.000, 3.000],  loss: 14.746906, mse: 11983.364930, mean_q: 96.889442, mean_eps: 0.661186
  56636/150000: episode: 573, duration: 0.906s, episode steps: 128, steps per second: 141, episode reward: -78.711, mean reward: -0.615 [-100.000, 10.307], mean action: 1.633 [0.000, 3.000],  loss: 23.071246, mse: 12152.539383, mean_q: 99.477964, mean_eps: 0.660571
  56774/150000: episode: 574, duration: 0.948s, episode steps: 138, steps per second: 146, episode reward: -203.564, mean reward: -1.475 [-100.000, 20.156], mean action: 1.797 [0.000, 3.000],  loss: 16.430852, mse: 11679.971213, mean_q: 96.896106, mean_eps: 0.659773
  56897/150000: episode: 575, duration: 0.833s, episode steps: 123, steps per second: 148, episode reward: -113.524, mean reward: -0.923 [-100.000, 26.106], mean action: 1.439 [0.000, 3.000],  loss: 28.565945, mse: 11699.485161, mean_q: 97.205807, mean_eps: 0.658990
  56978/150000: episode: 576, duration: 0.583s, episode steps:  81, steps per second: 139, episode reward: -163.032, mean reward: -2.013 [-100.000, 11.064], mean action: 1.469 [0.000, 3.000],  loss: 22.222713, mse: 11600.482759, mean_q: 96.177633, mean_eps: 0.658378
  57106/150000: episode: 577, duration: 0.960s, episode steps: 128, steps per second: 133, episode reward: -39.657, mean reward: -0.310 [-100.000,  6.268], mean action: 1.867 [0.000, 3.000],  loss: 39.047433, mse: 11604.788086, mean_q: 95.617880, mean_eps: 0.657751
  57210/150000: episode: 578, duration: 0.732s, episode steps: 104, steps per second: 142, episode reward: -84.377, mean reward: -0.811 [-100.000, 13.944], mean action: 1.452 [0.000, 3.000],  loss: 28.098936, mse: 11629.665828, mean_q: 95.653638, mean_eps: 0.657055
  57311/150000: episode: 579, duration: 2.664s, episode steps: 101, steps per second:  38, episode reward: -49.068, mean reward: -0.486 [-100.000, 15.233], mean action: 1.693 [0.000, 3.000],  loss: 13.986019, mse: 11994.211247, mean_q: 99.710182, mean_eps: 0.656440
  57417/150000: episode: 580, duration: 2.914s, episode steps: 106, steps per second:  36, episode reward: -73.314, mean reward: -0.692 [-100.000,  9.004], mean action: 1.679 [0.000, 3.000],  loss: 19.100109, mse: 11821.426260, mean_q: 98.946555, mean_eps: 0.655819
  57527/150000: episode: 581, duration: 4.411s, episode steps: 110, steps per second:  25, episode reward: -140.931, mean reward: -1.281 [-100.000, 18.108], mean action: 1.555 [0.000, 3.000],  loss: 21.150054, mse: 11572.826438, mean_q: 95.530217, mean_eps: 0.655171
  57630/150000: episode: 582, duration: 3.893s, episode steps: 103, steps per second:  26, episode reward: -71.686, mean reward: -0.696 [-100.000, 15.262], mean action: 1.631 [0.000, 3.000],  loss: 26.452164, mse: 11794.146039, mean_q: 98.106468, mean_eps: 0.654532
  57720/150000: episode: 583, duration: 1.398s, episode steps:  90, steps per second:  64, episode reward: -105.734, mean reward: -1.175 [-100.000, 11.571], mean action: 1.578 [0.000, 3.000],  loss: 19.517071, mse: 11661.878787, mean_q: 97.390111, mean_eps: 0.653953
  57807/150000: episode: 584, duration: 1.431s, episode steps:  87, steps per second:  61, episode reward: -80.454, mean reward: -0.925 [-100.000,  6.449], mean action: 1.437 [0.000, 3.000],  loss: 26.723856, mse: 11624.587172, mean_q: 97.208520, mean_eps: 0.653422
  57999/150000: episode: 585, duration: 3.104s, episode steps: 192, steps per second:  62, episode reward: -67.379, mean reward: -0.351 [-100.000, 12.200], mean action: 1.646 [0.000, 3.000],  loss: 19.625305, mse: 11481.623967, mean_q: 96.939286, mean_eps: 0.652585
  58082/150000: episode: 586, duration: 1.577s, episode steps:  83, steps per second:  53, episode reward: -165.404, mean reward: -1.993 [-100.000,  9.749], mean action: 1.687 [0.000, 3.000],  loss: 20.491359, mse: 11213.486881, mean_q: 95.945384, mean_eps: 0.651760
  58181/150000: episode: 587, duration: 1.466s, episode steps:  99, steps per second:  68, episode reward: -82.373, mean reward: -0.832 [-100.000, 32.462], mean action: 1.687 [0.000, 3.000],  loss: 29.284879, mse: 11435.059787, mean_q: 97.288677, mean_eps: 0.651214
  58274/150000: episode: 588, duration: 1.688s, episode steps:  93, steps per second:  55, episode reward: -29.170, mean reward: -0.314 [-100.000, 10.927], mean action: 1.516 [0.000, 3.000],  loss: 15.911868, mse: 11474.130030, mean_q: 95.902599, mean_eps: 0.650638
  58379/150000: episode: 589, duration: 1.927s, episode steps: 105, steps per second:  54, episode reward: -19.408, mean reward: -0.185 [-100.000,  7.346], mean action: 1.486 [0.000, 3.000],  loss: 19.921298, mse: 11255.955171, mean_q: 95.347700, mean_eps: 0.650044
  58482/150000: episode: 590, duration: 17.061s, episode steps: 103, steps per second:   6, episode reward: -50.217, mean reward: -0.488 [-100.000, 14.722], mean action: 1.718 [0.000, 3.000],  loss: 27.008141, mse: 11253.430313, mean_q: 95.393453, mean_eps: 0.649420
  58584/150000: episode: 591, duration: 2.269s, episode steps: 102, steps per second:  45, episode reward: -174.551, mean reward: -1.711 [-100.000,  4.226], mean action: 1.402 [0.000, 3.000],  loss: 20.178954, mse: 11373.615455, mean_q: 95.984192, mean_eps: 0.648805
  58711/150000: episode: 592, duration: 2.378s, episode steps: 127, steps per second:  53, episode reward: -77.717, mean reward: -0.612 [-100.000, 16.324], mean action: 1.701 [0.000, 3.000],  loss: 26.400579, mse: 11148.646830, mean_q: 93.234171, mean_eps: 0.648118
  58839/150000: episode: 593, duration: 1.275s, episode steps: 128, steps per second: 100, episode reward: -42.673, mean reward: -0.333 [-100.000, 17.751], mean action: 1.812 [0.000, 3.000],  loss: 18.340849, mse: 11086.695164, mean_q: 93.464427, mean_eps: 0.647353
  58952/150000: episode: 594, duration: 0.943s, episode steps: 113, steps per second: 120, episode reward: -83.519, mean reward: -0.739 [-100.000,  6.736], mean action: 1.717 [0.000, 3.000],  loss: 18.390174, mse: 11316.695814, mean_q: 95.337465, mean_eps: 0.646630
  59049/150000: episode: 595, duration: 0.807s, episode steps:  97, steps per second: 120, episode reward: -56.941, mean reward: -0.587 [-100.000,  8.661], mean action: 1.660 [0.000, 3.000],  loss: 15.152624, mse: 11148.851210, mean_q: 92.646148, mean_eps: 0.646000
  59174/150000: episode: 596, duration: 1.050s, episode steps: 125, steps per second: 119, episode reward: -156.992, mean reward: -1.256 [-100.000, 16.170], mean action: 1.600 [0.000, 3.000],  loss: 20.527838, mse: 11268.501773, mean_q: 94.093029, mean_eps: 0.645334
  59344/150000: episode: 597, duration: 1.299s, episode steps: 170, steps per second: 131, episode reward: -150.959, mean reward: -0.888 [-100.000, 80.055], mean action: 1.535 [0.000, 3.000],  loss: 22.424387, mse: 11198.415522, mean_q: 92.959559, mean_eps: 0.644449
  59434/150000: episode: 598, duration: 0.657s, episode steps:  90, steps per second: 137, episode reward: -110.247, mean reward: -1.225 [-100.000, 17.473], mean action: 1.800 [0.000, 3.000],  loss: 15.501565, mse: 11290.484701, mean_q: 94.593897, mean_eps: 0.643669
  59511/150000: episode: 599, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: -109.095, mean reward: -1.417 [-100.000, 12.289], mean action: 1.753 [0.000, 3.000],  loss: 16.111262, mse: 11376.135932, mean_q: 94.359481, mean_eps: 0.643168
  59592/150000: episode: 600, duration: 0.588s, episode steps:  81, steps per second: 138, episode reward: -51.230, mean reward: -0.632 [-100.000,  8.386], mean action: 1.593 [0.000, 3.000],  loss: 23.206228, mse: 11442.762599, mean_q: 96.122984, mean_eps: 0.642694
  59754/150000: episode: 601, duration: 1.138s, episode steps: 162, steps per second: 142, episode reward: -234.639, mean reward: -1.448 [-100.000, 105.652], mean action: 1.778 [0.000, 3.000],  loss: 23.232464, mse: 11278.511803, mean_q: 94.624409, mean_eps: 0.641965
  59852/150000: episode: 602, duration: 0.680s, episode steps:  98, steps per second: 144, episode reward: -42.268, mean reward: -0.431 [-100.000,  7.661], mean action: 1.735 [0.000, 3.000],  loss: 22.717002, mse: 11280.747280, mean_q: 95.311658, mean_eps: 0.641185
  59938/150000: episode: 603, duration: 0.617s, episode steps:  86, steps per second: 139, episode reward: -41.179, mean reward: -0.479 [-100.000, 23.445], mean action: 1.453 [0.000, 3.000],  loss: 23.252340, mse: 11167.352130, mean_q: 94.276011, mean_eps: 0.640633
  60108/150000: episode: 604, duration: 1.164s, episode steps: 170, steps per second: 146, episode reward: -212.853, mean reward: -1.252 [-100.000, 18.419], mean action: 1.594 [0.000, 3.000],  loss: 34.158806, mse: 10972.268601, mean_q: 92.399976, mean_eps: 0.639865
  60192/150000: episode: 605, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -116.001, mean reward: -1.381 [-100.000,  7.656], mean action: 1.988 [0.000, 3.000],  loss: 28.248786, mse: 10959.956183, mean_q: 93.027104, mean_eps: 0.639103
  60291/150000: episode: 606, duration: 0.689s, episode steps:  99, steps per second: 144, episode reward: -134.847, mean reward: -1.362 [-100.000, 18.925], mean action: 1.646 [0.000, 3.000],  loss: 30.192667, mse: 10932.720437, mean_q: 92.551030, mean_eps: 0.638554
  60372/150000: episode: 607, duration: 0.544s, episode steps:  81, steps per second: 149, episode reward: -103.135, mean reward: -1.273 [-100.000,  8.937], mean action: 1.543 [0.000, 3.000],  loss: 17.468463, mse: 10524.389896, mean_q: 90.723119, mean_eps: 0.638014
  60499/150000: episode: 608, duration: 0.863s, episode steps: 127, steps per second: 147, episode reward: -92.108, mean reward: -0.725 [-100.000,  7.507], mean action: 1.598 [0.000, 3.000],  loss: 37.514629, mse: 10667.741980, mean_q: 94.274843, mean_eps: 0.637390
  60602/150000: episode: 609, duration: 0.739s, episode steps: 103, steps per second: 139, episode reward: -31.127, mean reward: -0.302 [-100.000, 82.487], mean action: 1.709 [0.000, 3.000],  loss: 26.213005, mse: 10391.200556, mean_q: 91.277505, mean_eps: 0.636700
  60701/150000: episode: 610, duration: 0.704s, episode steps:  99, steps per second: 141, episode reward: -86.003, mean reward: -0.869 [-100.000, 10.903], mean action: 1.798 [0.000, 3.000],  loss: 26.983711, mse: 10236.703988, mean_q: 90.778674, mean_eps: 0.636094
  60785/150000: episode: 611, duration: 0.572s, episode steps:  84, steps per second: 147, episode reward: -61.864, mean reward: -0.736 [-100.000,  9.731], mean action: 1.774 [0.000, 3.000],  loss: 32.592613, mse: 10149.705886, mean_q: 90.905017, mean_eps: 0.635545
  60872/150000: episode: 612, duration: 0.639s, episode steps:  87, steps per second: 136, episode reward: -60.754, mean reward: -0.698 [-100.000, 19.924], mean action: 1.690 [0.000, 3.000],  loss: 27.175389, mse: 10232.940486, mean_q: 92.385158, mean_eps: 0.635032
  60962/150000: episode: 613, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -32.218, mean reward: -0.358 [-100.000, 11.837], mean action: 1.667 [0.000, 3.000],  loss: 19.097397, mse: 10483.354134, mean_q: 94.746048, mean_eps: 0.634501
  61094/150000: episode: 614, duration: 0.888s, episode steps: 132, steps per second: 149, episode reward: -18.609, mean reward: -0.141 [-100.000, 12.749], mean action: 1.689 [0.000, 3.000],  loss: 15.645373, mse: 10437.237090, mean_q: 93.772064, mean_eps: 0.633835
  61197/150000: episode: 615, duration: 0.744s, episode steps: 103, steps per second: 138, episode reward: -130.037, mean reward: -1.262 [-100.000,  4.366], mean action: 1.641 [0.000, 3.000],  loss: 28.209617, mse: 10617.657141, mean_q: 94.303141, mean_eps: 0.633130
  61285/150000: episode: 616, duration: 0.595s, episode steps:  88, steps per second: 148, episode reward: -112.132, mean reward: -1.274 [-100.000,  9.589], mean action: 1.557 [0.000, 3.000],  loss: 23.125288, mse: 10133.662548, mean_q: 92.198209, mean_eps: 0.632557
  61406/150000: episode: 617, duration: 0.817s, episode steps: 121, steps per second: 148, episode reward: -84.644, mean reward: -0.700 [-100.000, 10.698], mean action: 1.628 [0.000, 3.000],  loss: 23.789461, mse: 10315.294890, mean_q: 93.847125, mean_eps: 0.631930
  61507/150000: episode: 618, duration: 0.785s, episode steps: 101, steps per second: 129, episode reward: -85.106, mean reward: -0.843 [-100.000, 12.307], mean action: 1.604 [0.000, 3.000],  loss: 23.677195, mse: 10335.334545, mean_q: 94.480642, mean_eps: 0.631264
  61651/150000: episode: 619, duration: 1.252s, episode steps: 144, steps per second: 115, episode reward: -182.358, mean reward: -1.266 [-100.000, 29.494], mean action: 1.403 [0.000, 3.000],  loss: 26.622866, mse: 10450.557502, mean_q: 96.064178, mean_eps: 0.630529
  61756/150000: episode: 620, duration: 1.087s, episode steps: 105, steps per second:  97, episode reward: -0.096, mean reward: -0.001 [-100.000, 24.691], mean action: 1.743 [0.000, 3.000],  loss: 28.687503, mse: 10528.482822, mean_q: 95.498660, mean_eps: 0.629782
  61842/150000: episode: 621, duration: 0.838s, episode steps:  86, steps per second: 103, episode reward: -86.724, mean reward: -1.008 [-100.000,  5.839], mean action: 1.535 [0.000, 3.000],  loss: 21.888174, mse: 10228.082565, mean_q: 93.383093, mean_eps: 0.629209
  61974/150000: episode: 622, duration: 1.238s, episode steps: 132, steps per second: 107, episode reward: -288.899, mean reward: -2.189 [-100.000,  6.442], mean action: 1.629 [0.000, 3.000],  loss: 20.774224, mse: 10526.496549, mean_q: 95.195541, mean_eps: 0.628555
  62072/150000: episode: 623, duration: 0.736s, episode steps:  98, steps per second: 133, episode reward: -66.930, mean reward: -0.683 [-100.000, 19.020], mean action: 1.806 [0.000, 3.000],  loss: 24.530085, mse: 10469.090741, mean_q: 94.699212, mean_eps: 0.627865
  62206/150000: episode: 624, duration: 0.952s, episode steps: 134, steps per second: 141, episode reward: -62.952, mean reward: -0.470 [-100.000,  7.272], mean action: 1.724 [0.000, 3.000],  loss: 20.409323, mse: 10355.216607, mean_q: 93.098371, mean_eps: 0.627169
  62311/150000: episode: 625, duration: 0.773s, episode steps: 105, steps per second: 136, episode reward: -118.308, mean reward: -1.127 [-100.000, 10.909], mean action: 1.686 [0.000, 3.000],  loss: 17.108303, mse: 10515.974279, mean_q: 94.648733, mean_eps: 0.626452
  62388/150000: episode: 626, duration: 0.553s, episode steps:  77, steps per second: 139, episode reward: -99.428, mean reward: -1.291 [-100.000,  8.879], mean action: 1.649 [0.000, 3.000],  loss: 19.233148, mse: 10576.362634, mean_q: 94.771706, mean_eps: 0.625906
  62457/150000: episode: 627, duration: 0.465s, episode steps:  69, steps per second: 149, episode reward: -35.800, mean reward: -0.519 [-100.000, 15.410], mean action: 1.565 [0.000, 3.000],  loss: 30.692322, mse: 10471.094613, mean_q: 93.159386, mean_eps: 0.625468
  62548/150000: episode: 628, duration: 0.650s, episode steps:  91, steps per second: 140, episode reward: -81.660, mean reward: -0.897 [-100.000, 13.472], mean action: 1.670 [0.000, 3.000],  loss: 24.423221, mse: 10376.555305, mean_q: 92.019893, mean_eps: 0.624988
  62671/150000: episode: 629, duration: 1.188s, episode steps: 123, steps per second: 104, episode reward: -17.435, mean reward: -0.142 [-100.000, 28.205], mean action: 1.683 [0.000, 3.000],  loss: 20.413773, mse: 10408.130947, mean_q: 94.146943, mean_eps: 0.624346
  62784/150000: episode: 630, duration: 1.438s, episode steps: 113, steps per second:  79, episode reward: -161.322, mean reward: -1.428 [-100.000, 60.004], mean action: 1.593 [0.000, 3.000],  loss: 22.962930, mse: 10130.090293, mean_q: 90.901431, mean_eps: 0.623638
  62902/150000: episode: 631, duration: 1.253s, episode steps: 118, steps per second:  94, episode reward: -79.645, mean reward: -0.675 [-100.000, 10.269], mean action: 1.797 [0.000, 3.000],  loss: 23.579673, mse: 10457.809591, mean_q: 93.256996, mean_eps: 0.622945
  63004/150000: episode: 632, duration: 0.895s, episode steps: 102, steps per second: 114, episode reward: -108.469, mean reward: -1.063 [-100.000, 20.179], mean action: 1.559 [0.000, 3.000],  loss: 32.189983, mse: 10660.832376, mean_q: 94.616728, mean_eps: 0.622285
  63103/150000: episode: 633, duration: 0.804s, episode steps:  99, steps per second: 123, episode reward: -59.821, mean reward: -0.604 [-100.000,  9.417], mean action: 1.778 [0.000, 3.000],  loss: 23.993659, mse: 10463.141789, mean_q: 93.956308, mean_eps: 0.621682
  63193/150000: episode: 634, duration: 0.988s, episode steps:  90, steps per second:  91, episode reward: -100.606, mean reward: -1.118 [-100.000, 10.128], mean action: 1.622 [0.000, 3.000],  loss: 19.248207, mse: 10251.817855, mean_q: 91.868342, mean_eps: 0.621115
  63327/150000: episode: 635, duration: 1.444s, episode steps: 134, steps per second:  93, episode reward: -113.882, mean reward: -0.850 [-100.000,  8.588], mean action: 1.746 [0.000, 3.000],  loss: 25.751084, mse: 10402.142112, mean_q: 93.788811, mean_eps: 0.620443
  63456/150000: episode: 636, duration: 1.390s, episode steps: 129, steps per second:  93, episode reward: -101.961, mean reward: -0.790 [-100.000,  8.485], mean action: 1.705 [0.000, 3.000],  loss: 25.944703, mse: 10151.506703, mean_q: 91.429211, mean_eps: 0.619654
  63584/150000: episode: 637, duration: 1.464s, episode steps: 128, steps per second:  87, episode reward: -35.812, mean reward: -0.280 [-100.000,  8.002], mean action: 1.688 [0.000, 3.000],  loss: 40.027297, mse: 10368.088814, mean_q: 93.400301, mean_eps: 0.618883
  63685/150000: episode: 638, duration: 1.026s, episode steps: 101, steps per second:  98, episode reward: -74.892, mean reward: -0.742 [-100.000,  7.898], mean action: 1.673 [0.000, 3.000],  loss: 30.296248, mse: 10536.762613, mean_q: 94.944159, mean_eps: 0.618196
  63796/150000: episode: 639, duration: 1.152s, episode steps: 111, steps per second:  96, episode reward: -50.738, mean reward: -0.457 [-100.000, 16.872], mean action: 1.550 [0.000, 3.000],  loss: 24.829404, mse: 10437.596996, mean_q: 93.284419, mean_eps: 0.617560
  63902/150000: episode: 640, duration: 1.022s, episode steps: 106, steps per second: 104, episode reward: -94.645, mean reward: -0.893 [-100.000,  8.745], mean action: 1.660 [0.000, 3.000],  loss: 18.355254, mse: 10446.576983, mean_q: 94.071149, mean_eps: 0.616909
  64286/150000: episode: 641, duration: 3.452s, episode steps: 384, steps per second: 111, episode reward: -58.950, mean reward: -0.154 [-100.000, 15.793], mean action: 1.695 [0.000, 3.000],  loss: 22.372891, mse: 10398.204399, mean_q: 92.413831, mean_eps: 0.615439
  64398/150000: episode: 642, duration: 1.040s, episode steps: 112, steps per second: 108, episode reward: -73.716, mean reward: -0.658 [-100.000, 10.477], mean action: 1.598 [0.000, 3.000],  loss: 20.136682, mse: 10385.657523, mean_q: 91.783117, mean_eps: 0.613951
  64522/150000: episode: 643, duration: 1.143s, episode steps: 124, steps per second: 108, episode reward: -43.474, mean reward: -0.351 [-100.000,  8.595], mean action: 1.613 [0.000, 3.000],  loss: 37.610666, mse: 10241.863206, mean_q: 90.828555, mean_eps: 0.613243
  64619/150000: episode: 644, duration: 0.887s, episode steps:  97, steps per second: 109, episode reward: -37.306, mean reward: -0.385 [-100.000, 28.950], mean action: 1.773 [0.000, 3.000],  loss: 20.553739, mse: 10336.521988, mean_q: 92.690885, mean_eps: 0.612580
  64760/150000: episode: 645, duration: 1.279s, episode steps: 141, steps per second: 110, episode reward: -43.119, mean reward: -0.306 [-100.000,  9.047], mean action: 1.730 [0.000, 3.000],  loss: 25.025454, mse: 10330.440534, mean_q: 92.641820, mean_eps: 0.611866
  64912/150000: episode: 646, duration: 1.325s, episode steps: 152, steps per second: 115, episode reward: -69.927, mean reward: -0.460 [-100.000, 30.783], mean action: 1.757 [0.000, 3.000],  loss: 23.927378, mse: 10165.806865, mean_q: 91.915022, mean_eps: 0.610987
  65019/150000: episode: 647, duration: 0.878s, episode steps: 107, steps per second: 122, episode reward: -73.492, mean reward: -0.687 [-100.000, 18.183], mean action: 1.701 [0.000, 3.000],  loss: 19.491694, mse: 10047.897187, mean_q: 90.314535, mean_eps: 0.610210
  65129/150000: episode: 648, duration: 0.804s, episode steps: 110, steps per second: 137, episode reward: -25.323, mean reward: -0.230 [-100.000, 15.364], mean action: 1.682 [0.000, 3.000],  loss: 23.761487, mse: 10059.935938, mean_q: 89.988242, mean_eps: 0.609559
  65225/150000: episode: 649, duration: 0.716s, episode steps:  96, steps per second: 134, episode reward: 21.102, mean reward:  0.220 [-100.000, 16.321], mean action: 1.708 [0.000, 3.000],  loss: 23.296519, mse: 9855.777323, mean_q: 90.048317, mean_eps: 0.608941
  65387/150000: episode: 650, duration: 1.323s, episode steps: 162, steps per second: 122, episode reward: -158.242, mean reward: -0.977 [-100.000,  2.684], mean action: 1.642 [0.000, 3.000],  loss: 19.886405, mse: 9979.587689, mean_q: 91.418107, mean_eps: 0.608167
  65482/150000: episode: 651, duration: 0.755s, episode steps:  95, steps per second: 126, episode reward: -65.663, mean reward: -0.691 [-100.000,  9.409], mean action: 1.516 [0.000, 3.000],  loss: 25.284344, mse: 9822.468467, mean_q: 89.648233, mean_eps: 0.607396
  65596/150000: episode: 652, duration: 0.887s, episode steps: 114, steps per second: 129, episode reward: -57.567, mean reward: -0.505 [-100.000,  6.711], mean action: 1.614 [0.000, 3.000],  loss: 28.848712, mse: 9986.473646, mean_q: 92.612760, mean_eps: 0.606769
  65735/150000: episode: 653, duration: 0.945s, episode steps: 139, steps per second: 147, episode reward: -10.296, mean reward: -0.074 [-100.000, 11.912], mean action: 1.583 [0.000, 3.000],  loss: 26.664059, mse: 9899.334195, mean_q: 91.469913, mean_eps: 0.606010
  65848/150000: episode: 654, duration: 0.784s, episode steps: 113, steps per second: 144, episode reward: -95.592, mean reward: -0.846 [-100.000,  5.646], mean action: 1.469 [0.000, 3.000],  loss: 26.416228, mse: 9977.970344, mean_q: 91.655609, mean_eps: 0.605254
  65971/150000: episode: 655, duration: 0.848s, episode steps: 123, steps per second: 145, episode reward: -100.000, mean reward: -0.813 [-100.000,  7.985], mean action: 1.724 [0.000, 3.000],  loss: 23.069615, mse: 9901.010290, mean_q: 91.024751, mean_eps: 0.604546
  66041/150000: episode: 656, duration: 0.483s, episode steps:  70, steps per second: 145, episode reward: -41.369, mean reward: -0.591 [-100.000, 10.697], mean action: 1.800 [0.000, 3.000],  loss: 18.652310, mse: 9971.508105, mean_q: 90.579712, mean_eps: 0.603967
  66289/150000: episode: 657, duration: 1.716s, episode steps: 248, steps per second: 145, episode reward: -53.295, mean reward: -0.215 [-100.000, 43.021], mean action: 1.681 [0.000, 3.000],  loss: 25.933833, mse: 9896.679150, mean_q: 90.428440, mean_eps: 0.603013
  66385/150000: episode: 658, duration: 0.640s, episode steps:  96, steps per second: 150, episode reward: -70.241, mean reward: -0.732 [-100.000, 14.427], mean action: 1.656 [0.000, 3.000],  loss: 23.471639, mse: 9815.516978, mean_q: 90.123680, mean_eps: 0.601981
  66520/150000: episode: 659, duration: 0.929s, episode steps: 135, steps per second: 145, episode reward: -108.962, mean reward: -0.807 [-100.000,  7.798], mean action: 1.489 [0.000, 3.000],  loss: 22.141140, mse: 9949.658905, mean_q: 91.895876, mean_eps: 0.601288
  66693/150000: episode: 660, duration: 1.152s, episode steps: 173, steps per second: 150, episode reward: -103.425, mean reward: -0.598 [-100.000, 11.267], mean action: 1.647 [0.000, 3.000],  loss: 20.902700, mse: 9956.557326, mean_q: 91.690681, mean_eps: 0.600364
  66772/150000: episode: 661, duration: 0.557s, episode steps:  79, steps per second: 142, episode reward: -97.154, mean reward: -1.230 [-100.000,  6.179], mean action: 1.848 [0.000, 3.000],  loss: 24.706108, mse: 9918.757677, mean_q: 91.111312, mean_eps: 0.599608
  66852/150000: episode: 662, duration: 0.538s, episode steps:  80, steps per second: 149, episode reward: -64.236, mean reward: -0.803 [-100.000, 21.787], mean action: 1.738 [0.000, 3.000],  loss: 20.818056, mse: 9797.342322, mean_q: 90.395179, mean_eps: 0.599131
  66923/150000: episode: 663, duration: 0.498s, episode steps:  71, steps per second: 143, episode reward: -68.080, mean reward: -0.959 [-100.000,  8.883], mean action: 1.915 [0.000, 3.000],  loss: 35.107860, mse: 10051.456873, mean_q: 92.657650, mean_eps: 0.598678
  67025/150000: episode: 664, duration: 0.716s, episode steps: 102, steps per second: 142, episode reward: -43.537, mean reward: -0.427 [-100.000, 10.459], mean action: 1.725 [0.000, 3.000],  loss: 33.910481, mse: 9976.599681, mean_q: 91.885946, mean_eps: 0.598159
  67137/150000: episode: 665, duration: 0.934s, episode steps: 112, steps per second: 120, episode reward: -87.265, mean reward: -0.779 [-100.000, 22.594], mean action: 1.866 [0.000, 3.000],  loss: 26.436811, mse: 9945.286477, mean_q: 92.318165, mean_eps: 0.597517
  67228/150000: episode: 666, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -56.090, mean reward: -0.616 [-100.000, 10.371], mean action: 1.703 [0.000, 3.000],  loss: 19.583236, mse: 10056.100500, mean_q: 94.003190, mean_eps: 0.596908
  67336/150000: episode: 667, duration: 0.728s, episode steps: 108, steps per second: 148, episode reward: -14.586, mean reward: -0.135 [-100.000, 23.924], mean action: 1.704 [0.000, 3.000],  loss: 24.601201, mse: 9986.987418, mean_q: 93.388937, mean_eps: 0.596311
  67484/150000: episode: 668, duration: 1.037s, episode steps: 148, steps per second: 143, episode reward:  8.333, mean reward:  0.056 [-100.000, 19.537], mean action: 1.736 [0.000, 3.000],  loss: 18.543706, mse: 10222.938427, mean_q: 95.274303, mean_eps: 0.595543
  67585/150000: episode: 669, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -47.783, mean reward: -0.473 [-100.000, 15.622], mean action: 1.554 [0.000, 3.000],  loss: 22.323924, mse: 10107.552193, mean_q: 95.335552, mean_eps: 0.594796
  67724/150000: episode: 670, duration: 0.978s, episode steps: 139, steps per second: 142, episode reward: -73.324, mean reward: -0.528 [-100.000,  8.309], mean action: 1.655 [0.000, 3.000],  loss: 16.953812, mse: 10168.500878, mean_q: 96.367413, mean_eps: 0.594076
  67784/150000: episode: 671, duration: 0.406s, episode steps:  60, steps per second: 148, episode reward: -80.061, mean reward: -1.334 [-100.000,  9.063], mean action: 1.750 [0.000, 3.000],  loss: 38.201528, mse: 10222.413086, mean_q: 95.809748, mean_eps: 0.593479
  67930/150000: episode: 672, duration: 0.990s, episode steps: 146, steps per second: 148, episode reward: -25.562, mean reward: -0.175 [-100.000, 21.357], mean action: 1.630 [0.000, 3.000],  loss: 26.494060, mse: 10293.474810, mean_q: 96.329997, mean_eps: 0.592861
  68038/150000: episode: 673, duration: 0.763s, episode steps: 108, steps per second: 142, episode reward: -61.597, mean reward: -0.570 [-100.000,  9.786], mean action: 1.602 [0.000, 3.000],  loss: 34.635853, mse: 10361.891891, mean_q: 95.881972, mean_eps: 0.592099
  68145/150000: episode: 674, duration: 0.722s, episode steps: 107, steps per second: 148, episode reward: -61.772, mean reward: -0.577 [-100.000,  9.135], mean action: 1.710 [0.000, 3.000],  loss: 28.070874, mse: 10487.790550, mean_q: 96.760128, mean_eps: 0.591454
  68251/150000: episode: 675, duration: 0.716s, episode steps: 106, steps per second: 148, episode reward: -35.847, mean reward: -0.338 [-100.000, 12.471], mean action: 1.745 [0.000, 3.000],  loss: 48.360302, mse: 10780.435938, mean_q: 98.893907, mean_eps: 0.590815
  68354/150000: episode: 676, duration: 0.722s, episode steps: 103, steps per second: 143, episode reward: -92.532, mean reward: -0.898 [-100.000, 25.120], mean action: 1.476 [0.000, 3.000],  loss: 20.865933, mse: 10763.532264, mean_q: 98.080373, mean_eps: 0.590188
  68497/150000: episode: 677, duration: 0.963s, episode steps: 143, steps per second: 149, episode reward: -15.338, mean reward: -0.107 [-100.000, 20.132], mean action: 1.678 [0.000, 3.000],  loss: 29.935028, mse: 10551.754978, mean_q: 96.040476, mean_eps: 0.589450
  68625/150000: episode: 678, duration: 0.870s, episode steps: 128, steps per second: 147, episode reward: -5.121, mean reward: -0.040 [-100.000, 20.154], mean action: 1.555 [0.000, 3.000],  loss: 14.864242, mse: 10561.126724, mean_q: 96.724669, mean_eps: 0.588637
  68729/150000: episode: 679, duration: 0.706s, episode steps: 104, steps per second: 147, episode reward: -18.389, mean reward: -0.177 [-100.000, 14.392], mean action: 1.462 [0.000, 3.000],  loss: 27.027214, mse: 10585.251972, mean_q: 96.620098, mean_eps: 0.587941
  68846/150000: episode: 680, duration: 0.787s, episode steps: 117, steps per second: 149, episode reward: -18.729, mean reward: -0.160 [-100.000,  9.994], mean action: 1.735 [0.000, 3.000],  loss: 34.970072, mse: 10618.397469, mean_q: 96.292218, mean_eps: 0.587278
  68942/150000: episode: 681, duration: 0.665s, episode steps:  96, steps per second: 144, episode reward: -91.063, mean reward: -0.949 [-100.000, 10.407], mean action: 1.615 [0.000, 3.000],  loss: 21.285767, mse: 10584.186351, mean_q: 95.258843, mean_eps: 0.586639
  69034/150000: episode: 682, duration: 0.627s, episode steps:  92, steps per second: 147, episode reward: -39.265, mean reward: -0.427 [-100.000, 13.436], mean action: 1.717 [0.000, 3.000],  loss: 18.454771, mse: 10581.214982, mean_q: 94.974688, mean_eps: 0.586075
  69178/150000: episode: 683, duration: 0.963s, episode steps: 144, steps per second: 149, episode reward: -72.339, mean reward: -0.502 [-100.000, 11.768], mean action: 1.535 [0.000, 3.000],  loss: 24.339845, mse: 10674.686751, mean_q: 96.252198, mean_eps: 0.585367
  70178/150000: episode: 684, duration: 8.326s, episode steps: 1000, steps per second: 120, episode reward:  7.888, mean reward:  0.008 [-24.946, 23.761], mean action: 1.688 [0.000, 3.000],  loss: 23.351726, mse: 11060.363520, mean_q: 97.871591, mean_eps: 0.581935
  70273/150000: episode: 685, duration: 0.715s, episode steps:  95, steps per second: 133, episode reward: -66.943, mean reward: -0.705 [-100.000,  9.255], mean action: 1.516 [0.000, 3.000],  loss: 17.717300, mse: 10742.610578, mean_q: 95.994851, mean_eps: 0.578650
  70338/150000: episode: 686, duration: 0.571s, episode steps:  65, steps per second: 114, episode reward: -104.300, mean reward: -1.605 [-100.000, 14.823], mean action: 1.754 [0.000, 3.000],  loss: 23.140137, mse: 10986.363371, mean_q: 97.477907, mean_eps: 0.578170
  70418/150000: episode: 687, duration: 0.590s, episode steps:  80, steps per second: 136, episode reward: -55.169, mean reward: -0.690 [-100.000, 11.189], mean action: 1.613 [0.000, 3.000],  loss: 16.528081, mse: 10737.164868, mean_q: 95.278618, mean_eps: 0.577735
  70510/150000: episode: 688, duration: 0.727s, episode steps:  92, steps per second: 127, episode reward:  5.934, mean reward:  0.064 [-100.000, 11.275], mean action: 1.598 [0.000, 3.000],  loss: 17.731417, mse: 10810.553759, mean_q: 97.846533, mean_eps: 0.577219
  70608/150000: episode: 689, duration: 0.835s, episode steps:  98, steps per second: 117, episode reward: -36.892, mean reward: -0.376 [-100.000, 13.788], mean action: 1.714 [0.000, 3.000],  loss: 23.712698, mse: 10860.053611, mean_q: 95.292542, mean_eps: 0.576649
  70690/150000: episode: 690, duration: 0.586s, episode steps:  82, steps per second: 140, episode reward: -111.671, mean reward: -1.362 [-100.000, 15.785], mean action: 1.683 [0.000, 3.000],  loss: 20.728324, mse: 10631.340642, mean_q: 94.324271, mean_eps: 0.576109
  70779/150000: episode: 691, duration: 0.597s, episode steps:  89, steps per second: 149, episode reward: -59.285, mean reward: -0.666 [-100.000,  8.640], mean action: 1.640 [0.000, 3.000],  loss: 22.111691, mse: 10813.202269, mean_q: 96.066047, mean_eps: 0.575596
  70870/150000: episode: 692, duration: 0.640s, episode steps:  91, steps per second: 142, episode reward: -17.818, mean reward: -0.196 [-100.000, 24.882], mean action: 1.703 [0.000, 3.000],  loss: 18.978094, mse: 11027.197502, mean_q: 98.466198, mean_eps: 0.575056
  70946/150000: episode: 693, duration: 0.524s, episode steps:  76, steps per second: 145, episode reward: -73.491, mean reward: -0.967 [-100.000,  9.078], mean action: 1.684 [0.000, 3.000],  loss: 22.117983, mse: 10928.826712, mean_q: 97.511796, mean_eps: 0.574555
  71037/150000: episode: 694, duration: 0.613s, episode steps:  91, steps per second: 148, episode reward: 15.076, mean reward:  0.166 [-100.000, 16.069], mean action: 1.714 [0.000, 3.000],  loss: 22.711941, mse: 10754.574031, mean_q: 95.930748, mean_eps: 0.574054
  71127/150000: episode: 695, duration: 0.609s, episode steps:  90, steps per second: 148, episode reward: -57.271, mean reward: -0.636 [-100.000, 12.348], mean action: 1.733 [0.000, 3.000],  loss: 18.731455, mse: 10965.754199, mean_q: 97.660617, mean_eps: 0.573511
  71208/150000: episode: 696, duration: 0.563s, episode steps:  81, steps per second: 144, episode reward: -77.338, mean reward: -0.955 [-100.000, 11.393], mean action: 1.296 [0.000, 3.000],  loss: 28.917370, mse: 11392.142928, mean_q: 101.527197, mean_eps: 0.572998
  71347/150000: episode: 697, duration: 0.929s, episode steps: 139, steps per second: 150, episode reward: -166.181, mean reward: -1.196 [-100.000, 24.234], mean action: 1.482 [0.000, 3.000],  loss: 22.407762, mse: 11374.904768, mean_q: 100.804141, mean_eps: 0.572338
  71469/150000: episode: 698, duration: 0.835s, episode steps: 122, steps per second: 146, episode reward: -25.311, mean reward: -0.207 [-100.000, 20.582], mean action: 1.508 [0.000, 3.000],  loss: 23.324300, mse: 11140.283980, mean_q: 97.972479, mean_eps: 0.571555
  71575/150000: episode: 699, duration: 0.729s, episode steps: 106, steps per second: 145, episode reward: -54.670, mean reward: -0.516 [-100.000, 11.210], mean action: 1.585 [0.000, 3.000],  loss: 19.141866, mse: 10999.465756, mean_q: 96.385413, mean_eps: 0.570871
  71684/150000: episode: 700, duration: 0.723s, episode steps: 109, steps per second: 151, episode reward: -79.454, mean reward: -0.729 [-100.000, 21.314], mean action: 1.505 [0.000, 3.000],  loss: 21.112347, mse: 11169.361794, mean_q: 98.046106, mean_eps: 0.570226
  71810/150000: episode: 701, duration: 0.869s, episode steps: 126, steps per second: 145, episode reward: -26.869, mean reward: -0.213 [-100.000, 11.483], mean action: 1.762 [0.000, 3.000],  loss: 19.662821, mse: 11063.855407, mean_q: 99.149363, mean_eps: 0.569521
  71932/150000: episode: 702, duration: 0.833s, episode steps: 122, steps per second: 146, episode reward: -55.126, mean reward: -0.452 [-100.000, 19.544], mean action: 1.738 [0.000, 3.000],  loss: 19.504377, mse: 11132.679631, mean_q: 99.128906, mean_eps: 0.568777
  72053/150000: episode: 703, duration: 0.813s, episode steps: 121, steps per second: 149, episode reward: -81.307, mean reward: -0.672 [-100.000,  8.339], mean action: 1.686 [0.000, 3.000],  loss: 22.483783, mse: 11046.304470, mean_q: 97.733167, mean_eps: 0.568048
  72222/150000: episode: 704, duration: 1.160s, episode steps: 169, steps per second: 146, episode reward: 28.892, mean reward:  0.171 [-100.000, 45.783], mean action: 1.710 [0.000, 3.000],  loss: 20.942457, mse: 10921.339526, mean_q: 97.441606, mean_eps: 0.567178
  72372/150000: episode: 705, duration: 1.003s, episode steps: 150, steps per second: 150, episode reward: -90.437, mean reward: -0.603 [-100.000, 12.822], mean action: 1.500 [0.000, 3.000],  loss: 25.406752, mse: 11054.956296, mean_q: 97.613544, mean_eps: 0.566221
  72472/150000: episode: 706, duration: 0.705s, episode steps: 100, steps per second: 142, episode reward: -70.425, mean reward: -0.704 [-100.000,  6.872], mean action: 1.610 [0.000, 3.000],  loss: 25.674728, mse: 11120.046875, mean_q: 98.873891, mean_eps: 0.565471
  72607/150000: episode: 707, duration: 0.908s, episode steps: 135, steps per second: 149, episode reward: -140.761, mean reward: -1.043 [-100.000,  8.027], mean action: 1.704 [0.000, 3.000],  loss: 17.961501, mse: 10981.639855, mean_q: 97.477463, mean_eps: 0.564766
  72709/150000: episode: 708, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -26.440, mean reward: -0.259 [-100.000, 15.542], mean action: 1.784 [0.000, 3.000],  loss: 28.372471, mse: 10853.083151, mean_q: 97.141171, mean_eps: 0.564055
  72807/150000: episode: 709, duration: 0.678s, episode steps:  98, steps per second: 145, episode reward: -135.726, mean reward: -1.385 [-100.000, 16.783], mean action: 1.735 [0.000, 3.000],  loss: 22.769246, mse: 10949.432508, mean_q: 97.908462, mean_eps: 0.563455
  72925/150000: episode: 710, duration: 0.799s, episode steps: 118, steps per second: 148, episode reward: -31.336, mean reward: -0.266 [-100.000, 13.162], mean action: 1.712 [0.000, 3.000],  loss: 18.978930, mse: 10634.570660, mean_q: 95.052397, mean_eps: 0.562807
  73047/150000: episode: 711, duration: 0.821s, episode steps: 122, steps per second: 149, episode reward: -52.270, mean reward: -0.428 [-100.000, 10.216], mean action: 1.615 [0.000, 3.000],  loss: 19.296907, mse: 10625.731918, mean_q: 96.549236, mean_eps: 0.562087
  73165/150000: episode: 712, duration: 0.834s, episode steps: 118, steps per second: 141, episode reward: -78.141, mean reward: -0.662 [-100.000, 14.380], mean action: 1.424 [0.000, 3.000],  loss: 17.523822, mse: 10711.041736, mean_q: 97.002230, mean_eps: 0.561367
  73305/150000: episode: 713, duration: 1.073s, episode steps: 140, steps per second: 130, episode reward: -120.678, mean reward: -0.862 [-100.000, 21.537], mean action: 1.821 [0.000, 3.000],  loss: 18.569170, mse: 10837.787946, mean_q: 98.241208, mean_eps: 0.560593
  73414/150000: episode: 714, duration: 0.963s, episode steps: 109, steps per second: 113, episode reward: -104.969, mean reward: -0.963 [-100.000,  8.993], mean action: 1.679 [0.000, 3.000],  loss: 23.823383, mse: 10711.271368, mean_q: 97.609291, mean_eps: 0.559846
  73534/150000: episode: 715, duration: 1.001s, episode steps: 120, steps per second: 120, episode reward: -63.274, mean reward: -0.527 [-100.000, 12.646], mean action: 1.792 [0.000, 3.000],  loss: 19.036612, mse: 10776.467692, mean_q: 96.153859, mean_eps: 0.559159
  73656/150000: episode: 716, duration: 0.953s, episode steps: 122, steps per second: 128, episode reward: -55.284, mean reward: -0.453 [-100.000, 21.657], mean action: 1.648 [0.000, 3.000],  loss: 24.000689, mse: 10652.534860, mean_q: 95.357657, mean_eps: 0.558433
  73732/150000: episode: 717, duration: 0.579s, episode steps:  76, steps per second: 131, episode reward: -45.145, mean reward: -0.594 [-100.000,  8.041], mean action: 1.684 [0.000, 3.000],  loss: 25.682859, mse: 10752.654772, mean_q: 96.780224, mean_eps: 0.557839
  73835/150000: episode: 718, duration: 0.766s, episode steps: 103, steps per second: 135, episode reward: 16.731, mean reward:  0.162 [-100.000, 12.871], mean action: 1.748 [0.000, 3.000],  loss: 19.645053, mse: 10735.033450, mean_q: 95.074383, mean_eps: 0.557302
  73930/150000: episode: 719, duration: 0.745s, episode steps:  95, steps per second: 128, episode reward: -49.644, mean reward: -0.523 [-100.000, 20.024], mean action: 1.653 [0.000, 3.000],  loss: 37.093376, mse: 10682.084149, mean_q: 95.156891, mean_eps: 0.556708
  74047/150000: episode: 720, duration: 0.942s, episode steps: 117, steps per second: 124, episode reward: -66.779, mean reward: -0.571 [-100.000, 11.235], mean action: 1.650 [0.000, 3.000],  loss: 27.042335, mse: 10544.296241, mean_q: 94.598598, mean_eps: 0.556072
  74155/150000: episode: 721, duration: 0.969s, episode steps: 108, steps per second: 111, episode reward: -70.353, mean reward: -0.651 [-100.000,  8.924], mean action: 1.796 [0.000, 3.000],  loss: 22.815176, mse: 10465.952944, mean_q: 93.211696, mean_eps: 0.555397
  74301/150000: episode: 722, duration: 1.024s, episode steps: 146, steps per second: 143, episode reward: -0.114, mean reward: -0.001 [-100.000,  9.802], mean action: 1.842 [0.000, 3.000],  loss: 26.388696, mse: 10610.201543, mean_q: 95.565248, mean_eps: 0.554635
  74416/150000: episode: 723, duration: 0.822s, episode steps: 115, steps per second: 140, episode reward: -87.212, mean reward: -0.758 [-100.000, 15.491], mean action: 1.539 [0.000, 3.000],  loss: 19.515835, mse: 10677.079331, mean_q: 93.685432, mean_eps: 0.553852
  74544/150000: episode: 724, duration: 0.920s, episode steps: 128, steps per second: 139, episode reward: -0.262, mean reward: -0.002 [-100.000, 10.279], mean action: 1.664 [0.000, 3.000],  loss: 22.244488, mse: 10815.183479, mean_q: 96.108387, mean_eps: 0.553123
  74659/150000: episode: 725, duration: 0.768s, episode steps: 115, steps per second: 150, episode reward: -78.888, mean reward: -0.686 [-100.000,  6.232], mean action: 1.617 [0.000, 3.000],  loss: 25.640515, mse: 10922.794565, mean_q: 96.385340, mean_eps: 0.552394
  74820/150000: episode: 726, duration: 1.158s, episode steps: 161, steps per second: 139, episode reward: 10.902, mean reward:  0.068 [-100.000, 11.296], mean action: 1.758 [0.000, 3.000],  loss: 26.683736, mse: 10715.859296, mean_q: 96.204765, mean_eps: 0.551566
  74971/150000: episode: 727, duration: 1.024s, episode steps: 151, steps per second: 148, episode reward: -84.784, mean reward: -0.561 [-100.000,  7.514], mean action: 1.682 [0.000, 3.000],  loss: 19.119923, mse: 10716.720600, mean_q: 96.612823, mean_eps: 0.550630
  75076/150000: episode: 728, duration: 0.724s, episode steps: 105, steps per second: 145, episode reward:  8.711, mean reward:  0.083 [-100.000, 16.650], mean action: 1.771 [0.000, 3.000],  loss: 30.175024, mse: 10653.158640, mean_q: 94.792132, mean_eps: 0.549862
  75192/150000: episode: 729, duration: 0.826s, episode steps: 116, steps per second: 140, episode reward: -76.796, mean reward: -0.662 [-100.000, 10.275], mean action: 1.534 [0.000, 3.000],  loss: 28.334483, mse: 10500.422405, mean_q: 94.271607, mean_eps: 0.549199
  75304/150000: episode: 730, duration: 0.769s, episode steps: 112, steps per second: 146, episode reward: -51.685, mean reward: -0.461 [-100.000, 17.294], mean action: 1.759 [0.000, 3.000],  loss: 19.386593, mse: 10519.297934, mean_q: 95.044829, mean_eps: 0.548515
  75445/150000: episode: 731, duration: 0.988s, episode steps: 141, steps per second: 143, episode reward:  3.704, mean reward:  0.026 [-100.000, 12.427], mean action: 1.610 [0.000, 3.000],  loss: 26.002212, mse: 10690.232869, mean_q: 94.497764, mean_eps: 0.547756
  75555/150000: episode: 732, duration: 0.736s, episode steps: 110, steps per second: 149, episode reward: -61.508, mean reward: -0.559 [-100.000, 10.456], mean action: 1.618 [0.000, 3.000],  loss: 28.144834, mse: 10698.745694, mean_q: 96.102812, mean_eps: 0.547003
  76555/150000: episode: 733, duration: 9.234s, episode steps: 1000, steps per second: 108, episode reward: 112.115, mean reward:  0.112 [-23.916, 28.108], mean action: 1.198 [0.000, 3.000],  loss: 24.502354, mse: 11099.181137, mean_q: 98.301941, mean_eps: 0.543673
  76627/150000: episode: 734, duration: 0.577s, episode steps:  72, steps per second: 125, episode reward: -36.024, mean reward: -0.500 [-100.000, 11.923], mean action: 1.361 [0.000, 3.000],  loss: 34.448810, mse: 10937.749498, mean_q: 98.327438, mean_eps: 0.540457
  76740/150000: episode: 735, duration: 0.840s, episode steps: 113, steps per second: 134, episode reward: -44.489, mean reward: -0.394 [-100.000,  8.197], mean action: 1.584 [0.000, 3.000],  loss: 37.954270, mse: 10854.691933, mean_q: 98.217079, mean_eps: 0.539902
  76846/150000: episode: 736, duration: 0.813s, episode steps: 106, steps per second: 130, episode reward: -46.936, mean reward: -0.443 [-100.000, 10.793], mean action: 1.651 [0.000, 3.000],  loss: 31.768969, mse: 11028.535852, mean_q: 99.646602, mean_eps: 0.539245
  77020/150000: episode: 737, duration: 1.280s, episode steps: 174, steps per second: 136, episode reward: -268.991, mean reward: -1.546 [-100.000,  8.901], mean action: 1.667 [0.000, 3.000],  loss: 25.712446, mse: 11027.149571, mean_q: 98.640112, mean_eps: 0.538405
  77101/150000: episode: 738, duration: 0.566s, episode steps:  81, steps per second: 143, episode reward: -65.266, mean reward: -0.806 [-100.000, 16.942], mean action: 1.519 [0.000, 3.000],  loss: 27.893966, mse: 11073.545971, mean_q: 99.408221, mean_eps: 0.537640
  77202/150000: episode: 739, duration: 0.688s, episode steps: 101, steps per second: 147, episode reward: -104.945, mean reward: -1.039 [-100.000, 30.302], mean action: 1.842 [0.000, 3.000],  loss: 23.510398, mse: 11143.091236, mean_q: 99.555966, mean_eps: 0.537094
  77300/150000: episode: 740, duration: 0.671s, episode steps:  98, steps per second: 146, episode reward: -8.779, mean reward: -0.090 [-100.000, 15.189], mean action: 1.633 [0.000, 3.000],  loss: 23.740435, mse: 11114.046765, mean_q: 100.480476, mean_eps: 0.536497
  78300/150000: episode: 741, duration: 7.707s, episode steps: 1000, steps per second: 130, episode reward: -141.342, mean reward: -0.141 [-22.996, 22.735], mean action: 1.507 [0.000, 3.000],  loss: 29.681131, mse: 11677.343188, mean_q: 103.937960, mean_eps: 0.533203
  78388/150000: episode: 742, duration: 0.699s, episode steps:  88, steps per second: 126, episode reward: -47.185, mean reward: -0.536 [-100.000, 20.523], mean action: 1.591 [0.000, 3.000],  loss: 38.758714, mse: 11802.887873, mean_q: 104.718199, mean_eps: 0.529939
  78487/150000: episode: 743, duration: 0.730s, episode steps:  99, steps per second: 136, episode reward: 19.930, mean reward:  0.201 [-100.000, 20.802], mean action: 1.687 [0.000, 3.000],  loss: 26.609283, mse: 11942.584349, mean_q: 106.276258, mean_eps: 0.529378
  78570/150000: episode: 744, duration: 0.592s, episode steps:  83, steps per second: 140, episode reward: -58.603, mean reward: -0.706 [-100.000, 15.792], mean action: 1.614 [0.000, 3.000],  loss: 28.965658, mse: 11990.898085, mean_q: 104.903112, mean_eps: 0.528832
  78653/150000: episode: 745, duration: 0.631s, episode steps:  83, steps per second: 131, episode reward:  4.333, mean reward:  0.052 [-100.000, 21.838], mean action: 1.759 [0.000, 3.000],  loss: 29.681484, mse: 12240.759883, mean_q: 107.019298, mean_eps: 0.528334
  78785/150000: episode: 746, duration: 1.018s, episode steps: 132, steps per second: 130, episode reward: -61.995, mean reward: -0.470 [-100.000,  8.319], mean action: 1.833 [0.000, 3.000],  loss: 32.397399, mse: 12023.583947, mean_q: 105.637215, mean_eps: 0.527689
  78900/150000: episode: 747, duration: 0.790s, episode steps: 115, steps per second: 146, episode reward: -86.289, mean reward: -0.750 [-100.000,  8.594], mean action: 1.539 [0.000, 3.000],  loss: 41.022715, mse: 11882.268359, mean_q: 105.256261, mean_eps: 0.526948
  79017/150000: episode: 748, duration: 0.839s, episode steps: 117, steps per second: 139, episode reward: -48.320, mean reward: -0.413 [-100.000, 18.733], mean action: 1.427 [0.000, 3.000],  loss: 24.349608, mse: 12166.313819, mean_q: 107.124704, mean_eps: 0.526252
  79142/150000: episode: 749, duration: 0.877s, episode steps: 125, steps per second: 142, episode reward: -1.635, mean reward: -0.013 [-100.000, 19.700], mean action: 1.784 [0.000, 3.000],  loss: 21.914302, mse: 12209.437359, mean_q: 105.911703, mean_eps: 0.525526
  79202/150000: episode: 750, duration: 0.491s, episode steps:  60, steps per second: 122, episode reward: -46.110, mean reward: -0.768 [-100.000, 20.196], mean action: 1.450 [0.000, 3.000],  loss: 29.279131, mse: 11983.060905, mean_q: 104.892522, mean_eps: 0.524971
  79288/150000: episode: 751, duration: 0.728s, episode steps:  86, steps per second: 118, episode reward: -150.083, mean reward: -1.745 [-100.000, 45.152], mean action: 1.616 [0.000, 3.000],  loss: 27.388279, mse: 12494.028195, mean_q: 107.943174, mean_eps: 0.524533
  79370/150000: episode: 752, duration: 0.652s, episode steps:  82, steps per second: 126, episode reward: -52.704, mean reward: -0.643 [-100.000, 14.846], mean action: 1.622 [0.000, 3.000],  loss: 38.068688, mse: 12258.529380, mean_q: 106.370128, mean_eps: 0.524029
  79455/150000: episode: 753, duration: 0.658s, episode steps:  85, steps per second: 129, episode reward: -57.906, mean reward: -0.681 [-100.000,  8.611], mean action: 1.671 [0.000, 3.000],  loss: 24.681411, mse: 12284.411696, mean_q: 105.600128, mean_eps: 0.523528
  79579/150000: episode: 754, duration: 0.920s, episode steps: 124, steps per second: 135, episode reward: -85.964, mean reward: -0.693 [-100.000, 12.701], mean action: 1.710 [0.000, 3.000],  loss: 30.335035, mse: 12511.135695, mean_q: 106.756503, mean_eps: 0.522901
  79692/150000: episode: 755, duration: 0.766s, episode steps: 113, steps per second: 148, episode reward: -63.209, mean reward: -0.559 [-100.000,  7.042], mean action: 1.770 [0.000, 3.000],  loss: 25.641623, mse: 12318.266204, mean_q: 105.960842, mean_eps: 0.522190
  79807/150000: episode: 756, duration: 0.838s, episode steps: 115, steps per second: 137, episode reward: 40.428, mean reward:  0.352 [-100.000, 20.984], mean action: 1.609 [0.000, 3.000],  loss: 33.168030, mse: 12208.744090, mean_q: 105.815846, mean_eps: 0.521506
  79904/150000: episode: 757, duration: 0.688s, episode steps:  97, steps per second: 141, episode reward: -14.952, mean reward: -0.154 [-100.000, 12.706], mean action: 1.629 [0.000, 3.000],  loss: 31.802109, mse: 12143.311997, mean_q: 105.775412, mean_eps: 0.520870
  80904/150000: episode: 758, duration: 8.247s, episode steps: 1000, steps per second: 121, episode reward: 84.400, mean reward:  0.084 [-24.124, 23.772], mean action: 1.648 [0.000, 3.000],  loss: 30.600008, mse: 12472.380985, mean_q: 107.878910, mean_eps: 0.517579
  81055/150000: episode: 759, duration: 1.064s, episode steps: 151, steps per second: 142, episode reward: -243.762, mean reward: -1.614 [-100.000, 25.000], mean action: 1.868 [0.000, 3.000],  loss: 28.040691, mse: 12783.566380, mean_q: 110.009970, mean_eps: 0.514126
  81508/150000: episode: 760, duration: 3.292s, episode steps: 453, steps per second: 138, episode reward: -231.836, mean reward: -0.512 [-100.000, 30.861], mean action: 1.740 [0.000, 3.000],  loss: 29.720909, mse: 13097.124338, mean_q: 110.654525, mean_eps: 0.512314
  81624/150000: episode: 761, duration: 0.899s, episode steps: 116, steps per second: 129, episode reward: -127.884, mean reward: -1.102 [-100.000, 15.170], mean action: 1.612 [0.000, 3.000],  loss: 27.232984, mse: 12889.208488, mean_q: 109.085415, mean_eps: 0.510607
  81723/150000: episode: 762, duration: 0.849s, episode steps:  99, steps per second: 117, episode reward: -167.180, mean reward: -1.689 [-100.000, 11.391], mean action: 1.838 [0.000, 3.000],  loss: 43.040763, mse: 13078.024266, mean_q: 110.499044, mean_eps: 0.509962
  81862/150000: episode: 763, duration: 1.253s, episode steps: 139, steps per second: 111, episode reward: -36.960, mean reward: -0.266 [-100.000, 21.163], mean action: 1.640 [0.000, 3.000],  loss: 43.227783, mse: 12740.021217, mean_q: 107.379558, mean_eps: 0.509248
  81964/150000: episode: 764, duration: 0.926s, episode steps: 102, steps per second: 110, episode reward: -9.084, mean reward: -0.089 [-100.000, 12.210], mean action: 1.765 [0.000, 3.000],  loss: 33.168147, mse: 12681.883234, mean_q: 106.769474, mean_eps: 0.508525
  82051/150000: episode: 765, duration: 0.774s, episode steps:  87, steps per second: 112, episode reward: -159.002, mean reward: -1.828 [-100.000, 13.765], mean action: 1.264 [0.000, 3.000],  loss: 56.574011, mse: 13038.543968, mean_q: 109.803668, mean_eps: 0.507958
  82205/150000: episode: 766, duration: 1.362s, episode steps: 154, steps per second: 113, episode reward: -4.637, mean reward: -0.030 [-100.000, 15.043], mean action: 1.695 [0.000, 3.000],  loss: 28.449598, mse: 13096.274300, mean_q: 108.830788, mean_eps: 0.507235
  83205/150000: episode: 767, duration: 7.485s, episode steps: 1000, steps per second: 134, episode reward: 36.988, mean reward:  0.037 [-24.575, 24.926], mean action: 1.338 [0.000, 3.000],  loss: 35.434477, mse: 13812.385041, mean_q: 114.222274, mean_eps: 0.503773
  83304/150000: episode: 768, duration: 0.671s, episode steps:  99, steps per second: 148, episode reward: -75.285, mean reward: -0.760 [-100.000,  7.753], mean action: 1.737 [0.000, 3.000],  loss: 26.629700, mse: 14269.457623, mean_q: 115.432354, mean_eps: 0.500476
  83416/150000: episode: 769, duration: 0.773s, episode steps: 112, steps per second: 145, episode reward: -27.852, mean reward: -0.249 [-100.000, 18.931], mean action: 1.509 [0.000, 3.000],  loss: 34.560806, mse: 14655.728551, mean_q: 118.122372, mean_eps: 0.499843
  83538/150000: episode: 770, duration: 1.164s, episode steps: 122, steps per second: 105, episode reward: -65.761, mean reward: -0.539 [-100.000, 10.005], mean action: 1.631 [0.000, 3.000],  loss: 34.155108, mse: 14878.084841, mean_q: 119.925257, mean_eps: 0.499141
  83612/150000: episode: 771, duration: 0.639s, episode steps:  74, steps per second: 116, episode reward: -53.683, mean reward: -0.725 [-100.000,  6.303], mean action: 1.878 [0.000, 3.000],  loss: 24.007915, mse: 15036.329709, mean_q: 120.502561, mean_eps: 0.498553
  83736/150000: episode: 772, duration: 0.948s, episode steps: 124, steps per second: 131, episode reward: 10.035, mean reward:  0.081 [-100.000, 18.624], mean action: 1.702 [0.000, 3.000],  loss: 32.591645, mse: 14946.788929, mean_q: 118.463929, mean_eps: 0.497959
  83962/150000: episode: 773, duration: 1.671s, episode steps: 226, steps per second: 135, episode reward: -410.258, mean reward: -1.815 [-100.000, 17.640], mean action: 1.717 [0.000, 3.000],  loss: 32.543026, mse: 15082.393231, mean_q: 120.425474, mean_eps: 0.496909
  84071/150000: episode: 774, duration: 0.814s, episode steps: 109, steps per second: 134, episode reward: -69.260, mean reward: -0.635 [-100.000,  8.793], mean action: 1.661 [0.000, 3.000],  loss: 28.222730, mse: 15316.522255, mean_q: 121.604199, mean_eps: 0.495904
  84188/150000: episode: 775, duration: 0.883s, episode steps: 117, steps per second: 132, episode reward: -43.521, mean reward: -0.372 [-100.000, 10.311], mean action: 1.598 [0.000, 3.000],  loss: 28.369615, mse: 15435.909981, mean_q: 121.606802, mean_eps: 0.495226
  84283/150000: episode: 776, duration: 0.758s, episode steps:  95, steps per second: 125, episode reward: -24.666, mean reward: -0.260 [-100.000, 12.226], mean action: 1.632 [0.000, 3.000],  loss: 34.863354, mse: 15466.232442, mean_q: 122.789881, mean_eps: 0.494590
  84400/150000: episode: 777, duration: 0.814s, episode steps: 117, steps per second: 144, episode reward: -81.268, mean reward: -0.695 [-100.000,  8.186], mean action: 1.684 [0.000, 3.000],  loss: 31.823599, mse: 15377.735660, mean_q: 120.732760, mean_eps: 0.493954
  84519/150000: episode: 778, duration: 1.146s, episode steps: 119, steps per second: 104, episode reward: -44.593, mean reward: -0.375 [-100.000,  9.895], mean action: 1.529 [0.000, 3.000],  loss: 28.136812, mse: 15382.619830, mean_q: 121.403171, mean_eps: 0.493246
  84599/150000: episode: 779, duration: 0.743s, episode steps:  80, steps per second: 108, episode reward: -49.614, mean reward: -0.620 [-100.000, 12.333], mean action: 1.613 [0.000, 3.000],  loss: 39.295029, mse: 15414.168359, mean_q: 122.210616, mean_eps: 0.492649
  84672/150000: episode: 780, duration: 0.853s, episode steps:  73, steps per second:  86, episode reward: 21.941, mean reward:  0.301 [-100.000, 19.014], mean action: 1.932 [0.000, 3.000],  loss: 24.466622, mse: 15287.483492, mean_q: 120.749407, mean_eps: 0.492190
  84771/150000: episode: 781, duration: 0.988s, episode steps:  99, steps per second: 100, episode reward: -176.319, mean reward: -1.781 [-100.000, 28.847], mean action: 1.697 [0.000, 3.000],  loss: 29.523414, mse: 15165.097923, mean_q: 119.872862, mean_eps: 0.491674
  84861/150000: episode: 782, duration: 0.925s, episode steps:  90, steps per second:  97, episode reward: -15.480, mean reward: -0.172 [-100.000, 17.117], mean action: 1.500 [0.000, 3.000],  loss: 29.828444, mse: 15074.976335, mean_q: 118.952441, mean_eps: 0.491107
  84959/150000: episode: 783, duration: 0.923s, episode steps:  98, steps per second: 106, episode reward: -42.825, mean reward: -0.437 [-100.000, 11.067], mean action: 1.806 [0.000, 3.000],  loss: 33.903407, mse: 14993.708725, mean_q: 119.218117, mean_eps: 0.490543
  85052/150000: episode: 784, duration: 0.847s, episode steps:  93, steps per second: 110, episode reward: 21.864, mean reward:  0.235 [-100.000, 18.793], mean action: 1.624 [0.000, 3.000],  loss: 36.355974, mse: 14807.349231, mean_q: 117.680816, mean_eps: 0.489970
  85196/150000: episode: 785, duration: 1.283s, episode steps: 144, steps per second: 112, episode reward: 43.105, mean reward:  0.299 [-100.000, 15.943], mean action: 1.660 [0.000, 3.000],  loss: 25.281350, mse: 15032.218472, mean_q: 118.388458, mean_eps: 0.489259
  85303/150000: episode: 786, duration: 0.777s, episode steps: 107, steps per second: 138, episode reward: -30.693, mean reward: -0.287 [-100.000, 15.046], mean action: 1.701 [0.000, 3.000],  loss: 26.829623, mse: 14985.534207, mean_q: 119.158328, mean_eps: 0.488506
  85580/150000: episode: 787, duration: 2.046s, episode steps: 277, steps per second: 135, episode reward: -372.081, mean reward: -1.343 [-100.000, 12.296], mean action: 1.560 [0.000, 3.000],  loss: 30.739342, mse: 15412.369846, mean_q: 121.160601, mean_eps: 0.487354
  85688/150000: episode: 788, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: 27.133, mean reward:  0.251 [-100.000, 13.791], mean action: 1.667 [0.000, 3.000],  loss: 26.836257, mse: 15611.819019, mean_q: 122.100146, mean_eps: 0.486199
  86363/150000: episode: 789, duration: 5.836s, episode steps: 675, steps per second: 116, episode reward: -126.138, mean reward: -0.187 [-100.000, 28.381], mean action: 1.770 [0.000, 3.000],  loss: 35.340509, mse: 15606.254751, mean_q: 121.130925, mean_eps: 0.483850
  86477/150000: episode: 790, duration: 1.059s, episode steps: 114, steps per second: 108, episode reward: -231.670, mean reward: -2.032 [-100.000,  1.437], mean action: 1.789 [0.000, 3.000],  loss: 34.856869, mse: 16308.802820, mean_q: 125.799531, mean_eps: 0.481483
  86687/150000: episode: 791, duration: 1.892s, episode steps: 210, steps per second: 111, episode reward: -146.381, mean reward: -0.697 [-100.000,  5.135], mean action: 1.771 [0.000, 3.000],  loss: 32.619812, mse: 16601.494871, mean_q: 127.327989, mean_eps: 0.480511
  87073/150000: episode: 792, duration: 3.295s, episode steps: 386, steps per second: 117, episode reward: -188.533, mean reward: -0.488 [-100.000, 12.755], mean action: 1.658 [0.000, 3.000],  loss: 35.503370, mse: 16874.835060, mean_q: 127.925228, mean_eps: 0.478723
  87154/150000: episode: 793, duration: 0.573s, episode steps:  81, steps per second: 141, episode reward: -47.958, mean reward: -0.592 [-100.000, 11.499], mean action: 1.938 [0.000, 3.000],  loss: 31.427402, mse: 16739.588349, mean_q: 126.848427, mean_eps: 0.477322
  87510/150000: episode: 794, duration: 2.694s, episode steps: 356, steps per second: 132, episode reward: -167.025, mean reward: -0.469 [-100.000, 12.157], mean action: 1.705 [0.000, 3.000],  loss: 32.662229, mse: 17117.537949, mean_q: 128.453262, mean_eps: 0.476011
  87616/150000: episode: 795, duration: 0.839s, episode steps: 106, steps per second: 126, episode reward: -66.444, mean reward: -0.627 [-100.000, 11.109], mean action: 1.594 [0.000, 3.000],  loss: 43.348538, mse: 17250.633328, mean_q: 129.055929, mean_eps: 0.474625
  87721/150000: episode: 796, duration: 0.825s, episode steps: 105, steps per second: 127, episode reward: -34.310, mean reward: -0.327 [-100.000, 11.755], mean action: 1.571 [0.000, 3.000],  loss: 39.026297, mse: 17911.064118, mean_q: 131.639561, mean_eps: 0.473992
  87877/150000: episode: 797, duration: 1.219s, episode steps: 156, steps per second: 128, episode reward: -7.469, mean reward: -0.048 [-100.000, 14.935], mean action: 1.641 [0.000, 3.000],  loss: 31.308290, mse: 18109.453025, mean_q: 133.332857, mean_eps: 0.473209
  87973/150000: episode: 798, duration: 0.686s, episode steps:  96, steps per second: 140, episode reward:  7.631, mean reward:  0.079 [-100.000, 18.372], mean action: 1.688 [0.000, 3.000],  loss: 33.261351, mse: 18026.475413, mean_q: 133.375465, mean_eps: 0.472453
  88055/150000: episode: 799, duration: 0.560s, episode steps:  82, steps per second: 146, episode reward: -39.661, mean reward: -0.484 [-100.000,  7.855], mean action: 1.683 [0.000, 3.000],  loss: 49.564300, mse: 18212.365937, mean_q: 134.136380, mean_eps: 0.471919
  88223/150000: episode: 800, duration: 1.205s, episode steps: 168, steps per second: 139, episode reward: -90.164, mean reward: -0.537 [-100.000, 13.433], mean action: 1.500 [0.000, 3.000],  loss: 40.735188, mse: 18228.947742, mean_q: 134.251229, mean_eps: 0.471169
  88297/150000: episode: 801, duration: 0.549s, episode steps:  74, steps per second: 135, episode reward: -18.032, mean reward: -0.244 [-100.000, 13.780], mean action: 1.554 [0.000, 3.000],  loss: 23.742105, mse: 18633.648701, mean_q: 135.927029, mean_eps: 0.470443
  89279/150000: episode: 802, duration: 7.864s, episode steps: 982, steps per second: 125, episode reward: -190.798, mean reward: -0.194 [-100.000, 26.020], mean action: 1.678 [0.000, 3.000],  loss: 37.711219, mse: 19632.930251, mean_q: 140.449283, mean_eps: 0.467275
  89472/150000: episode: 803, duration: 1.405s, episode steps: 193, steps per second: 137, episode reward: -42.943, mean reward: -0.223 [-100.000, 17.601], mean action: 1.632 [0.000, 3.000],  loss: 27.246026, mse: 21024.732665, mean_q: 146.484537, mean_eps: 0.463750
  89579/150000: episode: 804, duration: 0.764s, episode steps: 107, steps per second: 140, episode reward: -52.444, mean reward: -0.490 [-100.000, 11.003], mean action: 1.757 [0.000, 3.000],  loss: 43.369357, mse: 20982.429605, mean_q: 146.664459, mean_eps: 0.462850
  89671/150000: episode: 805, duration: 0.642s, episode steps:  92, steps per second: 143, episode reward: 12.588, mean reward:  0.137 [-100.000, 17.226], mean action: 1.674 [0.000, 3.000],  loss: 53.437240, mse: 21184.637249, mean_q: 147.912369, mean_eps: 0.462253
  89772/150000: episode: 806, duration: 0.694s, episode steps: 101, steps per second: 146, episode reward: -8.843, mean reward: -0.088 [-100.000, 15.008], mean action: 1.644 [0.000, 3.000],  loss: 38.113781, mse: 21678.201559, mean_q: 150.242435, mean_eps: 0.461674
  89864/150000: episode: 807, duration: 0.808s, episode steps:  92, steps per second: 114, episode reward: -24.657, mean reward: -0.268 [-100.000, 18.309], mean action: 1.793 [0.000, 3.000],  loss: 32.668890, mse: 21833.844217, mean_q: 150.651198, mean_eps: 0.461095
  89955/150000: episode: 808, duration: 0.774s, episode steps:  91, steps per second: 118, episode reward: -20.635, mean reward: -0.227 [-100.000,  8.100], mean action: 1.626 [0.000, 3.000],  loss: 36.801244, mse: 21730.215895, mean_q: 149.519274, mean_eps: 0.460546
  90036/150000: episode: 809, duration: 0.712s, episode steps:  81, steps per second: 114, episode reward: -29.337, mean reward: -0.362 [-100.000, 14.983], mean action: 1.741 [0.000, 3.000],  loss: 44.596872, mse: 21926.122782, mean_q: 149.277949, mean_eps: 0.460030
  90125/150000: episode: 810, duration: 0.828s, episode steps:  89, steps per second: 107, episode reward:  0.550, mean reward:  0.006 [-100.000, 20.599], mean action: 1.573 [0.000, 3.000],  loss: 31.319953, mse: 22175.045866, mean_q: 150.997947, mean_eps: 0.459520
  90206/150000: episode: 811, duration: 0.699s, episode steps:  81, steps per second: 116, episode reward: -19.270, mean reward: -0.238 [-100.000,  9.180], mean action: 1.914 [0.000, 3.000],  loss: 43.131282, mse: 22485.184004, mean_q: 151.354931, mean_eps: 0.459010
  90348/150000: episode: 812, duration: 1.258s, episode steps: 142, steps per second: 113, episode reward: -20.409, mean reward: -0.144 [-100.000, 15.871], mean action: 1.592 [0.000, 3.000],  loss: 32.848170, mse: 22666.638562, mean_q: 151.971976, mean_eps: 0.458341
  90534/150000: episode: 813, duration: 1.482s, episode steps: 186, steps per second: 126, episode reward: -10.314, mean reward: -0.055 [-100.000,  9.105], mean action: 1.688 [0.000, 3.000],  loss: 29.727573, mse: 22862.606288, mean_q: 152.178166, mean_eps: 0.457357
  90663/150000: episode: 814, duration: 0.951s, episode steps: 129, steps per second: 136, episode reward: -38.581, mean reward: -0.299 [-100.000,  8.603], mean action: 1.721 [0.000, 3.000],  loss: 29.580304, mse: 22805.854288, mean_q: 152.708953, mean_eps: 0.456412
  90786/150000: episode: 815, duration: 0.870s, episode steps: 123, steps per second: 141, episode reward: -44.643, mean reward: -0.363 [-100.000, 21.639], mean action: 1.699 [0.000, 3.000],  loss: 36.300107, mse: 23107.565493, mean_q: 152.443118, mean_eps: 0.455656
  90850/150000: episode: 816, duration: 0.481s, episode steps:  64, steps per second: 133, episode reward: -57.701, mean reward: -0.902 [-100.000,  9.172], mean action: 1.719 [0.000, 3.000],  loss: 49.849778, mse: 22792.491852, mean_q: 151.181890, mean_eps: 0.455095
  91850/150000: episode: 817, duration: 7.826s, episode steps: 1000, steps per second: 128, episode reward: 77.471, mean reward:  0.077 [-24.328, 25.014], mean action: 1.403 [0.000, 3.000],  loss: 38.569745, mse: 22671.373038, mean_q: 150.874355, mean_eps: 0.451903
  92106/150000: episode: 818, duration: 1.765s, episode steps: 256, steps per second: 145, episode reward: -26.834, mean reward: -0.105 [-100.000, 41.435], mean action: 1.867 [0.000, 3.000],  loss: 35.317689, mse: 22263.731575, mean_q: 149.288766, mean_eps: 0.448135
  92235/150000: episode: 819, duration: 0.868s, episode steps: 129, steps per second: 149, episode reward: -130.054, mean reward: -1.008 [-100.000, 39.152], mean action: 1.853 [0.000, 3.000],  loss: 45.421433, mse: 22035.811031, mean_q: 148.845779, mean_eps: 0.446980
  92389/150000: episode: 820, duration: 1.062s, episode steps: 154, steps per second: 145, episode reward: -94.759, mean reward: -0.615 [-100.000, 16.928], mean action: 1.571 [0.000, 3.000],  loss: 31.821381, mse: 21954.972682, mean_q: 147.545568, mean_eps: 0.446131
  92473/150000: episode: 821, duration: 0.562s, episode steps:  84, steps per second: 149, episode reward: -57.948, mean reward: -0.690 [-100.000, 10.190], mean action: 1.619 [0.000, 3.000],  loss: 31.655487, mse: 22514.146426, mean_q: 151.194823, mean_eps: 0.445417
  92560/150000: episode: 822, duration: 0.583s, episode steps:  87, steps per second: 149, episode reward: 33.462, mean reward:  0.385 [-100.000, 21.070], mean action: 1.701 [0.000, 3.000],  loss: 46.136555, mse: 22497.748878, mean_q: 150.791582, mean_eps: 0.444904
  92758/150000: episode: 823, duration: 1.342s, episode steps: 198, steps per second: 148, episode reward: -47.732, mean reward: -0.241 [-100.000, 11.175], mean action: 1.702 [0.000, 3.000],  loss: 47.751929, mse: 23005.382211, mean_q: 153.270401, mean_eps: 0.444049
  92875/150000: episode: 824, duration: 0.810s, episode steps: 117, steps per second: 144, episode reward: -44.254, mean reward: -0.378 [-100.000, 14.382], mean action: 1.573 [0.000, 3.000],  loss: 46.753633, mse: 23150.612463, mean_q: 153.723482, mean_eps: 0.443104
  93875/150000: episode: 825, duration: 7.480s, episode steps: 1000, steps per second: 134, episode reward: -46.301, mean reward: -0.046 [-24.360, 24.505], mean action: 1.420 [0.000, 3.000],  loss: 41.489912, mse: 24408.519695, mean_q: 159.072617, mean_eps: 0.439753
  94752/150000: episode: 826, duration: 6.996s, episode steps: 877, steps per second: 125, episode reward: -203.143, mean reward: -0.232 [-100.000, 23.670], mean action: 1.612 [0.000, 3.000],  loss: 41.583313, mse: 25627.773023, mean_q: 164.148014, mean_eps: 0.434122
  94964/150000: episode: 827, duration: 1.549s, episode steps: 212, steps per second: 137, episode reward: -136.414, mean reward: -0.643 [-100.000, 25.449], mean action: 1.627 [0.000, 3.000],  loss: 37.419233, mse: 26262.512778, mean_q: 166.487370, mean_eps: 0.430855
  95366/150000: episode: 828, duration: 3.472s, episode steps: 402, steps per second: 116, episode reward: -295.546, mean reward: -0.735 [-100.000, 17.460], mean action: 1.706 [0.000, 3.000],  loss: 37.914004, mse: 26592.044766, mean_q: 167.811838, mean_eps: 0.429013
  95476/150000: episode: 829, duration: 0.982s, episode steps: 110, steps per second: 112, episode reward: -43.818, mean reward: -0.398 [-100.000, 23.813], mean action: 1.609 [0.000, 3.000],  loss: 48.344924, mse: 27107.263796, mean_q: 168.720683, mean_eps: 0.427477
  95615/150000: episode: 830, duration: 0.967s, episode steps: 139, steps per second: 144, episode reward: -115.385, mean reward: -0.830 [-100.000, 21.448], mean action: 1.842 [0.000, 3.000],  loss: 41.589958, mse: 27186.826453, mean_q: 168.928086, mean_eps: 0.426730
  95737/150000: episode: 831, duration: 1.235s, episode steps: 122, steps per second:  99, episode reward: -77.528, mean reward: -0.635 [-100.000,  9.618], mean action: 1.418 [0.000, 3.000],  loss: 51.817099, mse: 27172.250064, mean_q: 169.113763, mean_eps: 0.425947
  95872/150000: episode: 832, duration: 1.185s, episode steps: 135, steps per second: 114, episode reward: -276.668, mean reward: -2.049 [-100.000, 43.570], mean action: 1.481 [0.000, 3.000],  loss: 39.540384, mse: 26975.754832, mean_q: 168.076742, mean_eps: 0.425176
  95979/150000: episode: 833, duration: 0.998s, episode steps: 107, steps per second: 107, episode reward: -173.283, mean reward: -1.619 [-100.000, 51.419], mean action: 1.645 [0.000, 3.000],  loss: 54.489018, mse: 26959.354866, mean_q: 168.260817, mean_eps: 0.424450
  96979/150000: episode: 834, duration: 9.108s, episode steps: 1000, steps per second: 110, episode reward: 63.270, mean reward:  0.063 [-24.199, 23.080], mean action: 1.346 [0.000, 3.000],  loss: 45.266825, mse: 25722.574201, mean_q: 163.655895, mean_eps: 0.421129
  97145/150000: episode: 835, duration: 1.251s, episode steps: 166, steps per second: 133, episode reward:  0.670, mean reward:  0.004 [-100.000, 18.724], mean action: 1.735 [0.000, 3.000],  loss: 36.907349, mse: 25255.890872, mean_q: 161.288184, mean_eps: 0.417631
  97241/150000: episode: 836, duration: 0.740s, episode steps:  96, steps per second: 130, episode reward: -32.590, mean reward: -0.339 [-100.000, 12.088], mean action: 1.562 [0.000, 3.000],  loss: 44.848781, mse: 25201.129395, mean_q: 161.174419, mean_eps: 0.416845
  97366/150000: episode: 837, duration: 0.890s, episode steps: 125, steps per second: 141, episode reward: -12.682, mean reward: -0.101 [-100.000, 17.148], mean action: 1.784 [0.000, 3.000],  loss: 44.242165, mse: 25494.028125, mean_q: 160.893484, mean_eps: 0.416182
  98366/150000: episode: 838, duration: 8.314s, episode steps: 1000, steps per second: 120, episode reward: 50.083, mean reward:  0.050 [-23.751, 21.896], mean action: 1.576 [0.000, 3.000],  loss: 42.329394, mse: 25468.189191, mean_q: 162.577027, mean_eps: 0.412807
  98476/150000: episode: 839, duration: 0.874s, episode steps: 110, steps per second: 126, episode reward: -19.273, mean reward: -0.175 [-100.000, 23.133], mean action: 1.664 [0.000, 3.000],  loss: 34.415024, mse: 24989.572496, mean_q: 160.581412, mean_eps: 0.409477
  99476/150000: episode: 840, duration: 7.996s, episode steps: 1000, steps per second: 125, episode reward: -27.758, mean reward: -0.028 [-22.987, 22.546], mean action: 1.503 [0.000, 3.000],  loss: 39.856778, mse: 24313.945555, mean_q: 157.457801, mean_eps: 0.406147
  99554/150000: episode: 841, duration: 0.544s, episode steps:  78, steps per second: 143, episode reward: 18.464, mean reward:  0.237 [-100.000, 17.191], mean action: 1.756 [0.000, 3.000],  loss: 40.927833, mse: 23853.160281, mean_q: 157.114191, mean_eps: 0.402913
  99687/150000: episode: 842, duration: 1.031s, episode steps: 133, steps per second: 129, episode reward:  2.953, mean reward:  0.022 [-100.000, 14.844], mean action: 1.579 [0.000, 3.000],  loss: 38.764280, mse: 23752.921082, mean_q: 155.211281, mean_eps: 0.402280
 100687/150000: episode: 843, duration: 7.874s, episode steps: 1000, steps per second: 127, episode reward: 56.085, mean reward:  0.056 [-23.242, 26.070], mean action: 1.387 [0.000, 3.000],  loss: 33.272563, mse: 24545.119928, mean_q: 158.975752, mean_eps: 0.398881
 100859/150000: episode: 844, duration: 1.259s, episode steps: 172, steps per second: 137, episode reward: -93.334, mean reward: -0.543 [-100.000,  4.373], mean action: 1.686 [0.000, 3.000],  loss: 46.874507, mse: 23577.300872, mean_q: 154.884543, mean_eps: 0.395365
 100953/150000: episode: 845, duration: 0.686s, episode steps:  94, steps per second: 137, episode reward: -70.718, mean reward: -0.752 [-100.000,  9.022], mean action: 1.830 [0.000, 3.000],  loss: 32.670915, mse: 24121.006275, mean_q: 156.710866, mean_eps: 0.394567
 101050/150000: episode: 846, duration: 0.724s, episode steps:  97, steps per second: 134, episode reward:  7.291, mean reward:  0.075 [-100.000, 19.723], mean action: 1.557 [0.000, 3.000],  loss: 41.308320, mse: 23636.415271, mean_q: 153.793438, mean_eps: 0.393994
 102050/150000: episode: 847, duration: 8.171s, episode steps: 1000, steps per second: 122, episode reward: -24.384, mean reward: -0.024 [-23.065, 24.412], mean action: 1.627 [0.000, 3.000],  loss: 38.005013, mse: 23876.011018, mean_q: 156.511224, mean_eps: 0.390703
 102154/150000: episode: 848, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -8.969, mean reward: -0.086 [-100.000, 17.557], mean action: 1.663 [0.000, 3.000],  loss: 30.917274, mse: 24040.718900, mean_q: 157.436556, mean_eps: 0.387391
 102450/150000: episode: 849, duration: 2.373s, episode steps: 296, steps per second: 125, episode reward: -205.840, mean reward: -0.695 [-100.000, 13.010], mean action: 1.672 [0.000, 3.000],  loss: 30.546537, mse: 23151.826271, mean_q: 153.519787, mean_eps: 0.386191
 103450/150000: episode: 850, duration: 7.712s, episode steps: 1000, steps per second: 130, episode reward: 129.093, mean reward:  0.129 [-22.296, 23.169], mean action: 1.414 [0.000, 3.000],  loss: 32.958664, mse: 21931.959729, mean_q: 147.841281, mean_eps: 0.382303
 103549/150000: episode: 851, duration: 0.673s, episode steps:  99, steps per second: 147, episode reward: 22.418, mean reward:  0.226 [-100.000, 15.512], mean action: 1.747 [0.000, 3.000],  loss: 33.573063, mse: 22133.754153, mean_q: 149.419339, mean_eps: 0.379006
 103652/150000: episode: 852, duration: 0.700s, episode steps: 103, steps per second: 147, episode reward: -25.151, mean reward: -0.244 [-100.000, 17.445], mean action: 1.728 [0.000, 3.000],  loss: 33.184737, mse: 22120.350766, mean_q: 149.799435, mean_eps: 0.378400
 104558/150000: episode: 853, duration: 6.853s, episode steps: 906, steps per second: 132, episode reward: -235.733, mean reward: -0.260 [-100.000, 51.633], mean action: 1.523 [0.000, 3.000],  loss: 34.078694, mse: 21167.416840, mean_q: 147.050887, mean_eps: 0.375373
 105558/150000: episode: 854, duration: 7.668s, episode steps: 1000, steps per second: 130, episode reward: -93.474, mean reward: -0.093 [-19.213, 22.938], mean action: 1.617 [0.000, 3.000],  loss: 35.604474, mse: 20102.121035, mean_q: 142.813826, mean_eps: 0.369655
 105630/150000: episode: 855, duration: 0.500s, episode steps:  72, steps per second: 144, episode reward: -50.573, mean reward: -0.702 [-100.000,  5.815], mean action: 1.681 [0.000, 3.000],  loss: 25.810261, mse: 19394.594510, mean_q: 139.560053, mean_eps: 0.366439
 105733/150000: episode: 856, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -2.485, mean reward: -0.024 [-100.000, 15.966], mean action: 1.728 [0.000, 3.000],  loss: 33.738515, mse: 18864.874251, mean_q: 137.986525, mean_eps: 0.365914
 105854/150000: episode: 857, duration: 0.835s, episode steps: 121, steps per second: 145, episode reward: -11.872, mean reward: -0.098 [-100.000,  8.089], mean action: 1.736 [0.000, 3.000],  loss: 28.807098, mse: 19290.391787, mean_q: 139.158876, mean_eps: 0.365242
 105945/150000: episode: 858, duration: 0.628s, episode steps:  91, steps per second: 145, episode reward: 16.133, mean reward:  0.177 [-100.000, 18.079], mean action: 1.802 [0.000, 3.000],  loss: 24.189162, mse: 19064.195152, mean_q: 139.259057, mean_eps: 0.364606
 106039/150000: episode: 859, duration: 0.673s, episode steps:  94, steps per second: 140, episode reward: -18.773, mean reward: -0.200 [-100.000, 22.394], mean action: 1.436 [0.000, 3.000],  loss: 29.987235, mse: 19018.933604, mean_q: 138.446931, mean_eps: 0.364051
 106144/150000: episode: 860, duration: 0.712s, episode steps: 105, steps per second: 147, episode reward: -9.301, mean reward: -0.089 [-100.000, 14.750], mean action: 1.562 [0.000, 3.000],  loss: 33.614621, mse: 18633.988123, mean_q: 136.781791, mean_eps: 0.363454
 106250/150000: episode: 861, duration: 0.758s, episode steps: 106, steps per second: 140, episode reward: -135.290, mean reward: -1.276 [-100.000,  4.020], mean action: 1.585 [0.000, 3.000],  loss: 26.474739, mse: 18541.919397, mean_q: 135.851180, mean_eps: 0.362821
 106533/150000: episode: 862, duration: 2.179s, episode steps: 283, steps per second: 130, episode reward: -23.797, mean reward: -0.084 [-100.000, 39.282], mean action: 1.781 [0.000, 3.000],  loss: 28.832478, mse: 18165.456127, mean_q: 135.369098, mean_eps: 0.361654
 106668/150000: episode: 863, duration: 1.202s, episode steps: 135, steps per second: 112, episode reward: -243.692, mean reward: -1.805 [-100.000, 41.674], mean action: 1.867 [0.000, 3.000],  loss: 33.875079, mse: 18134.268417, mean_q: 134.952823, mean_eps: 0.360400
 106776/150000: episode: 864, duration: 0.882s, episode steps: 108, steps per second: 122, episode reward: -53.545, mean reward: -0.496 [-100.000,  6.946], mean action: 1.713 [0.000, 3.000],  loss: 25.167364, mse: 17531.736997, mean_q: 132.133439, mean_eps: 0.359671
 106847/150000: episode: 865, duration: 0.735s, episode steps:  71, steps per second:  97, episode reward: -55.408, mean reward: -0.780 [-100.000, 14.902], mean action: 1.930 [0.000, 3.000],  loss: 31.347616, mse: 16854.101824, mean_q: 128.809301, mean_eps: 0.359134
 106972/150000: episode: 866, duration: 1.114s, episode steps: 125, steps per second: 112, episode reward: -11.092, mean reward: -0.089 [-100.000, 14.115], mean action: 1.632 [0.000, 3.000],  loss: 27.771442, mse: 17117.870914, mean_q: 130.268147, mean_eps: 0.358546
 107132/150000: episode: 867, duration: 1.301s, episode steps: 160, steps per second: 123, episode reward: -186.741, mean reward: -1.167 [-100.000, 46.355], mean action: 1.656 [0.000, 3.000],  loss: 40.365634, mse: 16792.394592, mean_q: 129.557974, mean_eps: 0.357691
 107248/150000: episode: 868, duration: 0.900s, episode steps: 116, steps per second: 129, episode reward: -20.873, mean reward: -0.180 [-100.000, 12.480], mean action: 1.733 [0.000, 3.000],  loss: 26.363825, mse: 16725.121843, mean_q: 129.179254, mean_eps: 0.356863
 107338/150000: episode: 869, duration: 0.785s, episode steps:  90, steps per second: 115, episode reward: -31.293, mean reward: -0.348 [-100.000, 11.018], mean action: 1.833 [0.000, 3.000],  loss: 38.369446, mse: 16821.617459, mean_q: 130.001409, mean_eps: 0.356245
 107525/150000: episode: 870, duration: 1.347s, episode steps: 187, steps per second: 139, episode reward: -323.132, mean reward: -1.728 [-100.000,  5.225], mean action: 1.663 [0.000, 3.000],  loss: 35.240715, mse: 16826.054672, mean_q: 129.878383, mean_eps: 0.355414
 107667/150000: episode: 871, duration: 0.982s, episode steps: 142, steps per second: 145, episode reward: 17.470, mean reward:  0.123 [-100.000, 17.696], mean action: 1.641 [0.000, 3.000],  loss: 29.149170, mse: 17246.226363, mean_q: 132.406731, mean_eps: 0.354427
 107772/150000: episode: 872, duration: 0.735s, episode steps: 105, steps per second: 143, episode reward: -8.210, mean reward: -0.078 [-100.000, 13.594], mean action: 1.410 [0.000, 3.000],  loss: 27.191342, mse: 17149.583891, mean_q: 130.457124, mean_eps: 0.353686
 107887/150000: episode: 873, duration: 0.770s, episode steps: 115, steps per second: 149, episode reward: -0.389, mean reward: -0.003 [-100.000, 16.204], mean action: 1.626 [0.000, 3.000],  loss: 33.372508, mse: 17243.407261, mean_q: 132.155681, mean_eps: 0.353026
 107968/150000: episode: 874, duration: 0.560s, episode steps:  81, steps per second: 145, episode reward: 36.456, mean reward:  0.450 [-100.000, 15.491], mean action: 1.840 [0.000, 3.000],  loss: 33.022745, mse: 17336.151753, mean_q: 132.663893, mean_eps: 0.352438
 108133/150000: episode: 875, duration: 1.155s, episode steps: 165, steps per second: 143, episode reward: -179.664, mean reward: -1.089 [-100.000, 40.341], mean action: 1.618 [0.000, 3.000],  loss: 24.931820, mse: 17459.300556, mean_q: 133.158542, mean_eps: 0.351700
 108236/150000: episode: 876, duration: 0.707s, episode steps: 103, steps per second: 146, episode reward: 14.554, mean reward:  0.141 [-100.000, 14.307], mean action: 1.583 [0.000, 3.000],  loss: 23.878743, mse: 16937.990443, mean_q: 130.756611, mean_eps: 0.350896
 108312/150000: episode: 877, duration: 0.545s, episode steps:  76, steps per second: 139, episode reward: 22.066, mean reward:  0.290 [-100.000, 15.072], mean action: 1.632 [0.000, 3.000],  loss: 30.776783, mse: 16945.668097, mean_q: 130.464864, mean_eps: 0.350359
 108661/150000: episode: 878, duration: 2.485s, episode steps: 349, steps per second: 140, episode reward: -115.455, mean reward: -0.331 [-100.000, 17.733], mean action: 1.685 [0.000, 3.000],  loss: 30.638662, mse: 17152.582454, mean_q: 132.316011, mean_eps: 0.349084
 108746/150000: episode: 879, duration: 0.584s, episode steps:  85, steps per second: 146, episode reward: -6.425, mean reward: -0.076 [-100.000, 25.358], mean action: 1.682 [0.000, 3.000],  loss: 56.187774, mse: 17385.350919, mean_q: 134.188994, mean_eps: 0.347782
 109223/150000: episode: 880, duration: 3.487s, episode steps: 477, steps per second: 137, episode reward: -199.036, mean reward: -0.417 [-100.000, 20.747], mean action: 1.723 [0.000, 3.000],  loss: 38.402845, mse: 17360.927687, mean_q: 134.063854, mean_eps: 0.346096
 109618/150000: episode: 881, duration: 3.050s, episode steps: 395, steps per second: 130, episode reward: -255.812, mean reward: -0.648 [-100.000, 20.281], mean action: 1.446 [0.000, 3.000],  loss: 34.775579, mse: 17915.430810, mean_q: 135.854081, mean_eps: 0.343480
 110248/150000: episode: 882, duration: 4.849s, episode steps: 630, steps per second: 130, episode reward: -166.788, mean reward: -0.265 [-100.000, 22.704], mean action: 1.257 [0.000, 3.000],  loss: 30.268067, mse: 17576.798072, mean_q: 133.965648, mean_eps: 0.340405
 110353/150000: episode: 883, duration: 0.760s, episode steps: 105, steps per second: 138, episode reward:  5.739, mean reward:  0.055 [-100.000, 18.533], mean action: 1.914 [0.000, 3.000],  loss: 26.197005, mse: 16732.075400, mean_q: 129.652421, mean_eps: 0.338200
 110446/150000: episode: 884, duration: 0.635s, episode steps:  93, steps per second: 146, episode reward:  5.203, mean reward:  0.056 [-100.000, 17.968], mean action: 1.699 [0.000, 3.000],  loss: 42.520204, mse: 17136.000756, mean_q: 131.492269, mean_eps: 0.337606
 110536/150000: episode: 885, duration: 0.625s, episode steps:  90, steps per second: 144, episode reward: -4.651, mean reward: -0.052 [-100.000, 18.809], mean action: 1.567 [0.000, 3.000],  loss: 32.899154, mse: 17201.512684, mean_q: 132.233228, mean_eps: 0.337057
 110640/150000: episode: 886, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -12.487, mean reward: -0.120 [-100.000, 12.839], mean action: 1.558 [0.000, 3.000],  loss: 38.203527, mse: 16983.913180, mean_q: 131.595953, mean_eps: 0.336475
 110727/150000: episode: 887, duration: 0.600s, episode steps:  87, steps per second: 145, episode reward: 15.391, mean reward:  0.177 [-100.000, 14.155], mean action: 1.621 [0.000, 3.000],  loss: 40.921568, mse: 16659.587442, mean_q: 129.042978, mean_eps: 0.335902
 110819/150000: episode: 888, duration: 0.624s, episode steps:  92, steps per second: 147, episode reward: 62.293, mean reward:  0.677 [-100.000, 20.991], mean action: 1.880 [0.000, 3.000],  loss: 27.124660, mse: 16637.915708, mean_q: 128.937658, mean_eps: 0.335365
 110927/150000: episode: 889, duration: 0.773s, episode steps: 108, steps per second: 140, episode reward: -41.297, mean reward: -0.382 [-100.000,  9.091], mean action: 1.713 [0.000, 3.000],  loss: 29.762401, mse: 16718.461778, mean_q: 128.890997, mean_eps: 0.334765
 111035/150000: episode: 890, duration: 0.750s, episode steps: 108, steps per second: 144, episode reward: -33.020, mean reward: -0.306 [-100.000, 22.076], mean action: 1.713 [0.000, 3.000],  loss: 37.545785, mse: 16531.424434, mean_q: 128.239180, mean_eps: 0.334117
 111211/150000: episode: 891, duration: 1.273s, episode steps: 176, steps per second: 138, episode reward: -251.891, mean reward: -1.431 [-100.000, 14.255], mean action: 1.636 [0.000, 3.000],  loss: 30.518483, mse: 16231.131880, mean_q: 127.544950, mean_eps: 0.333265
 112017/150000: episode: 892, duration: 6.031s, episode steps: 806, steps per second: 134, episode reward: -120.542, mean reward: -0.150 [-100.000, 22.627], mean action: 1.860 [0.000, 3.000],  loss: 26.686313, mse: 15903.159391, mean_q: 125.926228, mean_eps: 0.330319
 112362/150000: episode: 893, duration: 2.415s, episode steps: 345, steps per second: 143, episode reward: -101.337, mean reward: -0.294 [-100.000, 13.457], mean action: 1.809 [0.000, 3.000],  loss: 33.653887, mse: 15599.464606, mean_q: 124.484386, mean_eps: 0.326866
 113362/150000: episode: 894, duration: 7.975s, episode steps: 1000, steps per second: 125, episode reward: 118.810, mean reward:  0.119 [-23.960, 24.303], mean action: 1.391 [0.000, 3.000],  loss: 29.177335, mse: 14900.336017, mean_q: 121.629033, mean_eps: 0.322831
 114362/150000: episode: 895, duration: 8.377s, episode steps: 1000, steps per second: 119, episode reward: 80.384, mean reward:  0.080 [-23.229, 22.571], mean action: 1.819 [0.000, 3.000],  loss: 28.046335, mse: 13851.460746, mean_q: 117.552095, mean_eps: 0.316831
 114511/150000: episode: 896, duration: 1.063s, episode steps: 149, steps per second: 140, episode reward: -224.350, mean reward: -1.506 [-100.000,  7.802], mean action: 1.638 [0.000, 3.000],  loss: 33.631373, mse: 13398.372215, mean_q: 114.939953, mean_eps: 0.313384
 114711/150000: episode: 897, duration: 1.389s, episode steps: 200, steps per second: 144, episode reward: -47.664, mean reward: -0.238 [-100.000, 11.958], mean action: 1.555 [0.000, 3.000],  loss: 30.813248, mse: 13462.538525, mean_q: 115.991109, mean_eps: 0.312337
 115054/150000: episode: 898, duration: 2.461s, episode steps: 343, steps per second: 139, episode reward: -201.662, mean reward: -0.588 [-100.000, 21.359], mean action: 1.714 [0.000, 3.000],  loss: 25.958996, mse: 12955.078347, mean_q: 113.607603, mean_eps: 0.310708
 115192/150000: episode: 899, duration: 0.994s, episode steps: 138, steps per second: 139, episode reward: -5.059, mean reward: -0.037 [-100.000, 20.127], mean action: 1.449 [0.000, 3.000],  loss: 24.415435, mse: 12812.189418, mean_q: 112.634230, mean_eps: 0.309265
 115379/150000: episode: 900, duration: 1.531s, episode steps: 187, steps per second: 122, episode reward: -38.069, mean reward: -0.204 [-100.000, 14.830], mean action: 1.759 [0.000, 3.000],  loss: 29.214248, mse: 12479.490600, mean_q: 111.339062, mean_eps: 0.308290
 115508/150000: episode: 901, duration: 1.066s, episode steps: 129, steps per second: 121, episode reward: -6.838, mean reward: -0.053 [-100.000, 16.441], mean action: 1.574 [0.000, 3.000],  loss: 21.236280, mse: 12362.058791, mean_q: 110.638418, mean_eps: 0.307342
 116030/150000: episode: 902, duration: 4.125s, episode steps: 522, steps per second: 127, episode reward: -246.482, mean reward: -0.472 [-100.000, 23.248], mean action: 1.402 [0.000, 3.000],  loss: 25.533646, mse: 11798.188718, mean_q: 107.390052, mean_eps: 0.305389
 116166/150000: episode: 903, duration: 1.004s, episode steps: 136, steps per second: 135, episode reward: -86.716, mean reward: -0.638 [-100.000,  4.113], mean action: 1.559 [0.000, 3.000],  loss: 23.975147, mse: 11383.050221, mean_q: 106.166865, mean_eps: 0.303415
 117166/150000: episode: 904, duration: 7.708s, episode steps: 1000, steps per second: 130, episode reward: 81.625, mean reward:  0.082 [-24.394, 24.248], mean action: 0.913 [0.000, 3.000],  loss: 25.790626, mse: 11229.321224, mean_q: 104.700282, mean_eps: 0.300007
 117355/150000: episode: 905, duration: 1.413s, episode steps: 189, steps per second: 134, episode reward: -152.444, mean reward: -0.807 [-100.000, 10.958], mean action: 1.423 [0.000, 3.000],  loss: 25.956042, mse: 10974.666501, mean_q: 102.250343, mean_eps: 0.296440
 117514/150000: episode: 906, duration: 1.124s, episode steps: 159, steps per second: 141, episode reward: -266.529, mean reward: -1.676 [-100.000,  4.283], mean action: 1.579 [0.000, 3.000],  loss: 22.492376, mse: 10800.281069, mean_q: 101.535462, mean_eps: 0.295396
 117678/150000: episode: 907, duration: 1.170s, episode steps: 164, steps per second: 140, episode reward: -181.236, mean reward: -1.105 [-100.000, 21.117], mean action: 1.701 [0.000, 3.000],  loss: 19.369183, mse: 10927.646630, mean_q: 102.871659, mean_eps: 0.294427
 118678/150000: episode: 908, duration: 7.771s, episode steps: 1000, steps per second: 129, episode reward: 105.559, mean reward:  0.106 [-20.696, 22.530], mean action: 1.590 [0.000, 3.000],  loss: 21.974126, mse: 10718.816650, mean_q: 101.976958, mean_eps: 0.290935
 118848/150000: episode: 909, duration: 1.252s, episode steps: 170, steps per second: 136, episode reward: -208.021, mean reward: -1.224 [-100.000, 11.882], mean action: 1.735 [0.000, 3.000],  loss: 20.566110, mse: 10228.812463, mean_q: 98.921111, mean_eps: 0.287425
 119848/150000: episode: 910, duration: 7.880s, episode steps: 1000, steps per second: 127, episode reward: 129.170, mean reward:  0.129 [-20.906, 22.503], mean action: 1.243 [0.000, 3.000],  loss: 21.925007, mse: 9887.800852, mean_q: 97.117795, mean_eps: 0.283915
 120848/150000: episode: 911, duration: 7.583s, episode steps: 1000, steps per second: 132, episode reward: 26.524, mean reward:  0.027 [-24.818, 22.956], mean action: 1.256 [0.000, 3.000],  loss: 20.653967, mse: 8905.771001, mean_q: 91.438819, mean_eps: 0.277915
 120992/150000: episode: 912, duration: 1.003s, episode steps: 144, steps per second: 144, episode reward: -168.356, mean reward: -1.169 [-100.000, 24.631], mean action: 1.840 [0.000, 3.000],  loss: 19.497234, mse: 8535.317166, mean_q: 88.874027, mean_eps: 0.274483
 121135/150000: episode: 913, duration: 1.014s, episode steps: 143, steps per second: 141, episode reward: 20.264, mean reward:  0.142 [-100.000, 13.184], mean action: 1.643 [0.000, 3.000],  loss: 15.187038, mse: 8390.058580, mean_q: 87.152066, mean_eps: 0.273622
 122135/150000: episode: 914, duration: 7.372s, episode steps: 1000, steps per second: 136, episode reward: 115.369, mean reward:  0.115 [-23.376, 24.217], mean action: 1.106 [0.000, 3.000],  loss: 20.566577, mse: 8118.626657, mean_q: 85.423484, mean_eps: 0.270193
 122377/150000: episode: 915, duration: 1.722s, episode steps: 242, steps per second: 141, episode reward: 235.608, mean reward:  0.974 [-22.316, 100.000], mean action: 1.207 [0.000, 3.000],  loss: 19.887210, mse: 7592.727513, mean_q: 82.229595, mean_eps: 0.266467
 122615/150000: episode: 916, duration: 1.662s, episode steps: 238, steps per second: 143, episode reward: -64.952, mean reward: -0.273 [-100.000,  7.704], mean action: 1.571 [0.000, 3.000],  loss: 15.628665, mse: 7536.608072, mean_q: 82.026431, mean_eps: 0.265027
 122713/150000: episode: 917, duration: 0.671s, episode steps:  98, steps per second: 146, episode reward: -55.369, mean reward: -0.565 [-100.000, 13.879], mean action: 1.490 [0.000, 3.000],  loss: 18.090228, mse: 7575.946618, mean_q: 82.849738, mean_eps: 0.264019
 122845/150000: episode: 918, duration: 0.900s, episode steps: 132, steps per second: 147, episode reward: 16.180, mean reward:  0.123 [-100.000, 17.971], mean action: 1.576 [0.000, 3.000],  loss: 17.869004, mse: 7535.920880, mean_q: 83.150827, mean_eps: 0.263329
 122955/150000: episode: 919, duration: 0.778s, episode steps: 110, steps per second: 141, episode reward: -34.340, mean reward: -0.312 [-100.000, 17.998], mean action: 1.873 [0.000, 3.000],  loss: 17.626587, mse: 7381.876722, mean_q: 81.502626, mean_eps: 0.262603
 123236/150000: episode: 920, duration: 2.001s, episode steps: 281, steps per second: 140, episode reward: -153.729, mean reward: -0.547 [-100.000, 15.134], mean action: 1.794 [0.000, 3.000],  loss: 21.223289, mse: 7170.306171, mean_q: 80.363621, mean_eps: 0.261430
 123387/150000: episode: 921, duration: 1.038s, episode steps: 151, steps per second: 145, episode reward: -22.422, mean reward: -0.148 [-100.000, 12.816], mean action: 1.669 [0.000, 3.000],  loss: 16.671333, mse: 7352.734902, mean_q: 82.346409, mean_eps: 0.260134
 124387/150000: episode: 922, duration: 8.210s, episode steps: 1000, steps per second: 122, episode reward: 64.638, mean reward:  0.065 [-21.575, 23.277], mean action: 1.107 [0.000, 3.000],  loss: 19.240664, mse: 7044.984781, mean_q: 80.441776, mean_eps: 0.256681
 125098/150000: episode: 923, duration: 5.428s, episode steps: 711, steps per second: 131, episode reward: 259.618, mean reward:  0.365 [-20.423, 100.000], mean action: 1.097 [0.000, 3.000],  loss: 18.861211, mse: 7032.559995, mean_q: 80.278617, mean_eps: 0.251548
 125324/150000: episode: 924, duration: 1.675s, episode steps: 226, steps per second: 135, episode reward: -47.990, mean reward: -0.212 [-100.000, 17.679], mean action: 1.783 [0.000, 3.000],  loss: 15.675455, mse: 6812.874646, mean_q: 79.218620, mean_eps: 0.248737
 126324/150000: episode: 925, duration: 7.771s, episode steps: 1000, steps per second: 129, episode reward: 136.109, mean reward:  0.136 [-21.235, 19.927], mean action: 1.169 [0.000, 3.000],  loss: 18.264202, mse: 6764.089952, mean_q: 78.858654, mean_eps: 0.245059
 127288/150000: episode: 926, duration: 7.422s, episode steps: 964, steps per second: 130, episode reward: 179.539, mean reward:  0.186 [-20.995, 100.000], mean action: 1.168 [0.000, 3.000],  loss: 20.770575, mse: 6826.365341, mean_q: 79.486135, mean_eps: 0.239167
 127407/150000: episode: 927, duration: 0.844s, episode steps: 119, steps per second: 141, episode reward: -238.369, mean reward: -2.003 [-100.000,  9.378], mean action: 1.529 [0.000, 3.000],  loss: 14.714675, mse: 6953.213732, mean_q: 81.500634, mean_eps: 0.235918
 127771/150000: episode: 928, duration: 2.570s, episode steps: 364, steps per second: 142, episode reward: 255.000, mean reward:  0.701 [-19.144, 100.000], mean action: 1.665 [0.000, 3.000],  loss: 16.972991, mse: 6873.687589, mean_q: 81.231214, mean_eps: 0.234469
 128563/150000: episode: 929, duration: 6.052s, episode steps: 792, steps per second: 131, episode reward: 180.944, mean reward:  0.228 [-24.427, 100.000], mean action: 1.230 [0.000, 3.000],  loss: 16.322607, mse: 6849.005677, mean_q: 80.633689, mean_eps: 0.231001
 129563/150000: episode: 930, duration: 7.676s, episode steps: 1000, steps per second: 130, episode reward: 134.357, mean reward:  0.134 [-21.912, 13.175], mean action: 1.421 [0.000, 3.000],  loss: 16.408144, mse: 6597.043724, mean_q: 79.194717, mean_eps: 0.225625
 129932/150000: episode: 931, duration: 2.622s, episode steps: 369, steps per second: 141, episode reward: 271.219, mean reward:  0.735 [-18.292, 100.000], mean action: 1.545 [0.000, 3.000],  loss: 16.001201, mse: 6165.214602, mean_q: 77.624931, mean_eps: 0.221518
 130932/150000: episode: 932, duration: 7.584s, episode steps: 1000, steps per second: 132, episode reward: 90.525, mean reward:  0.091 [-23.842, 17.712], mean action: 1.196 [0.000, 3.000],  loss: 16.799092, mse: 5900.092398, mean_q: 74.894335, mean_eps: 0.217411
 131932/150000: episode: 933, duration: 8.037s, episode steps: 1000, steps per second: 124, episode reward: 52.146, mean reward:  0.052 [-17.764, 19.270], mean action: 1.361 [0.000, 3.000],  loss: 16.380088, mse: 5558.705601, mean_q: 73.146607, mean_eps: 0.211411
 132024/150000: episode: 934, duration: 0.652s, episode steps:  92, steps per second: 141, episode reward: 30.281, mean reward:  0.329 [-100.000, 12.378], mean action: 1.837 [0.000, 3.000],  loss: 15.097853, mse: 5345.609550, mean_q: 71.547823, mean_eps: 0.208135
 132283/150000: episode: 935, duration: 2.184s, episode steps: 259, steps per second: 119, episode reward: -149.270, mean reward: -0.576 [-100.000,  4.735], mean action: 1.861 [0.000, 3.000],  loss: 13.791033, mse: 5187.703708, mean_q: 70.477017, mean_eps: 0.207082
 132432/150000: episode: 936, duration: 1.340s, episode steps: 149, steps per second: 111, episode reward: -9.964, mean reward: -0.067 [-100.000, 18.900], mean action: 1.765 [0.000, 3.000],  loss: 10.945723, mse: 5274.636668, mean_q: 70.996228, mean_eps: 0.205858
 132601/150000: episode: 937, duration: 1.544s, episode steps: 169, steps per second: 109, episode reward: -20.491, mean reward: -0.121 [-100.000, 17.673], mean action: 1.763 [0.000, 3.000],  loss: 12.678673, mse: 5273.909734, mean_q: 71.737519, mean_eps: 0.204904
 133601/150000: episode: 938, duration: 8.382s, episode steps: 1000, steps per second: 119, episode reward: 70.811, mean reward:  0.071 [-21.968, 23.016], mean action: 1.300 [0.000, 3.000],  loss: 15.332106, mse: 5090.002074, mean_q: 70.751986, mean_eps: 0.201397
 133705/150000: episode: 939, duration: 0.746s, episode steps: 104, steps per second: 139, episode reward: -10.241, mean reward: -0.098 [-100.000, 19.090], mean action: 1.760 [0.000, 3.000],  loss: 15.193053, mse: 4932.093661, mean_q: 70.037420, mean_eps: 0.198085
 134705/150000: episode: 940, duration: 7.480s, episode steps: 1000, steps per second: 134, episode reward: 116.650, mean reward:  0.117 [-21.532, 22.078], mean action: 1.122 [0.000, 3.000],  loss: 15.535012, mse: 4781.049776, mean_q: 68.836416, mean_eps: 0.194773
 134857/150000: episode: 941, duration: 1.068s, episode steps: 152, steps per second: 142, episode reward: -90.130, mean reward: -0.593 [-100.000,  5.133], mean action: 1.789 [0.000, 3.000],  loss: 20.223173, mse: 4675.659356, mean_q: 68.190758, mean_eps: 0.191317
 135317/150000: episode: 942, duration: 3.866s, episode steps: 460, steps per second: 119, episode reward: 273.162, mean reward:  0.594 [-20.147, 100.000], mean action: 1.439 [0.000, 3.000],  loss: 16.429587, mse: 4596.913399, mean_q: 67.373974, mean_eps: 0.189481
 136317/150000: episode: 943, duration: 9.574s, episode steps: 1000, steps per second: 104, episode reward: 110.267, mean reward:  0.110 [-19.879, 22.543], mean action: 1.039 [0.000, 3.000],  loss: 16.875560, mse: 4214.815334, mean_q: 64.916121, mean_eps: 0.185101
 136595/150000: episode: 944, duration: 2.619s, episode steps: 278, steps per second: 106, episode reward: -198.543, mean reward: -0.714 [-100.000, 22.454], mean action: 1.665 [0.000, 3.000],  loss: 12.796827, mse: 4109.483254, mean_q: 64.571311, mean_eps: 0.181267
 136940/150000: episode: 945, duration: 3.248s, episode steps: 345, steps per second: 106, episode reward: -205.002, mean reward: -0.594 [-100.000, 23.466], mean action: 1.777 [0.000, 3.000],  loss: 15.969038, mse: 4142.273899, mean_q: 64.874559, mean_eps: 0.179398
 137106/150000: episode: 946, duration: 1.570s, episode steps: 166, steps per second: 106, episode reward: -122.168, mean reward: -0.736 [-100.000,  8.315], mean action: 1.813 [0.000, 3.000],  loss: 17.563694, mse: 4153.024376, mean_q: 64.640186, mean_eps: 0.177865
 137259/150000: episode: 947, duration: 1.376s, episode steps: 153, steps per second: 111, episode reward: -725.448, mean reward: -4.741 [-100.000,  1.485], mean action: 1.824 [0.000, 3.000],  loss: 12.645193, mse: 4210.335765, mean_q: 64.981512, mean_eps: 0.176908
 137383/150000: episode: 948, duration: 1.076s, episode steps: 124, steps per second: 115, episode reward: 11.284, mean reward:  0.091 [-100.000, 16.789], mean action: 1.847 [0.000, 3.000],  loss: 13.098050, mse: 4341.675000, mean_q: 65.009081, mean_eps: 0.176077
 137481/150000: episode: 949, duration: 0.879s, episode steps:  98, steps per second: 112, episode reward: -81.313, mean reward: -0.830 [-100.000,  9.265], mean action: 1.929 [0.000, 3.000],  loss: 24.742871, mse: 4489.777376, mean_q: 66.114307, mean_eps: 0.175411
 137729/150000: episode: 950, duration: 2.195s, episode steps: 248, steps per second: 113, episode reward: -187.383, mean reward: -0.756 [-100.000, 12.079], mean action: 1.839 [0.000, 3.000],  loss: 27.703941, mse: 4433.067135, mean_q: 65.063828, mean_eps: 0.174373
 137905/150000: episode: 951, duration: 1.505s, episode steps: 176, steps per second: 117, episode reward: -115.566, mean reward: -0.657 [-100.000, 13.837], mean action: 1.841 [0.000, 3.000],  loss: 15.276652, mse: 4321.840501, mean_q: 64.527328, mean_eps: 0.173101
 138032/150000: episode: 952, duration: 1.150s, episode steps: 127, steps per second: 110, episode reward: -68.006, mean reward: -0.535 [-100.000,  6.243], mean action: 1.819 [0.000, 3.000],  loss: 12.664642, mse: 4339.367614, mean_q: 64.291815, mean_eps: 0.172192
 139032/150000: episode: 953, duration: 8.316s, episode steps: 1000, steps per second: 120, episode reward: 60.235, mean reward:  0.060 [-23.956, 25.576], mean action: 1.635 [0.000, 3.000],  loss: 16.795295, mse: 4211.437986, mean_q: 62.979100, mean_eps: 0.168811
 139140/150000: episode: 954, duration: 0.738s, episode steps: 108, steps per second: 146, episode reward: -45.149, mean reward: -0.418 [-100.000, 20.350], mean action: 1.704 [0.000, 3.000],  loss: 18.937556, mse: 4347.227390, mean_q: 64.478992, mean_eps: 0.165487
 139253/150000: episode: 955, duration: 0.838s, episode steps: 113, steps per second: 135, episode reward: -67.723, mean reward: -0.599 [-100.000, 28.406], mean action: 1.717 [0.000, 3.000],  loss: 13.372266, mse: 4470.398381, mean_q: 65.434989, mean_eps: 0.164824
 139902/150000: episode: 956, duration: 4.896s, episode steps: 649, steps per second: 133, episode reward: 244.880, mean reward:  0.377 [-17.675, 100.000], mean action: 1.245 [0.000, 3.000],  loss: 19.699310, mse: 4254.207989, mean_q: 63.821229, mean_eps: 0.162538
 140056/150000: episode: 957, duration: 1.052s, episode steps: 154, steps per second: 146, episode reward: 15.547, mean reward:  0.101 [-100.000, 16.457], mean action: 2.000 [0.000, 3.000],  loss: 17.307675, mse: 4122.851632, mean_q: 62.644440, mean_eps: 0.160129
 140391/150000: episode: 958, duration: 2.600s, episode steps: 335, steps per second: 129, episode reward: 265.209, mean reward:  0.792 [-18.866, 100.000], mean action: 1.406 [0.000, 3.000],  loss: 18.540872, mse: 4217.137155, mean_q: 63.227371, mean_eps: 0.158662
 141024/150000: episode: 959, duration: 6.079s, episode steps: 633, steps per second: 104, episode reward: 177.023, mean reward:  0.280 [-20.552, 100.000], mean action: 1.698 [0.000, 3.000],  loss: 18.158412, mse: 4075.187341, mean_q: 62.145717, mean_eps: 0.155758
 141399/150000: episode: 960, duration: 3.704s, episode steps: 375, steps per second: 101, episode reward: -199.473, mean reward: -0.532 [-100.000, 23.157], mean action: 1.632 [0.000, 3.000],  loss: 12.714714, mse: 3990.374347, mean_q: 62.462971, mean_eps: 0.152734
 142399/150000: episode: 961, duration: 10.284s, episode steps: 1000, steps per second:  97, episode reward: 165.061, mean reward:  0.165 [-22.300, 21.829], mean action: 0.769 [0.000, 3.000],  loss: 16.215433, mse: 4036.793021, mean_q: 63.306529, mean_eps: 0.148609
 143297/150000: episode: 962, duration: 7.407s, episode steps: 898, steps per second: 121, episode reward: 152.072, mean reward:  0.169 [-22.902, 100.000], mean action: 1.199 [0.000, 3.000],  loss: 15.533905, mse: 3977.129760, mean_q: 62.673222, mean_eps: 0.142915
 143714/150000: episode: 963, duration: 3.113s, episode steps: 417, steps per second: 134, episode reward: 156.957, mean reward:  0.376 [-18.231, 100.000], mean action: 2.149 [0.000, 3.000],  loss: 14.468026, mse: 3941.503255, mean_q: 62.665976, mean_eps: 0.138970
 144284/150000: episode: 964, duration: 4.540s, episode steps: 570, steps per second: 126, episode reward: 187.628, mean reward:  0.329 [-21.158, 100.000], mean action: 1.454 [0.000, 3.000],  loss: 15.433039, mse: 4042.094885, mean_q: 63.502053, mean_eps: 0.136009
 145103/150000: episode: 965, duration: 6.069s, episode steps: 819, steps per second: 135, episode reward: 250.648, mean reward:  0.306 [-18.309, 100.000], mean action: 1.004 [0.000, 3.000],  loss: 16.860891, mse: 4102.907560, mean_q: 63.292379, mean_eps: 0.131842
 145801/150000: episode: 966, duration: 5.232s, episode steps: 698, steps per second: 133, episode reward: 281.491, mean reward:  0.403 [-19.535, 100.000], mean action: 0.905 [0.000, 3.000],  loss: 16.377749, mse: 4079.129163, mean_q: 63.658332, mean_eps: 0.127291
 146258/150000: episode: 967, duration: 3.431s, episode steps: 457, steps per second: 133, episode reward: 204.398, mean reward:  0.447 [-18.128, 100.000], mean action: 1.274 [0.000, 3.000],  loss: 15.113171, mse: 4290.029965, mean_q: 65.698515, mean_eps: 0.123826
 146545/150000: episode: 968, duration: 2.032s, episode steps: 287, steps per second: 141, episode reward: 208.789, mean reward:  0.727 [-19.896, 100.000], mean action: 1.589 [0.000, 3.000],  loss: 15.630453, mse: 4033.038381, mean_q: 63.828780, mean_eps: 0.121594
 147052/150000: episode: 969, duration: 3.652s, episode steps: 507, steps per second: 139, episode reward: 249.777, mean reward:  0.493 [-19.439, 100.000], mean action: 1.047 [0.000, 3.000],  loss: 12.579131, mse: 4067.370974, mean_q: 64.379042, mean_eps: 0.119212
 147477/150000: episode: 970, duration: 3.208s, episode steps: 425, steps per second: 132, episode reward: 278.927, mean reward:  0.656 [-20.466, 100.000], mean action: 1.125 [0.000, 3.000],  loss: 12.192215, mse: 4143.426411, mean_q: 64.655052, mean_eps: 0.116416
 147663/150000: episode: 971, duration: 1.325s, episode steps: 186, steps per second: 140, episode reward: -221.172, mean reward: -1.189 [-100.000, 40.550], mean action: 1.403 [0.000, 3.000],  loss: 14.426377, mse: 4293.659747, mean_q: 65.466076, mean_eps: 0.114583
 147894/150000: episode: 972, duration: 1.623s, episode steps: 231, steps per second: 142, episode reward: 222.192, mean reward:  0.962 [-18.813, 100.000], mean action: 1.411 [0.000, 3.000],  loss: 15.084160, mse: 4436.568457, mean_q: 66.484769, mean_eps: 0.113332
 148064/150000: episode: 973, duration: 1.166s, episode steps: 170, steps per second: 146, episode reward: 46.125, mean reward:  0.271 [-100.000, 14.857], mean action: 1.712 [0.000, 3.000],  loss: 14.569271, mse: 4646.323389, mean_q: 67.654606, mean_eps: 0.112129
 148324/150000: episode: 974, duration: 1.943s, episode steps: 260, steps per second: 134, episode reward: 228.262, mean reward:  0.878 [-18.296, 100.000], mean action: 1.319 [0.000, 3.000],  loss: 16.017435, mse: 4838.883931, mean_q: 69.455775, mean_eps: 0.110839
 148946/150000: episode: 975, duration: 5.644s, episode steps: 622, steps per second: 110, episode reward: 232.635, mean reward:  0.374 [-18.888, 100.000], mean action: 0.986 [0.000, 3.000],  loss: 15.335046, mse: 4807.719743, mean_q: 69.226480, mean_eps: 0.108193
 149591/150000: episode: 976, duration: 5.023s, episode steps: 645, steps per second: 128, episode reward: -286.011, mean reward: -0.443 [-100.000, 37.137], mean action: 1.826 [0.000, 3.000],  loss: 15.444860, mse: 4812.916877, mean_q: 69.196553, mean_eps: 0.104392
done, took 1200.654 seconds
Testing for 5 episodes ...
Episode 1: reward: 167.956, steps: 633
Episode 2: reward: 245.685, steps: 239
Episode 3: reward: -222.675, steps: 380
Episode 4: reward: -210.316, steps: 225
Episode 5: reward: 269.210, steps: 226
Testing for 5 episodes ...
Episode 1: reward: 246.024, steps: 317
Episode 2: reward: -143.324, steps: 406
Episode 3: reward: -68.335, steps: 376
Episode 4: reward: 261.741, steps: 511
Episode 5: reward: 267.042, steps: 359
Testing for 5 episodes ...
Episode 1: reward: 178.453, steps: 374
Episode 2: reward: 69.505, steps: 1000
Episode 3: reward: -187.707, steps: 864
Episode 4: reward: 188.294, steps: 300
Episode 5: reward: 242.707, steps: 420
Testing for 5 episodes ...
Episode 1: reward: 250.716, steps: 321
Episode 2: reward: 257.224, steps: 242
Episode 3: reward: 240.793, steps: 361
Episode 4: reward: 241.769, steps: 296
