Model: "sequential"
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
flatten (Flatten)            (None, 8)                 0
_________________________________________________________________
dense (Dense)                (None, 64)                576
_________________________________________________________________
activation (Activation)      (None, 64)                0
_________________________________________________________________
dense_1 (Dense)              (None, 64)                4160
_________________________________________________________________
activation_1 (Activation)    (None, 64)                0
_________________________________________________________________
dense_2 (Dense)              (None, 32)                2080
_________________________________________________________________
activation_2 (Activation)    (None, 32)                0
_________________________________________________________________
dense_3 (Dense)              (None, 4)                 132
_________________________________________________________________
activation_3 (Activation)    (None, 4)                 0
=================================================================
Total params: 6,948
Trainable params: 6,948
Non-trainable params: 0
_________________________________________________________________
None
Training for 300000 steps ...
     71/300000: episode: 1, duration: 0.117s, episode steps:  71, steps per second: 609, episode reward: -124.805, mean reward: -1.758 [-100.000,  5.833], mean action: 1.282 [0.000, 3.000],  loss: --, mse: --, mean_q: --, mean_eps: --
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
C:\Users\nguye\anaconda3\lib\site-packages\tensorflow\python\keras\engine\training.py:2325: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.
  warnings.warn('`Model.state_updates` will be removed in a future version. '
    179/300000: episode: 2, duration: 1.046s, episode steps: 108, steps per second: 103, episode reward: -474.491, mean reward: -4.393 [-100.000,  0.740], mean action: 1.519 [0.000, 3.000],  loss: 42.677238, mse: 22.423970, mean_q: -0.740206, mean_eps: 0.999540
    309/300000: episode: 3, duration: 0.810s, episode steps: 130, steps per second: 160, episode reward: -305.061, mean reward: -2.347 [-100.000,  1.658], mean action: 1.538 [0.000, 3.000],  loss: 41.612902, mse: 31.892180, mean_q: -1.469247, mean_eps: 0.999196
    394/300000: episode: 4, duration: 0.556s, episode steps:  85, steps per second: 153, episode reward: -200.748, mean reward: -2.362 [-100.000, 17.915], mean action: 1.576 [0.000, 3.000],  loss: 37.331624, mse: 30.128592, mean_q: -1.079908, mean_eps: 0.998842
    484/300000: episode: 5, duration: 0.570s, episode steps:  90, steps per second: 158, episode reward: -117.244, mean reward: -1.303 [-100.000, 12.629], mean action: 1.489 [0.000, 3.000],  loss: 33.142310, mse: 28.075477, mean_q: -0.763673, mean_eps: 0.998553
    558/300000: episode: 6, duration: 0.475s, episode steps:  74, steps per second: 156, episode reward: -169.933, mean reward: -2.296 [-100.000,  7.823], mean action: 1.649 [0.000, 3.000],  loss: 32.240694, mse: 30.621139, mean_q: -0.503523, mean_eps: 0.998282
    623/300000: episode: 7, duration: 0.399s, episode steps:  65, steps per second: 163, episode reward: -180.513, mean reward: -2.777 [-100.000,  4.377], mean action: 1.723 [0.000, 3.000],  loss: 24.604420, mse: 25.609356, mean_q: -0.250054, mean_eps: 0.998053
    720/300000: episode: 8, duration: 0.596s, episode steps:  97, steps per second: 163, episode reward: -138.462, mean reward: -1.427 [-100.000, 80.981], mean action: 1.680 [0.000, 3.000],  loss: 39.008167, mse: 38.651630, mean_q: -0.258859, mean_eps: 0.997786
    790/300000: episode: 9, duration: 0.469s, episode steps:  70, steps per second: 149, episode reward: -71.235, mean reward: -1.018 [-100.000, 18.211], mean action: 1.600 [0.000, 3.000],  loss: 34.416830, mse: 33.137598, mean_q: -0.327002, mean_eps: 0.997510
    883/300000: episode: 10, duration: 0.676s, episode steps:  93, steps per second: 138, episode reward: -179.780, mean reward: -1.933 [-100.000,  5.165], mean action: 1.613 [0.000, 3.000],  loss: 37.129451, mse: 34.339936, mean_q: -0.279831, mean_eps: 0.997241
    981/300000: episode: 11, duration: 0.767s, episode steps:  98, steps per second: 128, episode reward: -290.925, mean reward: -2.969 [-100.000,  6.200], mean action: 1.480 [0.000, 3.000],  loss: 35.630061, mse: 33.790012, mean_q: -0.165620, mean_eps: 0.996926
   1071/300000: episode: 12, duration: 0.808s, episode steps:  90, steps per second: 111, episode reward: -211.028, mean reward: -2.345 [-100.000, 14.028], mean action: 1.600 [0.000, 3.000],  loss: 31.198054, mse: 38.182978, mean_q: -0.186019, mean_eps: 0.996616
   1172/300000: episode: 13, duration: 0.769s, episode steps: 101, steps per second: 131, episode reward: -132.720, mean reward: -1.314 [-100.000, 12.503], mean action: 1.554 [0.000, 3.000],  loss: 30.289548, mse: 35.214597, mean_q: -0.139596, mean_eps: 0.996301
   1257/300000: episode: 14, duration: 0.719s, episode steps:  85, steps per second: 118, episode reward: -228.552, mean reward: -2.689 [-100.000,  4.250], mean action: 1.553 [0.000, 3.000],  loss: 32.728908, mse: 38.480423, mean_q: -0.109946, mean_eps: 0.995994
   1332/300000: episode: 15, duration: 0.669s, episode steps:  75, steps per second: 112, episode reward: -138.513, mean reward: -1.847 [-100.000,  5.781], mean action: 1.533 [0.000, 3.000],  loss: 30.296862, mse: 45.287658, mean_q: -0.116191, mean_eps: 0.995730
   1420/300000: episode: 16, duration: 0.622s, episode steps:  88, steps per second: 141, episode reward: -138.819, mean reward: -1.577 [-100.000, 13.298], mean action: 1.273 [0.000, 3.000],  loss: 29.798178, mse: 42.175107, mean_q: 0.177448, mean_eps: 0.995461
   1510/300000: episode: 17, duration: 0.617s, episode steps:  90, steps per second: 146, episode reward: -121.763, mean reward: -1.353 [-100.000, 28.330], mean action: 1.489 [0.000, 3.000],  loss: 27.152956, mse: 46.799938, mean_q: 0.144489, mean_eps: 0.995167
   1577/300000: episode: 18, duration: 0.580s, episode steps:  67, steps per second: 116, episode reward: -234.618, mean reward: -3.502 [-100.000,  2.886], mean action: 1.358 [0.000, 3.000],  loss: 26.737176, mse: 54.094998, mean_q: 0.378045, mean_eps: 0.994908
   1718/300000: episode: 19, duration: 1.259s, episode steps: 141, steps per second: 112, episode reward: -206.941, mean reward: -1.468 [-100.000, 39.701], mean action: 1.482 [0.000, 3.000],  loss: 26.205836, mse: 39.608441, mean_q: 0.383614, mean_eps: 0.994565
   1816/300000: episode: 20, duration: 0.763s, episode steps:  98, steps per second: 128, episode reward: -235.344, mean reward: -2.401 [-100.000, 37.258], mean action: 1.571 [0.000, 3.000],  loss: 28.233452, mse: 50.673502, mean_q: 0.362409, mean_eps: 0.994171
   1901/300000: episode: 21, duration: 0.680s, episode steps:  85, steps per second: 125, episode reward: -220.394, mean reward: -2.593 [-100.000, 51.764], mean action: 1.565 [0.000, 3.000],  loss: 28.105908, mse: 42.177694, mean_q: 0.343676, mean_eps: 0.993869
   1991/300000: episode: 22, duration: 0.676s, episode steps:  90, steps per second: 133, episode reward: -189.953, mean reward: -2.111 [-100.000,  6.940], mean action: 1.567 [0.000, 3.000],  loss: 24.359942, mse: 47.488362, mean_q: 0.340147, mean_eps: 0.993580
   2059/300000: episode: 23, duration: 0.470s, episode steps:  68, steps per second: 145, episode reward: -89.548, mean reward: -1.317 [-100.000,  6.494], mean action: 1.515 [0.000, 3.000],  loss: 30.919738, mse: 48.219061, mean_q: 0.663708, mean_eps: 0.993319
   2161/300000: episode: 24, duration: 0.694s, episode steps: 102, steps per second: 147, episode reward: -122.757, mean reward: -1.203 [-100.000, 17.745], mean action: 1.510 [0.000, 3.000],  loss: 30.008942, mse: 55.813262, mean_q: 1.060771, mean_eps: 0.993039
   2256/300000: episode: 25, duration: 0.636s, episode steps:  95, steps per second: 149, episode reward: -112.465, mean reward: -1.184 [-100.000,  8.865], mean action: 1.537 [0.000, 3.000],  loss: 26.526310, mse: 53.687619, mean_q: 1.278011, mean_eps: 0.992714
   2371/300000: episode: 26, duration: 0.772s, episode steps: 115, steps per second: 149, episode reward: -354.764, mean reward: -3.085 [-100.000,  1.692], mean action: 1.504 [0.000, 3.000],  loss: 23.021824, mse: 43.083866, mean_q: 1.408783, mean_eps: 0.992367
   2446/300000: episode: 27, duration: 0.518s, episode steps:  75, steps per second: 145, episode reward: -120.069, mean reward: -1.601 [-100.000,  8.767], mean action: 1.453 [0.000, 3.000],  loss: 22.966657, mse: 47.299260, mean_q: 1.282459, mean_eps: 0.992054
   2527/300000: episode: 28, duration: 0.587s, episode steps:  81, steps per second: 138, episode reward: -114.121, mean reward: -1.409 [-100.000,  8.622], mean action: 1.432 [0.000, 3.000],  loss: 30.477930, mse: 53.922177, mean_q: 1.384025, mean_eps: 0.991796
   2613/300000: episode: 29, duration: 0.537s, episode steps:  86, steps per second: 160, episode reward: -245.702, mean reward: -2.857 [-100.000,  6.966], mean action: 1.523 [0.000, 3.000],  loss: 27.306515, mse: 49.410833, mean_q: 1.217476, mean_eps: 0.991521
   2705/300000: episode: 30, duration: 0.599s, episode steps:  92, steps per second: 154, episode reward: -287.634, mean reward: -3.126 [-100.000,  3.775], mean action: 1.533 [0.000, 3.000],  loss: 26.663963, mse: 54.388551, mean_q: 1.153323, mean_eps: 0.991227
   2768/300000: episode: 31, duration: 0.539s, episode steps:  63, steps per second: 117, episode reward: -84.074, mean reward: -1.335 [-100.000,  6.397], mean action: 1.460 [0.000, 3.000],  loss: 26.913664, mse: 52.890223, mean_q: 1.207846, mean_eps: 0.990971
   2907/300000: episode: 32, duration: 1.141s, episode steps: 139, steps per second: 122, episode reward: -188.860, mean reward: -1.359 [-100.000,  8.224], mean action: 1.424 [0.000, 3.000],  loss: 24.036276, mse: 48.537270, mean_q: 1.031902, mean_eps: 0.990638
   3006/300000: episode: 33, duration: 0.800s, episode steps:  99, steps per second: 124, episode reward: -270.986, mean reward: -2.737 [-100.000,  6.352], mean action: 1.566 [0.000, 3.000],  loss: 20.866819, mse: 47.636411, mean_q: 1.045246, mean_eps: 0.990245
   3093/300000: episode: 34, duration: 0.705s, episode steps:  87, steps per second: 123, episode reward: -60.654, mean reward: -0.697 [-100.000, 18.159], mean action: 1.287 [0.000, 3.000],  loss: 22.001200, mse: 46.472320, mean_q: 1.564005, mean_eps: 0.989938
   3189/300000: episode: 35, duration: 0.754s, episode steps:  96, steps per second: 127, episode reward: -104.187, mean reward: -1.085 [-100.000,  6.368], mean action: 1.427 [0.000, 3.000],  loss: 18.965878, mse: 50.641039, mean_q: 1.814836, mean_eps: 0.989636
   3285/300000: episode: 36, duration: 0.748s, episode steps:  96, steps per second: 128, episode reward: -385.150, mean reward: -4.012 [-100.000, 46.177], mean action: 1.500 [0.000, 3.000],  loss: 21.579523, mse: 60.141962, mean_q: 1.634745, mean_eps: 0.989320
   3362/300000: episode: 37, duration: 0.606s, episode steps:  77, steps per second: 127, episode reward: -4.104, mean reward: -0.053 [-100.000, 80.670], mean action: 1.636 [0.000, 3.000],  loss: 22.002608, mse: 77.459061, mean_q: 1.518007, mean_eps: 0.989034
   3479/300000: episode: 38, duration: 0.824s, episode steps: 117, steps per second: 142, episode reward: -140.872, mean reward: -1.204 [-100.000,  6.599], mean action: 1.632 [0.000, 3.000],  loss: 27.203080, mse: 68.909425, mean_q: 1.573288, mean_eps: 0.988714
   3548/300000: episode: 39, duration: 0.495s, episode steps:  69, steps per second: 139, episode reward: -62.911, mean reward: -0.912 [-100.000,  7.381], mean action: 1.551 [0.000, 3.000],  loss: 25.562321, mse: 69.087330, mean_q: 1.417701, mean_eps: 0.988407
   3669/300000: episode: 40, duration: 0.952s, episode steps: 121, steps per second: 127, episode reward: -210.317, mean reward: -1.738 [-100.000,  8.099], mean action: 1.496 [0.000, 3.000],  loss: 27.621272, mse: 72.685108, mean_q: 1.507747, mean_eps: 0.988094
   3770/300000: episode: 41, duration: 0.745s, episode steps: 101, steps per second: 135, episode reward: -183.144, mean reward: -1.813 [-100.000,  5.809], mean action: 1.525 [0.000, 3.000],  loss: 22.051030, mse: 79.963339, mean_q: 1.370940, mean_eps: 0.987727
   3836/300000: episode: 42, duration: 0.508s, episode steps:  66, steps per second: 130, episode reward: -144.545, mean reward: -2.190 [-100.000, 72.153], mean action: 1.682 [0.000, 3.000],  loss: 28.119815, mse: 75.771977, mean_q: 1.419841, mean_eps: 0.987452
   3921/300000: episode: 43, duration: 0.718s, episode steps:  85, steps per second: 118, episode reward: -94.051, mean reward: -1.106 [-100.000, 12.374], mean action: 1.365 [0.000, 3.000],  loss: 27.452631, mse: 68.393423, mean_q: 1.602039, mean_eps: 0.987203
   3992/300000: episode: 44, duration: 0.610s, episode steps:  71, steps per second: 116, episode reward: -252.170, mean reward: -3.552 [-100.000, 52.871], mean action: 1.507 [0.000, 3.000],  loss: 29.990764, mse: 66.408252, mean_q: 1.640874, mean_eps: 0.986945
   4133/300000: episode: 45, duration: 1.149s, episode steps: 141, steps per second: 123, episode reward: -140.196, mean reward: -0.994 [-100.000, 11.717], mean action: 1.496 [0.000, 3.000],  loss: 23.128802, mse: 90.608080, mean_q: 1.917510, mean_eps: 0.986595
   4258/300000: episode: 46, duration: 0.837s, episode steps: 125, steps per second: 149, episode reward: -221.520, mean reward: -1.772 [-100.000, 12.697], mean action: 1.552 [0.000, 3.000],  loss: 18.420886, mse: 80.387515, mean_q: 1.853343, mean_eps: 0.986157
   4381/300000: episode: 47, duration: 0.794s, episode steps: 123, steps per second: 155, episode reward: -102.858, mean reward: -0.836 [-100.000, 36.202], mean action: 1.634 [0.000, 3.000],  loss: 25.628809, mse: 92.837636, mean_q: 1.743766, mean_eps: 0.985747
   4488/300000: episode: 48, duration: 0.790s, episode steps: 107, steps per second: 135, episode reward: -6.277, mean reward: -0.059 [-100.000, 116.408], mean action: 1.486 [0.000, 3.000],  loss: 17.898428, mse: 91.430885, mean_q: 1.924394, mean_eps: 0.985368
   4580/300000: episode: 49, duration: 0.631s, episode steps:  92, steps per second: 146, episode reward: -145.635, mean reward: -1.583 [-100.000, 15.494], mean action: 1.609 [0.000, 3.000],  loss: 22.203273, mse: 97.859356, mean_q: 1.817190, mean_eps: 0.985039
   4663/300000: episode: 50, duration: 0.588s, episode steps:  83, steps per second: 141, episode reward: -129.073, mean reward: -1.555 [-100.000, 11.252], mean action: 1.651 [0.000, 3.000],  loss: 25.203037, mse: 100.842287, mean_q: 1.587818, mean_eps: 0.984751
   4775/300000: episode: 51, duration: 0.777s, episode steps: 112, steps per second: 144, episode reward: -343.804, mean reward: -3.070 [-100.000, 16.467], mean action: 1.455 [0.000, 3.000],  loss: 23.741958, mse: 106.989698, mean_q: 1.732773, mean_eps: 0.984429
   4857/300000: episode: 52, duration: 0.583s, episode steps:  82, steps per second: 141, episode reward: -207.055, mean reward: -2.525 [-100.000,  4.228], mean action: 1.659 [0.000, 3.000],  loss: 23.224162, mse: 89.728578, mean_q: 1.684118, mean_eps: 0.984109
   4922/300000: episode: 53, duration: 0.420s, episode steps:  65, steps per second: 155, episode reward: -73.833, mean reward: -1.136 [-100.000, 10.154], mean action: 1.246 [0.000, 3.000],  loss: 17.182007, mse: 92.637125, mean_q: 1.788512, mean_eps: 0.983866
   5052/300000: episode: 54, duration: 0.892s, episode steps: 130, steps per second: 146, episode reward: -82.358, mean reward: -0.634 [-100.000, 13.370], mean action: 1.523 [0.000, 3.000],  loss: 22.178225, mse: 107.994553, mean_q: 1.780112, mean_eps: 0.983545
   5125/300000: episode: 55, duration: 0.535s, episode steps:  73, steps per second: 136, episode reward: -121.938, mean reward: -1.670 [-100.000,  8.491], mean action: 1.603 [0.000, 3.000],  loss: 17.115659, mse: 124.848602, mean_q: 2.189037, mean_eps: 0.983210
   5224/300000: episode: 56, duration: 0.626s, episode steps:  99, steps per second: 158, episode reward: -422.388, mean reward: -4.267 [-100.000,  3.738], mean action: 1.556 [0.000, 3.000],  loss: 19.258746, mse: 138.786231, mean_q: 2.182250, mean_eps: 0.982926
   5332/300000: episode: 57, duration: 0.691s, episode steps: 108, steps per second: 156, episode reward: -368.496, mean reward: -3.412 [-100.000,  0.761], mean action: 1.435 [0.000, 3.000],  loss: 17.959038, mse: 118.143198, mean_q: 2.030141, mean_eps: 0.982584
   5408/300000: episode: 58, duration: 0.511s, episode steps:  76, steps per second: 149, episode reward: -167.203, mean reward: -2.200 [-100.000,  7.717], mean action: 1.434 [0.000, 3.000],  loss: 21.398105, mse: 129.471150, mean_q: 1.722807, mean_eps: 0.982281
   5472/300000: episode: 59, duration: 0.409s, episode steps:  64, steps per second: 156, episode reward: -127.858, mean reward: -1.998 [-100.000,  6.509], mean action: 1.609 [0.000, 3.000],  loss: 24.133076, mse: 134.466246, mean_q: 1.866784, mean_eps: 0.982050
   5554/300000: episode: 60, duration: 0.544s, episode steps:  82, steps per second: 151, episode reward: -293.892, mean reward: -3.584 [-100.000,  6.186], mean action: 1.427 [0.000, 3.000],  loss: 18.452274, mse: 125.127889, mean_q: 1.944971, mean_eps: 0.981809
   5622/300000: episode: 61, duration: 0.449s, episode steps:  68, steps per second: 152, episode reward: -91.333, mean reward: -1.343 [-100.000, 10.204], mean action: 1.544 [0.000, 3.000],  loss: 18.876530, mse: 150.327920, mean_q: 1.815258, mean_eps: 0.981561
   5724/300000: episode: 62, duration: 0.683s, episode steps: 102, steps per second: 149, episode reward: -297.203, mean reward: -2.914 [-100.000,  2.688], mean action: 1.186 [0.000, 3.000],  loss: 19.716748, mse: 137.669451, mean_q: 1.675393, mean_eps: 0.981281
   5850/300000: episode: 63, duration: 0.807s, episode steps: 126, steps per second: 156, episode reward: -201.714, mean reward: -1.601 [-100.000,  7.629], mean action: 1.500 [0.000, 3.000],  loss: 20.019404, mse: 139.883227, mean_q: 1.375315, mean_eps: 0.980905
   5916/300000: episode: 64, duration: 0.499s, episode steps:  66, steps per second: 132, episode reward: -217.935, mean reward: -3.302 [-100.000, 16.562], mean action: 1.273 [0.000, 3.000],  loss: 18.682487, mse: 139.795476, mean_q: 1.376906, mean_eps: 0.980588
   6005/300000: episode: 65, duration: 0.714s, episode steps:  89, steps per second: 125, episode reward: -82.778, mean reward: -0.930 [-100.000, 15.319], mean action: 1.551 [0.000, 3.000],  loss: 16.193360, mse: 124.181400, mean_q: 1.570925, mean_eps: 0.980332
   6094/300000: episode: 66, duration: 0.775s, episode steps:  89, steps per second: 115, episode reward: -132.281, mean reward: -1.486 [-100.000,  8.045], mean action: 1.506 [0.000, 3.000],  loss: 22.367122, mse: 156.887522, mean_q: 1.760047, mean_eps: 0.980038
   6201/300000: episode: 67, duration: 0.842s, episode steps: 107, steps per second: 127, episode reward: -112.034, mean reward: -1.047 [-100.000, 12.587], mean action: 1.449 [0.000, 3.000],  loss: 19.550145, mse: 145.189989, mean_q: 2.008466, mean_eps: 0.979715
   6279/300000: episode: 68, duration: 0.624s, episode steps:  78, steps per second: 125, episode reward: -363.452, mean reward: -4.660 [-100.000,  0.847], mean action: 1.500 [0.000, 3.000],  loss: 12.486192, mse: 135.104458, mean_q: 2.066759, mean_eps: 0.979410
   6382/300000: episode: 69, duration: 0.728s, episode steps: 103, steps per second: 141, episode reward: 16.284, mean reward:  0.158 [-100.000, 80.299], mean action: 1.592 [0.000, 3.000],  loss: 13.161086, mse: 149.551197, mean_q: 2.052229, mean_eps: 0.979111
   6463/300000: episode: 70, duration: 0.547s, episode steps:  81, steps per second: 148, episode reward: -198.466, mean reward: -2.450 [-100.000, 36.892], mean action: 1.630 [0.000, 3.000],  loss: 15.689875, mse: 150.938561, mean_q: 1.878175, mean_eps: 0.978807
   6562/300000: episode: 71, duration: 0.653s, episode steps:  99, steps per second: 152, episode reward: -154.140, mean reward: -1.557 [-100.000, 12.696], mean action: 1.616 [0.000, 3.000],  loss: 12.130871, mse: 146.204223, mean_q: 2.159149, mean_eps: 0.978510
   6664/300000: episode: 72, duration: 0.889s, episode steps: 102, steps per second: 115, episode reward: -222.053, mean reward: -2.177 [-100.000,  0.689], mean action: 1.500 [0.000, 3.000],  loss: 18.090254, mse: 153.144299, mean_q: 2.100250, mean_eps: 0.978179
   6730/300000: episode: 73, duration: 0.434s, episode steps:  66, steps per second: 152, episode reward: -98.331, mean reward: -1.490 [-100.000, 11.690], mean action: 1.500 [0.000, 3.000],  loss: 20.311182, mse: 145.374625, mean_q: 2.257640, mean_eps: 0.977902
   6847/300000: episode: 74, duration: 0.732s, episode steps: 117, steps per second: 160, episode reward: -117.261, mean reward: -1.002 [-100.000, 11.481], mean action: 1.291 [0.000, 3.000],  loss: 13.101216, mse: 137.947392, mean_q: 2.309161, mean_eps: 0.977600
   6950/300000: episode: 75, duration: 0.674s, episode steps: 103, steps per second: 153, episode reward: -166.951, mean reward: -1.621 [-100.000,  7.069], mean action: 1.583 [0.000, 3.000],  loss: 17.163146, mse: 156.518246, mean_q: 2.061035, mean_eps: 0.977237
   7021/300000: episode: 76, duration: 0.458s, episode steps:  71, steps per second: 155, episode reward: -203.968, mean reward: -2.873 [-100.000,  3.065], mean action: 1.535 [0.000, 3.000],  loss: 19.405643, mse: 163.339441, mean_q: 1.941417, mean_eps: 0.976949
   7131/300000: episode: 77, duration: 0.712s, episode steps: 110, steps per second: 154, episode reward: -195.757, mean reward: -1.780 [-100.000, 20.599], mean action: 1.509 [0.000, 3.000],  loss: 15.771761, mse: 181.565035, mean_q: 1.603643, mean_eps: 0.976651
   7229/300000: episode: 78, duration: 0.599s, episode steps:  98, steps per second: 164, episode reward: -150.207, mean reward: -1.533 [-100.000, 30.670], mean action: 1.357 [0.000, 3.000],  loss: 13.362847, mse: 185.073818, mean_q: 1.667086, mean_eps: 0.976308
   7332/300000: episode: 79, duration: 0.683s, episode steps: 103, steps per second: 151, episode reward: -108.314, mean reward: -1.052 [-100.000, 12.817], mean action: 1.621 [0.000, 3.000],  loss: 16.250897, mse: 201.870030, mean_q: 1.496901, mean_eps: 0.975976
   7398/300000: episode: 80, duration: 0.406s, episode steps:  66, steps per second: 162, episode reward: -145.402, mean reward: -2.203 [-100.000,  8.895], mean action: 1.409 [0.000, 3.000],  loss: 13.307347, mse: 165.102072, mean_q: 1.690715, mean_eps: 0.975697
   7498/300000: episode: 81, duration: 0.617s, episode steps: 100, steps per second: 162, episode reward: -286.016, mean reward: -2.860 [-100.000,  3.628], mean action: 1.510 [0.000, 3.000],  loss: 18.207812, mse: 191.912920, mean_q: 1.775930, mean_eps: 0.975423
   7626/300000: episode: 82, duration: 0.856s, episode steps: 128, steps per second: 150, episode reward: -130.527, mean reward: -1.020 [-100.000,  6.663], mean action: 1.445 [0.000, 3.000],  loss: 15.050126, mse: 186.757707, mean_q: 1.302040, mean_eps: 0.975047
   7706/300000: episode: 83, duration: 0.501s, episode steps:  80, steps per second: 160, episode reward: -219.585, mean reward: -2.745 [-100.000,  7.157], mean action: 1.650 [0.000, 3.000],  loss: 14.123952, mse: 200.797345, mean_q: 1.498581, mean_eps: 0.974704
   7786/300000: episode: 84, duration: 0.496s, episode steps:  80, steps per second: 161, episode reward: -94.756, mean reward: -1.184 [-100.000, 49.014], mean action: 1.637 [0.000, 3.000],  loss: 15.283862, mse: 165.097751, mean_q: 1.599245, mean_eps: 0.974440
   7937/300000: episode: 85, duration: 0.984s, episode steps: 151, steps per second: 153, episode reward: -304.439, mean reward: -2.016 [-100.000, 15.598], mean action: 1.589 [0.000, 3.000],  loss: 16.643499, mse: 187.785491, mean_q: 1.389854, mean_eps: 0.974059
   8073/300000: episode: 86, duration: 0.857s, episode steps: 136, steps per second: 159, episode reward: -79.023, mean reward: -0.581 [-100.000,  6.137], mean action: 1.404 [0.000, 3.000],  loss: 12.082414, mse: 188.122844, mean_q: 1.637552, mean_eps: 0.973585
   8178/300000: episode: 87, duration: 0.652s, episode steps: 105, steps per second: 161, episode reward: -298.153, mean reward: -2.840 [-100.000, 86.210], mean action: 1.590 [0.000, 3.000],  loss: 16.322651, mse: 232.486186, mean_q: 1.573176, mean_eps: 0.973187
   8278/300000: episode: 88, duration: 0.664s, episode steps: 100, steps per second: 151, episode reward: -136.554, mean reward: -1.366 [-100.000,  4.788], mean action: 1.450 [0.000, 3.000],  loss: 17.046129, mse: 265.928774, mean_q: 0.846819, mean_eps: 0.972849
   8356/300000: episode: 89, duration: 0.504s, episode steps:  78, steps per second: 155, episode reward: -128.813, mean reward: -1.651 [-100.000, 17.019], mean action: 1.321 [0.000, 3.000],  loss: 18.699101, mse: 238.103666, mean_q: 1.319413, mean_eps: 0.972556
   8478/300000: episode: 90, duration: 0.742s, episode steps: 122, steps per second: 164, episode reward: -138.842, mean reward: -1.138 [-100.000, 22.408], mean action: 1.500 [0.000, 3.000],  loss: 15.909702, mse: 227.576285, mean_q: 1.513380, mean_eps: 0.972226
   8587/300000: episode: 91, duration: 0.687s, episode steps: 109, steps per second: 159, episode reward: -126.794, mean reward: -1.163 [-100.000, 58.562], mean action: 1.532 [0.000, 3.000],  loss: 15.791031, mse: 225.790135, mean_q: 1.578363, mean_eps: 0.971844
   8673/300000: episode: 92, duration: 0.579s, episode steps:  86, steps per second: 149, episode reward: -113.106, mean reward: -1.315 [-100.000,  7.048], mean action: 1.477 [0.000, 3.000],  loss: 14.888667, mse: 253.306636, mean_q: 1.135195, mean_eps: 0.971523
   8744/300000: episode: 93, duration: 0.447s, episode steps:  71, steps per second: 159, episode reward: -80.690, mean reward: -1.136 [-100.000,  8.840], mean action: 1.662 [0.000, 3.000],  loss: 14.697182, mse: 242.566632, mean_q: 1.581933, mean_eps: 0.971264
   8845/300000: episode: 94, duration: 0.620s, episode steps: 101, steps per second: 163, episode reward: -182.345, mean reward: -1.805 [-100.000, 13.777], mean action: 1.515 [0.000, 3.000],  loss: 12.714277, mse: 228.854405, mean_q: 1.586554, mean_eps: 0.970980
   8948/300000: episode: 95, duration: 0.665s, episode steps: 103, steps per second: 155, episode reward: -435.472, mean reward: -4.228 [-100.000,  0.862], mean action: 1.476 [0.000, 3.000],  loss: 12.887776, mse: 220.111904, mean_q: 1.685470, mean_eps: 0.970643
   9017/300000: episode: 96, duration: 0.451s, episode steps:  69, steps per second: 153, episode reward: -212.934, mean reward: -3.086 [-100.000, 12.879], mean action: 1.391 [0.000, 3.000],  loss: 17.855659, mse: 234.586235, mean_q: 1.383877, mean_eps: 0.970359
   9125/300000: episode: 97, duration: 0.664s, episode steps: 108, steps per second: 163, episode reward: -277.064, mean reward: -2.565 [-100.000, 21.251], mean action: 1.509 [0.000, 3.000],  loss: 15.406246, mse: 246.469690, mean_q: 1.848223, mean_eps: 0.970067
   9203/300000: episode: 98, duration: 0.479s, episode steps:  78, steps per second: 163, episode reward: -132.438, mean reward: -1.698 [-100.000, 26.491], mean action: 1.577 [0.000, 3.000],  loss: 10.447009, mse: 245.280850, mean_q: 1.674777, mean_eps: 0.969760
   9314/300000: episode: 99, duration: 0.704s, episode steps: 111, steps per second: 158, episode reward: -352.585, mean reward: -3.176 [-100.000, 94.194], mean action: 1.441 [0.000, 3.000],  loss: 10.924098, mse: 241.665755, mean_q: 2.016408, mean_eps: 0.969449
   9380/300000: episode: 100, duration: 0.411s, episode steps:  66, steps per second: 160, episode reward: -78.094, mean reward: -1.183 [-100.000, 17.136], mean action: 1.348 [0.000, 3.000],  loss: 16.094731, mse: 254.507130, mean_q: 1.623778, mean_eps: 0.969157
   9464/300000: episode: 101, duration: 0.521s, episode steps:  84, steps per second: 161, episode reward: -71.055, mean reward: -0.846 [-100.000, 16.550], mean action: 1.488 [0.000, 3.000],  loss: 14.604510, mse: 263.067695, mean_q: 1.508147, mean_eps: 0.968909
   9577/300000: episode: 102, duration: 0.696s, episode steps: 113, steps per second: 162, episode reward: -295.097, mean reward: -2.611 [-100.000,  0.722], mean action: 1.558 [0.000, 3.000],  loss: 11.817694, mse: 259.413397, mean_q: 1.706829, mean_eps: 0.968584
   9674/300000: episode: 103, duration: 0.621s, episode steps:  97, steps per second: 156, episode reward: -89.449, mean reward: -0.922 [-100.000,  8.105], mean action: 1.577 [0.000, 3.000],  loss: 16.379242, mse: 257.422018, mean_q: 1.638782, mean_eps: 0.968237
   9780/300000: episode: 104, duration: 0.666s, episode steps: 106, steps per second: 159, episode reward: -445.541, mean reward: -4.203 [-100.000,  1.051], mean action: 1.613 [0.000, 3.000],  loss: 13.442346, mse: 240.335536, mean_q: 1.805730, mean_eps: 0.967903
   9911/300000: episode: 105, duration: 0.796s, episode steps: 131, steps per second: 165, episode reward: -211.715, mean reward: -1.616 [-100.000,  4.062], mean action: 1.443 [0.000, 3.000],  loss: 13.053551, mse: 251.963053, mean_q: 1.701001, mean_eps: 0.967511
   9986/300000: episode: 106, duration: 0.479s, episode steps:  75, steps per second: 156, episode reward: -114.478, mean reward: -1.526 [-100.000, 11.739], mean action: 1.467 [0.000, 3.000],  loss: 15.777410, mse: 260.987936, mean_q: 1.153555, mean_eps: 0.967172
  10056/300000: episode: 107, duration: 0.455s, episode steps:  70, steps per second: 154, episode reward: -102.357, mean reward: -1.462 [-100.000, 16.614], mean action: 1.357 [0.000, 3.000],  loss: 16.239461, mse: 288.356387, mean_q: 1.949341, mean_eps: 0.966932
  10160/300000: episode: 108, duration: 0.651s, episode steps: 104, steps per second: 160, episode reward: -29.618, mean reward: -0.285 [-100.000, 103.647], mean action: 1.404 [0.000, 3.000],  loss: 12.242150, mse: 285.286657, mean_q: 2.088220, mean_eps: 0.966645
  10281/300000: episode: 109, duration: 0.855s, episode steps: 121, steps per second: 142, episode reward: -119.234, mean reward: -0.985 [-100.000, 50.172], mean action: 1.322 [0.000, 3.000],  loss: 20.395552, mse: 298.689602, mean_q: 1.827085, mean_eps: 0.966274
  10370/300000: episode: 110, duration: 0.673s, episode steps:  89, steps per second: 132, episode reward: -145.972, mean reward: -1.640 [-100.000,  6.189], mean action: 1.562 [0.000, 3.000],  loss: 17.264411, mse: 304.979519, mean_q: 1.846239, mean_eps: 0.965927
  10484/300000: episode: 111, duration: 0.804s, episode steps: 114, steps per second: 142, episode reward: -140.860, mean reward: -1.236 [-100.000, 15.122], mean action: 1.465 [0.000, 3.000],  loss: 12.715438, mse: 295.565808, mean_q: 1.988006, mean_eps: 0.965593
  10599/300000: episode: 112, duration: 0.791s, episode steps: 115, steps per second: 145, episode reward: -331.962, mean reward: -2.887 [-100.000,  5.263], mean action: 1.435 [0.000, 3.000],  loss: 13.885488, mse: 283.736092, mean_q: 2.192856, mean_eps: 0.965215
  10676/300000: episode: 113, duration: 0.555s, episode steps:  77, steps per second: 139, episode reward: -121.459, mean reward: -1.577 [-100.000,  6.772], mean action: 1.688 [0.000, 3.000],  loss: 14.188435, mse: 306.600848, mean_q: 1.836160, mean_eps: 0.964898
  10765/300000: episode: 114, duration: 0.559s, episode steps:  89, steps per second: 159, episode reward: -72.688, mean reward: -0.817 [-100.000, 13.165], mean action: 1.506 [0.000, 3.000],  loss: 18.537410, mse: 316.308357, mean_q: 1.671499, mean_eps: 0.964624
  10849/300000: episode: 115, duration: 0.520s, episode steps:  84, steps per second: 161, episode reward: -156.109, mean reward: -1.858 [-100.000, 40.775], mean action: 1.548 [0.000, 3.000],  loss: 16.188713, mse: 288.960466, mean_q: 2.400201, mean_eps: 0.964339
  10950/300000: episode: 116, duration: 0.639s, episode steps: 101, steps per second: 158, episode reward: -279.379, mean reward: -2.766 [-100.000, 15.665], mean action: 1.584 [0.000, 3.000],  loss: 11.561851, mse: 276.129347, mean_q: 2.355548, mean_eps: 0.964033
  11079/300000: episode: 117, duration: 0.834s, episode steps: 129, steps per second: 155, episode reward: -235.146, mean reward: -1.823 [-100.000,  8.609], mean action: 1.419 [0.000, 3.000],  loss: 17.764376, mse: 332.474653, mean_q: 2.072494, mean_eps: 0.963654
  11181/300000: episode: 118, duration: 0.633s, episode steps: 102, steps per second: 161, episode reward: 43.656, mean reward:  0.428 [-100.000, 105.469], mean action: 1.235 [0.000, 3.000],  loss: 17.731304, mse: 367.384630, mean_q: 2.579502, mean_eps: 0.963273
  11245/300000: episode: 119, duration: 0.394s, episode steps:  64, steps per second: 162, episode reward: -100.840, mean reward: -1.576 [-100.000,  8.629], mean action: 1.766 [0.000, 3.000],  loss: 16.975672, mse: 351.036820, mean_q: 2.500616, mean_eps: 0.962999
  11339/300000: episode: 120, duration: 0.661s, episode steps:  94, steps per second: 142, episode reward: 19.557, mean reward:  0.208 [-100.000, 98.444], mean action: 1.681 [0.000, 3.000],  loss: 15.750954, mse: 382.671699, mean_q: 1.988738, mean_eps: 0.962738
  11411/300000: episode: 121, duration: 0.466s, episode steps:  72, steps per second: 155, episode reward: -63.155, mean reward: -0.877 [-100.000,  7.353], mean action: 1.569 [0.000, 3.000],  loss: 13.516816, mse: 349.414701, mean_q: 2.335626, mean_eps: 0.962464
  11496/300000: episode: 122, duration: 0.532s, episode steps:  85, steps per second: 160, episode reward: -244.532, mean reward: -2.877 [-100.000, 10.575], mean action: 1.647 [0.000, 3.000],  loss: 20.088061, mse: 373.448530, mean_q: 2.188853, mean_eps: 0.962205
  11598/300000: episode: 123, duration: 0.634s, episode steps: 102, steps per second: 161, episode reward: -387.669, mean reward: -3.801 [-100.000, 105.772], mean action: 1.373 [0.000, 3.000],  loss: 15.107651, mse: 331.605159, mean_q: 2.525609, mean_eps: 0.961897
  11687/300000: episode: 124, duration: 0.598s, episode steps:  89, steps per second: 149, episode reward: -209.025, mean reward: -2.349 [-100.000,  8.341], mean action: 1.584 [0.000, 3.000],  loss: 17.578878, mse: 371.243034, mean_q: 2.198227, mean_eps: 0.961581
  11800/300000: episode: 125, duration: 0.741s, episode steps: 113, steps per second: 152, episode reward: -187.851, mean reward: -1.662 [-100.000,  4.887], mean action: 1.681 [0.000, 3.000],  loss: 15.481957, mse: 358.670035, mean_q: 2.282702, mean_eps: 0.961248
  11874/300000: episode: 126, duration: 0.462s, episode steps:  74, steps per second: 160, episode reward: -83.697, mean reward: -1.131 [-100.000,  6.685], mean action: 1.649 [0.000, 3.000],  loss: 14.117626, mse: 327.318178, mean_q: 2.384661, mean_eps: 0.960940
  11956/300000: episode: 127, duration: 0.557s, episode steps:  82, steps per second: 147, episode reward: -104.322, mean reward: -1.272 [-100.000, 16.096], mean action: 1.634 [0.000, 3.000],  loss: 16.872353, mse: 345.531066, mean_q: 2.620708, mean_eps: 0.960682
  12049/300000: episode: 128, duration: 0.619s, episode steps:  93, steps per second: 150, episode reward: -132.549, mean reward: -1.425 [-100.000, 21.926], mean action: 1.667 [0.000, 3.000],  loss: 15.877746, mse: 382.465007, mean_q: 2.438896, mean_eps: 0.960393
  12119/300000: episode: 129, duration: 0.440s, episode steps:  70, steps per second: 159, episode reward: -65.699, mean reward: -0.939 [-100.000,  7.195], mean action: 1.629 [0.000, 3.000],  loss: 13.625226, mse: 405.020210, mean_q: 2.991274, mean_eps: 0.960124
  12200/300000: episode: 130, duration: 0.494s, episode steps:  81, steps per second: 164, episode reward: -96.791, mean reward: -1.195 [-100.000,  9.051], mean action: 1.432 [0.000, 3.000],  loss: 12.023633, mse: 434.982178, mean_q: 2.519936, mean_eps: 0.959875
  12272/300000: episode: 131, duration: 0.469s, episode steps:  72, steps per second: 153, episode reward: -77.120, mean reward: -1.071 [-100.000,  5.944], mean action: 1.903 [0.000, 3.000],  loss: 17.079015, mse: 428.111989, mean_q: 2.686607, mean_eps: 0.959623
  12363/300000: episode: 132, duration: 0.643s, episode steps:  91, steps per second: 142, episode reward: -289.183, mean reward: -3.178 [-100.000,  0.314], mean action: 1.429 [0.000, 3.000],  loss: 16.397100, mse: 386.008631, mean_q: 3.148571, mean_eps: 0.959354
  12477/300000: episode: 133, duration: 0.837s, episode steps: 114, steps per second: 136, episode reward: -127.854, mean reward: -1.122 [-100.000, 16.536], mean action: 1.605 [0.000, 3.000],  loss: 13.908339, mse: 420.930873, mean_q: 2.834261, mean_eps: 0.959016
  12535/300000: episode: 134, duration: 0.424s, episode steps:  58, steps per second: 137, episode reward: -188.958, mean reward: -3.258 [-100.000,  5.548], mean action: 1.517 [0.000, 3.000],  loss: 9.305879, mse: 415.724215, mean_q: 2.614715, mean_eps: 0.958732
  12616/300000: episode: 135, duration: 0.635s, episode steps:  81, steps per second: 128, episode reward: -106.820, mean reward: -1.319 [-100.000, 11.119], mean action: 1.642 [0.000, 3.000],  loss: 14.194046, mse: 435.422309, mean_q: 2.944711, mean_eps: 0.958503
  12672/300000: episode: 136, duration: 0.425s, episode steps:  56, steps per second: 132, episode reward: -108.120, mean reward: -1.931 [-100.000, 18.819], mean action: 1.500 [0.000, 3.000],  loss: 16.286757, mse: 452.024717, mean_q: 2.205671, mean_eps: 0.958276
  12748/300000: episode: 137, duration: 0.557s, episode steps:  76, steps per second: 136, episode reward: -115.317, mean reward: -1.517 [-100.000, 14.920], mean action: 1.645 [0.000, 3.000],  loss: 17.548723, mse: 407.604662, mean_q: 3.190501, mean_eps: 0.958059
  12852/300000: episode: 138, duration: 0.764s, episode steps: 104, steps per second: 136, episode reward: -170.026, mean reward: -1.635 [-100.000, 40.995], mean action: 1.519 [0.000, 3.000],  loss: 14.622893, mse: 417.934047, mean_q: 2.963517, mean_eps: 0.957762
  12918/300000: episode: 139, duration: 0.574s, episode steps:  66, steps per second: 115, episode reward: -105.915, mean reward: -1.605 [-100.000, 11.494], mean action: 1.348 [0.000, 3.000],  loss: 11.174521, mse: 409.425696, mean_q: 2.978530, mean_eps: 0.957481
  13011/300000: episode: 140, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -260.214, mean reward: -2.798 [-100.000,  7.423], mean action: 1.355 [0.000, 3.000],  loss: 13.943148, mse: 411.541231, mean_q: 2.981914, mean_eps: 0.957219
  13097/300000: episode: 141, duration: 0.551s, episode steps:  86, steps per second: 156, episode reward: -137.622, mean reward: -1.600 [-100.000, 34.846], mean action: 1.500 [0.000, 3.000],  loss: 19.872963, mse: 489.127358, mean_q: 3.162229, mean_eps: 0.956923
  13225/300000: episode: 142, duration: 0.855s, episode steps: 128, steps per second: 150, episode reward: -246.932, mean reward: -1.929 [-100.000, 49.890], mean action: 1.406 [0.000, 3.000],  loss: 16.929490, mse: 499.316841, mean_q: 3.293284, mean_eps: 0.956570
  13319/300000: episode: 143, duration: 0.588s, episode steps:  94, steps per second: 160, episode reward: -218.161, mean reward: -2.321 [-100.000,  5.934], mean action: 1.585 [0.000, 3.000],  loss: 13.119964, mse: 479.948591, mean_q: 3.666561, mean_eps: 0.956204
  13392/300000: episode: 144, duration: 0.457s, episode steps:  73, steps per second: 160, episode reward: -87.305, mean reward: -1.196 [-100.000,  8.719], mean action: 1.370 [0.000, 3.000],  loss: 21.943398, mse: 488.819108, mean_q: 3.368388, mean_eps: 0.955929
  13464/300000: episode: 145, duration: 0.459s, episode steps:  72, steps per second: 157, episode reward: -113.758, mean reward: -1.580 [-100.000, 15.349], mean action: 1.625 [0.000, 3.000],  loss: 14.913853, mse: 517.775364, mean_q: 3.210242, mean_eps: 0.955689
  13565/300000: episode: 146, duration: 0.685s, episode steps: 101, steps per second: 147, episode reward: -187.461, mean reward: -1.856 [-100.000, 65.338], mean action: 1.446 [0.000, 3.000],  loss: 12.934286, mse: 455.667145, mean_q: 3.711816, mean_eps: 0.955404
  13651/300000: episode: 147, duration: 0.547s, episode steps:  86, steps per second: 157, episode reward: -109.418, mean reward: -1.272 [-100.000,  7.633], mean action: 1.349 [0.000, 3.000],  loss: 11.194532, mse: 473.929735, mean_q: 3.466905, mean_eps: 0.955095
  13732/300000: episode: 148, duration: 0.510s, episode steps:  81, steps per second: 159, episode reward: -139.595, mean reward: -1.723 [-100.000, 40.329], mean action: 1.519 [0.000, 3.000],  loss: 10.513906, mse: 492.430291, mean_q: 3.227368, mean_eps: 0.954820
  13838/300000: episode: 149, duration: 0.667s, episode steps: 106, steps per second: 159, episode reward: -280.838, mean reward: -2.649 [-100.000,  3.160], mean action: 1.538 [0.000, 3.000],  loss: 12.002375, mse: 515.467438, mean_q: 3.537909, mean_eps: 0.954511
  13911/300000: episode: 150, duration: 0.497s, episode steps:  73, steps per second: 147, episode reward: -169.359, mean reward: -2.320 [-100.000, 10.754], mean action: 1.534 [0.000, 3.000],  loss: 18.520880, mse: 518.149209, mean_q: 2.904554, mean_eps: 0.954216
  14025/300000: episode: 151, duration: 0.721s, episode steps: 114, steps per second: 158, episode reward: -229.164, mean reward: -2.010 [-100.000,  1.305], mean action: 1.351 [0.000, 3.000],  loss: 17.490195, mse: 481.426102, mean_q: 4.193922, mean_eps: 0.953907
  14146/300000: episode: 152, duration: 0.750s, episode steps: 121, steps per second: 161, episode reward: -79.939, mean reward: -0.661 [-100.000, 19.898], mean action: 1.430 [0.000, 3.000],  loss: 17.254893, mse: 538.069258, mean_q: 4.013665, mean_eps: 0.953520
  14207/300000: episode: 153, duration: 0.405s, episode steps:  61, steps per second: 151, episode reward: -59.201, mean reward: -0.971 [-100.000, 14.725], mean action: 1.574 [0.000, 3.000],  loss: 10.784183, mse: 507.441482, mean_q: 5.036561, mean_eps: 0.953219
  14289/300000: episode: 154, duration: 0.537s, episode steps:  82, steps per second: 153, episode reward: -118.995, mean reward: -1.451 [-100.000, 14.862], mean action: 1.366 [0.000, 3.000],  loss: 15.989024, mse: 512.825389, mean_q: 4.728620, mean_eps: 0.952983
  14388/300000: episode: 155, duration: 0.621s, episode steps:  99, steps per second: 159, episode reward: -93.722, mean reward: -0.947 [-100.000, 12.423], mean action: 1.525 [0.000, 3.000],  loss: 15.488373, mse: 551.359909, mean_q: 3.985641, mean_eps: 0.952685
  14490/300000: episode: 156, duration: 0.634s, episode steps: 102, steps per second: 161, episode reward: -237.510, mean reward: -2.329 [-100.000,  9.049], mean action: 1.510 [0.000, 3.000],  loss: 18.096707, mse: 529.788375, mean_q: 4.687987, mean_eps: 0.952353
  14567/300000: episode: 157, duration: 0.511s, episode steps:  77, steps per second: 151, episode reward: -87.042, mean reward: -1.130 [-100.000,  8.876], mean action: 1.299 [0.000, 3.000],  loss: 12.938247, mse: 541.766707, mean_q: 4.665547, mean_eps: 0.952058
  14657/300000: episode: 158, duration: 0.566s, episode steps:  90, steps per second: 159, episode reward: -94.863, mean reward: -1.054 [-100.000, 17.832], mean action: 1.511 [0.000, 3.000],  loss: 11.620098, mse: 483.547636, mean_q: 4.811043, mean_eps: 0.951782
  14736/300000: episode: 159, duration: 0.492s, episode steps:  79, steps per second: 161, episode reward: -84.103, mean reward: -1.065 [-100.000, 10.584], mean action: 1.367 [0.000, 3.000],  loss: 9.294495, mse: 519.483657, mean_q: 4.573742, mean_eps: 0.951503
  14810/300000: episode: 160, duration: 0.466s, episode steps:  74, steps per second: 159, episode reward: -309.461, mean reward: -4.182 [-100.000, 112.276], mean action: 1.541 [0.000, 3.000],  loss: 14.245522, mse: 526.762632, mean_q: 4.880223, mean_eps: 0.951251
  14878/300000: episode: 161, duration: 0.438s, episode steps:  68, steps per second: 155, episode reward: -101.481, mean reward: -1.492 [-100.000, 10.169], mean action: 1.338 [0.000, 3.000],  loss: 13.042510, mse: 529.115317, mean_q: 4.975087, mean_eps: 0.951016
  14976/300000: episode: 162, duration: 0.643s, episode steps:  98, steps per second: 152, episode reward: -102.809, mean reward: -1.049 [-100.000, 10.596], mean action: 1.551 [0.000, 3.000],  loss: 13.407859, mse: 542.219252, mean_q: 4.528172, mean_eps: 0.950743
  15052/300000: episode: 163, duration: 0.482s, episode steps:  76, steps per second: 158, episode reward: -249.279, mean reward: -3.280 [-100.000,  4.761], mean action: 1.803 [0.000, 3.000],  loss: 12.565475, mse: 572.951854, mean_q: 5.472881, mean_eps: 0.950455
  15113/300000: episode: 164, duration: 0.387s, episode steps:  61, steps per second: 158, episode reward: -4.521, mean reward: -0.074 [-100.000, 81.801], mean action: 1.541 [0.000, 3.000],  loss: 13.815226, mse: 640.308309, mean_q: 5.030409, mean_eps: 0.950229
  15213/300000: episode: 165, duration: 0.640s, episode steps: 100, steps per second: 156, episode reward: -33.900, mean reward: -0.339 [-100.000, 96.176], mean action: 1.660 [0.000, 3.000],  loss: 13.143611, mse: 637.908539, mean_q: 4.421800, mean_eps: 0.949964
  15359/300000: episode: 166, duration: 0.969s, episode steps: 146, steps per second: 151, episode reward: -155.781, mean reward: -1.067 [-100.000,  5.893], mean action: 1.562 [0.000, 3.000],  loss: 14.979734, mse: 603.788955, mean_q: 5.262119, mean_eps: 0.949558
  15467/300000: episode: 167, duration: 0.711s, episode steps: 108, steps per second: 152, episode reward: -172.831, mean reward: -1.600 [-100.000, 11.599], mean action: 1.463 [0.000, 3.000],  loss: 11.631624, mse: 577.863896, mean_q: 5.598074, mean_eps: 0.949139
  15576/300000: episode: 168, duration: 0.763s, episode steps: 109, steps per second: 143, episode reward: -207.351, mean reward: -1.902 [-100.000,  3.644], mean action: 1.725 [0.000, 3.000],  loss: 16.015598, mse: 600.366444, mean_q: 4.990315, mean_eps: 0.948781
  15723/300000: episode: 169, duration: 1.083s, episode steps: 147, steps per second: 136, episode reward: -7.022, mean reward: -0.048 [-100.000, 84.456], mean action: 1.612 [0.000, 3.000],  loss: 13.232931, mse: 563.898525, mean_q: 5.650652, mean_eps: 0.948358
  15828/300000: episode: 170, duration: 0.793s, episode steps: 105, steps per second: 132, episode reward: -133.582, mean reward: -1.272 [-100.000,  6.605], mean action: 1.457 [0.000, 3.000],  loss: 16.773322, mse: 564.105366, mean_q: 5.252158, mean_eps: 0.947943
  15916/300000: episode: 171, duration: 0.681s, episode steps:  88, steps per second: 129, episode reward: -122.941, mean reward: -1.397 [-100.000,  7.169], mean action: 1.466 [0.000, 3.000],  loss: 13.667521, mse: 565.109778, mean_q: 5.709961, mean_eps: 0.947624
  16051/300000: episode: 172, duration: 0.970s, episode steps: 135, steps per second: 139, episode reward: -233.167, mean reward: -1.727 [-100.000, 31.719], mean action: 1.630 [0.000, 3.000],  loss: 15.084206, mse: 585.162213, mean_q: 5.264856, mean_eps: 0.947256
  16164/300000: episode: 173, duration: 0.933s, episode steps: 113, steps per second: 121, episode reward: -94.577, mean reward: -0.837 [-100.000, 14.680], mean action: 1.575 [0.000, 3.000],  loss: 15.890674, mse: 682.099032, mean_q: 5.105437, mean_eps: 0.946847
  16234/300000: episode: 174, duration: 0.547s, episode steps:  70, steps per second: 128, episode reward: -74.724, mean reward: -1.067 [-100.000,  9.443], mean action: 1.400 [0.000, 3.000],  loss: 16.768696, mse: 626.579504, mean_q: 6.219160, mean_eps: 0.946545
  16323/300000: episode: 175, duration: 0.641s, episode steps:  89, steps per second: 139, episode reward: -326.047, mean reward: -3.663 [-100.000, 66.413], mean action: 1.820 [0.000, 3.000],  loss: 13.068618, mse: 647.209037, mean_q: 5.585465, mean_eps: 0.946283
  16402/300000: episode: 176, duration: 0.545s, episode steps:  79, steps per second: 145, episode reward: -116.152, mean reward: -1.470 [-100.000, 11.230], mean action: 1.519 [0.000, 3.000],  loss: 20.321216, mse: 656.950345, mean_q: 5.719078, mean_eps: 0.946005
  16515/300000: episode: 177, duration: 0.766s, episode steps: 113, steps per second: 147, episode reward: -66.466, mean reward: -0.588 [-100.000, 12.469], mean action: 1.549 [0.000, 3.000],  loss: 12.712709, mse: 619.099759, mean_q: 6.236346, mean_eps: 0.945689
  16574/300000: episode: 178, duration: 0.399s, episode steps:  59, steps per second: 148, episode reward: -185.579, mean reward: -3.145 [-100.000,  6.720], mean action: 1.542 [0.000, 3.000],  loss: 10.861844, mse: 574.245359, mean_q: 6.603439, mean_eps: 0.945405
  16669/300000: episode: 179, duration: 0.670s, episode steps:  95, steps per second: 142, episode reward: -182.749, mean reward: -1.924 [-100.000, 15.037], mean action: 1.600 [0.000, 3.000],  loss: 14.802002, mse: 650.979574, mean_q: 6.177392, mean_eps: 0.945151
  16789/300000: episode: 180, duration: 0.821s, episode steps: 120, steps per second: 146, episode reward: -36.474, mean reward: -0.304 [-100.000, 19.965], mean action: 1.550 [0.000, 3.000],  loss: 9.251786, mse: 660.197218, mean_q: 5.882977, mean_eps: 0.944796
  16908/300000: episode: 181, duration: 0.805s, episode steps: 119, steps per second: 148, episode reward: -159.799, mean reward: -1.343 [-100.000,  7.775], mean action: 1.437 [0.000, 3.000],  loss: 15.343451, mse: 643.816251, mean_q: 6.183069, mean_eps: 0.944402
  17005/300000: episode: 182, duration: 0.623s, episode steps:  97, steps per second: 156, episode reward: -120.278, mean reward: -1.240 [-100.000,  6.081], mean action: 1.536 [0.000, 3.000],  loss: 18.821781, mse: 598.278766, mean_q: 7.219472, mean_eps: 0.944045
  17068/300000: episode: 183, duration: 0.445s, episode steps:  63, steps per second: 142, episode reward: -91.813, mean reward: -1.457 [-100.000,  7.311], mean action: 1.698 [0.000, 3.000],  loss: 19.574891, mse: 729.022072, mean_q: 5.722203, mean_eps: 0.943781
  17161/300000: episode: 184, duration: 0.599s, episode steps:  93, steps per second: 155, episode reward: -262.672, mean reward: -2.824 [-100.000,  5.677], mean action: 1.677 [0.000, 3.000],  loss: 18.253093, mse: 713.778503, mean_q: 6.527807, mean_eps: 0.943524
  17260/300000: episode: 185, duration: 0.637s, episode steps:  99, steps per second: 155, episode reward: -12.567, mean reward: -0.127 [-100.000, 70.905], mean action: 1.394 [0.000, 3.000],  loss: 22.598409, mse: 711.464571, mean_q: 6.843925, mean_eps: 0.943207
  17420/300000: episode: 186, duration: 1.062s, episode steps: 160, steps per second: 151, episode reward: 27.402, mean reward:  0.171 [-100.000, 89.627], mean action: 1.694 [0.000, 3.000],  loss: 17.046656, mse: 722.705285, mean_q: 6.963367, mean_eps: 0.942780
  17495/300000: episode: 187, duration: 0.502s, episode steps:  75, steps per second: 150, episode reward: -108.143, mean reward: -1.442 [-100.000,  7.026], mean action: 1.333 [0.000, 3.000],  loss: 25.506750, mse: 707.182284, mean_q: 6.879626, mean_eps: 0.942392
  17579/300000: episode: 188, duration: 0.583s, episode steps:  84, steps per second: 144, episode reward: -123.827, mean reward: -1.474 [-100.000,  9.728], mean action: 1.488 [0.000, 3.000],  loss: 17.152692, mse: 732.776864, mean_q: 6.682844, mean_eps: 0.942130
  17689/300000: episode: 189, duration: 0.726s, episode steps: 110, steps per second: 151, episode reward: -339.482, mean reward: -3.086 [-100.000,  0.393], mean action: 1.655 [0.000, 3.000],  loss: 17.398568, mse: 687.488765, mean_q: 7.177381, mean_eps: 0.941809
  17771/300000: episode: 190, duration: 0.551s, episode steps:  82, steps per second: 149, episode reward: -122.009, mean reward: -1.488 [-100.000,  6.393], mean action: 1.451 [0.000, 3.000],  loss: 18.783476, mse: 732.578441, mean_q: 6.545664, mean_eps: 0.941493
  17867/300000: episode: 191, duration: 0.707s, episode steps:  96, steps per second: 136, episode reward: -108.433, mean reward: -1.130 [-100.000,  7.958], mean action: 1.625 [0.000, 3.000],  loss: 16.569325, mse: 709.024654, mean_q: 6.973649, mean_eps: 0.941199
  17995/300000: episode: 192, duration: 0.990s, episode steps: 128, steps per second: 129, episode reward: -117.249, mean reward: -0.916 [-100.000, 11.124], mean action: 1.461 [0.000, 3.000],  loss: 15.401528, mse: 718.283492, mean_q: 6.814004, mean_eps: 0.940829
  18086/300000: episode: 193, duration: 0.601s, episode steps:  91, steps per second: 151, episode reward: -363.596, mean reward: -3.996 [-100.000,  0.303], mean action: 1.484 [0.000, 3.000],  loss: 12.098014, mse: 765.061782, mean_q: 6.627896, mean_eps: 0.940468
  18157/300000: episode: 194, duration: 0.448s, episode steps:  71, steps per second: 158, episode reward: -132.204, mean reward: -1.862 [-100.000, 14.985], mean action: 1.394 [0.000, 3.000],  loss: 15.566889, mse: 814.495091, mean_q: 7.183536, mean_eps: 0.940201
  18251/300000: episode: 195, duration: 0.603s, episode steps:  94, steps per second: 156, episode reward: -80.026, mean reward: -0.851 [-100.000, 21.146], mean action: 1.426 [0.000, 3.000],  loss: 17.596292, mse: 772.157502, mean_q: 7.204960, mean_eps: 0.939928
  18324/300000: episode: 196, duration: 0.543s, episode steps:  73, steps per second: 135, episode reward: -94.233, mean reward: -1.291 [-100.000, 10.887], mean action: 1.616 [0.000, 3.000],  loss: 13.479733, mse: 761.691990, mean_q: 7.237631, mean_eps: 0.939653
  18422/300000: episode: 197, duration: 0.650s, episode steps:  98, steps per second: 151, episode reward: -122.754, mean reward: -1.253 [-100.000,  8.998], mean action: 1.418 [0.000, 3.000],  loss: 13.113390, mse: 761.758576, mean_q: 7.393662, mean_eps: 0.939371
  18506/300000: episode: 198, duration: 0.534s, episode steps:  84, steps per second: 157, episode reward: -155.456, mean reward: -1.851 [-100.000, 15.737], mean action: 1.452 [0.000, 3.000],  loss: 23.095069, mse: 774.907840, mean_q: 7.355611, mean_eps: 0.939070
  18582/300000: episode: 199, duration: 0.487s, episode steps:  76, steps per second: 156, episode reward: -47.205, mean reward: -0.621 [-100.000, 13.360], mean action: 1.539 [0.000, 3.000],  loss: 18.029412, mse: 775.600256, mean_q: 6.704545, mean_eps: 0.938806
  18645/300000: episode: 200, duration: 0.425s, episode steps:  63, steps per second: 148, episode reward: -77.283, mean reward: -1.227 [-100.000,  6.197], mean action: 1.238 [0.000, 3.000],  loss: 13.498998, mse: 796.094715, mean_q: 6.682206, mean_eps: 0.938577
  18715/300000: episode: 201, duration: 0.474s, episode steps:  70, steps per second: 148, episode reward: -32.712, mean reward: -0.467 [-100.000, 23.889], mean action: 1.529 [0.000, 3.000],  loss: 19.766472, mse: 767.663342, mean_q: 7.904334, mean_eps: 0.938358
  18791/300000: episode: 202, duration: 0.491s, episode steps:  76, steps per second: 155, episode reward: -88.917, mean reward: -1.170 [-100.000,  6.317], mean action: 1.382 [0.000, 3.000],  loss: 16.891512, mse: 766.856616, mean_q: 7.313195, mean_eps: 0.938117
  18872/300000: episode: 203, duration: 0.503s, episode steps:  81, steps per second: 161, episode reward: -116.838, mean reward: -1.442 [-100.000, 15.223], mean action: 1.593 [0.000, 3.000],  loss: 15.009683, mse: 773.055166, mean_q: 7.367540, mean_eps: 0.937858
  18986/300000: episode: 204, duration: 0.774s, episode steps: 114, steps per second: 147, episode reward: -106.908, mean reward: -0.938 [-100.000, 12.668], mean action: 1.456 [0.000, 3.000],  loss: 13.155510, mse: 781.112660, mean_q: 7.544759, mean_eps: 0.937536
  19049/300000: episode: 205, duration: 0.416s, episode steps:  63, steps per second: 151, episode reward: -83.435, mean reward: -1.324 [-100.000,  6.685], mean action: 1.317 [0.000, 3.000],  loss: 15.863465, mse: 790.010276, mean_q: 8.667021, mean_eps: 0.937244
  19118/300000: episode: 206, duration: 0.445s, episode steps:  69, steps per second: 155, episode reward: -87.072, mean reward: -1.262 [-100.000, 11.151], mean action: 1.638 [0.000, 3.000],  loss: 19.795775, mse: 814.697129, mean_q: 8.502773, mean_eps: 0.937026
  19235/300000: episode: 207, duration: 0.752s, episode steps: 117, steps per second: 156, episode reward: -268.932, mean reward: -2.299 [-100.000,  7.825], mean action: 1.359 [0.000, 3.000],  loss: 17.874680, mse: 816.039970, mean_q: 8.517238, mean_eps: 0.936719
  19340/300000: episode: 208, duration: 0.749s, episode steps: 105, steps per second: 140, episode reward: -120.548, mean reward: -1.148 [-100.000, 11.543], mean action: 1.590 [0.000, 3.000],  loss: 15.979900, mse: 856.732995, mean_q: 7.910125, mean_eps: 0.936353
  19405/300000: episode: 209, duration: 0.424s, episode steps:  65, steps per second: 153, episode reward: -65.035, mean reward: -1.001 [-100.000,  7.067], mean action: 1.246 [0.000, 3.000],  loss: 20.264528, mse: 821.465126, mean_q: 8.057142, mean_eps: 0.936072
  19516/300000: episode: 210, duration: 0.690s, episode steps: 111, steps per second: 161, episode reward: -179.877, mean reward: -1.621 [-100.000,  2.046], mean action: 1.577 [0.000, 3.000],  loss: 17.994472, mse: 817.491725, mean_q: 7.545431, mean_eps: 0.935782
  19587/300000: episode: 211, duration: 0.452s, episode steps:  71, steps per second: 157, episode reward: -74.243, mean reward: -1.046 [-100.000, 20.988], mean action: 1.535 [0.000, 3.000],  loss: 12.470293, mse: 861.525491, mean_q: 7.720894, mean_eps: 0.935482
  19694/300000: episode: 212, duration: 0.751s, episode steps: 107, steps per second: 142, episode reward: -134.213, mean reward: -1.254 [-100.000,  7.970], mean action: 1.458 [0.000, 3.000],  loss: 16.339504, mse: 804.515825, mean_q: 8.728581, mean_eps: 0.935188
  19787/300000: episode: 213, duration: 0.586s, episode steps:  93, steps per second: 159, episode reward: -111.792, mean reward: -1.202 [-100.000,  8.677], mean action: 1.237 [0.000, 3.000],  loss: 18.426711, mse: 819.556123, mean_q: 8.336068, mean_eps: 0.934858
  19878/300000: episode: 214, duration: 0.568s, episode steps:  91, steps per second: 160, episode reward: -122.347, mean reward: -1.344 [-100.000,  8.909], mean action: 1.582 [0.000, 3.000],  loss: 15.683355, mse: 816.013797, mean_q: 8.950304, mean_eps: 0.934554
  19942/300000: episode: 215, duration: 0.451s, episode steps:  64, steps per second: 142, episode reward: -86.525, mean reward: -1.352 [-100.000, 21.368], mean action: 1.469 [0.000, 3.000],  loss: 14.724703, mse: 844.117778, mean_q: 7.956591, mean_eps: 0.934299
  20056/300000: episode: 216, duration: 0.877s, episode steps: 114, steps per second: 130, episode reward: -76.493, mean reward: -0.671 [-100.000,  9.959], mean action: 1.465 [0.000, 3.000],  loss: 13.135496, mse: 880.227685, mean_q: 8.646457, mean_eps: 0.934005
  20132/300000: episode: 217, duration: 0.566s, episode steps:  76, steps per second: 134, episode reward: -183.238, mean reward: -2.411 [-100.000,  5.116], mean action: 1.329 [0.000, 3.000],  loss: 10.268294, mse: 848.410307, mean_q: 9.746371, mean_eps: 0.933691
  20206/300000: episode: 218, duration: 0.603s, episode steps:  74, steps per second: 123, episode reward: -85.488, mean reward: -1.155 [-100.000, 13.462], mean action: 1.554 [0.000, 3.000],  loss: 12.472954, mse: 856.859214, mean_q: 9.799056, mean_eps: 0.933444
  20313/300000: episode: 219, duration: 0.811s, episode steps: 107, steps per second: 132, episode reward: -154.132, mean reward: -1.440 [-100.000,  9.897], mean action: 1.477 [0.000, 3.000],  loss: 20.630002, mse: 887.981061, mean_q: 9.240761, mean_eps: 0.933145
  20438/300000: episode: 220, duration: 0.843s, episode steps: 125, steps per second: 148, episode reward: -208.581, mean reward: -1.669 [-100.000, 68.874], mean action: 1.384 [0.000, 3.000],  loss: 12.526715, mse: 905.089819, mean_q: 9.407241, mean_eps: 0.932763
  20524/300000: episode: 221, duration: 0.569s, episode steps:  86, steps per second: 151, episode reward: -39.949, mean reward: -0.465 [-100.000, 16.776], mean action: 1.547 [0.000, 3.000],  loss: 14.540726, mse: 935.200888, mean_q: 8.922742, mean_eps: 0.932414
  20622/300000: episode: 222, duration: 0.691s, episode steps:  98, steps per second: 142, episode reward: -252.681, mean reward: -2.578 [-100.000, 47.298], mean action: 1.490 [0.000, 3.000],  loss: 14.384558, mse: 893.281922, mean_q: 9.202663, mean_eps: 0.932111
  20688/300000: episode: 223, duration: 0.427s, episode steps:  66, steps per second: 155, episode reward: -92.072, mean reward: -1.395 [-100.000, 18.339], mean action: 1.439 [0.000, 3.000],  loss: 17.921066, mse: 851.723923, mean_q: 9.962839, mean_eps: 0.931840
  20819/300000: episode: 224, duration: 0.876s, episode steps: 131, steps per second: 150, episode reward: -229.083, mean reward: -1.749 [-100.000, 39.842], mean action: 1.656 [0.000, 3.000],  loss: 13.301647, mse: 864.029765, mean_q: 9.654474, mean_eps: 0.931515
  20902/300000: episode: 225, duration: 0.589s, episode steps:  83, steps per second: 141, episode reward: -125.423, mean reward: -1.511 [-100.000,  6.700], mean action: 1.482 [0.000, 3.000],  loss: 16.946225, mse: 896.852410, mean_q: 9.281147, mean_eps: 0.931162
  20994/300000: episode: 226, duration: 0.596s, episode steps:  92, steps per second: 154, episode reward: -110.848, mean reward: -1.205 [-100.000,  6.359], mean action: 1.489 [0.000, 3.000],  loss: 22.096672, mse: 869.481833, mean_q: 9.616830, mean_eps: 0.930873
  21087/300000: episode: 227, duration: 0.579s, episode steps:  93, steps per second: 161, episode reward: -95.867, mean reward: -1.031 [-100.000, 10.808], mean action: 1.591 [0.000, 3.000],  loss: 14.262787, mse: 925.845139, mean_q: 9.507162, mean_eps: 0.930568
  21153/300000: episode: 228, duration: 0.422s, episode steps:  66, steps per second: 156, episode reward: -81.173, mean reward: -1.230 [-100.000,  7.129], mean action: 1.212 [0.000, 3.000],  loss: 9.578999, mse: 928.711644, mean_q: 10.282535, mean_eps: 0.930306
  21276/300000: episode: 229, duration: 0.826s, episode steps: 123, steps per second: 149, episode reward: -179.331, mean reward: -1.458 [-100.000,  4.517], mean action: 1.431 [0.000, 3.000],  loss: 15.201728, mse: 937.526442, mean_q: 9.904914, mean_eps: 0.929994
  21376/300000: episode: 230, duration: 0.646s, episode steps: 100, steps per second: 155, episode reward: -139.988, mean reward: -1.400 [-100.000,  7.731], mean action: 1.360 [0.000, 3.000],  loss: 15.883130, mse: 954.350837, mean_q: 9.280701, mean_eps: 0.929626
  21442/300000: episode: 231, duration: 0.444s, episode steps:  66, steps per second: 149, episode reward: -131.784, mean reward: -1.997 [-100.000,  9.149], mean action: 1.682 [0.000, 3.000],  loss: 12.171118, mse: 953.372712, mean_q: 9.477695, mean_eps: 0.929352
  21562/300000: episode: 232, duration: 0.818s, episode steps: 120, steps per second: 147, episode reward: -75.345, mean reward: -0.628 [-100.000, 102.336], mean action: 1.492 [0.000, 3.000],  loss: 13.206939, mse: 949.070293, mean_q: 9.281828, mean_eps: 0.929045
  21627/300000: episode: 233, duration: 0.445s, episode steps:  65, steps per second: 146, episode reward: -71.508, mean reward: -1.100 [-100.000, 56.642], mean action: 1.508 [0.000, 3.000],  loss: 12.246165, mse: 979.706354, mean_q: 9.114230, mean_eps: 0.928740
  21758/300000: episode: 234, duration: 0.841s, episode steps: 131, steps per second: 156, episode reward: -220.549, mean reward: -1.684 [-100.000,  1.644], mean action: 1.656 [0.000, 3.000],  loss: 18.996416, mse: 922.194444, mean_q: 10.015572, mean_eps: 0.928416
  21851/300000: episode: 235, duration: 0.585s, episode steps:  93, steps per second: 159, episode reward: -230.408, mean reward: -2.478 [-100.000,  5.383], mean action: 1.527 [0.000, 3.000],  loss: 17.500071, mse: 900.067885, mean_q: 10.292363, mean_eps: 0.928047
  21928/300000: episode: 236, duration: 0.535s, episode steps:  77, steps per second: 144, episode reward: -69.670, mean reward: -0.905 [-100.000,  6.513], mean action: 1.545 [0.000, 3.000],  loss: 12.065238, mse: 938.767040, mean_q: 9.012231, mean_eps: 0.927766
  21996/300000: episode: 237, duration: 0.442s, episode steps:  68, steps per second: 154, episode reward: -78.581, mean reward: -1.156 [-100.000,  6.443], mean action: 1.515 [0.000, 3.000],  loss: 20.183318, mse: 942.055572, mean_q: 8.972572, mean_eps: 0.927527
  22069/300000: episode: 238, duration: 0.501s, episode steps:  73, steps per second: 146, episode reward: -142.524, mean reward: -1.952 [-100.000, 26.927], mean action: 1.795 [0.000, 3.000],  loss: 15.154513, mse: 995.438916, mean_q: 10.284888, mean_eps: 0.927294
  22145/300000: episode: 239, duration: 0.657s, episode steps:  76, steps per second: 116, episode reward: -332.077, mean reward: -4.369 [-100.000,  2.209], mean action: 1.566 [0.000, 3.000],  loss: 14.199464, mse: 974.089069, mean_q: 10.240911, mean_eps: 0.927049
  22247/300000: episode: 240, duration: 0.821s, episode steps: 102, steps per second: 124, episode reward: -93.510, mean reward: -0.917 [-100.000, 12.241], mean action: 1.265 [0.000, 3.000],  loss: 14.728086, mse: 998.930605, mean_q: 9.521009, mean_eps: 0.926755
  22356/300000: episode: 241, duration: 0.798s, episode steps: 109, steps per second: 137, episode reward: -227.176, mean reward: -2.084 [-100.000,  0.752], mean action: 1.495 [0.000, 3.000],  loss: 22.438170, mse: 990.167653, mean_q: 9.228713, mean_eps: 0.926407
  22417/300000: episode: 242, duration: 0.438s, episode steps:  61, steps per second: 139, episode reward: -167.563, mean reward: -2.747 [-100.000,  6.692], mean action: 1.590 [0.000, 3.000],  loss: 16.406660, mse: 957.766703, mean_q: 10.522049, mean_eps: 0.926126
  22490/300000: episode: 243, duration: 0.567s, episode steps:  73, steps per second: 129, episode reward: -138.901, mean reward: -1.903 [-100.000,  7.815], mean action: 1.562 [0.000, 3.000],  loss: 14.428913, mse: 988.305162, mean_q: 10.117790, mean_eps: 0.925905
  22610/300000: episode: 244, duration: 0.856s, episode steps: 120, steps per second: 140, episode reward: -106.124, mean reward: -0.884 [-100.000,  6.558], mean action: 1.558 [0.000, 3.000],  loss: 12.738179, mse: 972.303615, mean_q: 10.258654, mean_eps: 0.925587
  22682/300000: episode: 245, duration: 0.515s, episode steps:  72, steps per second: 140, episode reward: -87.836, mean reward: -1.220 [-100.000, 11.902], mean action: 1.556 [0.000, 3.000],  loss: 15.219152, mse: 1000.638652, mean_q: 10.762109, mean_eps: 0.925270
  22753/300000: episode: 246, duration: 0.562s, episode steps:  71, steps per second: 126, episode reward: -51.891, mean reward: -0.731 [-100.000, 18.517], mean action: 1.408 [0.000, 3.000],  loss: 13.577103, mse: 997.235554, mean_q: 9.806698, mean_eps: 0.925034
  22869/300000: episode: 247, duration: 0.808s, episode steps: 116, steps per second: 144, episode reward: -103.499, mean reward: -0.892 [-100.000, 16.871], mean action: 1.586 [0.000, 3.000],  loss: 18.891254, mse: 1002.947463, mean_q: 10.029119, mean_eps: 0.924725
  22971/300000: episode: 248, duration: 0.672s, episode steps: 102, steps per second: 152, episode reward: -124.786, mean reward: -1.223 [-100.000, 40.145], mean action: 1.539 [0.000, 3.000],  loss: 14.844309, mse: 964.653066, mean_q: 9.977769, mean_eps: 0.924366
  23045/300000: episode: 249, duration: 0.505s, episode steps:  74, steps per second: 147, episode reward: 25.005, mean reward:  0.338 [-100.000, 119.185], mean action: 1.270 [0.000, 3.000],  loss: 15.816182, mse: 1023.605450, mean_q: 10.866819, mean_eps: 0.924075
  23138/300000: episode: 250, duration: 0.632s, episode steps:  93, steps per second: 147, episode reward: -91.781, mean reward: -0.987 [-100.000,  7.866], mean action: 1.527 [0.000, 3.000],  loss: 12.460614, mse: 1046.581221, mean_q: 11.000069, mean_eps: 0.923800
  23227/300000: episode: 251, duration: 0.575s, episode steps:  89, steps per second: 155, episode reward: -100.999, mean reward: -1.135 [-100.000, 15.539], mean action: 1.652 [0.000, 3.000],  loss: 13.427530, mse: 1051.626286, mean_q: 11.863524, mean_eps: 0.923499
  23331/300000: episode: 252, duration: 0.673s, episode steps: 104, steps per second: 155, episode reward: -59.815, mean reward: -0.575 [-100.000, 22.744], mean action: 1.596 [0.000, 3.000],  loss: 18.209808, mse: 1076.664402, mean_q: 11.349337, mean_eps: 0.923181
  23452/300000: episode: 253, duration: 0.846s, episode steps: 121, steps per second: 143, episode reward: -95.626, mean reward: -0.790 [-100.000,  8.085], mean action: 1.744 [0.000, 3.000],  loss: 15.999004, mse: 1047.489854, mean_q: 11.371272, mean_eps: 0.922810
  23518/300000: episode: 254, duration: 0.444s, episode steps:  66, steps per second: 149, episode reward: -65.805, mean reward: -0.997 [-100.000, 11.747], mean action: 1.682 [0.000, 3.000],  loss: 13.543417, mse: 1097.317293, mean_q: 10.560004, mean_eps: 0.922501
  23623/300000: episode: 255, duration: 0.688s, episode steps: 105, steps per second: 153, episode reward: -422.988, mean reward: -4.028 [-100.000, 89.256], mean action: 1.486 [0.000, 3.000],  loss: 20.700180, mse: 994.886297, mean_q: 11.689247, mean_eps: 0.922219
  23720/300000: episode: 256, duration: 0.671s, episode steps:  97, steps per second: 144, episode reward: -107.819, mean reward: -1.112 [-100.000,  8.143], mean action: 1.536 [0.000, 3.000],  loss: 12.947982, mse: 1055.210181, mean_q: 10.967408, mean_eps: 0.921886
  23809/300000: episode: 257, duration: 0.592s, episode steps:  89, steps per second: 150, episode reward: -64.504, mean reward: -0.725 [-100.000, 13.279], mean action: 1.618 [0.000, 3.000],  loss: 11.873205, mse: 1085.698196, mean_q: 11.173097, mean_eps: 0.921579
  23932/300000: episode: 258, duration: 0.786s, episode steps: 123, steps per second: 157, episode reward: -134.422, mean reward: -1.093 [-100.000, 13.709], mean action: 1.537 [0.000, 3.000],  loss: 10.854685, mse: 1055.160487, mean_q: 11.306601, mean_eps: 0.921229
  24008/300000: episode: 259, duration: 0.478s, episode steps:  76, steps per second: 159, episode reward: -94.742, mean reward: -1.247 [-100.000, 11.700], mean action: 1.289 [0.000, 3.000],  loss: 16.835011, mse: 1080.410841, mean_q: 11.191945, mean_eps: 0.920901
  24114/300000: episode: 260, duration: 0.728s, episode steps: 106, steps per second: 146, episode reward: -237.359, mean reward: -2.239 [-100.000,  7.076], mean action: 1.575 [0.000, 3.000],  loss: 17.525717, mse: 1154.974145, mean_q: 11.729197, mean_eps: 0.920600
  24197/300000: episode: 261, duration: 0.540s, episode steps:  83, steps per second: 154, episode reward: -269.911, mean reward: -3.252 [-100.000, 30.294], mean action: 1.446 [0.000, 3.000],  loss: 13.052253, mse: 1194.488692, mean_q: 11.265160, mean_eps: 0.920288
  24282/300000: episode: 262, duration: 0.566s, episode steps:  85, steps per second: 150, episode reward: -198.873, mean reward: -2.340 [-100.000,  6.999], mean action: 1.541 [0.000, 3.000],  loss: 13.244993, mse: 1109.747308, mean_q: 11.754257, mean_eps: 0.920011
  24378/300000: episode: 263, duration: 0.667s, episode steps:  96, steps per second: 144, episode reward: -67.832, mean reward: -0.707 [-100.000, 25.392], mean action: 1.385 [0.000, 3.000],  loss: 12.805665, mse: 1155.638125, mean_q: 11.876605, mean_eps: 0.919713
  24462/300000: episode: 264, duration: 0.548s, episode steps:  84, steps per second: 153, episode reward: -380.619, mean reward: -4.531 [-100.000,  0.106], mean action: 1.571 [0.000, 3.000],  loss: 20.775194, mse: 1178.286173, mean_q: 11.768903, mean_eps: 0.919416
  24558/300000: episode: 265, duration: 0.638s, episode steps:  96, steps per second: 150, episode reward: 33.134, mean reward:  0.345 [-100.000, 113.976], mean action: 1.615 [0.000, 3.000],  loss: 14.610115, mse: 1141.605383, mean_q: 11.914188, mean_eps: 0.919119
  24650/300000: episode: 266, duration: 0.606s, episode steps:  92, steps per second: 152, episode reward: -239.522, mean reward: -2.604 [-100.000,  6.813], mean action: 1.533 [0.000, 3.000],  loss: 17.339015, mse: 1125.462313, mean_q: 11.802079, mean_eps: 0.918808
  24737/300000: episode: 267, duration: 0.600s, episode steps:  87, steps per second: 145, episode reward: -77.671, mean reward: -0.893 [-100.000, 10.579], mean action: 1.690 [0.000, 3.000],  loss: 12.895320, mse: 1171.101244, mean_q: 11.517267, mean_eps: 0.918513
  24834/300000: episode: 268, duration: 0.622s, episode steps:  97, steps per second: 156, episode reward: -202.893, mean reward: -2.092 [-100.000, 17.977], mean action: 1.639 [0.000, 3.000],  loss: 13.224464, mse: 1114.280472, mean_q: 12.046202, mean_eps: 0.918210
  24935/300000: episode: 269, duration: 0.638s, episode steps: 101, steps per second: 158, episode reward: -281.866, mean reward: -2.791 [-100.000, 64.911], mean action: 1.604 [0.000, 3.000],  loss: 21.247521, mse: 1088.573906, mean_q: 12.126283, mean_eps: 0.917883
  25051/300000: episode: 270, duration: 0.807s, episode steps: 116, steps per second: 144, episode reward: -150.478, mean reward: -1.297 [-100.000, 20.184], mean action: 1.586 [0.000, 3.000],  loss: 23.049176, mse: 1179.767735, mean_q: 11.635509, mean_eps: 0.917525
  25166/300000: episode: 271, duration: 0.773s, episode steps: 115, steps per second: 149, episode reward: -173.852, mean reward: -1.512 [-100.000, 12.077], mean action: 1.548 [0.000, 3.000],  loss: 22.021685, mse: 1138.519789, mean_q: 12.993085, mean_eps: 0.917144
  25235/300000: episode: 272, duration: 0.443s, episode steps:  69, steps per second: 156, episode reward: -70.359, mean reward: -1.020 [-100.000,  7.930], mean action: 1.681 [0.000, 3.000],  loss: 18.293007, mse: 1244.523087, mean_q: 11.986986, mean_eps: 0.916840
  25301/300000: episode: 273, duration: 0.428s, episode steps:  66, steps per second: 154, episode reward: -94.134, mean reward: -1.426 [-100.000, 16.384], mean action: 1.712 [0.000, 3.000],  loss: 17.346097, mse: 1191.185902, mean_q: 13.420508, mean_eps: 0.916617
  25402/300000: episode: 274, duration: 0.681s, episode steps: 101, steps per second: 148, episode reward: -191.730, mean reward: -1.898 [-100.000,  9.034], mean action: 1.455 [0.000, 3.000],  loss: 21.324869, mse: 1159.825749, mean_q: 12.499360, mean_eps: 0.916342
  25509/300000: episode: 275, duration: 0.701s, episode steps: 107, steps per second: 153, episode reward: -98.988, mean reward: -0.925 [-100.000,  8.774], mean action: 1.542 [0.000, 3.000],  loss: 17.582305, mse: 1193.469966, mean_q: 13.191114, mean_eps: 0.915998
  25639/300000: episode: 276, duration: 0.834s, episode steps: 130, steps per second: 156, episode reward: -174.001, mean reward: -1.338 [-100.000,  8.413], mean action: 1.546 [0.000, 3.000],  loss: 17.806592, mse: 1246.473916, mean_q: 12.271784, mean_eps: 0.915607
  25756/300000: episode: 277, duration: 0.813s, episode steps: 117, steps per second: 144, episode reward: -184.924, mean reward: -1.581 [-100.000, 19.171], mean action: 1.752 [0.000, 3.000],  loss: 17.273161, mse: 1218.154198, mean_q: 12.223879, mean_eps: 0.915200
  25839/300000: episode: 278, duration: 0.536s, episode steps:  83, steps per second: 155, episode reward: -125.336, mean reward: -1.510 [-100.000,  5.480], mean action: 1.723 [0.000, 3.000],  loss: 15.864201, mse: 1178.516553, mean_q: 12.059218, mean_eps: 0.914870
  25926/300000: episode: 279, duration: 0.566s, episode steps:  87, steps per second: 154, episode reward: -273.849, mean reward: -3.148 [-100.000, 47.734], mean action: 1.632 [0.000, 3.000],  loss: 18.944316, mse: 1220.705769, mean_q: 11.357270, mean_eps: 0.914589
  26020/300000: episode: 280, duration: 0.662s, episode steps:  94, steps per second: 142, episode reward: -82.603, mean reward: -0.879 [-100.000, 10.336], mean action: 1.628 [0.000, 3.000],  loss: 22.897122, mse: 1206.654736, mean_q: 12.940655, mean_eps: 0.914291
  26134/300000: episode: 281, duration: 0.742s, episode steps: 114, steps per second: 154, episode reward: -330.628, mean reward: -2.900 [-100.000, 114.905], mean action: 1.430 [0.000, 3.000],  loss: 16.289470, mse: 1240.952286, mean_q: 13.251311, mean_eps: 0.913948
  26218/300000: episode: 282, duration: 0.535s, episode steps:  84, steps per second: 157, episode reward: -72.516, mean reward: -0.863 [-100.000, 79.284], mean action: 1.548 [0.000, 3.000],  loss: 16.738658, mse: 1253.119084, mean_q: 12.166640, mean_eps: 0.913621
  26281/300000: episode: 283, duration: 0.404s, episode steps:  63, steps per second: 156, episode reward: -102.854, mean reward: -1.633 [-100.000,  6.724], mean action: 1.683 [0.000, 3.000],  loss: 15.661249, mse: 1211.333856, mean_q: 12.858463, mean_eps: 0.913378
  26358/300000: episode: 284, duration: 0.570s, episode steps:  77, steps per second: 135, episode reward: -95.784, mean reward: -1.244 [-100.000,  9.275], mean action: 1.532 [0.000, 3.000],  loss: 20.780759, mse: 1231.929190, mean_q: 13.526713, mean_eps: 0.913147
  26476/300000: episode: 285, duration: 0.804s, episode steps: 118, steps per second: 147, episode reward: -164.722, mean reward: -1.396 [-100.000,  9.683], mean action: 1.636 [0.000, 3.000],  loss: 15.948679, mse: 1241.669652, mean_q: 12.991842, mean_eps: 0.912826
  26560/300000: episode: 286, duration: 0.546s, episode steps:  84, steps per second: 154, episode reward: -111.123, mean reward: -1.323 [-100.000,  8.450], mean action: 1.357 [0.000, 3.000],  loss: 18.186217, mse: 1248.537156, mean_q: 13.712537, mean_eps: 0.912492
  26651/300000: episode: 287, duration: 0.618s, episode steps:  91, steps per second: 147, episode reward: -265.560, mean reward: -2.918 [-100.000,  1.343], mean action: 1.681 [0.000, 3.000],  loss: 15.876215, mse: 1258.587354, mean_q: 13.787695, mean_eps: 0.912204
  26749/300000: episode: 288, duration: 0.663s, episode steps:  98, steps per second: 148, episode reward: -153.719, mean reward: -1.569 [-100.000,  7.634], mean action: 1.347 [0.000, 3.000],  loss: 18.238744, mse: 1285.317132, mean_q: 12.598138, mean_eps: 0.911892
  26819/300000: episode: 289, duration: 0.462s, episode steps:  70, steps per second: 152, episode reward: -84.947, mean reward: -1.214 [-100.000, 14.830], mean action: 1.457 [0.000, 3.000],  loss: 20.031452, mse: 1280.455636, mean_q: 12.639502, mean_eps: 0.911614
  26901/300000: episode: 290, duration: 0.530s, episode steps:  82, steps per second: 155, episode reward: -62.912, mean reward: -0.767 [-100.000, 10.256], mean action: 1.463 [0.000, 3.000],  loss: 15.286221, mse: 1274.853076, mean_q: 12.985892, mean_eps: 0.911364
  26993/300000: episode: 291, duration: 0.630s, episode steps:  92, steps per second: 146, episode reward: -124.761, mean reward: -1.356 [-100.000, 10.843], mean action: 1.598 [0.000, 3.000],  loss: 16.560516, mse: 1252.725410, mean_q: 13.410111, mean_eps: 0.911077
  27993/300000: episode: 292, duration: 7.039s, episode steps: 1000, steps per second: 142, episode reward: 58.509, mean reward:  0.059 [-22.040, 114.533], mean action: 1.513 [0.000, 3.000],  loss: 17.250833, mse: 1290.826133, mean_q: 14.030254, mean_eps: 0.909275
  28101/300000: episode: 293, duration: 0.721s, episode steps: 108, steps per second: 150, episode reward: -188.279, mean reward: -1.743 [-100.000,  2.373], mean action: 1.574 [0.000, 3.000],  loss: 18.186112, mse: 1307.477722, mean_q: 14.378473, mean_eps: 0.907447
  28191/300000: episode: 294, duration: 0.709s, episode steps:  90, steps per second: 127, episode reward: -84.257, mean reward: -0.936 [-100.000,  9.118], mean action: 1.611 [0.000, 3.000],  loss: 16.072246, mse: 1341.539799, mean_q: 14.875257, mean_eps: 0.907120
  28283/300000: episode: 295, duration: 0.659s, episode steps:  92, steps per second: 140, episode reward: -77.225, mean reward: -0.839 [-100.000, 82.616], mean action: 1.641 [0.000, 3.000],  loss: 13.647635, mse: 1324.856549, mean_q: 14.140221, mean_eps: 0.906820
  28349/300000: episode: 296, duration: 0.443s, episode steps:  66, steps per second: 149, episode reward: -83.330, mean reward: -1.263 [-100.000,  7.475], mean action: 1.424 [0.000, 3.000],  loss: 19.752785, mse: 1346.014762, mean_q: 14.210260, mean_eps: 0.906559
  28433/300000: episode: 297, duration: 0.562s, episode steps:  84, steps per second: 150, episode reward: -342.163, mean reward: -4.073 [-100.000,  2.365], mean action: 1.667 [0.000, 3.000],  loss: 17.784901, mse: 1303.810432, mean_q: 15.376181, mean_eps: 0.906311
  28562/300000: episode: 298, duration: 0.902s, episode steps: 129, steps per second: 143, episode reward: -81.578, mean reward: -0.632 [-100.000,  8.143], mean action: 1.519 [0.000, 3.000],  loss: 14.886051, mse: 1343.590133, mean_q: 14.770542, mean_eps: 0.905960
  28647/300000: episode: 299, duration: 0.568s, episode steps:  85, steps per second: 150, episode reward: -113.884, mean reward: -1.340 [-100.000,  6.385], mean action: 1.306 [0.000, 3.000],  loss: 20.588318, mse: 1338.624704, mean_q: 14.367050, mean_eps: 0.905607
  28761/300000: episode: 300, duration: 0.757s, episode steps: 114, steps per second: 151, episode reward: -250.587, mean reward: -2.198 [-100.000,  1.274], mean action: 1.614 [0.000, 3.000],  loss: 17.900276, mse: 1323.562901, mean_q: 14.393640, mean_eps: 0.905278
  28864/300000: episode: 301, duration: 0.708s, episode steps: 103, steps per second: 146, episode reward: -53.138, mean reward: -0.516 [-100.000, 13.052], mean action: 1.573 [0.000, 3.000],  loss: 16.616614, mse: 1352.717263, mean_q: 14.104766, mean_eps: 0.904920
  28943/300000: episode: 302, duration: 0.516s, episode steps:  79, steps per second: 153, episode reward: -96.229, mean reward: -1.218 [-100.000, 13.086], mean action: 1.557 [0.000, 3.000],  loss: 20.662216, mse: 1330.442289, mean_q: 13.894915, mean_eps: 0.904620
  29048/300000: episode: 303, duration: 0.707s, episode steps: 105, steps per second: 149, episode reward: -129.421, mean reward: -1.233 [-100.000,  8.541], mean action: 1.429 [0.000, 3.000],  loss: 19.249112, mse: 1383.730289, mean_q: 14.837916, mean_eps: 0.904317
  29110/300000: episode: 304, duration: 0.429s, episode steps:  62, steps per second: 145, episode reward: -144.668, mean reward: -2.333 [-100.000, 11.719], mean action: 1.452 [0.000, 3.000],  loss: 18.171575, mse: 1427.760675, mean_q: 14.844861, mean_eps: 0.904041
  29179/300000: episode: 305, duration: 0.489s, episode steps:  69, steps per second: 141, episode reward: -126.916, mean reward: -1.839 [-100.000,  7.427], mean action: 1.638 [0.000, 3.000],  loss: 15.218186, mse: 1424.582980, mean_q: 15.103413, mean_eps: 0.903825
  29260/300000: episode: 306, duration: 0.525s, episode steps:  81, steps per second: 154, episode reward: -6.149, mean reward: -0.076 [-100.000, 68.208], mean action: 1.543 [0.000, 3.000],  loss: 15.955274, mse: 1392.698854, mean_q: 16.138692, mean_eps: 0.903577
  29335/300000: episode: 307, duration: 0.474s, episode steps:  75, steps per second: 158, episode reward: -131.308, mean reward: -1.751 [-100.000, 11.750], mean action: 1.680 [0.000, 3.000],  loss: 15.428098, mse: 1385.366772, mean_q: 14.971642, mean_eps: 0.903320
  29442/300000: episode: 308, duration: 0.714s, episode steps: 107, steps per second: 150, episode reward: -259.513, mean reward: -2.425 [-100.000, 40.826], mean action: 1.383 [0.000, 3.000],  loss: 19.545639, mse: 1422.427078, mean_q: 14.793410, mean_eps: 0.903020
  29558/300000: episode: 309, duration: 0.827s, episode steps: 116, steps per second: 140, episode reward: -148.032, mean reward: -1.276 [-100.000,  6.067], mean action: 1.621 [0.000, 3.000],  loss: 17.974346, mse: 1412.983035, mean_q: 15.675117, mean_eps: 0.902652
  29680/300000: episode: 310, duration: 0.857s, episode steps: 122, steps per second: 142, episode reward: -257.922, mean reward: -2.114 [-100.000,  5.132], mean action: 1.443 [0.000, 3.000],  loss: 20.097545, mse: 1392.569151, mean_q: 15.365873, mean_eps: 0.902259
  29791/300000: episode: 311, duration: 0.836s, episode steps: 111, steps per second: 133, episode reward: -94.033, mean reward: -0.847 [-100.000,  7.157], mean action: 1.396 [0.000, 3.000],  loss: 19.941677, mse: 1414.511882, mean_q: 15.602015, mean_eps: 0.901875
  29871/300000: episode: 312, duration: 0.585s, episode steps:  80, steps per second: 137, episode reward: -136.090, mean reward: -1.701 [-100.000, 11.512], mean action: 1.538 [0.000, 3.000],  loss: 21.340977, mse: 1444.390200, mean_q: 14.166179, mean_eps: 0.901559
  30013/300000: episode: 313, duration: 1.010s, episode steps: 142, steps per second: 141, episode reward: -102.357, mean reward: -0.721 [-100.000, 39.045], mean action: 1.408 [0.000, 3.000],  loss: 16.370425, mse: 1444.416896, mean_q: 15.068945, mean_eps: 0.901193
  30129/300000: episode: 314, duration: 0.846s, episode steps: 116, steps per second: 137, episode reward: -65.230, mean reward: -0.562 [-100.000,  6.564], mean action: 1.578 [0.000, 3.000],  loss: 20.123297, mse: 1471.808872, mean_q: 15.838811, mean_eps: 0.900767
  30200/300000: episode: 315, duration: 0.499s, episode steps:  71, steps per second: 142, episode reward: -133.302, mean reward: -1.877 [-100.000, 14.512], mean action: 1.577 [0.000, 3.000],  loss: 31.169079, mse: 1486.978749, mean_q: 15.785814, mean_eps: 0.900459
  30284/300000: episode: 316, duration: 0.581s, episode steps:  84, steps per second: 145, episode reward: -254.206, mean reward: -3.026 [-100.000,  9.009], mean action: 1.679 [0.000, 3.000],  loss: 16.562053, mse: 1546.146085, mean_q: 15.147048, mean_eps: 0.900203
  30353/300000: episode: 317, duration: 0.463s, episode steps:  69, steps per second: 149, episode reward: -141.354, mean reward: -2.049 [-100.000,  8.967], mean action: 1.362 [0.000, 3.000],  loss: 23.741393, mse: 1454.986044, mean_q: 15.629910, mean_eps: 0.899951
  30453/300000: episode: 318, duration: 0.722s, episode steps: 100, steps per second: 139, episode reward: -119.289, mean reward: -1.193 [-100.000,  5.502], mean action: 1.730 [0.000, 3.000],  loss: 19.391005, mse: 1509.060691, mean_q: 15.907049, mean_eps: 0.899672
  30585/300000: episode: 319, duration: 0.858s, episode steps: 132, steps per second: 154, episode reward: -123.929, mean reward: -0.939 [-100.000, 16.694], mean action: 1.561 [0.000, 3.000],  loss: 12.840779, mse: 1492.144030, mean_q: 15.865376, mean_eps: 0.899289
  30663/300000: episode: 320, duration: 0.500s, episode steps:  78, steps per second: 156, episode reward: -123.944, mean reward: -1.589 [-100.000, 11.196], mean action: 1.526 [0.000, 3.000],  loss: 12.900975, mse: 1502.872826, mean_q: 16.329711, mean_eps: 0.898942
  30744/300000: episode: 321, duration: 0.592s, episode steps:  81, steps per second: 137, episode reward: -89.527, mean reward: -1.105 [-100.000,  4.983], mean action: 1.407 [0.000, 3.000],  loss: 11.656443, mse: 1459.376185, mean_q: 16.038409, mean_eps: 0.898680
  30845/300000: episode: 322, duration: 0.721s, episode steps: 101, steps per second: 140, episode reward: -133.501, mean reward: -1.322 [-100.000, 24.262], mean action: 1.634 [0.000, 3.000],  loss: 10.258359, mse: 1473.484364, mean_q: 16.298049, mean_eps: 0.898380
  30914/300000: episode: 323, duration: 0.454s, episode steps:  69, steps per second: 152, episode reward: -74.694, mean reward: -1.083 [-100.000,  8.879], mean action: 1.449 [0.000, 3.000],  loss: 14.563611, mse: 1544.658081, mean_q: 14.684999, mean_eps: 0.898099
  31023/300000: episode: 324, duration: 0.742s, episode steps: 109, steps per second: 147, episode reward: -105.214, mean reward: -0.965 [-100.000, 10.423], mean action: 1.495 [0.000, 3.000],  loss: 17.742179, mse: 1504.885332, mean_q: 15.930998, mean_eps: 0.897806
  31100/300000: episode: 325, duration: 0.544s, episode steps:  77, steps per second: 142, episode reward: -117.222, mean reward: -1.522 [-100.000, 18.653], mean action: 1.675 [0.000, 3.000],  loss: 11.541239, mse: 1543.276713, mean_q: 16.294621, mean_eps: 0.897499
  31216/300000: episode: 326, duration: 0.766s, episode steps: 116, steps per second: 152, episode reward: -117.890, mean reward: -1.016 [-100.000,  8.813], mean action: 1.474 [0.000, 3.000],  loss: 16.229252, mse: 1562.316149, mean_q: 15.778006, mean_eps: 0.897180
  31309/300000: episode: 327, duration: 0.644s, episode steps:  93, steps per second: 145, episode reward: -118.189, mean reward: -1.271 [-100.000,  6.479], mean action: 1.613 [0.000, 3.000],  loss: 16.211197, mse: 1527.413201, mean_q: 15.569236, mean_eps: 0.896835
  31410/300000: episode: 328, duration: 0.710s, episode steps: 101, steps per second: 142, episode reward: -116.531, mean reward: -1.154 [-100.000,  6.821], mean action: 1.436 [0.000, 3.000],  loss: 16.475956, mse: 1558.755583, mean_q: 15.214586, mean_eps: 0.896515
  31541/300000: episode: 329, duration: 0.891s, episode steps: 131, steps per second: 147, episode reward: -279.740, mean reward: -2.135 [-100.000,  4.135], mean action: 1.626 [0.000, 3.000],  loss: 17.729426, mse: 1563.347864, mean_q: 16.334601, mean_eps: 0.896133
  31616/300000: episode: 330, duration: 0.499s, episode steps:  75, steps per second: 150, episode reward: -92.866, mean reward: -1.238 [-100.000,  7.986], mean action: 1.627 [0.000, 3.000],  loss: 12.043547, mse: 1534.432537, mean_q: 16.736836, mean_eps: 0.895793
  31746/300000: episode: 331, duration: 1.067s, episode steps: 130, steps per second: 122, episode reward: -298.788, mean reward: -2.298 [-100.000, 90.378], mean action: 1.592 [0.000, 3.000],  loss: 18.295922, mse: 1568.810830, mean_q: 16.017830, mean_eps: 0.895454
  31822/300000: episode: 332, duration: 0.595s, episode steps:  76, steps per second: 128, episode reward: -57.438, mean reward: -0.756 [-100.000, 79.052], mean action: 1.500 [0.000, 3.000],  loss: 20.778175, mse: 1527.075769, mean_q: 17.681052, mean_eps: 0.895114
  31903/300000: episode: 333, duration: 0.634s, episode steps:  81, steps per second: 128, episode reward: 28.453, mean reward:  0.351 [-100.000, 110.391], mean action: 1.654 [0.000, 3.000],  loss: 14.705757, mse: 1576.855809, mean_q: 15.546234, mean_eps: 0.894855
  31981/300000: episode: 334, duration: 0.587s, episode steps:  78, steps per second: 133, episode reward: -104.751, mean reward: -1.343 [-100.000, 12.723], mean action: 1.462 [0.000, 3.000],  loss: 17.180087, mse: 1556.860802, mean_q: 16.111884, mean_eps: 0.894593
  32077/300000: episode: 335, duration: 0.715s, episode steps:  96, steps per second: 134, episode reward: -389.017, mean reward: -4.052 [-100.000, -0.046], mean action: 1.594 [0.000, 3.000],  loss: 19.233916, mse: 1546.997940, mean_q: 16.282712, mean_eps: 0.894306
  32173/300000: episode: 336, duration: 0.696s, episode steps:  96, steps per second: 138, episode reward: -123.232, mean reward: -1.284 [-100.000, 13.382], mean action: 1.583 [0.000, 3.000],  loss: 13.978415, mse: 1598.054137, mean_q: 16.925232, mean_eps: 0.893989
  32259/300000: episode: 337, duration: 0.711s, episode steps:  86, steps per second: 121, episode reward: -109.011, mean reward: -1.268 [-100.000, 11.299], mean action: 1.640 [0.000, 3.000],  loss: 20.785383, mse: 1577.481504, mean_q: 16.654776, mean_eps: 0.893689
  32382/300000: episode: 338, duration: 0.904s, episode steps: 123, steps per second: 136, episode reward: -205.609, mean reward: -1.672 [-100.000,  6.263], mean action: 1.545 [0.000, 3.000],  loss: 15.059578, mse: 1592.460305, mean_q: 16.823173, mean_eps: 0.893344
  32512/300000: episode: 339, duration: 0.928s, episode steps: 130, steps per second: 140, episode reward: -80.297, mean reward: -0.618 [-100.000,  6.421], mean action: 1.562 [0.000, 3.000],  loss: 20.935398, mse: 1582.377184, mean_q: 16.334019, mean_eps: 0.892927
  32606/300000: episode: 340, duration: 0.639s, episode steps:  94, steps per second: 147, episode reward: -93.738, mean reward: -0.997 [-100.000,  7.297], mean action: 1.372 [0.000, 3.000],  loss: 14.570202, mse: 1640.729560, mean_q: 16.945687, mean_eps: 0.892557
  32677/300000: episode: 341, duration: 0.470s, episode steps:  71, steps per second: 151, episode reward: -150.691, mean reward: -2.122 [-100.000,  8.497], mean action: 1.521 [0.000, 3.000],  loss: 14.512295, mse: 1604.334815, mean_q: 15.965927, mean_eps: 0.892285
  32760/300000: episode: 342, duration: 0.569s, episode steps:  83, steps per second: 146, episode reward: -138.651, mean reward: -1.670 [-100.000, 22.882], mean action: 1.361 [0.000, 3.000],  loss: 16.217181, mse: 1574.363525, mean_q: 16.958486, mean_eps: 0.892031
  32886/300000: episode: 343, duration: 0.887s, episode steps: 126, steps per second: 142, episode reward: -163.466, mean reward: -1.297 [-100.000, 26.922], mean action: 1.690 [0.000, 3.000],  loss: 15.180338, mse: 1603.509853, mean_q: 16.891832, mean_eps: 0.891686
  32990/300000: episode: 344, duration: 0.696s, episode steps: 104, steps per second: 149, episode reward: -83.092, mean reward: -0.799 [-100.000, 13.760], mean action: 1.510 [0.000, 3.000],  loss: 12.979673, mse: 1612.138296, mean_q: 16.999335, mean_eps: 0.891306
  33055/300000: episode: 345, duration: 0.446s, episode steps:  65, steps per second: 146, episode reward: -186.463, mean reward: -2.869 [-100.000,  7.870], mean action: 1.600 [0.000, 3.000],  loss: 22.182952, mse: 1584.075449, mean_q: 17.008899, mean_eps: 0.891027
  33161/300000: episode: 346, duration: 0.724s, episode steps: 106, steps per second: 146, episode reward: -109.224, mean reward: -1.030 [-100.000,  7.496], mean action: 1.264 [0.000, 3.000],  loss: 17.522125, mse: 1612.106762, mean_q: 16.767103, mean_eps: 0.890745
  33228/300000: episode: 347, duration: 0.455s, episode steps:  67, steps per second: 147, episode reward: -103.558, mean reward: -1.546 [-100.000,  7.860], mean action: 1.627 [0.000, 3.000],  loss: 20.690327, mse: 1658.775041, mean_q: 16.414462, mean_eps: 0.890460
  33321/300000: episode: 348, duration: 0.628s, episode steps:  93, steps per second: 148, episode reward: -99.812, mean reward: -1.073 [-100.000,  8.230], mean action: 1.538 [0.000, 3.000],  loss: 14.901813, mse: 1606.531350, mean_q: 16.406623, mean_eps: 0.890196
  33389/300000: episode: 349, duration: 0.452s, episode steps:  68, steps per second: 151, episode reward: -104.830, mean reward: -1.542 [-100.000, 13.314], mean action: 1.662 [0.000, 3.000],  loss: 18.167812, mse: 1626.035452, mean_q: 16.400246, mean_eps: 0.889930
  33486/300000: episode: 350, duration: 0.681s, episode steps:  97, steps per second: 142, episode reward: -105.057, mean reward: -1.083 [-100.000,  7.445], mean action: 1.495 [0.000, 3.000],  loss: 12.140822, mse: 1605.354120, mean_q: 17.308940, mean_eps: 0.889658
  33570/300000: episode: 351, duration: 0.553s, episode steps:  84, steps per second: 152, episode reward: -108.887, mean reward: -1.296 [-100.000, 11.096], mean action: 1.500 [0.000, 3.000],  loss: 20.688945, mse: 1617.291440, mean_q: 17.001263, mean_eps: 0.889359
  33641/300000: episode: 352, duration: 0.479s, episode steps:  71, steps per second: 148, episode reward: -78.029, mean reward: -1.099 [-100.000, 21.457], mean action: 1.746 [0.000, 3.000],  loss: 21.829263, mse: 1622.432328, mean_q: 17.514761, mean_eps: 0.889103
  33745/300000: episode: 353, duration: 0.724s, episode steps: 104, steps per second: 144, episode reward: -91.449, mean reward: -0.879 [-100.000,  9.568], mean action: 1.635 [0.000, 3.000],  loss: 12.781013, mse: 1579.203413, mean_q: 17.255999, mean_eps: 0.888815
  33849/300000: episode: 354, duration: 0.723s, episode steps: 104, steps per second: 144, episode reward: -113.004, mean reward: -1.087 [-100.000,  7.509], mean action: 1.317 [0.000, 3.000],  loss: 17.750021, mse: 1625.514994, mean_q: 16.415649, mean_eps: 0.888472
  33968/300000: episode: 355, duration: 0.787s, episode steps: 119, steps per second: 151, episode reward: -92.939, mean reward: -0.781 [-100.000, 17.747], mean action: 1.580 [0.000, 3.000],  loss: 21.024036, mse: 1578.808747, mean_q: 16.862536, mean_eps: 0.888104
  34115/300000: episode: 356, duration: 1.016s, episode steps: 147, steps per second: 145, episode reward: -68.975, mean reward: -0.469 [-100.000,  7.493], mean action: 1.490 [0.000, 3.000],  loss: 17.779024, mse: 1710.419897, mean_q: 17.767786, mean_eps: 0.887665
  34234/300000: episode: 357, duration: 0.803s, episode steps: 119, steps per second: 148, episode reward: -277.321, mean reward: -2.330 [-100.000, 97.221], mean action: 1.563 [0.000, 3.000],  loss: 19.926974, mse: 1706.924523, mean_q: 16.801760, mean_eps: 0.887226
  34336/300000: episode: 358, duration: 0.673s, episode steps: 102, steps per second: 152, episode reward: -83.902, mean reward: -0.823 [-100.000,  8.058], mean action: 1.598 [0.000, 3.000],  loss: 16.363639, mse: 1694.035953, mean_q: 18.009198, mean_eps: 0.886861
  34437/300000: episode: 359, duration: 0.712s, episode steps: 101, steps per second: 142, episode reward: -96.373, mean reward: -0.954 [-100.000,  7.345], mean action: 1.376 [0.000, 3.000],  loss: 16.238225, mse: 1705.792616, mean_q: 17.943541, mean_eps: 0.886526
  34580/300000: episode: 360, duration: 0.952s, episode steps: 143, steps per second: 150, episode reward: -9.235, mean reward: -0.065 [-100.000, 110.743], mean action: 1.594 [0.000, 3.000],  loss: 15.175847, mse: 1729.466043, mean_q: 17.504994, mean_eps: 0.886124
  34653/300000: episode: 361, duration: 0.493s, episode steps:  73, steps per second: 148, episode reward: -135.246, mean reward: -1.853 [-100.000,  8.019], mean action: 1.671 [0.000, 3.000],  loss: 21.074956, mse: 1708.458225, mean_q: 17.381841, mean_eps: 0.885767
  34725/300000: episode: 362, duration: 0.511s, episode steps:  72, steps per second: 141, episode reward: -58.910, mean reward: -0.818 [-100.000, 27.796], mean action: 1.569 [0.000, 3.000],  loss: 22.426326, mse: 1681.575770, mean_q: 17.063479, mean_eps: 0.885528
  34834/300000: episode: 363, duration: 0.739s, episode steps: 109, steps per second: 147, episode reward: -75.354, mean reward: -0.691 [-100.000,  8.290], mean action: 1.532 [0.000, 3.000],  loss: 18.521534, mse: 1765.691898, mean_q: 16.919679, mean_eps: 0.885229
  34948/300000: episode: 364, duration: 0.762s, episode steps: 114, steps per second: 150, episode reward: -66.906, mean reward: -0.587 [-100.000, 16.834], mean action: 1.351 [0.000, 3.000],  loss: 15.701158, mse: 1732.263275, mean_q: 17.455944, mean_eps: 0.884861
  35018/300000: episode: 365, duration: 0.474s, episode steps:  70, steps per second: 148, episode reward: -204.755, mean reward: -2.925 [-100.000,  4.447], mean action: 1.700 [0.000, 3.000],  loss: 18.918636, mse: 1746.818293, mean_q: 16.970141, mean_eps: 0.884558
  35143/300000: episode: 366, duration: 0.880s, episode steps: 125, steps per second: 142, episode reward: -125.604, mean reward: -1.005 [-100.000,  7.462], mean action: 1.424 [0.000, 3.000],  loss: 18.947990, mse: 1846.325448, mean_q: 18.756814, mean_eps: 0.884236
  35228/300000: episode: 367, duration: 0.565s, episode steps:  85, steps per second: 150, episode reward: -107.041, mean reward: -1.259 [-100.000,  8.167], mean action: 1.612 [0.000, 3.000],  loss: 15.356133, mse: 1777.396415, mean_q: 19.658795, mean_eps: 0.883889
  35339/300000: episode: 368, duration: 0.762s, episode steps: 111, steps per second: 146, episode reward: -330.584, mean reward: -2.978 [-100.000, 104.545], mean action: 1.586 [0.000, 3.000],  loss: 21.402156, mse: 1751.393478, mean_q: 18.679160, mean_eps: 0.883566
  35440/300000: episode: 369, duration: 0.707s, episode steps: 101, steps per second: 143, episode reward: -144.319, mean reward: -1.429 [-100.000,  5.108], mean action: 1.683 [0.000, 3.000],  loss: 19.316642, mse: 1773.331003, mean_q: 19.042779, mean_eps: 0.883216
  35520/300000: episode: 370, duration: 0.541s, episode steps:  80, steps per second: 148, episode reward: -110.556, mean reward: -1.382 [-100.000,  7.874], mean action: 1.613 [0.000, 3.000],  loss: 17.421023, mse: 1799.468250, mean_q: 18.895165, mean_eps: 0.882918
  35586/300000: episode: 371, duration: 0.429s, episode steps:  66, steps per second: 154, episode reward: -105.800, mean reward: -1.603 [-100.000,  8.361], mean action: 1.530 [0.000, 3.000],  loss: 26.077196, mse: 1740.711929, mean_q: 19.353017, mean_eps: 0.882677
  35682/300000: episode: 372, duration: 0.649s, episode steps:  96, steps per second: 148, episode reward: -114.008, mean reward: -1.188 [-100.000,  8.742], mean action: 1.573 [0.000, 3.000],  loss: 20.083799, mse: 1852.283857, mean_q: 18.621323, mean_eps: 0.882409
  35755/300000: episode: 373, duration: 0.514s, episode steps:  73, steps per second: 142, episode reward: -101.675, mean reward: -1.393 [-100.000, 11.205], mean action: 1.630 [0.000, 3.000],  loss: 14.367865, mse: 1778.740861, mean_q: 18.681560, mean_eps: 0.882131
  35823/300000: episode: 374, duration: 0.480s, episode steps:  68, steps per second: 142, episode reward: -125.599, mean reward: -1.847 [-100.000, 11.900], mean action: 1.706 [0.000, 3.000],  loss: 15.492335, mse: 1829.892422, mean_q: 18.860086, mean_eps: 0.881898
  35955/300000: episode: 375, duration: 0.879s, episode steps: 132, steps per second: 150, episode reward: -152.084, mean reward: -1.152 [-100.000, 12.502], mean action: 1.424 [0.000, 3.000],  loss: 14.773311, mse: 1810.593469, mean_q: 19.052419, mean_eps: 0.881568
  36051/300000: episode: 376, duration: 0.699s, episode steps:  96, steps per second: 137, episode reward: -116.610, mean reward: -1.215 [-100.000, 23.474], mean action: 1.438 [0.000, 3.000],  loss: 17.980275, mse: 1862.700788, mean_q: 19.283847, mean_eps: 0.881192
  36149/300000: episode: 377, duration: 0.657s, episode steps:  98, steps per second: 149, episode reward: -82.058, mean reward: -0.837 [-100.000, 20.244], mean action: 1.541 [0.000, 3.000],  loss: 20.577060, mse: 1859.784543, mean_q: 20.641578, mean_eps: 0.880872
  36241/300000: episode: 378, duration: 0.600s, episode steps:  92, steps per second: 153, episode reward: -108.765, mean reward: -1.182 [-100.000, 16.614], mean action: 1.598 [0.000, 3.000],  loss: 23.460426, mse: 1871.877204, mean_q: 19.954740, mean_eps: 0.880558
  36323/300000: episode: 379, duration: 0.570s, episode steps:  82, steps per second: 144, episode reward: -92.378, mean reward: -1.127 [-100.000, 12.987], mean action: 1.646 [0.000, 3.000],  loss: 16.834512, mse: 1851.630212, mean_q: 21.038223, mean_eps: 0.880271
  36453/300000: episode: 380, duration: 0.885s, episode steps: 130, steps per second: 147, episode reward: -9.367, mean reward: -0.072 [-100.000, 61.500], mean action: 1.631 [0.000, 3.000],  loss: 15.291670, mse: 1870.937576, mean_q: 20.321404, mean_eps: 0.879921
  36557/300000: episode: 381, duration: 0.686s, episode steps: 104, steps per second: 152, episode reward: -90.475, mean reward: -0.870 [-100.000,  9.463], mean action: 1.558 [0.000, 3.000],  loss: 14.902428, mse: 1863.693875, mean_q: 20.931923, mean_eps: 0.879535
  36627/300000: episode: 382, duration: 0.499s, episode steps:  70, steps per second: 140, episode reward: -253.898, mean reward: -3.627 [-100.000,  4.187], mean action: 1.757 [0.000, 3.000],  loss: 9.326609, mse: 1860.599472, mean_q: 19.998471, mean_eps: 0.879248
  36735/300000: episode: 383, duration: 0.759s, episode steps: 108, steps per second: 142, episode reward: -152.764, mean reward: -1.414 [-100.000, 17.561], mean action: 1.565 [0.000, 3.000],  loss: 17.002709, mse: 1878.462591, mean_q: 20.236628, mean_eps: 0.878954
  36802/300000: episode: 384, duration: 0.448s, episode steps:  67, steps per second: 149, episode reward: -87.955, mean reward: -1.313 [-100.000,  7.888], mean action: 1.433 [0.000, 3.000],  loss: 14.781754, mse: 1923.776464, mean_q: 19.086114, mean_eps: 0.878666
  36883/300000: episode: 385, duration: 0.555s, episode steps:  81, steps per second: 146, episode reward: -95.291, mean reward: -1.176 [-100.000,  9.737], mean action: 1.444 [0.000, 3.000],  loss: 11.582884, mse: 1908.729608, mean_q: 20.138802, mean_eps: 0.878421
  36959/300000: episode: 386, duration: 0.544s, episode steps:  76, steps per second: 140, episode reward: -236.624, mean reward: -3.113 [-100.000,  7.109], mean action: 1.368 [0.000, 3.000],  loss: 12.457543, mse: 1878.402376, mean_q: 20.206605, mean_eps: 0.878162
  37061/300000: episode: 387, duration: 0.695s, episode steps: 102, steps per second: 147, episode reward: -129.950, mean reward: -1.274 [-100.000,  5.159], mean action: 1.402 [0.000, 3.000],  loss: 22.082616, mse: 1898.767370, mean_q: 20.603460, mean_eps: 0.877869
  37176/300000: episode: 388, duration: 0.767s, episode steps: 115, steps per second: 150, episode reward: -137.298, mean reward: -1.194 [-100.000,  5.888], mean action: 1.548 [0.000, 3.000],  loss: 19.340920, mse: 1914.414877, mean_q: 21.794190, mean_eps: 0.877511
  37293/300000: episode: 389, duration: 0.833s, episode steps: 117, steps per second: 140, episode reward: -169.005, mean reward: -1.444 [-100.000, 10.126], mean action: 1.650 [0.000, 3.000],  loss: 14.076641, mse: 1910.465445, mean_q: 21.509773, mean_eps: 0.877128
  37407/300000: episode: 390, duration: 0.766s, episode steps: 114, steps per second: 149, episode reward: -15.235, mean reward: -0.134 [-100.000, 103.114], mean action: 1.570 [0.000, 3.000],  loss: 19.849298, mse: 1919.081590, mean_q: 20.929102, mean_eps: 0.876747
  37490/300000: episode: 391, duration: 0.565s, episode steps:  83, steps per second: 147, episode reward: -76.685, mean reward: -0.924 [-100.000, 11.688], mean action: 1.578 [0.000, 3.000],  loss: 21.256514, mse: 1876.647573, mean_q: 20.177381, mean_eps: 0.876422
  37612/300000: episode: 392, duration: 0.857s, episode steps: 122, steps per second: 142, episode reward: -35.039, mean reward: -0.287 [-100.000, 52.936], mean action: 1.672 [0.000, 3.000],  loss: 15.070587, mse: 1948.164487, mean_q: 21.777625, mean_eps: 0.876083
  37689/300000: episode: 393, duration: 0.536s, episode steps:  77, steps per second: 144, episode reward: -112.752, mean reward: -1.464 [-100.000, 10.469], mean action: 1.623 [0.000, 3.000],  loss: 31.038377, mse: 1975.071941, mean_q: 20.446076, mean_eps: 0.875755
  37762/300000: episode: 394, duration: 0.586s, episode steps:  73, steps per second: 125, episode reward: -49.140, mean reward: -0.673 [-100.000, 17.405], mean action: 1.562 [0.000, 3.000],  loss: 16.501875, mse: 1946.521314, mean_q: 20.334801, mean_eps: 0.875507
  37822/300000: episode: 395, duration: 0.425s, episode steps:  60, steps per second: 141, episode reward: -99.899, mean reward: -1.665 [-100.000,  5.287], mean action: 1.417 [0.000, 3.000],  loss: 22.325788, mse: 1910.792232, mean_q: 21.611289, mean_eps: 0.875288
  37932/300000: episode: 396, duration: 0.773s, episode steps: 110, steps per second: 142, episode reward: -82.350, mean reward: -0.749 [-100.000, 11.767], mean action: 1.545 [0.000, 3.000],  loss: 18.460315, mse: 1943.137415, mean_q: 20.405341, mean_eps: 0.875008
  38043/300000: episode: 397, duration: 0.758s, episode steps: 111, steps per second: 146, episode reward: -103.477, mean reward: -0.932 [-100.000, 14.902], mean action: 1.459 [0.000, 3.000],  loss: 18.216010, mse: 1964.431907, mean_q: 21.635334, mean_eps: 0.874643
  38127/300000: episode: 398, duration: 0.577s, episode steps:  84, steps per second: 145, episode reward: -61.981, mean reward: -0.738 [-100.000,  8.096], mean action: 1.524 [0.000, 3.000],  loss: 17.737797, mse: 2022.401241, mean_q: 22.808638, mean_eps: 0.874321
  38240/300000: episode: 399, duration: 0.814s, episode steps: 113, steps per second: 139, episode reward: -93.434, mean reward: -0.827 [-100.000,  8.991], mean action: 1.434 [0.000, 3.000],  loss: 20.110750, mse: 1996.648063, mean_q: 22.027886, mean_eps: 0.873996
  38331/300000: episode: 400, duration: 0.608s, episode steps:  91, steps per second: 150, episode reward: -128.182, mean reward: -1.409 [-100.000,  9.522], mean action: 1.703 [0.000, 3.000],  loss: 22.281393, mse: 2042.061722, mean_q: 22.706547, mean_eps: 0.873659
  38408/300000: episode: 401, duration: 0.512s, episode steps:  77, steps per second: 150, episode reward: -108.947, mean reward: -1.415 [-100.000, 13.215], mean action: 1.506 [0.000, 3.000],  loss: 14.063175, mse: 1996.759165, mean_q: 22.646184, mean_eps: 0.873382
  38481/300000: episode: 402, duration: 0.526s, episode steps:  73, steps per second: 139, episode reward: -87.942, mean reward: -1.205 [-100.000,  8.764], mean action: 1.795 [0.000, 3.000],  loss: 14.795531, mse: 2043.063605, mean_q: 23.566876, mean_eps: 0.873135
  38565/300000: episode: 403, duration: 0.593s, episode steps:  84, steps per second: 142, episode reward: -94.950, mean reward: -1.130 [-100.000,  8.120], mean action: 1.440 [0.000, 3.000],  loss: 17.608066, mse: 2068.874317, mean_q: 23.247473, mean_eps: 0.872876
  38667/300000: episode: 404, duration: 0.686s, episode steps: 102, steps per second: 149, episode reward: -122.213, mean reward: -1.198 [-100.000, 14.749], mean action: 1.382 [0.000, 3.000],  loss: 17.303912, mse: 2011.919722, mean_q: 23.469872, mean_eps: 0.872569
  38742/300000: episode: 405, duration: 0.515s, episode steps:  75, steps per second: 146, episode reward: -78.870, mean reward: -1.052 [-100.000,  9.822], mean action: 1.467 [0.000, 3.000],  loss: 14.415090, mse: 1997.420946, mean_q: 25.142025, mean_eps: 0.872277
  38828/300000: episode: 406, duration: 0.653s, episode steps:  86, steps per second: 132, episode reward: -84.162, mean reward: -0.979 [-100.000, 16.598], mean action: 1.523 [0.000, 3.000],  loss: 17.021543, mse: 2027.500146, mean_q: 23.118018, mean_eps: 0.872011
  38897/300000: episode: 407, duration: 0.474s, episode steps:  69, steps per second: 146, episode reward: -21.338, mean reward: -0.309 [-100.000, 13.192], mean action: 1.681 [0.000, 3.000],  loss: 14.689606, mse: 2024.892957, mean_q: 22.429787, mean_eps: 0.871755
  38991/300000: episode: 408, duration: 0.749s, episode steps:  94, steps per second: 126, episode reward: -57.893, mean reward: -0.616 [-100.000, 24.285], mean action: 1.734 [0.000, 3.000],  loss: 25.844370, mse: 2115.839381, mean_q: 22.799591, mean_eps: 0.871486
  39095/300000: episode: 409, duration: 0.801s, episode steps: 104, steps per second: 130, episode reward: -76.918, mean reward: -0.740 [-100.000, 21.028], mean action: 1.519 [0.000, 3.000],  loss: 17.031554, mse: 2101.964189, mean_q: 23.814644, mean_eps: 0.871160
  39179/300000: episode: 410, duration: 0.622s, episode steps:  84, steps per second: 135, episode reward: -107.331, mean reward: -1.278 [-100.000,  9.728], mean action: 1.619 [0.000, 3.000],  loss: 19.202400, mse: 2061.648801, mean_q: 24.164981, mean_eps: 0.870850
  39282/300000: episode: 411, duration: 0.753s, episode steps: 103, steps per second: 137, episode reward: -117.477, mean reward: -1.141 [-100.000,  8.461], mean action: 1.573 [0.000, 3.000],  loss: 15.380199, mse: 2119.769124, mean_q: 23.926103, mean_eps: 0.870541
  39372/300000: episode: 412, duration: 0.678s, episode steps:  90, steps per second: 133, episode reward: -97.703, mean reward: -1.086 [-100.000, 11.097], mean action: 1.544 [0.000, 3.000],  loss: 13.841225, mse: 2170.550772, mean_q: 22.797503, mean_eps: 0.870223
  39464/300000: episode: 413, duration: 0.716s, episode steps:  92, steps per second: 128, episode reward: -125.366, mean reward: -1.363 [-100.000,  8.804], mean action: 1.304 [0.000, 3.000],  loss: 18.625466, mse: 2088.377971, mean_q: 23.561205, mean_eps: 0.869922
  39532/300000: episode: 414, duration: 0.526s, episode steps:  68, steps per second: 129, episode reward: -95.127, mean reward: -1.399 [-100.000,  9.035], mean action: 1.765 [0.000, 3.000],  loss: 18.446107, mse: 2136.481210, mean_q: 23.991874, mean_eps: 0.869658
  39625/300000: episode: 415, duration: 0.688s, episode steps:  93, steps per second: 135, episode reward: -114.255, mean reward: -1.229 [-100.000, 10.456], mean action: 1.505 [0.000, 3.000],  loss: 21.196508, mse: 2106.868924, mean_q: 24.437238, mean_eps: 0.869393
  39708/300000: episode: 416, duration: 0.616s, episode steps:  83, steps per second: 135, episode reward: -80.513, mean reward: -0.970 [-100.000,  7.669], mean action: 1.554 [0.000, 3.000],  loss: 18.071426, mse: 2135.574317, mean_q: 23.128357, mean_eps: 0.869102
  39827/300000: episode: 417, duration: 0.839s, episode steps: 119, steps per second: 142, episode reward: -124.307, mean reward: -1.045 [-100.000, 24.712], mean action: 1.664 [0.000, 3.000],  loss: 18.057662, mse: 2127.937521, mean_q: 23.012494, mean_eps: 0.868769
  39924/300000: episode: 418, duration: 0.650s, episode steps:  97, steps per second: 149, episode reward: -161.066, mean reward: -1.660 [-100.000,  7.788], mean action: 1.526 [0.000, 3.000],  loss: 17.388750, mse: 2111.430507, mean_q: 22.567316, mean_eps: 0.868413
  39993/300000: episode: 419, duration: 0.520s, episode steps:  69, steps per second: 133, episode reward: -72.975, mean reward: -1.058 [-100.000, 14.141], mean action: 1.696 [0.000, 3.000],  loss: 13.928106, mse: 2128.542882, mean_q: 23.383473, mean_eps: 0.868139
  40110/300000: episode: 420, duration: 0.857s, episode steps: 117, steps per second: 137, episode reward: -125.240, mean reward: -1.070 [-100.000, 11.324], mean action: 1.462 [0.000, 3.000],  loss: 18.713929, mse: 2231.862914, mean_q: 24.685938, mean_eps: 0.867832
  40192/300000: episode: 421, duration: 0.622s, episode steps:  82, steps per second: 132, episode reward: -74.647, mean reward: -0.910 [-100.000, 16.950], mean action: 1.817 [0.000, 3.000],  loss: 16.067453, mse: 2264.944864, mean_q: 24.977651, mean_eps: 0.867503
  40288/300000: episode: 422, duration: 0.734s, episode steps:  96, steps per second: 131, episode reward: -98.869, mean reward: -1.030 [-100.000,  7.499], mean action: 1.625 [0.000, 3.000],  loss: 25.041014, mse: 2196.831252, mean_q: 24.621443, mean_eps: 0.867210
  40390/300000: episode: 423, duration: 0.699s, episode steps: 102, steps per second: 146, episode reward: -79.809, mean reward: -0.782 [-100.000,  9.490], mean action: 1.363 [0.000, 3.000],  loss: 17.181123, mse: 2232.877248, mean_q: 24.921454, mean_eps: 0.866883
  40515/300000: episode: 424, duration: 0.835s, episode steps: 125, steps per second: 150, episode reward: -67.747, mean reward: -0.542 [-100.000, 16.990], mean action: 1.528 [0.000, 3.000],  loss: 14.458110, mse: 2223.106804, mean_q: 24.386220, mean_eps: 0.866508
  40629/300000: episode: 425, duration: 0.809s, episode steps: 114, steps per second: 141, episode reward: -100.599, mean reward: -0.882 [-100.000, 26.797], mean action: 1.605 [0.000, 3.000],  loss: 16.839324, mse: 2284.007108, mean_q: 24.153976, mean_eps: 0.866114
  40736/300000: episode: 426, duration: 0.725s, episode steps: 107, steps per second: 148, episode reward: -185.804, mean reward: -1.736 [-100.000, 21.333], mean action: 1.617 [0.000, 3.000],  loss: 20.379587, mse: 2211.834303, mean_q: 24.236959, mean_eps: 0.865749
  40839/300000: episode: 427, duration: 0.694s, episode steps: 103, steps per second: 148, episode reward: -68.570, mean reward: -0.666 [-100.000, 12.750], mean action: 1.495 [0.000, 3.000],  loss: 15.714848, mse: 2198.649285, mean_q: 25.256566, mean_eps: 0.865403
  40959/300000: episode: 428, duration: 0.870s, episode steps: 120, steps per second: 138, episode reward: -135.358, mean reward: -1.128 [-100.000,  5.009], mean action: 1.450 [0.000, 3.000],  loss: 13.784185, mse: 2249.174641, mean_q: 24.974316, mean_eps: 0.865035
  41080/300000: episode: 429, duration: 0.932s, episode steps: 121, steps per second: 130, episode reward: -119.723, mean reward: -0.989 [-100.000, 13.994], mean action: 1.521 [0.000, 3.000],  loss: 14.418896, mse: 2229.545294, mean_q: 26.538339, mean_eps: 0.864637
  41182/300000: episode: 430, duration: 0.854s, episode steps: 102, steps per second: 119, episode reward: -89.197, mean reward: -0.874 [-100.000, 10.951], mean action: 1.539 [0.000, 3.000],  loss: 15.074741, mse: 2365.911921, mean_q: 26.323680, mean_eps: 0.864269
  41243/300000: episode: 431, duration: 0.485s, episode steps:  61, steps per second: 126, episode reward: -82.670, mean reward: -1.355 [-100.000, 10.341], mean action: 1.410 [0.000, 3.000],  loss: 13.069779, mse: 2310.332712, mean_q: 25.096632, mean_eps: 0.864000
  41323/300000: episode: 432, duration: 0.611s, episode steps:  80, steps per second: 131, episode reward: -269.610, mean reward: -3.370 [-100.000, 81.435], mean action: 1.750 [0.000, 3.000],  loss: 15.139138, mse: 2319.412749, mean_q: 25.958608, mean_eps: 0.863768
  41411/300000: episode: 433, duration: 0.677s, episode steps:  88, steps per second: 130, episode reward: -165.364, mean reward: -1.879 [-100.000, 29.318], mean action: 1.523 [0.000, 3.000],  loss: 17.616407, mse: 2330.792912, mean_q: 25.964811, mean_eps: 0.863491
  41473/300000: episode: 434, duration: 0.503s, episode steps:  62, steps per second: 123, episode reward: -92.023, mean reward: -1.484 [-100.000, 14.753], mean action: 1.629 [0.000, 3.000],  loss: 14.240698, mse: 2273.748072, mean_q: 26.061787, mean_eps: 0.863243
  41570/300000: episode: 435, duration: 0.738s, episode steps:  97, steps per second: 131, episode reward: -111.958, mean reward: -1.154 [-100.000,  6.619], mean action: 1.423 [0.000, 3.000],  loss: 13.398034, mse: 2315.815072, mean_q: 26.642377, mean_eps: 0.862981
  41641/300000: episode: 436, duration: 0.535s, episode steps:  71, steps per second: 133, episode reward: -105.770, mean reward: -1.490 [-100.000,  7.635], mean action: 1.423 [0.000, 3.000],  loss: 16.596007, mse: 2288.702681, mean_q: 24.386562, mean_eps: 0.862704
  41751/300000: episode: 437, duration: 0.799s, episode steps: 110, steps per second: 138, episode reward: -107.197, mean reward: -0.975 [-100.000,  8.040], mean action: 1.445 [0.000, 3.000],  loss: 13.373756, mse: 2308.668385, mean_q: 26.475265, mean_eps: 0.862405
  41876/300000: episode: 438, duration: 0.953s, episode steps: 125, steps per second: 131, episode reward: -125.663, mean reward: -1.005 [-100.000,  6.871], mean action: 1.672 [0.000, 3.000],  loss: 15.258931, mse: 2266.790378, mean_q: 26.711001, mean_eps: 0.862017
  41948/300000: episode: 439, duration: 0.497s, episode steps:  72, steps per second: 145, episode reward: -106.682, mean reward: -1.482 [-100.000,  8.298], mean action: 1.556 [0.000, 3.000],  loss: 11.064463, mse: 2289.694239, mean_q: 26.853234, mean_eps: 0.861692
  42066/300000: episode: 440, duration: 0.841s, episode steps: 118, steps per second: 140, episode reward: -183.092, mean reward: -1.552 [-100.000, 14.956], mean action: 1.636 [0.000, 3.000],  loss: 15.740569, mse: 2338.504389, mean_q: 26.343131, mean_eps: 0.861379
  42159/300000: episode: 441, duration: 0.659s, episode steps:  93, steps per second: 141, episode reward: -169.447, mean reward: -1.822 [-100.000,  6.073], mean action: 1.548 [0.000, 3.000],  loss: 17.312164, mse: 2377.427150, mean_q: 26.218743, mean_eps: 0.861030
  42242/300000: episode: 442, duration: 0.563s, episode steps:  83, steps per second: 148, episode reward: -111.958, mean reward: -1.349 [-100.000, 14.194], mean action: 1.434 [0.000, 3.000],  loss: 17.386637, mse: 2412.323751, mean_q: 26.734419, mean_eps: 0.860740
  42361/300000: episode: 443, duration: 0.847s, episode steps: 119, steps per second: 141, episode reward: -104.395, mean reward: -0.877 [-100.000, 10.861], mean action: 1.546 [0.000, 3.000],  loss: 14.187996, mse: 2408.813505, mean_q: 25.661219, mean_eps: 0.860407
  42440/300000: episode: 444, duration: 0.553s, episode steps:  79, steps per second: 143, episode reward: -113.690, mean reward: -1.439 [-100.000, 12.174], mean action: 1.709 [0.000, 3.000],  loss: 16.184329, mse: 2410.251086, mean_q: 27.457407, mean_eps: 0.860080
  42505/300000: episode: 445, duration: 0.452s, episode steps:  65, steps per second: 144, episode reward: -92.314, mean reward: -1.420 [-100.000,  7.309], mean action: 1.508 [0.000, 3.000],  loss: 17.296047, mse: 2329.981192, mean_q: 26.524359, mean_eps: 0.859842
  42597/300000: episode: 446, duration: 0.628s, episode steps:  92, steps per second: 147, episode reward: -110.171, mean reward: -1.198 [-100.000, 16.650], mean action: 1.413 [0.000, 3.000],  loss: 16.110303, mse: 2384.159173, mean_q: 27.783820, mean_eps: 0.859583
  42700/300000: episode: 447, duration: 0.749s, episode steps: 103, steps per second: 138, episode reward: -120.448, mean reward: -1.169 [-100.000,  6.709], mean action: 1.466 [0.000, 3.000],  loss: 17.557967, mse: 2372.540595, mean_q: 26.816869, mean_eps: 0.859262
  42773/300000: episode: 448, duration: 0.508s, episode steps:  73, steps per second: 144, episode reward: -110.252, mean reward: -1.510 [-100.000, 33.355], mean action: 1.370 [0.000, 3.000],  loss: 14.378686, mse: 2403.046974, mean_q: 26.837468, mean_eps: 0.858971
  42836/300000: episode: 449, duration: 0.445s, episode steps:  63, steps per second: 142, episode reward: -75.322, mean reward: -1.196 [-100.000,  7.206], mean action: 1.571 [0.000, 3.000],  loss: 10.192669, mse: 2371.182578, mean_q: 27.711906, mean_eps: 0.858747
  42960/300000: episode: 450, duration: 1.026s, episode steps: 124, steps per second: 121, episode reward: -64.789, mean reward: -0.522 [-100.000,  9.119], mean action: 1.492 [0.000, 3.000],  loss: 13.882733, mse: 2375.663572, mean_q: 27.223747, mean_eps: 0.858438
  43072/300000: episode: 451, duration: 0.978s, episode steps: 112, steps per second: 115, episode reward: -110.332, mean reward: -0.985 [-100.000, 15.986], mean action: 1.518 [0.000, 3.000],  loss: 19.833052, mse: 2405.904161, mean_q: 26.832582, mean_eps: 0.858049
  43170/300000: episode: 452, duration: 0.881s, episode steps:  98, steps per second: 111, episode reward: -338.318, mean reward: -3.452 [-100.000,  0.649], mean action: 1.561 [0.000, 3.000],  loss: 15.807101, mse: 2396.554886, mean_q: 27.988773, mean_eps: 0.857702
  43236/300000: episode: 453, duration: 0.578s, episode steps:  66, steps per second: 114, episode reward: -87.005, mean reward: -1.318 [-100.000,  7.865], mean action: 1.379 [0.000, 3.000],  loss: 22.342928, mse: 2387.120853, mean_q: 27.035754, mean_eps: 0.857432
  43357/300000: episode: 454, duration: 0.993s, episode steps: 121, steps per second: 122, episode reward: -7.643, mean reward: -0.063 [-100.000, 60.571], mean action: 1.488 [0.000, 3.000],  loss: 18.316857, mse: 2400.320823, mean_q: 26.960652, mean_eps: 0.857123
  43464/300000: episode: 455, duration: 0.793s, episode steps: 107, steps per second: 135, episode reward: -101.377, mean reward: -0.947 [-100.000,  9.979], mean action: 1.495 [0.000, 3.000],  loss: 19.489028, mse: 2416.894034, mean_q: 27.521677, mean_eps: 0.856747
  43526/300000: episode: 456, duration: 0.455s, episode steps:  62, steps per second: 136, episode reward: -187.652, mean reward: -3.027 [-100.000, 20.586], mean action: 1.468 [0.000, 3.000],  loss: 18.013235, mse: 2382.925196, mean_q: 28.500325, mean_eps: 0.856468
  43614/300000: episode: 457, duration: 0.632s, episode steps:  88, steps per second: 139, episode reward: -95.589, mean reward: -1.086 [-100.000,  7.784], mean action: 1.432 [0.000, 3.000],  loss: 16.089924, mse: 2361.958220, mean_q: 27.493886, mean_eps: 0.856221
  43729/300000: episode: 458, duration: 0.846s, episode steps: 115, steps per second: 136, episode reward: -117.690, mean reward: -1.023 [-100.000, 12.620], mean action: 1.565 [0.000, 3.000],  loss: 18.645959, mse: 2356.096925, mean_q: 27.856282, mean_eps: 0.855886
  43799/300000: episode: 459, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -190.355, mean reward: -2.719 [-100.000,  6.386], mean action: 1.386 [0.000, 3.000],  loss: 15.993889, mse: 2449.835292, mean_q: 27.029279, mean_eps: 0.855580
  43864/300000: episode: 460, duration: 0.473s, episode steps:  65, steps per second: 137, episode reward: -57.953, mean reward: -0.892 [-100.000,  7.339], mean action: 1.338 [0.000, 3.000],  loss: 15.657597, mse: 2403.585742, mean_q: 28.214093, mean_eps: 0.855358
  43938/300000: episode: 461, duration: 0.531s, episode steps:  74, steps per second: 139, episode reward: -58.813, mean reward: -0.795 [-100.000, 12.434], mean action: 1.649 [0.000, 3.000],  loss: 20.960091, mse: 2372.703095, mean_q: 27.317024, mean_eps: 0.855128
  44038/300000: episode: 462, duration: 0.758s, episode steps: 100, steps per second: 132, episode reward: -109.488, mean reward: -1.095 [-100.000, 11.864], mean action: 1.390 [0.000, 3.000],  loss: 21.375150, mse: 2417.078162, mean_q: 28.663465, mean_eps: 0.854841
  44122/300000: episode: 463, duration: 0.608s, episode steps:  84, steps per second: 138, episode reward: -98.330, mean reward: -1.171 [-100.000,  6.307], mean action: 1.512 [0.000, 3.000],  loss: 15.511731, mse: 2429.264664, mean_q: 28.877031, mean_eps: 0.854538
  44226/300000: episode: 464, duration: 0.713s, episode steps: 104, steps per second: 146, episode reward: -97.129, mean reward: -0.934 [-100.000, 17.603], mean action: 1.596 [0.000, 3.000],  loss: 23.193101, mse: 2454.405273, mean_q: 29.875326, mean_eps: 0.854227
  44330/300000: episode: 465, duration: 0.753s, episode steps: 104, steps per second: 138, episode reward: -29.773, mean reward: -0.286 [-100.000, 19.141], mean action: 1.538 [0.000, 3.000],  loss: 18.555397, mse: 2448.484413, mean_q: 28.133966, mean_eps: 0.853884
  44417/300000: episode: 466, duration: 0.637s, episode steps:  87, steps per second: 137, episode reward: -150.864, mean reward: -1.734 [-100.000,  5.377], mean action: 1.759 [0.000, 3.000],  loss: 15.172453, mse: 2451.633221, mean_q: 26.818264, mean_eps: 0.853569
  44487/300000: episode: 467, duration: 0.473s, episode steps:  70, steps per second: 148, episode reward: -119.143, mean reward: -1.702 [-100.000,  7.284], mean action: 1.571 [0.000, 3.000],  loss: 21.890670, mse: 2452.363649, mean_q: 27.910620, mean_eps: 0.853310
  44580/300000: episode: 468, duration: 0.630s, episode steps:  93, steps per second: 148, episode reward: -106.987, mean reward: -1.150 [-100.000,  9.229], mean action: 1.462 [0.000, 3.000],  loss: 13.737958, mse: 2493.706013, mean_q: 28.529158, mean_eps: 0.853041
  44659/300000: episode: 469, duration: 0.604s, episode steps:  79, steps per second: 131, episode reward: -105.587, mean reward: -1.337 [-100.000,  5.716], mean action: 1.443 [0.000, 3.000],  loss: 21.003359, mse: 2505.025069, mean_q: 27.985084, mean_eps: 0.852757
  44788/300000: episode: 470, duration: 0.916s, episode steps: 129, steps per second: 141, episode reward: -183.866, mean reward: -1.425 [-100.000,  6.385], mean action: 1.326 [0.000, 3.000],  loss: 14.733142, mse: 2426.241080, mean_q: 28.514189, mean_eps: 0.852414
  44879/300000: episode: 471, duration: 0.627s, episode steps:  91, steps per second: 145, episode reward: -58.903, mean reward: -0.647 [-100.000, 12.888], mean action: 1.615 [0.000, 3.000],  loss: 14.992062, mse: 2449.161567, mean_q: 27.916555, mean_eps: 0.852051
  44956/300000: episode: 472, duration: 0.560s, episode steps:  77, steps per second: 138, episode reward: -79.010, mean reward: -1.026 [-100.000,  6.290], mean action: 1.701 [0.000, 3.000],  loss: 11.009317, mse: 2392.218526, mean_q: 28.414284, mean_eps: 0.851774
  45023/300000: episode: 473, duration: 0.483s, episode steps:  67, steps per second: 139, episode reward: -182.954, mean reward: -2.731 [-100.000,  6.883], mean action: 1.701 [0.000, 3.000],  loss: 15.384688, mse: 2473.816517, mean_q: 29.486325, mean_eps: 0.851536
  45129/300000: episode: 474, duration: 0.733s, episode steps: 106, steps per second: 145, episode reward: -117.991, mean reward: -1.113 [-100.000, 18.594], mean action: 1.679 [0.000, 3.000],  loss: 20.287774, mse: 2523.745889, mean_q: 28.505294, mean_eps: 0.851251
  45230/300000: episode: 475, duration: 0.701s, episode steps: 101, steps per second: 144, episode reward: -116.704, mean reward: -1.155 [-100.000,  5.913], mean action: 1.554 [0.000, 3.000],  loss: 16.762852, mse: 2540.866800, mean_q: 28.313504, mean_eps: 0.850909
  45325/300000: episode: 476, duration: 0.698s, episode steps:  95, steps per second: 136, episode reward: -128.941, mean reward: -1.357 [-100.000,  6.355], mean action: 1.642 [0.000, 3.000],  loss: 19.840219, mse: 2568.690034, mean_q: 28.807289, mean_eps: 0.850586
  45470/300000: episode: 477, duration: 1.002s, episode steps: 145, steps per second: 145, episode reward: -177.022, mean reward: -1.221 [-100.000, 13.120], mean action: 1.545 [0.000, 3.000],  loss: 16.442977, mse: 2548.746038, mean_q: 28.346455, mean_eps: 0.850190
  45599/300000: episode: 478, duration: 0.939s, episode steps: 129, steps per second: 137, episode reward: -69.479, mean reward: -0.539 [-100.000, 18.226], mean action: 1.519 [0.000, 3.000],  loss: 14.364204, mse: 2611.594470, mean_q: 28.455145, mean_eps: 0.849738
  45702/300000: episode: 479, duration: 0.894s, episode steps: 103, steps per second: 115, episode reward: -63.015, mean reward: -0.612 [-100.000, 15.919], mean action: 1.660 [0.000, 3.000],  loss: 13.299633, mse: 2579.715852, mean_q: 28.836678, mean_eps: 0.849355
  45771/300000: episode: 480, duration: 0.518s, episode steps:  69, steps per second: 133, episode reward: -51.663, mean reward: -0.749 [-100.000, 12.117], mean action: 1.478 [0.000, 3.000],  loss: 21.806107, mse: 2551.130191, mean_q: 29.428192, mean_eps: 0.849071
  45899/300000: episode: 481, duration: 0.957s, episode steps: 128, steps per second: 134, episode reward: -57.103, mean reward: -0.446 [-100.000, 18.754], mean action: 1.477 [0.000, 3.000],  loss: 17.250804, mse: 2526.393877, mean_q: 28.485706, mean_eps: 0.848746
  45983/300000: episode: 482, duration: 0.596s, episode steps:  84, steps per second: 141, episode reward: -141.599, mean reward: -1.686 [-100.000,  6.704], mean action: 1.417 [0.000, 3.000],  loss: 13.377827, mse: 2548.295833, mean_q: 28.547132, mean_eps: 0.848396
  46102/300000: episode: 483, duration: 0.846s, episode steps: 119, steps per second: 141, episode reward: -96.726, mean reward: -0.813 [-100.000,  7.057], mean action: 1.555 [0.000, 3.000],  loss: 15.677633, mse: 2615.707791, mean_q: 30.441861, mean_eps: 0.848061
  46230/300000: episode: 484, duration: 0.947s, episode steps: 128, steps per second: 135, episode reward: -147.442, mean reward: -1.152 [-100.000,  9.272], mean action: 1.344 [0.000, 3.000],  loss: 21.232213, mse: 2673.365547, mean_q: 30.905846, mean_eps: 0.847654
  46290/300000: episode: 485, duration: 0.498s, episode steps:  60, steps per second: 120, episode reward: -63.152, mean reward: -1.053 [-100.000, 12.350], mean action: 1.567 [0.000, 3.000],  loss: 29.358346, mse: 2706.720817, mean_q: 29.014722, mean_eps: 0.847344
  46370/300000: episode: 486, duration: 0.601s, episode steps:  80, steps per second: 133, episode reward: -103.922, mean reward: -1.299 [-100.000, 16.490], mean action: 1.575 [0.000, 3.000],  loss: 14.073897, mse: 2649.613708, mean_q: 29.961175, mean_eps: 0.847113
  46435/300000: episode: 487, duration: 0.596s, episode steps:  65, steps per second: 109, episode reward: -93.225, mean reward: -1.434 [-100.000, 13.489], mean action: 1.554 [0.000, 3.000],  loss: 19.568689, mse: 2638.479143, mean_q: 30.985322, mean_eps: 0.846873
  46529/300000: episode: 488, duration: 0.737s, episode steps:  94, steps per second: 128, episode reward: -118.629, mean reward: -1.262 [-100.000,  8.968], mean action: 1.468 [0.000, 3.000],  loss: 15.620709, mse: 2628.132009, mean_q: 30.995187, mean_eps: 0.846611
  46625/300000: episode: 489, duration: 0.679s, episode steps:  96, steps per second: 141, episode reward: -180.994, mean reward: -1.885 [-100.000, 10.400], mean action: 1.542 [0.000, 3.000],  loss: 11.403916, mse: 2636.186607, mean_q: 30.794316, mean_eps: 0.846298
  46750/300000: episode: 490, duration: 0.911s, episode steps: 125, steps per second: 137, episode reward: -133.334, mean reward: -1.067 [-100.000,  5.875], mean action: 1.640 [0.000, 3.000],  loss: 12.820590, mse: 2651.033896, mean_q: 30.936409, mean_eps: 0.845933
  46847/300000: episode: 491, duration: 0.738s, episode steps:  97, steps per second: 131, episode reward: -223.619, mean reward: -2.305 [-100.000,  5.022], mean action: 1.381 [0.000, 3.000],  loss: 18.493163, mse: 2655.046747, mean_q: 29.935005, mean_eps: 0.845567
  46953/300000: episode: 492, duration: 0.773s, episode steps: 106, steps per second: 137, episode reward: -103.034, mean reward: -0.972 [-100.000,  7.680], mean action: 1.377 [0.000, 3.000],  loss: 25.267832, mse: 2655.813048, mean_q: 29.895986, mean_eps: 0.845232
  47038/300000: episode: 493, duration: 0.642s, episode steps:  85, steps per second: 132, episode reward: -180.316, mean reward: -2.121 [-100.000,  6.208], mean action: 1.624 [0.000, 3.000],  loss: 17.220169, mse: 2694.231197, mean_q: 30.262835, mean_eps: 0.844917
  47127/300000: episode: 494, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: -131.810, mean reward: -1.481 [-100.000,  9.594], mean action: 1.382 [0.000, 3.000],  loss: 10.818080, mse: 2725.656349, mean_q: 31.907356, mean_eps: 0.844629
  47206/300000: episode: 495, duration: 0.566s, episode steps:  79, steps per second: 139, episode reward: -78.672, mean reward: -0.996 [-100.000, 13.551], mean action: 1.595 [0.000, 3.000],  loss: 17.169183, mse: 2814.182228, mean_q: 31.242750, mean_eps: 0.844352
  47276/300000: episode: 496, duration: 0.517s, episode steps:  70, steps per second: 135, episode reward: -88.773, mean reward: -1.268 [-100.000, 33.458], mean action: 1.586 [0.000, 3.000],  loss: 16.417872, mse: 2747.856203, mean_q: 30.253310, mean_eps: 0.844106
  47394/300000: episode: 497, duration: 0.839s, episode steps: 118, steps per second: 141, episode reward: -10.266, mean reward: -0.087 [-100.000, 29.796], mean action: 1.525 [0.000, 3.000],  loss: 13.229039, mse: 2794.567669, mean_q: 31.734765, mean_eps: 0.843796
  47526/300000: episode: 498, duration: 0.931s, episode steps: 132, steps per second: 142, episode reward:  1.137, mean reward:  0.009 [-100.000, 87.516], mean action: 1.598 [0.000, 3.000],  loss: 19.185170, mse: 2778.319625, mean_q: 31.304812, mean_eps: 0.843384
  47606/300000: episode: 499, duration: 0.585s, episode steps:  80, steps per second: 137, episode reward: -111.417, mean reward: -1.393 [-100.000, 11.287], mean action: 1.450 [0.000, 3.000],  loss: 13.858006, mse: 2753.688205, mean_q: 32.000689, mean_eps: 0.843034
  47702/300000: episode: 500, duration: 0.701s, episode steps:  96, steps per second: 137, episode reward: -136.746, mean reward: -1.424 [-100.000, 14.409], mean action: 1.531 [0.000, 3.000],  loss: 13.796789, mse: 2743.055269, mean_q: 31.551768, mean_eps: 0.842743
  47783/300000: episode: 501, duration: 0.573s, episode steps:  81, steps per second: 141, episode reward: -82.062, mean reward: -1.013 [-100.000,  6.998], mean action: 1.617 [0.000, 3.000],  loss: 19.375858, mse: 2789.182673, mean_q: 31.035969, mean_eps: 0.842451
  47858/300000: episode: 502, duration: 0.563s, episode steps:  75, steps per second: 133, episode reward: -79.443, mean reward: -1.059 [-100.000,  9.239], mean action: 1.427 [0.000, 3.000],  loss: 16.024548, mse: 2750.970218, mean_q: 31.847761, mean_eps: 0.842194
  47979/300000: episode: 503, duration: 1.015s, episode steps: 121, steps per second: 119, episode reward: -146.703, mean reward: -1.212 [-100.000, 16.625], mean action: 1.496 [0.000, 3.000],  loss: 15.615602, mse: 2763.124944, mean_q: 31.952147, mean_eps: 0.841871
  48061/300000: episode: 504, duration: 0.626s, episode steps:  82, steps per second: 131, episode reward: -65.134, mean reward: -0.794 [-100.000, 17.857], mean action: 1.610 [0.000, 3.000],  loss: 17.455143, mse: 2855.391518, mean_q: 32.833211, mean_eps: 0.841536
  48166/300000: episode: 505, duration: 0.847s, episode steps: 105, steps per second: 124, episode reward: -100.109, mean reward: -0.953 [-100.000,  8.229], mean action: 1.467 [0.000, 3.000],  loss: 13.440133, mse: 2795.743436, mean_q: 31.879508, mean_eps: 0.841227
  48277/300000: episode: 506, duration: 0.864s, episode steps: 111, steps per second: 128, episode reward: -256.933, mean reward: -2.315 [-100.000,  1.118], mean action: 1.468 [0.000, 3.000],  loss: 20.311804, mse: 2869.770048, mean_q: 32.686400, mean_eps: 0.840871
  48343/300000: episode: 507, duration: 0.548s, episode steps:  66, steps per second: 121, episode reward: -178.655, mean reward: -2.707 [-100.000, 112.149], mean action: 1.288 [0.000, 3.000],  loss: 23.721542, mse: 2895.089722, mean_q: 32.733480, mean_eps: 0.840579
  48413/300000: episode: 508, duration: 0.629s, episode steps:  70, steps per second: 111, episode reward: -55.952, mean reward: -0.799 [-100.000,  6.982], mean action: 1.457 [0.000, 3.000],  loss: 18.198055, mse: 2795.605256, mean_q: 30.965242, mean_eps: 0.840354
  48491/300000: episode: 509, duration: 0.634s, episode steps:  78, steps per second: 123, episode reward: -92.839, mean reward: -1.190 [-100.000, 11.899], mean action: 1.321 [0.000, 3.000],  loss: 18.738127, mse: 2950.669440, mean_q: 31.859235, mean_eps: 0.840110
  48591/300000: episode: 510, duration: 0.790s, episode steps: 100, steps per second: 127, episode reward: -57.304, mean reward: -0.573 [-100.000, 12.288], mean action: 1.420 [0.000, 3.000],  loss: 20.161109, mse: 2885.521172, mean_q: 31.061264, mean_eps: 0.839816
  48672/300000: episode: 511, duration: 0.734s, episode steps:  81, steps per second: 110, episode reward: -72.203, mean reward: -0.891 [-100.000, 31.015], mean action: 1.568 [0.000, 3.000],  loss: 12.598777, mse: 2881.893630, mean_q: 32.283462, mean_eps: 0.839518
  48775/300000: episode: 512, duration: 0.894s, episode steps: 103, steps per second: 115, episode reward: -113.218, mean reward: -1.099 [-100.000, 14.334], mean action: 1.505 [0.000, 3.000],  loss: 14.830102, mse: 2840.754515, mean_q: 32.575050, mean_eps: 0.839214
  48914/300000: episode: 513, duration: 1.073s, episode steps: 139, steps per second: 129, episode reward: -168.632, mean reward: -1.213 [-100.000, 10.545], mean action: 1.791 [0.000, 3.000],  loss: 13.381830, mse: 2876.707383, mean_q: 31.827432, mean_eps: 0.838815
  49018/300000: episode: 514, duration: 0.839s, episode steps: 104, steps per second: 124, episode reward: -85.222, mean reward: -0.819 [-100.000, 16.812], mean action: 1.529 [0.000, 3.000],  loss: 14.398968, mse: 2840.461860, mean_q: 31.840711, mean_eps: 0.838414
  49132/300000: episode: 515, duration: 0.902s, episode steps: 114, steps per second: 126, episode reward: -46.490, mean reward: -0.408 [-100.000, 53.570], mean action: 1.579 [0.000, 3.000],  loss: 15.406142, mse: 2977.007967, mean_q: 34.602396, mean_eps: 0.838054
  49238/300000: episode: 516, duration: 0.906s, episode steps: 106, steps per second: 117, episode reward: -105.445, mean reward: -0.995 [-100.000,  8.515], mean action: 1.717 [0.000, 3.000],  loss: 16.881432, mse: 2930.001486, mean_q: 33.386143, mean_eps: 0.837691
  49336/300000: episode: 517, duration: 0.708s, episode steps:  98, steps per second: 138, episode reward: -88.732, mean reward: -0.905 [-100.000, 21.286], mean action: 1.541 [0.000, 3.000],  loss: 24.153116, mse: 2952.175385, mean_q: 32.348735, mean_eps: 0.837355
  49417/300000: episode: 518, duration: 0.699s, episode steps:  81, steps per second: 116, episode reward: -113.672, mean reward: -1.403 [-100.000, 22.342], mean action: 1.568 [0.000, 3.000],  loss: 20.760108, mse: 2971.477473, mean_q: 32.876925, mean_eps: 0.837059
  49497/300000: episode: 519, duration: 0.746s, episode steps:  80, steps per second: 107, episode reward: -122.103, mean reward: -1.526 [-100.000,  5.820], mean action: 1.712 [0.000, 3.000],  loss: 17.550101, mse: 2913.965106, mean_q: 33.331319, mean_eps: 0.836794
  49586/300000: episode: 520, duration: 0.697s, episode steps:  89, steps per second: 128, episode reward: -117.842, mean reward: -1.324 [-100.000,  6.778], mean action: 1.640 [0.000, 3.000],  loss: 15.076068, mse: 2928.278839, mean_q: 33.925990, mean_eps: 0.836515
  49676/300000: episode: 521, duration: 0.713s, episode steps:  90, steps per second: 126, episode reward: -114.562, mean reward: -1.273 [-100.000, 15.957], mean action: 1.478 [0.000, 3.000],  loss: 16.998997, mse: 2954.464317, mean_q: 33.870535, mean_eps: 0.836219
  49795/300000: episode: 522, duration: 1.464s, episode steps: 119, steps per second:  81, episode reward: -94.778, mean reward: -0.796 [-100.000,  7.600], mean action: 1.303 [0.000, 3.000],  loss: 15.323053, mse: 2956.866135, mean_q: 32.613848, mean_eps: 0.835874
  49877/300000: episode: 523, duration: 0.850s, episode steps:  82, steps per second:  96, episode reward: -117.682, mean reward: -1.435 [-100.000,  5.324], mean action: 1.732 [0.000, 3.000],  loss: 15.460969, mse: 2988.946313, mean_q: 31.760655, mean_eps: 0.835543
  49970/300000: episode: 524, duration: 1.207s, episode steps:  93, steps per second:  77, episode reward: -92.907, mean reward: -0.999 [-100.000, 12.040], mean action: 1.462 [0.000, 3.000],  loss: 17.333546, mse: 2870.011774, mean_q: 32.934814, mean_eps: 0.835254
  50047/300000: episode: 525, duration: 0.653s, episode steps:  77, steps per second: 118, episode reward: -120.363, mean reward: -1.563 [-100.000,  8.146], mean action: 1.779 [0.000, 3.000],  loss: 13.244701, mse: 3025.719784, mean_q: 33.082004, mean_eps: 0.834974
  50112/300000: episode: 526, duration: 0.524s, episode steps:  65, steps per second: 124, episode reward: -79.580, mean reward: -1.224 [-100.000, 10.531], mean action: 1.477 [0.000, 3.000],  loss: 22.058531, mse: 3029.462203, mean_q: 34.702637, mean_eps: 0.834739
  50203/300000: episode: 527, duration: 0.788s, episode steps:  91, steps per second: 116, episode reward: -44.764, mean reward: -0.492 [-100.000,  7.297], mean action: 1.604 [0.000, 3.000],  loss: 16.649656, mse: 2976.520186, mean_q: 34.952851, mean_eps: 0.834482
  50278/300000: episode: 528, duration: 0.589s, episode steps:  75, steps per second: 127, episode reward: -70.014, mean reward: -0.934 [-100.000, 13.226], mean action: 1.307 [0.000, 3.000],  loss: 17.381195, mse: 2990.901654, mean_q: 34.305338, mean_eps: 0.834208
  50362/300000: episode: 529, duration: 0.722s, episode steps:  84, steps per second: 116, episode reward: -59.856, mean reward: -0.713 [-100.000,  6.101], mean action: 1.286 [0.000, 3.000],  loss: 16.196862, mse: 3043.152053, mean_q: 34.102692, mean_eps: 0.833946
  50437/300000: episode: 530, duration: 0.722s, episode steps:  75, steps per second: 104, episode reward: -37.541, mean reward: -0.501 [-100.000,  9.982], mean action: 1.720 [0.000, 3.000],  loss: 17.682659, mse: 3026.888617, mean_q: 34.527038, mean_eps: 0.833683
  50500/300000: episode: 531, duration: 0.594s, episode steps:  63, steps per second: 106, episode reward: -94.602, mean reward: -1.502 [-100.000,  8.234], mean action: 1.556 [0.000, 3.000],  loss: 12.431721, mse: 2996.220881, mean_q: 33.752264, mean_eps: 0.833456
  50607/300000: episode: 532, duration: 0.965s, episode steps: 107, steps per second: 111, episode reward: -150.156, mean reward: -1.403 [-100.000,  9.827], mean action: 1.551 [0.000, 3.000],  loss: 21.535179, mse: 3034.285909, mean_q: 34.259395, mean_eps: 0.833175
  50685/300000: episode: 533, duration: 0.703s, episode steps:  78, steps per second: 111, episode reward: -43.496, mean reward: -0.558 [-100.000, 15.349], mean action: 1.654 [0.000, 3.000],  loss: 11.215667, mse: 3015.708365, mean_q: 34.720916, mean_eps: 0.832870
  50764/300000: episode: 534, duration: 0.699s, episode steps:  79, steps per second: 113, episode reward: -63.922, mean reward: -0.809 [-100.000, 61.557], mean action: 1.456 [0.000, 3.000],  loss: 17.649923, mse: 3013.068260, mean_q: 33.749419, mean_eps: 0.832611
  50846/300000: episode: 535, duration: 0.643s, episode steps:  82, steps per second: 128, episode reward: -91.685, mean reward: -1.118 [-100.000,  9.334], mean action: 1.427 [0.000, 3.000],  loss: 11.494923, mse: 3006.852625, mean_q: 33.219618, mean_eps: 0.832345
  50929/300000: episode: 536, duration: 0.668s, episode steps:  83, steps per second: 124, episode reward: -52.689, mean reward: -0.635 [-100.000, 16.811], mean action: 1.578 [0.000, 3.000],  loss: 14.234380, mse: 2981.541566, mean_q: 34.152940, mean_eps: 0.832073
  50996/300000: episode: 537, duration: 0.483s, episode steps:  67, steps per second: 139, episode reward: -158.318, mean reward: -2.363 [-100.000,  7.486], mean action: 1.552 [0.000, 3.000],  loss: 21.137106, mse: 2928.768234, mean_q: 33.901634, mean_eps: 0.831825
  51095/300000: episode: 538, duration: 0.711s, episode steps:  99, steps per second: 139, episode reward: -283.566, mean reward: -2.864 [-100.000, 68.685], mean action: 1.869 [0.000, 3.000],  loss: 17.136171, mse: 3073.716819, mean_q: 34.924870, mean_eps: 0.831551
  51198/300000: episode: 539, duration: 0.820s, episode steps: 103, steps per second: 126, episode reward: -247.896, mean reward: -2.407 [-100.000, 55.342], mean action: 1.495 [0.000, 3.000],  loss: 15.042527, mse: 3175.150779, mean_q: 35.716359, mean_eps: 0.831218
  51265/300000: episode: 540, duration: 0.493s, episode steps:  67, steps per second: 136, episode reward: -137.860, mean reward: -2.058 [-100.000, 15.744], mean action: 1.597 [0.000, 3.000],  loss: 20.287987, mse: 3085.323647, mean_q: 34.014403, mean_eps: 0.830938
  51331/300000: episode: 541, duration: 0.475s, episode steps:  66, steps per second: 139, episode reward: -73.561, mean reward: -1.115 [-100.000,  9.737], mean action: 1.833 [0.000, 3.000],  loss: 16.657424, mse: 3096.904112, mean_q: 35.077011, mean_eps: 0.830718
  51435/300000: episode: 542, duration: 0.736s, episode steps: 104, steps per second: 141, episode reward: -71.871, mean reward: -0.691 [-100.000, 14.319], mean action: 1.558 [0.000, 3.000],  loss: 15.302982, mse: 3154.261071, mean_q: 34.651165, mean_eps: 0.830438
  51529/300000: episode: 543, duration: 0.745s, episode steps:  94, steps per second: 126, episode reward: -79.421, mean reward: -0.845 [-100.000, 21.030], mean action: 1.585 [0.000, 3.000],  loss: 16.929871, mse: 3101.584247, mean_q: 34.333282, mean_eps: 0.830111
  51604/300000: episode: 544, duration: 0.691s, episode steps:  75, steps per second: 109, episode reward: -95.551, mean reward: -1.274 [-100.000, 13.302], mean action: 1.507 [0.000, 3.000],  loss: 17.896822, mse: 3180.247067, mean_q: 34.648994, mean_eps: 0.829832
  51691/300000: episode: 545, duration: 0.758s, episode steps:  87, steps per second: 115, episode reward: -203.207, mean reward: -2.336 [-100.000,  8.502], mean action: 1.724 [0.000, 3.000],  loss: 17.937035, mse: 3127.479439, mean_q: 33.616738, mean_eps: 0.829565
  51784/300000: episode: 546, duration: 0.765s, episode steps:  93, steps per second: 122, episode reward: -91.168, mean reward: -0.980 [-100.000, 17.671], mean action: 1.376 [0.000, 3.000],  loss: 17.125345, mse: 3118.610976, mean_q: 33.127131, mean_eps: 0.829268
  51901/300000: episode: 547, duration: 0.829s, episode steps: 117, steps per second: 141, episode reward: -60.084, mean reward: -0.514 [-100.000, 16.664], mean action: 1.504 [0.000, 3.000],  loss: 17.557482, mse: 3108.386675, mean_q: 34.500601, mean_eps: 0.828921
  51976/300000: episode: 548, duration: 0.548s, episode steps:  75, steps per second: 137, episode reward: -153.582, mean reward: -2.048 [-100.000,  7.788], mean action: 1.587 [0.000, 3.000],  loss: 18.832988, mse: 3217.778105, mean_q: 33.950541, mean_eps: 0.828605
  52091/300000: episode: 549, duration: 0.902s, episode steps: 115, steps per second: 128, episode reward: -122.035, mean reward: -1.061 [-100.000,  6.840], mean action: 1.583 [0.000, 3.000],  loss: 17.023897, mse: 3250.888759, mean_q: 35.884752, mean_eps: 0.828291
  52186/300000: episode: 550, duration: 0.680s, episode steps:  95, steps per second: 140, episode reward: -181.616, mean reward: -1.912 [-100.000, 81.977], mean action: 1.495 [0.000, 3.000],  loss: 18.242173, mse: 3232.405443, mean_q: 34.980247, mean_eps: 0.827945
  52293/300000: episode: 551, duration: 0.746s, episode steps: 107, steps per second: 143, episode reward: -215.120, mean reward: -2.010 [-100.000, 26.773], mean action: 1.486 [0.000, 3.000],  loss: 17.389025, mse: 3254.847563, mean_q: 34.878194, mean_eps: 0.827611
  52372/300000: episode: 552, duration: 0.596s, episode steps:  79, steps per second: 132, episode reward: -80.502, mean reward: -1.019 [-100.000, 12.364], mean action: 1.532 [0.000, 3.000],  loss: 15.854283, mse: 3289.135557, mean_q: 34.530728, mean_eps: 0.827304
  52507/300000: episode: 553, duration: 0.972s, episode steps: 135, steps per second: 139, episode reward: -127.605, mean reward: -0.945 [-100.000,  5.142], mean action: 1.637 [0.000, 3.000],  loss: 14.433458, mse: 3279.134136, mean_q: 35.197251, mean_eps: 0.826951
  52619/300000: episode: 554, duration: 0.849s, episode steps: 112, steps per second: 132, episode reward: -92.163, mean reward: -0.823 [-100.000,  8.357], mean action: 1.679 [0.000, 3.000],  loss: 22.050691, mse: 3250.026045, mean_q: 34.818658, mean_eps: 0.826544
  52688/300000: episode: 555, duration: 0.543s, episode steps:  69, steps per second: 127, episode reward: -104.753, mean reward: -1.518 [-100.000, 10.124], mean action: 1.493 [0.000, 3.000],  loss: 12.381830, mse: 3312.928700, mean_q: 37.480845, mean_eps: 0.826245
  52801/300000: episode: 556, duration: 0.889s, episode steps: 113, steps per second: 127, episode reward: -155.442, mean reward: -1.376 [-100.000,  5.641], mean action: 1.637 [0.000, 3.000],  loss: 27.787544, mse: 3264.489027, mean_q: 35.514549, mean_eps: 0.825945
  52909/300000: episode: 557, duration: 0.857s, episode steps: 108, steps per second: 126, episode reward: -243.937, mean reward: -2.259 [-100.000,  0.788], mean action: 1.713 [0.000, 3.000],  loss: 14.844729, mse: 3236.497927, mean_q: 34.840427, mean_eps: 0.825580
  53009/300000: episode: 558, duration: 0.829s, episode steps: 100, steps per second: 121, episode reward: -35.935, mean reward: -0.359 [-100.000, 57.708], mean action: 1.610 [0.000, 3.000],  loss: 16.324414, mse: 3200.964658, mean_q: 35.188468, mean_eps: 0.825237
  53074/300000: episode: 559, duration: 0.540s, episode steps:  65, steps per second: 120, episode reward: -59.774, mean reward: -0.920 [-100.000,  8.707], mean action: 1.385 [0.000, 3.000],  loss: 22.582168, mse: 3256.668070, mean_q: 35.490071, mean_eps: 0.824965
  53182/300000: episode: 560, duration: 0.934s, episode steps: 108, steps per second: 116, episode reward: -106.024, mean reward: -0.982 [-100.000, 15.953], mean action: 1.685 [0.000, 3.000],  loss: 18.606456, mse: 3255.395610, mean_q: 36.161663, mean_eps: 0.824679
  53313/300000: episode: 561, duration: 1.013s, episode steps: 131, steps per second: 129, episode reward: -73.317, mean reward: -0.560 [-100.000, 12.909], mean action: 1.626 [0.000, 3.000],  loss: 18.425480, mse: 3257.860059, mean_q: 36.455760, mean_eps: 0.824285
  53413/300000: episode: 562, duration: 0.908s, episode steps: 100, steps per second: 110, episode reward: -97.752, mean reward: -0.978 [-100.000, 13.381], mean action: 1.500 [0.000, 3.000],  loss: 19.908565, mse: 3235.484751, mean_q: 36.219480, mean_eps: 0.823904
  53523/300000: episode: 563, duration: 1.084s, episode steps: 110, steps per second: 101, episode reward: -94.374, mean reward: -0.858 [-100.000, 12.848], mean action: 1.609 [0.000, 3.000],  loss: 20.124019, mse: 3252.893435, mean_q: 34.973271, mean_eps: 0.823557
  53631/300000: episode: 564, duration: 1.099s, episode steps: 108, steps per second:  98, episode reward: -110.251, mean reward: -1.021 [-100.000,  5.962], mean action: 1.389 [0.000, 3.000],  loss: 17.155034, mse: 3250.276324, mean_q: 36.770279, mean_eps: 0.823198
  53706/300000: episode: 565, duration: 0.625s, episode steps:  75, steps per second: 120, episode reward: -85.592, mean reward: -1.141 [-100.000,  7.861], mean action: 1.587 [0.000, 3.000],  loss: 17.244100, mse: 3275.698783, mean_q: 36.679873, mean_eps: 0.822896
  53802/300000: episode: 566, duration: 0.817s, episode steps:  96, steps per second: 117, episode reward: -87.438, mean reward: -0.911 [-100.000, 10.152], mean action: 1.594 [0.000, 3.000],  loss: 18.223277, mse: 3303.936996, mean_q: 37.090692, mean_eps: 0.822613
  53893/300000: episode: 567, duration: 0.728s, episode steps:  91, steps per second: 125, episode reward: -105.261, mean reward: -1.157 [-100.000,  6.196], mean action: 1.308 [0.000, 3.000],  loss: 13.442051, mse: 3341.868196, mean_q: 35.487818, mean_eps: 0.822305
  53972/300000: episode: 568, duration: 1.564s, episode steps:  79, steps per second:  51, episode reward: -317.272, mean reward: -4.016 [-100.000,  7.124], mean action: 1.468 [0.000, 3.000],  loss: 23.284689, mse: 3322.979841, mean_q: 37.011768, mean_eps: 0.822024
  54089/300000: episode: 569, duration: 1.142s, episode steps: 117, steps per second: 102, episode reward: -180.641, mean reward: -1.544 [-100.000, 12.797], mean action: 1.444 [0.000, 3.000],  loss: 13.550754, mse: 3304.044127, mean_q: 36.965685, mean_eps: 0.821701
  54199/300000: episode: 570, duration: 0.818s, episode steps: 110, steps per second: 134, episode reward: -109.175, mean reward: -0.992 [-100.000, 13.268], mean action: 1.545 [0.000, 3.000],  loss: 13.980644, mse: 3333.176596, mean_q: 36.685338, mean_eps: 0.821326
  54304/300000: episode: 571, duration: 0.791s, episode steps: 105, steps per second: 133, episode reward: -88.485, mean reward: -0.843 [-100.000, 19.056], mean action: 1.610 [0.000, 3.000],  loss: 18.125849, mse: 3355.855287, mean_q: 36.544249, mean_eps: 0.820972
  54388/300000: episode: 572, duration: 0.622s, episode steps:  84, steps per second: 135, episode reward: -111.210, mean reward: -1.324 [-100.000, 10.453], mean action: 1.440 [0.000, 3.000],  loss: 13.567460, mse: 3395.098671, mean_q: 36.098880, mean_eps: 0.820660
  54456/300000: episode: 573, duration: 0.479s, episode steps:  68, steps per second: 142, episode reward: -108.321, mean reward: -1.593 [-100.000,  4.293], mean action: 1.838 [0.000, 3.000],  loss: 10.292023, mse: 3248.582117, mean_q: 37.040694, mean_eps: 0.820409
  54573/300000: episode: 574, duration: 0.821s, episode steps: 117, steps per second: 143, episode reward: -52.036, mean reward: -0.445 [-100.000,  8.002], mean action: 1.632 [0.000, 3.000],  loss: 10.677543, mse: 3264.740666, mean_q: 36.852425, mean_eps: 0.820104
  54670/300000: episode: 575, duration: 0.724s, episode steps:  97, steps per second: 134, episode reward: -101.718, mean reward: -1.049 [-100.000, 12.068], mean action: 1.577 [0.000, 3.000],  loss: 19.664366, mse: 3297.269848, mean_q: 37.216957, mean_eps: 0.819751
  54783/300000: episode: 576, duration: 0.789s, episode steps: 113, steps per second: 143, episode reward: -78.449, mean reward: -0.694 [-100.000, 12.410], mean action: 1.690 [0.000, 3.000],  loss: 17.219119, mse: 3316.583989, mean_q: 36.088154, mean_eps: 0.819404
  54874/300000: episode: 577, duration: 0.627s, episode steps:  91, steps per second: 145, episode reward: -101.735, mean reward: -1.118 [-100.000, 12.037], mean action: 1.516 [0.000, 3.000],  loss: 16.474327, mse: 3382.547844, mean_q: 36.671931, mean_eps: 0.819068
  54969/300000: episode: 578, duration: 0.715s, episode steps:  95, steps per second: 133, episode reward: -95.280, mean reward: -1.003 [-100.000,  7.181], mean action: 1.537 [0.000, 3.000],  loss: 14.082315, mse: 3287.878549, mean_q: 36.629890, mean_eps: 0.818761
  55047/300000: episode: 579, duration: 0.595s, episode steps:  78, steps per second: 131, episode reward: -36.868, mean reward: -0.473 [-100.000, 18.512], mean action: 1.538 [0.000, 3.000],  loss: 17.297678, mse: 3342.984585, mean_q: 36.444020, mean_eps: 0.818475
  55138/300000: episode: 580, duration: 0.649s, episode steps:  91, steps per second: 140, episode reward: -118.145, mean reward: -1.298 [-100.000, 13.075], mean action: 1.758 [0.000, 3.000],  loss: 15.767579, mse: 3420.414618, mean_q: 38.601798, mean_eps: 0.818196
  55225/300000: episode: 581, duration: 0.702s, episode steps:  87, steps per second: 124, episode reward: -106.557, mean reward: -1.225 [-100.000, 17.089], mean action: 1.402 [0.000, 3.000],  loss: 20.398342, mse: 3399.484788, mean_q: 38.454522, mean_eps: 0.817903
  55306/300000: episode: 582, duration: 0.649s, episode steps:  81, steps per second: 125, episode reward: -42.626, mean reward: -0.526 [-100.000,  8.266], mean action: 1.704 [0.000, 3.000],  loss: 17.225374, mse: 3380.903149, mean_q: 37.787208, mean_eps: 0.817626
  55376/300000: episode: 583, duration: 0.613s, episode steps:  70, steps per second: 114, episode reward: -90.630, mean reward: -1.295 [-100.000, 27.729], mean action: 1.457 [0.000, 3.000],  loss: 19.272363, mse: 3433.851775, mean_q: 38.307418, mean_eps: 0.817376
  55498/300000: episode: 584, duration: 1.080s, episode steps: 122, steps per second: 113, episode reward: -86.766, mean reward: -0.711 [-100.000, 13.357], mean action: 1.631 [0.000, 3.000],  loss: 16.300193, mse: 3374.976597, mean_q: 37.727982, mean_eps: 0.817060
  55578/300000: episode: 585, duration: 0.698s, episode steps:  80, steps per second: 115, episode reward: -103.788, mean reward: -1.297 [-100.000, 15.337], mean action: 1.738 [0.000, 3.000],  loss: 10.306029, mse: 3366.061914, mean_q: 38.660834, mean_eps: 0.816726
  55671/300000: episode: 586, duration: 0.738s, episode steps:  93, steps per second: 126, episode reward: -90.740, mean reward: -0.976 [-100.000,  8.802], mean action: 1.656 [0.000, 3.000],  loss: 16.915742, mse: 3390.741080, mean_q: 36.930372, mean_eps: 0.816441
  55778/300000: episode: 587, duration: 0.877s, episode steps: 107, steps per second: 122, episode reward: -232.069, mean reward: -2.169 [-100.000,  1.140], mean action: 1.617 [0.000, 3.000],  loss: 12.641933, mse: 3321.459733, mean_q: 37.445441, mean_eps: 0.816111
  55866/300000: episode: 588, duration: 0.673s, episode steps:  88, steps per second: 131, episode reward: -74.565, mean reward: -0.847 [-100.000,  8.484], mean action: 1.466 [0.000, 3.000],  loss: 18.519741, mse: 3363.116846, mean_q: 38.598783, mean_eps: 0.815789
  55992/300000: episode: 589, duration: 1.005s, episode steps: 126, steps per second: 125, episode reward: -85.924, mean reward: -0.682 [-100.000, 15.518], mean action: 1.627 [0.000, 3.000],  loss: 13.406851, mse: 3400.627147, mean_q: 37.250589, mean_eps: 0.815436
  56060/300000: episode: 590, duration: 0.598s, episode steps:  68, steps per second: 114, episode reward: -80.171, mean reward: -1.179 [-100.000, 11.807], mean action: 1.368 [0.000, 3.000],  loss: 14.856065, mse: 3460.607357, mean_q: 38.077065, mean_eps: 0.815116
  56128/300000: episode: 591, duration: 0.522s, episode steps:  68, steps per second: 130, episode reward: -98.826, mean reward: -1.453 [-100.000,  6.761], mean action: 1.235 [0.000, 3.000],  loss: 13.238364, mse: 3517.422776, mean_q: 37.998802, mean_eps: 0.814891
  56193/300000: episode: 592, duration: 0.510s, episode steps:  65, steps per second: 127, episode reward: -60.414, mean reward: -0.929 [-100.000,  8.743], mean action: 1.246 [0.000, 3.000],  loss: 13.962193, mse: 3366.749200, mean_q: 37.637668, mean_eps: 0.814672
  56276/300000: episode: 593, duration: 0.667s, episode steps:  83, steps per second: 125, episode reward: -94.552, mean reward: -1.139 [-100.000,  6.508], mean action: 1.458 [0.000, 3.000],  loss: 13.355648, mse: 3489.785600, mean_q: 38.231975, mean_eps: 0.814428
  56398/300000: episode: 594, duration: 0.929s, episode steps: 122, steps per second: 131, episode reward: -83.708, mean reward: -0.686 [-100.000, 12.167], mean action: 1.598 [0.000, 3.000],  loss: 15.961318, mse: 3458.755933, mean_q: 38.469754, mean_eps: 0.814090
  56496/300000: episode: 595, duration: 0.762s, episode steps:  98, steps per second: 129, episode reward: -108.697, mean reward: -1.109 [-100.000, 11.865], mean action: 1.500 [0.000, 3.000],  loss: 19.750446, mse: 3460.349756, mean_q: 37.230832, mean_eps: 0.813727
  56607/300000: episode: 596, duration: 0.882s, episode steps: 111, steps per second: 126, episode reward: -88.670, mean reward: -0.799 [-100.000, 17.227], mean action: 1.604 [0.000, 3.000],  loss: 19.745575, mse: 3495.934258, mean_q: 38.204615, mean_eps: 0.813382
  56704/300000: episode: 597, duration: 0.696s, episode steps:  97, steps per second: 139, episode reward: -75.528, mean reward: -0.779 [-100.000, 10.590], mean action: 1.557 [0.000, 3.000],  loss: 16.820421, mse: 3435.167941, mean_q: 38.189272, mean_eps: 0.813039
  56783/300000: episode: 598, duration: 0.552s, episode steps:  79, steps per second: 143, episode reward: -90.927, mean reward: -1.151 [-100.000,  8.419], mean action: 1.443 [0.000, 3.000],  loss: 16.176241, mse: 3510.930105, mean_q: 38.346432, mean_eps: 0.812748
  56861/300000: episode: 599, duration: 0.633s, episode steps:  78, steps per second: 123, episode reward: -58.332, mean reward: -0.748 [-100.000, 11.137], mean action: 1.513 [0.000, 3.000],  loss: 16.162825, mse: 3430.261218, mean_q: 39.576150, mean_eps: 0.812489
  56937/300000: episode: 600, duration: 0.611s, episode steps:  76, steps per second: 124, episode reward: -72.075, mean reward: -0.948 [-100.000,  9.325], mean action: 1.658 [0.000, 3.000],  loss: 13.837838, mse: 3503.622787, mean_q: 38.115030, mean_eps: 0.812235
  57018/300000: episode: 601, duration: 0.636s, episode steps:  81, steps per second: 127, episode reward: -75.287, mean reward: -0.929 [-100.000,  9.874], mean action: 1.667 [0.000, 3.000],  loss: 14.460365, mse: 3562.589907, mean_q: 39.833007, mean_eps: 0.811976
  57107/300000: episode: 602, duration: 0.672s, episode steps:  89, steps per second: 132, episode reward: -117.114, mean reward: -1.316 [-100.000, 12.628], mean action: 1.663 [0.000, 3.000],  loss: 12.226447, mse: 3541.872029, mean_q: 39.280187, mean_eps: 0.811695
  57248/300000: episode: 603, duration: 1.029s, episode steps: 141, steps per second: 137, episode reward: -63.799, mean reward: -0.452 [-100.000,  6.656], mean action: 1.482 [0.000, 3.000],  loss: 18.123102, mse: 3473.483111, mean_q: 36.093858, mean_eps: 0.811316
  57346/300000: episode: 604, duration: 0.944s, episode steps:  98, steps per second: 104, episode reward: -96.375, mean reward: -0.983 [-100.000, 11.219], mean action: 1.653 [0.000, 3.000],  loss: 13.087374, mse: 3578.275074, mean_q: 38.530926, mean_eps: 0.810922
  57455/300000: episode: 605, duration: 0.939s, episode steps: 109, steps per second: 116, episode reward: -141.812, mean reward: -1.301 [-100.000, 20.620], mean action: 1.771 [0.000, 3.000],  loss: 18.523550, mse: 3575.615010, mean_q: 38.881249, mean_eps: 0.810580
  57530/300000: episode: 606, duration: 0.664s, episode steps:  75, steps per second: 113, episode reward: -52.881, mean reward: -0.705 [-100.000, 16.399], mean action: 1.587 [0.000, 3.000],  loss: 14.720544, mse: 3496.904378, mean_q: 38.312221, mean_eps: 0.810276
  57603/300000: episode: 607, duration: 0.773s, episode steps:  73, steps per second:  94, episode reward: -68.288, mean reward: -0.935 [-100.000, 11.700], mean action: 1.616 [0.000, 3.000],  loss: 11.191971, mse: 3480.910019, mean_q: 38.124520, mean_eps: 0.810032
  57712/300000: episode: 608, duration: 1.433s, episode steps: 109, steps per second:  76, episode reward: -96.700, mean reward: -0.887 [-100.000, 13.321], mean action: 1.459 [0.000, 3.000],  loss: 14.000033, mse: 3448.830358, mean_q: 36.937785, mean_eps: 0.809732
  57822/300000: episode: 609, duration: 0.999s, episode steps: 110, steps per second: 110, episode reward: -132.512, mean reward: -1.205 [-100.000, 11.026], mean action: 1.418 [0.000, 3.000],  loss: 13.920079, mse: 3481.981854, mean_q: 38.756411, mean_eps: 0.809371
  57895/300000: episode: 610, duration: 0.619s, episode steps:  73, steps per second: 118, episode reward: -70.139, mean reward: -0.961 [-100.000,  8.051], mean action: 1.630 [0.000, 3.000],  loss: 13.628304, mse: 3487.045825, mean_q: 39.016258, mean_eps: 0.809069
  58013/300000: episode: 611, duration: 1.250s, episode steps: 118, steps per second:  94, episode reward: -189.811, mean reward: -1.609 [-100.000,  4.624], mean action: 1.483 [0.000, 3.000],  loss: 11.618552, mse: 3509.501177, mean_q: 40.071471, mean_eps: 0.808753
  58085/300000: episode: 612, duration: 0.609s, episode steps:  72, steps per second: 118, episode reward: -69.283, mean reward: -0.962 [-100.000, 20.029], mean action: 1.431 [0.000, 3.000],  loss: 12.187858, mse: 3628.425171, mean_q: 41.735102, mean_eps: 0.808440
  58163/300000: episode: 613, duration: 0.622s, episode steps:  78, steps per second: 125, episode reward: -133.330, mean reward: -1.709 [-100.000,  9.031], mean action: 1.756 [0.000, 3.000],  loss: 16.511695, mse: 3510.661308, mean_q: 39.664741, mean_eps: 0.808192
  58276/300000: episode: 614, duration: 0.896s, episode steps: 113, steps per second: 126, episode reward: -191.956, mean reward: -1.699 [-100.000,  1.163], mean action: 1.673 [0.000, 3.000],  loss: 14.184492, mse: 3566.632754, mean_q: 39.584609, mean_eps: 0.807877
  58341/300000: episode: 615, duration: 0.509s, episode steps:  65, steps per second: 128, episode reward: -45.485, mean reward: -0.700 [-100.000, 17.474], mean action: 1.477 [0.000, 3.000],  loss: 20.466504, mse: 3629.111948, mean_q: 40.679685, mean_eps: 0.807584
  58414/300000: episode: 616, duration: 0.565s, episode steps:  73, steps per second: 129, episode reward: -107.292, mean reward: -1.470 [-100.000, 19.813], mean action: 1.644 [0.000, 3.000],  loss: 14.145993, mse: 3643.026782, mean_q: 40.944367, mean_eps: 0.807356
  58493/300000: episode: 617, duration: 0.575s, episode steps:  79, steps per second: 137, episode reward: -85.860, mean reward: -1.087 [-100.000, 10.346], mean action: 1.797 [0.000, 3.000],  loss: 13.759632, mse: 3585.778058, mean_q: 39.551833, mean_eps: 0.807105
  58608/300000: episode: 618, duration: 0.866s, episode steps: 115, steps per second: 133, episode reward: -113.832, mean reward: -0.990 [-100.000, 10.435], mean action: 1.513 [0.000, 3.000],  loss: 18.034451, mse: 3494.866646, mean_q: 39.632918, mean_eps: 0.806785
  58745/300000: episode: 619, duration: 0.975s, episode steps: 137, steps per second: 140, episode reward: -77.551, mean reward: -0.566 [-100.000, 10.491], mean action: 1.526 [0.000, 3.000],  loss: 20.757609, mse: 3583.152611, mean_q: 39.755778, mean_eps: 0.806369
  58874/300000: episode: 620, duration: 0.937s, episode steps: 129, steps per second: 138, episode reward: -155.581, mean reward: -1.206 [-100.000,  4.898], mean action: 1.419 [0.000, 3.000],  loss: 9.574281, mse: 3611.829709, mean_q: 40.478790, mean_eps: 0.805930
  58983/300000: episode: 621, duration: 0.786s, episode steps: 109, steps per second: 139, episode reward: -97.434, mean reward: -0.894 [-100.000,  7.691], mean action: 1.404 [0.000, 3.000],  loss: 18.209005, mse: 3602.425911, mean_q: 41.074324, mean_eps: 0.805538
  59074/300000: episode: 622, duration: 0.709s, episode steps:  91, steps per second: 128, episode reward: -144.017, mean reward: -1.583 [-100.000,  9.580], mean action: 1.571 [0.000, 3.000],  loss: 15.147657, mse: 3656.114639, mean_q: 42.537714, mean_eps: 0.805208
  59148/300000: episode: 623, duration: 0.607s, episode steps:  74, steps per second: 122, episode reward: -70.341, mean reward: -0.951 [-100.000, 12.210], mean action: 1.486 [0.000, 3.000],  loss: 18.901318, mse: 3743.268208, mean_q: 43.173760, mean_eps: 0.804935
  59229/300000: episode: 624, duration: 0.629s, episode steps:  81, steps per second: 129, episode reward: -34.153, mean reward: -0.422 [-100.000, 11.408], mean action: 1.568 [0.000, 3.000],  loss: 15.378240, mse: 3616.587420, mean_q: 41.702990, mean_eps: 0.804680
  59311/300000: episode: 625, duration: 0.777s, episode steps:  82, steps per second: 106, episode reward: -111.267, mean reward: -1.357 [-100.000,  6.406], mean action: 1.488 [0.000, 3.000],  loss: 22.516128, mse: 3674.237534, mean_q: 41.568614, mean_eps: 0.804411
  59403/300000: episode: 626, duration: 0.732s, episode steps:  92, steps per second: 126, episode reward: -126.724, mean reward: -1.377 [-100.000, 18.158], mean action: 1.652 [0.000, 3.000],  loss: 18.467722, mse: 3663.624270, mean_q: 42.038623, mean_eps: 0.804124
  59482/300000: episode: 627, duration: 0.597s, episode steps:  79, steps per second: 132, episode reward: -97.943, mean reward: -1.240 [-100.000,  6.004], mean action: 1.608 [0.000, 3.000],  loss: 11.485086, mse: 3755.468503, mean_q: 41.688840, mean_eps: 0.803841
  59584/300000: episode: 628, duration: 0.752s, episode steps: 102, steps per second: 136, episode reward: -114.667, mean reward: -1.124 [-100.000, 12.230], mean action: 1.588 [0.000, 3.000],  loss: 18.028964, mse: 3645.122219, mean_q: 41.597045, mean_eps: 0.803543
  59650/300000: episode: 629, duration: 0.495s, episode steps:  66, steps per second: 133, episode reward: -107.647, mean reward: -1.631 [-100.000,  6.675], mean action: 1.833 [0.000, 3.000],  loss: 13.525649, mse: 3726.645952, mean_q: 43.246072, mean_eps: 0.803266
  59749/300000: episode: 630, duration: 0.757s, episode steps:  99, steps per second: 131, episode reward: -133.996, mean reward: -1.353 [-100.000,  6.069], mean action: 1.475 [0.000, 3.000],  loss: 23.137312, mse: 3684.060729, mean_q: 42.470808, mean_eps: 0.802993
  59855/300000: episode: 631, duration: 0.768s, episode steps: 106, steps per second: 138, episode reward: -55.408, mean reward: -0.523 [-100.000, 11.989], mean action: 1.453 [0.000, 3.000],  loss: 13.875226, mse: 3672.306005, mean_q: 40.998411, mean_eps: 0.802655
  59992/300000: episode: 632, duration: 1.008s, episode steps: 137, steps per second: 136, episode reward: -73.283, mean reward: -0.535 [-100.000, 16.950], mean action: 1.569 [0.000, 3.000],  loss: 10.459400, mse: 3677.261560, mean_q: 41.009273, mean_eps: 0.802254
  60093/300000: episode: 633, duration: 0.734s, episode steps: 101, steps per second: 138, episode reward: -89.248, mean reward: -0.884 [-100.000, 12.760], mean action: 1.505 [0.000, 3.000],  loss: 16.201469, mse: 3700.216243, mean_q: 41.899664, mean_eps: 0.801861
  60190/300000: episode: 634, duration: 0.716s, episode steps:  97, steps per second: 136, episode reward: -331.066, mean reward: -3.413 [-100.000, 92.925], mean action: 1.371 [0.000, 3.000],  loss: 12.669455, mse: 3747.278549, mean_q: 43.474125, mean_eps: 0.801535
  60274/300000: episode: 635, duration: 0.658s, episode steps:  84, steps per second: 128, episode reward: -125.157, mean reward: -1.490 [-100.000,  7.924], mean action: 1.738 [0.000, 3.000],  loss: 17.608945, mse: 3704.893537, mean_q: 41.535584, mean_eps: 0.801236
  60354/300000: episode: 636, duration: 0.885s, episode steps:  80, steps per second:  90, episode reward: -76.340, mean reward: -0.954 [-100.000, 15.967], mean action: 1.637 [0.000, 3.000],  loss: 15.776549, mse: 3800.066046, mean_q: 43.281728, mean_eps: 0.800965
  60448/300000: episode: 637, duration: 0.873s, episode steps:  94, steps per second: 108, episode reward: -161.134, mean reward: -1.714 [-100.000,  9.451], mean action: 1.404 [0.000, 3.000],  loss: 10.849307, mse: 3740.420859, mean_q: 41.566976, mean_eps: 0.800678
  60560/300000: episode: 638, duration: 0.902s, episode steps: 112, steps per second: 124, episode reward: -69.016, mean reward: -0.616 [-100.000, 12.471], mean action: 1.562 [0.000, 3.000],  loss: 16.877877, mse: 3701.022836, mean_q: 42.217591, mean_eps: 0.800338
  60660/300000: episode: 639, duration: 1.080s, episode steps: 100, steps per second:  93, episode reward: -222.471, mean reward: -2.225 [-100.000,  1.721], mean action: 1.780 [0.000, 3.000],  loss: 14.335560, mse: 3736.049507, mean_q: 41.717425, mean_eps: 0.799989
  60744/300000: episode: 640, duration: 0.665s, episode steps:  84, steps per second: 126, episode reward: -77.687, mean reward: -0.925 [-100.000,  6.872], mean action: 1.607 [0.000, 3.000],  loss: 14.023391, mse: 3697.839934, mean_q: 42.459698, mean_eps: 0.799685
  60824/300000: episode: 641, duration: 0.722s, episode steps:  80, steps per second: 111, episode reward: -70.834, mean reward: -0.885 [-100.000, 19.752], mean action: 1.425 [0.000, 3.000],  loss: 11.610648, mse: 3728.236761, mean_q: 42.947536, mean_eps: 0.799414
  60920/300000: episode: 642, duration: 1.132s, episode steps:  96, steps per second:  85, episode reward: -100.802, mean reward: -1.050 [-100.000, 17.378], mean action: 1.562 [0.000, 3.000],  loss: 12.306866, mse: 3718.178749, mean_q: 43.638870, mean_eps: 0.799124
  60998/300000: episode: 643, duration: 1.002s, episode steps:  78, steps per second:  78, episode reward: -77.966, mean reward: -1.000 [-100.000,  8.337], mean action: 1.782 [0.000, 3.000],  loss: 13.958672, mse: 3771.683854, mean_q: 42.016142, mean_eps: 0.798837
  61107/300000: episode: 644, duration: 0.833s, episode steps: 109, steps per second: 131, episode reward: -99.615, mean reward: -0.914 [-100.000, 16.584], mean action: 1.541 [0.000, 3.000],  loss: 17.802654, mse: 3812.488640, mean_q: 42.630400, mean_eps: 0.798528
  61205/300000: episode: 645, duration: 1.117s, episode steps:  98, steps per second:  88, episode reward: -118.049, mean reward: -1.205 [-100.000,  9.609], mean action: 1.643 [0.000, 3.000],  loss: 15.279122, mse: 3733.314291, mean_q: 42.801604, mean_eps: 0.798187
  61308/300000: episode: 646, duration: 0.901s, episode steps: 103, steps per second: 114, episode reward: -93.471, mean reward: -0.907 [-100.000, 25.896], mean action: 1.709 [0.000, 3.000],  loss: 17.421404, mse: 3748.504153, mean_q: 41.849590, mean_eps: 0.797855
  61456/300000: episode: 647, duration: 1.124s, episode steps: 148, steps per second: 132, episode reward: -33.958, mean reward: -0.229 [-100.000, 10.335], mean action: 1.554 [0.000, 3.000],  loss: 16.875710, mse: 3701.688605, mean_q: 41.660052, mean_eps: 0.797441
  61537/300000: episode: 648, duration: 0.573s, episode steps:  81, steps per second: 141, episode reward: -48.224, mean reward: -0.595 [-100.000,  9.321], mean action: 1.481 [0.000, 3.000],  loss: 17.106369, mse: 3710.698975, mean_q: 41.545110, mean_eps: 0.797063
  61669/300000: episode: 649, duration: 0.948s, episode steps: 132, steps per second: 139, episode reward: -84.481, mean reward: -0.640 [-100.000, 23.375], mean action: 1.500 [0.000, 3.000],  loss: 9.969036, mse: 3762.017708, mean_q: 42.368630, mean_eps: 0.796712
  61790/300000: episode: 650, duration: 0.917s, episode steps: 121, steps per second: 132, episode reward: -46.532, mean reward: -0.385 [-100.000, 19.311], mean action: 1.504 [0.000, 3.000],  loss: 24.060452, mse: 3693.096609, mean_q: 42.916366, mean_eps: 0.796294
  61890/300000: episode: 651, duration: 0.733s, episode steps: 100, steps per second: 136, episode reward: -82.394, mean reward: -0.824 [-100.000, 11.975], mean action: 1.530 [0.000, 3.000],  loss: 14.085570, mse: 3705.501929, mean_q: 42.695243, mean_eps: 0.795930
  61959/300000: episode: 652, duration: 0.564s, episode steps:  69, steps per second: 122, episode reward: -47.582, mean reward: -0.690 [-100.000, 21.163], mean action: 1.609 [0.000, 3.000],  loss: 12.046057, mse: 3698.123786, mean_q: 41.867486, mean_eps: 0.795651
  62056/300000: episode: 653, duration: 0.776s, episode steps:  97, steps per second: 125, episode reward: -50.275, mean reward: -0.518 [-100.000, 14.408], mean action: 1.763 [0.000, 3.000],  loss: 17.966122, mse: 3716.303333, mean_q: 42.758286, mean_eps: 0.795377
  62127/300000: episode: 654, duration: 0.574s, episode steps:  71, steps per second: 124, episode reward: -84.287, mean reward: -1.187 [-100.000,  6.639], mean action: 1.577 [0.000, 3.000],  loss: 12.955402, mse: 3701.471779, mean_q: 40.892808, mean_eps: 0.795100
  62211/300000: episode: 655, duration: 0.630s, episode steps:  84, steps per second: 133, episode reward: -88.243, mean reward: -1.051 [-100.000,  6.337], mean action: 1.655 [0.000, 3.000],  loss: 11.298354, mse: 3815.450431, mean_q: 42.136060, mean_eps: 0.794844
  62359/300000: episode: 656, duration: 1.138s, episode steps: 148, steps per second: 130, episode reward: -138.825, mean reward: -0.938 [-100.000,  7.430], mean action: 1.561 [0.000, 3.000],  loss: 11.079113, mse: 3862.055557, mean_q: 42.634489, mean_eps: 0.794461
  62458/300000: episode: 657, duration: 0.697s, episode steps:  99, steps per second: 142, episode reward: -138.690, mean reward: -1.401 [-100.000,  9.605], mean action: 1.545 [0.000, 3.000],  loss: 11.051683, mse: 3759.162280, mean_q: 41.015089, mean_eps: 0.794054
  62541/300000: episode: 658, duration: 0.613s, episode steps:  83, steps per second: 135, episode reward: -137.556, mean reward: -1.657 [-100.000,  8.287], mean action: 1.602 [0.000, 3.000],  loss: 11.454082, mse: 3716.710340, mean_q: 42.511061, mean_eps: 0.793753
  62637/300000: episode: 659, duration: 0.765s, episode steps:  96, steps per second: 125, episode reward: -109.483, mean reward: -1.140 [-100.000,  8.190], mean action: 1.604 [0.000, 3.000],  loss: 15.884001, mse: 3800.607104, mean_q: 43.324404, mean_eps: 0.793458
  62754/300000: episode: 660, duration: 0.850s, episode steps: 117, steps per second: 138, episode reward: -93.193, mean reward: -0.797 [-100.000,  6.459], mean action: 1.624 [0.000, 3.000],  loss: 12.452084, mse: 3776.111679, mean_q: 42.290833, mean_eps: 0.793106
  62814/300000: episode: 661, duration: 0.442s, episode steps:  60, steps per second: 136, episode reward: -83.425, mean reward: -1.390 [-100.000,  6.704], mean action: 1.317 [0.000, 3.000],  loss: 16.228539, mse: 3828.494629, mean_q: 43.417962, mean_eps: 0.792814
  62921/300000: episode: 662, duration: 0.826s, episode steps: 107, steps per second: 129, episode reward: -61.090, mean reward: -0.571 [-100.000, 23.479], mean action: 1.430 [0.000, 3.000],  loss: 14.585910, mse: 3749.860933, mean_q: 42.391732, mean_eps: 0.792539
  63023/300000: episode: 663, duration: 0.742s, episode steps: 102, steps per second: 137, episode reward: -111.224, mean reward: -1.090 [-100.000,  8.521], mean action: 1.647 [0.000, 3.000],  loss: 11.633160, mse: 3900.782823, mean_q: 42.550241, mean_eps: 0.792194
  63101/300000: episode: 664, duration: 0.573s, episode steps:  78, steps per second: 136, episode reward: -91.634, mean reward: -1.175 [-100.000,  5.194], mean action: 1.795 [0.000, 3.000],  loss: 17.918528, mse: 3978.885576, mean_q: 43.557561, mean_eps: 0.791897
  63178/300000: episode: 665, duration: 0.606s, episode steps:  77, steps per second: 127, episode reward: -108.403, mean reward: -1.408 [-100.000,  5.610], mean action: 1.506 [0.000, 3.000],  loss: 13.504580, mse: 3943.436920, mean_q: 42.516294, mean_eps: 0.791641
  63259/300000: episode: 666, duration: 0.605s, episode steps:  81, steps per second: 134, episode reward: -108.655, mean reward: -1.341 [-100.000, 13.958], mean action: 1.568 [0.000, 3.000],  loss: 6.013568, mse: 3898.301899, mean_q: 43.411249, mean_eps: 0.791381
  63387/300000: episode: 667, duration: 0.934s, episode steps: 128, steps per second: 137, episode reward: -45.616, mean reward: -0.356 [-100.000, 11.601], mean action: 1.438 [0.000, 3.000],  loss: 15.148752, mse: 3895.762999, mean_q: 42.760622, mean_eps: 0.791036
  63489/300000: episode: 668, duration: 0.770s, episode steps: 102, steps per second: 132, episode reward: -75.849, mean reward: -0.744 [-100.000,  6.312], mean action: 1.569 [0.000, 3.000],  loss: 15.262453, mse: 3955.036908, mean_q: 43.247443, mean_eps: 0.790656
  63568/300000: episode: 669, duration: 0.605s, episode steps:  79, steps per second: 131, episode reward: -101.651, mean reward: -1.287 [-100.000,  6.974], mean action: 1.519 [0.000, 3.000],  loss: 11.709541, mse: 3904.314441, mean_q: 43.270691, mean_eps: 0.790358
  63686/300000: episode: 670, duration: 0.850s, episode steps: 118, steps per second: 139, episode reward: -43.625, mean reward: -0.370 [-100.000, 12.888], mean action: 1.483 [0.000, 3.000],  loss: 11.121453, mse: 3963.250310, mean_q: 43.520348, mean_eps: 0.790033
  63782/300000: episode: 671, duration: 0.741s, episode steps:  96, steps per second: 130, episode reward: -116.281, mean reward: -1.211 [-100.000, 13.471], mean action: 1.469 [0.000, 3.000],  loss: 11.164396, mse: 3902.927757, mean_q: 43.302152, mean_eps: 0.789679
  63868/300000: episode: 672, duration: 0.676s, episode steps:  86, steps per second: 127, episode reward: -106.435, mean reward: -1.238 [-100.000, 12.844], mean action: 1.570 [0.000, 3.000],  loss: 12.898710, mse: 3918.971552, mean_q: 43.342825, mean_eps: 0.789379
  63972/300000: episode: 673, duration: 0.960s, episode steps: 104, steps per second: 108, episode reward: -3.090, mean reward: -0.030 [-100.000, 55.203], mean action: 1.365 [0.000, 3.000],  loss: 10.410562, mse: 3931.749674, mean_q: 42.798017, mean_eps: 0.789066
  64032/300000: episode: 674, duration: 0.505s, episode steps:  60, steps per second: 119, episode reward: -72.700, mean reward: -1.212 [-100.000,  9.335], mean action: 1.467 [0.000, 3.000],  loss: 13.142342, mse: 3862.847677, mean_q: 43.037684, mean_eps: 0.788795
  64124/300000: episode: 675, duration: 0.735s, episode steps:  92, steps per second: 125, episode reward: -148.270, mean reward: -1.612 [-100.000,  5.805], mean action: 1.696 [0.000, 3.000],  loss: 14.669126, mse: 4005.291135, mean_q: 43.506963, mean_eps: 0.788544
  64235/300000: episode: 676, duration: 0.905s, episode steps: 111, steps per second: 123, episode reward: -42.999, mean reward: -0.387 [-100.000, 11.976], mean action: 1.495 [0.000, 3.000],  loss: 15.634883, mse: 3934.518779, mean_q: 42.819578, mean_eps: 0.788209
  64342/300000: episode: 677, duration: 1.028s, episode steps: 107, steps per second: 104, episode reward: -120.559, mean reward: -1.127 [-100.000, 17.250], mean action: 1.439 [0.000, 3.000],  loss: 14.905140, mse: 4002.819274, mean_q: 43.698351, mean_eps: 0.787850
  64446/300000: episode: 678, duration: 0.826s, episode steps: 104, steps per second: 126, episode reward: -108.659, mean reward: -1.045 [-100.000, 15.777], mean action: 1.606 [0.000, 3.000],  loss: 10.209244, mse: 3930.811077, mean_q: 42.752140, mean_eps: 0.787501
  64548/300000: episode: 679, duration: 0.789s, episode steps: 102, steps per second: 129, episode reward: -85.753, mean reward: -0.841 [-100.000, 11.848], mean action: 1.578 [0.000, 3.000],  loss: 12.046886, mse: 3959.973927, mean_q: 42.299571, mean_eps: 0.787162
  64651/300000: episode: 680, duration: 0.771s, episode steps: 103, steps per second: 134, episode reward: -195.892, mean reward: -1.902 [-100.000,  0.939], mean action: 1.583 [0.000, 3.000],  loss: 10.586024, mse: 3998.644247, mean_q: 44.241407, mean_eps: 0.786823
  64730/300000: episode: 681, duration: 0.576s, episode steps:  79, steps per second: 137, episode reward: -59.657, mean reward: -0.755 [-100.000, 15.915], mean action: 1.595 [0.000, 3.000],  loss: 12.010132, mse: 3929.635507, mean_q: 43.081206, mean_eps: 0.786523
  64870/300000: episode: 682, duration: 1.521s, episode steps: 140, steps per second:  92, episode reward: -68.608, mean reward: -0.490 [-100.000, 20.331], mean action: 1.536 [0.000, 3.000],  loss: 9.969834, mse: 3986.732227, mean_q: 43.383698, mean_eps: 0.786162
  64963/300000: episode: 683, duration: 0.880s, episode steps:  93, steps per second: 106, episode reward: -112.863, mean reward: -1.214 [-100.000, 10.453], mean action: 1.882 [0.000, 3.000],  loss: 10.859863, mse: 3941.183536, mean_q: 43.360835, mean_eps: 0.785777
  65068/300000: episode: 684, duration: 1.116s, episode steps: 105, steps per second:  94, episode reward: -94.999, mean reward: -0.905 [-100.000,  6.640], mean action: 1.467 [0.000, 3.000],  loss: 10.834119, mse: 4085.673082, mean_q: 44.285828, mean_eps: 0.785450
  65154/300000: episode: 685, duration: 0.746s, episode steps:  86, steps per second: 115, episode reward: -68.601, mean reward: -0.798 [-100.000, 24.841], mean action: 1.744 [0.000, 3.000],  loss: 12.937848, mse: 4070.909041, mean_q: 45.155065, mean_eps: 0.785135
  65267/300000: episode: 686, duration: 1.253s, episode steps: 113, steps per second:  90, episode reward: -146.616, mean reward: -1.297 [-100.000,  3.317], mean action: 1.708 [0.000, 3.000],  loss: 11.224923, mse: 4105.468454, mean_q: 45.315192, mean_eps: 0.784807
  65380/300000: episode: 687, duration: 0.921s, episode steps: 113, steps per second: 123, episode reward: -129.673, mean reward: -1.148 [-100.000,  5.570], mean action: 1.673 [0.000, 3.000],  loss: 6.507715, mse: 4081.078758, mean_q: 43.759759, mean_eps: 0.784434
  65457/300000: episode: 688, duration: 0.572s, episode steps:  77, steps per second: 135, episode reward: -44.909, mean reward: -0.583 [-100.000, 18.202], mean action: 1.740 [0.000, 3.000],  loss: 12.503328, mse: 4088.916758, mean_q: 43.268257, mean_eps: 0.784121
  65532/300000: episode: 689, duration: 0.552s, episode steps:  75, steps per second: 136, episode reward: -43.567, mean reward: -0.581 [-100.000, 10.774], mean action: 1.800 [0.000, 3.000],  loss: 9.966371, mse: 4056.003353, mean_q: 43.389927, mean_eps: 0.783870
  65639/300000: episode: 690, duration: 0.730s, episode steps: 107, steps per second: 147, episode reward: -26.759, mean reward: -0.250 [-100.000, 15.106], mean action: 1.804 [0.000, 3.000],  loss: 7.809296, mse: 3981.286772, mean_q: 42.980356, mean_eps: 0.783569
  65707/300000: episode: 691, duration: 0.493s, episode steps:  68, steps per second: 138, episode reward: -55.658, mean reward: -0.818 [-100.000, 16.648], mean action: 1.647 [0.000, 3.000],  loss: 11.981218, mse: 4080.018939, mean_q: 44.310099, mean_eps: 0.783281
  65834/300000: episode: 692, duration: 1.116s, episode steps: 127, steps per second: 114, episode reward: -82.756, mean reward: -0.652 [-100.000, 16.990], mean action: 1.661 [0.000, 3.000],  loss: 8.736454, mse: 4044.613072, mean_q: 44.501860, mean_eps: 0.782959
  65904/300000: episode: 693, duration: 0.611s, episode steps:  70, steps per second: 115, episode reward: -49.422, mean reward: -0.706 [-100.000, 13.018], mean action: 1.457 [0.000, 3.000],  loss: 12.500589, mse: 4071.277291, mean_q: 43.916131, mean_eps: 0.782634
  65992/300000: episode: 694, duration: 0.703s, episode steps:  88, steps per second: 125, episode reward: -103.719, mean reward: -1.179 [-100.000,  9.206], mean action: 1.557 [0.000, 3.000],  loss: 17.291482, mse: 4105.207059, mean_q: 45.024300, mean_eps: 0.782373
  66084/300000: episode: 695, duration: 0.755s, episode steps:  92, steps per second: 122, episode reward: -124.010, mean reward: -1.348 [-100.000, 16.647], mean action: 1.902 [0.000, 3.000],  loss: 13.658417, mse: 4132.408827, mean_q: 46.148848, mean_eps: 0.782076
  66195/300000: episode: 696, duration: 0.851s, episode steps: 111, steps per second: 130, episode reward: -173.140, mean reward: -1.560 [-100.000, 34.739], mean action: 1.631 [0.000, 3.000],  loss: 9.838110, mse: 4084.902784, mean_q: 46.439873, mean_eps: 0.781741
  66290/300000: episode: 697, duration: 0.753s, episode steps:  95, steps per second: 126, episode reward: -232.954, mean reward: -2.452 [-100.000, 24.576], mean action: 1.421 [0.000, 3.000],  loss: 11.746005, mse: 4083.928146, mean_q: 45.352942, mean_eps: 0.781401
  66381/300000: episode: 698, duration: 0.676s, episode steps:  91, steps per second: 135, episode reward: -129.009, mean reward: -1.418 [-100.000, 25.301], mean action: 1.802 [0.000, 3.000],  loss: 14.024453, mse: 4080.940153, mean_q: 45.695284, mean_eps: 0.781095
  66497/300000: episode: 699, duration: 0.821s, episode steps: 116, steps per second: 141, episode reward: -121.258, mean reward: -1.045 [-100.000,  9.607], mean action: 1.638 [0.000, 3.000],  loss: 19.040567, mse: 4127.519005, mean_q: 45.729617, mean_eps: 0.780753
  66614/300000: episode: 700, duration: 0.832s, episode steps: 117, steps per second: 141, episode reward: -58.451, mean reward: -0.500 [-100.000, 16.690], mean action: 1.744 [0.000, 3.000],  loss: 13.186794, mse: 4170.414119, mean_q: 44.648320, mean_eps: 0.780369
  66727/300000: episode: 701, duration: 0.787s, episode steps: 113, steps per second: 144, episode reward: -116.722, mean reward: -1.033 [-100.000, 24.792], mean action: 1.504 [0.000, 3.000],  loss: 11.686642, mse: 4101.703894, mean_q: 45.872558, mean_eps: 0.779989
  66851/300000: episode: 702, duration: 0.939s, episode steps: 124, steps per second: 132, episode reward: -188.059, mean reward: -1.517 [-100.000, 11.367], mean action: 1.556 [0.000, 3.000],  loss: 14.857080, mse: 4138.608367, mean_q: 45.978365, mean_eps: 0.779598
  66979/300000: episode: 703, duration: 0.945s, episode steps: 128, steps per second: 135, episode reward: -231.104, mean reward: -1.805 [-100.000, 27.275], mean action: 1.523 [0.000, 3.000],  loss: 13.444617, mse: 4122.227524, mean_q: 44.956538, mean_eps: 0.779182
  67065/300000: episode: 704, duration: 0.637s, episode steps:  86, steps per second: 135, episode reward: -83.698, mean reward: -0.973 [-100.000,  9.147], mean action: 1.767 [0.000, 3.000],  loss: 14.447478, mse: 4138.146606, mean_q: 45.597717, mean_eps: 0.778829
  67165/300000: episode: 705, duration: 0.786s, episode steps: 100, steps per second: 127, episode reward: -123.070, mean reward: -1.231 [-100.000, 18.477], mean action: 1.510 [0.000, 3.000],  loss: 12.567030, mse: 4078.699722, mean_q: 44.457159, mean_eps: 0.778522
  67257/300000: episode: 706, duration: 0.738s, episode steps:  92, steps per second: 125, episode reward: -117.979, mean reward: -1.282 [-100.000,  5.570], mean action: 1.478 [0.000, 3.000],  loss: 19.439118, mse: 4181.406428, mean_q: 46.313055, mean_eps: 0.778205
  67341/300000: episode: 707, duration: 0.672s, episode steps:  84, steps per second: 125, episode reward: -80.636, mean reward: -0.960 [-100.000, 11.641], mean action: 1.512 [0.000, 3.000],  loss: 14.097622, mse: 4168.097345, mean_q: 46.434966, mean_eps: 0.777915
  67410/300000: episode: 708, duration: 0.539s, episode steps:  69, steps per second: 128, episode reward: -73.343, mean reward: -1.063 [-100.000,  6.214], mean action: 1.319 [0.000, 3.000],  loss: 10.048836, mse: 4263.560129, mean_q: 47.820951, mean_eps: 0.777662
  67531/300000: episode: 709, duration: 1.006s, episode steps: 121, steps per second: 120, episode reward: -110.921, mean reward: -0.917 [-100.000, 23.183], mean action: 1.628 [0.000, 3.000],  loss: 11.207969, mse: 4232.126753, mean_q: 46.813650, mean_eps: 0.777349
  67625/300000: episode: 710, duration: 0.699s, episode steps:  94, steps per second: 134, episode reward: -30.286, mean reward: -0.322 [-100.000, 17.726], mean action: 1.628 [0.000, 3.000],  loss: 16.360645, mse: 4180.028333, mean_q: 45.016471, mean_eps: 0.776994
  67726/300000: episode: 711, duration: 0.773s, episode steps: 101, steps per second: 131, episode reward: -80.754, mean reward: -0.800 [-100.000,  7.266], mean action: 1.584 [0.000, 3.000],  loss: 9.471669, mse: 4202.141689, mean_q: 45.906325, mean_eps: 0.776672
  67818/300000: episode: 712, duration: 0.698s, episode steps:  92, steps per second: 132, episode reward: -63.335, mean reward: -0.688 [-100.000,  6.661], mean action: 1.489 [0.000, 3.000],  loss: 9.530650, mse: 4176.401513, mean_q: 46.296810, mean_eps: 0.776354
  67917/300000: episode: 713, duration: 0.722s, episode steps:  99, steps per second: 137, episode reward: -227.134, mean reward: -2.294 [-100.000, 23.100], mean action: 1.636 [0.000, 3.000],  loss: 16.282229, mse: 4068.004890, mean_q: 44.777912, mean_eps: 0.776039
  68020/300000: episode: 714, duration: 0.778s, episode steps: 103, steps per second: 132, episode reward: -151.184, mean reward: -1.468 [-100.000,  5.118], mean action: 1.612 [0.000, 3.000],  loss: 12.991846, mse: 4188.042651, mean_q: 45.601912, mean_eps: 0.775706
  68129/300000: episode: 715, duration: 0.849s, episode steps: 109, steps per second: 128, episode reward: -9.713, mean reward: -0.089 [-100.000, 82.213], mean action: 1.578 [0.000, 3.000],  loss: 9.998339, mse: 4202.830752, mean_q: 44.826047, mean_eps: 0.775356
  68207/300000: episode: 716, duration: 0.586s, episode steps:  78, steps per second: 133, episode reward: -95.992, mean reward: -1.231 [-100.000, 17.345], mean action: 1.372 [0.000, 3.000],  loss: 10.513290, mse: 4226.923750, mean_q: 46.014800, mean_eps: 0.775047
  68308/300000: episode: 717, duration: 0.764s, episode steps: 101, steps per second: 132, episode reward: -147.180, mean reward: -1.457 [-100.000,  8.036], mean action: 1.752 [0.000, 3.000],  loss: 8.429212, mse: 4130.559418, mean_q: 44.557780, mean_eps: 0.774752
  68395/300000: episode: 718, duration: 0.669s, episode steps:  87, steps per second: 130, episode reward: -76.769, mean reward: -0.882 [-100.000, 12.999], mean action: 1.540 [0.000, 3.000],  loss: 11.945908, mse: 4200.702443, mean_q: 45.514415, mean_eps: 0.774442
  68474/300000: episode: 719, duration: 0.635s, episode steps:  79, steps per second: 124, episode reward: -95.051, mean reward: -1.203 [-100.000, 11.353], mean action: 1.418 [0.000, 3.000],  loss: 13.870798, mse: 4209.002077, mean_q: 44.531474, mean_eps: 0.774168
  68558/300000: episode: 720, duration: 0.700s, episode steps:  84, steps per second: 120, episode reward: -17.198, mean reward: -0.205 [-100.000, 31.066], mean action: 1.429 [0.000, 3.000],  loss: 15.811265, mse: 4153.176354, mean_q: 46.017169, mean_eps: 0.773899
  68635/300000: episode: 721, duration: 0.701s, episode steps:  77, steps per second: 110, episode reward: -74.278, mean reward: -0.965 [-100.000,  8.355], mean action: 1.727 [0.000, 3.000],  loss: 9.661827, mse: 4261.673035, mean_q: 46.344008, mean_eps: 0.773633
  68768/300000: episode: 722, duration: 0.980s, episode steps: 133, steps per second: 136, episode reward: -127.118, mean reward: -0.956 [-100.000, 10.507], mean action: 1.519 [0.000, 3.000],  loss: 7.859536, mse: 4223.212970, mean_q: 45.844486, mean_eps: 0.773287
  68867/300000: episode: 723, duration: 0.733s, episode steps:  99, steps per second: 135, episode reward: -121.973, mean reward: -1.232 [-100.000, 14.975], mean action: 1.727 [0.000, 3.000],  loss: 10.287889, mse: 4221.936274, mean_q: 45.985382, mean_eps: 0.772904
  68939/300000: episode: 724, duration: 0.511s, episode steps:  72, steps per second: 141, episode reward: -64.805, mean reward: -0.900 [-100.000, 13.106], mean action: 1.653 [0.000, 3.000],  loss: 11.285275, mse: 4259.680705, mean_q: 46.979039, mean_eps: 0.772622
  69050/300000: episode: 725, duration: 0.777s, episode steps: 111, steps per second: 143, episode reward: -73.561, mean reward: -0.663 [-100.000,  9.309], mean action: 1.486 [0.000, 3.000],  loss: 9.541760, mse: 4213.722872, mean_q: 46.911888, mean_eps: 0.772320
  69155/300000: episode: 726, duration: 0.743s, episode steps: 105, steps per second: 141, episode reward: -141.366, mean reward: -1.346 [-100.000,  7.942], mean action: 1.629 [0.000, 3.000],  loss: 14.596513, mse: 4292.001363, mean_q: 45.402512, mean_eps: 0.771963
  69224/300000: episode: 727, duration: 0.516s, episode steps:  69, steps per second: 134, episode reward: -74.414, mean reward: -1.078 [-100.000, 16.253], mean action: 1.551 [0.000, 3.000],  loss: 12.637882, mse: 4155.670269, mean_q: 45.425507, mean_eps: 0.771676
  69306/300000: episode: 728, duration: 0.598s, episode steps:  82, steps per second: 137, episode reward: -85.059, mean reward: -1.037 [-100.000, 10.278], mean action: 1.744 [0.000, 3.000],  loss: 9.149587, mse: 4268.925510, mean_q: 46.059763, mean_eps: 0.771427
  69398/300000: episode: 729, duration: 0.651s, episode steps:  92, steps per second: 141, episode reward: -52.223, mean reward: -0.568 [-100.000, 10.694], mean action: 1.739 [0.000, 3.000],  loss: 9.239123, mse: 4294.450479, mean_q: 47.606300, mean_eps: 0.771140
  69508/300000: episode: 730, duration: 0.876s, episode steps: 110, steps per second: 126, episode reward: -75.858, mean reward: -0.690 [-100.000,  7.735], mean action: 1.755 [0.000, 3.000],  loss: 11.583868, mse: 4277.399090, mean_q: 45.566183, mean_eps: 0.770807
  69592/300000: episode: 731, duration: 0.621s, episode steps:  84, steps per second: 135, episode reward: -109.132, mean reward: -1.299 [-100.000,  7.547], mean action: 1.643 [0.000, 3.000],  loss: 12.127600, mse: 4240.467957, mean_q: 45.270384, mean_eps: 0.770487
  69709/300000: episode: 732, duration: 0.867s, episode steps: 117, steps per second: 135, episode reward: -60.553, mean reward: -0.518 [-100.000, 11.567], mean action: 1.709 [0.000, 3.000],  loss: 12.744229, mse: 4260.621307, mean_q: 46.223565, mean_eps: 0.770155
  69778/300000: episode: 733, duration: 0.553s, episode steps:  69, steps per second: 125, episode reward: -90.691, mean reward: -1.314 [-100.000,  7.563], mean action: 1.536 [0.000, 3.000],  loss: 12.009554, mse: 4323.911288, mean_q: 45.676690, mean_eps: 0.769848
  69887/300000: episode: 734, duration: 0.813s, episode steps: 109, steps per second: 134, episode reward: -173.537, mean reward: -1.592 [-100.000,  2.931], mean action: 1.550 [0.000, 3.000],  loss: 14.382871, mse: 4227.529214, mean_q: 45.689510, mean_eps: 0.769554
  69973/300000: episode: 735, duration: 0.623s, episode steps:  86, steps per second: 138, episode reward: -172.239, mean reward: -2.003 [-100.000,  4.715], mean action: 1.895 [0.000, 3.000],  loss: 7.363354, mse: 4274.733620, mean_q: 46.593953, mean_eps: 0.769233
  70045/300000: episode: 736, duration: 0.538s, episode steps:  72, steps per second: 134, episode reward: -26.414, mean reward: -0.367 [-100.000, 12.087], mean action: 1.569 [0.000, 3.000],  loss: 7.080767, mse: 4235.422814, mean_q: 47.555584, mean_eps: 0.768972
  70155/300000: episode: 737, duration: 0.802s, episode steps: 110, steps per second: 137, episode reward: -84.972, mean reward: -0.772 [-100.000, 20.894], mean action: 1.618 [0.000, 3.000],  loss: 12.719940, mse: 4323.573540, mean_q: 47.261835, mean_eps: 0.768672
  70281/300000: episode: 738, duration: 0.936s, episode steps: 126, steps per second: 135, episode reward: -109.405, mean reward: -0.868 [-100.000, 32.407], mean action: 1.452 [0.000, 3.000],  loss: 11.152770, mse: 4251.652536, mean_q: 46.475294, mean_eps: 0.768282
  70375/300000: episode: 739, duration: 0.766s, episode steps:  94, steps per second: 123, episode reward: -112.052, mean reward: -1.192 [-100.000,  7.366], mean action: 1.574 [0.000, 3.000],  loss: 9.794086, mse: 4307.466870, mean_q: 46.725436, mean_eps: 0.767919
  70478/300000: episode: 740, duration: 0.798s, episode steps: 103, steps per second: 129, episode reward:  9.072, mean reward:  0.088 [-100.000, 16.836], mean action: 1.602 [0.000, 3.000],  loss: 13.597979, mse: 4210.424670, mean_q: 46.176771, mean_eps: 0.767594
  70541/300000: episode: 741, duration: 0.473s, episode steps:  63, steps per second: 133, episode reward: -72.998, mean reward: -1.159 [-100.000,  8.124], mean action: 1.587 [0.000, 3.000],  loss: 7.680551, mse: 4342.078764, mean_q: 46.779330, mean_eps: 0.767320
  70644/300000: episode: 742, duration: 0.803s, episode steps: 103, steps per second: 128, episode reward: -79.360, mean reward: -0.770 [-100.000, 10.556], mean action: 1.699 [0.000, 3.000],  loss: 8.775421, mse: 4290.549933, mean_q: 47.634095, mean_eps: 0.767046
  70716/300000: episode: 743, duration: 0.524s, episode steps:  72, steps per second: 138, episode reward: -65.617, mean reward: -0.911 [-100.000, 11.506], mean action: 1.472 [0.000, 3.000],  loss: 9.003677, mse: 4358.298584, mean_q: 48.169006, mean_eps: 0.766758
  70805/300000: episode: 744, duration: 0.687s, episode steps:  89, steps per second: 130, episode reward: -19.192, mean reward: -0.216 [-100.000, 14.004], mean action: 1.506 [0.000, 3.000],  loss: 10.273692, mse: 4384.121774, mean_q: 46.688694, mean_eps: 0.766492
  70897/300000: episode: 745, duration: 0.732s, episode steps:  92, steps per second: 126, episode reward: -169.777, mean reward: -1.845 [-100.000, 11.703], mean action: 1.728 [0.000, 3.000],  loss: 15.174709, mse: 4275.282097, mean_q: 46.750790, mean_eps: 0.766193
  70996/300000: episode: 746, duration: 0.865s, episode steps:  99, steps per second: 114, episode reward: -114.138, mean reward: -1.153 [-100.000,  6.885], mean action: 1.455 [0.000, 3.000],  loss: 13.441118, mse: 4294.984069, mean_q: 46.562040, mean_eps: 0.765878
  71077/300000: episode: 747, duration: 0.595s, episode steps:  81, steps per second: 136, episode reward: -46.572, mean reward: -0.575 [-100.000, 16.908], mean action: 1.556 [0.000, 3.000],  loss: 10.938980, mse: 4316.447003, mean_q: 46.686578, mean_eps: 0.765581
  71156/300000: episode: 748, duration: 0.589s, episode steps:  79, steps per second: 134, episode reward: -99.605, mean reward: -1.261 [-100.000, 11.172], mean action: 1.367 [0.000, 3.000],  loss: 12.774951, mse: 4361.381719, mean_q: 47.740868, mean_eps: 0.765317
  71259/300000: episode: 749, duration: 0.726s, episode steps: 103, steps per second: 142, episode reward: -128.140, mean reward: -1.244 [-100.000,  8.305], mean action: 1.534 [0.000, 3.000],  loss: 16.523691, mse: 4276.109517, mean_q: 46.934337, mean_eps: 0.765017
  71336/300000: episode: 750, duration: 0.539s, episode steps:  77, steps per second: 143, episode reward: -90.412, mean reward: -1.174 [-100.000, 14.079], mean action: 1.610 [0.000, 3.000],  loss: 11.857333, mse: 4418.351068, mean_q: 47.306750, mean_eps: 0.764720
  71410/300000: episode: 751, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -84.468, mean reward: -1.141 [-100.000, 40.113], mean action: 1.527 [0.000, 3.000],  loss: 9.211945, mse: 4364.415026, mean_q: 47.553826, mean_eps: 0.764471
  71509/300000: episode: 752, duration: 0.733s, episode steps:  99, steps per second: 135, episode reward: -114.745, mean reward: -1.159 [-100.000,  6.032], mean action: 1.626 [0.000, 3.000],  loss: 9.573239, mse: 4361.166913, mean_q: 46.696455, mean_eps: 0.764185
  71601/300000: episode: 753, duration: 0.644s, episode steps:  92, steps per second: 143, episode reward: -113.948, mean reward: -1.239 [-100.000,  7.482], mean action: 1.500 [0.000, 3.000],  loss: 12.588960, mse: 4327.066613, mean_q: 47.806660, mean_eps: 0.763870
  71708/300000: episode: 754, duration: 0.785s, episode steps: 107, steps per second: 136, episode reward: -60.272, mean reward: -0.563 [-100.000, 11.316], mean action: 1.589 [0.000, 3.000],  loss: 6.972643, mse: 4347.407904, mean_q: 47.170709, mean_eps: 0.763542
  71794/300000: episode: 755, duration: 0.748s, episode steps:  86, steps per second: 115, episode reward: -108.105, mean reward: -1.257 [-100.000,  9.816], mean action: 1.453 [0.000, 3.000],  loss: 9.070818, mse: 4332.000000, mean_q: 46.250553, mean_eps: 0.763223
  71923/300000: episode: 756, duration: 1.120s, episode steps: 129, steps per second: 115, episode reward: -97.439, mean reward: -0.755 [-100.000, 16.178], mean action: 1.512 [0.000, 3.000],  loss: 8.666305, mse: 4364.437814, mean_q: 47.813299, mean_eps: 0.762869
  72049/300000: episode: 757, duration: 1.161s, episode steps: 126, steps per second: 108, episode reward: -104.750, mean reward: -0.831 [-100.000, 18.955], mean action: 1.643 [0.000, 3.000],  loss: 10.013190, mse: 4330.214865, mean_q: 47.553249, mean_eps: 0.762448
  72192/300000: episode: 758, duration: 1.099s, episode steps: 143, steps per second: 130, episode reward: -13.992, mean reward: -0.098 [-100.000, 20.368], mean action: 1.657 [0.000, 3.000],  loss: 13.810965, mse: 4353.951703, mean_q: 47.212082, mean_eps: 0.762004
  72265/300000: episode: 759, duration: 0.577s, episode steps:  73, steps per second: 126, episode reward: -92.107, mean reward: -1.262 [-100.000, 12.938], mean action: 1.466 [0.000, 3.000],  loss: 12.221223, mse: 4413.991646, mean_q: 47.730352, mean_eps: 0.761648
  72336/300000: episode: 760, duration: 0.551s, episode steps:  71, steps per second: 129, episode reward: -41.732, mean reward: -0.588 [-100.000, 10.327], mean action: 1.761 [0.000, 3.000],  loss: 10.299384, mse: 4281.142011, mean_q: 47.639077, mean_eps: 0.761410
  72434/300000: episode: 761, duration: 0.763s, episode steps:  98, steps per second: 128, episode reward: -300.628, mean reward: -3.068 [-100.000,  7.625], mean action: 1.388 [0.000, 3.000],  loss: 12.348519, mse: 4359.186591, mean_q: 46.500030, mean_eps: 0.761131
  72550/300000: episode: 762, duration: 1.055s, episode steps: 116, steps per second: 110, episode reward: -73.006, mean reward: -0.629 [-100.000,  7.361], mean action: 1.664 [0.000, 3.000],  loss: 9.487952, mse: 4349.742082, mean_q: 46.881638, mean_eps: 0.760778
  72660/300000: episode: 763, duration: 1.017s, episode steps: 110, steps per second: 108, episode reward: -91.379, mean reward: -0.831 [-100.000, 11.510], mean action: 1.636 [0.000, 3.000],  loss: 14.297267, mse: 4365.572232, mean_q: 47.905555, mean_eps: 0.760405
  72771/300000: episode: 764, duration: 0.925s, episode steps: 111, steps per second: 120, episode reward: -125.198, mean reward: -1.128 [-100.000, 12.237], mean action: 1.649 [0.000, 3.000],  loss: 11.385749, mse: 4385.590908, mean_q: 46.397899, mean_eps: 0.760041
  72866/300000: episode: 765, duration: 0.732s, episode steps:  95, steps per second: 130, episode reward: -99.830, mean reward: -1.051 [-100.000,  6.159], mean action: 1.642 [0.000, 3.000],  loss: 11.413286, mse: 4394.895402, mean_q: 47.280559, mean_eps: 0.759701
  72959/300000: episode: 766, duration: 0.685s, episode steps:  93, steps per second: 136, episode reward: -100.642, mean reward: -1.082 [-100.000,  6.802], mean action: 1.645 [0.000, 3.000],  loss: 12.495675, mse: 4423.997104, mean_q: 47.319996, mean_eps: 0.759390
  73080/300000: episode: 767, duration: 0.865s, episode steps: 121, steps per second: 140, episode reward: -140.938, mean reward: -1.165 [-100.000,  7.117], mean action: 1.554 [0.000, 3.000],  loss: 9.809647, mse: 4319.127512, mean_q: 45.889479, mean_eps: 0.759037
  73171/300000: episode: 768, duration: 0.673s, episode steps:  91, steps per second: 135, episode reward: -50.862, mean reward: -0.559 [-100.000, 16.038], mean action: 1.648 [0.000, 3.000],  loss: 9.199741, mse: 4365.356247, mean_q: 47.109370, mean_eps: 0.758688
  73267/300000: episode: 769, duration: 0.816s, episode steps:  96, steps per second: 118, episode reward: -98.616, mean reward: -1.027 [-100.000, 15.383], mean action: 1.312 [0.000, 3.000],  loss: 8.810027, mse: 4380.739787, mean_q: 46.793465, mean_eps: 0.758379
  73372/300000: episode: 770, duration: 0.812s, episode steps: 105, steps per second: 129, episode reward: -110.257, mean reward: -1.050 [-100.000, 12.330], mean action: 1.514 [0.000, 3.000],  loss: 9.196906, mse: 4345.475577, mean_q: 46.714013, mean_eps: 0.758047
  73476/300000: episode: 771, duration: 0.746s, episode steps: 104, steps per second: 139, episode reward: -48.489, mean reward: -0.466 [-100.000, 15.816], mean action: 1.548 [0.000, 3.000],  loss: 8.430263, mse: 4370.452355, mean_q: 47.469588, mean_eps: 0.757702
  73601/300000: episode: 772, duration: 0.906s, episode steps: 125, steps per second: 138, episode reward: -203.224, mean reward: -1.626 [-100.000,  7.795], mean action: 1.680 [0.000, 3.000],  loss: 9.168891, mse: 4422.074547, mean_q: 47.128362, mean_eps: 0.757325
  73726/300000: episode: 773, duration: 0.948s, episode steps: 125, steps per second: 132, episode reward: -186.367, mean reward: -1.491 [-100.000, 18.909], mean action: 1.672 [0.000, 3.000],  loss: 10.186389, mse: 4395.924260, mean_q: 47.985773, mean_eps: 0.756912
  73822/300000: episode: 774, duration: 0.683s, episode steps:  96, steps per second: 141, episode reward: -88.467, mean reward: -0.922 [-100.000,  8.575], mean action: 1.688 [0.000, 3.000],  loss: 17.492519, mse: 4491.549377, mean_q: 47.946116, mean_eps: 0.756547
  73914/300000: episode: 775, duration: 0.648s, episode steps:  92, steps per second: 142, episode reward: -182.940, mean reward: -1.988 [-100.000,  6.832], mean action: 1.728 [0.000, 3.000],  loss: 12.918872, mse: 4310.509943, mean_q: 45.966417, mean_eps: 0.756237
  73991/300000: episode: 776, duration: 0.584s, episode steps:  77, steps per second: 132, episode reward: -80.196, mean reward: -1.042 [-100.000,  8.511], mean action: 1.623 [0.000, 3.000],  loss: 10.741435, mse: 4295.985694, mean_q: 47.096315, mean_eps: 0.755958
  74074/300000: episode: 777, duration: 0.574s, episode steps:  83, steps per second: 145, episode reward: -105.269, mean reward: -1.268 [-100.000, 19.787], mean action: 1.651 [0.000, 3.000],  loss: 9.760305, mse: 4363.775549, mean_q: 47.212965, mean_eps: 0.755694
  74157/300000: episode: 778, duration: 0.569s, episode steps:  83, steps per second: 146, episode reward: -50.845, mean reward: -0.613 [-100.000, 17.724], mean action: 1.723 [0.000, 3.000],  loss: 12.641978, mse: 4386.608557, mean_q: 47.002811, mean_eps: 0.755421
  74236/300000: episode: 779, duration: 0.577s, episode steps:  79, steps per second: 137, episode reward: -87.790, mean reward: -1.111 [-100.000,  7.162], mean action: 1.506 [0.000, 3.000],  loss: 8.139394, mse: 4387.853364, mean_q: 46.296895, mean_eps: 0.755153
  74374/300000: episode: 780, duration: 1.151s, episode steps: 138, steps per second: 120, episode reward: -136.312, mean reward: -0.988 [-100.000, 11.073], mean action: 1.543 [0.000, 3.000],  loss: 12.378678, mse: 4460.408725, mean_q: 47.041997, mean_eps: 0.754795
  74502/300000: episode: 781, duration: 1.045s, episode steps: 128, steps per second: 122, episode reward: -327.779, mean reward: -2.561 [-100.000,  4.936], mean action: 1.508 [0.000, 3.000],  loss: 8.794543, mse: 4361.628986, mean_q: 45.597314, mean_eps: 0.754356
  74570/300000: episode: 782, duration: 0.547s, episode steps:  68, steps per second: 124, episode reward: -60.316, mean reward: -0.887 [-100.000,  9.284], mean action: 1.647 [0.000, 3.000],  loss: 7.696408, mse: 4378.394582, mean_q: 47.005802, mean_eps: 0.754033
  74654/300000: episode: 783, duration: 0.659s, episode steps:  84, steps per second: 127, episode reward: -109.461, mean reward: -1.303 [-100.000,  8.062], mean action: 1.655 [0.000, 3.000],  loss: 10.738525, mse: 4382.446368, mean_q: 47.681587, mean_eps: 0.753782
  74804/300000: episode: 784, duration: 1.235s, episode steps: 150, steps per second: 121, episode reward: -35.037, mean reward: -0.234 [-100.000, 45.574], mean action: 1.440 [0.000, 3.000],  loss: 8.452259, mse: 4407.036147, mean_q: 45.793807, mean_eps: 0.753396
  74919/300000: episode: 785, duration: 0.926s, episode steps: 115, steps per second: 124, episode reward: -63.296, mean reward: -0.550 [-100.000, 15.709], mean action: 1.757 [0.000, 3.000],  loss: 9.716551, mse: 4374.633897, mean_q: 45.672655, mean_eps: 0.752959
  75032/300000: episode: 786, duration: 0.792s, episode steps: 113, steps per second: 143, episode reward: -27.393, mean reward: -0.242 [-100.000, 23.191], mean action: 1.814 [0.000, 3.000],  loss: 15.117088, mse: 4390.966021, mean_q: 46.484469, mean_eps: 0.752583
  75130/300000: episode: 787, duration: 0.710s, episode steps:  98, steps per second: 138, episode reward: -82.170, mean reward: -0.838 [-100.000,  9.031], mean action: 1.612 [0.000, 3.000],  loss: 13.684084, mse: 4450.906589, mean_q: 47.557266, mean_eps: 0.752234
  75197/300000: episode: 788, duration: 0.465s, episode steps:  67, steps per second: 144, episode reward: -98.692, mean reward: -1.473 [-100.000,  6.832], mean action: 1.328 [0.000, 3.000],  loss: 11.883154, mse: 4399.875292, mean_q: 45.899522, mean_eps: 0.751962
  75309/300000: episode: 789, duration: 0.829s, episode steps: 112, steps per second: 135, episode reward: -110.753, mean reward: -0.989 [-100.000,  6.431], mean action: 1.545 [0.000, 3.000],  loss: 12.128997, mse: 4494.898030, mean_q: 47.396782, mean_eps: 0.751667
  75412/300000: episode: 790, duration: 0.763s, episode steps: 103, steps per second: 135, episode reward: -242.493, mean reward: -2.354 [-100.000,  1.528], mean action: 1.447 [0.000, 3.000],  loss: 8.451081, mse: 4366.869719, mean_q: 45.873239, mean_eps: 0.751312
  75517/300000: episode: 791, duration: 0.742s, episode steps: 105, steps per second: 141, episode reward: -181.990, mean reward: -1.733 [-100.000,  5.835], mean action: 1.648 [0.000, 3.000],  loss: 14.601013, mse: 4522.391192, mean_q: 47.299887, mean_eps: 0.750969
  75635/300000: episode: 792, duration: 0.842s, episode steps: 118, steps per second: 140, episode reward: -83.995, mean reward: -0.712 [-100.000, 12.226], mean action: 1.542 [0.000, 3.000],  loss: 16.517022, mse: 4430.504483, mean_q: 47.391517, mean_eps: 0.750601
  75712/300000: episode: 793, duration: 0.581s, episode steps:  77, steps per second: 133, episode reward: -100.714, mean reward: -1.308 [-100.000,  6.117], mean action: 1.545 [0.000, 3.000],  loss: 13.070808, mse: 4408.362213, mean_q: 47.660500, mean_eps: 0.750279
  75828/300000: episode: 794, duration: 0.879s, episode steps: 116, steps per second: 132, episode reward: -47.317, mean reward: -0.408 [-100.000,  7.958], mean action: 1.603 [0.000, 3.000],  loss: 8.165527, mse: 4394.484674, mean_q: 47.244768, mean_eps: 0.749961
  75967/300000: episode: 795, duration: 1.059s, episode steps: 139, steps per second: 131, episode reward: -29.194, mean reward: -0.210 [-100.000, 47.762], mean action: 1.748 [0.000, 3.000],  loss: 11.454060, mse: 4443.292711, mean_q: 47.023235, mean_eps: 0.749540
  76070/300000: episode: 796, duration: 0.751s, episode steps: 103, steps per second: 137, episode reward: -133.832, mean reward: -1.299 [-100.000,  5.990], mean action: 1.563 [0.000, 3.000],  loss: 12.842361, mse: 4428.519425, mean_q: 46.191072, mean_eps: 0.749141
  76136/300000: episode: 797, duration: 0.459s, episode steps:  66, steps per second: 144, episode reward: -94.984, mean reward: -1.439 [-100.000,  5.715], mean action: 1.500 [0.000, 3.000],  loss: 8.743502, mse: 4522.379598, mean_q: 48.031533, mean_eps: 0.748862
  76222/300000: episode: 798, duration: 0.603s, episode steps:  86, steps per second: 143, episode reward: -77.759, mean reward: -0.904 [-100.000, 11.637], mean action: 1.593 [0.000, 3.000],  loss: 9.322050, mse: 4473.903025, mean_q: 47.701810, mean_eps: 0.748611
  76326/300000: episode: 799, duration: 0.749s, episode steps: 104, steps per second: 139, episode reward: -78.389, mean reward: -0.754 [-100.000, 20.979], mean action: 1.510 [0.000, 3.000],  loss: 8.186770, mse: 4469.353887, mean_q: 47.759569, mean_eps: 0.748297
  76393/300000: episode: 800, duration: 0.471s, episode steps:  67, steps per second: 142, episode reward: -64.746, mean reward: -0.966 [-100.000, 10.407], mean action: 1.343 [0.000, 3.000],  loss: 7.306423, mse: 4410.719555, mean_q: 47.790862, mean_eps: 0.748015
  76486/300000: episode: 801, duration: 0.639s, episode steps:  93, steps per second: 146, episode reward: -63.081, mean reward: -0.678 [-100.000, 15.851], mean action: 1.720 [0.000, 3.000],  loss: 10.051076, mse: 4438.757752, mean_q: 47.924216, mean_eps: 0.747751
  76620/300000: episode: 802, duration: 0.970s, episode steps: 134, steps per second: 138, episode reward: -103.053, mean reward: -0.769 [-100.000, 16.005], mean action: 1.634 [0.000, 3.000],  loss: 9.277901, mse: 4440.065767, mean_q: 46.917782, mean_eps: 0.747377
  76741/300000: episode: 803, duration: 0.879s, episode steps: 121, steps per second: 138, episode reward: -142.391, mean reward: -1.177 [-100.000, 13.980], mean action: 1.636 [0.000, 3.000],  loss: 9.063543, mse: 4441.122321, mean_q: 47.401902, mean_eps: 0.746956
  76834/300000: episode: 804, duration: 0.710s, episode steps:  93, steps per second: 131, episode reward: -97.078, mean reward: -1.044 [-100.000, 22.554], mean action: 1.677 [0.000, 3.000],  loss: 9.251773, mse: 4414.333236, mean_q: 46.518355, mean_eps: 0.746603
  76926/300000: episode: 805, duration: 0.722s, episode steps:  92, steps per second: 127, episode reward: -60.343, mean reward: -0.656 [-100.000, 15.650], mean action: 1.674 [0.000, 3.000],  loss: 12.260628, mse: 4427.610919, mean_q: 47.200222, mean_eps: 0.746298
  77029/300000: episode: 806, duration: 0.744s, episode steps: 103, steps per second: 138, episode reward: -81.503, mean reward: -0.791 [-100.000, 16.242], mean action: 1.670 [0.000, 3.000],  loss: 10.028568, mse: 4384.962061, mean_q: 46.910348, mean_eps: 0.745976
  77139/300000: episode: 807, duration: 0.845s, episode steps: 110, steps per second: 130, episode reward: -172.906, mean reward: -1.572 [-100.000,  5.032], mean action: 1.609 [0.000, 3.000],  loss: 6.527317, mse: 4483.784229, mean_q: 46.916937, mean_eps: 0.745624
  77238/300000: episode: 808, duration: 0.741s, episode steps:  99, steps per second: 134, episode reward: -118.368, mean reward: -1.196 [-100.000,  5.774], mean action: 1.535 [0.000, 3.000],  loss: 12.921044, mse: 4454.605437, mean_q: 46.376115, mean_eps: 0.745280
  77317/300000: episode: 809, duration: 0.568s, episode steps:  79, steps per second: 139, episode reward: -32.754, mean reward: -0.415 [-100.000, 14.875], mean action: 1.658 [0.000, 3.000],  loss: 8.857003, mse: 4418.593086, mean_q: 45.973003, mean_eps: 0.744986
  77442/300000: episode: 810, duration: 0.918s, episode steps: 125, steps per second: 136, episode reward: -137.818, mean reward: -1.103 [-100.000,  7.378], mean action: 1.592 [0.000, 3.000],  loss: 9.727699, mse: 4463.065689, mean_q: 45.999975, mean_eps: 0.744649
  77512/300000: episode: 811, duration: 0.505s, episode steps:  70, steps per second: 139, episode reward: -89.259, mean reward: -1.275 [-100.000,  5.129], mean action: 1.386 [0.000, 3.000],  loss: 6.074968, mse: 4447.149763, mean_q: 46.703245, mean_eps: 0.744328
  77597/300000: episode: 812, duration: 0.597s, episode steps:  85, steps per second: 142, episode reward: -85.323, mean reward: -1.004 [-100.000, 22.897], mean action: 1.576 [0.000, 3.000],  loss: 11.240532, mse: 4415.833263, mean_q: 47.067868, mean_eps: 0.744072
  77713/300000: episode: 813, duration: 0.851s, episode steps: 116, steps per second: 136, episode reward: -55.877, mean reward: -0.482 [-100.000, 12.410], mean action: 1.509 [0.000, 3.000],  loss: 7.662878, mse: 4501.078093, mean_q: 48.040404, mean_eps: 0.743740
  77798/300000: episode: 814, duration: 0.654s, episode steps:  85, steps per second: 130, episode reward: -71.581, mean reward: -0.842 [-100.000,  5.840], mean action: 1.859 [0.000, 3.000],  loss: 8.532375, mse: 4463.275121, mean_q: 46.308741, mean_eps: 0.743409
  77876/300000: episode: 815, duration: 0.597s, episode steps:  78, steps per second: 131, episode reward: -62.964, mean reward: -0.807 [-100.000, 12.547], mean action: 1.615 [0.000, 3.000],  loss: 7.935157, mse: 4480.808863, mean_q: 46.405110, mean_eps: 0.743140
  77989/300000: episode: 816, duration: 0.811s, episode steps: 113, steps per second: 139, episode reward: -99.660, mean reward: -0.882 [-100.000,  5.305], mean action: 1.708 [0.000, 3.000],  loss: 12.325961, mse: 4429.603343, mean_q: 46.129563, mean_eps: 0.742824
  78068/300000: episode: 817, duration: 0.603s, episode steps:  79, steps per second: 131, episode reward: -73.467, mean reward: -0.930 [-100.000,  6.599], mean action: 1.595 [0.000, 3.000],  loss: 8.008005, mse: 4598.145780, mean_q: 46.952746, mean_eps: 0.742508
  78176/300000: episode: 818, duration: 0.763s, episode steps: 108, steps per second: 142, episode reward: -108.615, mean reward: -1.006 [-100.000, 11.826], mean action: 1.676 [0.000, 3.000],  loss: 11.849330, mse: 4520.170001, mean_q: 46.789618, mean_eps: 0.742199
  78258/300000: episode: 819, duration: 0.575s, episode steps:  82, steps per second: 142, episode reward: -100.517, mean reward: -1.226 [-100.000, 11.612], mean action: 1.561 [0.000, 3.000],  loss: 9.480040, mse: 4517.069827, mean_q: 47.225502, mean_eps: 0.741886
  78341/300000: episode: 820, duration: 0.626s, episode steps:  83, steps per second: 133, episode reward: -52.016, mean reward: -0.627 [-100.000, 16.882], mean action: 1.687 [0.000, 3.000],  loss: 9.889164, mse: 4499.113975, mean_q: 47.396746, mean_eps: 0.741613
  78433/300000: episode: 821, duration: 0.655s, episode steps:  92, steps per second: 140, episode reward: -84.837, mean reward: -0.922 [-100.000,  6.504], mean action: 1.696 [0.000, 3.000],  loss: 10.590492, mse: 4532.025964, mean_q: 45.907131, mean_eps: 0.741325
  78548/300000: episode: 822, duration: 0.806s, episode steps: 115, steps per second: 143, episode reward: -191.686, mean reward: -1.667 [-100.000, 20.362], mean action: 1.678 [0.000, 3.000],  loss: 11.338577, mse: 4578.501677, mean_q: 47.034992, mean_eps: 0.740983
  78624/300000: episode: 823, duration: 0.555s, episode steps:  76, steps per second: 137, episode reward: -39.854, mean reward: -0.524 [-100.000, 12.155], mean action: 1.592 [0.000, 3.000],  loss: 6.890133, mse: 4646.733710, mean_q: 47.397477, mean_eps: 0.740668
  78730/300000: episode: 824, duration: 0.765s, episode steps: 106, steps per second: 139, episode reward: -85.296, mean reward: -0.805 [-100.000, 11.104], mean action: 1.594 [0.000, 3.000],  loss: 8.009774, mse: 4490.554178, mean_q: 45.593071, mean_eps: 0.740368
  78816/300000: episode: 825, duration: 0.598s, episode steps:  86, steps per second: 144, episode reward: -96.374, mean reward: -1.121 [-100.000, 17.204], mean action: 1.686 [0.000, 3.000],  loss: 9.160825, mse: 4572.141127, mean_q: 47.259092, mean_eps: 0.740051
  78915/300000: episode: 826, duration: 0.736s, episode steps:  99, steps per second: 135, episode reward: -92.690, mean reward: -0.936 [-100.000,  7.103], mean action: 1.636 [0.000, 3.000],  loss: 5.891986, mse: 4519.712289, mean_q: 46.845693, mean_eps: 0.739745
  78994/300000: episode: 827, duration: 0.573s, episode steps:  79, steps per second: 138, episode reward: -100.256, mean reward: -1.269 [-100.000,  9.774], mean action: 1.418 [0.000, 3.000],  loss: 13.893156, mse: 4509.910688, mean_q: 46.968751, mean_eps: 0.739452
  79130/300000: episode: 828, duration: 0.959s, episode steps: 136, steps per second: 142, episode reward: -92.849, mean reward: -0.683 [-100.000, 17.461], mean action: 1.581 [0.000, 3.000],  loss: 7.402291, mse: 4603.361423, mean_q: 47.004345, mean_eps: 0.739097
  79196/300000: episode: 829, duration: 0.511s, episode steps:  66, steps per second: 129, episode reward: -94.393, mean reward: -1.430 [-100.000,  5.994], mean action: 1.439 [0.000, 3.000],  loss: 8.961028, mse: 4612.017441, mean_q: 47.391818, mean_eps: 0.738764
  79307/300000: episode: 830, duration: 1.024s, episode steps: 111, steps per second: 108, episode reward: -102.616, mean reward: -0.924 [-100.000, 15.537], mean action: 1.667 [0.000, 3.000],  loss: 8.323837, mse: 4595.584985, mean_q: 47.851784, mean_eps: 0.738472
  79419/300000: episode: 831, duration: 0.817s, episode steps: 112, steps per second: 137, episode reward: -82.309, mean reward: -0.735 [-100.000,  9.706], mean action: 1.652 [0.000, 3.000],  loss: 7.374034, mse: 4567.806083, mean_q: 46.841399, mean_eps: 0.738104
  79528/300000: episode: 832, duration: 0.849s, episode steps: 109, steps per second: 128, episode reward: -51.714, mean reward: -0.474 [-100.000, 15.071], mean action: 1.514 [0.000, 3.000],  loss: 10.228667, mse: 4558.423111, mean_q: 45.972061, mean_eps: 0.737739
  79630/300000: episode: 833, duration: 0.721s, episode steps: 102, steps per second: 141, episode reward: -59.072, mean reward: -0.579 [-100.000,  8.288], mean action: 1.735 [0.000, 3.000],  loss: 12.209932, mse: 4580.410216, mean_q: 46.899423, mean_eps: 0.737391
  79730/300000: episode: 834, duration: 0.695s, episode steps: 100, steps per second: 144, episode reward: -79.986, mean reward: -0.800 [-100.000, 16.447], mean action: 1.500 [0.000, 3.000],  loss: 7.871325, mse: 4563.090876, mean_q: 47.226868, mean_eps: 0.737058
  79816/300000: episode: 835, duration: 0.664s, episode steps:  86, steps per second: 130, episode reward: -89.288, mean reward: -1.038 [-100.000,  6.689], mean action: 1.593 [0.000, 3.000],  loss: 8.102201, mse: 4591.939050, mean_q: 47.108922, mean_eps: 0.736751
  79946/300000: episode: 836, duration: 0.912s, episode steps: 130, steps per second: 142, episode reward: -102.218, mean reward: -0.786 [-100.000, 17.502], mean action: 1.646 [0.000, 3.000],  loss: 6.315785, mse: 4614.593756, mean_q: 47.904598, mean_eps: 0.736394
  80028/300000: episode: 837, duration: 0.566s, episode steps:  82, steps per second: 145, episode reward: -66.294, mean reward: -0.808 [-100.000,  7.945], mean action: 1.512 [0.000, 3.000],  loss: 9.283190, mse: 4662.928136, mean_q: 48.599828, mean_eps: 0.736045
  80121/300000: episode: 838, duration: 0.780s, episode steps:  93, steps per second: 119, episode reward: -113.426, mean reward: -1.220 [-100.000,  9.986], mean action: 1.774 [0.000, 3.000],  loss: 8.576362, mse: 4640.522621, mean_q: 47.549022, mean_eps: 0.735756
  80204/300000: episode: 839, duration: 0.648s, episode steps:  83, steps per second: 128, episode reward: -84.933, mean reward: -1.023 [-100.000,  6.816], mean action: 1.602 [0.000, 3.000],  loss: 9.507244, mse: 4680.160836, mean_q: 48.863226, mean_eps: 0.735465
  80281/300000: episode: 840, duration: 0.572s, episode steps:  77, steps per second: 135, episode reward: -66.406, mean reward: -0.862 [-100.000, 12.483], mean action: 1.779 [0.000, 3.000],  loss: 8.811421, mse: 4646.821774, mean_q: 47.670484, mean_eps: 0.735201
  80370/300000: episode: 841, duration: 0.710s, episode steps:  89, steps per second: 125, episode reward: -2.860, mean reward: -0.032 [-100.000, 12.026], mean action: 1.685 [0.000, 3.000],  loss: 12.047594, mse: 4601.927438, mean_q: 47.016979, mean_eps: 0.734927
  80451/300000: episode: 842, duration: 0.620s, episode steps:  81, steps per second: 131, episode reward: -105.115, mean reward: -1.298 [-100.000, 11.180], mean action: 1.630 [0.000, 3.000],  loss: 5.611809, mse: 4637.078077, mean_q: 47.364342, mean_eps: 0.734647
  80522/300000: episode: 843, duration: 0.533s, episode steps:  71, steps per second: 133, episode reward: -52.420, mean reward: -0.738 [-100.000, 17.665], mean action: 1.563 [0.000, 3.000],  loss: 5.983425, mse: 4625.425675, mean_q: 46.119200, mean_eps: 0.734396
  80605/300000: episode: 844, duration: 0.620s, episode steps:  83, steps per second: 134, episode reward: -78.887, mean reward: -0.950 [-100.000,  6.787], mean action: 1.602 [0.000, 3.000],  loss: 8.708191, mse: 4704.346612, mean_q: 48.547707, mean_eps: 0.734142
  80679/300000: episode: 845, duration: 0.590s, episode steps:  74, steps per second: 125, episode reward: -61.520, mean reward: -0.831 [-100.000,  7.758], mean action: 1.608 [0.000, 3.000],  loss: 10.986780, mse: 4541.118095, mean_q: 47.021972, mean_eps: 0.733883
  80796/300000: episode: 846, duration: 0.865s, episode steps: 117, steps per second: 135, episode reward: -92.339, mean reward: -0.789 [-100.000, 12.268], mean action: 1.547 [0.000, 3.000],  loss: 8.006365, mse: 4639.514598, mean_q: 48.156799, mean_eps: 0.733568
  80915/300000: episode: 847, duration: 0.896s, episode steps: 119, steps per second: 133, episode reward: -57.733, mean reward: -0.485 [-100.000, 11.068], mean action: 1.647 [0.000, 3.000],  loss: 8.456486, mse: 4638.550894, mean_q: 47.340886, mean_eps: 0.733179
  80984/300000: episode: 848, duration: 0.527s, episode steps:  69, steps per second: 131, episode reward: -52.126, mean reward: -0.755 [-100.000, 10.027], mean action: 1.435 [0.000, 3.000],  loss: 8.509937, mse: 4694.854015, mean_q: 48.076864, mean_eps: 0.732868
  81086/300000: episode: 849, duration: 0.894s, episode steps: 102, steps per second: 114, episode reward: -55.258, mean reward: -0.542 [-100.000,  6.050], mean action: 1.539 [0.000, 3.000],  loss: 9.892257, mse: 4550.908215, mean_q: 47.568223, mean_eps: 0.732586
  81190/300000: episode: 850, duration: 0.842s, episode steps: 104, steps per second: 124, episode reward: -172.389, mean reward: -1.658 [-100.000,  2.851], mean action: 1.490 [0.000, 3.000],  loss: 6.412303, mse: 4569.035795, mean_q: 47.785519, mean_eps: 0.732246
  81295/300000: episode: 851, duration: 0.823s, episode steps: 105, steps per second: 128, episode reward: -83.415, mean reward: -0.794 [-100.000, 10.005], mean action: 1.343 [0.000, 3.000],  loss: 9.088102, mse: 4608.764941, mean_q: 49.209027, mean_eps: 0.731901
  81390/300000: episode: 852, duration: 0.700s, episode steps:  95, steps per second: 136, episode reward: -104.094, mean reward: -1.096 [-100.000,  6.703], mean action: 1.526 [0.000, 3.000],  loss: 8.656832, mse: 4630.216645, mean_q: 47.679742, mean_eps: 0.731571
  81519/300000: episode: 853, duration: 1.018s, episode steps: 129, steps per second: 127, episode reward: -108.015, mean reward: -0.837 [-100.000,  6.195], mean action: 1.705 [0.000, 3.000],  loss: 6.973494, mse: 4651.400913, mean_q: 48.499090, mean_eps: 0.731202
  81607/300000: episode: 854, duration: 0.714s, episode steps:  88, steps per second: 123, episode reward: -101.439, mean reward: -1.153 [-100.000, 11.542], mean action: 1.602 [0.000, 3.000],  loss: 9.066399, mse: 4547.829995, mean_q: 48.043091, mean_eps: 0.730844
  81681/300000: episode: 855, duration: 0.576s, episode steps:  74, steps per second: 128, episode reward: -69.652, mean reward: -0.941 [-100.000, 10.514], mean action: 1.541 [0.000, 3.000],  loss: 6.391597, mse: 4661.712759, mean_q: 49.716253, mean_eps: 0.730576
  81771/300000: episode: 856, duration: 0.694s, episode steps:  90, steps per second: 130, episode reward: -87.384, mean reward: -0.971 [-100.000,  6.584], mean action: 1.789 [0.000, 3.000],  loss: 7.617315, mse: 4570.893031, mean_q: 47.368490, mean_eps: 0.730306
  81892/300000: episode: 857, duration: 0.869s, episode steps: 121, steps per second: 139, episode reward: -50.459, mean reward: -0.417 [-100.000, 18.585], mean action: 1.488 [0.000, 3.000],  loss: 8.468000, mse: 4622.093203, mean_q: 48.842602, mean_eps: 0.729958
  81993/300000: episode: 858, duration: 0.730s, episode steps: 101, steps per second: 138, episode reward: -55.152, mean reward: -0.546 [-100.000, 10.196], mean action: 1.584 [0.000, 3.000],  loss: 6.594812, mse: 4590.982693, mean_q: 48.850769, mean_eps: 0.729591
  82110/300000: episode: 859, duration: 0.925s, episode steps: 117, steps per second: 126, episode reward: -17.998, mean reward: -0.154 [-100.000, 30.681], mean action: 1.607 [0.000, 3.000],  loss: 8.635096, mse: 4681.219088, mean_q: 48.262542, mean_eps: 0.729232
  82208/300000: episode: 860, duration: 0.706s, episode steps:  98, steps per second: 139, episode reward: -91.021, mean reward: -0.929 [-100.000, 11.522], mean action: 1.714 [0.000, 3.000],  loss: 7.147912, mse: 4640.283108, mean_q: 48.683710, mean_eps: 0.728877
  82293/300000: episode: 861, duration: 0.628s, episode steps:  85, steps per second: 135, episode reward: -71.243, mean reward: -0.838 [-100.000,  8.881], mean action: 1.459 [0.000, 3.000],  loss: 6.150656, mse: 4654.735475, mean_q: 49.345525, mean_eps: 0.728575
  82428/300000: episode: 862, duration: 1.085s, episode steps: 135, steps per second: 124, episode reward: -204.963, mean reward: -1.518 [-100.000, 51.203], mean action: 1.644 [0.000, 3.000],  loss: 11.029150, mse: 4656.161225, mean_q: 48.999080, mean_eps: 0.728212
  82545/300000: episode: 863, duration: 0.974s, episode steps: 117, steps per second: 120, episode reward: -92.422, mean reward: -0.790 [-100.000, 10.743], mean action: 1.752 [0.000, 3.000],  loss: 12.434153, mse: 4735.599856, mean_q: 49.791970, mean_eps: 0.727796
  82690/300000: episode: 864, duration: 1.141s, episode steps: 145, steps per second: 127, episode reward: -44.534, mean reward: -0.307 [-100.000, 10.566], mean action: 1.566 [0.000, 3.000],  loss: 10.842052, mse: 4698.103396, mean_q: 49.346039, mean_eps: 0.727364
  82814/300000: episode: 865, duration: 0.911s, episode steps: 124, steps per second: 136, episode reward: -162.810, mean reward: -1.313 [-100.000, 14.348], mean action: 1.605 [0.000, 3.000],  loss: 9.215500, mse: 4719.414480, mean_q: 49.693040, mean_eps: 0.726920
  82928/300000: episode: 866, duration: 0.885s, episode steps: 114, steps per second: 129, episode reward: -42.215, mean reward: -0.370 [-100.000, 10.239], mean action: 1.684 [0.000, 3.000],  loss: 8.364060, mse: 4757.532514, mean_q: 49.577066, mean_eps: 0.726527
  83042/300000: episode: 867, duration: 0.930s, episode steps: 114, steps per second: 123, episode reward: -69.297, mean reward: -0.608 [-100.000,  6.293], mean action: 1.535 [0.000, 3.000],  loss: 9.800665, mse: 4681.022752, mean_q: 48.454684, mean_eps: 0.726151
  83138/300000: episode: 868, duration: 0.845s, episode steps:  96, steps per second: 114, episode reward: -56.292, mean reward: -0.586 [-100.000,  7.848], mean action: 1.677 [0.000, 3.000],  loss: 7.439222, mse: 4782.793325, mean_q: 48.937902, mean_eps: 0.725805
  83295/300000: episode: 869, duration: 1.256s, episode steps: 157, steps per second: 125, episode reward: -202.788, mean reward: -1.292 [-100.000, 64.407], mean action: 1.732 [0.000, 3.000],  loss: 6.226456, mse: 4731.850532, mean_q: 49.554881, mean_eps: 0.725387
  83389/300000: episode: 870, duration: 0.749s, episode steps:  94, steps per second: 125, episode reward: -115.549, mean reward: -1.229 [-100.000,  6.108], mean action: 1.628 [0.000, 3.000],  loss: 7.289766, mse: 4763.506376, mean_q: 49.485641, mean_eps: 0.724973
  83524/300000: episode: 871, duration: 1.093s, episode steps: 135, steps per second: 124, episode reward: -48.679, mean reward: -0.361 [-100.000,  9.430], mean action: 1.704 [0.000, 3.000],  loss: 6.305250, mse: 4734.136033, mean_q: 50.218337, mean_eps: 0.724595
  83611/300000: episode: 872, duration: 0.659s, episode steps:  87, steps per second: 132, episode reward: -107.335, mean reward: -1.234 [-100.000,  7.921], mean action: 1.506 [0.000, 3.000],  loss: 10.815420, mse: 4636.611199, mean_q: 48.681546, mean_eps: 0.724229
  83723/300000: episode: 873, duration: 0.928s, episode steps: 112, steps per second: 121, episode reward: -338.272, mean reward: -3.020 [-100.000, 116.746], mean action: 1.545 [0.000, 3.000],  loss: 9.143416, mse: 4727.813463, mean_q: 49.084656, mean_eps: 0.723901
  83847/300000: episode: 874, duration: 0.974s, episode steps: 124, steps per second: 127, episode reward: -24.997, mean reward: -0.202 [-100.000, 67.427], mean action: 1.444 [0.000, 3.000],  loss: 6.229787, mse: 4652.255286, mean_q: 47.331456, mean_eps: 0.723511
  83950/300000: episode: 875, duration: 0.779s, episode steps: 103, steps per second: 132, episode reward: -133.125, mean reward: -1.292 [-100.000, 16.117], mean action: 1.476 [0.000, 3.000],  loss: 8.474623, mse: 4737.362224, mean_q: 48.962845, mean_eps: 0.723137
  84040/300000: episode: 876, duration: 0.699s, episode steps:  90, steps per second: 129, episode reward: -50.815, mean reward: -0.565 [-100.000, 11.202], mean action: 1.822 [0.000, 3.000],  loss: 11.881449, mse: 4721.620660, mean_q: 48.813809, mean_eps: 0.722818
  84128/300000: episode: 877, duration: 0.681s, episode steps:  88, steps per second: 129, episode reward: -21.054, mean reward: -0.239 [-100.000, 28.279], mean action: 1.523 [0.000, 3.000],  loss: 5.375718, mse: 4782.131983, mean_q: 47.422018, mean_eps: 0.722524
  84195/300000: episode: 878, duration: 0.488s, episode steps:  67, steps per second: 137, episode reward: -88.212, mean reward: -1.317 [-100.000,  7.145], mean action: 1.716 [0.000, 3.000],  loss: 5.842251, mse: 4833.318676, mean_q: 49.694912, mean_eps: 0.722269
  84284/300000: episode: 879, duration: 0.693s, episode steps:  89, steps per second: 128, episode reward: -88.880, mean reward: -0.999 [-100.000,  8.659], mean action: 1.719 [0.000, 3.000],  loss: 6.180491, mse: 4781.440369, mean_q: 49.795702, mean_eps: 0.722011
  84391/300000: episode: 880, duration: 0.804s, episode steps: 107, steps per second: 133, episode reward: -129.138, mean reward: -1.207 [-100.000,  7.988], mean action: 1.346 [0.000, 3.000],  loss: 6.718978, mse: 4794.140536, mean_q: 50.219244, mean_eps: 0.721688
  84477/300000: episode: 881, duration: 0.639s, episode steps:  86, steps per second: 135, episode reward: -119.607, mean reward: -1.391 [-100.000, 11.114], mean action: 1.686 [0.000, 3.000],  loss: 5.046490, mse: 4778.080737, mean_q: 49.632299, mean_eps: 0.721369
  84582/300000: episode: 882, duration: 0.812s, episode steps: 105, steps per second: 129, episode reward: -147.654, mean reward: -1.406 [-100.000, 11.091], mean action: 1.790 [0.000, 3.000],  loss: 7.288798, mse: 4821.741416, mean_q: 50.565180, mean_eps: 0.721054
  84681/300000: episode: 883, duration: 0.717s, episode steps:  99, steps per second: 138, episode reward: -66.621, mean reward: -0.673 [-100.000, 21.849], mean action: 1.414 [0.000, 3.000],  loss: 7.287651, mse: 4768.730331, mean_q: 49.047448, mean_eps: 0.720718
  84782/300000: episode: 884, duration: 0.752s, episode steps: 101, steps per second: 134, episode reward: -110.515, mean reward: -1.094 [-100.000, 25.543], mean action: 1.733 [0.000, 3.000],  loss: 7.188158, mse: 4831.808993, mean_q: 49.117920, mean_eps: 0.720388
  84888/300000: episode: 885, duration: 0.817s, episode steps: 106, steps per second: 130, episode reward: -176.032, mean reward: -1.661 [-100.000,  3.983], mean action: 1.726 [0.000, 3.000],  loss: 7.876424, mse: 4728.224485, mean_q: 49.417813, mean_eps: 0.720046
  84998/300000: episode: 886, duration: 0.788s, episode steps: 110, steps per second: 140, episode reward: -92.791, mean reward: -0.844 [-100.000, 26.093], mean action: 1.655 [0.000, 3.000],  loss: 7.838427, mse: 4845.369494, mean_q: 50.041040, mean_eps: 0.719690
  85102/300000: episode: 887, duration: 0.740s, episode steps: 104, steps per second: 141, episode reward: -99.572, mean reward: -0.957 [-100.000, 26.792], mean action: 1.712 [0.000, 3.000],  loss: 7.966000, mse: 4851.784290, mean_q: 50.295394, mean_eps: 0.719337
  85191/300000: episode: 888, duration: 0.711s, episode steps:  89, steps per second: 125, episode reward: -99.851, mean reward: -1.122 [-100.000,  6.134], mean action: 1.719 [0.000, 3.000],  loss: 7.498621, mse: 4804.126479, mean_q: 50.099790, mean_eps: 0.719018
  85308/300000: episode: 889, duration: 0.841s, episode steps: 117, steps per second: 139, episode reward: -119.923, mean reward: -1.025 [-100.000, 20.262], mean action: 1.496 [0.000, 3.000],  loss: 6.313884, mse: 4815.326218, mean_q: 49.037658, mean_eps: 0.718678
  85415/300000: episode: 890, duration: 0.858s, episode steps: 107, steps per second: 125, episode reward: -48.878, mean reward: -0.457 [-100.000, 14.612], mean action: 1.589 [0.000, 3.000],  loss: 5.574353, mse: 4777.682154, mean_q: 49.261447, mean_eps: 0.718309
  85528/300000: episode: 891, duration: 0.859s, episode steps: 113, steps per second: 132, episode reward: -96.671, mean reward: -0.855 [-100.000, 11.232], mean action: 1.761 [0.000, 3.000],  loss: 7.234120, mse: 4797.563008, mean_q: 49.204399, mean_eps: 0.717946
  85650/300000: episode: 892, duration: 0.896s, episode steps: 122, steps per second: 136, episode reward: -65.016, mean reward: -0.533 [-100.000, 19.213], mean action: 1.516 [0.000, 3.000],  loss: 6.867719, mse: 4868.473227, mean_q: 50.111764, mean_eps: 0.717558
  85784/300000: episode: 893, duration: 1.022s, episode steps: 134, steps per second: 131, episode reward: -28.310, mean reward: -0.211 [-100.000, 10.859], mean action: 1.746 [0.000, 3.000],  loss: 8.803082, mse: 4809.786993, mean_q: 48.784641, mean_eps: 0.717136
  85913/300000: episode: 894, duration: 0.918s, episode steps: 129, steps per second: 141, episode reward: -71.863, mean reward: -0.557 [-100.000, 23.902], mean action: 1.558 [0.000, 3.000],  loss: 7.013173, mse: 4864.643814, mean_q: 50.908803, mean_eps: 0.716702
  85991/300000: episode: 895, duration: 0.629s, episode steps:  78, steps per second: 124, episode reward: -43.883, mean reward: -0.563 [-100.000,  9.131], mean action: 1.769 [0.000, 3.000],  loss: 9.533074, mse: 4794.960947, mean_q: 49.216782, mean_eps: 0.716360
  86097/300000: episode: 896, duration: 0.765s, episode steps: 106, steps per second: 139, episode reward: -54.157, mean reward: -0.511 [-100.000,  8.436], mean action: 1.500 [0.000, 3.000],  loss: 8.930756, mse: 4861.902622, mean_q: 49.113817, mean_eps: 0.716056
  86240/300000: episode: 897, duration: 1.125s, episode steps: 143, steps per second: 127, episode reward: -83.029, mean reward: -0.581 [-100.000,  7.907], mean action: 1.573 [0.000, 3.000],  loss: 6.161571, mse: 4875.311061, mean_q: 48.823325, mean_eps: 0.715646
  86316/300000: episode: 898, duration: 0.638s, episode steps:  76, steps per second: 119, episode reward: -165.727, mean reward: -2.181 [-100.000, 32.431], mean action: 1.934 [0.000, 3.000],  loss: 5.723062, mse: 4946.660609, mean_q: 49.018297, mean_eps: 0.715284
  86391/300000: episode: 899, duration: 0.566s, episode steps:  75, steps per second: 132, episode reward: -110.607, mean reward: -1.475 [-100.000,  5.301], mean action: 1.507 [0.000, 3.000],  loss: 10.633860, mse: 4840.476452, mean_q: 50.173052, mean_eps: 0.715035
  86506/300000: episode: 900, duration: 0.859s, episode steps: 115, steps per second: 134, episode reward: -74.755, mean reward: -0.650 [-100.000, 16.617], mean action: 1.443 [0.000, 3.000],  loss: 7.681283, mse: 4842.387328, mean_q: 49.298591, mean_eps: 0.714722
  86619/300000: episode: 901, duration: 0.878s, episode steps: 113, steps per second: 129, episode reward: -75.152, mean reward: -0.665 [-100.000, 15.624], mean action: 1.673 [0.000, 3.000],  loss: 8.348920, mse: 4783.475448, mean_q: 48.316246, mean_eps: 0.714345
  86711/300000: episode: 902, duration: 0.661s, episode steps:  92, steps per second: 139, episode reward: -44.139, mean reward: -0.480 [-100.000, 17.886], mean action: 1.717 [0.000, 3.000],  loss: 7.356943, mse: 4835.101565, mean_q: 48.491469, mean_eps: 0.714007
  86799/300000: episode: 903, duration: 0.653s, episode steps:  88, steps per second: 135, episode reward: -56.457, mean reward: -0.642 [-100.000, 11.421], mean action: 1.705 [0.000, 3.000],  loss: 5.767321, mse: 4947.099160, mean_q: 51.482130, mean_eps: 0.713710
  86904/300000: episode: 904, duration: 0.764s, episode steps: 105, steps per second: 137, episode reward: -160.870, mean reward: -1.532 [-100.000, 43.985], mean action: 1.648 [0.000, 3.000],  loss: 6.442244, mse: 4941.079515, mean_q: 50.374118, mean_eps: 0.713392
  87016/300000: episode: 905, duration: 0.786s, episode steps: 112, steps per second: 142, episode reward: -86.989, mean reward: -0.777 [-100.000,  7.258], mean action: 1.518 [0.000, 3.000],  loss: 7.416271, mse: 4941.083566, mean_q: 50.205758, mean_eps: 0.713034
  87140/300000: episode: 906, duration: 0.888s, episode steps: 124, steps per second: 140, episode reward: -211.118, mean reward: -1.703 [-100.000, 47.860], mean action: 1.774 [0.000, 3.000],  loss: 10.525663, mse: 4806.614177, mean_q: 48.896673, mean_eps: 0.712644
  87232/300000: episode: 907, duration: 0.643s, episode steps:  92, steps per second: 143, episode reward: -79.171, mean reward: -0.861 [-100.000, 13.245], mean action: 1.413 [0.000, 3.000],  loss: 10.557577, mse: 4716.218339, mean_q: 47.552183, mean_eps: 0.712288
  87324/300000: episode: 908, duration: 0.623s, episode steps:  92, steps per second: 148, episode reward: -95.556, mean reward: -1.039 [-100.000, 10.253], mean action: 1.348 [0.000, 3.000],  loss: 6.065247, mse: 4770.523920, mean_q: 48.200148, mean_eps: 0.711984
  87421/300000: episode: 909, duration: 0.685s, episode steps:  97, steps per second: 142, episode reward: -68.043, mean reward: -0.701 [-100.000, 21.674], mean action: 1.526 [0.000, 3.000],  loss: 7.620610, mse: 4837.465828, mean_q: 47.997902, mean_eps: 0.711672
  87498/300000: episode: 910, duration: 0.566s, episode steps:  77, steps per second: 136, episode reward: -64.556, mean reward: -0.838 [-100.000,  9.387], mean action: 1.636 [0.000, 3.000],  loss: 5.773556, mse: 4910.848703, mean_q: 50.525275, mean_eps: 0.711385
  87609/300000: episode: 911, duration: 0.758s, episode steps: 111, steps per second: 147, episode reward: -166.890, mean reward: -1.504 [-100.000,  5.298], mean action: 1.676 [0.000, 3.000],  loss: 8.108107, mse: 4823.794746, mean_q: 48.086056, mean_eps: 0.711075
  87715/300000: episode: 912, duration: 0.750s, episode steps: 106, steps per second: 141, episode reward: -119.312, mean reward: -1.126 [-100.000, 13.931], mean action: 1.547 [0.000, 3.000],  loss: 7.359870, mse: 4702.810823, mean_q: 47.187272, mean_eps: 0.710717
  87786/300000: episode: 913, duration: 0.525s, episode steps:  71, steps per second: 135, episode reward: -115.221, mean reward: -1.623 [-100.000, 11.199], mean action: 1.408 [0.000, 3.000],  loss: 7.818904, mse: 4912.554323, mean_q: 51.318952, mean_eps: 0.710425
  87889/300000: episode: 914, duration: 0.742s, episode steps: 103, steps per second: 139, episode reward: -120.460, mean reward: -1.170 [-100.000,  9.986], mean action: 1.680 [0.000, 3.000],  loss: 8.383764, mse: 4826.504053, mean_q: 48.654924, mean_eps: 0.710138
  87981/300000: episode: 915, duration: 0.640s, episode steps:  92, steps per second: 144, episode reward: -87.613, mean reward: -0.952 [-100.000, 10.077], mean action: 1.576 [0.000, 3.000],  loss: 6.300189, mse: 4781.661189, mean_q: 47.651892, mean_eps: 0.709816
  88075/300000: episode: 916, duration: 0.706s, episode steps:  94, steps per second: 133, episode reward: -109.470, mean reward: -1.165 [-100.000, 12.170], mean action: 1.649 [0.000, 3.000],  loss: 11.368315, mse: 4819.812097, mean_q: 48.578539, mean_eps: 0.709509
  88146/300000: episode: 917, duration: 0.499s, episode steps:  71, steps per second: 142, episode reward: -36.727, mean reward: -0.517 [-100.000, 11.974], mean action: 1.704 [0.000, 3.000],  loss: 7.004608, mse: 4912.193205, mean_q: 51.243514, mean_eps: 0.709237
  88246/300000: episode: 918, duration: 0.687s, episode steps: 100, steps per second: 146, episode reward: -29.965, mean reward: -0.300 [-100.000, 39.214], mean action: 1.680 [0.000, 3.000],  loss: 4.217125, mse: 4837.189624, mean_q: 49.680004, mean_eps: 0.708955
  88324/300000: episode: 919, duration: 0.552s, episode steps:  78, steps per second: 141, episode reward: -92.028, mean reward: -1.180 [-100.000, 11.914], mean action: 1.615 [0.000, 3.000],  loss: 5.123951, mse: 4831.344570, mean_q: 48.341132, mean_eps: 0.708661
  88436/300000: episode: 920, duration: 0.821s, episode steps: 112, steps per second: 136, episode reward: -37.900, mean reward: -0.338 [-100.000,  9.896], mean action: 1.652 [0.000, 3.000],  loss: 7.609400, mse: 4896.167005, mean_q: 48.873682, mean_eps: 0.708348
  88542/300000: episode: 921, duration: 0.774s, episode steps: 106, steps per second: 137, episode reward: -93.191, mean reward: -0.879 [-100.000,  9.165], mean action: 1.651 [0.000, 3.000],  loss: 10.656232, mse: 4882.428006, mean_q: 48.564298, mean_eps: 0.707988
  88652/300000: episode: 922, duration: 0.817s, episode steps: 110, steps per second: 135, episode reward: -84.039, mean reward: -0.764 [-100.000, 12.453], mean action: 1.445 [0.000, 3.000],  loss: 9.812687, mse: 4741.864253, mean_q: 46.169562, mean_eps: 0.707632
  88767/300000: episode: 923, duration: 0.816s, episode steps: 115, steps per second: 141, episode reward: -1.752, mean reward: -0.015 [-100.000, 18.689], mean action: 1.591 [0.000, 3.000],  loss: 5.898704, mse: 4774.799633, mean_q: 47.623985, mean_eps: 0.707260
  88843/300000: episode: 924, duration: 0.517s, episode steps:  76, steps per second: 147, episode reward: -61.123, mean reward: -0.804 [-100.000,  6.197], mean action: 1.618 [0.000, 3.000],  loss: 3.954722, mse: 4806.812728, mean_q: 48.530524, mean_eps: 0.706945
  88949/300000: episode: 925, duration: 0.783s, episode steps: 106, steps per second: 135, episode reward: -92.642, mean reward: -0.874 [-100.000,  8.135], mean action: 1.642 [0.000, 3.000],  loss: 6.472446, mse: 4881.731957, mean_q: 50.329382, mean_eps: 0.706645
  89023/300000: episode: 926, duration: 0.561s, episode steps:  74, steps per second: 132, episode reward: -43.965, mean reward: -0.594 [-100.000, 12.083], mean action: 1.595 [0.000, 3.000],  loss: 6.152992, mse: 4931.915260, mean_q: 49.192521, mean_eps: 0.706348
  89090/300000: episode: 927, duration: 0.488s, episode steps:  67, steps per second: 137, episode reward: -62.365, mean reward: -0.931 [-100.000, 12.064], mean action: 1.567 [0.000, 3.000],  loss: 8.356309, mse: 5030.783932, mean_q: 50.931627, mean_eps: 0.706115
  89161/300000: episode: 928, duration: 0.506s, episode steps:  71, steps per second: 140, episode reward: -48.810, mean reward: -0.687 [-100.000, 30.300], mean action: 1.746 [0.000, 3.000],  loss: 8.166700, mse: 4877.893238, mean_q: 48.388628, mean_eps: 0.705888
  89262/300000: episode: 929, duration: 0.789s, episode steps: 101, steps per second: 128, episode reward: -42.212, mean reward: -0.418 [-100.000,  7.436], mean action: 1.644 [0.000, 3.000],  loss: 8.863321, mse: 4916.297124, mean_q: 48.488421, mean_eps: 0.705604
  89328/300000: episode: 930, duration: 0.495s, episode steps:  66, steps per second: 133, episode reward: -75.618, mean reward: -1.146 [-100.000,  5.751], mean action: 1.636 [0.000, 3.000],  loss: 6.459115, mse: 4887.244189, mean_q: 48.623173, mean_eps: 0.705328
  89399/300000: episode: 931, duration: 0.496s, episode steps:  71, steps per second: 143, episode reward: -72.392, mean reward: -1.020 [-100.000, 11.472], mean action: 1.563 [0.000, 3.000],  loss: 6.929390, mse: 4828.736968, mean_q: 48.263174, mean_eps: 0.705102
  89509/300000: episode: 932, duration: 0.775s, episode steps: 110, steps per second: 142, episode reward: -105.669, mean reward: -0.961 [-100.000,  6.841], mean action: 1.464 [0.000, 3.000],  loss: 7.390838, mse: 5079.731270, mean_q: 50.920743, mean_eps: 0.704803
  89677/300000: episode: 933, duration: 1.214s, episode steps: 168, steps per second: 138, episode reward: -107.950, mean reward: -0.643 [-100.000, 34.799], mean action: 1.583 [0.000, 3.000],  loss: 5.055497, mse: 4912.223223, mean_q: 48.589849, mean_eps: 0.704345
  89786/300000: episode: 934, duration: 0.775s, episode steps: 109, steps per second: 141, episode reward: -122.872, mean reward: -1.127 [-100.000,  9.099], mean action: 1.771 [0.000, 3.000],  loss: 8.098024, mse: 4890.753655, mean_q: 49.042528, mean_eps: 0.703888
  89900/300000: episode: 935, duration: 0.921s, episode steps: 114, steps per second: 124, episode reward: -86.955, mean reward: -0.763 [-100.000, 18.262], mean action: 1.693 [0.000, 3.000],  loss: 4.583258, mse: 4900.808448, mean_q: 49.001801, mean_eps: 0.703520
  89990/300000: episode: 936, duration: 0.733s, episode steps:  90, steps per second: 123, episode reward: -62.481, mean reward: -0.694 [-100.000, 47.010], mean action: 1.522 [0.000, 3.000],  loss: 7.403931, mse: 4863.687722, mean_q: 48.619194, mean_eps: 0.703183
  90096/300000: episode: 937, duration: 0.918s, episode steps: 106, steps per second: 115, episode reward: -108.352, mean reward: -1.022 [-100.000,  7.427], mean action: 1.434 [0.000, 3.000],  loss: 6.611022, mse: 4847.918305, mean_q: 47.857654, mean_eps: 0.702860
  90192/300000: episode: 938, duration: 0.770s, episode steps:  96, steps per second: 125, episode reward: -109.679, mean reward: -1.142 [-100.000,  6.203], mean action: 1.406 [0.000, 3.000],  loss: 5.321694, mse: 4846.249537, mean_q: 46.822746, mean_eps: 0.702526
  90272/300000: episode: 939, duration: 0.628s, episode steps:  80, steps per second: 127, episode reward: -32.985, mean reward: -0.412 [-100.000, 12.634], mean action: 1.562 [0.000, 3.000],  loss: 9.264052, mse: 4885.523990, mean_q: 47.531505, mean_eps: 0.702236
  90375/300000: episode: 940, duration: 0.830s, episode steps: 103, steps per second: 124, episode reward: -152.920, mean reward: -1.485 [-100.000, 12.898], mean action: 1.592 [0.000, 3.000],  loss: 4.572888, mse: 4887.960207, mean_q: 47.608640, mean_eps: 0.701934
  90461/300000: episode: 941, duration: 0.634s, episode steps:  86, steps per second: 136, episode reward: -29.334, mean reward: -0.341 [-100.000, 12.663], mean action: 1.605 [0.000, 3.000],  loss: 7.340464, mse: 4889.555051, mean_q: 46.307450, mean_eps: 0.701622
  90557/300000: episode: 942, duration: 0.678s, episode steps:  96, steps per second: 142, episode reward: -84.221, mean reward: -0.877 [-100.000, 10.928], mean action: 1.552 [0.000, 3.000],  loss: 7.752032, mse: 4824.305300, mean_q: 47.364650, mean_eps: 0.701322
  90659/300000: episode: 943, duration: 0.764s, episode steps: 102, steps per second: 133, episode reward: -170.301, mean reward: -1.670 [-100.000, 15.093], mean action: 1.637 [0.000, 3.000],  loss: 8.733520, mse: 4882.939925, mean_q: 47.200336, mean_eps: 0.700995
  90762/300000: episode: 944, duration: 0.756s, episode steps: 103, steps per second: 136, episode reward: -46.747, mean reward: -0.454 [-100.000, 17.304], mean action: 1.631 [0.000, 3.000],  loss: 6.692453, mse: 4955.085305, mean_q: 47.172619, mean_eps: 0.700657
  90887/300000: episode: 945, duration: 0.881s, episode steps: 125, steps per second: 142, episode reward: -49.984, mean reward: -0.400 [-100.000, 11.031], mean action: 1.736 [0.000, 3.000],  loss: 8.810621, mse: 4894.888963, mean_q: 48.172051, mean_eps: 0.700281
  90984/300000: episode: 946, duration: 0.714s, episode steps:  97, steps per second: 136, episode reward: -94.195, mean reward: -0.971 [-100.000, 11.716], mean action: 1.732 [0.000, 3.000],  loss: 5.100410, mse: 5002.556857, mean_q: 49.488358, mean_eps: 0.699914
  91098/300000: episode: 947, duration: 0.820s, episode steps: 114, steps per second: 139, episode reward: -52.972, mean reward: -0.465 [-100.000,  7.762], mean action: 1.535 [0.000, 3.000],  loss: 8.606957, mse: 5001.852807, mean_q: 48.931225, mean_eps: 0.699566
  91258/300000: episode: 948, duration: 1.169s, episode steps: 160, steps per second: 137, episode reward: -206.846, mean reward: -1.293 [-100.000, 72.428], mean action: 1.700 [0.000, 3.000],  loss: 5.989013, mse: 5004.586018, mean_q: 48.688414, mean_eps: 0.699114
  91324/300000: episode: 949, duration: 0.478s, episode steps:  66, steps per second: 138, episode reward: -33.150, mean reward: -0.502 [-100.000, 15.853], mean action: 1.682 [0.000, 3.000],  loss: 7.349397, mse: 4989.701742, mean_q: 47.996336, mean_eps: 0.698741
  91397/300000: episode: 950, duration: 0.516s, episode steps:  73, steps per second: 142, episode reward: -46.789, mean reward: -0.641 [-100.000,  7.068], mean action: 1.534 [0.000, 3.000],  loss: 4.554255, mse: 5039.681818, mean_q: 49.654729, mean_eps: 0.698512
  91461/300000: episode: 951, duration: 0.483s, episode steps:  64, steps per second: 132, episode reward: -81.172, mean reward: -1.268 [-100.000, 19.571], mean action: 1.469 [0.000, 3.000],  loss: 4.976964, mse: 5018.018505, mean_q: 48.929786, mean_eps: 0.698286
  91541/300000: episode: 952, duration: 0.643s, episode steps:  80, steps per second: 124, episode reward: -34.574, mean reward: -0.432 [-100.000, 19.142], mean action: 1.575 [0.000, 3.000],  loss: 6.676001, mse: 5061.976675, mean_q: 49.752417, mean_eps: 0.698048
  91657/300000: episode: 953, duration: 0.864s, episode steps: 116, steps per second: 134, episode reward: -121.528, mean reward: -1.048 [-100.000, 11.026], mean action: 1.612 [0.000, 3.000],  loss: 5.703958, mse: 4999.387325, mean_q: 47.678625, mean_eps: 0.697725
  91741/300000: episode: 954, duration: 0.877s, episode steps:  84, steps per second:  96, episode reward: -48.649, mean reward: -0.579 [-100.000,  8.988], mean action: 1.631 [0.000, 3.000],  loss: 3.785827, mse: 5001.675773, mean_q: 48.084406, mean_eps: 0.697395
  91877/300000: episode: 955, duration: 1.508s, episode steps: 136, steps per second:  90, episode reward: -98.101, mean reward: -0.721 [-100.000, 26.283], mean action: 1.551 [0.000, 3.000],  loss: 6.882553, mse: 4989.427672, mean_q: 48.650783, mean_eps: 0.697032
  91986/300000: episode: 956, duration: 1.410s, episode steps: 109, steps per second:  77, episode reward: -97.404, mean reward: -0.894 [-100.000, 11.929], mean action: 1.642 [0.000, 3.000],  loss: 6.527206, mse: 4925.420377, mean_q: 47.848504, mean_eps: 0.696628
  92107/300000: episode: 957, duration: 1.389s, episode steps: 121, steps per second:  87, episode reward: -181.957, mean reward: -1.504 [-100.000, 15.649], mean action: 1.479 [0.000, 3.000],  loss: 8.266539, mse: 4968.357571, mean_q: 47.459002, mean_eps: 0.696248
  92193/300000: episode: 958, duration: 1.062s, episode steps:  86, steps per second:  81, episode reward: -96.931, mean reward: -1.127 [-100.000, 14.158], mean action: 1.547 [0.000, 3.000],  loss: 4.947098, mse: 5002.837624, mean_q: 49.459973, mean_eps: 0.695907
  92264/300000: episode: 959, duration: 0.646s, episode steps:  71, steps per second: 110, episode reward: -34.688, mean reward: -0.489 [-100.000,  8.306], mean action: 1.648 [0.000, 3.000],  loss: 7.009428, mse: 5082.917436, mean_q: 49.178590, mean_eps: 0.695648
  92353/300000: episode: 960, duration: 0.687s, episode steps:  89, steps per second: 130, episode reward: -59.005, mean reward: -0.663 [-100.000, 26.556], mean action: 1.494 [0.000, 3.000],  loss: 6.657855, mse: 4997.672887, mean_q: 48.823499, mean_eps: 0.695384
  92456/300000: episode: 961, duration: 0.776s, episode steps: 103, steps per second: 133, episode reward: -51.957, mean reward: -0.504 [-100.000,  9.012], mean action: 1.845 [0.000, 3.000],  loss: 6.090941, mse: 5027.173119, mean_q: 48.170174, mean_eps: 0.695067
  92555/300000: episode: 962, duration: 0.774s, episode steps:  99, steps per second: 128, episode reward: -41.162, mean reward: -0.416 [-100.000, 18.600], mean action: 1.495 [0.000, 3.000],  loss: 9.756280, mse: 5014.194005, mean_q: 48.824121, mean_eps: 0.694734
  92665/300000: episode: 963, duration: 0.792s, episode steps: 110, steps per second: 139, episode reward: -154.012, mean reward: -1.400 [-100.000, 29.833], mean action: 1.591 [0.000, 3.000],  loss: 5.023485, mse: 4984.441488, mean_q: 48.125414, mean_eps: 0.694389
  92755/300000: episode: 964, duration: 0.704s, episode steps:  90, steps per second: 128, episode reward: -47.336, mean reward: -0.526 [-100.000, 12.817], mean action: 1.678 [0.000, 3.000],  loss: 6.922028, mse: 4978.044404, mean_q: 46.620293, mean_eps: 0.694059
  92866/300000: episode: 965, duration: 0.853s, episode steps: 111, steps per second: 130, episode reward: -114.397, mean reward: -1.031 [-100.000,  5.932], mean action: 1.514 [0.000, 3.000],  loss: 8.588064, mse: 5013.115958, mean_q: 48.898020, mean_eps: 0.693727
  92998/300000: episode: 966, duration: 1.004s, episode steps: 132, steps per second: 131, episode reward: -65.124, mean reward: -0.493 [-100.000,  7.734], mean action: 1.712 [0.000, 3.000],  loss: 6.530489, mse: 4949.773844, mean_q: 47.922165, mean_eps: 0.693326
  93129/300000: episode: 967, duration: 0.999s, episode steps: 131, steps per second: 131, episode reward: -105.395, mean reward: -0.805 [-100.000, 12.773], mean action: 1.817 [0.000, 3.000],  loss: 6.681836, mse: 4979.940370, mean_q: 49.128879, mean_eps: 0.692892
  93257/300000: episode: 968, duration: 0.946s, episode steps: 128, steps per second: 135, episode reward: -106.117, mean reward: -0.829 [-100.000,  6.244], mean action: 1.453 [0.000, 3.000],  loss: 8.425847, mse: 4966.739794, mean_q: 48.722141, mean_eps: 0.692465
  93350/300000: episode: 969, duration: 0.720s, episode steps:  93, steps per second: 129, episode reward: -94.756, mean reward: -1.019 [-100.000, 25.059], mean action: 1.656 [0.000, 3.000],  loss: 5.250328, mse: 5015.426112, mean_q: 50.360294, mean_eps: 0.692100
  93464/300000: episode: 970, duration: 0.830s, episode steps: 114, steps per second: 137, episode reward: -68.822, mean reward: -0.604 [-100.000,  8.980], mean action: 1.561 [0.000, 3.000],  loss: 9.071238, mse: 4936.280813, mean_q: 48.804715, mean_eps: 0.691759
  93544/300000: episode: 971, duration: 0.619s, episode steps:  80, steps per second: 129, episode reward: -35.996, mean reward: -0.450 [-100.000, 12.879], mean action: 1.613 [0.000, 3.000],  loss: 5.149165, mse: 4931.118442, mean_q: 47.877223, mean_eps: 0.691438
  93654/300000: episode: 972, duration: 0.794s, episode steps: 110, steps per second: 139, episode reward: -111.186, mean reward: -1.011 [-100.000,  8.001], mean action: 1.673 [0.000, 3.000],  loss: 7.445543, mse: 4915.706836, mean_q: 49.025590, mean_eps: 0.691125
  93763/300000: episode: 973, duration: 0.752s, episode steps: 109, steps per second: 145, episode reward: -126.305, mean reward: -1.159 [-100.000,  7.214], mean action: 1.550 [0.000, 3.000],  loss: 8.045912, mse: 5002.126563, mean_q: 49.668873, mean_eps: 0.690764
  93831/300000: episode: 974, duration: 0.510s, episode steps:  68, steps per second: 133, episode reward: -41.986, mean reward: -0.617 [-100.000, 11.340], mean action: 1.632 [0.000, 3.000],  loss: 5.522817, mse: 4901.862057, mean_q: 48.887554, mean_eps: 0.690472
  93942/300000: episode: 975, duration: 0.816s, episode steps: 111, steps per second: 136, episode reward: -71.352, mean reward: -0.643 [-100.000, 14.120], mean action: 1.486 [0.000, 3.000],  loss: 6.311699, mse: 4915.184821, mean_q: 48.326361, mean_eps: 0.690176
  94024/300000: episode: 976, duration: 0.606s, episode steps:  82, steps per second: 135, episode reward: -19.523, mean reward: -0.238 [-100.000, 16.158], mean action: 1.610 [0.000, 3.000],  loss: 5.523684, mse: 4927.876822, mean_q: 48.156776, mean_eps: 0.689858
  94150/300000: episode: 977, duration: 0.952s, episode steps: 126, steps per second: 132, episode reward: -63.927, mean reward: -0.507 [-100.000,  8.804], mean action: 1.738 [0.000, 3.000],  loss: 12.290920, mse: 4974.392163, mean_q: 48.880989, mean_eps: 0.689515
  94234/300000: episode: 978, duration: 0.603s, episode steps:  84, steps per second: 139, episode reward: -100.960, mean reward: -1.202 [-100.000,  8.223], mean action: 1.548 [0.000, 3.000],  loss: 5.634343, mse: 4934.542582, mean_q: 48.384196, mean_eps: 0.689168
  94327/300000: episode: 979, duration: 0.656s, episode steps:  93, steps per second: 142, episode reward: -66.855, mean reward: -0.719 [-100.000,  9.964], mean action: 1.452 [0.000, 3.000],  loss: 9.632129, mse: 4981.307972, mean_q: 48.364201, mean_eps: 0.688876
  94439/300000: episode: 980, duration: 0.811s, episode steps: 112, steps per second: 138, episode reward: -107.897, mean reward: -0.963 [-100.000, 24.654], mean action: 1.670 [0.000, 3.000],  loss: 9.518950, mse: 4930.566895, mean_q: 49.343356, mean_eps: 0.688538
  94570/300000: episode: 981, duration: 0.918s, episode steps: 131, steps per second: 143, episode reward: -232.000, mean reward: -1.771 [-100.000, 93.776], mean action: 1.710 [0.000, 3.000],  loss: 5.123760, mse: 4922.615115, mean_q: 48.780684, mean_eps: 0.688137
  94646/300000: episode: 982, duration: 0.527s, episode steps:  76, steps per second: 144, episode reward: -87.846, mean reward: -1.156 [-100.000, 11.283], mean action: 1.513 [0.000, 3.000],  loss: 4.495916, mse: 4970.875854, mean_q: 47.814095, mean_eps: 0.687795
  94778/300000: episode: 983, duration: 0.971s, episode steps: 132, steps per second: 136, episode reward: -71.646, mean reward: -0.543 [-100.000,  6.676], mean action: 1.727 [0.000, 3.000],  loss: 9.409202, mse: 4947.003399, mean_q: 48.306777, mean_eps: 0.687452
  94880/300000: episode: 984, duration: 0.718s, episode steps: 102, steps per second: 142, episode reward: -85.479, mean reward: -0.838 [-100.000,  9.133], mean action: 1.539 [0.000, 3.000],  loss: 5.452320, mse: 4946.741034, mean_q: 48.749176, mean_eps: 0.687066
  94976/300000: episode: 985, duration: 0.677s, episode steps:  96, steps per second: 142, episode reward: -54.950, mean reward: -0.572 [-100.000, 25.930], mean action: 1.771 [0.000, 3.000],  loss: 6.822512, mse: 4826.076663, mean_q: 46.739754, mean_eps: 0.686739
  95053/300000: episode: 986, duration: 0.575s, episode steps:  77, steps per second: 134, episode reward: -89.356, mean reward: -1.160 [-100.000, 10.832], mean action: 1.584 [0.000, 3.000],  loss: 9.775110, mse: 4982.493979, mean_q: 48.251348, mean_eps: 0.686454
  95125/300000: episode: 987, duration: 0.518s, episode steps:  72, steps per second: 139, episode reward: -28.893, mean reward: -0.401 [-100.000, 12.209], mean action: 1.569 [0.000, 3.000],  loss: 6.075113, mse: 5004.041267, mean_q: 47.982436, mean_eps: 0.686208
  95248/300000: episode: 988, duration: 0.866s, episode steps: 123, steps per second: 142, episode reward: -78.819, mean reward: -0.641 [-100.000,  6.269], mean action: 1.715 [0.000, 3.000],  loss: 6.656473, mse: 5017.100211, mean_q: 48.288245, mean_eps: 0.685886
  95346/300000: episode: 989, duration: 0.735s, episode steps:  98, steps per second: 133, episode reward: -33.260, mean reward: -0.339 [-100.000,  9.212], mean action: 1.490 [0.000, 3.000],  loss: 4.260515, mse: 4943.015593, mean_q: 48.580662, mean_eps: 0.685522
  95438/300000: episode: 990, duration: 0.649s, episode steps:  92, steps per second: 142, episode reward: -19.265, mean reward: -0.209 [-100.000, 10.870], mean action: 1.685 [0.000, 3.000],  loss: 7.195671, mse: 5009.026288, mean_q: 47.588252, mean_eps: 0.685208
  95575/300000: episode: 991, duration: 0.948s, episode steps: 137, steps per second: 145, episode reward: -168.013, mean reward: -1.226 [-100.000, 47.264], mean action: 1.584 [0.000, 3.000],  loss: 7.640328, mse: 5121.514101, mean_q: 49.835345, mean_eps: 0.684830
  95694/300000: episode: 992, duration: 0.866s, episode steps: 119, steps per second: 137, episode reward: -25.945, mean reward: -0.218 [-100.000, 12.644], mean action: 1.622 [0.000, 3.000],  loss: 9.096629, mse: 4943.640861, mean_q: 47.548487, mean_eps: 0.684408
  95813/300000: episode: 993, duration: 0.855s, episode steps: 119, steps per second: 139, episode reward: -16.408, mean reward: -0.138 [-100.000, 23.155], mean action: 1.723 [0.000, 3.000],  loss: 9.511721, mse: 5117.154740, mean_q: 49.790091, mean_eps: 0.684015
  95931/300000: episode: 994, duration: 0.864s, episode steps: 118, steps per second: 137, episode reward: -52.403, mean reward: -0.444 [-100.000, 19.437], mean action: 1.720 [0.000, 3.000],  loss: 8.502208, mse: 5147.303771, mean_q: 51.174558, mean_eps: 0.683624
  96031/300000: episode: 995, duration: 0.737s, episode steps: 100, steps per second: 136, episode reward: -116.292, mean reward: -1.163 [-100.000,  9.283], mean action: 1.730 [0.000, 3.000],  loss: 6.747519, mse: 5034.728667, mean_q: 49.632313, mean_eps: 0.683264
  96144/300000: episode: 996, duration: 0.801s, episode steps: 113, steps per second: 141, episode reward: -23.875, mean reward: -0.211 [-100.000, 15.260], mean action: 1.522 [0.000, 3.000],  loss: 5.286940, mse: 4925.631810, mean_q: 47.086055, mean_eps: 0.682913
  96265/300000: episode: 997, duration: 0.894s, episode steps: 121, steps per second: 135, episode reward: -8.418, mean reward: -0.070 [-100.000, 15.064], mean action: 1.736 [0.000, 3.000],  loss: 5.837166, mse: 4996.389665, mean_q: 49.400059, mean_eps: 0.682527
  96401/300000: episode: 998, duration: 0.998s, episode steps: 136, steps per second: 136, episode reward: -99.624, mean reward: -0.733 [-100.000,  7.658], mean action: 1.316 [0.000, 3.000],  loss: 7.813341, mse: 4908.958634, mean_q: 48.111366, mean_eps: 0.682103
  96478/300000: episode: 999, duration: 0.565s, episode steps:  77, steps per second: 136, episode reward: -68.067, mean reward: -0.884 [-100.000,  9.631], mean action: 1.805 [0.000, 3.000],  loss: 4.881849, mse: 5017.293134, mean_q: 49.035870, mean_eps: 0.681751
  96562/300000: episode: 1000, duration: 0.675s, episode steps:  84, steps per second: 124, episode reward: -120.473, mean reward: -1.434 [-100.000,  6.088], mean action: 1.643 [0.000, 3.000],  loss: 7.085730, mse: 5018.087646, mean_q: 48.459345, mean_eps: 0.681486
  96694/300000: episode: 1001, duration: 1.025s, episode steps: 132, steps per second: 129, episode reward: -239.065, mean reward: -1.811 [-100.000, 50.823], mean action: 1.492 [0.000, 3.000],  loss: 6.602274, mse: 4976.191724, mean_q: 49.258272, mean_eps: 0.681129
  96777/300000: episode: 1002, duration: 0.631s, episode steps:  83, steps per second: 132, episode reward: -53.345, mean reward: -0.643 [-100.000, 10.982], mean action: 1.446 [0.000, 3.000],  loss: 6.404827, mse: 4962.206364, mean_q: 47.391382, mean_eps: 0.680774
  96874/300000: episode: 1003, duration: 0.777s, episode steps:  97, steps per second: 125, episode reward: -64.316, mean reward: -0.663 [-100.000,  6.464], mean action: 1.557 [0.000, 3.000],  loss: 9.061225, mse: 4960.289206, mean_q: 47.342526, mean_eps: 0.680478
  96988/300000: episode: 1004, duration: 0.838s, episode steps: 114, steps per second: 136, episode reward: -111.328, mean reward: -0.977 [-100.000, 10.064], mean action: 1.640 [0.000, 3.000],  loss: 5.976024, mse: 5030.392829, mean_q: 48.501682, mean_eps: 0.680129
  97051/300000: episode: 1005, duration: 0.472s, episode steps:  63, steps per second: 134, episode reward: -69.138, mean reward: -1.097 [-100.000,  5.051], mean action: 1.619 [0.000, 3.000],  loss: 8.080343, mse: 5078.183807, mean_q: 48.189726, mean_eps: 0.679837
  97199/300000: episode: 1006, duration: 1.091s, episode steps: 148, steps per second: 136, episode reward: -46.175, mean reward: -0.312 [-100.000, 13.883], mean action: 1.689 [0.000, 3.000],  loss: 5.614320, mse: 5018.531441, mean_q: 49.409629, mean_eps: 0.679489
  97327/300000: episode: 1007, duration: 0.900s, episode steps: 128, steps per second: 142, episode reward: -46.490, mean reward: -0.363 [-100.000,  7.802], mean action: 1.594 [0.000, 3.000],  loss: 6.588245, mse: 4984.425354, mean_q: 49.070722, mean_eps: 0.679034
  97428/300000: episode: 1008, duration: 0.736s, episode steps: 101, steps per second: 137, episode reward: -250.925, mean reward: -2.484 [-100.000, 82.356], mean action: 1.663 [0.000, 3.000],  loss: 5.325735, mse: 4922.472751, mean_q: 48.295354, mean_eps: 0.678656
  97551/300000: episode: 1009, duration: 0.875s, episode steps: 123, steps per second: 141, episode reward: -48.862, mean reward: -0.397 [-100.000,  9.057], mean action: 1.626 [0.000, 3.000],  loss: 6.828593, mse: 5049.247156, mean_q: 50.097165, mean_eps: 0.678286
  97659/300000: episode: 1010, duration: 0.798s, episode steps: 108, steps per second: 135, episode reward: -150.951, mean reward: -1.398 [-100.000,  7.844], mean action: 1.454 [0.000, 3.000],  loss: 7.267900, mse: 5011.273243, mean_q: 49.621434, mean_eps: 0.677905
  97739/300000: episode: 1011, duration: 0.626s, episode steps:  80, steps per second: 128, episode reward: -45.263, mean reward: -0.566 [-100.000,  9.894], mean action: 1.675 [0.000, 3.000],  loss: 7.556892, mse: 4976.654831, mean_q: 48.416456, mean_eps: 0.677595
  97847/300000: episode: 1012, duration: 0.777s, episode steps: 108, steps per second: 139, episode reward: -63.772, mean reward: -0.590 [-100.000, 20.173], mean action: 1.704 [0.000, 3.000],  loss: 6.668677, mse: 4987.756323, mean_q: 49.126876, mean_eps: 0.677285
  97936/300000: episode: 1013, duration: 0.655s, episode steps:  89, steps per second: 136, episode reward: -55.137, mean reward: -0.620 [-100.000,  9.661], mean action: 1.685 [0.000, 3.000],  loss: 8.157126, mse: 4878.723520, mean_q: 48.108320, mean_eps: 0.676960
  98094/300000: episode: 1014, duration: 1.181s, episode steps: 158, steps per second: 134, episode reward: -4.401, mean reward: -0.028 [-100.000, 20.594], mean action: 1.652 [0.000, 3.000],  loss: 4.934325, mse: 5072.925837, mean_q: 49.436483, mean_eps: 0.676552
  98195/300000: episode: 1015, duration: 0.716s, episode steps: 101, steps per second: 141, episode reward: -73.941, mean reward: -0.732 [-100.000, 10.088], mean action: 1.663 [0.000, 3.000],  loss: 7.322541, mse: 5131.982439, mean_q: 50.353619, mean_eps: 0.676125
  98325/300000: episode: 1016, duration: 0.990s, episode steps: 130, steps per second: 131, episode reward: -148.989, mean reward: -1.146 [-100.000,  8.487], mean action: 1.777 [0.000, 3.000],  loss: 6.737260, mse: 5116.480290, mean_q: 50.378638, mean_eps: 0.675744
  98400/300000: episode: 1017, duration: 0.572s, episode steps:  75, steps per second: 131, episode reward: -43.271, mean reward: -0.577 [-100.000, 10.371], mean action: 1.653 [0.000, 3.000],  loss: 8.100733, mse: 5130.075931, mean_q: 50.235695, mean_eps: 0.675405
  98518/300000: episode: 1018, duration: 0.955s, episode steps: 118, steps per second: 124, episode reward: -40.922, mean reward: -0.347 [-100.000, 41.490], mean action: 1.585 [0.000, 3.000],  loss: 6.265357, mse: 5125.846225, mean_q: 50.576036, mean_eps: 0.675087
  98604/300000: episode: 1019, duration: 0.738s, episode steps:  86, steps per second: 117, episode reward: -38.828, mean reward: -0.451 [-100.000,  8.685], mean action: 1.767 [0.000, 3.000],  loss: 5.203783, mse: 5173.776112, mean_q: 50.996925, mean_eps: 0.674750
  98731/300000: episode: 1020, duration: 1.000s, episode steps: 127, steps per second: 127, episode reward: -38.327, mean reward: -0.302 [-100.000, 69.902], mean action: 1.756 [0.000, 3.000],  loss: 8.808688, mse: 5019.811112, mean_q: 48.555051, mean_eps: 0.674399
  98838/300000: episode: 1021, duration: 0.859s, episode steps: 107, steps per second: 124, episode reward: -96.747, mean reward: -0.904 [-100.000, 12.832], mean action: 1.626 [0.000, 3.000],  loss: 6.532294, mse: 5083.985199, mean_q: 50.365687, mean_eps: 0.674013
  98925/300000: episode: 1022, duration: 0.678s, episode steps:  87, steps per second: 128, episode reward: 25.853, mean reward:  0.297 [-100.000, 19.540], mean action: 1.644 [0.000, 3.000],  loss: 7.878913, mse: 5081.198186, mean_q: 49.312916, mean_eps: 0.673693
  99028/300000: episode: 1023, duration: 0.732s, episode steps: 103, steps per second: 141, episode reward: -53.866, mean reward: -0.523 [-100.000, 13.904], mean action: 1.709 [0.000, 3.000],  loss: 6.536319, mse: 5114.319853, mean_q: 49.596981, mean_eps: 0.673379
  99096/300000: episode: 1024, duration: 0.496s, episode steps:  68, steps per second: 137, episode reward: -10.091, mean reward: -0.148 [-100.000, 16.447], mean action: 1.735 [0.000, 3.000],  loss: 7.769680, mse: 5095.859903, mean_q: 49.668316, mean_eps: 0.673097
  99210/300000: episode: 1025, duration: 0.970s, episode steps: 114, steps per second: 118, episode reward: -57.307, mean reward: -0.503 [-100.000, 28.653], mean action: 1.711 [0.000, 3.000],  loss: 7.469664, mse: 5128.359707, mean_q: 50.070328, mean_eps: 0.672797
  99300/300000: episode: 1026, duration: 0.792s, episode steps:  90, steps per second: 114, episode reward: -93.274, mean reward: -1.036 [-100.000,  7.493], mean action: 1.633 [0.000, 3.000],  loss: 5.895114, mse: 5081.451302, mean_q: 49.828137, mean_eps: 0.672460
  99425/300000: episode: 1027, duration: 1.064s, episode steps: 125, steps per second: 117, episode reward: -59.766, mean reward: -0.478 [-100.000,  7.583], mean action: 1.712 [0.000, 3.000],  loss: 6.233556, mse: 5180.347668, mean_q: 51.302126, mean_eps: 0.672105
  99546/300000: episode: 1028, duration: 1.197s, episode steps: 121, steps per second: 101, episode reward: -95.767, mean reward: -0.791 [-100.000,  7.555], mean action: 1.702 [0.000, 3.000],  loss: 5.541691, mse: 5156.535289, mean_q: 50.695130, mean_eps: 0.671700
  99679/300000: episode: 1029, duration: 1.166s, episode steps: 133, steps per second: 114, episode reward: -37.613, mean reward: -0.283 [-100.000,  6.950], mean action: 1.632 [0.000, 3.000],  loss: 7.555353, mse: 5131.232209, mean_q: 51.098715, mean_eps: 0.671280
  99776/300000: episode: 1030, duration: 0.724s, episode steps:  97, steps per second: 134, episode reward: -69.403, mean reward: -0.715 [-100.000, 22.414], mean action: 1.629 [0.000, 3.000],  loss: 6.138585, mse: 5152.985744, mean_q: 49.256985, mean_eps: 0.670901
  99902/300000: episode: 1031, duration: 1.104s, episode steps: 126, steps per second: 114, episode reward: -71.709, mean reward: -0.569 [-100.000, 13.153], mean action: 1.532 [0.000, 3.000],  loss: 7.506361, mse: 5166.159100, mean_q: 50.593253, mean_eps: 0.670533
  99999/300000: episode: 1032, duration: 0.701s, episode steps:  97, steps per second: 138, episode reward: -56.243, mean reward: -0.580 [-100.000,  6.037], mean action: 1.588 [0.000, 3.000],  loss: 7.082213, mse: 5144.810325, mean_q: 50.227255, mean_eps: 0.670165
 100076/300000: episode: 1033, duration: 0.571s, episode steps:  77, steps per second: 135, episode reward: -61.374, mean reward: -0.797 [-100.000, 12.769], mean action: 1.857 [0.000, 3.000],  loss: 8.348283, mse: 5131.141332, mean_q: 50.312668, mean_eps: 0.669878
 100227/300000: episode: 1034, duration: 1.181s, episode steps: 151, steps per second: 128, episode reward: -64.912, mean reward: -0.430 [-100.000, 54.736], mean action: 1.550 [0.000, 3.000],  loss: 5.795689, mse: 5085.558709, mean_q: 49.868659, mean_eps: 0.669502
 100382/300000: episode: 1035, duration: 1.516s, episode steps: 155, steps per second: 102, episode reward: -196.976, mean reward: -1.271 [-100.000, 54.123], mean action: 1.645 [0.000, 3.000],  loss: 6.456834, mse: 5129.324014, mean_q: 50.553376, mean_eps: 0.668997
 100509/300000: episode: 1036, duration: 1.119s, episode steps: 127, steps per second: 113, episode reward: -36.901, mean reward: -0.291 [-100.000,  6.304], mean action: 1.646 [0.000, 3.000],  loss: 6.032577, mse: 5088.797492, mean_q: 50.231331, mean_eps: 0.668531
 100586/300000: episode: 1037, duration: 0.647s, episode steps:  77, steps per second: 119, episode reward: -88.732, mean reward: -1.152 [-100.000,  8.640], mean action: 1.636 [0.000, 3.000],  loss: 6.203617, mse: 5040.512616, mean_q: 49.685574, mean_eps: 0.668195
 100693/300000: episode: 1038, duration: 1.201s, episode steps: 107, steps per second:  89, episode reward: -76.122, mean reward: -0.711 [-100.000, 11.698], mean action: 1.645 [0.000, 3.000],  loss: 6.964965, mse: 5048.130328, mean_q: 49.076499, mean_eps: 0.667891
 100796/300000: episode: 1039, duration: 1.168s, episode steps: 103, steps per second:  88, episode reward: -34.918, mean reward: -0.339 [-100.000, 11.418], mean action: 1.650 [0.000, 3.000],  loss: 6.297799, mse: 5093.841778, mean_q: 48.945033, mean_eps: 0.667545
 100911/300000: episode: 1040, duration: 1.580s, episode steps: 115, steps per second:  73, episode reward: -11.247, mean reward: -0.098 [-100.000, 11.197], mean action: 1.730 [0.000, 3.000],  loss: 6.335452, mse: 5099.003342, mean_q: 50.289548, mean_eps: 0.667185
 101036/300000: episode: 1041, duration: 1.347s, episode steps: 125, steps per second:  93, episode reward: -53.780, mean reward: -0.430 [-100.000,  9.432], mean action: 1.640 [0.000, 3.000],  loss: 5.940419, mse: 5105.468115, mean_q: 49.771151, mean_eps: 0.666789
 101103/300000: episode: 1042, duration: 0.671s, episode steps:  67, steps per second: 100, episode reward: -57.080, mean reward: -0.852 [-100.000, 11.028], mean action: 1.687 [0.000, 3.000],  loss: 7.063387, mse: 5121.861590, mean_q: 49.047837, mean_eps: 0.666472
 101244/300000: episode: 1043, duration: 1.587s, episode steps: 141, steps per second:  89, episode reward: -221.649, mean reward: -1.572 [-100.000, 19.124], mean action: 1.801 [0.000, 3.000],  loss: 6.311338, mse: 5098.082928, mean_q: 49.077121, mean_eps: 0.666129
 101355/300000: episode: 1044, duration: 0.819s, episode steps: 111, steps per second: 136, episode reward: -20.154, mean reward: -0.182 [-100.000, 13.331], mean action: 1.667 [0.000, 3.000],  loss: 7.873992, mse: 5207.750396, mean_q: 50.617966, mean_eps: 0.665713
 101458/300000: episode: 1045, duration: 0.774s, episode steps: 103, steps per second: 133, episode reward: -13.931, mean reward: -0.135 [-100.000, 15.895], mean action: 1.699 [0.000, 3.000],  loss: 5.516877, mse: 5179.179337, mean_q: 51.611051, mean_eps: 0.665360
 101568/300000: episode: 1046, duration: 1.140s, episode steps: 110, steps per second:  96, episode reward: -36.055, mean reward: -0.328 [-100.000, 15.249], mean action: 1.436 [0.000, 3.000],  loss: 6.672139, mse: 5145.985489, mean_q: 50.762447, mean_eps: 0.665009
 101668/300000: episode: 1047, duration: 0.995s, episode steps: 100, steps per second: 101, episode reward: -87.762, mean reward: -0.878 [-100.000, 21.717], mean action: 1.420 [0.000, 3.000],  loss: 5.831860, mse: 5186.084370, mean_q: 51.848670, mean_eps: 0.664662
 101756/300000: episode: 1048, duration: 0.763s, episode steps:  88, steps per second: 115, episode reward: -67.398, mean reward: -0.766 [-100.000, 13.211], mean action: 1.625 [0.000, 3.000],  loss: 6.544520, mse: 5154.346524, mean_q: 50.475872, mean_eps: 0.664352
 101876/300000: episode: 1049, duration: 0.888s, episode steps: 120, steps per second: 135, episode reward: -123.873, mean reward: -1.032 [-100.000,  5.430], mean action: 1.650 [0.000, 3.000],  loss: 6.792310, mse: 5216.845494, mean_q: 51.232480, mean_eps: 0.664009
 101972/300000: episode: 1050, duration: 0.864s, episode steps:  96, steps per second: 111, episode reward: -23.553, mean reward: -0.245 [-100.000,  9.999], mean action: 1.719 [0.000, 3.000],  loss: 6.435553, mse: 5051.665337, mean_q: 50.258459, mean_eps: 0.663652
 102090/300000: episode: 1051, duration: 0.997s, episode steps: 118, steps per second: 118, episode reward: -18.338, mean reward: -0.155 [-100.000, 17.532], mean action: 1.720 [0.000, 3.000],  loss: 8.845862, mse: 5131.090493, mean_q: 50.128574, mean_eps: 0.663299
 102228/300000: episode: 1052, duration: 1.052s, episode steps: 138, steps per second: 131, episode reward: -9.156, mean reward: -0.066 [-100.000, 11.549], mean action: 1.746 [0.000, 3.000],  loss: 6.590336, mse: 5116.684818, mean_q: 50.174119, mean_eps: 0.662877
 102335/300000: episode: 1053, duration: 0.765s, episode steps: 107, steps per second: 140, episode reward: -142.746, mean reward: -1.334 [-100.000, 15.598], mean action: 1.458 [0.000, 3.000],  loss: 7.238959, mse: 5074.109959, mean_q: 49.593963, mean_eps: 0.662473
 102466/300000: episode: 1054, duration: 0.992s, episode steps: 131, steps per second: 132, episode reward: -186.026, mean reward: -1.420 [-100.000,  5.512], mean action: 1.718 [0.000, 3.000],  loss: 6.884812, mse: 5221.982606, mean_q: 50.529540, mean_eps: 0.662080
 102686/300000: episode: 1055, duration: 2.460s, episode steps: 220, steps per second:  89, episode reward: -65.310, mean reward: -0.297 [-100.000, 39.148], mean action: 1.559 [0.000, 3.000],  loss: 5.033343, mse: 5050.649967, mean_q: 49.844443, mean_eps: 0.661501
 102792/300000: episode: 1056, duration: 0.759s, episode steps: 106, steps per second: 140, episode reward: -35.892, mean reward: -0.339 [-100.000, 21.264], mean action: 1.679 [0.000, 3.000],  loss: 8.001933, mse: 5121.596394, mean_q: 49.168465, mean_eps: 0.660963
 102885/300000: episode: 1057, duration: 0.638s, episode steps:  93, steps per second: 146, episode reward: -61.347, mean reward: -0.660 [-100.000, 10.850], mean action: 1.323 [0.000, 3.000],  loss: 9.191567, mse: 5088.070494, mean_q: 48.568144, mean_eps: 0.660635
 102969/300000: episode: 1058, duration: 0.592s, episode steps:  84, steps per second: 142, episode reward: -102.067, mean reward: -1.215 [-100.000, 10.525], mean action: 1.774 [0.000, 3.000],  loss: 7.796095, mse: 5160.741717, mean_q: 51.048539, mean_eps: 0.660343
 103075/300000: episode: 1059, duration: 0.749s, episode steps: 106, steps per second: 141, episode reward: -95.404, mean reward: -0.900 [-100.000, 11.111], mean action: 1.745 [0.000, 3.000],  loss: 8.969651, mse: 5137.973635, mean_q: 50.202308, mean_eps: 0.660029
 103254/300000: episode: 1060, duration: 1.241s, episode steps: 179, steps per second: 144, episode reward: -31.959, mean reward: -0.179 [-100.000, 45.120], mean action: 1.609 [0.000, 3.000],  loss: 8.646854, mse: 5214.733898, mean_q: 51.123703, mean_eps: 0.659559
 103379/300000: episode: 1061, duration: 0.915s, episode steps: 125, steps per second: 137, episode reward: -7.055, mean reward: -0.056 [-100.000, 18.234], mean action: 1.640 [0.000, 3.000],  loss: 7.683528, mse: 5200.022344, mean_q: 50.708941, mean_eps: 0.659057
 103465/300000: episode: 1062, duration: 0.585s, episode steps:  86, steps per second: 147, episode reward: -56.872, mean reward: -0.661 [-100.000,  9.400], mean action: 1.558 [0.000, 3.000],  loss: 11.553197, mse: 5251.212959, mean_q: 51.615518, mean_eps: 0.658709
 103551/300000: episode: 1063, duration: 0.599s, episode steps:  86, steps per second: 143, episode reward: -51.373, mean reward: -0.597 [-100.000, 13.672], mean action: 1.616 [0.000, 3.000],  loss: 6.954703, mse: 5224.381183, mean_q: 51.293548, mean_eps: 0.658425
 103636/300000: episode: 1064, duration: 0.628s, episode steps:  85, steps per second: 135, episode reward: -16.258, mean reward: -0.191 [-100.000, 16.306], mean action: 1.647 [0.000, 3.000],  loss: 8.012360, mse: 5165.208640, mean_q: 50.805037, mean_eps: 0.658143
 103756/300000: episode: 1065, duration: 0.861s, episode steps: 120, steps per second: 139, episode reward: -39.012, mean reward: -0.325 [-100.000, 25.098], mean action: 1.733 [0.000, 3.000],  loss: 11.019165, mse: 5170.065468, mean_q: 50.779683, mean_eps: 0.657805
 103839/300000: episode: 1066, duration: 0.589s, episode steps:  83, steps per second: 141, episode reward: -40.155, mean reward: -0.484 [-100.000, 10.580], mean action: 1.735 [0.000, 3.000],  loss: 8.319833, mse: 5264.128730, mean_q: 52.939312, mean_eps: 0.657470
 103973/300000: episode: 1067, duration: 0.979s, episode steps: 134, steps per second: 137, episode reward: -209.363, mean reward: -1.562 [-100.000, 66.421], mean action: 1.515 [0.000, 3.000],  loss: 6.401392, mse: 5204.664859, mean_q: 51.621487, mean_eps: 0.657112
 104086/300000: episode: 1068, duration: 0.786s, episode steps: 113, steps per second: 144, episode reward: -6.135, mean reward: -0.054 [-100.000, 19.790], mean action: 1.673 [0.000, 3.000],  loss: 6.761817, mse: 5272.273772, mean_q: 52.190300, mean_eps: 0.656704
 104207/300000: episode: 1069, duration: 0.856s, episode steps: 121, steps per second: 141, episode reward: -161.987, mean reward: -1.339 [-100.000, 10.449], mean action: 1.711 [0.000, 3.000],  loss: 6.115657, mse: 5280.002583, mean_q: 51.953816, mean_eps: 0.656318
 104305/300000: episode: 1070, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -31.406, mean reward: -0.320 [-100.000,  6.833], mean action: 1.806 [0.000, 3.000],  loss: 6.842537, mse: 5205.150685, mean_q: 50.710026, mean_eps: 0.655957
 104451/300000: episode: 1071, duration: 1.005s, episode steps: 146, steps per second: 145, episode reward:  8.563, mean reward:  0.059 [-100.000, 12.003], mean action: 1.548 [0.000, 3.000],  loss: 5.696257, mse: 5249.343900, mean_q: 51.786114, mean_eps: 0.655554
 104562/300000: episode: 1072, duration: 0.814s, episode steps: 111, steps per second: 136, episode reward: -89.886, mean reward: -0.810 [-100.000,  9.859], mean action: 1.865 [0.000, 3.000],  loss: 6.887910, mse: 5211.175607, mean_q: 50.952320, mean_eps: 0.655130
 104687/300000: episode: 1073, duration: 0.873s, episode steps: 125, steps per second: 143, episode reward: -63.380, mean reward: -0.507 [-100.000, 10.153], mean action: 1.632 [0.000, 3.000],  loss: 6.265394, mse: 5340.945035, mean_q: 53.107938, mean_eps: 0.654741
 104833/300000: episode: 1074, duration: 1.286s, episode steps: 146, steps per second: 113, episode reward: -34.850, mean reward: -0.239 [-100.000, 45.541], mean action: 1.637 [0.000, 3.000],  loss: 8.527679, mse: 5207.338111, mean_q: 51.677932, mean_eps: 0.654294
 104922/300000: episode: 1075, duration: 0.707s, episode steps:  89, steps per second: 126, episode reward: -20.620, mean reward: -0.232 [-100.000, 13.954], mean action: 1.730 [0.000, 3.000],  loss: 4.815379, mse: 5204.008468, mean_q: 51.110937, mean_eps: 0.653906
 105007/300000: episode: 1076, duration: 0.836s, episode steps:  85, steps per second: 102, episode reward: -43.454, mean reward: -0.511 [-100.000,  5.815], mean action: 1.753 [0.000, 3.000],  loss: 4.745342, mse: 5126.729667, mean_q: 50.434959, mean_eps: 0.653619
 105075/300000: episode: 1077, duration: 0.562s, episode steps:  68, steps per second: 121, episode reward: -64.828, mean reward: -0.953 [-100.000, 21.748], mean action: 1.500 [0.000, 3.000],  loss: 5.971452, mse: 5306.912871, mean_q: 52.836130, mean_eps: 0.653366
 105186/300000: episode: 1078, duration: 0.948s, episode steps: 111, steps per second: 117, episode reward: -106.339, mean reward: -0.958 [-100.000,  8.143], mean action: 1.586 [0.000, 3.000],  loss: 5.807065, mse: 5314.311044, mean_q: 51.813504, mean_eps: 0.653071
 105299/300000: episode: 1079, duration: 0.816s, episode steps: 113, steps per second: 138, episode reward: -54.910, mean reward: -0.486 [-100.000,  6.945], mean action: 1.619 [0.000, 3.000],  loss: 7.552565, mse: 5302.614662, mean_q: 51.962813, mean_eps: 0.652701
 105371/300000: episode: 1080, duration: 0.581s, episode steps:  72, steps per second: 124, episode reward: -100.230, mean reward: -1.392 [-100.000,  6.557], mean action: 1.569 [0.000, 3.000],  loss: 4.884840, mse: 5296.938633, mean_q: 51.453706, mean_eps: 0.652396
 105494/300000: episode: 1081, duration: 0.861s, episode steps: 123, steps per second: 143, episode reward: -84.003, mean reward: -0.683 [-100.000, 13.952], mean action: 1.748 [0.000, 3.000],  loss: 8.074225, mse: 5276.678241, mean_q: 51.204442, mean_eps: 0.652074
 105580/300000: episode: 1082, duration: 0.590s, episode steps:  86, steps per second: 146, episode reward: -51.736, mean reward: -0.602 [-100.000,  8.444], mean action: 1.605 [0.000, 3.000],  loss: 7.918933, mse: 5229.572728, mean_q: 50.588490, mean_eps: 0.651730
 105672/300000: episode: 1083, duration: 0.742s, episode steps:  92, steps per second: 124, episode reward: -86.046, mean reward: -0.935 [-100.000,  8.638], mean action: 1.641 [0.000, 3.000],  loss: 6.459570, mse: 5226.397501, mean_q: 51.147121, mean_eps: 0.651436
 105766/300000: episode: 1084, duration: 1.153s, episode steps:  94, steps per second:  82, episode reward: -76.200, mean reward: -0.811 [-100.000,  7.880], mean action: 1.436 [0.000, 3.000],  loss: 6.164016, mse: 5267.047504, mean_q: 50.771233, mean_eps: 0.651129
 105849/300000: episode: 1085, duration: 0.655s, episode steps:  83, steps per second: 127, episode reward: -105.397, mean reward: -1.270 [-100.000, 12.162], mean action: 1.639 [0.000, 3.000],  loss: 7.081903, mse: 5234.618011, mean_q: 51.105661, mean_eps: 0.650837
 105950/300000: episode: 1086, duration: 0.742s, episode steps: 101, steps per second: 136, episode reward: -53.850, mean reward: -0.533 [-100.000,  9.659], mean action: 1.723 [0.000, 3.000],  loss: 6.198357, mse: 5298.724624, mean_q: 52.704649, mean_eps: 0.650533
 106056/300000: episode: 1087, duration: 0.750s, episode steps: 106, steps per second: 141, episode reward: -45.134, mean reward: -0.426 [-100.000,  7.885], mean action: 1.594 [0.000, 3.000],  loss: 6.031147, mse: 5274.330783, mean_q: 51.607016, mean_eps: 0.650192
 106142/300000: episode: 1088, duration: 0.661s, episode steps:  86, steps per second: 130, episode reward: 37.874, mean reward:  0.440 [-100.000, 18.814], mean action: 1.721 [0.000, 3.000],  loss: 5.095103, mse: 5297.589185, mean_q: 53.234769, mean_eps: 0.649875
 106240/300000: episode: 1089, duration: 0.705s, episode steps:  98, steps per second: 139, episode reward: -61.475, mean reward: -0.627 [-100.000, 13.100], mean action: 1.551 [0.000, 3.000],  loss: 5.553100, mse: 5396.477703, mean_q: 53.455451, mean_eps: 0.649571
 106330/300000: episode: 1090, duration: 0.624s, episode steps:  90, steps per second: 144, episode reward: -167.859, mean reward: -1.865 [-100.000, 16.743], mean action: 1.367 [0.000, 3.000],  loss: 6.221155, mse: 5282.787590, mean_q: 52.153768, mean_eps: 0.649261
 106412/300000: episode: 1091, duration: 0.639s, episode steps:  82, steps per second: 128, episode reward: -31.811, mean reward: -0.388 [-100.000, 26.221], mean action: 1.720 [0.000, 3.000],  loss: 7.523216, mse: 5274.124208, mean_q: 50.890398, mean_eps: 0.648977
 106519/300000: episode: 1092, duration: 0.870s, episode steps: 107, steps per second: 123, episode reward: -93.177, mean reward: -0.871 [-100.000,  7.069], mean action: 1.449 [0.000, 3.000],  loss: 8.457116, mse: 5318.816477, mean_q: 52.091608, mean_eps: 0.648666
 106634/300000: episode: 1093, duration: 1.024s, episode steps: 115, steps per second: 112, episode reward: -202.671, mean reward: -1.762 [-100.000, 49.493], mean action: 1.574 [0.000, 3.000],  loss: 5.355497, mse: 5257.455804, mean_q: 51.451464, mean_eps: 0.648299
 106738/300000: episode: 1094, duration: 0.838s, episode steps: 104, steps per second: 124, episode reward: -180.354, mean reward: -1.734 [-100.000, 47.848], mean action: 1.673 [0.000, 3.000],  loss: 10.040490, mse: 5307.396447, mean_q: 52.470850, mean_eps: 0.647938
 106856/300000: episode: 1095, duration: 0.873s, episode steps: 118, steps per second: 135, episode reward: -191.930, mean reward: -1.627 [-100.000, 53.413], mean action: 1.653 [0.000, 3.000],  loss: 7.961962, mse: 5374.656043, mean_q: 53.544409, mean_eps: 0.647572
 106981/300000: episode: 1096, duration: 0.951s, episode steps: 125, steps per second: 131, episode reward: -31.305, mean reward: -0.250 [-100.000, 43.274], mean action: 1.688 [0.000, 3.000],  loss: 6.807586, mse: 5352.462336, mean_q: 53.543572, mean_eps: 0.647171
 107115/300000: episode: 1097, duration: 0.980s, episode steps: 134, steps per second: 137, episode reward: -80.563, mean reward: -0.601 [-100.000, 21.040], mean action: 1.642 [0.000, 3.000],  loss: 6.926093, mse: 5343.034484, mean_q: 52.175547, mean_eps: 0.646743
 107187/300000: episode: 1098, duration: 0.515s, episode steps:  72, steps per second: 140, episode reward: -33.659, mean reward: -0.467 [-100.000, 12.907], mean action: 1.736 [0.000, 3.000],  loss: 6.475937, mse: 5353.958133, mean_q: 52.766177, mean_eps: 0.646403
 107324/300000: episode: 1099, duration: 1.009s, episode steps: 137, steps per second: 136, episode reward: -47.747, mean reward: -0.349 [-100.000, 12.310], mean action: 1.591 [0.000, 3.000],  loss: 7.613162, mse: 5388.567181, mean_q: 53.472478, mean_eps: 0.646058
 107415/300000: episode: 1100, duration: 0.632s, episode steps:  91, steps per second: 144, episode reward: -97.820, mean reward: -1.075 [-100.000,  6.333], mean action: 1.780 [0.000, 3.000],  loss: 7.425118, mse: 5431.123235, mean_q: 53.259006, mean_eps: 0.645682
 107492/300000: episode: 1101, duration: 0.538s, episode steps:  77, steps per second: 143, episode reward: -64.888, mean reward: -0.843 [-100.000, 19.056], mean action: 1.532 [0.000, 3.000],  loss: 7.670738, mse: 5366.152588, mean_q: 52.760260, mean_eps: 0.645405
 107584/300000: episode: 1102, duration: 0.721s, episode steps:  92, steps per second: 128, episode reward: -83.536, mean reward: -0.908 [-100.000, 17.293], mean action: 1.630 [0.000, 3.000],  loss: 6.639460, mse: 5262.704478, mean_q: 51.667712, mean_eps: 0.645126
 107688/300000: episode: 1103, duration: 0.782s, episode steps: 104, steps per second: 133, episode reward: -104.644, mean reward: -1.006 [-100.000, 10.836], mean action: 1.635 [0.000, 3.000],  loss: 5.707411, mse: 5412.315932, mean_q: 54.302351, mean_eps: 0.644803
 107778/300000: episode: 1104, duration: 0.673s, episode steps:  90, steps per second: 134, episode reward: -42.309, mean reward: -0.470 [-100.000, 18.498], mean action: 1.711 [0.000, 3.000],  loss: 6.742273, mse: 5370.820331, mean_q: 51.244686, mean_eps: 0.644483
 107860/300000: episode: 1105, duration: 0.604s, episode steps:  82, steps per second: 136, episode reward: -84.499, mean reward: -1.030 [-100.000, 11.097], mean action: 1.659 [0.000, 3.000],  loss: 11.648776, mse: 5278.771079, mean_q: 52.145588, mean_eps: 0.644199
 107946/300000: episode: 1106, duration: 0.689s, episode steps:  86, steps per second: 125, episode reward: -73.678, mean reward: -0.857 [-100.000, 16.512], mean action: 1.628 [0.000, 3.000],  loss: 11.417501, mse: 5387.417875, mean_q: 52.346067, mean_eps: 0.643922
 108032/300000: episode: 1107, duration: 0.661s, episode steps:  86, steps per second: 130, episode reward: -56.256, mean reward: -0.654 [-100.000, 16.714], mean action: 1.593 [0.000, 3.000],  loss: 11.049263, mse: 5294.421330, mean_q: 52.736739, mean_eps: 0.643638
 108180/300000: episode: 1108, duration: 1.109s, episode steps: 148, steps per second: 133, episode reward: -23.524, mean reward: -0.159 [-100.000, 54.931], mean action: 1.514 [0.000, 3.000],  loss: 8.737004, mse: 5440.990904, mean_q: 53.496570, mean_eps: 0.643252
 108280/300000: episode: 1109, duration: 0.760s, episode steps: 100, steps per second: 132, episode reward: -44.086, mean reward: -0.441 [-100.000, 15.816], mean action: 1.730 [0.000, 3.000],  loss: 6.717005, mse: 5492.498086, mean_q: 52.687583, mean_eps: 0.642843
 108399/300000: episode: 1110, duration: 1.076s, episode steps: 119, steps per second: 111, episode reward: -40.499, mean reward: -0.340 [-100.000, 18.686], mean action: 1.378 [0.000, 3.000],  loss: 9.243848, mse: 5406.242393, mean_q: 52.964818, mean_eps: 0.642481
 108526/300000: episode: 1111, duration: 1.061s, episode steps: 127, steps per second: 120, episode reward: -59.610, mean reward: -0.469 [-100.000, 25.762], mean action: 1.535 [0.000, 3.000],  loss: 5.931685, mse: 5431.601757, mean_q: 52.845016, mean_eps: 0.642075
 108663/300000: episode: 1112, duration: 1.075s, episode steps: 137, steps per second: 127, episode reward: -30.083, mean reward: -0.220 [-100.000, 11.355], mean action: 1.620 [0.000, 3.000],  loss: 7.305227, mse: 5484.656685, mean_q: 53.928810, mean_eps: 0.641640
 108750/300000: episode: 1113, duration: 0.667s, episode steps:  87, steps per second: 130, episode reward: -38.296, mean reward: -0.440 [-100.000, 11.187], mean action: 1.759 [0.000, 3.000],  loss: 6.996939, mse: 5497.391237, mean_q: 53.995123, mean_eps: 0.641270
 108861/300000: episode: 1114, duration: 0.823s, episode steps: 111, steps per second: 135, episode reward: -20.506, mean reward: -0.185 [-100.000, 10.559], mean action: 1.838 [0.000, 3.000],  loss: 9.750684, mse: 5509.009730, mean_q: 54.021734, mean_eps: 0.640943
 108981/300000: episode: 1115, duration: 0.954s, episode steps: 120, steps per second: 126, episode reward: -26.492, mean reward: -0.221 [-100.000, 12.756], mean action: 1.833 [0.000, 3.000],  loss: 6.556922, mse: 5455.838224, mean_q: 53.115619, mean_eps: 0.640562
 109119/300000: episode: 1116, duration: 0.963s, episode steps: 138, steps per second: 143, episode reward: -60.887, mean reward: -0.441 [-100.000,  9.499], mean action: 1.442 [0.000, 3.000],  loss: 6.635050, mse: 5481.050357, mean_q: 52.604772, mean_eps: 0.640137
 109246/300000: episode: 1117, duration: 0.878s, episode steps: 127, steps per second: 145, episode reward: -132.928, mean reward: -1.047 [-100.000, 25.708], mean action: 1.748 [0.000, 3.000],  loss: 6.699919, mse: 5457.843031, mean_q: 54.151899, mean_eps: 0.639699
 109331/300000: episode: 1118, duration: 0.616s, episode steps:  85, steps per second: 138, episode reward: -44.435, mean reward: -0.523 [-100.000,  8.769], mean action: 1.765 [0.000, 3.000],  loss: 7.024703, mse: 5525.465005, mean_q: 53.475390, mean_eps: 0.639350
 109453/300000: episode: 1119, duration: 0.826s, episode steps: 122, steps per second: 148, episode reward: -89.842, mean reward: -0.736 [-100.000, 20.292], mean action: 1.746 [0.000, 3.000],  loss: 8.772285, mse: 5637.440458, mean_q: 54.790464, mean_eps: 0.639008
 109589/300000: episode: 1120, duration: 0.953s, episode steps: 136, steps per second: 143, episode reward: -158.865, mean reward: -1.168 [-100.000, 16.127], mean action: 1.456 [0.000, 3.000],  loss: 7.139184, mse: 5509.622253, mean_q: 52.888012, mean_eps: 0.638582
 109695/300000: episode: 1121, duration: 0.732s, episode steps: 106, steps per second: 145, episode reward: -96.425, mean reward: -0.910 [-100.000,  5.660], mean action: 1.547 [0.000, 3.000],  loss: 10.018285, mse: 5480.570073, mean_q: 53.098777, mean_eps: 0.638183
 109804/300000: episode: 1122, duration: 0.746s, episode steps: 109, steps per second: 146, episode reward: -46.429, mean reward: -0.426 [-100.000, 14.800], mean action: 1.606 [0.000, 3.000],  loss: 7.355859, mse: 5503.961497, mean_q: 53.473996, mean_eps: 0.637828
 109950/300000: episode: 1123, duration: 1.043s, episode steps: 146, steps per second: 140, episode reward: -27.499, mean reward: -0.188 [-100.000,  8.889], mean action: 1.644 [0.000, 3.000],  loss: 6.898655, mse: 5542.701556, mean_q: 53.191175, mean_eps: 0.637408
 110044/300000: episode: 1124, duration: 0.648s, episode steps:  94, steps per second: 145, episode reward: -34.142, mean reward: -0.363 [-100.000, 15.498], mean action: 1.745 [0.000, 3.000],  loss: 6.420189, mse: 5515.810859, mean_q: 54.107860, mean_eps: 0.637012
 110117/300000: episode: 1125, duration: 0.504s, episode steps:  73, steps per second: 145, episode reward: -39.464, mean reward: -0.541 [-100.000, 10.169], mean action: 1.699 [0.000, 3.000],  loss: 8.115029, mse: 5494.900772, mean_q: 53.411097, mean_eps: 0.636736
 110256/300000: episode: 1126, duration: 1.007s, episode steps: 139, steps per second: 138, episode reward:  0.524, mean reward:  0.004 [-100.000, 12.851], mean action: 1.403 [0.000, 3.000],  loss: 6.704385, mse: 5710.432392, mean_q: 55.231837, mean_eps: 0.636386
 110331/300000: episode: 1127, duration: 0.511s, episode steps:  75, steps per second: 147, episode reward: -52.483, mean reward: -0.700 [-100.000, 10.258], mean action: 1.467 [0.000, 3.000],  loss: 7.181044, mse: 5623.015788, mean_q: 54.405593, mean_eps: 0.636033
 110433/300000: episode: 1128, duration: 0.702s, episode steps: 102, steps per second: 145, episode reward: 13.800, mean reward:  0.135 [-100.000, 20.028], mean action: 1.667 [0.000, 3.000],  loss: 7.159440, mse: 5603.147820, mean_q: 53.196899, mean_eps: 0.635741
 110546/300000: episode: 1129, duration: 0.805s, episode steps: 113, steps per second: 140, episode reward: -67.064, mean reward: -0.593 [-100.000, 31.992], mean action: 1.628 [0.000, 3.000],  loss: 8.422771, mse: 5627.847963, mean_q: 55.521814, mean_eps: 0.635386
 110634/300000: episode: 1130, duration: 0.604s, episode steps:  88, steps per second: 146, episode reward: -65.973, mean reward: -0.750 [-100.000, 12.561], mean action: 1.500 [0.000, 3.000],  loss: 11.901928, mse: 5520.577370, mean_q: 53.314213, mean_eps: 0.635055
 110723/300000: episode: 1131, duration: 0.622s, episode steps:  89, steps per second: 143, episode reward: -47.377, mean reward: -0.532 [-100.000,  9.168], mean action: 1.753 [0.000, 3.000],  loss: 7.001337, mse: 5697.571525, mean_q: 54.867266, mean_eps: 0.634763
 110805/300000: episode: 1132, duration: 0.594s, episode steps:  82, steps per second: 138, episode reward: -38.721, mean reward: -0.472 [-100.000, 11.263], mean action: 1.817 [0.000, 3.000],  loss: 6.294137, mse: 5561.169085, mean_q: 53.981039, mean_eps: 0.634480
 110897/300000: episode: 1133, duration: 0.646s, episode steps:  92, steps per second: 142, episode reward: -9.746, mean reward: -0.106 [-100.000, 15.412], mean action: 1.804 [0.000, 3.000],  loss: 8.900078, mse: 5471.345836, mean_q: 51.455322, mean_eps: 0.634193
 110993/300000: episode: 1134, duration: 0.654s, episode steps:  96, steps per second: 147, episode reward: -68.359, mean reward: -0.712 [-100.000,  6.777], mean action: 1.552 [0.000, 3.000],  loss: 7.532108, mse: 5649.026779, mean_q: 54.263601, mean_eps: 0.633883
 111113/300000: episode: 1135, duration: 0.843s, episode steps: 120, steps per second: 142, episode reward: -51.047, mean reward: -0.425 [-100.000, 21.940], mean action: 1.533 [0.000, 3.000],  loss: 8.687386, mse: 5621.726208, mean_q: 54.570701, mean_eps: 0.633527
 111269/300000: episode: 1136, duration: 1.094s, episode steps: 156, steps per second: 143, episode reward: -221.195, mean reward: -1.418 [-100.000, 44.755], mean action: 1.596 [0.000, 3.000],  loss: 6.449516, mse: 5599.150118, mean_q: 54.873885, mean_eps: 0.633071
 111376/300000: episode: 1137, duration: 0.726s, episode steps: 107, steps per second: 147, episode reward: -109.000, mean reward: -1.019 [-100.000, 11.423], mean action: 1.626 [0.000, 3.000],  loss: 7.287429, mse: 5475.381667, mean_q: 53.076035, mean_eps: 0.632637
 111467/300000: episode: 1138, duration: 0.673s, episode steps:  91, steps per second: 135, episode reward: -59.387, mean reward: -0.653 [-100.000, 11.416], mean action: 1.802 [0.000, 3.000],  loss: 8.535543, mse: 5503.327771, mean_q: 52.727633, mean_eps: 0.632311
 111570/300000: episode: 1139, duration: 0.718s, episode steps: 103, steps per second: 144, episode reward: -91.958, mean reward: -0.893 [-100.000, 11.316], mean action: 1.777 [0.000, 3.000],  loss: 8.888383, mse: 5560.437189, mean_q: 54.225349, mean_eps: 0.631991
 111725/300000: episode: 1140, duration: 1.070s, episode steps: 155, steps per second: 145, episode reward: -110.160, mean reward: -0.711 [-100.000, 15.447], mean action: 1.645 [0.000, 3.000],  loss: 6.945046, mse: 5534.069626, mean_q: 53.767820, mean_eps: 0.631565
 111833/300000: episode: 1141, duration: 0.748s, episode steps: 108, steps per second: 144, episode reward: -66.691, mean reward: -0.618 [-100.000, 10.056], mean action: 1.657 [0.000, 3.000],  loss: 7.305552, mse: 5590.846004, mean_q: 54.355426, mean_eps: 0.631131
 111941/300000: episode: 1142, duration: 0.734s, episode steps: 108, steps per second: 147, episode reward: -76.586, mean reward: -0.709 [-100.000, 13.081], mean action: 1.537 [0.000, 3.000],  loss: 9.738393, mse: 5608.993015, mean_q: 54.455706, mean_eps: 0.630775
 112070/300000: episode: 1143, duration: 0.915s, episode steps: 129, steps per second: 141, episode reward: -219.887, mean reward: -1.705 [-100.000,  4.352], mean action: 1.628 [0.000, 3.000],  loss: 10.697729, mse: 5576.108262, mean_q: 53.527030, mean_eps: 0.630384
 112151/300000: episode: 1144, duration: 0.568s, episode steps:  81, steps per second: 143, episode reward: -80.552, mean reward: -0.994 [-100.000, 16.805], mean action: 1.593 [0.000, 3.000],  loss: 8.908411, mse: 5630.896888, mean_q: 54.143690, mean_eps: 0.630037
 112272/300000: episode: 1145, duration: 0.816s, episode steps: 121, steps per second: 148, episode reward: -144.932, mean reward: -1.198 [-100.000, 10.035], mean action: 1.636 [0.000, 3.000],  loss: 8.257140, mse: 5609.633751, mean_q: 54.638744, mean_eps: 0.629704
 112372/300000: episode: 1146, duration: 0.711s, episode steps: 100, steps per second: 141, episode reward: -76.158, mean reward: -0.762 [-100.000, 13.374], mean action: 1.570 [0.000, 3.000],  loss: 7.282233, mse: 5604.417285, mean_q: 54.186958, mean_eps: 0.629339
 112500/300000: episode: 1147, duration: 0.870s, episode steps: 128, steps per second: 147, episode reward: -29.609, mean reward: -0.231 [-100.000, 47.891], mean action: 1.422 [0.000, 3.000],  loss: 8.850818, mse: 5621.938980, mean_q: 54.702347, mean_eps: 0.628963
 112606/300000: episode: 1148, duration: 0.722s, episode steps: 106, steps per second: 147, episode reward: -94.492, mean reward: -0.891 [-100.000, 22.088], mean action: 1.528 [0.000, 3.000],  loss: 7.909720, mse: 5530.216509, mean_q: 53.489897, mean_eps: 0.628577
 112730/300000: episode: 1149, duration: 0.882s, episode steps: 124, steps per second: 141, episode reward: -52.684, mean reward: -0.425 [-100.000,  7.473], mean action: 1.669 [0.000, 3.000],  loss: 11.017055, mse: 5550.265859, mean_q: 53.168564, mean_eps: 0.628197
 112816/300000: episode: 1150, duration: 0.613s, episode steps:  86, steps per second: 140, episode reward: -51.695, mean reward: -0.601 [-100.000,  5.910], mean action: 1.814 [0.000, 3.000],  loss: 7.651334, mse: 5549.787183, mean_q: 53.985215, mean_eps: 0.627851
 112930/300000: episode: 1151, duration: 0.777s, episode steps: 114, steps per second: 147, episode reward: 12.231, mean reward:  0.107 [-100.000, 18.469], mean action: 1.491 [0.000, 3.000],  loss: 9.695783, mse: 5496.106522, mean_q: 52.455060, mean_eps: 0.627521
 113029/300000: episode: 1152, duration: 0.730s, episode steps:  99, steps per second: 136, episode reward: -15.058, mean reward: -0.152 [-100.000, 18.462], mean action: 1.616 [0.000, 3.000],  loss: 10.547029, mse: 5596.941031, mean_q: 54.607869, mean_eps: 0.627169
 113135/300000: episode: 1153, duration: 0.727s, episode steps: 106, steps per second: 146, episode reward: -29.059, mean reward: -0.274 [-100.000, 18.413], mean action: 1.642 [0.000, 3.000],  loss: 8.171676, mse: 5768.790822, mean_q: 55.623065, mean_eps: 0.626831
 113250/300000: episode: 1154, duration: 0.788s, episode steps: 115, steps per second: 146, episode reward: -72.529, mean reward: -0.631 [-100.000,  9.150], mean action: 1.687 [0.000, 3.000],  loss: 6.909897, mse: 5694.430851, mean_q: 55.377795, mean_eps: 0.626466
 113354/300000: episode: 1155, duration: 0.758s, episode steps: 104, steps per second: 137, episode reward: -57.377, mean reward: -0.552 [-100.000, 15.242], mean action: 1.712 [0.000, 3.000],  loss: 8.598705, mse: 5753.948477, mean_q: 55.113752, mean_eps: 0.626105
 113465/300000: episode: 1156, duration: 0.750s, episode steps: 111, steps per second: 148, episode reward: -27.756, mean reward: -0.250 [-100.000,  8.845], mean action: 1.649 [0.000, 3.000],  loss: 6.946778, mse: 5707.632043, mean_q: 55.684568, mean_eps: 0.625750
 113596/300000: episode: 1157, duration: 0.907s, episode steps: 131, steps per second: 144, episode reward: -113.965, mean reward: -0.870 [-100.000,  7.769], mean action: 1.817 [0.000, 3.000],  loss: 5.526628, mse: 5720.684738, mean_q: 55.445716, mean_eps: 0.625351
 113664/300000: episode: 1158, duration: 0.507s, episode steps:  68, steps per second: 134, episode reward: -98.865, mean reward: -1.454 [-100.000,  5.556], mean action: 1.368 [0.000, 3.000],  loss: 6.809494, mse: 5752.186667, mean_q: 56.268708, mean_eps: 0.625023
 113781/300000: episode: 1159, duration: 0.889s, episode steps: 117, steps per second: 132, episode reward: -37.646, mean reward: -0.322 [-100.000, 21.694], mean action: 1.658 [0.000, 3.000],  loss: 9.392379, mse: 5728.693272, mean_q: 55.483531, mean_eps: 0.624717
 113893/300000: episode: 1160, duration: 0.813s, episode steps: 112, steps per second: 138, episode reward: -56.848, mean reward: -0.508 [-100.000, 17.524], mean action: 1.768 [0.000, 3.000],  loss: 8.231684, mse: 5700.361254, mean_q: 55.173776, mean_eps: 0.624340
 113971/300000: episode: 1161, duration: 0.564s, episode steps:  78, steps per second: 138, episode reward: -18.073, mean reward: -0.232 [-100.000, 13.728], mean action: 1.744 [0.000, 3.000],  loss: 7.820024, mse: 5607.982697, mean_q: 55.764760, mean_eps: 0.624026
 114074/300000: episode: 1162, duration: 0.738s, episode steps: 103, steps per second: 140, episode reward: -112.849, mean reward: -1.096 [-100.000, 10.235], mean action: 1.437 [0.000, 3.000],  loss: 6.286305, mse: 5775.630689, mean_q: 55.278863, mean_eps: 0.623727
 114144/300000: episode: 1163, duration: 0.489s, episode steps:  70, steps per second: 143, episode reward: -14.316, mean reward: -0.205 [-100.000, 20.644], mean action: 1.614 [0.000, 3.000],  loss: 8.618999, mse: 5703.406787, mean_q: 55.241058, mean_eps: 0.623442
 114295/300000: episode: 1164, duration: 1.100s, episode steps: 151, steps per second: 137, episode reward: -67.780, mean reward: -0.449 [-100.000, 41.039], mean action: 1.682 [0.000, 3.000],  loss: 7.197570, mse: 5626.731111, mean_q: 53.685115, mean_eps: 0.623077
 114372/300000: episode: 1165, duration: 0.554s, episode steps:  77, steps per second: 139, episode reward: -85.460, mean reward: -1.110 [-100.000,  6.372], mean action: 1.662 [0.000, 3.000],  loss: 5.947393, mse: 5627.409922, mean_q: 53.096653, mean_eps: 0.622701
 114500/300000: episode: 1166, duration: 1.066s, episode steps: 128, steps per second: 120, episode reward: -86.346, mean reward: -0.675 [-100.000, 10.082], mean action: 1.602 [0.000, 3.000],  loss: 6.763122, mse: 5727.139271, mean_q: 55.669540, mean_eps: 0.622363
 114620/300000: episode: 1167, duration: 0.910s, episode steps: 120, steps per second: 132, episode reward: -96.407, mean reward: -0.803 [-100.000,  7.337], mean action: 1.617 [0.000, 3.000],  loss: 7.052958, mse: 5746.760567, mean_q: 54.207631, mean_eps: 0.621954
 114740/300000: episode: 1168, duration: 0.937s, episode steps: 120, steps per second: 128, episode reward: -5.939, mean reward: -0.049 [-100.000, 19.803], mean action: 1.550 [0.000, 3.000],  loss: 6.332975, mse: 5767.841569, mean_q: 55.317909, mean_eps: 0.621558
 114825/300000: episode: 1169, duration: 0.707s, episode steps:  85, steps per second: 120, episode reward: -64.182, mean reward: -0.755 [-100.000, 10.080], mean action: 1.635 [0.000, 3.000],  loss: 6.852748, mse: 5689.788873, mean_q: 54.041984, mean_eps: 0.621219
 114951/300000: episode: 1170, duration: 0.985s, episode steps: 126, steps per second: 128, episode reward: -47.975, mean reward: -0.381 [-100.000, 20.384], mean action: 1.579 [0.000, 3.000],  loss: 8.674669, mse: 5640.043631, mean_q: 54.382216, mean_eps: 0.620871
 115049/300000: episode: 1171, duration: 0.728s, episode steps:  98, steps per second: 135, episode reward: -53.733, mean reward: -0.548 [-100.000,  6.798], mean action: 1.796 [0.000, 3.000],  loss: 6.247215, mse: 5600.993727, mean_q: 53.402755, mean_eps: 0.620502
 115134/300000: episode: 1172, duration: 0.628s, episode steps:  85, steps per second: 135, episode reward: -67.423, mean reward: -0.793 [-100.000, 10.285], mean action: 1.694 [0.000, 3.000],  loss: 7.245934, mse: 5805.320111, mean_q: 54.512825, mean_eps: 0.620200
 115222/300000: episode: 1173, duration: 0.644s, episode steps:  88, steps per second: 137, episode reward: -61.048, mean reward: -0.694 [-100.000, 31.853], mean action: 1.761 [0.000, 3.000],  loss: 5.259502, mse: 5739.619246, mean_q: 53.949688, mean_eps: 0.619914
 115318/300000: episode: 1174, duration: 0.716s, episode steps:  96, steps per second: 134, episode reward: -2.395, mean reward: -0.025 [-100.000, 13.924], mean action: 1.479 [0.000, 3.000],  loss: 8.703133, mse: 5656.039378, mean_q: 53.356889, mean_eps: 0.619611
 115439/300000: episode: 1175, duration: 0.891s, episode steps: 121, steps per second: 136, episode reward: -230.465, mean reward: -1.905 [-100.000, 45.989], mean action: 1.587 [0.000, 3.000],  loss: 8.971222, mse: 5807.717945, mean_q: 54.303806, mean_eps: 0.619253
 115532/300000: episode: 1176, duration: 0.723s, episode steps:  93, steps per second: 129, episode reward: -67.242, mean reward: -0.723 [-100.000,  9.560], mean action: 1.624 [0.000, 3.000],  loss: 6.320759, mse: 5785.097257, mean_q: 55.618989, mean_eps: 0.618900
 115643/300000: episode: 1177, duration: 0.901s, episode steps: 111, steps per second: 123, episode reward: -42.534, mean reward: -0.383 [-100.000, 16.020], mean action: 1.577 [0.000, 3.000],  loss: 6.736673, mse: 5801.242240, mean_q: 53.220348, mean_eps: 0.618563
 115751/300000: episode: 1178, duration: 0.801s, episode steps: 108, steps per second: 135, episode reward: -155.658, mean reward: -1.441 [-100.000,  6.670], mean action: 1.556 [0.000, 3.000],  loss: 6.370825, mse: 5859.303435, mean_q: 54.165036, mean_eps: 0.618202
 115828/300000: episode: 1179, duration: 0.566s, episode steps:  77, steps per second: 136, episode reward: -62.053, mean reward: -0.806 [-100.000, 12.505], mean action: 1.701 [0.000, 3.000],  loss: 5.826910, mse: 5811.372977, mean_q: 55.061654, mean_eps: 0.617896
 115951/300000: episode: 1180, duration: 0.966s, episode steps: 123, steps per second: 127, episode reward: -144.705, mean reward: -1.176 [-100.000, 11.942], mean action: 1.488 [0.000, 3.000],  loss: 8.571690, mse: 5715.205975, mean_q: 53.540569, mean_eps: 0.617566
 116081/300000: episode: 1181, duration: 1.083s, episode steps: 130, steps per second: 120, episode reward: -15.741, mean reward: -0.121 [-100.000, 16.398], mean action: 1.585 [0.000, 3.000],  loss: 7.619303, mse: 5745.679143, mean_q: 53.955348, mean_eps: 0.617149
 116163/300000: episode: 1182, duration: 0.657s, episode steps:  82, steps per second: 125, episode reward: -86.142, mean reward: -1.051 [-100.000,  6.585], mean action: 1.598 [0.000, 3.000],  loss: 7.994769, mse: 5796.292582, mean_q: 52.919308, mean_eps: 0.616799
 116282/300000: episode: 1183, duration: 0.859s, episode steps: 119, steps per second: 138, episode reward: -19.731, mean reward: -0.166 [-100.000, 23.183], mean action: 1.546 [0.000, 3.000],  loss: 4.953194, mse: 5761.115042, mean_q: 53.888190, mean_eps: 0.616467
 116370/300000: episode: 1184, duration: 0.616s, episode steps:  88, steps per second: 143, episode reward: -55.969, mean reward: -0.636 [-100.000,  9.919], mean action: 1.682 [0.000, 3.000],  loss: 7.790682, mse: 5754.784773, mean_q: 53.944127, mean_eps: 0.616126
 116466/300000: episode: 1185, duration: 0.723s, episode steps:  96, steps per second: 133, episode reward: -90.712, mean reward: -0.945 [-100.000,  6.677], mean action: 1.583 [0.000, 3.000],  loss: 8.345763, mse: 5672.625483, mean_q: 53.342913, mean_eps: 0.615822
 116555/300000: episode: 1186, duration: 0.670s, episode steps:  89, steps per second: 133, episode reward: -51.406, mean reward: -0.578 [-100.000, 18.286], mean action: 1.562 [0.000, 3.000],  loss: 7.007493, mse: 5700.127568, mean_q: 52.620525, mean_eps: 0.615517
 116682/300000: episode: 1187, duration: 0.918s, episode steps: 127, steps per second: 138, episode reward: -8.174, mean reward: -0.064 [-100.000,  8.958], mean action: 1.669 [0.000, 3.000],  loss: 7.389591, mse: 5780.341416, mean_q: 54.084939, mean_eps: 0.615161
 116790/300000: episode: 1188, duration: 0.838s, episode steps: 108, steps per second: 129, episode reward: -77.965, mean reward: -0.722 [-100.000, 14.717], mean action: 1.509 [0.000, 3.000],  loss: 6.561442, mse: 5791.441031, mean_q: 53.901910, mean_eps: 0.614773
 116945/300000: episode: 1189, duration: 1.289s, episode steps: 155, steps per second: 120, episode reward:  6.966, mean reward:  0.045 [-100.000, 16.601], mean action: 1.594 [0.000, 3.000],  loss: 6.328776, mse: 5863.307954, mean_q: 54.945609, mean_eps: 0.614339
 117073/300000: episode: 1190, duration: 0.972s, episode steps: 128, steps per second: 132, episode reward: -57.689, mean reward: -0.451 [-100.000, 14.612], mean action: 1.633 [0.000, 3.000],  loss: 8.007868, mse: 5715.350826, mean_q: 53.831983, mean_eps: 0.613872
 117195/300000: episode: 1191, duration: 0.859s, episode steps: 122, steps per second: 142, episode reward: -59.380, mean reward: -0.487 [-100.000, 14.134], mean action: 1.467 [0.000, 3.000],  loss: 6.446898, mse: 5685.484699, mean_q: 53.909398, mean_eps: 0.613459
 117310/300000: episode: 1192, duration: 0.884s, episode steps: 115, steps per second: 130, episode reward: -47.410, mean reward: -0.412 [-100.000, 17.718], mean action: 1.678 [0.000, 3.000],  loss: 6.298184, mse: 5688.626214, mean_q: 53.495016, mean_eps: 0.613068
 117440/300000: episode: 1193, duration: 1.123s, episode steps: 130, steps per second: 116, episode reward: -6.367, mean reward: -0.049 [-100.000, 30.291], mean action: 1.523 [0.000, 3.000],  loss: 7.670666, mse: 5665.044144, mean_q: 53.924509, mean_eps: 0.612664
 117555/300000: episode: 1194, duration: 0.960s, episode steps: 115, steps per second: 120, episode reward: -53.823, mean reward: -0.468 [-100.000, 14.488], mean action: 1.496 [0.000, 3.000],  loss: 7.018000, mse: 5809.661744, mean_q: 54.985902, mean_eps: 0.612260
 117667/300000: episode: 1195, duration: 0.889s, episode steps: 112, steps per second: 126, episode reward: -43.170, mean reward: -0.385 [-100.000, 16.564], mean action: 1.607 [0.000, 3.000],  loss: 7.006732, mse: 5765.135986, mean_q: 53.886044, mean_eps: 0.611885
 117788/300000: episode: 1196, duration: 0.931s, episode steps: 121, steps per second: 130, episode reward: -70.744, mean reward: -0.585 [-100.000, 10.570], mean action: 1.595 [0.000, 3.000],  loss: 8.500467, mse: 5726.805842, mean_q: 53.781101, mean_eps: 0.611501
 117889/300000: episode: 1197, duration: 0.842s, episode steps: 101, steps per second: 120, episode reward: -68.915, mean reward: -0.682 [-100.000,  9.948], mean action: 1.683 [0.000, 3.000],  loss: 6.662801, mse: 5679.390243, mean_q: 53.957200, mean_eps: 0.611135
 118019/300000: episode: 1198, duration: 1.024s, episode steps: 130, steps per second: 127, episode reward: -19.990, mean reward: -0.154 [-100.000, 34.434], mean action: 1.554 [0.000, 3.000],  loss: 8.156389, mse: 5701.992511, mean_q: 54.210179, mean_eps: 0.610753
 118172/300000: episode: 1199, duration: 1.155s, episode steps: 153, steps per second: 132, episode reward: -65.045, mean reward: -0.425 [-100.000,  7.016], mean action: 1.582 [0.000, 3.000],  loss: 8.848899, mse: 5632.458343, mean_q: 54.761883, mean_eps: 0.610286
 118299/300000: episode: 1200, duration: 0.872s, episode steps: 127, steps per second: 146, episode reward: -7.635, mean reward: -0.060 [-100.000, 11.186], mean action: 1.638 [0.000, 3.000],  loss: 5.768888, mse: 5626.036214, mean_q: 53.612151, mean_eps: 0.609824
 118418/300000: episode: 1201, duration: 0.881s, episode steps: 119, steps per second: 135, episode reward: -73.290, mean reward: -0.616 [-100.000,  6.830], mean action: 1.529 [0.000, 3.000],  loss: 7.070929, mse: 5662.074268, mean_q: 53.792145, mean_eps: 0.609419
 118538/300000: episode: 1202, duration: 0.979s, episode steps: 120, steps per second: 123, episode reward: -89.893, mean reward: -0.749 [-100.000,  9.371], mean action: 1.492 [0.000, 3.000],  loss: 6.871902, mse: 5694.057869, mean_q: 54.378259, mean_eps: 0.609024
 118671/300000: episode: 1203, duration: 1.121s, episode steps: 133, steps per second: 119, episode reward: -50.984, mean reward: -0.383 [-100.000, 13.518], mean action: 1.504 [0.000, 3.000],  loss: 6.086358, mse: 5620.020020, mean_q: 53.683150, mean_eps: 0.608607
 118770/300000: episode: 1204, duration: 0.803s, episode steps:  99, steps per second: 123, episode reward: -18.198, mean reward: -0.184 [-100.000, 15.439], mean action: 1.667 [0.000, 3.000],  loss: 6.473989, mse: 5607.544439, mean_q: 52.774096, mean_eps: 0.608224
 118850/300000: episode: 1205, duration: 0.870s, episode steps:  80, steps per second:  92, episode reward: -53.186, mean reward: -0.665 [-100.000,  7.325], mean action: 1.575 [0.000, 3.000],  loss: 7.319336, mse: 5684.288800, mean_q: 56.063190, mean_eps: 0.607929
 118975/300000: episode: 1206, duration: 1.036s, episode steps: 125, steps per second: 121, episode reward: -102.770, mean reward: -0.822 [-100.000, 22.683], mean action: 1.736 [0.000, 3.000],  loss: 6.520609, mse: 5714.034332, mean_q: 54.930438, mean_eps: 0.607590
 119100/300000: episode: 1207, duration: 0.865s, episode steps: 125, steps per second: 144, episode reward: -32.933, mean reward: -0.263 [-100.000,  8.788], mean action: 1.736 [0.000, 3.000],  loss: 5.633676, mse: 5657.274395, mean_q: 53.893582, mean_eps: 0.607178
 119218/300000: episode: 1208, duration: 0.835s, episode steps: 118, steps per second: 141, episode reward: -76.574, mean reward: -0.649 [-100.000, 10.034], mean action: 1.661 [0.000, 3.000],  loss: 6.483423, mse: 5641.819837, mean_q: 53.046936, mean_eps: 0.606777
 119327/300000: episode: 1209, duration: 0.776s, episode steps: 109, steps per second: 140, episode reward: -117.275, mean reward: -1.076 [-100.000, 36.802], mean action: 1.734 [0.000, 3.000],  loss: 6.138941, mse: 5717.186828, mean_q: 55.501967, mean_eps: 0.606402
 119451/300000: episode: 1210, duration: 0.944s, episode steps: 124, steps per second: 131, episode reward: -69.428, mean reward: -0.560 [-100.000, 16.927], mean action: 1.685 [0.000, 3.000],  loss: 6.954459, mse: 5817.873972, mean_q: 55.634227, mean_eps: 0.606018
 119544/300000: episode: 1211, duration: 0.763s, episode steps:  93, steps per second: 122, episode reward: -62.794, mean reward: -0.675 [-100.000, 11.076], mean action: 1.570 [0.000, 3.000],  loss: 6.334843, mse: 5702.703262, mean_q: 54.277959, mean_eps: 0.605660
 119618/300000: episode: 1212, duration: 0.769s, episode steps:  74, steps per second:  96, episode reward: -64.730, mean reward: -0.875 [-100.000,  9.619], mean action: 1.784 [0.000, 3.000],  loss: 6.487374, mse: 5640.844601, mean_q: 54.633701, mean_eps: 0.605384
 119793/300000: episode: 1213, duration: 1.457s, episode steps: 175, steps per second: 120, episode reward: -8.453, mean reward: -0.048 [-100.000, 18.510], mean action: 1.600 [0.000, 3.000],  loss: 7.361090, mse: 5640.946024, mean_q: 54.464483, mean_eps: 0.604973
 119928/300000: episode: 1214, duration: 0.958s, episode steps: 135, steps per second: 141, episode reward: -24.498, mean reward: -0.181 [-100.000, 13.978], mean action: 1.467 [0.000, 3.000],  loss: 6.215485, mse: 5673.312091, mean_q: 55.135546, mean_eps: 0.604462
 120012/300000: episode: 1215, duration: 0.599s, episode steps:  84, steps per second: 140, episode reward: -90.189, mean reward: -1.074 [-100.000, 13.689], mean action: 1.524 [0.000, 3.000],  loss: 7.004369, mse: 5724.861439, mean_q: 54.217135, mean_eps: 0.604101
 120121/300000: episode: 1216, duration: 0.772s, episode steps: 109, steps per second: 141, episode reward: -25.699, mean reward: -0.236 [-100.000, 11.121], mean action: 1.642 [0.000, 3.000],  loss: 5.972909, mse: 5682.176547, mean_q: 54.515894, mean_eps: 0.603782
 120239/300000: episode: 1217, duration: 0.822s, episode steps: 118, steps per second: 144, episode reward: -56.829, mean reward: -0.482 [-100.000, 13.020], mean action: 1.500 [0.000, 3.000],  loss: 6.438786, mse: 5711.454908, mean_q: 53.758468, mean_eps: 0.603408
 120361/300000: episode: 1218, duration: 0.912s, episode steps: 122, steps per second: 134, episode reward: -18.100, mean reward: -0.148 [-100.000, 17.372], mean action: 1.508 [0.000, 3.000],  loss: 6.685002, mse: 5721.963643, mean_q: 53.980898, mean_eps: 0.603012
 120468/300000: episode: 1219, duration: 0.761s, episode steps: 107, steps per second: 141, episode reward: -56.781, mean reward: -0.531 [-100.000, 17.026], mean action: 1.598 [0.000, 3.000],  loss: 6.673550, mse: 5757.635865, mean_q: 55.311174, mean_eps: 0.602634
 120562/300000: episode: 1220, duration: 0.748s, episode steps:  94, steps per second: 126, episode reward: -26.333, mean reward: -0.280 [-100.000, 12.174], mean action: 1.809 [0.000, 3.000],  loss: 8.050130, mse: 5740.663237, mean_q: 55.812477, mean_eps: 0.602302
 121122/300000: episode: 1221, duration: 5.252s, episode steps: 560, steps per second: 107, episode reward: -197.348, mean reward: -0.352 [-100.000, 21.635], mean action: 1.538 [0.000, 3.000],  loss: 7.111861, mse: 5671.873311, mean_q: 54.032450, mean_eps: 0.601223
 121223/300000: episode: 1222, duration: 1.233s, episode steps: 101, steps per second:  82, episode reward: -41.893, mean reward: -0.415 [-100.000, 12.045], mean action: 1.604 [0.000, 3.000],  loss: 7.739480, mse: 5701.525183, mean_q: 53.910678, mean_eps: 0.600132
 121295/300000: episode: 1223, duration: 0.777s, episode steps:  72, steps per second:  93, episode reward: -36.475, mean reward: -0.507 [-100.000, 13.776], mean action: 1.514 [0.000, 3.000],  loss: 6.640142, mse: 5740.965732, mean_q: 54.326067, mean_eps: 0.599847
 121401/300000: episode: 1224, duration: 0.923s, episode steps: 106, steps per second: 115, episode reward: -89.758, mean reward: -0.847 [-100.000, 13.793], mean action: 1.462 [0.000, 3.000],  loss: 8.547853, mse: 5685.293344, mean_q: 54.273868, mean_eps: 0.599553
 121505/300000: episode: 1225, duration: 0.946s, episode steps: 104, steps per second: 110, episode reward: -35.958, mean reward: -0.346 [-100.000, 12.289], mean action: 1.538 [0.000, 3.000],  loss: 7.027052, mse: 5586.341766, mean_q: 53.651173, mean_eps: 0.599207
 121596/300000: episode: 1226, duration: 0.745s, episode steps:  91, steps per second: 122, episode reward: -55.218, mean reward: -0.607 [-100.000, 19.184], mean action: 1.604 [0.000, 3.000],  loss: 6.303964, mse: 5755.976820, mean_q: 54.638657, mean_eps: 0.598885
 121733/300000: episode: 1227, duration: 1.021s, episode steps: 137, steps per second: 134, episode reward: -42.538, mean reward: -0.310 [-100.000, 13.385], mean action: 1.635 [0.000, 3.000],  loss: 6.944786, mse: 5679.332904, mean_q: 52.956945, mean_eps: 0.598509
 121878/300000: episode: 1228, duration: 1.066s, episode steps: 145, steps per second: 136, episode reward: -10.104, mean reward: -0.070 [-100.000, 16.165], mean action: 1.566 [0.000, 3.000],  loss: 7.017571, mse: 5713.416701, mean_q: 54.116329, mean_eps: 0.598043
 121990/300000: episode: 1229, duration: 0.767s, episode steps: 112, steps per second: 146, episode reward: -15.165, mean reward: -0.135 [-100.000,  9.339], mean action: 1.545 [0.000, 3.000],  loss: 8.325215, mse: 5686.430023, mean_q: 53.322971, mean_eps: 0.597619
 122114/300000: episode: 1230, duration: 0.887s, episode steps: 124, steps per second: 140, episode reward: -28.352, mean reward: -0.229 [-100.000, 36.483], mean action: 1.548 [0.000, 3.000],  loss: 7.501349, mse: 5767.033652, mean_q: 54.855593, mean_eps: 0.597230
 122211/300000: episode: 1231, duration: 0.678s, episode steps:  97, steps per second: 143, episode reward:  0.794, mean reward:  0.008 [-100.000, 10.768], mean action: 1.567 [0.000, 3.000],  loss: 6.017421, mse: 5720.499683, mean_q: 55.956149, mean_eps: 0.596865
 122330/300000: episode: 1232, duration: 0.955s, episode steps: 119, steps per second: 125, episode reward: -59.655, mean reward: -0.501 [-100.000,  8.346], mean action: 1.571 [0.000, 3.000],  loss: 7.946432, mse: 5774.827386, mean_q: 53.656490, mean_eps: 0.596509
 122415/300000: episode: 1233, duration: 0.804s, episode steps:  85, steps per second: 106, episode reward: -28.657, mean reward: -0.337 [-100.000, 13.974], mean action: 1.600 [0.000, 3.000],  loss: 7.052238, mse: 5591.970545, mean_q: 54.778499, mean_eps: 0.596172
 122514/300000: episode: 1234, duration: 0.766s, episode steps:  99, steps per second: 129, episode reward: -34.060, mean reward: -0.344 [-100.000, 10.360], mean action: 1.586 [0.000, 3.000],  loss: 5.647542, mse: 5791.292436, mean_q: 54.833334, mean_eps: 0.595869
 122644/300000: episode: 1235, duration: 0.953s, episode steps: 130, steps per second: 136, episode reward: -43.734, mean reward: -0.336 [-100.000,  8.703], mean action: 1.477 [0.000, 3.000],  loss: 6.111710, mse: 5772.840242, mean_q: 55.373960, mean_eps: 0.595491
 122774/300000: episode: 1236, duration: 1.342s, episode steps: 130, steps per second:  97, episode reward: -21.954, mean reward: -0.169 [-100.000, 16.204], mean action: 1.677 [0.000, 3.000],  loss: 5.336897, mse: 5801.963653, mean_q: 56.385247, mean_eps: 0.595062
 122857/300000: episode: 1237, duration: 0.584s, episode steps:  83, steps per second: 142, episode reward: -92.591, mean reward: -1.116 [-100.000, 11.651], mean action: 1.578 [0.000, 3.000],  loss: 5.760174, mse: 5831.382142, mean_q: 56.038186, mean_eps: 0.594711
 122931/300000: episode: 1238, duration: 0.550s, episode steps:  74, steps per second: 135, episode reward: -66.697, mean reward: -0.901 [-100.000,  9.209], mean action: 1.527 [0.000, 3.000],  loss: 6.868639, mse: 5916.597669, mean_q: 57.178452, mean_eps: 0.594451
 123033/300000: episode: 1239, duration: 0.722s, episode steps: 102, steps per second: 141, episode reward: -84.671, mean reward: -0.830 [-100.000, 16.784], mean action: 1.608 [0.000, 3.000],  loss: 6.262713, mse: 5741.943836, mean_q: 54.517580, mean_eps: 0.594161
 123134/300000: episode: 1240, duration: 0.689s, episode steps: 101, steps per second: 147, episode reward: -47.461, mean reward: -0.470 [-100.000, 17.950], mean action: 1.861 [0.000, 3.000],  loss: 5.752267, mse: 5806.412777, mean_q: 55.775348, mean_eps: 0.593826
 123252/300000: episode: 1241, duration: 0.877s, episode steps: 118, steps per second: 135, episode reward: -50.628, mean reward: -0.429 [-100.000, 29.633], mean action: 1.424 [0.000, 3.000],  loss: 7.588617, mse: 5824.736647, mean_q: 54.238925, mean_eps: 0.593465
 123326/300000: episode: 1242, duration: 0.517s, episode steps:  74, steps per second: 143, episode reward: -29.980, mean reward: -0.405 [-100.000, 15.609], mean action: 1.743 [0.000, 3.000],  loss: 5.406578, mse: 5901.891404, mean_q: 56.003301, mean_eps: 0.593148
 123452/300000: episode: 1243, duration: 0.859s, episode steps: 126, steps per second: 147, episode reward: -52.630, mean reward: -0.418 [-100.000, 10.898], mean action: 1.643 [0.000, 3.000],  loss: 7.177420, mse: 5727.175522, mean_q: 53.754665, mean_eps: 0.592818
 123566/300000: episode: 1244, duration: 0.806s, episode steps: 114, steps per second: 141, episode reward: -36.784, mean reward: -0.323 [-100.000, 21.197], mean action: 1.605 [0.000, 3.000],  loss: 6.601243, mse: 5822.944837, mean_q: 54.767579, mean_eps: 0.592422
 123718/300000: episode: 1245, duration: 1.039s, episode steps: 152, steps per second: 146, episode reward: -9.225, mean reward: -0.061 [-100.000, 17.719], mean action: 1.566 [0.000, 3.000],  loss: 5.866300, mse: 5777.687458, mean_q: 55.089924, mean_eps: 0.591983
 123837/300000: episode: 1246, duration: 0.965s, episode steps: 119, steps per second: 123, episode reward: -73.281, mean reward: -0.616 [-100.000, 15.660], mean action: 1.630 [0.000, 3.000],  loss: 6.587850, mse: 5842.034204, mean_q: 55.320051, mean_eps: 0.591536
 123952/300000: episode: 1247, duration: 0.870s, episode steps: 115, steps per second: 132, episode reward: -41.036, mean reward: -0.357 [-100.000, 12.461], mean action: 1.643 [0.000, 3.000],  loss: 5.540674, mse: 5965.541096, mean_q: 55.430601, mean_eps: 0.591150
 124046/300000: episode: 1248, duration: 0.681s, episode steps:  94, steps per second: 138, episode reward: -78.169, mean reward: -0.832 [-100.000, 11.750], mean action: 1.404 [0.000, 3.000],  loss: 8.250606, mse: 5847.764456, mean_q: 55.888069, mean_eps: 0.590805
 124162/300000: episode: 1249, duration: 1.003s, episode steps: 116, steps per second: 116, episode reward: -60.135, mean reward: -0.518 [-100.000, 11.960], mean action: 1.647 [0.000, 3.000],  loss: 6.660145, mse: 5926.505363, mean_q: 56.183743, mean_eps: 0.590458
 124221/300000: episode: 1250, duration: 0.777s, episode steps:  59, steps per second:  76, episode reward: -72.771, mean reward: -1.233 [-100.000, 12.402], mean action: 1.492 [0.000, 3.000],  loss: 5.557553, mse: 5900.115168, mean_q: 56.059883, mean_eps: 0.590170
 124314/300000: episode: 1251, duration: 0.800s, episode steps:  93, steps per second: 116, episode reward: -61.808, mean reward: -0.665 [-100.000,  5.736], mean action: 1.634 [0.000, 3.000],  loss: 6.033279, mse: 5789.960790, mean_q: 55.969145, mean_eps: 0.589919
 124437/300000: episode: 1252, duration: 1.032s, episode steps: 123, steps per second: 119, episode reward: -159.905, mean reward: -1.300 [-100.000, 17.736], mean action: 1.569 [0.000, 3.000],  loss: 7.060926, mse: 5770.624813, mean_q: 54.747647, mean_eps: 0.589563
 124516/300000: episode: 1253, duration: 0.555s, episode steps:  79, steps per second: 142, episode reward: -20.444, mean reward: -0.259 [-100.000, 10.718], mean action: 1.722 [0.000, 3.000],  loss: 8.960571, mse: 5895.349486, mean_q: 55.276733, mean_eps: 0.589229
 124600/300000: episode: 1254, duration: 0.652s, episode steps:  84, steps per second: 129, episode reward: -49.917, mean reward: -0.594 [-100.000,  8.600], mean action: 1.536 [0.000, 3.000],  loss: 7.749969, mse: 5832.730126, mean_q: 56.071088, mean_eps: 0.588960
 124705/300000: episode: 1255, duration: 0.909s, episode steps: 105, steps per second: 115, episode reward: 51.805, mean reward:  0.493 [-100.000, 40.373], mean action: 1.962 [0.000, 3.000],  loss: 6.755151, mse: 5845.161765, mean_q: 56.327332, mean_eps: 0.588648
 124849/300000: episode: 1256, duration: 1.350s, episode steps: 144, steps per second: 107, episode reward: -70.738, mean reward: -0.491 [-100.000, 13.327], mean action: 1.840 [0.000, 3.000],  loss: 6.211441, mse: 5788.055766, mean_q: 55.686976, mean_eps: 0.588238
 124970/300000: episode: 1257, duration: 1.083s, episode steps: 121, steps per second: 112, episode reward: -71.109, mean reward: -0.588 [-100.000,  7.238], mean action: 1.479 [0.000, 3.000],  loss: 9.307387, mse: 5810.509156, mean_q: 55.225537, mean_eps: 0.587800
 125142/300000: episode: 1258, duration: 1.462s, episode steps: 172, steps per second: 118, episode reward: 35.206, mean reward:  0.205 [-100.000, 58.244], mean action: 1.558 [0.000, 3.000],  loss: 6.915737, mse: 6009.422085, mean_q: 57.215223, mean_eps: 0.587317
 125266/300000: episode: 1259, duration: 1.101s, episode steps: 124, steps per second: 113, episode reward: -39.736, mean reward: -0.320 [-100.000, 11.881], mean action: 1.718 [0.000, 3.000],  loss: 5.925549, mse: 5902.241404, mean_q: 56.604463, mean_eps: 0.586828
 125342/300000: episode: 1260, duration: 0.742s, episode steps:  76, steps per second: 102, episode reward: -35.156, mean reward: -0.463 [-100.000, 12.061], mean action: 1.618 [0.000, 3.000],  loss: 6.191813, mse: 6005.607717, mean_q: 58.135053, mean_eps: 0.586498
 125455/300000: episode: 1261, duration: 0.911s, episode steps: 113, steps per second: 124, episode reward: -32.813, mean reward: -0.290 [-100.000, 15.277], mean action: 1.602 [0.000, 3.000],  loss: 7.664079, mse: 5851.077326, mean_q: 55.838434, mean_eps: 0.586187
 125954/300000: episode: 1262, duration: 5.235s, episode steps: 499, steps per second:  95, episode reward: -187.741, mean reward: -0.376 [-100.000, 25.589], mean action: 1.747 [0.000, 3.000],  loss: 6.134568, mse: 5943.994693, mean_q: 57.258180, mean_eps: 0.585177
 126054/300000: episode: 1263, duration: 1.177s, episode steps: 100, steps per second:  85, episode reward: -108.593, mean reward: -1.086 [-100.000, 11.389], mean action: 1.700 [0.000, 3.000],  loss: 6.282110, mse: 6036.350161, mean_q: 55.876052, mean_eps: 0.584188
 126188/300000: episode: 1264, duration: 1.351s, episode steps: 134, steps per second:  99, episode reward: -36.559, mean reward: -0.273 [-100.000,  7.751], mean action: 1.694 [0.000, 3.000],  loss: 8.471916, mse: 6015.420720, mean_q: 55.546515, mean_eps: 0.583802
 126296/300000: episode: 1265, duration: 1.238s, episode steps: 108, steps per second:  87, episode reward: -64.697, mean reward: -0.599 [-100.000, 11.874], mean action: 1.731 [0.000, 3.000],  loss: 8.159720, mse: 5986.673570, mean_q: 56.113118, mean_eps: 0.583403
 126381/300000: episode: 1266, duration: 0.795s, episode steps:  85, steps per second: 107, episode reward: -58.526, mean reward: -0.689 [-100.000, 11.194], mean action: 1.635 [0.000, 3.000],  loss: 7.629052, mse: 5973.343003, mean_q: 57.030449, mean_eps: 0.583085
 126494/300000: episode: 1267, duration: 1.066s, episode steps: 113, steps per second: 106, episode reward: -88.824, mean reward: -0.786 [-100.000, 16.389], mean action: 1.761 [0.000, 3.000],  loss: 7.065623, mse: 5938.035070, mean_q: 55.588927, mean_eps: 0.582758
 126594/300000: episode: 1268, duration: 0.827s, episode steps: 100, steps per second: 121, episode reward: -63.120, mean reward: -0.631 [-100.000, 15.724], mean action: 1.590 [0.000, 3.000],  loss: 7.320911, mse: 6020.282676, mean_q: 55.802608, mean_eps: 0.582406
 126714/300000: episode: 1269, duration: 1.122s, episode steps: 120, steps per second: 107, episode reward: -54.855, mean reward: -0.457 [-100.000, 10.700], mean action: 1.767 [0.000, 3.000],  loss: 7.397123, mse: 5981.764128, mean_q: 56.431474, mean_eps: 0.582043
 126820/300000: episode: 1270, duration: 0.768s, episode steps: 106, steps per second: 138, episode reward: -16.928, mean reward: -0.160 [-100.000, 14.650], mean action: 1.745 [0.000, 3.000],  loss: 6.977511, mse: 5946.904523, mean_q: 56.169082, mean_eps: 0.581671
 126949/300000: episode: 1271, duration: 1.046s, episode steps: 129, steps per second: 123, episode reward: -11.691, mean reward: -0.091 [-100.000, 22.559], mean action: 1.659 [0.000, 3.000],  loss: 6.444819, mse: 6014.323435, mean_q: 56.799233, mean_eps: 0.581283
 127054/300000: episode: 1272, duration: 0.773s, episode steps: 105, steps per second: 136, episode reward: -48.414, mean reward: -0.461 [-100.000, 12.959], mean action: 1.724 [0.000, 3.000],  loss: 6.670914, mse: 5997.629906, mean_q: 56.607029, mean_eps: 0.580897
 127174/300000: episode: 1273, duration: 0.912s, episode steps: 120, steps per second: 132, episode reward: 40.795, mean reward:  0.340 [-100.000, 43.171], mean action: 1.542 [0.000, 3.000],  loss: 6.276628, mse: 6148.189791, mean_q: 58.451548, mean_eps: 0.580525
 127253/300000: episode: 1274, duration: 0.591s, episode steps:  79, steps per second: 134, episode reward: -19.454, mean reward: -0.246 [-100.000,  7.942], mean action: 1.646 [0.000, 3.000],  loss: 7.412280, mse: 6207.256929, mean_q: 58.080850, mean_eps: 0.580197
 127364/300000: episode: 1275, duration: 0.790s, episode steps: 111, steps per second: 140, episode reward: -66.235, mean reward: -0.597 [-100.000, 11.984], mean action: 1.559 [0.000, 3.000],  loss: 6.078342, mse: 6066.487327, mean_q: 57.364682, mean_eps: 0.579884
 127437/300000: episode: 1276, duration: 0.508s, episode steps:  73, steps per second: 144, episode reward: -69.184, mean reward: -0.948 [-100.000, 10.195], mean action: 1.699 [0.000, 3.000],  loss: 6.813931, mse: 5914.125194, mean_q: 55.123673, mean_eps: 0.579580
 127638/300000: episode: 1277, duration: 1.446s, episode steps: 201, steps per second: 139, episode reward: -92.818, mean reward: -0.462 [-100.000, 12.113], mean action: 1.587 [0.000, 3.000],  loss: 7.604674, mse: 6065.619631, mean_q: 56.882855, mean_eps: 0.579128
 127747/300000: episode: 1278, duration: 0.761s, episode steps: 109, steps per second: 143, episode reward: -81.077, mean reward: -0.744 [-100.000, 11.407], mean action: 1.752 [0.000, 3.000],  loss: 6.558704, mse: 6139.015997, mean_q: 58.078437, mean_eps: 0.578616
 127845/300000: episode: 1279, duration: 0.720s, episode steps:  98, steps per second: 136, episode reward: -43.184, mean reward: -0.441 [-100.000, 10.118], mean action: 1.673 [0.000, 3.000],  loss: 7.363777, mse: 6138.827771, mean_q: 58.940700, mean_eps: 0.578275
 127953/300000: episode: 1280, duration: 0.753s, episode steps: 108, steps per second: 143, episode reward: -63.049, mean reward: -0.584 [-100.000, 12.783], mean action: 1.528 [0.000, 3.000],  loss: 6.052981, mse: 6028.188192, mean_q: 57.000571, mean_eps: 0.577935
 128065/300000: episode: 1281, duration: 0.797s, episode steps: 112, steps per second: 141, episode reward: -49.947, mean reward: -0.446 [-100.000,  6.759], mean action: 1.741 [0.000, 3.000],  loss: 7.288479, mse: 6036.441071, mean_q: 57.634772, mean_eps: 0.577572
 128188/300000: episode: 1282, duration: 0.889s, episode steps: 123, steps per second: 138, episode reward: -38.225, mean reward: -0.311 [-100.000,  9.135], mean action: 1.512 [0.000, 3.000],  loss: 7.178877, mse: 6111.213609, mean_q: 56.851108, mean_eps: 0.577184
 128301/300000: episode: 1283, duration: 0.813s, episode steps: 113, steps per second: 139, episode reward: -63.312, mean reward: -0.560 [-100.000, 19.275], mean action: 1.708 [0.000, 3.000],  loss: 7.104197, mse: 6121.414110, mean_q: 57.930023, mean_eps: 0.576795
 128446/300000: episode: 1284, duration: 1.204s, episode steps: 145, steps per second: 120, episode reward: -26.386, mean reward: -0.182 [-100.000, 10.290], mean action: 1.621 [0.000, 3.000],  loss: 7.005315, mse: 6067.928149, mean_q: 57.390110, mean_eps: 0.576369
 128561/300000: episode: 1285, duration: 0.793s, episode steps: 115, steps per second: 145, episode reward: -60.640, mean reward: -0.527 [-100.000, 21.062], mean action: 1.557 [0.000, 3.000],  loss: 7.007973, mse: 6153.699036, mean_q: 58.868987, mean_eps: 0.575940
 128659/300000: episode: 1286, duration: 0.690s, episode steps:  98, steps per second: 142, episode reward: -87.364, mean reward: -0.891 [-100.000, 17.048], mean action: 1.653 [0.000, 3.000],  loss: 8.263993, mse: 6090.729726, mean_q: 58.160015, mean_eps: 0.575589
 128779/300000: episode: 1287, duration: 0.847s, episode steps: 120, steps per second: 142, episode reward: -27.756, mean reward: -0.231 [-100.000, 11.999], mean action: 1.458 [0.000, 3.000],  loss: 6.631276, mse: 6174.362390, mean_q: 59.072132, mean_eps: 0.575229
 128907/300000: episode: 1288, duration: 0.896s, episode steps: 128, steps per second: 143, episode reward: -39.588, mean reward: -0.309 [-100.000, 18.750], mean action: 1.547 [0.000, 3.000],  loss: 7.733065, mse: 6099.850296, mean_q: 57.448727, mean_eps: 0.574820
 129003/300000: episode: 1289, duration: 0.706s, episode steps:  96, steps per second: 136, episode reward: -40.887, mean reward: -0.426 [-100.000,  8.023], mean action: 1.656 [0.000, 3.000],  loss: 5.553297, mse: 5996.538447, mean_q: 57.237444, mean_eps: 0.574450
 129098/300000: episode: 1290, duration: 0.676s, episode steps:  95, steps per second: 141, episode reward: -28.857, mean reward: -0.304 [-100.000, 12.550], mean action: 1.579 [0.000, 3.000],  loss: 6.501640, mse: 6094.615558, mean_q: 56.469043, mean_eps: 0.574135
 129210/300000: episode: 1291, duration: 0.789s, episode steps: 112, steps per second: 142, episode reward: -53.431, mean reward: -0.477 [-100.000, 12.875], mean action: 1.643 [0.000, 3.000],  loss: 6.741908, mse: 6190.780095, mean_q: 58.885525, mean_eps: 0.573793
 129279/300000: episode: 1292, duration: 0.520s, episode steps:  69, steps per second: 133, episode reward: -21.422, mean reward: -0.310 [-100.000,  9.086], mean action: 1.725 [0.000, 3.000],  loss: 7.213155, mse: 6280.236286, mean_q: 57.705376, mean_eps: 0.573495
 129355/300000: episode: 1293, duration: 0.560s, episode steps:  76, steps per second: 136, episode reward: -35.076, mean reward: -0.462 [-100.000, 10.876], mean action: 1.605 [0.000, 3.000],  loss: 7.558154, mse: 6275.711927, mean_q: 58.325819, mean_eps: 0.573256
 129441/300000: episode: 1294, duration: 0.593s, episode steps:  86, steps per second: 145, episode reward: -56.603, mean reward: -0.658 [-100.000, 16.495], mean action: 1.721 [0.000, 3.000],  loss: 7.659685, mse: 6067.293724, mean_q: 57.751180, mean_eps: 0.572988
 129521/300000: episode: 1295, duration: 0.553s, episode steps:  80, steps per second: 145, episode reward: -2.781, mean reward: -0.035 [-100.000, 14.798], mean action: 1.625 [0.000, 3.000],  loss: 9.340069, mse: 6101.229382, mean_q: 57.112946, mean_eps: 0.572714
 129628/300000: episode: 1296, duration: 0.787s, episode steps: 107, steps per second: 136, episode reward: -22.785, mean reward: -0.213 [-100.000,  8.914], mean action: 1.654 [0.000, 3.000],  loss: 6.548852, mse: 6236.774994, mean_q: 58.101190, mean_eps: 0.572406
 129728/300000: episode: 1297, duration: 0.699s, episode steps: 100, steps per second: 143, episode reward: -45.946, mean reward: -0.459 [-100.000, 17.315], mean action: 1.650 [0.000, 3.000],  loss: 7.720938, mse: 6129.687871, mean_q: 57.219222, mean_eps: 0.572064
 130148/300000: episode: 1298, duration: 3.181s, episode steps: 420, steps per second: 132, episode reward: -225.196, mean reward: -0.536 [-100.000, 43.035], mean action: 1.619 [0.000, 3.000],  loss: 7.784760, mse: 6134.537796, mean_q: 57.534459, mean_eps: 0.571206
 130248/300000: episode: 1299, duration: 0.719s, episode steps: 100, steps per second: 139, episode reward: -46.240, mean reward: -0.462 [-100.000, 11.611], mean action: 1.540 [0.000, 3.000],  loss: 7.735628, mse: 6208.647578, mean_q: 56.736655, mean_eps: 0.570348
 130368/300000: episode: 1300, duration: 0.837s, episode steps: 120, steps per second: 143, episode reward: -99.494, mean reward: -0.829 [-100.000,  8.843], mean action: 1.667 [0.000, 3.000],  loss: 8.624535, mse: 6162.735478, mean_q: 56.815744, mean_eps: 0.569985
 130455/300000: episode: 1301, duration: 0.640s, episode steps:  87, steps per second: 136, episode reward: -58.476, mean reward: -0.672 [-100.000, 11.535], mean action: 1.701 [0.000, 3.000],  loss: 5.913039, mse: 6218.745173, mean_q: 57.703927, mean_eps: 0.569644
 130559/300000: episode: 1302, duration: 0.751s, episode steps: 104, steps per second: 138, episode reward: -24.915, mean reward: -0.240 [-100.000, 18.551], mean action: 1.760 [0.000, 3.000],  loss: 7.314787, mse: 6174.182185, mean_q: 56.797403, mean_eps: 0.569329
 130658/300000: episode: 1303, duration: 0.682s, episode steps:  99, steps per second: 145, episode reward:  5.758, mean reward:  0.058 [-100.000, 14.904], mean action: 1.677 [0.000, 3.000],  loss: 8.428155, mse: 6081.002880, mean_q: 55.550470, mean_eps: 0.568994
 130766/300000: episode: 1304, duration: 0.763s, episode steps: 108, steps per second: 142, episode reward:  1.306, mean reward:  0.012 [-100.000, 13.807], mean action: 1.676 [0.000, 3.000],  loss: 9.912655, mse: 6231.150490, mean_q: 58.396461, mean_eps: 0.568652
 130885/300000: episode: 1305, duration: 0.834s, episode steps: 119, steps per second: 143, episode reward: -16.755, mean reward: -0.141 [-100.000, 11.336], mean action: 1.571 [0.000, 3.000],  loss: 8.443648, mse: 6107.629912, mean_q: 56.544932, mean_eps: 0.568277
 130977/300000: episode: 1306, duration: 0.628s, episode steps:  92, steps per second: 146, episode reward: -51.562, mean reward: -0.560 [-100.000, 15.820], mean action: 1.641 [0.000, 3.000],  loss: 6.186552, mse: 6179.338236, mean_q: 56.380308, mean_eps: 0.567929
 131137/300000: episode: 1307, duration: 1.297s, episode steps: 160, steps per second: 123, episode reward: -51.664, mean reward: -0.323 [-100.000,  7.822], mean action: 1.675 [0.000, 3.000],  loss: 8.011767, mse: 6394.334167, mean_q: 59.569500, mean_eps: 0.567514
 131256/300000: episode: 1308, duration: 0.921s, episode steps: 119, steps per second: 129, episode reward: 18.317, mean reward:  0.154 [-100.000, 19.757], mean action: 1.647 [0.000, 3.000],  loss: 7.382239, mse: 6281.515498, mean_q: 58.638191, mean_eps: 0.567053
 132103/300000: episode: 1309, duration: 6.813s, episode steps: 847, steps per second: 124, episode reward: -122.801, mean reward: -0.145 [-100.000, 22.099], mean action: 1.555 [0.000, 3.000],  loss: 8.603873, mse: 6207.214826, mean_q: 57.238911, mean_eps: 0.565459
 132202/300000: episode: 1310, duration: 0.773s, episode steps:  99, steps per second: 128, episode reward:  0.453, mean reward:  0.005 [-100.000, 32.140], mean action: 1.646 [0.000, 3.000],  loss: 8.129292, mse: 6112.702799, mean_q: 56.891746, mean_eps: 0.563898
 132340/300000: episode: 1311, duration: 1.021s, episode steps: 138, steps per second: 135, episode reward: -38.284, mean reward: -0.277 [-100.000, 10.224], mean action: 1.652 [0.000, 3.000],  loss: 7.293540, mse: 6139.644284, mean_q: 56.843287, mean_eps: 0.563507
 132440/300000: episode: 1312, duration: 0.853s, episode steps: 100, steps per second: 117, episode reward: -166.809, mean reward: -1.668 [-100.000, 52.397], mean action: 1.840 [0.000, 3.000],  loss: 9.895992, mse: 6029.496914, mean_q: 55.435380, mean_eps: 0.563115
 132536/300000: episode: 1313, duration: 0.827s, episode steps:  96, steps per second: 116, episode reward: -44.298, mean reward: -0.461 [-100.000, 16.529], mean action: 1.708 [0.000, 3.000],  loss: 7.878600, mse: 6079.336655, mean_q: 55.465097, mean_eps: 0.562791
 132635/300000: episode: 1314, duration: 0.723s, episode steps:  99, steps per second: 137, episode reward: -69.539, mean reward: -0.702 [-100.000, 18.713], mean action: 1.626 [0.000, 3.000],  loss: 10.347304, mse: 6034.456592, mean_q: 56.003130, mean_eps: 0.562469
 132715/300000: episode: 1315, duration: 0.631s, episode steps:  80, steps per second: 127, episode reward: -1.707, mean reward: -0.021 [-100.000, 12.257], mean action: 1.725 [0.000, 3.000],  loss: 9.365539, mse: 6150.763623, mean_q: 56.761607, mean_eps: 0.562174
 132819/300000: episode: 1316, duration: 0.787s, episode steps: 104, steps per second: 132, episode reward: -99.723, mean reward: -0.959 [-100.000, 17.007], mean action: 1.702 [0.000, 3.000],  loss: 9.842567, mse: 5992.452055, mean_q: 55.333654, mean_eps: 0.561871
 132911/300000: episode: 1317, duration: 0.660s, episode steps:  92, steps per second: 139, episode reward: -5.634, mean reward: -0.061 [-100.000, 17.751], mean action: 1.652 [0.000, 3.000],  loss: 7.179684, mse: 6192.889197, mean_q: 56.615574, mean_eps: 0.561547
 133020/300000: episode: 1318, duration: 0.803s, episode steps: 109, steps per second: 136, episode reward:  1.689, mean reward:  0.015 [-100.000,  7.035], mean action: 1.642 [0.000, 3.000],  loss: 9.318079, mse: 6141.745498, mean_q: 57.736793, mean_eps: 0.561215
 133111/300000: episode: 1319, duration: 0.646s, episode steps:  91, steps per second: 141, episode reward: -85.961, mean reward: -0.945 [-100.000,  9.854], mean action: 1.495 [0.000, 3.000],  loss: 8.046659, mse: 6064.595730, mean_q: 55.861514, mean_eps: 0.560886
 133208/300000: episode: 1320, duration: 0.708s, episode steps:  97, steps per second: 137, episode reward: -44.726, mean reward: -0.461 [-100.000,  7.018], mean action: 1.670 [0.000, 3.000],  loss: 7.195909, mse: 6200.534467, mean_q: 56.609897, mean_eps: 0.560575
 133293/300000: episode: 1321, duration: 0.674s, episode steps:  85, steps per second: 126, episode reward: -41.529, mean reward: -0.489 [-100.000, 12.895], mean action: 1.624 [0.000, 3.000],  loss: 6.284298, mse: 6148.484369, mean_q: 55.419593, mean_eps: 0.560275
 133377/300000: episode: 1322, duration: 0.670s, episode steps:  84, steps per second: 125, episode reward: -15.921, mean reward: -0.190 [-100.000, 21.787], mean action: 1.798 [0.000, 3.000],  loss: 8.137201, mse: 6196.393061, mean_q: 57.215814, mean_eps: 0.559996
 133526/300000: episode: 1323, duration: 1.090s, episode steps: 149, steps per second: 137, episode reward: 24.621, mean reward:  0.165 [-100.000, 13.394], mean action: 1.664 [0.000, 3.000],  loss: 7.026390, mse: 6161.901764, mean_q: 57.125979, mean_eps: 0.559612
 133621/300000: episode: 1324, duration: 0.779s, episode steps:  95, steps per second: 122, episode reward: -41.788, mean reward: -0.440 [-100.000, 11.894], mean action: 1.632 [0.000, 3.000],  loss: 6.829016, mse: 6125.185413, mean_q: 56.567851, mean_eps: 0.559209
 133754/300000: episode: 1325, duration: 0.990s, episode steps: 133, steps per second: 134, episode reward: -5.808, mean reward: -0.044 [-100.000, 18.971], mean action: 1.647 [0.000, 3.000],  loss: 7.187720, mse: 6246.868975, mean_q: 57.862348, mean_eps: 0.558833
 133850/300000: episode: 1326, duration: 0.784s, episode steps:  96, steps per second: 122, episode reward: -49.637, mean reward: -0.517 [-100.000, 11.241], mean action: 1.771 [0.000, 3.000],  loss: 7.669371, mse: 6153.268778, mean_q: 56.895066, mean_eps: 0.558455
 133923/300000: episode: 1327, duration: 0.640s, episode steps:  73, steps per second: 114, episode reward: -19.128, mean reward: -0.262 [-100.000,  7.646], mean action: 1.904 [0.000, 3.000],  loss: 7.134264, mse: 6110.596713, mean_q: 56.237540, mean_eps: 0.558176
 134052/300000: episode: 1328, duration: 1.041s, episode steps: 129, steps per second: 124, episode reward: -285.879, mean reward: -2.216 [-100.000, 76.927], mean action: 1.930 [0.000, 3.000],  loss: 7.113351, mse: 6189.613361, mean_q: 57.130278, mean_eps: 0.557843
 134235/300000: episode: 1329, duration: 1.508s, episode steps: 183, steps per second: 121, episode reward: 38.120, mean reward:  0.208 [-100.000, 15.922], mean action: 1.716 [0.000, 3.000],  loss: 7.430431, mse: 6208.047075, mean_q: 57.113541, mean_eps: 0.557328
 134425/300000: episode: 1330, duration: 1.608s, episode steps: 190, steps per second: 118, episode reward: 25.733, mean reward:  0.135 [-100.000, 15.786], mean action: 1.700 [0.000, 3.000],  loss: 8.039359, mse: 6266.558387, mean_q: 57.072727, mean_eps: 0.556713
 134551/300000: episode: 1331, duration: 0.920s, episode steps: 126, steps per second: 137, episode reward: -20.430, mean reward: -0.162 [-100.000, 17.932], mean action: 1.659 [0.000, 3.000],  loss: 7.416414, mse: 6173.890799, mean_q: 56.354451, mean_eps: 0.556191
 134650/300000: episode: 1332, duration: 0.852s, episode steps:  99, steps per second: 116, episode reward:  7.019, mean reward:  0.071 [-100.000, 18.520], mean action: 1.859 [0.000, 3.000],  loss: 8.355738, mse: 6294.942999, mean_q: 58.569678, mean_eps: 0.555820
 134747/300000: episode: 1333, duration: 0.716s, episode steps:  97, steps per second: 135, episode reward: -26.046, mean reward: -0.269 [-100.000, 10.259], mean action: 1.701 [0.000, 3.000],  loss: 7.141775, mse: 6320.928278, mean_q: 56.560092, mean_eps: 0.555497
 134888/300000: episode: 1334, duration: 0.999s, episode steps: 141, steps per second: 141, episode reward: -34.283, mean reward: -0.243 [-100.000, 12.726], mean action: 1.560 [0.000, 3.000],  loss: 8.104719, mse: 6148.858253, mean_q: 56.356365, mean_eps: 0.555104
 134964/300000: episode: 1335, duration: 0.572s, episode steps:  76, steps per second: 133, episode reward: -42.257, mean reward: -0.556 [-100.000,  8.326], mean action: 1.658 [0.000, 3.000],  loss: 7.035287, mse: 6399.507318, mean_q: 59.153610, mean_eps: 0.554746
 135189/300000: episode: 1336, duration: 1.588s, episode steps: 225, steps per second: 142, episode reward: -47.356, mean reward: -0.210 [-100.000, 20.467], mean action: 1.547 [0.000, 3.000],  loss: 6.893536, mse: 6253.765458, mean_q: 57.130454, mean_eps: 0.554249
 135296/300000: episode: 1337, duration: 0.901s, episode steps: 107, steps per second: 119, episode reward: -9.694, mean reward: -0.091 [-100.000, 18.454], mean action: 1.710 [0.000, 3.000],  loss: 7.854538, mse: 6255.325994, mean_q: 56.921591, mean_eps: 0.553701
 135388/300000: episode: 1338, duration: 0.689s, episode steps:  92, steps per second: 134, episode reward: -44.158, mean reward: -0.480 [-100.000, 16.023], mean action: 1.576 [0.000, 3.000],  loss: 8.202945, mse: 6190.973718, mean_q: 56.351627, mean_eps: 0.553373
 135497/300000: episode: 1339, duration: 0.761s, episode steps: 109, steps per second: 143, episode reward: -134.184, mean reward: -1.231 [-100.000, 13.048], mean action: 1.716 [0.000, 3.000],  loss: 8.396952, mse: 6286.568498, mean_q: 59.246463, mean_eps: 0.553041
 135910/300000: episode: 1340, duration: 3.424s, episode steps: 413, steps per second: 121, episode reward: -171.112, mean reward: -0.414 [-100.000, 15.576], mean action: 1.622 [0.000, 3.000],  loss: 7.361527, mse: 6245.575465, mean_q: 57.076275, mean_eps: 0.552180
 136000/300000: episode: 1341, duration: 0.682s, episode steps:  90, steps per second: 132, episode reward: -87.075, mean reward: -0.968 [-100.000,  9.712], mean action: 1.700 [0.000, 3.000],  loss: 8.264739, mse: 6169.914567, mean_q: 55.736191, mean_eps: 0.551350
 136119/300000: episode: 1342, duration: 1.083s, episode steps: 119, steps per second: 110, episode reward: -86.160, mean reward: -0.724 [-100.000,  7.131], mean action: 1.504 [0.000, 3.000],  loss: 8.360368, mse: 6306.851398, mean_q: 58.438749, mean_eps: 0.551005
 136644/300000: episode: 1343, duration: 4.957s, episode steps: 525, steps per second: 106, episode reward: -279.843, mean reward: -0.533 [-100.000, 33.538], mean action: 1.798 [0.000, 3.000],  loss: 9.133580, mse: 6182.757241, mean_q: 56.707932, mean_eps: 0.549943
 136747/300000: episode: 1344, duration: 0.743s, episode steps: 103, steps per second: 139, episode reward: -98.445, mean reward: -0.956 [-100.000, 11.802], mean action: 1.631 [0.000, 3.000],  loss: 9.130203, mse: 6106.419040, mean_q: 56.667701, mean_eps: 0.548906
 136884/300000: episode: 1345, duration: 0.995s, episode steps: 137, steps per second: 138, episode reward:  2.282, mean reward:  0.017 [-100.000, 13.884], mean action: 1.474 [0.000, 3.000],  loss: 9.994886, mse: 6098.352799, mean_q: 56.290609, mean_eps: 0.548511
 136975/300000: episode: 1346, duration: 0.643s, episode steps:  91, steps per second: 142, episode reward: -52.902, mean reward: -0.581 [-100.000,  7.429], mean action: 1.692 [0.000, 3.000],  loss: 8.990637, mse: 6320.124115, mean_q: 58.553163, mean_eps: 0.548134
 137623/300000: episode: 1347, duration: 5.102s, episode steps: 648, steps per second: 127, episode reward: -124.822, mean reward: -0.193 [-100.000, 43.993], mean action: 1.710 [0.000, 3.000],  loss: 9.305190, mse: 6200.987757, mean_q: 56.809025, mean_eps: 0.546915
 137757/300000: episode: 1348, duration: 1.048s, episode steps: 134, steps per second: 128, episode reward:  8.436, mean reward:  0.063 [-100.000, 13.240], mean action: 1.597 [0.000, 3.000],  loss: 10.315746, mse: 6185.319514, mean_q: 55.587783, mean_eps: 0.545625
 137882/300000: episode: 1349, duration: 0.897s, episode steps: 125, steps per second: 139, episode reward: -12.921, mean reward: -0.103 [-100.000, 17.498], mean action: 1.752 [0.000, 3.000],  loss: 9.172547, mse: 6106.910543, mean_q: 55.346942, mean_eps: 0.545197
 138011/300000: episode: 1350, duration: 0.947s, episode steps: 129, steps per second: 136, episode reward: -51.050, mean reward: -0.396 [-100.000,  6.563], mean action: 1.659 [0.000, 3.000],  loss: 10.196420, mse: 6096.593856, mean_q: 55.796658, mean_eps: 0.544778
 138125/300000: episode: 1351, duration: 0.793s, episode steps: 114, steps per second: 144, episode reward: -41.051, mean reward: -0.360 [-100.000, 24.019], mean action: 1.763 [0.000, 3.000],  loss: 8.195137, mse: 6316.657801, mean_q: 56.853588, mean_eps: 0.544377
 138238/300000: episode: 1352, duration: 0.829s, episode steps: 113, steps per second: 136, episode reward: 22.737, mean reward:  0.201 [-100.000, 13.510], mean action: 1.655 [0.000, 3.000],  loss: 10.250395, mse: 6400.232197, mean_q: 57.522437, mean_eps: 0.544003
 138384/300000: episode: 1353, duration: 1.018s, episode steps: 146, steps per second: 143, episode reward: -157.655, mean reward: -1.080 [-100.000, 33.901], mean action: 1.829 [0.000, 3.000],  loss: 8.747779, mse: 6261.275588, mean_q: 57.455105, mean_eps: 0.543575
 138469/300000: episode: 1354, duration: 0.585s, episode steps:  85, steps per second: 145, episode reward: 14.930, mean reward:  0.176 [-100.000, 13.784], mean action: 1.671 [0.000, 3.000],  loss: 9.461216, mse: 6291.995634, mean_q: 56.714310, mean_eps: 0.543194
 138553/300000: episode: 1355, duration: 0.621s, episode steps:  84, steps per second: 135, episode reward: -124.566, mean reward: -1.483 [-100.000,  4.924], mean action: 1.810 [0.000, 3.000],  loss: 10.886794, mse: 6295.023966, mean_q: 57.036191, mean_eps: 0.542915
 138678/300000: episode: 1356, duration: 0.875s, episode steps: 125, steps per second: 143, episode reward:  9.558, mean reward:  0.076 [-100.000, 15.121], mean action: 1.792 [0.000, 3.000],  loss: 10.458254, mse: 6259.513152, mean_q: 55.861705, mean_eps: 0.542571
 139547/300000: episode: 1357, duration: 6.793s, episode steps: 869, steps per second: 128, episode reward: -332.104, mean reward: -0.382 [-100.000, 16.759], mean action: 1.663 [0.000, 3.000],  loss: 10.664458, mse: 6364.633805, mean_q: 57.978016, mean_eps: 0.540930
 139786/300000: episode: 1358, duration: 1.818s, episode steps: 239, steps per second: 131, episode reward: -237.957, mean reward: -0.996 [-100.000, 13.990], mean action: 1.682 [0.000, 3.000],  loss: 8.251195, mse: 6357.973888, mean_q: 58.071102, mean_eps: 0.539102
 139899/300000: episode: 1359, duration: 0.791s, episode steps: 113, steps per second: 143, episode reward: -29.928, mean reward: -0.265 [-100.000, 29.581], mean action: 1.708 [0.000, 3.000],  loss: 9.458636, mse: 6445.619357, mean_q: 59.238298, mean_eps: 0.538521
 139983/300000: episode: 1360, duration: 0.636s, episode steps:  84, steps per second: 132, episode reward: -70.195, mean reward: -0.836 [-100.000,  9.624], mean action: 1.583 [0.000, 3.000],  loss: 8.712041, mse: 6389.906616, mean_q: 58.321427, mean_eps: 0.538196
 140573/300000: episode: 1361, duration: 4.801s, episode steps: 590, steps per second: 123, episode reward: -167.358, mean reward: -0.284 [-100.000, 13.483], mean action: 1.756 [0.000, 3.000],  loss: 9.897855, mse: 6409.285523, mean_q: 57.979250, mean_eps: 0.537084
 140678/300000: episode: 1362, duration: 0.785s, episode steps: 105, steps per second: 134, episode reward: -33.210, mean reward: -0.316 [-100.000,  9.315], mean action: 1.629 [0.000, 3.000],  loss: 10.980945, mse: 6418.514509, mean_q: 57.743795, mean_eps: 0.535937
 140805/300000: episode: 1363, duration: 1.008s, episode steps: 127, steps per second: 126, episode reward: -48.446, mean reward: -0.381 [-100.000,  9.879], mean action: 1.528 [0.000, 3.000],  loss: 8.855703, mse: 6325.650883, mean_q: 56.814494, mean_eps: 0.535555
 140879/300000: episode: 1364, duration: 0.546s, episode steps:  74, steps per second: 136, episode reward: -59.577, mean reward: -0.805 [-100.000, 10.466], mean action: 1.514 [0.000, 3.000],  loss: 8.030328, mse: 6416.654086, mean_q: 58.424662, mean_eps: 0.535223
 141005/300000: episode: 1365, duration: 0.916s, episode steps: 126, steps per second: 138, episode reward: -2.685, mean reward: -0.021 [-100.000,  8.380], mean action: 1.635 [0.000, 3.000],  loss: 11.423919, mse: 6410.194677, mean_q: 57.907048, mean_eps: 0.534893
 141129/300000: episode: 1366, duration: 0.892s, episode steps: 124, steps per second: 139, episode reward: -21.790, mean reward: -0.176 [-100.000, 15.789], mean action: 1.758 [0.000, 3.000],  loss: 9.948545, mse: 6417.895291, mean_q: 58.460266, mean_eps: 0.534481
 141758/300000: episode: 1367, duration: 4.764s, episode steps: 629, steps per second: 132, episode reward: -89.547, mean reward: -0.142 [-100.000, 25.763], mean action: 1.666 [0.000, 3.000],  loss: 9.771866, mse: 6391.150433, mean_q: 58.028089, mean_eps: 0.533238
 141887/300000: episode: 1368, duration: 0.914s, episode steps: 129, steps per second: 141, episode reward: -196.499, mean reward: -1.523 [-100.000, 47.785], mean action: 1.597 [0.000, 3.000],  loss: 9.601542, mse: 6318.125322, mean_q: 56.714695, mean_eps: 0.531987
 142020/300000: episode: 1369, duration: 0.942s, episode steps: 133, steps per second: 141, episode reward: -29.705, mean reward: -0.223 [-100.000, 12.357], mean action: 1.654 [0.000, 3.000],  loss: 10.629887, mse: 6391.623605, mean_q: 58.063694, mean_eps: 0.531555
 142139/300000: episode: 1370, duration: 0.830s, episode steps: 119, steps per second: 143, episode reward: -34.429, mean reward: -0.289 [-100.000, 13.111], mean action: 1.765 [0.000, 3.000],  loss: 9.375894, mse: 6471.919811, mean_q: 59.414526, mean_eps: 0.531139
 142239/300000: episode: 1371, duration: 0.769s, episode steps: 100, steps per second: 130, episode reward:  2.856, mean reward:  0.029 [-100.000, 21.837], mean action: 1.720 [0.000, 3.000],  loss: 9.157184, mse: 6323.447534, mean_q: 56.944236, mean_eps: 0.530778
 142353/300000: episode: 1372, duration: 1.001s, episode steps: 114, steps per second: 114, episode reward: -11.034, mean reward: -0.097 [-100.000,  9.216], mean action: 1.711 [0.000, 3.000],  loss: 11.724510, mse: 6384.925160, mean_q: 58.561999, mean_eps: 0.530425
 142492/300000: episode: 1373, duration: 1.196s, episode steps: 139, steps per second: 116, episode reward: -45.688, mean reward: -0.329 [-100.000, 12.669], mean action: 1.496 [0.000, 3.000],  loss: 9.976156, mse: 6490.289523, mean_q: 60.176876, mean_eps: 0.530007
 142590/300000: episode: 1374, duration: 0.992s, episode steps:  98, steps per second:  99, episode reward: 37.440, mean reward:  0.382 [-100.000, 12.428], mean action: 1.735 [0.000, 3.000],  loss: 7.113285, mse: 6546.475466, mean_q: 59.869025, mean_eps: 0.529616
 142703/300000: episode: 1375, duration: 0.978s, episode steps: 113, steps per second: 116, episode reward: -42.534, mean reward: -0.376 [-100.000,  7.674], mean action: 1.637 [0.000, 3.000],  loss: 11.680419, mse: 6569.464420, mean_q: 60.008793, mean_eps: 0.529268
 142782/300000: episode: 1376, duration: 0.722s, episode steps:  79, steps per second: 109, episode reward: -41.212, mean reward: -0.522 [-100.000,  7.503], mean action: 1.797 [0.000, 3.000],  loss: 9.526034, mse: 6580.545973, mean_q: 59.052569, mean_eps: 0.528951
 142894/300000: episode: 1377, duration: 0.880s, episode steps: 112, steps per second: 127, episode reward: -28.636, mean reward: -0.256 [-100.000, 17.320], mean action: 1.625 [0.000, 3.000],  loss: 8.921555, mse: 6516.201433, mean_q: 60.101566, mean_eps: 0.528636
 142980/300000: episode: 1378, duration: 0.670s, episode steps:  86, steps per second: 128, episode reward: -19.342, mean reward: -0.225 [-100.000, 12.465], mean action: 1.837 [0.000, 3.000],  loss: 9.841673, mse: 6507.940804, mean_q: 60.263476, mean_eps: 0.528310
 143080/300000: episode: 1379, duration: 0.747s, episode steps: 100, steps per second: 134, episode reward: -11.678, mean reward: -0.117 [-100.000, 17.478], mean action: 1.650 [0.000, 3.000],  loss: 12.507154, mse: 6570.229907, mean_q: 60.097751, mean_eps: 0.528003
 143246/300000: episode: 1380, duration: 1.186s, episode steps: 166, steps per second: 140, episode reward: 14.139, mean reward:  0.085 [-100.000, 12.418], mean action: 1.669 [0.000, 3.000],  loss: 10.655845, mse: 6624.523955, mean_q: 61.228617, mean_eps: 0.527564
 143382/300000: episode: 1381, duration: 0.975s, episode steps: 136, steps per second: 140, episode reward: -68.613, mean reward: -0.505 [-100.000, 12.098], mean action: 1.743 [0.000, 3.000],  loss: 10.035691, mse: 6423.230332, mean_q: 59.656919, mean_eps: 0.527065
 143520/300000: episode: 1382, duration: 0.971s, episode steps: 138, steps per second: 142, episode reward: -40.891, mean reward: -0.296 [-100.000,  8.822], mean action: 1.543 [0.000, 3.000],  loss: 10.762021, mse: 6598.442224, mean_q: 60.872085, mean_eps: 0.526613
 143611/300000: episode: 1383, duration: 0.691s, episode steps:  91, steps per second: 132, episode reward:  4.500, mean reward:  0.049 [-100.000, 14.994], mean action: 1.813 [0.000, 3.000],  loss: 10.504602, mse: 6687.034829, mean_q: 61.011260, mean_eps: 0.526235
 144611/300000: episode: 1384, duration: 8.246s, episode steps: 1000, steps per second: 121, episode reward:  5.470, mean reward:  0.005 [-19.554, 17.151], mean action: 1.627 [0.000, 3.000],  loss: 9.552260, mse: 6672.236081, mean_q: 61.493233, mean_eps: 0.524435
 145162/300000: episode: 1385, duration: 4.508s, episode steps: 551, steps per second: 122, episode reward: -209.289, mean reward: -0.380 [-100.000, 23.786], mean action: 1.590 [0.000, 3.000],  loss: 10.077960, mse: 6610.305122, mean_q: 60.819600, mean_eps: 0.521876
 145305/300000: episode: 1386, duration: 1.129s, episode steps: 143, steps per second: 127, episode reward: -41.760, mean reward: -0.292 [-100.000, 14.224], mean action: 1.790 [0.000, 3.000],  loss: 12.605533, mse: 6425.989370, mean_q: 60.035437, mean_eps: 0.520731
 146305/300000: episode: 1387, duration: 9.438s, episode steps: 1000, steps per second: 106, episode reward: -66.144, mean reward: -0.066 [-17.833, 16.417], mean action: 1.662 [0.000, 3.000],  loss: 11.060957, mse: 6580.491471, mean_q: 61.149775, mean_eps: 0.518845
 147305/300000: episode: 1388, duration: 8.104s, episode steps: 1000, steps per second: 123, episode reward: -21.062, mean reward: -0.021 [-19.657, 20.423], mean action: 1.735 [0.000, 3.000],  loss: 11.567625, mse: 6460.807962, mean_q: 60.338688, mean_eps: 0.515545
 147464/300000: episode: 1389, duration: 1.185s, episode steps: 159, steps per second: 134, episode reward: 12.744, mean reward:  0.080 [-100.000, 21.283], mean action: 1.673 [0.000, 3.000],  loss: 10.986714, mse: 6513.084651, mean_q: 60.690557, mean_eps: 0.513633
 147539/300000: episode: 1390, duration: 0.542s, episode steps:  75, steps per second: 138, episode reward: -43.939, mean reward: -0.586 [-100.000, 11.945], mean action: 1.733 [0.000, 3.000],  loss: 8.242519, mse: 6507.611452, mean_q: 61.264537, mean_eps: 0.513247
 147693/300000: episode: 1391, duration: 1.141s, episode steps: 154, steps per second: 135, episode reward: 27.257, mean reward:  0.177 [-100.000, 14.315], mean action: 1.812 [0.000, 3.000],  loss: 12.663283, mse: 6377.515219, mean_q: 60.632301, mean_eps: 0.512869
 147833/300000: episode: 1392, duration: 0.980s, episode steps: 140, steps per second: 143, episode reward: 26.470, mean reward:  0.189 [-100.000, 14.956], mean action: 1.664 [0.000, 3.000],  loss: 12.064106, mse: 6479.851824, mean_q: 61.309284, mean_eps: 0.512384
 147907/300000: episode: 1393, duration: 0.511s, episode steps:  74, steps per second: 145, episode reward: -64.986, mean reward: -0.878 [-100.000, 19.058], mean action: 1.635 [0.000, 3.000],  loss: 10.542952, mse: 6441.018852, mean_q: 61.091916, mean_eps: 0.512031
 148048/300000: episode: 1394, duration: 1.022s, episode steps: 141, steps per second: 138, episode reward: -3.297, mean reward: -0.023 [-100.000, 14.557], mean action: 1.702 [0.000, 3.000],  loss: 12.567276, mse: 6569.798748, mean_q: 62.461848, mean_eps: 0.511676
 148166/300000: episode: 1395, duration: 0.936s, episode steps: 118, steps per second: 126, episode reward: -107.775, mean reward: -0.913 [-100.000, 13.187], mean action: 1.686 [0.000, 3.000],  loss: 15.794670, mse: 6454.984768, mean_q: 61.835301, mean_eps: 0.511249
 148313/300000: episode: 1396, duration: 1.078s, episode steps: 147, steps per second: 136, episode reward: -33.341, mean reward: -0.227 [-100.000, 15.586], mean action: 1.673 [0.000, 3.000],  loss: 10.875285, mse: 6579.947747, mean_q: 62.453570, mean_eps: 0.510811
 148631/300000: episode: 1397, duration: 2.575s, episode steps: 318, steps per second: 123, episode reward: -187.736, mean reward: -0.590 [-100.000, 20.717], mean action: 1.720 [0.000, 3.000],  loss: 12.739883, mse: 6445.981716, mean_q: 61.559120, mean_eps: 0.510044
 148759/300000: episode: 1398, duration: 1.032s, episode steps: 128, steps per second: 124, episode reward: -22.610, mean reward: -0.177 [-100.000, 14.622], mean action: 1.539 [0.000, 3.000],  loss: 13.930142, mse: 6613.151951, mean_q: 62.365773, mean_eps: 0.509308
 148918/300000: episode: 1399, duration: 1.264s, episode steps: 159, steps per second: 126, episode reward: -12.050, mean reward: -0.076 [-100.000, 18.059], mean action: 1.597 [0.000, 3.000],  loss: 14.063775, mse: 6345.062095, mean_q: 59.848214, mean_eps: 0.508835
 149013/300000: episode: 1400, duration: 0.696s, episode steps:  95, steps per second: 137, episode reward: -19.903, mean reward: -0.210 [-100.000, 18.143], mean action: 1.779 [0.000, 3.000],  loss: 12.482253, mse: 6597.012921, mean_q: 61.525913, mean_eps: 0.508415
 149120/300000: episode: 1401, duration: 0.853s, episode steps: 107, steps per second: 125, episode reward: -79.350, mean reward: -0.742 [-100.000, 11.231], mean action: 1.692 [0.000, 3.000],  loss: 10.243989, mse: 6473.394960, mean_q: 60.345498, mean_eps: 0.508082
 150120/300000: episode: 1402, duration: 8.393s, episode steps: 1000, steps per second: 119, episode reward: -39.107, mean reward: -0.039 [-20.877, 15.728], mean action: 1.739 [0.000, 3.000],  loss: 11.811751, mse: 6492.400063, mean_q: 60.937142, mean_eps: 0.506256
 150250/300000: episode: 1403, duration: 0.989s, episode steps: 130, steps per second: 131, episode reward: -40.896, mean reward: -0.315 [-100.000,  7.688], mean action: 1.762 [0.000, 3.000],  loss: 11.338036, mse: 6426.733320, mean_q: 60.744891, mean_eps: 0.504391
 150440/300000: episode: 1404, duration: 1.757s, episode steps: 190, steps per second: 108, episode reward: -41.073, mean reward: -0.216 [-100.000, 16.676], mean action: 1.721 [0.000, 3.000],  loss: 13.894532, mse: 6466.691190, mean_q: 61.408619, mean_eps: 0.503863
 150561/300000: episode: 1405, duration: 0.974s, episode steps: 121, steps per second: 124, episode reward: -9.396, mean reward: -0.078 [-100.000,  6.980], mean action: 1.843 [0.000, 3.000],  loss: 11.260470, mse: 6535.066273, mean_q: 61.638843, mean_eps: 0.503350
 151561/300000: episode: 1406, duration: 9.108s, episode steps: 1000, steps per second: 110, episode reward: -15.770, mean reward: -0.016 [-18.195, 17.046], mean action: 1.733 [0.000, 3.000],  loss: 11.491123, mse: 6515.984697, mean_q: 62.374112, mean_eps: 0.501500
 151662/300000: episode: 1407, duration: 0.783s, episode steps: 101, steps per second: 129, episode reward: 25.649, mean reward:  0.254 [-100.000, 20.231], mean action: 1.762 [0.000, 3.000],  loss: 10.354188, mse: 6583.022688, mean_q: 63.225260, mean_eps: 0.499684
 151812/300000: episode: 1408, duration: 1.047s, episode steps: 150, steps per second: 143, episode reward:  3.446, mean reward:  0.023 [-100.000, 17.151], mean action: 1.700 [0.000, 3.000],  loss: 10.407827, mse: 6544.897020, mean_q: 64.040245, mean_eps: 0.499270
 152555/300000: episode: 1409, duration: 5.531s, episode steps: 743, steps per second: 134, episode reward: -221.355, mean reward: -0.298 [-100.000, 20.720], mean action: 1.824 [0.000, 3.000],  loss: 11.171912, mse: 6508.758723, mean_q: 63.214016, mean_eps: 0.497796
 152714/300000: episode: 1410, duration: 1.089s, episode steps: 159, steps per second: 146, episode reward: -12.702, mean reward: -0.080 [-100.000, 16.313], mean action: 1.629 [0.000, 3.000],  loss: 13.020883, mse: 6482.482569, mean_q: 63.280276, mean_eps: 0.496308
 152834/300000: episode: 1411, duration: 0.887s, episode steps: 120, steps per second: 135, episode reward: -18.719, mean reward: -0.156 [-100.000, 20.988], mean action: 1.775 [0.000, 3.000],  loss: 12.517964, mse: 6650.105082, mean_q: 64.765615, mean_eps: 0.495847
 152975/300000: episode: 1412, duration: 0.972s, episode steps: 141, steps per second: 145, episode reward: 19.019, mean reward:  0.135 [-100.000, 20.303], mean action: 1.645 [0.000, 3.000],  loss: 12.767278, mse: 6590.330508, mean_q: 65.041987, mean_eps: 0.495417
 153157/300000: episode: 1413, duration: 1.334s, episode steps: 182, steps per second: 136, episode reward:  7.007, mean reward:  0.039 [-100.000, 12.953], mean action: 1.725 [0.000, 3.000],  loss: 12.052090, mse: 6514.874356, mean_q: 64.208673, mean_eps: 0.494884
 154157/300000: episode: 1414, duration: 8.765s, episode steps: 1000, steps per second: 114, episode reward: -32.791, mean reward: -0.033 [-24.066, 17.800], mean action: 1.670 [0.000, 3.000],  loss: 11.772960, mse: 6398.607081, mean_q: 63.354084, mean_eps: 0.492934
 154244/300000: episode: 1415, duration: 0.639s, episode steps:  87, steps per second: 136, episode reward: -52.554, mean reward: -0.604 [-100.000,  9.797], mean action: 1.747 [0.000, 3.000],  loss: 10.290263, mse: 6134.882770, mean_q: 62.378670, mean_eps: 0.491140
 154372/300000: episode: 1416, duration: 1.020s, episode steps: 128, steps per second: 126, episode reward: -67.757, mean reward: -0.529 [-100.000, 17.693], mean action: 1.711 [0.000, 3.000],  loss: 12.165010, mse: 6245.107140, mean_q: 62.948215, mean_eps: 0.490785
 154496/300000: episode: 1417, duration: 0.934s, episode steps: 124, steps per second: 133, episode reward: -59.007, mean reward: -0.476 [-100.000, 11.721], mean action: 1.766 [0.000, 3.000],  loss: 15.072368, mse: 6315.996027, mean_q: 63.346370, mean_eps: 0.490369
 154654/300000: episode: 1418, duration: 1.247s, episode steps: 158, steps per second: 127, episode reward: -28.644, mean reward: -0.181 [-100.000,  9.754], mean action: 1.551 [0.000, 3.000],  loss: 10.375865, mse: 6261.416108, mean_q: 62.139581, mean_eps: 0.489904
 155654/300000: episode: 1419, duration: 8.624s, episode steps: 1000, steps per second: 116, episode reward: -36.969, mean reward: -0.037 [-21.195, 18.668], mean action: 1.786 [0.000, 3.000],  loss: 11.513853, mse: 6281.741010, mean_q: 63.816843, mean_eps: 0.487993
 155766/300000: episode: 1420, duration: 0.858s, episode steps: 112, steps per second: 131, episode reward: -72.917, mean reward: -0.651 [-100.000,  6.448], mean action: 1.795 [0.000, 3.000],  loss: 9.570536, mse: 6223.763393, mean_q: 64.041786, mean_eps: 0.486159
 155900/300000: episode: 1421, duration: 0.976s, episode steps: 134, steps per second: 137, episode reward: -2.139, mean reward: -0.016 [-100.000, 13.667], mean action: 1.694 [0.000, 3.000],  loss: 10.505098, mse: 6235.236959, mean_q: 63.170086, mean_eps: 0.485753
 156042/300000: episode: 1422, duration: 1.077s, episode steps: 142, steps per second: 132, episode reward:  3.407, mean reward:  0.024 [-100.000, 10.111], mean action: 1.676 [0.000, 3.000],  loss: 11.395964, mse: 6329.071691, mean_q: 64.714451, mean_eps: 0.485297
 156688/300000: episode: 1423, duration: 5.322s, episode steps: 646, steps per second: 121, episode reward: -169.892, mean reward: -0.263 [-100.000, 39.368], mean action: 1.650 [0.000, 3.000],  loss: 12.227919, mse: 6214.113729, mean_q: 63.808210, mean_eps: 0.483997
 156788/300000: episode: 1424, duration: 0.913s, episode steps: 100, steps per second: 110, episode reward: 39.874, mean reward:  0.399 [-100.000, 20.563], mean action: 1.820 [0.000, 3.000],  loss: 14.121845, mse: 6332.924585, mean_q: 65.134311, mean_eps: 0.482766
 156915/300000: episode: 1425, duration: 1.055s, episode steps: 127, steps per second: 120, episode reward: -48.211, mean reward: -0.380 [-100.000, 12.528], mean action: 1.488 [0.000, 3.000],  loss: 11.620575, mse: 6200.782678, mean_q: 63.925361, mean_eps: 0.482392
 157915/300000: episode: 1426, duration: 8.182s, episode steps: 1000, steps per second: 122, episode reward:  2.729, mean reward:  0.003 [-19.481, 13.894], mean action: 1.753 [0.000, 3.000],  loss: 11.359882, mse: 6319.368140, mean_q: 65.280042, mean_eps: 0.480532
 158915/300000: episode: 1427, duration: 9.538s, episode steps: 1000, steps per second: 105, episode reward: -39.387, mean reward: -0.039 [-19.902, 21.468], mean action: 1.693 [0.000, 3.000],  loss: 12.239812, mse: 6276.445401, mean_q: 64.817892, mean_eps: 0.477232
 159040/300000: episode: 1428, duration: 0.942s, episode steps: 125, steps per second: 133, episode reward: -25.884, mean reward: -0.207 [-100.000, 14.974], mean action: 1.840 [0.000, 3.000],  loss: 12.137373, mse: 6211.448801, mean_q: 64.133546, mean_eps: 0.475376
 159155/300000: episode: 1429, duration: 0.809s, episode steps: 115, steps per second: 142, episode reward: -93.555, mean reward: -0.814 [-100.000, 12.923], mean action: 1.478 [0.000, 3.000],  loss: 13.311043, mse: 6195.795037, mean_q: 64.388331, mean_eps: 0.474980
 159302/300000: episode: 1430, duration: 1.090s, episode steps: 147, steps per second: 135, episode reward: 37.197, mean reward:  0.253 [-100.000, 12.851], mean action: 1.680 [0.000, 3.000],  loss: 12.884894, mse: 6289.976041, mean_q: 64.611404, mean_eps: 0.474548
 159498/300000: episode: 1431, duration: 1.409s, episode steps: 196, steps per second: 139, episode reward: 43.074, mean reward:  0.220 [-100.000, 19.009], mean action: 1.694 [0.000, 3.000],  loss: 11.634124, mse: 6326.410552, mean_q: 65.762169, mean_eps: 0.473982
 159623/300000: episode: 1432, duration: 0.937s, episode steps: 125, steps per second: 133, episode reward: -21.387, mean reward: -0.171 [-100.000,  8.302], mean action: 1.608 [0.000, 3.000],  loss: 15.215966, mse: 6269.017855, mean_q: 64.543316, mean_eps: 0.473452
 159749/300000: episode: 1433, duration: 0.911s, episode steps: 126, steps per second: 138, episode reward: 22.654, mean reward:  0.180 [-100.000, 21.615], mean action: 1.833 [0.000, 3.000],  loss: 10.939741, mse: 6360.379402, mean_q: 66.019102, mean_eps: 0.473038
 160749/300000: episode: 1434, duration: 8.178s, episode steps: 1000, steps per second: 122, episode reward: 11.633, mean reward:  0.012 [-22.576, 28.795], mean action: 1.702 [0.000, 3.000],  loss: 13.062991, mse: 6294.805641, mean_q: 65.611184, mean_eps: 0.471180
 161749/300000: episode: 1435, duration: 8.068s, episode steps: 1000, steps per second: 124, episode reward: 51.120, mean reward:  0.051 [-20.817, 18.568], mean action: 1.741 [0.000, 3.000],  loss: 12.363903, mse: 6272.563557, mean_q: 66.593310, mean_eps: 0.467880
 162749/300000: episode: 1436, duration: 9.096s, episode steps: 1000, steps per second: 110, episode reward:  9.652, mean reward:  0.010 [-22.417, 20.809], mean action: 1.701 [0.000, 3.000],  loss: 12.145004, mse: 6207.643780, mean_q: 66.878876, mean_eps: 0.464580
 163749/300000: episode: 1437, duration: 8.884s, episode steps: 1000, steps per second: 113, episode reward: 68.006, mean reward:  0.068 [-22.237, 27.466], mean action: 1.799 [0.000, 3.000],  loss: 12.413931, mse: 6142.669963, mean_q: 66.795338, mean_eps: 0.461280
 164749/300000: episode: 1438, duration: 9.195s, episode steps: 1000, steps per second: 109, episode reward:  2.899, mean reward:  0.003 [-24.281, 28.443], mean action: 1.663 [0.000, 3.000],  loss: 12.419283, mse: 6078.138272, mean_q: 67.193897, mean_eps: 0.457980
 164830/300000: episode: 1439, duration: 1.020s, episode steps:  81, steps per second:  79, episode reward: -27.338, mean reward: -0.338 [-100.000, 11.089], mean action: 1.827 [0.000, 3.000],  loss: 12.618763, mse: 6139.300576, mean_q: 67.562361, mean_eps: 0.456196
 164963/300000: episode: 1440, duration: 1.040s, episode steps: 133, steps per second: 128, episode reward: -0.204, mean reward: -0.002 [-100.000,  7.153], mean action: 1.857 [0.000, 3.000],  loss: 10.150244, mse: 6057.593465, mean_q: 67.291867, mean_eps: 0.455843
 165057/300000: episode: 1441, duration: 0.723s, episode steps:  94, steps per second: 130, episode reward: -27.917, mean reward: -0.297 [-100.000,  6.923], mean action: 1.777 [0.000, 3.000],  loss: 13.674474, mse: 5934.114320, mean_q: 67.175100, mean_eps: 0.455469
 165161/300000: episode: 1442, duration: 0.750s, episode steps: 104, steps per second: 139, episode reward: -42.105, mean reward: -0.405 [-100.000,  8.148], mean action: 1.683 [0.000, 3.000],  loss: 12.697569, mse: 6050.706041, mean_q: 67.371648, mean_eps: 0.455142
 166161/300000: episode: 1443, duration: 9.375s, episode steps: 1000, steps per second: 107, episode reward: 18.075, mean reward:  0.018 [-23.883, 24.511], mean action: 1.717 [0.000, 3.000],  loss: 13.366471, mse: 6014.902510, mean_q: 67.579061, mean_eps: 0.453320
 166259/300000: episode: 1444, duration: 0.812s, episode steps:  98, steps per second: 121, episode reward: 13.108, mean reward:  0.134 [-100.000, 16.628], mean action: 1.816 [0.000, 3.000],  loss: 15.158517, mse: 5966.305300, mean_q: 68.615539, mean_eps: 0.451509
 167259/300000: episode: 1445, duration: 8.783s, episode steps: 1000, steps per second: 114, episode reward: 73.140, mean reward:  0.073 [-22.589, 27.412], mean action: 1.666 [0.000, 3.000],  loss: 13.045624, mse: 5791.811833, mean_q: 66.979533, mean_eps: 0.449697
 167461/300000: episode: 1446, duration: 2.366s, episode steps: 202, steps per second:  85, episode reward: -40.591, mean reward: -0.201 [-100.000, 15.487], mean action: 1.718 [0.000, 3.000],  loss: 10.552160, mse: 5657.253326, mean_q: 66.574424, mean_eps: 0.447714
 167552/300000: episode: 1447, duration: 1.221s, episode steps:  91, steps per second:  75, episode reward: -10.980, mean reward: -0.121 [-100.000, 16.880], mean action: 1.637 [0.000, 3.000],  loss: 12.839942, mse: 5690.965724, mean_q: 66.438770, mean_eps: 0.447230
 167813/300000: episode: 1448, duration: 1.947s, episode steps: 261, steps per second: 134, episode reward: -130.083, mean reward: -0.498 [-100.000, 17.376], mean action: 1.720 [0.000, 3.000],  loss: 12.019426, mse: 5787.345257, mean_q: 67.459848, mean_eps: 0.446649
 167926/300000: episode: 1449, duration: 1.337s, episode steps: 113, steps per second:  85, episode reward: -2.264, mean reward: -0.020 [-100.000, 11.678], mean action: 1.690 [0.000, 3.000],  loss: 15.947786, mse: 5730.867291, mean_q: 66.689317, mean_eps: 0.446032
 168003/300000: episode: 1450, duration: 0.618s, episode steps:  77, steps per second: 125, episode reward: -29.773, mean reward: -0.387 [-100.000,  9.718], mean action: 1.688 [0.000, 3.000],  loss: 13.628308, mse: 5577.942881, mean_q: 65.774458, mean_eps: 0.445719
 169003/300000: episode: 1451, duration: 10.353s, episode steps: 1000, steps per second:  97, episode reward: 92.539, mean reward:  0.093 [-23.231, 24.452], mean action: 1.471 [0.000, 3.000],  loss: 13.622791, mse: 5642.433262, mean_q: 66.471990, mean_eps: 0.443942
 170003/300000: episode: 1452, duration: 8.760s, episode steps: 1000, steps per second: 114, episode reward: 108.922, mean reward:  0.109 [-22.145, 24.054], mean action: 1.710 [0.000, 3.000],  loss: 12.853305, mse: 5651.657889, mean_q: 66.712565, mean_eps: 0.440642
 171003/300000: episode: 1453, duration: 8.386s, episode steps: 1000, steps per second: 119, episode reward: 124.524, mean reward:  0.125 [-21.632, 23.174], mean action: 1.408 [0.000, 3.000],  loss: 12.688442, mse: 5527.093086, mean_q: 66.365461, mean_eps: 0.437342
 172003/300000: episode: 1454, duration: 9.757s, episode steps: 1000, steps per second: 102, episode reward: 110.288, mean reward:  0.110 [-24.689, 22.821], mean action: 1.262 [0.000, 3.000],  loss: 12.805846, mse: 5612.495025, mean_q: 67.542607, mean_eps: 0.434042
 173003/300000: episode: 1455, duration: 8.521s, episode steps: 1000, steps per second: 117, episode reward: -36.755, mean reward: -0.037 [-23.708, 24.283], mean action: 1.795 [0.000, 3.000],  loss: 11.584315, mse: 5537.029119, mean_q: 67.766254, mean_eps: 0.430742
 174003/300000: episode: 1456, duration: 9.428s, episode steps: 1000, steps per second: 106, episode reward: 75.917, mean reward:  0.076 [-23.318, 22.724], mean action: 1.298 [0.000, 3.000],  loss: 11.090391, mse: 5484.962987, mean_q: 67.623059, mean_eps: 0.427442
 175003/300000: episode: 1457, duration: 8.407s, episode steps: 1000, steps per second: 119, episode reward: 93.159, mean reward:  0.093 [-20.969, 24.168], mean action: 1.630 [0.000, 3.000],  loss: 10.843428, mse: 5465.498431, mean_q: 67.718205, mean_eps: 0.424142
 175412/300000: episode: 1458, duration: 3.584s, episode steps: 409, steps per second: 114, episode reward: -306.270, mean reward: -0.749 [-100.000, 12.251], mean action: 1.797 [0.000, 3.000],  loss: 10.911339, mse: 5337.603632, mean_q: 67.676667, mean_eps: 0.421817
 175561/300000: episode: 1459, duration: 1.064s, episode steps: 149, steps per second: 140, episode reward: -9.774, mean reward: -0.066 [-100.000, 17.927], mean action: 1.644 [0.000, 3.000],  loss: 10.679397, mse: 5298.906288, mean_q: 67.256890, mean_eps: 0.420896
 175745/300000: episode: 1460, duration: 1.517s, episode steps: 184, steps per second: 121, episode reward: -17.582, mean reward: -0.096 [-100.000, 36.383], mean action: 1.870 [0.000, 3.000],  loss: 8.045172, mse: 5298.068654, mean_q: 67.336293, mean_eps: 0.420347
 176745/300000: episode: 1461, duration: 9.288s, episode steps: 1000, steps per second: 108, episode reward: 84.217, mean reward:  0.084 [-22.845, 26.189], mean action: 1.611 [0.000, 3.000],  loss: 9.850999, mse: 5321.085833, mean_q: 67.499109, mean_eps: 0.418393
 176907/300000: episode: 1462, duration: 1.446s, episode steps: 162, steps per second: 112, episode reward: 37.524, mean reward:  0.232 [-100.000, 12.910], mean action: 1.815 [0.000, 3.000],  loss: 9.625815, mse: 5226.200436, mean_q: 67.284241, mean_eps: 0.416476
 177014/300000: episode: 1463, duration: 0.783s, episode steps: 107, steps per second: 137, episode reward: -19.906, mean reward: -0.186 [-100.000, 12.589], mean action: 1.748 [0.000, 3.000],  loss: 9.480816, mse: 5264.544890, mean_q: 66.851051, mean_eps: 0.416032
 177127/300000: episode: 1464, duration: 1.591s, episode steps: 113, steps per second:  71, episode reward: -1.145, mean reward: -0.010 [-100.000, 24.127], mean action: 1.823 [0.000, 3.000],  loss: 7.468536, mse: 5327.829315, mean_q: 68.181007, mean_eps: 0.415669
 178127/300000: episode: 1465, duration: 9.447s, episode steps: 1000, steps per second: 106, episode reward: 118.619, mean reward:  0.119 [-20.819, 23.046], mean action: 1.146 [0.000, 3.000],  loss: 9.863173, mse: 5359.286629, mean_q: 68.039093, mean_eps: 0.413833
 179127/300000: episode: 1466, duration: 8.040s, episode steps: 1000, steps per second: 124, episode reward: 20.950, mean reward:  0.021 [-24.142, 24.199], mean action: 2.173 [0.000, 3.000],  loss: 11.018701, mse: 5316.442873, mean_q: 67.768019, mean_eps: 0.410533
 179228/300000: episode: 1467, duration: 0.863s, episode steps: 101, steps per second: 117, episode reward: 30.334, mean reward:  0.300 [-100.000, 19.383], mean action: 1.911 [0.000, 3.000],  loss: 8.110251, mse: 5302.752753, mean_q: 68.109812, mean_eps: 0.408716
 180228/300000: episode: 1468, duration: 8.958s, episode steps: 1000, steps per second: 112, episode reward: 89.981, mean reward:  0.090 [-22.963, 23.827], mean action: 1.448 [0.000, 3.000],  loss: 10.660997, mse: 5164.910173, mean_q: 67.363960, mean_eps: 0.406899
 181228/300000: episode: 1469, duration: 10.669s, episode steps: 1000, steps per second:  94, episode reward: 59.529, mean reward:  0.060 [-23.825, 22.885], mean action: 1.360 [0.000, 3.000],  loss: 10.117717, mse: 5136.864112, mean_q: 67.679828, mean_eps: 0.403599
 181311/300000: episode: 1470, duration: 0.636s, episode steps:  83, steps per second: 131, episode reward: -28.466, mean reward: -0.343 [-100.000, 15.783], mean action: 1.699 [0.000, 3.000],  loss: 12.557745, mse: 5120.303223, mean_q: 67.067276, mean_eps: 0.401812
 182311/300000: episode: 1471, duration: 11.651s, episode steps: 1000, steps per second:  86, episode reward: 121.737, mean reward:  0.122 [-19.784, 23.444], mean action: 1.757 [0.000, 3.000],  loss: 10.340372, mse: 5133.373708, mean_q: 67.349766, mean_eps: 0.400025
 183311/300000: episode: 1472, duration: 9.759s, episode steps: 1000, steps per second: 102, episode reward: 104.791, mean reward:  0.105 [-24.123, 26.771], mean action: 1.589 [0.000, 3.000],  loss: 9.889954, mse: 5098.958457, mean_q: 67.691077, mean_eps: 0.396725
 184311/300000: episode: 1473, duration: 8.307s, episode steps: 1000, steps per second: 120, episode reward: 96.925, mean reward:  0.097 [-20.975, 25.635], mean action: 1.677 [0.000, 3.000],  loss: 8.548239, mse: 5162.616196, mean_q: 67.985892, mean_eps: 0.393425
 185311/300000: episode: 1474, duration: 8.844s, episode steps: 1000, steps per second: 113, episode reward: 103.883, mean reward:  0.104 [-21.091, 23.053], mean action: 1.818 [0.000, 3.000],  loss: 10.411327, mse: 5020.641548, mean_q: 67.339757, mean_eps: 0.390125
 186311/300000: episode: 1475, duration: 9.951s, episode steps: 1000, steps per second: 100, episode reward: 147.515, mean reward:  0.148 [-23.924, 23.319], mean action: 1.424 [0.000, 3.000],  loss: 9.176729, mse: 5045.854699, mean_q: 67.877856, mean_eps: 0.386825
 186430/300000: episode: 1476, duration: 0.900s, episode steps: 119, steps per second: 132, episode reward:  2.848, mean reward:  0.024 [-100.000, 16.448], mean action: 1.832 [0.000, 3.000],  loss: 8.065892, mse: 4999.458080, mean_q: 67.208446, mean_eps: 0.384979
 187430/300000: episode: 1477, duration: 9.538s, episode steps: 1000, steps per second: 105, episode reward: -9.463, mean reward: -0.009 [-21.699, 15.994], mean action: 1.780 [0.000, 3.000],  loss: 9.241665, mse: 5034.904874, mean_q: 67.665982, mean_eps: 0.383133
 188430/300000: episode: 1478, duration: 8.711s, episode steps: 1000, steps per second: 115, episode reward: 140.674, mean reward:  0.141 [-24.446, 24.204], mean action: 1.609 [0.000, 3.000],  loss: 8.273734, mse: 5055.942106, mean_q: 67.109199, mean_eps: 0.379833
 189430/300000: episode: 1479, duration: 9.194s, episode steps: 1000, steps per second: 109, episode reward: 101.711, mean reward:  0.102 [-24.855, 23.133], mean action: 1.243 [0.000, 3.000],  loss: 8.204506, mse: 5010.152665, mean_q: 66.867421, mean_eps: 0.376533
 190430/300000: episode: 1480, duration: 8.490s, episode steps: 1000, steps per second: 118, episode reward: 77.031, mean reward:  0.077 [-21.475, 23.060], mean action: 1.476 [0.000, 3.000],  loss: 7.211714, mse: 4986.376910, mean_q: 67.262154, mean_eps: 0.373233
 191430/300000: episode: 1481, duration: 9.123s, episode steps: 1000, steps per second: 110, episode reward: 120.734, mean reward:  0.121 [-24.326, 24.155], mean action: 1.433 [0.000, 3.000],  loss: 7.651410, mse: 5024.024488, mean_q: 67.831106, mean_eps: 0.369933
 192430/300000: episode: 1482, duration: 8.298s, episode steps: 1000, steps per second: 121, episode reward: 134.772, mean reward:  0.135 [-23.315, 24.106], mean action: 1.385 [0.000, 3.000],  loss: 7.587710, mse: 4983.829857, mean_q: 67.907015, mean_eps: 0.366633
 193430/300000: episode: 1483, duration: 9.312s, episode steps: 1000, steps per second: 107, episode reward: 89.972, mean reward:  0.090 [-20.801, 23.064], mean action: 1.593 [0.000, 3.000],  loss: 7.134401, mse: 4877.974833, mean_q: 67.565868, mean_eps: 0.363333
 194430/300000: episode: 1484, duration: 9.477s, episode steps: 1000, steps per second: 106, episode reward: 72.036, mean reward:  0.072 [-24.254, 23.765], mean action: 1.904 [0.000, 3.000],  loss: 6.618250, mse: 4836.682262, mean_q: 67.342874, mean_eps: 0.360033
 195430/300000: episode: 1485, duration: 9.090s, episode steps: 1000, steps per second: 110, episode reward: 128.989, mean reward:  0.129 [-22.524, 22.880], mean action: 1.360 [0.000, 3.000],  loss: 6.705053, mse: 4745.016720, mean_q: 66.418555, mean_eps: 0.356733
 196430/300000: episode: 1486, duration: 7.912s, episode steps: 1000, steps per second: 126, episode reward: 102.257, mean reward:  0.102 [-23.025, 22.644], mean action: 1.248 [0.000, 3.000],  loss: 5.365617, mse: 4703.804702, mean_q: 65.951381, mean_eps: 0.353433
 196563/300000: episode: 1487, duration: 0.974s, episode steps: 133, steps per second: 137, episode reward: -37.053, mean reward: -0.279 [-100.000, 10.479], mean action: 1.699 [0.000, 3.000],  loss: 6.273392, mse: 4642.745466, mean_q: 65.379090, mean_eps: 0.351563
 196854/300000: episode: 1488, duration: 2.148s, episode steps: 291, steps per second: 135, episode reward: -93.986, mean reward: -0.323 [-100.000, 11.002], mean action: 1.608 [0.000, 3.000],  loss: 6.644565, mse: 4696.791000, mean_q: 65.708365, mean_eps: 0.350864
 197854/300000: episode: 1489, duration: 7.838s, episode steps: 1000, steps per second: 128, episode reward: 100.075, mean reward:  0.100 [-20.451, 23.704], mean action: 1.082 [0.000, 3.000],  loss: 5.658807, mse: 4786.732075, mean_q: 66.187509, mean_eps: 0.348733
 198786/300000: episode: 1490, duration: 7.564s, episode steps: 932, steps per second: 123, episode reward: 164.206, mean reward:  0.176 [-20.681, 100.000], mean action: 2.267 [0.000, 3.000],  loss: 5.762836, mse: 4599.195385, mean_q: 65.086053, mean_eps: 0.345546
 199786/300000: episode: 1491, duration: 8.086s, episode steps: 1000, steps per second: 124, episode reward: 117.746, mean reward:  0.118 [-20.286, 22.988], mean action: 1.278 [0.000, 3.000],  loss: 5.964542, mse: 4521.963438, mean_q: 64.418225, mean_eps: 0.342358
 200786/300000: episode: 1492, duration: 7.787s, episode steps: 1000, steps per second: 128, episode reward: 85.409, mean reward:  0.085 [-20.218, 23.078], mean action: 2.275 [0.000, 3.000],  loss: 5.871870, mse: 4434.116580, mean_q: 63.654228, mean_eps: 0.339058
 201786/300000: episode: 1493, duration: 8.179s, episode steps: 1000, steps per second: 122, episode reward: 144.104, mean reward:  0.144 [-23.091, 23.765], mean action: 1.060 [0.000, 3.000],  loss: 5.141963, mse: 4428.033888, mean_q: 63.230814, mean_eps: 0.335758
 202786/300000: episode: 1494, duration: 7.502s, episode steps: 1000, steps per second: 133, episode reward: 132.737, mean reward:  0.133 [-22.693, 24.840], mean action: 0.971 [0.000, 3.000],  loss: 4.525958, mse: 4451.180064, mean_q: 63.430813, mean_eps: 0.332458
 203786/300000: episode: 1495, duration: 8.374s, episode steps: 1000, steps per second: 119, episode reward: 56.105, mean reward:  0.056 [-24.449, 23.960], mean action: 2.033 [0.000, 3.000],  loss: 5.562212, mse: 4307.194617, mean_q: 62.335244, mean_eps: 0.329158
 204786/300000: episode: 1496, duration: 7.835s, episode steps: 1000, steps per second: 128, episode reward: 114.959, mean reward:  0.115 [-24.093, 24.192], mean action: 1.207 [0.000, 3.000],  loss: 5.092856, mse: 4293.760040, mean_q: 62.169640, mean_eps: 0.325858
 205786/300000: episode: 1497, duration: 7.850s, episode steps: 1000, steps per second: 127, episode reward: 109.523, mean reward:  0.110 [-21.829, 24.442], mean action: 1.127 [0.000, 3.000],  loss: 4.089974, mse: 4211.988256, mean_q: 61.315078, mean_eps: 0.322558
 206786/300000: episode: 1498, duration: 7.706s, episode steps: 1000, steps per second: 130, episode reward: 51.086, mean reward:  0.051 [-21.628, 25.839], mean action: 1.238 [0.000, 3.000],  loss: 4.539338, mse: 4189.224708, mean_q: 60.988654, mean_eps: 0.319258
 207786/300000: episode: 1499, duration: 7.847s, episode steps: 1000, steps per second: 127, episode reward: 78.865, mean reward:  0.079 [-21.916, 22.352], mean action: 2.235 [0.000, 3.000],  loss: 5.261390, mse: 4121.991706, mean_q: 59.329174, mean_eps: 0.315958
 208786/300000: episode: 1500, duration: 7.658s, episode steps: 1000, steps per second: 131, episode reward: 71.742, mean reward:  0.072 [-24.260, 23.284], mean action: 1.450 [0.000, 3.000],  loss: 4.157723, mse: 4077.349053, mean_q: 58.711636, mean_eps: 0.312658
 209786/300000: episode: 1501, duration: 8.767s, episode steps: 1000, steps per second: 114, episode reward: 26.436, mean reward:  0.026 [-19.495, 24.439], mean action: 1.580 [0.000, 3.000],  loss: 4.241467, mse: 3969.355118, mean_q: 57.941337, mean_eps: 0.309358
 210786/300000: episode: 1502, duration: 7.525s, episode steps: 1000, steps per second: 133, episode reward: 77.730, mean reward:  0.078 [-24.680, 24.196], mean action: 2.101 [0.000, 3.000],  loss: 3.855122, mse: 3956.607658, mean_q: 58.132425, mean_eps: 0.306058
 211786/300000: episode: 1503, duration: 8.727s, episode steps: 1000, steps per second: 115, episode reward: 49.314, mean reward:  0.049 [-22.651, 24.712], mean action: 1.392 [0.000, 3.000],  loss: 3.402468, mse: 3928.517972, mean_q: 57.860590, mean_eps: 0.302758
 212786/300000: episode: 1504, duration: 7.869s, episode steps: 1000, steps per second: 127, episode reward: 94.660, mean reward:  0.095 [-20.439, 21.404], mean action: 1.048 [0.000, 3.000],  loss: 2.687985, mse: 3914.241260, mean_q: 57.639787, mean_eps: 0.299458
 213786/300000: episode: 1505, duration: 7.451s, episode steps: 1000, steps per second: 134, episode reward: 105.349, mean reward:  0.105 [-20.622, 23.615], mean action: 1.036 [0.000, 3.000],  loss: 2.970455, mse: 3910.337783, mean_q: 57.322486, mean_eps: 0.296158
 214786/300000: episode: 1506, duration: 7.840s, episode steps: 1000, steps per second: 128, episode reward: 147.222, mean reward:  0.147 [-23.795, 22.441], mean action: 1.138 [0.000, 3.000],  loss: 2.569053, mse: 3823.537589, mean_q: 56.689416, mean_eps: 0.292858
 215786/300000: episode: 1507, duration: 9.086s, episode steps: 1000, steps per second: 110, episode reward: 100.664, mean reward:  0.101 [-21.561, 22.994], mean action: 1.428 [0.000, 3.000],  loss: 3.077000, mse: 3763.226992, mean_q: 55.889334, mean_eps: 0.289558
 216786/300000: episode: 1508, duration: 8.014s, episode steps: 1000, steps per second: 125, episode reward: 103.949, mean reward:  0.104 [-22.821, 22.132], mean action: 1.168 [0.000, 3.000],  loss: 2.975070, mse: 3806.677498, mean_q: 56.565667, mean_eps: 0.286258
 217011/300000: episode: 1509, duration: 1.609s, episode steps: 225, steps per second: 140, episode reward: 24.227, mean reward:  0.108 [-100.000, 10.238], mean action: 1.849 [0.000, 3.000],  loss: 2.254705, mse: 3781.702592, mean_q: 56.605843, mean_eps: 0.284237
 218011/300000: episode: 1510, duration: 8.071s, episode steps: 1000, steps per second: 124, episode reward: 122.321, mean reward:  0.122 [-19.111, 24.019], mean action: 1.026 [0.000, 3.000],  loss: 2.869757, mse: 3840.294235, mean_q: 56.886963, mean_eps: 0.282215
 219011/300000: episode: 1511, duration: 8.657s, episode steps: 1000, steps per second: 116, episode reward: 124.033, mean reward:  0.124 [-21.186, 23.064], mean action: 1.197 [0.000, 3.000],  loss: 2.398401, mse: 3664.606851, mean_q: 55.795919, mean_eps: 0.278915
 220011/300000: episode: 1512, duration: 8.785s, episode steps: 1000, steps per second: 114, episode reward: 150.216, mean reward:  0.150 [-23.368, 23.160], mean action: 1.744 [0.000, 3.000],  loss: 2.740814, mse: 3708.903167, mean_q: 56.165625, mean_eps: 0.275615
 220205/300000: episode: 1513, duration: 1.393s, episode steps: 194, steps per second: 139, episode reward: 52.096, mean reward:  0.269 [-100.000, 18.057], mean action: 1.747 [0.000, 3.000],  loss: 1.548102, mse: 3740.758990, mean_q: 56.337991, mean_eps: 0.273645
 221205/300000: episode: 1514, duration: 8.315s, episode steps: 1000, steps per second: 120, episode reward: 122.123, mean reward:  0.122 [-24.020, 24.289], mean action: 0.841 [0.000, 3.000],  loss: 2.912858, mse: 3716.703951, mean_q: 56.128807, mean_eps: 0.271675
 222205/300000: episode: 1515, duration: 7.920s, episode steps: 1000, steps per second: 126, episode reward: 159.127, mean reward:  0.159 [-20.358, 22.454], mean action: 0.922 [0.000, 3.000],  loss: 2.168986, mse: 3727.643870, mean_q: 56.195710, mean_eps: 0.268375
 223205/300000: episode: 1516, duration: 8.010s, episode steps: 1000, steps per second: 125, episode reward: 117.183, mean reward:  0.117 [-22.139, 23.393], mean action: 0.980 [0.000, 3.000],  loss: 3.361668, mse: 3721.580973, mean_q: 56.122135, mean_eps: 0.265075
 223467/300000: episode: 1517, duration: 2.060s, episode steps: 262, steps per second: 127, episode reward: 231.755, mean reward:  0.885 [-20.962, 100.000], mean action: 1.531 [0.000, 3.000],  loss: 3.644251, mse: 3700.157186, mean_q: 56.004997, mean_eps: 0.262993
 223864/300000: episode: 1518, duration: 3.092s, episode steps: 397, steps per second: 128, episode reward: 251.339, mean reward:  0.633 [-20.331, 100.000], mean action: 1.181 [0.000, 3.000],  loss: 2.198930, mse: 3780.650355, mean_q: 56.905306, mean_eps: 0.261905
 224864/300000: episode: 1519, duration: 7.739s, episode steps: 1000, steps per second: 129, episode reward: 125.546, mean reward:  0.126 [-22.118, 24.891], mean action: 0.835 [0.000, 3.000],  loss: 2.505462, mse: 3801.534900, mean_q: 56.846856, mean_eps: 0.259600
 225864/300000: episode: 1520, duration: 8.804s, episode steps: 1000, steps per second: 114, episode reward: 122.769, mean reward:  0.123 [-20.392, 23.087], mean action: 0.828 [0.000, 3.000],  loss: 2.721551, mse: 3754.879786, mean_q: 56.700163, mean_eps: 0.256300
 226864/300000: episode: 1521, duration: 8.651s, episode steps: 1000, steps per second: 116, episode reward: 87.118, mean reward:  0.087 [-19.349, 22.634], mean action: 2.341 [0.000, 3.000],  loss: 2.362465, mse: 3670.064811, mean_q: 55.954617, mean_eps: 0.253000
 227864/300000: episode: 1522, duration: 8.882s, episode steps: 1000, steps per second: 113, episode reward: 123.234, mean reward:  0.123 [-22.024, 23.435], mean action: 0.841 [0.000, 3.000],  loss: 2.523309, mse: 3585.691590, mean_q: 55.350130, mean_eps: 0.249700
 228864/300000: episode: 1523, duration: 7.706s, episode steps: 1000, steps per second: 130, episode reward: 107.340, mean reward:  0.107 [-19.993, 22.372], mean action: 1.029 [0.000, 3.000],  loss: 2.000832, mse: 3570.917044, mean_q: 55.545219, mean_eps: 0.246400
 229864/300000: episode: 1524, duration: 7.472s, episode steps: 1000, steps per second: 134, episode reward: 156.567, mean reward:  0.157 [-21.906, 22.365], mean action: 0.918 [0.000, 3.000],  loss: 1.696842, mse: 3616.364482, mean_q: 55.840190, mean_eps: 0.243100
 230216/300000: episode: 1525, duration: 2.803s, episode steps: 352, steps per second: 126, episode reward: 248.536, mean reward:  0.706 [-17.722, 100.000], mean action: 1.534 [0.000, 3.000],  loss: 1.673862, mse: 3721.591658, mean_q: 56.409466, mean_eps: 0.240870
 230745/300000: episode: 1526, duration: 4.103s, episode steps: 529, steps per second: 129, episode reward: 203.460, mean reward:  0.385 [-23.346, 100.000], mean action: 2.104 [0.000, 3.000],  loss: 2.238373, mse: 3657.586643, mean_q: 55.974740, mean_eps: 0.239416
 231479/300000: episode: 1527, duration: 5.607s, episode steps: 734, steps per second: 131, episode reward: 185.232, mean reward:  0.252 [-19.515, 100.000], mean action: 1.114 [0.000, 3.000],  loss: 1.735920, mse: 3695.039464, mean_q: 56.435073, mean_eps: 0.237332
 232479/300000: episode: 1528, duration: 8.838s, episode steps: 1000, steps per second: 113, episode reward: 111.444, mean reward:  0.111 [-19.751, 22.831], mean action: 2.340 [0.000, 3.000],  loss: 1.945510, mse: 3611.749934, mean_q: 55.839065, mean_eps: 0.234471
 232822/300000: episode: 1529, duration: 2.505s, episode steps: 343, steps per second: 137, episode reward: 266.317, mean reward:  0.776 [-10.714, 100.000], mean action: 1.318 [0.000, 3.000],  loss: 1.794635, mse: 3624.655934, mean_q: 55.919752, mean_eps: 0.232255
 233821/300000: episode: 1530, duration: 9.411s, episode steps: 999, steps per second: 106, episode reward: 211.519, mean reward:  0.212 [-18.794, 100.000], mean action: 0.969 [0.000, 3.000],  loss: 2.142875, mse: 3675.277247, mean_q: 56.185884, mean_eps: 0.230041
 234821/300000: episode: 1531, duration: 7.369s, episode steps: 1000, steps per second: 136, episode reward: 158.570, mean reward:  0.159 [-20.058, 23.234], mean action: 0.807 [0.000, 3.000],  loss: 1.882494, mse: 3617.208330, mean_q: 55.756404, mean_eps: 0.226742
 235404/300000: episode: 1532, duration: 5.251s, episode steps: 583, steps per second: 111, episode reward: 262.956, mean reward:  0.451 [-19.466, 100.000], mean action: 0.993 [0.000, 3.000],  loss: 1.699144, mse: 3621.767172, mean_q: 55.498809, mean_eps: 0.224130
 236381/300000: episode: 1533, duration: 8.048s, episode steps: 977, steps per second: 121, episode reward: 262.582, mean reward:  0.269 [-22.548, 100.000], mean action: 0.939 [0.000, 3.000],  loss: 2.300664, mse: 3598.914581, mean_q: 55.178594, mean_eps: 0.221556
 236874/300000: episode: 1534, duration: 4.190s, episode steps: 493, steps per second: 118, episode reward: 187.667, mean reward:  0.381 [-18.881, 100.000], mean action: 1.499 [0.000, 3.000],  loss: 2.377735, mse: 3578.294583, mean_q: 54.962065, mean_eps: 0.219131
 237352/300000: episode: 1535, duration: 3.623s, episode steps: 478, steps per second: 132, episode reward: 277.034, mean reward:  0.580 [-18.362, 100.000], mean action: 1.107 [0.000, 3.000],  loss: 1.686506, mse: 3596.009144, mean_q: 55.053378, mean_eps: 0.217529
 238352/300000: episode: 1536, duration: 8.882s, episode steps: 1000, steps per second: 113, episode reward: 150.661, mean reward:  0.151 [-22.665, 23.252], mean action: 1.474 [0.000, 3.000],  loss: 2.224138, mse: 3558.721359, mean_q: 54.689727, mean_eps: 0.215090
 239201/300000: episode: 1537, duration: 6.838s, episode steps: 849, steps per second: 124, episode reward: 254.535, mean reward:  0.300 [-20.593, 100.000], mean action: 0.933 [0.000, 3.000],  loss: 2.234154, mse: 3586.566415, mean_q: 55.038688, mean_eps: 0.212039
 240201/300000: episode: 1538, duration: 7.783s, episode steps: 1000, steps per second: 128, episode reward: 158.258, mean reward:  0.158 [-23.052, 23.618], mean action: 0.763 [0.000, 3.000],  loss: 1.962427, mse: 3553.897231, mean_q: 54.702072, mean_eps: 0.208988
 241201/300000: episode: 1539, duration: 8.110s, episode steps: 1000, steps per second: 123, episode reward: 140.612, mean reward:  0.141 [-23.905, 24.182], mean action: 0.984 [0.000, 3.000],  loss: 2.590633, mse: 3567.931027, mean_q: 54.825363, mean_eps: 0.205688
 242187/300000: episode: 1540, duration: 8.665s, episode steps: 986, steps per second: 114, episode reward: 257.612, mean reward:  0.261 [-20.802, 100.000], mean action: 0.803 [0.000, 3.000],  loss: 2.422810, mse: 3465.242448, mean_q: 54.008315, mean_eps: 0.202411
 243187/300000: episode: 1541, duration: 9.052s, episode steps: 1000, steps per second: 110, episode reward: 137.044, mean reward:  0.137 [-20.206, 23.156], mean action: 0.875 [0.000, 3.000],  loss: 2.115236, mse: 3419.651638, mean_q: 53.621337, mean_eps: 0.199135
 244187/300000: episode: 1542, duration: 8.801s, episode steps: 1000, steps per second: 114, episode reward: 110.185, mean reward:  0.110 [-19.546, 23.025], mean action: 0.762 [0.000, 3.000],  loss: 1.891502, mse: 3427.023972, mean_q: 53.555731, mean_eps: 0.195835
 244750/300000: episode: 1543, duration: 4.471s, episode steps: 563, steps per second: 126, episode reward: 245.079, mean reward:  0.435 [-24.272, 100.000], mean action: 1.085 [0.000, 3.000],  loss: 1.838601, mse: 3398.600641, mean_q: 53.322271, mean_eps: 0.193256
 245750/300000: episode: 1544, duration: 8.458s, episode steps: 1000, steps per second: 118, episode reward: 89.839, mean reward:  0.090 [-19.597, 23.341], mean action: 2.171 [0.000, 3.000],  loss: 2.333901, mse: 3305.276243, mean_q: 52.583311, mean_eps: 0.190677
 245876/300000: episode: 1545, duration: 0.933s, episode steps: 126, steps per second: 135, episode reward: 28.380, mean reward:  0.225 [-100.000, 10.845], mean action: 1.833 [0.000, 3.000],  loss: 2.660138, mse: 3312.800437, mean_q: 52.448126, mean_eps: 0.188819
 245964/300000: episode: 1546, duration: 0.614s, episode steps:  88, steps per second: 143, episode reward: -122.978, mean reward: -1.397 [-100.000,  7.086], mean action: 0.920 [0.000, 3.000],  loss: 2.044216, mse: 3280.421890, mean_q: 52.337159, mean_eps: 0.188466
 246963/300000: episode: 1547, duration: 8.428s, episode steps: 999, steps per second: 119, episode reward: 84.944, mean reward:  0.085 [-20.721, 100.000], mean action: 1.608 [0.000, 3.000],  loss: 2.140641, mse: 3408.008818, mean_q: 53.026664, mean_eps: 0.186672
 247963/300000: episode: 1548, duration: 7.866s, episode steps: 1000, steps per second: 127, episode reward: 118.153, mean reward:  0.118 [-19.665, 23.428], mean action: 2.127 [0.000, 3.000],  loss: 3.398074, mse: 3323.680710, mean_q: 52.351085, mean_eps: 0.183374
 248359/300000: episode: 1549, duration: 3.160s, episode steps: 396, steps per second: 125, episode reward: 274.328, mean reward:  0.693 [-19.526, 100.000], mean action: 1.288 [0.000, 3.000],  loss: 2.548175, mse: 3332.430225, mean_q: 52.682008, mean_eps: 0.181070
 248868/300000: episode: 1550, duration: 4.135s, episode steps: 509, steps per second: 123, episode reward: 251.949, mean reward:  0.495 [-19.569, 100.000], mean action: 1.041 [0.000, 3.000],  loss: 2.719412, mse: 3369.737764, mean_q: 52.901972, mean_eps: 0.179577
 248947/300000: episode: 1551, duration: 0.564s, episode steps:  79, steps per second: 140, episode reward: -238.885, mean reward: -3.024 [-100.000, 21.151], mean action: 1.759 [0.000, 3.000],  loss: 1.902130, mse: 3352.185684, mean_q: 52.556323, mean_eps: 0.178607
 249947/300000: episode: 1552, duration: 8.082s, episode steps: 1000, steps per second: 124, episode reward: 119.486, mean reward:  0.119 [-22.392, 22.970], mean action: 0.943 [0.000, 3.000],  loss: 6.012079, mse: 3374.376530, mean_q: 52.886591, mean_eps: 0.176827
 250465/300000: episode: 1553, duration: 4.042s, episode steps: 518, steps per second: 128, episode reward: 244.533, mean reward:  0.472 [-18.316, 100.000], mean action: 1.046 [0.000, 3.000],  loss: 2.811032, mse: 3369.832549, mean_q: 53.006462, mean_eps: 0.174322
 251424/300000: episode: 1554, duration: 7.676s, episode steps: 959, steps per second: 125, episode reward: 232.701, mean reward:  0.243 [-23.596, 100.000], mean action: 0.865 [0.000, 3.000],  loss: 2.557584, mse: 3420.818487, mean_q: 53.472122, mean_eps: 0.171885
 252189/300000: episode: 1555, duration: 6.098s, episode steps: 765, steps per second: 125, episode reward: 257.215, mean reward:  0.336 [-20.359, 100.000], mean action: 0.965 [0.000, 3.000],  loss: 2.552426, mse: 3404.191185, mean_q: 53.379062, mean_eps: 0.169040
 252629/300000: episode: 1556, duration: 3.443s, episode steps: 440, steps per second: 128, episode reward: 284.424, mean reward:  0.646 [-17.777, 100.000], mean action: 1.105 [0.000, 3.000],  loss: 1.867185, mse: 3429.268450, mean_q: 53.650702, mean_eps: 0.167052
 253257/300000: episode: 1557, duration: 4.714s, episode steps: 628, steps per second: 133, episode reward: 283.211, mean reward:  0.451 [-19.851, 100.000], mean action: 0.725 [0.000, 3.000],  loss: 2.393191, mse: 3486.082753, mean_q: 54.014094, mean_eps: 0.165290
 253746/300000: episode: 1558, duration: 3.532s, episode steps: 489, steps per second: 138, episode reward: -179.483, mean reward: -0.367 [-100.000, 36.084], mean action: 1.423 [0.000, 3.000],  loss: 2.592834, mse: 3479.624700, mean_q: 53.893730, mean_eps: 0.163447
 254216/300000: episode: 1559, duration: 3.452s, episode steps: 470, steps per second: 136, episode reward: 236.164, mean reward:  0.502 [-21.486, 100.000], mean action: 0.972 [0.000, 3.000],  loss: 3.159475, mse: 3508.780979, mean_q: 54.330711, mean_eps: 0.161864
 254550/300000: episode: 1560, duration: 2.498s, episode steps: 334, steps per second: 134, episode reward: 252.840, mean reward:  0.757 [-18.841, 100.000], mean action: 1.377 [0.000, 3.000],  loss: 3.385745, mse: 3586.274120, mean_q: 54.942366, mean_eps: 0.160538
 254939/300000: episode: 1561, duration: 3.004s, episode steps: 389, steps per second: 130, episode reward: 244.342, mean reward:  0.628 [-2.982, 100.000], mean action: 1.524 [0.000, 3.000],  loss: 4.891763, mse: 3632.366985, mean_q: 55.428207, mean_eps: 0.159345
 255532/300000: episode: 1562, duration: 4.449s, episode steps: 593, steps per second: 133, episode reward: 262.062, mean reward:  0.442 [-18.545, 100.000], mean action: 0.978 [0.000, 3.000],  loss: 4.606491, mse: 3587.083930, mean_q: 55.147232, mean_eps: 0.157724
 255860/300000: episode: 1563, duration: 2.392s, episode steps: 328, steps per second: 137, episode reward: 252.150, mean reward:  0.769 [-3.038, 100.000], mean action: 1.619 [0.000, 3.000],  loss: 3.885427, mse: 3659.813529, mean_q: 55.592648, mean_eps: 0.156205
 256281/300000: episode: 1564, duration: 3.367s, episode steps: 421, steps per second: 125, episode reward: 261.587, mean reward:  0.621 [-9.164, 100.000], mean action: 1.406 [0.000, 3.000],  loss: 2.711206, mse: 3611.517818, mean_q: 55.326203, mean_eps: 0.154969
 256868/300000: episode: 1565, duration: 4.705s, episode steps: 587, steps per second: 125, episode reward: 178.922, mean reward:  0.305 [-18.604, 100.000], mean action: 2.290 [0.000, 3.000],  loss: 2.935600, mse: 3665.520794, mean_q: 55.946630, mean_eps: 0.153306
 257195/300000: episode: 1566, duration: 2.527s, episode steps: 327, steps per second: 129, episode reward: 246.546, mean reward:  0.754 [-19.445, 100.000], mean action: 1.226 [0.000, 3.000],  loss: 3.054222, mse: 3689.648143, mean_q: 56.094054, mean_eps: 0.151798
 257757/300000: episode: 1567, duration: 4.357s, episode steps: 562, steps per second: 129, episode reward: 255.839, mean reward:  0.455 [-18.821, 100.000], mean action: 1.297 [0.000, 3.000],  loss: 3.619050, mse: 3734.976755, mean_q: 56.405656, mean_eps: 0.150331
 258417/300000: episode: 1568, duration: 5.386s, episode steps: 660, steps per second: 123, episode reward: 262.079, mean reward:  0.397 [-18.077, 100.000], mean action: 0.967 [0.000, 3.000],  loss: 4.297281, mse: 3684.597822, mean_q: 56.010085, mean_eps: 0.148315
 258734/300000: episode: 1569, duration: 2.410s, episode steps: 317, steps per second: 132, episode reward: 260.580, mean reward:  0.822 [-9.005, 100.000], mean action: 1.457 [0.000, 3.000],  loss: 4.456602, mse: 3774.070272, mean_q: 56.550532, mean_eps: 0.146702
 259143/300000: episode: 1570, duration: 3.112s, episode steps: 409, steps per second: 131, episode reward: 252.992, mean reward:  0.619 [-11.824, 100.000], mean action: 1.340 [0.000, 3.000],  loss: 3.774168, mse: 3786.593467, mean_q: 56.813005, mean_eps: 0.145505
 259529/300000: episode: 1571, duration: 3.023s, episode steps: 386, steps per second: 128, episode reward: 297.394, mean reward:  0.770 [-19.950, 100.000], mean action: 1.244 [0.000, 3.000],  loss: 4.160369, mse: 3855.632945, mean_q: 57.604970, mean_eps: 0.144193
 259927/300000: episode: 1572, duration: 3.494s, episode steps: 398, steps per second: 114, episode reward: 222.257, mean reward:  0.558 [-9.608, 100.000], mean action: 1.214 [0.000, 3.000],  loss: 2.739886, mse: 3861.208674, mean_q: 57.614197, mean_eps: 0.142899
 260510/300000: episode: 1573, duration: 5.028s, episode steps: 583, steps per second: 116, episode reward: 277.803, mean reward:  0.477 [-18.519, 100.000], mean action: 1.232 [0.000, 3.000],  loss: 4.217814, mse: 3793.961356, mean_q: 57.300213, mean_eps: 0.141281
 260859/300000: episode: 1574, duration: 2.586s, episode steps: 349, steps per second: 135, episode reward: 275.018, mean reward:  0.788 [-17.786, 100.000], mean action: 1.195 [0.000, 3.000],  loss: 3.346784, mse: 4006.003352, mean_q: 58.811909, mean_eps: 0.139743
 261272/300000: episode: 1575, duration: 3.213s, episode steps: 413, steps per second: 129, episode reward: 222.582, mean reward:  0.539 [-20.169, 100.000], mean action: 1.429 [0.000, 3.000],  loss: 4.625845, mse: 3907.982699, mean_q: 57.963710, mean_eps: 0.138485
 261690/300000: episode: 1576, duration: 3.219s, episode steps: 418, steps per second: 130, episode reward: 210.885, mean reward:  0.505 [-20.394, 100.000], mean action: 1.148 [0.000, 3.000],  loss: 3.094372, mse: 3975.474678, mean_q: 58.527960, mean_eps: 0.137114
 262230/300000: episode: 1577, duration: 4.186s, episode steps: 540, steps per second: 129, episode reward: 230.097, mean reward:  0.426 [-9.835, 100.000], mean action: 1.350 [0.000, 3.000],  loss: 4.199977, mse: 4045.099828, mean_q: 58.950361, mean_eps: 0.135534
 262628/300000: episode: 1578, duration: 3.001s, episode steps: 398, steps per second: 133, episode reward: 263.805, mean reward:  0.663 [-19.571, 100.000], mean action: 1.560 [0.000, 3.000],  loss: 4.561667, mse: 4047.058242, mean_q: 59.048893, mean_eps: 0.133986
 263206/300000: episode: 1579, duration: 4.770s, episode steps: 578, steps per second: 121, episode reward: 163.861, mean reward:  0.283 [-11.947, 100.000], mean action: 1.474 [0.000, 3.000],  loss: 3.429475, mse: 3976.037733, mean_q: 58.645362, mean_eps: 0.132376
 263713/300000: episode: 1580, duration: 3.990s, episode steps: 507, steps per second: 127, episode reward: 267.725, mean reward:  0.528 [-11.259, 100.000], mean action: 1.653 [0.000, 3.000],  loss: 4.333372, mse: 4076.878420, mean_q: 59.375894, mean_eps: 0.130585
 264011/300000: episode: 1581, duration: 2.289s, episode steps: 298, steps per second: 130, episode reward: 225.831, mean reward:  0.758 [-10.157, 100.000], mean action: 1.416 [0.000, 3.000],  loss: 4.397702, mse: 4104.479501, mean_q: 59.550627, mean_eps: 0.129257
 264220/300000: episode: 1582, duration: 1.571s, episode steps: 209, steps per second: 133, episode reward: -205.054, mean reward: -0.981 [-100.000, 22.995], mean action: 1.598 [0.000, 3.000],  loss: 3.237947, mse: 4084.378007, mean_q: 59.494780, mean_eps: 0.128420
 264572/300000: episode: 1583, duration: 2.746s, episode steps: 352, steps per second: 128, episode reward: 240.969, mean reward:  0.685 [-10.689, 100.000], mean action: 1.384 [0.000, 3.000],  loss: 4.344313, mse: 4182.725290, mean_q: 60.420230, mean_eps: 0.127495
 265419/300000: episode: 1584, duration: 6.918s, episode steps: 847, steps per second: 122, episode reward: 140.851, mean reward:  0.166 [-18.756, 100.000], mean action: 1.301 [0.000, 3.000],  loss: 4.694167, mse: 4172.495992, mean_q: 60.244841, mean_eps: 0.125516
 265830/300000: episode: 1585, duration: 3.152s, episode steps: 411, steps per second: 130, episode reward: 249.568, mean reward:  0.607 [-6.958, 100.000], mean action: 1.360 [0.000, 3.000],  loss: 4.316880, mse: 4115.601546, mean_q: 59.762494, mean_eps: 0.123441
 266042/300000: episode: 1586, duration: 1.619s, episode steps: 212, steps per second: 131, episode reward: 271.309, mean reward:  1.280 [-8.862, 100.000], mean action: 1.358 [0.000, 3.000],  loss: 3.525534, mse: 4118.169583, mean_q: 59.643570, mean_eps: 0.122413
 267011/300000: episode: 1587, duration: 8.149s, episode steps: 969, steps per second: 119, episode reward: 145.284, mean reward:  0.150 [-20.305, 100.000], mean action: 1.523 [0.000, 3.000],  loss: 4.438464, mse: 4181.876321, mean_q: 60.416704, mean_eps: 0.120464
 267466/300000: episode: 1588, duration: 3.497s, episode steps: 455, steps per second: 130, episode reward: 272.319, mean reward:  0.599 [-17.742, 100.000], mean action: 1.387 [0.000, 3.000],  loss: 3.653104, mse: 4200.506161, mean_q: 60.680881, mean_eps: 0.118115
 268025/300000: episode: 1589, duration: 4.745s, episode steps: 559, steps per second: 118, episode reward: 202.528, mean reward:  0.362 [-9.317, 100.000], mean action: 1.590 [0.000, 3.000],  loss: 5.061880, mse: 4194.937982, mean_q: 60.623366, mean_eps: 0.116441
 269025/300000: episode: 1590, duration: 9.090s, episode steps: 1000, steps per second: 110, episode reward: 98.962, mean reward:  0.099 [-19.056, 21.695], mean action: 1.643 [0.000, 3.000],  loss: 3.772230, mse: 4166.085857, mean_q: 60.253506, mean_eps: 0.113869
 269501/300000: episode: 1591, duration: 3.755s, episode steps: 476, steps per second: 127, episode reward: 266.828, mean reward:  0.561 [-17.837, 100.000], mean action: 1.307 [0.000, 3.000],  loss: 4.472767, mse: 4253.611083, mean_q: 60.995401, mean_eps: 0.111434
 269833/300000: episode: 1592, duration: 2.546s, episode steps: 332, steps per second: 130, episode reward: 264.910, mean reward:  0.798 [-8.253, 100.000], mean action: 1.491 [0.000, 3.000],  loss: 4.669538, mse: 4290.908617, mean_q: 61.329402, mean_eps: 0.110101
 270318/300000: episode: 1593, duration: 3.632s, episode steps: 485, steps per second: 134, episode reward: 290.079, mean reward:  0.598 [-17.609, 100.000], mean action: 0.940 [0.000, 3.000],  loss: 3.615516, mse: 4281.867109, mean_q: 61.287345, mean_eps: 0.108752
 270772/300000: episode: 1594, duration: 3.389s, episode steps: 454, steps per second: 134, episode reward: 260.886, mean reward:  0.575 [-18.092, 100.000], mean action: 1.057 [0.000, 3.000],  loss: 3.926508, mse: 4297.213422, mean_q: 61.527786, mean_eps: 0.107203
 271139/300000: episode: 1595, duration: 2.768s, episode steps: 367, steps per second: 133, episode reward: 248.620, mean reward:  0.677 [-11.402, 100.000], mean action: 1.496 [0.000, 3.000],  loss: 3.987865, mse: 4413.185220, mean_q: 62.351870, mean_eps: 0.105848
 271960/300000: episode: 1596, duration: 6.176s, episode steps: 821, steps per second: 133, episode reward: 212.009, mean reward:  0.258 [-21.856, 100.000], mean action: 1.380 [0.000, 3.000],  loss: 4.070842, mse: 4364.307409, mean_q: 61.885965, mean_eps: 0.103888
 272677/300000: episode: 1597, duration: 5.620s, episode steps: 717, steps per second: 128, episode reward: 233.424, mean reward:  0.326 [-21.628, 100.000], mean action: 1.172 [0.000, 3.000],  loss: 4.241265, mse: 4301.741931, mean_q: 61.383300, mean_eps: 0.101351
 273071/300000: episode: 1598, duration: 2.973s, episode steps: 394, steps per second: 133, episode reward: 242.465, mean reward:  0.615 [-9.796, 100.000], mean action: 1.467 [0.000, 3.000],  loss: 4.476638, mse: 4299.991642, mean_q: 61.468250, mean_eps: 0.099517
 273682/300000: episode: 1599, duration: 4.534s, episode steps: 611, steps per second: 135, episode reward: 159.968, mean reward:  0.262 [-14.289, 100.000], mean action: 1.463 [0.000, 3.000],  loss: 4.267324, mse: 4322.692029, mean_q: 61.737393, mean_eps: 0.097859
 273920/300000: episode: 1600, duration: 1.763s, episode steps: 238, steps per second: 135, episode reward: -121.916, mean reward: -0.512 [-100.000, 10.246], mean action: 1.832 [0.000, 3.000],  loss: 4.982458, mse: 4386.851839, mean_q: 62.206069, mean_eps: 0.096458
 274533/300000: episode: 1601, duration: 4.674s, episode steps: 613, steps per second: 131, episode reward: 199.414, mean reward:  0.325 [-17.429, 100.000], mean action: 1.732 [0.000, 3.000],  loss: 5.689105, mse: 4342.871572, mean_q: 61.756515, mean_eps: 0.095054
 274992/300000: episode: 1602, duration: 3.864s, episode steps: 459, steps per second: 119, episode reward: 222.492, mean reward:  0.485 [-17.564, 100.000], mean action: 1.290 [0.000, 3.000],  loss: 5.363158, mse: 4286.220819, mean_q: 61.398505, mean_eps: 0.093285
 275934/300000: episode: 1603, duration: 7.155s, episode steps: 942, steps per second: 132, episode reward: 208.090, mean reward:  0.221 [-19.407, 100.000], mean action: 1.529 [0.000, 3.000],  loss: 4.970073, mse: 4441.234959, mean_q: 62.607284, mean_eps: 0.090974
 276362/300000: episode: 1604, duration: 3.248s, episode steps: 428, steps per second: 132, episode reward: 171.324, mean reward:  0.400 [-17.297, 100.000], mean action: 1.661 [0.000, 3.000],  loss: 6.252264, mse: 4465.160857, mean_q: 63.222592, mean_eps: 0.088713
 276903/300000: episode: 1605, duration: 4.656s, episode steps: 541, steps per second: 116, episode reward: 205.716, mean reward:  0.380 [-11.328, 100.000], mean action: 1.237 [0.000, 3.000],  loss: 4.549157, mse: 4453.043962, mean_q: 63.085623, mean_eps: 0.087114
 277351/300000: episode: 1606, duration: 3.390s, episode steps: 448, steps per second: 132, episode reward: 147.457, mean reward:  0.329 [-15.703, 100.000], mean action: 1.991 [0.000, 3.000],  loss: 5.035513, mse: 4455.056112, mean_q: 63.281735, mean_eps: 0.085483
 278351/300000: episode: 1607, duration: 7.863s, episode steps: 1000, steps per second: 127, episode reward: -17.647, mean reward: -0.018 [-4.346,  4.854], mean action: 1.602 [0.000, 3.000],  loss: 4.531363, mse: 4476.383867, mean_q: 63.736905, mean_eps: 0.083093
 279058/300000: episode: 1608, duration: 5.837s, episode steps: 707, steps per second: 121, episode reward: -132.156, mean reward: -0.187 [-100.000, 13.902], mean action: 1.521 [0.000, 3.000],  loss: 5.710928, mse: 4485.834246, mean_q: 63.850786, mean_eps: 0.080277
 279404/300000: episode: 1609, duration: 2.480s, episode steps: 346, steps per second: 139, episode reward: 263.872, mean reward:  0.763 [-8.169, 100.000], mean action: 1.058 [0.000, 3.000],  loss: 6.228995, mse: 4489.226511, mean_q: 63.960277, mean_eps: 0.078539
 280404/300000: episode: 1610, duration: 7.884s, episode steps: 1000, steps per second: 127, episode reward: -34.494, mean reward: -0.034 [-19.025, 24.224], mean action: 1.666 [0.000, 3.000],  loss: 6.120942, mse: 4562.684044, mean_q: 64.584826, mean_eps: 0.076318
 281404/300000: episode: 1611, duration: 7.585s, episode steps: 1000, steps per second: 132, episode reward: 111.971, mean reward:  0.112 [-19.790, 22.652], mean action: 0.818 [0.000, 3.000],  loss: 4.613996, mse: 4441.580968, mean_q: 63.957453, mean_eps: 0.073018
 281850/300000: episode: 1612, duration: 3.416s, episode steps: 446, steps per second: 131, episode reward: 189.840, mean reward:  0.426 [-13.046, 100.000], mean action: 1.946 [0.000, 3.000],  loss: 4.344088, mse: 4426.858683, mean_q: 63.809561, mean_eps: 0.070633
 282495/300000: episode: 1613, duration: 5.408s, episode steps: 645, steps per second: 119, episode reward: 232.046, mean reward:  0.360 [-19.947, 100.000], mean action: 1.408 [0.000, 3.000],  loss: 4.489666, mse: 4577.287004, mean_q: 65.232101, mean_eps: 0.068832
 282892/300000: episode: 1614, duration: 3.053s, episode steps: 397, steps per second: 130, episode reward: 219.077, mean reward:  0.552 [-18.510, 100.000], mean action: 1.549 [0.000, 3.000],  loss: 4.002014, mse: 4544.334839, mean_q: 65.086613, mean_eps: 0.067113
 283812/300000: episode: 1615, duration: 7.775s, episode steps: 920, steps per second: 118, episode reward: 169.009, mean reward:  0.184 [-18.322, 100.000], mean action: 1.272 [0.000, 3.000],  loss: 4.261643, mse: 4561.415823, mean_q: 65.122955, mean_eps: 0.064940
 284448/300000: episode: 1616, duration: 5.180s, episode steps: 636, steps per second: 123, episode reward: 136.364, mean reward:  0.214 [-20.014, 100.000], mean action: 1.616 [0.000, 3.000],  loss: 5.780763, mse: 4535.143821, mean_q: 65.035437, mean_eps: 0.062373
 285177/300000: episode: 1617, duration: 6.593s, episode steps: 729, steps per second: 111, episode reward: 129.125, mean reward:  0.177 [-13.071, 100.000], mean action: 1.641 [0.000, 3.000],  loss: 5.014446, mse: 4523.845818, mean_q: 65.126408, mean_eps: 0.060120
 285485/300000: episode: 1618, duration: 2.354s, episode steps: 308, steps per second: 131, episode reward: 190.071, mean reward:  0.617 [-11.307, 100.000], mean action: 2.114 [0.000, 3.000],  loss: 5.522599, mse: 4616.465623, mean_q: 65.953469, mean_eps: 0.058409
 285848/300000: episode: 1619, duration: 2.791s, episode steps: 363, steps per second: 130, episode reward: 156.913, mean reward:  0.432 [-9.731, 100.000], mean action: 1.860 [0.000, 3.000],  loss: 5.059183, mse: 4665.875311, mean_q: 66.508110, mean_eps: 0.057302
 286272/300000: episode: 1620, duration: 3.294s, episode steps: 424, steps per second: 129, episode reward: 260.060, mean reward:  0.613 [-10.783, 100.000], mean action: 1.528 [0.000, 3.000],  loss: 3.251492, mse: 4568.322177, mean_q: 65.799192, mean_eps: 0.056004
 286557/300000: episode: 1621, duration: 2.132s, episode steps: 285, steps per second: 134, episode reward: 282.465, mean reward:  0.991 [-10.679, 100.000], mean action: 1.126 [0.000, 3.000],  loss: 4.191195, mse: 4659.687760, mean_q: 66.581296, mean_eps: 0.054834
 286851/300000: episode: 1622, duration: 2.143s, episode steps: 294, steps per second: 137, episode reward: 310.695, mean reward:  1.057 [-18.406, 100.000], mean action: 1.102 [0.000, 3.000],  loss: 5.540655, mse: 4650.986231, mean_q: 66.475052, mean_eps: 0.053878
 287287/300000: episode: 1623, duration: 3.421s, episode steps: 436, steps per second: 127, episode reward: 257.866, mean reward:  0.591 [-19.066, 100.000], mean action: 1.030 [0.000, 3.000],  loss: 5.177882, mse: 4597.987189, mean_q: 65.956126, mean_eps: 0.052674
 288287/300000: episode: 1624, duration: 8.323s, episode steps: 1000, steps per second: 120, episode reward: -78.795, mean reward: -0.079 [-4.468,  4.749], mean action: 1.856 [0.000, 3.000],  loss: 4.481938, mse: 4607.589424, mean_q: 66.021535, mean_eps: 0.050305
 288859/300000: episode: 1625, duration: 4.326s, episode steps: 572, steps per second: 132, episode reward: 204.244, mean reward:  0.357 [-22.224, 100.000], mean action: 1.425 [0.000, 3.000],  loss: 4.860783, mse: 4615.096226, mean_q: 66.037169, mean_eps: 0.047711
 289116/300000: episode: 1626, duration: 1.941s, episode steps: 257, steps per second: 132, episode reward: 250.428, mean reward:  0.974 [-6.645, 100.000], mean action: 2.039 [0.000, 3.000],  loss: 4.875713, mse: 4697.754795, mean_q: 66.940342, mean_eps: 0.046343
 289900/300000: episode: 1627, duration: 6.192s, episode steps: 784, steps per second: 127, episode reward: 87.157, mean reward:  0.111 [-13.144, 100.000], mean action: 1.672 [0.000, 3.000],  loss: 4.684435, mse: 4678.306601, mean_q: 66.997255, mean_eps: 0.044625
 290666/300000: episode: 1628, duration: 6.129s, episode steps: 766, steps per second: 125, episode reward: 108.784, mean reward:  0.142 [-21.881, 100.000], mean action: 1.659 [0.000, 3.000],  loss: 5.345917, mse: 4699.613706, mean_q: 67.228633, mean_eps: 0.042068
 291145/300000: episode: 1629, duration: 3.763s, episode steps: 479, steps per second: 127, episode reward: 193.886, mean reward:  0.405 [-19.724, 100.000], mean action: 1.718 [0.000, 3.000],  loss: 4.373270, mse: 4771.736827, mean_q: 67.963240, mean_eps: 0.040013
 291420/300000: episode: 1630, duration: 2.221s, episode steps: 275, steps per second: 124, episode reward: 303.941, mean reward:  1.105 [-3.056, 100.000], mean action: 1.549 [0.000, 3.000],  loss: 4.231365, mse: 4701.854285, mean_q: 67.463184, mean_eps: 0.038769
 291960/300000: episode: 1631, duration: 4.312s, episode steps: 540, steps per second: 125, episode reward: 231.105, mean reward:  0.428 [-20.118, 100.000], mean action: 1.937 [0.000, 3.000],  loss: 5.305338, mse: 4711.442396, mean_q: 67.624955, mean_eps: 0.037425
 292454/300000: episode: 1632, duration: 3.946s, episode steps: 494, steps per second: 125, episode reward: 135.897, mean reward:  0.275 [-10.776, 100.000], mean action: 1.804 [0.000, 3.000],  loss: 4.545182, mse: 4720.485544, mean_q: 67.691893, mean_eps: 0.035719
 292808/300000: episode: 1633, duration: 2.531s, episode steps: 354, steps per second: 140, episode reward: 246.427, mean reward:  0.696 [-11.154, 100.000], mean action: 1.472 [0.000, 3.000],  loss: 5.093236, mse: 4757.957017, mean_q: 67.983013, mean_eps: 0.034319
 293152/300000: episode: 1634, duration: 3.167s, episode steps: 344, steps per second: 109, episode reward: 197.770, mean reward:  0.575 [-12.462, 100.000], mean action: 1.788 [0.000, 3.000],  loss: 5.414816, mse: 4869.634265, mean_q: 68.875241, mean_eps: 0.033168
 293546/300000: episode: 1635, duration: 3.288s, episode steps: 394, steps per second: 120, episode reward: 142.810, mean reward:  0.362 [-22.358, 100.000], mean action: 1.726 [0.000, 3.000],  loss: 4.892390, mse: 4857.697878, mean_q: 69.100572, mean_eps: 0.031950
 294006/300000: episode: 1636, duration: 3.656s, episode steps: 460, steps per second: 126, episode reward: 205.380, mean reward:  0.446 [-13.620, 100.000], mean action: 1.841 [0.000, 3.000],  loss: 5.093460, mse: 4883.572405, mean_q: 69.321705, mean_eps: 0.030541
 294571/300000: episode: 1637, duration: 4.210s, episode steps: 565, steps per second: 134, episode reward: 173.729, mean reward:  0.307 [-13.071, 100.000], mean action: 1.664 [0.000, 3.000],  loss: 5.723667, mse: 4901.039107, mean_q: 69.281411, mean_eps: 0.028850
 295571/300000: episode: 1638, duration: 8.002s, episode steps: 1000, steps per second: 125, episode reward: 55.288, mean reward:  0.055 [-22.975, 19.012], mean action: 1.205 [0.000, 3.000],  loss: 5.050814, mse: 4900.732726, mean_q: 69.570984, mean_eps: 0.026267
 296013/300000: episode: 1639, duration: 3.327s, episode steps: 442, steps per second: 133, episode reward: 224.352, mean reward:  0.508 [-15.314, 100.000], mean action: 1.654 [0.000, 3.000],  loss: 4.375932, mse: 4941.320143, mean_q: 69.941453, mean_eps: 0.023888
 296606/300000: episode: 1640, duration: 4.571s, episode steps: 593, steps per second: 130, episode reward: 145.991, mean reward:  0.246 [-12.877, 100.000], mean action: 1.771 [0.000, 3.000],  loss: 4.402950, mse: 4964.821299, mean_q: 70.100280, mean_eps: 0.022180
 297312/300000: episode: 1641, duration: 5.401s, episode steps: 706, steps per second: 131, episode reward: 125.364, mean reward:  0.178 [-14.134, 100.000], mean action: 1.636 [0.000, 3.000],  loss: 6.049298, mse: 4949.117693, mean_q: 70.091209, mean_eps: 0.020037
 297846/300000: episode: 1642, duration: 4.387s, episode steps: 534, steps per second: 122, episode reward: 215.024, mean reward:  0.403 [-12.375, 100.000], mean action: 2.300 [0.000, 3.000],  loss: 4.490254, mse: 4946.886690, mean_q: 70.316779, mean_eps: 0.017991
 298183/300000: episode: 1643, duration: 2.569s, episode steps: 337, steps per second: 131, episode reward: 232.071, mean reward:  0.689 [-9.864, 100.000], mean action: 1.570 [0.000, 3.000],  loss: 4.894863, mse: 5038.729229, mean_q: 71.265583, mean_eps: 0.016554
 298725/300000: episode: 1644, duration: 4.273s, episode steps: 542, steps per second: 127, episode reward: 195.966, mean reward:  0.362 [-14.367, 100.000], mean action: 1.806 [0.000, 3.000],  loss: 6.347965, mse: 5074.764854, mean_q: 71.771438, mean_eps: 0.015103
 299082/300000: episode: 1645, duration: 2.642s, episode steps: 357, steps per second: 135, episode reward: 282.005, mean reward:  0.790 [-9.685, 100.000], mean action: 1.134 [0.000, 3.000],  loss: 5.375070, mse: 5062.530152, mean_q: 71.884506, mean_eps: 0.013620
 299572/300000: episode: 1646, duration: 3.906s, episode steps: 490, steps per second: 125, episode reward: 165.606, mean reward:  0.338 [-15.221, 100.000], mean action: 1.514 [0.000, 3.000],  loss: 4.587294, mse: 5003.720569, mean_q: 71.510349, mean_eps: 0.012223
 299790/300000: episode: 1647, duration: 1.695s, episode steps: 218, steps per second: 129, episode reward: 270.297, mean reward:  1.240 [-3.862, 100.000], mean action: 1.202 [0.000, 3.000],  loss: 5.285504, mse: 4994.593307, mean_q: 71.447802, mean_eps: 0.011054
done, took 2385.112 seconds
Testing for 5 episodes ...
Episode 1: reward: 149.698, steps: 530
Episode 2: reward: 251.664, steps: 489
Episode 3: reward: 120.949, steps: 592
Episode 4: reward: 209.980, steps: 407
Episode 5: reward: 140.299, steps: 480
Testing for 5 episodes ...
Episode 1: reward: 262.472, steps: 289
Episode 2: reward: 170.284, steps: 516
Episode 3: reward: -16.139, steps: 203
Episode 4: reward: -226.806, steps: 531
Episode 5: reward: 209.279, steps: 457